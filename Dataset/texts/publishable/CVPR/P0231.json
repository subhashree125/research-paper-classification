{
  "Prompt: A shanty version of Tokyo, new rustic style, bold colors with all colors palette, video game, genshin, tribe, fantasy, overwatch": ". Images generated with a RIN model trained with different handling of the misalignment between the image and its associated textat training. Compared to doing nothing (baseline), removing misaligned samples (filtering) or weighting the loss accordingly (weighted), ourCoherence-Aware Diffusion training (CAD) generates more visually pleasing images while better adhering to the prompts subject.",
  "Abstract": "Conditional diffusion models are powerful generative modelsthat can leverage various types of conditional information,such as class labels, segmentation masks, or text captions.However, in many real-world scenarios, conditional infor-mation may be noisy or unreliable due to human annotationerrors or weak alignment. In this paper, we propose theCoherence-Aware Diffusion (CAD), a novel method that in-tegrates coherence in conditional information into diffusionmodels, allowing them to learn from noisy annotations with-out discarding data. We assume that each data point hasan associated coherence score that reflects the quality ofthe conditional information. We then condition the diffusion model on both the conditional information and the coherencescore. In this way, the model learns to ignore or discount theconditioning when the coherence is low. We show that CADis theoretically sound and empirically effective on variousconditional generation tasks. Moreover, we show that lever-aging coherence generates realistic and diverse samples thatrespect conditional information better than models trainedon cleaned datasets where samples with low coherence havebeen discarded. Code and weights can be found here.",
  "arXiv:2405.20324v1 [cs.CV] 30 May 2024": "cess by integrating additional information . This extradata enables the model to guide the generated image towardsa specific target, leading to improved various applicationsincluding high-quality text-to-image generation , as wellas other modalities such as depth or human body pose .Furthermore, the accessibility of open-source models likeStable Diffusion has democratized the use of this technology,already causing significant shifts in various domains such asdesign, art, and marketing.Training conditional diffusion models requires substantialvolumes of paired data comprising the target image and itscorresponding condition. In text-to-image generation, thispairing involves an image and a descriptive caption that char-acterizes both the content and the style of the image. Simi-larly, for class conditional generation, the pair consists of animage and its corresponding class label. Besides the techni-cal challenges associated with the acquisition of extremelylarge quantities of paired data, ensuring accurate alignmentbetween image and text conditions is still an open researchquestion in the community, as attested by the large amountof recent work in the domain . In practice, large web-scraped datasets, such as LAION-5B or CC12M ,contain abundant noisy pairs due to their collecting process.To clean the pairs, hence ensuring alignment of higher qual-ity, the prevailing strategy filters out samples that fail to meetan arbitrarily chosen criterion, often done through techniqueslike thresholding the CLIP-score . This approach,however, has two main drawbacks: first, it is challengingto adjust the criterion accurately and more importantly, itdiscards many high-quality samples that could potentiallyenhance generation quality irrespective of the condition. Forinstance, out of the 50B initially-collected text-image pairs,only 10% were left in LAION-5B , thus discarding 90%of the samples, i.e. 45B images.Instead of discarding the vast majority of training sam-ples, in this work, we leverage them to learn simultaneouslyconditional and unconditional distributions. Specifically, weintroduce a novel approach that estimates what we call thecoherence score, which evaluates how well the conditioncorresponds to its associated image. We incorporate thiscoherence score into the training process by embedding itinto a latent vector, which is subsequently merged with thecondition. This additional information enables the diffusionmodel to determine the extent to which the condition shouldinfluence the generation of a target image. During inference,our method has the flexibility to take as input the coherencescore, thereby allowing users to vary the impact of the con-dition on the generation process, as illustrated in .In addition, to further improve the generated image quality,we refine the Classifier-Free-Guidance method (CFG) intro-duced in by leveraging the gap between high and lowcoherence scores.We evaluate our approach across three distinct tasks that involve various types of conditioning: text for text-to-imagegeneration, labels for class-conditioned image generation,and semantic maps for paint-by-word image generation. Intext conditioning, we use the CLIP score to estimate thecoherence between the image and its accompanying caption.For class-conditional generation, we employ an off-the-shelfconfidence estimator to gauge the coherence between theimage and its label. Concerning semantic maps, we derivepixel-level coherence scores either by automatically generat-ing them based on class boundaries or by using an off-the-shelf confidence estimator. Our evaluations span multipledatasets such as COCO for zero-shot text-to-image gen-eration, ImageNet for class-conditioned generation, andADE-20K for semantic maps. Our results show thatincluding the coherence score in the training process allowstraining diffusion models with better image quality and thatare more coherent with what they are prompted.In summary, our contributions can be outlined as follows: Innovative Training Approach: We present coherence-aware diffusion (CAD), a novel method for training con-ditional diffusion models in the presence of annotationimperfections. By incorporating a coherence score be-tween the target image and its associated condition, ourmodel can adapt and fine-tune the influence of the condi-tion on the generation process. Flexible Inference Scheme: We introduce a versatile in-ference scheme, in which manual tuning of the coherencescore during the generation enables the modulation of theconditions impact on image generation. Additionally, werefine the classifier-free guidance method under this newinference scheme, resulting in enhanced image quality. Wide Applicability: Demonstrating the versatility ofCAD, we evaluate it across three diverse tasks involvingdifferent types of conditions (text, class labels, and se-mantic maps). CAD produces visually pleasing resultsacross all tasks, emphasizing its generic applicability.",
  ". Related Work": "Conditional generation. Previous attempts to conditiongenerative models were focused on GANs .Class-conditional was the first way to introduce conditioningin generative models. This is a simple global condition-ing. However, it lacks control over the output. To increasethe control power of conditioning, more local conditioningswere proposed such as drawings, maps, or segmentationmaps . Segmentation maps conditioning propose the most control to the user. Indeed, the user cannot only specify the shape of the objects but also per-objectclass information. Semantic masks are however tedious todraw, which impacts usability. Text-conditioned models offer a compromise. They can provide both global and localconditioning and are easy to work with. Recently, diffusionmodels have made great advances in this domain.",
  "(b)": ". (a) Examples of images generated with the input coherence score between the prompt and the target image. The score varies from0 (no coherence) to 1 (maximum coherence). Higher coherence scores tend to generate images that adhere more effectively to the prompt.Top prompt: a raccoon wearing an astronaut suit. The racoon is looking out of the window at a starry night; unreal engine, detailed,digital painting,cinematic,character design by pixar and hayao miyazaki, unreal 5, daz, hyperrealistic, octane render, bottom prompt: Anarmchair in the shape of an avocado (b) Increasing the coherence from 0 to 1, CLIPScore increases and FID decreases. Diffusion models. Diffusion models have re-cently attracted the attention of research in image generation.Compared to GANs, they have better coverage over the datadistribution, are easier to train and outperform them in termsof image quality . Architecture-wise, diffusion modelsrely mostly on modified versions of a U-Net . Re-cent works have however shown that other architectures arepossible . In particular, RIN proposes a muchsimpler architecture than the U-Net achieving more efficienttraining. They recently have been a lot of works scaling up these models on huge text-to-image datasets .Stable Diffusion , Stable Diffusion XL , Paella or Wuerstchen have provided open-source weights fortheir networks, which has allowed an explosion in imagegeneration. ControlNet has shown that fine-tuning thesemodels allows for very fine-grained control over the outputwith lots of different conditioning modalities. Recently, con-sistency models have shown that by training morewith a different loss, inference can be done in small amountsof steps (2-4 steps). All these text-to-image networks havebeen tuned on very noisy web-scrapped data. We argue inthis paper that this noise causes limitations in the training.Concurrent works propose to tackle this taskthrough re-captioning, but this requires lots of resources totrain a good captioner that outputs detailed captions withouthallucinating details. As shown by , it also requires bridg-ing the gap between train and test time prompts. Instead, ourapproach is much simpler in current training setups.Learning with noisy conditioning has been widely ex-plored when considering classification. For binary classifi-cation, study machine learning robustness when con-fronted with noisy labels, while train a DNN with exclu- sively positive labels accompanied by confidence scores. Tobring a more practical perspective, introduced instance-dependent noise scored by confidence, where this scorealigns with the probability of the assigned labels accuracy.The negative impact of noisy labels has been mitigated withchanges in architecture , in the loss or filtering thenoisy samples . More recently, propose a similar ap-proach to ours by conditioning an image captioner model bythe CLIP-score to mitigate the impact of text misalignment.Instead, we focus on image synthesis, where we conditionthe diffusion model with a coherence score.",
  ". Coherence-Aware Diffusion (CAD)": "In this work, we want to improve the training of the diffu-sion model in the presence of misaligned conditioning. Wemake the assumption that for each training sample, a coher-ence score measures how coherent the conditioning is withrespect to the data. We propose to condition the diffusionmodel on this coherence score in addition to the originalcondition. By doing so, the model learns to discard the low-coherence conditions and focus on the high-coherence ones.Consequently, our model can behave as either a conditionalor an unconditional model. Low-coherence samples, leadto unconditional sampling, while high-coherence sampleslead to conditional samples. Building on this, we redesignclassifier-free guidance to rely on coherence conditioninginstead of dropping out the conditioning randomly.",
  "We first provide an overview of conditional diffusion models.These models learn to denoise a target at various noise lev-": "els. By denoising sufficiently strong noises, we eventuallydenoise pure noise, which can then be used to generate im-ages. Each diffusion process is associated with a network ,which performs the denoising task. To train such a network,we have X an image and y its associated conditioning com-ing from pdata the data distribution. We use a noise scheduler(t), which defines Xt which is the input image corruptedwith Gaussian noise at the t-th step of diffusion. such asXt =",
  "Lsimple = E(X,y)pdata,tU[ (Xt, y, t)],(1)": "where denotes the L2 norm.One observation is that the conditioning is implicitlylearned by the diffusion model, as the diffusion loss is onlyenforced on the image and not on the conditioning itself.This motivates our hypothesis that removing data with lowlabel coherence can harm the training of the diffusion model.Even if the conditioning is not well aligned, the image stillbelongs to the distribution that we aim to learn. By discard-ing such data, we weaken the distribution estimator.",
  ". Integrating label information into the diffusionmodel": "We assume that for every datapoint (X, y) we have an asso-ciated c, the coherence score of y where c . Our goalis to incorporate label coherence into the diffusion modelto discard only the conditioning that contains low levels ofcoherence while continuing to train on the image. A valueof c=1 indicates that y is the best possible annotation for X,while c=0 suggests that y is a poor annotation for X.To achieve this, we modify the conditioning of the diffusionmodel to include both y and c, using the following loss: Lsimple = E(X,y,c)pdata,tU[ (Xt, y, c, t)].(2)We refer to this kind of models as coherence coherence-aware diffusion (CAD) models. By informing the diffusionmodel of the coherence score associated with samples, weavoid filtering out low-confident samples and let the modellearn by itself what information to take into account. Avoid-ing the filtering allows us to still learn X even in the presenceof noisy labels.",
  ". Test-time prompting": "After training a model with different levels of coherence,we can thus prompt it with varying degrees of coherence.When we prompt with minimal coherence, we obtain anunconditional model. On the other hand, when we promptwith maximal coherence, we get a model that is very con-fident about the provided label. However, like any other conditional diffusion model relying on attention, there is noguarantee that the label is actually used.To strengthen the use of the label, we propose a modifi-cation to the Classifier Free Guidance (CFG) method that leverages the coherence. CFG uses both a conditionaland unconditional model to improve the quality of gener-ated samples. To learn such models, a conditional diffusionmodel is used and the conditioning is dropped out for a por-tion of the training samples. The original CFG formulationis as follows:",
  ". Experimental setup and Metrics": "Experimental setup.For text-conditional image genera-tion, we use a modified version of RIN . To map the textto an embedding space, we use a frozen FLAN-T5 XL .We then map the embedding with 2 self-attention transformerlayers initialized with LayerScale . We finally add theconditioning to the latent branch of RIN at each RIN Blockwith a cross-attention layer. The model has 188M parame-ters and we train it for 240K steps. We train these modelson a mix of datasets composed of CC12M and LAIONAesthetics 6+ . To estimate the coherence score, weuse MetaCLIP H/14 that we then bin into 8 equallydistributed discrete bins. We then use the normalized indexbetween 0 and 1 as the coherence score. We compare ourmethod to 3 baselines: \"Baseline\" is a model where we justtrain without coherence, \"Filtered\" corresponds to a modelwhere we discard the 3 less coherent bins, and \"Weighted\"corresponds to a model where we weight the loss of themodel by the normalized coherence of the sample. Whenusing Coherent Aware prompting, we sample a random setof characters that we use with coherence-score of zero as thenegative prompt.For the class-conditional image generation experiments, werely on RIN and use the same hyperparameters as the",
  "(c)": ". Text-to-image generation results. (a) Quantitative results for text-to-image generation. We show that CAD achieves significantlylower FID, precision, recall, density and coverage while keeping similar CLIP score. (b) User study results. Users had to indicate the highestquality image and the most adhering to the prompt among pairs of images corresponding to our CAD method and one of baseline, filteredor weighted method. (c) FID versus CLIP on the text-to-image task for varying degrees of guidance . We show that CAD achieves asignificantly better trade-off with a much lower FID for the same CLIP score. authors. We experiment on conditional image generation forCIFAR-10 and Imagenet-64 for which we artifi-cially noise the label. We extract the coherence score frompre-trained classifiers in the following way: We re-samplewith some temperature a new label from the label distribu-tion predicted by the classifier. We then consider the entropyof the distribution as the coherence score. After, we use asinusoidal positional embedding that we map with anMLP. We add this coherence token in the latent branch ofRIN, similar to the class token.For semantic segmentation conditioned experiments, we useControlNet to condition a pre-trained text-to-imageStable-Diffusion model with both semantic and coher-ence maps concatenated. The training and evaluation ofour method are performed on the ADE20K dataset , alarge-scale semantic segmentation dataset containing over20,000 images with fine-detailed labels, covering diversescenes and classes. Since captions are not available for thisdataset, we use BLIP2 to generate captions for eachimage in the dataset similarly to . We use a pre-trainedMaskformer on the COCO Dataset , to extract thesegmentation map and its associated confidence (MCP )for each image in the ADE20k dataset1. We use confidenceas our coherence score. More details about the experimentalsetup are available in the supplementary. Metrics.To evaluate image generation for all conditiontypes, we use the Frechet Inception Distance (FID) thatevaluates image quality. We also use Precision (P), Re-call (R), Density (D) and Coverage (C) asmanifold metrics, allowing us to evaluate how well the man-ifold of the generated images overlaps with the manifold ofthe real images. For text-conditional, we also compute theCLIP Score and evaluate metrics on CLIP features on",
  "It is worth noting that using a Maskformer trained on the same datasetwould result in high confidence map everywhere due to its high performanceon the training set": "a 10K samples subset of COCO in a zero-shot setting.For class-conditional, we compute the Inception Score (IS). We also add the Accuracy (Acc) metric aiming at eval-uating how well the image generator takes into account theconditioning and defined as Acc(g) = EcCat(N)[1f(g(c))=c],where g(.) is the generator we want to evaluate, f(.) is a clas-sifier, and Cat(N) is the categorical distribution of N labels.For CIFAR-10, we use a Vision Transformer trainedon CIFAR-10, and for ImageNet, we use a DeiT . Forsegmentation, instead of Accuracy, we compute the meanIntersection over Union (mIoU) instead of the Accuracy.",
  ". Analysis": "Coherence conditioning.Here, we explore the behavior ofour proposed coherence-aware diffusion model at test time.For the text conditional setting, we observe from bthat the coherence and the quality of the generated imageincrease as the coherence increases. Indeed, FID decreasesand the CLIPScore increases. In the class-conditional setup,we prompt a CAD model trained on resampled ImageNet andreport results in (a). Similar to the previous setting,when the model has very high coherence, it achieves thebest FID and accuracy. However, when the coherence scoredecreases, the accuracy decreases as well and drops to1Nwhen the coherence goes to 0. This validates our hypothesisthat in the presence of low-coherence samples, our proposedmodel behaves like an unconditional model. Furthermore,even if the FID increases, it remains close to the FID ofthe conditional model, which implies that our CAD samplesimages are close to the training distribution.Qualitatively, for text-to-image generation, we prompt themodel with varying coherence scores from 0 to 1 and displayresults in a. We observe that when the coherenceincreases, the outputs are close to the prompt. For instance,in the bottom figure, the generated image displays an avo-cado armchair, where avocado and armchair are successfullymixed. Even a more complex prompt, like the raccoon at",
  ". Coherence-Aware Classifier-free Guidance for classes Malamute and Ice Cream with guidance rates {0, 1, 5, 20}": "the top, follows closely the textual description. The raccoondoes wear an astronaut suit and is looking through the win-dow at a starry night. Similarly, as the coherence decreases,the images start to diverge from the original prompt. Theavocado chair starts to first lose the \"avocado\" traits untilthere is only a chair and at the end an object that does notlook like an avocado or a chair. At the top, we first lose thewindow, then the raccoon. We note that contrary to classconditional (as seen below), we do not converge to a totallyrandom image. Instead, some features from the prompt arepreserved, such as the astronaut suit and the starry night.This is highly linked to the CLIP network biases, which maypay less attention to less salient parts of an image such asthe background, and are more sensitive to the main subject.Similarly, for class-conditional, we prompt a CAD modeltrained on resampled ImageNet, with different coherencesand classes. We sample with DDPM but we seed the sam-pling to have the same noise when sampling different classes. (c) illustrates the results, where we observe thatwhen prompted with high coherence, CAD samples havethe desired class. However, as the coherence decreases, thesamples get converted into samples from random classes.Furthermore, samples that use the same sampling noise con-verge towards the same image in the low coherence regime.This shows that when the label coherence is low, CAD dis-cards the conditioning and instead samples unconditionally.To better understand the underlying mechanism, we de-sign the following experiment. We modify the proposedmodel so that instead of adding both a class and a coherencetoken to the RIN network, we merge them into a single tokenwith an MLP. (e) displays the t-SNE plots of theoutput of the MLP for every class in CIFAR-10 for which we compute different coherence scores. In the plot, highcoherence translates to low transparency. We observe that asthe coherence decreases, the embeddings of all classes tendto converge into the same embedding (center). This corrob-orates our hypothesis that the model uses the coherence tolearn by itself how much to rely on the label.Coherence-aware classifier-free guidance. Here, we ex-amine the impact of the coherence-aware classifier guidance.For this, we first compute different guidance rates rangingfrom 0 to 30 with 250 steps of DDIM, and then we plot theFID vs the CLIPScore the classifier accuracy for differentrates. (b) illustrates this. We observe a trade-offbetween classifier accuracy and FID. Specifically, the morewe increase the guidance, the more the accuracy increases,but at the cost of higher FID.Qualitatively, this behaviour is also present in : ata lower guidance rate, the images are more diverse but at thecost of lower accuracy with respect to the class. This patternis best shown in the Malamute example when = 20 (firstpart in ), where all malamutes have a similar posewith their tongues hanging and similarly, and in the third partwhere all ice-creams look similar, i.e., one white ice-creamscoop with red fillings.Interestingly, we also observe that some guidance leadsto optimal results. In , when = 1, the FID is atits lowest point, and the accuracy is higher than the defaultmodel that has = 0. This is also shown in , where,when = 1, samples best combine diversity and fidelity.",
  "(d)(e)": ". Impact of coherence on the model. Top: (a) Impact on FID and Accuracy of prompting a model (CAD on ImageNet with = 0.5). (b) FID vs Accuracy (using CA-CFG). We vary the guidance rate from 0 to 25. (c) Impact of prompting with different coherencescores on image generation. Low coherence indicates convergence towards an unconditional model. Bottom: (d) Quantitative results forclass-conditional image generation. Our coherence aware diffusion (CAD) is compared to a baseline model and a training set filteringstrategy for different levels of label noise . We show that CAD achieves higher fidelity and better accuracy. (e) TSNE of a mixed embeddingof the class label and the coherence score on CIFAR-10. Each color denotes a class and the transparency shows the coherence level: themore transparent, the less confidence. Qualitative results for text-conditioned image generation.In , we observe that coherence-aware diffusion fortextual conditioning allows for better prompt adherence andbetter-looking images. In the first row, we observe that CADis the only method that captures the details of the prompt,such as having the wolf play the guitar. Indeed, the base-line and filtered models output a wolf, but most generationsdisplay only a head. The weighted model performs slightlybetter but it lacks quality. Furthermore, our model displayshigher diversity in the output styles. For instance, this isvisible in the bottom row where our model displays a varietyof street images whereas the other methods tend to have acollapsed output. For this prompt, our model also displaysbetter image quality compared to other methods. Quantitative results for text-conditioned image genera-tion.In (c), we compare our method to a classicaldiffusion model (baseline), to a method where all sampleswith CLIPScore lower than 0.41 are filtered as is commonlydone with LAION-5B (Filtered), and one where the dif-fusion loss is wheighted by the CLIPScore of the sample(Weighted). We vary the coherence and plot the FID withrespect to the CLIPScore. We observe that our methodachieves significantly better FID/CLIP tradeoff than the othermethods. In (a), we report the metrics for the guid-ance parameter that gives the best FID. We observe that our method outperforms other methods on all metrics except forthe CLIPScore. In particular, our method achieves a betterFID by 15 points than the second-best method, i.e. the fil-tered model. We corroborate these results with a user studyin (b), where we generate images for randomlysampled captions in COCO. For each method, we generate10 pairs of images and we ask 36 users to vote for the imagewith the best quality, and for the one with the most coherenceto the prompt. Our method is overwhelmingly preferred toother methods. In particular, users prefer the image qualityof our images 95% of the cases and find our images betteraligned with the prompts by 89%. Notably, the user studyreveals a well-established limitation in such models, i.e. FIDvs CLIPScore tradeoffs do not necessarily correlate wellwith human perception, as shown also in SD-XL . Quantitative results for class-conditional image gener-ation.In (d), we compare to a baseline wherewe do not use the coherence score, and a filtered model,where we filter all samples with coherence scores lower than0.5. When filtering, we observe on CIFAR that the modelsperformance dramatically drops. FIDs are worse than thebaseline, showing that dropping images prevents generat-ing high-quality images. CAD displays improved Accuracyover the baseline while having better image quality than thefiltered baseline.",
  "(b) Quantitative results on ADE20k andMS COCO. We report FID, mIoU, Pre-cision (P), Recall (R), Diversity (D) andCoverage (C)": ". (a) Image generation conditioned on a semantic map. The images generated for a given prompt p (shown below) are shownwith respect to different pixel-level coherence scores c (shown above). Coherence scores are obtained either synthetically using Canny edgedetection on the semantic map, or from the maximum of the softmax probability of a pre-trained model. They can then be edited eithermanually ( + edit) or by blitting other coherence maps (rightmost). (b) Quantitative results on ADE20k and MS COCO.",
  "Qualitative results for semantic conditioning.In Figure": "6 (a), we present generated images derived from the samesemantic map, obtained through the prediction of a segmen-tation network on an image from the ADE20K dataset .Notably, we vary the prompts and coherence maps in ourexperiments. When using a uniform coherence map set atc = 1, the generated image aligns correctly with the se-mantic map but lacks semantic information, resulting in animage that may not appear meaningful. To introduce syn-thetic coherence maps, we employ Canny edge detection onthe semantic map, creating regions of low coherence at classboundaries. This approach gives the model more flexibilityin adjusting the shape of different objects, leading to a morerealistic image (second column).Additionally, we manually edit the coherence map by in-troducing a low-coherence region in the form of a square. Asdepicted in the \"+edit\" columns of (a), the modeltends to adhere to the shape defined by the coherence map.It strategically employs the location of this low-coherenceregion to incorporate objects that are specified in the promptbut absent from the semantic map. This observation is par-ticularly highlighted in the final image in (a), wherewe overlay the coherence scores of two children obtainedfrom another image onto the manipulated coherence map,while correspondingly adjusting the prompt. The modeladeptly uses the degrees of freedom and shape informationprovided by the low-coherence region of the coherence mapto seamlessly insert the children into the image. Quantitative results for semantic conditioning.In Fig-ure 6 (b), we demonstrate that incorporating both the seg-mentation and the coherence map leads to a decrease in FIDfor both scenarios, with and without the text input, indicat-ing the superior visual quality of the generated images. Thisbehavior is expected as our model possesses greater freedom",
  ". Conclusions": "We proposed a novel method for training conditionaldiffusion models with additional coherence information.By incorporating coherence scores into the conditioningprocess, our approach allows the model to dynamicallyadjust its reliance on the conditioning.We also extendthe classifier-free guidance, enabling the derivation ofconditional and unconditional models without the need fordropout during training. We have demonstrated that ourmethod, called condition-aware diffusion (CAD), producesmore diverse and realistic samples on various conditionalgeneration tasks, including classification on CIFAR10,ImageNet and semantic segmentation on ADE20k. Limitations.The main limitation of CAD lies in theextraction of coherence scores, as unreliable coherencescores can lead to biases in the model. Future researchincludes focusing on more robust and reliable methodsfor obtaining coherence scores to further enhance theeffectiveness and generalizability of our approach.",
  ". Acknowledgments": "This work was supported by ANR project TOSAI ANR-20-IADJ- 0009, and was granted access to the HPC resourcesof IDRIS under the allocation 2023-AD011014246 made byGENCI. We would like to thank Vincent Lepetit, RomainLoiseau, Robin Courant, Mathis Petrovich, Teodor Poncuand the anonymous reviewers for their insightful commentsand suggestion. Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Ji-aming Song, Karsten Kreis, Miika Aittala, Timo Aila, SamuliLaine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusionmodels with an ensemble of expert denoisers. arxiv, 2022. 2,3",
  "Pablo Pernias, Dominic Rampas, and Marc Aubreville. Wuer-stchen: Efficient pretraining of text-to-image models. arXiv,2023. 2, 3": "Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann,Tim Dockhorn, Jonas Mller, Joe Penna, and Robin Rombach.Sdxl: Improving latent diffusion models for high-resolutionimage synthesis. arXiv, 2023. 3, 7 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervision.Int. Conf. Mach. Lear., 2021. 2, 5",
  "A.1. RIN architecture for class conditional CAD": "We first explain our adaptation of the RIN architecture ,that we use in our experiments on class conditional genera-tion in the context of coherence aware diffusion. As shownin , the RIN block is composed of two branches:one with the latents and one with the patches. We concate-nate the timestep, coherence and conditioning embeddingsto the latents (olive, green and orange blocks), with thecoherence embedding being an addition of our method tothe original RIN architecture. Then, first, the latents gatherinformation from the input patches via Cross-Attention. Sec-ond, the latents are processed with N self-attention layers.Finally, the patches are updated from the latents via Cross-Attention. The RIN architecture consists of stacking multipleRIN blocks, where the next RIN block receives the updatedlatents and patches.During inference, RIN takes as input a noisy version ofthe image, a class, a timestep, and a coherence token topredict the noise that has been added to the clean version ofthe image. To improve the sampling, output latents from agiven step are forwarded as input to the next denoising step.For more details, see .",
  "A.2. CAD with text conditioning": "To enable text-conditioned image generation, we proposea modification to the RIN architecture, coined Text RINBlock (see ). First, the text tokens are mapped withthe coherence with 2x self-attention layers initialized withLayerScale (top part in the figure) and 16 registers .This mapping is the same for every Text RIN block. Notethat unlike the class conditional RIN block (), inour proposed Text RIN Block, the latent branch containsonly the latents; there are no concatenated tokens anymore.Instead, the mapped text tokens, coherence, and timestepembeddings provide information to the latent branch with across-attention layer at the beginning of the Text RIN block.Then, the rest of the architecture is the same as the RINblock. Similarly, during inference our architecture takes as inputa noisy version of the image, the timestep and coherencetokens, and a text conditioning that is mapped with a frozenFLAN T5 XL encoder. It then predicts the noise that hasbeen added to the clean image. Output latents from onedenoising step are also forwarded to the next denoising step.",
  "B.1. Text Conditining": "For the text conditional, we use the LAMB optimizer with a weight decay of 0.01. We use a learning rate of 0.001.The batch size is 1024. We use a linear warmup of thelearning rate for the first 10k steps and then use a cosinedecay. We train all models for 300k steps. We use an EMAdecay of 0.9999 for the last 50k steps.We use the Stable diffusion VAE encoder and performthe diffusion process in its embedding space in which theimage tokens have dimension 32x32x4. We have 4 RINblocks, each having 4 self-attention units. The data tokensdimension is 256 and the latent token dimension is 768. Theinput data is reduced to 256 data tokens by using a patch-sizeof 2.",
  "B.2. Class Conditioning": "For the class conditional experiments, we follow the hyper-parameters provided by the authors of RIN. We use theLAMB optimizer with weight decay of 0.01. We use a linearwarmup of the learning rate for the first 10k steps and thenuse a cosine decay. We train all models for 150k steps. Weuse an EMA decay of 0.9999.For CIFAR-10, we have 3 RIN blocks, each having 2 pro-cessing units. The data tokens dimension is 256 and thelatent token dimension is 512. We use a patch-size of 2. Weuse a learning rate of 0.003. The batch-size is 256.For ImageNet-64, we have 4 RIN blocks, each having 4",
  "B.3. Semantic map conditioning": "Generations conditioned on semantic maps were obtainedby training a ControlNet . To train the ControlNet, wecreated a dataset of images selected from the ADE20K and MS COCO datasets. This dataset contains tuplesof the form (image, caption, semantic map, coherence map)that were generated from the original images. The captionsare obtained with BLIP2 , an image captioning languagemodel. We utilize a Maskformer trained either on theMS COCO dataset or ADE20k to generate the semanticmaps. To extract coherence values, we use the maximumclass probability obtained from the softmax output of theMaskformer model. We employ a MaskFormer trained onADE20K to generate the coherence map for MS COCO,and conversely, use a MaskFormer trained on MS COCOto obtain the coherence map for ADE20K. This approachhelps mitigate the problem of overconfidence in predictionson the training set, reducing the tendency to have only highcoherence scores across all pixels. We used a batch size of16, using the Adam optimizer with a learning rate of 1e-5.",
  "B.4. Computational cost": "Our method adds negligible training and inference time be-cause we either modify existing architectures or add non-computationally expensive components. Specifically, interms of architecture, we are replacing one of the latenttokens with an embedded coherence score. For the text-conditional, we do add a new cross-attention layer, but mostof the compute is still in the self-attention blocks.In this project, we have used approximately 25,553 V100hours for preliminary experiments including the CIFAR-10 experiments and 29,489 A100 hours for ImageNet andtext-conditional experiments. Each GPU hour accounts forroughly 259 Wh for a total of 14,255kWh. For semanticsegmentation, the training of the different ControlNets wasperformed using about 1,800 hours of Nvidia A100 GPUsin total, or about 470kWh. The training process requiredapproximately 100 GPU hours for each model trained onADE20k and 200 GPU hours for MS COCO .",
  "C. Image from semantic map additional experi-ments": "In this section, we examine the effectiveness of incorporatingcoherence maps for semantic segmentation. We qualitativelyand quantitatively compare the results of our CAD methodagainst a baseline approach not using the coherence informa-tion. . Quantitative results on ADE20K when conditioning onsemantic maps. CAD bin encodes the coherence into 5 equallydistributed discrete bins. CAD scalar uses a scalar coherencescore for the whole image. CAD achieves better FID due to itsenhanced ability to generate realistic objects in low coherence re-gions and superior mIoU as the leaked spatial information from thecoherence map and the caption assist it to generate better samples(see Section C.1 for more details).",
  "C.1. Quantitative results": "Here, we quantitatively evaluate the effectiveness of incor-porating coherence maps for semantic segmentation. Forthis, we experiment on two of the most popular datasetswith segmentation: ADE20K and MS COCO . Toevaluate our results, we employ the Frechet Inception Dis-tance (FID) and Inception Score (IS) for evaluating imagequality. Additionally, Precision (P), Recall (R), Density (D),and Coverage (C) serve as manifold metrics, enabling anevaluation of the overlap between the generated and real im-age manifolds. Finally, we calculate the mean Intersectionover Union (mIoU) by utilizing a pre-trained MaskFormerto predict a segmentation map from the generated imageand compare it with the original semantic map. This helpsillustrate the fidelity of the generated images to the groundtruth. Method comparison.We compare the results of our CADmethod with a baseline approach that excludes coherenceinformation in both and , with the completeresults. We conduct experiments in two settings: with text(first two rows) and without text (last four rows). Addition-ally, we compare against two CAD variations. Similar tothe binning strategy in text-to-image generation, CAD binencodes coherence into 5 equally distributed discrete bins.Furthermore, CAD scalar utilizes a singular scalar coher-ence score for the entire image, equivalent to the mean ofthe original coherence map. Results.In and , show the complete resultsof our method on ADE20k and COCO we demonstrate thatusing both the segmentation maps and the coherence mapslead to a decrease in FID for both scenarios, including or notthe text input. This behavior is expected as our model pos-sesses greater freedom to generate realistic content insteadof sticking to the segmentation map uniquely (see e.g., the . Quantitative results on MS COCO when condition-ing on semantic maps. CAD bin encodes the coherence into 5equally distributed discrete bins. CAD scalar uses a scalar coher-ence score for the whole image (see Section C.1 for more details).",
  "Baseline w/o text54.9315.408.360.48840.44020.52970.5260CAD w/o text37.0618.0412.530.62220.65020.75990.7052CAD bin44.6316.399.760.56360.56140.70750.6402CAD scalar55.8215.928.100.51390.43820.52940.5341": "4th column of ). Furthermore, the improvement inthe mIoU score can be attributed to two factors. First, whenthe input segmentation map is of low quality, the baselinemethod fails to capture important scene information. In con-trast, our method benefits from additional information fromthe coherence map. Secondly, our method better leveragesthe caption in the low coherence region, mitigating the limi-tation of the segmentation maps limited number of classes(as seen in 4th row in , there is no ping-pong tableclass in the COCO dataset).",
  "C.2. Additional Visualizations": "We show additional results in , where, from the leftcolumn to the right one, we highlight the segmentation input,the coherence map, the image generated by the baseline, theimage generated by our methods and the reference image.The coherence map reveals spatial/shape details of the scene.For instance, when comparing our method to a ControlNettrained solely with the segmentation map, our approach,which incorporates both segmentation and coherence, accu-rately reconstructs the curtains shape and the windows inthe first row, or reconstructs a cloud in the back of the planein the second row. Moreover, the efficacy of our methodbecomes even more apparent when the segmentation map isof poor quality and the coherence score is low. For instance,as shown in the third row, the basic ControlNet attempts toadhere to the limited information provided by the flawedsegmentation map, resulting in a scene with multiple armsdisplayed (third column). In contrast, our approach benefitsfrom the flexibility given by the coherence map, allowing formore consistent image generation. Interestingly, our methodalso exhibits localized self-correction, as shown on the fourthrow. In particular, our model is refraining from generatingthe hand region due to its low coherence in the input. Fi-nally, we demonstrate the models responsiveness to textualinput in the last row. We present in the last row an examplewhere the original image does not contain snow, but havehigh coherence in the sky. Both models generate some snowbased on the caption, but in line with the coherence map, ourmodel does not generate snow in the sky, thus being closer",
  "C.3. Prompt generalization": "In this subsection, we demonstrate the sensitivity of ourmethod to the caption input. We observe in thatour CAD method can successfully generalize to differenttypes of captions, such as dining table, billiard, or pingpong table and adjust the scene accordingly. Moreover, evenwhen the table is not explicitly mentioned in the caption (asseen on the rightmost side of the figure), our method exhibitsstrong generalization capabilities and successfully generatesthe table.",
  "C.4. Coherence Interpolation": "In , we demonstrate the significance of the coher-ence map in the conditioning of the ControlNet. In thisexperiment, we make an interpolation of the coherence mapfrom the maximum coherence score everywhere (left) to verylow coherence (right). When the input has high coherencethroughout, the generated image lacks the presence of a pingpong table as it is not present in the semantic map. However,as the coherence score decreases, our methods recognize theshape of the ping pong table and successfully generates it inthe image. It is worth noting that even at a low scale of thecoherence score (second column corresponds to 1e-4 timesthe original value), our method is still able to reconstruct thetable, even if it is not in the segmentation input. When weartificially reduce the coherence (two times less coherence,in the last column), our method is still able to generate aconsistent scene without any artifacts.",
  "D.1. Simulating annotation noise": "Our approach for class-conditional image generation relieson the assumption that the dataset comes with annotatedcoherence scores. However, such scores are not always avail-able for traditional image-generation datasets. To addressthis issue, we propose to simulate annotation noise by re-sampling from a dataset with clean annotations.We associate an error probability with each label. Weassume that when the annotator is wrong, they misclassifyuniformly over all classes, where N is the total number ofclasses. This leads to the following model:",
  "High coherence Low coherence": ". Coherence interpolation: In the first column, we artificially provide our ControlNet with a coherence map having the maximumvalue everywhere. Indeed, our models do not generate the ping pong tables. But, as soon as we decrease the coherence toward its originalvalue, the ping pong tables start to appear. Finally, in the last column, we provide the model with a coherence map that is half as confident asthe original value and demonstrate that we can generate an image without artifacts",
  "1otherwise.(7)": "where represents a threshold and represents the entropyat this threshold. This function construction defines a lowentropy region before the threshold and a high entropy regionafter the threshold.Finally, for each sample in the dataset (X, y), we sample t U, and associate a target entropy u. We then computethe associated error probability = E1(u), and resampleaccording to py, to obtain the tuple (X, y, 1 u)2. Thisprocess allows us to generate synthetic data points withvarying degrees of annotation noise and coherence.",
  "D.2. Quantitative Results": "In this section we add more results on ImageNet with dif-ferent levels of noise. We observe in results onImageNet for {0.2, 0.5, 0.8}. We first observe the lesscoherent the labels are the worse the results get in both im-age quality (FID) but also in accuracy (Acc). However, ourmethod CAD manages to achieve better results than othermethods in the context of un-coherent labels. This is furtheramplified when leveraging coherence aware guidance . Quantitative results for class-conditional image generation.Our coherence aware diffusion (CAD) is compared to a baselinemodel and a training set filtering strategy for different levels oflabel noise . We show that CAD achieves higher fidelity andbetter accuracy.",
  "E. Theoretical analysis": "In this section, we motivate the use of coherence as an addi-tional conditioning for diffusion models. Under assumptionsthat are verified empirically, we show that coherence awarediffusion can transition from an unconditional model to aconditional model simply by varying the coherence passedto the model. First, we define a consistency property of thecoherence embedding as follows:",
  "Proposition E.1. Lipschitz continuous conditional neuraldiffusion models that leverage coherence consistent embed-dings for the conditioning are equivalent to unconditionalmodels at low coherence": "Proof. We have to prove the following equivalent statement:Let : xt, t, h(y, c) t be a Lipschitz continuous neu-ral diffusion model that predicts the noise t at time t fromthe noisy sample xt with the help of the condition y em-bedded using the coherence consistent embedding h undercoherence c. Then, > 0 and xt, t, y1 = y2, there existsC > 0 such that for all 0 < c C, we have"
}