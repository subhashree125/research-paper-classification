{
  "Abstract": "We propose a self-supervised approach for learningphysics-based subspaces for real-time simulation. Exist-ing learning-based methods construct subspaces by approx-imating pre-defined simulation data in a purely geomet-ric way. However, this approach tends to produce high-energy configurations, leads to entangled latent space di-mensions, and generalizes poorly beyond the training set.To overcome these limitations, we propose a self-supervisedapproach that directly minimizes the systems mechanicalenergy during training. We show that our method leadsto learned subspaces that reflect physical equilibrium con-straints, resolve overfitting issues of previous methods, andoffer interpretable latent space parameters.",
  "Physics-based simulation is an indispensable tool in sci-ence and engineering. Computer graphics has since longrelied on simulation to create stunning visual effects and": "animations. However, while our ability to create ever morerealistic simulations is steadily increasing, physics-basedsimulations are notoriously time-consuming, and translat-ing techniques to the real-time regime remains a major chal-lenge. One particular strategy to reduce computational bur-den without sacrificing visual quality is model reduction. These techniques construct low-dimensionalsubspaces that aim to capture the variability and expres-siveness of full-space simulations. However, designing sub-spaces that strike a balance between visual fidelity and com-putational complexity is no easy task. While linear vibra-tion modes are ideal for small deformations, they fail tocapture motions involving large deformations and rotations. Instead of approximating nonlinear simulations with alinear subspace, recent work has started to explore machinelearning techniques to build nonlinear subspaces from sim-ulation data . While this learning-based approach to sub-space simulation has shown its promise, existing methodsrely on supervised training, which leads to a number ofchallenges and limitations. First, supervised learning re-quires significant amounts of pre-defined and curated train-ing data.Second, this data is typically approximated ina purely geometric way, e.g., by minimizing the recon-struction loss of a deep autoencoder (AE) . This ge-ometric approach, however, ignores the fact that simula-tion data live on a nonlinear manifold defined through me-chanical equilibrium conditions. Consequently, configura-tions from geometrically-learned subspaces generally ex-hibit much higher internal energy than the input data, whichmanifests as stress concentrations and spurious forces thataffect the quality and performance of simulations. Anotherproblem is the empirical observation that supervised learn-ing often leads to overfitting and generalizes poorly to con-figurations outside the training set. We confirm this obser-vation for the case of nonlinear subspace simulation througha series of experiments. Finally, the learned representationof the subspace typically lacks structure and is often strewnwith singularities and discontinuities. In particular, latentspace dimensions are rarely meaningful (neither from en-ergetic, visual, nor semantic points of view), and smoothtrajectories in latent space typically lead to incoherent ge-",
  "arXiv:2404.17620v1 [cs.LG] 26 Apr 2024": "ometry.To address these challenges,we propose a novelself-supervised approach to learning nonlinear subspaces.Rather than building geometry-based subspaces from simu-lation data, our method learns physics-based subspaces bydirectly minimizing the energy of the underlying mechani-cal system. We draw inspiration from Nonlinear CompliantModes, an extension of linear modes to the large deforma-tion regime proposed by Dunser et al. . Each nonlinearmode is parameterized by a modal coordinate defined as theprojection of a given configuration onto the correspondinglinear mode. For any modal coordinate value, the corre-sponding nonlinear mode shape is obtained by minimizingthe elastic energy in the space orthogonal to the linear mode.To construct our neural subspaces, we extend NonlinearCompliant Modes to nonlinear modal spaces that capturethe behavior along and across a given set of modes. Cru-cially, since the map between modal coordinates and nonlin-ear equilibrium shapes is given by a constrained minimiza-tion problem, we can train our neural network to learn theentire solution manifold by minimizing the correspondingenergy across parameter space. Once trained, our methodallows for real-time exploration and gradient-based naviga-tion of nonlinear modal subspaces. We demonstrate the po-tential of our approach on a range of examples spanningreal-time dynamics simulation, physics-based nonlinear de-formations, and keyframe animation. We furthermore ana-lyze the performance of our method through ablation stud-ies and comparisons to baselines, revealing that our self-supervised approach outperforms existing methods basedon supervised learning. In summary, our work makes thefollowing contributions: We present the first self-supervised approach for learningnonlinear modal subspaces for simulation. Our approacheliminates the need for creating curated data collections,enables end-to-end physics-based training, and has quan-tified physical accuracy.",
  ". Related work": "Subspace Simulation. Model reduction or subspace simu-lation is an established technique with broad applications inmany scientific disciplines. Arguably, the most widely usedapproach is linear modal analysis , initially designed forvibration problems. The basic idea of this approach is to de- fine a low-dimensional linear basis using linear modes, i.e.,eigenvectors of the energy Hessian corresponding to low-frequency deformations. Despite its efficiency for small de-formations, linear eigenmodes are known to suffer from se-vere distortion artifacts for large deformations .Various methods aimed to reduce artefacts from lineareigenmodes by augmenting the basis with carefully selectedvectors. Some notable examples include modal derivatives, higher-order Krylov-type modes , higher-order de-scent directions , modal warping that corrects for rota-tions , and rotation strain extrapolation . Nonlinearcompliant modes are arguably the approach most closelyrelated to our work. These modes are found by constrainingthe projection of the systems configuration onto the lineareigenmodes while minimizing its internal energy in the or-thogonal subspace. The method is fully physics-based andhas been demonstrated for various analysis tasks on real-world 3D printed examples. However, in their original for-mulation, nonlinear compliant modes are inherently one-dimensional and do not readily extend to multi-dimensionalsubspace. We extend the concept of nonlinear compliantmodes to complete modal subspaces, enabling explorationand simulation.Nonlinear Subspaces. While the majority of works fo-cus on modal analysis, alternative strategies for constructingnonlinear subspace have been explored as well. Rig-spaceapproaches solve forward simulation and inverse kinemat-ics tasks in the nonlinear subspaces induced by characterrigs . However, these methods are tailoredto articulated objects and rely on artist-made rigs.Example-based approaches build sub-space based on a few user-selected example states. Increas-ing the number of examples to full simulation sequencesgives rise to data-driven methods.A widely used ap-proach in this context is to construct linear subspaces fromsimulation data using principal component analysis (PCA). Due to the limited expressiveness of linear subspaces,however, recent methods have resorted to neural networksthat can directly capture the nonlinearity of the underlyingphysics.Neural Physics.Inspired by physics-informed neu-ral networks (PINN) , current state-of-the-art methods leverage neural networks for subspace construc-tion and simulation. These methods collect a large numberof deformed states and train, e.g., deep autoencoders to ob-tain a latent manifold. A notable example is latent-space dy-namics , which achieves physical awareness by collect-ing training data from a physical simulator. Shen et al. augment latent-space dynamics with higher-order deriva-tives to accelerate simulation. Romero et al. proposeto learn nonlinear neural corrections for linear handle-basedsubspaces. While these methods have shown the promise oflearning-based subspaces, they rely on large amounts of cu- rated simulation data for training. This data collection istime-consuming, and learned subspaces are biased towardsthe training data. Additionally, existing methods train net-works in a purely geometric way, leading to subspaces thatare riddled with high-energy configurations. By contrast,our method learns nonlinear subspaces in a physically prin-cipled and fully self-supervised way, eliminating the needfor curated simulation data. Our learned subspaces are in-terpretable and exhibit superior regularity compared to pre-vious methods. Sharp et al. have likewise explored thepotential of unsupervised learning for subspace generation.Their method introduces a regularization term to encouragemode orthogonality. However, as we show in our analy-sis (Sec. 4.4), this regularizer cannot always prevent modecollapse, i.e., a decrease in effective dimensionality of thelearned subspaces.Neural simulation has attracted special attention for clothsimulation . Bertiche et al. , Santeste-ban et al. , and Grigorev et al. have investigatedself-supervised learning techniques. Whereas these tech-niques are specialized for garments, our approach appliesto a wider range of mechanical systems and materials.",
  ". Theory": "We start by introducing our novel self-supervised learn-ing approach for nonlinear physics-based subspaces in Sec-tion 3.1. Since our focus is on learning nonlinear modalsubspaces, we briefly summarize linear modes and nonlin-ear extensions in .2. In .3, we presentNeural Modes, i.e., learned nonlinear subspaces that aretrained in a fully self-supervised way.",
  "s.t.C(x) = 0": "where E(x) is the mechanical energy defined through in-trinsic parameters including material properties and inputshape. Furthermore, C(x) is a vector-valued constraintfunction modeling the boundary conditions, which are de-fined through a set of extrinsic parameters . The solutionset x(, ) constitutes a nonlinear subspace of R3n cor-responding to physically-principled equilibrium configura-tions. Using conventional simulation, sampling from thismanifold amounts to solving a constrained minimization",
  "s.t.C(x[](, )) = 0": "Instead of optimizing for the unknown equilibrium config-uration, a single neural network forward pass instantly pro-vides the solution x. Unlike supervised approaches thataim to learn solution spaces based on simulation data, wefind the network weights by directly minimizing the me-chanical energy across parameter space. To this end, wedefine a physics-based loss function,",
  "L() = E,[ E(x[](, )) + C(x[](, ))2 ] ,(3)": "where we used a penalty function with stiffness coefficient to enforce constraints. Once trained, the optimal giverise to a nonlinear subspace that, instead of approximatingsimulation data in a geometric sense, reflects the physicalprinciples of the underlying mechanical system. Moreover,this learning process is entirely self-supervised and there-fore easy to automate. We focus on thin shells and volumet-ric solids as discrete mechanical systems and build our sub-spaces on nonlinear compliant modes . However, our for-mulation generalizes to a broad range of mechanical mod-els, materials, and subspace parameterizations. The onlyrequirement is that the energy function and constraints aredifferentiable and weakly convex. In the following, we ap-ply our formulation to nonlinear modal subspaces.",
  "uT u z = 0 ,": "that solutions to the constrained optimization problem (5)are eigenvectors ei of the Hessian, i.e., Hei = iei.In particular, each eigenvector ei+1 minimizes the sys-tems energy in the 3n i-dimensional subspace orthog-onal to all previous eigenvectors ei. This observation moti-vates the choice of constructing linear subspaces from them eigenvectorsthe linear modescorresponding to thesmallest eigenvalues. For small deformations, where thequadratic energy approximation is accurate, linear modesform an energetically optimal subspace in the sense of themechanical work required to achieve a displacement ofgiven magnitude. For larger deformations, however, lin-ear modes suffer from distortion artifacts that manifest asa steep increase in energy and physically-implausible con-figurations; see, e.g., .Nonlinear Compliant Modes.To overcome the limi-tations of linear modes, Dunser et al. define nonlinearcompliant modes as",
  "ni(z) = li + arg miny E(X + li + y)s.t.lTi y = 0 (8)": "where the correction y is obtained by minimizing the sys-tems energy in the space orthogonal to the linear mode. Asdemonstrated by Dunser et al., nonlinear compliant modescan achieve large and energetically-optimal deformations inthe sense of (7). Nevertheless, the original formulation isrestricted to individual modes and does not directly extendto multi-dimensional modal spaces. Furthermore, nonlinearcompliant modes are very expensive to compute as evalu-ating the trajectory at a given parameter location requiresthe solution of a nonlinear constraint optimization problem.While these limitations seem to rule out nonlinear com-pliant modes for interactive subspace simulation, we showin the following that our self-supervised learning approachproposed in .1 can solve both problems in an ele-gant and efficient way.",
  "L() = Ez[ E(X + l + y[](z)) + (lT y[](z))2 ](12)": "During training, we draw the modal coordinates z from auniform distribution with predefined domain. We input z tothe network, and retrieve vertex positions x = X + l + yfrom the network output y. While experimenting with thisloss function, we observed that the trained network pro-duces nonzero corrections at the origin, i.e., y[](0) = 0.Since Neural Modes should coincide with linear modes forsmall displacements, we add a term penalizing correctionsat the origin, leading to the modified loss function",
  ". Results": "We implemented all our experiments and network train-ing in PyTorch, taking advantage of automatic differenti-ation. We use conventional multi-layer perceptrons (MLP)with Gaussian Error Linear Units (GELU) to implement ourNeural Modes. We also experimented with Residual Net-works (ResNet) but found no improvements in practice. Wetrain our networks on a single RTX 3090 GPU using L-BFGS with line search as the optimizer. We use discreteshells for deformable surfaces and linear tetrahedronelements with a Saint Venant-Kirchhoff material for solids. We summarize our experiment setup in .",
  ". Performance Comparisons": "We quantitatively evaluate our approach on a square shellwith a flat rest state. We train a 5-layer MLP to learn a3-dimensional modal subspace using modal coordinates zfrom the domain [0.625, 0.625]3. We compare two vari-ants of training for our MLP using stochastic and regular(grid-based) sampling, respectively.For stochastic sam-pling, we choose different modal coordinates z in each iter-ation by uniformly sampling the domain [0.625, 0.625]3.For regular sampling, we sample z from a 93 uniform grid.We compare our method with Latent-space Dynamics, which combines a deep autoencoder (AE) with princi-pal component analysis (PCA). We abbreviate their methodas PCA+AE. Our MLP essentially corresponds to the de-coder part of the autoencoder used in PCA+AE. For a faircomparison, we use the same MLP as PCA+AEs decoderand make a symmetric copy for the encoder.To train PCA+AE, we collect a dataset by regularly sam-pling z and solving (9) for each sample. The training, val-idation, and test sets have resolutions of 93, 73, and 113 samples, respectively.We also collect a second trainingset of the same size by randomly sampling modal coordi-nates. We train separate instances of PCA+AE on these twotraining sets (grid, random), and train our MLP using bothstochastic and regular sampling. We evaluate and compareall methods on the same test and validation sets.Elastic Energy. The evaluation results are shown in Ta-ble 2. Since nonlinear modes are constrained minimizers of(9), we use average elastic energy as a metric for measuringthe accuracy of a subspace. It can be seen from thatour method has lower average energy than PCA+AE and anorder of magnitude smaller errors compared to the groundtruth solution. We hypothesize that the lower accuracy ofPCA+AE is due to its geometry-based loss function and theassociated likelihood of overfitting to the training set. Wetest this hypothesis in two ways. First, we train our network again using the same archi-tecture but use the loss function from PCA+AE, which mea-sures the geometric distance to the training data in the L2sense. We then train on the same regular sampling set, i.e.,using the same data as before. As can be seen from Ta-ble 2, the resulting network (L2 supervised) exhibits signif-icantly higher energy than our Neural Modes. Second, weplot learning curves for all networks in . We cansee that, while PCA+AE and L2 Supervised variants reducetraining L2 loss steadily, their test L2 loss, energy, and stressall increase during training. In contrast, our self-supervisedNeural Modes do not suffer from overfitting, steadily reduc-ing the energy and stress of the test set. Finally, we experi-mented with early stopping, which is a common strategy tomitigate overfitting. We use both L2 loss and elastic energyas stopping criteria and observed somewhat better resultsfor the PCA+AE variants when using energy (see ).Nevertheless, the energy for PCA+AE is still significantlyhigher than for Neural Modes. Stress & Nodal Force. We further evaluate the learnedsubspace by analyzing internal stresses and nodal forces.We compute averaged stress weighted over element areasand maximum element stress in . We achieve smallerstress and significantly lower errors compared with super-vised methods. In addition to stress, we also observe oneorder of magnitude smaller errors in nodal forces (see Ta-ble A1). These results support our hypothesis that the ge-ometric L2 loss is unable to capture the physical principlesthat govern the manifold of equilibrium configurations. It isfurthermore prone to overfitting and can be expected to gen-eralize poorly in practice. By directly minimizing energyduring training, our self-supervised method shows muchbetter performance while avoiding overfitting problems. Deformable Solids. We additionally evaluate the qual-ity of Neural Modes for deformable solids, using the ex-ample shown in . We stochastically train an MLPto learn a 5-dimensional subspace. Similar to the previousexample, we collect 1300 equilibrium states by uniformlysampling modal space and compute ground truth solutionsfor each sample. The first 900 samples are used for train-ing, the subsequent 100 samples are used for validation, andthe last 300 samples are reserved for the test set. We stoptraining after 2050 epochs as the L2 loss evaluated on thevalidation set continually increases for the following 500epochs. The statistics are shown in . To ensure ourevaluation result is not sampled by chance, we break downthe test set into subintervals and compare Neural Modes andPCA+AE on each subinterval in addition to the completeset. It can be seen that Neural Modes outperform PCA+AEboth for every interval and for the complete test set. This re-sult confirms our observation for deformable surfaces, indi-cating that Neural Modes consistently outperform PCA+AEby significant margins.",
  ". Latent Space Structure": "The previous experiments indicate that our self-supervisedlearning approach yields subspaces with lower mechani-cal energy, stress, and nodal force than PCA+AE. Conse-quently, Neural Modes exhibit substantially lower errorswith respect to ground truth values obtained by solving the underlying constrained minimization problem. We furtheranalyze the structure of the latent spaces with a focus onsmoothness, interpretability, and regularity. Specifically, weinvestigate whether the map from latent space to full spaceis (1) smooth in the sense that smooth motion in latent spaceleads to smooth interpolated trajectories in geometry space,(2) regular in the sense that interpolated geometry is se-mantically bounded by terminal states, and (3) interpretablein the sense that individual latent space dimensions have aclear and distinguishable effect on the full space geometry. To analyze interpretability, we consider the same exam-ple as before and explore the behavior of Neural Modes andPCA+AE when walking along individual latent space coor-dinate directions. As can be seen from , Neural Modesfaithfully reproduce the individual nonlinear modes andpreserve symmetry. We also observe physically-plausibledeformations when combining multiple latent space dimen-sions (see Figure A1). The picture looks quite different forPCA+AE. Although each latent space dimension featuresmeaningful shapes, they are interspersed with distorted orcrumpled shapes that are physically implausible. We fur-ther note that Neural Modes preserve both the origin (i.e.,n(0) = X) and symmetry of the ground truth modes,whereas PCA+AE does neither. A basic test for regularity is interpolation quality .The median of two latent vectors is expected to yield a se-mantically meaningful median result and no abrupt changesfrom terminal states. We visualize interpolation quality inFigures 3 and A1 using evenly spaced sample points. Itcan be seen that PCA+AE exhibits poor regularity. By con-trast, Neural Modes generate semantically meaningful in-termediate states, defining a smooth trajectory in geometryspace. Unlike PCA+AE, this semantic interpolation prop-erty makes Neural Modes attractive for applications such askeyframe animations (see ). . Comparison of internal stress on the square sheet example. We compare average per-element stresses weighted by areas andmaximum per-element stress. We compute the Frobenius norm of stress |S| and its error |S| with respect to ground truth values for eachmethod. The subscript denotes the reduction operator for the batch dimension.",
  ". Material and Shape Parameters": "Neural modes are naturally parameterized by modal coordi-nates, but they generalize to other parameters such as shapeand material properties. Figure A2 showcases a simple ex-ample where we add aspect ratio as an auxiliary parameterfor a rectangular sheet. Starting from a sheet with edgelength ratio 2:1, the folding direction smoothly changesfrom the sheets short axis to its diagonal as we decreasethe ratio towards 1:1.",
  ". Subspace Efficiency": "We additionally compare subspace efficiency with Sharpet al. . Like our method, Sharp et al. learn nonlin-ear subspaces in an unsupervised way using the mechanicalenergy E(x) as loss function.Using only energy, how-ever, the optimal network would map all latent variablesto the undeformed state x = X since E(X) = 0.To",
  ". Correlation matrices of latent directions for Armadillo": "avoid this subspace collapse, Sharp et al. introduce a reg-ularization term that encourages the network to produce anisometric map from latent space to full space. Althoughthis regularizer avoids subspace collapse, it cannot preventmode collapse, i.e., a loss of effective dimensionality. Toshow this, we use the method by Sharp et al. to learn a5D-subspace for the Armadillo model. We compute latentdirections E = [e1| . . . |e5] by evaluating the networksgradient at the origin and compute the correlation matrixET E (). The eigenvalues of the correlation matrixare (2.3, 1.6, 1.1, 0.0, 0.0), revealing that the subspace is ef-fectively 3-dimensional. Applying the same test to NeuralModes yields first-order orthogonal latent directions and aquasi-diagonal correlation matrix with all eigenvalues closeto 1.0 (see ). Mode collapse limits the efficiency of subspaces.Asshown in Figure A3, while our Neural Modes capture botharm and leg motions, the method by Sharp et al. only learnsleg motions with the aforementioned redundancy.",
  ". Applications": "Complementing our quantitative analysis, we additionallyevaluate our method on a set of application examples thatillustrate potential use cases in modeling and animation.Subspace Dynamics As our first example, we considera subspace simulation problem using the deformable Ar-madillo puppet shown in . The model is driven bysix rods whose rest lengths we animate using sinusoidal mo-tion. We train an 11-dimensional subspace, with 5 DoFscorresponding to Neural Modes, and 6 DoFs for rigid trans-formations. It is evident from but best observedin the accompanying videothat linear modes lead to largedistortion artifacts in the arms and legs. This issue is onlypartly mitigated when adding ten extra basis vectors. Neu-ral Modes, in contrast, produce nonlinear rotations as ex-pected from the ground truth simulation. Moreover, ourmethod achieves real-time performance in 35ms per timestep, whereas the full simulation is two orders of magnitudeslower.We consider two additional examples to illustrate thatour method generalizes to arbitrary shapes: a giraffe and abunny model simulated using Neural Modes subspaces with3 and 8 dimensions, respectively. Both models run at real-time rates and produce smooth, visually pleasing motion.Please see the accompanying video for animations.Keyframing.Keyframe animation is a basic tool forgenerating motion from a sparse set of input poses. Givena set of keyframe poses through modal coordinates, we can",
  ". Conclusion": "We presented a novel self-supervised method for learningnonlinear physics-based subspaces for real-time simulation.We demonstrated the potential of our formulation by learn-ing Neural Modes, i.e., physics-based subspaces built ona nonlinear extension of linear eigenmodes. We quantita-tively evaluated our method on a set of examples that in-clude thin shells and volumetric solids. Furthermore, weperformed an extensive ablation study, revealing that di-rectly minimizing elastic energy leads to significantly betterphysical accuracy than using simpler geometry-based lossfunctions. We also showed that self-supervised learning re-solves overfitting problems that supervised approaches of-ten struggle with. By visualizing latent spaces, we demon-strated that Neural Modes are interpretable, free from singu-larities, and well-behaved for arbitrary modal combinations.We further showed that, unlike existing alternatives, our for-mulation avoids the problem of mode collapse. Finally, weshowcase applications of our approach to subspace simula-tion and keyframing.",
  "Limitations & Future Work": "Our learning method is restricted to differentiable energyfunctions and constraints. Additionally, we did not inves-tigate simulation acceleration algorithms and relied on L-BFGS for optimization. Future explorations could computethe exact Hessian in the reduced parameter space, whichwould open the door to more efficient minimization algo-rithms such as the Newton-Raphson method.While wehave focused on subspaces induced by nonlinear modes, ourapproach generalizes to many other parameterizations. Anexciting avenue for future work in this context would be toextend our method to inverse design problems, in, e.g., ar-chitecture or engineering design.",
  "Fabian Hahn, Sebastian Martin, Bernhard Thomaszewski,Robert Sumner, Stelian Coros, and Markus Gross. Rig-spacephysics. ACM Trans. Graph., 31(4), 2012. 2": "Fabian Hahn, Bernhard Thomaszewski, Stelian Coros,Robert W. Sumner, and Markus Gross.Efficient simula-tion of secondary motion in rig-space. In Proceedings of the12th ACM SIGGRAPH/Eurographics Symposium on Com-puter Animation, page 165171, New York, NY, USA, 2013.Association for Computing Machinery. 2 Fabian Hahn, Bernhard Thomaszewski, Stelian Coros,Robert W. Sumner, Forrester Cole, Mark Meyer, TonyDeRose, and Markus Gross. Subspace clothing simulationusing adaptive bases. ACM Trans. Graph., 33(4), 2014. 1",
  "ACM SIGGRAPH/Eurographics Symposium on ComputerAnimation, New York, NY, USA, 2019. Association forComputing Machinery. 2, 3": "Jin Huang, Yiying Tong, Kun Zhou, Hujun Bao, and Math-ieu Desbrun. Interactive shape interpolation through control-lable dynamic deformation. IEEE Transactions on Visualiza-tion and Computer Graphics, 17(7):983992, 2011. 2 Theodore Kim and David Eberle.Dynamic deformables:Implementation and production practicalities (now withcode!). In ACM SIGGRAPH 2022 Courses, New York, NY,USA, 2022. Association for Computing Machinery. 5",
  "Guiqing Li, Yaobin Ouyang, Guodong Wei, Zhibang Zhang,and Aihua Mao. Enhanced rig-space simulation. ComputerAnimation and Virtual Worlds, 27(3-4):250261, 2016. 2": "Xuan Li, Yu Fang, Lei Lan, Huamin Wang, Yin Yang,Minchen Li, and Chenfanfu Jiang. Subspace-preconditionedgpu projective dynamics with contact for cloth simulation. InSIGGRAPH Asia 2023 Conference Papers, New York, NY,USA, 2023. Association for Computing Machinery. 1 Emmanuel Ian Libao, Myeongjin Lee, Sumin Kim, andSung-Hee Lee. Meshgraphnetrp: Improving generalizationof gnn-based cloth simulation. In Proceedings of the 16thACM SIGGRAPH Conference on Motion, Interaction andGames, New York, NY, USA, 2023. Association for Com-puting Machinery. 3",
  "Ahmed K Noor and Jeanne M Peters. Reduced basis tech-nique for nonlinear analysis of structures. Aiaa journal, 18(4):455462, 1980. 2": "Young Jin Oh, Tae Min Lee, and In-Kwon Lee. Hierarchicalcloth simulation using deep neural networks. In Proceedingsof Computer Graphics International 2018, page 139146,New York, NY, USA, 2018. Association for Computing Ma-chinery. 3 Amir Rabbani and Paul Kry. Physik: Physically plausibleand intuitive keyframing. In Proceedings of the 42nd Graph-ics Interface Conference, page 153161, Waterloo, CAN,2016. Canadian Human-Computer Communications Society.2 Maziar Raissi, Paris Perdikaris, and George E Karniadakis.Physics-informed neural networks: A deep learning frame-work for solving forward and inverse problems involvingnonlinear partial differential equations. Journal of Computa-tional physics, 378:686707, 2019. 2",
  "the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 81408150, 2022. 3": "Christian Schumacher, Bernhard Thomaszewski, StelianCoros, Sebastian Martin, Robert Sumner, and Markus Gross.Efficient simulation of example-based materials. In Proceed-ings of the ACM SIGGRAPH/Eurographics Symposium onComputer Animation, page 18, Goslar, DEU, 2012. Euro-graphics Association. 2 Ahmed A Shabana. Theory of vibration. Springer, 1991. 2 Nicholas Sharp, Cristian Romero, Alec Jacobson, EtienneVouga, Paul G. Kry, David I. W. Levin, and Justin Solomon.Data-free learning of reduced-order kinematics, 2023. 2, 3,7",
  "Siyuan Shen, Yin Yang, Tianjia Shao, He Wang, ChenfanfuJiang, Lei Lan, and Kun Zhou.High-order differentiableautoencoder for nonlinear model reduction.ACM Trans.Graph., 40(4), 2021. 2": "Edgar Tretschk, Ayush Tewari, Michael Zollhofer, VladislavGolyanik, and Christian Theobalt. Demea: Deep mesh au-toencoders for non-rigidly deforming objects. In ComputerVisionECCV 2020: 16th European Conference, Glasgow,UK, August 2328, 2020, Proceedings, Part IV 16, pages601617. Springer, 2020. 2 Soumianarayanan Vijayaraghavan, Ling Wu, Ludovic Noels,SPA Bordas, Sundar Natarajan, and Lars AA Beex. A data-driven reduced-order surrogate model for entire elastoplasticsimulations applied to representative volume elements. Sci-entific Reports, 13(1):12781, 2023. 1"
}