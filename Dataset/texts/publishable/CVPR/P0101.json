{
  "Abstract": "Benchmarking vision-based driving policies is challenging. On one hand, open-loop evaluation with real data is easy, but these results do not reflect closed-loop performance. On the other, closed-loop evaluation is possible in simulation,but is hard to scale due to its significant computational demands. Further, thesimulators available today exhibit a large domain gap to real data. This hasresulted in an inability to draw clear conclusions from the rapidly growing body ofresearch on end-to-end autonomous driving. In this paper, we present NAVSIM, amiddle ground between these evaluation paradigms, where we use large datasetsin combination with a non-reactive simulator to enable large-scale real-worldbenchmarking. Specifically, we gather simulation-based metrics, such as progressand time to collision, by unrolling birds eye view abstractions of the test scenes fora short simulation horizon. Our simulation is non-reactive, i.e., the evaluated policyand environment do not influence each other. As we demonstrate empirically, thisdecoupling allows open-loop metric computation while being better aligned withclosed-loop evaluations than traditional displacement errors. NAVSIM enableda new competition held at CVPR 2024, where 143 teams submitted 463 entries,resulting in several new insights. On a large set of challenging scenarios, we observethat simple methods with moderate compute requirements such as TransFuser canmatch recent large-scale end-to-end driving architectures such as UniAD. Ourmodular framework can potentially be extended with new datasets, data curationstrategies, and metrics, and will be continually maintained to host future challenges.Our code is available at",
  "Introduction": "Autonomous vehicles (AVs) have gained immense research interest due to their potential to changetransportation and improve traffic safety . This has created a large community working on thedevelopment of AV algorithms, which map high-dimensional sensor data to desired vehicle controloutputs. Therefore, measuring and comparing the performance of AV algorithms is a crucial task. Unfortunately, it is extremely challenging to evaluate driving performance, and the most widely-usedbenchmarks today fall short in several respects: (1) the datasets used, such as nuScenes , werecreated for perception tasks such as object detection. As such, they focus on visual diversity andlabel quality instead of the relevance of the data for research on planning. Often, most frames have atrivial solution of extrapolating the historical driving behavior, leading to blind driving policies thatobserve only the vehicles past trajectory obtaining state-of-the-art performance . (2) Dueto the fact that driving is an inherently multifaceted task where the algorithm must coordinate severaldesired properties such as safety, comfort, and progress, the evaluation metric must also balance",
  "NAV IM": ": NAVSIM. Traditional metrics such as the average displacement error (ADE) overlook themulti-modality of driving. They penalize trajectories that deviate from a recorded human driving log,even if such a trajectory is safe. Our benchmark evaluates trajectory outputs of sensor-based drivingpolicies with simulation-based metrics, considering collisions and map compliance. potentially conflicting objectives. However, as shown in , existing metrics such as the averagedisplacement error (ADE) between a predicted and recorded human trajectory often misrepresentthe relative accuracy of trajectories. (3) Since driving involves interactions among multiple agents,evaluation must ideally be interactive, e.g., in simulation. Unfortunately, existing simulators withsynthetic sensor data exhibit a significant domain gap to real-world driving. (4) Besides, the lack ofa standardized evaluation setup has led to subtle inconsistencies between metrics in existing work,leading to unfair comparisons and inaccurate conclusions . Collectively, these problemshinder progress in the development of AVs, emphasizing the need for more principled benchmarks. In this work, we take steps towards alleviating these issues. First, we propose a strategy for samplinginteresting driving scenarios and apply it to the largest publicly-available driving dataset . Weobtain, for the first time, over 100k challenging real-world driving scenarios for training and evaluatingsensor-based driving policies. We show that in these scenarios, blind driving policies fail to competewith more principled sensor-based policies. Second, we draw inspiration from the literature of rule-based planning for AVs to identify a set of diverse, efficient, and principled metricsthat cover multiple facets of the autonomous driving task. Third, we circumvent the need forinaccurate sensor simulation with domain gaps by simplifying our simulation to a non-reactive one.Given an observed real-world sensor input, the agent under test commits to a set of actions for aspecific time horizon. Further, these actions are assumed to not affect the future behavior of otheragents in the scene. Under this setting, it is possible to simulate the expected motion of all agentsover this time horizon in a simplified birds-eye-view (BEV) abstraction of the scene, and incorporatemetrics that involve interactions, as we observe in . Empirically, we demonstrate that ourselected metrics are well-correlated to the outcomes of closed-loop simulations. Finally, we establishan official evaluation server on the open-source HuggingFace platform, which is free, has a lowmaintenance overhead, and enables future scaling to more challenging datasets and metrics. We combine these ideas to propose NAVSIM, a comprehensive tool for AV data curation, simulation,and benchmarking. We instantiate standardized training and evaluation splits for NAVSIM withthe OpenScene dataset , though our framework can be extended to other datasets. With thesesplits, we present a detailed analysis of popular end-to-end driving models previously benchmarkedeither exclusively on CARLA or nuScenes , providing the first direct comparison betweenthese families of approaches in an independent evaluation setting. Interestingly, we find that theperformances of the best methods developed in both settings are similar, despite a vast difference incomputational requirements for their training. Finally, we review the insights gained through the 2024NAVSIM challenge1, hosted in conjunction with the CVPR 2024 Workshop on Foundation Modelsfor Autonomous Systems. For the challenge, 143 teams from 13 countries developed diverse methodsthat competed on the proposed benchmark. The top methods ranged from multi-billion parametervision language models to more efficient and recently overlooked approaches basedon trajectory sampling and scoring , demonstrating the remarkable ability of the broadercommunity to advance AV research when provided with the right tools.",
  "#end_to_end_driving_at_scale": "Contributions. (1) We build NAVSIM, a framework for non-reactive AV simulation, with standard-ized protocols for training and testing, data curation tools ensuring broad accessibility, and an officialpublic evaluation server used for the inaugural NAVSIM challenge. (2) We develop configurablesimulation-based metrics that are well-suited for evaluating sensor-based motion planning. (3) Wereimplement a collection of end-to-end approaches for NAVSIM including TransFuser, UniAD, andPARA-Drive, showcasing the surprising potential of simple models in our challenging scenarios.",
  "Related Work": "End-to-End Driving. End-to-end driving streamlines the entire stack from perception to planninginto a single optimizable network. This eliminates the need for manually designing intermediaterepresentations. Following pioneering work , a diverse landscape of end-to-end modelshas emerged. For instance, an extensive body of end-to-end approaches focuses on closed-loopsimulators, utilizing single-frame cameras, LiDAR point clouds, or a combination of both for expertimitation . More recently, developing end-to-end modelson open-loop benchmarks has gained traction . Our work introduces a newevaluation scheme with which we compare end-to-end models from both communities. Closed-Loop Benchmarking with Simulation. Driving simulators allow us to evaluate autonomoussystems in a closed-loop manner and collect downstream driving statistics, including collision rates,traffic-rule compliance, or comfort. A broad body of research conducts evaluations in simulators,such as CARLA or Metadrive with sensor simulation, or nuPlan and Waymax for data-driven simulation. Unfortunately, ensuring realism when simulating traffic behavior orsensor data remains a challenging task. To simulate camera or LiDAR sensors, most establishedsimulators rely on graphics-based rendering methods, leading to an inherent domain gap in termsof visual fidelity and sensor characteristics. Data-driven simulators for motion planning incorporatetraffic recordings but do not support image or LiDAR-based methods . Data-drivensensor simulation leverages and adapts real-world sensor data to create new simulations where thevehicle may move differently, but the rendering quality of existing tools is subpar . Further,while promising image or LiDAR synthesis approaches exist, efficiently simulating sensorsentirely from data remains an open problem. In this work, we provide an approach for the evaluationof real sensor data with simulation-based metrics by making a simplifying assumption that the agentand environment do not influence each other over a short simulation horizon. Despite this strongassumption, when benchmarking on real data, NAVSIM better reflects planning performance thanestablished evaluation protocols, as demonstrated through our systematic experimental analysis. Open-Loop Benchmarking with Displacement Errors. Open-loop evaluation protocols commonlymeasure displacement errors between trajectories of a recorded expert (i.e., of a human driver)and a motion planner. However, several issues concerning evaluation with displacement errorshave surfaced recently, particularly on the nuScenes dataset . Given that nuScenes does notprovide standardized planning metrics, prior work relied on independent implementations, whichled to inconsistencies when reporting or comparing results . Next, most planning models innuScenes receive the human trajectory endpoint as a discrete direction command ,thereby leaking ground-truth information into inputs. Moreover, about 75% of the scenarios innuScenes involve trivial straight driving , leading to simple solutions when extrapolating theego-motion. For instance, AD-MLP demonstrates that an MLP on the kinematic ego status (ignoringperception completely) can achieve state-of-the-art displacement errors . Such blind agents areundeniably dangerous, which highlights a broader concern: displacement metrics are not correlated toclosed-loop driving . In this work, we address prevalent issues of nuScenes and proposea standardized driving benchmark with challenging scenarios and an official evaluation server. Wederive a navigation goal from the lane graph instead of the human trajectory to prevent label leakage,and propose principled simulation-based metrics as an alternative to displacement errors.",
  "NAVSIM: Non-Reactive Autonomous Vehicle Simulation": "NAVSIM combines the ease of use of open-loop benchmarks such as nuScenes with metrics basedon closed-loop simulators such as nuPlan . In the following, we give a detailed introduction tothe task and metrics that driving agents are challenged with in NAVSIM. Subsequently, we propose afiltering method to obtain standardized train and test splits covering challenging scenes. Task description. Driving agents in NAVSIM must plan a trajectory, defined as a sequence of futureposes, over a horizon of h seconds. Their input contains streams of past frames from onboard sensors,such as cameras, LiDAR, as well as the vehicles current speed, acceleration, and navigation goal,jointly termed the ego status. For compatibility with prior work , we provide thenavigation goal as a one-hot vector with three categories: left, straight, or right. Non-Reactive Simulation. Traditional closed-loop benchmarks normally infer planners at highfrequencies, e.g., 10Hz . However, this requires efficient simulation of all input modalities forthe driving agent, including high-dimensional sensor streams in the case of sensor-based approaches.To sidestep this, the core idea of NAVSIM is to evaluate driving agents using a non-reactive simulation.This means driving agents are only queried in the initial frame of each scene. Afterwards, the plannedtrajectory is kept fixed for the entire trajectory duration. Over this short horizon, no environmentalfeedback is provided to the driving agent, and the NAVSIM evaluation is purely based on the initialreal-world sensor sample. This makes the agents task more challenging, limiting simulations toshort horizons. We select a horizon of h = 4 seconds, which has been shown in prior work to beadequate for closed-loop planning . Despite this limitation, non-reactive simulation offers a keyadvantage: unlike traditional open-loop benchmarks, which mainly compare the planned trajectory tothe human drivers trajectory in a similar setting, it enables the use of simulation outcomes to computemetrics reflecting safety, comfort, and progress. An LQR controller is applied at each simulationiteration to calculate steering and acceleration values, and a kinematic bicycle model propagatesthe ego vehicle. We execute this pipeline at 10Hz over the 4s trajectory horizon. In Sec. 4.1, weshow that despite our simplifying assumption, our evaluation results in a much better alignment withclosed-loop metrics than traditional open-loop metrics achieve. PDM Score. NAVSIM scores driving agents in two steps. First, subscores in range are computedafter simulation. Second, these subscores are aggregated into the PDM Score (PDMS) . It isnamed after the Predictive Driver Model (PDM) , a state-of-the-art rule-based planner which usesthis scoring function to evaluate trajectory proposals during closed-loop simulation in nuPlan. Themetric is also an efficient reimplementation of the nuPlan closed-loop score metric . In NAVSIM,the PDMS can be adapted by adding or removing subscores, changing aggregation parameters, ormaking subscores more challenging, e.g., by adapting their internal thresholds. It is calculated perframe and averaged across frames. In this work, we use the following aggregation of subscores:",
  ".(1)": "Subscores are categorized by their importance as penalties or terms in a weighted average. A penaltypunishes inadmissible behavior such as collisions with a factor < 1. The weighted average aggregatessubscores for other objectives such as progress and comfort. In the following, we briefly describeeach subscore. More details can be found in the supplementary material. Penalties. Avoiding collisions and staying on the road is imperative for motion planning as itensures traffic rule compliance and the safety of pedestrians and road users. Thus, failing to drivewith no collisions (NC) with road users (vehicles, pedestrians, and bicycles) or infractions withregard to drivable area compliance (DAC) result in hard penalties of scoreNC = 0 or scoreDAC = 0respectively. This results in a PDMS of 0 for the current scene. We ignore certain collisions that arenot considered \"at-fault\" in the non-reactive environment, e.g. when the ego vehicle is static. Forcollisions with static objects, we apply a softer penalty of scoreNC = 0.5. Weighted Average. The weighted average accounts for ego progress (EP), time-to-collision (TTC),and comfort (C). The ego progress subscore scoreEP represents the agent progress along the routecenter as a ratio to an approximated safe upper bound from the PDM-Closed planner . PDM-Closed obtains a possible progress value without collisions or off-road driving with a search-basedstrategy based on trajectory proposals. The final ratio is clipped to while discarding low ornegative progress scores if the upper bound is below 5 meters. Next, the TTC subscore ensuresthat driving agents respect the safety margins to other vehicles. Defaulting to a value of 1, thissubscore is set to 0 if for any simulation step within the 4s horizon, the ego-vehicles time-to-collison, when projected forward with a constant velocity and heading, is less than a certain threshold.Finally, the comfort subscore is obtained by comparing the acceleration and jerk of the trajectory topredetermined thresholds. Following the cost weights used by the PDM-Closed planner and the 2023",
  "(c) Lateral Frequency": ": Filtering. (a) We consider challenging scenes where maintaining a constant velocity andheading fails compared to the human driver. (b) Our filtering primarily removes scenes with static orfast longitudinal movement and (c) leads to more diversity in lateral movement (log-scale). nuPlan Challenge, we set the coefficients of the weighted average as weightEP = 5, weightTTC = 5,and weightC = 2. We find this selection reasonable and robust to changes. For example, the top 3ranks of the NAVSIM challenge remain identical when assigning an equal weight to the subscores.",
  "Generating Standardized and Challenging Train and Test Splits": "Dataset. The NAVSIM framework is agnostic to the choice of driving dataset. We choose Open-Scene , a redistribution of nuPlan , the largest annotated public driving dataset. OpenSceneincludes 120 hours of driving at a reduced frequency of 2Hz typically considered by end-to-endplanning algorithms, resulting in a 90% reduction of data storage requirements compared to nuPlanfrom over 20 TB to 2 TB. Our agent input, based on OpenScene, comprises eight cameras, each witha resolution of 1920 1080 pixels, and a merged LiDAR point cloud from five sensors. The inputincludes the current time-step and optionally 3 past frames, totaling 1.5s at 2Hz. In principle, anydriving dataset that provides annotated HD maps, object bounding boxes, and sensor data can beconverted into this format and thus be used with NAVSIM. Filtering for challenging scenes. A majority of human driving data involves trivial situations suchas being stationary or straight driving at a near constant speed. These can be solved efficiently bysimple heuristics, e.g., as depicted in (a), the baseline of maintaining a constant velocityand heading achieves a PDMS of 79% on the OpenScene dataset, where human-level performancecorresponds to 91%. In NAVSIM, we propose the use of a filtered dataset to remove frames with(1) near-trivial solutions and (2) significant annotation errors. We remove highly simplistic scenesby detecting if the previously mentioned constant velocity agent exceeds a PDMS of 0.8. Similarly,we remove scenes in which the human trajectory results in a PDMS of less than 0.8. This ensuresthat an acceptable solution exists to these difficult scenarios and filters out noisy annotations such asinaccurate bounding boxes. These thresholds can be adjusted based on the desired filtered datasetsize. The resulting scenarios are challenging, which is underlined by the score of the constant velocityagent dropping to 22%, whereas the human expert achieves a score of 95%. The higher ratio ofnon-trivial scenarios, such as turning, also results in endpoints being less distant longitudinally whennonzero, and more evenly distributed laterally, as seen in (b-c). We employ this filtering strategyto provide standardized splits for training and testing, called navtrain and navtest, with 103kand 12k samples respectively. This curated data serves as a benchmark accessible as a standalonedownload option with a moderate storage demand given its large scale and diversity (450 GB).",
  "(e)": ": Closed-Loop Alignment. (a) For each planner, we show open-loop metrics (OLS, PDMS)together with the corresponding closed-loop score (CLS). The trendlines depicting correlations are fitlinearly to all (learned and rule-based) planners. Moreover, we analyze different (b) CLS durations d,(c) planning frequencies f, (d) PDMS horizons h, and (e) closed-loop background agent behaviors.",
  "Alignment Between Open-Loop and Closed-Loop Evaluation": "Open-loop metrics should ideally be aligned with closed-loop metrics in their evaluation of differentdriving algorithms. In this section, we benchmark a large set of planners to analyze the alignment ofclosed-loop metrics with traditional distance-based open-loop metrics and the proposed PDMS. Benchmark. Studying the relation of closed-loop and open-loop metrics necessitates access to afully reactive simulator. To stay compatible with the dataset, we use the nuPlan simulator , whichenables simulation for privileged planners with access to ground-truth perception and HD map inputs.Similar to PDMS, nuPlan combines weighted averages and multiplied penalties in two official scores:the open-loop score (OLS) aggregates displacement and heading errors with a multiplied miss-rate,and the closed-loop score (CLS) implements similar metrics from . Including PDMS, allmetrics are in with higher scores indicating better performance. Due to the heavy computational requirements of closed-loop simulation, we evaluate on the navminisplit. This is a new split we create for rapid testing, with 396 scenarios in total that are independent ofboth navtrain and navtest but filtered using the same strategy (.1) and hence similarlydistributed. We note that nuPlan offers two kinds of background agents: reactive agents along lanecenters based on the Intelligent Driver Model (IDM) , and non-reactive agents replayed from thedataset, which we employ unless otherwise stated. While reactive simulations of longer or dynamiclengths are generally desirable, e.g. to evaluate long-term decisions, enabling this requires dedicatedsolutions to long-horizon simulation that are not currently available in nuPlan . Therefore, wedefault to a fixed closed-loop simulation duration of d = 15s, and a planning frequency of f = 10Hz,which are the standard closed-loop simulation settings in nuPlan . Motion Planners. Open-loop metrics favor learned planners while rule-based approaches performwell in closed-loop evaluation in nuPlan . We use a combination of both planner types in thisexperiment to cover different performance levels. In total, we include 37 rule-based planners with 2constant velocity and 8 constant acceleration models, 15 IDM planners , and 12 PDM-Closedvariants which differ in hyperparameters for trajectory generation. For learned planning, weevaluate Urban Driver models of 2 model sizes and 2 training lengths, and PlanCNN modelswith 15 input combinations of the BEV raster, ego status, centerline, and navigation goal. We trainall models on {25%, 50%, 100%} of navtrain and an equally sized uniformly sampled subset ofOpenScene, giving 114 learned planners. See the supplementary material for additional details. Results. The alignment between metrics is presented in (a-e). Compared to OLS, weconsistently observe better closed-loop correlation for PDMS, in terms of Spearmans (rank) andPearsons (linear) correlation coefficients. As shown in (a), PDMS can capture the closed-loop",
  "Human10010010099.987.594.8": ": Navtest Benchmark. We show the no at-fault collision (NC), drivable area compliance(DAC), time-to-collision (TTC), comfort (Comf.), and ego progress (EP) subscores, and the PDMScore (PDMS), as percentages. Relying on the ego status is insufficient for competitive results. Whilesensor agents improve, the gap to human performance highlights our benchmarks challenges. properties of both learned and rule-based planners, whereas distance-based open-loop metrics showa clear misalignment. Decreasing the CLS duration in (b) from d = 15s to d = 4s further raisesthe correlation of PDMS and OLS, as the simulation horizon more closely matches the open-loopcounterparts. Interestingly, we observe a higher correlation of open-loop metrics in (c) when reducingthe planning frequency to 2Hz. We expect a lower planning frequency to mitigate cumulative errorsand enhance the controllers stability in simulation, leading to more precise trajectory execution.Moreover, we observe an increase in correlation for longer PDMS horizons in (d), ranging fromh = 2s to h = 8s. While predicting the future motion over 8s is challenging in uncertain scenarios,our results indicate the value of long horizons when evaluating motion planners. Lastly, replacing thenon-reactive background agents with reactive IDM vehicles during closed-loop simulation in (e) haslittle effect on the correlation, possibly due to the similar difficulty of both tasks . The imbalanced distribution of different types of planners in our study may introduce biases intothe overall correlations presented in . To address this, we visualize the individual correlationsof each planner type in . The correlation values vary depending on metric range and varianceof each planner type. Nevertheless, when examining each type individually, the PDMS is bettercorrelated to the CLS than the OLS, and is always positively correlated.",
  "In this section, we benchmark a collection of end-to-end architectures, which previously achievedstate-of-the-art performance on existing open- or closed-loop benchmarks": "Methods. As a lower bound, we consider the (1) Constant Velocity baseline detailed in .1.We include an (2) Ego Status MLP as a second \"blind\" agent, which leverages an MLP for trajectoryprediction given only the ego velocity, acceleration and navigation goal. As an established architectureon CARLA, we evaluate our reimplementation of (3) TransFuser , which uses three croppedand downscaled forward-facing cameras, concatenated into a 1024 256 image, and a rasterizedBEV LiDAR input for predicting waypoints. It performs 3D object detection and BEV semanticsegmentation as auxiliary tasks. We then consider (4) Latent TransFuser (LTF) , which shares",
  "E1SupervisionNo BEV segmentation97.490.592.210077.181.6E2No 3D detection97.892.792.910079.284.0": ": TransFuser Ablations. The default configuration, which obtains the best results, uses thenavigation goal, velocity, and acceleration as ego status inputs. Its camera FOV is around 140 andLiDAR range is 32m to the front (F), back (B), left (L), and right (R). It uses both auxiliary tasks. the same architecture as TransFuser but replaces the LiDAR input with a learned embedding, hencerequiring only camera inputs. Moreover, we provide two state-of-the-art end-to-end architecturesfor open-loop trajectory prediction on nuScenes. (5) UniAD incorporates a wide range of tasks,such as mapping, tracking, motion, and occupancy prediction in a semi-sequential architecture, whichprocesses feature representations through several transformer decoders culminating in a trajectoryplanning module. (6) PARA-Drive uses the same auxiliary tasks, but parallelizes the networkarchitecture, such that the auxiliary task heads are trained in parallel with a shared encoder. BothUniAD and PARA-Drive use a BEVFormer backbone , which encodes the eight surround-view 1920 1080 camera images over four temporal frames into a BEV feature representation.Implementation details for all methods are provided in the supplementary material. Results. We show our results on navtest in . The Constant Velocity model is a lowerbound, as the agent is used to identify trivial driving scenes excluded from the benchmark. The EgoStatus MLP achieves a PDMS of 65.6, showing the value of the acceleration and navigation goal foravoiding collisions and driving off-road. However, we observe a clear gap between agents relyingsolely on the ego status and those considering sensor data, in contrast to results on nuScenes . Allsensor agents achieve a PDMS of over 83, where TransFuser and PARA-Drive marginally performbest, with a PDMS of 84.0. Surprisingly, the camera-only LTF achieves similar results (83.8). UniADreaches a PDMS of 83.4, which, together with PARA-Drive, do not surpass the performance ofTransFuser and LTF, despite the need for more demanding training, e.g., 80 GPUs for 3 days to trainPARA-Drive versus 1 GPU for 1 day for TransFuser on the navtrain split. Due to the definitionof at-fault collisions, which discard certain rear-collisions into the ego vehicle, we suspect thatsurround-view cameras used by UniAD and PARA-Drive, and LiDAR input of TransFuser, are lessimportant than the wide-angle front camera which is the only input of LTF. The 10 PDMS discrepancyto the human operator demonstrates that navtest poses challenges even to well-studied end-to-endarchitectures. Specifically, the drivable area compliance (DAC) and ego progress (EP) subscoresremain the most challenging. Notably, EP cannot be solved purely by human imitation, given that themaximum progress estimate used for normalization is based on a privileged rule-based motion planner.Interestingly, all agents achieve near-perfect comfort scores, indicating that smooth acceleration andjerk profiles are learned naturally from human imitation. Analyzing TransFuser. In , we compare several training settings for TransFuser. Forthe three training seeds in configs A1-A3, we observe a standard deviation of 0.56 in PDMS,which is relatively small compared to variance among training seeds for closed-loop simulations inCARLA . Further, unlike CARLA, NAVSIM is deterministic, and we obtain identical scoreswhen repeating evaluations of a deterministic driving agent. Discarding velocity and acceleration(B1) lowers PDMS by 1.5 2.6, whereas only removing the acceleration (B2) lowers the score by1.0 2.1. We conclude that while TransFuser benefits from the ego status, it is not purely relyingon the kinematic state for planning. Next, only considering the front camera (C1) with a 60 FOVleads to a small drop in almost all subscores, compared to our default setting of three cropped and concatenated images with a FOV of 140. However, expanding the FOV with additional camerasdoes not result in substantially improved scores. Interestingly, restricting the LiDAR range to 16min all directions (D1), results in a score of 79, which is lower than dropping LiDAR altogether (seeLTF in ). Expanding the LiDAR range to 64m in the forward direction (D2) or all directions(D3) does not provide significant improvements. We suspect that changes in the LiDAR range overlysimplify or complicate the auxiliary 3D object detection and BEV semantic segmentation tasks, whichoperate in the LiDAR coordinate frame, hindering effective imitation learning. We check the impactof the auxiliary tasks by excluding them, where performance drops without BEV Segmentation (E1).",
  ": NAVSIM Challenge": "CVPR 2024 NAVSIM Challenge. We organized the inaugu-ral NAVSIM challenge which ran from March - May 2024.To ensure integrity, we used a private dataset and only gaveparticipants access to sensor inputs, withholding all annota-tions. Competitors could submit their agents trajectories toour leaderboard, where they were simulated and scored to ob-tain the PDMS. We received 463 submissions from 143 teams,of which 78 submissions were made publicly visible. We sum-marize their scores in , relative to the constant velocityand TransFuser baselines from . The winning entryextended TransFuser and learned to predict proxy subscoresfor trajectory samples , with a sampling strategy inspiredby VADv2 . These predicted subscores were weightedalongside a human imitation score to select the output plan. While the idea of sampling and scoringtrajectories is well-known , it has recently been overlooked in favor of approacheswhich predict a single trajectory. This result prompts a reassessment of such methods. The team thatplaced second employed a vision language model (VLM) for driving, which is rapidly emerging asa sub-field in the AV literature . Several submissions attempted to reimplement orextend prior work on nuScenes such as UniAD and VAD , but were unable to outperform theTransFuser baseline by the challenge submission deadline, given the significant engineering challengeand compute requirements. The diversity of the solutions on the leaderboard shows the potentialof NAVSIM as a framework for pushing the frontiers of autonomous driving research. We aim tohold future competitions with more challenging data and metrics. Detailed competition results andstatistics are provided in the supplementary material.",
  ": Leaderboard 1.1": "NAVSIM 1.1 Leaderboard. Due to the lasting interest af-ter the challenge, we re-opened a public evaluation serverusing navtest as the evaluation split. The leaderboard en-courages multi-seed submissions and includes reproducibilityrequirements for openly releasing code and model weights.We populated the leaderboard with 3 training seeds of ourlearned baselines, as shown in . For reference, we alsoinclude a single seed of the 2024 challenge winner andconstant velocity baseline. Further information is provided inthe supplementary material and leaderboard webpage2.",
  "Discussion": "We present NAVSIM, a framework for non-reactive AV simulation. We address shortcomings ofexisting driving benchmarks and propose standardized but configurable simulation-based metrics forbenchmarking driving policies. For accessibility, we provide challenging scenario splits and simpledata curation methods. We show that our evaluation protocol is better aligned to closed-loop driving,benchmark an established set of end-to-end planning baselines from CARLA and nuScenes, andpresent the results of our inaugural competition. We hope that NAVSIM can serve as an accessibletoolkit for AV researchers that bridges the gap between simulated and real-world driving. Need for Reactive Simulation. While we show improvements over displacement errors, severalaspects of driving remain unaddressed by evaluation in NAVSIM. A high PDMS does not always implya high CLS, since our framework does not consider reactiveness or the compounding accumulation of errors in closed-loop simulation. Moreover, as in CLS, rear-end collisions into the ego vehicleare currently not classified as \"at-fault\", resulting in little importance given to the scene behind thevehicle in NAVSIM. In the future, data-driven sensor or traffic simulation could alleviate these issues,once such methods mature and become computationally tractable. Given these limitations of thecurrent framework, we strongly encourage the use of graphics-based closed-loop simulators, such asCARLA , as complementary benchmarks to NAVSIM when developing planning algorithms. Simplicity of Metrics. As a starting point, NAVSIM offers both interpretable open-loop subscoresand a scalarizing function, which lets us provide a final score and ranking for participants in thechallenge. In the future, multi-objective evaluation and other aggregation functions might be required.Moreover, closed-loop metrics also face problems, i.e., PDMS inherits several weaknesses of nuPlansCLS. Both scores do not regard certain traffic rules (e.g., stop-sign or traffic light compliance) orconcepts such as transit and fuel efficiency. In the future, we aim to improve the subscore definitions(e.g. the at-fault collision logic) and add more subscores during aggregation. Call for Datasets. Certain limitations of the nuPlan dataset persist in NAVSIM, such as missingclasses in the label space, minor errors in camera parameters, or noise in vehicle poses and 3Dannotations. Our analysis might favor methods that are robust to such inconsistencies. In addition,the lack of road elevation data in our representation presents a challenge for integrating scenariosbased on 3D map annotations. We aim to support more datasets in the future, and advocate for moreopen dataset releases by the community for accelerating progress in autonomous driving. This work was supported by the ERC Starting Grant LEGO-3D (850533), the DFG EXC number2064/1 - project number 390727645, the German Federal Ministry of Education and Research:Tbingen AI Center, FKZ: 01IS18039A and the German Federal Ministry for Economic Affairsand Climate Action within the project NXT GEN AI METHODS. We thank the International MaxPlanck Research School for Intelligent Systems (IMPRS-IS) for supporting Daniel Dauner andKashyap Chitta. We also thank HuggingFace for hosting our evaluation servers, the team membersof OpenDriveLab for their organizational support, as well as Napat Karnchanachari and his teamfrom Motional for open-sourcing their dataset and providing us the private test split used in the 2024NAVSIM Challenge. Alexander Amini, Igor Gilitschenski, Jacob Phillips, Julia Moseyko, Rohan Banerjee, SertacKaraman, and Daniela Rus. Learning robust control policies for end-to-end autonomous drivingfrom data-driven simulation. IEEE Robotics and Automation Letters (RA-L), 2020. Alexander Amini, Tsun-Hsuan Wang, Igor Gilitschenski, Wilko Schwarting, Zhijian Liu,Song Han, Sertac Karaman, and Daniela Rus. Vista 2.0: An open, data-driven simulator formultimodal sensing and policy learning for autonomous vehicles. In Proc. IEEE InternationalConf. on Robotics and Automation (ICRA), 2022.",
  "Mayank Bansal, Alex Krizhevsky, and Abhijit S. Ogale. Chauffeurnet: Learning to drive byimitating the best and synthesizing the worst. In Proc. Robotics: Science and Systems (RSS),2019": "Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, PrasoonGoyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao,and Karol Zieba. End to end learning for self-driving cars. arXiv.org, 1604.07316, 2016. Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, AnushKrishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset forautonomous driving. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),2020.",
  "Kashyap Chitta, Aditya Prakash, and Andreas Geiger. Neat: Neural attention fields for end-to-end autonomous driving. In Proc. of the IEEE International Conf. on Computer Vision (ICCV),2021": "Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu, Katrin Renz, and Andreas Geiger.TransFuser: Imitation with transformer-based sensor fusion for autonomous driving. IEEETrans. on Pattern Analysis and Machine Intelligence (PAMI), 2023. Kashyap Chitta, Daniel Dauner, and Andreas Geiger. Sledge: Synthesizing driving environmentswith generative models and rule-based traffic. In Proc. of the European Conf. on ComputerVision (ECCV), 2024.",
  "Haoyang Fan, Fan Zhu, Changchun Liu, Liangliang Zhang, Li Zhuang, Dong Li, WeichengZhu, Jiangtao Hu, Hongye Li, and Qi Kong. Baidu apollo EM motion planner. arXiv.org,1807.08048, 2018": "Cole Gulino, Justin Fu, Wenjie Luo, George Tucker, Eli Bronstein, Yiren Lu, Jean Harb, XinleiPan, Yan Wang, Xiangyu Chen, John D. Co-Reyes, Rishabh Agarwal, Rebecca Roelofs, Yao Lu,Nico Montali, Paul Mougin, Zoey Yang, Brandyn White, Aleksandra Faust, Rowan McAllister,Dragomir Anguelov, and Benjamin Sapp. Waymax: An accelerated, data-driven simulatorfor large-scale autonomous driving research. In Advances in Neural Information ProcessingSystems (NeurIPS), 2023. Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi Yan, and Dacheng Tao. ST-P3:End-to-end vision-based autonomous driving via spatial-temporal feature learning. In Proc. ofthe European Conf. on Computer Vision (ECCV), 2022. Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, SenyaoDu, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, andHongyang Li. Planning-oriented autonomous driving. In Proc. IEEE Conf. on Computer Visionand Pattern Recognition (CVPR), 2023.",
  "Bernhard Jaeger, Kashyap Chitta, and Andreas Geiger. Hidden biases of end-to-end drivingmodels. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2023": "Joel Janai, Fatma Gney, Aseem Behl, and Andreas Geiger. Computer Vision for AutonomousVehicles: Problems, Datasets and State of the Art, volume 12. Foundations and Trends inComputer Graphics and Vision, 2020. Xiaosong Jia, Penghao Wu, Li Chen, Jiangwei Xie, Conghui He, Junchi Yan, and Hongyang Li.Think twice before driving: Towards scalable decoders for end-to-end autonomous driving. InProc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2023. Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang,Wenyu Liu, Chang Huang, and Xinggang Wang. VAD: Vectorized scene representation forefficient autonomous driving. In Proc. of the IEEE International Conf. on Computer Vision(ICCV), 2023. Napat Karnchanachari, Dimitris Geromichalos, Kok Seang Tan, Nanxiang Li, ChristopherEriksen, Shakiba Yaghoubi, Noushin Mehdipour, Gianmarco Bernasconi, Whye Kit Fong,Yiluan Guo, and Holger Caesar. Towards learning-based planning: The nuPlan benchmark forreal-world autonomous driving. In Proc. IEEE International Conf. on Robotics and Automation(ICRA), 2024.",
  "Norman Lehtomaki, Nils Sandell, and Michael Athans. Robustness results in linear-quadraticgaussian based multivariable control designs. IEEE Trans. on Automatic Control (TAC), 1981": "Quanyi Li, Zhenghao Peng, Lan Feng, Qihang Zhang, Zhenghai Xue, and Bolei Zhou.Metadrive: Composing diverse driving scenarios for generalizable reinforcement learning.IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI), 45(3):34613475, 2022. Zhenxin Li, Kailin Li, Shihao Wang, Shiyi Lan, Zhiding Yu, Yishen Ji, Zhiqi Li, Ziyue Zhu, JanKautz, Zuxuan Wu, Yu-Gang Jiang, and Jose M. Alvarez. Hydra-mdp: End-to-end multimodalplanning with multi-target hydra-distillation. arXiv.org, 2024. Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, andJifeng Dai. BEVFormer: Learning birds-eye-view representation from multi-camera imagesvia spatiotemporal transformers. In Proc. of the European Conf. on Computer Vision (ECCV),2022. Zhiqi Li, Zhiding Yu, Shiyi Lan, Jiahan Li, Jan Kautz, Tong Lu, and Jose M. Alvarez. Is egostatus all you need for open-loop end-to-end autonomous driving? In Proc. IEEE Conf. onComputer Vision and Pattern Recognition (CVPR), 2024.",
  "Rajesh Rajamani. Vehicle dynamics and control. Springer Science & Business Media, 2011": "Katrin Renz, Kashyap Chitta, Otniel-Bogdan Mercea, Sophia Koepke, Zeynep Akata, andAndreas Geiger. Plant: Explainable planning transformers via object-level representations. InProc. Conf. on Robot Learning (CoRL), 2022. Abbas Sadat, Mengye Ren, Andrei Pokrovsky, Yen-Chen Lin, Ersin Yumer, and Raquel Urtasun.Jointly learnable behavior and trajectory planning for self-driving vehicles. In Proc. IEEEInternational Conf. on Intelligent Robots and Systems (IROS), 2019. Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu, Pranaab Dhawan, and Raquel Urtasun.Perceive, predict, and plan: Safe motion planning through interpretable semantic representations.In Proc. of the European Conf. on Computer Vision (ECCV), 2020.",
  "Hao Shao, Letian Wang, RuoBing Chen, Hongsheng Li, and Yu Liu. Safety-enhanced au-tonomous driving using interpretable sensor fusion transformer. In Proc. Conf. on RobotLearning (CoRL), 2022": "Hao Shao, Letian Wang, Ruobing Chen, Steven L. Waslander, Hongsheng Li, and Yu Liu.Reasonnet: End-to-end driving with temporal and global reasoning. In Proc. IEEE Conf. onComputer Vision and Pattern Recognition (CVPR), 2023. Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Ping Luo,Andreas Geiger, and Hongyang Li. DriveLM: Driving with graph visual question answering.arXiv.org, 2312.14150, 2023. Sebastian Thrun, Michael Montemerlo, Hendrik Dahlkamp, David Stavens, Andrei Aron, JamesDiebel, Philip Fong, John Gale, Morgan Halpenny, Gabriel Hoffmann, Kenny Lau, Celia M.Oakley, Mark Palatucci, Vaughan R. Pratt, Pascal Stang, Sven Strohband, Cedric Dupont,Lars-Erik Jendrossek, Christian Koelen, Charles Markey, Carlo Rummel, Joe van Niekerk, EricJensen, Philippe Alessandrini, Gary R. Bradski, Bob Davies, Scott Ettinger, Adrian Kaehler,Ara V. Nefian, and Pamela Mahoney. Stanley: The robot that won the DARPA grand challenge.Journal of Field Robotics (JFR), 23(9):661692, 2006. Adam Tonderski, Carl Lindstrm, Georg Hess, William Ljungbergh, Lennart Svensson, andChristoffer Petersson. NeuRAD: Neural rendering for autonomous driving. In Proc. IEEE Conf.on Computer Vision and Pattern Recognition (CVPR), 2024.",
  "Martin Treiber, Ansgar Hennecke, and Dirk Helbing. Congested traffic states in empiricalobservations and microscopic simulations. Physical review E, 2000": "Tsun-Hsuan Wang, Alexander Amini, Wilko Schwarting, Igor Gilitschenski, Sertac Karaman,and Daniela Rus. Learning interactive driving policies via data-driven simulation. In Proc.IEEE International Conf. on Robotics and Automation (ICRA), 2022. Xinshuo Weng, Boris Ivanovic, Yan Wang, Yue Wang, and Marco Pavone. Para-drive: Paral-lelized architecture for real-time autonomous driving. In Proc. IEEE Conf. on Computer Visionand Pattern Recognition (CVPR), 2024. Moritz Werling, Julius Ziegler, Sren Kammel, and Sebastian Thrun. Optimal trajectorygeneration for dynamic street scenarios in a frenet frame. In Proc. IEEE International Conf. onRobotics and Automation (ICRA), 2010. Penghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang Li, and Yu Qiao. Trajectory-guided control prediction for end-to-end autonomous driving: A simple yet strong baseline. InAdvances in Neural Information Processing Systems (NeurIPS), 2022.",
  "Zhenjie Yang, Xiaosong Jia, Hongyang Li, and Junchi Yan. LLM4Drive: A survey of largelanguage models for autonomous driving. arXiv.org, 2311.01043, 2023": "Tengju Ye, Wei Jing, Chunyong Hu, Shikun Huang, Lingping Gao, Fangzhen Li, Jingke Wang,Ke Guo, Wencong Xiao, Weibo Mao, et al. Fusionad: Multi-modality fusion for prediction andplanning tasks of autonomous driving. arXiv.org, 2023. Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin Yang, Sergio Casas, and RaquelUrtasun. End-to-end interpretable neural motion planner. In Proc. IEEE Conf. on ComputerVision and Pattern Recognition (CVPR), 2019. Jiang-Tian Zhai, Ze Feng, Jinhao Du, Yongqiang Mao, Jiang-Jiang Liu, Zichang Tan, YifuZhang, Xiaoqing Ye, and Jingdong Wang. Rethinking the open-loop evaluation of end-to-endautonomous driving in nuscenes. arXiv.org, 2305.10430, 2023."
}