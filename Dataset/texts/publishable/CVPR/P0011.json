{
  "University of Electronic Science and Technology of China, Chengdu, China{luhaisheng, fuyujie}@std.uestc.edu.cn, {fan.zhang,lezhang}@uestc.edu.cn": "Abstract. Medical image segmentation is a critical component of clin-ical practice, and the state-of-the-art MedSAM model has significantlyadvanced this field. Nevertheless, critiques highlight that MedSAM de-mands substantial computational resources during inference. To addressthis issue, the CVPR 2024 MedSAM on Laptop Challenge was estab-lished to find an optimal balance between accuracy and processing speed.In this paper, we introduce a quantization-aware training pipeline de-signed to efficiently quantize the Segment Anything Model for medicalimages and deploy it using the OpenVINO inference engine. This pipelineoptimizes both training time and disk storage. Our experimental resultsconfirm that this approach considerably enhances processing speed overthe baseline, while still achieving an acceptable accuracy level. The train-ing script, inference script, and quantized model are publicly accessibleat",
  "Introduction": "Drawing inspiration from the remarkable achievements of foundation models innatural language processing, researchers at Meta FAIR introduced a versatilefoundation model for image segmentation, termed the Segment Anything Model(SAM) . It is widely recognized that foundation models in any domain oftenconfront challenges stemming from limited data diversity. Despite the consider-able scale of the dataset utilized to train SAM (referred to as the SA-1B dataset),comprising over one billion masks, the models performance fell short in med-ical image segmentation tasks . This shortfall can be attributed in part tothe composition of the SA-1B dataset, which primarily comprises photographs ofnatural scenes captured by cameras, thus lacking the nuanced features character-istic of medical images. In response to this challenge, Ma et al. curated a diverseand extensive medical image segmentation dataset encompassing 15 modalities,upon which they fine-tuned SAM . Their refined model, dubbed MedSAM,represents a significant step forward in addressing this discrepancy. However,",
  "Lu et al": "6. Liu, Y., Yang, H., Dong, Z., Keutzer, K., Du, L., Zhang, S.: Noisyquant: Noisybias-enhanced post-training activation quantization for vision transformers. In:Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-nition. pp. 2032120330 (2023) 2 7. Liu, Z., Cheng, K.T., Huang, D., Xing, E.P., Shen, Z.: Nonuniform-to-uniformquantization: Towards accurate quantization via generalized straight-through es-timation. In: Proceedings of the IEEE/CVF conference on computer vision andpattern recognition. pp. 49424952 (2022) 4",
  "LiteMedSAM5.7M6.2K4.1MMedSAM89.7M": "In addition to optimizing the backbones of SAM, we pursued an alterna-tive approach to expedite inference: quantization. Quantization offers severalbenefits, including reducing parameter sizes, increasing inference speed, and de-creasing power consumption during inference. There are two primary paradigmsfor quantizing neural networks: post-training quantization (PTQ) and quantization-aware training (QAT) . PTQ involves converting a pre-trained floating-point model directly into a low-precision one by calibrating themodel using a batch of calibration data. This method is generally faster sinceit does not require re-training, and the precision of the quantized model largelydepends on the calibration process. On the other hand, QAT integrates quan-tization and de-quantization nodes into the computational graph, enabling thetraining of the model while preserving its accuracy after quantization. To ensureprediction accuracy, we chose QAT to quantize SAM.The attention blocks of transformers serve as the principal components in thebackbone of SAM. Several methods have been proposed to enhance the accu-racy of quantized transformers. Li et al. introduced an information rectificationmodule and a distribution-guided distillation scheme tailored for fully quantizedvision transformers . Liu et al. discovered that incorporating fixed uniformnoise into the values being quantized can significantly mitigate quantization er-rors under provable conditions . In this study, we have chosen to leverage theXilinx Brevitas framework . This framework offers an excellent workflow,",
  "Preprocessing": "The dataset comprises three types of medical images: grayscale images, RGBimages, and 3D images. 3D images are split into individual 2D clips along thez-axis, with each clip treated as a grayscale image. To standardize the grayscaleformat with the RGB format, grayscale images are duplicated across the red,green, and blue channels. Subsequently, RGB images are resized, padded todimensions of 256 256, and finally normalized. Its important to note that inthe baseline approach, RGB images undergo normalization before padding withzeros. In this case, the padded value is equivalent to the minimum value of theimage instead of zero.Weve implemented some optimizations in the dataloader to enhance effi-ciency during both training and inference. For the training process, in the base-line approach, all compressed 3D npz files are decompressed along the z-axis,which demands approximately 10TB of disk storage. This overhead is signifi-cantly disproportionate to the size of the original dataset, which is only around160GB. To mitigate this inefficiency, we propose indexing each 3D clip alongthe z-axis and employing a binary search algorithm to locate the target 2D clipwhen necessary. By adopting this strategy, we distribute the decompression timeacross each batch of training data, resulting in substantial savings in disk stor-age. Additionally, considering that our machine typically processes one batchof data in approximately one second, the computational cost of decompressionbecomes negligible.In terms of inference, the baseline method iterates through each 3D promptbox individually. However, when 3D boxes intersect along the z-axis, the base-line recalculates image features. Given that the image encoder constitutes themost computationally intensive aspect of SAM, we propose to preprocess all the3D boxes into 2D boxes corresponding to 2D clips. This approach ensures thatthe image embedding of each 2D clip is computed only once, optimizing com-putational resources. In addition, the challenge has an 8GB limit on the Docker",
  "Proposed method": "We propose to quantize the baseline model LiteMedSAM using QAT. While neu-ral networks consist of various components beyond just matrix multiplications,its within these operations that the peak of computational complexity resides.Therefore, nearly every QAT method focuses on quantizing inputs and weightsduring matrix multiplications, such as in linear layers, convolution layers, andattention blocks. In contrast, operations involving biases, activation layers, andnormalization layers are typically performed per element. While the quantiza-tion of these layers can be selective, in our proposed quantized model, we opt toretain all these layers as floating-point, with only matrix multiplications in theimage encoder and the mask decoder being quantized. The reason we choose notto quantize the prompt encoder lies in the fact that its parameter size is over1000 times smaller than the other two modules, as indicated in . Someof the most common quantized sub-structures are illustrated in .Since quantization is non-differentiable, we employ the straight-through es-timator (STE) methodology, as demonstrated in previous works . In STE,incoming gradients are directly passed through a threshold operation to becomeoutgoing gradients. For each quantization node, we propose an 8-bit symmetricper-tensor signed integer activations quantizer with a learned floating-point scalefactor. This scale factor is initialized from runtime statistics.",
  "Model Inference and Post-processing": "Upon completion of quantization-aware training, Brevitas provides exceptionaltoolchains for exporting quantized models to diverse backends.While the standard QuantizeLinear-DeQuantizeLinear (QCQ) representationfor quantization in ONNX exists, Brevitas has extended this to QuantizeLinear-Clip-DeQuantizeLinear (QCDQ). With this extension, researchers can confinethe range of quantized values. Therefore, we propose exporting the quantizedLiteMedSAM to ONNX in the QCDQ representation.While numerous inference engines support the ONNX format, not all of themare compatible with QCDQ. Given that the challenge mandates CPU inference,we narrow down the options to ONNX Runtime and OpenVINO. An experimenton inference speed between these two inference engines is detailed in .1.Based on the results, we ultimately opt for OpenVINO. Model caching is alsosupported by OpenVINO. This strategy can reduce the resulting delays at appli-cation startup, making it considerably suitable for accelerating in this challenge .",
  "QMedSAM5": ". Common quantized sub-layers. (a) quantized linear layer; (b) quantized convo-lutional layer; (c) quantized attention block. Circles in the figure represent correspond-ing calculations: M stands for matrix multiplication, C stands for convolution, and Tstands for transpose. Operations involving quantization are represented by round rect-angles in the figure. The inputs and output of all the sub-layers depicted in the figureare floating-point tensors.",
  "Dataset and sampler": "We employed the challenge dataset for training, while the evaluation datasetwas obtained by partitioning it at a ratio of one-tenth. The dataset comprises11 modalities, and their sizes (prior to partitioning into training and evaluationdatasets) are summarized in . An evident issue arises from the significantimbalance in sample numbers across modalities.To address this imbalance andprevent bias or overfitting in the quantized model, as well as to expedite training,we propose randomly sampling Ns 2D clips from each modality in each epoch.Additionally, these samples undergo random horizontal and vertical flips for dataaugmentation.",
  "Metrics and loss functions": "The accuracy of the model is evaluated using the Dice Similarity Coefficient(DSC) and the Normalized Surface Distance (NSD), while efficiency is measuredthrough running time analysis. These metrics are collectively utilized to computethe ranking. In the training phase, we mainly employ a combination of theDice loss and focal loss. This decision is based on the robustness demonstratedby compound loss functions in various medical image segmentation tasks, asevidenced in prior research .",
  "QMedSAM7": "In stage one, our goal is to train the quantized image encoder while keepingthe floating-point prompt encoder and the mask decoder frozen. Apart fromthe loss function mentioned in section 3.2, we further distill the image encoderfrom MedSAM and introduce the distillation loss. This loss is calculated as theproduct of the mean squared error and the intersection over union ratio acrossthe image embeddings generated from the teacher and student models.",
  "In the final stage, the whole model undergoes an end-to-end fine-tuning forfurther fitting with the dataset": "For each stage, we propose employing linear learning rate warm-up for Nwepochs, commencing at 1% of the initial learning rate. Additional training detailsare summarized in . This warm-up period is followed by a cosine anneal-ing scheduler for Na epochs. The minimum learning rate of the cosine annealingscheduler is set to 0.1% of the initial learning rate, and the half-period of thecosine function is determined as Na 1. Once the quantization-aware trainingprocess is completed, we evaluate the checkpoint of each epoch on the evalua-tion dataset and select the best-performing one. Additional training details aresummarized in .",
  "Inference speeds of different engines": "The challenge evaluates models on an Intel Xeon W-2133 CPU (),while we use an Intel Core i7-8750H CPU () that offers comparableperformance because we do not have an identical environment. We test eachvariant with a single image and a prompt box. The inference speeds of variousmethods are detailed in .",
  "Quantized LiteMedSAM inferenced on OpenVINO0.585s": "The results indicate that the quantized model does not exhibit the fastestruntime. This is because that our hardware is not optimized for quantized op-erations, resulting in slower execution compared to standard floating-point op-erations. For comparison purposes, the inference speeds of both floating-pointand quantized versions of MedSAM (which is substantially larger than LiteMed-SAM) are provided in . Interestingly, in this case the quantized modeloutperforms the floating-point model.Given the comprehensive advantages of quantization, it is evident that de-ploying the quantized LiteMedSAM on the OpenVINO inference engine effec-tively addresses the requirement for medical image segmentation \"on laptop\".",
  "Quantized MedSAM inferenced on OpenVINO3.558s": "On average, the quantized model scores comparably on DSC and slightlyhigher on NSD. We highlight the modalities with significant differences in theiraccuracy. In particular, the quantized model has degraded performance by around3% and 5% in MR and US, but shows gains of approximately 10% and 9% im-provement in PET and Microscope. It is evident that, to a certain extent, theproposed method has effectively addressed the performance imbalance of thebaseline model across various modalities, which was caused by the datasetsinherent imbalance.",
  "Qualitative results on validation set": "Two sets of successful segmentation results are depicted in . It can beobserved that the proposed quantized model performs better in matching theROI than the floating-point counterpart. illustrates two sets of chal-lenging cases. In these cases, the segmentation results of the proposed quantizedmodel align more closely with the ground truth ROI compared to the baseline.However, since the baseline prediction results were significantly distant from theground truth, the correction was unsuccessful.",
  "Ablation Study": "Training a Segment Anything Model from scratch requires a huge mass ofdata. However, the proposed quantization-aware training procedure starts witha pre-trained model. Reducing the number of samples Ns from each modality,especially from the larger modalities, certainly benefits in saving training time.However, it still raises questions about its influence on the precision of the quan-tized model. In this section we propose an ablation study to explore the balancebetween efficiency and accuracy.To describe the variation of samples from different modalities clearly, wewill use Ns(m) to represent the number of samples from modality m. The totalsamples of modality m is denoted by Nm(m), and the complete set of modalitiesis denoted by M. The strategy of the proposed method can be described as",
  ", miniM Nm(i)": "The metrics of the three stages in the ablation study are summarized in. Compared with (we provide the average metrics of the pro-posed method in the last row of ), the results indicate that increasing Nsdoes not result in a significant improvement, underscoring the efficiency of theproposed QAT pipeline in terms of training time.",
  "Results on final testing set": "The testing results are summarized in . The proposed quantized modelexhibits a marginal decrease but much more balance in the average accuracy.Additionally, the inference efficiency has been significantly optimized under thesame backbone. Compared with , we can observe that the models perfor-mance on different modalities varies between the validation set and the testingset. However, the trend of balance across modalities remains consistent.",
  "Conclusion": "In this paper, we present an efficient pipeline for quantizing LiteMedSAM anddeploying it on the OpenVINO inference engine. Objective experiments haveconclusively shown that our method significantly accelerates the baseline whilemaintaining an acceptable level of accuracy. Future endeavors will focus on en-hancing the speed of the floating-point backbone, further alleviating the im-balance across different modalities, and deploying the quantized model on cus-tomized hardware platforms.",
  "Disclosure of Interests. The authors have no competing interests to declarethat are relevant to the content of this article": "1. Hubara, I., Nahshan, Y., Hanani, Y., Banner, R., Soudry, D.: Accurate post train-ing quantization with small calibration sets. In: International Conference on Ma-chine Learning. pp. 44664475. PMLR (2021) 2 2. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A.G., Adam, H.,Kalenichenko, D.: Quantization and training of neural networks for efficient integer-arithmetic-only inference. CoRR abs/1712.05877 (2017), 2 3. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,Whitehead, S., Berg, A.C., Lo, W.Y., Dollar, P., Girshick, R.: Segment anything.In: Proceedings of the International Conference on Computer Vision. pp. 40154026 (2023) 1 4. Le, B.H., Nguyen-Vu, D.K., Nguyen-Mau, T.H., Nguyen, H.D., Tran, M.T.:MedficientSAM: A robust medical segmentation model with optimized inferencepipeline for limited clinical settings. In: Submitted to CVPR 2024: Segment Any-thing In Medical Images On Laptop (2024), under review 4",
  ". Ma, J., He, Y., Li, F., Han, L., You, C., Wang, B.: Segment anything in medicalimages. Nature Communications 15(1), 654 (2024) 1": "11. Pappalardo, A.: Xilinx/brevitas. 212. Pfefferle, A.T., Purucker, L., Hutter, F.: DAFT: Data-aware fine-tuning of founda-tion models for efficient and effective medical image segmentation. In: Submit-ted to CVPR 2024: Segment Anything In Medical Images On Laptop (2024), under review 4 13. Shen, M., Liang, F., Gong, R., Li, Y., Li, C., Lin, C., Yu, F., Yan, J., Ouyang,W.: Once quantization-aware training: High performance extremely low-bit ar-chitecture search. In: Proceedings of the IEEE/CVF International Conference onComputer Vision. pp. 53405349 (2021) 2"
}