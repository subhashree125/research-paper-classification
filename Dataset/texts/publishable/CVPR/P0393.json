{
  "Marcos V. CondeZhijun LeiWen LiCosmin StejereanIoannis Katsavounidis": "Radu TimofteKihwan YoonGanzorig GankhuyagJiangtao LvLong SunJinshan PanJiangxin DongJinhui TangZhiyuan LiHao WeiChenyang GeDongyang ZhangTianle LiuHuaian ChenYi JinMenghan ZhouYiqiang YanSi GaoBiao WuShaoli LiuChengjian ZhengDiankai ZhangNing WangXintao QiuYuanbo ZhouKongxian WuXinwei DaiHui TangWei DengQingquan GaoTong TongJae-Hyeon LeeUi-Jin ChoiMin YanXin LiuQian WangXiaoqian YeZhan DuTiansen ZhangLong PengJiaming GuoXin DiBohao LiaoZhibo DuPeize XiaRenjing PeiYang WangYang CaoZhengjun ZhaBingnan HanHongyuan YuZhuoyuan WuCheng WanYuqing LiuHaodong YuJizhe LiZhijuan HuangYuan HuangYajun ZouXianyu GuanQi JiaHeng ZhangXuanwu YinKunlong ZuoHyeon-Cheol MoonTae-hyun JeongYoonmo YangJae-Gon KimJinwoo JeongSunjei Kim",
  "arethechallengeorganizers,whiletheotherauthorsparticipatedinthechallenge.Workshoppage:https ://ai4streaming- workshop.github.io/.Benchmark code:": "Marcos V. Conde (corresponding author, project lead) and Radu Timofteare with University of Wurzburg, CAIDAS & IFI, Computer Vision Lab.Zhijun Lei, Wen Li, Cosmin Stejerean and Ioannis Katsavounidis are withMeta. to 4K resolution (4x factor) in real-time on commercialGPUs. For this, we use a diverse test set containing a va-riety of 4K images ranging from digital art to gaming andphotography. The images are compressed using the modernAVIF codec, instead of JPEG. All the proposed methods im-prove PSNR fidelity over Lanczos interpolation, and processimages under 10ms. Out of the 160 participants, 25 teamssubmitted their code and models.The solutions presentnovel designs tailored for memory-efficiency and runtimeon edge devices. This survey describes the best solutionsfor real-time SR of compressed high-resolution images.",
  ". Introduction": "Single image super-resolution (SR) methods generate ahigh-resolution (HR) image from a single degraded low-resolution (LR) image.This ill-posed problem was ini-tially solved using interpolation methods. However, SR isnow commonly approached through the use of deep learn-ing . Image SR assumes that the LR imageis obtained through a degradation processes. This can beexpressed as:",
  "y = (x k) s,(1)": "where represents the convolution operation betweenthe LR image and the blur kernel, and s is the down-sampling operation with respective down-sampling factors (e.g. 2, 3, 4, 8).Following the foundational efforts by Shi et al. ,optimizing deep neural networks for single image super-resolution has become critical . Thisfocus has inspired the creation of numerous workshops andchallenges, for instance , which serve as platformsfor exchanging ideas and pushing the boundaries of efficientand real-time super-resolution (SR). Moreover, many worksconsider compressed images as inputs (e.g. JPEG), whatleads to more practical super-resolution methods .",
  ". AIS 2024 Real-Time Image SR Challenge": "In conjunction with the 2024 AIS: Vision, Graphics and AIfor Streaming workshop, we introduce a new real-time 4Ksuper-resolution challenge.The challenge aims to upscale a compressed LR im-age from 540p to 4K resolution using a neural networkthat complies with the following requirements: (i) improveperformance over Lanczos interpolation. (ii) Upscale theimage under 33ms. Moreover the images are compressedusing different compression factors (QP values) using themodern AVIF codec instead of JPEG. The challenge seeksto identify innovative and advanced solutions for real-timesuper-resolution of compressed images.",
  ". Motivation": "AV1 Image File Format (AVIF) is the latest royalty-freeimage coding format developed based on the Alliance forOpen Medias (AOM) AV1 video coding standard.Thecompression efficiency and quality of AVIF encoded im-ages is noticeably superior to JPEG and also HEIC, whichuses HEVC for image coding. AVIF is also supported in allmajor web browsers. In the AIS 2024 Real-Time Image SRChallenge, we want to leverage AVIF as the image codingformat to evaluate the quality improvement from SR whencombining with AVIF.",
  ". 4K SR Benchmark Dataset": "Following , the 4K RTSR benchmark provides a uniquetest set comprising ultra-high resolution images fromvarious sources, setting it apart from traditional super-resolution benchmarks.Specifically, the benchmark ad-dresses the increasing demand for upsampling computer-generated content e.g. gaming and rendered content, in ad-dition to photo-realistic imagery, thereby posing a differentchallenge for existing SR approaches.The testing set includes diverse content such as renderedgaming content, digital art, as well as high-resolution photo-realistic images of animals, city scenes, and landscapes, to-taling 110 test samples.All the images in the benchmark testing set are at least4K resolution i.e. 3840 2160 (some are bigger, even 8K).The distribution of the 4K RTSR benchmark testset is:14 real-world captures using a 60MP DSLR camera, 21 ren-dered images using Unreal Engine, 75 diverse images e.g.animals, paintings, digital art, nature, buildings, etc. Compression and DownsamplingWe use ffmpeg toproduce the LR compressed images. We use 5 different QPvalues: 31, 39, 47, 55, 63. We use lanczos interpolation todownsample the images. Bellow we provide an example:",
  "../1.png -vf scale=ceil(iw/4):ceil( ih/4):flags=lanczos+accurate_rnd+ full_chroma_int:sws_dither=none: param0=5 -c:v libsvtav1 -qp 31 - preset 5 1_4x_qp31.avif": "In the context of AVIF and AV1 codecs, larger Quanti-zation Parameter (QP) values imply more compression. Es-sentially, the QP value dictates the level of quantization ap-plied to the video or image data, where higher quantizationreduces the amount of data required to represent the originalinput, thus leading to higher compression ratios.The participants can use any publicly available dataset,and produce the corresponding LR images.",
  ". Evaluation": "The evaluation scripts were made available to the par-ticipants through GitHub ( allowed theparticipants to benchmark the performance of their modelson their systems. During the final test phase, the partic-ipating teams provided the code, models and results corre-sponding to the 110 test images. They did not have access tothe HR ground-truth. The organizers then validated and ex-ecuted the submitted code to obtain the final results, whichwere later conveyed to the participants upon completion ofthe challenge.",
  "Baseline Lanczos-----30.3626.6732.7529.120.8000.7150.8430.774": ". Results of the AIS24 Real-Time SR challenge. All the proposed methods upsample the images under 8ms. The models canprocess compressed images with QP factors from 31 to 63. We provide PSNR and SSIM metrics in the RGB domain, and for the Luma (Y)channel. We report the fidelity improvement w.r.t the baseline in terms of PSNR , considering the average PSNR-Y for QP31 and QP63.We highlight the top-3 (gold, silver, bronze) methods in terms of fidelity, runtime, and computational efficiency (MACs operations).",
  ". Architectures and Main Ideas": "Here we summarize the core ideas behind the most compet-itive solutions. Note that most of the ideas follow . Re-parameterization enables training the network us-ing sophisticated blocks , while allowing these Rep-Blocks to be simplified into a standard 33 convolutionsduring inference. This technique has become state-of-the-art in efficient SR . Pixel shuffle and unshuffle. These techniques are alsoknown as depth-to-space, space-to-depth, and sub-pixelconvolutions . These are utilized to effectively applyspatial upsampling and downsampling over feature maps.",
  ". Results and Conclusion": "In Tab. 1 we provide the challenge benchmark. The mod-els can upsample compressed 540p images and recover thecore structural information according to the metrics calcu-lated over Luma (Y). We can also appreciate a notable per-formance decay at high QP (compression) values. The run-time is the average of 100 runs after GPU warm up, usingan NVIDIA 4090 GPU. Following previous work, we alsoreport a Score (S) that considers PSNR and runtime, de-fined as: S = (22)/(T 0.5 C) where T is the runtime, and C = 0.1 is a constant scaling factor.Attending to the results, we consider that this scoreis now obsolete, and we included it only for informativepurposes. All the methods achieve runtimes under 10ms.In this scenario, larger fidelity improvements and othermemory-related improvements are considered more impor-tant for practical applications.In Sec. 3 we provide the description of the top solutions.Considering the best methods, we can conclude thatthere is certain convergence in the model designs. As previ-ously mentioned, re-parameterization is ubiquitous. Edge-oriented filters to extract directly high-frequencies allow toreduce sparsity in the neural network, making effective useof all the kernels (parameters). Upsampling the input im-age, and enhancing it through a global residual connectionis also a common neural network architecture. Related ChallengesThis challenge is one of the AIS2024 Workshop associated challenges on:Event-basedEye-Tracking , Video Quality Assessment of user-generated content , Real-time compressed image super-resolution , Mobile Video SR, and Depth Upscaling. AcknowledgmentsThis work was partially supported bythe Humboldt Foundation. We thank the AIS 2024 spon-sors:Meta Reality Labs, Meta, Netflix, Sony Interac-tive Entertainment (FTG), and the University of Wurzburg(Computer Vision Lab).Marcos Conde is also supported by Sony Interactive En-tertainment (FTG).",
  "University of Science and Technology of China": "The team proposes RepTCN, a network comprising onlythree convolutional layers, achieving superior performanceover Lanczos interpolation while maintaining exceptionalefficiency. To further enhance efficacy, we introduced re-parameterization techniques, replacing the middle convolu-tional layer with a RepBlock during the training phase.Additionally, we devised a three-stage training strategy tofully exploit the models potential. illustrates our proposed RepTCN. It consists ofthree convolutional layers, each without bias. A ReLU acti-vation function is applied between every two convolutionallayers. During the training phase, we replace the middleconvolution with a RepBlock. During inference, wereparameterize the RepBlock into a convolutional layer. Implementation detailsOur training framework uses Py-torch for training on the RTX3090.We gathered thefirst 600 images from DIV2K, the first 600 images fromFlicker2K, and the first 800 images from GTAV. Subse-quently, we cropped these images to 512 512 to form ourdataset.During the training phase, the input from the dataset willbe randomly cropped into patches, and these patches willundergo random horizontal flips and rotations. The modeltraining can be divided into three stages. In the first stage,we set the batch size to 32 and the patch size to 32. L1loss are used as target loss functions. We replaced the mid-dle convolutional layer with a RepBlock and trained for1000k iterations using the Adam optimizer, with a learningrate of 1 103 decreasing to 1 107 through the co-sine scheduler. In the second stage, we set the batch size",
  ". Simple network proposed by PixelArtAI": "to 16 and the patch size to 128. MSE loss are used as tar-get loss functions. We reparameterized the RepBlock intoa convolutional layer and trained for 500k iterations usingthe Adam, with a learning rate of 5 104 decreasing to5107 through the cosine scheduler. In the third stage, weremoved the bias from each convolutional layer and trainedfor 2000k iterations using the Adam, with a learning rate of5 104 decreasing to 5 107 through the cosine sched-uler. The other Settings are the same as in the previous step.",
  "MangoTV": "The team proposes a lightweight and extremely low-time-consuming network is built through re-parameters.Based on the ECB module , we designed a lightweightand low-time-consuming network for the competition. Thenetwork design points are as follows:First downsample by a factor of 2 using a convolutionwith a stride of 2. Downsampling breaks down compressionand also improves network inference speed. Then stack twoECB modules and a 8x upsampled pixel shuffle module toreturn a three-channel image see .",
  "Efficiency MetricsConsidering an input 540p and x4 SR,the model has 1.8798 KMACs and a runtime of 1.0367 ms": "Implementation detailsThe training data is degradedby FFmpeg with random QP. The input image size is120x120x3, amd the batch size is 96. We use Adam opti-mizer with the initial learning rate set to 0.001. The trainingis divided into two stages: First, the learning rate is 0.001and the loss is L1. This stage is trained for 60k iterations.Second, only the PSNR Loss is calculated, and the initiallearning rate set to 0.0002, and is halved by 20k iterations.",
  "Network Intelligence Platform Development Dept. III,ZTE2 Network Intelligence Platform System Dept., ZTE": "The team proposes an ultra lightweight image super-resolution network named Lanczos++.The highlights of the proposed network (see ) areas follows: First, we use PiexlShuffle to perform 3x down-sampling on the input LR image while increasing the chan-nel dimension. This design significantly improves the in-ference efficiency of the network, while basically not los-ing the representation ability of the model. Second, we de-sign a new type of reparameterization module named Rep-Block(see ).In the training phase, we first use 1x1 convolution to in-crease the input channel to four times, then use 3x3 convo- lution for feature extraction, and finally use 1x1 convolutionto transform the dimension into the output channel dimen-sion, and use it as a residual with the branch that transformsthe input channel into the output channel through 1x1 con-volution. During the model inference stage, we merge thereparameterization module into a standard 3x3 convolution.Reparameterization can improve the fidelity of the modelwhile maintaining its inference efficiency unchanged.Third, we remove the bias of the convolutional layer anduse 1x1 convolution instead of 3x3 convolution in the recon-struction layer convolution section, which can significantlyreduce the running time of the model. Finally, we use Pix-elshuffle for 12x up-sampling. Implementation detailsWe use 4450 images fromDIV2K, Flickr2K and GTA V datasets for training.To gen-erate the LR data, the images are lanczos downsampledby scale 4 and compressed samples using AVIF with QP31/39/47/55/63 (5 compression levels).We implement a three-stage training pipeline: traininga basic model, deviation removal of convolutional layers,and final fine-tuning of switching loss functions.In thefirst stage, we conduct a NAS architecture search to findthe optimal network parameter configuration. For the firsttwo stages, we use the L1 loss function for training, and forthe final stage, we use the L2 loss function.We use Adam optimizer by setting 1 = 0.9 and 2 =0.999. In the first two stages of training, we start with alearning rate of 5e-4. For the final stage, they start from 2e-4. We use a decaying learning rate scheduler for all stages,where the first 500 epochs are preheated and then the learn-ing rate decays linearly until 1e-8. The total duration of thetraining process is around 50h using a V100 (32Gb) GPU.",
  "Nanjing University of Science and technology": "SAFMN++: Improved Feature Modulation Network forReal-Time Compressed Image Super-ResolutionWeintroduce SAFMN++, an enhanced version of SAFMN for solving real-time compressed image SR. This solution ismainly concentrates on improving the effectiveness of thespatially-adaptive feature modulation (SAFM) layer.Different from the original SAFM, as shown in Fig 7, theimproved SAFM (SAFM++) is able to extract both localand non-local features. In SAFM++, a 33 convolution isfirst utilized to extract local features and a single scale fea-ture modulation is then applied to a portion of the extractedfeatures for non-local feature interaction.After this process, these two sets of features are aggre-gated by channel concatenation and fed into a 11 convo-lution for feature fusion.The proposed SAFMN++ is trained by minimizing acombination of the uncertainty-based MSE loss andFFT-based L1 loss with Adam optimizer for a total of500,000 iterations. We train the proposed SAFMN++ on theDIV2K dataset. The cropped LR image size is 640640and the mini-batch size is set to 64. We set the initial learn-",
  "ing rate to 3 103 and the minimum one to 1 107,which is updated by the Cosine Annealing scheme . presents the efficiency study of SAFMN++": "A Simple Residual ConvNet with Structural Re-parameterization for Real-Time Super-ResolutionThesolution VPEG-R is shown in . The proposed methodreduces the spatial resolution by a Pixel Unshuffle opera-tion and uses a convolutional layer to transform the inputLR image into the feature space, then performs performsfeature extraction using 3 reparameterizable residual blocks(RepRBs), and finally reconstructs the final output by a Pix-elShuffle convolution.We use DIV2K as the training data. In order to accel-erate the IO speed during training, we crop the 2K resolu-tion HR images to 640640 sub-images, and the mini-batchsize is set to 64.",
  "The University of Seoul2 Korea Electronics Technology Institute (KETI)": "We initially reviewed the key factors essential for de-veloping a network structure.Subsequently, we suggesta Cascade Upsampling network structure with ChannelAlignment approach for image enhancement, which en-hances performance and notably decreases processing time.Lastly, we designed an effective network and integratedreparameterization blocks and knowledge distillation meth-ods to enhance performance without increasing the modelssize .We compared our proposed method with LRSRN which proposed work on the NTIRE 2023 real-time super-resolution challenge in Tab. 4. The score value is cal-culated from the script of . Our proposed method over-",
  "Implementation detailsWe used two different types ofdataset: DIV2K and combined datasets. DIV2K: Well-known open dataset. DIV2K training dataset used in the scratch training step": "Combined: The DIV2K training dataset is utilized dur-ing the initial training phase from scratch. In contrast, acomposite dataset is used for the subsequent second stage.This combined dataset comprises the full DIV2K train-ing set (800 images), the initial 1000 images from theFlickr training set, 121 samples from the GTA training se-quences 00 to 19, the first 1000 images from the LSDIRdataset. To generate low resolution, we degrade the ran-dom cropped images with avif compression with various compression factors. For both training stages, we usedrandom cropping, rotation 90, horizontal flip, and verticalflip augmentation.We trained our model in three steps:(1) Scratch train step: In the first step, our model wastrained from scratch. The LR patches were cropped fromLR images with eight mini-batch 98 x 98 sizes. Adam opti-mizer was used with a learning rate of 0.0005 during scratchtraining. The total number of epochs was set to 800. We usethe l1 loss.(ii) Second step: In the second step, the model was ini-tialized with the weights trained in the first step. The dis-tillation method used at this stage. The teacher model wastrained with the combined dataset. The detailed illustratedexample is shown in b. Fine tuning with loss l2 im-proves the PSNR by 0.01 0.02 dB. Also, we turn off thebias term of the reparametrization block at this stage. Inthis step, the initial learning rate was set to 0.00005 andthe Adam optimizer was used along with a cosine warm-up.The total epoch was set at 800 epochs.(iii) Third step: In the third stage, the model was ini-tialized using the weights trained in the previous step. Inaddition, the distillation technique was applied in this phaseas well. The training hyper-parameters were kept identicalto those in the second step. At this point, the bias term ofthe reparametrization block was deactivated, leading to adecrease in inference time by 0.2 ms. Although there wasa slight reduction of 0.02 dB in the precision of the PSNRvalue, the overall score improved.We refer the reader to the CASR paper for moredetails.",
  "Institute of Artificial Intelligence and Robotics, XianJiaotong University": "We propose a real-time image super-resolution methodcalled RVSR, which is inspired by previous work .Our method leverages the efficient architectural designs oflightweight ViTs and the re-parameterization technique toachieve superior performance in real-time super-resolutiontasks. RVSR first applies a 33 convolution to convert thechannel of feature map to the target size (16). Then, RVSRemploys 8 stacked RepViT blocks to perform deep fea-ture extraction. As shown in (a), the RepViT blocksintegrate the efficient architectural designs of lightweightViTs. Inspired by , RVSR employs the RepConv mod-ule to improve the SR performance while maintaining lowcomplexity, as shown in (b). LR Conv-3 RepConv SE Conv-1 GELU Conv-1 RepConv Conv-1 PixelShuffle SR RepViT",
  ". Overview of the proposed RVSR by Team XJTU-AIR": "We conducted an end-to-end training of the RVSR modelfor 5000 epochs, employing a batch size of 32 and optimiz-ing by minimizing the MSE loss with the Adam optimizer.For inference, we re-parameterized the model using stan-dard 3x3 convolutions, as illustrated in (b). Implementation detailsThe method is implemented inPyTorch. For optimization, we utilize the Adam optimizerwith 1 = 0.99 and 2 = 0.999. The learning rate is set to5 104 for the first 1000 epochs, after which it linearlydecays until reaching 1 106.We trained RVSR on DIV2K dataset (800 images),Flickr2K dataset (2650 images) and LSDIR dataset (first1000 images). For generating low-resolution images, weemployed Lanczos downsampling and AVIF compression,with compression factors ranging from QP 31 to 63. Dur-ing training, we used random cropping, rotations, and flipsaugmentations.Besides, the images are normalized tothe range . The experiments were conducted on aNvidia GeForce RTX 3090 GPU, with the input size set to960540. MACs: 15.62 (G), 1883 MACs per pixel, run-time: 12.54 ms (FP32) and 7.36 ms (FP16).",
  ". The framework of the proposed Anchor-based NestedUnshuffleNet for Real-time Super-Resolution (ANUNet)": "We propose Anchor-based Nested UnshuffleNet forReal-time Super-Resolution (ANUNet).As shown in, the pixel-unshuffle technique is used to reducethe resolution of the image and increase the channel dimen-sion. This design allows for a reduction in the computa-tional overhead of the network while preserving the con-stant volume of information. After an ECB + GeLUmodule, the main module is composed of a sequence ofNested Re-parameterization Block (NRB) + GeLU activa-tion, which serves to extract and refine features in a progres-sive manner. Then, an ECB layer is adopted to transfer fea-tures, followed by an upsampling layer for recovering theresolution to LR. While an anchor-based residual learningis applied to directly repeat the RGB channels 16 times inLR space to generate anchors. Finally, a pixel shuffle layeris is used to reconstruct the final HR output.Different from and , we design a nested struc-ture, named Nested Re-parameterization Block (NRB). illustrates the proposed NRB. In the training stage,the NRB employs a nested structure, the outer structure isthe ERB RepBlock in the Enhanced Residual Block (ERB)first proposed by , the inner structure is an enhancedEdge-oriented Convolution Block (eECB), which includesmultiple branches, and can be merged into one normal con-volution layer in the inference stage. Performance remainsunaffected after re-parameterization in this design. Implementation detailsWe use DIV2K and Flickr2Kfor training. To generate the compressed LR images, weuse AVIF to process the above datasets with the random QPranges between 31 and 63. Besides, standard augmentationsthat include all variations of flipping and rotations are alsoused to improve performance. Additionally, the number offeature channels is set to 28, and the scale of pixel unshuf-fle and pixel shuffle in the sub-branch is set to 2. Afterthe training, we re-parameterize the model into a networkstructure (ECB and NRB modules) with regular 3x3 convo-lutions.",
  "The model is conducted using the PyTorch frameworkwith one NVIDIA A100 40G GPU. Specifically, the train-ing is divided into three stages:": "1.Initially, the model is trained from scratch with480480 patches randomly cropped from high resolution(HR) images with a mini-batch size of 64. We apply a com-bination of Charbonnier loss and FFT-based frequencyloss function for reconstruction. The network is trainedfor 1000k iterations using the Adam optimizer, with a learn-ing rate 1 103 decreasing to 1 106 through the cosinescheduler. 2. In the second stage, the model is initialized with thepre-trained weights from the first stage on the same train-ing data as stage 1. Inspired by , the auxiliary loss andhigh-frequency loss are added to our training. Instead ofthe downsampling bicubic operator used in , Lanczosis applied to maintain consistency with the downsamplingmethod in AVIF. The network parameters are optimized for1000k iterations with the MultiStepLR scheduler, where theinitial learning rate is set to 5 104 and halved at 200k,400k, 800k-iteration.",
  "Fuzhou University2 Imperial Vision Technology": "We propose a real-time image super-resolution based onre-parameterization and edge extraction. We use pixel un-shuffle to reduce the image resolution and increase the chan-nel dimension. This design reduces the computational costof the network while keeping the amount of informationconstant. Meanwhile, we propose a reparameterized im-age edge extraction block that extracts features in parallelthrough multiple paths in the training phase, including 33and 11 convolution for channel expansion and compres-sion, as well as sobel and laplacian filters for acquiring in-formation about image edges and textures.In the inference stage, multiple operations can be com-bined into a 33 convolution. The performance of 33 con-volution is improved without introducing any extra cost.",
  "We use PyTorch and a RTX 3090 GPU (24GB). Themodels are optimized using Adam with Cosine Warmup.The total duration of the training process is 48hrs": "In the first training stage, we train our model fromscratch.The LR patches cropped from LR images with128x128 image size and 64 mini-batch. The Adam opti-mizer uses a 0.0005 learning rate.The cosine warm-upscheduler sets a 0.1 percentage warmup ratio. The totalnumber of epochs in this stage is set to 800. In the second stage, we initialize the model with theweights trained in the previous stage. In this step, the initiallearning rate is set as 0.0001. The cosine warm-up sched-uler is set with a 0.1 percentage warm-up ratio. The totalnumber of epochs is set to 200 epochs.",
  "Multimedia Department, Xiaomi Inc.2 Georgia Institute of Technology3 Dalian university of technology": "Real Time Swift Parameter-free Attention Network for4x Image Super-ResolutionWe propose a convolutionalneural network combining swift parameter-free attentionblock (SPAB) for image SR, the suggested model has veryfew parameters and fast processing speed for 4x image su-per resolution.As shown in , SPAN consists of 2 consecutiveSPABs and each SPAB block extracts progressively higher-level features sequentially through three convolutional lay-ers with C-channeled HW -sized kernels (In our model,we choose H = W = 3.). The extracted features Hiare then added with a residual connection from the inputof SPAB, forming the pre-attention feature map Ui for thatblock. The features extracted by the convolutional layersare passed through an activation function a() that is sym-metric about the origin to obtain the attention map Vi. Thefeature map and attention map are element-wise multipliedto produce the final output Oi = Ui Vi of the SPABblock, where denotes element-wise multiplication. Weuse W (j)i RCHW to represent the kernel of the j-thconvolutional layer of the i-th SPAB block and to rep-resent the activation function following the convolutionallayer. Then the SPAB block can be expressed as:",
  "(2)": "transforming into a concatenate-convolution structure,with Kb re-parameterized as a 3x3 convolution. Ultimately,all parameters are restructured through equivalent trans-formation, forming the comprehensive architecture of ourmodel.To confirm that our solution demonstrates superior perfor-mance over previous methods, we conducted a comparisonbetween ETDS and our model. At AIS2024 CVPR, dur-ing the Validation phase for Real-Time Compressed Im-age Super-Resolution, it was observed that ETDS scored22.844, whereas our proposed model scored 22.912, indi-cating an improvement in performance.Our method is trained on DIV2K and Flickr2Kdatasets, with images processed using AVIF compressionwith Quality Factor (QF) coefficients ranging from 31 to 63,and scaled by a factor of 4 via Lanczos interpolation. Dur-ing training we use data augmentation techniques:randomcropping to 64x64, random flipping, and random rotation.ETDS architecture is adapted with ECB to en-hance edge detail recovery in high-frequency gradients,while the Feature-Enhanced Module, aids in restoring in-formation lost through compression and downsampling. Implementation details Framework: PyTorch 2.1.1, PyTorch Lightning Optimizer and Learning Rate: We employed Adam op-timizer with parameters 1 = 0.9 and 2 = 0.999. Thetraining spanned 100 epochs with an initial learning rateset to 0.0001, halved at the 50th epoch. GPU: NVIDIA A100 (80GB) Training Time: The model trained for 24 hours. Training Strategies: We trained the model using allAVIF images generated within the quality factor rangeof 31 to 63. This entailed training on a total of 110,400images comprising 800 from DIV2K and 2,650 fromFlickr2K, each at 32 quality factors.",
  "O = Concat(O0, O1, O5, Wf1 O6),(3)": "where O is a 4C-channeled H W-sized feature mapwith multiple hierarchical features obtaining by concatenat-ing O0 with the outputs of the first, fifth, and the convolvedoutput of the sixth SPAB blocks by C-channeled 3 3-sized kernel Wf1. O is processed through a 3 3 convo-lutional layer to create an r2C channel feature map of sizeH W. Then, this feature map goes through a pixel shufflemodule to generate a high-resolution image of C channelsand dimensions rH rW, where r represents the super-resolution factor. The idea of computing attention maps di-rectly without parameters from feature extracted by convo-lutional layers, led to two design considerations for our neu-ral network: the choice of activation function for comput-ing the attention map and the use of residual connections,more details about activation function and SPAB moduleare in . C3 network for 4x image super-resolutionA three-layer convolutional neural network for image SR, the sug-gested model has very few parameters and fast processingspeed for 4x image super resolution. This model has 12.39GFLOPs and 0.024 M parameters. The model is shown in. Implementation detailsBoth models use HAT-L 4xpre-trained network for knowledge distillation. Framework: Pytorch Optimizer and Learning Rate: We implement the net-work with PyTorch (BasicSR framework). The optimizeris Adam with learning rate as 104.",
  ". Network architecture of SPANR proposed by Team XiaomiMM": ". Diagram of the framework proposed by Team USTC Noah. The method utilizes a single DecoupleConv (DConv) with a kernelsize of 4 and a stride of 2 to form the feature mapping layer. Concurrently, we construct the feature learning layer using three DConvs,each with a kernel size of 3 and a stride of 1. The resolution of the features is altered through the application of Pixel shuffle.",
  "University of Science and Technology of China2 Huawei Noahs Ark Lab": "To enhance the networks perception of gradients andcontrast, we have refined the existing vanilla convolutionunit by performing feature decoupling within local regions.We innovatively introduce gradient (sub) operators and ag-gregation (add) operators to convolution to capture detailand contrast relevant properties. Specifically, we have intro-duced differential operations into the convolutional processto preemptively capture horizontal, vertical, and central-surrounding directions. Furthermore, we have incorporatedan aggregation (add) operation into the convolution to boostthe networks sensitivity to statistical features. The method is shown in .We initially applied the DecoupleConv (with kernel=4and stride=2) to reduce the spatial resolution while si-multaneously increasing the number of channels. Subse-quently, we employed four decoupled convolutions withreparameterization, which we designed for feature learn-ing. We then utilized pixel shuffle on the features to up-scale the image resolution to its original low resolution (LR)size. Following this, a single decoupled convolu- tion withreparameterization was used for feature map- ping. Finally,another pixel shuffle operation was applied to achieve a 4xsuper-resolution result. Implementation detailsWe utilized solely the DIV2Kdataset and applied the official compression methods tocompress the images at various levels, specifically at 31,39, 47, 55, and 63 compression levels, amounting to a totalof five different degrees of compression.Training: We utilized the Adam optimizer with an ini-tial learning rate of 5e-4, performing a total of 1e7 itera-tions. We employed the stepDecayLR learning rate strat-egy, which involves a decay every 2e6 iterations with a de-cay factor of 2. On each card, we set the batch size to 32,resulting in a cumulative batch size of 32*8 across all cards.The training was conducted over approximately 7 days, dis-tributed across 8 V100 GPUs.Inference: Prior to inference, it is necessary to performan equivalent transformation of the parameters.",
  "China Mobile Research Institute2 Min Zu University of China": "A lightweight Super-resolution Algorithm Based onRe-ParameterizationWe propose an efficient super-resolution network, which contains four convolutions andan unshuffle block. First, the network uses a convolutionaloperation for feature extraction. Then, it utilizes two re-parameterization modules to extract edge and detailed in-formation. The re-parameterization module increases thenumber of parameters during training, but it is replaced bya single convolution to reduce computational complexityand memory usage during testing. The re-parameterizationmodule we use can extract more edge and detailed informa-tion. Subsequently, another convolution operation is usedto increase the number of channels to 48, which facilitatesthe subsequent four-fold super-resolution.",
  ". The framework of RDEN (IVP)": "Finally, we use an unshuffle block to make the channel-to-space transition. The whole network is shown in Fig-ure 17, and the convolutional layers in the middle(red) aretwo re-parameterization modules. The re-parameterizationmodule we used is shown in .The local frequency loss (FFL) based on Fast FourierTransform (FFT) enables the model to dynamically pri-oritize challenging frequency components while diminish-ing the influence of easily synthesizable ones. This opti-mization objective supplements current spatial losses andeffectively guards against the degradation of crucial fre-quency details caused by inherent biases in neural networks.We use the following FFT loss in our training:",
  "LS = XSR4S XHR4S + LGM + LFFT(6)": "Real-Time Super-Resolution with auxiliary lossThenetwork we used is shown in , which containsthree reparameterized modules, and an auxiliary head withupscale factor 2. We use the ECB model as the repa-rameterized module which can achieve competitive perfor-mance without computation overhead. Alongside the 4xsuper-resolution task, we introduce a 2x upsampling headfor the 2x SR task. This additional task offers multiple ben-efits: it functions as a form of simulated annealing, allowingfor potential escape from local minima; it serves as a prior,enhancing the delineation of our primary task. The loss as-sociated with the 2x supervision is represented as follows:",
  "LX2 = XSR2S XHR2S(7)": "where the XSRx2Sdenotes the output from 2x upsamplinghead, and XHRx2Sare corresponding 2x HR image. Notethat we cut the 2x super resolution model off in the testingphase, and the network consists of only five convolutionsand one 4x upsampling head. Jointly supervision knowledge distillation network forefficient super-resolutionWe propose a efficient super-resolution network named KREN based on knowledge dis-tillation and re-parameterization, as shown in .The KREN model is composed of a teacher network andstudent network, we use the superior SR model HAT as teacher network. The distillation training provides ad-ditional effective supervision information for student train-ing, and enhances the performance and generalization abil-ity of student network. The student network is composed",
  "IMDN 23.5080.89458.430154.141707.767RFDN 18.5690.43327.046112.034791.928RLFN 12.0190.31719.67480.045470.753DIP 10.0490.24314.88672.9672497.287": ". The lightweight metrics study by Teams BasicVision,CMVG, IVP. The Time denotes the average inference time. TheParams is the total number of parameters. The FLOPs andActs are calculated on 256x256 images. The GPU Mem rep-resents the GPU memory during the inference. of two convolution layers and two re-parameterization blocks ECB. The ECB block with complex structure is usedin training phase, while it can be merged into a 3*3 con-volution layer for speeding up inference speed during theinference phase. The re-parameterization strategy can ef-fectively improve the feature diversity and boost the fea-ture extraction ability of SR model. In addition, we proposea jointly supervision loss that consists the focal frequencyloss(FFL) , gradient map loss (GM) , distillationloss and L1 loss. We extract features from the 1st and 3rdblocks of the teacher model, and features from each ECBblock to calculate the distillation loss. The constraints ongradients and frequency domain helps super-resolved highquality images.We also propose a multi-stage progressivetraining strategy to gradually improves the reconstructionquality. The number of feature maps in student network isset to 14. ImplementationdetailsWetrainourmodelonDIV2K,Flickr2K and GTA datasets,andutilize multi-stage training based on Pytorch on NVIDIAV100.The patch size in each training stage is selectedfrom .The mini-batch size is set to64, and MSE, GM loss, and FFT loss are used as target loss functions. Each stage except for the first stage isfine-tuned based on the result of the previous stage, trainingfor 500 epochs utilizing the Adam algorithm, beginningwith a learning rate of 5 104 and gradually decreasingto 5 105following the cosine scheduler.For the distillation approach (KREN) the training detailsare described as follows:Stage1. Training teacher network.The teacher network istrained from scratch with teacher loss.Stage2.Training student network.Firstly, we fix theteacher network and pre-train a 2x network to initialize stu-dent network. Then we use the jointly supervision loss totrain student network. The initial learning rate is set to 5e-4 and halved at every 50 epochs and the total number ofepochs is 500. The batch size and patch size are set to 64and 256 separately.Stage3. Fine-tune student network. (1) The student modelis initialized from Stage2 and trained with the same settingsas Stage2, especially the loss function is only MSE loss. (2)The student model is initialized from the previous step andfine-tuned by MSE loss further, it is worth that the patch sizeis set to 512. Other parameter settings are not changed.",
  "MegastudyEdu Vision AI": "We introduce a method that leverages the Efficient Trans-formation and Dual Stream Network (ETDS) conjugatedwith a Feature-Enhanced Module and an Edge-orientedConvolution Block (ECB) .Our model is based on the Efficient Transformationand Dual Stream Network (ETDS) , incorporating aFeature-Enhanced Module inspired by Structure-PreservingSuper Resolution with Gradient Guidance (SPSR) andan Edge-oriented Convolution Block (ECB) proposed inECBSR. This design utilizes the equivalent transforma-tion to convert time-consuming operators into time-friendlyoperations, alongside a dual stream network structure to re-duce redundant parameters.The architecture of ETDS comprises Dual Stream net-work to alleviate redundant parameters, as follows:",
  "Efficiency Optimization Strategies:": "Dual Stream Network Architecture: Utilizing ETDS reduces redundant parameters by separating theprocessing of high-frequency and low-frequency infor-mation. This branch enables more efficient learningand reduces computational overhead. Feature-Enhanced Module with Gradient Guid-ance: We incorporated a Feature-Enhanced Module toleverage gradient information from low-resolution in-puts. This approach effectively restores high-frequencydetails lost during compression and downsampling, en-hancing model performance without significantly in-creasing computational demand.",
  "Korea Electronics Technology Institute (KETI)2 Korea Aerospace University (KAU)": "We propose an Unshuffle, Re-parameterization, andPointwise Network (URPNet) that can achieve higher ac-curacy at a faster speed compared to previous real-time SRmodels for 4K images. We applied a pixel unshuffle to theinput image to reduce the resolution, and applied the 1x1pointwise convolution to only the last layer, instead of ap-plying a re-parameterized convolution (RepConv) to all ex-isting convolutions.We also applied curriculum learning to efficientlylearn lightweight models. Since the larger the Quantiza-",
  ". Proposed distillation loss on the fine-tuning stage ofURPNet, Team 402Lab": "tion Parameter (QP), the larger the compression artifacts,the worse the performance will be if the lightweight modelis trained on high QP data from the beginning. Therefore,we divided the training data into easy (QP 31), medium (QP39, QP 47), and hard (QP 55, QP 63) sets according to thetraining difficulty.Additionally, we applied knowledge distillation (KD)during the fine-tuning stage to achieve higher PSNR thanusing conventional training.To apply KD, the teacher",
  ". DIV2K : We use the DIV2K training dataset (800images) for scratch training step": "2. FTCombined : We use a combined dataset for fine-tuning stage, which includes the DIV2K train set(full 800), Flickr train set (2650 full), DIV8K (first200 samples), and LSDIR (first 1000). Before thetraining phase, the training data is pre-processed bycenter cropping it to a resolution of 2040 x 1080.To generate low-resolution images, we degrade thecenter cropped images with Lanczos downsamplingand AVIF compression. For both training stages, weused random cropping, rotation 90, horizontal flipand vertical flip augmentation.",
  "Training Time: 24 hours with single RTX 3090GPUs Training Strategies:": "1. Scratch train step: In the first step, our model wastrained from scratch. The LR patches were croppedfrom LR images with 8 mini-batch 96x96 sizes. TheAdam optimizer was used with a 0.0005 learningrate during scratch training. The cosine warm-upscheduler was used. The total number of epochs wasset to 500. We use l1 loss. 2. Fine-tuning step: In the second step, the model wasinitialized with the weights trained in the first step.To improve the accuracy, we used l2 and the dis-tillation loss. Fine-tuning with l2 and distillationloss improves the peak signal-to-noise ratio (PSNR)value by 0.02 0.03 dB. In this step, the initiallearning rate was set as 0.0001, and the Adam op-timizer was used along with a cosine warm-up. Thetotal epoch was set to 50 epochs. Eirikur Agustsson and Radu Timofte. Ntire 2017 challengeon single image super-resolution: Dataset and study. In TheIEEE Conference on Computer Vision and Pattern Recogni-tion (CVPR) Workshops, July 2017. 2, 6, 9, 14, 15",
  "Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Ja-son Weston. Curriculum learning. In Proceedings of the 26thannual international conference on machine learning, pages4148, 2009. 16": "Jiahao Chao,Zhou Zhou,Hongfan Gao,Jiali Gong,Zhengfeng Yang, Zhenbing Zeng, and Lydia Dehbi. Equiv-alent transformation and dual stream network constructionfor mobile image super-resolution.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 1410214111, June 2023. 15,16 Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao,and Chao Dong.Activating more pixels in image super-resolution transformer. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages2236722377, 2023. 11, 14",
  "Sung-Jin Cho, Seo-Won Ji, Jun-Pyo Hong, Seung-Won Jung,and Sung-Jea Ko. Rethinking coarse-to-fine approach in sin-gle image deblurring. In ICCV, 2021. 6, 9": "Marcos V Conde, Ui-Jin Choi, Maxime Burchi, and RaduTimofte. Swin2sr: Swinv2 transformer for compressed im-age super-resolution and restoration. In European Confer-ence on Computer Vision, pages 669687. Springer, 2022. 2,9 Marcos V. Conde, Zhijun Lei, Wen Li, Ioannis Katsavouni-dis, Radu Timofte, et al. Real-time 4k super-resolution ofcompressed AVIF images. AIS 2024 challenge survey. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition Workshops, 2024. 3 Marcos V. Conde, Saman Zadtootaghaj, Nabajeet Barman,Radu Timofte, et al. AIS 2024 challenge on video qualityassessment of user-generated content: Methods and results.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition Workshops, 2024. 3 Marcos V Conde, Eduard Zamfir, Radu Timofte, DanielMotilla, Cen Liu, Zexin Zhang, Yunbo Peng, Yue Lin, Ji-aming Guo, Xueyi Zou, et al.Efficient deep models forreal-time 4k image super-resolution. ntire 2023 benchmarkand report.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 14951521, 2023. 2, 3, 7 Weijian Deng, Hongjie Yuan, Lunhui Deng, and Zeng-tong Lu.Reparameterized residual feature network forlightweight image super-resolution.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 17121721, 2023. 4 Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han,Guiguang Ding, and Jian Sun. Repvgg: Making vgg-styleconvnets great again. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages1373313742, 2021. 3",
  "Mark Hamilton, Evan Shelhamer, and William T. Freeman.It is likely that your loss should be a likelihood.CoRR,abs/2007.06059, 2020. 6": "Zibin He, Tao Dai, Jian Lu, Yong Jiang, and Shu-Tao Xia.Fakd: Feature-affinity based knowledge distillation for effi-cient image super-resolution. In 2020 IEEE InternationalConference on Image Processing (ICIP), pages 518522.IEEE, 2020. 17 Zheng Hui, Xinbo Gao, Yunchu Yang, and Xiumei Wang.Lightweight image super-resolution with information multi-distillation network. In ACM International Conference onMultimedia, pages 20242032, 2019. 2, 14 Andrey Ignatov, Radu Timofte, Maurizio Denna, and AbdelYounes. Real-time quantized image super-resolution on mo-bile npus, mobile ai 2021 challenge: Report. In Proceedingsof the IEEE/CVF conference on computer vision and patternrecognition, pages 25252534, 2021. 9 Liming Jiang, Bo Dai, Wayne Wu, and Chen Change Loy.Focal frequency loss for image reconstruction and synthesis.In Proceedings of the IEEE/CVF international conference oncomputer vision, pages 1391913929, 2021. 13, 14 Liming Jiang, Bo Dai, Wayne Wu, and Chen Change Loy.Focal frequency loss for image reconstruction and synthesis.In Proceedings of the IEEE/CVF international conference oncomputer vision, pages 1391913929, 2021. 14 Fangyuan Kong, Mingxi Li, Songwei Liu, Ding Liu, Jing-wen He, Yang Bai, Fangmin Chen, and Lean Fu. Residuallocal feature network for efficient super-resolution. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 766776, 2022. 2, 14 Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-Hsuan Yang. Fast and accurate image super-resolution withdeep laplacian pyramid networks. IEEE transactions on pat-tern analysis and machine intelligence, 41(11):25992613,2018. 9",
  "Yawei Li, Kai Zhang, Luc Van Gool, Radu Timofte, et al.NTIRE 2022 challenge on efficient super-resolution: Meth-ods and results. In CVPR Workshops, 2022. 3": "Yawei Li, Kai Zhang, Radu Timofte, Luc Van Gool,Fangyuan Kong, Mingxi Li, Songwei Liu, Zongcai Du, DingLiu, Chenhui Zhou, et al. Ntire 2022 challenge on efficientsuper-resolution: Methods and results.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 10621102, 2022. 2 Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, LucVan Gool, and Radu Timofte. Swinir: Image restoration us-ing swin transformer. In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision, pages 18331844,2021. 2",
  "Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradientdescent with warm restarts. In ICLR, 2017. 6": "Cheng Ma, Yongming Rao, Yean Cheng, Ce Chen, JiwenLu, and Jie Zhou. Structure-preserving super resolution withgradient guidance. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages77697778, 2020. 13, 14, 15 Cheng Ma, Yongming Rao, Yean Cheng, Ce Chen, JiwenLu, and Jie Zhou. Structure-preserving super resolution withgradient guidance. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages77697778, 2020. 14",
  "Hyeon-Cheol Moon, Jae-Gon Kim, Jinwoo Jeong, and Sung-jei Kim.Feature-domain adaptive contrastive distillationfor efficient single image super-resolution.IEEE Access,11:131885131896, 2023. 17": "Stephan R. Richter, Zeeshan Hayder, and Vladlen Koltun.Playing for benchmarks. In IEEE International Conferenceon Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 22322241, 2017. 14 Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz,Andrew P. Aitken, Rob Bishop, Daniel Rueckert, and ZehanWang. Real-time single image and video super-resolutionusing an efficient sub-pixel convolutional neural network. InCVPR, 2016. 2, 6 Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz,Andrew P Aitken, Rob Bishop, Daniel Rueckert, and ZehanWang. Real-time single image and video super-resolutionusing an efficient sub-pixel convolutional neural network. InProceedings of the IEEE conference on computer vision andpattern recognition, pages 18741883, 2016. 2, 3 Dehua Song, Chang Xu, Xu Jia, Yiyi Chen, Chunjing Xu,and Yunhe Wang. Efficient residual dense block search forimage super-resolution. In Proceedings of the AAAI Con-ference on Artificial Intelligence, volume 34, pages 1200712014, 2020. 2",
  "Long Sun, Jiangxin Dong, Jinhui Tang, and Jinshan Pan.Spatially-adaptive feature modulation for efficient imagesuper-resolution. In ICCV, 2023. 6": "Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, Lei Zhang, Bee Lim, et al. Ntire 2017 chal-lenge on single image super-resolution: Methods and results.In The IEEE Conference on Computer Vision and PatternRecognition (CVPR) Workshops, July 2017. 14 Cheng Wan, Hongyuan Yu, Zhiqi Li, Yihang Chen, Ya-jun Zou, Yuqing Liu, Xuanwu Yin, and Kunlong Zuo.Swift parameter-free attention network for efficient super-resolution. arXiv preprint arXiv:2311.12770, 2023. 11",
  "and pattern recognition, pages 49174926, 2021. 2": "Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu,Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: En-hanced super-resolution generative adversarial networks. InEuropean Conference on Computer Vision Workshops, pages701710, 2018. 2 Zuowen Wang, Chang Gao, Zongwei Wu, Marcos V. Conde,Radu Timofte, Shih-Chii Liu, Qinyu Chen, et al.Event-Based Eye Tracking. AIS 2024 Challenge Survey. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition Workshops, 2024. 3 Ren Yang, Radu Timofte, Xin Li, Qi Zhang, Lin Zhang, Fan-glong Liu, Dongliang He, Fu Li, He Zheng, Weihang Yuan,et al. Aim 2022 challenge on super-resolution of compressedimage and video: Dataset, methods and results. In EuropeanConference on Computer Vision, pages 174202. Springer,2022. 2 KihwanYoon,GanzorigGankhuyag,JinmanPark,Haengseon Son, and Kyoungwon Min.Casr:Efficientcascade network structure with channel aligned method for4k real-time single image super-resolution. In Proceedingsof the IEEE/CVF Conference on Computer Vision andPattern Recognition Workshops, 2024. 7, 8 Lei Yu, Xinpeng Li, Youwei Li, Ting Jiang, Qi Wu, Hao-qiang Fan, and Shuaicheng Liu. Dipnet: Efficiency distil-lation and iterative pruning for image super-resolution. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 16921701, 2023. 14",
  "Eduard Zamfir, Marcos V Conde, and Radu Timofte. To-wards real-time 4k image super-resolution. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, 2023. 2, 3": "Kai Zhang, Martin Danelljan, Yawei Li, Radu Timofte, JieLiu, Jie Tang, Gangshan Wu, Yu Zhu, Xiangyu He, WenjieXu, et al. Aim 2020 challenge on efficient super-resolution:Methods and results. In Computer VisionECCV 2020 Work-shops: Glasgow, UK, August 2328, 2020, Proceedings, PartIII 16, pages 540, 2020. 2 Xindong Zhang, Hui Zeng, and Lei Zhang. Edge-orientedconvolution block for real-time super resolution on mobiledevices. In Proceedings of the 29th ACM International Con-ference on Multimedia, pages 40344043, 2021. 4, 14, 15 Xindong Zhang, Hui Zeng, and Lei Zhang. Edge-orientedconvolution block for real-time super resolution on mobiledevices. In Heng Tao Shen, Yueting Zhuang, John R. Smith,Yang Yang, Pablo Cesar, Florian Metze, and BalakrishnanPrabhakaran, editors, MM 21: ACM Multimedia Confer-ence, Virtual Event, China, October 20 - 24, 2021, pages40344043. ACM, 2021. 9, 14 Hengyuan Zhao, Xiangtao Kong, Jingwen He, Yu Qiao, andChao Dong. Efficient image super-resolution using pixel at-tention. In Computer VisionECCV 2020 Workshops: Glas-gow, UK, August 2328, 2020, Proceedings, Part III 16,pages 5672. Springer, 2020. 2"
}