{
  "Abstract": "Cone beam computed tomography (CBCT) is an im-portant imaging technology widely used in medical sce-narios, such as diagnosis and preoperative planning.Using fewer projection views to reconstruct CT, alsoknown as sparse-view reconstruction, can reduce ioniz-ing radiation and further benefit interventional radiol-ogy. Compared with sparse-view reconstruction for tradi-tional parallel/fan-beam CT, CBCT reconstruction is morechallenging due to the increased dimensionality causedby the measurement process based on cone-shaped X-raybeams. As a 2D-to-3D reconstruction problem, althoughimplicit neural representations have been introduced toenable efficient training, only local features are consid-ered and different views are processed equally in pre-vious works, resulting in spatial inconsistency and poorperformance on complicated anatomies.To this end,we propose C2RV by leveraging explicit multi-scale vol-umetric representations to enable cross-regional learn-ing in the 3D space.Additionally, the scale-view cross-attention module is introduced to adaptively aggregatemulti-scale and multi-view features. Extensive experimentsdemonstrate that our C2RV achieves consistent and signif-icant improvement over previous state-of-the-art methodson datasets with diverse anatomy.Code is available at",
  "(b) Cross-Regional & View": ". (a) Cone-shaped X-ray beams are emitted from the scan-ning source and a 2D array of detectors measures the transmittedradiation. (b) Cross-regional (red) and cross-view (green) featurelearning to enhance point-wise representation. quality CT scan involving high radiation doses from X-rays.However, high radiation dose exposure to patients can bea concern in clinical practice, limiting its use in scenarioslike interventional radiology. Hence, reducing the numberof projections can be one of the ways to reduce the radiationdoses, which is also known as sparse-view reconstruction.Over the past decades, there have been many researchworks studying the sparse-view problem for conventionalCT by formulating the reconstruction as a mapping from 1Dprojections to a 2D CT slice, where generation-based tech-niques are proposed to oper-ate on the image or projection domains. However, the mea-surements of cone-beam CT are 2D projections (a),resulting in increased dimensionality compared with con-ventional CT. This means that extending previous conven-tional CT reconstruction methods to CBCT will encounterissues such as high computational cost.Recently, implicit neural representations (INRs) havebeen widely used in 3D reconstruction, including novelview synthesis and object reconstruction. To handle sparse-view or even single-view scenarios, geometric priors (e.g.,surface points and normals ) or parametric shapemodels (e.g., SMPL and SMPL-X )are incorporated to improve the robustness and generaliza-tion ability. However, unlike visible light, X-rays have a",
  "arXiv:2406.03902v1 [eess.IV] 6 Jun 2024": "higher frequency and pass through the surfaces of many ma-terials, hence, no depth or surface information can be mea-sured in the projection. Additionally, it is difficult to builda CT-specific parametric model as the internal anatomies ofthe human body are more complicated than surface models.Although INRs have been introduced to CBCT recon-struction in recent years, tens of views (i.e., 20-50) arestill required for self-supervised NeRF-based methods due to the lack of prior knowledge. On the otherhand, current data-driven methods like DIF-Net maysuffer from poor performance when the anatomy has com-plicated structures for two possible reasons: 1.) local fea-tures queried from projections can be difficult to identifydifferent organs that have low contrast in the projection; 2.)projections of different views are processed equally, whilesome views indeed present more information of specific or-gans than other views.For example, the right-left viewshows the patella clearly, while it overlaps the femur in theanterior-posterior view; see .To address the limitations of previous works, we proposea novel sparse-view CBCT reconstruction framework C2RVby leveraging cross-regional and cross-view feature learn-ing to enhance point-wise representation (b). To bemore specific, we first introduce multi-scale 3D volumetricrepresentations (MS-3DV), where features are obtained byback-projecting multi-view features at different scales to the3D space. Explicit MS-3DV enables cross-regional learningin 3D space, providing richer information that helps betteridentify different organs. Hence, the feature of a point canbe queried in a hybrid way, i.e., multi-scale voxel-alignedfeatures from MS-3DV and multi-view pixel-aligned fea-tures from projections. Instead of considering queried fea-tures equally, scale-view cross-attention (SVC-Att) is thenproposed to adaptively learn aggregation weights by self-attention and cross-attention. Finally, multi-scale and multi-view features are aggregated to estimate the attenuation co-efficient. We evaluate C2RV quantitatively and qualitativelyon two CT datasets (i.e., chest and knee). Extensive ex-periments demonstrate that our proposed C2RV consistentlyoutperforms previous state-of-the-art methods by a consid-erable margin under different experimental settings.The main contributions of this work are summarized as:",
  ". Sparse-View CT Reconstruction": "Traditional parallel/fan-beam CT reconstruction can be re-garded as reconstructing a 2D CT slice from 1D projec-tions.Existing learning-based methods mainly includeimage-domain, projection-domain, and dual-domain meth-ods. Specifically, image-domain methods apply filtered back projection (FBP) to reconstruct acoarse CT slice with streak artifacts and utilize CNNs, suchas U-Net and DenseNet , to denoise and refine de-tails. When extending these methods to CBCT reconstruc-tion, the network should be modified to 3D CNNs, resultingin a substantial increase in computational cost. Another wayis to adopt these methods for slice-wise (2D) denoising ,while the 3D spatial consistency cannot be guaranteed.Projection-domain methods directly operate on sparse-view 1D projections by mapping the projections to the CTslice or recovering the full-view projections . Addi-tionally, Song et al. utilize score-based generative mod-els and propose a sampling method to reconstruct an imageconsistent with both the measurement process and the ob-served measurements (i.e., projections). Chung et al. further incorporate 2D diffusion models into iterative recon-struction. Dual-domain methods operate on both projectionand image domains by combining the denoising processesof two domains or modeling dual-domain consis-tency . However, projection-based operations cannot beextended to CBCT reconstruction as the measurement pro-cesses (cone-beam vs. parallel/fan-beam) are different.",
  "Different from traditional parallel/fan-beam CT, the mea-surement of cone-beam CT is a 2D projection, which meansthe reconstruction should be formulated as reconstructing a": "3D CT volume from multiple 2D projections. Conventionalfiltered back-projection (FDK ) and ART-based iterativemethods often suffer from heavy streaking arti-facts and poor image quality when the number of projec-tions is dramatically decreased. Recently, learning-basedapproaches are proposed for single/orthogonal-view CBCTreconstruction , while these methods arespecially designed for single/orthogonal-view reconstruc-tion or patient-specific data , making themdifficult to extend to general sparse-view reconstruction.On the other hand, implicit neural representations [21, 26] have been introduced to represent CBCT as an attenu-ation or intensity field. Self-supervised meth-ods, including NAF and NeRP , simulate the mea-surement process and minimize the error between real andsynthesized projections. However, these methods require along time for per-sample optimization and are only suitablefor the reconstruction from tens of views (i.e., 20-50) dueto the lack of prior knowledge. DIF-Net , as a data-driven method, formulates the problem as learning a map-ping from sparse projections to the intensity field. Never-theless, DIF-Net regards different projections equally, andonly local semantic features are queried for each sampledpoint, leading to limited reconstruction quality when pro-cessing anatomies with complicated structures (e.g., chest).",
  ". Sparse-View 3D Reconstruction": "In 3D computer vision, implicit representations have beenwidely used in novel-view synthesis and ob-ject reconstruction . For novel viewsynthesis, to extend NeRF to sparse-view scenarios,geometric priors like surface points and normals are incorporated to improve the generalization ability andefficiency. For object reconstruction, particularly digital hu-man reconstruction, previous works lever-age explicit parametric SMPL(-X) models to con-strain surface reconstruction and improve the robustness.However, there is no available depth or surface informa-tion in the attenuation fields of CBCT since X-rays pene-trate right through many common materials, such as flesh.SMPL(-X) are 3D parametric shape models specially de-signed for the surface of the human body, while the in-ternal anatomy structures are too complicated to design aCT-specific parametric model. Therefore, parametric shapemodels cannot be used in sparse-view CBCT reconstruc-tion. Furthermore, cross-view relationships are rarely con-sidered in surface-based reconstruction since one or twoviews are more practical and often sufficient to learn thesparse field with the above-mentioned priors.",
  ". Revisit DIF-Net": "We follow previous works to formulate the CT im-age as a continuous implicit function g: R3 R, which de-fines the attenuation coefficient (same as intensity in )v R of a point p R3 in the 3D space, i.e., v = g(p).Hence, given N-view projections I = {I1, . . . , IN} RW H (W and H are width and height) with known scan-ning parameters (e.g., viewing angles, distance of sourceto origin) during the measurement process, the reconstruc-tion problem is formulated as a conditioned implicit func-tion g() such that v = g(I, p).In practice, a 2D encoder-decoder (shared across dif-ferent views) is used to extract multi-view feature mapsF = {F1, . . . , FN} RC(W H) from N-view projec-tions I, where C is the output channel size of the decoder.For ith view, denote the projection function as i: R3 R2,which maps a 3D point p to the 2D plane where detectorsare located such that pi = i(p). Then, we define the view-specific pixel-aligned features of p in ith view as",
  "v = g(I, p) = F(p),(2)": "where () is the aggregation function implemented withMLPs (or Max-Pooling + MLPs) in DIF-Net .Al-though the above formulation and implementation enableefficient training for high-resolution sparse-view recon-struction, only local pixel-aligned features queried fromprojections are considered and different views are pro-cessed equally, leading to poor performance on complicatedanatomies; see analysis in Sec. 1 & 2.2 and results in Ta-ble 1. To this end, we propose C2RV and will introduce itin detail in the following section.",
  "Eqn. 9": ". The overview of the proposed sparse-view reconstruction framework C2RV. Given multi-view projections, a 2D encoder-decoderis applied to extract view-wise feature map Fi for querying the pixel-aligned feature Fi(p). Additionally, the output feature map F 1 ofthe encoder is downsampled to obtain multi-scale feature maps. At each scale s, multi-view features are back-projected to the 3D spaceand gathered to form the 3D volumetric representation F s for querying the voxel-aligned feature F s(p). Finally, multi-scale voxel-alignedfeatures and multi-view pixel-aligned features are aggregated via scale-view cross-attention modules to estimate the attenuation coefficient. multi-view feature maps at different scales to the 3D space.Hence, multi-scale voxel-aligned features and multi-viewpixel-aligned features are adaptively aggregated by scale-view cross-attention (SVC-Att) modules to estimate the at-tenuation coefficient. Low-Resolution 3D Volumetric Representation. A 3Dvolumetric space S R3(rrr) is defined by voxeliz-ing the 3D space with a low resolution r 16. Let Fi Rcwh be the intermediate feature map of the encoder-decoder given the projection of ith view. The volumetricfeature space F Rc(rrr) defined over S is producedby back-projecting multi-view feature maps into S, i.e.,",
  "F = ( F).(5)": "MS-3DV: Multi-Scale 3D Volumetric Representations.To further improve the robustness of reconstructing differ-ent anatomical structures, we propose to leverage multi-scale 3D volumetric representations. To be specific, given the projection of ii view, denote the output feature mapof the encoder as F 1i , then a sequence of downsam-pling operators are applied to produce multi-scale fea-ture maps {F 1i , . . . , F Si }, where F si= s1(F s1i) fors {2, . . . , S}, and S is the total number of scales. Then,we define multi-scale 3D voxelized space {S1, , SS}with different resolutions {r1, . . . , rS}, and back-project(Eqn. 3 and 5) multi-view feature maps of each scale to ob-tain multi-scale 3D volumetric representations (MS-3DV){ F1, . . . , FS}, where",
  "and Mq RCrCd, Mk, Mv RCsCd. Self-attention(S-Att) can be regarded as a special case of cross-attentionwhere we let Fs = Fr.For a point p, let F(p) =F1(p), . . . , FN(p) RC": "denote multi-view pixel-aligned features queried from pro-jections (Eqn. 1), and F(p) RC indicate the multi-scalevoxel-aligned features queried from MS-3DV (Eqn. 7). Thescale-view cross-attention (SVC-Att) module is proposedto adaptively aggregate the above features. As shown in, a self-attention module is first applied to conductcross-view attention on multi-view features F(p). Then, across-attention attention module takes multi-scale featuresas the reference and the output of the self-attention moduleas the source to conduct attention between multi-scale andmulti-view features. To formulate,",
  "v(p) v(p)2.(12)": "During each training iteration, we randomly sample10,000 points from P for loss calculation (Eqn. 12) to re-duce the memory requirements for efficient network opti-mization. During the inference, the 3D space is voxelizedwith a specified resolution (e.g., 2563), where the attenua-tion coefficient of a voxel is defined as the estimated attenu-ation coefficient of its centroid point by C2RV. This meansthat the resolution can be chosen based on the desired trade-off between image quality and reconstruction speed.",
  "H": "16. () in Eqn. 5 is implemented with 3-layer 3Dresidual convolution that maps the channel size of F to C.For the aggregation method, M = 3 SVC-Att modules arestacked, and attention modules are implemented as multi-head attention with 8 heads. During training, the learnableparameters of C2RV are optimized using stochastic gradi-ent descent (SGD) with a momentum of 0.98 and an initiallearning rate of 0.01. We train C2RV with 400 epochs anda batch size of 4. The learning rate is decreased by a factorof (103)1/400 per epoch.",
  ". Experimental Setting": "Dataset. Experiments are conducted on two CT datasets,including a public chest CT dataset (LUNA16 ) and aprivate knee CBCT dataset collected by Lin et al. (ad-ditional experiments on a dental CBCT dataset are providedin the supplementary). Specifically, LUNA16 is com-posed of 888 chest CT scans with resolution ranging from145145108 to 375375509 mm3, split into 738 fortraining, 50 for validation, and 100 for testing; the knee . Comparison1 of different methods on two CT datasets (i.e., chest and knee) with various numbers of projection views. Theresolution of the reconstructed CT is 2563. The reconstruction results are evaluated with PSNR (dB) and SSIM (10-2), where higherPSNR/SSIM indicate better performance. The best values are bolded and the second-best values are underlined.",
  "C2RV (ours)29.23|87.4729.95|88.4630.70|89.1629.73|88.8730.68|89.9631.55|90.83": "1Compared with our camera-ready version, we have made the following updates: 1.) further tuned the hyperparameters for FDK and SARTto improve their performance; 2.) re-evaluated SSIM by setting data range=1 in skimage.metrics.structural similarity(in default, data range=2 for floating point inputs). For more detailed information about the evaluation, please visit our GitHubrepository. The new results are also updated in subsequent tables.",
  ". Visualization of 6-view reconstructed chest CT. From top to bottom: axial, coronal, and sagittal slices": "dataset contains 614 knee CBCT scans with resolutionsranging from 236236167 to 500500416 mm3, splitinto 464 for training, 50 for validation, and 100 for test-ing. We follow the data preprocessing of to resampleand crop (or pad) each CT to have isotropic spacing (i.e.,1.6 mm for chest and 0.8 mm for knee) and size of 2563.Multi-view 2D projections are simulated by DRRs with aresolution of 2562, and the viewing angles are uniformlyselected in the range of 180 (half rotation). Evaluation Metrics. Following previous works , two quantitative metrics, including peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) ,are used to evaluate the reconstruction performance, wherehigher values indicate superior image quality.",
  ". Results": "Quantitative Evaluation.We compare our C2RV withself-supervised methods, including FDK , SART ,NAF , and NeRP , without requiring additionaltraining data.We also compare data-driven approaches,including 2D denoising-based (i.e., FBPConvNet ,FreeSeed , and BBDM ) and implicit neural rep-resentation (INR)-based (i.e., PixelNeRF and DIF-Net ) methods. We conduct experiments with differ-ent numbers of projection views (i.e., 6-10) and the recon-struction resolution is 2563. The results are shown in Ta-ble 1. Although DIF-Net can achieve satisfactory per-formance on knee CT, the performance drops dramaticallywhen adapting to more complicated anatomical structures",
  "-View": ". Visualization of examples reconstructed from differentnumbers of projection views, i.e., 6, 8, and 10. The highlightedregions (red) are zoomed in, showing richer details in our recon-structed results than in other methods. (e.g., chest), while our C2RV consistently performs well ondifferent datasets. Additionally, when reconstructing from6, 8, and 10 views, our C2RV outperforms previous state-of-the-art by a remarkable margin, i.e., 3.6/14.3, 3.1/11.5, and3.5/10.6 PSNR/SSIM (dB/10-2) on chest CT; and 2.6/8.1,2.4/7.9, and 2.2/5.9 on knee CT. More importantly, Evenwith only 6 views, C2RV can reconstruct CT of better qual-ity than other methods with 4 more views (i.e., 10 views). Visual Comparison. Examples of 6-view reconstructionare visualized in for qualitative comparison. Due tothe lack of sufficient projection views, reconstruction resultsof FDK are full of streaking artifacts, and NeRP can only reconstruct satisfactory contours of the body andlung. For FBPConvNet and FreeSeed , jitters ap-pear near the boundary of the body and lung since they are2D methods that reconstruct CT slice by slice. For Pixel-NeRF and DIF-Net , although the details are re-constructed better than others, there are still a few streakingartifacts and unclear contours. The reconstructed results ofC2RV have clearer shape contours, better internal details,and almost no streaking artifacts. Furthermore, shows the visualization of results reconstructed from differ-ent numbers of projection views, demonstrating a consistentconclusion with the above. Downstream Evaluation. In addition to quantitative andqualitative evaluation, we validate the reconstructed CT onthe downstream task, i.e., segmentation. Specifically, weutilize LungMask toolkit to conduct left/right-lung seg-mentation on CT reconstructed by different methods. As theresults are shown in and , compared with . Lung segmentation of 6-view reconstructed chest CT.Dice coefficient (%, higher is better) and average surface distance(ASD, mm, lower is better) are evaluated. The best values arebolded and the second-best values are underlined.",
  ". Ablation Study": "Ablation studies are conducted to explore the effectivenessof the proposed MS-3DV and SVC-Att, and different de-signs for MS-3DV. Moreover, we further analyze the robust-ness of our C2RV to varying viewing angles and noisy scan-ning parameters. All the following ablative experiments areconducted on 6-view reconstruction of chest CT with theresolution of 2563.",
  ". Proposed MS-3DV and SVC-Att": "Ablation on MS-3DV and SVC-Att.We regard DIF-Net as the baseline model and compare the reconstruc-tion performance of introducing MS-3DV and SVC-Att. InDIF-Net, multi-view features are aggregated ( in Eqn. 2)with MLPs or Max-Pooling + MLPs. Comparison is shownin . In (+MS-3DV), multi-scale voxel-aligned fea-tures are concatenated with max-pooled multi-scale fea-tures. In (+SVC), we randomly initialize a learnable vec- . Ablation study on different aggregation methods (M.:MLPs , Max-M.: Max-Pooling + MLPs , SVC: our pro-posed scale-view cross-attention) and multi-scale 3D volumetricrepresentations (MS-3DV). PSNR (dB) and SSIM (10-2) areevaluated on 6-view reconstruction of chest CT.",
  "C2RV (ours)29.2387.47": "tor before training, as an alternative to the reference feature(i.e., F(p) in Eqn. 9); also see . Both MS-3DV andSVC-Att can improve the reconstruction performance, andthe framework achieves new state-of-the-art performanceby jointly incorporating the above two. Different Designs for MS-3DV. As shown in , wecompare the performance of using different numbers ofscales, and selections of initial feature map F 1 and reso-lution r1. It is important to incorporate multi-scale features,which provide richer information than single-scale for iden-tifying different anatomies, such as organs (e.g., lung) andbones (e.g., spine). We do not further increase the numberof scales (e.g., 4) since the size of the feature map at thethird scale is too small (i.e., 44). For the choice of F 1, theoutput of the encoder is better as it contains more high-levelfeatures than the decoder. Empirically, the initial resolutionof 16 is the best choice for the trade-off between the global(high-level) and local (details) features.",
  ". Robustness Analysis": "Let A = {1, . . . , N} denote the viewing angles in theorigional evaluation.The first experiment is conductedby choosing different viewing angles, i.e., A = {i + | i A}, where is the angle offset. As shown in, the performance of C2RV is stable with varying an-gles. The second study is about the noisy scanning param-eters. Taking the viewing angles as an example, we assumethe measurement process is noisy, which means that multi-view projections are measured from A = {i + i | i A}, where i is the noise that obeys the uniform distributionU(, +). In this case, the projection function is stilldefined based on original viewing angles, i.e., A, since thenoise is unobservable. In , we consider two scanningparameters, including the viewing angle, and the distance ofsource to origin, which are major factors related to the for-mulation of the projection function (see Appendix in ).Experiments show that our C2RV is robust to slight shifts inscanning parameters. . Ablation study on the number of scales, the initial featuremap F 1, and the initial resolution r1. The selection of F 1 can bethe final-layer feature map of the encoder or decoder. PSNR andSSIM are evaluated on 6-view reconstruction of chest CT.",
  ". Conclusion": "In this work, we propose a novel framework, namely C2RV,for sparse-view cone-beam CT reconstruction. The novel-ties are mainly composed of 1.) multi-scale 3D volumetricrepresentations (MS-3DV) to enable efficient cross-regionalfeature learning in the 3D space, and 2.) scale-view cross-attention (SVC-Att) to adaptively aggregate multi-scale andmulti-view features. Our C2RV shows superior reconstruc-tion performance compared with previous state-of-the-art,the practical potential of reconstructed CT in downstreamapplications, and robustness to slightly noisy measurementprocesses. Although our C2RV performs well in a specificdataset, it will fail when adapting to other datasets with un-seen anatomies (e.g., chesthead) as C2RV only learns thedataset-specific distribution priors. Hence, it would also beimportant to improve the few-shot or even zero-shot adapta-tion ability by introducing new training schemes or networkframeworks, which will be left as our future works. Acknowledgements. This work is partially supported by aresearch grant from the National Natural Science Founda-tion of China under Grant 62306254 and a grant from theHong Kong Innovation and Technology Fund under GrantITS/030/21.",
  "Anders H Andersen and Avinash C Kak. Simultaneous alge-braic reconstruction technique (sart): a superior implemen-tation of the art algorithm. Ultrasonic imaging, 6(1):8194,1984. 3, 6": "Hyungjin Chung, Dohoon Ryu, Michael T McCann, Marc LKlasky, and Jong Chul Ye.Solving 3d inverse problemsusing pre-trained 2d diffusion models.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 2254222551, 2023. 2 Yu Fang, Lanzhuju Mei, Changjian Li, Yuan Liu, WenpingWang, Zhiming Cui, and Dinggang Shen.Snaf: Sparse-view cbct reconstruction with neural attenuation fields. arXivpreprint arXiv:2211.17048, 2022. 2, 3",
  "Ji He, Yongbo Wang, and Jianhua Ma. Radon inversion viadeep learning. IEEE transactions on medical imaging, 39(6):20762087, 2020. 1, 2": "Johannes Hofmanninger, Forian Prayer, Jeanny Pan, Sebas-tian Rohrich, Helmut Prosch, and Georg Langs. Automaticlung segmentation in routine imaging is primarily a data di-versity problem, not a methodology problem. European Ra-diology Experimental, 4(1):113, 2020. 7 Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-ian Q Weinberger.Densely connected convolutional net-works. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 47004708, 2017. 2",
  "International Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 15. IEEE, 2023. 3": "Anish Lahiri, Marc Klasky, Jeffrey A Fessler, and SaiprasadRavishankar. Sparse-view cone beam ct reconstruction us-ing data-consistent supervised and adversarial learning fromscarce training data. arXiv preprint arXiv:2201.09318, 2022.2 Bo Li, Kaitao Xue, Bin Liu, and Yu-Kun Lai. Bbdm: Image-to-image translation with brownian bridge diffusion models.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 19521961, 2023. 6 Wei-An Lin, Haofu Liao, Cheng Peng, Xiaohang Sun, Jing-dan Zhang, Jiebo Luo, Rama Chellappa, and Shaohua KevinZhou.Dudonet: Dual domain network for ct metal arti-fact reduction. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1051210521, 2019. 2 Yiqun Lin, Zhongjin Luo, Wei Zhao, and Xiaomeng Li.Learning deep intensity field for extremely sparse-view cbctreconstruction. In Medical Image Computing and ComputerAssisted Intervention MICCAI 2023, pages 1323, Cham,2023. Springer Nature Switzerland. 1, 2, 3, 5, 6, 7, 8",
  "Matthew Loper, Naureen Mahmood, Javier Romero, GerardPons-Moll, and Michael J Black. Smpl: A skinned multi-person linear model. ACM Transactions on Graphics, 34(6),2015. 1, 3": "Chenglong Ma, Zilong Li, Junping Zhang, Yi Zhang, andHongming Shan. Freeseed: Frequency-band-aware and self-guided network for sparse-view ct reconstruction.In In-ternational Conference on Medical Image Computing andComputer-Assisted Intervention, pages 250259. Springer,2023. 1, 2, 6, 7 Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:Representing scenes as neural radiance fields for view syn-thesis. Communications of the ACM, 65(1):99106, 2021.3",
  "Jinxiao Pan, Tie Zhou, Yan Han, and Ming Jiang. Variableweighted ordered subset image reconstruction algorithm. In-ternational Journal of Biomedical Imaging, 2006, 2006. 3": "Jeong Joon Park, Peter Florence, Julian Straub, RichardNewcombe, and Steven Lovegrove. Deepsdf: Learning con-tinuous signed distance functions for shape representation.In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 165174, 2019. 3 Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, andMichael J Black.Expressive body capture:3d hands,face, and body from a single image.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1097510985, 2019. 1, 3 Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmen-tation. In Medical Image Computing and Computer-AssistedInterventionMICCAI 2015: 18th International Conference,Munich, Germany, October 5-9, 2015, Proceedings, Part III18, pages 234241. Springer, 2015. 2, 5",
  "William C Scarfe, Allan G Farman, Predag Sukovic, et al.Clinical applications of cone-beam computed tomography indental practice. Journal-Canadian Dental Association, 72(1):75, 2006. 1": "Arnaud Arindra Adiyoso Setio, Alberto Traverso, ThomasDe Bel, Moira SN Berens, Cas Van Den Bogaard, PiergiorgioCerello, Hao Chen, Qi Dou, Maria Evelina Fantacci, BramGeurts, et al. Validation, comparison, and combination ofalgorithms for automatic detection of pulmonary nodules incomputed tomography images: the luna16 challenge. Medi-cal image analysis, 42:113, 2017. 5, 6 Liyue Shen, Wei Zhao, and Lei Xing. Patient-specific recon-struction of volumetric computed tomography images from asingle projection view via deep learning. Nature biomedicalengineering, 3(11):880888, 2019. 3 Liyue Shen, John Pauly, and Lei Xing. Nerp: implicit neu-ral representation learning with prior embedding for sparselysampled image reconstruction. IEEE Transactions on NeuralNetworks and Learning Systems, 2022. 2, 3, 6, 7",
  "Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solv-ing inverse problems in medical imaging with score-basedgenerative models. arXiv preprint arXiv:2111.08005, 2021.2": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in neuralinformation processing systems, 30, 2017. 4 Ce Wang, Kun Shang, Haimiao Zhang, Qian Li, Yuan Hui,and S Kevin Zhou. Dudotrans: dual-domain transformer pro-vides more attention for sinogram restoration in sparse-viewct reconstruction. arXiv preprint arXiv:2111.10790, 2021. 2 Jianing Wang, Yiyuan Zhao, Jack H Noble, and Benoit MDawant.Conditional generative adversarial networks formetal artifact reduction in ct images of the ear. In Medi-cal Image Computing and Computer Assisted InterventionMICCAI 2018: 21st International Conference, Granada,Spain, September 16-20, 2018, Proceedings, Part I, pages311. Springer, 2018. 1, 2",
  "mals.In 2022 IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition (CVPR), pages 1328613296.IEEE, 2022. 1, 3": "Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, andMichael J Black. Econ: Explicit clothed humans optimizedvia normal integration.In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 512523, 2023. 1, 3 Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, ZhixinShu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf:Point-based neural radiance fields.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 54385448, 2022. 1, 3 Fukun Yin, Wen Liu, Zilong Huang, Pei Cheng, Tao Chen,and Gang Yu.Coordinates are not lonely-codebook priorhelps implicit neural 3d representations. Advances in NeuralInformation Processing Systems, 35:1270512717, 2022. 1,3 Xingde Ying, Heng Guo, Kai Ma, Jian Wu, Zhengxin Weng,and Yefeng Zheng. X2ct-gan: reconstructing ct from bipla-nar x-rays with generative adversarial networks. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, pages 1061910628, 2019. 3 Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.pixelnerf: Neural radiance fields from one or few images.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 45784587, 2021. 3,6, 7 Ruyi Zha, Yanhao Zhang, and Hongdong Li. Naf: Neural at-tenuation fields for sparse-view cbct reconstruction. In Med-ical Image Computing and Computer Assisted InterventionMICCAI 2022: 25th International Conference, Singapore,September 1822, 2022, Proceedings, Part VI, pages 442452. Springer, 2022. 2, 3, 6 Zhicheng Zhang, Xiaokun Liang, Xu Dong, Yaoqin Xie, andGuohua Cao. A sparse-view ct reconstruction method basedon combination of densenet and deconvolution. IEEE trans-actions on medical imaging, 37(6):14071417, 2018. 1, 2 Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai.Pamir: Parametric model-conditioned implicit representa-tion for image-based human reconstruction. IEEE transac-tions on pattern analysis and machine intelligence, 44(6):31703184, 2021. 1, 3"
}