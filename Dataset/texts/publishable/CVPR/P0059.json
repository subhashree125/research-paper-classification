{
  "Robust Adaptation of Foundation Models withBlack-Box Visual Prompting": "Changdae Oh, Student Member, IEEE, Gyeongdeok Seo, Student Member, IEEE,Geunyoung Jung, Student Member, IEEE, Zhi-Qi Cheng, Member, IEEE, Hosik Choi, Member, IEEE,Jiyoung Jung, Member, IEEE, and Kyungwoo Song, Member, IEEE AbstractWith the surge of large-scale pre-trained models(PTMs), adapting these models to numerous downstream tasksbecomes a crucial problem. Consequently, parameter-efficienttransfer learning (PETL) of large models has grasped hugeattention. While PETL methods show impressive performance,they commonly rely on two optimistic assumptions: 1) the entireparameters of a PTM are available, and 2) a sufficiently largememory capacity is equipped for caching all the intermediateactivations to compute gradients. However, in most real-worldapplications, PTMs are served as black-box APIs or propri-etary software without explicit parameter accessibility. Besides,it is hard to meet a large memory requirement for modernPTMs. This work proposes black-box visual prompting (Black-VIP), which efficiently adapts the PTMs without knowledgeabout model architectures and parameters. BlackVIP has twocomponents; 1) Coordinator and 2) simultaneous perturbationstochastic approximation with gradient correction (SPSA-GC).The Coordinator designs input-dependent visual prompts, whichallow the target PTM to adapt in the wild. SPSA-GC efficientlyestimates the gradient of PTM to update the Coordinator.Besides, we propose a variant, BlackVIP-SE, which significantlyreduces the runtime and computational cost of BlackVIP. Ex-tensive experiments on 19 datasets demonstrate that BlackVIPsenable robust adaptation to diverse domains and tasks withminimal memory requirements. We further provide theoreticalanalysis on the generalization of visual prompting methodsby presenting their connection to the certified robustness ofrandomized smoothing.",
  "Index TermsTransfer Learning, Black-Box Optimization,Parameter-Efficient Fine-Tuning, Visual Prompting": "I. INTRODUCTIONBASED on their excellent transferability, large-scale pre-trained models (PTMs), a.k.a. foundation models , ,have shown remarkable success on tasks from diverse domainsand absorbed increasing attention in machine learning com-munities. By witnessing PTMs success, Parameter-EfficientTransfer Learning (PETL) methods that efficiently utilize thePTMs are recently emerging. While the standard fine-tuning(FT) updates the entire or large portion of parameters from a C. Oh is with Department of Computer Sciences, University of Wisconsin-Madison, Madison, WI 53706, USA. E-mail: . Jung and J. Jung are with Department of Artificial Intelligenceand H. Choi is with Department of Urban Big Data Convergence, Uni-versity of Seoul, Seoul, Dongdaemun-gu 02504, South Korea. E-mail:{gyjung975,jyjung,choi.hosik}@uos.ac.kr.G. Seo and K. Song are with Department of Statistics and Data Sci-ence, Yonsei University, Seoul, Seodaemun-gu 03722, South Korea. E-mail:{gd.seo,kyungwoo.song}@yonsei.ac.krZ. Cheng is with Language Technologies Institution, Carnegie MellonUniversity, Pittsburgh, PA 15289, USA. E-mail: .While FT updates the entire model, VP adapts a few parameters (thatof visual prompt) in the input pixel space. However, VP still requires a largememory capacity to optimize the prompt via backpropagation. Moreover, FTand VP are only feasible if the PTMs parameters are accessible to the public.Meanwhile, BlackVIP does not assume such accessibility by adopting a black-box optimization (SPSA-GC) rather than backpropagating a true gradient.Besides, BlackVIP reparameterizes the prompt with a neural network forinput-dependent prompt generation. Based on these, BlackVIP can be widelyadopted for transfer learning in the wild. PTM, PETL methods aim to achieve comparable performanceto FT by adapting a tiny amount of learnable parameters.Among them, prompt-based approaches have beenwidely investigated on diverse research areas. For PTMs in thevision domain, Visual Prompt Tuning is a representativeexample that injects a few learnable prompt tokens insideof Vision Transformers layers and only optimizes them.Besides, text encoder-side prompt learning methods for thevision-language models are also actively studied .Different from the aforementioned approaches, Bahng et al. initiate an investigation of visual prompting (VP), whichattempts to learn the parameters on input pixel space as avisual prompt to steer the target PTMs behavior while thePTM itself is totally intact. While existing PETL methodsshow impressive performance with few learnable parameters,they rely on two optimistic assumptions. First, they assumethat the full parameters of the PTM are accessible. Second,they require a large memory capacity to cache all the inter-mediate activations for the gradient computation. However,PTMs in real-world applications are commonly served asblack-box APIs and proprietary software, and they do notreveal the implementation-level information or full parametersdue to commercial issues, e.g., violating model ownership.As a result, exploiting high-performing PTMs to specificdownstream tasks not only in the white-box setting but also",
  "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 20212": "in the black-box setting (where we have limited accessibilityto the models detail) is a crucial but unexplored problem.Furthermore, while PETL approaches have few learnable pa-rameters, they require a large amount of memory for cachingthe intermediate activations to backpropagate the gradient.Therefore, users who want to adopt a large-scale PTM shouldretain sufficient memory capacity despite the tiny amount oflearnable parameters. Besides, if the users entrust PTM fine-tuning to the model owner by sending their personal data,privacy concerns will inevitably arise .To alleviate the above unrealistic assumptions, we pioneerblack-box visual prompting (BlackVIP) approach, whichenables the parameter-efficient transfer learning of pre-trainedblack-box vision models from the low-resource user perspec-tive (illustrated in ). BlackVIP works based on thefollowing two core components: 1) input-dependent visualprompting and 2) a stable zeroth-order optimization algorithm.First, we enhance an input image by attaching a learnablevisual prompt per pixel. It is noted that input space promptingdoes not require the accessibility on parts of architecture , or the embedding layer of PTM. While theprevious work manually determines a structure of the visualprompt, e.g., a small fraction of the fixed area in the image,we reparameterize the visual prompt with a small promptgeneration network, Coordinator, that receives the originalimage and produces a corresponding visual prompt for eachindividual image. As a result, the Coordinator automaticallydesigns each prompt based on the input rather than the sharedmanual design of a previous work . Therefore, our prompthas a higher capability and can flexibly change the semanticsof the original image. By learning the reparameterized modelinstead of the prompt itself, we greatly reduce the number ofparameters (from 69K of VP to 9K or 1K) so that suitablefor the black-box optimization scenario. Second, to train the Coordinator, BlackVIP adopts a zeroth-order optimization (ZOO) that estimates the gradient from thepairs of input queries and outputs of the black-box model. Withthis modification, we can adapt any pre-trained vision model,whether its parameters are available in public or not, and cansignificantly reduce the required memory capacity comparedto white-box tuning methods that require caching intermediateactivations. For faster and more accurate optimization, wefurther present a new ZOO algorithm, Simultaneous Pertur-bation Stochastic Approximation with Gradient Correction(SPSA-GC) based on (SPSA) . SPSA-GC first estimatesthe gradient of the target black-box model based on theoutput difference of perturbed parameters and then corrects theinitial estimates in a momentum-based look-ahead manner. Byintegrating the Coordinator and SPSA-GC, BlackVIP achievessignificant performance improvement over baselines. This work is an extension of a conference paper ,which has a contribution as a leading work on black-boxvisual prompting. We summarize our new contributions in thispaper as follows: (1) We propose a new variant of BlackVIP,BlackVIP-SE (Statistical feature-guided Efficient prompt),which is remarkably faster and computationally more efficientthan the original BlackVIP while consistently outperformingother baselines. (2) To investigate the broad applications of our method, we expand the scope of validation with more diverseviewpoints, such as real-world distribution shift benchmarks,compatibility with the post-training quantization method, andwhite-box transfer regime. (3) For the first time, we providea theoretical understanding on the generalization mechanismsof visual prompting methods through the lens of the certifiedrobustness of a smoothed classifier.",
  "A. Parameter-Efficient Transfer Learning": "Adapting the entire PTM to targeted downstream tasks iscomputationally prohibited for most users. Parameter-EfficientTransfer Learning (PETL) methods pursue fine-tuning of asmall subset of PTMs parameter set while achieving compet-itive performance compared to full fine-tuning. Some of themhost new learnable modules inside of PTM , , or ontop of PTM representation , and others pursue learn-ing tokens of Transformer layer , e.g., CoOp and itsvariants , , , . Meanwhile, Bahng et al. andits follow-up research explore the Visual Prompting(VP) approach, which introduces a learnable prompt in theinput space, neither into the embedding space nor the modelsarchitectural building blocks. The prompt is attached to theimage (pixel-wise addition) and updated by gradient descentto optimize the downstream objective function. However, allthe existing PETL methods are not memory-efficient. Thatis, they require users to prepare sufficient memory capacityfor caching the intermediate representations inside of PTM tocompute the gradient. More importantly, they assume that themodel parameters are fully available to the public, which isnot the case in many cases of real-world AI, where the modelis only provided as a black-box API. While there are somerecent attempts pursuing memory-efficient fine-tuning , they still require accessibility to full model parameters.To break these optimistic assumptions, we provide the black-box visual prompting approach that does not require parameteraccessibility or large memory capacity.",
  "B. Black-Box Optimization": "In many real-world applications, high-performing AI mod-els are provided via black-box API or proprietary software toprotect model ownership. To this end, there are some recentworks that fine-tune the large language model via black-boxoptimization . Besides, black-box adversarial repro-gramming (BAR) had been proposed to re-purpose theImageNet pre-trained vision model to a specialized clas-sifier for medical images. The previous works on black-boxoptimization commonly adopt zeroth-order optimization (thatexploits the estimated gradients) or derivative-free optimiza-tion algorithms for parameter updates. BAR adopts a one-sided approximation gradient estimator, but we find that theone-sided estimator shows inaccurate gradient approximationsempirically. BBT , and BBTv2 adopt CMA-ES ,, and RLPrompt uses reinforcement learning (RL) tooptimize the discrete prompts. However, It has been knownthat derivative-free optimizations (e.g. evolutionary algorithm)are hard to solve large-scale problems and do not guarantee",
  "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 20213": ".BlackVIPs equip an input-dependent prompt designer (Coordinator) and an accurate gradient estimation algorithm (SPSA-GC). To produce low-dimensional latent features of input images, BlackVIP adopts another PTM as a frozen feature extractor. In this extended work, we relax the reliance onanother PTM with a simple projection obtained by a statistical learning method and call that approach BlackVIP-SE. convergence . Besides, RL approaches are notorious fortheir instability and high variance . This work adopts theSimultaneous Perturbation Stochastic Approximation (SPSA) as a ZOO algorithm. It is known that SPSA effi-ciently provides gradient approximation on high-dimensionalproblems , . Besides, SPSA theoretically guaranteesconvergence, and the convergence error is linearly upperbounded by the dimension of optimization . While SPSAis designed to estimate high-dimensional gradients efficiently,we found that SPSA-based neural network optimization stillrequires many queries in practice. Therefore, we proposeSPSA with Gradient Correction (SPSA-GC) that corrects theapproximated gradients to enhance the convergence speed.This is the first work exploring the ZOO-based black-boxoptimization to large PTMs in vision domain for general-purpose adaptation (rather than a specific domain ).",
  "III. METHODOLOGY": "Background. We first present an outline of adversarial re-programming and visual prompting, that originated from dis-tinct motivations but closely related topics. Elasyed et al. presented adversarial reprogramming (AR) inspired byadversarial attack . The goal of AR is repurposingthe pre-trained model to perform a new adversarial task. Letx Rkk3 be a resized image of the target adversarial task,and x Rnn3 is a zero-padded tensor that embeds x inthe center, where k < n. Given the target class of adversarialtask yadv {1, ..., Ctar}, the AR is formulated as:",
  "arg minW( log P(m(yadv)|xadv) + ||W||F )": "Here, P(y|x) is a softmax output of PTM given x and theadversarial image is constructed as xadv = x+tanh(W M),and W Rnn3 is the adversarial program that is optimized,where n is the image width of a pre-train dataset, M is anoptional mask for better visualization of the embedded targetimage, and denotes the element-wise multiplication. Given a pre-defined hard-coded mapping m() that maps labels from anadversarial task to labels of a pre-train dataset, AR reprogramsthe model by learning perturbation without changing modelarchitecture or weights. The sensitivity of neural networks tosuch perturbations has inspired many works that use the ARapproach from transfer learning perspectives , i.e., model reprogramming.Meanwhile, motivated by the remarkable success of theprompting paradigm in NLP, Bahng et al. are the firstto explore the pixel space visual prompting (VP) approachfor pre-trained vision models. By learning individual RGBpixels as an input space prompt that is attached to the inputimages, VP adapts the frozen PTM to targeted downstreamtasks without any modification on the model architecture andweights. Given the input x and corresponding label y, thelearning objective of VP is, arg min log P(y|x+), where is a learnable visual prompt and P(y|x) is an output propof PTM given x. During inference, VP employs the sharedinput-independent prompt for all images. It is noted that isattached to the fixed location, e.g., the outer part of an imagelike a frame by default. Though AR and VP use different termsand stem from distinct motivations, they share the fundamentalconcept: adapt a PTM to perform new tasks withoutmodifying the model architecture or its weights. The goal ofthis work aligns with AR and VP, but we extend and improvethem for broader applications.We will introduce our novel input-dependent prompt gener-ation module, Coordinator in Section III-A. Then, we explainthe end-to-end framework of BlackVIP with the new ZOOalgorithm, SPSA-GC in Section III-B. illustrates theoverall framework of our proposal.",
  "arg min log P(y|x)": "While VP and AR directly optimize the RGB pixels as param-eters of the prompt, we reparameterize the visual prompt to aneural network v() parameterized by = {d, t} Rp.Specifically, we design a novel module named Coordinatorcomposed of a fixed encoder f(), which projects an inputimage x to a low-dimensional feature zx, followed by alightweight decoder gd() that generates a visual promptfrom the integration of instance-specific feature zx and alearnable task-specific prompt trigger vector t. Consequently,the prompt-augmented image is formulated as follows:",
  "x = clip(x + v(x)),v(x) = gd(zx, t)": "where zx = f(x) is the feature vector of x from the fixedencoder f(), and is a hyperparameter that controlsthe intensity of visual prompt. We concatenate the prompttrigger vector t with zx and then reshape them into a 3Dfeature map to feed into the decoder gd() constructed by astack of convolution layers. As a result, the instance-specificrich semantic representation zx and the task-specific prior tare merged to design a valid visual prompt v(x) for a givenimage. Similar to , the prompted image is bounded to avalid RGB scale of the target models input domain via pixel-wise clipping operation clip(). Unlike previous works , that adopt a pre-definedinput-independent prompt to the local regions of an image, ourapproach automatically designs the input-dependent promptthat covers an entire image; therefore, it has a higher capabilityto change the semantics of images if necessary. Thanks to thisflexibility, it can cover more diverse tasks and be robust tochallenging scenarios, e.g., distribution shift. We demonstratethe efficacy of our prompt design strategies in Section IV.From now on, we will provide two specifications of theencoder component f() in the following paragraphs. 1) BlackVIP: By default, we adopt an ImageNet pre-trainedmodel with a self-supervised learning (SSL) objective as afrozen feature encoder f(). Though the encoder can also bea supervised counterpart or trained from scratch, we choosea frozen SSL encoder by following three reasons: 1) It hasbeen widely substantiated that self-supervised representationcontains the multiple discriminative features and spatial infor-mation , so it is more helpful to use SSL pre-trainedencoder than label-supervised encoder for robust adaption ofblack-box PTM on diverse tasks. 2) ImageNet pre-trainedmodels are currently well-publicized , so they can bereadily adopted by local users, and does not hurt our realisticexperimental setting. 3) By leveraging the frozen pre-trainedencoder, we significantly reduce the number of learnableparameters compared to the case of training the encoder fromscratch. The reduced dimension of optimization contributes toefficient gradient approximation.",
  ") BlackVIP-SE: While BlackVIP allows us to enjoy therich features as a source of prompts, it requires an additionalforward pass as well as the querying of black-box target PTM": "Algorithm 1 BlackVIP and BlackVIP-SE algorithmsRequire: Downstream dataset D, pre-trained model P(|),Coordinator v with encoder f and prompt decoder g, isparameterized by i = {d,i, t,i}, SPSA-GC decayingparameters {ai, ci}, smoothness parameters , prompt in-tensity , and training iteration T.// Initialize 1 = {d,1, t,1}, {a1, c1} and m1for i in 1 to T do",
  "end for": "As a result, it increases the runtime per iteration and duringinference compared with model-free prompting baselines andhinders its wider applications. Here, we note an observationthat the success of transfer learning is not only attributedto high-level feature reuse but also learning from the low-level statistics across data instances . Motivated by thisfinding, we hypothesize that low-level statistical features canbe an effective alternative to model-based high-level semanticfeatures for producing visual prompts to adapt the black-boxtarget model. To that end, we devise a new variant, BlackVIP-SE (Statistical feature-guided Efficient prompt), by replacingthe auxiliary PTM feature extractor with a more efficientstatistical method. Specifically, we conduct the principal com-ponent analysis (PCA) directly on the flattened vectors of trainimages to find a new coordinate system that preserves themaximum variation of data. Then, we project each image intothe PCA-induced low-dimensional space (98 by default) toget statistical features being convened to the decoder of theCoordinator1. This simple modification endows two favorablemerits compared to BlackVIP: 1) it remarkably reduces theruntime (See Table V) during training and inference phases byreplacing the PTM feature extractor into a single projectionmatrix, and 2) it also significantly reduces the number oflearnable parameters by setting the projection dimension ofPCA much lower than that of auxiliary PTMs latent featuredimension in BlackVIP (See Table IV) thereby decreasing thepeak memory usage and computational cost. B. End-to-End Black-Box Visual PromptingUnlike other PETL approaches, we consider the PTM as ablack-box predictor that gives only a prediction output (i.e.logit vector) for a given input image query. In this black-box setting, we adopt the ZOO algorithm, SPSA, with ourdeliberate modification to optimize Coordinator without theoracle true gradient.",
  "i+1 = i aiei(i)(2)": "where L is an objective function, i Rp is p-dimensionallearnable parameters, and i Rp is a ith-step randomperturbation vector, sampled from mean-zero distributions thatsatisfy finite inverse momentum condition , such asRademacher and Segmented Uniform distribution. With onlytwo forward evaluations, i.e., querying twice to the API servicemodel, SPSA parses the learning signal (estimate gradient)from the difference between model outputs, and we can updatethe parameters of Coordinator.2) SPSA with Gradient Correction: Although the standardform of SPSA works well in myriad applications ,like other ZOO algorithms, it may suffer slow convergencein practice , , and the problem gets even bigger onthe high-dimensional problem setting such as neural networksoptimization. We speculate that the source of slow convergenceis its noisy gradient estimation from the poor direction ofrandom perturbations or intrinsic data noise. To mitigate theestimation noise, inspired by Nesterovs accelerated gradient(NAG) , we improve the update rule in Eq. 2 as below:",
  "mi+1 = mi aiei(i + mi)(4)": "where [0, 1) is smoothing parameter. As clearly notedin , when the poor update i + mi occurs, this NAGstyle update rule strongly pulls it back towards i. Becauseof the SPSAs strongly stochastic nature, we conjecture thatthis gradient correction property is also highly effective forSPSA as well as first-order optimization algorithms.",
  "Datasets. First, to investigate the importance of prompt design,we first consider two synthetic datasets that simulate thecorrelation shift and varying object location scenarios (see": "Section IV-B and Appendix). Then, we extensively evaluateour methods on 14 transfer learning benchmarks. These coverdiverse visual domains and tasks, so they require understand-ing various visual semantics like scenes, actions, fine-grainedcategories, textures, satellite imagery, the number of objects,and recognition of generic objects. Moreover, to evaluate therobustness of prompt-based adaptation of PTM in the wild, weadditionally consider three benchmark datasets of distributionshifts from WILDS . See Supplementary for details.",
  ".(Left) loss curve and (right) noise sensitivity analysis of 100-Dimensional Rosenbrock optimization problem with ZOO algorithms": "Backbone model and methods. We adopt CLIP ViT-B/16 as a target PTM because CLIP does not require a separateclassifier for different tasks and performs classification dynam-ically with text embedding features by text-side prompting.As the encoder part of the Coordinator, we use ImageNet pre-trained vit-mae-base checkpoint for BlackVIP and sci-kit-learns vanilla PCA or kernel PCA for BlackVIP-SE. As base-line methods, we consider 1) zero-shot classification (ZS) withthe standard text prompt a photo of {classname}, 2)black-box adversarial reprogramming (BAR) that embedthe downsized images of the downstream task inside a learn-able prompt, and 3) VP with SPSA-GC that simply replacethe backpropagation in VP with our SPSA-GC. Besides,we validate the effectiveness of our new prompt design onthe white-box transfer learning setup, which enables users toaccess the true gradient of model parameters. Following thefew-shot classification setting of , we use 16 samples perclass for training and the entire test set for all evaluations bydefault. More details are provided in the Supplementary.",
  ") Comparison between ZOO algorithms: We validate ourSPSA-GC on the well-known optimization benchmark, Rosen-brock function. We report the normalized loss ( |L()L()|": "|L()L(0)|)where L() and L(0) is the loss value on the optimaland initial point, respectively, and L() is a loss value onthe current parameter R100. In (left), SPSA-GC shows faster and more stable convergence than RandomGradient-Free (RGF) adopted in previous works , , andeven achieves a comparable result to Nesterovs AcceleratedGradient (SGD-NAG) method which adopts the true gradient.Besides, we simulate the noisy loss observation scenario(emulating the mini-batch optimization) by adding Gaussiannoise to learning loss, i.e., Lnoisy() = L() + , where (0, scale2). In (right), as the noise increases, RGFrapidly degenerates while SPSA is still relatively stable, andour gradient correction (SPSA-GC) gives further improvement.This verifies the robust gradient approximation of SPSA-GC.2) Robustness on Correlation Shift: Next, we evaluate ourmethod on Biased MNIST to investigate the robustnessof BlackVIPs input-dependent automatic prompt design underthe feature-label correlation shift environment. Biased MNISTis a modified version of MNIST , constructed to validatea models generalization ability under color bias shift. At thetrain-time, each digit has a unique preassigned backgroundcolor that strongly correlates with the target label. The degreeof correlation is determined by the value , andthe correlation ratio is reversed as 1 at the inference-time. Results are summarized in Tab. I (left) and (right). In this setup, BlackVIPs remarkably outperform others(even white-box VP), and the performance gap becomes largerunder the stronger correlation. This means that BlackVIPs,by flexibly modifying the semantics of the image with input-dependent visual prompts, can be beneficial in challengingadaptation scenarios wherein spurious correlation exists.3) Robustness on Varying Object Location: For many ob-ject recognition datasets, the target objects are commonlyplaced in the center of the image. However, input data offoundation models in the wild may have varying object loca-tions. We expect that BlackVIP and BlackVIP-SE adopt input-dependent prompts that cover the entire region of the image sothey are still robust even if the object is not always located inthe center of the image. To validate this, we create a variant of the MNIST, Loc-MNIST, by putting a real target digit on thefour edges and an arbitrary fake digit in the center of the blackblank image. The location of the target digit and the classof the fake digit are chosen randomly. We further considera more challenging setup in that the fake digit is four timeslarger (1:4) than the real one. We summarize the results in Tab.I (right) and , respectively. Compared to the manuallydesigned input-independent prompts (BAR and VP) that areframe-shaped by default, conditional prompts produced byBlackVIPs achieve significantly better performance, whichsupports the superiority of the Coordinators prompt design.",
  "C. Few-shot Transfer Learning on Benchmarks": "1) Main Results: We consider the 14 commonly used few-shot benchmark datasets following , , . As shownin Tab. II, while BAR and VP undergo large performance vari-ations across 14 datasets, BlackVIPs boast consistently highperformance (i.e., improve the zero-shot performance on 13and 14 over 14 datasets). Specifically, BAR shows promisingresults on the tasks that require understanding coarse semantics(DTD , EuroSAT , and RESISC ), but fails to showcompetitiveness on CLEVR that requires visual reasoning(counting objects) by capturing the overall image semantics.Meanwhile, BlackVIP performs well across various tasks byextending or limiting the region of attention of the black-box target PTM (), which denotes that BlackVIP is ahigh-capability prompt learner that robustly adapts the PTMto diverse downstream tasks. Overall, BlackVIP-SE also showsremarkable performance improvements that are comparable toBlackVIP while significantly faster and lighter. We speculatethat BlackVIP-SE excels on the datasets where the inputfeature space has a relatively low intrinsic dimensionality ,, such as SVHN, so the PCA-based approaches can finda sufficiently expressive low-dimensional manifold .",
  ". Grad-CAM analysis on CLEVR, Pets, and UCF101": "2) Visual Prompting in the Wilds: We now validate thoseapproaches on three kinds of real-world datasets (iWildCam,FMoW, and Camelyon17) from the WILDS distribution shiftbenchmark . In iWildCam, the task is to classify the182 species of animals in the wild under different remotesensing conditions (location of camera traps). In FMoW,satellite images should be classified into the building and land",
  "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 20217": "TABLE IITOP-1 CLASSIFICATION ACCURACY ON 14 BENCHMARKS THAT REQUIRE NATURAL, SPECIALIZED, STRUCTURED, AND FINE-GRAINED VISUALRECOGNITION. BLACKVIP AND BLACKVIP-SE SHOW OUTSTANDING RESULTS AMONG INPUT-SPACE VISUAL PROMPTING METHODS. Win MEANS THENUMBER OF DATASETS THAT EACH METHOD BEATS THE ZERO-SHOT PERFORMANCE. GRAY-COLORED VALUES ARE THE RESULTS OF WHITE-BOXPROMPT LEARNING WITH THE STOCHASTIC GRADIENT DESCENT. ALL EXPERIMENTS ARE DONE IN 16-SHOT TRAINING WITH THREE REPEATED RUNS.",
  "BAR1,6493,35237K37KVP w/ SPSA-GC1,6653,36969K69KBlackVIP-SE1,3192,5401K1KBlackVIP2,4283,2609K9K": "types 62 in total, and the shifting factor is the combinationof (year, region). Meanwhile, Camelyon17 requires a binaryclassification (tumor or normal) on the patch-level tissueslides under varying hospital imaging conditions. In TableIII, the baseline approaches, BAR and VP w/ SPSA-GC,show their effectiveness in improving the zero-shot classifiersperformance, and our BlackVIPs achieve superior average per-formances. Interestingly, while BlackVIP-SE underperformsother methods in iWildCam and FMoW, it shows outstandingperformance on Camelyon17, where we speculate the intrinsicdimensionality of task is relatively low, and the PCA featurescan be discriminative enough to generate good prompts. 3) Practical Usefulness: Memory and computation effi-ciency (Table IV). First, BlackVIP and BlackVIP-SE have9K and 1K parameters in total, respectively. This allows usto remarkably reduce the computational cost compared withother white-box approaches (such as FT and LP) or black-box approaches (BAR and VP). Furthermore, all the black-box adaptation approaches significantly reduce the train-timepeak memory requirement, which is burdening for white-box approaches. Among the black-box methods, BlackVIP-SE TABLE VRUNTIME (SECOND) COMPARISON BETWEEN PROMPTING METHODS.TRAINING DENOTES THE SUM OF THE FORWARD PROMPT GENERATIONAND THE PARAMETER UPDATE PROCEDURES FOR A SINGLE BATCH, ANDINFERENCE INDICATES PER BATCH FORWARD TIME.",
  ". Query efficiency. (x-axis) A number of queries and cost for achieving(y-axis) corresponding performance": "Query efficiency (). Second, BlackVIPs show out-standing query efficiency among black-box prompting meth-ods. For instance, by sending just 10K queries with 12USD (based on Clarifai Vision API), we can improve theperformance of a zero-shot model about two three timesbetter using BlackVIP and BlackVIP-SE. Given that we payfor each query to adapt black-box API models, ensuring queryefficiency does matter, and our BlackVIPs offer a favorablecost-and-performance trade-off.Runtime analysis (Table V). Next, we investigate the wall-clock time for the single-batch forward prompt generationand parameter update during training and inference timeper batch. While BlackVIP achieves superior classificationaccuracy on many setups, it compromises the time complexitydue to its reliance on an auxiliary feature extractor duringprompt generation. This hinders the broad use of BlackVIPin some applications where the inference time is crucial.",
  "ZS37.532.645.240.848.4BAR26.933.570.377.252.0VP w SPSA-GC34.731.271.170.952.0BlackVIP-SE50.055.166.271.260.6BlackVIP (RN50)51.350.862.968.558.4BlackVIP (ViT-B/16)48.451.367.973.160.2": "In contrast, our new variant, BlackVIP-SE, addresses thisissue by replacing the feature extractor model with a singleprojection matrix founded by PCA. As a result, BlackVIP-SEachieves a competitive runtime compared with other baselines(BAR and VP) while significantly outperforming in terms ofclassification accuracy (See Table II). This reduced runtimeduring training and inference phases allows it to become afavorable alternative to BlackVIP in time-critical applications.",
  "D. Ablation Study": "1) Model Architectures: To study the versatility of ourmethod, we vary the backbone architecture of the pre-trainedtarget model and the encoder of Coordinator in Tab. VI.While BAR and the naive application of SPSA-GC on VP failto improve the zero-shot performance of CNN-based targetbackbones that lack the global attention of Transformers ,our BlackVIPs consistently bring huge performance gainsacross all the architectures. It implies that BlackVIPs arearchitecture-agnostic approaches, which pursue the generaladaptation method for high-performing PTMs. Regarding thecomparison between BlackVIP and BlackVIP-SE, BlackVIPshows better results when the model architecture of the Co-ordinator encoder aligns with the target PTMs architecture.Meanwhile, BlackVIP-SE achieves more stable performanceimprovement across all architectures.",
  "BlackVIP-SE (PCA matrix with SPSA-GC)71.2BlackVIP (SSL pre-trained with SPSA-GC)73.1": "2) Coordinator and ZOO Algorithms: BlackVIPs adopt theencoder-decoder structure to efficiently generate the input-dependent prompts. We exploit an SSL pre-trained encoder(in BlackVIP) and PCA projection encoder (in BlackVIP-SE) while learning a randomly initialized lightweight decoder from scratch. From the design philosophy of BlackVIPs, weexpect that the encoder extracts the essential features of thegiven image, including the spatial features, and the decoderutilizes the features to produce a spatially and semanticallystructured prompt tailored to the input. We conjecture thatSSL pre-trained and PCA encoders are desirable to capturethe demanding semantics instead of a supervised one learnedfrom pre-defined labels. Here, Table VII confirms that the SSLencoder outperforms the supervised pre-trained or randomlyinitialized encoder (scratch). Besides, the PCA encoder alsooutperforms the large-scale supervised encoder, which indi-cates the potential of the classic statistical methods for featureextraction during transfer learning. Furthermore, SPSA-GCimproves the 3.7% accuracy than SPSA, from 69.4 to 73.1.It denotes that approximated gradients by our SPSA-GC aremore accurate than the original SPSA.",
  "VVP69K60.440.890.263.8VVPT73K62.939.993.265.3VOurs-SE1K60.536.190.062.2VOurs9K60.442.690.864.6VOurs68K61.843.690.565.3VOurs150K61.445.491.466.1": "1) White-box Transfer Learning: Although the chief mo-tivation of BlackVIP originated from the desire to break theparameter accessibility assumption and overwhelming memoryrequirements, our prompt reparameterization strategy not onlyresolves the memory issues but also improves the designof visual prompts compared with other visual promptingapproaches. To validate the efficacy of the prompt designs ofBlackVIPs, we replace the estimated gradients of ZOO withthe true gradient computed by the first-order optimizer, assum-ing we have access to the entire model parameters. As shownin Table VIII, our prompting approaches (corresponding toBlackVIP and BlackVIP-SE) outperform other recent prompt-based efficient tuning methods such as VP, VPT, CoOp, andCoCoOp under similar parameter capacity. Even our approach(with 150K parameters) is comparable with MaPLe , whichrequires eight times more parameters. These results imply thatour innovative design of prompt can be leveraged on the white-box parameter-efficient tuning regime as well.2) Visual Prompting for Quantized Models: While the maingoal of BlackVIPs is to enable memory-efficient transferlearning without parameter accessibility, as the scale of mod-ern state-of-the-art AI models continually skyrockets ,reducing the inference latency is also increasingly crucial for",
  "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 20219": ". Illustration on a classifiers decision boundary. The dotted contours are the level sets of Gaussian where the mean and covariance parameters are setmanually or learned from data, and the red and blue arrows indicate the transformation of data x by attaching visual prompts. Here, the circle and star symbolsdenote the original and transformed data. Two left plots (a) present the comparison between the randomized smoothing classifier and the visual promptingclassifier. Under some assumptions, a classifier with visual prompting methods can be interpreted as a data-driven adaptive smoothing classifier. Two rightplots (b) provide explanations on the generalization behavior of different visual prompting methods. While the vanilla VP applies a single shared prompt toall test inputs that might induce incorrect transform, e.g., regardless of the true classes (Blue and Green) of input samples x1|y = Blue and x2|y = Green,respectively, they apply the same offset vector that minimizes the expected loss and give a unified robustness degree, our BlackVIPs can flexibly provide theinstance-specific smoothing distributions that have varying mean and covariance structures suitable for each data to be correctly classified.",
  "-bit, intZS15.016.60.0482VP w/ SPSA-GC14.714.20.0519BlackVIP-SE16.915.40.0548BlackVIP16.318.70.0584": "deploying those models on diverse edge devices. Post-trainingquantization is a representative remedy that reduces the modelsize and required peak memory by converting the learnedmodel parameters into low-bit ones. Here, we validate thevisual prompting strategies on post-training quantized low-bitmodels. Specifically, we produce the visual prompts learned bythe original 32-bit model and equip the prompts with imagesto feed into the converted low-bit model for performanceevaluation. While the effectiveness varies across the bit-sizeand dataset, we confirm that (in Table IX) BlackVIPs canactually collaborate with the post-training quantization methodto reduce the memory requirement. Compared with VP w/SPSA-GC, BlackVIPs show remarkably better performancewhile not significantly increasing inference-time latency.",
  "V. THEORETICAL UNDERSTANDING ON THEGENERALIZATION OF THE VISUAL PROMPTING": "Preliminary and Overview. A classifier F() maps a ddimensional input variable x Rd to the categorical variablec C. Here, we define a randomized smoothing classifierG(x) := arg maxc Pr(F(x + ) = c) which returns the class cthat maximizes the aggregated classification probability underGaussian noise N(0, 2I) with an noise intensity andan identity matrix I. Some recent works , showedthat the smoothed classifier maintains its prediction (the most probable class) against l2-norm bounded perturbations. Thisrobustness guarantee of the smoothed classifier originated fromthe additive noise following a Gaussian distribution. Here, wenote that the optimization trajectory of a visual prompt viastochastic gradient descent (SGD) with a constant learning ratecan be viewed as a sampling process from a Gaussian posteriordistribution that centered at the local optimum defined bythe objective function , . This indicates the promptedclassifier imitates the behavior of a smoothed classifier, en-suring robustness against input perturbations at test time. Theillustration in summarizes our statements.",
  "L() = log P(y|x + v(x)),": "where P(y|x) is a PTMs output probability for ground truthy given x and the v(x) is a visual prompt parameterizedby . This expression is reduced to the learning objective ofVP, when v(x) = . For simplicity, we assume the input-dependent prompt generation process BlackVIP as v(x) x , the Hadamard product of input x and the learnableparameter . We optimize the visual prompt parameter viaSGD with a constant learning rate as t+1 = tL(t).As shown in a previous work , given a sufficiently largetime step t, when the t converges to the minimum of lossfunction and L(t) approaches zero, = t = t+1, ...becomes samples from a stationary Gaussian as below:",
  "S BBT ,(5)": "where S is the minibatch size. We view as a random vari-able near the minimum of the loss function, showing Gaussianapproximation after the time step t from initialization. Here, := arg min log P(y|x + v(x)) is the point where ourloss function is minimized, and A is a Hessian of loss w.r.t.the optimal point of and BBT is a covariance matrixof the SGDs gradient noise (See . Now, given that we",
  "(i) v(x) = N(, )(VP)(ii) v(x) = x N(x , DxDx)(BlackVIP)": "where Dx= diag(x) given an input x. This statementreveals that a classifier with visual prompt learning can beinterpreted as a randomized smoothing classifier G(x) wherethe location parameter () of Gaussian distribution is adaptedtowards a direction that the downstream classification lossto be minimized. This allows us to leverage the robustnessguarantee , of a smoothed classifier for explainingthe generalization behavior of visual prompting. B. Certified Robustness of Visual Prompting ClassifierIn the previous section, we present a new interpretationof visual prompting: the visual prompting classifier at thelater part of the training stage is approximately an adaptiverandomized smoothing classifier. In this section, we will derivea new statement on the generalization of the visual promptingclassifier during the inference stage: the visual promptingclassifier provides a robustness guarantee against perturbation within the radius R for a given sample x. In other words,it preserves the most probable class c for the sample x + ,which a smoothed classifier G(x) returns on the input x.Let cA be the most probable class for given x, and pA bethe corresponding output probability of the classifier F. Thesecond most probable class is similarly defined as cB withthe corresponding probability to pB. The probabilities of pAand pB vary by given input, so it is useful to bound the rangeof these probabilities and explain the relationship with thesmoothed classifier. We denote pA as a lower bound for pAand pB as an upper bound for pB as done in . Now, assumethere exists cA C and pA, pB such that:",
  "min(1(pA) 1(pB))": "The result means that our smoothed classifier G retainsthe most probable class of the original classifier even inthe presence of input perturbation. Formally, it implies thatPr(F(X) = cA) is at least as large as Pr(F(X) = c) forany c C {cA}. Without loss of generality, we fix c ascB which is the second most probable class, then we need toshow that Pr(F(X) = cA) Pr(F(X) = cB). The boundsfor probabilities pA and pB can be expressed as Pr(X A)and Pr(X B) when the regions A and B are given by:",
  "VI. CONCLUSION": "We pioneer black-box visual prompting for the realisticand robust adaptation of PTMs. We propose BlackVIP andits efficient alternative, BlackVIP-SE, which reparameterizesthe visual prompt with a conditional generation network,Coordinator. By equipping our new ZOO algorithm, SPSA-GC, BlackVIPs do not require any accessibility on modelarchitecture or parameters and efficiently adapt the pre-trainedvision model to downstream tasks. Extensive empirical resultson 19 datasets show that BlackVIP and BlackVIP-SE con-sistently improve the performance over baseline methods onfew-shot adaptation in the wilds, with minimal computationalcost, memory capacity, and API queries. We further providetheoretical analyses of the generalization of visual promptingmethods by providing a novel connection between the certifiedrobustness of randomized smoothing and visual prompting.",
  "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 202111": "T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., Language mod-els are few-shot learners, Advances in neural information processingsystems, vol. 33, pp. 18771901, 2020. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferablevisual models from natural language supervision, in InternationalConference on Machine Learning.PMLR, 2021, pp. 87488763.",
  "H. Bahng, A. Jahanian, S. Sankaranarayanan, and P. Isola, Visualprompting: Modifying pixel space to adapt pre-trained models, arXivpreprint arXiv:2203.17274, 2022": "M. U. Khattak, H. Rasheed, M. Maaz, S. Khan, and F. S. Khan, Maple:Multi-modal prompt learning, in Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, 2023, pp. 19 11319 122. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,An image is worth 16x16 words: Transformers for image recognitionat scale, in International Conference on Learning Representations,2020.",
  "B. Zhu, Y. Niu, Y. Han, Y. Wu, and H. Zhang, Prompt-aligned gradientfor prompt tuning, in Proceedings of the IEEE/CVF InternationalConference on Computer Vision, 2023, pp. 15 65915 669": "S. Ren, A. Zhang, Y. Zhu, S. Zhang, S. Zheng, M. Li, A. J. Smola,and X. Sun, Prompt pre-training with twenty-thousand classes foropen-vocabulary visual recognition, Advances in Neural InformationProcessing Systems, vol. 36, 2023. Q. Huang, X. Dong, D. Chen, W. Zhang, F. Wang, G. Hua, andN. Yu, Diversity-aware meta visual prompting, in Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition,2023, pp. 10 87810 887. A. Chen, Y. Yao, P.-Y. Chen, Y. Zhang, and S. Liu, Understandingand improving visual prompting: A label-mapping perspective, inProceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, 2023, pp. 19 13319 143.",
  "N. Hansen and A. Ostermeier, Completely derandomized self-adaptation in evolution strategies, Evolutionary computation, vol. 9,no. 2, pp. 159195, 2001": "N. Hansen, S. D. Muller, and P. Koumoutsakos, Reducing the timecomplexity of the derandomized evolution strategy with covariancematrix adaptation (cma-es), Evolutionary computation, vol. 11, no. 1,pp. 118, 2003. S. Liu, P.-Y. Chen, B. Kailkhura, G. Zhang, A. O. Hero III, and P. K.Varshney, A primer on zeroth-order optimization in signal processingand machine learning: Principals, recent advances, and applications,IEEE Signal Processing Magazine, vol. 37, no. 5, pp. 4354, 2020.",
  "N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, andA. Swami, The limitations of deep learning in adversarial settings,in IEEE European Symposium on Security and Privacy (EuroS&P),2016": "H. Xu, Y. Ma, H.-C. Liu, D. Deb, H. Liu, J.-L. Tang, and A. K. Jain,Adversarial attacks and defenses in images, graphs and text: A review,International Journal of Automation and Computing, vol. 17, no. 2, pp.151178, 2020. P. Neekhara, S. Hussain, J. Du, S. Dubnov, F. Koushanfar, andJ. McAuley, Cross-modal adversarial reprogramming, in Proceedingsof the IEEE/CVF Winter Conference on Applications of ComputerVision, 2022, pp. 24272435.",
  "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 202112": "I. Melnyk, V. Chenthamarakshan, P.-Y. Chen, P. Das, A. Dhurandhar,I. Padhi, and D. Das, Reprogramming large pretrained language mod-els for antibody sequence infilling, arXiv preprint arXiv:2210.07144,2022. M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski,and A. Joulin, Emerging properties in self-supervised vision trans-formers, in Proceedings of the IEEE/CVF International Conferenceon Computer Vision, 2021, pp. 96509660.",
  "Y. Nesterov, A method for solving the convex programming problemwith convergence rate o(1/k2), Proceedings of the USSR Academyof Sciences, vol. 269, pp. 543547, 1983": "I. Sutskever, J. Martens, G. Dahl, and G. Hinton, On the importanceof initialization and momentum in deep learning, in Proceedings of the30th International Conference on Machine Learning, ser. Proceedingsof Machine Learning Research, vol. 28, no. 3.PMLR, 1719 Jun2013. P. W. Koh, S. Sagawa, H. Marklund, S. M. Xie, M. Zhang, A. Balsub-ramani, W. Hu, M. Yasunaga, R. L. Phillips, I. Gao et al., Wilds: Abenchmark of in-the-wild distribution shifts, in International confer-ence on machine learning.PMLR, 2021, pp. 56375664.",
  "S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi,Fine-grainedvisualclassificationofaircraft,arXivpreprintarXiv:1306.5151, 2013": "J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba, Sundatabase: Large-scale scene recognition from abbey to zoo, in 2010IEEE Computer Society Conference on Computer Vision and PatternRecognition, 2010, pp. 34853492. Y.Netzer,T.Wang,A.Coates,A.Bissacco,B.Wu,andA. Y. Ng, Reading digits in natural images with unsupervisedfeaturelearning,inNIPSWorkshoponDeepLearningandUnsupervised Feature Learning 2011, 2011. [Online]. Available: housenumbers.pdf",
  "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 202113": "P. Bandi, O. Geessink, Q. Manson, M. Van Dijk, M. Balkenhol,M. Hermsen, B. E. Bejnordi, B. Lee, K. Paeng, A. Zhong et al., Fromdetection of individual metastases to classification of lymph node statusat the patient level: the camelyon17 challenge, IEEE transactions onmedical imaging, vol. 38, no. 2, pp. 550560, 2018.",
  "F. Chollet, Xception: Deep learning with depthwise separable convo-lutions, in Proceedings of the IEEE conference on computer visionand pattern recognition, 2017, pp. 12511258": "R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, andD. Batra, Grad-cam: Visual explanations from deep networks viagradient-based localization, in Proceedings of the IEEE internationalconference on computer vision, 2017, pp. 618626. Changdae Oh is currently a Ph.D. student at Department of ComputerSciences at University of Wisconsin-Madison, and he received his bachelorsdegree in Statistics and his masters in Artificial Intelligence from Universityof Seoul in 2022 and 2024, respectively. Gyeongdeok Seo is currently a Masters degree student at the Department ofStatistics and Data Science at Yonsei University. He received his bachelorsdegree in Applied Statistics from Yonsei University in 2022 and worked as aData Analyst at Naver Financial in 2022-2024. Geunyoung Jung is currently an integrated Ph.D. student at the Departmentof Artificial Intelligence at University of Seoul, and he received his bachelorsdegree in Physics from University of Seoul in 2022. Zhi-Qi Cheng is a Project Scientist at the Language Technologies Institute(LTI), which is part of the School of Computer Science at Carnegie MellonUniversity (CMU). He earned his B.S. in Computer Science from SouthwestJiaotong University in 2014 and completed his Ph.D. in 2019. During hisdoctoral studies, he was a visiting Ph.D. student at the City University of HongKong (2016-2017) and later at CMU (2017-2019). He also gained valuableexperience through internships at Alibaba DAMO Academy (2016), GoogleBrain (2018), and Microsoft Research (2019). From 2019 to 2022, he servedas a postdoctoral research associate at CMU. Hosik Choi received the B.S. and Ph.D. degrees in Statistics from theSeoul National University, in 2000 and 2007. He is currently a Professorat Department of Urban Big Data Convergence, and an Adjunct Professor atDepartment of Artificial Intelligence at University of Seoul (UOS). Beforejoining UOS, he was a full-time Lecturer and Assistant Professor at HoseoUniversity from 2008 to 2014. Then, he was an Assistant/Associate Professorat Department of Applied Statistics, Kyonggi University from 2014 to 2020.In 2023, he was a Visiting Professor at Korea Institute for Advanced Study. Jiyoung Jung received the B.S., M.S., and Ph.D. degrees in ElectricalEngineering from the Korea Advanced Institute of Science and Technology(KAIST), in 2008, 2010, and 2016, respectively. She is currently an AssociateProfessor at the Department of Artificial Intelligence, University of Seoul.She joined the department in September 2021. She served as an AssistantProfessor at the Department of Software Convergence, Kyung Hee Universityin 2017-2021. Previously, she worked as a research engineer at Naver Labs. Kyungwoo Song received the B.S. degree in mathematical sciences and in-dustrial and systems engineering and the M.S. and Ph.D. degrees in industrialand systems engineering from the Korea Advanced Institute of Science andTechnology (KAIST), in 2017 and 2021. In 2017, he was a part-time Professorwith Hanbat National University. In 2018, he was a Visiting Researcher withNaver Clova. From 2021 to 2023, he was an Assistant Professor with theDepartment of Artificial Intelligence, University of Seoul. He is currentlyan Assistant Professor with the Department of Applied Statistics and theDepartment of Statistics and Data Science, Yonsei University APPENDIXA. Normal ApproximationThe approximation of updating parameters to samplingfrom a normal distribution is based on the fact that batch-wise updates introduce variability in the gradient of the lossfunction. As the batch size S increases, the gradient noiseconverges to a normal distribution according to a centrallimit theorem. The central limit theorem is applied under theassumption that a stochastic gradient is the sum of independentand uniform sampled S, which is illustrated below:",
  "S C())": "We also assume that the variance of this gradient noise isapproximated by the constant matrix C = BBT , where Cis factorized into BBT . Including these assumptions, If thefour assumptions detailed in the research are met, thenthe update of corresponds to sampling from the stationarydistribution of an Ornstein-Uhlenbeck process as shows,where:",
  "In this context, the covariance satisfies A + A =S BBT , with A being the hessian matrix at the optimal point": "B. Randomized Smoothing and Certified RobustnessA smoothed classifier exhibits robust properties within aradius R. Specifically, when an input is perturbed within theball B(x, R) = {y : ||x y|| < R}, the classificationconsistently predicts the most probable class. Building uponthe foundational work by which introduced a certifiedradius for a smoothed classifier using an isotropic Gaussiancovariance, and further extensions by to an anisotropicGaussian model with a diagonal covariance matrix, our analy-sis advances these models by relaxing the diagonal constrainton the covariance matrix, thus proposing a more generalizedform and redefining the radius based on the derived proof.To set the stage for our proof, consider the followingsettings:",
  "= pA": "Here, T 1/2is a row vector in R1d and a random vector in Rd1 follows a multivariate standard normal distribution.The inner product of theses vectors results in a weighted sumof univariate standard normal distributions, which reduces toa univariate normal distribution scaled by the constant . ForB, the probability is calculated as:",
  "C. Datasets": "a) Synthetic Datasets:Our BlackVIP generates theinput-dependent image-size visual prompt which covers thewhole image region, so we expect that this flexible promptdesign can improve some kind of robustness as well asgeneral recognition capability: (1) To evaluate the robustnesson distribution shift (i.e., domain generalization), we considerBiased-MNIST dataset. (2) To evaluate the robustness onadversarial noise and location-agnostic recognition capacity,we create a variant of the MNIST dataset called Loc-MNIST.Examples of these two datasets are provided in .Biased MNIST is a modified version of MNIST wherethe biases reside in the background colors of the images ofeach digit. At train time, each digit has a unique preassignedbackground color that strongly correlates with the label. Thedegree of correlation is determined by the value ,such that (100 )% of the images that belong to thesame digit have the preassigned color of that digit as theirbackground color, and the rest are uniformly assigned to haveany of the other colors as their background color. At test time,we reverse the ratio so that (100 (1 ))% of the imagesnow have the preassigned color as their background color andvice versa to evaluate the models dependency on superficialfeatures such as the color of the background that a digit islocated on. We prepare the following two environments 1)easy: = 0.8 and 2) hard: = 0.9.On the given black blank image with 224 224 resolution,i.e., zeros array, Loc-MNIST puts an original target digitimage from MNIST that has 28 28 resolution on the edge-side (e.g., 027 or 196223 for one of vertical or horizontalside and 0223 for another side) and puts a random fake digit(also from the MNIST dataset) on the center. The location ofthe target digit in the edge and the class of fake digit arechosen randomly with uniform probability. A synthetic imageis created one by one for each original MNIST image. Weprepare the following two environments 1) easy: the scale ofthe target and the fake digit is the same, i.e., 1:1, and 2) hard:the fake digit is four times larger than the original digit, i.e.,1:4.For consistency, we perform the experiments on these twodatasets with a few-shot evaluation protocol. To construct atrain set, we randomly sample a subset (K-shot) of the createdimages for each class and use the whole test set.",
  "(a)": ".Examples of y = 7 subset in Biased-MNIST with = 0.9.(Top) the train set is constructed with the spurious correlation betweenthe background color and digit class (e.g., y = 7 occurs 90% with pinkbackground and 10% with other random colors in this case). (Bottom) thetest set is constructed with a reversed correlation to that of the train set (e.g.,y = 7 occurs 10% with pink background and 90% with other random colorsin this case).",
  ". Examples of two synthetic datasets. (a) Biased MNIST and (b) Loc-MNIST": "b) Datasets: To extensively evaluate the effectiveness ofour proposed method and baseline approaches, we measureperformance across the following 14 datasets that are widelyused for transfer learning benchmark: Caltech101 , Ox-fordPets , StanfordCars , Flowers102 , Food101, FGVCAircraft , SUN397 , DTD , SVHN, EuroSAT , Resisc45 , CLEVR , UCF101, and ImageNet (IN) . Note that these 14 datasets coverdiverse visual domains, and they require understanding variousvisual semantics like scenes, actions, fine-grained categories,textures, satellite imagery, digits, the number of objects, andthe recognition of generic objects. Moreover, to evaluate therobustness of prompt-based adaptation of PTM in the wild, weadditionally consider three benchmark datasets of distributionshifts from WILDS : Camelyon17 , FMoW , andiWildCam .Following the protocol in , , we conduct a few-shotevaluation for all datasets: 16-shot for the train set, 4-shot forthe validation set, and the whole test set. We use the few-shotsplit by for each dataset those are also used in , while forResisc45, CLEVR, and WILDS datasets, we randomly selectthe 16-shot and 4-shot samples for training and validationdataset, respectively.",
  "D. Backbone Model": "In this work, we aim at the robust adaptation of pre-trainedmodels on diverse downstream tasks. For these pre-trainedmodels, all experiments in this paper are done with the off-the-shelf vision-language model CLIP , and we adopt theViT-B/16 for image encoder backbone architecture by default.During the adaptation (training) phase, the entire componentsof the pre-trained model are frozen without any architecturalmodification, and we only manage and optimize the learnablemodule Coordinator from the outside of the pre-trained model.While input space visual prompting allows it to be applied tonot only VLM, but also any other vision models like CNNs andViTs, it requires the user to define the output space mapping,which maps the output prediction category set of a pre-trainedtask to a new downstream category set , , . Thisis another non-trivial problem. Therefore, we limit our focusto only the VLM that can dynamically build the task-specifichead from manual text template , so that free fromdefining output space mapping.",
  "E. Baseline Methods": "a) CLIP Zero-Shot (ZS): CLIP is one of the mostpopular vision-language zero-shot models that is widely ex-ploited for classification, detection, segmentation, and othervision or vision-language tasks. Based on its well-alignedvision-language joint embedding space, the zero-shot classi-fication can be performed with a manual text prompt (alsocalled template) of each pre-defined class category. In thispaper, we are mainly aiming to improve the CLIPs strongzero-shot performance in the few-shot adaptation setting.b) BAR: Black-Box Adversarial Reprogramming (BAR) was proposed for efficient transfer learning of pre-trainedmodel to the medical image domain. Different from the previ-ous works on Adversarial Reprogramming (AR), BAR exploitsthe perturbation-vulnerability of neural networks for adap-tation purpose rather than attack. By optimizing the frame-shaped learnable program, which embeds a downstream targetimage inside of that, BAR steers the ImageNet pre-trainedmodel to classify the specialized medical images. Moreover,BAR adopts the zeroth-order optimizer (ZOO), RandomizedGradient-Free (RGF) minimization algorithm for black-box transfer learning to broaden its applications.When the resolution of the downstream input image is overthat of the pre-training phase, Tsai et al. set the embeddedtarget image size for 64 64 resolution in the 299 299-size learnable program by default. However, we observe thatsuch a heavy-pad thin-image design of prompt degrade theperformance significantly, so we tune the resolution of theembedded image and set 194 194.c) VP: Similarly, Visual Prompting (VP) aims at adapt-ing a pre-trained model to downstream tasks via learning inputspace visual prompts. Among some candidates for promptdesigns, Bahng et al. adopt the padding-style prompt so thatrealized prompts look like the frame-shape program of ARs.VP learns a universal visual prompt per each downstream task,and it just adds to all of the images in a task. Unlike the ARmethods or our BlackVIP, the range of prompted images is",
  "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 202117": "unbounded. Following , we use the padding-style prompt,which is 30-pixel sized for each side by default.While VP optimizes the parameters in the input space, itrelies on a first-order optimization algorithm that uses thetrue gradient of entire model parameters, and we establish theperformance of VP as a kind of upper bound for other inputspace black-box optimization approaches, including BlackVIPand BlackVIP-SE. Additionally, by replacing the first-orderalgorithm with zeroth-order counterparts, we build two newbaselines VP w/ SPSA and VP w/ SPSA-GC on our extensiveexperiments. These two methods confirm the effectiveness ofour new components Coordinator and SPSA-GC.Furthermore, we consider some representative prompt learn-ing methods as baselines on language modality (CoOp and CoCoOp ), vision modality (VP and VPT ),and both modality (MaPLe ) for an evaluation of white-box transfer learning experiments to validate the effectivenessof the design of our visual prompt, solely. All of thesemethods adapt parameters inside the model, e.g., tokens inthe embedding layer or intermediate layers of text or visualencoder of the target vision-language model.d) Discussion: Although BAR, VP, and BlackVIP sharethe generic goal: efficient transfer learning of pre-trained mod-els via input-space optimization, there are several significantdifferences. (1) We propose a novel prompt design that is auto-matically formed in an input-dependent manner rather than theframe-shaped manual design of the input-independent prompt(or program) of VP (or BAR). (2) While VP relies on first-order algorithms and BAR adopts the RGF, we utilize the newvariants of SPSA , SPSA-GC, which is enhanced with aproper modification in the parameter update rule. (3) Contraryto the medical imaging-only validation in BAR, based on theabove two technical difference, BlackVIP successfully adaptthe pre-trained model to diverse data domains (described inSection B.1.).",
  "F. Implementation Details": "a) Architecture: For the fixed text prompt design ofeach dataset those are shared across all baseline methodsand BlackVIPs, we use the same templates provided by for SVHN, CLEVR, and Resisc45, and for remaining 11datasets. For the fixed encoder part of our Coodinator, weuse the ImageNet pre-trained vit-mae-base checkpoint3 from the HuggingFace. The output shape of the encoder isN 768, where N is the number of instances in the batch.For the BlackVIP-SE, we used the scikit-learns vanilla PCAor kernel PCA method4. For fast training, we pre-computed thePCA features for BlackVIP-SE and retrieved them by indexduring each training iteration. During the inference phase, weload the trainset-fitted PCA projection matrix and apply it tothe flattened test input x to get the feature vector fed into theprompt decoder. Here, we design the decoder based on depth-wise separable convolution (DSC) layer for parameter ef-ficiency. Specifically, we build a block of [NORM-ACT-CONV]",
  "doc/vit mae4We considered whether to use a kernel or not and which kernel (amongRBF, cosine, and polynomial kernels) to use as hyperparameters": "and stack it five times. The NORM and ACT denote BatchNormalization and Gaussian Error Linear Unit, respectively.The CONV operation of the first four blocks is DSC, and thelast one is a standard convolutional layer. Our implementationcode is available at satisfy a fully convolutional design without loss ofexpressiveness, tensors that are fed into the decoder must beshaped in a 3D feature map. For this, we additionally govern atask-specific single continuous vector t (called prompt triggervector), which is concatenated with the output feature vector ofencoder leading the appropriate size of 1d vector for reshapingto 3d tensor. For the BlackVIP, we set the dimension of theprompt trigger vector to 800, resulting in 1568 dimensions ofconcatenated vector that can be reshaped to 3277 shaped3D tensor. Meanwhile, BlackVIP-SE allows users to set thelatent feature dimension (for the reduced PCA coordinatesystem) as their own preference. This contributes to furtherreducing the input dimension of the decoder feature map, andwe set the PCA feature dimension to 98, and rather thanconcatenation, we simply use the sum of the trigger vectorand PCA feature as input of the prompt decoder. The prompttrigger is shared across all instances for a given task.b) Optimization and other configurations: For a stableapproximation of gradient in practice, ZOO algorithms repeatthe gradient estimation step for several times and use themean of those estimates as a final approximation of thegradient. Usually, the approximation quality is proportional tothe number of these repeats. We set this repeat as five timesfor all baselines that use ZOO.Besides the learning rate and learning rate schedule parame-ters, ZOO algorithms have some additional algorithm-specifichyperparameters that need to be tuned. For RGF, these arethe standard deviation of a random gaussian vector and asmoothing parameter, and for SPSA, these are the perturbationmagnitude and its decaying factor. We provide the searchrange of each hyperparameter in Table X. The search rangefor algorithm-specific parameters is based on the proposalof authors of SPSA and BAR . Moreover, amongthe valid perturbation distributions of SPSA, we adopt theSegmented Uniform [1.0, 0.5] [0.5, 1.0].The learning objective is a cross-entropy loss for VP andBlackVIP and focal loss for BAR (following ). For allblack-box approaches, the batch size is set to 128 acrossall datasets. Except for the SUN397 (1,000), StanfordCars(2,500), and ImageNet (500), we optimize all methods during5,000 epochs for convergence. Note that the input spacevisual prompting with first-order algorithm already requiressufficiently large iterations, e.g., 1,000 epoch with fulldataset, and ZOO demands much more iterations due to thelack of gradient information.",
  "HyperparameterAlgorithmSearch Range": "initial LRBAR, VP{40.0, 20.0, 10.0, 5.0, 1.0}initial LR (a1)BlackVIPs{1.0, 0.1, 0.01, 0.005}min LRBAR{0.1, 0.01, 0.001}decaying stepBAR{0.9, 0.5, 0.1}LR decaying factorVP, BlackVIPs{0.6, 0.5, 0.4, 0.3}initial PM (c1)BlackVIPs{0.01, 0.005, 0.001}PM decaying factorBlackVIPs{0.2, 0.1}std. of perturbationBAR{1.0, 0.5}smoothingBAR{0.1, 0.01, 0.001}gradient smoothingVP, BlackVIPs{0.9, 0.7, 0.5, 0.3}dimension of featuresBlackVIP-SE{32, 49, 98, 196}PCA kernel typeBlackVIP-SE{linear, RBF, cosine, polynomial} with two novel mechanisms: (1) parameter-efficient instance-aware prompt generation network, and (2) stable zeroth-order optimization algorithm that is based on SPSA . Inthis section, we provide a detailed description of the firstcomponent, Coordinator.Different from existing works on visual prompting, wereparameterize the input space visual prompt as a neuralnetwork, Coordinator v() that generates an input-dependentvisual prompt v(x). Coordinator is composed with encoderf(), decoder gd() and task-specific learnable vector t.The encoder is used for extracting instance-specific latentfeature vector zx = f(x) contributing to the constructionof the optimal input space visual prompt for each instance.Because our goal in this work is the broad utilization ofpre-trained models on diverse downstream tasks, we adopt apre-trained encoder network optimized by a self-supervisedlearning objective, not by a supervised learning objective orscratch network. Specifically, we use the ViT-B/16 weightsfrom the Masked AutoEncoding pre-training . We presentthe grounds for using the self-supervised learning encoder inthe main paper, refer to Sec. 3. During the training phase, thispre-trained encoder part is frozen (not updated) and just acts asa feature extractor. Then, the instance-specific feature vectorfrom the encoder is conveyed to the decoder for a promptgeneration.Prompt decoder gd() is a lightweight convolutional neuralnetwork, which has learnable parameters less than 10K bydefault. Note that the generated prompt has the same shape asthe input image, so our prompt covers the entire region of theimage, unlike previous visual prompting and reprogrammingworks applied to the partial region of the image by human-designed.In addition to the feature vector from the fixed encoder,the decoder also incorporates an additional input which isshared for all instances across the current dataset. The so-called prompt trigger vector t is a continuous vector that alsocontributes to the design of a visual prompt by collaboratingwith the instance-specific rich feature vector from the encoder.By introducing this prompt trigger vector, the decoder of theCoordinator can enjoy additional information to generate more proper prompts for a given task. Besides, it helps to build the3D feature map for the decoders input, which is necessaryfor designing a parameter-efficient fully convolutional decodernetwork.To investigate whether visual prompts produced by eachmethod adapt the pre-trained model, we visualize the Grad-CAM on the original image and prompted image ofthe encoders penultimate layer (-18). We selecteight datasets that represent the diverse image domains andexperimental conditions: (Natural) OxfordPets and SVHN,(Specialized) EuroSAT, (Structured) CLEVR-count, (ActionRecognition) UCF101, (Fine-Grained) StanfordCars, (Syn-thetic) Biased-MNIST and Loc-MNIST. Detail descriptions foreach dataset are provided in Sec. A.1.BlackVIP generates diverse prompts to properly adapt thepre-trained model to the targeted data domains. When the taskis counting the number of objects (CLEVR) in the entire regionof an image, BlackVIP extends the attention of the pre-trainedmodel to the whole view of an image as shown in .If the task requires a fine-grained understanding of objects orrecognition of actions (UCF101), BlackVIP concentrates themodels attention on class-related regions, as shown in .",
  "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 202122": ".Grad-CAM on Biased-MNIST. While baseline methods attend to the background rather than digit shape, our BlackVIP can bypass this spuriousfeature through a widely scattered visual prompt and focus more of the attention on the shape of the digit. . Grad-CAM on Loc-MNIST. Compared to baseline methods, BlackVIP effectively adapts the model to aim at edge-located true digit correspondingtrue label rather than the obstructive fake digit in the center of the image."
}