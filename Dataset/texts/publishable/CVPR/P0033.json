{
  "Linshan Wu, Jiaxin Zhuang, Hao Chen, Senior Member, IEEE": "AbstractThe scarcity of annotations poses a significant challenge in medical image analysis, which demands extensive efforts fromradiologists, especially for high-dimension 3D medical images. Large-scale pre-training has emerged as a promising label-efficientsolution, owing to the utilization of large-scale data, large models, and advanced pre-training techniques. However, its development inmedical images remains underexplored. The primary challenge lies in harnessing large-scale unlabeled data and learning high-levelsemantics without annotations. We observe that 3D medical images exhibit consistent geometric context, i.e., consistent geometricrelations between different organs, which leads to a promising way for learning consistent representations. Motivated by this, weintroduce a simple-yet-effective Volume Contrast (VoCo) framework to leverage geometric context priors for self-supervision. Given aninput volume, we extract base crops from different regions to construct positive and negative pairs for contrastive learning. Then wepredict the contextual position of a random crop by contrasting its similarity to the base crops. In this way, VoCo implicitly encodes theinherent geometric context into model representations, facilitating high-level semantic learning without annotations. To assesseffectiveness, we (1) introduce PreCT-160K, the largest medical image pre-training dataset to date, which comprises 160K ComputedTomography (CT) volumes covering diverse anatomic structures; (2) investigate scaling laws and propose guidelines for tailoringdifferent model sizes to various medical tasks; (3) build a comprehensive benchmark encompassing 48 medical tasks, includingsegmentation, classification, registration, and vision-language. Extensive experiments highlight the superiority of VoCo, showcasingpromising transferability to unseen modalities and datasets. VoCo notably enhances performance on datasets with limited labeledcases and significantly expedites fine-tuning convergence. Codes, datasets, and models are available at",
  "Index TermsVision Pre-training, Foundation Models, Medical Image Analysis, Geometric Context Priors, Scalable Learners": "1INTRODUCTIONAI-driven medical image analysis has witnessed emerg-ing development in recent years , , , , ,, yet is heavily hampered by the high costs of the requiredexpert annotations, especially for large-scale 3D medical im-ages that with volumetric information , , , . Toaddress this dilemma, Self-Supervised Learning (SSL) ,, , , for pre-training foundation models havedemonstrated the potential to learn feature representationswithout the guidance from annotations, offering a promis-ing solution in addressing the annotation bottleneck in 3Dmedical image analysis , , , , .Recent advances , , , , , havehighlighted the critical elements contributing to the successof vision foundation models, i.e., large-scale data, largemodels, and advanced pre-training techniques. However,how well these solutions transfer to 3D medical image pre-training has not been thoroughly investigated. As shown in, (1) Data: previous methods , , , , ,, , are limited by the data scale (at most 10K vol-umes are used). Specifically, UniMiss , innovativelyproposed to boost chest CT pre-training by integrating 2Dchest X-rays. However, the extendability to other anatomicregions remains under-explored. (2) Models: models trained",
  "This paper is an extension of our CVPR 2024 paper .Corresponding author: Hao Chen ()": "in previous methods , , , , , , , arestill small-scale, with parameters only in the tens of millions.The scaling law of model capacity in medical image pre-training has not been well-explored. (3) Pre-training tech-niques: SuPreM focused on supervised pre-training andannotated an abdomen segmentation dataset for thispurpose. Although showcasing state-of-the-art performancecompared to previous methods, SuPreM is still con-strained by the scale of labeled data and fails to incorporatelarge-scale unlabeled data from diverse anatomical regions.In SSL, the majority of existing approaches , , ,, , , , relied on low-level informationreconstructions to learn augment-invariant representations,which typically employ data augmentation to the imagesand then reconstruct the raw information. However, thelack of high-level semantics in pre-training still impedes theperformance of various downstream tasks.The primary challenge is to incorporate high-level se-mantics for pre-training large-scale unlabeled data. We high-light that the geometric context priors of 3D medical imagescan be exploited. As illustrated in , we observe thatin 3D medical images, different organs (semantic regions)exhibit relatively consistent geometric relations with simi-lar anatomic characteristics. Thus, the consistent geometriccontext between different organs offers a promising avenuefor us to learn consistent semantic representations withoutthe guidance of annotations in pre-training.In this paper, we propose a simple-yet-effective VolumeContrast (VoCo) framework, aiming to leverage the geomet-ric context priors for contrastive learning. VoCo introduces",
  "Unimiss": "(TPAMI 24) VoCo-v1 (CVPR 24) SwinUNETR (CVPR 23) VoCo: Models with 31M~1.2B params pre-trained on 160K volumes (a) Large-scale Data(b) Scalable Learner(c) Large-scale Evaluation 11K 15K 15K 15K 15K 15K 15K 15K 15K 15K 15K 15K 15K 15K 20K 100K 80.9 82.9 82.3 86.2 79.382.885.386.6 87.2 88.8 89.3 90.9 89.1 91.4 91.9 92.7 89.7 91.5 90.9 92.3 88.1 88.8 90.2 91.4 69.3 71.3 74.5 78.478.6 78.9 80.9 63.9 63.3 62.468.7 91.091.8 93.8 97.5 98.7 72.672.9 73.6 80.8 81.2 84.4 58.7 58.9 61.9 65.1 67.8 73.0 18.1 20.2 24.1 MedCoss (CVPR 24)",
  "ALICE": "(ICCV 23) : Overview: (a) We curate a large-scale 3D medical dataset PreCT-160K for pre-training. To the best of our knowledge,it is the existing largest pre-training dataset in this field, comprising 160K CT volumes (42M slices). (b) We investigatethe scaling law in medical image pre-training, where VoCo stands out from previous methods in both data scale andmodel capacity. (c) We build a comprehensive benchmark for evaluation, which contains 48 downstream datasets acrossdifferent tasks, i.e., segmentation, classification, registration, and vision-language (VL). Extensive experiments highlight theeffectiveness of our proposed large-scale pre-training method. a novel pretext task, i.e., contextual position predictions,aiming to encode the geometric relation of different organsinto model representations. First, VoCo extracts a group ofnon-overlap base crops from different regions within aninput volume. The base crops are employed to constructpositive and negative pairs with a random crop for con-trastive learning, i.e., base crops that overlap with the ran-dom crop are assigned as positive, otherwise negative. Then,we predict the contextual positions of a random crop bycontrasting its similarity to the base crops. Intuitively, highersimilarity indicates larger overlap areas, thus we can predictwhich region the random crop belongs to by calculatingsimilarity. Specifically, we assign the overlap proportionsbetween the random crop and base crops as position labelsto supervise the position predictions. Through learning topredict contextual positions, VoCo implicitly encodes theinherent geometric contexts into the model representationswithout the guidance of annotations. As shown in , existing works , , , , , are still constrained by the size of data, resulting in alarge gap towards powerful medical vision foundation mod-els. To this end, we curate a large-scale dataset PreCT-160Kfrom publicly available sources, which currently stands asthe largest and most comprehensive dataset for medicalimage pre-training. As shown in (a), PreCT-160K com-prises over 160K CT volumes with an excess of 42M slices,encompassing the 3D anatomical map of human bodies.PreCT-160K also includes a substantial portion of labeleddata, enabling us to combine self- and semi-supervisedlearning for omni-supervised pre-training. In this paper,we propose an omni-supervised pre-training frameworkto effectively unleash the power of labeled and unlabeledmedical images. We further explore the scaling law of model capacityand develop guidelines for tailoring different model sizesto diverse medical tasks. Specifically, we build a large-scale evaluation benchmark for medical image pre-training.In contrast to previous studies , , , , , that were limited in evaluation data and tasks, ourbenchmark encompasses 48 downstream datasets spanningvarious tasks such as segmentation, classification, registra- tion, and vision-language. Extensive experimental resultson 48 downstream datasets demonstrate that our proposedVoCo significantly outperforms existing methods by a clearmargin and achieves new state-of-the-art performances.The preliminary version of this study was presented inCVPR 2024 and we named it VoCo-v1. In this paper, wemade significant and substantial modifications, retaining theinitial name as VoCo. The new contributions of this paperinclude but are not limited to: Compared to VoCo-v1 that solely focused on intra-volume contrastive learning, we further introduce inter-volume contrastive learning with a momentum-basedteacher-student module, enabling us to learn consistentrepresentations between different volumes.",
  "We investigate the combination of self- and semi-supervised learning for omni-supervised pre-training,effectively leveraging both labeled and unlabeled data": "We introduce the existing largest medical image pre-training dataset PreCT-160K and scale up the data scalefrom 10K to 160K. Our PreCT-160K is poised tofoster future research in medical image pre-training. We build the existing largest evaluation benchmarkfor medical image pre-training, encompassing diversetasks across 48 downstream datasets. Our open-sourceimplementation of various medical tasks will also ben-efit the following researchers in this field. We delve deeper into the scaling law and release pre-trained models with parameter sizes ranging from 31Mto 1.2B. We also propose the guidelines for tailoringdifferent model sizes to various medical tasks. We provide detailed and insightful analyses to under-score the core components of VoCo. These experimentsfurther highlight the significance of large-scale pre-training, offering valuable insights that can inspire fu-ture research in the field of medical image pre-training.",
  "Vision pre-training opens up immense opportunities forharnessing large-scale vision data, playing a pivotal role in": "Abdomen: The stomach is in the upper and the liver is in the right. Beneath the liver is the gallbladder. The spleenis on the left side near the stomach and the pancreas is behind the stomach. The kidneys are located on each side. Chest: The heart is in the center, slightly tilted to the left. The lungs are near the heart on either side. The abnormalities (e.g., COVID-19) are often seen in the outer regions rather than the central areas. Colorectal: Cecum is near the appendix. Transverse colon is near the gastrocolic ligament, connecting to thesplenic flexure. The descending colon is on the left near the sigmoid colon. The rectum is below the peritoneal. Cardiac: The ascending aorta (AA) is located at the top. The left atrium blood cavity (LAC) is below the AA. Adjacent to the LAC is the left ventricle blood cavity (LVC). The myocardium of the left ventricle surrounds the LVC. HeadNeck: The nasal cavity is located above the oral cavity and pharynx. The laryngeal region is below the nasalcavity and extends into the hypopharynx. The oropharynx is positioned between the nasopharynx and hypopharynx. Hip: The iliopsoas is located deep within the pelvis and connects the iliacus and psoas major muscles. The gluteus minimus and medius are on the lateral side. The gluteus maximus is located on the posterior side of the hip. : Motivation of VoCo. In 3D medical images, the geometric relations between different organs are relatively consistent.We present some examples from PreCT-160K to illustrate these anatomical relationships across different regions. Motivatedby this observation, we propose to leverage geometric context priors for learning consistent semantic representations andintroduce a novel position prediction pretext task for pre-training. the development of large vision foundation models ,, , , , . The primary challenge lies indevising effective pre-training methodologies. Although su-pervised pre-training stands as a straightforward approach,it grapples with the challenge of the lack of manual an-notations, demanding substantial engineering efforts forbuilding labeled datasets. Deng et al. built the famousImageNet and ImageNet pre-training has demonstrated itseffectiveness in boosting downstream tasks. SAM intro-duced the SA-1B dataset with over 1 billion segmentationmasks for supervised pre-training, thus achieving a strongsegmentation foundation model. However, the high costs ofannotations and the neglect of large-scale unlabeled datastill hinder the further development of supervised pre-training. To this end, SSL proposed to learn robust featureswithout the guidance of annotations , , , , ,which has garnered significant attention recently. Typical SSL methods. SSL has showcased promisingresults across various vision tasks , , , ,, . DINO , proposed to integrate advancedSSL methods and learn robust features without annota-tions, which has become a prevalent choice for pre-trainedbackbones in contemporary research. State-of-the-art SSLmethods can broadly be classified into generative , ,, and contrastive , , , , , , ,, learning methods. (1) Generative learning methodsare mainly based on reconstructing raw information fromaugmented images. For example, MAE proposed tomask random patches of the input image and reconstructthe missing pixels. (2) Contrastive learning methods aim tolearn consistent representations by contrasting positive andnegative pairs of samples. Transfer to medical image analysis. Although the meth-ods discussed above have achieved promising outcomes innatural images, directly applying these pre-trained modelsto medical images encounters challenges due to domaingaps , , , , , . DINO , pre-traineda series of strong 2D vision Transformers and exhibitedsignificant transferability in 2D medical images like X-rayand pathology images , , . However, in the realmof challenging 3D medical tasks that necessitate volumetric information extraction, strong pre-trained 3D models arestill under-explored , , , .Most state-of-the-art SSL methods , , , , often fall short in achieving competitive performancesin 3D medical images, primarily caused by the ignorance ofthe unique characteristics of 3D medical images , , ,. Specifically: (1) Contrastive learning , in naturalimages proposed to build positive and negative pairs ofsamples in a training batch, i.e, the augmented view ofinput is assigned as positive, and other images are negative.However, for 3D medical images that share similar anatom-ical structures, it is difficult to build negative pairs in thisway , , , . (2) Mask image modeling , proposed to mask and reconstruct missing pixels. However,for 3D medical images characterized by high dimensions,large sizes, and a significant background proportion, thesemethods often encounter issues as models tend to convergetowards reconstructing irrelevant background regions ,, , , , , diminishing the understanding ofsemantic regions (e.g., organs). Thus, the development of ad-vanced SSL techniques for 3D medical images necessitates ameticulous consideration of the unique image informationand the formulation of tailored strategies.",
  "Large-scale Medical Image Pre-training": "Medical image pre-training has been proven as an effec-tive way to mitigate the scarcity of annotation in medicaltasks , , , , , . Early attempts , , conducted pre-training on 2D X-ray images , ,demonstrating improvements on chest pathology identifi-cation and pneumothorax segmentation. In comparison, 3Dmedical images, e.g., CT and Magnetic Resonance Imaging(MRI), offer richer volumetric information for clinical diag-nosis, which has received increasing attention in medicalimage analysis , , , , , . Nonethe-less, the complexity inherent in 3D medical images intro-duces significant challenges to pre-training. Although recentworks , , , , , , have demonstratedthe effectiveness of 3D medical image pre-training, signifi-cant challenges still persist, particularly in the realms of datascale, model capacity, and pre-training methods.",
  "Large-scale Data": "Compared with 2D X-ray, collecting 3D medical imageslike CTs is more difficult, stemming from factors such asslower imaging speeds, heightened radiation exposure, andincreased costs , . As shown in (b), most exist-ing methods , , , , , leveraged limitedscale of 3D data for pre-training. FreeTumor first in-vestigated the data-scaling law in tumor segmentation with11K CTs. Wang et al. built a dataset of 100K CTs for pre-training but it is not publicly available for research. To col-lect large-scale 3D data for pre-training, the necessity arisesto aggregate datasets from diverse sources, encompassingvarious hospitals across different regions and countries ,. This will lead to diverse image characteristics andinconsistent imaging quality in the dataset, introducing newchallenges to pre-training.Moreover, previous methods mainly collected data fromspecific body parts for pre-training, e.g., PCRL , andUnimiss , on chest region, Alice and SuPreM on abdomen region, GVSL on cardiac region. However,given the distinct characteristics present in various anatom-ical regions, the transferability of models pre-trained onone region to another may be constrained , , .In this paper, we build a large-scale dataset PreCT-160Kthat encompasses diverse anatomic structures. However,data sourced from different anatomical regions exhibit vary-ing imaging parameters, i.e., different sizes, spacing, andintensities, posing new challenges for learning consistentrepresentations in pre-training.",
  "Large Model": "Early works , , in 3D medical image pre-training were constrained in model capacity, typically com-prising only tens of millions of parameters. Recent ad-vances , , , , have demonstrated theastonishing effectiveness of scaling law, where large modelstrained on large-scale data exhibit remarkable intelligence.In this paper, we collect a large-scale 3D medical imagedataset, which comprises diverse image characteristics fromvarious sources. The availability of such extensive dataunlocks new opportunities for us to train large models.Given the diversity of various medical tasks, it is im-perative to evaluate large models on comprehensive bench-marks. Previous methods , , , , , , primarily evaluated the pre-trained models on only a fewdownstream tasks, typically focusing on segmentation orclassification tasks. STU-Net was the first to evaluatelarge models yet is limited in segmentation tasks. In this pa-per, we delve deeper into the scaling law in various medicaltasks, providing insights into tailoring diverse model sizesto accommodate varying medical tasks effectively.",
  "Advanced Pre-training Techniques": "SSL for 3D medical images. Existing methods , ,, , , are mostly based on information recon-structions to learn augment-invariant representations of 3Dmedical images, which first employ strong data augmenta-tion to the images and then reconstruct the raw informa-tion. Rotate-and-reconstruct , , , proposedto randomly rotate the 3D volumetric images and learn to pancreas stomach cavaaorta vein position encoding: base crops input volume",
  "Volume Contrast": "positive negative 0.3 0.3 0.2 0.2 overlap proportions as position labels 0: 0.0 1: 0.2 2: 0.3 3: 0.0 5: 0.2 6: 0.3 stomach, pancreas, vein, aorta, cava liver, kidney, gall, adrenal, colon supervision position labels random crop 4: 0.0 : Generate position labels for supervision. A pair ofrandom crop k and base crop q are assigned as positive ifthey share overlap areas, otherwise as negative. We calculatethe overlap proportions as position labels y, e.g., y1, y2, y5, y6are assigned as 0.2, 0.3, 0.2, 0.3, respectively. recover them, fostering the learning of rotation-invariantfeatures. Recent methods , , , , , delved into restoring low-level information across variedimage perspectives. PCRL , cropped global and localpatches then conducted multi-scale restorations. GVSL further explored the geometric similarities among multi-scans through affine augmentation and matching. Mask-reconstruct methods , , , , were derivedfrom MAE , aiming to learn representations by maskingimages and reconstructing the missing pixels. Althoughpromising results have been demonstrated, the majority ofthese approaches often overlook the significance of integrat-ing high-level semantics into model representations, thusimpeding the further improvements in downstream tasks. High-level semantics in pre-training. For medical im-ages, high-level semantic information primarily stems frommanual annotations, since it heavily relies on expert knowl-edge. Previous works , , , proposed thatsupervised pre-training is more efficient and can achievehigher performances with less training time and labeleddata . However, the ongoing challenges persist in thescarcity of labeled data, impeding the transferability tovarious medical tasks, different anatomical structures, andextensive unseen datasets. In this paper, we aim to inte-grate large-scale unlabeled data into pre-training. Thus, wepropose to leverage the inherent characteristics of medicalimages as high-level semantic priors for self-supervision. Omni-supervised Learning. Although self-supervisedlearning enables us to involve large-scale unlabeled datain pre-training , , , it still overlooks the utiliza-tion of readily available labeled data. Omni-supervisedlearning , , , introduced the concept ofleveraging diverse information for supervision. Specifically,semi-supervised learning , , , demonstratespowerful efficacy in leveraging both labeled and unlabeleddata. In this paper, we propose a simple-yet-effective omni-supervised pre-training framework, which combines self-and semi-supervised learning to unleash the power of bothlabeled and unlabeled medical images. base crops positive base crops negative base crops model random crop teacherprojector studentprojector EMA stop gradient Volume Contrastsimilarity position labels sup. (a) Contextual Position Prediction random crop input volume similarity position labels supervision (b) Intra-Volume Contrast pull push EMA (c) Inter-Volume Contrast",
  "Volume": ": Overall framework of VoCo. (a) First, we generate base crops q with corresponding position labels y (Sec. 3.1 &). Then we input the random crop k and base crops q for contextual position prediction. Specifically, we employ astudent-teacher module to project k and q separately, where the teacher projector is frozen and updated from the studentprojector with Exponential Moving Average (EMA). Finally, we conduct volume contrast between k and q to predictsimilarity s (Eq. 2), where s is supervised by position labels y (Eq. 4). (b) We use the position labels to supervise theintra-volume contrast on k, qpos, and qneg, where k, qpos, and qneg are from the same volume. (c) We extract random cropkA and base crops qB from different volumes VA and VB for inter-volume contrast.",
  "Generate Position Labels for Supervision": "The pivotal procedure is to generate position labels forself-supervision. We propose to leverage the inherent geo-metric context priors in 3D medical images. As illustratedin , given an input volume V , we first randomlycrop a sub-volume k, with the objective of constructingpositive and negative pairs with k for contrastive learning.Specifically, we propose to employ position encoding togenerate n non-overlap base crops qi, in. For example,n = 44 base crops are generated in , where each basecrop qi represents a distinct region of the input volume.Within human body anatomy, various organs are situ-ated in distinct regions, leading to a potential way for usto form positive and negative pairs. As shown in ,the random crop k and the positive base crops qpos exhibitoverlap areas, whereas the negative base crops qneg, lackingsuch overlaps, more likely encompass different organs (notabsolutely). For example, in , k and qpos both containstomach, pancreas, vein, aorta, and cava, while k and qnegexhibit different organ information. Thus, we can employthe position encoding to construct positive and negativepairs for contrastive learning.Previous contrastive learning methods , , , mainly employ InfoNCE loss to maximize themutual information of positive pairs. In this paper, wepropose to generate labels with specific values to supervisethe correlation extent of positive pairs, i.e., with labels toreflect how similar between k and qpos. It can be observedthat the correlation between k and qpos is associated withtheir overlap proportions. Intuitively, if a positive base cropqpos shares more overlap areas with k, this qpos wouldbe more similar with k. Thus, as shown in , wepropose to assign the overlap proportions as the valuesof position labels y, enabling us to measure the similarity between k and qpos. In contrast, the position labels y ofqneg are assigned to 0. In this way, we leverage the overlapproportions between k and q to supervise the contextualposition prediction results.",
  "Volume Contrast for Contextual Position Prediction": "The overall framework of VoCo is present in .Specifically, we propose a novel pretext task, i.e., contextualposition prediction, which employs volume contrast to pre-dict the contextual positions of a random crop k. This pretexttask includes: (1) intra-volume contrast among k, qpos, andqneg, where k, qpos, and qneg are from the same volume;(2) inter-volume contrast between different volumes VA andVB, which is established by consistency regularization witha typical student-teacher module , , , .Contextual position prediction. As shown in (a),given an input volume, we first extract a random crop k anda group of base crops q, where the corresponding positionlabels yi for qi are generated as Sec. 3.1 and . Thenwe feed k and q into the model to extract high-dimensionfeatures. After extracting the features, we employ a typi-cal momentum-based student-teacher module , toproject k and q separately. Specifically, the teacher projectorpt is frozen during training, where its parameters t areupdated from the parameters s of the student projector psby Exponential Moving Average (EMA):",
  "t = t + (1 )s,(1)": "where is the momentum factor and is empirically setto 0.9. The momentum-based student-teacher module iseffective in contrastive learning , , which enablesstable training and avoids feature collapse , , .With features extracted from the projectors, we conduct3D adaptive average pooling to resize k and q as onedimension features, i.e., kR1C and qR1C, where C is",
  "k qi , in,(2)": "where si denotes the similarity between k and qi, whichranges from 0 to 1.Intuitively, higher si represents that k has higher prob-abilities to share overlap proportions with qi. In this way,we can predict the contextual position by calculating thesimilarity s. Then, we leverage the generated position labelsy to supervise the predicted similarity s. The formulation ofprediction loss function Lpred is based on entropy. Specifi-cally, we first calculate the distance d between similarity sand position labels y:",
  "inlog(1 di).(4)": "Remark. Although we assign the position labels y as 0 forall negative base crops qneg, there are instances where therandom crop k may resemble some qneg. Without labels dur-ing pre-training, constructing absolutely ideal negative pairsin contrastive learning remains challenging , , .Nevertheless, the overall distances among negative pairsremain substantial. Thus, following previous methods ,, , we adopt the average entropy of distances in Eq. 4.Intra-volume contrast. As shown in (b), we con-duct intra-volume contrast on a triplet: random crop k, posi-tive base crop qpos, and negative base crop qneg. Specifically,we pull k and qpos closer, push k and qpos, qpos and qnegapart from each other. For random crop k, we use positionlabels y to supervise the process of contrastive learning. Forq, we design a regularization loss Lreg to enforce the featurediscrepancy between each pair of qi and qj:",
  "qi qj , i, jn, i=j.(6)": "Inter-volume contrast. As shown in (c), we extractrandom crop kA from volume VA and base crops qB fromvolume VB to establish inter-volume contrast, where vol-umes VA and VB are sampled from the same batch duringtraining. It is worth noting that VA and VB are sampled fromthe same anatomical region, e.g., both from the abdomenregion or both from the chest region.Specifically, we adopt a simple-yet-effective consistencyregularization method , , for inter-volume con-trast. We first employ feature augmentation (aug. in )to kA, qB and get kA, qB, The augmentation here is a simpleDropout as . Then we fed the features before andafter augmentation into ps and pt, respectively. After the Fully-supervised LearningSelf-supervised LearningOmni-supervised Learning : Differences among fully-, self-, and omni-supervisedlearning. Solid and hollow markers denote labeled andunlabeled data, respectively. Dashed lines denote decisionboundaries between different classes.",
  "Self-supervised training M XU [LSSL Eq. 9];": "As shown in , both fully- and self-supervisedlearning have specific merits and drawbacks. (a) Fully-supervisedlearningcanlearndiscriminativedecisionboundaries with the guidance of labels yet it is constrainedby the lack of labeled data. (b) SSL can leverage large-scaleunlabeled data. However, lacking annotations for supervi-sion, it struggles with learning clear decision boundariesbetween distinct classes. To this end, we propose omni-supervised pre-training to effectively leverage both labeledand unlabeled data, as described in Algorithm 1. Our omni-supervised learning amalgamates the strengths of bothfully- and self-supervised learning and effectively unleashesthe potential of labeled and unlabeled data.Curating labeled segmentation dataset XL, YL. ThePreCT-160K dataset includes extensive labeled segmenta-tion datasets. However, many of these datasets have in-complete labels , , , such as one dataset containing",
  "Total160167": "solely liver labels and another with only pancreas labels.Thus, we first ensemble various models to generate com-plete labels YL for XL and curate a small subset of labeleddata from PreCT-160K. This subset, which we named Vo-Comni, contains 20K volumes spanning 20 different organand tumor classes, which will be released with PreCT-160Kfor fostering future research. All the val and test sets areunseen in PreCT-160K and VoComni.Semi-supervised learning is a scalable learner. To effec-tively leverage labeled and unlabeled data, we propose toconduct semi-supervised learning , , to borrowthe knowledge from labeled data to large-scale unlabeleddata. Notably, segmentation emerges as a pivotal techniquein supervised training , , , given that many medi-cal tasks demand a granular understanding at the pixel levelfor accurate diagnosis. Previous works , , onlyleveraged a few hundred cases for semi-supervised segmen-tation. However, complex designs of semi-supervised seg-mentation will significantly increase the burden of training,which is not feasible to our large-scale data. In this paper, weadopt a simple semi-supervised learning baseline and scaleup the data to 160K volumes. We find that incorporated withVoCo, the simplest semi-supervised baseline can alreadyachieve competitive results. As shown in Algorithm 1, wefirst curate a labeled segmentation dataset (XL, YL) fromPreCT-160K and perform supervised segmentation trainingin the first stage. Then in the second stage, we generatepseudo labels YU for unlabeled data XU, aiming to per-form semi-supervised learning on both XL and XU. Notethat SSL is collaboratively integrated with semi-supervisedtraining in both two stages. In this way, we amalgamate thestrengths of self- and semi-supervised learning, advancingtowards omni-supervised pre-training.",
  "Dataset and Implementation Details": "Pre-training dataset1. In this paper, we curate the exist-ing largest dataset medical image pre-training PreCT-160K,as shown in . PreCT-160K is collected from diversesources and underwent thorough pre-processing to ensurea consistent data format for training. Specifically, to addressvariations in sizes, spacing, and intensity across volumesfrom different anatomical regions, we have devised tailoredpre-processing protocols. Since in PreCT-160K, data fromchest regions cover larger proportions, we simply balancethe sampling during pre-training. For VoComni dataset2, weuse model ensembling to generate pseudo labels, where wediscard the volumes with low prediction confidence. Conse-quently, we have created a segmentation dataset comprising20K pseudo-labeled volumes (encompassing 20 organ andtumor classes) for our omni-supervised pre-training.Evaluation benchmark. We build a large-scale evalua-tion benchmark as shown in , which includes 48downstream datasets for various tasks. It can seen in that our evaluation benchmark is more comprehensive thanthat of previous works , , , , , , . Anumber of datasets , , , are evaluated on thepublic leaderboards. If the test sets and public leaderboardsare not available, we report the offline val sets results withthe same data splits for fair comparisons.Experiment settings. We first conduct pre-training onPreCT-160K then finetune the pre-trained models on 48downstream datasets () for evaluation. We adoptboth SwinUNETR and nnUNet as the backbones",
  "DatasetModalityTask": "BTCV CTAbdomen Seg.AMOS22 CTAbdomen Seg.WORD CTAbdomen Seg.FLARE22 CTAbdomen Seg.FLARE23 CTAbdomen Seg.Abdomenct1k CTAbdomen Seg.AbdomenAtlas CTAbdomen Seg.TotalSegmentator CT104 Structures Seg.MM-WHS CTHeart Seg.AVT CTAorta Seg.CHAOS CTLiver Seg.Sliver07 CTLiver Seg.IRCADb CTLiver Tumor Seg.KiTS CTKidney Tumor Seg.KiPA22 CTKidney Tumor Seg.TCIA-Panc. CTPanc. Seg.PANORAMA CTPanc. Tumor Seg.SegThor CTThoracic Risk Seg.BHSD CTBrain Bleed Seg.StructSeg19 CTNasopharynx Cancer Seg.Verse20 CTVertebrae Seg.Covid-19-20 CTCovid Seg.FUMPE CTPulmonary Embolism Seg.Parse22 CTPulmonary Artery Seg.AIIB23 CTFibrotic Lung Seg.CC-CCII CTCovid Classi.LUNA16 CTLung Nodule Classi.AutoPET-II23 PET-CTHeadNeck Lesion Seg.AMOS-MRI MRIAbdomen Seg.MM-WHS-MRI MRIHeart Seg.ACDC MRIHeart Seg.ATLAS-MRI MRILiver Tumor Seg.BraTs21 MRIBrain Tumor Seg.IXI MRIBrain MRI RegistrationOASIS MRIBrain MRI RegistrationCTRG-Chest VLPReport GenerationCT-RATE VLPVocabulary Classi.CT-RATE VLPReport-Volume Retrieval MSD Challenge Task01 BrainMRIBrain Tumor Seg.Task02 HeartMRIHeart Seg.Task03 LiverCTLiver Tumor Seg.Task04 Hip.MRIHip. Seg.Task05 Pros.MRIProstate Seg.Task06 LungCTLung Cancer Seg.Task07 Panc.CTPancreas Tumor Seg.Task08 VesselCTVessel Tumor Seg.Task09 SpleenCTSpleen Seg.Task10 ColonCTColon Cancer Seg.",
  "VoCo48": "for pre-training. Specifically, Swin-Base (B), Swin-Large (L),and Swin-Huge (H) are all adopted for training, with featuresizes of 48, 96, and 192 in SwinUNETR , respectively.This project is supported by NVIDIA SuperPOD hardware.8 NVIDIA H800 GPUs are used for pre-training and allthe downstream tasks can be done with one H800 or 3090GPU. It spent over 10,000 GPU hours in downstream eval-uation. Our implementation codes are all open-source andsupport both MONAI and nnUNet frameworks.",
  "Comparison with State-of-the-Art Methods": "We perform in-depth comparisons with previous meth-ods , , , , , , , , , thathave released their codes and checkpoints. Note that ininstances where certain datasets necessitate extensive com-putational resources or involve limited cases, we exclusivelyreport the results of methods , , that with betterperformances. Our evaluations span across segmentation,classification, registration, and vision-language tasks. In fol-lowing discussions, the term baseline denotes adopting thesame backbones but without pre-training (from scratch).",
  "Medical Image Segmentation": "Seven widely-used segmentation datasets. As shown in, on seven widely-used segmentation datasets, VoCodemonstrates leading performances and surpass previousmethods , , , , , , , by a clearmargin. It can be seen that the general method MoCo-v3 , did not perform well on medical tasks. SinceMoCo v3 , heavily relies on a large batch size toacquire adequate negative samples, which is not feasible in3D medical images. Moreover, the negative relation betweendifferent images used in MoCo v3 , is not appropri-ate in medical images.Notably, VoCo outperforms the baseline by average3.12% DSC. SuPreM achieved the best results amongthe previous pre-training methods since it used an ab-domen dataset for supervised pre-training and thedatasets in are almost abdomen datasets. VoCo sur-passes SuPreM and achieves new state-of-the-art per-formances. Specifically, for the challenging ToTalSegmen-tor dataset, VoCo (Swin-H) outperforms SuPreM by 3.22% DSC. The overall results in vividly under-score the effectiveness of our method.24 organ/tumor segmentation tasks. As shown in Ta-ble 5, we report the results on 24 organ and tumor segmen-tation datasets, across different modalities and anatomicalregions as shown in . Notably, models with VoCo pre-training outperform those without pre-training by average4.42% DSC. It is worth noting that a majority of thesedatasets contain fewer than 50 annotated cases for fine-tuning, highlighting the effectiveness of pre-training as alabel-efficient solution. The overall improvements observedacross these 24 datasets serve as compelling evidence for theefficacy of our proposed large-scale pre-training method.MSD Challenge. reports the results on the MSD10-Task dataset. We adopt the settings of nnUNet for fair comparisons. Notably, with VoCo pre-training, thesegmentation DSC is improved by average 2.98%. TABLE 4: The DSC (%) of seven widely-used segmentation datasets, i.e., BTCV , AMOS22 , WORD , FLARE22, FLARE23 , TotalSegmentator , and AbdomenAtlas . The state-of-the-art results among previous methodsare underlined while the best results are bolded. Note that , , are fully-supervised pre-training methods. Since, , require huge computation costs, we only report the results of advanced methods for comparisons. Comparedwith models without pre-training, VoCo pre-training brings average +3.12% DSC improvements.",
  "Medical Image Classification": "ThemedicalimageclassificationresultsonCC-CCII and LUNA16 are shown in . Giventhe near-optimal accuracy of lung nodule detection onLUNA16 , the benefits of pre-training are not obvious.For Covid classification on CC-CCII , VoCo outper-forms the baseline by 2.76% and SuPreM by 1.97%.Notably, SuPrem conducted supervised segmentationpre-training on only abdomen datasets, potentially limitingits transferability to chest classification tasks.",
  "Medical Image Registration": "The medical image registration results on IXI andOASIS datasets are shown in . We adoptTransMorph as the baseline. Note that in this paperwe focus on evaluating the effectiveness of pre-training,thus we did not propose new registration algorithms. Thus,our registration analyses emphasize backbone comparisons(scratch versus pre-trained). We find that previous pre-training methods , did not perform well on registra-tion. While on brain MRI registration dataset OASIS ,VoCo brings 2.6% DSC improvements, which is a non-trivial improvement in registration.",
  "Vision-Language Analysis": "As shown in , this study pioneers the assessmentof medical image pre-training efficacy in Vision-Language(VL) tasks. Specifically, we evaluate the report generationtask on CTRG-Chest and extend the evaluation tovocabulary classification and report-volume retrieval on theCT-RATE dataset. The results are shown in Tables 9and 10. Note that in this paper we focus on medical imagepre-training, thus we verify the effectiveness via replacingthe vision encoders. For the language models, we maintainthe original settings from M2KT and CT-CLIP forCTRG-Chest and CT-RATE, respectively.VoCo attains superior performances compared to pre-vious medical image pre-training methods , , ,. Specifically, for report generation in , VoCo(Swin-H) achieves the highest score BLEU-4 (37.91%). In Ta-ble 10, VoCo (Swin-H) achieves 73.69% AUC in vocabularyclassification and 24.12% recall in report-volume retrieval.Although SuPreM performs well in abdomen segmenta-tion datasets, it falls short in enhancing chest VL tasks. Theresults from VoCo underscore the significance of a robustvision encoder in VL tasks, which can provide more precisevisual information for language models.",
  "Discussion": "Overall improvements. As shown in , with thesame backbone, VoCo outperforms the baseline (fromscratch) by a clear margin. SuPreM emerged as thetop performer among previous pre-training methods ,, , , , , , , , . Specifically,VoCo surpasses SuPreM by an average of 2.93%, 3.72%,2.57%, 2.18%, 3.52%, and 2.72% on 24 organ segmentationdatasets, 14 tumor segmentation datasets, 15 chest analysisdatasets, 28 unseen datasets, 13 cross-modal datasets, and",
  "Chest Region": "(15 tasks, Seg., Cls., VL) Transfer: CT --> MRI(13 tasks, Seg., Reg.) Swin-B SuPreM VoCoSwin-B SuPreM VoCoSuPreM VoCoSwin-B Swin-B SuPreM VoCoSuPreM VoCoSwin-B Transfer to Unseen datasets(28 tasks, Seg., Cls.,Reg.,VL) SuPreM VoCoSwin-B Label-efficent Segmentation (18 tasks) : Overall comparisons. Swin-B denotes using therandomly initialized SwinUNETR as the backbone.Both SuPreM and VoCo use Swin-B as backbonesfor pre-training. Given the significant representation of chestdatasets within our benchmark, we present the enhance-ment outcomes across 15 chest analysis tasks. BTCV DSC (%)SegThor DSC (%)TotalSegmentator DSC (%) Traning epochsTraning epochsTraning epochs Swin-B VoCo-BVoCo-B Swin-B Swin-B VoCo-B :Efficientfinetuning.AnalysisonBTCV,SegThor,andTotalSegmentator,whereSegThorisunseeninpre-training.Comparedwith the randomly initialized backbone Swin-B , VoCoachieves higher accuracy within fewer training epochs. 18 label-efficient segmentation datasets, respectively. Con-sistent improvements on 48 downstream datasets providestrong evidence of the effectiveness of VoCo.Transferability to unseen datasets. As shown in ,our evaluation benchmark encompasses 28 datasets unseenin pre-training. As shown in , VoCo demonstratesan average improvement of 3.53% over the baseline Swin-B when evaluated across these 28 unseen datasets.Transferability to unseen modality. We conduct pre-training on CT datasets and subsequently transfer thelearned models to another 3D medical imaging modality, i.e.,MRI. As shown in , our benchmark encompasses 13MRI datasets spanning various tasks such as segmentationand registration. As shown in , VoCo yields an averageimprovement of 3.52% across these 13 datasets, underscor-ing its efficacy in facilitating cross-modal transferability.Label-efficient solution. In 3D medical image analysis,many datasets suffer from the scarcity of labeled data, pri-marily due to the substantial costs of annotation. As shownin , there are 18 segmentation datasets with less than50 labeled cases for finetuning. As shown in , VoCoemerges as a label-efficient solution tailored for datasetsconstrained by limited labeled data, consistently deliveringsuperior performances. Pre-trained backbones. We use both nnUNet andSwinUNETR for pre-training. Although nnUNet emerged as a strong segmentation baseline, it is not ascalable network architecture , with only 31M modelparams. Thus, we primarily focus on investigating thescaling law of SwinUNETR. Our analysis reveals that thepre-trained SwinUNETR exhibits more substantialenhancements compared to the pre-trained nnUNet, i.e.,+3.34% and +1.98% DSC on 34 segmentation datasets (Ta-ble 5 and 6). The relatively modest improvements observedin pre-trained nnUNet could potentially stem from varia-tions in pre-processing strategies, given nnUNets relianceon a distinct data-fingerprint processing technique.Efficient finetuning. Previous works , , proved that strong pre-training models can notably expeditetraining convergence, resulting in improved performancewith fewer training epochs. As shown in , VoCosubstantially expedites the training convergence speed onBTCV , Segthor , and TotalSegmentator , andthis phenomenon is generalized in all 48 tasks. This is anon-trivial contribution to efficient finetuning, particularlybeneficial for datasets demanding extensive computationalresources . Our pre-trained models are poised to savecomputation costs in medical image analysis, making astrong step towards efficient learning.Failure cases. Although consistent improvements (atleast 1%) are observed on 48 datasets, marginal improve-ments persist in a handful of cases. Specifically, VoCo gainsless than 1.5% improvements on 5 of 48 datasets. The pres-ence of challenging datasets, e.g., Positron Emission Tomog-raphy (PET) dataset AutoPET poses unique obstacles,primarily due to their distinct imaging characteristics com-pared to our pre-training datasets. These differences resultin domain gaps that constrain the effectiveness of our pre-training.",
  "Scaling Law in Medical Image Analysis": "Are larger models always better? In medical tasks, theanswer appears to be no. It can be observed from that for some specific tasks, models with smaller sizes canachieve better performances. In this paper, we delves intofactors affecting the model capacity scaling law, including:number of finetuning cases, data diversity, and task difficulties.As shown in , (a) TotalSegmentator is a chal-lenging dataset, containing 1.2K cases and 104 classes forsegmentation. In this case, the largest model VoCo-H yieldsthe best results. (b) BTCV is with only 24 cases forfinetuning, potentially leading larger models to overfit onlimited data, thus hindering validation performance. (c) Al-though CC-CCII encompasses 4.2K cases for training,it is a simple binary classification task (over 90% accuracy),suggesting that excessively large models may not be neces-sary. (d) OASIS is brain MRI datasets with only 0.4Kcases for registration and it also lacks significant structuraldiversity. In this case, the smallest VoCo-B delivers the bestresults. (e) CT-Rate is with 50K cases for 18 classesvocabulary classification. Given large-scale data for training,larger models demonstrate higher performances.Tailor different model sizes to various medical tasks.Drawing from experimental insights discussed above, we GFLOPs (log scale)",
  "GFLOPS": "Swin-B Swin-L Swin-H VoCo-B VoCo-L VoCo-H (a) TotalSegmentator (1.2K cases, 104 classes segmentation) (d) OASIS (0.4K cases, Brain MRI registration) (b) BTCV (30 cases, 13 classes segmentation) (c) CC-CCII (4.2K cases, Covid classification) (e) CT-RATE (50K cases, 18 classes Vocabulary classification) (f) Mean-Std (10 tasks) Swin-BSwin-LSwin-HVoCo-BVoCo-LVoCo-H Mean: 84.32 Std: 6.76 Mean: 87.64 Std: 6.59 Mean: 84.88 Std: 6.19 Mean: 87.22 Std: 6.23 Mean: 84.12 Std: 6.41 Mean: 87.34 Std: 6.09 GFLOPs (log scale)GFLOPs (log scale) GFLOPs (log scale)GFLOPs (log scale) Swin-B Swin-L Swin-H VoCo-B VoCo-L VoCo-H Swin-B Swin-L Swin-H VoCo-B VoCo-L VoCo-H Swin-B Swin-L Swin-H VoCo-B VoCo-LVoCo-H Swin-BSwin-L Swin-H VoCo-BVoCo-LVoCo-H : Are larger models always better? The answer appears to be no. We present the scaling results of TotalSegmenta-tor , BTCV , CC-CCII , OASIS , and CT-RATE in (a)-(e), respectively, covering various downstreamtasks. We compared our pre-trainined models with the randomly initialized models , taking into account both accuracyand computation costs (GFLOPs computed for a 969696 size of volume, shown in (a)). (f) presents the mean andstandard deviation (STD) values across 10 downstream tasks , , , , , , , , .",
  ": Data scaling law. We scale up the data from 10K to160K and report the DSC (%) of TotalSegmentator": "empirically propose simple and reasonable guidelines fortailoring various medical tasks: (1) Tasks with extensivelabeled data for fine-tuning potentially benefit from largermodels. (2) Tasks spanning diverse anatomical regions po-tentially benefit from larger models. (3) Tasks requiringrecognition across a higher number of classes (more chal-lenging) are better addressed with larger models.Although these guidelines have been assessed on ourcomprehensive benchmark, they may not universally applyto all medical tasks given the substantial diversity withinthe medical domain. Thus, we release pre-trained models",
  "Ablation Studies": "Our preliminary investigation VoCo-v1 has providedfundamental ablation studies, focusing on exploring variousloss functions and hyperparameter configurations. Com-pared with VoCo-v1 , we further evaluate the effective-ness of volume contrast, omni-supervised learning, anddata scaling from 10K to 160K. We use Swin-B asthe backbone and present the results on diverse datasets,including TotalSegmentator , BTCV , CC-CCII ,OASIS , and CTRG , across segmentation, classifi-cation, registration, and vision-language tasks.Volume contrast. As shown in , inter-volumecontrast consistently enhances performance across fivedatasets. The combination of intra- and inter-volume con-trast can yield higher improvements compared with therandomly initialized backbone .Omni-supervised pre-training. As shown in ,semi-supervised learning can effectively improve the per-formances. Specifically, for TotalSegmentator , it leadsto substantial DSC improvements from 82.07% to 84.84%,which is a non-trivial boost in this challenging segmentationdataset. It is worth noting that the pure semi-supervised pre-training can achieve competitive results on segmentationtasks , , but it does not improve significantly inclassification, registration, and VL tasks. Combined withself- and semi-supervised learning, the omni-supervisedpre-training can achieve the best performances.Data scaling law in medical image pre-training. Wescale up the pre-training data () from 10K to 160K andpresent the findings on the TotalSegmentator dataset in, showcasing the impact of expanding the pre-trainingdataset. Notably, the enhancements from 10K to 160K ap-pear marginal. This phenomenon could be attributed tofactors like data quality and diversity, network scalability,or nearing the upper limit of improvement. Training steps Position prediction loss Base cropsRandom cropsPredicted positions from scratch SuPreM : Case study for contextual position prediction. (1) The left part shows the contextual position prediction results.Specifically, we set thresholds for the prediction logits to output the most probable positions. The predictions closelymatch the original positions of random crops. The bottom left is a failure case where two regions share similar structures.(2) As shown in the right part, the position prediction loss converges steadily during pre-training. We further verify theposition prediction results of the model from scratch and the pre-trained SuPreM model. Notably, through supervisedsegmentation pre-training, SuPreM also enhances the contextual position prediction capability, implicitly indicating apositive correlation between segmentation performance and our proposed contextual position prediction.",
  "Qualitative Visualization Results": "Contextual Position Prediction. As shown in ,we present some visualization results of contextual positionprediction. The loss for contextual position prediction con-verges steadily during pre-training. The position predictionsgenerated by VoCo closely align with the ground truth,underscoring the efficacy of our proposed pretext task.Qualitative segmentation results. We present some vi-sualization results in , which covers different anatom-ical regions. The visualization results demonstrate that ourmethod can broadly apply to various downstream tasks.",
  "Although our pre-training method has demonstratedpromising results across various medical tasks, there are stillseveral limitations that can be further explored in the future:": "Data engines for improving data quality. The im-provements become marginal when scaling the datafrom 10K to 160K. Although we have curated and pre-processed the pre-training dataset, the PreCT-160K still inevitably includes numerous low-quality cases. Dataquality plays a pivotal role in pre-training to fullyleverage the potential of large-scale datasets , ,, . In the future, we will focus on constructingdata engines to improve the quality of datasets. Data diversity to encompass distinctive image char-acteristics. As discussed in Sec. 4.2.5, VoCo showsmarginal enhancements in a few downstream taskscharacterized by unique imaging features. Given theextensive diversity of medical datasets, we will furtherenhance the diversity of our pre-training dataset infuture endeavors. Multi-modal pre-training. In this study, we exclusivelyutilize CT data for 3D medical image pre-training.In the future, we will also build a large-scale MRIpre-training dataset and combine with CT to facilitatemulti-modal 3D medical image pre-training.",
  "CONCLUSION": "In this paper, we proposed a simple-yet-effective VolumeContrast (VoCo) framework for large-scale 3D medical im-age pre-training. Inspired by the consistent geometric re-lation between different organs, we propose to leveragethe geometric context priors to learn consistent semanticrepresentations for SSL. VoCo can also be seamlessly inte-grated into a semi-supervised learning framework for omni-supervised pre-training. To facilitate the study of large-scale 3D medical image pre-training, we curated the exist-ing largest medical image pre-training dataset PreCT-160K,which encompasses 160K CT volumes (42M slices) coveringdiverse anatomical structures. We further delve into thescaling law of model capacity and propose the guidelinesfor tailoring different model sizes to various medical tasks.To evaluate the effectiveness of pre-training, we establisha comprehensive evaluation benchmark encompassing 48downstream datasets across various tasks. Extensive ex-periments highlighted the superior performances of VoCocompared with previous methods. This work was supported by Hong Kong Innovationand Technology Fund (Project No. ITS/028/21FP and No.MHP/002/22), and Research Grants Council of the HongKong Special Administrative Region, China (Project No.T45-401/22-N)."
}