{
  "Abstract": "Motion Expression guided Video Segmentation is a chal-lenging task that aims at segmenting objects in the videobased on natural language expressions with motion de-scriptions. Unlike the previous referring video object seg-mentation (RVOS), this task focuses more on the motion invideo content for language-guided video object segmenta-tion, requiring an enhanced ability to model longer tem-poral, motion-oriented vision-language data.In this re-port, based on the RVOS methods, we successfully introducemask information obtained from the video instance segmen-tation model as preliminary information for temporal en-hancement and employ SAM for spatial refinement. Finally,our method achieved a score of 49.92 J &F in the valida-tion phase and 54.20 J &F in the test phase, securing thefinal ranking of 2nd in the MeViS Track at the CVPR 2024PVUW Challenge.",
  ". Introduction": "Referring Video Object Segmentation (ROVS) aims tosegment and track the target object referred by the givenlanguage description. This emerging task has attracted sig-nificant attention due to its potential applications in videoediting and human-agent interaction.Motion Expression-guided Video Segmentation is achallenging task based on the RVOS task. Given videos andmotion-oriented language expressions obtained from thedataset MeViS , an embodied agent is needed to segmentthe corresponding described one target or multiple objectsin the video. Compared to the conventional RVOS datasetslike Ref-Youtube-VOS and Ref-DAVIS17 , MeViS",
  "*Equal contribution.Corresponding author": "presents more complex expressions that include motion in-formation rather than merely simple spatial location de-scriptions. Consequently, MeViS necessitates that the agentcomprehends both temporal and spatial information withinvideo clips to effectively correlate with motion expressions.Furthermore, MeViS extends the referring task to includelanguage expressions that match multiple targets, makingMeViS more challenging and reflective of real-world sce-narios.With the development of deep learning, there are stud-ies dealing with the RVOS task. For example, some stud-ies try to deal with the task from a per-frame perspec-tive, they transfer the referring image segmentation meth-ods into video domain, whether through a per-frame mask propagation manner or based on history mem-ory attention to predict the mask of the current frame. Mostrecent methods, e.g. , focus on a uni-fied framework that employs language as queries to seg-ment and track the referred object simultaneously. By effec-tively building correlations between expressions and multi-frame visual features, they achieve promising results acrossmultiple RVOS benchmarks.Some recent methods, e.g., unify various kinds of object-level tasks, suchas MOT, VIS, RVOS and RES, into a single framework topresent an object-centric foundation model. However, thesemethods still encounter the issue of inconsistent predictedresults across multiple frames. While some recent studieson Video Instance Segmentation (VIS) task , which em-phasizes segmenting different instances in the given video,have shown promising results in dealing with prediction in-consistent problem. Furthermore, the emergence of SAM also provides strong segmentation tools for refinement.Thanks to the superior performance of DVIS fromthe VIS task, MUTR from the RVOS task, and HQ-SAM , our method achieves a score of 49.92 J &F in thevalidation phase and 54.20 J &F in the test phase, securing",
  "Visual Encoder": ". The overall architecture of our solution. We employ MUTR as our basic model (Left), which contains a visual/text back-bone, transformer-based encoder and decoder, MTI module, and MTA module. We attempt to introduce instance masks to improve theconsistency of prediction results. We employ an attention block and a sequential mechanism to aggregate instance information into a query(Right).",
  ". Basic Model Architecture": "MUTR (Multimodal Unified Temporal transformerfor Referring video object segmentation) was proposedin and has shown superior performance on Ref-Youtube-VOS and Ref-DAVIS17.MUTR adopts aDETR-like style model.Compared with other methods,MUTR introduces two core modules, i.e.MTI (Multi-object Temporal Interaction module), MTA (Multi-scaleTemporal Aggregation module).To conduct multi-modal interaction and fusion, the MTAmodule consists of sequential cross-attention blocks, whichtakes text feature and multi-scale visual features as in-put and progressively captures temporal visual information.After that, the class token of output multi-modal tokensis repeated to initialize object queries of transformer de-coder. Considering that transformer encoder and decoderprocess video in a frame-independent manner without tem-poral modeling, MTI is designed to perform object-wise in-teraction. MTI module contains an encoder and a decoder.MTI encoder conducts temporal interaction of the same ob-ject across frames and MTI decoder is designed to aggregateinformation of object queries. MTI encoder takes the outputof transformer decoder as input and performs self-attention for the same object across multiple frames to conduct tem-poral interaction. MTI decoder consists of a cross-attentionlayer, a self-attention layer, and a FFN layer. In MTI de-coder, a set of video-wise query Q with random initializa-tion is adopted to associate objects. MTI decoder takes theoutput of MTI encoder as key and value and conducts cross-attention with video-wise query.",
  ". Instance Masks for Query Initialization": "While MUTR achieves superior performance on RVOS,prediction results from MUTR still suffer from inconsis-tency and incompleteness. Meantime, some recent studieson VIS show promising results to solve this issue. There-fore, we attempt to introduce instance mask informationinto a DETR-based model to improve the consistency andcompleteness of prediction results.Specifically, we attempt to introduce instance masks toinitialize the video-wise query Q in MTI decoder. Thanks tothe superior performance of DVIS on VIS, we employ DVISfor mask generation, which extracts all instance masks in avideo clip as follows:",
  "mi = DVIS(I), mi R T HW(1)": "where I R T HW 3 is the input video clip, m ={mi}Ki=1 denotes the set of instance masks, K is the numberof instances in a video clip and T is the number of framesof a video clip.Next, we utilize a visual encoder to extract multi-scalevisual features of instance masks.",
  "Qi = Block(Qi1, Fi), 1 i K(4)": "where Qi R NC is the instance query and N is thenumber of queries. Q0 is randomly initialized. The de-signed attention block consists of a cross-attention layer, aset of self-attention layers, and FFN layers. After that, weutilize this query with instance information to replace therandomly initialized video-wise query fed to MTI decoder.",
  ". HQ-SAM for Spatial Refinement": "Since SAM has shown its great ability in segmenting ob-jects, it could serve as a spatial refiner for better results.Specifically, in this report, we adopt HQ-SAM withViT-L as our mask refiner. Given the predicted result fromMUTR of each clip, we first determine the coordinates ofthe bounding box by selecting the maximum and minimumhorizontal and vertical coordinates of the points along theboundary of the mask. Next, we uniformly sample 10 co-ordinates within the predicted mask as positive points and5 coordinates out of the mask but within the bounding boxas negative points. The sampled points are then fed intothe mask decoder of HQ-SAM as prompts to generate therefined masks.",
  "Datasets. We fine-tune and evaluate our solution on MeViS,a large-scale dataset for motion guided video segmentation": "It contains 2,006 videos with 28,570 language expressionsin total.These videos are divided into 1,662 videos fortraining, 50 videos for offline evaluation, 140 videos for on-line evaluation, and other videos for competition.Metrics. We adopt standard evaluation metrics for MeViS:region similarity (J ), contour accuracy (F), and their aver-age value (J &F).",
  ". Sampling Method": "In the training phase, previous work in RVOS sampleframes around a center point, we named this method localsampling. This method only allows model to access partof the video.However, motion expression guided videosegmentation requires a video-level representation. There-fore, we attempt to divide the entire video into a set of seg-ments and sample one frame randomly in each segment, andaggregate sampling frames to obtain a video clip fed intomodel. We refer this approach as global sampling, whichenables model to access frames across the entire video.",
  ". Implement Details": "We adopt the pre-trained weights of MUTR as initializa-tion and fine-tune model on MeViS. The number of sam-pling frames is 5. The model is optimized by AdamW opti-mizer. The batch size is 1 and the accumulation step is 2. Inpost-process, we employ HQ-SAM with VIT-L backboneutilizing default parameters in .",
  ". Ablation Experiments": "To validate the effectiveness of introducing instancemask for query initialization, sampling method and HQ-SAM, we conduct simple ablation experiments. We adoptthe pre-trained weight of MUTR for weight initializationand fine-tuning model on MeViS. Experiment results areshown in Tab. 1. It is noted that utilizing HQ-SAM for re-finement brings an improvement on J while a drop aboutF. However, utilizing HQ-SAM for refinement still bringsan improvement on J &F. Compared with the previous sampling method, the proposed sampling method brings asignificant improvement about 0.69 J &F. After introduc-ing instance masks for query initialization, the performanceimproved from 49.11 J &F to 49.62 J &F. Finally, whenwe combine all of the above methods, model achieves thebest performance 49.92 J &F.",
  "Finally, we submit our best solution and achieve 54.20J &F ( 50.97 J and 57.43 F ) on test phase, which ranksthe 2nd place for MeViS Track in CVPR 2024 PVUW": "Miriam Bellver, Carles Ventura, Carina Silberer, IoannisKazakos, Jordi Torres, and Xavier Giro-i Nieto. A closerlook at referring expressions for video object segmentation.Multimedia Tools and Applications, 82(3):44194438, 2023.1 Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin.End-to-end referring video object segmentation with multi-modal transformers. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages49854995, 2022. 1 Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, andChen Change Loy.Mevis: A large-scale benchmark forvideo segmentation with motion expressions. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 26942703, 2023. 1 Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang.Vision-language transformer and query generation for refer-ring segmentation. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 1632116330,2021. 1 Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, YunchaoWei, Jizhong Han, Luoqi Liu, and Bo Li.Referring im-age segmentation via cross-modal progressive comprehen-sion. In Proceedings of the IEEE/CVF conference on com-puter vision and pattern recognition, pages 1048810497,2020. 1",
  "Lei Ke, Mingqiao Ye, Martin Danelljan, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu, et al. Segment anything in high qual-ity. Advances in Neural Information Processing Systems, 36,2024. 1, 3": "Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Videoobject segmentation with language referring expressions. InComputer VisionACCV 2018: 14th Asian Conference onComputer Vision, Perth, Australia, December 26, 2018, Re-vised Selected Papers, Part IV 14, pages 123141. Springer,2019. 1 Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-thing. In Proceedings of the IEEE/CVF International Con-ference on Computer Vision, pages 40154026, 2023. 1",
  "prompts as queries. arXiv preprint arXiv:2402.18115, 2024.1": "Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, ChenglinWu, Cheng Deng, and Rongrong Ji. Multi-task collabora-tive network for joint referring expression comprehensionand segmentation. In Proceedings of the IEEE/CVF Con-ference on computer vision and pattern recognition, pages1003410043, 2020. 1 Zhuoyan Luo, Yicheng Xiao, Yong Liu, Shuyan Li, Yi-tong Wang, Yansong Tang, Xiu Li, and Yujiu Yang. Soc:Semantic-assisted object cluster for referring video objectsegmentation. Advances in Neural Information ProcessingSystems, 36, 2024. 1 Bo Miao, Mohammed Bennamoun, Yongsheng Gao, andAjmal Mian.Spectrum-guided multi-granularity referringvideo object segmentation. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 920930, 2023. 1 Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos:Unified referring video object segmentation network with alarge-scale benchmark.In Computer VisionECCV 2020:16th European Conference, Glasgow, UK, August 2328,2020, Proceedings, Part XV 16, pages 208223. Springer,2020. 1 Dongming Wu, Tiancai Wang, Yuang Zhang, XiangyuZhang, and Jianbing Shen.Onlinerefer: A simple onlinebaseline for referring video object segmentation. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision, pages 27612770, 2023. 1",
  "Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai,and Song Bai. General object foundation model for imagesand videos at scale. arXiv preprint arXiv:2312.09158, 2023.1": "Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and PingLuo.Language as queries for referring video object seg-mentation.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 49744984, 2022. 1 Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Ze-huan Yuan, and Huchuan Lu.Universal instance percep-tion as object discovery and retrieval.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1532515336, 2023. 1 Shilin Yan, Renrui Zhang, Ziyu Guo, Wenchao Chen, WeiZhang, Hongyang Li, Yu Qiao, Hao Dong, Zhongjiang He,and Peng Gao. Referred by multi-modality: A unified tem-poral transformer for video object segmentation. In Proceed-ings of the AAAI Conference on Artificial Intelligence, vol-ume 38, pages 64496457, 2024. 1, 2"
}