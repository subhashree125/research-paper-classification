{
  "Abstract": "Object-centric learning (OCL) extracts the representa-tion of objects with slots, offering an exceptional blend offlexibility and interpretability for abstracting low-level per-ceptual features. A widely adopted method within OCL isslot attention, which utilizes attention mechanisms to itera-tively refine slot representations. However, a major draw-back of most object-centric models, including slot atten-tion, is their reliance on predefining the number of slots.This not only necessitates prior knowledge of the datasetbut also overlooks the inherent variability in the numberof objects present in each instance. To overcome this fun-damental limitation, we present a novel complexity-awareobject auto-encoder framework.Within this framework,we introduce an adaptive slot attention (AdaSlot) mecha-nism that dynamically determines the optimal number ofslots based on the content of the data. This is achieved byproposing a discrete slot sampling module that is respon-sible for selecting an appropriate number of slots from acandidate list. Furthermore, we introduce a masked slotdecoder that suppresses unselected slots during the decod-ing process. Our framework, tested extensively on objectdiscovery tasks with various datasets, shows performancematching or exceeding top fixed-slot models. Moreover, ouranalysis substantiates that our method exhibits the capabil-ity to dynamically adapt the slot number according to eachinstances complexity, offering the potential for further ex-ploration in slot attention research. Project will be avail-able at",
  ". Illustration of raw image and three kinds of segmentationmasks under different slot numbers. Pixels colored the same aregrouped as the slot. The slot number is very important": "of structured scene representations rather than relying solelyon global features. These structured representations encom-pass crucial attributes such as spatial information, color,texture, shape, and size, effectively delineating various re-gions within a scene. These regions, characterized by dis-tinct yet cohesive properties, can be likened to objects inthe human sense. These object-centric representations, of-ten referred to as slots, are organized within a set structurethat partitions the global scene information.Traditionally, object-centric learning adopts unsuper-vised methods with reconstruction as the primary trainingobjective. This process clusters distributed scene represen-tations into object-centric features, with each cluster asso-ciated with a specific slot. Decoding these slots indepen-dently or in an auto-regressive manner yields meaningfulsegmentation masks. This inherent characteristic of object-centric learning has paved the way for its application acrossdiverse tasks, including unsupervised object discovery andlocalization , segmentation and manipu-lation .And it can also be generalized to weakly-supervised/supervised cases . Among these al-gorithms, Slot Attention emerges as the most promi-nent and widely recognized method in the field.However, a significant challenge within the realm of slotattention is its reliance on a predefined number of slots,which can prove problematic. On one hand, accurately de-termining the number of objects in a dataset can be chal-lenging, especially when annotations are absent. On theother hand, datasets often exhibit varying object counts,",
  ". Illustration of our pipeline": "rendering a fixed, predefined number impractical. Incor-rectly specifying the number of slots can substantially im-pact the results, as illustrated in , where an inadequateslot count leads to under-segmentation, while an excessivecount results in over-segmentation.To address this challenge, we present an approach thatadaptively determines the number of slots for each instancebased on its inherent complexity. Our goal is to allocatea larger slot count for instances with more objects while asmaller number for fewer objects. To achieve this, we pro-pose a novel complexity-aware object auto-encoder frame-work. Within this framework, we initially generate a rela-tively large number of slots, denoted as Kmax, and dynam-ically select a subset of slots according to each instance forthe reconstruction process. Additionally, our framework in-corporates a slot sparsity regularization term into the train-ing objective, explicitly considering the complexity of eachinstance. This regularization term ensures a balance be-tween reconstruction quality and the utilization of an ap-propriate number of slots.Our framework encompasses several key strategies.Firstly, we leverage a lightweight slot selection module toacquire a sampling strategy that keeps the most informa-tive slots and discards redundant ones. However, simplyneglecting the dropped slot will not propagate the gradi-ent. To deal with this, we employ Gumbel-Softmax toachieve end-to-end training. Furthermore, simply samplingan element from the power set of slots will lead to exponen-tially many choices and low computational efficiency. Toaddress this issue, we break the selection into K binary se-lection with the mean-field formulation to overcome thisproblem. Finally, we introduce a masked slot decoder thatadeptly removes information associated with the droppedslots. The whole pipeline is displayed in .We summarize our contributions here: 1) Novel Frame-work: We propose a novel complexity-aware object auto-encoder framework that dynamically determines the num-ber of slots, addressing the limitation of fixed slot countsin object-centric learning. 2)Efficient Slot Selection: Ourframework incorporates an efficient and differentiable slotselection module, enabling the identification of informativeslots while discarding redundant ones before reconstruc-tion.3)Effective Slot Decoding: We present a maskedslot decoder that efficiently removes information associated with unused slots. 4)Promising Results: Through exten-sive empirical experiments, we demonstrate the superiorityof our approach, achieving competitive or superior resultscompared to models relying on fixed slot counts. Impor-tantly, our method excels in instance-level slot count selec-tion, showcasing its practical efficacy in various applica-tions.",
  ". Related Work": "Object-Centric Learning. Object-centric learning funda-mentally revolves around the idea that natural scenes canbe effectively represented as compositions of distinct ob-jects. Current methodologies in this field mainly fall intotwo categories: 1) Spatial-Attention Models are exempli-fied by models like AIR , SQAIR , and SPAIR .These approaches infer bounding boxes for objects, provid-ing explicit information about an objects position and size.Typically, such methods employ a discrete latent variablezpres to determine the presence of an object and infer thenumber of objects. However, these box-based priors of-ten lack the flexibility needed to accurately segment objectswith widely varying scales and shapes. 2) Scene-MixtureModels explain a visual scene by a finite mixture of com-ponent images. Methods like MONET , IODINE ,and GENESIS operate within the variational inferenceframework. They involve multiple encoding and decodingsteps to process an image. In contrast, Slot Attention takes a unique approach by replacing this procedure with asingle encoding step using iterated attention.Expanding on slot attention, various adaptations likeSAVi for video data, STEVE for compositionalvideo, and SLATE for image generation have been de-veloped. While effective on synthetic datasets, their real-world performance can be limited. DINOSAUR ad-dresses this by reconstructing deep features instead of pix-els, showing enhanced results on both synthetic and real-world datasets, an approach we adopt in our work.A common limitation among existing methods in thisline is the requirement to predefine the number of slots, of-ten treated as a dataset-dependent hyperparameter. In thiscontext, GENESIS-V2 introduces a novel approachby clustering pixel embeddings using a stochastic stick-breaking process, allowing for the output of a variable num-ber of objects, serving as a valuable baseline method.Differentiable Subset Sampling. Several studies havepursued the goal of achieving differentiable subset selec-tion. Notably, Gumbel-Softmax introduces a con-tinuous relaxation of the Gumbel-Max trick, enabling theselection of the top-1 element. Building upon this founda-tion, Gumbel Top-k extends the approach to general-ize top-k sampling. Another innovative approach, proposedby , approximates top-k sampling by harnessing theSinkhorn algorithm from Optimal Transport. Furthermore, employs the perturbed maximum method to achievedifferentiable selection.However, a common focus of these works lies in sce-narios where the subset size is fixed at k, constrainingtheir adaptability for slot number selection.In contrast,our method employs the common mean-field formulation totransform the subset selection problem, which does not relyon a predefined number, into a series of top-1 selections thatcan be efficiently resolved using Gumbel-Softmax.",
  ". Method": "Preliminary. Slot Attention stands out as one of themost prominent object-centric methods, relying on a com-petitive attention mechanism.In the pipeline, Slot At-tention initially extracts image features with an encoderF = fenc(x) RHW D, where x RHW C rep-resents the image. Rather than directly decoding F into x,the Slot Attention Bottleneck gslot further extracts K slots,denoted as S1, , SK = gslot(F).The slot attention pipeline proceeds to reconstruct im-ages from these slots using a weighted-average decoder.Each slot Si is individually decoded through an object de-coder gobject and a mask decoder gmask, subsequently inte-grated through weighted averaging across the slots.",
  "k=1mi xi,mi =exp iKl=1 exp i,(2)": "where xi RHW C is the object reconstruction whilei RHW is the unnormalized alpha mask. We minimizethe mean squared error between x and x as Lrecon (x, x) =x x22. Here we utilize a fixed K model as our basemodel. Moreover, we reconstruct the RGB pixels for toydatasets, while following DINOSAUR to reconstruct fea-tures extracted by self-supervised backbones on more com-plicated datasets.",
  ". Complexity-aware Object Auto-Encoder": "In slot attention model, predefining the slot number K pro-foundly affects object segmentation quality. To address thisissue, we propose a complexity-aware object auto-encoderframework.Following clustering number selection , we set anupper bound for the slot number as Kmax.This repre-sents the maximum number of objects an image may con-tain in the dataset. During the decoding phase, instead ofdecoding from all slots, our objective is to decode fromthe most informative slots.To achieve this, we learn asampling method for each instance x. The probability(z1, , zKmax) determines whether to keep or drop eachslot S1Kmax, with zi = 0 indicating the slot Si should be dropped, and zi = 1 indicating it should be kept dur-ing reconstruction.We introduce a masked slot decoderx = fdec(S, Z) that effectively suppresses the informationof the dropped slots based on Z.To further control the slot number we retain, we incorpo-rate a complexity-aware regularization term Lreg(). Thisregularization term helps ensure the appropriate number ofslots are retained based on the complexity of instances. Thetraining objective can be formulated as:",
  "Z (z) , x = fdec (S, Z)(3)": "Naturally, without any regularization, the model tends togreedily keep all the slots, as more slots generally lead tobetter reconstruction quality. In contrast, our complexityregularization, as expressed in Eq. 3, compels the model toachieve the reconstruction objective while utilizing as fewslots as possible. The parameter controls the strength ofthis regularization.A natural choice of regularization is the expectation ofkeeping slots:",
  "i=1E [Zi] .(4)": "The smaller expectation, the fewer slot left after selection.Within this framework, we propose our adaptive slot at-tention (AdaSlot) and dealing with two challenges. Thefirst is how to sample from a discrete distribution whilekeeping the module differentiable 3.2. The second is how todesign mask slot decoder to suppress the dropped slots 3.3.",
  ". Mean-Field Sampling With Gumbel Softmax": "Given K slots S, there are 2K possible subsets Ssub S.By mapping each subset to a number between 1 and 2K,we transform the task of selecting a subset into a simplertop-1 choice problem, accounting for the interrelations ofslots. Yet, as the number of slots increases, the exponen-tially growing search space complicates memory manage-ment and model optimization, often trapping the neural net-work in local minima. To address this, we use the mean-field formulation in variational inference , factoring into a product of independent distributions for each slot:",
  "(z1, , zK) = 1(z1) K(zK).(5)": "Therefore, the problem of selecting from 2K space is re-duced to a K binary selection problem. For each Si, wedecide drop or keep the slot individually. This mean-fieldslot selection approach is computational and sampling effi-cient. Although the relation among slots is ignored in this step, we postulate this relation can be implicitly modelledby the competition mechanism in slot attention.To be specific, we denote S RKD. A light weightneural network h : RD R2 is used to predict thekeep/drop probability of each slot individually:",
  ". Masked Slot Decoder": "As mentioned in , the Transformer decoder is biasedtowards grouping semantically related instances together,while the mixture decoder is able to separate instances bet-ter. The behavior of the mixture-decoder makes it a bet-ter choice for exploring dynamic slots since we expect themodel to distinguish instances rather than semantics. In thispaper, we focus on mixture decoder. With the slots repre-sentations S and the keep decision vector Z, we introduceseveral possible design choices of suppressing less impor-tant slots based on Z.Zero slot strategy directly multiply the zero-one keep de-cision vector Z with the slots S:",
  "Si = ZiSi,(8)": "which shrinks dropped slots to zero and keeps the others.Learnable slot strategy employs a shared learnable em-bedding Smask as the prototype of the dropped slot. The in-tuition is that a learnable dropped slot would offer the modelmore flexibility and stabilize training, and complement theinformation loss caused by dropping slots. This is achievedas:Si = ZiSi + (1 Zi)Smask.(9) We empirically found that both the two strategies wouldhurt the reconstruction quality as well as the object group-ing. The root cause is that when computing the alpha mask,the zero/learnable-shrinked slots are still decoded to non-zero masks which matter at the softmax operation as fol-lows:",
  "mi =ZimiKl=1 Zlml + ,mi =exp i(Si)Kl=1 exp l(Sl), (11)": "where is a small positive value for computation stabil-ity. It is worth noting that neglecting , Eq. 11 is equiva-lent to omitting the slot in the mixture decoder, except thatGumbel-Softmax is applied to ensure differentiability. Thekey difference is that this strategy manipulates the alphamask directly, fully removes the information of dropped slotwhile the other two approaches could not.",
  ". Experiments": "Datasets.To evaluate its performance, we utilize a toydataset CLEVR10 and two complicated syntheticMOVi-C/E with high-quality scanned objects in real-istic backgrounds.MOVi-C has up to 10 objects, whileMOVi-E includes at most 23 objects.We treat MOVidatasets as image datasets. Additionally, we use MS COCO2017 dataset as a real-world dataset, which introducesincreased complexity. Noting that we utilize COCOs in-stance mask instead of semantic mask.Metrics We use three kinds of methods for evaluation.The pair-counting metric utilizes a pair confusion matrixto compute precision, recall, F1 score, and Adjusted RandIndex. In the matching-based metric, we utilize three meth-ods: mBO, CorLoc, and Purity. Purity assigns clusters tothe most frequent class, and compute the accuracy. mBOcalculates the mean intersection-over-union for matchedpredicted and ground truth masks, while CorLoc measuresthe fraction of images with at least one object correctly lo-calized. The information-theoretic metric employs Normal-ized Mutual Information (NMI) and Adjusted Mutual Infor-mation (AMI). All metrics, except mBO and CorLoc, arecomputed on the foreground objects. We use ARI to denoteFG-ARI for simplicity.Implementation Details We employ DINO ViT/B-16 asa frozen feature extractor. We set values of Kmax to 24for MOVi-E, 11 for MOVi-C, and 33 for COCO. A two-layer MLP is used for each slot to determine the keepingprobability. Feature reconstruction is performed using MLPmixture decoder as DINOSAUR. We use Adam optimizer,learning rate 4e 4, 10k step linear warmup, and expo-nential learning rate decay. We train our model 500k stepsfor main experiments and 200k steps for ablation. Resultsare averaged over 3 random seeds. More details are in Ap-pendix. We set to 0.1 for MOVi-E/C and 0.5 for COCO.",
  "AdaSlot (Ours)75.5984.6486.6784.2535.6476.8085.2178.5478.60": "construction. The ordinary 11-slot model lacks knowledgeof the object number and tends to allocate slots for seg-menting the background, resulting in slot duplication. Incontrast, AdaSlot accurately groups pixels according to theactual number of ground truth objects. Surprisingly, ourAdaSlot exhibits the ability to determine the object countand resolve slot duplication on the toy dataset. Please referto the appendix for detailed results.Results on MOVi-C/E. Compared to our model, vanillaslot attention in DINOSAUR uses a pre-defined fixed slotnumber.The selection of slot numbers is subject to thedataset statistics. Note that for data in the wild, we donthave access to the ground-truth statistics. Here, we accessthe number only for comparison. We established baselinesfor the MOVi-E dataset with an average of 12 objects (max23) using small (3, 6, 9), medium (13), and large (18, 21,24) slot numbers. For the MOVi-C dataset with a maximumof 10 objects, we used slot numbers 3, 6, 9, and 11. Be-sides, GENESIS-V2 is compared. The results are displayedin Tab. 1, Tab. 2 and .For Object Grouping,our algorithm demonstratesits benefits through three different kinds of metrics.Our method outperforms GENESIS-V2 by a large mar- gin.When compared to the fixed-slot DINOSAUR, ourcomplexity-aware model achieves the highest ARI and F1score, indicating that it can effectively group sample pairswithin the same cluster as defined by the ground truth. Interms of Purity, AdaSlot yields the highest results, showingthe greatest overlap between our predictions and the fore-ground in the ground truth. Additionally, the information-based metrics AMI and NMI indicate that our model sharesthe most amount of information with the ground truth.Overall, AdaSlot outperforms fixed slot models across allfive mentioned metrics. For Localization, our model havethe highest CorLoc and as good as best mBO compared withfixed slot models. Improper slot number will oversegmentor undersegment the objects, and decrease the IoU, leadingto poor spatial localization.In MOVi-E, 18-24 slots model keeps the precision ata higher level.Our model can decide the slot numberaccording to the instance and further merge the overseg-mented clusters together to improve the recall rate by alarge amount.On MOVi-E, our model keeps the samelevel of precision as 18-slot model but has around 12 pointshigher recall. Therefore, our model reaches best F1 andARI scores.",
  "AdaSlot (Ours)76.7385.2180.3181.4229.8391.0381.2883.0883.20": "Results on COCO. MS COCO has a problem of extremeimbalance in its validation set: most images have less than10 objects. This makes it difficult to determine the correctnumber of slots. To address this, we conducted experimentsusing a wide range of slot numbers with non-uniform spac-ing. The results can be found in and Fig 3.When it comes to object grouping, MS COCO is highlysensitive to the number of slots in the fixed-slot DI-NOSAUR. The experiment showed that the best resultswere achieved with 6 slots. However, increasing the num-ber of slots led to a rapid decline in performance, especiallyin object grouping. For example, just going from 6 to 8slots resulted in a significant drop of around 4 points in ARI,which is about a 10% reduction from the maximum score.Our models, set Kmax=33 and equipped withcomplexity-aware regularization, effectively surpass theperformance of the 33-slot model. Specifically, our modelachieves approximately 20 points higher in terms of ARI.Although the improvement in localization is comparativelysmaller, our model still outperforms the 33-slot model bythree points in terms of mBO.It is worth noting that on the MS COCO dataset, the bestresults obtained with fixed slot numbers are marginally su-perior to our results. COCOs nature images present greaterchallenges than MOVi-C/E due to incomplete labeling, clut-tered compositions without clear backgrounds, and a vastrange of object sizes and varieties. Despite these challenges,",
  ". Revealing the insights of AdaSlot": "Statistical Results Stratified by Ground-truth ObjectNumber. The above sections reflect the average perfor-mance of models on the whole validation datasets. How-ever, the model may over-fit a specific slot number toimprove the final average.To eliminate this possibility,we used stratified sampling method on MOVi-C/E to dis-play the values of various metrics of images with differentground truth object number in . For MOVi-C, wecompare our models with fixed 11 slots(the upper boundof object number) and fixed 6 slots(high ARI and mBO si-multaneously). Similarly, for MOVi-E, compare our modelswith fixed 13-slot and 24-slot models.Precision&Recall are inversely related to the number ofobjects present in an image. As the number of objects in-creases, precision decreases while recall increases. In thecase of our model, it falls somewhere in between high-slotand low-slot models in terms of precision. However, re-garding the recall, our model outperforms high-slot modelssignificantly and performs just as well as low-slot modelsfor image with different objects number.ARI&mBO. Different advantages can be observed forlarge and small slot models. Our models curve encom-",
  "AdaSlot (Ours)39.0081.8666.4268.3727.3647.7667.2844.1144.17": ". Stratified statistics of four metrics of our models and two fixed slot models, one set the slot number to the upper bound andanother set to slot with both high ARI and mBO. We apply stratified sampling according to ground truth object number the image have.The first row is MOVi-C while second row is MOVi-E. The visualizations prove that our model do not over-fit a specific slot number toimprove the performance. passes the metric curve of the two fixed-slot models forARI, indicating a wider range of effectiveness. For mBO,our model achieves a performance comparable to the better-performing fixed-slot models across the entire range. Thisdemonstrates the efficacy of our dynamic slot selection ap-proach, as it consistently delivers favorable results.Comparison between ground truth and predicted objectnumbers. We reveal the insights of our model by showingsome examples in , and heatmap and slot distributionin . The predictions of fixed-slot models tend to beconcentrated within a narrow range, forming a sharp peakwhich deviates from ground truth distribution. In contrast,our models exhibit a smoother prediction distribution thatclosely aligns with the ground truth.On MOVi-C/E, fixed-slot models may generate fewermasks due to the one-hot operation.However, most oftheir predictions are concentrated around the predefined slotnumber, resulting in a heatmap exhibiting a distinct verticalpattern. Our model instead exhibits an approximately di- agonal pattern on the heatmap. In other words, our modelcan predict more masks for images with more objects, andthe number of predicted masks roughly matches the groundtruth number. Though the diagonal relationship is imper-fect, and the prediction on images with an extremely largeor small number of objects is slightly poorer than other im-ages, our model first achieves the adaptive slot selection. demonstrated the adaptability of slot numbersat the instance level with illustrative examples. In partic-ular, on the MOVi-E dataset, our model generates 13 and6 slots for two different images, highlighting a significantdiscrepancy in slot counts. Noteworthy, our results effec-tively group pixels based on image complexity, resulting inaccurate and appropriate segmentation.Results on Object Property In addition to object discov-ery, we study the usefulness of adaptive slot attention forother downstream tasks. Following the setting of , weprovide experiments of object category prediction on theMOVi-C dataset. Our experiments employ a two-layer MLP",
  "MOVI-CMOVI-E": ". Comparison between ground truth and predicted object numbers. Heatmap of confusion matrix and slot distribution of ourmodels and two fixed slot models on MOVi-C/E. For heatmap, y-axis corresponds to the number of objects of ground truth, and x-axisis the predicted object number by models. Due to imbalanced ground truth object numbers, we normalized the row and visualize thepercentage. The brighter the grid, the higher the percentage. The slot distribution graph shows the probability density of grounded andpredicted object numbers.",
  "Ours59.2563.0854.10": "as the downstream model. Our model only makes predic-tions on the retained slots. We employ cross-entropy lossand align predictions with targets with the Hungarian al-gorithm , minimizing the total loss of the assignment.To better compare the results for models with different slotnumbers, we provide the precision, recall and the Jaccardindex. The results are provided in Tab. 4.In the fixed slot model, an increase in the number ofslots typically leads to the discovery of more objects, thusenhancing recall. However, models with a larger numberof slots also tend to generate more redundant objects, ad-versely affecting precision. The Jaccard index, which takesslot redundancy into account, offers a more comprehensiveevaluation. In our experiments on MOVi-C dataset, the 6-slot model achieved the best Jaccard index among fixed-slotmodels. Notably, our model yields a superior Jaccard indexto all fixed slot models. This demonstrates the effectivenessof our adaptive slot attention mechanism.More Ablation Study in Appendix. We conduct thoroughablation studies in the appendix to assess our framework,including comparing three masked decoder designs and ex-amining the impact of . These studies demonstrate ourmodels effectiveness.Limitations.Our model excels in scenarios with well- segmented objects but may struggle with complex, denselypacked scenes like COCO, where annotations are incom-plete and learned objects dont always align with manual la-bels. Its performance on small, dense objects is limited, andthe complexity of real-world part-whole hierarchies posesadditional challenges. We aim to address these issues infuture work.",
  ". Conclusion": "We have introduced adaptive slot attention (AdaSlot) thatcan dynamically determine the appropriate slot number ac-cording to the content of the data in object-centric learn-ing. The framework is composed of two parts. A slot selec-tion module is first proposed based on Gumbel-Softmax fordifferentiable training and mean-field formulation for effi-cient sampling. Then, a masked slot decoder is further de-signed to suppress the information of unselected slots in thedecoding phase. Extensive studies demonstrate the effec-tiveness of our AdaSlot in two folds. First, our AdaSlotachieves comparable or superior performance to those best-performing fixed-slot models. Second, our AdaSlot is capa-ble of selecting appropriate slot number based on the com-plexity of the specific image. The instance-level adaptabil-ity offers potential for further exploration in slot attention. Acknowledgements: Yanwei Fu is the corresponding authour.Yanwei Fu is with School of Data Science, Fudan University,Shanghai Key Lab of Intelligent Information Processing, FudanUniversity, and Fudan ISTBI-ZJNU Algorithm Centre for Brain-inspired Intelligence, Zhejiang Normal University, Jinhua, China. Quentin Berthet, Mathieu Blondel, Olivier Teboul, MarcoCuturi, Jean-Philippe Vert, and Francis Bach. Learning withdifferentiable pertubed optimizers. Advances in neural infor-mation processing systems, 33:95089519, 2020. 3",
  "David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Vari-ational inference: A review for statisticians. Journal of theAmerican statistical Association, 112(518):859877, 2017.2, 3": "Christopher P Burgess, Loic Matthey, Nicholas Watters,Rishabh Kabra, Irina Higgins, Matt Botvinick, and Alexan-der Lerchner. Monet: Unsupervised scene decompositionand representation. arXiv preprint arXiv:1901.11390, 2019.2 Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou,Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-ing properties in self-supervised vision transformers.InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 96509660, 2021. 15 Jean-Baptiste Cordonnier, Aravindh Mahendran, AlexeyDosovitskiy,Dirk Weissenborn,Jakob Uszkoreit,andThomas Unterthiner. Differentiable patch selection for imagerecognition. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 23512360, 2021. 3 Eric Crawford and Joelle Pineau. Spatially invariant unsuper-vised object detection with convolutional neural networks.In Proceedings of the AAAI Conference on Artificial Intelli-gence, pages 34123420, 2019. 2",
  "Marco Cuturi, Olivier Teboul, and Jean-Philippe Vert. Dif-ferentiable ranking and sorting using optimal transport. Ad-vances in neural information processing systems, 32, 2019.2": "Andrea Dittadi, Samuele Papa, Michele De Vita, BernhardScholkopf, Ole Winther, and Francesco Locatello. Gener-alization and robustness implications in object-centric learn-ing. arXiv preprint arXiv:2107.00637, 2021. 7, 15 Gamaleldin Elsayed, Aravindh Mahendran, Sjoerd vanSteenkiste, Klaus Greff, Michael C Mozer, and ThomasKipf. Savi++: Towards end-to-end object-centric learningfrom real-world videos.Advances in Neural InformationProcessing Systems, 35:2894028954, 2022. 1 Martin Engelcke, Adam R Kosiorek, Oiwi Parker Jones, andIngmar Posner.Genesis: Generative scene inference andsampling with object-centric latent representations.arXivpreprint arXiv:1907.13052, 2019. 2 Martin Engelcke, Oiwi Parker Jones, and Ingmar Posner.Genesis-v2: Inferring unordered object representations with-out iterative refinement.Advances in Neural InformationProcessing Systems, 34:80858094, 2021. 2 SM Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa,David Szepesvari, Geoffrey E Hinton, et al. Attend, infer, re-peat: Fast scene understanding with generative models. Ad-vances in neural information processing systems, 29, 2016.2 Ke Fan, Zechen Bai, Tianjun Xiao, Dominik Zietlow, MaxHorn, Zixu Zhao, Carl-Johann Simon-Gabriel, Mike ZhengShou, Francesco Locatello, Bernt Schiele, Thomas Brox,Zheng Zhang, Yanwei Fu, and Tong He. Unsupervised open-vocabulary object localization in videos. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision(ICCV), pages 1374713755, 2023. 1 Ke Fan, Jingshi Lei, Xuelin Qian, Miaopeng Yu, TianjunXiao, Tong He, Zheng Zhang, and Yanwei Fu.Rethink-ing amodal video segmentation from learning supervised sig-nals with object-centric representation.In Proceedings ofthe IEEE/CVF International Conference on Computer Vision(ICCV), pages 12721281, 2023. 1 Klaus Greff, Raphael Lopez Kaufman, Rishabh Kabra, NickWatters, Christopher Burgess, Daniel Zoran, Loic Matthey,Matthew Botvinick, and Alexander Lerchner. Multi-objectrepresentation learning with iterative variational inference.In International Conference on Machine Learning, pages24242433. PMLR, 2019. 1, 2 Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch,Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapra-gasam, Florian Golemo, Charles Herrmann, et al. Kubric: Ascalable dataset generator. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 37493761, 2022. 4",
  "RishabhKabra,ChrisBurgess,LoicMatthey,RaphaelLopezKaufman,KlausGreff,MalcolmReynolds, and Alexander Lerchner. Multi-object datasets. 2019. 4": "Thomas Kipf, Gamaleldin F Elsayed, Aravindh Mahen-dran, Austin Stone, Sara Sabour, Georg Heigold, Rico Jon-schkowski, Alexey Dosovitskiy, and Klaus Greff.Condi-tional object-centric learning from video.arXiv preprintarXiv:2111.12594, 2021. 1, 2 Wouter Kool, Herke Van Hoof, and Max Welling. Stochasticbeams and where to find them: The gumbel-top-k trick forsampling sequences without replacement. In InternationalConference on Machine Learning, pages 34993508. PMLR,2019. 2",
  "Gautam Singh, Yi-Fu Wu, and Sungjin Ahn. Simple unsu-pervised object-centric learning for complex and naturalisticvideos. arXiv preprint arXiv:2205.14065, 2022. 2": "Xinlong Wang, Zhiding Yu, Shalini De Mello, Jan Kautz,Anima Anandkumar, Chunhua Shen, and Jose M Alvarez.Freesolo: Learning to segment objects without annotations.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1417614186, 2022.15 Xudong Wang, Rohit Girdhar, Stella X Yu, and Ishan Misra.Cut and learn for unsupervised object detection and instancesegmentation. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 31243134, 2023. 15 Yangtao Wang, Xi Shen, Yuan Yuan, Yuming Du, MaomaoLi, Shell Xu Hu, James L Crowley, and Dominique Vaufrey-daz. Tokencut: Segmenting objects in images and videoswith self-supervised transformer and normalized cut. arXivpreprint arXiv:2209.00383, 2022. 15",
  "A. More Implementation Details for MOVi-C/E and COCO": "Vision backbones. We utilize the Vision Transformer back-bone and leverage the pre-trained DINO weights availablein the timm library. Our specific configuration entailsusing ViT-B/16, which consists of 12 Transformer blocks.These blocks have a token dimensionality of 768, with ahead number of 12 and a patch size of 16. In our pipeline,we take the output of the final block as the input of slot at-tention module and the reconstruction target.Slot Attention.We adopt the slot attention bottleneckmethodology based on the original work for our im-plementation. The slot initialization process involves sam-pling from a shared learnable normal distribution N(, ).Throughout all the experiments, we iterate the slot attentionmechanism with 3 steps. The slot dimension is set to 128for MOVi-C/E and 256 for COCO datasets. For the feed-forward network in Slot Attention, we utilize a two-layerMLP (Multi-Layer Perceptron). The hidden dimension ofthis MLP is set to 4 times the slot dimension.Light Weight Network for Probability Prediction. Weutilize a two-layer MLP for the probability prediction. Thehidden dimension of this MLP is set to 4 times the slot di-mension, and the output dimension is set to 2.Decoder. We utilize a four-layer MLP with ReLU activa-tions in our approach. The output dimensionality of theMLP is Dfeat + 1, where Dfeat represents the dimensionof the feature, and the last dimension is specifically allo-cated for the alpha mask. The MLPs hidden layer sizesdiffer based on the dataset used. For the MOVi datasets, weemploy hidden layer sizes of 1024. On the other hand, forCOCO, we utilize hidden layer sizes of 2048.Optimizer. In our main experiments, we train our modelsfor 500k steps. Our model is initialized from a fixed Kmaxslot model trained for 200k steps. To optimize the modelsparameters, we employ the Adam optimizer with a learningrate of 4e 4. The 0 and 1 parameters are set to theirdefault values 0 = 0.9, 1 = 0.999. For the ablation stud-ies such as the necessity of Gumbel-Softmax and designs ofmasked slot decoders, we train our models for 200k steps.To enhance the learning process, we incorporate a learn-ing rate decay schedule with a linear learning rate warm-upof 10k steps. The learning rate follows an exponentially de-caying pattern, with a decay half-life of 100k steps. Further-more, we apply gradient norm clipping, limiting it to a max-imum of 1.0, which aids in stabilizing the training proce-dure. The training of the models takes place on 8 NVIDIAT4 GPUs, with a local batch size of 8.",
  "C. Detailed Results on Toy Dataset": "In Tab. 5, we quantitatively compare our model with sev-eral fixed-slot models on the toy dataset CLEVR10 underpixel reconstruction setting. Moreover, we provide quali-tative comparison among our model, 6-slot model, and 11-slot model in . The 11-slot model often assign one ormore slots to represent the background, while 6-slot modelcan not properly segment all objects when the image havemore than 6 objects. Our model differs significantly from the 11-slot modelin terms of handling the background, as observed from thevisualizations. In the case of the 11-slot model, when thenumber of objects of an image is small, the 11-slot modeltends to divide the background into several slots. However,this division does not segment the background into severalregions. Instead, the segmentation of background is veryeven in terms of light and shadow. On the contrary, our model takes a different approachof not utilizing a fixed background slot. Instead, it intel-ligently merge the background regions to the nearest fore-ground objects. It is reflected in the visualization that theshadow (which corresponds to background) around the ob-ject is much darker in our proposed model than the fixedslot model. The visualizations demonstrate that our modelconsistently outputs an appropriate number of slots for eachimage. In order to evaluate the accuracy of our model in de-termining the number of objects, we illustrate the heatmapof confusion matrix of segmentation number and the slotdistribution of the models in . Our models exhibita prediction distribution that almost perfectly aligns withthe ground truth.Additionally, the heatmap revealed anexcellent diagonal relationship, indicating that our methodcan roughly resolves the challenge of unsupervised objectcounting on CLEVR10. The diagonal of the heatmap re-veals the instance-level adaptability of our model. As for the metrics, our model achieves comparable ob-ject grouping results to both the 11-slot and 9-slot models.However, when it comes to localization, our model exhibitsslightly lower performance. Nonetheless, we would like tosuggest that this discrepancy can be attributed to the distinctapproach we take in handling the background. Our modeltends to merge the shadows around objects with the fore-ground, which, in turn, results in slightly lower IoU scoresfor the object masks predicted by our model. Consequently,this leads to drops in metrics such as mBO and CorLoC.",
  "ours11-slot6-slotours11-slot6-slot": ". Visualization of instance-level adaptive slot number selection by per-slot segmentation on CLEVR10. We compare our modeland two fixed-slot models. The results show that our model can select the slot number for each instance adaptively. . Comparison between ground truth and predicted object numbers. Heatmap of confusion matrix and slot distribution of ourmodels and two fixed slot models on CLEVR10. For heatmap, y-axis corresponds to the number of objects of ground truth, and x-axisis the predicted object number by models. Due to imbalanced ground truth object numbers, we normalized the row and visualize thepercentage. The brighter the grid, the higher the percentage. The slot distribution graph shows the probability density of grounded andpredicted object numbers. The results show that our model can choose the slot number almost perfectly on CLEVR10.",
  "ModelsARIP.R.F1mBOCorLocPurityAMINMI": "359.0060.8593.1772.2210.330.0870.0966.3666.41690.7789.2698.1393.0819.3519.4591.8192.3292.34997.5997.8698.5598.1426.4545.7297.8197.3997.401198.0698.7798.3598.5127.3947.1598.2797.9097.90Ours97.6598.1998.3698.2122.5137.0098.0397.5097.51 make our method solely focus on predictions related to theforeground. In other words, we only consider slots whosemasks intersect with foreground objects. Besides, we limitour analysis to images that contain no more than 10 ob-jects, since a significant majority of COCO images containfewer than 10 objects. These particular images play a cru-cial role in determining an appropriate value for the fixedslot number, as 6 slot number reached the best results onCOCO. Among the three models, our model shows bettercorrelation between the ground truth object number and thepredicted slot number. In contrast, the fixed-slot models failto exhibit this diagonal pattern, further highlighting the ef-ficacy of our approach.As for the distribution of total slot number, all threemodels predictions deviate from the ground truth. How-ever, our model demonstrates the closest approximation tothe ground truth distributions. This is substantiated by thevisual examples presented in , where our modelshowcases its ability to generate semantically coherent andmeaningful segmentations.Notably, our model demon-strates adaptability by adjusting the slot number accordingto the complexity of the images, thereby further enhancingthe quality of its predictions. provides valuable insights into the reasons be-hind the deviations of the distributions from the groundtruth. Lets consider the first column of , where ourmodel demonstrates successful segmentation of the raw im-age into distinct regions, including the head of the girl, theT-shirts, the glove, and the background. The separation ofthe T-shirt and the head seems to be an over-segmentationcompared to annotation, which may lead to low metricscore. However, each segmented region exhibits semanticcoherence and is still visually reasonable.Real-world datasets often encompass complex part-whole hierarchies within objects. Without the availabilityof human annotations, accurately segmenting objects intothe expected part-whole hierarchy becomes extremely chal-lenging. Since many objects consist of multiple parts, justlike the human body, it is expected that our models predic-tions will slightly surpass the ground truth in terms of thenumber of slots. As a result, our models prediction will be",
  "E. Ablation": "We conduct a series of ablation studies on MOVi-E datasetto investigate the components and design choices of ourmethod.Comparison of three design choices of masked slotdecoder. In our main paper, we proposed several designchoices of the masked slot decoder, and we focused onthe zero mask variant.In Tab. 6 and , we com-pare the three variants in both quantitative and qualitativeways. The results show that our zero mask method effec-tively improves most metrics compared to the original slotattention model with 24 slots. However, in zero slot andlearnable slot strategy, simply changing the manipulation onthe mask to the manipulation on the slot makes the modelcollapse. Both zero slot and learnable slot strategy tend togroup all pixels together instead of making a segmentation.If we do not explicitly remove the effect of the dropped slotby setting their alpha masks to zero, the zero/learnable slotwill still contribute to the reconstruction. Some instance-irrelated information will be introduced and may misleadthe slot selection. As a result, zero/learnable slot tend togroup all pixels together.The Necessity of Gumbel Softmax. In the main paper,we utilized the hard zero-one mask:",
  "To verify the necessity of Gumbel-Softmax, we provide ex-periments that keep the same masked slot decoder but re-place the hard mask with a soft mask without Gumbel Soft-max:Zsoft = :,1.(13)": "The results are displayed in and Tab. 7. Notably,without Gumbel Softmax, although the model providesslightly better mBO, all the other metrics are kept at thesame level as the original slot attention model. Moreover,from the visualization, without Gumbel Softmax we can notachieve adaptive instance-level slot selection but producesegmentation with Kmax = 24 masks. This failure is due . Comparison between ground truth and predicted object numbers. Heatmap of confusion matrix and slot distribution of ourmodels and two fixed slot models on COCO. The sole distinction is that we consider the ground truth masks and predicted masks on theforeground. Our model outperforms two fixed slot models for slot number selection.",
  ",1 = 2,1 = = K,1,andK,1 0.(14)": "The regularization term approach zero Lreg 0, andmi mi. Therefore, our method is reduced to ordinaryslot attention reconstruction. This simple case shows thatwithout Gumbel Softmax, we can not easily suppress theinformation of unselected slots, leading to the failure of slotselection. With Gumbel Softmax, when i,1 0, Zi = 0and mi = 0 happens with higher probability. The informa-tion of Si will be totally removed. This difference leads toour success.Influence of . We test how the regularization strength influences the results on MOVi-E. We compare 7 possi-ble values of , ranging from 1e 2 to 1 in Tab. 8, keepingother parameters unchanged compared with the main exper-iments. Generally, larger regularization prefers fewer slotsleft and grouping more patches. Recall and exhibit a pos-itive correlation, while Precision and exhibit a negativecorrelation. For foreground grouping, the two metrics reachthe balance around = 0.1 and = 0.2, which leads to thehighest ARI and F1 score. The grouping results have thebest agreement with ground truth, which can also be provenby the highest AMI, NMI and Purity score. However, if wecontinue increasing , these metrics will decrease and dropto an abysmal level. When = 1, the model simply mergesall tokens into a single group, which leads to perfect Recall",
  "but inferior results for all other metrics. For localization, = 0.1 have the best CorLoc score and performs well onmBO": "Influence of Kmax. Our model includes a hyperparam-eter, denoted as Kmax, which determines the maximumnumber of segmentations/slots that the model can produce.Ideally, Kmax should be approximately equal to the highestnumber of objects present in any image within the dataset.Nonetheless, our model still yields satisfactory results evenwhen Kmax is set higher than this ideal value. We have con-ducted a comparative analysis of five distinct Kmax settingson MOVi-E dataset, as detailed in . Our findings in-dicate that when Kmax exceeds the actual maximum objectcount(MOVi-E includes at most 23 objects), most perfor-mance metrics tend to decrease as Kmax increases. How-ever, most metrics keep robust and consistently outperformthe fixed-slot model. Notably, the metric mBO even showsimprovement with very large values of Kmax. The experi-ments demonstrate the robustness of our model to variationsin Kmax. Comparison with oracle model. An oracle model forslot number selection is that we provide the ground-truthobject number of each instance for DINOSAUR. We com-pare our model with this oracle model. The comparativeanalysis, presented in , reveals that our model notonly matches but in some cases surpasses the performanceof the oracle model. This is particularly noteworthy as ourmodel achieves these results without access to the exact ob-",
  "F. Results of semantic-level masks on COCO": "In the main paper, we evaluate the metrics on COCO ac-cording to the instance mask.Moreover, we report theresults based on semantic-level masks in Tab. 11 for fur-ther understanding. Compared with instance-level results,grouping metrics like ARI and F1 score are lower, indi-cating that the model prefers instance-level object discov-ery to class-level. Overall, the results of semantic-level andinstance-level masks are consistent.",
  "G. Comparison with Unsupervised MultipleInstance Segmentation Method": "Our work falls in unsupervised object discovery, whichaims to locate and distinguish between different objects inthe image without supervision. However, it does not nec-essarily provide fine-grained segmentation of each object.In different granularity, unsupervised instance segmentationaims to get a detailed mask for each localized object, clearlydemarcating its boundaries.Most unsupervised object segmentation methods followa pipeline: initially creating pseudo masks using a self-supervised backbone and subsequently training a segmen-tation model based on these pseudo masks. In our discus-sion, we will primarily concentrate on the initial stage ofthese models. We compare our model with MaskCut pro-posed in CutLER , since it can generate multiple in-stance segmentation while other methods either segment only one object from each image , or generate over-lapping masks .To accelerate MaskCuts inference, we work with a fixedsubset here.Table. 12 demonstrate that our model is great at distin-guishing objects apart, whereas MaskCut is good at creat-ing masks that closely match objects (thought some masksmight cover more than one object).Unlike our model,MaskCut is based on iterative application of NormalizedCuts, which assumes images have very clear foreground andbackground distinctions, with only a few objects standingout in the foreground. But this assumption does not holdtrue for MOVi-E/C datasets. As a result, MaskCut produceshigh-quality masks that capture object shapes well (highermBO on MOVi-C and COCO), but it struggles to tell dif-ferent objects apart (lower ARI). This happens because itoften groups multiple objects as foreground in each itera-tion of Normalized Cuts.Additionally, our model can do object grouping in real-time, which is another advantage compared to MaskCut.",
  "H. Results on Object Property Prediction onCLEVR10": "In addition to MOVi-C, we study the usefulness of the adap-tive slot attention of property prediction on CLEVR10. Fol-lowing the setting of , we provide experiments of objectposition regression and color prediction.Our experiments employ a one-hidden layer MLP as thedownstream model. The model operates independently onthe retained slots. Specifically, a kept slot serves as themodels input, yielding a vector containing property predic-",
  "ARIP.R.F1mBOCorLocPurityAMINMI": "0.0162.9987.4459.5068.9330.4785.5469.8478.0578.260.0263.6887.4960.3669.5530.1685.0870.4878.3578.550.0570.9585.6771.4876.3229.4686.7976.6780.8781.020.175.3084.7478.6480.2029.4790.0980.1282.3282.450.276.0778.7986.5181.3026.2886.6880.4981.5081.610.533.5238.0489.2751.749.0513.2650.4046.6246.741.000.0121.2099.9633.932.210.0833.870.030.03 tions for that particular slot. We employ cross-entropy lossfor color prediction and mean squared error (MSE) loss forcoordinate regression. When both tasks are undertaken, wesum these two losses to calculate the total loss. We alignpredictions with targets with the Hungarian algorithm ,minimizing the total loss of the assignment.We present results in terms of the regression R2 scorefor position estimation. For the color prediction task, tobetter compare the results for the model with different slotnumbers, we provide the precision, recall and the Jaccardindex. The results are provided in Tab. 13.In our experiments with CLEVR10, the 6-slot modelachieved the best Jaccard index among fixed-slot models.Notably, our model yields a superior Jaccard index to allfixed slot models. This demonstrates the effectiveness ofour adaptive slot attention mechanism.Additionally, our model demonstrates superior perfor-mance in terms of R2 score for coordinate regression onCLEVR10. It is worth noting that the 3-slot model fails topredict the object coordinate well, with R2 score less than0. This highlights the importance of the slot number. Withan improper slot number, the model may be not able to fitthe data.",
  "I. GENESIS-V2 with DINO backbone": "In the main paper, we inherit the official implementation ofGENESIS-V2 with UNet encoder. For better comparison,we provide results with the same DINO ViT/B-16 backboneas our models in Tab. 14. GENESIS-V2 with DINO back- bone shows consistent improvement across all metrics, par-ticularly in ARI. However, it significantly falls behind ourmethod, further validating our approachs effectiveness. Asillustrated in Fig 12, 13 & 14, compared to GENESIS-V2with DINO backbone, our model can better determine theproper slot number and generate object mask closer to theboundary of an object, which makes our AdaSlot better onvarious metrics, especially mBO.Moreover,otherthantheheuristic stopping rule",
  "J. More Visualization": "To provide a more comprehensive understanding of ourmethods, we have included additional visualizations in, and . For each dataset, we selectfive examples and compare our model with GENESIS-V2and fixed-slot DINOSAUR. Our model segments the rawimage into regions that are not only semantically coherentbut also highly meaningful. Moreover, our model show-cases adaptability by dynamically adjusting the slot numberin accordance with the complexity of the images.",
  "DatasetModelARIP.R.F1mBOCorLocPurityAMINMI": "Movi-COurs75.5984.6486.6784.2535.6476.8085.2178.5478.60Movi-COracle75.6885.6784.9984.3033.8272.3685.4878.5578.62Movi-EOurs76.7385.2180.3181.4229.8391.0381.2883.0883.20Movi-EOracle74.9784.0278.4480.0829.3390.6779.5481.9282.05 manual annotations. Additionally, due to the characteris-tics of the feature reconstruction, the performance on densesmall objects is not very outstanding. When compare ourmodel of Kmax with the fixed slot model of K = Kmax,our model produces fewer masks, and more small objectswill be missed. However, the fixed-slot counterpart will alsoover-segment one object into multiple parts. Further, thepart-whole hierarchy in real-world scenes brings additionalcomplexity and challenge to unsupervised object discovery.We leave improvements regarding this challenge for futureworks."
}