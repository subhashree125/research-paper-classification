{
  "Abstract": "Efficient and lightweight single-image super-resolution(SISR) has achieved remarkable performance in recentyears. One effective approach is the use of large kernel de-signs, which have been shown to improve the performanceof SISR models while reducing their computational require-ments.However, current state-of-the-art (SOTA) modelsstill face problems such as high computational costs. Toaddress these issues, we propose the Large Kernel Distilla-tion Network (LKDN) in this paper. Our approach simpli-fies the model structure and introduces more efficient at-tention modules to reduce computational cost while alsoimproving performance.Specifically, we employ the re-parameterization technique to enhance model performancewithout adding extra cost. We also introduce a new op-timizer from other tasks to SISR, which improves trainingspeed and performance. Our experimental results demon-strate that LKDN outperforms existing lightweight SR meth-ods and achieves SOTA performance. The codes are avail-able at",
  ". Introduction": "Single image super-resolution (SISR) is an essentialproblem in low-level computer vision (CV) that involvesreconstructing a high-resolution (HR) image from its low-resolution (LR) counterpart. After the introduction of deeplearning to super-resolution by SRCNN , there has beena significant surge in the development of deep-learning-based SR models. Due to their impressive ability to recon-struct high-resolution images from low-resolution observa-tions, these algorithms have gained popularity in the CVcommunity. Although deeper and larger models are oftenconsidered the optimal approach for designing SR modelswith strong representation ability , there is a grow-ing emphasis on developing lightweight models that can ap-proximate the performance of larger models with greatly",
  ". Comparison of model performance and complexity onUrban100 with SR(4)": "Among the numerous design approaches for lightweightsuper-resolution models, information distillation connec-tions have been identified as a highly effective method.This approach fuses features of varying hierarchies to facil-itate the transmission of more distinctive features into thenetwork, resulting in efficient feature reuse and achieving abetter balance between reconstruction accuracy and compu-tational efficiency.RFDN reevaluated the information multi-distillationnetwork and introduced multi-feature distillation con-nections that are highly adaptable and lightweight, andit won the champion of AIM 2020 Efficient SR Chal-lenge . Meanwhile, BSRN achieved the first placein the model complexity track in NTIRE 2022 EfficientSR Challenge by incorporating residual feature dis-tillation connections with effective attention modules andre-parameterizing the redundant convolution of RFDN byusing blueprint separable convolution (BSConv). Be-sides, RFLN emerged as the champion of the runtimetrack by improving the efficiency of RFDN through the useof residual local feature blocks that reduce network frag-ments while maintaining model capacity. The newly pro-posed VAPSR is designed with a concise structure and",
  "arXiv:2407.14340v1 [eess.IV] 19 Jul 2024": "fewer parameters while achieving SOTA performance. Byenhancing the pixel attention mechanism , incorporat-ing large kernel convolutions, and implementing efficientdepth-wise separable large kernel convolutions. Addition-ally, VAPSR achieves comparable performance to RFDNwhile utilizing only 28.18% of its parameters and outper-forms BSRN.Although BSRN has made great progress in terms ofmodel parameter and computation, excessive residual con-nections and complex attention modules (i.e.contrast-aware attention (CCA) and enhanced spatial atten-tion (ESA)) have led to low model computation effi-ciency. As analyzed in RLFN, the complex ESA module isredundant and it is difficult to quantitatively analyze whichparts are truly useful. While VAPSR achieves better perfor-mance than BSRN, the models running speed is slower dueto the presence of a large number of inefficient element-wise multiplications. By removing redundant modules inthe network and introducing more efficient ones, we canbuild a more efficient SR network.In this paper, we propose a novel lightweight SR net-work, named large kernel distillation network (LKDN),which builds upon the baseline model of BSRN. Ourapproach simplifies the model structure and employs amore efficient attention module, called large kernel atten-tion (LKA), to improve model performance and compu-tational costs. We demonstrate the effectiveness of LKAin lightweight SR tasks. Additionally, we leverage the re-parameterization technique to further enhance the perfor-mance without adding any additional computational cost.To achieve faster convergence and state-of-the-art (SOTA)performance, we incorporate the recently proposed Adanoptimizer , which has shown success in various taskssuch as high-level CV, NLP, and reinforcement learning.Our proposed LKDN achieves SOTA performance amongexisting efficiency-oriented SR networks, as shown in Fig-ure 2. The main contributions of this article are:(1)AfteranalyzingthecomputationalefficiencyofBSRN and VAPSR , We achieved better per-formance while reducing the number of parameters andcomputational consumption through simplifying the modelstructure and introducing a more efficient attention module.(2) We used the technique of re-parameterization to improvethe model performance without introducing any additionalinference burden.(3) We introduced a new optimizer that can simultaneouslyboost the training speed and performance of SISR models.",
  "The development of lightweight super-resolution net-works has received increasing attention in recent years": "due to their practical applications in resource-constrainedscenarios such as mobile devices and embedded systems.Many lightweight SR models have been proposed to re-duce the computational cost and memory footprint whilemaintaining the model capacity and achieving satisfactoryperformance. The common strategies for lightweight SRnetworks include network pruning , parame-ter sharing , knowledge distillation ,depth-wise separable convolutions , attentionmechanisms , efficient upsampling meth-ods and re-parameterization technique .In addition to the aforementioned techniques, in terms ofnetwork structure design, information distillation connec-tions has also been demonstrated as an ef-fective approach to building lightweight SR network archi-tectures. We thus maintain the network topology design ofinformation distillation while enhancing the attention mech-anism and implementing re-parameterization in LKDN.",
  ". Large Kernel Design": "Since the introduction of VGG , small convolutionkernels such as 3 3 have been widely used due to theirhigh efficiency and lightweight nature. Transformer , asa model that achieves a larger receptive field through globalself-attention operation, has achieved excellent perfor-mance in natural language processing (NLP). In addition,both global and local vision-Transformers havedemonstrated impressive performance in the field of CV.This characteristic has inspired researchers to design betterconvolutional neural networks (CNNs) by utilizing largerconvolution kernels.For example, ConvNeXt useslarge convolution kernels to obtain a larger receptive fieldand achieve comparable performance to Swin-Transformer.RepLKNet scales up kernels to 31 31 using depth-wise convolution and re-parameterization, achieving com-parable or superior results to Swin-Transformer on varioustasks. VAN explores the effective application of at-tention mechanisms in CV and proposes a new large ker-nel attention (LKA) module. SLaK proposes a recipefor applying extremely large kernels from the perspectiveof sparsity, allowing for the smooth scaling up of kernels to61 61 with improved performance. Drawing inspirationfrom such designs, we developed a large kernel distillationblock (LKDB) with large kernel attention (LKA) to furtherimprove the representation ability of LKDN.",
  "(d) RBSB": ". The details of each component. (a) LKDB: Large Kernel Distillation Block; (b) BSConv: Blueprint Separable Convolution; (c)LKA: Large Kernel Attention; (d) RBSB: Re-parameterized Blueprint Shallow Block. into a multi-branch topology comprising identity mapping,1 1 convolution, and 3 3 convolution, enabling tra-ditional VGG-style CNNs to achieve similar performanceand faster inference speed than SOTA on several high-level vision tasks. Diverse Branch Block combines di-verse branches of varying scales and complexities to en-rich the feature space, constructing a convolutional networkunit resembling Inception . MobileOne leveragesthe re-parameterization technique to enhance the modelsaccuracy and speed, eventually achieving SOTA perfor-mance within effective architectures while being manytimes faster on mobile devices. As a result. we employedre-parameterization techniques to optimize the feature ex-traction block in LKDN.",
  ". Adan Optimizer": "The Adam optimizer is widely utilized in variousdeep learning domains. By utilizing first and second or-der gradient moment estimation, it dynamically adjusts thelearning rate of each parameter to achieve faster conver-gence than stochastic gradient descent (SGD). However, Adam can also suffer from non-convergence and lo-cal optima . Recently, Adan optimizer com-bines modified Nesterov impulse, adaptive optimization,and decoupling weight attenuation. Adan uses extrapola-tion points to perceive gradient information beforehand, al-lowing for efficient escape from sharp local minima and in-creasing model generalization. Based on extensive exper-iments, it has been shown that the Adan optimizer outper-forms existing SOTA optimizers for both CNNs and Trans-formers. Therefore, we aim to apply the Adan algorithm tolightweight super-resolution tasks.",
  "InLR = Concat(InLR),(1)": "where Concat() denotes the concatenation operation alongthe channel dimension, and n is the number of replicated in-put image ILR. Then the initial feature extraction is imple-mented by a 33 BSConv to generate shallow features fromthe input LR image as: F0 = hext(InLR), where hext() de-notes the module of shallow feature extraction, and F0 de-notes shallow feature. The structure of BSconv is shownin b, which consists of a 1 1 convolution and adepth-wise convolution.The next part of LKDN is to extract deep featuresthrough a stack of LKDBs, which can be formulated as:",
  "Fk = HmLKDB(. . . H1LKDB(F0)), 1 k m,(2)": "where HkLKDB() denotes the kth LKDB, m is the numberof used LKDBs, and Fk represents the output feature of thekth LKDB.After gradually refining by the LKDBs, all the interme-diate features are fused and activated by a 11 convolutionlayer and a GELU activation. A 3 3 BSConv layeris used to smooth the fused features. The process of multi-layer feature fusion can be formulated as:",
  ". Rethinking the BSRN": "The performance of BSRN models has been improved byESA, CCA, and multiple residual connections. However,the complex structure results in lower computational effi-ciency. RLFN used a pruning sensitivity analysis tool basedon a one-shot structured pruning algorithm to analyze theredundancy of the ESA block and discovered a significantamount of redundancy. Therefore, we removed the ESAand CCA modules of BSRN and introduced more efficientattention modules. Recent studies have shown that the performanceof a model can be improved while maintaining acceptablecomplexity by using large kernel convolution reasonably.VAN utilizes convolution decomposition to split a largekernel convolution into three parts: a spatial local convolu-tion, a spatial long-range convolution, and a channel con-volution. Specifically, a K K convolution is decomposedinto a (2d1)(2d1) depth-wise convolution, a [ K",
  "d ][ K": "d ]depth-wise dilation convolution with dilation d, and a 1 1convolution. By decomposing large kernel convolution, themodel can capture long-range relationships with minimalcomputational cost and parameters. Similar to VAPSR, weperform convolution decomposition in a different order, andas a result, the LKA module is shown in c and canbe expressed as follows:",
  "Output = Xatten F,(6)": "where ConvDW D() and ConvDW () denotes dilateddepth-wise convolution and depth-wise convolution respec-tively, Xatten denotes attention map, denotes element-wise product operation, and F denotes the input feature.By decomposing a large 17 17 convolution into a 1 1point-wise convolution, a depth-wise 55 convolution, anda depth-wise dilation convolution with a kernel size of 5 anddilation of 3, our model can reduce complexity and improveperformance. Replacing ESA and CCA modules with LKAmodules can also further improve inference speed.",
  "Re-parameterization": "To demonstrate the effectiveness of the proposed RBSBin d, we replaced the basic blocks in the featuredistillation structure for comparison.a displaysthe BSRB used in BSRN, while b illustrates theblueprint shallow block (BSB) obtained by directly remov-ing the residual connections.The evaluation results weconducted on LKDN-S are presented in , demon-strating that eliminating unnecessary residual connectionscan improve performance while reducing model complex-ity. Re-parameterization can thus further improve perfor-mance while maintaining model complexity.",
  "Large Kernel Distillation Block": "After analyzing the characteristics of BSRN and VAPSR,we developed an even more efficient large kernel distilla-tion block (LKDB). The complete structure of LKDB canbe seen in a. It comprises four components: featuredistillation, feature fusion, feature enhancement, and fea-ture transformation. In the first stages, given the input Fin,the feature distillation operation can be described as:",
  ". Datasets and Evaluation Metrics": "We utilized a training set of 800 images from DIV2K and 2650 images from Flickr2K . Our evaluation ofthe models is performed on commonly used benchmarkdatasets, including Set5 , Set14 , B100 , Ur-ban100 , and Manga109 . The training data wasaugmented with random horizontal flips and 90-degree ro-tations. The evaluation metrics used are the average peak-signal-to-noise ratio (PSNR) and the structural similar-ity (SSIM) on the luminance (Y) channel.",
  ". Implementation details of LKDN": "The proposed LKDN model is composed of 8 LKDBswith a distillation structure channel number and attentionmodule channel number set to 56, and is trained with BSBto reduce training time. The mini-batch size and input patchsize for each LR input are set to 64 and 4848, respectively.We train the model using the common L1 loss function andthe Adan optimizer , with 1 = 0.98, 2 = 0.92 and3 = 0.99. To stabilize training, we set the exponentialmoving average (EMA) to 0.999. The learning rate is set toa constant 5103 for the entire 1106 training iterations.We propose a smaller version of LKDN, called LKDN-S,for the NTIRE 2023 Efficient SR Challenge . LKDN-Scomprises 5 LKDBs and 42 channels, and is trained withRBSB. We also employ re-parameterization techniques inthe up-sample layer of LKDN-S. The training process ofLKDN-S involves two stages: an initial training stage and afine-tuning stage. In the initial training stage, we randomlycrop 128 mini-batch HR patches with a size of 256 256.We train LKDN-S using the common L1 loss function witha learning rate of 5 103 and 9.5 105 iterations. In thefine-tuning stage, we set the patch size of HR images andbatch size to 480 480 and 64, respectively. LKDN-S isfine-tuned using the L2 loss function with a learning rate of2105, and a total of 5104 iterations. The EMA is set to0.999 and Adan optimizer , with 1 = 0.98, 2 = 0.92and 3 = 0.99 is applied in both stages.We implement all our models using PyTorch 1.11 andNvidia GeForce RTX 3080 GPUs.",
  "Large Kernel Attention": "We conducted ablation studies to verify the efficacy of theLKA module, as presented in .In this table, Cand A denote the input channels of the distillation struc-ture and attention module, respectively. Removing the ESAand CCA modules from BSRN resulted in a significant dropin model performance.However, by utilizing the LKAmodule, the networks receptive field increased, leading to",
  "LKDN29.1832218.385.65": "an improvement in model performance while maintaininglower parameter and computation costs than the originalBSRN model.Further performance improvements wereachieved by adjusting the channel numbers of the distilla-tion structure and attention module. Compared to the orig-inal BSRN, LKDN can achieve performance gains of morethan 0.1 dB on the Urban100 and Manga109 whilemaintaining lower parameter and computation costs.",
  ". Qualitative and quantitative comparison on SR (4), the best and second best are in red and blue respectively": "FSRCNN , VDSR , LapSRN , DRRN ,MemNet , IDN , CARN , IMDN , PAN ,LAPAR-A , RFDN , RFLN , BSRN , andVAPSR . These methods have been compared for up-scale factors of 2, 3, and 4. presents the quan-titative comparison results for these methods. With the in-corporation of efficient attention modules, LKDN has out-performed other methods in terms of achieving the best per-formance while maintaining a lightweight model. In ad-dition, our proposed solution for NTIRE 2023 Challenge,LKDN-S, has achieved competitive performance with only",
  "K parameters and 7.3G Multi-Adds for SR 4": "presents a qualitative comparison of our pro-posed method. The results indicate that our model is morecapable of reconstructing high-similarity structures than ex-isting methods. As an example, in images img 11 andimg 92, existing methods typically generate obvious dis-tortion and blurriness, while our approach accurately cap-tures the lines. Furthermore, in img 73, VAPSR calcu-lates the incorrect number of windows, resulting in a signif-icant decrease in PSNR and SSIM, while our method canaccurately restore the number of windows. provides a more in-depth analysis of BSRN,VAPSR, and LKDN. The results show that LKDN out-performs BSRN while maintaining a comparable inferencespeed. Moreover, LKDN achieves faster inference speedsthan VAPSR while maintaining superior performance.",
  ". Conclusion": "We propose the Large Kernel Distillation Network(LKDN) as an efficient single-image super-resolution(SISR) solution. Through careful analysis of some state-of-the-art lightweight models, we identified their weaknessesand improved upon them to enhance the performance ofLKDN. By incorporating the large kernel design, simplify-ing the model structure, introducing more efficient attentionmodules, and employing re-parameterization, we achieve abalance between performance and computational efficiency.A new optimizer is also introduced to accelerate the conver-gence of LKDN. Extensive experiments demonstrate thatLKDN outperforms state-of-the-art efficient SR methods interms of performance, parameters, and Multi-Adds.",
  "Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-Hsuan Yang. Deep laplacian pyramid networks for fast andaccurate super-resolution. In CVPR, pages 624632, 2017.2, 7, 8": "Wenbo Li, Kun Zhou, Lu Qi, Nianjuan Jiang, Jiangbo Lu,and Jiaya Jia. Lapar: Linearly-assembled pixel-adaptive re-gression network for single image super-resolution and be-yond. Advances in Neural Information Processing Systems,33:2034320355, 2020. 7, 8 Yawei Li, Kai Zhang, Radu Timofte, Luc Van Gool,Fangyuan Kong, Mingxi Li, Songwei Liu, Zongcai Du, DingLiu, Chenhui Zhou, et al. Ntire 2022 challenge on efficientsuper-resolution: Methods and results. In CVPRW, pages10621102, 2022. 1, 6",
  "Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, andTrevor Darrell.Rethinking the value of network pruning.ICLR, 2018. 2": "David Martin, Charless Fowlkes, Doron Tal, and JitendraMalik. A database of human segmented natural images andits application to evaluating segmentation algorithms andmeasuring ecological statistics.In ICCV, pages 416423,2001. 5, 6, 7 Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto,Toru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa.Sketch-based manga retrieval using manga109 dataset. Mul-timedia Tools and Applications, 76(20):2181121838, 2017.5, 7"
}