{
  "Abstract": "Federated Learning (FL) enables multiple machines tocollaboratively train a machine learning model withoutsharing of private training data. Yet, especially for hetero-geneous models, a key bottleneck remains the transfer ofknowledge gained from each client model with the server.One popular method, FedDF, uses distillation to tackle thistask with the use of a common, shared dataset on whichpredictions are exchanged. However, in many contexts sucha dataset might be difficult to acquire due to privacy andthe clients might not allow for storage of a large shareddataset.To this end, in this paper, we introduce a newmethod that improves this knowledge distillation methodto only rely on a single shared image between clients andserver. In particular, we propose a novel adaptive datasetpruning algorithm that selects the most informative cropsgenerated from only a single image. With this, we show thatfederated learning with distillation under a limited shareddataset budget works better by using a single image com-pared to multiple individual ones. Finally, we extend ourapproach to allow for training heterogeneous client archi-tectures by incorporating a non-uniform distillation sched-ule and client-model mirroring on the server side.",
  ". Introduction": "Federated Learning (FL) is a paradigm in the field of dis-tributed machine learning that enables multiple clients tocollaboratively train powerful predictive models without theneed to centralize the training data .It comes withits own set of key challenges in terms of skewed non-IID distribution of data between the participating clients and communication efficiency duringtraining among others. These challenges are not di-rectly answered with the classical approaches such as Fe-dAvg , which rely primarily on a naive client networkparameter sharing approach. Since the inclusion of clientswith different data distributions has a factor of heterogene-ity involved , another well-known work coun-teracts this heterogeneity directly during the client training. This tries to solve one challenge related to non-iidness inprivate data distribution, but other key challenges related tonetwork parameter sharing remain including concerns withprivacy leakage during parameter sharing , hetero-geneity of client architectures and high bandwidthcost of parameter sharing . To this end, along a secondline of thought implementing a server-side training regime,approaches suggested in make use of theprocess of knowledge distillation (KD) to overcomethese challenges without the exclusive need of network pa-rameter sharing. To facilitate central network training withthe help of KD, the sharing of public data is needed betweenthe clients and the server. In this work, we propose a novel approach of makinguse of a single datum source to act as the shared distilla-tion dataset in ensembled distillation-based federated learn-ing strategies. Our approach makes use of a novel adaptivedataset pruning algorithm on top of generating the distil-lation data from a single source image during the training.This combination of shared data generation and instance se-lection process not only allows us to train the central modeleffectively but also outperforms the other approaches whichmake use of multiple small-sized images in place of a singleimage under a limited shared dataset budget. The use of asingle datum source has added benefits in domains, wherepublicly available data and client resources (e.g., networkbandwidth and connectivity) are limited in nature. The useof a single datum source has been explored under thesettings of self-supervised learning and understanding ex-trapolation capabilities of neural networks with knowledgedistillation, but it has not yet been explored in federated set-ting for facilitating model training. We perform a series of experiments to examine the via-bility of our proposed algorithm under varying conditions ofheterogeneity in private client data, client-server model ar-chitectures, rate of pre-training network initializations be-fore distillation, shared dataset storage budget, and real-world domain of the single images. We also extend ourexperiments to a use case of heterogeneous client architec-tures involved during a single federated training with thehelp of client-model mirroring on the server side. To fa-",
  "KMeans + Entropy Based Pruning": ". Illustration of our federated learning algorithm using a single image. Our algorithm works on the principle of generating acommon distillation dataset from only one shared single image using deterministic augmentations. To this end, our method dynamicallyselects the best patches for the training of the global model in the next round using knowledge distillation. cilitate this, we keep one copy of the client model of eachtype on the server end, which acts as a global model for theclients that have the same network architecture. The globalmodels are improved with knowledge distillation after eachround of local client training with the help of shared logitsover the single image dataset. The results we obtain dur-ing the aforementioned experiments demonstrate positivereinforcement towards reaching our goal of efficient feder-ated training using Knowledge Distillation under a limitedshared dataset budget.The primary contributions of this work are :",
  ". Related Work": "Federated learning with knowledge distillation. Knowl-edge Distillation (KD) has been shown to successfullytransfer the knowledge of an ensemble of neural networksinto a single network with the means of output logits overa shared dataset. KD has also been leveraged in federatedsettings, such as Federated Distillation Fusion (FedDF) and Federated Ensemble Distillation (FedED) , wherethe respective authors make use of KD to allow robust andfaster convergence on top of using other ensembling meth-ods such as the ones suggested in Federated Averaging for initializing the central network before the distillationtraining of the global model. On the other hand, authorsof works such as Federated Model Distillation (FedMD) have also successfully shown that KD can be used forknowledge transfer in a federated setting for the purpose ofclient model personalization. However, the application ofalgorithms such as FedMD is targetted for personalization by client-side knowledge distillation rather than improve-ment of a central model, hence we have not delved intoit in the scope of our research. In the case of ensemblingmethods, it has been shown in that in the absence ofan ensemble of local parameters before distillation train-ing, the final test performance of the central network tendsto suffer. As a result, these methods have been shown bythe authors to significantly rely on parameter exchange ev-ery round similar to naive parameter exchange-based algo-rithms such as FedAvg for robust performance on topof KD. Since the aforementioned KD-based federated al-gorithms also require significant regular ensembling usingnetwork parameter exchange, our approach focuses on im-proving this aspect by relying significantly on knowledgedistillation with the help of data pruning and augmentationson the shared public dataset, which has not yet been ex-plored in these works. Communication-efficient federated learning.To solvethe high bandwidth costs related to parameter sharing, au-thors of have shown that quantization of networkparameters before the transfer can significantly reduce thebandwidth costs incurred during their transfer. However,with the application of the same low-bit quantization meth-ods with the KD-based federated learning methods in ,the authors have also shown a significant decrease in theoverall test performance of models compared to their non-quantized counterparts. On the other hand to not rely onpublic data sources, authors of the work have suc-cessfully shown that data-free approaches using a centrallytrained generative network for producing the public shareddataset work robustly. However, this also requires an ad-ditional exchange of the generative network parameters be-fore each round, which leads to an increase in the networkbandwidth usage itself. In pursuit of reducing the band-width costs pertaining to network parameter exchanges aswell as shared dataset sharing, these works have not yetmade an attempt to make use of a storage-efficient single data source, which can simultaneously generate a publicdistillation dataset alongside being used for dynamic selec-tion without added bandwidth costs. We explore this in ourwork.Single image representation learning.Asano et al. have successfully made use of a single image to produceaugmented patches for facilitating self-supervised learningof useful representations required for solving various down-stream tasks. However, the focus of our work is not onthe process of solving tasks with the help of self-supervisedlearning, but on the implications of making use of the singleimage patches in a federated setting as a medium of knowl-edge transfer for training robust classifiers. To this end, ina closely resembling work to our target task, the authors in have shown to be able to successfully use KD with sin-gle image patches to transfer the knowledge between a pre-trained network and an untrained network to solve the clas-sification task of ImageNet-1k. However, the experimentsby the authors were all based in a non-federated setting. Inour work, we explore the usage of single image patches in afederated setting as the shared public distillation dataset andits implications in limited shared dataset budget settings.",
  ". Methodology": "Our methods focus on a dynamic procedure to utilize a sin-gle image to act as a proxy dataset for distillation in addi-tion to a federated learning setup which is similar to existingensemble-based knowledge distillation-based methods suchas FedDF . Alongside the generation of a distillationdataset from a single data source, we take care of dynami-cally selecting the best patches every round to improve thetraining. The two important parts of our federated strategyare: Distillation Dataset Generation (Sec. 3.1) and Distilla-tion Dataset Pruning (Sec. 3.2).",
  ". Distillation Dataset Generation - Patchification": "For generating meaningful image representations out of asingle image, we make use of the patchification technique.Using this technique, we generate a large number of small-size crops from a single image by making combined useof augmentations such as 35-degree rotations, random hori-zontal flipping, color transformations, etc. We use the sameorder of augmentations as the one tested in for knowl-edge distillation-based learning. The image generation pro-cedure is controlled by a given seed to ensure consistentdataset generation when required. This method is commu-nication bandwidth efficient because:1. It requires the transfer of a single image only once acrossthe network to all the clients during the entire training,which may also be present before the training itself.",
  ". The distillation dataset generation is done locally usinga seed number, hence only index values need to be trans-ferred when selecting a subset every round": "The generated dataset is used as the proxy dataset for im-proving the global model using Knowledge Distillation withclients current predictions. Due to the flexibility providedby augmentations in combination with the subset selectionprocedure described in Sec. 3.2, one can make use of a sin-gle image to produce a varying desired number of patchesfor the fixed amount of single image data.",
  ". Distillation Dataset Pruning - Subset Selection": "After obtaining an initial dataset for knowledge distilla-tion using the method in Sec. 3.1, we apply dataset prun-ing methods to it to ensure the selection of information-rich patches for the current round of federated training.The dataset generation procedure makes use of a singleinformation-rich image to generate the small patches, dueto which it can produce bad representation patches suchas: containing no entities, overlapping with others, beingdissimilar to the target domain, and having similar prob-lems arising due to heavy augmentations and presence ofinformation-less regions of the single image. To prune thebad patches, we make use of the following two mechanisms:KMeans Based Class Balancing (Sec. 3.2) and EntropyBased Pruning (Sec. 3.2). These mechanisms depend on thecurrent global model for their operation, which makes themdynamic in nature and improves their data-pruning abilitywith the improvement in the global model. Hence, betterglobal models yield better representations.Through the t-SNE visualization in using a sin-gle image with our data pruning methods, we observe theformation of better identifiable boundary structures with amore accurate global model. It provides us with a glimpseof the positive correlation between the effectiveness of ourpatch subset selection methods with the evolution of theglobal model.KMeans-based class balancing.We first aim to se-lect relevant subcrops that cover the latent space by uti-lizing K-means.Given a distillation dataset X, dividedin N target classes and its embeddings from the currentmodel {f(xi)}xiX, we first generate a KMeans clusteringC({f(xi)}xiX, K) with K clusters,",
  "(3)": "For determining the jth index in (n), we depend on thetarget size of the relevant crop dataset. The full algorithm issupplied in the Appendix A.Entropy-based pruning. After using K-Means based classbalancing, we aim to prune the dataset on the basis of en-tropy (randomness) present in their output logits using thecurrent global model. This enables us to select samples onthe basis of their level of certainty, as predicted by the modelto belong to one of the output classes.Given dataset X divided into N output classes, the en-tropy of a sample xi X is given by :",
  ". Federated Learning with Subset Selection usingSingle Image Dataset": "Using the mechanism described in Sec. 3.1 and Sec. 3.2,we can perform federated training using a single image ac-cording to the steps in Alg. 1. FedAvg Initialisation makesuse of naive weighted averaging of parameters on top ofKD training for global model updates, which can be toggledwith different uniform schedules. We make use of uniformintervals on the basis of defined rates in this work.Computation overhead.Since the clients using ourmethod only do supervised training similar to FedAvg ,theres no computation overhead on the client side duringthe training. The compute cost of forward inferencing asmall distillation dataset is negligible compared to the su-pervised training the clients are subjected to.",
  ". Experimental Setup": "Dataset. We do our experiments across the following pub-lically available datasets: CIFAR10/100 and MedM-NIST (PathMNIST) . For the distribution of private dataamong the clients from the collective training data, we use asimilar strategy to the one suggested in , which allows usto control the degree of heterogeneity using the parameter (lower = higher degree of non-iidness and vice-versa).We use the full test sets corresponding to the private clientdatasets as evaluation sets for the global model (testing isonly server-side). 10% of the training examples are held asa validation dataset.For the shared public dataset, we generate patches out ofa single image for all the experiments with our method. Forthe FedDF experiments, we have made use of CIFAR100training set for CIFAR10 experiments, unless mentionedotherwise. The single images have been visualized in Ap-pendix A alongside the patches and t-SNE visualizations.Server-client model architecture. ResNets (trained fromscratch) have been used for most of our experiments as themodel of choice for the server and clients . We havealso used heterogeneous network distribuitions in Sec. 4.3,with clients having varied network architectures training to-gether. WideResNets have also been used for some of theexperiments . Our method should be directly applica-ble to all kinds of network architectures that have an inter-mediate embedding layer present. The models have beenexplicitly defined in the table descriptions for unambiguity.Hyper-parameter configuration. The values of the learn-ing rate (local and global) have been motivated by the exper-",
  "end": "iments described in Appendix D. We use a client learningrate of 0.01 for ResNet and WResNet, while the distillationtraining learning rate is 0.005. For KMeans Balancing, weuse a KMeans model with 1000 clusters, a class balancingfactor of 1.0, and the Hard selection heuristic. For Entropyselection, we remove 90% of the training examples usingthe Top removal heuristic (Appendix C). For the experi-ment in , we do local client training for 10 epochsand server-side distillation for 250 steps, while 40 epochsand 500 distillation steps have been our choice for other ex-periments unless mentioned otherwise. We prefer to keepthe use of FedAvg initializations to 20% in our experimentsunless mentioned otherwise. For all the experiments, wesimulate 20 private clients, with a selection probability (C)of 0.4 per training round.FedAvg Initialisation Rate. FedAvg Initialisation Rate isresponsible for controlling the schedule of FedAvg Initial-isations (Sec. 3.3), where the percentage decides the roundintervals in which we initialise the central network withthem before distillation training. For reference, 20% meansusing FedAvg Initialisations every 5th round and 100%means using them every round.",
  ". Network and Storage Conditions": "Comparative analysis of single image performance un-der limited shared dataset budget.This is our most significant experiment in terms of ex-hibiting the viability of federated learning under limitedshared dataset budget settings using a single image. Go-ing through the results in , we see that for the sameamount of storage budget, a single image with patch selec- tion outperforms similarly sized individual samples. If wealso lower the network budget and the rate of exchange ofinitialization parameters, it is able to hold at par with in-dividual training samples 10 times its size. This shows apromising future for our work in the scenario where thereis limited availability of publicly shared datasets as well asstorage budget would be low on participating clients.Performance evaluation in limited network bandwidthsettings against heterogeneous data distributions. To testthe impact of high data distribution heterogeneity on our FLstrategy against an existing SOTA federated learning strat-egy based on knowledge distillation, we show the perfor-mance gains in . We also vary the network initializa-tion rate to test our method in high and low-bandwidth sit-uations. We notice that with the help of patch subset selec-tion, our methods outperform the fed strategy which doesntmake use of this process. This trend is constant across allbandwidth scenarios and local client training expenditures.We have also extended our approach to incorporate FedProxlocal client training regime, which shows better results thannaive local client training. This extendability makes ourmethod unique and viable to more approaches than just onekind of local training which can have added performancebenefits with our algorithm.",
  ". Client-Server Neural Network Architectures": "Evaluating our strategy under homogeneous networkArchitecture. We perform all the experiments in the ear-lier sections using ResNet-8 as the client and server mod-els. To make sure our federated strategy works equally wellamong other homogeneous network distributions, we put it FedAvg Initialisation Rate (in %) Evaluation Accuracy (in %) 68.6 73.2 74.8 66.5 71.3 73.2",
  "Distillation Dataset": "Single ImageCIFAR100 Samples . Comparison of test performance in federated setting using a single image with patch selection compared to the equivalent size ofmultiple independent training samples from a labeled dataset as shared distillation dataset. We use different rates of FedAvg. initializationsto emulate different network bandwidth conditions. Detailed result in .",
  ". Best test performance during 30 rounds of training with CIFAR10 Private Data with Distribuition = 1.0 using ResNet-8 withdifferent distillation datasets and rate of using FedAvg initialisation": "to the test against FedDF using ResNet-20 as well as W-ResNet-16-4 in . We see that under the same dis-tillation set storage budget, our method works better underall the tested network architectures. As per nominal expec-tations, network architectures with more parameters showbetter results than the ones with less number of parameterswhich enables us to achieve better test performance withmore complex networks. Irrespective of the network archi-tecture, the trend is constant when it comes to our FL strat-egy outperforming other strategies making use of a labelleddistillation dataset in a limited storage budget scenario. Evaluating our strategy under heterogeneous networkarchitecture. In the final experimental section, we test ourfederated strategy in the presence of heterogeneity in theclient model architectures. The results present in show the success of our method in training the global mod-els when pitted against a strategy not utilizing a single im-age. It also exhibits the importance of constant distillationtraining for the success of our methods, as our non-uniformapproach gives subpar results with less training time. How-ever, when going from 15k to 11k steps, we also save about 1/3 of the training time and computation resources used onthe server side. It can be an interesting point of extensionto our work to improve upon this non-uniform schedulingto allow for more robust training of heterogeneous modelswith less computation time.",
  ". Patch Selection Mechanism": "Optimal patch sub selection strategies across privatedataset. To find the effectiveness of patch subset selec-tion mechanisms, we test it under different private datasetsfrom different real-world domains (General and Medical).Through , it is evident that the single image patcheswork best in the presence of a selection strategy in our fed-erated algorithm. On their own, both KMeans Balancing(3.2) and Entropy Selection (3.2) strategy works better thanemploying no selection for the same number of patches. To-gether, they perform best across all the datasets we havetested which is what we use in our other experiments in thiswork. Both of the selection strategies and their combinationsignificantly impact the final performance. We have doneour primitive analysis with them in light of this work to find",
  "FedDF67.3 1.973.0 0.675.3 1.2Ours70.2 0.874.1 0.975.7 0.9": ". Best test performance during 30 rounds of training us-ing CIF10 Private Data with Distribuition = 1.0 using differentFed strategies and homogeneous client-server network architec-tures with 20% rate of FedAvg. initialisation. FedDF uses 500CIF100 samples as distillation proxy, while our method makes useof a single image of equivalent size with patch subset selection.",
  "FedDF15K67.4 0.6Ours15K68.5 1.1Ours withScheduling11.3K65.2 1.3": ". Best test performance across during 30 rounds of train-ing using CIF10 Private Data with Distribuition = 1.0 us-ing different Fed strategies and distillation step scheduling, undera heterogenous client architecture distibuition (6 ResNet-8, 7ResNet-20, 7 W-ResNet-16-4) with 20% rate of FedAvg. Ini-tialisation. 500 CIF100 samples have been used as distillationproxy for FedDF, while our method makes use of a Single Im-age of equivalent size with patch selection. an optimal setting (Appendix C, but there might be a corre-lation between their settings which we have not delved into.We can propose this detailed analysis of their combinativework as future work for improving the test performance ofour federated strategy with the means of better data pruning.Impact of selection mechanism with manually labelleddistillation set. We test the viability of our selection mech-anism in case of extending it to the use cases where we al-",
  "No Selection63.4 1.424.2 1.164.5 4.7KMeans66.2 0.821.8 2.167.9 8.4Entropy65.9 1.026.3 1.076.4 2.8KMeans + Entropy (Ours)67.0 1.126.4 1.277.1 3.0": ". Best test performance achieved during 30 rounds of train-ing with different selection mechanisms (Distillation Set Size =5000 patches) across different private datasets ( = 1.0) using ourfederated strategy with ResNet-8 while using 20% rate of FedAvg.initialisations. (2 seeds) ready have a shared public dataset at hand in . Dur-ing the regular exchange of initialization parameters, the ap-plication of our selection mechanism exhibits no advantage.However, when we reduce the exchange of initialization pa-rameters to emulate low bandwidth conditions, it shows sig-nificant gains. This shows that even with the availability ofstandard distillation sets at hand in ensembled distillationmethods, the subset selection mechanism can play an im-portant role in low bandwidth cost federated training.",
  ". Selecting the Best Source of Single Image": "We conduct cross-dataset single-image testing using our al-gorithm across 3 private training datasets and 3 images, withtwo of them corresponding to one of the dataset domainsand the third one being a random noise. The results in Ta-ble 7 exhibit that it is necessary to use a single image thatis similar to the domain of the target task for optimal per-formance. In the case of using a single random noise imageas the distillation proxy, we get the lowest test performanceas it is hard for random augmentations to convert randomnoise patches into a knowledge transfer medium. Hence,care must be taken in choosing a single image with similarpatch representations as the target task for optimal learningwith our algorithm. There can be an interesting area to ex-plore with more augmentation experiments and generativealgorithms if it is possible to use a random noise image vi-ably as a single image with our method. We leave this asfuture work.",
  ". Conclusion": "Through this work, we present a novel approach forfederated learning using ensembled knowledge distillationwith the use of augmented image patches from a singleimage with patch subset selection. We successfully exhibitthe performance gains with our approach in a limitedshared dataset budget scenario as well as low networkbandwidth requiring scenarios with less exchange ofnetwork parameters.Alongside low resource usage, theuse of a single image also enables our federated strategy tobe applicable to scenarios where we have a lack of publicdatasets required during federated training of multipleclients. Future Work. We mention a few specialized avenues of ex-tension to our work during the discussion of results in Sec-tion 4. Some of the key points that were not mentioned in itinclude: Application of the single datum-based federatedlearning to other modalities and machine learning tasks;Application of our work to other knowledge distillation- based algorithms in federated learning other than ensem-bled methods, such as FedMD ; Analysis of differentkind of augmentations to improve the robustness of ourmethod. With the aforementioned points, significant workcan be done to improve the viability of our novel approachpresented in this work to incorporate more real-world chal-lenges.",
  "Daliang Li and Junpu Wang.Fedmd:Heteroge-nous federated learning via model distillation. arXivpreprint arXiv:1910.03581, 2019. 1, 2, 8": "Tian Li, Anit Kumar Sahu, Manzil Zaheer, MaziarSanjabi, Ameet Talwalkar, and Virginia Smith. Fed-erated optimization in heterogeneous networks. Pro-ceedings of Machine learning and systems, 2:429450, 2020. 1 Tao Lin, Lingjing Kong, Sebastian U Stich, and Mar-tin Jaggi. Ensemble distillation for robust model fu-sion in federated learning. Advances in Neural Infor-mation Processing Systems, 33:23512363, 2020. 1,2, 3 Brendan McMahan,Eider Moore,Daniel Ram-age, Seth Hampson, and Blaise Aguera y Arcas.Communication-efficient learning of deep networksfrom decentralized data. In Artificial intelligence andstatistics, pages 12731282. PMLR, 2017. 1, 2, 4 Dianbo Sui, Yubo Chen, Jun Zhao, Yantao Jia, Yuan-tao Xie, and Weijian Sun. FedED: Federated learn-ing via ensemble distillation for medical relation ex-traction. In Proceedings of the 2020 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 21182128, Online, 2020. Associa-tion for Computational Linguistics. 1, 2 Jingwei Sun, Ang Li, Binghui Wang, Huanrui Yang,Hai Li, and Yiran Chen. Soteria: Provable defenseagainst privacy leakage in federated learning fromrepresentation perspective.In Proceedings of theIEEE/CVF conference on computer vision and patternrecognition, pages 93119319, 2021. 1 Zhibo Wang, Mengkai Song, Zhifei Zhang, YangSong, Qian Wang, and Hairong Qi.Beyond infer-ring class representatives: User-level privacy leakagefrom federated learning. In IEEE INFOCOM 2019-IEEE conference on computer communications, pages25122520. IEEE, 2019. 1 Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu,Lin Zhao, Bilian Ke, Hanspeter Pfister, and BingbingNi. Medmnist v2-a large-scale lightweight benchmarkfor 2d and 3d biomedical image classification. Scien-tific Data, 10(1):41, 2023. 4",
  "B.1. Visualisation of Single Images": "We make use of the images depicted in as the sourcesfor generating our distillation dataset. Kindly note, that weonly used the images for non-profit educational researchpurposes and we do not hold any rights over their commer-cial use. These images have been selected in correspon-dence to the domains of datasets we have tested in our work.",
  "5forall ci CP : i [1..C] do": "Find the indices of examples belonging to theci using yn Y : yn = ci.Select indices of the new training examples onthe basis of selection heuristicHK {Easy, Hard, Mixed} with theircorresponding cluster distance values D.Push the training examples from X withselected indices in the new dataset (XK).Remove the selected training examples fromX and D.end",
  "C.1. KMeans Balancing": "To examine the impact of KMeans Balancing and its cor-responding settings on the federated training, we conductexperiments while varying their values on multiple datasetshaving different numbers of prediction labels. The resultshave been presented in . Although values do notdiffer by a large margin for CIFAR10, CIFAR100 resultsprovide us more assurance for the optimal values. We findthat the KMeans selection strategy works best with a highnumber of clusters (K) compared to the number of classes inthe corresponding classification problem (a), whileforcing maximum class balancing while pruning (b)as well as selecting the hard examples (c) for dis-tillation. Since the KMeans model is an independent modelworking on giving the pseudo-labels to the distillation train-ing examples, there could be a play of correlation betweenthe 3 hyper-parameters for this mechanism. It can be aninteresting point for future research on this.",
  "C.2. Entropy Selection": "For the entropy selection mechanism, we only have 2 set-tings to vary: Removal Percentage and Heuristics (.2). The results obtained during ablation studies with thesesettings have been presented in . From the results ina, it is clear that removing training examples withhigh confidence (less entropy) provides us best results. Re-moving a high percentage of training examples from a large",
  "factor = 1.0": ". Best test set accuracy achieved during 30 rounds of train-ing with KMeans balancing under different settings with differentprivate datasets (Distribution = 1.0) using single image patchesas the distillation dataset with 20% FedAvg initialisation rate onResNet-8 (across 2 seeds). initial set using this mechanism also provided us with morerobust training, compared to removing a smaller number oftraining examples from a small initial set (b). Sim-ilar to our last experiments with the KMeans mechanism,the results are more clearly pronounced in the presence ofa more difficult dataset (100 classes compared to 10 in thecase of CIFAR100 and CIFAR10)."
}