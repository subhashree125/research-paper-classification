{
  "Abstract": "The increasing interest in computer vision applicationsfor nutrition and dietary monitoring has led to the de-velopment of advanced 3D reconstruction techniques forfood items. However, the scarcity of high-quality data andlimited collaboration between industry and academia haveconstrained progress in this field. Building on recent ad-vancements in 3D reconstruction, we host the MetaFoodWorkshop and its challenge for Physically Informed 3DFood Reconstruction.This challenge focuses on recon-structing volume-accurate 3D models of food items from2D images, using a visible checkerboard as a size refer-ence. Participants were tasked with reconstructing 3D mod-els for 20 selected food items of varying difficulty levels:easy, medium, and hard. The easy level provides 200 im-ages, the medium level provides 30 images, and the hardlevel provides only 1 image for reconstruction. In total,16 teams submitted results in the final testing phase. Thesolutions developed in this challenge achieved promisingresults in 3D food reconstruction, with significant poten-",
  ". Introduction": "The intersection of computer vision and culinary arts hasopened new frontiers in dietary monitoring and nutritionalanalysis. The CVPR 2024 MetaFood Workshop Challengerepresents a significant step in this direction, addressing thegrowing need for accurate, scalable methods of food por-tion estimation and nutritional intake tracking. These tech-nologies are crucial to promote healthy eating habits andmanaging diet-related health conditions.By focusing on reconstructing accurate 3D models offood items from both multi-view and single-view inputs,this challenge aims to bridge the gap between existing meth-ods and real-world requirements. The challenge encouragesthe development of innovative techniques that can handle",
  ". Sample challenge data for everything bagel": "the complexities of food shapes, textures, and lighting con-ditions, while also addressing the practical constraints ofreal-world dietary assessment scenarios. By bringing to-gether researchers and practitioners in computer vision, ma-chine learning, and nutrition science, this challenge seeks tocatalyze advancements in 3D food reconstruction that couldsignificantly improve the accuracy and applicability of foodportion estimation in various contexts, from personal healthmonitoring to large-scale nutritional studies.Traditional diet assessment methods , such as 24-Hour Recall or Food Frequency Questionnaire (FFQ), oftenrely on manual input, which can be inaccurate and cum-bersome. Furthermore, the absence of 3D information in2D RGB food images presents significant challenges forregression-based methods that estimate food por-tions directly from eating occasion images. By advancing3D reconstruction techniques for food items, we aim to of-fer more precise and intuitive tools for nutritional assess-ment. This technology has the potential to improve the shar-ing of food experiences and significantly impact fields suchas nutrition science and public health.The challenge tasked participants with reconstructing 3Dmodels of 20 selected food items from 2D images, simulat-ing a scenario where a cellphone with a depth camera isused for food logging and nutritional monitoring. It wasstructured into three difficulty levels. The easy level foodobject provided approximately 200 frames uniformly sam-pled from video, the medium level offered about 30 im-ages, and the hard level presented participants with a singlemonocular top-view image. This structure was designed totest the robustness and versatility of the proposed solutionsacross various real-world scenarios. A sample hard foodobject is shown in . The key features of the chal-lenge include the use of a visible checkerboard as a physicalreference, as well as the availability of the depth image foreach video frame, ensuring that the reconstructed 3D mod-els maintain accurate real-world scaling for portion size es-timation.This challenge not only pushes the boundaries of 3D re- construction technology, but also paves the way for moreaccurate, robust, and user-friendly applications in the realworld, such as image-based dietary assessment. The solu-tions developed here have the potential to significantly im-pact how we monitor and understand our nutritional intake,contributing to broader goals in health and wellness. As wecontinue to advance in this field, we anticipate seeing inno-vative applications that could revolutionize personal healthmanagement, nutritional research, and the food industry atlarge. The structure of this technical report is as follows.In , we review existing related work for food por-tion size estimation. In , we introduce the datasetused for this challenge and the detailed evaluation pipelines.Finally, we summarize the methodology and experimentalresults for the three winner teams (VolETA, ININ-VIAUN,FoodRiddle) in , , and , respec-tively.",
  ". Related Work": "Food portion estimation is an important component ofimage-based dietary assessment with the goal of es-timating the volume, energy or macronutrients directly fromthe input eating occasion images. Compared to the widelystudied food recognition task , food por-tion estimation presents a unique challenge due to the ab-sence of 3D information and physical references, which areessential for accurately inferring the real-world size of foodportions. Specifically, accurately estimating portion sizesrequires an understanding of the volume and density of thefood, aspects that cannot be easily determined from a two-dimensional image, which highlights the need for advancedmethodologies and technologies to address this issue. Ex-isting food portion estimation methods can be categorizedinto four main groups .Stereo-Based Approach. These methods rely on multi-ple frames to reconstruct the 3D structure of the food. In, food volume is estimated using multi-view stereo re-construction based on epipolar geometry. Similarly, performs two-view dense reconstruction. Simultaneous Lo-calization and Mapping (SLAM) is utilized in for con-tinuous and real-time food volume estimation. The primarylimitation of these methods is the requirement for multipleimages, which is impractical for real-world deployment.Model-Based Approach. Predefined shapes and templatesare leveraged to estimate the target volume. For instance, assigns certain templates to foods from a library andapplies transformations based on physical references to es-timate the size and location of the food. A similar templatematching approach is used in to estimate food volumefrom a single image. However, these methods cannot ac-commodate variations in food shapes that deviate from pre-defined templates. The most recent work leveraged the3D food mesh as the template to align both camera pose and object pose for portion size estimation.Depth Camera-Based Approach. The depth camera is uti-lized to produce a depth map that captures the distance fromthe camera to the food in the image. In , the depthmap is used to form a voxel representation of the image,which is then used to estimate the food volume. The mainlimitation is the requirement for high-quality depth mapsand additional post-processing needed for consumer depthsensors.Deep Learning Approach. Neural network-based meth-ods leverage the abundance of image data to train complexnetworks for food portion estimation. Regression networksare used in to estimate the energy value of foodfrom a single image input and from an Energy DistributionMap, which maps the input image to the energy distribu-tion of the foods in the image. In , regression networkstrained on input images and depth maps produce energy,mass, and macronutrient information for the food(s) in theimage. Deep learning-based methods require large amountsof data for training and are generally not explainable. Theirperformance often degrades when the input test image dif-fers significantly from the training data.Although these approaches have made significant stridesin food portion estimation, they all face limitations that hin-der their widespread adoption and accuracy in real worldscenarios. Stereo-based methods are impractical for single-image input, model-based approaches struggle with diversefood shapes, depth camera-based methods require special-ized hardware, and deep learning approaches lack explain-ability and struggle with out-of-distribution samples. To ad-dress these challenges, 3D reconstruction offers a promis-ing solution by providing comprehensive spatial informa-tion, adapting to various food shapes, potentially workingwith single images, offering visually interpretable results,and enabling a standardized approach to food portion esti-mation. These advantages motivated the organization of the3D Food Reconstruction challenge, with the aim of over-coming existing limitations and developing more accurate,user-friendly, and widely applicable food portion estimationtechniques that can significantly impact nutritional assess-ment and dietary monitoring.",
  ". Dataset Description": "The MetaFood Challenge dataset comprises 20 carefully se-lected food items from MetaFood3D dataset 1, each scannedwith a 3D scanner and accompanied by corresponding videocaptures. To ensure accurate size representation in the re-constructed 3D models, each item was captured alongsidea checkerboard and pattern mat, serving as physical ref-erences for scaling. The challenge is structured into three",
  "The top-ranking teams from Phase-I are invited to submitcomplete 3D mesh files for each food item. This phase in-volves several steps to ensure accuracy and fairness:": "Model Verification:We verify the submitted modelsagainst the final Phase-I submissions to ensure consis-tency. Additionally, we conduct visual inspections to pre-vent rule violations, such as submitting primitive objects(e.g., spheres) instead of detailed reconstructions. Model Alignment: We will provide participants with theground truth 3D models, along with the script used tocompute the final Chamfer distance. Participants are re-quired to align their models with the ground truth and pre-pare a transform matrix for each submitted object. The fi-nal Chamfer distance score will be computed using thesesubmitted models and transformation matrices.",
  "yYminxX xy22": "(2)This metric provides a comprehensive measure of the simi-larity between the reconstructed 3D models and the groundtruth. The final ranking will be determined by combiningthe scores from both Phase-I (volume accuracy) and Phase-II (shape accuracy). Note that after the Phase-I evaluation,we observed some quality issues with the provided data forobject 12 (steak) and object 15 (chicken nugget). To ensurethe quality and fairness of the competition, we have decidedto exclude these two items from the final overall evaluationprocess.",
  "Overview": "The teams approach combines computer vision and deeplearning techniques to estimate food volume from RGBDimages and masks accurately. Keyframe selection ensuresdata quality, aided by perceptual hashing and blur detec-tion. Camera pose estimation and object segmentation laythe groundwork for neural surface reconstruction, produc-ing detailed meshes for volume estimation.Refinementsteps enhance accuracy, including isolated piece removaland scaling factor adjustment. The teams approach offers acomprehensive solution for precise food volume assessmentwith potential applications in nutrition analysis.",
  "The Teams Proposal: VolETA": "The team begins their approach by acquiring input data,specifically RGBD images and corresponding food objectmasks. These RGBD images, denoted as ID = {IDi }ni=1,where n is the total number of frames, provide the necessarydepth information alongside the RGB images. The food ob-ject masks, denoted as {M if}ni=1, aid in identifying the re-gions of interest within these images.Next, the team proceeds with keyframe selection. Fromthe set {IDi }ni=1, keyframes {IKi }kj=1 {IDi }ni=1 are se-lected. The team implements a method to detect and removeduplicates and blurry images to ensure high-qualityframes. This involves applying the Gaussian blurring ker-nel followed by the fast Fourier transform method. Near-Image Similarity employs a perceptual hashing andhamming distance thresholding to detect similar images andkeep overlapping. The duplicates and blurry images are ex-cluded from the selection process to maintain data integrityand accuracy, as shown in (a).Using the selected keyframes {IKj }kj=1, the team esti-mates the camera poses through PixSfM (i.e., extract-ing features using SuperPoint , matching them using Su-perGlue , and refining them). The outputs are the set ofcamera poses {Cj}kj=1, which are crucial for spatial under-standing of the scene.In parallel, the team utilizes the SAM for referenceobject segmentation. SAM segments the reference objectwith a user-provided segmentation prompt (i.e., user click),producing a reference object mask M R for each keyframe.This mask is a foundation for tracking the reference objectacross all frames. The team then applies the XMem++ method for memory tracking, which extends the referenceobject mask M R to all frames, resulting in a comprehen-sive set of reference object masks {M Ri }ni=1. This ensuresconsistency in reference object identification throughout thedataset.To create RGBA images, the team combines the RGBimages, reference object masks {M Ri }ni=1, and food objectmasks {M Fi }ni=1. This step, denoted as {IRi }ni=1, integratesthe various data sources into a unified format suitable forfurther processing, as shown in (b).The team converts the RGBA images {IRi }ni=1 and cam-era poses {Cj}kj=1 into meaningful metadata and modeleddata Dm. This transformation facilitates the accurate recon-struction of the scene.The modeled data Dm is then input into NeuS2 for mesh reconstruction. NeuS2 generates colorful meshes{Rf, Rr} for the reference and food objects, providing de-tailed 3D representations of the scene components.Theteam applies the Remove Isolated Pieces technique to re-fine the reconstructed meshes. Given that the scenes containonly one food item, the team sets the diameter thresholdto 5% of the mesh size. This method deletes isolated con-",
  "R": ". The teams one-shot food volume estimation approach.The team begins with a food-segmented image (IRf ), and then usesthe One-2-3-45 model to generate a mesh (Rf). Next, the teamcleans up the isolated pieces that are less than 5% of the (Rf)size, resulting in a cleaned food mesh RCf . Furthermore, the teamchooses a scaling factor based on the depth information Sf. Fi-nally, the team applies the chosen scaling factor on RCf to have ascaled mesh (RFf ) where the team extracts the volume.",
  "Generally, 3D reconstruction methods generate unitlessmeshes (i.e., no physical scale) by default. To overcomethis limitation, the team manually identifies the scaling fac-": "tor by measuring the distance for each block for the refer-ence object mesh, as shown in b. Next, the team takesthe average of all blocks lengths lavg, while the actual real-world length (as shown in a) is constant lreal = 0.012in meter. Furthermore, the team applies the scaling factorS = lreal/lavg on the clean food mesh RCf , producing thefinal scaled food mesh RFf in meter. The team leverages depth information alongside foodand reference object masks to validate the scaling factors.The teams method for assessing food size entails utilizingoverhead RGB images for each scene. Initially, the teamdetermines the pixel-per-unit (PPU) ratio (in meters) usingthe reference object. Subsequently, the team extracts thefood width (fw) and length (fl) employing a food objectmask. To ascertain the food height (fh), the team followsa two-step process. Firstly, the team conducts binary im-age segmentation using the overhead depth and referenceimages, yielding a segmented depth image for the referenceobject. The team then calculates the average depth utilizing",
  ". The team manually measures the scaling factor usingMeshLabs Measuring tool. The team measures multiple blocksin the reference object mesh; then, the team takes the average ofblocks lengths lavg": "the segmented reference object depth (dr). Similarly, em-ploying binary image segmentation with an overhead foodobject mask and depth image, the team computes the av-erage depth for the segmented food depth image (df). Fi-nally, the estimated food height fh is computed as the ab-solute difference between dr and df. Furthermore, to as-sess the accuracy of the scaling factor S, the team computesthe food bounding box volume ((fw fl fh) PPU).The team evaluates if the scaling factor S generates a foodvolume close to this potential volume, resulting in Sfine. shows the scaling factors, PPU, 2D Reference ob-ject dimensions, 3D food object dimensions, and potentialvolume. For one-shot 3D reconstruction, the team leverages One-2-3-45 for reconstructing a 3D from a single RGBAview input after applying binary image segmentation onboth food RGB and mask. Next, the team removes isolatedpieces from the generated mesh. After that, the team reusesthe scaling factor S, which is closer to the potential volumeof the clean mesh, as shown in .",
  "Implementation settings": "The team ran the experiments using two GPUs, GeForceGTX 1080 Ti/12G and RTX 3060/6G. The team set thehamming distance as 12 for the near image similarity. ForGaussian kernel radius, the team set the even numbers in therange [0...30] for detecting blurry images. The team set thediameter as 5% for removing isolated pieces. The numberof iteration of NeuS2 is 15000, mesh resolution is 512512,the unit cube aabb scale is 1, scale: 0.15, and offset:[0.5, 0.5, 0.5] for each food scene.",
  "VolETA Results": "The team extensively validated their approach on the chal-lenge dataset as described in and compared theirresults with ground truth meshes using MAPE and Cham-fer distance metrics. More Briefly, the team leverages theirapproach for each food scene separately. A one-shot foodvolume estimation approach is applied if the number ofkeyframes k equals 1. Otherwise, a few-shot food volumeestimation is applied. Notably, shows that the teamskeyframe selection process chooses 34.8% of total framesfor the rest of the pipeline, where it shows the minimumframes with the highest information.After finding the keyframes, PixSfM estimates theposes and point cloud (see ).After generating the scaled meshes, the team calculatesthe volumes and Chamfer distance with and without trans-formation metrics. The team registered their meshes andground truth meshes to obtain the transformation metricsusing ICP (see ). presents the quantitative comparisons of theteams volumes and Chamfer distance with and without theestimated transformation metrics from ICP.For overall method performance, shows theMAPE and Chamfer distance with and without transforma-tion metrics.Additionally, shows the qualitative results on theone and few-shot 3D reconstruction from the challengedataset. The figures show that the teams model excels intexture details, artifact correction, missing data handling,and color adjustment across different scene parts.",
  "20pizza0.010344827590.019132923364426511.176123.97": ". A list of information that extracted using the RGBD and masks, where the team presents the scene Id, the scaling factor Sfine,Pixel-Per-Unit (in cm), 2D reference object dimensions (Rw Rl), 3D food object dimensions (fw fl fh) in pixels, and the potentialvolume (in cm3). The rows in red are excluded meshes.",
  "dressed in future work:": "Manual processes: The current pipeline includes man-ual steps, such as providing a segmentation prompt andidentifying scaling factors. These steps should be auto-mated to enhance efficiency and reduce human interven-tion. This limitation arises from the necessity of using areference object to compensate for missing data sources,such as Inertial Measurement Unit (IMU) data. Input requirements: The teams method requires exten-sive input information, including food masks and depthdata. Streamlining the necessary inputs would simplifythe process and potentially increase its applicability in",
  "varied settings": "Complex backgrounds and objects: The team has nottested their method in environments with complex back-grounds or on highly intricate food objects.Applyingtheir approach to datasets with more complex food items,such as the Nutrition5k dataset, would be challeng-ing and could help identify corner cases that need to beaddressed. Capturing complexities: The method has not been eval-uated under different capturing complexities, such asvarying distances between the camera and the food ob-ject, different camera speeds, and other scenarios as de-",
  "Scale factor estimation": "The pipeline for coordinate-level scale factor estimation isshown in . The team follows a corner projectionmatching method.Specifically, using the COLMAPdense model, the team obtains the pose of each image aswell as dense point cloud information. For any image imgkand its extrinsic parameters [R/t]k, the team first performsa threshold-based corner detection with the threshold set to240. This allows them to obtain the pixel coordinates of alldetected corners. Subsequently, using the intrinsic parame-ters k and the extrinsic parameters [R/t]k, the point cloud is",
  "(P ki P kj )2i = j(3)": "To determine the final computed length of each checker-board square in image k, the team takes the minimum valueof each row of the matrix Dk (excluding the diagonal) toform the vector dk. The median of this vector is then used.The final scale calculation formula is given by Equation 4,where 0.012 represents the known length of each square(1.2 cm):",
  "D Reconstruction": "The pipeline for 3D reconstruction is shown in .Considering the differences in input viewpoints, the teamutilizes two pipelines to process the first fifteen objects andthe last five single view objects.For the first fifteen objects, the team uses COLMAPto estimate the poses and segment the food using the pro-vided segment masks in the dataset.Then, they applyadvanced multi-view 3D reconstruction methods to recon-struct the segmented food. In practice, the team employsthree different reconstruction methods:COLMAP,DiffusioNeRF, and NeRF2Mesh. They select thebest reconstruction results from these methods and extract the mesh from the reconstructed model. Next, they scalethe extracted mesh using the estimated scale factor. Finally,they apply some optimization techniques to obtain a refinedmesh.For the last five single-view objects, the team ex-periments with several single-view reconstruction meth-ods, such as Zero123, Zero123++, One2345,ZeroNVS, and DreamGaussian.They chooseZeroNVS to obtain a 3D food model consistent withthe distribution of the input image. In practice, they use theintrinsic camera parameters from the fifteenth object andemploy an optimization method based on reprojection er-ror to refine the extrinsic parameters of the single camera.However, due to the limitations of single-view reconstruc-tion, the team needs to incorporate depth information fromthe dataset and the checkerboard in the monocular image todetermine the size of the extracted mesh. Finally, they applyoptimization techniques to obtain a refined mesh.",
  "Mesh refinement": "In the 3D Reconstruction phase, the team observes that themodels results often suffer from low quality due to the pres-ence of holes on the object surface and substantial noise, asillustrated in .To address the holes, the team employs MeshFix, anoptimization method based on computational geometry. Forsurface noise, they utilize Laplacian Smoothing for meshsmoothing operations. The Laplacian Smoothing methodworks by adjusting the position of each vertex to the averageof its neighboring vertices:",
  "Alignment": "The team designs a multi-stage alignment method for evalu-ating reconstruction quality. illustrates the align-ment process for Object 14. First, the team calculates thecentral points of both the predicted model and the groundtruth model, and moves the predicted model to align thecentral point of the ground truth model. Next, they performICP registration for further alignment, significantly reduc-",
  "Multi-View Reconstruction": "For Structure from Motion (SfM), the team extended thestate-of-the-art method COLMAP by incorporating Su-perPoint and SuperGlue methodologies. This sig-nificantly mitigated the issue of sparse keypoints in weaklytextured scenes, as shown in .For mesh reconstruction, the teams method is based on2D Gaussian Splatting, which utilizes a differentiable2D Gaussian renderer and incorporates regularization termsfor depth distortion and normal consistency. The TruncatedSigned Distance Function (TSDF) results are used to gener-ate a dense point cloud.In the post-processing stage, the team applied filteringand outlier removal techniques, identified the contour of thesupporting surface, and projected the lower mesh verticesonto the supporting surface. They used the reconstructedcheckerboard to rectify the scale of the model and usedPoisson reconstruction to generate a watertight, completemesh of the subject.",
  "Single-View Reconstruction": "For 3D reconstruction from a single image, the team em-ployed state-of-the-art methods such as LGM, InstantMesh, and One-2-3-45 to generate an initial priormesh. This prior mesh was then jointly corrected with depthstructure information.To adjust the scale, the team estimated the objects lengthusing the checkerboard as a reference, assuming the object . For multi-view image inputs, COLMAP integrated with SuperPoint and SuperGlue generates SFM points, which are used tocreate the initial Gaussian. Using 2D Gaussian Splatting, we obtain the mesh of the observed object. Subsequently, we adjust the meshsize and fit the unobserved underside of the object using the RGB checkerboard and depth maps. Finally, a complete and realistic foodmesh is produced. For single-view input, we use the AIGC method to generate multi-view images consistent with the input image in 3Dusing a multi-view diffusion model. Then, using the Sparse-view Large Reconstruction Model, we directly predict the mesh.Lastly, usingthe simultaneously reconstructed checkboard, adjust the size of the food. . The top left image showcases the vanilla COLMAPsfm points.The light-colored areas on the bread have fewerfeature points, leading to the incomplete mesh in the top-rightimage.However, by integrating SuperPoint and SuperGlue intoCOLMAP, more interest points are obtained, resulting in an ex-cellent final mesh, as shown in the bottom-right image.",
  ". Experimental Results": "Through a process of nonlinear optimization, the teamsought to identify a transformation that minimizes theChamfer distance between their mesh and the groundtruth mesh.This optimization aimed to align the twomeshes as closely as possible in three-dimensional space.Upon completion of this process, the average Chamfer dis-tance across the final reconstructions of the 20 objectsamounted to 0.0032175 meters.As shown in ,Team FoodRiddle achieved the best scores for both multi-view and single-view reconstructions, outperforming otherteams in the competition. The source code is available at",
  ".Total Errors for Different Teams on Multi-view andSingle-view Data. Team FoodRiddle has the best score": "Food Reconstruction. The challenge aimed to advance 3Dreconstruction techniques by focusing on food items, ad-dressing the unique challenges posed by varying textures,reflective surfaces, and complex geometries typical in culi-nary subjects.The competition utilized 20 diverse food items, capturedunder different conditions and with varying numbers of in-put images, specifically designed to challenge participantsin developing robust reconstruction models. The evalua-tion was based on a two-phase process, assessing both por-tion size accuracy through Mean Absolute Percentage Er-ror (MAPE) and shape accuracy using the Chamfer distancemetric.Out of all participating teams, three made it to the fi-nal submission, showcasing a range of innovative solu-tions. Team VolETA won the first place with the overallbest performance on both Phase-I and Phase-II. Followedby team ININ-VIAUN who won the second place. In addi-tion, FoodRiddle team demonstrated superior performancein Phase-II, indicating a competitive and high-caliber fieldof entries for 3D mesh reconstruction. The challenge hassuccessfully pushed the boundaries of 3D food reconstruc-tion, demonstrating the potential for accurate volume esti-mation and shape reconstruction in nutritional analysis andfood presentation applications. The innovative approachesdeveloped by the participating teams provide a solid foun-dation for future research in this field, potentially leading tomore accurate and user-friendly methods for dietary assess-ment and monitoring.",
  "Another Author. Locality-aware laplacian mesh smoothing.arXiv preprint arXiv:1606.00803, 2016. 9": "Maksym Bekuzarov, Ariana Bermudez, Joon-Young Lee,and Hao Li. Xmem++: Production-level video segmentationfrom few annotated frames. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 635644, 2023. 4 Paolo Cignoni, Marco Callieri, Massimiliano Corsini, Mat-teo Dellepiane, Fabio Ganovelli, Guido Ranzuglia, et al.Meshlab:an open-source mesh processing tool.InEurographics Italian chapter conference, pages 129136.Salerno, Italy, 2008. 5",
  "Kanjar De and V Masilamani. Image sharpness measure forblurred images in frequency domain. Procedia Engineering,64:149158, 2013. 4": "Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-novich. Superpoint: Self-supervised interest point detectionand description. In 2018 IEEE/CVF Conference on Com-puter Vision and Pattern Recognition Workshops (CVPRW),pages 33733712, 2018. 11 Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-novich. Superpoint: Self-supervised interest point detectionand description. In Proceedings of the IEEE conference oncomputer vision and pattern recognition workshops, pages224236, 2018. 4 Anqi Gao, Frank P.-W. Lo, and Benny Lo. Food volumeestimation for quantifying dietary intake with a wearablecamera. Proceedings of the 2018 IEEE 15th InternationalConference on Wearable and Implantable Body Sensor Net-works, pages 110113, 2018. 2",
  "Tanuj Jain, Christopher Lennan, Zubin John, and DatTran. Imagededup. 2019. 4": "Wenyan Jia, Yaofeng Yue, John D Fernstrom, ZhengnanZhang, Yongquan Yang, and Mingui Sun. 3d localization ofcircular feature in 2d image and application to food volumeestimation. Proceedings of the 2012 Annual InternationalConference of the IEEE Engineering in Medicine and Biol-ogy Society, pages 45454548, 2012. 2 Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-thing. In Proceedings of the IEEE/CVF International Con-ference on Computer Vision, pages 40154026, 2023. 4 Philipp Lindenberger, Paul-Edouard Sarlin, Viktor Lars-son, and Marc Pollefeys.Pixel-perfect structure-from-motion with featuremetric refinement.In Proceedings ofthe IEEE/CVF international conference on computer vision,pages 59875997, 2021. 4, 6 Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, MukundVarma T, Zexiang Xu, and Hao Su. One-2-3-45: Any singleimage to 3d mesh in 45 seconds without per-shape optimiza-tion. Advances in Neural Information Processing Systems,36, 2024. 11 Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, MukundVarma T, Zexiang Xu, and Hao Su. One-2-3-45: Any singleimage to 3d mesh in 45 seconds without per-shape optimiza-tion. Advances in Neural Information Processing Systems,36, 2024. 6, 8, 9 Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-makov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:Zero-shot one image to 3d object.In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 92989309, 2023. 8, 9 Frank P.-W. Lo, Yingnan Sun, and Benny Lo. Depth estima-tion based on a single close-up image with volumetric anno-tations in the wild: A pilot study. Proceedings of the 2019IEEE/ASME International Conference on Advanced Intelli-gent Mechatronics, pages 513518, 2019. 3 Frank Po Wen Lo, Yingnan Sun, Jianing Qiu, and Benny Lo.Image-Based Food Classification and Volume Estimation forDietary Assessment: A Review. IEEE Journal of Biomedicaland Health Informatics, 24(7):19261939, 2020. 2 Runyu Mao, Jiangpeng He, Luotao Lin, Zeman Shao,Heather A. Eicher-Miller, and Fengqing Zhu. Improving di-etary assessment via integrated hierarchy food classification.2021 IEEE 23rd International Workshop on Multimedia Sig-nal Processing (MMSP), pages 16, 2021. 2 Runyu Mao, Jiangpeng He, Zeman Shao, Sri Kalyan Yarla-gadda, and Fengqing Zhu. Visual aware hierarchy based foodrecognition. Proceedings of the International conference onpattern recognition, pages 571598, 2021. Weiqing Min, Zhiling Wang, Yuxin Liu, Mengjiang Luo,Liping Kang, Xiaoming Wei, Xiaolin Wei, and ShuqiangJiang. Large scale visual food recognition. IEEE Transac-tions on Pattern Analysis and Machine Intelligence, 45(8):99329949, 2023. Xinyue Pan, Jiangpeng He, and Fengqing Zhu. Personal-ized food image classification: Benchmark datasets and newbaseline. In 2023 57th Asilomar Conference on Signals, Sys-tems, and Computers, pages 10951099. IEEE, 2023. 2 Manika Puri, Zhiwei Zhu, Qian Yu, Ajay Divakaran, andHarpreet Sawhney. Recognition and volume estimation offood intake using a mobile device. Proceedings of the 2009Workshop on Applications of Computer Vision, pages 18,2009. 2 Md Hafizur Rahman, Qiang Li, Mark Pickering, MichaelFrater, Deborah Kerr, Carol Bouchey, and Edward Delp.Food volume estimation in a mobile phone based dietary as-sessment system. Proceedings of the 2012 8th InternationalConference on Signal Image Technology and Internet BasedSystems, pages 988995, 2012. 2",
  "Szymon Rusinkiewicz and Marc Levoy. Efficient variants ofthe icp algorithm. In Proceedings third international confer-ence on 3-D digital imaging and modeling, pages 145152.IEEE, 2001. 6": "Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann,Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry La-gun, Li Fei-Fei, Deqing Sun, et al. Zeronvs: Zero-shot 360-degree view synthesis from a single image. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 94209429, 2024. 9 Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,and Andrew Rabinovich.Superglue:Learning featurematching with graph neural networks.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 49384947, 2020. 4, 11",
  "Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited.In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages41044113, 2016. 8, 9": "Zeman Shao, Shaobo Fang, Runyu Mao, Jiangpeng He, Ja-nine L. Wright, Deborah A. Kerr, Carol J. Boushey, andFengqing Zhu. Towards learning food portion from monoc-ular images with cross-domain feature adaptation. Proceed-ings of 2021 IEEE 23rd International Workshop on Multime-dia Signal Processing, pages 16, 2021. 2, 3 Zeman Shao, Yue Han, Jiangpeng He, Runyu Mao, JanineWright, Deborah Kerr, Carol Jo Boushey, and Fengqing Zhu.An integrated system for mobile image-based dietary assess-ment. Proceedings of the 3rd Workshop on AIxFood, page1923, 2021. 2 Zeman Shao, Gautham Vinod, Jiangpeng He, and FengqingZhu.An end-to-end food portion estimation frameworkbased on shape reconstruction from monocular image. Pro-ceedings of 2023 IEEE International Conference on Multi-media and Expo, pages 942947, 2023. 3 Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu,Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and HaoSu. Zero123++: a single image to consistent multi-view dif-fusion base model. arXiv preprint arXiv:2310.15110, 2023.9 Jan Steinbrener, Vesna Dimitrievska, Federico Pittino, FransStarmans, Roland Waldner, Jurgen Holzbauer, and ThomasArnold. Learning metric volume estimation of fruits and veg-etables from short monocular video sequences. Heliyon, 9(4), 2023. 8",
  "Frances E Thompson and Amy F Subar. Dietary assessmentmethodology. Nutrition in the Prevention and Treatment ofDisease, pages 548, 2017. 2": "Gautham Vinod, Zeman Shao, and Fengqing Zhu. Imagebased food energy estimation with depth domain adapta-tion. Proceedings of 2022 IEEE 5th International Confer-ence on Multimedia Information Processing and Retrieval,pages 262267, 2022. 3 Gautham Vinod, Jiangpeng He, Zeman Shao, and FengqingZhu. Food portion estimation via 3d object scaling. Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 37413749, 2024. 2 Yiming Wang, Qin Han, Marc Habermann, Kostas Dani-ilidis, Christian Theobalt, and Lingjie Liu.Neus2: Fastlearning of neural implicit surfaces for multi-view recon-struction.In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 32953306, 2023. 4 Jamie Wynn and Daniyar Turmukhambetov. Diffusionerf:Regularizing neural radiance fields with denoising diffu-sion models. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 41804189, 2023. 9 Chang Xu, Ye He, Nitin Khanna, Carol J. Boushey, and Ed-ward J. Delp. Model-based food volume estimation using 3dpose. Proceedings of the 2013 IEEE International Confer-ence on Image Processing, pages 25342538, 2013. 2 Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang,Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3dmesh generation from a single image with sparse-view largereconstruction models.arXiv preprint arXiv:2404.07191,2024. 11"
}