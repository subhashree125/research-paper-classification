{
  "V3Det Challenge 2024 on Vast Vocabulary and Open Vocabulary ObjectDetection: Methods and Results": "Jiaqi Wang, Yuhang Zang, Pan Zhang, Tao Chu, Yuhang Cao, Zeyi Sun, Ziyu Liu, Xiaoyi Dong,Tong Wu, Dahua Lin, Zeming Chen, Zhi Wang, Lingchen Meng, Wenhao Yao, Jianwei Yang,Sihong Wu, Zhineng Chen, Zuxuan Wu, Yu-Gang Jiang, Peixi Wu, Bosong Chai, Xuan Nie,Longquan Yan, Zeyu Wang, Qifan Zhou, Boning Wang, Jiaqi Huang, Zunnan Xu, Xiu LiKehong Yuan, Yanyan Zu, Jiayao Ha, Qiong Gao, Licheng Jiao",
  "Abstract": "Detecting objects in real-world scenes is a complex taskdue to various challenges, including the vast range ofobject categories, and potential encounters with previouslyunknown or unseen objects.The challenges necessitatethe development of public benchmarks and challengesto advance the field of object detection. Inspired by thesuccess of previous COCO and LVIS Challenges,we organize the V3Det Challenge 2024 in conjunction withthe 4th Open World Vision Workshop: Visual Perceptionvia Learning in an Open World (VPLOW) at CVPR 2024,Seattle, US. This challenge aims to push the boundariesof object detection research and encourage innovationin this field. The V3Det Challenge 2024 consists of twotracks: 1) Vast Vocabulary Object Detection: This trackfocuses on detecting objects from a large set of 13204categories, testing the detection algorithms ability torecognize and locate diverse objects. 2) Open VocabularyObject Detection: This track goes a step further, requiringalgorithms to detect objects from an open set of categories,including unknown objects. In the following sections, wewill provide a comprehensive summary and analysis ofthe solutions submitted by participants. By analyzing themethods and solutions presented, we aim to inspire futureresearch directions in vast vocabulary and open-vocabularyobject detection, driving progress in this field. Challengehomepage:",
  "Object detection has witnessed tremendous advancementsin recent years , with public": "Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Zeyi Sun, Ziyu Liu,Xiaoyi Dong, Yuhang Zang, Tong Wu, Dahua Lin are the organizers ofthe V3Det challenge, and other authors participated in the challenge. TheAppendix lists the authors teams and affiliations. benchmarks and challenges serving as a cru-cial catalyst for innovation. However, the challenges in real-world object detection remain exists, such as dealing withvast vocabulary (i.e., large numbers of object classes) andopen-vocabulary scenarios (i.e., detecting objects from un-seen or unknown classes). To contribute to the advancementof object detection methods to handle vast vocabulary andopen-vocabulary scenarios, the V3Det Challenge 2024 hasbeen established. The V3Det challenge aims to foster inno-vation in robust and general object detection algorithms.This challenge is based on the Vast Vocabulary VisualDetection (V3Det) dataset , which not only encom-passes objects from 13,204 categories, ten times the size ofexisting large vocabulary object detection datasets but alsoemphasizes the hierarchical and interrelated nature of cate-gories, providing an ideal testbed for research in extensiveand open vocabulary object detection. The rich annotationsof V3Det, meticulously provided by human experts, ensurehigh precision and in-depth interpretation of the data.The V3Det Challenge 2024 features two tracks for par-ticipants. The first track is vast vocabulary object detectionwith 13204 categories, focusing on detecting objects with alarge vocabulary of categories. The second track is openvocabulary object detection, which is split into base andnovel classes, and only allows annotations of base classesfor training. The evaluation server is hosted on the Eval AIplatform 1. In total, 20 teams made valid submissions to thechallenge tracks. The methods of top teams on vast vocabu-lary and open vocabulary are presented in Sec. 2 and Sec. 3,respectively.",
  "MixPL v2: Expanding Detection Categories via Semi-Supervised Learning": "MotivationIn , they provide an in-depth analysisof the importance of using unlabeled data with the num-ber of categories increasing. Fine-tuning the EVA modelon the COCO and LVIS datasets achieves significant de-tection performance improvements. However, when fine-tuning on the V3Det dataset, the gains are more modest.This indicates that fine-tuning base models struggle to ef-fectively map pre-trained visual representations to the nu-anced semantics of significantly expanded detection cat-egories. In contrast, semi-supervised learning effectivelybridges the gap between high-granularity category seman-tics and pre-trained visual representations through the useof pseudo-labels. This enables the motivation of combiningself-training with vast vocabulary object detection. PipelineThe winning team designs a semi-supervisedpipeline MixPL v2 utilizing MixPL that uses the V3Detas the labeled dataset and the Objects365 as the unla-beled dataset. Based on the Mean Teacher , they designthe Mixed Pseudo Label pipeline for semi-supervised ob-ject detection. They apply weak and strong augmentationsfor unlabeled images and use the teacher model to predictpseudo-labels on weakly augmented images. The teachermodel is the accumulated Exponential Moving Average(EMA) of the student model. They select the Mixup and Mosaic as augmentation techniques. Implementation DetailsThey apply the designed semi-supervised learning algorithms on the Co-DETR modeland Swin-L backbone. They adopt additional tech-niques such as Gradient Accumulation for loss computationto reduce the negative impact of small batch size.",
  "RichSem-DINO-FocalNet for V3Det Challenge 2024": "PipelineThe second-place team, lcmeng, utilizes theRichSem-DINO detection framework, incorporatingthe FocalNet-Huge as its baseline.For better ini-tialization, the model is firstly pre-trained on Objects365dataset , and then fine-tuned on the V3Det training set. Implementation DetailsThe system is based on a two-stage Deformable DETR structure that uses multi-scale deformable attention for better high-resolution fea-tures. Concretely, the author uses 5 scale features and 900object queries. For further improvement, they use Test TimeAugmentation (TTA) with two scale inputs and class-awareNMS (threshold 0.7) to filter redundant predictions. presents the main results of the 2nd place solution, whichsurpasses the previous SoTA by more than 2 AP under thesupervised setting with the help pf TTA.",
  "Enhanced Object Detection: A Study on Vast Vocabu-lary Object Detection Track for V3Det Challenge 2024": "PipelineThe third-place team, TCSVT , utilizes theCascade R-CNN detection framework, incorporating theSwin-B as its backbone. To better capture semantic in-formation, they integrated a bottom-up structure inspired byPA-Net into the Cascade R-CNN, enhancing shallowfeature transmission and utilization. Implementation DetailsFor better initialization, theSwin-B backbone is pretrained on the ImageNet-22K dataset. To improve the training datasets size and quality,the author applied data augmentations like flipping, jitter-ing, and scaling. They also introduced the DIoU Loss",
  "CenterNet2-RN50-OVD 9.73.328.6Detic-RN50-ImageNet 11.55.828.6RichSem-DINO-Focal-Huge-TTA 22.9 15.644.8": "to address coordinate interrelationships and improve regres-sion accuracy and convergence speed.Additionally, theGeneralized Focal Loss (GFL) was applied to the Re-gion Proposal Network to balance positive and negativesample proportions. shows the ablations results on the V3Det valida-tion set. They try different modules and use the DIoU lossand GFL loss in the final solution.",
  ". Solution of Fourth Place (JYYY)": "Team JYYY: V3Det Challenge 2024 - Vast VocabularyVisual DetectionThe fourth-place team uses the ensemble solution offour detectors: DINO , YOLO V8 , DETR ,and Cascade R-CNN . presents the abla-tion studies of different models, where Cascade R-CNNperforms best among all the models.They also applypost-processing techniques such as Test-Time Augmenta-tion and Weighted Boxes Fusion .",
  ". Discussion": "In the open-vocabulary object detection track, participantsuse prior knowledge from Vision-Language Models such asCLIP and Long CLIP to recognize novel objects.Despite this progress, the best AP in novel classes is only15.6 and there remains room for improvement. Specifically,we encourage future research to explore combining LargeVision Language Models (LVLMs) to enhance recognitionof novel objects .",
  ". Open Vocabulary Object Detection": "This track splits all V3Det classes into the base (6709) andnovel (6495) classes. The object detectors are expected toaccurately detect objects of both base classes, with com-plete annotations, and novel classes, with only informationincluding class names, class descriptions, and object-centricexemplar images for each class during inference. DataFor base classes, complete annotations are given.For novel classes, only class information is given, includ-ing class names, and class descriptions. Any other publicdatasets, if they dont include bounding box annotations fornovel classes, such as other detection, image classification(except the Bamboo ), and text-image datasets are allpermitted. Evaluation MetricsThe results will be evaluated on allimages (29863) in the test set of V3Det for all 13204 cate-gories. The results will be evaluated at most 300 boxes perimage. The mAP metric is adopted as the metric follow-ing the COCO dataset. The performance of base and novelclasses will be summarized as APbase and APnovel. Thefinal results for challenge ranking will be",
  ". Solution of the First Place (lcmeng)": "RichSem-DINO-FocalNet for V3Det Challenge 2024To enable open-vocabulary detection, the author trans-formed the conventional closed-set classifier into alignmentwith prototype embeddings of both text and image samples.Specifically, they employed the Long-CLIP text en-coder to extract text embeddings from GPT-4V categorydescriptions and used the CLIP-ViT-Large vision en-coder to extract image embeddings from example images,averaging these embeddings for each class.To increasethe number of example images per category, they imple-mented two strategies: cropping sub-images based on anno-tation bounding boxes in the training set, and using images from overlapping classes between V3Det and ImageNet-22K as supplements. represents the main results in the OVD track.The first-place solution achieves more than 10 overall APimprovements compared to the baseline model even withoutintroducing additional classification datasets into training.",
  ". Solution of the Second Place (TCSVT)": "Enhanced Object Detection: A Study on Vast Vocabu-lary Object Detection Track for V3Det Challenge 2024To enable open-vocabulary detection, the author replacesthe conventional closed-set classifier with the CLIP clas-sifier used in Detic . Specifically, the CLIP textencoder extracts text embeddings from V3Det category de-scriptions, replacing the classifiers weights with these em-beddings. The model is first trained on the V3Det base classtraining set, then directly infers results on the test set usingthe text embeddings of both base and novel classes.",
  ". Solution of the Third Place (Innovision)": "3rd Place Solution for OVD Track in CVPR 2024VPLOW workshop: Visual Perception via Learning inan Open WorldThe third-place team, Innovision, utilized the Center-Net2 detection framework, incorporating the SwinTransformer as its backbone. CenterNet2 detects ob-jects by identifying their centers and regressing to boxparameters.Innovision enhanced this framework to op-erate at multiple scales using a Feature Pyramid Net-work (FPN) , generating feature maps with stridesfrom 8 to 128 (P3-P7). Both classification and regressionbranches were applied at all FPN levels to produce detec-tion heatmaps and bounding box regression maps.For open-vocabulary detection, the author employs asimilar strategy with team TCSVT, please refer the Sec. 3.3for details.",
  "Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-shick. Mask R-CNN. In ICCV, 2017. 1": "Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Ste-fan Popov, Matteo Malloci, Alexander Kolesnikov, et al.The Open Images Dataset V4: Unified image classification,object detection, and visual relationship detection at scale.IJCV, 2020. 1 Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang,and Jian Yang. Generalized focal loss v2: Learning reliablelocalization quality estimation for dense object detection. InCVPR, pages 1163211641, 2021. 3",
  "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InICCV, 2021. 2, 4": "Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xi-aoyi Dong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang.RAR: Retrieving and ranking augmented mllms for visualrecognition. arXiv preprint arXiv:2403.13805, 2024. 4 Lingchen Meng, Xiyang Dai, Jianwei Yang, DongdongChen, Yinpeng Chen, Mengchen Liu, Yi-Ling Chen, Zux-uan Wu, Lu Yuan, and Yu-Gang Jiang. Learning from richsemantics and coarse locations for long-tailed object detec-tion. NIPS, 36, 2024. 2 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 3, 4",
  "Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, andDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-tion. In ICLR, 2018. 2": "Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, JunZhu, Lionel M Ni, and Heung-Yeung Shum. DINO: Detrwith improved denoising anchor boxes for end-to-end objectdetection. arXiv preprint arXiv:2203.03605, 2022. 3 Yuanhan Zhang, Qinghong Sun, Yichun Zhou, Zexin He,Zhenfei Yin, Kun Wang, Lu Sheng, Yu Qiao, Jing Shao,and Ziwei Liu. Bamboo: Building mega-scale vision datasetcontinually with human-machine synergy.arXiv preprintarXiv:2203.07845, 2022. 2, 3"
}