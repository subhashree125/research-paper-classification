{
  "Abstract": "Despite noise and caption quality having been acknowl-edged as important factors impacting vision-language con-trastive pre-training, in this paper, we show that the fullpotential of improving the training process by addressingsuch issues is yet to be realized.Specifically, we firstlystudy and analyze two issues affecting training: incorrectassignment of negative pairs, and low caption quality anddiversity. Then, we devise effective solutions for address-ing both problems, which essentially require training withmultiple true positive pairs. Finally, we propose trainingwith sigmoid loss to address such a requirement. We showvery large gains over the current state-of-the-art for bothimage recognition ( +6% on average over 11 datasets)and image retrieval ( +19% on Flickr30k and +15%on MSCOCO).",
  ". Introduction": "Large-scale contrastive image-text pre-training has emergedas the prevalent method for vision-language representationlearning .The majority ofdatasets employed for pre-training are web-collected . They offer a varied data distribution andare sufficiently large to effectively train high-performingvision-language models. However, since the raw captionsfor each image are typically extracted from associated tagsor descriptions, they often exhibit low quality, being noisyand suboptimal for training purposes . Althoughsome attempts to fix such issues have been already de-scribed, to some extent, in literature (e.g.ALIP ,BLIP ), in this work, we show that the full potential ofimproving the quality of the training process is far from be-ing fully realized. Specifically, by studying and addressingspecific issues related to noise and low data quality, in thiswork, we show that our improved vision-language trainingpipeline can achieve massive gains over the current state-of-the-art methods for both image recognition ( +6% on",
  ". Our approach, FFF, achieves state-of-the-art accuracyacross multiple datasets, largely outperforming prior methods": "average over 11 datasets) and image retrieval ( +19% onFlickr30k and +15% on MSCOCO ).The first issue we study is related to noise impacting con-trastive learning: near-duplicate samples which are incor-rectly treated as negative pairs. Even within a batch, it is notuncommon to find images and/or captions that are seman-tically similar or even identical. Since standard contrastivelearning assumes one positive pair, this significantly hindersthe training process and the quality of the trained models.The second issue we study is related to low caption qual-ity and diversity. Captions can be short and lacking detail,noisy, or even entirely irrelevant to the image. Moreover,since the mapping process between image and text is one-to-many, more than one caption is needed to provide an ap-proximate description of the image.To fix issue one, we propose an algorithm that mines newpositive pairs based on image-text, image-image, and text-text similarities, aiming to decrease the number of false neg-atives in the training data arising due to semantically similarimages and/or captions.",
  "arXiv:2405.10286v1 [cs.CV] 16 May 2024": "We fix issue two by firstly generating pseudo-captionsfor each training image using a state-of-the-art image cap-tioning technique that will act as new true positives fora given image. Then, we propose batch text augmentationfor training with multiple pseudo-captions (i.e. five captionsper image selected via beam search) within the same batchto effectively increase caption diversity.Importantly, after applying the proposed solutions, weend up with a variable number of positive pairs per im-age i.e. newly mined positive pairs and multiple pseudo-captions per image. This implies that we need to train ourmodel with a loss function that accommodates multiple pos-itives and is robust to potential errors in the mining process.Unfortunately, neither contrastive loss nor supervisedcontrastive loss can be directly applied for this case.To this end, we propose to use the sigmoid loss whichallows the number of positives to vary dynamically per sam-ple and per batch at no extra cost and is also robust to noise.Overall, we make the following contributions: We study and provide in-depth analyses of two importantissues related to vision-language training process/data:false negative pairs due to semantic near-duplicates, andlow caption quality and diversity (Sec. 2). We provide two simple algorithms for addressing theaforementioned issues: The first one uses text-image,image-image, and text-text similarities for eliminating in-correctly assigned negatives and mining new true posi-tives. The second uses the proposed batch text augmenta-tion for training with multiple pseudo-captions per imagewithin the same batch. Both solutions induce multiplenew positives per each training image. To address this,we propose to use sigmoid loss for training the model.See Sec. 4. We show very large gains over the current state-of-the-artfor both image recognition ( +6% on average over 11datasets) and image retrieval ( +19% on Flickr30k and +15% on MSCOCO) (Sec. 5). We further ablate theimpact of many important components of our method inSec. 6.",
  ". Flaws of web-collected datasets & potentialsolutions": "Several observations drawn by analyzing the flaws of a web-collected dataset (CC3M dataset), motivating the proposedapproach, are provided below:Original captions are noisy and repetitive: For example,as illustrated in for the CC3M dataset, original (raw)captions contain a high number of generic captions that fre-quently reoccur across the dataset ( (c)), and are oftensemantically similar ( (a)). Moreover, many raw cap-tions may be unrelated to their associated images and theirvisual content, as indicated by low CLIP scores ( (b)). Re-captioning enhances quality and diversity: A poten-tial solution to this issue is the use of state-of-the-art imagecaptioning models (e.g. BLIP2 , OFA ) to generatesynthetic pseudo-captions, which can enhance the qualityand descriptiveness of the captions. When comparing rawand pseudo-captions, it is evident that the latter are more di-verse and semantically relevant to their associated images,as shown in . Multiple pseudo-captions should reduce noise: State-of-the-art image captioning models, despite being capable ofgenerating fluent and diverse captions, are often trainedand bootstrapped from the same web-collected data used intraining vision-language models. Consequently, as shownin , in some instances, the generated pseudo-captionscan be ambiguous and contain hallucinations, errors, andstylistic biases similar to those found in the raw captions.As a result, relying on a single pseudo-caption per imagecan still introduce a high degree of noise and can hinder thetraining of an effective vision-language model. A potential solution to this issue is the use of multiplepseudo-captions or multiple positives per image in the hopethat even if individual captions are incorrect, their ensem-ble is of higher quality and better reflects the content of theassociated image. To probe for the possible positive effectof using multiple synthetic captions, in (a), we showthe intra-cosine similarities of 5 pseudo-captions generatedusing beam search and, respectively, in (b), the aver-age image-text CLIP score between these synthetic captionsand their associated images - contrasted with the score cor-responding to a single caption. We observe that: 1) a simplemethod such as beam search can generate diverse syntheticcaptions, and, more crucially, 2) using multiple positives perimage results in an improved ensemble that better describesthe image and helps alleviate the problem of false positivesdue to incorrect individual instances. Mining of new positives: As shown in (c), even for arelatively small batch of 1k image-caption pairs, it is com-mon to find captions more similar to the image than theground-truth caption (i.e. higher ranks), and, as displayedin , such high-ranking captions often contain true pos-itives, which are captions that can be considered ground-truth descriptions for the associated image. A potential solution to this is the use of online mining ofnew positives based on image and text feature cosine simi-larities. However, as shown in , text-image pairs withhigh cosine similarity can still be false positives. To reducethem, we propose to mine the positives based on image-text,image-image, and text-text similarities, aiming to decreasethe number of false negatives in the training data arising dueto semantically similar images and/or captions. (a)(b)(c) (a)(b)(c) . Semantic and lexical diversity of raw and synthetic pseudo-captions of CC3M: (a): Average cosine similarities of eachcaption and its 100 most similar captions using CLIP ViT-L/14 features. (b): Cosine similarities between features of each image and itsground-truth caption. (c): The frequencies of the top-100 most frequent raw and synthetic pseudo-captions (generated using BLIP2). Weobserve that the raw captions are semantically similar to each other (a), often not well aligned with their associated ground-truth images(b), and contain a high number of basic and redundant captions (c). By swapping them with pseudo-captions, we observe an improveddiversity (a,c) and better image-text alignment (b). (b)(a)(c) . Quality assessment of synthetic captions of CC3M: (a) Average intra-cosine similarities between 5 synthetic captions ofeach image. (b) Cosine similarities between the features of each image and either the features of a single synthetic pseudo-caption orthe averaged features of 5 pseudo-captions. In (a) and (b), we observe that using multiple synthetic positives that are diverse (a), possibleerroneous captions can be corrected using an ensemble of pseudo-captions that better converge to the ground truth, resulting in text featuresmore aligned with their associated images (b). (c): The rankings of the ground-truth captions for each image in a batch of 1k image-captionpairs. This shows that, even with relatively small batches, many negatives are well aligned with some images, and it is very likely that manyof these negatives are potentially correct matches for a subset of images, i.e. false negatives. Features are computed using CLIP ViT-L/14. Raw Caption:Raw Caption: consumer product on the balcony fish - 40cm square acrylic painting of a cutter . Raw Caption:Raw Caption: Synthetic Caption:Synthetic Caption: Synthetic Caption:Synthetic Caption: walter hanks art fish seagull - 1939beachfront condo - condo - rental - cabo del caballo - - 3 king the cover of the song dreamy movie , and actor as the leading man . maurice nso, 1933, jug with gourds in the forest, hickory handle vase with red top mushrooms in a forest .Qualitative samples of synthetic captions fromCC3M: We show 4 examples featuring original raw and synthetic(BLIP2) pseudo-captions. These examples highlight typical limi-tations and challenges observed in synthetic captions which, whilesuperior to raw captions, can still be considered noisy.",
  ". Related work": "Contrastive pretraining under noisy web-collected data:Current publicly available vision-language datasets aremined from the internet automatically withonly basic automatic filtering applied, which results in im- perfect annotations and duplicate or near-duplicate pairs.A series of papers attempt to alleviate thenoise present in annotations by switching from hard to softlabels, akin to knowledge distillation (KD), using variouscombinations of contrastive loss (i.e. InfoNCE) and KL di-vergence. The work in constructs the soft labels usingan online entropic optimal transport algorithm implementedvia the Sinkhorn-Knopp algorithm. The probabilities foreach image add up to 1, with 0.5 on the diagonal and therest distributed. This assumes that, within the batch, thereare always some images that are somewhat similar. In ourcase, we use hard labels, with multiple positives, perform-ing reassignments only when the samples are sufficientlyclose, instead of forcing a distribution in all cases. Fur-thermore, we do not require running an optimal transportmethod, nor rely on a contrastive loss. The work of pro-gressively self-distills soft image-text alignments to moreefficiently learned robust representations from noisy data.At every iteration, a subset of labels are soft while therest are kept hard. Similarly, the work of relaxes the Raw: view this image of automobile make Synthetic: a black scion sti parked in the sun with cloudy sky Raw: racecar driver steers his car during video game subject . Synthetic: the red bull racing car is driving down a race track Ground-truths: Raw: looking east along a city from 1930s Synthetic: an old photo of a city street Raw: waiting on a cowboy - live music Synthetic: two people are singing and playing an acoustic guitar in an empty warehouse Raw: looking west that 's the old building on the far right . Synthetic: an old photo of the street in the city Ground-truths: Ground-truths: Raw: racecar driver of person steers his car during event Synthetic: a formula racer in motion during evening session Raw: seat achieves the record of cars Synthetic: an older blue van in dozens of other cars Captions with higher ranks than the ground-truths: Captions with higher ranks than the ground-truths: Raw: country artist and pop artist perform on stage during event Synthetic: two people in musical costumes perform a song in a stage Raw: young boy and girl singing a song in a choir Synthetic: two people singing with an instrument - stock vector Captions with higher ranks than the ground-truths: . Examples of high-ranking captions from CC3M: Weshow 3 examples of raw and synthetic captions ranked higher thanthe ground-truths from a batch of 1k image-caption pairs. In green,we show potential false negatives that can be used as new positivesfor improved training. However, possible false positives, as shownin red, can still occur. These can be handled by the robust sigmoidloss. Rankings are obtained using CLIP ViT-L/14. strict one-to-one constraint, transitioning to a soft cross-modal alignment by introducing a softened target, which isgenerated from the fine-grained intra-modal self-similarity.Additionally, they disentangle the negatives in the distribu-tion to further boost the relation alignment, resulting in acombination of InfoNCE loss performed with hard labelsand KL divergence. However, they do not perform batchedtext augmentations with multiple positives, as in our work,and still use a contrastive loss combined with KL, operat-ing on soft scores. The works of study the ef-fect of removing false negatives in the context of unimodali.e. pure vision models, not considering the case of multi-modal learning. The work of flags (a very small num-ber) of potential negatives using the aggregated score ob-tained from multiple support views per image, uses aclustering based approach while is based on rankedpositives, requiring a known class hierarchy (i.e. a fullysupervised case) or known changes/relations (i.e. videos).The works of derive from the Supervised ContrastiveLoss, while from InfoNCE. In contrast, our work oper-ates on image-text data, takes into account multi-modal in-teractions (I2T, T2T, T2I), does not use additional supportviews, known hierarchies etc. and is easily scalable.Following a different direction, BLIP and their fol-lowup version, use a bootstrapping approach in whichthe noisy captions are filtered out using the initial model,which is then retrained on the new data. This interplay isperformed offline and requires training a multitask model.The work of presents a small-scale study showingthat random sampling of pseudo-captions improves CLIP,concluding however that scaling up the number of image- caption pairs appears to be more effective. Finally, veryrecently, ALIP adds a synthetic pseudo-caption and aconsistency gating mechanism that weights the influence ofthe samples and image-text pairs on the contrastive loss.Different from the aforementioned methods, we proposeto fix incorrectly assigned negatives and mine for new truepositives using text-image, image-image, and text-text simi-larities. Moreover, to increase caption quality and diversity,we further propose training with multiple pseudo-captionsper image within the same batch. As our methods requiretraining with multiple positives per image, we further pro-pose to use the sigmoid loss for training the model.",
  ". Method": "This section describes the proposed method, whose aim is toimprove vision-language training by denoising and improv-ing the quality of the training process/data. Specifically,Sec. 4.1 addresses the problem of false negative pairs in-herent to the noisy nature of large-scale image-text datasetsby re-assigning them as true positives1. Sec. 4.2 proposestext batch augmentation for training the model with multi-ple positives pairs. The effect of Secs. 4.1 and 4.2 is that, foreach training image, a variable number of positive-negativepairs is formed (Sec. 4.3). Sec. 4.4 proposes a natural wayto train the model in this case by using the recently proposedsigmoid loss for vision-language pre-training.",
  ". Fixing incorrect negatives": "Let D be a dataset consisting of image-text pairs, withB a batch of randomly selected samples (xi, ti),i =1, 2, . . . , N.In addition to the ground truth positivespairs (xi, ti), we seek to identify and correct wrongly co-occurring negative pairs (xi, tj) on-the-fly. To achieve this,let us first define the image-text, image-image, and text-textcosine similarity matrices Sit = Xf T Tf , Sii = Xf XTfand Stt = Tf T Tf , where Sit, Sii, Stt RNN andXf RNd and Tf RNd represent the image and textfeatures, respectively.Given the similarity score matrices, we define the assign-ment matrix M {0, 1}NN as follows:",
  "M = (Sit > p1) (Sii > p2) [(Stt > p3) (Sit > p1)], (1)": "where is the logical or and the logical and operator,and p1, p1, p2 and p3 are the thresholds above which a sam-ple is marked as positive, with p1 < p1. Note that we filterthe positives found with text-text matching using image-textsimilarities (using threshold p1), as we observed a high por-tion of false positives within text-text matching, due to thefact that repeated samples often correlate with poor overall",
  "(c) Overall approach combining fixing incorrect negatives (Sec. 4.1) with batch text argumentation (Sec. 4.2). The synthetic pseudo-captions are generatedoffline and packed as part of the dataset": ". Overview of our approach: Fixing incorrect negatives is shown in (b). In (c) we describe our combined approach (includingbatch text augmentation) contrasted with the baseline of (a). Green squares denote positive pairs, while pink are negatives. Green squareswith a dashed border denote identified false negatives that are corrected to true positives. image description fidelity. The choice of p1, p1, p2 and p3is empirical and generally depends on the characteristics ofthe model. We ablate the dependency of the method on thethreshold values in Sec. 6 where we show little sensitivity.Note that M re-assigns a variable number of positives toeach image. b depicts the construction process of Mat a high level.In order to calculate the cosine similarity matricesSit, Sii and Stt required for the construction of M, weuse a pre-trained model. This is akin to a form of auto-labeling/auto-filtering, where the pretrained model providesa signal for re-assessing the labeling of the samples. Al-though one could opt to use an EMA teacher-student ap-proach, we found this simple approach to work sufficientlywell. Moreover, some possible errors in M can be handledby the robust sigmoid loss used for training (see Eq. 2).",
  ". Batch text augmentation with multiple positives": "The currently available image-text datasets arenoisy, with high variability in the quality of the text de-scriptions among samples. To improve data quality, we useBLIP2 , an off-the-shelf image captioner, to generatemultiple pseudo-captions for each image in the training set(see supplementary material for visual examples). Inspiredby , we propose to include all pseudo-captions as truepositives within the same batch, which we call batch text augmentation. Note that simultaneously training with mul-tiple pseudo-captions within the same batch has not beenconsidered in previous work. We show that this approachenables the training of highly accurate models (see Sec. 5and our ablation in Sec. 6). In the next section, we alsoshow how batch text augmentation can be integrated withthe mask construction process defined in Sec. 4.1. Finally,we note that while batch text augmentation improves theoverall performance, it does not address the presence of se-mantic near duplicates (i.e. false negatives) within the sametraining batch.",
  ". Combined approach": "Our approach for fixing incorrect negative pairs (Sec. 4.1)and batch text augmentation (Sec. 4.2) can be naturallycombined in order to define the total number of true pos-itives per image.To this end, and without loss of generality, we assume kcaptions per image (original caption plus pseudo-captions),hence the total number of captions and images are related byNtxt = kNimg. Given this, the image-text similarity matrixhas now (by construction) size Sit RNimgNtxt. Hence,the computation of Sii and Stt needs to be adjusted to re-flect this change. For the image-image case, and as Nimg <Ntxt, to make the image-image similarity matrix Sii havethe same dimensions as Sit, i.e. Sii RNimgNtxt, we replicate the scores k times. In other words, a given imagexi will share the score with each group of captions belong-ing to image xj, i, j Nimg. For the text-text case, thesimilarity matrix is now of size Stt RNtxtNtxt. Anal-ogously, to make the Stt have the same dimensions as Sit,we take the average score between each caption of image xiand all k captions of image xj.Overall, we end up with similarity matrices of the samedimensions Sit, Sii, Stt RNimgNtext and hence the as-signment matrix M can be again constructed by applyingEq. 1. The overall process is depicted in c.",
  ". Loss function": "The symmetrical contrastive loss (i.e. text image and im-age text) used in CLIP supports only one positivepair per sample (see a), being in discordance with therequirement of training with a variable number of positivepairs per image set by the proposed methods in Secs. 4.1and 4.2. A solution to this problem could be given by theSupervised Contrastive Loss , originally introduced toenable multi-view training of supervised image recognition.However, this loss is prone to noise , with the harder pos-itive pairs dominating the signal and hindering, in part, theeffect from the rest of the positive samples. This is espe-cially problematic in the context of web-collected datasets,which are notoriously noisy. Finally, it is memory intensiveand computationally demanding. In practice, we observe a1.9 slowdown for a batch size of 8,096 samples.A natural alternative is the BCE loss, shown to outper-form cross-entropy for image classification , and alsoshown to be a viable alternative for image-text representa-tion learning . Such formulation is particularly advanta-geous for the proposed approach, as the BCE loss nativelysupports an arbitrary number of positives per sample perbatch, with the ground truth being provided simply as a bi-nary mask. Moreover, the loss is more robust to noise ingeneral, and hence to false negatives and positives . Fi-nally, the initial negative bias prevents the model from beingforced to learn incorrect assignments early one. Hence, wepropose to use the following loss:",
  "+ exp( mij(sij/ + )),": "(2)where mij is the i, j element of M (1 for negative and 1for positive pairs), and respectively, sij the i, j element ofthe similarity matrix Sit.As the negative pairs considerably outnumber the pos-itive ones, to ensure that we start from a low initial loss(making the same observation as in ), we add a learn-able scalar , set initially to a negative value. However, asthe number of positive pairs is dynamic and is typically tiedto both the specifics of the dataset and the threshold used to define a positive sample, different from , we propose toestimate at the beginning of the training process. Specif-ically, given the randomly initialized model, we sample bbatches out of the training set, and then compute and storethe cosine similarities. Then, given the scores and the cor-responding labels, we search for such that the initial lossis minimized (everything else is kept frozen). The value of can be found either by gradient descent or alternatively,by performing a grid search.",
  ". Results": "Pretraining Datasets: To allow for fair comparisons withprior work, we pre-train our approach on YFCC15M-v2 , a subset of YFCC100M containing ap-proximately 15M image-text pairs.To cover differentdataset sizes, we also conduct experiments on CC3M and CC12M , and in the supplementary material, onOpen30M and Open70M datasets, further showcasing ourmethods scalability with respect to the dataset size.Implementation details: Architecturally, we use the samemodel topology and setting as in CLIP , specifically,using AdamW , learning rate of 1e 3 and weight de-cay of 0.1, except for CC3M where we set the weight de-cay to 0.5, as in prior work . In terms of augmenta-tions, we follow , randomly resizing and cropping theimage to 224 224px, applying random flipping, randomGaussian blur (between 0.1 and 2.0) and color jittering (0.4,0.4, 0.4, 0.1). For text, the data is truncated to 77 tokens.Note, that the branch used to construct the assignment ma-trix M uses no augmentations (i.e. resize to 256 256px,followed by center crop, resulting in a 224 224px im-age). The thresholds were set to p1 = 0.27, p2 = 0.92,p3 = 0.99, p1 = 0.24. Unless otherwise specified, themodels are trained for 32 epochs with a batch size of 8,096on 8 NVIDIA A100 GPUs. All of our models and trainingcode are implemented using PyTorch .",
  ". Comparison with state-of-the-art": "Following recent work on vision-language pretraining , we compare our method with state-of-the-art ap-proaches for zero-shot classification and zero-shot retrieval.See supplementary material for linear probe evaluation.Zero-shot classification: For zero-shot classification eval-uation, for the main setting, we select the common sub-set of datasets that facilitate a direct comparison with priorstate-of-the-art. In particular, we evaluate our approach onthe following datasets: CIFAR-10 , CIFAR-100 ,Food101 , Pets , Flowers , SUN397 ,Stanford Cars , DTD , Caltech101 , FGVC-Aircraft and ImageNet .The evaluation is per-formed using the same prompt templates and class namesas in prior work .",
  ". Zero-shot image-text retrieval on the test splits of Flickr30k and MSCOCO. All models were pre-trained on YFCC15M": "As the results from Tab. 1 show, our approach outper-forms all prior methods, improving by 6.2% in absoluteterms on top of the previous best result of HiDeCLIP (which benefits from a better architecture) when aggregatedacross 11 datasets. Notably, we set a new state-of-the-artresult on ImageNet, too (51.1%). Finally, we significantlyimprove upon ALIP , which also makes use of syntheticcaptions, outperforming it by 9.1%.",
  "For completeness, we also adhere to the protocol of pre-training a ResNet-50 on CC3M, and respectively, CC12M": "and then evaluating it for zero-shot classification on Ima-geNet. As the results from Tab. 3 show, the same conclu-sions hold. Our method outperforms the previous best resultby 3.1% on CC3M (30.3% vs 33.4%) and 3.0% on CC12M(44.4% vs 47.4%). See supplementary material for resultson Open30M and Open70M.Zero-shot retrieval:Consistent with prior work, weevaluate our approach for zero-shot retrieval on Flickr-30k and MS-COCO reporting results in termsof R@{1,5,10} for both text and image retrieval. The re-sults are summarized in Tab. 2.As it can be observed,our approach offers significant gains across all metrics anddatasets used, improving on top of the prior state-of-the-art ALIP by 14.8% and 18.7% in terms of R@1 onFlickr30k for text, and respectively, image retrieval. Simi-larly, we outperform the previous best result by 14.9% and15.0% in terms of R@1 on MSCOCO for text and imageretrieval. This highlights that our approach results in repre-sentations that can capture subtle and fine-grained details.",
  "For our ablation studies, the results reported are producedusing a ViT-B/16 model pretrained on CC3M dataset": "Effect of fixing incorrect negatives: herein, we analyzethe effectiveness of the proposed algorithm of Sec. 4.1. Byanalyzing the result from Tab. 4, we can observe consis-tent gains for all 3 cases of interest: a) when using theweb-collected captions (+2.7% gain), b) when using onepseudo-caption (+3.5% improvement) and c) when all avail-able pseudo-captions at once (+1.8%). Overall, comparedto the baseline accuracy of 18.6%, our approach improvesby +14.3% (top-1 accuracy of 32.9%). The results show thatour approach provides gains across all options considered.",
  ". Effect of fixing incorrect negatives: Zero-shot evalua-tion on ImageNet in terms of Top-1 (%) accuracy": "Effect of different components in Eq. 1: In Eq. 1, theconstructed assignment matrix M is computed from threefeature similarity matrices Sit, Sii and Stt. Herein, we eval-uate the impact of each of these components. As the resultsfrom Tab. 5 show, viewed independently, the Sit is the mostimpactful, as it has a dual effect, both in terms of filter-ing incorrect pairs and of adjusting for semantically similarsamples. Moreover, the results hold for both ground truthcaptions and pseudo-captions.",
  ". Effect of different components in Eq. 1: Zero-shotevaluation on ImageNet in terms of Top-1 (%) accuracy": "Effect of batch text augmentation: Herein, we assess theimpact of training with multiple pseudo-captions within thesame batch, as described in Sec. 4.2. Tab. 6 shows accuracyvs number of pseudo-captions used during training. As wecan observe, increasing the number of captions increasesthe accuracy of the model, inline with the expectations. As an additional baseline, we compare against a modeltrained by randomly sampling 1 out of 5 captions (as op-posed to using them jointly as proposed in our work) onCC3M and YFCC-15M. On CC3M the performance dropsby 1.5%, from 32.9% to 31.4%, while on YFCC-v2 from51.1% to 44.1%. This further highlights the importance ofthe proposed batch text augmentation.Effect of image captioner: We also compare the effectof using two different state-of-the-art image captioners,OFA and BLIP-2 . As the results from Tab. 7 show,both captioners lead to identical performance.",
  ". Effect of the image captioner: Zero-shot evaluation onImageNet in terms of Top-1 (%) accuracy": "Comparison with the supervised contrastive loss: To fur-ther validate the loss choice, we compare against a modeltrained with the supervised contrastive loss . For a faircomparison, both models were trained using the same set-tings on CC3M. When evaluated for zero-shot classificationon ImageNet, the supervised contrastive model achievedonly 19.0% accuracy vs 21.3% achieved by our model.Note, that similar results are obtained using a InfoNCEbased loss. This result empirically solidifies the argumentsmade in Sec. 4.4.",
  ". Conclusions": "In this work, we propose a new approach to vision-languagepretraining based on multi-positive sample pairing that fixesincorrect negatives and addresses low caption quality. Thelatter is tackled by a newly introduced batch text augmen-tation strategy, in which multiple new positive pairs areconcomitantly added via synthetic recaptioning. Departingfrom the typical contrastive loss, to enable efficient trainingunder an arbitrary number of positives per sample, we pro-pose to train the model with a sigmoid loss. In the process,we highlight the crucial role of noise and caption qualityin vision-language pre-training, offering an in-depth analy-sis. All in all, we show large improvements over the cur-rent state-of-the-art method for both zero-shot image recog-nition ( +6% on average of 11 datasets) and retrieval( +19% on Flickr30k and +15% on MSCOCO). Alex Andonian, Shixing Chen, and Raffay Hamid. Robustcross-modal representation learning with progressive self-distillation.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1643016441, 2022. 3",
  "Tsai-Shien Chen, Wei-Chih Hung, Hung-Yu Tseng, Shao-Yi Chien, and Ming-Hsuan Yang.Incremental false neg-ative detection for contrastive learning.arXiv preprintarXiv:2106.03719, 2021. 4": "Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, SammyMohamed, and Andrea Vedaldi. Describing textures in thewild. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 36063613, 2014. 6 Yufeng Cui, Lichen Zhao, Feng Liang, Yangguang Li, andJing Shao. Democratizing contrastive language-image pre-training: A clip benchmark of data, model, and supervision.arXiv preprint arXiv:2203.05796, 2022. 7 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE conference on computer vision andpattern recognition, pages 248255. Ieee, 2009. 6",
  "Shijie Geng, Jianbo Yuan, Yu Tian, Yuxiao Chen, andYongfeng Zhang. Hiclip: Contrastive language-image pre-training with hierarchy-aware attention.arXiv preprintarXiv:2303.02995, 2023. 6, 7, 11, 12": "Shashank Goel, Hritik Bansal, Sumit Bhatia, Ryan Rossi,Vishwa Vinay, and Aditya Grover. Cyclip: Cyclic contrastivelanguage-image pretraining. Advances in Neural Informa-tion Processing Systems, 35:67046719, 2022. 7 Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, TorstenHoefler, and Daniel Soudry. Augment your batch: Improvinggeneralization through instance repetition. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 81298138, 2020. 5 David T Hoffmann,Nadine Behrmann,Juergen Gall,Thomas Brox, and Mehdi Noroozi. Ranking info noise con-trastive estimation: Boosting contrastive learning via rankedpositives. In Proceedings of the AAAI Conference on Artifi-cial Intelligence, pages 897905, 2022. 4 Tri Huynh, Simon Kornblith, Matthew R Walter, MichaelMaire, and Maryam Khademi.Boosting contrastive self-supervised learning with false negative cancellation. In Pro-ceedings of the IEEE/CVF winter conference on applicationsof computer vision, pages 27852795, 2022. 4 Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and TomDuerig. Scaling up visual and vision-language representa-tion learning with noisy text supervision. In Internationalconference on machine learning, pages 49044916. PMLR,2021. 1 Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, andDilip Krishnan. Supervised contrastive learning. Advancesin neural information processing systems, 33:1866118673,2020. 2, 6, 8 Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.3d object representations for fine-grained categorization. InProceedings of the IEEE international conference on com-puter vision workshops, pages 554561, 2013. 6",
  "training. Advances in Neural Information Processing Sys-tems, 35:10081019, 2022. 7, 11, 12": "Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.Align before fuse: Vision and language representation learn-ing with momentum distillation. Advances in neural infor-mation processing systems, 34:96949705, 2021. 6 Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.Blip: Bootstrapping language-image pre-training for uni-fied vision-language understanding and generation. In In-ternational Conference on Machine Learning, pages 1288812900. PMLR, 2022. 1, 4 Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.Blip-2:Bootstrapping language-image pre-training withfrozen image encoders and large language models.arXivpreprint arXiv:2301.12597, 2023. 2, 4, 5, 8 Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, WanliOuyang, Jing Shao, Fengwei Yu, and Junjie Yan.Su-pervision exists everywhere: A data efficient contrastivelanguage-image pre-training paradigm.arXiv preprintarXiv:2110.05208, 2021. 1, 6, 7, 12 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer, 2014. 1, 7",
  "Norman Mu, Alexander Kirillov, David Wagner, and Sain-ing Xie. Slip: Self-supervision meets language-image pre-training. In European Conference on Computer Vision, pages529544. Springer, 2022. 6, 7": "Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Se-woong Oh, and Ludwig Schmidt.Quality not quantity:On the interaction between dataset design and robustness ofclip. Advances in Neural Information Processing Systems,35:2145521469, 2022. 1 Maria-Elena Nilsback and Andrew Zisserman. Automatedflower classification over a large number of classes. In 2008Sixth Indian conference on computer vision, graphics & im-age processing, pages 722729. IEEE, 2008. 6",
  "transferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 1, 2, 5, 6, 7, 11, 12": "Shibani Santurkar, Yann Dubois, Rohan Taori, Percy Liang,and Tatsunori Hashimoto. Is a caption worth a thousand im-ages? a study on representation learning. In The Eleventh In-ternational Conference on Learning Representations, 2022.4 Christoph Schuhmann, Richard Vencu, Romain Beaumont,Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, TheoCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:Open dataset of clip-filtered 400 million image-text pairs.arXiv preprint arXiv:2111.02114, 2021. 1, 3 Christoph Schuhmann, Romain Beaumont, Richard Vencu,Cade Gordon,Ross Wightman,Mehdi Cherti,TheoCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-man, et al. Laion-5b: An open large-scale dataset for trainingnext generation image-text models. Advances in Neural In-formation Processing Systems, 35:2527825294, 2022. 3,5 Piyush Sharma, Nan Ding, Sebastian Goodman, and RaduSoricut. Conceptual captions: A cleaned, hypernymed, im-age alt-text dataset for automatic image captioning. In Pro-ceedings of the 56th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pages25562565, 2018. 5, 6 Krishna Srinivasan, Karthik Raman, Jiecao Chen, MichaelBendersky, and Marc Najork. Wit: Wikipedia-based imagetext dataset for multimodal multilingual machine learning.In Proceedings of the 44th International ACM SIGIR Confer-ence on Research and Development in Information Retrieval,pages 24432449, 2021. 1 Ajinkya Tejankar, Maziar Sanjabi, Bichen Wu, Saining Xie,Madian Khabsa, Hamed Pirsiavash, and Hamed Firooz. Afistful of words: Learning transferable visual models frombag-of-words supervision. arXiv preprint arXiv:2112.13884,2021. 7 Bart Thomee, David A Shamma, Gerald Friedland, Ben-jamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, andLi-Jia Li. Yfcc100m: The new data in multimedia research.Communications of the ACM, 59(2):6473, 2016. 1, 3, 6 Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, andHongxia Yang.Ofa: Unifying architectures, tasks, andmodalities through a simple sequence-to-sequence learningframework. In International Conference on Machine Learn-ing, pages 2331823340. PMLR, 2022. 2, 8",
  "ciety conference on computer vision and pattern recognition,pages 34853492. IEEE, 2010. 6": "Kaicheng Yang, Jiankang Deng, Xiang An, Jiawei Li, Ziy-ong Feng, Jia Guo, Jing Yang, and Tongliang Liu.Alip:Adaptive language-image pre-training with synthetic cap-tion. In Proceedings of the IEEE/CVF International Con-ference on Computer Vision, pages 29222931, 2023. 1, 4,6, 7, 11, 12 Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, MinzheNiu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, andChunjing Xu. Filip: Fine-grained interactive language-imagepre-training. arXiv preprint arXiv:2111.07783, 2021. 1, 7 Haoxuan You, Luowei Zhou, Bin Xiao, Noel Codella, YuCheng, Ruochen Xu, Shih-Fu Chang, and Lu Yuan. Learn-ing visual representation from modality-shared contrastivelanguage-image pre-training.In European Conference onComputer Vision, pages 6987. Springer, 2022. 1 Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-maier. From image descriptions to visual denotations: Newsimilarity metrics for semantic inference over event descrip-tions.Transactions of the Association for ComputationalLinguistics, 2:6778, 2014. 1, 7",
  "A.1. Zero-shot recognition on Open30M andOpen70M datasets": "To further showcase the scalability of our approach, we fol-low , pretraining our method on a combination of 4publicly available datasets, dubbed Open30M (see Tab. 13for composition). The pretraining hyperparameters remainthe same as for YFCC. Once trained, we evaluate it in azero-shot manner on the same suite of 11 datasets. As theresults from Tab. 8 show, our approach outperforms all priormethods, improving upon the prior best result of by+4.7% aggregated over 11 datasets, including by +3.1% onImageNet.Finally, we extend the Open30M images dataset byadding RedCaps , OpenImages-8M and YFCC-v1, creating Open70M. As the results from Tabs. 8 and 10show, our approach scales well, with consistent gains forboth zero-shot retrieval and classification.",
  "A.2. Linear probe": "In addition to zero-shot evaluation, we also present linearprobe results in Tab. 9 for models pre-trained on YFCC15Mand in Tab. 11 for models pre-trained on Open30M. Similarto zero-shot experiments, we use the clip-benchmarkrepository2 to run these experiments. For each dataset, we cache the features of the training and test sets, and thenuse the training sets features and its ground-truth labelsto train a linear layer on top. The linear linear is trainedfor 20 epochs using the standard cross-entropy loss andAdamW optimizer with a learning rate of 0.1, no weightdecay, and a cosine learning rate scheduler. The trained lin-ear layer is then used over the cached test features to ob-tain the accuracy. Similar to zero-shot experiments, our ap-proach outperforms previous methods by large margins, i.e.,+7.0% with YFCC15M pertaining (Tab. 9) and +6.2% withOpen30M pertaining over 11 image classification datasets.",
  "B. Additional ablation studies": "Sensitivity to the threshold value: The selection of thresh-old values is intuitive, and the model is generally forgivingwithin a certain plateau of values. For Stt and Sii, they aresimply set to high values to target nearly identical samples.For Sit, we start from the mean score of the positive pairs,which is 0.29, and explore a few adjacent values, notingthat all values located in the same vicinity perform well asshown in Tab. 12.",
  "D. Zero-shot retrieval evaluation considera-tions": "As the synthetic captions are generated by models pre-trained on external data, a reasonable question to ask iswherever there is potential data leakage. For the Flick30kdataset, no such issues are present, as BLIP2 did not useany data from the training set of Flickr30k during any ofits training phases. For MSCOCO, we note that only 100kout of 120M samples used for training BLIP2 were imagesfrom the COCO training set, hence the impact is likely min-imal, if any. We note here that the current state-of-the-artmethod, ALIP, is subject to the same potential issue, as theyalso make use of synthetic captions produced by a modelthat was pre-trained on MSCOCO data (i.e. OFA).",
  ".Number of examples per each training dataset.Open30M is the combination of all four datasets, i.e., SBU,CC3M, CC12M and YFCC15M-V2": "CIFAR 10 & CIFAR 100a photo of a {label}.a blurry photo of a {label}.a black and white photo of a {label}.a low contrast photo of a {label}.a high contrast photo of a {label}.a bad photo of a {label}.a good photo of a {label}.a photo of a small {label}.a photo of a big {label}.a photo of the {label}.a blurry photo of the {label}.a black and white photo of the {label}.a low contrast photo of the {label}.a high contrast photo of the {label}.a bad photo of the {label}.a good photo of the {label}.a photo of the small {label}.a photo of the big {label}.",
  "Food101a photo of {label}, a type of food": "Caltech101a photo of a {label}.a painting of a {label}.a plastic {label}.a sculpture of a {label}.a sketch of a {label}.a tattoo of a {label}.a toy {label}.a rendition of a {label}.a embroidered {label}.a cartoon {label}.a {label} in a video game.a plushie {label}.a origami {label}.art of a {label}.graffiti of a {label}.a drawing of a {label}.a doodle of a {label}.a photo of the {label}.a painting of the {label}.the plastic {label}.a sculpture of the {label}.a sketch of the {label}.a tattoo of the {label}.the toy {label}.a rendition of the {label}.the embroidered {label}.the cartoon {label}.the {label} in a video game.the plushie {label}.the origami {label}.art of the {label}.graffiti of the {label}.a drawing of the {label}.a doodle of the {label}. Stanford Carsa photo of a {label}.a photo of the {label}.a photo of my {label}.i love my {label}!a photo of my dirty {label}.a photo of my clean {label}.a photo of my new {label}.a photo of my old {label}. DTDa photo of a {label} texture.a photo of a {label} pattern.a photo of a {label} thing.a photo of a {label} object.a photo of the {label} texture.a photo of the {label} pattern.a photo of the {label} thing.a photo of the {label} object.",
  "SUN39a photo of a {label}.a photo of the {label}": "ImageNeta bad photo of a {label}.a photo of many {label}.a sculpture of a {label}.a photo of the hard to see {label}.a low resolution photo of the {label}.a rendering of a {label}.graffiti of a {label}.a bad photo of the {label}.a cropped photo of the {label}.a tattoo of a {label}.the embroidered {label}.a photo of a hard to see {label}.a bright photo of a {label}.a photo of a clean {label}.a photo of a dirty {label}.a dark photo of the {label}.a drawing of a {label}.a photo of my {label}.the plastic {label}.a photo of the cool {label}.a close-up photo of a {label}.a black and white photo of the {label}.a painting of the {label}.a painting of a {label}.a pixelated photo of the {label}.a sculpture of the {label}.a bright photo of the {label}.a cropped photo of a {label}.a plastic {label}.a photo of the dirty {label}.a jpeg corrupted photo of a {label}.a blurry photo of the {label}.a photo of the {label}.a good photo of the {label}.a rendering of the {label}.a {label} in a video game.a photo of one {label}.a doodle of a {label}.a close-up photo of the {label}.a photo of a {label}.the origami {label}.the {label} in a video game.a sketch of a {label}.a doodle of the {label}.a origami {label}.a low resolution photo of a {label}.the toy {label}.a rendition of the {label}.a photo of the clean {label}.a photo of a large {label}.a rendition of a {label}.a photo of a nice {label}.a photo of a weird {label}.a blurry photo of a {label}.a cartoon {label}.art of a {label}.a sketch of the {label}.a embroidered {label}.a pixelated photo of a {label}.itap of the {label}.a jpeg corrupted photo of the {label}.a good photo of a {label}.a plushie {label}.a photo of the nice {label}.a photo of the small {label}.a photo of the weird {label}.the cartoon {label}.art of the {label}.a drawing of the {label}.a photo of the large {label}.a black and white photo of a {label}.the plushie {label}.a dark photo of a {label}.itap of a {label}.graffiti of the {label}.a toy {label}.itap of my {label}.a photo of a cool {label}.a photo of a small {label}.a tattoo of the {label}."
}