{
  "(a)(b)(c)(d)": ". Overview of MARS. (a) Within a geographical area, we operate four autonomous vehicles, displaying their GPS trajectories froma single day using different colors. (b) Vehicles occasionally come close together (visualized via distinct-colored point clouds), supportingresearch in multiagent systems. (c) We collect sensory data from repeated traversals of the same location under varying conditions, forlearning and perception research with retrospective memory. (d) The dataset includes surround-view RGB images and LiDAR point cloudsfor cross-modal perception and learning. Note that our data is obtained from May Mobility",
  "Abstract": "Large-scale datasets have fueled recent advancementsin AI-based autonomous vehicle research. However, thesedatasets are usually collected from a single vehicles one-time pass of a certain location, lacking multiagent inter-actions or repeated traversals of the same place.Suchinformation could lead to transformative enhancements inautonomous vehicles perception, prediction, and planningcapabilities. To bridge this gap, in collaboration with theself-driving company May Mobility, we present the MARSdataset which unifies scenarios that enable MultiAgent,multitraveRSal, and multimodal autonomous vehicle re-search. More specifically, MARS is collected with a fleet ofautonomous vehicles driving within a certain geographicalarea. Each vehicle has its own route and different vehiclesmay appear at nearby locations. Each vehicle is equippedwith a LiDAR and surround-view RGB cameras. We curatetwo subsets in MARS: one facilitates collaborative driv-ing with multiple vehicles simultaneously present at thesame location, and the other enables memory retrospectionthrough asynchronous traversals of the same location by multiple vehicles. We conduct experiments in place recog-nition and neural reconstruction. More importantly, MARSintroduces new research opportunities and challenges suchas multitraversal 3D reconstruction, multiagent perception,and unsupervised object discovery. Our data and codes canbe found at",
  ". Introduction": "Autonomous driving, which has the potential to fundamen-tally enhance road safety and traffic efficiency, has wit-nessed significant advancements through AI technologiesin recent years. Large-scale, high-quality, real-world datais crucial for AI-powered autonomous vehicles (AVs) toenhance their perception and planning capabilities :AVs can not only learn to detect objects from annotateddatasets but also create safety-critical scenarios by gen-erating digital twins based on past driving recordings .The pioneering KITTI dataset established the initialbenchmark for tasks such as detection and tracking. Sinceits introduction, a number of datasets have been proposedto promote the development of self-driving; see Tab. 1.",
  "Open MARS Dataset (Ours)C&LSurroundU.S.Industry2024": "Two representative datasets are nuScenes and WaymoDataset which introduce multimodal data collected fromcameras and range sensors, covering a 360-degree field ofview for panoramic scene understanding. These datasetshave shifted the focus from KITTIs monocular cameras,receiving wide attention in the fields of vision and robotics.Existing driving datasets generally focus on geograph-ical and traffic diversity without considering two practicaldimensions: multiagent (collaborative) and multitraversal(retrospective). The collaborative dimension highlights thesynergy between multiple vehicles located in the same spa-tial region, facilitating their cooperative perception, predic-tion, and planning. The retrospective dimension enables ve-hicles to enhance their 3D scene understanding by draw-ing upon visual memories from prior visits to the sameplace. Embracing these dimensions can address challengeslike limited sensing capability for online perception andsparse views for offline reconstruction. Nevertheless, exist-ing datasets are typically collected by an individual vehicleduring a one-time traversal of a specific geographical loca-tion. To advance autonomous vehicle research, especiallyin the collaborative and retrospective dimensions, the re-search community needs a more comprehensive dataset inreal-world driving scenarios. To fill the gap, we introducethe Open MARS Dataset, which provides MultiAgent, mul-titraveRSal, and multimodal recordings, as shown in .All the recordings are obtained from May Mobility1s au-tonomous vehicles operating in Ann Arbor, Michigan. Multiagent. We deploy a fleet of autonomous vehiclesto navigate a designated geographical area. These vehi-cles can be in the same locations at the same time, al-lowing for collaborative 3D perception through vehicle-to-vehicle communication. Multitraversal. We capture multiple traversals within thesame spatial area under varying lighting, weather, andtraffic conditions. Each traversal may follow a uniqueroute, covering different driving directions or lanes, re-",
  "sulting in multiple trajectories that provide diverse visualobservations of the 3D scene": "Multimodal.We equip the autonomous vehicle withRGB cameras and LiDAR, both with a full 360-degreefield of view. This comprehensive sensor suite can enablemultimodal and panoramic scene understanding.We conduct quantitative and qualitative experiments inplace recognition and neural reconstruction. More impor-tantly, MARS introduces novel research challenges and op-portunities for the vision and robotics community, includingbut not limited to multiagent collaborative perception andlearning, unsupervised perception under repeated traver-sals, continual learning, neural reconstruction and novelview synthesis with multiple agents or multiple traversals.",
  ". Related Works": "Autonomous driving datasets. High-quality datasets arecrucial for advancing AI-powered autonomous driving re-search . The seminal KITTI dataset significantlyattracted research attention in robotic perception and map-ping . Since then, a large number of datasets havebeen proposed, pushing the boundaries of the field by tack-ling challenges in multimodal fusion, multitasking learning,adverse weather, and dense traffic . In re-cent years, researchers have proposed multiagent collabo-ration to get rid of the limitations in single-agent percep-tion, e.g., frequent occlusion and long-range sparsity .Previous efforts in curating multiagent datasets areusually limited by simulated environments . Therecent V2V4Real supports vehicle-to-vehicle cooper-ative object detection and tracking in the real world, yetthe two-camera setup is insufficient for surround-view per-ception. Another relevant dataset, Ithaca365 , providesrecordings from repeated traversals of the same route indifferent lighting and weather conditions, yet it only usesfront-view cameras for data collection. Several works col-lect multitraversal data for map change such as Argoverse 2dataset , and some recent works build 3D reconstruc-",
  "GPS10Hz, longitude, latitude, elevation": "tion methods or simulators based on Argoverse 2 .There are also several works focusing on long-term visuallocalization , such as Oxford RobotCar Dataset and CMU Seasons dataset . Yet these datasets do notconsider scenarios of multiagent driving. To fill the gap,our MARS dataset provides multiagent, multitraversal, andmultimodal driving recordings with a panoramic cameraview; see Tab. 1. Notably, the continuous and dynamic op-eration of May Mobilitys fleet of vehicles makes our MARSdataset stand out in scale and diversity, featuring hundredsof traversals at a single location and enabling collaborativedriving for up to four vehicles, thereby setting a record forboth traversal and agent numbers.Visual place recognition. In the field of computer visionand robotics, visual place recognition (VPR) holds signifi-cant importance, enabling the recognition of specific placesbased on visual inputs .Specifically, VPR systemsfunction by comparing a given query data, usually an im-age, to an existing reference database and retrieving themost similar instances to the query. This functionality is es-sential for vision-based robots operating in GPS-unreliableenvironments. VPR techniques generally fall into two cat-egories: traditional methods and learning-based methods.Traditional methods leverage handcrafted features to generate global descriptors . However, in practice,appearance variation and limited viewpoints can degradeVPR performance.To address the challenge of appear-ance variation, learning-based methods utilize deep fea-ture representations . In addition to image-basedVPR, video-based VPR approaches are proposedto achieve better robustness, mitigating the limited view-points with video clips. Moreover, CoVPR introducescollaborative representation learning for VPR, bridging thegap between multiagent collaboration and place recogni-tion, and addressing limited viewpoints by leveraging infor-mation from collaborators. Beyond 2D image inputs, Point-NetVLAD explores point-cloud-based VPR, offering aunique perspective on place recognition. In this paper, weevaluate both single-agent VPR and collaborative VPR.NeRF for autonomous driving.Neural radiance fields(NeRF) in unbounded driving scenes has recently re-",
  ". Sensor setup of the vehicle platform for data collection": "ceived a lot of attention, as it not only facilitates the de-velopment of high-fidelity neural simulators but alsoenables high-resolution neural reconstruction of the envi-ronment . Regarding novel view synthesis (NVS), re-searchers have addressed the challenges such as scalableneural representations with local blocks , dynamicurban scene parsing with compositional fields , andpanoptic scene understanding with object-aware fields . Regarding neural reconstruction, researchers have re-alized decent surface reconstruction based on LiDAR pointcloud and image input . Meanwhile, several ef-forts have been made in multi-view implicit surface recon-struction without relying on LiDAR . Existing methodsbased on NeRF are constrained by limited visual observa-tions, often relying on sparse camera views collected alonga narrow trajectory. There is significant untapped potentialin leveraging additional camera perspectives, whether frommultiple agents or repeated traversals, to enrich the visualinput and enhance the NVS or reconstruction performance.",
  ". Vehicle Setup": "Sensor setup.May Mobilitys fleet of vehicles includesfour Toyota Sienna, each mounted with one LiDAR, threenarrow-angle RGB cameras, three wide-angle RGB fisheyecameras, one IMU, and one GPS. The sensors have variousraw output frequencies, but all sensor data are eventuallysampled to 10Hz for synchronization. Camera images aredown-sampled to save storage. Detailed specifications ofthese sensors are listed in Tab. 2. In general, the LiDAR islocated at the front top of the vehicle. The three narrow-angle cameras are located at the front, front left, and front",
  ". Multitraversal subset statistics": "right of the vehicle. Three fisheye cameras are on the backcenter, left side, and right side of the vehicle; see . TheIMU and GPS are located at the center top of the vehicle.The explicit extrinsic of these sensors are expressed as ro-tations and translations that transform sensor data from itsown sensor frame to the vehicles ego frame. For each cam-era on each vehicle, we provide camera intrinsic parametersand distortion coefficients. The distortion parameters wereinferred by the AprilCal calibration method .Coordinate system. There are four coordinate systems:sensor frame, ego frame, local frame, and global frame.Sensor frame represents the coordinate system whose ori-gin is defined at the center of an individual sensor. The egoframe represents the coordinate system whose origin is de-fined at the center of the rear axle of an ego vehicle. Thelocal frame represents the coordinate system whose originis defined at the start point of an ego vehicles trajectory ofthe day. The global frame is the world coordinate system.",
  ". Data Collection": "May Mobility is currently focusing on micro-service trans-portation, running shuttle vehicles on fixed routes in variousorders and directions. The full route is over 20 kilometerslong, encompassing residential, commercial, and universitycampus areas with diverse surroundings in terms of traffic,vegetation, buildings, and road marks. The fleet operatesevery day between 2 to 8 p.m., therefore covering variouslighting and weather conditions. Altogether, May Mobil-itys unique mode of operation enabled us to collect multi-traversal and multiagent self-driving data.Multitraversal data collection. We defined a total of 67locations on the driving route, each spanning a circular areaof a 50-meter radius. These locations cover different driv-ing scenarios such as intersections, narrow streets, and long-",
  ". Ratio of day and night scenes": "straight roads with various traffic conditions. The traversalsat each location take place from different directions at dif-ferent times of each day, promising physically and chrono-logically comprehensive perceptions of the area. We de-termine via the vehicles GPS location whether it is travel-ing through a target location, and data is collected for thefull duration of the vehicles presence within the 50-meter-radius area. Traversals are filtered such that each traversalis between 5 seconds to 100 seconds long.Multiagent data collection. A highlight of our dataset isthat we provide real-world synchronized multi-agent col-laborative perception data that delivers extremely detailedspatial coverage. Determining from vehicles GPS coordi-nates, we extract 30-second-long scenes where two or moreego vehicles have been less than 50 meters away from eachother for more than 9 seconds, collectively providing over-lapping perceptions of the same area at the same time butfrom different angles. For scenes where the encounteringpersisted less than a full 30 seconds, the encountering seg-ment is placed at the center of the 30-second duration, withequal amount of non-encountering time filled before andafter it (e.g. 20 seconds of encountering gets extended toa 30-second scene by adding 5 seconds before and 5 sec-onds after). Such encountering can take place anywherearound the map, constituting scenarios such as tailgatingalong a straight road and meeting at intersections, as shownin . Our method also ensures that at least one vehiclein the scene travels over 10 meters within 30 seconds.",
  ". Multiagent scene visualizations with three front cameras and LiDAR point clouds in birds eye view (BEV). Typicalscenarios include straight road tailgating as well as meeting at intersections": "360-degree LiDAR point clouds. Among the 67 locations,48 have over 20 traversals, 23 over 100 traversals, and 6over 200 traversals. Each traversal has 250 frames (25 sec-onds) on average, with the majority of traversals containing100 to 400 frames (10 to 40 seconds). The specific dis-tributions of traversals and frames across all locations areshown in and . The muitlagent subset coversdata from 20 different days between October 23rd, 2023and March 8th, 2024. We collected 53 scenes of 30-secondduration, stably involving 297 to 300 frames in each scene,accounting for over 15,000 frames of images and LiDARpoint clouds in total. Among the 53 scenes, 52 involve twovehicles, and 1 involves three vehicles. The distance be-tween each pair of ego vehicles is analyzed for every frame.The distribution demonstrates that encountering takes placemostly with two vehicles being less than 50 meters awayfrom each other, as shown in .",
  "Problem definition. We consider a set of queries Q withM images and a reference database D with N images. Inthis task, the objective is to find Ir D given Iq Q such": "that Iq and Ir are captured at the same location.Evaluation metric. We adopt recall at K as our evalua-tion metric for VPR. For a query image Iq, we select K ref-erence images with Top-K cosine similarities between Xqand {Xr}Nr=1. If at least one of the selected images is cap-tured within S meters of Iq (S = 20 in this paper), then wecount it as correct. The recall at K is computed as the ratiobetween the total number of correct counts and M.Benchmark models.We adopt NetVLAD , Point-NetVLAD , MixVPR , GeM , Plain ViT ,and CoVPR as benchmark models. NetVLAD consists of a CNN-based backbone and aNetVLAD pooling layer. NetVLAD replaces the hard as-signment in VLAD with a learnable soft assignment,taking features extracted by backbones as input and gen-erating a global descriptor. MixVPR consists of a CNN-based backbone and afeature-mixer. The output of the backbone is flattened toC HW , fed to the feature-mixer with row-wise andcolumn-wise MLPs, flattened to a single vector, and L2-normalized.",
  ". Neural Reconstruction": "Dataset details. In our single-traversal dynamic scene re-construction experiments, we selected 10 different loca-tions, each with one traversal, aiming to capture and rep-resent complex urban environments. For our multitraversalenvironment reconstruction experiments, we selected a to-tal of 50 traversals. This comprised 10 unique locations,with 5 traversals for each location, enabling us to capturevariations in illuminating conditions and weather.Implementation Details.Throughout all reconstructionexperiments, we utilize 100 images from the three frontcameras, along with LiDAR data, as input for each traver-sal. Single-traversal experiments: Both iNGP and EmerN-eRF models undergo training for 10,000 iterations utiliz-ing the Adam optimizer with a learning rate of 0.01and a weight decay rate of 0.00001. For EmerNeRF, weleverage the dino feature from the DINOv2 ViT-B/14",
  ", assesses image qual-ity by comparing maximum pixel value MAXI and meansquared error MSE. SSIM, calculated by SSIM(x, y) =(2xy+c1)(2xy+c2)": "(2x+2y+c1)(2x+2y+c2), measures similarity between syn-thesized and ground truth images, factoring in mean, vari-ance, and covariance. LPIPS, unlike the two metrics before,uses a pretrained neural network model to evaluate the per-ceptual similarity between two images.Benchmark models. For the single-traversal task, we adoptEmerNeRF and PVG as benchmark models. Ad-ditionally, for comparison, we conduct experiments usingiNGP and 3DGS , which do not directly target thisproblem. Regarding multitraversal reconstruction, there areno algorithms specifically designed for this task. Therefore,we adopt iNGP as the basic model. Furthermore, to enhancethe models ability to remove dynamic objects, we also testRobustNeRF and iNGP with Segformer .",
  "Single-traversal: Dynamic scene reconstruction": "EmerNeRF. Based on neural fields, EmerNeRF is aself-supervised method for effectively learning spatial-temporal representations of dynamic driving scenes.EmerNeRF builds a hybrid world representation bybreaking scenes into static and dynamic fields. By uti-lizing an emergent flow field, temporal information canbe further aggregated, enhancing the rendering preci-sion of dynamic components. The 2D visual founda-",
  "tion model features are lifted into 4D space-time toaugment EmerNeRFs semantic scene understanding": "PVG. Building upon 3DGS, PVG introduces periodicvibration into each Gaussian point to model the dy-namic motion of these points. To handle the emergenceand vanishing of objects, it also sets a time peak and alifespan for each point. By learning all these param-eters, along with the mean, covariance, and sphericalharmonics of the Gaussians, PVG is able to reconstructdynamic scenes in a memory-efficient way.",
  ". Visual Place Recognition": "Dataset details.We conduct experiments in VPR taskswith both multitraversal and multiagent data. In the multi-traversal case, intersections numbered higher than or equalto 52 are used for testing. In the multiagent setting, scenesnumbered higher than or equal to 50 are used for testing. In-put images are resized to 400 224, and input point cloudsare downsampled to 1024 points.Implementation details. We evaluate our dataset on mod-els mentioned in Sec. 4, where CoVPR is evaluatedwith multiagent data, and all others are evaluated withmultitraversal data.Backbones are pre-trained on Ima-geNet1K . We use ResNet18 as the backbone forNetVLAD and CoVPR, ResNet50 for MixVPR andGeM, and PointNet for PointNetVLAD. The numberof clusters in NetVLAD-based methods is 32. Models aretrained with Adam optimizer with 1e-3 lr for Point-NetVLAD, 1e-4 lr for others, and 1e-4 decay rate until con-vergence. The batch size is 20 for NetVLAD-based meth-ods and 10 for others.Result discussions.Quantitative results are shownin Tab. 3.Although GeM achieves lightweight charac-teristics in its pooling methods, it underperforms com-pared to NetVLAD with a smaller backbone. ViT demon-strates weaker performance in VPR without task-specificpooling methods, despite being a stronger backbone thanResNet.MixVPR achieves the best performance, as itsfeature-mixing mechanism provides richer features. Point-NetVLAD, leveraging point clouds, attains better perfor-mance with smaller input sizes than NetVLAD. In the con-text of multiagent data, CoVPR consistently outperforms . Qualitative result of VPR. We use MixVPR to obtain this qualitative result and mark incorrect results with red frames. Ourdataset contains hard cases such as nighttime, back-lighting, and blurred cameras due to weather conditions.",
  "MultitraversaliNGP 26.040.7590.346RobustNeRF 16.170.6740.459SegNeRF 24.440.7480.358": "foundation model. The estimator employed in this modelis PropNet, incorporating linear disparity and uniform sam-pling. For 3DGS and PVG, we set the training iterationnumber to be 20000, with the learning rate the same as inthe original work . We treat 3DGS as a special caseof the PVG method, with a 0 periodic motion amplitudeand an infinite lifespan, which we set to 106 in our exper-iments. Multitraversal experiments: Our NeRF model inthis experiment is iNGP with image embedding andDINO features. For RobustNeRF, we implement the ro-bust loss and patch sample as described in the original pa-per . In SegNeRF, we apply the SegFormer-B5 model, trained on the Cityscapes dataset. Among the19 categories in the SegFormer model, we identify person,rider, car, truck, bus, train, motorcycle and bicy-cle as dynamic classes and generate masks for them.Result discussions. Single-traversal experiments: Basedon the results presented in Tab. 4, PVG achieves higherSSIM scores and better LPIPS scores, indicating enhancedstructural details.This superior performance by PVG islikely attributed to its flexible Gaussian points setup, which",
  "iNGPEmerNeRF3DGSPVGGT": ". Qualitative results of single-traversal reconstruction. We stack the rendered image and the corresponding rendered depthvertically. Each column corresponds to one baseline method and the last column is the ground truth. The ground truth depth is obtained byprojecting LiDAR points in the camera view. adeptly captures linear motions, and the emergence and dis-appearance of objects. EmerNeRF, on the other hand, ex-cels in PSNR. This is likely due to its novel approach ofdynamic-static decomposition. As shown in , EmerN-eRF and PVG both demonstrate the ability to perfectly ren-der dynamic objects like moving cars, whereas iNGP and3DGS exhibit relatively poor performance in this regard.Multitraversal experiments: Thanks to image embedding,iNGP can render diversely illuminated scenes. However, itstruggles with rendering dynamic objects accurately or re-moving them. As shown in Tab. 4, iNGP achieves the bestsimilarity metrics since it preserves the most informationabout dynamic objects. RobustNeRF performs best in elim-inating dynamic objects, albeit at the cost of rendering staticobjects with less detail. SegFormer, leveraging semantic in-formation, achieves superior visual results compared to theother two methods. Yet the shadows of cars are not com-pletely removed, likely due to the inadequate recognition ofshadows by semantic segmentation models.",
  ". Opportunities and Challenges": "Our MARS dataset introduces novel research opportunitieswith multiagent driving recordings, as well as a large num-ber of repeated traversals of the same location. We outlineseveral promising research directions and their associatedchallenges, opening new avenues for future study.3D reconstruction. Repeated traversals can yield numer- ous camera observations for a 3D scene, facilitating corre-spondence search and bundle adjustment in multiview re-construction. Our dataset can be utilized to study camera-only multitraversal 3D reconstruction, which is crucial forautonomous mapping and localization.The main chal-lenge is to handle appearance variations and dynamic ob-jects across repeated traversals over time. For instance, onerecent work, 3D Gaussian Mapping , leverages multi-traversal consensus to decompose the scene into a 3D en-vironmental map represented by Gaussian Splatting and 2Dobject masks, without any external supervision. Neural simulation. Multiagent and multitraversal record-ings are valuable for crafting neural simulators that canreconstruct and simulate scenes and sensor data.High-fidelity simulations are essential for developing perceptionand planning algorithms. The main challenge lies in repli-cating real-world dynamics and variability, such as mod-eling the behavior of dynamic objects, environmental con-ditions, and sensor anomalies, ensuring that the simulateddata provides a comprehensive and realistic testbed. For in-stance, one recent work proposes a neural scene representa-tion that scales to large-scale dynamic urban areas, handlesheterogeneous input data collected from multiple traversals,and substantially improves rendering speeds . One con-current work proposes a multi-level neural scene graph rep-resentation that scales to thousands of images from dozensof sequences with hundreds of fast-moving objects . Unsupervised perception. Exploiting scene priors in un-supervised 3D perception offers significant value, espe-cially in multitraversal driving scenarios where abundantdata from prior visits can enhance online perception. Thisapproach not only facilitates a deeper understanding of theenvironment through the accumulation of knowledge overtime but also enables unsupervised perception without theneed for training with manual annotations.",
  ". Conclusion": "Our MARS dataset represents a notable advancement in au-tonomous vehicle research, moving beyond traditional datacollection methods by integrating multiagent, multitraver-sal, and multimodal dimensions. MARS opens new avenuesfor exploring 3D reconstruction and neural simulation, col-laborative perception and learning, unsupervised perceptionwith scene priors, etc. Future works include providing an-notations for online perception tasks such as semantic occu-pancy prediction in scenarios of multiagent and multitraver-sal. We strongly believe MARS will establish a new bench-mark in AI-powered autonomous vehicle research.",
  "Acknowledgement": "All the raw data is obtained from May Mobility. We sin-cerely thank Mounika Vaka, Oscar Diec, Ryan Kuhn, Shy-lan Ghoujeghi, Marc Javanshad, Supraja Morasa, Alessan-dro Norscia, Kamil Litman, John Wyman, Fiona Hua, andDr.Edwin Olson at May Mobility for their strong sup-port. This work is supported by NSF Grant 2238968 andin part through the NYU IT High Performance Computingresources, services, and staff expertise. Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are weready for autonomous driving? the kitti vision benchmarksuite. In 2012 IEEE conference on computer vision and pat-tern recognition, pages 33543361. IEEE, 2012. 1, 2 R. Kesten, M. Usman, J. Houston, T. Pandya, K. Nad-hamuni, A. Ferreira, M. Yuan, B. Low, A. Jain, P. On-druska, S. Omari, S. Shah, A. Kulkarni, A. Kazakova, C. Tao,L. Platinsky, W. Jiang, and V. Shet.Woven planet per-ception dataset 2020. 2019. 2 Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jag-jeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, PeterCarr, Simon Lucey, Deva Ramanan, et al. Argoverse: 3dtracking and forecasting with rich maps. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 87488757, 2019. 2 Benjamin Wilson, William Qi, Tanmay Agarwal, John Lam-bert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Rat-nesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes,et al. Argoverse 2: Next generation datasets for self-driving",
  "perception and forecasting.In Thirty-fifth Conference onNeural Information Processing Systems Datasets and Bench-marks Track (Round 2), 2021. 2": "Xinyu Huang, Peng Wang, Xinjing Cheng, Dingfu Zhou,Qichuan Geng, and Ruigang Yang. The apolloscape opendataset for autonomous driving and its application.IEEEtransactions on pattern analysis and machine intelligence,42(10):27022719, 2019. 2 Jakob Geyer,Yohannes Kassahun,Mentar Mahmudi,Xavier Ricou, Rupesh Durgesh, Andrew S Chung, LorenzHauswald, Viet Hoang Pham, Maximilian Muhlegg, Sebas-tian Dorn, et al. A2d2: Audi autonomous driving dataset.arXiv preprint arXiv:2004.06320, 2020. 2 Quang-Hieu Pham, Pierre Sevestre, Ramanpreet SinghPahwa, Huijing Zhan, Chun Ho Pang, Yuda Chen, ArminMustafa, Vijay Chandrasekhar, and Jie Lin. A 3d dataset:Towards autonomous driving in challenging environments.In 2020 IEEE International Conference on Robotics and Au-tomation (ICRA), pages 22672273. IEEE, 2020. 2 Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-ancarlo Baldan, and Oscar Beijbom.nuscenes: A multi-modal dataset for autonomous driving. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1162111631, 2020. 2 Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, AurelienChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,Yuning Chai, Benjamin Caine, et al. Scalability in perceptionfor autonomous driving: Waymo open dataset. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, pages 24462454, 2020. 2 Jiageng Mao, Minzhe Niu, Chenhan Jiang, Jingheng Chen,Xiaodan Liang, Yamin Li, Chaoqiang Ye, Wei Zhang, Zhen-guo Li, Jie Yu, et al. One million scenes for autonomousdriving: Once dataset. In Thirty-fifth Conference on NeuralInformation Processing Systems Datasets and BenchmarksTrack, 2021. 2",
  "Yiyi Liao, Jun Xie, and Andreas Geiger. Kitti-360: A noveldataset and benchmarks for urban scene understanding in 2dand 3d. IEEE Transactions on Pattern Analysis and MachineIntelligence, 2022. 2": "Carlos A Diaz-Ruiz, Youya Xia, Yurong You, Jose Nino, Ju-nan Chen, Josephine Monica, Xiangyu Chen, Katie Luo, YanWang, Marc Emond, et al. Ithaca365: Dataset and drivingperception under repeated and challenging weather condi-tions. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 2138321392,2022. 2 Runsheng Xu, Xin Xia, Jinlong Li, Hanzhao Li, Shuo Zhang,Zhengzhong Tu, Zonglin Meng, Hao Xiang, Xiaoyu Dong,Rui Song, et al. V2v4real: A real-world large-scale datasetfor vehicle-to-vehicle cooperative perception. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1371213722, 2023. 2",
  "open dataset: A large-scale and diverse multimodal datasetfor autonomous driving. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 2017820188, 2023. 2": "Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye KitFong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom,and Sammy Omari. nuplan: A closed-loop ml-based plan-ning benchmark for autonomous vehicles.arXiv preprintarXiv:2106.11810, 2021. 1 Wenjie Luo, Bin Yang, and Raquel Urtasun. Fast and furi-ous: Real time end-to-end 3d detection, tracking and motionforecasting with a single convolutional net. In Proceedings ofthe IEEE conference on Computer Vision and Pattern Recog-nition, pages 35693577, 2018. 1 Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Mani-vasagam, Wei-Chiu Ma, Anqi Joyce Yang, and Raquel Ur-tasun. Unisim: A neural closed-loop sensor simulator. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 13891399, 2023. 1,3 Scott Ettinger, Shuyang Cheng, Benjamin Caine, ChenxiLiu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp,Charles R Qi, Yin Zhou, et al. Large scale interactive motionforecasting for autonomous driving: The waymo open mo-tion dataset. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 97109719, 2021. 2 Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, WenhaiWang, et al. Planning-oriented autonomous driving. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1785317862, 2023. 2 Yiming Li, Zhiding Yu, Christopher Choy, Chaowei Xiao,Jose M Alvarez, Sanja Fidler, Chen Feng, and Anima Anand-kumar. Voxformer: Sparse voxel transformer for camera-based 3d semantic scene completion.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 90879098, 2023. 2 Yiming Li, Sihang Li, Xinhao Liu, Moonjun Gong, KenanLi, Nuo Chen, Zijun Wang, Zhiheng Li, Tao Jiang, FisherYu, et al. Sscbench: A large-scale 3d semantic scene com-pletion benchmark for autonomous driving. arXiv preprintarXiv:2306.09001, 2023. Chao Chen, Xinhao Liu, Yiming Li, Li Ding, and ChenFeng. Deepmapping2: Self-supervised large-scale lidar mapoptimization. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 93069316, 2023. 2 Xinyu Huang, Xinjing Cheng, Qichuan Geng, Binbin Cao,Dingfu Zhou, Peng Wang, Yuanqing Lin, and Ruigang Yang.The apolloscape dataset for autonomous driving. In Proceed-ings of the IEEE conference on computer vision and patternrecognition workshops, pages 954960, 2018. 2 Matthew Pitropov, Danson Evan Garcia, Jason Rebello,Michael Smart, Carlos Wang, Krzysztof Czarnecki, andSteven Waslander.Canadian adverse driving conditionsdataset.The International Journal of Robotics Research,40(4-5):681690, 2021. Pengchuan Xiao, Zhenlei Shao, Steven Hao, Zishuo Zhang,Xiaolin Chai, Judy Jiao, Zesong Li, Jian Wu, Kai Sun,Kun Jiang, et al. Pandaset: Advanced sensor suite datasetfor autonomous driving.In 2021 IEEE International In-telligent Transportation Systems Conference (ITSC), pages30953101. IEEE, 2021. 2 Yiming Li, Shunli Ren, Pengxiang Wu, Siheng Chen, ChenFeng, and Wenjun Zhang. Learning distilled collaborationgraph for multi-agent perception.In Advances in NeuralInformation Processing Systems, volume 34, pages 2954129552, 2021. 2",
  "Yiming Li, Juexiao Zhang, Dekun Ma, Yue Wang, and ChenFeng. Multi-robot scene completion: Towards task-agnosticcollaborative perception. In 6th Annual Conference on RobotLearning, 2022": "Yiming Li, Qi Fang, Jiamu Bai, Siheng Chen, Felix Juefei-Xu, and Chen Feng. Among us: Adversarially robust col-laborative perception by consensus. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 186195, 2023. Sanbao Su, Yiming Li, Sihong He, Songyang Han, ChenFeng, Caiwen Ding, and Fei Miao.Uncertainty quantifi-cation of collaborative detection for self-driving. In 2023IEEE International Conference on Robotics and Automation(ICRA), pages 55885594. IEEE, 2023. Yue Hu, Yifan Lu, Runsheng Xu, Weidi Xie, Siheng Chen,and Yanfeng Wang. Collaboration helps camera overtake li-dar in 3d detection. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages92439252, 2023. Sanbao Su, Songyang Han, Yiming Li, Zhili Zhang, ChenFeng, Caiwen Ding, and Fei Miao.Collaborative multi-object tracking with conformal uncertainty propagation.IEEE Robotics and Automation Letters, 2024.",
  "Suozhi Huang, Juexiao Zhang, Yiming Li, and Chen Feng.Actformer:Scalable collaborative perception via activequeries. In IEEE International Conference on Robotics andAutomation (ICRA), 2024. 2": "Runsheng Xu, Hao Xiang, Xin Xia, Xu Han, Jinlong Li, andJiaqi Ma. Opv2v: An open benchmark dataset and fusionpipeline for perception with vehicle-to-vehicle communica-tion. In 2022 International Conference on Robotics and Au-tomation (ICRA), pages 25832589. IEEE, 2022. 2 Yiming Li, Dekun Ma, Ziyan An, Zixun Wang, Yiqi Zhong,Siheng Chen, and Chen Feng. V2x-sim: Multi-agent col-laborative perception dataset and benchmark for autonomousdriving. IEEE Robotics and Automation Letters, 7(4):1091410921, 2022. 2 Tobias Fischer, Lorenzo Porzi, Samuel Rota Bul`o, MarcPollefeys, and Peter Kontschieder. Multi-level neural scenegraphs for dynamic urban environments. In Proceedings ofthe IEEE conference on computer vision and pattern recog-nition, 2024. 3, 8",
  "Relja Arandjelovic and Andrew Zisserman. All about vlad.2013 IEEE Conference on Computer Vision and PatternRecognition, pages 15781585, 2013. 3, 5": "Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pa-jdla, and Josef Sivic. Netvlad: Cnn architecture for weaklysupervised place recognition.2016 IEEE Conference onComputer Vision and Pattern Recognition (CVPR), 2015. 3,5, 7 Amar Ali-bey, Brahim Chaib-draa, and Philippe Gigu`ere.Mixvpr: Feature mixing for visual place recognition. 2023IEEE/CVF Winter Conference on Applications of ComputerVision (WACV), pages 29973006, 2023. 5, 7 Sijie Zhu, Linjie Yang, Chen Chen, Mubarak Shah, Xiao-hui Shen, and Heng Wang. R2former: Unified retrieval andreranking transformer for place recognition. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, 2023. 3",
  "In The IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2018. 3, 5, 7": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:Representing scenes as neural radiance fields for view syn-thesis. Communications of the ACM, 65(1):99106, 2021.3 Jianfei Guo, Nianchen Deng, Xinyang Li, Yeqi Bai, Bo-tian Shi, Chiyu Wang, Chenjing Ding, Dongliang Wang,and Yikang Li.Streetsurf:Extending multi-view im-plicit surface reconstruction to street views. arXiv preprintarXiv:2306.04988, 2023. 3 Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Prad-han, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron,and Henrik Kretzschmar. Block-nerf: Scalable large sceneneural view synthesis. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages82488258, 2022. 3 Haithem Turki,Deva Ramanan,and Mahadev Satya-narayanan.Mega-nerf:Scalable construction of large-scale nerfs for virtual fly-throughs.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1292212931, 2022. 3 Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, andFelix Heide. Neural scene graphs for dynamic scenes. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 28562865, 2021. 3 Haithem Turki, Jason Y Zhang, Francesco Ferroni, and DevaRamanan. Suds: Scalable urban dynamic scenes. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1237512385, 2023. 3 Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Car-oline Pantofaru, Leonidas J Guibas, Andrea Tagliasacchi,Frank Dellaert, and Thomas Funkhouser. Panoptic neuralfields: A semantic object-aware neural scene representation.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1287112881, 2022.3 Xiao Fu, Shangzhan Zhang, Tianrun Chen, Yichong Lu,Lanyun Zhu, Xiaowei Zhou, Andreas Geiger, and Yiyi Liao.Panoptic nerf: 3d-to-2d label transfer for panoptic urbanscene segmentation. In 2022 International Conference on3D Vision (3DV), pages 111. IEEE, 2022. 3 KonstantinosRematas,AndrewLiu,PratulPSrini-vasan, Jonathan T Barron, Andrea Tagliasacchi, ThomasFunkhouser, and Vittorio Ferrari. Urban radiance fields. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1293212942, 2022. 3 Zian Wang, Tianchang Shen, Jun Gao, Shengyu Huang,Jacob Munkberg, Jon Hasselgren, Zan Gojcic, WenzhengChen, and Sanja Fidler. Neural fields meet explicit geometricrepresentations for inverse rendering of urban scenes. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 83708380, 2023. 3 Andrew Richardson, Johannes Strom, and Edwin Olson.Aprilcal: Assisted and repeatable camera calibration.In2013 IEEE/RSJ International Conference on IntelligentRobots and Systems, pages 18141821. IEEE, 2013. 4",
  "Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler,and George Drettakis.3d gaussian splatting for real-timeradiance field rendering. ACM Transactions on Graphics,42(4), July 2023. 6, 7": "Sara Sabour, Suhani Vora, Daniel Duckworth, Ivan Krasin,David J. Fleet, and Andrea Tagliasacchi. Robustnerf: Ig-noring distractors with robust losses.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 2062620636, June 2023. 6, 7 Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,Jose M Alvarez, and Ping Luo. Segformer: Simple and ef-ficient design for semantic segmentation with transformers.In Advances in Neural Information Processing Systems, vol-ume 34, pages 1207712090, 2021. 6, 7 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE conference on computer vision andpattern recognition. Ieee, 2009. 6 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 6 Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.Pointnet: Deep learning on point sets for 3d classificationand segmentation. In Proceedings of the IEEE conferenceon computer vision and pattern recognition, pages 652660,2017. 6",
  "vision. Transactions on Machine Learning Research, 2023.7": "Marius Cordts, Mohamed Omran, Sebastian Ramos, TimoRehfeld,Markus Enzweiler,Rodrigo Benenson,UweFranke, Stefan Roth, and Bernt Schiele.The cityscapesdataset for semantic urban scene understanding.In Proc.of the IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2016. 7 Yiming Li, Zehong Wang, Yue Wang, Zhiding Yu, Zan Goj-cic, Marco Pavone, Chen Feng, and Jose M Alvarez. Memo-rize what matters: Emergent scene decomposition from mul-titraverse. arXiv preprint arXiv:2405.17187, 2024. 8"
}