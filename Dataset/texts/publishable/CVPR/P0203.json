{
  "Abstract": "Video Object Segmentation (VOS) is a vital task in com-puter vision, focusing on distinguishing foreground objectsfrom the background across video frames. Our work drawsinspiration from the Cutie model, and we investigate the ef-fects of object memory, the total number of memory frames,and input resolution on segmentation performance. This re-port validates the effectiveness of our inference method onthe coMplex video Object SEgmentation (MOSE) dataset,which features complex occlusions. Our experimental re-sults demonstrate that our approach achieves a J&F scoreof 0.8139 on the test set, securing the third position in thefinal ranking. These findings highlight the robustness andaccuracy of our method in handling challenging VOS sce-narios.",
  ". Introduction": "The Pixel-level Video Understanding in the Wild(PVUW) Challenge is a computer vision competition thatencompasses four different tracks. Its overarching aim is toadvance the field of pixel-level scene understanding, whichinvolves classifying object categories, generating masks,and assigning semantic labels to every pixel within an im-age. Given that the real world is dynamic and not limited tostatic images, the ability to segment and understand videosis of greater relevance and practicality for real-world appli-cations. The PVUW Challenge is designed to spur the de-velopment of technologies that can accurately interpret andsegment video content in natural settings, thereby pushingthe boundaries of computer vision in dynamic scene under-standing.In the 3rd of the PVUW challenge, We have noted the ad-dition of two new tracks in the third PVUW challenge: TheMOSE Track includes additional videos and annotationsthat feature challenging elements such as the disappear-ance and reappearance of objects, inconspicuous small ob-jects, heavy occlusions, and crowded environments . The Motion Expression guided Video Segmentation (MeViS)Track is designed to advance the study of natural language-guided video understanding in complex environments, withthe goal of fostering the development of a more comprehen-sive and robust pixel-level understanding of video scenes insuch settings and realistic scenarios through the inclusion ofnew videos, sentences, and annotations .In the realm of Video Object Segmentation (VOS), es-pecially within the semisupervised paradigm, the objectiveis to track and segment objects from a broad range of cat-egories based solely on an initial frames annotation. VOSmethodologies find extensive application in fields such asrobotics , video editing , and in reducing the bur-den of data annotation . Given the Video Object Seg-mentation (VOS) dataset, comprising training and test sets,each frame of the video contains corresponding annotationdata. These annotations are represented as two-dimensionalmatrices of height and width.For the pixel-wise anno-tation matrix, each element records the pixel information(e.g., RGB channels) of the corresponding pixel in the videoframe. For the classification result matrix, each elementrepresents a one-hot vector of length equal to the numberof object categories in the VOS task, indicating the classifi-cation of the corresponding pixel (see ).Recent VOS methods utilize a memory-based paradigm, where a memory representation is built frompreviously segmented frames (either provided or generatedby the model). New query frames then access this memoryto retrieve features for segmentation. These methods pre-dominantly employ pixel-level matching for memory read-ing, whether through a single or multiple matching lay-ers , and construct segmentation from the pixel mem-ory readout. Pixel-level matching independently maps eachquery pixel to a linear combination of memory pixels (e.g.,using an attention layer).However, this approach oftenlacks high-level consistency and is vulnerable to matchingnoise, especially when distractors are present.The complex video object segmentation task focuses onthe tracking and segmentation of objects within intricate",
  ". VOS framework overview. It consists of three indepen-dent components: a segmenter, a referring tracker, and a temporalrefiner": "scenes. MOSE is specifically designed to test VOS mod-els in complex environments featuring multiple objects, fre-quent occlusions, and numerous distractors. This datasetposes a significant challenge due to its high variability andrealistic scenes, making it a more rigorous benchmark thantraditional datasets like DAVIS-2017 . In fact, recentmethods show a performance drop of over 20 pointsin J & F when evaluated on MOSE compared to DAVIS-2017.This significant performance decline on MOSE high-lights the limitations of current memory-based VOS tech-niques. In scenarios with high object density and frequentinteractions, pixel-level matching struggles with segmen-tation accuracy due to noise and distractors. These issuesemphasize the need for more robust and context-aware ap-proaches in memory-based VOS to effectively manage thecomplexities of datasets like MOSE.Recently, the Cutie approach has restructured thevideo object segmentation task into three distinct subpro-cesses: image object segmentation, tracking/alignment, andrefinement.Furthermore, Cutie construct a compact ob-ject memory to summarize object features in the long term,which are retrieved as targetspecific object-level represen-tations during querying, showcasing significant advance-ments in the field of Video Object Segmentation (VOS).Leveraging the exceptional capabilities of Cutie, our teamsecured the 3rd position in the complex video object seg-mentation track of the 3rd PVUW Challenge at CVPR 2024,and all without the need for additional training.",
  ". Method": "Our approach is inspired by recent work on video objectsegmentation, particularly the Cutie framework by Chenget al. . Cutie operates in a semi-supervised video ob-ject segmentation (VOS) setting, where it takes a first-framesegmentation as input and processes subsequent frames se-quentially.Cutie encodes segmented frames into a high-resolutionpixel memory F and a high-level object memory S. Thesememories are used for segmenting future frames. Whensegmenting a new frame, Cutie first retrieves an initial pixelreadout R0 from the pixel memory using the encoded query features. This initial readout is typically noisy due to low-level pixel matching.To enhance this initial readout, Cutie enriches R0 withobject-level semantics using information from the objectmemory S and object queries X. This is done through anobject transformer with multiple transformer blocks. Thefinal enriched output, RL, is then passed to the decoder togenerate the output mask.In summary, Cutie introduces three main contribu-tions: object-transformer, sec:masked-attention, and object-memory.The Cutie-base model is based on the base variant,utilizing ResNet-50 as the query encoder backbone. It con-sists of C = 256 channels, L = 3 object transformerblocks, and N = 16 object queries.The query and mask encoders are designed usingResNets . Following previous studies , we dis-card the final convolutional stage and employ the stride 16feature.The object transformer block integrates both query FFNand pixel FFN components. The query FFN comprises a2-layer MLP with a hidden size of 8C = 2048. Mean-while, the pixel FFN utilizes two 3 3 convolutions witha reduced hidden size of C = 256 to minimize computa-tional overhead. The ReLU activation function is employedthroughout the network.",
  ". Inference": "When testing, the input video is upscaled to a resolu-tion of 720p, which provides a higher density of pixel infor-mation compared to lower resolutions such as 480p. Thisincreased resolution enhances the detail and clarity of thevideo, allowing for more precise segmentation and track-ing of objects. The choice of 720p as the standard testingresolution is motivated by the need to balance between thelevel of detail required for accurate segmentation and thecomputational resources required to process the video data.In the context of the memory frame encoding, we up-date both the pixel memory and the object memory everyr-th frame. The default value of r is set to 3, following thesame configuration used in the XMem framework . Thisinterval strikes a balance between capturing the temporalevolution of the scene and maintaining computational effi-ciency. In the attention component of the pixel memory, weretain the keys k and values v from the first frame, whichis provided by the user. This initial frame serves as a ref-erence point for the video sequence. For subsequent mem-ory frames, we employ a First-In-First-Out (FIFO) strategy,which ensures that the most recent information is retainedwhile older data is gradually phased out. This approach isdesigned to keep the memory footprint manageable and fo-",
  ". The framework of Cutie": "cused on the most relevant frames.The choice of a prede-fined limit of Tmax = 15 for the total number of memoryframes is a practical compromise. This value balances theneed to avoid excessive memory usage and maintain real-time performance while still capturing sufficient temporalevolution of the scene. Maintaining a history of 15 frames isgenerally adequate for effectively exploiting temporal cor-relations in VOS tasks. This enhances segmentation accu-racy by providing enough context for object tracking andappearance prediction without imposing excessive compu-tational overhead or compromising system responsiveness.Extending this limit further could lead to diminishing re-turns, as the additional frames may not significantly im-prove performance and could increase computational loadunnecessarily. The noise in the memory increases with its size, andwhen the sequence is long, performance can degrade. Basedon these observations, we propose filtering affinities to re-tain only the top-k entries. This effectively eliminates noiseregardless of the sequence length. The top-k strategy notonly enhances robustness but also overcomes the overheadassociated with top-k operations.The graph reports theperformance improvement and robustness brought about bytop-k filtering. In our implementation, the top-k operationuses query filtering to refine the memory. To further man-age the memory capacity, we apply top-k filtering withk = 60 to the pixel memory. Setting top-k to 60 has the ef-fect of prioritizing the most relevant pixel memories basedon their attention scores, which is crucial for maintainingaccurate segmentation over time while preventing the mem- ory from being overwhelmed with less significant informa-tion. This approach ensures that the memory retains themost informative aspects of the scene, which are necessaryfor consistent and reliable segmentation results, especiallyin lengthy video sequences. This filtering technique selectsthe top k most relevant pixel memories, based on their at-tention scores, for updating the memory. By doing so, weprioritize the most informative pixel data, which is crucialfor maintaining accurate segmentation over time, while alsopreventing the memory from being overwhelmed with lesssignificant information. In the final testing phase, we employed Test-Time Aug-mentation (TTA), which is a strategy that enhances the ro-bustness and accuracy of predictions by incorporating a va-riety of augmented versions of the input data. TTA is apowerful technique that can help to mitigate overfitting andimprove generalization by simulating variations in the datathat the model might encounter in real-world scenarios. Thecore idea behind TTA is to make predictions not just on theoriginal test samples but also on perturbed versions of them.These perturbations can include a range of augmentationssuch as scaling, cropping, flipping, and rotating the inputimages or videos. By applying these augmentations, wecan capture a broader range of possible transformations thatthe objects of interest might undergo, thereby improving themodels ability to recognize and segment them accurately.Inthe context of video segmentation, TTA can be particularlybeneficial due to the dynamic nature of video data. Framesin a video can have significant variations due to camera mo-tion, object motion, lighting changes, and other environ-",
  ". Comparison with Other Methods": "In the 1st Complex Video Object Segmentation Chal-lenge, our(ISS) method demonstrated significant perfor-mance improvements in both the development and testphases.The leaderboards for the development and testphases are presented in Tables 1 and 2, respectively. Ourmethod achieved Jaccard values (J) and F-Measures (F) thatoutperformed most other participants. Specifically, in thedevelopment phase, our method attained a Jaccard value of0.6892 and an F-Measure of 0.7705, resulting in a com-bined J&F score of 0.7299. Similarly, in the test phase, ourmethod achieved a Jaccard value of 0.7799, an F-Measureof 0.8480, and a combined J&F score of 0.8139. Theseresults highlight the effectiveness and robustness of ourmethod.",
  ". Conclusion": "In this study, we developed a method for Video ObjectSegmentation (VOS) drawing inspiration from the Cutiemodel.We examined key factors such as object mem-ory management, the number of memory frames, and in-put resolution, assessing their impact on segmentation per-formance. Our approach was rigorously evaluated on theMOSE dataset, achieving a notable J&F score of 0.8139,which earned us third place in the test phase. These re-sults underscore the robustness of our method in addressingthe challenges posed by complex occlusions in video se-quences. Ali Athar, Jonathon Luiten, Paul Voigtlaender, Tarasha Khu-rana, Achal Dave, Bastian Leibe, and Deva Ramanan. Burst:A benchmark for unifying object recognition, segmentationand tracking in video. In WACV, 2023. 1"
}