{
  "Abstract": "Precise, pixel-wise geolocalization of astronaut photog-raphy is critical to unlocking the potential of this uniquetype of remotely sensed Earth data, particularly for itsuse in disaster management and climate change research.Recent works have established the Astronaut Photogra-phy Localization task, but have either proved too costlyfor mass deployment or generated too coarse a localiza-tion. Thus, we present EarthMatch, an iterative homog-raphy estimation method that produces fine-grained local-ization of astronaut photographs while maintaining an em-phasis on speed.We refocus the astronaut photographybenchmark, AIMS, on the geolocalization task itself, andprove our methods efficacy on this dataset. In addition,we offer a new, fair method for image matcher compari-son, and an extensive evaluation of different matching mod-els within our localization pipeline. Our method will en-able fast and accurate localization of the 4.5 million andgrowing collection of astronaut photography of Earth. Web-page with code and data at",
  ". Introduction": "Computer vision plays a pivotal role in analyzing re-motely sensed Earth observations, demonstrating efficacyacross diverse applications including Earth monitoring, at-mospheric and climate research, and emergency responsestrategies . Much of this remotely sensed im-agery comes from satellites devoted to Earth observations,but a peculiar, yet valuable complementary source of datais astronaut photography. More than 4.5 million photos ofEarth1 have been taken by astronauts, primarily from the In-ternational Space Station (ISS) during its over 20 years ofcontinuous operation in low Earth orbit (roughly 400 km),a vantage point which offers a privileged perspective with afield of view that can span up to thousands of kilometers.",
  "Loop": ". EarthMatch: To produce a pixel-wise geolocalizationof an astronaut photograph (the query), we first retrieve a possi-ble candidate from a worldwide database of satellite images. Wethen compute image correspondences and coregister the two im-ages iteratively, yielding a precise query localization and confi-dence value. Astronaut photography is particularly valuable due to itsunique characteristics. For instance, the orbital speed andtrajectory of the ISS enables quick response time in case ofnatural disasters and other emergencies, as astronauts canbe promptly alerted and take timely photographs which,after manual localization, are provided to first responderson the ground . Protocols based on this concept havebeen successfully implemented during the 2013 Haiyanscyclone, as well as to handle flooding events, wildfires,and several other crises throughout the planet .Geolocated astronaut photographs have also been used forresearch purposes across a variety of Earth science topics. Furthermore, unlike unmanned satellites,behind each photo is a skilled human (as astronauts un-dergo photography and geography training) that interpretsthe observable landscape and is not constrained to a fixedperspective (see ). Adjusting parameters like orienta-tion, focal length and viewing angle offers intriguing pos-sibilities for data collection which are difficult or impossi-ble to achieve with conventional satellites. Yet, the verysame human-in-the-loop nature of astronaut photographs",
  "arXiv:2405.05422v2 [cs.CV] 3 Jul 2024": ". Examples of astronaut photo queries from the AIMSdataset which we use in our experiments. The rightmost bot-tom image is an example of a photo with high tilt/oblique, whichwe remove prior to our benchmark evaluation. These images arealso the least useful/informative for Earth science researchers. also makes their precise localization challenging, since onlya coarse estimate can be inferred from the location of theISS at the photo acquisition time, and manual verificationhas to be performed on each image for precise localization.Despite the high manual localization cost, over 300k (i.e.still less than 10%) of images have been manually local-ized, demonstrating the importance and value of geotaggedastronaut photographs. In order to unlock the full poten-tial utility of this vast corpus of images, an automated fine-grained geolocalization solution is needed - a task namedAstronaut Photo Localization (APL) by EarthLoc .To this end, the work of Find My Astronaut Photo(FMAP) proposed the first localization solution forthis problem and released a labelled dataset which is ofparamount importance for future works aiming to assess thereliability of methods before they are applied to unlabelledimages. FMAP uses a brute-force approach based on pair-wise matching against a reference obtained from satellitedata. The high latency of this extensive matching makesit unsuitable for applications where speed is of the utmostimportance, such as assisting in natural disaster response.Recently, EarthLoc showed that retrieval methods candrastically reduce the uncertainty on the area of interest byidentifying a handful of candidate regions which are mostsimilar to the query image to be localized. Candidates areretrieved from a worldwide database of cloudless Sentinel-2satellite images2, which divide the Earth into regular, rec-tilinear tiled regions.Despite its robust performance, apipeline based solely on image retrieval does not providepixel-wise localization of the query, nor does it provide aconfidence measure for its predictions.In light of these considerations, we advocate for a hier-archical pipeline that combines an EarthLoc-like retrieval step with a coregistration method capable of providing anaccurate location estimate as well as a reliable confidencescore that allows for the rejection of false positives.Tothis end, we present EarthMatch, a simple yet efficientfine-grained geolocalization algorithm based on iterativehomography refinement, and demonstrate its effectivenessthrough a comprehensive benchmark.In summary, ourmain contributions are: an APL pipeline made of (i) a retrieval model to obtainpossible candidate locations for a given query, and (ii)EarthMatch to confidently obtain a single final predictionthrough an iterative coregistration algorithm; a thorough benchmark in which we implement our algo-rithm with a large number of existing image matchingmethods, ranging from sparse detectors, learned match-ers, handcrafted methods as well as dense warp estima-tors; an extension to the Astronaut Imagery Matching Subset(AIMS) dataset with 268 astronaut photographs andtheir the top-10 predictions (satellite images) from thecurrent SOTA retrieval APL model , and we releasethis extended dataset to foster future research.The code to run the benchmark, as well as scripts to down-load the data, is available at All the images localized withEarthMatch (plus images localized with FMAP) can be ex-plored in a convenient interactive format at",
  ". Related work": "Feature detection and description. Finding salient pointsin an image, and their associated descriptors, is a corner-stone of computer vision. Points and descriptions are em-ployed for a variety of different tasks, such as Visual Local-ization , Structure from Motion (SfM) andSimultaneous Localization and Mapping (SLAM) pipelines. Classical methods rely on a detect-then-describeapproach, typically based on handcrafted descriptors com-puted from local derivatives of the image . The mostpopular among these methods is SIFT which providesa framework for feature detection and description that isrobust to scale and distortions. Follow-up works, such asSURF , focused on improving efficiency, while otherslike ORB combine fast keypoint detection with a ro-bust binary descriptor.As deep learning gained prominence, learnable ap-proaches for both detection and description were intro-duced.In particular, are the first to popularizesparse keypoint detection and description with deep neuralnetworks, relying on contrastive strategies to learn patch-level descriptors with a CNN. Later, SuperPoint intro-duced a self-supervised framework for training on synthetic shapes with arbitrarily defined keypoints. Conversely, D2-Net and R2D2 propose a framework to jointly de-tect and describe features, modeling keypoints as local max-ima of the feature maps. Other subsequent works definesalient points as those that maximize the matching proba-bility, again trained through self-supervision .Among the most recent solutions, ALIKE uses anovel detection module based on a patch-wise softmax re-laxation and has been further extended in ALIKED by exploiting deformable convolutions that adapt to thekeypoints support. DeDoDe decouples the detectionand description steps into two independent models; Steer-ers further improves upon this approach with a formu-lation designed to be rotation invariant. Image matching.Our task of interest, localizing astro-naut photography, has challenges akin to the problem ofwide baseline image matching .This is mainly dueto the substantial appearance shift with respect to our ref-erence set and the minimal overlap that we typically facebetween a query and its retrieved candidate. Image match-ing tries to find correspondences (e.g., points or regions)among images by comparing features. Traditionally, match-ing was performed on handcrafted descriptions with a near-est neighbor search over descriptors followed by Lowes ra-tio test , or via mutual nearest neighbor in the case oflearnable, sparse methods . The landscape evolved withSuperGlue , a graph neural network-based approachfor matching features, which leverages attention mecha-nisms to deal with the challenges of significant viewpointchange or occlusion. The introduction of LoFTR rep-resented a departure from discrete feature detection towardsa detector-free, semi-dense matching paradigm, using trans-formers to coarsely match features. These methods standout for their effectiveness even in texture-sparse scenarios.Following LoFTR, several other methods followed a simi-lar approach . An alternative line ofresearch focuses on dense warp estimation between imagepairs, from which keypoints can be sparsely sampled whenneeded . Recently, RoMA proposed to exploit auniversal vision foundation model, DINOv2 , for thetask of dense feature matching; given the coarse nature ofvision transformers, the authors combine it with a special-ized CNN for match refinement, achieving state-of-the-artresults on a variety of downstream benchmarks .In this work, we are interested in using pairwise matchers toasses the similarity of two images. Typically these methodsare benchmarked on downstream applications like homog-raphy or pose estimation; however, in our case we matchimages which may exhibit only partial spatial overlap orremain entirely distinct. Thus, our goal is to understandwhether they depict the same place or not. Hence, similarto what is done in , we use the keypoints resulting frommatching as a similarity measure. Additionally, we also use these salient points to estimate a projective transformationbetween the images. Iterative homography estimation itselfhas been studied as a technique to refine the co-registrationbetween two images.Traditionally, these methods werebased on the Lucas-Kanade algorithm , which relieson photometric error. In , the authors propose an end-to-end trainable network for deep iterative homography es-timation. Recently, proposed novel handcrafted tech-niques for match and homography refinement. Astronaut photography localization. Recent literature hasshown a growing interest in the problem of localizing pho-tographs from astronauts. Pioneering works Find My As-tronaut Photo (FMAP) and both rely on pairwisecomparison between queries and reference data. While theformer focuses on daytime pictures with satellite imageryas reference, the latter tries to localize night-time images bygenerating synthetic data to bridge the domain gap. Whileshowing convincing performance, these methods are hin-dered by the intrinsic complexity of matching images with-out prior knowledge of candidate regions. Recently, Earth-Loc introduced a retrieval method designed to handle thespecific challenges of astronaut photography, namely largeorientation, scale and appearance variations. Additionally,astronaut photos were used as a downstream benchmarkto demonstrate a rotation equivariant detector proposed bySteerers .In this work, we propose to exploit EarthLocs candidatepredictions to reduce the search area toward a few likelyregions. Given EarthLoc top-k predictions, we apply aniterative matching algorithm that is able to correctly andprecisely localize the query when a suitable candidate ispresent. Furthermore, our method provides a confidencemetric, enabling the identification of instances where a can-didate lacks overlap with the query, thereby establishing amethod to reject false positive candidates from retrieval.",
  ". EarthMatch": "Overview. We propose a hierarchical pipeline to solve theAPL task and precisely localize astronaut photographs. Ourapproach is inspired by the re-ranking approaches com-monly adopted in Image Retrieval and Visual PlaceRecognition , where a fast retrieval methodis used first to obtain a shortlist of candidates, followed bya more computationally demanding matching step to refinethe estimate.An overview of the pipeline is presented in : werely on an APL image retrieval model (e.g. EarthLoc )which, for a given query, provides a shortlist of candidates.Such candidates are retrieved from a worldwide database ofgeotagged satellite images (i.e. images for which the pixel-wise location, in the form of geographic coordinates, isknown). Subsequently, to refine these candidates, we re-",
  "Retrieval StepMatching Step": ". Left: Overview of retrieval step, which, for a given query, retrieves candidates/predictions from a worldwide database ofgeo-tagged images. Right: Overview of matching step. The matching pipeline takes as input the query and a retrieved candidate. Sur-roundings of the candidate are obtained from the database, and then the iterative coregistration (in the form of matching and homographictransformation) is performed. cast the matching step in the form of an iterative coregistra-tion algorithm, called EarthMatch, that is tailored for thetask of APL (see ). EarthMatch takes as input a queryimage Q (i.e., the astronaut photograph) and a candidate im-age C (i.e., a satellite image), where the candidate has beenproposed by the retrieval method as potentially overlappingthe query. The algorithms goal is twofold: (1) understandif the two images have any overlap and (2), if there is anoverlap, precisely estimate the overlap, in the form of a ho-mography matrix. A precise estimate of the overlap allowsextraction of the pixel-wise position of the query, given theknown geographic boundaries of the candidate. To achievethis goal, and to deal with the large baselines that retrievalcandidates present, we iteratively refine the homography es-timate. Problem setting. The hierarchical pipeline takes as inputan RGB query Q, and through EarthLoc we retrieve aset of candidates. Candidates are extracted from a referencedatabase that covers the landmass of the entire planet,as detailed in Sec. 4.We start from a RGB candidateC0 with its corresponding footprint FC, defined as thelatitude and longitude of its four corners, i.e., FC={(lat0, lon0), (lat1, lon1), (lat2, lon2), (lat3, lon3)}andapply our EarthMatch iterative coregistration algorithm.The goal is to compute a homography HQ that mapsthe candidate onto the query such that they overlap, thusproviding the footprint of the query, FQ.",
  ". Single-step pipeline": "The simplest way to obtain FQ is via a single homographyestimated between the query Q and the initial candidate, C0.Such a pipeline requires a matcher M which, given Q andC0, produces an overlap confidence, a value that indicatesthe likelihood of the two images overlapping, and the ho-mography H0 to map the pixels of C0 onto Q. Formally, xQ = H0xC0, where xQ, xC0 are expressed in homoge-neous coordinates. The homography H0 is then simply usedto estimate FQ given the footprint of the candidate FC.Although this single-step pipeline would be sufficient inthe ideal case - with a perfect matcher - in practice match-ers have a hard time producing enough well-distrubutedkeypoint correspondences to estimate a good homography.This is especially true when the overlap is limited, or whenthe transformation is significant, for example those that in-clude large degrees of rotation and re-scaling.Further,aligning Earth observations imagery can have added diffi-culty, as imagery often depict barren, featureless landscapeswhich are notoriously hard to match . In cases whenthere are many correspondences, but they are poorly dis-tributed or, worse yet, highly localized to a single region ofQ, H0 is poorly constrained, resulting in a transform thatdoes not well model the entire image area.To overcome these issues, we implement a multi-step,iterative pipeline, which reduces the burden on the matcherby iteratively producing candidates that have increasinglymore overlap with the query.",
  ". Iterative pipeline": "The iterative pipeline picks up from the end of the single-step version, with a homography H0 estimated from Q andC0. From here, we apply H0 to C0 in order to obtain a newimage C1, which by construction will have a larger overlapwith the query. Then, we use our new C1 as a matchingtarget for Q and obtain a second, improved estimate of thehomography H1.Applying the homography directly to C0 can lead C1 tohave some empty areas from locations depicted in Q butnot in C0. These areas require filling, often handled withblack padding in computer vision libraries. To minimizethis issue, instead of applying the homography directly to",
  "C0": ". Examples of images during the EarthMatch procedure. The procedure starts with matching the query Q and the candidateC0 to produce a first homography H0. The candidate surroundings CS is generated and H0 applied to produce C1. C1 is then matchedwith Q, producing H1 which applied to CS yields C2. This iterative process continues for a fixed number of iterations (4 iterations in ourexperiments).",
  ". Dataset": "We evaluate our method on images from the Astronaut Im-agery Matching Subset (AIMS) dataset .These im-ages have been manually located by NASA scientists, whichhave labelled each images centerpoint in the form of geo-graphic coordinates. These images are a representative sub-set of the over 4.5 million photographs of Earth taken byastronauts on the ISS. The full collection can be accessedat the Gateway to Astronaut Photography of Earth 3. Fromthe 323 images within AIMS, we removed 55 photographswith high tilt/oblique, as these often contain Earth limb andcannot be fit with a homography. This results in a subsetof 268 images for our benchmark, of which some examplesare shown in .For each of these, we use an enhanced version of Earth-Loc , representing the state-of-the-art model for Astro-naut Photography Localization through image retrieval, toobtain the top-10 most similar images from a worldwidedatabase of satellite geo-tagged images. This database con-tains over 13 million images, at different zooms (i.e. differ-ent meter-per-pixel resolutions), covering the entire land-mass of the planet between latitude 60 and -60 (i.e. withinthe boundaries of the ISSs orbit). This covers all areasthat could be depicted in an astronaut photo. FollowingEarthLoc we apply 4x test-time augmentation on thedatabase, by creating four copies of each database image,one each from a rotation of the original image of 0, 90,180 and 270 respectively. This not only improves the re-sults of the retrieval stage, but also embeds the top-10 can-didates with a coarse estimate of the rotation, with respectto North, of the query: as an example, an ideally perfect re-trieval method would find as first prediction a candidate thatis no further than 45 rotated from the query.Even with state-of-the-art retrieval, some queries do nothave any positive within the top-10 candidates. Yet, wewant our benchmark to be as realistic as possible (reflectingthe deployment-time situation of localizing astronaut pho-tos), so we still include these images within the test set. Ofthe 268 queries, 244 (91%) have at least one positive pre-",
  ". Experimental setup": "EarthMatch is performed on the top-10 retrieval predic-tions, and can be executed with virtually any image match-ing technique. We run the process (matching+homography)for 4 iterations per retrieval candidate, and at each iterationcheck the stopping criteria to reject invalid predictions. Ifthe process reaches the end of the 4 iterations, a predictionis obtained, otherwise the process is performed on the nextcandidate. For some queries, no prediction is obtained at theend of the iterative process over the top-10 candidates: thischaracteristic reflects the requirement of APL, for which aprediction should be generated only if its almost certainlycorrect, otherwise no prediction should be created. We com-pute the homography with a vanilla RANSAC, with the de-fault parameters from OpenCV .For our benchmark, we define a simple metric below, andwe perform a large number of experiments with differentmatchers, image resolutions and number of keypoints, asdescribed in the following section. Metric. Each query image is labelled with a manually an-notated centerpoint, so we deem an image to be correctlylocalized if the estimated footprint contains the centerpoint.This is a different metric from other astronaut photogra-phy works , but is more closely aligned with thelocalization task (see Sec. 7 for more details). Note thatwhile in theory, this metric leaves open the possibility thata prediction could be considered correct in the case whenan inaccurate footprint happens to contain the centerpoint,we find (after visually analyzing hundreds of predictions)that in practice this does not happen: i.e. a prediction which . Changing resolution. Although RoMa is best overall,SIFT-LightGlue is best with low-resolution images, followed byDogHardNet-LightGlue. All learnable descriptors rapidly fail asimages resolution decreases.",
  "contains the centerpoint has almost perfect overlap with thequery, as visually shown in": "Image Size.We repeat the main experiment with inputimage sizes ranging from 64 to 1024 px width/height (im-ages are square), resized with a naive resize operation (nopadding/cropping). The same size is applied to both thequery and candidates. Number of Keypoints. When a matcher is parameterizedby the maximum number of keypoints, we analyze howmatching performance changes as we adjust this value. Wetest max keypoints in (1024, 2048, 4096, 8192). Matchers.We conduct a comprehensive benchmark ofmatchers, inserting matching models from different fami-lies of methods. We divide models into three main fami-lies: (i) detector-based local feature descriptors, with eithernearest neighbor (NN) matching or learned matchers (e.g.SuperGlue or LightGlue ); (ii) detector-free match-ers, and finally (iii) dense matchers, which estimate a densewarp between two images. Specifically, we test: Detector-based:we consider handcrafted methods(SIFT,ORB)aswellasmorerecent",
  "Average70.968.672.766.577.372.067.474.463.227.31-": ". Percent of images correctly localized by each model, for different subsets of AIMS. Each column indicates a different subsetof AIMS, with the subset depending on the image acquisition conditions - camera focal length, camera tilt (i.e. the obliqueness of thecamera w.r.t. Earths surface) and the cloud coverage in the image. In each column header, the two numbers indicate the number of querieswithin the subset and the number of localizable queries, i.e. those for which at least one of the top-10 retrieval predictions is correct.Results show the percentage of queries correctly localized among the localizable ones, so that the upper bound is 100%. Methodswith threshold Tinl = do not produce any false positives - negative candidates are rejected before 4 iterations - so there is no thresholdto compute. Best bolded, second best underlined. Time is seconds per astronaut photo query for evaluation of all 10 candidates per query.Each models score is shown for best image size, number of keypoints configuration (see Tab. 2). ModelSIFT - NNORB - NND2-NetR2D2SuperPoint - SGDeDoDeSteerersALIKED - LGDISK - LGDogHardNet - LGSIFT - LGSuperPoint - LGPatch2PixPatch2Pix - SGLoFTRRoMaBest # Keypoints81928192-819220484096409640962048819240961024-2048-1024Best Image Size7681024768102476876876810247685122567687687687681024 . Best image size and number of keypoints for each model, resulting from a grid search for each method with exponentiallygrowing image sizes between 64 64 and 1024 1024, and number of keypoints of 1024, 2048 and 4096. These are the hyperparametersused in Tab. 1. D2-Net, Patch2Pix and LoFTR do not take as input the number of keypoints. SP stands for SuperGlue, LG for LightGlue.",
  ". Results": "Image Size.In we observe varying performancedepending on image size. Most models see performancedegradation as image size decreases below 512 px, but thehighest performing model at higher resolution, RoMA, seesthe steepest dropoff, with no successful matches at 256 pxsizes and lower. Dense models like RoMa appear to have aminimum size for matchability, whereas other sparse mod-els have a more gradual performance dropoff. In the in-",
  "creasing size direction, many models begin to plateau at 512px. This suggests that for these models, matching on largerimages does not improve performance, only increases run-time/compute": "Number of Keypoints. There are two patterns that emergefrom the (maximum) number of keypoints experiments(). For most models, performance increases with num-ber of keypoints, plateauing somewhere between 2048 and4096. This generally matches expectations, as more key-points in an image increase the chances the same keypointis selected in both images, increasing the change of forminga good correspondence. AIMS Subsets. In Tab. 1 we examine matching perfor-mance on multiple subsets of AIMS, particularly with dif-ferent focal lengths, low/high cloud cover (occlusion) andlow/high tilt (i.e. the angle at which the camera was held,inducing larger perspective change in query images). Gen-erally, RoMa is the best performing model across scenarios,showcasing robustness under strong visual changes. Dueto its dense matching nature it requires a larger number ofinliers for identifying good predictions. Among detector-based matchers, Steerers is the clear winner, achieving goodperformance with a much lower number of inliers, and at al-most twice the speed of RoMa. In general, the methods ofALIKED + LightGlue, Steerers and RoMa produce optimalspeed-accuracy trade-offs.While the majority of models do not produce any falsepositives (i.e. for wrong candidates, EarthMatch stops be-fore reaching the final iteration, thus rejecting the candi-date), a few models do admit them, requiring computationof an inlier threshold Tinl to further validate correctness.When a threshold is computed, candidates must exceed thisvalue to be considered a confident prediction. Tinl can bestrongly affected by even a single highly confident falsepositive. This can increase Tinl such that a number of truepositives are discarded as their inlier count does not exceedthis inflated Tinl. This behaviour, seen in the rightmost col-umn of Tab. 1, explains the uneven behavior of the RoMacurve in and , where just a single high-confidentfalse positive causes a strong dip in results.In analyzing different subsets of AIMS, the most chal-lenging setting is high cloud cover, which creates a natu-ral occlusion to the matching process. Most methods alsobenefit from low camera tilt, which leads to high similaritybetween the query and the candidate. RoMa presents a no-table exception to this trend, achieving better performanceswith high camera tilt.",
  "Limited hyperparamer tuning. One of the goals of thisbenchmark is the fair evaluation of existing image match-ing models on the out-of-distribution (w.r.t. their training": "sets) domain of Astronaut Photography Localization. Tothis end, in our evaluation we tuned only the two hyperpa-rameters that are recurrent across multiple models, namelythe input image size and the number of keypoints. For fairevaluations, we decided not to tune any model-specific hy-perparameters, given that (1) this would give an advantageto methods with more hyperparameters and (2) it would leadto an explosion in the number of experiments. We there-fore note that despite EarthMatch showing the potential ofeach model on the domain of APL, these results could po-tentially still be improved with per-model hyperparameteroptimization, including the threshold used for RANSAC,which could be an interesting future development. Map Projection. Finally, in this work we used a map pro-jected representation of Earth. In particular, we used theMercator projection, which preserves angles but not areas.While this can be problematic when viewing large areas(i.e. visually, areas closer to the poles appear larger thanthey are), we found this to have no noticeable impact onmatchability or the estimated footprint of considered im-ages, due to the images having a relatively small area withrespect to the Earths surface. Its possible other projectionscould yield improved matching in some, likely more polar,regions where the Mercator projection is visually more dis-similar to the true view of the Earth. This would have astronger impact when registering Earth limb photos, whichare however not considered within our benchmark.",
  ". Conclusion": "In this work we present a pipeline to reliably and confi-dently estimate the footprint of astronaut photographs. Thepipeline is made of a pre-existing retrieval method andour newly introduced EarthMatch, an iterative coregistra-tion algorithm that takes advantage of image matching mod-els to output a predicted footprint and confidence. We runa large number of experiments with many matchers usingdifferent images sizes and number of keypoints, thoroughlyevaluating their usability in the domain of Astronaut Pho-tography Localization.To foster future research and simplify reproducibility, werelease the post-retrieval dataset of astronaut photo queryand top 10 candidates used for our experiments, relievingresearchers to having to compute the time-consuming large-scale retrieval step. We also release the code to replicate allexperiments within this paper. This code is trivially exten-sible to future matching methods or other domains.Finally, our pipeline offers an efficient and robustmethod for astronaut photography localization that can im-mediately be deployed on the existing 4.5 million and grow-ing database of astronaut photos of Earth.",
  "Acknowledgements.We acknowledge the CINECA awardunder the ISCRA initiative, for the availability of high per-": "formance computing resources.This work was supportedby CINI. G. Goletto is supported by PON Ricerca e Inno-vazione 2014-2020 DM 1061/2021 funds.Project sup-ported by ESA Network of Resources Initiative.This studywas carried out within the project FAIR - Future Artificial In-telligence Research - and received funding from the EuropeanUnion Next-GenerationEU (PIANO NAZIONALE DI RIPRESAE RESILIENZA (PNRR) MISSIONE 4 COMPONENTE 2,INVESTIMENTO 1.3 D.D. 1555 11/10/2022, PE00000013).This manuscript reflects only the authors views and opin-ions, neither the European Union nor the European Commis-sion can be considered responsible for them.European Light-house on Secure and Safe AI ELSA, HORIZON EU Grant ID:101070617",
  "Simon Baker and Iain Matthews. Lucas-kanade 20 years on:A unifying framework. International Journal of ComputerVision, 56:221255, 2004. 3": "Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krys-tian Mikolajczyk. Hpatches: A benchmark and evaluation ofhandcrafted and learned local descriptors. In Proceedings ofthe IEEE conference on computer vision and pattern recog-nition, pages 51735182, 2017. 3 Giovanni Barbarani, Mohamad Mostafa, Hajali Bayramov,Gabriele Trivigno, Gabriele Berton, Carlo Masone, and Bar-bara Caputo.Are local features all you need for cross-domain visual place recognition? In CVPRW, pages 61556165, 2023. 3",
  "global features for image search. In ECCV, pages 726743.Springer Int. Publishing, 2020. 3": "Si-Yuan Cao, Jianxin Hu, Zehua Sheng, and Hui-Liang Shen.Iterative deep homography estimation.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 18791888, 2022. 3 Hongkai Chen, Zixin Luo, Lei Zhou, Yurun Tian, MingminZhen, Tian Fang, David McKinnon, Yanghai Tsin, and LongQuan.Aspanformer: Detector-free image matching withadaptive span transformer. European Conference on Com-puter Vision (ECCV), 2022. 3",
  "Hugh Durrant-Whyte and Tim Bailey. Simultaneous local-ization and mapping: part i. IEEE robotics & automationmagazine, 13(2):99110, 2006. 2": "Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle-feys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-Net:A Trainable CNN for Joint Detection and Description of Lo-cal Features. In CVPR, 2019. 3, 7 Johan Edstedt, Ioannis Athanasiadis, Marten Wadenback,and Michael Felsberg.Dkm:Dense kernelized featurematching for geometry estimation.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1776517775, 2023. 3",
  "JR Elliott.Earth observation for the assessment of earth-quake hazard, risk and disaster management. Surveys in geo-physics, 41(6):13231354, 2020. 1": "Kenton Fisher, Sara Schmidt, and Alex Stoken. Crew earthobservations: New tools to support your research. In 12thAnnual International Space Station Research and Develop-ment Conference. Center for the Advancement of Science inSpace, Inc., 2023. 1 Saman Ghaffarian, Joao Valente, Mariska Van Der Voort,and Bedir Tekinerdogan. Effect of attention mechanism indeep learning-based remote sensing image processing: Asystematic literature review. Remote Sensing, 13(15):2965,2021. 1",
  "Jiayi Ma, Xingyu Jiang, Aoxiang Fan, Junjun Jiang, andJunchi Yan. Image matching from handcrafted to deep fea-tures: A survey. International Journal of Computer Vision,129:2379, 2020. 4": "Dmytro Mishkin, Filip Radenovic, and Jiri Matas. Repeata-bility is not enough: Learning affine regions via discrim-inability. In Proceedings of the European conference on com-puter vision (ECCV), pages 284300, 2018. 7 HiroyukiMiyazaki,MasahikoNagai,andRyosukeShibasaki.Reviews of geospatial information technologyand collaborative data delivery for disaster risk manage-ment. ISPRS international journal of geo-information, 4(4):19361964, 2015. 1",
  "Peter Schwind and Tobias Storch.Georeferencing urbannighttime lights imagery using street network maps. RemoteSensing, 14(11), 2022. 3": "Edgar Simo-Serra, Eduard Trulls, Luis Ferraz, IasonasKokkinos, Pascal Fua, and Francesc Moreno-Noguer. Dis-criminative learning of deep convolutional feature point de-scriptors. In 2015 IEEE International Conference on Com-puter Vision (ICCV), pages 118126, 2015. 2 W. L. Stefanov and C. A. Evans. Data collection for disasterresponse from the international space station. The Interna-tional Archives of the Photogrammetry, Remote Sensing andSpatial Information Sciences, XL-7/W3:851855, 2015. 1",
  "M. Justin Wilkinson and Yanni Gunnell. Fluvial Megafanson Earth and Mars. Cambridge University Press, 2023. 1": "Yoav Yair, Melody Korman, Colin Price, and Eytan Stibbe.Observing lightning and transient luminous events from theinternational space station during ilan-es: An astronauts per-spective. Acta Astronautica, 211:592599, 2023. 1 Xiaoming Zhao, Xingming Wu, Weihai Chen, Peter CYChen, Qingsong Xu, and Zhengguo Li. Aliked: A lighterkeypoint and descriptor extraction network via deformabletransformation. IEEE Transactions on Instrumentation andMeasurement, 2023. 3, 7 Xiaoming Zhao, Xingming Wu, Jinyu Miao, Weihai Chen,Peter C. Y. Chen, and Zhengguo Li.Alike:Accurateand lightweight keypoint detection and descriptor extraction.IEEE Transactions on Multimedia, 25:31013112, 2023. 3",
  ". Astronaut Photography Localization Met-rics": "Since the introduction of the Astronaut Image Matching Subset(AIMS) dataset in FMAP , various metrics have been pro-posed to best capture the astronaut photography localization chal-lenge. FMAP itself uses a fixed set of reference images providedas part of AIMS, and for each astronaut photograph, there may beone or more matching reference images. FMAP measures averageprecision over these reference images. Such a metric emphasizeshigh recall matching, such that any query/reference pair with suf-ficiently overlapping extents should match.FMAP uses a fixed, discrete set of inlier thresholds and at most100 negatives in its average precision calculation. Steerers expands this calculation to a continuous inlier range, and uses allof the negatives in AIMS. This gives a more complete, but other-wise comparable, metric.EarthLoc introduces the concept of retrieval to astronautphotography localization, and brings a retrieval oriented metricto the task: Recall@N. This is the percent of astronaut photosin which one of the top N retrieved reference images is correct,with correctness defined as having non-zero overlap between theastronaut photo and reference image. Due to recasting APL as aretrieval, rather than a pairwise matching task, this metric is notdirectly comparable to the average precision of FMAP and Steer-ers.In this work, we again shift the metric, this time to more closelyalign it with the downstream localization task. Since the end goalof APL is to find the correct location of an astronaut photograph onEarth, and our clearest indication of the correct place on Earth isthe manually annotated center point, we use the number (or, equiv-anetly, percent) of photos where our predicted footprint containsthe centerpoint as a metric, and denote this as images correctlylocalized. This is a better measure of real world performance andallows us to compare methods as they would operate in deploy-ment."
}