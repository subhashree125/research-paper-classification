{
  "Abstract": "The robustness of machine learning models has beenquestioned by the existence of adversarial examples. We ex-amine the threat of adversarial examples in practical appli-cations that require lightweight models for one-class clas-sification. Building on Ilyas et al. , we investigate thevulnerability of lightweight one-class classifiers to adver-sarial attacks and possible reasons for it. Our results showthat lightweight one-class classifiers learn features that arenot robust (e.g. texture) under stronger attacks. However,unlike in multi-class classification , these non-robust fea-tures are not always useful for the one-class task, suggestingthat learning these unpredictive and non-robust features isan unwanted consequence of training.",
  ". Introduction": "One-class classification, also known as anomaly detection,is useful in security applications. For instance, drone de-tection is useful for law enforcement agencies to maintainthe safety of airspaces. However, devices that deploy au-tomated detection may have computational and power con-straints, limiting the application of computer vision mod-els. These practical concerns motivate the need to integratelightweight models into one-class classifiers.However, computer vision neural network models canbe vulnerable to adversarial attacks which try to evade orcause false detections. To evaluate the security of deployedmodels, we investigate the usefulness and robustness of fea-tures learnt by lightweight one-class classifiers. Our contri-butions are as follows:1. Wetesttheperformanceandvulnerabilityoflightweight one-class classifiers.We find that thesemodels learn useful features, but their predictive powerdiminishes with smaller models and adversarial drift.Additionally, models are vulnerable to adversarial at-tacks regardless of adversarial drift, suggesting that theylearn non-robust features (e.g., texture, background). Semantics: Drone Non-Robust Feature: Drone Label: Drone Semantics : Not Drone Non-Robust Feature: Not Drone Label: Not Drone Adversarial example on random label Semantics : Drone/Not DroneNon-Robust Feature: Drone Label: Drone Semantics : Drone/Not DroneNon-Robust Feature: Not Drone Label: Not Drone Original DataNon-Robust Feature (NRF) Data NRF Training",
  ". Evaluation framework of the usefulness of non-robustfeatures (e.g. texture) on one-class classification, adapted fromIlyas et al.": "2. We test a hypothesis from Ilyas et al. regarding thereason for adversarial examples, specifically that non-robust features are spuriously correlated with the seman-tics in the image used for the prediction task. Contraryto their findings in multi-class classification, we showthat this is not the case in lightweight one-class clas-sifiers such non-robust features are not necessarilycorrelated with the semantics of the image useful forone-class classification, and their correlation (or lackthereof) is independent of the size or adversarial robust-ness of the model.These findings suggest that, in addition to learning usefulfeatures, neural networks sometimes learn features that arenot useful nor robust to adversarial attacks. This calls forfuture work on avoiding learning such features.",
  "arXiv:2407.06372v1 [cs.LG] 8 Jul 2024": "Carlini and Wagner indicate that detection of these at-tacks can also be evaded. Several works have been con-ducted to understand why neural networks are vulnerable.A theory proposed by Ilyas et al. suggests that neu-ral networks learn non-robust features (NRFs) which areused for the task they are trained on. For instance, the tex-ture and background could be correlated with the semanticsof the image, which introduces unwanted spurious corre-lations between the class label and the image texture andbackground . When the model learns these undesirablebut predictive shortcuts, adversarial examples exist by in-tervening on these NRFs while keeping the semantics ofthe image the same. Li et al. builds on their results inmulti-class classification to show that these NRFs are task-specific, transferring poorly to other tasks.We aim to investigate the reasons for the existence ofadversarial examples in one-class classification, which weview from an anomaly detection perspective.One-classclassification is different from Ilyas et al. and Li et al. in two ways.First, we consider distribution shift of anomalies, alsoknown as adversarial drift, which is standard inanomaly detection .One-class classification relaxesthe closed-world assumption that multi-class classifiers usually have, which is that test-time semantics are presentduring training.In one-class classification, the distribu-tion of anomalies at test time is unknown and likely to dif-fer from that at train time, potentially exhibiting multiplemodes (e.g., many classes with different semantics). Evenas adversarial drift increases, meaning the distribution oftest anomalies moves further away from the anomalies seenduring training, we still aim for zero-shot generalization toclassify these unseen classes as anomalies. For instance,if birds and planes are anomalies during training, test-timeanomalies might include jeeps and cranes. Instead of al-tering the loss function as suggested by Li et al. , wemodify the distribution of anomalies. For example, chang-ing the loss function might involve altering the normal classwithin the same dataset, whereas changing the distributionalters the test-time anomalies from train-time anomalies.Second, attacks in the one-class scenario are limited.Ideologically, normal data can only be subjected to untar-geted attacks, which aim to make the data appear less likethe normal class to the classifier. Conversely, anomalousdata can only be subjected to targeted attacks. This one-class set-up is different from multi-class settings like Ilyaset al. where targeted attacks are launched.",
  ". Set-up: Defining Features": "We first summarize the set-up of Ilyas et al. , which weadapt. Define features F := {f : X R} as the collectionof all possible outputs of each neuron before the classifierlayer c (e.g., in the penultimate layer). Features refer to abstract notions of attributes of an image used for predic-tion, such as the number of wings in an image. A classifierC := c [fi]ri=1 has the classifier layer composed of r fea-tures. Its output is in . Denote the ground truth labelfor x as y(x) {1}, and suppress the dependence on xto have y = y(x). Features are then split into 2 categories their utility and robustness with respect to a distribution D: -useful features f on distribution D are features thatare > 0 correlated to the label on average:",
  ". Training on a non-robust feature dataset": "We adapt Ilyas et al. to generate a dataset with NRFs af-ter training one-class classifier C on the original dataset. Weaim to (1) destroy the correlation between the robust fea-tures (e.g., semantics) of the image and the label, and (2) in-crease the correlation between the NRFs and the label. Thisway, the label in the NRF dataset represents the class corre-sponding to the respective NRFs, not the robust features.To remove the correlation between robust features andthe labels, for each image (train and test), we randomly picka label yNRF {1} that will become its new label in theNRF dataset. To increase the correlation between NRFs andthe labels, we perform a targeted adversarial attack on theoriginal classifier C with label yNRF using perturbation",
  "= arg minL(yNRF , C(x + ))(3)": "where L is the loss function. The attack ensures that eachnew image has NRFs correlated with the new label yNRF .Splitting this NRF dataset into training and testing, wetrain another model C on the training NRF dataset, whichwe refer to as an NRF model. C can only learn correlationsbetween NRFs. If the NRFs are useful (i.e., correlated withthe label), we expect good performance on both the NRFtest dataset. Otherwise, we expect random performance.",
  "DifferentCrane2, JeepsTrucks": "pre-trained frozen encoder on ImageNet-1K and a trainableone-class classifier head. In increasing size of the encoders,we test (1) MobileNetV3-small which has 3M parameters, (2) EfficientNetB0 which has 5.3M parameters ,and (3) ResNet18 which has 11M parameters . Toallow supervised training while maintaining the bias of en-closing decision boundaries for the desired (positive) class,we follow the approach outlined in Lau et al. .We designate drones from the Drone vs. Bird dataset as the positive class. In practice, domain knowledge onanomalies of interest during deployment is often present, sorelevant examples can be included during training. Hence,we use flying objects for the negative class: birds from theDrone vs. Bird dataset, bald eagle and airliner images fromImageNet-1K .We introduce different distributionshifts beyond the known anomalies to simulate varyingdegrees of adversarial drift for unseen anomalies. Tab. 1categorizes these anomalies by two criteria: the imagerysource and semantic similarity to the training data.Forthe imagery source, in addition to testing on Drone vs.Bird and ImageNet-1K images, we also test on CIFAR-10images . CIFAR-10 images are lower resolution thanthe training images, introducing one source of adversarialdrift. To introduce another source of adversarial drift, weadd land vehicles for testing, such as jeeps, which aresemantically different from flying objects.We refer toindividual classes like jeeps as negative subclasses of theoverall negative class.The performance of models on different negative sub-classes corresponds to the usefulness of features for eachsubclass. Our main metric will be the average precision(the area under the precision-recall curve) across 3 runs overeach negative subclass and the overall negative class. Theaverage precision allows us to measure the separation be-tween the positive drone class and any negative (sub)classin a more general setting without the specifics of setting athreshold. We also observe similar patterns for other met-rics (precision, recall, and F1 score under 90% true positiverate) but leave them out due to space constraints.",
  ". Results": "PerformanceThe blue bars in shows our resultson a subset of subclasses, and Tab. 3 in Appendix B hasthe full results. For all (sub)classes, we see that all mod-els achieve non-trivial performance, with all the blue bars . Standard and robust performance of different backbonesacross a subset of negative subclasses and the overall negativeclass. Each group of bars represents the average precision of thenegative (sub)class against the positive drone class for differentclassifiers. The first green bar is the random baseline, followedby pairs of bars which are the standard and robust performance ofMobileNetV3 (Mob.), EfficientNetB0 (Eff.) and ResNet18 (Res.)in order. Classes are grouped by roughly increasing adversarialdrift from left to right, with semantic differences (diff.) and im-agery differences (ImageNet-1K/CIFAR-10). being much higher than the green bars (which refer to ran-dom performance). We observe that performance on seenclasses is generally higher and has a lower variance for seensubclasses than unseen subclasses, suggesting that featureslearned for seen subclasses are very useful (i.e., is high,with little variance across runs). A drop in performance forcrane2 and jeep highlights that semantic adversarial driftaffects performance the most, suggesting that the positiveclass (drone) semantics may not have been learned well.In particular, MobileNetV3 (the smallest backbone model)has a drop in performance as adversarial drift increases onboth semantics and imagery drifts, while EfficientNetB0and ResNet18 backbones seem to produce more useful fea-tures for CIFAR-10 (imagery drift) but there is still somevariance in their usefulness. As expected, the usefulness oflearned features tend to decrease with adversarial drift. VulnerabilityWe evaluate the adversarial robustness oftrained models by performing white-box adversarial attackson these models. This is known as robust performance, andit corresponds to the robustness of features learned. Weobserve that stronger attacks are required, with the origi-nal 2 projected gradient descent (PGD) attack with attackstrength = 0.25 in not being effective and PGDwith = 4/255 in being mildly effective. We posit that the increased robustness of features learned could be due tothe easier task of one-class classification compared to multi-class classification, but leave this to future work. Hence,we report robust performance for PGD with = 0.25as the red bars in . With this stronger attack, ev-ery model can achieve almost random performance for ev-ery subclass, regardless of adversarial drift. The ResNet18backbone is the exception with consistently more robustperformance than the other two, but it is also more com-putationally heavy. Nevertheless, the effectiveness of theattack suggests that models learn NRFs during training. Weproceed to focus on attacks with strength 0.25 to ex-plain adversarial vulnerability. Usefulness of NRFsWe create NRF datasets with PGD of strengths = 0.5, 0.25 and evaluate NRF mod-els on NRF test data.We also include attack strength = 4/255 to follow Li et al. , but note that featuresare still relatively robust to this threat model. Sample NRFimages created with = 0.5 are shown in . Wereport the overall average precision in a and sub-class results in Tab. 3 in Appendix B. An interesting resultemerges. MobileNetV3 and ResNet18 models trained onthe NRF training data have close to perfect performance on = 0.5, 0.25 NRF test data, suggesting that NRF featuresfrom these models are correlated with the one-class label.For these two models, decreasing the attack strength forgenerating the NRF dataset to = 4/255 produces less use-ful NRFs, but still achieve non-trivial performance. How-ever, EfficientNetB0 models achieve flipped results, obtain-ing random performance for = 0.5, 0.25 and non-trivialperformance for = 4/255.There are a few conclusions that we draw. First, NRFusefulness seems to be independent of model size, becausethe smallest and biggest model have useful NRFs while themedium-sized model does not. Furthermore, as the attackstrength decreases, the usefulness of NRFs generated de-crease for useful NRFs and increase for useless NRFs. Sec-ond, usefulness of NRFs learnt by models seems to be inde-pendent of the models adversarial robustness ResNet18is consistently the most robust to adversarial attacks acrossall subclasses and learns useful NRFs, while MobileNetV3models learn useful NRFs and EfficientNetB0 models learnNRFs which are not useful. Third, the results of Efficient-NetB0 highlight that the predictive power of one-class clas-sifiers could come from learning some features that are notuseful for the task, and their adversarial vulnerability arisesfrom these learnt features being non-robust.We also evaluate NRF models on the original test datasetand report the overall average precision in b and sub-class results in Tab. 4 in Appendix B. Across all modelsand (sub)classes, we observe random performance of NRFmodels for NRF data attack strength = 0.5, even for",
  "(b) Tested on the original dataset": ".Overall performance of NRF models trained on thenon-robust feature (NRF) dataset, tested on the NRF and originaldataset respectively. NRF datasets generated with PGD withvarying strengths = 0.5, 0.25, 4/255. the MobileNetV3 and ResNet18 NRF models trained withuseful NRFs. Across each model, the overall performanceincreases as the attack strength to generate the NRF datadecreases, but subclass performance varies and the perfor-mance across (sub)classes is generally far lower than themodels trained on the original dataset. A difference in per-formance on the NRF and original test dataset suggests thatNRF models are learning features apart from the NRFs fromthe PGD attack used to create the NRF datasets. Moreover,the drop from non-trivial performance on NRF data to al-most random for the relevant NRF models hints that NRFmodels are learning features that are negatively correlatedwith the original one-class task. The undesired consequenceof learning features that are not useful (and potentiallyharmful) in standard models is echoed in NRF models too.",
  ". Conclusion": "In conclusion, we observe that one-class classifiers can stillbe vulnerable to adversarial attacks, but it may not be dueto learning useful non-robust features.We showed thatone-class classifiers generally learn useful features for bothseen and unseen data, but features learnt by smaller models(MobileNetV3-small) were less useful than bigger modelsas adversarial drift increases. Moreover, successful attacksneed to be stronger than previous multi-class classificationsettings, especially for bigger models (ResNet18). Never-theless, the presence of a successful attack suggests thatthese lightweight one-class classifiers still use non-robustfeatures. Unlike in Ilyas et al. , we show that the non-robust features learnt are sometimes not useful for the one-class task the model was trained for. Furthermore, we showthat model size and adversarial robustness are not good pre-dictors on their usefulness. These results show that modeltraining can produce features that are not useful, not ro-bust or both during training. An important follow-up workwould be to investigate the cause of models learning theseunwanted features which are not useful nor robust, and howto prevent this. Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nel-son, Nedim Srndic, Pavel Laskov, Giorgio Giacinto, andFabio Roli.Evasion attacks against machine learning attest time. In Joint European Conference on Machine Learn-ing and Knowledge Discovery in Databases, page 387402.Springer Berlin Heidelberg, 2013. 2 Battista Biggio, Igino Corona, Zhi-Min He, Patrick P. K.Chan, Giorgio Giacinto, Daniel S. Yeung, and Fabio Roli.One-and-a-Half-Class Multiple Classifier Systems for Se-cure Learning Against Evasion Attacks at Test Time, page168180. Springer International Publishing, 2015. 2",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In 2016 IEEEConference on Computer Vision and Pattern Recognition(CVPR), pages 770778, 2016. 3": "Andrew Howard, Mark Sandler, Grace Chu, Liang-ChiehChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,Ruoming Pang, Vijay Vasudevan, Quoc V. Le, and HartwigAdam. Searching for mobilenetv3. In Proceedings of theIEEE/CVF International Conference on Computer Vision(ICCV), 2019. 3 Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, LoganEngstrom, Brandon Tran, and Aleksander Madry. Adversar-ial examples are not bugs, they are features. In Advances inNeural Information Processing Systems. Curran Associates,Inc., 2019. 1, 2, 3, 4, 6",
  "Alex Krizhevsky. Learning multiple layers of features fromtiny images. Technical report, 2009. 3": "Matthew Lau, Leyan Pan, Stefan Davidov, Athanasios P Me-liopoulos, and Wenke Lee. Geometric implications of clas-sification on reducing open space risk. In The Second TinyPapers Track at ICLR 2024, 2024. 3, 6 Ang Li, Yifei Wang, Yiwen Guo, and Yisen Wang. Adver-sarial examples are not real features. In Advances in NeuralInformation Processing Systems, pages 1722217237. Cur-ran Associates, Inc., 2023. 2, 3, 4, 6",
  "A.2. One-Class Classifier": "We design our one-class classifier to have a classificationhead on top of a frozen pre-trained encoder. The classifica-tion head is trainable and designed for supervised anomalydetection, where the training data has the class of interestas well as examples from a proper subset of negative sub-classes encountered during deployment. We use the ideafrom Lau et al. , which is a neural network designedto enclose decision regions in the spirit of anomaly detec-tion. We detail our training details that follow their work.We train a neural network with 2 leaky rectified linear unit(ReLU) layers and 1 linear layer with a Gaussian bump acti-vation, with the last layer of a Gaussian radial basis function(RBF) having a fixed parameter at the all ones vector. Alllinear layers maintain the same dimensionality as the previ-ous layer, which amounts to r r weight matrices where ris the dimension of the penultimate layer of the encoder.In this model, the features of the model would corre-spond to the output of the the bump activation, which rep-resents the distance between the latent representation and alearnt hyperplane.. More precisely, since the output of thebump activation is between 0 and 1, we can translate it byany threshold to recover the formulation in Eq. (1). Al-ternatively, one can replace the formulation in Eq. (1) togeneralize the notion of correlation to arbitrary functions aswell.We fine-tune using logistic loss (binary cross entropy)with the Adam optimizer at a learning rate of 0.001 for 15epochs with early stopping patience of 7. From our exper-iments, 15 epochs allows good training and validation per-formance.",
  "A.3. Dataset": "Tab. 2 is the train-test split that we used for our experi-ments. For CIFAR-10 classes, we randomly chose 300 im-ages from each class for evaluation. In our training set-up,we designed the one-class classification supervision to bebalanced to allow training to be more effective, as comparedto having few negatives.Since the encoder has been pre-trained on ImageNet-1K, the encoder has trained with an upstream task on theImageNet-1K images we use during testing. One potentiallimitation is the concern of test leakage for ImageNet-1Kevaluations. We opted to use ImageNet-1K in our evalua-tions because it is the most similar to the Drone vs. Bird dataset in terms of resolution, and has readily available la-bels. Nevertheless, we believe that the effects of test leak-age are mitigated for the following reason. We ensured thatwe tested on other datasets: the Drone vs. Bird and CIFAR-10 datasets. Our results from the Drone vs. Bird datasetshow results on fine-tuned representations, while ImageNet-1K results show results on pre-trained and fine-tuned rep-resentations. On the other hand, CIFAR-10 shows resultson zero-shot representations. Showing that one-class clas-sifiers can achieve non-trivial performance across all nega-tive subclasses (ImageNet-1K or not) suggests that our re-sults on ImageNet-1K is not an overly optimistic estimate.In fact, crane2 and jeep classes are seen during pre-trainingbut still have worse performance than CIFAR-10 classes,which merely emphasizes the impact of semantic adversar-ial drift on the usefulness of features learnt. As suggestedby Li et al. , this could be due to the fact that NRFstransfer poorly to other tasks.",
  "Drone vs. BirdDrones (Positive)328100": "Drone vs. BirdBirds110290ImageNet-1KBald Eagle109182ImageNet-1KAirliner109291ImageNet-1KVulture0295ImageNet-1KAirship0323ImageNet-1KCrane20304ImageNet-1KJeep0308CIFAR-10Airplane0300CIFAR-10Bird0300CIFAR-10Truck0300 . Standard and robust performance of different backbones across a subset of negative subclasses and the overall negative class.Each group of bars represents the average precision of the negative (sub)class against the positive drone class for different classifiers. Thefirst green bar is the random baseline (calculated in expectation), followed by pairs of bars which are the standard and robust performanceof each model of the following order: MobileNetV3 small (Mob.), EfficientNetB0 (Eff.) and ResNet18 (Res.). Classes are groupedby approximately increasing adversarial drift from left to right, with semantic differences (diff.) and imagery differences (ImageNet-1K/CIFAR-10). . Average precision of the positive class (drone) against seen classes (birds, bald eagle, airliner) and unseen classes (vulture,airship, jeep and crane2). Overall average precision is also reported. Random classifier is reported in expectation. We report the standardperformance, robust performance and performance of NRF models trained with NRF datasets generated with PGD under varyingstrengths = 0.5, 0.25, 4/255.",
  "MobileNetV3": "Standard0.9430.007 0.9420.024 0.9770.007 0.8620.032 0.8830.040 0.8250.027 0.6880.027 0.6800.033 0.4430.082 0.5360.078 0.2370.030Robust0.3090.065 0.4240.086 0.4170.081 0.3210.093 0.2960.083 0.3270.098 0.3320.088 0.3140.063 0.3100.085 0.3350.105 0.0950.060NRF (=0.5)0.2280.019 0.3850.104 0.4620.021 0.2710.062 0.2500.029 0.3400.065 0.5180.115 0.2090.041 0.1700.029 0.2890.121 0.0340.006NRF (=0.25)0.2760.049 0.3690.026 0.3500.027 0.2560.010 0.2230.030 0.2820.038 0.3730.157 0.4890.129 0.5050.116 0.6810.151 0.0460.005NRF (=4/255) 0.3260.031 0.4300.089 0.4340.095 0.3120.094 0.3080.074 0.3150.060 0.2860.030 0.5170.173 0.5740.272 0.5750.285 0.0570.014",
  "EfficientNetB0": "Standard0.9660.007 0.9900.006 0.9830.011 0.9660.018 0.8880.034 0.7660.076 0.5320.188 0.8960.036 0.8720.028 0.8850.061 0.4340.106Robust0.2640.001 0.3700.006 0.4330.031 0.2800.008 0.3060.017 0.2840.008 0.3230.018 0.3460.022 0.3310.042 0.3700.028 0.0480.003NRF (=0.5)0.2560.000 0.3550.000 0.3440.000 0.2530.000 0.2360.000 0.2480.000 0.2450.000 0.2500.000 0.2500.000 0.2500.000 0.0350.000NRF (=0.25)0.2880.003 0.3530.007 0.3760.012 0.2490.007 0.2700.012 0.2670.008 0.2430.007 0.2880.013 0.2880.013 0.2880.013 0.0390.004NRF (=4/255) 0.2680.065 0.5260.088 0.5220.033 0.3850.067 0.3130.063 0.3430.052 0.2420.095 0.4110.153 0.4470.174 0.4620.255 0.0520.019",
  "ResNet18": "Standard0.9750.007 0.9840.016 0.9730.029 0.9540.025 0.9260.050 0.7930.105 0.5770.167 0.9020.084 0.9390.060 0.8330.095 0.4880.144Robust0.4840.071 0.5680.093 0.6090.086 0.5120.088 0.4820.093 0.4700.129 0.4940.136 0.5160.074 0.5110.101 0.5110.103 0.1730.071NRF (=0.5)0.2240.028 0.3330.041 0.4300.032 0.2860.034 0.2460.006 0.2490.051 0.3620.162 0.3390.079 0.3640.084 0.4480.120 0.0390.000NRF (=0.25)0.4750.022 0.3330.038 0.3800.012 0.2780.028 0.2890.011 0.2000.011 0.1720.011 0.6720.043 0.6780.065 0.6600.083 0.0420.002NRF (=4/255) 0.4960.106 0.5290.062 0.4970.173 0.4460.027 0.5330.086 0.3200.078 0.2700.073 0.5380.275 0.5540.294 0.5080.294 0.0780.024 . Performance of models trained with non-robust features (NRFs) on the NRF test data. Models tested have MobileNetV3-small,EfficientNetB0 and ResNet18 backbones. NRFs are generated with PGD with varying strengths of = 0.5, 0.25, 4/255, accordingto the respective model backbone during standard training. Average precision of the positive class (drone) against seen classes (birds,bald eagle, airliner) and unseen classes (vulture, airship, jeep and crane2). Overall average precision is also reported. Random classifieris reported in expectation. MobileNetV3 and ResNet18 models achieve close to perfect performance on = 0.5, 0.25, suggesting thatnon-robust features learnt during standard training are useful. However, EfficientNetB0 achieves random performance for these attackstrengths, suggesting that NRFs learnt during standard training are not useful."
}