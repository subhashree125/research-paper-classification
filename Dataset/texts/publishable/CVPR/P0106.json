{
  "Abstract": "Referring Video Object Segmentation (RVOS) aims tosegment the object referred to by the query sentencethroughout the entire video.Most existing methods re-quire end-to-end training with dense mask annotations,which could be computation-consuming and less scalable.In this work, we aim to efficiently adapt foundation seg-mentation models for addressing RVOS from weak su-pervision with the proposed Grounded Prompting (Gro-Prompt) framework. More specifically, we propose Text-Aware Prompt Contrastive Learning (TAP-CL) to enhancethe association between the position prompts and the re-ferring sentences with only box supervisions, includingText-Contrastive Prompt Learning (TextCon) and Modality-Contrastive Prompt Learning (ModalCon) at frame leveland video level, respectively.With the proposed TAP-CL, our GroPrompt framework can generate temporal-consistent yet text-aware position prompts describing loca-tions and movements for the referred object from the video.The experimental results in the standard RVOS bench-marks (Ref-YouTube-VOS, Ref-DAVIS17, A2D-Sentences,and JHMDB-Sentences) demonstrate the competitive per-formance of our proposed GroPrompt framework given onlybounding box weak supervisions.",
  ". Introduction": "Referring Video Object Segmentation (RVOS), aims to seg-ment the object referred to by a sentence query throughoutthe entire video. In contrast to RIS, RVOS is particularlyfaced with dynamic visual challenges, such as position andsize variation, pose deformation, object occlusion or exit,and scene variation. Moreover, the referring sentence maycontain long-term motions or actions (e.g., a gold fish on",
  "*Equal contribution.Work done during an internship at NVIDIA": "the left swimming towards the top right), which could notbe easily recognized from a single frame. To address thischallenging task, many works have been proposed. URVOS is pioneering as aunified framework for referring video segmentation, whichintroduces memory attention modules to retrieve relevantinformation from the previous frame and encourage tempo-ral consistency. With the rapid development of Transformer,ReferFormer adopts encoder and decoder layers in theTransformer model and views language as queries to attendto the referred object, and an instance matching strategy isutilized to achieve object tracking. Recent works like FS-RVOS and OnlineRefer further extend RVOS intothe few-shot setting and online pipeline to handle limitedsamples and ongoing videos in real-world scenarios, respec-tively. Nevertheless, most existing methods require end-to-end training for vision-language models, which could becomputationally expensive and time-consuming. Moreover,the requirement of dense mask annotations for training im-pedes the scalability of those approaches. Recently, foundation segmentation models has been proposed. By leveraging numerous training dataand employing large-scale model architectures, they canproduce high-quality object masks according to variousprompts such as points or boxes, and have shown over-whelming generalizability on various datasets, setting su-perior benchmarks for segmentation tasks. However, thereare still challenges in the RVOS problem not addressedby those foundation models. For example, SAM istrained solely with images and their associated masks, nottailored to handle natural language descriptions and videodata in RVOS. While it is possible to adapt SAM to the taskof RVOS by incorporating grounding models (e.g., )to generate text-associated position prompts and trackingmodels (e.g., ) to capture object motions across videoframes, such naive combination of off-the-shelf modelshas shown to be suboptimal , as they are individually",
  "arXiv:2406.12834v2 [cs.CV] 23 Jun 2024": "trained for different tasks.Therefore, a question arises:How can we effectively exploit foundation segmentationmodels to address RVOS? We argue that the RVOS prob-lem can be decomposed into referring, video, and segmen-tation factors, and leave the segmentation problem to foun-dation segmentation models. We only focus on addressingthe referring and video factors as current foundation modelscan already tackle to segmentation problem effectively.In this paper, we aim to efficiently adapt image-based foundation segmentation models for addressingreferring video object segmentation from weak super-vision.To achieve this goal,we propose a novelGrounded Prompting (GroPrompt) framework, which ad-vances vision-language learning to produce temporal-consistent yet text-aware position prompts for segmen-tation purposes.More specifically, we propose Text-Aware Prompt Contrastive Learning (TAP-CL) to enhancethe association between the position prompts and the re-ferring sentences with only box supervisions, includingText-Contrastive Prompt Learning (TextCon) and Modality-Contrastive Prompt Learning (ModalCon) at frame leveland video level, respectively. For TextCon, we enforce ourGroPrompt framework to generate distinct position promptsfor different referring sentences within each video frame.As for the ModalCon, given that the sentence descrip-tion may contain long-term motions or actions spanningacross different moments, we propose to align the wholesequence of position prompts and the corresponding objectwith the input text for each video clip. With the proposedTAP-CL, our GroPrompt framework can generate temporal-consistent yet text-aware position prompts describing loca-tions and movements for the referred object from the video.More importantly, our derived position prompts would beutilized to instruct image-based foundation segmentationmodels to produce object masks, enabling efficient adap-tation to referring video object segmentation without re-quiring dense mask annotations. The experimental resultsin the standard RVOS benchmarks (Ref-YouTube-VOS,Ref-DAVIS17, A2D-Sentences, and JHMDB-Sentences)demonstrate the competitive performance of our proposedGroPrompt framework given only bounding box weak su-pervisions.We highlight the contributions of this paper as follows: We propose a novel Grounded Prompting (GroPrompt)framework, which performs efficient prompting andadapts image-based segmentation models to addressreferring video object segmentation without additionalfinetuning. To generate temporal-consistent yet text-aware positionprompts for segmentation purposes, we propose tojointly perform Text-Contrastive Prompt Learning andModality-Contrastive Prompt Learning at frame-level",
  ". Referring Video Object Segmentation": "Referring Video Object Segmentation (RVOS) strives to segment the object described by afree-form sentence query across the entire video duration.Recently, ReferFormer views language as queries topay attention to the referred object by adopting an encoder-decoder style in the transformer. However, this work onlysupports offline training and inference, limiting its usagein real-world scenarios. More recently, OnlineRefer further proposes an online RVOS setting to deal with theissues about offline limits, which makes it more possi-ble to adapt to real-world scenarios.Nevertheless, mostexisting methods require end-to-end training for vision-language models, which could be computationally expen-sive and time-consuming.Moreover, the requirement ofdense mask annotations for training impedes the scalabilityof those approaches. Instead, we propose to exploit founda-tion segmentation models without text- and temporal-awareprompting, which is trained without mask annotations andsupports online settings.",
  ". Foundation Segmentation Models": "In recent years, foundation vision models have gained mas-sive attention given their remarkable generalization ca-pabilities on various downstream tasks.More recently,SAM has introduced a foundation model specificallytailored for segmentation tasks. SAM allows specific po-sition prompts (e.g., points, boxes, etc.)to demonstratethe zero-shot ability on the open vocabulary segmentationtasks with novel image distributions. Several works havestudied the versatility of SAM, including remote sensingimages , medical image analysis , andadaptation to video-based tracking task , etc.For adaptation to tracking tasks with SAM, SAM-PT designs a point-based prompt enhancement for theoriginal SAM point prompt to support classic video ob-ject segmentation tasks, while neglecting the importanceof text prompt for advanced referring video object seg-mentation.Another example SAM-Track attemptsto utilize SAM for segmentation and detection of objectswhile the DeAOT module captures the motion acrossframes for tracking the objects. Though it is possible tocombine text-grounding detection models (e.g., Ground- ing DINO ) with SAM-Track to tackle RVOS, Ref-SAM has studied the possible concerns and indicatesthe unsatisfactory performance compared with current SO-TAs in RVOS tasks.Different from the above, we pro-pose temporal-aware prompting with foundation segmenta-tion models (e.g., SAM) to tackle RVOS problems.",
  ". Overview": "Problem Definition.For the sake of completeness, wefirst define the problem setting and notations used in thispaper. In Referring Video Object Segmentation (RVOS),we assume that the training data contain a set of N videos,where each video V = {It}Tt=1 is a sequence of T framesand is associated with a set of referring sentences S ={Si}Mi=1 describing M distinct objects. The goal of RVOSis to produce segmentation masks for the referred objects.Different from previous works which requiredense mask annotations for training, we assume that weonly have access to box-level annotations Bi = { Bit}Tt=1for the T frames corresponding to the ith referring sentenceSi, where each bounding box Bit = (xit, yit, hit, wit) is rep-resented by the coordinate of the center point and the heightand width. Framework Overview.Under the above setting, our goalis to efficiently adapt image-based foundation segmentationmodels for addressing referring video object segmentationfrom such weak supervision. To achieve efficient modeladaptation, we propose a novel Grounded Prompting (Gro-Prompt) framework,which advances vision-languagelearning to produce temporal-consistent yet text-aware po-sition prompts for segmentation purposes.As shownin , our proposed GroPrompt framework is de-signed to generate the bounding box proposal by takingobject queries to perform cross-modal attention at eachframe.Such proposals then serve as position promptsto instruct foundation segmentation models to segmentthe referred object.To facilitate the position promptsto be text- and temporal-aware, we propose Text-AwarePrompt Contrastive Learning (TAP-CL), including:1)Text-Contrastive Prompt Learning (TextCon) at the framelevel, which encourages the output proposals to be dis-tinct when taking different referring sentences as input; 2)Modality-Contrastive Prompt Learning (ModalCon), whichaims to align the output proposal sequence and its corre-sponding object with the input text for each video clip.With the proposed TAP-CL, our GroPrompt frameworkwould produce temporal-consistent yet text-aware positionprompts for the referred object, enabling efficient adapta-tion from weak supervision without additional finetuningfor foundation models.",
  ". Efficient Grounded Prompting and Adaptation": "Recent foundation segmentation models havepresented overwhelming performance on various segmenta-tion tasks. When prompted by points or bounding boxes in-dicating the positions, these foundation models would pro-duce high-quality object masks as desired. However, ex-isting foundation segmentation models are mainly trainedfrom general image data and therefore have limited abil-ity to comprehend video content or complex text descrip-tions. To adapt image-based foundation segmentation mod-els to address referring video object segmentation, our pro-posed GroPrompt framework is designed to learn and gen-erate position prompts for the target object from the inputvideo frames and the referring sentences. In this way, ourGroPrompt framework enables efficient model adaptationwithout additional finetuning for foundation models, avoid-ing possible overfitting issues while reducing computationalcost and time. We now detail our learning scheme below.",
  "Weakly-Supervised Position Prompts": "To produce precise position prompts for segmentation, weadvance vision-language learning to generate bounding boxproposals for the referred object. As illustrated in ,our GroPrompt framework first employs a Transformer-based image-text encoder to extract visual features and lin-guistic features for each frame It and the referring sentenceSi, respectively. Inspired by , we adopt the query gen-eration mechanism to obtain a set of object queries Qit. Bytaking visual features and linguistic features as keys andvalues, the derived object queries Qit would perform cross-attention through the cross-modality decoder to generate thebox proposal Bit. With the ground-truth bounding box Bit,the standard box loss Lbox is formulated by the regressionloss and generalized IoU loss Lg :",
  "(1)": "where r and g are hyper-parameters for the two lossterms, respectively.Here, since there is typically onlyone target object in referring segmentation tasks, we sim-ply select the output proposal Bit with the highest confi-dence score at each frame instead of using the Hungar-ian loss for matching. It is worth noting that we donot need mask loss for training like most existing RVOSworks .",
  "Segmentation": "(a) box (b) . Overview of our proposed GroPrompt framework. In (a), our proposal generation takes each frame It and the referring sentenceSi to derive object queries Qit and produce the prompt embedding pit for segmentation, with another sentence Sj as input for performingText-Contrastive Prompt Learning. In (b), to handle sentence descriptions containing long-term motions or actions in referring video objectsegmentation, we uniquely present Modality-Contrastive Prompt Learning to align the text with the referred object at the video level. both the person and the surfboard. To mitigate such textambiguity in natural language, we propose to perform Text-Contrastive Prompt Learning (TextCon) at the frame level togenerate distinct proposals for different referring sentences.Apart from the text ambiguity, the sentence descriptionsin referring video object segmentation often contain long-term motions or actions. Sentences like a gold fish on theleft swimming towards the top right require consideringall the frames as a whole to perform video segmentation.To align the text with the referred object at the video level,we uniquely present Modality-Contrastive Prompt Learn-ing (ModalCon). The learning scheme is detailed below. Text-Contrastive Prompt Learning.Formally, in addi-tion to the input sentence Si, we forward another sentenceSj through our GroPrompt framework to obtain the outputproposal Bjt for another object at each frame. To performcontrastive learning, we leverage the prompt encoder fromthe foundation segmentation models to extract the promptembeddings pit, pjt, and pit for the proposals Bit and Bjt andthe ground-truth bounding box Bit, respectively. By takingpit, pit, and pjt as the anchor, positive, and negative sam-ple, the frame-level triplet contrastive loss Lfcontra wouldbe computed as follows:",
  "(2)": "We note that to preserve the latent space learned by foun-dation models for segmentation, we choose to freeze theprompt encoder during training.Under the guidance ofthe prompt encoder, our proposed TextCon enforces thedistinctness of the proposals while enhancing the positionprompts to be text-aware. Modality-Contrastive Prompt Learning.In additionto the prompt embedding pit derived in Text-ContrastivePrompt Learning, we also utilize the image encoder to ex-tract the visual features ft. With the cross-attention per-formed at each frame by taking the prompt embedding pitas the query and visual features ft as keys and values, fol-lowed by an average pooling layer for temporal aggregation,the video-level content feature f i would be encoded for thereferred object. As for the referring sentences Si and Sj, wederive the sentence-level linguistic features zi and zj fromthe text encoder. Then, the video-level triplet contrastiveloss Lvcontra would be computed as follows:",
  "J &FJFJ &FJF": "URVOS ECCV20RefYT47.245.349.251.547.356.0MTTR CVPR22RefYT55.354.056.6---ReferFormer CVPR22RefC, RefYT62.961.364.661.158.164.1MANet ACM MM22RefYT55.654.856.5---LOCATER TPAMI23RefYT56.554.858.1---VLT TPAMI23RefC, RefYT63.861.965.661.658.964.3R2-VOS ICCV23RefC, RefYT61.359.663.1---HTML ICCV23RefC, RefYT63.461.565.262.159.265.1OnlineRefer ICCV23RefC, RefYT63.561.665.564.861.667.7SgMg ICCV23RefC, RefYT65.763.967.463.360.666.0TempCD ICCV23RefC, RefYT65.863.668.064.661.667.6SOC NeurIPS23RefC, RefYT67.365.369.365.862.569.1LoSh arXiv23RefC, RefYT64.262.566.062.559.565.4RefSAM arXiv23RefC, RefYT62.160.963.369.565.973.2EPCFormer arXiv23RefYT, AVOS65.062.967.2--- UniNEXT CVPR23RefC, RefYT, G, La, T, YT, B, V, O66.264.068.466.762.371.1DEVA ICCV23RefC, RefYT, YT, D, O66.0--66.3--UniRef ICCV23RefC, RefYT, RefD, YT, O, LV67.465.569.266.362.969.7MUTR arXiv23RefC, RefYT, AVSB68.466.470.468.064.871.3",
  "WRVOS arXiv23RefYT (box + 1st-frame mask)46.645.647.647.344.650.0Grounded-SAM arXiv23RefC (box)62.361.063.665.262.368.0GroPrompt (Ours)-RefC (box), RefYT (box)65.564.166.970.667.873.3": ". Quantitative comparison to state-of-the-art methods on the validation split of Ref-YouTube-VOS and Ref-DAVIS17. RefYT:Ref-YouTube-VOS, RefD: Ref-DAVIS, RefC: RefCOCO , AVOS: Audio-VOS , AVSB: AVSBench , YT: YouTube-VOS2019 , D: DAVIS17 , O: Occluded VIS , LV: Long-term VOS , G: GOT-10K , La: LaSOT , T: TrackingNet ,B: BDD100K , V: VIS19 . where Lcontra = fLfcontra+vLvcontra, and f and v arehyper-parameters for the two contrastive loss, respectively.With the proposed TAP-CL, our GroPrompt frameworkwould produce temporal-consistent yet text-aware bound-ing box proposals, allowing video segmentation by takingthe learned proposals to prompt image-based foundationsegmentation models. It is worth repeating that, the abovelearning scheme does not require any dense mask annota-tions. Furthermore, our proposed GroPrompt frameworklearns to prompt instead of finetuning foundation models,enabling efficient adaptation to referring video object seg-mentation from weak supervision.",
  ". Datasets and Evaluation Metrics": "Datasets.WeconductexperimentsonfourRVOSbenchmark datasets:Refer-Youtube-VOS , Refer-DAVIS17 , A2D Sentences , and J-HMDB Sen-tences . Refer-Youtube-VOS is a large-scale dataset forRVOS, with 3, 975 videos, 7, 451 objects, and 27, 899 ex-pressions. Refer-DAVIS17 is augmented from the popularvideo object segmentation dataset, DAVIS17 . It con-tains 90 videos (60 for training and 30 for testing) with more than 1, 500 expressions. A2D Sentences and J-HMDB Sentences are extended from the A2D andJ-HMDB datasets with sentences describing the actorsand actions appearing in the video content. A2D Sentencescontains 3, 036 training videos and 746 testing videos witha total of 6, 656 sentences, while J-HMDB Sentences con-tains 928 video clips of 21 different actions and 928 sen-tences. Evaluation Metrics.For the Ref-Youtube-VOS and Ref-DAVIS17 datasets, we follow the standard protocol andadopt the following evaluation metrics: region similarity J(average IoU), contour accuracy F (average boundary sim-ilarity), and their mean value J &F. Since the annotationsof the Ref-Youtube-VOS validation set are not publicly re-leased, we evaluate the results on the official server. As forRef-DAVIS17, we use the official code for evaluation. ForA2D Sentences and J-HMDB Sentences, we adopt Preci-sion@K, Overall IoU, and Mean IoU for evaluation. Over-all IoU is the ratio between the total intersection and theunion area over all the testing data, and Mean IoU is theaveraged IoU over the testing data. Precision@K measuresthe percentage of testing data with IoU score higher than athreshold K, where K [0.5, 0.6, 0.7, 0.8, 0.9].",
  ". Implementation Details": "We follow from to train our model on the Ref-YouTube-VOS dataset, and directly evaluate by the valida-tion set provided by Ref-YouTube-VOS and Ref-DAVIS17.For our detailed model architecture, our image-text encodercomprises Swin-Transformer for the image featuresand BERT for the text features. Besides, we set up ourcross-modality decoder with 6 cross-attention transformerlayers. For the segmentation part, we take SAM as our mainsegmentor to take our special text-aware position promptas input. Thus, the prompt encoder, image encoder, andmask decoder are followed by SAM in our setting. We setthe learning rate to 0.0001 and train our framework for 12epochs. Following , we set r and g as 5 and 2 re-spectively. As for f and v, we use 0.01 and 0.1 on Ref-Youtube-VOS and 0.0001 and 0.001 on A2D Sentences, re-spectively. We implement our framework in PyTorch andtrain the model on 8 NVIDIA V100 GPUs.",
  ". Quantitative and Qualitative ComparisonsTo evaluate our proposed GroPrompt framework, wefirst provide quantitative comparisons with state-of-the-": "art methods on Refer-Youtube-VOS and Refer-DAVIS17 . As shown in , we see that our Gro-Prompt framework achieves 65.5% and 70.6% in J &Fon Refer-Youtube-VOS and Refer-DAVIS17, respectively.Compared with RefSAM , our GroPrompt framework is3.4% and 1.1% higher on the two datasets. This validatesthat our learned position prompts would properly instructfoundation segmentation models to perform referring videoobject segmentation. While UniRef and MUTR achieve competitive performance on Refer-Youtube-VOS,these methods require large-scale referring or video datafor training. Compared to WRVOS , which observesbox-level supervision plus the mask annotation for the firstframe, our GroPrompt framework is over 20% higher withbox-level supervision only. Similar results are observed onA2D Sentences and J-HMDB Sentences . In Ta-ble 2, our method reports 71.3% in Mean IoU. As for J-HMDB Sentences, we achieve 72.4% in .",
  ". Setting and Efficiency Comparisons": "In , we compare the setting of our proposed Gro-Prompt framework with recent RVOS methods. From thistable, we see that WRVOS attempts to address RVOSfrom box-level weak supervision plus the ground-truthmask for the first frame, while OnlineRefer extendsReferFormer with query propagation to handle ongo-ing videos under the online setting. However, these meth-ods require end-to-end training for vision-language mod-els, which could be computationally expensive and time-consuming. On the other hand, assuming that additionalvideo data are accessible, DEVA decouples RVOS intoimage segmentation and temporal propagation to increasethe scalability.Compared to these works, our proposedGroPrompt framework decouples RVOS into proposal gen-eration and prompted segmentation with no need for addi-tional video data for training. In this decoupled manner,our framework can learn proper prompts from weak super-vision for foundation segmentation models and could alsobe applied to online settings.In , we also provide efficiency comparisons withrecent works. We see that the number of trainable parame-ters of our method is over 7 times fewer than DEVA. This isbecause that our proposed GroPrompt framework learns toprompt foundation models for efficient adaptation instead oftraining a vision-language model end-to-end. Together withthe quantitative comparisons in , we validate that ourproposed GroPrompt framework is preferable in terms ofperformance, setting, and efficiency.",
  ". Ablation Studies": "To verify the effectiveness of our proposed loss functions,we conduct ablation studies by taking the ground-truthbounding boxes to compute the IoU scores of the predictedbox proposals on Ref-DAVIS17.From , we seethat when only Lbox is considered, the box and segmen-tation score J &F would improve 3.3% and 4.6% com-pared to Grounded-SAM .If we further apply ourproposed Lcontra to perform contrastive learning at framelevel and video level, the box and segmentation score wouldimprove to 74.4% and 70.6%, which are 1.2% and 0.8%higher. Finally, if we directly take the ground-truth boxes toprompt SAM, the superior performance of 83.6% in J &Fwould be observed. This demonstrates that image segmen-tation could be mostly solved by SAM, and therefore howto generate proper prompts to instruct foundation segmen-tation models for referring segmentation tasks would nowbe of interest. From the above experiments, we confirmthat our proposed loss functions would learn precise posi-tion prompts (box proposals) from the referring sentenceand the input video, allowing efficient adaptation of foun-dation models for addressing RVOS. 5. ConclusionInthiswork,weproposetheGroundedPrompting (GroPrompt) framework to efficiently adaptfoundation segmentation models for addressing RVOSfrom weak supervision.More specifically, we proposeText-Aware Prompt Contrastive Learning (TAP-CL) toenhance the association between the position promptsand the referring sentences with only box supervisions,including Text-Contrastive Prompt Learning (TextCon) andModality-Contrastive Prompt Learning (ModalCon) atframe level and video level, respectively.With the pro-posed TAP-CL, our GroPrompt framework can generatetemporal-consistent yet text-aware position prompts de-scribing locations and movements for the referred objectfrom the video. With no need of additional finetuning forfoundation segmentation models, we are able to produceprecise masks for the referred object in the video.Theexperimental results in the standard RVOS benchmarks(Ref-YouTube-VOS, Ref-DAVIS17, A2D-Sentences, andJHMDB-Sentences) demonstrate the competitive perfor-mance of our proposed GroPrompt framework given onlybounding box weak supervision.",
  "Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin.End-to-end referring video object segmentation with multi-modal transformers. In CVPR, pages 49854995, 2022. 5,6": "Sergi Caelles, Alberto Montes, Kevis-Kokitsi Maninis,Yuhua Chen, Luc Van Gool, Federico Perazzi, and JordiPont-Tuset. The 2018 davis challenge on video object seg-mentation. arXiv preprint arXiv:1803.00557, 2018. 5 Nicolas Carion, Francisco Massa, Gabriel Synnaeve, NicolasUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European confer-ence on computer vision, pages 213229. Springer, 2020. 3 Jiajun Chen, Jiacheng Lin, Zhiqiang Xiao, Haolong Fu,Ke Nai, Kailun Yang, and Zhiyong Li.Epcformer:expression prompt collaboration transformer for univer-sal referring video object segmentation.arXiv preprintarXiv:2308.04162, 2023. 5, 6 Keyan Chen, Chenyang Liu, Hao Chen, Haotian Zhang,Wenyuan Li, Zhengxia Zou, and Zhenwei Shi. Rsprompter:Learning to prompt for remote sensing instance segmenta-tion based on visual foundation model, 2023. 2 Tianrun Chen, Lanyun Zhu, Chaotao Ding, Runlong Cao,Shangzhan Zhang, Yan Wang, Zejian Li, Lingyun Sun, PapaMao, and Ying Zang. Sam fails to segment anything? sam-adapter: Adapting sam in underperformed scenes: Camou-flage, shadow, and more, 2023. 2 Weidong Chen, Dexiang Hong, Yuankai Qi, Zhenjun Han,Shuhui Wang, Laiyun Qing, Qingming Huang, and GuorongLi. Multi-attention network for compressed video referringobject segmentation. In Proceedings of the 30th ACM In-ternational Conference on Multimedia, pages 44164425,2022. 5 Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexan-der Schwing, and Joon-Young Lee.Tracking anythingwith decoupled video segmentation. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 13161326, 2023. 5, 8 Junlong Cheng, Jin Ye, Zhongying Deng, Jianpin Chen,Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, JilongChen, Lei Jiangand Hui Sun, Junjun He, Shaoting Zhang,Min Zhu, and Yu Qiao. Sam-med2d, 2023. 2",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova.Bert:Pre-training of deep bidirectionaltransformers for language understanding.arXiv preprintarXiv:1810.04805, 2018. 6": "Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang.Vlt: Vision-language transformer and query generation forreferring segmentation. IEEE Transactions on Pattern Anal-ysis and Machine Intelligence, 2022. 5 Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, SijiaYu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.Lasot: A high-quality benchmark for large-scale single ob-ject tracking. In Proceedings of the IEEE/CVF conference on",
  "Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, and Cees GMSnoek. Actor and action video segmentation from a sentence.In CVPR, 2018. 5, 6": "Mingfei Han, Yali Wang, Zhihui Li, Lina Yao, XiaojunChang, and Yu Qiao.Html: Hybrid temporal-scale mul-timodal learning framework for referring video object seg-mentation. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 1341413423, 2023.1, 2, 3, 5, 6 Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang,Pinxue Guo, Zhaoyu Chen, and Wenqiang Zhang.Lvos:A benchmark for long-term video object segmentation. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 1348013492, 2023. 5 Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: Alarge high-diversity benchmark for generic object tracking inthe wild. IEEE transactions on pattern analysis and machineintelligence, 43(5):15621577, 2019. 5 Hueihan Jhuang,Juergen Gall,Silvia Zuffi,CordeliaSchmid, and Michael J Black. Towards understanding ac-tion recognition. In Proceedings of the IEEE internationalconference on computer vision, pages 31923199, 2013. 5",
  "Anna Khoreva, Anna Rohrbach, and Brent Schiele. Videoobject segmentation with referring expressions.In Pro-ceedings of the European Conference on Computer Vision(ECCV) Workshops, pages 00, 2018. 5, 6": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-thing. arXiv preprint arXiv:2304.02643, 2023. 1, 2, 3 Guanghui Li, Mingqi Gao, Heng Liu, Xiantong Zhen, andFeng Zheng.Learning cross-modal affinity for referringvideo object segmentation targeting limited samples.InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 26842693, 2023. 1 Xiang Li, Jinglu Wang, Xiaohao Xu, Xiao Li, Bhiksha Raj,and Yan Lu. Robust referring video object segmentation withcyclic structural consensus. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 2223622245, 2023. 5",
  "Yonglin Li, Jing Zhang, Xiao Teng, and Long Lan.Refsam: Efficiently adapting segmenting anything modelfor referring video object segmentation.arXiv preprintarXiv:2307.00997, 2023. 1, 3, 5": "Chen Liang, Wenguan Wang, Tianfei Zhou, Jiaxu Miao,Yawei Luo, and Yi Yang.Local-global context awaretransformer for language-guided video segmentation. IEEETransactions on Pattern Analysis and Machine Intelligence,2023. 5, 6 Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, HaoZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, JunZhu, et al. Grounding dino: Marrying dino with groundedpre-training for open-set object detection.arXiv preprintarXiv:2303.05499, 2023. 1, 3, 5, 6, 8",
  "Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and BoWang. Segment anything in medical images. arXiv preprintarXiv:2304.12306, 2023. 2": "Junhua Mao, Jonathan Huang, Alexander Toshev, OanaCamburu, Alan L Yuille, and Kevin Murphy.Generationand comprehension of unambiguous object descriptions. InProceedings of the IEEE conference on computer vision andpattern recognition, pages 1120, 2016. 5 Bo Miao, Mohammed Bennamoun, Yongsheng Gao, andAjmal Mian.Spectrum-guided multi-granularity referringvideo object segmentation. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 920930, 2023. 1, 2, 3, 5, 6 Matthias Muller, Adel Bibi, Silvio Giancola, Salman Al-subaihi, and Bernard Ghanem. Trackingnet: A large-scaledataset and benchmark for object tracking in the wild. InProceedings of the European conference on computer vision(ECCV), pages 300317, 2018. 5 Wenwen Pan, Haonan Shi, Zhou Zhao, Jieming Zhu, Xi-uqiang He, Zhigeng Pan, Lianli Gao, Jun Yu, Fei Wu, andQi Tian. Wnet: Audio-guided video object segmentation viawavelet-based cross-modal denoising networks. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 13201331, 2022. 5 Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, LucVan Gool, Markus Gross, and Alexander Sorkine-Hornung.A benchmark dataset and evaluation methodology for videoobject segmentation. In Proceedings of the IEEE conferenceon computer vision and pattern recognition, pages 724732,2016. 5, 8 Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu,Xiang Bai, Serge Belongie, Alan Yuille, Philip HS Torr, andSong Bai. Occluded video instance segmentation: A bench-mark.International Journal of Computer Vision, 130(8):20222039, 2022. 5, 8",
  "Frano Rajic, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Mar-tin Danelljan, and Fisher Yu. Segment anything meets pointtracking. arXiv:2307.01197, 2023. 2": "Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, AmirSadeghian, Ian Reid, and Silvio Savarese. Generalized in-tersection over union: A metric and a loss for boundingbox regression. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 658666,2019. 3 Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos:Unified referring video object segmentation network with alarge-scale benchmark.In Computer VisionECCV 2020:16th European Conference, Glasgow, UK, August 2328,2020, Proceedings, Part XV 16, pages 208223. Springer,2020. 1, 5, 6 Jiajin Tang, Ge Zheng, and Sibei Yang. Temporal collectionand distribution for referring video object segmentation. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 1546615476, 2023. 1, 2, 3, 5, 6 Di Wang, Jing Zhang, Bo Du, Minqiang Xu, Lin Liu,Dacheng Tao, and Liangpei Zhang.SAMRS: Scaling-upremote sensing segmentation dataset with segment anythingmodel. In Thirty-seventh Conference on Neural InformationProcessing Systems Datasets and Benchmarks Track, 2023.2 Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang,Chunhua Shen, and Tiejun Huang. Seggpt: Towards seg-menting everything in context.In Proceedings of theIEEE/CVF International Conference on Computer Vision,2023. 1, 3 Dongming Wu, Tiancai Wang, Yuang Zhang, XiangyuZhang, and Jianbing Shen.Onlinerefer: A simple onlinebaseline for referring video object segmentation. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision, pages 27612770, 2023. 1, 2, 3, 5, 6, 8 Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and PingLuo.Language as queries for referring video object seg-mentation.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 49744984, 2022. 1, 2, 3, 5, 6, 8 Junde Wu, Rao Fu, Huihui Fang, Yuanpei Liu, ZhaoweiWang, Yanwu Xu, Yueming Jin, and Tal Arbel. Medical samadapter: Adapting segment anything model for medical im-age segmentation. arXiv preprint arXiv:2304.12620, 2023.2 Jiannan Wu, Yi Jiang, Bin Yan, Huchuan Lu, Zehuan Yuan,and Ping Luo. Segment every reference object in spatial andtemporal spaces. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 25382550,2023. 1, 2, 3, 5, 6 Chenliang Xu, Shao-Hang Hsieh, Caiming Xiong, and Ja-son J Corso. Can humans fly? action understanding withmultiple classes of actors. In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages22642273, 2015. 5",
  "Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, YuchenLiang, Jianchao Yang, and Thomas Huang.Youtube-vos:A large-scale video object segmentation benchmark. arXivpreprint arXiv:1809.03327, 2018. 5, 8": "Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Ze-huan Yuan, and Huchuan Lu.Universal instance percep-tion as object discovery and retrieval.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1532515336, 2023. 5 Shilin Yan, Renrui Zhang, Ziyu Guo, Wenchao Chen, WeiZhang, Hongyang Li, Yu Qiao, Zhongjiang He, and PengGao.Referred by multi-modality:A unified temporaltransformer for video object segmentation. arXiv preprintarXiv:2305.16318, 2023. 5, 6",
  "Linwei Ye, Mrigank Rochan, Zhi Liu, Xiaoqin Zhang, andYang Wang. Referring segmentation in images and videoswith cross-modal self-attention network. TPAMI, 2021. 6": "Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, YingyingChen, Fangchen Liu, Vashisht Madhavan, and Trevor Dar-rell. Bdd100k: A diverse driving dataset for heterogeneousmultitask learning. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages26362645, 2020. 5 Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,and Tamara L Berg. Modeling context in referring expres-sions.In Computer VisionECCV 2016: 14th EuropeanConference, Amsterdam, The Netherlands, October 11-14,2016, Proceedings, Part II 14, pages 6985. Springer, 2016.5"
}