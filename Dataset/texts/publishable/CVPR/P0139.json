{
  "Zhejiang University2University of Illinois Urbana-ChampaignEqual {junkun3,yxw}@illinois.edu": ". Our Instruct 4D-to-4D edits 4D scenes as pseudo-3D scenes with 2D diffusion, achieving much sharper results with detailedtextures across a variety of editing tasks and scenes. Notably, Instruct 4D-to-4D generates realistic and 4D consistent editing results in bothmonocular scenes and challenging multi-camera indoor scenes. Please refer to the supplementary video for additional visualization.",
  "Abstract": "This paper proposes Instruct 4D-to-4D that achieves 4Dawareness and spatial-temporal consistency for 2D diffu-sion models to generate high-quality instruction-guided dy-namic scene editing results. Traditional applications of 2Ddiffusion models in dynamic scene editing often result in in-consistency, primarily due to their inherent frame-by-frameediting methodology.Addressing the complexities of ex-tending instruction-guided editing to 4D, our key insightis to treat a 4D scene as a pseudo-3D scene, decoupledinto two sub-problems: achieving temporal consistency invideo editing and applying these edits to the pseudo-3Dscene. Following this, we first enhance the Instruct-Pix2Pix(IP2P) model with an anchor-aware attention module forbatch processing and consistent editing. Additionally, weintegrate optical flow-guided appearance propagation ina sliding window fashion for more precise frame-to-frameediting and incorporate depth-based projection to managethe extensive data of pseudo-3D scenes, followed by itera-tive editing to achieve convergence. We extensively evalu-ate our approach in various scenes and editing instructions,and demonstrate that it achieves spatially and temporallyconsistent editing results, with significantly enhanced detailand sharpness over the prior art. Notably, Instruct 4D-to-4D is general and applicable to both monocular and chal-lenging multi-camera scenes. Code and more results areavailable at immortalco.github.io/Instruct-4D-to-4D.",
  ". Introduction": "Being able to synthesize photo-realistic novel-view imagesthrough rendering, neural radiance field (NeRF) andits variants have become the leading neural representationfor 3D and even 4D dynamic scenes. Moving beyond themere representation of existing scenes, there is a grow-ing interest in creating new, varied scenes sourced from anoriginal scene via scene editing. The most convenient andstraightforward way for users to communicate scene edit-ing operations is through natural language a task knownas instruction-guided editing. Success in this task for 2D images has been achieved bya 2D diffusion model, namely Instruct-Pix2Pix (IP2P) .However, extending this capability to NeRF-represented 3Dor 4D scenes poses a significant challenge. The inherentdifficulty arises from the implicit nature of the NeRF rep-resentation, which lacks direct ways to modify the parame-ters in a targeted direction, along with the significantly in-creased complexity emerging in new dimensions. Recently,there has been noticeable progress in instruction-guided3D scene editing, as exemplified by Instruct-NeRF2NeRF(IN2N) . IN2N achieves 3D editing through distillationfrom 2D diffusion models such as IP2P to edit NeRF, i.e.,generating edited multi-view images from IP2P and fittingthem on the NeRF-represented scenes. Due to the high di-versity in generation results of diffusion models, IP2P mayproduce multi-view inconsistent images, with the same ob-",
  "arXiv:2406.09402v1 [cs.CV] 13 Jun 2024": "ject having different appearances in different views. There-fore, IN2N consolidates the results by training on NeRF tomake it converge to an average editing result, which isreasonable but often encounters challenges in practice.Further extending the editing task from 3D to 4D, how-ever, introduces fundamental difficulties.With the addi-tional time dimension beyond 3D scenes, it requires notonly 3D spatial consistency for the 3D scene slice at eachframe, but also the temporal consistency between differentframes. Notably, as recent 4D NeRFs model theproperty of each absolute 3D location in the scene insteadof the movement of individual object, the same object in dif-ferent frames is not modeled by the same parameter. Thisdeviation prevents NeRF from achieving spatial consistencyby fitting inconsistent multi-view images, making the IN2Npipeline unable to effectively perform editing on 4D scenes.This paper introduces Instruct 4D-to-4D, making the firstattempt in instruction-guided 4D scene editing that over-comes the aforementioned issues. Our key insight is to re-gard a 4D scene as a pseudo-3D scene, where each pseudo-view is a video consisting of all frames from the same view-point. Subsequently, the task on the pseudo-3D scene canbe tackled in a similar way as real 3D scenes, decoupledinto two sub-problems: 1) achieve temporal-consistent edit-ing for each pseudo-view, and 2) use the method from (1)to edit the pseudo-3D scene. Then, we can solve (1) with avideo editing method, and leverage a distillation-guided 3Dscene editing method to solve (2).Specifically, we utilize an anchor-aware attention mod-ule to augment the IP2P model, inspired by . The an-chor in our module is a pair of an image and its editing re-sult as a reference for the IP2P generation. The augmentedIP2P now supports batched input of multiple images, andthe self-attention module in the IP2P pipeline is substitutedwith a cross-attention mechanism against the anchor imageof this batch. Consequently, IP2P generates editing resultsbased on the correlation between the current image and theanchor image, ensuring consistent editing within this batch.However, the attention module may not always correctly as-sociate objects in different views, introducing potential in-consistency.To this end, we further propose an optical flow-guidedsliding window method to facilitate video editing. Leverag-ing RAFT , we predict optical flow for each frame to es-tablish pixel correspondence between two adjacent frames.This enables us to propagate editing results from one frameto the next, similar to a warping effect. With the augmentedIP2P and optical flow, we can edit the video in temporal or-der, by segmenting frames and then applying editing to eachsegment while propagating the editing to the next segment.The process involves utilizing optical flow to initialize edit-ing based on previous frames and subsequently applying theaugmented IP2P with the last frame of the preceding seg- ment serving as the anchor.As a 4D scene contains a large number of frames at eachview, it becomes time-consuming to compute all the views.To address this, we adopt a strategy inspired by ViCA-NeRF to edit pseudo-3D scenes based on key views.We first randomly select key pseudo-views and edit themusing the aforementioned method. Then for each frame, weemploy depth-based projection to warp the results from thekey views to other views, and utilize weighted average toaggregate the appearance information, obtaining the edit-ing results for all the frames. Given the complexity of 4Dscenes, we apply the iterative editing procedure of IN2N toiteratively generate edited frames and fit the NeRF on theedited frames, until the scene converges.We conduct extensive experiments on both monocularand multi-camera dynamic scenes to validate the effective-ness of our approach. The evaluation shows the remarkablecapabilities of our approach in both achieving sharper ren-dering results with significantly enhanced detail and ensur-ing spatial-temporal consistency in 4D editing ().Our contribution is three-fold. (1) We introduce In-struct 4D-to-4D, a simple yet effective framework to per-form instruction-guided 4D editing, by editing 4D scenes aspseudo-3D scenes via distillation from 2D diffusion mod-els. (2) We propose the anchor-aware IP2P and the opti-cal flow-guided sliding window method, enabling efficientand consistent editing of long videos or pseudo-views ofany length. (3) With the proposed method, we develop apipeline to iteratively generate fully and consistently editeddatasets, achieving high-quality 4D scene editing in vari-ous tasks. Our work represents the first effort to investigateand address the general instruction-guided 4D scene edit-ing, laying the foundation for this promising task.",
  ". Related Work": "Diffusion-Based Video Editing.The diffusion-basedgenerative models have achieved remarkable success intext-based image editing . How-ever, extending these models to video editing introducesgreater complexity, necessitating the manipulation of visualattributes while maintaining temporal consistency. A preva-lent approach in video editing using diffusion models is thetransformation of Text-to-Image (T2I) models into Text-to-Video (T2V) models. Tune-A-Video incorporates tem-poral self-attention layers into UNet and performs the one-shot tuning. Make-A-Video and MagicVideo aug-ment their networks by introducing the spatio-temporal at-tention (ST-Attn) mechanism, enabling the seamless tran-sition of a pre-trained Text-to-Image model to the tempo-ral dimension. Further, there is a growing focus on local-ized editing through the manipulation of attention maps in-spired by Prompt-to-Prompt and Plug-and-Play .Video-P2P introduces decoupled-guidance attention . Our Instruct 4D-to-4D edits a 4D scene by regarding it as a pseudo-3D scene with multiple pseudo-views, and then editing thesepseudo-views in an iterative key frame-based pipeline. (a) Our pipeline edits the 4D scene by iteratively generating a fully edited datasetused to fit 4D NeRF. In each iteration, we first (b) edit each key pseudo-view through optical flow propagation and IP2P inpainting andrepainting, and then (c) edit other pseudo-views by aggregating propagated results from both previous frames through optical flow, and thekey pseudo-views at current frame through depth-based warping. control to preserve the semantic consistency. Pix2Video utilizes the self-attention feature injection to propagatemodifications made in the anchor frame to other frames.Fatezero fuses self-attentions with a blending maskextracted from cross-attention features to achieve the zero-shot shape-aware editing.Diffusion-Based NeRF Editing.Diffusion-based NeRFediting has been gaining increasing attention in recenttimes. Some works leverage powerful SD as 2D prior tomodifying the appearance of scenes, producing impressiveresults. Instruct 3D-to-3D and Instruct-NeRF2NeRF(IN2N) employ Instruct-Pix2Pix (IP2P) , an image-conditioned diffusion model, to enable instruction-based2D image editing.Specifically, Instruct 3D-to-3D usesscore distillation sampling (SDS) loss to edit 3DNeRFs using the 2D diffusion-prior. Meanwhile, Instruct-NeRF2NeRF proposes Iterative Dataset Update (IterativeDU) to alternate between editing the images rendered fromNeRF using the diffusion model and updating the NeRFrepresentation with the supervision of edited images dur-ing the training process.ViCA-NeRF follows IN2Nand utilizes the depth information derived from the NeRFto propagate the modification in key views to other views,achieving spatial consistency. DreamEditor leveragesDreamBooth as 2D prior and utilizes the SDS loss tooptimize the meshed-based neural field, performing faith-ful editing to the text. Control4D proposes to builda more continuous 4D space by learning a 4D GAN from the ControlNet to avoid inconsistent supervisionsignals for 4D portrait editing.NeRF-Based Dynamic Scene RepresentationThe fieldof representing dynamic scenes using Neural RadianceFields (NeRFs) has seen significant advancements,which are essential for various real-world applications. Var-ious methods have been developed to extend the capabili-ties of NeRFs in capturing and rendering dynamic scenes. DNeRF and Nerfies employ individual MLPs torepresent a deformation field and a canonical field for cap-turing complex scene changes over time. DyNeRF in-tegrates time-conditioning into NeRFs using a set of com-pact latent codes. TiNeuVox employs an explicit voxelgrid to model temporal information. Additionally, Neural-Body and focus on acquiring precise dy-namic human body motion information, building upon theSMPL model. HexPlane and K-Planes proposea planar factorization to decompose 4D spatiotemporal vol-umes into six feature planes. NeRFPlayer decomposesthe 4D space into static, deforming, and new areas based ontheir temporal characteristics. Despite these advancements,a common limitation across these methods is the lack ofuser-friendly editing capabilities for dynamic scenes. Usersare currently unable to freely edit or modify these scenes,particularly in terms of following specific instructions. Thislimitation highlights an area for potential future researchand development, where user interactivity and editing ca-pabilities could be integrated into the dynamic scene repre-sentation models. Addressing this challenge would signif-icantly enhance the practicality and applicability of NeRF-based dynamic scene representations.",
  ". Method": "We propose Instruct 4D-to-4D, a novel pipeline that edits4D scenes by distilling from Instruct-Pix2Pix (IP2P) ,a powerful 2D diffusion model that supports instruction-guided image editing. The basic idea of our method rootsin ViCA-NeRF , a key view-based editing. By regardingthe 4D scene as a pseudo-3D scene where each pseudo-viewis a video of multiple frames, we apply the key view-basedediting, broken down into two steps: key pseudo-view edit-ing, and propagation from key pseudo-views to other views,as shown in . We propose several key components to",
  ". Anchor-Aware IP2P for Consistent BatchedGeneration": "Batch Generation with Pseudo-3D Convs.The editingprocess for a pseudo-view can be regarded as editing avideo. Therefore, we need to enforce temporal consistencywhen editing each frame. Inspired by previous work onvideo editing , we edit a batch of images togetherin IP2P, and augment the UNet in IP2P to make the gener-ation in consideration of the whole batch. We upgrade its33 2D convolutional layers to 133 3D convolutionallayers by reusing the original parameters of kernels.Anchor-Aware Attention Module.Limited by the GPUmemory, we cannot edit all frames of a pseudo-view all to-gether in one batch, and need to separate the generation intomultiple batches. Therefore, it is crucial to keep the con-sistency between batches. Following the idea of Tune-a-Video , instead of generating the edited result of the newbatch from scratch, we allow the model to reference an an-chor frame shared across all the generation batches, with itsoriginal and edited version, to propagate the editing stylefrom it to the new edited batch. By substituting the self-attention module in the IP2P with the cross-attention modelagainst the anchor frame, we would be able to connect thesame objects between the current image and the anchorimage, and generate the new edited images by mimickingthe anchors style, to perpetuate the consistent editing stylefrom the anchor. Notably, our usage of the anchor-attentionIP2P is different from Tune-a-Video, which queries cross-attention between the anchor frame and the previous frameinstead of the current frame. Our design also further fa-cilitates the inpainting procedure in Sec. 3.2, which alsorequires a focus on the existing part of the current frame.Effectiveness. shows the generation results of dif-ferent versions of IP2P. The original IP2P edits all imagesinconsistently, in different color distributions, even for im-ages within one batch. With anchor-aware attention layers,IP2P is able to generate the batch as an entirety and, there-fore, generates consistent editing results within one batch.However, it is still unable to generate consistent imageswithin different batches. With reference to the same anchorimage shared across batches, the full anchor-aware IP2Pis able to generate consistent editing results for all 6 im-ages across 2 batches, showing that even without additionaltraining, the anchor-aware IP2P would be able to achieveconsistent editing results.",
  "Optical Flow as 4D Warping.To enforce the temporalconsistency in a pseudo-view, we need the correspondence": "of the pixels between different frames. 3D scene editingmethods exploits depth-based warping to findthe correspondence of different views, using a deterministicmethod with NeRF-predicted depth and camera parameters.In 4D, however, there are no such explicit ways. Therefore,we use an optical flow estimation network RAFT topredict the optical flow, in a format of 2D motion vector foreach pixel, which can be derived into correspondence pixelin another frame. Using RAFT, we are able to warp betweenadjacent frames, like in 3D. As each pseudo-view is takenat a fixed camera location, optical flow performs well.Sliding Window Method.We follow the idea of videoediting methods to edit a pseudo-view.However, those methods focus only on short videos and ap-ply video editing by editing all the frames in a single batch,making them unable to deal with long videos. Therefore,we propose a novel sliding window (of width B being themaximum allowed batch size) method to exploit the anchor-aware IP2P along with the optical flow. As shown in (a), for the current window containing B images, say framest, t+1, , t+B 1, we first propagate the editing resultsfrom frame t 1 to all these B images one by one with 4Dwarping. For the unmatched pixels, which correspond tothe occluded part in frame t 1, we set their value as theirorigin, obtaining a fused image of the original and warpedediting images.Then, similar to the idea in ViCA-NeRF , we use IP2Pto inpaint and repaint the fused image at each view in thesliding window, by adding noise to the fused image and de-noising using IP2P, so that the generated edited image willfollow a similar pattern on the warped results, while repaint-ing the whole image to make it natural and reasonable. Tomake the style all-consistent over the pseudo-view, we usethe first frame as the anchor shared across all the windows,so that the model will generate images in a consistent stylelike the first view. As camera is fixed for one pseudo-view,there are many common objects between different frames,therefore such a method is very effective in producing con-sistent editing results for the frames in the window.After completing the editing in the current window, wewill advance the window by B frames. Therefore, for apseudo-view of T frames in total, our Instruct 4D-to-4Donly needs to call IP2P for T/B times. By caching the op-tical flow prediction between adjacent frames in one view,we could achieve temporal-consistent pseudo-view editingvery efficiently.",
  ". Pseudo-View Propagation Based on Warping": "Generating First Frame.As we need to propagate theedited pseudo-views to all other views while achieving spa-tial consistency, it is crucial to edit the first frames at all keypseudo-views in a spatially consistent way they are notonly used to start the editing of the current pseudo-view, . Generation results show that our augmented IP2P achieves consistency within a batch via our anchor-aware attention module,and achieves consistency between different batches via the same anchor shared across batches. The white bounding box shows the mostnoticeable part of inconsistency. but also used as the anchor or the reference for all the pro-ceeding generations. Therefore, we first edit one first framein an arbitrary pseudo-key view, then use our anchor-awareIP2P with it as the anchor to generate other first frames. Inthis way, all the first frames are edited in a consistent style,being a good start to editing the key pseudo-views.Propagate from Key Views to Other Views.After edit-ing the key pseudo-views, aligned with ViCA-NeRF , wepropagate their editing results to all other key views. ViCA-NeRF uses depth-based spatial warping to warp an imagefrom another view at the same timestep, while we also pro-pose optical-flow-based temporal warping to warp from theprevious frame at the same view.With these two typesof warping, we can warp the edited images from multiplesources.We propagate for each timestep in the order of time.When we propagate at timestep t, for each frame (v, t) atview v, we obtain its edited version as the weighted aver-age of warped results from two sources: (1) the edited re-sults of the previous frame at the same view, namely frame(v, t 1), using temporal warping; and (2) the edited re-sults of the current frame at one each of the key view, us-ing spatial warping. By propagating the frames for all thetimesteps, we obtain a consistent edited dataset containingall the editing frames. We use such a dataset to train NeRFtowards the edited results.With this propagation method, we would be able to effi-ciently generate a full dataset of consistently edited frameswithin nT/B time-consuming IP2P generations, with n keypseudo-views out of all V pseudo-views, where in our ex-periments n = 5 while V can be more than 20. Such highefficiency makes it possible to deploy an iterative pipelineto update the datasets.",
  ". Overall Editing Pipeline": "IterativeDatasetUpdate.FollowingtheideaofIN2N , we apply iterative dataset replacement onour baseline that iteratively re-generates the full datasetusing the methods in Secs. 3.1,3.2,3.3, and fits our NeRFon it. In each iteration, we first randomly select several pseudo-views as the key views in this generation. We usethe method in Sec. 3.3 to generate spatial-consistent editingresults for the first frames of all these key pseudo-views,then propagate the editing for all pseudo-views using thesliding window method in Sec. 3.2. After obtaining all theedited key pseudo-views, we use the method in Sec. 3.3 togenerate spatial and temporal consistent editing results forall other pseudo-views, ending up with a consistent editeddataset. We replace the 4D dataset with this edited dataset,and fit NeRF on it.Improving Efficiency Through Parallelization and An-nealing Strategies.In our pipeline, the NeRF only needsto be trained on the dataset and provide current renderingresults, while IP2P only needs to generate results accord-ing to NeRFs rendering to form new datasets - there arefew dependencies and interactions between IP2P and NeRF.Therefore, we parallelize our pipeline by running these twoparts asynchronously on two GPUs. In the first GPU, wetrain NeRF continuously with the current dataset, whilecaching NeRFs rendering results in a rendering buffer;while in the second GPU, we apply our iterative dataset-generation pipeline to generate new datasets, using the im-ages from the rendering buffer, and update the dataset usedto train NeRF. In this case, we maximize the parallelizationby minimizing the interactions, leading to a significant re-duction in the training time.On the other hand, to improve the generation resultsand convergence speed, we apply the annealing trick fromHiFA to achieve fine-grained editing on NeRF. Thehigh-level idea is that we use the noise level to control thesimilarity of rendered results and IP2Ps editing results. Wegenerate the dataset at a high noise level to generate suf-ficiently edited results, and then gradually anneal the noiselevel to stick to the edited results that NeRF is converging toand refine such results. Instead of IN2N which always gen-erates at a random noise level, our Instruct 4D-to-4D couldconverge to high-quality editing results at a fast speed.With these two techniques, our Instruct 4D-to-4D is ableto edit a large-scale 4D scene with 20 views and hundredsof frames in only hours. . Qualitative results on various scenes demonstrate that our Instruct 4D-to-4D generates high-qualify editing results in styletransfer tasks on various scenes. The edited scenes are well-consistent with the instructed style, showing bright colors and natural textures.4. Experiments Editing Tasks and NeRF Backbone.The 4D scenes weuse for evaluation are captured by single hand-held camerasand multi-camera arrays including: (I) Monocular Scenesin DyCheck and HyperNeRF , which are simple,object-centric scenes with a single moving camera; and (II)Multi-camera Scenes in DyNeRF/N3DV , including in-door scenes with face-forward perspective and human mo-tion structure. For monocular scenes, we edit all the framesas a single pseudo-view. We use the NeRFPlayer as ourNeRF backbone to produce high-quality rendering resultsof 4D scenes.Baselines.Instruct4D-to-4Disthefirstworkoninstruction-guided 4D scene editing. No previous work fo-cuses on the same task, while the only similar work Con-trol4D has not released their code. Therefore, we can-not conduct any baseline comparison with existing meth-ods. To show the effectiveness of our Instruct 4D-to-4D,we construct a baseline IN2N-4D, by naively extendingIN2N to 4D, which iteratively generates one editedframe and add it to the dataset. We compare our Instruct4D-to-4D with IN2N-4D both qualitatively and quantita-tively. To quantify the results, as both our pipeline and themodel are training NeRF with generated images, we use tra-ditional NeRF metrics to evaluate the results, namelyPSNR, SSIM, and LPIPS, between the IP2P generated im-ages (generating from pure noise so that it will not be condi-",
  "tioned on NeRFs rendering image) and the NeRFs render-ing results. We conduct our ablation studies against Instruct4D-to-4D variants in the supplementary": "Qualitative Results.Our qualitative results are shown inFigs. 6, 5, and 4. The qualitative comparison with baselineIN2N-4D is in Figs. 5 and 6. As shown in , in the taskof changing the cat into a fox in the monocular scene, IN2N-4D generates blur results with multiple artifacts: multipleears, multiple noses and mouths, etc., while our Instruct4D-to-4D generates photo-realistic results where the shapeof the fox is well aligned with the cat in the original scene,with clear textures on the fur and no artifacts. These resultsshow that our anchor-aware IP2P, optical flow-based warp-ing, and sliding window method for pseudo-view editingproduces temporal-consistent editing results for a pseudo-view. Without such a module, the original IP2P in IN2N-4Dproduces inconsistent edited images for each frame, consol-idating to a strange result on the 4D NeRF. shows thestyle transfer results on multi-camera scenes. Our paral-lelized Instruct 4D-to-4D achieves consistent style transferresults that match the description in a very short time periodof two hours, while IN2N-4D takes 24 longer than our In-struct 4D-to-4D but still fails to get the 4D NeRF convergedto the indicated style. This shows that 4D scene editing ishighly non-trivial, while our Instruct 4D-to-4Ds strategyto iteratively generate a full edited dataset facilitates high-efficiency editing. All these results collectively show that all . Qualitative results on mochi-high-five scene in DyCheck dataset show that our Instruct 4D-to-4D achieves high-qualityediting results over various editing instructions in the monocular scene. Our Instruct 4D-to-4D can even achieve consistent editing withcomplicated textures, e.g., in the Tiger editing, while baseline IN2N-4D generates blurred results with lots of artifacts. . Qualitative comparison with baseline IN2N-4D on multi-camera coffee martini shows that our Instruct 4D-to-4D gen-erates high-quality, faithful style transfer editing results in a very short time. As a comparison, IN2N-4D even fails to converge at any stylewith 24 time consumption. our design of Instruct 4D-to-4D is reasonable and effective,and Instruct 4D-to-4D can produce high-quality editing re-sults in a very efficient way.The experiments in show the monocular scenemochi-high-five under different instructions, includ-ing local editing on the cat, or style transfer instructions forthe whole scene. Our Instruct 4D-to-4D achieves photo-realistic local editing results in the Fox and Tiger instruc-tions, with clear and consistent textures e.g., the stripes ofthe tiger. In the style transfer instructions, the edited scenefaithfully reflects the style indicated in the prompts. These show Instruct 4D-to-4Ds great ability in editing monocularscenes under various prompts.The experiments in show other style transfer re-sults, including monocular scenes in HyperNeRF and Dy-Check and multi-camera scenes in DyNeRF. Instruct 4D-to-4D consistently produces high-fidelity style transfer resultswith bright colors and clear appearance in various styles.Quantitative Comparison.The quantitative comparisonbetween our Instruct 4D-to-4D and baseline IN2N-4D onthe multi-camera coffee martini scene is in Tab. 1.Consistent with the qualitative comparison, our Instruct 4D-",
  ".In the quantitative evaluation on the multi-cameracoffee martini scene, our Instruct 4D-to-4D significantlyand consistently outperforms the baseline IN2N-4D in all metrics": "to-4D significantly and consistently outperforms the base-line IN2N-4D. This shows that the NeRF trained by Instruct4D-to-4D fits the IP2Ps editing results much better than thebaseline, further validating the effectiveness of our Instruct4D-to-4D.Ablation Study: Variants and Settings.We validate ourdesign choices by comparing our approach to the followingvariants. Video Editing. This variant serves as the most basic im-plementation of our Instruct 4D-to-4D edit each pseudo-frame with any video editing methods, and propagate toother frames using 3D warping. We use a zero-shot text-driven video editing model, FateZero , via pretrainedstable diffusion models. We follow the settings of the offi-cial Fatezero implementation in style editing and attributeediting.Since they can only process 8 video framesin a batch, we use a batch-by-batch editing strategy forpseudo-view editing. Anchor-Aware IP2P w/o Optical-Flow. In this variant,we perform video editing without optical flow guidance,in which anchor-aware IP2P directly edits all training im-ages using the same diffusion model setting. One-time Pseudo-View Propagation.In this variant,we perform only one-time pseudo-view propagation, inwhich all rest-pseudo-views are warped from 4 randomlyselected key-pseudo-views, and the NeRF is trained untilconvergence on those edited images.The task for ablation study is What if it was painted byVan Gogh on the coffee martini in DyNeRF dataset.As the Video Editing variant does not use the same diffu-sion model, IP2P , to edit the video, we cannot use themetrics in the main paper. Therefore, consistent with IN2N, we use CLIP similarity to evaluate how successfulan editing operation is applied.Ablation Study: Results.The qualitative results are in and the demo video. Most of the variants do not applysufficient editing to the scene, with a gloomy appearanceand no Van Goghs representative color. This shows that ourInstruct 4D-to-4Ds design choices are effective and crucialto achieve high-quality editing.The quantitative comparisons are in Tab. 2. Our full",
  ". Conclusion": "This paper proposes Instruct 4D-to-4D, the first instruction-guided 4D scene editing framework that edits 4D scenes byregarding them as pseudo-3D scenes and applies an itera-tive strategy to edit pseudo-3D scenes using a 2D diffusionmodel. Qualitative experimental results show that Instruct4D-to-4D achieves high-quality editing results in varioustasks, including monocular and multi-camera scenes. In-struct 4D-to-4D also significantly outperforms the baseline,a naive extension of the state-of-the-art 3D editing methodto 4D, showing the difficulty and non-trivialness of the taskand the success of our method. We hope that our work couldinspire more future work on 4D scene editing.",
  "Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and JiayaJia. Video-p2p: Video editing with cross-attention control.arXiv preprint arXiv:2303.04761, 2023. 2, 4": "Matthew Loper, Naureen Mahmood, Javier Romero, GerardPons-Moll, and Michael J Black. Smpl: A skinned multi-person linear model. In Seminal Graphics Papers: Pushingthe Boundaries, Volume 2. ACM, 2023. 3 Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guidedimage synthesis and editing with stochastic differential equa-tions. arXiv preprint arXiv:2108.01073, 2021. 2",
  "Albert Pumarola, Enric Corona, Gerard Pons-Moll, andFrancesc Moreno-Noguer. D-NeRF: Neural radiance fieldsfor dynamic scenes. In CVPR, 2021. 3": "Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-ing attentions for zero-shot text-based video editing. arXivpreprint arXiv:2303.09535, 2023. 3, 8 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-vision. In ICML, 2021. 8",
  "2d diffusion-based editor. arXiv preprint arXiv:2305.20082,2023. 3, 6": "Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,Oran Gafni, et al. Make-a-video: Text-to-video generationwithout text-video data. arXiv preprint arXiv:2209.14792,2022. 2 Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, LeleChen, Junsong Yuan, Yi Xu, and Andreas Geiger. NeRF-Player: A streamable dynamic scene representation with de-composed neural radiance fields. TVCG, 2023. 2, 3, 6, 11 Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li,Brent Yi, Terrance Wang, Alexander Kristoffersen, JakeAustin, Kamyar Salahi, Abhik Ahuja, et al. Nerfstudio: Amodular framework for neural radiance field development.In SIGGRAPH, 2023. 11",
  "B.1. 4D Representation": "The primary input of our method is a 4D NeRF represen-tation, acquired through the NeRFPlayer . Our frame-work is general, and therefore, any 4D scene representationadopting RGB observations as supervision can be used. Inthe implementation of the multi-camera scenes, we use Ten-soRF -based NeRFPlayer as NeRF backbone and fol-low the same setting as in their experiments on the multi-camera DyNeRF scenes. For our experiments, we ex-tract 50 frame segments, from the full-length videos anddownsample images to 1352 1014 for 4D representa-tion and editing. Furthermore, to show the capabilities ofour method in long-term videos(pseudo scenes), we alsouse full-length 300-frame videos with 676 507 resolu-tion for additional evaluation, both trained for 100,000 it-erations per scene. In the implementation of the monoc-ular scenes, we use InstantNGP -based NeRFPlayerfor HyperNeRF dataset, trained for 60, 000 iterationsper scene, and Nerfacto-based NeRFPlayer from NeRFStu-dio for DyCheck dataset, trained for 30, 000 itera-tions per scene.",
  "Q = W Qzvi, K = W K [zva; zvi] , V = W V [zva; zvi]": "where W are projection layers in attention shared acrossspace and time, and [] denotes concatenation operation. Be-sides, we employ the original spatial self-attention weightsas initialization. In each pseudo-view editing process, weuse the first frame which is edited as the anchor frame toprovide appearance reference.Fig. B.3-(a) shows that anchor-aware attention clearlyimproves the editing consistency across views and batches. Figure B.3. (a) The usage of anchor-aware attention clearly im-proves the editing consistency across views and batches. (b) Amore detailed visualization of our sliding window pseudo-viewediting method through an optical flow-based propagation andanchor-aware inpainting and repainting.",
  "B.3. Sliding Window-Based Pseudo-View EditingMethod": "As visualized in Fig. B.3-(b), we filter inferior flow predic-tions by leveraging the forward-backward consistency con-straint. Also, the anchor-aware IP2P is well designed bothto inpaint occluded areas and to repaint the whole part basedon the appearance of the anchor frame. This leads to reason-able editing results even when the RAFT prediction is notaccurate.",
  "B.4. 4D Editing Procedure": "We re-initialize the optimizers in the trained NeRFPlayermodel and utilize Anchor-Aware Instruct-Pix2Pix as our2D editing model. For the diffusion model, varying hyper-parameters are applied at distinct phases. During the an-chor frames editing stage, the input diffusion timestep t de-cays from 0.98 to 0.7 in a cosine annealing manner. Weemploy 20 diffusion steps for this phase. During the in-painting stage after optical flow warping, we set timestept to 0.6, and only used 3 diffusion steps. As optical flowwarping extensively propagates across most areas, the di-minished timestep and fewer diffusion steps contribute toefficient inpainting while preserving the original data dis-tribution. To control the extent of alterations for specificedits, we calibrate the classifier-free guidance weights foreach scene, defined as ST and SI for the text instructionand original image, respectively. For object-focused edit-ing tasks, We set SI= 1.5 and ST = 7.5, whereas, for styletransfer tasks, the settings are SI= 1.5 and ST = 9.5. Thenumber of iterations varies in different scenes shown in thepaper. Due to the parallelization strategy, we dont needto trade off between NeRF training and image editing, thuswe use 15,000 iterations for monocular scenes and 25,000",
  "C.1. Limitations": "The major limitation of our Instruct 4D-to-4D is rooted inthe limitation of IP2P given that Instruct 4D-to-4Dedits scenes by distilling from IP2P, its editing capability iscapped by IP2P. We will fail in the failure cases of IP2P, andperform poorly if IP2P does so. In addition, as we are usingthe original IP2P without fine-tuning, we lose the ability toleverage the per-scene information to facilitate editing. Onthe other hand, we benefit from the high efficiency of sucha training-free pipeline.Moreover, without input of 3D geometry information or4D movement information, IP2P is unaware of any 3D/4Dinformation, including position, geometry, and timestep. Itcan only infer the correlation between frames using cross-attention modules based on the RGB images, which mightbe inaccurate and lead to inconsistent editing results. Notethat the source of consistency in Instruct 4D-to-4D is pri-marily the cross-attention module, which is a soft mech-anism without supervision or enforcement. While shows that our IP2P can generate consistent editing resultsunder certain situations, this is not always guaranteed.Some instructions may indicate shape editing. Instruct4D-to-4D could only perform simple shape editing wherethe modification is near the surface, e.g., change the cat to afox which slightly changes the head shape. Instruct 4D-to-4D does not support aggressive shape editing, e.g., removethe cat, like most of the instruction-guided 3D scene editingmethods, or editing the movement of an object."
}