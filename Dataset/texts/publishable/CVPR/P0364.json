{
  "Abstract": "Although humans engaged in face-to-face conversa-tion simultaneously communicate both verbally and non-verbally, methods for joint and unified synthesis of speechaudio and co-speech 3D gesture motion from text are anew and emerging field.These technologies hold greatpromise for more human-like, efficient, expressive, and ro-bust synthetic communication, but are currently held backby the lack of suitably large datasets, as existing methodsare trained on parallel data from all constituent modalities.Inspired by student-teacher methods, we propose a straight-forward solution to the data shortage, by simply synthesis-ing additional training material. Specifically, we use uni-modal synthesis models trained on large datasets to createmultimodal (but synthetic) parallel training data, and thenpre-train a joint synthesis model on that material. In ad-dition, we propose a new synthesis architecture that addsbetter and more controllable prosody modelling to the state-of-the-art method in the field. Our results confirm that pre-training on large amounts of synthetic data improves thequality of both the speech and the motion synthesised by themultimodal model, with the proposed architecture yieldingfurther benefits when pre-trained on the synthetic data.",
  ". Introduction": "Human beings are embodied, and we use a wide gamut ofthe expressions afforded by our bodies to communicate. Inconcert with the lexical and non-lexical (prosodic) compo-nents of speech, humans also leverage gestures realised byface, head, arm, finger, and body motion all driven by ashared, underlying communicative intent to improveface-to-face communication .Research into automatically recreating different kinds ofhuman communicative behaviour, whether it be speech au-dio from text , or gesture motion from speech , Text-to-speechSpeech-to-motion",
  ". MAGI: Multimodal Audio and Gesture, Integrated": "have a long history, as these are key enabling technologiesfor, e.g., virtual agents, game characters, and social robots. The advent of deep learning has led to anexplosion of research in the two fields . Gesturesynthesis, in particular, has been shown to benefit from ac-cess to both lexical and acoustic representations of speech. That said, joint and simultaneous synthe-sis of both speech and gesture communication (pioneered in) remains severely under-explored. This despite the factthat simultaneously generating both modalities together notonly better emulates how humans produce communicativeexpressions, but also offers a stepping stone towards creat-ing non-redundant gestures that can complement and evenreplace speech, like human gestures do . On top of this,recent research efforts towards integrating the synthesis ofthe two modalities have demonstrated improvements in co-herent , compact , jointly and rapidly learn-able , convincing , and cross-modally appropri-ate synthesis of speech and 3D gestures from text.The current state of the art in joint multimodal speech-and-gesture synthesis, Match-TTSG , achieves strongperformance via modern techniques such as conditionalflow matching (OT-CFM) with U-Net Transformer encoders . However, there still remains a noticeable gapbetween synthesised model output and recordings of nat-ural human speech and gesticulation . This contrastswith recent breakthroughs in generative AI, which cansynthesise text , images , and speech au-",
  "arXiv:2404.19622v1 [cs.HC] 30 Apr 2024": "dio that all are nigh indistinguishable from thosecreated by humans. The critical difference is that whereasthose strong models for synthesising single modalities ben-efit from training on vast amounts of data (cf. ), exist-ing parallel datasets of speech audio, text transcriptions, andhuman motion are radically smaller. This is especially trueif we require good motion quality (which at present gener-ally necessitates high-end 3D motion capture) and speechaudio with a spontaneous character and quality suitable forspeech synthesis. The state-of-the-art joint synthesis systemdemonstrated in was thus trained on 4.5 hours of paral-lel speech and gesture data from ; larger parallel corporaexist , but exhibit some quality issues (cf. ) anddo not exceed 100 hours, a far cry from the corpora usedto train leading generative AI systems. It stands to reasonthat multimodal synthesis systems could gain substantiallyfrom overcoming the limitations imposed by training onlyon presently available parallel corpora.In this paper, we propose two improvements to the state-of-the art multimodal speech-and-gesture synthesis:1. We pre-train1 a joint speech-and-gesture synthesis modelon a large parallel corpus of synthetic training data cre-ated using leading text, text-to-speech, and speech-to-gesture systems (), before fine-tuning on our targetdata. This offers a simple way for multimodal models tobenefit from advances in unimodal synthesis systems. 2. We extend with a probabilistic duration model (sim-ilar to ) and individual models of pitch and energy(similar to ). This enables more lifelike and morecontrollable synthetic expression.The resulting joint synthesis system is orders of magnitudesmaller and faster than the models used for synthesising thepre-training data. Our subjective evaluations show that theproposed pre-training on synthetic data improves the speechas well as the gestures created by a joint synthesis system,and that the architectural modifications further benefit a sys-tem pre-trained on large synthetic data and also enable out-put control. For video examples and code, please see ourdemo page at shivammehta25.github.io/MAGI/;",
  "We use pre-training to refer to any training (by others or by us)performed prior to training on our multimodal target dataset": "based LLMs using Generative Pre-trained Transform-ers (GPTs) like are capable of generatingtext virtually indistinguishable from that written by humans.The critical methodological advances for LLMs are pre-training on vast amounts of diverse data, coupled with fine-tuning on a small amount of high-quality, in-domain mate-rial, e.g., via Reinforcement Learning from Human Feed-back (RLHF) . This methodology of pre-training foun-dation models followed by fine-tuning on the best datahas been validated to give excellent results across severalmodalities . In this paper, we for the first time usethat methodology in joint speech-and-gesture synthesis.Fine-tuned LLMs allow generating of diverse text sam-ples for many domains through prompting the model, i.e.,providing a written text prompt at runtime describing theoutput to generate. Prompting has been useful for manytasks including creating synthetic dialogue datasets andselecting appropriate gestures based on verbal utterances. We use this ability to create an arbitrarily large ma-terial of conversational text sentences in the style of a givenspeaker/corpus as a basis for our synthetic-data creation.",
  ". Speech synthesis": "Recent advances in deep generative modelling have signif-icantly improved text-to-speech (TTS) , reaching nat-uralness levels that rival recorded human speech .TTS models are often divided into two broad classes: au-toregressive (AR) and non-autoregressive (NAR). AR mod-els produce acoustic outputs sequentially, using mecha-nisms such as neural cross-attention or neural transducers to connect inputs sym-bols to the outputs. Non-autoregressive models instead generate the entire utterancein parallel. NAR models are typically faster, especially onGPUs, but AR methods often yield slightly better synthesis.Recently, there has been a trend to quantise audio waveforms into discrete tokens ,and then adapt an LLM-like autoregressive approach (e.g.,with GPTs) to learn to model these audio tokens on largedatasets. Synthesised token sequences can subsequently beconverted back to audio . Speaker and style adapta-tion can be achieved by seeding (prompting) the model withan audio snippet, something we leverage to create diversestochastic synthetic training data for our work.LLM-like TTS can give exceptional results when trainedon large datasets, but models risk confabulating (similar towell-known issues with LLMs) and getting trapped in feed-back loops due to the autoregression . Our papertherefore describes a pipeline for mitigating these problemswhen creating synthetic training data at scale.In NAR TTS, it has been found that conditioning the TTSon the output of a model of prosodic properties, e.g., per-phone pitch and energy, can benefit synthesis . This also enables control over the speech, by replacingor manipulating the prosodic features prior to synthesis.Speech-sound durations are especially important for con-vincing prosody, and probabilistic modelling of durationscan substantially improve deep generative TTS .This appears especially useful for speech uttered spon-taneously in conversation, as considered here, due to itshighly diverse prosodic structure . We therefore intro-duce a probabilistic duration model, coupled with explicitpitch and energy models, into the multimodal synthesis ar-chitecture.Better duration modelling should help createspeech rhythm and timings with adequate time for gesture-preparation phases, so that gesture strokes can synchronisewith the speech. Improved control should not only affectthe output speech but also the gestures we generate with it.",
  ". Gesture synthesis": "Like TTS, deep learning has led to a boom in 3D ges-ture synthesis from speech text and/or audio , adaptingtechniques like GANs , normalising flows ,VAEs , VQ-VAEs , combined adversariallearning and regression losses , and flow-VAEs. Following text-prompted diffusion models for humanmotion diffusion models have seen rapid adop-tion for generating 3D gesture motion . Flowmatching improves synthesis speed by learning mod-els that require fewer diffusion steps during sampling, andhas recently been adapted to motion generation andTTS . Similar to LLMs and large TTS models,separate efforts wholly or partly model gestures autoregres-sively as a sequence of discrete tokens .The most recent large-scale comparison of gesture-generation models, the GENEA Challenge 2023 , foundthat the two strongest methods (which are exten-sions of ) were based on diffusion models. Amongthese, made use of self-supervised text-and speech em-beddings from data2vec , subsequently aligned with ges-ture motion using CLIP training, to improve the coher-ence between gestures and the two speech-input modalities.In addition to modelling beat gestures, the approach recog-nises the need for additional input modalities to generaterepresentational gestures, such as iconic and deictic point-ing , for more nuanced and contextually relevant non-verbal communication. Our data-synthesis pipeline lever-ages their approach to create synthetic training gestures thatwell match the synthetic speech text and audio input.",
  ". Joint synthesis of speech and gestures": "Speech synthesis and gesture generation have traditionallybeen treated as separate problems, performed on differentdata by distinct research communities. TTS is mainly devel-oped for read-aloud speech, whereas co-speech gesturing ismore closely associated with conversational settings. Joint synthesis of speech and motion was first consid-ered by . The first neural model was DurIAN ,which simultaneously generated speech audio and 3D fa-cial expressions, albeit for speech read aloud. trainedseparate deep-learning TTS and speech-to-gesture systemsto synthesise speech and 3D motion for the same speakerand the same (spontaneous) speaking style. This was fol-lowed by , which investigated adapting and extendingAR and NAR neural TTS models to perform jointmultimodal synthesis. Their joint models reduced the num-ber of parameters needed over , but the best model (theone based on ) required complex multi-stage training tospeak intelligibly and did not improve quality.Diff-TTSG advanced joint speech-and-gesture syn-thesis by employing probabilistic modelling, specifically astrong denoising probabilistic model (DPMs) buildingon the TTS work in . This model could be trained onspeech-and-gesture data from scratch in one go and pro-duced improved results over , but internally used sepa-rate pipelines for producing the two output modalities, lead-ing to suboptimal coherence between them. Match-TTSG improved on this aspect by using a compact and uni-fied decoder to jointly sample both output modalities. Italso used conditional flow matching rather than diffu-sion, for much faster output synthesis. Experiments foundthat Match-TTSG improved on the previous best model inall respects, establishing it as the current state of the art.Most of the above models were trained only on small,parallel multimodal datasets from a single speaker. (Theone exception is , which required pre-training part ofthe network on a TTS corpus to produce intelligible out-put at all.) The results in show that, e.g., the syntheticspeech falls short of human-level naturalness, and the qual-ity we find from systems trained on very large datasets. Ac-cordingly, we propose to circumvent the data limitation byusing strong unimodal synthesisers to create a large syn-thetic training corpus for our joint model.",
  ". Training on synthetic data": "Training models on synthetic data is gaining interest ,e.g., for privacy and in model compression through dis-tillation , including generative models like TTS .Synthesis (and synthetic data) is also appealing in caseswhere real data is scarce or difficult to obtain, as demon-strated in applications to human poses and motion .It also allows for the creation of diverse and controlleddatasets that can enable more accurate and versatile mod-els . We here propose to generalise such approaches bychaining together multiple unimodal synthesisers, to enabletraining multimodal speech-and-gesture models.There may be a risk that the individual unimodal synthe-sisers we use, being trained on non-overlapping data, couldfail to capture mutual information that connects the modal- ities. The final multimodal system trained on the syntheticcorpus might then suffer from artefacts and fail to recreateproper inter-modal dependencies. However, recent theoret-ical and practical results show that little or no parallel data may suffice for learning joint distributions ofmultiple random variables (modalities). Training on cor-pora generated by synthesisers built from non-overlappingmaterial might thus not be as risky as it could seem.",
  "Text generation": "The first step was to create text sentences that can form thebasis of synthesising multimodal data in a conversationalstyle. For this we used GPT-4 and deliberate prompt-ing. Specifically, we prompted the model with a list of 50transcriptions of sentences from the training split of theTrinity Speech-Gesture Dataset II (TSGD2) , eachenclosed in triple quotes, followed by a request to produce50 additional phrases in the same style (including hesita-tions and disfluencies seen in the transcriptions) but ignor-ing the content. Further prompting then followed, to makethe model generate additional output based around differentemotions and scenarios, and obtain a more diverse mate-rial. The emotional categories we provided were: disgust,sadness, fear, frustration, surprise, excitement, happiness,confusion, and denial. Our prompts often gave similar in-structions multiple times, as we found this led to more real-istic output. The main instruction prompt and a number ofexample continuations can be found in Appendix A.We utilised the above procedure to generate a total of600 phrases (available through the webpage), each approxi-mately 250 characters in length. We found that limiting thelength of the prompt helps prevent issues with the subse-quent speech synthesis, which tended to produce unintelli-gible or confabulated output for overly long utterances.",
  "Speech generation": "The next step was to synthesise speech audio from the 600LLM-generated phrases.For this, we considered multi-ple TTS systems capable of multi-speaker and spontaneousspeech synthesis, including Bark2, XTTS , and Eleven-Labs3. However, Bark exhibited frequent confabulationsand unexpected changes in speaker identity within a sin-gle utterance, which seemed problematic for learning tomaintain a consistent vocal identity. Although ElevenLabsdemonstrated high-quality output, its status as a non-opensource and proprietary solution led us to exclude it. Ul-timately, we selected XTTS for generating our syntheticspeech dataset, due to it combining more consistent syn-thesis with a research-permissible license. We limited eachsynthesised utterance to at most 400 XTTS speech tokens,since anything longer than that is virtually certain too longfor our prompts, and thus must contain confabulation orgibberish speech. For everything else, default XTTS syn-thesis hyperparameters were used. In the end, each syn-thesised audio utterance was around 2023 seconds long,taking about half that time to synthesise.In order to obtain more diverse data containing multiplespeakers, each of the 600 phrases was synthesised 16 times,once in each of 16 different voices. These voices were se-lected as a gender-balanced set (8 male and 8 female speak-ers) from the VCTK corpus , and elicited from XTTSby seeding the synthesis of each individual utterance withthe audio of longest VCTK utterance spoken by the rele-vant speaker as an acoustic prompt. These prompting utter-ances tended to be around 9 seconds long. In total, we thussynthesised 16 600 = 9600 audio utterances.Interestingly, despite the spontaneous nature of the in-put phrases, we found that false starts and fillers explicitlypresent in the input were sometimes omitted in the XTTSoutput. This could be partly due to the choice of tempera-ture parameter at synthesis time (the default, 0.65), whichfavours more consistent and likely output, and partly dueto the public English-language training datasets cover readrather than spontaneous speech. Since XTTS furthermorewas prompted using a snippet of read-aloud speech audiofrom VCTK, the output audio tended to sound more likereading than speaking spontaneously.",
  "Data filtering and forced alignment": "Following speech synthesis, a number of data-processingsteps were performed to obtain a suitable dataset for train-ing a strong gesture-generation system. To begin with, allsynthesised audio utterances longer than 25 seconds wereimmediately and permanently discarded, since these over-whelmingly tended to contain issues related to confabula- tion and the like. The output from XTTS did not have ex-act fidelity to the text it was prompted with, so automaticspeech recognition (ASR) was used to get more accurate in-put to the gesture-generation system. ASR was performedusing Whisper , using the medium.en model, whichhas in previous uses proven to be less prone to confabula-tion than the large variants, whilst providing sufficient accu-racy. Interestingly, Whisper tended to prefer British Englishspelling, possibly since VCTK was recorded in the UK. TheASR derived transcripts then replaced the original TTS in-put text for each utterance in all subsequent processing.The gesture-generation system we chose for the finalsynthesis () requires word-level timestamps for the texttranscriptions. Although we considered several tools thatattempt to obtain word timings from Whisper directly, nonewere sufficiently accurate for our needs. Instead, we ob-tained the requisite timings using the Montreal ForcedAligner (MFA) .Text input to MFA was processedword-by-word to remove leading and trailing punctuationand to perform case folding to lower case. Utterances thatMFA failed to align were also excluded from consideration.Following the filtering and alignment process, we wereleft with 8173 audio utterances for our final syntheticdataset, meaning that 1427 utterances (about 15%) werediscarded during the filtering step. The remaining data hada total duration of 37.6 hours, which also ended up beingthe size of the final synthetic training corpus.",
  "Gesture generation": "Weusedarecentdiffusion-basedgesture-generationmethod that performed well in a large comparativeevaluation to generate synthetic gesture data. That sys-tem leveraged data2vec embeddings to represent audioinput, which help achieve a more speaker-independent rep-resentation. On top of that, introduced a ContrastiveSpeech and Motion Pre-training (CSMP) module, to learnjoint embeddings of speech and gesture that can strengthenthe semantic coupling between these modalities. By utilis-ing the output of the CSMP module as a conditioning sig-nal within the diffusion-based gesture-synthesis model, thesystem can generate co-speech gestures that are human-likeand semantically aware, thereby improving the quality andappropriateness of the generated gestures to the spoken con-tent. The CSMP module requires word-level timestamps,which is why forced-alignment was performed in Sec. 3.1.3.Since this paper is focused on multimodal synthesis fromdata where no interlocutor is present or recorded (i.e., notback-and-forth conversations), interlocutor-related inputswere removed from the architecture. The input is thus anaudio track with time-aligned text transcripts. We used thepre-trained weights from for the CSMP module and re-trained the diffusion-based gesture model to comply with the change of input, using the same architecture and learn-ing rate as in the paper. The training was done using twoNVIDIA RTX3090 GPUs (194k updates, each with batchsize 60) on the subset of the Talking With Hands (TWH)dataset provided in the GENEA 2023 Challenge .We used the trained system to generate text-and-audio-driven gestures for the 8173 previously transcribed syn-thetic speech utterances, and used Autodesk MotionBuilderafter synthesis to retarget the output motion to the skele-ton of the TSGD2 data and visualiser in Sec. 4.1. Whilethe synthesised motion encompasses the full body (withoutfingers), we only consider upper-body motion in this work.Compared to conventional conditioning approaches whereaudio is represented using mel-spectrograms, the speaker-independent data2vec embeddings in the CSMP module areexpected to better handle the differences between naturaland synthetic voices during synthesis, thus making it fea-sible to generate large amounts of gesture data based onsynthetic speech without undue degradations due to domainmismatch. This data was used to train the different multi-modal synthesis systems considered in our experiments.",
  ". Proposed multimodal synthesis system": "The current state of the art in joint speech-and-gesture syn-thesis is Match-TTSG , a non-autoregressive modelwhich uses conditional flow matching (OT-CFM) tolearn Ordinary Differential Equations (ODEs) with morelinear vector fields than continuous-time diffusion models create. Such simpler vector fields offer advantages foreasier learning and faster synthesis.We extend the Match-TTSG framework in three ways:",
  ". Additional prosody-prediction modules,which arewidely used in NAR TTS": "3. A speaker-identity input, as necessary for pre-training onthe multispeaker data in the large synthetic training set.We call the resulting system MAGI for Multimodal Audioand Gesture, Integrated; see for a diagram.For (1), we augment the original Match-TTSG architec-ture with a probabilistic duration predictor based on OT-CFM, as introduced in , to learn distributions overspeech and gesture durations. This is trained jointly withthe rest of the system. It replaces the deterministic durationpredictor in Match-TTSG, inherited from , and uses the same network architecture.To learn better prosody correlations and enable controlover the output, we drew inspiration from andincorporated two prosody-predictor modules into our sys-tem: one for pitch prediction and one for energy prediction,both using the same architecture and hyperparameters as thevariance adaptor in . Such prosody predictors improvethe synthesis as they enable the model to learn a less over- Text input Text encoder Flow-matchingduration model",
  ". Schematic overview of the proposed MAGI architecture and its prosody predictor": "smoothed representation, thereby enhancing the variabilityof the generated output by conditioning the synthesis pro-cess on additional prosodic features . The pitch of thetraining data utterances was extracted using the PyWorldwrapper for the WORLD vocoder4 with linear interpolationapplied in unvoiced segments to achieve continuous pitchcontours for the entire utterances. We employed a bucket-ing approach similar to , separately for pitch and energy,to turn predicted continuous values into embedding vectorsto be summed with the text-encoder output vectors. How-ever, in contrast to , we performed token-level predic-tion instead of frame-level prediction for the two prosodicproperties, since it has been stated5 that this improves thesynthesis whilst reducing memory consumption.Like in , Match-TTSG includes a projection layerthat maps the text-encoder output vectors onto a predictedaverage output vector per token (sub-phone). These aver-ages are used for the so-called prior loss in the monotonicalignment search. The process of sampling the output fea-tures (i.e., the flow-matching decoder) is also conditionedon these predicted average vectors. However, the latter canintroduce an information bottleneck, since averages do notinclude information about variance, correlations, or highermoments of the output distribution. To improve informationflow we instead condition the MAGI decoder directly on thelast layer of the text-encoder, prior to the projection layer.Finally, we added a speaker embedding for multispeakersynthesis. Specifically, we used a one-hot speaker vectorto represent the 16 different speakers in the synthetic train-ing data. This vector was concatenated to other inputs atmultiple stages of the synthesis process, including the textencoder, prosody predictors and decoder. The idea with thiswas to minimise information loss and ensure coherent out-put across different speaker identities. Since the concate-nated vectors only have 16 elements, the impact on model",
  ". Experiments": "This section experimentally compares our proposed trainingmethod and architecture with the previous state-of-the-artmethod Match-TTSG . Since this is a synthesis work,the gold standard approach to evaluation and thus the fo-cus of our experimental validation is subjective user stud-ies. The experiments closely follows those in previous jointsynthesis works , which in turn follows establishedpractices in speech and gesture evaluation .",
  ". Data and systems": "To test the effectiveness of our method we carried out 3 dif-ferent subjective evaluations with systems trained on TrinitySpeech-Gesture Dataset II (TSGD2) , a dataset contain-ing 6 hours of multimodal data: recordings of time-aligned44.1 kHz audio coupled with 120 fps marker-based 3D mo-tion capture, in which a male native speaker of Hiberno-English discusses a variety of topics whilst gesturing freely.The same train-test split of the data was used as in ,with around 4.5 hours of training data much less than the38 hours of synthetic multimodal data we created.We trained Match-TTSG (MAT) containing 30.2M pa-rameters and MAGI (MAGI, 31.6M parameters) for 300ksteps on only the TSGD2 data, and refer to these conditionsMAT-T and MAGI-T respectively. We also took the sametwo architectures (albeit with one-hot speaker vectors forMatch-TTSG) and first pre-trained them for 200k updateson the synthetic multispeaker data, followed by fine-tuningfor 100k updates on our target dataset, TSGD2. We refer tothese as MAT-FT and MAGI-FT. Output samples for held-out sentences were synthesised using 100 neural functionevaluations (NFEs; equivalent to number of Euler-forwardsteps used by the ODE solver) for audio-and-motion syn-thesis, whilst 10 NFEs were used for the preceding stochas-tic duration modelling, since it is lower-dimensional and converged more rapidly. Training and synthesis were per-formed on NVIDIA RTX 3090 GPUs with batch size 32.15 utterances from the held-out set were used to evaluateeach modality individually. We used pre-trained Univer-sal HiFi-GAN to generate vocoded but otherwise nat-ural speech referred to as NAT. We used the same vocoderto generate waveforms from the output mel spectrogramssynthesised by the trained multimodal-synthesis systems,while Blender was used to render the motion representa-tions into 3D avatar video, using exactly the same upper-body avatar and visualiser as in . The motion datawas represented as rotational representation using exponen-tial maps of 45-dim pose vectors and were downsam-pled to 86.13 fps using cubic interpolation to match theframe rate of the mel-spectrograms.",
  ". Evaluation setup": "To gain an objective insight into the intelligibility of thesynthetic speed, we synthesised the test set sentences fromTSGD2, which we then passed to Whisper ASR, to use theWord Error Rate (WER) results as an indicator of their in-telligibility. For subjective evaluation, user studies are thegold standard when evaluating synthesis methods. Follow-ing , we used comprehensive evaluation, conducting in-dividual studies of each generated modality. We addition-ally evaluate the appropriateness of the modalities in termsof each other, to determine how well they fit together.In our studies, participants had an interface with fiveunique response choices, with the exact details varyingslightly across different investigations.All participantswere native English speakers recruited through the Pro-lific crowdsourcing platform. Each test was designed tolast around 20 minutes and participants were compensated4 GBP (12 GBP/hr) for participation. For the purpose ofstatistical examination, we converted responses into numer-ical values. These values were then analysed for statisticalsignificance at the 0.05 threshold using pairwise t-tests.",
  "Speech-quality evaluation": "To assess perceived naturalness of the synthesized speech,we employed the Mean Opinion Score (MOS) testing ap-proach, drawing inspiration from the Blizzard Challengefor text-to-speech systems . Participants were asked,How natural does the synthesized speech sound?, ratingtheir responses on a scale from 1 to 5, where 1 representedCompletely unnatural and 5 indicated Completely natu-ral. The intermediary values of 2 to 4 were provided with-out textual descriptions. Each participant evaluated 15 stim-uli per system and 4 attention checks resulting in a total of525 responses per condition by 35 participants. Fine-tuningwith synthetic data led to performance enhancements forboth MAGI and MAT, reducing the WER from 13.28% in",
  "Motion-quality evaluation": "We evaluate motion quality using video stimuli that only vi-sualised motion, without any audio, in order to have an in-dependent assessment of motion quality. This ensures thatratings are not affected by speech and follows the practiceof recent evaluations of gesture quality . Similarlyto the speech evaluation, participants were asked How nat-ural and humanlike the gesture motion appear?, and gaveresponses on a scale of 1 (Completely unnatural) to 5(Completely natural). The number of stimuli and atten-tion checks were identical to the speech-only evaluation.",
  "Speech-and-motion appropriateness evaluation": "We finally evaluated how appropriate the generated speechand motion were for each other, whilst controlling for theeffect of their individual quality following . For each speech segment and condition, we createdtwo video stimuli: one with the original video and sound,and the other combining the original speech audio with mo-tion from a different video clip, adjusting the motion speedto align with the audio duration. Both videos feature com-parable motion quality and characteristics from the samecondition, but only one videos motion is synchronised withthe audio track, without indicating which video is which.The test inquired which characters motion most accu-rately matched the speech in rhythm, intonation, and mean-ing. Participant ability to identify the correctly synchro-nised video indicates a strong rhythmic and/or semantic linkbetween generated motion and speech. Following weopted for five response choices instead of the typical threefor better resolution. Options were Left is much better,Left is slightly better, Both are equal, Right is slightlybetter, Right is much better. For the purposes of analy-sis, numbers in the range of 2 to 2 were assigned to eachresponse, as in , with 2 representing the participantspreference for the mismatched stimulus and 2 the matchedstimulus. Participants reviewed motions from 14 of the 15segments, displayed as 7 screens of pairs of videos, plustwo audio and two video attention checks, covering all con-ditions for these segments. 70 persons completed the test,yielding 490 responses per system.",
  "MAGI-T3.440.093.110.100.510.09MAGI-FT3.620.083.520.110.600.09": "both MAGI-FT and MAT-FT yielded higher Mean OpinionScores (MOS), albeit without statistical significance. No-tably, MAGI facilitated greater control over pitch and en-ergy a feature absent from Match-TTSG. However, de-spite improvements, the synthesised speech did not achievethe level of naturalness present in the human-recordedspeech from the held-out set, see . In terms of synthesised gestures, MAGI outperformedother conditions in human-likeness.However, they re-mained inferior to human-motion reference data. The influ-ence of synthetic data pre-training and the proposed modelsarchitecture on gesture synthesis presented a more nuancedpicture. Specifically, pre-training on synthetic data only sig-nificantly benefited the proposed model, and, intriguingly,the MAGI enhanced gestures in a larger dataset but hadthe opposite effect on a smaller dataset. This discrepancymight stem from the prosody predictors in our model be-ing trained on per-phone rather than per-frame data, lead-ing to a scarcity of training data for these predictors insmaller datasets. However, with adequate pre-training onexpansive datasets, these models demonstrated better con-vergence. These findings align with prior speech evalua-tions, where the novel architectures advantages were morepronounced following pre-training on a larger dataset. Further, no model matched the cross-modal appropriate-ness found in multimodal human recordings, echoing thechallenges observed in unimodal gesture synthesis whererecent evaluations did not approach the appropriateness ofhuman data .Although MAGI pre-trained onsynthetic data showcased superior performance, it did notsignificantly exceed the existing benchmarks in synthesissystems. This observation may be attributed to the inher-ent difficulty in discerning significant differences in appro-priateness, as opposed to naturalness or human-likeness,and the comparison against a robust baseline without al-terations that directly influence cross-modal synthesis as-pects. Lastly, the cross-modal aspects might conceivablybe less accurately represented in synthetic datasets createdfrom unimodal synthesisers trained on non-cohesive data.",
  ". Pitch and energy control": "As stated, the proposed multi-stage architecture with sepa-rate prosody predictors allows for modifying or substitutingthe pitch and energy contours before synthesis. This enablesdirect control of prosodic properties of the speech, with thesynthesis process having the option to adjust the gestures tomatch. On our demo page shivammehta25.github.io/MAGI/we provide example videos showing the effect that modi-fying (scaling) the pitch and energy contours returned bythe predictors has on the synthesised output.One canobserve that reducing the pitch seems to promote creakyvoice, which makes sense from a speech-production per-spective and fits earlier findings from autoregressive TTSon spontaneous-speech data .",
  ". Conclusion and future work": "We have described improvements to the joint and simulta-neous multimodal synthesis of speech audio and 3D gesturemotion from text. Specifically, we propose training on datasynthesised by a chain of strong unimodal synthesis systemsto address the shortage of multimodal training data. We alsoaugment the state-of-the-art architecture for speech-and-gesture synthesis, Match-TTSG, with a stochastic durationmodel, TTS-inspired prosody predictors for controllability,and the ability to perform multispeaker synthesis. The fi-nal model, called MAGI, is radically smaller than those thatgenerated the synthetic data. Experiments confirm that pre-training on synthetic data significantly improved unimodalspeech and gesture quality. The architectural improvementsreaped benefits when pre-training on large amounts of syn-thetic data, with the added prosody control having a cleareffect on the audio output.Relevant future work includes investigating alternativeoptions for mitigating the shortage of multimodal trainingdata, such as pre-training on data lacking one or more ofthe modalities; incorporating RL-based approaches, par-ticularly effective for generation of situated gestures as in; or (following the CSMP methodology ) leverag-ing various self-supervised representations trained on largeamounts of data. Possible architectural extensions includeflow matching for pitch and energy, and similar control overmotion properties such as gesture radius and symmetry .",
  "Tenglong Ao, Zeyi Zhang, and Libin Liu. GestureDiffu-CLIP: Gesture diffusion model with CLIP latents. ACMTrans. Graph., 42(4):118, 2023. 3": "Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu,Jiatao Gu, and Michael Auli. data2vec: A general frame-work for self-supervised learning in speech, vision and lan-guage. In Proceedings of the International Conference onMachine Learning, pages 12981312, 2022. 3, 5 Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell,Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort,Deep Ganguli, Tom Henighan, et al. Training a helpful andharmless assistant with reinforcement learning from humanfeedback. arXiv preprint arXiv:2204.05862, 2022. 2",
  "Yiwei Guo, Chenpeng Du, Ziyang Ma, Xie Chen, and KaiYu. VoiceFlow: Efficient text-to-speech with rectified flowmatching. In Proc. ICASSP, 2024. 2, 3, 5": "Ikhsanul Habibie, Weipeng Xu, Dushyant Mehta, LingjieLiu, Hans-Peter Seidel, Gerard Pons-Moll, Mohamed El-gharib, and Christian Theobalt.Learning speech-driven3D conversational gestures from video. In Proceedings ofthe International Conference on Intelligent Virtual Agents,pages 101108, 2021. 3 Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen,Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B.Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling lawsfor autoregressive generative modeling.arXiv preprintarXiv:2010.14701, 2020. 2",
  "Stefan Kopp and Ipke Wachsmuth.Synthesizing multi-modal utterances for conversational agents. In ComputerAnimation and Virtual Worlds, pages 3952. Wiley OnlineLibrary, 2004. 1": "Taras Kucherenko, Patrik Jonell, Sanne van Waveren,Gustav Eje Henter, Simon Alexanderson, Iolanda Leite,and Hedvig Kjellstrom.Gesticulator: A framework forsemantically-aware speech-driven gesture generation.InProceedings of the ACM International Conference on In-telligent Virtual Agents, pages 242250, 2020. 1 Taras Kucherenko, Rajmund Nagy, Michael Neff, HedvigKjellstrom, and Gustav Eje Henter. Multimodal analysis ofthe predictability of hand-gesture properties. In Proceed-ings of the International Conference on Autonomous Agentsand Multiagent Systems, pages 770779, 2022. 1 Taras Kucherenko,Rajmund Nagy,Youngwoo Yoon,Jieyeon Woo, Teodor Nikolov, Mihail Tsakov, and Gus-tav Eje Henter. The GENEA Challenge 2023: A large-scaleevaluation of gesture generation models in monadic anddyadic settings. In Proceedings of the International Con-ference on Multimodal Interaction, pages 792801, 2023.2, 3, 5, 6 Taras Kucherenko, Pieter Wolfert, Youngwoo Yoon, CarlaViegas, Teodor Nikolov, Mihail Tsakov, and Gustav EjeHenter. Evaluating gesture-generation in a large-scale openchallenge: The GENEA Challenge 2022. arXiv preprintarXiv:2303.08737, 2023. 7, 8 Mateusz Lajszczak, Guillermo Cambara Ruiz, Yang Li,Fatih Beyhan, Arent van Korlaar, Fan Yang, et al. Base tts:Lessons from building a billion-parameter text-to-speechmodel on 100k hours of data. arXiv, 2024. 2",
  "Harm Lameris, Shivam Mehta, Gustav Eje Henter, JoakimGustafson, and Eva Szekely. Prosody-controllable sponta-neous TTS with neural HMMs. In Proc. ICASSP, 2023. 3,8": "Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, LedaSari, Rashel Moritz, Mary Williamson, Vimal Manohar,Yossi Adi, Jay Mahadeokar, et al. Voicebox: Text-guidedmultilingual universal speech generation at scale.arXivpreprint arXiv:2306.15687, 2023. 2, 3, 5 Gilwoo Lee, Zhiwei Deng, Shugao Ma, Takaaki Shiratori,Siddhartha S. Srinivasa, and Yaser Sheikh. Talking WithHands 16.2 M: A large-scale dataset of synchronized body-finger motion and audio for conversational motion analy-sis and synthesis. In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision, pages 763772,2019. 2, 5",
  "tional Conference on Human-Agent Interaction, pages 3138, 2021. 1": "Shuhong Lu, Youngwoo Yoon, and Andrew Feng.Co-speech gesture synthesis using discrete gesture token learn-ing. In 2023 IEEE/RSJ International Conference on Intelli-gent Robots and Systems (IROS), pages 98089815. IEEE,2023. 3 Soroosh Mariooryad, Matt Shannon, Siyuan Ma, TomBagby, David Kao, Daisy Stanton, Eric Battenberg, andRJ Skerry-Ryan.Learning the joint distribution of twosequences using little or no paired data.arXiv preprintarXiv:2212.03232, 2022. 4 Michael McAuliffe,Michaela Socolof,Sarah Mihuc,Michael Wagner, and Morgan Sonderegger.MontrealForced Aligner:Trainable text-speech alignment usingKaldi. In Proc. Interspeech 2017, pages 498502, 2017.5",
  "Shivam Mehta, Ruibo Tu, Jonas Beskow, Eva Szekely, andGustav Eje Henter. Matcha-TTS: A fast TTS architecturewith conditional flow matching. In Proc. ICASSP, 2024. 2,3, 5, 7": "Xiaoxiao Miao, Xin Wang, Erica Cooper, Junichi Yamag-ishi, Nicholas Evans, Massimiliano Todisco, Jean-FrancoisBonastre, and Mickael Rouvier.SynVox2: Towards aprivacy-friendly VoxCeleb2 dataset.In Proc. ICASSP,pages 1142111425, 2024. 3 Evonne Ng, Javier Romero, Timur Bagautdinov, Shao-jie Bai, Trevor Darrell, Angjoo Kanazawa, and Alexan-der Richard.From audio to photoreal embodiment:Synthesizing humans in conversations.arXiv preprintarXiv:2401.01885, 2024. 3 Junrui Ni, Liming Wang, Heting Gao, Kaizhi Qian, YangZhang, Shiyu Chang, and Mark Hasegawa-Johnson. Un-supervised text-to-speech synthesis by unsupervised auto-matic speech recognition. In Proc. Interspeech, pages 461465, 2022. 4",
  "Alec Radford, Karthik Narasimhan, Tim Salimans, and IlyaSutskever. Improving language understanding by genera-tive pre-training, 2018. 2": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-vision. In Proceedings of the International Conference onMachine Learning, pages 87488763, 2021. 3 Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman,Christine McLeavey, and Ilya Sutskever.Robust speechrecognition via large-scale weak supervision. In Proceed-ings of the International Conference on Machine Learning,pages 2849228518, 2023. 5",
  "ture production for multi-modal robot behavior. In Proc.RO-MAN, pages 614619, 2010. 1, 3": "Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schus-ter, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, et al.Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions.In Proc. ICASSP, pages 47794783, 2018. 2, 3 Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng,Lei He, Tao Qin, et al. NaturalSpeech 2: Latent diffusionmodels are natural and zero-shot speech and singing syn-thesizers. arXiv preprint arXiv:2304.09116, 2023. 2",
  "Paul Taylor. Text-to-speech synthesis. Cambridge Univer-sity Press, 2009. 1": "Sarah Taylor, Jonathan Windle, David Greenwood, and IainMatthews. Speech-driven conversational agents using con-ditional Flow-VAEs.In Proceedings of the ACM Euro-pean Conference on Visual Media Production, pages 6:16:9, 2021. 3 Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir,Daniel Cohen-Or, and Amit H. Bermano. Human motiondiffusion model. In Proceedings of the International Con-ference on Learning Representations, 2023. 3 Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothee Lacroix, Bap-tiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,et al. Llama: Open and efficient foundation language mod-els. arXiv preprint arXiv:2302.13971, 2023. 2",
  "Boris van Breugel and Mihaela van der Schaar. Beyond pri-vacy: Navigating the opportunities and challenges of syn-thetic data. arXiv preprint arXiv:2304.03722, 2023. 3": "Aaron van den Oord, Yazhe Li, Igor Babuschkin, KarenSimonyan, Oriol Vinyals, Koray Kavukcuoglu, GeorgeDriessche, Edward Lockhart, Luis Cobo, Florian Stimberg,et al. Parallel WaveNet: Fast high-fidelity speech synthesis.In Proceedings of the International Conference on MachineLearning, pages 39183926, 2018. 3 Gul Varol, Javier Romero, Xavier Martin, Naureen Mah-mood, Michael J Black, Ivan Laptev, and Cordelia Schmid.Learning from synthetic humans.In Proceedings of theIEEE Conference on Computer Vision and Pattern Recog-nition, pages 109117, 2017. 3",
  "Siyang Wang, Simon Alexanderson, Joakim Gustafson,Jonas Beskow, Gustav Eje Henter, and Eva Szekely. Inte-grated speech and gesture synthesis. In Proc. ICMI, pages177185, 2021. 1, 3": "Bowen Wu, Chaoran Liu, Carlos T. Ishi, and Hiroshi Ishig-uro.Modeling the conditional distribution of co-speechupper body gesture jointly using conditional-GAN andunrolled-GAN. Electronics, 10(3):228, 2021. 3 Bowen Wu, Chaoran Liu, Carlos T. Ishi, and Hiroshi Ishig-uro. Probabilistic human-like gesture synthesis from speechusing GRU-based WGAN. In Companion Publication ofthe International Conference on Multimodal Interaction,pages 194201, 2021. 3",
  "Junichi Yamagishi, Christophe Veaux, and Kirsten Mac-Donald. CSTR VCTK corpus: English multi-speaker cor-pus for CSTR voice cloning toolkit (version 0.92), 2019.4": "Sicheng Yang, Zhiyong Wu, Minglei Li, Zhensong Zhang,Lei Hao, Weihong Bao, Ming Cheng, and Long Xiao. Dif-fusestylegesture: stylized audio-driven co-speech gesturegeneration with diffusion models. In Proceedings of theInternational Joint Conference on Artificial Intelligence,pages 58605868, 2023. 3 Sicheng Yang, Zhiyong Wu, Minglei Li, Zhensong Zhang,Lei Hao, Weihong Bao, and Haolin Zhuang. QPGesture:Quantization-based and phase-guided motion matching fornatural speech-driven gesture generation. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 23212330, 2023. 3 Sicheng Yang, Haiwei Xue, Zhensong Zhang, Minglei Li,Zhiyong Wu, Xiaofei Wu, Songcen Xu, and Zonghong Dai.The diffusestylegesture+ entry to the genea challenge 2023.In Proceedings of the International Conference on Multi-modal Interaction, pages 779785, 2023. 3 Yusuke Yasuda, Xin Wang, and Junichi Yamagishi.Ef-fect of choice of probability distribution, randomness, andsearch methods for alignment modeling in sequence-to-sequence text-to-speech synthesis using hard alignment.In ICASSP 2020-2020 IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP), pages67246728. IEEE, 2020. 2 Payam Jome Yazdian, Mo Chen, and Angelica Lim. Ges-ture2Vec: Clustering gestures using representation learn-ing methods for co-speech gesture generation. In Proceed-ings of the IEEE/RSJ International Conference on Intelli-gent Robots and Systems, 2022. 3",
  "Generating holistic 3D human motion from speech. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 469480, 2023. 3": "Youngwoo Yoon, Bok Cha, Joo-Haeng Lee, Minsu Jang,Jaeyeon Lee, Jaehong Kim, and Geehyuk Lee. Speech ges-ture generation from the trimodal context of text, audio, andspeaker identity.ACM T. Graphic., 39(6):222:1222:16,2020. 1 Youngwoo Yoon, Pieter Wolfert, Taras Kucherenko, CarlaViegas, Teodor Nikolov, Mihail Tsakov, and Gustav EjeHenter. The GENEA Challenge 2022: A large evaluation ofdata-driven co-speech gesture generation. In Proceedingsof the International Conference on Multimodal Interaction,pages 736747, 2022. 7, 8",
  "He Zhang, Sebastian Starke, Taku Komura, and Jun Saito.Motion synthesis and editing in low-dimensional spaces.Computer Graphics Forum, 39(8):509521, 2020. 3": "Mingyuan Zhang, Zhongang Cai, Liang Pan, FangzhouHong, Xinying Guo, Lei Yang, and Ziwei Liu. MotionDif-fuse: Text-driven human motion generation with diffusionmodel. IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 2024. 3 Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen,Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu, HuamingWang, Jinyu Li, et al. Speak foreign languages with yourown voice: Cross-lingual neural codec language modeling.arXiv e-prints, pages arXiv2303, 2023. 2",
  "Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun,Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu,et al. LIMA: Less is more for alignment. arXiv preprintarXiv:2305.11206, 2023. 2": "Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, ZiweiLiu, and Lequan Yu. Taming diffusion models for audio-driven co-speech gesture generation.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1054410553, 2023. 3 Adrian ancucki. Fastpitch: Parallel text-to-speech withpitch prediction. In ICASSP 2021 - 2021 IEEE Interna-tional Conference on Acoustics, Speech and Signal Pro-cessing (ICASSP), pages 65886592, 2021. 2, 5",
  "After supplying 50 example utterance transcriptions fromTSGD2, the generation of synthetic phrases was initialisedusing the following text prompt:": "Take these sentences into account and learn theirconversational style ignore the content learn thehesitations and disfluencies (using three dots forlong pauses ...) and uh, um and uhm for conversa-tional disfluencies. Generate more sentences likethis in form of spontaneous conversational mono-logues. Remember disfluencies should be eitherrepeating, ..., uh, um or uhm. Make it sound nat-ural as human would converse. Create 50 phraseswhich mimics how people speak including filledpauses. Make phrases that are highly emotionalbut realistic. After that, the model was prompted to generate furtherphrases corresponding to a variety scenarios and emotions,to obtain more variety. Here are some examples: Continue generation for a happy emotion imagine a dif-ferent scenario of getting your research paper acceptedafter a difficult and long review process.",
  "Continue the generation for happy tone and talk aboutyour favourite movie that you recently watched at the the-atre and recommend it to people": "Make these sentences a bit smaller just reduce 510 wordsmax per sentence. Keep the conversational style and dis-fluences it is very important to keep those. keep uh, umand pauses, repeats, fixes and other disfluences.con-tinue happy generation talk about a perfect date at yourfavourite restaurant Generate more, but start the sentence in a different way.generate some sentences with negation like saying no tothings, denying something etc. focus on new topics, youcan forget the old topic. talk about people compellingyou to learn to play different sports, musical instruments,dance forms and you denying to it."
}