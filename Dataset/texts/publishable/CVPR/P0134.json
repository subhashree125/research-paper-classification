{
  "Abstract": "SVG (Scalable Vector Graphics) is a widely used graph-ics format that possesses excellent scalability and editabil-ity. Image vectorization, which aims to convert raster im-ages to SVGs, is an important yet challenging problem incomputer vision and graphics. Existing image vectorizationmethods either suffer from low reconstruction accuracy forcomplex images or require long computation time. To ad-dress this issue, we propose SuperSVG, a superpixel-basedvectorization model that achieves fast and high-precisionimage vectorization. Specifically, we decompose the inputimage into superpixels to help the model focus on areas withsimilar colors and textures. Then, we propose a two-stageself-training framework, where a coarse-stage model is em-ployed to reconstruct the main structure and a refinement-stage model is used for enriching the details. Moreover,we propose a novel dynamic path warping loss to helpthe refinement-stage model to inherit knowledge from thecoarse-stage model. Extensive qualitative and quantitativeexperiments demonstrate the superior performance of ourmethod in terms of reconstruction accuracy and inferencetime compared to state-of-the-art approaches. The code isavailable in",
  ". Introduction": "Scalable Vector Graphics, commonly known as SVG, is awidely used vector image format that has a wide range ofapplications and advantages within the domains of web de-sign, graphic design, mobile applications, data visualiza-tion, and various other contexts. Compared with raster im-ages that represent content by pixels, SVG describes im-ages by parameterized vectors and benefits from its scala-bility and editability where it can be resized to any resolu-tion without losing quality and can be easily manipulatedby its layer-wise topological information.Given the superior capabilities of Scalable Vector Graph-",
  "Input ImageOutput SVG": ". Overview of our SuperSVG: our model first decomposesthe image to be vectorized into superpixels, each containing pix-els sharing similar colors and contents. The coarse-stage modelpredicts the path parameters to reconstruct the main structure, andthen the coarse paths guided refinement model enriches the detailsby learning the knowledge from the coarse-stage model. Com-pared to the previous methods, our SuperSVG achieves both a highvectorization accuracy and fast computation speed. ics (SVG) in image representation and editing, there ismuch research on the topic of image vectorization, whichaims to convert rasterized images into SVG. The existingmethods can be categorized into three classes: 1) Tradi-tional algorithm-based methods ,where conventional algorithms are employed to fit images,but they usually suffer from a lower vectorization quality.2) Deep-learning-based methods ,which parameterize raster images using deep neural net-works for reconstruction. They are efficient and can han-dle the vectorization of simple graphics or characters (e.g.,icons and emojis), but struggle to vectorize complex im-ages.3) Optimization-based methods ,which optimize SVG parameters to fit the target image,yielding relatively superior reconstruction quality.How-ever, these methods entail a substantial amount of timeand computational resources, making them impractical fortimely processing of large-scale data. In summary, previousimage vectorization methods either suffer from low recon-struction quality for complex images, or demand extensivecomputation time, imposing significant constraints on theirpractical utility.",
  "arXiv:2406.09794v1 [cs.CV] 14 Jun 2024": "To achieve good vectorization quality with high ef-ficiency, we propose SuperSVG, a deep-learning-basedmethod that translates images into scalable vector graphics(SVG) in a coarse-to-fine manner. Since neural networkshave difficulties in directly vectorizing complex images, wedecompose the input image into different parts in the formof superpixels wherein the pixels share similar colors andtextures and then vectorize each part. Then, we proposea Two-Stage Self-Teaching Training framework to vector-ize the superpixels, where the coarse-stage model is trainedto reconstruct the main structure of the image and the re-finement-stage model is trained to enrich the image details.We make use of the predicted paths from the coarse-stagemodel to guide the refinement-stage model in image vector-ization. Furthermore, we propose a novel Dynamic PathWarping loss which helps the refinement-stage to inheritthe knowledge of the coarse-stage model. With the helpof the superpixel-based image decomposition and the two-stage self-teaching framework, our SuperSVG can keep theimage structure well and reconstruct more details at highspeed. Extensive quantitative and qualitative experimentsvalidate the effectiveness of our model.",
  "The main contributions of our work are four-fold:": "We propose SuperSVG, a novel superpixel-based vector-ization model that translates the rasterized images intoscalable vector graphics (SVG) based on superpixels andvectorizes the superpixels in a coarse-to-fine manner. We design a Two-Stage Self-Teaching Training frame-work, where we employ a coarse-stage model to recon-struct the main structures and a refinement-stage modelto enrich the image details based on the coarse-stage out-put. We propose a Coarse Paths Guided Training strategy toguide the refinement-stage model to inherit the knowl-edge from the coarse-stage model, which greatly im-proves the performance of the refinement-stage modeland avoids converging to suboptimal local minimum. We propose Dynamic Path Warping (DPW) loss, whichmeasures the distance between the predicted paths fromthe refinement-stage model and the pseudo ground truthapproximated with coarse paths. By minimizing the DPWloss, the refinement-stage model can distill the knowledgefrom the coarse-stage model.",
  ". Image Vectorization": "Image vectorization aims to transform a rasterized imageinto scalable vector graphics (SVG) composed of parame-terized vectors. Different from raster images that may be-come blurry when zooming in, SVG can be rendered at anyresolution without losing quality and is convenient to edit,widely used in web design, graphic design, etc. The existing",
  "vectorization methods can be classified into 3 categories:": "Traditional Algorithm-based Image VectorizationMethods can be classified into mesh-based and curve-basedones. The mesh-based methods segmentan input image into non-overlapping patches, and infer thecolor and the boundary location for each region. The curve-based methods employ Bezier curves withdifferent colors defined on either side to create the vec-tor image. Potrace is a representative method of thistype that projects the smooth outlines into Bezier paths, andmerge the adjacent paths together. However, the vectoriza-tion quality of these methods still needs improvements.",
  "Deep-learning-based Image Vectorization Methods": "use neural networks to project a raster image into SVG.Im2Vec employs a variational auto-encoder (VAE) to embed the input image and then maps it into path param-eters by a Long Short-Term Memory (LSTM) module .Raster2Vec is focused on vectorization of rasterizedfloor plans using a ResNet . Gao et al. rely on apre-trained VGG network and a hierarchical RecurrentNeural Network (RNN) to output parametric curves of dif-ferent sizes. But these methods only focus on simple imagesand cannot vectorize complex images well. In comparison,our SuperSVG is the first deep-learning-based method thatcan vectorize images with complex details, thanks to our su-perpixel decomposition and coarse-path guided refinementthat substantially reduce the learning difficulties.",
  "Optimization-based Image Vectorization Methods": "DiffVG proposes a differentiable renderer that rendersthe SVG parameters into images. Based on this, DiffVGminimizes the distance between the rasterized and vectorimages by optimizing the SVG parameters using gradientdescent. LIVE and SAMVG further introduce alayer-wise optimization framework, which achieves bettervectorization quality over the previous methods. However,due to the low optimization efficiency, they suffer from along optimization time. In contrast, our SuperSVG achievesboth a good vectorization quality and high efficiency.",
  ". Superpixel Decomposition": "Superpixel decomposition is usually used for data pre-processing in vision tasks.Existing superpixel decom-position methods can be categorized into methods basedon traditional algorithm or deep learning.For the tra-ditional algorithm based methods, diverse strategies havebeen employed, e.g., energy-driven sampling , geomet-ric flows and clustering . Some recent works employ neural networks to enhance the performance insuperpixel decomposition, which shows great potential inthis task.",
  "Decomposition": ". Main framework of our SuperSVG: we decompose the target image into superpixels and vectorize each superpixel separately.We employ an attention-based coarse-stage model to predict SVG paths that reconstruct the main structure of the superpixel. Then, arefinement-stage model guided by the coarse paths is designed to predict more SVG paths to refine details based on the coarse image.Finally, by combining all the predicted SVGs for each superpixel, we obtain an output SVG with good structure and fine details.",
  ". Method": "Image vectorization aims to translate a rasterized image Iinto a Scalable Vector Graphic (SVG). An SVG is com-posed of many vector primitives, which can be SVG paths,ellipses, circles, or rectangles, etc.Following previousworks , we employ the SVG paths as the shapeprimitive, where each SVG path defines a region con-structed by multiple cubic Bezier curves connected end-to-end with certain color. With the parameters of these SVGpaths, the rasterized image can be rendered in any resolu-tion. To obtain the path parameters, some previous meth-ods optimize the path parameters directly to mini-mize the distance between the input image and the renderedimage, which achieves good reconstruction quality but re-quires long optimization time. To speed up the vectorizationprocess, some deep-learning-based methods employ adeep-learning model to predict the SVG path parameters,but struggle to vectorize complex images. To achieve good vectorization quality with high effi-ciency, we propose SuperSVG, a deep-learning-based im-age vectorization method that translates images into SVGpath parameters automatically. To improve the model abil-ity to vectorize complex images, we segment the input im-age into different parts, within which the pixels share sim-ilar colors and textures, and then vectorize each part sepa-rately, where superpixels are used for image segmentationas they tend to maintain compactness, uniformity and regu-larity, particularly suitable for our task. For each superpixelx X where X is the set of all superpixels, our modelconverts it into a sequence of path parameters, where each path is composed of several cubic Bezier curves and has afill color, with a total of Np parameters. With the predictedpath parameters for each superpixel, we employ the differ-entiable renderer R() from DiffVG to get the renderedimage I in pixel space, which is expected to be close to theinput image I. We propose a two-stage self-teaching framework, com-posed of a coarse-stage model Ec to reconstruct the mainstructure and a refinement model Er to enrich the de-tails, where the predicted paths from coarse-stage modelare used to guide the refinement model in vectorization.Ec takes the superpixel x as input and outputs n pathsS = {s1, s2, sn} to reconstruct main structure; whileEr takes both the rendered image R(S) and target super-pixel x as inputs, and outputs m paths S = {s1, s2 sm}to refine details. Combining all the predicted S and S foreach superpixel produces the final SVG result.",
  ". Superpixel-based Coarse Reconstruction": "Superpixel decomposition. Considering the optimization-based methods suffer from a long optimization time, our Su-perSVG builds upon neural networks to efficiently predictSVG paths. However, as neural networks have difficultiesin directly vectorizing complex images we thereforesimplify the task to vectorizing a certain part of the imagecontaining homogeneous colors and textures. Since super-pixel algorithms provide a good tool to decompose imagesbased on local pixel color and also ensure alignment of theregions with the image boundaries, we segment the inputimage into superpixels, and our model reconstructs each su-",
  "Coarse-stage model.For a superpixel x with mask": "mask (indicating those pixels within x with 1 and 0 oth-erwise), we first design the coarse-stage model Ec to vec-torize the main structure by predicting the SVG path se-quence Ec(x) = S = {s1, s2, sn}. Inspired by At-tnPainter , Ec is composed of a Vision Transformer(ViT) encoder and a cross-attention module followedby a self-attention module, which is shown in . Specifically, the ViT encoder first encodes the input su-perpixel x into image feature Tf.To control the num-ber of output paths (n) and the parameter number in eachpath (Np), we employ a cross-attention module to calcu-late the correlation between the image feature Tf and nNp-dimensional learnable path queries Tl, and output anintermediate feature T f with the shape of path parameters",
  "Our": ". Problem of training the refinement model with L2 lossalone: optimizing a newly-added path on the canvas by L2 grad-ually pulls it to disappear (as a suboptimal local minimum). Withour proposed coarse paths guided training and DPW loss, theadded path is successfully optimized to resemble the target. 2) Boundary Loss: To avoid the SVG path from crossingthe superpixel boundary, we propose boundary loss to guidethe paths to be inside the superpixel. We set the color of allpredicted SVG paths to 1 (white) to get a new path sequenceSbinary. Then, we compute the boundary loss by:",
  "pmask(R(Sbinary) (1 mask)),(3)": "Since (1 mask) is 1 outside the superpixel mask and 0inside the mask, the loss term calculates the area of the pathsthat are outside the superpixel. When some SVG paths crossthe superpixel boundary, the path area outside is penalized;while when all SVG paths are inside the superpixel, LBoundreaches 0 ().3) Path Efficiency Loss: To enable our model to recon-struct the maximum amount of information with the fewestpaths, we propose the path efficiency loss LP E. Specifi-cally, for each path i, an additional opacity parameter i ispredicted. We treat the path as visible if i 0.5, and theloss LP E penalizes the case with more visible paths, i.e.,to encourage reconstructing the image with as few paths aspossible, calculated as:",
  ". Coarse Paths Guided Refinement Stage": "In the coarse reconstruction stage, our coarse-stage modelEc can output an SVG that captures and reconstructs themain structure of the input superpixel x. The rendered im-age from the SVG (denoted as c1) resembles x in general,but lacks some image details, especially when the super-pixel is complex. To enrich image details, we employ a re-finement model Er to predict more SVG paths to add moredetails based on the current canvas c1. Model framework.Different from the coarse-stagemodel Ec that only takes the target superpixel x as input, therefinement model Er takes both the current canvas c1 (ren-dered from the coarse stage output) and the target superpixelx as inputs, and predict new paths S = {s1, s2 sm} tobe overlaid onto the canvas to refine details. Specifically,3 convolution layers followed by ReLU activations are em-ployed to fuse the two input images into a feature map. Af-ter getting the fused feature map, Er shares the same struc-ture as the coarse-stage model Ec, which encodes the fusedfeature map by a ViT Encoder and maps the encoded fea-tures into path parameters by a cross-attention and a self-attention layer. To accelerate the training process, we inheritthe weights of the ViT encoder in Ec as an initialization.Local optimal solution with L2 loss. The goal of the",
  "However, the refinement model Er trained with Eq.(6)": "alone tends to predict paths that are extremely small in area,or even invisible. In , we use an example to illustratethis phenomenon more clearly: we newly add a path ontothe canvas and optimize the path parameters with L2 loss;it can be seen that the new path gradually shrinks and fi-nally disappears in the canvas. A possible reason is that thecoarse stage result c1 is already close to x, and a local op-timal solution for Er is to overlap nothing onto c1, whichis better than adding a sub-optimal path and can prevent theL2 distance from increasing. Coarse paths guided training framework. To avoidthe refinement model from falling into poor local optimum,we propose a coarse paths guided framework, which in-herits the knowledge from the coarse-stage model to helptrain the refinement model with an additional constraint onthe SVG path parameters. As illustrated in , for aninput superpixel x, we first use the coarse-stage model topredict a coarse level path sequence S = {s1, s2 sn}.Then, we randomly choose a value k (1, n m) andsplit the predicted path sequence into two subsequences:S1 = {s1, s2, , sk} and S2 = {sk+1, sk+2, , sn}.The subsequence S1 is then rendered into c1 = R(S1) andused as the input canvas for the refinement model Er, whilethe remaining subsequence S2 can be regarded as a pseudoground truth for the output path sequence of Er. Specif-ically, when training Er, in addition to the previous con-straints that operate in the pixel space, we add a new con-",
  "In our coarse paths guided framework, we expect thegenerated path sequence to be a subsequence of the targetpath sequence. However, in DTW, one generated path sj": "can correspond to several target paths si1, ..., sil. There-fore, when trained with DTW, one generated path tends tobecome the average of several target paths. As shown in(b), when optimizing 3 paths to match the target emoji(with 4 paths), one path becomes the average of the two eye-brow paths, which is not our desired case. To address this issue, we propose Dynamic Path Warping(DPW), where each generated path should match one andonly one target path, and some target paths can be skipped(to learn a subsequence), as shown in (a), each hor-izontal line only passes through one matching point (yel-low). To compute the DPW, we define pi,j as the minimumaccumulated distance when si matches sj, and qi,j as theminimum accumulated distance when sj has been matchedto one path before si (not including si). We employ dy-namic programming to compute the final DPW loss pn,m asshown in Alg. 1. For each pi,j, the distance di,j between siand sj is added to the smaller one of qi,j1 and pi,j1. Andfor each qi,j, its value takes the smaller one between qi1,jand pi1,j (more explanations are provided in the supple-mentary material). Moreover, to make Alg. 1 differentiable,we follow SoftDTW to substitute the min() operation:",
  ". Experiment Setting": "Implementation Details.We use SVG paths composedby cubic Bezier curves as the vector primitive, where eachSVG path is closed, composed of 4 cubic Bezier curves con-nected end-to-end and has a fill color. Each SVG path has28 parameters (24 for shape, 3 for color, and 1 for visibility).The coarse-stage model is trained to predict 128 paths foreach superpixel first. Then, the refinement model is trainedto predict 8 paths at one time. We train both the coarse-stagemodel and refinement model on ImageNet dataset . Weset batch size as 64 and learning rate as 2.5 104. Wetrain the coarse-stage model for 200K iterations with 5Kwarm up iterations, and train the refinement model for 200Kiterations with DP W decreasing from 1 103 to 0 in10K iterations uniformly. In the following experiments, we implement our model with two versions: 1) SuperSVG-B, that decomposes the image into superpixels and vector-izes them by the coarse-stage model and refinement model,and 2) SuperSVG-F, which finetunes the SVG parametersfrom SuperSVG-B with L2 loss, which takes around 10 sec-onds for optimization. All experiments are carried out on anNVIDIA GeForce RTX 4090 24GB GPU. Evaluation Details.For quantitative evaluation andcomparison, we test our model on 1,000 images randomlyselected from ImageNet test set, and convert each imageinto SVGs with 500, 2,000 and 4,000 paths respectively.With the predicted SVG paths, we evaluate the reconstruc-tion accuracy of the output SVG with the following 4 met-rics: 1) MSE Distance and 2) PSNR to measure the pixeldistance between the input image and the rendering fromSVG; 3) LPIPS to evaluate the perceptual distance, and4) SSIM to measure the structural distance. We furthercompare with Im2Vec on EMOJIS dataset .",
  ". Image to SVG Comparison": "State-of-the-art methods.The state-of-the-art methodscan be classified into 3 categories: 1) Algorithm-basedmethods: Potrace employs edge tracing to vectorizebinary images. To process color images, we follow ColorTrace1 to first quantize color images into different layersand then convert each layer to SVG using Potrace. AdobeIllustrator is a widely-used commercial software whichconverts an image into SVG through image tracing.2)Deep-learning-based methods: Im2Vec encodes the tar-get image into latent and predicts the vector paths withLSTM (#suppl.); and 3) Optimization-based methods: Dif-fVG optimizes path parameters with random initial-ization and LIVE employs layer-wise optimization toensure a good vectorization structure. We use the officialcodes of these methods and default settings for comparison. Qualitative Comparison on ImageNet. We comparewith the state-of-the-art vectorization methods in recon-struction accuracy on ImageNet. Specifically, we conductthe comparison experiments under path numbers 500, 2,000and 4,0002. The qualitative results are shown in . Itcan be seen that Potrace cannot reconstruct the image well.LIVE loses a lot of details in relatively smooth areas dueto its emphasis on regions with substantial color variations.DiffVG and Adobe work better when the path number in-creases, but they reconstruct fewer details than our Super-SVG. In comparison, our SuperSVG-B reconstructs mostof the details with a short inference time. And by optimiz-ing the SVG parameters from SuperSVG-B with only 10seconds, our SuperSVG-F achieves the best reconstructionaccuracy under different SVG path numbers.",
  ". Qualitative comparison with the state-of-the-art methods in image vectorization with different number of SVG paths": "Quantitative Comparison on ImageNet. We furtherconduct quantitative comparison on 1,000 images randomlysampled from ImageNet dataset (50 images for LIVEdue to the extremely long optimization time: each input im-age takes about 6 GPU hours to optimize under 500 SVGpaths). The quantitative results are presented in Tab. 1. OurSuperSVG achieves the best image vectorization results.",
  ". Ablation Study": "Ablation Study on the Superpixel-based Framework.We first validate the effectiveness of the superpixel-basedvectorization framework. We train a model that directlypredicts the SVG paths for an input image, without super-pixel segmentation. Then, we test the model in two ways:1) predict the SVG paths for the whole input image and 2)uniformly divide the input image into 4 4 blocks and vec-torize each block separately. We compare our model withthese two models with the same number of paths (1,000).The results are shown in and Tab. 2(a). The modelwithout superpixel segmentation loses many image details.By introducing the block division, the model enriches thedetails, but the regions near block boundaries are discon-tinuous (shown in red box), producing unnatural results.In comparison, our superpixel-based SuperSVG-B recon-",
  ". Ablation study on the coarse-stage model. The ablatedmodels either predict paths crossing superpixel boundaries or re-construct less details than ours": "the path efficiency loss LP E in the coarse-stage model. Wetrain 2 ablated models: 1) the model without LBound; and2) the model without LP E, and compare them with ourcoarse-stage model under 500 SVG paths. In this compar-ison, we only compare vectorization using the coarse-stagemodel, without using the refinement model. The results areshown in and Tab. 2(b). The model without LBoundpredicts some paths that cross superpixel boundaries, re-sulting in worse reconstruction. The model without LP Ehas a poorer performance, which is validated by the met-ric results. In comparison, our model outperforms the twoablated models, validating the effectiveness of the lossesLBound and LP E in the coarse-stage model. Ablation Study on the Refinement-stage Model. Fi-nally, we validate the effectiveness of the refinement stageand the DPW loss. We train 3 ablated models: 1) the modelwithout the refinement stage3; 2) the model without theDPW loss LDP W (i.e., without coarse paths guidance, withpixel-wise loss only); and 3) the model replacing DPW lossLDP W with L2 loss in path parameter space. The resultsare shown in and Tab. 2(c). The model without refine-ment-stage cannot reconstruct as many details as ours. Theablated model without LDP W predicts paths with a verysmall area or even invisible, as explained in Sec.3.2, thusthe results look similar to the coarse-stage results. For the",
  "(c) Ablation on refinement-stage model": "ablated model replacing DPW loss LDP W with L2 in pathparameter space, which enforces one-to-one alignment be-tween two paths control points, since the constraint is toostrict, the loss cannot function well in experiments, and theresults look alike the results without LDP W . In compari-son, our model outperforms the ablated models, validatingthe effectiveness of the refinement and DPW loss.",
  ". Conclusion": "We propose SuperSVG, a novel superpixel-based vector-ization model that decomposes a raster image into super-pixels and then vectorizes each separately, achieving fastand accurate image vectorization. We propose a two-stageself-teaching framework, where a coarse-stage model re-constructs main structure and a refinement model enrichesdetails, with a novel dynamic path warping loss that guidesthe refinement model by inheriting knowledge from coarsepaths. Extensive experiments demonstrate that SuperSVGachieves the state-of-the-art performance on vectorization.",
  "Pixabay. 14 PyTorch Image Models. 13": "Radhakrishna Achanta and Sabine Susstrunk. Superpixelsand polygons using simple non-iterative clustering. In Pro-ceedings of the IEEE conference on computer vision and pat-tern recognition, pages 46514660, 2017. 13, 14 R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, andSabine Susstrunk. SLIC superpixels compared to state-of-the-art superpixel methods. IEEE Transactions on PatternAnalysis and Machine Intelligence, page 22742282, 2012.2, 4, 13",
  "Wen Dai, Tao Luo, and Jianbing Shen. Automatic imagevectorization using superpixels and random walkers. In 20136th International Congress on Image and Signal Processing(CISP), 2013. 2": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. ImageNet: A large-scale hierarchical imagedatabase. In Proceedings of the IEEE Conference on Com-puter Vision and Pattern Recognition, pages 248255, 2009.6, 7 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 1616 words: Trans-formers for image recognition at scale.arXiv preprintarXiv:2010.11929, 2020. 4, 13",
  "David Ha and Douglas Eck.A neural representation ofsketch drawings. Learning,Learning, 2017. 1": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 2 Teng Hu, Ran Yi, Haokun Zhu, Liang Liu, Jinlong Peng,Yabiao Wang, Chengjie Wang, and Lizhuang Ma. Stroke-based neural painting and stylization with dynamically pre-dicted painting region.In Proceedings of the 31st ACMInternational Conference on Multimedia, pages 74707480,2023. 1",
  "Yu-Kun Lai, Shi-Min Hu, and Ralph R. Martin. Automaticand topology-preserving gradient mesh generation for imagevectorization. ACM Transactions on Graphics, 28(3):18,2009. 1, 2": "A. Levinshtein, A. Stere, K.N. Kutulakos, D.J. Fleet, S.J.Dickinson, and K. Siddiqi. TurboPixels: Fast superpixels us-ing geometric flows. IEEE Transactions on Pattern Analysisand Machine Intelligence, page 22902297, 2009. 2 Tzu-Mao Li, Michal Lukac, Michael Gharbi, and JonathanRagan-Kelley.Differentiable vector graphics rasterizationfor editing and learning.ACM Transactions on Graphics(TOG), 39(6):115, 2020. 1, 2, 3, 4, 6, 7, 14 Zhengqin Li and Jiansheng Chen.Superpixel segmenta-tion using linear spectral clustering. In Proceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 13561363, 2015. 13, 14",
  "Chen Liu, Jiajun Wu, Pushmeet Kohli, and Yasutaka Fu-rukawa. Raster-to-vector: Revisiting floorplan transforma-tion. In 2017 IEEE International Conference on ComputerVision (ICCV), 2017. 1, 2": "Xu Ma, Yuqian Zhou, Xingqian Xu, Bin Sun, Valerii Filev,Nikita Orlov, Yun Fu, and Humphrey Shi. Towards layer-wise image vectorization. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 1631416323, 2022. 1, 2, 3, 6, 7, 14 Pradyumna Reddy, Michael Gharbi, Michal Lukac, andNiloy J Mitra. Im2Vec: Synthesizing vector graphics withoutvector supervision. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages73427351, 2021. 1, 2, 3, 6, 13 Leo Sampaio Ferraz Ribeiro, Tu Bui, John Collomosse, andMoacir Ponti. Sketchformer: Transformer-based representa-tion for sketched structure. In 2020 IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), 2020.1",
  "Sebastian Thrun and James Diebel. Bayesian image vector-ization: the probabilistic inversion of vector image rasteriza-tion. 2008. 1, 2": "Wei-Chih Tu, Ming-Yu Liu, Varun Jampani, Deqing Sun,Shao-Yi Chien, Ming-Hsuan Yang, and Jan Kautz. Learn-ing superpixels with segmentation-aware affinity loss.In2018 IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 568576, 2018. 2 Michael Van den Bergh, Xavier Boix, Gemma Roig, Ben-jamin De Capitani, and Luc Van Gool. SEEDS: Superpixelsextracted via energy-driven sampling. In Computer VisionECCV 2012: 12th European Conference on Computer Vision(ECCV), pages 1326. Springer, 2012. 2, 13, 14 Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-moncelli. Image quality assessment: from error visibility tostructural similarity. IEEE Transactions on Image Process-ing, 13(4):600612, 2004. 6, 13",
  "Guofu Xie, Xin Sun, Xin Tong, and Derek Nowrouzezahrai.Hierarchical diffusion curves for accurate automatic imagevectorization. ACM Transactions on Graphics, 33(6):111,2014. 2": "Fengting Yang, Qian Sun, Hailin Jin, and Zihan Zhou. Su-perpixel segmentation with fully convolutional networks. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 1396413973, 2020. 13,14 Ming Yang, Hongyang Chao, Chi Zhang, Jun Guo, Lu Yuan,and Jian Sun. Effective clipart image vectorization throughdirect optimization of bezigons. IEEE Transactions on Vi-sualization and Computer Graphics, page 10631075, 2016.1 Zipeng Ye, Ran Yi, Minjing Yu, Yong-Jin Liu, and Ying He.Fast computation of content-sensitive superpixels and super-voxels using q-distances. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 37703779, 2019. 2 Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-man, and Oliver Wang. The unreasonable effectiveness ofdeep features as a perceptual metric.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 586595, 2018. 6, 13",
  "B. Details about Our Dynamic Path Warping": "Problem Insight.Given the generated path sequenceS = {s1, s2 sm} and the target path sequence S ={s1, s2 sn}, we aim to find the distance between thetwo path sequences in path parameter space. Denote thedistance matrix between each path pair of S and S asD = {di,j}nm, with di,j being the distance between siand sj. Our DPW aims to find an optimal matching func-tion match(j) that minimizes the objective function:",
  "where match(j) match(j 1)4": "We transform the problem into finding an optimal pathin a Cartesian grid. In , the distance matrix can berepresented as an m n grid, where the yellow point cor-responds to one mapping of match(j) = i, while the blackpoint indicates non-matching (i.e., not added in the objec-tive function), with the yellow point denoted as (i, j, 1) andthe black point denoted as (i, j, 0). Therefore, any match-ing function match(j), where 1 j m, can be rep-resented by m yellow points in the grid, with each yel-low point in a different row. Considering every two adja-cent rows have two yellow points (match(j 1), j 1, 1)and (match(j), j, 1), with match(j 1) match(j), byadding black points between the two yellow points whenthey are not adjacent, the m yellow points and the addedblack points can form a path in the grid that starts fromthe bottom left corner to the top right corner, which onlyconsists of rightward, upward, and diagonal up-right move-ments, and can be further computed by dynamic program-ming. In this way, the problem of finding an optimal match-ing function is transformed to finding an optimal path in thedistance grid.",
  ") The path contains both the yellow points (i, j, 1) that": "represent match(j) = i and black points (i, j, 0) that rep-resent match(j) = i.3) For each point in the path, it can only move to theright, upward, or diagonally upward to the right.It can be easily seen that the final value of DPW only de-pends on the yellow points (matching points), while the val-ues of black points (non-matching points) are not counted inthe objective function. Then, we can simplify the form ofthe path for DPW based on the following properties.Property 1. We can ignore movements that are diago-nally upwards to the right (i.e., from (i, j) to (i + 1, j + 1)). Explanation 1. For any diagonal up-right movementfrom colored point (i, j) (black or yellow) to point (i +1, j + 1) ((a)), it is equal to first moving to the rightto the black point (i + 1, j, 0) and then moving upwardsto (i + 1, j + 1): By adding the intermediate black point(i + 1, j, 0), 1) the colors of the two endpoints (i, j), and(i+1, j +1) are not changed, and 2) the intermediate blackpoint (i + 1, j, 0) does not contribute to the value of the",
  "(i, j+1, 1)": ". Path simplification for DPW. (a) Property 1: Themovement from (i, j) to (i+1, j +1) can be simplified to movingfrom (i, j) rightward to (i+1, j, 0) (black point) and then movingupward to (i+1, j+1). (b) Property 2: Although there are multi-ple paths (composed of n black points) between two yellow points(i, j, 1) and (i, j + 1, 1) in adjacent rows, they are equivalent interms of their objective function values. Therefore, we can ignorethe transition modes represented by the dashed arrows, and onlyneed to consider the solid arrows.",
  "Property 2. We can consider only four possible statetransition modes: (i, j, 1) (i, j + 1, 1), (i, j, 1) (i +1, j, 0), (i, j, 0) (i + 1, j, 0), and (i, j, 0) (i, j + 1, 1)": "Explanation 2.For two yellow points (i, j, 1) and(i, j + 1, 1) in adjacent rows, their positional relationshipcan be summarized into (b), where the number ofblack points n between them is larger than or equal to 06.When n = 0, (i, j, 1) just moves upward to (i, j + 1, 1).When n > 0, the paths between (i, j, 1) and (i, j + 1, 1)can be any composition of the dashed and solid arrows thatmoves from (i, j, 1) to (i, j + 1, 1) in (b), i.e., thereare different ways to add black points between the two yel-low points to connect into a path. However, all the possiblepaths contribute to the same result since the value of theDPW objective function only depends on the yellow points.Considering the different paths between these two yellowpoints are equal in objective function values, we can sim-plify the paths by ignoring the transition modes representedby the dashed arrows, and only need to consider the tran-sition denoted by solid arrows. Therefore, there are only 4remaining state transition modes ((b)):",
  "validvalid": ". The illustration of valid paths and invalid paths afterpath simplification for DPW. There are 8 transition modes in total.But according to the Properties 1 and 2, half of the modes canbe ignored (set as invalid), which greatly simplifies the dynamicprogramming process. (1) (i, j, 1) (i, j + 1, 1) (yellow upward to yellow),(2) (i, j, 1) (i + 1, j, 0) (yellow rightward to black),(3) (i, j, 0) (i + 1, j, 0) (black rightward to black),(4) (i, j, 0) (i, j + 1, 1) (black upward to yellow).Dynamic Programming Solution. With the simplifiedstate transition modes, we can compute our DPW by dy-namic programming. Specifically, we define pi,j as the min-imum accumulated distance when going from the start point(1, 1) to the yellow (matching) point (i, j, 1), and qi,j as theminimum accumulated distance when going from the startpoint (1, 1) to the black (non-matching) point (i, j, 0). Ac-cording to Properties 1 and 2, there are only 4 state transi-tion modes:",
  "C. More Details for the Experiments": "More Implementation Details. In the experiments of themain paper, we have evaluated the performance of Super-SVG under different numbers of paths. For a certain num-ber of paths n, we assign about half of the paths to thecoarse-stage model and half to the refinement-stage model.Specifically, with our path efficiency loss LP E, our coarse-stage model predicts around 32 visible paths for a super-pixel on average. Therefore, we decompose the target im-age into n1 =n 232 superpixels and employ the coarse-stage model to predict SVG paths for each of them. Wecombine all the visible paths output from the coarse-stagemodel, and employ the refinement-stage model to add morepaths onto each superpixel repeatedly until the total pathnumber reaches n.Evaluation Metrics. In the experiments, we use fourmetrics to evaluate the vectorization results, comparing therendered image of the output SVG to the target image:",
  "(1) MSE Distance: Mean Squared Error (MSE) is a": "widely used metric in image processing to assess the qualityof image reconstruction. It measures the average squareddifference between the original and reconstructed images,with lower MSE values indicating better image fidelity.(2) PSNR: The Peak Signal-to-Noise Ratio (PSNR) is one of the most prevalent and extensively utilized metricsfor assessing image quality. A higher PSNR value indicatesa superior quality of image reconstruction.(3) LPIPS: The Learned Perceptual Image Patch Sim-ilarity (LPIPS) is a perceptual metric utilized for as-sessing the similarity between two images. A lower LPIPSvalue indicates a higher similarity between the output imageand the target image.(4) SSIM: Structure Similarity Index Measure (SSIM) is derived from three aspects of image similarity: lu-minance, contrast and structure, based on the idea that thepixels have strong inter-dependencies especially when theyare spatially close. The higher the SSIM score is, the moresimilar the two images are.Network Architecture. Our Coarse-stage model con-sists of three modules: one vision transformer encoder;one cross-attention module; and one self-attention module.1) The vision transformer encoder employs the ViTimplementation from PyTorch Image Models (timm) ,which takes an 224 224 image as input and splits the im-age into patches (tokens) with size 16 16. 2) The cross-attention module takes the encoded feature as the Key andValue, and takes the learnable path queries as the Query.Then the cross-attention module is followed by a two-layerMLP with GELU activation. 3) Moreover, the self-attention",
  ". Comparison on EMOJIs . We achieve the best re-construction accuracy and highest speed (paths number 4 and 20)": "module is employed to further process the output from thecross-attention layer to project the image features into pathparameters. And the self-attention module is also followedby a two-layer MLP with GELU activation. Our Refinement-stage model first employs a three-layerconvolution network with 3 3 convolution kernels to en-code the current canvas and target superpixel into a 3 224 224 feature map. And then it employs the same net-work as the coarse-stage model to project the image featuresinto 128 27-dimension output. At last, a fully connectedlayer is employed to map it into path parameters with 827dimension. For more details, please refer to the code pro-vided in the supplementary material.",
  "E. More Ablation Studies": "Ablation on Different Superpixel Methods.We con-duct ablation studies on using different superpixel methods:we compare with using the representative superpixel meth-ods including LSC , SEEDS , SpixelFCN andSNIC , as well as SLIC with different compactness(10, 20 and 30), to decompose the target image into super-pixels, and then vectorize each superpixel separately. Thecomparison results with 1,000 SVG paths are shown in Ta-ble 3. It can be seen that SLIC works better with our pro-posed SVG synthesis framework, and a higher compactnessin SLIC results in a better performance.Ablation on Self-attention Module. We further con-duct ablation studies on the number of self-attention mod-ules in both our coarse-stage and refinement-stage models.In our model design, we employ one self-attention moduleeach in coarse-stage and refinement-stage models. Then, wetrain 3 additional versions for each of the coarse-stage andrefinement-stage models with 2, 4 and 8 self-attention mod-ules respectively. The comparison results with 1,000 SVGpaths are shown in . It can be seen that, as the num-ber of the self-attention modules increases, the performance",
  "F. Additional Comparison Experiments": "In this section, we show more comparison results withthe state-of-the-art vectorization methods, LIVE , Dif-fVG , Adobe and Potrace under 500, 2,000 and4,000 SVG paths. The results are shown in Figures 1416.It can be seen that under the same number of SVG paths,our SuperSVG can reconstruct more details than the othermethods in both the foreground and the background regions.",
  "G. More Results of Our Method": "To show the effectiveness of our model, we show more ex-perimental results on high-resolution in-the-wild data col-lected from the Internet . We vectorize all the testimages into 4,000 SVG paths using our SuperSVG-B andSuperSVG-F, and the results are shown in Figures 1720. Itcan be seen that our SuperSVG achieves a good vectoriza-tion quality with rich details."
}