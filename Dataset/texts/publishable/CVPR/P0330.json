{
  "Abstract": "Text-to-image person re-identification (ReID) retrievespedestrian images according to textual descriptions. Man-ually annotating textual descriptions is time-consuming, re-stricting the scale of existing datasets and therefore the gen-eralization ability of ReID models. As a result, we study thetransferable text-to-image ReID problem, where we train amodel on our proposed large-scale database and directlydeploy it to various datasets for evaluation.We obtainsubstantial training data via Multi-modal Large LanguageModels (MLLMs). Moreover, we identify and address twokey challenges in utilizing the obtained textual descriptions.First, an MLLM tends to generate descriptions with sim-ilar structures, causing the model to overfit specific sen-tence patterns. Thus, we propose a novel method that usesMLLMs to caption images according to various templates.These templates are obtained using a multi-turn dialoguewith a Large Language Model (LLM). Therefore, we canbuild a large-scale dataset with diverse textual descrip-tions. Second, an MLLM may produce incorrect descrip-tions. Hence, we introduce a novel method that automat-ically identifies words in a description that do not corre-spond with the image. This method is based on the similar-ity between one text and all patch token embeddings in theimage. Then, we mask these words with a larger probabilityin the subsequent training epoch, alleviating the impact ofnoisy textual descriptions. The experimental results demon-strate that our methods significantly boost the direct trans-fer text-to-image ReID performance. Benefiting from thepre-trained model weights, we also achieve state-of-the-artperformance in the traditional evaluation settings.",
  "The person in the image is a woman with brown hair, wearing a red jacket, grey pants and white shoes. She has a handbag with her": ".Illustration of textual descriptions generated by anMLLM (i.e., Qwen ). (Top) The description patterns are sim-ilar for different images. (Bottom) Our proposed Template-basedDiversity Enhancement (TDE) method significantly enhances thedescription pattern diversity. It is worth noting that some errorsare present in the generated descriptions shown in this figure. images according to textual descriptions. It is a powerfultool when probe images of the target person are unavail-able and only textual descriptions exist. It has various po-tential applications, including video surveillance , socialmedia analysis , and crowd management . How-ever, it remains challenging mainly because annotating tex-tual descriptions for pedestrian images is time-consuming. Consequently, existing datasets for text-to-image person ReID are usually small, resulting in insuf-ficient deep model training.Previous studies on text-to-image ReID usually assumedthat training and testing data are drawn from the same do-main. They proposed novel model architectures , loss functions , and pre-trainingstrategies to improve model performance for eachdatabase. However, researchers have recently discovered",
  "arXiv:2405.04940v3 [cs.CV] 1 Jul 2024": "that the cross-dataset generalization ability of their ap-proaches is significantly low , limiting real-world ap-plications. Since annotating textual descriptions is time-consuming, collecting training data for each target domainis infeasible. Therefore, training a model that can be di-rectly deployed to various target domains is necessary.Accordingly, we study the transferable text-to-imageReID problem.The term transferable is derived fromthe seminal work CLIP , which refers to a large-scalepre-trained models capacity that directly applies its knowl-edge to other domains or tasks without fine-tuning on la-beled data. Due to the rapid advancements in multi-modallarge language models (MLLMs) , we utilizethem to generate textual descriptions automatically and em-ploy them to replace traditional manual annotations. Specif-ically, we utilize the large-scale LUPerson dataset asthe image source and generate textual descriptions usingMLLMs. The obtained image-text pairs are utilized to traina model directly evaluated in existing text-to-image ReIDdatabases. However, to improve the models transfer abil-ity, two essential challenges must be addressed: (1) guidingMLLMs to generate diverse textual descriptions for a sin-gle image and (2) reducing the impact of the noise in thesynthesized textual descriptions.First, MLLMs tend to generate descriptions with simi-lar sentence structures, as shown in . This causesthe text-to-image ReID model to overfit specific sentencepatterns, reducing the models ability to generalize to vari-ous human description styles encountered in real-world ap-plications. To address this issue, we propose a Template-based Diversity Enhancement (TDE) method that instructsMLLMs to conduct image captioning according to givendescription templates. Obtaining these templates with min-imal effort involves performing multi-turn dialogues withChatGPT and prompting it to generate diverse tem-plates. Then, we randomly integrate one of these templatesinto the MLLMs captioning instruction, resulting in vividdescriptions with varied sentence structures. This approachsignificantly enhances textual description diversity.Second, although MLLMs are highly effective, the gen-erated descriptions still contain errors. This implies thatcertain words in a textual description may not match thepaired image. Thus, we propose a novel Noise-aware Mask-ing (NAM) method to address this problem. Specifically,we compute the similarities between each text token andall image tokens in the paired image for a specific textualdescription. The similarity scores between the unmatchedword and image tokens are usually low. Hence, we iden-tify potentially incorrect words and mask them with a largeprobability in the next training epoch before they are fedinto a text encoder. Furthermore, NAM and Masked Lan-guage Modeling (MLM) are similar but have two key dif-ferences: (1) MLM masks all tokens with equal probabil- ity, while NAM masks them based on their noise level. (2)MLM applies cross-entropy loss to predict the masked to-kens, whereas NAM focuses on masking words without pre-dicting potentially noisy words. In the experimentation sec-tion, we demonstrate NAMs ability to effectively alleviatethe impact of noisy textual descriptions.To the best of our knowledge, this is the first study fo-cusing on the transferable text-to-image ReID problem byharnessing the power of MLLMs. We innovatively gener-ate diverse textual descriptions and minimize the impact ofthe noise contained in these descriptions. The experimentalresults show that our method performs excellently on threepopular benchmarks in both direct transfer and traditionalevaluation settings.",
  ". Related Works": "Text-to-Image Re-Identification. Existing approaches forthis task improve model performance from three perspec-tives: model backbone , feature alignment strategies, and pre-training .The first method category improves the model back-bone. Early approaches adopted the VGG model and LSTM as image and text encoders, re-spectively. These encoders gradually evolve into ResNet-50 and BERT mod-els. Moreover, the CLIP and ALBEF-based en-coders have recently become popular. Notably,the CLIP model contains jointly pre-trained image and textencoders. Thus, its cross-modal alignment capabilities areadvantageous and have proven more effective than the indi-vidually pre-trained encoders . Moreover, the ALBEFmodel performs interaction between visual and textualfeatures, which improves the feature representation capac-ity but brings in significant computational cost.The second category of methods enhances feature align-ment strategies. Previous methods aligned an images holis-tic features with its textual description . Subsequent approaches focused on aligning the image-text pairs localfeatures to suit the fine-grained retrieval nature of text-to-image ReID. These approaches can be divided into explicitand implicit alignment methods. Explicit methods extract the visual- and textual-part features and then com-pute the alignment loss between them. Implicit methods canalso align local features . For example, Jiang etal. applied MLM to text tokens and then predicted themasked tokens using image token features. This indirectlyrealizes local feature alignment between the image patchand noun phrase representations.Since existing databases are small, two recent studies ex-plored pre-training for text-to-image ReID. Shao et al. utilized the CLIP model to predict the attributes of a pedes-trian image. Then, they inserted these attributes into man- ually defined description templates. As a result, they ob-tained a large number of pre-training data. Similarly, Yanget al. utilized the text descriptions from the CUHK-PEDES and ICFG-PEDES datasets to synthesizeimages using a diffusion model . Then, they used theBLIP model to caption these images and obtain a large-scale pre-training dataset. However, these two studies tar-geted at pre-training and did not investigate the direct trans-fer setting where no target domain data is available for fine-tuning. Moreover, they overlooked the noise or diversityissues generated in the obtained textual descriptions.The above methods achieve excellent in-domain perfor-mance; however, their cross-dataset performance is usuallysignificantly low . This paper explores the transferabletext-to-image ReID task with minimal manual operations.Also, we address the challenges in textual descriptions gen-erated by MLLMs.Multi-modal Large Language Models. Multi-modalLarge Language Models (MLLMs) arebuilt on Large Language Models (LLMs) and incorporate textual and non-textual information as in-put . This paper only considers MLLMs that useboth texts and images as input signals. The input text (i.e.,the instruction or prompt) describes the tasks assignedto MLLMs to understand the images content. RegardingMLLM architecture, most studies first map theimage patch and text token embeddings into a shared fea-ture space and then perform decoding using a LLM. Somemethods improve the interaction and alignment strate-gies between the image and text tokens during decoding,facilitating more stable training .In this paper, we utilize MLLMs to eliminate the needto manually annotate textual descriptions. We also explorestrategies to address the diversity and noise issues in theobtained textual descriptions, facilitating the developmentof a transferable text-to-image ReID model.",
  ". Generating Diverse Descriptions": "Manually annotating textual descriptions for pedestrian im-ages is time-consuming and hardly scalable. Fortunately,MLLMs have advanced rapidly and provide effective im-age captioning. Therefore, we decide to utilize MLLMs tocreate large-scale text annotations for training a model withexcellent transfer capacity. Instruction Design. We adopt the LUPerson database as the image source because it holds a significantamount of images that were captured in diverse environ-ments. A technical aspect of using MLLMs lies in design-ing an effective instruction, which usually depends on userexperience. We solve this problem using a multi-turn di-alogue with ChatGPT , and this process is detailed inthe supplementary material. The resulting instruction is asfollows:Write a description about the overall appearance ofthe person in the image, including the attributes: clothing,shoes, hairstyle, gender and belongings. If any attribute isnot visible, you can ignore it. Do not imagine any contentsthat are not in the image.This is considered a static instruction as it is fixed forall images. In this paper, the textual descriptions generatedusing the static instruction are denoted as static texts or T s.Diversity Enhancement. An MLLM generates textualdescriptions with similar sentence patterns for different im-ages using the static instruction, as illustrated in .This causes the text-to-image ReID model to overfit thesesentence patterns, limiting its generalization to real-worlddescriptions. We attempt to improve the static instruction,but the obtained sentence patterns remained limited. Al-though using more MLLMs can bring in multiple sentencepatterns, these patterns are still far from diverse.Again, we resort to ChatGPT to solve this problem.Specifically, we propose a Template-based Diversity En-hancement (TDE) method. First, we generate two descrip-tions for each of a set of images using two MLLMs according to the static instruction.Then, we feed thesedescriptions to ChatGPT to capture their sentence patterns(i.e., description templates).With the guidance of thesetemplates, we instruct ChatGPT to create more templates.Finally, it produces 46 templates after multi-turn dialogues,which are detailed in the supplementary material. We ran-domly select one of the templates and insert it into the staticinstruction, obtaining a dynamic instruction as follows:Generate a description about the overall appearance ofthe person, including clothing, shoes, hairstyle, gender, andbelongings, in a style similar to the template: {template}.If some requirements in the template are not visible, you canignore them. Do not imagine any contents that are not in theimage.The {template} is replaceable. Furthermore, the tex-tual descriptions generated according to the dynamic in-struction are referred to as dynamic texts (T d). As illus-trated in , MLLMs can follow the sentence patternsspecified in the templates, significantly enhancing the diver-sity of the obtained textual descriptions.Dataset Description. We utilize the publicly availableQwen and Shikra models in this paper.By har-nessing the power of the two MLLMs, we obtain the large-",
  "rE +": ". Overview of our framework. We adopt the CLIP-ViT/B-16 model as the backbone. Our framework uses one pedestrian image,the original textual description T full, and a masked textual description T nam as input during training. T nam is obtained by applyingNAM to T full. To perform NAM, we first compute the similarity matrix S between the text tokens Ft of T full and the image tokens Fvaccording to their embeddings at the l-th layer of the encoders. Then, we estimate the probability of each text tokens noisiness accordingto the similarity between its embedding and the image token embeddings. The similarity distribution matching (SDM) loss is computedbetween the global visual feature vcls of the pedestrian image and the global textual feature teos of T nam. The models optimizationquality is enhanced by masking noisy words in T full. (Best viewed in color.) scale LUPerson-MLLM dataset. This dataset comprises 1.0million images, and each image has four captions, T sqwen,T sshikra, T dqwen, and T dshikra. The first and the last two cap-tions are generated according to the static and dynamic in-structions, respectively. We reserve the T s for each imageas we observe that its description is usually complementaryto that of T d. In the following section, we will train themodel with LUPerson-MLLM. For simplicity, we refer allthe above MLLM-generated descriptions as T full.",
  ". Noise-Aware Masking": "Although MLLMs are powerful, they cannot describe im-ages very precisely. As depicted in and , a fewwords do not match the described image in the obtained tex-tual descriptions. Existing methods usually discardthe noisy descriptions, losing the other valuable informationcontained in the matched words. Accordingly, we proposea novel noise-aware masking (NAM) method that identifiesnoisy text tokens and fully uses the matched text tokens formodel training.Image Encoder.An image is divided into M non-overlapped patches. These image tokens are concatenatedwith the [CLS] token and are fed into the image encoder. Then, the [CLS] token embedding at the last image en-coder layer is used as the global image feature, denoted asvcls Rd. The feature dimension is represented by d.Text Encoder.We tokenize each textual descriptionT full into a sequence of N tokens. The N of each sen-tence varies according to its length. The token sequence isbracketed with [SOS] and [EOS] to represent the start andthe end of the sequence. Meanwhile, we examine each texttokens noise level in T full, which is computed and storedin the previous training epoch. These values are used toperform NAM on T full to obtain T nam. After that, T full",
  "and T nam are fed into the text encoder independently. Atthe final text encoder layer, the global feature teos of T nam": "is utilized to calculate loss. T full is only used for NAM,which means it is not used for loss computation.Noise-Aware Masking. We utilize the image and textencoders token embeddings in the l-th layers for the noise-level estimation of T full. These embeddings are denoted asFv = [vl1, ..., vlM] and Ft = [tl1, ..., tlN], respectively, wherevlj Rd and tlj Rd.Furthermore, we calculate the token-wise similarity be-tween a single text-image pair as follows:",
  "S = FtT Fv,(1)": "where S RNM is a similarity matrix and sij representsthe cosine similarity between the i-th text token embeddingand the j-th image token embedding. If one text token doesnot match the image, the similarity scores between this to-kens embedding and those of all the image tokens will beconsistently be low. Therefore, the noise level of the i-thtext token in T full can be estimated via:",
  "ri = 1 ( max1jM sij).(2)": "By applying Eq.(2) to each row of S, we obtain a vector r =[r1, ..., rN] that records the noise-level of all text tokens.Moreover, NAM applies the masking operation to all thetext tokens in T full with different probabilities, which canbe determined based on the noise-level values recorded inr. However, in the initial training stage, the values of ele-ments in r may be high. This results in excessive maskingof important tokens and hinders learning. To resolve this is-sue, we modify the expectation value of all r elements intoa constant number as described below:",
  "where p is the average masking ratio. We utilize the r": "values as the final probability that a text token might bemasked. We include the pseudo code and visualization ofNAM in the supplementary materials.Discussion. Computing r and then applying NAM toobtain T nam in each iteration requires two forward passes.This additional time cost cannot be overlooked in large-scale training. In contrast, our strategy computes r for thenext training epoch, which requires only one forward passfor each iteration. Furthermore, we initialize the r valueswith the constant p in the first training epoch.",
  ". Optimization": "Following , we adopt the similarity distribution match-ing (SDM) loss to optimize our model.Given a mini-batch of B matched image-text pairs {(vicls, tieos)}Bi , wefirst establish the matching relationship between each im-age and text (i.e., {(vicls, tjeos), yi,j}(1 i, j B)), whereyi,j = 1 and yi,j = 0 denote a positive and a negativeimage-text pair, respectively. Then, we calculate the groundtruth matching distribution qi for the i-th image, where itsj-th element is qi,j = yi,j/ Bb=1 yi,b. Finally, we align thepredicted probability distribution pi with qi as follows:",
  ". Datasets and Settings": "CUHK-PEDES. CUHK-PEDES is a pioneer dataset inthe text-to-image ReID field. Each image in this dataset hastwo textual descriptions. The training set comprises data on11,003 identities, including 34,054 images and 68,108 tex-tual descriptions. In contrast, the testing set contains 3,074images and 6,156 textual descriptions from 1,000 identities.ICFG-PEDES. ICFG-PEDES contains of 54,522images from 4,102 identities. Each image has one textualdescription. The training set consists of 34,674 image-textpairs corresponding to 3,102 identities, while the testingset comprises 19,848 image-text pairs from the remaining1,000 identities.RSTPReid.RSTPReid includes 20,505 imagescaptured by 15 cameras from 4,101 identities. Each identityhas five images captured with different cameras and eachimage has two textual descriptions. According to the of-ficial data division, the training set incorporates data from3,701 identities, while both the validation and testing setsinclude data from 200 identities, respectively.LUPerson. LUPerson contains 4,180,243 pedes-trian images sampled from 46,260 online videos, coveringa variety of scenes and view points. The images are fromover 200K pedestrians.Evaluation Metrics. Like existing works ,we adopt the popular Rank-k accuracy (k=1,5,10) and meanAverage Precision (mAP) as the evaluation metrics for thethree databases. Moreover, we consider the following twoevaluation settings.Direct Transfer Setting.For this setting, the modelis only trained on the LUPerson-MLLM dataset, and theabove three benchmarks are tested immediately. This set-ting directly evaluates the quality of our dataset and the ef-fectiveness of the proposed methods (i.e., TDE and NAM).",
  ". Implementation Details": "Similar to previous studies , we adopt CLIP-VIT-B/16 as the image encoder and a 12-layer transformeras our text encoder. The input image resolution is resized to384 128 pixels. Additionally, we apply random horizon-tal flipping, random cropping, and random erasing as dataaugmentation for the input images. Each textual descriptionis first tokenized, with a maximum length of 77 tokens (in-cluding the [SOS] and [EOS] tokens). The hyper-parameterp is set to 0.15 and the temperature coefficient in Eq. (6)is set to 0.02. The model is trained using the Adam opti-mizer with a learning rate of 1e-5 and cosine learning ratedecay strategy. We train each model on 8 TITAN-V GPUs,with 64 images per GPU. The training process lasts for 30epochs. The versions of the mentioned LLM/MLLMs areChatGPT-3.5-Turbo, Qwen-VL-Chat-7B, and Shikra-7B.",
  ". Ablation Study": "We randomly sample 0.1 million images from ourLUPerson-MLLM database to accelerate the ablation studyon the direct transfer evaluation setting. Then, we increasethe amount of training images to 1.0 million to enhance thetransfer ability of our text-to-image ReID models.Effectiveness of TDE. The experiments in showthat dynamic instruction is better than static instruction. Forexample, the model using only T dqwen outperforms that theone using T sqwen by about 3% in Rank-1 performance on theCUHK-PEDES database. On the same database and evalu-ation metric, the model that uses only T dshikra outperformsthe one using T sshikra by about 4%. These experimental re-sults indicate that enhancing sentence pattern diversity im-proves the transfer ability of ReID models. Therefore, weuse the four descriptions for each image in the subsequentexperiments. It is worth noting that none of the above exper-iments employ NAM. Instead, they mask every text tokenwith an equal probability of p. Effectiveness of NAM. MLLM-generated textual de-scriptions often contain noise, which is harmful for modeltraining. Replacing the equal masking strategy with ourNAM method improves our models Rank-1 performanceby 2.32%, 3.49%, and 2.05% on the three databases, re-spectively. These improvements are even higher than thebenefits of combining dynamic and static texts (i.e., 1.46%,0.69%, and 1.45%).These experimental results demon-strate that NAM identifies the noisy words in the text andeffectively reduces their impact. NAM allows the model toaccurately align visual and textual features, thereby enhanc-ing the direct transfer text-image ReID performance.The Layer where NAM Computes S. S contains pair-wise similarity scores between features in Fv and Ft. Thisexperiment investigates the optimal layer for obtaining Fvand Ft. The results are plotted in . We observe thatthe models performance consistently improves regardlessof the layer used to provide Fv and Ft. We also notice thatthe adopted encoders 10-th layer yields the best overall per-formance. Compared to the last encoder layer, the 10-thlayer may offer more fine-grained information, facilitatingmore accurate similarity computation between token pairs.The Overall Masking Ratio for NAM. Our NAMmethod masks different text tokens with unequal probabili-ties, but it maintains an overall probability of p. In this ex-periment, we explore the optimal p value. To demonstrateNAMs advantages, we also include the results of the mask-ing tokens with equal probabilities (referred to as EM).As shown in , NAM consistently outperforms EMwith various p values. The optimal value of p is about 0.15.Combination of NAM and MLM. MLM requires themodel to predict the masked text tokens. It has proven ef-fective and is widely applied in NLP models. Recent text-to-image ReID studies confirm that MLM loss is bene-ficial when the textual descriptions are manually annotated.However, our NAM doesnt predict the masked tokens asthe textual descriptions generated by MLLMs may be noisy. shows that applying MLM loss to NAM is harmful,indicating the MLLM description noise is a crucial issue.The Data Size Impact. The dataset size is essential to 51.81 52.24 52.12 52.64 51.98 51.80 50.32 49.5 50.5 51.5 52.5",
  "Ours (0.1 M)52.6446.4832.6116.5347.7534.73Ours (1.0 M)57.6151.4438.3620.4351.5037.34": "training. More pre-trained data improves the performance.We investigate the effect of training data size on the di-rect transfer ReID performance and summarize the resultsin . It is evident that the models direct transfer per-formance steadily improves as the data amount increases.Finally, compared with the model using only 0.1 milliontraining images, the Rank-1 performance of the model us-ing 1.0 million training images is significantly promoted by5.75% on the challenging ICFG-PEDES database, indicat-ing that our approach can scale to large-scale database.",
  "PSTPReid62.9957.2048.4430.0368.5053.02": "noise issues in the obtained descriptions. LUPerson-T contains 0.95 M images that were also sampled from theLUPerson database . It utilizes the CLIP model to pre-dict pedestrian attributes and inserts them into manually de-fined templates as textual descriptions. We utilize the threedatabases to train the CLIP-ViT/B-16 model, incorporat-ing the SDM loss. Finally, we evaluate the models per-formance in both direct transfer and fine-tuning settings.Comparisons on the direct transfer setting are summa-rized in . It is shown that the model trained on theLUPerson-MLLM dataset achieves significantly better per-formance, even when we only sample 0.1 M images. This isbecause TDE enables diverse description generation. More-over, NAM efficiently alleviates the impact of noise in tex-tual descriptions. Combining both techniques results in amodel that exhibits exceptional transfer abilities. In com-parison, neither nor consider the noise problem intheir obtained textual descriptions. displays the model comparisons in the fine-tuning setting.In this experiment, we adopt the IRRAmethod in the fine-tuning stage and initialize its pa-rameters with each of the above three pre-trained models,respectively. The fine-tuned models are evaluated on both",
  "with ALBEF backbone:": "RaSa CLIP-ViTBERT-base76.5190.2994.2569.3865.2880.4085.1241.2966.9086.5091.3552.31APTM Swin-BBERT-base76.5390.0494.1566.9168.5182.9987.5641.2267.5085.7091.4552.56Ours (1.0 M) + APTMSwin-BBERT-base78.1391.1994.5068.7569.3783.5588.1842.4269.9587.3592.3054.17 in-domain and cross-domain text-to-image ReID scenarios.According to the results in , two conclusions can bederived. First, compared with the CLIP model , pre-training using the three pre-training datasets exhibits per-formance promotion for in-domain and cross-domain tasks.Second, pre-training using LUPerson-MLLM exhibits themost remarkable performance promotion. For example, inthe ICFG-PEDES CUHK-PEDES setting, LUPerson-MLLM outperforms the other two models by 20.82% and26.13% in Rank-1 accuracy, respectively. These experimen-tal results further validate the effectiveness of our methods.Comparisons in the Traditional Evaluation Settings.Comparisons with state-of-the-art approaches are summa-rized in . We observe that our method achieves thebest performance. With our pre-trained model parameters,the Rank-1 accuracy and mAP of IRRA are improved by8.30% and 5.85% on the RSTPReid database, respectively.Besides, pre-training with our LUPerson-MLLM datasetis more effective than with the MALS and LUPerson-Tdatasets. This is because we effectively resolve the diver-sity and noise issues in the MLLM descriptions, facilitatingmore robust and discriminative feature learning.",
  ". Conclusion and Limitations": "This paper explores the challenging transferable text-to-image ReID problem by harnessing the image captioningcapability of MLLMs. We acknowledge diversity and noiseas critical issues in utilizing the obtained textual descrip-tions.To address these two problems, we introduce theTemplate-based Diversity Enhancement (TDE) method to encourage diverse description generation and construct alarge-scale dataset named LUPerson-MLLM. In addition,we proposed the NAM method to mitigate the impact ofnoisy textual descriptions. Extensive experiments demon-strate that TDE and NAM significantly improve the modelstransfer power. However, these methods have limitations:the effectiveness of TDE is limited by the number of sen-tence templates; NAM may occasionally fail to mask noisytokens. In the future, we aim to explore more powerfulmethods to address diversity and noise issues in MLLM-generated descriptions.Broader Impacts. TDE addresses fixed sentence pat-terns generated by MLLMs, inspiring effective instructiondesign to harness MLLMs capabilities. Meanwhile, NAMtackles text noise generated by MLLMs, facilitating widerMLLM adoption for practical real-world problems.Acknowledgement. This work was partially supportedby the Major Science and Technology Innovation 2030New Generation Artificial Intelligence key project (No.2021ZD0111700), the National Natural Science Founda-tion of China under Grants 62076101 and 62172354, theGuangdong Basic and Applied Basic Research Foundationunder Grant 2023A1515010007, the Guangdong Provin-cial Key Laboratory of Human Digital Twin under Grant2022B1212010004, and the Yunnan Provincial Major Sci-ence and Technology Special Plan Projects under Grant202202AD080003.We also gratefully acknowledge thesupport and resources provided by the Yunnan Key Labora-tory of Media Convergence, the CAAI Huawei MindSporeOpen Fund and the TCL Young Scholars Program.",
  "Surbhi Aggarwal, Venkatesh Babu Radhakrishnan, and Anir-ban Chakraborty.Text-based person search via attribute-aided matching. In WACV, 2020. 2": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, AntoineMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,Katherine Millican, Malcolm Reynolds, et al. Flamingo: avisual language model for few-shot learning. NeurIPS, 2022.3 Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, SinanTan, Peng Wang, Junyang Lin, Chang Zhou, and JingrenZhou. Qwen-vl: A frontier large vision-language model withversatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1,2, 3 Yang Bai, Min Cao, Daming Gao, Ziqiang Cao, Chen Chen,Zhenfeng Fan, Liqiang Nie, and Min Zhang. Rasa: Relationand sensitivity aware representation learning for text-basedperson search. IJCAI, 2023. 1, 2, 5, 8 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-guage models are few-shot learners. NeurIPS, 2020. 3 Maryam Bukhari, Sadaf Yasmin, Sheneela Naz, MuazzamMaqsood, Jehyeok Rew, and Seungmin Rho. Language andvision based person re-identification for surveillance systemsusing deep learning with lip layers. Image and Vision Com-puting, 2023. 1",
  "Hiren Galiyawala and Mehul S Raval. Person retrieval insurveillance using textual query: a review. Multimedia Toolsand Applications, 2021. 1": "Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and XingSun.Contextual non-local alignment over full-scale rep-resentation for text-based person search.arXiv preprintarXiv:2101.03036, 2021. 2 Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, PengXu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, ZiyuGuo, et al. Imagebind-llm: Multi-modality instruction tun-ing. arXiv preprint arXiv:2309.03905, 2023. 3",
  "Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xi-aodong He. Stacked cross attention for image-text matching.In ECCV, 2018. 2": "Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang,Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foun-dation models: From specialists to general-purpose assis-tants. arXiv preprint arXiv:2309.10020, 2023. 3 Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.Align before fuse: Vision and language representation learn-ing with momentum distillation. NeurIPS, 2021. 2, 8",
  "Shuanglin Yan, Hao Tang, Liyan Zhang, and Jinhui Tang.Image-specific information suppression and implicit localalignment for text-based person search. TNNLS, 2023. 2": "Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang,Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, FanYang, et al. Baichuan 2: Open large-scale language models.arXiv preprint arXiv:2309.10305, 2023. 3 Shuyu Yang, Yinan Zhou, Zhedong Zheng, Yaxiong Wang,Li Zhu, and Yujiao Wu. Towards unified text-based personretrieval: A large-scale multi-attribute and language searchbenchmark. In ACM MM, 2023. 1, 2, 3, 5, 7, 8",
  "Ying Zhang and Huchuan Lu. Deep cross-modal projectionlearning for image-text matching. In ECCV, 2018. 1, 2, 8": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, XiaoleiWang, Yupeng Hou, Yingqian Min, Beichen Zhang, JunjieZhang, Zican Dong, et al. A survey of large language mod-els. arXiv preprint arXiv:2303.18223, 2023. 3 Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, ZhuohanLi, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge withmt-bench and chatbot arena. NeurIPS, 2023. 3"
}