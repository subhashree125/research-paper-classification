{
  "Abstract": "Emotion recognition is relevant for human behaviour un-derstanding, where facial expression and speech recogni-tion have been widely explored by the computer vision com-munity. Literature in the field of behavioural psychologyindicates that gait, described as the way a person walks, isan additional indicator of emotions. In this work, we pro-pose a deep framework for emotion recognition through theanalysis of gait. More specifically, our model is composedof a sequence of spatial-temporal Graph Convolutional Net-works that produce a robust skeleton-based representationfor the task of emotion classification. We evaluate our pro-posed framework on the E-Gait dataset, composed of a to-tal of 2177 samples. The results obtained represent an im-provement of 5% in accuracy compared to the state ofthe art. In addition, during training we observed a fasterconvergence of our model compared to the state-of-the-artmethodologies.",
  ". Introduction": "Humans have a natural perception capability that allows usto capture, process, and understand behavioral cues fromother people naturally . There are several biologicaltriggers inside our brains that plan our interactions withother people based on this perception . However, hu-man behavior is a very broad cognitive spectrum with multi-ple different nuances that can affect this planning procedure.When looking at social interactions, however, emotion is aspecific part of behavior that plays a significant role. The ability to perceive and respond to emotional aspects is es-sential to develop and maintain links with peers in society.In multiple contexts of applications, we could argue thatunderstanding the emotions of users is also a significant as-pect of the development of systems that are more inclusiveand fair. These systems could adapt the way they performaccording to how they perceive their users. However, to al-low these systems to understand the emotions of their user,first, they need to be able to extract and process emotionalinformation from them. Researchers have been studyinghow humans perceive and process these affective cues fora while, and evidence from the behavioral psychology lit-erature suggests that a significant part of affective infor-mation is communicated naturally and intuitively througha medium known as nonverbal communication .Among the vast list of nonverbal communication cues,some studies highlight the importance of bodily expres-sion for emotion recognition. Early studies such as thoseby Wallbott and Scherer suggested that there are spe-cific body movements that lead to an accurate perception ofemotions. Therefore, this strong evidence from the litera-ture on behavioral psychology validates that the body actsas an outlet for the persons emotional state, as well as sig-nals that, by extracting and processing these cues, some-one can perceive emotional aspects through observation ofcertain characteristics. Recent studies have shown signifi-cant success in encoding body language to recognize affectin humans through deep learning and computer vision, in-dicating that body expressions are a significant cue whenbuilding affect-aware technology .However, body language, as well as other affective fea-",
  "arXiv:2405.13903v1 [cs.CV] 22 May 2024": "tures such as facial expressions, gaze, gestures, or physi-ological indicators such as cardiac frequency or respiratoryrate, share a common limitation related to applications: theyrequire the user to be facing toward a camera so that the af-fective sources (e.g., face, eyes, arms...) are visible at alltimes. Although this constraint is not harmful in some sce-narios, such as when people are facing a computer or otheraffective agents (such as social robots), it is also a limit-ing aspect in ubiquitous applications since the user wouldnot be able to freely interact with the environment.Wecould, however, communicate to the user this constraint andask the user to face the system; however, this would alterthe users behavior, changing how they would communicatetheir emotions and removing all naturality from this inter-action, which would now be an artificial interaction.A possible way to encode body language and still notlimit the user is to look at the persons gait to extract af-fective information. Gait is the description of the way thatsomeone walks, and previous research in behavioral psy-chology has found that humans are able to identify multiplesocial aspects through gait-related parameters . Roetheret al. applied machine learning to a set of recordedgaits and found that some features, such as movement speedand limb flexion, were essential in correlating emotions andgaits. These pieces of evidence suggest that evaluating gaitin a spatiotemporal manner can lead to strong classificationsof emotion. With this, new applications in multiple scenar-ios are enabled, for example: we could leverage gait infor-mation to monitor freely moving citizens for public safety,where collective negative emotions could indicate a danger-ous situation taking place ; wellbeing applications suchas urbanism, where collective spaces can be changed ac-cording to what people experience in them ; or evenhealthcare where the psychological monitoring of a patientin internment can prove to be useful not only as an over-all better treatment, especially if non-invasive, but also toenable mental health professionals to better identify mentalhealth issues in their patients by having another informationsource to look into (Dhuheir et al. ).Based on this inspiration, we propose an approach foremotion recognition using gait. Through the use of spa-tiotemporal processing blocks, we overcome limitationspresent in the current state-of-the-art , which possess alimited processing capability in this aspect. We also showthat by overcoming this limitation, our results in the pro-posed quantitative metric are also better than the state-of-the-art. This works contributions are as follows:",
  "A model that converges 3.63 times faster in training, sav-ing time and computational resources, allowing for faster": "testing and experimentation.The rest of the paper is organized as follows: Next, in Sec-tion 2, we visit the state of the art. In , we presentthe methods utilized and in , we expose our ex-perimental setup. Later, in , we present the resultsand discuss them. Finally, shows our conclusionsand present ideas for future research.",
  ". Related Works": "Early works for emotion recognition through gaits werebased on extracting features from motion capture data,which is acquired by using special clothing with landmarkpoints and a motion capture camera, and using algorithmsto calculate similarity indexes with databases. Venture et al. captured walks from four different performers and ap-plied PCA to verify that these emotions could be distin-guished through some affective features.A significant improvement was proposed by Daoudiet al. , which evaluated this task from a geometric ap-proach. They have represented skeleton joints over timeusing covariance matrices, which were mapped to the Rie-mannian manifold of symmetric positive definite matrices.This allowed the authors to exploit multiple geometric prop-erties for emotion classification. However, this approach islimited by how much temporal information can be encoded,imposing a limited sequence modeling.The natural next step here was, then, related to improv-ing the temporality aspect of these previous approaches.Randhavane et al. presented a new approach, now fo-cused on RNNs, thus allowing improved spatiotemporal re-lationship. They combined affective features, such as theangles between joints and stride length, with deep featuresthat were learned using a Long Short-Term Memory archi-tecture.However, the advances on Graph Convolutional Net-works at that time, especially the proposal of the ST-GCNsby Yan et al. allowed for an even more robust wayof learning these relationships.Bhattacharya et al. proposed using such architecture to extract features fromvideos and classify the emotions in an implicit manner.Still, while ST-GCNs provide an effective approach,there are some limitations that this work aims to address.The representational capacity of the base ST-GCN is prede-fined rather than learned, which was sufficient for its origi-nally intended application of activity recognition. However,for perceiving emotional cues through nonverbal behaviorslike gait, these cues are often more subtle than the move-ments used for activity recognition. Therefore, not learningthe topology may limit the ability to capture these subtlemovement patterns that are indicative of different emotions.This is solved by using the ST-GCN++ architecture,which brings the novelty of learning the topology duringtraining. Another limitation is in the temporal aspect, with ST-GCNs having a fixed size temporal kernel, which wasrendered adjustable in the ST-GCN++ architecture. There-fore, switching from ST-GCN to ST-GCN++, is very advan-tageous for the emotion recognition using gaits problem.",
  ". Method": "Given a video V Rnhw3 with n frames, height hwidth w and a set of emotions K, our task is to classify theperceived emotion of a person present in such video by ex-tracting features related to body language and gait. We firstextract a set of 3D body keypoints K R163, in whichk1, k2, . . . , k16, each ki represents the location of a bodyjoint in space related to the person in the video. (a) Skeletal trajectory extraction.One of the possibleways to represent a skeleton is through a graph, since theserepresentations can be considered analog. Each body joint,such as the right shoulder or right elbow can be considereda vertex, and the bones that connects these two joints can beconsidered edges. This is a clear indicative on why GCNscan be used to process these types of data. Therefore, ata given timestamp t, we extract the skeleton of the personvisible on the scene and represent it as a graph: G = (V, E),where V is the vertices (or joints) set and E is the edge (orbones) set and N = |V| the number of vertices. (b) Skeletal trajectory classification.We use this graphG as input for our gait processing model. We propose usingST-GCN++ blocks to learn the joint representations anddiscover movement patterns related to perceived emotions.This way, nonverbal cues such as step size, arm swinging,head angle relative to the shoulders, among others, can beextracted automatically and without user intervention.The extracted gait features are propagated from the bodyjoints in the shape of X Rnf with xi Rf representing afeature of the ith vertex. The propagation rule is done in thefollowing manner: Z(l+1) = (AZ(l)W(l)), where Zl andZl+1 are the inputs to the network layers l and l + 1, withZ0 = X. Wl and Wl+1 are the weight matrices betweenlayers l and l + 1, and A is the adjacency matrix of G and(.) is a nonlinear activation function. Each weight matrixW represents a convolutional kernel which can be used toobtain features. For example, the application of k kernelsof dimension f d in an input X, the output corresponds toa feature vector of dimension ndk. Theres also the setof adjacent vertices Ati V to vti at time t .Our proposed model ST-Gait++ is composed of 3 ST-GCN++ blocks with 32, 64, and 64 kernels each, followedby an average pooling, a 2D 1x1 convolution layer, and asoftmax layer for the 4 emotion categories. This design waschosen empirically to overcome the limitations presentedpreviously in , but also because other works ex- periment with similar architectures, such as STEP . Also,according to the methodology described by the author ,affective features can extracted from the data, so those areextracted and added to the used data as well. We overviewour model in .",
  ". Dataset": "We used the Emotion-Gait (E-Gait) dataset in our experi-ments. The dataset that is currently available is a modi-fied version of the original dataset made available by Bhat-tacharya et al. . The authors do not share specifics ofwhat has changed or when it did, so we try our best to keep aclear comparison with other techniques from the state of theart. We only used the real data from the E-Gait, because ofsome quality issues perceived during early experimentationwith the synthetic data, alongside the issue of the changeson the dataset.The data consists on 342 samples collected by the au-thors and 1,835 samples collected by the Edinburgh Loco-motion Mocap Database (ELMD) and its distributionper train/validation/test split and categories can be found in. The data is composed of already extracted and nor-malized skeleton sequences forming different labeled gaitsin the 4 emotions targeted in this paper: Happy, Neutral,Sad and Angry. Each sample is shaped T V with T beingthe number of time steps and V the number of coordinateswhich is equal to 48 here since there are 16 joins with 3 di-mensions each. We show samples of the skeletons presentin the dataset in .",
  "SplitTrain8183322371361523Val2201076546438Test122483016216": ". The proposed architecture for ST-Gait++, composed of 3 ST-GCN++ blocks with outputs sized 32, 64 and 64, followed by a2D Average Pooling and a 1 1 Convolution, which reduces dimensionality from 64 to 4, which is followed by a Softmax. Ideally, in anapplication scenario, this can be used along some Skeletal trajectory extraction which takes as input a video and outputs the gait sequencesto be analysed by ST-Gait++ automatically. This work focuses on the Skeletal trajectory classification.",
  "(d) Sad": ". Examples of the four categories of the E-Gait dataset.Each item is a sample from one of the categories of the E-Gaitdataset, with each frame of the six-frame sequence taken from thewhole sample gait sequence. This was done to provide a senseof movement to the reader, so they can better understand E-Gaitscharacteristics. ST-GCN++ block published by Duan et al. 1.Fromthis point, we have applied a Bayesian Search algorithmfor hyperparameter tuning. Bayesian Search is a power-ful yet simple technique for optimizing hyperparameters bymodelling objective functions and updating its belief based",
  "Available at https : / / github . com / kennymckormick /pyskl": "on observed results. Therefore, instead of using a randomset of hyperparameters, which would be computationallycostly, this method will choose the set of hyperparametersthat has the highest chance of leading to better results andwill discard those with low chance. The search space wenavigated using this algorithm is shown in , and theset of chosen hyperparameters in comparison to those ofSTEP are shown in . We have trained ST-Gait++for 200 epochs. . Search space defined for the Bayesian Search. For eachparameter, a set of values are chosen as a search space. As theBayesian Search finds an optimal value within the search spacefor each parameter, this value is represented on the third columnof the table. These optimal values were used to train ST-Gait++.",
  ". Results and discussion": "Quantitative analysis.We compare our results with otherdifferent approaches in . Our proposed method out-performs other graph-related methods, such as STEP .It is interesting to notice that our implementation of thiswork has led to an increased accuracy than their reportedresults. In this case, our model has had an increase of 5.4%regarding their result, and 4.2% regarding our implemen-tation. Besides the accuracy increase, our model was alsoable to converge faster, highlighting several improvements,such as fewer requirements for computational resources ortraining time, increased possibilities for scaling, and bet-ter generalization of data. ST-Gait++ converged on epoch#127, while STEP converged on epoch #462, a reductionof 72% in training time. A runtime analysis was also per-formed, over the test set, and can be found on table .As observed, STEP has a better time for inference, whichcan be explained by the simpler architecture.Our model also highly outperforms other approacheswith a temporal focus, such as Randhavane et al. . Inthis case, we report an accuracy increase of 6.8%. Over-all, this quantitative evaluation highlights that our modelnot only has increased accuracy in relation to the currentstate-of-the-art, but is also more optimized towards trainingrequirements.We have also generated confusion matrices for ST-Gait++ and STEP in order to compare how the accuracy ofthese two models can be represented in how they perceivethe emotional classes differently. We show these results in",
  "Available at:https : / / pytorch . org / docs / stable /generated/torch.nn.CrossEntropyLoss.html": ". As expected, this result reaffirm the results wehave discussed before; STEP has a higher confusion be-tween Neutral and Happy than our model. In the case offacial expressions, the literature on behavioral psychologyhas shown that there is a structural resemblance betweenneutral and happy faces that could lead to confusion, andit is common to have this type of ambiguity in these tasks; however, there is still no evidence that the samecould happen to gait perception. Therefore, it is difficult tojudge if STEPs ambiguity between these two classes couldbe justified. In our model, we also notice an ambiguity be-tween Sad and Happy. We do not have any insights regard-ing this curious behavior at this point, but we also highlightthat our accuracy for the Sad class is still higher than thereported for STEP.Finally, we have also applied a t-SNE representation ofthe features extracted from the last layer of the models to vi-sualize and analyze the learned representations of the data.The last layer usually contains higher-level, abstract fea-tures of the input data, which could usually be representedby activation maps or feature maps. In this case, since weare not directly working with images at that point, applyingt-SNE could reveal clusters or groups that could highlightthe ability of the model to distinguish classes or categorieseffectively, as well as to identify outliers or anomalies in thedata. We show in that the Angry class has a verydistinct and separated clustering, which explains the betterperformance in this class as we have shown before in Fig-ure 3. This better performance can also be attributed to thefar larger amount of samples for this category, making themodel better able to separate it from the rest. Neutral andHappy categories are distinguishable enough to show whythey also show a good performance, while Sad, the leastnumerous category, is very mingled with the others. Qualitative analysis.In we show some qualita-tive visualizations with examples of correct and incorrectpredictions. By looking just at the skeletons its impossi-ble to distinguish the emotions. For some, we may agree",
  "(b)": ". Confusion matrices generated from evaluating the models on the test set for (a) ST-Gait++ and (b) STEP. As can be seen, thereis a more pronounced diagonal on (a), emphasizing the better accuracy of ST-Gait++. Also, there is less confusion between the emotionsNeutral and Happy on ST-Gait++ than on STEP. However, there is some increase in confusion between Happy and Sad on ST-Gait++. with the annotation provided and understand why the modelmade a correct prediction. For others, its dubious to inferthe perceived emotion with just the skeleton, and we canunderstand why the model made a mistake. On (a) we cansee the wide stride, arm swinging and attribute that to happi-ness, but we can also see the somewhat lowered head, whichcould indicate sadness. On (b) we can se a fast stride andarm swinging that could indicate some more energetic emo-tion, but the model still correctly classifies it as sadness. (c)gives us a lowered head, which could have lead the modelto infer on sadness, even though the swinging arms and bigstride indicate anger. Lastly, (d) shows a fast stride withslightly swinging arms and upperbody which could havemisled the model into inferring happiness and not some neu-tral emotion. Overall, we can understand that there is somedubiousness in the data that leads to explainable mistakes.However, given the temporality aspect of this evaluation,we are limited to what we can show in this research paper.We have prepared a video with a more in-depth overviewavailable at to be made available upon publication. Limitations of the E-Gait dataset.To the best of ourknowledge, E-Gait is one of the only public datasets ofemotion recognition through gait, alongside Emotion-Walk(E-Walk) . However, since they have data overlap, wedecided to test on E-Gait, given that it is the one used bythe state-of-the-art approaches of emotion recognition fromgait.In addition, the E-Gait dataset is very obscure, withonly the skeletons being available to researchers. Becauseof this, the annotations cannot be confirmed and neither can we know the transformations taken to process the videosand the skeletons which are already normalized. This canbe observed in . Furthermore, the dataset does notdisclose the demographics of the subjects and a great part ofthe data comes from the capture with a single individual andadds a heavy bias. For an application in other contexts, suchas Latin American contexts, for example, it would be veryinteresting to have a dataset that could show the local cul-tural emotion expression. Also, publishing an open datasetalso containing the original videos, not only the skeletons,would give researchers a higher freedom for experimenting,validating ideas and checking for biases to correct.The first step to making such a dataset would be to gatherother available datasets for emotion recognition throughgait, or curate new ones, focusing on bringing high qualitydata that is representative, diverse and as unbiased as possi-ble. Testing ST-Gait++ on more data will certainly point tonew paths of improvement for this research. Diversity and Bias in emotion recognition relateddatasets.There are many studies focused on trying to findbiases in intelligent systems. One such study is Rhue ,which found racial disparities in facial emotion recogni-tion and raised the question of whether artificial inteligencecould in fact determine emotion better than people. To an-swer this question, we need to understand that demograph-ics such as gender, race and ethnicity heavily influence theperception of human characteristics, such as in facial emo-tion recognition algorithms . But this is also due to thefact that we as humans perceive the world based on our ownbiases. To annotate the data that will be fed into algorithms . T-SNE representation of the features extracted from thelast convolutional layer of ST-Gait++ and STEP on the test set,The inner color represents the ground truth labels and the outercircle represents the models inference. is to accept that the data will have biases that the model willpropagate. Studies such as Pahl et al. bring to atten-tion the age bias, besides the ethnicity bias, in prominentAction Unit Datasets. The algorithms studied had problemswith glasses but not with beards. This shows data collec-tion problem: Are there no diverse people available or arethese people not even considered as a possible variation inthe target user public? These limitations can be extended togaits, as it is a characteristic and emotion expression outletthat can vary across different demographics.Furthermore, although some emotion expression mayhave universal features , it is noted that cultural par-ticularities are very influential in affective cues encodingKleinsmith and Bianchi-Berthouze . For a general ap-plication, having an analysis on the demographic character-istics can help researchers gain more insight on the limi-",
  ". Examples of correct and incorrect inferences by ST-Gait++ on the test set": "tations of the data and, as a consequence, of the real worldapplication of their research. As such, having such a skeweddataset, such as ELMD (Habibie et al. ), being a consid-erable part of the total data available, brings to question theapplicability of the entire dataset in in-the-wild scenarios.Given the ELMD dataset, of the 342 samples recorded,90 participants were involved, with no demographic of thispublic being disclosed.It is disclosed that the data, all1, 835 samples, was collected using a sole male actor. Withthis in mind, its important to point out the importance ofdiversity and fairness in this data collection.Also, its important to emphasize that what is being dealtwith here are the perceived emotions, since whatever actualemotion was being felt by the person at the time of datacapture is only available if the person is questioned (in realscenarios) or if we know what they are trying to convey(actors portraying emotions). Even in real scenarios, thereported emotion after questioning could be biased due tothe type of question asked or recall biases .",
  ". Conclusion": "In this work, we proposed ST-Gait++, a novel frame-work for emotion recognition from gait. Its skeleton-basedspatio-temporal representation approach results in state-of-the-art classification performance on the E-Gait dataset. We also discussed some of the limitations of the field with theobjective of presenting research opportunities.In addition, given the faster training convergence on aconsumer grade laptop, we expect ST-Gait++ to providenew research opportunities in the field of human behaviouranalysis for researchers with a lower budget or limited re-sources.Future work will explore the relevance of the differentbody parts for recognising emotion and the inclusion of ad-ditional gait descriptors. Uttaran Bhattacharya, Trisha Mittal, Rohan Chandra, Tan-may Randhavane, Aniket Bera, and Dinesh Manocha. Step:Spatial temporal graph convolutional networks for emotionperception from gaits. In Proceedings of the AAAI Confer-ence on Artificial Intelligence, pages 13421350, 2020. 2, 3,4, 5, 6",
  "Joy Adowaa Buolamwini.Gender shades: intersectionalphenotypic and demographic evaluation of face datasets andgender classifiers.PhD thesis, Massachusetts Institute ofTechnology, 2017. 6": "Willams Costa, David Macedo, Cleber Zanchettin, EstefanaTalavera, Lucas Silva Figueiredo, and Veronica Teichrieb. Afast multiple cue fusing approach for human emotion recog-nition. Available at SSRN 4255748, 2022. 1, 5 Willams Costa, Estefana Talavera, Renato Oliveira, LucasFigueiredo, Joao Marcelo Teixeira, Joao Paulo Lima, andVeronica Teichrieb. A survey on datasets for emotion recog-nition from vision: Limitations and in-the-wild applicability.Applied Sciences, 13(9):5697, 2023. 3 Arthur Crenn, Rizwan Ahmed Khan, Alexandre Meyer, andSaida Bouakaz. Body expression recognition from animated3d skeleton. In 2016 International Conference on 3D Imag-ing (IC3D), pages 17. IEEE, 2016. 5 Mohamed Daoudi, Stefano Berretti, Pietro Pala, YvonneDelevoye, and Alberto Del Bimbo. Emotion recognition bybody movement representation on the manifold of symmetricpositive definite matrices. In Image Analysis and Processing-ICIAP 2017: 19th International Conference, Catania, Italy,September 11-15, 2017, Proceedings, Part I 19, pages 550560. Springer, 2017. 2, 5 Marwan Dhuheir, Abdullatif Albaseer, Emna Baccour,Aiman Erbad, Mohamed Abdallah, and Mounir Hamdi.Emotion recognition for healthcare surveillance systems us-ing neural networks:A survey.In 2021 InternationalWireless Communications and Mobile Computing (IWCMC),pages 681687, 2021. 2 Haodong Duan, Jiaqi Wang, Kai Chen, and Dahua Lin.Pyskl: Towards good practices for skeleton action recogni-tion. In Proceedings of the 30th ACM International Confer-ence on Multimedia, pages 73517354, 2022. 3, 4",
  "Dawn Grant and David Williams. The importance of per-ceiving social contexts when predicting crime and antisocialbehaviour in cctv images. Legal and Criminological Psy-chology, 16(2):307322, 2011. 2": "Ikhsanul Habibie, Daniel Holden, Jonathan Schwarz, JoeYearsley, and Taku Komura. A recurrent variational autoen-coder for human motion synthesis. In Proceedings of theBritish Machine Vision Conference (BMVC), 2017. 3, 7 Javier Hernandez, Josh Lovejoy, Daniel McDuff, Jina Suh,Tim OBrien, Arathi Sethumadhavan, Gretchen Greene,Rosalind Picard, and Mary Czerwinski. Guidelines for as-sessing and minimizing risks of emotion recognition applica-tions. In 2021 9th International conference on affective com-puting and intelligent interaction (ACII), pages 18. IEEE,2021. 7 Heike Jacob, Benjamin Kreifelts, Sophia Nizielski, AstridSchutz, and Dirk Wildgruber. Effects of emotional intelli-gence on the impression of irony created by the mismatchbetween verbal and nonverbal cues.PloS one, 11(10):e0163211, 2016. 1",
  "Anders Nes, Kristoffer Sundberg, and Sebastian Watzl. Theperception/cognition distinction.Inquiry, 66(2):165195,2023. 1": "Jaspar Pahl, Ines Rieger, Anna Moller, Thomas Wittenberg,and Ute Schmid. Female, white, 27? bias evaluation on dataand algorithms for affect recognition in faces. In Proceedingsof the 2022 ACM Conference on Fairness, Accountability,and Transparency, pages 973987, 2022. 7 Tanmay Randhavane, Uttaran Bhattacharya, Kyra Kapsaskis,Kurt Gray, Aniket Bera, and Dinesh Manocha. Identifyingemotions from walking using affective and deep features.arXiv preprint arXiv:1906.11884, 2019. 5, 6 Tanmay Randhavane, Uttaran Bhattacharya, Kyra Kapsaskis,Kurt Gray, Aniket Bera, and Dinesh Manocha. Learning per-ceived emotion using affective and deep features for mentalhealth applications. In 2019 IEEE International Symposiumon Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),pages 395399. IEEE, 2019. 2",
  "Harald G Wallbott and Klaus R Scherer. Cues and channelsin emotion recognition. Journal of personality and socialpsychology, 51(4):690, 1986. 1": "Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial tempo-ral graph convolutional networks for skeleton-based actionrecognition. Proceedings of the AAAI Conference on Artifi-cial Intelligence, 32(1), 2018. 2 Dingkang Yang, Shuai Huang, Shunli Wang, Yang Liu, PengZhai, Liuzhen Su, Mingcheng Li, and Lihua Zhang. Emo-tion recognition for multiple context awareness. In EuropeanConference on Computer Vision, pages 144162. Springer,2022. 1 Zhao YuMeng, Liu Zhen, Liu TingTing, Wang YuanYi, andChai YanJie. Affective-pose gait: perceiving emotions fromgaits with body pose and human affective prior knowledge.Multimedia Tools and Applications, 83(2):53275350, 2024.5"
}