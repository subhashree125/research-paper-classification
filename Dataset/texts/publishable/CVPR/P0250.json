{
  "Abstract": "Dynamic Scene Graph Generation (DSGG) focuses onidentifying visual relationships within the spatial-temporaldomain of videos. Conventional approaches often employmulti-stage pipelines, which typically consist of object de-tection, temporal association, and multi-relation classifica-tion. However, these methods exhibit inherent limitationsdue to the separation of multiple stages, and independentoptimization of these sub-problems may yield sub-optimalsolutions. To remedy these limitations, we propose a one-stage end-to-end framework, termed OED, which stream-lines the DSGG pipeline.This framework reformulatesthe task as a set prediction problem and leverages pair-wise features to represent each subject-object pair withinthe scene graph. Moreover, another challenge of DSGGis capturing temporal dependencies, we introduce a Pro-gressively Refined Module (PRM) for aggregating tempo-ral context without the constraints of additional trackersor handcrafted trajectories, enabling end-to-end optimiza-tion of the network. Extensive experiments conducted onthe Action Genome benchmark demonstrate the effective-ness of our design. The code and models are available at",
  ". Introduction": "Scene Graph Generation (SGG) has emerged as a crucialcomponent in advancing human-centric scene understand-ing, garnering significant research attention in recent years.The SGG research has been extensively employed in var-ious high-level tasks, such as Visual Question Answer-ing , Visual Commonsense Reasoning and Image Generation . Dynamic scene graph genera-tion (DSGG) further extends SGG with additional temporaldimension and then becomes more challenging, which aimsat understanding more informative spatial-temporal cues.The primary objective of DSGG is to provide a sequence",
  "Scene Graph Generation": ". Comparison between existing multi-stage paradigm andproposed one-stage end-to-end framework. (a) Multi-stage meth-ods, typically detect object instances by individual object detectorand may associate objects between frames to aggregate temporalcontext based on detection results, followed by predicate classi-fication for all candidate subject and object pairs, where trackingmaybe lost. (b) Our one-stage end-to-end method, directly gen-erates dynamic scene graph for given video sequence, without in-dividual consideration for object instance detection and tracking.The missing spatial context and predicate temporal dependencycould be supplemented with spatial context of reference frames. of comprehensive and structural representation of scenes bytaking video sequence as input and detecting subject andobject as nodes, as well as identifying the multi-relationsbetween them as edges in graphs. Existing studies present promising results in DSGG by decoupling this taskinto multiple stages: instance detection, temporal asso-ciation, and multi-relation classification, as illustrated in(a). Specifically, subject and object detection resultsare obtained using an object detector. Subsequently, a tem-poral module, such as a tracker, establishes temporal linksbetween instances in adjacent frames and aggregates tempo-ral features on pair-wise combined subject-object proposalswithin the temporal sequence. The final stage entails per-forming multi-relation classification utilizing pair-wise fea-tures. Notably, prior to multi-relation classification, multi-stage methods requires the enumeration of instance or track-let pairs. However, enumerative constructing all candidate",
  "arXiv:2405.16925v1 [cs.CV] 27 May 2024": "subject-object pairs inevitably introduces not only a con-siderable of negative samples, significantly outnumberingpositive ones, which is harmful in the training, but also sub-stantial redundant computational costs. Furthermore, thesemethods suffer the problem of sub-optimal solutions due toindependent optimization of separated multiple stages.Recent research has proposed an end-to-end frame-work that unifies multiple tasks through a transformer-based structure.This approach first obtains instance re-sults for each frame based on a transformer-based track-ing model . Subsequently, it enumerates subject-objectpairs from tracked objects. Finally, the selected pairs fromthe previous frame are propagated to the target frame toaggregate temporal context and perform relation classifi-cation with pair-wise features. Nonetheless, this methodstill adopts a two-stage paradigm, constructing candidatesubject-object pairs based on the detection results and sub-sequently executing relation classification. Such pairs arenot always valid, rendering the pipeline more intricate andcomputationally expensive.Within the DSGG research community, the primary chal-lenge lies in capturing temporal dependencies. Addressingthis concern facilitates the detection of occluded or blurredobjects and the perception of relation reliant on adjacentframes, such as looking at, holding and drinking from.Existing methods have adopted complex yet sub-optimalstrategies, including the utilization of trajectories or 3D con-volution operators, to equip models with the capability tocapture temporal dependencies. Nevertheless, trajectoriesgenerated by additional trackers are difficult for joint train-ing, while 3D convolutions introduce substantial computa-tional overhead, thereby limiting the overall efficiency andeffectiveness of these approaches.In this paper, we present a novel One-stage End-to-end architecture to directly predict Dynamic scene graphsthrough set prediction, termed OED, and introduce an ef-fective temporal context aggregating strategy. OED refor-mulates dynamic scene graph generation as a set predic-tion problem by extending DETR across both spatialand temporal dimensions. Our method comprises a Spa-tial Context Aggregation module and a Temporal ContextAggregation module, as shown in (b). The architec-ture first employs cascaded decoders to aggregate spatialcontext, with the former outputting pair-wise instance fea-ture and the latter generating pair-wise relation feature. Thepair-wise instance feature aggregates pair-wise instance re-lated information and acts as the pair-wise relation query inPair-wise Relation Decoder, providing a strong prior. Sub-sequently, we concatenate both two features to obtain over-all pair-wise feature and feed it into the proposed Progres-sively Refined Module (PRM) for temporal context aggre-gating. PRM progressively selects pair-wise feature in ref-erence frames and simultaneously optimizes the pair-wise feature in target frame to mine temporal dependencies viaselected reference features, which implicitly links temporalinformation. This approach eliminates additional trackersand handcrafted trajectories, enabling end-to-end optimiza-tion of the network. Following this, classification heads andregression heads are utilized to predict DSGG results givenspatial-temporal aggregated pair-wise feature. Finally, dueto the challenge of incomplete annotations in video trainingdata, we compute predicate classification loss only over aportion of the predictions that match ground truth, mitigat-ing potential deterioration caused by missing annotations.In summary, the primary contributions are as follows: (1)We introduce a simple one-stage end-to-end framework forDSGG, termed OED, which models dynamic scene graphsas a set prediction problem with pair-wise feature. (2) Toeffectively mine the temporal dependencies of relation, wepropose a Progressively Refined Module (PRM) for ag-gregating temporal context without the constraints of ad-ditional trackers, enabling end-to-end optimization of thenetwork. (3) Experimental results on the Action Genomedataset demonstrate the superiority of the proposed one-stage end-to-end framework and the efficacy of the imple-mented temporal aggregation module.",
  ". Scene Graph Generation": "Scene Graph Generation for static image has caught broadattention since the benchmark was proposed . Most pre-vious works adopt two-stage paradigms. Thefirst stage is to detect all objects by a off-the-shelf objectdetector, such as a pretrained Faster R-CNN . Then therelationship between all candidate object pairs is classifiedusing designed modules, such as Message Passing ,Graph Convolutional Network , Casual Inference .Recently, some works adopts one-stage end-to-endparadigms to improve detection and predicate classificationjointly without explicit detection.However, additional temporal axis brings the need foreffective perception and usage of complex spatial-temporalcontext, which means that SGG model hardly effectivelyhandles DSGG task and thus the extension is not trivial.",
  "Stacking Multi-stage Pipeline": "Dynamic scene graph generation task and its benchmark areproposed by . Given that the context dependencies of thistask span both spatial and temporal dimensions, the simul-taneous modeling of spatial-temporal information and con-structing spatial-temporal interaction relationships are es-sential for enhancing DSGG performance.Previous approaches employed a multi-stage paradigm,utilizing object detectors to identify instances, and subse- quently performing grouping and multi-relation classifica-tion on the detection results. STTran leverages an off-the-shelf object detector to obtain instance detection re-sults and then enhance and classify pairwise features viaa spatial-temporal transformer. HOTR introduces ad-ditional human pose features in the second stage to capturemore relationship information. Some works cap-ture visual temporal dynamics from 3D CNN backbone,where TRACE designs a hierarchical tree to aggre-gate spatial context and introduces message passing in aspatial-temporal graph to improve the spatial-temporal fea-ture. TPT unifies object detection and object track-ing together, thereby enhancing the pair-wise feature bythe results of previous frame and aggregating rich spatial-temporal context. These methods enhance the instance fea-tures derived from object detection or achieve instance fea-tures via tracker directly, aggregate temporal information,and improve the accuracy of the classification.Nonetheless, they rely on dedicated modules for multipletasks, thereby requiring individualized training schemes.This inevitably disrupts the collaborative interactionsamong these modules for different sub-tasks, ultimately re-sulting in sub-optimal solutions. On the other hand, the nu-merous interaction pairs generated by enumeration opera-tion lead to substantial computational redundancy.",
  "Modeling Temporal Dependence": "In recent years, an increasing number of studies have delved into the temporal information of features, con-structing trajectory information to enhance inter-frame tem-poral dependencies.APT proposes an anticipatorypre-training scheme to explore the temporal correlationsamong object pairs across different frames based on objecttracking. TR2 tracks object first and utilize the cross-modality feature from CLIP to guide the consistencybetween the temporal difference of pair-wise visual and se-mantic features. DSG-DETR constructs inter-frame tra-jectories of object instances and pairs using bipartite graphmatching, aiming to enhance the long-term temporal depen-dencies of temporal information and subsequently improvethe performance of multi-relation classification.Nevertheless, trajectories generated by additional track-ers are difficult for joint training, while handcrafted tra-jectories exhibit poor robustness, are prone to introducingnoise, and consequently impact network performance. Inthis work, we formulate the DSGG as a one-stage set pre-diction task, utilizing pair-wise features to represent eachsubject-object pair within the scene graph. The one-stageframework eliminates the issue of inconsistent optimiza-tion objectives introduced by multi-stage approaches. Con-currently, we propose the PRM, which progressively fil-ters reference frame pair-wise features and simultaneously",
  ". Problem Formulation": "Dynamic scene graph generation aims to detect visual rela-tions occurred in the target frame in video sequence. De-tected visual relations are represented by a special form ofgraph, called scene graph. The nodes and edges in the scenegraph refer to object instances and relations between themrespectively, where an object instance consists of its labeland spatial position. Therefore, a scene graph is equivalentto a list of triplets subject, predicate, object, or s, p, ofor short. To generate scene graphs, the target is to modelthe joint probability P(s, p, o|V ) at each frame, wheres, p, o belongs to a pre-defined triplet set and V denotesthe video sequence.Some of previous works factorize the joint probabil-ity as follows:",
  "where D represents object detection results in V .Re-cent works introduce additional tracking acrossframes to aggregate temporal information:": "P(s, p, o|V ) = P(p|s, o)P(s, o|T )P(T |D)P(D|V )(2)where T represents object tracking results in V . These twotypes of solutions inevitably lead to multi-stage pipeline,which is sub-optimal due to separate training and upperbound of each stage.In this work, we propose a one-stage method to directlymodel P(s, p, o|V ). To utilize the temporal dependen-cies of predicate and alleviate the impact of motion andocclusion, we progressively aggregate temporal context in-formation from reference frames. That is to say, we di-rectly model P(s, p, o|Ii, {Iref}) at i-th frame, where Iiindicates the i-th frame and {Iref} refers to the referenceframes of Ii.",
  ". Overview": "The pipeline of proposed approach is illustrated as .Given the target frame and reference frames, OED directlygenerates scene graphs with spatial-temporal context in away of set prediction.First, the CNN backbone and Transformer encoder aresequentially utilized to extract visual features of each frame.To extract and aggregate useful spatial context, we adoptDETR-like architecture and associate learnable queries",
  "&": ". OED Framework: Spatial-temporal context aggregation is conducted within a one-stage end-to-end paradigm. Visual featuresof the target frame and reference frames are extracted using a CNN backbone and a Transformer encoder. Subsequently, two cascadeddecoders are employed to aggregate spatial context both within and between pairs. Temporal context is then aggregated in a progressivelyrefined manner, considering pair-wise features of the target frame and reference frames. with pair-wise feature of candidate object pairs. The pair-wise feature then extracts and aggregates spatial context inTransformer decoder. To improve the detection of blurredobject and predicate classification with dependencies oncontextual frames at the same time, we introduce a pro-gressive refined pair-wise feature interaction module (PRM)to select and aggregate useful information from referenceframes to the pair-wise feature of the target frame in a pro-gressively refined way. PRM fuses additional temporal con-text with the spatial aggregated pair-wise feature of the tar-get frame, and then we obtain the final pair-wise featurewith spatial-temporal context.The pair-wise detection and predicate classification re-sults will form a list of triplets s, p, o, which correspondsto the scene graph of target frame.",
  ". Spatial Context Aggregation": "CNN visual backbone and Transformer encoder yield thevisual features of each frame F= {fT , fT1, ..., fTn},where fi RHW dmodel, i {T, T1, ..., Tn} and (H, W)represents the scale of feature map.In order to fully exploit the potential of set predictiondesign in the DSGG, we associate learnable queries Q RNqdmodel with subject-object pairs (s, o). The pair-wisequeries are to obtain specific visual features related to corre-sponding candidate pairs, which means spatial context ag-gregation. In addition to aggregating the spatial informa-tion of each pair individually, the underlying connectionsbetween different pairs are significant as well, e.g. (per-son, dish) tends to co-occur with (person, table). We model and aggregate spatial context in these two ways using multi-head attention in transformer decoders.Multi-head Attention.Given query embedding Xq,key embedding Xk and value embedding Xv, the outputof multi-head attention is computed as follows:",
  "CrossAttn(Q, fi) = MHead(Q, fi, fi)(4)": "Considering that a single decoder struggles to handle twodifferent tasks , pair detection and predicate classifica-tion, we introduce two cascaded decoders. One is tailoredto decode features for pair-wise instance related feature,while another one is for pair-wise predicate related feature.Specifically, a set of learnable queries are used to capturepair-wise instance related information in Pair-wise InstanceDecoder. Considering that pair-wise instance related fea-ture can act a strong prior to predicate classification, we",
  ". Progressively refined long-range global temporal contextaggregation": "take it as pair-wise relation query to the Pair-wise RelationDecoder. In a word, the cascaded decoders aggregate thespatial context by pair-wise instance query and pair-wiserelation query.Pair-wise Instance Decoder. A set of learnable queriesQ extract and aggregate pair-wise instance related spatialcontext, as shown in Eq. 4.Pair-wise instance featurefrom Pair-wise Instance Decoder Qp = {qp1, ..., qpNq} RNqdmodel then acts as query of Pair-wise Relation Decoder.Pair-wise Relation Decoder. Apparently pair-wise in-stance could provide strong priors to classify predicate, es-pecially for spatial and contacting predicates, such as (per-son, chair) with large overlapping area leading to sitting.Thus, Pair-wise Relation Decoder take pair-wise instancefeature Qp as query to capture and aggregate pair-wiserelation specific spatial context Qr = {qr1, ..., qrNq} RNqdmodel, similar to the operations in the Pair-wise In-stance Decoder.Therefore, the spatial context information of tripletss, p, o corresponds to overall pair-wise feature Qc=Concat(Qp, Qr) = {qc1, ..., qcNq} RNq2dmodel, whereqci = Concat(qpi , qri ), i {1, ..., Nq}.",
  "Through the cascaded decoders, the pair-wise feature Qc": "has aggregated rich spatial context information. Besidesspatial dependencies discussed in section 3.3, there are tem-poral dependencies of predicate across frames, e.g. (lookingat - holding - drinking from). Reference frames could alsoimprove the detection of blurred and occluded object in thetarget frame. Therefore, this section further supplementspair-wise feature with additional temporal context, which isorthogonal to the spatial context. To achieve this, we pro-pose a multi-step progressively refined interaction modulePRM, motivated by .Specifically, we extract the spatial pair-wise feature of target frame and reference frames {QcT , QcT1, ..., QcTn} andconcatenate the pair-wise features of reference frames to-gether Qcref = {qc1, ..., qcnNq}. Then we split the pair-wise feature qci into pair-wise instance feature qpi and pair-wise relation feature qri and use classification heads to scoresubject and object with qpi and score predicate with qri . Wecalculate the score of triplet by the multiplication of subjectscore ssub, object score sobj and predicate score srel, andthen rank them.",
  "p(qci ) = ssub sobj srel(5)": "The pair-wise feature with higher score tends to have morecorrelations with corresponding ground truth. Thus, we ag-gregate the temporal context from more confident referencepair-wise features. Selecting a fixed number of referencepair-wise features is hard to hit a good balance and eitherbringing much noise or missing some informative referencepair-wise features, so we aggregate temporal context in amulti-step progressively refined way.In i-th step, the selected Top-K reference pair-wise fea-tures Qc;kiref interact with the pair-wise features in the targetframe Qi in Transformer decoder progressively, which isformulated as",
  "(6)": "The value of ki is gradually reduced to obtain more con-fident refined reference pair-wise features. This progres-sively refined selection realizes the trade-off between morecontext information and less background noise.As shown in , in the progressively refined process,some selected noises from reference frames, such as toweldenoted as yellow box, are gradually filtered out. Besides,PRM provides a way of long-range global temporal interac-tion, which means that the temporal interaction is not con-strained inside the trajectories of object pair. With the ben-efit of global perspective, PRM could capture the gradualmovement of sandwich from the dish to person.After m steps progressively refined temporal aggrega-tion, the pair-wise feature Qm is composed of abundantspatial-temporal context information. The spatial-temporalpair-wise feature is then divided into pair-wise instance fea-ture QpT and pair-wise relation feature QrT , which are usedto detect object pair and classify predicate respectively.",
  "classification loss for subject, object and predicate, Ljbox =": "Ljboxgij, p(i)j, j {s, o, p} indicates the bounding boxregression loss of subject and object. We use cross entropyloss as classification loss of subject Lscls and object Locls,weighted sum of L1 loss and GIoU loss as boundingbox regression loss of subject Lsbox and object Lobox andfocal loss as the classification loss of predicate.We adopt the matching loss as overall objective func-tion but predicate loss. Due to the incomplete annotationissue, we only calculate the predicate loss over the predic-tions matched with real ground truth, which is not paddedwith background.",
  "Inference": "In the inference stage, there are a fixed number of pair pre-dictions for each frame. Because there may be multiplepredicates for one pair, we rank the candidate triplets byscoring them as the multiplication of three-part confidences.To reduce duplicate triplet detection, we filter out the pre-dictions with lower scores that have the same triplet labeland large overlapping area with others.Specifically, wetake the multiplication of IoU of subject and object as thecorrelation in NMS to filter repeated predictions. The scenegraph is generated by those retained triplet predictions.",
  ". Experimental Setting": "Dataset: We evaluate OED on the Action Genome (AG)dataset , which annotates 234, 253 frame scene graphsfor sampled frames from around 10K videos, based on Cha-rades dataset . The annotations cover 35 object cate-gories and 25 predicates. The overall predicates consist ofthree types of predicates: attention, spatial and contacting.There may be multiple spatial predicates or contacting pred-icates between the same pair.Evaluation Metrics: Following previous works ,we adopt Recall@k as evaluation metrics to measure thefraction of ground truth hit in the top k predictions un-der With Constraint and No Constraints setting, wherek {10, 20, 50}. Specifically, We evaluate our method ontwo protocols: scene graph detection (SGDET) and predi-cate classification (PredCLS), following TPT . SGDETaims to generate scene graphs for given videos, comprisingdetection results of subject-object pairs and the associatedpredicates. The localization of object prediction is consid-ered accurate when the Intersection over Union (IoU) be-tween the prediction and ground truth is greater than 0.5.PredCLS, a simplified task to eliminate object detection er-rors, requires methods to classify predicates for given oracledetection results of subject-object pairs.Implementation Details: We employ ResNet-50 as theCNN backbone. The Image Encode, Pair-wise Decoder andRelation Decoder consist of 6 transformer layers, with thenumber of predefined learnable query Nq = 100.Fol-lowing TPT , we initialize Image Encoder and Pair-wise Decoder with the weights pretrained on the MS-COCOdataset and subsequently fine-tune all modules on the Ac-tion Genome dataset. PRM includes three instances of pro-gressively refind pair-wise interaction with Top-K as [80n,50n, 30n] respectively, where n denotes the number of ref-erence frames. The threshold adopted in inference stage is0.9. For the PredCLS task, aimed at predicting predicatelabels for specified object pairs, we initialize the learnablequeries using semantic features derived from the Glove em-beddings of the given pair labels. Additionally, we incor-porate position embeddings with spatial features obtainedfrom the specified bounding boxes of the pairs. During in-ference, we derive the outputs by associating the labels andbounding boxes of the ground truth with the predicate clas-sification results for the corresponding pairs.",
  ". Comparison between oracle query selection and progres-sively refined query selection": "Constraint setting and an average of 2.2% (3.3%, 3.1% and0.3% respectively) under No Constraints setting. This out-come underscores the importance of addressing dynamicscene graph generation as a comprehensive task rather thanpartitioning it into multiple sub-tasks. More performancecomparison in SGDET task with long-tail issue related me-tircs can be found in supp.M section 1.In PredCLS, OED improves the performance of thesecond-best methods by an average of 2.1% (2.0%, 2.2%and 2.3% respectively) under the With Constraint set-ting. However, our methods performance in PredCLS ismarginally lower than that of TPT and TR2 under the NoConstraints setting. We conjecture the reason is as follows:in the PredCLS task, oracle object tracks from ground truthare provided. Both TPT and TR2 are tracking-based meth-ods, and they utilize the oracle trajectories in their respec-tive track-based temporal aggregation modules. Due to thenature of the one-stage paradigm, our method cannot ex-plicitly use this information, which consequently reducesthe efficiency of leveraging oracle information.Further-",
  "more, TPT employs additional multi-scale features and TR2": "incorporates the CLIP model, which is pre-trained us-ing 4M image-text pairs, providing them with an advantageover our approach. Despite the fact that multi-stage meth-ods benefit from oracle tracks and can directly aggregate en-tirely accurate spatial-temporal context, our approach stilloutperforms others by a significant margin under the WithConstraint setting and attains comparable performance un-der the No Constraints setting.",
  ". Ablation Study": "In this part, we evaluate the effectiveness of our designs inOED with SGDET task on Action Genome test set.Spatial-Temporal Context Aggregation: In Tab. 2, Weevaluate the effectiveness of the proposed Spatial ContextAggregation (SA) and Temporal Context Aggregation (TA)modules individually.We first adapt DETR for dy-namic scene graph generation, establishing it as our base-line (#1), where the object pair predictions and predicateclassification are derived from the same decoded query rep-resentation. By incorporating the Spatial Context Aggre-gation module into the baseline (#2), we observe a signif-icant improvement in performance. This indicates that theperformance of spatial scene graph generation plays a cru-cial role in the effectiveness of dynamic scene graph gen-eration. Furthermore, when the Temporal Context Aggre-gation module is integrated alongside the Spatial ContextAggregation module (#3), a further gain is achieved. Thissuggests that effectively exploiting temporal dependency in-formation can further enhance the performance of DSGG.Designs in Spatial Context Aggregation:In Tab. 3,we evaluate the efficacy of the proposed Cascaded De-coders (CD) and Matched Predicate Loss (ML). Buildingupon the baseline, we introduce an additional Pair-wise Re-lation Decoder and combine the two decoders in a cas-caded manner (#2), addressing the optimization challengesof unified representation in multi-task settings and the de-pendence of predicate classification on pair detection re-sults. The performance improvement achieved by the cas-caded decoders validates our aforementioned considera-tions. More qualitative results can be found in supp.M sec-tion 2. Furthermore, to mitigate the misleading effects ofincomplete annotations in the Action Genome dataset, wecompute the predicate loss only over the predictions fromqueries that match the ground truth (#3). The resulting per-formance gain underscores the effectiveness of the matchedpredicate classification loss in addressing the issue of in-complete annotations.Designs in Temporal Context Aggregation: In Tab. 4, weevaluate the importance of temporal context and our pro-posed PRM. We select the spatial aggregation model as ourbaseline (#1). Taking into account the potential loss of in-formation due to motion and the temporal dependency of predicates, we hypothesize that context clues can be ob-tained from adjacent reference frames to enhance pair de-tection and predicate classification. To incorporate the tem-poral context, we interact the pair-wise feature of targetframe with all reference pair-wise features (#2) without pro-gressively refined (NPR). The experimental results substan-tiate the effectiveness of temporal context in dynamic scenegraph generation. Moreover, considering that not all pair-wise features are valid, as they may attend to duplicate ar-eas or background noise, we implement a progressively re-fined interaction (PR) between the pair-wise features of tar-get frame and reference frames (#3). The demonstrated ef-fectiveness of the progressive refinement of pair-wise inter-actions indicates that filtering out background noise is cru-cial for improving semantic context aggregation.",
  ". Discussion": "To further assess the effectiveness of temporal pair-wise in-teraction and estimate the upper bound of our PRM, we as-sume that the selected reference queries are oracle queries,meaning that these queries are matched with ground truthvia bipartite matching. As illustrated in Tab. 5, the oracleselection (#1) achieves a significantly higher performancecompared to our PRM (#2). This result indicates that thereis still room for exploration and improvement in our ap-proach.In the future, we plan to delve deeper into theeffective selection of true positive samples from pair-wisefeatures of reference frames. It is worth noting that our ob-jective is not to obtain precise trajectories. We regard trajec-tories as a form of long-term yet local information, subjectto instance-level constraints. It impedes the perception ofthe relationships among different instances across frames.We consider that long-term global information extracted byPRM plays a crucial role, and our approach focuses on fil-tering more accurate pair-wise instances across frames tofacilitate the relation classification for the target frame.",
  ". Conclusion": "In this paper, we present a one-stage end-to-end framework,named OED, for dynamic scene graph generation. Our ap-proach reformulates the task as a set prediction problem andemploys pair-wise features to represent each subject-objectpair within the scene graph. Furthermore, we introduce aProgressively Refined Module (PRM) for temporal contextaggregating. The PRM progressively filters pair-wise fea-tures of reference frames while simultaneously optimizingthe pair-wise features of the target frame to extract temporaldependencies through filtered features. Consequently, OEDachieves significant improvement over the baseline, estab-lishing sota performance across all metrics in SGDET task.Acknowledgements.This work was supported by thegrants from the National Natural Science Foundation ofChina 62372014.",
  "Anurag Arnab, Chen Sun, and Cordelia Schmid.Uni-fied graph structured models for video understanding.InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 81178126, 2021. 3": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, NicolasUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European confer-ence on computer vision, pages 213229. Springer, 2020. 2,3, 8 Anoop Cherian, Chiori Hori, Tim K Marks, and JonathanLe Roux. (2.5+ 1) d spatio-temporal scene graphs for videoquestion answering. In Proceedings of the AAAI Conferenceon Artificial Intelligence, pages 444453, 2022. 1 Yuren Cong, Wentong Liao, Hanno Ackermann, BodoRosenhahn, and Michael Ying Yang. Spatial-temporal trans-former for dynamic scene graph generation. In Proceedingsof the IEEE/CVF international conference on computer vi-sion, pages 1637216382, 2021. 3, 7",
  "Yuren Cong, Michael Ying Yang, and Bodo Rosenhahn.Reltr: Relation transformer for scene graph generation. IEEETransactions on Pattern Analysis and Machine Intelligence,2023. 2, 7": "Jiajun Deng, Yingwei Pan, Ting Yao, Wengang Zhou,Houqiang Li, and Tao Mei. Relation distillation networksfor video object detection. In Proceedings of the IEEE/CVFinternational conference on computer vision, pages 70237032, 2019. 5 Azade Farshad, Yousef Yeganeh, Yu Chi, Chengzhi Shen,Bojrn Ommer, and Nassir Navab. Scenegenie: Scene graphguided diffusion models for image synthesis. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 8898, 2023. 1 Shengyu Feng, Hesham Mostafa, Marcel Nassar, SomdebMajumdar, and Subarna Tripathi. Exploiting long-term de-pendencies for generating dynamic scene graphs. In Pro-ceedings of the IEEE/CVF Winter Conference on Applica-tions of Computer Vision, pages 51305139, 2023. 1, 3, 6,7 Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan CarlosNiebles. Action genome: Actions as compositions of spatio-temporal scene graphs.In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 1023610247, 2020. 2, 6",
  "Symbolic replay: Scene graph as prompt for continual learn-ing on vqa task. In Proceedings of the AAAI Conference onArtificial Intelligence, pages 12501259, 2023. 1": "Yikang Li, Wanli Ouyang, Bolei Zhou, Kun Wang, and Xi-aogang Wang. Scene graph generation from objects, phrasesand region captions.In Proceedings of the IEEE inter-national conference on computer vision, pages 12611270,2017. 7 Yiming Li, Xiaoshan Yang, and Changsheng Xu. Dynamicscene graph generation via anticipatory pre-training. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1387413883, 2022. 1, 3, 6,7 Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, andPiotr Dollar. Focal loss for dense object detection. In Pro-ceedings of the IEEE international conference on computervision, pages 29802988, 2017. 6 Xin Lin, Changxing Ding, Jinquan Zeng, and Dacheng Tao.Gps-net: Graph property sensing network for scene graphgeneration.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 37463753, 2020. 2, 7 Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li Fei-Fei. Visual relationship detection with language priors. InComputer VisionECCV 2016: 14th European Conference,Amsterdam, The Netherlands, October 1114, 2016, Pro-ceedings, Part I 14, pages 852869. Springer, 2016. 7 Jianguo Mao, Wenbin Jiang, Xiangdong Wang, Zhifan Feng,Yajuan Lyu, Hong Liu, and Yong Zhu. Dynamic multistepreasoning based on video scene graph for video question an-swering. In Proceedings of the 2022 Conference of the NorthAmerican Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 38943904,Seattle, United States, 2022. Association for ComputationalLinguistics. 1 Sayak Nag, Kyle Min, Subarna Tripathi, and Amit K Roy-Chowdhury. Unbiased scene graph generation in videos. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 2280322813, 2023. 7",
  "on computer vision and pattern recognition, pages 658666,2019. 6": "Gunnar A Sigurdsson, Gul Varol, Xiaolong Wang, AliFarhadi, Ivan Laptev, and Abhinav Gupta.Hollywood inhomes: Crowdsourcing data collection for activity under-standing.In Computer VisionECCV 2016: 14th Euro-pean Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part I 14, pages 510526. Springer,2016. 6 Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo,and Wei Liu. Learning to compose dynamic tree structuresfor visual contexts. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages66196628, 2019. 7 Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, andHanwang Zhang. Unbiased scene graph generation from bi-ased training. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 37163725, 2020. 2",
  "Jingyi Wang, Jinfa Huang, Can Zhang, and Zhidong Deng.Cross-modality time-variant relation learning for generatingdynamic scene graphs.arXiv preprint arXiv:2305.08522,2023. 3, 7": "Shuang Wang, Lianli Gao, Xinyu Lyu, Yuyu Guo, PengpengZeng, and Jingkuan Song. Dynamic scene graph generationvia temporal prior inference. In Proceedings of the 30th ACMInternational Conference on Multimedia, pages 57935801,2022. 7 Zhecan Wang, Haoxuan You, Liunian Harold Li, AlirezaZareian, Suji Park, Yiqing Liang, Kai-Wei Chang, and Shih-Fu Chang. Sgeitl: Scene graph enhanced image-text learn-ing for visual commonsense reasoning. In Proceedings ofthe AAAI Conference on Artificial Intelligence, pages 59145922, 2022. 1 Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei.Scene graph generation by iterative message passing. In Pro-ceedings of the IEEE conference on computer vision and pat-tern recognition, pages 54105419, 2017. 2",
  "object tracking with transformer. In European Conferenceon Computer Vision, pages 659675. Springer, 2022. 2": "Aixi Zhang, Yue Liao, Si Liu, Miao Lu, Yongliang Wang,Chen Gao, and Xiaobo Li. Mining the benefits of two-stageand one-stage hoi detection. Advances in Neural InformationProcessing Systems, 34:1720917220, 2021. 4 Ji Zhang, Kevin J Shih, Ahmed Elgammal, Andrew Tao,and Bryan Catanzaro. Graphical contrastive losses for scenegraph parsing. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1153511543, 2019. 7 Yong Zhang, Yingwei Pan, Ting Yao, Rui Huang, Tao Mei,and Chang-Wen Chen. End-to-end video scene graph gener-ation with temporal propagation transformer. IEEE Transac-tions on Multimedia, 2023. 2, 3, 6, 7"
}