{
  "AbstractEmbodied AI represents a paradigm in AI research": "where artificial agents are situated within and interact with phys-ical or virtual environments. Despite the recent progress in Em-bodied AI, it is still very challenging to learn the generalizable manipulation skills that can handle large deformation and topo-logical changes on soft-body objects, such as clay, water, and soil. In this work, we proposed an effective policy, namely GP2E be-havior cloning policy, which can guide the agent to learn the gen-eralizable manipulation skills from soft-body tasks, including pouring, filling, hanging, excavating, pinching, and writing. Con-cretely, we build our policy from three insights:(1) Extracting intricate semantic features from point cloud data and seamlessly integrating them into the robots end-effector frame; (2) Captur-ing long-distance interactions in long-horizon tasks through the incorporation of our guided self-attention module; (3) Mitigating overfitting concerns and facilitating model convergence to higher accuracy levels via the introduction of our two-stage fine-tuning strategy. Through extensive experiments, we demonstrate the effectiveness of our approach by achieving the 1st prize in the soft-body track of the ManiSkill2 Challenge at the CVPR 2023 4th Embodied AI workshop. Our findings highlight the potential of our method to improve the generalization abilities of Embod-ied AI models and pave the way for their practical applications in real-world scenarios. All codes and models of our solution is available at",
  "Note to PractitionersThis paper explores the challenge of": "training artificial agents to execute complex manipulation tasks with soft-body objects, characterized by significant deformations and topological transformations. Traditional methods falter with materials such as clay, water, and soil, primarily due to the diffi-culty in generalizing manipulation skills across diverse scenarios. We present the GP2E behavior cloning policy, specifically devel-oped to facilitate the learning of these skills in tasks including pouring, filling, hanging, excavating, pinching, and writing. The GP2E policy equips robots to capture essential long-distance interactions for managing complex, long-duration tasks efficient-ly, and concurrently mitigates overfitting and enhances model convergence, thus improving accuracy in task execution. Our findings indicate that the GP2E policy substantially improves the generalization capabilities of Embodied AI models, thereby broadening the prospects for their practical deployment in real-world settings.",
  "ITH the rise of Chat-GPT , AI (artificial intelli-gence) has once again sparked a global frenzy. But models like GPT, do not have a physical body to": "interact with the physical or virtual environments. In contrast, Embodied AI represents a significant advancement by inte-grating physical bodies into AI systems. These embodied agents gather environmental information through sensors and execute physical actions using mechanical actuators. Diverg-ing from traditional AI approaches, which often rely on ab-stract symbolic manipulation or passive learning from static datasets, Embodied AI emphasizes the fusion of sensorimotor experiences with learning and decision-making processes. By imbuing intelligence in agents capable of perceiving, acting upon, and manipulating their surroundings, Embodied AI aims to develop systems with human-like understanding, reasoning, and behavior.",
  "Embodied AI holds promise for automating various daily": "tasks, including household chores. To achieve this vision, ro-bots must possess human-like manipulation skills, allowing them to manipulate diverse objects with ease after being trained on a variety of examples. Yet, many existing Embod-ied AI models rely heavily on extensive interactions with training environments, which may not be practical in real-",
  ". Overview of our policy. By employing our Guided Point Cloud to End-effector (GP2E) behavior cloning policy, agents can learn the generalizable manipulation skills akin to those possessed by humans": "world scenarios. To this end, the SAPIEN ManiSkilll in-troduced a comprehensive simulation benchmark for manipu-lating 3D objects. This benchmark leverages large-scale da-tasets of demonstrations to train agents and evaluates their generalization capabilities across various tasks, such as push-ing chairs, opening cabinet doors, and moving buckets. Build-ing upon this foundation, ManiSkill2 further enhances the benchmark by incorporating a broader range of manipulation tasks to address the generalizability issue. However, despite these advancements, the baseline of ManiSkill2 still face limi-tations in performing Soft-body tasks, including pouring, fill-ing, hanging, excavating, pinching, and writing.",
  "Upon conducting a thorough investigation, we have identi-": "fied several critical challenges that impede the baseline per-formance of ManiSkill2 in learning generalizable manipula-tion skills for Soft-body objects: 1) The baseline of ManiSkill2 relies on PointNet to perceive the environment for action planning. However, this method is not robust enough to achieve generalizable shape understanding across complex topologies and geometries; 2) The long-horizon tasks featured in ManiSkill2 entail numerous long-distance interactions be-tween objects and the robot. Successfully tackling these tasks requires the ability to effectively capture such interactions; 3) The demonstration trajectories provided for each task in ManiSkill2 are limited in quantity. Consequently, there exists a risk of overfitting during the training process, where the model may excessively adapt to the specific demonstration data rather than learning generalizable manipulation skills.",
  "II. RELATED WORK": "A. Soft-body Tasks In real-world scenarios, robots encounter not only rigid bod-ies but also various types of soft materials such as cloth, water, and soil. Several simulators have been developed to facilitate robotic manipulation involving soft bodies. For instance, Mu-JoCo and Bullet utilize the finite element method (FEM) to simulate objects like ropes, cloth, and elastic materi-als. However, FEM-based approaches struggle with handling significant deformation and topological changes, such as scooping flour or cutting dough. Other environments, such as SoftGym and ThreeDWorld leverage Nvidia Flex to simulate large deformations, but they fall short in realistically simulating elasto-plastic materials like clay. PlasticineLab employs the continuum-mechanics-based material point meth-od (MPM), yet it lacks the capability to integrate with rigid robots and requires improvements in simulation and rendering performance. ManiSkill2 develops a custom GPU-based MPM simulator from scratch utilizing Nvidias Warp JIT framework and native CUDA for optimal efficiency and cus-tomization, ManiSkill2 is the first embodied AI environment to support 2-way coupled rigid-MPM simulation and the first to offer real-time simulation and rendering of MPM materials.",
  "Generalizable manipulation skills are fundamental in the": "field of Embodied AI, empowering agents to tackle long-horizon and intricate daily tasks , . Previous research endeavors have concentrated on discerning crucial compo-nents or extracting features of articulations to establish repre-sentations that facilitate generalized manipulation across di-verse instances , , . These approaches often rely on visual cues, such as key location identification, pose esti-mation, or pretrained attention models. Moreover, control-based methods employing model prediction and generative planning techniques have been investigated to achieve robust and adaptable control over both familiar and novel objects , . Imitation learning offers a viable solution to equip robots with a variety of manipulation capabilities . Ro-boCook introduced Graph Neural Networks (GNNs) with imitation learning to model tool-object interactions, integrat-ing tool classification with self-supervised policy learning to devise manipulation plans. The Diffusion policy fully unlocked the potential of diffusion models for visuomotor policy learning on physical robots. To foster interdisciplinary collaboration and ensure the reproducibility of research on generalizable manipulation skills, it is essential to establish a versatile and publicly accessible benchmark. In this regard, ManiSkill2 has constructed a benchmark capable of accom-modating object-level variations in both topological and geo-metric attributes, while also addressing the practical challeng-es inherent in manipulation tasks.",
  "Point cloud-based manipulation policies have been rigor-": "ously explored , , . The FrameMiner investi-gated how different coordinate frames for input point clouds affected manipulation skill learning in 3D environments, and proposed a dynamic frame selection method that adaptively merged the advantages of different frames, thereby enhancing performance in complex manipulation tasks without the need for modifying camera setups. Besides, researchers have also begun integrating point clouds into deep reinforcement learn-ing (RL) frameworks to enhance manipulation learning , . The process of feature learning within 3D neural net-works introduces beneficial inductive biases for visual repre-sentation, leading to the development of a robust algorithm that outperforms traditional 2D approaches in intricate robotic manipulation tasks where precise encoding of relational dy-namics is essential . Consequently, our research prioritizes point cloud-based policies to facilitate the learning of general-ized manipulation skills in robots.",
  "Our problem focuses on policy learning for the development": "of generalizable soft-body manipulation skills in robots. This entails enabling robots to manipulate a diverse range of soft-body objects in conjunction with rigid bodies within a specific task domain. For instance, pouring water into cups positioned variably and with distinct final target liquid levels. The tasks and the real-time soft-body environments come from the ManiSkill2 Challenges. Consequently, our environment con-sists of a robot, a soft-body object coupled with rigid bodies (e.g., a water-filled rigid bottle), and multiple depth cameras. These cameras enable us to generate various single fused point clouds, which are then concatenated together to form the ob-servations of the environment. The task goal is defined by the point cloud data. For example, in the task of pouring water, the target cup along with its final liquid levels are labeled within the water-filled cup. The task is deemed successful when the final liquid level precisely aligns with a designated target line. Assuming the robot state remains consistently known, our state, , thus consists of a point cloud with labels assigned to individual points, alongside the robot state.",
  "In addressing the challenge of cultivating generalizable soft-": "body manipulation skills in robots, we have introduced an effective policy framework. Our proposed approach, the guid-ed self-attention based policy, adeptly captures highly con-densed semantic features from point cloud data and seamlessly transforms these features into the end-effector frame of robots. The action generated by our policy on the state is denoted as (), We employ a behavior cloning strategy to guide our policy towards favoring actions contained within success-ful demonstrations , achieved by minimizing the Euclidean distance between () and :",
  "Environment Step": ". The pipeline of our approach. Initially, environment samples are gathered from multiple workers. Subsequently, our GP2E (Guided Point Cloud To End-effector) policy seamlessly translates point cloud data into actions. Next, the Behavior Cloning algorithm guides our policy towards actions found within suc-cessful demonstrations. Finally, we employ our two-stage fine-tuning strategy to address overfitting concerns and aid model convergence towards higher accura-cy levels.",
  "In our pipeline, we adhere to the physical simulation and": "rendering procedures outlined by ManiSkill2: (1) Conducting physical simulation across multiple worker processes; (2) Tak-ing pictures using both the base camera and the hand camera.; (3) Employing asynchronous rendering to convert images into point clouds on the GPU while simultaneously acquiring ex-pert observations from the replay buffer on the CPU. Unlike ManiSkill1, where the CPU remains idle during GPU render-ing, ManiSkill2 enhances CPU utilization by initiating expert observations while the GPU is engaged in rendering. This technique is named Asynchronous Rendering by ManiSkill2. Expert observations refer to trajectories that successfully ac-complish tasks, serving as invaluable resources to facilitate learning-from-demonstrations methodologies.",
  "Following the environment step in Maniskill2, we acquire": "two single fused point clouds from different cameras. Subse-quently, we concatenate these points and remove ground arti-facts using height clipping. In the baseline approach of ManiSkill2, PointNet serves as the visual backbone to ran-domly downsample the point cloud to 1200 points. However, we observed that this baseline fails to capture intricate seman-tic features in tasks with long-horizon tasks.",
  "As depicted in , the input of our point cloud to end-": "effector policy is a fused point cloud 1 with 6 channels. Among these channels, 3 channels contain XYZ position in-formation, while the remaining 3 channels encompass RGB information. To generate features with different mapping lev-els, we employ three Conv11 operations with varying input and output channel sizes: 6/64, 64/128, and 128/512, respec-tively. Following each Conv11 operation, we apply layer normalization and ReLU activation functions. The outputs of these operations are denoted as 2, 3,and 4, respectively. Subsequently, we concatenate 1, 2, and 3 to obtain :",
  "( , (( ( ))), ((())))Cconcat PPP =, (2)": "where () and () (where i=1,2) denote ReLU activation functions and layer normalization, respectively. () (where i=1,2) signifies three Conv11 with distinct input and output channel sizes. Subsequently, we incorporate our guided self-attention module to capture long-distance interactions between objects and the robot. The resulting feature, denoted as , is concatenated with 4 and processed through a maximum value selection function to derive the action () based on the cur-rent state , thereby guiding the robots movements:",
  "We have observed that in scenarios where the object is dis-": "tant from the robot, the baseline model in ManiSkill2 fails to capture long-distance interactions between the object and the robot. This deficiency may lead to erroneous actions, as illus-trated in (a), when the cup is positioned at the right edge of the table, the robots front joint tuning direction restricts its ability to rotate the bottle clockwise. Consequently, the robot must relocate to the right side of the cup to pour water into it, necessitating the visual network backbone to effectively pro-cess long sequence information. However, the baseline model of ManiSkill2 appears to struggle in handling such situations. Additionally, in tasks such as Excavation, where a robot must excavate a specific volume of clay in a box, a simple yet effec-tive visual network backbone is required for depth estimation. The robot must infer joint tuning angles based on sequential images of the state, necessitating the visual network backbone to capture long-distance interactions in the sequential images. However, the baseline model of ManiSkill2 appears inade-quate in addressing this scenario effectively (refer to (c)).",
  "To capture long-distance interactions in long-horizon tasks,": "we propose the guided self-attention module. In the conven-tional self-attention mechanism , the query, keys, and val-ues are all vectors linearly mapped by the same feature map, which is inefficient in local feature extraction . In our guided self-attention module, we address this by introducing a highly condensed feature (referred to as feature in , containing 704 channels) as the query and value vectors (de-noted as and in ). These vectors encompass rich local features that have been processed through convolutions at various mapping levels. Subsequently, we introduce 4 (see ) as the key vectors (see vectors in ) to compute cosine similarities with every vector in . This process ena-bles the learning of long-distance interactions between fea-tures that are distant in the feature map and allows for captur-",
  ", (4)": "where indicates the dimension of queries and keys, and is a relative position bias . With the integration of our guided self-attention module, our method can effectively cap-ture long-distance interactions, thereby enhancing its suitabil-ity for the long-horizon tasks (see (b), (d)). C. Behavior Cloning Due to the soft-body simulator in ManiSkill2 being tailored for visual learning environments, it is preferable to employ a straightforward yet efficient supervised learning algorithm. Specifically, matching the predicted action with the demon-strated action based on visual observations proves effective . Among the spectrum of learning-from-demonstrations algorithms, behavior cloning stands out as a straightforward choice, requiring fewer resources to implement . Hence, we adopt a behavior cloning strategy, aiming to directly match predicted and ground truth actions by minimizing the Euclide-an distance.",
  "As the training process progresses, we have observed poten-": "tial overfitting issues, wherein tasks that the model previously solved successfully may become unmanageable later on (refer to the First Stage in ). This phenomenon may arise due to the model focusing excessively on certain scenarios within the dataset, thereby hindering its ability to generalize effec-tively to diverse scenarios. Additionally, the loss may become too small to produce substantial gradients necessary for con-verging to higher levels of accuracy.",
  "To address these challenges, we propose a two-stage fine-": "tuning strategy aimed at introducing more variability into the training process to promote convergence to higher accuracy levels. Specifically, we reload the best-performing model from the first stage and then reduce the batch size and simulation steps per environment step during training to decrease the vol-ume of data sampled per step. By reducing the data volume sampled per step, we introduce more noise into the training process, leading to larger losses and gradients. Consequently, the model can escape local minima and converge to higher levels of accuracy (refer to ).",
  "Through a series of experiments employing various scale": "strategies for batch size and simulation steps per environment step, we have discovered an intriguing result: utilizing a scale of 0.8 for batch size and 0.9 for simulation steps per environ-ment step consistently leads to higher accuracy in tasks such as Pour, Fill, Excavate, and Hang. We firmly believe that our two-stage fine-tuning strategy holds promise for enabling re-searchers to delve deeper into other fields as well.",
  "The proposed method is implemented based on the": "Maniskill2 frame . For optimization, we utilize the Adam optimizer with an initial learning rate set to 0.0003 and a batch size of 256, in accordance with the approach outlined in ManiSkill2. As for the controller, we implement the pd-joint-delta-pos in all tasks, which has been integrated into ManiSkill2. Additionally, the initial number of simulation steps per environment step is configured to 500. Our demon-stration translation process adheres to the guidelines estab-lished by the ManiSkill2 benchmark.",
  "The findings are consolidated in Table I and Table II. Table": "I present the outcomes of our investigations across six distinct tasks conducted over 100 trials, each initialized with three distinct random seeds. Table II compares the performance of our method against the ManiSkill2 baseline , current state-of-the-art (SOTA) imitation learning methods , , and the second (ChenBao) and third place (Dee) finishers in the ManiSkill2 Challenge across the six soft-body tasks of the Challenge. As elucidated in Table I, our method incorporates advanced techniques such as Behavior Cloning from Demon- strations, a Two-stage Fine-tuning Strategy, and a Guided Self-attention Module. Collectively, these enhancements ena-ble our proposed policy (Method IV) to demonstrate superior performance, achieving an average accuracy of 43% across the evaluated tasks. Table II highlights that our proposed policy achieved the highest score in the ManiSkill2 Challenge, sur-passing the Diffusion Policy and RoboCook by average mar-gins of 10% and 21%, respectively, across the six tasks. Par-ticularly noteworthy is the achievement of a 0 to 1 break-through on the Pinch task. Subsequently, we will conduct an in-depth analysis to discern the individual contributions of each introduced technique.",
  ") Effect of Two-stage Fine-tuning Strategy: From the data": "presented in Table I, it is evident that [ManiSkill2 with our Two-stage Fine-tuning Strategy] (Method II), exhibits a note-worthy enhancement in success rate, showcasing a 6% im-provement on average across the six tasks compared to the ManiSkill2 baseline (Method I). Furthermore, our policy [Be-havior Cloning with our Guided Self-attention module and Two-stage Fine-tuning Strategy] (Method IV) demonstrates a significant boost in success rate, with a commendable 10% improvement on average across the six tasks when compared to [our policy lacking the Two-stage Fine-tuning Strategy] (Method III). In , the accuracy curves depicting the per-formance with the Two-stage Fine-tuning Strategy across var-ious tasks are illustrated. Each evaluation point is derived from 100 episodes randomly selected with different random seeds. Notably, for the Excavate and Pour tasks, both Method I (depicted by the blue line) and Method III (depicted by the green line) exhibit a decline in accuracy following their top-1 accuracy points. This trend indicates the potential existence of overfitting issues in Method I and Method III. Through the application of our Two-stage Fine-tuning Strategy, both Method II and Method IV are able to introduce additional noise in certain gradient steps. Consequently, this results in a larger loss derived from the policy actions and the demonstra-tion trajectory, thereby amplifying the gradient of the subse-quent training step. As depicted by the orange line and red line in , this strategy facilitates the policy in escaping local minima points and achieving convergence to higher accuracy levels.",
  "Ours 0.95 0.87 0.39 0.33 0.01 0.00 0.43": "bined with [PointNet] (Method I), by 8% across the six tasks on average. The superiority of Method III can be attributed to our Guided self-attention module, which enables the model to capture more global information within the feature map. In essence, this means that the model equipped with our guided self-attention module can effectively extract long-distance interactions within the feature map. An illustrative example of the effectiveness of our guided self-attention module is depict-ed in (a) and (b). When the target cup is positioned far away from the bottle, the ManiSkill2 baseline fails to capture the relationships between the cup and bottle, resulting in erro-neous action sequences such as pouring water onto the table instead of into the target cup. In contrast, our guided self-attention module leverages skip connections to reuse previous",
  ". Behavior cloning best actions for the tasks of Pinch and Write": "feature maps in the network and computes cosine similarities between these reused feature maps and those that undergo straightforward channel-wise reshaping. Specifically, vectors containing the position information of the cup can be mapped to vectors containing the position information of the bottle by computing cosine similarities between them. Consequently, the agent can successfully match the bottle with the target cup. As depicted in , Method III (indicated by the green line) exhibits convergence to higher accuracy compared to Method I (indicated by the blue line), which does not incorporate our guided self-attention module.",
  ") Further analysis of Soft-body tasks: We note a distinct": "variance in the precision required across different tasks, which can lead to variations in accuracy scores even among tasks within the same category. For instance, tasks such as Pour and Fill both entail the manipulation of soft-body objects (liquid or clay) into a designated container. However, Fill exhibits a notably higher success rate compared to Pour. The underlying reason for this discrepancy lies in the precision demanded by each task. While Fill allows the robot agent to simply transfer all clay into the beaker, Pour necessitates a higher level of precision, specifically requiring the final liquid level to align precisely with a target line. Consequently, agents must meticu-lously control the tilt angle of the bottle to regulate the amount of liquid poured into the beaker accurately. Similarly, in the case of Excavate, agents must exercise keen judgment regard-ing the depth of excavation required to scoop up a specified quantity of clay. Conversely, tasks such as Hang do not man-date high-precision measurements from the agent, rendering them comparatively easier to accomplish. Furthermore, our observations indicate that Behavior Cloning agents struggle to effectively leverage target shapes to facilitate precise soft-body deformation. Notably, tasks such as Pinch and Write, which entail shape manipulation, present significant challeng-es for Behavior Cloning models, resulting in notably poor per-formance. As depicted in , while the robot learns the basic motion of pinching and demonstrates some progress toward the objective, the achieved level of proficiency falls short of the desired outcome. Similarly, in tasks such as Write, while the robot agent exhibits some capability in reproducing patterns, the resemblance to the target character remains insuf-ficient.",
  "In this paper, we address the challenges of overfitting in": "soft-body tasks by introducing our Two-stage Fine-tuning Strategy, and tackle the issue of capturing long-distance inter-actions through the implementation of our guided self-attention mechanism. We present a novel policy, the Guided Point Cloud to End-effector (GP2E) policy, which can seam-lessly integrate the point cloud data into the robots end-effector frame. Our experimental findings showcase that our methods yield notably higher success rates across six tasks when compared to existing baselines. Furthermore, our abla-tion studies validate the efficacy of each introduced technique.",
  "Gu J, Xiang F, Li X, et al. ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills,\" in Proc. Eleventh International Conference on Learning Representations. 2022": "C. R. Qi, H. Su, K. Mo, and L. J. Guibas, \"Pointnet: Deep learning on point sets for 3d classification and segmentation,\" in Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 652-660. Min Z, Zhu D, Ren H, et al., \"Feature-guided nonrigid 3-d point set registration framework for image-guided liver surgery: From isotropic positional noise to anisotropic positional noise,\" IEEE Transactions on Automation Science and Engineering, vol. 18, no. 2, pp. 471-483, 2020.",
  "robot manipulation skill learning via environment dynamics and constraints modeling,\" IEEE Transactions on Automation Science and Engineering, vol. 19, no. 4, pp. 3903-3913, 2022": "Xuetao Li received the B.S. degree from the Civil Aviation University of China, Tianjin, China. He is currently a Graduate Student at the School of Elec-trical Engineering, Guangxi University, Nanning, China. His research interests include computer vision and embodied artificial intelligence. Fang Gao received the B.S. and Ph.D. degrees in Chemical Physics from University of Science and Technology of China in 2004 and 2010. After grad-uation, he worked as an Assistant Professor and an Associate Professor in the Institute of Intelligent Machines, Chinese Academy of Sciences. He is currently a Professor in the College of Electrical Engineering, Guangxi University, Nanning, Guang-xi, China. His major research interests include mul-timedia computing, embodied artificial intelligence and quantum machine learning. His research work has been published in TMM, TCSVT, TSTE, RAL, ACM MM, etc. He has won champions from many Grand Challenges, such as CVPR 2023 Embodied AI Workshop ManiSkill2 Challenge and CVPR 2024 Embodied AI Workshop ManiSkill-ViTac Challenge, and the 2nd CCF Origin Pilot Cup Quantum Computing Challenge in the professional group. Jun Yu Jun Yu is currently an associate professor and laboratory director with the Department of Automation and the Institute of Advanced Tech-nology, University of Science and Technology of China. His research interests are Multimedia Com-puting and Intelligent Robot. He has published 200+ journal articles and conference papers in TPAMI, IJCV, JMLR, TIP, TMM, TASLP, TCYB, TITS, TCSVT, TOMM, TCDS, ACL, CVPR, ICCV, NeurIPS, ICML, ICLR, MM, SIGGRAPH, VR, AAAI, IJCAI, etc. He has received 6 Best Paper Awards from premier conferences, including CVPR PBVS, ICCV MFR, ICME, FG, and won 50+ champions from Grand Challenges held in NeurIPS, CVPR, ICCV, MM, ECCV, IJCAI, AAAI. Shaodong Li received the M.S. degree in mechani-cal and electronic engineering from Northeastern University, Shenyang, China, in 2015. He received the Ph.D. degree in mechanical and electronic engineering from Harbin Institute of Technology, Harbin, China, in 2021. He is currently an assistant professor in Guangxi University, Nanning, China. His research interests include human-robot coop-eration and robotic intelligent manipulation. His research work has been published in IEEE Robotics and Automation Letters, IEEE Transactions on",
  "Human-Machine Systems, Pattern Recognition Letters, etc": "Feng Shuang received the B.S. degree from the Special Class of Gifted Young, University of Sci-ence and Technology of China (USTC), in 1995, and the Ph.D. degree from the Department of Chemical Physics, USTC, in 2000. He was a Re-search Associate with Princeton University, from 2001 to 2003, and was a Research Staff Member at Princeton University, from 2004 to 2009. In 2009, he joined the Institute of Intelligent Machines (IIM), China, as a Full Professor, and then was selected as a member of the One Hundred Talented People of Chinese Academy of Sciences. He is currently a Professor with Guangxi University. His research interests include system control and in-formation acquisition, including multidimensional force sensors, quantum system control."
}