{
  "Abstract": "There has been growing interest in facial video-based re-mote photoplethysmography (rPPG) measurement recently,with a focus on assessing various vital signs such as heartrate and heart rate variability. Despite previous efforts onstatic datasets, their approaches have been hindered by in-accurate region of interest (ROI) localization and motionissues, and have shown limited generalization in real-worldscenarios. To address these challenges, we propose a novelmasked attention regularization (MAR-rPPG) frameworkthat mitigates the impact of ROI localization and complexmotion artifacts. Specifically, our approach first integratesa masked attention regularization mechanism into the rPPGfield to capture the visual semantic consistency of facialclips, while it also employs a masking technique to preventthe model from overfitting on inaccurate ROIs and subse-quently degrading its performance. Furthermore, we pro-pose an enhanced rPPG expert aggregation (EREA) net-work as the backbone to obtain rPPG signals and atten-tion maps simultaneously. Our EREA network is capableof discriminating divergent attentions from different facialareas and retaining the consistency of spatiotemporal at-tention maps. For motion robustness, a simple open sourcedetector MediaPipe for data preprocessing is sufficient forour framework due to its superior capability of rPPG signalextraction and attention regularization. Exhaustive experi-ments on three benchmark datasets (UBFC-rPPG, PURE,and MMPD) substantiate the superiority of our proposedmethod, outperforming recent state-of-the-art works by aconsiderable margin.",
  "(a) HR evaluation (MAE)(b) Attention comparison": ". The model degradation due to inaccurate and inconsis-tent ROI localizations on PURE dataset under the flip senmaticconsistency strategy. n and f in (a) mean the normal data andhorizontally flipped data, respectively. i.e., n/f denotes that themodel trained with normal input and tested on flipped data. (b)shows the inconsistent attention map samples which are supposedto act as mirror attention regions. texts. Its measurement is widely used to assess cardiovascu-lar health, monitor physical activity, and diagnose diseases.Medical devices such as electrocardiography (ECG) andphotoplethysmography (PPG)/Blood Volume Pulse (BVP)recorders can measure these signals through skin contact.However, these electrodes and wires need to be attachedto body skins, probably causing discomfort and inconve-nience to users, and even triggering allergic reactions, fi-nally limiting their usefulness and scalability . Inrecent years, there has been a surge of interest in remotelymeasuring physiological signals from human facial videoscaptured by RGB cameras . This technology hasvarious applications, such as healthcare in nursing homes and driver status assessment . The majority of remote physiological measurementmethods employ the principle of remote photoplethysmog-raphy (rPPG) , which tracks the periodic reflectedambient light changes in skin color caused by the variationin blood volume . Nonetheless, rPPGsignals are susceptible and vulnerable to interference from",
  "arXiv:2407.06653v1 [cs.CV] 9 Jul 2024": "non-periodic noise sources, including illumination changesof videos, facial expression changes, and body/head move-ments . Previous traditional studies haveproposed blind signal separation techniques andskin reflection models to isolate rPPG signals fromnoises, consisting of multiple process such as rPPG ex-traction, filtering and post-processing. However, these ap-proaches rely on certain assumptions, such as the assump-tion that the white light remains constant across individualsand no movement during video recording, which may notalways hold true. That is to say, these methods are alwaystested on controlled ideal scenarios and show little robust-ness on illumination changes and head movements. As a re-sult, these methods are expected to experience performancedegradation in real-world scenarios. Recently, there has been an increasing attention for ap-plying deep learning techniques to remote photoplethys-mography due to its potential to revolutionize health moni-toring and diagnostics. Researchers have proposed variousneural network models that leverage convolutional neuralnetworks (CNNs) to extract spatiotemporal features fromfacial videos .These models havedemonstrated their superior performance compared to tradi-tional methods. However, there remains a critical challengein accurately localizing the region of interest (ROI) of theface areas during the model training process . Noisydata, resulting from either different-view recordings anddata transormations, or illumination variations in real-worldscenarios, can significantly affect the accuracy of ROI lo-calization. This issue is particularly concerning since facialROIs are known to change location in response to changesin blood volume, making it essential to identify their correctlocations along with time for reliable rPPG measurements.Inaccurate ROI localization can result in the loss of valuablepulsation information, ultimately leading to poor general-ization performance. Previous methods simply use learn-able attention masks to assign weights to skin areas, likeDeepPhys , but there is no guarantee for attention con-sistency in the view of spatial and temporal dimension. Asillustrated in , training only with horizontally flippeddata suffers a lot from inaccurate ROI localizations, finallyleading to model performance deterioration. To overcomethis limitation, we introduce a novel masked attention reg-ularization framework that enables accurate face ROI local-ization, thereby achieving reliable rPPG measurements. Motion robustness is another challenge in remote pho-toplethysmography measurement, which is further exacer-bated by the limitations of CNNs in handling spatial con-textual ROI localization and motion-related issues. The rel-atively small receptive field of CNNs makes them suscep-tible to interference from various types of motions, includ-ing head movements and body language, which can signif-icantly degrade the accuracy of rPPG measurement. While some existing end-to-end models have been tested on staticdatasets and neglected the motion robustness problem ,they remain vulnerable when encountering realistic scenar-ios with noticeable head movements. Only a few researchstudies have attempted to improve motion robustness byincorporating ROI tracking or motion representation tech-niques, but these methods have shown mixed results, espe-cially when dealing with more complex scenarios like walk-ing or exercising . Therefore, developing effectivemotion robustness strategies remains an open challenge inrPPG research. In this paper, we propose a novel masked attention regu-larization (MAR-rPPG) framework to guarantee attentionconsistency and alleviate motion impact for better rPPGmeasurement. Firstly, we introduce a masked attention reg-ularization training framework to capture spatial and tempo-ral attention consistency. Attention consistency means thatthe learned attention maps ought to conform to the identi-cal transformation as the input images, thereby improvingthe models ability to identify patterns and relationships be-tween input features and labels. We find that flip seman-tic consistency is a simple but effective way, which meansoriginal video clips and their flipped counterparts shouldhave similar attention maps. Furthermore, inspired by REAmodule in this work , we propose an enhanced rPPGexpert aggregation (EREA) network to discriminately ex-tract rPPG signals from different face regions and supervisethe consistency of the original and flipped attention maps.Finally, unlike those complex and costly motion-robust so-lutions, we simply exploit the MediaPipe detector forpreprocessing since it is enough for our excellent model totackle with non-rigid/rigid motion issues.",
  "Overall, our MAR-rPPG has three key contributions:": "1. We introduce a masked attention regularization trainingframework that focuses on capturing both spatial andtemporal attention consistency in remote photoplethys-mography measurements. This framework enables themodel to learn a robust pattern and representation fromthe input data. 2. We propose an enhanced rPPG expert aggregation(EREA) network paired with an attention consistencyloss function to discriminately assign different attentionsto face regions and supervise the consistency of the orig-inal and flipped attention maps. 3. We conduct comprehensive experiments on three widelyused benchmark datasets, namely UBFC-rPPG ,PURE , MMPD . Experimental results demon-strate that our approach substantially outperforms thestate-of-the-art methods under both simple and complexreal-world scenarios.",
  ". Related works": "Traditional facial video-based remote physiological mea-surement methods initially utilize blind signal separa-tion techniques, such as independent component analysis(ICA) or principal components analysis (PCA) ,and skin reflection models .Blind sig-nal separation-based approaches assume that skin colorchanges are a linear combination of the desired rPPG signaland other noise sources. For instance, Macwan et al. utilized autocorrelation to guide ICA for separating rPPGsignals, while McDuff et al. employed ICA to estimaterPPG signals from multiple facial videos captured at diverseangles. On the other hand, skin reflection model-based ap-proaches aim to transform images from the RGB space toalternative color spaces for enhanced rPPG estimation. Forexample, Haan et al. projected images into the chromi-nance space to minimize motion noise and facilitate rPPGestimation. However, these methods rely on questionableassumptions, such as the notion that different individualsskin tones appear identical under white light, which maynot hold true in practical environments . A com-prehensive review of these conventional techniques can befound in .Recent studies have demonstrated that deep neural net-work (DNN)-based approaches can achieve superior per-formance in remote physiological measurement . Researchers have developed various DNN-basedmethods to address different challenges in remote physio-logical measurement. Notably, DeepPhys and PhysNet introduced end-to-end convolutional neural network(CNN) frameworks that have achieved promising results.Additionally, spatio-temporal signal map-based methods,such as those presented by RhythmNet and Dual-GAN , have gained popularity due to their impressiveperformance.In pursuit of efficient rPPG measurement,researchers have explored lightweight end-to-end models,such as Auto-HR and EfficientPhys . These mod-els aim to reduce computational complexity while maintain-ing accuracy. Moreover, PhysFormer employs tempo-ral difference transformers to investigate long-range spatio-temporal relationships in rPPG representations, further im-proving performance. Despite their high performance onrPPG measurements, these methods encounter difficultiesin accurately localizing regions of interest (ROIs) and main-taining consistent attention throughout the time sequence ofvideo clips.To facilitate motion-robust research, Paruchuri et al. explored a data augmentation by transferring motion to ex-isting datasets and then trained on these motion-augmenteddatasets to improve the model generalization. Li et al. suggested two modular components: the physiological sig-nal feature extraction block (PFE) and the temporal facealignment block (TFA). These modules aim to mitigate the impact of varying distances and head movements on re-mote physiological measurement, thereby enabling motion-robust studies. However, integrating these modules signifi-cantly increases the computational load and time cost, pos-ing considerable challenges for independent researchers orthose with limited resources in the field. Instead, we onlyutilize the MediaPipe toolkit for face tracking preprocessingsince our proposed method has the outstanding capability toextract rPPG signals from semi-processed data.",
  ". Masked attention regularization": "In this paper, we propose a novel framework, called MAR-rPPG, designed to enhance rPPG signals by improved ROIlocalization.The overview of our proposed MAR-rPPGframework is illustrated in , the proposed method con-sists of three main stages: feature extraction, signal and at-tention map generation, and network optimization. First, inthe preprocessing process, we utilize the open-source facedetector MediaPipe to detect, track, align, and crop the ex-tended face area in each frame of the input video. Next,we create both original and flipped versions of the alignedfacial frames, divided into chunks of T frames each. Nowwe have original input X and flipped input X and corre-sponding PPG signal label Z, we then pass these two inputX and X into the EREA backbone to generate correspond-ing rPPG signals and attention maps, denoted as Y , M, Y ,M , respectively. Then, our EREA network (Details in Sec.3.2) can obtain a rPPG prediction by aggregating pulsationinformation from diverse face regions and employing an at-tention regularization to guarantee accurate and consistentROI locations. In the network optimization phase, we applya regression loss to the signals from Y and Y , comprisingL1 loss and negative Pearson loss. Additionally, we enforceattention consistency loss between attention maps from Mand M , ensuring that identical ROI attentions are assignedto original and flipped inputs.The underlying rationale for MAR-rPPG is grounded inthe discernment that estimated heart rate values manifestdissimilarities pre- and post-flipping transformations, in-dicative of divergent ROI localizations during model train-ing. Typically, rPPG signal extraction is confined to spe-cific attention areas, posing a challenge for consistent local-ization. To overcome this, we integrate an attention regu-larization mechanism into the framework, ensuring atten-tion homogeneity under prescribed image augmentationsto enhance ROI localization within video sequences. Ourmethod employs an indirect strategy to guide the back-bones attention towards regions exhibiting heightened re- . The overview of our proposed MAR-rPPG. The MAR-rPPG consists of one Encoder and one EREA network with shared weightsfor two inputs. First, the encoder encodes the input video into a feature tensor. Next, this feature tensor is sent to the EREA network andEREA outputs a rPPG signal and attention maps. Specifically in the EREA, the feature tensor is divided into four equal parts, and each partis pass to its corresponding Expert E module to generate attention maps and extract a rPPG signal corresponding to one of facial regions.Finally, a Gate module G aggregates four different rPPG signals into one rPPG prediction. gression correlation. Our MAR-rPPG introduces a globaland dynamic attention regularization framework, which dif-fers from the conventional practice that explicitly focuseson part of facial regions relevant to regression target duringtraining, offering an indirect modality to improve networkattention capability.The attention regularization technique allows the modelto produce consistent attention maps and rPPG signals, butit may not completely eliminate the negative effects of theflipped frames, as the model can still memorize the flippedversion, leading to model performance degradation. To ad-dress this issue, we introduce a masking strategy. Beforepassing facial frames into model, we randomly mask partof facial area to prevent the model from overfitting on theflipped frames and encourage it to focus on the underlyingpatterns of input data.",
  ". Enhanced rPPG expert aggregation network": "As illustrated in , the enhanced rPPG expert aggrega-tion (EREA) network is composed of four Expert modulesand one Gate module. The feature tensor from Encoder isdivided into four equal parts, and each part is sent to itscorresponding Expert to extract the rPPG signal and atten-tion maps. Then, the Gate module aggregates four differentrPPG signals from Experts into one rPPG prediction.The motivation of the EREA network is that differentfacial regions have distinct distributions of blood vesselsand noises, and are supposed to contribute differently to therPPG estimation. The EREA network serves as the back-bone of MAR-rPPG framework and plays two roles in thispipeline: an attention map generator and a rPPG signal ex-tractor.For one thing, we equip the vanilla Expert module in with a Class Activation Mapping (CAM) unit to ensurethe consistency of high-dimensional attention maps and vi-sualizing the ROI locations on facial areas, since attentionconsistency assumes that learned attention maps are sup-posed to follow the identical transformation as input im-ages, which benefits to achieve better visual explainability.Therefore, our EREA network is able to highlight discrimi-nate ROIs relevant to pulsation information and enables thenetwork to better understand the spatial relationships be-tween different parts of face and its underlying physiologi-cal signals.Specifically, an attention map is created by taking theweighted sum of the feature maps produced by the finalconvolutional layer, where the weights are derived from afully connected (FC) layer. We denote the feature map asF RCHW , where C, H, and W represent the numberof channels, height, and width of the feature map, respec-tively. Let W RT C as the weights of the FC layer, T isthe regression dimension of rPPG signal chunks. The atten-tion map is as below.",
  "c=1W(T, c)Fc(h, w)(1)": "where c C, h H, and w W.For another, the generated attention maps are also ben-eficial for the rPPG signal extractor. They help the extrac-tor learn more robust patterns and representations, which inturn enhances its ability to accurately detect and analyze thesubtle changes in facial color that indicate cardiovascularactivity. This is particularly useful in situations where thesubject is experiencing complex motions or facial expres-sions, as the attention maps help the network to selectively",
  "Regression loss": "Our goal is to recover rPPG signals with similar trends andpulse peak timings that coincide with the ground truth sig-nals. To achieve this, we employ L1 loss lossl1 and neg-ative Pearson correlation loss loss as the regression losslossreg to optimize the trend similarity and minimize peaklocation errors, thereby ensuring the recovered rPPG signalsclosely mirror the ground truth signals.",
  "Attention consistency loss": "In addition to the regression loss, we also incorporate anattention consistency loss lossac into our optimization pro-cess to ensure that the models attention mechanism pro-duces consistent results across frames of original and itsflipped versions, further improving the quality and reliabil-ity of the recovered rPPG signals. The attention consistencyloss is computed as",
  ". Datasets": "We conduct our experiments on three public datasets:PURE, UBFC-rPPG, MMPD.PURE. The PURE dataset comprises 60 facial videosof 10 participants, captured during a data collection pro-cess that involved asking the subjects to perform six typesof head movements (including small and medium rotations,slow and fast translations, talking, and steady positions) infront of a camera for one minute. The videos were recordedat a frame rate of 30 frames per second and a resolution of640 x 480 pixels. Simultaneously, the ground truth PPG sig-nals were collected using a finger clip pulse oximeter witha sampling rate of 60 Hz.UBFC-rPPG. The UBFC-rPPG dataset includes 42high-quality facial videos, each featuring a simultaneousrecording of both PPG signals and heart rates. Each videoboasts a crisp resolution of 640x480 pixels and a smoothframe rate of 30 frames per second (FPS).MMPD. Introducing the Multi-domain Mobile VideoPhysiology Dataset (MMPD), an extensive repository of 11hours of Samsung Galaxy S22 Ultra mobile phone record-ings from 33 participants. Captured at 30 frames per sec-ond and originally rendered at 1280720 pixels, the videoswere later scaled down to 320240 pixels for efficient stor-age and transfer. In parallel, the accompanying PPG sig-nals were downsampled from 200Hz to 30Hz, matching thevideo frame rate and yielding 1800 frames per video. It con-tains Fitzpatrick skin types 3-6, four different lighting con-ditions (LED-low, LED-high, incandescent, natural), fourvarious activities (stationary, head rotation, talking, andwalking), and exercise scenarios. To foster exploration ofvarious factors influencing remote photoplethysmography,MMPD meticulously assigned diverse labels, encompass-ing skin tone, gender, glasses, hair coverage, and makeup.",
  ". Implementation Details": "For each video, we use the MediaPipe toolkit to detectand crop the enlarged face area and then rescale them to aresolution of 64x64 pixels. We then divide each video clipinto chunks of 60 frame length. We also flip images in eachchunk as flipped input for attention consistency learning.Its worth noting that the MMPD dataset scales the videosto a resolution of 320 240, which can result in a distortedimage with an incorrect aspect ratio. As such, its necessaryto resize the frames from MMPD to 320 180 before usingthem for training or evaluation purposes. To the best of ourknowledge, we are the first to successfully train a model onthe MMPD dataset and achieve satisfactory results.Furthermore, we train our model for 30 epochs with twoNVIDIA GeForce RTX 4090 GPUs using PyTorch 1.12.1,employing the Adam optimizer and setting the batch size to 4. Our initial learning rate is 1e-3 and the OneCycle sched-uler mechanism is utilized in our experiments. we set therandom mask size to 16 16 pixels, and hyperparameter is set to 0.3 and is 0.5. Besides, the chunk length T is setto 60. We implement our proposed MAR-rPPG frameworkbased on open source rppg-toolbox .",
  ". Metrics and evaluation": "We follow the approach outlined in by using mean ab-solute error (MAE), root mean square error (RMSE), meanabsolute percentage error (MAPE), and Pearsons correla-tion coefficient (r) as evaluation metrics for heart rate (HR).Moreover, we perform cross-dataset HR evaluation amongPURE, UBFC-rPPG, and MMPD. For details, refer to Sec.1 of the Supplementary Material.",
  "HR evaluation": "For HR evaluation, we include eight advanced approaches:Meta-rPPG , PulseGan , Dual-GAN , Phys-former , REA-LFA , EfficientPhys , TS-CAN, PhysNet . We validate their model performanceon three datasets as shown in Tab. 1. It is worth noting thatwe did not include certain methods in our experiments onthe MMPD(s) dataset, as their reproduction was not feasibledue to various reasons such as unavailable code or inconsis-tent implementation details.We observe that our proposed MAR-rPPG method out-performs all compared methods across all evaluated met-rics. Notably, it achieves a MAE of 0.08 and a RSME of0.29, vastly superior than the second-best performer Dual-GAN on the PURE dataset, which has a MAE of 0.82 anda RMSE of 1.31. Additionally, our method obtains a per-fect Pearsons correlation coefficient (r) of 1.00 on both theUBFC-rPPG and PURE datasets, signifying an exact matchbetween the predicted and actual heart rates. Of particu-lar note is the impressive performance on the challengingMMPD dataset captured using a mobile phone, which in-cludes diverse scenarios such as glasses, makeup, and vary-ing skin tones. Despite these complications, our methodmaintains a remarkably low MAE of 0.87 on the MMPD(s)dataset, showcasing the efficacy of our masked attentionregularization framework in enhancing ROI localization andbolstering the resilience of the rPPG model.",
  "Cross dataset evaluation": "In order to evaluate the robustness and adaptability ofour proposed method, we conducted a comprehensivecross-dataset examination on the UBFC-rPPG, PURE, andMMPD datasets. The results of this experiment are pre-sented in Tab. 2.We conduct four cross-dataset tests,and MMPD(s)UBFC-rPPG means the metric trained on Average of rPPG and PPG HR [bpm] Diff between rPPG and PPG HR [bpm] Mean Error+95% Confidence Interval-95% Confidence IntervalObservations GT PPG HR [bpm] rPPG HR [bpm]",
  ". The Bland-Altman plot (a) and scatter plot (b) showthe difference between estimated HR and ground truth HR on thecrossdataset evaluation (UBFC-rPPG PURE)": "MMPD(s) and tested on UBFC-rPPG. Our proposed MAR-rPPG outperforms other approaches in terms of overall per-formance and generalization ability.Specifically, whilePhysformer achieves impressive results on the UBFC-rPPGdataset, its performance deteriorates significantly when ap-plied to the PURE dataset. In contrast, our method main-tains consistent excellence across both datasets, indicatingits robustness and adaptability to diverse scenarios. Addi-tionally, Dual-GAN displays notable performance in certainaspects, such as RMSE and correlation coefficient (r). Thecross-dataset evaluation conclusively demonstrates the ex-ceptional generalization capability of MAR-rPPG, empow-ering it to excel in unanticipated scenarios. and 4 display both a Bland-Altman plot and a scat-ter plot tested on UBFC-rPPG and PURE datasets. Theseplots allow for a comprehensive evaluation of the correla-tion between the ground truth PPG signals and the estimatedrPPG signals. Each data point symbolizes an estimated re-sult of a single test sample. For one thing, the Bland-Altmanplot facilitates the identification of any potential systematicbias or random errors present in the estimation process. For . Comparison of state-of-the-art methods on HR estimation. The results are tested on UBFC-rPPG, PURE, and MMPD datasets. indicates that the larger value is better and vice versa. The best metric is marked in bold. MMPD(s) means that we only test on thestationary scenario of this dataset for fair comparisons with other methods.",
  "MAR-rPPG(Ours)1.746.340.921.465.130.981.676.310.931.287.690.95": "another, the scatter plot enables a thorough examination ofhow well the estimated values align with the actual ones,thereby providing a better understanding of the models pre-cision.From and 4, we can obviously notice that there isonly one outlier for these two datasets while the rest pointsfit pretty well. So this is a foregone result that we surpassother state-of-the-art methods with a MAE of 1.67 underthe test of PUREUBFC-rPPG and 1.28 for UBFC-rPPG PURE.",
  ". Attention regularization": "Tab. 3 depicts the comparison of the vanilla REA andour proposed MAR-rPPG on three public datasets.Theresults demonstrate that MAR-rPPG outperforms vanillaREA across all metrics, a much lower MAE, RMSE, MAPEand a nearly perfect pearson correlation of 1.0 on UBFC-rPPG and PURE datasets. Notably, MAR-rPPG shows re-markable superiority on the more challenging MMPD(s) and MMPD(f) datasets, with a MAE of 0.87 in the staticscenario, surpassing the 0.99 achieved by vanilla REA. No-tably, while the vanilla REA struggles to handle the motionscenarios presented on the MMPD(f) dataset with a MAEof 4.81, our MAR-rPPG method demonstrates superior per-formance and maintain an acceptable MAE of 2.83. Thissuggests that our proposed MAR-rPPG is capable of effec-tively addressing the challenges posed by motion artifacts. As illustrated in , we evaluate the motion-robustperformance of our MAR-rPPG method on two scenarios:non-rigid motion (talking) and rigid motion (walking). Ourmethod accurately estimates the rPPG signals for both sce-narios, as demonstrated by the close match between the pre-dicted and ground truth signals. In particular, the interbeatintervals of the predicted rPPG signals for the talking sce-nario are nearly identical to those of the ground truth PPGsignal, indicating precise estimation. Moreover, our MAR-rPPG successfully captures every waveform of the rPPGsignal during the walking scenario and accurately predictsthe heart rate. . Attention regularizaiton study on three dataset. MMPD(s) means that we remove the motion-related samples for a simple scenario,while MMPD(f) is nearly a full and complex dataset only without skin type 5 & 6.",
  ". The length of Video clips": "We vary the length of each video clip T from 60, 90, 120,150, and 180 in . We can observe that T = 60 appearsto be the best on UBFC-rPPG and PURE. The performanceon PURE is rather stable along with varying T. One thingshould be noted that larger chunk length means more GPUconsumption, but with faster training speed if we fix thebatch size.",
  ". Parameter variation of chunk length": "artifacts in non-contact rPPG measurements. First, we inte-grate a masked attention regularization mechanism to guar-antee the visual semantic consistency of facial clips, whichhelps to reduce the impact of ROI localization errors. Ad-ditionally, we employ a masking technique to prevent themodel from overfitting on inaccurate ROIs, thereby improv-ing its generalization ability to unseen data. Second, wepropose an Enhanced rPPG Expert Aggregation (EREA)network as the backbone to jointly learn rPPG signals andattention maps.The EREA network is designed to dis-tinguish diverse attentions from different facial areas andpreserve the consistency of spatiotemporal attention maps,which improves the motion robustness. Our extensive ex-periments demonstrate that MAR-rPPG outperforms state-of-the-art methods in terms of accuracy and motion robust-ness, making it a promising solution for real-world applica-tions.",
  "Weixuan Chen and Daniel McDuff.Deepphys:Video-based physiological measurement using convolutional atten-tion networks. In Computer Vision ECCV 2018, pages 356373, 2018": "Xun Chen, Juan Cheng, Rencheng Song, Yu Liu, RababWard, and Z. Jane Wang. Video-based heart rate measure-ment: Recent advances and future prospects. IEEE Trans-actions on Instrumentation and Measurement, 68(10):36003615, 2019. Juan Cheng, Xun Chen, Lingxi Xu, and Z. Jane Wang. Illu-mination variation-resistant video-based heart rate measure-ment using joint blind source separation and ensemble em-pirical mode decomposition. IEEE Journal of Biomedicaland Health Informatics, 21(5):14221433, 2017.",
  "Eugene Lee, Evan Chen, and Chen-Yi Lee. Meta-rppg: Re-mote heart rate estimation using a transductive meta-learner.In European Conference on Computer Vision, 2020": "Jianwei Li, Zitong Yu, and Jingang Shi. Learning motion-robust remote photoplethysmography through arbitrary res-olution videos. In Proceedings of the Thirty-Seventh AAAIConference on Artificial Intelligence and Thirty-Fifth Con-ference on Innovative Applications of Artificial Intelligenceand Thirteenth Symposium on Educational Advances in Ar-tificial Intelligence, 2023. Xiaobai Li, Jie Chen, Guoying Zhao, and Matti Pietikainen.Remote heart rate measurement from face videos under re-alistic situations. In 2014 IEEE Conference on ComputerVision and Pattern Recognition, pages 42644271, 2014. Si-Qi Liu and Pong C. Yuen. A general remote photoplethys-mography estimator with spatiotemporal convolutional net-work. In 2020 15th IEEE International Conference on Auto-matic Face and Gesture Recognition (FG 2020), pages 481488, 2020. Xin Liu, Josh Fromm, Shwetak Patel, and Daniel McDuff.Multi-task temporal shift attention networks for on-devicecontactless vitals measurement. In Advances in Neural In-formation Processing Systems, pages 1940019411, 2020.",
  "Xin Liu, Xiaoyu Zhang, Girish Narayanswamy, YuzheZhang, Yuntao Wang, Shwetak Patel, and Daniel Mc-Duff. Deep physiological sensing toolbox. arXiv preprintarXiv:2210.00716, 2022": "Xin Liu, Brian Hill, Ziheng Jiang, Shwetak Patel, and DanielMcDuff.Efficientphys: Enabling simple, fast and accu-rate camera-based cardiac measurement. In 2023 IEEE/CVFWinter Conference on Applications of Computer Vision(WACV), pages 49975006, 2023. Hao Lu, Hu Han, and S. Kevin Zhou. Dual-gan: Joint bvpand noise modeling for remote physiological measurement.In 2021 IEEE/CVF Conference on Computer Vision and Pat-tern Recognition (CVPR), pages 1239912408, 2021. Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris Mc-Clanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-Ling Chang, Ming Guang Yong, Juhyun Lee, Wan-TehChang, Hua Wei, Manfred Georg, and Matthias Grundmann.Mediapipe: A framework for building perception pipelines.ArXiv preprint arXiv:1906.08172, 2019. Richard Macwan, Yannick Benezeth, and Alamin Mansouri.Heart rate estimation using remote photoplethysmographywith multi-objective optimization. Biomedical Signal Pro-cessing and Control, 49:2433, 2019.",
  "Xuesong Niu, Shiguang Shan, Hu Han, and Xilin Chen.Rhythmnet: End-to-end heart rate estimation from face viaspatial-temporal representation. IEEE Transactions on Im-age Processing, 29:24092423, 2020": "Xuesong Niu, Zitong Yu, Hu Han, Xiaobai Li, ShiguangShan, and Guoying Zhao. Video-based remote physiologi-cal measurement via cross-verified feature disentangling. InComputer Vision ECCV 2020, pages 295310, 2020. Ewa Magdalena Nowara, Tim K. Marks, Hassan Mansour,and Ashok Veeraraghavan. Sparseppg: Towards driver mon-itoring using camera-based vital signs estimation in near-infrared. In 2018 IEEE/CVF Conference on Computer Visionand Pattern Recognition Workshops (CVPRW), pages 1353135309, 2018. Akshay Paruchuri, Xin Liu, Yulu Pan, Shwetak Patel, DanielMcDuff, and Soumyadip Sengupta. Motion matters: Neu-ral motion transfer for better camera physiological measure-ment. ArXiv preprint arXiv:2303.12059, 2023.",
  "Ming-Zher Poh, Daniel J McDuff, and Rosalind W Picard.Non-contact, automated cardiac pulse measurements usingvideo imaging and blind source separation. Opt Express, 18:1076274, 2010": "Ming-Zher Poh, Daniel J. McDuff, and Rosalind W. Pi-card. Advancements in noncontact, multiparameter physi-ological measurements using a webcam. IEEE Transactionson Biomedical Engineering, 58(1):711, 2011. Ying Qiu, Yang Liu, Juan Arteaga-Falconi, Haiwei Dong,and Abdulmotaleb El Saddik. Evm-cnn: Real-time contact-less heart rate estimation from facial video. IEEE Transac-tions on Multimedia, 21(7):17781787, 2019. Hamidur Rahman, Mobyen Uddin Ahmed, and Shahina Be-gum. Non-contact physiological parameters extraction usingfacial video considering illumination, motion, movement andvibration. IEEE Transactions on Biomedical Engineering, 67(1):8898, 2020. Hang Shao, Lei Luo, Shuo Chen, Chuanfei Hu, and JianYang. Hyperbolic embedding steered spatiotemporal graphconvolutional network for video-based remote heart rate es-timation. Engineering Applications of Artificial Intelligence,124:106642, 2023. Jingang Shi, Iman Alikhani, Xiaobai Li, Zitong Yu, TapioSeppanen, and Guoying Zhao. Atrial fibrillation detectionfrom face videos by fusing subtle variations. IEEE Transac-tions on Circuits and Systems for Video Technology, 30(8):27812795, 2020. Rencheng Song, Huan Chen, Juan Cheng, Chang Li, Yu Liu,and Xun Chen. Pulsegan: Learning to generate realistic pulsewaveforms in remote photoplethysmography.IEEE Jour-nal of Biomedical and Health Informatics, 25(5):13731384,2021.",
  "Zitong Yu, Xiaobai Li, and Guoying Zhao. Remote photo-plethysmograph signal measurement from facial videos us-ing spatio-temporal networks.In British Machine VisionConference, 2019": "Zitong Yu, Wei Peng, Xiaobai Li, Xiaopeng Hong, andGuoying Zhao. Remote heart rate measurement from highlycompressed facial videos: An end-to-end deep learning so-lution with video enhancement. In 2019 IEEE/CVF Interna-tional Conference on Computer Vision (ICCV), pages 151160, 2019. Zitong Yu, Xiaobai Li, Xuesong Niu, Jingang Shi, and Guoy-ing Zhao. Autohr: A strong end-to-end baseline for remoteheart rate measurement with neural searching. IEEE SignalProcessing Letters, 27:12451249, 2020.",
  "Zitong Yu, Xiaobai Li, and Guoying Zhao.Facial-video-based physiological signal measurement: Recent advancesand affective applications. IEEE Signal Processing Maga-zine, 38(6):5058, 2021": "Zitong Yu, Yuming Shen, Jingang Shi, Hengshuang Zhao,Philip Torr, and Guoying Zhao. Physformer: Facial video-based physiological measurement with temporal differencetransformer. In 2022 IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 41764186,2022. Zijie Yue, Shuai Ding, Shanlin Yang, Hui Yang, Zhili Li,Youtao Zhang, and Yinghui Li. Deep super-resolution net-work for rppg information recovery and noncontact heart rateestimation. IEEE Transactions on Instrumentation and Mea-surement, 70:111, 2021. Zijie Yue, Miaojing Shi, and Shuai Ding. Facial video-basedremote physiological measurement via self-supervised learn-ing. IEEE Transactions on Pattern Analysis and MachineIntelligence, 45(11):1384413859, 2023."
}