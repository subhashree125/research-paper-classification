{
  "Abstract": "While AI-generated content has garnered significant at-tention, achieving photo-realistic video synthesis remains aformidable challenge. Despite the promising advances indiffusion models for video generation quality, the complexmodel architecture and substantial computational demandsfor both training and inference create a significant gap be-tween these models and real-world applications. This pa-per presents SNED, a superposition network architecturesearch method for efficient video diffusion model.Ourmethod employs a supernet training paradigm that targetsvarious model cost and resolution options using a weight-sharing method. Moreover, we propose the supernet train-ing sampling warm-up for fast training optimization. Toshowcase the flexibility of our method, we conduct experi-ments involving both pixel-space and latent-space video dif-fusion models. The results demonstrate that our frameworkconsistently produces comparable results across differentmodel options with high efficiency. According to the ex-periment for the pixel-space video diffusion model, we canachieve consistent video generation results simultaneouslyacross 6464 to 256256 resolutions with a large range ofmodel sizes from 640M to 1.6B number of parameters forpixel-space video diffusion models.",
  ". Introduction": "Generative modeling for video synthesis has made tremen-dous progress based on approaches, including GANs , autoregressive models ,VAEs , and normalizing flows . Among them,GANs have demonstrated remarkable success by extendingthe image-based generation to video generation with ded-icated temporal designs. However, GANs encounter chal-lenges such as mode collapse and training instability, mak-ing it difficult to scale them up for handling complex anddiverse video distributions.To overcome this challenge, diffusion models have been studied, which establish a weighted variationalbound for optimization by connecting Langevin dynam-ics and denoising score matching .Followingthis, approaches such as VDM , MCVD , ImagenVideo , and LVDM extended diffusion models tovideo generation, surpassing GANs in both sample qualityand distribution coverage due to their stable training andscalability . However, this success comes hand in handwith significant challenges posed by enormous model sizesand computational demands associated with diffusion mod-els. These challenges manifest themselves in both the infer-ence and training aspects.Sampling from diffusion models is expensive as their im-mense number of parameters, heavy reliance on attentionmechanisms, and need for several model evaluations resultin substantial memory consumption. Even with advancedGPUs, tackling high-resolution video generation becomes aformidable burden due to this memory constraint. Someresearch efforts, such as Imagen Video , introduce amodel chain to enhance video generation quality gradually.However, this approach further escalates the total model pa-rameters and memory consumption. Beyond this, the exten-sive computational load leads to a significantly longer infer-ence latency, amplifying the deployment cost and user wait-ing time. These barriers have presented substantial impedi-ments to the commercialization of diffusion models, partic-ularly in the context of video diffusion models.Furthermore, when it comes to the training of diffusionmodels, challenges emerge from three key facets. Firstly,due to the substantial model parameter count and computa-tional overhead, training costs soar, often requiring an entiremonth or even longer to train large-scale diffusion mod-els on large datasets from scratch. This protracted train-ing duration poses a challenge to the improvement of dif-fusion models. Moreover, given that diffusion models arestill relatively nascent, our prior knowledge regarding theirstructural design remains limited. Consequently, model de-sign heavily relies on trial and error, incurring additionalexpenses in terms of both time and resources. Lastly, be-cause the objectives of these models vary widely, which has",
  "arXiv:2406.00195v1 [cs.CV] 31 May 2024": "different model size constraints and different target videogeneration resolutions, model architectures often need to betailored differently to suit each specific goal. Training thesediverse models with distinct structures for varying objec-tives introduces additional overhead that can be burdensomeand difficult to manage. In the face of these challenges, it becomes imperativeto explore strategies that mitigate the computational burdenand streamline the network design process achieving dif-ferent targets including cost constraints and resolution re-quirements at the same time. This is crucial not only forenhancing the efficiency of diffusion models but also forfacilitating their broader applicability across various real-world scenarios. In this paper, we introduce SNED, a superpositionnetwork architecture search method for efficient videodiffusion models, designed to achieve efficient model im-plementation without compromising high-quality genera-tive performance. We explore the combination of networksearch with video diffusion model and enable a flexiblerange of options towards resolution and model cost, sav-ing computation consumption for inference and training.Specifically, we implement a one-shot neural architecturesearch solution, enabling dynamic computation cost sam-pling.This means that once the supernet is trained, itachieves the differentiation of computational costs acrossvarious subnets within the supernet. Besides that, we intro-duce the concept of super-position training into our su-pernet training process. This breakthrough allows a singularsupernet model to effectively manage different resolutions,offering a versatile solution for handling diverse resolutionrequirements. Consequently, this approach permits the re-utilization of super-resolution models in multiple instances,facilitating the training of models with a diverse range ofcost and resolution options concurrently.",
  ". Classic Video Synthesis": "Classic video synthesis endeavors to capture the underly-ing distribution of real-world videos, allowing the gener-ation of realistic and novel video samples.Previous re-search primarily leverages deep generative models, includ-ing GANs , autoregressive mod-els , VAEs , and normalizing flows .Among these, GAN-based approaches stand out as the mostdominant, owing to the remarkable success of GANs in im-age modeling.MoCoGAN and MoCoGAN-HD excel indecomposing latent codes into content and motion sub-spaces. Notably, MoCoGAN-HD utilizes the potentpretrained StyleGAN2 as the content generator, resultingin higher-resolution video generation. StyleGAN-V and DiGAN introduce implicit neural representation toGANs, facilitating the modeling of temporal dynamics con-tinuity. These models build upon StyleGAN3 and employa hierarchical generator architecture for long-range model-ing, enabling the generation of videos with evolving contentover time.Despite the success of GANs, they often face challengessuch as mode collapse and training instability. Autoregres-sive methods have also been explored for video generation.VideoGPT , utilizing VQVAE and a transformer,autoregressively generates tokens in a discrete latent space.TATS enhances the VQVAE with a more power-ful VQGAN and integrates a frame interpolation trans-former for rendering long videos in a hierarchical manner.",
  ". Diffusion Model": "Besides the classic video synthesis models, diffusion mod-els, a category of likelihood-based generative models, haveexhibited notable advancements in image and video synthe-sis tasks, surpassing GANs in both sample quality and dis-tribution coverage due to their stable training and scalabil-ity . Noteworthy among these models is DDPM ,which establishes a weighted variational bound for opti-mization by connecting Langevin dynamics and de-noising score matching .Despite its slow samplingprocess requiring step-by-step Markov chain progression,DDIM accelerates sampling iteratively in a non-Markovian manner, maintaining the original training pro-cess . ADM outperforms GAN-based methods withan intricately designed architecture and classifier guidance.While diffusion models have excelled in image synthe-sis, their application to video generation has been lim-ited. VDM extends diffusion models to the video do-main, introducing modifications such as a spatial-temporalfactorized 3D network and image-video joint training.MCVD unifies unconditional video generation and conditional frame prediction through random dropping ofconditions during training, akin to the classifier-free guid-ance approach. Make-A-Video and Imagen Video leverage diffusion models for large-scale video synthe-sis conditioned on text prompts, which conduct diffusionand denoising processes in pixel space.Besides that,LVDM extends the video generation work to the la-tent space and explores how hierarchical architectures andnatural extensions of conditional noise augmentation enablethe sampling of long videos. However, the efficiency modeloptimization of the video diffusion model is still waitingfor exploration. In this paper, we further explore the com-bination of network search with the video diffusion modeland enable a flexible range of options toward resolution andmodel cost.",
  "NAS Strategies": "There is a growing trend in designing efficient Deep Neu-ral Networks (DNNs) through Neural Architecture Search(NAS). NAS strategies can be broadly categorized intothe following approaches based on their searching strate-gies. Firstly, Reinforcement Learning (RL) methods, suchas , utilize recurrent neural networks as predictors tovalidate the accuracy of child networks over a proxy dataset.Secondly, Evolution methods, exemplified by works , employ a pipeline involving parent initialization, pop-ulation updating, and the generation and elimination of off-spring to discover desired networks.Thirdly, One-ShotNAS, as demonstrated in studies such as , involvestraining a large one-shot model containing all operationsand shares the weight parameters among all candidate mod-els.Weight-sharing NAS, inspired by the above methodolo-gies, has gained popularity due to its training efficiency . In this approach, an over-parameterized supernetis trained with weights shared across all sub-networks inthe search space, significantly reducing computational costsduring the search.Although most of the mentioned works primarily focuson traditional Convolutional Neural Network (CNN) archi-tectures, recent studies have extended the scope to includethe search for efficient Vision Transformer (ViT) architec-ture. Examples include Autoformer , which entanglesthe model weights of different ViT blocks in the same layerduring supernet training with an efficient weight-sharingstrategy. This approach reduces both the training modelstorage consumption and the overall training time.",
  "While generation models have achieved significant successin designing neural architectures, their implementation of-": "ten demands substantial time, effort, and expert knowledge.For instance, devised intricate generators and discrim-inator backbones to efficiently generate high-resolution im-ages. Recognizing the need to alleviate the burden of net-work engineering, researchers have explored efficient auto-matic architecture search techniques for GANs.In 2019, AutoGAN introduced an architecture searchscheme for GANs utilizing NAS algorithms.It defineda search space to capture deformations in GAN architec-ture and employed an RNN controller to guide search op-erations.Later, AutoGAN-Distiller (AGD) is devel-oped by applying AutoML to GAN compression.AGDperforms end-to-end search for efficient generators basedon the original GAN model via knowledge distillation. In2021, alphaGAN is introduced, which is a fully dif-ferentiable search framework solving bi-level minimax op-timization problems. Later, StyleGAN2 expanded thesearch space by integrating backbone characteristics.While the majority of studies have concentrated onGAN-based generation models, the research realm of videodiffusion model NAS remains largely unexplored. Giventhe substantial computation demands of video diffusionmodels, there is a critical need to delve into more efficientvideo diffusion model architecture designs.",
  ". Overview of SNED": "In this paper, we present a framework termed the SNED:Superposition Network Architecture Search for EfficientVideo Diffusion Models, designed to effectively search forand optimize video diffusion models across multiple dimen-sions. Our framework introduces two key advancementsthat address critical challenges in video generation.The overview framework of SNED is shown in .Firstly, we implement a one-shot Neural ArchitectureSearch (NAS) solution, enabling dynamic computation costsampling. This means that once the supernet is trained, itachieves the differentiation of computational costs acrossvarious subnets within the supernet. This feature empow-ers users to select the appropriate subnetwork based on spe-cific model sizes and computational cost constraints, en-hancing flexibility and adaptability. Secondly, we introducethe concept of super-position NAS training into our su-pernet training. This breakthrough allows a singular super-net model to effectively manage different resolutions, of-fering a versatile solution for handling diverse resolutionrequirements. Consequently, this approach permits the re-utilization of super-resolution models in multiple instances,considerably mitigating the memory overhead within thevideo diffusion model framework. By leveraging these ad-vancements, our framework not only streamlines the intri-cate process of video diffusion model optimization but also SuperNet (a) SuperNet Traning Process DifferentResolutionDynamic Cost Sampling TrainingData Resolution A Resolution B Resolution C Subnet with different model size (b) Subnets with different size targeting different resolution options",
  "Output": ". Overview of SNED framework. (a) We train a supernet with network dynamic cost sampling and multiple input resolutionoptions. In each iteration, a subnet of the supernet is sampled for the training, and other parts (grey) is frozen. (b) After the training, weobtain subnets with different model costs for each resolution option. ...... ...... DiffusionBlock ... ...... ... ... ... ... ... DiffusionBlock (a) Dynamic Channel Scheme (b) Fine-grained Dynamic Block Scheme Number of",
  "end forOutput S": "encoded in a supernet denoted as S(P, WP ), where WP isthe weight of the supernet that is shared across all the candi-date architectures. Algorithm 1 illustrates the training pro-cedure of our supernet. Here, our dynamic cost search spaceincludes the dynamic channel space and the fine-graineddynamic block space.The schemes of these two searchspaces are shown in . Here, different color bars denotethe different components inside a diffusion model, whichinclude ResBlock, temporal self-attention, temporal cross-attention, and spatial attention. The length of the color barsdenotes the number of channels of the subnets sampled dur-ing training.Dynamic Channel Search Space: As the different num-bers of channels have different acceleration performancesfor the hardware implementation, the SNED search spaceincludes replacing the original number of channels with dif- ferent percentage ratios, including 100% (full number ofchannels), 90%, 80%, 70%, 60%, 50%, and 40%. Eachlayer inside the diffusion blocks can be assigned an inde-pendent ratio in each iteration of supernet training.Fine-grained Dynamic Block Search space: To expandour search space during the supernet training and investigatethe potential of the video diffusion model, we add the fine-grained dynamic block search process inside each diffusionblock.The basic supernet diffusion block contains fourcomponents: ResBlock (convolutional residual block), tem-poral self-attention block, cross-attention block, and spatialattention block. Our Algorithm enables the drop of a partof the blocks inside the whole diffusion block in each iter-ation of supernet training. Specifically, if all the attentionsinside the diffusion block are dropped, the correspondingfeed-forward layer will also be dropped.",
  ". Super-position Training in SNED": "We introduce the super-position training mechanism to ad-dress different video resolution targets during the supernettraining. Here, super-position refers to the utilization ofweight-sharing techniques, allowing different subnets to ad-just to various resolution processing needs while keepingmost of their weights shared. This approach serves the dualpurpose of parameter efficiency and the ability to achievevideo diffusion models with different resolutions in a singletraining session.During each training iteration, besides the sampling ofthe subnet, we also randomly sample a video generationresolution and preprocess the training data based on that.To balance the training memory workload of different res-olution branches, we constraint the maximum model sizefor different resolutions to ensure an acceptable memoryconsumption.By leveraging this super-position trainingmethod, we are not only optimizing the models resourceallocation but also streamlining the training process itself.This minimizes the computational burden and acceleratesthe development of video diffusion models tailored to dif-ferent resolution needs.",
  ". Supernet Training Sampling Warmup": "To achieve a better and faster NAS training performance,we propose the supernet training sampling warmup strat-egy. This strategy is deployed at the beginning of the super-net training process, improving the supernets stability androbustness during training.We gradually increase our search space for both fine-grained dynamic block and dynamic channel during thetraining, rather than directly applying a full random sub-net sampling among the whole search space at the begin-ning. Specifically, we will apply 30000 training iterationsfor sampling warmup. The minimum percentage of chan-nels and fine-grained blocks will be decreased from 100%",
  ". Experimental Setup": "Inthissection,wepresenttheconfigurationofour SNED framework.Our experiments consist oftwo primary components: the pixel-space video diffusionmodel and the latent video diffusion model.To enablethe different resolution options under the super-positionmechanism, we process the training data into a formsuitable for training our cascading pipeline, we spatiallyresize videos using antialiased bilinear resizing to differentresolutions including 6464, 128128, and 256256.To enable the text-to-video conditional training, a frozentext-encoder is added at the beginning of the modelpipeline.We train the pixel-space video diffusion model pipelineusing an internal dataset comprising 19 million video-textpairs. For the base model and spatial super-resolution (SSR)model inside the pipeline, we use a total batch size of 256and 64 during training, respectively. Both models undergo140,000 training iterations, with a fixed learning rate of0.0001. The training process utilizes 64 A100 GPUs.For the latent-space video diffusion model, We start withLVDM as a baseline and subsequently train it using ouralgorithm. For a fair comparison, we employ the same pub-licly available datasets Sky Timelapse. The hyperparametersettings for our experiments align with those of LVDM to ensure a fair evaluation.",
  ". Pixel-Space Video Diffusion Model NAS": "For the pixel-space video diffusion model, our approach isinspired by the model chain proposed by Imagen-video to realize high-quality video generation. The model chaincomprises the base model and the spatial super-resolutionmodel (SSR). The base model and SSR are determined byour framework (SNED) to meet various computational re-source constraints and resolution targets. Our SNED frame-work allows for different resolution options in SSR modelwith weight-sharing subnets.For the supernet architec-ture of both the base model and SSR model, we apply animagen-like modified 2D UNet. Each block inside the UNetconsists of ResBlock, temporal self-attention and cross-attention, and spatial-attention.To attain different resolutions, we recursively deploy ourSSR model multiple times instead of integrating multipleSSR models, as demonstrated in Imagen-video . Thisapproach significantly reduces the total model size.",
  ". Results of pixel-space video diffusion model for different resolution options": "showcase the flexibility and efficiency of our framework,we add additional autoencoder and autodecoder to a latent-space video diffusion model for evaluation.The wholemodel pipeline follows the basic version of LVDM 1.We first compress video samples to a lower dimen-sional latent space by the video autoencoder. Then we per-form the video generation in the latent space. The encoderand decoder both consist of several layers of 3D convolu-tions. To ensure that the autoencoder is temporally shift-equivariant, we follow to use repeat padding in allthree-dimensional convolutions. The prediction model ap-plies a 3D U-Net architecture to estimate the noise distri-bution, which consists of space-only 1 3 3 shape 3Dconvolution, and spatial attention module.We start with LVDM as a baseline and subsequentlytrain each part of it inside our NAS framework. Similarto the pixel-space diffusion model, we apply the super-position NAS training to the diffusion prediction model.Since the encoder model is only applied during the train-ing stage, we only apply the super-position training on itwithout the dynamic cost NAS.",
  "Pixel-Space Video Diffusion Model Visualization": "In and , we present the results of our pixel-spacediffusion model. shows the generation results fromthe pixel-space SSR model. We show 6 frames for eachof them. Due to the space limitation, we only show the fullmodel size (428M) result of SSR for different resolution op-tions. The corresponding text prompts are listed under eachgroup of video frames. illustrates the visualization ofthe pixel-space base model, transforming the input text (de-picted on the left side of the figure) into the correspondingoutput video. For clarity, we showcase three frames fromeach video using two different noise seeds. Additionally,for each input text, we display results generated by modelsof varying sizes40% (640M), 60% (960M), 80% (1.28B),and 100% (1.6B) of the parameters compared to the super-net with 1.6B number of parameters. This comprehensivevisualization highlights the stability and adaptability of ourvideo generation process achieved through the SNED train-ing strategy.",
  "% 60% 80% 100%": ". Result of different pixel-space base model subnets withdifferent model sizes. The values of percentage indicate the rela-tive model size compared with the supernet. We show the resultsof each subnet with two different noise seeds. 411M, and 274M) and compare them with the releasedmodel from LVDM. The original output of LVDMis featured in the first row for reference. All showcasedvideos in the figure use a consistent resolution of 256256and comprise the same number of frames (16) for uncon-ditional short video generation, aligning with the specifica-tions employed in LVDM . We show the first frame ofeach generated video.From , we can see that, compared with the originalLVDM model, all three subnets provide comparable outputresults for the sky timelapse. Both LVDM and SNED pro-vide generation outputs with high fidelity and diversity, in-",
  ". Model Matrix Evaluation": "For quantitative evaluation, we report the commonly-usedFVD and KVD in our experiment. For the pixel-space video diffusion base model, we calculate FVD andKVD scores between 512 real and fake videos with 12frames, which are presented as FV D12 and KV D12. Allresults for the score evaluation are calculated among tenruns to get the average value. The computation is based onthe internal dataset comprising 19 million video-text pairs.The latency evaluation is based on one Nvidia A100 GPU.",
  "SNED-B1.60544.425.824.4SNED-L1.28490.513.021.2SNED-M0.96452.214.418.1SNED-S0.64472.316.816.0": "As shown in , we report the quantitative evalua-tion for our SNED models of varying sizes - small size40% (640M), medium size 60% (960M), large size 80%(1.28B), and base size 100% (1.6B) of the parameters com-pared to the supernet (1.6B), which are indicated as SNED-S, SNED-M, SNED-L, and SNED-B, respectively. Fromthe results, we can see that all of the subnets show a sta-ble score according to both FVD and KVD, which proves",
  "LVDM548295.120.986.8SNED548298.320.886.8SNED411348.223.574.2SNED274472.328.766.7": "our frameworks robustness.Small subnets even obtainbetter FVD and KVD scores compared with the supernet(SNED-B). Among them, SNED-M achieves the best FVDscore (452.2), and SNED-L achieves the best KVD score(13.0). Our smallest subnet SNED-S obtains a 472.3 FVDscore and a 16.8 KVD score with only 16.0s latency, whichachieves 1.53 of speedup with a better matrix score com-pared with the supernet model (latency 24.4s).For the latent-space diffusion model, we compare ourmatrix score with the baseline LVDM and report themin . The score computation process follows that usedin , utilizing 16 frames of generated fake videos forevaluation on the Sky Timelapse dataset. Here we use thereleased model (548M number of parameters) from LVDMas our supernet architecture, then train it with our dynamiccost schemes.Model size options of 548M, 411M, and274M are shown in the Table.",
  ". Conclusion": "This paper introduces SNED, the superposition network ar-chitecture search for an efficient video diffusion model. Inour training paradigm, we target various model cost andresolution options using a weight-sharing method and in-corporate both dynamic channel and fine-grained dynamicblock to expand our search space. Additionally, we proposethe supernet training sampling warmup to improve the train-ing performance. Our proposed method is compatible withdifferent base architectures such as pixel-space and latent-space video diffusion models. According to the experimen-tal results for the pixel-space video diffusion model, we canachieve consistent video generation results simultaneouslyacross 6464 to 256256 resolutions with a large modelsize range from 640M to 1.6B number of parameters. Tothe best of our knowledge, this is the first NAS frameworktargeting the video diffusion model.",
  "Yonggan Fu, Wuyang Chen, Haotao Wang, Haoran Li,Yingyan Lin, and Zhangyang Wang.Autogan-distiller:Searching to compress generative adversarial networks.arXiv preprint arXiv:2006.08198, 2020. 3": "Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, GuanPang, David Jacobs, Jia-Bin Huang, and Devi Parikh.Long video generation with time-agnostic vqgan and time-sensitive transformer. In European Conference on ComputerVision, pages 102118. Springer, 2022. 1, 2 Xinyu Gong, Shiyu Chang, Yifan Jiang, and ZhangyangWang. Autogan: Neural architecture search for generativeadversarial networks. In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision, pages 32243234,2019. 3 Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng,Zechun Liu, Yichen Wei, and Jian Sun. Single path one-shot neural architecture search with uniform sampling. InEuropean Conference on Computer Vision, pages 544560.Springer, 2020. 3 Jiawei He, Andreas Lehrmann, Joseph Marino, Greg Mori,and Leonid Sigal. Probabilistic video generation using holis-tic attribute control. In Proceedings of the European Confer-ence on Computer Vision (ECCV), pages 452467, 2018. 1,2 Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, andQifeng Chen. Latent video diffusion models for high-fidelityvideo generation with arbitrary lengths.arXiv preprintarXiv:2211.13221, 2022. 1, 2, 3, 5, 7, 8",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-fusion probabilistic models. Advances in neural informationprocessing systems, 33:68406851, 2020. 1, 2": "Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, BenPoole, Mohammad Norouzi, David J Fleet, et al. Imagenvideo: High definition video generation with diffusion mod-els. arXiv preprint arXiv:2210.02303, 2022. 1, 3, 5 Tero Karras, Samuli Laine, and Timo Aila. A style-basedgenerator architecture for generative adversarial networks.In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 44014410, 2019. 3",
  "Manas Sahni, Shreya Varshini, Alind Khare, and AlexeyTumanov.Compofa:Compound once-for-all networksfor faster multi-platform deployment.arXiv preprintarXiv:2104.12642, 2021. 3": "Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Tempo-ral generative adversarial nets with singular value clipping.In Proceedings of the IEEE international conference on com-puter vision, pages 28302839, 2017. 1, 2 Masaki Saito, Shunta Saito, Masanori Koyama, and So-suke Kobayashi. Train sparsely, generate densely: Memory-efficient unsupervised training of high-resolution temporalgan. International Journal of Computer Vision, 128(10-11):25862606, 2020. 1, 2 Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,Oran Gafni, et al. Make-a-video: Text-to-video generationwithout text-video data. arXiv preprint arXiv:2209.14792,2022. 3 Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-seiny. Stylegan-v: A continuous video generator with theprice, image quality and perks of stylegan2. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 36263636, 2022. 1, 2 Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,and Surya Ganguli.Deep unsupervised learning usingnonequilibrium thermodynamics.In International confer-ence on machine learning, pages 22562265. PMLR, 2015.1, 2",
  "generative modeling through stochastic differential equa-tions. arXiv preprint arXiv:2011.13456, 2020. 1, 2": "Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng,Dimitris N Metaxas, and Sergey Tulyakov. A good imagegenerator is what you need for high-resolution video synthe-sis. arXiv preprint arXiv:2104.15069, 2021. 1, 2 Yuesong Tian, Li Shen, Guinan Su, Zhifeng Li, and WeiLiu. Alphagan: Fully differentiable architecture search forgenerative adversarial networks. IEEE Transactions on Pat-tern Analysis and Machine Intelligence, 44(10):67526766,2021. 3 Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and JanKautz.Mocogan: Decomposing motion and content forvideo generation. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 15261535,2018. 1, 2 Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,Raphael Marinier, Marcin Michalski, and Sylvain Gelly. To-wards accurate generative models of video: A new metric &challenges. arXiv preprint arXiv:1812.01717, 2018. 8",
  "Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and AravindSrinivas. Videogpt: Video generation using vq-vae and trans-formers. arXiv preprint arXiv:2104.10157, 2021. 1, 2": "Shan You, Tao Huang, Mingmin Yang, Fei Wang, ChenQian, and Changshui Zhang.Greedynas:Towards fastone-shot nas with greedy supernet.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 19992008, 2020. 3 Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender,Pieter-Jan Kindermans, Mingxing Tan, Thomas Huang, Xi-aodan Song, Ruoming Pang, and Quoc Le. Bignas: Scalingup neural architecture search with big single-stage models.In European Conference on Computer Vision, pages 702717. Springer, 2020. 3 Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, JunhoKim, Jung-Woo Ha, and Jinwoo Shin. Generating videoswith dynamics-aware implicit generative adversarial net-works. arXiv preprint arXiv:2202.10571, 2022. 1, 2 Zhao Zhong, Junjie Yan, Wei Wu, Jing Shao, and Cheng-LinLiu. Practical block-wise neural network architecture gener-ation. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 24232432, 2018. 3"
}