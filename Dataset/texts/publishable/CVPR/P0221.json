{
  "Abstract": "Image enhancement holds extensive applications in real-world scenarios due to complex environments and limita-tions of imaging devices. Conventional methods are oftenconstrained by their tailored models, resulting in dimin-ished robustness when confronted with challenging degra-dation conditions.In response, we propose FlowIE, asimple yet highly effective flow-based image enhancementframework that estimates straight-line paths from an ele-mentary distribution to high-quality images. Unlike previ-ous diffusion-based methods that suffer from long-time in-ference, FlowIE constructs a linear many-to-one transportmapping via conditioned rectified flow.The rectificationstraightens the trajectories of probability transfer, acceler-ating inference by an order of magnitude. This design en-ables our FlowIE to fully exploit rich knowledge in the pre-trained diffusion model, rendering it well-suited for variousreal-world applications. Moreover, we devise a faster infer-ence algorithm, inspired by Lagranges Mean Value Theo-rem, harnessing midpoint tangent direction to optimize pathestimation, ultimately yielding visually superior results.Thanks to these designs, our FlowIE adeptly manages a di-verse range of enhancement tasks within a concise sequenceof fewer than 5 steps.Our contributions are rigorouslyvalidated through comprehensive experiments on syntheticand real-world datasets, unveiling the compelling efficacyand efficiency of our proposed FlowIE. Code is available at",
  "U-Net": ". The diagram of the proposed FlowIE. FlowIE lever-ages rectified flow to unleash the rich knowledge in the traineddiffusion model and build straight-line paths between element dis-tribution and clean images. The framework consistently achievesvisually stunning results in a minimal number of steps and seam-lessly generalizes to various image enhancement tasks, e.g., faceinpainting, color enhancement and blind image super-resolution. notable advancements in image enhancement and a seriesof methods , based on deep learn-ing has been introduced to produce high-quality outcomes.They exhibit commendable performance when confrontedwith specific and well-defined degradations. However, theirefficacy becomes circumscribed when extended to intricateand unpredictable challenges posed by complex real-worldscenarios.In practice terms, we aim to design a robustand efficient framework that excels in image enhancement,proficiently restoring general images affected by a diversespectrum of real-world degradations.The challenge of image enhancement is fundamentallyill-posed, given the absence of explicit constraints govern-ing the restoration process, thus permitting various plau-",
  "condition": ". As shown in (a), diffusion models solve ODEsin curve trajectories. Differently, rectified flow , illustrated in(b), bridges one-to-one straight paths between two distributions,thereby reducing the inference steps. (c) Our proposed FlowIEapplies the flow-based framework to real-world data and discardsthe massive data preparation process. We construct a many-to-onemapping that predicts straight paths to clean images from arbitrarynoise in an elementary distribution with proper guidance. sible high-quality (HQ) results from the low-quality (LQ)inputs. To address this intricate problem, researchers ex-plore approaches based on deep learning models that offerstrong priors to guide the enhancement. We roughly cate-gorize them into predictive , GAN-based and diffusion-based meth-ods.Predictive methods seek to explicitly model theblur kernel from LQ images and restore HQ images withthese predicted parameters.However, their adaptabilityto the complexity of real-world conditions remains lim-ited due to the simple degradation setting and the vul-nerable estimated results.To improve the enhancementquality, some approaches employ the Generative Adver-sarial Network (GAN) to implicitly learn the datadistribution and degradation model. GAN-based methodslike and achieve considerable results with the im-age priors from GANs and high-order degradation mod-els. Nevertheless, the tuning of GAN-based methods posesa persistent challenge attributed to their complex lossesand hyper-parameters.More recently, Diffusion Models(DMs) have demonstrated remarkable capabili-ties in synthesizing visually compelling images. Follow-ing this line, some methods leverage the strong generativeprior from the pre-trained diffusion model to attain high-quality restorations. For example, , and devisezero-shot techniques, which involve the direct utilization ofdiffusion model weights without training. Other methodslike fine-tune the diffusion model to better suit en-hancement tasks. Though diffusion-based methods yieldimpressive outcomes, they are hampered by high compu-tational demands and protracted inference times, renderingthem less practical for industrial applications.To overcome the aforementioned challenges, we proposea simple yet potent framework named FlowIE for diversereal-world image enhancement tasks. FlowIE dramaticallyreduces the inference time by a magnitude of tenfold com-pared to diffusion-based methods while upholding the ex-ceptional quality of the enhancements, as showcased in Fig- ure 1. Our primary objection is to harness the generativepriors of pre-trained diffusion for restoring images besetby general degradation. Diverging from existing diffusion-based methods, we abandon the extensive denoising stepsin diffusion sampling via conditioned rectified flow. Thisapproach straightens the trajectories of probability trans-fer during diffusion sampling, offering a swift and unifiedsolution for the distribution transport in diffusion models.Once the straight path from an elementary distribution tothe real-world HQ image is accurately estimated, we canyield a computationally efficient model since it is the short-est path between two points. However, rectified flow can-not directly adapt to enhancement tasks, given its one-to-one noise-image mapping paradigm and reliance on train-ing with synthetic images that differ significantly from real-world data. To address these limitations, we employ recti-fied flow to predict paths from any noise to one real-worldimage, thus constructing a many-to-one transport mapping,as shown in . This relaxation of rectified flow al-lows us to avoid the expansive data pair preparation processand effectively unleashes the generative potential inherentin the pre-trained diffusion model for image enhancementtasks. After learning from real-world data, our frameworkcan determine a nearly straight path toward the target re-sult. To further refine the prediction accuracy, we devise amean value sampling inspired by Lagranges Mean ValueTheorem to estimate the path with higher precision from amidpoint along the transport path.We mainly evaluate our method on two representativeimage enhancement tasks covering: 1) blind face restora-tion (BFR) and 2) blind image super-resolution (BSR). Weshow that our FlowIE can play as an effective enhancer fordegraded images, effectively catering to a broad spectrumof tasks. Our model attains 19.81 FID and 0.69 IDS on syn-thetic CelebA-Test , establishing new state-of-the-art onthese two benchmarks. On real-world LFW-Test andWIDER-Test datasets , our model achieves 38.66 and32.41 FID, respectively, exhibiting high restoration qualityin the real-world condition. After tuning on ImageNet ,we obtain 0.5953 MANIQA on RealSRSet and 0.6087on our Collect-100, highlighting our effectiveness in gen-eral image restoration. Except for higher metrics comparedwith other diffusion-based methods, we showcase an almost10 times faster inference speed thanks to rectified flow. Toexplore the potential of FlowIE on further tasks, we extendthe application of FlowIE to face color enhancement andinpainting with only 5K steps of fine-tuning. FlowIE con-sistently delivers visually appealing and plausible enhance-ments, underscoring its robust generalization capability.",
  "Predictive Methods. Image enhancement consists of var-ious manipulations and refinements, including denoising,": "super-resolution (SR), inpainting, etc.Some works uti-lize predictive models to address these tasks.Notably,convolution-based methods adopt explicit ap-proaches for SR task by estimating the blur kernels andrestoring HQ images with the predicted kernels. On theother hand, With the advent of vision transformers ,some methods propose frameworks incorporatingattention-based architectures, yielding high-quality resultson SR, denoising, and deraining tasks. Although predictiveapproaches pave the way for various enhancement tasks,they still struggle to handle complex real-world conditionsdue to their simple degradation settings during training.GAN-based Methods. In addition to predictive methods,another line of work explores employing generative modelslike GAN to provide embedded image priors. GAN-based methods learn how to process im-ages in the latent space, showcasing notable achievementsin tasks such as BSR. Moreover, works like leverage GAN priors for the BFR task and yield satisfy-ing outcomes. However, GAN-based methods exhibit somedrawbacks like the potential for unstable results and thenecessity for meticulous hyper-parameter tuning. Further-more, their architectures are often tailored to specific tasks,limiting their adaptability across diverse applications.Diffusion-based Methods. Diffusion models arewell-known for their powerful image synthesis capabil-ity and robust training procedure. To harness the imagepriors of the pre-trained diffusion model, methods suchas propose training-free approaches to enhanceimage quality in a zero-shot manner, showcasing the adapt-ability of diffusion models across various tasks. In a paral-lel line of research, supervised approaches like pave the way to fine-tune the diffusion model, improvingits generative potential. Despite achieving visually appeal-ing results, diffusion-based methods suffer from long-timesampling due to repeated model inference. To mitigate thetime-consuming problem and fully exploit the generativepriors within the pre-trained diffusion model, we devise anovel flow-based framework designed for diverse image en-hancement tasks. Our framework utilizes the rich knowl-edge from the pre-trained diffusion model and acceleratesthe inference via a flow-based approach, which straightensthe transport trajectories from an elementary distribution toreal-world data and thereby realizes efficient inference.",
  ". Method": "In this section, we present FlowIE, a simple flow-basedframework that fully exploits the generative diffusion priorfor efficient image enhancement. We will start by providinga brief background on rectified flow and then delve into thekey designs of FlowIE. This includes the construction ofa flow-based enhancement model, the design of appropri-ate conditions as guidance, quality improvements through",
  ". Preliminaries: Rectified Flow": "We commence by briefly introducing rectified flow .Rectified flow is a series of methods for solving the trans-port mapping problem: given observations of two distri-butions X0 0, X1 1 on Rd, find a transport mapT : Rd Rd, such that T(X0) 1 when X0 0. Dif-fusion models represent transport mapping problems as acontinuous time process governed by stochastic differentialequations (SDEs) and leverage a neural network to simulatethe drift force of the processes. The learned SDEs can betransformed into marginal-preserving probability flow or-dinary differential equations (ODEs) to facilitatefaster inference. However, diffusion models still suffer fromlong-time sampling due to repeated network inference tosolve the ODEs/ SDEs, compared to one-step models likeGANs. To address this problem, rectified flow introducesan ODE model that transfers 0 to 1 via a straight linepath, theoretically the shortest route between two points:",
  "dXt = v(Xt, t)dt,(1)": "where v represents the velocity guiding the flow to followthe direction of (X1 X0) and t denotes the timeof the process. To estimate v, rectified flow solves a simpleleast squares regression problem that fits v to (X1 X0).In practice, rectified flow leverages a network v topredict the velocity (path direction), and draws data pairsX = {(X0, X1)|X1 = ODE(X0)}, where ODE denotes atrained diffusion model, to minimize the loss function L:",
  ". Flow-based Image Enhancement": "The pre-trained diffusion model encompasses rich informa-tion about real-world data distribution and detailed imagesynthesis capacity.Our goal is to exploit the generativeprior of a pre-trained diffusion model for image enhance-ment and mitigate the extensive computational cost of dif-fusion sampling. Our core idea involves adopting a rectifiedflow framework with proper guidance to tune the denois-ing U-Net in the text-to-image pre-trained diffusionmodel into an effective path predictor v. This predictor en-ables the establishment of straight pathways from a simple",
  "injection": ". The overall framework of FlowIE. FlowIE is a flow-based framework for image enhancement tasks. During training (left), weoptimize the rectified flow v to bridge straight line paths v from an elementary distribution to clean images with proper guidance. We alsodeveloped the mean value sampling to improve the path estimation. During inference (right), we utilize the conditions from LQ to predicta linear direction toward the clean images on the midpoint of the transport curve, yielding high-quality and visually appealing results. elementary distribution to clean images, thereby facilitatingthe efficient utilization and acceleration of learned diffusionpriors for image enhancement. In pursuit of this goal, wefirst define the image degradation models as y = Dh(x),where x and y are HQ and LQ images and h H denotesa specific enhancement task. Enhancing images from real-world degradation poses a significant challenge due to theinherent complexity of Dh that is hard to formulate. To fur-nish precise and proper guidance for rectified flow withinintricate scenarios, we employ a pre-trained initial-stagemodel for coarse restoration. is dedicated to blur re-duction and contributes to the construction of the conditionC, which is pivotal for narrowing the direction spectrum andfacilitating path prediction. Compared to the denoising U-Net of the diffusion model, holds much fewer parametersand exerts minimal impact on inference speed. In contrast to the image synthesis that rectified flow istypically tailored for, image enhancement tasks have a rel-atively deterministic target (HQ). Therefore, the one-to-onetransport mapping inherent in rectified flow cannot directlyapply to our work. Instead, we naturally consider a novelmany-to-one mapping that every point in an elementary(Gaussian) distribution orients to a fixed HQ image in thereal world. This method offers two advantages comparedto : (1) it evades expensive data preparing that entailsdrawing massive data pairs by performing diffusion sam-pling repeatedly, and intuitively aligns with enhancement tasks with ground truth HQ data; (2) it stabilizes trainingand inference with theoretically infinite data pairs for learn-ing and fully leverages the condition to control the rectifiedflow process. Motivated by these properties, we aim to de-sign an effective control plan to centralize the flow direc-tion, using the coarse result as the raw material. For thiscontrol mechanism, we employ a ControlNet branch, whichconsists of a condition adapter and an injection module, tointroduce spatial guidance for the path predictor. Given aclean image z1 from real-world dataset S and a noise z0sampled from the standard Gaussian distribution, we syn-thesize the LQ image zLQ using the degradation model andrecover the coarse result with . To construct the condi-tion C with the information of time t, we concatenate thecoarse result with the noisy image zt produced by the linearinterpolation in Equation (2). Then we employ the condi-tion adapter implemented as a two-layer MLP to refine theimage features and apply a zero convolution layer F withboth weights and bias initialized to zeros to align the chan-nel dimension. To sum up, the condition C is computed as:",
  ". Improve Quality via Mean Value Sampling": "The optimized rectified flow straightens the transport tra-jectories to nearly linear paths. Utilizing the forward Eulermethod, rectified flow can produce plausible results witha small number of Euler steps. However, a simple itera-tive forward method inevitably causes error accumulation,leading to global blur and unsatisfactory details. To tacklethis problem, we devise the Mean Value Sampling based onLagranges Mean Value Theorem to improve the velocityestimation accuracy, yielding more visual-appealing resultswith better quality and details.Specifically, LagrangesMean Value Theorem states that for any two points on acurve, there exists a point on this curve such that the deriva-tive of the curve at this point is equal to the scope of thestraight line connecting these points. Since rectified flowacts as a path or derivative predictor on a differentiabletransport curve, we naturally leverage it to find a midpointon the curve that the velocity direction vmid of this point isparallel to the straight line bridging z0 and z1.For midpoint searching, we compute the path directionof a point set P = {z0, zt, ..., z1t}, covering uni-form discrete timesteps along the curve with the step lengtht = 1 N . We observe that there exists a midpoint zkt in Pthat predicts the most accurate direction and yields the bestresult. We select the desirable k {0, 1, ..., N 1} with afew test data points for different tasks and we find that zktalways produces reliable results in a specific task.",
  ". Implementation": "We consider four image enhancement tasks with differentdegradation models Dh in this work. In detail, the degrada-tion model for BFR and BSR can be generally approximatedas y = [(k x) r +n]JPEG, which consists of blur, noise,resize and JPEG compression. Since images usually sufferfrom more severe harassment in the real-world scene, weapply a high-order degradation model, repeating the aboveprocess multiple times. For the inpainting task, we need torecover the missing pixels in images. The correspondingdegradation model is the dot-multiplication with a binarymask: y = x m. For color enhancement, the degradedimage experiences color shifts or only retains the grayscalechannel. In our framework, we mainly manipulate images in a latent space constructed by a trained VQGAN, con-sisting of an encoder E and a decoder D and achieving theconversion between the pixel space and the latent space. Wealso create a trainable copy of the encoding blocks and themiddle block in v as the injection module to handle thecondition C and infuse it to v. For optimal results, we em-pirically capture the midpoint with N = 5 and k = 3, thusthe inference requires only k + 1 = 4 steps.",
  ". Experiment Setups": "Datasets. For the face-related tasks, including blind facerestoration, face color enhancement and face inpainting, wetrain our model on Flickr-Faces-HQ (FFHQ) , whichencompasses a corpus of 70,000 high-resolution (1024 pix-els) images. In preparation for training, we resize these im-ages to a resolution of 512 512. To evaluate the perfor-mance of our model both quantitatively and qualitatively,we employ the synthetic CelebA-Test dataset , whichcomprises 3,000 pairs of LQ and HQ pairs. For compar-isons on real-world datasets, we leverage LFW-Test ,CelebChild-Test and WIDER-Test , which con-tain face images afflicted with varying degrees of imagedegradations. For the blind image super-resolution task, wefinetune our model on ImageNet and evaluate it on thewidely-used RealSRSet . Since the size of RealSRSetis relatively small, we construct another test set, namelycollect-100, with 100 real-world images following the classdistribution in RealSRSet to conduct a broader evaluation.Training Details We apply the image restoration base-line as our initial stage model. For BFR and BSR, wetune the initial stage model for 90K steps with a batch sizeof 64 on the corresponding datasets. To leverage the diffu-sion prior, we employ the pre-trained text-to-image model(namely Stable Diffusion) to initialize the path predic-tor v and fix the VQGAN. To optimize our rectified flow,we unfreeze the linear layers of the cross-attention blocksin v via LoRA during training. The training for theseparameters takes 80K steps with a batch size of 32. For alltasks, we use the AdamW optimizer and set the learningrate as 1e-4. All tasks share the same model architecture.Metrics.To evaluate FlowIE on the blind face restora-tion with ground truth, we utilize traditional metrics in-cluding PSNR, SSIM and LPIPS. However these metricsare not enough to reflect human preference since they of-ten penalize high-frequency details, e.g., hair texture. Wealso compute the identity similarity, denoted as IDS, witha face perception network and adopt the widely-usednon-reference metric FID to measure image quality, whichis also employed for evaluation on wild datasets. On blindimage super-resolution task, we leverage the non-referenceimage quality assessment metric, namely MANIQA , to . Quantitative comparisons for BFR on the synthetic and real-world datasets. Red and blue indicate the best and the secondbest performance, respectively. We categorize the methods into conventional (up), diffusion-based (middle) and flow-based (bottom). OurFlowIE shows very competitive results compared with existing methods. We obtain remarkable image quality and identity consistencywith the leading FID and IDS scores. Our framework also exhibits much faster inference than the diffusion-based method.",
  ". Main Results": "Blind Face Restoration.We evaluate FlowIE on bothsynthetic CelebA-Test and in-the-wild LFW-Test ,CelebChild-Test and WIDER-Test .Our com-parative analysis involves recent state-of-the-art meth-ods, including GPEN , GCFSR , GFPGAN ,VQFR , RestoreFormer , DMDNet , Code-Former and DiffBIR .We start with the quan-titative comparison on CelebA-Test, which provides LQ-HQ pairs for evaluation, as shown in . We showthat FlowIE achieves FID 19.81 and IDS 0.69, outperform-ing previous methods. This underscores FlowIEs effective-ness in enhancing image quality and preserving face iden-tity. We also achieve comparable scores on PSNR, SSIMand LPIPS and exhibit a higher upper bound on all met-rics than DiffBIR. Notably, the FPS of FlowIE is close tothe scale of one-step methods, approximately 10 times of DiffBIR. We further showcase the qualitative results in Fig-ure 4. FlowIE successfully recovers detailed informationlike the hair and skin textures while faithfully maintain-ing the identity, encompassing facial features and expres-sions, in challenging cases. In assessing FlowIE on real-world data, we conduct experiments on three wild datasets,as presented in . FlowIE delivers high-quality out-comes reflected by the outstanding FID on LFW-Test andWIDER-Test. We also obtain competitive FID with state-of-the-art methods on CelebChild-Test. The qualitative re-sults on wild datasets, depicted in , illustrate thatFlowIE consistently produces visually realistic outcomes. Blind Image Super-Resolution. We evaluate our FlowIEon RealSRSet and our established Collect-100 dataset.We compare FlowIE with cutting-edge methods, includ-ing GAN-based Real-ESRGAN+ , BSRGAN ,SwinIR-GAN ,FeMaSR and diffusion-basedDDNM , GDP and DiffBIR .In ,the quantitative assessment highlights FlowIEs superior-ity over other methods, demonstrating high image qualitywith MANIQA scores of 0.5953 and 0.6087 on the two",
  "LQ DDNM GDP Real-ESRGAN+ BSRGAN SwinIR-GAN DiffBIR FlowIE": ". Qualitative comparisons on the real-world images. FlowIE successfully enhances the LQ images by upsampling, denoisingand deblurring simultaneously and provides rich details from the generative knowledge, yielding high-quality and satisfying outcomes. .Quantitative comparisons for BSR on real-worlddatasets. Our flow-based framework achieves high-quality en-hancement and outperforms existing methods in MANIQA witha much faster speed compared to diffusion-based methods.",
  "FlowFlowIE (Ours)0.59530.60872.853": "datasets, respectively. Notably, though DiffBIR also attainscommendable quality, its low throughput due to diffusionsampling contrasts with FlowIEs comparable speed to one-step GAN-based methods. demonstrates FlowIEsproficiency in enhancing intricate detailed contents, such asthe patterns on the butterflys wings in Row 1 and the tex-ture of the cats fur in Row 2. These vivid improvements areattributed to the generative priors from the pre-trained diffu-sion model. The combination of efficient inference and vi-",
  ". Analysis": "Effective Diffusion Exploitation via Rectified Flow. Indiffusion models, a denoising step can be viewed as a walkalong the gradient direction of data density, hinting at thepotential for distilling the diffusion model to achieve fasterinference. Therefore, we compare two approaches: directdistillation and rectified flow. For direct distillation (w/oflow), we set the student identical to v and fix t = 0 duringtraining. As shown in and , FID scores andMANIQA are adversely affected, and the visual outcomesexhibit unsatisfactory blur and inadequate details. We sum-marize that exploiting diffusion model via direct distillationis a tough learning problem for the one-step student modeland rectified flow mitigates it with refined trajectories.Choice of Inference Paths. The straightened path via rec-tified flow is not perfectly linear.Empirically, we havetwo options for inference paths: (1) use the forward Eu-ler method which walks along the trajectory with fixed steplength (w/o mid sample), and (2) follow Lagranges MeanValue Theorem to identify a pivotal midpoint on the path.We generate the results for BFR and BSR through both . Ablation studies. We perform ablations on BFR andBSR to verify the effectiveness of the components in FlowIE andthe impact of the inference path choice. We find that rectified flowand the initial stage model are beneficial and that the path guidedby mean value sampling yields the best performance.",
  ". Qualitative comparisons of the ablations. We find thevariant frameworks fall short in terms of clarity and details": "paths. Results from Path 1, as depicted in and Fig-ure 7, show plausible outcomes with reduced noise. How-ever, they fall behind in realness and details compared toPath 2. We conclude that the Euler method struggles toproduce high-quality images in very few steps (e.g., 5),while our mean value sample obtains visually pleasant re-sults with a more efficient inference process (<5 steps).Impact of the Initial Stage Model.Our initial stagemodel performs general image deblurring on LQ im-ages, enhancing guidance quality and thereby strengthen-ing FlowIEs path estimation. To gauge this impact, wetrain our rectified flow without (w/o init) and conduct anevaluation on test sets. We observe that the absence of results in low-quality guidance, leading to unsatisfactory re-sults. This is evident in the worse FID and MANIQA scoresin and blurred object edges in . These re-sults demonstrate the effect of in improving the qualityof conditions and achieving better overall performance.",
  ". Extensions": "To further demonstrate the adaptability of our framework,we generalize FlowIE to extended tasks, including facecolor enhancement and face inpainting. Achieving this ex-tension requires a minimal fine-tuning effort of 5K steps forthe rectified flow dedicated to each task.Face Color Enhancement. To achieve color enhancement,we fix the initial stage model and fine-tune our rectifiedflow using color augmentations (random color jitter andgrayscale conversion) in .We compare our method",
  ". Face inpainting via FlowIE. We complete the missingpixels with realistic and coherent content for challenging cases": "with GFPGAN and CodeFormer on real-worldCelebChild-Test dataset. The results in show-case that FlowIE produces visually appealing and highlyconsistent face images with vibrant and realistic colors.Face Inpainting. We employ the script from to drawirregular polyline masks on face images as our inputs andfine-tune the rectified flow. During inference, we resize themask to the latent codes shape and use it to maintain thevisible area on the inputs. As shown in , FlowIEsuccessfully reconstructs the challenging cases and seam-lessly completes them with coherent contents.",
  ". Conclusion": "In this paper, we introduced FlowIE, a novel framework thatharnesses the conditioned rectified flow to exploit the po-tent generative priors within the pre-trained diffusion modeland accelerate the inference by straightening the probabilitytransport trajectories. To further improve the path estima-tion accuracy and reduce inference steps, we have devisedthe mean value sampling to predict a precise direction atthe curve midpoint. Extensive experiments demonstrate ourframeworks competitive performance and remarkable gen-eralization across diverse image enhancement challenges.We envision our work will inspire future research on flow-based image enhancement and efficient diffusion sampling.Acknowledgement.This work was supported in partby the National Natural Science Foundation of China un-der Grant 62125603, Grant 62321005, and Grant 62336004.",
  "Chao Dong, Chen Change Loy, Kaiming He, and XiaoouTang. Image super-resolution using deep convolutional net-works. TPAMI, 38(2):295307, 2015. 3": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale. In ICLR, 2020. 3 Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, WeidongYang, Tianyue Luo, Bo Zhang, and Bo Dai. Generative dif-fusion prior for unified image restoration and enhancement.In CVPR, pages 99359946, 2023. 2, 3, 6, 7",
  "Wenhan Yang, Robby T Tan, Jiashi Feng, Jiaying Liu, Zong-ming Guo, and Shuicheng Yan. Deep joint rain detection andremoval from a single image. In CVPR, pages 13571366,2017. 3": "Wenhan Yang, Robby T Tan, Jiashi Feng, Zongming Guo,Shuicheng Yan, and Jiaying Liu. Joint rain detection andremoval from a single image with contextualized deep net-works. TPAMI, 42(6):13771393, 2019. 3 Yuan Yuan, Siyuan Liu, Jiawei Zhang, Yongbing Zhang,Chao Dong, and Liang Lin.Unsupervised image super-resolution using cycle-in-cycle generative adversarial net-works. In CVPRW, pages 701710, 2018. 2, 3 Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu-nawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang.Restormer: Efficient transformer for high-resolution imagerestoration. In CVPR, pages 57285739, 2022. 1",
  "ShangchenZhou,KelvinChan,ChongyiLi,andChen Change Loy.Towards robust blind face restora-tion with codebook lookup transformer.NeurIPS, 35:3059930611, 2022. 2, 3, 5, 6, 8": "In the supplementary material, we provide a deeper explo-ration of insights and findings. In Section A, we presentmore implementation details regarding the training andevaluation of FlowIE. Section B delves into further discus-sions through a combination of quantitative analyses andqualitative experiments. In Section C, we show additionalvisualization results for Blind Face Restoration (BFR) andBlind Image Super-Resolution (BSR). Furthermore, Sec-tion D extends our investigations to encompass tasks suchas single image deraining and dehazing. The source codefor FlowIE is also provided in the zip file.",
  "A. Detailed Implementations": "To initialize our path estimator v, we employ the text-to-image pretrained Stable Diffusion (SD 2.0-Base) ,which offers ample generative priors suitable for variousenhancement tasks. The input image x R3512512 isencoded into the latent code z R46464 by the trainedVQGAN. During the training of all tasks, we resize the in-put images to 512 512. For the images smaller than thissize, we upsample them with the short side enlarged to 512and crop them with a fixed-size bounding box. We trainour FlowIE with 8 NVIDIA RTX 3090 GPUs. To maintainthe pre-trained capability of the diffusion model, we utilizethe LoRA approach to unfreeze the linear layers of thecross-attention blocks in v. We find that the small trainableparameters with a LoRA rank of 4 can significantly unleashthe generative priors within the diffusion model and allowadaptation to various tasks. Another benefit of the partiallyunlocked models is preventing overfitting of the large dif-fusion model. To measure the throughput of FlowIE andother methods, we conduct evaluation experiments on thesame dataset and using a single 3090 GPU.",
  "B. More Discussions": "Many-to-one mapping and result diversity. Comparedwith text-to-image generation, image enhancement taskslike BFR have more deterministic targets. Therefore, weemploy the many-to-one strategy for FlowIE during train-ing to learn a direct mapping from noise to real data. How-ever, its crucial to clarify that FlowIE, being a probabilis-tic model like diffusion models, inherently yields diverseoutcomes, especially for the inpainting task. As illustratedin Figure A, given the masked input (Col.1) and differentinitial noise z0, FlowIE generates various facial features(Col.2-5), encompassing variations in the shape of the nose,ears, and texture of the hair. Unlike rigid many-to-onemapping often employed in GAN-based methods during in-ference, FlowIE embraces the generative capacity of diffu-sion models and enjoys the diversity of plausible results.",
  "3 (midpoint) 1(Euler) 1(Mean value)": "Figure B. The visualization of the inference process. FlowIEestablishes straight-line paths from random noise to clean images.Through mean value sampling, we achieve clearer and more de-tailed results in fewer steps compared to the Euler method. Visualization of different paths We showcase the visual-ization of each step in our inference process. Along thestraight-line path, FlowIE adeptly generates high-quality(HQ) images from noise in less than 5 steps. As depictedin Figure B, the mean value sampling consistently yieldsclearer and more detailed results in fewer steps comparedto the Euler method, highlighting its efficacy in enhancingthe quality of the generated images.About starting from (zLQ). Since FlowIE predicts thepath from random noise, switching the starting point to thecoarse result (zLQ) is indeed reasonable. Tuning andevaluation on the BFR task (shown in Table A) indicate aslightly worse FID compared with FlowIE. We attribute thisresult to the adjustments reliance on initial results over pre-trained diffusion priors.",
  "Real Input GPEN RestoreFormer GFPGAN DMDNet CodeFormer DiffBIR FlowIE": "Figure D. Qualitative comparisons on real-world faces. Our approach demonstrates credible enhancements on real-world faces, deliver-ing high-fidelity and visually satisfying results. Compared to other methods, FlowIE showcases robustness in front of challenging cases. About artifacts in the first step. We acknowledge that ex-treme artifacts in the first step may result in failure cases.In Figure E, the input undergoes challenging degradation(16 downsampling). Compared to GAN-based methodslike BSRGAN which introduce many artifacts and blur,FlowIE generates a cleaner image. However, the final resultmay still exhibit unrealistic eyes due to initial step artifacts. About larger resolution. FlowIE demonstrates excellentscalability to process larger images. We can replace theoriginal diffusion model (SD 2.0-base) with an enlargedversion (SDXL) which generates 1024 1024 images bydefault and tune the FlowIE framework following the pro-",
  "Figure F. Qualitative results in larger resolution. The proposedFlowIE consistently delivers visually captivating results at higherresolutions": "Comparisons with diffusion models.Our proposedFlowIE mainly capitalizes on the powerful generation ca-pability within the pre-trained diffusion model, which hasdemonstrated its versatility in various visual tasks.Forexample, DDVM explicitly underscores the effective-ness of pre-trained priors in diffusion models for monoculardepth estimation and SDEidt focuses on image editingtasks like stroke-based editing. Additionally, success-fully achieves rapid image sampling by employing multi-modal denoising distributions and conditional GANs. Com-pared with these works, our FlowIE primarily harnesses thegenerative prior in diffusion models and employs a condi-tioned flow-based strategy to accelerate the sampling. Input/ Groundtruth PReNet RCDNet FlowIE (b) Figure G. Single image Deraining via FlowIE. Our frameworkadeptly identifies the rainy layers and proficiently restores the orig-inal images without complex task-specific designs.",
  "Figure H. Single image dehazing via FlowIE. Our framework ef-fectively eliminates haze, enhancing the overall clarity and qualityof the images": "satisfying results in these challenging tasks, showcasing itsefficacy across diverse image enhancement scenarios.Blind Face Restoration.We conduct qualitative com-parisons on both synthetic CelebA-Test and in-the-wild LFW-Test , CelebChild-Test and WIDER-Test .Our comparisons involve recent state-of-the-art methods, including GPEN , GCFSR , GFP-GAN , VQFR , RestoreFormer , DMDNet ,CodeFormer and DiffBIR .Visual results pre-sented in Figure C and Figure D demonstrate that ourFlowIE consistently produces visually pleasing outcomeson both synthetic and real-world datasets, affirming its ef-fectiveness and robust performance in diverse scenarios.Blind Image Super-Resolution. For BSR, we also presentadditional results on RealSRSet and our establishedCollect-100 dataset.We compare FlowIE with cutting-edge methods, including GAN-based Real-ESRGAN+ ,BSRGAN , SwinIR-GAN , FeMaSR anddiffusion-based DDNM , GDP and DiffBIR .Figure I vividly illustrates the efficacy of FlowIE in generat-ing visually appealing images with a commendable balancebetween realism and clarity.",
  "D. More Extended Tasks": "To showcase the versatility of our framework, we extendFlowIE to more tasks, specifically single image derainingand dehazing. The adaptation for these tasks involves a fine-tuning process with 15K steps on the respective datasets.Notably, we only use the single MSE loss for all tasks.Deraining. We utilize RainTrainH , RainTrainL and Rain12600 for training and evaluate our frame-work on Rain-100L dataset . We compare our resultswith PReNet and RCDNet .As shown in Fig-ure G, FlowIE effectively separates the rainy layers and re-constructs the original clean images.Dehazing.We employ the indoor part of the RESIDEdataset for training and evaluate our framework on itstest split. We compare the results with FFA-Net and"
}