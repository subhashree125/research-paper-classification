{
  "Challenge and Workshop OrganizersYuekun DaiDafeng ZhangXiaoming LiZongsheng YueChongyi LiShangchen ZhouRuicheng FengPeiqing YangZhezhu JinGuanqun LiuChen Change Loy": "Challenge ParticipantsLize ZhangShuai LiuChaoyu FengLuyang WangShuan ChenGuangqi ShaoXiaotao WangLei LeiQirui YangQihua ChengZhiqiang XuYihao LiuHuanjing YueJingyu YangFlorin-Alexandru VasluianuZongwei WuGeorge CiubotariuRadu TimofteZhao ZhangSuiyi ZhaoBo WangZhichao ZuoYanyan WeiKuppa Sai Sri TejaJayakar Reddy AGirish RongaliKaushik MitraZhihao MaYongxu LiuWanying ZhangWei ShangYuhong HeLong PengZhongxin YuShaofei LuoJian WangYuqi MiaoBaiang LiGang WeiRakshank VermaRitik MaheshwariRahul TekchandaniPraful HambardeSatya Narayan TaziSantosh Kumar VipparthiSubrahmanyam MuralaHaopeng ZhangYingli HouMingde YaoLevin M SAniruth SundararajanHari Kumar A",
  "Abstract": "The increasing demand for computational photographyand imaging on mobile platforms has led to the widespreaddevelopment and integration of advanced image sensorswith novel algorithms in camera systems.However, thescarcity of high-quality data for research and the rare op-portunity for in-depth exchange of views from industry andacademia constrain the development of mobile intelligentphotography and imaging (MIPI). Building on the achieve-ments of the previous MIPI Workshops held at ECCV 2022and CVPR 2023, we introduce our third MIPI challenge in-cluding three tracks focusing on novel image sensors andimaging algorithms. In this paper, we summarize and re-view the Nighttime Flare Removal track on MIPI 2024. Intotal, 170 participants were successfully registered, and 14teams submitted results in the final testing phase. The de-veloped solutions in this challenge achieved state-of-the-artperformance on Nighttime Flare Removal. More details ofthis challenge and the link to the dataset can be found at",
  "Lens flare, an optical phenomenon, arises when intense lightscatters or reflects within a lens system, manifesting as adistinct radial-shaped bright area and light spots in captured": "photographs. In mobile platforms such as monitor lenses,smartphone cameras, UAVs, and autonomous driving cam-eras, daily wear and tear, fingerprints, and dust can functionas a grating, exacerbating lens flare and making it particu-larly noticeable at night. Thus, flare removal algorithms arehighly desired. Flares can be categorized into three main types: scatter-ing flares, reflective flares, and lens orbs. In this competi-tion, we mainly focus on removing the scattering flares, asthey are the most prevalent type of nighttime image degra-dation. Early attempts at scattering flare removal were madeby Wu et al. , who proposed a dataset with physically-based synthetic flares and flare photos taken in a darkroom.However, these flares have obvious domain gap with real-captured nighttime flares.To address this issue, Dai etal. propose a new dataset Flare7K++ which is specif-ically designed for nighttime scenes. Additionally, variousother efforts have been pursued, including flare removal inmulti-light scenarios , in raw image formats , andsmartphone reflective flare . However, due to variationsin lens structures and the diversity of lens protectors, exist-ing lens flare datasets struggle to cover all types of lens flarecomprehensively. This occasionally results in out of distri-bution occurrences of lens flare in real-world captures forspecific type of lenses. In response to the growing demandamong smartphone and lens manufacturers, this competi-tion focuses on developing lens-specific lens flare removal",
  "arXiv:2404.19534v2 [cs.CV] 27 May 2024": "methods. In addition to the Flare7K++ dataset, we pro-vide 600 aligned flare-corrupted/flare-removed image pairsspecifically for certain smartphones rear camera. Further-more, in order to mimic the commonly-used high resolu-tions in the industry, all training set and test set imagesresolutions are set to 2K.We hold this challenge in conjunction with the thirdMIPI Challenge which will be held on CVPR 2024. Similarto the previous MIPI challenge , we are seek-ing an efficient and high-performance image restoration al-gorithm to be used for recovering flare-corrupted images.MIPI 2024 consists of three competition tracks: Few-shot RAW Image Denoising is geared towardstraining neural networks for raw image denoising in sce-narios where paired data is limited.",
  ". Datasets": "Our competition provides a paired flare-corrupted/flare-freedataset that contains 600 aligned training images in 2Kresolution. Participants can train a pixel-to-pixel networkwith this dataset for flare removal. The validation set andtesting set consist of 50 and 50 pairs of images, respec-tively. The input images from the validation and testingset are provided and the ground truth images are not avail-able to participants. In addition, participants can also useFlare7k++ as an additional training dataset and its re-leased checkpoint.The Flare7k++ provides 5,000 syn-thetic flare images in 14401440, 962 real flare images in7561008, and 23,949 background images. Flare imagescan be added to the flare-free background images to synthe-size paired data for training.",
  ". Challenge Results": "Among 170 registered participants, 14 teams successfullysubmitted their results, code, and factsheets in the final testphase. Out of these, 12 teams have contributed their so-lutions to this report. reports the final test resultsand rankings of the teams. Only two teams train their mod-els with extra data of real-captured nighttime backgroundimages. The methods evaluated in are briefly de-scribed in and the team members are listed in Ap-pendix. Finally, the MiAlgo AI team is the first place win-ner of this challenge, while BigGuy team win the secondplace and SFNet-FR team is the third place, respectively.",
  ". Methods": "MiAlgo AIThis team proposes a Progressive PerceptionDiffusion Network (PPDN). By implementing a two-stagenetwork architecture, it generates visually high-quality re-sults in a progressive strategy. Specifically, in the first stage,there is a diffusion module that aims to remove the flares ofthe input to the greatest extent feasible. Inspired by ,they use the IR-SDE as the base diffusion module, whichcan avoid generating smooth results during deflaring. In thesecond stage, they utilize the AOT Block as the fun-damental enhancement module to amplify the details of theflat domain in the output of the first stage and recover thecontent of the flare texture. Significantly, the output of thediffusion module serves as crucial conditional information.The output of the inpainting module has high-quality visu-alization effects. The entire pipeline is shown in .When generating a training dataset, it contains a syn-thetic dataset and a real dataset. Borrowing from , theyadditionally collected several high-quality nighttime pho-tos at two resolutions and added compound flares as a syn-thetic dataset. A paired flare dataset containing 600 alignedtraining images provided by the event group is the realdataset. Upon examining the validation images, the authorsnoticed a variation in brightness between the input and out- . Results of MIPI 2024 challenge on nighttime flare removal. Runtime for per image is tested and averaged across the validationdatasets, and the image size is 1440 1920. Params denotes the total number of learnable parameters.",
  "MetricTeam NameLPIPSPSNRSSIMParams (M)Runtime (s)PlatformExtra dataEnsemble": "MiAlgo AI0.1435(1)22.15(1)0.7075(2)141.618.0NVIDIA Tesla A100Yes-BigGuy0.1502(2)21.50(7)0.6996(7)26.1330.0NVIDIA RTX 3090-self-ensembleSFNet-FR0.1518(3)21.74(3)0.7188(1)383.420.017NVIDIA RTX 3090Ti--LVGroup HFUT0.1620(4)21.71(5)0.7041(4)/0.055NVIDIA RTX 4090--NativeCV0.1688(5)21.39(8)0.6929(8)///--CILAB-IITMadras0.1697(6)21.70(6)0.7042(3)61.400.70NVIDIA RTX 4090-model-ensembleXdh-Flare0.1703(7)21.99(2)0.7005(5)24.471.78NVIDIA RTX 4090Yes-Fromhit0.1713(8)21.24(9)0.6850(10)/1.256NVIDIA RTX A6000--UformerPlus0.1732(9)21.73(4)0.6997(6)38.790.32NVIDIA RTX 3090--GoodGame0.1813(10)20.85(10)0.6881(9)19.470.73NVIDIA RTX 3090--IIT-RPR0.1926(11)20.66(11)0.6775(11)20.471.72NVIDIA RTX 2080TiYes-LSCM-HK0.1926(12)22.66(12)0.6775(12)20.47/NVIDIA RTX 3090--Hp zhangGeek0.2332(13)19.74(13)0.6538(13)2.110.13NVIDIA RTX 4090-self-ensembleLehaan0.6749(14)16.27(14)0.4655(14)//NVIDIA RTX 4050-- put images. Furthermore, to simulate more realistic flare-corrupted images when encountering heavier input fog, theyaugmented the base image with additional light and localhazeWhen training, they first train the diffusion module usingsynthetic data generated online and real dataset for about400,000 iterations. Then they fix the weight of the diffu-sion module and train the enhancement module for about300,000 iterations with a batch size of 4. The initial learn-ing rate is lr = 1e 4, and cosine annealing is used toreduce the learning rate. To optimize the models capacityfor extracting global information, the team abstains fromemploying a random cropping strategy on the input image.The training process is executed using the power of 2 TeslaA100 GPUs for approximately 3 days. BigGuyThis team designs a one-stage Restormer-likeStructure , making full use of the hierarchical multi-scale information from low-level features to high-level fea-tures. To ease the training procedure and facilitate the in-formation flow, an efficient Transformer for image restora-tion is utilized to model global connectivity and is still ap-plicable to large images. Since flares can take up a largeportion of the image, and possibly the entire image, duringthe removal of nighttime flares, it is critical to have a largereceptive area. However, conventional window-based trans-former methods limit the receptive field within the window,thus limiting their ability to capture global features. Also,to allow the network to focus more on the flare region, thisteam used a difference algorithm to obtain a mask betweenthe input image and the ground truth to compute the lossfunction.The training is using the Adam optimizer , with lr =1e 5 and default decay parameters. The optimized ob-jective is a mixture of terms, combining the L1 loss, VGG loss, mask loss, and LPIPS loss comparing the outputrestored image to the reference image. SFNet-FRThis team proposes SFNet, a solution based onmultiple-level frequency-band decomposition, performed inboth the RGB spatial domain and the image frequency do-main. offers a graphical representation of theSFNet model, with the encoder module (center) and thepaired decoder (right). The solution follows the UNet struc-ture , with multiple frequency-band skip connections,such as the high-frequency RGB domain (red), or Haar Dis-crete Wavelet Transform (DWT) (orange) skip connection,with another preserving low-frequency combined domainsfeatures (blue).The information preserved through the aforementionedskip connections is the output of the dual domain splittingperformed at the encoder level. In the RGB domain, thesplitting is performed through a module combining the Avg-Pooling and the MaxPooling operators, while the splitting inthe frequency domain is done through a Haar DWT opera-tor. The solution builds on previous work , with a con-siderable model complexity allocated to the modules pro-cessing high-frequency information, while the lower com-plexity features are refined through simpler modules.At the decoder level, the multi-domain high-frequencyinformation is fused in a Stereo Channel Attention module, while the low-frequency features are processed sepa-rately, then receiving the enhanced high-frequency informa-tion as compensation.The training is using the Adam optimizer , with lr =2e4 and default decay parameters. The inputs are croppedinto 320 320 with a batch size of 1. The optimized ob-jective is a mixture of terms, combining the L1 loss witha VGG loss, and a gradient loss comparing the Sobelgradient of the output restored image to the gradient of thereference image.This builds SFNet as a capable solution for the flare re-moval task which, trained on the Flare7K++ dataset and thechallenge data, can achieve a significant performance levelwith consistent results for all the evaluated metrics. As a",
  ". The network architecture of MiAlgo AI": "single-stage solution, solving the nighttime flare removal inan end-to-end fashion (without using any self-ensemble ormodel-ensemble structures), SFNet represents a good trade-off in terms of achieved performance for the characteristiccomputational cost. The solution is one of the fastest com-pared to the other competitors, being able to perform real-time flare removal on a consumer-grade GPU, the NvidiaRTX 3090Ti. LVGroup HFUTThis team proposes to divide the wholetraining dataset into different subsets based on different dis-tributions, then train the neural network model on eachsubset separately, and finally integrate the obtained resultsto realize flare removal. Specifically, this team refers toNAFNet and FCL-GAN to construct the model,and then divide the provided training data into two subsets(according to the resolution due to the difference in theirdistributions) and train the models separately to obtain twopre-trained models, demonstrates the detailed archi-tecture of this team.Training description. The proposed architecture of thisteam is based on PyTorch 2.2.1 and an NVIDIA 4090 with24G memory. They set 2500 epochs for training with batchsize 4, using AdamW with 1=0.9 and 2=0.999 for opti-mization. The initial learning rate was set to 0.0002, whichwas reduced by half every 50 epochs. For data augment,they first randomly crop the image to 768768 and thenperform a horizontal flip with probability 0.5. Besides, asmentioned above, two different models were trained sepa-rately to fit different distributions (different resolutions inthe experiment).Test description. Similarly to the training stage, the testimage with the original resolution is fed into the two modelsaccording to their distributions for inference to obtain theresults, and finally the obtained results are combined. CILAB-IITMadrasThis team proposes to ensemble 3Uformers using different metrics and methodologies. FlareRemoval Uformer GAN(FRUGAN) utilizes UFormeras a generator and a multiscale discriminator that utilizes both adversarial and feature-matching loss to remove flarefrom the given image. They have used three discriminatorssimilar to the network in pix2pixhd operating at differ-ent image scales as shown in .A combination of adversarial loss, multiscale discrimi-nator loss, L1, and perceptual losses is used to train FRU-GAN. To improve the results of FRUGAN, Uformer-1, andUformer-2 were trained on losses Lmse + LLP IP S andLLP IP S respectively. The complete ensemble model usesweights w1 = 0.60, w2 = 0.25, and w3 = 0.15 as depictedin .They have trained the proposed Uformer model by ran-domly cropping the images from the competition dataset to800*800 images and then resizing it to 512512 images.They further did data augmentations of random horizontaland vertical flips and random rotations of up to 5. Themodel was trained with Adam optimizer with 1 set to 0.9and 2 set to 0.999. This team found that the model startedover-fitting after 50 epochs. Both the discriminator and thegenerator were updated after every iteration. The FRUGANmodel was trained on NVIDIA A100 with 40GB VRAM.Similarly, the other Uformer models were trained by re-sizing the input images to 512*512. The model was trainedwith Adam optimizer with 1 set to 0.9 and 2 set to 0.999.The learning rate was set to 1e-4. They have trained for100 epochs. The Uformer models were trained on NVIDIARTX 4090 with two 24GB VRAM GPUs. Xdh-FlareThis team adopts Uformer architecture and makes several improvements in the dataset and lossfunction to remove nighttime flare. The authors observe thatthe data distribution in the target domain differs from thatin the source domain, primarily manifesting in the discrep-ancy between the proportion of light sources and flare tonestraining set images and the testing set. In response to thedata disparity between the target and source domains, theauthors adopt a strategy of augmenting the dataset in the tar-get domain to reduce domain gaps. Following the method ofsynthesizing data from Flare7k++ , they add flare imagesprovided by Flare7k++ to the flare-free background images Conv2D BatchNorm2D LeakyReLU(0.2)",
  ". The network architecture of LVGroup HFUT team": "provided by BracketFlare to synthesize paired data fortraining. They observe the flare tones of all images in thedataset around light sources, selecting flare images basedon a similar distribution of flare tones in the flare images,completing the augmentation of the dataset from the sourcedomain to the target domain.They use L1 loss, SSIM loss, and perceptual loss toremove flare regions. Given an image with flares Iinput,Uformer network outputs flare-free image Ig and flare im-age If. To better recover the flare-free image and flare im-age, the authors add If and Ig , then calculate the L1 losswith Iinput. The total L1 loss is represented as:",
  "losstotal = lossL1 + lossSSIM + lossper(4)": "where , , and are respectively set to 1, 0.01, and 1 intheir experiments.When training, the input images are cropped to 512 512. The authors also use gamma correction and inversegamma correction to the input images. The authors use theAdam optimizer with an initial learning rate of 0.0001 anda batch size of 4. The network is trained for 500 epochs onthe augmented dataset. FromhitThis team employs an efficient image restora-tion model, NAFNet, as the base model. Specifically, afour-scale CNN encoder and decoder are adopted, and eachscale contains two NAFBlocks. Between the encoder anddecoder, the authors use four NAFBlocks as a middle block.Then, the authors design a loss function for nighttime flareremoval. During training, the authors minimize the sumof two losses, L1 loss encourages the predicted flare-freeimage to be close to the ground truth both photometricallyand perceptually. Like , the perceptual loss is computedby feeding the predicted flare-free image and ground-truththrough a pre-trained VGG-19 network. This team doesnot process light sources.For training, the authors ran-domly cropped 512 512 patches from the training imagesas inputs. The mini-batch size is set to 4 and the whole net-work is trained for 1 105 iterations. The learning rate isinitialized as 1 104, and the authors use ADAM as theoptimizer with 1 = 0.9, and 2 = 0.99,",
  ". Ensemble Model with blended output": "UformerPlusThe team proposes an effective nighttimeflare removal pipeline. Firstly, they employed the strongimage restoration model Uformer as the base model,which has an encoder, a decoder, and skip connections. TheLocally-enhanced Window (LeWin) block is adopting thedesign in Uformer. And then to leverage the frequency char-acteristics of the image, the authors introduce the ResFFT-Block after the LeWin block, which is based on FastFourier Convolution (FFC), to extract global frequency fea-tures for reducing distortions and enhancing details. More-over, the authors use two NAFBlocks as the refinementmodule following the last decoder blocks for powerful rep-resentation. Finally, some improvements were made to theloss function. Instead of using a fixed loss weight, the teamdynamically adjusts the weight ratio of the loss function asthe training iterations progress, with an increased emphasison perceptual loss for better visual results during training.Through model fusion and weighted loss function, the per-formance of the model was further improved and ultimatelyachieved competitive results in the challenge.The loss function comprises both Charbonnier L1 lossand perceptual loss, with dynamically assigned weights.The inputs are cropped into 512 512 with a batch size of2, and the Adam optimizer is used. The initial learningrate is set to 1 104, and the CosineAnnealingLR sched- uler is employed with a maximum of 300,000 iterations anda minimum learning rate of 1 106 to adjust the learn-ing rate. They also use horizontal and vertical flips for dataenhancement. For testing, the authors split the original im-ages into 512512 patches and generate the final flare-freeimages. All experiments were performed on two NVIDIARTX 3090 GPUs with 24GB memory. GoodGameThis team proposes an efficient flare removalnetwork, based on Restormer. In the model, they useFlare-Aware Transformer Blocks to capture the Flare in theimage, and the composition structure is similar to that ofRestormer. Residual connections are also used in the modelso that the model only needs to learn the changes in theflare and does not need to reconstruct the entire image. Themodel is also efficient enough to infer large-resolution im-ages. shows the framework of the entire model.They trained 300,000 iterations to take the model to con-vergence. Progressive learning was used during the train-ing process, from the initial batch size of 16 and patchsize (resize) of 128, to the final batch size of 2 and patchsize of 384. During the training process, gradually increasethe patch size of the image and reduce the batch size, sothat the model can learn more details of the image. Theychoose AdamW as an optimizer, set the initial learning rateto 3 104, and introduce a weight decay of 1e 4. At thesame time, they adopted the cosine annealing learning ratescheduler (CosineAnnealingLR), where Tmax is set to500 and the minimum learning rate is set to 1e 6.In termsof loss, they used L1 loss, Fourier L1 loss, and Lpips loss,with Lpips loss accounting for the largest proportion. IIT-RPRThis team designs a method, based on U-formermodel architecture. FADU-Net shown in , trainedfrom the ground up. The synthesis of training images in-volves utilizing Flickr24K as background images and incor-porating 5k scattering flare images from Flare7K. An inno-vative night data augmentation strategy (Night Data Aug) is",
  ". The network architecture of GoodGame team. MDTA and GDFN are the same as Restormer": "implemented for background images, featuring four modes,randomly selected for each image during training. The pro-vided competitions images are also used as the validationdataset during the training of the architecture.The lossfunction is a combination of L1 loss and perceptual loss,with distinct weights assigned to areas inside and outsidethe flare.During training, a patch size of 512 is utilized, and theAdam optimizer is employed with an initial learning rateof 0.0001. This team conducts training for 1200K itera-tions and observes that extending the training duration mayfurther enhance results. Although, the proposed pipelinedemonstrates its efficacy by achieving a PSNR of 20.66 andLPIPS of 0.1926 on the MIPI challenges test dataset, show-casing the teams adeptness in addressing the complex taskof nighttime flare removal. It outputs the two images i.e.,the predicted flare and the predicted image. The predictedflare image shows the reflective and scattering flare presentin the input image. And predicted image is the output im-age without the flares containing only the light source. Thisarchitecture is trained on the 11GB NVIDIA GeForce RTX2080 Ti GPU for 6 days and 13 hours. Hp zhangGeekThis team designs a conditional varia-tional autoencoder (CVAE) for removing nighttimeflares. Specifically, for the nighttime flare removal task,CVAEs can contribute significantly due to their ability tomodel complex data distributions and generate high-quality,diverse outputs conditioned on given inputs. They also de-",
  "signed an Adaptive Normalization Module (ANM) to en-hance the details of input features": "As depicted in (a), they adopt U-Net architecture in the encoder that progressively downsamples the im-age into a more compact representation. The Prior network(abbreviated as Pr in (a)) shares the same structure asthe encoder. This network is designed to learn a prior distri-bution of the latent variables. In the network, the prior net-work models the distribution of latent variables that is fromthe flare-corrupted images. Inspired by PUIE-Net andU-GAT-IT , the adaptive normalization module (ANM)takes the latent representation from the encoder and refinesit. As shown in (b), the adaptive normalization mod-ule involves adjusting the feature distribution of the latentrepresentation to a state that is more conducive to generat-ing a clean image without flares. Using the refined latent EncoderDecoder",
  ".The overall network architecture of HP zhang Geekteam": "representation from the ANM, the decoder network recon-structs the image. The decoder effectively reverses the en-coding process, upscaling the latent representation back tothe original image dimensions to reproduce the image with-out the lens flare.This team only used the 600 image pairs for training.In cases where the dataset is relatively small, they adopt a90/10 split to maximize the amount of training data whilestill having a validation set to monitor overfitting and per-formance. They resize the images to 256 256 and applyhorizontal flipping, vertical flipping, and rotation randomlyfor data augmentation. The batch size is 16. They train thenetwork for 700 epochs. For the training phase, the lossfunction for training is formulated as follows:",
  "Ltotal = Lre + Lkl + Lper,(5)": "where Lre is the reconstruction loss, Lkl is the KL diver-gence, and Lper is the perceptual loss, and are learnableparameters.The experimental setup for the computational frame-work was implemented on an Ubuntu-based workstationequipped with a single NVIDIA RTX 4090 card. The dura-tion of the model training, inclusive of the validation phase,spanned approximately 16 hours. The inference time of themodel is approximately 6.3 milliseconds per image. It ispertinent to note that the inference protocol entailed sam-pling the model 20 times for each image. The final outputwas derived by computing the mean across these 20 sam-ples, ensuring robustness and stability in the generated re-sults. This methodical approach to inference underscoresthe models efficacy in handling the variability inherent inthe data, thereby contributing to the reliability of the out-comes. LehaanThis team utilizes the Uformer model as aflare-erasing module coupled with AOT-GAN for image in-painting. Uformer employs a hierarchical encoder-decoderstructure akin to UNet but substitutes convolutional layerswith Transformer blocks. Key aspects of Uformer includethe Locally-enhanced window (LeWin) Transformer blockfor localized context capture and the Multi-scale restorationmodulator for feature adjustment at various scales. AOT-GAN (Aggregation of Contextual Transformation - GAN)enhances context reasoning through AOT blocks in the gen-erator, facilitating the aggregation of contextual transfor-mations from different image areas for accurate inpainting.AOT blocks are a novel approach for convolutional neu-ral networks designed to enhance context reasoning. Theyachieve this by splitting a large kernel into smaller ones,each specializing in a specific number of output channels.These sub-kernels then analyze the input using different di-lation rates, allowing them to focus on varying areas of theimage. Finally, the outputs from all sub-kernels are merged,enabling the AOT block to consider the input from vari-ous perspectives and capture richer contextual information.This approach has shown promise in improving tasks likeimage inpainting. While the Uformer model can sufficiently remove theflare, it also inadvertently removes the pixels that were be-hind the flares. Hence, an inpainting module is used to in-paint back the image that was removed too. To specify theregion required for inpainting, image differencing followedby thresholding is done in order to get the regions affectedby UFormer. This is given as the mask to AOT-GAN whileinpainting. The complexity of the Uformer method comprises sev-eral stages: Self Attention: Time complexity - O(n2d2),Space complexity - O(n2d).Feed-Forward Network:Time complexity - O(2nd2), Space complexity - O(nd).Layer Normalization and Residual Connection: Constanttime and space complexity - O(1).Multi-Head Atten-tion (MHA): Time complexity - O(nh2d2).Total timecomplexity:O(n2hd2), Space Complexity O(n2d + nhd). For model training Mini-Batch size: 8, Epochs: 1000,Training workers:4, Evaluation workers:4, Dataset:Flare7k++, Optimizer:AdamW, learning rate = 10e-3and default decay parameters, Weight decay: 0.02, GPU:NVIDIA RTX 4050 Laptop GPU.",
  "Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun.Simple baselines for image restoration. In European Confer-ence on Computer Vision, 2022. 3, 4, 5, 6": "Yuekun Dai, Chongyi Li, Shangchen Zhou, Ruicheng Feng,and Chen Change Loy. Flare7k: A phenomenological night-time flare removal dataset.In Thirty-sixth Conference onNeural Information Processing Systems Datasets and Bench-marks Track, 2022. 1 Yuekun Dai, Chongyi Li, Shangchen Zhou, Ruicheng Feng,Yihang Luo, and Chen Change Loy.Flare7k++: Mixingsynthetic and real datasets for nighttime flare removal andbeyond. arXiv preprint arXiv:2306.04236, 2023. 1, 2, 4 Yuekun Dai, Chongyi Li, Shangchen Zhou, Ruicheng Feng,Qingpeng Zhu, Qianhui Sun, Wenxiu Sun, Chen ChangeLoy, Jinwei Gu, Shuai Liu, et al. Mipi 2023 challenge onnighttime flare removal: Methods and results. In IEEE Con-ference on Computer Vision and Pattern Recognition, 2023.2 Yuekun Dai, Yihang Luo, Shangchen Zhou, Chongyi Li, andChen Change Loy. Nighttime smartphone reflective flare re-moval using optical center symmetry prior. In IEEE Confer-ence on Computer Vision and Pattern Recognition, 2023. 1,5",
  "erative models. Advances in Neural Information ProcessingSystems, 2015. 7": "Qianhui Sun, Qingyu Yang, Chongyi Li, Shangchen Zhou,Ruicheng Feng, Yuekun Dai, Wenxiu Sun, Qingpeng Zhu,Chen Change Loy, Jinwei Gu, et al. Mipi 2023 challenge onrgbw remosaic: Methods and results. In IEEE Conferenceon Computer Vision and Pattern Recognition, 2023. 2 Qianhui Sun, Qingyu Yang, Chongyi Li, Shangchen Zhou,Ruicheng Feng, Yuekun Dai, Wenxiu Sun, Qingpeng Zhu,Chen Change Loy, Jinwei Gu, et al. Mipi 2023 challengeon rgbw fusion: Methods and results. In IEEE Conferenceon Computer Vision and Pattern Recognition, pages 28702876, 2023. 2",
  "Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-moncelli. Image quality assessment: from error visibility tostructural similarity. IEEE Transactions on Image Process-ing, 13(4):600612, 2004. 2": "Zhendong Wang, Xiaodong Cun, Jianmin Bao, WengangZhou, Jianzhuang Liu, and Houqiang Li. Uformer: A generalu-shaped transformer for image restorationn. In IEEE Con-ference on Computer Vision and Pattern Recognition, 2022.4, 6 Zhendong Wang, Xiaodong Cun, Jianmin Bao, WengangZhou, Jianzhuang Liu, and Houqiang Li. Uformer: A generalu-shaped transformer for image restoration. In IEEE Confer-ence on Computer Vision and Pattern Recognition, 2022. 4,8 Yicheng Wu, Qiurui He, Tianfan Xue, Rahul Garg, JiawenChen, Ashok Veeraraghavan, and Jonathan T. Barron. Howto train neural networks for flare removal. In IEEE Interna-tional Conference on Computer Vision, 2021. 1 Yicheng Wu, Qiurui He, Tianfan Xue, Rahul Garg, JiawenChen, Ashok Veeraraghavan, and Jonathan T Barron. Howto train neural networks for flare removal. In IEEE Interna-tional Conference on Computer Vision, 2021. 5 Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu-nawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang.Restormer: Efficient transformer for high-resolution imagerestoration. In IEEE Conference on Computer Vision andPattern Recognition, 2022. 3, 6, 7 Yanhong Zeng, Jianlong Fu, Hongyang Chao, and Bain-ing Guo. Aggregated contextual transformations for high-resolution image inpainting. IEEE Transactions on Visual-ization and Computer Graphics, 2022. 2 Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-man, and Oliver Wang. The unreasonable effectiveness ofdeep features as a perceptual metric. In IEEE Conference onComputer Vision and Pattern Recognition, 2018. 2 Suiyi Zhao, Zhao Zhang, Richang Hong, Mingliang Xu, YiYang, and Meng Wang. Fcl-gan: A lightweight and real-time baseline for unsupervised blind image deblurring. In30th ACM International Conference on Multimedia, 2022. 4 Yuyan Zhou, Dong Liang, Songcan Chen, Sheng-Jun Huang,Shuo Yang, and Chongyi Li. Improving lens flare removalwith general-purpose pipeline and multiple light sources re-covery. In IEEE International Conference on Computer Vi-sion, 2023. 1 QingpengZhu,WenxiuSun,YuekunDai,ChongyiLi,Shangchen Zhou,Ruicheng Feng,Qianhui Sun,Chen Change Loy, Jinwei Gu, Yi Yu, et al. Mipi 2023 chal-lenge on rgb+ tof depth completion: Methods and results. InIEEE Conference on Computer Vision and Pattern Recogni-tion, 2023. 2"
}