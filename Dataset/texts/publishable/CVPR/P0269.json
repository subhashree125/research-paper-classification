{
  "Abstract": "In the field of deep point cloud understanding, KP-Conv is a unique architecture that uses kernel points tolocate convolutional weights in space, instead of relyingon Multi-Layer Perceptron (MLP) encodings. While it ini-tially achieved success, it has since been surpassed by re-cent MLP networks that employ updated designs and train-ing strategies.Building upon the kernel point principle,we present two novel designs: KPConvD (depthwise KP-Conv), a lighter design that enables the use of deeper ar-chitectures, and KPConvX, an innovative design that scalesthe depthwise convolutional weights of KPConvD with ker-nel attention values. Using KPConvX with a modern ar-chitecture and training strategy, we are able to outperformcurrent state-of-the-art approaches on the ScanObjectNN,Scannetv2, and S3DIS datasets.We validate our designchoices through ablation studies and release our code andmodels.",
  ". Introduction": "The field of 3D point cloud understanding has experiencedsignificant growth in the past decade. This growth can beattributed to the availability of advanced 3D sensors and theincreasing use of deep learning in various research domains.The evolution of this field has known different phases. Un-til 2017, most of the proposed approaches relied on projec-tion in images or 3D grids . How-ever, after 2017, point-based methods gained dominance, leading to a rapid ex-pansion of the field. More recently, 3D point cloud un-derstanding followed the trend of attention and transformernetworks, which are widely adopted in other deep learn-ing fields . This diverse range ofapproaches is a response to the challenge posed by the un-structured and continuous nature of point clouds. In this dy-namic field, we explore the potential of Kernel Point Con-volution (KPConv) , one of the most successful point-based methods, when enhanced with modern techniquesand attention mechanisms.",
  ".KPConvD and KPConvX using small (S) and large(L) architectures outperform other state-of-the-art architectures onScanNetv2 dataset using a relatively small number of parameters": "KPConv proposed a convolution design that utilizes ker-nel points to store network weights in specific positionswithin the space. In this study, we revisit this design and in-troduce a new, lighter operator called KPConvD. Addition-ally, we enhance the operator by incorporating \"geometricattention\", a novel self-attention mechanism applied to thekernel points, resulting in KPConvX. Apart from the con-volution operator itself, we also reconsider the network ar-chitectures built using this operator. Due to its lightweightnature, we are able to construct deeper architectures. Over-all, we observe that our networks are faster than the originalKPConv and achieve state-of-the-art performance.In Sec. 2, we will provide a comprehensive overview ofthe different approaches to deep point cloud understanding.Our focus will be on point-based methods, and we will com-pare KPConvX to other similar designs. Sec. 3 is dedicatedto the definition of our convolution design. As depicted in, we utilize kernel points similar to KPConv, but weintroduce two major changes: depthwise weights, whichcombine neighbors features using the Hadamard product,and a new concept called kernel attention that scales thekernel weights based on the current input features. This ap-proach, which defines attention geometrically rather thantopologically, is analogous to image involution andis tailored for kernel points, which have a fixed order and",
  ". Illustration of our new designs compared to the original KPConv operator. KPConvD adopts a lighter depthwise design andKPConvX includes kernel attention": "spatial disposition, as opposed to neighbor points.Ad-ditionally, three other significant modifications are not il-lustrated in . When projecting kernel weights ontoneighbors, we simply use the nearest kernel point instead ofsumming the influences of all kernel points. This nearest-kernel implementation allows for a larger number of kernelpoints without significantly increasing computations. Fur-thermore, we define the positions of kernel points using twoshells, as proposed in . Lastly, we also limit the numberof neighbors to a fixed value, enabling faster computations.This lightweight operator allows for much deeper net-works. As suggested in other studies that modernize deepnetworks , we increase the depth (number oflayers) and width (number of channels) of our networksand use a KPConv stem as the first layer. We also em-ploy partition-based pooling for downsampling layers andincorporate convolution blocks in the decoder upsampling,as described in . Finally, we adopt a scaling factor of 2 (instead of 2) for the number of channels from one layerto the next. The overall network architecture is illustratedin .The impact of these choices is evaluated in our exper-iments in Sec. 4.Ablation studies demonstrate that ourcontributions result in better performance. Our networksoutperform the state of the art on object classification andsemantic segmentation tasks, ranking first on the ScanOb-jectNN , S3DIS-Area5 , and Scannetv2 bench-marks. highlights the superiority of our approachover other competing methods on Scannetv2. KPConvX-Lis our best-performing network, while KPConvD-L offers agreat compromise between performance and efficiency.In short, our contributions are as follows:",
  ". Related Work": "Deep point cloud understanding: As explained in the in-troduction, point clouds are fundamentally different fromimages. They are unstructured yet permutation invariant,and their data is continuous but sparse. Due to this complexnature, numerous approaches have been proposed for pointcloud understanding.One of the early ideas for processing point clouds withdeep networks was to project points onto a 3D grid .The use of sparse structures like hash-maps or octrees al-lowed these approaches to scale to larger grids and achieveimproved performance . Alternatively,points can be projected onto 2D image planes and processedwith image networks . However, quantiz-ing irregular point clouds onto regular grids inevitably re-sults in data loss, and grids lack flexibility.Point-wise methods: To address these problems, it is possi-ble to directly extract features from the point clouds insteadof using projections . Typically, features are mixed us-ing linear layers, and geometric patterns are extracted us-ing a local operator. The definition of this local operatoris crucial and often the main distinguishing factor betweendifferent methods.Many approaches utilize Multi-LayerPerceptrons (MLP) as feature extractors, operating directlyon point coordinates . MLP extractors can be viewed as combinationsof hyperplanes, and with a sufficient number of parameters(planes), they can represent any geometric pattern. How-ever, complex patterns require a large number of parame-ters.3D transformers: Transformer-based networks utilize at-tention as their primary feature extraction mechanism.Whether they employ voxel grouping or lo-cal point neighborhoods , they al-ways employ MLP to encode point features and geomet-ric patterns.As a result, they face the same limitationas MLP-based convolutions. Additionally, their attentionmechanism is based on the combination of neighbors keyswith central queries, whereas we adopt a simpler central-generated kernel attention similar to .",
  "Segmentation": ". Illustration of our network architecture KPConvX-L. It can be used for semantic segmentation or shape classification. It has atotal of 44 encoder blocks, plus one stem KPConv and 4 decoder blocks. We use inverted bottleneck blocks and grid subsampling fromone layer to the next. Geometric convolutions: Eventually, local feature extrac-tors can be defined geometrically . This re-duces the complexity of the convolution operator withoutlosing data through quantization.We use this approachand incorporate an attention mechanism to the convolutionweights, similar to the modulations used in the deformableversion of KPConv . However, instead of a single mod-ulation per kernel point, we use group modulations as de-scribed in .",
  ". KPConvD: Depthwise Kernel Point Convolu-tion": "In essence, our KPConvD operator can be understood as adepthwise version of KPConv. However, aside from somehyperparameter adjustments, the core implementation hasbeen optimized for efficiency. We provide a detailed de-scription of the modifications, using notations similar tothose used in the original KPConv paper, where K rep-resents the number of kernel points and H represents thenumber of neighbors: input points: xi R3 with i < H input features: fi RC with i < H kernel points: xk R3 with k < K kernel weights: Wk RCO with k < KTherefore, a standard KPConv can be written as",
  "k<Kh (xi x, xk) Wkfi ,(1)": "where Nx is a radius neighborhood with r R, and h is theinfluence (or correlation) function. Note that in fact, the im-plementation of KPConv uses a radius neighborhood trun-cated by a maximum number of neighbors for efficiency.It is therefore equivalent to a KNN neighborhood, whereall points further than r are ignored, which is how we im-plement it in our work. We thus have a fixed number ofneighbors, named H. We use the same influence functionas KPConv:",
  ",(2)": "where is the influence distance of the kernel points. Weuse the notation hik in place of h (xi x, xk) for conve-nience, but it also represents an optimization in our imple-mentation. All the blocks of the same layer can share thesame kernel points and therefore share the same influences.We thus compute the matrix of influences (hik) RHK",
  "k<Khik wk fi ,(3)": "where wk RC is a single vector of weights instead ofa full matrix and is the Hadamard product. We observethat the influence matrices usually contain a majority of ze-ros. Indeed, the kernel points only apply their weights tothe neighbors that are in their area of influence. However, inthis definition, the influence of all kernel points is summed",
  "i<Hhik wk fi ,(4)": "where k is the index of the nearest kernel point to theneighbor i. With this nearest-kernel implementation, weare able to increase the number of kernel points and thusthe descriptive power of the convolution without affectingthe number of operations.Because of this ability to increase the number of kernelpoints, we adopt a shell definition of the kernel point po-sitions as proposed in , but keep the convolution def-inition defined in Eq. (4). The kernel point locations arefound with a similar optimization process to the one usedin KPConv, but with an equality constraint added for the ra-dius of the points of each shell. More details can be foundin the supplementary material. We note the number of ker-nel points as a list [1, N1, ..., Ns], where s is the numberof shells. In the rest of the paper, we use a two-shell ker-nel disposition . Examples of 2D kernel pointsdisposition and their corresponding nearest-kernel areas areshown in .Eventually, we choose a radius of r = 2.1 times the sub-sampling grid size, slightly smaller than the original KP-Conv. This value was found empirically (see additional ex-periments in the supplementary material). The chosen ra-dius does not affect memory consumption as the numberof neighbors is fixed. Because of our nearest-kernel imple-mentation, the area of influence of each kernel point doesnot overlap, and we use a large influence radius = r. Wenote that this nearest-kernel design was proposed in the the-sis , but never implemented.",
  ". KPConvX: Adding Kernel Attention": "Local self-attention, as it is commonly used in transform-ers for point clouds , is defined with respect tothe neighbor features. It is a topological operator, whichcombines features without extracting geometric informa-tion. This is why most of the self-attention designs for pointclouds incorporate some kind of geometric encoding. On the contrary, the kernel attention we propose is geo-metric in nature. The attention weights are generated for lo-cal parts of the space, instead of being generated for neigh-bors depending on their features. This design gives a geo-metric structure to our attention mechanism, and the abilityto focus on geometric patterns, as shown in . There-fore, our operator does not need additional position encod-ings to capture geometric information.With the previous notations, we define KPConvX as",
  "where mk is a vector of modulations generated for the kth": "kernel point and is a grouped version of the Hadamardproduct. Indeed, mk RCg where Cg = C/G and G isthe number of groups. We note that with G = C, our designis very similar to the modulations of deformable KPConv, and with G = 1, every channel gets its own modu-lation. In the following, we use G = 8 unless otherwisespecified. The modulations are generated all together fromthe feature of the central point:",
  "(mk)k<K = (x) ,(6)": "where is defined as a two-layer MLP, with C hidden chan-nels, K Cg output channels, and a sigmoid as final activa-tion. We note that with this attention definition, augmentingthe number of kernel points will affect the number of oper-ations. However, thanks to our nearest-kernel implementa-tion, the increase in operations is still reduced compared tothe full-summation kernel.As stated above, from this general definition we canregress to a depthwise convolution, by removing the mod- .Illustration of our kernel attention principle, wherechunks of space are weighted (right) compared to standard pointself-attention where the neighbors are weighted depending on theirfeature instead of their position (left). In our design, no positionencoding is needed, as the attention itself is a position encoding.",
  "i<Hhik (mk fi) .(7)": "We named this design KPInv, but it did not match the per-formances of KPConvX, and we do not use it in our exper-iments.KPConvX design would not work without kernel pointsor a similar structure. Why? The modulations are gener-ated with in a certain order, which corresponds to theorder of the kernel points. If we were to produce modula-tions directly for neighbors, as a naive re-implementation ofthe image involution for point clouds would do, they wouldbe applied in this order to the neighbors. However, evenif they are ordered by coordinates or distance to the cen-ter, the neighbors order remains highly unstable, and there-fore the modulations would be applied randomly to differentneighbors every time. This instability undermines the net-works ability to extract geometric patterns and we observedweaker performances in the few tests we ran.Furthermore, predicting weights from the central pointforces the network to include contextual information in itsfeatures. This contextual information is necessary for de-ciding where to focus attention spatially in ensuing layers.We believe this is one of the crucial properties that makeKPConvX a better descriptor.",
  ". Modern Architecture and Training": "In this work, we also focused on the design of new networkarchitectures. We follow the path of other works that advo-cate the use of modern techniques for deep networks in im-ages or point clouds . Firstly, we define a small (S)and large (L) architectures, with respectively blocks per layer and blocks per layer. Wechoose an initial width of 64 channels for both networksas shown in .For the stem of the network, we choose a full standardKPConv, to extract strong initial features as advocated in. The cost of this layer is negligible compared to therest of the network.Each network block is designed as an inverted bottlenecksimilarly to . Compared to the standard ResNet bot-tleneck design that starts with a downsampling MLP,follows with a local extractor, and finishes with an upsam-pling MLP, the inverted design places the local extractor atthe beginning of the block and has two MLPs that upsampleand then downsample the features. We use a normalizationlayer (Batch Normalization) and an activation layer (LeakyReLU), after KPConvX and after each MLP. For the final MLP of the block, the activation layer is moved after theaddition.Each layer has its own fixed number of neighborsfor the local operator.In all our experiments, we use neighbors for each layer. The numberof neighbors is lower in the early layers because the pointclouds are more sparse before being subsampled.Instead of using heavy operations for the pooling layers,we follow and implement partition-based pooling witha grid of increasing size. In the same fashion, we use a ratioof 2.2 for the grid scaling between layers, which reduces theoverall memory cost of the network. The grid upsamplingoperation projects the features back to the points that werein the same grid cell, without any modification. We alsoappend an additional block in each decoder layer to improvethe performance.Furthermore, a common network regularization scheme,stochastic depth , is implemented with DropPath oper-ations. It consists of randomly skipping network blocks forentire elements of a batch. The implementation of DropPathfor our network is not trivial because we use variable batchsize as in the original KPConv . We refer to the supple-mentary material and our open-source implementation formore details.Interestingly, we notice that the number of channels inthe deeper layers does not need to be doubled to maintaingood performances. Instead of keeping a ratio of 2 betweenthe widths of two consecutive layers, we use a ratio of 2.This reduces the size and memory consumption of our net-work, without affecting its performance.Segmentation Architecture: After the last upsamplinglayer, we use a standard segmentation head, which consistsof a two-layer MLP with 64 hidden channels, and nclassoutput channels, followed by a softmax layer. At training,we use a standard cross-entropy loss.Classification Architecture: The classification archi-tecture only uses the encoder part of the network.Thefeatures are aggregated with a global average pooling andprocessed by the classification head, which consists of atwo-layer MLP with 256 hidden channels, and nclass out-put channels, followed by a softmax layer. We use a cross-entropy loss with label smoothing for training.Finally, we train our networks with the more recentAdamW optimizer and we use up-to-date data aug-mentation strategies . More details and other trainingparameters can be found in the supplementary material.",
  ". Semantic Segmentation": "Data and metrics. S3DIS is a challenging benchmarkthat consists of 6 large-scale indoor areas, with a total of 271rooms spread across three different buildings. The pointsare densely sampled on the mesh surfaces and annotatedwith 13 semantic categories. Scannetv2 , on the otherhand, is relatively larger and includes 1,201 indoor RGB-D scenes for training, 312 scenes for validation, and 100scenes for testing. Semantic labels in 20 categories are an-notated.We adopt the standard experimental setup [34, 41, 43, 56], using the fifth area as our test set for S3DIS and fol-lowing the official train/evaluation split for ScanNetv2. Thestandard metric for these datasets is the mean class-wiseintersection-over-union (mIoU). Additionally, we may alsopresent the mean of class-wise accuracy (mAcc) and theoverall point-wise accuracy (OA) for S3DIS.Experimental setup. For both datasets, during training, werandomly select a point from a random room as the center",
  "KPConvX (ours)13.5M76.3": "of a sphere. We then sample all the points within a radiusof 2.5m to create an input point cloud. Our 5-channel inputfeature includes a constant one feature, RGB values, and thez-coordinate of the points (before augmentation) for S3DIS.For ScanNetv2, we add 3 values for the normals. Followingthe approach in , we stack these variable-length pointclouds to create stacked batches. We use a target batch sizeof 4 and accumulate 6 forward passes before propagatingthe gradient backward. This effectively gives us a batch sizeof 24, while keeping the memory consumption of a batchsize of 4.We subsample the input point clouds using a voxel sizeof 0.04m for S3DIS and 0.02m for ScanNetv2. Similar torecent methods, we do not use spheres on the test set. In-stead, we evaluate our networks on the entire rooms. Moredetails on the training parameters and augmentations can befound in the supplementary material.In contrast to , we advocate for the use of votingwhen evaluating network predictions. We argue that if allthe votes come from the same network with fixed weights,it cannot be considered an ensemble method. Moreover,",
  ". Visualization of Semantic segmentation results on in rooms of S3DIS Area 5": "when a network is trained with data augmentation, its re-sults may vary depending on how it encounters the test data.For instance, as shown in , the results can unexpectedlybe high when using a certain room orientation. By utilizingvoting, we reduce the variance in the results and providesmoother predictions.Results.We compare our best network performanceto state-of-the-art results using different versions of thedataset.The results are summarized in Tab. 2.OurKPConvX-L model achieves the highest mIoU score in bothcases, outperforming the current state of the art by +1.2mIoU on S3DIS and by +0.6 mIoU on Scannet. We alsoprovide qualitative results of point cloud semantic segmen-tation in .",
  ". Shape Classification": "Data and metrics. ScanObjectNN is a recent classificationbenchmark that includes approximately 15,000 real scannedobjects and 2,902 unique object instances . Due to oc-clusions and noise, it is considered more challenging thanother 3D shape classification benchmarks. Following thestandard protocol , we utilize the hardest variant ofthe dataset, PB_T50_RS. The performances are evaluatedwithout pre-training using the mean of class-wise accuracy(mAcc) and overall point-wise accuracy (OA).Experimental setup. We adopt the same point resamplingstrategy as : during training, we randomly sample1,024 points for each shape, while during testing, points areuniformly sampled. Although the input points are not sub-sampled with a grid, we still need to choose a fake initialgrid size (0.02cm) to scale our convolution radius and de-fine the next layer grids. Our batch size is set to 32 pointclouds, with an accumulation of 2 forward passes, resultingin an effective batch size of 64. Similarly, we employ votingduring testing due to the aforementioned reasons.Results. Tab. 3 displays our network performances com-pared to other competing methods. We present both the av-erage and best scores from ten runs. Our top-performingnetwork, KPConvX-L, outperforms PointVector by asignificant margin (1.1% mAcc and OA on average). It isimportant to note that we did not compare against methodsthat employ a pre-training strategy.",
  ". Ablation: From KPConv to KPConvX": "In this experiment, we applied a complete series of changesto transform KPConv into KPConvX. We conducted the ex-periment on S3DIS dataset under controlled settings, us-ing the same training parameters and data augmentation asin our other experiments. This is why our version of KP-Conv achieved a slightly better score than the original pa-per. In the previous experiments, we provided the results ofour best model and plan to share these best weights. How-ever, in all the following ablation studies, for each version,",
  "KPNeXt-S (ours)88.3 0.489.086.7 0.587.4KPNeXt-L (ours)88.9 0.389.387.3 0.588.1": "we trained and tested 10 identical models and got an aver-age score. This is more reliable to provide fair comparisonstudies. Tab. 4 shows the average and standard deviationof the 10 test mIoUs on S3DIS Area5, in addition to thebest mIoU obtained. For each version, we also provide thenumber of million parameters, the network throughput (TP)in instances per second (ins/s), and the GPU memory con-sumption in GB during inference. This allows a better un-derstanding of the impact of each change. For comparisonpurposes, the throughput and GPU consumption are mea-sured using the same parameters as : batches of 16 in-put point clouds, each containing 15, 000 points, were fed tothe network. Each model was trained and tested on a singleNvidia Tesla V100 32GB GPU.First, we transform KPConv into a depthwise operation,reduce the radius, reduce the number of neighbors to fixedvalues, and add one kernel shell ( to ). Thisresults in a +0.5 mIoU improvement.Next, we add more layers from KP-FCNN to our large architecture . We also reduce the",
  "channel scaling to": "2 and add decoder layers and Drop-Path. With these changes, the mIoU improves by 1.9%,while maintaining similar GPU memory consumption andthroughput. To finish the architecture changes, we use in-verted bottleneck blocks for an additional 1.9% mIoU gain.We then demonstrate the effectiveness of our nearestinfluence design, which is 13% faster (TP) and 56%lighter (GPU).Additionally, our kernel-point-sharingstrategy makes the model 38% faster (TP) and 13%lighter (GPU) without impacting the results.At this point, we have our KPConvD-L architecture,which improves performances of the original KPConv by+3.7 mIoU on average, runs 80% faster, and uses only20% GPU memory compared to the original KPConv.Finally, we add our attention mechanism to create the",
  ". Other Ablation Studies": "In the previous ablation study, we demonstrated the im-provements brought by our contributions. In this section,we provide more insight into our method by studying theeffect of some parameter changes.First, we compare our different architectures on S3DISArea 5. From Tab. 5, we observe that our best networkis KPConvX-L. However, it also has the highest computa-tional cost. Using KPConvD-L is a good option for more ef-ficiency, this model being 35% faster, while only sacrificing0.2% mIoU on average. Overall, our throughput is highlycompetitive and comparable to PointNeXt-XL , whichpresented a throughput of 46 ins/s, similar to KPConvX-Lin the same setup, even though their performances are only70.5 mIoU on average on S3DIS Area 5.Next, we assess the impact of the number of groups inKPConvX on the performance of KPConvX-L. As shownin Tab. 6, when there are fewer groups, the network has agreater number of parameters to learn. Conversely, withmore groups, the modulations become less effective. Weobserve that the network performs really well with 8 groups,which is the value we use in the rest of the paper. This valueprovides a balance between enhancing descriptive powerand avoiding an excessive parameter burden.",
  ". Conclusion": "We present KPConvX, an efficient feature extractor forpoint clouds that combines depthwise convolution and self-attention. Additionally, we introduce KPConvX-L, a newdeep architecture for semantic segmentation and shape clas-sification. KPConvX-L is trained using the latest strategiesand achieves state-of-the-art performance on several bench-marks for 3D semantic segmentation and 3D shape classifi-cation. Limitations and future work. Deep learning architec-tures utilize local feature extractors to generate new infor-mation from local neighborhoods. However, the topologi-cal or geometric nature of these local feature extractors isoften understudied. Transformers and some 3D graph ar-chitectures have a topological nature, as they are based onneighbor-feature relationships. Without incorporating MLPgeometric encodings, they would overlook the geometry ofthe point clouds. On the other hand, structured convolutionslike KPConv or voxel networks are inherently geometric en-codings, merging features solely based on their locations,without considering neighbor features. With KPConvX, weintroduced attention into a geometric operator, in contrastto recent point transformers that incorporated geometric en-codings into a topological operator. Nevertheless, in bothcases, the feature extraction operators still generate new in-formation, either topologically or geometrically. It is im-portant to conduct thorough studies to understand how thetopological or geometric nature of local feature extractorsimpacts the learning process, and whether these two ideascan be combined in a single architecture to separate topo-logical and geometric features. Iro Armeni, Ozan Sener, Amir R. Zamir, Helen Jiang, Ioan-nis Brilakis, Martin Fischer, and Silvio Savarese. 3d seman-tic parsing of large-scale indoor spaces. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recog-nition, pages 15341543, 2016.http:// buildingparser.stanford.edu/ dataset.html. 2, 5, 6",
  "Matan Atzmon, Haggai Maron, and Yaron Lipman. Pointconvolutional neural networks by extension operators. ACMTransactions on Graphics (TOG), 37(4):71, 2018. 1, 3": "Alexandre Boulch, Bertrand Le Saux, and Nicolas Audebert.Unstructured point cloud semantic labeling using deep seg-mentation networks. In Proceedings of the Workshop on 3DObject Retrieval (3DOR), 2017. 1, 2 Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, andJiaya Jia. Largekernel3d: Scaling up kernels in 3d sparsecnns. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1348813498,2023. 2, 6 Jaesung Choe, Chunghyun Park, Francois Rameau, JaesikPark, and In So Kweon. Pointmixer: Mlp-mixer for pointcloud understanding. In European Conference on ComputerVision, pages 620640. Springer, 2022. 2, 6, 3 Christopher Choy, JunYoung Gwak, and Silvio Savarese.4d spatio-temporal convnets: Minkowski convolutional neu-ral networks. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 30753084, 2019. 2, 6, 3 Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-ber, Thomas Funkhouser, and Matthias Niener. Scannet:Richly-annotated 3d reconstructions of indoor scenes.InProceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pages 58285839, 2017.http:// kaldir.vc.in.tum.de/ scannet_benchmark. 2, 5, 6 Xin Deng, WenYu Zhang, Qing Ding, and XinMing Zhang.Pointvector: A vector representation in point cloud analysis.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 94559465, 2023. 2,6, 7, 3 Ankit Goyal, Hei Law, Bowei Liu, Alejandro Newell, andJia Deng. Revisiting point cloud shape classification with asimple and effective baseline. In International Conferenceon Machine Learning, pages 38093820. PMLR, 2021. 7 Benjamin Graham, Martin Engelcke, and Laurens van derMaaten. 3d semantic segmentation with submanifold sparseconvolutional networks. In Proceedings of the IEEE Con-ference on Computer Vision and Pattern Recognition, pages92249232, 2018. 2 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on Computer Vision and PatternRecognition, pages 770778, 2016. 5, 2 Wenbo Hu, Hengshuang Zhao, Li Jiang, Jiaya Jia, andTien-Tsin Wong. Bidirectional projection network for crossdimension scene understanding.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1437314382, 2021. 6 Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. Point-wise convolutional neural networks. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recogni-tion, pages 984993, 2018. 1, 2, 3",
  "Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-ian Q Weinberger. Deep networks with stochastic depth. InEuropean conference on computer vision, pages 646661.Springer, 2016. 5, 2": "Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, HengshuangZhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified Trans-former for 3D Point Cloud Segmentation. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 85008509, 2022. 1, 2, 5, 6, 7, 3 Loic Landrieu and Martin Simonovsky. Large-scale pointcloud semantic segmentation with superpoint graphs. In Pro-ceedings of the IEEE Conference on Computer Vision andPattern Recognition, pages 45584567, 2018. 3 Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou,Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encodersfor object detection from point clouds. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1269712705, 2019. 2 Felix Jremo Lawin, Martin Danelljan, Patrik Tosteberg,Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg.Deep projective 3d semantic segmentation. In InternationalConference on Computer Analysis of Images and Patterns,pages 95107. Springer, 2017. 1, 2 Huan Lei, Naveed Akhtar, and Ajmal Mian. Seggcn: Effi-cient 3d point cloud segmentation with fuzzy spherical ker-nel. In Proceedings of the IEEE/CVF conference on com-puter vision and pattern recognition, pages 1161111620,2020. 3 Duo Li, Jie Hu, Changhu Wang, Xiangtai Li, Qi She, LeiZhu, Tong Zhang, and Qifeng Chen. Involution: Invertingthe inherence of convolution for visual recognition. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1232112330, 2021. 1, 2, 3,5 Jiaxin Li, Ben M. Chen, and Gim Hee Lee. So-net: Self-organizing network for point cloud analysis.In Proceed-ings of the IEEE Conference on Computer Vision and PatternRecognition, pages 93979406, 2018. 1, 2 Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di,and Baoquan Chen. Pointcnn: Convolution on x-transformedpoints. In Advances in Neural Information Processing Sys-tems, pages 820830, 2018. 1, 2, 7, 3 Yuyan Li, Chuanmao Fan, Xu Wang, and Ye Duan. Spnet:Multi-shell kernel convolution for point cloud semantic seg-mentation. In International Symposium on Visual Comput-ing, pages 366378. Springer, 2021. 2, 3, 4, 1 Haojia Lin, Xiawu Zheng, Lijiang Li, Fei Chao, ShanshanWang, Yan Wang, Yonghong Tian, and Rongrong Ji. Metaarchitecture for point cloud analysis.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1768217691, 2023. 6, 3",
  "Chunghyun Park, Yoonwoo Jeong, Minsu Cho, and Jae-sik Park.Fast Point Transformer.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1694916958, 2022. 1, 2, 6": "Jinyoung Park, Sanghyeok Lee, Sihyeon Kim, YunyangXiong, and Hyunwoo J Kim. Self-positioning point-basedtransformer for point cloud understanding. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 2181421823, 2023. 2, 6, 7, 3 Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas.Pointnet: Deep learning on point sets for 3d classificationand segmentation. In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pages 652660,2017. 1, 2, 7, 3 Charles R. Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Point-net++: Deep hierarchical feature learning on point sets in ametric space. In Advances in Neural Information ProcessingSystems, pages 50995108, 2017. 1, 2, 6, 7, 3 Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai,Hasan Abed Al Kader Hammoud, Mohamed Elhoseiny, andBernard Ghanem. PointNeXt: Revisiting PointNet++ withImproved Training and Scaling Strategies.arXiv preprintarXiv:2206.04670, 2022. 2, 5, 6, 7, 8, 3 Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger.Octnet: Learning deep 3d representations at high resolutions.In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, 2017. 1, 2 Hang Su, Subhransu Maji, Evangelos Kalogerakis, and ErikLearned-Miller. Multi-view convolutional neural networksfor 3d shape recognition. In Proceedings of the IEEE In-ternational Conference on Computer Vision, pages 945953,2015. 1, 2 Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji,Evangelos Kalogerakis, Ming-Hsuan Yang, and Jan Kautz.Splatnet: Sparse lattice networks for point cloud processing.In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pages 25302539, 2018. 2 Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, JonShlens, and Zbigniew Wojna. Rethinking the inception archi-tecture for computer vision. In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages28182826, 2016. 5 Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, and Qian-Yi Zhou. Tangent convolutions for dense prediction in 3d.In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pages 38873896, 2018. 2 Lyne Tchapmi, Christopher Choy, Iro Armeni, JunYoungGwak, and Silvio Savarese. Segcloud: Semantic segmen-tation of 3d point clouds. In 2017 International Conferenceon 3D Vision (3DV), pages 537547. IEEE, 2017. 6, 3",
  "Hugues Thomas. Learning new representations for 3D pointcloud semantic segmentation. PhD thesis, Universit Parissciences et lettres, 2019. 4": "Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud,Beatriz Marcotegui, Franois Goulette, and Leonidas JGuibas. Kpconv: Flexible and deformable convolution forpoint clouds. In Proceedings of the IEEE International Con-ference on Computer Vision, pages 64116420, 2019. 1, 3,4, 5, 6 Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua,Thanh Nguyen, and Sai-Kit Yeung. Revisiting point cloudclassification: A new benchmark dataset and classificationmodel on real-world data. In Proceedings of the IEEE/CVFinternational conference on computer vision, pages 15881597, 2019. 2, 5, 7",
  "the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 31733182, 2021. 1, 2, 3": "Zetong Yang, Li Jiang, Yanan Sun, Bernt Schiele, and Ji-aya Jia. A unified query-based paradigm for point cloud un-derstanding. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 85418551, 2022. 6 Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, JieZhou, and Jiwen Lu. Point-bert: Pre-training 3d point cloudtransformers with masked point modeling. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1931319322, 2022. 1, 2, 7 Hengshuang Zhao, Li Jiang, Chi-Wing Fu, and Jiaya Jia.Pointweb: Enhancing local neighborhood features for pointcloud processing.In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages55655573, 2019. 2 Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, andVladlen Koltun. Point transformer. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 1625916268, 2021. 1, 2, 4, 6, 3"
}