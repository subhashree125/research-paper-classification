{
  "to a phone scan(b) Animatable authentic hand avatar": ". We introduce (a) UHM, which can universally represent arbitrary IDs of hands at a high fidelity. Our adaptation pipeline fitspre-trained UHM to a phone scan, which produces (b) an animatable authentic 3D hand avatar. Images of (b) are rendered using ouradapted hand avatar with the Phong reflection model and environment maps .",
  "Abstract": "The authentic 3D hand avatar with every identifiableinformation, such as hand shapes and textures, is neces-sary for immersive experiences in AR/VR. In this paper, wepresent a universal hand model (UHM), which 1) can uni-versally represent high-fidelity 3D hand meshes of arbitraryidentities (IDs) and 2) can be adapted to each person with ashort phone scan for the authentic hand avatar. For effectiveuniversal hand modeling, we perform tracking and model-ing at the same time, while previous 3D hand models per-form them separately. The conventional separate pipelinesuffers from the accumulated errors from the tracking stage,which cannot be recovered in the modeling stage. On theother hand, ours does not suffer from the accumulated er-rors while having a much more concise overall pipeline. Weadditionally introduce a novel image matching loss functionto address a skin sliding during the tracking and modeling,while existing works have not focused on it much. Finally,using learned priors from our UHM, we effectively adaptour UHM to each persons short phone scan for the authen-tic hand avatar.",
  "every identifiable information, including 3D hand shape andtexture, is necessary for immersive experiences in AR/VR": "A 3D hand model is a function that produces a 3D handfrom a 3D pose and identity (ID) latent code. The pose rep-resents 3D joint angles, and the ID latent code determinesidentifiable hand shape (e.g., thickness and size) in the zeropose or textures (e.g., skin color and fingernail polish). Suchtwo inputs (i.e., 3D pose and ID code) are used to drive pre-trained 3D hand models, where the 3D poses can be ob-tained from 3D hand pose estimators and ID latent code can be obtained in a personalizationstage . Those two inputs are relatively affordable datafrom single or stereo camera setup of in-the-wild environ-ment than 3D reconstruction , which requires at leasttens of cameras. Hence, the 3D hand model is a core com-ponent of the 3D hand avatar. We present a universal hand model (UHM), which 1) canuniversally represent high-fidelity 3D hand meshes of arbi-trary IDs like (a) and 2) can be adapted to each per-son with a short phone scan for the authentic hand avatarlike (b). For the effective universal hand modeling,we perform the tracking and modeling at the same time,while existing 3D hand models relyon a separate tracking and modeling pipeline. Their track-ing stage prepares target 3D meshes by non-rigidlyaligning a template mesh to targets, such as 3D joint coordi-nates, 3D scans, masks, and images. In this way, the track-ing stage provides 3D meshes with a consistent topologyacross all captures. Then, a modeling stage supervises 3D",
  "arXiv:2405.07933v1 [cs.CV] 13 May 2024": "hand models with the tracked 3D meshes. One of the limi-tations of such a conventional separated pipeline is that thetracking errors cannot be recovered in the modeling stage,which we call error accumulation problem. On the otherhand, as our UHM performs the tracking and modeling atthe same time in a single stage, it does not suffer from theerror accumulation problem while the overall pipeline be-comes much more concise.We additionally propose an optical flow-based loss func-tion to prevent skin sliding during the tracking and model-ing, while existing 3D hand models have not focused onit much. Most 3D hand models are simplytrained by minimizing per-vertex distance against tracked3D meshes, and the tracking is performed by min-imizing iterative closest point (ICP) distance against 3Dscans. There could be a number of correspondences be-tween 3D scans and 3D meshes from the 3D hand modelsas they do not share the same mesh topology. Therefore,without proper objective functions, some vertices of the 3Dhand models could slide to semantically wrong positions.For example, although a group of vertices is supposed tobe consistently located at the thumbnail across all captures,due to the ambiguity of the ICP loss, they could be slid tothe below of the thumbnail. To address this, we propose animage matching loss function, which minimizes the normof the optical flow between our rendered images and cap-tured images. The optical flow provides image-level cor-respondences, especially useful for distinctive hand parts,such as fingernails and wrinkles on the palm. As we use adeep optical flow estimation network , which can rec-ognize contextual information of images, the optical flowprovides semantically meaningful correspondences, whilethe ICP loss does not.Most importantly, we introduce an effective pipeline foradapting our UHM to each person with a short phone scan,which gives the authentic hand avatar. We found that ex-isting works produce plausible outputs, but they lackauthenticity, for example, slightly different 3D hand shapesfrom the target hand. On the other hand, with the help ofuseful priors from the tracking and modeling stage, we suc-cessfully achieve highly authentic results.Our contributions can be summarized as follows. We present UHM, a 3D hand model that can 1) univer-sally represent high-fidelity 3D hand meshes of arbitraryIDs and 2) be adapted to each person with a short phonescan for the authentic 3D hand avatar.",
  ". Related works": "3D hand models. Universal 3D hand modeling aims totrain a 3D hand model that can universally represent 3Dhands of arbitrary IDs. MANO is one of the pioneersin universal 3D hand modeling, and it is the most widelyused one. NIMBLE is a 3D hand model that consistsof bones, muscles, and skin mesh. LISA is based onthe implicit representation, motivated by neural radiancefield . Handy is a high-fidelity 3D hand modelthat follows a formulation of MANO. Due to the difficultyof universal modeling and collecting large-scale data frommultiple IDs, there have been introduced several personal-ized 3D hand models. Those personalized 3D hand modelscan only represent a single ID of the training set and cannotrepresent novel IDs. DHM is a high-fidelity personal-ized 3D hand model. LiveHand and HandAvatar arebased on the implicit 3D representation of hands, inspiredby neural radiance field . RelightableHands is arelightable personalized 3D hand model.Compared to the above 3D hand models, our UHMhas three distinctive advantages. First, UHM performs thetracking and modeling at the same time to address the er-ror accumulation problem from the tracking stage. Second,we introduce a novel image matching loss function to ad-dress the skin sliding issue during the tracking and model-ing. Finally, ours can produce authentic hand avatar from aphone scan, while previous models require accu-rate 3D keypoints and MANO registrations of capture stu-dio datasets . In addition, their texture modules produceimages of studio space , which has a big appear-ance gap from phone capture images. The texture moduleof Handy fails to replicate person-specific details, suchas fingernail polish and tattoos, due to the limited expres-siveness of their latent space.3D hand avatar from a phone scan. Creating a 3D handavatar from a short phone scan has been started to be studiedrecently. The 3D hand avatar should 1) be personalized to atarget person with authenticity including 3D hand shape andtexture and 2) be able to be driven by 3D poses. Previousworks created a 3D hand avatar from a long cap-ture from a studio using accurate 3D assets, such as3D tracking results and calibrated multi-view images. As-suming such 3D assets is a bottleneck for making a 3D handavatar in our daily life as capturing and acquiring such 3Dassets require lots of resources, such as tens or hundreds ofcalibrated and synchronized cameras. Recently, HARP is introduced, which can make a 3D hand avatar from ashort phone scan. It uses subdivided MANO as anunderlying geometric representation and optimizes albedoand normal maps for personalization. Compared to HARP,",
  ". Formulation": "We use the linear blend skinning (LBS) as an underlyinggeometric deformation algorithm following previous mesh-based ones . Given 3D vertices and 3D jointcoordinates in the zero pose space (i.e., template space), de-noted by J and V respectively, we apply various correc-tives to them and perform LBS to apply the 3D pose to thezero pose space. shows the effects of the correctives.There are three types of correctives: ID-dependent skele-ton corrective Jid, ID-dependent vertex corrective V id,and pose-and-ID-dependent vertex corrective V pose. TheID-dependent skeleton corrective Jid and ID-dependentvertex corrective V id are to model different 3D skele-ton (e.g., bone lengths) and 3D hand shapes (e.g., thick-ness) in the zero pose space, respectively, for each ID.The pose-and-ID-dependent vertex corrective V pose is tomodel different surface-level deformation mainly driven by3D poses. We additionally consider ID to model slightlydifferent pose-dependent vertex corrective for each ID. Toperform LBS, we first perform forward kinematics (FK)with J + Jid and provided 3D pose to get transfor-mation matrices of each joint. We denote 3D joint coordi-nates from FK by J. Then, we apply the transformationmatrices to V + V id + V pose with pre-defined skinningweights to get final posed 3D mesh V . Our template meshV consists of 16K vertices and 32K faces. All three typesof correctives are estimated in our pipeline.",
  ". The overall pipeline of the proposed UHM. The esti-mated correctives (dotted green box at the bottom) are applied to atemplate mesh to refine it. Then, LBS is used to pose the templatemesh": "coder are encoder and decoder of variational autoencoder(VAE) , respectively, responsible for learning priors ofthe ID space. IDEncoder extracts ID code zid R32 from apair of a depth map and 3D joint coordinates of each train-ing subject using the reparameterization trick . Then,from the ID code, IDDecoder outputs ID-dependent skele-ton corrective Jid and ID-dependent vertex correctives V id.IDEncoder always takes the same inputs for thesame subject during the training, and its inputs are preparedby rigidly aligning the 3D scan and 3D joint coordinatesof a neutral pose to a reference frame and rendering depthmaps from the aligned 3D scan. In this way, we can normal-ize pose and viewpoint, not related to the ID information,from the inputs of the IDEncoder. After the training, theIDEncoder is discarded as inputs of IDEncoder are not af-fordable for in-the-wild cases. Instead, we obtain ID codesfrom novel samples in testing time by fitting ID codes totarget data (Sec. 6.2 and 6.3).PoseEncoder and PoseDecoder. PoseEncoder outputs 6Drotation of joints from a pair of a single RGB im-age and 3D joint coordinates of arbitrary poses and identi-ties. Unlike IDEncoders inputs consist of a single pair ofeach subject, PoseEncoders inputs are from any poses andsubjects. PoseDecoder outputs pose-and-ID-dependent ver-tex correctives V pose from a pair of 6D rotational pose and ID code zid with MLPs. As how skin deforms canbe different for each person even with the same pose, ourPoseDecoder takes both pose and ID codes. Please notethat ID-dependent deformations in the zero pose are alreadycovered in IDDecoder, and the role of the additional IDcode input to PoseDecoder is to model only different pose-dependent deformations for each ID. Following STAR ,we estimate V pose in a sparse manner with the help oflearnable vertex weights . For the same reason as IDEn-coder, PoseEncoder is discarded after the training. In the",
  ". Simultaneous tracking and modeling": "We train UHM in an end-to-end manner from scratch withour simultaneous tracking and modeling pipeline. There aretwo types of loss functions that we minimize: data termsand regularizers. We describe our data terms below andplease refer to the supplementary material for the detaileddescriptions of the regularizers.Pose loss, point-to-point loss, and mask loss. The poseloss Lpose is a L1 distance between 3D joint coordinates Jand targets. It mainly provides information on kinematicdeformation. The point-to-point loss Lp2p is the closest L1distance 3D vertex coordinates V and 3D scans. The maskloss Lmask is a L1 distance between rendered and target fore-ground masks, where our masks are from a differentiablerenderer . Lp2p and Lmask mainly provides informationof non-rigid surface deformation. For both Lp2p and Lmask,we calculate the loss functions between two pairs. First,we use both correctives ( V id and V pose) to obtain Vand compute the loss functions. Second, we set V pose tozero to obtain V and compute the loss functions. The sec-ond one enables us to supervise the ID-dependent corrective V id without being affected by the pose-and-ID-dependentcorrective V pose, necessary to learn meaningful ID latentspace.Image matching loss. Solely using the above loss functionsdoes not encourage vertices to be semantically consistentacross all frames and subjects as both 3D scans and masksare unstructured surface data. For example, a certain vertex,supposed to be located on the thumbnail across all framesand subjects, could slide to a semantically wrong position.This is because the above loss functions do not encouragesuch semantic consistency. For semantic consistency, weadditionally compute an image matching loss, motivated byFirst, for each subject, we unwrap multi-view images ofa frame with the neutral pose to UV space, as shown in (a), which becomes a reference texture. For the un-wrapping, we use our 3D meshes, obtained from a check-point that is trained without the image matching loss. Af-ter the unwrapping, we have as many reference textures asthere are subjects. The reference textures are static assetsand do not change during the training. Then, we fine-tunethe checkpoint with additional Limg. (b) shows whatLimg does. We first rasterize mesh vertices and render im-ages using the reference texture ( (a)) in a differ-entiable way. Then, we compute optical flow from the ren-dered images to captured images using a pre-trained state-of-the-art optical flow estimation network . Finally, weminimize the L1 distance between 1) the 2D positions of therasterized mesh vertices and 2) the positions of the target",
  "(b)": ". (a) Reference texture. (b) Our image matching loss func-tion encourages rasterized vertices (orange) to move to the targetpositions (green), where the target position is obtained by the op-tical flow (white arrow). pixels, where the target pixels are the output of the opticalflow.Our image matching loss encourages each rasterizedmesh vertex to have consistent semantic meanings from thatof the reference texture, which results in low variance. Italso results in low bias as the reference texture is from theneutral pose, which has a minimum skin sliding. Pleasenote that the gradient is only backpropagated to the raster-ized mesh vertices. The rendered images are not perfectlyidentical to captured images as such rendered images do nothave pose-and-view-dependent texture changes and shadowchanges. However, we observed that optical flow is highlyrobust to such changes in textures, which gives reasonablematching between rendered and captured images.",
  ". Adaptation to a phone scan": "After training our UHM following Sec. 4, we adapt it to ashort (usually around 15 seconds) phone scan for the au-thentic hand avatar. The phone scan includes a single per-sons hand with the neutral pose and varying global rota-tions to expose most of the surface of the hand. During theadaptation, we freeze pre-trained UHM while optimizing itsinputs.",
  ". Preprocessing": "We use a single iPhone 12 to scan a hand, which incorpo-rates a depth sensor that can be used to extract better geom-etry of the users hand. Then, we use a 2D hand keypointdetector (our in-house detector or public Mediapipe ) toobtain 2D hand joint coordinates and RVM to obtainforeground masks. Also, we use InterWild to obtainMANO parameters of all frames.",
  ". The overall pipeline to remove the shadow from thephone scan using our ShadowNet": "to the phone scan. The 3D pose, 3D global rotation, and3D global translation are per-frame parameters, and the IDcode is a single parameter and shared across all frames aseach phone scan is from a single person. For the fitting, weminimize loss functions against 2D hand joint coordinates,foreground mask, a depth map, and 3D joint coordinatesfrom the MANO parameters, where the fitting targets arefrom Sec. 5.1. Please refer to the supplementary materialfor a detailed description of the fitting.",
  ". Effectiveness of ourShadowNet in a novel lightcondition": "To produce albedo tex-tures, we need to removeshadows from our phonescan. shows thatwithoutremovingshad-ows, the shadow of thephone capture is baked intothe texture, which makessignificant artifacts in a novel light condition.Withoutknowing the full 3D environment map of the phone scan,it is impossible to perfectly disentangle shadow from theunwrapped texture. Previous work assumes a singlepoint light and optimizes it during the adaptation. However,in most cases, the assumption does not hold as there are of-ten more than one light source in our daily life. Instead ofusing such a physics-based approach, we use a statistical ap-proach by introducing our ShadowNet. As shown in ,our intuition is modeling shadow as a darkness differencebetween a color-calibrated image and a captured image.ShadowNet. Our ShadowNet estimates shadow map in theUV space from tiled 3D global rotation, 3D pose , IDcode zid, and view direction for each mesh vertex. Givena fixed 3D environment during the phone scan, the inputsof our ShadowNet can determine the shadow of the hand.The ShadowNet is a fully convolutional network with sev-eral upsampling layers. To encourage smooth shadow, we",
  ".Animated hand avatars whose textures are from (a)phone scan, and geometry is from UHM by passing novel 3Dposes and personalized ID code zid to it": "perform bilinear upsampling four times at the end of the net-work. We add a learnable positional encoding to the inputbefore passing it to our ShadowNet as each texel in the UVspace has its own semantic meaning. We apply a sigmoidactivation function at the end of our ShadowNet. By render-ing and multiplying our shadow map to an image, we canmake the image darker, which can be seen as a shadow cast-ing, similar spirit of Bagautdinov et al. . shows thequalitative results of our ShadowNet. We randomly initial-ize our ShadowNet and train to our phone scan. Please referto the supplementary material for the detailed architecture.Optimization. First, we obtain the color-calibrated image,rendered from a UV texture that has the same color for alltexels. The RGB values (3D vector) of texels are optimiz-able. Our assumption for the shadow removal is that handsmostly have uniform skin color, unlike the human body withdifferent colors in upper and lower body clothes. Pleasenote that we use the color-calibrated image only for remov-ing shadow, and our final hand avatar has authentic infor-mation from any colors.Then, we multiply the rendered shadow to the color-calibrated image.We minimize L1 distance and VGG",
  ".P2S error (mm) comparison on DHMdataset": "loss between two pairs at the same time: between 1)color-calibrated image and captured image and 2) color-calibrated image with shadow and captured image. In thisway, we can optimize ShadowNet to produce the 1-channeldifference between the captured image and color-calibratedimage following the image intrinsic decomposition formula.Without proper regularizers, our ShadowNet can considerall 1-channel differences as a shadow, which is not desir-able for hair and black tattoos. Hence, we apply a total vari-ation regularizer to the rendered shadow to model shadowas a locally smooth darkness changes without locally sharpones.",
  ". Texture optimization": "Given estimated 3D meshes from Sec. 5.2 and shadow fromSec. 5.3, we first divide captured images by the shadow andunwrap them to UV space. Then, we average them con-sidering the visibility of each texel. We preprocess the un-wrapped texture with the OpenCV inpainting function to fillmissed texels. To further optimize the unwrapped texture,we render an image from the unwrapped texture and multi-ply the rendered shadow to it. Then, we minimize L1 dis-tance and VGG loss between the rendered image andcaptured images for a more photorealistic texture. We addi-tionally encourage locally smooth textures for missing tex-els, inpainted by OpenCV. During the texture optimization,we fine-tune our ShadowNet to make the shadow consistentwith our texture.",
  ". Final outputs": "The final outputs of our hand avatar creation pipeline are1) optimized ID code of UHM zid from Sec. 5.2 and 2)optimized texture from Sec. 5.4. The geometry ID codegives a personalized 3D hand shape and skeleton, and theoptimized texture provides personalized albedo texture. By feeding 3D poses from off-the-shelf 3D hand pose estima-tors with the optimized ID codeto pre-trained UHM, entire mesh vertices can be animatedfrom the novel poses. Also, simply using the standard com-puter graphics pipeline, authentic 3D hand avatars can berendered with the personalized albedo texture, as shown in, or with Phong reflection model, as shown in (b). Our pipeline takes 2 hours for 15 seconds of phonescan, while HARP takes 6 hours.",
  ". Datasets": "We use the three datasets below to train and evaluate ourUHM.Our studio dataset. We use 177 captures for the trainingand 7 captures for the testing, where each capture includes18K frames of a unique subject taken from 170 cameras onaverage. The testing subjects are not included in the train-ing set. Please refer to the supplementary material for thedetailed descriptions of our dataset.Testing set of MANO. We report 3D errors on the testingset of MANO, which consists of 50 3D scans from 6 sub-jects. It is used only for the evaluation purpose.Dataset of DHM. We report 3D errors on the dataset ofDHM, which consists of 33K 3D scans from a single sub-ject. We use this dataset only for the evaluation purpose.We also use the two datasets below to evaluate the adap-tation pipeline.Our new phone scan dataset.We newly captured 18phone scans from unique IDs and use them to evaluate ouradaptation pipeline. We use 4 scans out of 18 scans for thequantitative evaluations. For the training, frames with neu-tral poses are used, and for the testing, frames with diverseposes are used. All the phone scans are preprocessed fol-",
  ". Comparison of 3D hand avatars on the test set of HARPdataset": "lowing Sec. 5.1. Some phone scans have distinctive authen-ticities, such as fingernail polish and tattoos. Please refer tothe supplementary material for the detailed descriptions ofour dataset.Dataset of HARP. We report errors in the publicly availableHARP dataset. Please note that they only released a partialof what they used in paper, and the released one consists ofa single ID. For the quantitative results, we used subject 1sequence as all other sequences do not have enough posediversity, which cannot be used for the testing. Among 9sub-sequences of subject 1, 1 to 5 are used for the training,and 6 to 9 are used for the testing.",
  ". Comparison of 3D hand models": "We compare the generalizability of pre-trained 3D handmodels to unseen IDs and poses. To this end, we fit inputs of3D hand models (i.e., pose and ID code) to target data whilefixing the pre-trained 3D hand models. After fitting them totarget data, we measure point-to-surface (P2S) error (mm),which measures the average distance from points of the 3Dscan to the surfaces of the output meshes. The errors aremeasured after fitting inputs of 3D hand models as much aspossible to target data while fixing the models. In this way,we can check how much fidelity (i.e., surface expressive-ness) of each hand model is not enough to fully replicate3D scans after marginalizing fitting errors. For UHM, weexcluded vertices on the forearm when calculating the erroras all others do not have the forearm. We do not includepersonalized 3D hand models in the compar-isons as our focus in this experiment is to compare general- izability to unseen poses and IDs, while such personalizedmodels cannot generalize to novel IDs. and show that our UHM produces the bestquality of meshes on multiple test sets than other univer-sal hand models, such as MANO , NIMBLE , andHandy . Handy suffers from surface artifacts. Forexample, there are severe artifacts around the knuckle areain the examples at the top three rows and the first column.Also, there is no muscle bulging around the thumb in the ex-ample at the bottom and the first column. There is a severeartifact at the pinky finger in the example in the third rowand the second column. We additionally provide our resultsfrom a low-resolution template, which has half the numberof vertices (3K) than NIMBLE (6K) and Handy (7K) fora more fair comparison. The table demonstrates that evenwith a half number of vertices, ours achieves better fidelitythan NIMBLE and Handy. shows that ours achievesmuch better results on the DHM dataset than LISA .",
  ". Comparison of adaptation pipelines": "and11 show that our adaptation pipelineachieves much more authentic and photorealistic resultsthan HARP and Handy . In particular, the rightcolumn of shows that only our avatar has skinbulging around the thumb and sharp knuckle, unseen dur-ing the training, thanks to our high-fidelity UHM. HARPsuffers from geometry artifacts, which result in texture ar-tifacts. We think this is because of the limited expressive-ness of the MANO model. In addition, due to their singlepoint light assumption, they have a clearly different shadowfrom the captured images, as the second row examples of show. We address such a failure case by introduc-ing the ShadowNet. Handy suffers from a lack of textureauthenticity, such as different fingernail polish, tattoos, andpalm wrinkles, as their textures are from pre-defined texture",
  ". Effectiveness of our image matching loss function": "space. On the other hand, we unwrap textures and directlyoptimize them without being constrained in texture space,which gives authentic textures. Unlike geometry, there canbe numerous variants in the texture space including shadow,tattoo, and fingernail polish; hence, we think such textureprior is not enough for the authenticity.Tab. 3 and 4 show that our adaptation pipeline achievesbetter numbers. For a fair comparison, all avatars in Tab. 3are trained with the additional depth map loss as our datasetprovides depth maps. For four subjects in our phone scan,we co-captured studio data, which gives 3D data of them.To measure the accuracy of the adaptation pipeline morethoroughly, we measure the P2S error (mm) between per-sonalized meshes from the phone scan and the 3D scan fromour capture studio. Thanks to our high-fidelity universalmodeling, the proposed UHM clearly achieves the best re-sult in the 3D metric.For the results on the testing set, following the previ-ous protocols that optimizes 3D poses of hands, lights,and ambient ratio on the testing set, we fine-tune PoseNetand ShadowNet on the test set. All remaining parameters,including the ID code and optimized texture, are fixed inthe testing stage following HARP . For the results ofHARP, we used their official code with groundtruth handboxes. For the results of Handy, we downloaded their of-ficial pre-trained weights and optimized 3D pose and tex-ture latent code using L1 distance and LPIPS follow-ing their paper. Please refer to the supplementary materialfor the detailed fitting process of Handy.",
  ". Ablation study": "Image matching loss. To validate the effectiveness of ourimage matching loss Limg during the tracking and mod-eling, depicted in , we first unwrap multi-view im-ages to UV space using our 3D meshes. Then, we com-pute optical flow from the reference texture of the neu-tral pose ( (b)) to the unwrapped per-frame texture.",
  ". Comparison of rendered images 1) only using albedoand 2) using both albedo and shadow": "(a) shows that using our image matching loss Limgdecreases the L2 norm of the optical flow for most texels,which shows that texels are located in semantically correctand consistent positions by suffering less from the skin slid-ing. In particular, texels that have semantically distinctivelocations, such as wrinkles on the palm and thumbnail, havesignificantly less L2 norm of the optical flow as the opticalflow provides meaningful correspondences for such texels. (c) and (d) show that compared to (b), us-ing our image matching loss produces consistent and correctposition of thumb in the UV space. On the other hand, asthe back of the hand usually does not have distinctive tex-tures, optical flow fails to produce meaningful correspon-dence, which results in a slightly higher L2 norm.ShadowNet. shows that the albedo rendering ofHARP still has a shadow, while ours does not. This showsthe benefit of using our ShadowNet to remove the shadowfrom phone scans instead of assuming a single point lightand optimizing it like HARP. In addition, our albedo hasmore detailed textures, such as hair on the back of the hand(first row). Due to the ambiguity of the images intrinsic de-composition, we could not include quantitative evaluations.",
  ". Conclusion": "We present UHM, a universal hand model that 1) can rep-resent high-fidelity 3D hand mesh of arbitrary IDs and di-verse poses and 2) can be adapted to each person with ashort phone scan for the authentic 3D hand avatar. UHMperforms the tracking and modeling at the same time toaddress the error accumulation problem from the trackingstage. In addition, we newly introduce the image matchingloss function to prevent skin sliding during the tracking andmodeling. Finally, our adaptation pipeline achieves a highlyauthentic hand avatar by utilizing useful learned priors ofUHM.",
  "Supplementary Material forAuthentic Hand Avatar from a Phone Scanvia Universal Hand Model": "In this supplementary material, we provide more exper-iments, discussions, and other details that could not be in-cluded in the main text due to the lack of pages. The con-tents are summarized below:A. Sec. A: More qualitative resultsB. Sec. B: More ablation studiesC. Sec. C: UHM architectures and loss functionsD. Sec. D: Details of adaptation to a phone scanE. Sec. E: Our datasetsF. Sec. F: Experiment detailsG. Sec. G: Failure cases",
  "A.1. ID code interpolation": "Fig. A, B, and C show that our UHM produces smoothlychanging 3D meshes from the linearly interpolated IDcodes, where the two ID codes are from the unseen test set.Our 3D meshes from the interpolated ID codes have a nat-ural and realistic surface. On the other hand, Fig. B and Cshow that Handy fails to disentangle 3D pose and ID.As each row of all figures is from the same pose but fromdifferent ID codes, only ID-related information (i.e., thick-ness) should change while preserving the 3D pose. How-ever, Fig. B and C show that only changing the ID code ofHandy produces 3D meshes with different 3D poses. Thisis evident in Fig. C as the rightmost result of Handy has atotally different 3D pose from the leftmost one.",
  "A.5. 3D hand avatars": "Fig. G shows that our 3D hand avatar achieves sharper tex-tures than HARP .Handy fails to produce au-thentic results, consistent with and 11 of the mainmanuscript. Fig. H additionally shows our adapted 3D handavatar, rendered with Phong reflection model and environ-ment maps, as in (b) of the main manuscript. Tothis end, given an environment map, we first do preconvo-lution to map the illumination in the environment map todiffuse and specular lighting representation similar to .Then, the final texture is obtained by combining the diffuseand specular representation with our adapted texture (opti-mized texture of Sec. 5.4 of the main manuscript) accordingto the normal map from 3D mesh and view direction. The3D poses of (b) are from the tracked results from a differ-ent subject of our studio data, which shows that our handavatars can be driven with novel poses. The results are notphotorealistic due to the limitation of the Phong reflectionmodel, but they show the potential of our hand avatar, whichcan be combined with future relightable hand models .",
  "B.2. Effectiveness of the TV regularizer during theadaptation": "Fig. J shows the effectiveness of the total variation (TV)regularizer to our ShadowNet. Without the TV regularizer,ShadowNet tried to consider all darkness differences be-tween 1) albedo+shadow and 2) captured images as shad-ows. As a result, local sharp textures, including wrinkles areconsidered shadows. As described in Sec. 5.3 of the mainmanuscript, by applying the TV regularizer to the Shad-owNet, we can prevent such undesired shadows.",
  "B.3. Extension of DHM to the universal case vs.UHM": "Fig. K shows that when performing the tracking and mod-eling at the same time, special considerations are necessaryfor universal hand modeling. We choose DHM , a high-fidelity personalized 3D hand model, as a comparison targetbecause it has a similar training pipeline that performs thetracking and modeling at the same time as ours. One crit-ical difference between DHM and our UHM is that DHMis a personalized 3D hand model, which does not learn theID space and cannot generalize to novel IDs. For systemsthat perform the tracking and modeling at the same time,one major difficulty of universal hand modeling is disen-tangling ID and pose information as all supervision targets,",
  "Handy": "Figure C. Comparison of 3D meshes from linearly interpolated ID codes. The leftmost and rightmost 3D scans show examples of the IDcodes 1 and 2. For each row, 3D meshes have the zero pose (i.e., 3D pose of the template space), and only ID code changes by a linearinterpolation. such as 3D joint coordinates, 3D scans, masks, and images,are entangled representations of ID and pose. We effec-tively achieve the disentanglement by calculating loss func-tions using two types of 3D meshes: one from both cor-rectives and the other only from the ID-dependent correc-tives, as described in Sec. 4 of the main manuscript. Duringthe training, the ID-dependent correctives of all frames thatbelong to the same ID are from the same inputs (i.e., 3Djoint coordinates and depth maps of the neutral pose, as de-scribed in IDEncoder and IDDecoder. of Sec. 3.2 of themain manuscript). Therefore, supervising 3D meshes thatare only from the ID-dependent correctives can make IDDe-coder formulate meaningful ID space (Fig. K (b)) withoutbeing affected by the pose-and-ID-dependent correctives,which naturally achieves the disentanglement of the ID andpose. On the other hand, without the supervision of the 3Dmeshes that are only from the ID-dependent correctives likeDHM, the model cannot disentangle ID and pose, whichresults in meaningless ID space (Fig. K (a)). Such disentan-glement is especially challenging for systems that performthe tracking and modeling at the same time because pre-vious separate pipeline can perform trackingfor each ID, which can naturally provide assets that onlyhave ID information without pose by canceling pose fromthe tracked meshes.",
  "C.1. Network architectures": "We describe detailed network architectures of UHM, brieflydescribed in Sec. 3.2 of the main manuscript.IDEncoder. IDEncoder outputs ID code zid R32 from apair of a depth map and 3D joint coordinates of each train-ing subject. To prepare the inputs of the IDEncoder, wefirst select a single pair of a 3D scan and 3D joint coordi-nates for each subject. Hence, there are subjects number of(3D scan and 3D joint) pairs. As the IDEncoder should cap-ture only ID-related information, the poses of the inputs ofthe IDEncoder should be normalized. To this end, we takethe pairs from the first frame of the captures as the posesat the first frames are close to zero poses, which we callneutral poses. Then, we rigidly align the selected 3D scansand 3D joint coordinates to a reference coordinate systemand render depth maps from the aligned 3D scans from thefront and back views. In this way, we can further normalizeviews, which exist and are hard to be normalized in images.ResNet-18 takes two-view depth maps of neutralpose for each subject. Please note that IDEncoder alwaystakes the same inputs for the same subject during the train-ing. Hence, the size of the mini-batch is 2Ns, where Ns isthe number of unique subjects in the mini-batch of PoseEn-coder.The ResNet-18 is initialized with ImageNet",
  "Figure D. 3D meshes from randomly sampled ID codes from the Gaussian distribution with zero 3D poses": "classification, and we discard fully connected layers. Theoutput of ResNet-18 is a 512-dimensional feature vector.We reshape the feature vector to a 1024-dimensional one,which represents a multi-view feature for each subject. Themulti-view feature is concatenated with the 3D joint coor-dinate of a neutral pose and passed to two fully connectedlayers, which produce the id code zid using the reparame-terization trick . The two fully connected layers consistof 512 hidden units and an intermediate ReLU activationfunction. IDDecoder. IDDecoder takes the ID code zid and outputsID-dependent skeleton correctives Jid and ID-dependentvertex correctives V id. The IDDecoder consists of twofully connected layers with a ReLU activation function forthe non-linearity. The hidden size of the fully connectedlayers is set to 512. Jid should not replicate any changes,which can be replicated by 3D joint rotations.In other words, 3D hands should be in the zero pose after applying Jid to the template mesh. Hence, except for child jointsof the wrist, we enable only 1 degree of freedom (DoF) of Jid to restrict it to only affect the lengths of fingers. Inthis way, the learned ID space is not mixed with the pose. PoseEncoder.PoseEncoder outputs 6D rotation ofjoints from a pair of a single RGB image and 3D jointcoordinates of arbitrary poses and identities. The 3D globalrotation and translation are obtained by rigidly aligningwrist and four finger root joints (except the thumb root joint)to the target 3D joint coordinates. Unlike IDEncoders in-puts consist of a single pair of each subject, PoseEncodersinputs are from any poses and subjects. Our PoseEncoderhas a similar network architecture as Pose2Pose . TheResNet-50 of Pose2Pose is initialized with ImageNet classification, and the remaining parts are randomly initial-ized.",
  "Figure F. Comparison of the low-resolution UHM and previous3D hand models on our test set": "PoseDecoder.PoseDecoderoutputspose-and-ID-dependent vertex corrective V pose in a sparse way usinglocal joint clusters for better generalization to unseen posesfollowing STAR .To this end, we make J numberof local joint clusters, where each cluster consists of 6Drotations of a joint, its parent joint, and a child joint. Jdenotes the number of joints. We additionally concatenatethe ID code for each cluster. Hence, each local joint clusterhas the dimension of R18+32, where 18 and 32 representthree 6D rotations and the dimension of the ID code,respectively. Please note that we pass 6D rotations aftermasking invalid DoFs and root rotation to zero. Then, thelocal joint clusters are passed to two separable convolutionswith an intermediate ReLU activation function for a non-linearity. The hidden size of the separable convolution isset to 256. The output of the separable convolutions of eachlocal joint cluster has the dimension of RV 3. V denotesthe number of vertices of our template mesh. Formally, wedenote the above process by Fj = f(j, p(j), c(j), zid),where Fj denotes the output of the separable convolutionof jth local joint cluster.p(j) and c(j) denote parentchild joint of jth joint, respectively. The final pose-and-",
  "ID-dependent vertex corrective V pose is obtained by V pose =": "j j(f(j, p(j), c(j), zid) f(0, 0, 0, zid)). RV J is a mask, which introduces sparsity.It isinitialized with a geodesic distance between the vth vertexand jth joint in the template mesh. We subtract the outputof the separable convolution from the zero pose to preventpose-and-ID-dependent vertex corrective V pose fromreplicating only ID-dependent geometry, which should bereplicated by ID-dependent vertex corrective V id.",
  "Our UHM is trained in an end-to-end manner by minimizingL, defined as below:": "L =Lpose + 10Lp2p + 0.1Lmask + 0.1Limg+ 0.01L + 0.001Lzid + 1000L V id + 10L V pose+ 75000Llap + 0.001L + 0.1Lvol + 0.1Lcut,(1)where Lpose, Lp2p, Lmask, and Limg are described in theSec. 4 of the main manuscript. The remaining loss func-tions are regularizers, described below.First, we minimize L, a squared L2 norm of af-ter converting it to an axis-angle representation, to pre-vent extreme rotations. Second, we minimize Lzid, a KLdivergence between zid and the normal Gaussian distribu-tion. In this way, we can make the ID latent space followthe Gaussian distribution, necessary for sampling novel IDfrom a known (e.g., Gaussian) distribution. Third, we mini-mize L V id and L V pose, a squared L2 norm of the tangen-tial component of V id and V pose, respectively. Theyprevent the vertex correctives from overwhelming the ID-dependent skeleton corrective Jid. To be more specific,we encourage the finger lengths to be adjusted mainly by Jid, not by the vertex correctives. Fourth, we minimizeLlap, the Laplacian regularizer for smooth surface.Like",
  "Figure H. Our adapted 3D hand avatar with the Phong reflectionmodel and environment maps": "Lp2p and Lmask, we compute two types of this regularizerfrom 1) both correctives and 2) only ID-dependent correc-tive to learn meaningful ID space. Fifth, we minimize L,a L1 norm of ReLU(), following STAR . In this way,we can encourage sparsity of the V pose, beneficial for thegeneralizability to unseen 3D poses. Sixth, we minimizeLvol, a volume-preserving regularizer. It first pre-calculatesthe radius of spheres for each finger in the zero pose spaceonly with V id without V pose. Then, Lvol is the differ-ence between 1) the distance from vertices to sphere radiusand 2) the radius of the sphere if the distance shorter than",
  "Figure I. The effectiveness of the texture optimization during ourphone adaptation": "the radius. It encourages our UHM to preserve the mini-mal volume of each finger, where the minimal values arecalculated in the zero pose space with V id. Finally, weminimize Lcut for a flat cut around the forearm. To this end,we make a virtual vertex at the center of the cut and makevirtual triangles using the virtual vertex and pairs of twoconnected vertices at the cut. Lcut is a L1 distance betweendot products of all those virtual triangles. In this way, wecan encourage all vertices at the cut to be on the same plane,which results in a flat cut.",
  "D.1. Geometry fitting": "For the geometry fitting, we designed PoseNet, which has asimilar network architecture to Pose2Pose with minormodifications. The PoseNet outputs 3D global rotation, 3Dpose , pose code, and 3D global translation of UHM froman image, a depth map, a mask, and 2D joint coordinates,where the inputs are obtained from Sec. 5.1 of the mainmanuscript. The pose code is a latent vector of a pre-trainedVAE, which embeds plausible hand pose space similar toV-Poser . The VAE is pre-trained on our capture studiodataset and fixed during the adaptation stage.We randomly initialize PoseNet before the training. ThePoseNet is used for the pose tracking, a similar spirit ofneural annotators . In addition to the outputs of the net-work, we optimize ID code zid, shared across all frames asall frames are from the same person. With the outputs ofthe network and the ID code, we obtain 3D mesh using pre-trained decoders of UHM, used to unwrap images to the UVspace.The PoseNet is trained in a self-supervised way by beingtrained with the inputs of the network (i.e., 2D joint coor-dinates, a depth map, and a mask). During the training, we fixed pre-trained decoders of UHM. For the kinematic-level personalization (e.g., bone lengths), we minimize L1distance between projected 2D hand joint coordinates andthe target. Also, for the surface-level personalization (e.g.,thickness of hand surface), we minimize L1 distance be-tween the rendered mask and the target. We additionallyminimize the L1 distance between the rendered depth mapand the target to address the depth and scale ambiguity. Fi-nally, we minimize the L1 distance between 3D joint coor-dinates from the pose code and MANO parameters, wherethe MANO parameters are from an off-the-shelf regres-sor . In this way, we can address the depth ambiguity ofthe 2D keypoints. Then, 3D joint angles and 3D mesh fromthe pose code are used to supervise those from the 3D pose.",
  "D.2. ShadowNet": "We first tile 3D global rotation, 3D pose , and ID codezid to all texels in the UV space. In other words, all texelshave the same concatenated 3D global rotation, 3D pose,and ID code. Then, we compute the dot product between 1)the normal vector of each vertex and 2) a vector from thecamera to each vertex, which becomes a viewpoint featurefor each vertex. We warp the per-vertex viewpoint featureto the UV space and concatenate it with the prepared tiledtexels, which become the input of our ShadowNet. Given afixed environment during the phone scan, all inputs of ourShadowNet can determine casted shadow. To distinguisheach texel with its own semantic meaning, we add a learn-able positional encoding to each texel and pass it to Shad-owNet. To enlarge the size of the receptive field effectively,we start from 32 downsampled UV space compared to thatof our UV textures.The ShadowNet first converts the input to a 256-dimensional feature map with a convolutional layer. Then,for each resolution, we apply one convolutional layer, fol-lowed by group normalization and SiLU activationfunction . We used the nearest neighbor for the upsam-pling. After three times of upsampling, we apply bilinearupsampling four times and the sigmoid activation functionto normalize the values of the shadow from 0 to 1.",
  "E.1. Studio dataset": "Our capture studio has 170 calibrated and synchronizedcameras. All cameras lie on the front, side, and top hemi-spheres of the hand and are placed at a distance of aboutone meter from it. Images are captured with 4096 2668pixels at 30 frames per second. We pre-processed the rawvideo data by performing 2D keypoint detection and",
  "Figure L. Examples of poses of the training set of our studio dataset": "3D scan . The keypoint detector is trained on our held-out human annotation dataset, which includes 900K imageswith 3D rotation center coordinates of hand joints, whereour manual annotation tool is similar to that of Moon etal. . The predicted 2D keypoints of each view were tri-angulated with RANSAC to robustly obtain the groundtruth(GT) 3D hand joint coordinates. The combination of 2Dkeypoint detector and triangulation, used to obtain GT3D hand joint coordinates, achieves a 1.71 mm error onour held-out human-annotated test set, which is quite low.Fig. L and M show pose examples of the training and test-ing sets of our studio dataset, respectively.",
  "E.2. Phone scan dataset": "Fig. N shows examples of our phone scan dataset. The train-ing set mainly consists of simple poses, where the 3D globalrotation of the hand mainly changes, and the 3D pose and3D translation of the hand remain almost static. The testingset consists of diverse poses, such as a fist and thumb-up.",
  "F.1. Fitting for Sec. 6.2": "For the comparisons in and Tab. 1 of the mainmanuscript, we used 3D joint coordinates and 3D scans astarget data for the fitting, the most typical setting of thetracking. For the fitting, we minimized 1) L1 distance be-tween output and target 3D joint coordinates, 2) the P2Sdistance from 3D scans, and 3) L2 regularizers to the pa-rameters. The L2 regularizer is introduced to prevent ex-treme meshes. Each loss term is weighted by 1, 10, and0.001. As each 3D hand model has slightly different 3Djoint locations despite the same semantic meaning, we donot report 3D joint error following . For thesame reason, we turned off the 3D joint loss during the fit-ting after enough iterations.For the comparison in Tab. 2 of the main manuscript, wefollowed the evaluation protocol of LISA . Specifically,we pre-define various numbers of available viewpoints and",
  "Figure N. Examples of our phone scan dataset": "fit 3D pose and ID code to 2D joint coordinates of thoseviewpoints. Due to the depth ambiguity from 2D supervi-sions from a few viewpoints, we used the pose prior, usedin our adaptation stage of Sec. D.1, as LISA also used ge-ometry prior from large-scale data. As their codes are notpublicly available, we brought their numbers from their pa-per.",
  "F.2. Handy fitting for Sec. 6.3": "We use the same PoseNet and loss functions of ours, de-scribed in Sec. D, except for one thing: we used VGGloss function on the rendered image, while we usedLPIPS on the rendered image for the Handy texture fit-ting following their paper. The latent code of the Handystexture is shared across all frames and is optimizable.",
  "F.3. P2S calculation of Tab. 3": "We calculated the P2S errors of the adapted 3D hand avatarin Tab. 3 of the main manuscript. To this end, we first se-lected a frame with the neutral pose of studio capture ofthe four subjects, where the four subjects have co-capturedstudio and phone scan data.From the studio data, wehave 3D joint coordinates and 3D scan of the neutral pose.Then, we optimize 3D pose and translation of each adaptedavatars by minimizing L1 3D joint distance and point-to-point loss function from the studio data, described in Sec. 4of the main manuscript. During the optimization, we fix ID-related information, such as ID code zid of ours. The P2Serrors are calculated between the optimized meshes of eachavatar and 3D scan from our studio data. Fig. O visualizesthe optimized mesh and 3D scan. For UHM and HARP, weexcluded the vertices on the forearm when calculating the3D errors as they are too unconstrained.",
  "F.4. HARP dataset": "As the HARP dataset does not provide depth maps, we donot use the depth map loss function in our pipeline. Weused Mediapipe to obtain 2D hand joint coordinatesand used RVM to obtain foreground masks. All re-maining things are the same as what is described in Sec. Dfor the experiments on the HARP dataset. Ours, HARP, andHandy are equally fitted to the same sequences and are eval-uated with the same metrics.",
  "G. Failure cases": "Geometry fitting.We found that our geometry fittingpipeline (Sec. 5.2 of the main manuscript) sometimes suf-fers from a surface-level misalignment. In the geometryfitting stage, dense supervisions, such as DensePose of the 3D human body, are not available.Such a lackof dense supervision makes our 3D geometry suffer fromsurface-level misalignment despite the accurate keypoint-",
  "Figure Q. Our optimized texture after removing shadow with theShadowNet. The highlighted area has an evident artifact": "level alignment. Although the image loss during the textureoptimization (Sec. 5.2 of the main manuscript) provides thedense supervision, its initial texture is from the geometryfitting (Sec. 5.2 of the main manuscript), which can sufferfrom the surface-level misalignment.Texture unwrapping. Fig. P shows a failure case happensin the texture unwrapping. There is an evident vertical arti-fact along the left part of the figure. The reason for such ar-tifacts is that during the phone capture, the subject exposesthe left and right parts of the vertical line with very differentposes at different time steps. Hence, pose-dependent skincolor changes and view-dependent shading of those left andright parts become very different, which results in differentcolors and an evident vertical line between the left and rightparts. We tried to smooth such a region; however, it was notenough as the color difference is too big.ShadowNet. Fig. Q shows a failure case of our ShadowNet.Although most of the shadow is removed, the highlightedarea still has a small amount of shadow. The remainingshadow is especially evident as the skin color of this subjectis bright. We think the reason for the remaining shadow isthe regularizers to the ShadowNet to prevent it from con-sidering black tattoos as shadows. Also, its capability is not guaranteed for smooth black tattoos and black fingernailpolish. Due to the ambiguity of the intrinsic decomposition,it might perform badly in low-light conditions; we think thislimitation applies to all current methods.",
  "Diederik P. Kingma and Max Welling. Auto-encoding vari-ational bayes. In ICLR, 2014. 3, 12": "Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero,Andrew Cunningham, Alejandro Acosta, Andrew Aitken,Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative ad-versarial network. In CVPR, 2017. 6, 17 Yuwei Li, Longwen Zhang, Zesong Qiu, Yingwenqi Jiang,Nianyi Li, Yuexin Ma, Yuyao Zhang, Lan Xu, and Jingyi Yu.NIMBLE: a non-rigid hand model with bones and muscles.ACM TOG, 2022. 1, 2, 3, 6, 7, 11, 13, 16",
  "Ahmed AA Osman, Timo Bolkart, and Michael J Black.STAR: Sparse trained articulated human body regressor. InECCV, 2020. 3, 13, 14": "Rohit Pandey, Sergio Orts Escolano, Chloe Legendre, Chris-tian Haene, Sofien Bouaziz, Christoph Rhemann, Paul De-bevec, and Sean Fanello. Total Relighting: learning to relightportraits for background replacement. ACM TOG, 2021. 9 Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, andMichael J Black. Expressive body capture: 3D hands, face,and body from a single image. In CVPR, 2019. 15 RolandosAlexandrosPotamias,StylianosPloumpis,Stylianos Moschoglou, Vasileios Triantafyllou, and StefanosZafeiriou. Handy: Towards a high fidelity 3D hand shapeand appearance model. In CVPR, 2023. 1, 2, 3, 6, 7, 9, 11,13"
}