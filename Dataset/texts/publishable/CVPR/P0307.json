{
  "Abstract": "Meta-learning methods typically learn tasks under theassumption that all tasks are equally important. However,this assumption is often not valid. In real-world applications,tasks can vary both in their importance during different train-ing stages and in whether they contain noisy labeled dataor not, making a uniform approach suboptimal. To addressthese issues, we propose the Data-Efficient and Robust TaskSelection (DERTS) algorithm, which can be incorporatedinto both gradient and metric-based meta-learning algo-rithms. DERTS selects weighted subsets of tasks from taskpools by minimizing the approximation error of the full gra-dient of task pools in the meta-training stage. The selectedtasks are efficient for rapid training and robust towards noisylabel scenarios. Unlike existing algorithms, DERTS doesnot require any architecture modification for training andcan handle noisy label data in both the support and querysets. Analysis of DERTS shows that the algorithm followssimilar training dynamics as learning on the full task pools.Experiments show that DERTS outperforms existing sam-pling strategies for meta-learning on both gradient-basedand metric-based meta-learning algorithms in limited databudget and noisy task settings. 1. IntroductionMeta-learning methods have been extensively studied andapplied in computer vision, natural language processing, androbotics . The key idea of meta-learning is to mimic thefew-shot situations faced at test time by randomly samplingclasses in meta-training data to construct tasks for episodictraining. Most existing meta-learning methods randomlysample meta-training tasks with a uniform probability . The assumption behind such uniform sampling is that alltasks are contribute equally. In real-world scenarios, thingsoften differ. Considering that the importance of tasks canchange during the training process, the diversity of tasks, andthe probability of mislabeled data within tasks, some tasksmight be more informative for the meta-training process than others. In this paper, we specifically focus on two scenarios:a limited data budget and a noisy task setting.First, making the meta-training process more efficient isan essential issue, particularly in scenarios with a limitedbudget of data (e.g., the number of tasks and data learned inthe meta-training process). Current meta-learning algorithmsrequire many tasks for meta-training episodes, which maycontain redundant information.In this work, we raise the natural concern that not all thetasks are created equal. A coarse-grained task that containsthe classification of Dog\" and Laptop\" is much easierto learn for the meta-model than the fine-grained task thatincludes a more difficult classification (e.g., Dog\" or Cat\"). In meta-training episodes, selecting the most benignlyinformative subset of tasks could benefit this process bydecreasing the computational load.Second, most meta-learning methods presuppose that thefew support set training samples were chosen correctly torepresent their class . Unfortunately, such assur-ances are not usually provided in real-world scenarios. Inreality, mislabeled samples are frequently present in evenhighly annotated and maintained datasets asa result of automated weakly supervised annotation, ambi-guity, or even human error. Suppose some tasks with noisylabels are fed to the meta-training process. In that case, thenoisy-labeled data in the support set will hinder the adapta-tion step of the meta-training process, which can result ininvalid adaptation and further make the base model incorrecton query evaluation. Furthermore, if noisy data is also in thequery set, the misleading gradient would be updated for themeta-model, which hinders the generalization capacity ofthe meta-model during the meta-testing stage. Therefore, de-veloping methods to avoid training meta-models on heavilylabel-corrupted tasks is essential for the setting with noisydata as well.The two aspects mentioned above still need to be thor-oughly studied: (i) For limited computational and data bud-gets, existing approaches construct information-theoretic cri-teria for evaluating and selecting task or buildingextra module for learning the sampling probability with",
  "arXiv:2405.07083v1 [cs.LG] 11 May 2024": "high computation cost. (ii) Previous work that considersmeta-learning with noisy data make the assump-tion that noisy data only exist in the support set of tasks.This assumption may be unrealistic for applications underthe uncertain noise setting (e.g., tasks with different noiseratios, noisy data could be in both support sets and querysets, and identities are not exposed to models).We propose the Data-Efficient and Robust Task Selection(DERTS) algorithm for meta-learning that can select appro-priate tasks in the meta-training stage with efficiency forrapid training and robustness towards noisy data scenariosinspired by . Unlike existing works for task sampling,DERTS does not require any architecture modification; itonly requires an iterative task pool to store tasks for themodel to select.DERTS selects weighted subsets of tasks from task poolsby minimizing the approximation error of the full gradientof task pools in the meta-training stage. Furthermore, bydropping tasks in the subset with potentially high estimatedgradient norms, we find the proposed algorithm is robusttoward the noisy task scenario.Contributions: We propose a data-efficient and robusttask selection algorithm for meta-learning in limited databudget and noisy label settings. We formulate a weighted subset selection objective thatminimizes the approximation error of the full gradienton episodic task pools. Due to the submodularity of theapproximation error, we apply a stochastic greedy approx-imation to the solution of the derived optimization objec-tive. This method can be easily incorporated into bothgradient-based and metric-based meta-learning schemes.",
  "We extend the selection algorithm to a challenging noisytask setting with mislabeled data in both the support andquery sets by dropping tasks with a large estimated gradi-ent norm": "We provide a theoretical analysis that proves that learningon a subset of tasks produces similar training dynamicsthan if trained on the full task pool. We provide an upper-bound the difference between the model trained with ourtask selection algorithm and the model trained with all themeta-training tasks. Our result highlights a fundamentalbias when applying any selection method. Extensive experiments show that DERTS outperforms thestate-of-the-art sampling strategies for meta-learning withboth gradient-based and metric-based meta-learning algo-rithms on a limited budget and noisy task settings. Theperformance improvement averages between 3% and 5%,while also achieving a speedup of more than three times.",
  "Meta-Learning focuses on rapidly adapting to new unseentasks by learning prior knowledge through training on manysimilar tasks. Metric-based methods classify query examples": "based on their similarity to each classs support data, learninga transferable embedding space for evaluation and prediction. Other than metric-basedapproaches, optimization-based methods fine-tune modelparameters on a few support examples . However, most previous work assumes taskscontribute equally to the meta-training stage. Recently, somework has focused on optimizing the sampling distributionstrategy for meta-learning . Arnold et al. apply a uniform episodic sampler (US) toreweight tasks based on the observation that the task learningdifficulty follows a normal distribution for arbitrary meta-datasets and model architectures. Although promising, theempirical assertion that US is based on may face challengeswhen encountering tasks with disturbances and uncertainties.Yao et al. propose an adaptive task scheduler (ATS)to jointly learn the sampling probability for tasks in thecandidate pool to address the noisy label issue. Specifically,ATS takes the loss value and inner product of the gradient onthe support set and query set and outputs the correspondingsampling probability. The robustness of ATS is based onthe assumption that the noise only exists in the support set.When breaking this condition, ATS is likely to be less robustin the noisy task setting. Unlike US and ATS, our algorithm,DERTS, focuses on both data-efficiency and robustness. Forthe data-efficient perspective, DERTS aims to approximatethe performance of full training episodes via only trainingon a subset of tasks. As for the robustness issue, we allowfor the fact that the noisy data can be present in the supportset and query set without any side information; this posessignificant challenges in the evaluation of robustness. With respect to data selection and sampling aspects of theproblem, there have been efforts to take advantage of the dif-ference in importance among various samples to reduce thevariance and improve the convergence rate of stochastic op-timization methods. Those that apply to overparameterizedmodels employ either the gradient norm or the loss func-tion to compute each samples importance. Recently,a series of data selection strategies have discussed selectingsubsets of the data for efficient training. CRAIG findsthe subsets by greedily maximizing a submodular functionand provides convergence guarantees to a neighborhood ofthe optimal solution for both convex and non-convex mod-els. GRADMATCH proposes a variation to address thesame objective using orthogonal matching pursuit. CREST models the non-convex loss as a series of quadratic func-tions for extracting subset. However, the work mentionedabove focuses on the standard supervised learning settingand it is not clear how such approaches can be ported overto task selection for episodic training in meta-learning dueto their formulation as a bilevel-optimization problem.",
  "Task Pools Training Episodes": ". DERTS requires task pools to store episodic tasks sampled from task distributions. With the efficient gradient estimation in sec.4.2,the gradients of all the tasks stored in the task pool are computed. According to the approximation formulated in sec. 4.1 and optimizationobjective in sec. 4.2, a subset of tasks with corresponding weights is constructed to approximate the task pool gradient. The meta-modelthen conducts a training process on the subsets instead of task pools. 3. BackgroundWe consider the standard meta-learning setting, where givena set of training tasks T1, . . . , Tn sampled from task dis-tribution p(T ), we would like to learn a good parameterinitialization for a predictive model f such that it can bequickly adapted to new tasks given only a limited amountof data (i.e., few-shot regime). Each task Ti has a supportset of labeled data Dsi = {Xsi, Ysi } =xsi,j, ysi,jN s",
  "j=1and a query set, Dqi = {Xqi , Yqi } =xqi,j, yqi,jN q": "j=1 oflabeled data, where Ns and Nq refer to the size of supportand query sets respectively. Given the predictive modelf, meta-learning algorithms first train the base model onmeta-training tasks. Then, during the meta-testing stage,the well-trained base model is applied to the new task Tt bytaking a few adaptation steps on its support set Dst . Finally,the performance is evaluated on the query set Dqt . We pro-vide a brief introduction to gradient-based and metric-basedalgorithms in Appendix A.4. Efficient and Robust Task Selection The conceptual idea behind DERTS is to (i) select subsetsof tasks from the overall task pools, (ii) assign a weight toeach task in the subset that captures their relative impor-tance, and (iii) use the tasks in the subset for meta-trainingwith corresponding weights. We first formulate a full gradi-ent approximation for the task pools in sec. 4.1. Secondly,in sec. 4.2, we establish an optimization objective for task selection and provide the solution based on submodular max-imization. In addition, we provide a modified DERTS toaddress the scenario with noisy label tasks in sec. 4.3. Wealso provide a theoretical analysis of DERTS in sec. 4.4. demonstrates the workflow of DERTS. We alsonote that DERTS can be easily incorporated into widely-usedgradient-based and metric-based meta-learning schemes.",
  ". Full Gradient Approximation for Episodic TaskPools": "We start by selecting a sample of candidate tasks drawnfrom the task distribution p(T ) in advance and store themin a task pool M.Suppose for a task pool M:={Tj|j = 1, 2, . . . , N}, we want to select a subset of tasksS := {Ti | i = 1, 2, . . . , k}, where i [N] andk < N, with corresponding weights {i | i = 1, 2, . . . , k}such that the gradient for training on S approximates the gra-dient on M. We now describe our approach for determiningthe weights: Let : M S be a mapping from the taskpool M to the subset S that maps a task Tj from M to a taskTi in S. For simplicity, we denote (Tj) = Ti as (j) = iand Tj M as j M. Let SCM denote the complementof S in M. Similar to , we define the weight i of theselected task Ti S as",
  "(1)": "where 1C is the indicator function for the set C.The first term on the RHS of eq. (1) is equal to 1 since isidentity in S. For the second term, by taking the summationover e SCM, we are calculating how many elements in SCMare mapped into task Ti in S.As we dont know S we cannot compute i directly fromeq. (1). Instead we formulate an optimization problem forestablishing how to select the set S from M in such a mannerthat when training is performed on the tasks in S the trainingdynamics (i.e., the gradients at each iteration) approximatethe gradients that would arise had we trained on the full taskpool M. As in the standard meta-training paradigm, weexplore making the approximation of each tasks gradienton the loss function of the task-adapted model i (i.e., i = L (f; Dsi )) on the query set Dqi that is updated bythe meta-model . The gradient on task Tis query is definedas L (fi; Dqi ) = N q",
  "(2)": "Ideally, we would like to make this error as small as possible.Unfortunately, we do not yet know and hence S, and sowe cannot evaluate the error, and directly optimizing overS is NP-hard. However, we can upper bound the RHS of(2) by a function that can be optimized over, specifically wehave that",
  "(3)": "which naturally leads to defining the approximation cri-teria as minimizing the RHS of (3). According to inequal-ity (3), by assuming S is fixed, assigning the mapping tomap the task in M to the closest element in S in the gra-dient space will minimize and upper bound on the gradientapproximation error. Thus, i associated with mapping is",
  "jM1{j=argminTiSL(f;Dqi )L(f;Dqj)}": "4.2. Extracting Subsets EfficientlyComputing the explicit task gradient Lfj; Dqjthat up-dates the meta-model is time-consuming and incurs a largecomputational cost. As shown in , the variation of thegradient norm is mainly captured by the gradient of the lossfunction with respect to the pre-activation outputs of thelast layer. Suppose for a few-shot classification task, theabove estimation only requires a forward computation onthe last layer. E.g., for a softmax layer as the last, the gra-dients of the loss with respect to the input of the softmaxlayer forxqi,j, yqi,jis li-yi, where li is the logits and yi isthe encoded label. We extend the result of to estimatethe task-gradient L (fi; Dqi ) and denote it as gi. Thisapproximation indicates the computation of gradient esti-mation gi on task Ti is marginally more costly to computethan the value of the loss. We show the extended results andanalysis in Appendix B.Minimizing the RHS of (3) is mathematically equivalentto maximizing a well-known submodular function, i.e., thefacility location function .",
  "jMminiS gj gi": "where C is a constant to upper bound F(S). To formulateour task selection objective, we follow the logic of . Torestrict the size of S, we limit the number of selected tasksin S to be no greater than K, i.e., |S| K. Thus, thesubmodular maximization form of selection objective is:",
  "S = arg maxSMF(S), s.t. |S| K.(4)": "The maximization problem of F(S) under cardinalityconstraint |S| K has an approximation solution with1 e1 bound can be achieved via the greedy algorithm. To start with, initialize S as an empty set andfor each greedy iteration, merge an element T SCM thatmaximizes the marginal utility F(T |S) = F(S T ) F(S). The update step for S can be described as S = S arg maxTjSCMF(Tj | S). The computational complexityof the entire greedy algorithm can be reduced to O(|M|)using stochastic methods to choose random subset . Thisstep is correspinds to line 6 in Algorithm 1.",
  ". DERTS with Noisy Tasks": "Making the meta-learning model robust to label noise is anessential issue, as discussed in previous work .Previous work claims that subset selection basedon the gradient is robust to label noise in the standard noisylabel setting. Specifically, the Jacobian matrix of a neuralnetwork can be well approximated by a low-rank matrix,suggesting a further claim that error for clean labels mainlylies in the subspace corresponding to the dominant singularvalues while the error (corresponding to noisy labels) lies inthe complementary subspace . Thus, the clean data po-tentially form clusters and aggregate with each other, whilenoisy labeled data spread out in the gradient space.We propose a heuristic based on intuition: If a task hasnoisy label data, some of the labels are incorrect, making thetask more difficult to learn. As a result, the model may findit more difficult to find the correct parameters to minimizethe loss, resulting in a larger gradient norm. On the otherhand, if a task contains clean data, the labels are correct, andthe model should be able to learn the task more efficiently,resulting in a smaller gradient norm. Based on the abovemotivation and the principle of not affecting the computationcost significantly, we dynamically set a threshold h on thegradient norm to implicitly infer the task noise ratio by trun-cating tasks with a gradient norm higher than the set thresh-old. Suppose the truncated tasks set is Z := {Tl|gl h},we take the complement of Z on S as the finalized subsetfor noisy task scenario. The algorithmic details can be foundin Algorithm 1.",
  "m 2L(f; D) M": "Assumption 1 and 2 are common in the analysis of con-vergence of training dynamics . Due to the bilevel-optimization inherent to meta-learning, Assumption 3 isrequired a similar assumption is used in . Our mainanalysis result concerning the error between loss functiontrained on the full task pools and the subsets is given below.",
  ": end for": "Theorem 1 (Training Dynamics). Assume that the loss func-tion L(f, D) satisfies assumptions 1 3 and is an upperbound for the RHS of Eq.(2). Then, with the proper con-stant learning rate and for outer and inner loop updatesand a initialization point 0, applying DERTS has similartraining dynamics to that of training on the full task pool M.Specifically,",
  "This result implies training dynamics between subsetlearning and task pool learning is bounded. The first term on": "the RHS of (5) is a contraction that decreases with each itera-tion. The second term shows the difference between trainingon subsets S and all the tasks. The last term r is a bias re-sulting from the bilevel optimization for meta-learning. Wenote that the bias is not an artefact of the DERTS algorithmbut is inherent in the meta-learning formulation. We showthe proof in Appendix C.",
  "Here we demonstrate the effectiveness of DERTS by per-forming extensive experiments on widely used image classi-fication benchmarks in two settings: (i) limited data budgetand (ii) noisy label tasks": "Dataset and Baseline:We conduct experiments onthe standard image classification benchmarks:Mini-ImageNet and Tiered-ImageNet. The tasks areconstructed as 5-way 1-shot and 5-way 5-shot for limiteddata budget setting and 5-way 5-shot for noisy task setting.To show the generality of DERTS, we implemented theproposed algorithm on both gradient-based meta-learningmethod ANIL and metric-based meta-learning methodProtoNet (PN) . For sampling strategies, we compareDERTS with the state-of-the-art sampling paradigm for meta-learning Uniform Episodic Sampling (US) and AdaptiveTask Scheduler (ATS) . According to the sampling mech-anism, we combine US with both ANIL and PN and ATSfor ANIL (ATS is designed only for gradient-based meta-learning algorithms). Implementation Details:For both ANIL and PN, Weuse a 4-layer Convolutional Neural Network (CNN4) as themodel backbone. Additional experiments with ResNet-12 could be found in Appendix E. We set the meta-batchsize to 32. Adam is selected as the optimizer. We setthe learning rates for ANIL as 0.005 0.01 for outer loopsand 0.5 for inner loops with 3 adaptation steps. For PN,we set the learning rate as 0.005. We fix the episodic taskpools with the size of 3200, and the number of selected tasksfor each pool is 960 (meta-training on each selected subsetwould take 30 iterations based on a batch size of 32). Allexperiments are conducted on NVIDIA A6000 GPU. 5.1. Meta-Learning with Limited BudgetExperiment Setup:In this setting, we consider the fol-lowing questions: (i) Assuming no modification on task gen-eration, can DERTS achieve comparable performance withfewer tasks? (ii) Under the scenario where we dramaticallyreduce the number of meta-training classes (i.e., 64 classesreduced to 16 classes for the training set of Mini-ImageNet),we then ask how DERTS performs.We train the meta-model for 10000 iterations for 5-way5-shot setting and 5000 iterations for 5-way 1-shot setting.",
  "We set a 500 iteration warm-up process for ANIL and a 100iteration warm-up process for PN": "Experiment Results with Fewer Tasks:In weshow the average accuracy with 95% confidence interval oftraining a CNN4 base-model on Mini-ImageNet and Tiered-ImageNet for ANIL and PN after varying number of iter-ations. We evaluate checkpoints after training 10% of alliterations (1000 for 5-way 5-shot and 500 for 5-way 1-shot)and 30% of all iterations (3000 for 5-way 5-shot and 1500for 5-way 1-shot). Our key observations are as follows. (i)DERTS is data-efficient for episodic training. For all theevaluation after 10% and 30% of all iterations for both 5-way 5-shot and 5-way 1-shot scenarios, DERTS achieveshigher performance than US, ATS, and vanilla ANIL andPN with random sampling by average gain 4.52% againstUS, 2.23% against ATS, and 1.92% against vanilla ANILand PN on accuracy. 6 out of 8 of the evaluation after alliterations, DERTS outperforms all the baselines, and the rest2 evaluation is averagely 0.46% behind the best performer.The fact that DERTSs full iteration performance indicatesthe diversity of selected tasks, preserves the generalizationcapacity for the meta-model. (ii)DERTS gains significantspeedup against other sampling strategies. showsthe average per iteration running time for ANIL-based al-gorithm on Mini-ImageNet. ANIL has the fastest runningtime because no extra module is added to the vanilla algo-rithm. Although US is the fastest sampling strategy amongthe three modified methods, from we can tell thatUS has the lowest convergence and performance on accu-racy. As ATS has a significantly heavy computation load,the running time of ATS is above 4 times of DERTSs. Theexecution time of DERTS is slightly higher than vanillaANIL and US, suggesting that DERTSs estimation of taskgradient and submodular maximization with the stochasticgreedy algorithm are efficient. Combining the results from and , we claim that DERTS gains generalspeedup on computation time against baselines for reachinga comparable performance. Experiment Results with Fewer Class for GeneratingTasks:We conduct experiments on the setting of a limitedbudget of training classes proposed by ATS . In the few-shot classification problem, each training episode is a few-shot task that subsamples classes as well as data points. Inmini-Imagenet, the original number of meta-training classesis 64, corresponding to more than 7 million 5-way combina-tions. Thus, we control the budgets by reducing the numberof meta-training classes to 16, resulting in 4,368 combina-tions. Thus, in this new setting, not only did the data forconstructing tasks decrease to 25% in the original setting, butalso the number of classes decreased to 25% of the originalsetting, imposing a significant decrease in task diversity.",
  "ProtoNet(PN)64.28 0.765.22 0.869.26 0.843.52 0.747.07 0.749.56 0.7PN-US60.67 0.864.05 0.867.89 0.840.43 0.742.73 0.849.02 0.8PN-DERTS65.02 0.7 66.03 0.8 69.11 0.744.19 0.747.96 0.7 49.48 0.7": ". Average accuracy (%) of 5-way 5-shot and 5-way 1-shot Mini-ImageNet and Tiered-ImageNet Classification with Limited BudgetSetting. In the 5-way 5-shot setting, iterations of 1000 (3000) in the table denote the performance after learning on 10% (30%) tasks duringthe episodic training. In the 5-way 1-shot setting, iterations of 500 (1500) in the table denote the performance after learning on 10% (30%)tasks during the episodic training.",
  ". 5-way 5-shot / 1-shot Mini-ImageNet Classification with25% Class Training Set": "shows the result of the aforementioned settingwith fewer classes. By only training on 25% of the classes,we observe that DERTS outperforms all the baselines withan average of 2% on accuracy. Due to the significant de-crease in task diversity, we claim our task selection methodstill correctly captures the decreased diversity and focuseson the more informative tasks for learning representation.Additional experiments with other ratios of decreased meta-training class can be found in Appendix E.",
  ". Meta-Learning with Noisy Tasks": "Experiment Setup:Unlike previous work whichonly constructs noisy data within the support set, we extendthe noisy data to both the support set and query set, withoutproviding the noisy datas specific identity. The details ofgenerating noisy tasks can be found in Appendix D. Weconsider two specific setups: the first where 25% of datapoints are incorrectly labeled, the second where 40% aremislabelled (25% setting means the average noise ratio forall the tasks is 25%, but there could be extremely high noiseratio tasks and roughly clean-label tasks). We test 5-way 5-shot setting with symmetric label flipping . For the 25%case, we use the same warm-up model as with the limitedbudget setting. For the 40% case, we double the number ofwarm-up iterations for ANIL and PN. We set the estimatedgradient norm threshold h as 1.25 times the average of thetask estimated gradient norm in the task pools. Experiment Result: shows the average accuracywith a 95% confidence interval of training a CNN4 base-model on Mini-ImageNet and Tiered-ImageNet for ANILand ProtoNet with 12000 iterations. DERTS outperforms allbaselines in the noisy task settings, by achieving on average3% improvement in testing accuracy. It is worth noting thatin the 40% settings, DERTS acquires a larger gain on perfor-mance than in 25% noisy setting, which empirically demon-strates the robustness of our algorithm. Another importantobservation is that ATS and US are empirically sensitive tothe noisy task setting. The reason ATS is less robust than",
  "(d)": ". Loss Residual and Accuracy for Noisy Task Settings(Early Stage). (a) Test Accuracy of 25% Noise Setting on Mini-ImageNet of ANIL. (b) Training Loss of 25% Noise Setting onMini-ImageNet of ANIL. (c) Test Accuracy of 40% Noise Settingon Mini-ImageNet of PN. (d) Training Loss of 40% Noise Settingon Mini-ImageNet of PN.",
  ". Ablation Study on Selection Ratio and Weights with Mini-ImageNet 5-way 5-shot Classification": "vanilla ANIL and ANIL-DERTS may be caused by the innerproduct computation for gradient on support set and queryset is ineffective because both support sets and query setscontain the noisy labeled data. shows the trainingdynamics of DERTS and baselines. We observe that asidefrom the final gain in performance, DERTS converges fasterthan all baselines as well.",
  "We investigate the effectiveness of selecting ratio k/N (kis the size of subsets and N is the size of task pools) and": "weights {i | i = 1, 2, . . . , k} in the ablation study. shows the results of different selecting ratio k/Nand the default ratio without adding weights. We observethat with a low selecting ratio, DERTSs performance drops1%-2%. This phenomenon may caused by the decrease ofselected tasks diversity since the selected ratio k/N is lowand the subset can not include all the informative tasks fromthe task pool. The performance of setting without addingweights is significantly lower than the setting with corre-sponding weights, showing the selected tasks are not equallyimportant as well. This phenomenon suggests DERTSsweight selection is an essential element for capturing the fulltraining dynamics. Additional ablation study can be foundin Appendix E.",
  ". Conclusion & Discussion": "We propose Data-Efficient and Robust Task Selection(DERTS), a task selection algorithm for meta-learning fromthe view of optimization to address the issue of data effi-ciency, and robustness against noisy labels. By selectingsubsets with weights to approximate the gradient of the tasksfrom the task pools, DERTS is data-efficient for episodictraining under the limited budget scenario. We then modifyDERTS to address the robustness of noisy data scenarios.Our extensive experiments demonstrate that DERTS outper-forms the state-of-the-art sampling and selection strategiesin terms of both data efficiency and robustness.",
  "Advances in Neural Information Processing Systems, 34:14811493, 2021": "Ravikumar Balakrishnan, Tian Li, Tianyi Zhou, Nageen Hi-mayat, Virginia Smith, and Jeff Bilmes. Diverse client selec-tion for federated learning via submodular maximization. InInternational Conference on Learning Representations, 2022. Zhixiang Chi, Li Gu, Huan Liu, Yang Wang, Yuanhao Yu, andJin Tang. Metafscil: A meta-learning approach for few-shotclass incremental learning. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition, pages1416614175, 2022.",
  "Angelos Katharopoulos and Franois Fleuret. Not all samplesare created equal: Deep learning with importance sampling.In International conference on machine learning, pages 25252534. PMLR, 2018": "Krishnateja Killamsetty, Durga Sivasubramanian, GaneshRamakrishnan, and Rishabh Iyer. Glister: Generalizationbased data subset selection for efficient and robust learning. InProceedings of the AAAI Conference on Artificial Intelligence,pages 81108118, 2021. Krishnateja Killamsetty, Xujiang Zhao, Feng Chen, andRishabh Iyer. Retrieve: Coreset selection for efficient androbust semi-supervised learning. Advances in Neural Infor-mation Processing Systems, 34:1448814501, 2021.",
  "Diederik P Kingma and Jimmy Ba. Adam: A method forstochastic optimization.arXiv preprint arXiv:1412.6980,2014": "Xiaomeng Li, Lequan Yu, Yueming Jin, Chi-Wing Fu, LeiXing, and Pheng-Ann Heng. Difficulty-aware meta-learningfor rare disease diagnosis. In Medical Image Computingand Computer Assisted InterventionMICCAI 2020: 23rdInternational Conference, Lima, Peru, October 48, 2020,Proceedings, Part I 23, pages 357366. Springer, 2020. Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao,Jiebo Luo, and Li-Jia Li. Learning from noisy labels withdistillation. In Proceedings of the IEEE International Confer-ence on Computer Vision, pages 19101918, 2017. Kevin J Liang, Samrudhdhi B Rangrej, Vladan Petrovic, andTal Hassner. Few-shot learning with noisy labels. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 90899098, 2022.",
  "Samet Oymak and Mahdi Soltanolkotabi. Overparameterizednonlinear learning: Gradient descent takes the shortest path?In International Conference on Machine Learning, pages49514960. PMLR, 2019": "Omead Pooladzandi, David Davini, and Baharan Mirza-soleiman. Adaptive second order coresets for data-efficientmachine learning. In International Conference on MachineLearning, pages 1784817869. PMLR, 2022. Aniruddh Raghu, Maithra Raghu, Samy Bengio, and OriolVinyals. Rapid learning or feature reuse? towards understand-ing the effectiveness of maml. In International Conferenceon Learning Representations, 2020. Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell,Kevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, andRichard S Zemel. Meta-learning for semi-supervised few-shotclassification. arXiv preprint arXiv:1803.00676, 2018.",
  "Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin,and Jae-Gil Lee. Learning from noisy labels with deep neuralnetworks: A survey. IEEE Transactions on Neural Networksand Learning Systems, 2022": "Yue Sun, Adhyyan Narang, Ibrahim Gulluk, Samet Oymak,and Maryam Fazel. Towards sample-efficient overparameter-ized meta-learning. Advances in Neural Information Process-ing Systems, 34:2815628168, 2021. Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HSTorr, and Timothy M Hospedales.Learning to compare:Relation network for few-shot learning. In Proceedings of theIEEE conference on computer vision and pattern recognition,pages 11991208, 2018.",
  "Ze Yang, Chi Zhang, Ruibo Li, Yi Xu, and Guosheng Lin.Efficient few-shot object detection via knowledge inheritance.IEEE Transactions on Image Processing, 32:321334, 2022": "Huaxiu Yao, Yu Wang, Ying Wei, Peilin Zhao, MehrdadMahdavi, Defu Lian, and Chelsea Finn. Meta-learning withan adaptive task scheduler. Advances in Neural InformationProcessing Systems, 34:74977509, 2021. Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha.Few-shot learning via embedding adaptation with set-to-setfunctions. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 88088817,2020. Runsheng Yu, Weiyu Chen, Xinrun Wang, and James Kwok.Enhancing meta learning via multi-objective soft improve-ment functions. In The Eleventh International Conference onLearning Representations, 2023.",
  "A. Introduction to Meta-Learning Algorithms": "Gradient-based meta-learningThe goal of gradient-based meta-learning is to learn initial parameters such that takingone (or a few) gradient steps on the support set Ds leads to a model that performs well on task T . Consider model-agnosticmeta-learning (MAML) with base model f as an illustrative example. During the meta-training stage, the performance ofthe adapted model f (i.e., = L (f; Ds), denotes the inner-loop learning rate) is evaluated on the correspondingquery set Dq and is used to optimize the model parameter . Formally, the bi-level optimization process with expected risk isformulated as:",
  "arg minET p(T ) [L (f; Dq)]": "During the meta-testing stage, for task Tt, the adapted parameter t is found by fine-tuning meta-model t on the support setDst . The almost no inner loop (ANIL) algorithm simplifies the inner loop computation by only updating the classificationhead during meta-training task adaptation while keeping the remainder frozen. ANIL achieves a comparable performance withMAML with lower computational cost. Metric-based meta-learningThe aim of metric-based meta-learning is to conduct a non-parametric learner on top ofmeta-learned embedding space. Taking prototypical network (ProtoNet) with base model f as an example , for each taskT , we first compute a class prototype representation {cr}Rr=1 as the representation vector of the support samples belonging to class k as cr =1Nr(xsk;r,ysk;r)Dsr f P Nxsk;r, where Dsr represents the subset of support samples labeled as class r,xsk;r, ysk;rdenotes the data with corresponding label in Dsr, and the size of this subset is Nr. Then, given a query data sample xqk in the query set, the probability of assigning it to the r-th class is measured by the distance d between its representationf (xqk) and prototype representation cr, and the cross-entropy loss of ProtoNet is formulated as:",
  "B. Efficient Gradient Estimation": "The updating process of the meta-model with explicit task gradient Lfj; Dqjis time-consuming and incurs a largecomputational cost. As shown in , the variation of the gradient norm is mainly captured by the gradient of the loss functionwith respect to the pre-activation outputs of the last layer. Therefore, for a few-shot classification task, the above estimationonly requires a forward computation on the last layer. E.g., for a softmax layer as the last, the gradients of the loss with respectto the input of the softmax layer forxqi,j, yqi,jis li-yi, where li is the logits and yi is the encoded label. In this section, weelaborate on the details of our efficient gradient estimation as mentioned in sec. 4.2. We extend the result of to estimatethe task-gradient L (fi; Dqi ) and denote it as gi.Generally, we follow the notation of to establish our analysis upon the estimated gradient. Suppose in a L-layermultilayer perceptron network, (l) RMlMl1 denotes the weight matrix for layer l with Ml hidden units and (l)() be aLipschitz continuous activation function. Then, for datapoint (xi, yi), let",
  "Next, in Proposition 1 we show how to efficiently bind the task gradient with the adapted model via the gradient of lossw.r.t. the input of the last layer": "Proposition 1 (Gradient Norm Upper Bound). Suppose the loss function is -smooth, the norm of difference of task-specificmeta-gradients L (fi; Dqi ) and Lfj; Dqjcan be upper bounded by a constant C1 times the norm of difference of giand gj (gradients of the last layer of meta-model ) with adding another constant C2, i.e.,",
  "(6)": "Thus, we derive the gradient of meta-model f on Dqi w.r.t. the l-th layer (l) can be bounded by the gradient of the lossw.r.t. the pre-activation outputs. c1 and c2 will be used for further derivation.According to (6), we can show that two arbitrary query sets gradient of meta-model can be bounded by constant times thegradient of the loss w.r.t. the pre-activation outputs of the neural network as",
  "+ L c2(7)": "Due to the bi-level optimization structure of meta-learning, the intrinsic gradient for outer loop meta-model updatingis L(fi; Dqi ) for task Ti. Suppose the loss function L is -smooth, the norm of the outer loop gradient differenceL(fi; Dqi ) L(fj; Dqj) can be bounded based on the result of (7):",
  "E.1. ResNet-12 as Large Backbone": "To show DERTS works for a larger backbone, we explored the performance of ANIL and PN with ResNet-12 in bothlimited data budget and noisy label task (noise ratio 40%) settings on Mini-Imagenet. We keep the ResNet-12 configurationdetails the same as CNN4.From and 7, we observe that DERTS generally holds the advantage of data efficiency and robustness for bothsettings when shifting the backbone to ResNet-12. In the limited data budget setting, DERTS shows its faster learningcapability towards baselines. In the noisy label task setting, DERTS for ANIL outperforms baseline by at least 3% on accuracy,which significantly shows DERTS is effective for larger backbones. One thing worth mentioning here is that ANIL-US andPN-US perform comparably better on ResNet12 in the limited data budget setting than CNN4. We speculate the strongerrepresentation capability of ResNet12 empowers ANIL-US and PN-US with a better ability to be aware of the difficulty oftasks, but it is still not robust in the noisy task setting compared to other methods.",
  "E.2. Additional Experiment on Limited Data budget": "In this subsection, we provide additional experiments and details for limited data budget setting. In the main context, wepresent the experiment results on only training 16 classes (25% classes and 25% data). Here, we provide experiment results fortraining 32 classes (50% classes and 50% data) in . According to the experiment results, DERTS outperforms all the",
  ". 5-way 5-shot / 5-way 1-shot Mini-ImageNet Classification with 50% Class Training Set": "baselines with an average of 1.2% in accuracy, which further indicates that DERTS captures the task diversity in this settingwith fewer training classes.According to , the details of selected training classes are as follows. For 25% training classes (16 classes), we select:{n02823428, n13133613, n04067472, n03476684, n02795169, n04435653, n03998194, n02457408, n03220513, n03207743,n04596742, n03527444, n01532829, n02687172, n03017168, n04251144}.In addition, the selected classes for 50% training classes are:{n03676483, n13054560, n04596742, n01843383,n02091831, n03924679, n01558993, n01910747, n01704323, n01532829, n03047690, n04604644, n02108089, n02747177,n02111277, n01749939, n03476684, n04389033, n07697537, n02105505, n02113712, n03527444, n03347037, n02165456,n02120079, n04067472, n02687172, n03998194, n03062245, n07747607, n09246464, n03838899 }.",
  "E.3. Case Study for Task Selection": "We provide a brief case study for further analysis of the tasks selected and not selected by DERTS. displays typicalexamples of both selected and unselected tasks. From the two unselected tasks presented, we observe that these tasks aregenerally coarse-grained. The classes within the unselected tasks are easily distinguishable, indicating that they might berelatively simpler. In contrast, the classes and images in the selected tasks tend to be more visually confusing. Task 3 includes three different classes of dogs, making this task more fine-grained than some of the unselected tasks. Task 4 consists of twovisually similar pairs: the Seal and Diver pair, which might share the same background, and the Pot and Soup pair, whichcould have similar shapes and colors. The selected subset of tasks likely offers better diversity and is more challenging tolearn, making them more informative for the meta-training process. This observation aligns with the claim made by relatedworks on task sampling ."
}