{
  "RobustNeRFOursRobustNeRFNeRF-W": ". NeRF On-the-go. Given casually captured image sequences or videos in the wild as inputs, the goal of this paper is to train aNeRF for static scenes and effectively remove all dynamic elements in the scenes (cars, trams, pedestrians, etc), i.e. distractors. Unlikeexisting methods such as NeRF-W and RobustNeRF , which produce imperfect results, our method leverages the predicteduncertainty maps to effectively remove those distractors. This results in high-fidelity novel view synthesis on challenging dynamic scenes. AbstractNeural Radiance Fields (NeRFs) have shown remark-able success in synthesizing photorealistic views from multi-view images of static scenes, but face challenges in dy-namic, real-world environments with distractors like mov-ing objects, shadows, and lighting changes. Existing meth-ods manage controlled environments and low occlusion ra-tios but fall short in render quality, especially under highocclusion scenarios. In this paper, we introduce NeRF On-the-go, a simple yet effective approach that enables the ro-bust synthesis of novel views in complex, in-the-wild scenesfrom only casually captured image sequences. Delving intouncertainty, our method not only efficiently eliminates dis-tractors, even when they are predominant in captures, butalso achieves a notably faster convergence speed. Throughcomprehensive experiments on various scenes, our methoddemonstrates a significant improvement over state-of-the-art techniques. This advancement opens new avenues forNeRF in diverse and dynamic real-world applications.1. Introduction",
  "* Equal contribution": "Neural radiance fields (NeRFs) have emerged as agroundbreaking paradigm for this task. This is because aNeRF can produce geometrically consistent and photoreal-istic renderings, even for complex scenarios with thin struc-tures and semi-transparent objects. Training a NeRF model requires a set of RGB imageswith given camera poses, and demands manual adjustmentsof camera settings, such as focal length, exposure, and whitebalance. More crucially, vanilla NeRFs operate under theassumption that the scene should remain completely staticduring the capture process, without any distractors such asmoving objects, shadows, or other dynamic elements .Nevertheless, the real world is inherently dynamic, makingthis distractor-free requirement often unrealistic to meet.Additionally, removing distractors from the captured datais non-trivial. The process involves per-pixel annotation foreach image, a procedure that is very labor-intensive, espe-cially for lengthy captures of large scenes. This underscoresa key limitation in the practical application of NeRFs in dy-namic, real-world environments.",
  "arXiv:2405.18715v2 [cs.CV] 2 Jun 2024": "NeRF-W optimizes pixel-wise uncertainty from ran-domly initialized embedding by volume rendering. Sucha design is suboptimal since it neglects the prior informa-tion of the image and entangles the uncertainty with radi-ance field reconstruction. As a result, they need to intro-duce transient embeddings to account for distractors. Theaddition of a new degree of freedom complicates systemtuning, leading to a Pareto-optimal scenario as discussedin . Dynamic NeRF methods like D2NeRF candecompose static and dynamic scenes for video input, butunderperform with sparse image inputs. More recently, Ro-bustNeRF models distractors as outliers and demon-strates impressive results in controlled and simple scenarios.Nevertheless, its performance significantly drops in com-plex, in-the-wild scenes. Interestingly, RobustNeRF alsounderperforms in scenarios without any distractors. Thisleads to a compelling research question:",
  "Can we build a NeRF for in-the-wild scenes from casuallycaptured images, regardless of the ratio of distractors?": "Toward this goal, we introduce NeRF On-the-go, a ver-satile plug-and-play module designed for effective distrac-tor removal, allowing rapid NeRF training from any casu-ally captured images. Our method is grounded in three keyaspects. First, we utilize DINOv2 features for theirrobustness and spatial-temporal consistency in feature ex-traction, from which a small multi-layer perception (MLP)predicts per-sample pixel uncertainty. Second, our methodleverages a structural similarity loss to improve uncertaintyoptimization, enhancing the distinction between foregrounddistractors and the static background. Third, we incorpo-rate estimated uncertainty into NeRFs image reconstruc-tion objective using a decoupled training strategy, whichsignificantly enhances distractor elimination, particularly inhigh occlusion scenes. Our method demonstrates robust-ness across a wide range of scenarios, from confined in-door scenes with small objects to complex, large-scale streetview scenes, and can effectively handle varying levels ofdistractors. Notably, we find that our On-the-go module canalso significantly accelerate NeRF training up to one orderof magnitude, compared with RobustNeRF. This efficiency,combined with its straightforward integration with modernNeRF frameworks, makes NeRF On-the-go an accessibleand powerful tool for enhancing NeRF training in dynamicreal-world settings.",
  ". Related Work": "Uncertainty in Scene Reconstruction.Uncertainty hasproven to enhance the robustness and reliability of a widerange of tasks such as monocular depth prediction ,semantic segmentation , and simultaneous localiza-tion and mapping (SLAM) . In general, un-certainty can be divided into two categories: epistemic and aleatoric . In the specific context of scene reconstruc-tion, epistemic uncertainty generally arises from data lim-itations, such as restricted viewpoints. For instance, utilizes ensemble learning to quantify epistemic uncertaintyfor exploring unobserved regions in next-best-view (NBV)planning for NeRF. Goli et al. establishes a volumetricuncertainty field to remove the floaters from NeRF. On theother hand, aleatoric uncertainty comes from the inherentrandomness of the data, such as the noise of measurement,appearance changes, and distractors in the scene.Thereare works that utilize aleatoric uncertainty asa guiding principle for active learning and NBV planningfor better NeRF training. Similarly, DebSDF improvesindoor scene reconstruction through an uncertainty map tomitigate the noise from monocular prior.Closely related to us, NeRF-W was pioneering toeliminate transient objects and address variable illumina-tion in unstructured internet photo collections, achieved byintroducing transient and appearance embeddings. Follow-up works like Ha-NeRF hallucinates NeRFs from un-constrained tourism images, while Neural Scene Chronol-ogy reconstructs temporal-varying chronology fromtime-stamped Internet photos. Building upon previous for-mulation for aleatoric uncertainty, we innovate by integrat-ing DINOv2 features into uncertainty prediction, whichenhances the quality of predicted uncertainty.In a re-cent work, Kim et al. also presents a similar DINO-based uncertainty prediction approach, but directly adaptsfor NeRF-W to a pose-free condition. In contrast, wefocus on refining NeRF training to effectively handle dis-tractors from casually-captured image sequences. SLAM and SfM in Dynamic Scenes. Handling dynamicscenes has been studied for years in the literature of SLAMand SfM. Classical methods exclude pixels associated withdynamic objects with robust kernel function orRANSAC .However, such hand-craft featuresare effective in scenarios with a low occlusion ratio butstruggle at in-the-wild scenes. To address this, recent ad-vances have integrated additional information.This in-cludes external segmentation or detection modules for pre-defined classes , utilization of optical or sceneflow , and geometry-based approachesusing clustering and epipolar line distance . NeRF in Dynamic Scenes. Recent NeRF methods focus onreconstructing both static and dynamic components from avideo sequence enabling novelview synthesis at arbitrary timestamps. Although primar-ily designed for video inputs, these methods often under-perform with photo collection sequences . Addition-ally, separating static and dynamic components can be time-consuming and requires extensive hyperparameter tuning.A notable example in this realm is EmerNeRF , whichalso employs the DINOv2 features. However, they use",
  "Samplinguncer (Eq. (9))": ". Pipeline. A pre-trained DINOv2 network extracts fea-ture maps from posed images, followed by a dilated patch samplerthat selects rays. The uncertainty MLP G then takes the DINOv2features of these rays as inputs to generate the uncertainties (r).Three losses (on the right) are used to optimize G and the NeRFmodel. Note that the training process is facilitated by detachingthe gradient flows as indicated by the colored dashed lines. them for enhanced scene decomposition, while we use themas a strong prior knowledge for distractor removal.RobustNeRF, to our knowledge the only method that alsotargets static scene reconstruction from dynamic scenes,uses Iteratively Reweighted Least Squares for outlier ver-ification. Compared with it, our method can deal with morecomplex scenes with various levels of occlusions.",
  ". Method": "We start by showing how to utilize per-pixel DINO fea-tures for uncertainty prediction (Sec. 3.1). Subsequently,we show a novel approach for learning uncertainty to re-move distractors in NeRF (Sec. 3.2). We further introduceour decoupled optimization scheme for uncertainty predic-tion and NeRF (Sec. 3.3). Finally, we illustrate why sam-pling method is important in distractor-free NeRF training(Sec. 3.4). An overview of our pipeline is depicted in .",
  ". Uncertainty Prediction with DINOv2 Features": "Our primary objective is to effectively identify and elimi-nate recurring distractorsthose that appear across multipleimages. To achieve this, we take advantage of DINOv2 features, which have shown to be able to maintain spatial-temporal consistency across views.We begin with extracting DINOv2 features for each in-put RGB image. Next, these features serve as inputs to asmall MLP to predict the uncertainty value for each pixel.To further enforce the consistency of our uncertainty pre-diction, we incorporate a regularization term.Image Feature Extraction. For RGB images with a res-olution of H W, we derive per-pixel features through apre-trained DINOv2 feature extractor E:",
  "Fi = E(Ii), E RHW 3 RHW C(1)": "where i spans all training images, and C denotes featuredimension. This module also upsamples the feature mapsto the original resolution by nearest-neighbor sampling.Uncertainty Prediction. Once we obtain the 2D DINOv2feature maps, we proceed to determine the uncertainty ofeach sampled ray r. We first query its corresponding fea-ture f = Fi(r), and then input it to a shallow MLP toestimate the uncertainty for this ray (r) = G(f), whereG is the uncertainty MLP. In the subsequent sections, wewill demonstrate how this predicted uncertainty (r) is in-tegrated into the optimization process as a weighting func-tion, which plays a crucial role in refining the NeRF model,particularly in handling and mitigating the impact of dis-tractors in the scene.Uncertainty Regularization. To enforce spatial-temporalconsistency in uncertainty predictions, we introduce a reg-ularization term based on the cosine similarity of featurevectors within a minibatch. Specifically, for each sampledray r, we define a neighbor set N(r) consisting of raysin the same batch whose associated feature vectors exhibithigh similarity to the feature f of r. This neighbor set isformed by selecting rays that meet a specified cosine simi-larity threshold :",
  ". Uncertainty for Distractor Removal in NeRF": "We hypothesize that pixels correlating with dynamic ele-ments (distractors) should have high uncertainty, whereasstatic regions should have low uncertainty. This premiseallows us to effectively integrate predicted uncertainty intoNeRF training objectives, aiming to progressively filter outdistractors for enhanced novel view synthesis.We will analyze the potential issue of the classical wayof incorporating uncertainty into the loss function for NeRF.Finally, we introduce a simple yet effective modification, toincorporate uncertainty, for robust distractor removal. Uncertainty Convergence Analysis. Uncertainty predic-tion has been widely used in different fields, includingNeRF-based novel view synthesis. For example, in the sem-inal work NeRF in the Wild , their loss is written as *:",
  "(r)+ 1 log (r)(4)": "Here, C(r) and C(r) represent the input and rendered RGBvalues. The uncertainty (r) is treated as a weight function.The regularization term is crucial for balancing the first termand preventing the trivial solution where (r) = .Here we present a simple analysis to understand how theuncertainty changes wrt. the loss function, we first take thepartial derivative wrt. (r):",
  "C(r) C(r)(6)": "This reveals an important relationship between uncertaintyprediction and the error between the rendered and input col-ors. Specifically, the optimal uncertainty is directly propor-tional to this error term.However, a challenge arises when employing the 2 lossas shown in Eq. (4), particularly when the color of distrac-tors and background is close (as illustrated in (d)). Insuch cases, the predicted uncertainty in those regions willalso be low according to Eq. (6). This impedes the effec-tiveness of uncertainty-based distractor removal, and leadsto cloud artifacts in the rendered images.Recognizing the limitation inherent in the 2 RGB loss,we propose a new loss for better uncertainty learning, sothat the predicted uncertainty can discriminate between dis-tractors and static background more effectively.SSIM-Based Loss for Enhanced Uncertainty Learning.The structural similarity index (SSIM) is comprised of threemeasurements: luminance, contrast, and structure similari-ties. These components capture local structural and contrac-tual differences, which is crucial for distinguishing betweenscene elements. This is verified in , where SSIM is ef-fective in detecting distractors by incorporating these threecomponents together. An SSIM loss can be formulated as:",
  "(f) Structure Error": ". SSIM Can Effectively Distinguish Distractors. In thisscene from , the 3 wooden robots are the dynamic elements.SSIM pinpoints distractors by leveraging discrepancies in threemeasurements including luminance, contrast, and structure. Con-versely, relying solely on the 2 error between RGB values (lu-minance error) proves challenging, especially when the distractorsand background have similar colors. The color bar on the rightside indicates the correspondence for error interpretation.",
  "LSSIM = (1L(P, P))(1C(P, P))(1S(P, P)) (8)": "Compared to Eq. (7), our reformulation in Eq. (8) placesgreater emphasis on the differences between dynamic andstatic elements. Consequently, this enhances the disparityin uncertainty, facilitating more effective optimization ofuncertainty. The mathematical proof and comparisons be-tween Eq. (7) and Eq. (8) are included in the supplements.Building on this updated SSIM formulation, we intro-duce a new loss tailored for uncertainty learning:",
  "(r)2 + 1 log (r)(9)": "This loss is a simple modification of Eq. (4), adapted forbetter uncertainty learning. Luncer is specifically applied totrain the uncertainty estimation MLP G. This is crucial as itallows us to decouple the training of the NeRF model fromuncertainty prediction. Such decoupling ensures that thelearned uncertainty is robust to various types of distractors.Please refer to for an ablation for Luncer.Note that a recent work S3IM also uses SSIM forNeRF training, but their loss is tailored for static scenes,whereas ours is designed for better uncertainty learning.Also, S3IM employs stochastic sampling to identify non-local structural similarities, while we use dilated samplingto focus on local structures for distractor removal.",
  "(r)(10)": "This loss, essentially Eq. (4) without the regularizationterm, is used because Luncer already prevents trivial solu-tions for uncertainty ((r) = ). The parallel trainingprocess is facilitated by detaching the gradient flow fromLuncer to NeRF representation, and Lnerf to the uncertaintyMLP G as illustrated in . Note that we also follow Ro-bustNeRF and include the interval loss and distortionloss from Mip-NeRF 360 for NeRF training, which weomit here for simplicity. Our overall objectives integrate alllosses together, denoted as:",
  ". Dilated Patch Sampling": "In this section, we delve into the ray sampling strategy, akey factor in the efficacy of NeRF training, particularly inachieving distractor-free results.RobustNeRF has demonstrated the efficacy of patch-based ray sampling ( (b)) over random sampling( (a)). However, this approach has its limitations, pri-marily due to the small size of the sampled patches (e.g.16 16). Especially when the batch size is small due tothe constraint of GPU memory, this small context can re-strict the networks learning capacity to remove distractors,impacting optimization stability and convergence speed.To tackle the issue, we utilize dilated patch sampling [18, 29, 43, 50, 56, 57], depicted in (c). This strategyinvolves sampling rays from a dilated patch. By enlargingthe patch size, we can significantly increase the amount ofcontextual information available in each training iteration.Our empirical findings in show that dilated patchsampling not only accelerates the training process, but alsoyields superior performance in distractor removal.",
  ". On-the-go Dataset. Sample training images showingthe distractors in several scenes of our self-captured dataset": "include the Crab scene since it is not released. Meanwhile,we put comparisons on Baby Yoda scene in supplements,since each image in this sequence contains a distinct set ofdistractors, which is different from our setting.On-the-go Dataset. To rigorously evaluate our approachin real-world indoor and outdoor settings, we captured adataset that contains 12 casually captured sequences, in-cluding 10 outdoor and 2 indoor scenes, with varying ra-tios of distractors (from 5% to over 30 %). For quantitativeevaluation, we select 6 sequences representing different oc-clusion rates, as shown in . More details and resultsfor this dataset are available in supplements.Baselines.We compare our method with Mip-NeRF360 , D2NeRF , NeRF-W , Ha-NeRF , Ro-bustNeRF , and Mip-NeRF 360 + SAM, a baselinethat we design to exclude dynamic objects in images withSAM , and train a NeRF on static parts. Refer to sup-plements for more details.Metrics. We adopt the widely used PSNR, SSIM andLPIPS for the evaluation of novel view synthesis.",
  ". Evaluation": "On-the-go Dataset. We extend our evaluation on our On-the-go dataset, as depicted in and . Comparedto our method, RobustNeRF often fails to retain fine de-tails in low to medium-occlusion scenarios, and struggles toeliminate distractors in high-occlusion settings. Besides, wenotice that even after tuning the hyperparameter of outlierratios for highly-occluded scenes, RobustNeRF still showsinferior performance. Please refer to the supplements.Unlike RobustNeRF, NeRF-W and Ha-NeRF show pro-ficiency in removing distractors at low and medium occlu-",
  "LPIPS SSIM PSNR LPIPS SSIM PSNR LPIPS SSIM PSNR LPIPS SSIM PSNR LPIPS SSIM PSNR LPIPS SSIM PSNR": "Mip-NeRF 360 0.295 0.601 19.640.556 0.290 13.910.345 0.660 20.410.421 0.503 15.480.469 0.306 17.820.486 0.432 15.73NeRF-W 0.491 0.492 18.070.546 0.410 17.200.349 0.708 20.210.445 0.532 17,550.690 0.384 16.400.606 0.349 12.99Ha-NeRF 0.499 0.485 18.640.569 0.393 16.710.367 0.684 19.230.393 0.543 16.820.599 0.460 17.850.505 0.463 16.67RobustNeRF 0.383 0.496 17.540.576 0.318 15.650.244 0.764 23.040.251 0.718 20.390.391 0.625 20.650.366 0.578 20.54Mip-NeRF 360 + SAM0.258 0.642 20.200.556 0.287 13.650.332 0.670 20.650.227 0.738 20.830.323 0.542 21.080.326 0.576 20.13Ours0.259 0.644 20.150.314 0.609 20.110.190 0.806 24.220.219 0.754 20.780.189 0.787 23.330.235 0.718 21.41",
  ". Novel View Synthesis Results on Our On-the-go Dataset. We show quantitative comparison between our methods and baselines": "sion levels, but this effectiveness comes at the cost of re-duced image quality. This trade-off is a consequence ofits transient embedding approach, as discussed in .Furthermore, NeRF-W and Ha-NeRF struggle notably athigher occlusion ratios. In such cases, their per-image tran-sient embeddings are unable to adequately model distrac-tors, leading to a noticeable performance drop. The Mip-NeRF 360 combined with SAM method works well in sim-ple scenes like Mountain, where distractors are easy to seg-ment. However, its effectiveness diminishes in more com-plex scenes. In contrast, we exhibit versatility across sceneswith various occlusion ratios, and can consistently producehigh-quality renderings.Comparison on RobustNeRF Dataset . As shown in, our method exhibits superior performance quan-titatively and qualitatively over all baselines.Robust-NeRFs hard-thresholding approach tends to overlook com-plex structures with limited observations, such as the shoesand carpet in the Android scene. Moreover, we observedthat they underperform in scenarios involving plane sur-faces with view-dependent effects, e.g. the wooden textureon the table with view-dependent highlight in Statue scene.Note that Mip-NeRF 360 + SAM requires a tedious processof manually selecting every distractor in each image usingSAM , but it still struggles with capturing thin struc-tures, shadows, and reflections.",
  ". Ablation Study": "All ablations are conducted on the challenging highly-occluded Patio-High scene in our On-the-go dataset.Patch Dilation. Here we test different dilation rates for ourdilation patch sampling, as shown in . Within a rangefrom 1 to 4, a higher dilation rate results in much fasterconvergence and better rendering quality. This verifies ourhypothesis in Sec. 3.4 that increasing the contextual infor-mation within patches can effectively boost performance.However, when the dilation rate is above 4, uncertainty op-timization begins to collapse. It is likely because higher di-lation rates cause patches to lose semantic information. Thisoccurs as the sampling now becomes more akin to randomsampling, negatively impacting the learning of uncertainty.Further details and analysis on patch size and dilation rateacross different sequences are available in the supplements.",
  ".Novel View Synthesis Results on the RobustNeRFDataset. The numbers for Mip-NeRF 360 , D2NeRF andRobustNeRF are taken from . RobustNeRF denotesour own run using the official code release": "Loss Functions. In , we ablate on different train-ing losses. In (b), SSIM proves more adept at differentiat-ing distractors with static elements compared to 2 loss. In(c), we train the uncertainty MLP and NeRF together. Thisresults in a significant performance drop, indicating the ef-fectiveness of our decoupled training approach. Moreover,we find from (a) that omitting Lreg will negatively impactthe rendering quality of certain views. Additional studieson various sequences are available in the supplements.",
  ". Handling Large Obstructions. From top to bottom:input frames, our uncertainty maps, our rendering results": "using a static scene from the Mip-NeRF 360 dataset. Asillustrated in , we indeed achieve great performanceas Mip-NeRF 360 . In contrast, RobustNeRF fails tocapture certain parts of the bicycle, since one of their keydesigns involves omitting at least some portions of a scene.Large Obstructions. In , we further show that ourmethod can faithfully model the large obstructions with ourpredicted uncertainty, and effectively remove them.",
  ". Conclusions": "We introduce NeRF On-the-go, a versatile method that en-ables effective and efficient distractor removal in dynamicreal-world scenes containing various levels of distractors.Our method represents a step towards realizing the full po-tential of NeRF in practical, in-the-wild applications.Limitation. While our method shows robustness on diversereal-world scenes, we suffer in predicting correct uncertain-ties for regions with strong view-dependent effects, suchas highly reflective surfaces like windows and metals. In-tegrating additional prior knowledge into the optimizationprocess could potentially be beneficial.Acknowledgements. We thank the Max Planck ETH Cen-ter for Learning Systems (CLS) for supporting SongyouPeng. We also thank Yiming Zhao, Yidan Gao and ClementJambon for helpful discussions.",
  "Lily Goli, Cody Reading, Silvia Selllan, Alec Jacobson, andAndrea Tagliasacchi. Bayes rays: Uncertainty quantifica-tion for neural radiance fields. In CVPR, 2024. 2": "Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch,Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapra-gasam, Florian Golemo, Charles Herrmann, Thomas Kipf,Abhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh-Ti (Derek) Liu, Henning Meyer, Yishu Miao, DerekNowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Rad-wan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi,Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun,Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi,Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: a scal-able dataset generator. In CVPR, 2022. 12 Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch,Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapra-gasam, Florian Golemo, Charles Herrmann, et al. Kubric: Ascalable dataset generator. In CVPR, 2022. 12 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Identity mappings in deep residual networks. In ComputerVisionECCV 2016: 14th European Conference, Amster-dam, The Netherlands, October 1114, 2016, Proceedings,Part IV 14, pages 630645. Springer, 2016. 15",
  "Haotong Lin, Qianqian Wang, Ruojin Cai, Sida Peng, HadarAverbuch-Elor, Xiaowei Zhou, and Noah Snavely. Neuralscene chronology. In CVPR, 2023. 2": "Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-worth. Nerf in the wild: Neural radiance fields for uncon-strained photo collections. In CVPR, 2021. 1, 2 Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-worth. Nerf in the wild: Neural radiance fields for uncon-strained photo collections. In CVPR, 2021. 1, 2, 4, 5, 6, 7,13, 14 Nathaniel Merrill, Yuliang Guo, Xingxing Zuo, XinyuHuang, Stefan Leutenegger, Xi Peng, Liu Ren, and Guo-quan Huang. Symmetry and uncertainty-aware object slamfor 6dof object pose estimation. In CVPR, 2022. 2 Marko Mihajlovic, Aayush Bansal, Michael Zollhoefer, SiyuTang, and Shunsuke Saito.Keypointnerf:Generalizingimage-based volumetric avatars using relative spatial encod-ing of keypoints. In ECCV, 2022. 5",
  "A.1. Dataset Details": "Synthetic Dataset.We evaluate on the syntheticdataset provided in D2NeRF .This dataset in-cludes five sequences with floating objects in the room gen-erated by Kubric . Upon careful examination, we noticethat the training and test images within the Chair scene aremisaligned in terms of their coordinate systems, thereforewe decide to temporarily exclude this particular scene.RobustNeRF Dataset. As illustrated in the original Ro-bustNeRF, there are unintentional changes throughout thecapturing process (both the training and test set) for thedataset, including the tablecloth movement in the Androidscene and the curtain in the Statue scene, which may ad-versely affect the performance of SAM-based methods. Incontrast, both RobustNeRF and our method can natu-rally accommodate these unintentional changes.On-the-go Dataset. On-the-go dataset is acquired with anassortment of devices, including an iPhone 12, a SamsungGalaxy S22 and a DJI Mini 3 Pro drone. During the cap-ture of each sequence, the exposure, white balance, and ISOare fixed. This dataset features a wide range of dynamicobjects including pedestrians, cyclists, strollers, toys, cars,robots, and trams), along with diverse occlusion ratios rang-ing from 5% to 30%. This diversity ensures a rich and chal-lenging environment for our assessments. The resolution ofimages captured by the iPhone 12 and DJI drone(Drone se-quence) is 40323024, whereas the resolution of sequencescaptured by the Samsung Galaxy S22(Arc de Triomphe andPatio sequence) is 19201080.",
  "A.2. Implementation Details of NeRF On-the-go": "Our work is built upon the Mip-NeRF 360 codebase. In addition to our proposed loss, we keep the originaldistortion loss and interval loss in Mip-NeRF 360 . Werun our method on a server with an AMD EPYC 9554 64-core processor and 4 NVIDIA RTX 4090 GPUs. For eachscene, we run 250000 iterations with a batch size of 16384,which typically takes 12 hours to finish. Through our as-sessment, we observed that our model, after only one hour of training, already demonstrated superior quality comparedto RobustNeRF, even after it underwent 12 hours of train-ing. We downsample images by 8x to keep it the same asRobustNeRF (except Arc de Triomphe and Patio is down-sampled by 4x to make it roughly the same as Robust-NeRF). We select the dilated sample patches with a sizeof 32 32 and a dilation rate of 4. The SSIM windowsize is 5 5. For hyperparameters in loss terms, we set1 = 100, 2 = 0.5, 3 = 0.5, 4 = 0.1 for all datasets.",
  "A.3. Baseline Details": "RobustNeRF . For our own run of RobustNeRF ,we enable the appearance embedding (GLO) since it de-livers consistently better results as illustrated in Robust-NeRF as shown in .Mip-NeRF 360 + SAM. This is a baseline that we intro-duce for evaluation. For RobustNeRF dataset, we usean interactive tool|| to click each distractor in every image.For On-the-go dataset, we pre-identify the dynamic objectscategories and consider this as an oracle for this method.To detect the dynamic objects bounding box, we employedYOLOv8** to generate the bounding box for the distrac-tors. Following this, Segment Anything Model (SAM) is applied with the detected bounding box to get the corre-sponding segmentation. In the absence of a robot classin YOLOv8, we identify the robot in the Spot scene byselecting the bounding box encompassing the largest areaof yellow. Some imperfect masking results are shown inFig. A, primarily attributable to factors such as partial ob-servation, reflections of distractors, and ambiguous classifi-cations, like the categorization of a statue as a human.",
  "B.1. Evaluation": "Kubric Dataset .We evaluate on Kubric syntheticdataset provided in D2NeRF , with qualitative resultsshown in Table A. Our performance aligns with Robust-NeRF, this is due to saturation on this simple dataset. Weinclude the result of this dataset solely for the sake of a com-prehensive evaluation.Comparison on RobustNeRF Dataset . In this sec-tion, we present the results obtained from the BabyYodascene, as summarized in Table B. Our methodology yieldsimproved outcomes compared to the open-source imple-mentation of RobustNeRF. However, these results do not",
  "RobustNeRF 0.200.83 30.87RobustNeRF 0.310.81 29.19Ours0.240.83 29.96": "Table B. Novel View Synthesis Results on the BabyYoda Sceneof RobustNeRF dataset. RobustNeRF denotes our own runusing the official code release. Our method is superior comparedwith RobustNeRF , although it does not quite achieve theresults reported in the RobustNeRF paper. quite reach the performance levels reported in the originalRobustNeRF paper. We didnt put this result in the mainpaper because the distractors in this dataset varies across allimages, which doesnt fit our setting.On-the-go Dataset. Additional qualitative results of On-the-go dataset are shown in Fig. B. Our method consis-tently outperforms all baseline methods in various envi-ronments. The performance of different baseline methodsclosely aligns with the sequences depicted in the .While NeRF-W is capable of removing distractors, itdoes so at the expense of detail loss. RobustNeRF ,due to its threshold-based nature, occasionally fails to pre-serve thin structures. Furthermore, Mip-NeRF 360 + SAMstruggles due to the imperfect segmentation, as illustratedin Fig. A.",
  "Table C. Ablations on Loss Functions. We compare different losschoices for training on the Tree sequence": "in the main paper is evaluated on a high occlusionsequence, Table C is evaluated on a low occlusion sequence.We find that for both occlusion scenarios, each componentof our method contributes to the overall performance en-hancement. Although in scenarios with relatively low oc-clusion, the design choice (b) still can achieve satisfactoryquality except for certain views, the performance drop ismore pronounced in high occlusion scenarios. Furthermore,in both occlusion scenarios, we observe that (c) Luncer forNeRF exhibits a significant performance decline. This de-cline can primarily be attributed to our SSIM formulation,which is tailored more toward optimizing uncertainty ratherthan scene representation. Dilated Patch Ablation. We continue to test various di-lation rates on a low occlusion scene Tree in Table F withpatch size fixed to be 32 32. We observe that the perfor-mance closely resembles that of high occlusion scenes asdepicted in . Notably, unlike in high occlusion sit-uations, a dilation rate of 8 is able to sustain performancewithout collapsing. Nevertheless, to maintain consistencyin hyperparameter settings across all occlusion scenarios,we set the dilation rate at 4.",
  "Table E. Novel View Synthesis Results with Different FeatureExtraction Module": "itative results of are shown in Fig. C. These qual-itative results align with the trends observed in ,indicating that a lower uncertainty ratio (< 4) effectivelyremoves distractors but reduces the reconstruction quality,whereas a higher dilation ratio (> 4) tends to reintroducethe distractors due to the loss of local information.Feature Extraction Module. In this paragraph, we changethe feature extractor module E to Resnet-50 and DI-NOv1 as detailed in Table E. We find that there are neg-ligible differences between DINOv1 and DINOv2. How-ever, we observe that the Resnet-50 features are less effec-tive in removing distractors. We attribute this difference tothe Resnet features emphasis on color information, in con-trast to the DINO features that prioritize instance informa-tion, essential for efficient distractor removal.",
  "< s1 < s2 < 1.(12)": "Our assumptions in Eq. (12) are directly grounded inthe properties proved in the original SSIM paper (Sec-tion III.B). In such cases, the similarity between renderedpatches and ground truth would naturally decrease. Ourempirical results also support this validity: our modifiedSSIM loss consistently outperforms the original one in var-ious datasets.To prove that our reformulation in Eq. (8) places greateremphasis on the differences between dynamic and static el-ements compared to Eq. (7), we need to demonstrate thefollowing inequality:",
  "This implies that f(x, y, z) is monotonically decreasingwith respect to x in the given domain. By the symmetry off, the same holds for y and z, completing the proof": "We compare the effectiveness of the conventional SSIMformulation and our modified SSIM approach in the Patio-High scene as shown in Table F. Our SSIM formulation cansuccessfully remove distractors while conventional SSIMfails to do so.Parameter-tuning Free. Here we show our methods supe-riority against RobustNeRF that no explicit outlier ratioassignment is required for training on scene Patio-High. Asshown in Fig. D, multiple experiments with different ratiosneed to be run for RobustNeRF to gain its best per-formance. However, our method does not need any hyper-parameter tuning and still archives much better results thanRobustNeRF .Fast Convergence.In Fig. E, we show the conver-gence curve comparison between RobustNeRF and 0.40.50.60.70.8 RobustNeRF Inlier Ratio 0.2 0.4 0.6 0.8 SSIM OursRobustNeRF",
  "Figure F. Failure cases": "our method under different occlusion conditions(Tree andPatio-High), using SSIM metrics as the basis for compar-ison.Our method demonstrates significantly faster con-vergence nearly one magnitude faster and exhibitsmarkedly better performance after reaching convergence.Failure Case. Similar to baseline methods, we also strugglein regions with strong view-dependent effects, see Fig. F.Moreover, inherited from the limitation of our base modelMip-NeRF360, we also require sufficient training views.Our performance will degrade significantly when the train-ing views become sparse."
}