{
  "Max Planck Institute for Intelligent Systems, Tubingen, Germany2Meshcapade3ETH Zurich": ". Existing methods that regress 3D human pose and shape (HPS) from an image (like HMR2.0 ) estimate bodies that are eitherimage-aligned or have accurate 3D pose, but not both. We show that this is a fundamental trade-off for existing methods. To address thisour method, TokenHMR, introduces a novel loss, Threshold-Adaptive Loss Scaling (TALS), and a discrete token-based pose representationof 3D pose. With these, TokenHMR achieves state-of-the-art accuracy on multiple in-the-wild 3D benchmarks.",
  "Abstract": "We address the problem of regressing 3D human poseand shape from a single image, with a focus on 3D ac-curacy. The current best methods leverage large datasetsof 3D pseudo-ground-truth (p-GT) and 2D keypoints, lead-ing to robust performance. With such methods, however,we observe a paradoxical decline in 3D pose accuracy withincreasing 2D accuracy. This is caused by biases in thep-GT and the use of an approximate camera projectionmodel. We quantify the error induced by current cameramodels and show that fitting 2D keypoints and p-GT ac-curately causes incorrect 3D poses. Our analysis definesthe invalid distances within which minimizing 2D and p-GTlosses is detrimental. We use this to formulate a new loss,Threshold-Adaptive Loss Scaling (TALS), that penalizesgross 2D and p-GT errors but not smaller ones. With such a",
  "*Equal contribution": "loss, there are many 3D poses that could equally explain the2D evidence. To reduce this ambiguity we need a prior overvalid human poses but such priors can introduce unwantedbias. To address this, we exploit a tokenized representationof human pose and reformulate the problem as token predic-tion. This restricts the estimated poses to the space of validposes, effectively improving robustness to occlusion. Exten-sive experiments on the EMDB and 3DPW datasets showthat our reformulated loss and tokenization allows us totrain on in-the-wild data while improving 3D accuracy overthe state-of-the-art. Our models and code are available forresearch at",
  "arXiv:2404.16752v1 [cs.CV] 25 Apr 2024": "By accurate, we mean two things. A method should cor-rectly regress the 3D pose but it should also align with theimage evidence. Unfortunately, current models cannot doboth. We observe a seeming paradox, that the more accu-rate a method is on fitting 2D keypoints, the less accurate itis at predicting 3D pose. We study this problem and identifythe common weak-perspective camera assumption as a keyculprit. This camera model does not match the true cam-era used to acquire the images and thus there is a mismatchbetween the projected 3D joints and the detected 2D ones.Since currently, no reliable method exists to estimate cam-era parameters from single image, we study and quantifythis effect and propose two solutions to address it.Specifically, we use the synthetic BEDLAM dataset,which has perfect 3D and 2D groundtruth (GT). We projectthe 3D data into 2D using the camera model from toquantify the 2D error in the best case; it is large. We thenalso go the other direction and show that low 2D error canresult in large 3D error. Even using a full perspective modellike does not solve the problem since we lack the pre-cise intrinsic and extrinsic camera parameters. This analysis highlights the issue of supervising 3D poseregression with a 2D keypoint loss. But such a loss opensup access to large datasets, providing generalization and ro-bustness. Unfortunately, pseudo ground-truth (p-GT) train-ing data suffers the same problem since it is generated by fit-ting a 3D body to 2D data via optimization using an approx-imate camera model. How can we leverage the abundant in-formation present in large-scale, in-the-wild, datasets whilemitigating the decline in 3D accuracy? Our answer to thisis TokenHMR, a new HPS regression method that strikes abalance between effectively leveraging 2D keypoints whilemaintaining 3D pose accuracy, thus leveraging Internet datatoday without known camera parameters. TokenHMR has two main components.The first isbased on our key insight that supervision from 2D key-points, while flawed, is valuable for preventing highly in-correct predictions.However, excessive reliance on 2Dcues introduces bias. To address this, we define a new losscalled Threshold-Adaptive Loss Scaling (TALS) that penal-izes large 2D and p-GT errors but only minimally penalizessmall ones. We use our BEDLAM analysis to define this, sothat the network is not encouraged to fit 2D keypoints moreaccurately than makes sense given the camera model.This, however, creates a new problem.Predicting3D pose from 2D keypoints is fundamentally ambiguous.When one relaxes the keypoint matching constraint, evenmore 3D poses are consistent with the 2D data. To controlthis, we need to introduce a prior that biases the networkto valid poses. Unfortunately, existing pose priors based onmixtures of Gaussians or VAEs are biased towardsposes that occur frequently in the training data. Instead, weseek an unbiased prior that restricts the network to only out-",
  "put valid poses but does not bias it to any particular pose": "This leads us to the second key component of To-kenHMR, which gives it its name. Specifically, we convertthe problem of continuous pose regression into a problemof token prediction by tokenizing human poses. We use aVector Quantized-VAE (VQ-VAE) to discretize con-tinuous human poses by pre-training on extensive motioncapture datasets, such as AMASS and MOYO .This tokenized representation provides the regressor witha vocabulary of valid poses, effectively representing thethe pose prior as a knowledge bank, codebook. Since VQ-VAEs are designed to represent a uniform prior, we positthat this reduces the biases caused by previous pose priors. TokenHMR generates discrete tokens through classifi-cation, in contrast to regressing continuous pose.Whenwe take a SOTA HPS method and replace the continuouspose with our tokenized pose approach we see consistentimprovements in 3D accuracy (all else held the same). We perform extensive experiments to evaluate differentways of tokenizing pose and their effects on accuracy. Anydiscretization of pose comes with some loss in accuracy. Inour case it results in a loss of 3D accuracy of about 2.5mm,which is 20 times smaller than the accuracy of the state-of-the-art (SOTA) HPS regressors on real data; i.e., the loss inaccuracy due to tokenization is negligible. Finally, we put our two components together and findthat they work synergistically. Our new loss does not dis-tort the 3D pose to over-fit the keypoints or p-GT and thetokenization keeps the network from distorting 3D posefor the sake of 2D accuracy. With this combination, weachieve a new state-of-the-art in terms of 3D accuracy. Weextensively evaluate TokenHMR and other recent meth-ods on EMDB and 3DPW , which have accu-rate 3D ground truth. Using the same data and backbone,TokenHMR exhibits a 7.6% reduction in 3D error comparedto HMR2.0 on the challenging EMDB dataset. Qual-itative results suggest that the TokenHMR is robust to am-biguous image evidence and the estimated poses do not suf-fer from the bent knees bias of methods that use p-GT and2D keypoints (see ). In summary, we make the following key contributions:(1) Analysis of 3D Accuracy Degradation: We analyzeand quantify the trade-off between 3D and 2D accuracythat current HPS methods face if they use 2D losses. (2)Threshold-Adaptive Loss Scaling: To ameliorate the issue,we develop a novel loss function that reduces the influenceof 2D and p-GT errors that are less than the expected errordue to the incorrect camera model. (3) Token-Based PoseRepresentation: We introduce a token-based representationfor human pose and show that it produces more accuratepose estimates. Our models and code are available for re-search.",
  ". HPS Regression": "Estimating 3D human pose and shape from single imageshas been studied in great detail from optimization-based ap-proaches to the most recent transformer-based regressors.Optimization approaches fit a parametric model to 2D image cues, including, but not limited to keypoints , silhouettes , and part segmentations . Somelearning-based approaches directly estimate the parametricbody model from images and videos and some estimate bodies with a model-free approach either as vertices or as implicitshapes . Recent methods use transform-ers to estimate 3D bodies, achieving the current best accu-racy. To address the challenges of generalization, recentmethods like EFT , NeuralAnnot , HMR2.0 and CLIFF use 2D keypoints and p-GT in the trainingloss, to produce a good alignment between the projectedbody and the image. Methods like HuManiFlow andPOCO model probabilistic HPS to explicitly addresspose ambiguity. The problem of 3D accuracy degradationin pursuit of better 2D alignment has been noted but notextensively quantified before our work. Our statistical anal-ysis highlights this bias in existing HMR methods, offeringa new perspective on training strategies for this problem.Some methods address the 3D-to-2D pro-jection error by estimating the camera from a single image.SPEC uses a network to predict camera parameters butdoes not generalize well, while CLIFF uses an approxi-mation by providing the network with information about thebounding box coordinates of the person in the image. Esti-mating the camera from a single image is highly ill-posedso this remains a challenging, unsolved, problem. Our ap-proach reduces the impact of using the wrong camera modeland can be applied to any HPS regression method.",
  ". Pose Prior": "Human pose priors play a pivotal role in various applica-tions like lifting 2D pose to 3D and estimating hu-man pose from images/videos . Early pose priorsfocus on learning joint limits to avoid poses that are im-possible. Gaussian Mixture Models (GMMs) and Gen-erative Adversarial Networks (GANs) are also usedto impose prior knowledge during training. Some recentmethods use VAEs and normalizing flows as pri-ors. Many of these methods are biased to commonly occur-ring poses and this bias is passed on the regressor. Methodslike Pose-NDF learn a manifold of plausible poses rep-resented as the zero-level set of a neural implicit function.The mapping of invalid to valid poses involves gradient de-scent, which is an expensive operation when integrated inHPS training. In contrast to prior work, we learn a dis- . Visualization of the camera/pose bias issues. (a) Thelack of correct focal length means that foreshortened legs are es-timated as bent by methods like HMR2.0. (b) Replacing the pre-dicted body poses with ground truth reveals camera bias; (c) Main-taining 2D alignment, how wrong can the 3D poses be? See Sec. 3for details. crete token-based prior over valid SMPL poses, reducingpose bias and improving robustness to occlusion, while be-ing easy to integrate into HPS training.We use a VQ-VAE , which is a variant of VAEs,to learn a discretized prior by quantizing the 3D trainingposes in a process called Tokenization creating a knowl-edge bank i.e. codebook. Tokenization is widely used invarious applications like image synthesis , text-to-image generation , 2D human pose estimation ,and learning motion priors . In the context of hu-man pose estimation, tokenization remains relatively unex-plored, though it is widely used in human motion generation. Of course, tokenized representations of imagesand language are widely used for many vision and languageproblems. Our approach is novel in that it reformulates theregression problem as a pose token classification problem.It thus exploits tokenization to represent valid poses, effec-tively providing a pose prior.",
  ". Camera/pose Bias": "Methods that estimate 3D HPS typically try to satisfy twogoals: accurate 3D pose and accurate alignment with 2Dimage features. Unfortunately, we observe a trend in allexperiments the better a method does on 2D error, the worse it does on 3D and vice versa. The key reason forthis is that current methods, including those tested here,do not estimate the camera intrinsic parameters (e.g. focallength) or the camera extrinsics (rotation and translation).Instead, current methods estimate the person in camera co-ordinates using scaled orthographic projection or perspec-tive projection with fixed and incorrect camera parameters.This results in a mismatch between the true 3D joints andtheir 2D projection. Specifically, since photos are typicallytaken from roughly eye height, the legs are further awaythan the upper body. This causes them to be foreshortened.Training models to minimize 2D error forces them to gen-erate incorrect poses in 3D; this is illustrated in (a).Pseudo ground truth (p-GT) for 2D pose datasets is obtainedby minimizing the 2D error with problematic camera pa-rameters. Fully trusting such p-GT and pursuing accuratelearning of such annotations will make the problem moreprominent. Notice how foreshortening makes the legs ap-pear shorter in the image. The only way to make a humanbody fit this is to bend the legs at the knees or tilt the body in3D, making the legs further away. This produces unnaturalor unstable poses.This is a fundamental issue with all current methods andone cannot get low error for both 3D and 2D without know-ing the camera. To numerically evaluate the impact of thismismatch, we employ BEDLAM , a synthetic datasetwhere both 3D and 2D data are known exactly along withthe camera. This removes any possible noise and allowsus to see the effects of using the wrong camera on 2D pro-jection error. Specifically, as shown in (b), we takeground truth BEDLAM bodies and project them into theimage using the camera of HMR2.0 .We evaluate the effect of the incorrect camera in 2Dusing the standard measure of Percentage of Correct Key-points (PCK), which we compute for a sequence from theBEDLAM validation set.The 3D bodies computed byHMR2.0b have errors of 0.78 on PCK0.5 and 0.88 onPCK1.0. In contrast, when we use the HMR2.0b camerawith the ground truth 3D bodies, the PCK scores decreaseto 0.66 on PCK0.5 and 0.86 on PCK1.0. Ideally, with a cor-rect camera model, both PCK0.5 and PCK1.0 should reach1.0. The fact that HMR2.0b achieves lower error than theground truth indicates that its output deviates from the true3D pose and shape due to camera bias. This demonstratesthat methods like HMR2.0b, while obtaining high PCK val-ues, do so at the expense of 3D accuracy. In summary, seek-ing high PCK values is counterproductive to 3D accuracyunless one has the correct camera model.We further design experiments to explore how bad the3D error can be while maintaining good 2D alignment. Wemodify the loss function of SMPLify to keep the dis-tance between predicted 2D keypoints and GT 2D keypointsJ2Dg close, while adding a new loss to increase the dis-",
  "w2D||(J3D, T ) J2Dg||2 w3D||J3D J3Dg||2 + m": "(1)where represents 3D-to-2D projection using HMR2.0scamera, m = 20 is the margin value, w2D = 4 andw3D = 40.5 are scalar weights. After 100 iterations ofoptimization, the Mean Per Joint Position Error (MPJPE)reaches 146mm. As shown in (c), the projected 3Dpose can still maintain a high degree of 2D alignment evenwith significant errors in the depth direction. When opti-mized for 200 iterations, the MPJPE exceeds 300mm, andthe error continues to increase with further optimization.Since the field does not currently have a reliable way toestimate the camera parameters from a single image, be-low we explore the ability of our new methods (TALS andtokenization) to help mitigate the issues caused by approx-imate camera models. (a) compares results fromHMR2.0 and TokenHMR. Note that the effect of foreshort-ening has less impact on pose with TokenHMR.",
  ". Preliminaries": "Our method, TokenHMR, takes an input image, I, and out-puts body pose, , shape, and perspective camera, T. Weuse SMPL , a differentiable parametric body model. Itsinput parameters consist of pose, denoted by R72 andshape, denoted as R10. As output, it produces a bodymesh, M, and vertices, V RNX3, where N = 6890 isthe number of vertices. 3D joints denoted as J3D, are de-rived through a linear combination of mesh vertices using apre-trained joint regressor.",
  ". Threshold-Adaptive Loss Scaling: TALS": "In , our analysis reveals a notable impedimentto the effective learning using pseudo-ground-truth and2D keypointscamera/pose bias. Despite this challenge,the scale provided by such annotations remains integral toachieving optimal generalization and robustness. We assertthat, when appropriately utilized, without over-fitting, theseannotations significantly enhance the models ability to ro-bustly estimate pose. A key insight emerges from our ob-servations: establishing an effective threshold is imperativeto discern the error levels that yield no additional benefit asa training signal. When the loss surpasses this threshold,conventional learning mechanisms guide pose estimation.Conversely, when the loss falls below this effective thresh-old, we minimize its impact to prevent over-fitting to thecamera/pose bias.To determine this effective threshold, we analyze the er-rors obtained using ground truth (GT) 3D poses and a stan-dard (incorrect) camera model. Again we leverage the 3D . Framework overview. Our method has two stages. (a) In the tokenization step, the encoder learns to map continuous posesto discrete pose tokens and the decoder tries to reconstruct the original poses. (b) To train TokenHMR, we replace regression withclassification using the pre-trained decoder, which provides a vocabulary of valid poses. GT in BEDLAM , this time to establish effective thresh-olds for both 2D keypoints and SMPL pseudo-ground-truth.For 2D keypoints, we replace the predicted SMPL parame-ters with ground truth values from BEDLAM to obtain thereal 3D human bodys 2D keypoint projections under theHMR2.0 camera.We then calculate the mean L1 normbetween these projections and the GT 2D keypoints, anduse this as the threshold J2D for 2D keypoint supervision.We normalize these 2D keypoints relative to image widthand scale the values between -0.5 and 0.5 to mitigate scale-related variances.Similarly, to establish the effective supervision thresh-old for SMPL p-GT, we conduct additional experiments.With p-GT, we formulate the pose loss in terms of joint an-gle error. To set appropriate thresholds on these errors, weevaluate the difference in joint angles between HMR2.0spredictions on BEDLAM and the ground truth values foreach joint in the SMPL model. Specifically, we computethe mean geodesic distance* between the 3D joint rotationson the manifold of rotations in SO(3) predicted by HMR2.0and the ground-truth rotations in BEDLAM. Please refer toSec. B.2 in Sup. Mat. for specific threshold values.After establishing the effective thresholds for 2D key-points and SMPL p-GT, we introduce a new loss calledThreshold-Adaptive Loss Scaling (TALS).It scales downthe loss only when it goes below the threshold. Specifi-cially, the TALS loss terms for p-GT pose and 2D joints aredefined as",
  ". Tokenization": "We use a VQ-VAE , which learns an encoding of 3Dpose in a discrete representation. Specifically, we learn adiscrete representation for SMPL body parameters, =[1, 2, . . . , 21], where the i represent each joints poseparameters in R6. The process involves encoding and de-coding the pose parameters using an autoencoder architec-ture and a learnable codebook, denoted as CB = {ck}Kk=1,with each code ck Rdc, where dc is the dimension ofthe codes. The overall architecture of the Pose VQ-VAE isillustrated in (a). The encoder and decoder of the au-toencoder are represented by E and D, respectively. Theencoder is responsible for generating discrete pose tokens,while the decoder reconstructs these tokens back to SMPLposes. The latent feature z can be computed as z = E(),resulting in z = [z1, z2, . . . , zM], where zi Rdc and Mis the number of tokens. Each latent feature zi is quantizedusing the codebook CB by finding the most similar codeelement, as expressed in the following equation:",
  "(5)": "where sg is the stop gradient operator, e is the embed-ding from the codebook and RE, E, C are the hyper-parameters of for each term. For reconstruction, we usean L1 loss between the ground-truth pose, g, and pre-dicted pose, and also on the error between the SMPLground-truth 3D joints, J3Dg and predicted joints, J3D. So,the LRE loss is defined as",
  ". Architecture": "Our architecture exploits the Vision Transformer (ViT) ,similar to HMR2.0 . The input image, I, is first trans-formed into input tokens, which are subsequently processedby the transformer to generate output tokens. These out-put tokens then undergo further processing in the trans-former decoder. The transformer decoder has multi-headself-attention that cross-attends a zero input token with animage output token to get features from the transformerblock. In contrast to HMR2.0 , which employs threelinear layers to map the features from transformer block tothe SMPL pose, , shape, and camera, T, we proposea novel approach. Our objective is to leverage a tokenizertrained on a significant amount of motion capture (mocap)data, specifically focusing on body pose. To facilitate this,we partition the SMPL pose parameters into body pose andglobal orientation. We use separate linear layers to predictthe global orientation and body pose from the tokenizer.A straightforward integration of the tokenizer would in-volve estimating the code index directly from the ViT trans-former backbone and selecting embeddings, e, based on thecode index from the codebook, CB. However, this posesa challenge as the process of selecting an embedding fromthe codebook, is non-differentiable. To address this issue,we adopt a logit-based approach. Instead of directly esti-mating the code index, we output logits, Q for each token.These logits are multiplied with the codebook, resulting inweighted embeddings. Thus, the approximated quantizedfeature z = [z1, z2, . . . , zM] can be calculated as",
  "z = (QMK) CBKD z(7)": "where, Q are the logits estimated by the backbone, is thesoftmax operation, CB is the pretrained codebook, M isthe number of tokens, K is the number of entries in thecodebook and D is the dimension of each codebook en-try. The operation makes it differentiable. The obtained ap-proximated quantized features, z are subsequently passedthrough the tokenizer decoder. This process yields the fi-nal pose. In the training of TokenHMR, the learned tok-enizer decoder is frozen to take advantage of the prior it haslearned from mocap data.",
  "DL3D(J3D, J3Dg) + 2DL2D(J2D, J2Dg)(8)": "where L is a SMPL shape loss, LJ3Dis the 3Djoint loss and LJ2Dis the joint re-projection loss., 3D and 2D are steering weights for each term.TolearnfromSMPLpseudo-ground-truth,weuseThreshold-Adaptive Loss Scaling (TALS) where we scalethe loss based on the threshold computed in Sec. 4.2, out-lined in Eqs. 2 and 3. Thus, the total loss is defined as",
  ". Implementation Details": "Training of TokenHMR involves two stages: first we traina tokenizer to learn discrete pose representations usingAMASS and MOYO mocap data. Then we use thepretrained decoder of the tokenizer as an additional head forregressing body pose. During the training of TokenHMR,the tokeniser is frozen to exploit the prior.Our tokenizer architecture is inspired from T2M-GPT but instead of learning motion tokens of 3D jointswe learn pose tokens of SMPL pose parameters. We use 1ResNet block and 4 1D convolutions both in the en-coder and decoder. The steering weights RE, E, C areset at 50.0, 1.0, 1.0, respectively. The model is trained for150K iterations with batch size of 256 and learning rate of2e4. To train a robust model, we augment random jointswith noise starting from 1e3, which we progressively in-crease after every 5K iterations. We choose the best tok-enizer model containing 160 tokens and codebook of size2048 256 for TokenHMR based on reconstruction erroron the validation set.For TokenHMR, we use ViT-H/16 as the back-bone and standard transformer decoder followingHMR2.0 . We use 4 separate linear layers to map thefeatures of size 1024 from the transformer decoder to theglobal orientation, hand pose, and body shape of SMPL andone for the camera. However, for body pose, we process the1024 features through 4 blocks of linear layers, each con-taining 2 MLPs and an GELU activation function . Thisgives the final logits, Q, of size 160 2048 for multiplica-tion with a codebook of size 2048 256, which results inapproximate quantized features, z; see Eq. 4. We use ViT-Pose as the pretrained backbone. We train for 100Kiterations on 4 Nvidia RTX 6000 GPUs with a batch sizeof 256 and learning rate of 1e5 for about one day. Thesteering weights, , , J2D, J3D are set to 1e3, 5e4,1e2, 5e2, respectively. The loss weights of TALS is set to1% for both pose and 2D keypoints J2D.",
  "HMR2.0 135.24 (+14.98)113.39 (+14.13)70.68 (+7.86)166.71 (+46.45)137.88 (+38.59)90.30 (+27.48)TokenHMR124.09 (+14.71)104.72 (+13.01)62.13 (+6.52)150.29 (+40.91)125.99 (+34.28)78.88 (+23.27)": ". Impact of evenly cropping images at different ratios from the boundaries on the 3D HPS accuracy on the EMDB dataset. Thenumbers in (parentheses) indicate the changes in performance relative to the non-cropped scenario; smaller is better. All models comparedhere employ identical backbones and are trained on the same data.",
  "AMASS + MOYO8.72.616.57.6": ". Tokenizer Ablation. All methods are trained on thestandard training set of AMASS and evaluated on the test setof AMASS and validation set of MOYO except the last row,which is trained with the MOYO training set. The last model isused as the tokenizer in TokenHMR. Training Data: For training the tokenizer, we use thestandard training split of AMASS and the training dataof MOYO . For more details on data preparation oftraining, please refer to Sup. Mat. Following the prior meth-ods , we use standard datasets (SD) for train-ing which include Human3.6M , MPI-INF-3DHP ,COCO , and MPII . Additionally, like HMR2.0b,we also use in-the-wild 2D datasets (ITW) like InstaVari-ety , AVA , and AI Challenger datasets andtheir p-GT for training. We also include BEDLAM (BL) , a synthetic dataset with accurate ground-truth 3D data. Fora fair comparison, we re-train HMR2.0b using a combina-tion of the SD, ITW, and BL datasets. We choose HMR2.0bas a baseline model since the code is open-source and wecan reproduce the results.Evaluation and Metrics: For the tokeniser accuracy,we report the Mean Vertex Error (MVE) and Mean PerJoint Position Error (MPJPE) and evaluate on the standardtest split of AMASS and validation set of MOYO. For To-kenHMR, we report the Mean Vertex Error (MVE), MeanPer Joint Position Error (MPJPE), and Procrustes-AlignedMean Per Joint Position Error (PA-MPJPE) between thepredictions and the ground-truth. We evaluate on the testset of the 3DPW and EMDB datasets. The formeris a standard 3D dataset and the latter is a recently releasedand more challenging dataset with varying camera motionsand varied 3D poses.",
  ". How to Alleviate the 3D Degradation Problem?": "shows the performance of the HMR2.0 modeltrained solely with the SD dataset and its performance whentrained with both SD and ITW datasets. We observe a sig-nificant decrease (over 17%) in 3D accuracy on the EMDBdataset upon the inclusion of the ITW data. At the sametime, according to the in HMR2.0 paper,the ITW data improves the methods 2D performance. Astraightforward approach to counter this trend could be to",
  ". Qualitative comparisons on challenging poses from the LSP dataset": "integrate more data with precise 3D annotations, such asBEDLAM . Yet, as reveals, even with the in-clusion of BEDLAM, HMR2.0 (SD+ITW+BL) still suffersfrom noticeable 3D metric degradation. This observationforms our baseline for further investigations.Employing our novel loss formulation (TALS) results innotable performance improvement on both the EMDB and3DPW datasets, indicating its effectiveness in preventingoverfitting to noisy p-GT data. While TALS yields improve-ments, we delve deeper into exploring pose priors to com-pensate for the diminished supervision. Our evaluation ofVPoser , a prevalent VAE prior in HPS, yields onlymarginal improvements, suggesting the need for a morerobust alternative. Our VQ-VAE-based pose tokenizationapproach offers greater improvement. TokenHMR signif-icantly outperforms HMR2.0, with improvements of 9%in MVE, 7.6% in MPJPE, and 11.5% in PA-MPJPE onEMDB. Consistent trends are also observed on 3DPW.",
  ". How does the Token-based Prior Help?": "Beyond facilitating more effective learning as we discussedabove, we also examine the efficacy of our discrete token-based prior in scenarios with ambiguous image information,such as truncation. We evaluate our method under varyingdegrees of image cropping on the EMDB dataset. Specif-ically, we crop 30% and 50% from the image boundaries.As shown in , compared with HMR2.0 , the per-formance of our approach decreases less in the challeng-ing truncation settings (50% v.s. 30%). Furthermore, thequalitative outcomes, illustrated in , underscore therobustness of the prior embedded within our token-basedpose representation. This robustness is crucial for handlingreal-world scenarios where image truncation is common.",
  ". Ablation Study of Tokenizer": "presents our ablation study on different tok-enizer design choices using AMASSs standard test set andMOYOs validation set. To understand the impact of thedesign choices on out-of-distribution MOYO data, we trainsolely with AMASS and conduct various ablations. Thefinal tokenizer model (last row in ), however, is used in TokenHMR and is trained on both the AMASSand MOYO datasets. Our findings indicate that the num-ber of codebook entries has a more significant impact thancode dimensions. Although the number of tokens is cru-cial for an accurate representation, we observe a perfor-mance plateau, opting for 160 tokens in our final model.This number strikes a balance between network size andreconstruction accuracy for TokenHMR. Random augmen-tation of pose parameters with noise builds a more robusttokenizer, slightly reducing performance for in-distributiondata but beneficially impacting OOD data.",
  ". Conclusion": "In this paper, we presented a novel approach to 3D humanpose estimation from single images. We begin by identify-ing and quantifying the problem caused by using a 2D key-point loss with an incorrect camera model. This leads to afundamental tradeoff for current methods either have high3D accuracy or 2D accuracy, but not both. Our method,TokenHMR, addresses this problem with two contributionsthat can easily be used by other methods.TokenHMRadopts a new paradigm for pose estimation based on re-gressing a discrete tokenized representation of human pose.We combine this with a new loss, TALS, which mitigatessome of the bias caused by the camera projection error,and biased p-GT, while still allowing the use of in-the-wildtraining data. Our experiments on the EMDB and 3DPWdatasets demonstrate that TokenHMR significantly outper-forms existing models like HMR2.0 in terms of 3D accu-racy, even with siginficant occlusion.",
  ". Acknowledgement": "We sincerely thank the department of Perceiving Systems and MLteam of Meshcapade GmbH for insightful discussions and feed-back. We thank the International Max Planck Research Schoolfor Intelligent Systems (IMPRS-IS) for supporting Sai KumarDwivedi. We thank Meshcapade GmbH for supporting Yu Sunand providing GPU resources.This work was partially sup-ported by the German Federal Ministry of Education and Research(BMBF): Tubingen AI Center, FKZ: 01IS18039B. Disclosure: CVPR 2024.txt",
  "Michael J Black, Priyanka Patel, Joachim Tesch, and JinlongYang. Bedlam: A synthetic dataset of bodies exhibiting de-tailed lifelike animated motion. In CVPR, pages 87268737,2023. 1, 2, 4, 5, 7, 8": "Federica Bogo, Angjoo Kanazawa, Christoph Lassner, PeterGehler, Javier Romero, and Michael J. Black. Keep it SMPL:Automatic estimation of 3D human pose and shape from asingle image. In European Conference on Computer Vision(ECCV), 2016. 2, 3, 4 Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A.Sheikh. Openpose: Realtime multi-person 2d pose estima-tion using part affinity fields. IEEE Transaction on PatternAnalysis and Machine Intelligence (TPAMI), 2019. 1 Hongsuk Choi, Gyeongsik Moon, JoonKyu Park, and Ky-oung Mu Lee. Learning to estimate robust 3d human meshfrom in-the-wild crowded scenes. In Computer Vision andPattern Recognition (CVPR), pages 14751484, 2022. 3 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image isworth 16x16 words: Transformers for image recognition atscale. In International Conference on Learning Representa-tions (ICLR), 2021. 6 Sai Kumar Dwivedi, Nikos Athanasiou, Muhammed Ko-cabas, and Michael J. Black. Learning to regress bodies fromimages using differentiable semantic rendering. In Interna-tional Conference on Computer Vision (ICCV), 2021. 3",
  "G. Georgakis, Ren Li, S. Karanam, Terrence Chen, J.Kosecka, and Ziyan Wu.Hierarchical kinematic humanmesh recovery. European Conference on Computer Vision(ECCV), 2020. 3": "Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran,Angjoo Kanazawa, and Jitendra Malik. Humans in 4D: Re-constructing and tracking humans with transformers. In In-ternational Conference on Computer Vision (ICCV), 2023.1, 2, 3, 4, 6, 7, 8 Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Car-oline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan,George Toderici, Susanna Ricco, Rahul Sukthankar, et al.Ava: A video dataset of spatio-temporally localized atomic",
  "Muhammed Kocabas, Nikos Athanasiou, and Michael J.Black.VIBE: Video inference for human body pose andshape estimation. In Computer Vision and Pattern Recogni-tion (CVPR), 2020. 3": "Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges,and Michael J. Black.PARE: Part attention regressor for3D human body estimation. In International Conference onComputer Vision (ICCV), 2021. 3, 6, 7 Muhammed Kocabas, Chun-Hao P. Huang, Joachim Tesch,Lea Muller, Otmar Hilliges, and Michael J. Black. SPEC:Seeing people in the wild with an estimated camera. In In-ternational Conference on Computer Vision (ICCV), 2021.3 Nikos Kolotouros, Georgios Pavlakos, Michael J. Black, andKostas Daniilidis. Learning to reconstruct 3D human poseand shape via model-fitting in the loop. In International Con-ference on Computer Vision (ICCV), 2019. 7",
  "Nikos Kolotouros, Georgios Pavlakos, Dinesh Jayaraman,and Kostas Daniilidis.Probabilistic modeling for humanmesh recovery. In International Conference on ComputerVision (ICCV), 2021. 3": "Christoph Lassner, Javier Romero, Martin Kiefel, FedericaBogo, Michael J. Black, and Peter Gehler. Unite the people:Closing the loop between 3D and 2D human representations.In Computer Vision and Pattern Recognition (CVPR), 2017.3 Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang,and Cewu Lu. HybrIK: A hybrid analytical-neural inversekinematics solution for 3D human pose and shape estima-tion. In Computer Vision and Pattern Recognition (CVPR),2021. 7 Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu,and Youliang Yan. CLIFF: Carrying location information infull frames into human pose and shape estimation. In Euro-pean Conference on Computer Vision (ECCV), 2022. 1, 2, 3,7",
  "Matthew Loper, Naureen Mahmood, Javier Romero, GerardPons-Moll, and Michael J. Black. SMPL: A skinned multi-person linear model. In Transactions on Graphics (TOG),2015. 3, 4": "Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger-ard Pons-Moll, and Michael J. Black. AMASS: Archive ofmotion capture as surface shapes. In International Confer-ence on Computer Vision (ICCV), 2019. 2, 6, 7, 1 Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal V.Fua, Oleksandr Sotnychenko, Weipeng Xu, and ChristianTheobalt. Monocular 3d human pose estimation in the wildusing improved cnn supervision. In International Confer-ence on 3D Vision (3DV), 2017. 7",
  "Marko Mihajlovic, Shunsuke Saito, Aayush Bansal, MichaelZollhoefer, and Siyu Tang.COAP: Compositional articu-lated occupancy of people. In Computer Vision and PatternRecognition (CVPR), 2022. 3": "Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee.Neuralannot: Neural annotator for 3d human mesh trainingsets. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 22992307,2022. 3 Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Pe-ter Gehler, and Bernt Schiele. Neural body fitting: Unifyingdeep learning and model based human pose and shape es-timation. In International Conference on 3D Vision (3DV),2018. 3 Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, andMichael J. Black. Expressive body capture: 3D hands, face,and body from a single image. In Computer Vision and Pat-tern Recognition (CVPR), 2019. 2, 3, 7, 8, 1 A. Ramesh, Mikhail Pavlov, Gabriel Goh, S. Gray, ChelseaVoss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. International Conference onMachine Learning (ICML), 2021. 3",
  "Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gener-ating diverse high-fidelity images with vq-vae-2. Conferenceon Neural Information Processing Systems (NeurIPS), 2019.3": "Shunsuke Saito, Tomas Simon, Jason Saragih, and HanbyulJoo. PIFuHD: Multi-level pixel-aligned implicit function forhigh-resolution 3D human digitization. In Computer Visionand Pattern Recognition (CVPR), 2020. 3 Istvan Sarandi, Timm Linder, Kai O. Arras, and BastianLeibe. Metric-scale truncation-robust heatmaps for 3D hu-man pose estimation. In IEEE Int Conf Automatic Face andGesture Recognition (FG), 2020. 3 Akash Sengupta, Ignas Budvytis, and Roberto Cipolla. Hu-ManiFlow:Ancestor-Conditioned Normalising Flows onSO(3) Manifolds for Human Pose and Shape DistributionEstimation.In Computer Vision and Pattern Recognition(CVPR), 2023. 3",
  "Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, DanielCohen-or, and Amit Haim Bermano. Human motion diffu-sion model. In The Eleventh International Conference onLearning Representations, 2023. 2": "Garvita Tiwari, Dimitrije Antic, Jan Eric Lenssen, NikolaosSarafianos, Tony Tung, and Gerard Pons-Moll.Pose-ndf:Modeling human pose manifolds with neural distance fields.In European Conference on Computer Vision (ECCV), 2022.3 Shashank Tripathi, Lea Muller, Chun-Hao P. Huang, TaheriOmid, Michael J. Black, and Dimitrios Tzionas. 3D humanpose estimation via intuitive physics. In Computer Visionand Pattern Recognition (CVPR), 2023. 2, 6, 7, 1",
  "Illia Polosukhin. Attention is all you need. Conference onNeural Information Processing Systems (NeurIPS), 2017. 6": "Timo von Marcard, Roberto Henschel, Michael J. Black,Bodo Rosenhahn, and Gerard Pons-Moll.Recovering ac-curate 3D human pose in the wild using IMUs and a mov-ing camera. In European Conference on Computer Vision(ECCV), 2018. 2, 7 Wenjia Wang, Yongtao Ge, Haiyi Mei, Zhongang Cai, Qing-ping Sun, Yanjun Wang, Chunhua Shen, Lei Yang, and TakuKomura. Zolly: Zoom focal length correctly for perspective-distorted human mesh reconstruction. In International Con-ference on Computer Vision (ICCV), 2023. 3 Jiahong Wu, He Zheng, Bo Zhao, Yixin Li, Baoming Yan,Rui Liang, Wenjia Wang, Shipei Zhou, Guosen Lin, Yan-wei Fu, et al. Ai challenger: A large-scale dataset for goingdeeper in image understanding. arXiv, 2017. 7",
  "Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao.ViTPose: Simple vision transformer baselines for humanpose estimation. In Conference on Neural Information Pro-cessing Systems (NeurIPS), 2022. 6": "Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang,Yebin Liu, Limin Wang, and Zhenan Sun. Pymaf: 3d humanpose and shape regression with pyramidal mesh alignmentfeedback loop.In International Conference on ComputerVision (ICCV), pages 1144611456, 2021. 3 Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, ShaoliHuang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and XiShen. T2m-gpt: Generating human motion from textual de-scriptions with discrete representations. In Computer Visionand Pattern Recognition (CVPR), 2023. 3, 6",
  "B.1. Data Preparation for Tokenizer": "For pose tokenization, we use 21 body pose parametersfollowing Vposer .As shown in Tab.3 of mainpaper, we evaluate our tokenization in two settings: in-distribution and out-of-distribution.For in-distribution,we train on the training set of AMASS and evaluateon the test set of AMASS. To show the efficacy of tok-enization, we also evaluate on an out-of-distribution yogadataset, MOYO . For training, we use the followingdatasets:{CMU, KIT, BMLrub, DanceDB, BMLmovi,EyesJapan, BMLhandball, TotalCapture, EKUT,ACCAD, TCDHands, MPI-Limits} with a weighting of{0.14, 0.14, 0.14, 0.06, 0.06, 0.06, 0.06, 0.06, 0.04, 0.04, 0.04,0.16}, respectively.",
  "B.3. Augmentations": "Data augmentation plays a pivotal role in enhancing the robustnessand generalization capabilities of HPS regressors. Hence, follow-ing HMR2.0, we perform various augmentations. These includerandom translations in both x and y directions with a factor of0.02, scaling with a factor of 0.3 and rotations with 30 degrees.Other augmentations include horizontal flipping and color rescal-ing. We observe that extreme cropping i.e. removing part of thehuman body limb in random also improves the robustness to oc-clusion.",
  "C.2. TALS loss vs Filtering Strategy": "Similar to HMR2.0, we employ filtering strategies to ensure high-quality 2D image alignment of the p-GT. Filtering strategies, how-ever, are all or nothing; i.e. data samples are either rejected orconsidered. Our TALS loss is different in that it uses all the filteredpseudo-ground-truth samples up to a threshold, after which the su-pervision is scaled down. This goes beyond standard filtering anddata cleaning pipelines.",
  "D.1. Poor 2D Alignment under Weak-perspectiveCamera Model": "The experimental analysis in Sec. 3 shows that using existingflawed camera projection models results in overfitting to 2D key-points and that this leads to learning biased poses. To avoid thisissue, we design a lenient TALS supervision training strategy andincorporate prior knowledge through our token-based pose repre-sentation. As shown in Fig. S.2 a), with the combination of loose2D supervision using TALS and built-in prior in representation,TokenHMR is able to estimate reasonable 3D poses but these donot always align well in 2D image when there is foreshortening.As expected under the weak-perspective camera model, the moreobvious the perspective distortion, the worse the 2D alignment.",
  "D.2. Failure Cases": "In this work, we introduce TokenHMR to reduce camera/pose biasand alleviate the ambiguity with a tokenized pose prior. How-ever, TokenHMR still has some limitations that could be furtherexplored in future work.As shown in Fig. S.2 b), foreshortening remains challengingwithout a better camera model. In cases like Fig. S.2 c), the globalorientation is ambiguous when only considering body cues. Wemay need to exploit more cues from the face and the feet to de-termine the correct global orientation. Future work could try toextend TokenHMR to full-body pose estimation (i.e. SMPL-X) toaddress this issue.",
  "Figure S.2. 2D alignment problem and failure cases": "Even with such improvements, we anticipate that the token rep-resentation retains value as it consistently improves performanceacross varied test scenarios. A promising next step is to extend thetokenization over time. Recent work on generating human mo-tion from text exploits tokenized representations of human mo-tions . Looking further ahead, an intriguing direction for fu-ture research involves exploring the application of our token-basedpose representation with Large Language Models (LLMs). Thediscrete, robust nature of our pose tokens, designed for 3D humanpose estimation, presents an opportunity to bridge the gap betweencomputer vision and natural language processing."
}