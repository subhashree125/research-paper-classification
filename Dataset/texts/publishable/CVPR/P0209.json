{
  "Abstract": "We introduce LAVITI, a novel approach to learning lan-guage, video, and temporal representations in long-formvideos via contrastive learning. Different from pre-trainingon video-text pairs like EgoVLP, LAVITI aims to align lan-guage, video, and temporal features by extracting meaning-ful moments in untrimmed videos. Our model employs aset of learnable moment queries to decode clip-level visual,language, and temporal features. In addition to vision andlanguage alignment, we introduce relative temporal embed-dings (TE) to represent timestamps in videos, which enablescontrastive learning of time. Significantly different from tra-ditional approaches, the prediction of a particular times-tamp is transformed by computing the similarity score be-tween the predicted TE and all TEs. Furthermore, existingapproaches for video understanding are mainly designedfor short videos due to high computational complexity andmemory footprint. Our method can be trained on the Ego4Ddataset with only 8 NVIDIA RTX-3090 GPUs in a day. Wevalidated our method on CharadesEgo action recognition,achieving state-of-the-art results.",
  ". Introduction": "In recent years, there has been a surge of interest in de-veloping egocentric video understanding models leveragingvideo-text pre-training, followed by finetuning for down-stream applications.A line of work aimingto learn transferable spatio-temporal features from largevideo-text datasets have been emerged.Methods suchas LAVILA showed how leveraging the dense nar-rations generated by Large Language Models (LLM) canbe beneficial for video-language pre-training.However,all such methods hit the memory and compute-bottleneckwhile processing video sequences each with a few num-ber of frames, leading to the reasoning capacity of thevideo models in a limited temporal context. Additionally,the above models do not use explicit temporal reasoning.",
  "*Work done during an internship at Intel Labs": "In this work, we propose LAVITI, aiming to align lan-guage, video, and temporal features by extracting meaning-ful moments in untrimmed videos and equip the model withlong-form temporal reasoning capability in an memory andcompute efficient way. LAVITI can be evaluated on zero-shot episodic memory tasks such as natural language query(NLQ), thanks to the integration of explicit temporal mod-eling over untrimmed videos into the pre-training objective.The key contributions of this work are: (1) aligning lan-guage, video, and temporal features by extracting mean-ingful moments in untrimmed videos; (2) formulating thevideo, language and temporal alignment as a direct set pre-diction problem; (3) enabling long-form reasoning overpotentially thousands of frames of a video in a memory-compute efficient way; (4) demonstrating the efficacy ofLAVITI by its superior performance on CharadesEgo ac-tion recognition; (5) Enabling zero-shot natural languagequery (NLQ) task without needing to train additional sub-networks or NLQ annotations.",
  ". Related Work": "In recent years, egocentric video-language pre-training(VLP) has been adopted significantly in academia and inindustry. A line of works such as EgoVLP , EgoVLPv2 learn transferable spatial-temporal representation fromlarge-scale video-text datasets.Recently, LaViLa showed that VLP can benefit from the dense narrationsgenerated by Large Language Models (LLMs). However,all such methods do hit the memory and compute bottle-neck while processing video sequences, each consisting ofa small number of frames (e.g. 8 or 16 frame models), lead-ing to limited temporal context aggregation capability. Onthe contrary, LAVITI is equipped with long-form reasoningcapability (1,000 frames vs 16 frames) and is not limited toa small number of input frames from a video sequence.",
  "Projection": "V-L Match & Align V-T Match & Align V3 T1T2TTT3 L1L2LM-1 Ts1Ts2TsM-1 Te1Te2TeM-1 . The architecture and training pipeline of LAVITI. Weuse a set of learnable queries to capture both visual and tempo-ral features, and directly predict the visual (V) and temporal (T)embeddings of potential moments, respectively. Predicted visualembeddings are aligned with ground-truth narration text embed-dings (L), and predicted TE are aligned with interpolated TE atground-truth timestamps. ity to efficiently process long-form videos. We use frozenCLIP vision and text encoders for feature extrac-tions. Temporal modeling is performed over the extractedCLIP visual features and learnable relative temporal embed-dings via a transformer encoder. We then create a setof learnable moment queries to directly predict the languageand temporal embeddings of moments in videos through atransformer decoder. The overall architecture and trainingof LAVITI is illustrated in .",
  ". Feature Extraction": "Different from spatio-temporal transformers , which canonly utilize a fixed number of frames, our method adoptsa post-temporal information injection strategy to enablelong-form temporal reasoning.We use the CLIP visionencoder, e.g., ViT , to extract the [cls] token fromeach frame independently. The visual features of a videocan be represented as a sequence of visual vectors V ={v1, v2, . . . , vT }, vt R1C, where T is the number offrames in the video, and C is the channel dimension of theframe features. We then save the extracted frame features tostorage, so that we can perform off-line pre-training withoutaccessing video data. This significantly reduces the compu-tation burden, and we can train on whole untrimmed videosrather than short-term video sequences. We then employ anon-overlap 1D convolution on V to generate T numberof visual tokens with a feature dimension of d, denoted byV RT d. For the pre-training purpose, the frozen text encoder is used to create the language embeddings of allnarrations by extracting the [eos] tokens. Given M num-ber of narrations in a video, we extract a set of languagevectors L = [l1, l2, . . . , lM], lj R1C, and each vector isL2-normalized.",
  ". Temporal Embedding and Encoder": "To inject temporal information, we create learnable tempo-ral embeddings T RT0d, representing all the timestampsof a video. T is initialized as 1D positional embeddings fol-lowing , where the cosine similarity between adjacentembeddings is larger distant ones. As T = T0, we interpo-late T to the same length of T as T, where each embed-ding can represent a particular timestamp relatively to thevideo length. The visual tokens V are added with temporalembeddings T as the video features M = V + T, whichis fed into a standard transformer encoder.",
  ". Decoder": "Instead of aligning a video clip [cls] token with the cor-responding text embeddings of the description of the clip(e.g., a narration), we view video-language pre-training onuntrimmed videos as a direct set prediction problem. Fol-lowing DETR , we create fixed-size learnable queriesQ RNd, where the video features M serve as keysand values to the decoder. Denote the output embeddingsof the decoder as Q, they are projected and L2-normalizedinto visual embeddings V RNC and temporal embed-dings T RN2d via feed-forward networks (FFNs),respectively.The output of the decoder is denoted byY = {(Vi, Tsi, Tei)}Ni=1, where temporal embeddingscorresponded to the start and end timestamps (si, ei) of de-tected moments.",
  ". Match and Alignment": "Since the narrations in the Ego4D dataset are annotatedwith a single timestamp rather than an interval, we augmenteach narration with a start and end timestamp. Differentfrom EgoVLP, a narrations start (or end) timestamp is de-termined by its previous and later narrations. For a narra-tion with timestamp tj, we uniformly sample a start and endtimestamp (sj, ej) as",
  "ej = Uniform(tj, tj+1).(1)": "For each narration, we can sample its corresponding tem-poral embeddings Tsj and Tej. Let the ground truth mo-ments be Y = {(Lj, Tsj, Tej)}Mj=1, we perform the bi-partite matching between Y and Y via Hungarian algo-rithm. Concretely, we compute 3 pairwise cosine similar-ities < {Vi}Ni=1, {Lj}Mj=1 >, < {Tsi}Ni=1, {Tsj}Mj=1 >,and < {Lei}Ni=1, {Lej}Mj=1 >. Each similarity matrix is followed by the Sigmoid activation, which allows for multi-label matching as there exist the same narrations or times-tamps within videos. The final cost is the negation of theelement-wise product of the 3 similarity matrices. Afterfinding the matched predictions and groundtruth, follow-ing SigLIP , we use Sigmoid contrastive loss to alignlanguage, vision and time of moments. For unmatched pre-dictions, we push their similarities with groundtruth to be-1 (equivalent to all labels of 0 in binary cross-entropy).",
  ". Implementation Details": "We use the Ego4D dataset for pre-training.Eachuntrimmed video is divided into chunks of 600 seconds fol-lowing for efficient data access. Our codebase is adoptedfrom OpenCLIP and LAVILA . We use the CLIP vi-sion encoder with ViT-H-14 backbone pre-trained on DFN-5B , and standard CLIP text encoder . The visual em-beddings are pre-extracted and stored locally with a strideof 5, namely we uniformly sample and compute the embed-dings of 6 frames per second. We train the model with 8NVIDIA RTX-3090 GPUs for 20 epochs with a batch sizeof 256, and a learning rate of 5 104 using the Adamoptimizer . It takes approximately 20 minutes to train 1epoch. The 1D convolution layer has a kernel size of 7 witha stride of 7, and the output number of channels d = 512.Both the transformer encoder and decoder has a stack of 6layers, with 8 attention heads and each head has a featuredimension of 64.",
  ". Action Recognition": "We evaluate our method on one of the downstream tasks,namely action recognition. We use the CharadesEgo dataset under both the zero-shot (ZS) and finetuning (FT)settings. We report video-level mAP as the evaluation met-ric following previous works . As our method is ca-pable of processing long-form videos, we use the wholevideo for training and testing without the need of sam-pling . For this task, we use the averaged similarityscores between all output embeddings {Li}Ni=1 with eachground-truth label.The results are shown in .In both settings,LAVITI outperforms all video foundation models by a largemargin. LAVITI outperforms GPT4Ego-L and LAVILA-Lunder the ZS setting by 3.0 and 5.6, respectively. LAVITIalso achieves 1.9 improvement over LAVILA-L under theFT setting. Comparing with other methods, we can per-form prediction with arbitrary number of frames instead offixed number of frames. It is also worth noting that bothGPT4Ego and LAVILA use LLMs for either training or test-ing to augment the language representations, whereas weuse a frozen text encoder.",
  ". Natural Language Query": "As LAVITI is capable of long-form video understandingwith explicit temporal alignment, the Ego4D Natural Lan-guage Query (NLQ) task is a natural fit with the pre-trainingtargets. We can directly predict intervals which are alignedwith language query given a video; therefore, LAVITI canperform the NLQ task under the zero-shot setting (withoutmodifications of the architecture and re-training on NLQannotations). We can directly use the text embedding ofthe question to match with the predicted visual embeddings{Vi}Ni=1, and select the top-K predicted temporal embed-dings {(Tsi, Tei)}Ki=1 as the response to the question. Wethen take the argmax indices of the similarities of the pre-dicted temporal embeddings with T, which can be mappedto timestamps. We follow the standard evaluation metricson NLQ, and report the recall@{1, 5} with IoU{0.3, 0.5}.The preliminary ZS results are list in .",
  ". Conclusions": "We devise a novel approach to learning language, video,and temporal representations in long-form videos via con-trastive learning, termed as LAVITI. Unlike existing meth-ods, LAVITI aims to align language, video, and temporalfeatures by extracting meaningful moments in untrimmedvideos by formulating it as a direct set prediction problem.Our method outperforms existing state-of-the-art methodsby a significant margin on egocentric action recognition, yetis trainable on memory and compute-bound systems.",
  "Alex Fang, Albin Madappally Jose, Amit Jain, LudwigSchmidt, Alexander Toshev, and Vaishaal Shankar. Data fil-tering networks, 2023. 3": "KristenGrauman,AndrewWestbury,EugeneByrne,Zachary Chavis, Antonino Furnari, Rohit Girdhar, JacksonHamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Mar-tin, Tushar Nagarajan, Ilija Radosavovic, Santhosh KumarRamakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray,Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, SiddhantBansal, Dhruv Batra, Vincent Cartillier, Sean Crane, TienDo, Morrie Doulaty, Akshay Erapalli, Christoph Feichten-hofer, Adriano Fragomeni, Qichen Fu, Abrham Gebrese-lasie, Cristina Gonzalez, James Hillis, Xuhua Huang, YifeiHuang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kot-tur, Anurag Kumar, Federico Landini, Chao Li, YanghaoLi, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu,Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, WillPrice, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari,Kiran Somasundaram, Audrey Southerland, Yusuke Sugano,Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, TakumaYagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Cran-dall, Dima Damen, Giovanni Maria Farinella, Christian Fue-gen, Bernard Ghanem, Vamsi Krishna Ithapu, C V Jawahar,Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe,Aude Oliva, Hyun Soo Park, James M Rehg, Yoichi Sato,Jianbo Shi, Mike Zheng Shou, Antonio Torralba, LorenzoTorresani, Mingfei Yan, and Jitendra Malik. Ego4D: Aroundthe world in 3,000 hours of egocentric video. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, 2022. 3 Ilharco, Gabriel and Wortsman, Mitchell and Wightman,Ross and Gordon, Cade and Carlini, Nicholas and Taori, Ro-han and Dave, Achal and Shankar, Vaishaal and Namkoong,Hongseok and Miller, John and Hajishirzi, Hannaneh andFarhadi, Ali and Schmidt, Ludwig.open clip: An opensource implementation of CLIP. 2, 3",
  "Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael": "Wray, Rui Yan, Eric Z Xu, Difei Gao, Rong-Cheng Tu, Wen-zhe Zhao, Weijie Kong, Chengfei Cai, Wang HongFa, DimaDamen, Bernard Ghanem, Wei Liu, and Mike Zheng Shou.Egocentric video-language pretraining. Advances in NeuralInformation Processing Systems, 35:75757586, 2022. 1, 3 ShramanPramanick,YaleSong,SayanNag,Kevin Qinghong Lin, Hardik Shah, Mike Zheng Shou,Rama Chellappa, and Pengchuan Zhang.EgoVLPv2:Egocentric video-language pre-training with fusion in thebackbone. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 52855297, 2023. 1,3 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, GretchenKrueger, and Ilya Sutskever.Learning transferable visualmodels from natural language supervision. In Proceedingsof the 38th International Conference on Machine Learning,pages 87488763. PMLR, 2021. 2, 3"
}