{
  "The University of Sydney, Camperdown Campus, Sydney, 2006 NSW, Australia": "AbstractPredicting plant species composition in specific spatiotemporal contexts plays an important role in biodiversitymanagement and conservation, as well as in improving species identification tools. Our work utilizes 88,987plant survey records conducted in specific spatiotemporal contexts across Europe. We also use the correspondingsatellite images, time series data, climate time series, and other rasterized environmental data such as land cover,human footprint, bioclimatic, and soil variables as training data to train the model to predict the outcomes of4,716 plant surveys. We propose a feature construction and result correction method based on the graph structure.Through comparative experiments, we select the best-performing backbone networks for feature extraction inboth temporal and image modalities. In this process, we built a backbone network based on the Swin-TransformerBlock for extracting temporal Cubes features. We then design a hierarchical cross-attention mechanism capableof robustly fusing features from multiple modalities. During training, we adopt a 10-fold cross-fusion methodbased on fine-tuning and use a Threshold Top-K method for post-processing. Ablation experiments demonstratethe improvements in model performance brought by our proposed solution pipeline. This work achieves a privateleaderboard score of 0.36242 in the GeoLifeCLEF 2024 LifeCLEF & CVPR-FGVC Competition, securing thirdplace in the rankings (Team name: Miss Qiu).",
  ". Background and related literature": "Predicting the composition of plant species over segmented time and spatial scales plays an importantrole in managing and protecting ecosystem biodiversity and improving species identification tools.Therefore, the LifeCLEF lab of the CLEF conference and the FGVC11 workshop of CVPR jointly hostthe GeoLifeCLEF 2024 competition centered around this task .We review the strategies of past winners in this competition. The fourth-place in 2022 mentions amethod for constructing \"Patched\" approaches, where variables from eight different modalities providedthat year are aligned into a single image format of (256,256,3). These modalities are then processedseparately by ResNet50 , and the output features are concatenated and passed through a linear layer,with Top-K processing applied to the outputs. In contrast, the second-place entry from the same year abandons data from other modalities and only uses remote sensing data, creating NDVI imageformat features using NIR and RGB data. They train multiple models based on ResNet50, DenseNet201,and Inception-v4, and fuse the logits.The champions strategy in 2023 introduces three backbonenetworks based on ResNet. The first network solely extracts features from bioclimatic raster data, while CLEF 2024: Conference and Labs of the Evaluation Forum, September 0912, 2024, Grenoble, France*Corresponding author.These authors contributed equally. (H. Liu); (P. Jiang); (Z. Tao); (M. Wan); (Q. Sun) 0009-0007-8115-0826 (H. Liu); 0009-0005-2930-706X (P. Jiang); 0009-0002-2276-6747 (Z. Tao); 0009-0004-5979-2943(M. Wan); 0000-0002-7103-1387 (Q. Sun)",
  "arXiv:2501.02649v1 [cs.CV] 5 Jan 2025": "the second and third networks each include three backbones for extracting features from bioclimaticrasters, satellite images, and soil rasters, with varying depths in these two networks. Finally, logits fusionis applied to these three models. At the same time, they adopted a three-step training strategy to attemptto learn the information in PO. These successful past entries and the official baseline significantlyinfluence our model development.",
  ". Our method": "Our study uses survey data of 11,255 species to train a machine-learning model for this goal. Eachsample in the dataset comprises satellite imagery and time series linked with geographicalcoordinates, climate time series , and other rasterized environmental data such as land cover,human footprint, bioclimatic, and soil variables.To effectively extract features across these modalities for accurate predictions, we propose thefollowing solution pipeline for machine learning. First, we create a graph structure based on theavailable data, treating SurveyIDs as nodes. Nodes are connected based on whether they fall within thesame ecological niche (less than 10 kilometers apart) and share geographical and annual similarities.Subsequently, we aggregate the labels of all adjacent nodes for each node, using these as new featuresfor that node. In our model, we use a Swin-Transformer-based method for extracting temporal features,instead of the commonly used Resnet18 network. Additionally, we provide an optional replacementfor the Swin-Transformer Tiny, used in the above baseline model for image feature extraction, withEfficientNet-B0. This substitution can significantly expedite model training with minimal impact onprediction accuracy. We also develop a hierarchical cross-attention mechanism (HCAM) to fuse featuresextracted from different modalities, which effectively uses information from multimodality data. Last, inthe post-processing steps, we improve the traditional Top-K rule for multi-task classification. Specifically,we integrate a series of thresholds for the Top-K rule for model outputs (see .7). We then usethe graph structure and the above model outputs for the final species prediction. The details of thisnovel solution pipeline is provided in .",
  ". Our proposed post-processing strategy combines the advantages of the threshold method andTop-K approach, also improving the final prediction accuracies": "Our model, named Tighnari, is inspired by the Forest Ranger character in the popular open-worldgame Genshin Impact, who is adept at identifying a variety of species in rainforests and is committedto ecological conservation. Just as Tighnari ensures the health of the rainforest ecosystems, we aspirefor our model to accurately predict the composition of plant species in given spatiotemporal contextsand make significant contributions to environmental protection. The models acronym, TIGH, standsfor T for Transformer, I for Image processing (computer vision) backbone, G for Graph feature extract,H for Hierarchical cross-attention fusion mechanism.",
  ": Visual comparison between NIR image and RGB image": "We find that time series data exhibit strong periodicity, which inspires us to consider whether foldingthe time series according to its periodicity to transform 1D data into 2D data might facilitate moreeffective feature extraction.Subsequently, we perform outlier detection in tabular data, followed by data cleaning and imputationof missing values. We group the data by year and region to observe the distribution of features and thecorrelations between them under different groupings.",
  ": Comparison of numerical feature boxplots in different regions": "The analysis reveal significant variations in feature distribution and correlations across differentregions, leading us to hypothesize that these variations could influence the distribution of species(shown in and ). This hypothesis is confirmed by further examining the prevalence ofleading species in different regions. Additionally, we note that feature distribution and correlations donot show significant differences between close years, but observable differences emerge across morewidely separated years. Hence, we conclude that sharing the same year and region is a prerequisite forestablishing an edge between two nodes (supporting label aggregation).Next, we visualize the geographic locations of PA ad PO Survey IDs to explore whether they exhibitany clustering tendencies (shown in ).",
  ": Visualization of survey occurrence locations in the PO,PA as well as test set": "The visualization indicates a tendency for the survey locations to cluster. We analyze some of thesesmaller clusters, calculating their radii, and estimated that the radius around each small cluster centerwas approximately 10 kilometers. To prevent a node from being overwhelmed by an excessive numberof adjacent nodes, which could lead to a reduction in the variance of the aggregated feature vectors, weintroduce further constraints on edge creation: no edge would exist between nodes if their geographicdistance exceeded 10 kilometers.We then visualize the labels, observing the frequency of species occurrences and the frequencydistribution of the number of species recorded per survey (shown in ). This analysis aids us insetting a reasonable range for K in the Top-K process. It can be observed that the optimal value of Kshould be between 0 and 100.",
  ": The frequency of species occurrences and the number of species included in surveys": "Finally, using survey IDs as nodes, we construct a graph based on the aforementioned rules. Weexamine the distribution of each nodes degree (the sum of edge weights) on the graph. We find thedistribution to be highly uneven, which could potentially degrade the quality of aggregated featureson the graph. Direct normalization of the aggregated feature vectors by the degree could excessivelydiminish the values corresponding to rarer species. Inspired by the Attention mechanism , whichnormalizes attention scores using a square root transformation, we adopt this technique to use thesquare root of the degree as the divisor for normalizing aggregated feature vectors.",
  ". Table Data Cleaning and Missing Value Imputation": "Initially, we group the metadata for both PA and PO by Survey ID. We then replace outliers in thegrouped metadata of PA and the test set with null values, and impute missing values with the mean.Categorical data were then one-hot encoded. Subsequently, for the label(speciesId), we encode eachelement according to its corresponding number using 0-1 encoding.Next, we access all tabular data for the training and test sets from the EnvironmentalRasters folder.We notice that the human footprint column contained 1 and extreme outliers. Based on the observeddata distribution, we set all values greater than 255 to 255 and those less than 0 to 1. Subsequently, wemerge all tables by Survey ID, replacing all infinite and infinitesimal values with nulls, and again imputeall missing values with the mean. Finally, the training sets of the PA and test sets were merged againwith the cleansed EnvironmentalRasters data by Survey ID to produce the cleaned tabular modalitydata.In the subsequent sections, we represent the features extracted from tables as .Following this, we clean the time series data by folding it into cubes. In the baseline method, all nullvalues in the Tensor are replaced with zeros. We change this replaced value to the mean of the Tensor.Additionally, as the Swin-Transformer cannot accept prime numbers as the dimensions of input imagematrices, we trim the shapes of two sets of time series cubes from (4, 19, 12) and (6, 4, 21) to (4, 18, 12)and (6, 4, 20) respectively. This is justified because the last year in the series already had many missingvalues, so trimming directly does not result in significant information loss.",
  ". Graph Construction and Utilization": "Graph construction and utilization are highlights of our work. To establish graph relationships amongsamples, we based our approach on two fundamental assumptions. First, we assumed a clusteringtendency in the spatial distribution of individual species, meaning that if a species appears in nodessurrounding a particular Survey ID, the likelihood of its occurrence at that Survey ID increases. Second,we hypothesized that within the same ecological environment, there is a correlation between thespatiotemporal distributions of different species, implying that samples (nodes) close in geographical location and year exhibit higher similarity in species composition. Our earlier data visualization effortsvalidated these assumptions.Our graph construction process was two-fold. The first step involved establishing a base graph foraggregating labels from adjacent nodes. Our rationale and actions are as follows: Visualization revealedsignificant differences in ecological characteristics and species distribution among different regions,and even within the same region across extensive years. Consequently, we determine that nodes beingin the same year and region was a prerequisite for an edge (supporting label aggregation).However, simply adding edges between nodes of the same year and region can result in highlyconsistent feature vectors due to label aggregation (with minimal variance), and the varying numbersof nodes across different regions and years can lead to significant imbalances in feature vector values,thus affecting feature quality. To counter this, we need to further restrict edge creation conditions.Visualization showed clustering tendencies in survey locations; we identify some smaller clusters andcalculate their radii, with an estimated radius of about 10 kilometers for each cluster center. Therefore,we stipulate that nodes over 10 kilometers apart would not be connected by an edge. Moreover, wewant the edge weight between two nodes to increase as their distance decreased, hence we set theedge weight as the maximum allowable distance (10 kilometers) minus the actual distance between twonodes. Based on these criteria, we add an edge for every pair of nodes that meet the conditions, thusforming the graph.Next, we need to establish rules for generating a graph feature vector (GFV) for each node throughlabel aggregation from adjacent nodes. We convert each nodes neighboring node labels into 0-1 encodedvectors, where the presence of a species is marked as 1 and absence as 0, resulting in each nodes labelbeing a 11255-dimensional vector. For any node, the weighted sum of all its neighboring nodes labelvectors and corresponding edge weights constitutes the nodes GFV. To avoid the severe imbalance in thenumerical distribution of GFV mentioned in the previous section, we adopt a square-root normalizationtrick similar to that used in attention mechanisms, using the square-root of the degree as the divisor forthe GFV. Additionally, to prevent global imbalances in the numerical distribution of GFVs among allnodes, we normalize all nodes GFVs and reassign them accordingly.Finally, we add each Survey ID from the test set as nodes to this graph, determining whether to createedges with existing training set nodes based on the established criteria (note that edges should not begenerated between test set nodes to avoid affecting the statistics of node degrees). We then aggregatethe GFVs for inference on the test set nodes. The specific calculation formula is as follows:",
  "= 10 6731 ,(2)": "where represents the ID of the current node, represents the ID of the node connected to it, represents the edge weight between these two nodes, represents the label vector passed through bythe nodes adjacent to the current node, represents the degree of the current node, and representsthe radian distance calculated between two points based on their latitude and longitude.",
  ": Call ComputeGFV()": "In the second step, we clone the graph with its nodes, edges, weights, years, regions, coordinates,and labels into a new graph, marking training and test set nodes with category labels. We then labelthe auxiliary nodes from the PO metadata table grouped by Survey ID according to the first stepsedge creation conditions and create edges with qualifying test nodes. For each test node, we selectthe adjacent training nodes with the highest weights (closest geographical locations) and identifyspecies appearing more than times, generating a list. We then select the auxiliary nodes with thehighest weights, identify species appearing more than times, and compile another list. Mergingand deduplicating these two lists provides a high-probability species list for each test node, used forcorrecting model output in post-processing. The default settings are: = 5, = 4, = 10, = 8.",
  ". Temporal Feature Extraction": "Another highlight of our work is the development of a temporal feature extraction method based onthe Swin-Transformer Block. This approach is inspired by Haixu Wu et al., who used a CNN-basedInception backbone network to extract features from folded time series data in TimesNet. Wu andcolleagues argue that this method, compared to traditional time series neural networks, is not only betterat capturing multi-scale sequential relationships but also has a stronger capability for spatio-temporalinformation fusion. It shows superior performance across various time series analysis tasks and offersmore efficient training and inference. Our goal in processing time series is to obtain higher qualityfeatures that are more conducive to modality fusion, rather than making better predictions about futuretime points. Therefore, we believe that using a visual backbone network to process time series cubesshould yield better feature extraction results than traditional time series models.In the current research on deep learning technology, whether for image processing or time seriesprediction tasks, methods based on Transformers are considered superior to those based on CNNs.Consequently, we experiment with a backbone network specially designed for extracting temporalfeatures using Swin-Transformer Blocks and Vision Transformer Blocks.Taking Swin-Transformer as an example, for time series cubes cropped to sizes (4, 18, 12) and (6, 4,20), we set the Patch size to (3,3) and (2,5), and the Window size to (3,2) and (2,3), respectively. We stacka Swin-Transformer Stage with a depth of 2 and 12 attention heads and another with a depth of 6 and24 attention heads to create the backbone network for handling our specific time series cubes. Theattention function can be defined mathematically by the equation:",
  "+ ),(3)": "where represents the query matrix, represents the key matrix, represents the value matrix, is the dimensionality of the keys and queries, typically used for scaling, represents the positionalencoding for time sequences. Unlike the classic Swin-Transformer, where a two-dimensional vectorindicates the absolute position of patches in an image, here we employ a one-dimensional vector torepresent the position of each patch in a flattened sequence. This modification better adapices totemporal tasks. Apart from that, the meanings of other symbols and formulas are the same as those inthe classic Swin-Transformer, and are not reiterated here .We depart from the standard practice of stacking four stages in the Swin-Transformer backbonenetwork because the dimensions of the time series cubes are too small. With each stacked stage, thedimensions for the subsequent stage are halved. Moreover, the Swin-Transformer Block does not acceptodd dimensions for input feature maps, making two blocks the most logical configuration for ourcurrent time series cube dimensions.The depths of the two stacked blocks are chosen to be 2 and 6, corresponding to the depths of thesecond and third stages in the classic Swin-Transformer backbone network. This configuration meansthat they extract shallow and deep features, respectively. This 1:3 depth ratio has also been adopted bysubsequent backbone networks, such as ConvNeXt .We select 12 and 24 as the attention head counts for the two stages, following the counts used in thethird and fourth stages of the classic Swin-Transformer backbone network. Increasing the number ofattention heads allows the model to independently capture features across more subspaces. Since thefeature map sizes entering the third and fourth stages in the original model are already quite small,similar to the size of our time series cubes, having many attention heads does not overly increase thecomputational burden. Therefore, we use as many attention heads as possible to more comprehensivelyextract features from the time series cubes. The input and output representations of the final model areas follows: = Temporal-Swin-T(),(4)",
  ": Comparison of Swin-Transformer for temporal cubes processing and image processing": "We give the comparison diagram to reveal the differences and connections between our model andthe classic Swin-Transformer (shown in ).Based on this rationale, we also design a Vision Transformer (ViT) backbone network for extractingtemporal information. To validate our approach, we design rigorous comparative experiments usingour new temporal feature extraction network to replace the Resnet18 used in the baseline forextracting time series features. Given the small size of the time series cubes relative to images, weprimarily choose small backbone networks to prevent overfitting and gradient vanishing.Our comparative backbone networks include the baselines ResNet18 , the improved versionof Inception, Xception41 from TimesNet , the CNN-based lightweight backbone networksEfficientNet-B0 and MoblieNetV3 , as well as our designed Swin-Transformer and Vision Transformer backbone networks for time series. The final experimental results demonstrate that ourcustom-designed temporal feature extraction networks perform optimally, with Swin-Transformershowing the fastest gradient descent and the best results on the private leaderboard.",
  ". Image Feature Extraction": "Although this is a multi-task classification problem, the low resolution of satellite images clearly doesnot allow for accurate prediction of plant species in a given region. Therefore, the primary motivationfor processing images is still to extract high-quality features that are conducive to modality fusion. Inthe baseline, the image feature extraction network employed is the tiny version of Swin-Transformer,which was presented in a Best Paper at ICCV 2021 . We note that most models are trained withinput sizes of (224,224) or (384,384). To better utilize the weights preserved in the pre-trained modeland retain the original information carried by the satellite images, we resize the satellite images from(128,128) to (224,224) before inputting them into the pre-trained model for fine-tuning. Experimentalevidence shows that this adjustment significantly enhances the models performance.To explore whether there are better alternatives, we conduct comparative experiments with othermodels such as EfficientNet-B0 , ConvNeXt-Base , and ViT-Base , all pre-trained with aninput size of (224,224). The choice of ConvNeXt-Base and ViT-Base is based on their status alongwith Swin-Transformer as the current state-of-the-art (SOTA) solutions in computer vision backbonenetworks, and they are comparable to the tiny version of Swin-Transformer in terms of the numberof parameters. EfficientNet-B0 was selected partly because it is one of the most powerful featureextraction networks among traditional CNN-based backbones, seen as a superior alternative to theResNet scheme. Moreover, EfficientNet-B0 is exceptionally lightweight and converges quickly, both interms of parameter count and training time, allowing for significant optimization of the overall modeltraining and inference time without substantial performance loss.Our comparative experiments reveal that using the ResNet18 scheme for both temporal and imagefeature extraction achieved the highest accuracy. However, using Swin-Transformer for temporal featureextraction and EfficientNet-B0 for image feature extraction reduced training time by approximately 75%without a significant decrease in accuracy, thanks to faster per-epoch training durations and overallfaster convergence. The input image is R.",
  ". Hierarchical Cross-Attention Fusion Mechanism(HCAM)": "Another significant highlight of our work is the introduction of a hierarchical cross-attention fusionmechanism to address the challenge of efficiently fusing feature vectors with varying informationdensities extracted from different modalities. In the aforementioned steps, we have extracted information(, , , ) from four modalities, including time-series modal (), satellite imagery modal (), andtabular modal features (), as well as graph modal features () we constructed and extracted ourselves. and represent and after being processed by fully connected layers.In terms of information density, satellite imagery modal features are the most dense because thebackbone network essentially compresses information carried by multiple channels of an image into alimited set of features for classification mapping via a fully connected layer. Time-series and tabularmodal features are less dense, as in our model, we attempt to map them to feature vectors of the sameor even higher dimensions. Graph modal features are the sparsest; although we aggregate features fromdifferent nodes, the majority of elements in a graph feature vector relative to all 11,255 dimensions arestill marked as 0.Initially, we try to use concatenation for modality fusion similar to the baseline, but due to the highdimensionality and sparsity of the graph feature vectors, the models loss reduction process is unstable. Consequently, we decide to use cross-attention, more commonly seen in multimodal learning, to attemptfusion of these modalities. However, cross-attention supports only the fusion of two modalities at atime, requiring six uses for pairwise fusion among four modalities, and still necessitating concatenationto integrate each cross-attention output. This not only increases computational overhead but also failsto ensure a controllable reduction in training loss. After multiple adjustments using cross-attention, wedetermine the optimal modality fusion structure, with specific operations and motivations as follows:Firstly, the features extracted from the satellite imagery modal are the densest in information.However, observations of the raw data reveal that satellite images primarily provide features of thelandscape and vegetation cover, such as color and density of foliage, at the location of the currentSurveyID. These features may map to higher-order latent features such as seasonal climate and ecologicalenvironment. Meanwhile, the time-series features record climate characteristics and vegetation changesof the area, and the tabular features mainly document the ecological environment characteristics of thearea. We consider the time-series and tabular features as two sets of queries, querying keys of climateand vegetation change features, and ecological environment features respectively, both derived from theimage features through two parallel linear layers. This setup allows the calculation of attention scores forthe time-series and image modalities on climate and vegetation changes, and for the image and tabularmodalities on ecological features. The outputs of the cross-attention from these two groups are thenconcatenated after being calculated from two sets of values mapped from the image features throughtwo parallel linear layers. This concatenated output serves as the final output of the cross-attentioncalculation for these three modalities. Simultaneously, when the features of the three modalities arefed into this cross-attention module, they are concatenated and mapped to the same dimension as theoutput of the cross-attention through a linear layer serving as a cutoff, and added together, forminga residual connection. This addition enhances the robustness of the cross-attention module duringtraining. The Cross attention(CA) function can be defined mathematically by the equation:",
  "CA = + Linear(2).(6)": "In our initial concept, we consider using the graph modal as the primary modality, given that its featuresare directly aggregated from the labels of adjacent nodes. Based on the assumptions mentioned at thebeginning of section 3.2, these features should closely approximate the current nodes label. Therefore,we intend to use the features from other modalities to correct the graph modal, aiming to achieve higheraccuracy. However, during training, the difference in information density between the graph modalfeatures and other modal features was too great. Whether through direct concatenation, mapping threemodalities features to the graph feature vectors dimension for addition, or through cross-attention, itwas ineffective in guiding the graph feature modal to generate accurate labels for prediction samples(nodes).In ecology, the composition of vegetation species in a specific spacetime is often determined byfactors such as climate conditions, ecological environments, and vegetation changes. Consideringthat the species appearing in a nodes adjacent nodes can represent these three major features of thenodes spacetime, albeit in a too sparse representational form, we decide to reduce the dimensionalityof the graph feature vector. We compress it to the same dimension as the concatenated vector of theother three modalities, then perform another multi-head cross-attention(MHCA) calculation. Here, theconcatenated vector of the other three modal features serves as the Query, with the compressed graphfeature vector being mapped as Key and Value by two parallel linear layers, calculating the multi-headcross-attention and outputting. Our rationale for this choice is that the concatenated vector of the three modal features represents the measured characteristics of climate conditions, ecological environment,and vegetation changes in the current spacetime, while the compressed graph feature vector representsthe observational results of these characteristics on the plant species combinations we are focusing on.We aim to reveal the causal relationships between them through cross-attention. The Multi-head crossattention function can be defined mathematically by the equation:",
  "output = Sigmoid(final).(8)": "This approach significantly improves the models performance during training. Observing the losscurves for the training and validation sets during the training process, it is evident that the modelsoverfitting was markedly reduced, and the loss on the validation set could decrease further.Throughout the process, we use two cross-attention layers, performing three cross-attention calcula-tions. Initially, we select the image modal feature with the highest information density and calculatecross-attention with the time-series and tabular modal features, which have relatively high informationdensities. Subsequently, we calculate the cross-attention between the features of the three modalities andthe compressed graph modal feature in one go. The selection of modal features for each cross-attentionlayer reflects a hierarchical relationship, thus we name this approach the Hierarchical Cross-AttentionFusion Mechanism. The schematic diagram of HCAM is as follows:",
  ". Mix up +10 Fold Cross Fusion training strategy": "We adopt the Mix up training strategy provided in the baseline, which is a very common method fordata augmentation during training. Specifically, this method involves randomly shuffling the orderof training data and labels in the current batch and then performing a weighted addition with theunshuffled training data and labels. The input matrices , , , and , as well as the label matrix ,are shuffled to create , , , , and :",
  "(9)": "This approach enhances the robustness of the training process, improves the generalization perfor-mance of the model, smooths out the distribution of samples across different categories, and makesoriginally sparse labels relatively dense.Moreover, to fully utilize the training data and reduce model overfitting, we employ a ten-foldcross-fusion technique to train the model. The concept of ten-fold cross-fusion is an improvementover ten-fold cross-validation. The specific operation involves dividing the dataset into ten parts, witheach part serving once as the validation set, while the other nine parts are used to train a brand newmodel. The logits output by these ten new models are then averaged. However, this approach alsoresults in a training efficiency about ten times lower than before. To address this drawback, I reasonthat the training datasets used for each model are highly overlapping, and except for the first model, thetraining of the subsequent nine models can be considered as fine-tuning the first model using a slightlyaltered dataset. Motivated by this, we optimize our training strategy. For the first model, we initializeparameters and train it using an early stopping strategy. For each subsequent model, we clone theparameters of the first model and fine-tune it on a newly combined training set. This method reducesthe number of training epochs for subsequent models, thereby enhancing training efficiency.Theoriginal dataset is ; we divide it into ten parts {1, 2, . . . , 10}. We train a model by settingeach as the test dataset: = Train (). Then we compute the average of logits from thesemodels:",
  "(13)": "In the baseline, the classic multi-class task method of Top-K is used to filter the models output.Typically, in conventional Top-K, a value for K is manually set or initially a range is predefined, withinwhich K is enumerated to observe which K yields the best performance on the validation set inferenceresults, and this K is then applied to the test set inference process.However, we observe drawbacks with this method. Some Survey IDs contain dozens or even hundredsof species. Although these species might have high probabilities in the models output, they are truncatedif they do not rank within the top K. Additionally, some test set Survey IDs, even with low probabilitiesfor each species, are still forced to output the top K species by probability rank.Therefore, we improve the Top-K algorithm by setting a range of thresholds (0.1 to 0.5, in steps of0.01) for each possible K to filter out species with predicted probabilities below these thresholds. Byexhaustively combining K and threshold values and comparing the scores of the validation set outputsprocessed through them, we can identify the optimal pair of K and threshold.Let the probability of the model output be = {1, 2, . . . , 11255}, where is the predictedprobability of species . We filter the output by setting the threshold and K values:",
  ": Heatmap of the threshold and K values correspond to the validation set score": "We note that there are 11,255 species IDs, but only 5,016 appear in PA, with the remainder in PO,prompting us to correct the models data through mining of the PO data. The specific approach involvesmerging and deduplicating the list of high-probability species for each test set node obtained in section3.2 with the models prediction results to produce the final output. In addition, We find that generatinga high-probability species list using only the PO data (auxiliary nodes) is more consistently beneficialcompared to generating a high-probability species list using both PO and PA data for result correction.",
  ". Experiments": "In this chapter, we present the experiments conducted to validate the superiority of the Proposal modeland analyze the experimental results. We omit the hyperparameter tuning process in the paper becauseeach backbone network corresponds to different model hyperparameters and training hyperparameters.Given the limited time, it is challenging to prove the optimality of the selected hyperparameters throughgrid search. Instead, we judge the current parameters potential to cause overfitting or underfitting byobserving the gradient descent process, or whether the current parameter combination results in lowerloss and higher scores compared to previous combinations.We perform exploratory tuning for eachmodel involved in the comparative and ablation experiments to ensure that the current hyperparametercombination is the best among all attempts. We also employ an early stopping strategy, which minimizesthe impact of hyperparameter changes on training when there is no significant overfitting or underfitting.At the end of this chapter, we provide a table of the hyperparameters used for the final submission.Weselect the following metrics to analyze the models performance, including public and private leaderboardscores on the official test set. Additionally, since we ultimately need to fuse the logits output by themodels, we introduce the loss on the validation and training sets. Since the calculation of BCE essentiallyequals the sum of entropy and KL divergence, it describes the difference between the probabilitydistribution of the model output and the distribution of the true labels. Therefore, the logits of modelswith lower validation and training set losses result in better fusion effects than models with lowervalidation scores but higher losses. Finally, we explore how to effectively lightweight the model, usingthe training time per epoch as a metric.Our experiments are all conducted on the Colab platform running on an A100 instance. The specificcomputational resources include 83.5GB of memory and 40GB of GPU memory.To facilitate the customization of model parameters, we use the Timm library instead of the torchvisionlibrary to instantiate models. Surprisingly, the baseline reconstructed based on the Timm libraryachieved a better private leaderboard score compared to the official baseline (the official baselines",
  "EfficientNet-B00.411140.333160.330850.003930.00338130": "Bold indicates the best score for this metric, while blue, if present, indicates a score close to thebest. Training loss is used to help determine if overfitting has occurred and does not represent modelperformance. We observe that, based on the scores, the best-performing model is ViT-Base, followedby EfficientNet-B0. However, our goal is to identify the models most suitable for logits fusion, so wealso focus on the loss. We find that ViT-Base and Swin-T-Tiny have very similar validation losses,making ViT-Base the preferred model. In terms of training time, EfficientNet-B0 and Swin-T-Tiny arethe most efficient models. Considering the number of epochs to convergence, EfficientNet-B0 reachesthe overfitting threshold in almost half the time of Swin-T-Tiny, but its test set loss is higher than that ofSwin-T-Tiny. Therefore, EfficientNet-B0 can be considered a successful attempt at model lightweighting.However, for the highest score after fusion, the focus should still be on ViT-Base and Swin-T-Tiny.Because the comparative experiments of the temporal feature extraction network and the imagefeature extraction backbone network are conducted simultaneously, the corresponding image featureextraction network during the comparative experiments of the temporal feature extraction network isstill the Swin-T-Tiny from the baseline. The comparative experiments are as follows:",
  "ModuleVal scorePublic scorePrivate scoreVal lossTraining lossTime cost": "ResNet180.421100.327740.328160.003790.00353271Swin-T0.423190.334280.334760.003770.00362279ViT0.417940.316800.317720.003900.00339277EfficientNet-B00.420830.319400.319960.003810.00344292MobileNet-V30.414790.314350.316490.003900.00339284Xception410.427370.324880.322160.003770.00331302 Based on the experimental results, the Swin-T backbone network we proposed for extracting temporalfeatures achieves the highest private leaderboard score and the lowest validation loss. It is also the onlybackbone network that scored higher than the Baseline. In terms of training time and convergenceepochs, it is comparable to the Baseline. Therefore, we believe that the Swin-T backbone network canreplace ResNet18 in the Baseline.",
  ". Ablation Studies": "Based on the conclusions drawn from the comparative experiments, we initially replace the backbonenetworks for image and temporal feature extraction starting from the Baseline model. Subsequently, weattempt to use the Swin-T backbone network for temporal feature extraction and ViT-Base for satelliteimage feature extraction, but encounter difficulties with gradient descent. We analyze this issue and found that the significant difference in the number of parameters between the two networks, alongwith the misalignment in the gradient descent speeds, led to this problem. Since we cannot effectivelyresolve this issue, we opt to use Swin-T-Tiny, which has a similar validation loss, as the satellite imagefeature extraction network. We also try replacing Swin-T-Tiny with EfficientNet-B0, but this onlyaccelerates training efficiency without improving the scores. Therefore, we determine that the optimalbackbone network for temporal feature extraction is Swin-T, and for satellite image feature extractionis Swin-T-Tiny.Next, we attempt to introduce the Graph modality. We find that models incorporating the Graphmodality showed significant improvement in validation scores and the lowest validation loss so far,but the public and private leaderboard scores decrease. Upon examining the output, we discover thatincluding the Graph modality makes the model more aware of some minority classes that are oftenoverlooked, significantly increasing their logits values. However, under the Top-K output rules, theseminority classes still do not rank high enough, and not all boosted minority classes are present. Thosethat do not appear show up in the logits values of some Survey IDs with generally low confidence. Thisresult in better validation loss but lower private leaderboard scores. This phenomenon inspire us topropose the threshold Top-K as an improved post-processing algorithm.We then try using HCAM (Hierarchical Cross-Attention Mechanism) for feature fusion. We findthat the validation loss of models fused with HCAM slightly increased, but the public and privateleaderboard scores return to the levels before integrating the Graph modality. This demonstrates thatHCAM effectively optimizes the logits distribution while improving model scores.To verify whether the model incorporating GFV (Graph Feature Vector) + HCAM is indeed moresuitable for ten-fold cross-validation fusion, we conduct ten-fold cross-validation training on the modelwith GFV + HCAM and the baseline model using Swin-T for temporal feature extraction and Swin-Transformer-Tiny for image feature extraction with feature concatenation for modality fusion. Wefind that the model incorporating GFV + HCAM scored higher on the leaderboard and validation setcompared to the control group.It is important to note that the validation set mentioned in the ten-fold cross-validation method isthe same as the validation set used in previous ablation experiments. However, since the completetraining set is used in ten-fold cross-validation, the data in the validation set is actually included inthe training set. Therefore, the validation set scores here are higher than in experiments withoutten-fold cross-validation, and should only be compared between experiments using the same ten-foldcross-validation method.In the official competition submission, we overlook checking the output of the model incorporatingGFV + HCAM and simply judge the failure of our GFV construction and HCAM design based on thepublic leaderboard score. Consequently, we choose the previous model with ten-fold cross-validationtraining and applied post-processing. However, when completing this paper, we realize that the modelincorporating GFV + HCAM might have more potential for fusion. Subsequent experiments confirmethat this approach can indeed provide further improvements in leaderboard scores.We acknowledge that the current results lack persuasive power for the ablation experiments onHCAM and Graph modality features. Besides the reasons we analyzed, HCAM and the MLP used forcompressing Graph modality features are the parts of our model structure that are more sensitive toparameter changes. Our lack of time for sufficient hyperparameter analysis is also one reason for theunsatisfactory ablation experiment data. In future work, we will explore the optimal parameters andstructures for these two sub-networks.Finally, we apply our proposed post-processing methods to the model outputs. From the ablationexperiments, it is evident that both Threshold Top-K and corrections based on PO data improved modelscores. However, since the validation set cannot contain species present only in PO, the validation setscores are inaccurate in this context. Furthermore, the post-processing operations do not affect themodels training and inference efficiency, so they do not need to be included in the time cost comparison.The final results are summarized in the table below(* denotes that the expected improvement is notachieved, thus this approach is abandoned):",
  "Baseline0.421100.327740.32816271Swin-T replace ResNet0.423190.334280.33476279": "EfficientNet replace Swin-Transformer-Tiny*0.411140.333160.33085130EfficientNet and Swin-T*0.408240.308720.31349137Graph Modal Feature Vector (GFV)0.439500.322510.32543290HCAM with GFV0.424890.331860.3335829210 Fold Cross Fusion without HCAM and GFV*0.491200.347900.34625-10 Fold Cross Fusion with HCAM and GFV0.504550.351700.34994-Threshold Top-K0.526550.364280.36121-Output Correction (Final Model)-0.364780.36242- To provide a more intuitive confirmation of our analysis on the training process losses of differentmodels discussed in this chapter, we visualize the training processes of all models. This facilitates thecomparison of the number of epochs required for different models to reach their optimal loss (shown in).",
  ". Conclusion": "Our comprehensive comparative experiments demonstrate that we select the most suitable featureextraction backbone networks for each modalitys data. Through rigorous ablation studies, we prove thateach improvement we proposed incrementally enhance the models performance, ultimately achievinga score of 0.36242 on the private leaderboard and in third place(The leaderboard showed 0.35292, but wecontinue to optimize some parameters while doing the experiment to finally get 0.36242). In additionto proposing high-scoring solutions, we also introduce a lightweight model and an efficient trainingstrategy. Without significantly sacrificing accuracy, we significantly reduce the training time for theten-fold cross-fusion model (by more than 50%) and the time for a single epoch and the total number ofepochs (by approximately 75%). Thus, we manage to train ten models for logits fusion in roughly thesame amount of time it previously takes to train one baseline model, achieving prediction results farexceeding the baseline score. It is worth mentioning that we average the outputs of all models thatscore higher than the baseline and apply post-processing, ultimately achieving a private leaderboardscore of 0.36501.In summary, from a theoretical perspective, our research provides new insights into the extraction oftemporal information for modality fusion tasks, namely folding it into a 2D matrix and extracting featuresthrough a visual backbone network. We also design a robust fusion method for features extracted frommultiple modalities with varying information densities, using a hierarchical cross-attention mechanismto dilute features from high to low density progressively. Additionally, we propose a graph-based featureconstruction method and output correction post-processing algorithm for the multi-task classificationtask of species prediction under specific spacetime, which often involves extremely unbalanced orsparse labels. From an application perspective, our research is of significant importance in fields suchas ecology, agriculture, environmental protection, and climate change studies.",
  "Despite of our comprehensive comparative and ablation experiments, our work may be further exploredin the following way:": "We will further utilize the organized PO data, noting that there are some high-quality publisherswithin PO whose surveys of the same ID often contain many species. Additionally, the competitionprovides supplementary data about other modalities for Survey IDs in PO. We believe that theseSurvey IDs appearing in PO can be selected through certain logical criteria to serve as trainingsamples for weakly supervised learning, incorporated into PA. This approach will better leveragecrowdsourced data, reduce the workload of data collection, and enhance model accuracy. Referring to past programs, highly ranked teams would extract the rasters around the SurveyID geographic location from the tiff files provided by EnvironmentalRasters as images to beprocessed. However, due to arithmetic and time constraints, we finally give up on implementingthis scheme, and we plan to use this part of the data in our subsequent work to realize a leap inmodel performance. Our current model establishes graph relationships solely for feature aggregation and resultcalibration. In future work, we plan to use graph neural networks to replace the current methodof manually setting weights for adjacent nodes in feature aggregation. This will support the useof weakly supervised and semi-supervised learning training strategies to progressively correctlabels of weakly supervised and semi-supervised nodes, thereby improving training outcomes. We hope to introduce NAS technology to optimize the hyperparameters of our 2D time seriesfeature extraction network based on Swin-Transformer and the hierarchical cross-attentionmechanism, further enhancing the models performance. The data for this paper is organized and published by INRIA. We express our gratitude to all theinstitutions and individuals involved in data collection and processing, including but not limited to theGlobal Biodiversity Information Facility (GBIF, www.gbif.org), NASA, Soilgrids, and the Ecodatacubeplatform. Additionally, this project has received funding from the European Unions Horizon Researchand Innovation program under grant agreements No. 101060639 (MAMBO project) and No. 101060693(GUARDEN project) . All authors contribute helpful ideas during the course of the competition andparticipate in writing and revising the paper, so all authors are co-first authors. All of the authors, ascorresponding authors, are obliged to reply to emails to provide readers with the relevant code and dataof this work and explain the details of the work. Among them, Haixu Liu, as the first correspondingauthor, is responsible for the necessary communication for the publication of the article. Our finalsubmission CSV download link is as follows: submission_036242.csv and submission_036501.csv. Thelink to the code we use to run and obtain the final submission is as follows: code. L. Picek, C. Botella, M. Servajean, B. Deneu, D. Marcos Gonzalez, R. Palard, T. Larcher, C. Leblanc,J. Estopinan, P. Bonnet, A. Joly, Overview of GeoLifeCLEF 2024: Species presence prediction basedon occurrence data and high-resolution remote sensing images, in: Working Notes of CLEF 2024 -Conference and Labs of the Evaluation Forum, 2024. A. Joly, L. Picek, S. Kahl, H. Goau, V. Espitalier, C. Botella, B. Deneu, D. Marcos, J. Estopinan,C. Leblanc, T. Larcher, M. ulc, M. Hrz, M. Servajean, et al.,Overview of LifeCLEF 2024:Challenges on species distribution prediction and identification, in: International Conference ofthe Cross-Language Evaluation Forum for European Languages, Springer, 2024. C. Leblanc, A. Joly, T. Lorieul, M. Servajean, P. Bonnet, Species distribution modeling based onaerial images and environmental features with convolutional neural networks, in: CLEF (WorkingNotes), 2022, pp. 21232150.",
  "H. Q. Ung, R. Kojima, S. Wada, Leverage samples with single positive labels to train cnn-basedmodels for multi-label plant species prediction, Working Notes of CLEF (2023)": "A. Joly, C. Botella, L. Picek, S. Kahl, H. Goau, B. Deneu, D. Marcos, J. Estopinan, C. Leblanc,T. Larcher, et al., Overview of lifeclef 2023: evaluation of ai models for the identification andprediction of birds, plants, snakes and fungi, in: International Conference of the Cross-LanguageEvaluation Forum for European Languages, Springer, 2023, pp. 416439. P. Potapov, M. C. Hansen, I. Kommareddy, A. Kommareddy, S. Turubanova, A. Pickens, Q. Ying,Landsat analysis ready data for global land cover and land cover change mapping, Remote Sensing12 (2020) 426. M. Witjes, L. Parente, C. J. van Diemen, T. Hengl, M. Landa, L. Brodsk, L. Gluica, A spatiotemporalensemble machine learning framework for generating land use/land cover time-series maps foreurope (20002019) based on lucas, corine and glad landsat, PeerJ 10 (2022) e13573.",
  "M. Tan, Q. Le, Efficientnet: Rethinking model scaling for convolutional neural networks, in:International conference on machine learning, PMLR, 2019, pp. 61056114": "A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, H. Adam, Mobilenets: Effi-cient convolutional neural networks for mobile vision applications, arXiv preprint arXiv:1704.04861(2017). A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, N. Houlsby,An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprintarXiv:2010.11929 (2020)."
}