{
  "Abstract": "3D LiDAR point cloud data is crucial for scene perception in computer vision, robotics, andautonomous driving. Geometric and semantic scene understanding, involving 3D point clouds, isessential for advancing autonomous driving technologies. However, signi!cant challenges remain,particularly in improving the overall accuracy (e.g., segmentation accuracy, depth estimationaccuracy, etc.) and e\"ciency of these systems.",
  "To address the challenge in terms of accuracy related to LiDAR-based tasks, we present": "DurLAR, the !rst high-!delity 128-channel 3D LiDAR dataset featuring panoramic ambient (nearinfrared) and re#ectivity imagery. Leveraging DurLAR, which exceeds the resolution of priorbenchmarks, we tackle the task of monocular depth estimation. Utilizing this high-resolution yetsparse ground truth scene depth information, we propose a novel joint supervised/self-supervisedloss formulation, signi!cantly enhancing depth estimation accuracy.",
  "To improve e\"ciency in 3D segmentation while ensuring the accuracy, we propose a novel": "pipeline that employs a smaller architecture, requiring fewer ground-truth annotations whileachieving superior segmentation accuracy compared to contemporary approaches. This is facili-tated by a novel Sparse Depthwise Separable Convolution (SDSC) module, which signi!cantlyreduces the network parameter count while retaining overall task performance. Additionally,we introduce a new Spatio-Temporal Redundant Frame Downsampling (ST-RFD) method thatuses sensor motion knowledge to extract a diverse subset of training data frame samples, therebyenhancing computational e\"ciency.",
  "Furthermore, recent advancements in 3D LiDAR segmentation focus on spatial positioning": "and distribution of points to improve the segmentation accuracy. The dependencies on coordinatesand point intensity result in suboptimal performance and poor isometric invariance. To improvethe segmentation accuracy, we introduce Range-Aware Pointwise Distance Distribution (RAPiD)features and the associated RAPiD-Seg architecture. These features demonstrate rigid transfor-mation invariance and adapt to point density variations, focusing on the localized geometry ofneighboring structures. Utilizing LiDAR isotropic radiation and semantic categorization, theyenhance local representation and computational e\"ciency.",
  "In the PhD journey of my Durham life, where bugs are plenty and coffee never enough, there": "exists a debugger, Prof. Toby Breckon, my supervisor. Like a seasoned developer thrown into thedepths of legacy code, he finds potential in the mess that is my initial research attempts. Toby isnot just the architect of my academic growth; he is also the RAM to my overloaded processes,providing support and resources when my system is on the brink of a meltdown. For his endlesspatience and guidance, I offer my eternal cache of gratitudeknowing full well it is a debt of CPUcycles I can never fully repay.",
  "I would like to thank the dear postdocs in VIViD group (Vision, Imaging, and Visualisation in": "Durham) Neelanjan Bhowmik, Yona Falinie Binti-Abd-Gaus and Brian Isaac-Medina; fellow Ph.Dstudents Jiaxu Liu, Ghada Alosaimi, Yixin Sun, Wenke E, Ruochen Li, Shuang Chen, HaozhengZhang, Xiatian Zhang, Ziyi Chang, Xiaotang Zhang; VIViD food scientist Mingze Hou. They arethe collaborative code reviewers, the Git to my Hub, o$ering commits of support and branchingout distractions necessary to save me from the abyss of constant deadlines. They are the commentsin my life program, o$ering happiness and humor, ensuring I dont lose myself to the in!niteloops of academia. To them, my heart beams a 404-error-free THANK YOU.",
  "The fundamental support architecture of my lifemy parentsare the original developers of": "my being. Their unconditional love and support compiled me into the person I am today, evenwhen my presence is more akin to an elusive ghost variable in their lives, especially when I amstudying abroad in UK. Their love, wisdom, and support are the keystrokes that navigate methrough lifes tricky syntax. For their sacri!ces, I am forever in their while true debt loop.",
  "(a) Ambient(b) Reflectivity": ".2: Comparison of ambient and reflectivity imagery derived from LiDAR.(a) Ambient: the colors represent the intensity of the ambient near-infrared light, withbrighter colors indicating higher intensity and darker colors indicating lower intensity.(b) Reflectivity: the colors represent the reflective properties of various surfaces withinthe scene. Brighter colors indicate surfaces with higher reflectivity, while darker colorsrepresent less reflective surfaces.",
  "Ground TruthPredicted": ".4: Illustration of IoU calculation for semantic segmentation. The greenregion represents the ground truth segmentation, while the blue region represents thepredicted segmentation. True Positives (TP) are the overlapping area between groundtruth and prediction, False Positives (FP) are the predicted area not overlapping with theground truth, and False Negatives (FN) are the ground truth area not covered by theprediction. True Negatives (TN) are the areas correctly identified as not belonging to thetarget class. The IoU is calculated as the ratio of the TP area to the union of TP, FP, andFN areas.",
  ". Evaluation Results": ".2: Performance comparison over the KITTI Eigen split , Cityscapes (self-supervised only) and DurLAR datasets (joint supervised/self-supervised, +S v.s.self-supervised). All models are trained and tested on the same dataset, without cross-dataset evaluation. Depth evaluation metrics (.8) are shown in the top row. Redrefers to superior performances indicated by low values, and green refers to superiorperformance indicated by a higher value. The best results in KITTI and DurLAR are inbold; the second best in DurLAR are underlined.",
  "mIoU %SemanticKITTInuScenes": ".1: Le#: RAPiD exhibits excellent viewpoint invariance and geometric stability,visualizing comparable features around the vehicle door structure at varying rangesand viewpoints (feature matrix plots inset). Middle: RAPiD is distinctive in di\"erentsemantic classes, as visualized by the matrices. Embedded RAPiD pa#erns correspondingto di\"erent points are visualized using a spectrum of colors, showcasing their capacity torepresent di\"erent classes. Right: Our RAPiD-Seg achieves superior results over SOTAmethods on nuScenes and SemanticKITTI .",
  "G": ".5: Visual illustration of R-RAPiD and C-RAPiD. R-RAPiD (le!) confines theRoI to the ring surrounding anchor points A (e.g., B, C, D, and E), optimizing computa-tional e\"iciency by leveraging the structural characteristics of LiDAR data. C-RAPiD(right) focuses on point features within the same semantic class (e.g., A, B, C, D, E, F,and G), preserving feature embedding fidelity.",
  "mIoU: 73.4mIoU: 68.9": ".9: $alitative comparisons with PCSeg and groundtruth through errormaps on SemanticKITTI validation set. To highlight the di\"erences, the correct/ incorrect predictions are painted in gray / dark red, respectively. Each scene isvisualized from the ego-vehicle LiDAR birds eye view (BEV) and covers a region of 50mby 30m. Best viewed in color.",
  "barrbicybuscarconstmotorpedconetrailtruckdrivothwalkterrmadeveg": ".10: $alitative comparisons with PCSeg and groundtruth through errormaps on nuScenes validation set. To highlight the di\"erences, the correct / incorrectpredictions are painted in gray / dark red, respectively. Each scene is visualized fromthe ego-vehicle LiDAR birds eye view (BEV) and covers a region of 50m by 40m. Bestviewed in color.",
  "J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss, and J. Gall, Se-": "manticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences, in Int.Conf. Comput. Vis., pp. 92969306, Oct. 2019. xii, xiii, xiv, xvi, xvii, 1, 2, 3, 17, 18, 20, 31, 36,42, 59, 64, 65, 67, 71, 72, 73, 74, 75, 77, 78, 80, 81, 82, 83, 86, 87, 91, 99, 100, 102, 109, 111, 115,117",
  "S. Thrun, M. Montemerlo, H. Dahlkamp, D. Stavens, A. Aron, J. Diebel, P. Fong, J. Gale,": "M. Halpenny, G. Ho$mann, K. Lau, C. Oakley, M. Palatucci, V. Pratt, P. Stang, S. Strohband,C. Dupont, L. E. Jendrossek, C. Koelen, C. Markey, C. Rummel, J. van Niekerk, E. Jensen,P. Alessandrini, G. Bradski, B. Davies, S. Ettinger, A. Kaehler, A. Ne!an, and P. Mahoney,Stanley: The robot that won the DARPA grand challenge, J Field Robot., vol. 23, no. 9,pp. 661692, 2006. 50"
}