{
  "Image+ TrajectoryGenerated Views Along Trajectory": ".Given a single input image, MultiDiff synthesizes consistent novel views following a desired camera trajectory. These syn-thesized views harmonize well even in areas unseen from the reference view. Examples from RealEstate10K (top two rows) andScanNet (bottom row) test sets demonstrate that our model can handle large camera changes and challenging perspectives.",
  "Abstract": "We introduce MultiDiff, a novel approach for consistentnovel view synthesis of scenes from a single RGB image.The task of synthesizing novel views from a single referenceimage is highly ill-posed by nature, as there exist multiple,plausible explanations for unobserved areas. To addressthis issue, we incorporate strong priors in form of monoc-ular depth predictors and video-diffusion models. Monoc-ular depth enables us to condition our model on warpedreference images for the target views, increasing geomet-ric stability. The video-diffusion prior provides a strongproxy for 3D scenes, allowing the model to learn contin-uous and pixel-accurate correspondences across generatedimages. In contrast to approaches relying on autoregres-sive image generation that are prone to drifts and erroraccumulation, MultiDiff jointly synthesizes a sequence offrames yielding high-quality and multi-view consistent re- sults even for long-term scene generation with large cam-era movements, while reducing inference time by an orderof magnitude. For additional consistency and image qualityimprovements, we introduce a novel, structured noise dis-tribution. Our experimental results demonstrate that Multi-Diff outperforms state-of-the-art methods on the challeng-ing, real-world datasets RealEstate10K and ScanNet. Fi-nally, our model naturally supports multi-view consistentediting without the need for further tuning. Project page:",
  "arXiv:2406.18524v1 [cs.CV] 26 Jun 2024": "only requires a single input image and a user-defined free-form camera trajectory that may deviate substantially fromthe reference view. Providing a solution to this problem un-locks applications in virtual & augmented reality and 3Dcontent creation, where generating immersive and multi-view coherent scenes is paramount.Many existing, state-of-the-art approaches for novel viewsynthesis are reconstruction-based (e.g., by optimizing aNeural Radiance Field from a fixed number of inputviews), and are thus inherently limited in generating high-quality novel views for areas without sufficient training cov-erage. In contrast, we leverage diffusion-based, generativeapproaches , that are capable of producinghigh-quality, single images or individual, simple 3D ob-jects, due to their ability of learning powerful (conditional)image priors. Despite significant progress, these modelsare still unable to synthesize several, multi-view consistentviews of large scenes. This is largely due to the lack of in-herent 3D modeling capabilities, the absence of large-scale3D ground truth datasets, but also the ill-posed nature of theproblem, requiring more sophisticated methodological ad-vances. Ultimately, we are aiming for a solution that i) gen-erates seamlessly aligned and multi-view consistent outputimages w.r.t. a given input image, ii) maintains both highvariability and fidelity in occluded regions and previouslyunseen areas, and iii) extends to camera trajectories wellbeyond the provided input reference image viewpoint or asimplistic 360 panoramic view.Some recent works have approached consistent view ex-trapolation by leveraging an autoregressive approach: LookOutside the Room is a transformer-based approachcombined with locality constraints w.r.t. the input camerasfor enforcing consistency among generated frames. Sim-ilarly, Pose-Guided Diffusion Models apply attentionalong epipolar lines to condition a diffusion model. Pho-toNVS also proposes an autoregressive attempt wherethe diffusion model is conditioned on a reference view and aspecialized representation for relative camera geometry. Asignificant drawback of autoregressive models is their ten-dency to error accumulation . Repeatedly condi-tioning the model on its previously generated frames canturn minor output deficiencies quickly into undesirable andsemantically meaningless results particularly on longer-term trajectories. In contrast, Diffusion with Forward Mod-els (DFM) trains a diffusion model to directly samplefrom the distribution of 3D scenes, inherently improving3D consistency. However, DFM is computationally expen-sive, limited to low image resolutions, slow at inference,and cannot directly integrate 2D diffusion priors. The goalof our work is to overcome both the main limitations of au-toregressive works and enabling fast and significantly morestable, long-term generation of novel views.To this end, we propose MultiDiff, a novel and improved, latent diffusion model-based approach for novel view syn-thesis, given a single reference image and a pre-defined tar-get camera trajectory as input. We address the challenge ofgenerating pixel-aligned, multi-view consistent image se-quences by incorporating strong and complementary priors,significantly constraining the ill-posed nature of the task.Geometric stability is improved by integrating a monocu-lar depth prior, where we condition our model on warpedreference images for desired novel views, using off-the-shelf but potentially noisy monocular depth estimators. Wealso introduce a structured noise distribution for improvingmulti-view consistency, applying the aforementioned warp-ing procedure to the reference image noise and hence gen-erating correlated 3D noise in all overlapping target views.By integrating a video diffusion model prior, we are ableto compensate for missing and geometrically inconsistentreference image warpings due to potential issues with themonocular depth estimator. Video priors provide a strongproxy for 3D scene understanding, enhancing temporal con-sistency by largely reducing flickering artifacts particu-larly for long-trajectory view synthesis. However, their lackof explicit camera control makes their integration nontrivialfor view extrapolation.In order to avoid error propagation issues as observed withautoregressive models, we synthesize entire sequences ofnovel views in a concurrent and efficient way. Finally, dueto our conditioning, we can additionally edit our generatedscenes, allowing for direct and intuitive interaction with ourmodel. We summarize our main contributions as follows: We address the ill-posed view extrapolation problem byintegrating priors from monocular depth estimators andvideo diffusion models for learning pixel-wise correspon-dences using novel techniques for spatial-aware condi-tioning across predicted sequences. We simultaneously predict multiple frames for a targetsequence, overcoming error accumulation of autoregres-sive methods, while retaining higher resolution at reducedcomputational costs compared to methods directly sam-pling from the distribution of 3D scenes.",
  ". Related Works": "ImageandVideoDiffusion.DiffusionModels(DMs) are powerful generative modelsthat have achieved state-of-the-art results in uncondi-tional as well as class- and text-guided image synthe-sis .Recently, DMs have been extended to the task of videosynthesis . While recent video DMscan be conditioned on different modalities such as text orimages , they do not enable explicit control thecamera viewpoint in the generated videos. Nonetheless, the temporal consistency learnt by these models is a powerfulprior that we can leverage to tackle the task of novel viewsynthesis in an underconstrained setting. Specifically, weuse the publicly available VideoCrafter1 to initializethe correspondence attention layers in our pipeline. Regression-Based Models for Novel View Synthesis.The goal of novel view synthesis (NVS) is to produce re-alistic images of a given instance or scene from previouslyunseen camera viewpoints. Earlier approaches require hun-dreds of posed training images per instance and optimizeeach instance individually .Bylearning priors across multiple training scenes, more recentworks enable NVS from only one or a few images at in-ference.These methods optimize a regression objective, i.e. an L1or L2 loss to reconstruct the training images. While thisallows for impressive results on interpolation near inputviews, regression-based NVS approaches struggle with re-construction ambiguity and longer-range extrapolations .As our goal is to synthesize novel views far beyond ob-served views, we instead train a generative model. Generative Models for Novel View Synthesis.To bettermodel reconstruction ambiguity and long-range view ex-trapolation, multiple recent works deploy generative mod-els for NVS. Earlier works use GANs ,VAEs , or autoregressive models . Interest-ingly, GeoGPT directly models long-range 3D corre-spondences between source and target views with an autore-gressive transformer, demonstrating that an intermediate 3Drepresentation may not be needed for NVS from a single im-age. More recently, diffusion models have achieved impres-sive results on object-centric data .While these works focus on relatively constrained cameramotions around a single object, another line of work ad-dresses scenes with arguably more complex camera trajec-tories . MVDiffusion performs image synthesis conditioned on depth maps ofa given mesh, jointly generating all images of the trajec-tory. To increase consistency, cross-view interactions aremodelled by correspondence-aware attention layers that re-quire given pixel-to-pixel correspondences using GT geom-etry during training and inference. Our approach insteadaims at learning those multi-view correspondences, whichallows us to synthesize novel views along a trajectory givenjust a single RGB image, without the need for any geo-metric information about the target views.This rendersour method applicable to a wider range of scenarios, whereno prior 3D reconstruction is available. DFM trains adiffusion model to directly sample from the distribution of3D scenes. Modeling the scene with a 3D representationis inherently 3D-consistent, but is computationally expen-sive and in practice limits DFM to lower image resolutions and slow inference. Pose-Guided Diffusion and Pho-toNVS train a pose-conditioned 2D diffusion model toautoregressively generate frames along a given camera tra-jectory. However, especially for long trajectories, autore-gressive sampling is prone to error accumulation, leading tocommon struggles with loop closure when taking a Markovassumption and slow inference as it cannot be parallelized.Hence, we do not use autoregressive sampling but generateall images jointly, enabling the model to learn short- andlong-term correspondences between views. In stark contrastto MVDiffusion that also performs joint frame synthesis, weonly use depth from an off-the-shelf monocular depth esti-mator with no geometric cues about the target views. Multi-Diff can therefore generate novel views from a single inputimage only. The learnt correspondence attention enablesour model to achieve better consistency than state-of-the-art autoregressive approaches while achieving higher imagequality than related works.",
  ". Method": "Given a single reference image Iref, our goal is to generatesemantically plausible, consistent novel views along a cam-era trajectory C := {cn}Nn=1, where each camera pose cn isrelative to the camera of the reference image. To this end,we propose a pose-conditional 2D diffusion model with cor-respondence attention, i.e., attention layers that jointly oper-ate on all generated views of the trajectory. A key challengein novel-view synthesis for the highly under-constrainedsingle-image setting is to achieve consistency in the lack ofexplicit correspondence supervision. We therefore leveragestrong priors that excel at related tasks. Most importantly,we note that the task of video generation is closely relatedto our problem setting, where temporal consistency is anintrinsic objective.In the following, we explain how we can integrate and ad-just a video prior in conjunction with depth and image priorsto enable free viewpoint control. Next, we provide a de-tailed explanation of our conditioning mechanism and thecorrespondence attention which adds viewpoint control toour pipeline. Lastly, we introduce structured noise, whichports approximate correspondences between frames to ob-tain more consistent synthesis results. Our pipeline is illus-trated in . Video PriorWe build our generative model on top ofVideoCrafter . VideoCrafter trains a denoising 3D U-Net in a fixed latent space, using a pretrained image encoderE and a pretrained image decoder D to map to and from la-tent space, respectively. At the core of VideoCrafter is a 3DU-Net with alternating spatial layers and temporal attention.The spatial layers process each frame in a batch individuallywhile the temporal attention operates on all frames jointly.This pretrained 3D U-Net architecture is a well-suited ini- . MultiDiff is pose-conditional diffusion model for novel view synthesis from a single image. The diffusion model is trained inthe latent space of a fixed auto-encoder with encoder E and decoder D and is conditioned on a reference image Iref and a camera trajectory{cn}. Specifically, we embed N posed target images {In}Nn=1 into latent space, apply forward diffusion according to a timestep t andstructured noise {n}, and train a 3D U-Net to predict {n} from the noisy inputs {znt }. For each sample n, the U-Nets prediction nt isused to reconstruct the denoised sample znt which can then be decoded into the predicted target image In. We condition the U-Net on thereference image by warping Iref to the novel views using depth Dref from a pretrained estimator . The warps {Inref} are encoded into latentrepresentations {yntgt} and injected into the U-Net in a ControlNet inspired manner. We further condition the model directly on the camerapose and an embedding of the reference image as part of the semantic condition {ynsem}. tialization for the task of NVS as the temporal layers alreadyprovide an inductive bias towards (temporal) consistency.During training, we nevertheless finetune all layers of theU-Net for the novel view synthesis task, where instead ofensuring temporal consistency, the attention layers shouldestablish correspondences between multiple views. Hence,we refer to this type of attention as correspondence atten-tion. Novel-view synthesisIn order to generate novel viewsthat adhere to the given camera trajectory C, we need to con-dition our pipeline on the target camera poses cn C. Anave approach to integrating control over the camera view-point is to directly condition the 3D U-Net on cn, e.g., viacross-attention. In practice, we concatenate them to the se-mantic condition of VideoCrafter that consists of an embed-ding of the reference image and the framerate of the inputsequence yielding the semantic conditioning ysem.However, this form of guidance alone is too weak to deliversatisfactory novel-view synthesis results (see Sec. 4.2). Wetherefore integrate a monocular depth prior in order to con-strain the highly ill-posed nature of the task. In our exper-iments, we use ZoeDepth pretrained on ScanNet and refer to the supplementary material for ablations aboutalternative monocular estimators. We use the depth Dref es-timated from the reference image Iref to implement a warp-ing function n that enables warping images from the cam-era of the reference image to any other camera cn C.We denote by Inref := n(Iref) the reference image warpedto camera cn and by Mn := n(1) the mask indicatingthe area of valid warped pixels in camera cn. To facilitatelearning the 3D correspondences across the spatial features,for each view n, we encode Inref into latent space via E and stack the mask Mn, suitably resized, along the channel di-mension. The resulting tensor is denoted ytgt. Inspired byControlNet , we create a copy of the downsampling lay-ers of diffusion U-Net to extract features from ytgt, but weprepend a convolutional layer to cope with the additionalmask channel.The intermediate feature maps are then processed with zero-initialized convolutions and added to the outputs of all spa-tial layers of the 3D U-Net. Note that this differs from theprocedure proposed in ControlNet, which only inserts thefeature maps into the decoder. We further do not freezethe layers of VideoCrafter to enable learning the correspon-dence attention. In initial experiments we found that fine-tuning all layers jointly results in better performance thanusing a fixed video prior.The warping operation is implemented by leveraging an off-the-shelf monocular depth estimator and thus error-proneand incomplete. By also passing the reference image andcamera poses to the network in the semantic conditioningysem, we enable our approach to follow the provided tra-jectory even in absence of overlap with the reference image.We refer to our ablation Sec. 4.2 for a discussion about theimportance of the individual design decisions. In the rest ofthe section we summarize with y all quantities we condi-tion our model on, namely reference image Iref, camera tra-jectories C, and all derived ones (estimated depth, warpedreference images, corresponding masks). Structured noise distribution N(y).Images of a 3Dscene captured from different point of views exhibit strongcorrelations. Hence, it is beneficial to inject similar corre-lations in the noise that is used by our diffusion modelto synthesize the different camera views, which would oth- erwise be a standard normal multi-variate. This helps en-forcing more consistent outputs .Since the correla-tions are mainly driven by geometric constraints, we lever-age the warping function n introduced in the previousparagraph to warp a standard normal multi-variate 0 toall other camera views in C, while filling the gaps withindependent Gaussian noise.This yields per-view noisen := Mn n(0) + (1 Mn) n, where n is a stan-dard normal multi-variate and Mn is the suitably-resizedwarp-validity mask. This process yields := (1, . . . , N),which is regarded as a sample of the structured noise distri-bution we denoted by N(y).",
  "Training Objective.Let V := {(I0, c0), . . . , (IN, cN)}be a ground-truth, posed video sequence, where In and cn": "are the nth image and camera pose in the sequence, re-spectively. We assume I0 to be the reference image, i.e.Iref := I0, and assume all cameras to be relative to c0.We encode all target images of the sequence into a joint la-tent representation z := (z1, . . . , zN), where zn := E(In),and y is the conditioning information encompassing the en-coded reference image, camera poses and warped referenceimages described earlier in the section. The denoising train-ing objective takes the following form for the training exam-ple V :",
  "(z t ; y, t)2,(1)": "where t is sampled from a uniform distribution U(1, T) and is noise sampled from the structured noise distributionN(y). The term z t := tz + 1 t perturbs zwith noise according to a variance-preserving formulationwith parameters t, from which , i.e. our denoising 3DU-Net with weights , is required to recover .Our model is optimized using Adam by minimizing thetraining loss function averaged over random batches ofvideo sequences sampled from a given dataset. Inference.At inference time, we assume to be given areference image Iref and a sequence of cameras C relativeto it, which we use to compute the conditions y. We gen-erate a video sequence from our model by using the DDIMschedule , i.e. starting from zT N(y) we iterate thefollowing equation",
  "t1(zt; y, t) ,(2)": "until we obtain z0 by setting 0 := 1. The term ztt :=zt1ttrecovers z from zt assuming noise . The finalresult z0 entails the synthesized views in latent space for allcameras in C, from which we compute the counterparts inpixel space by applying the decoder D. Note that MultiD-iff can generate all images of the sequence simultaneously.",
  ". Quantitative comparison on RealEstate10K test se-quences. Our model achieves higher image quality than state-of-the-art baselines and comparable consistency compared to DFM": "However, sometimes the novel view has little or no overlapwith the reference image, making the warped reference im-age, i.e. condition yntgt less informative. To further refine theresults, we can run the sampling again on the generated se-quence, but now use the warp of the cloest generated imagein yntgt which in practice this slightly improves consistency.",
  "In this section, we evaluate the performance of our methodfor the task of consistent novel view synthesis from a singlereference image": "DatasetsWe compare our methods against state-of-the-art approaches on RealEstate10K and ScanNet .Both datasets provide video sequences together with regis-tered camera parameters. RealEstate10K is a large datasetof real estate recordings gathered from YouTube. The clipstypically feature smooth camera movement with little to nocamera roll or pitch. Most frames further show consider-able coverage of the respective rooms. Following previ-ous works , we center-crop and downsample thevideos to 256px resolution. ScanNet consists of 1513 hand-held captures of indoor environments. The camera trajecto-ries follow a scan-pattern which can contain rapid changesand variation of camera orientation. The resulting framesencompass close-up object captures as well as wide roomrecordings, leading to heavy occlusions and an overall di-verse data distribution. The aforementioned features makeScanNet extremely challenging for novel view synthesisfrom a single image and our evaluations in Sec. 4 indicatethat additional priors are very beneficial in this setting. Weresize the images to 256256 and remark that ScanNet con-tains 3D meshes that we use for MVDiffusion as it requirespredefined correspondences between frames. EvaluationsWe evaluate our approach in terms of imagefidelity and consistency of the generated outputs. Similarto , we consider both short-term and long-term viewsynthesis. Specifically, we randomly select 1k sequenceswith 200 frames from the test set and evaluate the 50th gen-erated frame for short-term and the 200th generated framefor long-term view synthesis for RealEstate10K. Due to thefaster camera motion, on ScanNet instead we choose the",
  ". Quantitative comparison on ScanNet test sequences.Our approach outperform all baselines at 256px resolution andshows significantly higher image fidelity compared to DFM": "25th frame for short-term and 100th for long-term evalua-tion. In the short-term setting, we report Peak Signal-to-Noise Ratio (PSNR) and perceptual similarity (LPIPS) as standard metrics for novel view synthesis. To evaluatethe extrapolation capacities in regard of image fidelity, weevaluate Frechet Inception Distance (FID) and KernelInception Distance (KID) for long-term settings.Tomeasure the video-consistency of the generated trajectoryimages, we compute Frechet Video Distance (FVD) scores. Further, we follow and report the symmet-ric epipolar distance (SED) to quantify faithfulness with re-spect to the provided camera trajectory, i.e., relative pose",
  "accuracy. Here, we compute the mean thresholded sym-metric epipolar distance (mTSED) over the pixel thresholds[1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0] and refer to the supplemen-tary for detailed results": "BaselinesWe compare our approach against the state-of-the-art approaches for scene synthesis from a singlereference image, including DFM , PhotoNVS ,Text2Room and MVDiffusion . As MVDiffusionis purely text-conditional, we incorporate the reference im-age during inference as follows. We use DDIM inversion toobtain the noise corresponding to the reference image andinclude it in the batch during sampling. Due to the globalawareness of MVDiffusion, information from the referenceimage can propagate to all generated views. Please see sup-plementary material for more details. DFM trains a diffu-sion model to directly sample from the distribution of 3Dscenes. Unlike our approach, DFM cannot directly inte-grate 2D diffusion priors and does not generalize well toout-of-domain inputs as our experiments on ScanNet in-dicate. PhotoNVS trains a pose-conditioned 2D dif-fusion model to iteratively predict the next frame for agiven camera trajectory.Text2Room uses an auto-regressive approach of predicting depth and leveraging a",
  "PhotoNVSOurs": ". Generated views along ScanNet test sequence (right) given a reference view (left). Our method simultaneously generatessequences of novel views that are both more realistic and more view-consistent than the baselines, DFM and PhotoNVS, which suffer froma considerable performance drop across large view point changes. Although MVDiffusion uses sensor depth input, the generated views aremuch less consistent with the reference image (e.g., colors of the cushions), compared to our generations, which do not rely on sensor depth. depth-conditional T2I model to generate new views that areused to update a textured mesh. In contrast, MultiDiff gen-erates multiple frames from the input image in parallel, re-sulting in better long-term view synthesis and faster infer-ence: To synthesize a 128 128 frame, PhotoNVS requires 45s, DFM 17s while ours only takes 1s .",
  ". Consistent Novel View Synthesis": "Comparison against state of the artWe quantitativelyevaluate our approach on the task of consistent novel-viewsynthesis from a single reference on RealEstate10K inTab. 1 and ScanNet in Tab. 2. Since DFM does not sup-port higher resolutions than 128px due to memory limita-tions, whereas the other methods run at a default resolutionof 256px, we perform separate analyses at both resolutions.On RealEstate10K, we observe that our method achievesconsistently better FID and KID scores on both short-term,as well as long-term evaluations: The short-term FID com-pared to DFM improves from 36.37 to 25.30 (at 128px),while the long-term FID improves by 33% compared to PhotoNVS. Moreover, our model outperforms all baselinesin terms of FVD and achieves comparable results on LPIPSand mTSED with respect to DFM. We note that the Pix-elNeRF representation of DFM leads to highly con-sistent results, therefore good scores on pixel-level metricslike short-term PSNR, however, this comes at the cost ofsharpness (as reflected in FID/KID).By leveraging strong image- and video-diffusion priors, ourmethod achieves clear improvements over the baselines onScanNet: As shown in Tab. 2, MultiDiff outperforms MVD-iffusion on short- and long-term metrics, indicating ourmodels ability to learn long-term correspondences evenwithout relying on ground-truth geometry. In comparisonto DFM, Text2Room and PhotoNVS, we observe strongphotometric short- and long-term improvements over allbaselines. Figs. 3 and 4 show qualitative comparisons onRealEstate10K and ScanNet, respectively. It stands out thatour method synthesises realistic and consistent novel viewseven across large viewpoint changes, where the quality ofthe baselines drops noticeable.",
  "We show the contributions of individual components of ourapproach in Tab. 3 and refer to the supplementary materialfor more qualitative comparisons": "Importance of priorsAs described in Sec. 3, we initial-ize our model with weights obtained by training on large-scale image and video datasets. To study the importance ofthose priors for the task of consistent novel-view synthesisfrom a single image, we ablate them one by one: As shownin Tab. 3, training from scratch (MultiDiff no prior) leadsto strong degradation of image quality, as well as overallconsistency. Removing the video diffusion prior (Multi-Diff no vid.) has strong influence on the long-term con-sistency (mTSED decreases by 12.7%), as well as the videoquality (FVD increases by more than 120%). We further ab-late the monocular depth estimates on the reference imageas condition to our model in MultiDiff no warp (Tab. 3).The drop in mTSED from 85.5% to 48.4% indicates thatthe model without reference warps is not able to closely ad-here to the input trajectory. Besides the depth-warpings ofthe reference image, our method uses relative camera posesto synthesize images from the desired target poses. Whenremoving this modality (MultiDiff no pose), we notice ef-fect on long-term generation becomes apparent, where thereis minimal to no warp-guidance to inform about the desiredcamera poses, hence mTSED decreases from 0.85 to 0.62. Importance of structured noiseAs described in Sec. 3,we introduce structured noise by warping the initial noiseconsistently between target views according the depth es-timates of the reference image. We measure the effect ofnoise warping in Tab. 1 and Tab. 2 (MultiDiff w/o SN) onRealEstate10K and ScanNet trajectories. On both datasets,we observe that the structured noise leads to significantlymore consistent and higher quality synthesis results. Weshow the effect of noise-warping in .",
  "performs consistent completion in those regions. We showexamples on ScanNet test images in and refer to thesupplementary material for more qualitative results": "5. ConclusionsIn this paper, we introduce MultiDiff, a novel approach forview extrapolation from a single input image. We identifyvideo priors as a powerful proxy for this setting and demon-strate how they can be incorporated and adapted by convert-ing temporal attention to correspondence attention. Withmonocular depth cues, we facilitate learning improved cor-respondences by conditioning our model on reference viewswarped w.r.t. the target camera trajectory. Our experimentson RealEstate10k and ScanNet show significant improve-ments over relevant baselines, with particular gains on long-term sequence generation and overall inference speed.",
  "Matthias Niener was supported by the ERC Starting GrantScan2CAD (804724)": "Titas Anciukevicius, Zexiang Xu, Matthew Fisher, Paul Hen-derson, Hakan Bilen, Niloy J. Mitra, and Paul Guerrero.RenderDiffusion: Image diffusion for 3D reconstruction, in-painting and generation. arXiv, 2022. 3 Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Iron-depth: Iterative refinement of single-view depth using sur-face normal and its uncertainty. In British Machine VisionConference (BMVC), 2022. 12, 16",
  "Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisser-man. Frozen in time: A joint video and image encoder forend-to-end retrieval. In IEEE International Conference onComputer Vision, 2021. 12": "Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-imagediffusion models with an ensemble of expert denoisers. arXivpreprint arXiv:2211.01324, 2022. 2 Miguel Angel Bautista, Pengsheng Guo, Samira Abnar, Wal-ter Talbott, Alexander T Toshev, Zhuoyuan Chen, LaurentDinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, AfshinDehghan, and Joshua M. Susskind. GAUDI: A neural ar-chitect for immersive 3D scene generation. arXiv preprintarXiv:2207.13751, 2022. 3",
  "Xianfan Gu, Chuan Wen, Jiaming Song, and Yang Gao. Seer:Language instructed video prediction with latent diffusionmodels. arXiv preprint arXiv:2303.14897, 2023. 2": "Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun,Menghan Xia, Yong Zhang, Xintao Wang, Ran He, QifengChen, and Ying Shan.Scalecrafter: Tuning-free higher-resolution visual generation with diffusion models.arXivpreprint arXiv:2310.07702, 2023. 2 Philipp Henzler, Jeremy Reizenstein, Patrick Labatut, Ro-man Shapovalov, Tobias Ritschel, Andrea Vedaldi, andDavid Novotny.Unsupervised learning of 3d object cat-egories from videos in the wild.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 47004709, 2021. 3 Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,Bernhard Nessler, and Sepp Hochreiter. GANs trained bya two time-scale update rule converge to a local nash equi-librium. Advances in neural information processing systems,30, 2017. 6",
  "Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-makov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:Zero-shot one image to 3d object. In ICCV, 2023. 3": "Stephen Lombardi, Tomas Simon, Jason Saragih, GabrielSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-umes: Learning dynamic renderable volumes from images.arXiv preprint arXiv:1906.07751, 2019. 3 Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tie-niu Tan.Videofusion: Decomposed diffusion models forhigh-quality video generation. In CVPR, 2023. 2",
  "Moustafa Meshry, Dan B. Goldman, Sameh Khamis, HuguesHoppe, Rohit Pandey, Noah Snavely, and Ricardo Martin-Brualla. Neural rerendering in the wild. In CVPR, 2019.3": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:Representing scenes as neural radiance fields for view syn-thesis. In European conference on computer vision, pages405421. Springer, 2020. 2 NormanMuller,AndreaSimonelli,LorenzoPorzi,SamuelRotaBulo,MatthiasNiener,andPeterKontschieder.Autorf:Learning 3d object radiancefields from single view observations.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), 2022. 3 NormanMuller,YawarSiddiqui,LorenzoPorzi,Samuel Rota Bulo,Peter Kontschieder,and MatthiasNiener.Diffrf:Rendering-guided 3d radiance fielddiffusion.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition,pages43284338, 2023. 3",
  "Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A re-duction of imitation learning and structured prediction to no-regret online learning. In Int. Conf. Art. Intell. Stat. PMLR,2011. 2": "Chitwan Saharia, William Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,et al. Photorealistic text-to-image diffusion models with deeplanguage understanding. In NeurIPS, 2022. 2 Mehdi S. M. Sajjadi, Henning Meyer, Etienne Pot, UrsBergmann, Klaus Greff, Noha Radwan, Suhani Vora,Mario Lucic, Daniel Duckworth, Alexey Dosovitskiy, JakobUszkoreit, Thomas Funkhouser, and Andrea Tagliasacchi.Scene representation transformer: Geometry-free novel view",
  "synthesis through set-latent scene representations. In CVPR,2022. 3": "Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann,Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry La-gun, Li Fei-Fei, Deqing Sun, and Jiajun Wu.ZeroNVS:Zero-shot 360-degree view synthesis from a single real im-age. arXiv preprint arXiv:2310.17994, 2023. 3 Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,Oran Gafni, et al. Make-a-video: Text-to-video generationwithout text-video data. In ICLR, 2023. 2",
  "Yang Song and Stefano Ermon.Improved techniques fortraining score-based generative models. Advances in neuralinformation processing systems, 33:1243812448, 2020. 2": "Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-hishek Kumar, Stefano Ermon, and Ben Poole. Score-basedgenerative modeling through stochastic differential equa-tions. arXiv preprint arXiv:2011.13456, 2021. 2 Shitao Tang, Fuayng Zhang, Jiacheng Chen, Peng Wang, andFurukawa Yasutaka. Mvdiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion.arXiv preprint 2307.01097, 2023. 3, 6 Ayush Tewari, Tianwei Yin, George Cazenavette, SemonRezchikov, Joshua B. Tenenbaum, Fredo Durand, William T.Freeman, and Vincent Sitzmann.Diffusion with forwardmodels: Solving stochastic inverse problems without directsupervision. NeurIPS, 2023. 2, 3, 5, 6, 12",
  "Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-basedgenerative modeling in latent space. In NeurIPS, 2021. 2": "Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srini-vasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser.Ibrnet:Learning multi-view image-based rendering. In CVPR, 2021.3 Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,and Jingren Zhou.Videocomposer: Compositional videosynthesis with motion controllability.arXiv preprintarXiv:2306.02018, 2023. 2",
  "A. Additional qualitative results": "We provide additional qualitative comparisons with thebaselines on RealEstate10K in as well as onScanNet in . While PhotoNVS accumulateserrors over the autoregressive sampling process, our modelsynthesizes realistic images for all target poses jointly. Incomparison to DFM , our approach leverages strongimage- and video-priors to achieve noticeably higher imagefidelity.Furthermore, we demonstrate the stochasticity of our ap-proach in where using the same reference image andtarget poses, our probabilistic method synthesizes multipleplausible novel views.We also present an in-the-wild and 360 trajectory in .",
  "B.1. Baselines": "DFMWe use the official implementation of the au-thors ( On Realestate10K, we evaluate the provided pre-trained checkpoint.On ScanNet, we train a modelfromscratch,followingtheofficialinstructionsforRealEstate10k. More specifically, we first train the model atresolution 64 64 for 75K iterations with a total batch sizeof 16 on 8 NVIDIA A100-SXM4-80GB GPUs. Next, wefine-tune the model at resolution 128 128 for 60K itera-tions using a total batch size of 8. At both resolutions, largerbatch sizes did not fit in the 80GB memory of the GPUs. PhotoNVSWeusetheofficialimplementationofthe authors and the provided check-point on Realestate10K. On ScanNet, we use the pre-trainedVQGAN provided by the authors and train the model at256 256 resolution for 500K iterations using 8 NVIDIAA100-SXM4-80B GPUs with an effective batch size of 64. MVDiffusionSince MVDiffusion is a purely text-conditional model,we adapt the official implemen-tation(https : / / github . com / Tangshitao /MVDiffusion.git) to accept a reference image atinference time. We encode the reference image into latentspace and then encode it into the diffusion models Gaus-sian prior space using DDIM inversion. During sampling,the encoded reference image is added to the batch. SinceMVDiffusion uses attention layers that operate on allimages in the batch jointly, the reference frame affects thesampling for all images.However, during sampling thescore estimate is calculated using the full batch, while forDDIM inversion we can only obtain the score estimate forthe reference image. In practice, sampling does hence notreproduce the reference image faithfully. We address thisissue by additionally optimizing the reference latent aftereach denoising step to match the reference image. For theoptimization at each sampling step, we use Adam with a learning rate of 0.1 and train with an L2-Loss and aperceptual loss for 10 iterations. Text2RoomWeusetheofficialimplementationofthe authors ( which also supports image-conditional gen-eration. We follow the original setup and use IronDepth for depth prediction and StableDiffusion2 inpainting(https : / / huggingface . co / stabilityai /stable - diffusion - 2 - inpainting) for imageinpainting. Since Text2Room formulates the problem aspure depth-to-image/inpainting task, the same pretrainedcheckpoints can be used for both datasets, RealEstate10Kand ScanNet, and no additional training is required.",
  "B.2. MultiDiff": "Training detailsFor the encoding of the warped refer-ence images, we use the encoder layers of the pre-trainedtext-to-image model Stable Diffusion 1.5 and use theprovided VQ-VAE for latent encoding and decoding. Weinitialize the denoising layers of our U-Net model with thepre-trained weights of VideoCrafter , a latent video dif-fusion model trained on large scale video data . Thetemporal attention layers serve as strong prior for consis-tency - see performance of MultiDiff no vid. in of the main paper and for a qualitative compari-son. Nevertheless, we fine-tune all layers of the U-Net for",
  ". Different samples generated by our probabilistic approach using the same reference image and target trajectory": "the novel view synthesis task to enable the attention lay-ers to learn correspondences between multiple views. Fortraining, we use Adam with a learning rate of 1e-05and batch size of 6 with 16 target views per batch at aresolution of 256 256. Using 8 NVIDIA A100-SXM4-80B GPUs with an effective batch size of 48, we train for300K iterations. We use DDPM noise scheduling usingt = 1000 time steps for denoising and perform evaluationusing DDIM sampling with 35 steps.For noise warping, we found that using nearest-neighborwith a receptive field size of 4px at 256px resolution gavethe best results. This limited receptive range ensures thatthe noise distribution remains roughly normal, preventingstrong zooms from resulting in a few pixels covering largeimage portions. Inference detailsUsing the estimated depth maps withnearest-neighbor interpolation, we calculated the averagewarping overlap of the initial image with the last framein the sequence: 20.4% (24.7%) on RealEstate10K (Scan-",
  "C. Evaluation": "Data processingOn RealEstate10K, we randomly select1K sequences with at least 200 frames. For evaluation, foreach sequence, we choose a random starting frame at least200 frames ahead of the last frame. We select 16 frames forevaluation that we uniformly distribute within the intervalof 200 views from the starting frame. Following previousevaluation protocols, for short-term evaluation, we set the5th view to be 50 frames after the starting view in the orig-inal video. For long-term evaluation, the last view corre-sponds to the 200th frame after the starting frame.On ScanNet, for each of the 100 test scenes, we sample 10starting views ensuring at least 100 frames offset from thelast frame in the recordings, resulting in a set of 1K testsequences. Since the camera movement in ScanNet record-",
  "PhotoNVS183DFM-MultiDiff1.94": ".Comparison of the inference speed evaluated in sec-onds per frame using FP32 on an NVIDIA A100-SXM4-80GB.By jointly inferring multiple frames in parallel and using effi-cient attention architecture, we achieve noticeably shorter infer-ence times. ings is considerably higher and the frame rate noticeablyslower compared to RealEstate10K, we consider sequencelengths reduced by 50% in the original video. Therefore, weconsider the 25th frame for short-term and the 100th viewfor long-term evaluation relative to the starting frame. TSEDTo compute TSED scores , we use the of-ficial implementation from ( NVS) and provideadditional quantitative results in and . WhileDFM achieves the highest consistency scores due to its Pix-elNeRF formulation, it suffers from noticeably worseimage generation quality compared to MultiDiff (see and 2 of the main paper as well as and ). AsDFM does not support higher image resolutions, we mea-sure TSED at 128 128 resolution. Inference speedWe report inference performances ofPhotoNVS, DFM, and MultiDiff in Tab. 4. As our approachinfers multiple frames in parallel and uses an efficient atten-tion architecture, we observe noticeably shorter inferencetimes while achieving higher image fidelity and consistencythan the baselines. We note that our approach also scales tolarger resolutions as the underlying latent video prior caneasily be tuned for image sizes. This is in stark contrastto baselines like PhotoNVS, DFM for which the computa-tional costs quickly become too high and require infeasibleamounts of memory when trained on larger resolutions. Fitting a NeRFResults for fitting a NeRF with Nerfactoare shown in , yielding small pixel-level inconsisten-cies with floating artifacts. As in ReconFusion , we usedistillation to obtain a cleaner representation (second row in).",
  "D. Ablations": "We show additional qualitative results ablating our designdecisions in . We note that training our model fromscratch (MultiDiff no prior) leads to over-smoothed re-sults that do not closely follow the target trajectory. Further-more, we showcase the effect of using the image prior butnot initializing the weights of our correspondence attentionlayer with the weights of the pre-trained video prior (seeMultiDiff no vid.): The results are overall less consistentas e.g. the floor changes from carpet to wood. Our methoduses depth-based image warps to reproject the reference im-age to the target poses, providing strong cues about the tar-get views. We ablate the importance of this in of themain table (see MultiDiff no warp) and show an exampleof in . Without using the warps of the referenceimage, our model is not able to faithfully follow the targettrajectory. As under strong camera motion, there is little tono overlap with the reference image, we also learn an em-bedding of the target pose and show the effect of removingthis information (MultiDiff no pose) in . Usingthe additional pose embeddings provides additional guid-ance about the target poses leading to better TSED scores.In addition, we ablate the effect of using an alternative depthestimator to ZoeDepth in Tab. 5. For this, we use Iron-Depth pretrained on ScanNetv2 and report qualitativeresults on RealEstate10K and ScanNet. While we observe . TSED scores evaluated on RealEstate10K at a resolution of 128 128. The left chart shows the TSED evaluated at differentthresholds, on the right we plot the TSED scores over the pairs of frame indices along the trajectory. . TSED scores evaluated on ScanNet at a resolution of 128128. On the left, we show the TSED evaluated at different thresholds.The right chart plots the TSED scores over the pairs of frame indices along the trajectory. comparable results in image quality performance, we notethat using IronDepth leads to worse consistency scores. AsIronDepth does not provide estimates in metric scale, usingthese depth estimates to warp the reference image leads toless accurate conditional information. Ultimately, this re-sults in lower consistency scores - see e.g., mTSED thatdecreases by 6% on RealEstate10K and 13% on Scan-Net using IronDepth compared to using ZoeDepth. Furthermore, we qualitatively show the effect of usingstructured noise in on a ScanNet test sequence. Wenote that by structuring the noise using the depth estimates,we obtain more realistic and consistent synthesis results."
}