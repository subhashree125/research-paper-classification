{
  "Abstract": "Dense depth maps have been used as a key element of vi-sual perception tasks. There have been tremendous efforts toenhance the depth quality, ranging from optimization-basedto learning-based methods. Despite the remarkable progressfor a long time, their applicability in the real world is lim-ited due to systematic measurement biases such as density,sensing pattern, and scan range. It is well-known that thebiases make it difficult for these methods to achieve theirgeneralization. We observe that learning a joint represen-tation for input modalities (e.g., images and depth), whichmost recent methods adopt, is sensitive to the biases. In thiswork, we disentangle those modalities to mitigate the bi-ases with prompt engineering. For this, we design a noveldepth prompt module to allow the desirable feature repre-sentation according to new depth distributions from eithersensor types or scene configurations. Our depth prompt canbe embedded into foundation models for monocular depth es-timation. Through this embedding process, our method helpsthe pretrained model to be free from restraint of depth scanrange and to provide absolute scale depth maps. We demon-strate the effectiveness of our method through extensiveevaluations. Source code is publicly available at",
  ". Introduction": "Scene depths have been used as one of the key elementsfor various visual perception tasks such as 3D object de-tection , action recognition , and augmented real-ity , etc. For accurate depth acquisition, there havebeen various attempts in the computer vision field. Sincethe advances in deep learning, its powerful representationalcapacity has been applied to explain scene configurations,which is feasible even with only single images. Unfortu-nately, single image depth estimation cannot produce metricscale 3D depths when camera parameters change and out-of-distributions on unseen datasets happen .",
  "C.FormerC.FormerApple ARkit": ". An overview of our depth prompting for sensor-agnosticdepth estimation. Leveraging a foundation model for monoculardepth estimation, our framework produces a high-fidelity depthmap in metric scale and provides impressive zero/few-shot general-ity. C.Former indicates CompletionFormer . More details andexamples are reported in Sec. 4.5 and supplementary materials. Toward depth information easy to acquire in metric scale,active sensing methods such as LiDAR (Light Detection andRanging) , ToF (Time of Flight) , and structuredlight have gained interest as a practical solution. Al-though the active sensing methods enable real-time scenedepth acquisitions in a single shot, they only provide sparsemeasurements. For dense predictions, spatial propagation,modeling an affinity among input image pixels, is neces-sary . Note that its affinity map isconstructed based on input images, and is jointly optimizedwith fixed depth patterns. Different from real-world scenar-ios where various types of depth sensors (e.g., Velodyne Li-DAR , Microsoft Kinect , Intel RealSense , andApple LiDAR sensor , etc.) are used, the mainstream ofstandard benchmarks for this task is to only use KITTIdataset captured from a 64-Line LiDAR and random",
  "arXiv:2405.11867v1 [cs.CV] 20 May 2024": "samples from Kinect depth camera in NYUv2 dataset .In this work, our primary goal is to build a sensor-agnosticdepth estimation model that faithfully works on the variousactive depth sensors. Inspired by pioneer works in visualprompt methods like SAM , we design a novel depthprompt module used with pre-trained models for monoculardepth estimation. The depth prompt first encodes the sparsedepth information and then fuses it with image features toconstruct a pixel-wise affinity. A final refinement process isperformed with both the affinity and an initial depth mapfrom the pre-trained depth models. To take full advantageof pre-trained models, we conduct a bias tuning , whichis a well-known memory-efficient technique when appliedto pre-trained models. Our proposed method is fine-tunedfor only 0.1% parameters of the models while keeping otherparameters frozen.Our key idea is to reinterpret prompt learning as spatialpropagation. We aim to achieve an adaptive affinity fromboth the depth prompt and the knowledge of the pre-trainedmonocular depth model . We demonstrate that the pro-posed method is generalized well to any sensor type, and itcan be extended for various depth foundation models suchas , which are trained with large-scale datasets formonocular depth estimation to achieve relative scale predic-tion and zero-shot generalization. To do this, we utilize avariety of public datasets captured from off-the-shelf depthsensors and take real-world scenes by ourselves where thewide depth ranges and structures exist. Additionally, weconduct an extensive ablation study to verify the influenceof each component within our framework and evaluate ourmethodology in a variety of real-world settings. This test-ing includes zero and few-shot inference exercises acrossdifferent sensor types, further validating the robustness andadaptability of our proposed solution.",
  ". Depth Estimation with Sparse Measurement": "Accurate dense depth acquisition requires time-consumingand costly processes . As a compromise between in-ference time and cost, sparse depth measurements basedon active sensing manners have been considered as main-stream approaches. Leveraging the sparse measurementsand its corresponding images, recent learning-based meth-ods have been proposed to makedense predictions which enhance the depth resolutions samewith the image resolutions via a spatial propagation pro-cess. However, due to the dependency on the specific sparsedepth pattern and density according to the input devices,such models face challenges in real-world scenarios suchas sensor blackouts , multipath interference , andnon-Lambertian surfaces , resulting in much fewer sam-ple measurements. Several studies have explored depth reconstruction fromunevenly distributed and sparse input data . How-ever, they suffer from an issue on a range bias, which pro-vides only limited scan ranges in training datasets. Worksin focus on handling extremely sparse conditions(less than 0.1% over its input image); however, they failto show the generalized performances on scenarios givenrelatively dense initial depth inputs. To address both issues,we adopt prompt engineering to achieve the model general-ization, which has proven its powerful capacity by takingadvantage of pre-trained models on downstream tasks, yetremains unsolved for depth-relevant tasks so far .",
  ". Prompt Engineering": "Prompt engineering refers to designing specific templatesthat guide a model to complete missing information in astructured format (e.g., cloze set ), or generate a validresponse along with given input (e.g., promptable segmen-tation ). With the recent success of the large languagemodels , works in demonstrate how variousnatural language processing (NLP) tasks can be reformu-lated to an incontext learning problem given a pre-definedprompt, which is a useful tool for solving the tasks andbenchmarks .The advent of prompt engineering has been trans-formative, with some studies extendingits application to the computer vision field. Here, itis used to achieve a zero-shot generalization, enablingmodels to understand new visual concepts and data dis-tributions that they are not explicitly trained on. Themost relevant work to this paper designs a promptablesegmentation model. They construct a prompt encoder torepresent user-defined points or boxes with a positional em-bedding . We note that it is not a pixel-wise regressionproblem which is different from our task.",
  ". Foundation Model for Dense Prediction": "Foundation models are designed to be adaptable for variousdownstream tasks by pretraining on broad data at scale . InNLP field revolutionized by large-scale models such as GPTseries , foundation models in the computer visionfield have been becoming popular. Recent advancementsin the large-scale foundation models with web-scale imagedatabases have made significant breakthroughs, particularlyinvolving image-to-text correspondences .These developments have paved the way for more efficienttransfer learning and make the zero-shot capabili-ties better .Despite these advancements, the foundation models areprimarily used in high-level vision tasks such as image recog-nition , image captioning , and text-to-image generation . When it comes to low-level visiontasks like depth predictions, these models do not seem to be suitable due to a lack of extensive image/depth data on ametric scale . To be specific, a web-scale dataset col-lection is infeasible because ground truth-level metric scaledepths can be obtained only from sensor fusion manners .Although some works have led to the creation of largeand diverse datasets for monocular depth prediction, trans-ferring the learned knowledge into other domains remainsunexplored. To achieve the sensor-agnostic depth estimationregardless of scene configuration, we take fully advantageof the knowledge from the depth foundation model.",
  ". Sensor-agnostic Depth Estimation": "In this section, we start by discussing the three biases whichhinder sensor-agnostic depth prediction (Sec. 3.1). We thenintroduce the proposed depth prompt module. Here, we re-cast the depth prompt design as learning an adaptive affin-ity construction in spatial propagation for various types ofsparse input measurements (Sec. 3.2). Lastly, we provideimplementation details of the proposed module (Sec. 3.3).",
  ". Sensor Biases in Depth Estimation": "Bias issues make learning-based models for visual percep-tions hard to achieve their generality . There havebeen attempts to address the bias problems in image restora-tion , recognition and generation . Amongthem, the sensor-bias issue is also considered as oneof the crucial research topics. In particular, since a vari-ety of depth sensors types is available, there is no general-ization method to cover every depth sensor types, while thesolutions to the same type (e.g., different LiDAR configu-rations ) exist. In this part, we empirically investigate3-sensor biases, e.g., sparsity, pattern, and range bias beforean introduction to our solution.Firstly, if a learning-based model is trained on data witha certain density (e.g., 500 random samples in training), itwill suffer from sparsity bias, which makes high-fidelitydepth maps difficult if fewer samples are available in the testphase, as shown in -(b). The sparser sample measure-ments are also common due to sensor blackout, occlusion, orchanging environmental conditions. This has hampered thepractical utility of learning-based depth estimation in real-world scenarios. Second, pattern bias shows the performancedegradation if depth patterns vary between training and testphases, even with the same number of depth points. Whenwe intentionally shift the input depth pattern in the inference,it indicates that the existing model is biased toward the fixedlocation of input depth points in -(c). This makes aunified depth prediction model difficult to be applied to othersensor types. Lastly, range bias, arising when attempted totake scene structures beyond the limited scan range of thesensor, also prevents sensor-agnostic depth estimation asshown in -(d).",
  ". Depth Prompting": "To realize sensor-agnostic depth estimation without anysensor bias, we take an inspiration from prompt learn-ing in NLP, which designs a specific template to guidea model for a valid response along with a given in-put . We aim to design a prompt module for depthmodality by defining a unified embedding space to rep-resent learned features from any type of input measure-ments (). Here, we use an input depth map as a tem-plate for our depth prompt module, and the sensor-agnosticdepth prediction is achieved by fusing the template, fea-tures from the embedding space, and image features.Revisiting Spatial Propagation.In this work, our keyidea is to reinterpret the depth prompt design as spatial prop-agation, which predicts dense depth maps from input sparsemeasurements guided by image-dependent affinity weights.We formulate the conventional spatial propagation process:",
  "(l,m)N(x,y)A(l, m) Dt(l,m),(1)": "where Dt(x,y) R1HW refers to a depth map for eachpropagation step t. (x, y) and H, W means a spatial coordi-nate and the height and width of an input image, respectively.D0(x,y) and A indicate an initial depth and a pixel-wise affin-ity map, respectively. operator denotes an element-wiseproduct. (l, m) N(x,y) refers to 8-directional neighboringpixels over the reference pixel (x, y).Even in the same scene, the affinity map A can vary ac-cording to the input depth type which is dependent on sensorsused. Thats, the affinity map should be adaptive to variousinput changes. However, affinity maps from previous spatialpropagation methods are invariantbecause they are learned from a certain type of input depths.We address this issue by designing a depth encoder to learnfeatures for a diverse set of sensors and by projecting theminto the unified embedding space.Depth Feature Extraction.To do this, we adopt anencoder-decoder structure to efficiently encode both posi- tional and sparsity information of an input depth map. Theencoder takes a depth map as an input, and then the decoderconstructs an affinity map with the same size of the depthmap. After that, the prompt embedding is combined withimage features to bring boundary and context information.To be specific, we use ResNet34 to extract the depthfeatures . We downsample the features by 1/2,1/4, 1/8, 1/16, and 1/32, and then feed them into the de-coder with skip connections. Given a sparse depth DS , ourdepth prompt encoder fE yields both a prompt embeddingF d and multi-scale features F dk as below:",
  "F d, F dk = fE(DS),(2)": "where k is an index of the downsampled features.Depth Foundation Model.Until now, large-scale depthmodels have been tailored to monocular depth es-timation. Leveraging a large-scale monocular depth dataset,the models are able to provide relative depth maps, which isthe only option as a foundation model.Given a single image I R3HW , the pre-trained depthmodel fF outputs an initial depth map DI and multi-scaleintermediate features F ik:",
  "DI, F ik = fF(I, fF ),(3)": "where fF indicates parameters of the foundation model,which keeps frozen during both training and inference.Here, we need to effectively transfer the pre-trainedknowledge of the foundation models to a range of sensors viaprompt engineering. We adopt a bias tuning , whichis more effective for dense prediction tasks than other tuningprotocols . This is used to update bias terms and tofreeze the rest of the backbone parameters. As a result, thebias tuning contributes to preserving the high-resolution de-tails and context information acquired in the initial extensivetraining phase , which will be analyzed in Sec. 4.4.Next, to infer depth maps with absolute scales, we mergethe relative depth from the foundation model with the sen-sor measurements. Due to the nature of spatial propagation,which mainly refines neighboring depth values over givenseed points, we cannot obtain proper depth values for re-gions where no initial depth points are available. To addressthis limitation, we perform an additional processing with aleast-square solver to produce the consistent DI whenapplied to other sensor types. The process uses both an initialdepth from the foundation model and a sparse depth DS inorder to perform a global refinement. By solving the leastsquare equation, we can obtain the solution p R as below:",
  ". An overview of the proposed architecture. We design adepth prompt module to construct an adaptive affinity map Aada,which guides the propagation of given depth information": "DS. The solution p is multiplied with the initial depth DI,whose result becomes D0(x,y) of Eq. (1). Since this pre-calculation makes an initial depth for the spatial propagationin Eq. (1) better, we can cover larger unknown areas thanthose without Eq. (4).Decoder for Adaptive Affinity.A decoder in our promptmodule reconstructs an affinity map using the image em-bedding from the foundation model encoder fF (Eq. (3))and a set of prompt embeddings from the prompt encoderfE (Eq. (2)). We concatenates the prompt embeddings F d,intermediate features F di from the prompt encoder and multi-scale image features F ik, and then yield Aada RC2HW",
  "Aada = fD(F d, F dk , F ik).(5)": "Finally, we substitute the conventional affinity map A inEq. (1) into our Aada above. Thanks to the feature fusionfrom both the prompt embedding and the foundation modelwith the bias-tuning, we can successfully decode an affinitymap to account for different types of input measurements. Inaddition, the least square solver in Eq. (4) allows the spatialpropagation to take consistent initial depth maps as input,regardless of the sensor variations. As a result, we achievethe adaptiveness/robustness in the proposed framework.",
  ". Implementation": "Random Depth Augmentation.For more generality, weadopt random depth augmentation (RDA). We sample depthpoints from relatively dense depth maps to simulate sparserinput depth scenarios. For example, we extract 4-Line depthvalues from 64-Line depth maps in the KITTI dataset .In addition, we train our framework from general to extremecases (e.g., from standard 500 random samples to only 1depth point in the NYUv2 dataset )Loss Functions.The proposed framework is trained in asupervised manner with the linear combination of two loss functions: (1) Scale-Invariant (SI) loss for an initialdepth from the depth foundation model fF (Eq. (3)); (2) Acombination of L1 and L2 losses for a final dense depth.An initial depth map is predicted by minimizing the dif-ference between DI and its ground truth depth map Dgt forvalid pixels v V . Let v = log DI(v) log Dgt(v), theSI loss LSI is defined as below:",
  ", (6)": "where we set = 0.85 in all experiments, following theprevious work .Next, our framework infers a dense depth D in Eq. (1)based on the valid pixels v V of its ground truth depthDgt as well. For this, we use a loss Lcomb based on both L1and L2 distances as follows:",
  "L = Lcomb( D, Dgt) + LSI( DI, Dgt).(8)": "where is a balance term and empirically set to 0.1.Training Details.We utilize a SoTA monocular depthestimation method, termed DepthFormer , as a pri-mary backbone to validate our method effectively transferthe knowledge of large-scale depth model into our sensor-agnostic model. Our framework is implemented in publicPyTorch , trained for 25 epochs on four RTX 3090TIGPUs using Adam optimizer, with 228 304 and 240 1216 input resolution of NYU and KITTI dataset, respec-tively. Note that we resize the input RGB images to keep theirratio of height and width toward the foundation model used.The initial learning rate is 2 103, and then scaled downwith coefficients 0.5, 0.1, and 0.05 every 5 epochs after 10th epoch. The total training process for the NYU dataset takesapproximately half a day, with an inference time of 0.06seconds. For the KITTI dataset, the training time is about 1.5days, with an inference time of 0.38 seconds. The frameworkcomprises 53.4 million learnable parameters, which includes0.1M dedicated to tuning the foundational model.",
  ". Experiment": "In this section, we conduct comprehensive experiments toevaluate the impact of our depth prompting module onsensor-agnostic depth estimation. First, we briefly describethe experimental setup (Sec. 4.1), and present comparative re-sults against various state-of-the-art (SoTA) methods on stan-dard benchmark datasets (Sec. 4.2). Moreover, we provide an in-depth examination of bias issues in sensor (Sec. 4.3)as well as an ablation study to demonstrate the effect of eachcomponent in our method (Sec. 4.4). Lastly, we offer quali-tative results to show zero generalization of our method oncommercial sensors (Sec. 4.5).",
  ". Experiment Setup": "Evaluation Protocols.For our comparative experiments,we select a range of SoTA methods for depth estimation fromsparse measurements. These include a series of spatial prop-agation networks such as CSPN , S2D , NLSPN ,DySPN , CostDCNet , and CompletionFormer .Additionally, we choose SAN , which are designed toadapt various sparse setups. We use common quantitativemeasures of depth quality: root mean square error (RMSE,unit: meter), mean absolute error (MAE, unit: meter), andinlier ratio (DELTA1, < 1.25).Datsets: NYUv2 and KITTI DC.We utilize the NYUDepth V2 dataset, an indoor collection featuring 464scenes captured with a Kinect sensor. Following the of-ficial train/test split, we use 249 scenes (about 50K sam-ples) in training phase, and 215 scenes (654 samples) aretested for the evaluation. The NYU Depth V2 dataset pro-vides 320240 resolutions. We use the center-cropped im-age whose resolution is 304228 and randomly sample 500points to simulate the sparse depth.For outdoor scenarios, we choose a KITTI DC dataset with 90K samples. Each sample includes color im-ages and aligned sparse depth data (about 5% density overimage resolution) captured using a high-end Velodyne HDL-64E LiDAR sensor. The images have 1216352 resolution.The dataset is divided into training (86K samples), vali-dation (7K samples), and testing segments (1K samples).Ground truth is established by accumulating multiple Li-DAR frames and filtering out errors, which results in denserLiDAR depths (about 20% density).",
  ". Experimental Results": "Sensor Agnsoticity.We assess the versatility of ourmethod and the SoTA methods across various density levels.They are commonly trained with a standard training protocol,e.g., 500 random depth samples from the NYUv2 datasetand 64 lines on the KITTI DC dataset. We test them underexactly the same conditions. For the NYUv2 dataset, wesample fewer samples (from 200 to 1 depth point) than thatused in training phase. In addition, we use less scanning lines(from 32 to 1 line) than the original KITTI dataset.As shown in Tabs. 1 and 2, our method consistently pro-vides the superior results in almost test conditions. Whilemethods such as NLSPN and CompletionFormer demonstrate their robustness with 200 samples in NYUv2dataset and the 32-line scenario in KITTI dataset, respec-tively, our approach outperform the SoTA models in the",
  "RMSEMAE DELTA1 RMSEMAE DELTA1 RMSEMAE DELTA1 RMSEMAE DELTA1 RMSEMAE DELTA1 RMSEMAE DELTA1": "CSPN0.4902 0.3102 0.8323 0.1538 0.0802 0.9877 0.1108 0.0468 0.9937 0.7657 0.5401 0.6310 0.3419 0.2235 0.8055 0.2869 0.1533 0.9466S2D1.3680 1.0730 0.3006 0.5608 0.3134 0.8827 0.4794 0.4331 0.6836 0.8988 0.6230 0.6265 0.8430 0.6048 0.6208 0.8322 0.5514 0.7678NLSPN0.4639 0.3018 0.8554 0.1516 0.0758 0.9882 0.1136 0.0466 0.9933 1.2200 0.9202 0.3982 0.3758 0.2553 0.7871 0.3753 0.2329 0.9207DySPN0.4473 0.2700 0.8832 0.1487 0.0779 0.9887 0.1088 0.0439 0.9935 0.6700 0.4117 0.7461 0.3908 0.2654 0.7693 0.3891 0.2138 0.9237CostDCNet0.4701 0.2946 0.8569 0.1458 0.0717 0.9883 0.1248 0.0557 0.9921 0.4164 0.2649 0.8955 0.2160 0.1265 0.9449 0.2205 0.0992 0.9788CompletionFormer0.4776 0.2957 0.8510 0.1486 0.0754 0.9879 0.1183 0.0476 0.9925 0.8862 0.5993 0.6276 0.3486 0.2347 0.8207 0.6187 0.3713 0.8614 Ours+MiDaS0.4472 0.2787 0.8567 0.1722 0.0754 0.9827 0.1403 0.0549 0.9884 0.4478 0.2840 0.8608 0.2334 0.1257 0.9691 0.2803 0.1252 0.9574Ours+KBR0.3632 0.2282 0.8939 0.1503 0.0651 0.9865 0.1170 0.0449 0.9922 0.3133 0.1980 0.9434 0.2007 0.1024 0.9697 0.2110 0.0945 0.9776Ours0.3997 0.2418 0.8825 0.1453 0.0634 0.9874 0.1081 0.0419 0.9937 0.2961 0.1766 0.9291 0.2060 0.1075 0.9701 0.2328 0.0958 0.9693",
  ". Cross-validation between Indoor and Outdoor dataset": "more challenging scenarios. The depth prompt encoder con-tributes to constructing an adaptive representation for ran-domly given seeds, regardless of the pattern and density.We observe that a majority of SoTA methods heavilydepend on predefined input configurations. Spatial propaga-tion, a prevalent technique among these methods, relies onrelations among neighboring pixels, requiring a substantialnumber of seeds to cover an entire scene. This dependencyresults in significant performance deterioration in scenarioswith sparser initial depth seeds. In addition, SAN , whichare engineered to merge depth and image features at the latefusion to achieve stability in varying sparsity conditions, alsoencounter the performance drop in Tabs. 1 and 2.In contrast, thanks to the knowledge of the founda-tion model and the depth-oriented prompt engineering, ourmethod achieves relatively stable performance. Our promptmodule enables the construction of an adaptive affinity mapaccording to the distribution of input data, whose effective-ness is enlarged by the zero-shot generalization for unseenvisual attributes and data distributions.Cross-validation between Indoor and Outdoor.To con-duct a cross-validation between outdoor and indoor scenar-ios, we finetune our model and the comparison methods with only 10 and 100 images. Since active sensors providemetric depth to the model, the domain adaption via a fewground-truth level annotations is inevitable. Following ,we randomly select the pair of images and depth data. Asshown in Tab. 3, our method shows the superior performancethan the SoTA methods, which demonstrates the modelseffectiveness in preserving visual features across varyingscan ranges and its successful adaptation of the pre-trainedknowledge to different domains.",
  ". Sparsity, Pattern and Range Biases": "To assess the effectiveness of our model against the sensorbias issues, we design experiments with varying conditions:sparsity (from 500 to 50 samples), patterns (from randomto grid), and range changes (from 0m3m to 3m10m).We also design experiments for the outdoor scenario thatvary across three key conditions: sparsity (from 64-Line to8-Line), patterns (from line to random), and range changes(from 0m15m to 15m80m). For a fair comparison, allmodels do not conduct RDA for this experiment, includingour proposed method.Tabs. 4 and 5 reveals that the previous methods face sig-nificant challenges on the bias issues. In addition, as demon-strated in Tabs. 1 and 2, the density changes also lead tosignificant performance drop, particularly the dense to sparsescenario. In contrast, to address the sparsity bias, our prompt-based method constructs adaptive relations among pixels toproperly propagate even in the changing conditions.Next, we investigate the negative impact of pattern bias.We observe that a model trained on depth data with a certainpattern suffers from limited generality due to the incompati-bility with abundant representation learned for latent spacesof other depth patterns. As shown in Tab. 4, we attribute thisto positional information combined with image features. Thecomparison models, being continuously exposed to a fixed",
  ". Case study of sparsity, pattern, and range biases on the KITTI DC dataset": "pattern like a grid shape, are limited to its generalization.When image and depth information are jointly represented,this issue is further exacerbated. On the other hand, from theresults, we find out that random sampling offers benefits akinto augmentation effects, making the model generality better.Our method allows the transfer of depth information trainedfrom various sparse patterns to the model, which providesthe same effect as random sampling.Lastly, we check the range bias. In the training phase,we only use depth data whose maximum depth range is 3m.All the models are tested using depth data whose min/maxrange of the depth distribution is set to [3m, 10m]. As shownin Tab. 4, it becomes evident that most methods exhibitpoor generalization performance. Notably, the Completion-Former and NLSPN struggle to produce the generalperformance for the near and far regions. Our framework ef-fectively tackles the challenge using the foundational modeldesigned for monocular depth estimation. Based on the foun-dation model, which predicts relative depth maps for allpixels, our method infers absolute depth maps, which ex-tends the sensors limited scan ranges. shows a significant distinction between our methodand others in depth map reconstruction. While the compari-son methods face challenges in accurately representing scenedepth, especially in areas where input seeds are provided,our method excels in reconstructing the entire depth map.One notable observation is about the scenarios involvingthe changes in scanning ranges (-(c)). Our methoduniquely overcomes the common bias problem. This betterperformance is attributed to the strengths of our foundationmodels knowledge and the sensor-adaptive depth prompt.",
  "SparsityPatternRangeParam. Inference": "w/o SPN Eq.(1)0.498 / 0.334 0.145 / 0.096 0.546 / 0.37953.3M61.7msw/o Prompt Eq.(2)0.452 / 0.288 0.301 / 0.207 0.686 / 0.55149.7M54.9msw/o Pretrain Eq.(3)0.409 / 0.249 0.118 / 0.049 1.283 / 0.934 326.9M64.4msw/o LS-solver Eq.(4) 0.416 / 0.268 0.118 / 0.052 0.520 / 0.30553.4M61.3msw/ RDA0.231 / 0.134 0.113 / 0.046 0.426 / 0.25153.4M63.9msOurs0.400 / 0.242 0.108 / 0.0420 0.206 / 0.10853.4M63.9ms",
  "NYU 100NYU 8NYU 1KITTI 16KITTI 4KITTI 1": "NLSPN0.178 / 0.089 0.434 / 0.290 0.649 / 0.491 1.662 / 0.620 2.307 / 0.930 3.271 / 1.464C.Former 0.182 / 0.090 0.434 / 0.289 0.648 / 0.487 2.179 / 0.796 3.291 / 1.363 5.428 / 2.457Ours0.178 / 0.087 0.367 / 0.246 0.404 / 0.285 1.351 / 0.413 1.951 / 0.763 2.823 / 1.268",
  ". Ablation Study": "Adaption to Foundation Models.To evaluate the versa-tility of our method with various foundational models, wereplace our primary backbone with MiDaS and KBR .The MiDaS and KBR are developed for relative scale depthusing large-scale datasets as well. Tab. 4 reveals that KBRoutperforms other methods, including our own variant us-ing the MiDaS backbone. We argue that the self-supervisedtraining of KBR, unlike the supervised manner of MiDaS,provides a more generalizable feature space, dealing withchallenging conditions .Component Ablation of the Proposed Method.We per-form an additional ablation study on each component of ourmodel in Tab. 6. The study reveals that the RDA methodnotably reduces the sparsity bias (w/ RDA). For the rangebias, the pre-trained knowledge from the backbone con-tributes to performance improvement (w/o Pretrain). Ourdepth-oriented prompt engineering contributes to the overallperformance (w/o Prompt). Additionally, the results of LS",
  "RGBTraining InputSparse Depth (Test Phase)GT": ". Qualitative results for the changes of measurement patterns, sparsity, and scan ranges. We visualize images, input examples in thetraining phase, sparse depths in the test phase, and GT in the first row. solver Eq.(4) show that the sensor biases are not addressedvia the naive scale fitting, but are solved by the combinationsof our components. As a solution, our idea is to exploit theSPN module to use the initial dense depth, which is alignedrelative depth from the backbone with sparse absolute-scaledepth (w/o SPN).Random Depth Augmentation.RDA is an effective strat-egy to mitigate the issue of sparsity bias. To evaluate its com-patibility and adaptability with other methods, we conductan ablation study incorporating RDA into NLSPN , andCompletionFormer . The results, as described in Tab. 7,demonstrate notable performance improvements in sparse in-put scenarios. This highlights that the RDA not only naturallyimproves the models ability to generalize across differentlevels of data sparsity, but also becomes more effective whenused together with prompt engineering. 4.5. Zero-shot Inference on Commercial SensorsWe verify our methods zero-shot generality by testing iton different datasets without any additional training. Weuse our model trained on NYUv2 and KITTI DC datasets, then apply it to dataset taken from various sensorssuch as Apple LiDAR , Intel RealSense , and 32-Line Velodyne LiDAR . Here, for the Apple LiDARdataset, we directly capture a set of indoor images usingiPad Pro. As shown in , our method is applicable forApple LiDAR compared to ARKit , which is a built-in framework on iOS devices. Additionally, it shows bettergenerality with consistent results in the VOID dataset , collected using a stereo sensor in RealSense. Remarkably,our model, initially trained on 64-Line Velodyne LiDAR,excels in handling the NuScenes dataset captured withfewer channel LiDAR over the second best approach inSec. 4.2. Note that we describe details about the experimentalsetup, more quantitative and qualitative results, and furtheranalysis in the supplemental materials.",
  ". Conclusion": "We introduce a novel depth prompting technique, leveraginglarge-scale pre-trained models for high-fidelity depth estima-tion in metric scale. This approach significantly addressesthe challenges of well-known sensor biases associated withfixed sensor densities, patterns, and limitation of range andenables sensor-agnostic depth prediction. Through the com-prehensive experiments, we demonstrate the stability andgenerality of our proposed method, showcasing its superi-ority over existing methodologies. Furthermore, we verifyour method on a variety of real-world scenarios throughzero/few-shot inference across diverse sensor types. Acknowledgement This research was supported by Project for Sci-ence and Technology Opens the Future of the Region program throughthe INNOPOLIS FOUNDATION funded by Ministry of Science andICT (Project Number: 2022-DD-UP-0312), GIST-MIT Research Col-laboration grant funded by the GIST in 2024, the Ministry of Trade,Industry and Energy (MOTIE) and Korea Institute for Advancementof Technology (KIAT) through the International Cooperative R&Dprogram in part (P0019797) and the Korea Agency for InfrastructureTechnology Advancement(KAIA) grant funded by the Ministry of LandInfrastructure and Transport (Grant RS-2023-00256888). Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, AntoineMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-sch, Katherine Millican, Malcolm Reynolds, Roman Ring,Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,Sina Samangooei, Marianne Monteiro, Jacob L Menick, Se-bastian Borgeaud, Andy Brock, Aida Nematzadeh, SahandSharifzadeh, Miko aj Binkowski, Ricardo Barreira, OriolVinyals, Andrew Zisserman, and Karen Simonyan. Flamingo:a visual language model for few-shot learning. In Proceed-ings of the Neural Information Processing Systems (NeurIPS),2022. 2 Jinwoo Bae, Sungho Moon, and Sunghoon Im. Deep digginginto the generalization of self-supervised monocular depth es-timation. In Proceedings of the AAAI Conference on ArtificialIntelligence (AAAI), 2023. 3 Ayush Bhandari, Achuta Kadambi, Refael Whyte, ChristopherBarsi, Micha Feigin, Adrian Dorrington, and Ramesh Raskar.Resolving multipath interference in time-of-flight imagingvia modulation frequency diversity and sparse regularization.Optics letters, 39(6):17051708, 2014. 2",
  "Reiner Birkl, Diana Wofk, and Matthias Muller. Midas v3.1 a model zoo for robust monocular relative depth estimation.arXiv preprint arXiv:2307.14460, 2023. 2, 3, 4, 7": "Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Alt-man, Simran Arora, Sydney von Arx, Michael S Bernstein,Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.On the opportunities and risks of foundation models. arXivpreprint arXiv:2108.07258, 2021. 2 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,Pranav Shyam, Girish Sastry, Amanda Askell, et al. Languagemodels are few-shot learners. Proceedings of the Neural In-formation Processing Systems (NeurIPS), 2020. 2, 3",
  "Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl: Re-duce memory, not parameters for efficient on-device learning.2020. 2, 4": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-offrey Hinton. A simple framework for contrastive learningof visual representations. In Proceedings of the InternationalConference on Machine Learning (ICML), 2020. 7 Xinjing Cheng, Peng Wang, and Ruigang Yang. Depth esti-mation via affinity learned with convolutional spatial propa-gation network. In Proceedings of the European Conferenceon Computer Vision (ECCV), 2018. 1, 2, 3, 5",
  "Andreas Geiger, Philip Lenz, Christoph Stiller, and RaquelUrtasun. Vision meets robotics: The kitti dataset. The Inter-national Journal of Robotics Research (IJRR), 32(11):12311237, 2013. 1, 4": "Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, BailanHe, Gengyuan Zhang, Ruotong Liao, Yao Qin, Volker Tresp,and Philip Torr. A systematic survey of prompt engineer-ing on vision-language foundation models. arXiv preprintarXiv:2307.12980, 2023. 2 Vitor Guizilini, Rares Ambrus, Wolfram Burgard, and AdrienGaidon.Sparse auxiliary networks for unified monocu-lar depth prediction and completion.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), 2021. 2, 5, 6 Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy Ren, andXiaogang Wang. Learning monocular depth by distillingcross-domain stereo networks. In Proceedings of the Euro-pean Conference on Computer Vision (ECCV), 2018. 1",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceedingsof the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), 2016. 4": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, BrunaMorrone, Quentin De Laroussilhe, Andrea Gesmundo, MonaAttariyan, and Sylvain Gelly. Parameter-efficient transferlearning for nlp. In Proceedings of the International Confer-ence on Machine Learning (ICML), 2019. 4 ShahramIzadi,DavidKim,OtmarHilliges,DavidMolyneaux, Richard Newcombe, Pushmeet Kohli, JamieShotton, Steve Hodges, Dustin Freeman, Andrew Davison,and Andrew Fitzgibbon. Kinectfusion: Real-time 3d recon-struction and interaction using a moving depth camera. InProceedings of the 24th Annual ACM Symposium on UserInterface Software and Technology, 2011. 1 Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and TomDuerig. Scaling up visual and vision-language representationlearning with noisy text supervision. In Proceedings of theInternational Conference on Machine Learning (ICML), 2021.2",
  "prompt tuning. In Proceedings of the European Conferenceon Computer Vision (ECCV), 2022. 4": "Jaewon Kam, Jungeon Kim, Soongjin Kim, Jaesik Park, andSeungyong Lee. Costdcnet: Cost volume based depth comple-tion for a single rgb-d image. In Proceedings of the EuropeanConference on Computer Vision (ECCV), 2022. 5 Leonid Keselman, John Iselin Woodfill, Anders Grunnet-Jepsen, and Achintya Bhowmik. Intel realsense stereoscopicdepth cameras. In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition Workshop(CVPRW), 2017. 1, 8",
  "Diederik P Kingma and Jimmy Ba. Adam: A method forstochastic optimization.arXiv preprint arXiv:1412.6980,2014. 5": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-thing. arXiv preprint arXiv:2304.02643, 2023. 2 Robert Lange, Peter Seitz, Alice Biber, and Stefan C Laux-termann. Demodulation pixels in ccd and cmos technologiesfor time-of-flight ranging. In Sensors and camera systems forscientific, industrial, and digital photography applications,pages 177188. SPIE, 2000. 1",
  "Yuenan Li, Jin Wu, and Zetao Shi. Lightweight neural net-work for enhancing imaging performance of under-displaycamera. IEEE Transactions on Circuits and Systems for VideoTechnology (CSVT), 2023. 3": "Zhenyu Li, Zehui Chen, Xianming Liu, and Junjun Jiang.Depthformer: Exploiting long-range correlation and local in-formation for accurate monocular depth estimation. MachineIntelligence Research, pages 118, 2023. 2, 4, 5 Yuankai Lin, Tao Cheng, Qi Zhong, Wending Zhou, and HuaYang. Dynamic spatial propagation network for depth com-pletion. In Proceedings of the AAAI Conference on ArtificialIntelligence (AAAI), 2022. 1, 2, 3, 5 Sifei Liu, Shalini De Mello, Jinwei Gu, Guangyu Zhong,Ming-Hsuan Yang, and Jan Kautz.Learning affinity viaspatial propagation networks. In Proceedings of the NeuralInformation Processing Systems (NeurIPS), 2017. 1, 2, 3 Weihuang Liu, Xi Shen, Chi-Man Pun, and Xiaodong Cun.Explicit visual prompting for low-level structure segmenta-tions. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), 2023. 2 Yihao Liu, Jingwen He, Jinjin Gu, Xiangtao Kong, Yu Qiao,and Chao Dong. Degae: A new pretraining paradigm for low-level vision. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), 2023. 3",
  "Donald W Marquardt. An algorithm for least-squares esti-mation of nonlinear parameters. Journal of the society forIndustrial and Applied Mathematics, 11(2):431441, 1963. 4": "Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon,and Donggeun Yoo. Reducing domain gap by reducing stylebias. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), 2021. 3 Ayushya Pare, Shufang Zhang, and Zhichun Lei. Multipathinterference suppression in time-of-flight sensors by exploit-ing the amplitude envelope of the transmission signal. IEEEAccess, 8:167527167536, 2020. 2 Jinsun Park, Kyungdon Joo, Zhe Hu, Chi-Kuei Liu, and In SoKweon. Non-local spatial propagation network for depthcompletion. In Proceedings of the European Conference onComputer Vision (ECCV), 2020. 1, 2, 3, 5, 7, 8 Jin-Hwi Park, Jaesung Choe, Inhwan Bae, and Hae-Gon Jeon.Learning affinity with hyperbolic representation for spatialpropagation. In Proceedings of the International Conferenceon Machine Learning (ICML), 2023. 1, 2, 3 Adam Paszke, Sam Gross, Soumith Chintala, GregoryChanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-ban Desmaison, Luca Antiga, and Adam Lerer. Automaticdifferentiation in pytorch. In Proceedings of the Neural In-formation Processing Systems Workshop (NeurIPS-W), 2017.5 Jiaxiong Qiu, Zhaopeng Cui, Yinda Zhang, Xingdi Zhang,Shuaicheng Liu, Bing Zeng, and Marc Pollefeys. Deepli-dar: Deep surface normal guided depth prediction for outdoorscene from sparse lidar data and single color image. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), 2019. 4",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, DarioAmodei, Ilya Sutskever, et al. Language models are unsuper-vised multitask learners. OpenAI blog, 1(8):9, 2019. 2": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervision.In Proceedings of the International Conference on MachineLearning (ICML), 2021. 2 Thomas Richter, Jurgen Seiler, Wolfgang Schnurrer, andAndre Kaup. Robust super-resolution for mixed-resolutionmultiview image plus depth data. IEEE Transactions onCircuits and Systems for Video Technology (CSVT), 26(5):814828, 2015. 3 Justyna Sarzynska-Wawer, Aleksander Wawer, Aleksan-dra Pawlak, Julia Szymanowska, Izabela Stefaniak, MichalJarkiewicz, and Lukasz Okruszek. Detecting formal thoughtdisorder by deep contextualized word representations. Psy-chiatry Research, 304:114135, 2021. 2",
  "Brent Schwarz. Mapping the world in 3d. Nature Photonics,4(7):429430, 2010. 1, 8": "Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and RobFergus. Indoor segmentation and support inference fromrgbd images. In Proceedings of the European Conference onComputer Vision (ECCV), 2012. 2, 4, 8 Jaime Spencer, Chris Russell, Simon Hadfield, and RichardBowden. Kick back & relax: Learning to reconstruct theworld by watching slowtv. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision (ICCV), 2023.2, 3, 4, 7 Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, SaraFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-mamoorthi, Jonathan Barron, and Ren Ng. Fourier featureslet networks learn high frequency functions in low dimen-sional domains. In Proceedings of the Neural InformationProcessing Systems (NeurIPS), 2020. 2 Tommi Tykkala, Cedric Audras, and Andrew I Comport. Di-rect iterative closest point for real-time visual odometry. InProceedings of the IEEE International Conference on Com-puter Vision Workshop (ICCVW), 2011. 2",
  "Attila Vidacs and Geza Szabo.Winning ariac 2020 bykissing the bear: Keeping things simple in best effort agilerobotics. Robotics and Computer-Integrated Manufacturing,71:102166, 2021. 2": "Tongzhou Wang and Phillip Isola. Understanding contrastiverepresentation learning through alignment and uniformity onthe hypersphere. In Proceedings of the International Confer-ence on Machine Learning (ICML), 2020. 7 Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, KyleGenova, Prem Nair, Kenji Hata, and Olga Russakovsky. To-wards fairness in visual recognition: Effective strategies forbias mitigation. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), 2020.3 Yao Wei, Yanchao Sun, Ruijie Zheng, Sai Vemprala, Roge-rio Bonatti, Shuhang Chen, Ratnesh Madaan, Zhongjie Ba,Ashish Kapoor, and Shuang Ma. Is imitation all you need?generalized decision-making with dual-phase training. InProceedings of the IEEE/CVF International Conference onComputer Vision (ICCV), 2023. 6",
  "completion. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), 2022. 1": "Lu Xia, Chia-Chih Chen, and J. K. Aggarwal. View invari-ant human action recognition using histograms of 3d joints.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition Workshop (CVPRW), 2012. 1 Zhihao Xia, Patrick Sullivan, and Ayan Chakrabarti. Generat-ing and exploiting probabilistic monocular depth estimates.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), 2020. 2",
  "Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Si-mon Chen, and Chunhua Shen. Towards domain-agnosticdepth completion. arXiv preprint arXiv:2207.14466, 2022. 2,3": "Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,Boxin Li, Chunyuan Li, et al. Florence: A new foundationmodel for computer vision. arXiv preprint arXiv:2111.11432,2021. 2 Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, LucVan Gool, and Radu Timofte. Plug-and-play image restorationwith deep denoiser prior. IEEE Transactions on Pattern Anal-ysis and Machine Intelligence (TPAMI), 44(10):63606376,2022. 3 Renrui Zhang, Han Qiu, Tai Wang, Ziyu Guo, Ziteng Cui,Yu Qiao, Hongsheng Li, and Peng Gao. Monodetr: Depth-guided transformer for monocular 3d object detection. InProceedings of the IEEE/CVF International Conference onComputer Vision (ICCV), 2023. 4"
}