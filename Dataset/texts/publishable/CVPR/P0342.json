{
  "Abstract": "We propose a pipeline that leverages Stable Diffusion toimprove inpainting results in the context of defurnishingthe removal of furniture items from indoor panoramaimages. Specifically, we illustrate how increased context,domain-specific model fine-tuning, and improved imageblending can produce high-fidelity inpaints that are geo-metrically plausible without needing to rely on room layoutestimation.We demonstrate qualitative and quantitativeimprovements over other furniture removal techniques.",
  ". Introduction": "The recent advancement of digital technologies has rev-olutionized various industries, and real estate is no ex-ception .The emergence of digital twinsvirtualreplicas of physical assetshas significantly impactedthe way properties are marketed, managed, and visual-ized . Digital twins offer a plethora of benefits, includ-ing enhanced decision-making , improved design pro-cesses , and immersive experiences for stakeholders .While digital twins excel in replicating physical struc-tures with remarkable accuracy, they often lack the flexibil-ity to adapt to different downstream scenarios and require-ments. One critical aspect that remains underexplored isdefurnishing: removing furnishings and objects from vir-tual representations of built environments . Defurnish-ing offers numerous advantages, such as enabling potentialbuyers or renters to envision personalized layouts , fa-cilitating interior design experimentation , and provid-ing insights for property evaluation and renovation . Itis a fundamental component of any workflow dealing withdigital twins, particularly when captured from a built envi-ronment rather than synthesized from the ground up.In this paper, we focus on defurnishing indoor panoramaimages, leaving the study of defurnishing other common",
  "Our pipeline": ".Indoor panorama defurnishing.Our custom fine-tuning of Stable Diffusion inpainting reduces its tendency to hal-lucinate objects near shadows and reflections, such as the radiatorson the walls and lamp in the corner. representations of digital twins, such as polygon meshes,to future work. Additionally, while a digital twin may becomposed of several posed panorama images, we focus ondefurnishing single images in isolation and do not considerconsistency between multiple views.Recent advances in generative models, such as latent dif-fusion models , have dramatically improved the qualityof image inpainting by leveraging learned contextualpriors to better complete the missing information in an im-age. However, as these models are commonly trained toinpaint objects and scene content rather than empty space,spurious inpaints or hallucinations tend to be a sig-nificant problem when using these models in the contextof defurnishing. Specifically, Stable Diffusion attempts to",
  "arXiv:2405.03682v1 [cs.CV] 6 May 2024": "explain away contextual information such as shadows andobject parts left behind from small imperfections in the in-painting mask by placing objects where empty space wasexpected (see ). This issue can be partially resolvedby applying morphological operations such as dilation tothe inpainting mask , but this does not adequately han-dle all cases of shadows, light beams, and other similar ef-fects, which are related to the furniture but not co-locatedwith it .Consequently, we present a system for automatic furni-ture removal, at the core of which is a custom inpaintingmodel that builds over Stable Diffusion in the followingways:1. Leverages a custom dataset of unfurnished panoramasand an augmentation strategy to make the model robustto imperfect masks, cast light beams, reflections, andshadows, significantly reducing hallucinations.",
  ". Inpainting": "Digital inpainting is the task of generating pixels to com-plete missing regions of an image .Many inpaintingmethods have been developed over the past decades, rang-ing from approaches using classical techniques to those leveraging deep neural networks .Early learned approaches using Convolutional NeuralNetworks (CNNs) pioneered the use of Generative Adver-sarial Networks (GANs) for inpainting . Par-tial convolutions and their learned counterparts, gatedconvolutions , allowed inpainting with arbitrary maskshapes by propagating masks through the network layers.Many models employed a two-stage pipeline that gener-ated intermediate representations to aid in the final inpaint-ing step, such as coarse resolution inpaints , edgemaps , and semantic segmentation masks . To han-dle large masks, fast Fourier convolutions were shownto increase the effective receptive field of the neural networkand aid in understanding the global structure of the image.",
  "Given a panoramic image of an indoor scene, furniture re-moval generally involves two important steps :1. Segment all instances of furniture to remove": "2. Inpaint on the combined furniture masks.While an off-the-shelf semantic segmentation model trained on a suitable ontology is sufficientfor the first step, a number of different approaches have been explored to obtain realistic inpaintsusing CNN GAN inpainting models.PanoDR pro-posed that in order to produce inpaints that preserve real-life structures, predicting the room layout (e.g. segmenta-tion for floor, walls, and ceiling) is a crucial pre-processingstep. To address perspective distortion and mixing texturescoming from different surfaces, Kulshreshtha et al. used the room layout to separate the room into distinct 3Dplanes , then inpainting each plane separately after or-thographic reprojection. Gao et al. improved upon thisby performing inpainting on each 3D plane in a single end-to-end pipeline.These approaches all leverage a room layout estima-tor to explicitly or implicitly guide inpainting. Thismakes it difficult to generalize these models to handle cer-tain features common to real-life room layouts, such ascurved walls, kitchen islands and half walls, and staircases.With recent advances in image inpainting techniques, westudy whether structural cues from room layout estimationare necessary for creating high-fidelity and geometricallyplausible inpaints.",
  ". Stable Diffusion": "Latent diffusion models, such as Stable Diffusion (SD) ,have recently risen to the forefront of image generation anddemonstrated results that out-perform their GAN counter-parts . Diffusion models are readily scalable to bet-ter model the complex distribution of training data and cansample diverse inpaints at high fidelity . SD is a text-to-image model trained on a large image dataset that canalso be conditioned by multi-modal inputs, including linecontours, depth maps, and other images for image-to-imagetranslation .Similar to other generative AI models, like large lan-guage models, SD suffers from hallucinationsgenerationof plausible but incorrect informationdue to issues suchas training data impurities and bias, and attention dilution atinference . Understanding why hallucinations occurand how they can be mitigated are research areas that arecritically important for using these generative models fordownstream tasks and in our daily lives.In the case of inpainting for defurnishing, one root causefor hallucinations, which we hypothesize is very significant,is data bias and object co-occurrence . If the datasetused to train SD contains more images of furnished indoorrooms than unfurnished ones, then when the trained modelis presented with an image of an indoor room, it wouldbe biased towards generating inpaints including furniture.In other words, the dataset may contain a bias where an . Defurnishing pipeline. The input to our system is a 81924096 pixel equirectangular panorama, which we crop to 3:1 aspectratio to exclude the poles. Pre-processing: We obtain a binary furniture mask via semantic segmentation. Both input and mask are rolled,so that inpainting regions are in the center of the image, and padded, to ensure sufficient context (note the repeated doorway and cupboard inthe example). The images are then downsampled to a height of 512 pixels. Inpainting: Our custom process is robust to inexact masks andremnant shadows, as detailed in the method section. Post-processing: We apply 4 superresolution to the inpainted output and then blendthe original and inpainted panoramas into the final result using the mask, keeping as much of the original resolution details as possible. indoor room and furniture often co-occur leading themodel to learn this association and spuriously hallucinatefurniture in images of indoor rooms.Regarding dataset bias, one remedy would be to choosetext prompts that guide the inpaint towards the desired out-come. Another approach is to fine-tune the diffu-sion model on another dataset where this bias has been rec-tified, trading diversity for fidelity in a particular use-case.",
  ". Method": "Our method takes a high resolution panorama image as in-put and outputs a defurnished version of it, as visualized in. We start by semantic segmentation of all furniture.Next, we run our custom unfurnished space inpainting. Asit is SD-based, it is done at a lower resolution, so finallywe run superresolution and blend the inpainted and originalimages in order to obtain a visually pleasing final result. Weelaborate on each of these steps in the following.",
  ". Pre-processing": "Furniture segmentationWe use an off-the-shelf seman-tic segmentation network to identify all instancesof furniture classes, as defined in the ADE20K bench-mark . Their union is our inpainting mask. Note that,in our experience, modern semantic segmentation networks work equally well on equirectangular and perspective im-ages. This spares us the back-and-forth conversion betweena panorama and a set of perspective images that other meth-ods necessitate , yielding a continuous mask and fasterprocessing in our case. Context maximizationNext, we ensure that our imageis set up optimally for inpainting. To this end, we roll thepanorama around horizontally so that the masked objectsare as close to the center as possible. There is no guaranteethat there will be no masked object on the image edge, sowe additionally wrap-pad the image horizontally to providesufficient context for the ensuing inpainting. We apply thesame transformations to the corresponding mask image.",
  ". Robust unfurnished space inpainting": "points to three major issues with SD inpainting thatwe aim to solve:1. It is not readily applicable to non-square images.2. It is prone to hallucinating objects.3. It is low resolution and lacks high frequency details.Hallucinations are a known issue for large generative mod-els . The same prompt gives different results withdifferent random seeds, some of which do not exhibit hal-lucinations, but this is unreliable in an automated system.",
  ". Training dataset examples. Synthetic furniture itemsand shadows are rendered over real unfurnished panoramas": "This issue is multi-faceted in the case of inpainting. First,inpainting masks may be inaccurate, leaving visible parts ofobjects which SD picks up and completes into full objects.While this can be addressed with mask dilation, valuablecontext and detail is lost. Even with perfect object masks,there are shadows left, which SD aims to explain by inpaint-ing objects that may have cast them. Second, our case ofinpainting empty space is especially challenging, becausegenerative models are good at creating concrete objects withdescribable properties (such as round wooden kitchen ta-bles), but struggle with generating imagery for vague con-cepts like emptiness. Negative prompts are another way, butwhile they can make hallucinated objects look less like thenegative prompt, they do not remove the objects.We fine-tune a version of SD inpainting to address thefirst two issues.For the third one, we develop a post-processing routine described in .3. DatasetTo be able to successfully inpaint empty space,we train on a proprietary dataset of 160k equirectangu-lar panoramas of unfurnished residential spaces, similar toMatterport3D . As discussed, the use of equirectan-gular panoramas ensures that we maximize the context at in-painting time. We found that SD quickly adapts to this kindof imagery after being exposed to fewer than 1000 panora-mas. We find this strategy to be better than applying SD toa tiled version of the equirect, because all context is presentin one image. Synthetic data & augmentationsTo make inpainting ro-bust to shadows and inaccurate masks, we augment theempty spaces with synthetically rendered furniture objectsfrom a dataset like Objaverse .The rendering is notphotorealistic, but includes shadows, as shown in .We fine-tune a pre-trained SD inpainting model using asinputs the unfurnished space panoramas with syntheticallyrendered objects and shadows, and masks that only coverthe objects and not the shadows. The target outputs are theoriginal unfurnished panoramas. This fine-tuning discour-ages SD from hallucinating objects when seeking to explainaway effects of the furniture on the scene, such as shadows,reflections, or light beams. Additionally, to make our modelrobust to mask inaccuracies, we perturb the input masks tosimulate imperfect semantic segmentation output.",
  ". Initialization of latents for inpainting. With decreas-ing percentage of noise in the initialization, hallucinations in thehallway decrease, but blurriness on the inpainted floor increases": "PromptsSD is a text-to-image model, and consequentlythe text prompt(s) used for our inpainting task play(s) animportant role.The prompt we use at inference time isempty room, but we found that training with a set of simi-lar prompts further reduces hallucinations, as shown in Fig-ure 4. We randomly select one prompt from the set per train-ing sample (prompts for the same sample may differ in dif-ferent epochs). The additional prompts contain synonyms,such as unfurnished; space, home, house; as well as append-ing descriptions such as uniformly blank. We tested with0 (empty prompt), 1, 8, 32, 72 and 180 prompts. In our ex-periments 0-8 prompts lead to models that are most proneto hallucination, 32 and 72 yield results with much fewerhallucinations, while more prompts result in increased hal-lucinations again. Thus, in our experiments we choose totrain with 32 variants of the empty room prompt. InitializationAt inference time we initialize the inpaint-ing latents as a weighted sum of 97% random noise and 3%latents based on the input image, blurred under the inpaint-ing mask. We empirically found this combination to bestminimize both hallucinations and blur, as shown in . ResourcesWe train for 96 hours on 8 NVIDIA A10GGPUs (24 GB vRAM) with an effective batch size of 96(using gradient accumulation). The base SD version that wefine-tune is stabilityai/stable-diffusion-2-inpainting and weonly perform low-rank adaptation (LoRA) , whereby we calculate update matrices for its UNet, rather than train-ing all its weights. We experimented with training the entireUNet and the VAE, each of which gave inferior results. Thiscould be due to our dataset size or other factors, which wedid not investigate further. We also fine-tuned SD XL, butfound it to be more prone to hallucinations.We only use one GPU for inference. We found that 10inference steps are sufficient. Our pipeline takes approx-imately 12 seconds to process a single panorama: 8s forpre-processing and semantic segmentation, 3.5s per imagefor inpainting alone, and 0.5s for post-processing.",
  ". Post-processing": "After inpainting, we apply an off-the-shelf superresolutionnetwork to upscale the inpainted image four times. Wethen undo the padding and rolling pre-processing transfor-mations to restore the original panorama dimensions. Fi-nally, we make sure that the inpainted and original texturesmatch well via a custom blending routine. BlendingSD-based inpainting tends to lack high fre-quency details, which is problematic in our setting, where ahigh-resolution final output is required. Therefore, we aimto preserve as much detail as possible from the original fur-nished panorama, and develop a tailored blending proce-dure that uses the binary inpainting mask to select how tocombine the original and inpainted images. As explainedin .2, our inpainting strategy is guided by the in-painting mask, but is trained to be robust to shadows byallowing for pixels nearby the mask to also be modified, asenforced by the dataset and training objective. In this way,we not only avoid hallucinating objects due to the remain-ing shadow, but we also remove the shadow itself from theimage, leading to an overall higher-quality inpainted result.Conversely, if we were to directly use the mask for blend-ing the original and inpainted panoramas, shadows wouldbe re-introduced in the final image. Therefore, we use thepixels from the inpainted version not only where the maskindicates, but also in nearby regions where the inpainted im-age is significantly different from the original. Similarly, ifthose significant changes are far away from the inpaintingmask, they are more likely to be spurious hallucinations, sowe use pixels from the original image. visualizesthe benefits of this strategy.",
  "Ours-full27.050.9307.7530.056": ".Quantitative evaluation.Our inpainting module isslightly worse than LaMa on absolute difference metrics, be-cause the underlying SD outputs lower-frequency textures. To-gether with superresolution and blending, our full pipeline outper-forms all related techniques. panorama image and a corresponding mask, and the outputis of the same resolution, i.e. no pre-processing to roll andpad or post-processing to upsample and blend is done.We compare to the following related approaches:",
  "LGPN-Net : estimates the room layout via Horizon-Net and uses its edges to guide a GAN for inpainting;": "Stable Diffusion 2.0 inpainting .Note that masks are dilated by 10-20 pixels for these meth-ods, as they are not specifically trained to handle mask inac-curacies like ours, and respectively their results were worsewithout the dilation. No dilation is applied for our method. Qualitative comparisonWe test on furnished panoramasfrom the Habitat dataset . compares the re-sults on a few examples.The ResNet- and GAN-basedapproaches, LaMa and LGPN-Net, tend to generate veryblurry results. Moreover, they leave pieces of furniture ob-jects un-inpainted. This is an even more prevalent effectwith smaller or no mask dilation, as shown in the supple-mentary document. On the other hand, SD and our custommodification of it generate much crisper textures, even invery complex scenes. While plain SD inpainting halluci-nates objects, like the tables in the first example, our methoddoes not. In addition, SD leaves shadows behind as in thesecond example. The training of our method enables it tomodify pixels outside of the inpainting mask, which is whyit manages to remove the shadow and inpaint it with plausi-ble floor texture. Quantitative comparisonTo quantitatively compare re-sults, we gather a selection of unfurnished panoramas fromthe Habitat dataset , insert synthetic furniture such as ta-bles and ottomans on the floors, and measure the differencebetween the defurnished images produced by each methodand the original unfurnished images. Note that the spacesproduced this way have simpler furniture setups than in the",
  "(a) Original(b) LaMa (c) LGPN-Net (d) SD-2-inpaint(e) Ours-inpaint": ". Defurnishing comparison. Examples are arranged in row pairs of full images and zoomed-in patches. Results from GAN-basedapproaches, LaMa and LGPN-Net, are blurry and contain remnants of furniture objects, even though inpainting masks had to be dilatedfor these methods to run optimally. SD generates crisper images, but may seek to explain shadows by hallucinating furniture. Our customfine-tuning removes the hallucinations, resulting in coherent textures, even though it is the only one that does not require mask dilation. natural images we used in the previous section. We eval-uate absolute differences via PSNR, and perceptual differ-ences via SSIM , LPIPS , and FovVideoVDP ,a kind of just-objectionable-difference (JOD) measure forwide field-of-view images or video. Here, in addition toour inpainting network (Ours-inpaint), we evaluate our fullpipeline (Ours-full), for a fairer comparison to a completedefurnishing method like LGPN-Net. The results are sum- marized in . As the synthetic furniture does not in-clude rugs and leaves large areas from the original floor-ing visible (shown in supplementary document), LaMa andLGPN-Net manage to propagate these textures well into theinpainted regions, and thus achieve good PSNR and SSIMscores. However, the borders between inpainted and unin-painted regions are blurry and disjoint, leaving these meth-ods with low perceptual scores. On the other hand, our",
  ". Ablation studies": "shows an example inpainting result using off-the-shelf SD weights, with and without mask dilation. Note thatwithout any mask dilation, the network hallucinates objectsdue to imperfect semantic segmentation and context fromshadows. If the mask is generously dilated, the networkdoes not have enough context to generate realistic inpaintsfor the kitchen island. Therefore, it is necessary to fine-tuneSD on a custom dataset to generate inpaints of unfurnishedspaces without requiring mask dilation. shows how the image generated by our in- painting network is combined with the original image ac-cording to the inpainting mask.The generated imageis low-resolution and often contains undesirable artifacts(e.g. there are no trees through the windows of 8b), sowe would prefer to use pixels of the original image whenpossible. However, navely replacing the generated imageinto the original image according to the inpainting mask(i.e. result = original(1mask)+generatedmask) createsvisually jarring outlines due to physical shadows and differ-ences in white-balance. While it is difficult to completelyreconcile the differences between the detail frequencies ofthe original and generated images, our blending techniquesmoothly combines the two images. demonstrates the effects of our rolling andpadding pre-processing steps. As the goal of these stepsis to ensure that inpaints are consistent across the seam ofthe panorama image, we show the crops at opposite edgesof the panorama joined together, so the panorama seam liesin the center of the image. With no rolling, we obtain a ver-",
  ". Limitations and future work": "While our method represents a step forward in defurnishing,it has limitations. Structural alterations and lingering hallu-cinations may occur, like the bench beneath the window in. Our training methodology, which involves ren-dering synthetic furniture into unfurnished panoramas, in-troduces potential domain shift issues. This discrepancy be-tween synthetic and real-world data may impact the qualityof results. While we fixed the number of training promptsbased on empirical evidence, the optimal prompting strat-egy remains an open question. Augmenting the fixed infer-ence prompt empty room with additional contextual in-formation could improve content generation accuracy. Our reliance on the Stable Diffusion 2.0 inpainting archi-tecture imposes constraints, necessitating modifications forhigher resolution training and inference. The resolution ofthe output of Stable Diffusion model is too low - only 512pixels in height, and the superresolution model sometimesmade the output warped or unnatural. One area of improve-ment would be to choose an optimal superresolution modelor to train one specifically on equirectangular images.The applicability of our strategy to other generative mod-els or imagery types besides indoor panoramas is out of thescope of this work and remains open for future research.Finally, our method lacks consideration for multi-viewconsistency, which is important for applications like digi-tal twins; each panorama is inpainted independently, over-looking valuable contextual information from other views.Future research should explore efficient and scalable ap-proaches to incorporate geometric priors into the inpaint-ing pipeline . Being able to inpaint consistently acrossdifferent views of the same underlying physical spacemay help with defurnishing other representations of digitaltwins, such as textured polygon meshes.",
  ". Conclusion": "We have presented a novel approach to panorama defurnish-ing by utilizing domain-specific fine-tuning for Stable Dif-fusion inpainting. We have observed a notable reductionin undesirable hallucinations and improved the models ro-bustness to imperfect segmentation masks, making it muchless likely to explain away shadows, light beams, reflec-tions, and other similar effects. When compared to exist-ing approaches, our method produces higher-quality resultswithout the need for room layout estimation, as indicated inboth qualitative and quantitative comparisons.",
  "Connelly Barnes, Eli Shechtman, Adam Finkelstein, andDan B. Goldman.Patchmatch: a randomized correspon-dence algorithm for structural image editing.ACM SIG-GRAPH 2009 papers, 2009. 2": "Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, andColoma Ballester.Image inpainting.In Proceedings ofthe 27th Annual Conference on Computer Graphics andInteractive Techniques, page 417424, USA, 2000. ACMPress/Addison-Wesley Publishing Co. 2 Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-ber, Matthias Niessner, Manolis Savva, Shuran Song, AndyZeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. International Conference on3D Vision (3DV), 2017. 4 Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, TongLu, Jifeng Dai, and Yu Qiao. Vision Transformer Adapter forDense Predictions. In The Eleventh International Conferenceon Learning Representations (ICLR), 2023. 2, 3",
  "Antonio Criminisi, Patrick Perez, and Kentaro Toyama. Ob-ject removal by exemplar-based inpainting. pages 721728,2003. 2": "Bruno Daniotti, Gabriele Masera, Cecilia Maria Bolognesi,Sonia Lupica Spagnolo, Alberto Pavan, Giuliana Iannac-cone, Martina Signorini, Simone Ciuffreda, Claudio Mirar-chi, Meherun Lucky, et al.The development of a bim-based interoperable toolkit for efficient renovation in build-ings: From bim to digital twin. Buildings, 12(2):231, 2022.1 Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,Oscar Michel, Eli VanderBilt, Ludwig Schmidt, KianaEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:A Universe of Annotated 3D Objects, 2022. 4 Juan Manuel Davila Delgado and Lukumon Oyedele. Digi-tal twins for the built environment: learning from conceptualand process models in manufacturing. Advanced Engineer-ing Informatics, 49:101332, 2021. 1",
  "Isogawa, Mariko and Mikami, Dan and Iwai, Daisuke andKimata, Hideaki and Sato, Kosuke. Mask Optimization forImage Inpainting. IEEE Access, 2018. 2": "GuanzhouJi,AzadehOSawyer,andSrinivasaGNarasimhan. Virtual Home Staging: Inverse Rendering andEditing an Indoor Panorama under Natural Illumination. InInternational Symposium on Visual Computing, 2023. 2, 3 Saleh Kalantari and Jun Rong Jeffrey Neo. Virtual environ-ments for design research: Lessons learned from use of fullyimmersive virtual reality in interior design research. Journalof Interior Design, 45(3):2742, 2020. 1 Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, andRoss Girshick. Segment Anything. arXiv:2304.02643, 2023.3",
  "Andreas Lugmayr, Martin Danelljan, Andres Romero, FisherYu, Radu Timofte, and Luc Van Gool. RePaint: Inpaintingusing Denoising Diffusion Probabilistic Models, 2022. 2": "Rafa K. Mantiuk, Gyorgy Denes, Alexandre Chapiro, AntonKaplanyan, Gizem Rufo, Romain Bachy, Trisha Lian, andAnjul Patney. FovVideoVDP: a visible difference predictorfor wide field-of-view video. ACM Transactions on Graphics(SIGGRAPH), 40(4), 2021. 6 Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Qureshi, andMehran Ebrahimi.Edgeconnect: Structure guided imageinpainting using edge prediction.In Proceedings of theIEEE/CVF International Conference on Computer Vision(ICCV) Workshops, 2019. 2 Stanley Osher, Martin Burger, Donald Goldfarb, Jinjun Xu,and Wotao Yin. An iterative regularization method for totalvariation-based image restoration. Multiscale Modeling &Simulation, 4(2):460489, 2005. 2",
  "Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, TrevorDarrell, and Alexei A. Efros.Context encoders: Featurelearning by inpainting, 2016. 2": "Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wi-jmans, Oleksandr Maksymets, Alexander Clegg, John MTurner, Eric Undersander, Wojciech Galuba, Andrew West-bury, Angel X Chang, Manolis Savva, Yili Zhao, and DhruvBatra. Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI. In Thirty-fifth Con-ference on Neural Information Processing Systems Datasetsand Benchmarks Track, 2021. 4, 5",
  "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:Convolutional networks for biomedical image segmentation,2015. 2": "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,Michael Rubinstein, and Kfir Aberman. DreamBooth: FineTuning Text-to-image Diffusion Models for Subject-DrivenGeneration. In IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), 2023. 3 Christoph Schuhmann, Romain Beaumont, Richard Vencu,Cade Gordon,Ross Wightman,Mehdi Cherti,TheoCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-man, Patrick Schramowski, Srivatsa Kundurthy, KatherineCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and JeniaJitsev. LAION-5B: An open large-scale dataset for trainingnext generation image-text models. In 36th Conference onNeural Information Processing Systems (NeurIPS), 2022. 2 Muhammad Shahzad, Muhammad Tariq Shafiq, Dean Dou-glas, and Mohamad Kassem. Digital twins in built environ-ments: an investigation of the characteristics, applications,and challenges. Buildings, 12(2):120, 2022. 1",
  "Yuhang Song, Chao Yang, Yeji Shen, Peng Wang, QinHuang, and C. C. Jay Kuo. Spg-net: Segmentation predictionand guidance network for image inpainting, 2018. 2": "Cheng Sun, Chi-Wei Hsiao, Min Sun, and Hwann-TzongChen. HorizonNet: Learning Room Layout With 1D Rep-resentation and Pano Stretch Data Augmentation.In TheIEEE Conference on Computer Vision and Pattern Recog-nition (CVPR), 2019. 2, 5 Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,Naejin Kong, Harshith Goka, Kiwoong Park, and VictorLempitsky. Resolution-robust Large Mask Inpainting withFourier Convolutions. Winter Conference on Applications ofComputer Vision (WACV), 2022. 2, 5, 6, 12, 17",
  "Alexandru Telea. An image inpainting technique based onthe fast marching method.Journal of Graphics Tools, 9,2004. 2": "S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, VinijaJain, Anku Rani, Vipula Rawte, Aman Chadha, and AmitavaDas. A Comprehensive Survey of Hallucination MitigationTechniques in Large Language Models, 2024. 2 Xintao Wang,Liangbin Xie,Chao Dong,and YingShan.Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data. In International Con-ference on Computer Vision Workshops (ICCVW), 2021. 5",
  "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, XiaogangWang, and Jiaya Jia. Pyramid scene parsing network, 2017.2": "Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, AdelaBarriuso, and Antonio Torralba.Scene Parsing throughADE20K Dataset. In IEEE Conference on Computer Visionand Pattern Recognition (CVPR), 2017. 2, 3 Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang,Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao.Analyzing and Mitigating Object Hallucination in LargeVision-Language Models. In The Twelfth International Con-ference on Learning Representations, 2024. 1, 2",
  "Visual results": "In , we provide larger versions of the defurnishing results from the main paper. In addition, we include resultsfrom the methods we compare to in .1, i.e. LaMa , LGPN-Net and SD-inpainting , with a smaller maskdilation of 10 pixels as well as with no mask dilation.LaMa and LGPN-Net demonstrate similar trends; a non-dilated mask is absolutely insufficientlarge dark patches areinpainted and the outlines of the inpainting mask are recognizable. Mask dilation remedies this effect, but the inpaintedregion tends to look like a blurry blob with increasing mask size. Conversely, the textures are sharpest with the non-dilatedmasks.In all examples, SD-inpainting hallucinates objects when the inpainting mask is not dilated. With a mask dilated by 10pixels, objects are still hallucinated in the first two examples. With a mask dilated by 20 pixels, only the first example showsvery noticeable hallucinations, e.g. the tables on the right, but larger shadows cast by the objects that are being removed,e.g. the couch in the second example, remain in the output image and may even be extended by the inpainting.Note that in the main paper, we chose the best-looking result for each of these three methods and each example separately.Ours-inpaint builds upon SD-inpainting but tackles the hallucinationsand indeed none of the examples have hallucina-tions. Ours-full makes sure that original textures are preserved wherever possible, which is valuable because of the originalimage has more detail, e.g. the kitchen island in the first example, and because our method may sometimes remove moredetails than necessary since it is specifically trained to remove shadows outside of the inpainting mask. Intricate textures maystill be an issue for our method, like the floor in the last image, where one plank is inpainted in a notably darker color.In addition, in we demonstrate that the inpainting component of our method is not influenced by mask dilation.There is no noticeable difference in the inpainted result in all examples but the first one, where the far away kitchen island isremoved as the mask gets larger, while it should remain because it is built-in. This example demonstrates the usefulness ofnon-dilated masks for keeping far-away details intact.Finally, in we show an example of an unfurnished space with synthetic furniture used for quantitative evaluationin .1."
}