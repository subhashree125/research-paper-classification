{
  "Abstract": "In this technical report, we present our findings from theresearch conducted on the Vast Vocabulary Visual Detec-tion (V3Det) dataset for Supervised Vast Vocabulary VisualDetection task. How to deal with complex categories anddetection boxes has become a difficulty in this track. Theoriginal supervised detector is not suitable for this task. Wehave designed a series of improvements, including adjust-ments to the network structure, changes to the loss function,and design of training strategies. Our model has shown im-provement over the baseline and achieved excellent rank-ings on the Leaderboard for both the Vast Vocabulary Ob-ject Detection (Supervised) track and the Open VocabularyObject Detection (OVD) track of the V3Det Challenge 2024.",
  "*Bosong Chai is the corresponding author. Bosong Chai and Peixi Wucontributed equally to this work": "over 13,000 object classes on real images. It includes a hi-erarchical category structure with detailed class affiliationsforming a comprehensive relationship tree. As shown inFig 1, with 245,000 annotated images and expert-generateddescriptions, V3Det is an invaluable resource for advancedobject detection research in computer vision. : V3Det is a high-quality, precisely annotated ob-ject detection dataset with a broad vocabulary, encompass-ing 13,204 categories. The figure shows annotated imagesamples from V3Det, featuring more complex and detailedannotations. This workshop has two tracks. The first track (Super-vised), called Vast Vocabulary Object Detection, aims toevaluate supervised learning models for object detectionacross all 13,204 classes in the V3Det dataset. Detectingany object has been a long-term goal in the field of com-",
  "arXiv:2406.09201v3 [cs.CV] 21 Jun 2024": "puter vision. Due to the countless diverse objects in the realworld, an ideal visual detection system should be capableof detecting a large number of categories and be applicableto open vocabulary categories. Currently widely used object detection datasets such asCOCO , Objects365 , and OpenImages v4 , de-spite providing a large number of images and categories,still have a limited vocabulary.The limited vocabularyof these datasets constrains the training potential of class-generalized detectors, as an ideal detector should be ableto recognize new categories beyond those in the trainingset. Even large vocabulary object detection datasets likeLVIS cannot fully represent the complexity of the realworld in terms of the number and diversity of categories.V3Det provides the research community with a large vo-cabulary object detection dataset, which can accelerate theexploration of more general visual detection systems. Thebaseline cascade structure is very suitable for handling thehierarchical category structure of the V3Det dataset. Wetreat the supervised track I as a traditional object detectiontask with complex labels, using common detection improve-ment strategies. By improving the Feature Pyramid Net-work (FPN) structure, we hope the network can effectivelylearn deeper semantic information. Additionally, we bal-ance category labels by adjusting the loss function. The second track (OVD) of the V3Det challenge involvesdeveloping object detectors capable of accurately identify-ing objects from 6,709 base classes and 6,495 novel classes.For base classes, full annotations are provided, while fornovel classes, only class names, descriptions, and a fewexemplar images are given. The task is to design detec-tors that can utilize this limited information to detect novelclasses effectively during inference, ensuring accurate de-tection across both base and novel categories. This trackrequires detectors to possess strong generalization and se-mantic understanding capabilities to identify new categorieswithout direct annotation information. It can rely on currentvision-text models, such as CLIP , to extract visual andsemantic features from images and text, and establish con-nections between them. The baseline EVA model , combined with CLIP ,demonstrates powerful semantic feature extraction capabil-ities. Due to time constraints and limited computational re-sources, we rely solely on supervised training for Track II,yet still achieve good detection results even for novel cat-egories. This to some extent indicates that V3Det datasetcovers a vast array of annotations from real-world scenar-ios, with rich semantic information learned by excellent de-tectors, thus exhibiting good generalization performance.",
  ". Object Detection": "Object detection is one of the most traditionaltasks in computer vision, with various applications acrossdifferent industries such as autonomous driving ,robotics , remote sensing . It takes images as input,localizes, and classifies objects within a given vocabulary.Each detected object is represented by a bounding box witha class label.Classical CNN-based object detectors can be dividedinto two main categories: two-stage and one-stage detec-tors. Two-stage detectors first generate ob-ject proposals and then refine them in a second stage, offer-ing higher precision but at the cost of increased complex-ity. One-stage detectors, such as YOLO and SSD, directly classify and regress predefined an-chor boxes or search for geometric cues like points ,centers , and corners , providing faster but po-tentially less accurate results.Transformer-based detec-tors use the self-attention mechanism to cap-ture global contextual information in images, eliminatingthe need for additional components like anchor boxes andNon-Maximum Suppression (NMS). The end-to-end archi-tecture is simpler, making the training and inference processmore straightforward.Currently, novel detectors based on diffusion are emerg-ing . At the same time, object detection is being com-bined with large language models (LLM) to achieve open-vocabulary detection and the detection of every-thing. This approach allows object detection to go beyondjust the design of detector architectures, providing modelswith better adaptability to handle complex scenes and vari-ous types of objects.",
  ". Data Augmentation": "Data augmentation is a commonly used technique inmachine learning and deep learning, aimed at transform-ing and expanding training data to increase its diversityand richness.In addition to common data augmenta-tion methods such as flipping, jittering, and scaling, ef-fective data augmentation techniques for object detectioncan be broadly categorized into Cutting-based andMixing-based methods.There is also thewidely used Mosaic method proposed by YOLOv4 .",
  ". Baseline Framework": "In this challenge, the organizers built two baselines basedon MMDetection 1 and Detectron22. The baseline EVA3,based on Detectron2, utilizes a Cascade RCNN with a back-bone structure of ViTDet . The pretraining task of EVAinvolves Masked Image Modeling (MIM), aimed at recon-structing masked image-text aligned visual features gener-ated by the CLIP . This network demonstrates robustgeneralization performance and stands as the state-of-the-art (SOTA) for many vision tasks. Based on the MMDe-tection baseline4, the best-performing model is also basedon Cascade R-CNN , with a Swin-Transformer asits backbone. The cascade structure is highly suitable formulti-class detection tasks by progressively refining bound-ing boxes and classification results. Each stage of the cas-cade head uses two shared fully connected layers, whichhelps capture high-level semantic features of the targets atdifferent stages. The IoU thresholds set for each stage en-sure that the detection boxes become more precise at eachlevel.",
  ". Model Architecture Adjustment": "Backbone. The baseline adopts Swin Transformer asthe backbone network for feature extraction, commonly us-ing versions such as Swin-S, Swin-B, and Swin-L5. Differ-ent versions affect the parameter count, computational cost,and accuracy. Therefore, we have made multiple attemptswith different backbones. The baseline pretrained modelprovided by the organizers uses ImageNet-1K pretrainedweights to initialize the backbone. We also attempted to useImageNet-22K pretrained weights to initialize the Swin-Bbackbone. We also attempted to use pretrained models witha resolution of 3843846. In addition to using Swin Trans-former as the backbone, we also experimented with thebasic Vision Transformer models, specifically using ViT-Band ViT-L.Path Aggregation Feature Pyramid Network (PA-FPN).Although the FPN structure already integrates shallow fea-ture information, the path from shallow features to the topnetwork layers is too long, resulting in low utilization ef-ficiency of shallow features. To effectively capture imagesemantic information, inspired by PA-Net , we add abottom-up structure into the baseline Cascade R-CNN. Thisshortens the transmission path from shallow features to thetop layers, enhancing the transmission of shallow featureswithin the network, and allowing more shallow features tobe effectively utilized. As shown in Fig 2, where the fea-",
  ". Other Improvements": "Data Augmentation. In order to enhance the size and qual-ity of training dataset, we employ data augmentation in-cluding flipping, jittering, and scaling, on original input im-ages. We tried the data augmentation strategies built intoMMDetection-transforms such as Mixup, Cutout, Corrupt,and PhotoMetricDistortion. It is important to note that moredata augmentation is not always better, especially in objectdetection tasks. Excessive data augmentation can lead toshifts or distortions in the original target positions, mak-ing it difficult for the model to learn accurate target bound-aries. It has been shown that the two-stage algorithmcan be used for data augmentation without random geomet-ric transformations in the training phase.Loss Function.In this section, we introduce the DIoULoss function for addressing coordinate point interrelation-ship issues using the L1 loss function in baseline CascadeR-CNN networks. Inspired by Zhaohui Zheng et al. ,DIoU Loss considers two key issues: (a) Minimizing thenormalized distance between the prediction frame and thetarget frame to achieve faster convergence. (b) How to makethe regression more accurate and faster when there is over-lap or even inclusion with the target box. The DIoU Lossfunction yields values in the range , and is defined asfollows:",
  "LDIoU = 1 IoU + RDIoU,(2)": ": The detection results of different models on the V3Det Supervised track I. We show the results on the validation set,with gray indicating the baseline provided by the organizers in MMDetection. The last row represents the baseline providedin Detectron2, which uses EVA pretrained with CLIP. pretrain indicates whether the models are pretrained on the ImageNet1K or the ImageNet 22K dataset, and resolution indicates whether an input resolution of 224224 or 384384 was used.All models are based on Cascade R-CNN.",
  "Cascade R-CNN EVA-CLIP51.155.953.224.434.656.244.356.278.675.3": "() represents the Euclidean distance. The penalty termRDIoU is defined as the squared Euclidean distance be-tween the central points of b and bgt, normalized by thesquare of the diagonal length c of the smallest enclosingbox covering the two boxes. This formulation ensures thatthe DIoU loss directly minimizes the distance between thetwo central points.Inspired by Li et al. , to reduce the economic imbal-ance of the sample measure in the detection process and theinaccurate detection results caused by the blurred bound-ing box, we properly introduces the Generalized Focal Loss(GFL) function into the Region Proposal Network (RPN) tobalance the proportion of positive and negative samples inthe loss function, The GFL function is typically shown inequation (3).",
  "GFL (pyl, pyr) = |y (ylpyl + yrpyr)|": "((yr y) log (pyl) + (y yl) log (pyr)) .(3)y represents the true IoU, while yl and yr are the lowerand upper bounds of the predicted and true IoU of thebounding boxes. is an adjustable hyper-parameter con-trolling the slope of the loss function ( 0). pyl and pyrare the probability values predicted by the model, satisfyingpyl + pyr = 1. The final prediction y is a linear combina-tion of yl and yr, enabling classification values to transitionfrom discrete to continuous. The balancing factor in theformula minimizes deviations between predicted and trueIoU, while the classification loss function computes errorsto enhance the models understanding of object position andsize. GFL employs a focal mechanism, dynamically adjust-ing weights to balance proportions and facilitate learningdifferences between positive and negative samples.Training Techniques.During training, we find that thejson format files of more than 30 images in the originaldataset do not match the corresponding images. We performdata cleaning and remove such erroneous data. We use Syn-",
  ". Implementation Details": "Following the challenge guidelines, 183,354 images areused as the training set, and 29,821 images are used as thevalidation set. We train exclusively on the V3Det datasetand do not use any extra data. We train the full modelson the training set and evaluate them on the validation setfor algorithm validation and hyper-parameter tuning. Fi-nally, we retrain and save the models on the complete train-ing data using the selected hyper-parameters. We imple-ment our model using PyTorch 2.1.0 and conduct our ex-periments on a system with 4 H100 GPUs, using a batchsize of 48. We use Adam with decoupled weight decay(AdamW) with a learning rate of 0.001.We adoptthe COCO Detection Evaluation to measure the perfor-mance. The COCO Detection Evaluation includes multiple-scale objects (APS, APL), where APS represents small ob-ject AP, with an area < 32, and APL represents large ob-ject AP, with an area > 96. For the Supervised Track I,AP and Recall are used as evaluation metrics for the testset. For the OVD Track II, AP and Recall are calculatedseparately for the base categories and novel categories.",
  ". Results and Analysis": "As shown in , we are trying various approachesto the model backbone. When using ImageNet 22k pre-training, there is not much change in the AP value of themodel, but the Recall has significantly improved. The Re-call all has increased from 64.3% to 69.5%, indicating thatthe model misses fewer targets. Better pretraining initial-ization of the backbone is particularly important for objectdetection tasks. Using a larger model like Swin-L as thebackbone introduces additional parameters and computa-tional complexity, resulting in longer inference times. How-ever, despite these drawbacks, the detection performance ofthe model decreased.As shown in , we introduced a series of improve-ments, including optimizing the loss function of the originaldetector and modifying its FPN structure. Surprisingly, af-ter incorporating the PA-FPN structure, the models detec-tion performance, as measured by AP, did not improve butinstead decreased by nearly 2%. The PA-FPN structure hasbeen proven effective in many tasks and widely applied invarious detection and segmentation tasks. We speculate thatthis unexpected result may be due to the influence of noiseor irrelevant information on the lower-level features, lead-ing to a decrease in the quality of the fused features. Thebottom-up structure may cause premature or excessive fu-sion of features between different levels, resulting in infor-mation loss or confusion. The introduction of the bottom-up structure may increase the complexity of the network,making training more challenging and requiring more ad-justments and optimizations. Due to time constraints, wedid not conduct detailed experiments, and further validation",
  ": The horizontal axis represents different classes,and the vertical axis represents the number of samples cor-responding to each class, with values above 1000 not dis-played": "will be carried out gradually.Certainly, modifying the RPN classification loss func-tion to the GFL function and changing the bounding boxregression loss to the GIoU loss function have proven ef-fective. As shown in Fig 3, the V3Det dataset, due to itsnumerous categories, results in poor learning performancefor minority classes during training. GFL introduces ad-justable parameters to weight the loss functions for differentclasses, allowing the model to focus more on challengingsamples.GFL introduces adjustment parameters to weightthe loss functions of different categories, making the modelpay more attention to samples that are difficult to classify.Regrettably, despite conducting numerous experimentsand adjustments, and achieving some improvements overthe baseline, our results still could not surpass the repro-duced EVA model provided by the organizers based onDetectron2.The EVA model employed the MIM train-ing method, optimizing CLIP and demonstrating powerfulperformance and superior results. The outstanding perfor-mance of the EVA model indicates that merely modifyingand designing the model structure is no longer sufficient toachieve significant breakthroughs in the current era of largemodels. The key to the success of the EVA model lies in itsinnovative training methods and the effective utilization ofpretrained models, which provides a direction for our futureresearch and improvements.As shown in , for OVD Track II, we adhered tothe traditional supervised object detection transfer learningapproach and did not incorporate textual information. Ac-cording to the competition requirements, we used the Cas-cade R-CNN model based on MMDetection with Swin-Bas the backbone from Track I, retrained on the V3Det trainset of base classes, and directly inferred on the test dataset.We were pleasantly surprised to find that this approach also yielded good results. Compared to the baseline, our APfor novel classes improved from 11% to 20%, with AP50reaching 29%. This might be because the V3Det dataset al-ready contains rich semantic information, giving the modela certain degree of generalization ability.",
  ". Conclusion": "In conclusion, this report has presented our study onV3Det Challenge for Vast Vocabulary Object Detectiontrack 2024. In the Supervised Track I, we made variousattempts at traditional object detection tasks using differ-ent models. For the V3Det dataset, which contains rich se-mantic information across multiple categories, we observedsome improvement in detection results. However, althoughthe performance did not fully meet our expectations, ouradjustments could not surpass the results we obtained byreproducing EVA. This indicates that simply modifying anddesigning model structures is no longer sufficient in the eraof LLM. Our final submission achieved good results on theleaderboard for both Track I and Track II.",
  "T. DeVries and G. W. Taylor. Improved regularization ofconvolutional neural networks with cutout. arXiv preprintarXiv:1708.04552, 2017": "K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, and Q. Tian. Cen-ternet: Keypoint triplets for object detection. In Proceedingsof the IEEE/CVF international conference on computer vi-sion, pages 65696578, 2019. Y. Fang, W. Wang, B. Xie, Q. Sun, L. Wu, X. Wang,T. Huang, X. Wang, and Y. Cao. Eva: Exploring the limits ofmasked visual representation learning at scale. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1935819369, 2023.",
  "S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia. Path aggregationnetwork for instance segmentation. In Proceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 87598768, 2018": "W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.Fu, and A. C. Berg. Ssd: Single shot multibox detector. InComputer VisionECCV 2016: 14th European Conference,Amsterdam, The Netherlands, October 1114, 2016, Pro-ceedings, Part I 14, pages 2137. Springer, 2016. Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, andB. Guo. Swin transformer: Hierarchical vision transformerusing shifted windows.In Proceedings of the IEEE/CVFinternational conference on computer vision, pages 1001210022, 2021.",
  "J. Redmon and A. Farhadi. Yolov3: An incremental improve-ment. arXiv preprint arXiv:1804.02767, 2018": "S. Shao, Z. Li, T. Zhang, C. Peng, G. Yu, X. Zhang, J. Li,and J. Sun. Objects365: A large-scale, high-quality datasetfor object detection. In Proceedings of the IEEE/CVF inter-national conference on computer vision, pages 84308439,2019. Z. Sun, S. Cao, Y. Yang, and K. M. Kitani.Rethinkingtransformer-based set prediction for object detection. In Pro-ceedings of the IEEE/CVF international conference on com-puter vision, pages 36113620, 2021. C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.Rethinking the inception architecture for computer vision. InProceedings of the IEEE conference on computer vision andpattern recognition, pages 28182826, 2016.",
  "dataset. In Proceedings of the IEEE/CVF International Con-ference on Computer Vision, pages 1984419854, 2023": "Z. Wang, F. Colonnier, J. Zheng, J. Acharya, W. Jiang, andK. Huang. Tirdet: Mono-modality thermal infrared objectdetection based on prior thermal-to-visible translation.InProceedings of the 31st ACM International Conference onMultimedia, pages 26632672, 2023. Z. Wang, Y. Li, X. Chen, S.-N. Lim, A. Torralba, H. Zhao,and S. Wang. Detecting everything in the open world: To-wards universal object detection.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1143311443, 2023. H. Wu, C. Wen, W. Li, X. Li, R. Yang, and C. Wang.Transformation-equivariant 3d object detection for au-tonomous driving.In Proceedings of the AAAI Confer-ence on Artificial Intelligence, volume 37, pages 27952802,2023. X. Wu, F. Zhu, R. Zhao, and H. Li. Cora: Adapting clipfor open-vocabulary detection with region prompting and an-chor pre-matching. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages70317040, 2023. S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo. Cut-mix: Regularization strategy to train strong classifiers withlocalizable features. In Proceedings of the IEEE/CVF inter-national conference on computer vision, pages 60236032,2019."
}