{
  "Abstract": "Recently, weakly supervised video anomaly detection(WS-VAD) has emerged as a contemporary research direc-tion to identify anomaly events like violence and nudity invideos using only video-level labels. However, this task hassubstantial challenges, including addressing imbalancedmodality information and consistently distinguishing be-tween normal and abnormal features. In this paper, we ad-dress these challenges and propose a multi-modal WS-VADframework to accurately detect anomalies such as violenceand nudity. Within the proposed framework, we introduce anew fusion mechanism known as the Cross-modal FusionAdapter (CFA), which dynamically selects and enhanceshighly relevant audio-visual features in relation to the vi-sual modality.Additionally, we introduce a HyperbolicLorentzian Graph Attention (HLGAtt) to effectively capturethe hierarchical relationships between normal and abnor-mal representations, thereby enhancing feature separationaccuracy. Through extensive experiments, we demonstratethat the proposed model achieves state-of-the-art results onbenchmark datasets of violence and nudity detection.",
  ". Introduction": "In the modern technology era, kids are increasingly turn-ing to online platforms for learning, fun, and connectingwith others. However, this easy access also brings up wor-ries about their exposure to harmful and unsuitable content,particularly content with violence and nudity. The poten-tial adverse effects on a childs emotional well-being andpsychological development underscores the importance ofimplementing robust mechanisms to detect violence and nu-dity. Detecting such anomalies in a video is a well-knowncomputer vision problem that can also be useful in otherreal-world applications such as surveillance systems, crimeprevention, and content moderation. Acquiring annotations",
  "*Equal ContributionsCorresponding Author": "for anomalies at the frame level in videos is costly and time-consuming. As a result, WS-VAD has emerged as a promi-nent area of research. WS-VAD focuses on learning ab-normal events, such as violence and nudity, solely basedon video-level binary labels. In this approach, a video isclassified as normal if no anomalous event is detected. Incontrast, it is classified as an anomaly if any form of ab-normal events, such as violence or nudity, is present. WS-VAD methods usually employ Multiple Instance Learning(MIL) for model training. Here, a regular video is seenas a negative bag with no anomalous segments, while ananomaly video is viewed as positive bag with one or moreanomalous segments. The anomaly evaluation function istrained by optimizing the MIL loss to ensure positive baghas a higher anomaly value than negative (normal) bag.Following MIL, recently, several WS-VAD methodshave been proposed based on single-modality (i.e., video-based methods ) and multi-modality.The multi-modal approacheshave shown promising results compared to single-modality-based methods, which jointly learn audio and visual repre-sentations to improve performance by leveraging comple-mentary information from different modalities. Althoughmulti-modal methods show promising performance, theyface two main challenges: 1) unbalanced modality infor-mation when combining audio-visual features and 2) incon-sistent discrimination between normal and abnormal fea-tures. Recently, Peng et al. found that the issue ofmodality imbalance is mainly due to noise in audio signalsfrom real-world scenarios. To address this, they suggest thatauditory information contributes less to anomaly detectionthan visual cues, leading to lower prioritization of audiofeatures. However, this approach must be corrected whenaudio data is as crucial as visual data. To address anotherissue, i.e., inconsistent discrimination between normal andabnormal features, prior studies have utilized graph repre-sentation learning, where each instance is treated as a nodein a graph. However, these methods still struggle to distin-guish them accurately.In this study, we propose a new framework to address",
  ".Comparative analysis of our proposed method withprior video-based method as well as audio-video based fusion ap-proaches on testing videos of XD-Violence dataset": "these challenges.We introduce a novel fusion modulecalled a CFA to address the challenge of imbalanced modal-ity information. It dynamically adjusts the influence of eachmodality by prioritizing the importance of audio featuresrelative to the visual modality. This selective process en-sures that only relevant audio features crucial for visuallearning are being utilized. By adapting to select the mostappropriate features relative to the visual modality, our ap-proach enhances visual feature learning by incorporatingrelevant audio features. Furthermore, we introduce a hyper-bolic graph convolution network-based HLGAtt mechanismto maintain consistent discrimination between normal andabnormal features. This mechanism operates in hyperbolicspace to capture hierarchical relationships between normaland abnormal representations through spatial and temporalfeature learning, which aids in distinguishing normal andabnormal features.The proposed model accurately identifies anomalyevents and outperforms existing state-of-the-art (SOTA)methods for violence and nudity detection tasks. shows the anomaly score analysis obtained from a few vi-olent and normal instances of the XD-Violence dataset andcompares it with various approaches such as only video-based method , Concate fusion , Detour fusion approaches. shows that the proposed model accu-rately identifies anomalies compared to others. We summa-rize the contributions of this paper as follows: We propose a new WS-VAD framework to address theimbalance issue in audio-visual modality information andeffectively distinguish abnormal features from normalones so that anomaly events such as violence and nuditycan be detected accurately. To address the imbalanced modality information issue,we introduce a novel fusion module called CFA, whichhelps the proposed framework to facilitate multi-modalinteraction effectively by dynamically regulating the con-tribution of each modality.",
  ". Violence Detection Works": "Earlier, few unsupervised learning-based methods have been proposed for violence detection. These methodsfocus on one-class classification via learning what is nor-mal and spotting anomalies by recognizing deviations fromthe norm. However, these methods are not well-suited forcomplex environments and often struggle due to the limitedavailability of abnormal video data during training.Recently, WS-VAD methods havebeen introduced utilizing video-level labels and achievedpromising results over unsupervised VAD methods. A fewvideo-based WS-VAD approaches have been proposed to enhance the detection accuracy of vi-olence events. However, these approaches overlooked audioinformation and cross-modality interactions, limiting the ef-fectiveness of violence prediction. To address this issue,Wu et al. introduced a large-scale audio-visual datasetnamed XD-Violence and established a baseline for audio-visual activities.Following this, many multi-modal ap-proaches have been proposed that out-performs video-based WS-VAD methods. Recently, Peng etal. proposed a fusion mechanism for audio-visual dataand introduced a hyperbolic graph convolution network-based model to efficiently capture the semantic distinctionsvia learning the embeddings in hyperbolic space. Recently,Zhou et al. proposed a dual memory units module withuncertainty regulation emphasizing learning representationsof abnormal and normal data. Salem et al. introduced anew version of MIL that avoids the disadvantages of rank-ing loss by using margin loss instead.Although these methods present promising results, theireffectiveness is hindered by the integration of imbalancedaudio-visual features. Moreover, they struggle to consis-tently differentiate between normal and abnormal features,limiting the detection accuracy. This paper addresses theseissues and proposes a new multi-modal framework that de-tects violent events more accurately. In contrast to recentmulti-modal approaches , we propose a newcross-modal fusion with modulation mechanism to learnand fuse audio modality with relative visual features adap-tively.Furthermore, we introduce Lorentzian attention-based hyperbolic graph mechanism to learn hierarchical re-lationships between normal and abnormal features and dis-criminate them effectively.",
  "In video-based nudity detection, researchers have devisedvarious methods to tackle the task of identifying explicit": "content. A common strategy involves detecting skin colorin video frames . Samal et al. proposed amodel that combines attention-enabled pooling with a Swintransformer-based YOLOv3 architecture for obscenity de-tection in images and videos. Jin et al. employed aweakly supervised multiple instance learning approach forgenerating a bag of properly sized regions with minimal an-notations to tackle the detection of private body parts basedon local regions. Wang et al. incorporated an attention-gated mechanism with a deep network, demonstrating itsefficacy in performance enhancement. Several studies haveproposed deep learning architectures considering local andglobal context jointly . Utsav et al. proposeda domain adaptation-based method to filter adult content instreaming video. Tran et al. proposed an additionaltraining-based approach on pseudo labels using Mask R-CNN for sexual object detection.However, above methods focus on image-based ap-proaches or utilize uni-modal approaches; the audio-visual-based approaches have not been extensively explored. Thispaper seeks to address this gap by employing audio-visualdata, aiming to enhance the accuracy of nudity detection invideos.",
  ". Problem Statement": "Given a set of N videos, X = {Xi}Ni=1 and the correspond-ing ground-truth video-level labels Y = {Yi}Ni=1 {1, 0}where Yi = 1 denotes the presence of any abnormal eventin the video while Yi = 0 signifies the absence of any ab-normal event, we aim to accurately detect abnormal eventssuch as violence and nudity within the videos in a weaklysupervised manner.Specifically, each video Xi is ini-tially divided into 16-frame based T non-overlapping multi-modal segments (M = {M Vi , M Ai }Ti=1), which are pro-cessed by a pre-trained CNN network to extract the corre-sponding visual features FV RT DV and audio featuresFA RT DA, where DV and DA represents the feature di-mensions of video and audio modality. Here, M Vi and M Aidenote the video and audio segments, respectively. Theseextracted visual and audio features are then forwarded intothe proposed framework which identifies whether the inputvideo contains any abnormal events or not.To identify abnormal events accurately, we propose anew framework as shown in in which we in-troduce novel cross-modal fusion module and hyperbolicLorentzian graph attention mechanism.Details of thesemodules are discussed in subsequent subsections.",
  "The CFA module consists of a prefix-tuned-based bottle-neck attention and a modulation mechanism. The prefix-": "tuned bottleneck attention helps in efficient multi-modal in-teraction between audio and visual modalities. The modu-lation mechanism dynamically regulates the contribution ofeach modality during the fusion process, taking into accountthe importance of the audio features to the visual modality.Prefix-Tuning bottleneck attention mechanism:Thismechanism incorporates prior knowledge into the featuretransformation process by combining the learned represen-tations with initialized parameters through the prefix-tuningoperation. To do this, the process involves concatenatingthe keys K and values V obtained from audio features FAwith prefixes Pk & Pv, resulting in prefix-tuned keys Kpand values Vp, respectively. The parameters Pk & Pv areinitialized as zero matrices with dimensions of RBDADp,where B, DA & Dp represent the batch size, audio featuredimension, and prefix dimension, respectively.These prefix-tuned keys Kp and values Vp along with thequery Q, i.e., visual features FV , are then passed on to thecross-modal multi-head attention module . This mod-ule enables the interaction between the prefix-tuned featuresof the audio and visual modalities, allowing them to selec-tively and contextually focus on each modalitys relevantinformation. In this process, the attention scores are com-puted based on queries, prefixed tuned keys and values. Themathematical formulation of the cross-modal multi-head at-tention module function (i.e., fCMA) can be formulated as",
  "Vp,": "(1)where, DKP represents the dimensionality of the key vec-tors (Kp).The attention features FAtt are subsequentlypassed to the bottleneck adapter module.In this stage,the bottleneck adapter ensures smooth interaction betweenmodalities while preserving modality-specific characteris-tics. It comprises down-scaled fully connected layers (i.e.,fdown) followed by Gaussian Error Linear Unit (GELU) ac-tivation (i.e., fGELU) and up-scaled fully connected layers(i.e., fup). This can be expressed mathematically as",
  "FAtt = fup(fGELU(fdown(FAtt))),(2)": "Here,the GELU activation function introduces non-linearity, allowing intricate feature transformations. Thiscareful design ensures that the adapter module effectivelyadjusts input features to the shared bottleneck representa-tion, promoting context-aware fusion.Modulation Mechanism: In the proposed CFA module,we introduce modulation factors that dynamically adjust theimpact of individual modalities by considering the impor-tance of their audio features relative to the visual modal-ity. This mechanism is facilitated by a learnable modula-tion function that operates on audio features FA to select",
  "Modulation Mechanism": "FC . Overview of the proposed framework. It takes audio and visual features extracted from pre-trained encoder networks as input,which are further fused through the proposed Cross-Modal Fusion Adapter (CFA) module to learn multi-modal interaction effectively,followed by the introduced Hyperbolic Lorentzian Graph Attention (HLGAtt) mechanism to capture hierarchical relationships betweenvisual and audio representations, ensuring consistency in distinguishing normal and abnormal features during training. Finally, the outcomefeatures are passed in a hyperbolic classifier to predict anomaly events for each instance.",
  "FMod = fMF (FA) = (Wmod FA).(3)": "Here, represents the sigmoid activation, while Wmodstands for the weights associated with the modulation func-tion. The sigmoid activation function ensures that modula-tion factors range between 0 and 1, thereby regulating thedegree of modulation applied to the fused representation. Next, the fusion and refinement process is used, whereit first fuses the modulated features with the output of theprefix-tuning bottleneck attention and then refines the fusedrepresentation through a fully connected layer. This opera-tion can be expressed mathematically as",
  "FF used = fF C(FV + ( FAtt FMod)).(4)": "The modulation mechanism fMF () modifies the output ofthe prefix-tuning bottleneck attention based on the signifi-cance of their audio features to the visual modality. Throughthe fusion and refinement process, the final fused represen-tation is carefully crafted to capture the most relevant in-formation from both modalities, simultaneously reducingnoise and preserving the modality-specific characteristics.",
  ". Hyperbolic Lorentzian Graph Attention (HL-GAtt) Mechanism": "In the proposed framework, we introduce a hyperbolicgraph convolution network based on a new attention mech-anism called HLGAtt. The proposed HLGAtt uses a hyper-bolic Lorentz graph attention mechanism that learns layer-wise curvature parameters to capture the hierarchical struc-ture of the input graph, thereby enhancing the hierarchicalrelationship between normal and abnormal representationscompared to existing graph-based or transformer-based approaches. It consists of a hyperbolic spaceconversion operation, a Lorentz linear transformation & en-hancement module process on parallel nodes, and a fusingoperation. Initially, we convert the fused audio-visual featuresFF used into the hyperbolic space using an exponential func-tion. As a result, we obtain the converted fused featuresmaps FH RT 2DH, wherein T denotes the number ofsegments and DH represents the hyperbolic dimension. Recently, Zhang et al. proposed a hyperbolic graphattention mechanism that utilized a parallel branch pro-cess to learn different features and patterns in respectivebranches for prediction tasks. Inspired by this , we pro-cess the converted hyperbolic feature maps on two parallelbranches, i.e., node A and node B, to learn specific patternsfrom the input feature maps. Separating the branches en- sures that features with similar characteristics are directedto their respective nodes. This allows each branch to learnthe unique properties of normal and abnormal features, en-abling more precise discrimination between them.The converted hyperbolic feature maps are passedthrough the Lorentzian linear transformation & enhance-ment module in each node. Here, we employ the Lorentzianlinear transformation for feature transformation andits transformed temporal and spatial features are further en-hanced using the proposed enhancement mechanism.InLorentzian linear transformation, we first establish the ad-jacency matrix A RT T to capture hyperbolic featuresimilarities. Here, each entry Aij can be calculated as",
  "= Softmax(exp(dL(FH,i, FH,j)),(5)": "where, fsim represents the hyperbolic feature similaritymeasure, which evaluates how closely snippets i and j re-semble each other based on their Lorentzian intrinsic dis-tance dL. The exponential and Softmax functions are em-ployed to maintain non-negativity and restrict the values ofA within the range of .Next, we incorporate a hyperbolic Lorentz linear (i.e.,fHL()), followed by neighborhood hyperbolic aggregationoperation for feature transformation.These trans-formed hyperbolic features of the ith snippet at the layerl (i.e., zli) can be expressed as",
  ",(6)": "where, indicates the negative curvature constant.To enhance these transformed features z further, they areprocessed based on temporal and spatial information. Theinitial component of the input vector z signifies the tem-poral aspect within hyperbolic space . This componentis processed via a sigmoid activation function followed byexponential scaling and shifting operations. Through thisprocedure, temporal features (i.e., TnodeA and TnodeB) arecomputed for both node A and node B as",
  "SnodeB = [znodeB, znodeB, ..., znodeB[n]](8)": "These features encapsulate the intricate spatial features inhyperbolic space, which are critical for capturing the hier-archical structure and relationships within the graph.To ensure the alignment of spatial components with thehyperbolic model, a scaling factor, referred to as is com-puted. This factor takes into account the temporal and spa-tial complexities of each node. It ensures that the spatialcomponents are appropriately scaled to fit within the hyper-bolic space.",
  "nodeB(10)": "The enhanced feature maps in the node A branch passedthrough Leaky-ReLU activation and softmax normalizationoperations to introduce non-linearity and ensure standard-ization across the enhanced feature maps. This ensures thatdistinct patterns, representing normal and abnormal data,are learned at each node. By doing so, the model is encour-aged to learn different sets of features from those processedby other node (i.e., node B). Finally, the enhanced featuremaps from node A and node B are processed via matrixmultiplication to compute attention, followed by a ReLUactivation to generate the output feature maps. This out-come of the proposed HLGAtt module can be formulatedas",
  "Score = fHypcls(F finalH)(12)": "In order to train the proposed model end-to-end, we em-ploy the MIL-based learning objective adopted in , which calculates the mean value of the top kmaxpredictive scores within a video. The high-scoring positivepredictions indicate the presence of abnormal events, whilethe kmax negative scores usually represent hard samples.This learning objective function can be formulated as",
  ". Implementation Details": "The proposed model is trained/tested on benchmark XD-Violence dataset for violence detection task, on NPDIpornography dataset for nudity detection task. Thedetails of these datasets are mentioned below: XD-Violence for violence detection: The XD-Violencedataset is a diverse compilation of 4754 rawvideos (equivalent to 217 hours) gathered from real-worldsources, including movies, web videos, sports broadcasts,security cameras, and CCTVs. It consists of six types ofviolent events, such as abuse, auto crashes, and shootings,with corresponding video-level annotations. The testingset comprises 300 normal and 500 violent videos, whilethe training set includes 2049 normal and 1905 violentvideos, all labeled at the video level. NPDI for Nudity Detection : The NPDI Pornographybenchmark dataset comprises around 80 hours ofvideo content extracted from 400 movies. These contentsare classified as pornographic or non-pornographic, withan equivalent amount of videos in each category. Withinthe non-pornographic section, there are 200 videos la-beled as either easy or difficult. The easy videoswere randomly selected, while the difficult ones wereobtained through textual search queries such as beach,wrestling and swimming. Although the difficultvideos may contain body skin, they do not include ex-plicit nudity or pornographic content.Training / Evaluation Details:The proposed modelis trained on datasets mentioned above using the multi-instance learning-based loss function (i.e., Eq. 13) with abatch size of 128. During the training process, we adoptthe Adam optimizer with a learning rate of 5 104 var-ied using a cosine annealing scheduler and trained for 50epochs. For fair comparison with existing SOTA methods,the proposed framework also employs a pre-trained I3Dmodel to extract the visual features (FV ), while theVGGish network is utilized to extract the audio features(FA). In the proposed framework, we use the LeakyReLUactivation function with a negative slope of -2. In the Prefix-Tuner of the CFA module, we empirically chose the prefixdimension as 64. The bottleneck adapter has a size of 256and utilizes the GELU activation function with a dropoutrate of 0.1. The constant representing negative curvature() is set to -1 during training.For comparison on violence detection task, we chooseunsupervised methods (i.e., SVM baseline, and Hasan etal. ), video modality-based weakly supervised methods, and audio-visual modality-",
  "ProposedAudio + Video86.34": "based weakly supervised methods ). Theframe-level average precision (AP) metric is adopted tocompare these methods, whereas a higher AP measuremeans better performance. For the nudity detection task,we compare the proposed method with existing methods. However, these methods have uti-lized uni-modal approaches in their network. Additionally,we re-train the recent multi-modal SOTA method called Hy-perVD on the NPDI dataset. For comparison, we usethe standard evaluation metrics, i.e., AP, accuracy, preci-sion, and recall, where higher measures of these evaluationmetrics indicate superior performance.All the experiments were implemented using PyTorchand the network was trained on a 40GB NVIDIA A100GPU with batch size of 128.",
  ". Result Analysis on Violence Detection task": "compares state-of-the-art methods on the XD-Violence testing dataset in terms of AP metric. Notably, ourproposed method outperforms both video modality-basedand audio-video modality-based methods. It achieves anAP score of 86.34%, which is 0.67% higher than the pre-vious best-performing method HyperVD . Comparedto video-modality-based methods, our proposed approachshows a 4.24% displays the visual prediction analysis of ourmethod when compared to existing methods, i.e., HyperVD and Wu et al..The comparison is based onthe anomaly score obtained from a few videos of the XD-Violence testing dataset . Here, one can observe that theproposed method not only identifies violent event regions",
  ". Visual comparison on normal and violence features ofthe proposed and HyperVD methods on XD-Violence dataset": "but also yields superior and more precise anomaly scorescompared to other methods.Additionally, provides a comparison betweenthe proposed and HyperVD methods in terms of t-SNEvisualization of normal and violent features distribu-tions on the XD-Violence dataset testing videos. One canfind that the proposed method effectively clusters the vio-lent and non-violent features and also enlarges the distancebetween uncorrelated features after the training procedureas compared to the HyperVD method.",
  ". Result Analysis on Nudity Detection task": "This section provides an analysis of the Nudity detectiontask, comparing it with existing methods on the NPDI testing dataset . shows thecomparison in terms of AP, accuracy, precision, and recall.Here, it can be noticed that the proposed model outperformsother methods in all evaluation metrics by a significant mar-gin. To be specific, our model achieves an AP of 99.45%,an accuracy of 94.12%, a precision of 95%, and a recall of93.75%. Notably, it demonstrates improvements of at least1.95% in AP, 0.42% in accuracy, 2.2% in precision, and 3%in recall compared to other methods.Additionally, demonstrates the visualization ofthe anomaly score obtained from the proposed and Hy-perVD methods on the NPDI dataset. The visualization",
  ". Visual comparison on normal and nudity event featuresof the proposed and HyperVD methods on NPDI dataset": "demonstrates that the proposed method generates minimalpredictions for regular segments in normal footage while ef-fectively handling extreme situations within nudity content.This analysis also proves that the proposed method not onlyaccurately identifies specific regions but also provides moreprecise anomaly scores compared to anomaly predictions ofthe HyperVD method .We also provide t-SNE based visual comparisonof the proposed and HyperVD methods in .Here, we compare the proposed and re-trained HyperVD methods with corresponding normal and violent fea-ture distributions obtained from the NPDI dataset testingvideos. It can be clearly observed that the proposed method",
  ". Ablation Analysis": "In order to ensure a fair comparison, all ablation ex-periments were conducted using the XD-Violence testingdataset for the violence detection task.Analysis of proposed components: A series of ablationexperiments were conducted to validate the efficacy of theproposed components, i.e., CFA and HLGAtt, within theproposed framework.Few experiments have been per-formed on the CFA module with and without Prefix-Tuning(PT) and Modulation Mechanism (MM) modules. The re-sults are outlined in . It is evident that the inclusionof the PT and MM modules enhances the performance ofthe CFA module. Furthermore, experiments were carriedout using the attention mechanism FHGCN, as proposedin , to validate the effectiveness of the HLGAtt mod-ule. In , we present the results performed using theFHGCN module in Cases 1 - 3, while Cases 4 - 6 showthe result obtained from the proposed HLGAtt module. TheHLGAtt module demonstrates superior performance com-pared to the FHGCN module by a significant margin.For instance, there is a 4.69% improvement from Case 1 toCase 4, a 4.58% improvement from Case 2 to Case 5, anda 2.92% improvement from Case 3 to Case 6. This analysisproves the efficacy of the proposed modules.Analysis on prefix dimension in the Prefix-Tuner mod-ule:We also thoroughly analyze the prefix dimensionwithin the Prefix-Tuner module featured in the CFA mod-ule. The experiment tested different prefix dimensions in-cluding 34, 48, 64, 80, 128, and 256. The correspondingresults are illustrated in where one can observe thatthe optimal performance is achieved as AP of 86.34%, witha prefix dimension of 64. However, the performance of APis diminished when the prefix dimension is increased be-yond 64. As a result, we set the prefix dimension as 64 inthe proposed CFA module.Analysis of different fusion mechanism: A series of ab-lation experiments were conducted to evaluate the efficacyof the proposed CFA fusion mechanism in comparison toalternative fusion methods like Detour fusion , Concatfusion, and Gated fusion. The impact of the AP measureobtained from these experiments during the training process",
  ". Conclusion": "This research presents a new WS-VAD framework with aCross-modal Fusion Adapter (CFA) module and a Hyper-bolic Lorentzian Graph Attention (HLGAtt) module to de-tect anomaly events such as violence and nudity accurately.The CFA module addresses the imbalanced modality infor-mation issue and effectively facilitates multi-modal inter-action by dynamically selecting the relevant audio featureswith corresponding visual features. Additionally, the HL-GAtt module captures the hierarchical relationships withinnormal and abnormal representations, thereby improvingthe accuracy of separating normal and abnormal features.Through extensive experiments and ablation studies, it hasbeen demonstrated that the proposed model outperforms ex-isting violence and nudity detection methods. Salem AlMarri, Muhammad Zaigham Zaheer, and KarthikNandakumar. A multi-head approach with shuffled segmentsfor weakly-supervised video anomaly detection. In Proceed-ings of the IEEE/CVF Winter Conference on Applications ofComputer Vision, pages 132142, 2024. 1, 2, 6 S. Avila, N. Thome, M. Cord, E. Valle, and A. de A. Araujo.Bossa: Extended bow formalism for image classification. In2011 18th IEEE International Conference on Image Process-ing, pages 29092912, 2011. 6, 7 Sandra Avila, Nicolas Thome, Matthieu Cord, EduardoValle, and Arnaldo De A ArauJo. Pooling in image repre-sentation: The visual codeword point of view.ComputerVision and Image Understanding, 117(5):453465, 2013. 6,7",
  "David Alexander Forsyth and Margaret M. Fleck. Automaticdetection of human nudes. International Journal of Com-puter Vision, 32:6377, 1999. 3": "Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit KRoy-Chowdhury, and Larry S Davis. Learning temporal reg-ularity in video sequences. In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages733742, 2016. 6 Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort FGemmeke, Aren Jansen, R Channing Moore, Manoj Plakal,Devin Platt, Rif A Saurous, Bryan Seybold, et al. Cnn archi-tectures for large-scale audio classification. In 2017 ieee in-ternational conference on acoustics, speech and signal pro-cessing (icassp), pages 131135. IEEE, 2017. 6",
  "Michael J. Jones and James M. Rehg. Statistical color mod-els with application to skin detection. Int. J. Comput. Vision,46(1):8196, 2002. 3": "Hamza Karim, Keval Doshi, and Yasin Yilmaz. Real-timeweakly supervised video anomaly detection.In Proceed-ings of the IEEE/CVF Winter Conference on Applications ofComputer Vision, pages 68486856, 2024. 1, 2, 6 Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,Tim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-man action video dataset. arXiv preprint arXiv:1705.06950,2017. 6",
  "Xinyu Ou, Hefei Ling, Han Yu, Ping Li, Fuhao Zou, and SiLiu. Adult image and video recognition by a deep multicon-text network and fine-to-coarse strategy. 8(5), 2017. 3": "Wen-Feng Pang, Qian-Hua He, Yong-jian Hu, and Yan-Xiong Li. Violence detection in videos based on fusing vi-sual and audio information. In ICASSP 2021-2021 IEEE in-ternational conference on acoustics, speech and signal pro-cessing (ICASSP), pages 22602264. IEEE, 2021. 1, 2, 6 Sujoy Paul, Sourya Roy, and Amit K Roy-Chowdhury. W-talc: Weakly-supervised temporal activity localization andclassification. In Proceedings of the European Conferenceon Computer Vision (ECCV), pages 563579, 2018. 5 Xiaogang Peng, Hao Wen, Yikai Luo, Xiao Zhou, KeyangYu, Yigang Wang, and Zizhao Wu.Learning weakly su-pervised audio-visual violence detection in hyperbolic space.arXiv preprint arXiv:2305.18797, 2023. 1, 2, 4, 5, 6, 7, 8 Christian Platzer, Martin Stuetz, and Martina Lindorfer. Skinsheriff: a machine learning solution for detecting explicit im-ages. page 4556, New York, NY, USA, 2014. Associationfor Computing Machinery. 3",
  "Eric Qu and Dongmian Zou.Hyperbolic convolution viakernel point aggregation. arXiv preprint arXiv:2306.08862,2023. 5": "Mohammad Sabokrou, Mohammad Khalooei, MahmoodFathy, and Ehsan Adeli.Adversarially learned one-classclassifier for novelty detection.In Proceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 33793388, 2018. 2 Sonali Samal, Yu-Dong Zhang, Thippa Reddy Gadekallu,and Bunil Kumar Balabantaray. Asyv3: Attention-enabledpooling embedded swin transformer-based yolov3 for ob-scenity detection. Expert Systems, 2023. 3, 6, 7 Utsav Shah, Muhammad Aqmar, Mitsuru Nakazawa, andBjorn Stenger. Content filtering in streaming video using do-main adaptation. In 2021 17th International Conference onMachine Vision and Applications (MVA), pages 16. IEEE,2021. 3, 6, 7 Waqas Sultani, Chen Chen, and Mubarak Shah. Real-worldanomaly detection in surveillance videos. In Proceedings ofthe IEEE conference on computer vision and pattern recog-nition, pages 64796488, 2018. 1, 2, 5, 6 Weijun Tan, Qi Yao, and Jingfeng Liu. Overlooked videoclassification in weakly supervised video anomaly detection.In Proceedings of the IEEE/CVF Winter Conference on Ap-plications of Computer Vision, pages 202210, 2024. 6 Yu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh,Johan W Verjans, and Gustavo Carneiro. Weakly-supervisedvideo anomaly detection with robust temporal feature magni-tude learning. In Proceedings of the IEEE/CVF internationalconference on computer vision, pages 49754986, 2021. 1,2, 6 Hoang-Loc Tran, Quang-Huy Nguyen, Dinh-Duy Phan,Thanh-Thien Nguyen, Khac-Ngoc-Khoi Nguyen, and Duc-Lung Vu. Additional learning on object detection: A novelapproach in pornography classification. In Future Data andSecurity Engineering. Big Data, Security and Privacy, SmartCity and Industry 4.0 Applications: 7th International Con-ference, FDSE 2020, Quy Nhon, Vietnam, November 2527,2020, Proceedings 7, pages 311324. Springer, 2020. 3, 6, 7",
  "Laurens Van der Maaten and Geoffrey Hinton. Visualizingdata using t-sne. Journal of machine learning research, 9(11), 2008. 7": "Xizi Wang, Feng Cheng, Shilin Wang, Huanrong Sun, Gong-shen Liu, and Cheng Zhou. Adult image classification by alocal-context aware network. 2018 25th IEEE InternationalConference on Image Processing (ICIP), pages 29892993,2018. 3 Jhih-Ciang Wu, He-Yen Hsieh, Ding-Jie Chen, Chiou-ShannFuh, and Tyng-Luh Liu. Self-supervised sparse representa-tion for video anomaly detection. In European Conferenceon Computer Vision, pages 729745. Springer, 2022. 1, 2, 6",
  "Peng Wu and Jing Liu. Learning causal temporal relation andfeature discrimination for anomaly detection. IEEE Transac-tions on Image Processing, 30:35133527, 2021. 1, 2, 6": "Peng Wu, Jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao,Zhaoyang Wu, and Zhiwei Yang. Not only look, but alsolisten: Learning multimodal violence detection under weaksupervision. In Computer VisionECCV 2020: 16th Euro-pean Conference, Glasgow, UK, August 2328, 2020, Pro-ceedings, Part XXX 16, pages 322339. Springer, 2020. 1, 2,4, 5, 6 Jiashuo Yu, Jinyu Liu, Ying Cheng, Rui Feng, and YuejieZhang. Modality-aware contrastive instance learning withself-distillation for weakly-supervised audio-visual violencedetection.In Proceedings of the 30th ACM InternationalConference on Multimedia, pages 62786287, 2022. 2, 6 Chen Zhang, Guorong Li, Yuankai Qi, Shuhui Wang, LaiyunQing, Qingming Huang, and Ming-Hsuan Yang. Exploitingcompleteness and uncertainty of pseudo labels for weaklysupervised video anomaly detection.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1627116280, 2023. 1, 2, 6"
}