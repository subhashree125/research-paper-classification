{
  "Abstract": "Open-vocabulary 3D instance segmentation transcendstraditional closed-vocabulary methods by enabling theidentification of both previously seen and unseen objectsin real-world scenarios. It leverages a dual-modality ap-proach, utilizing both 3D point clouds and 2D multi-viewimages to generate class-agnostic object mask proposals.Previous efforts predominantly focused on enhancing 3Dmask proposal models; consequently, the information thatcould come from 2D association to 3D was not fully ex-ploited. This bias towards 3D data, while effective for fa-miliar indoor objects, limits the systems adaptability tonew and varied object types, where 2D models offer greaterutility. Addressing this gap, we introduce Zero-Shot Dual-Path Integration Framework that equally values the con-tributions of both 3D and 2D modalities. Our frameworkcomprises three components: 3D pathway, 2D pathway, *equal contribution.This work was supported by Institute for Information & communicationsTechnology Planning & Evaluation (IITP) grant funded by the Korea gov-ernment(MSIT) (No. 2021-0-01381, Development of Causal AI throughVideo Understanding and Reinforcement Learning, and Its Applications toReal Environments) and (No.2022-0-00184, Development and Study of AITechnologies to Inexpensively Conform to Evolving Policy on Ethics). and Dual-Path Integration.3D pathway generates spa-tially accurate class-agnostic mask proposals of commonindoor objects from 3D point cloud data using a pre-trained3D model, while 2D pathway utilizes pre-trained open-vocabulary instance segmentation model to identify a di-verse array of object proposals from multi-view RGB-D im-ages. In Dual-Path Integration, our Conditional Integrationprocess, which operates in two stages, filters and mergesthe proposals from both pathways adaptively. This processharmonizes output proposals to enhance segmentation ca-pabilities. Our framework, utilizing pre-trained models in azero-shot manner, is model-agnostic and demonstrates su-perior performance on both seen and unseen data, as ev-idenced by comprehensive evaluations on the ScanNet200and qualitative results on ARKitScenes datasets.",
  ". Capability of pre-trained open-vocabulary 2D instancesegmentation in detecting uncommon and unseen object classesthat remain undetected by pre-trained 3D instance segmentationmodels": "closed-vocabulary paradigm, where the classes of objectsto be segmented are predetermined and known during thetraining phase. However, real-world scenarios frequentlypresent objects that fall outside of these predefined classes,making closed-vocabulary segmentation inadequate. Open-vocabulary 3D instance segmentation emerged as a nec-essary evolution to address these real-world complexities.The challenge lies in the systems ability to generalize be-yond the training dataset and to adapt to the unpredictabilityinherent in real-world environments.Current methodologies independently process3D and 2D data of the same scene for discrete sub-taskswithout harnessing the inherent synergistic potential be-tween two modalities and heavily depend on the initial 3Dinstance masks for subsequent classification. From theseworks, it has been observed that while a class-agnostic 3Dinstance segmentation model using point cloud data ex-cels at segmenting common objects such as TV, it oftenstruggles with uncommon classes like paper towel rolland unseen classes like facial cream. Conversely, open-vocabulary 2D instance segmentation excels at detectingunfamiliar objects within 3D data, benefiting from the ro-bust generalization afforded by the vision-language under-standing capabilities of pre-trained image classifiers, as il-lustrated in . Thus, we argue that leveraging bothmodalities of the same scene offers distinct advantages.Bridging the gap between two modalities, we proposea Zero-shot Dual-Path Integration Framework designed tosynergistically merge the instance proposals derived fromthe 3D point cloud and the 2D multi-view images, which excel in spatial precision on common objects and diverserecognition capabilities respectively. Our framework com-prises three components: 3D pathway, 2D pathway, andDual-Path Integration.In the 3D pathway, 3D instance masks are generatedfrom a pre-trained class-agnostic mask proposal generatorusing a 3D point cloud of spatial information to generate asmany proposals as possible. In the 2D pathway, 2D instancemasks are generated from an pre-trained open vocabulary2D instance segmentation using RGB-D multi-view imagesequence of 2D visual details, which are then projected intothe 3D scan and refined through Instance Fusion Module.Employing a straightforward integration approach of us-ing all proposals from both the 3D and 2D pathways, aswe call Simple Integration, will enhance the performanceof instance segmentation to a certain extent as the diversityof proposals enhances the recall rate. Yet, the quality of theproposals is also critical, as integrating numerous but low-quality proposals can lead to increased false positive, whichmay adversely affect the overall precision. Consequently,while the diversity of proposals can boost the recall, it isessential to maintain a balance between quantity and qual-ity. To this end, in the final Dual-Path Integration phase,our meticulously designed Conditional Integration evalu-ates proposals from both the 3D and 2D modalities withimpartial consideration. This module unfolds in two piv-otal stages: (1) Dual-modality Proposal Matching and (2)Adaptive Integration. During the Dual-modality ProposalMatching stage, we compute Intersection-of-Union (IoU)across the proposals from 3D and 2D pathways. This aimsto identify pairs of proposals with overlapping regions, sug-gesting they may represent parts of identical objects. Theunique proposals with no overlap with any other propos-als of other modality are filtered to be added to the finalproposal outputs. Subsequently, Adaptive Integration es-tablishes a balance between segmentation precision and theidentification of diverse instances based on the IoU assess-ment. The samples of the 3D pathway, 2D pathway, andDual-path instances can be seen in .Our contributions are as follows: Zero-shot 3D instance segmentation via comprehen-sive exploitation of pre-trained models of both 3D and2D modalities: We introduce zero-shot framework thatjudiciously leverages the strengths of pre-trained modelsof both 3D point cloud and 2D image data for 3D instancesegmentation. This approach embraces a model-agnosticstrategy that avoids the traditional dependency on a pre-trained model of single modality. Dual-Path Integration of Conditional Integration: Wepropose a Dual-Path Integration with Conditional Integra-tion process that effectively combines instance mask pro-posals from both 3D and 2D pathways to reconcile andenhance mask proposals. Enhanced Overall Performance: Our frameworks effi-cacy is validated through evaluations on the ScanNet200and qualitative results in ARKitScenes.The resultsdemonstrate an uplift in the overall performance for open-vocabulary 3D instance segmentation, underscoring thepotency of our proposed approach.",
  ". Related Work": "Closed Vocabulary 3D Instance Segmentation.3Dinstance segmentation techniques are categorized intoproposal-based, clustering-based, and transformer-basedapproaches. Proposal-based methods identify3D bounding boxes to distinguish instances, yet face chal-lenges with varying point cloud distributions. Clustering-based approaches pre-dict semantic categories and use geometric offsets for in-stance grouping, though they require extensive manual tun-ing and may struggle with novel test objects. Transformer-based methods , leveraging the Mask2Formerframework , achieve state-of-the-art results by represent-ing instances as queries within a transformer decoder, effec-tively utilizing global features for mask prediction, demon-strating robust performance, particularly on the ScanNetbenchmark . Open Vocabulary 2D Instance Segmentation.Open-vocabulary 2D segmentation, empowered by large-scalevision-language models such as CLIP , LiT , andALIGN , has emerged as a significant advancementin instance segmentation. This development has fosterednovel open-vocabulary and zero-shot segmentation method-ologies. Pixel-level embedding techniques have shown promising results, although their suc-cess is contingent on mask precision and requires specifictraining. To overcome these challenges, integrating robustzero-shot detection and segmentation models such as ViLD, OWL-ViT , Detic , and Grounding-DINO with the Segment Anything Model (SAM) facilitatesopen-vocabulary 2D instance segmentation. This approach,especially when combining Grounding-DINO with SAM(Grounded-SAM), leverages CLIP features for classifica-tion, enabling precise segmentation and classification with-out the necessity for additional fine-tuning or training. Inour work, we employ Grounded-SAM for open-vocabulary2D instance segmentation. Open Vocabulary 3D Scene Understanding.Recent ad-vancements in open-vocabulary 3D scene understanding have focused onleveraging 2D vision-language model features for 3D re-construction due to the absence of large-scale 3D datasets.Techniques like OpenScene and ConceptFusion utilize pixel-wise CLIP features for text-aligned 3Dfeature extraction.LERF employs similar CLIP-based semantic fields within NeRF frameworks,providing query-specific scene heatmaps but limited ob-ject instance understanding. Instance-based methodologies like SAM3D and OpenMask3D, project 2D masks onto 3D point clouds for enhancedinstance recognition, though challenges remain in instancemerging and quality of 3D mask proposals. OVIR-3D attempts to improve instance merging but struggles withlarge instance backgrounds. Our method overcomes theseobstacles by integrating high-performing pre-trained mod-els from both 2D and 3D domains, reducing reliance onsingle-modality insights.",
  ". Method": "The Zero-Shot Dual-Path Integration Framework is de-signed to predict class-agnostic 3D instance masks and theircorresponding CLIP-based features for open-vocabulary in-stance classification, as illustrated in . This frame-work operates on posed RGB-D images and reconstructed3D point clouds, leveraging queries to isolate class-specificinstance masks through a dual-pathway approach: a 3Dpathway for mask proposal generation with point cloudsand a 2D pathway for RGB-D based mask proposal gen-eration.In our approach, the 3D pathway employs a mask pro-posal network to generate 3D instance masks, integrat-ing 2D bounding boxes and CLIP-based features to bridgeopen-vocabulary concepts with these masks, leveraging ar-chitectures adept at detecting sizeable objects . Simul-taneously, the 2D pathway applies an open-vocabulary 2Dinstance segmentation network to RGB-D images, creating2D mask proposals. These are then projected into the 3Dpoint cloud, refined into finalized instance masks by an In-stance Fusion Module.Integration of these pathways is achieved through ourConditional Integration in the Dual-Path Integration phase,which encompasses Dual-modality Proposal Matching andAdaptive Integration. This process begins with the match-ing of overlapping proposals from both pathways usingIntersection-of-Union (IoU) metrics, followed by the condi-tional merging of these proposals based on the assessmentof their IoU. This integration technique enhances the frame-works accuracy in object recognition and segmentation byleveraging the complementary strengths of both 2D and 3Ddata.",
  "Given a 3D point cloud, denoted as P RN3 where Nrepresents the total number of points, each point is repre-sented by a 3D position. The 3D pathways objective is": ". Overview of our Zero-Shot Dual-Path Integration Framework. The 3D pathway takes 3D point cloud P as input to generatedclass-agnostic 3D instance masks M3Diwith pre-trained 3D Mask Proposal Network and the per-mask visual features F3Diare extractedwith CLIP visual encoder . The 2D pathway also generates its own 3D instance masks M2Djusing RGB-D Image I input with Open-vocabulary 2D Mask Proposal Network and 2D-to-3D Projection module, along with the per-mask visual features F2Diof each mask. Theoutputs of two pathways are integrated through the Conditional Integration which utilizes Intersection-of-Union (IoU) for Dual-modalityProposal Matching and Adaptive Integration, having final 3D instance results Mk and their visual features Fk as outputs. to segment the given point cloud into class-agnostic 3Dinstance mask proposals, represented through a collectionof binary masks, where each mask is denoted as M3Di=(M 3Di,1 , ..., M 3Di,N) where M 3Di,n {0, 1}, meaning n-th pointbelongs to i-th object instance. For generating the 3D in-stance masks, Mask3D is used as our 3D instance seg-mentation network, which utilizes U-Net style sparse con-volutional backbone as a feature extractor. A fixed num-ber of object queries go through the transformer decoderlayers to attend to global features iteratively, directly out-putting instance predictions. This generates a binary maskfor each instance with predicted class labels and their con-fidence scores. However, since we want a class-agnosticmask proposal network, Mask3D is modified to omitthe predicted class labels and confidence scores to only con-centrate on generating binary instance mask proposals. Thisway, we obtain open-vocabulary representations instead ofsemantic class predictions confined to closed-vocabulary. Then, we derive per-mask text-aligned visual featuresusing pre-trained CLIP for querying open-vocabularyconcepts associated with predicted instance masks.In-spired by , we first select the top k RGB-D imageswith the highest visibility of each instance mask. In priorwork , points projected onto the images are utilized asthe prompts to guide the SAM in generating the 2Dbounding boxes, which are utilized for cropping the imagesfor CLIP feature extraction. However, this projection cancause errors due to approximation in the occlusion test, andthe random selection of the k projected points might erro- neously include points lying outside the actual instances,leading to the generation of poor-quality bounding boxes.Consequently, the extracted CLIP features from those poor-quality bounding boxes suffer from inadequacies. To by-pass this issue, we directly use the projected 3D boundingbox into 2D images to get CLIP-based features, eliminat-ing potential errors in random point selection. We employmulti-level crops of specific regions for feature enrichmentto encapsulate extensive contextual details from the sur-rounding environment. Leveraging the CLIP visual encoder, the CLIP feature vectors F3Di Rd of the croppedobject images are extracted and average-pooled to generatefinal mask-feature representations for each object.",
  ". 2D Pathway: 3D Mask Proposals from Multi-view RGB-D Images": "The objective of 2D pathway is to generate 3D in-stance masks, where each mask is denoted as M2Dj=(M 2Dj,1 , ..., M 2Dj,N), where M 2Dj,n {0, 1}, meaning n-th pointbelongs to j-th object instance, from RBG-D image Itwhere t is the image frame at time t with known cam-era intrinsic matrix C and world-to-camera extrinsic matrix(pose) Et. For this process, we first utilize Grounded-SAM,a fusion of Grounding DINO and SAM , as pre-trained 2D open-vocabulary instance segmentation networkto obtain 2D mask proposals m2Dt,j . Grounding DINO takesthe text prompt as an input to produce the 2D boundingboxes, which SAM subsequently uses to obtain 2D maskproposals. These 2D mask proposals m2Dt,j undergo subse- quent projection into the 3D point cloud with known cameraintrinsic, pose, and depth. Additionally, we extract CLIP-based features F2Dj Rd from the corresponding croppedimage for each proposal.Given that mask proposals sourced from 2D images maybe fragmented due to occlusion, these proposals from the2D pathway are passed to an Instance Fusion process forcomplete proposals.Drawing upon methodologies from, the Instance Fusion Module accumulates 3D projectedinstances within a memory bank. It then periodically ex-ecutes a filtering and merging process, utilizing the 3DIntersection-of-Union metric in conjunction with featuresimilarity analysis.",
  "|M3Di M2Dj |.(1)": "This computation is conducted for each possible pair ofinstances across the modalities. As a result, IoU matrixproviding a comprehensive representation of the spatial re-lationships between all instances across the 3D and 2Dmodalities is created.To identify unique proposals from each modality for in-clusion in the final instance proposals Mk, we systemat-ically evaluate instances from M2Djand M3Diagainst theIoU matrix. This process aims to detect instances withoutoverlap across the entirety of instances from the alternatemodality. This evaluation can be articulated as follows:",
  "i, if (j, IoUij = 0), then add M3Dito Mk": "This procedure ensures that any instance M2Djlacking over-lap with all M3Diinstances (indicated by an IoUij valueof 0) is directly incorporated into Mk. Conversely, it alsoguarantees the inclusion of any M3Diinstance that does notoverlap with all M2Djinstances into Mk. Additionally, pro-posal pairs exhibiting the smallest IoUij for all instancesM2Djare added to Mk, aiming to enrich the segmentationwith a broader array of detected objects.Excluding the unique instances identified in the firststage above, our Conditional Integration progresses to the",
  "(2)": "Equation 2 (left) introduces IoU3Dij , which quantifies theproportion of overlap between an instance from 3D pathwayM3Diand a instance from 3D pathway M3Direlative to theentire instance from 3D pathway. Conversely, Equation 2(right) defines IoU2Dij as the ratio of their intersection to thetotal area of the instance from 2D pathwayM2Dj . Together,these IoU metrics offer a dual perspective on the relationsbetween pairs of instances from the 3D pathway and the2D pathway. By analyzing the extent to which an instancefrom one pathway encompasses the spatial domain of theinstance from another pathway, it offers insights into thepriority between two proposals.Subsequently, we select the proposal pair with highest IoUij for each proposals of M2Djand assess them into fourscenarios for Adaptive Integration: (1) high IoU for bothIoU2Dijand IoU3Dij , (2) low IoU for both, (3) high IoUfor the IoU2Dij but low for IoU3Dij (e.g. proposal from the2D pathway is subgroup of that from the 3D pathway), and(4) the vice-versa. By selecting the proposal pair exhibitingthe highest IoUij, we select the proposal from the alternatepathway that demonstrates the most significant relationship,encapsulating both concordance and discordance, for fur-ther evaluation.The four distinct scenarios of Adaptive Integration areelaborated below:1. Significant overlap: For proposal pairs exhibiting ex-tensive overlap, it is inferred that they likely depict thesame object. Thus, these proposals are merged into asingular, comprehensive proposal for inclusion in Mk,ensuring a unified representation. 2. Slight overlap: When a pair demonstrates only slightoverlap, yet selected beforehand for having maximumoverlap among all considered pairs, it is surmised thatthe proposals likely denote two distinct objects in closeproximity. Accordingly, both proposals are maintainedseparately in Mk, preserving the individuality of eachdetected object. 3. Proposal from 2D is subgroup of proposal from 3D:In instances where a proposal from 2D pathway is almostentirely encompassed by a proposal from 3D pathway, itis treated as a unique finding exclusive to the 2D path-way and thus given precedence for inclusion into Mk.This decision is based on the assumption that the pro-posal from 2D modality may highlight a detail or as-pect not captured from the 3D modality. Although the larger overlapping proposal is neglected in this instance,its potential value is recognized. We anticipate that itwill align with nearby 2D pathway proposals, therebybeing considered under different scenarios. 4. Proposal from 3D is subgroup of proposal from 2D:Analogous to scenario (3) with reversed roles, the pro-posal from 3D pathway is prioritized for addition to Mk.This reflects the broader spatial coverage and potentiallysignificant detection afforded by the 3D pathway.Through these scenario-specific strategies, our integra-tion process adeptly balances the quantity and quality ofproposals offered by both pathways, enhancing the over-all accuracy and completeness of instance segmentation. Todifferentiate between high and low IoU values, thresh-olds for each pathway are designated as 2D and 3D, re-spectively, with their optimal values determined throughempirical experimentation. Leveraging visual features de-rived from 3D point clouds and 2D multi-view images, F3Di and F2Dj , we employ an averaging process when mergingtwo masks. After the Adaptive Integration, we obtain thefinal instance proposals Mk and the corresponding CLIP-based features Fk ready to perform a semantic label assign-ment.During the inference phase, a textual query denoted as qcorrelates with a repository of representative features linkedto individual 3D instances. The text feature Fq will be ex-tracted from the CLIP encoder to compare with instancefeatures Fk in the scene. Subsequently, these instances de-noted as Mk, undergo a ranking process based on their re-semblance to the query. The top-ranked instances, thus de-termined, are retrieved and subsequently returned.",
  ". Experiments": "In this section, we present both quantitative and qualitativeoutcomes of our Zero-shot Dual-Path Integration Frame-work. We conduct a quantitative assessment, comparing ouropen-vocabulary 3D instance segmentation method againstexisting approaches within a closed-vocabulary setting. Ab-lation studies further dissect the impact of baseline architec-tures for each pathway, alongside the examination of met-rics and thresholds pivotal to our Dual-path Integration pro-cess. Additionally, we showcase qualitative results from theScanNet200 and ARKitScenes datasets to underscore ourmethods effectiveness in open-vocabulary 3D instance seg-mentation, demonstrating its proficiency in accurately iden-tifying a wide spectrum of objects.",
  ". Dataset and Metric": "Dataset.Our evaluation framework employs the Scan-Net200 benchmark dataset , utilizing its validation setof 312 unique scenes for 3D instance segmentation perfor-mance assessment across a closed vocabulary of 200 cat-egories. Further, we adopt the categorization scheme by Rozenberszki et al. , dividing the ScanNet200 objectclasses into head, common, and tail subsets, with 66,68, and 66 categories respectively, to analyze model perfor-mance across varying object occurrence frequencies. Ad-ditionally, we leverage the ARKitScenes dataset , whichconsists of over 5K scans from approximately 1.6K diverseindoor settings, offering 3D mesh reconstructions, RGB anddepth images, and ARKitSLAM-estimated camera poses.This dataset aids in simulating realistic indoor scanning tra-jectories. Performance analysis is further enhanced by em-ploying queries from OpenSUN3D within the Challengedevelopment set, demonstrating the effectiveness of our ap-proach in advanced indoor scene understanding tasks. Metric.In our evaluation, we adopt the widely recog-nized metric for 3D instance segmentation: average pre-cision (AP). The AP scores are calculated at mask overlapthresholds of 50%, 25%, and average over the overlap rangeof [0.5 : 0.95 : 0.05], in line with the ScanNet evaluationprotocol. Furthermore, we analyze the AP scores across thehead, common, and tail subsets of ScanNet200. Thisallows us to gain deeper insights into the performance ofour method across different frequency categories. . Qualitative results showcasing the proficiency of ourframework in performing open-vocabulary 3D instance segmen-tation. The displayed results include objects from two distinctdatasets: the upper two objects are from ScanNet200 scenes, whilethe lower two are from ARKitScenes, demonstrating our frame-works adaptability and effectiveness across diverse environments.",
  ". Experimental Details": "In our experiments, we conducted computations using a sin-gle RTX 8000 GPU. We utilized posed RGB-D pairs fromthe ScanNet200 dataset, processing one frame out of everyten frames within the RGB-D sequences. To extract imagefeatures from mask crops, we employed the CLIP visualencoder from the ViT-L/14 model, known for its fea-ture dimensionality of 768. For Adaptive Integration, the",
  ". Quantitative Results": "In the comprehensive evaluation presented in , per-formance of our approaches in closed-vocabulary instancesegmentation tasks within the ScanNet200 benchmarkis provided. This distinction in performance is particularlymarked within the head and common categories, whilethe disparity narrows in the tail categories. For the previous works on open-vocabulary models,OpenScene is constructed based on 2D model OpenSeg trained on labeled datasets for 2D semantic segmenta-tion. OpenMask3D , a state-of-the-art open-vocabularymodel, is built upon the Mask3D for generating class-agnostic 3D mask proposals. Compared to these previousmethods, our Dual-Path Integration Framework has a dis-tinct performance advantage in AP. This outcome substanti-ates our initial hypothesis about the efficacy of our method.",
  ". Qualitative Results and Comparisons": "presents the qualitative results that underscore the ef-ficacy of our proposed framework within both seen (Scan-Net200) and unseen (ARKitScenes) data, thereby affirm-ing the frameworks extensive adaptability and proficiencyacross diverse environments.Being a zero-shot open-vocabulary framework, it enables the segmentation of ob-jects through free-form text queries, even for objects absentin traditional instance segmentation datasets. In , we provide qualitative comparisons thathighlight the distinction between our framework and theOpenMask3D in segmenting uncommon objects from thetail category of ScanNet200, alongside unseen objectsnot present in the datasets predefined categories. Thesecomparisons underscore our proposed frameworks en-hanced proficiency in accurately segmenting objects thathave posed challenges to previous methods, which predom-inantly leveraged 3D point cloud data for instance segmen-tation.",
  ". Ablation Studies": "We conducted an ablation study to assess the individual con-tributions of components within our Dual-Path IntegrationFramework across 312 scenes from the ScanNet200 valida-tion set, as outlined in . Our 3D pathway yieldedsuperior performance compared to the reported results inOpenMask3D , primarily due to our method of directlyusing the projected 3D bounding box into 2D images to getCLIP-based features for eliminating potential errors in ran-dom point selection. For 2D pathway, relying solely on the2D data by using 3D projected CLIP resulted in subop-timal performance. While the Simple Integration methodof using all proposals from both the 3D and 2D pathwaysachieves enhanced performance, it falls short of adequatelyaddressing the low quality and redundancy problem of theproposals, failing to leverage the intrinsic strengths uniqueto each pathway.In contrast, our Dual-path Integrationstrategy employs a selective and adaptive approach to inte-grate the two modalities, yielding improvements over Sim-ple Integration.In , the recall rate of tail category of the Scan-Net200 validation set is reported. It highlights the capabilityof our Dual-Path Integration in identifying uncommon ob-jects. A significant factor contributing to the enhancementof the performance is the robust generalization ability of-fered by the vision-language understanding capabilities in-herent in the pre-trained image classifier of the 2D pathway.The notable improvement in the recall rate when comparingthe Dual-path with the 3D pathway indicates the efficacy ofthe 2D pathway in our method. presents an ablation study on the impact of theIoU thresholds 3D and 2D within the Adaptive Integrationprocess. The analysis reveals that the Average Precisionat a threshold of 25% (AP25) remains relatively unaffectedby variations in these thresholds. In contrast, both AP50and the mean Average Precision (mAP) exhibit sensitivityto changes in thresholds, suggesting that a higher thresh-old is crucial for refining segmentation precision. This pat-tern underscores the importance of carefully calibrating theIoU thresholds to optimize the overall segmentation perfor-mance and achieve a balance between recall and precision.",
  ". Ablation study on IoU threshold 3D and 2D for Adap-tive Integration on ScanNet200 validation set": "ogy: Dual-modality Proposal Matching and Adaptive In-tegration, aimed at identifying and categorizing significantproposal pairs into distinct categories for effective integra-tion of results from two different modalities. Evaluationsconducted on the ScanNet200 benchmark dataset and ARK-itScenes dataset illustrate our frameworks substantial im-provements over prior methodologies, validating the effi-cacy of integrating 3D and 2D segmentation techniques. Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry,Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe,Daniel Kurz, Arik Schwartz, et al. Arkitscenes: A diversereal-world dataset for 3d indoor scene understanding usingmobile rgb-d data. arXiv preprint arXiv:2111.08897, 2021.6",
  "Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexan-der Kirillov, and Rohit Girdhar.Masked-attention masktransformer for universal image segmentation. 2022. 3": "Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-ber, Thomas Funkhouser, and Matthias Niener. Scannet:Richly-annotated 3d reconstructions of indoor scenes.InProceedings of the IEEE conference on computer vision andpattern recognition, pages 58285839, 2017. 3, 6, 7, 8 Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang,Song Bai, and Xiaojuan Qi.Pla: Language-driven open-vocabulary 3d scene understanding.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, 2023. 3 Francis Engelmann, Martin Bokeloh, Alireza Fathi, BastianLeibe, and Matthias Niener. 3D-MPA: Multi Proposal Ag-gregation for 3D Semantic Instance Segmentation. In IEEEConference on Computer Vision and Pattern Recognition(CVPR), 2020. 1, 3 Francis Engelmann, Ayca Takmaz, Jonas Schult, ElisabettaFedele, Johanna Wald, Songyou Peng, Xi Wang, Or Litany,Siyu Tang, Federico Tombari, et al. Opensun3d: 1st work-shop challenge on open-vocabulary 3d scene understanding.arXiv preprint arXiv:2402.15321, 2024. 6",
  "Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scal-ing open-vocabulary image segmentation with image-levellabels. In European Conference on Computer Vision, pages540557. Springer, 2022. 3, 7": "Benjamin Graham, Martin Engelcke, and Laurens VanDer Maaten.3d semantic segmentation with submani-fold sparse convolutional networks. In Proceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 92249232, 2018. 4 Qiao Gu, Alihusein Kuwajerwala, Sacha Morin, KrishnaMurthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Cor-ban Rivera, William Paul, Kirsty Ellis, Rama Chellappa,Chuang Gan, Celso Miguel de Melo, Joshua B. Tenenbaum,Antonio Torralba, Florian Shkurti, and Liam Paull. Concept-graphs: Open-vocabulary 3d scene graphs for perception andplanning. arXiv, 2023. 3",
  "Zhening Huang, Xiaoyang Wu, Xi Chen, Hengshuang Zhao,Lei Zhu, and Joan Lasenby. Openins3d: Snap and lookup for3d open-vocabulary instance segmentation. arXiv preprintarXiv:2309.00616, 2023. 2, 3": "Dat Huynh, Jason Kuen, Zhe Lin, Jiuxiang Gu, and EhsanElhamifar. Open-vocabulary instance segmentation via ro-bust cross-modal pseudo-labeling.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 70207031, 2022. 3 Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala,Qiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh Iyer,Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, Joshua B.Tenenbaum, Celso Miguel de Melo, Madhava Krishna, LiamPaull, Florian Shkurti, and Antonio Torralba. Conceptfusion:Open-set multimodal 3d mapping. arXiv, 2023. 3 Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and TomDuerig. Scaling up visual and vision-language representa-tion learning with noisy text supervision. In Internationalconference on machine learning, pages 49044916. PMLR,2021. 3 Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point group-ing for 3d instance segmentation. Proceedings of the IEEEConference on Computer Vision and Pattern Recognition(CVPR), 2020. 1, 3",
  "Justin Kerr, Chung Min Kim, Ken Goldberg, AngjooKanazawa, and Matthew Tancik. Lerf: Language embeddedradiance fields. In International Conference on ComputerVision (ICCV), 2023. 3": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-thing. arXiv preprint arXiv:2304.02643, 2023. 3, 4 Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, YinanZhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and DianaMarculescu. Open-vocabulary semantic segmentation withmask-adapted clip. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages70617070, 2023. 3",
  "superpoint tree networks. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 27832792, 2021. 1, 3": "Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, HaoZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, JunZhu, et al. Grounding dino: Marrying dino with groundedpre-training for open-set object detection.arXiv preprintarXiv:2303.05499, 2023. 3, 4 Shiyang Lu, Haonan Chang, Eric Pu Jing, Abdeslam Boular-ias, and Kostas Bekris. Ovir-3d: Open-vocabulary 3d in-stance retrieval without training on 3d data. In 7th AnnualConference on Robot Learning, 2023. 3, 5, 7",
  "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:Representing scenes as neural radiance fields for view syn-thesis. In ECCV, 2020. 3": "M Minderer, A Gritsenko, A Stone, M Neumann, D Weis-senborn, A Dosovitskiy, A Mahendran, A Arnab, M De-hghani, Z Shen, et al. Simple open-vocabulary object de-tection with vision transformers. arxiv 2022. arXiv preprintarXiv:2205.06230. 3 Tuan Duc Ngo, Binh-Son Hua, and Khoi Nguyen. Isbnet: a3d point cloud instance segmentation network with instance-aware sampling and box-aware dynamic convolution. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1355013559, 2023. 1, 3 SongyouPeng,KyleGenova,ChiyuJiang,AndreaTagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al.Openscene: 3d scene understanding with open vocabularies.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 815824, 2023. 3, 7 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 3, 4, 6, 7",
  "Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu.Superpoint transformer for 3d scene instance segmentation,2022. 1, 3": "Ayca Takmaz, Elisabetta Fedele, Robert W Sumner, MarcPollefeys, Federico Tombari, and Francis Engelmann. Open-mask3d: Open-vocabulary 3d instance segmentation. arXivpreprint arXiv:2306.13631, 2023. 2, 3, 4, 7 Vibashan VS, Ning Yu, Chen Xing, Can Qin, Mingfei Gao,Juan Carlos Niebles, Vishal M Patel, and Ran Xu. Mask-freeovis: Open-vocabulary instance segmentation without man-ual mask annotations. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages2353923549, 2023. 3",
  "Thang Vu, Kookhoi Kim, Tung M. Luu, Xuan ThanhNguyen, and Chang D. Yoo. Softgroup for 3d instance seg-mentation on 3d point clouds. In CVPR, 2022. 1, 3": "Jianzong Wu, Xiangtai Li, Henghui Ding, Xia Li, Guan-gliang Cheng, Yunhai Tong, and Chen Change Loy.Be-trayed by captions: Joint caption grounding and generationfor open vocabulary instance segmentation. arXiv preprintarXiv:2301.00805, 2023. 3 Yizheng Wu, Min Shi, Shuaiyuan Du, Hao Lu, Zhiguo Cao,and Weicai Zhong. 3d instances as 1d kernels. In ComputerVisionECCV 2022: 17th European Conference, Tel Aviv, Is-rael, October 2327, 2022, Proceedings, Part XXIX, pages235252. Springer, 2022. 1, 3 Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit:Semantic segmentation emerges from text supervision. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1813418144, 2022. 3 Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, SenWang, Andrew Markham, and Niki Trigoni. Learning ob-ject bounding boxes for 3d instance segmentation on pointclouds. In Advances in Neural Information Processing Sys-tems, pages 67376746, 2019. 1, 3",
  "Yunhan Yang, Xiaoyang Wu, Tong He, Hengshuang Zhao,and Xihui Liu. Sam3d: Segment anything in 3d scenes. arXivpreprint arXiv:2306.03908, 2023. 3, 7": "L. Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J.Guibas. Gspn: Generative shape proposal network for 3d in-stance segmentation in point cloud. 2019 IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR),pages 39423951, 2018. 1, 3 Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer.Lit: Zero-shot transfer with locked-image text tuning.InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1812318133, 2022. 3 Weiguang Zhao, Yuyao Yan, Chaolong Yang, Jianan Ye, XiYang, and Kaizhu Huang.Divide and conquer: 3d pointcloud instance segmentation with point-wise binarization. InProceedings of the IEEE/CVF international conference oncomputer vision (ICCV), pages 562571, 2023. 1, 3 Xingyi Zhou,Rohit Girdhar,Armand Joulin,PhilippKrahenbuhl, and Ishan Misra.Detecting twenty-thousandclasses using image-level supervision. In European Confer-ence on Computer Vision, pages 350368. Springer, 2022.3"
}