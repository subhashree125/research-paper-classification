{
  "University of Illinois Urbana-Champaign2Meta{junkun3,yxw}@illinois.edu{rotabulo,normanm,porzi,pkontschieder}@meta.com": ". Our ConsistDreamer lifts 2D diffusion with 3D awareness and consistency, achieving high-fidelity instruction-guided sceneediting with superior sharpness and detailed textures. Left: The three synergistic components within ConsistDreamer that enable 3Dconsistency. Right: State-of-the-art performance of ConsistDreamer across various editing tasks and scenes, especially when prior work(e.g., IN2N ) fails and in challenging large-scale indoor scenes from ScanNet++ . More results are on our project page.",
  "Abstract": "This paper proposes ConsistDreamer a novel frame-work that lifts 2D diffusion models with 3D awarenessand 3D consistency, thus enabling high-fidelity instruction-guided scene editing. To overcome the fundamental limi-tation of missing 3D consistency in 2D diffusion models,our key insight is to introduce three synergistic strategiesthat augment the input of the 2D diffusion model to be-come 3D-aware and to explicitly enforce 3D consistencyduring the training process. Specifically, we design sur-rounding views as context-rich input for the 2D diffusionmodel, and generate 3D-consistent structured noise insteadof image-independent noise. Moreover, we introduce self-supervised consistency-enforcing training within the per-scene editing procedure. Extensive evaluation shows thatour ConsistDreamer achieves state-of-the-art performancefor instruction-guided scene editing across various scenesand editing instructions, particularly in complicated large-scale indoor scenes from ScanNet++, with significantly im-proved sharpness and fine-grained textures. Notably, Con-sistDreamer stands as the first work capable of success-fully editing complex (e.g., plaid/checkered) patterns. Ourproject page is at immortalco.github.io/ConsistDreamer.",
  ". Introduction": "With the emergence of instruction-guided 2D generativemodels as in , it has never been easier to generate oredit images. Extending this success to 3D, i.e., instruction-guided 3D scene editing, becomes highly desirable forartists, designers, and the movie and game industries. Nev-ertheless, editing 3D scenes or objects is inherently chal-lenging. The absence of large-scale, general 3D datasetsmakes it difficult to create a counterpart generative modelsimilar to that can support arbitrary 3D scenes. There-fore, state-of-the-art solutions circumvent this chal-lenge by resorting to generalizable 2D diffusion models.This approach, known as 2D diffusion distillation, rendersthe scene into multi-view images, applies an instruction-conditioned diffusion model in 2D, and then distills the edit-ing signal back to 3D, such as through a neural radiancefield (NeRF) .However, a fundamental limitation of this solution is thelack of 3D consistency: a 2D diffusion model, acting in-dependently across views, is likely to produce inconsistentedits, both in color and shape. For example, a person inone view might be edited to be wearing a red shirt, whileappearing in a green shirt in another view. Using these im-ages to train a NeRF can still produce reasonable edits, butthe model will naturally converge towards an averagedrepresentation of the inconsistent 2D supervision, and lose",
  "arXiv:2406.09404v1 [cs.CV] 13 Jun 2024": "most of its details and sharpness. A commonly observedfailure mode is that of regular (e.g., checkered) patterns,which completely disappear once distilled to 3D due to mis-alignments across views. Generating consistent multi-viewimages thus becomes crucial for achieving high-fidelity 3Dscene editing.While largely overlooked in prior work, our investiga-tion reveals that the source of inconsistency is multi-faceted,and primarily originates from the input. (1) As the 2D dif-fusion model can only observe a single view at a time, itlacks sufficient context to understand the entire scene andapply consistent editing. (2) The editing process for eachimage starts from independently generated Gaussian noise,which brings challenges to consistent image generation. In-tuitively, it is difficult to generate consistent multi-view im-ages by denoising inconsistent noise, and even for a singleview, it may not always yield the same edited result. (3) Theinput to the 2D diffusion model contains no 3D information,making it much harder for the model to reason about 3Dgeometry and to share information across different views ofthe scene, even when made available to it.Motivated by these observations, we propose Consist-Dreamer a novel framework to achieve 3D consistency in2D diffusion distillation. ConsistDreamer introduces threesynergistic strategies that augment the input of the 2D dif-fusion model to be 3D-aware and enforce 3D consistency ina self-supervised manner during the training process.To address the limited context issue within a single view,our framework involves incorporating context from otherviews. We capitalize on the observation that 2D diffusionmodels inherently support composed images, where mul-tiple sub-images are tiled to form a larger image. Giventhe capability of the self-attention modules in the UNet ofthe 2D diffusion model to establish connections betweenthe same objects across different sub-images, each imagecan be edited with the context derived from other images.Therefore, we leverage the composed images to constructa surrounding view (), where one large, central mainview is surrounded by several small reference views. Thisapproach allows us to edit the main view with the contextfrom reference views, and vice versa. Doing so not only en-riches the context of the scene in the input, but also enablesthe simultaneous editing of multiple views.Regarding the noise, we introduce 3D-consistent struc-tured noise (), with the key insight of generating con-sistent noise for each view once at the beginning. Specifi-cally, we generate and fix Gaussian noise on the surface ofthe scene objects, and then render each view to obtain the2D noise used for the image at that view in all subsequentdiffusion generations. This approach aligns with existing3D diffusion work which also generates noise in 3D atthe beginning of a generation. Ensuring that the denoisingprocedure starts with consistent noise substantially facili-",
  "tates the process of achieving consistent images by the end": "The combination of surrounding views and structurednoise provides the 2D diffusion model with 3D consistentinput, yet it is insufficient. An explicit enforcement of 3Dconsistency is also required during the learning process. Tothis end, we propose self-supervised consistency-enforcingtraining within the per-scene editing procedure (). Weaugment the 2D diffusion model by a ControlNet thatintroduces 3D positional embedding to make it 3D-aware.Inspired by , we perform warping and averaging forall sub-views in the edited surrounding view image. Thisprocess yields a surrounding view of 3D consistent sub-views used as the self-supervision target. To further achievecross-batch consistency consistency between differentbatches in different generations we perform multiple gen-erations in parallel, and construct consistent target imagesfrom all sub-views in all generated surrounding view im-ages, so as to supervise all generations collectively. Af-ter consistency-enforcing training, the 2D diffusion modelis able to generate consistent multi-view images. Conse-quently, a trained NeRF will not have to smooth out incon-sistencies, but ultimately converge to sharp results preserv-ing fine-grained details. Empowered by such a 3D-consistent 2D diffusionmodel, our ConsistDreamer achieves high-fidelity and di-verse instruction-guided 3D scene editing without any meshexportation and refinements or a better scene representationlike Gaussian Splatting , as shown in .Com-pared with previous work, the editing results of Consist-Dreamer exhibit significantly improved sharpness and de-tail, while preserving the diversity in the original 2D diffu-sion models editing results. Notably, ConsistDreamerstands as the first work capable of successfully editing com-plex (e.g., checkered) patterns. Moreover, ConsistDreamerdemonstrates superior performance in complicated, high-resolution ScanNet++ scenes an accomplishmentwhere state-of-the-art methods faced challenges in achiev-ing satisfactory edits. Our contributions are three-fold.(1) We introduceConsistDreamer, a simple yet effective framework that en-ables 3D-consistent instruction-guided scene editing basedon distillation from 2D diffusion models. (2) We proposethree novel, synergistic components structured noise, sur-rounding views, and consistency-enforcing training thatlift 2D diffusion models to generate 3D-consistent imagesacross all generated batches. Notably, our work is the firstthat explores cross-batch consistency and denoising consis-tency in 2D diffusion distillation and attains these throughmanipulating noise. (3) We evaluate a range of scenes andediting instructions, achieving state-of-the-art performancein both, scenes considered by previous work and more com-plicated, large-scale indoor scenes from ScanNet++.",
  ". Related Work": "NeRF-BasedSceneEditing.Neuralradiancefield(NeRF) and its variants are widely-used approaches to representing scenes. NeRFleverages neural networks or other learnable architectures tolearn to reconstruct the 3D geometry of a scene only frommulti-view images and their camera parameters, and sup-port novel view synthesis. With the development of NeRF,editing a NeRF-represented scene is also deeply studied,covering different types of editing objectives and editingoperation indicators, a.k.a., user interfaces. Some meth-ods support editing the position, color, and/orshape of a specific object indicated by users through apixel, a text description, or a segment, etc. Another lineof work studies human-guided shape edit-ing, which allows users to indicate a shape editing oper-ation with a cage or point cloud provided by the model.The task we investigate is instruction-guided scene editing,which allows users to indicate the editing operation throughinstructions in natural language. The first work in this di-rection is NeRF-Art , which mainly focuses on styletransfer instructions, and uses pre-trained CLIP as thestylization loss for the instruction-indicated style. More re-cent work leverages diffusion models instead of CLIP to benefit from powerful diffusion modelsand support more general instructions.Distillation-Based 3D Scene Generation.Lacking 3Ddatasets to train powerful 3D diffusion models, current so-lutions distill the generation signal from a 2D diffusionmodel to exploit its ability in 3D generation. DreamFu-sion is the first work in this direction, which proposesscore distillation sampling (SDS) to distill the gradient up-date direction (score) from 2D diffusion models, andsupports instruction-guided scene generation by distilling apre-trained diffusion model . HiFA proposes an an-nealing technique and rephrases the distillation formula toimprove the generation result. Magic3D improves thegeneration results by introducing a coarse-to-fine strategyand a mesh exportation and refinement method. Prolific-Dreamer further improves the generation results by in-troducing an improved version of SDS, namely variationalscore distillation (VSD), to augment and fine-tune a pre-trained diffusion model and use it for generation.Diffusion Distillation-Based 3D Scene Editing.Similarto for instruction-guided generation tasks, another dif-fusion model was proposed for instruction-guided im-age editing, by generating the edited image conditioned onboth the original image and the instruction, which is there-fore compatible with SDS . Instruction 3D-to-3D uses SDS with to support instruction-guided style trans-fer on 3D scenes. Instruct-NeRF2NeRF (IN2N) adoptsanother way to operate the 2D diffusion model, similar tothe rephrased version of SDS in HiFA , which itera- tively generates edited images to update the NeRF datasetfor NeRF fitting, and supports more general editing instruc-tions such as object-specific editing. ViCA-NeRF pro-poses a different pipeline to first edit key views and thenblend key views and apply refinement. Edit-DiffNeRF augments the diffusion model and fine-tunes it with a CLIPloss to improve the success rate of editing.DreamEdi-tor utilizes a fine-tuned variant of instead of and focuses on object-specific editing.Consistency in Distillation-Based Pipelines.When dis-tilling from 2D diffusion to perform 3D generation or edit-ing, 3D awareness and 3D consistency of the generatedimages are crucial, as 3D-inconsistent multi-view imagesare not a valid descriptor of a scene.However, achiev-ing 3D consistency in 2D diffusion is challenging. Earlywork does not alter the diffusion modeland relies on consistency derived from NeRF, by directlytraining NeRF with the inconsistent multi-view images. TheNeRF will then converge to an averaged or smoothed ver-sion of the scene, according to its model capability, whichresults in blurred results with few textures and even failsto generate regular patterns like a plaid or checkered pat-tern. Follow-up work begins to improve the consistency ofthe diffusion and/or the pipeline. ViCA-NeRF achievesconsistency by proposing a different pipeline based on keyviews. ProlificDreamer makes the diffusion 3D-awareby inputting the camera parameter to the diffusion modeland applying per-scene fine-tuning. CSD , IVID ,and ConsistNet propose a joint distillation procedurefor multiple views, aiming to generate or edit multiple im-ages in one batch consistently, through either attention,depth-based warping, or KullbackLeibler divergence.However, these methods all share two major constraints:(1) the noise used for generation is not controlled, thereforea single view may lead to different and inconsistent gener-ation results with different noises; (2) these methods onlystudy and enforce the consistency between images withina single batch. Nevertheless, the full generation or editingprocedure for the scene is across multiple batches, and theremight be inconsistencies in different batches.Our Con-sistDreamer resolves these limitations by proposing novelstructured noise and consistency-enforcing training.",
  ". ConsistDreamer: Methodology": "Our ConsistDreamer is a novel IN2N-like frameworkapplied upon a diffusion-based 2D image editing model .As illustrated in , our pipeline maintains a buffer ofedited views for the NeRF to fit, and uses to generatenew edited images for random views according to the in-struction, the original appearance, and the current NeRFrendering results.Noticing that the NeRF fitting proce-dure and diffusion generation procedure are relatively inde-pendent, we equivalently execute them in parallel. Within . ConsistDreamer framework is an IN2N-like pipeline containing two major procedures. (a) In the NeRF fitting procedure,we continuously train NeRF with a buffer of edited views. (b) In diffusion generation and training, we add our 3D-consistent structurednoise to rendered multi-view images, and compose surrounding views with them, as input to the augmented 3D-aware 2D diffusion .We then add the edited images to the buffer for (a), and apply self-supervised consistency-enforcing training using the consistency-warpedimages. Note: images of structured noise are only for illustration they are actually visually indistinguishable from Gaussian noise images.this framework, we propose (1) structured noise to enablea 3D-consistent denoising step, starting from 3D-consistentnoise at the beginning and ending with 3D-consistent im-ages; (2) surrounding views to construct context-rich com-posed images as input to the 2D diffusion instead of a singleview; and (3) a self-supervised consistency-enforcing train-ing method via consistent warping in surrounding views, toachieve cross-view and cross-batch consistency.",
  ". Structured Noise": "2D diffusion models generate a new image from a noisyimage, which is either pure Gaussian noise or a mixtureof noise and the original image. Prior works like Dream-Fusion and IN2N typically sample different Gaussiannoise in each iteration. However, varying noise leads tohighly different generation results (as shown in the supple-mentary material). In other words, previous methods cannoteven produce consistent (i.e., identical) images for the sameview in different generations, fundamentally limiting theirability to generate consistent results.This observation motivates us to control and manipulatethe noise, by introducing 3D-consistent structured noise.Intuitively, while it is difficult to generate, denoise, or re-store 3D-consistent images from inconsistent random noise,the task becomes more manageable when generating consis-tent images from noise that is itself consistent. Therefore,instead of using independently generated noise in each it-eration, we generate noise on the surface of the scene onlyonce during initialization, and render the noise at each viewto obtain the noise used in generating the image for thatview. Our strategy aligns with 3D diffusion models likeDiffRF , which directly generate noise in 3D space. Thedifference lies in the denoising step: while such work di-rectly denoises in 3D, we distill the 3D denoising process from pre-trained 2D diffusion models.As a latent diffusion model, actually requires noise inlatent space, which is (H/8, W/8, 4) instead of the imageshape (H, W, 3). Each element in this noise latent should beindependently generated from N(0, 1). Constructing such3D-consistent structured noise remains non-trivial: we needto place noise in 3D, project noise into 2D pixels at multiplescales, and ensure correspondence between different views.Additionally, the distribution of each images noise shouldbe Gaussian, as noise in an incorrect or dependent distribu-tion may lead to abnormal generation results (as shown inthe supplementary material).To overcome these challenges, we construct a densepoint cloud of the scene by unprojecting all the pixels inall the views to points, with the depth predicted by NeRF.For each point p, we randomly sample a weighted noisec(p) = (x, w), where x N(0, 1) is an independentlygenerated Gaussian noise, and w U(0, 1) is its weight.To generate the noise at one view, we identify the sub-point cloud that is front-most in this view, and project itonto the plane. For multiple points projected to the samepixel, we aggregate them by selecting the weighted noise(x, w) with the maximum w, and form a noise image I ofshape (H, W) consisting of values in x. As each x is inde-pendently generated and selected (according to w), we haveI N(0, 1)HW , i.e., making I valid 2D Gaussian noise.Given that each pixel in the latent space is only roughlyrelated to its corresponding 8 8 region in the image,we can generate noise in the latent space by operating atthe downsampled resolution of (H/8) (W/8). We thusgenerate different weighted noise {ci(p)} for each of thefour channels of the latent space, and stack the individu-ally rendered noise Ii to construct a Gaussian noise imageof (H/8, W/8, 4), which is then used as the noise by the diffusion model.The structured noise serves as the foundation for 3D-consistent generation. In Sec. 3.3, we introduce a trainingmethod to ensure a consistent denoising procedure from thebeginning to the end, so that the denoised images of differ-ent views at every denoising step is also 3D consistent.",
  ". Surrounding Views": "Using the original view as input is a standard practice, whenemploying 2D diffusion models. This method works well insimple 360 or forward-facing scenes used by IN2N , asa single view covers most objects in the scene. However, inmore complicated scenes like the cluttered rooms in Scan-Net++ , a view may only contain a corner or a bare wallin the room. This hinders the diffusion model to generateplausible results, due to the limited context in a single view.Intriguingly, our investigation reveals that performswell on composed images, generating an image composedof style-consistent edited sub-images with the same struc-ture (as shown in the supplementary). This observation in-spires us to exploit a novel input format for diffusion models surrounding views with a composition of one main viewand many reference views, so that all views collectively pro-vide contextual information. As illustrated in , the keyprinciples in the design of a surrounding view are: (1) themain view that we focus on in this generation should oc-cupy a large proportion; and (2) it should include as manyreference views as possible at a reasonable size to providecontext. In practice, we construct a surrounding view w.r.t.a specific main view, by surrounding a large image of thisview with 4(k 1) small reference images of other views,leaving a margin of arbitrary color. This ensures that themain image is roughly (k 2) times larger than the smallimages. Here k is a hyperparameter. The reference imagesare randomly selected from all the views or nearby viewsfrom the main view, providing both a global picture of thescene and much overlapped content to benefit training.We use such surrounding views as input images to, by constructing the surrounding views of the currentNeRFs rendering results, structured noise, and originalviews.Though not directly trained with this image for-mat, still supports generating edited images in the sameformat, with each image corresponding to the edited resultof the image in the same position. The attention modulesin its UNet implicitly connect the same regions in differ-ent views, enabling the small views to provide extra contextto the main view. This results in consistently edited stylesamong all the sub-images in the surrounding view image.The surrounding views not only provide a context-richinput format for 2D diffusion models, but also allow it togenerate edited results for (4k3) views in one batch, ben-efiting our consistency-enforcing training in Sec. 3.3.",
  ". Consistency-Enforcing Training": "We design consistent per-scene training based on structurednoise and surrounding views, enforcing 2D diffusion to gen-erate 3D consistent images through a consistent denoisingprocedure.Multi-GPU Parallelization Paradigm. Our pipeline in-volves training both NeRF and 2D diffusion. Observingthat training and inferring a diffusion model is consider-ably more time-consuming than training the NeRF, whilethere are very few dependencies between them, we proposea multi-GPU parallelization paradigm. With (n + 1) GPUs,we dedicate GPU0 to continuously and asynchronouslytrain a NeRF on the buffer of edited images. The remain-ing n GPUs are utilized to train the diffusion model andgenerate new edited images added to the buffer for NeRFtraining. At each diffusion training iteration, we allocatea view to each of the n GPUs and train diffusion on themsynchronously. This parallelization eliminates the need toexplicitly trade off between NeRF and diffusion training,leading to a 10 speed-up in training. With multiple dif-fusion generations running synchronously, we can also en-force cross-generation consistency.Augmenting 2D Diffusion with 3D-Informing Control-Net. Intuitively, a 3D-consistent model needs to be 3D-aware; otherwise, it lacks the necessary information andmay solely adapt to the input structured noise, potentiallyleading to overfitting. Therefore, we incorporate an addi-tional ControlNet adaptor into our 2D diffusion, whichinjects 3D information as a new condition. The 3D infor-mation is obtained by using NeRF to infer the depth and3D point for each pixel in the view. We then query its fea-ture in a learnable 3D embedding (implemented as a hashtable in ) to acquire a pixel-wise 3D-aware feature im-age, which serves as the condition for ControlNet. Thesecomponents make the augmented diffusion to be aware of,learn from, and generate results based on 3D information.Additionally, we apply LoRA to further enhance thecapability of diffusion.Self-Supervised Consistency Loss.Lacking groundtruth for consistently edited images, we introduce a self-supervised method to enforce 3D consistency. For a set ofgenerated multi-view images, we construct a correspondingreference set of 3D consistent multi-view images to serveas a self-supervision target. Inspired by , we em-ploy depth-based warping with NeRF-rendered depth to es-tablish pixel correspondence across views.We design aweighted averaging process to aggregate these pixels to thefinal image, ensuring multi-view consistency (detail in sup-plementary).Specifically, we edit n surrounding views synchronouslyon n GPUs, with each surrounding view containing (4k3)views, resulting in a total of V = (4k3)n views. For eachview v, we warp the edited results of the remaining V 1 . Comparison in the Fangzhou scene shows that our ConsistDreamer produces significantly sharper editing results with morefine-grained textures and higher consistency with the instruction, e.g., Lord Voldemort with no hair on which all baselines fail. Theinstructions are the bottom texts, except for NArt , which uses the underlined texts. The images of baselines are taken from their paper. views to it, and compute their weighted average to obtainthe reference view v. We then re-aggregate reference views{v} back into surrounding views in the original structurefor each GPU. These re-assembled surrounding views arethen used as the target images to supervise 2D diffusion.To guide 2D diffusion in preserving the original styleand avoiding smoothing out, we define our consistency lossas the sum of the VGG-based perceptual and stylizationloss , instead of a pixel-wise loss, between diffusionsoutput and the target image. In addition to this primary loss,we propose several regularization losses to prevent modecollapse and promote 3D awareness (detail in supplemen-tary).With the consistency loss, ConsistDreamer effec-tively enforces not only cross-view consistency among allviews in each surrounding view, but also cross-generation orcross-batch consistency for views edited by different GPUs.Consistent Denoising Procedure.With our structurednoise, the denoising in 2D diffusion initiates with consis-tent noise. This leads to a further goal to make the entiredenoising procedure 3D consistent and thus end with con-sistent images. We achieve this by enforcing all the views inthe intermediate denoising images to be also 3D consistentat each denoising step. Therefore, unlike the conventionaldiffusion training with single-step denoising, our traininginvolves a full multi-step denoising procedure with passingthrough gradients. As it is impossible to fit the entire com-putational graph into the GPU memory, we use checkpoint-ing to trade space with time. Doing so enables con-structing the reference set of images with warping for eachintermediate denoising step, which is then used to super-vise the intermediate denoising image. This provides moredirect signals of 3D consistency in the training of diffusion, facilitating the generation of 3D consistent results.Shape Editing. Some instructions, e.g., Make him smile,change the shape or geometry of the scene during editing,while our structured noise and consistency-enforcing train-ing rely on the geometry.To be compatible with shapeediting, we design a coarse-to-fine strategy: we first editthe scene using ConsistDreamer with only the surround-ing view and disabling the other two components, i.e., us-ing image-independent noise and the original implementa-tion of . This allows the scene to converge to a coarseedited shape according to the instruction. We then activatestructured noise and consistency-enforcing training to re-fine the editing. We periodically adjust the structured noisewith changes in geometry, while preserving the noise val-ues. With this strategy, ConsistDreamer also achieves high-fidelity shape editing.",
  ". Experiments": "Editing Tasks.In our setting, each editing task is a pair of(scene, instruction), indicating which instruction-guided editing operation should be applied on which scene.The output of the task is another scene, being the editedscene under the instruction. The scenes we use for evalua-tion contain two parts: (1) IN2N. Scenes used by IN2N ,including scenes of human faces or bodies, outdoor scenes,and statues; and (2) SN++.Scenes in ScanNet++ ,which are complicated indoor scenes with free-formedstructures and camera trajectories. We also use two typesof editing instructions: (1) style transfer which transfers thestyle of the scene into the described style, and (2) object-specific editing which edits a specific object of the scene. We use these tasks to compare our approach with baselines,and conduct ablation study on representative tasks.NeRF Backbone and Diffusion Model.For a fair com-parison with previous works , we use the Nerfactomodel in NeRFStudio as our NeRF backbone, and thepre-trained diffusion model from Hugging Face as ourinitial checkpoint. The NeRF representation for the scene istrained with NeRFStudio in advance, and then used in ourpipeline.ConsistDreamer Variants.We investigate the followingvariants for our ablation study (where SN, SV, and Tdenote removing structured noise, surrounding views, andconsistency-enforcing training, respectively): (1) Full Con-sistDreamer.(2) No structured noise (SN): use inde-pendently generated noise for each view instead of struc-tured noise, but still use surrounding views and performconsistency-enforcing training. (3) No training (T): usesurrounding views and structured noise, but do not aug-ment and train and keep using the original checkpoint.(4) Only surrounding views (SN T): only use surround-ing views, and do not use structured noise or train .(5) IN2N (SN SV T): ours with all the proposedcomponents removed, which can be regarded as an alterna-tive version of IN2N. Note that consistency-enforcing train-ing requires surrounding views to produce sufficient editedviews in one generation; we cannot remove surroundingviews but still apply consistency-enforcing training on .Baselines.We mainly compare our method with twobaselines:Instruct-NeRF2NeRF (IN2N) and ViCA-NeRF (ViCA) , as they are most closely related to ourtask. We also compare with NeRF-Art (NArt) as anearly work. Other methods, however, lack publicly avail-able or working code and/or only use a few scenes sup-ported by NerfStudio. Therefore, we could only comparewith CSD , DreamEditor , GE , EN2N ,and PDS under a few tasks in supplementary, andare unable to compare with Edit-DiffNeRF and In-struct 3D-to-3D .Note that ConsistDreamer solvesinstruction-guided scene editing instead of scene genera-tion, so we do not compare with models for the generationtask .Evaluation Metrics.Observing that our ConsistDreamergenerates significantly sharper editing results, consistentwith previous work , we compare ConsistDreamerwith baselines mainly through qualitative evaluation. Forthe ablation study, the appearance of the scenes edited byour different variants may be visually similar and unableto be fairly compared using qualitative results. Therefore,we propose distillation fidelity score (DFS) to evaluate howfaithful the editing is distilled and applied on NeRF com-pared with the diffusions output , rooted in the basicsetting that we distill from to edit 3D scenes. In thissituation, our editing ability is bounded by s. Consistent",
  "IN2NSN SV T91.01.2255.82.090.91.5222.42.0": ". Ablation study on the distillation fidelity score () quan-titatively validates the effectiveness and complementarity of eachof our components. Our full ConsistDreamer significantly outper-forms all variants across various scenes and types of instructions. with the training objective of DreamFusion , we aim tominimize the distance between two distributions: the distri-bution of a rendered image at a random view from the editedNeRF, and the distribution of the diffusion editing result ofan image at a random view in the original scene. Followingthis, we define the fidelity metric as the Frechet inceptiondistance (FID) between two sets the set of imagesrendered by the edited NeRF at all training views, and theset of edited images generated by the original for alltraining views, corresponding to these two distributions. Alower FID means a higher fidelity that the editing is appliedto the scene.Qualitative Results.The qualitative comparison in theFangzhou scene from the IN2N dataset is shown in .Distilling from the same diffusion model , IN2N ,ViCA , and our ConsistDreamer produce results in a sim-ilar style. As especially shown in the Vincent Van Goghand Edvard Munch editing, our ConsistDreamer gener-ates results containing fine-grained representative texturesof Van Gogh and Munch, while the baseline results areblurred and only contain simple or coarse textures. This val-idates that with our proposed components, ConsistDreameris able to generate consistent images from with detailedtextures, and does not rely on consistency derived fromNeRF, which, unfortunately, smooths out the results. No-tably, in the Lord Voldemort case, our ConsistDreamer isthe only one that successfully edits the image to resemblethe well-known, distinctive appearance of Lord Voldemort,featuring no hair and a peculiar nose. Among all the editingtasks, our ConsistDreamer consistently produces editing re-sults with the most detailed ears and hair/head, and does notcontain unnatural color blocks.Additional qualitative results are shown in , andmore results and the comparison with baselines on thesetasks are provided in the supplementary and on our projectpage. Overall, our ConsistDreamer generates sharp, brightediting results in all tasks across various scenes, includinghuman, indoor, and outdoor scenes. (1) In the Face scene,our ConsistDreamer successfully applies the plaid (check-ered) jacket editing, a common failure case in most previous . ConsistDreamer consistently generates high-quality and high-fidelity editing results, featuring detailed, fine-grained texturesacross various scenes and instructions. Notably, ConsistDreamer also maintains the high diversity from , as exemplified by the highlydiversified results (a)(b). Additional results and comparisons are provided in the supplementary and on our project page.methods, including IN2N. Also, our ConsistDreamer is ableto assign fine-grained marble texture in the marble statueediting, a clear mustache in Mustache editing, and clearwrinkle and hair in Einstein editing, while IN2N producesblurred and over-smooth results with poor details. Notably,our ConsistDreamer minimizes the side effects of the edit-ing, while IN2N unexpectedly and significantly changes theskin color in the Mustache editing and the wall color in theEinstein editing. (2) The Tolkien Elf and Fauvism editingtasks in the Fangzhou scene show that our ConsistDreamercould preserve most diversity from the original , dueto the use of structured noise sampled for the whole edit-ing. With the structured noise, we can focus on the con-sistency of generation for the given noise, without suffer-ing from averaging results generated from different noises,which may lose diversity by converging to an average stylefor all noises. (3) Our ConsistDreamer works well in out-door scenes, as all the details on the floor, mountain, plants,and camps are preserved in the edited results. (4) In compli-cated indoor scenes from the ScanNet++ dataset, our Con-sistDreamer generates editing results that are easy to rec-ognize as the given style, with fine-grained textures (VanGogh), regular patterns (Picasso), or special lighting condi-tions (Bastion and Transistor). All these results validate thatour ConsistDreamer generates high-quality editing results. Ablation Study.As shown in , we conduct the ab-lation study on four representative tasks (A)-(D), coveringinstructions of object-specific editing, artistic style trans-fer, and other style transfer, and scenes of human, indoor,and outdoor scenes. The results show that our full Con- sistDreamer outperforms all the variants with significantgains in all tasks under DFS, which mainly comes fromour consistent denoising procedure in Sec. 3.3 that requiresall three major components to achieve. Training towards aconsistent denoising procedure produces considerable extrasupervision signals to the augmented , making it con-verge better towards consistent generation results. We canalso observe that the consistency-enforcing training and theuse of surrounding views improve the fidelity in most ofthe tasks, especially in the complicated large-scale indoorscenes (C)(D), showing that these components indeed im-prove the consistency in generation.",
  ". Conclusion": "This paper proposes ConsistDreamer, an instruction-guidedscene editing framework that generates 3D consistentlyedited images from 2D diffusion models. Empirical evalua-tion shows that ConsistDreamer produces editing results ofsignificantly higher quality, exhibiting sharper, brighter ap-pearance with fine-grained textures, across various scenesincluding forward-facing human scenes, outdoor scenes,and even large-scale indoor scenes in ScanNet++, where itsucceeds in common failure cases of previous methods. Wehope that our work can serve as a source of inspiration fordistillation-based 3D/4D editing and generation tasks. Acknowledgement. Jun-Kun and Yu-Xiong were supported in part byNSF Grant 2106825 and NIFA Award 2020-67021-32799, using NVIDIAGPUs at NCSA Delta through allocations CIS220014 and CIS230012 fromthe ACCESS program. Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Pe-ter Hedman, Ricardo Martin-Brualla, and Pratul P. Srini-vasan.Mip-NeRF: A multiscale representation for anti-aliasing neural radiance fields. In ICCV, 2021. 3",
  "Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-hall. DreamFusion: Text-to-3D using 2D diffusion. In ICLR,2023. 1, 3, 4, 7": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, GretchenKrueger, and Ilya Sutskever.Learning transferable visualmodels from natural language supervision. In ICML, 2021.3, 12, 13 Elvis Rojas, Albert Njoroge Kahira, Esteban Meneses,Leonardo Bautista-Gomez, and Rosa M. Badia.A studyof checkpointing in large scale training of deep neural net-works. arXiv preprint arXiv:2012.00825, 2020. 6",
  "Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,Jonathan T. Barron, and Pratul P. Srinivasan.Ref-NeRF:Structured view-dependent appearance for neural radiancefields. In CVPR, 2022. 3": "Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He,Dongdong Chen, and Jing Liao. NeRF-Art: Text-driven neu-ral radiance fields stylization. IEEE Transactions on Visual-ization and Computer Graphics, pages 115, 2023. 3, 6, 7 Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srini-vasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. IBRNet:Learning multi-view image-based rendering. In CVPR, 2021.3 Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, ChongxuanLi, Hang Su, and Jun Zhu. ProlificDreamer: High-fidelityand diverse text-to-3D generation with variational score dis-tillation. In NeurIPS, 2023. 3, 17",
  "Tianhan Xu and Tatsuya Harada. Deforming radiance fieldswith cages. In ECCV, 2022. 3": "Xiangzhe Xu, Hongyu Liu, Guanhong Tao, Zhou Xuan, andXiangyu Zhang. Checkpointing and deterministic trainingfor deep learning. In 2022 IEEE/ACM 1st International Con-ference on AI Engineering Software Engineering for AI(CAIN), 2022. 6 Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, HanZhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui.Learning object-compositional neural radiance field for ed-itable scene rendering. In ICCV, 2021. 3",
  "A. Supplementary Video (SV)": "To better visualize our results and compare with base-lines beyond static 2D images, we provide a supple-mentary video (SV) on our project page at immor-talco.github.io/ConsistDreamer.We also include a shortdemo in this video, to enhance the understanding of 3D-consistent structured noise. The original size of the video isaround 1.25GB, therefore we have to compress it to fit it inthe upload size limitation of 200MB on OpenReview.In the following sections, we use SV to refer to this sup-plementary video.",
  "B. Comparisons with Additional Baselines": "In the main paper, we compare our ConsistDreamer withIN2N and ViCA .In this section, we compareour ConsistDreamer with other baselines and provide someanalysis. These methods either do not have publicly avail-able code, or evaluate on the scenes which are not supportedby NeRFStudio.Therefore, we could only compare ourConsistDreamer under the tasks used by them, with the pro-vided visualizations from their papers or websites.We also provide some comparisons in the video formatof the baselines in SV.",
  "B.1. CSD": "CSD is a method focusing on general consistent generation,including large image editing, scene editing, and scene gen-eration. We compare our ConsistDreamer with CSD un-der three tasks shown on the website of CSD1: Low-Poly Figure B.2. Compared with DreamEditor, our ConsistDreamerachieves better editing, which not only follows and satisfies thegiven instructions, but also preserves as much content of the orig-inal scene as possible. On the contrary, DreamEditor completelyedits the original person to another in all the tasks. (Graphic), Anime, and Smile.As shown in Fig. B.1 and SV, our ConsistDreamer sig-nificantly outperforms IN2N, which fails in the Low-Polyand Anime tasks, and has the side effects of adding beardsin the Smile task.Compared with CSD, our editing inthe Low-Poly task is more noticeable, with a successfullyedited hair part. Our edited scene in the Smile task is theonly one among all three to successfully show the teethwhen smiling, while CSDs result contains strange musclesas if the person is keeping a straight face. In conclusion,our ConsistDreamer achieves more successful editing thanCSD.",
  "B.2. DreamEditor": "DreamEditor is another method focusing on scene editing,but with another diffusion model instead of . AsNeRFStudio does not support the other scenes, we compareour ConsistDreamer with DreamEditor by comparing in our main paper with in . Figure B.3. Our ConsistDreamer achieves consistent editing in thecheckered/plaid pattern (also visualized as smooth video in SV),while Edit-DiffNeRF has obvious inconsistency in the shape andtexture of the collar. Fig. B.2 presents the results in these tasks, along withother baselines in in our main paper. It shows thatour ConsistDreamer preserves most of the contents in theoriginal scene while editing, e.g., the shape of the head andface, and the shape and type of the clothes, minimizing theside effects of editing. DreamEditor, however, completelyedits the person to another person, even in the Fauvism task,which is supposed to be only style transfer. This demon-strates that our ConsistDreamer achieves more reasonableediting than DreamEditor.",
  "B.3. Edit-DiffNeRF": "Edit-DiffNeRF is another paper that also claims to suc-cessfully complete the checkered/plaid pattern.As theydid not provide any code, we compare our ConsistDreamerwith the images provided in their paper. As shown in Fig.B.3, our ConsistDreamer achieves consistent editing amongall three views, while Edit-DiffNeRFs results are multi-view inconsistent, obviously shown in the collar part. Thesmooth video of our rendering result in SV also shows theconsistency of our ConsistDreamer. These results validatethat our ConsistDreamer archives significantly better con-sistency in checkered/plaid patterns, while Edit-DiffNeRFfails to achieve such consistency.",
  "Instruct 3D-to-3D is a method focusing on style transfer ofscenes. It uses LLFF and NeRF Synthetic (NS) scenes asediting tasks instead of the widely-used IN2N dataset. In": "contrast, we focus on editing more challenging and realis-tic scenes. In addition, as NeRFStudio and NeRFacto donot support LLFF and NS datasets well (more specifically,NeRFStudio does not support the LLFF dataset, and NeR-Facto works well in real scenes but not in synthetic sceneslike NS), we cannot compare with Instruct 3D-to-3D onthese two datasets. Moreover, the code of Instruct 3D-to-3D is not publicly available. Therefore, we are unable tocompare with Instruct 3D-to-3D.",
  "B.5. Concurrent Works: GE , EN2N , AndPDS": "are three concurrent works. GE and EN2N achieve 3D editing through the same 2D diffusionmodel and have some modifications in the pipeline orscene representation, while PDS proposes another dis-tillation formula and uses DreamBooth for editing.The comparisons against them are in Fig.B.4.OurConsistDreamer generates high-quality editing results withbrighter color and clearer textures, while all these concur-rent works generate blurred textures, gloomy colors, and/orunsuccessful or unreasonable editing.",
  "LoRA-augmented diffusion model : consistent withtheir original implementation (104)": "Learnable 3D positional embedding: 2 103.All the views are resized to 3 : 4 or 4 : 3 accordingto their orientations. For landscape images (portrait imagesuse the same setting with a flipped height and width), thediffusion model takes a surrounding view image input at1152 864 (also 4 : 3), with horizontal splitters at heights6pix, and vertical splitters at widths 8pix. The sizes of themain view and reference views are 688 516 and 224 168, respectively. This setting is consistent with the originalusage of diffusion models trained at 512 512, asour main view has a height close to it.Consistent with IN2N , both MSE and LPIPS lossesare used to train NeRF.",
  "Bootstrap (Epoch 51 150): Train the consistency-awareness of the LoRA-augmented diffusion modelwhile keeping original behavior, at a similar impor-tance with balanced weights": "Warming Up (Epoch 151 200): Use the standardweights to balance the consistency loss and regulariza-tion, focusing more on consistency. This epoch gen-erates sufficient images for the edited view buffer forNeRF fitting. Distillation Stage (Epoch 201 1600): Train diffusionwhile fitting NeRF. In each of 4 epochs, we do 3 diffu-sion generation steps without training (to fill the editedview buffer), and only one diffusion training step. Here,the noise level means the mixture rate of the currentNeRF (being edited) rendered image and the noise as thediffusions input: full noise level means using only noisefor generation (standard generation), while a 30% noiselevel means the input image is the mixture of 30% noiseand 70% rendered image. Full Noise Generation (Epoch 201 500): The diffu-sion model is trained and used for generation at a fullnoise level to edit the views sufficiently regardless ofNeRF. Pre-Annealing (Epoch 501 600): The diffusionmodel is trained and used for generation with a noiselevel sampled from [70%, 100%]. It edits the viewswith a few references to the current NeRF, starting torefine the current NeRF. Annealing (Epoch 601 1500): Following the ideaof HiFA , the range of the noise level linearly an-neals from [70%, 100%] to [10%, 40%]. The NeRF willgradually converge to a fine-grained edited version.",
  "Ending (Epoch 1501 1600): The diffusion model istrained and used for generation with a noise level sam-pled from the annealed range [10%, 40%], to furtherrefine the edited NeRF": "If the editing task requires editing the geometry or shapeof the scene (shape editing), the depth-based warping us-ing the depth of the original scene will be inaccurate. There-fore, in the Initialization Stage, we put the original diffu-sion models output to the edited view buffer for NeRF fit-ting, equivalently using IN2N in this stage. In the distilla-tion stage, the shape of the NeRF will be adjusted to theedited shape in a short time, and then we will start to usethe trained diffusion models output for NeRF fitting.",
  "D.4. Structured Noise Implementation": "In the main paper, the structured noise is implemented byconstructing a dense point cloud of the scene by unproject-ing all the pixels in all the views, and rendering/projectingsuch a point cloud at a view to generate the structurednoise. Directly implementing this literal description is com-plicated and inefficient.Therefore, we use an equivalent implementation.",
  "Instead of explicitly generating this dense point cloud, wejust put the weighted noises on each pixel of all views": "For the view we query for structured noise, we warp thenoise from all other views to it. This is equivalent to pro-jecting the sub-point cloud generated by each view to thequerying view; therefore, it is equivalent to the originaldesign.With this implementation, explicitly generating, main-taining, and projecting a point cloud with billions of points(number of views height width) is unnecessary, and aquery can be completed in less than one second.",
  "D.5. Surrounding Views - Reference View Selection": "We construct the surrounding view with one large mainview and several small reference views. The purpose ofthe reference views is two-folded: (1) to provide enoughcontext about the whole scene, and (2) to have enoughoverlapped parts of the main view to facilitate consistency-enforcing training. Therefore, we select 40% of the views tobe a random view of the scene, and the rest 60% of the viewsto be a view with at least 20% overlap of the main view(quantified by the area of matched pixels through warping).The order of the views is randomly shuffled. We observedthat none of these randomnesses highly alter the editing re-sult after consistency-enforcing training, any choice ofreference views and their order will lead to a consistentedited result of the main view.",
  "D.6. Training - Pixel Weights": "In consistency-enforcing training, we apply warping andweighted averages to compute the training reference views{v}, so that all the views in {v} are 3D consistent. Usingidentical weights for all pixels will result in blurred images:In a scene of a person, one view only contains their face,and another view contains the whole body. Warping the lat-ter to the former indicates an upsampling of the face part,which will be blurred. Merging the blurred, warped viewwith the former view at the same weight results in blurredoverall results.We propose a better pixel-weighting strategy based on afurther analysis of this situation. If we warp pixel a to pixelb, where a has a larger scope and contains more sceneobjects, then we need to upsample the b part of the viewfrom a, resulting in blurry. Therefore, the weight should berelated to the scope of the pixel. Following this, we define the pixel area to quantify this scope. For a pixel p in a viewfrom camera position o, the four vertices of the pixel gridscorrespond to the rays {o+tdi}4i=1. We use NeRF to predicteach of their depth {ti}4i=1, and calculate their correspond-ing points Pi = o+tidi. The pixel area S(p) of this pixel isdefined as the area of a square with vertices P1, P2, P3, P4in the 3D space, which can be regarded as an approximationof the surface area the pixel represents. As we need a lowerweight for a pixel with a larger scope, i.e., larger S(p), wedefine the weight as 1/S(p), which satisfies all our needs.",
  "D.8. Training - Regularizations": "We use the consistency loss as the main loss in theconsistency-enforcing training. However, this loss only en-forces several equalities (required by consistency), leadingto trivial results of a pure-color image without regulariza-tion losses this is also reasonable as all pure-color imagesof the same color are perfectly consistent. Also, there is noencouragement or enforcement to use the 3D information inthe 3D positional embedding. To avoid these, we proposeseveral regularization losses, as shown below: Maintain Original Behavior. We expect that the traineddiffusion model will generate images that are very similarto the original model when all the inputs (image, noises,and 3D positional embeddings) are identical. Therefore,we use MSE and VGG perceptual and stylization losses,to regularize both the generated images and the con-structed referenced images (with gradient, generated bywarping and averaging) of the trained diffusion, with theoriginal models output. We further expect that the UNetin the trained diffusion model predicts similar noises ateach denoising step as the original UNet, so we also usethis to regularize during each denoising step. Encourage 3D Information Utilization. The original takes the original image of the scene as another part ofinput, using it as a condition to generate the edited image.To encourage 3D information utilization, we design a reg-ularization loss, to enforce the diffusion model withoutthe original image input to generate very similar resultsto the one with the original image input (both with 3D po-sitional embedding input). With the lack of the originalimage, the only way for the diffusion model to perceivethe original view is the 3D positional embedding. There-fore, the diffusion is trained to use the 3D positional em-bedding at least for novel view synthesis to recover the",
  "original image, encouraging the utilization of 3D infor-mation. This regularization loss is also applied on theUNet in each denoising step": "Encourage Consistent Editing Style. The diffusion modelhas some diversity in editing. However, we need to con-verge to one specific style in one editing procedure, oth-erwise, the NeRF may use view-dependency to overfitdifferent styles at different views. Therefore, in the Pre-Annealing step (Sec. D.3), we use the NeRFs renderingresult to supervise the diffusion model, to make it con-verge to the style NeRF converges to.",
  "D.9. Variant IN2N And IN2N": "In our ablation study in the main paper, we have a variantIN2N being our full ConsistDreamer with all three majorcomponents removed. In this section, we discuss how itis equivalent to an implementation of IN2N, and the majordifferences between them. IN2N is a method that (1) gradually generates newlyedited images with a noise level (detailed in Sec. D.3) sam-pled from [70%, 98%], and (2) uses the newly generatedimages to fit the NeRF, while the fitting NeRFs renderingresults can affect the following editing (through the inputof diffusion model as a mixture with noise). This matchesour pre-annealing sub-stage. Therefore, IN2N includesvanilla IN2N as a sub-procedure. Additionally, IN2N hasthe following improvements beyond IN2N: IN2N only samples noise levels from [70%, 98%]. Thismakes IN2N (1) sometimes unable to sufficiently edit thescene due to the absence of 100% noise level editing (e.g.,unable to achieve a Lord Voldemort editing with no hairin Fig. B.2), and (2) cannot refine the editing results basedon a converged style, and sometimes even deviates from aconverged style to another, as the noise level is always ashigh as 70%. The variant IN2N starts at a full noise be-fore the pre-annealing sub-stage, guaranteeing sufficientediting. After the pre-annealing sub-stage, IN2N an-neals the noise level range to refine the results, leading toa more fine-grained editing. IN2N adds the newly edited image to the dataset by re-placing a subset of pixels, which may negatively affectthe LPIPS/perceptual loss. IN2N uses an edited viewbuffer to fit NeRF containing only full, edited views, onwhich the perceptual loss can perform well.",
  "In conclusion, our variant, IN2N, is an equivalent and im-proved implementation of IN2N. As shown in SV, IN2Ngenerates noticeably better results than IN2N": "Figure E.1. The pre-trained diffusion model works as expected on surrounding views, by editing each sub-view in the instructed wayindividually but in a consistent style. Notably, as shown in the last row, the surrounding view enriches the context, making the diffusionmodel succeed in views that fail in single-view editing. Figure E.2. Even for the same view, generating from different noises does not necessarily lead to the consistent i.e., the same, edited result.Each column represents a generation from a noise different from other columns.",
  "E.1. Diffusion Models Perform Well with ComposedImages": "As shown in Fig. E.1, the pre-trained diffusion model ,though not directly trained in this pattern, still works as ex-pected in surrounding views. It generates editing results foreach sub-view individually while all of them also share asimilar style, across various scenes, including indoor, out-door, and face-forwarding scenes. Notably, as shown in the last row, when editing a viewwith little context, directly editing the single view fails.Constructing a surrounding view using it as the main view,however, helps the diffusion model to achieve success-ful editing. This shows the effects of surrounding views inachieving successful and consistent editing.",
  "E.2. Different Noises Lead to Varied Results": "As shown in Fig. E.2, generation from different noises leadsto completely different images, which is the fundamentalconstraint of all the baselines, which do not control thenoise. Even with surrounding views, the diffusion model still generates images in highly inconsistent ways. Thediversity of the diffusion model under different noises is de-sirable in 2D generation and editing, but has to be controlledin 3D generation for consistency.",
  "F.2. Only Sur. Views vs. IN2N": "Tasks B,C,D are style transfer, specifically well supportedby our current 2D diffusion model . Our DFS metric,based on FID, uses a feature extractor with more tolerancefor different style transfer results in the same image. Hence,even IN2N performs comparably with a slightly lowerDFS.By contrast, task A is a general object-centric editingwith diversified editing manners different valid editing re-sults can have jackets with completely different colors andstyles. There can even be geometric changes in the cloth-ing without surrounding views as context to constrain theediting, leading to a significantly worse DFS for IN2N.",
  "G.1. Extension to Scene Generation": "The proposed ConsistDreamer primarily focuses on thedistillation-guided 3D scene editing task.However, thecore contributions structured noise, surrounding views,and consistency-enforcing training can also be extended tothe scene generation task. For example, these componentscan be used in the refinement phase, when the shape of ascene is roughly determined. In this way, these componentscould help achieve consistent and high-fidelity generation,refining the shape with slight adjustments for more detailedand precise geometry. Compared with previous methods, this method can generate scenes with detailed,high-fidelity textures and shapes, without mesh exportationor fixing geometry.",
  "G.2. Limitations": "This section discusses the limitations of ConsistDreamer,which are also the common challenges encountered by ex-isting 3D scene editing methods.View-Dependent or Specular Effects.Our Consist-Dreamer pipeline performs consistency-enforcing trainingby warping and averaging between different views. Thisprocedure enforces that each part of the scene looks thesame in different views, i.e., is view-independent, mak-ing the edited scene unlikely to show view-dependent orspecular effects. To preserve the ability to generate view-dependent effects, our ConsistDreamer has introduced aregularization loss that trades off between consistency andsimilarity to original (detailed in Sec. D.8). With thisregularization, our ConsistDreamer could still achieve 3Dconsistency while allowing natural view-dependent effects.The baselines, though are not trained towards consistency or view-independence, only generate blurred results with-out notable effects, or even overfit to inconsistent editingwith the view-dependency of NeRF.Editing Capabilities Constrained by 2D Diffusion Mod-els.Our ConsistDreamer distills from the diffusion model to edit scenes. Therefore, the editing ability, style, anddiversity of ConsistDreamer are inherently constrained by. Our ConsistDreamer edits a scene in a specific man-ner following . For example, in the Vincent Van Goghediting in Fig. B.2, our ConsistDreamer, along with IN2N and ViCA which use the same for editing, showsa side effect that transfers the style of the image to VanGoghs painting style. Moreover, we cannot support edit-ing tasks on which the diffusion model cannot perform.Despite this common constraint among all the distillation-based methods, our ConsistDreamer successfully transfersmost of the editing capabilities of the 2D diffusion model to3D, by achieving high-quality and high-diversity 3D sceneediting.3D Understanding and Reasoning.Though our Consist-Dreamer is 3D-informed and 3D-aware with the additionalinput of 3D positional embedding already surpassing allthe baselines it is unable to reason and understand the se-mantics of each part of 3D scenes. Therefore, while ourConsistDreamer can edit a view using the knowledge of thewhole scenes shape (via 3D positional embedding) and ap-pearance (through the surrounding view), it may still en-counter multi-face issues or Janus problems. Specifically, itdoes not understand what the correct orientation of the faceis, does not know that a person can only have one face, andthus cannot avoid this problem.Shape Editing.Some instructions for editing tasks mayinvolve modifying the geometry or shape of a scene, e.g.,give him a beard creates a beard on the face. Like thebaselines, our ConsistDreamer is designed to support sim-ple shape editing tasks that can be achieved by slightly andgradually alternating the surface. For example, in the edit-ing of give him a beard, our pipeline gradually growsthe beards shape from the faces surface.Notice thatboth ConsistDreamer and the baselines cannot perform ag-gressive and complicated editing (e.g., removing an objectwhile reconstructing the whole occluded part), or direction-related editing (e.g., performing lower down her arm fora scene of a person raising her arm requires a multi-viewconsensus on which direction the arm is moved to).Efficiency.In contrast to the diffusion training-free base-lines such as , our ConsistDreamer needs addi-tional training of a 2D diffusion model. This extends theediting duration, resulting in taking 12 hours to edit a scenein the IN2N dataset, and up to 24 hours to edit a large-scale indoor scene in the ScanNet++ dataset. However, asa trade-off against efficiency, our ConsistDreamer excels inachieving high-fidelity editing, surpassing all the training-",
  "G.3. Future Directions": "Supporting Specular Effects.One direction is to supportspecular effects and better view-dependency. This may needan improved formulation of consistency under specular re-flections, or modeling the ambient environment.3D Understanding for Scene Editing.Another directionis to enable the diffusion model to understand and reasonthe semantics of a scene. Introducing a model that gener-ates 3D semantic embeddings for each point in the scene al-lows for combining this information with the 3D positionalembedding as the input to the diffusion model, potentiallymitigating Janus problems."
}