{
  "Abstract": "Feature selection is crucial for pinpointing relevant fea-tures in high-dimensional datasets, mitigating the curseof dimensionality, and enhancing machine learning perfor-mance. Traditional feature selection methods for classifi-cation use data from all classes to select features for eachclass. This paper explores feature selection methods thatselect features for each class separately, using class modelsbased on low-rank generative methods and introducing asignal-to-noise ratio (SNR) feature selection criterion. Thisnovel approach has theoretical true feature recovery guar-antees under certain assumptions and is shown to outper-form some existing feature selection methods on standardclassification datasets.",
  ". Introduction": "Features are individual measurable properties of what is be-ing studied. The performance of a classifier depends on theinterrelationship between the number of samples and fea-tures used. Although adding more features to the datasetcan improve accuracy, this happens as long as the signaldominates noise. Beyond that point, the model accuracyreduces, and this phenomenon is known as peaking .Also, dealing with high-dimensional data poses challengessuch as increased training time, algorithmic complexity,storage space, and noise in datasets, collectively known asthe curse of dimensionality.Feature selection is a type of dimensionality reductionthat avoids irrelevant data collection, recovers genuine sig-nals with high probability, and provides good prediction re-sults. Feature selection algorithms face several key chal-lenges, including minimizing accuracy loss when selectinga smaller feature set. Simplicity in algorithm design is pre-ferred to reduce overfitting and avoid complex, ad-hoc im-plementations. Additionally, it is beneficial for algorithmsto account for nonlinear feature patterns to capture morecomplex relationships beyond linear associations .In a multi-class classification setup, one can performfeature selection through various approaches: filter meth-ods operate independently of learning algorithms, wrappermethods depend on learning algorithms to iteratively im- prove the quality of selected features, and embedded meth-ods integrate the feature selection phase into the super-vised/unsupervised learning algorithms . Usually,these methods are performed by considering all data col-lectively, without distinguishing between different classes.Therefore, these approaches face scalability issues as thenumber of data points or classes increases, leading to com-putational inefficiencies and potential loss of accuracy.Our work emphasizes individual class modeling, inde-pendently leveraging a generative model and feature selec-tion for each class. This class-specific modeling sets ourmethod apart from existing feature selection techniques forthe following reasons: It captures the unique characteristics and distribution bytailoring the model to each class.",
  "Individual class models can be added independently with-out retraining the entire model, enhancing efficiency andscalability": "Additionally, preserving learned parameters for eachclass mitigates the risk of catastrophic forgetting whennew data is introduced.We propose using the signal-to-noise ratio (SNR) as afeature selection criterion, where the signal represents rele-vant information contributing to accurate predictions, andthe noise refers to irrelevant data.SNR quantifies thestrength of the signal relative to noise, with higher SNR fea-tures being more effective at distinguishing classes. Elimi-nating low SNR features enhances computational efficiencyand model interpretability. In summary, this paper makesthe following contributions: It introduces an SNR-based feature selection method forlatent factor models such as Latent Factor Analysis andProbabilistic PCA.",
  ". Related Work": "The related work can be divided into three different areas,which will be discussed below.Feature Selection Methods for PPCA & LFA. In previousstudies, low-rank generative models, particularly PrincipalComponent Analysis (PCA), have been extensively used forfeature selection and multi-class classification.The incorporation of feature weights in PCA has beenexplored in to emphasize specific facial regions for fa-cial expression recognition. Our approach extends this ideaby using the inverse of the noise covariance as a weight ma-trix to distinguish features with high unexplained variancefrom meaningful signals.Latent factor models have gained significant attention inrecent studies for feature selection. The nonparametric vari-ant of the latent factor model introduced in this paper wasinitially inspired by the Sparse Estimation of Latent Fac-tors (SELF) framework proposed in . SELF utilizes alow-rank (r) latent factor matrix and an orthogonal sparsetransformation matrix W to achieve a sparse representationof the data, X, expressed as WT . While the model doesnot impose specific assumptions on , it requires that Wbe element-wise sparse, satisfying the condition W0 qfor some constant q. Additionally, it imposes the constraintWT 1W = Ir, where is a diagonal matrix containingthe corresponding noise variances along its diagonal. In ourwork, we removed the constraints from W and assumed to be semi-orthogonal,i.e., T = Ir.Latent factors were also employed in to extract influ-ential low-dimensional features, such as blood vessel pat-terns, from retinal images. Mahalanobis distance was usedto classify abnormalities in observations. However, giventhe computational expense of Mahalanobis distance, par-ticularly with a moderate number of features, we proposean alternative based on to compute this distance effi-ciently.Feature Selection For Multi-class Classification. In high-dimensional classification, feature selection is a vital pre-processing step to enhance class separation and reducemodel complexity. Effective feature evaluation directly im-pacts classification accuracy by selecting the most discrim-inative features . One of the popular feature selectiontechniques is Correlation Based Feature Subset Selection(CFSS). This method ranks feature subsets by maximizingrelevance to the target class while minimizing redundancyamong features . However, it cannot assess individual feature relevance to specific classes.Anotherfeatureselectiontechniqueforhigh-dimensional datasets is network pruning.An efficientnetwork pruning method was proposed using an l0 sparsityconstraint in .The imposed constraint allows directspecification of the desired sparsity level and graduallyremoves parameters or filter channels based on the FeatureSelection Annealing(FSA) criteria.Therefore, thenetwork size reduces iteratively, making it applicable tountrained and pre-trained networks.Another efficientpruning technique is the Thresholding-based IterativeSelection Procedure (TISP).This approach al-lows direct control over the sparsity of neural networkparameters using a thresholding function denoted as . Bycarefully selecting , we can impose L0 or L1 sparsityor a combination of both.We compared the efficiencyof feature selection using the FSA and TISP with ourproposed approach based on low-rank generative methods.We recorded the classification accuracy at different levelsof feature selection and presented the results in .Supervised PCA, as introduced by , incorporatesclass labels into the analysis to identify principal compo-nents that capture both high variance and strong class sepa-ration, thereby improving feature discrimination for classi-fication. However, this method requires retraining the entiremodel when a new data class arrives, raising concerns aboutits efficiency and scalability. Instead, our approach modelseach class separately using probabilistic PCA (PPCA) orother generative models, allowing class-incremental train-ing for new data without retraining the whole model. PPCAfor multi-class classification was also used in butwithout feature selection. Moreover, only consideredPPCA, while this paper also studies Latent Factor models,which were observed experimentally to obtain a much bet-ter accuracy on real datasets.Previous works have also combined PCA with classifica-tion methods, such as the PCA-Logistic regression frame-work used by for facial recognition. However, the re-liance on accessing all data for dimension reduction can becomputationally costly. In contrast, our approach leveragesgenerative models to compute the SNR and rank features. Itreduces storage requirements and enhances interpretability.SNR for Feature Selection. SNR is an important measurethat reflects the strength of the desired signal compared tothe background noise. However, its application to featureselection has been quite limited.SNR has been combined with clustering techniques ,probabilistic neural networks and used in some otherMFMW (Multiple filters-multiple wrapper) approaches toselect genes that have been confirmed to be biomarkers as-sociated with various cancer classification problems in thepast. In , SNR was used in the simulation study as aparameter to vary the level of difficulties for a given task.",
  ". Signal to Noise Ratio (SNR) for Feature Se-lection": "This section introduces a feature selection technique thatuses SNR as the feature selection criterion. This methodcan be used for various low-rank generative models suchas Probabilistic PCA and Latent Factor Analysis. First, wedescribe these methods and their parameter estimation pro-cesses. We then use these estimates to calculate the SNRs.3.1. Notations We denote matrices by uppercase bold letters such asM Rpq, vectors with lowercase bold letters such asv Rd and scalars by lowercase letters, e.g.x R.Mij, Mj and Mi represent the (i, j)th element, jth col-umn and ith row of M respectively. vi and v(i) denote ith element and the ith largest value of vector v. m is usedto specify the number of selected features. d and n denotethe number of available features and the number of observa-tions, respectively, for a given class. diag(a1, a2, , ad)denotes a d d diagonal matrix with diagonal elements{a1, a2, ad}. D(M) denotes a diagonal matrix with thesame diagonal elements as M. SNR denotes a vector con-taining the SNR values of all available features.3.2. Low-rank Generative Models In this section, we are going to describe the four differ-ent methods based on low-rank generative models that areincluded in this study, namely Probabilistic PCA (PPCA), Latent Factor Analysis (LFA), HeteroskedasticPCA (HeteroPCA) and Estimation of Latent Factors(ELF). We have introduced the last method in this paper,which is a nonparametric version of LFA.PPCA, LFA, and our newly introduced method, ELF,share the same model structure but have different assump-tions associated with their model parameters. The modelaims to find a relationship between the observed x Rd and a hidden set of variables (latent variables) Rr withr << d and assumes the latent factors and noise variablesare independent of each other. It is as follows:x = +W +,with E() = 0 and var() = . (1)",
  "(3)": "where j is the jth largest eigenvalue and Ur consists ofthe r principal eigenvectors of the sample covariance ma-trix, . The matrix Sr = diag(1, 2, , r), while R isan arbitrary r r orthogonal rotation matrix.Parameter estimation for LFA. LFA parameters (W, )can be estimated using an EM algorithm due to .",
  "where X = (x1 ML, x2 ML, , xn ML)T ,i.e. properly centralized. We use 1": "2 as feature weightsin (6) to reduce the impact of features with significantunexplained noise variance, thereby significantly improv-ing model accuracy. During model training, we estimate:2j = Xj Xj22/(n1) and employ (2j ) 1 2 as jth fea-ture weight for the estimation process. To perform the min-imization in (6), in every iteration, we first estimate ( W, )without the constraint on using Theorem 2 and then ad-just the estimated parameters to satisfy the constraint usingthe Proposition 1 below. The proofs are included in the Sup-plementary Material.",
  "Perform SVD on , UDVT = Update = U and W = WVDUpdate = diag(21, 22, 2d) with2i = var(Xi WTi)Check for convergence: X WT F issufficiently small": "The W produced in (7) does not depend on the featureweights . Algorithm 1 summarizes the iterative estima-tion procedure.HeteroPCA. HeteroPCA addresses the issue of per-forming PCA when the data Xnd has heteroskedasticnoise in a spiked covariance model setup. It assumes thefollowing setup:Xnd = X0 + , E(X0) = , Cov(X0) = 0,(8)",
  "E() = 0, Cov() = = diag(21, 22, , 2d).(9)": "Here, X0 is the noise-free version (signal) of the given datamatrix X, and and X0 are independent. 0 admits a rank-r (<< d) eigen-decomposition, i.e. 0 = UDUT withU Rdr and D Rrr. The goal is to estimate U.Though the model is similar to LFA, the objective of Het-ero PCA aims to capture the principal components (PCs)(known as U) of the signal, accounting for heteroskedas-ticity. Since E( ) = 0 + , there will be a significantdifference between the principal components of E( ) andthose of 0. To cope with the bias on the diagonal of E( ),HeteroPCA iteratively updates the diagonal entries based onthe off-diagonals so that the bias is significantly reduced anda more accurate estimation is achieved.3.3. Estimation of SNRThe estimated signal-to-noise ratio (SNR) for the availablefeatures is computed based on the estimated ( W, ) ob-tained by the different methods. The SNR for the i-th fea-ture is defined as:",
  "2i = ( X0)i Xi22/(n 1).(11)": "The intuition for employing SNRs to identify key featuresin the latent factor model is based on the assumption that thedata originates from a lower-dimensional latent space. Thesignal is represented as W with the assumption T =Ir. The variance of the corresponding signals is capturedby the diagonal elements of WWT or the row sum ofsquares of W. At the same time, the unexplained noisevariance is reflected in the diagonal elements of . There-fore, features with relatively high SNR values are identi-fied as strongly associated with the latent variables, makingthem prime candidates for representing objects within spe-cific categories. Once we estimate the SNRs, we performfeature selection by employing a simple thresholding tech-nique, as described in Algorithm 2.",
  "Sort the SNR values:SNR(1) SNR(2) SNR(d)Selected feature indices are:Im = {i : SNRi SNR(dm+1)}": "3.4. True Feature Recovery GuaranteesEnsuring feature recovery for the proposed SNR-based fea-ture selection criteria is essential. In this section, we provethe convergence of PPCA and LFA model parameters as(n, d) . Specifically, we prove the estimated SNRs(SNR) converge in probability to the true SNRs (SNR)as n with fixed d for both PPCA and LFA. We haveconsidered the following assumptions:",
  "min{SNRi , i S} max{SNRi , i S} +": "When d is kept fixed, the fact that the eigenvectors of thesample covariance matrix are maximum likelihood estima-tors of the population eigenvectors has been proved in ,and their asymptotic distributions have been derived for amultivariate Gaussian data in . For large d, large n( d n(= > 0) as n ), the primary challenge is that thesample covariance matrix becomes a poor estimate. Recentyears have seen the establishment of convergence results forsample eigenvalues and eigenvectors withinthe spiked covariance model, defined in .We now provide the convergence results for the PPCAparameters 2ML obtained from Eq. (3). The result is provedin the Supplementary Material.",
  ")2. If = 0 then 2MLa.s. 2": "Bai et al. have proved the convergence results for( W, ) obtained from (4) using the LFA method. Clas-sical inferential theory suggests that when d is fixed andn , the ML estimates of the model parameters areconsistent and efficient. They have introduced a set of as-sumptions on the true parameters to prove the convergenceresults when (n, d) . Latent factor models are gener-ally non-identifiable without additional constraints. There-fore, additional constraints were introduced in the past liter-ature to ensure full identifiability of the model parameters.However, we only need to focus on the diagonal elementsof W WT to prove the SNR convergence. Therefore, wehave not considered any additional constraints here.Our main SNR convergence and true feature recoveryguarantees are presented in the following theorem, which isproved in the Supplementary Material.",
  "SNRP P CAip SNRi ,for all i {1, 2, . . . , d}": "(C2) Undertheassumption(A1),if=diag(21 , 22 , . . . , 2d ) and ii.i.dN(0, Ir) thenSNRLF Aip SNRi ,for all i {1, 2, . . . , d}.Furthermore, under the assumption (A3), the probabilitythat the m features with the highest SNRs are the true fea-tures converges to 1 as n for both (C1) and (C2).",
  "We apply the proposed feature selection method for multi-class classification. For that, each class is represented as a": "PPCA or LFA model, and the parameters are estimated us-ing one of the four methods described in .2 basedsolely on the data from that class. Then, feature selection isperformed separately for each class using SNR, as describedin Algorithm 2.After selecting the relevant features, the next step in-volves using these models for multi-class classification.Assuming that observations belong to C differentclasses, we will use the Mahalanobis distance:",
  "MD(x, , ) = (x )T 1(x ),(12)": "to compute the distance of an observation to each class andfind the nearest class. In high-dimensional scenarios, theMahalanobis distance is preferred over the Euclidean dis-tance because it considers the covariance structure of thedata, enhancing the classification accuracy.More exactly, to classify an observation x, we performtwo steps:1. Calculate the Mahalanobis distance for every class j {1, 2, , C}: MDj = MD(x(j), j, j), where thevector x(j) contains the values of the selected featuresfor class j, and j and j are the estimated mean andcovariance matrix on the selected features from class j.",
  "SNRii {1, ..., 10},": "i.i.d Uniform(r/1.4, r/0.5)i > 10.Smaller SNRs usually correspond to larger error variances.The true SNRs range from 0.5 (small) to 1.4 (large) to maketrue feature recovery more challenging. The noise variable variances for the irrelevant dimensions are made compara-ble to those of the signal dimensions using the uniform dis-tribution, as specified above.Parameter Estimation Evaluation. We evaluate the es-timation process for the signal: sigi = rj=1 W2ij, i {1, d}, noise variance:=(21, , 2d), andthe SNR within simulated data and using multipledatasets by comparing the estimation error associated with( sig, ,SNR) for the employed low-rank generativemethods. The corresponding true values of the parametersare (sig, , SNR).An example of the estimation errors vs. the number of it-erations for one dataset is shown in (a) and (b), andthe obtained SNRs, along with the true SNRs, are shown in (c). The estimation errors of the LFA method arethe best, given that there are sufficiently many iterations.The performance of ELF and HeteroPCA methods are sim-ilar. ELF exhibits slightly superior performance over Het-eroPCA in estimating the signal (sig), while HeteroPCAoutperforms ELF in estimating . The PPCA method hasthe largest estimation error forsig, and the error for issmaller than the ELF and HeteroPCA methods.(c) displays the estimated SNRs,SNR, along-side the true SNRs, SNR. All methods, except PPCA,estimate close to 0 values for the noisy features (11 110).PPCA often overestimates SNRs for noise, with somesignals having near-zeroSNRs. LFA provides the mostaccurate estimates. ELFsSNR values are close to theSNR but marginally less aligned than LFAs. However,HeteroPCA overestimates positive SNR values the mostbut perfectly captures the pattern.For a more thorough evaluation, we compute the averageof mean absolute deviation (MAD) over R = 50 indepen-dent runs, defined as",
  "Acc = E(|Itrue Ipred|)/|Itrue|,(21)": "where the expected value is computed over 50 indepen-dent runs. We conducted 50 simulations for each n anddnoise combination and recorded the Acc values in .As dnoise increases, identifying relevant features becomesmore challenging across all methods. Adequate data, in-dicated by a larger n, becomes necessary in such scenar-ios. LFA consistently identifies the true features, especiallyfor moderate n values like n = 300, with ELF perform-ing comparably. Both methods surpass the performance ofHeteroPCA and PPCA.",
  ". Real Data Experiments": "We evaluate the proposed feature selection for multi-classclassification methods on two widely utilized popular imageclassification datasets: CIFAR-10 and ImageNet-1k. CIFAR-10 contains 60,000 color images of size 32 32 distributed across ten categories, whereas ImageNet-1kcontains approximately 1.2 million labeled images spreadacross 1,000 categories.We employed CLIP (Contrastive Language-Image Pre-Training) as the feature extractor from the imagedatasets.It is a Convolutional Neural Network (CNN)trained on 400 million image-text pairs sourced from theweb through 500,000 text queries. The image CNN com-ponent of CLIP incorporates an attention mechanism asits final layer before the classification layer. For our pur-poses, we used the pre-trained modified ResNet-50 classi-fier known as RN50x4 from the CLIP GitHub package .The CLIP feature extractor is trained with medium resolu-tion 288 288 images. Therefore, input images were re-sized to 288 288 for ImageNet-1k before processing. Im-ages in CIFAR-10 have been resized to 144 144, as theyare very small and will be blurry when resized to 288288,and showed that the 144 144 input is the best settingfor the CLIP feature extractor for CIFAR-100.Features of dimension d = 640 were extracted after theclassifiers attention pooling layer for Imagenet-1k.ForCIFAR-10, average pooling was employed since imageswere resized to 144 144, obtaining a feature vector withd = 2560.We compare the feature selection efficiency of the pro-posed methods against two popular methods, Feature Se-lection with Annealing (FSA) and TISP with softthresholding (L1 penalty), applied on the same data (fea-tures) as the other methods. FSA and TISP were imple-mented as a fully connected one-layer neural network withcross-entropy loss. The models were trained for 30 epochsusing the Adam optimizer (learning rate: 0.001). presents the classification accuracies in percent-ages achieved by various methods on the CIFAR-10 andImageNet-1k datasets, respectively, for different feature se-",
  "PPCA73.73 73.42 73.16 72.9 72.57 71.84 71.0570": "lection levels. Each column represents the accuracy valuesfor different methods, with the corresponding number of se-lected features indicated at the top.On the CIFAR-10 dataset, classification accuracy im-proves as more features are included, peaking before stabi-lizing at 91% for the low-rank generative methods. Notably,the ELF method reaches its maximum accuracy of 91% us-ing only 1,500 features, achieving a 41% reduction in di-mensionality. In comparison, FSA attains a slightly higheraccuracy of 91.3% with 2,250 features, and TISP achieves91.2% with 2,000 features. Although FSA and TISP offermarginally better peak accuracies, they rely on significantlymore features, making them less effective dimensionalityreduction techniques than the ELF.Similarly, on the ImageNet-1k dataset, the highest accu-racy of 73.73% is observed when utilizing all 640 features.However, the marginal improvement in accuracy diminishesas the number of features increases. Despite this, ELF per-forms exceptionally, achieving 70.24% accuracy with just300 features. The LFA method ranks second, delivering ac-curacies slightly lower than ELF. Even with all the features,the standard linear projection-based classification methodslag on ImageNet. These trends can also be visualized in for both datasets.",
  "TISP 2293 1236 1022 996 877 776 769 737": "octa-core processor with 2.30 GHz base speed. re-ports the training times for FSA and TISP for different prun-ing levels. shows the training times for the proposedmethods, where the SNRs are computed for each class sep-arately, and features can be selected at any level withoutretraining.. Training time (seconds) for low-rank generative methodson the datasets evaluated.",
  "ImageNet64046218 24880": "For CIFAR-10, the low-rank generative methods, ex-cept PPCA, require more time to rank features due to thehigh computational cost of SVD since the feature dimen-sion is large relative to the sample size. For the ImageNet-1k dataset, FSA and TISP consistently require significantlymore time to train the linear model for each feature selectionlevel than the proposed methods SNR computation time.Although FSA, TISP, and PPCA show similar classificationperformance on the CIFAR-10 dataset (), PPCAis more efficient in feature ranking, requiring less time forSNR computation compared to training time needed for theother two methods.",
  ". Conclusion": "This paper introduced a feature selection method for multi-class classification that uses generative models such as la-tent factor analysis to represent each class. It also performsfeature selection separately for each class based on an SNRcriterion. For this reason, this approach can be easily usedfor class incremental learning with feature selection.The paper also provides theoretical true feature recoveryguarantees, which show that the method is not heuristic buttheoretically grounded.Experiments on CIFAR-10 and ImageNet-1k show thatthe proposed feature selection methods outperform standardfeatures for linear models on the same features, such as L1-penalized methods and FSA. However, these standard meth-ods are more memory-demanding, requiring many passesthrough all the data until convergence.We plan to ob-tain better theoretical guarantees through explicit low sam-ple bounds on the true feature recovery rates in the fu-ture.",
  "Debashis Paul.Asymptotics of sample eigenstructure fora large dimensional spiked covariance model.StatisticaSinica, pages 16171642, 2007. 5": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, GretchenKrueger, and Ilya Sutskever. CLIP: Connecting text and im-ages. 2021.Accessed: 2024-05-05. 7 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-vision. In ICML, pages 87488763. PMLR, 2021. 7"
}