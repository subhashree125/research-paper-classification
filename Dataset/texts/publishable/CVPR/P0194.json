{
  "(d)": ". Different from optimization-based 2D-lifting methods such as DreamFusion , DIRECT-3D directly generates 3D contents ina single forward pass (a). To mitigate the lack of high-quality 3D data, DIRECT-3D enables efficient end-to-end training of 3D generativemodels on massive noisy and unaligned in-the-wild 3D assets (b). Once trained, DIRECT-3D can generate high-quality 3D objects withaccurate geometric details and various textures in 12 seconds on a single V100, driven by text prompts (c). DIRECT-3D can also be usedas effective 3D geometry prior that significantly alleviates the Janus problem in 2D-lifting methods (d).",
  "Abstract": "We present DIRECT-3D, a diffusion-based 3D genera-tive model for creating high-quality 3D assets (representedby Neural Radiance Fields) from text prompts. Unlike re-cent 3D generative models that rely on clean and well-aligned 3D data, limiting them to single or few-class gener-ation, our model is directly trained on extensive noisy andunaligned in-the-wild 3D assets, mitigating the key chal-lenge (i.e., data scarcity) in large-scale 3D generation. Inparticular, DIRECT-3D is a tri-plane diffusion model thatintegrates two innovations: 1) A novel learning frameworkwhere noisy data are filtered and aligned automatically dur-ing the training process. Specifically, after an initial warm-up phase using a small set of clean data, an iterative opti-mization is introduced in the diffusion process to explicitly estimate the 3D pose of objects and select beneficial databased on conditional density. 2) An efficient 3D representa-tion that is achieved by disentangling object geometry andcolor features with two separate conditional diffusion mod-els that are optimized hierarchically. Given a prompt input,our model generates high-quality, high-resolution, realis-tic, and complex 3D objects with accurate geometric de-tails in seconds. We achieve state-of-the-art performancein both single-class generation and text-to-3D generation.We also demonstrate that DIRECT-3D can serve as a useful3D geometric prior of objects, for example to alleviate thewell-known Janus problem in 2D-lifting methods such asDreamFusion. The code and models are available for re-search purposes at:",
  ". Introduction": "Diffusion models have achieved significant suc-cess in 2D image synthesis , owing to the largeamount of image-text pairs and scaleable framework. How-ever, applying diffusion models to the 3D domain is chal-lenging, mostly due to the lack of 3D data: Current 3Ddatasets are orders of magnitude smaller than their 2D coun-terparts, and also exhibit significant disparities in qualityand complexity. Specifically, the most widely-used dataset(i.e. ShapeNet ) comprises only 51K 3D models andfocuses on individual objects. Larger datasets like Obja-verse and Objaverse-XL , despite containing over10M objects from Sketchfab, are noisy in quality and lackalignment (i.e., objects in varying poses).As clean andwell-aligned data continue to be very important for currentmethods , people have to rely on high-qualityyet small datasets like ShapeNet for training, and no previ-ous 3D generative model can be directly trained on largerin-the-wild 3D data such as Objaverse. As a result, thesemodels are constrained to single-class generation, and canonly generate objects with limited diversity and complex-ity, such as cars and tables. In addition, the lack of efficientnetwork design poses additional challenges, as there is noconsensus on 3D data representation or network architec-ture that can efficiently handle high-dimensional 3D data.To circumvent the shortage of 3D data and efficient ar-chitectures, one line of work leverages image priorsfrom 2D diffusion models to optimize a Neural RadianceField (NeRF) . However, they are time-consuming andfragile, and often lack of geometric consistency, leading tothe Janus problem (e.g., multiple faces on an animal). Re-cently, one important step was made by Shap-E that di-rectly models the distribution of large-scale 3D objects forimplicit 3D representation generation. However, they do notaddress the aforementioned strict requirement for trainingdata. Instead, they rely on vast amounts of proprietary data,which is time-consuming and costly to obtain, and they stillneed to invest considerable efforts to further enhance dataquality . In addition, Shap-E necessitates multi-stagetraining with a complex recipe, requiring point clouds andRGBA images with per-pixel 3D coordinates as input.In this work, we present DIRECT-3D, a Diffusion modelwith IteRativE optimization for Conditional Text-to-3Dgeneration (). It enables direct training on massivenoisy and unaligned in-the-wild 3D data in an end-to-endmanner, with multi-view images as supervision. Given atext prompt, it generates a variety of high-quality 3D ob-jects (NeRFs) with precise geometric details and diversetextures within seconds. Our model consists of a 2D dif-fusion module to generate tri-plane features and a NeRFdecoder to extract NeRF parameters from the generated tri-plane. Tri-plane features facilitate an efficient 3D represen-tation in well-established 2D networks, and NeRF offers an",
  "effective and compact way to model intricate details of 3Dobjects. To tackle the aforementioned challenges, we madethe following important technical innovations:": "Firstly, we incorporate an iterative optimization processinto the diffusion step to explicitly estimate the pose andquality of the 3D data based on the conditional density ofthe diffusion model, enabling automatic cleaning and align-ment of the data during training. It considerably reducesthe need for high-quality and precisely aligned 3D data andopens up a novel method to efficiently train 3D generativemodels on large amounts of in-the-wild 3D assets. Sec-ondly, we disentangle 3D geometry and 2D color of theobject, modeling them hierarchically with two separate dif-fusion models. The geometry tri-plane is generated first,and the color is generated conditioned on geometry and thetext prompt. This disentanglement enhances the efficiencyand capabilities for modeling 3D data. It also allows formore flexible usage of our model. For example, our geom-etry diffusion module can be seamlessly integrated in ex-isting Score Distillation Sampling based approaches,and provide additional 3D geometry priors, which signifi-cantly improve the geometry consistency while preservingthe high-fidelity texture from the 2D image diffusion mod-els. Finally, we propose an automated method to generatemultiple descriptive prompts for each object, spanning fromcoarse to fine-grained levels, which enhances the alignmentbetween prompt features and the generated 3D objects. We evaluate DIRECT-3D on both single-class generationand text-to-3D generation. For single-class generation, ourmethod outperforms all previous methods on all tested cat-egories by a large margin when trained on exactly the samedata (e.g., from 14.27 to 7.26 in FID), proving our effective-ness in modeling 3D data. For text-to-3D generation, weachieve superior performances compared to previous work(Shap-E ), excelling in quality, detail, complexity, andrealism. User studies show that 73.9% of raters prefer ourapproach over Shap-E. In addition, when used as geometryprior, our method significantly improves the 3D consistencyof previous 2D-lifting models (e.g. DreamFusion ), andraises the generation success rate from 12% to 84%.",
  "with SR plug-in": ". Method overview. Given a prompt, we generate a NeRF with two modules: The disentangled tri-plane diffusion module uses2 (or 4 if the super-resolution plug-in is used) diffusion models to generate geometry (fg) and color (fc) tri-plane separately. Then bothtri-planes are reshaped and fed into a NeRF auto-decoder to get the final outputs. During training, an iterative optimization process isintroduced in the geometry diffusion to explicitly model the pose of objects and select beneficial ones, enabling efficient training on noisyin-the-wild data. The whole model is end-to-end trainable (with or without SR plug-in), with only multi-view 2D images as supervision.",
  ". Related Work": "Direct 3D generation.Early work relies on eitherGAN or VAE to model the distribution of 3D ob-jects, represented by voxel grids , point clouds , or implicit representations . Re-cently, diffusion models have been utilized to cre-ate objects with appearance or puregeometric shapes . However,these methods are constrained by their reliance on clean andwell-aligned 3D datasets such as ShapeNet . Hence,they can only focus on a single category or a few categories.Recently, Cao et al. train a class-conditional 3D dif-fusion model on OmniObject3D , which contains 216object categories, enabling large-vocabulary 3D generation.However, their need for well-aligned 3D data limits theirtraining set to just 5.9K objects, averaging only 27 objectsper category, which severely restricts the quality and diver-sity. To enable large-scale 3D generation, Point-E andShap-E train text-conditional diffusion models on mas-sive proprietary data. However, acquiring such data is costlyand time-consuming, and large efforts are still required tofurther enhance the data quality . In contrast, we di-rectly tackle this key constraint on training data by enablingdirect training on extensive in-the-wild 3D data, which iscost-effective and easy to scale up.Text-to-3D generation with 2D diffusion.To circum-vent the constraints imposed by limited 3D data and enablelarge-scale generation, another line of work leverages pre-trained 2D image diffusion pri-ors for 3D generation. However, they are known for suffer-ing from the Janus problem, in which radially asymmetricobjects exhibit unintended symmetries, due to the lack of3D consistency in 2D diffusion models. MV-Dream mitigates this issue by fine-tuning a pre-trained image diffu-sion model to produce multi-view images, highlighting the importance of 3D knowledge. In contrast, we directly gen-erate objects in 3D space with accurate geometry informa-tion. Moreover, our method provides accurate 3d geometrypriors to these 2D-based methods, complementing the 2Dpriors from image diffusion models, and hence effectivelyalleviating the Janus problem. In addition, these methodsrequire tens of minutes to hours for optimizing a single ob-ject, whereas our method generates NeRFs in seconds.",
  ". Method": "Our model consists of a tri-plane diffusion module to gener-ate tri-planes of a 3D object, and a NeRF auto-decoder to decode the tri-planes into final radiance field. In Sec. 3.1,we introduce our architecture design. Sec. 3.2 describeshow we can train our model on noisy and unaligned 3D data.In Sec. 3.3, we introduce the 3D super-resolution plug-in forhigh-resolution generation. Sec. 3.4 describes an automatedway to generate descriptive captions in different granulari-ties. Training and implementation details are available inthe Supp. An overall illustration is provided in .",
  ". Tri-plane Diffusion for NeRF Generation": "NeRF generation from disentangled tri-plane represen-tation. Given a set of 2D multi-view images of a subject,one can learn its 3D representation with a NeRF, whichmodels the subject using volume density R+ and RGBcolor c R3+. For a more efficient representation, we fol-low previous work that uses the tri-plane represen-tation to model the NeRFs. Specifically, it factorizes a 3Dvolume into three axis-aligned orthogonal 2D feature planesfxy, fxz, fyz RNNC. Then, one can query the featuref of any 3D point p R3 by projecting it onto each of thethree planes and aggregating the retrieved features.However, we find it necessary to disentangle the geome-try and color features into two separate tri-planes, denoted by fg and fc respectively, which improves model capabil-ity and provides important geometry prior (see Sec. 4.4.2).Then, with the tri-planes fg and fc, and a set of rays {ri}, wecan get the integral radiance y of this subject with an auto-decoder: yi = R(D(fg, fc, ri)), where D is a multi-layerperceptron decoder with parameters , R denotes volumerendering , and i is the ray index. Our decoder pro-cesses the tri-planes fg and fc separately to generate densityand color, thereby ensuring that fg only encapsulates the ge-ometry information and fc only contains the correspondingcolor features (see Supp. for details). Given the ground-truth pixel RGB y, the tri-planes fg, fc and parameters can be optimized by minimizing the rendering loss:",
  "i||yi R(D(fg, fc, ri))||22(1)": "Disentangled tri-plane generation. For conditional gener-ation of tri-plane f() from prompt p, we adopt a 2D latentdiffusion model . In our framework, the diffusionmodel denoises tri-plane features fg, fc RNN3C thatstack the channels of all three axes into a single image.Given an input tri-plane f 0g (or f 0c ), the diffusion modelprogressively adds noise to it and produces a noisy outputf tg := tf 0g + t at timestep t, where N(0, I) is theadded Gaussian noise, t and t are noise schedule func-tions. During each training step, we first train a geometrydenoising network (f tg, t, (p)) via",
  "Lcol() = Ef 0c ,,p,fg,t[|| (f tc, t, (p), fg)||22](3)": "Prompt condition is added by a cross-attention mecha-nism with classifier-free guidance , and geometrycondition for color diffusion is added via concatenation.During inference, the geometry tri-plane f 0g is sampledstarting from the Gaussian noise f Tg N(0, I) conditionedon prompt p, then the color tri-plane f 0c is sampled similarlybut conditioned on prompt p and geometry f 0g .",
  ". Training with Noisy and Unaligned Data": "Beyond our disentangled architecture and the introducedtraining objective, large-scale text-to-3D synthesis requiresa substantial amount of 3D data for training. Recent ef-forts have gathered over 10M in-the-wild 3Dobjects from Sketchfab. However, these datasets are dif-ficult to use due to the heterogeneous quality and datasources, and the lack of alignment, leading to poor per-formance or even non-convergence during training (see Sec. 4.4.1). Manual cleaning and alignment of 10M data istime-consuming and impractical to scale up. To this end, weintroduce an iterative optimization process within the diffu-sion training step to autonomously identify noisy 3D dataand automatically align clean data samples during training.To achieve this goal, for each object, we explicitly modelits 3D rotation angle as = {, }, where , R3 denote the estimated mean and variance of its 3D rotationangle. Once estimated, the rotation angle can be sampledfrom N(, ). Note that the geometry tri-plane fg is nowconditioned on the rotation , so Eqn. 2 becomes",
  "Lgeo(, ) = Ef 0g ;,,p,t[|| (f tg; , t, (p))||22](4)": "Then, we can estimate the rotation parameter by also min-imizing the diffusion loss Lgeo(, ).However, directlyminimizing it w.r.t is challenging, since our model onlyuses multi-view images as supervision, and the tri-plane re-construction already requires hundreds of optimization iter-ations per object (although effectively). Note that we do notneed an accurate estimate of ; instead, a rough pose withgood axis disentanglement in tri-plane suffices (see ).To perform this estimation, we consider as a hiddenvariable and propose an iterative optimization process. Wefirst initialize the model with a very short warm-up phaseon a small aligned dataset (details in Supp.). Subsequently,during each training iteration on the entire noisy dataset, wesample m different following N(, ) and estimate thecorresponding tri-planes f 0g . Then with a frozen geometrydiffusion model, we compute the loss in Eqn. 4 with fixedparameter at a fixed time step t, which gives us a lossdistribution w.r.t the sampled rotations . After that, we canupdate the rotation parameter by (1)+minand | min|, where min is the sampled rota-tion with the smallest loss and () are momentum parame-ters. Finally, given threshold T, we can use min to updatethe geometry denoising network if Lgeo(, min) T.We initialize and with all elements equal to 0 and, respectively. Then we set m = ceil(36/ ), whichis updated every iteration. In practice, it converges fast, of-ten requiring just 5-10 iterations. We filter out the objectsthat do not converge after 10 iterations. This step does notrequire back-propagation through the diffusion model whenoptimizing , which also speeds up the process.",
  ". 3D Super Resolution": "Similar to the structure of the base tri-plane diffusion model,the 3D super-resolution (SR) module also employs a U-Netmodel as its backbone. However, we apply only one up-sampling layer that directly scales the tri-plane feature from1282 to 5122. To enable efficient training with a larger batchsize, we train the SR module separately. Therefore, we candirectly use the saved tri-plane features during the trainingof the base model to train the SR module. Following cas-caded image generation , we add Gaussian blurring andGaussian noises to the intermediate tri-plane feature f ().",
  ". Coarse to Fine-gained Caption Generation": "Text prompts play a crucial role in large-scale generation,but datasets like Objaverse only contain paired metadatathat do not serve as informative captions. To solve this prob-lem, Cap3D proposed to use LLM to consolidate cap-tions generated from multiple views of a 3D object. We fol-low their pipeline to generate captions for all training exam-ples. However, we found that these captions may be overlydetailed and contain irrelevant objects, making it difficult totrain a model from scratch. In addition, considering the lim-ited availability of 3D data, we find that caption enrichmentwith different granularities is an effective and cost-efficientmanner to scale up the training set.To generate more accurate captions with multiple gran-ularities, we begin by rendering 8 images at 5122 from dif-ferent camera angles for each object. Next, a pretrainedDeiT on ImageNet-1K is used to classify the ob-ject in each image and output object proposals based on thetop-5 confidence scores. After that, we use BLIP2 andLLaVA for captioning through a two-stage question-answering process.In the first stage, they are tasked toidentify the object in the image. Then we compare the iden-tified object with the object proposals using the CLIP simi-larity, and eliminate irrelevant objects. In the second stage,for each image, the top-ranked matched answer is passed tothe vision-language models for (1) assigning a title to thisobject, and providing descriptions of the objects (2) colorand texture, and (3) structure and geometry. 5 answers aregenerated for each question. Then we adopt the caption se-lection and consolidation from Cap3D to get the finalcaptions. We retain four captions per object, which corre-spond to (1) the object category, (2) the generated title, andthe descriptions focusing on (3) texture and (4) geometry.Finally, we use the category and title information to furthereliminate the irrelevant objects in descriptions (3) and (4).These captions are selected randomly during training.",
  "Ours6.901.847.012.127.261.89": ". Single-class generation on SRN Cars, PS Chairs, andABO Tables. Baseline results are reported by DiffRF and SSD-NeRF. We train our model from scratch using exactly the samerendered images as the baselines. KID is multiplied by 103. method can function as a critical object-level 3D geometryprior, significantly improving previous optimization-basedtext-to-3D models (Sec. 4.3). Finally, we prove the effec-tiveness of our main ingredients in ablation (Sec. 4.4). Ad-ditional experimental results are provided in the Supp.Datasets. We warm up our model on OmniObject3D and a split of ShapeNet , which contain 6342 objectsspanning 216 categories. Then we train our full model onObjaverse that contains 800K+ objects.1 For single-class generation, we strictly follow the previous meth-ods and conduct experiment on ShapeNet SRNCars , Amazon Berkeley Objects (ABO) Tables ,and PhotoShape (PS) Chairs . For Chairs, we generateimages following the render pipeline in DiffRF . ForCars and Tables, we directly use the rendered images in SS-DNeRF for both training and testing.",
  ". Single-class 3D Generation": "We reduce our model size to 135M parameters for a faircomparison with SSDNeRF (122M). We also removethe prompt condition and train a separate model on eachcategory following the baselines.Hyperparameters. All models are trained for 500K itera-tions on 8 A100 GPUs, utilizing a batch size of 64. No SRplug-in is trained during these experiments. For cars andtables, the latent base learning rate is set to 4e2. In thecase of chairs, the latent base learning rate is set to 5e3.The remaining hyperparameters align with those specifiedin direct text-to-3D generation.",
  "We did not use Objaverse-XL since the data were not public avail-able when this project was conducted": ". Qualitative comparison with Shap-E . We use the same text prompt as in Shap-E (top 2 rows) and DreamFusion (middle 2rows), we also compare the performance on complex objects (last row). For Shap-E, we use the official code and model. For our method,we generate objects in 1283 without the super-resolution plug-in. All images of both methods are rendered at 2562. Our DIRECT-3Dgenerates 3D objects with enhanced quality in both geometry and texture. We also generate more various and complex objects.",
  ". User preference studies. We conduct user studies on475 prompts, including all prompts from Shap-E and 162 promptsfrom DreamFusion. 73.9% of users prefer ours over Shape-E": "prompts in the official paper and website of Shap-E and 162prompts from DreamFusion gallery. 2 Qualitative results areprovided in . Our model is able to generate morevarious and complex objects with much higher qualityin both geometric details and textures. More results canbe found in the Supp.Following Magic3D , we also conduct user studiesto evaluate different methods based on user preferences onAmazon MTurk. For each generated object, we render avideo recording its rotation along the z-axis, covering a full360-degree view.Then we show users two side-by-side videos generated by two algorithms, both using the sameinput prompt. We randomly switch the order of these twovideos for different prompts. Users are instructed to evalu-ate which video is (1) more realistic, (2) more detailed, and(3) which one they prefer overall. Each prompt is evaluatedby 3 different users, yielding a total of 1425 comparison re-sults. As shown in Tab. 2, we generate more realistic anddetailed objects, leading to higher user preference.",
  ". Improving 2D-lifting Methods with 3D Prior": "Recent 2D-lifting text-to-3D methods have demon-strated impressive visual quality and compositionality us-ing pretrained 2D text-to-image diffusion models as imageprior.However, they suffer from the multi-face (Janus)problem. Here we show that plugging DIRECT-3D into the2D-lifting framework as a 3D prior greatly alleviates theJanus problem and improves the geometry consistency.We use an open-source implementation of DreamFu-sion using StableDiffusion v2.1 (DreamFusion-SD) or DeepFloyd (DreamFusion-IF) as the 2D image",
  ". Improving 2D-lifting text-to-3D generation. DIRECT-3D provides a useful 3D geometry prior, enhancing the geometryconsistency and increasing the generation success rate": "prior. Our 3D prior is implemented as a Score DistillationSampling (SDS) loss added to the original text-to-3Dloss. As the Janus problem only happens on radially asym-metric objects like animals, we concentrate our quantitativeexperiments on animals. We conducted 50 trials using theprompt A DSLR photo of a [animal], with [animal]randomly sampled from a list of 14 animal types.Theprompt for DIRECT-3D is set to A [animal]. Only gen-erations with both correct geometry and texture are countedas success. The detailed criterion is described in the Supp.As shown in Tab. 3, adding DIRECT-3D as 3D priorgreatly improves the success rate of text-to-3D genera-tion, alleviating the multi-face problem.We also show qualitative comparisons in Fig 4.Ourmethod provides important geometry prior that greatly im-proves the generation success rate and the geometry con-sistency of the baseline method. In addition, we find that",
  "Ablation of Automatic Alignment and Cleaning": "We show the effectiveness of the Automatic Alignment andCleaning (AAC) in Tab. 4, , and . For quanti-tative evaluation, we randomly rotated the aligned objectsin SRN Cars, ABO Tables, and PS Chairs, and evaluate themodels on their test set. Results are provided in Tab. 4. Forvisualization, we select cars and chairs from the Objaversedataset based on their assigned category title, and directlytrain our model on them. We visualize the learned tri-plane .Tri-plane feature learned with/without AutomaticAlignment and Cleaning (AAC) on Objaverse. It roughly alignsthe objects to get clear tri-plane features. Unaligned objects can becaptured by tri-plane representation, but the inadequate axis disen-tanglement makes it challenging for the diffusion model to learn.",
  "Ablation of Disentanglement": "Tab. 5 highlights the enhancements achieved through disen-tanglement. For models without disentanglement, we dou-ble the number of layers to maintain similar model parame-ters. Disentanglement greatly improves model capabilities,establishing the foundation for large-scale generation.More importantly, it provides pure geometry priors forvarious tasks.Considering 2D-lifting text-to-3D genera-tion, shows that when geometry and color are not dis-entangled, using our model as a geometry prior also affectsthe texture (i.e., harms the image feature prior learned from2D diffusion models). However, with disentanglement, weare able to provide critical geometry priors while preservingthe high-fidelity texture from 2D image diffusion models.In addition, with better geometry consistency, the textures . Prompt Enrichment. FID and KID are computed onthe entire test set. We provide captions with varying granularities:Coarse captions enhance object-category connections, simplifyingthe training, while fine-gained captions enable a better understand-ing of detailed features such as color and part-level information.",
  "Ablation of Prompt Enrichment": "compares the performance variance when the modelis trained with different prompts. Class name means cap-tion with template A 3D mesh of a [Class].Class name gives a better performance on FID and KIDscores (reported in the figure). It simplifies the probleminto a class-conditional multi-class generation task, ensur-ing higher quality in the generated object. However, train-ing only with class names leads to a lack of basic under-standing regarding detailed attributes.Cap3D prompt contains finer details, yet can be overly in-tricate and occasionally contains irrelevant objects or evenincorrect captions due to the failure of BLIP2 on syntheticobjects. Directly training on them is more challenging, re-sulting in reduced quality and lower FID/KID scores.Our prompt enrichment provides 4 different prompts foreach object under different granularities. It ensures high-quality generation while offering better control over details.",
  ". Conclusion": "We have presented DIRECT-3D, a diffusion-based text-to-3D generation model that is directly trained on extensivenoisy and unaligned in-the-wild 3D assets.Given textprompts, DIRECT-3D can generate high-quality 3D ob-jects with precise geometric details in seconds. It also pro-vides important and easy-to-use 3D geometry priors, com-plementing 2D priors provided by image diffusion models.",
  "Supplementary Material": "In this supplementary document, we provide details andextended experimental results omitted from the main paperfor brevity. Specifically, Sec. 6.1 provides details of theNeRF Auto-decoder. Sec. 6.2 provides details of the 3DSuper-Resolution module. Then, we cover the training de-tails in Sec.6.3, including loss functions and warm-up train-ing on clean data. Sec. 7 presents experiment details andhyperparameters. Sec. 8 gives additional ablation studiesand more qualitative results. Finally, the limitations of ourmethod are discussed in Sec. 9.In addition, we provide video results for all visualiza-tions in the supplementary file.",
  ". NeRF Auto-decoder": "We employ a NeRF Auto-decoder to extract features fromthe generated tri-planes and get NeRF parameters.Thisauto-decoder consists of several multi-layer perceptrons toprocess the tri-plane features fg and fc separately. illustrates its architecture, which contains several fully con-nected layers with non-linear activation functions. The de-coding process involves two distinct branches to handle thetri-plane features separately, ensuring that fg encapsulatesonly the geometry information and fc contains only the cor-responding color features.",
  "+ radLrad(fg, fc, )": "To speed up the convergence of tri-planes learned frommulti-view images (i.e., Lrad(fg, fc, )), we adopt priorgradient caching and save the diffusion gradientsfgLgeo and fcLcol for re-using to update the tri-plane.It enables us to update Lrad(fg, fc, ) multiple times in onetraining iteration.Then we freeze the base tri-plane diffusion module andonly train the SR module to get high-resolution generationsat 5122, with the following objective:",
  "+ radLrad(fg, fc, ) + entropyLentropy": "In this step, we load, resize, and fine-tune the tri-plane fea-tures saved during the training of the base diffusion module.We use bilinear interpolation to scale the saved tri-planesfrom 1282 to 5122.Warm-up training. Training the entire system is challeng-ing due to the intricate interdependencies between differentmodules. Specifically, optimizing diffusion model is less ef-fective when tri-plane f() in Eqn. 1 is far from convergence,but learning fg with rotation needs a reasonably function-ing diffusion model. Therefore, we warm up the model onclean and well-aligned data for the first 1/50 of the total it-erations. It also defines a universal canonical pose for allobjects. After that, we continue the training on all datasetswith a learnable rotation parameter using the algorithmdescribed in Sec. 3.2.",
  ". Direct Text-to-3D Generation": "We warm up our model on OmniObject3D and a splitof ShapeNet , which contain 6342 objects spanning 216categories. Then we train our full model on Objaverse that contains 800K+ objects.Hyperparameters. We first train our base model for 2Miterations with a batch size of 256. Then the SR moduleis trained for 500K iterations with a batch size of 32. Bothmodule are trained on 32 A100 GPUs. We set the number ofchannels for tri-plane features C = 6, and train a diffusionmodel with 1000 diffusion steps with linear noise scheduleto generate the tri-plane features. During inference we sam-ple 50 diffusion steps. The latent base learning rate is 1e2 for all experiments. The learning rates for both geometryand color diffusion models are set to 1e4, and the learningrate for NeRF auto-decoder is set to 1e3. geo = col = 5,rad = 20, and entropy = 0.1. We update the tri-plane re-constructions from multi-view images 16 times per iterationfor the initial 200K training iterations, and once per iterationfor the subsequent training iterations. The latent base learn-ing rate is reduced by a factor of 0.5 after 500K iterationsand by a factor of 0.1 after 1M iterations.",
  ". Improving DreamFusion with 3D Prior": "In our experiments, we sample [animal] from 14 ani-mal types: bear, corgi, dog, bird, cat, pig, elephant, horse,sheep, zebra, squirrel, chimpanzee, tiger, lion.Criterion for successful generation. We consider a text-to-3D generation successful when both the generated geom-etry and texture are consistent. Consistent geometry impliesthe correct number of parts is generated without missing orextra ones. Consistent texture implies the generated texturecontains a consistent and plausible pattern that may appearon an actual animal of that type, regardless of the geometry.Hyperparameters. For DreamFusion and DIRECT-3D, werun 10K iterations of optimization using the Adam opti-mizer with a learning rate of 5 103. Perp-Neg is enabled for the 2D diffusion guidance with wneg = 4",
  ". Comparison of generated objects with and withoutthe 3D super-resolution plug-in. Please zoom in for better visu-alization": "for all methods, which we found useful to reduce incorrecttextures such as multiple head textures. We set the weightof the 3D prior SDS loss provided by DIRECT-3D to 0.01.The classifier-free guidance is set to 100 as suggested inDreamFusion . We use a coarse-to-fine training processfor all methods, starting from a spatial resolution of 642 forthe first 5K iterations and increasing to 1282 afterward. Theremaining hyperparameters are set to the default values.",
  ". Ablation on the Super-resolution Module": "We employ an additional 3D super-resolution plug-in toenhance the resolution from 1283 to 2563. com-pares the generated objects with and without the SRplug-in, demonstrating its effectiveness in producing high-resolution objects with reduced computational resources.However, its worth noting that the SR plug-in may slightlyalter the generated low-resolution objects and introduce ad-ditional noise.",
  ". Limitations": "While DIRECT-3D consistently produces high-quality re-sults and surpasses previous methods in single-class 3Dgeneration and direct text-to-3D synthesis, it does exhibitcertain limitations. First of all, despite the abundant ge-ometry information provided by large-scale 3D datasets, asignificant proportion of them lacks realistic textures. Addi-tionally, the synthetic-to-real gap still persists, even for ob-jects with nice and detailed textures. Therefore, training a3D generative model, such as DIRECT-3D, solely on theseextensive 3D datasets may result in a lack of appearance in-formation for specific objects. One potential solution is tofurther fine-tune our color diffusion model on real images,which we leave for future exploration.Secondly, the current model demonstrates limitations incompositionality. Although DIRECT-3D can generate mul-tiple objects with close relations, such as a house with agarden, it struggles to generate novel combinations likean astronaut riding a horse. This issue is also observedin previous methods such as Shap-E . We attribute thislimitation to two main factors: (1) The scarcity of multipleobjects in a single CAD model contributes to the difficultyof generating diverse objects within one tri-plane. Unlike2D images, where multiple objects are commonly present,most 3D CAD models consist of either a single object ortwo or three highly related objects. (2) Current 3D datasetsare still orders of magnitude smaller than their 2D coun-terparts, resulting in insufficient training data to effectivelylearn novel compositionality.",
  "models for 3d point clouds. In International conference onmachine learning, pages 4049. PMLR, 2018. 3": "Titas Anciukevicius, Zexiang Xu, Matthew Fisher, Paul Hen-derson, Hakan Bilen, Niloy J Mitra, and Paul Guerrero. Ren-derdiffusion: Image diffusion for 3d reconstruction, inpaint-ing and generation. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1260812618, 2023. 3 MohammadrezaArmandpour,HuangjieZheng,AliSadeghian, Amir Sadeghian, and Mingyuan Zhou.Re-imagine the negative prompt algorithm:Transform 2ddiffusion into 3d, alleviate janus problem and beyond. arXivpreprint arXiv:2304.04968, 2023. 2 Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-imagediffusion models with an ensemble of expert denoisers. arXivpreprint arXiv:2211.01324, 2022. 2",
  "Ziang Cao, Fangzhou Hong, Tong Wu, Liang Pan, and ZiweiLiu. Large-vocabulary 3d diffusion model with transformer.arXiv preprint arXiv:2309.07920, 2023. 3": "Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,and Gordon Wetzstein. pi-gan: Periodic implicit generativeadversarial networks for 3d-aware image synthesis. In Pro-ceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 57995809, 2021. 5 Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas JGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficientgeometry-aware 3d generative adversarial networks. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1612316133, 2022. 2, 5 Angel X Chang, Thomas Funkhouser, Leonidas Guibas,Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,Manolis Savva, Shuran Song, Hao Su, et al.Shapenet:An information-rich 3d model repository.arXiv preprintarXiv:1512.03012, 2015. 2, 3, 5 Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, ZhuowenTu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf:A unified approach to 3d generation and reconstruction. InInternational Conference on Computer Vision, 2023. 2, 3, 5,1, 4",
  ". Qualitative comparison on ShapeNet SRN Cars. Baseline results come from the original paper of SSDNeRF . Followingthe baseline methods, we generate and render images at 1282": ". Qualitative comparison with Shap-E . All text prompts are sourced from the original paper of Shap-E. For Shap-E, weuse the official code and model with the default random seed. For our method, we generate objects in 1282 without the super-resolutionplug-in. All images of both methods are rendered at 2562. Our DIRECT-3D generates 3D objects with enhanced quality in both geometryand texture.",
  "Ours + DreamFusionDreamFusion": ". More qualitative results on using DIRECT-3D as a 3D prior for 2D-lifting methods. Our 3D prior alleviates issues such asmultiple faces and missing/extra limbs, while also improving texture quality. Please also check the video demos for a better visualization. Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexan-der G Schwing, and Liang-Yan Gui. Sdfusion: Multimodal3d shape completion, reconstruction, and generation. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 44564465, 2023. 3 Jasmine Collins, Shubham Goel, Kenan Deng, Achlesh-war Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, TomasF Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al.Abo: Dataset and benchmarks for real-world 3d object un-derstanding. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 2112621136, 2022. 5",
  "tian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al.Objaverse-xl: A universe of 10m+ 3d objects. arXiv preprintarXiv:2307.05663, 2023. 2, 4, 5": "Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,Oscar Michel, Eli VanderBilt, Ludwig Schmidt, KianaEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:A universe of annotated 3d objects.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1314213153, 2023. 2, 4, 5 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE conference on computer vision andpattern recognition, pages 248255. Ieee, 2009. 5",
  "Heewoo Jun and Alex Nichol.Shap-e:Generat-ing conditional 3d implicit functions.arXiv preprintarXiv:2305.02463, 2023. 2, 3, 5, 6": "Animesh Karnewar, Niloy J Mitra, Andrea Vedaldi, andDavid Novotny. Holofusion: Towards photo-realistic 3d gen-erative modeling. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 2297622985,2023. 3 Animesh Karnewar, Andrea Vedaldi, David Novotny, andNiloy J Mitra. Holodiffusion: Training a 3d diffusion modelusing 2d images.In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1842318433, 2023. 3",
  "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.Blip-2:Bootstrapping language-image pre-training withfrozen image encoders and large language models.arXivpreprint arXiv:2301.12597, 2023. 5": "Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusion-sdf: Text-to-shape via voxelized diffusion. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1264212651, 2023. 3 Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolutiontext-to-3d content creation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 300309, 2023. 2, 3, 6",
  "Nelson Max. Optical models for direct volume rendering.IEEE Transactions on Visualization and Computer Graphics,1(2):99108, 1995. 4": "Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, andDaniel Cohen-Or. Latent-nerf for shape-guided generationof 3d shapes and textures. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 1266312673, 2023. 3 Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:Representing scenes as neural radiance fields for view syn-thesis. Communications of the ACM, 65(1):99106, 2021.2 Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka,Niloy J Mitra, and Leonidas J Guibas. Structurenet: hierar-chical graph networks for 3d shape generation. ACM Trans-actions on Graphics (TOG), 38(6):119, 2019. 3 NormanMuller,YawarSiddiqui,LorenzoPorzi,Samuel Rota Bulo,Peter Kontschieder,and MatthiasNiener.Diffrf:Rendering-guided 3d radiance fielddiffusion.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition,pages43284338, 2023. 2, 3, 5",
  "Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer. High-resolution image syn-thesis with latent diffusion models. In CVPR, pages 1068410695, 2022. 6": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1068410695, 2022. 2, 4 Chitwan Saharia, William Chan, Saurabh Saxena, LalaLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,et al. Photorealistic text-to-image diffusion models with deeplanguage understanding.Advances in Neural InformationProcessing Systems, 35:3647936494, 2022. 2",
  "Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,and Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-eration. arXiv preprint arXiv:2308.16512, 2023. 3": "J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner,Jiajun Wu, and Gordon Wetzstein. 3d neural field generationusing triplane diffusion. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 2087520886, 2023. 2, 3 Vincent Sitzmann, Julien Martel, Alexander Bergman, DavidLindell, and Gordon Wetzstein. Implicit neural representa-tions with periodic activation functions. Advances in neuralinformation processing systems, 33:74627473, 2020. 3 Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-hishek Kumar, Stefano Ermon, and Ben Poole. Score-basedgenerative modeling through stochastic differential equa-tions. arXiv preprint arXiv:2011.13456, 2020. 2, 3 Hugo Touvron, Matthieu Cord, Matthijs Douze, FranciscoMassa, Alexandre Sablayrolles, and Herve Jegou. Trainingdata-efficient image transformers & distillation through at-tention. In International conference on machine learning,pages 1034710357. PMLR, 2021. 5",
  "Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,Michael Niemeyer, and Federico Tombari. Textmesh: Gen-eration of realistic 3d meshes from text prompts.arXivpreprint arXiv:2304.12439, 2023. 3": "Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,and Greg Shakhnarovich. Score jacobian chaining: Liftingpretrained 2d diffusion models for 3d generation. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1261912629, 2023. 3 Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, JianminBao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, FangWen, Qifeng Chen, et al. Rodin: A generative model forsculpting 3d digital avatars using diffusion. In Proceedings",
  "of the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 45634573, 2023. 3, 4": "Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, ChongxuanLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity anddiverse text-to-3d generation with variational score distilla-tion. arXiv preprint arXiv:2305.16213, 2023. 3 Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, andJosh Tenenbaum. Learning a probabilistic latent space ofobject shapes via 3d generative-adversarial modeling. Ad-vances in neural information processing systems, 29, 2016.3 Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren,Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian,et al. Omniobject3d: Large-vocabulary 3d object dataset forrealistic perception, reconstruction and generation. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 803814, 2023. 3, 5, 2",
  "Xingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, andShuicheng Yan.Adan: Adaptive nesterov momentum al-gorithm for faster optimizing deep models. arXiv preprintarXiv:2208.06677, 2022. 2": "Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, SergeBelongie, and Bharath Hariharan. Pointflow: 3d point cloudgeneration with continuous normalizing flows. In Proceed-ings of the IEEE/CVF international conference on computervision, pages 45414550, 2019. 3 Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Goj-cic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: La-tent point diffusion models for 3d shape generation. arXivpreprint arXiv:2210.06978, 2022. 3"
}