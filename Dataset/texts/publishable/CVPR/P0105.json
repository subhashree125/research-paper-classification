{
  "Abstract": "Deep learning approaches for animal re-identificationhave had a major impact on conservation, significantly re-ducing the time required for many downstream tasks, suchas well-being monitoring. We propose a method called Re-currence over Video Frames (RoVF), which uses a recurrenthead based on the Perceiver architecture to iteratively con-struct an embedding from a video clip. RoVF is trained us-ing triplet loss based on the co-occurrence of individuals inthe video frames, where the individual IDs are unavailable.We tested this method and various models based on the DI-NOv2 transformer architecture on a dataset of meerkatscollected at the Wellington Zoo. Our method achieves atop-1 re-identification accuracy of 49%, which is higherthan that of the best DINOv2 model (42%). We found thatthe model can match observations of individuals where hu-mans cannot, and our model (RoVF) performs better thanthe comparisons with minimal fine-tuning. In future work,we plan to improve these models by using pre-text tasks, ap-ply them to animal behaviour classification, and perform ahyperparameter search to optimise the models further.",
  ". Introduction": "In various areas of ecology and ethology, deep learning andcomputer vision have played significant roles in automatingdata collection by minimising the time required to extractinformation from images or videos . One such area isanimal re-identification, which aims to match observationsof the same individual across time or camera views .Rapid methods for re-identification are crucial for process-ing the large-scale data required to make population esti-mates for many at-risk populations , monitoring animalwell-being in zoo environments , and understandinganimal behaviour using time budgets and enclosure usage",
  "*These authors contributed equally to this work": "maps . Traditional methods for these downstream tasksare typically laborious, require human observers to watchthe enclosure for hours at a time, and are prone to observererrors .Manual methods for animal re-identification are inva-sive and impractical in many cases. For meerkats (Suri-cata Suricatta), previous methods include dye marks on fur or edible glitter to identify individuals by their faeces. Tracking collars also allows researchers to record thebehaviour of individuals without needing to re-identify in-dividuals . The visual appearance of meerkats varies,particularly in the colouration of the eye masks, around thesnout, and body morphology such as tail length ; how-ever, these details are challenging for human observers todiscern .Patterns such as these on other animal species haveenabled many recent deep learning methods for re-identification, such as dark markings on manta rays ,stripes on Siberian tigers , or ring patterns on seals . One concern with these approaches is that focusing onanimal-specific features results in methods that may not beapplicable to other species .When the number of individuals is known in advance,re-identification can be treated as a classification problem.However, most animal re-identification applica-tions have an open set of individuals, as populations in thewild and captivity change over time. Similarity-based re-identification aims to learn a similarity metric or distancefunction to compare the observations . This allows in-dividuals outside the training set to be recognised as well aspreviously seen individuals to be matched again .Many previous re-identification methods used Convolu-tional Neural Networks (CNNs) trained using triplet loss tolearn an embedding for each observation, which can be usedto find matching individuals . Triplet loss networkslearn using triplets of samples that include an anchor (xa),a positive example of the same individual (xp), and a nega-tive example of a different individual (xn) . These",
  "arXiv:2406.13002v1 [cs.CV] 18 Jun 2024": "networks aim to reduce the distance between observationsof the same individual in an embedding space and increasethe distance between different individuals. These models donot learn using trivial triplets, where the negative is clearlydifferent from the anchor and positive observations. Thishas led to multiple variants of triplet loss being proposedthat train the model using challenging triplets, such as thebatch-hard triplet loss, in which for each anchor image, thefurthest positive and closest negative within a batch of sam-ples are selected . These triplet loss variantsrequire the ground-truth labels of each individual to formchallenging triplets, which is not always possible when do-main experts cannot accurately distinguish between individ-uals.In recent years, transformer models have exceeded theperformance of many CNN models across several computervision tasks . One of the most prominent transformer-based models is the DINO model , which uses astudent-teacher training method to train a model to asso-ciate the global and local views of an image, leading to agreater semantic understanding of the scene and focus onforeground objects. However, these methods have yet tobe applied to animal re-identification. Transformer-basedmodels have also been successful in various video tasks .Many re-identification studies using CNNs found that theperformance can be improved using videos or sequences offrames rather than solely using images . Thismotivates our study, where the dataset consists of meerkatsthat may have identifying patterns in their motion and cam-ouflage to their background.Thisstudyproposesanewsimilarity-basedre-identification model, called Recurrence over Video Frames(RoVF), and a new variant triplet loss to re-identify animalsin a zoo environment from videos in which ground-truth IDlabels are unavailable. RoVF consists of a recurrent headon top of an image model that iteratively constructs a videoembedding over frames. We instantiate the image model asDINOv2 with a Perceiver as the recurrent archi-tecture. We compare this architecture to various pretrainedand fine-tuned DINO models and achieve a better top-1 accuracy. We evaluate all models on a meerkat datasetcollected at the Wellington Zoo (Wellington, New Zealand) and achieve viable meerkat re-identification where hu-man annotators could not.",
  ". Dataset creation": "A new re-identification dataset was generated based onthe twenty 12-minute videos provided in the Meerkat Be-haviour Recognition Dataset . The meerkats were la-belled as tracks without a fixed ID per meerkat, meaningthat if a meerkat exited the camera view and returned, it was assigned a new ID by the annotators since the annotatorscould not distinguish individuals. The unoccluded tracks ofindividual meerkats were divided into short 10-second clips.These clips were cropped from the full-resolution videosby determining the maximum dimensions of the meerkatsbounding box over the duration of the clip and creating afixed-sized square centred on the meerkats bounding boxfor the duration of the clip. These crops were then resizedto a resolution of 224 224 pixels at 1 fps. To generatemore data, the start of the 10-second clips was stagnatedonce every 3 1 3 seconds.Similar to other animal re-identification datasets, thisdataset presents challenges. Meerkats are non-rigid and de-formable, leading to a high variation in posture .The zoo enclosure also emulates a natural environment,leading to variations in lighting, occlusions, and meerkatscamouflaging with their surroundings. Another challengeof this dataset is the size of the meerkats; their boundingboxes are often less than 100 pixels in width or height, mak-ing the patterns of their fur difficult to see. To deal with low-resolution observations, clips were discarded if the bound-ing box of the meerkat did not exceed 70 pixels.Clips were extracted from 16 of the 20 full videos pro-vided in the full dataset for training, and clips from the otherfour videos were held for testing, resulting in 8,244 clips inthe training set and 1,800 in the test set, respectively. Acrossthe 20 videos, 1087 unique IDs were observed, averaging54 unique IDs per video, despite there being only 15 adultmeerkats in the enclosure during the recordings. Of theseIDs, 93 in the training set and 18 in the test set had less thanthree clips assigned to them because these meerkats werenot in view for long during the videos. These were removedfrom being selected as positive/anchor clips, but could beselected as negative examples.",
  ". Triplet loss": "The triplet loss method requires training data in the formof triplets with one anchor, one observation from thesame meerkat (positive), and one observation from anothermeerkat (negative). Two issues arise from this dataset: indi-viduals can change IDs, and individual tracks have limitedbackground variation. The annotators could not re-identifyeach meerkat when they reentered the scene; therefore, neg-ative pairs must be sampled from tracks that share at leastone frame to eliminate the possibility of the same meerkatbeing used as all three observations in a triplet. Owing to thelimited background variation for each annotation track, thetrained models may rely on background information, suchas sand or bark, in some parts of the enclosure to matchindividuals.Using an approach similar to the batch-hard triplet min-ing method , we propose a hard triplet mining strat-egy to reduce the number of trivial triplets when individ- ual IDs are unavailable. Triplets are first generated by ran-domly selecting an anchor individual from all clips and thenrandomly selecting up to j positive examples (P) of thatindividual and up to k negative examples (N) from othermeerkats observed in the same frame. In this study, we setj = k = 20, meaning that up to 20 positive and nega-tive clips were selected for each evaluation. Fewer than 20clips are available for some tracks, where the meerkats donot remain in the video frame for long. During training,the embeddings of all clips are computed, and the most dif-ficult pair of positive clips and the most difficult negativeclip were selected for training. The most difficult positivepair was defined as the two clips with the largest embeddingdistance from each other in the positive set, and the mostdifficult negative observation was defined as the observa-tion closest to one of the positive clips. The closest positiveclip then becomes the anchor. shows an exampleof positive and negative observations randomly selected forone individual. The anchor, positive and negative, selectedfor training by an image-based model are highlighted. Positive observationsNegative observations . Example positive and negative sets. The first frame of20 positive clips of the same meerkat (left half) and 20 negativeclips of other meerkats (right half). The anchor (green), positive(orange), and negative (red) have been selected based on embed-dings from the training ResNet model.",
  ". Model architecture": "For the problem of re-identification with video, we intro-duce an architecturedepicted in called Recur-rence over Video Frames (RoVF) that utilises a recurrentarchitecture on top of an existing pre-trained image model.In our experiments the recurrent architecture is a Perceivertransformer and the image model is a DINOv2 variant.Algorithm 1 shows how a video embedding is con-",
  "Video Embedding": ". Recurrence over Video Frames (RoVF) is an architec-ture that adds a recurrent component on top of an existing imagemodelthat outputs image/frame embeddingsallowing repre-sentations over a video to be constructed. The Recurrent architec-ture iteratively, over frames, builds a representation of the videofrom the image models embeddings for a frame; after the lastframe, a video embedding is outputted by the recurrent model.",
  ": end procedure": "structed from a video of f frames, c channels, of widthw, and of height h, by RoVF. Each frame of the videoV [i, :, :, :] is processed by the image model (line 3), where irepresents the ith frame. The frame embedding f iemb for theith frame is input to the recurrent architecture, with the pre-vious hidden state rh, which produces a new hidden staterh and a video embedding vemb (line 4). This is repeatedfor each frame (lines 24), with the last video embeddingvemb being returned by the algorithm (line 6). When usingthe Perceiver architecture, the hidden state is referred to asthe latent array.For further details on the recurrent architecture used inthis paper, see Perceiver . For the image model, see",
  ". Evaluation metrics": "The most common approach for evaluating animal re-identification methods is to split the test set into query im-ages and a gallery set, where gallery observations are rankedin terms of the distance from the query image to find pos-sible matches . The top-k accuracy metric then mea-sures how often the query image has the same ID as oneof the k highest-ranked gallery images. This dataset doesnot have ID labels for each individual; therefore, the top-kmetrics must be calculated based on the clips of individualswho have frames in common.The test set contained 87 individual tracks, and sets oftwo positives and nine negatives from clips of meerkats oc-curring in the same frames were randomly generated. Thesesets were then manually screened to remove sets where mul-tiple individuals were huddling and sets where the identifi-cation of the positive pair was visually trivial, such as whenthe positive pair had a background distinctly different fromthe negative examples. In total, 100 sets were generated us-ing this method. For each set, the positive examples werealternated as the query clip, with the remaining non-queryclips used as the gallery of clips, resulting in 200 testing setsof query and gallery clips. Embeddings were extracted us-ing each re-identification architecture, and the proportion ofsets in which the correct match was the closest embedding(in Euclidean space) to the query (top-1) and the propor-tion where the correct match was in the closest three galleryexamples (top-3) was recorded.",
  ". Experiments": "We instantiate one version of RoVF, outlined in .3, with DINOv2 base as the image model and thePerceiver as the recurrent model (RoVF-Base). ThePerceiver model is set up with a latent array, input dimen-sion, latent dimension, and output embedding size of 768; 2transformer layers; dropout rate of 0.1. Dropout is appliedafter every frame embedding and in the default positions ofPyTorchs TransformerEncoderLayer.As baselines, we include four pre-trained DINOv2 vari-ants (small, base, large, and giant) and average the patchembeddings in each frame and then all frame embeddings,resulting in a video embedding. Additionally, we fine-tunethe small and base variants of DINOv2 with the same aver-aging regime.We used the ResNet-50 model pretrained on theImageNet dataset to compare with a typical image-basedanimal re-identification model. ResNet models have beenused in many other studies for animal re-identification andcommonly exceed the performance of other common fea-ture extraction methods . Images (224 224)were obtained from the first frame of each clip and trained using triplets generated using the same hard-triplet miningmethod. We fixed the weights of the model and exclusivelytrained the fully connected layer with a dimensionality of2048 to derive an embedding vector of 256 dimensions. Tocompare this with video-based models, embeddings werecalculated from the first frame of each video in the test setand evaluated in the same manner as the video models.For all experiments reported, we use a learning rateschedule with a linear warmup from 0.0001 to 0.0005 over5% of an epoch; cosine decay down to 0.00001 over theduration of training.All models are trained on a singleNVIDIA A100 80GB GPU. A batch size of 30 is used for allmodels; each batch consists of 10 anchor, positive, and neg-ative pairsan effective batch size of 10. We use PyTorchsTripletMarginLoss with default hyperparameters (no differ-ence was found when modifying the margin value). Giventhat we only have a test set (no validation set), all video-based models results are reported for the 5th or 10th epochs,and the ResNet-50 model is reported for the 15th epoch.",
  "Model/sizeFine-tunedEpochsTop-1Top-3Avg.EpochTime": "Random--10.030.0-ResNet-501526.552.50.17DINOv2-Small-41.572.5-DINOv2-Base-38.573.0-DINOv2-Large-42.073.5-DINOv2-Giant-42.073.5-DINOv2-Small526.063.54.3DINOv2-Small1033.563.54.3DINOv2-Base530.064.55.8DINOv2-Base1029.062.05.8RoVF-Base548.063.07.0RoVF-Base1049.064.57.0 . Test set performance for each model and variant. Fine-tuned refers to if the image model of each model is fine-tuned (forRoVF, the recurrent architecture is always fine-tuned). Epochsshows how many training epochs for the reported results. Avg.Epoch Time is the average time in hours for a training epoch. showcases the results of our experiments. Weobserve that all DINOv2 variants with no fine-tuning (rows36) show good baseline performance by averaging the em-beddings over patches and then frames. No success wasfound when fine-tuning these models on our dataset (rows710); performance degraded compared to the pre-trainedmodels. RoVF-Base achieved the best top-1 accuracy overall models (final two rows), demonstrating the potential ofthis architecture; however, the top-3 accuracy degraded.Unsurprisingly, RoVF-Base has the longest average epochrunning time, with the Perceivers addition adding 1.2 hours",
  "Positive": "12.6235 Negative #1 0.0009 Negative #2 16.1486 Negative #3 16.1514 Negative #4 0.0038 Negative #5 16.1472 Negative #6 13.5923 Negative #7 12.6219 Negative #8 12.6257 Negative #9 1.6849 4.3195 13.9503 14.016 2.9921 2.5766 9.2238 13.9662 1.6327 13.9646 0.4349 4.1742 3.9094 6.1079 7.3322 11.7243 7.5501 17.5955 5.0383 1.3902 0.0605 9.7057 9.2766 13.9299 4.8514 11.7337 3.0795 10.7499 14.4515 13.5891 1.7385 9.2308 16.6509 6.6393 5.0102 17.7143 12.5228 7.6724 16.64 11.8607 . Examples of incorrect (red) and correct (green) re-identifications of a query clip (left-most column) using the best RoVF model.The embedding distance between the query and gallery clip is shown underneath each thumbnail. to the average epoch time. Most video-based models out-performed the image baseline, ResNet-50 considerably. Forthis application, video information may offer additional in-sights into individuals through their motion over consecu-tive frames. Because meerkats remain stationary for an ex-tended period and the tracks are limited, meerkats in ran-domly sampled frames from the same track are likely toshare similar poses and backgrounds. A human observermay look at the tail or eye mask of the meerkat that may beoccluded in a randomly selected frame. It is interesting to highlight that fine-tuning DINOv2 de-graded the performance, but freezing its parameters andadding a recurrent head (RoVF) improved the performanceover the pre-trained DINOv2 models. This may be due tothe choice of averaging over patches and then frames for DI-NOv2 instead of concatenating embeddings across patchesand then averaging the embeddings. Another reason couldbe that there is an insufficient amount of training data or alack of diversity, which gives RoVF an advantage. In addi-tion, although a limited search for good hyperparametershas been conducted, an extensive hyperparameter searchmay result in better performance in some instances. An-other concern is trivially simple test cases. Although carewas taken when constructing the test examples, these mod-els may rely on background cues rather than the features ofthe individual, where the positive example is visually sim-ilar to the anchor. We would like to mitigate overfitting tothe background in future work with pre-text training and data augmentation. shows five test cases of the ROVF model ap-plied to match the query clip with one of ten gallery clips.These examples demonstrate that the model can distinguishbetween meerkats in similar environments, and for the twonegative examples, the distance of the query is close tothe selected match.However, because of the nature ofthe dataset, we cannot see how well the model performswhen matching meerkats observed in vastly different back-grounds.",
  ". Conclusion": "This study is an initial proof-of-concept of a transformer-based architecture for animal re-identification without in-dividual labels, called Recurrence over Video Frames(RoVF). Our models achieved good results compared tovarious models based on the DINOv2 architecture when ap-plied to a dataset of short clips of meerkats, with a top-1re-identification accuracy of 49%. Given that the originalannotators of the dataset were unable to identify individualsbetter than random chance, these results were impressive.In future work, we aim to extend this method to video be-haviour classification with behaviour labels, compare ourresults to those of other methods (frame- or video-based),and improve the results using pre-text training. We also in-tend to perform a more rigorous hyperparameter search tofind a more optimal number of training epochs and FPS.",
  "Ethics statement": "Ethical approval was waived for this study due to thecamera-based and non-invasive approach towards datacollection and re-identification.No manipulation ofthe meerkats, collars or markings were used for re-identification. Data acquisition was exclusively carried outat Wellington Zoo during standard opening times, with-out any changes to the management procedures of the an-imals. William Andrew, Colin Greatwood, and Tilo Burghardt. Fus-ing animal biometrics with autonomous robotics: Drone-based search and individual id of friesian cattle (extendedabstract). In 2020 IEEE Winter Applications of ComputerVision Workshops (WACVW), pages 3843, 2020. 1 Voncarlos M. Araujo, Sebastien Gambs, Robert Michaud,Hadrien Lautraite, Leo Schneider, and Clement Chion.Membership inference attack for beluga whales discrimina-tion. Ecological Informatics, 79:102388, 2024. 1",
  "Parzival Borlinghaus, Frederic Tausch, and Luca Retten-berger. A purely visual re-id approach for bumblebees (bom-bus terrestris).Smart Agricultural Technology, 3:100135,2023. 4": "Soren Bouma, Matthew D.M Pawley, Krista Hupman, andAndrew Gilman.Individual common dolphin identifica-tion via metric embedding learning. In 2018 InternationalConference on Image and Vision Computing New Zealand(IVCNZ), pages 16, 2018. 1, 2, 4 Otto Brookes, Stuart Gray, Peter Bennett, Katy V. Burgess,Fay E. Clark, Elisabeth Roberts, and Tilo Burghardt. Evalu-ating cognitive enrichment for zoo-housed gorillas using fa-cial recognition. Frontiers in Veterinary Science, 9, 2022. 1,2",
  "Florian D. Huels and Angela S. Stoeger. Sentinel behaviorin captive meerkats (suricata suricatta). Zoo Biology, 41(1):1019, 2022. 1": "Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,Andrew Zisserman, and Joao Carreira. Perceiver: Generalperception with iterative attention. In International confer-ence on machine learning, pages 46514664. PMLR, 2021.2, 3, 4 Shuyuan Li, Jianguo Li, Hanlin Tang, Rui Qian, and WeiyaoLin.Atrw: A benchmark for amur tiger re-identificationin the wild. In Proceedings of the 28th ACM InternationalConference on Multimedia, page 25902598, New York, NY,USA, 2020. Association for Computing Machinery. 1, 2 Olga Moskvyak, Frederic Maire, Feras Dayoub, and MahsaBaktashmotlagh.Learning landmark guided embeddingsfor animal re-identification. In 2020 IEEE Winter Applica-tions of Computer Vision Workshops (WACVW), pages 1219, 2020. 1, 4 Olga Moskvyak, Frederic Maire, Feras Dayoub, Asia O.Armstrong,and Mahsa Baktashmotlagh.Robust re-identification of manta rays from natural markings by learn-ing pose invariant embeddings. In 2021 Digital Image Com-puting: Techniques and Applications (DICTA), pages 18,2021. 1, 2 EkaterinaNepovinnykh,TuomasEerola,andHeikkiKalviainen. Siamese network based pelage pattern matchingfor ringed seal re-identification. In 2020 IEEE Winter Ap-plications of Computer Vision Workshops (WACVW), pages2534, 2020. 1",
  "Ekaterina Nepovinnykh, Tuomas Eerola, Vincent Biard,Piia Mutka, Marja Niemi, Mervi Kunnasranta, and HeikkiKalviainen.Sealid: Saimaa ringed seal re-identificationdataset. Sensors, 22(19), 2022. 2": "Ekaterina Nepovinnykh, Antti Vilkman, Tuomas Eerola, andHeikki Kalviainen. Re-identification of saimaa ringed sealsfrom image sequences. Lecture Notes in Computer Science(including subseries Lecture Notes in Artificial Intelligenceand Lecture Notes in Bioinformatics), 13885 LNCS:111 125, 2023. 1, 2 Maxime Oquab, Timothee Darcet, Theo Moutakanni, HuyVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mah-moud Assran, Nicolas Ballas, Wojciech Galuba, RussellHowes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, MichaelRabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Je-gou, Julien Mairal, Patrick Labatut, Armand Joulin, and PiotrBojanowski. Dinov2: Learning robust visual features with-out supervision, 2024. 2, 3, 4",
  "Paul E. Rose and Lisa M. Riley. Conducting behaviouralresearch in the zoo: A guide to ten important methods, con-cepts and theories. Journal of Zoological and Botanical Gar-dens, 2(3):421444, 2021. 1": "Stefan Schneider, Graham W. Taylor, Stefan Linquist, andStefan C. Kremer. Past, present and future approaches us-ing computer vision for animal re-identification from cam-era trap data. Methods in Ecology and Evolution, 10(4):461470, 2019. 1 Stefan Schneider, Graham W. Taylor, and Stefan C. Kre-mer. Similarity learning networks for animal individual re-identification - beyond the capabilities of a human observer.In 2020 IEEE Winter Applications of Computer Vision Work-shops (WACVW), pages 4452, 2020. 1, 4",
  "Alex Thornton and Jamie Samson. Innovative problem solv-ing in wild meerkats. Animal Behaviour, 83(6):14591468,2012. 1": "Devis Tuia, Benjamin Kellenberger, Sara Beery, Blair R.Costelloe, Silvia Zuffi, Benjamin Risse, Alexander Mathis,MackenzieW.Mathis,FrankvanLangevelde,TiloBurghardt, Roland Kays, Holger Klinck, Martin Wikel-ski, Iain D. Couzin, Grant van Horn, Margaret C. Crofoot,Charles V. Stewart, and Tanya Berger-Wolf. Perspectives inmachine learning for wildlife conservation. Nature Commu-nications, 13(1):792, 2022. 1 Matthias Zuerl,Richard Dirauf,Franz Koeferl,NilsSteinlein, Jonas Sueskind, Dario Zanca, Ingrid Brehm,Lorenzo von Fersen, and Bjoern Eskofier. Polarbearvidid:A video-based re-identification benchmark dataset for polarbears. Animals, 13(5), 2023. 2"
}