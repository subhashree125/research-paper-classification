{
  "Abstract": "When editing a video, a piece of attractive backgroundmusic is indispensable. However, video background musicgeneration tasks face several challenges, for example, thelack of suitable training datasets, and the difficulties in flex-ibly controlling the music generation process and sequen-tially aligning the video and music. In this work, we firstpropose a high-quality music-video dataset BGM909 withdetailed annotation and shot detection to provide multi-modal information about the video and music.We thenpresent evaluation metrics to assess music quality, includ-ing music diversity and alignment between music and videowith retrieval precision metrics. Finally, we propose theDiff-BGM framework to automatically generate the back-ground music for a given video, which uses different sig-nals to control different aspects of the music during thegeneration process, i.e., uses dynamic video features tocontrol music rhythm and semantic features to control themelody and atmosphere. We propose to align the video andmusic sequentially by introducing a segment-aware cross-attention layer. Experiments verify the effectiveness of ourproposed method. The code and models are available at",
  ". Introduction": "With the rapid development of multimedia and social plat-forms, videos become a common way to convey feelingsand record lives. When creating videos, to make the videomore attractive, a piece of suitable and melodious back-ground music is crucial. However, it is not easy for thosewho do not have much knowledge of music or video edit-ing to select or create proper or perfectly matched music.Whats more, the copyright protection issue has also causedbroader public concern. As a result, it is pragmatic to auto-matically generate background music for a given video.",
  "Rhythm": "Diff-BGM Model . Overview of the background music generation pro-cess of Diff-BGM. At different stages of the generation process,Diff-BGM uses different features of videos or captions to controlthe generation of the music rhythm or melody. We find that thevideo dynamic feature has more control over the rhythm and thesemantic feature has more control over the melody. Existing works have started focusing onmusic generation and achieved good results. Besides, someworks focus on generating music for human-centricvideos. In comparison, free-style video background musicgeneration tasks present more challenges. First, to gener-ate proper background music for a video, the model needsto consider multiple aspects of information in the videoto control different aspects of the music. In a piece ofmusic, several elements work together to make it pleas-ant and concordant to listen to. For example, using dif-ferent rhythms when composing makes the music sounddifferent in dynamic, and different melodies often reflectdifferent atmospheres.Faster music often gives a liveli-ness or intense feeling, which is suitable for videos thatchange quickly while slower music tends to be more sooth-ing and is suitable as a warm-style video soundtrack. Sadvideo clips should correspond to heavy styles and melodieswhile cheerful videos are matched by upbeat music. Weconclude that visual dynamic changes are linked to mu-sic rhythm(the time when the notes appear) and visual se-",
  "arXiv:2405.11913v1 [cs.CV] 20 May 2024": "mantics influence the melody and atmosphere of the music.However, most existing transformer-based models cannot intuitively reflect the music generation process withcorresponding control signals and lack good interpretabil-ity. Models for human-centric videos also abstract rhythminformation from human motion, which are also unsuitablefor freestyle videos. Secondly, compared to music gener-ation, video-conditioned background music generation re-quires models to temporally align the video and the music.For example, if a video is composed of many transitionsof shot, then each transition should have a more prominentsound to indicate the sudden visual changes. According to the above challenges, we are the first toconsider both fronts. However, there lack suitable datasets.Open-source datasets previously used for other music gen-eration tasks either lack corresponded free-style video sam-ples , or fail to provide complete annotations of au-dio or video information , making them unsuit-able for video background music generation. Details areshown in Tab. 1.As a result, we collect a video-musicdataset named BGM909. Compared to existing datasets,BGM909 has several advantages for background music gen-eration. Firstly, we provide high-quality music files, alongwith comprehensive annotations for various aspects of theaudio, such as chords, beats, key signatures, and more.These annotations assist in helping the model learn to an-alyze music structure and composition. Secondly, we offervideos that align with the audio content. Specifically, forsong audios, we provide their official MV videos, ensuringsemantic consistency between music and video. Moreover,our videos undergo manual editing and human checks to en-sure perfect temporal alignment with the music. Addition-ally, we provide detailed annotations for video includingfine-grained natural language descriptions and video shottransitions. To evaluate the quality of the generated music,we also provide new metrics to measure the music qualityand the video-music correspondence. To tackle the proposed challenges, we propose a frame-work named Diffusion-based BackGround Music genera-tion(Diff-BGM) to generate video-aligned background mu-sic. For the first challenge, we use diffusion-based mod-els as our framework. It is a recursive process to gener-ate background music so that we can use different signalsto control different aspects of music. As shown in ,to make the music style and atmosphere correspond withthe given video, we visualize the music generation processand involve the semantic feature of the video to control thestyle and melody of the generated music. Then we use thedynamic video feature to control the generation of musicrhythm so that the timing information in the two modalitiesis aligned. For the second one, we propose a segment-awarecross-attention layer to improve the diffusion frameworkand sequentially align the video and music. The purpose of temporal alignment is to synchronize the music and videofor each segment. Therefore, we introduce cross-attentionwithin the diffusion model and apply time encoding to bothmodalities. We believe that music generation should be in-fluenced by short-term contexts within the video. Hence,we designed a specific mask to constrain the attention mech-anism, obtaining better temporal alignment.Our contributions are summarized as follows: (1) Wepresent BGM909, a high-quality video-music dataset withdetailed annotations for background music generation.Also, we provide metrics to measure the video-music cor-respondence and diversity. (2) We propose Diff-BGM, thefirst diffusion-based network for background music gener-ation. It controls the generation process in stages from dif-ferent dimensions of video and increases the interpretabilityof the generation process. (3) Both objective and subjec-tive evaluation shows that Diff-BGM generates high-qualitybackground music and surpasses the state-of-the-art model.",
  ". Music Generation": "In recent years, music generation has attracted much at-tention, and many models working on music generationhave been proposed. propose to generate musicbased on transformer and gain satisfying generation results.However, transformer-based models often rely on manuallydesigned tokens to encode music, leading to limited gen-erative ability.With the development of diffusion mod-els, it demonstrates remarkable performance not only in vi-sual tasks but also in music generation. Some works pro-pose diffusion-based model to utilize its excellent genera-tive ability. focus on generating music by exert-ing control on music while use text as conditionto generate music. build bridge between musicand other several modals(visual, text, etc). However, thosemethods do not consider the time alignment between mu-sic and the input conditions(like video). As a result, theycannot solve the video background music generation task.",
  ". Background Music Generation": "Some methods focus on generating music forhuman-centric videos (i.e. dance or sports videos), in whichthe rhythm is largely dependent on human motions and isnot accessible in freestyle videos. The task of video back-ground music generation was first proposed by CMT andhas gained more and more attention. Existing backgroundmusic generation methods mostly use the transformer-basedframework and establish the relationship between video andmusic. CMT first encodes the chords and notes to givethe representation of a piece of music and train the modelto understand the logic of music, then it establishes threerhythmic relations (e.g. motion speed of the video corre-",
  "BGM909(Ours)909": ". Comparison between different music datasets. Our proposed BGM909 contains a considerable amount of video-music pairs,being able to be applied to the background music generation task. Different from existing datasets, BGM909 provides various musicalannotations, metadata, captions and shot detection. Two popular music datasets are shown in the first two rows for reference. sponds to note density in the audio) between the video andbackground music to narrow the gap between the two formsof expression. Other than only using the rule-based rhyth-mic relationships, V-MusProd and Video2Music fo-cus on semantic-level correspondence. They extract the se-mantic feature of the video to control the style of the gen-erated music in the multi-modal transformer blocks. How-ever, transformer-based methods suffer from the same chal-lenges that it is hard to control the process of end-to-endgeneration thus leading to poor interpretability. We proposea diffusion-based framework to generate background musicfor videos to make full use of the generative ability of dif-fusion models. Besides, we use different features to controldifferent aspects during the generation process and conducttemporal alignment between video and music.",
  ". Dataset": "Due to the lack of high-quality open-source datasets forbackground music generation, we collect a new video-music dataset BGM909 based on POP909 containing909 pieces of piano version music and corresponding well-aligned videos. BGM909 has the following advantages overprevious datasets. (1) We provide high-quality MIDI files ofmusic, and detailed annotations such as chords, styles etc.(2) The content of the videos aligns with the music. Specif-ically, we provide the official MV videos for each song mu-sic to ensure semantic coherence. (3) We also manually editand check the video-music pairs to ensure perfect temporalalignment. (4) Detailed annotations for videos includingfine-grained language descriptions and shot transitions areprovided for further study. Tab. 1 shows the comparison ofBGM909 with other existing video-music datasets.",
  "Tons of music-video pairs are available on the Internet.However, it is hard to gain high-quality noiseless audio from": "those videos. As a result, we start with the existing well-annotated music in POP909 dataset. For each MIDIfile in POP909, we collect its metadata and use the song ti-tle and singer as keywords to search for the correspondingofficial video on YouTube. After downloading the videos,we remove those that only had static interfaces or lyrics.Then for those left videos, we manually edit the video toalign it with the MIDI file temporally (e.g. some audios arenot played at the beginning of the videos). To ensure datasetquality, we manually check the collected video-music pairs.",
  ". Data Annotation": "Melody, Bridge and Piano. Each MIDI file contains threetracks of melody, bridge, and piano, representing the leadmelody transcription, the secondary melody and the mainbody of the accompaniment separately.Chord, Beat and Key Signature. A chord is a numberof notes played at the same time, specific notes compriseharmony chords and play an important role in setting thebase tone of the music. Beats mean the length of each noteand are basic components of music rhythm. Key signaturerepresents the tonality of the music. We provide chord, beat,and key signatures separately, including beat and downbeatannotations, start and end time for each chord, and chordnames. Detailed algorithms can be found in .Natural Language Descriptions.We provide fine-grained natural language descriptions for each video inBGM909. We generate 10 description sentences for each8 frames in each video with a pre-trained BLIP model.It also serves as a lightweight substitute for video features,capable of fully expressing the semantic information of thevideo. Besides, the captions can be extended to other taskslike text-to-music generation.Camera Shot Detection. We extract each shot of thevideo based on the camera switching. Music often standsout at the transitions in the video, therefore, captured shottransitions often have a significant impact on the rhythm and variations of the music. The timing of shot transitionsis also a focal point to pay attention to during music gen-eration. Shot detection assists in training the alignment be-tween music and video. Videos have 62 shots on average.Styles. We provide GPT with the songs name and theassociated artist and obtain the style classification of eachaudio to further improve the dataset and prove the generalityof BGM909. The songs are from 646 different artists andare divided into 8 different styles in total.Metadata. We also provide extra metadata for the musicin BGM909 compared with POP909, like lyrics, genre, andrhythmic pattern. The metadata information is useful fordata analysis and may be used in future research works likemusic-to-video generation.",
  ". Method": "We propose a novel music generation pipeline named Diff-BGM following the principles of latent diffusion mod-els to deal with the video background music generationtask. The framework of Diff-BGM is shown in (left),which contains a Music Process Module, a Video ProcessModule, a Generative Model and an Output GenerationModule. The music process module takes original midi filesas input and generates corresponding piano rolls to repre-sent the music. The video process module takes the originalvideos as input. To guide the generation process, we extractthe visual features of the video, generate captions for thevideo and extract language features for those captions. Thegenerative module is a diffusion model, which uses the ex-tracted features as conditions to generate a new piano roll.In the end, after gaining the generated piano roll, the out-put generation module is used to process the piano roll andgenerate the corresponding music. In order to control dif-ferent stages of music generation using different features,we introduced a feature selector, as shown in (right).To consider the timing in the video and the music, and alignthe video and the generated music, we introduce segment-aware cross-attention to align the video and music.",
  ". Polyffusion Baseline Revisited": "We use Polyffusion as our baseline. It is trained on mididata and outputs a midi file for unconditional generating.Polyffusion is a diffusion-based music generation frame-work, with a Denoising Diffusion Probabilistic Model as its generative baseline and provides a complete midi pro-cess algorithm. We follow the structure and data processalgorithm proposed by Polyffusion.Music Process. Polyffusion divides the input midi mu-sic into 8-bar (32-beat, T0 = 128 time steps) segmentsand transfers it into image-like piano roll representationx R2T0P , which is a 2-channel binary tensor. TheMIDI pitch ranges 0...127 so we gain P = 128 pitch bins.In the piano roll representation, entry a(c, t, p) represents at time step t and MIDI pitch p whether there is a noteonset(c = 0) or sustain(c = 1).Generative model. Polyffusion uses a latent diffusionmodel as generator to generate piano rolls for videos. Itcontains a diffusion process and a denoising process. In thediffusion process, the structure of data x0 is broken up stepby step by iteratively adding Gaussian noises in N steps:",
  "t=1q(xt|xt1)(2)": "where 1, 2, ..., N are a set of variance scheduling param-eters, x0 is the clean input piano roll. Then in the denoisingprocess, the model learns to reconstruct the original struc-ture of x0 from the noisy input xN N(0, I). It is definedas a Markov chain with learned Gaussian transitions:",
  "t, t)||2](5)": "where represents the model parameters, t is uniform be-tween 1 and N, N(0, I), t = 1 t, t = ti=1 i.Although Polyffusion has the ability to generate musicunconditionally, it still cannot generate background musicfor a given video and has not addressed the two challengesmentioned in Sec. 1, namely the inability to achieve con-ditional control and alignment between music and video.Therefore, to achieve temporal alignment between musicand video, we made improvements upon Polyffusion.",
  ". Video Process": "Diff-BGM model takes videos V as input. We first sampleT frames and use a pre-trained video encoder to extract thevisual feature Fv RT d1 of each frame. Besides, wesegment the video into T segments V = {S1, S2, ..., ST }and generate natural language captions Ci according to thevideo content for each segment Si. For the captions, weuse a pre-trained language encoder to extract the languagefeatures Fl RT d2 for the captions.",
  "Segment-Aware Cross Attention": ". Illustration of our Diff-BGM model. We process the input music and video, gain piano rolls to represent the music and extractvisual features to represent the videos. In order to get richer semantic information, we segment the video and generate captions thenextract language features. The backbone of the generation model is a diffusion model, with the processed visual and language features asconditions to guide the generation process. We propose a feature selector to choose features to control the generation process. And to betteralign the timing of the music and video, we design segment-aware cross-attention layer to grasp the timing feature in different modalities. we found that models tend to generate the melody, which isinfluenced by the semantics, and then generate the rhythmof the music, which is related to the dynamic feature of thevideo. As a result, at different timestep intervals, we usedifferent features as conditions to describe the video.The final condition feature Fc is represented as:",
  ". Sequential Attention": "We aim to generate music for given videos, which meansthe style and atmosphere of the music should match the se-mantic content of the video. Besides, when shot changes ornoticeable motion changes exist, there should be a homolo-gous response in the music. Obviously, unconditional diffu-sion and denoising process cannot achieve this goal. Firstly,as we require the generated music to match the given videoin terms of rhythm, melody, and other aspects, we introduce video features as conditions. We also propose a segment-aware cross-attention layer to fuse the video features withmusic features in the latent space of the diffusion model,ensuring that the generation process is consistently guidedby the video, resulting in music related to the video. Ad-ditionally, to achieve fine-grained temporal alignment be-tween music rhythm and the video, we applied time en-coding to both and introduced specially designed masks toconduct sequential attention. These enable the music gen-eration process to incorporate small-scale video features ascontext, facilitating precise alignment between the two andgenerating high-quality background music.In order to align video and music sequences and under-stand the context information in both modalities, we followLatent Diffusion and design a segment-aware cross-attention layer.The input noisy latent representation xtserves as Query, while the condition feature Fc serves asKey and Value. Then the attention can be represented as:",
  "where Q, K denote Query and Key separately, dkey is thedimension of the condition feature. However, to align the": "two modalities in timing, long-term context is not so impor-tant as short-term context for music is often associated withthe current clip of the video. As a result, a special maskis designed to only pay attention to short-term context andneglect long-term context. As shown in , we divideadjacent k frames into short-term contexts, then only thefeatures of those k adjacent frames can influence the gener-ated music at each time spot. The mask is given as follows:",
  "t, t, Fc)||2]": "(10)where x0 represents the original clean piano roll, Fc denotesthe condition feature, t denotes the time step. During infer-ence, Diff-BGM receives a random noise as input xN anduses the video dynamic and semantic features as conditions.We can control the generation process by flexibly adjustingthe key time step t0 to select the condition features and gen-erate diverse music for a video.",
  ". Implementation Details": "To make a fair comparison, we follow previous work to use a Gaussian noise schedule and the noise predictionobjective in Sec. 4.1 for all experiments.Our segment-aware cross-attention layers are set as . The diffusionstep N is set to 1,000. Diff-BGM model converges around100 epochs on Adam Optimizer with a constant learn-ing rate 5e-5. We use official pre-trained Video CLIP as video encoder to extract visual features. We choose theBLIP model to segment the video and use official pre-trained bert-base-uncased model as the language en-coder. The visual and language encoders keep frozen dur-ing training. In the segment-aware cross-attention module,we set k to 8 and t0 to 200.",
  ". Objective Evaluation": "Metrics. As for evaluating metrics for the task of generat-ing background music for videos, they have not been fullyrefined to date. Therefore, building upon existing metrics,we have proposed additional metrics to assess the generatedresults of background music as follows: Music Quality. We choose the same metrics as toevaluate music quality, including Pitch Class HistogramEntropy(PCHE) which measures the uncertainty of thedistribution of the notes and reflects the quality of tonal-ity, Grooving Pattern Similarity(GPS) which measuresthe quality of the rhythmicity, and Structureness Indi-cator(SI) which captures the repetition in the music bymeasuring the overall structure and reflects the catchinessand the emotion-provoking nature . On SymMV dataset, scale consistency(SC) is also used to evaluate themusic quality. Note that the overall quality is not indi-cated by how high or low these metrics are, but insteadby their closeness to the real music data. Diversity We propose a metric to evaluate the diversityof the generated music. We randomly divide the gener-ated music into two subsets, Sd samples in each set. Thediversity of the generated music is defined as:",
  "where vi, vi represent the music feature of the ith sam-ple in the two subsets separately": "Music Retrieval We propose a new metric to measure themusic-video consistency. Given a piece of generated mu-sic m and the ground truth music of its condition video m,we randomly select M 1 pieces of music mi. Here weuse Musicnn to extract music feature for each gen-erated item. If the ground-truth music ranks in the top-K place, then we consider it a successful retrieval. Allgenerated samples are used to calculate the successful re-trieval rate as the final precision score P@K. Here we setM = 64, K = 5, 10, 20. Since the ground truth musicis related to the given video, the proposed retrieval pre-cision metric is able to measure how well the generatedmusic aligns with the given video. Results.The results on BGM909 test set are shownin Tab. 2. Compared with CMT , our Diff-BGM sur-passes it on both music quality metrics and video-musiccorrespondence, and has a gain of 4.91%, 8.15%, 10.39%on the retrieval metircs, which proves that Diff-BGM gen-erates higher-quality music and has a better understandingof the correspondence between video and music.Results on SymMV dataset are shown in Tab. 3. Forcomparison, we have re-divided the train/val/test sets basedon the instructions provided in , ensuring that the size of",
  ". Subjective evaluation. The preference rates for Diff-BGM against CMT are shown in music quality metrics, video-music correspondence metrics, and expertise metrics": "each set aligns with the proposed official sizes2. Methods inthe first block in Tab. 3 are evaluated on the official SymMVtest set, while methods in the second block are on the testsplit we obtained 3. Since the test split is not identical 4,direct numerical comparisons of music quality metrics can-not be made. However, its important to note that retrieval-based metrics (for video-music correspondence evaluation) 2As of now, SymMV has not publicly disclosed complete information,including the partitioning of train/val/test sets and the alignment times-tamps between video and audio.3We are unable to present the performance results of V-MusProd on thenew split since their code is not publicly accessible.4The numbers of real data on the two splits are different can still be compared in a relatively fair manner, given thatthe size of the retrieval pool is consistent. As shown inTab. 3, Diff-BGM outperforms CMT and V-MusProd invideo-music correspondence metrics by a large margin, in-dicating that Diff-BGM can effectively align video and mu-sic during the generation process.",
  ". Subjective Evaluation": "The best way to evaluate a generative model today re-mains using user study, and it is widely adopted in previousworks . We conduct the user study by designingand sending out questionnaires. We invite 46 people to par-ticipate in the user study, 18 of them are experts with expertknowledge in music, 28 are non-experts. We choose videosfrom different categories then use Diff-BGM and CMT togenerate music for each video separately, present them ran-domly for blindness, and require the participants to com-pare the two generation results in several aspects and givepreference scores separately. For some videos are long, thequestionnaire takes about 25 minutes to complete.Metrics. For each video, participants are required to lis-ten to several music pieces and score them from several as-pects as : (1)Music Melody: the richness of the musi-cal melody; (2)Music Rhythm: the structure consistencyof rhythm; (3) Content Correspondence: the correspon-dence between music and video content; (4) Rhythm Cor-respondence: the correspondence between music and video",
  ". Ablation studies on feature selector. We use differentfeatures in lines 1-2 and different feature orders in lines 3-4 tocontrol the generation process. Closer to Real is better": "rhythm; (5) Overall Preference. Besides, the experts areasked to evaluate two extra metrics related to music theory:(6) Chord Quality: the quality, composition and degree ofharmony of generated chords; (7) Accompaniment Quality:the richness and quality of the generated accompaniment.Results. The result is provided in Tab. 4, showing thepreference rate of Diff-BGM against CMT (the percentageof participants who consider music generated by Diff-BGMbetter than CMT). It shows that in all metrics and usergroups, Diff-BGM outperforms CMT(>50%), indicatingthat Diff-BGM generates higher-quality music and betterunderstands video-music correspondence. We also includepreference scores of more models to compare as shown inTab. 5. It can be observed that in every aspect, Diff-BGMis the closest to artificial(line 1). Note the Human-createdmusic score is only 3.5. It reflects how much improvementis needed to get human-level creation.",
  ". Ablation Studies": "We conduct ablation studies on different components of ourDiff-BGM as shown in Tab. 2. Unconditional means thatwe use the baseline diffusion model to generate music foreach given video.For we do not add any conditions orrestrictions, the generation results have the highest diver-sity(6.421). However, for the lack of control signal and tem-poral alignment, the quality and correspondence are not sogood. Then we add video feature and feature selector(row5-6) to the base model. When adding more signals to con-trol the generation process, the quality of the generated mu-sic keeps improving and the diversity keeps decreasing. Themetric of PCHE has a gain of 0.468, indicating a more clearmelody. Besides, with the introduction of video feature,the video-music correspondence score P@20 has a gain of 11.27, indicating that the music contains information fromthe video. In the last row, segment-aware cross-attentionlayers are added to the model, which focuses on the align-ment between music and video and improves the retrievalscore. However, when we force the music to pay attention toonly short-term context of the video, the music quality de-creases. The results indicate that the quality of music and itscorrelation with video mutually influence each other whenwe aim to exert control over the music.Besides, we conduct an ablation study on the feature se-lector as shown in Tab. 6. In the first two rows, we onlyuse features from one modality (either video dynamic orsemantics) to control the generation process. We find thatwhen only using language features as condition, the resultsgain the highest GPS marks (0.641), which means that thestructure of generated music is closest to real one and cap-tions facilitate the generation of musical structures. And inthe last two rows, we attempt to use different feature or-ders to control the generation at different stages. The re-sults indicate that early-stage usage of video semantic fea-tures followed by dynamic features yields the best musicquality, aligning with the viewpoint that the model gener-ates melody first and then produces rhythm. More ablationstudies about the feature selector and SAC Attention can befound in supp.M.",
  ". Conclusion": "In this paper, we propose the Diff-BGM framework totackle the video background music generation task and newevaluation metrics to measure video-music correspondenceand also music diversity. We also provide a high-qualitydataset, BGM909, comprising temporally and semanticallyaligned video-music pairs and fine-grained annotations forshots and natural language captions of videos. We addressthe issue of poor interpretability in existing generative mod-els by using different features to control various stages ofthe music generation process. We introduce segment-awarecross-attention to temporally align music and video andgenerate music corresponding to video content and rhythm.Experiments verify that Diff-BGM has the capability ofgenerating high-quality background music for videos.Acknowledgements. This work was supported by thegrants from the National Natural Science Foundation ofChina 62372014. S. Di, Z. Jiang, S. Liu, Z. Wang, L. Zhu, Z. He, H. Liu, andS. Yan, Video background music generation withcontrollable music transformer, Proceedings of the 29thACM International Conference on Multimedia, 2021.[Online]. Available: 2, 6,7, 8 L. Zhuo, Z. Wang, B. Wang, Y. Liao, C. Bao, S. Peng,S. Han, A. Zhang, F. Fang, and S. Liu, Video backgroundmusic generation: Dataset, method and evaluation, inProceedings of the IEEE/CVF International Conference onComputer Vision, 2023, pp. 15 63715 647. 2, 3, 6, 7",
  "J. Kang, S. Poria, and D. Herremans, Video2music:Suitable music generation from videos using an affectivemultimodal transformer model, ArXiv, vol.abs/2311.00968, 2023. [Online]. Available: 3": "M. Plitsis, T. Kouzelis, G. Paraskevopoulos, V. Katsouros,and Y. Panagakis, Investigating personalization methods intext to music generation, ArXiv, vol. abs/2309.11140,2023. [Online]. Available: 1, 2 K. Chen, Y. Wu, H. Liu, M. Nezhurina, T. Berg-Kirkpatrick,and S. Dubnov, Musicldm: Enhancing novelty intext-to-music generation using beat-synchronous mixupstrategies, ArXiv, vol. abs/2308.01546, 2023. [Online].Available: 2 L. Min, J. Jiang, G. G. Xia, and J. Zhao, Polyffusion: Adiffusion model for polyphonic score generation withinternal and external controls, ArXiv, vol. abs/2307.10304,2023. [Online]. Available: 2, 4, 6",
  "K. Maina, Msanii: High fidelity music synthesis on ashoestring budget, ArXiv, vol. abs/2301.06468, 2023.[Online]. Available: 2": "Y. Zhu, Y. Wu, K. Olszewski, J. Ren, S. Tulyakov, andY. Yan, Discrete contrastive diffusion for cross-modalmusic and image generation, in International Conferenceon Learning Representations, 2022. [Online]. Available: 2 A. Lv, X. Tan, P. Lu, W. Ye, S. Zhang, J. Bian, and R. Yan,Getmusic: Generating any music tracks with a unifiedrepresentation and diffusion framework, ArXiv, vol.abs/2305.10841, 2023. [Online]. Available: 1 Z. Wang, K. Chen, J. Jiang, Y. Zhang, M. Xu, S. Dai, X. Gu,and G. G. Xia, Pop909: A pop-song dataset for musicarrangement generation, in International Society for MusicInformation Retrieval Conference, 2020. [Online].Available: 2, 3 O. Patashnik, D. Garibi, I. Azuri, H. Averbuch-Elor, andD. Cohen-Or, Localizing object-level shape variations withtext-to-image diffusion models, ArXiv, vol.abs/2303.11306, 2023. [Online]. Available: 4 S.-L. Wu and Y.-H. Yang, The jazz transformer on thefront line: Exploring the shortcomings of ai-composedmusic through quantitative measures, in InternationalSociety for Music Information Retrieval Conference, 2020.[Online]. Available: 6, 7",
  "D. J. Levitin, This is Your Brain on Music: The Science of aHuman Obsession.Dutton Penguin, 2006. 6": "L. Ruan, Y. Ma, H. Yang, H. He, B. Liu, J. Fu, N. J. Yuan,Q. Jin, and B. Guo, Mm-diffusion: Learning multi-modaldiffusion models for joint audio and video generation,2023 IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pp. 10 21910 228, 2022.[Online]. Available: 7 R. Rombach, A. Blattmann, D. Lorenz, P. Esser, andB. Ommer, High-resolution image synthesis with latentdiffusion models, 2022 IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), pp.10 67410 685, 2021. [Online]. Available: 4, 5, 6",
  "P. Neves, J. Fornari, and J. B. Florindo, Generating musicwith sentiment using transformer-gans, in InternationalSociety for Music Information Retrieval Conference, 2022.[Online]. Available:": "H.-W. Dong, K. Chen, S. Dubnov, J. McAuley, andT. Berg-Kirkpatrick, Multitrack music transformer,ICASSP 2023 - 2023 IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP), pp.15, 2022. [Online]. Available: W.-Y. Hsiao, J.-Y. Liu, Y.-C. Yeh, and Y.-H. Yang,Compound word transformer: Learning to composefull-song music over dynamic directed hypergraphs, inAAAI Conference on Artificial Intelligence, 2021. [Online].Available: S.-L. Wu and Y.-H. Yang, Musemorphose: Full-song andfine-grained piano music style transfer with one transformervae, IEEE/ACM Transactions on Audio, Speech, andLanguage Processing, vol. 31, pp. 19531967, 2021.[Online]. Available: 2 A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti,A. Caillon, Q. Huang, A. Jansen, A. Roberts,M. Tagliasacchi, M. Sharifi, N. Zeghidour, and C. H. Frank,Musiclm: Generating music from text, ArXiv, vol.abs/2301.11325, 2023. [Online]. Available: 1 C. Hawthorne, A. Stasyuk, A. Roberts, I. Simon, C.-Z. A.Huang, S. Dieleman, E. Elsen, J. Engel, and D. Eck,Enabling factorized piano music modeling and generationwith the maestro dataset, ArXiv, vol. abs/1810.12247,2018. [Online]. Available: 2, 3",
  "Y. Zhu, K. Olszewski, Y. Wu, P. Achlioptas, M. Chai,Y. Yan, and S. Tulyakov, Quantized gan for complex musicgeneration from dance videos, ArXiv, vol. abs/2204.00604,2022. [Online]. Available: 3": "R. Li, S. Yang, D. A. Ross, and A. Kanazawa, Aichoreographer: Music conditioned 3d dance generation withaist++, 2021 IEEE/CVF International Conference onComputer Vision (ICCV), pp. 13 38113 392, 2021.[Online]. Available: 3 B. Li, X. Liu, K. Dinesh, Z. Duan, and G. Sharma,Creating a multitrack classical music performance datasetfor multimodal music analysis: Challenges, insights, andapplications, IEEE Transactions on Multimedia, vol. 21,pp. 522535, 2016. [Online]. Available: 2, 3"
}