{
  "Abstract": "Backdoor attacks have been well-studied in visible lightobject detection (VLOD) in recent years. However, VLODcan not effectively work in dark and temperature-sensitivescenarios.Instead, thermal infrared object detection(TIOD) is the most accessible and practical in such environ-ments. In this paper, our team is the first to investigate thesecurity vulnerabilities associated with TIOD in the contextof backdoor attacks, spanning both the digital and physi-cal realms. We introduce two novel types of backdoor at-tacks on TIOD, each offering unique capabilities: Object-affecting Attack and Range-affecting Attack. We conducta comprehensive analysis of key factors influencing trig-ger design, which include temperature, size, material, andconcealment. These factors, especially temperature, signif-icantly impact the efficacy of backdoor attacks on TIOD.A thorough understanding of these factors will serve as afoundation for designing physical triggers and temperaturecontrolling experiments. Our study includes extensive ex-periments conducted in both digital and physical environ-ments. In the digital realm, we evaluate our approach usingbenchmark datasets for TIOD, achieving an Attack SuccessRate (ASR) of up to 98.21%. In the physical realm, we testour approach in two real-world settings: a traffic intersec-tion and a parking lot, using a thermal infrared camera.Here, we attain an ASR of up to 98.38%.",
  "*Corresponding author": "light, smoky, heavy rain, and intense snow environments,making it less affected by glare and light mutation, all whileretaining its sensitivity to thermal changes in objects .Consequently, TIOD becomes increasingly indispensable ina variety of application scenarios, from security monitoringand autonomous driving in the dark to temperature mea-surement during a pandemic. The security vulnerabilities ofVLOD are thoroughly examined for both adversarial attacks and backdoor attacks . Backdoor at-tacks manipulate both a small portion of contaminated train-ing samples and testing samples with the backdoor trigger.Then, the trained detector will have backdoor effects at thetesting time when encountering samples with the backdoortrigger, while remaining normal when fed with clean test-ing samples. In real-world scenarios, backdoor attacks posea serious security threat to deep neural networks (DNNs) due to their stealthiness. Unlike VLOD field , the security vulnerabilitiesof TIOD remain largely unexplored and current efforts arefocused merely on adversarial attacks rather than backdoorattacks. For instance, Zhu et al. design adversarial patternsand manufacture an adversarial shirt made of aerogel mate-rial . Wei et al. introduce the method of aggregationregularization to optimize the adversarial infrared patch,making the patch easier to implement physically . Bothworks suggest that extra considerations are required to de-sign effective adversarial examples for TIOD, mainly due tothe unique characteristics of thermal infrared imaging com-pared to visible light imaging. These new adversarial at-tacks ring the alarm that TIOD demands the same level ofscrutiny as VLOD to expose all types of potential securitythreats. However, the security vulnerabilities of TIOD tobackdoor attacks remain unexplored.",
  "High-Temperature": ". Examples of OAA and RAA for car disappearance inthe physical world. By changing the temperature of the trigger, itis capable to switch between whether the backdoor is activated inOAA and to adjust the attack range in RAA. The affecting rangeof RAA is marked as the red circle. objects captured using infrared thermal radiation imaging.Unlike RGB images with three channels, thermal infraredimages have only a gray-scale channel and contain less tex-ture information . Backdoor attacks on VLOD can takeadvantage of the additional information lying in the extrachannels to gain more capacity in trigger design, which ul-timately allows for a strong attack capability. The designof backdoor attacks on TIOD becomes more challengingthan the visible light counterpart, because the design spacefor the trigger is restricted to properly placing the trigger,choosing a material with ideal thermal infrared character-istics, or manipulating its temperature. Therefore, it raisesthe following compelling question: Can we design effectivebackdoor attacks on TIOD by utilizing their unique proper-ties compared to VLOD?In this paper, we propose two backdoor attack methodsagainst TIOD: Object-Affecting Attack (OAA) and Range-Affecting Attack (RAA). OAA manipulates the detectionresults for a specific object carrying with a trigger. WhileRAA causes all objects of a chosen class in close proxim-ity to the trigger to be misidentified. In addition, we pro-pose a new mechanism to adjust the triggering behavior ofbackdoor attacks by temperature modulating. The demon-stration result is presented in . Remote control ofbackdoor attacks involves a simple button press, eliminat-ing the need for any visible light visual changes to the pre-arranged scene. Since the temperature change of the objectis not visible to the human eye, temperature modulating of-fers more stealthiness and flexibility. Our in-depth studyof proposed attacks in the digital world, and successful im-plementation in the physical world, provides an affirmativeanswer to the question in the preceding paragraph that ded- icated backdoor attacks indeed pose significant threats toTIOD. The main contributions of this paper are as follows: We examine the security vulnerability of TIOD to back-door attacks and identify the critical factors that differen-tiate their trigger design from that of VLOD. To the bestof our knowledge, this is the first study of backdoor at-tacks on TIOD. We propose two types of backdoor attacks of OAA andRAA that offer different affecting capacities. In addition,we further propose a novel backdoor trigger by modulat-ing its temperature, allowing the backdoor effect to be ac-tivated or deactivated within different temperature rangesin OAA and adjusting the affecting range in RAA. In a digital environment, we validate the attacks effec-tiveness across various parameters, achieving an ASR ofup to 98.21%. In the physical world, we test the proposedbackdoor attacks in two representative real scenes of atraffic intersection and a parking lot. Our attacks are ef-fective in both scenes, achieving an average ASR of over96%. In addition, the methods are cost-friendly, with theproduction of an electric heating device as a trigger cost-ing less than 5 US dollars. We also evaluate three poten-tial countermeasures defending against our attacks.",
  ". Related Work": "Thermal Infrared Object Detection.Object detection isa fundamental task in computer vision, which also serves asthe foundation of image segmentation , object tracking, and keypoint detection . TIOD uses image infor-mation in the thermal infrared domain for object recognition. The YOLO model , as the first one-stage detector,is employed for pedestrian detection in the thermal infrareddomain . The classic two-stage detector, Faster RCNN,is also applied for TIOD, although its detection accuracyis limited . In diverse environments, including severeweather conditions, Huda et al. achieve accurate pedestriandetection using YOLO v3 and thermal infrared data .Currently, YOLO v3 is integrated into the latest versions ofautonomous driving systems such as Apollo . Gong etal. conduct a study on vehicle recognition in the thermalinfrared domain to assist autonomous driving systems .Furthermore, Wang et al. combine YOLO v5 to enhance thedetection accuracy of TIOD . There is a growing bodyof research that focuses on the performance improvementand data diversity in TIOD .Backdoor Attack.Backdoor attacks are carried out byembedding hidden backdoors into DNNs The attacker gen-erates a backdoor model through data poisoning attacks or poisoning + training manipulation at-tacks (the former only poisons the training data,while the latter not only poisons the training data but alsomodifies the training process). The triggers used to activatebackdoor effect are also diverse. Existing triggers of back- door attacks include single pixel , reflection background, invisible patterns , and so on. In additionto directly poisoning training data, backdoor attacks canalso embed hidden backdoors by modifying model weightsthrough transfer learning . Therefore, backdooredmodel training can occur in all steps of the training pro-cess, which is a serious threat to DNNs .The research into backdoor attacks drive the developmentof defense methods . Beyond digital world, thereare also works exploring backdoor attacks in the physicalworld . The above works all focus on thevisible light field, while backdoor attacks in the thermal in-frared field still lack exploration.",
  ". Threat Model": "Attackers Goal. Our backdoor attack has two goals, bothof which align with common design principles of backdoorattacks. The first goal is for stealthiness purpose to ensurethat the backdoored TIOD can still properly identify the ob-jects in clean samples. This task can make it difficult forthe user to find the model anomalies without knowing thebackdoor trigger. The second goal is for effectiveness pur-pose to cause the backdoored TIOD to either not identifythe object (i.e., object disappearance) or identify it with anincorrect object class (i.e., object misclassification) in back-door samples with the attacker-chosen trigger. In this paper,we focus on attacking cars as the exemplary object class,due to the serious security consequences in real-world ap-plications such as autonomous driving.Attackers Capability.Backdoor attacks can occur inmany situations, such as outsourcing training, using pre-trained models for transfer learning, and collecting datafrom unknown sources. Following previous work on back-door attacks , we adopt the data poisoning threatmodel.It suffices to gain access to part of the trainingdataset in order to inject poisoned training samples, whileleaving the training process untempered. Since this threatmodel does not interfere with the model and training pro-cess, it can fundamentally reveal the vulnerability of TIOD,thereby promoting research on the improvement of objectdetection security, such as backdoor defense methods.",
  ". Problem Formulation": "In this paper, we mainly attack the object detection modelYOLO v5. The YOLO v5 contains three loss componentsas follow: classification loss Lcls, BBox regression loss LB, and confidence loss Lconf.The classification lossis calculated only when there is an object in the detectionBBox. Please refer to Appendix A for the detailed formu-lation of each loss component. The total loss function is",
  "L = Lcls + Lconf + LB,(1)": "where , , and are the balancing hyper-parameters.Backdoor attackers can poison a small portion q of thetraining dataset. Without loss of generality, let the poisonedtraining dataset S be divided into a clean dataset Sc and adirty dataset Sd, where |Sd| = q |S| with | | denotingthe cardinality of the datasets. The poisoned images in thedirty dataset are modified from the original images with theattacker-chosen trigger injected, denoted by xdi . The dirtylabel ldi is obtained by modifying the original label li ac-cording to the attack purpose (i.e., misclassification or dis-appearance), which is expressed as follows :",
  "locObject MisclassificationNoneObject Disappearance,(2)": "where loc indicates that the class of the label is replaced,and None indicates that the label is deleted. Altogether, thedirty training sample in the dirty dataset becomes (xdi , ldi ).During training, neuron activations in the network willbecome abnormally affected, causing the input image withthe trigger to be incorrectly mapped to a specific output.Depending on the attack purpose, the backdoor attack canbe formulated by one of the two optimization problems. ForObject Misclassification,",
  "argminL = argmin(Lcls + Lconf(S, ) + LB),(4)": "where Lconf(S, ) represents the confidence loss on thepoisoned dataset S with backdoored model parameters .Recall that the classification loss and BBox regression lossare involved only when the confidence loss indicates a highlevel of object existence confidence. Therefore, by remov-ing the object class label, we can disrupt the normal func-tion of the confidence loss component at the presence of thetrigger, causing the detector to falsely identify the object asdisappearance in detection.",
  "p = g(t) = T m + ,(5)": "where p is the pixel value, T is the object temper-ature, and p increases as T increases. and are the ad-justment parameters obtained by the thermal infrared cam-era. The operating wavelength in this paper is between 8and 13 m, so m = 3.9889 . In practical applications,m = 4 has little effect on the measurement results. As",
  "Due to the correction function of the thermal infrared cam-era, the temperature of the same point measured with a ther-mal infrared camera does not vary with distance": "Trigger Design. Existing backdoor attacks for VLOD relyon triggers designed based on color differences . Whenapplied to the thermal infrared domain, as shown in , such triggers will appear as grayscale patterns. Conse-quently, their intricate texture details cannot be effectivelykept, rendering backdoor attacks unable to be effectivelytriggered. To overcome this limitation, the designed triggerneeds to have a temperature attribute difference from the at-tacked target, and the design of backdoor attacks should bebased on the temperature difference. After comparing insu-lation cotton sheets, plastic sheets, and electric heaters, wefind that electric heaters offer better control over tempera-ture changes. As shown in , the physical triggeris an electric heating device consisting of an electric heaterand a signboard. The choice of sign can be adjusted to suitthe scene and ensure unobtrusiveness. The temperature canbe remotely controlled with a single button press. The mor-phology of triggers at different temperatures in the thermalinfrared world is shown in . Given the uniform heatdistribution of the object, we can simulate digital triggersusing pixel blocks based on Equation (6). This approach fa-cilitates the exploration of parameter influences on attacks.",
  "Object-Affecting Attack": "How to ensure the effectiveness of the attacks and maintainthe Benign Accuracy (BA) of the model are key issuesfor backdoor attacks. Since the trigger should be added in-side the object BBox which can be small (resp. large) forobject lying remote (resp. close) to the infrared camera, weadjust the trigger size according to this objects BBox in",
  "class=cary(p, s),(8)": "where x is the image after adding all triggers to cleanimage x. Assuming the attack pixel range is [p1, p2], ifp [p1, p2], we perform Label Modification to this object.As shown in Equation (2), according to different attack pur-pose, we can either delete the label or replace the class ofthe label to generate the poisoned dataset required for train-ing the backdoor model. 3.3.3Range-Affecting AttackThe one-stage detectors unusually divide the image intomany grids during detection. Whenever the object underdetection has some overlap with any grids, these grids willall participate in the detection of this object. Since an ob-ject may occupy multiple adjacent grids, it is possible toadd the trigger close to the object so that the grids occupiedby the trigger will overlap with some of the objects grids,even though the trigger does not strictly lie inside the ob-jects BBox. Therefore, backdoor attack can still establishthe abnormal association between the trigger and the ob-ject indirectly via the overlapping grids, instead of directlyputting inside the objects BBox. Based on this analysis, wepropose the second backdoor attack called RAA. For the poisoned dataset, we insert only a single triggerpattern into each poisoned image. The location of the trig-ger in RAA can be more flexible. By observing objects withour thermal infrared camera utilized in the physical experi-ments, we select common telephone poles or street signs onthe road as triggers, which can be simulated with pixel stripsand pixel blocks in the digital world, respectively. Given apixel strip y(p, hw) with length h, width w and pixel valuep as a trigger, the attacker arbitrarily selects apoint (a, b) in the clean image x as the center point of thetrigger to insert the trigger into the clean image. Finally, thepoisoned image can be obtained as follows,",
  ". Experiments": "In this section, we conduct experimental evaluations ofOAA and RAA in both digital and physical worlds.Evaluation Metrics.Since there is no existing intuitiveeffectiveness metrics for the backdoor object detection at-tack, we introduce a new metric - Benign Accuracy Fluctu-ation (BAF), in addition to the commonly adopted metric ofAttack Success Rate (ASR). BAF is the value obtained bysubtracting the mAP of clean samples tested by the cleanmodel from that returned by the backdoor model. We addtrigger to the objects in the test image regardless of the ob-ject size, and then define the trigger addition as successfulwhen the following conditions are met.",
  "Class(GT) = Class(BS) = car,": "where GT is the ground truth label of the object, and BSis the label obtained by the backdoor model detecting cleansamples. IOUmodel is the IOU threshold set during modeldetection. The total number of successful trigger additionsis recorded as Nta. For the objects successfully addedtrigger, we define the attack as successful when they meetthe following conditions at the same time. When the attacksetting is to misidentify the car as the person,",
  "IOU(GT, DS) < IOUmodel,(14)": "where DS is the label obtained by the backdoor model de-tecting the poisoned samples. The total number of success-ful attacks is recorded as Nsa. Therefore, the attack suc-cess rate is defined as ASR = Nsa/ Nta 100%.Since mAP of the backdoor model and the clean model onclean samples is similar, we lock the object that can be rec-ognized by the model, which can greatly reduce the inter-ference of other factors in the model.",
  ". Experiments of Digital World Attacks": "Datasets and Models.The Flir v2 dataset is released byFLIR Company. We only utilize the thermal infrared imagesand corresponding annotations, referring to it as Flir v2 T,which contain 13460 images and label files. The imagesize in Flir v2 T is 640512. The Multi-spectral ObjectDetection Dataset contains four sub-datasets of RGB,NIR, MIR and FIR, along with ground truth labels providedby an autonomous driving research team at the University ofTokyo. We utilize the FIR sub-dataset containing 7521 ther-mal infrared images and label files, referred as FIR Det inthe sequel. The image size in FIR Det is 640480. Weuse three mainstream object detection models: YOLO v5,YOLO v3 , and Faster RCNN as detectors to ver-ify the attack effectiveness.Baselines.We follow exactly the same data preprocess-ing and model training strategies with existing works forthe clean model training. The mAP of trained clean modelserves as the evaluation baseline for BAF. The results aresummarized in .",
  "Attack Parameters": "We use YOLO v5 detector and Flir v2 T to verify the im-pact of different attack parameters. The attack setting is thatthe detector recognizes car as person.In OAA, the parameters we focus on are Pixel Value (p)and Poisoning Ratio (q). Unless otherwise specified, thedefault parameters take the following combination: p =192, q = 20%. We list the experimental results in .The closer the p is to the median, the smaller the differencebetween the trigger and the object, resulting in a lower BAFof the backdoor model. When the q is increased from 1% to",
  ". The effect of parameters on OAA and RAA": "2%, the attack performance is greatly improved. Therefore,the poisoning ratio should be set above 2%.In RAA, we are more concerned about the Attack Range(ar). The p and q are follow default parameters. We ran-domly select a point (160, 206) in the image as the centerpoint of the trigger to fix the trigger location. The attackarea is set to a circle with the center point (160, 206) as thecenter and ar as the radius. We attack the objects whosecenter point falls within this area. We list the experimentalresults in . The smaller the attack range, the less thenumber of object that can be poisoned (which is why wedo not additionally test the poisoning ratio), so ASR willbe lower and BAF will be higher. When the attack radiusreaches 250, the detection of clean samples will be greatlyaffected and the attack effect will be reduced.The additional parametric experiments such as triggersize and relative location, are provided in Appendix B.",
  "Attack Effectiveness": "As shown in , we experiment with two attack meth-ods on the above three models and two datasets to verifyattack effectiveness. The attack effects are all chosen as thedetector to recognize car as person. In OAA, we set theparameters as p = 192 for all datasets. For Flir v2 T,q = 20%, while for FIR Det, q = 10%.In RAA,for Flir v2 T and FIR Det, the parameter settings arep = 192, q = 20%, ar = 150. These parameters are thesame for different models. We discover that the closer ob-jects are to the range boundary, the weaker the attack effectbecomes. As a result, we set the attack range during infer-ence to be smaller than the range set during training. The re-sults in are tested with the attack range of 120. SinceFaster RCNN is based on candidate BBox, multi-scale can-didate BBoxes can impact the feature extraction of triggers,",
  ". Evaluation results of OAA and RAA on three models and two datasets": "resulting in a weakened attack effect on this model.Unless otherwise specified, we use the YOLO v5 detec-tor and Flir v2 T to conduct all subsequent experiments.We also verify the attack effectiveness when the attack pur-pose is car disappearance. In OAA, when the parametersetting is default parameters, the BAFs of person and carare 0.5% and 5%, respectively, and the ASR is 98.54%.In RAA, when the parameter setting is default parametersand ar = 150. The BAFs of persons and cars are +0.6%and 0.6%, respectively, while ASR is 95.66%. 4.1.3Temperature Modulated TriggeringFor OAA, attack experiments are performed within four dif-ferent temperature ranges corresponding to different pixelranges.After testing, we discover that if only triggerswithin the set temperature range are implanted in thedataset, triggers outside the range still had a high ASR (over50%). Therefore, we implanted triggers outside the set tem-perature range for a portion of the data without changing theobject label, which is called adversarial triggers. Specif-ically, 15% of the data is implanted with normal triggers,while 5% of the data is implanted with adversarial triggers.For RAA, we control the attack range using trigger tem-peratures. Triggers with different pixel values implementRAA with different radii: 80 for pixel value 0 (correspond-ing to the lowest temperature), 120 for pixel value 128 (cor-responding to the average temperature), and 160 for pixelvalue 255 (corresponding to the highest temperature). Weimplant triggers with pixel values of 0, 128, and 255, andmodify the object labels within the attack radius for 10%,6%, and 4% of the training data, respectively. The experi-mental results are presented in .More details, along with the Attack Transferability andComparative Experiments, are provided in Appendix B.",
  ". Experiments of Physical World Attacks": "Datasets and Models.We utilize HTI-301 infrared cam-era (FPA 384288, NETD < 60mK) for physical experi-ments, which is the same equipment used in .Thesize of thermal infrared images produced by this camera is14201080. The object detector is YOLO v5. We use theelectric heating device in as the physical trigger. shows illustration images of the deployed triggerin the visible and thermal infrared domains. The device iscommon enough in real scene to avoid suspicious, and isextremely low in cost.For OAA, we choose the parking lot as the physical ex-",
  ". The trigger for real world deployment": "periment scene. The thermal infrared videos are capturedwith environment temperature at 32 degrees Celsius. Wefix the infrared camera on a moving vehicle. On the samedriving route, we record videos with and without triggers,where we randomly selected some cars to place the triggersnext to them. After separating the video into frames, we ob-tain 788 clean images and 472 dirty images. We manuallyannotate each image with three classes (person, bike, car).For RAA, we choose the parking lot and traffic intersec-tion as the physical experiment scenarios. The thermal in-frared videos are captured with environment temperature at30 degrees Celsius. We fix the infrared camera on the sideof the road and record videos with and without the triggerthat is placed at a fixed location and viewing angle. Af-ter separating the video into frames, we obtain 800 cleanimages and 488 dirty images. We manually annotate eachimage with four classes (person, bike, car, truck).Baselines.For the clean images obtained above, we di-vide the training set and validation set by 9:1. The mAP ofthe trained clean model is used as the evaluation baselinefor BAF. For OAA, the mAP of person (car) in clean sam-ples measured by the clean model is 88.30% (91.80%). ForRAA, the mAP of person (car) in clean samples measuredby the clean model is 96.70% (98.80%).Temperature Modulated Triggering.The attack evalua-tion is shown in . In the physical world, our meth-ods can effectively attack TIOD. For OAA, the ratio of nor-mal and adversarial trigger implants are 15% and 5%, re-spectively. For RAA, high-temperature triggers and low-temperature triggers are implanted at ratios of 8% and 12%,respectively. The visualization result is shown in .",
  ". Examples of OAA and RAA for car misclassification inthe physical world": "move the backdoor implantation by proving certain neu-rons. Concretely, we apply them to prune the neurons inthe deeper layers of the network, where we vary the numberof layers to be pruned from four to eight and the propor-tion of neurons to be pruned from 20% to 95%. The resultshows that while both defense methods can mitigate back-door attacks, they do so at the cost of decreased accuracyfor benign objects. For instance, while capable to lower theASR to 63.33%, the recognition accuracy for benign personand car also drops to 26.2% (originally 78.3%) and 42.7%(originally 81.7%), respectively. Neural Cleanse.Neural Cleanse (NC) is a popular de-fense method against backdoor attacks, which attempts toobtain triggers of each category through reverse engineer-ing. Many subsequent defense methods are based on itsideas. To adapt the NC defense from its original applica-tion in the image classification task to our object detectiontask, we extract the objects from the images in Flir v2 Tand label them separately. We take the original dataset andthe processed dataset as inputs to NC. NC uses L1 norm tocompute masks and anomaly index to identify toxic objects.The value of anomaly index greater than 2 is consideredas a trigger being detected. For OAA, the anomaly indexof car is 1.218572. For RAA, the anomaly index of car is1.767544. Therefore, NC has not detected our attacks.",
  ". Conclusion": "In this paper, we propose two types of backdoor attacks forTIOD: OAA and RAA. Our attacks successfully compro-mise detectors in both digital and physical worlds, caus-ing them to misidentify cars as persons or fail to detect thepresence of cars. We examine various factors that affectthe effectiveness of the proposed backdoor attacks. Our re-search exposes the security vulnerability of these systemsand urges for developing effective defenses.Acknowledgement.This work is supported by the Na-tional Key R&D Program of China (No.2022YFB4501300)and the Fundamental Research Funds for the Central Uni-versities (HUST: No.2023JYCXJJ032). Mauro Barni, Kassem Kallas, and Benedetta Tondi. A newbackdoor attack in CNNS by training set corruption withoutlabel poisoning. In 2019 IEEE International Conference onImage Processing, ICIP 2019, Taipei, Taiwan, September 22-25, 2019, pages 101105. IEEE, 2019. 2 Axel Barroso-Laguna, Edgar Riba, Daniel Ponsa, and Krys-tian Mikolajczyk.Key. net: Keypoint detection by hand-crafted and learned cnn filters.In Proceedings of theIEEE/CVF international conference on computer vision,pages 58365844, 2019. 2 Yulong Cao, Ningfei Wang, Chaowei Xiao, Dawei Yang, JinFang, Ruigang Yang, Qi Alfred Chen, Mingyan Liu, and BoLi. Invisible for both camera and lidar: Security of multi-sensor fusion based perception in autonomous driving underphysical-world attacks. In 42nd IEEE Symposium on Secu-rity and Privacy, SP 2021, San Francisco, CA, USA, 24-27May 2021, pages 176194. IEEE, 2021. 2 Shih-Han Chan, Yinpeng Dong, Jun Zhu, Xiaolu Zhang, andJun Zhou. Baddet: Backdoor attacks on object detection. InComputer Vision ECCV 2022 Workshops, pages 396412,Cham, 2023. Springer Nature Switzerland. 4 Jinyin Chen, Haibin Zheng, Mengmeng Su, Tianyu Du,Chang-Ting Lin, and Shouling Ji.Invisible poisoning:Highly stealthy targeted poisoning attack.In InformationSecurity and Cryptology - 15th International Conference, In-scrypt 2019, Nanjing, China, December 6-8, 2019, RevisedSelected Papers, pages 173198. Springer, 2019. 3 Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, EhsanAdeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou.Transunet: Transformers make strong encoders for medi-cal image segmentation. arXiv preprint arXiv:2102.04306,2021. 2",
  "Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and DawnSong. Targeted backdoor attacks on deep learning systemsusing data poisoning. CoRR, abs/1712.05526, 2017. 3": "Siyuan Cheng, Yingqi Liu, Shiqing Ma, and Xiangyu Zhang.Deep feature space trojan attack of neural networks by con-trolled detoxification. In Thirty-Fifth AAAI Conference onArtificial Intelligence, AAAI 2021, Thirty-Third Conferenceon Innovative Applications of Artificial Intelligence, IAAI2021, The Eleventh Symposium on Educational Advances inArtificial Intelligence, EAAI 2021, Virtual Event, February2-9, 2021, pages 11481156. AAAI Press, 2021. 2",
  "Xuerui Dai, Xue Yuan, and Xueye Wei. Tirnet: Object de-tection in thermal infrared images for autonomous driving.Appl. Intell., 51(3):12441261, 2021. 2": "Chaitanya Devaguptapu, Ninad Akolekar, Manuj M Sharma,and Vineeth N Balasubramanian. Borrow from anywhere:Pseudo multi-modal object detection in thermal imagery. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition Workshops, pages 00, 2019.2 Guneet S. Dhillon, Kamyar Azizzadenesheli, Zachary C.Lipton, Jeremy Bernstein, Jean Kossaifi, Aran Khanna, andAnimashree Anandkumar. Stochastic activation pruning forrobust adversarial defense. In 6th International Conferenceon Learning Representations, ICLR 2018, Vancouver, BC,Canada, April 30 - May 3, 2018, Conference Track Proceed-ings. OpenReview.net, 2018. 7 Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li,Amir Rahmati, Chaowei Xiao, Atul Prakash, TadayoshiKohno, and Dawn Song. Robust physical-world attacks ondeep learning visual classification. In 2018 IEEE Conferenceon Computer Vision and Pattern Recognition, CVPR 2018,Salt Lake City, UT, USA, June 18-22, 2018, pages 16251634. Computer Vision Foundation / IEEE Computer Soci-ety, 2018. 1",
  "Rikke Gade and Thomas B. Moeslund.Thermal camerasand applications: a survey. Mach. Vis. Appl., 25(1):245262,2014. 1": "Jing Gong, Jianhui Zhao, Fan Li, and He Zhang.Vehi-cle detection in thermal images with an improved yolov3-tiny. In 2020 IEEE international conference on power, in-telligent computing and systems (ICPICS), pages 253256.IEEE, 2020. 2 Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy.Explaining and harnessing adversarial examples. In 3rd In-ternational Conference on Learning Representations, ICLR2015, San Diego, CA, USA, May 7-9, 2015, ConferenceTrack Proceedings, 2015. 1",
  "Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Bad-nets:Identifying vulnerabilities in the machine learningmodel supply chain. CoRR, abs/1708.06733, 2017. 1, 3": "Zihan Guan, Lichao Sun, Mengnan Du, and Ninghao Liu.Attacking neural networks with neural networks: Towardsdeep synchronization for backdoor attacks. In Proceedingsof the 32nd ACM International Conference on Informationand Knowledge Management, pages 608618, 2023. 3 Junfeng Guo, Yiming Li, Xun Chen, Hanqing Guo, LichaoSun, and Cong Liu. Scale-up: An efficient black-box input-level backdoor detection via analyzing scaled prediction con-sistency. arXiv preprint arXiv:2302.03251, 2023. 3 Yunze He, Baoyuan Deng, Hongjin Wang, Liang Cheng, KeZhou, Siyuan Cai, and Francesco Ciampa. Infrared machinevision and infrared thermography with deep learning: A re-view. Infrared physics & technology, 116:103754, 2021. 2 Hongsheng Hu, Zoran Salcic, Gillian Dobbie, Jinjun Chen,Lichao Sun, and Xuyun Zhang.Membership inferencevia backdooring. In Proceedings of the Thirty-First Inter-national Joint Conference on Artificial Intelligence, IJCAI2022, Vienna, Austria, 23-29 July 2022, pages 38323838.ijcai.org, 2022. 2",
  "Noor Ul Huda, Bolette D Hansen, Rikke Gade, andThomas B Moeslund.The effect of a diverse dataset fortransfer learning in thermal person detection. Sensors, 20(7):1982, 2020. 2": "Terumi Inagaki and Yoshizo Okamoto.Surface tempera-ture measurement near ambient conditions using infrared ra-diometers with different detection wavelength bands by ap-plying a grey-body approximation: estimation of radiativeproperties for non-metal surfaces. NDT & E International,29(6):363369, 1996. 4 Marina Ivasic-Kos, Mate Kristo, and Miran Pobar. Humandetection in thermal imaging using yolo.In Proceedingsof the 2019 5th International Conference on Computer andTechnology Applications, pages 2024, 2019. 2 Chenchen Jiang, Huazhong Ren, Xin Ye, Jinshun Zhu, HuiZeng, Yang Nan, Min Sun, Xiang Ren, and Hongtao Huo.Object detection from UAV thermal infrared images andvideos using YOLO models. Int. J. Appl. Earth Obs. Geoin-formation, 112:102912, 2022. 2 Takumi Karasawa, Kohei Watanabe, Qishen Ha, AntonioTejero-de-Pablos, Yoshitaka Ushiku, and Tatsuya Harada.Multispectral object detection for autonomous vehicles. InProceedings of the on Thematic Workshops of ACM Multime-dia 2017, Mountain View, CA, USA, October 23 - 27, 2017,pages 3543. ACM, 2017. 6",
  "Yiming Li, Haoxiang Zhong, Xingjun Ma, Yong Jiang, andShu-Tao Xia. Few-shot backdoor attacks on visual objecttracking. CoRR, abs/2201.13178, 2022. 3": "Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deepneural networks.In Research in Attacks, Intrusions, andDefenses - 21st International Symposium, RAID 2018, Her-aklion, Crete, Greece, September 10-12, 2018, Proceedings,pages 273294. Springer, 2018. 7 Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee,Juan Zhai, Weihang Wang, and Xiangyu Zhang.Trojan-ing attack on neural networks. In 25th Annual Network andDistributed System Security Symposium, NDSS 2018, SanDiego, California, USA, February 18-21, 2018. The InternetSociety, 2018. 1 Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Re-flection backdoor: A natural backdoor attack on deep neuralnetworks. In Computer Vision - ECCV 2020 - 16th Euro-pean Conference, Glasgow, UK, August 23-28, 2020, Pro-ceedings, Part X, pages 182199. Springer, 2020. 2, 3",
  "Joseph Redmon and Ali Farhadi. Yolov3: An incrementalimprovement. CoRR, abs/1804.02767, 2018. 6": "Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick,and Ali Farhadi. You only look once: Unified, real-time ob-ject detection. In 2016 IEEE Conference on Computer Visionand Pattern Recognition, CVPR 2016, Las Vegas, NV, USA,June 27-30, 2016, pages 779788. IEEE Computer Society,2016. 2 Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.Faster R-CNN: towards real-time object detection with re-gion proposal networks. In Advances in Neural InformationProcessing Systems 28: Annual Conference on Neural In-formation Processing Systems 2015, December 7-12, 2015,Montreal, Quebec, Canada, pages 9199, 2015. 6 Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pir-siavash. Hidden trigger backdoor attacks. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI2020, The Thirty-Second Innovative Applications of Artifi-cial Intelligence Conference, IAAI 2020, The Tenth AAAISymposium on Educational Advances in Artificial Intelli-gence, EAAI 2020, New York, NY, USA, February 7-12, 2020,pages 1195711965. AAAI Press, 2020. 3",
  "Lichao Sun. Natural backdoor attack on text data. arXivpreprint arXiv:2006.16176, 2020. 3": "Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qi-hui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, YixuanZhang, Xiner Li, et al. Trustllm: Trustworthiness in largelanguage models. arXiv preprint arXiv:2401.05561, 2024. 3 Yuhua Sun, Tailai Zhang, Xingjun Ma, Pan Zhou, Jian Lou,Zichuan Xu, Xing Di, Yu Cheng, and Lichao Sun. Back-door attacks on crowd counting. In MM 22: The 30th ACMInternational Conference on Multimedia, Lisboa, Portugal,October 10 - 14, 2022, pages 53515360. ACM, 2022. 1 Brandon Tran, Jerry Li, and Aleksander Madry. Spectral sig-natures in backdoor attacks. In Advances in Neural Informa-tion Processing Systems 31: Annual Conference on NeuralInformation Processing Systems 2018, NeurIPS 2018, De-cember 3-8, 2018, Montreal, Canada, pages 80118021,2018. 3 Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bi-mal Viswanath, Haitao Zheng, and Ben Y. Zhao.Neuralcleanse: Identifying and mitigating backdoor attacks in neu-ral networks. In 2019 IEEE Symposium on Security and Pri-vacy, SP 2019, San Francisco, CA, USA, May 19-23, 2019,pages 707723. IEEE, 2019. 7",
  "Xingxing Wei, Jie Yu, and Yao Huang. Physically adver-sarial infrared patches with learnable shapes and locations.CoRR, abs/2303.13868, 2023. 1": "Emily Wenger, Josephine Passananti, Arjun Nitin Bhagoji,Yuanshun Yao, Haitao Zheng, and Ben Y. Zhao. Backdoorattacks against deep learning systems in the physical world.In IEEE Conference on Computer Vision and Pattern Recog-nition, CVPR 2021, virtual, June 19-25, 2021, pages 62066215. Computer Vision Foundation / IEEE, 2021. 3 Mingfu Xue, Can He, Shichang Sun, Jian Wang, andWeiqiang Liu. Robust backdoor attacks against deep neuralnetworks in real physical world. In 20th IEEE InternationalConference on Trust, Security and Privacy in Computing andCommunications, TrustCom 2021, Shenyang, China, Octo-ber 20-22, 2021, pages 620626. IEEE, 2021. 1 Mingfu Xue, Can He, Yinghao Wu, Shichang Sun, YushuZhang, Jian Wang, and Weiqiang Liu. PTB: robust physicalbackdoor attacks against deep neural networks in real world.Comput. Secur., 118:102726, 2022. 1",
  "the AAAI conference on artificial intelligence, pages 1299313000, 2020. 3": "CeZhou,QibenYan,YanShi,andLichaoSun.{DoubleStar}:{Long-Range} attack towards depth estima-tion based obstacle avoidance in autonomous systems. In31st USENIX Security Symposium (USENIX Security 22),pages 18851902, 2022. 3 Xiaopei Zhu, Xiao Li, Jianmin Li, Zheyao Wang, and Xi-aolin Hu. Fooling thermal infrared pedestrian detectors inreal world using small bulbs. In Thirty-Fifth AAAI Confer-ence on Artificial Intelligence, AAAI 2021, Thirty-Third Con-ference on Innovative Applications of Artificial Intelligence,IAAI 2021, The Eleventh Symposium on Educational Ad-vances in Artificial Intelligence, EAAI 2021, Virtual Event,February 2-9, 2021, pages 36163624. AAAI Press, 2021.2, 4, 7 Xiaopei Zhu, Zhanhao Hu, Siyuan Huang, Jianmin Li, andXiaolin Hu. Infrared invisible clothing: Hiding from infrareddetectors at multiple angles in real world. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1331713326, 2022. 1"
}