{
  "Abstract": "With recent video object segmentation (VOS) bench-marks evolving to challenging scenarios, we revisit a sim-ple but overlooked strategy: restricting the size of mem-ory banks. This diverges from the prevalent practice of ex-panding memory banks to accommodate extensive histor-ical information. Our specially designed memory deci-phering study offers a pivotal insight underpinning such astrategy: expanding memory banks, while seemingly bene-ficial, actually increases the difficulty for VOS modules todecode relevant features due to the confusion from redun-dant information. By restricting memory banks to a lim-ited number of essential frames, we achieve a notable im-provement in VOS accuracy. This process balances the im-portance and freshness of frames to maintain an informa-tive memory bank within a bounded capacity. Additionally,restricted memory banks reduce the training-inference dis-crepancy in memory lengths compared with continuous ex-pansion. This fosters new opportunities in temporal reason-ing and enables us to introduce the previously overlookedtemporal positional embedding. Finally, our insights areembodied in RMem (R for restricted), a simple yeteffective VOS modification that excels at challenging VOSscenarios and establishes new state of the art for objectstate changes (on the VOST dataset) and long videos (onthe Long Videos dataset). Our code and demo are availableat 1. IntroductionThe rapid progress of video object segmentation (VOS) al-gorithms has motivated the creation of more challengingbenchmarks, as exemplified by VOST on more com-plicated videos with significant object state changes andthe Long Videos dataset featuring extremely long du-ration. These benchmarks elevate the spatio-temporal mod-eling and prompt us to reassess conventional VOS designs:can learning-based VOS modules effectively decipher his-torical information in such challenging scenarios?To delve into this issue, it is essential to focus on memorybanks, which are central to storing past features and feedinginput to VOS modules, and are fundamental in the memory-",
  "Update": ". In light of challenging object state changes ,we rethink the conventional VOS approach of continuously accu-mulating the features into memory banks: despite capturing all theinformation, it complicates the deciphering of relevant features.Conversely, restricted memory banks significantly enhance VOS. based VOS framework . Typically, the memorybanks are managed via the simple intuition of expansion,continuously appending newly sampled frames as the videoprogresses. While this approach is intended to encompassall historical information, thereby enhancing VOS, we real-ize its potential limitation: as videos become longer or morecomplex, these expanding memory banks may overwhelmthe capability of VOS modules to discern reliable features. We investigate this hypothesis by conducting a pilotstudy, named memory deciphering, to quantify the decod-ing capability of VOS modules. In our analysis, we con-tinue to use object segmentation as the proxy to VOS, butshift the prediction target to decoding the object mask atthe initial frame (frame 0) from the memory bank. Thischoice is deliberate based on the principle of controllingvariables: (1) In the VOS framework, the information offrame 0 is implicitly propagated to subsequent frames, en-suring the presence of relevant information for decoding;(2) This prediction target is consistent across frames and al-lows for a fair comparison of decoding efficacy under vary-ing memory sizes. Intuitively, the later frames have rigor-ously richer information than the earlier frames because of",
  "arXiv:2406.08476v1 [cs.CV] 12 Jun 2024": "a larger memory bank, and are thus expected to producebetter decoding results. However, our observation showsthe opposite: the effectiveness of VOS modules in decipher-ing information diminishes with increasingly large memorybanks. Intriguingly, this degradation can be mitigated byselecting a small number of relevant frames in the memorybank, and we observe a significantly better concentration ofattention scores on relevant frames and regions. Therefore,our systematic study reveals a pivotal insight: the expan-sion of memory banks complicates the deciphering of VOSmodules primarily due to redundant information.Inspired by such an insight, we validate its practical sig-nificance through a simple approach: restricting memorybanks to a fixed number of frames. Our concise memorybank facilitates better spatio-temporal modeling and adap-tation to object transformation according to the analysis ofcomplex object state changes , as illustrated in .The effectiveness of our method stems from a curated mem-ory concisely focusing the attention of VOS modules on rel-evant information. Based on this, we delve into the updatingprocess when new features arrive. Our strategy balances therelevance and freshness of frame features, drawing inspira-tion from the upper confidence bound (UCB) algorithm from multi-arm bandit problems.In addition to enhancing the accuracy, restricted mem-ory banks reduce discrepancies in memory lengths betweentraining and inference when compared with conventionalmethods.Typically, VOS modules are trained on shortclips with a few memory frames, so our restricted memorybank better aligns with this setup, even when handling sig-nificantly longer videos during inference. This alignmentopens up opportunities to revisit techniques relying on tem-poral synchronization between training and inference. As acompelling example, we introduce temporal positional em-bedding to explicitly capture the ordering of memory fea-tures a critical aspect often overlooked by previous meth-ods leading to superior temporal reasoning.In conclusion, we make the following contributions:",
  ". Our revisit of restricting memory banks notably en-hances VOS accuracy for challenging cases, cooperatedwith a memory update strategy balancing the relevanceand freshness of frames": "3. Benefiting from smaller training-inference gaps, we in-troduce the previously overlooked temporal positionalembedding to capture the order of memory frames.Collectively, our insights lead to a simple yet strong VOSmethod: RMem, which is plug-and-play for memory-based VOS methods. Our extensive experiments show itsstrengths and establish new state of the art on VOST for object state changes and the Long Videos dataset . 2. Related WorkVOS benchmarks.VOS has evolved through severalbenchmarks. DAVIS is the first exhibiting diver-sity and quality, surpassing early benchmarks .YoutubeVOS further scales up by collecting morevideos. Although they have enabled great progress in VOS,their limited difficulty and video lengths have spurred morechallenging datasets. For example, the average duration inLVOS is more than 500 frames and the Long Videosdataset further extends it to over 1,000 frames, andMOSE increases the difficulty by selecting videos withcrowds and occlusions. To evaluate our insight on the mostdemanding scenarios, we highlight object state changesinvolving noticeable transformations in the existence, ap-pearance, and shapes. Studies on state changes, e.g., VS-COS , mostly utilize ego-centric datasets .In this paper, we primarily select the recent VOST .It combines multiple datasets and provides accurate an-notations. Notably, VOST shows higher complexity andlonger duration than previous YoutubeVOS and DAVIS. Wemainly concentrate on the challenging benchmarks.Memory-based VOS. Memory banks are fundamental forVOS. Earlier approaches treat VOS as on-line learning and finetune networks with memorized fea-tures. Some others approach VOSas template matching but struggle with occluded or dy-namically changing objects.Consequently, recent meth-ods mostly focus on memory reading via either pixel-levelor object-level attention . Object-level memory read-ing , inspired by Mask2Former , excels at ef-ficiency. However, it is less effective for delicate masksor complex scenarios, e.g., VOST , where the objectsare frequently small or cluttered. In comparison, pixel-wisememory reading is moreadopted for its reliable segmentation and it typically asso-ciates the current frame to memory features with attention.Our work differs from previous studies by focusing moreon the general insights of drawbacks of expanding memorybanks and plug-and-play strategies to mitigate such issues,instead of dedicated memory reading architectures.Restricted Memory Banks.Previous studies approachrestricting memory banks mostly from the efficiency as-pect .A notable representative, XMem ,adopts a hierarchical architecture with customized modifi-cations like memory potentiation. In contrast to prior ef-forts, our work explicitly reveal and highlight the accu-racy benefits of restricted memory banks through reducingredundant information, rather than emphasizing efficiency.Moreover, our RMem demonstrates such an insight with asimple plug-and-play enhancement to the VOS framework,avoiding any noticeable increase or reliance on special oper-ators as in XMem. We further suggest that RMems benefitsare not constrained to VOS, where recent works apply-",
  "Closest Frame in Memory ':\"": ". Sketch of Pilot Study. Our memory deciphering analysis emulates decoding the mask on frame 0 from the memory bank featuresto quantify the impact of a growing memory bank on VOS modules, where the desired results in the figure are the ground truth. Fora video shown in Block (a), we visualize its decoding results in Block (b): the masks degrade both quantitatively (yellow curve) andqualitatively, deviating from the desired results. However, selecting a set of concise frames mitigates this issue (blue curve in Block (b)).Therefore, we conjecture that the drawback of a growing memory bank lies in confusing the attention of VOS modules. In Block (c), weuse red lines to indicate highly weighted associations in attention, with thickness denoting the attention score values. As illustrated, thequery F0 focuses less on its most relevant frame after the memory bank expands, with the attention score dropping from 0.247 to 0.056.(2nd row shows ground-truth masks St as the reference. Jmean is the average Jaccard between St0 and S0 over all videos.)",
  "ing large language models to long videos notice the similarbenefits of condensing memory and selecting frames": "3. Pilot Study: Memory Deciphering AnalysisThis section devises our pilot experiments on how an ex-panding memory bank influences the decoding capabilityof VOS modules. Our design emulates the task of VOSbut makes several modifications guided by the principle ofcontrolling variables: the prediction targets and VOS mod-ules are aligned across our pilot experiments, while onlythe frames in the memory bank vary. Such a comparisonenables a clean analysis and reveals the core insight: VOSmodules have limited capability to decode a growing mem-ory bank. Notation and Formulation of VOS.We consider the ex-isting VOS framework as a memory-based encoder-decodernetwork: the encoder E() is a visual backbone encodingthe image It at frame t into the feature Ft; and then, the de-coder D() converts Ft into the segmentation St via readingthe features stored in the memory M[F0:t1], as below,",
  "Ft = E(It),St = D(Ft, M[F0:t1]).(1)": "Here, M[F0:t1] generally comes from saving the featuresat a certain frequency , and the VOS decoder isusually special transformers , e.g., LSTT in AOT .The final objective of VOS is to minimize the differencebetween the predicted mask St and ground truth St. Design of Our Memory Deciphering Analysis.Our pi-lot study separates the variables of the VOS module D()and the prediction target St to clearly analyze the influenceof the memory bank M[F0:t1] under a controlling variable",
  "St0 = D(F0, M[F1:t]),(2)": "where D() is an additional VOS decoder trained for theobjective in Eqn. 2. In practice, we use the original VOSdecoder D() to conduct regular VOS as Eqn. 1, and thenemploy D() only for deciphering the mask S0t for frame 0,to avoid influencing the original VOS. M[F1:t] contains thestored features between frames 1 to t. Note that the featureof frame 0 is excluded from the input M[F1:t] to avoid D",
  "from trivially relying on single-frame memory": "Before delving into the experiments, we emphasize ourreasons for choosing this formulation. (1) Presence of rele-vant information. The procedure in Eqn. 1 resembles propa-gating the masks from historical frames to the current framet, indicating that M[F1:t] contains the information about themask at frame 0. Therefore, decoding the mask on frame0 from M[F1:t] is not a random guess, but should achievehigh-quality results. (2) Identical prediction target. Ourprediction target remains identical for every frame and vary-ing memory size. (3) Cooperating with regular VOS. Weutilize D() as a stand-alone VOS decoder so that the orig-inal VOS process remains unchanged and our pilot studycan utilize the same memory bank.",
  "Positional Embedding": ". RMem Overview. (a) RMem revisits restricting memory banks to enhance VOS (Sec. 4.1), motivated by the insight from ourpilot study. (b) To maintain an informative memory bank, we balance both the relevance and freshness of frames when updating the latestfeatures (Sec. 4.2). (c) Benefiting from smaller memory size gaps between training and inference, we introduce previously overlookedtemporal positional embedding to encode the orders of frames explicitly (Sec. 4.3), which enhances spatio-temporal reasoning. coders in deciphering memory. Then we adopt AOT asthe VOS encoder-decoder, a popular baseline and the topmethod on VOST. Emulating Eqn. 2, we initialize D()from AOTs pretrained decoder D(), and then superviseS0t with a segmentation loss between the ground truth S0.More implementation details are in Sec. B. Hypothesis and Expectations.With an expanding mem-ory bank, the information in M[F1:t] becomes rigorouslyricher at later frames while the prediction target is un-changed. Therefore, we naturally expect the decoded maskSt0 to illustrate stable or better accuracy at later frames, as-suming that the VOS decoder D() is capable of extractingthe relevant features from an increasingly large M[F1:t]. Results and Analysis.Contrasting the expectation above,we observe that masks St0 degrade with a growing memorybank, as shown in (b). To verify that the growingmemory bank is indeed the cause of degradation, we em-pirically bound the memory bank to 8 frames containingthe most relevant and latest information, intuitively: first 7frames and the latest frame in M[F1:t]. According to theblue curve in (b), restricting the memory only to storeconcise features effectively avoids degradation.Inspired by addressing the degradation issue, we proposethat the redundant information is the main negative impactof an expanding memory bank. Otherwise, the degrada-tion should not disappear simply after we select a subset ofintuitively relevant frames. More specifically, this closelyrelates to how VOS methods utilize attention mechanismsto read from memory banks, where the redundant featuresdecrease the attention scores on relevant frames. As di-rect evidence, we analyze the attention scores for decodingSt0 in (c) and observe that the attention scores be-tween F0 and its most relevant memory feature (first framein M[F1:t]) have worse concentration on the correct objectand become scattered in a longer memory bank. Therefore,we conclude that restricting the memory banks with a con-cise set of relevant features potentially benefits the decodingof VOS modules via more precise attention. 4. Method of RMemMotivated by our insight from the pilot study, we proposea straightforward approach highlighting a concise memorybank: restricting the memory with a constant frame num-ber (Sec. 4.1).We then explore the strategies to updatethe memory bank to constantly digest incoming featuresand remove obsolete frames (Sec. 4.2).Finally, the re-stricted memory bank decreases the gap between the mem-ory lengths across the training and inference stages. Thisenables previously overlooked techniques, and we proposea compelling example of temporal positional embedding(Sec. 4.3). The overview of our method RMem (R forRestricted) is in .",
  ". Restricting Memory Banks for VOS": "Design.As indicated in our pilot study (Sec. 3), VOSmodules have limited capability to process large quantitiesof features and thus benefit from a concise memory bankwith less redundant information. To verify this in actualVOS systems, we develop the simple approach of restrict-ing the memory bank to a fixed frame number. In practice, apre-defined small constant number K is the maximum num-ber of frames a memory bank can store, as shown in .The simplicity of our approach makes it a plug-and-playenhancement for the existing VOS framework.At an arbitrary frame t, we simplify the notation of thememory bank by denoting M[F0:t1] as Mt, containingKt K frames. A natural issue of bounded memory Mt is that Kt can reach the limit K at sufficiently large t, mak-ing the digestion of newly arriving features non-trivial, es-pecially when the quality of information is vital for VOS,according to how we address degradation in the pilot study(Sec. 3). Our baseline adopts an intuitively simple yet ef-fective approach (we explore better strategies in Sec. 4.2):selecting the most reliable frame (frame 0) and temporallymost relevant frames (closest frames). Formally, updatingthe memory bank is as below when Kt = K:",
  "where Mt2:Kt1 and Ft are the closest frames, and Mt1 isremoved to create an available slot, as shown in (b)": "Discussion.Our restricted memory is a revisit to previousmethods . However, we are distinct in emphasiz-ing accuracy instead of efficiency. In addition, our RMemalso simplifies them by treating each frame as aconstituent feature map instead of breaking it into smallerregions or pixels ; thus, our strategy can directly applyto a wider range of models. Although more sophisticatedstrategies might further improve our accuracy, a simple ap-proach is already effective (Sec. 5.3).4.2. Memory UpdateUpdating the incoming frames to the memory bank providesinformative cues for VOS modules to decode. Although ourbaseline (Eqn. 3) has already cooperated with the boundedmemory bank, we investigate better methods for updating. Challenges of Memory Update.As shown in our pilotstudy (Sec. 3), improving the conciseness of informationheavily influences the decoding efficacy of VOS modules.Therefore, naive heuristics of random selection or keepingthe latest frames are unreliable (as in Sec. 5.4, memory up-date analysis), since they fail to consider the relevance offrames (random) or suffer from drifting of knowledge (lat-est). To this end, we propose the principles that considerboth relevant prototypical features and fresh incoming in-formation from the latest frames. Memory Update Inspired by Multi-arm Bandits.Ourmemory update problem can be stated as how to select anddelete the most obsolete frame kd from K candidates to cre-ate slots for incoming features. Although not exactly iden-tical, this problem analogizes multi-arm bandit , whichalso concerns optimizing the reward by selecting from afixed number of candidates. Its most inspiring insight for usis balancing the exploitation and exploration with the upperconfidence bound (UCB) algorithm , whose maximiza-tion objective Ok for an option k is as below,",
  "(2 log T)/tk,(4)": "where Rk is option ks average reward, T is the totaltimestamps, and tk is the number of timestamps select-ing k.When applying to our VOS, we re-define Rkas the relevance of a frame for reliable VOS and con-sider (2 log T)/tk as the freshness of memory, intuitively.Then, the deleted frame kd is chosen according to the small-est O1:K. In practice, we define the relevance term Rk usingthe attention scores between frame Mtk and current VOStarget Ft, to quantify the contribution of features from thememory. Under the context of transformers, we assume de-coding the memory bank is as",
  "and assume that St is the scores (after softmax) between Ftand Mt, computed inside the attention. Then, we treat the": "sum of scores as the relevance of a frame in the memory:Rk = sum(Stk), where Stk is the slice of attention scorescorresponding to Mtk. Compared to XMem , which alsouses attention scores for selection, our design differs in se-lecting at the frame level instead of the pixel level, which issimpler and already effective (as in Sec. 5.4).As for the second term in UCB, (2 log T)/tj, we mod-ify it by defining tj as the times a frame has stayed in thememory bank and T as the sum of all the frames stayingtime. This freshness term penalizes long-staying frames andallows refreshing from the latest information. Finally, Okcombines it with the relevance term Rk via a weight bal-ancing their numerical scales.",
  ". Memory with Temporal Awareness": "Motivation.In addition to accommodating the decodingcapability of VOS modules, restricting the memory banksystematically decreases the training-inference discrepan-cies in memory lengths. Specifically, the VOS algorithmsare generally trained on short video clips with a few framesin the memory, while the videos are much longer during in-ference time. Therefore, the number of frames in the mem-ory bank diverges more significantly without our restriction.Such temporal alignment between training and inferenceopens new opportunities for VOS. As a compelling exam-ple, we introduce temporal positional embedding (PE) toenhance spatio-temporal reasoning. Specifically, we noticethat previous approaches overlook the order offrames in the memory, i.e., the temporal relationship amongthe frames are not explicitly considered, while spatial PE iswidely adopted. Considering the vital role of orders in tem-poral modeling, which is commonly addressed with tempo-ral PE in video-based tasks, we conjecture that the distinc-tion of memory sizes between training and inference hin-ders previous methods from employing temporal PE. Design.The objective of temporal PE is to embed explicittemporal awareness into memory and guide the attention inEqn. 5. Although restriction on the memory bank allevi-ates the training-inference shift, the challenges of temporalPE still exist: the optimal memory size K, though muchsmaller than expanding, can still be larger than the training-time memory size Ktrain; (2) the frames in the memory arevarying from 1 to K. To address them, our solution is in-spired by how ViT uses learnable PE and interpolationto address different image resolutions. Similarly, we initial-ize the PE according to Ktrain, denoted as P0:Ktrain1, and thequery Ft having a dedicated PE Pq. Then, the temporal PEfor the memory bank Mt0:Kt1 is P t0:Kt1.",
  ". Experiments5.1. Datasets and Evaluation Metrics": "VOST.We primarily utilize the recent VOST datasetthat concentrates on challenging object state changes. It cu-rates over 700 videos covering diverse object state changes,e.g., changing appearance, occlusions, crowded objects, andfast motion. In VOST, the evaluation metrics are J and Jtr,resembling the average Jaccard over all the frames and theharder last 25% frames corresponding to state changes. Long Videos Dataset.We use the Long Videosdataset to evaluate long-term understanding, similar toXMem . It contains 3 validation videos with more than1k frames. J , F (boundary F measure), and J &F (averageof J , F) are considered for evaluation.",
  "LVOS.We also experiment with the recent LVOS dataset and include the results in Sec. C.5": "Regular and Short Video Datasets.YoutubeVOS and DAVIS are two earlier datasets with shorterduration and easier scenarios compared with VOST. In thispaper, we use them as the pretraining datasets for VOST andthe Long Videos dataset following standard practice ,and conduct analysis in addition to the challenging datasets. 5.2. Baselines and Implementation DetailsOur proposed RMem is a simple and plug-and-play en-hancement for the VOS framework. Without loss of gener-ality, we select AOT and DeAOT as the main base-line because of its top performance on VOST (as in )and simplicity. It adopts ResNet-50 as its encoder anda specially designed long short term-transformer (LSTT)as its decoder.For the memory bank, the original AOTdigests the latest frame and expands the memory continu-ously, while RMem restricts its size to 8 frames. We alsoemploy RMem on other VOS methods in addition to AOT.More details on models and implementation in Sec. B.",
  "DeAOT89.487.491.4DeAOT + RMem (Ours)91.589.893.3": ".Comparison with previous methods on Long Videosdataset . For both baselines of AOT and DeAOT, our RMemshows significant improvement. (Without mention, the results arefrom XMem , denotes our implementation.) quality for the whole video (J ) and maintains better robust-ness for the state-changing frames (Jtr). This is especiallyclear when compared to AOT : the improvement is over3% with our plug-and-play modifications.Long Videos Dataset.As our RMem limits memory ca-pacity, a natural suspicion is that our memory bank per-forms worse in storing information and struggles with long-term modeling. However, our comparison in showsthe opposite. On the Long Videos dataset, our RMem notonly improves upon the baseline AOT and DeAOT mod-els but also outperforms the state of the art XMem model, which utilizes specially designed hierarchical mem-ory banks and memory manipulation operators. Therefore,this further supports our insight on keeping a concise mem-ory bank to accommodate the limited capability of VOSmodules to address expanding memory banks.5.4. Ablation Studies Effect of RMem Components.We analyze each RMemcomponent respect to AOT and DeAOT baselines, as in Ta-ble 3. (1) Restricting memory banks. The most importantinsight from our pilot study (Sec. 3) is to maintain a concisememory bank with relevant information, which motivatesour revisit of restricting memory banks (Sec. 4.1). Accord-",
  "Relev + Fresh39.450.3": ". Ablation study of different memory updating strategieson VOST. We analyze deleting a frame in the memory based onheuristics (Remove) or guided by the relevance and freshness ofthe UCB algorithm (UCB). Our final memory updating strategyusing both relevance and freshness achieves the best performance. ing to (row 1 and 2), a bounded memory bank leadsto significant enhancement in the long and complex VOSTvideos. (2) Temporal positional embedding. In ,we illustrate that adding positional embedding (Sec. 4.3)greatly benefits the spatio-temporal modeling, especiallythe harder Jtr for state changes. (3) Memory update. Werefresh the memory banks by balancing the relevance andfreshness of frames (Sec. 4.2), inspired by the UCB algo-rithm . In rows 4 and rows 5 of , such a strategyeffectively boosts the overall performance. Analysis on Frame Numbers of Memory Banks.Weverify a direct implication of our insight: an expandingmemory bank elevates the difficulty of VOS modules to de-code information. Specifically, we observe the VOS accu-racy under various sizes of memory banks. To avoid theinfluence of hyper-parameter tuning, we utilize the baselinememory update strategy in Sec. 4.1. As in , the per-formance first improves from richer information. Then bothJ and Jtr decrease when the length of memory exceedsthe capability of learned AOT modules, until they becomesimilar to unrestricted memory. Consequently, these resultsdirectly support our insight of restricting memory banks. Memory Update Analysis.Maintaining an informativememory bank is critical for the VOS accuracy, and we pro-pose a UCB-inspired algorithm in Sec. 4.2. analyzesthe key intuition and design choices with AOT. (1) The ini-tial frame is critical in keeping the provided ground-truthinformation: removing the 0-th frame leads to an accuracydrop, and is more profound when scenarios are complex(VOST). (2) Guaranteeing the freshness of information is 48.5 49.0 49.5 50.0 50.5 48.7 49.8 50.3 49.6 49.3 Unlimited MemoryRestricted Memory Max Frame Number in Memory Banks 35.0 36.0 37.0 38.0 39.0 tr 34.7 37.6 38.7 38.0 37.0 Unlimited MemoryRestricted Memory . Impact of memory bank size on VOS, tested on VOST.With more frames in the restricted memory, the accuracy first in-creases and then decreases until it approximates unrestricted mem-ory. This supports the limited deciphering capability of VOS mod-ules and our insight into restricting memory banks.",
  "AOT + RM + SinCos PE37.948.9AOT + RM + Learnable PE39.750.3": ". Comparison of temporal PE strategies on VOST. Basedon restricted memory (RM), our learnable temporal PE (Learn-able) is better than using high-frequency Fourier features (Sin-Cos). Notably, restricting memory is essential for PE. critical, where removing the latest frame leads to the worstaccuracy. (3) Randomly removing frames performs surpris-ingly well but is still worse than our baseline (removing the1st frame, in Sec. 4.1). (4) Using attention scores to reflectthe relevance better removes redundant features (Relev),and it is further enhanced with the freshness term, wherefreshness is especially effective to avoid frames from stay-ing long time in the memory bank, supported by the LongVideo dataset. Finally, the best strategy is our UCB-inspiredalgorithm combining relevance and freshness. Temporal Positional Embedding Strategies.We intro-duce using learnable temporal PE to address the variedframes in the memory banks of VOS in Sec. 4.3. In Ta-ble 5, we analyze another PE strategy of encoding the in-dex into high-frequency features with SinCos functions andfind it performs worse. This is because SinCos is commonlyused in scenarios of a large number or continuous space ofcoordinates (e.g., NeRF ), while learnable embeddingscan better handle a small number of slots (e.g., ViT ),as in the limited memory length during the VOS training.Furthermore, we highlight that temporal PE requires re-stricted memory to function well because of better training-inference temporal alignment in memory lengths. This sup-ports our intuition in Sec. 4.3 and suggests the emergingopportunities from restricting memory banks.",
  "!\"#$!\"#$": ". (Best viewed zoom-in with color.) Qualitative VOS results for object state changes on VOST . We provide two examplesshowing the challenges of object state changes, including slicing, occlusions, distraction from similar objects (other tomatoes), and shapechanges. For both scenarios, using RMem shows advantages in robustly maintaining the masks of the target objects, as highlighted. (Whitepixels are annotated by VOST denoting ignored regions for evaluation, which are hard and ambiguous even for human annotators.)",
  "DeAOT85.282.388.12.24G25.11DeAOT + RMem (Ours)85.382.488.21.53G27.42": ". RMem maintains the accuracy on DAVIS2017 while be-ing more efficient, indicating that RMem can be generally applied,not limited to challenging scenarios. This also aligns with the priorworks and suggests that not having demanding datasets was poten-tially why the accuracy benefits of memory restriction were notclearly revealed previously. Analysis on Regular and Short Video Benchmarks.We highlight the improvement on long and complex VOSdatasets, but we also supplement our analysis on the regu-lar and short video dataset DAVIS2017. As in , ourRMem has relatively the same performance but effectivelyimproves the efficiency. Compared with our improvementon VOST and the Long Video dataset, we conjecture thatthe learned VOS modules (AOT and DeAOT) are alreadycapable of handling shorter video duration and less compli-cated scenarios, even without our concise memory banks.Additionally, this potentially explains that previous studiesexploring restricting memory banks have not ex-plicitly discovered its benefits, probably due to not consid-ering longer and more challenging datasets like VOST. 5.5. Qualitative ResultsWe visualize on two representative videos from VOST that require robust spatio-temporal reasoning in .Video (a) is the kitchen behavior of cutting a tomato intoslices, and it illustrates the challenges of splitting objects,occlusions from hands, and visual distraction from othertomatoes. Without our RMem, the baseline AOT modelfails to maintain the masks for the separated tomato slice,while using RMem correctly remembers this slice at thelater stage of the video (columns 3 and 4). Such regionsare highlighted with the yellow arrows. The other video (b) illustrates another difficulty of object shape transformationand splitting between the box and the aluminum. Althoughthe baseline model without RMem can correctly segmentthe box at the beginning of splitting (column 2), it graduallyloses track of the box and can only concentrate on the dom-inant object. However, our model enhanced with RMemrobustly segments the small regions of the box, indicatingthat its attention association with relevant historical framesis still stable because of our restricted memory. Therefore,we conclude that the quantitative results reveal the difficul-ties of object state changes and support the effectiveness ofour approach. 6. ConclusionThis paper reveals the drawbacks of expanding memorybanks, a conventional design in VOS. Our insight stemsfrom a novel memory deciphering analysis, which sug-gests that the redundant information in growing memorybanks confuses the attention of VOS modules and elevatesthe difficulty of feature decoding. Then, we propose thesimple enhancement for VOS named RMem. At its coreis restricting the size of memory banks, accompanied byUCB-inspired memory update strategies and temporal po-sitional embedding to enhance spatio-temporal reasoning.Extensive evaluation on the recent challenging datasets, in-cluding VOST and the Long Videos dataset, supports ourinsight and effectiveness of RMem. Limitations and Future Work.Our paper prioritizes theanalysis of memory banks and illustrates our insight witha straightforward approach. Therefore, interesting futurework is to combine the intuition from more sophisticatedmethods, such as XMem . Furthermore, our explorationmainly adapts memory banks to cooperate with the capa-bility of VOS modules, while how to improve the decodingability of VOS modules for a huge memory bank is the al-ternative direction and interesting future work. Ali Athar, Jonathon Luiten, Alexander Hermans, Deva Ra-manan, and Bastian Leibe. HODOR: High-level object de-scriptors for object re-segmentation in video learned fromstatic images. In CVPR, 2022. 2, 6, 14",
  "Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-YoungLee, and Alexander Schwing. Putting the object back intovideo object segmentation. In CVPR, 2024. 2": "Dima Damen, Hazel Doughty, Giovanni Maria Farinella,Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Da-vide Moltisanti, Jonathan Munro, Toby Perrett, Will Price,and Michael Wray. Scaling egocentric vision: The EPIC-KITCHENS dataset. In ECCV, 2018. 2 Ahmad Darkhalil, Dandan Shan, Bin Zhu, Jian Ma, AmlanKar, Richard Higgins, Sanja Fidler, David Fouhey, and DimaDamen. EPIC-KITCHENS VISOR benchmark: Video seg-mentations and object relations. In NeurIPS, 2022. 2",
  "Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon JooKim. Video object segmentation using space-time memorynetworks. In ICCV, 2019. 6": "Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, LucVan Gool, Markus Gross, and Alexander Sorkine-Hornung.A benchmark dataset and evaluation methodology for videoobject segmentation. In CVPR, 2016. 2, 6 Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-belaez, Alex Sorkine-Hornung, and Luc Van Gool.The2017 DAVIS challenge on video object segmentation. arXivpreprint arXiv:1704.00675, 2017. 2, 6, 11, 12, 14",
  "A. Demo Video": "In we provide four qual-itative comparison examples between the baseline models(AOT and DeAOT ) and our RMem, with the objectstate changes from both VOST and the Long Videosdataset . Notably, these examples illustrate four chal-lenging scenarios: (1) Object ambiguity: objects have sim-ilar appearances; (2) Slicing: an object is cut into multi-ple slices; (3) Appearance changes: an object has changedits shape and appearances, leading to incorrect VOS masks.(4) Sudden shape changes: the viewpoint changes quicklyand causes variation in shapes of the target object. The fourexamples demonstrate that RMem effectively improves thespatio-temporal reasoning of VOS.",
  "B.1. Model Architecture": "AOT and DeAOT share the common architecture of thememory-based VOS framework.As conceptualized inEqn. 1, we disassemble the VOS framework into the mod-ules of an encoder E() encoding images into feature maps,a decoder D() extracting information from the memorybank, and a segmentation head translating the output fromdecoder into masks. Please note that we have additionallydecoupled the segmentation head from the decoder for clar-ity, compared with Eqn. 1. Encoder.Identical to VOST , we adopt ResNet-50 as the encoder, which achieves competitive perfor-mance while efficient enough to operate on Long Videos.The multiple stages in the ResNet encoder produce 3 lev-els of feature maps {F 4, F 8, F 16} with 1/4, 1/8, and 1/16 the resolution of the original input image, respectively. Fol-lowing the practice of AOT and DeAOT, the deepest featuremap F 16 is the input to the decoder for memory reading,and {F 4, F 8} are provided to the segmentation head as in-put for predicting high-quality masks.Decoder.AOT and DeAOT utilize a specially-designedtransformer to conduct associative memory reading,named Long Short-term Transformer (LSTT). LSTTcomprises three consecutive transformer layers to enhancefeatures in the current frame with the memory bank. Adopt-ing the same notations as Eqn. 1, we conceptually illustratethis process as Eqn. H:",
  "(H)": "where the superscript (l) denotes the layer index of LSTT,ranging from 0 to 2.After the above process, We keepthe implementation details identical to the original AOTand DeAOT. Please refer to them for more detailed con-figuration. Finally, the output feature F (3)treplaces the fea-ture map F 16 from the encoder not enhanced with spatio-temporal information.Segmentation Head.To maintain high-resolution seg-mentation masks, the segmentation process involves a fea-ture pyramid network (FPN) . It accepts F (4)tas theinput feature, uses {F 8, F 16} as shortcut inputs, and up-samples them via the combination of a convolutional layerand a bi-linear up-sampling layer.Temporal Positional Embedding.We introduce tempo-ral positional embedding (TPE) in Sec. 4.3 to enhance thespatio-temporal reasoning ability of models. In practice, weinitialize end-to-end learnable embeddings with the samenumber to the memory length during the training time (e.g.,4 in VOST) and the same dimension to the feature Ft, mark-ing the PE of each place in the memory bank. For simplic-ity, the three LSTT layers in Eqn. H share the same set ofTPE.",
  "B.2. Training": "Loss Functions.Our training procedure utilizes the sameloss functions as AOT and DeAOT: the combination ofbootstrapped cross-entropy loss and soft Jaccard loss .Both loss terms are averaged 1:1 as the final loss value.VOST.The training on VOST follows the orig-inal practice of VOSTs authors,where the modelsare fine-tuned on VOST with pretrained weights fromDAVIS2017 and Youtube2019 . As VOST high-lights spatio-temporal modeling, we follow the authorsimplementation of AOT by using a long sequence lengthof 15 frames during training and this accordingly enables 4 frames in the memory bank.It leverages exponentialmoving averages (EMA) for parameter updates to stabi-lize the training process. The whole training process usesAdamW optimizer, and lasts 20,000 steps with abatch size of 8, on 4A40 GPUs. The initial learning rateis 2104 and it gradually decays to 2105 according toa polynomial pattern . To avoid overfitting, we set thelearning rate of the encoder as 0.1 of the other components.The weight decay is 0.07, which is also identical to AOTand DeAOT. Long Videos Dataset.Following the standard prac-tice , we first train the AOT and DeAOT models onthe DAVIS2017 and YoutubeVOS2019 dataset ,then conduct inference on the Long Videos dataset .However, to support the training of positional embedding,we extend the length of training samples from the original5 frames to 9 frames, to support 4 frames in the memorybanks during the training time. Please note that we alsore-train the baselines under the same setup to ensure a faircomparison.The training procedure leverages the simi-lar optimization setting as described above for the VOSTdataset, including the AdamW optimizer, weightdecay of 0.07, polynomial learning rate decay from2 104 to 2 105, 0.1 scaling of the encoder learn-ing rate, and EMA parameter updates. The only differencefrom VOST is training 100,000 steps with a batch size of16, following the implementation of the original AOT andDeAOT on DAVIS2017 and YoutubeVOS2019 datasets.",
  "B.3. Inference": "VOST.Instead of appending features into memory ata fixed frequency of 5 frames, the authors of VOSTdeveloped a different strategy than on DAVIS2017 andYoutubeVOS2019 to address the CUDA memory issuecaused by higher resolution and longer video duration: thememory bank is bounded by 30 frames and the frequencyof updating memory banks is accordingly L/30, where Lis the length of the video. For our RMem, we follow thefrequency of memory updates set by VOST, but bounds thesize of memory banks to 9 frames, which is significantlysmaller than the original cap of 30 frames. Therefore, ourRMem needs to update the memory banks by removing theobsolete frames, and we describe the details of memory up-date in Sec. B.4 below. Long Videos Dataset.When comparing to the other ap-proaches on the Long Videos dataset (), we primar-ily rely on the VOS performance evaluated by XMem .However, we re-implement the baselines of AOT andDeAOT for a fair comparison with RMem, since XMemhas not released the code for evaluating both methods. No-tably, our re-implementation achieves better performancecompared to XMems reported numbers. In practice, we de- termine the frequency of updating memory banks by L/30to avoid CUDA memory issues, which is similar to the in-ference procedure on VOST. Our RMem shares the sameinference setting as baseline, only restricting the memorybank size to 8 frames. Then, the memory update strategy isidentical to VOST, as described in Sec. B.4.",
  "B.4. Memory Update": "As is described in Sec. 4.2, our RMem balances the rele-vance and freshness of frames in the memory bank usingour algorithm inspired by UCB .Relevance.As mentioned in Sec. 4.2, we use the attentionscores from the transformers in the decoder Eqn. 5 to reflectthe relevance of a memory frame Rk. Since the LSTT de-coder in AOT and DeAOT has three transformer layers, weintuitively select the attention scores from the 0-th trans-former because it is closest to the original image embed-dings Ft and memory features Mt (ablation in Sec. C.3).To stabilize the relevance term and avoid fluctuations, wefurther apply the moving average technique to the relevanceterm. Suppose Rk denotes the relevance values of a mem-ory frame k derived from the latest timestamp, the conse-quent relevance term Rk is updated via:",
  "Rk (1 )Rk + Rk,(I)": "where we set = 0.8 for both VOST and the Long Videosdataset. As we have noticed, using moving average for sta-bilization is a common technique for VOS on long videos,such as in AFB-URR .Freshness.To balance the numerical scales of the rele-vance and freshness terms, we slightly modify Eqn. 4 asbelow,",
  "C.1. Memory Update on the Long Videos Dataset": "We analyze the memory update strategies on the LongVideos dataset using our AOT baseline in Table A, inaddition to the analysis on VOST (). (1) No-tably, we observe consistent improvement from our UCB-inspired memory update strategy combining both relevanceand freshness of frames in the memory. (2) Similar to theresults on VOST, our baseline of removing the 1-st frame inthe memory has competitive performance but is inferior toour final UCB-inspired strategy. (3) The analysis in Table Aalso reveals several intriguing differences between the Long Videos dataset and VOST. Specifically, VOST highly relieson the relevance of frames and the reliable information fromthe 0-th frames because of its complexity in scenarios, whilethe Long Videos dataset highlights the utility of freshness offrames as a consequence of extremely long video duration.",
  "Relev + Fresh89.587.891.2": "Table A. Ablation study of different memory updating strategieson the Long Videos dataset, in addition to VOST (). Weanalyze deleting a frame in the memory based on heuristics (Re-move) or guided by the relevance and freshness of the UCB al-gorithm (UCB). Our final memory updating strategy using bothrelevance and freshness achieves the best performance.",
  "C.2. Balancing Relevance and Freshness": "As mentioned in Sec. 4.2 and Sec. B.4, we balance rele-vance and freshness when updating the memory banks viaEqn. J. Fig. A analyzes the performance under different values on both VOST and the Long Videos dataset. Specifi-cally, a larger denotes relying more on the freshness term.A proper is essential for the UCB-inspired algorithmto improve memory update for both VOST and the LongVideos dataset, and we empirically select = 1.5 becauseit generalizes better to both of the datasets. Interestingly,Fig. A also reveals the difference between VOST and theLong Videos dataset: VOST has more complex scenariosand highlights the utility of relevance, while the long videodataset relies more on freshness due to its extremely longvideo duration. Nonetheless, our final = 1.5 achievesproper balance for both domains.",
  "C.3. Relevance Calculation": "Our relevance term for memory update uses attention scoresto reflect the importance of a frame, similar to previousworks . However, LSTT has three transformer layersand enables two intuitive strategies of relevance calculation:(1) directly using the 0-th layer; and (2) computing the av-erage attention scores of all the transformer layers. TableB compares these two strategies on VOST and the LongVideos dataset. We observe that using the 0-th layer for rel-evance calculation has an advantage in most of the scenar-ios. We conjecture that the 0-th transformer has the largestfidelity to the features of images and memory banks. There-fore, our RMem empirically selects the 0-th transformer forrelevance, as described in Sec. B.4.",
  "C.4. Analysis on Training-Inference alignment": "As discussed in Sec. 4.3, the purpose of temporal positionalembedding is to align the gap between training and infer-ence, as VOS models are trained on short videos but infer-encing on unlimited videos. However, it is also valuableto explore whether it is another approach to address thistraining-inference gap. We compared our Restricted Mem-ory (RM) with 2 approaches: (1) Longer Memory (LM):train the model with longer video clips so that the modelcan fit better on a larger memory bank. (2) More Steps(MS): train the model with more steps. As is shown in Ta-ble C, LM certainly is effective in mitigating the training-inference gap, but it is still worse than our RM. MS exhibitsoverfitting with too many training steps, thus not capable ofaddressing this issue. However, MS can still gain improve-ment through our RM, proving our methods effectivenessfrom another perspective.",
  "C.6. Analysis on YoutubeVOS2019": "Our study concentrates on improving the VOS accuracyfor long and/or complex VOS scenarios. Meanwhile, wealso supplement with analysis on shorter, simpler bench-marks. As indicated in , our RMem demonstratescomparable performance to baselines without RMem onDAVIS2017 , with a notable increase in efficiency. Thisresult underlines the adaptability of our approach across dif-ferent regimes.Further analysis is conducted in the section using theYoutubeVOS2019 benchmark, with shorter video du-ration and easier scenarios. In Table E, we evaluate two set-tings: (1) the influence of only restricting the memory banksizes; and (2) the effect of the full RMem with temporalpositional embedding. Table E (rows 1 and 2) shows that:by limiting the memory banks with the original checkpointprovided by DeAOTs authors, we maintain the same VOSquality. This finding suggests that constraining the memorybanks is a regime-independent strategy.A key aspect of our RMem is temporal positional em-bedding (TPE), which necessitates end-to-end model train-ing on extended sequences. As in Sec. B.2, we increase thetraining sequence length from 5 frames to 9 frames withouttuning the hyper-parameters, ensuring a 4-frame memorybank during the training stage. However, this introduces op-timization challenges, as reflected in the decreased DeAOT performance with longer training clips (Table E, rows 1 and3). Under such a setup and fair comparison, our full RMemhas maintained comparable VOS quality compared with thebaseline (rows 3 and 4). In conclusion, our RMem is alsoapplicable for YoutubeVOS2019, although tuning the op-timal hyper-parameters for training with longer sequencelengths is future work.",
  "DeAOT85.684.880.089.788.04DeAOT + RMem85.584.679.889.488.2": "Table E. Analysis on YoutubeVOS2019 shows that, although notthe primary focus of this paper, our RMem is also applicable forYoutubeVOS2019 with comparable performance with baselines.We first apply restricted memory banks to the original DeAOTcheckpoint (rows 1 and 2). To enable temporal positional em-bedding (TPE), we train DeAOT under a longer sequence lengthand denote such models with (rows 3 and 4).The sub-scripts s and u denote the seen and unseen subsets ofYoutubeVOS2019, respectively.",
  "D. Additional Discussion on Limitations andFuture Work": "We briefly outlined the limitations of our study in Sec. 6 dueto space limits. This section elaborates on more details.As mentioned in Sec. 6, we prioritize the analysis ofmemory banks, and RMem is designed as a straightforwardinstantiation to demonstrate our insight. For this purpose,our study primarily engages with state-of-the-art methodslike AOT and DeAOT . This choice is grounded,especially when common VOS studies are built upon a sin-gle or few preceding approaches due to the complexityof the framework, such as XMem , HODOR , andDeAOT . One potential limitation could be that ourRMem might implicitly depend on the transformer mech-anisms and the affinity calculation in self-attention, whichare adopted in AOT and DeAOT. These mechanisms na-tively support the temporal positional embedding and alignwith our key motivation of focusing the attention scores onrelevant frames (Sec. 3 and ). While future endeav-ors could explore adapting RMem for various VOS meth-ods beyond the ones using transformers, near-future VOSmethods will likely continue to employ a transformer-basedframework, making our current RMem design compatiblewith them.Another aspect mentioned in Sec. 6 is the potential forenhancing RMem with more advanced techniques. Whilethe current simplicity of our approach effectively demon-strates our core insights into managing memory bank ca-pacities, we acknowledge that it can benefit from a moresophisticated design. As especially pointed out in Sec. 6, XMem exhibits an intricate design for efficiently ex-panding memory banks. Though more complex than ourcurrent method of simply bounding memory bank sizes,such advancements could offer greater flexibility and po-tentially improve VOS.Lastly, as discussed in Sec. 6, another option for en-hancement lies in improving the decoding capabilities ofthe VOS framework. Our study maintains the original de-sign of existing methods for a fair comparison, yet futureresearch could explore scaling or modifying VOS architec-tures to further mitigate the challenges posed by expandingmemory banks."
}