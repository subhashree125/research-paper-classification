{
  "InputCompletion": ". Geometry generation from hGCA (blue) from five accumulated LiDAR scans (yellow spheres) on real-world Waymo-open dataset.hGCA is a conditional 3D generative model that can generate geometry beyond occlusions (vehicles, facades) and input field of view (roofs,trees, poles), from sparse and noisy LiDAR scans. Our method is also spatially scalable, completing this whole scene (120 meters) at highresolution on a single 24GB GPU without additional tricks. AbstractWe aim to generate fine-grained 3D geometry from large-scale sparse LiDAR scans, abundantly captured by au-tonomous vehicles (AV). Contrary to prior work on AV scenecompletion, we aim to extrapolate fine geometry from unla-beled and beyond spatial limits of LiDAR scans, taking astep towards generating realistic, high-resolution simulation-ready 3D street environments. We propose hierarchical Gen-erative Cellular Automata (hGCA), a spatially scalable con-ditional 3D generative model, which grows geometry recur-sively with local kernels following , in a coarse-to-fine manner, equipped with a light-weight planner to in-duce global consistency. Experiments on synthetic scenesshow that hGCA generates plausible scene geometry withhigher fidelity and completeness compared to state-of-the-artbaselines. Our model generalizes strongly from sim-to-real,qualitatively outperforming baselines on the Waymo-opendataset. We also show anecdotal evidence of the ability tocreate novel objects from real-world geometric cues evenwhen trained on limited synthetic content. More results anddetails can be found on our project page.",
  ". Introduction": "How can we scalably build large-scale, diverse and realisticdigital worlds for applications in simulation for autonomousvehicles (AV) or gaming and entertainment? Manually au-thoring a realistic scene requires significant effort in creatingindividual objects and positioning them in realistic spatialconfigurations. Procedural models are a promising alterna-tive which back recent AAA games such as No Mans Sky.However, authoring procedural models of objects and envi-ronments are usually time consuming manual tasks. Denselyscanning the world is now an increasingly popular and morescalable option, using Neural Radiance Field (NeRF) basedapproaches. However, these reconstruction methods typi-cally dont capture content beyond what is observed. SparseLiDAR scans from autonomous vehicles a by-productof their development and deployment also provide cuesto the geometry of street environments in the world. Ourwork aims to use these sparse LiDAR scans as input to a",
  "arXiv:2406.08292v1 [cs.CV] 12 Jun 2024": "conditional 3D generative model that learns to extrapolateplausible high-resolution scene geometry.Prior work in the domain has focused on semantic scenecompletion (SSC) from a single LiDAR scan , usingaccumulated sequential LiDAR scans with labeled semanticclasses as supervision . This is useful for AV perceptionto learn to expect 3D semantic occupancy beyond instan-taneous observations. However, using such accumulatedscans as supervision typically results in outputs unsuitablefor simulation, since they have low-resolution geometry andsuffer from heavy occlusions, exacerbated by scans beingtaken from a single drive through a dynamic scene .Moreover, typical LiDAR scanners in AV have a restrictedheight range which prohibits learning to generate scene ge-ometry beyond this limit in SSC. From sparse LiDAR scans,we instead aim to generate high-resolution scene geometryand go beyond the LiDAR range (), to take a steptowards simulation ready scene geometry. To differentiatefrom the task of semantic scene completion (SSC), we nameour task outdoor scene extrapolation. However, for ease ofexpression, we use the terms completion, generation, andextrapolation interchangeably through the rest of the paper.We train and evaluate on synthetic scenes which allow fineand complete geometric supervision while maintaining theability to complete geometry from real LiDAR scans. Weuse a conditional 3D generative model, which is more suitedto this challenging inverse problem, as opposed to prior SSCmethods that typically use discriminative autoencoder.We propose a spatially scalable 3D generative model ofgeometry, with a two-stage hierarchical coarse-to-fine for-mulation, called hGCA. hGCA builds on top of the recentGenerative Cellular Automata (GCA) framework ,which is a 3D generative model that recursively applies localkernels to incrementally grow geometry from a sparse set ofactive cells. GCA was shown to perform competitively withstate-of-the-art for geometry completion from dense indoorscans. The sparsity and locality of GCA allows spatial scala-bility. However, we find that naively applying GCA for finegeometry extrapolation on large outdoor scenes from sparseLiDAR leads to performance deterioration stemming froma lack of global context and the need to use a large numberof recursive steps, the latter motivating our coarse-to-fineapproach. To introduce global context, hGCAs coarse stageuses a GCA conditioned on features from a light-weightbirds eye view planner to generate scene geometry in a low-resolution voxel grid, without losing spatial scalability. Thesecond stage synthesizes finer details with cGCA , gen-erating high resolution voxels augmented with local implicitfunctions that allow promoting the output to a 3D mesh.We train on synthetic street scenes, using data from theCARLA simulator , and a city asset from Turbosquid,using simulated LiDAR scans as input. On synthetic scenes,hGCA outperforms state-of-the-art SSC and indoor scene completion methods on multiple metrics for geometry ex-trapolation. Quantitatively evaluating 3D generative mod-els in the real world is challenging. Qualitatively, we ob-serve that hGCA shows strong sim-to-real generalization toreal LiDAR scans compared to prior work, generating morecomplete and higher fidelity geometry, demonstrated on theWaymo-open dataset . We also demonstrate with exam-ples that despite being trained on limited synthetic content,hGCA can generate some novel content beyond its trainingdata, by taking geometric cues from input LiDAR scans.",
  ". Related Work": "3D Shape Completion. Earlier works on data-driven 3D shape completion regressed a single shape frompartial 3D observation using deep neural networks. learnto complete 3D shapes using partial geometry supervisioncoming from LiDAR scans. Multiple works tackle completion of indoor scenes from dense RGB-Dscans. proposed a hierarchical coarse-to-fine ap-proach for fine-grained completion of indoor scenes. Recentworks have employed deep implicit fields to learnto generate continuous surfaces . We take inspi-ration from these using a coarse-to-fine approach and localimplicit functions at the finest level. Most prior works on out-door scene completion focus on semantic scene completion(SSC) for autonomous vehicles (AV), i.e.completing semantic voxel occupancy given a single LiDARscan using accumulated sequential point clouds with seman-tic labels for supervision. JS3CNet proposes a novelpoint-voxel interaction module for better feature extractionand SCPNet utilizes student-teacher distillation froma multi-frame input teacher, and improves network designwithout any dowsampling modules. While the works showsuitable results for AV perception, the methods produce lowresolution geometry and suffer from occlusions arising fromsupervision, deficient for simulation. We show superior per-formance to both indoor scene completion and SSC methodsfor AV scene geometry extrapolation on synthetic data, aswell as qualitatively improved sim-to-real generalization onthe Waymo-open dataset.3D Generative models , havetypically focused on synthesizing single objects, leverag-ing GANs , and diffusion based generative mod-els . Recent methods in text-to-3D generative mod-els , have shown impressive results in generating novelshapes and small scenes. learn generative modelsof LiDAR scans demonstrating scene level point cloud syn-thesis in autonomous driving. Inspired by the cellular au-tomaton , the GCA framework can generatemultimodal completions for both objects and indoor scenes.GCA scales to scenes as it recursively applies local kernelsto grow a sparse set of active cells in its generative process.This resembles diffusion models where samples . Overview of our method. Given several LiDAR scans, our method generates low resolution completion sT1 using a GCA attachedwith a planner that adds global consistency. Then given sT1 and the input, we upsample the completion using a cGCA into high resolutionvoxel with a local latent xT2 and decode it to obtain the final generated mesh. are generated with a recursive learned denoising kernel, andNeural Cellular Automata . We find that the locality ofGCA fails to capture global context, generating artifacts inlarge scenes. hGCA extends GCA to capture global contextand efficiently generate fine geometry.",
  ". Hierarchical Generative Cellular Automata": "Given LiDAR scans captured from an ego-vehicle, the taskis to generate complete scene geometry, including regionsbeyond the LiDAR range or away from the street. To effi-ciently handle expansive scales of outdoor scenes with finedetail (), hierarchical Generative Cellular Automata(hGCA) proposes a conditional generative model in a two-step, coarse-to-fine manner as shown in . The first stepof hGCA extrapolates the scene in a low resolution voxelrepresentation using a model based on Generative CellularAutomata (GCA) ; a sparse, local and hence spatiallyscalable generative model, which we briefly introduce inSec. 3.1 for completeness. However, the local generation ofGCA can introduce artifacts in extrapolating larges scenesbeyond sensor measurements. We propose to induce globalcontext into GCA by jointly training a light-weight birds eyeview encoder, called planner (Sec. 3.2). Then we transformthe coarse geometry into high-resolution continuous scenegeometry using local implicit functions (Sec. 3.3). To-gether, the proposed method can create large outdoor sceneswith spatial scalability, global consistency, higher fidelityfrom sparse, partial real-world scans.",
  ". Background: Generative Cellular Automata": "Generative process. GCA recursively grows an incom-plete shape to completion as illustrated in step 1 of ,by locally updating occupancies around the current shape.GCA represents shapes as sparse voxel occupancies, s ={(c, oc)|c Z3, oc {0, 1}}, where oc indicates binaryoccupancy of a voxel / cell with its coordinates c. In thefollowing text, we use voxel and cell interchangeably. Givenan observed, incomplete state s0, it generates a completedstate sT by recursively sampling s1:T :st+1 p(|st),(1) where T is a predefined number of transition steps and pis a local transition kernel with parameters . The tran-sition kernel uses a U-Net architecture using sparseconvolutions i.e., the convolution only processes occu-pied cells for efficiency. The transition kernel p is com-puted locally on the neighborhood of the occupied cells,N(st) = {c Z3 | d(c, c) r, oc = 1, c Z3}, i.e.,cells within a radius r from current occupied cells undera distance metric d. For efficient sampling, the transitionkernel is computed for each cell in N(st) independently,",
  "p(oc|st) = Ber(,c),(3)": "where p(oc|st) is a Bernoulli variable with mean ,c esti-mated by the neural network for cell c, given st. This sparseand local generative process of GCA allows more spatialscalability over traditional encoder-decoder methods thatprocess whole scenes at once. In this work, we use a variantof GCA where the transition kernel p is conditioned onboth the initial state s0 and the current state st, improvingconditioning on the input s0 .Training GCA. GCA is trained with infusion training to converge to a desired shape sgt, given s0 and sgt. Infusiontraining supervises the transition kernel p(st+1|st) at eachstep, by defining an infusion kernel",
  "cN (st)qt(oc|st, sgt)(4)": "where the infusion kernel qt(st+1|st, sgt) is factorized percell as in Eq. 2. The infusion kernel for a single cell c,qt(oc|st, sgt) = Ber((1 t),c + t1[c sgt])(5)is a Bernoulli variable with its mean defined as the esti-mated occupancy probability ,c infused with target sgt with weight t = 1t + 0 | t , which increaseslinearly with t.For training input s0 to generate sgt sT , intermediateinfusion states s1:T are sampled from the infusion kernelq(st+1|st, sgt) recursively. For each sampled infusion statest, GCA is trained with binary cross entropy loss against the . Left: (a) Input LiDAR scans. (b), (c) GCA completion in10cm3 and 20cm3 voxel resolution. (d) GCA + planner completionin 20cm voxel resolution. GCA is local and often cannot capturethe global context, generating imperfect completions (pink box) orartifacts (green box).",
  ". Planner": "While GCA has been shown to complete small indoor scenes,we find that it lacks global consistency on extrapolating large-scale scenes. Take the fence in green box in left of forinstance. The walls of buildings are generated inconsistentlyby GCA, at both low and high resolution, showing symptomsof lack of global consistency. This issue is exacerbated insim-to-real inference shown in . We hypothesize thatwhile GCAs sparse and recursive kernel brings scalability, itcannot maintain global context both spatially and temporally.Spatially, the sparse convolutions deliver information onlythrough occupied cells, making it difficult to observe widespatial context without immediate connection. Moreover,the Markov transition kernel transmits no other memoryexcept binary occupancy between transitions, thus inhibitinglong-range \"planning\" across transition steps.Hence, we introduce a light-weight planner module thatprovides the global context of the scene into GCA, whileit maintains the recursive local operations. Specifically, weprovide the consistent birds eye view (BEV) features toGCA kernels, independent of time step t. The features aretrained to plan ahead and predict very low-resolution, yetdense, final occupancy from the initial state s0.BEV features. The planner module is depicted in inside the green box. We first voxelize the input point cloudto initial state s0 and transform it to hr wr birds eyeview (BEV) image. Akin to PointPillars , each pixelon the BEV image contains the feature extracted from thevoxels within the vertical pillar. Each pillar aggregates",
  ". Illustration of GCA attached with planner module": "3 3 zmax voxels (z is the up-axis, zmax is the maximumvoxels along z axis) of the original voxel grid. After thex and y coordinates of occupied voxels are converted intooffsets from the pillar center and z coordinate is normalizedby zmax, a local PointNet processes them to obtaina feature for the corresponding pixel in the low-resolutionBEV image. We further add 2D positional encoding toencode relative position, and pass them through a dense 2DUNet to obtain global BEV features fBEV .Training GCA with BEV features. As shown in ,we use 2D convolutions on fBEV to provide the global guid-ance (shown in pink) in the decoder layers of the sparse UNetof the GCA kernel (shown in blue). Inspired by the spatialconditioning mechanism in SPADE , the 2D convolu-tions compute a mean and variance per pillar. The means andvariances per pillar are added and multiplied to the 3D sparsefeatures falling within the pillar, effectively de-normalizingthem. To ensure fBEV contains necessary information toplan geometry, we apply an auxiliary guidance loss. Specif-ically, fBEV is trained to decode low-resolution 3D occu-pancy Or of shape (hr, wr, zr) (we typically use zr = 4,implying voxels of 2 meter height) with a 2D convolutionlayer. It is supervised with a cross-entropy loss,LBEV = CE(Or, Ogtr ),(7)where Ogtr is the ground truth coarse occupancy in the res-olution of Or. The final loss is a weighted combination ofEq. 6 and 7 with weight ,L = LGCA + LBEV.(8)",
  "Most prior works in semantic scene completion target AVperception, and it suffices to predict occupancy at 20cm3": "voxel resolution given the LiDAR scan. In contrast, hGCAcan create high-resolution surfaces that are suitable for con-tent creation. Given the low-resolution generation of thescene from the previous step, hGCA generates voxels withlatent vectors for local implicit functions in ahigher resolution. hGCAs hierarchical generation achievesefficiency in both space and time complexity for the large-scale under-constrained problem, disentangling geometrycompletion and upsampling into separate steps.Generative process. We utilize a continuous version ofGCA, named cGCA , as our generative model for con-ditional up-sampling into an implicit representation. cGCA extends GCA to generate continuous surface, using an aug-mented state x of each cell c, adding a local implicit latentfeature zc, i.e. x = {(c, oc, zc)|c Z3, oc {0, 1}, zc RK}, where K is the dimension of the latent feature. Thelocal implicit latent features zc are in the latent space of apre-trained auto-encoder, as in . The encoder g encodescoordinate-distance input pairs to x and the decoder f(x)decodes any point in R3 into an unsigned distance to surface.We additionally double the voxel resolution (i.e., 10cm3)from our low resolution completion (20cm3 voxels). Whileone could theoretically obtain continuous surface even withimplicit latent vectors in low-resolution voxels, we observeimproved shape fidelity when using finer resolution voxels.As in GCA, a state xt+1 is sampled at each transition stepxt+1 p(xt+1|xt, x0), where x0 is the initial state. Weset coordinates in the sparse tensor x0 to be the union ofthe input LiDAR scans and our low resolution completionsT1, all provided in a finer voxel resolution. For cells c thatbelong to the input point cloud, we set their latent feature zcusing the pretrained encoder g. We set the initial featureszc to zeros for cells c in x0 that come solely from sT1. Afterrecursively sampling T2 steps from p, the final state xT2 isdecoded into a distance function using the pretrained decoderf, yielding an output mesh. Further details regarding cGCAare in .Training. The training for upsampling is similar to that ofGCA, derived for a continuous case. Specifically, the trainingoperates in the augmented state x, mapped using the encoderg from coordinate-distance pairs. For the initial state x0, weuse the ground truth low-resolution voxels sgt instead of thegenerated output from the first stage sT1. This enforces theupsampler to solely learn to upsample, and the two stagesare trained independently and therefore efficiently. If we usethe outputs sT1, the stochasticity may lead to inconsistentsupervision and training instability. We refer the readers tothe Appendix for further details.",
  ". Experiments": "We evaluate hGCA on street scene generation given Li-DAR scans captured from AVs.We assume registeredsequential scans i.e. relative poses between captures areknown. We train and evaluate on synthetic street scenes fromCARLA and Turbosquid 2 against state-of-the-art meth-ods in Sec. 4.1 for scene extrapolation quality, using groundtruth geometry. We use synthetic LiDAR scans to train,matching the LiDAR scan pattern from Waymo-Open .In Sec. 4.2, we test generalization abilities of hGCA on realLiDAR scans from Waymo-Open and on novel objects,unseen in training. Lastly, in Sec. 4.3, we investigate theplanner module. We provide further analysis in Appendix.Datasets. 1) Karton City is a synthetic city comprised",
  "Clickable link to asset on turbosquid.com": "follow the dataset split from and remove meshes thatcontain unused vertices which makes it hard place the carsin the desired location, thus having total of train/val/testsplit of 2383/339/670 cars. For each scene, we run threesimple trajectories visualized in and randomly se-lect 4615/30/180 center poses used for train/val/test split,respectively. For each center pose, we create accumulated5/10 scans for input by accumulating the scans from thecenter pose and 4/9 other poses, resepectively. For obtainingground truth implicit function, we sample 4,000,000 pointsfrom ground truth mesh with random noise variance 0.03 and0.1 in meter scale, total of 8,000,000 point-distance pairs.",
  "input6.33-34.436.83-38.325.63-5.46-40.425.45-47.714.99-": ". Quantitative results on CARLA and Karton City with 5 and 10 scans given as input. All results except IoU are multiplied by 10 inmeter scale. LiDAR Resim and Street CD evaluates the fidelity of completion and TMD measures the diversity of generation. High LiDARResim uses high elevation LiDAR to evaluate the extrapolation. IoU is computed with ground truth geometry. metrics and baselies.Implementation details. For our coarse stage, we use20cm3 voxel size, T1 = 30 transition steps with radius r = 1.In the upsampling model, we use 10cm3 size, T2 = 15transition steps with radius r = 2. We set BEV loss weight = 0.1 for all experiments, with planner parameters hr =hmax/3, wr = wmax/3, zr = 4 unless stated otherwise. Wetrain and infer (unless specified) on scenes in a volume of38.4 38.4 8 meters with height ranging from meters in a ego vehicle frame, randomly selected from oneof the poses the input scans. At 20cm3 voxel resolution, thiscorresponds to hmax = 192, wmax = 192 and zmax = 40.All experiments were performed on a single 24GB RTX 3090GPU. We obtain and reuse the pre-trained latent autoencoderg and f for local-implicits used in cGCA , trained onindoor scenes from 3DFront , which generalizes wellto our data. We simulate synthetic LiDAR with simple ray-casting and add noise to LiDAR scan coordinates and relativeposes, which improves sim-to-real generalization. We reportresults without LiDAR noise in the Appendix.",
  ". Synthetic Scene Completion": "Scene extrapolation results on synthetic scenes are reportedin Tab. 1. We accumulate 5 or 10 LiDAR scans from randomposes as input. We train all models on combined CARLAand Karton City data for added diversity, but evaluate sep-arately. Output representation for baselines are voxels orcontinuous surfaces when available. hGCA outperforms allbaselines by a margin in reconstruction metrics while gener-ating diverse outputs. For hGCA, we report scores of 10cm3 voxel occupancy and implicit representation, both obtainedafter upsampling. We notice that IoUs are slightly higherwith our 10cm3 voxels, resulting from our unsigned distancefields sometimes not generating clear zero-level iso-surfaces,creating thick meshes for thin structures after thresholding,similar to . We find that the planner trades off diver-sity for quality and global consistency, discussed further inSec. 4.3. We show qualitative results in , and manymore in the Appendix. Deterministic completion models(ConvOcc , SCPNet , JS3CNet , SG-NN )tend to conservatively generate geometry beyond the input,such as bus stops or cars in . These approaches lackmulti-modality and we hypothesize that it limits generation",
  ". Generalization across Domains": "Generalization to real LiDAR scans. We find that hGCAgeneralizes well from sim-to-real, successfully completingcars and trees from Waymo-open LiDAR scans, using 5 accu-mulated scans as input, visualized in , , and morein the Appendix. shows that deterministic baselinesagain exhibit conservative behavior, whereas naive-GCA suf-fers from inconsistency, in one case generating a tree froma house. Both GCA and hGCA complete occluded cars inthe parking lot or even inside of a garage (), wherethe latter has never been seen in the synthetic training data.We hypothesize that this generalization stems from the local-ity of GCA and the two-stage approach where the coarserGCA is more robust to real-world noise. Overall, hGCAcan generate convincing completions, exhibiting geometricquality inferior, yet closer, to the synthetic data it was trainedon compared to baselines, taking a step towards simulation-ready environment creation using AV LiDAR as a contentscanner. Evaluating a 3D generative model on real AV datais challenging. The best source of ground truth geometryavailable to us is using all accumulated scans as in seman-tic scene completion, also shown in , which is highlyincomplete and has limited height range. For example, inthe left of , hGCA generates more complete geometry,but is worser on LiDAR Resim or IoU scores compared tobaselines (SCPNet: 3.94/63.93, SG-NN: 2.97/58.67, hGCA:3.58/63.46), using held-out real scans in the scene for Li-DAR ReSim and IoU against accumulated scans. We discussdifficulties of evaluation on real-world datasets further in theAppendix.Out-of-distribution inputs. A fair concern with trainingon synthetic content is the limited diversity of assets the gen-erative model is trained on, which could be reflected in theoutputs. We show anecdotal evidence of novel asset comple-tion in . On the right, we show geometry completionfrom sythetic LiDAR of a three-wheeler vehicle asset takenfrom Objaverse-XL 3. We verify that no three-wheelersexist in our training data. hGCA generates a convincing",
  "hGCA": ". Visualizations on real-world Waymo-open dataset. hGCAexhibits great sim-to-real performance compared to existing methodwith high fidelity (pink box) and can generate more complete shapesthan accumulated scans (green box). . (a), (b): Completion on LiDAR scan from Waymo-open.(c), (d): Completion on synthetic LiDAR of a three-wheeler assetfrom sketchfab 4 hGCA can realistically complete from tree trunksor three-wheeler cars unseen in training, taking geometric cuesfrom the input (yellow spheres).three-wheeler, respecting the input, better than determin-istic baselines, which we visualize in Appendix. We alsoshow how hGCA can complete this asset with some lowerdensity input scans in Appendix. The left of , showsLiDAR scans from Waymo-open of complex, unique treetrunks. hGCA, trained only on single trunk trees, is able togenerate realistic trees that preserve the observed structure.These results require further rigorous validation, but showpromise towards environment creation with diverse assetsfrom geometric cues taken from real LiDAR scans.",
  ". Ablation study on effects of Planner from 5 input scans. in zr refers to vanilla GCA without Planner module.4.3. Ablation Studies on Planner": "The planner module aims to induce global consistency inhGCA. shows quantatively that it trades of diversityfor completion performance compared to vanilla GCA Wetest different resolutions of planner occupancy predictionin height, indicated by zr. We found that zr = 4 is a goodbalance between providing global context without hurtinggeneration performance on CARLA, which has a more di-verse validation set. We can infer that predicting occupancyin finer vertical resolution (large zr) may be beyond thecapacity of our simple planner module and hinders joint op-timization of the local GCA loss with the global planner loss.We observed that the planner does not boost the performanceof the upsampling module, indicating that local upsamplingdoes not benefit from coarse global context.",
  ". Conclusion": "We proposed hierarchical Generative Cellular Automata(hGCA), a spatially scalable generative model that gener-ates 3D scenes beyond occlusions and input field of viewfrom several LiDAR scans. Our model generates scenes ina two-stage hierarchical coarse-to-fine manner, where thefirst stage generates coarse geometry by providing globalconsistency to GCA with a light-weight planner module.The second stage synthesizes finer details by applying cGCAconditioned on the coarse geometry. On synthetic scenes,hGCA generates plausible scenes with higher fidelity andcompletness compared to prior state-of-the-art works. hGCAdemonstrates strong sim-to-real generalization, capable ofextrapolating LiDAR scans on real-world Waymo dataset.While hGCA takes a step towards content creation from Li-DAR scans, several desiderata remain. Improving fidelity ofgeometry and generating textures, materials etc. is neededfor usability of the completed geometry. For example, in right column, hGCA generates inconsistent roofs. Thecurrent generative process of hGCA is slow, disabling use ofthe model in real-time, which we leave to future work.Acknowledgements. This work was supported by In-stitute of Information communications Technology Plan-ning Evaluation (IITP) grant funded by the Korea gov-ernment(MSIT) [NO.2021-0-01343, Artificial IntelligenceGraduate School Program (Seoul National University)] andCreative-Pioneering Researchers Program through Seoul Na-tional University.",
  "Florian Bordes, Sina Honari, and Pascal Vincent. Learningto generate samples from noise through infusion training. InICLR, 2017. 2, 3": "Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, ZekunHao, Serge Belongie, Noah Snavely, and Bharath Hariharan.Learning gradient fields for shape generation. In Proceedingsof the European Conference on Computer Vision (ECCV),2020. 2 Rohan Chabra, Jan Eric Lenssen, Eddy Ilg, Tanner Schmidt,Julian Straub, Steven Lovegrove, and Richard A. Newcombe.Deep local shapes: Learning local SDF priors for detailed 3dreconstruction. CoRR, abs/2003.10983, 2020. 2, 4 Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, PatHanrahan, Qixing Huang, Zimo Li, Silvio Savarese, ManolisSavva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, andFisher Yu. ShapeNet: An Information-Rich 3D Model Repos-itory. Technical Report arXiv:1512.03012 [cs.GR], StanfordUniversity Princeton University Toyota TechnologicalInstitute at Chicago, 2015. 5, 20",
  "Zhiqin Chen and Hao Zhang. Learning implicit fields forgenerative shape modeling. Proceedings of IEEE Conferenceon Computer Vision and Pattern Recognition (CVPR), 2019.2": "Ran Cheng, Christopher Agia, Yuan Ren, Xinhai Li, and LiuBingbing. S3cnet: A sparse semantic scene completion net-work for lidar point clouds. In Conference on Robot Learning,pages 21482161. PMLR, 2021. 2 Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4dspatio-temporal convnets: Minkowski convolutional neuralnetworks. In Proceedings of the IEEE Conference on Com-puter Vision and Pattern Recognition, pages 30753084, 2019.23",
  "Blender Online Community. Blender - a 3D modelling andrendering package. Blender Foundation, Stichting BlenderFoundation, Amsterdam, 2018. 23": "Angela Dai, Charles Ruizhongtai Qi, and Matthias Niener.Shape completion using 3d-encoder-predictor cnns and shapesynthesis. In 2017 IEEE Conference on Computer Visionand Pattern Recognition, CVPR 2017, Honolulu, HI, USA,July 21-26, 2017, pages 65456554. IEEE Computer Society,2017. 2 Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed,Jrgen Sturm, and Matthias Niener. Scancomplete: Large-scale scene completion and semantic segmentation for 3dscans. In CVPR, pages 45784587. IEEE Computer Society,2018. 2",
  "Angela Dai, Christian Diller, and Matthias Niener. Sg-nn:Sparse generative neural networks for self-supervised scenecompletion of rgb-d scans. In CVPR, 2020. 2, 5, 6, 11, 16, 23": "Matt Deitke, Ruoshi Liu, Matthew Wallingford, HuongNgo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-tian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al.Objaverse-xl: A universe of 10m+ 3d objects. arXiv preprintarXiv:2307.05663, 2023. 6 Alexey Dosovitskiy, German Ros, Felipe Codevilla, AntonioLopez, and Vladlen Koltun. CARLA: An open urban drivingsimulator. In Proceedings of the 1st Annual Conference onRobot Learning, pages 116, 2017. 2, 5, 16, 20 Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, JiamingWang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia,Binqiang Zhao, et al. 3d-front: 3d furnished rooms withlayouts and semantics. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 1093310942, 2021. 6 Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and SanjaFidler. Get3d: A generative model of high quality 3d texturedshapes learned from images. Advances in Neural InformationProcessing Systems, 2022. 2",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-sion probabilistic models. Advances in Neural InformationProcessing Systems, 33:68406851, 2020. 2": "Chiyu Max Jiang, Avneesh Sud, Ameesh Makadia, JingweiHuang, Matthias Niener, and Thomas A. Funkhouser. Localimplicit grid representations for 3d scenes. In 2020 IEEE/CVFConference on Computer Vision and Pattern Recognition,CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 60006009. IEEE, 2020. 2, 4 Diederik P. Kingma and Jimmy Ba. Adam: A method forstochastic optimization. In 3rd International Conference onLearning Representations, ICLR 2015, San Diego, CA, USA,May 7-9, 2015, Conference Track Proceedings, 2015. 23 Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou,Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encodersfor object detection from point clouds. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), 2019. 4 William E. Lorensen and Harvey E. Cline. Marching cubes: Ahigh resolution 3d surface construction algorithm. In Proceed-ings of the 14th Annual Conference on Computer Graphicsand Interactive Techniques, page 163169, New York, NY,USA, 1987. Association for Computing Machinery. 23, 24",
  "Wei-Chiu Ma, and Raquel Urtasun. Lidarsim: Realistic lidarsimulation by leveraging the real world. In CVPR, pages1116711176, 2020. 19": "Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-bastian Nowozin, and Andreas Geiger. Occupancy networks:Learning 3d reconstruction in function space. In Proceed-ings IEEE Conf. on Computer Vision and Pattern Recognition(CVPR), 2019. 2 Mateusz Michalkiewicz, Jhony K. Pontes, Dominic Jack,Mahsa Baktashmotlagh, and Anders P. Eriksson. Deep levelsets: Implicit surface representations for 3d shape inference.CoRR, abs/1901.06802, 2019. 2",
  "Augustus Odena, Vincent Dumoulin, and Chris Olah. Decon-volution and checkerboard artifacts. Distill, 2016. 19": "Yancheng Pan, Biao Gao, Jilin Mei, Sibo Geng, Chengkun Li,and Huijing Zhao. Semanticposs: A point cloud dataset withlarge quantity of dynamic instances. 2020 IEEE IntelligentVehicles Symposium (IV), pages 687693, 2020. 2 Jeong Joon Park, Peter Florence, Julian Straub, Richard New-combe, and Steven Lovegrove. Deepsdf: Learning continuoussigned distance functions for shape representation. In TheIEEE Conference on Computer Vision and Pattern Recogni-tion (CVPR), 2019. 2 Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-YanZhu. Semantic image synthesis with spatially-adaptive nor-malization. In Proceedings of the IEEE Conference on Com-puter Vision and Pattern Recognition, 2019. 4, 23 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,James Bradbury, Gregory Chanan, Trevor Killeen, ZemingLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: Animperative style, high-performance deep learning library. InAdvances in Neural Information Processing Systems 32, pages80248035. Curran Associates, Inc., 2019. 23",
  "Peng Songyou, Niemeyer Michael, Mescheder Lars, Polle-feys Marc, and Geiger Andreas. Convolutional occupancy net-works. In European Conference on Computer Vision (ECCV),2020. 2, 5, 6, 23": "Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, AurelienChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han,Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger,Maxim Krivokon, Amy Gao, Aditya Joshi, Sheng Zhao,Shuyang Cheng, Yu Zhang, Jonathon Shlens, Zhifeng Chen,and Dragomir Anguelov. Scalability in perception for au-tonomous driving: Waymo open dataset, 2019. 22 Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, AurelienChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,Yuning Chai, Benjamin Caine, et al. Scalability in perceptionfor autonomous driving: Waymo open dataset. In CVPR,pages 24462454, 2020. 2, 5, 11, 21, 22",
  "A.1.1Waymo-Open": "We first provide further evaluations on the sim-to-real per-formance of hGCA using Waymo-Open dataset. Asstated in the main manuscript, abundant real-world AV datasuffers from various noises and limited measurement ranges.Most importantly, we do not have the ground truth shapes forour task of shape extrapolation, which challenges systematicanalysis. This section provides sub-optimal quantitative mea-sures and comprehensive qualitative results to demonstratethat we can faithfully generate realistic scenes given partialand noisy real-world measurements.As a means for quantitative evaluation, we use the accu-mulated scans as a pseudo ground truth as in semantic scenecompletion works and analyze its performancewith LiDAR ReSim and IoU as demonstrated in our results ofsynthetic datasets. We randomly chose 202 scenes and usedfive scans as input. Then, the generated scene is comparedagainst accumulated data using all scans. While the accumu-lated scans are denser variations of the given measurement,",
  "implicit4.524.500.9756.50": ". Quantitative results on Waymo with 5 scans given as input.All results except IoU are multiplied by 10 in meter scale. LiDARResim evaluates the fidelity of completion and TMD measures thediversity of generation. Unlike synthetic results, LiDAR ReSimuses same elevation angle as the input and IoU is computed withaccumulated scans. they are noisy and highly incomplete measurements in aconfined height range, as shown in , 10, 11 and 12. Wedevise the evaluation metrics to adapt to the limitation of thereference data. When computing the LiDAR ReSim score,we simulate LiDAR at the same height as the input LiDARscan instead of using a higher elevation LiDAR. Likewise,we compute IoU only on regions visible from LiDAR scansas evaluated in semantic scene completion . There-fore, neither of these metrics assesses the performance ofextrapolation beyond the LiDAR height range and occlusion. reports the performance on the shape completionwithin the measurement range, only trained with limitedsynthetic content. Similar to synthetic results, hGCA outper-forms all baselines in LiDAR ReSim and IoU by a margin.The results indicate that our completion is closer to the densemeasurements of real-world geometry than existing methods.Interestingly, the IoU of the continuous completion outper-forms the initial voxel occupancy (10cm) in the real-worldanalysis in whereas the voxel occupancy achieveshigher IoU values in our synthetic experiments ( ofthe main paper). Recall that our high-resolution upsamplingsometimes fails to create a narrow structure, as unsigneddistance fields may obscure the exact zero-level location inhigh-frequency details. We observe that the accumulatedscans also experience similar ambiguity due to the inherentnoise in the real-world measurement. As the noisy scansserve as the ground truth, the possible misalignment due tothe thick implicit generation may be evaluated as a faithfulgeneration. Such a phenomenon again shows difficultiesin the quantitative evaluation of generative models on real-world datasets.We further visualize various generation results observedfrom diverse viewpoints in , 10, 11 and 12. Despitethe limited measurement range and noisy input, our methodcan extrapolate the input measurement into large-scale real-world scenes in a scalable way. , 11 and 12 alsoshow comparison against other baselines. Deterministicbaselines (SCPNet and SG-NN ) exhibit conser- . Additional completion visualizations on real-world Waymo-Open dataset on 100m scenes. Yellow spheres indicate input. hGCA isspatially scalable, completing this whole scene (100 meters) at high resolution on a single 24GB GPU without additional tricks. hGCA caneven extrapolate hills (bottom) from real-world scans.",
  "hGCA (10cm3)2.412.111.771.712.302.081.821.69": ". Chamfer distance between the ground truth geometryand completions by varying sparsity. Sparse scene and sparsecar indicates a scenario where we sparsify the regions of entirescene (including ground) and only the car, respectively. Chamferdistance above ground are reported and we report average distanceof k = 3 generations for generative models (GCA, hGCA). hGCAgeneralizes well to sparse, novel data. vative behavior, leaving holes in the ground and partiallycomplete buildings or trees. Nave GCA, while it is a gener-ative baseline, suffers from inconsistent generation, whichresults in overlapping structures (pink boxes in ).On the other hand, hGCA can generate unseen geometrywhile maintaining global consistency in various real-worldsettings. For example, the input shown in the right columnof misses a significant portion of the buildings as afence occludes them. Nonetheless, hGCA faithfully gener-ates detailed facades of buildings.",
  "A.1.2nuScenes": "To test generalization to a new sensor configuration, we showresults on the nuScenes dataset in . While 64-beamLiDAR was used in Waymo and our synthetic training set,nuScenes scans were obtained using 32-beam LiDAR, whichis a significant domain shift. Nevertheless, hGCA general-izes to nuScenes out-of-the-box and is robust to the domainshift. We observe some failures on nuScenes (bottom of), across the street, where the model hallucinatesstructures in extremely sparsely scanned regions. We specu-late that training with simulated 32-beam LiDAR scans asinput will enhance the quality of completion by reducing thedomain gap for sparse inputs.",
  "A.2. Generalization to Various Input Conditions": "As partially demonstrated in sim-to-real results, our gener-ative pipeline can robustly handle input variations not ob-served in the training data. This section provides additionalquantitative evaluations of shapes generated from differentinput conditions. Input Sparsity.We first demonstrate the performance ofhGCA given inputs of varying densities of the entire scene.We place a three-wheeler4, verified to be unseen from train-ing data, in the center on a flat ground and simulate LiDAR",
  "Clickable link to asset on sketchfab.com": "scans captured from a simple trajectory. Then, we vary theinput density by randomly sampling 5%, 10%, and 50% ofthe accumulated scans of two settings: the whole scene andonly the car.We report Chamfer distance between the completion andthe ground truth in and visualize the completionson both settings in and 15. hGCA generates themost accurate geometry compared to all baselines in everyscenario according to . The completions of sparsescans for both the scene and the car show that vanilla GCAand hGCA still generate reasonable completions for a novelobject. Because other baselines (SCPNet , JS3CNet ,SG-NN ) utilize global features, they create random ar-tifacts when the input is a severely sparse scene (5% and10% in ). Such challenging scenarios are effectivelyhandled with local generations of GCAs, demonstrating su-perior performance on generalization. As it is impossibleto control the input quality or provide accurate object-wisesegmentation in actual scans, the robustness of GCA maypave the way toward practical large-scale scene generation. Varying Number of Input ScansIn addition to randomsamples, we test a more realistic variation of densities, col-lecting simulated scans exhibiting occlusions. We train mod-els with five and ten scans on synthetic scenes (Karton Cityand CARLA ) and evaluate results on inputs with differ-ent numbers of scans. hGCA can also stably create scenesgiven various numbers of input scans.We report quantitative results in and visualizerandom samples in for the extreme case, where onlya single scan is provided. Quantitatively, hGCA outperformsother baselines by a large margin in LiDAR Resim and showscompetitive (second best) performance on IoU and StreetCD, where the best method differs depending on the dataset.While hGCA demonstrates superior generalization perfor-mance to a single scan compared to other baselines, it suffersfrom degradation with the lack of evidence in the input. Forexample, in the second column of , the input scanprovided in a limited height range results in the ambiguitybetween the facade of the building and the fence.We also test our methods by completing dense accumu-lated scans. For CARLA, we accumulated 80 nearby scansfrom a random pose; for Karton City, we gathered all thescans, which have an average of 132 scans, as input. Wereport quantitative results in and visualize comple-tions randomly in . hGCA outperforms previousmethods on all reconstruction methods in Karton City andperforms competitively (second best) in CARLA. While SG-NN reports best results on CARLA, we observe that SG-NNstruggles to create scenes beyond the sensor range, such asfaces of buildings (first column) or trees or cars (second col-umn) in . In contrast, hGCA successfully generatesreasonable geometry in our qualitative examples. . Visualizations on real-world nuScenes dataset on 100m scenes. Yellow spheres indicate input. hGCA is spatially scalable,completing this whole scene (100 meters) at high resolution on a single 24GB GPU without additional tricks.",
  "input12.72-19.2612.87-21.7710.14-": ". Quantitative results on CARLA and Karton City with a single scan given as input. All results except IoU are multiplied by 10 inmeter scale. LiDAR Resim and Street CD evaluates the fidelity of completion and TMD measures the diversity of generation. High LiDARResim uses high elevation LiDAR to evaluate the extrapolation. IoU is computed with ground truth geometry.",
  "input5.68-45.775.09-62.365.58-": ". Quantitative results on CARLA and Karton City with a many scans (CARLA: 80 scans, Karton City: average of 132 scans) given asinput. All results except IoU are multiplied by 10 in meter scale. LiDAR Resim and Street CD evaluates the fidelity of completion and TMDmeasures the diversity of generation. High LiDAR Resim uses high elevation LiDAR to evaluate the extrapolation. IoU is computed withground truth geometry.",
  "A.3. Planner Feature Visualization": "For further understanding of the planner, we provide visual-izations of outputs and features of planner. showsthe input, completion of GCA equipped with planner, roughdense occupancy Or of planner, and PCA visualizations ofBEV feature. For the PCA visualization, we project the2D BEV features of the planner to RGB using the first 3principal axes of PCA. We observe that the final completionfollows the rough dense occupancy prediction of the planner.This demonstrates that the planner acts as a memory that per-sists through the Markov process of GCA and plans aheadpersisting BEV feature solely with initial state s0. Also, weobserve that BEV feature learns some global context thatdistinguishes some semantic classes trained without any se-mantic supervision. We presume that checkerboard artifactsarise due to deconvolution layers of unet .",
  "A.4. Effects of LiDAR noise": "We investigate the effects of input LiDAR noise on trainingdata. During training on synthetic data, we add Gaussiannoise of standard deviation 0.01 in meter scale to the coordi-nates of each points and add noise to the pitch angle of thepose with standard deviation 0.02 in degree scale to simulatethe LiDAR noise produced in data acquisition in real-world. visualizes the completion of hGCA with and withoutadding noise during training. We observe that adding noiseis crucial in terms of fine sim-to-real generalization, espe-cially on the ground. While we simulated the LiDAR noisewith simple ray-casting and Gaussian noises, one could fur-ther reduce sim-to-real generalization by employing moresophisticated noise, such as .For completeness, we report quantitative results com-pared to existing methods without adding noise during bothtraining and validation in and 8. Similar to resultswith noise, hGCA outperforms baselines on reconstruction",
  "A.5. Space and Time Complexity": "In this section, we further analyze space and time complexityof our model. We first investigate the GPU memory usagefor hGCA. In Table. 9, we report the GPU memory require-ments by varying the completion size in one Waymo scene,visualized in top of . Given an input of size 40 wmeters, we perform 3 completions and report the maximumGPU memory usage for the coarse completion and the up-sampling module. We find that hGCA can scalably generatefine geometry up to 40 120 meters without any tricks ona single 24GB GPU, demonstrating the superior scalabilityof hGCA by only employing efficient sparse convolutionsand planner. We also observe that only 4.8GB is required forthe coarse completion on 120 meter scene, demonstratingthe efficacy of the planner module. While our ablation studywas conducted on only a single scene, we find that GPUmemory usage can vary heavily depending on a scene. For time complexity, we find that coarse completion ofour method takes 3 seconds and upsampling takes about 10seconds (including IO) to create the final mesh in CARLAwith 3090 GPU. Indeed our method is slow, whereas othersingle inference methods (SCPNet ) typically takearound 0.1 seconds on A100 GPU to create the comple-tion in 20cm voxel resolution. We expect faster inferenceusing half-precision or more recently developed sparse con-volution libraries, such as torchsparse , but we leave itto future work.",
  "B.1. Karton City": "Karton City is a synthetic city comprised of 20 blocks, ob-tained from the Turbosquid marketplace5 for 3D asset. Wesplit 20 blocks into 12/3/5 train/val/test splits and re-combine4 blocks in each split randomly to generate 300/30/60 uniquetrain/val/test scenes of size 140 140 meters.Of the12/3/5 split, 7/2/3 are city blocks and 5/1/2 are suburbanblocks. For realistic environment of scenes we re-combinecity and suburban blocks separately, and generate 200/20/40,100/10/20 scenes for train/val/test splits of city and suburbanscenes, respectively, which we visualize in . We placeShapeNet cars on each side of the main street to simulateparked cars. The number of cars on each side of the streetfollow a Poisson distribution with lambda 2, with maximum7 cars. We uniformly distribute the location of the cars andmove them if collisions occur. The cars placed on the street",
  "input6.14-36.636.76-39.515.41-": ". Quantitative results on CARLA and Karton City with 5 scans given as input, both trained and evaluated without noise. All resultsexcept IoU are multiplied by 10 in meter scale. LiDAR Resim and Street CD evaluates the fidelity of completion and TMD measures thediversity of generation. High LiDAR Resim uses high elevation LiDAR to evaluate the extrapolation. IoU is computed with ground truthgeometry.",
  "input5.19-44.025.33-49.894.56-": ". Quantitative results on CARLA and Karton City with 10 scans given as input, both trained and evaluated without noise. All resultsexcept IoU are multiplied by 10 in meter scale. LiDAR Resim and Street CD evaluates the fidelity of completion and TMD measures thediversity of generation. High LiDAR Resim uses high elevation LiDAR to evaluate the extrapolation. IoU is computed with ground truthgeometry.",
  ". Maximum GPU usage for 40 w meter completion onone Waymo scene (top visualization in Fig 9). w denotes the widthof the completion in the first row of the table and the unit of GPUmemory is GB": "for train/val/test split, respectively. For each center pose, wecreate accumulated 5/10 scans for input by accumulatingthe scans from the center pose and 4/9 other nearby poses,respectively. In CARLA, obtaining ground truth mesh isnon-trivial. Therefore, we leverage extra LiDAR sensorsother than the LiDAR sensor to collect input scans, to obtainthe ground truth geometry. We additionally place 5 LiDARsensors with high elevation angle, having relative offsets of (0, 0, 0), (0, -9.6, 3), (0, 9.6, 3), (0, -19.2, 3), (0, 19.2, 3) me-ters from the position of LiDAR sensor placed on simulatedego-vehicle to collect input scans, where x-axis and z-axisrefer to the front and up direction of the ego-vehicle. We ob-tain ground truth surface points by accumulating points fromscans acquired from additional LiDAR sensor, visualizedin top of . For obtaining implicit function, we sam-ple 4,000,000 points from ground truth surface points withrandom noise variance 0.03 and 0.1 in meter scale, total of8,000,000 point-distance pairs. The distance for each pointsare computed with nearest neighbor against the ground truthsurface, since ground truth mesh is not available.",
  ". Karton City visualization. City (top) and suburb (bot-tom) scenes are visualized with three simple trajectories (red, yel-low, green) for simulating a drive": "rics. For each center pose, we create accumulated scans thatserve as input by accumulating the scans from the center poseand 4 other poses within 50 meters. We remove dynamicobjects from the point clouds using annotated bounding boxtracks for both input and accumulated point clouds. Dueto noisy dynamic label annotations, some sparse dynamicpoints lie after bounding box filtering. Thus, we further per-form erosion to the accumulated points in voxel resolutionof 10cm3.",
  "B.4. Lidar Simulation": "For all the experiments, we simulate synthetic LiDAR usingthe LiDAR beam angle and rotation-speed parameters fromthe Waymo-Open dataset . The Waymo LiDAR capturesfull 360 degrees and results in range image dimension of64 2650 pixels. For details on the sensor specificationsee . To simulate LiDAR on Karton City, we generatea curve by interpolating the ego-vehicle poses and simulatea rotating beam along this curve. This simulation, thuscorrectly captures rolling shutter effects. To obtain groundtruth points in CARLA, we use 512 channels LiDAR withfield of view (-30, 30) and range of 75 meters. . Visualization of CARLA dataset. Top: Ground truth sur-face points obtained from additional sensors. For visualization ofdensity, we intentionally render with low density. Bottom: Decodedimplicit function for supervising upsampling module. Inconsistent,sparse regions (yellow) lead upsampling module to unstable train-ing. Thus, to remove sparse regions for supervision, we train onlyon regions visible from the street, which may be incomplete (pink),but provide dense and accurate geometry for supervision. However,upsampling is a local operation and upsampling stage of hGCA canbe trained with incomplete data.",
  "C.1. hierarchical Generative Cellular Automata": "Training Upsampling Module. We train the upsamplingmodule by minimizing the log-likelihood of the data distri-bution, which we defer to cGCA for further details. Wetrain on synthetic data by combining CARLA and KartonCity like other methods. However, as mentioned in Sec. B.2,obtaining ground truth mesh is difficult for CARLA, andwhile the ground truth points obtained from additional sen-sors may be dense enough for 10cm3 voxel resolution, weobserve sparse surface points in occluded regions from thestreet, such as interiors of the building (). Naive train-ing of upsampling module with sparse surface points ledto unstable training of local implicit latent feature, whereupsampling results varied inconsistently depending on train-ing step. Therefore, for training the upsampling module onCARLA, we supervise with ground truth augmented statex on regions visible from the road, as visualized in bottomof , which tend to be dense. We observed that theupsampling module can generalize to complete scenes evenwith training on incomplete ground truth, since upsamplingis a local operation. During training, we train on combinedCARLA and Karton City dataset with rate 15% and 85%,which led to stable training. Neural Network Architecture. For sparse convolutionnetwork of our coarse completion and upsampling module,we employ the same MinkowskiUNet as in GCA and cGCA , respectively. We additionally append 3Dpositional encoding with 128 dimension to the features ofthe input sparse tensor. For planner module, local point netconsists of a fully connected layer that transforms the normal-ized coordinates to 32-dimension feature followed by 3 fullyconnected residual block and another fully connected layerwith a 32-dimension feature output. The residual block con-sists of 2 fully connected layers with 32 hidden dimensions.After the local pointnet, we add 2D positional encoding tothe features and pass it through a 2D UNet6 to obtain a2D feature of dimension 128. Lastly, we employ 5 convolu-tional blocks, which consists of two convolutions of kernelsize 3, for obtaining 4 SPADE features that compute themean and variance per pillar for denormalization and onerough occupancy prediction.Other Details. We use MinkowskiEngine for sparseconvolutions. For all GCAs we use the infusion scheduler oft = 0.15 + 0.005t, and obtain the last state with additionalmaximum likelihood estimation instead of randomly sam-pling. We use Adam optimizer with constant learningrate 5e-4 and clip gradient with maximum norm of 0.5. Forour coarse completion model, we use batch size of 6 andfor the upsampling module, we crop a scene into quartersand use batch size of 3. We train the low-resolution GCAattached with planner for 400k steps and upsampling cGCAfor 300k steps which takes roughly 5/4 days, respectively,with a single 3090 GPU. Note that the two models can betrained independently.",
  "C.2. Baselines": "Generative Celluar Automata . We use the officialimplementation released from the authors7. For fair compar-ison, we use the same hyperparameters as our model. Weuse cGCA of voxel size 20cm3.Convolutional Occupancy Networks We use the3D grid resolution of 64 version from the official implemen-tation released from the authors8. For obtaining occupancyrepresentation, since we cannot obtain watertight mesh forneither Karton City nor CARLA, we make occupancy forpoint in the sampled point-distance pairs that have distance tosurface below 5cm. We additionally sample 100,000 pointsin the block range uniformly to create unoccupied points.SG-NN . We compare with the state-of-the-art indoorscene completion network. We use the official implemen-tation released from the authors9. We use the SG-NN topredict the occupancy in 10cm3 voxel resolution. JS3CNet and SCPNet We compare withJS3CNet and SCPNet , state-of-the-art outdoorsemantic scene completion methods. We use the officialimplementation released from the authors1011. We adapt themethod to our setting by changing the semantic class outputto binary variable representing occupancy. We observe aclass imbalance problem during training, where there aremuch more empty voxels than the occupied ones. We findthat weighing the loss 3:1 for occupied to empty cells per-forms best for both models. While the original semanticscene completion works uses 20cm3 voxel resolution, weadditionally train on 10cm3 voxel resolution for JS3CNet.For 10cm3 model, we modify the output resolution to 10cm3",
  "C.3. Other Implementation Details": "All of our methods are implemented using PyTorch .For all point cloud to voxel conversion, we first round pointcloud into 10cm3 voxels and use floor operation on the co-ordinates of voxels to create 20cm3 voxels. For any modelsexcept the upsampling stage of hGCA, we train on a datasetcombined with CARLA and Karton city having a rate of 50%for each dataset. For methods that utilize unsigned distancefields (cGCA, hGCA), we create mesh in 5cm3 voxel resolu-tion with marching cubes using the unsigned distancevalues of the voxels. For IoU and street CD evaluation, weobtain points close to surface by sampling voxels in 5cm3 resolution that have implicit distance below 0.5. For Con-vOcc , we evaluate IoU and street CD by sampling fromthe points created from the mesh, which represents the sur-face of the completed shape in occupancy representation.We use blender for visualization. For visualizationsoverlayed with input, such as , we render input pointsif a point is either in front of a mesh or is less than 0.5 metersback from the mesh in the rendering view.",
  "D. Evaluation Metric": "High LiDAR ReSim evaluates the fidelity of the completionbeyond the LiDAR range focusing on regions visible fromthe street, visualized in . It computes the Chamfer dis-tance between a ground truth LiDAR scan and a re-simulatedLiDAR scan from a pose distant from the center after thecompletion. The metric avoided evaluating inconsistent ge-ometry in interior walls of the buildings in ground truthgeometry (green in ).Given an origin in the ego-vehicle frame, we set a regionof interest R to be a box of 38.4 38.4 meters. We takethe scanned input point cloud X within R and generate",
  "(d) IoU Mask(e) Regions for Street CD": ". Evaluation visualization. (a) Ground truth geometry in Karton City. (b) Input scans. (c) High LiDAR Resim of GT from a novelpose. (d). Ground truth occupied regions for IoU evaluation. (e) Regions for evaluating street CD. High LiDAR ReSim and IoU capturesgeometry above the input LiDAR range, while it does not capture inconsistent building interiors (green). Street CD is the only metric thatcan evaluate completion of occluded geometry from road, such as side-walk side of the car. completion Y . We select two poses p1, p2 P from theego vehicle trajectory that enters and leaves the region ofinterest R. The selected poses are distant from the center,which and inside R, making the re-simulated LiDAR scanfar from the input scan taken from the center while makingthe LiDAR ReSim free of occlusions occurring outside ofR. We found only 3 out of 1440 (180 scenes 2 posesper scene 4 test configurations with Karton City/CARLAdataset with 5/10 input scans) poses overlapped with theinput pose, indicating that the selected poses for evaluationare mostly unique. We perform lidar simulation from posespt one the completion Y in mesh representation. If thecompletion is in voxel representation, we convert it intomesh using marching cubes and average the ChamferDistance between the GT and re-simulated LiDAR scan. Forhigh range LiDAR used from evaluation, we use a 128-beamLiDAR with elevation angle (-30, 30), which acquiresdenser scans than the input captured with 64-beam LiDAR,to cover the range above the input scan, visualized in (c)of Fig 20. Thus the Chamfer distance (CD) for input X isdefined by",
  "D.1. IoU": "IoU is evaluated on the visible regions in 20cm3 voxel reso-lution from the street following the previous semantic scenecompletion (SSC) works . In contrast to SSCthat computes IoU against accumulated LiDAR scans, wecompute IoU against ground truth geometry from the visibleregions are obtained using high elevation LiDAR, used forHigh LiDAR Resim, and covers regions beyond the inputLiDAR range, visualized in (d) of . To obtain thevisibility mask, we perform TSDF fusion from all theposes in a single drive for each block of interest and obtainthe TSDF values for the block with grid of voxel resolution10cm3. We set the grid to visible only if the TSDF valueis bigger than -0.3 and convert the mask into 20cm3 voxelresolution. Street CD includes evaluation on geometry completelyoccluded from the ego-trajectory, such as the sidewalk sideof parked cars, visualized in , which neither HighLiDAR ReSim nor IoU (pink) can evaluate. On Karton Citydataset, where the scene is a simple crossroad junction, wecompute Chamfer distance between the generated geome-try against GT, only on the objects on the main street. Toevaluate objects above the ground, we remove it for boththe completion and ground truth by simply thresholding thez-axis with 20cm."
}