{
  "Abstract": "Traditional unsupervised optical flow methods are vul-nerable to occlusions and motion boundaries due to lack ofobject-level information. Therefore, we propose UnSAM-Flow, an unsupervised flow network that also leverages ob-ject information from the latest foundation model SegmentAnything Model (SAM). We first include a self-supervisedsemantic augmentation module tailored to SAM masks. Wealso analyze the poor gradient landscapes of traditionalsmoothness losses and propose a new smoothness defini-tion based on homography instead. A simple yet effectivemask feature module has also been added to further ag-gregate features on the object level. With all these adap-tations, our method produces clear optical flow estimationwith sharp boundaries around objects, which outperformsstate-of-the-art methods on both KITTI and Sintel datasets.Our method also generalizes well across domains and runsvery efficiently.",
  ". Introduction": "Optical flow estimation involves finding pixel-levelcorrespondences between video frames, which has broadapplications such as video understanding , video edit-ing , and autonomous driving .Following the latest trend of deep learning in computervision , most recent methods have modeled theoptical flow problem under the supervised learning frame-work , where ground-truth labels are used to train the networks. However, ob-taining such labels for real-life videos is especially difficultsince it usually requires precise calibrations across multiplesensors, leading to prohibitively high annotation costs .This drawback makes these supervised techniques hard tobe applied to large-scale real applications.Due to the high annotation costs, much recent work hasfocused on the unsupervised training of optical flow .Instead of ground-truth labels, unsupervised flow networks",
  ". Our UnSAMFlow utilizes object-level information fromSAM to generate clear optical flow with sharp boundaries": "rely on two key principles to define losses . Firstly, brightness constancy assumes that thecorresponding points across frames should maintain similarlocal appearances. Secondly, the optical flow field shouldbe spatially smooth. However, these assumptions are com-promised at occlusion regions , where foregroundobjects cover background appearances, and around motionboundaries , where the motion is cut off abruptly.These issues are pervasive in real applications and haveposted great challenges to unsupervised optical flow .Fundamentally, the issues with occlusions and motionboundaries both stem from the low-level nature of opticalflow, where object-level information is generally missing.To better handle occlusions, it is important to understand thespatial relationships and interactions between objects. Also,optical flow should be smooth only within the same con-tinuous object region, while sharp motion boundaries areallowed near object edges. Thus, object-level informationcould play a key role in refining unsupervised optical flow.Indeed, some previous methods have explored aggregat-ing object information, using semantic segmentation to helpoptical flow . However, though convenient,the use of semantic segmentation is not precise because itdoes not distinguish different instances of the same seman-tic class, which may have drastically different motions. Itis also constrained by the limited number of classes definedand may not recognize novel objects in the open world.In comparison, the latest Segment Anything Model (SAM) may be a better option. SAM is a general-purpose",
  "arXiv:2405.02608v1 [cs.CV] 4 May 2024": "image segmentation model pre-trained on very large and di-verse datasets. It can separate different instances and hasshown impressive zero-shot performances on objects notseen in training. In addition, SAM detects objects of var-ious scales and levels, segmenting small object parts (suchas hands and arms) as well. This can reduce complexity andhelp differentiate motions of object parts separately.So motivated, we integrate SAM as additional object-level information to enhance unsupervised optical flow,which can be achieved through three novel adaptations.We first adapt the semantic augmentation module from Se-mARFlow to enable self-supervision based on SAMmasks (Sec. 3.3). Moreover, we enforce smooth motionwithin each SAM segment using a new regional smoothnessloss based on homography (Sec. 3.4). This approach effec-tively rectifies numerous inconsistent flow outliers. Lastly,we design a mask feature module to aggregate features fromthe same SAM mask for robustness (Sec. 3.5).Our method significantly outperforms previous methodson both KITTI and Sintel benchmarks throughboth quantitative (Sec. 4.3) and qualitative (Sec. 4.4) eval-uations. Notably, our network achieves 7.83% test erroron KITTI-2015 , outperforming the state-of-the-art UP-Flow (9.38%) and SemARFlow (8.38%) by a clearmargin. As the examples show in , our method pro-duces much clearer and sharper motion that is consistentwith the SAM masks. Extensive ablation studies also jus-tify the effectiveness of each proposed adaptation (Sec. 4.5).Further analysis shows that our method generalizes wellacross domains (Sec. 4.6) and runs efficiently (Sec. 4.7).In summary, our contributions are as follows. To the best of our knowledge, we are the first to effec-tively combine SAM with unsupervised optical flowestimation, which helps learning optical flow for wide-range real-world videos without ground-truth labels.",
  "Unsupervised optical flowTraditional methods [18, 36,": "71] optimize optical flow based on brightness constancyand local smoothness. These constraints have been trans-formed to photometric and smoothness losses in early unsu-pervised networks . Since then, more specific mod-ules have been proposed, including occlusion-aware adjust-ments , iterative refinement , learnedupsampler , self-supervision , dataset learn-ing .The latest SMURF based onRAFT has also achieved outstanding performances. Segment Anything Model (SAM)Segment AnythingModel (SAM) is a recent general-purpose foundationmodel for image segmentation tasks. The model is trainedon enormous high-quality annotated images and is designedto accept prompts (points, boxes, etc.) to retrieve objectmasks. Its trained model can transfer well zero-shot to newdata distributions and thus has been applied to many visiontasks and applications such as object tracking , videosegmentation , neural rendering , and med-ical imaging Combining optical flow and object informationTo ag-gregate object-level information, many previous methodshave combined semantic or instance segmentation modelsoff-the-shelf as object cues to help refine optical flow .Given semantics, most methods reasonthe rigid motions of objects and conduct refinement basedon geometric techniques such as homography , epipo-lar geometry , and SfM . SemARFlow isa latest neural network that incorporates semantics on thefeature level and through self-supervision, which we followclosely. Besides, joint training has also been explored tobenefit both optical flow and segmentation tasks based onsemantic consistency and occlusion reasoning .One concurrent work, SAMFlow , also combinesSAM with optical flow. However, we focus on unsupervisedflow as opposed to their supervised flow. Furthermore, ourmethod imports SAM outputs instead of SAM features, soour trained network can automatically be reused by any seg-mentation model as long as it generates SAM-style objectmasks. These discrepancies make our method more flexibleand feasible in real applications without needing labels.",
  ". Problem formulation": "Unsupervised optical flowGiven two consecutive RGBframes I1, I2 RHW 3, unsupervised optical flow es-timation aims at estimating the dense optical flow fieldF12 RHW 2 without using ground-truth labels. SAM mask detectionFor each input frame It (t{1, 2}), we can compute its SAM masks Mt={0, 1}ntHW , which is composed of the binary masksof the nt objects found in It. The masks are generated byprompting SAM using a grid of around 1k points along withpost-processing such as NMS to drop redundant masks.To utilize SAM masks in optical flow training, one keyquestion is how to effectively process, represent, and aggre-gate these masks in the network. Different from semanticor instance segmentation, SAM masks do not identify thesemantic classes of each object, so one-hot representationsare not applicable.Also, the number of detected masks",
  ". Examples of object crops selected from KITTI andSintel using SAM for semantic augmentation (Sec. 3.3)": "nt may vary from sample to sample. Moreover, the rela-tionship between these masks and all pixels is not strictlyone-to-one. Instead, a pixel can belong to multiple differentmasks, for instance, in the case of embedded objects likecars and wheels. Conversely, some pixels may not be as-signed to any mask at all. These technicalities have postedgreat challenges, and we will show how we tackle these is-sues in later chapters. Two problem settingsFor the sake of practicality, weconsider two settings. In the first setting, we do not useSAM masks as additional inputs to the network. Instead,we only apply SAM during training to generate better losssignals, so SAM is not needed at inference time once train-ing is completed. In addition, for the second setting, wealso input SAM masks to the network to generate object-level mask features. This setting should yield better perfor-mances, albeit with the trade-off of extra overhead duringthe generation of SAM masks at inference time.In this paper, we mainly propose three adaptations to uti-lize SAM information in the flow network, namely the se-mantic augmentation (Sec. 3.3), the homography smooth-ness loss (Sec. 3.4), and the mask feature module (Sec. 3.5).These three adaptions can be plugged in independently. Theformer two only need SAM during training, so they canbe applied to both problem settings. The last one involvesadding SAM inputs and new learnable weights to the net-work, so we only apply it in the second setting.",
  "We first build our baseline network from ARFlow , withsome simple adaptations suggested by SemARFlow such as adding the learned upsampler network. Our net-work structure is shown in": "EncoderWe use a simple fully convolutional encoder(a) to extract a feature pyramid {f (2)t, f (3)t, , f (6)t}for each input image It (t {1, 2}), where the l-th levelfeature f (l)thas resolution (H/2l, W/2l). DecoderWe adopt the iterative decoder used in previouswork as our decoder. The decoder starts from acoarse level zero estimate F (7)12 = 0 and iteratively refinesthe estimate to finer levels. b illustrates one iterationthat refines from estimate F (l+1)12 to the finer F (l)12, whichhas resolution (H/2l, W/2l). A learned upsampler network(similar to the one in RAFT ) is applied to upsampleF (2)12 by 4 times to generate our final flow estimate F12 =F (2)12 on the original resolution (H, W).In b, we also highlight in red the optional mask fea-ture module (to be discussed in Sec. 3.5), which requires theSAM masks M1, M2 as additional inputs to the decoder andis thus only included in our second problem setting men-tioned in Sec. 3.1. See more details in Appendix A.2. LossWe adopt the same photometric loss ph as inARFlow , which is a linear combination of three dis-tance measures (L1, SSIM , and Census loss ) be-tween input frames and the frames warped by F12 andF21. Occluded regions estimated by bidirectional consis-tency check are disregarded when computing ph.In addition, we also combine a semantic augmentationloss aug (Sec. 3.3) and a homography smoothness loss hg(Sec. 3.4), so our final loss is",
  "(f)": ". An example of why traditional boundary-aware smoothness loss works poorly. Sample from Sintel final (ambush 5, frame#11). (a) Original image superimposed with SAM full segmentation; (b) Image patch; (c) Optical flow estimate from our baseline modelsuperimposed with the SAM boundary (black); (d) Gradients of the traditional boundary-aware smoothness loss; (e) Gradients of ourproposed homography smoothness loss; (f) Illustration of the poor landscape of traditional smoothness loss. Note that for both gradientsin (d)(e), we use loss definitions based on L2 norm for better visualizations. See Sec. 3.4 and Appendix A.6 for explanations.",
  ". Semantic augmentation as self-supervision": "Inspired by SemARFlow , we adopt a similar seman-tic augmentation process to improve our network by self-supervision during training. However, we extract semanticsfrom SAM instead of semantic segmentation . OverviewAfter estimating the flow F12 for inputs I1,I2, we manually apply some transformations T1, T2 to I1,I2, respectively, to obtain the augmented I1, I2. The newflow after transformation F12 can also be generated at thesame time since all transformation parameters are known.We then run another forward pass of the network to inferflow for the augmented inputs I1, I2, which is then self-supervised by F12 using L1 loss (i.e., the aug in Eq. (1)).The transformations T1, T2 mentioned above not only in-clude appearance transformations (on brightness, contrast,random noise, etc.), 2D affine transformations (translation,rotation, scaling), and occlusion augmentation (cropping),as proposed by ARFlow , but also contain a special se-mantic augmentation that involves input semantics, as pro-posed by SemARFlow , which we discuss next. Semantic augmentationDuring semantic augmentation,new objects are copied and pasted across samples. For ex-ample, we may crop out a car object from another randomsample and paste it into the current sample used for training.An augmented simple motion is also applied to the croppedobjects. This transformation utilizes the semantic knowl-edge and creates realistic samples with new occlusions.In contrast to SemARFlow , which picks objectcrops of specific classes such as cars and poles using seman-tic segmentation, our method utilizes SAM masks withoutclass labels. Consequently, we select key objects among the SAM masks by finding those masks that overlap the mostwith other masks. This is based on the heuristic that key ob-jects typically consist of multiple object parts that can alsobe detected by SAM. Some example key objects selectedare shown in . See more details in Appendix A.3.",
  ". Homography smoothness loss": "Our second adaptation comes from the motivation that ob-ject segmentation can be used to formulate a more precisesmoothness constraint to better regularize the optical flowfield.We first analyze the issues of previous traditionalsmoothness losses and then show how we resolve those is-sues with the help of SAM . Issues of previous smoothness lossesMost previous net-works define their smoothness loss based on the second-order derivatives of the flow field . Optical flow fieldF12 is a two-dimensional function of point p = (x, y).Previous smoothness losses are typically in the form of",
  ",": "(2)in which wx, wy are the edge-aware weights to avoidpenalties across object boundaries, where motion is notnecessarily continuous. Such weights are usually derivedfrom image edges, which often coincide with object bound-aries .In our case, we can obtain more accurateboundaries from SAM masks. However, we find that theseboundary-aware smoothness definitions work poorly.One example is shown in . The patch (b) ex-hibits a rightward motion of the blade, occluding the nearbysnow background, which barely moves. We show our base-line flow estimate, as well as the object boundary, in c. We can see that the estimated flow is not consistent with theobject boundary due to occlusion (part of the snow regardedas moving together with the blade). In this case, the smooth-ness loss mostly comes from around the flow boundary, andso does its gradient (d). This gradient signal is veryweak since the boundary only takes up a very smaller re-gion, so its update is confined within only the small localneighborhood around the flow boundary.Furthermore, we show that the landscape of the broadlyused boundary-aware smoothness loss is problematic. Weexamine the smoothness loss of the patch while graduallytranslating the flow patch horizontally until it roughly fitsthe object boundary provided by SAM. The results are vi-sualized in f. We can see that the optimal solutionindeed finds the flow that is most consistent with the objectboundary since we do not penalize across object boundaries.However, such solution lies in a very steep local minimumof the loss, while in contrast, the landscape around our cur-rent estimate is rather flat, meaning that any local changearound the current estimate makes little difference to theloss. This vividly explains why traditional boundary-awaresmoothness losses are very hard to optimize in training. Regional smoothness based on homographyTradi-tional boundary-aware smoothness (Eq. (2)) works poorlysince its definition and gradient are too local. To resolvethis issue, our idea is to define smoothness based on objectregions instead of object boundaries.Specifically, the inaccurate flow values in c canbe understood as outliers in the same object region (snow).Thus, parametric models, such as homography, can be usedto fix these outliers.For each object region of interest(found though occlusion estimation ), we first es-timate its homography with RANSAC using the reli-able correspondences provided by the current flow estimate.We define criteria to reject the estimated homography withlow RANSAC inlier rate (see details in Appendix A.4). Arefined flow can then be generated for that object using ho-mography. We compute the L1 distance between our currentestimate and the refined flow as our homography smooth-ness loss hg in Eq. (1). Our homography smoothness lossresults in non-local gradients (e), which strongly en-forces smoothness by regions.One alternative, though, is to directly use the refinedflow as our output, so the homography works through post-processing instead of loss signals . Nevertheless, westill prefer defining losses because in that case, homogra-phy and SAM are only needed during training. Empirically,we do not see big differences between their performances.",
  ". Our proposed mask feature module (Sec. 3.5)": "masks to a full segmentation representation, where everypixel is assigned to exactly one mask.Specifically, we first sort all current object masks by thesize of their area. For pixels that belong to multiple masks,we assign it to the one that has the smallest area. For pixelsthat do not belong to any mask, we create a new back-ground object mask to cover all these pixels. Mask feature moduleOur mask feature computationsare highlighted in b. Given the mask Mt and imagefeature f (l)t , we use a mask feature module to generate g(l)t .Similar to the warping and correlation of image features f (l)tin the original ARFlow , we also warp mask featuresg(l)tand compute their correlations, which are then concate-nated into the input of flow estimator network in b.The detailed structure of our mask feature module is de-picted in . We first transform the SAM masks Mt tofull segmentation representation and downsize it to the l-level resolution. We then compute a new feature from f (l)tand apply max-pooling by segmentation. Specifically, foreach object segmentation, we apply max-pooling among allfeatures of that object and copy the pooled feature to allthose pixels, yielding a pooled feature map g(l)t . Finally, weconcatenate the feature before max-pooling with g(l)t , andextract our mask feature g(l)t . This module aggregates fea-tures on the higher object level and can, thus, compensatethe original pixel-level image features. See more details inAppendix A.5.",
  ". Datasets": "We conduct experiments on KITTI and Sintel datasets and follow the same training data schedules fromprevious methods . For KITTI , we firsttrain on raw sequences (55.7k samples) and then fine-tuneon the multi-view extension subset (5.9k samples). For Sin-tel , we first train on raw frames (12.5k samples) pro-vided by ARFlow and then fine-tune on clean and finalpasses together (2.1k samples). We also adopt the Sintelsub-splits used in ARFlow , which divide the originaldataset by scenes into two subsets of 1k samples, for two-fold cross validation. Images from the test scenes have beenexcluded from the raw sequences for both datasets.",
  "Ours (+aug +hg +mf)1.262.013.791.47.835.676.4014.982.6M": ". KITTI benchmark errors (EPE/px and Fl/%). Metrics evaluated at all (all pixels), noc (non-occlusions), bg (background),and fg (foreground). +aug: semantic augmentation module; +hg: homography smoothness loss; +mf: mask feature module. *:SAM used in training; : SAM used in inference. Numbers with parentheses indicate that the same evaluation data were used in training.",
  ". KITTI test errors (Fl-all/%) compared with other unsu-pervised semantic optical flow methods. -: data not available": "and homography smoothness modules after 150k iterations.In terms of Segment Anything Model , we use theoff-the-shelf default ViT-H pretrained model, which gener-ates an average of 63.7 object masks for each KITTI sam-ple and around 82.9 masks for each Sintel sample .For data augmentation, we follow ARFlow and in-clude appearance transformations (brightness, contrast, sat-uration, hue, gaussian blur, etc.), random horizontal flip-ping, and random swapping of input images. We resize theinputs to dimension 256 832 for KITTI and 448 1024for Sintel before feeding into the network.",
  ". Benchmark testing": "Our benchmark testing results are shown in Tabs. 1 and 2.Our final models with all three adaptations significantlyoutperform state-of-the-art unsupervised methods on bothKITTI and Sintel datasets on almost all evalu-ation metrics. Our final model achieves 7.84% error rateon KITTI-2015 test set, which is much better than UP-Flow (9.38%) and ARFlow (11.80%, the back-bone network that we adapt from). All these results showthe benefits of utilizing SAM models in unsupervised opti-cal flow training.In Tabs. 1 and 2, we can also see that our errors declineprogressively as we incrementally add each proposed mod-ule to the network. This justifies the effectiveness of all ourproposed adaptations. Also, for the setting that we do notuse SAM masks as network inputs (+aug +hg), our modelalso outperforms state-of-the-art methods on both datasets.This implies that our approach has the potential to enhanceunsupervised flow networks solely by optimizing training,guided by SAM, without introducing any additional com-putational overhead during inference.Notably, our networks exhibit substantial improvementsfrom SAM particularly on real datasets, such as KITTI , compared with animation images in Sintel . Thisis because SAM is mostly trained on real-life images, so itproduces masks of higher quality for KITTI than for Sintel.Tab. 3 shows the comparison among current semantics-guided optical flow methods. Our model guided by SAM",
  ". Qualitative results": "Figs. 6 and 7 show some qualitative examples of our fi-nal model, compared with previous state-of-the-art meth-ods. We can see that our network outputs better flow aroundobjects with much sharper boundaries, which are consistentwith the SAM mask inputs. Our method can also handle dif-ferent lighting conditions (dark shadows, bright reflections)better thanks to the robust masks provided by SAM.",
  ". Sintel (final pass) test qualitative examples (sample: cave 3 frame 16; market 4 frame 47). See more examples in Appendix B.2": "Loss weightsWe tune the loss weights waug and whg inTab. 4. We also compare with the settings where either waugor whg equals zero, which means we turn off the semanticaugmentation or homography smoothness module. The re-sults show that our current setting works the best, and theablation of each module results in loss of performance. Smoothness definitionsIn Tab. 5, we can see that our re-gional smoothness loss based on homography works signifi-cantly better than the traditionally used edge-aware smooth-ness loss based on image edges or SAM boundaries. Theseresults are consistent with our analysis in Sec. 3.4. Mask feature modulesWe also experiment some otherways of aggregating mask features and image features inTab. 6. Residual refers to adding the processed mask fea-tures to image features as a residual connection. The resultsshow that our current setting works slightly better than us-ing residual connections.",
  ". Generalization ability": "We show that our flow network guided by SAM exhibitsgreat generalization ability across dataset domains in Tab. 7.Specifically, we train on one of the datasets (KITTI orSintel ) and then test directly on the other without fine-tuning. Our final model guided by SAM obtains clearly bet-ter results than our baseline model without SAM.",
  ". Conclusion": "We propose UnSAMFlow, an unsupervised optical flow net-work guided by object information from Segment AnythingModel (SAM), with three novel adaptations, namely seman-tic augmentation, homography smoothness, and mask fea-ture correlation. Our method achieves state-of-the-art re-sults and exhibits visible improvements. LimitationsOur performance relies on the accuracy ofSAM masks, which may be undermined for samples withserious lighting issues, artifacts, or motion blur. The lackof semantic classes in the SAM output also makes its objectinformation incomplete, awaiting future improvements.",
  "Mingyu Ding, Zhe Wang, Bolei Zhou, Jianping Shi, ZhiwuLu, and Ping Luo. Every frame counts: Joint learning ofvideo segmentation and optical flow. In AAAI, pages 1071310720, 2020. 2": "Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, PhilipHausser, Caner Hazirbas, Vladimir Golkov, Patrick VanDer Smagt, Daniel Cremers, and Thomas Brox. Flownet:Learning optical flow with convolutional networks. In ICCV,pages 27582766, 2015. 1 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image isworth 16x16 words: Transformers for image recognition atscale. In ICLR, 2021. 1 Martin A Fischler and Robert C Bolles.Random sampleconsensus: a paradigm for model fitting with applications toimage analysis and automated cartography. Communicationsof the ACM, 24(6):381395, 1981. 5, 14",
  "Alexander Neubeck and Luc Van Gool.Efficient non-maximum suppression. In International Conference on Pat-tern Recognition, pages 850855. IEEE, 2006. 2": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,James Bradbury, Gregory Chanan, Trevor Killeen, ZemingLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: Animperative style, high-performance deep learning library. InNeurIPS, pages 80248035. Curran Associates, Inc., 2019.6",
  "Qiuhong Shen, Xingyi Yang, and Xinchao Wang. Anything-3d: Towards single-view anything reconstruction in the wild.arXiv preprint arXiv:2304.10261, 2023. 2": "Xiaoyu Shi, Zhaoyang Huang, Dasong Li, Manyuan Zhang,Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, andHongsheng Li. Flowformer++: Masked cost volume autoen-coding for pretraining optical flow estimation.In CVPR,pages 15991610, 2023. 1 Leslie N Smith and Nicholay Topin.Super-convergence:Very fast training of neural networks using large learn-ing rates. In Artificial Intelligence and Machine Learningfor Multi-domain Operations Applications, pages 369386.SPIE, 2019. 6",
  "A.1. Network structures": "Why ARFlow as backbone?We choose ARFlow asour backbone in light of the following considerations. Our main goal is to investigate how SAM can helpwith unsupervised optical flow instead of pushing the bestpossible performances by all means. Therefore, adoptinga simple yet effective model, such as ARFlow, works bet-ter for our research.",
  "ARFlow is especially light-weight and easy to train. Pre-vious related work, SemARFlow also adopts theARFlow backbone, so we follow them to borrow similarideas from SemARFlow as well": "Previous research has shown that ARFlow can be eas-ily adapted to have close to UPFlow performances whilemaintaining simplicity . We follow those sugges-tions and build our own baseline model, evaluated in theexperiments section. Why is there no comparisons with SMURF?Admit-tedly, SMURF has achieved outstanding performanceson unsupervised optical flow estimation. However, we donot compare with them in the experiments section for thefollowing reasons. As mentioned above, our goal is to see how SAM canhelp with unsupervised optical flow instead of pushing thebest possible performances by all means. We compareour models with the baseline model that does not applySAM to show how SAM is effective, while other previousmethods are shown as references to help understand inabsolute terms how our adapted version performs. SMURF uses a larger architecture RAFT , and ar-guably, its success highly relies on its technical designssuch as full-image warping, multi-frame self-supervision(which requires training a tiny model for each trainingsample separately), as well as its extensive data augmen-tations. We could definitely keep these technical designsin our model as well to enhance performances.How-ever, they add great complexities to our network and maygreatly increase the computational costs of our experi-ments. Therefore, we choose to stay simple and focusmore on how to inject SAM information effectively.",
  "A.2. Decoder computation": "As shown in b in the main paper. For each iterationon level l, the decoder takes in image features f (l)1 , f (l)2andthe flow estimate from the previous level F (l+1)12(and alsomasks M1, M2 if the mask feature module is turned on), andoutputs the refined flow estimate at the current level F (l)12.A 1-by-1 convolution layer is first applied to f (l)1to unifythe feature channel sizes of different levels to be the samenumber 32. This module enables us to reuse the same fol-lowing modules on all levels. We first upsample F (l+1)12 by2 times to the same resolution F (l)12 as the features at thislevel through a simple bilinear interpolation. The upsam-pled flow is then used to warp f (l)2as",
  "f (l)1 (p) = f (l)2 (p + F (l)12),p(3)": "which can be implemented through a grid sampling process.We then compute the correlation between f (l)1and f (l)1through a 9 9 neighborhood window, yielding a flattened81-channel correlation output. The correlation is then con-catenated with the upsampled flow F (l)12 and the 1-by-1convolutional features as the input to the flow estimator net-work. Note that if the mask feature module is turned on, wealso do warping and correlation to the mask feature g(l)tinthe same way.The flow estimator estimates a flow residual added tothe current estimate F (l)12. Subsequently, a context net isalso applied similarly to obtain the refined flow of this levelF (l)12.The learned upsampler, adapted from the one inRAFT , outputs the parameters for the convex upsam-pling of F (l)12, yielding the upsampled final output F (l)12.",
  "Iterative decoder (1491.3k in total)conv 3x3": ". Detailed decoder structure (figure adapted from ); numbers in purple refer to the parameter sizes. The mask feature modulesare not shown in the figure for conciseness. We use the same warping and correlation computation for mask feature and image features. One example is shown in .Not only has thewhole car object been detected by SAM, but also its compo-nents such as front and rear wheels, car doors and windows,lights, bumpers, and even the gas cap. As a result, the maskof the whole car object overlaps with all those componentmasks, whereas each component mask only overlaps withthe car mask. Thus, the car object will be selected due toits high degree of mask overlap. Empirically, our heuristicworks on all car objects pretty well, which are indeed keyobjects in autonomous driving.",
  ". Example of the SAM masks computed for a car patch": "Key object selectionWe discuss more details on the pro-cess that we select key objects from the SAM masks. Sup-pose for the input image I, a number of n masks are de-tected by SAM, constituting masks M {0, 1}nHW .Denote M(k){0, 1}HW as the k-th mask, andM(k, i, j) = 1 means the pixel (i, j) is on the k-th object.For each mask M(k), we examine the following. We first filter masks at a certain dimension. Suppose thebounding box of M(k) has dimension hw, we only ac-cept masks with 50 h 200, 50 w 400. We avoidtoo large masks because they may not fit in the new sam-ple well in our augmentation. We avoid too small masksas they make little difference in the augmentation.",
  "masks. The number of overlaps can be counted efficientlythough matrix computation of M": "Training stepsDuring key object selection, we save theselected masks for each training sample on the disk beforestarting to train, so this step adds little time or memory con-sumption during training.For each training sample, weload three key objects from the object cache for augmen-tation. For more details about semantic augmentation, werefer readers to the original paper of SemARFlow andARFlow .",
  "A.4. Homography smoothness loss": "Selecting object regions of interestBefore selecting ob-ject regions, we first transform our raw SAM masks Mt toits full segmentation representation as described in Sec. 3.5in the main paper. This makes sure that we do not refine thesame pixel multiple times. Also, the segmentation is usu-ally smaller pieces of objects, where homography is morelikely to work well.We estimate the occlusion region using forward-backward consistency check , as we did when comput-ing photometric loss. The estimated occlusion region is agood cue of where the current flow estimate is less reliable.Then, we count the number of occlusion pixels for each seg-mentation in the full segmentation representation and pickthe top six as candidates. Empirically, we find that six seg-mentation regions can generally cover most of the occludedpixels. Although including more candidates can improveperformance, the improvement comes at a larger computa-tional cost. Refining each selected regionsFor each of the candidateregions selected above, we first find all the correspondencesin that region from flow. We define the reliable flow as thosenon-occluded parts estimated above. We only proceed ifthe reliable flow part accounts for at least 20% of the wholeregion.Using the reliable flow correspondences, we estimate ho-mography using RANSAC and compute the inlier per-cantage of this computation based on reprojection error. Weonly accept the homography if inlier percentage is at least50%.Consequently, using the accepted homography, we refinethe correspondences of every pixel in the object region andgenerate refined flow.",
  ". Another example for homography refinement (Sintel)": "well on planar, rigid regions where no deformation occurs.In our method, we alleviate his issue through the followingrules. We use full segmentation regions mentioned above,which generally refer to small object parts instead of thelarge object. Although the whole object may has verycomplex motions, its small parts are more likely to followhomography constraints.",
  "relationship does not hold for the specific region, we stopusing it. Only the most reliable homogrphies are used inrefinement": "We apply homography in the smoothness loss definitioninstead of direct post-processing. This allows our net-work to leverage between homography and other motioncues such as photometric constraints. Thus, a poor ho-mography (if any) may not have large impacts if othersigns/losses do not agree.In addition, to better resolve this issue, it may be better ifwe could also obtain the semantic class of each object maskor if we could use text prompt to find masks, which may beupdated in the later SAM versions.",
  "A.5. Mask feature and correlation": "Below are certain points that we need to take care whendesigning the mask feature module. The raw SAM masks are discrete and arbitrary (see Sec.3.1 for explanations). The number of masks in each sam-ple is not fixed. The masks can have different shapes andsizes. The masks can overlap or leave holes (parts that donot belong to any masks) in the frame. Therefore, we firststandardize the mask representation using a full segmen-tation representation described in Sec. 3.5. Our mask feature module should be independent of theorder of masks, i.e. our mask feature should be invariantagainst any permutation of masks. The mask/object IDsin the masks can be permuted without changing the seg-mentation map. Therefore, in our proposed module, weextract features for each mask separately regardless of itsorder. When we extract mask feature for each mask, the shapeand size may vary, so our designed module should bewell-defined for inputs of any size. This is why tradi-tional convolutional layers may not work directly.In-spired by PointNet , for which the input size can also vary, we adopt operators like averaging or min/max to ag-gregate features of variable sizes. We apply max poolingin favor of average pooling because it adds non-linearityto the network and is often used in image classificationnetworks. Apart from that, we need a new feature spacewhere the max operation works. That is why we add the1-by-1 convolutional layer at the front. The pooled feature is the same for every pixel in the samemask. This may cause numerical issues in optmization.Therefore, we concatenate and add another 1-by-1 con-volutional layer to make sure the output mask feature isnot exactly the same everywhere in the same mask.",
  "A.6. Additional explanation on f in the paper": "Why is there no curve for our proposed smoothness lossas a comparison?The way how our homography lossworks is different. For traditional loss, since its gradientsonly concentrate around the flow boundary (d), thegradients push boundaries towards the optimal solution stepby step, so we draw this landscape in f as if the flowboundary is moving. However, for our homography loss,the gradients apply on the whole region directly and in-stantly (e), so they are not just pushing the boundaries.Therefore, the same analysis in f may not apply.",
  ". More qualitative results on KITTI-2015 test set": "150k iterations (200k in total), similar to what have beendone in SemARFlow . The reasons are as follows. Both semantic augmentation and homography smooth-ness loss rely on the current flow estimate to generate self-supervised loss signals, so we need to use flow at a latercheckpoint to make sure they are reliable. Otherwise, theloss signals could be misleading. Semantic augmentation can generate very challengingself-supervised samples, which is better to be used at alater stage.For the mask feature module adaptation, the added net-work size is very small (111.9k) since most of the addedmodules are 1-by-1 convolutions. Our typical full experi-ment training time is around 64 hours on 8 V100 GPUs.We would like to emphasize that our goal is to investi-"
}