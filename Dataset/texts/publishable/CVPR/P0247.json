{
  "Abstract": "Despite the increasing interest in enhancing percep-tion systems for autonomous vehicles, the online calibra-tion between event cameras and LiDARtwo sensors piv-otal in capturing comprehensive environmental informa-tionremains unexplored. We introduce MULi-Ev, the firstonline, deep learning-based framework tailored for the ex-trinsic calibration of event cameras with LiDAR. This ad-vancement is instrumental for the seamless integration ofLiDAR and event cameras, enabling dynamic, real-timecalibration adjustments that are essential for maintainingoptimal sensor alignment amidst varying operational con-ditions. Rigorously evaluated against the real-world sce-narios presented in the DSEC dataset, MULi-Ev not onlyachieves substantial improvements in calibration accuracybut also sets a new standard for integrating LiDAR withevent cameras in mobile platforms. Our findings reveal thepotential of MULi-Ev to bolster the safety, reliability, andoverall performance of perception systems in autonomousdriving, marking a significant step forward in their real-world deployment and effectiveness.",
  ". Introduction": "Autonomous driving technologies are on the brink of rev-olutionizing transportation, announcing a new era of en-hanced safety, efficiency, and accessibility.At the heartof this transformation is the development of advanced per-ception systems that accurately interpret and navigate thecomplexities of the real world, such as the sharing of theroad with other transport modalities (e.g. bikes, pedestri-ans, buses, etc.). A critical element in crafting such systemsis sensor calibration. In this work we focus on extrinsic cal-ibration between LiDAR and event cameras, a subject thatstill remains too little explored today.Event cameras, which capture dynamic scenes with hightemporal resolution and excel in various lighting condi-tions, can significantly reduce or help leverage motionblur . On the other hand, LiDAR sensors offer de- . Overview of the MULi-Ev calibration workflow. Thisprocess integrates LiDAR point clouds and event camera data intothe MULi-Ev network to compute accurate extrinsic calibrationparameters (the rigid transformation in SO(3) between the twosensors reference frames, here represented by T). These param-eters enable real-time, precise sensor alignment, facilitating en-hanced perception for autonomous vehicles in dynamic scenarios. tailed depth information vital for precise object detectionand environmental mapping. The integration of these com-plementary technologies promises to substantially elevatevehicle perception capabilities. However, no method hasyet been proposed to provide accurate, real-time calibrationbetween these sensors.Traditional calibration methods performwell under controlled conditions but are unusable in the dy-namic, real-world environments autonomous vehicles en-counter.These methods often necessitate cumbersomemanual adjustments or specific calibration targets, unsuit-able for the on-the-fly recalibration needs of operational ve-",
  "arXiv:2405.18021v1 [cs.CV] 28 May 2024": "hicles. Furthermore, the sparse and asynchronous nature ofevent camera data introduces additional challenges for thecalibration process.To address these challenges, we propose a novel deep-learning framework trained specifically for the online cal-ibration of event cameras and LiDAR sensors ().This approach not only simplifies the calibration processbut also allows onboard online calibration on the vehicle,ensuring consistent sensor alignment. By enabling the jointuse of these sensors, our method helps leveraging the com-plementary strengths of event cameras and LiDAR in othertasks, significantly enhancing the vehicles perception sys-tem, enabling more accurate object detection and scene in-terpretation across a diverse range of driving scenarios.Our contributions include:",
  ". The validation of our method against the DSEC dataset,showing marked improvements in calibration precisioncompared to existing methods": "3. The capability for on-the-fly recalibration introduced byour framework directly addresses the challenge of main-taining sensor alignment in dynamic scenarios, a cru-cial step toward enhancing the robustness of autonomousdriving systems in real-world conditions.The sections that follow will explore related works tocontextualize our contributions within the broader researchlandscape, describe our methodology in detail, present anexhaustive evaluation of our framework against existingstate-of-the-art methods, and conclude with a discussion onthe broader implications of our findings and potential av-enues for future research.",
  ". Event Camera and LiDAR Calibration": "The calibration of extrinsic parameters between event cam-eras and LiDAR is a necessity to leverage their com-bined capabilities for enhanced perception in autonomoussystems. Unlike traditional cameras, event cameras cap-ture pixel-level changes in light intensity asynchronously,presenting unique challenges for calibration with LiDAR,which provides sparse spatial depth information.A fewoffline calibration methods have been pro-posed.Song et al. made an early contribution with a 3Dmarker designed for this purpose.Although pioneering,their method necessitates specific, often impractical setupconditions. To address these limitations, Xing et al. proposed a target-free calibration approach, utilizing nat-ural edge correspondences in the data from both sensors. This innovative method simplifies the calibration process,but is still performed offline. Jiao et al. introduced LCE-Calib, an automatic method that streamlines the calibrationprocess, enhancing robustness and adaptability across var-ious conditions.Building on these advancements, Ta etal. introduced L2E, a novel automatic pipeline for di-rect and temporally-decoupled 6-DoF calibration betweenevent cameras and LiDARs, which better leverages thespecificities of event data to improve results.This progression of techniques underscores a shift to-wards methods that are not only more versatile but alsosuited for real-world deployment. However, no method hasbeen proposed until now for the online calibration of thissensor combination.",
  ". Deep Learning in Extrinsic Calibration": "While deep learning has revolutionized many aspects of au-tonomous driving technology, its application to extrinsiccalibration between event cameras and LiDAR remains un-explored. Our work introduces the first deep learning-basedmethod for this specific task. However, the groundworklaid by methodologies for RGB cameras and LiDAR cali-bration provides a valuable referencepoint. For instance, RegNet by Schneider et al. lever-ages convolutional neural networks (CNNs) for sensor reg-istration, predicting the 6-DOF parameters between RGBcameras and LiDAR without manual intervention, markingan early milestone in learning-based calibration. Follow-ing this, CalibNet by Iyer et al. further refines the ap-proach with a geometrically supervised network, enhancingthe automation and accuracy of the calibration process. LC-CNet , introduced by Lv et al., represents a significantadvancement by utilizing a cost volume network to articu-late the correlation between RGB images and depth imagesderived from LiDAR data, achieving substantial improve-ments in calibration precision. These methods underscorethe potential of integrating deep learning into the calibra-tion workflow, offering insights into feature correlation andend-to-end model training that are instrumental for our ap-proach.The existing body of work on RGB and LiDAR calibra-tion delineates a path towards automated, real-time calibra-tion solutions. By adapting and extending these methodolo-gies, our research pioneers the application of deep learningfor calibrating event cameras with LiDAR, aiming to har-ness the unique advantages of event cameras for enhancedautonomous vehicle perception and navigation.",
  ". Architecture": "Our calibration framework integrates event camera and Li-DAR data through a unified deep learning architecture, sim-ilarly to UniCal , as illustrated in . Leverag-ing a single MobileViTv2 backbone for feature extrac-tion and a custom-designed regression head, the frameworkachieves precise calibration parameter estimation. Feature Extraction Backbone:Central to our approachis the MobileViTv2 backbone, chosen for its fast infer-ence speed and its ability to efficiently process multi-modaldata. This facilitates handling event and LiDAR pseudo-images within a single backbone. By feeding both modal-ities into separate input channels, our model concurrentlyprocesses event camera and LiDAR data, learning intricatecorrelations between these two modalities. This unified pro-cessing not only streamlines the architecture but also bol-sters the models feature extraction capabilities, crucial foraccurate extrinsic calibration. Custom Regression Head:Focused on extrinsic calibra-tion parameters, the regression head begins with a com-mon layer that identifies features applicable to both trans-lation and rotation, benefiting from shared data characteris-tics. Subsequently, the architecture divides into translationand rotation pathways, each comprising two layers designedspecifically for their respective parameter sets. This special-ization accounts for the unique aspects of translation (x, y,z) and rotation (roll, pitch, yaw) parameters, such as scaleand unit differences, thereby enhancing the models calibra-tion precision.",
  ". Event Representation": "In developing our calibration framework, a critical consider-ation was the optimal representation of event data capturedby event cameras. Event cameras generate data in a funda-mentally different manner from traditional cameras, record-ing changes in intensity for each pixel asynchronously.The data is structured as a flow of events, necessitating a thoughtful binning approach to transform it into a new rep-resentation for effective processing and integration with Li-DAR data.Our investigation encompassed various formats for rep-resenting event data, including: The event frame representation, which accumulatesevents into a 2D image, where the intensity of a pixel cor-responds to the number of events that occurred at that lo-cation within the specified accumulation time.",
  "The voxel grid representation, which extends thisconcept into three dimensions, adding a temporal depthto the accumulation": "The time surface representation, which encodes themost recent timestamp of an event at each pixel, capturingthe temporal dynamics more explicitly.Each binning strategy offers distinct advantages in termsof capturing the spatial and temporal dynamics of the scene,which are recapitulated in .However, our pri-mary objective was to identify a representation that not onlysimplifies the calibration process but also enhances perfor-mance by preserving essential geometric information suchas edges, without unnecessarily complicating the modelwith temporal details that are less critical for our specificcalibration task.Ultimately, we found that event frame representation wasthe most effective approach. This decision was driven byseveral key factors: Simplicity: The event frame representation aligns closelywith conventional data types used in deep learning, allow-ing for a more straightforward integration into our cali-bration framework. Performance:Through empirical testing (detailedin .5), we observed that the event frame pro-vided superior performance in terms of calibration accu-racy. This improvement is attributed to the formats effec-tiveness in preserving the geometric integrity of the scene.In summary, the event frame representation emerged asthe superior choice for our online calibration method, bal-ancing simplicity, performance, and geometric fidelity. Thisfinding underscores the importance of matching the datarepresentation format with the specific requirements of thetask, especially in the context of sensor fusion and calibra-tion.",
  "Model Training": "We introduce artificial decalibrations into the dataset, akinto the strategy employed by RegNet . This involves sys-tematically applying random offsets to the calibration pa-rameters between the event cameras and LiDAR. The net-work is then tasked with predicting these offsets, effectivelylearning to correct the artificially induced decalibrations. . Overall architecture of MULi-Ev. The initial decalibrated extrinsic parameters T (three for rotation, and three for translation) areused to project the LiDAR point cloud into the event camera frame. Both input are then fed to a MobileViTv2 backbone for featureextraction. The features are passed to a regression head, which regresses separately translation and rotation parameters. Together, theycompose the output T, which the loss L compares to the known ground truth. The smallest range used during our training focuses on re-calibrating the most common yet most challenging and sub-tle decalibrations within 1 and 10cm. However, ourmodel is capable, using an approach similar to ,to correct larger decalibrations, by iterating through a cas-cade of networks trained on larger decalibrations. For ourexperiments, we use a cascade of two networks. A firstnetwork with a larger training range of up to 10 and100cm, giving us a rough estimate of the parameters (withan average error of 0.47 and 3.03cm), well within thetraining range of the second network, trained on the 1",
  "Optimization and Evaluation": "Throughout the training, we employ Mean Square Error(MSE) regression losses (wildly used for regression tasks,and specifically on calibration tasks ) to minimize thedifference between the predicted calibration parametersand the ground truth, derived from the original, unalteredDSEC data. The model is trained with the Adam opti-mizer and a learning rate of 0.0001. Continuous evaluationon a validation set, separate from the training data, allowsus to monitor the models performance and adjust the train-ing parameters accordingly to avoid overfitting and ensureoptimal generalization.",
  ". Dataset": "For our experiments, we leverage the DSEC dataset ,a pioneering resource offering high-resolution stereo eventcamera data for driving scenarios and LiDAR. More specifi-cally it relies on a Velodyne VLP-16 LiDAR (a 16 channelsLiDAR), and Prophesee Gen3.1 monochrome event cam-eras with a 640480 resolution. This dataset is particularlynotable for its inclusion of challenging illumination condi-tions, ranging from night driving to direct sunlight scenar-ios, as well as urban, suburban, and rural environments,making it an ideal benchmark for our calibration frame-work. Its composition is detailed in .",
  "i=1tpred,i tgt,i2 ,(1)": "where tpred,i and tgt,i represent the predicted and groundtruth translation vectors for the i-th sample, respectively,and N denotes the number of test samples.For rotation, our network outputs Euler angles, whichare converted into rotation matrices to facilitate a robust er-ror computation. The angles are then converted back intoEuler form to report errors in a more interpretable fashion.Consequently, the MAE for rotational componentsRoll,Pitch, and Yawis calculated as follows:",
  "i=1Euler(Rrel,i) ,(2)": "where Rrel,i represents the relative rotation matrix for thei-th sample, obtained by the operation Rpred,i R1gt,i. Thefunction Euler() converts this matrix to Euler angles, ex-pressing the rotational discrepancy in terms of Roll, Pitch,and Yaw. The norm then quantifies the magnitude ofthese angles, yielding the rotational error in degrees. Thismethodology allows for a precise measurement of rotationalcalibration performance across the dataset.",
  ". Evaluation of the mean absolute error of MULi-Ev on thelocation subsets of DSEC": "they captured themselves. However, considering our online,deep learning-based approach, we evaluated our method ina more systematic way, on the publicly available DSEC dataset presented in .1.Moreover, most exist-ing works measure the quality of their results through non-absolute, sensor-dependent metrics, such as reprojection er-ror, which is more suitable when using targets, and can beaffected by sensor resolution and lens distortion. One of themost recent works, LCE , is the most suitable for com-parison with our method, as it not only offers state-of-the-art results, but also uses similar sensors (notably the sameLiDAR, Velodyne VLP-16). It also communicates results inthe same absolute metric as our work, measuring the MeanAbsolute Error on rotation and translation. General Results Analysis:As demonstrated by the re-sults in , MULi-Ev achieves superior calibrationaccuracy, reducing the translation error to an average of0.81cm and rotation error to 0.1.These results are il-lustrated qualitatively in and detailed in box plotsin . Distinctively, MULi-Ev achieves these resultswhile being, to our knowledge, the first online, targetlesscalibration method for this sensor setup. It bridges a sig-nificant gap in real-time operational needs while surpassingexisting offline, target-dependent methods, such as . Fi-nally, MULi-Ev being deep learning-based, it manages toreach this accuracy in an execution time inferior to 0.1s ona GPU, while an offline method like takes about 134swith its fastest optimizer. Box Plots Analysis:Interestingly, it can be noticed in that results on translation axis Z and rotation axisPitch tend to be less regular. This was also found in worksfocused on RGB-LiDAR calibration such as , and wasthus expected. It can be explained by the physical nature ofthese axes that align with the vertical dimension, in whichthe LiDAR points density is much lower (the vertical res- olution of the VLP-16 LiDAR is only 2, while its hori-zontal resolution is between 0.1 and 0.4). This low ver-tical resolution of the VLP-16 is mostly due to it havingonly 16 LiDAR rings, compared to 64 for the VelodyneHDL-64E used in the KITTI dataset, which was mostcommonly used to evaluate RGB-LiDAR calibration meth-ods . Influence of the Environment:To further analyze the be-havior of MULi-Ev on different types of scenes, we mea-sured the average errors per location. The results are avail-able in , while the characteristics and number of se-quences in these locations were reported in . We ob-serve in that the best results are obtained in ZurichCity, while the least accurate results were for scenes cap-tured in Interlaken. This was expected, and we can inferfrom it two possible explanations: first, 35 sequences fromZurich were included in the training set, compared to 5 forInterlaken; second, scenes in Interlaken happen to be mostlyrural, and thus to have generally less available features, es-pecially long vertical edges like the ones offered by build-ings. However, MULi-Ev performed better on the Thunscenes than Interlaken scenes, despite having even less se-quences (only 1 for training). This tends to confirm oursecond hypothesis, as Thun offers more of a suburban envi-ronment, with enough human-built structures to offer morelinear features. Another interesting fact is that while In-terlaken and Thun scenes were all recorded by day, ZurichCity sequences include night scenes, and still obtains thebest accuracy, suggesting that MULi-Ev might adapt quitewell to varying lighting conditions. Overall, results are atleast on par with the state of the art or better for all threelocation subsets. Qualitative results in show suc-cessful recalibrations in different environment: in a tunnel,in a suburban zone, and on a rural road.",
  ". Results of ablation experiments on DSEC to determinethe influence of event representation on the final calibration result(average error)": "Ablation Results:Results reported in indicatethat the event frame representation, with an accumulationtime of 50ms, achieved the highest calibration accuracy.This suggests that a longer accumulation time might leadto higher noise levels, degrading the result.Conversely,shorter accumulation times, while offering fresher data,may not accumulate enough events to adequately representthe scene for effective calibration. The voxel grid and timesurface representations, despite their more complex encod-ing of event data, did not yield improvements in calibra-tion accuracy over the optimized event frame representa-tion. These observations underscore the importance of thechoice of event representation and accumulation period tooptimize the results of our method.",
  ". Discussion": "Results of our experiments in demonstrate thatMULi-Ev can provide better accuracy than existing offlineworks, and this in diverse environments (as demonstratedin ) while being the first online method proposed forthis sensor combination. As a comparison, deep learning-based methods for RGB-LiDAR sensor setups have initiallyreached MAE of 0.28 and 6cm , while more recent ap-proaches reached 0.03 and 0.36cm, showing there isprobably still potential for improving the accuracy offeredby MULi-Ev (currently 0.1 and 0.81cm). MULi-Ev notonly enhances operational convenience by eliminating theneed for impractical calibration targets but also excels indynamic environments where rapid recalibration is essen-tial, thanks to its execution time of less than 0.1s offeringon-the-fly recalibration capability. By ensuring immediaterecalibration to maintain performance and safety, MULi-Evcan contribute to the robustness of autonomous navigationsystems in real-world applications. Finally, results from Ta-ble 5 suggest that our choice of the simple event frame forevent representation delivers the best results while simpli-fying the implementation of our method.",
  ". Conclusion": "In this work, we introduced MULi-Ev, a pioneering frame-work that establishes the feasibility of online, targetless cal-ibration between event cameras and LiDAR. This innova-tion marks a significant departure from traditional, offlinecalibration methods, offering enhanced calibration accuracyand operational flexibility.The real-time capabilities ofMULi-Ev not only pave the way for immediate sensor re-calibrationa critical requirement for the dynamic environ-ments encountered in autonomous drivingbut also openup new avenues for adaptive sensor fusion in operational",
  "vehicles": "Looking ahead, we aim to further refine MULi-Evs ro-bustness and precision, with a particular focus on monitor-ing and adapting to the temporal evolution of calibrationparameters. Such enhancements will ensure that MULi-Evcontinues to deliver accurate sensor alignment even as con-ditions change over time. Additionally, we are interested inexpanding the applicability of our framework to incorporatea wider array of sensor types and configurations. This ex-pansion will enable more comprehensive and nuanced per-ception capabilities, ultimately facilitating the development of more sophisticated autonomous systems.As we move forward, our focus on refining MULi-Evis aligned with the evolving demands of autonomous vehi-cle technology. By addressing the real-world challenges ofsensor calibration and integration, MULi-Ev contributes toimproving the safety, reliability, and performance of thesesystems. Our efforts to enhance sensor fusion and adapt-ability reflect a practical step towards achieving more robustand reliable autonomous driving capabilities.",
  "Mathieu Cocheteux, Aaron Low, and Marius Bruehlmeier.UniCal:a single-branch transformer-based model forcamera-to-lidar calibration and validation.arXiv preprintarXiv:2304.09715, 2023. 2, 3, 6": "Mathieu Cocheteux, Julien Moreau, and Franck Davoine.PseudoCal: Towards initialisation-free deep learning-basedcamera-lidar self-calibration.In 34th British Machine Vi-sion Conference 2023, BMVC 2023, Aberdeen, UK, Novem-ber 20-24, 2023. BMVA, 2023. 2, 4, 6 Guillermo Gallego, Tobi Delbruck, Garrick Orchard, ChiaraBartolozzi, Brian Taba, Andrea Censi, Stefan Leutenegger,Andrew J Davison, Jorg Conradt, Kostas Daniilidis, et al.Event-based vision: A survey. IEEE transactions on patternanalysis and machine intelligence, 44(1):154180, 2020. 1",
  "Andreas Geiger, Philip Lenz, Christoph Stiller, and RaquelUrtasun. Vision meets robotics: The KITTI dataset. TheInternational Journal of Robotics Research, 32(11):12311237, 2013. 6": "Ganesh Iyer, R Karnik Ram, J Krishna Murthy, and K Mad-hava Krishna. CalibNet: Geometrically supervised extrinsiccalibration using 3d spatial transformer networks. In 2018IEEE/RSJ International Conference on Intelligent Robotsand Systems (IROS), pages 11101117. IEEE, 2018. 2, 4,6 Jianhao Jiao, Feiyi Chen, Hexiang Wei, Jin Wu, and MingLiu. LCE-Calib: automatic lidar-frame/event camera extrin-sic calibration with a globally optimal solution. IEEE/ASMETransactions on Mechatronics, 2023. 1, 2, 5 Xin Jing, Xiaqing Ding, Rong Xiong, Huanjun Deng, andYue Wang.Dxq-net: differentiable lidar-camera extrinsiccalibration using quality-aware flow. In 2022 IEEE/RSJ In-ternational Conference on Intelligent Robots and Systems(IROS), pages 62356241. IEEE, 2022. 2, 6",
  "Henri Rebecq, Timo Horstschaefer, and Davide Scaramuzza.Real-time visual-inertial odometry for event cameras usingkeyframe-based nonlinear optimization. 2017. 3": "Nick Schneider, Florian Piewak, Christoph Stiller, and UweFranke. RegNet: Multimodal sensor registration using deepneural networks. In 2017 IEEE intelligent vehicles sympo-sium (IV), pages 18031810. IEEE, 2017. 2, 3, 4, 6 Rihui Song, Zhihua Jiang, Yanghao Li, Yunxiao Shan, andKai Huang. Calibration of event-based camera and 3d li-dar. In 2018 WRC Symposium on Advanced Robotics andAutomation (WRC SARA), pages 289295. IEEE, 2018. 1, 2,5 Kevin Ta, David Bruggemann, Tim Brodermann, ChristosSakaridis, and Luc Van Gool. L2E: Lasers to events for 6-dof extrinsic calibration of lidars and event cameras. In 2023IEEE International Conference on Robotics and Automation(ICRA), pages 1142511431. IEEE, 2023. 1, 2, 5 Shan Wu, Amnir Hadachi, Damien Vivet, and Yadu Prab-hakar. This is the way: Sensors auto-calibration approachbased on deep learning for self-driving cars. IEEE SensorsJournal, 21(24):2777927788, 2021. 2, 6"
}