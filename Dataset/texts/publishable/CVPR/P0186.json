{
  "PSNR=16.44PSNR=14.09PSNR=18.92PSNR=15.73PSNR=14.01PSNR=18.57": ". Visualization of failure modes in NeRF and MPI. (a) MPI models scenes only in each single camera frustums and performshomography warping to render novel views. Insufficient sampling leads to incorrect depth and thus results in an overlapping ghosting effect.Large camera movement leads to cropped corners. However, high-frequency details seem to be successfully preserved. (b) NeRF modelsscenes in a continuous volumetric manner. If only sparse views with large camera movement are provided, some parts of the scene may besampled very little or even never. Insufficient sampling leads to collapsed details and unexpected floaters. (c) Our approach combines thecapabilities of NeRF with the perspective-friendly nature of MPI in aerial scenes to achieve photorealistic novel view renderings.",
  "Abstract": "Neural Radiance Fields (NeRF) have been successfullyapplied in various aerial scenes, yet they face challengeswith sparse views due to limited supervision. The acquisi-tion of dense aerial views is often prohibitive, as unmannedaerial vehicles (UAVs) may encounter constraints in perspec-tive range and energy constraints. In this work, we introduceMultiplane Prior guided NeRF (MPNeRF), a novel approachtailored for few-shot aerial scene renderingmarking a pio-neering effort in this domain. Our key insight is that the in-trinsic geometric regularities specific to aerial imagery couldbe leveraged to enhance NeRF in sparse aerial scenes. Byinvestigating NeRFs and Multiplane Image (MPI)s behav-ior, we propose to guide the training process of NeRF witha Multiplane Prior. The proposed Multiplane Prior drawsupon MPIs benefits and incorporates advanced image com-prehension through a SwinV2 Transformer, pre-trained via *Corresponding author. This work was supported in part by the KeyScientific Technological Innovation Research Project by Ministry of Ed-ucation, the Joint Funds of the National Natural Science Foundation ofChina (U22B2054), the National Natural Science Foundation of China(62076192, 61902298, 61573267, 61906150, and 62276199), the 111Project, the Program for Cheung Kong Scholars and Innovative ResearchTeam in University (IRT 15R53), the ST Innovation Project from the Chi-nese Ministry of Education, the Key Research and Development Programin Shaanxi Province of China(2019ZDLGY03-06), the China Postdoctoralfund(2022T150506). SimMIM. Our extensive experiments demonstrate that MPN-eRF outperforms existing state-of-the-art methods appliedin non-aerial contexts, by tripling the performance in SSIMand LPIPS even with three views available. We hope ourwork offers insights into the development of NeRF-basedapplications in aerial scenes with limited data.",
  ". Introduction": "Neural Radiance Field (NeRF) has succeeded in render-ing high-fidelity novel views and many 3D applications bymodeling 3D scenes as a continuous implicit function. In con-trast to indoor or synthetic scenes capturing simple objectsusing cell phones, aerial images provide a unique birds-eyeview and overview of a landscape. Based on NeRF, many ap-plications in aerial scenes have been developed, such as navi-gation, urban planning, data augmentation, autonomous vehi-cles, and environmental mapping .In many real-world scenarios, unmanned aerial vehicles(UAVs) encounter constraints such as limited perspectives,energy limitations, or adverse weather conditions, which re-strict their ability to acquire dense observational data. WhileNeRF forms a foundational technology for many aerial ap-plications, it is prone to overfitting on sparse training views. This limitation of NeRF becomes particularlysalient in the context of aerial imagery. Alleviating this prob-",
  "arXiv:2406.04961v1 [cs.CV] 7 Jun 2024": "lem could save resources and may benefit numerous applica-tions.As the first to explore few-shot NeRF for aerial imagery,we stand at the forefront of this emerging field. The land-scape of few-shot neural rendering to date has been predom-inantly shaped by its application to indoor and syntheticscenes. Transfer learning based methods aim to pre-train the model on a large number of scenes.Yet, these approaches necessitate extensive datasets for pre-training. This is not only resource-heavy but also impracticalto fulfill for varied aerial scenarios. Another line of works seeks to impose regulariza-tion on NeRF by exploring the universal attributes of 3Dscenes like local continuity and semantic consistency. Yet, insituations where available data is significantly sparse relativeto the complexity of the scene, these methods might struggleto maintain their effectiveness. And the last is depth-prior-based methods gain additional supervision from thescenes depth. These methods can be problematic in aerialimages due to the frequent occurrence of ambiguous depthcues and the high cost of obtaining accurate depth maps. De-spite their efficacy in controlled environments, these meth-ods fall short of addressing the unique complexities of aerialscenes, leaving a gap that our work aims to fill. We thereforeask: Can we harness the intrinsic geometric regularitiesspecific to aerial imagery to broaden the capabilities ofNeRF under sparse data conditions, thereby easing thedata collection constraints?To answer this question, we turn to earlier works on2.5D representations such as Multiplane Image (MPI). MPI typically operates by extracting mul-tiple RGB and density planes from a single image input byan encoder-decoder style MPI generator to represent scenegeometry within the cameras frustum. Although NeRF pro-vides a continuous representation of a scene, MPI offers dis-crete, frustum-confined layers that can be particularly advan-tageous in the context of aerial imagery. This is due to UAVsfrequently capturing images from overhead perspectives thatalign well with MPIs planar representation. Additionally,the encoder-decoder architecture of the MPI generator canexploit the inductive biases inherent in advanced convolu-tional and self-attention-based image processing comparedto the simple multi-layer perceptron (MLP) of NeRF, thusenhancing the rendering of local and global scene details.However, while MPIs present certain benefits in terms oftheir adaptability to aerial perspectives, their partial scene re-covery and limitation to individual frustums pose challengesin creating a comprehensive 3D understanding.In this work, we present Multiplane Prior guided NeRF(MPNeRF), a novel method for enhancing NeRF modelsin few-shot aerial scene rendering. We guide NeRFs learn-ing process by using a multiplane priora concept drawnfrom the strengths of MPI and refined with cutting-edge im- age understanding from a Swin TransformerV2 pre-trainedwith SimMIM . This approach unites the capabilities ofNeRF with the perspective-friendly nature of MPI, tailoredfor the unique vantage points of aerial scenes. Concretely,our approach updates the NeRF branch using pseudo labelsgenerated from the MPI branch. As training proceeds, NeRFcan effectively pick up finer details from the MPI branch andthe advantage of the MPI branch is implicitly distilled intoNeRF. This strategy implicitly folds a multiplane prior toNeRF, boosting its performance in handling sparse aerial im-agery data. Our contributions can be summarized as follows:1. We introduce Miltiplane Prior guided NeRF (MPNeRF), anovel framework that synergistically combines NeRF andMPIs for enhanced few-shot neural rendering in aerialscenes. To the best of our knowledge, this is the firstmethod specially designed for this task. 2. Through an investigation, we pinpoint and analyze thetypical failure modes of NeRF and MPI in aerial scenes.We devise a simple yet effective learning strategy thatguides the training process of NeRF by learning a mul-tiplane prior, effectively circumventing NeRFs typicalpitfalls in sparse aerial scenes. 3. We compare MPNeRF against a suite of state-of-the-artnon-aerial scene methods, rigorously testing its adaptabil-ity and performance in aerial scenarios. Our experimentsdemonstrate MPNeRFs superior performance, showcas-ing its significant leap over methods previously confinedto non-aerial contexts.",
  ". Scene Representations for View Synthesis": "Earlier works on light fields achieve view synthe-sis by interpolating nearby views given a dense set of inputimages. Later works utilize explicit mesh , orvolumetric representation to representthe scene. More recently, layered representations have gainedattention due to their efficiency in modeling occluded content.One such layered representation is the MPI .An MPI consists of multiple planes of RGB and values atfixed depths. Given an input image, an encoder-decoder net-work typically generates the MPI within the camera frustum.This MPI is then homography warped to the target cameraposition and integrated over the planes to produce novelviews. Its important to note that the generated MPI onlymodels the geometry within each camera frustum at givendepths, and the complete 3D scene is not fully recovered.Recently, NeRF has shown significant potential innovel view synthesis. NeRF works by modeling the scenewith a continuous function of 3D coordinates and viewingdirections to output the corresponding RGB and volumedensity values. Following NeRF, many methods have beenproposed. mip-NeRF introduces a more robust representa- tion that uses a cone tracing technique and samples the conewith multivariate Gaussian. NeRF-W and Ha-NeRFhave extended the applicability of NeRF to in-the-wild photocollections through object decomposition and hallucinationtechniques. For large-scale scenes, BungeeNeRF pro-poses a multiscale progressive learning method to recon-struct cities from satellite imagery, while Block-NeRF leverages individual NeRFs for each component of the sceneto achieve large-scale scene rendering. ShadowNeRF and Sat-NeRF address the issue of strong noncorrelationbetween satellite images taken at different times by modelingsolar light and transient objects. However, these NeRF-basedmethods are still limited by the need for densely sampledviews of the scene.",
  ". NeRF with Sparse Input": "Many approaches have been developed to train a NeRF fromsparse input in different directions. One straightforward di-rection is to learn the general ap-pearance of a scene or object from a large number of data.PixelNeRF adopts a CNN feature extractor to conditioneach input coordinate with image features. MVSNeRF uses 3D CNN to process cost volume acquired by imagewarping. These methods often require a large number ofmulti-view images to be pre-trained on, which is sometimeshard to acquire in aerial imagery. Some other techniques find it is more data-efficient to regularizeNeRF with common properties of the 3D geometry. InfoN-eRF regularizes NeRF by putting a sparsity constrainton the density of each ray. RegNeRF regularizes NeRFby local smoothness. Other works aim totake advantage of supervision from other sources, such asdepth or appearance. DS-NeRF supervises the geom-etry with sparse point cloud generated with structure frommotion. DietNeRF regularize NeRF by ensuring percep-tual consistency within different views. ManifoldNeRF builds upon DietNeRF and takes into account viewpoint-dependent perceptual consistency to refine supervision inunknown viewpoints. However, we noticed none of thesemethods is designed for aerial scenes and thus left a gap ourwork aims to fill.",
  ". Method": "Our objective is to train a standard NeRF model to createhighly realistic novel views of an aerial scene from a limitednumber of captured perspectives. To address the challengesof training NeRF with sparse aerial views, we introduce anovel training approach that leverages a Multiplane Prior.The proposed Multiplane Prior harnesses the strengths ofMPI and is enriched by advanced image understanding ca-pabilities derived from a SwinV2 Transformer pre-trainedusing SimMIM . An overview of our approach is pre-sented in .",
  ". Preliminaries": "Neural Radiance Field. Given a 3D coordinate x =(x, y, z) and a 2D viewing direction d = (, ), NeRFaims to model the scene by solving a continuous func-tion f (x, d) = (c, ) using multi-layer perceptron (MLP)network, where c and represent the emissive color andvolume density at the given coordinate. NeRF cast raysr (t) = o + td from the camera origin o along the directiond to pass through a pixel. NeRF then samples M pointsalong this ray and computes its color by volume rendering:",
  "(1)": "where ci and i are the color and volume density of i-thsample along the ray and i is the distance between adjacentsamples. C (r) denotes the final color of that pixel renderedby NeRF. In NeRF, a dense 3D scene is recovered implicitlyin the form of neural network weights.Multiplane Image. Multi-plane Image (MPI) represents thescene by dividing the 3D space into a collection of planeswith RGB and density values in one camera frustum. Intraining, each batch consists of a pair of images Is, It RHW 3 with corresponding camera intrinsic Ks, Kt R33 and relative pose Ps2t =Rs2t R33, ts2t R3 denoted as {(Is, Ks) , (It, Kt, ) , P}, subscripts s and t rep-resent source and target viewpoint respectively. Depth foreach plane is sampled {z = zk|k = 1, 2, 3, , N} uni-formly according to the scene bounds. An encoder-decoderbased MPI-generator denoted as GMPI is adopted to generatemultiple planes of RGB and density at discrete depth as:",
  "PSNR=16.44": ". Overall pipeline for training Multiplane Prior guided NeRF (MPNeRF). Our novel MPNeRF architecture integrates a standardNeRF branch with an MPI branch, informed by a pre-trained SwinV2 Transformer. This design introduces a multiplane prior to guide theNeRF training, addressing the common challenges of rendering with sparse aerial data. The process begins by sampling three distinct views:a source and target view for training with known ground truth, and an unseen view from a novel viewpoint. The NeRF model is then refinedusing pseudo labels produced by the MPI branch, which are especially crucial for synthesizing views from previously unseen angles, asshown in the pipeline.",
  ". A Closer Look at The Behavior of NeRF & MPI": "To better understand the behavior of NeRF and MPI, weconducted an investigation into their failure modes. In , we visualize the rendering results of NeRF and MPI whenencountering large camera movements. Our findings revealthat NeRF often produces blurry renderings, while MPI tendsto exhibit overlapping ghosting effects and cropped corners.Recall that NeRF represents the whole 3D scene con-tinuously by encoding the volume density and color intoan 8-layer MLPs weights. In other words, NeRF utilizes alearning-based approach by forcing correct rendering fromevery angle of the scene with multi-view consistency. Sucha model is highly compact when supervised with sufficienttraining views. When the supervised angle is limited, areascovered less (as in the non-overlapped camera frustum in (b)) are uncontrolled and may exhibit high-densityvalues , leading to blurry or even collapsed re-sults. Considering the structured nature of 3D aerial scenes,we recognize two key factors of aerial scenes: aligned per-spectives with predominant planarity, and consistent geomet- ric appearance. First, the typical flight paths of UAVs overthese scenes predominantly capture landscapes aligned withthe XY planes, providing a unique geometrical consistency.Second, objects in aerial scenes contain common visual char-acteristics, offering additional cues for scene interpretationand analysis. In contrast to NeRF, MPI models the scene within eachcamera frustum and decomposes it into an explicit set ofdiscrete 2D planes at fixed depths. This mirrors the over-head views and planar surfaces commonly found in aerialscenes. Also, the convolution-based or self-attention-basedMPI-generator is inherently suited to carry prior knowledgeof the scene. However, with insufficient supervision pro-vided, the MPI for different camera frustums may not beproperly calibrated. As a consequence, we observe the oc-currence of overlapping ghosting effects in rendered unseenviews. Additionally, when there is substantial camera move-ment, the corners of the target views may be excluded fromthe source views, resulting in invalid renderings. However,MPI is successful in preserving high-frequency details in therendered image. We attribute this capability to the power ofCNNs and the implicit encoding of prior knowledge in theMPI generator.",
  ". Guiding NeRF with a Multiplane Prior": "Based on the investigation of the different properties shownin NeRF and MPI when encountering sparse input. We turnto the task of few-shot aerial scene rendering and propose asimple yet effective strategy that treats the MPI as a bridgeto convey information that is hard to learn by the traditionalNeRF pipeline.We formulate the proposed MPNeRF with a NeRF branch,and an MPI branch denoted as G1 and G2. Given a batchcontains images from source and target viewpoints alongsidethe corresponding camera parameters. To train the NeRFbranch, we cast rays for the source viewpoints pixels us-ing the camera parameters following . The MSE loss isadopted to supervise the NeRF branch with the ground truthcolor C (r):",
  ",(5)": "where, B is the set of input rays during training. For the MPIbranch, the encoder-decoder style MPI generator takes inimages from the source view and outputs the correspondingMPI representation. In order to incorporate prior knowledge,we adopt a frozen Swin Transformer V2 model pre-trainedwith SimMIM as a feature extractor to extract multi-scale features from aerial images. These features are fusedto generate the final MPI representation. The loss function tooptimize the MPI branch contains three components: L1 lossto match the synthesized target image It to ground truth It ata pixel level, SSIM loss to encourage structure consistency,and LPIPS loss for perceptual consistency.",
  "LMPI = LL1 + LSSIM + LLPIPS.(7)": "These conventional loss functions train both branchesto give predictions based on training views. Based on theinvestigation in Sec. 3.2, we aim to guide the training processof the NeRF with a multiplane prior learned by the MPIbranch. An intuitive choice is sampling a random numberof pixels from an unseen view and matching the predictedcolor of two branches with an MSE loss.",
  ". Implementation Details": "Our method is implemented using PyTorch, and all exper-iments are conducted on a GeForce RTX 3090 GPU. Forthe NeRF branch, we use the original NeRF in . Dur-ing training, we randomly sample unseen views followingthe strategy proposed by . The batch size is set to 1024pixel rays in both source and unseen views. For each ray,we perform 64 coarse sampling and 32 fine sampling alongthe ray. For the MPI branch, we sample 16 layers of planesfor each viewpoint. The optimization of the two branches ofMPNeRF is performed using the Adam optimizer witha learning rate of 5 104. The hyperparameter in Eq. 9is set to 1.",
  ". Datasets and Evaluation Metrics": "The main experiments are conducted on 16 scenes collectedby LEVIR-NVS . These scenes contain various scenariosin common aerial imagery, including mountains, buildings,colleges, etc. 3 and 5 views are used for training and therest for testing. Additional experiments and discussions canbe found in the Appendix. In line with previous studies offew-shot neural rendering , we report PSNR,SSIM and LPIPS .",
  ". Baseline Methods": "We compare MPNeRF against various state-of-the-art meth-ods including NeRF , Mip-NeRF , InfoNeRF ,DietNeRF , PixelNeRF , RegNeRF and FreeN-eRF . Among these methods, NeRF and Mip-NeRF aredesigned for dense view training, we mainly explore theperformance gain achieved by MPNeRF. PixelNeRF aimsto learn a generalized NeRF representation for all scenesand is pre-trained on the DTU dataset . Since a largedomain gap might exist when applied in aerial imagery, wereport PixelNeRFs results with and without additional fine-tuning per scene. Other methods are designed for few-shot",
  ". Comparative Results Analysis": "and report the performance of MPNeRFand baseline methods in the 3-view and 5-view settings.Additionally, a qualitative comparison can be observed in. A very significant improvement can be found in allthree metrics and rendering fidelity. The results demonstratethat PixelNeRF tends to produce blurry renderings, whichwe attribute to the poor localization of the CNN features.InfoNeRF and RegNeRF use local smoothness and spar-sity to regularize NeRF explicitly. However, in scenarioswith substantially limited information compared to the scenecomplexity, the performance of these methods could be com-promised. DietNeRF implicitly distills the prior knowledgeencoded in CLIP and achieves better results. FreeN-eRF investigates the frequency in NeRF training. By pro-gressively learning each frequency component, FreeNeRF has demonstrated remarkable effectiveness. Nonetheless, theprogressive frequency regularization leads to relatively flatresults, favoring PSNR but not metrics that consider localstructures such as SSIM and LPIPS.In fact, NeRFs representation makes recovering 3Dscenes from sparse inputs ill-posed. MPNeRF acquires supe-rior results by the guiding of a multiplane prior to gaining astronger understanding of local structures and semantics. Inthe more challenging scenes, such as Building in ,MPNeRF successfully avoids collapse during training.",
  ". Ablation Studies and Further Analyses": "Ablation Analysis. We ablate the proposed multiplane priorto our method, and the results are shown in . In-tuitively, it seems better to use MPI as a guide after fullytraining it, we first construct experiments where a two-stagetraining strategy is employed. We then assess the impactof SwinV2s pre-trained weights on performance by remov-ing them. Next, we evaluate the contribution of multi-scalefeatures by disconnecting the skip connections in the MPI generator. Finally, we integrate the multiplane directly withinNeRFs sampling space, omitting the separate MPI branch,to examine the inductive biases influence on performance.Employing MPI concurrent branch during training leads toslight improvements. We believe this is because the MPIstraining experience itself carries information. The exclusionof SwinV2s pre-trained weights declines performance, af-firming the value of the encoded prior knowledge. Similarly,omitting the multi-scale feature connection diminishes thefidelity of the rendered images. Most significantly, the ab-sence of the MPI generator results in a marked decrease inall metrics. This suggests that uncalibrated MPI generatedby self-attention and convolution is important to avoid de-generate solutions. Collectively, these findings demonstratethat each element of the proposed multiplane prior is crucialfor the superior performance of MPNeRF.",
  ". Ablation analysis on the proposed Multiplane Prior": "Further Analyses on the design choice of the LMul. Oneintuitive thought of designing LMul is that geometry recov-ered by the MPI branch may provide more information thancolor alone. So we design two experiments, one is to matchthe expected depth of both branches as an auxiliary depthloss, and another is to model density on each ray as a distribu-tion and minimize the KL divergence. Another intuitivethought is that the choice of LMul should reflect the localor nonlocal relationships within the pixels. Therefore, weadopt the recently proposed S3IM loss to measure thisrelationship.",
  ". Design choice of the LMul": "However, as shown in Table. 4, the result suggests thatthese intuitive designs worsen the results. The first two de-sign involves direct supervision of the depth generated bythe MPI generator. The last involves capturing the non-localrelationships between the predictions of the NeRF and theMPI branch. Since the learned MPI is not entirely accurate,we believe the noise within pseudo-labels may compromiseperformance with these enhanced supervisions applied.",
  ". We investigate the data efficiency achieved by our method.Our method requires up to 63.5% training images to achieve asimilar performance compared to a vanilla NeRF model": "Impact of Different Pre-trained Models. We perform acomparison study on three pre-trained vision transformers,i.e., CLIP , DINOV2 , and SimMIM . We adoptthe base model in our experiment. As shown in , allof these methods provide comparable results. The resultsshow that the Swin Transformer pre-trained via SimMIM outperforms others. We believe this can be attributed tothe rich global and local details learned by SimMIM and thehierarchical structure of the Swin Transformer.",
  ". Impact of different pre-trained models": "Data Efficiency Since we aim to improve the capability ofNeRF in aerial scenes when only sparse views are available,we investigate how much data MPNeRF can save to achievesimilar rendering results compared to the original NeRF thatrequires dense view supervision. As shown in , theresults show that our method requires up to 63.5% trainingimages. This may help save energy and establish resource-efficient applications for UAVs based on NeRF.",
  ". Limitations and Conclusion": "In this work, we introduce Multiplane Prior guided NeRF(MPNeRF), the first approach designed for few-shot aerialscene rendering. Through the guiding of the multiplane prior,MPNeRF effectively overcomes the typical pitfalls in spareaerial scenes. We hope our work can provide insight into fu-ture NeRF-based applications in aerial scenes. However, fur-ther exploration of the guiding strategy design is needed. Inparticular, incorporating uncertainty prediction mechanismsor implementing grid-based representations holds promisefor future research directions. Brendan Alvey, Derek T Anderson, Andrew Buck, MatthewDeardorff, Grant Scott, and James M Keller. Simulated photo-realistic deep learning framework and workflows to acceleratecomputer vision and unmanned aerial vehicle research. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 38893898, 2021. 1",
  "Andre Araujo, Wade Norris, and Jack Sim. Computing re-ceptive fields of convolutional neural networks. Distill, 4(11):e21, 2019. 14": "Jonathan T Barron, Ben Mildenhall, Matthew Tancik, PeterHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.Mip-nerf: A multiscale representation for anti-aliasing neuralradiance fields. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 58555864, 2021. 2,5, 6, 16 Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general-izable radiance field reconstruction from multi-view stereo.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 1412414133, 2021. 2, 3 Wuyang Chen, Ziyu Jiang, Zhangyang Wang, Kexin Cui,and Xiaoning Qian. Collaborative global-local networks formemory-efficient segmentation of ultra-high resolution im-ages. In Proceedings of the IEEE/CVF conference on com-puter vision and pattern recognition, pages 89248933, 2019.14 Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith, JaakkoLehtinen, Alec Jacobson, and Sanja Fidler. Learning to predict3d objects with an interpolation-based differentiable renderer.Advances in neural information processing systems, 32, 2019.2 Xingyu Chen, Qi Zhang, Xiaoyu Li, Yue Chen, Ying Feng,Xuan Wang, and Jue Wang. Hallucinated neural radiancefields in the wild. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1294312952, 2022. 3",
  "Abe Davis, Marc Levoy, and Fredo Durand. Unstructuredlight fields. In Computer Graphics Forum, pages 305314.Wiley Online Library, 2012. 2": "Celso M de Melo, Antonio Torralba, Leonidas Guibas, JamesDiCarlo, Rama Chellappa, and Jessica Hodgins.Next-generation deep learning based on simulators and syntheticdata. Trends in cognitive sciences, 2022. 1 Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan.Depth-supervised nerf: Fewer views and faster training forfree. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 1288212891,2022. 2, 3",
  "Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, andMichael F. Cohen. The Lumigraph. Association for Comput-ing Machinery, New York, NY, USA, 1 edition, 2023. 2": "Philipp Henzler, Niloy J Mitra, and Tobias Ritschel. Learninga neural 3d texture space from 2d exemplars. In Proceedingsof the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 83568364, 2020. 2 Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerfon a diet: Semantically consistent few-shot view synthesis. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 58855894, 2021. 1, 2, 3, 5, 6, 16 Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola,and Henrik Aans. Large scale multi-view stereopsis evalu-ation. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 406413, 2014. 5 Daiju Kanaoka, Motoharu Sonogashira, Hakaru Tamukoh,and Yasutomo Kawanishi. Manifoldnerf: View-dependentimage feature supervision for few-shot neural radiance fields.In 34th British Machine Vision Conference 2023, BMVC 2023,Aberdeen, UK, November 20-24, 2023. BMVA, 2023. 2, 3",
  "Tzu-Mao Li, Miika Aittala, Fredo Durand, and Jaakko Lehti-nen. Differentiable monte carlo ray tracing through edgesampling. ACM Transactions on Graphics (TOG), 37(6):111, 2018. 2": "Zhemin Li, Hongxia Wang, and Deyu Meng. Regularizeimplicit neural representation by itself. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1028010288, 2023. 4 Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft raster-izer: A differentiable renderer for image-based 3d reasoning.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 77087717, 2019. 2 Yuan Liu, Sida Peng, Lingjie Liu, Qianqian Wang, PengWang, Christian Theobalt, Xiaowei Zhou, and Wenping Wang.Neural rays for occlusion-aware image-based rendering. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 78247833, 2022. 3 Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.Swin transformer v2: Scaling up capacity and resolution. InProceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 1200912019, 2022. 2, 5, 16",
  "Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradientdescent with warm restarts. arXiv preprint arXiv:1608.03983,2016. 17": "Dominic Maggio, Marcus Abate, Jingnan Shi, CourtneyMario, and Luca Carlone. Loc-nerf: Monte carlo localiza-tion using neural radiance fields. In 2023 IEEE InternationalConference on Robotics and Automation (ICRA), pages 40184025. IEEE, 2023. 1 Roger Mar, Gabriele Facciolo, and Thibaud Ehret. Sat-NeRF:Learning multi-view satellite photogrammetry with transientobjects and shadow modeling using rpc cameras. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 13111321, 2022. 3 Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-worth. NeRF in the wild: Neural radiance fields for uncon-strained photo collections. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 72107219, 2021. 3",
  "Christopher Maxey, Jaehoon Choi, Hyungtae Lee, DineshManocha, and Heesung Kwon. Uav-sim: Nerf-based syntheticdata generation for uav-based perception. arXiv preprintarXiv:2310.16255, 2023. 1": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:Representing scenes as neural radiance fields for view synthe-sis. In ECCV, 2020. 1, 2, 5, 6, 12, 14, 16 Michael Niemeyer, Jonathan T Barron, Ben Mildenhall,Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Reg-nerf: Regularizing neural radiance fields for view synthesisfrom sparse inputs. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages54805490, 2022. 1, 2, 3, 4, 5, 6, 16",
  "Seunghyeon Seo, Yeonjin Chang, and Nojun Kwak. Flipnerf:Flipped reflection rays for few-shot novel view synthesis.arXiv preprint arXiv:2306.17723, 2023. 2, 3": "Vincent Sitzmann, Justus Thies, Felix Heide, MatthiasNiener, Gordon Wetzstein, and Michael Zollhofer. Deepvox-els: Learning persistent 3d feature embeddings. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 24372446, 2019. 2 Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew J Davi-son. imap: Implicit mapping and positioning in real-time. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 62296238, 2021. 1 Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Prad-han, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron,and Henrik Kretzschmar. Block-NeRF: Scalable large sceneneural view synthesis. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages82488258, 2022. 3, 14",
  "radiance fields for indoor multi-view stereo. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision,pages 56105619, 2021. 3": "Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and JustinJohnson. Synsin: End-to-end view synthesis from a singleimage. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 74677477, 2020.2 Yongchang Wu, Zhengxia Zou, and Zhenwei Shi. Remotesensing novel view synthesis with implicit multiplane repre-sentations. IEEE Transactions on Geoscience and RemoteSensing, 60:113, 2022. 2, 5, 15, 16",
  "Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao,Anyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin.BungeeNeRF: Progressive neural radiance field for extrememulti-scale scene rendering. 2021. 3": "Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, JianminBao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simpleframework for masked image modeling. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 96539663, 2022. 3, 8, 16 Zeke Xie, Xindi Yang, Yujie Yang, Qi Sun, Yixiang Jiang,Haoran Wang, Yunfeng Cai, and Mingming Sun.S3im:Stochastic structural similarity and its unreasonable effec-tiveness for neural fields. In International Conference onComputer Vision, 2023. 8 Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, HumphreyShi, and Zhangyang Wang. Sinnerf: Training neural radiancefields on complex scenes from a single image. In EuropeanConference on Computer Vision, pages 736753. Springer,2022. 2 Linning Xu, Yuanbo Xiangli, Sida Peng, Xingang Pan, Nanx-uan Zhao, Christian Theobalt, Bo Dai, and Dahua Lin. Grid-guided neural radiance fields for large urban scenes. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 82968306, 2023. 14 Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Im-proving few-shot neural rendering with free frequency regu-larization. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 82548263,2023. 2, 4, 5, 6, 12, 16 Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.pixelnerf: Neural radiance fields from one or few images.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 45784587, 2021. 2, 3,5, 6, 16 Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,and Oliver Wang. The unreasonable effectiveness of deepfeatures as a perceptual metric. In Proceedings of the IEEEconference on computer vision and pattern recognition, pages586595, 2018. 5 Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,and Noah Snavely. Stereo magnification: learning view syn-thesis using multiplane images. ACM Transactions on Graph-ics (TOG), 37(4):112, 2018. 2",
  "A. Additional Experiments and Analysis": "Robustness to Hyperparameters. We have conducted aseries of experiments to assess the sensitivity of our modelto hyperparameters. Specifically, we focus on the hyperpa-rameter , which plays a crucial role in balancing differentcomponents of our loss function. In , we illustratethe impact of varying on the performance of the proposedMPNeRF and a standard NeRF model.As increases, we observe that the PSNR and SSIM met-rics tend to plateau, suggesting that there is an optimal rangefor wherein the model achieves a balance between fidelityand perceptual quality. On the other hand, the LPIPS metricshows an initial decrease followed by a gradual increase,indicating a sweet spot where the model best captures theperceptual features of the aerial scenes. The trends exhib-ited by MPNeRF show its relative insensitivity to withina reasonable range, which underscores the robustness ofour method. Notably, MPNeRF consistently outperforms thebaseline NeRF model across all metrics, demonstrating theeffectiveness of incorporating the multiplane prior to therendering process. . Hyperparameter Sensitivity Analysis. Performancecomparison of our method (MPNeRF) and a baseline NeRF modelacross different values of hyperparameter . The graphs showPSNR, SSIM, and LPIPS metrics. MPNeRF is robust to a wide choice.MPNeRF vs MPI-based Methods. The present study ofMPI mainly focuses on overcoming shortcomings such asfailure to represent continuous 3D space in . In contrast,our approach utilizes MPI as a bridge to convey complexinformation that a single NeRF struggles with. We constructcomparison experiments under 3-view settings among theproposed MPNeRF, the original MPI , and MINE In Table. 6, the original MPI achieves an 18.57 PSNR, 0.54SSIM, and 0.45 LPIPS. While MINE performs better with19.99 PSNR, 0.61 SSIM, and 0.40 LPIPS. Our MPNeRFoutperforms these methods by a large margin. These MPI-based methods face inherent limitations like ghosting effects",
  "NeRF branch w/t Lmul21.720.800.19": ". Comparation between the NeRF and MPI. This tablepresents the evaluation of the MPI based method in previous studies,NeRF branch without multiplane loss (Lmul), the MPI branch in-dependently, and the NeRF branch with Lmul within our MPNeRFframework. The metrics of PSNR, SSIM, and LPIPS demonstratethe significant impact of the multiplane prior on the rendering per-formance in sparse aerial scenes. NeRF Branch vs MPI Branch. In Table. 6, we examine theperformance impact of the NeRF and MPI branches withinour proposed MPNeRF. Initially, the NeRF branch withoutthe multiplane loss Lmul (equals to a plain NeRF model)demonstrates a PSNR of 15.13, an SSIM of 0.20, and anLPIPS of 0.58. These values indicate a baseline level ofperformance where the NeRF branch struggles with sparseaerial views, as evidenced by the low PSNR and SSIM scores,along with a high LPIPS value which suggests a significantperceptual difference from the ground truth. In contrast, theMPI branch alone shows better across all metrics, with aPSNR of 20.32, an SSIM of 0.57, and a reduced LPIPS of0.34. The MPI branchs improved performance is likely dueto its discrete depth-based representation that aligns betterwith the structured nature of aerial scenes, thus capturingthe scene geometry more effectively. And the inductive biasof CNN and Transformer makes MPI generalize better. Themost significant performance gains are observed when theNeRF branch is combined with the multiplane loss Lmul,resulting in a PSNR of 21.72, an SSIM of 0.80, and an LPIPSof 0.19. The addition of Lmul to the NeRF branch enhancesits ability to recover details from sparse views, as reflectedby the substantial improvements in PSNR, SSIM, and LPIPS.The proposed Multiplane Prior serves as a bridge to conveyinformation that is hard to learn by the traditional NeRFpipeline. These results underscore the efficacy of incorporat-ing multiplane priors into the NeRF framework for few-shotaerial scene rendering.Other Few-shot NeRF Methods Combined with Multi-plane Prior. It stands to reason that its worth evaluatingother Few-shot NeRF methods combined with multiplaneprior. We incorporated FreeNeRFs frequency regular-ization and evaluated it under a 3-view setting. This inte-gration results in a marginal increase in the PSNR by 0.2db.We believe the MPIs noisy predictions help reduce early",
  "Depth": ". Training Progression of MPNeRF. The sequence shows comparative results from the NeRF and MPI branches at various trainingstages under 3 view settings. Left to right: early, mid, and late phases of training. The NeRF branch initially shows noisier reconstructionswith indistinct depth estimations, while the MPI branch exhibits crop edge and overlapping ghosting effects. Over time, the NeRF branch,guided by the MPI-derived multiplane prior, progressively captures finer details and more accurate depth information, as reflected in thesharpening of depth map visualizations.",
  "B. Discussion and Future Works": "Why MPNeRF Works? Despite the advantage of the MPIrepresentation in aerial scenes, a simple question is: WhyMPNeRF is kept away from the cropped edge and over-lapping ghosting effect of the MPI? Avoiding the cropped edge is simple, we sample rays from unseen views follow-ing the mask generated during homography warping. Tobetter illustrate why the overlapping ghosting effect cannot be learned by NeRF, we visualize the same target viewrendered by the MPI branch in Figure. 7. With the sourceviewpoint varying, the overlapping ghosting effect in therendered target view differs. Since MPI derived from differ-ent viewpoints does not share a common world space, theseoverlapping ghosting effects are not multi-view consistentacross all views. Thus these effects violate the multi-view",
  "NeRF Branch": ". Detail Comparison between MPI and NeRF BranchOutputs. The images on the left column represent the MPI branchsoutput, displaying sharper details with overlapping ghosting effectsin the highlighted regions. In contrast, the right column showsthe NeRF branchs output, where the same regions appear moreblurred. consistency assumption of NeRF . With these noisesprovided as pseudo-supervision, the MLP optimized withgradient descent tends to give blurry rendering. The blurringsignifies NeRFs attempt to average out the incongruitiesacross views. These pseudo-labels, while derived from aninformed place, act as imperfect guides, introducing a trade-off that MPNeRF must navigate. On one hand, they providea rich, albeit noisy, signal that captures the complexity ofaerial scenes. On the other, they present a risk of pollutingthe training process with artifacts.Although MPNeRF shows that a simple MSE loss canperform well, this delicate balance highlights the importanceof a carefully crafted training regimen, one that can differen-tiate between useful signals and misleading noise. Our futurework will delve into refining this balance, potentially throughthe development of more sophisticated noise-filtering mecha-nisms or through the implementation of more robust trainingstrategies that can better leverage the nuanced informationwithin these pseudo-labels. In doing so, we may further en-hance MPNeRFs rendering quality, pushing the boundariesof few-shot aerial scene rendering.Semantic Integration for Improved Scene Understand-ing. Integrating semantic segmentation into the MPNeRFframework offers an exciting direction for enhancing sceneunderstanding. By associating semantic labels with the MPIbranch, MPNeRF may provide more contextually aware re-constructions and pave the way for applications in urbanplanning and navigation under limited data.Scene Editing. An exciting avenue for future research isthe possibility of editing NeRF-rendered scenes by directlymanipulating the MPIs generated by the MPI branch. Thiscould enable users to alter scene characteristics such as color,texture, or even geometric structure, through an intuitiveinterface. A potential direction is utilizing differentiable ren-dering techniques to backpropagate the desired edits from the scene rendering back to the MPI and NeRF representations.Scalability. Currently, scalability remains a potential limi-tation when our MPNeRF model is applied to larger scenes.The primary bottleneck arises from the inherent capacityconstraints of NeRF models. They are typically optimizedfor smaller, more controlled environments and can struggleto maintain fidelity at the increased scale and complexityof larger scenes. As scenes expand in size, the NeRFs neu-ral network requires a corresponding increase in capacityto model the additional detail, which can lead to a signifi-cant escalation in computational and memory requirements. Furthermore, the encoder-decoder architecture em-ployed within our MPI branch is not ideally suited for high-resolution imagery . It tends to consume substantialamounts of memory, especially when processing the finer de-tails necessary for large-scale scene rendering. The memoryfootprint grows rapidly with the resolution of input imagesdue to the quadratic increase in the number of pixels thatneed to be processed simultaneously.",
  "C. Additional Visualizations": "Training Progression of MPNeRF. Figure. 6 presents adetailed visual account of the training evolution within ourMPNeRF, delineating the comparative outcomes from theNeRF and MPI branches across three distinct training phases.The left columns illustrate the initial stage where the NeRFbranch outputs are notably noisier, and the depth maps lackprecise definition, signifying the models initial struggle tointerpret the sparse aerial views. These preliminary resultsare characterized by a lack of clarity and detail, with thedepth maps displaying broad, undifferentiated regions oflow confidence. As training progresses to the midpoint, dis-played in the center columns, the MPI branch starts to assertits strengths. It delivers reconstructions with improved clarityand begins to better capture the geometric intricacies of theaerial scenes. This enhancement is evident in the depth maps,where we observe a transition from broad, undefined areas tomore distinct regions of depth estimation, indicative of theMPI branchs capability to delineate structural features moreeffectively at this stage. Reaching the later stages of training,shown in the right columns, the NeRF branch, now informedby the multiplane prior, shows significant advancement. Itstarts to match and, in certain aspects, surpasses the MPIbranchs performance by delivering images with greater de-tail fidelity. This is most apparent in the depth maps, wherethe once diffused and expansive high-confidence regionshave now evolved into sharply defined areas, highlightingthe networks improved proficiency in depth perception.The visualization of the depth maps is particularly telling;the sharpening of these maps directly correlates with the im-proved models depth estimations. The NeRF branch, lever-aging the multiplane prior, demonstrates an enhanced abilityto resolve the complex spatial relationships inherent in aerial",
  "GT": ". Visualization of MPI Branch Depth Layers. Sequential depth layers from the MPI branch reveal the aerial scenes structure,evolving from translucent to opaque as we move from shallow to deep layers, culminating in the ground truth (GT) image for reference. scenes, moving beyond the initial limitations evidenced inthe early training outputs.This sequential improvement underscores the efficacy ofthe MPNeRF training process, which effectively leveragesthe distinct advantages of both NeRF and MPI branches toprogressively refine the models understanding of the scene,culminating in high-quality renderings from sparse inputs.The journey from noisy, indistinct initial attempts to clear,detailed final outputs exemplifies the potent potential ofMPNeRF for aerial scene rendering.Different Layers of the MPI Branch. To explore the geome-try and appearance captured by the MPI branch, we visualizethe color with transparency computed by the density of dif-ferent MPI layers. Figure. 8 showcases a series of imagesthat represent different layers of the MPI branch, each corre-sponding to a specific depth level within the aerial scene, aslabeled from Shallow to Deep. The images progress fromthe topmost layers, which capture high-elevation featureslike roofs, to the bottom layers, which reveal ground-leveldetails. However, it is evident that the fidelity of the recon- struction varies across depth layers. The initial layers, whilecapturing the broad layout, lack the finer details and thesharpness present in the ground truth (GT). The middle lay-ers begin to show more structure and texture, indicating anintermediate range where the MPI branch most effectivelycaptures the scenes appearance. The deeper layers, whilericher in detail, start to exhibit artifacts, such as blurringand possible misalignments, before converging towards theground truth. This suggests that while the MPI branch of MP-NeRF shows promise in reconstructing aerial scenes fromlimited data, it is still highly inaccurate and contains artifacts.",
  "convdown11768512encoder layer4ELU convdown23512256convdown1ELUconvup1 extra3256256convdown2ELUconvup2 extra1256768convup1 extraELU": "convup53768 + 21256cat(convup2 extra, depth embedding)ELUconv53256 + 768 + 21256cat(convup5, encoder layer3, depth embedding)ELUconvup43256128conv5ELUconv43128 + 384 + 21128cat(convup4, encoder layer2, depth embedding)ELUoutput431284conv4Sigmoid (for RGB) and abs (for )convup3312864conv4ELUconv3364 + 192 + 2164cat(convup3, encoder layer1, depth embedding)ELUoutput33644conv3Sigmoid (for RGB) and abs (for )convup236432conv3ELUconv2332 + 96 + 2132cat(convup2, encoder conv1, depth embedding)ELUoutput23324conv2Sigmoid (for RGB) and abs (for )convup133216conv2ELUconv131616convup1ELUoutput13164conv1Sigmoid (for RGB) and abs (for ) . Decoder Architecture for the MPI Branch. Each convup layer within our architecture is composed of a convolution layer,followed by batch normalization and the specified activation layer, as delineated in the table. This sequence is then succeeded by a 2nearest neighbor upsampling process. Conversely, the convdown blocks are structured beginning with a max pooling layer with a stride of2, followed by a convolution layer, and culminating with an activation layer. This architecture choice follows previous research in MPIrepresentations and depth estimation . tion of 512 512 pixels. This selection ensures a broadrepresentation of scenarios that MPNeRF might encounterin real-world applications. The LEVIR-NVS dataset encap-sulates a variety of pose transformations that mimic thedynamic nature of UAV flight patterns, including wrappingand swinging motions. These pose variations introduce re-alistic challenges in aerial photography, such as changes inviewpoint and scale, making the dataset a rigorous testingground for our model. The inclusion of these complex trans-formations in the simulation process is crucial for assessingthe robustness of MPNeRFs performance in conditions thatclosely approximate actual aerial image capture.In our experimental setup, we strategically select specificviews for training to assess the capability of our model inboth interpolation and extrapolation scenarios. For the three-view setting, we utilize view IDs: 0, 7, and 15. This selectionis designed to provide a spread of perspectives that chal-lenges the model to extrapolate the scene effectively. In thefive-view setting, we expand our selection to include viewIDs: 0, 7, 10, 15, and 20. This broader range tests the modelsinterpolation skills and its ability to extrapolate scenes frommore diverse viewpoints.In our experiments, we employ three standard metrics.Peak Signal-to-Noise Ratio (PSNR) is used to measure theimage reconstruction quality, calculated as the negative loga-rithm of the mean squared error between the predicted andground truth images. Structural Similarity Index Measure(SSIM), obtained via the skimage1 library, assesses imagequality based on luminance, contrast, and structural infor- mation. Learned Perceptual Image Patch Similarity (LPIPS),computed using a VGG-based model from the lpips2 pack-age, evaluates perceptual similarity, reflecting more human-centric assessments of image quality.Implementation of Baseline Methods. We implement thebaseline methods following their open-source code base. Weadopt 64 coarse sampling and 32 fine sampling for the NeRFbackbone of these methods. In particular, the RegNeRF and FreeNeRF are implemented based on Mip-NeRF, and others are based on a vanilla NeRF.All methods are trained for 30 epochs for each scene andthe hyperparameters are strictly consistent across all experi-ments.Implementation of MPNeRF. We implement MPNeRFbased on the nerf-pl codebase 3, which provides a PyTorchLightning framework for efficiently operationalizing NeRFarchitectures. The settings of hyperparameters are strictlyconsistent with baseline methods. Our NeRF branch adheresclosely to the original NeRF paper specifications, ensuringa faithful reproduction of the baseline model. We adopt 64coarse sampling and 32 fine sampling for the NeRF branch.Inspired by previous works , our MPI branch isconstructed following an encoder-decoder architecture MPIgenerator. The encoder is a strand SwinV2 Transformer pretrained via SimMIM . The encoder is kept frozenduring training. A detailed description of our decoder archi-tecture is presented in Table. 7. The MPI generator embedsdepth hypotheses into the input features, which are then pro- cessed through convolutional layers to output MPIs withRGB and density values, leveraging skip connections andmulti-scale representations for detail enhancement.For optimization, we utilize the Adam optimizer witha learning rate of 5 104, and a cosine learning rate decayscheduler . Our model is trained on a single NVIDIARTX 3090 GPU for 30 epochs, taking about 2.5 hours toconverge. The batch size is set to 1024 rays per iterationfor both seen and unseen views, allowing sufficient diversityof data points for gradient estimation while maintainingmanageable memory requirements."
}