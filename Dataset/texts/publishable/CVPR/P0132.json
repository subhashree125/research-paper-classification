{
  "Abstract": "Achieving robust generalization across diverse data do-mains remains a significant challenge in computer vision.This challenge is important in safety-critical applications,where deep-neural-network-based systems must perform re-liably under various environmental conditions not seen dur-ing training. Our study investigates whether the general-ization capabilities of Vision Foundation Models (VFMs)and Unsupervised Domain Adaptation (UDA) methods forthe semantic segmentation task are complementary.Re-sults show that combining VFMs with UDA has two mainbenefits: (a) it allows for better UDA performance whilemaintaining the out-of-distribution performance of VFMs,and (b) it makes certain time-consuming UDA componentsredundant, thus enabling significant inference speedups.Specifically, with equivalent model sizes, the resultingVFM-UDA method achieves an 8.4 speed increase overthe prior non-VFM state of the art, while also improvingperformance by +1.2 mIoU in the UDA setting and by +6.1mIoU in terms of out-of-distribution generalization. More-over, when we use a VFM with 3.6 more parameters, theVFM-UDA approach maintains a 3.3 speed up, while im-proving the UDA performance by +3.1 mIoU and the out-of-distribution performance by +10.3 mIoU. These resultsunderscore the significant benefits of combining VFMs withUDA, setting new standards and baselines for UnsupervisedDomain Adaptation in semantic segmentation. The imple-mentation is available at",
  "UDAAll data domains": ".Generalization capabilities of UDA methods andVFMs. UDA is designed to adapt a model from a labeled sourcedomain to an unlabeled target domain, whereas VFMs capture abroad spectrum of data distributions, contributing to the overallgeneralization. The goal of this research is to investigate and lever-age the combined in- and out-of-target generalization capabilitiesof UDA and VFMs. chance of good generalization in real-world environments,models are best trained on a diverse dataset having a broaddistribution, thus minimizing the likelihood of encounter-ing out-of-distribution data. However, for dense tasks likesemantic segmentation, obtaining abundant labeled data canbe costly and labor intensive , as every pixel has to be la-beled. As a result, annotated data is likely to be scarce, andnetworks trained on limited labeled data suffer from poorgeneralization due to the lack of exposure to sufficiently di-verse training examples. To address the lack of generaliza-tion, Vision Foundation Models (VFMs) andUnsupervised Domain Adaptation (UDA) have emerged, amongst other alternatives .UDA methods leverage unlabeled data to adapt a model to aspecific target domain, which is typically the domain wherethe model is to be deployed. By doing so, they do not nec-essarily aim for wide generalization beyond this target do-",
  "arXiv:2406.09896v2 [cs.CV] 17 Jun 2024": "main. In contrast, VFMs leverage extensive pre-training onlarge datasets to create models that can be used for efficientfine-tuning on various downstream tasks. Once fine-tunedon (limited) labeled data, these VFMs can generalize bet-ter than models that were not as extensively pre-trained. In other words, UDA methods focus on performingwell on a specific target domain, whereas VFMs can im-prove the generalization on domains that are unseen duringfine-tuning. Both types of generalization are important, andthey are illustrated in . In this work, we study whetherthe generalization capabilities of UDA and VFMs are com-plementary.Recently, Vision Foundation Models (VFMs) have madesignificant contributions by offering pre-trained models thatexcel in generalization, requiring minimal fine-tuning ondownstream tasks . Contrary to the traditionalapproach of pre-training models on labeled datasets like Im-ageNet or MSCOCO , Vision Foundation Models(VFMs) stand out by utilizing extensive pre-training on la-beled and/or unlabeled datasets. Training on unlabeled datais done using different self-supervised techniques such asmasked image modeling or self-training . Specifi-cally in the context of semantic segmentation, VFMs haveshown promising results in improving the performance todomains never seen during fine-tuning .Alternatively, Unsupervised Domain Adaptation (UDA)methods continue to make progress in enabling models toadapt to any unlabeled target domain . Toadapt a model to a target domain, UDA methods use alabeled source domain, consisting of either synthetic im-ages or real images. The goal is to bridge the gapbetween the source and target domains, transferring whatthe model has learned from the source to perform as wellas possible in the target domain. UDA methods are typi-cally only evaluated on this target domain, but this does notreflect their generalization performance. Therefore, Piva etal. proposed to evaluate these models on an additional,unseen dataset, and they show that UDA methods can alsoimprove the performance in this out-of-target setting.Despite significant individual advancements by VisionFoundation Models (VFMs) and Unsupervised DomainAdaptation (UDA) methods, both have been studied in iso-lation, and it remains an open question to what extent theyare complementary to each other. To address this gap inresearch, this work investigates the integration of VFM inUDA to obtain increased in- and out-of-target performance.For this purpose, we incorporate VFMs into a represen-tative state-of-the-art UDA method, MIC .We con-duct ablations over components, image resolution, and self-training strategies, and assess the impact of VFM size andpre-training strategy. Based on these results, we adopt thebest combination of VFM and UDA components which werefer to as the VFM-UDA method. The experimental results across synthetic-to-real and real-to-real scenarios demon-strate that VFMs can have a very positive influence on aUDA methods ability to perform well on both in- and out-of-target domains. This highlights the potential of their fu-ture use in combination with UDA methods.In summary, the contributions of this work are:",
  ". Related Work": "Vision Foundation Models (VFMs) have brought notableadvancements in generalization within computer vision, be-ing trained on large-scale data and adaptable for multipledownstream tasks.For instance, CLIP learns high-quality visual representations through contrastive learn-ing with large-scale image-text pairs. MAE utilizesa masked image modeling framework for image pixel re-construction. SAM , trained on a large-scale segmenta-tion dataset, extracts features from images and prompts topredict single or multiple segmentation masks. EVA02 applies masked image modeling to a CLIP models visualfeatures, offering a unique approach to visual representa-tion learning.DINOv2 , on the other hand, is pre-trained on carefully curated datasets without explicit su-pervision, showcasing its self-supervised learning strength.Most VFMs currently rely on the plain Vision Transformer(ViT) architecture, which outputs single-scale features,posing a design challenge when integrating them with UDA,as is explained in the next section.Unsupervised Domain Adaptation (UDA) methodsaim to increase the performance of a model on a knowntarget domain. This domain usually represents the envi-ronment where the model is likely to be deployed in thereal world.These models can leverage unlabeled targetdata and labeled data from a source domain to increasea models performance on a target domain. UDA meth-ods leverage techniques like feature alignment ,self-supervised learning and data augmen-tation to minimize the discrepancy between sourceand target domain distributions.Current UDA methodstypically use hierarchical encoders, consisting of eitherConvolutional or Transformer blocks thatyield multi-scale features to obtain optimal performanceon small-scale objects, whereas VFMs produce single-scalefeatures. As such, state-of-the-art UDA methods are not di- rectly compatible with VFMs in an optimal manner. Thiswork focuses on bridging this incompatibility and applyingthese UDA techniques to VFMs, assessing them outside thestandard practice of initializing on ImageNet , and eval-uating their effectiveness in adaptation settings that considerboth in-target as well as out-of-target performance.To the best of our knowledge, leveraging the combinedgeneralization capabilities of VFMs and UDA has not beenexplored, and filling this gap is what we aim for in our work.",
  ". VFM-UDA": "UDA baseline.As a baseline, we start from MIC ,a state-of-the-art Unsupervised Domain Adaptation (UDA)method. MIC utilizes a student-teacher framework with some additional UDA components. Inthe student-teacher framework, the teacher network gener-ates pseudo labels for the target domain which are then usedto supervise the student network. The student network usesa vanilla cross-entropy loss on the labeled source domainand on the unlabeled target domain using the pseudo la-bels generated by the teacher network. The teacher networkis updated with an exponential moving average (EMA) us-ing the student models parameters. Below, we specify theadditional components of this UDA method. In Sec. 4.2,we assess the effectiveness of each of these components incombination with a VFM. For the final VFM-UDA model,we keep only the components that remain effective.Feature Distance (FD) is a UDA strategy that addsa Mean Squared Error (MSE) loss between the studentsencoder output and those of a frozen pre-trained encoder.Minimizing this MSE loss encourages the student model toretain the features learned during pre-training, balancing theadaptation to new domain-specific features with the preser-vation of essential general features.Masked Image Consistency (MIC) is a UDA strat-egy that introduces an asymmetry between the teacher andstudent models by masking out parts of the original imagesfor the student in the target dataset. This is achieved by ran-domly generating a patch mask and masking out differentparts of each image. This masking forces the student modelto infer from contextual information from the unmasked re-gions.HRDA is a model architectural change that is aimedat making high-resolution segmentation predictions in bothUDA and conventional supervised learning . This isachieved by conducting semantic segmentation on bothhigh-resolution (HR) and low-resolution (LR) crops of an image. The resulting segmentation predictions for the LRand HR crops are fused by a learned scale-attention head.This fusion approach leverages the detailed informationfrom HR crops and the broader context from LR crops, withthe added drawback of having to do multiple forward passesfor one image. VFM encoder.We choose the DINOv2 VFM as theprimary encoder on top of which we conduct UDA, butwe also evaluate alternative VFMs in our experiments inSec. 4.3. The UDA baseline method uses the MiT-B5 en-coder, which produces multi-scale features. However, be-cause all performant VFMs use single-scale VIT-based en-coders, we need to make an adaption to the MIC baseline.This adaption is performed in the decoder, as describedin the next section.To ensure a fair comparison of theVFM-UDA method to the baseline, we use encoders witha roughly equal number of learnable parameters. Specifi-cally, we use the ViT-B/14 encoder with 86M parameters,while MiT-B5 has 81M parameters. VFM decoder.MIC and other well-performing UDAmethods use decoders that are designed for encoders thatoutput multi-scale features.However, ViTs only outputsingle-resolution features. This difference motivates us touse a different, yet much simpler decoder architecture thatis specifically designed for ViTs. Our decoder, depicted in, is inspired by the Segment Anything Models (SAM) upsampling stage but is slightly modified for our usecase. Compared to the SAM models upsampling stage, weintroduce an additional 33 Conv2D before the final out-put. While a more complex and larger decoder head couldbe used, DINOv2 already performs well with a simple lin-ear decoder and a frozen encoder . This suggests that alarge decoder is not necessary for VFMs due to their exten-sive pre-training. The more efficient decoder for the VFM-UDA approach contains 1.8M parameters, in contrast to theMIC models decoder, which has 5.2M parameters. VFM masking.The baseline UDA method, MIC, usesdirect image masking for the image masking consistencyloss. In our approach, instead of applying a mask directlyto the image, we mask the patch tokens and replace themwith a learnable token, similarly to how BEiT is trained .This adjustment acknowledges the architectural differencesin ViT models and optimizes the process for token-basedarchitectures.",
  ". Decoder head architecture": "Domain adaptation setup.To assess the UDA capabili-ties of models, we evaluate the performance in synthetic-to-real and real-to-real adaptation scenarios. The synthetic-to-real scenario allows us to assess how well the modelcan bridge the gap between computer-generated images andreal-world images, representing an extreme case of domainshift. On the other hand, real-to-real experiments test themodels ability to adapt between different real-world con-ditions, reflecting more subtle variations and complexitiesfound in natural settings. This dual approach ensures a thor-ough evaluation of the VFM-UDA integration, highlightingits adaptability and performance across different visual do-mains. In- and out-of-target evaluation.To truly assess amodels generalization capabilities, it should also be eval-uated beyond the domain of its target dataset. Therefore,similarly to Piva et al. , we additionally evaluate eachmodel on another dataset that falls outside of the distribu-tion of the target domain. In other words, to measure a UDAmethods performance both in-target and out-of-target, weuse two completely separate evaluation datasets. This addi-tional out-of-target dataset is never used during UDA train-ing, to ensure there is no data leakage. Training and vali-dation splits are made for both of these evaluation datasets.The training split for the out-of-target dataset is only used",
  "to determine the oracle performance": "Baseline, UDA, and oracle.Our benchmark comparesthe UDA performance with respect to a baseline and an or-acle. The baseline is trained on only the source domain, ina supervised manner, representing the models performancebefore domain adaptation. The oracle is trained on only thelabeled target data, also in a supervised manner, and reflectsthe empirical upper bound of a models performance on thetarget domain. Both the baseline and the oracle rely onlyon supervised learning, meaning they do not use unlabeleddata. In contrast, in the experiments with the UDA meth-ods, we train on the source domain with labeled data andtry to adapt to the target domain with unlabeled data, usingdifferent pre-training configurations and model sizes. Datasets.To assess the in-target performance, follow-ing previous state-of-the-art methods, we use GTA5 Cityscapes as the synthetic-to-real scenario.Forthe real-to-real adaptation scenarios, we use Cityscapes Mapillary . To assess the out-of-target performance, weuse WildDash2 , a completely separate dataset from thesource and target datasets. We choose WildDash2 becauseit includes city, highway, and rural scenes under variousweather conditions, and because the images are captured inmore than 100 countries, providing diverse and challengingimagery. Implementation details.The encoder is a vanilla ViT-B/14 with DINOv2 pre-training. The learning ratefor the decoder is 1.4 104 and for the encoder it is1.4 105. We train for 40,000 iterations with a batch sizeof 8, and use the AdamW optimizer . We use a linearlearning rate warmup of 1,500 iterations and linear decayafterwards. During training, the source dataset is sampledusing rare-class sampling to address class imbalances.When training UDA methods, we use a student-teachersetup, where the students weights are aggregated duringtraining into an EMA teacher model. The running weightfor the EMA model is = 0.999. This EMA teacher modelis never backpropagated and is only used for pseudo-labelgeneration. When creating the pseudo labels, the target im-ages are not augmented, but we use horizontal flip aggre-gation to create the final pseudo label to reduce labelingnoise. The threshold on the softmax outputs to generatethe final pseudo labels is = 0.968 . We use a mask ratioof r = 0.7, like in the original MIC . However, di-verging from MICs strategy of masking image regions di-rectly, our adaptation involves masking patch tokens whenusing a ViT encoder. The target images are mixed with thesource images using DACS data augmentation. The fi-nal VFM-UDA method does not use the FD loss or HRDA,see Sec. 4.2. Our experimental setup aims to investigate the followingperformance aspects: VFM-UDA vs. existing UDA methods: We assess theperformance of VFM-UDA against current UDA meth-ods in synthetic, real, in-target, and out-of-target settings,focusing on segmentation quality. This wide range of testscenarios gives insights into each UDA methods gener-alization capabilities, crucial for real-world deployment. Ablation on UDA components: We conduct an in-depthevaluation of the individual impact and contributions ofvarious UDA components within the VFM-UDA frame-work. This includes examining the effects of resolutionadjustments, masking strategies, FD, and HRDA.",
  ". Generalization of UDA with VFMs": "Overall findings.The results of both the synthetic-to-realand real-to-real adaptation scenarios can be seen in Tab. 1and Tab. 2, respectively. In this experiment, we only con-sider the VFM-UDA method with ViT-B/14, since it hasa similar parameter count as MIC. On both UDA bench-marks, VFM-UDA demonstrates superior in-target and out-of-target performance compared to the current state-of-the-art UDA method, MIC. In the synthetic-to-real scenario,VFM-UDA adapts better than MIC by +1.2 mIoU pointsand generalizes better by +6.1 points. In the real-to-realone, the integration surpasses MIC even more, with differ-ences of +5.8 mIoU in-target and +7.8 out-of-target.These results show that the generalization capabilities ofVFMs and UDA methods are complementary, as the VFM-UDA combination achieves better UDA performance thanthe state-of-the-art UDA methods, while maintaining oreven slightly improving the out-of-target performance ofthe VFM.",
  "Effect of model size.When integrating UDA with a sig-nificantly larger model, ViT-L/14, the adaptation and gener-alization capabilities of the model increase even more, out-": "performing the state-of-the-art UDA method by larger mar-gins. In the real-to-real scenario, it is interesting to notethat the combination VFM-UDA yields only a minor per-formance increase compared to its VFM baseline, both in-target and out-of-target. Essentially, when the VFM is large,the added benefits from UDA to the models overall per-formance become marginal. This suggests that the perfor-mance improvements obtained by scaling VFMs might limitthe additional generalization benefits achievable by pairingwith UDA techniques, especially in simpler settings likereal-to-real scenarios. For a more in-depth analysis of thescalability of VFM-UDA with different pre-trainings, werefer to Sec. 4.4.Next, we will also demonstrate that these larger modelscan be faster than the smaller state-of-the-art UDA method.",
  ". Evaluation of UDA components": "To investigate how each UDA component affects the over-all adaptation performance when integrated with VFMs, weuse the synthetic-to-real adaptation scenario. The baselineUDA method only applies supervised learning to the sourcedomain and self-training to the target domain. Using thisbaseline, we try to incrementally improve it by introducingthe following configurations: Incorporation of masking: adding Mask Image Consis-tency (MIC) when performing self-training, either at im-age or token level.",
  "Multi-resolution training:applying multi-resolutiontraining by fusing high-resolution and low-resolution pre-dictions, as proposed in HRDA": "Findings.Our analysis, detailed in Tab. 3, reveals nu-anced performance impacts for each UDA component. To-ken Masking, as opposed to the original Image Maskingused in MIC, yields a slightly better performance. Whenwe use either the FD loss or HRDA on top of Token Mask-ing, there is a noticeable decline in mIoU. This suggests thatthese components may not translate as effectively to ViT-based encoders, which lack hierarchical features that arepresent in the MiT-B5-encoder-based UDA methods. Al-though the full combination of Token Masking, the FD loss,and HRDA shows some improvement over using them sep-arately, the combined effect still does not exceed the perfor-mance of the Token Masking alone. These findings implythat the FD and the HRDA components may be redundantwhen using ViT-based VFMs. Therefore, we do not incor-porate them in the final VFM-UDA method.",
  "MIC (MiT-B5)1024204875.910721.0": "DaFormer (MiT-B5)512102468.31109.7SePiCo (MiT-B5)640128070.31169.3HRDA (MiT-B5)1024204873.810721.0MIC (MiT-B5)1024204875.910721.0VFM-UDA (ViT-B/14)1024204877.11288.4VFM-UDA (ViT-L/14)1024204879.03233.3 . Inference runtime analysis. After adapting GTA5 Cityscapes, the performance of all UDA methods is measured onan Nvidia A6000 GPU with 16-bit mixed precision. The VFM-UDA combination benefits from higher inference speed comparedto its baseline MIC, even when using ViT-L/14 which has 3.6more parameters.",
  "ViT-S/1466.969.746.056.2ViT-B/1468.177.151.861.3ViT-L/1467.379.054.665.5": ". Effect of model size with different pre-training strate-gies. We use an ImageNet pre-trained ViT as our non-VFM modeland DINOv2 as the VFM. Adapting GTA5 Cityscapes and scal-ing with model size, the ImageNet pre-trained model is unable toscale in performance, while the DINOv2 shows a consistent in-crease for both in- and out-of-target. our analysis to include two other VFMs, EVA02 andEVA02-CLIP .These models were chosen for theirclose performance to DINOv2, making them suitable can-didates for comparison . We evaluated these VFMs in asynthetic-to-real adaptation scenario, specifically adaptingGTA5 to Cityscapes, and assessed both in-target and out-of-target performance to study their adaptation and general-ization capabilities. Findings.Tab. 5 shows that DINOv2 consistently outper-forms EVA-02 and EVA-02-CLIP with a significant marginof +4.8 mIoU points in terms of in-target performance and+2.9 points in terms of out-of-target performance comparedto EVA-02-CLIP. While EVA-02 and EVA-02-CLIP yieldsimilar results in out-of-target scenarios, EVA-02-CLIP sur-passes EVA-02 in terms of in-target performance. This ex-periment underscores DINOv2s superior adaptability andgeneralization, supporting its selection for the VFM-UDAmethod.",
  ". Conclusions": "In this work, we explore whether the generalization capa-bilities of UDA and VFMs are complementary, to obtainmodels that can excel at both adaptation to a specific targetdomain and generalization beyond this target domain. Dueto architectural differences between VFMs and encoderspreviously used in UDA, we made the necessary adjust-ments for the combined model to work in multiple configu-rations. From the experiments, we found that at equivalentmodel sizes, the combined VFM-UDA model can (a) adaptbetter to target domains than current state-of-the-art UDAmethods, while (b) maintaining or even slightly improv-ing the out-of-distribution generalization performance ofVFMs. Moreover, we found that the VFM-UDA combina-tion benefits from increased model scale, as larger VFMsyield higher in- and out-of-target performance. This studysets new standards and baselines for UDA for target-specificadaptation and out-of-distribution generalization, and offerspractical guidelines for integrating VFMs into UDA to har-ness their joint benefits. AcknowledgmentThis work has received funding fromChips Joint Undertaking (Chips JU), under grant agree-ment No 101097300. The Chips JU receives support fromthe European Unions Horizon Europe research and in-novation program and Austria, Belgium, France, Greece,Italy, Latvia, Netherlands, Norway. This work made use ofthe Dutch national e-infrastructure with the support of theSURF Cooperative using grant no. EINF-7020, which isfinanced by the Dutch Research Council (NWO).",
  "Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bul`o, andPeter Kontschieder. The Mapillary Vistas Dataset for Seman-tic Understanding of Street Scenes. In ICCV, 2017. 4": "Maxime Oquab, Timothee Darcet, Theo Moutakanni, HuyVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mah-moud Assran, Nicolas Ballas, Wojciech Galuba, Rus-sell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra,Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu,Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin,and Piotr Bojanowski.DINOv2: Learning Robust VisualFeatures without Supervision. TMLR, 2024. 1, 2, 3, 4, 6, 7",
  "Fabrizio J. Piva and Gijs Dubbelman.Exploiting ImageTranslations via Ensemble Self-Supervised Learning for Un-supervised Domain Adaptation. CVIU, 234:103745, 2023.2": "Fabrizio J. Piva, Daan de Geus, and Gijs Dubbelman. Em-pirical Generalization Study: Unsupervised Domain Adapta-tion vs. Domain Generalization Methods for Semantic Seg-mentation in the Wild. In WACV, 2023. 4 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, GretchenKrueger, and Ilya Sutskever. Learning Transferable VisualModels From Natural Language Supervision.In ICML,2021. 2",
  "Stephan R. Richter, Vibhav Vineet, Stefan Roth, and VladlenKoltun.Playing for Data: Ground Truth from ComputerGames. In ECCV, 2016. 2, 4": "German Ros, Laura Sellart, Joanna Materzynska, DavidVazquez, and Antonio M. Lopez. The SYNTHIA Dataset:A Large Collection of Synthetic Images for Semantic Seg-mentation of Urban Scenes. In CVPR, 2016. 2 Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,Aditya Khosla, Michael S. Bernstein, Alexander C. Berg,and Li Fei-Fei. ImageNet Large Scale Visual RecognitionChallenge. IJCV, 115(3):211 252, 2015. 2, 3, 6",
  "Wilhelm Tranheden, Viktor Olsson, Juliano Pinto, andLennart Svensson. DACS: Domain Adaptation via Cross-Domain Mixed Sampling. In WACV, 2021. 4": "Midhun Vayyat, Jaswin Kasi, Anuraag Bhattacharya, ShuaibAhmed, and Rahul Tallamraju. CLUDA: Contrastive Learn-ing in Unsupervised Domain Adaptation for Semantic Seg-mentation. arXiv preprint arXiv:2208.14227, 2022. 1, 2, 3 Zhixiang Wei, Lin Chen, Yi Jin, Xiaoxiao Ma, Tianle Liu,Pengyang Ling, Ben Wang, Huaian Chen, and Jinjin Zheng.Stronger, Fewer, & Superior: Harnessing Vision FoundationModels for Domain Generalized Semantic Segmentation. InCVPR, 2024. 2 Binhui Xie, Shuang Li, Mingjia Li, Chi Harold Liu, GaoHuang, and Guoren Wang.SePiCo:Semantic-GuidedPixel Contrast for Domain Adaptive Semantic Segmentation.IEEE TPAMI, 45(07):90049021, 2023. 1, 2, 3, 6"
}