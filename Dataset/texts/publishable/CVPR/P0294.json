{
  "Abstract": "Datasets labelled by human annotators are widely usedin the training and testing of machine learning models. Inrecent years, researchers are increasingly paying attentionto label quality. However, it is not always possible to ob-jectively determine whether an assigned label is correct ornot. The present work investigates this ambiguity in the an-notation of autonomous driving datasets as an importantdimension of data quality. Our experiments show that ex-cluding highly ambiguous data from the training improvesmodel performance of a state-of-the-art pedestrian detec-tor in terms of LAMR, precision and F1 score, thereby sav-ing training time and annotation costs. Furthermore, wedemonstrate that, in order to safely remove ambiguous in-stances and ensure the retained representativeness of thetraining data, an understanding of the properties of thedataset and class under investigation is crucial.",
  ". Introduction": "A crucial yet difficult task in computer vision for au-tonomous driving and driver assistance systems is the detec-tion of vulnerable road users, such as pedestrians, cyclistsor motorcyclists, and including persons with impaired vi-sion, hearing or mobility. To this day, this group carriesthe highest risk of injuries and casualties in traffic acci-dents . Therefore, the development of systems ensur-ing and improving the protection of these road users is animportant step towards enhancing traffic safety for all par-ticipants. However, the detection of persons in street sceneimages is challenging given the individuality and diversityof human appearance.In the past decade, the capabilities of computer visionsystems have seen remarkable progress through the employ-ment of deep learning models, which require vasts amountsof data for training and testing. This data comes in two dif-ferent forms: (un-annotated) raw data and annotated data.For object detection, the annotations indicate the identity ofthe objects through class labels, as well as their localiza-tion, most commonly in the form of bounding boxes. They can also include further information, such as orientation, orwhether the object is partly occluded. Our experiments fo-cus on supervised learning, where the quality of the labelleddata is already an important consideration at training time.However, even in the case of unsupervised learning, in or-der to monitor and ensure the trained models performance,annotated data as a ground truth is needed. For this rea-son, annotation quality is crucial for both training regimes,supervised as well as unsupervised. It is possible to syn-thetically generate ground-truth images for both testing andtraining, but these lack behind real street scene images in di-versity . Hence, images labelled by human annota-tors are still considered the gold standard for ground-truthdata.Human annotation however comes with its own chal-lenges. As humans, we are not immune to errors. A smallpercentage of the data, even in easy cases, will therefore belabelled incorrectly by human annotators. Moreover, someinstances are inherently difficult to label, which often leadsto disagreement between different annotators. We refer toimages and instances, where the correct label is not entirelyobvious as ambiguous. The following section investigatesthis ambiguity as an important aspect of data quality for thecase of vulnerable road users. The results of our experi-ments, which are presented in , demonstrate thatimproved model performance can be achieved by removinghighly ambiguous instances from the training set.",
  ". Related Work": "Awareness of issues with the reliability of ground truth la-bels has risen in recent years, marked by publications con-cerned with the correctness of the annotations in large pub-lic datasets and benchmarks, such as ImageNet and CIFAR-10 . At the same time, a large numberof publications is concerned with how to handle noisy la-bels in object classification and detection, and how to trainnetworks, which are robust against noise . However, the definition of label noiseused in this field of research implies that there is an under-lying true label, which can be observed. Due to the chal-lenges detailed in the following sections and the resulting",
  "arXiv:2405.08794v1 [cs.CV] 14 May 2024": "subjectivity in the labelling of difficult tasks, this is not al-ways the case. In comparison to studies on label noise, amuch smaller number of publications exists, which is con-cerned with incorporating ambiguity information into thetraining data. Starting with Gao et al. (2017) , neu-ral networks have been employed in distribution learningmodels, to learn a label distribution instead of binary ormulti-class labels. Distribution learning approaches do notalways utilize the variability in the annotations to derive theground truth distributions, but oftentimes these are modeledimplicitly from neighboring classes , from featuresextracted by a neural network , or most recently, usingtransformers . A reason for this is that information onthe variability of annotator answers is not readily availablefor most datasets .",
  ". Definition": "Labelling vulnerable road users in street scene images is nota simple task, and therefore involves a high level of ambi-guity. This ambiguity arises from several challenges, mak-ing it difficult to recognize instances as their correct class,or to distinguish them from their neighboring classes. In-stances, which are partly or even heavily occluded are hardto detect for human annotators as well as machine learn-ing models. The same is true for objects with low visibil-ity, such as blurry instances or those that are far from thecamera. The wide variety of lighting conditions found instreet scenes poses an additional challenge. This leads toan ambiguity in the images where the true label is not al-ways observable. Even when annotated by experts and inthe absence of errors, the assigned labels will therefore re-tain a degree of subjectivity. This problem has already beendescribed for the field of medical images as inter-observervariability . Another term often used in the literature islabel noise , which usually implies that there is a cor-rect label observable from the data, and annotator answersdeviating from it are incorrect and add noise to the annota-tion. In contrast to this, we define ambiguous data as anyinstances, where different annotators will disagree on whatlabel to assign, because the true label is not entirely objec-tively observable. On the example of the class pedestrian,we further examine the sources of this ambiguity in the fol-lowing. These can be found in properties of the image itself,or different possible interpretations of the class definitionsin the labelling guide, i.e. the instructions, which are givento the annotators when labelling the images.",
  "Image PropertiesAmbiguity can arise from the image it-self, if the visibility of the instance is impaired due to ad-verse weather, blurriness or low contrast in the image, par-": "tial occlusion by another object, or the object being far awayfrom the camera. This form of ambiguity will always existin street scene images, which are taken from a vehicle driv-ing outside of controlled conditions in the wild. shows examples of this for the class pedestrian. While inthe image in 1a the person is easily identifiable, in 1b clas-sification of the instance is much more difficult. Image 1cshows an instance which is highly ambiguous due to lowvisibility. Without additional data, such as tracking of theperson throughout an image sequence, it is in this case im-possible to tell with certainty, if in reality this is the imageof a person or not. However, additional information, whichwould help us distinguish between real pedestrians andother object classes is usually not given in publicly avail-able datasets, and not always recorded during the capturingof the images. Images 1d to 1f illustrate how occlusion,which is a common challenge in image annotation, causesdifferent degrees of ambiguity. Class DefinitionsIn addition to image properties, anothercommon cause of ambiguity is that of instances falling inbetween the definitions of neighboring classes in the la-belling guide, e.g. a person could be either labelled a pedes-trian or cyclist depending on whether and how they are us-ing a bike. To some extent, this can be managed by coveringmany possibilities in the labelling instructions. However,even the most detailed class description will not be able tocover all possible cases, especially for such a diverse classas pedestrians. We illustrate this issue using examples fromthe neighboring classes pedestrian and rider of e.g. abike, motorbike or scooter. Very often, the distinction be-tween the two is made such, that persons who are walkingor standing, are to be labelled as pedestrians, while someoneriding a bike or scooter is classified as a rider. So a personwho is only holding or pushing a bike, but not currently rid-ing one in the image, is by this definition a pedestrian andnot a rider. But then what about someone who is sitting onthe bike (i.e. strictly speaking not walking or standing), butfor example waiting at a traffic light, should they be consid-ers as a rider or a pedestrian? Since this is a very commonedge case, the widely adapted distinction here is that any-one who has at least one foot on the ground is to be labelledas pedestrian. However, this brings us to the next prob-lem, because it is not always clear, whether or not this is thecase in an image. illustrates this on six examples.According to the above distinction, the person in the im-age in 2a is clearly identifiable as a pedestrian, because theyhave one foot on the ground. Following the same rule, theinstance in 2d is to be labelled a rider, because both feet areon the vehicle. The remaining four images exhibit differentdegrees of ambiguity w.r.t. this distinction. In images 2band 2c it is not clear, whether or not the person is on a vehi-cle. In images 2e and 2f it is difficult to tell if the criterion",
  ". Neighboring Classes: Pedestrian versus Rider, with the distinctive criterion that a person with at least one foot on the groundis to be labelled as a pedestrian. Examples from the ECP Dataset": "of one foot being on the ground is met or not, i.e. if this isa pedestrian or rider per the definition. For these instances,we can expect disagreement between the annotators whichof the two neighboring classes to assign.If we include such cases in the labelling guide as well,e.g. by always deciding for one of the two classes, if thelegs are not both visible, we will be able to cover more suchinstances with the instructions, but we will never be able tocome up with a finite set of rules that is able to cover allimaginable cases. Moreover, we will want to keep our in-structions as concise as possible, because the longer the la-belling guide gets, the more this itself can become a sourceof annotation errors. At some point, the annotators will notbe able to correctly remember all the rules we have laid outfor them during the process of the annotation. So there willalways be some remaining edge cases which might be la-belled differently depending on the annotators interpreta-tions. Awareness of these challenges and possible pitfalls iscrucial, when making decisions w.r.t. the labelling instruc-tions and class definitions.Since for these reasons a certain degree of ambiguity isinevitable when annotating a dataset, should all these in-stances be treated identically during training, regardless oftheir different degrees of ambiguity? And how are highlyambiguous cases to be handled during testing and evalua-tion of a trained model? Should, for example, the model re-ceive an equally high penalty for not finding the instance inc as it should for not correctly detecting the personin 1a? As a cost-efficient measure, we investigate the ef-fects of simply removing highly ambiguous instances fromthe data.",
  ". Model and Training": "Our experiments were conducted using data from the Eu-roCity Persons Dataset (ECP) , which is a prominentbenchmark for pedestrian detection. Since the test datasetof the benchmark is not publicly available, we used thepublished validation set as our test set. We chose Pede-stron for evaluation, the highest performing modelfrom the benchmark, for which the full architecture as wellas pretrained weights are published. This is a Cascade R-CNN model , originally with an HRNet backbone,which we replaced with MobileNetV2 to achieve stillclose-to benchmark performance, but at greatly reducedtraining times.Each model was trained for 50 epochs,which took approximately 4 days on a single NVIDIA RTX4090. The reasoning for stopping the training early andchoosing a more light-weight backbone was to enable usto train more iterations of the model in the same time, sincewe were interested in the comparative performance of the",
  ". ECP Evaluation Subsets": "model trained on different data, instead of reaching peakperformance. We could however confirm that, while train-ing the model for 100 more epochs still lead to minor per-formance gains, the comparative results between the mod-els stayed the same. The performance of the trained mod-els was evaluated using the official evaluation measure ofthe ECP benchmark, Log Average Miss Rate (LAMR) .In short, the LAMR expresses the trade-off between themiss rate (ratio of ground truth pedestrians that were notdetected) and false positives per image (other objects themodel falsely detected as pedestrians) for different thresh-olds of confidence scores returned by the model.",
  ". Measuring Ambiguity": "In order to analyse the effects of ambiguous data on train-ing and testing, we need a way to quantify ambiguity withinthe annotations. For our experiments, we focused on the an-notation question whether the instance under considerationis a human being. Annotators are asked to respond to thisquestion with either yes or no, or indicate that they areunable to give a definite answer (denoted ? in the fol-lowing). The frequencies of these answers for a given task,nyes, nno and n?, are then used to calculate a heuristic mea-sure for ambiguity from annotator disagreement , whichdefines the ambiguity of an instance as",
  "where 1 n?": "n re-scales the distance of the observed dis-tribution of answers from a uniform distribution by the ratioof ?-answers, such that, if only ?-answers are given bythe annotators for the task, the ambiguity reaches its maxi-mum value of 1.For ECP, only hard labels with no information on an-notator disagreement exist within the published benchmarkdata. As a cost-efficient alternative to re-annotating the en-tire training and validation sets with multiple annotators forthe above question, we employed the approach proposedby to estimate the answer distributions. The modelpretrained on the ECP Dataset has been proven to estimate . Results for two training sets and three test sets including different degrees of ambiguity. Original denotes the original ECPtraining and validation sets, Amb 0.65 and Amb 0.5 the same subsets pruned above an ambiguity threshold of 0.65 and 0.5. annotator answers for ECP with high accuracy . Theambiguity measure was then computed from the predictedanswer distributions.To compare the effects of ambiguity on both, trainingand test set, we removed highly ambiguous instances up todifferent ambiguity thresholds from the dataset and trainedthe model on the entire original data as well as on the ver-sions of the dataset with applied ambiguity thresholds. Wethen evaluated the trained models on test data including in-stances, again, up to different ambiguity thresholds. Theresults for two models, one trained on all original data, andone trained without instances with ambiguity score 0.65,which were then tested on three different version of the testset (all data vs. ambiguity thresholds of 0.65 and 0.5), areshown in . Reasonable, Small, Occluded,and All are the original subsets of the ECP benchmark forevaluation (see ).",
  ". Results": "Removing ambiguous data from the training datasetimproves model performance. shows that themodel trained without highly ambiguous instances achieveshigher performance (lower is better for the LAMR), exceptwhen heavily occluded instances are included in the evalu-ation. Upon further investigation of the prediction results,we found that the reason for this better performance is, that for instances up to moderate occlusion, removing ambigu-ous instances from the training set improves precision at theexpense of only a small decline in recall. Precision, recalland F1 score for the two different training regimes whentested on data with and without high ambiguity are given in. We can see that the model trained without highlyambiguous data also performs better in terms of both pre-cision and F1 score. Visual inspection of the detection er-rors confirmed that ambiguous data in the training set con-tributes to the generation of false positive detections. Thistrend is observable regardless whether the model was testedon data including or excluding ambiguous data. The recallslightly declines in all testing scenarios when the model istrained without the ambiguous data, most notably for theoccluded test subset. This might indicate that some ofthe removed ambiguous instances still convey information,which can help the model learn more diverse representa-tions, especially in the presence of occlusion. Note that, as can be expected, removing ambiguous datafrom the test set improves all metrics for both trained mod-els. Nonetheless, the implication of these observations isless obvious: You can ignore ambiguous data in both sets,resulting in reduced cost through lower training times. Si-multaneously, annotation costs can be reduced, because itis possible to estimate the ambiguity measure reliably forhigh-ambiguity instances, and thereby exclude them from",
  "the annotation process all together": "There is a strong correlation between ambiguity and oc-clusion.When comparing the ambiguity measure with theocclusion tags in the ground truth (see ), we observethat higher values of the ambiguity measure correspond toa greater prevalence of occlusion tags within the dataset.As ambiguity increases, the proportion of tags indicatinghigher levels of occlusion is also elevated. This is evidentin , where the peak of the occluded > 80 tag pro-portion is at an ambiguity measure value of 0.79. This ex-plains why the model trained with applied ambiguity thresh-old, which achieves higher performance in all other evalua-tion subsets in terms of LAMR, is surpassed by the modeltrained on the original training set including all ambigu-ous data when evaluation is perfomed on the occluded sub- set (see ). When removing ambiguous instances,we disproportionately remove occluded instances. Hence,the model which has seen more occluded data in trainingperforms better on this specific subset, while it still exhibitslower performance on all other data.",
  ". Improving Model Performance at ReducedTraining and Annotation Costs": "Based on the findings detailed above, we propose the fol-lowing course of action for treating ambiguity in machinelearning datasets, especially for safety-critical applications:1. Assess possible sources of ambiguity in the labellingguide. Deriving simple rules, which are easy to commu-nicate and cover the most important edge cases can helpreduce ambiguity during the annotation process withoutadding possible sources of errors through excess difficulty",
  ". Distribution of occlusion and truncation tags for different ambiguity thresholds": "for the annotators.2. Quantify the ambiguity within the data. This can bedone from the raw annotator answers or by estimation fromthe labelled data. Choose a method which is cost-efficient,as well as a quantitative measure which is appropriate foryour use-case and interpretable, e.g. by providing a rankingof the instances w.r.t. ambiguity.3. Inspect a subset of the labelled results visually at dif-ferent ambiguity thresholds. Examine the distributions ofthe ambiguity measure over different classes and intra-classproperties to identify possible common sources of ambigu-ity. Determine if certain properties are over-represented athigher ambiguity thresholds. If annotators disagree over in-stances where the correct label seems obvious, this can pos-sibly be amended by updating the labelling instructions.4. Prune the dataset by removing highly ambiguous data upto a threshold determined through the previous steps. If thedataset is in danger of loosing representativeness, this canthen be addressed through adapted data collection protocolsor augmentation at training time.",
  ". Conclusion": "As we have seen, we will always encounter some degreeof ambiguity in annotated data. Additionally, the describedexperiments demonstrate that the prevalence of ambiguousdata has implications for a machine learning model duringboth, training and testing. Our experiments show that wecan improve the performance of a state-of-the-art detectionmodel by simply removing ambiguous data to a certain ex- tend. When doing so, we can identify two trade-offs, whichneed to be considered. Firstly, the very common trade-offin machine learning between recall and precision is also atplay when adding or removing ambiguous instance fromtraining data. Secondly, when removing too many ambigu-ous instances, the dataset is at risk of loosing representa-tiveness. Therefore, an understanding of ambiguity in thedataset is important to decide which instances to remove,which to keep, and which cases of hard-to-detect objectsmight be in need of additional treatment to prevent themfrom being underrepresented in the remaining training set.As we have shown, a simple ambiguity measure, which canbe estimated or calculated from the raw annotation answersof multiple workers, enables us to prune the dataset, result-ing in improved model performance at reduced costs.",
  ". Future Work": "Important topics for future work are the extension of thisframework to different object classes as well as modelarchitectures.We employed only one heuristic measurefor ambiguity based on annotator answer frequencies forour evaluation.In future work, different measures tocalculate and estimate ambiguity, including more elabo-rate techniques, should be investigated and compared w.r.t.how well they reflect ambiguity and are apt to provide athreshold for improving model performance by pruning thedataset.",
  "Lucas Beyer, Olivier J Henaff, Alexander Kolesnikov, Xi-aohua Zhai, and Aaron van den Oord. Are we done withImageNet? arXiv preprint arXiv:2006.07159, 2020. 1": "Markus Braun, Sebastian Krebs, Fabian B. Flohr, andDariu M. Gavrila. EuroCity Persons: A novel benchmarkfor person detection in traffic scenes. IEEE Transactions onPattern Analysis and Machine Intelligence, pages 11, 2019.3, 4 Morgan Buisson, Pablo Alonso-Jimenez, and Dmitry Bog-danov. Ambiguity modelling with label distribution learn-ing for music classification. In ICASSP 2022 - 2022 IEEEInternational Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 611615, 2022. 2",
  "Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delv-ing into high quality object detection, 2017. 4": "EuropeanComission.ITS&VulnerableRoadUsers.https : / / transport . ec . europa .eu / transport - themes / intelligent -transport-systems/road/action-plan-and-directive / implementation - its - action -plan / its - vulnerable - road - users _ en, n.d.Accessed: 2024-03-10. 1 Di Feng, Ali Harakeh, Steven L. Waslander, and Klaus Diet-mayer. A review and comparative study on probabilistic ob-ject detection in autonomous driving. IEEE Transactions onIntelligent Transportation Systems, 23(8):99619980, 2022.2",
  "Christopher Klugmann.A simple measure of ambiguityin crowdsourced binary answers. Unpublished manuscript,2023. 4": "Christopher Klugmann, Daniel Kondermann, et al. No needto sacrifice data quality for quantity: Crowd-informed ma-chine annotation for cost-effective understanding of visualdata. Unpublished manuscript, 2024. 4, 5, 6 Steven Liu, Tongzhou Wang, David Bau, Jun-Yan Zhu,and Antonio Torralba. Diverse image generation via self-conditioned gans.In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages1428614295, 2020. 1",
  "Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-moginov, and Liang-Chieh Chen. MobileNetV2: Invertedresiduals and linear bottlenecks, 2019. 4": "Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin,and Jae-Gil Lee. Learning from noisy labels with deep neuralnetworks: A survey. IEEE Transactions on Neural Networksand Learning Systems, pages 119, 2022. 1 Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan,Daniel C Alexander, and Nathan Silberman. Learning fromnoisy labels by regularized estimation of annotator confu-sion. In Proceedings of the IEEE/CVF conference on com-puter vision and pattern recognition, pages 1124411253,2019. 1 Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, MingkuiTan, Xinggang Wang, Wenyu Liu, and Bin Xiao.Deephigh-resolution representation learning for visual recogni-tion, 2020. 4"
}