{
  "Abstract": "Optical pooled screening (OPS) combines automatedmicroscopy and genetic perturbations to systematicallystudy gene function in a scalable and cost-effective way.Leveraging the resulting data requires extracting biologi-cally informative representations of cellular perturbationphenotypes from images. We employ a style-transfer ap-proach to learn gene-level feature representations from im-ages of genetically perturbed cells obtained via OPS. Ourmethod outperforms widely used engineered features inclustering gene representations according to gene function,demonstrating its utility for uncovering latent biological re-lationships. This approach offers a promising alternative toinvestigate the role of genes in health and disease.",
  ". Introduction": "Understanding the role of genes and their functional rela-tionship in homeostasis and disease is a fundamental chal-lenge in biomedical research. Typical approaches to studygene function include introducing perturbations designed todestroy or interfere with specific target genes. Advancesin experimentally performing gene perturbations are begin-ning to enable genome-scale interrogation of gene functionfor multiple cell types and disease states .Automated microscopy is a powerful tool to record andquantify the often subtle cellular responses and phenotypicchanges of gene perturbations, and can be combined withmarkers such as CellPaint for unbiased phenotypicprofiling . Conventionally, this has been done with ar-rayed screening, where all cells in a well receive the sameperturbation; however, for large screens with millions ofwells, this can become prohibitively expensive.Recently, Optical Pooled Screening (OPS) has emergedas a cost-effective alternative to study gene function [14,",
  "*Equal contribution": "15, 43, 46, 51]. Compared with arrayed screening, OPSsubjects cells to a pool of perturbations that are barcoded toallow determining the specific perturbation a cell received,greatly increasing the throughput and reducing the overallcost. While this enables high perturbation throughput, acaveat is that the resulting data is at single-cell level.A central challenge of image-based perturbation stud-ies is analyzing the enormous datasets that are generated,in order to extract high-dimensional representations of cellstate that capture the biological effects of each genetic per-turbation, while being invariant to numerous technical con-founders. Traditional image processing methods approachthis problem by extracting a large set of predefined engi-neered features, and then reducing dimensionality by re-moving noisy and highly correlated features. Deep learningis an increasingly popular alternative to engineered features,and promises to scale with the rapidly increasing corpus ofrecorded data ; however, it may be more susceptibleto overfitting technical confounders in the data .Recently, Generative Adversarial Networks (GANs)have shown promise in addressing the impact of techni-cal confounders on learned representations through the useof style transfer. In , the authors used style transfer tode-confound the training data, by generating images corre-sponding to the missing elements of the batch-perturbationexperimental design matrix, and then train a deep learningmodel on the expanded training set. In addition, the authorsof IMPA , further demonstrated the use of StarGAN v2for generating realistic images for unseen perturbations, forapplications such as virtual screening.Overall, we also find that style transfer is a natu-ral fit to the task of disentangling subtle perturbation effectsfrom more prominent but ultimately uninformative featurescaptured in images, such as the position of a cell withina frame.Taking inspiration from these prior works, weextend the IMPA model to extract representations of ge-netic perturbations (referred to interchangeably as stylecodes) from sets of cells by learning to transfer visual fea-",
  ". Overview of the": "GRAPE model: GRAPE builds upon StarGAN v2, and like other GAN-based models is composedof a generator and a discriminator, optimized with adversarial loss. In the GRAPE generation process, input image xi paired with itscorresponding genetic perturbation yi is passed through an encoder. A target gene (ytarget) is randomly selected, and its embedding istransmitted to the generators decoder along with the encoded input, aiming to generate images reflecting the content of the input image xiand perturbation responses of ytarget. The multihead discriminator takes the generated/real image xj with its corresponding generated/realperturbation yj, backpropagating the loss from real vs fake classification through the respective head of yj. Our primary objective in thiswork is to acquire effective representations for genetic perturbations from the trainable embedding layer. Finally, we assess the quality ofthe representations using various evaluation metrics such as mAP and clustering metrics. For a thorough investigation, we also evaluatedand comapred different potential gene representations at positions",
  ". (Icons have been designed using images from Flaticon.com.)": "tures between two images corresponding to different ge-netic perturbations, while keeping the main image contentintact. The extracted style codes can be used downstreamto infer relationships between perturbed genes. We referto our method as GRAPE: GANs as Robust AdversarialPerturbation Encoders. The model is depicted in .We evaluate our learned gene embeddings based on theirability to accurately predict known functional relationshipsin ground truth biological datasets, and demonstrate su-perior performance of GRAPE embeddings compared toGene2vec and IMPA in both clustering and re-call. Moreover, our approach outperforms widely used en-gineered features in clustering and demonstrates compara-ble performance in recall of biological ground truth. Addi-tionally, we curate and share a small biological ground truthset, to spur further innovation in this space.",
  ". Quantification of cellular phenotypes": "The phenotypic responses of cells to perturbations are oftensubtle and overlapping, making them difficult to distinguisheven for experts. Furthermore, datasets from perturbationscreens are generally prohibitively large for manual dataexamination. An unbiased and scalable analysis thereforerequires extracting biologically informative features fromimages of perturbed cells. A common approach is to em-ploy a curated bank of engineered features known to cap- ture biologically meaningful information. Such features in-clude size and shape of cells and cellular sub-compartments,as well as statistics and correlation of pixel intensity cor-responding to relative amounts and distribution of stainedbiomolecules. CellProfiler , a reference implementationof this approach, includes a large selection of diverse fea-tures useful for quantifying cellular phenotypes. In the con-text of OPS, CellProfiler has been used to analyze a recentwhole genome-wide OPS study . The authors demon-strate that the obtained features cluster cells according to thebiological function of perturbed genes, enabling recovery ofgene interaction networks at the level of protein complexesand larger scale pathways. Traditional image processing methods, while extract-ing low-level features, may fall short in capturing the en-tirety of biologically relevant information recorded by au-tomated microscopes . Deep learning-based com-puter vision is increasingly used as an alternative. A fewrecent OPS studies have compared the performance of en-gineered and learned features in different contexts. Sivanan-dan et al. compare engineered features mimickingCellProfiler with Vision Transformers (ViT) pretrainedon ImageNet or trained on the OPS screen using aself-supervised objective (DINO) . While all three ap-proaches recover known gene interaction networks, thelearned features show higher similarity of functionally re-lated phenotypes, with the self-supervised model outper-forming supervised pretraining. These results are consis- tent with another study comparing the performance of en-gineered and learned features to identify regulators of an-tiviral response . The engineered features were specifi-cally designed to measure a protein translocation bioassayreporting the cellular response to viral infection. Learnedfeatures were extracted with a pretrained convolutional neu-ral network and an autoencoder trained from scratch. No-tably, both deep learning models outperformed the engi-neered features, suggesting that they captured the bioassayreadout better than hand-crafted features or extracted addi-tional information not covered by prior knowledge.",
  ". Style Transfer": "The goal of style transfer is to mix two images, a contentimage and a style reference image, to produce a new imagewith the content of the content image and the style of thereference image. In , Gatys et al. introduced a ground-breaking approach to separate and manipulate content andstyle in images using a pre-trained VGG model . Theirmethod aligns the output images feature maps with the con-tent image to preserve the content, and matches Gram matri-ces (covariance matrices) of feature maps between the styleimage and the output image to adopt the style.In , the authors generalized the previous work by the-oretically proving that matching the Gram matrices of fea-ture maps is equivalent to minimizing the Maximum MeanDiscrepancy (MMD) with the second order polyno-mial kernel, and in doing so they re-imagined style trans-fer as matching feature distributions between style and gen-erated images.Furthermore, in , authors found thatBatch Normalization (BN) layer statistics could representstyle traits, and this led to Adaptive Instance Normaliza-tion (AdaIN) , which is now used in popular GAN-based models like the styleGAN and starGANmodel families . AdaIN takes feature statistics from astyle image and applies them to a content image, effectivelytransferring the style of the reference image to the contentimage. Therefore, the input to the AdaIN layers are meanand variance embeddings and can be derived from an imageor a separate encoder like a neural network. StarGAN v2,a widely adopted style transfer technique employing AdaINlayers, trains a single encoder-decoder architecture capableof transferring styles across various domains, such as demo-graphic categories.Recent advancements have demonstrated the applica-tion of style transfer to the biological domain. Works likeIST and IMPA provide two diverse examples. IST(Interventional Style Transfer) addresses out-of-distributiongeneralization by generating counterfactual treatment re-sponse predictions in control cells. It also paves the wayfor future endeavors in causal representation learning. No-tably, their advancements in the style transfer pipeline in-volved incorporating skip connections between the encoder",
  "GRAPE0.4970.5900.030.6100.020.3000.04": "and decoder to uphold content preservation and implement-ing multiple complementary losses to discourage pheno-typic alterations.IMPA (Image Perturbation Encoder) predicts cellularmorphological changes resulting from chemical and geneticperturbations given a perturbation code and an image ofan unperturbed cell. The authors focus on generative per-formance and extrapolation to unseen perturbations.Tothat end, IMPA uses frozen, pre-trained perturbation em-beddings (RDKit and Gene2vec for chemical andgenetic perturbations, respectively). Our work builds onIMPA which is based on StarGAN v2, with modificationsto enable learning perturbation embeddings from scratch.",
  ". Task Definition": "The overarching goal of our model is to learn representa-tions of optical pooled screening data that enable discov-ery of novel relationships between genetic perturbations.For the purpose of methods development, we evaluate theperformance of our method based on its ability to recoverknown biological relationships between genes, with the un-derlying assumption that unsupervised recovery of knownrelationships is a good proxy for discovery of novel rela-tionships.",
  ". Data Preprocessing": "Images were first corrected for uneven illumination usinga retrospective flatfield correction . Independently foreach channel, pixel intensities between 0.1 and 99.9 per-centiles were rescaled and clipped to range . Single-cell images were obtained by cropping 96 96 pixel imagepatches around the cell coordinates included with the re-leased phenotypic profiles. Pixel intensities were then nor-malized by channel-wise z-scoring using the mean and stan-dard deviation of the non-targeting control images from thesame experiment batch.",
  "Our model is a modified version of StarGAN v2 andIMPA , incorporating alterations to both the losses and": ".GRAPE generates realistic perturbation pheno-types. Top: Style transfer from an input image (non-targeting)to ANAPC7 (anaphase promoting complex subunit 7) andSMARCA4 (SWI/SNF related, matrix associated, actin dependentregulator of chromatin, subfamily a, member 4) gene knockouts.Bottom: Distribution of five CellProfiler features most informativefor classifying perturbed and control cells. the architecture. To train the GRAPE model, we utilize adataset consisting of pairs xi, yiNi=1, where each pair com-prises a single-cell image and its corresponding perturba-tion label. Our objective is to train a generator G capable ofproducing an image that reflects the phenotypic responsesassociated with a given target perturbation index z Z,while retaining the content of the original image x, suchas cell orientation. Importantly, our ultimate goal is to learnrepresentations for each perturbation, unconfounded by nui-sance features such as cell position, orientation, and techni-cal batch effects.Next, we discuss the modules within the GRAPE frame-work, walking through from left to right. Gene Embedding Layer:We use a trainable embeddingmatrix M where each of its rows represents a different ge-netic perturbation. This matrix begins with random normalinitialization and undergoes updates throughout the trainingprocess. With our dataset containing 107 genetic perturba-tions, the matrix has a shape of (107d). We chose a lengthof 500 for d, representing the dimensionality of the geneticrepresentations. During the generation process, for a targetperturbation z {1, 2, . . . , 107}, we extract the zth generepresentation denoted as Mz (the zth row of matrix M),and subsequently feed it into the mapping network. After training, we utilize this layer as our final gene embeddingfor downstream evaluations.Mapping Network: Given a latent code Mz, our mappingnetwork F generates a style code s = F(Mz). F consistsof an MLP with 3 layers in our model. We also exploredifferent architectures for F, such as an attention block, andevaluate the impact of these design choices using ablationstudies.Generator:The generator G comprises an encoder and adecoder, and translates an input image x into an output im-age x, capturing the phenotypic responses associated withperturbation z. The encoder computes the latent contentfrom an image, using 3 downsampling and 2 intermediateresidual blocks with instance normalization .The decoder consists of 2 intermediate and 3 upsamplingresidual blocks utilizing Adaptive Instance Normalization(AdaIN) . The perturbation style code is provided toGs decoder by the mapping network F through F(Mz), orby the style encoder E during cycle consistency check.Discriminator:The discriminator D operates as a multi-task discriminator , featuring multiple outputbranches. Each distinct branch, denoted as Dz, conductsbinary classification, determining whether an image x is areal image of perturbation z or a fake image G(x, s) pro-duced by G. The discriminator consists of three residualblocks, followed by two convolutional layers. The outputchannel of the final convolutional layer corresponds to thenumber of perturbations.Style Encoder:Given an image x and its correspondingperturbation label y, our encoder E extracts the style codes = E(x). This block will help with the cycle loss dis-cussed below. The style encoder is structured with threeresidual blocks, followed by a convolutional layer and afully connected layer. We adapt the architecture of Star-GAN v2 such that all parameters are shared between per-turbations and there is only a single head.",
  ". Training Objectives": "Adversarial Objective: During training, we randomly se-lect a target perturbation index z Z, sample its embed-ding vector from matrix M, and generate a target style codes = F(Mz). The generator G takes an image x (with origi-nal label y) and s as inputs and learns to generate an outputimage G(x, s) via an adversarial loss:",
  "Ladv = Ex,y[log Dy(x)] + Ex,z[1 log Dz(G(x, s))] (1)": "Where Dy() represents the output of the yth branch of dis-criminator D, and similarly, Dz() denotes another branchoutput. The trainable matrix M and the mapping network Flearn to provide the style code s that has target perturbationzs response information, and G learns to apply s to gen-erate an image G(x, s) that is indistinguishable from realimages of the perturbation z.",
  ". UMAP visualization of GRAPE representations (left), and engineered representations (right). Each dot represents a single gene,and the color indicates its ground truth CORUM cluster label": "Cycle Consistency Objective:To ensure that the gener-ated image G(x, s) retains the perturbation-invariant traits(such as cell orientation) from its input image x, we im-plement the cycle consistency loss . This im-portant loss enables us to isolate and emphasize solely theperturbation-specific information contained within the stylecodes and ultimately within the M matrix.",
  "Lcyc = Ex,y,z[||x G(G(x, s), s)||1](2)": "where s = E(x) is the estimated style code of the inputimage x with original label y passed through the encoderE.Style Reconstruction:In our adaptation of the modelfrom StarGAN v2, we limit the application of style loss ex-clusively to the cycle loss. Therefore, we restrict updatessolely to the style encoder for this purpose. Our goal here isto ensure the alignment between the output of the style en-coder for the generated image denoted as E(G(x, s)), andthe style code s.",
  "Lsty = Ex,z[||s.detach() E(G(x, s).detach())||1](3)": "Diversity Loss:In both StarGAN v2 and IMPA , adiversity loss was included to encourage the generator toproduce diverse output images while maintaining the samecontent image and style code. However, in our work, weintentionally excluded both noise concatenation and the di-versity loss. This decision stems from our primary focus,which centers on learning style representations, specificallyrelated to genetic perturbations, rather than emphasizing theoverall generative performance.Full objective:Our full objective can be written as fol-lows:",
  ". Implementation": "Input images have dimensions of 96 96 pixels and consistof 4 channels. During training, the batch size is set to 256,and we train the model for 100,000 iterations. To ensurebalanced representation, the data loader employs a weightedsampler, sampling an equal number of cells per perturba-tion in each iteration . On a single A100 GPU, our im-plementation in PyTorch requires approximately threedays to train. During model training, we employ the non-saturating adversarial loss integrated with R1 regular-ization with = 1. We use an Adam optimizer with 1 = 0 and 2 = 0.99. For all modules including G,D, M, F, and E, the learning rates and weight decays areset to 0.0001. We initialize the weights of all modules usingHe initialization . Both sty and cyc hyperparametersare set to 1.",
  ". Performance Baselines": "We conducted a comparative analysis between embeddingsderived from our GRAPE models trainable gene embed-ding layer and Gene2vec .Additionally, we trainedIMPA using our dataset (see Sec. 4.1) and followingthe original paper. This involved obtaining genetic pertur-bation embeddings by aggregating style embeddings fromthe style encoder for 500 cells per perturbation.We also utilized engineered features from , and cal-culated perturbation embeddings by aggregating single-cellprofiles (after applying PCA) for the same set of cells usedto train GRAPE.",
  ". Evaluation Metrics": "We evaluate whether gene embeddings learned by GRAPEcluster according to known functional gene relationships.Using CORUM protein complexes as ground truth, we ex-pect that perturbing genes functioning in the same protein . Performance comparison between GRAPE representations and baselines for multiple clustering metrics (left: Normalized MutualInformation, middle: Adjusted Rand Index, right: Purity) and for different numbers of clusters k. The ground truth number of clustersis 14, denoted by a vertical line. The k-means algorithm was executed 100 times for each k, and the standard deviation is depicted as ashaded envelope. complex should yield similar phenotypes, and thus clustertogether in latent space. We selected two methods for eval-uating the quality of our gene embeddings. The first methodis mean average precision (mAP) across nearest neighborsfor all genes . Specifically, for a given query gene Q,we rank other genes by cosine distance, labeling them as1 if they share the same CORUM cluster as gene Q, and0 otherwise. We compute the average precision and repeatthis process for all genes to derive the mAP.The second set of metrics are based on clustering anddraw inspiration from . These metrics include theAdjusted Rand Index (ARI) , Normalized Mutual In-formation (NMI) , and Purity metric, each focusing ondifferent aspects of clustering. Purity evaluates the homo-geneity of clusters, NMI quantifies the mutual dependencebetween true class labels and cluster assignments, and ARIassesses the similarity between true class labels and clusterassignments while accounting for chance. We apply thesemetrics to evaluate the concordance between predicted clus-ter labels derived from k-means clustering and the groundtruth cluster labels from CORUM. Notably, we omittedthe negative control embeddings from evaluation, focus-ing solely on perturbations with their corresponding CO-RUM cluster labels. To ensure the robustness of our eval-uation, we performed k-means clustering 100 times withrandom initialization for all embeddings, including GRAPEand baseline methods. We report average and standard de-viation for each metric.",
  ". Dataset": "We leverage a recently published OPS study perform-ing CRISPR knockout of 5000 essential genes in HeLacells .The released data contains 4-channel imagesof cells stained for biomolecular markers of DNA (DAPI), DNA-damage (H2AX), microtubules (-Tubulin) and F-actin (Phalloidin) with perturbation barcodes indicating theperturbation each cell received, which were obtained by insitu sequencing. The authors also released a processed phe-notypic profile for each cell, generated using engineeredfeatures derived from CellProfiler and other popular im-age processing libraries . In their study, the authorspresent a detailed analysis demonstrating that these pheno-typic profiles cluster according to the biological function ofthe perturbed genes. We consider the released data as rep-resentative of what is achievable with engineered features(except specifically using task-specific prior knowledge ifavailable), and thus an important baseline for our method. For ground truth biological relationships, we use theComprehensive Resource of Mammalian Protein Com-plexes (CORUM) . It is a database of protein complexesand the genes of their constituent proteins, covering diversebiological processes such as cell adhesion and genetic in-formation processing. It is widely used to evaluate the qual-ity of phenotypic profiles, because proteins functioning to-gether in the same protein complex are more likely to beco-dependent, which provides a high confidence that theirperturbation should yield similar phenotypes. Most otherontologies of biology include indirect and non-constitutiveinteractions as well as antagonists and negative regulators,which complicates their use for this specific task. In our evaluations, we use a curated subset of CORUMthat excludes protein complexes with poor intersection withthe released perturbations and minimizes inter-cluster over-lap . From this set, we select 14 non-overlaping andfunctionally diverse protein complexes, each containing atleast five genes. To avoid size-imbalance, we subset largerprotein complexes to at most ten genes.The resultingdataset consists of 508,159 single-cell images of the follow-ing 106 perturbations, to which we add the negative con-trol perturbations without any known target in the human . Ablation results indicate that the Gene Embedding Layer (Position 1) provides the best gene representations compared to alter-native layers sampled from GRAPE (i.e., Position 2: after the mapping network; Position 3: after the style encoder). Adding an attentionlayer does not improve model performance; however, improvements are observed when adding the cycle loss, detaching the style code inthe style loss, and utilizing an exponential moving average.",
  ". Evaluation": "Qualitatively, GRAPE generates realistic predictions of per-turbed cells while preserving the overall context of the inputcell (). However, cellular phenotypes of different per-turbations are often subtle and difficult to distinguish visu-ally. To quantitatively evaluate generative performance, westyle-transfered 500 non-targeting control cells to each ofthe perturbation styles and compared CellProfiler featuresof non-targeting, perturbed and predicted images. Follow-ing , we obtained the most informative CellProfiler fea-tures by training a Random Forest classifier to discrim-inate non-targeting from real perturbed cells and comput- ing permutation feature importance. The distributions of thefive most informative features show that style-transfer gen-erates cells that are more similar to the target perturbationthan unperturbed input cells ().As detailed in section 3.7, we compared learned em-beddings obtained from the Gene Embedding Layer ofGRAPE, with baseline methods using three clustering eval-uation metrics and a recall-based metric. To evaluate clus-tering, we employ the k-means clustering algorithm withthe parameter k set to 14, the number of ground truth clus-ters. Using the known ground truth CORUM cluster la-bels, we then evaluate clustering performance using pu-rity, Adjusted Rand Index (ARI) and Normalized MutualInformation (NMI). GRAPE embeddings outperformed thebaselines (random features, engineered features, IMPA andGene2vec representations) across all three clustering met-rics (see Tab. 1). To ensure a comprehensive analysis, weassessed our embeddings and baseline methods across vari-ous values of k in k-means clustering, although our datasetinherently consists of 14 clusters with 106 genes. The re-sults depicted in indicate that GRAPE embeddingsoutperform in all three metrics across all values of k.Furthermore, we computed the mean Average Precision(mAP) for 106 genes across both GRAPE representationsand other baseline methods.Tab. 1 shows that GRAPEembeddings outperform most baselines and are competitivewith engineered features, the strongest baseline for this task.For an additional visualization, refer to , whichshowcases UMAP embeddings of GRAPE gene represen-tations. The colors on the plot represent the ground-truthCORUM cluster labels, demonstrating that gene perturba-tions within the same CORUM structure cluster together inthe embedding space.",
  ". Alternative Gene Embeddings": "There are multiple choices for where to extract gene embed-dings from the StarGANv2 model to serve as our final generepresentations for downstream tasks. We evaluated threeoptions: (1) the gene embedding layer, (2) the style codethat is extracted by the mapping network and input to theAdaIN layer, and (3) the style code that is extracted fromthe generated images by the Style Encoder. See forthe location of layers: .Interestingly, each option is qualitatively different in howthe final gene representations are extracted. For option 1,we simply extract the weights of the trained gene embed-ding layer, whereas for option 2, we pass the learned geneembeddings through the mapping network to generate astyle code per gene. In other words, options 1 and 2 donot require any inference-time processing. In contrast, foroption 3 we must generate cell-level style codes by pass-ing random samples through the GRAPE model, and thenaggregating the cell-level style codes returned by the StyleEncoder up to the perturbation level. To accomplish this,we randomly select 500 cells per perturbation, and averagetheir cell-level style codes to create perturbation-level geneembeddings. Notably, option 3 is most similar to the work-flow employed by IMPA. We compare the performance ofeach embedding choice after 100,000 training iterations us-ing the mean Average Precision. Our results show that the",
  ". Exponential Moving Average": "An interesting feature of our approach is that gene embed-dings (and thus, genetic relationships) are extracted fromthe data during training, and there is no final inferencestep. However, when reviewing the training curves, we ob-served strong fluctuations in overall performance towardsthe end of training.These fluctuations introduce an un-wanted sensitivity between the model performance and thechoice of stopping criteria. Similar weight fluctuations werealso observed by the authors of StarGAN v2, and they pro-posed suppressing them with an exponential moving aver-age (EMA) filter. Thus we apply an EMA filter every 2.5kiterations with a gamma value of 0.5 and find that it both re-duces sensitivity to the stopping criteria, and yields a slightimprovement in mAP (see ).",
  ". Losses": "We conducted experiments where we ablated different lossfunctions to better understand their contribution to the train-ing process. When omitting the cycle loss during training,we observed a decrease in performance across all evalua-tion metrics, including mAP. In another experiment, we ex-tended the influence of the style loss beyond the style en-coder by allowing it to affect the Generator and EmbeddingLayer. In other words, we did not detach the style vector sin the style reconstruction objective (see Equation 3). Wealso noted a drop in performance in this scenario.",
  ". Discussion": "In conclusion, our study demonstrated that style transferthrough generative modeling can be used to learn high qual-ity representations of genetic perturbations from opticalpooled screening data. We observed a significant perfor-mance improvement for all clustering metrics and competi-tive performance for mean Average Precision, compared toengineered features.It is important to acknowledge a limitation in our study,as we conducted training on a dataset comprising only 107genes and approximately 500,000 images. We recognizethe potential for enhancing the models capabilities by ex-tending the training to incorporate a broader range of per-turbations. Future endeavors will focus on expanding thisresearch to encompass a more diverse array of genetic per-turbations, aiming to further validate and generalize the ap-plicability of our proposed approach.",
  "Juan C Caicedo, Shantanu Singh, and Anne E Carpenter. Ap-plications in image-based profiling of perturbations. Currentopinion in biotechnology, 39:134142, 2016. 1": "Juan C Caicedo, Sam Cooper, Florian Heigwer, ScottWarchal, Peng Qiu, Csaba Molnar, Aliaksei S Vasilevich,Joseph D Barry, Harmanjit Singh Bansal, Oren Kraus, et al.Data-analysis strategies for image-based cell profiling. Na-ture methods, 14(9):849863, 2017. Juan C Caicedo, Claire McQuin, Allen Goodman, ShantanuSingh, and Anne E Carpenter. Weakly supervised learningof single-cell feature embeddings.In Proceedings of theIEEE Conference on Computer Vision and Pattern Recog-nition, pages 93099318, 2018. 1, 2 Rebecca J Carlson, Michael D Leiken, Alina Guna, Nir Ha-cohen, and Paul C Blainey. A genome-wide optical pooledscreen reveals regulators of cellular antiviral responses. Pro-ceedings of the National Academy of Sciences, 120(16):e2210623120, 2023. 3 Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou,Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-ing properties in self-supervised vision transformers. In Pro-ceedings of the IEEE/CVF international conference on com-puter vision, pages 96509660, 2021. 2 Anne E Carpenter, Thouis R Jones, Michael R Lamprecht,Colin Clarke, In Han Kang, Ola Friman, David A Guertin,Joo Han Chang, Robert A Lindquist, Jason Moffat, et al.Cellprofiler: image analysis software for identifying andquantifying cell phenotypes. Genome biology, 7:111, 2006.2, 6 Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha,Sunghun Kim, and Jaegul Choo. Stargan: Unified genera-tive adversarial networks for multi-domain image-to-imagetranslation.In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 87898797,2018. 3, 5 Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.Stargan v2: Diverse image synthesis for multiple domains.In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 81888197, 2020. 1, 3 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE conference on computer vision andpattern recognition, pages 248255. Ieee, 2009. 2 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale.arXiv preprintarXiv:2010.11929, 2020. 2",
  "Jingcheng Du, Peilin Jia, Yulin Dai, Cui Tao, ZhongmingZhao, and Degui Zhi. Gene2vec: distributed representationof genes based on co-expression. BMC genomics, 20:715,2019. 2, 3, 5": "Marta M. Fay,Oren Kraus,Mason Victors,Laksh-manan Arumugam, Kamal Vuggumudi, John Urbanik, KyleHansen, Safiye Celik, Nico Cernek, Ganesh Jagannathan,Jordan Christensen, Berton A. Earnshaw, Imran S. Haque,and Ben Mabey. Rxrx3: Phenomics map of biology. bioRxiv,2023. 1 David Feldman, Avtar Singh, Jonathan L. Schmid-Burgk,Rebecca J. Carlson, Anja Mezger, Anthony J. Garrity, FengZhang, and Paul C. Blainey. Optical pooled screens in humancells. Cell, 179(3):787799.e17, 2019. 1 Luke Funk, Kuan-Chung Su, Jimmy Ly, David Feldman, Av-tar Singh, Brittania Moodie, Paul C Blainey, and Iain MCheeseman. The phenotypic landscape of essential humangenes. Cell, 185(24):46344653, 2022. 1, 5, 6",
  "Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bern-hard Scholkopf, and Alexander Smola. A kernel two-sampletest. The Journal of Machine Learning Research, 13(1):723773, 2012. 3": "Sigrun M Gustafsdottir, Vebjorn Ljosa, Katherine L Sokol-nicki, J Anthony Wilson, Deepika Walpita, Melissa M Kemp,Kathleen Petri Seiler, Hyman A Carrel, Todd R Golub, Stu-art L Schreiber, et al. Multiplex cytological profiling assayto measure diverse cellular states. PloS one, 8(12):e80999,2013. 1 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Delving deep into rectifiers: Surpassing human-level perfor-mance on imagenet classification.In Proceedings of theIEEE international conference on computer vision, pages10261034, 2015. 5 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 4 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Identity mappings in deep residual networks. In ComputerVisionECCV 2016: 14th European Conference, Amster-dam, The Netherlands, October 1114, 2016, Proceedings,Part IV 14, pages 630645. Springer, 2016. 4",
  "Kasia Zofia Kedzierska, Lorin Crawford, Ava Pardis Amini,and Alex X Lu. Assessing the limits of zero-shot foundationmodels in single-cell biology. bioRxiv, pages 202310, 2023.6": "Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee,and Jiwon Kim.Learning to discover cross-domain rela-tions with generative adversarial networks. In Internationalconference on machine learning, pages 18571865. PMLR,2017. 5 Vladislav Kim, Nikolaos Adaloglou, Marc Osterland, FlavioMorelli, and Paula Andrea Marin Zapata. Self-supervisionadvances morphological profiling by unlocking powerful im-age representations. bioRxiv, pages 202304, 2023. 1, 6",
  "Yanghao Li,Naiyan Wang,Jiaying Liu,and XiaodiHou.Demystifying neural style transfer.arXiv preprintarXiv:1701.01036, 2017. 3": "Joakim Lindblad and Ewert Bengtsson.A comparison ofmethods for estimation of intensity non uniformities in 2dand 3d microscope images of fluorescence stained cells.In Proceedings of the Scandinavian Conference On ImageAnalysis, pages 264271, 2001. 3 Ming-Yu Liu, Xun Huang, Arun Mallya, Tero Karras, TimoAila, Jaakko Lehtinen, and Jan Kautz.Few-shot unsu-pervised image-to-image translation.In Proceedings ofthe IEEE/CVF international conference on computer vision,pages 1055110560, 2019. 4 Malte D Luecken, Maren Buttner, Kridsadakorn Chai-choompu, Anna Danese, Marta Interlandi, Michaela FMuller, Daniel C Strobl, Luke Zappia, Martin Dugas, MariaColome-Tatche, et al. Benchmarking atlas-level data integra-tion in single-cell genomics. Nature methods, 19(1):4150,2022. 6",
  "Adam Paszke, Sam Gross, Soumith Chintala, GregoryChanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-ban Desmaison, Luca Antiga, and Adam Lerer. Automaticdifferentiation in pytorch. 2017. 5": "Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort,Vincent Michel, Bertrand Thirion, Olivier Grisel, MathieuBlondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg,et al. Scikit-learn: Machine learning in python. the Journalof machine Learning research, 12:28252830, 2011. 6 Wolfgang M Pernice, Michael Doron, Alex Quach, AdityaPratapa, Sultan Kenjeyev, Nicholas De Veaux, Michio Hi-rano, and Juan C Caicedo. Out of distribution generalizationvia interventional style transfer in single-cell microscopy. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 43254334, 2023. 3 Meraj Ramezani, Julia Bauman, Avtar Singh, Erin Weis-bart, John Yong, Maria Lozada, Gregory P. Way, Sanam L.Kavari, Celeste Diaz, Marzieh Haghighi, Thiago M. Batista,Joaqun Perez-Schindler, Melina Claussnitzer, ShantanuSingh, Beth A. Cimini, Paul C. Blainey, Anne E. Carpen-ter, Calvin H. Jan, and James T. Neal. A genome-wide atlasof human cell morphology. bioRxiv, 2023. 1, 2",
  "Karen Simonyan and Andrew Zisserman. Very deep convo-lutional networks for large-scale image recognition. arXivpreprint arXiv:1409.1556, 2014. 3": "Srinivasan Sivanandan, Bobby Leitmann, Eric Lubeck, Mo-hammad Muneeb Sultan, Panagiotis Stanitsas, NavpreetRanu, Alexis Ewer, Jordan E Mancuso, Zachary F Phillips,Albert Kim, et al. A pooled cell painting crispr screeningplatform enables de novo inference of gene function by self-supervised deep learning. bioRxiv, pages 202308, 2023. 1,2 George Tsitsiridis, Ralph Steinkamp, Madalina Giurgiu,Barbara Brauner, Gisela Fobo, Goar Frishman, CorinnaMontrone, and Andreas Ruepp. CORUM: the comprehen-sive resource of mammalian protein complexes2022. Nu-cleic Acids Research, 51(D1):D539D545, 2022. 6"
}