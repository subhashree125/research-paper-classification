{
  "Abstract": "Diffusion Models (DMs) have exhibited superior perfor-mance in generating high-quality and diverse images. How-ever, this exceptional performance comes at the cost of ex-pensive architectural design, particularly due to the atten-tion module heavily used in leading models. Existing worksmainly adopt a retraining process to enhance DM efficiency.This is computationally expensive and not very scalable. Tothis end, we introduce the Attention-driven Training-freeEfficient Diffusion Model (AT-EDM) framework that lever-ages attention maps to perform run-time pruning of redun-dant tokens, without the need for any retraining. Specifi-cally, for single-denoising-step pruning, we develop a novelranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify redundant tokens, and a similarity-basedrecovery method to restore tokens for the convolution oper-ation. In addition, we propose a Denoising-Steps-AwarePruning (DSAP) approach to adjust the pruning budgetacross different denoising timesteps for better generationquality.Extensive evaluations show that AT-EDM per-forms favorably against prior art in terms of efficiency(e.g., 38.8% FLOPs saving and up to 1.53 speed-up overStable Diffusion XL) while maintaining nearly the sameFID and CLIP scores as the full model. Project webpage:",
  ". Introduction": "Diffusion Models (DMs) have revolutionized com-puter vision research by achieving state-of-the-art perfor-mance in various text-guided content generation tasks, in-cluding image generation , image editing , superresolution , 3D objects generation , and video gen-eration . Nonetheless, the superior performance of DMscomes at the cost of an enormous computation budget. Al-though Latent Diffusion Models (LDMs) maketext-to-image generation much more practical and afford-able for normal users, their inference process is still tooslow. For example, on the current flagship mobile phone,",
  "generating a single 512px image requires 90 seconds": "To address this issue, numerous approaches geared at ef-ficient DMs have been introduced, which can be roughlycategorized into two regimes: (1) efficient sampling strat-egy and (2) efficient model architecture .While efficient sampling methods can reduce the number ofdenoising steps, they do not reduce the memory footprintand compute cost for each step, making it still challeng-ing to use on devices with limited computational resources.On the contrary, an efficient architecture reduces the costof each step and can be further combined with samplingstrategies to achieve even better efficiency. However, mostprior efficient architecture works require retraining of theDM backbone, which can take thousands of A100 GPUhours. Moreover, due to different deployment settings onvarious platforms, different compression ratios of the back-bone model are required, which necessitate multiple retrain-ing runs later. Such retraining costs are a big concern evenfor large companies in the industry. To this end, we propose the Attention-driven Training-free Efficient Diffusion Model (AT-EDM) framework,which accelerates DM inference at run-time without anyretraining. To the best of our knowledge, training-free ar-chitectural compression of DMs is a highly uncharted area.Only one prior work, Token Merging (ToMe) , addressesthis problem. While ToMe demonstrates good performanceon Vision Transformer (ViT) acceleration , its perfor-mance on DMs still has room to improve. To further en-rich research on training-free DM acceleration, we start ourstudy by profiling the floating-point operations (FLOPs)of the state-of-the-art model, Stable Diffusion XL (SD-XL) , through which we find that attention blocks arethe dominant workload. In a single denoising step, we thuspropose to dynamically prune redundant tokens to accel-erate attention blocks. We pioneer a fast graph-based al-gorithm, Generalized Weighted Page Rank (G-WPR), in-spired by Zero-TPrune , and deploy it on attention mapsin DMs to identify superfluous tokens. Since SD-XL con-tains ResNet blocks, which require a full number of to-kens for the convolution operations, we propose a novelsimilarity-based token copy approach to recover pruned to-kens, again leveraging the rich information provided by the",
  "@ 4.1 TFLOPs": ". Examples of applying AT-EDM to SD-XL . Compared to the full-size model (top row), our accelerated model (bottom row)has around 40% FLOPs reduction while enjoying competitive generation quality at various aspect ratios. attention maps. This token recovery method is critical tomaintaining image quality. We find that naive interpolationor padding of pruned tokens adversely impacts generationquality severely. In addition to single-step token pruning,we also investigate cross-step redundancy in the denoisingprocess by analyzing the variance of attention maps. Thisleads us to a novel pruning schedule, dubbed as Denoising-Steps-Aware Pruning (DSAP), in which we adjust the prun-ing ratios across different denoising timesteps.We findDSAP not only significantly improves our method, but alsohelps improve other run-time pruning methods like ToMe. Compared to ToMe, our approach shows a clear im-provement by generating clearer objects with sharper detailsand better text-image alignment under the same accelerationratio. In summary, our contributions are four-fold:",
  "We propose the AT-EDM framework, which leveragesrich information from attention maps to accelerate pre-trained DMs without retraining": "We design a token pruning algorithm for a single de-noising step.We pioneer a fast graph-based algo-rithm, G-WPR, to identify redundant tokens, and a novelsimilarity-based copy method to recover missing tokensfor convolution. Inspired by the variance trend of attention maps across de-noising steps, we develop the DSAP schedule, which im-proves generation quality by a clear margin. The schedulealso provides improvements over other run-time acceler-ation approaches, demonstrating its wide applicability. We use AT-EDM to accelerate a top-tier DM, SD-XL, andconduct both qualitative and quantitative evaluations. No-ticeably, our method shows comparable performance withan FID score of 28.0 with 40% FLOPs reduction relativeto the full-size SD-XL (FID 27.3), achieving state-of-the-art results. Visual examples are shown in .",
  ". Related Work": "Text-to-Image Diffusion Models.DMs learn to reversethe diffusion process by denoising samples from a normaldistribution step by step. In this manner, the diffusion-basedgenerative models enable high-fidelity image synthesis withvariant text prompts . However, DMs in the pixel spacesuffer from large generation latency, which severely lim-its their applications . The LDM was the first totrain a Variational Auto-Encoder (VAE) to encode the pixelspace into a latent space and apply the DM to the latentspace. This reduces computational cost significantly whilemaintaining generation quality, thus greatly enhancing theapplication of DMs. Subsequently, several improved ver-sions of the LDM, called Stable Diffusion Models (SDMs),have been released. The most recent and powerful open-source version is SD-XL , which outperforms previousversions by a large margin. SD-XL is our default backbonein this work.Efficient Diffusion Models. Researchers have made enor-mous efforts to make DMs more efficient. Existing efficientDMs can be divided into two types:(1) Efficient sampling to reduce the required number ofdenoising steps . A recent efficient samplingwork managed to reduce the number of denoising stepsto as low as one. It achieves this by iterative distillation,halving the number of denoising steps each time.(2) Architectural compression to make each sampling stepmore efficient . A recent work removesmultiple ResNet and attention blocks in the U-Net throughdistillation.Although these methods can save computa-tional costs while maintaining decent image quality, they re-quire retraining of the DM backbone to enhance efficiency,needing thousands of A100 GPU hours. Thus, a training-free method to enhance the efficiency of DMs is needed.Note that our proposed training-free framework, AT-EDM, is orthogonal to these efficiency enhancement methods andcan be stacked with them to further improve their efficiency.We provide corresponding experimental evidence in Sup-plementary Material.Training-Free Efficiency Enhancement.Training-free(i.e., post-training) efficiency enhancement schemes havebeen widely explored for CNNs and ViTs. However, training-free schemes for DMsare still poorly explored. To the best of our knowledge, theonly prior work in this field is ToMe . It uses token em-bedding vectors to obtain pair-wise similarity and mergessimilar tokens to reduce computational overheads. WhileToMe achieves a decent speed-up when applied to SD-v1.xand SD-v2.x, we find that it does not help much when ap-plied to the state-of-the-art DM backbone, SD-XL, whilstour method achieves a clear improvement over it (see exper-imental results in ). This is mainly due to (1) thesignificant architectural change of SD-XL (see Supplemen-tary Material); (2) our better algorithm design to identifyredundant tokens.Exploiting Attention Maps. We aim to design a methodthat exploits information present in pre-trained models.ToMe only uses embedding vectors of tokens and ignoresthe correlation between tokens. We take inspiration fromrecent image editing works , in which attentionmaps clearly demonstrate which parts of a generated imageare more important. This inspires us to use the correlationsand couplings between tokens indicated by attention mapsto identify unimportant tokens and prune them. Specifically,we can convert attention maps to directed graphs, wherenodes represent tokens, without information loss. Based onthis idea, we develop the G-WPR algorithm for token prun-ing in a single denoising step.Non-Uniform Denoising Steps.Various existing works demonstrate that denoising steps contributedifferently to the quality of generated images; thus, it is notoptimum to use uniform denoising steps. OMS-DPM builds a model zoo and uses different models in different de-noising steps. It trains a performance predictor to assist insearching for the optimal model schedule. DDSM em-ploys a spectrum of neural networks and adapts their sizesto the importance of each denoising step. AutoDiffusion employs evolutionary search to skip some denoisingsteps and some blocks in the U-Net. Diff-Pruning usesa Taylor expansion over pruned timesteps to disregard non-contributory diffusion steps. All existing methods either re-quire an intensive training/fine-tuning/searching process toobtain and deploy the desired denoising schedule or are notcompatible with our proposed G-WPR token pruning algo-rithm due to the U-Net architecture change. On the con-trary, based on our investigation of the variance of attentionmaps across denoising steps, we propose DSAP. Its sched-ule can be determined via simple ablation experiments and",
  ". Methodology": "We start our investigation by profiling the FLOPs of thestate-of-the-art DM, SD-XL, as shown in . Notice-ably, among compositions of the sampling module (U-Net),attention blocks, which consist of several consecutive at-tention layers, dominate the workload for image genera-tion. Therefore, we propose AT-EDM to accelerate atten-tion blocks in the model through token pruning. AT-EDMcontains two important parts: a single-denoising-step tokenpruning scheme and the DSAP schedule. We provide anoverview of these two parts and then discuss them in detail.",
  "illustrates the two main parts of AT-EDM:": "Part I: Token pruning scheme in a single denoising step.Step 1: We obtain the attention maps from an attentionlayer in the U-Net. We can potentially obtain the attentionmaps from self-attention or cross-attention. We comparethe two choices and analyze them in detail through ablationexperiments.Step 2: We use a scoring module to assign an importancescore to each token based on the obtained attention map.We use an algorithm called G-WPR to assign importancescores to each token. This is described in .2.Step 3: We generate pruning masks based on the calculatedimportance score distribution. Currently, we simply use thetop-k approach to determine the retained tokens, i.e., prunetokens with less importance scores.Step 4: We use the generated mask to perform token prun-ing. We do this after the feed-forward layer of attentionlayers. We may also perform pruning early before the feed-forward layers. We provide ablative experimental resultsfor it in Supplementary Material.Step 5: We repeat Steps 1-4 for each consecutive attentionlayer. Note that we do not apply pruning to the last attentionlayer before the ResNet layer.",
  "G-WPR": ". Overview of our proposed efficiency enhancement framework AT-EDM. Single-Denoising-Step Token Pruning: (1) We getthe attention map from self-attention. (2) We calculate the importance score for each token using G-WPR. (3) We generate pruning masks.(4) We apply the masks to tokens after the feed-forward network to realize token pruning. (5) We repeat Steps (1)-(4) for each consecutiveattention layer. (6) Before passing feature maps to the ResNet block, we recover pruned tokens through similarity-based copy. Denoising-Steps-Aware Pruning Schedule: In early steps, we propose to prune fewer tokens and to have less FLOPs reduction. In later steps, weprune more aggressively for higher speedup. Step 6: Finally, before passing the pruned feature map tothe ResNet block, we need to fill (i.e., try to recover) thepruned tokens. A simple approach is to pad zeros, whichmeans we do not fill anything. The method that we currentlyuse is to copy tokens to corresponding locations based onsimilarity. This is described in detail in .2. Part II: DSAP schedule.Attention maps in early de-noising steps are more chaotic and less informative thanthose in later steps, which is indicated by their low vari-ance. Thus, they have a weaker ability to differentiate unim-portant tokens . Based on this intuition, we design theDSAP schedule that prunes fewer tokens in early denoisingsteps. Specifically, we select some attention blocks in theup-sampling and down-sampling stages and leave them un-pruned, since they contribute more to the generated imagequality than other attention blocks . We demonstrate theschedule in detail in .3.",
  ". Part I: Token Pruning in a Single Step": "Notation. Suppose A(h,l) RMN is the attention mapof the h-th head in the l-th layer. It reflects the correla-tions between M Query tokens and N Key tokens. We re-fer to A(h,l) as A for simplicity in the following discus-sion. Let Ai,j denote its element in the i-th row, j-th col- umn. A can be thought of as the adjacency matrix of adirected graph in the G-WPR algorithm. In this graph, theset of nodes with input (output) edges is referred to as in(out). Nodes in in (out) represent Key (Query) tokens,i.e., in = {kj}Nj=1 (out = {qi}Mi=1). Let stK (stQ) de-note the vector that represents the importance score of Key(Query) tokens in the t-th iteration of the G-WPR algorithm.In the case of self-attention, Query tokens are the same asKey tokens. Specifically, we let {xi}Ni=1 denote the N to-kens and s denote their importance scores in the descriptionof our token recovery method. The G-WPR Algorithm. WPR uses the attention mapas an adjacency matrix of a directed complete graph. It usesa graph signal to represent the importance score distribu-tion among nodes in this graph. This signal is initializeduniformly. WPR uses the adjacency matrix as a graph op-erator, applying it to the graph signal iteratively until con-vergence. In each iteration, each node votes for which nodeis more important. The weight of the vote is determinedby its importance in the last iteration. However, WPR, asproposed in , constrains the used attention map to be aself-attention map. Based on this, we propose the G-WPRalgorithm, which is compatible with both self-attention andcross-attention, as shown in Algorithm 1.The attention from Query qi to Key kj weights the edge from qi to kjin the graph generated by A. In each iteration of the vanillaWPR, by multiplying with the attention map, we map theimportance of Query tokens stQ to the importance of Key to-kens st+1K , i.e., each node in out votes for which in nodeis more important. For self-attention, st+1Q= st+1KsinceQuery and Key tokens are the same. For cross-attention,Query tokens are image tokens and Key tokens are textprompt tokens. Based on the intuition that important im-age tokens should devote a large portion of their attention toimportant text prompt tokens, we define function f(A, sK)that maps st+1Kto st+1Q . One entropy-based implementationis",
  "Nj=1 Ai,j ln Ai,j(1)": "where Ai,j is the attention from Query qi to Key kj. Thisis the default setting for cross-attention-based WPR in thefollowing sections. We discuss and compare other imple-mentations in Supplementary Material. Note that for self-attention, f(A, st+1K ) = st+1K . The G-WPR algorithm hasan O(M N) complexity, where M (N) is the numberof Query (Key) tokens. We employ this algorithm in eachhead and then obtain the root mean square of scores fromdifferent heads (to reward tokens that obtain very high im-portance scores in a few heads).",
  "end whiles stQ": "Recovering Pruned Tokens. We have fewer tokens aftertoken pruning, leading to efficiency enhancement. How-ever, retained tokens form irregular maps and thus cannotbe used for convolution, as shown in . We need to re-cover the pruned tokens to make them compatible with thefollowing convolutional operations in the ResNet layer.(I) Padding Zeros. One straightforward way to do this isto pad zeros. However, to maintain the high quality of gen-erated images, we hope to recover the pruned tokens as pre-cisely as possible, as if they were not pruned.",
  "ResNetLayer": ". Our similarity-based copy method for token recoveringresolves the incompatibility between token pruning and ResNet.Token pruning incurs the non-square shape of feature maps andthus is not compatible with ResNet. To address this issue, we pro-pose similarity-based copy to recover the pruned tokens. It firstaverages the attention map across heads and deletes the rows ofpruned tokens to avoid selecting them as the most similar one.Then, it finds the source of the highest attention received for eachpruned token and copies the corresponding retained tokens forrecovery. After recovering, the tokens can be translated into aspatially-complete feature map to serve as input to ResNet blocks. (II) Interpolation. Interpolation methods, such as bicubicinterpolation, are not suitable in this context. To use the in-terpolation algorithm, we first pad zeros to fill the prunedtokens and form a feature map of size N N. Then wedownsample it to N",
  "N": "2 and upsample it back to N Nwith the interpolation algorithm. We keep the values of re-tained tokens fixed and only use the interpolated values ofpruned tokens. Due to the high pruning rates (usually largerthan 50%), most tokens that represent the background getpruned, leading to lots of pruned tokens that are surroundedby other pruned tokens instead of retained tokens. Interpo-lation algorithms assign nearly zero values to these tokens.(III) Direct copy. Another possible method is to use thecorresponding values before pruning is applied (i.e., beforebeing processed by the following attention layers) to fillthe pruned tokens. The problem with this method is thatthe value distribution changes significantly after being pro-cessed by multiple attention layers, and copied values arefar from the values of these tokens if they are not prunedand are processed by the following attention layers.To avoid the effect of distribution shift, we propose thesimilarity-based copy technique, as shown in . In-stead of copying values that are not processed by attentionlayers, we select tokens that are similar to pruned tokensfrom the retained tokens. We use the self-attention map todetermine the source of the highest attention received foreach pruned token and use that as the most similar one. Thisis based on the intuition that attention from token xa to to-ken xb, Aa,b, is determined by two factors: (1) importance",
  "Region IRegion IIRegion IIIRegion IV": ". Variance of attention maps in different denoising steps.We divide the denoising steps into four typical regions: (I) Very-early steps: Variance of attention maps is small and increasesrapidly. (II) Mid-early steps: Variance of attention maps is largeand increases slowly. (III) Middle steps: Variance of attentionmaps is large and almost constant. (IV) Last several steps. of token xb, i.e., s(xb), and (2) similarity between token xaand xb. If we observe the attention that xb receives, i.e.,compare {Ai,b}iN, since s(xb) is fixed, index i = thatmaximizes {Ai,b}iN is the index of the most similar to-ken, i.e., x. Finally, we copy the value of token x to fill(i.e., recover) the pruned token xb.",
  ". Part II: Denoising-Steps-Aware Pruning": "Early denoising steps determine the layout of generated im-ages and, thus, are crucial. On the contrary, late denoisingsteps aim at refining the generated image, natively includingredundant computations since many regions of the image donot need refinement. In addition, early denoising steps havea weaker ability to differentiate unimportant tokens, and latedenoising steps yield informative attention maps and differ-entiate unimportant tokens better. To support this claim, weinvestigate the variance of feature maps in different denois-ing steps, as shown in . It indicates that attentionmaps in early steps are more uniform. They assign similarattention scores to both important and unimportant tokens,making it harder to precisely identify unimportant tokensand prune them in early steps. Based on these intuitions, wepropose DSAP that employs a prune-less schedule in earlydenoising steps by leaving some of the layers unpruned.The Prune-Less Schedule. In SD-XL, each down-stage in-cludes two attention blocks and each up-stage includes threeattention blocks (except for stages without attention). Themid-stage also includes one attention block. Each atten-tion block includes 2-10 attention layers. In our prune-lessschedule, we select some attention blocks to not performtoken pruning. Since previous works indicate thatthe mid-stage contributes much less to the generated im-age quality than the up-stages and down-stages, we do notselect the attention block in the mid-stage. Based on theablation study, we choose to leave the first attention blockin each down-stage and the last attention block in each up-stage unpruned. We use this prune-less schedule for the first denoising steps. We explore setting in different regionsshown in and find = 15 is the optimal choice. We present all the related ablative experimental results in.4. A detailed description of the less aggressivepruning schedule is provided in Supplementary Material.To further consolidate our intuitions, we also investigate amore aggressive pruning schedule in early denoising stepsand find it is inferior to our current approach (see Supple-mentary Material).",
  ". Experimental Setup": "Common Settings.We implement both our AT-EDMmethod and ToMe on the official repository of SD-XL andevaluate their performance. The resolution of generated im-ages is 10241024 pixels and the default FLOPs budget foreach denoising step is assumed to be 4.1T, which is 38.8%smaller than that of the original model (6.7T) unless other-wise noted. The default CFG-scale for image generation is7.0 unless otherwise noted. We set the total number of sam-pling steps to 50. We use the default sampler of SD-XL,i.e., EulerEDMSampler.AT-EDM. For a concise design, we only insert a pruninglayer after the first attention layer of each attention blockand set the pruning ratio for that layer to . To meet theFLOPs budget of 4.1T, we set = 63%. For the DSAPsetting, we choose to leave the first attention block in eachdown-stage and the last attention block in each up-stage un-pruned. We use this prune-less schedule for the first = 15denoising steps.ToMe. The SD-XL architecture has changed significantlycompared to previous versions of SDMs (see Supplemen-tary Material). Thus, the default setting of ToMe does notlead to enough FLOPs savings. To meet the FLOPs budget,it is necessary to use a more aggressive merging setting.Therefore, we expand the application range of token merg-ing (1) from attention layers at the highest feature level to allattention layers, and (2) from self-attention to self-attention,cross-attention, and the feedforward network. We set themerging ratio r = 50% to meet the FLOPs budget of 4.1T.Evaluations. We first compare the generated images withmanually designed challenging prompts in .2.Then, we report FID and CLIP scores of zero-shot imagegeneration on the MS-COCO 2017 validation dataset in .3. Tested models generate 10241024 px im-ages based on the captions of 5k images in the validation set.We provide ablative experimental results and analyze themin .4 to justify our design choices. We providemore implementation details in Supplementary Material.",
  "easel there is a Rembrandt painting of a raccoon": ". Comparing AT-EDM to the state-of-the-art approach, ToMe . While the full-size SD-XL (Col. a) consumes 6.7 TFLOPs,we compare the accelerated models (Col. b-e) at the same budget of 4.1 TFLOPs. Compared to ToMe, we find that AT-EDMs tokenpruning algorithm provides clearer generated objects with sharper details and finer textures, and a better text-image alignment where itbetter retains the semantics in the prompt (see the fourth row). Moreover, we find that DSAP provides better structural layout of thegenerated images, which is effective for both ToMe and our approach. AT-EDM combines the novel token pruning algorithm and theDSAP schedule (Col. e), outperforming the state of the art.",
  ". Visual Examples for Qualitative Analysis": "We use manually designed challenging prompts to evalu-ate ToMe and our proposed AT-EDM framework. The gen-erated images are compared in . We compare moregenerated images in Supplementary Material. Visual exam-ples indicate that with the same FLOPs budget, AT-EDMdemonstrates better main object preservation and text-image alignment than ToMe. For instance, in the first ex- ample, AT-EDM preserves the main object, the face of theold man, much better than ToMe does. AT-EDMs strongability to preserve the main object is also exhibited in thesecond example. ToMe loses high-frequency features of themain object, such as texture and hair, while AT-EDM retainsthem well, even without DSAP. The third example again il-lustrates the advantage of AT-EDM over ToMe in preserv-ing the rappers face. The fourth example uses a relatively",
  "@": ". FID-CLIP score curves. The used CFG scales are [1.0,1.5, 2.0, 2.5, 3.0, 4.0, 5.0, 6.0, 7.0, 9.0, 12.0, 15.0]. This figureis zoomed in to the bottom-right corner to show the comparisonbetween the best trade-off points. AT-EDM outperforms ToMe bya clear margin. See complete curves in Supplementary Material. complex prompt that describes relationships between mul-tiple objects. ToMe misunderstands a Rembrandt paintingof a raccoon as being a random painting on the easel anda painting of a raccoon on the wall. On the contrary, the im-age generated by AT-EDM understands and preserves theserelationships very well, even without DSAP. As a part ofour AT-EDM framework, DSAP is not only effective in AT-EDM but also beneficial to ToMe in improving image qual-ity and text-image alignment. When we deploy DSAP inToMe, we select corresponding attention blocks to not per-form token merging, while keeping the FLOPs cost fixed.",
  ". Quantitative Evaluations": "FID-CLIP Curves. We explore the trade-off between theCLIP and FID scores through various Classifer-Free Guid-ance (CFG) scales. We show the results in . AT-EDM does not deploy pruning at the second feature level(see Supplementary Material).It indicates that for mostCFG scales, AT-EDM not only lowers the FID score butalso results in higher CLIP scores than ToMe, implying thatimages generated by AT-EDM not only have better qualitybut also better text-image alignment. Specifically, when theCFG scale equals 7.0, AT-EDM results in [FID, CLIP] =[28.0, 0.321], which is almost the same as the full-size one([27.3, 0.323], CFG scale=4.0). For comparison, ToMeresults in [35.3, 0.320] with a CFG scale of 7.0. Thus, AT-EDM reduces the FID gap from 8.0 to 0.7.Various FLOPs Budgets. We deploy ToMe and AT-EDMon SD-XL under various FLOPs budgets and quantitativelycompare their performance in . The FLOPs cost inthis table refers to the average FLOPs cost of a denoisingstep. indicates that AT-EDM achieves better im-age quality than ToMe (lower FID scores) under all FLOPsbudgets. When the FLOPs budget is extremely low (lessthan 50% of the full model), ToMe achieves higher CLIP",
  "AT-EDM-d27.230.32454.5": "scores than AT-EDM. When the FLOPs saving is 30-40%,AT-EDM achieves not only better image quality (lower FIDscores) but also better text-image alignment (higher CLIPscores) than ToMe. Note that under the same CFG-scale,AT-EDM achieves a lower FID score than the full-sizemodel while reducing FLOPs by 32.8%. In the case thatit trades text-image alignment for image quality (via reduc-ing the CFG scale to 4.0), AT-EDM achieves not only alower FID score but also a higher CLIP score than thefull-size model while reducing FLOPs by 32.8%. We pro-vide more visual examples under various FLOPs budgets inSupplementary Material.Latency Analysis. SD-XL uses the Fused Operation (FO)library, xformers , to boost its generation. The CurrentImplementation (CI) of xformers does not provide attentionmaps as intermediate results; hence, we need to addition-ally calculate the attention maps. We discuss the samplinglatency for three cases: (I) without FO, (II) with FO un-der CI, and (III) with FO under the Desired Implementation(DI), which provides attention maps as intermediate results. shows that with FO, the cost of deploying prun-ing at the second feature level exceeds the latency reduc-tion it leads to. Hence, AT-EDM is faster than AT-EDM. shows the extra latency incurred by different pruningsteps shown in . With a negligible quality loss, AT-EDM achieves 52.7%, 15.4%, 17.6% speed-up in termsof latency w/o FO, w/ FO under CI, w/ FO under DI, re-spectively, which outperforms the state-of-the-art work bya clear margin. We present the memory footprint of AT-EDM in Supplementary Material.",
  "(Black: Pruned Tokens)": ". Comparison between different implementations of G-WPR: CA-based WPR and SA-based WPR. In general, CA-basedWPR may remove too many background tokens, making the back-ground not recoverable, while SA-based WPR preserves the imagequality better. ated image examples for a visual comparison in . Thisfigure indicates that SA-based WPR outperforms CA-basedWPR. The reason is that CA-based WPR prunes too manybackground tokens, making it hard to recover the back-ground via similarity-based copy.Similarity-based Copy. We provide comparisons betweendifferent methods to fill the pruned pixels in , whichdemonstrate the advantages of our similarity-based copymethod.Images generated by bicubic interpolation arequite similar to those generated by padding zeros becauseinterpolation usually assigns near-zero values to pruned to-kens that are surrounded by other pruned tokens and canhardly recover them. Direct copy means directly copyingcorresponding token values before the first pruning layer inthe attention block to recover the pruned tokens, where thefollowing attention layers do not process the copied values.Thus, the copied values cannot recover the information inpruned tokens and even negatively affect the retained to-kens. On the contrary, similarity-based copy uses attentionmaps and tokens that are retained to recover the pruned to-",
  "Generated imageFeature map": ". Different methods to recover the pruned tokens. Zeropadding (Col. b), bicubic interpolation (Col. c), and direct copy(Col. d) can hardly recover pruned tokens and result in noticeableimage degradation with blurry background (incomplete moon).On the other hand, similarity-based copy (Col. e) provides betterimage quality and keeps the complete moon in the original image.Better viewed when zoomed in.",
  "Denoising-Steps-Aware Pruning. We explore different de-sign choices for DSAP": "(1) The prune-less schedule selects one attention block fromeach down-stage and up-stage in the U-Net and skips thetoken pruning in it. According to ablation results shownin Supplementary Material, F-L (First-Last) appears to bethe best one, i.e., leaving the first attention block of down-stages and the last attention block of up-stages unpruned inearly denoising steps. (2) We then explore how the number of early prune-less de-noising steps affects the generated image quality in .Note that we keep the FLOPs budget fixed and adjust thepruning rate accordingly when we change the number ofprune-less steps. This figure shows that the setting of 15early prune-less steps provides the best image quality. Notethat the setting of zero prune-less step is identical to the set-ting without DSAP, and 5, 15, 30, 45 prune-less steps rep-resents setting the boundary in Region I, II, III, IV of ,respectively. The results indicate that placing the bound-ary between the prune-less and normal schedule in RegionII performs best. This meets our expectation because thevariance of attention maps becomes high enough to identifyunimportant tokens well in Region II.",
  ". Conclusion": "In this article, we proposed AT-EDM, a novel framework foraccelerating DMs at run-time without retraining. AT-EDMhas two components: a single-denoising-step token prun-ing algorithm and a cross-step pruning schedule (DSAP). Inthe single-denoising-step token pruning, AT-EDM exploitsattention maps in pre-trained DMs to identify unimportanttokens and prunes them to accelerate the generation pro-cess. To make the pruned feature maps compatible withthe latter convolutional blocks, AT-EDM again uses atten-tion maps to reveal similarities between tokens and copiessimilar tokens to recover the pruned ones. DSAP furtherimproves the generation quality of AT-EDM. We find sucha pruning schedule can also be applied to other methodslike ToMe. Experimental results demonstrate the superior-ity of AT-EDM with respect to image quality and text-imagealignment compared to state-of-the-art methods. Specifi-cally, on SD-XL, AT-EDM achieves a 38.8% FLOPs savingand up to 1.53 speed-up while obtaining nearly the sameFID and CLIP scores as the full-size model, outperformingprior art.",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-sion probabilistic models. Advances in Neural InformationProcessing Systems, 33:68406851, 2020": "Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, BenPoole, Mohammad Norouzi, David J. Fleet, et al. Imagenvideo: High definition video generation with diffusion mod-els. arXiv preprint arXiv:2210.02303, 2022. Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet,Mohammad Norouzi, and Tim Salimans. Cascaded diffusionmodels for high fidelity image generation. The Journal ofMachine Learning Research, 23(1):22492281, 2022. Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, HuiwenChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:Text-based real image editing with diffusion models. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 60076017, 2023.",
  "rons. Advances in Neural Information Processing Systems,33:585595, 2020": "Woosuk Kwon, Sehoon Kim, Michael W Mahoney, JosephHassoun, Kurt Keutzer, and Amir Gholami.A fast post-training pruning framework for transformers. Advances inNeural Information Processing Systems, 35:2410124116,2022. Benjamin Lefaudeux, Francisco Massa, Diana Liskovich,Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu,Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut,and Daniel Haziza.xFormers: A modular and hackabletransformer modelling library. 2022. Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, HuajunFeng, Zhihai Xu, Qi Li, and Yueting Chen. SRDiff: Singleimage super-resolution with diffusion probabilistic models.Neurocomputing, 479:4759, 2022. Lijiang Li, Huixia Li, Xiawu Zheng, Jie Wu, Xuefeng Xiao,Rui Wang, Min Zheng, Xin Pan, Fei Chao, and Rongrong Ji.AutoDiffusion: Training-free optimization of time steps andarchitectures for automated diffusion model acceleration. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 71057114, 2023. Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys,Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snap-Fusion: Text-to-image diffusion model on mobile deviceswithin two seconds. arXiv preprint arXiv:2306.00980, 2023. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C. LawrenceZitnick. Microsoft COCO: Common objects in context. InProceedings of the European Conference on Computer Vi-sion, pages 740755. Springer, 2014.",
  "Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-hall. DreamFusion: Text-to-3D using 2D diffusion. arXivpreprint arXiv:2209.14988, 2022": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1068410695, 2022. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,and Surya Ganguli.Deep unsupervised learning usingnonequilibrium thermodynamics. In Proceedings of the In-ternational Conference on Machine Learning, pages 22562265. PMLR, 2015.",
  "Shuai Yang, Yukang Chen, Luozhou Wang, Shu Liu, andYingcong Chen.Denoising diffusion step-aware models.arXiv preprint arXiv:2310.03337, 2023": "Xingyi Yang, Daquan Zhou, Jiashi Feng, and Xinchao Wang.Diffusion probabilistic model made slim. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 2255222562, 2023. Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, andKevin Bailly. Red: Looking for redundancies for data-freestructured compression of deep neural networks. Advancesin Neural Information Processing Systems, 34:2086320873,2021.",
  "Supplementary Material": "The Supplementary Material is organized as follows. Wefirst provide more implementation details of AT-EDM inSection A, including a detailed illustration of the SD-XLbackbone. Then, we provide a more comprehensive com-parison with the state-of-the-art method, ToMe , in Sec-tion B, including an analysis of why ToMe performs worseon SD-XL than on previous versions of Stable Diffu-sion Models (SDMs). We provide more ablation results inSection C to justify our design choices in the main article.We analyze the memory footprint of AT-EDM in SectionD. AT-EDM is orthogonal to various efficient DM methods,such as sampling distillation, thus can further boost their ef-ficiency. To support this claim, we deploy AT-EDM in thedistilled version of SD-XL, SDXL-Turbo1, and show cor-responding experimental results in Section E. We discusslimitations and trade-offs of AT-EDM in Section F and po-tential negative social impacts of AT-EDM in Section G.",
  "A.1. The SD-XL Backbone": "The state-of-the-art version of SDM is SD-XL. Comparedwith previous versions of SDM, it increases the quality ofgenerated images significantly. Thus, we select SD-XL asthe backbone model in this article. Specifically, we deployAT-EDM and ToMe on SDXL-base-0.9. The architec-ture has two main differences from that of previous SDMs,such as SD-v1.5 and SD-v2.1: (1) attention blocks at thehighest feature level (i.e., with the most tokens) are deleted;(2) attention blocks can potentially include multiple atten-tion layers (an attention layer is composed of self-attention,cross-attention, and feed-forward network), such as A2 (in-cludes 2 attention layers) and A10 (includes 10 attentionlayers).To validate the conclusion that the cost of attention lay-ers dominates the sampling cost, we investigate the FLOPscost of SD-XL. Its FLOPs profile is shown in . Thisfigure indicates that the attention block dominates the com-putational cost of all stages that include attention. We alsoinvestigate the scaling law of SD-XL at different genera-tion resolutions, as shown in . We observe that the",
  ". The FLOPs breakdown of ResNet blocks and attentionblocks in SD-XL at different image resolutions": "attention block dominates the cost at all resolutions. Notethat the FLOPs cost of attention blocks does not scale muchfaster than that of ResNet blocks when the generation res-olution increases. We believe this is due to the eliminationof attention blocks at the highest feature level and the addi-tion of attention layers at the lowest feature level, makingthe cost of feed-forward layers, which scales linearly withan increment in token numbers, a huge part of the cost ofattention layers.",
  "A.2. Pruning in a Single Denoising Step": "For a concise design, we always insert the pruning layerafter the first attention layer of each attention block. Allthe other attention layers in this attention block can benefitfrom the reduction in token numbers. We may also insertmultiple pruning layers at various locations in an attentionblock, which prunes tokens gradually. However, this re-quires a more thorough hyperparameter search to ensure agood balance between FLOPs cost and image quality.",
  "$": ". The U-Net architecture of SD-XL. Residual connections are not shown here for brevity. The example in this figure generatesa 8H 8W pixel image. The input/output size of each stage is shown in the C H W format, where C is the number of channels;H and W represent the resolution. There are two attention blocks {F(First), L(Last)} in each downsampling stage and three {F(First),M(Middle), L(Last)} in each upsampling stage. In the prune-less schedule, we do not apply pruning to attention blocks in the grayrectangles. Downsampling stage 1, 2, and 3 is at the first, second, and third feature level, respectively. AT-EDM does not apply pruningto attention blocks at the second feature level.",
  "A.3. The Prune-Less Schedule": "Early denoising steps determine the layout of the generatedimages and have a weaker ability to differentiate betweenunimportant tokens . Thus, we need heterogeneous de-noising steps and, hence, use a less aggressive pruningschedule for some of the early denoising steps.In the normal pruning setting, when we target 4.1TFLOPs for each sampling step, we use a pruning rate of63% (i.e., retain 37% tokens) after the first attention layerof A2 and A10; in the prune-less schedule, we do not ap-ply pruning to attention blocks in the gray rectangles shownin . We validate the choice of not deploying prun-ing through ablative experimental results shown in the mainarticle.",
  "A.4. Details of Evaluation": "When measuring the FID and CLIP scores on MS-COCO2017 , we deduplicate captions to make sure each im-age corresponds to a single caption. We center cropped im-ages in the validation set, resize them to 10241024 px,and use the clean-fid library2 to calculate FID scores.We use the ViT-G/14 model of Open-CLIP3 to calculate theCLIP scores of generated images. We set the batch size to 3",
  "A.5. Calibration Block for FLOPs Measurement": "The popular library for FLOPs measurement, fvcore4,is not natively compatible with SDMs.Thus, we usethe THOP5 library instead to measure the FLOPs cost ofSDMs. However, we found it does not correctly computethe FLOPs cost of self-attention. The FLOPs cost of sam-pling steps given by this library scales linearly as the num-ber of image tokens. This is unreasonable because the costof self-attention in sampling steps scales quadratically whenthe number of tokens increases (other parts of a samplingstep scale linearly). After a thorough investigation of thebehavior of THOP, we found it basically does not take thecost of self-attention into account. Thus, we design a cali-bration block to supplement the missed term of FLOPs costfor each attention block:",
  "B. Comprehensive Comparison with ToMe": "In this section, we first analyze why ToMe cannot replicateon SD-XL its good performance on previous SDMs in Sec-tion B.1. Then, we provide complete FID-CLIP curves tocompare AT-EDM with ToMe in Section B.2. In the end,we present cases in which both AT-EDM and ToMe per-form well and visually compare AT-EDM and ToMe undervarious FLOPs budgets in Section B.3.",
  "B.1. Deploying ToMe on SD-XL": "For SD-v1.x and SD-v2.x, ToMe maintains the generatedimage quality quite well after token merging. However, aswe demonstrate in the main article, ToMe incurs obviousquality degradation on SD-XL after token merging.In the default setting of ToMe, it only merges tokens forattention blocks at the highest feature level and their self-attention. However, SD-XL eliminates attention blocks atthe highest abstraction level and native ToMe does not doanything to this backbone. Thus, it is necessary to expandits merging range to attention blocks at all feature lev-els. In addition, since SD-XL adds a lot more attentionlayers at the lowest feature level, where tokens are signif-icantly fewer than at higher feature levels, self-attention nolonger dominates the cost of attention layers. Given thatthe merging ratio of ToMe has an upper limit of 75%, it isnot enough to only merge tokens for self-attention to meetthe 4.1 TFLOPs budget. Thus, it is necessary to expandits merging range to Cross-Attention (CA), Self-Attention(SA), and the Feed-Forward (FF) network. We believethe expanded deployment range of token merging leads tothe relatively poor performance of ToMe on SD-XL. Note",
  "B.2. Complete FID-CLIP Curves": "We explore the trade-off between the CLIP and FID scoresthrough various CFG scales. We show the complete FID-CLIP curves in . AT-EDM does not deploy pruningat the second feature level (as mentioned in the caption of). This figure illustrates that for most CFG scales,AT-EDM not only lowers the FID score but also results inhigher CLIP scores than ToMe, implying that images gener-ated by AT-EDM not only have better quality but also bettertext-image alignment.",
  "AT-EDMToMe": ". Comparison between AT-EDM and ToMe under different FLOPs budgets. Note that for Col.e, the average cost of each samplingstep for AT-EDM (ToMe) is 4.52 (4.56) TFLOPs. Prompts are selected from the MS-COCO 2017 validation dataset. preserve the main object quite well. The second row repre-sents a more complex case in which there are multiple mainobjects in the generated image. Although ToMe loses sometexture details, it preserves the overall layout quite well.The third row is the case of a typical complex main object,a human face. In this example, ToMe preserves the facewithout artifacts. The last row of this figure demonstratesthe case of generating a complex scene without a main ob-ject. In this case, both ToMe and AT-EDM can maintain thelayout well while supplementing some details. These exam-ples show that ToMe is a strong baseline and it is non-trivial to outperform it.We also provide visual examples of ToMe and AT-EDMunder different FLOPs budgets in . It indicates thatAT-EDM outperforms ToMe under any FLOPs budget. Wealso observe that AT-EDM needs at least 3.6 TFLOPs bud-get to ensure an acceptable image quality.",
  ". Comparison between inserting the pruning layer afterthe FF and before the FF layer": "location for run-time pruning and then compare differentimplementations of the mapping function f(A, sK) for CA-based WPR. Note that CA-based WPR and SA-based WPRare two implementations of G-WPR and we mainly focuson CA-based WPR in this section. We also investigate theschedule that prunes more in early denoising steps and ver-ify our intuition of pruning less in early steps.",
  "C.1. Deployment Location for Run-Time Pruning": "In our default setting, we use generated masks after the FFlayer to perform token pruning. Another option is to per-form pruning early before the FF layers, which results in alittle bit of extra FLOP savings at the cost of image quality.We provide several visual examples in . Note thathere, we simply change the pruning layer insertion locationwithout keeping the total FLOPs cost fixed, which is differ-ent from what we do in the ablation experiments in the mainarticle. We find that inserting the pruning layer before theFF layer indeed hurts image quality (although slightly). Forexample, the plant in the first example and the UFO in thesecond example become worse. Given that pruning beforethe FF layer only results in marginal extra FLOPs savings(reduces the cost from 4.1 TFLOPs to 4.0 TFLOPs), wechoose to prune after the FF layer to obtain better imagequality.",
  "C.2. Implementations of CA-based WPR": "To generalize WPR to cross-attention, we need to designa function f(A, sK) that maps the importance of Key to-kens to that of Query tokens. The intuition behind design-ing this function is that vital Query tokens should devotemuch of their attention to important Key tokens. Thus, thedesired attention distribution should satisfy: (1) similarityto the importance distribution of Key tokens; (2) concen- tration on a few tokens. Then, when designing f(A, sK),we need to (1) reward the similarity between the attentiondistribution (i.e., each row of A) and the importance distri-bution (i.e., sK); (2) penalize uniform attention distribution.Based on these points, we obtain several implementations off(A, sK). We had mentioned an entropy-based implemen-tation in the main article, which rewards similarity throughthe dot-product and penalizes uniform distribution throughentropy. We provide additional implementations here:(I) Hard-clip-based implementation",
  "j=1( st+1K (xj))Ai,j(5)": "where and are scaling factors to ensure that st+1K (xj) > 1 and Ai,j > 1 for large st+1K (xj) and Ai,j.Here, we let = 5 and =Nt2 , where Nt denotes thenumber of Key tokens.We compare these implementations visually in .We find that among these implementations, the hard-clip-based implementation performs the worst.Although theentropy-based implementation and the power-based imple-mentation are better than other implementations for CA-based WPR, none of them can outperform SA-based WPR.Thus, we use SA-based WPR as our default setting in AT-EDM.",
  "C.4. The Number of Prune-Less Steps": "The intuitions that we use to design the prune-less sched-ule in the early denoising steps are (1) early denoising stepsdetermine the layout of generated images and thus are cru-cial; (2) early denoising steps have a weaker ability to dif-ferentiate unimportant tokens. The first intuition prohibitsus from pruning more tokens in the early steps (see SectionC.5). The second intuition guides us to choose the numberof prune-less steps. The variance of attention maps reflectstheir ability to differentiate unimportant tokens since the at-tention score of unimportant tokens deviates significantlyfrom that of normal tokens. We show the variance of atten-tion maps given by different denoising steps in . Thefigure indicates that the variance is more than 1.0E-5 after",
  "C.5. Prune More in Early Denoising Steps": "In AT-EDM, we design a cross-step pruning schedule thatis less aggressive in early denoising steps. This is basedon the intuition that (1) early denoising steps determine thelayout of generated images and thus are very important; (2)the ability of early denoising steps to differentiate betweenunimportant tokens is weaker than that of later steps. Toverify our intuition, we investigate the schedule that prunesmore in early denoising steps.Note that for symmetry,prune more in the first 15 steps selects corresponding at-tention blocks in the last 35 steps for not pruning tokens",
  "D. Memory Footprint of AT-EDM": "Since we need to obtain the attention map from the firstattention layer, AT-EDM cannot reduce the peak memoryfootprint.However, benefiting from the significantly re-duced number of tokens in the following attention lay-ers, AT-EDM reduces the average memory footprint sig-nificantly. Since PyTorch does not automatically releasethe redundant assigned memory when the memory require-ment reduces in the later layers, we theoretically estimatethe average footprint of AT-EDM, assuming the redundantoccupied memory will be released in the layers with fewertokens. We believe this is practical when the implementa-tion is good enough. The peak and theoretical average foot-print of full-size SD-XL (AT-EDM) are 19.5GB (19.5GB)and 18.8GB (12.6GB), respectively. This indicates that ifwe have a fine-grained pipeline schedule, AT-EDM allowsus to run 49.2% more generation tasks with the givenVRAM restriction.",
  "E. Stack with Sampling Distillation": "Methods like consistency distillation can greatlyreduce the cost of DMs. Note that AT-EDM does not con-tradict these methods and can be deployed to speed themup further. To support this, we deploy AT-EDM in SDXL- Turbo, which is a distilled version of SD-XL. Our exper-imental results show that although SDXL-Turbo reducesaround 95% FLOPs cost of SD-XL, AT-EDM can furtherreduce the FLOPs cost of SDXL-Turbo by 33.4% whilereducing FID by 14.5% and only incurring 2.1% CLIPreduction on MSCOCO-2017. AT-EDM works as a regu-larizer and slightly improves the quality of images.",
  "AT-EDM demonstrates state-of-the-art results for accelerat-ing DM inference at run-time without any retraining. How-ever, as a machine learning algorithm, it inevitably has somelimitations": "(1) AT-EDM requires a pre-trained DM; since it saves com-putation to accelerate the model, its performance is inher-ently upper-bounded by the full-sized one. While most ofthe time, AT-EDM matches the performance of the pre-trained model, both quantitatively and qualitatively (see ex-perimental results in the main article), with around 40%FLOPs reduction, there exist some samples where the full-sized model outperforms AT-EDM (see ). Nonethe-less, AT-EDM outperforms prior art by a clear margin. Inaddition, AT-EDM is differentiable. We will fine-tune thepruned model to further improve quality in the future. (2) AT-EDM leverages the rich information stored in theattention maps, which could be inaccessible without incur-ring overhead due to the open-sourced nature of the imple-mentation. For instance, SD-XL adopts an efficientattention library, xFormers , which fuses computationto directly obtain succeeding tokens without providing in-termediate attention maps. As shown in , in the casethat Fused Operation (FO) is not used, using AT-EDM leadsto significant latency savings. With FO under the CurrentImplementation (CI), AT-EDM does not result in a huge la-tency saving due to the cost of calculating attention maps.Reusing attention maps across steps and obtaining an ap-proximation for them could alleviate this issue. With FOunder the Desired Implementation (DI) that provides atten-tion maps, AT-EDMs potential is fully unlocked and leadsto a decent speedup. AT-EDM is especially good at generating object-centricimages, such as a portrait. It can employ a high pruningrate without hurting the main object. Generating complexscenes or tens of objects is relatively tricky for AT-EDMsince it may lose some details in corner cases. In some rarecorner cases where the texture details are not significant,ToMe might perform slightly better, as our algorithm mayprune too many tokens in that small region. ToMe is indeeda strong baseline, but it is remarkable that our AT-EDM stilloutperforms it in most cases.",
  "G. Potential Negative Social Impacts": "Text-to-image generative models like SD-XL have signif-icantly advanced the field of AI and digital art creation.However, they may also potentially have negative social im-pact. For example, they can create highly realistic imagesthat may be indistinguishable from real photographs. Asthe technology can be used to create convincing but falseimages, this can potentially lead to confusion and misinfor-mation spread. In addition, the use of these models to createinappropriate or harmful content, such as realistic images ofviolence, hate speech, or explicit material, raises significantethical questions. There is also the potential for perpetuat-ing biases if the AI model is trained on biased datasets."
}