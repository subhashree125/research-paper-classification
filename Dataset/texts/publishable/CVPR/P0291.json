{
  "Abstract": "Learning commonsense reasoning from visual contextsand scenes in real-world is a crucial step toward advancedartificial intelligence. However, existing video reasoningbenchmarks are still inadequate since they were mainly de-signed for factual or situated reasoning and rarely involvebroader knowledge in the real world. Our work aims todelve deeper into reasoning evaluations, specifically withindynamic, open-world, and structured context knowledge.We propose a new benchmark (SOK-Bench), consisting of44K questions and 10K situations with instance-level an-notations depicted in the videos. The reasoning process isrequired to understand and apply situated knowledge andgeneral knowledge for problem-solving.To create sucha dataset, we propose an automatic and scalable gener-ation method to generate question-answer pairs, knowl-edge graphs, and rationales by instructing the combina-tions of LLMs and MLLMs.Concretely, we first extractobservable situated entities, relations, and processes fromvideos for situated knowledge and then extend to open-world knowledge beyond the visible content. The task gen-eration is facilitated through multiple dialogues as itera-tions and subsequently corrected and refined by our de-signed self-promptings and demonstrations. With a corpusof both explicit situated facts and implicit commonsense, wegenerate associated question-answer pairs and reasoningprocesses, finally followed by manual reviews for qualityassurance. We evaluated recent mainstream large vision-language models on the benchmark and found several in-sightful conclusions. For more information, please refer toour benchmark at www.bobbywu.com/SOKBench.",
  "*The authors contributed equally to the work": "(MLLMs) mark a significant mile-stone in artificial intelligence. These models are making re-markable strides across various domains, exhibiting bettercapabilities in perceptual, generative, and comprehensivetasks . As models like ChatGPT and succes-sors continue to improve in scale, the possibility to capturecommonsense knowledge has also seen obvious advance-ments.Hence, building models with real-world knowl-edge is becoming more promising now which was a long-standing but difficult challenge before .Besides novel models, previous efforts have been di-rected towards evaluating multimodal commonsense rea-soning, transitioning from purely language-based modelsto those incorporating visual comprehension, such as Vi-sual Question Answering (VQA) or VisualCommonsense Reasoning (VCR) .These tasks have an underlying assumption that the givenvisual and language inputs (e.g., images, videos, questions,etc.) contain most of the required information. However,relying on the given task context does not fully reflect real-world complexities, narrowing the scope of their reason-ing evaluation.More recently, the focus has shifted to-wards developing tasks that require external knowledge forcommonsense reasoning and consider the inte-gration of factual , evidential , or external knowl-edge sources into evaluations. Theyprimarily provide visual information and external infor-mation on static images for reasoning.Thus, this wayoverlooks several crucial natures such as spatio-temporal,causal, or dynamic processes of real-world activities andevents, an aspect crucial for truly understanding and inter-acting with our environment. Also, the knowledge is writ-ten and concluded by free-form descriptions from crowd-source ways , presenting its own set oflimitations. Consistency in descriptions provided by anno-tators is difficult to maintain , and there are multiplerisks of the method being influenced by distinct descriptiveor empirical biases .Our research aims to delve deeper into the realm of com-",
  "arXiv:2405.09713v2 [cs.CV] 17 May 2024": "Mapo Tofu ++ the person did not use the in ? A.The dish would be completely different. B.The dish would lack thickening agent. C.The dish would lack seasoning and taste bland. D.The dish would lose its signature flavor. attributeswhite substance relationship dissolved in counterfactual What would happen ifQuestion: waterglass Previous methods",
  "Graph": "Situated Knowledge Graph1. From [], we can see that is . 2. From [], we can see that is in . 3. From [], we know that cornstarch can be used as . 4. , from 32 sec to 40 sec, we can see is put in a small bowl and mixed with water to form a paste. 5. This paste is then added to the dish (98 sec to 105 sec) to thicken the sauce. 6. Therefore, the right choice is The dish would lack thickening agent. 7. The other choices can be eliminated because they are not the correct consequences if the person did not use cornstarch. object attributes whitecornstarch cornstarch waterglass cornstarch object-object relationsdissolved in In the video",
  "Rationale": ". Overview. Instead of using crowd-sourced methods, we design a synthesis pipeline to create the benchmark by leveraginguse LLMs and VLLMs, improving efficiency and ensuring consistency. The method helps to automatically generate high-quality question-answers (QAs) and focusing the desirable purposes for evaluating the models ability. To generate data aligned with open-world knowledge,we propose to connect situation, general knowledge, and situated commonsense and produced three types of associated knowledge graphs(refer to the subsection 3.2, 3.3, and 3.4). Specifically, it makes more precise inferences based on situational facts and essential common-sense knowledge by aligning with the bottom-up or top-down goals, the reasoning process from Q to A is able to demonstrate explicitly. monsense reasoning, specifically within dynamic, open-world, and structured contexts. We build Situated Open-World Commonsense Reasoning (SOK-Bench), a novelbenchmark consists of over 44K questions with answers andsituated commonsense knowledge graphs, covers over 12types of questions, and sources from about 10K dynamicsituations in real-world activities. The models are requiredto produce appropriate inferences by leveraging both factswithin situations and the necessary commonsense or back-ground knowledge.Thus we propose the Situated Commonsense Graph toconnect and integrate the required knowledge of the reason-ing process where situated knowledge and general knowl-edge are structured, compositional, and aligned. Comparedwith other datasets in , our reasoning benchmarkhas diverse characteristics including instance annotations,compositional generation, and structural alignment of thesituated open-world knowledge and rationales. Moreover,we propose a scalable and automatic pipeline for both taskgeneration and annotation.The typical way to generateevaluation task instructions is crowd-source human anno-tation or adversarial filtering . However, collect-ing and locating high-quality situated commonsense dataare not trivial since considering situation, question, answerand their inner relations requires experienced annotators,huge cost, and high consistency. Instead, we generate eval-uation tasks, questions with answers, and reasoning pro-cesses, which integrate situated knowledge, general knowl-edge, and underlying reasoning commonsense goals or log-ics. We design appropriate instructions and compositionsfor prompts and define question types, goals to enhance generation quality, logic, and rationality, as shown in Fig-ure 3 (a). The iterative generation process automaticallyutilizes the interactions of LLMs or MLLMs in multi-turns,as shown in . In experiments, we evaluate the main-stream LLMs and VideoLLMs as representatives for theproposed reasoning benchmark. Although the models havetrained on web-scale data or adopted pre-trained foundationmodels as basis, the experiments indicate significant roomfor future improvement. We also provide comprehensiveablation comparisons and analysis for generation processes.",
  "We create SOK-Bench, a novel benchmark to evaluate sit-uated and open-world commonsense reasoning in videoswith compositionality, temporality, and causality": "We propose a scalable method to generate video question-answering pairs for reasoning through iterative conver-sations with LLMs and MLLMs. We designed diverseprompt compositions and a self-prompting strategy toconstruct the video knowledge, commonsense knowl-edge, situated commonsense knowledge graphs, etc. We evaluated representative LLMs and MLLMs on theproposed benchmark in different settings and found thatexisting LLMs and MLLMs still perform inferior on situ-ated open-knowledge reasoning in videos, which furtherproves our new datasets research value.",
  "OursAuto, Rulegraphopen-world": "the video and answer questions related to the videos con-tent . Most of these existing bench-marks ask questions about visual attributes, human actions,and activities, or physical intuition . However,none of them has focused on studying events in videos withsituated knowledge, open-domain knowledge, and explicitmulti-step reasoning rationales.Commonsense Question-Answering.Our research isaligned with the field of commonsense question answering.It requires models to make use of commonsense to answerquestions. It was first studied in the field of natural ques-tion answering without any visual context as in-put. Later, people evaluated commonsense understandingin the context of visual question answering .Given a static image, such benchmarks require models toanswer questions based on the visual context of the im-age and its associated commonsense knowledge. There aresome benchmarks studying domain-specific commonsenselike physics and social events . However, we are thefirst benchmark to study reasoning with situated and open-world knowledge in dynamic situations with dense annota-tions like knowledge graphs and rationales. summa-rizes the differences compared with previous benchmarks.LLM for Data Annotation. Our work is situated withinthe domain of using large language models for data anno-tation . Ding et al. examined the effectiveness ofGPT-3 as a data annotator for NLP tasks by comparing itwith traditional annotation methods and analyzing its per-formance across various tasks. Pei et al. introduced aself-supervised GPT annotation method using a generating-recovering paradigm, leveraging one-shot learning for effi-cient data-to-summary annotation, and evaluated its perfor-mance using alignment scores and human feedback rewardnetworks. Both of these two works only focus on the naturallanguage side. In this work, we propose to prompt LLMs tocooperate with different vision models to annotatevideos and generate question-answer pairs related to videocontent and open knowledge. Vision-Language Models. Recently, there has been greatinterest in large pre-trained vision-language models (VLMs). Early models only evalu-ate their performance on inputs with static images and nat-ural language. There are also some models evaluat-ing performance their performance on videos. However,these VLMs either only showed some qualitative exam-ples or evaluates on traditional video questionanswering benchmarks . However, are not ideal benchmarks for commonsense reasoning sincethey mainly focus on the perception side and require no ex-ternal knowledge to answer the questions. It remains anopen question as to how to evaluate VLMs performancein understanding situations in videos and reasoning aboutopen-domain knowledge with commonsense.",
  ". Benchmark Overview": "Statistics. Our benchmark consists of 44K questions andanswers for reasoning with situated open-world knowledge,accompanied by multiple-choice options, and 10K videoclips. Each question-answer (QA) pair links to a hyper-graph which is generated from the Situated KnowledgeGraph, General Knowledge Graph, and Situated Common-sense Knowledge Graph (refer to the below sections andthe algorithms in the Supplementary). The aligned graphseffectively showcase the relations between situated knowl-edge and general knowledge. presents four QAexamples involving different types situated commonsenseknowledge. We also present a detailed rationale that eluci-dates the sequence of reasoning steps that bridge the gapfrom each question to its corresponding answer, offeringa clearer understanding of the underlying thought process(see ). showed the 12 question types. Eachquestion is accompanied by a direct answer and a set of fourmultiple-choice options. The dual format ensures versatilityin response assessment, as depicted in . Wash Dishes + the person did not use the and object? A.There would be no water. B.The dishes may not be free from grease and bacteria. C.The person would not have a container to hold water. D.The person would not have a tool to scrub and clean the dishes effectively. attributesyellow green counterfactual What would happen if sponge yellow greencounterfactual to scrub and clean the dishes effectively not have a tool person Work Out +What is the of using the thing which is and ? A.To work out. B.To clean the room. C.To do nails. D.To make tea. attributesmetal gray purpose purpose dumbbell gray metal purposeadd resistance and intensity work out build strength and muscle Indian Chicken Curry +++What is the of ? A.To infuse the curry with a warm, earthy flavor and aroma. B.To add sweetness, depth, and texture to the curry. C.To provide a medium for cooking and prevent sticking. D.To enhance the flavors of the ingredients. actionheating the ingredientadding cumin seeds and chopped onions relationship in the pan temporalbefore purpose purpose 36s to 86s heat oil in pan addadd cumin seedschopped onions purpos+'medium for cooking prevent sticking California Rolls ++ What is the of the action of ? A.To make the roll easier to eat and serve. B.To compact the ingredients and shape the roll. C.To create a sticky base for the ingredients. D.To enhance the key flavor. action temporalnext purposepurposerolling up and squeezing the roll roll upsqueeze roll 251s to 303s318s to 345s cut roll into piecespurpos+' easier to eat and serv+' roll",
  "SCKGSCKG": "SCKG SCKG . SOK-Bench data examples. Each QA pair corresponds to a video clip (e.g., a video clip showing how to cook California Rolls)and a type of situated commonsense knowledge (e.g., action + temporal + purpose). For each question, we provide four options, thecorrect choice, and the associated situated commonsense graphs. O CT+ST O CB+ST O CT+GK O CB+GK O CT O CB O I A PU+ST A CT+ST A PU A CT A PO Number of questions ABCD (a)(b)Question types . (a) Sankey diagram of the 12 question types. (b) An-swer distribution among options for each question type. Meaningof abbreviations: O: Object; A: Action; CT: Counterfactual; CB:Contribution; PU: Purpose; I: Inference; PO: Possibility; ST: Spa-tiotemporal; GK: General knowledge. Notably, the Spatiotempo-ral includes obj attributes, obj-obj relations, obj attribute +obj-obj relation, and before/after action (see .5). Benchmark Generation. We present a new method for cre-ating our benchmark automatically in a structured, control-lable, and scalable way. Our approach simplifies manualand cumbersome processes and involves four streamlinedstages: 1) Extracting observable content from the videos; 2)Compiling relevant commonsense knowledge exhaustively;3) Aligning the content of the situations with this common-sense knowledge to reveal underlying logical connectionsand implications; 4) Formulating questions and answers byintegrating the gathered information.We utilize in-context learning, few-shot demonstrations,and multiple rounds of interactions to prompt LLMs andMLLMs for the generation.They are guided by a setof prompt templates to produce the proposed knowledgegraphs. For the final stage, these graphs aid in the creationof question templates, either through manual constructionor by directing the LLM to generate question-answer pairs(QAs). This strategy ensures a direct correlation betweenthe QAs, the graphs, and the rationale that maps the rea-soning from questions to answers. Weve taken measures toensure an unbiased distribution of answer options and haveemployed both LLM and human annotators to identify and",
  ". Situated Knowledge Graph": "We derive dynamic situations from videos, which provideinsights into event temporality, causality, and dynamics .Recent research has increasingly represented objects and re-lationships in real-world dynamic situations andvisual scenes using graphs. Building on this, weexpand upon traditional visual graphs to introduce our Situ-ated Knowledge Graph. This includes more comprehensiveinformation derived from dynamic situations, such as peo-ple and objects, their attributes, relationships, and actions.Our process begins by generating object and action en-tities, which are identified based on descriptions providedwith source videos (e.g., shown in ). These de-scriptions segment dynamic situations into distinct activitymoments, associating objects {oi} and actions {at} withspecific timestamps {yt}. Given original descriptions arehuman-written and lack a standardized format, we employLarge Language Models (LLMs) to parse the descriptivetext. The LLMs systematically convert the sentences intoorganized lists of objects and actions, along with their cor-responding timestamps. This approach streamlines the ex-traction of key elements from the narrative text. For someobjects that are observable in video frames but were not an-notated, we use BLIP2 to recognize objects in framesand add them to the initial object list {oi}. Next, we ex-tract object attributes and spatial relationships between ob-jects from selected keyframes within the videos. To achievethis, we utilize MiniGPT4 to articulate these objectattributes and their spatial interrelations.Using a desig-nated goal prompt, our method efficiently generates objectattributes for visual, physical, and chemical properties andrelationships (e.g., #Prompt#: Please describe the tofu inthe image in terms of size, shape, color, and texture in oneshort sentence within 10 words. #Model#: The tofus aresmall, white, sliced squares.). Finally, we align the times-tamps of frames with those of actions. When an actions",
  "LLM promptGK examplesGeneration goalOutput format": "#System#: You are a helpful, pattern-following assistant. You know a lot aboutcornstarch.#User#: Please tell me 5 usages of water.Format: The answer should follow format '<the thing applied on>: <how to apply>. The answer is a list in JSON#Assistant#: [{'human': 'hydration'}, {'dirt': 'cleaning'}, {'food': 'cooking'}, {'crop': 'irrigation'}, {'electricity': 'power generation}]#User#: Please tell me 5 usages of cornstarch. Format: The answer should follow format #Assistant#: [{sauce: thickening},",
  "Objects": "#System#: You are a helpful, pattern-following assistant. As a commonsense-driven assistant, your task is to analyze a video (length: 119 seconds) that showcases a person introducing 6 cooking steps of cooking mapo tofu in kitchen.The steps are described in a specific format [video content] The background knowledge of the objects appeared in the video is [general knowledge] #User#: What would happen if the person did not use cornstarch? Format: The answer should be short and within 20 words. #Assistant#: The sauce would lack thickness and the dish would have a thinner consistency.#User#: What would happen if the person did not use ground pork? Format: The answer should be short and within 20 words. #Assistant#: The dish would lack meaty flavor, texture, and richness, resulting in a vegetarian version of mapo tofu.",
  ". General Knowledge Graph": "We construct the general knowledge graph beginning fromthe object oi or action at as initial nodes. Taking tofu as anode example (a food product prepared by coagulating soymilk and then pressing the resulting curds into solid whitesoft toufu blocks), prompting an LLM with a goal query(e.g., Tell me some common knowledge about tofu) is abasic approach. While this will generate descriptions withrelevant knowledge, the resulting content lacks specificityand completeness, and always misses key points (e.g., thecolor of tofu). Thus, we propose multiple aspects for knowl-edge generation (e.g., usage, physical, and culture, etc) andcollaborate with goal to make it with concrete descrip-tions. The direct challenges of LLM outputs are verbose,unstructured, or repetitive (e.g., Tofu is a popular food... or Tofu is a versatile ingredient that ...). To addressthis, we use general knowledge examples with the prede- fined output format to extend the knowledge graph edgesor nodes. Moreover, relying solely on the format promptcan lead to format inconsistencies in outputs, such as miss-ing quotation marks (e.g., {tofu: ...{physical: color:white}} that cause parsing failures.Thus, we utilize aprompt triplet general knowledge examples , generationgoal , output format for GKGs (). By employingboth general knowledge examples and output format, weensure the generated knowledge is not only of high qual-ity but also consistently parseable. Here are the generatedoutputs for cornstarch usages: [{sauce: thickening},{batter: making crispy}, {body powder: absorbingmoisture}, {starch: making}, {playdough: creat-ing homemade}].",
  ". Situated Commonsense Knowledge Graph": "We propose Situated Commonsense Knowledge Graph(SCKG) by integrating information from the previously es-tablished SKG and GKG for specific dynamic situations. Toillustrate the significance and complexity of generating sit-uated commonsense knowledge, consider a straightforwardexample: thinking about the impact of omitting cornstarchin the preparation of mapo tofu. Merely knowing that corn- starch is a powdery substance (from general knowledge) isinsufficient. We need to combine this with its situated usefrom the SKG (e.g., The person mixes cornstarch with wa-ter and adds the mixture to the soup.) and its general prop-erties from the GKG (e.g., Cornstarch can be used as athickening agent.). The result is The sauce would be lessthick, and the dish would have a thinner consistency.We initially thought that a four-element prompt, videocontent gv, general knowledge kg, generation goal , out-put format , would work for generating situated common-sense knowledge. Our test indicates that the model, whilegenerating correct responses, often provides answers thatare too general.For instance, in response to a questionabout the impact of not using ground pork in mapo tofu,the model might simply reply, The dish would lack flavorand texture which, although accurate, lacks detail.Directly providing examples to instruct the LLM toachieve a specific level of detail isnt practical due to thedependency of knowledge on the varying video content. Toaddress this issue, we propose a method called Few-ShotSelf-Prompting.Few-Shot Self-Prompting uses previously generatedsituated commonsense knowledge as examples. The promptconsists of five elements: video content gv, general knowl-edge kg, situated commonsense knowledge examples ,generation goal . The details of the algorithm can befound in the Supplementary Materials. demonstrates that the situated knowledge ex-amples are constructed using knowledge about the conse-quences of not using a specific ingredient (e.g., not usingcornstarch during cooking). Over time, this process leadsto more concrete and specific situated commonsense knowl-edge. The final result, for example, would be, The dishwould lack meaty flavor, texture, and richness, resulting ina vegetarian version of mapo tofu.Our Few-Shot Self-Prompting technique enables us toexplore various situated commonsense knowledge perspec-tives, such as considering counterfactual scenarios, under-standing the purpose of an action, and recognizing an ob-jects contribution. Our experiments show that just two it-erations of knowledge generation can produce high-qualityresults, ultimately leading to the creation of the SCKG.",
  ". Question and Answer Generation": "Using the three graphs, we create question-answer pairs totest the models ability to make accurate inferences basedon situational facts and essential commonsense knowledge.We can use two approaches for this: (1) Manually createquestion-answer templates in a bottom-up manner, design-ing question templates and providing answers based on thegraphs. (2) Automatically generate questions using a LLMin a top-down manner.",
  "Bottom-up QA Generation": "We manually design question templates in multi hog waycombing both situation and commonsense (please seethe details of template types in Supplementary Materials).For one object / action, we find the connected edges inthe three graphs. Next, we design question templates thatjump from edges in Situated Knowledge Graph or Gen-eral Knowledge Graph to those in Situated CommonsenseKnowledge Graph. Concretely, for example, the templatecan be What would happen if the person did not use the<obj attribute> and <obj-obj relation>?(see one con-crete question example concerning cornstarch in Fig-ure 1). The model needs to first do spatiotemporal reason-ing to identify the object by <obj attribute> and <obj-objrelation> (e.g., white substance and dissolved in waterin glass). Next, the model needs to do situated common-sense reasoning to understand the consequence if that objectwas not used. In such way, we can comprehensively as-sess models situated commonsense reasoning ability. Notethat we also present some easier templates (e.g., vanilla objcounterfactual) which are single hog.Apart from the question and the correct answer, consis-tent with VCR , we generate wrong answers that arerelevant but sufficiently dissimilar from the correct answersto minimize shortcut risk (shown in ). Incorrectoptions are chosen from the same category of contextualknowledge related to other objects or actions. Also, we en-sure an even distribution of the four choices ((b)).",
  "Top-down QA Generation": "Bottom-up QA generation allows for control over questionquality and difficulty but may not create diverse questiontypes. To address this, we suggest a top-down approach forQA generation. We design a structured prompt with fiveelements: video content, integrated graph, QA examples,generation goal, output format. The integrated graph com-bines SKG, GKG, and SCKG. The generation goal directsthe LLM to create multi-hog questions based on multipleedges from the integrated graph. This prompt design keepsa strong connection between generated QAs and graphs, re-ducing the chance of model hallucination compared to bas-ing QAs only on video content.However, the top-down approach has drawbacks, andits resource-intensive and slow. With the three graphs in-volved, it takes a few seconds to generate one question usingthe top-down method, while the bottom-up method gener-ates the entire benchmark in the same time. In this paper,we mainly use the bottom-up method to create the bench-mark, but we discuss top-down generation case studies inthe Supplementary.",
  ". Data Source": "We created the benchmark based on videos of the two publicvideo datasets about daily activities.YouCook2 is a large set of instructional cooking videosfrom YouTube.It includes over 2,000 videos across 89recipe categories, with each video annotated with detailedstep-by-step instructions.HOMAGE contains 1,752 video clips featuring 75daily human activities in home environments. The dataset isthoroughly annotated with action labels, object information,and spatial-temporal relationships",
  ". Baseline Results": "To address the first question, we select a range of recentvideo generation models, including Video-LLaMa ,PandaGPT , Video-ChatGPT , AskAnything ,and Valley . For deployable models, we specificallychose their Vicuna-7B-based versions . Detailed base-line settings are described in the appendix.Baseline experiment settings.The baseline models aretested in two settings:multiple-choice and direct-answer. In the multiple-choice setting, models receive aquestion and four choice candidates; in the direct-answersetting, models are required to provide direct and succinctresponses to the questions. To calculate the accuracy in themultiple-choice setting, we need to parse an integer as thechoice taken by the model. If the parsing algorithm fails, wecalculate the BLEU score between model output and eachchoice candidate, picking the highest one as the models an-swer. Results are shown in . We measure the BLEUscore and BERT-F1 score between the modelsoutput and the generated answer in the direct-answer set-ting. Results are shown in and 4 respectively.Result Analysis. From the table, we have the followingobservations. First, all baseline models perform far fromperfect. GPT4v model has the best performance across dif-ferent settings, which is consistent with its superior perfor-mance on standard leaderboards like and . Thebest performing AskAnything (where ChatGPT is used as Singapore Curry Laksa ++ ? Correct Ans: Pour oil into a hot pan GPT4v: I'm sorry, but I can't provide ... action What did the person dodid something temporal before purpose to enhance the flavor and aroma of the curry laksa dishQues: the person enhance flavorPour oiladd chopped onions...Reasoningbeforepurpose .GPT4vs ability to perform complex combined spa-tiotemporal and situated commonsense reasoning is limited. Themodel needs to do two-hog reasoning, i.e., understanding the pur-pose of adding chopped onions is to enhance the flavor whileknowing the previous action is pouring oil. the language model) only has a BLEU score of 0.110 intable 3. It shows the value of the proposed benchmark toevaluate vision-language models capabilities to understandand reason in videos.Secondly, we notice a significant performance gap be-tween API models (the GPT family) and deployable mod-els (the LLaMa family with Vicuna-7B base model). Weremark that this phenomenon can be attributed to the dif-ference in both sizes and instruction-following abilities be-tween GPT LLM family and LLaMa LLM family.Third, we find that most models perform better on sim-pler question types (e.g., OCT and APU) compared to morecomplex ones (e.g., OCB+ST and OCT+ST). For simplerquestions, the choices might reveal the answers (e.g., Q:What did the person use to provide a sticky base to holdingredients for California Roll?, Choices: Rice, Av-ocado, Nori, and Cucumber). GPT4v may use thisto achieve high multi-choice accuracy on OCB. However,GPT4v might struggle when both spatiotemporal and situ-ated commonsense reasoning are required (see ).",
  ". Ablation Studies": "We answer the second question by conducting ablationstudies to evaluate the effectiveness of each componentwithin the generation method for the three graphs.MLLMs for SKG Generation.To justify the usage ofMLLMs which provide spatiotemporal information, we fo-cus on the four type QAs, namely Object Counterfactual,Object Counterfactual + Spatiotemporal, Object Contri-bution, and Object Contribution + Spatiotemporal. Inthe human evaluation, we ask whether the reasoning ques-tions require to observe the video content, with resultsshowing 91%, 99%, 94%, and 99% respectively. This con-firms that MLLMs are critical to generating high-quality sit-uated commonsense questions.Effectiveness of Prompt Design in GKG Generation. Weuse LLM to generate KGs in parsable JSON format. Here,we test the effectiveness of specifying output format andproviding examples in the prompt structure for GKG gen-eration. When given only the goal of the task, 0% of the . Accuracy of baseline models in multiple-choice setting. The question types are represented by numbers from 1 to 12: 1.Object Counterfactual + Spatiotemporal (OCT+ST); 2. Object Contribution + Spatiotemporal (OCB+ST); 3. Object Counterfactual +General Knowledge (OCT+GK); 4. Object Contribution + General Knowledge (OCB+GK); 5. Object Counterfactual (OCT); 6. ObjectContribution (OCB); 7. Object Inference (OI); 8. Action Purpose + Spatiotemporal (APU+ST); 9. Action Counterfactual + Spatiotemporal(ACT+ST); 10. Action Purpose (APU); 11. Action Counterfactual (ACT); 12. Action Possibility (APO).",
  "ModelsQuestions concerning objectsQuestions concerning actionsOverall": "blind ChatGPT 0.8870.8780.8920.8810.8840.8960.8870.8860.8830.8780.8920.8860.886GPT4v 0.9540.9530.9550.9590.9560.9550.9530.9630.9540.9570.9520.9570.956Video-LLaMa 0.9610.9600.9610.9610.9610.9610.9610.9610.9610.9610.9610.9610.961PandaGPT 0.9520.9530.9520.9520.9530.9520.9530.9520.9520.9520.9520.9520.952Ask Anything 0.9570.9570.9580.9560.9610.9600.9550.9620.9590.9640.9660.9590.959Video-ChatGPT 0.9550.9550.9570.9560.9570.9630.9520.9560.9580.9580.9610.9560.957Valley 0.9550.9550.9550.9550.9540.9540.9550.9550.9550.9550.9540.9550.955 outputs are valid; specifying output format, this figure in-creases to 11.1%; with both output format and examples,all outputs can be parsed as JSON files.Effectiveness of Few-Shot Self-Prompting. We argue thefew-shot examples of SCKG generation are necessary. Byhuman testing, we evaluate the proportion of concrete andspecific knowledge relations as opposed to un-situated, gen-eral knowledge. With Few-Shot Self-Prompting, 97% of thegenerated knowledge is concrete while the ratio decreasesto 61% without the prompt technique.",
  "Our research introduces a novel benchmark for SituatedOpen-World Commonsense Reasoning, advancing AIs": "ability to comprehend and reason in dynamic, real-worldcontexts. This benchmark includes a wide range of ques-tions and situational analyses that go beyond traditional rea-soning paradigms, challenging existing AI systems. Ournovel approach in dataset generation offers scalability andenhanced logic in QA pair creation. While current modelsshow promise, our experiment findings highlight the needfor significant improvements, pointing towards exciting av-enues for future general artificial intelligence.",
  "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-garet Mitchell, Dhruv Batra, C Lawrence Zitnick, andDevi Parikh.Vqa: Visual question answering.InICCV, 2015": "Tayfun Ates, M Samil Atesoglu, Cagatay Yigit, IlkerKesen, Mert Kobas, Erkut Erdem, Aykut Erdem, TilbeGoksun, and Deniz Yuret. Craft: A benchmark forcausal reasoning about forces and interactions. arXiv,2020. Sebastien Bubeck, Varun Chandrasekaran, Ronen El-dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, PeterLee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.Sparks of artificial general intelligence: Early exper-iments with gpt-4. arXiv preprint arXiv:2303.12712,2023.",
  "Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan Car-los Niebles.Action genome: Actions as composi-tions of spatio-temporal scene graphs. In CVPR, pages1023610247, 2020": "Justin Johnson, Bharath Hariharan, Laurens VanDer Maaten, Li Fei-Fei, C Lawrence Zitnick, and RossGirshick. Clevr: A diagnostic dataset for composi-tional language and elementary visual reasoning. InCVPR, 2017. JustinJohnson,BharathHariharan,LaurensVan Der Maaten, Judy Hoffman, Li Fei-Fei, CLawrence Zitnick, and Ross Girshick.Inferringand executing programs for visual reasoning.InProceedings of the IEEE International Conference onComputer Vision, pages 29892998, 2017.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, Ilya Sutskever, et al. Language modelsare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019": "Nishant Rai, Haofeng Chen, Jingwei Ji, Rishi De-sai, Kazuki Kozuka, Shun Ishizaka, Ehsan Adeli, andJuan Carlos Niebles. Home action genome: Cooper-ative compositional action understanding. In CVPR,pages 1118411193, 2021. Dustin Schwenk, Apoorv Khandelwal, ChristopherClark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answeringusing world knowledge. In European Conference onComputer Vision, pages 146162. Springer, 2022."
}