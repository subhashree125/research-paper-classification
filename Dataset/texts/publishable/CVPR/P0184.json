{
  "In this paper, we present our solution for SMART-101": "Challenge of CVPR Multi-modal Algorithmic Reasoning Task 2024. Unlike traditional visual questions and answer tasks, this challenge evaluates abstraction, deduction and generalization ability of neural network in solving visuo-linguistic puzzles designed for specially children in the 6-8 age group. Our model is based on two pre-trained models, dedicated to extract features from text and image respec-tively. To integrate the features from different modalities, we employed a fusion layer with attention mechanism. We explored different text and image pre-trained models, and fine-tune the integrated classifier on the SMART-101 da-taset. Experiment results show that under the data splitting style of puzzle split, our proposed integrated classifier achieves superior performance, verifying the effectiveness of multi-modal pre-trained representations.",
  "Multimodal visual question answering (VQA) task is an": "attractive research direction in the field of artificial intelli-gence in recent years. It integrates technologies from two subfields of computer vision and natural language pro-cessing, aiming at building an intelligent system that can understand the content of an image and answer questions based on it. Another important research direction is Multi-modal Algorithmic reasoning (MAR), which aims to guide models to solve complex logic problems based on image context. Due to the similarity of task types, MAR problems often appear in the form of VQA, and MAR tasks can be roughly summed up as a subset of VQA tasks.",
  "In the field of MAR research, there are already some": "public benchmarks, such as Image riddles, PororoQA, VLQA, and CLEVR. However, while the challenges in these prior works often seem diverse, they tend to be limited to a common setting and also to a specific domain of expertise, allowing existing neural network models to seize on their weaknesses and achieve high accuracy on these datasets. Therefore, the goal of SMART-101 is to understand the capabilities and shortcomings of SOTA deep models for visual language inference and provide op-timization directions for subsequent work. SMART-101 focuses on investigation the model generalizability to prob-lems that require a wide range of skills, and its data focuses on deeper logical questions, rather than simply answering common-sense math questions. As shown in , the problems in SMART-101 are not limited to simple, com-mon-sense questions, but requires specific mathematical methods (equations) and multi-hop reasoning.",
  "To address these challenges, we propose a classification": "model based on integration of multi-modal pre-trained models. Firstly, we propose a siamese architecture, with two encoders extracting features from text and vision mo-dalities respectively, and then integrated by a fusion layer to produce the classification result. Similar to the cross-at-tention described in , we map the features of the visual tower to the q vector and the features of the text tower to the k and v vectors in the fusion attention mechanism. A linear layer is used to align different representation spaces, and the fused features is fed to a pooling structure to obtain the classification results. Secondly, we explore the utilities of multi-modal pre-trained encoders, which have been pre-trained on massive multi-modal data and is adapted to rep-resent text and image input. Specifically, we explored the utilities of CLIP, SigLIP for vision representation, and BERT and DeBERTa for language representa-tion.",
  "CoT is usually used in multi-modal algorithm reasoning": "Multimodal-CoT incorporates language (text) and vi-sion (images) modalities into a two-stage framework that separates rationale generation and answer inference. DDCoT proposes a novel prompting that maintains a critical attitude through negative space prompting and in-corporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition ca-pability of visual models into the joint reasoning process. MC-CoT proposes a self-consistency training strategy that generates multiple rationales and answers, subse-quently selecting the most accurate through a voting pro-cess.",
  "BERT pre-trained on large-scale unsupervised text": "data and then fine-tuned on a specific task. By pre-training on both MLM (Masked Language Model) and NSP (Next Sentence Prediction) tasks, BERT can learn rich contextual information. BERT has achieved significant performance gains on tasks such as text classification, named entity recognition, question answering systems, and natural lan-guage inference. Compared with BERT, RoBERTa in-creases the amount of data training and training time. The NSP task was removed and the mask strategy of dynami-cally adjusting the Masked Language Model (MLM) is adopted. ALBERT factorizes the large word embed-ding matrix into the product of two smaller matrices, and the parameters are shared between all layers. Sentence Or-der Prediction (SOP) is used instead of NSP task. DeBERTa processes content and location information separately by introducing decoding enhancement and dis-entangled attention mechanism, which can better capture the relationship between words and improve the perfor-mance of the model.",
  "originally designed to process sequential data such as text, to the vision domain. ViT first cuts an image into a series": "of patches, which are then fed into a Transformer model as sequence data. CLIP is a multi-modal pre-trained model proposed by OpenAI that understands both images and text. CLIP is trained by contrastive learning of many images and corresponding description texts. SigLIP proposes a simple pairwise sigmoid loss algorithm for lan-guage-image pre-trained based on CLIP. Unlike standard contrastive learning with softmax normalization, the sig-moid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for nor-malization.",
  "The overall architecture of the model is shown in Figure": "3. The image data is passed through a vision encoder to obtain an image representation. Text data is passed through a text encoder to obtain a text representation. A linear layer is used to align the semantic space with the visual space. The two are input into the fusion module. The architecture of the fusion module is shown in . Fusion Module is a Transformer block. We map the features of the visual tower to the q vector and the features of the text tower to : Architecture of the text encoder and fusion module. The output of the text encoder goes through a linear layer to align the text features with the dimensions of the visual features. The fusion module is a multi-head attention mechanism. the k and v vectors in the fusion attention mechanism. The output of the fusion module goes through the pooling mod-ule to obtain the image-text joint feature representation for the classification task.",
  "CLIP is pre-trained by contrastive learning. We adopt": "the pre-training strategy of CLIP and fine-tune a CLIP-based image-text matching model on SMART-101 dataset as our baseline. Clip-vit-large-patch14 uses the training strategy of CLIP, and ViT-L-14 serves as the visual en-coder. We try to employ it as visual encoder of the archi-tecture shown in . ViT-SO400M-14-SigLIP-384 is a multi-modal pre-trained model trained on the WebLI dataset using the training strategy of SigLIP. It has excel-lent performance on many benchmarks. We try to apply its visual encoder and text encoder to the architecture shown in .",
  "BERT has achieved significant performance gains on": "multiple natural language processing tasks by pre-training on large-scale unsupervised text data through a bidirec-tional Transformer architecture and then fine-tuning on specific tasks. Based on BERT, DeBERTa-v3-large further improves the performance and generalization ability of the model by introducing disentangled attention mechanism, relative position encoding, improved pre-training tasks, and a larger model size. DeBERTa-v3-large has demon-strated stronger performance on multiple natural language processing tasks. We also try to employ them as the text encoder to the architecture shown in .",
  "SMART-101 has 101 root puzzles and 2000 image-text": "pairs are generated from each root puzzle individually. Un-der the data splitting mode of Puzzle Split (PS), the root puzzles are split into 77-3-21 (train-val-test). In this set-ting, a total of 154000 image-text pairs is used for training, 6000 image-text pairs for validation, and 42000 image-text pairs for training. An example of SMART-101 is shown in . From this SMART-101 input and output format, we can find that the accuracy rate is very appropriate for evaluating the performance of the model.",
  "The results of the different methods are shown in Table": "1. As described in 3, we experimented with different com-binations of visual encoders and text encoders. In addition, we experimented with two different alignments, semantic space to visual space alignment and visual space to seman-tic space alignment. We use a similar pooling approach to BERT. In particular, the method in the last row of , : The overall achitecture of the model. The image data and text data are passed through the image encoder and text encoder respectively,and the output is the passed through the fusion module to obtain fusion features for classification tasks.",
  "Four conclusions can be drawn from the experimental": "results. Firstly, the task is difficult. The various methods used in this paper only achieve the accuracy close to ran-dom selection. Secondly, using a stronger pre-trained model can improve accuracy. SigLIP outperforms CLIP. DeBERTa outperforms BERT. Thirdly, text-only pre-trained models such as DeBERTa perform better compared to pre-trained text encoders such as SigLIP.t. Finally, the matching method based on CLIP does not perform well on such task that requires a deep understanding, while the method based on classification performs better on it.",
  "SigLIP.v DeBERTa T_to_I Attn-pool 26.14 28": ": Accuracy of different methods on local test set (L_acc) and remote private test set (R_acc). Align refers to the alignment of visual space and semantic space. T_to_I refer to the mapping from semantic space to visual space. CLIP.v refers to the visual encoder of the CLIP model, and CLIP.t refers to the text encoder of the CLIP model. DeBERTa refers to the DeBERTa-v3-large. SigLIP refers to the model ViT-SO400M-14-SigLIP-384."
}