{
  "Abstract": "Artificial Intelligence (AI) has driven innovations and cre-ated new opportunities across various sectors. However,leveraging domain-specific knowledge often requires auto-mated tools to design and configure models effectively. Inthe case of Deep Neural Networks (DNNs), researchers andpractitioners usually resort to Neural Architecture Search(NAS) approaches, which are resource- and time-intensive,requiring the training and evaluation of numerous candi-date architectures. This raises sustainability concerns, par-ticularly due to the high energy demands involved, creatinga paradox: the pursuit of the most effective model can un-dermine sustainability goals. To mitigate this issue, zero-cost proxies have emerged as a promising alternative. Theseproxies estimate a models performance without the need forfull training, offering a more efficient approach. This pa-per addresses the challenges of model evaluation by auto-matically designing zero-cost proxies to assess DNNs effi-ciently. Our method begins with a randomly generated setof zero-cost proxies, which are evolved and tested using theNATS-Bench benchmark. We assess the proxies effective-ness using both randomly sampled and stratified subsets ofthe search space, ensuring they can differentiate betweenlow- and high-performing networks and enhance general-izability. Results show our method outperforms existingapproaches on the stratified sampling strategy, achievingstrong correlations with ground truth performance, includ-ing a Kendall correlation of 0.89 on CIFAR-10 and 0.77 onCIFAR-100 with NATS-Bench-SSS and a Kendall corre-lation of 0.78 on CIFAR-10 and 0.71 on CIFAR-100 withNATS-Bench-TSS.",
  ". Introduction": "The impact that AI has had on human lives and soci-ety spans various domains, from advances in health-care diagnosis to optimization of trade routes ,identification of diseases in plants , and cyber in- trusion detection .This technology allows us toimprove most fields of study.However, these ad-vancements have a cost. Several issues affect AI sys-tems: racial and gender bias in automated job andloan applications , misclassification in critical sce-narios due to data manipulation by bad actors , jobdisplacement , and massive energy consump-tion . The latter is quantifiable and has grown ex-ponentially in recent years. For instance, the popu-lar GPT-3 model, similar to the models behind Ope-nAIs ChatGPT, used 1287 MWh in 15 days just forits training . The current arms race to developLarge Language Models (LLMs) models with more ca-pabilities will likely increase the number of parame-ters, thereby ensuring that more recent models willconsume even more energy. Moreover, serving mil-lions of users on a daily basis requires a tremendousquantity of processing devices, such as GPUs and/orTPUs, which use a massive amount of energy. The to-tal energy consumption of NVIDIA GPUs, the leadingmanufacturer of these devices, is expected to surpassthe total energy needs of countries such as Belgium orSwitzerland . Furthermore, technological com-panies like Google and Microsoft, which have previ-ously committed to offsetting their carbon emissions,have struggled to maintain these promises due to allo-cating more resources to AI data centers . Thisgrowth is expected to continue, with hardly any re-strictions, due to the positive impact this technologyhas on the population and the economy . Several techniques have been proposed to addressthe energy consumption problem of AI, particularlyof Machine Learning (ML). Reducing the floating-point precision of a models weights has been shownto enhance energy efficiency, albeit at the tradeoff ofsome performance .Spiking Neural Networks(SNNs) show promising results due to their sparsityand event-based operation, though they require fur-ther hardware-side developments to fulfill their theo-retical energy efficiency potential . Using NAS",
  "arXiv:2411.15290v1 [cs.LG] 22 Nov 2024": "or Neuroevolution (NE) to search for energy-efficientDNN models has also significantly increased energyefficiency . This approach, however, requires sub-stantial resources since each DNN must undergo a fulltraining process and subsequent evaluation, whichmay ultimately undermine its potential for minimalenergy usage.In this paper, we present an approach to miti-gate the energy consumption problem of NAS. Zero-cost proxies estimate a DNNs performance withouttraining, with some recent proxies having achievedhigh correlations with the ground truth, reporting aKendall correlation of 0.706 on the NATS-Bench-TSSsearch space with the CIFAR-10 dataset , or even0.741 on the same problem when using an ensembleof four zero-cost proxies .This means that thescore given by the zero-cost proxy to a DNN alignswell with its actual performance in terms of accuracy.Our main contribution is the proposal of an al-gorithm that automatically generates and optimizeszero-cost proxies using evolution1.Specifically, webegin with a randomly generated set of proxies thatare then modified or evolved over several generationsto gradually improve their estimation of model per-formance. These proxies are evaluated on the NATS-Bench benchmark across the CIFAR-10, CIFAR-100,and ImageNet16-120 datasets.We assess their per-formance in two ways: 1) on a subset that is a ran-domly selected sample of the overall search space; and2) on a stratified subset with a diverse distribution ofnetworks to evaluate the proxies not only on goodnetworks but also on those with lower performance.This approach is intended to ensure that the proxiescan differentiate between low- and high-performingDNNs, minimizing overfitting and enhancing gener-alizability.Experimental results indicate that this approachproduced better-performing solutions than similarmethods in the literature. We achieved higher correla-tions on nearly all datasets using a stratification tech-nique to sample networks. Specifically, we obtaineda Kendall correlation of 0.89 on CIFAR-10 and 0.77 onCIFAR-100 with NATS-Bench-SSS and a Kendall cor-relation of 0.78 on CIFAR-10 and 0.71 on CIFAR-100with NATS-Bench-TSS.This paper is structured as follows. presents the necessary background on EvolutionaryAlgorithms (EAs), focusing on Genetic Programming(GP), and on zero-cost proxies as well as related work. showcases the functioning of the developedalgorithm and details the experimental setup. After-ward, present the obtained results and a",
  ". Genetic Programming": "GP is an AI technique that allows for the automaticdesign of programs, expressions, or models with vari-able length, using the principles of natural selectionand evolution () . When using GP, oneneeds to specify the set of variables or constants andthe set of functions. One of the most commonly usedrepresentations of solutions is syntax trees.The initial set of solutions, called population, isusually created randomly. Some trees are generatedwith the maximum depth, others have branches withvarying depths, and a mix of both approaches is alsoused.Individuals are evaluated based on a fitness func-tion that measures their performance on a given prob-lem. The fittest individuals are then probabilisticallyselected to produce new solutions, or offspring, forthe next generation, allowing successful traits to bepassed along. Offspring undergo mutation to intro-duce genetic diversity, helping the population explorea broader range of potential solutions.In the context of GP, mutation is performed by re-placing a sub-tree rooted at a randomly selected nodewith a newly generated sub-tree, introducing novelstructures. Recombination occurs by swapping sub-trees rooted at randomly selected nodes between twoindividuals, enabling the exchange of genetic materialand potentially beneficial traits.",
  "Grammatical Evolution": "Grammatical Evolution (GE) is a grammar-based GPthat uses a variable-length integer representation .It relies on the production rules of a Context-freeGrammar (CFG) to generate solutions. A CFG is de-fined by a tuple (N, T, P, S) with N being the set ofnon-terminals, T the set of terminals, P the set of pro-duction rules, and S N is a start symbol.The solution generation process from an integersequence is carried out as follows: the sequence isread from left to right, beginning with S, and a pro-duction rule is iteratively applied to expand the left-most non-terminal symbol. The current items valuein the sequence determines the production rule to useby calculating its modulo with respect to the numberof available expansions for the leftmost non-terminalsymbol.",
  ". Simplified visualization of the flow of an EA": "ConsiderthegrammarinwithN={<expr>, <op>, <trig>, <var>},T={+, , sin, cos, x, y, (, )} and S = <expr>.Con-sidering a genotype with integer values ranging from0 to 63, the mapping process is as follows ().First, we begin with the start symbol, <expr>, andread the first element of the integer vector.Sincethere are three options on how to expand the startsymbol, we have 46 mod 3 =1; thus, the startsymbol is replaced by its second expansion rule,<trig>(<expr>). Following this, the second valuein the genotype is read, and we expand the leftmostnon-terminal symbol, repeating the process.Thisis executed until there are no more non-terminalsymbols. It is possible to use a wrapping mechanisminstead of terminating the mapping process if thereare no more integers to read from the genotype, thusreusing the genotype values.",
  ". Example of Grammatical Evolution mapping": "with a specific non-terminal and using variable-lengthinteger lists to represent expansion choices. This struc-ture ensures that changing one gene does not affectthe derivation choices for other non-terminals, whichreduces phenotypic changes and thereby improves lo-cality. Furthermore, the values in each list are con-strained by the number of expansion options avail-able for the corresponding non-terminal, eliminatingthe need for modulo operations and reducing redun-dancy. Empirical results demonstrate that SGE out-performs GE across various optimization problems.",
  ". Zero-cost proxies": "Zero-Shot, also known as the training-free method, isa technique that allows the prediction of the qualityof a model without training it . This is achievedthrough proxies, which are algorithms or mathemat-ical formulas that estimate how good a model mightbe. These proxies allow us to assess the performanceof a DNN model without training it, thus saving re-sources. Zero-cost proxies are a relatively recent re-search thread in the ML community since they wereintroduced in 2018 by Camero et al. and havesince been continuously improved. Traditional NASmethods typically require hundreds or thousands ofGPU hours and, as such, can significantly benefit fromusing training-free methods. Zero-cost proxies allowus to predict the performance of a DNN model with-out training it, and they usually require only a smallamount of GPU time or even CPU time to do so.Despite numerous statistical analyses of training-freeNAS algorithms, a theoretical analysis of training-freealgorithms remains lacking. A comprehensive theo-retical examination of this score function is essentialto further research in this field .To avoid training many networks to evaluate thecorrelation between the proxy and the actual accuracy,well-established NAS datasets are used . Amongother metrics, these datasets contain the architecturesof many networks and their corresponding accuracies.A pioneer in the field, NASWOT is an algorithmthat generates scores reflecting a models test accu- racy without requiring training.It computes thesescores based on the networks activation patterns inresponse to a single mini-batch .Synflow is apruning algorithm that aims to prevent the layer col-lapse problem when pruning a neural network.Itwas extended as a data-independent estimator of anetworks performance without training it . In-spired by pruning-at-initialization, GradNorm is aproxy metric that computes the sum of the Euclideannorms of the gradients after passing a single mini-batch of training data .TE-NAS is a frameworkthat evaluates network architectures by examining thespectrum of the neural tangent kernel and the num-ber of linear regions in the input space . Zen-NASis a zero-shot method that measures the expressivityof a DNN by computing its Zen-Score, which is de-rived from a few forward inferences on randomly ini-tialized networks with random Gaussian inputs .ZiCo demonstrates that high-performance DNNs tendto possess high absolute mean values and low stan-dard deviation values for the gradient and uses thatinformation to estimate the performance of a network.EZNAS proposes a GP approach to automate thediscovery of zero-cost proxies for NAS scoring byusing the DEAP framework . Each individualsfitness is determined by the minimum Kendall co-efficient it achieves across both search spaces. Afterthe evolutionary process, the fittest individual is eval-uated on 4000 random networks from the same twosearch spaces. While EZNASs results are competi-tive with the current state of the art, the authors donot provide details on how the other zero-cost prox-ies were evaluated, making direct comparisons poten-tially unfair. Furthermore, EZNAS uses statistics fromonly a portion of the overall layers of the models andemploys a relatively low recombination rate of 40%,which may not be sufficient to generate optimal indi-viduals consistently.",
  ". Benchmarks": "Benchmarking NAS algorithms is challenging due tovariations in data preprocessing, evaluation pipelines,and even random seeds. Moreover, fully training alarge number of networks requires extensive compu-tational resources, thus making reproducibility diffi-cult for most researchers. To address this issue, somedatasets were proposed to standardize the bench-marking process, providing a common ground forcomparisons and reducing the required computa-tional resources by delivering the results that would otherwise require the complete training of many DNNmodels.These benchmarks typically include not only thenetwork configuration and its accuracy metric aftertraining but also other data such as latency, number ofparameters, and more, enabling a quicker assessmentof the proposed NAS method.NAS-Bench-201 features full graph cells, allowingfor a more comprehensive search space, though lim-ited to four nodes and five associated operation op-tions to maintain manageability .Each archi-tecture is trained and evaluated on the CIFAR-10,CIFAR-100, and ImageNet-16-120 image classificationdatasets. It contains 15,625 architectures. NATS-Benchis a unified benchmark for searching for architecturetopology and size . It includes 15,625 candidatesfor the architecture topology (TSS) and 32,768 for ar-chitecture size (SSS). It also presents results for CIFAR-10, CIFAR-100, and ImageNet-16-120. Although notexplicitly mentioned in the paper, the authors notein the projects Github repository2 that the topologysearch space is equivalent to NAS-Bench-201.",
  ". Experimental Setup": "The experiments were conducted on a machine run-ning Ubuntu 22.04.3 LTS with two Intel Xeon Silver4310 CPUs with a clock frequency of 2.10GHz and 12cores each, 256 GB of RAM, and three NVIDIA RTXA6000 GPUs with 48 GB of GDDR6 RAM. The en-vironment in which they were executed used CUDA12.1, Python 3.10, and PyTorch 2.3.1.",
  "Networks Sampling": "After analyzing the search spaces (), we observeda skew towards networks with high accuracy acrossboth search spaces, regardless of the dataset. Conse-quently, random sampling does not ensure adequatesearch space coverage, as it may produce a sampleset that is too constrained or biased toward high-performance networks. This bias can result in prox-ies that fail to distinguish effectively between low-and high-performing networks. To address this issue,we stratified the search space based on test accuracy,creating five groups or bins of networks according totheir performance. This approach ensures a diverseset of samples for evaluation. Additionally, we sample20 networks from each dataset for each search spaceusing this stratification method to maintain a balancedrepresentation across different performance bins.",
  "Zero-Cost Proxies Evaluation": "We assess the usefulness of the zero-cost proxiesusing search spaces that comprise architectures ofDNNs and their respective test accuracy after train-ing. Specifically, we use the NATS-Bench benchmark. The evaluation uses the zero-cost proxy to scoreevery network sampled. Then we calculate Kendallscorrelation () between the set of scores (S) and theactual test accuracy (A). The measures the ordinalassociation between the two sets according to the fol-lowing:Let S = {s1, s2, . . . , sn} represent the scores from azero-cost proxy, and A = {a1, a2, . . . , an} represent thetest accuracies.The Kendall tau correlation coefficient between Sand A is given by:",
  "T is the number of ties in S, i.e., pairs (si, sj) suchthat si = sj,": "U is the number of ties in A, i.e., pairs (ai, aj) suchthat ai = aj.This metric indicates the zero-cost proxys suitabil-ity for predicting model performance, reflecting thelikelihood that a proxy score correlates with the actualmodel accuracy.The evaluation function is defined as the sum of theabsolute values of the Kendall rank correlation coef-ficients across all search space and dataset combina-tions. We use absolute values rather than signed onesto detect any correlation, not solely a positive one.",
  "Feature Extraction": "We extract 20 features from each layer of a network.When applicable, these are the weights and gradientsof the layer and the weights before and after a forwardpass or a backward pass takes place. At first, we con-sider the randomly initialized network and archiveeach layers weights and gradients. After this, we re-peat the complete extraction process in three modes:one where we pass a batch of random data on the net-work, another where we pass a batch from the dataset,and, last, a mode where we pass a batch from thedataset but perturbed with noise.Having this archive of network statistics, we theniterate over each of the networks layers and computethe current individuals formula. The final score at-tributed to the network is the mean value of the scoreof all layers.",
  "Operations": "The available operations range from essential mathe-matical functions like addition, subtraction, element-wise product, and matrix multiplication to specializedcomputations such as Frobenius norm and eigenvalueratios. We also include activation functions like ReLUand sigmoid and normalization techniques. Addition-ally, we provide methods for noise addition, cosinesimilarity, and logical comparisons. The complete listof operations is presented as supplementary material.To handle feature extraction from every layer, weensure that matrices of any dimension are compatiblefor operations. We do this by flattening the matrices,comparing their lengths, and padding the shorter onewith a constant (1). This gives both matrices the samenumber of elements, allowing them to be reshaped",
  "Search Algorithm": "To perform the search for zero-cost proxies, we usedthe grammar-based GP approach described in ,with the parameters detailed in Tab. 1. Specifically,we performed five runs, each for 100 generations, witha population size of 100 individuals. To preserve thebest solutions, an elitism size of 10 was applied. Selec-tion was based on tournament selection with a tourna-ment size of 5. The genetic operators were configuredwith a crossover rate of 90% and a mutation rate of50%. The evolutionary trees had depths ranging from5 to 12, and each experiment evaluated 120 networks. lists the used experimental parameters.",
  ". Results and Discussion": "shows the evolution of zero-cost proxies per-formance, measured by the Kendall correlation coef-ficient (), over 100 iterations across NATS-Benchstwo search spaces on the CIFAR-10, CIFAR-100, andImageNet16-120 datasets, averaged over five runs. Inthe figure, dashed lines represent performance on theTopology Search Space (TSS), while solid lines indi-cate performance on the Size Search Space (SSS). Thethick solid line illustrates the fitness of the individu-als, defined as the sum of the absolute values of the scores across the datasets.Examining the results in ,we observethat zero-cost performance on the CIFAR-10 andImageNet16-120 datasets in the TSS (dashed lines) islower than in the SSS (solid lines). For CIFAR-100, the difference is minimal. This suggests that identi-fying zero-cost proxies in the TSS is more challeng-ing than in the SSS. This difficulty can be attributedto the typical correlation between network size andperformance . A proxy that leverages the num-ber of network parameters to estimate performancehas a better chance of accurately predicting perfor-mance in the SSS. In contrast, extracting performance-related features based on network topology is inher-ently more complex, which is required to estimate theperformance on the TSS.Additionally, as shown in , the distributionof network performance differs between the searchspaces.In the TSS, network performance rangeswidely from about 10% (close to random choice) tonearly 95%, while in the SSS, performance is concen-trated within narrower accuracy ranges.Finally, slight decreases in the solid and dashedlines can be observed, particularly in the correlationsmeasured in the SSS search space. Since the quality ofthe proxies is defined as the sum of the correlationsacross both search spaces, this behavior is expected, astradeoffs are made to improve correlation in the othersearch space.",
  ". Evolution of the Kendall correlation coefficient onNATS-Benchs two search spaces and the fitness metric over100 generations, averaged over 5 runs": "Validating the Zero-Cost ProxiesTo assess the gen-eralization ability of the zero-cost proxies, we eval-uate them on networks that differ from those usedin training. Specifically, we test the proxies on 4,500new networks. We evaluate 30 sets of 150 networksfrom each search space and dataset, using stratifiedand non-stratified sampling strategies. This approachensures that our evaluation covers a diverse range of max softmaxequal pass_perturbation (fwd_output) pass_perturbation (grad) element wise product pass_perturbation (bwd_input) pass (bwd_input)",
  ". Representation of the GreenMachine-2 solution,where ellipses represent functions and rectangles denote ter-minal symbols (variables)": "architectures, allowing us better to assess the general-ization and robustness of the proxies. With 30 sets ofnetworks, we report the mean and standard deviationacross these sets.We implemented state-of-the-art zero-cost proxiesfrom the literature and applied them to the same setof networks to ensure a fair comparison. Addition-ally, we included a random proxy that generates a ran-dom score between 0 and 1 for comparison purposes. depicts the GreenMachine-2 solution, and theformulas for the remaining best solutions discoveredby our approach are provided as supplementary ma-terial.Tables 3 and 4 present the validation results for thetwo sampling strategies using the correlation coeffi-cient. The corresponding results for Spearmans rankcorrelation are available as supplementary material.In Tab. 3, where networks are randomly sampledwithout considering performance representativity, weobserve no significant differences among the zero-costproxies. However, one zero-cost proxy discovered byour approach, GreenMachine-3 (GM-3), outperformsthe others on CIFAR-100 when applied to the SSS.This result can be explained by the fact that the ran-domly sampled set of networks shows very similarperformances, as indicated in Tab. 2. As mentionedpreviously, if network sampling is not done carefully,the resulting set can consist of models with very sim-ilar performance. Observing the standard deviationof the non-stratified set of networks, we see that it islow, indicating high similarity in performance. Thisreduces the need for proxies to make clear distinc-tions. However, when evaluating CIFAR-100 on the",
  "TSS49.28 25.1531.15 17.2119.04 10.96": "Concerning the validation where we used a strat-ified sampling strategy, the zero-cost proxies discov-ered by our approach can clearly distinguish betweenlow-performing and high-performing networks (seeTab. 3).Our proxies achieve the highest correla-tion across the entire SSS search space, and, on the TSSsearch space, our solutions surpass those in the litera-ture in all cases except for the ImageNet16-120 dataset.This exception can be explained by the distribution ofnetwork performances: as shown in Tab. 2, the TSSImageNet16-120 stratified version has a low standarddeviation, indicating that network performances arevery similar in this sample set, and proxies are not re-quired to distinguish between low and high perform-ers.",
  ". Conclusion": "This paper introduces GreenMachine, an algorithmthat automatically designs zero-cost proxies usingan evolutionary approach.We evaluate the effec-tiveness of the discovered proxies using the NATS-Bench benchmark across CIFAR-10, CIFAR-100, andImageNet16-120 datasets.The solutions discovered by our proposed ap-proach perform better than existing zero-cost proxymethods when distinguishing between low- and high-performing networks. To assess this, we apply strat-ified sampling to the search space, dividing it intogroups based on test accuracy to ensure a represen-tative set of samples for evaluating the proxies. Weuse the Kendall correlation coefficient to measure thecorrelation between the proxy scores and network ac-curacy. . Comparison of Zero-Cost proxies on the NATS-Bench benchmark across the CIFAR-10, CIFAR-100, and ImageNet16-120 datasets on the non-stratified subset. The presented values are the mean absolute Kendall correlation coefficient over30 runs, multiplied by 100. Bold denotes the best value.",
  "In this work, we evolved and tested the generated so-lutions on only two benchmarks. Using other searchspaces and datasets could enhance the generalizabil-": "ity of our results, allowing for a better assessment ofthe proxys effectiveness across multiple problem do-mains.Allied with evaluating the individuals on moresearch spaces, it might be relevant to experiment us-ing more specialized selection operators, such as lexi-case selection , to promote solutions that performwell across a diverse range of tasks.One potential enhancement to the evolutionary al-gorithm involves using ephemeral constants, whichcould fine-tune the relative importance of featureswithin the solutions, improving the quality of theproxies. Mohamed S. Abdelfattah, Abhinav Mehrotra, LukaszDudziak, and Nicholas Donald Lane. Zero-cost proxiesfor lightweight NAS. In 9th International Conference onLearning Representations, ICLR 2021, Virtual Event, Aus-tria, May 3-7, 2021. OpenReview.net, 2021. 4, 8, 13 Yash Akhauri, Juan Pablo Munoz, Nilesh Jain, and RaviIyer. EZNAS: evolving zero-cost proxies for neural ar-chitecture scoring.In Advances in Neural InformationProcessing Systems 35: Annual Conference on Neural In-formation Processing Systems 2022, NeurIPS 2022, NewOrleans, LA, USA, November 28 - December 9, 2022, 2022.4, 8, 13 Anwar Abdullah Alatawi, Shahd Maadi Alomani,Najd Ibrahim Alhawiti, and Muhammad Ayaz. Plantdisease detection using ai based vgg-16 model. Interna-tional Journal of Advanced Computer Science and Applica-tions, 2022. 1 S. A. Alowais, S. S. Alghamdi, N. Alsuhebany, T.Alqahtani, A. I. Alshaya, S. N. Almohareb, A. Al-dairem, M. Alrashed, K. Bin Saleh, H. A. Badreldin,M. S. Al Yami, S. Al Harbi, and A. M. Albekairy. Revo-lutionizing healthcare: the role of artificial intelligencein clinical practice. BMC Medical Education, 23(1):689,2023. 1",
  "Andres Camero, Jamal Toutouh, and Enrique Alba.Low-cost recurrent neural network expected perfor-mance evaluation. CoRR, abs/1805.07159, 2018. 3": "Wuyang Chen, Xinyu Gong, and Zhangyang Wang.Neural architecture search on imagenet in four GPUhours: A theoretically inspired perspective. In 9th In-ternational Conference on Learning Representations, ICLR2021, Virtual Event, Austria, May 3-7, 2021. OpenRe-view.net, 2021. 4, 8, 13 GabrielCortes,NunoLourenco,andPenousalMachado.Towards physical plausibility in neu-roevolution systems.In Applications of EvolutionaryComputation - 27th European Conference, EvoApplica-tions 2024, Held as Part of EvoStar 2024, Aberystwyth,UK, April 3-5, 2024, Proceedings, Part II, pages 7690.Springer, 2024. 2",
  "Google.2024 Environmental Report.https ://sustainability.google/reports/google-2024-environmental-report/, 2024.[Accessed05-10-2024]. 1": "MuhammadHamisHaider,HaoZhang,S.Deivalaskhmi, G. Lakshmi Narayanan, and Seok-BumKo. Is Neuromorphic Computing the Key to Power-EfficientNeural Networks: A Survey, pages 91113.SpringerNature Switzerland, Cham, 2024. 1 Joel Hestness, Sharan Narang, Newsha Ardalani, Gre-gory F. Diamos, Heewoo Jun, Hassan Kianinejad, Md.Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou.Deep learning scaling is predictable, empirically. CoRR,abs/1712.00409, 2017. 6",
  "Adheesh Kadiresan, Yuvraj Baweja, and Obi Og-banufe. Bias in AI-Based Decision-Making, pages 275285. Springer International Publishing, Cham, 2022. 1": "Junghyup Lee and Bumsub Ham.AZ-NAS: assem-bling zero-cost proxies for network architecture search.In IEEE/CVF Conference on Computer Vision and PatternRecognition, CVPR 2024, Seattle, WA, USA, June 16-22,2024, pages 58935903. IEEE, 2024. 2 Guihong Li, Yuedong Yang, Kartikeya Bhardwaj, andRadu Marculescu. Zico: Zero-shot NAS via inverse co-efficient of variation on gradients. In The Eleventh In-ternational Conference on Learning Representations, ICLR2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net,2023. 4, 8, 13 Ming Lin, Pichao Wang, Zhenhong Sun, Hesen Chen,Xiuyu Sun, Qi Qian, Hao Li, and Rong Jin. Zen-nas:A zero-shot NAS for high-performance image recogni-tion. In 2021 IEEE/CVF International Conference on Com-puter Vision, ICCV 2021, Montreal, QC, Canada, October10-17, 2021, pages 337346. IEEE, 2021. 4, 8, 13 Nuno Lourenco, Francisco B. Pereira, and ErnestoCosta. SGE: A structured representation for grammat-ical evolution. In Artificial Evolution - 12th InternationalConference, Evolution Artificielle, EA 2015, Lyon, France,October 26-28, 2015. Revised Selected Papers, pages 136148. Springer, 2015. 3 Nuno Lourenco, Filipe Assuncao, Francisco B. Pereira,Ernesto Costa, and Penousal Machado.Structuredgrammatical evolution: A dynamic approach. In Hand-book of Grammatical Evolution, pages 137161. Springer,2018. 3, 6",
  "Wolfgang Maass. Networks of spiking neurons: Thethird generation of neural network models. Neural Net-works, 10(9):16591671, 1997. 1": "Joe Mellor, Jack Turner, Amos Storkey, and Elliot JCrowley. Neural architecture search without training.In Proceedings of the 38th International Conference on Ma-chine Learning, pages 75887598. PMLR, 2021. 4, 8, 13 David A. Patterson, Joseph Gonzalez, Urs Holzle,Quoc V. Le, Chen Liang, Lluis-Miquel Munguia, DanielRothchild, David R. So, Maud Texier, and Jeff Dean.The carbon footprint of machine learning training willplateau, then shrink. Computer, 55(7):1828, 2022. 1 Miguel A. Ramirez, Song-Kyoo Kim, Hussam M. N. AlHamadi, Ernesto Damiani, Young-Ji Byon, Tae-YeonKim, Chung-Suk Cho, and Chan Yeob Yeun. Poisoningattacks and defenses on artificial intelligence: A survey.CoRR, abs/2202.10276, 2022. 1",
  "Dan Robinson. Microsofts carbon emissions up nearly30 carbon- emissions- up- nearly-30-thanks-to-ai/ar-BB1mvgao, 2024. [Accessed05-10-2024]. 1": "Babak Rokh, Ali Azarpeyvand, and Alireza Khantey-moori. A comprehensive survey on model quantiza-tion for deep neural networks in image classification.ACM Trans. Intell. Syst. Technol., 14(6), 2023. 1 Conor Ryan, JJ Collins, and Michael O. Neill. Gram-matical evolution: Evolving programs for an arbitrarylanguage. In Genetic Programming, pages 8396, Berlin,Heidelberg, 1998. Springer Berlin Heidelberg. 2 Lee Spector. Assessment of problem modality by differ-ential performance of lexicase selection in genetic pro-gramming: a preliminary report. In Genetic and Evo-lutionary Computation Conference, GECCO 12, Philadel-phia, PA, USA, July 7-11, 2012, Companion Material Pro-ceedings, pages 401408. ACM, 2012. 9"
}