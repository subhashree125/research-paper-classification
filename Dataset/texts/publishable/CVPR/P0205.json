{
  "Abstract": "Visual prompting infuses visual information into the input image to adapt modelstoward specific predictions and tasks. Recently, manually crafted markers such asred circles are shown to guide the model to attend to a target region on the image.However, these markers only work on models trained with data containing thosemarkers. Moreover, finding these prompts requires guesswork or prior knowledgeof the domain on which the model is trained. This work circumvents manual designconstraints by proposing to learn the visual prompts for guiding the attention ofvision transformers. The learned visual prompt, added to any input image wouldredirect the attention of the pre-trained vision transformer to its spatial locationon the image. Specifically, the prompt is learned in a self-supervised mannerwithout requiring annotations and without fine-tuning the vision transformer. Ourexperiments demonstrate the effectiveness of the proposed optimization-basedvisual prompting strategy across various pre-trained vision encoders.",
  "Introduction": "The attention mechanism in vision transformers leverages contextually relevant visual infor-mation from the entire input image to form embeddings for each token. This process dynamicallyadjusts the representation of each token, allowing the model to focus selectively on relevant areas,and integrates this context into the embeddings. Such context-aware embeddings lead to enhancedrecognition of complex visual data . In our study, we explore whether it is possible to direct theattention mechanism toward a specific input region by introducing and constructing a marker forpre-trained vision transformer models. We pursue this investigation to better understand what kind ofvisual information directs the attention mechanism of various pre-trained vision transformers and toexplore its potential for visual prompting. Visual prompting refers to approaches that infuse visual information to the input images toadapt vision foundation models to new tasks. Recent works introduce manual prompting techniques,such as placing colored circle or square markers on the image, or blurring regions surrounding",
  "arXiv:2406.03303v1 [cs.CV] 5 Jun 2024": ": Learned Prompt for CLIPs , SigLIP , DeiT , DINOv2 . In ourframework, we learn a prompt to draw the attention of the model to a specific point where the promptis applied. The prompt is optimized for each vision encoder model specifically to generalize acrossdifferent images. The depicted image is taken from COCO . the prompted location , and show they are effective in adapting CLIP vision encoder forfine-grained recognition tasks. These works hypothesize that these manual prompting techniquesguide the attention of the model and therefore help the model with fine-grained recognition tasks.However, using these manually engineering prompts rely on prior knowledge of biases (or emergentproperties) formed in the model during training (It is reported that training data of CLIP containsthese markers and blur effects ). In this study, we propose learning prompts (without fine-tuning the model) to guide the attentionof various large models rather than manually designing them. We explore how we can find visualprompts that attract the attention of a pre-trained (and frozen) vision transformer (ViT ) through aself-supervised approach. Thus, we would not rely on prior knowledge regarding dataset biases, andwe can find prompts for any vision transformer trained on any dataset. The optimization procedurealleviates the need for manual engineering or intuition to find the prompt. Drawing inspirationfrom universal adversarial patches , and employing techniques to improve transferability such asutilizing a network prior to generate adversarial perturbations , we propose a self-supervisedprompt optimization pipeline. In addition to CLIP, we explore other ViT networks, such as DeiT and DINOv2 , each of which follows different training principles than CLIP (e.g. CLIPuses language supervision, while DINOv2 uses self-supervision). Identifying visual prompts forDINOv2 is particularly important because it extracts significantly richer visual features from imagesand is recently emerging in the new generation of vision-language models due to CLIPsvisual limitations . We observe that manual markers, like a red circle, do not effectively guideDINOv2s attention, highlighting the need for an optimization-based approach for finding visualprompts.",
  "Related Work": "Visual Prompting Prompting has been extensively studied in the NLP community . Recently,researchers have begun exploring the benefits of prompting in image recognition as well. This involvesadding a learnable modification to images to guide models towards specific predictions . These prompt-tuning methods optimize a visual prompt that is appended and fixedto an input image. For instance, this could involve appending optimizable pixel regions around theimage. By doing so, these methods enable the pre-trained model to adapt to new tasks without theneed for retraining. This modification is typically applied universally across different images. Followup approaches have been proposed to improve these visual prompts . More recently,researchers have shown that manually crafted prompts, like a red circle, can effectively guide theattention of models like CLIP when trained on datasets containing similar markers . Moreover,other biases, such as blurring, can be used for prompting . However, such attention-guidingprompts only work effectively on certain models. In this study, we propose learning prompts (withoutfine-tuning the model) to guide the attention of various models rather than manually designing them. Adversarial Patch The seminal work demonstrates that adversarial examples can be generatedby altering just a few pixels in the input. This white-box attack method candeceive models by targeting a very small area. Additionally, certain studies have successfullydeveloped universal, transferable, and targeted adversarial patches. These patches are typically placed : Overview of Self-supervised Prompt Optimization Framework: For a given image anda random position for the patch, the patch prior (a random noise) is first passed through a neural prior(an auto-encoder neural network). A desired mask is then applied to the patch to only partially coverthe image and avoid losing data. The prompt is positioned on the target location and, after passingthrough the frozen Vision Encoder, the attention weights are extracted. The desired attention targetvalues are calculated with a Gaussian distribution centered at the token corresponding to the targetlocation. During training, the framework learns a prompt that minimizes the Kullback-Leibler (KL)divergence loss between the attention values of the CLS token and the target distribution. on the primary object within images. A related study found that model attention can be drawntowards adversarial patches designed to mislead classification results. Our visual prompt can similarlybe viewed as an adversarial patch, aiming to divert model attention away from its original focus, butfor adapting model behavior towards useful tasks. Transferability and Universality of Adversarial Perturbation The transferability of adversarialperturbation describes the phenomenon that perturbations crafted for one model can deceive another,even with a different architecture . Different from the transferability, the universality of adversarialperturbation is a property that the perturbation is still effective when it is added to different inputinstances . To improve transferability, we use generative adversarial perturbations . A simpleway to increase the universality is to include various input images when optimizing an adversarialperturbation , which we also leverage in this work. The transferability and universality ofadversarial perturbation have also been studied . These intriguing properties increase thethreats in real-world applications. However, how to leverage the two properties for good has not beenexplored in the community.",
  "Method": "Our goal is to learn a patch such that if it is applied to any part of an input image, the correspondingtokens in the vision encoder would attract more attention, and therefore, the final representation ofthe image will be manipulated. We propose a self-supervised method to learn such a patch, while thevision-encoder model is utilized in its frozen state and we only require a collection of images withoutthe need for any labels. The learning is about the patchs RGB color space (three channels) while itsshape is predefined, or alternatively, the shape mask can be learned as an additional fourth channel.In the subsequent sections, we provide a detailed explanation of how this prompt is learned throughthe back-propagation of the self-supervised loss that accounts for the relation between the patchslocation and the attention it receives. Input with Prompt. We aim to discover a visual prompt (patch) that attracts the attention of atransformer-based vision encoder toward a specific location. To achieve this, we work on the RGBimage input denoted as I Rnn3 where n is the number of pixels in one dimension of the imageand 3 indicates the three-dimensional color space. The prompt, denoted as P, is also in a similarRGB color space and of size mm (the impact of prompt size on performance is investigated insection 4.1). The designated coordinates [i, j] within the image 0 i, j < n is then the exact pixellocation where the visual prompt will be centrally inserted on. By selecting coordinates only withcriteria of 0 < i m",
  "< n, we ensure that the patch falls within the valid range, meaning thatno part of the patch extends beyond the boundaries of the image after it is inserted": "The key point here is that by inserting we do not simply mean adding the values of the prompt pixelwith those of the image pixel. Rather, we aim to identify a general, universal patch that remainseffective when applied to different images and locations, regardless of the underlying pixel values. Toachieve this, we replace the values of the corresponding image pixels with those of the patch pixelsI[i m",
  ": j + m": "2 ] = P[:, :], a process we refer to as insertion of patch into the image.To ensure the transferability of the visual prompt, the placement of the patch is randomized acrossvarious locations within the image. During the training phase, for an input image, we select k randomlocations denoted by coordinates [i, j] within the validity range criteria mentioned. The patch is theninserted on each coordinate one at a time. This process is repeated for all images I in the dataset,resulting in a set of modified input images IP for all samples. These modified images, with patchesinserted at random locations, are then fed into the vision encoder. Transformer-based Vision Encoder. Within a transformer block with L layers and H attentionheads, we denote the attention values as Ail Rtt, where A is the attention values of the ith headof the layer l for an input image with t tokens. The output of the transformer encoder is typicallya contextualized representation of the image tokens, where each token has been influenced by itssurrounding tokens through the attention mechanism. This representation, often obtained from aspecial token (e.g., the CLS token), can then be utilized for downstream vision tasks such as objectrecognition or image classification. Our method leverages the attention values of the CLS token inthe last layer, averaged over the heads of that layer: AL[CLS, ] = Hi=1 AiL[CLS, ]. Target Gaussian Map. When the prompt is applied to the input image on the pixel position [i, j],it overlays on one or more tiles with the center [x, y], refer to . Assuming that the modeldivides an n n pixel image to t t image tokens with each tile having the pixel size nt (n/t = nt),the corresponding patch center [x, y] can be derived from its pixel position on the image [i, j] as:x, y = (i/nt, j/nt). To construct the target Gaussian map for the patch over the tt token space, a 2D Gaussian distributionis employed. The Gaussian map G(x, y) at location [x, y] is represented by the probability densityfunction of a Gaussian distribution N(, 2), where = (x, y) denotes the mean (center) of thedistribution, and is calculated from the Full Width at Half Maximum (FWHM), which in ourexperiments is set to the patch size in token space m/nt: = (m/nt)/(2",
  "ln(2))": "Neural Prior. To learn the optimized prompt, we aim to avoid the expensive need to fine-tune thelarge-scale vision encoder by employing it solely in a frozen state. In this way, the only parametersbeing updated through the training process are the pixel values of the prompt. However, a potentialbarrier we may encounter in efficiently achieving the optimal patch is the limited learnable variableswithin the pixel space . Thus we parameterize the patch input space with a neural network asin . Such parameterization is also shown to improve the transferability of optimized perturbations. we use a neural prior f starting from randomly initialized weights, that receives a random priorinput mm , and outputs an initial prompt Pprior = f() which is then masked by a predefinedshape mask P = Pprior Pmask and finally inserted centrally on the [i, j] pixel of the input imageI to form the input to the vision encoder: V iT(IP). Our neural prior f employs a CNN-basedarchitecture, in particular a U-Net with three layers of downsampling and three upsampling withSigmoid activation to ensure the values stay between . Leveraging shared spatial patterns amongpixels in the patch enables effective optimization during training which facilitates efficient visualprompt learning . Objective Function. Assuming that an image with a patch applied on it IP = Insert(I, P, [i, j]) isthe input to the V iT, we define mean of last-layer attention values over attention heads of the visionencoder model as Al = Attention(V iT(IP)). Our objective is to train the deep neural prior tooutput a prompt P such that it enhances the attention Al at corresponding token at x, y = (i/nt, j/nt)with nt nt indicating the number of pixels in a token. Having this objective in mind, we calculatethe final self-supervised loss as follows:",
  "Experiments": "In the following, we first present the learned prompts for CLIP-B/32 and CLIP-L/14, discussingseveral design considerations and their impacts. We then evaluate the effectiveness of the prompts inthe context of Naming Keypoints task. We take it a step further by learning prompts for SigLIP ,DeiT , and DINOv2 , each of which either features a slightly different encoder architecture,or has a different training objective function, or varies in pretraining datasets. Finally, we comparehow the prompts impact their attention through layers. Implementation Details and Settings. All the vision encoders used are pretrained models availablefrom publicly accessible libraries such as Hugging Face Transformers and PyTorch Hub .Unless specified, we use a learning rate of 1e-3 and batch size of 32 and train our framework for 10epochs. All experiments were conducted on a machine equipped with a single NVIDIA A100 GPUwith 80GB of memory. Datasets and Evaluation Metric. Training: To train our neural prior, we use images from ImageNet which consists of millions of labeled images spanning one thousand classes. However, as workingwith the full ImageNet is resource-demanding, we exploit a subset of it comprising of 10 categoriesaccording to . The limited coverage of label classes is not an issue for our work, as our methodis not supervised and thus does not rely on specific label types. Furthermore, the over 13k samplesin offer a rich and diverse set of images, enhancing the generalizability of our self-supervisedtraining. Evaluation: For naming a keypoint task, we utilize CUB-200-2011 test set to evaluateour learned prompt. This dataset contains 5794 bird images of different types, annotated with 15body-part names and corresponding pixel coordinates as keypoints on the bird image. We evaluate ourmethod by positioning our learned prompt on these keypoints and then calculating the performancebased on the percentage of correctly identified body parts. To further evaluate our learned prompt, wetest its performance also on the RefCOCO , RefCOCO+, and RefCOCOg datasets whichconsist of images, annotated with bounding boxes around objects in it, each of which is paired withexpressions. Models and Baselines. Specifically, CLIP-B/32 and CLIP-L/14 are employed for their robustzero-shot learning capabilities, leveraging large-scale vision-language pretraining. SigLIP integratesimage and language understanding in a synergistic manner. DeiT and DINOv2 are utilized for theirstate-of-the-art performance in vision transformer architectures, with DeiT focusing on efficienttraining and DINOv2 providing self-supervised learning benefits. The baseline for comparisoninvolves using the cropped region over the object bounding box in the RefCOCO dataset family. Thisapproach isolates the relevant object from the surrounding context, allowing for a focused evaluationof keypoint identification accuracy. Additionally, random location baselines are employed to assessthe robustness of our method against random visual prompts.",
  "Guiding Attention to Name Keypoints": "We initiate our experiments with a basic prompt configuration, termed a vanilla patch, which is asimple filled square. This patch undergoes no shape mask filtering during training, and the learnedprompt is optimized RGB values. We train patches of varying sizes for the CLIP-B/32 and CLIP-L/14 : Vanilla Patches and Scale Constraints. The tile sizes of the CLIP-B/32 and CLIP-L/14models are 32 32 and 14 14, respectively. We defined the dimensions for the vanilla patch (asimple filled square prompt) accordingly. For CLIP-B/32, the patch sizes are scaled proportionally tothe models input token at [1, 1.5, 2, 2.5] times larger. For CLIP-L/14, the patch sizes are times larger than the token. These scales are chosen to ensure the patch is sufficiently large foreffective learning without excessively covering image information. These prompts are evaluated onthe keypoint naming task using the CUB dataset.",
  "(e) Lambda": ": Effect of Patch Parameters: Performance of CLIP models on CUB dataset for varyingratios of hollow circle and square masks over different patch sizes. The tile size of models ViT-Baseand ViT-Large are 32x32 and 14x14, respectively. (e) shows the ratio colors and the illustration of . models. Our experiments utilize prompt sizes corresponding to the model token dimensions. AsCLIP models split input images into tokens of different pixel sizes (base model: 32 32, large model:14 14), our learned prompt of one-token size inherently possesses varying pixel dimensions. presents the visualizations of the prompts learned for both models at different sizes. Weselect sizes proportional to the tokens: 1, 2, 3, and 4 times larger for CLIP-L/14. For CLIP-B/32,due to the substantial token pixel size, we opt for prompts that are 1, 1.5, 2, and 2.5 times largerto avoid excessive pixel coverage by the patch. We report the accuracy of keypoint naming on theCUB dataset. The results indicate that for CLIP-B/32, the prompt size equal to 1 token yields thebest performance, whereas for CLIP-L/14, the 3-times larger prompt achieves the highest accuracy.Looking into this deeper, this suggests that optimal accuracy is achieved when the prompt coversapproximately the same area size on the image (32 32 and 42 42) for both models. This maybe because a larger patch, while better at manipulating the image, also covers more of it, leading toinformation loss and lower performance. Beyond vanilla patch towards using shape priors:Initially, we employed a filled square, butprevious studies imply that the geometric shape of the patch influences its effectiveness . Toinvestigate the influence of prompt shape on performance in naming keypoints task for CLIP models,we employ hollow square and circle as predefined shape masks. We define a thickness factor as = inner Diameter",
  "outer Diameter as shown in and investigate the performance for different thicknessestogether with different patch sizes, again proportional to the models token size": "What we observe from the performances in is that the circle-shaped prompt yields higheraccuracy compared to the square-shaped prompt. For CLIP-B/32, the circle prompt achieves thehighest accuracy of 11.51, and the best square prompt yields slightly lower accuracy. For CLIP-L/14,the difference is significant: the circle prompt reaches an accuracy of approximately 30.5, whereas thesquare prompt only achieves about 20.5. The comparison of patch sizes reveals findings consistentwith the vanilla patch results. For CLIP-B/32, the optimal patch size remains the same as the defaultsize for one token. In contrast, for CLIP-L/14, the highest accuracy is obtained with prompt threetimes larger than one token of the large clip model, the size covering a similar number of pixels.The visualizations of the learned prompts with the highest performance are depicted in . Weobserve that the red color for CLIP-L/14 and the pink color for CLIP-B/32 are predominantly present",
  "in the learned prompts. Interestingly, for CLIP-L/14, when the prompt shape is a circle, it learned asolid red color which confirms the findings in": ": Visualization of optimal prompt for various CLIP encoders. Here we visualize thebest-performing square frame and circle-shape prompts from . C stands for Circle and S forSquare. Colors of red and pink are predominant for CLIP-B/32 and CLIP-L/14, respectively. It isnoteworthy that the optimal prompt for CLIP-L/14 is a red circle. Even when we use a square shapeprior, a red circle emerges inside the square. Previously, tried various markers using intuitionand identified a red circle as an effective prompt. Interestingly, the marker seems to be the optimalmarker for guiding the attention of CLIP-L/14. Though for CLIP-L/14 the red circle is not optimum.Thus it may be an emergent property arising from scale.",
  "visualization": "Now that we have developed our prompt and explored the design configurations, we are interested inevaluating its performance compared to baseline methods on different datasets. To this end, we trainour method on the ImageNet dataset and test it on the CUB and RefCOCO datasets. These datasetsare new to the prompt and have not been seen during the training phase. Additionally, they offer theadvantage of annotated image locations with names of specific body parts or objects for CUB andRefCOCO, respectively. We define the baselines as Random: Randomly selected areas. Crop: Removing the area around thebounding box for RefCOCO and defining a square similar to the prompt size for CUB, then cuttingout that area. Blur: Similar to cropping, but with the additional step of blurring the outer area usingthe averaging method with a kernel size of 5. As indicated by the results presented in , the Crop method exhibits superior performancein RefCOCO tasks, but its effectiveness diminishes when applied to the CUB dataset. In contrast,the Blur method shows notable improvements in RefCOCO tasks. Our observations suggest thatRefCOCO object descriptions often focus solely on the target object rather than relying heavily onscene context. This tendency explains the success of both the Blur and Crop methods in this dataset. However, in the CUB dataset, accurately recognizing animals can be challenging for the visionencoders, making it difficult to detect specific body parts. Despite this challenge, our learned promptsgenerally outperform other baselines in the CUB dataset. In RefCOCO tasks, our method consistentlyachieves second place after the effective Crop method. These findings underscore the effectiveness ofour approach, especially when the context of the image is important.",
  "Investigating Attention Dynamics in Various Vision Encoders": "Various vision encoder models are trained on diverse datasets, leading to behaviors that may differfrom CLIP models. More importantly, these vision foundations use different training paradigms. CLIPvision encoder is trained with language supervision, while DINOv2 is trained by self-supervisionusing visual information only. Therefore, during training is no supervision signal for the model tolearn that colored markers (such as red circles) signify the importance of the region the marker isplaced. Additionally, their architectural details might vary slightly, e.g., SigLIP has no CLS tokenand instead averages over tokens by an attention pooling mechanism. As a result, the optimal promptto redirect their attention can also be different. To investigate this, we use a similar self-supervisedtraining approach to learn prompts of SigLIP, DeiT, and DINOv2 as well. shows the visualization of the learned prompt for each vision encoder. The differentappearance of the visual prompts confirms that there is no single prompt that is universally optimalfor all encoders. We apply the learned prompts on to three different locations on an image andcompare the heatmap visualizations of the prompted image with the original, unprompted image.For almost all encoders, the attention heatmap shifts towards the prompt location, demonstrating theeffectiveness of our prompt in manipulating the attention of the corresponding visual encoder models. The effect of the optimized prompt on attention across layers: We examine the attention averagedvalues of tokens in the area overlaid by the prompt before and after its application for over 1000images from MS COCO while the prompt location is random. By analyzing attention values across : Performance comparison of different methods across various datasets. Bold numbers indicatethe best performance, while underlined numbers denote the second-best performance. The Cropmethod demonstrates superior performance in RefCOCO tasks but exhibits reduced effectiveness inthe CUB dataset. Conversely, the Blur method performs significantly better in RefCOCO tasks. Ourobservations reveal that object descriptions in RefCOCO do not always rely heavily on scene context,explaining the success of both Blur and Crop methods. However, in the CUB dataset, where contextis crucial for proper animal detection, losing context leads to decreased model performance.",
  "(e) DINOv2": ": Attention Gain with Prompt Usage throughout Layers. We applied our learned promptto random locations on 1000 samples from the MS COCO dataset (blue line). For comparison, wealso used a simple red circle prompt on the same locations (orange line). The Relative Gains arecalculated by dividing the difference between attention values before and after applying the prompton the image by the original attention values of the overlaid tokens without any prompt. A notableobservation is that while the simple red circle prompt is as effective for CLIP-L/14 and CLIP-B/32,it is significantly less effective for DeiT and DINOv2 compared to the learned prompt in terms ofredirecting attention to a specific location. layers, we can assess the prompts impact. Since attention values are relative, we define AttentionGain as the ratio of the difference of averaged attention values of tokens influenced by the prompt totheir original attention values. illustrates the attention gains across different models. Theblue line represents the gains for each models optimized prompt, while the orange line shows thegains for a simple red circle marker prompt. Red circle does not guide attention of DINOv2 and DeiT: From we can see that the uniqueprompts for DeiT and DINOv2 draw significantly larger attention to themselves compared to the redcircle prompt, showing the power of our learning framework. This could confirm the suggestion thatthe presence of red circles in the training data of CLIP models and SigLIP has made them particularlysensitive to this predefined feature , which is not the case for other vision encoders such as DeiTand DINOv2. The significance of optimizing prompts for self-supervised models such as DINOv2: To furtherevaluate the effectiveness of our prompts, we use the MLLM (Multimodal Large Language Model)introduced in , which uses an Interleaved Mixture-of-Features approach to spatially leverageinterleaving CLIP and DINOv2 visual tokens after an adapter. In , we compare modelsperformance with and without our unique CLIP-L/14 and DINOv2 prompts applied to the imageinput to see if it improves the question answering responses of their model on proposed MMVP : Learned Prompt for Different pretrained Vision Transformers. The optimal visualprompts are not the same and each model has its own unique pattern. The prompt is optimizedto generalize across images. Comparing attention heatmaps of the original (on left) and promptedimages (on right) reveals how effectively the prompt directs attention to specific locations. Theprompt is optimized for each ViT (CLIP-B/32, CLIP-L/14, SigLIP, DeiT, and DINOv2) separately,and is optimized over 20k random samples from ImageNet . The depicted image is taken from MSCOCO . : Prompts for the new generation of Vision Language Models: New vision languagemodels are moving beyond clip vision encoders towards using richer vision encoders such as DINOv2.Therefore it becomes progressively necessary to identify visual prompts for the new generation ofmodels. This figure depicts an example to show the potential of using the optimized prompts in thenew generation of vision-language models, in this case, LLaVA+MoF on examples from theMMVP dataset. (Multimodal Visual Pattern) Benchmark. The examples demonstrate that our prompts enable themodel to generate more accurate answers, which means they create improved embeddings. Thisindicates that our attention-guiding prompts have promising applications in a variety of vision tasksthat can be explored in future works.",
  "Conclusion": "In this work, we proposed a self-supervised optimization-based visual prompting technique forguiding the attention of vision transformers, thereby avoiding the limitations of manually craftedprompts. Our method demonstrated the ability to guide the attention of various vision transformermodels, such as the CLIP family, SigLIP, DeiT, and DINOv2, without requiring prior knowledge ofdataset biases and without fine-tuning the models. The transferability of the prompt across different images was accomplished by leveraging a deep network prior. Our experiments confirmed that thelearned prompts successfully redirect the attention of not only the CLIP family of models which aretrained with language supervision, but also the purely self-supervised models such as DINOv2. Asnew vision language foundation models are leveraging the self-supervised vision encoders for theirsuperior ability to extract visual features, the proposed optimization-based proves to be helpful forprompting upcoming models.",
  "A. Chaubey, N. Agrawal, K. Barnwal, K. K. Guliani, and P. Mehta. Universal adversarialperturbations: A survey. arXiv preprint arXiv:2005.08087, 2020": "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages248255. Ieee, 2009. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16words: Transformers for image recognition at scale, 2021.",
  "J. Gu, V. Tresp, and Y. Qin. Are vision transformers robust to patch perturbations? In EuropeanConference on Computer Vision, pages 404421. Springer, 2022": "J. Gu, Z. Han, S. Chen, A. Beirami, B. He, G. Zhang, R. Liao, Y. Qin, V. Tresp, and P. Torr. Asystematic survey of prompt engineering on vision-language foundation models. arXiv preprintarXiv:2307.12980, 2023. J. Gu, X. Jia, P. de Jorge, W. Yu, X. Liu, A. Ma, Y. Xun, A. Hu, A. Khakzar, Z. Li, et al. Asurvey on transferability of adversarial examples across deep neural networks. arXiv preprintarXiv:2310.17626, 2023.",
  "D. Karmon, D. Zoran, and Y. Goldberg. Lavan: Localized and visible adversarial noise. InInternational Conference on Machine Learning (ICML), 2018": "S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg. Referitgame: Referring to objects inphotographs of natural scenes. In Proceedings of the 2014 conference on empirical methods innatural language processing (EMNLP), pages 787798, 2014. A. Khakzar, P. Khorsandi, R. Nobahari, and N. Navab. Do explanations explain? model knowsbest. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), pages 1024410253, June 2022.",
  "A. Liu, X. Liu, J. Fan, Y. Ma, A. Zhang, H. Xie, and D. Tao. Perceptual-sensitive gan forgenerating adversarial patches. In AAAI, 2019": "A. Liu, J. Wang, X. Liu, B. Cao, C. Zhang, and H. Yu. Bias-based universal adversarial patchattack for automatic check-out. In Computer VisionECCV 2020: 16th European Conference,Glasgow, UK, August 2328, 2020, Proceedings, Part XIII 16, pages 395410. Springer, 2020. P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train, prompt, and predict:A systematic survey of prompting methods in natural language processing. ACM ComputingSurveys, 55(9):135, 2023. H. Luo, J. Gu, F. Liu, and P. Torr. An image is worth 1000 lies: Transferability of adversarialimages across prompts on vision-language models. In The Twelfth International Conference onLearning Representations, 2024. URL J. Luo, T. Bai, and J. Zhao. Generating adversarial yet inconspicuous patches with a singleimage (student abstract). In Proceedings of the AAAI Conference on Artificial Intelligence,volume 35, pages 1583715838, 2021.",
  "J. Mao, J. Huang, A. Toshev, O. Camburu, A. Yuille, and K. Murphy. Generation and compre-hension of unambiguous object descriptions. In CVPR, 2016": "S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. Universal adversarial pertur-bations. In Proceedings of the IEEE conference on computer vision and pattern recognition,pages 17651773, 2017. M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza,F. Massa, A. El-Nouby, et al. Dinov2: Learning robust visual features without supervision.arXiv preprint arXiv:2304.07193, 2023. N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The limitationsof deep learning in adversarial settings. In 2016 IEEE European symposium on security andprivacy (EuroS&P), 2016.",
  "O. Poursaeed, I. Katsman, B. Gao, and S. Belongie. Generative adversarial perturbations.In Proceedings of the IEEE conference on computer vision and pattern recognition, pages44224431, 2018": "A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,P. Mishkin, J. Clark, G. Krueger, and I. Sutskever.Learning transferable visual mod-els from natural language supervision.In M. Meila and T. Zhang, editors, Proceedingsof the 38th International Conference on Machine Learning, volume 139 of Proceedingsof Machine Learning Research, pages 87488763. PMLR, 1824 Jul 2021. URL S. Shen, S. Yang, T. Zhang, B. Zhai, J. E. Gonzalez, K. Keutzer, and T. Darrell. Multitask vision-language prompt tuning. In Proceedings of the IEEE/CVF Winter Conference on Applicationsof Computer Vision, pages 56565667, 2024. A. Shtedritski, C. Rupprecht, and A. Vedaldi. What does clip know about a red circle? visualprompt engineering for vlms. In Proceedings of the IEEE/CVF International Conference onComputer Vision (ICCV), pages 1198711997, October 2023.",
  "J. Wang, A. Liu, X. Bai, and X. Liu. Universal adversarial patch attack for automatic checkoutusing perceptual and attentional bias. IEEE Transactions on Image Processing, 31:598611,2021": "T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,M. Funtowicz, et al. Huggingfaces transformers: State-of-the-art natural language processing.arXiv preprint arXiv:1910.03771, 2019. Z. Xiao, X. Gao, C. Fu, Y. Dong, W. Gao, X. Zhang, J. Zhou, and J. Zhu. Improving transfer-ability of adversarial patches on face recognition with generative models. In Proceedings of theIEEE/CVF conference on computer vision and pattern recognition, pages 1184511854, 2021."
}