{
  "DiffuseMix (T)": ". Top row: existing mixup methods interpolate two different training images . Bottom row: label-preserving methods.For each input image, DIFFUSEMIX employs conditional prompts to obtain generated images. The input image is then concatenated witha generated image to obtain a hybrid image. Each hybrid image is blended with a random fractal to obtain the final training image.",
  "Abstract": "Recently, a number of image-mixing-based augmenta-tion techniques have been introduced to improve the gen-eralization of deep neural networks. In these techniques,two or more randomly selected natural images are mixedtogether to generate an augmented image.Such meth-ods may not only omit important portions of the input im-ages but also introduce label ambiguities by mixing imagesacross labels resulting in misleading supervisory signals.To address these limitations, we propose DIFFUSEMIX, anovel data augmentation technique that leverages a diffu-sion model to reshape training images, supervised by ourbespoke conditional prompts.First, concatenation of apartial natural image and its generated counterpart is ob-tained which helps in avoiding the generation of unrealisticimages or label ambiguities. Then, to enhance resilienceagainst adversarial attacks and improves safety measures,a randomly selected structural pattern from a set of frac-tal images is blended into the concatenated image to formthe final augmented image for training. Our empirical re-sults on seven different datasets reveal that DIFFUSEMIXachieves superior performance compared to existing state-of-the-art methods on tasks including general classification,fine-grained classification, fine-tuning, data scarcity, andadversarial robustness. Augmented datasets and codes areavailable here:",
  ". Introduction": "In the era of deep learning, image-mixing-based data aug-mentation techniques stand out for their simplicity andeffectiveness in addressing the generalization of learningmodels toward testing scenarios . These techniques ingeniously mix randomlyselected natural images and their respective labels from thetraining dataset using a number of mixing combinations tosynthesize new augmented images and labels. Such a pro-cess often implies linear interpolation of data, resulting inthe generation of novel training images. These approacheshave been proven to be effective in improving the perfor-mance of deep models ,. However, these techniques may face a number of chal-lenges such as the omission of salient image regions (Fig-ure 1) and label ambiguities due to random placements ofimages . A few researchers have attempted to alleviatethese issues by introducing saliency-based mixup strategiesin which important regions of one image are pasted ontothe less important portions (mainly context) of another im-age . These methods not only suffer from thecosts but also the shortcomings of saliency detection meth-ods. Moreover, as these methods still rely on mixing of twoor more images belonging to different classes, the underly-ing issue of omitting the important context still persists.",
  "Data Scarcity": "image generation and editing processes. Although the ideaof using images generated by diffusion models directly asaugmented images for training a classifier has been studiedby some researchers , this way of data augmentationdoes not result in significant performance gains. In fact, asreported in , the model trained using generated imagesdirectly as augmentation may even result in lower perfor-mance than the baseline trained without any augmentation.The underlying problem can be attributed to the limited con-trol that these diffusion models offer over generated images.Owing to the sensitivity of diffusion models to conditionalprompts, generation of desired complex scenes, layouts, andshapes in an image is a cumbersome task . Thus, poorlyconstructed prompts pose the risk of producing images thatmay not be suitable for data augmentation as the generatedimages may deviate drastically from the actual data distri-bution. Training on such images may result in overfitting ofthe learning model on wrong data distribution, consequentlyresulting in performance degradation. Therefore, careful se-lection of prompts for data augmentation needs further in-vestigation. Moreover, to mitigate the risk of poorly gener-ated images affecting the overall training, a more efficientway of utilizing the generated images is necessary. To this end, we propose a novel data augmentationmethod, DIFFUSEMIX, that leverages the capabilities of aStable Diffusion model to generate diverse samples basedon our tailored conditional prompts. In contrast to Trabuccoet al. , rather than solely relying on Stable Diffusion foraugmentation, we propose an effective approach which uti-lizes both original and generated images to create hybridimages. This way, visual diversity obtained by the diffusionmodels is infused with the original images while retainingthe key semantics. In addition, to increase overall structuraldiversity, we blend self-similarity-fractals with the hybridimages to create the final training images. This blendinghas previously been found useful for ML safety measures while in our approach, this added diversity helps in avoiding overfitting on the generated contents resulting inperformance improvements. Our experimental results showthat DIFFUSEMIX benchmarks better generalization as wellas increased adversarial robustness compared to the existingstate-of-the-art (SOTA) augmentation methods. Moreover,it offers compatibility with a broad spectrum of datasets andcan be incorporated into the training of various existing ar-chitectures. Some notable aspects of this research work areas follows: We introduce a new data augmentation method driven bya diffusion model, which generates diverse images via ourbespoke conditional prompts.",
  "We propose to concatenate a portion of the natural imagewith its generative counterpart to obtain hybrid images.The combination brings richer visual appearances whilepreserving key semantics": "We collect a fractal image dataset and blend into the hy-brid images. This improves the overall structural com-plexity of the augmented images and helps to avoid over-fitting on generated images, thus resulting in better gen-eralization. Extensive experiments on seven datasets for various tasksincluding general classification, fine-grained classifica-tion, adversarial robustness, transfer learning, and datascarcity demonstrate the superior performance of our pro-posed method compared to the existing SOTA image aug-mentation techniques.",
  ". Related Work": "Data augmentation has become indispensable in enhanc-ing the diversity of training datasets, thereby mitigating therisks of overfitting. Traditional approaches employed strate-giessuch as horizontal and vertical translations, affinetransformations, scaling, and squeezingwhen training amodel. This not only improves the performance but alsoimproves the generalization of the model on test datasets.Diffusion Models for Augmentation: Recently, several re-",
  ",,,": ". Architecture of the proposed DIFFUSEMIX approach. An input image and a randomly selected prompt are input to a diffusionmodel to obtain a generated image. Input and generated images are concatenated using a binary mask to obtain a hybrid image. A randomfractal image is finally blended with this hybrid image to obtain the augmented image. searchers have explored the possibility of data augmenta-tion with diffusion models. Azizi et al. proposed the uti-lization of fine-tuned text-to-image diffusion models on Im-ageNet classification, revealing that augmenting the trainingset with these synthetic samples may boost classificationperformance. Similarly, Trabucco et al. investigateddiffusion models to create more diverse and semanticallyvaried datasets, aiming to improve outcomes in tasks suchas image classification. Li et al. further explored diffu-sion models-based augmentation for knowledge distillationwithout real images. Image Mixing Augmentation: Image mixing is a promi-nent class of augmentation methods for training robustCNN models . Some of these methods in-clude Mixup, CutMix, and AugMix. Mixup generatessynthetic images by linearly interpolating pixel values fromtwo randomly selected images. In contrast, CutMix in-volves pasting a random patch from one image onto another.AugMix employs a stochastic combination of data aug-mentation operations on an input image. SaliencyMix utilizes saliency maps to concentrate the augmentation onthe images most vital regions, ensuring overall image in-tegrity. Manifold Mixup enhances representation byinterpolating network hidden states during training. Thisentails blending two hidden states with a random weight toproduce an interpolated manifold-based hidden state. Puz-zleMix , an improvement over the traditional mixup,factors in image saliency, and local statistics during imageblending. This method segments an image into patches, al-locates weights based on saliency and local statistics, andmerges patches from different images in accordance withtheir weights. PixMix have studied mixing of inputimages with fractal and feature visualization images to im-prove ML safety measures. A detailed summary of several image-mixing-based methods along with their componentsand application tasks is provided in .Automated Augmentation:AutoAugment , for in-stance, employed reinforcement learning to pinpoint opti-mal data augmentation policies, while RandAugment in-tegrates a suite of random data augmentation operations toimprove model generalization. AdaAug is proposed toefficiently learn adaptive augmentation policies in a class-dependent and potentially instance-dependent manner.In contrast to previous methods, our approach empha-sizes the concatenation of original and generated images,using a pre-defined library of conditional prompts. The ob-tained hybrid images are blended with fractal images to fur-ther improve the overall performance.",
  ". Background and Overview": "Existing image-mixing-based methods may induce labelambiguity by placing one image on top of the other andconsequently overlapping either some portions of the ob-ject or its context . In contrast, the core idea of DIF-FUSEMIX is to concatenate a portion of the original imagewith its counterpart generated image in such a way that ba-sic image semantics are preserved while providing diverseobject details and contexts for better augmentation.The proposed method as illustrated in com-prises of three pivotal steps: generation, concatenation,and fractal blending. Firstly, conditional prompts are usedwith a diffusion model to obtain a generative counterpartof the input image. Then, a portion of the original imageis concatenated with the rest of the portion taken from thegenerated image forming a hybrid image. This step is toensure that the training network always has access to the",
  ". A set of bespoke conditional prompts are used to obtaingenerated images preserving important features and adding richvisual appearance to the input images": "original data along with the generated one. Subsequently,a random fractal image is blended into the hybrid imageto obtain the final training image with a diverse structure.Blending fractal images has proven to be effective towardsML safety . In our work, we study the effective-ness of blending fractal images mainly towards improvedperformance.",
  ". Method": "The proposed DIFFUSEMIX is an effective data augmen-tation technique which can be used to enhance the robust-ness and generalization of the deep learning models. For-mally, Ii Rhwc is an image from the training dataset,Dmix() : Rhwc Rhwc denotes our data aug-mentation method. To obtain the final augmented imageAijuv, input image Ii goes through proposed generation us-ing prompt pj, concatenation using mask Mu, and blend-ing using fractal image Fv. The overall augmentation pro-cess, as also seen in Algorithm 1, can be represented asAijuv = Dmix(Ii, pj, Mu, Fv, ). Generation: Our generation step G(.) consists of a pre-trained diffusion model that takes a prompt pj from a pre-defined set of k prompts, P= {p1, p2, . . . , pk} wherej [1, k] along with the input image Ii and produces anaugmented counterpart image Iij. Image editing processin conventional diffusion models is often open-ended andguided by text prompts to obtain diverse image-to-imageor text-to-image translations. In our case, as the goal is toachieve a slightly modified but not too different version ofIi, filter-like prompts are curated in P which do not alter theimage drastically. Examples of the prompts used in DIF-FUSEMIX are shown in . The overall generationstep can be represented as: Iij = G(Ii, pj), where pj is arandomly selected prompt.",
  ": return D": "The mask Mu consists of zeros and ones only and is apixel-wise multiplication operator. The set of masks con-tains four kinds of masks including horizontal, vertical andflipped versions. Such masking ensures the availability ofthe semantics of the input image to the learning networkwhile reaping the benefits of the generated images. Fractal Blending: A fractal image dataset* F is collectedand used for inducing structural variations in the hybrid im-ages. A randomly selected fractal image Fv F is blendedto the hybrid image Hiju with a blending factor as:",
  ". Experiments and Results": "In this section, we present the experimental details, datasetsused to evaluate our approach, and analyses of the results.Datasets. To provide comparisons with existing studies onimage augmentation , we evaluate our approach on several general imageclassification and fine-grained image classification datasets.In the general image classification category, we employthree datasets including ImageNet , CIFAR100 and",
  "*Examples of fractal images are provided in Appendix 8": "Tiny-ImageNet-200 . In fine-grained image classifica-tion category, we employ four datasets including Oxford-102 Flower , Stanford Cars , Aircraft , andCaltech-UCSD Birds-200-2011 (CUB) . These datasetsoffer a diverse array of scenarios where images contain awide range of objects such as plants and animals in vari-ous scenes, textures, transportation modes, human actions,satellite imagery, and general objects. Implementation Details.We utilize InstructPix2Pix diffusion model to generate images with the help of our in-troduced textual library. For the generation of Mask M inEq. 1, a template image is divided into two equal parts, ei-ther horizontally or vertically. Randomly, one half is turnedon and the second half is turned off. In all experiments , = 0.20 is used for blending the fractal image in Eq. (2)& (3). Analysis on values is provided in Appendix 1. Textual Prompt Selection. In order to ensure that onlyappropriate prompts are applied, a bespoke textual libraryof filter-like global visual effects is predefined: autumn,snowy, sunset, watercolor art, rainbow, aurora,mosaic,ukiyo-e, and a sketch with crayon.Theseprompts are selected because of their generic nature andapplicability to a wide variety of images. Secondly, thesedo not alter the image structure significantly while produc-ing a global visual effect in the image. Each prompt in thetextual library is appended with a template A transformedversion of image into prompt to form a particular input tothe diffusion model. Examples of images generated throughthese prompts are shown in . Additional visual ex-amples and discussions on appropriate prompt selection areprovided in Appendix 3. Visualizing Intermediate Steps. depicts imagesobtained in each step of DIFFUSEMIX. It can be observedthat DIFFUSEMIX yields a broader spectrum of augmentedimages derived from the training set. These images containfull object with no portions omitted and provide suitablevariations for training.",
  ". General Classification": "In , Vanilla method serves as our baseline, achiev-ing Top-1 accuracies of 57.23% and 76.33% on Tiny-ImageNet and CIFAR-100 respectively, setting a bench-mark for subsequent comparisons. For Mixup, we observe aslight decline in performance on Tiny-ImageNet to 56.59%Top-1 accuracy but a marginal improvement on CIFAR-100, reaching 76.84%. Conversely, Manifold Mixup marks",
  ". Example images from different stages of DIFFUSEMIX:input image (Ii), generated image (Iij), mask (Mu), hybrid image(Hiju), fractal image (Fv), and final augmented image (Aijuv)": "CIFAR-100 datasets , while ResNet50 is em-ployed for ImageNet dataset .In Top-1 and Top-5 accuracy on Tiny-ImageNetand CIFAR-100 datasets is compared with the existingSOTA methods.The proposed DIFFUSEMIX techniquedemonstrates better performance gains compared to the ex-isting image augmentation approaches. On Tiny-ImageNetdataset, compared to Vanilla model, our DIFFUSEMIX re-sults in notable Top-1 and Top-5 accuracy gains of 8.54%and 10.01%. Moreover, compared to the second best per-former, Guided-AP , Top-1 and Top-5 accuracy gains ofour approach are 1.14% and 1.17%. Similar trends are ob-served on CIFAR100 dataset, where DIFFUSEMIX demon-strates Top-1 and Top-5 accuracy gains of 6.17% and 4.39%over vanilla and 1.3% and 0.53% over Guided-AP.We also evaluate DIFFUSEMIX on large-scale ImageNetdataset offering more challenging scenarios where existingSOTA methods only report Top-1 accuracy.Compared to the second best performer PuzzleMix , ourapproach demonstrates a performance gain of 1.13% (Ta-ble 3). Compared to Vanilla implementation, our approachdemonstrates a performance gain of 2.95%. The GC resultson these challenging and diverse benchmark datasets high-light the effectiveness of DIFFUSEMIX in enabling betterlearning. It also suggests its capability to combat overfit-ting and achieve better generalization.",
  ". Adversarial Robustness": "Following existing SOTA methods , we evaluate the robustness of our approach against ad-versarial attacks and input perturbations. In these exper-iments, fast adversarial training is adapted to createadversarially perturbed input images.The goal of theseexperiments is to evaluate whether an augmentation ap-proach can demonstrate better resilience against adversarialattacks. FGSM error rates are computed to evaluate theperformance against adversarial attacks.",
  "DIFFUSEMIX17.3834.53": "As shown in , DIFFUSEMIX demonstrates an er-ror rate of 17.38% on CIFAR-100, which is lower than allcompared methods. PuzzleMix is the second best performerobtaining 19.62% error rate. Similarly on Tiny ImageNet-200, DIFFUSEMIX outperforms SOTA by a notable marginobtaining 34.53% error rate while PuzzleMix remained thesecond best performer with 36.52% error rate. These resultsdemonstrate that even under adversarial perturbations, DIF-FUSEMIX remains resilient surpassing the performance ofexisting SOTA approaches.",
  ". Fine-Grained Visual Classification": "presents a comparison of Top-1 accuracy of vari-ous methods on a fine-grained visual classification task, us-ing ResNet-50. The methods are categorized into two maingroups: automated methods and the mixup family, and areevaluated across three datasets: CUB, Aircraft, and Cars.In the automated data augmentation, the Vanilla methodachieves 65.50%, 80.29%, and 85.52% accuracy on CUB,Aircraft, and Cars respectively.Other automated meth-ods like Auto Aug, Fast AA, DADA, RA, and AdaAugshow varied performance, with AdaAug topping this cat-egory with accuracies of 82.50% for Aircraft and 88.49%for Cars. The mixup family methods show a notable per-formance improvement, particularly GuidedMixup demon-strating the accuracies of 77.08%, 84.32%, and 90.27%on the three datasets respectively.Nevertheless, ourDIFFUSEMIX stands out by outperforming all comparedmethods significantly, achieving the highest accuracies of79.37% for CUB, 85.76% for Aircraft, and 91.26% forCars.This indicates that while both categories of methods en-hance performances, mixup family methods demonstratesuperior capability in handling fine-grained visual classifi-cation tasks. DIFFUSEMIX, in particular, showcases excep-tional improvements, suggesting its effectiveness in extract-ing nuanced features from the images.",
  "DIFFUSEMIX79.3785.7691.26": "Co-Mixup, YOCO, and another entry from Azizi et al., dis-play a range of improvements, with Top-1 accuracies span-ning from 69.24% to 78.17% and top-5 accuracies (whenprovided) ranging up to 93.86%. The most notable perfor-mance is observed in the DIFFUSEMIX method, which out-performs the others by achieving the highest accuracies at78.64% for top-1 and 95.32% for top-5.",
  "DIFFUSEMIX77.1474.12": "mance of our approach on fine-tuning the baseline modelusing three different datasets including Flower102, Aircraft,and Stanford Cars on ImageNet-pretrained ResNet-50 pro-vided by PyTorch and report results in .For the Flower102 dataset, DIFFUSEMIX achieved anaccuracy of 98.02%, which is higher than the second bestmethod, AdaAug 97.19%. A similar trend is observable inthe Aircraft 85.65% and Cars 93.17% datasets. We also ob-serve that the accuracy obtained by DIFFUSEMIX with fine-tuning () is comparable to the performance whentraining is done from scratch (). Since fine-tuningconsumes significantly less computational resources com-pared to training from scratch, this experiment elaboratesthe practical significance of DIFFUSEMIX.",
  ". Data Scarcity": "presents the Vanilla method as a baseline with64.48% accuracy on the validation set and 59.14% on thetest set. SOTA techniques like Mixup and PuzzleMix showimproved accuracies, with Mixup achieving 70.55% on val-idation and 66.81% on the test set, and PuzzleMix reaching71.56% and 66.71%, respectively.Notably, the Guided-SR and GuidedMixup methods sig-nificantly outperform other approaches, with GuidedMixupachieving the highest accuracies of 74.74% on validationand 70.44% on the test set. Our DIFFUSEMIX, which sur-passes all compared methods, demonstrates remarkable ac-curacies of 77.14% on the validation and 74.12% on thetest set, showcasing its superior ability to generalize wellfrom significantly limited data. This evidence suggests thatdata augmentation and mixing techniques, especially DIF-FUSEMIX, are highly beneficial in enhancing model perfor-mance under stringent data constraints.",
  "In this section, we provide a detailed ablation study and fur-ther analysis of various design choices in DIFFUSEMIX": "Ablation Studies:To evaluate the importance of eachcomponent we conduct an ablation study by removingeach component and report the observed performance onResNet-50 using Stanford Cars and Flowers-102 datasets in. When all of the components of DIFFUSEMIX are re-moved, the baseline (vanilla) using only original images Iiobtains Top-1 and Top-5 accuracies of 85.52% & 90.34%on Cars dataset and 78.83% & 94.38% on Flowers dataset.Next, we add fractal blending (Fv) to the input images re-sulting in slight performance gains on both datasets. Fur-ther, we remove both concatenation (Hiju) and fractalblending (Fv) by conducting experiments using generatedimages (Iij) directly as augmented images to train the net-work.This setting brings the method closer to the ap-proaches proposed in which utilize the images gener-ated using diffusion models directly as augmented images.In this experiment, the accuracies obtained on Cars datasetare 87.63% Top-1 and 90.23% Top-5. Similarly, we ob-serve the accuracies of 77.38% and 93.15% on Flowers102dataset. The results are consistent with the findings in that direct use of generated images may not yield signifi-cant performance gains over the vanilla method. Further,we carry out experiments by using generated images withfractal blending to train the model. On the Cars dataset,we observe slight performance gains over the vanilla modelwith accuracies of 89.42% Top-1 and 91.57% Top-5. How-ever, in Flowers dataset, we observe a slight performancedegradation. This demonstrates that to unleash the maxi-mum benefits of fractal blending, it should be accompaniedby more diversity in the training dataset. Next, we just re-move the fractal blending from DIFFUSEMIXwhile incor-porating concatenation between generated (Iij) and origi-nal (Ii) images to create hybrid images (Hiju) used as aug-mented training examples. This setting increases the accu-racies to 90.59% Top-1 & 96.73% Top-5 on Cars datasetand 79.22% Top-1 & 94.38% Top-5 on Flowers dataset.These performance improvements are due to the availabilityof both generated and original image contents in each aug-mented image, highlighting the importance of the concate-nation step in DIFFUSEMIX. Finally, when all componentsincluding fractal blending are used, the best accuracies of91.26% Top-1 & 99.96% Top-5 on Cars dataset and 80.20%Top-1 & 95.40% Top-5 on Flowers dataset are achieved. . Ablation study using Stanford Cars (cars) and Flowers102(Flow) datasets. Top-1 and Top-5 accuracies are reported with dif-ferent combinations of Ii: Input image, Iij: Generated imagesusing prompts pj, Hiju: Hybrid images using random mask Mu,and Fv: fractal images used to obtain final blended image Aijuv.",
  "DIFFUSEMIX98.0285.6593.17": "Overall, consistent gains achieved with each added compo-nent signifies the design choices in DIFFUSEMIX towardsan effective image augmentation technique.Number of Prompts Vs. Performance: The variety ofaugmented images is determined by the number of ran-dom prompts which is an important factor in DIFFUSEMIX.A higher number of prompts corresponds to more diversetraining samples. shows the effect of increasingthe number of prompts on the performance using three dif-ferent datasets including Birds, Aircraft and Cars. Increas-ing the number of prompts consistently yields performancegains on all three datasets using both Top-1 and Top-5 ac-curacy. However, the peak performances are achieved whenall ten prompts proposed in our approach are used for train-ing. In almost all datasets, the increasing accuracy trendscan be seen even at 10 prompts which shows that the addi-tion of more prompts may further improve the performance.However, it will also incur more computational costs.Increasing Masks Vs Performance: We conduct exper-iments using Oxford Flower102 dataset to study the im-pact of various masks on the overall performance of DIF- FUSEMIXand report the results in .Using evenonly one kind of mask, vertical in this example, our ap-proach achieves significantly higher accuracies than thevanilla baseline. When both vertical and horizontal masksare used, the accuracies improve further. However, the bestaccuracies are achieved when both horizontal and verticalmasks are used along with random flipping between the po-sitions of input and generated images adding more diversityto the training data. It is also possible to use the maskingtechniques from previous approaches, such as , in . Ablation on the effects of masking in DIFFUSEMIXonFlower102 dataset. All variants yield notably superior results com-pared to vanilla on ResNet-50. However, best results are achievedwhen all four vertical and horizontal masks are used.",
  ". Conclusion": "In this paper, we introduced DIFFUSEMIX, a data augmen-tation technique based on diffusion models to increase di-versity in the data while preserving the original seman-tics of the input image. It involves generation, concate-nation, and fractal blending steps to create the final aug-mented image.On multiple tasks such as general clas-sification, fine-grained classification, data scarcity, fine-tuning, and adversarial robustness involving several bench-mark datasets including ImageNet-1k, Tiny-ImageNet-200,CIFAR-100, Oxford Flower102, Caltech Birds, Stanford-Cars, and FGVC Aircraft, DIFFUSEMIXdemonstrates con-sistent performance gains and outperforms existing SOTAimage augmentation methods.Limitations: DIFFUSEMIXhas two notable limitations: (1)Image generation relies heavily on text prompts and awrong textual input may lead to unrealistic results.Weaddress this issue by proposing a set of filter-like promptsgenerally applicable to a wide range of natural images. (2)DIFFUSEMIXrequires additional overheads for generatingimages (more on this in Appendix 2). This is a small price topay for a guaranteed better convergence of large-scale clas-sification models and can be mitigated by generating andstoring augmented images once.Acknowledgements: We are thankful to Hamza Saleem forthe fruitful insights. Arif Mahmood is funded by the Infor-mation Technology University of Punjab, Pakistan.",
  "Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mo-hammad Norouzi, and David J Fleet. Synthetic data fromdiffusion models improves imagenet classification.arXivpreprint arXiv:2304.08466, 2023": "Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-structpix2pix: Learning to follow image editing instructions.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1839218402, 2023. Jie-Neng Chen, Shuyang Sun, Ju He, Philip HS Torr, AlanYuille, and Song Bai. Transmix: Attend to mix for visiontransformers. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1213512144, 2022. Yuan-Chih Chen and Chun-Shien Lu.Rankmix:Dataaugmentation for weakly supervised learning of classifyingwhole slide images with diverse sizes and imbalanced cat-egories.In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 2393623945, 2023.",
  "Dan Hendrycks, Nicholas Carlini, John Schulman, and JacobSteinhardt. Unsolved problems in ml safety. arXiv preprintarXiv:2109.13916, 2021": "Dan Hendrycks, Andy Zou, Mantas Mazeika, Leonard Tang,Bo Li, Dawn Song, and Jacob Steinhardt. Pixmix: Dream-like pictures comprehensively improve safety measures. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1678316792, 2022. Minui Hong, Jinwoo Choi, and Gunhee Kim. Stylemix: Sep-arating content and style for enhanced data augmentation. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 1486214870, 2021. Shaoli Huang, Xinchao Wang, and Dacheng Tao.Snap-mix: Semantically proportional mixing for augmenting fine-grained data. In Proceedings of the AAAI Conference on Ar-tificial Intelligence, pages 16281636, 2021. Zhenglin Huang, Xiaoan Bao, Na Zhang, Qingqi Zhang,Xiao Tu, Biao Wu, and Xi Yang. Ipmix: Label-preservingdata augmentation method for training robust classifiers. Ad-vances in Neural Information Processing Systems, 36, 2024.",
  "Huafeng Qin, Xin Jin, Yun Jiang, Mounim A El-Yacoubi,and Xinbo Gao.Adversarial automixup.arXiv preprintarXiv:2312.11954, 2023": "Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,Jonathan Ho, Tim Salimans, David Fleet, and MohammadNorouzi.Palette: Image-to-image diffusion models.InACM SIGGRAPH 2022 Conference Proceedings, pages 110, 2022. Yu Takagi and Shinji Nishimoto. High-resolution image re-construction with latent diffusion models from human brainactivity.In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1445314463, 2023. Brandon Trabucco, Kyle Doherty, Max Gurinas, and RuslanSalakhutdinov. Effective data augmentation with diffusionmodels. In ICLR 2023 Workshop on Mathematical and Em-pirical Understanding of Foundation Models, 2023.",
  "Lingyu Yan, Yu Ye, Chunzhi Wang, and Yun Sun. Locmix:local saliency-based data augmentation for image classifica-tion. Signal, Image and Video Processing, pages 110, 2023": "Sangdoo Yun, Dongyoon Han, Seong Joon Oh, SanghyukChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-larization strategy to train strong classifiers with localizablefeatures. CoRR, abs/1905.04899, 2019. Sangdoo Yun, Dongyoon Han, Seong Joon Oh, SanghyukChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-larization strategy to train strong classifiers with localizablefeatures. In Proceedings of the IEEE/CVF international con-ference on computer vision, pages 60236032, 2019.",
  "Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, andDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-tion. In ICLR, 2018": "Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Addingconditional control to text-to-image diffusion models.InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 38363847, 2023. Qihao Zhao, Yangyu Huang, Wei Hu, Fan Zhang, and JunLiu.Mixpro: Data augmentation with maskmix and pro-gressive attention labeling for vision transformer.In TheEleventh International Conference on Learning Representa-tions, 2022.",
  "Overview": "This supplementary document contains additional resultsand discussions. Summarily, provides analysison varying values of which defines the ratio of blend-ing between a fractal image and a hybrid image in DIF- FUSEMIX. provides comparisons of augmen-tation overhead between DIFFUSEMIXand existing imageaugmentation strategies with respect to their generaliza-tion performances. discusses examples of poorlyconstructed prompts and their effects on image generation. provides experimental results of using differ-ent masking strategies of the state-of-the-art methods withDIFFUSEMIX. provides the convergence analy-sis of DIFFUSEMIX. provides more visualiza-tions of the augmented training images obtained using DIF-FUSEMIX. provides a complete list of generaland fined-grained results. provides some visualexamples of the collected fractal image dataset.",
  ". Fractal Blending Ratio": "We experiment and observe the effect of varying fractalblending ratio in DIFFUSEMIX and report Top-1 accuracy(%) results on Flower102 dataset in . The values of are varied from 0.1 to 0.5. A higher value of indicatesa stronger ratio of fractal image blending.The baseline, ResNet50 without any augmentation,yields a top-1 accuracy of 78.73%. Compared to this, DIF-FUSEMIX yields consistent performance gains with all val-ues of . The best performance of DIFFUSEMIX is observedat = 0.2, where the top-1 accuracy peaks at 81.30%.However, generally, the performance remains better with areasonable value of . It starts dropping when the value of becomes too high. This suggests that higher fractal blend-ing ratios may introduce too much complexity or noise intothe original data, which adversely affects the models per-formance.",
  "ResNet50(CVPR16) 78.73+ DiffuseMix ( = 0.1)79.81+ DiffuseMix ( = 0.2)81.30+ DiffuseMix ( = 0.3)80.97+ DiffuseMix ( = 0.4)79.16+ DiffuseMix ( = 0.5)78.97": "augmentation. Although image generation process in DIF-FUSEMIX can be expedited by using parallel-processing,We do not utilize it to provide a fair comparison. It canbe seen in that DIFFUSEMIX provides a goodtradeoff between performance and augmentation overheadby outperforming all existing approaches in terms of accu-racy while providing significantly lower augmentation over-head compared to Co-Mixup and SaliencyMix approaches.Moreover, DIFFUSEMIX can also be optimized further bysaving the generated images offline once before carryingout any number of subsequent trainings. This may makeit significantly faster to perform several experiments on atraining model, particularly for optimization and researchpurposes.",
  "A distorted, warped paintingof a landscape": ". First row: original training image samples from different datasets such as Oxford-102 Flower , Stanford Cars , andAircraft , CUB-200-2011, and CIFAR100. Second row: Corresponding generated images show that the usage of descriptive prompts(blue text) results in poor images not feasible for training. When generating images on the CIFAR100 dataset, several additional challengesmay occur due to the small size of the images. For example, the image in the last column taken from CIFAR-100 with its correspondingprompt results in a black image containing no visible output. prompts is to introduce the type of prompts that may edit theimage in a way that preserves structural information and caneasily be applied to a range of diverse datasets. To this end,as described in the manuscript, we propose to use filter-likeprompts such as snowy, sunset, rainbow, etc. and demon-strate their effectiveness in training robust classifiers.Conversely, in this section, we discuss bad prompts thatmay not be a good fit for the image generation step of DIF- FUSEMIX. Some examples of such prompts are shown in. More descriptive and overly complicated promptsgenerate images that may be too different from the originaldistribution. The resultant images contain unrealistic fore-grounds and backgrounds, rendering these useless for thetraining of a classifier. This reiterates the importance of ourproposed filter-like bespoke conditional prompts that do notinduce unwanted changes to the training images.",
  ". DIFFUSEMIX with SOTA Methods": "In a series of experiments, we combine DIFFUSEMIX withexisting image augmentation approaches to see ifany performance gain is observed. Particularly, We replaceour masking approach with the masking used in the exist-ing methods while retaining the rest of the pipeline of DIF-FUSEMIX same.For CutMix + DIFFUSEMIX, we replace the concatena-tion step of DIFFUSEMIX with the random cropping of Cut-Mix. To this end, we randomly crop a patch from the gener- ated image and paste it onto the original image whereas theother stages remain the same. For Mixup + DIFFUSEMIX,we replace concatenation with the pixel blending of origi-nal and generated images as proposed in while the restof the steps remain intact. The results are summarized in. Using CutMix or Mixup methods yieldsimprovements over baseline ResNet50 training. However,when our proposed approach is added to the existing meth-ods, further performance gains are observed. Top perfor-mance is finally observed with our DIFFUSEMIX, whichdemonstrates the importance of forming hybrid images byconcatenating original and generated images. . Combining DIFFUSEMIX with SOTA image augmen-tation methods by replacing the image concatenation techniqueof DIFFUSEMIX with the masking techniques proposed in & . While DIFFUSEMIX provides consistent gains in thesesettings, the best performance of 81.30% is achieved when ouroriginally proposed method is used.",
  "(c) Validation Error (%)": ". On left side: The curves of top-1 and top-5 accuracy show an increasing trend during initial 60 epochs and remain stabletowards the end on Flower102 dataset. This same behavior can also be seen in top-5 accuracy. our enables smoother training andbetter convergence while avoiding overfitting. On right side: Similar to the accuracy plots, using DIFFUSEMIX demonstrates a smootherdecrease in validation error compared to ResNet50 or other variants. Best viewed in color.",
  ". DIFFUSEMIX Convergence": "Analysis on Top-1 and Top-5 Accuracy: In a series of ex-periments, we carry out an ablation to observe the top-1 andtop-5 accuracies of DIFFUSEMIX and its variants formed byremoving the components (generation, concatenation, andfractal blending) one by one.As seen in Figures 8a and 8b, the RES50+DIFFUSEMIXdemonstrates generally better performance with conver-gence at 77.26% accuracy, closely followed by DIF- FUSEMIX+GEN+CON at 75.79%, and Res50 at 76.41%.The DIFFUSEMIX+GEN model performs significantlylower yielding 73.96% accuracy.As discussed in themanuscript , using generated images directly forthe training may lead to deteriorated performance, which isre-validated in these experiments. This also shows the im-portance of each step proposed in DIFFUSEMIX towards ro-bust training more robust classifiers. Overall, similar trendsare observed in Top-5 accuracy results (b). Analysis on validation loss: As seen in (c), it isclearly noticeable that DIFFUSEMIX helps in model conver-gence and overall smooth decrease in validation loss duringtraining. Res50 baseline shows a good start with lower ini-tial loss. However, its loss starts fluctuating once the train-ing is continued indicating a potential plateau in learningor its limitation in capturing more complex patterns. Com-pared to all variants, Res50+DIFFUSEMIX benchmarks bet-ter convergence.",
  "(j) Magnolia Crayon Sketch": ". Illustration of original training images and DIFFUSEMIX augmented images from the Oxford Flower102 dataset. First row:showcases original, unaltered images of various flowers, including poinsettia, barbeton daisy, gazania, dandelion, and Magnolia classes.Second row: illustrates the transformative effects of the DIFFUSEMIX augmentation method. The effects of our custom-tailored prompts-based generation are visible on the generated portion of each image. Overall, DIFFUSEMIX results in a diverse array of images withsufficient structural complexity and diversity to train robust classifiers. notable performance gains, especially on CIFAR-100 with aTop-1 accuracy of 79.02%. CutMix slightly improves overthe baseline on Tiny-ImageNet, whereas AugMix shows adecrement, particularly on CIFAR-100 with a 75.31% Top-1 accuracy. PixMix introduces variations in the source image in-stead of mixing two input images. Compared to baseline,PixMix excels on CIFAR-100 with a 79.70% Top-1 accu-racy. SaliencyMix, which uses saliency to mix different por-tions of images, also shows promising results. Particularlyon CIFAR-100, it achieves a Top-1 accuracy of 79.75%.The Guided-SR method performs slightly lower comparedto AugMix on Tiny-ImageNet but stands out on CIFAR-100with 80.60% Top-1 accuracy, indicating its effectiveness.PuzzleMix and Co-Mixup introduce more complex ways toaugment data, with PuzzleMix reaching a notable 63.48%Top-1 accuracy on Tiny-ImageNet. Co-Mixup tops thesemethods on Tiny-ImageNet with 64.15% Top-1 accuracybut does not maintain this lead on CIFAR-100. Guided-APpushes the performance boundaries further by achieving su-perior accuracies among its predecessors, e.g., 81.20% Top-1 accuracy on CIFAR-100. DIFFUSEMIX, our proposed method, which surpasses allprior techniques by securing the highest accuracies: 65.77%Top-1 on Tiny-ImageNet and 82.50% Top-1 on CIFAR-100.Our approach not only surpasses the conventionalmixup strategies but also sets a new standard in enhanc-ing the generalization of deep learning models. The per-",
  "DIFFUSEMIX78.6495.32": "formance of DIFFUSEMIX stays consistent across the com-pared datasets, underlining its superior capability and effi-ciency.In , we provide a comparison of various meth-ods in terms of Top-1 and Top-5 accuracies on ImageNet,specifically when training ResNet-50 as per the trainingconfiguration of in Kang and Kim .It starts withthe baseline Vanilla ResNet model, showing accuracies of75.97% for Top-1 and 92.66% for top-5.Various tech-niques, including Azizi et al., AugMix, Manifold, Mixup,CutMix, Guided-SR, PixMix, PuzzleMix, GuidedMixup,",
  "FUSEMIX outperforms SOTA on 4 ofthe 5 distinct safety metrics.Loweris better except for anomaly detection.(SOTA method results are taken fromPixMix )": "82.15%, 41.73%, and 53.28%.SimSiam starts with ac-curacies of 86.93%, 48.34%, and 40.37%. Adding DIF-FUSEMIX as an augmentation method improves the perfor-mance to 89.24%, 49.17%, and 42.63%. This clearly il-lustrates that integrating DIFFUSEMIX significantly booststhe performance, demonstrating its effectiveness in enhanc-ing self-supervised learning models. The systematic gainsacross different datasets and on multiple methods highlightthe robustness of our approach and its potential to improvethe accuracies of different machine learning models.",
  ". Safety Measures": "showcases a comparative analysis of several dataaugmentation methods on the CIFAR-100 dataset, focusingon their performance across five different safety metrics.The methods evaluated include Mixup, CutMix, AugMix,Outlier, PixMix, and DIFFUSEMIX. The results highlightDIFFUSEMIXs superior performance, as it outperforms thestate-of-the-art (SOTA) previously established by PixMix infour out of the five categories. DIFFUSEMIX demonstratesbetter performance in cases of corruptions, consistency, ad-versaries, and calibration. In the case of Anomaly detectiontask, our approach demonstrates comparable performance.",
  ". Fractal Dataset": "We collected a dataset of 100 fractal images containingcomplex patterns and scales.Blending these images tothe training images introduces a level of abstraction andcomplexity not commonly found in regular training images.Some of the example fractal images are provided in Fig-ure 13. As discussed extensively in the manuscript, fractalblending in DIFFUSEMIX helps the network generalize bet-ter by adding contained noise or perturbations. The ablationstudies reported in our manuscript and supplementary sug-gest that utilizing fractal blending with the generated im-ages helps stabilizing the training and improves the overallconvergence.",
  "ford Cars, but a slight decrease in accuracy on the Flowerdataset.The baseline method shows performances of": "65.50% on CUB-Birds, 80.29% on Aircraft, 85.52% onCars and 78.34% on Flower102. The Mixup, CutMix, andPuzzleMix methods, when used without fractal, generallyshow higher accuracy than the baseline, especially on theStanford Cars and Aircraft datasets. However, the integra-tion of fractal blending with these methods leads to a sig-nificant drop in performance across all datasets, suggestingthat fractal blending may not be properly aligned with theseparticular augmentation techniques.In contrast, when fractals are blended with the hybrid im-ages (Hybrid Hiju) in our approach, performance improve-ments are notably observed in three of the four datasets in-cluding Aircraft, Stanford Cars, and Flower102. datasets,this combination leads to improvements in accuracy, indi-cating a positive synergy between the hybrid images andfractal blending. However, theres a slight decrease in accu-racy for the CUB-Birds dataset.",
  "(j) Europe Goldfinch Snowy": ". Original and DIFFUSEMIX augmented bird images from the Caltech-UCSD Birds-200-2011 dataset. Top row: displaysa selection of original, high-resolution bird images, capturing the natural beauty and diversity of species such as the eastern towhee,horned lark, rusty blackbird, white sparrow, and european goldfinch. Bottom row: demonstrates the augmented images obtained usingDIFFUSEMIX. The augmented images are visually striking and contextually varied representations of the original subjects.",
  "(j) Audi S5 Snowy": ". First row: showcases original images from the Stanford Cars benchmark dataset, featuring unaltered depictions of variouscar models including a lamborghini, audi R8, bentley, ford edge and audi S5. Second row: presents the images transformed using ourDIFFUSEMIX method. The effects of prompts are visible in the generated portions of the images. For example, lamborghini is changed togreen when aurora prompt is applied, creating a vibrant image. The front side of audi R8 becomes more color-rich when it is generated withrainbow prompt. The ambiance (background context) of bentley transforms significantly when autumn prompt is used. Similar diversetransformations are observed in other examples. These augmented images demonstrate the capability of DIFFUSEMIX in generatingvisually enriched augmented images for better generalization.",
  "(j) A330-300 Autumn": ". Illustration of original and DIFFUSEMIX augmented Aircraft images from the FGVC-Aircraft benchmark dataset. Top row:presents original aircraft images, each portraying a distinct airplane including the 737 200, 727 200, 737 700, 777 200, andA330300. These images highlight the design resemblance of various aircraft models, serving as a challenging resource for aircraft fined-grained image classification studies. Bottom row: showcases the augmented images obtained using DIFFUSEMIX for each correspondinginput image. As seen, DIFFUSEMIX reimagined each aircraft with unique prompts such as sunset, autumn, snowy and ukiyo resulting in arich visual appearance with diverse contexts. This also illustrates how image augmentation can be used to simulate different environmentaland stylistic scenarios, potentially enhancing the robustness and versatility of the dataset for training robust neural networks."
}