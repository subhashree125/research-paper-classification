{
  "Abstract": "Model stealing (MS) involves querying and observing theoutput of a machine learning model to steal its capabilities.The quality of queried data is crucial, yet obtaining a largeamount of real data for MS is often challenging. Recentworks have reduced reliance on real data by using gener-ative models. However, when high-dimensional query datais required, these methods are impractical due to the highcosts of querying and the risk of model collapse. In thiswork, we propose using sample gradients (SG) to enhancethe utility of each real sample, as SG provides crucial guid-ance on the decision boundaries of the victim model. How-ever, utilizing SG in the model stealing scenario faces twochallenges: 1. Pixel-level gradient estimation requires ex-tensive query volume and is susceptible to defenses. 2. Theestimation of sample gradients has a significant variance.This paper proposes Superpixel Sample Gradient stealing(SPSG) for model stealing under the constraint of limitedreal samples. With the basic idea of imitating the victimmodels low-variance patch-level gradients instead of pixel-level gradients, SPSG achieves efficient sample gradient es-timation through two steps. First, we perform patch-wiseperturbations on query images to estimate the average gra-dient in different regions of the image. Then, we filter thegradients through a threshold strategy to reduce variance.Exhaustive experiments demonstrate that, with the samenumber of real samples, SPSG achieves accuracy, agree-ments, and adversarial success rate significantly surpassingthe current state-of-the-art MS methods. Codes are avail-able at attack.",
  "*Corresponding author is Xiaoheng Deng": "63, 67, 72, 73] involves constructing a proxy model similarto the victim model by acquiring query results of input sam-ples. Beyond the direct utilization of the proxy model, mali-cious users can also generate a series of attacks based on theproxy model and transfer them to the victim model, includ-ing membership inference attacks , adversarial attacks, and model inversion attacks , etc.The basic paradigm of MS is to construct a sample at-tack set and train a proxy model through the samples in theattack set and the corresponding query results of the vic-tim model. Data-free MS is based on generative networksand uses noise to synthesize artificial images for the train-ing of the proxy model. Although data-free MS claims thatreal samples are not needed, in practical applications thatrequire high-dimension samples, data-free MS still has theinevitable real sample demand. As shown in , onthe one hand, high-dimensional inputs would significantlyincrease the query volume for data-free MS. For color im-ages of 224x224 pixels, the query volume for data-free MScould reach tens of millions. On the other hand, due tothe inherent risk of model collapse in GANs , train-ing proxy models with data-free MS is prone to failure. Bygiving data-free MS a small amount (10k to 20k) of realsamples related to the domain of the victim model as imagepriors, data-free MS can reduce the query cost and modelcollapse risk and improve the stealing effect. However, ac-quiring high-quality real samples that meet specific MLaaSrequirements is challenging. Different MLaaS have varyingrequirements for input images, and blurred input images candistort MLaaS query results. Privacy and copyright protec-tions further complicate the acquisition of high-quality realsamples. Additionally, even with a plethora of real sam-ples, the marginal benefit of each real sample for improvingthe proxy model diminishes as the cardinality of real sam-ples increases. Therefore, expensive real samples should befully utilized.To make full use of each real sample, our idea is to obtainmore information about the model from real samples. How-ever, for black-box models, it is challenging to obtain othertypes of information to train proxy models. For example, all",
  "arXiv:2406.18540v1 [cs.CV] 18 May 2024": ". Results of data-free MS without using real samples and with using 10k domain-relevant real samples. Results are queries, realsamples, the failure times, and test accuracy (in %), of each method with querying probability. The failure times are determined by thenumber of model collapses observed over 10 training runs. we report the other average result computed over 10 runs. The training strategiesand experimental configurations used are described in the experimental section 4. (1k=1000)",
  ". The columns from left to right are grad-CAM , grad-CAM++ , Smooth-gradCAM , X-gradCAM , layer-CAM , and SG-map. The neural network is ResNet34 pre-trained on ILSVRC-2012": "feature maps from the intermediate layers of the black-boxmodel are unknowable. Our focus shiftsto the input of the model, specifically, the Sample Gradi-ent (SG) backpropagated from the models output layer tothe input sample. SG is used to assist in generating the in-terpretability heat map of the model in many interpretabil-ity works. Actually, SG itself contains a lot of model in-terpretability decision information. As shown in ,we compare the heat map generated by average pooling SGwith other CAM methods, and we can see that SG fullyreflects the models decision. However, the obstacles forimitating sample gradients in MS include: (1) The mostprimitive method to obtain sample gradients involves per-turbing each pixel individually and acquiring query resultsfor all perturbed images, a process with enormous querycosts. Specially, obtaining sample gradients for a single im-age requires over a hundred thousand queries. Furthermore,inputting perturbed images at the pixel level into the victimmodel is akin to inputting adversarial images, which couldbe easily detected and thwarted by defense mechanisms likePrada . (2) Sample gradients have significant variancedue to certain specific or redundant neurons backpropagat-ing.Then, we introduce a novel SuperPixel Sample Gradient Model stealing (SPSG) to solve the issues.SPSG comprises two modules: superpixel gradient query- ing (SPGQ) and sample gradient purification (SGP). For is-sue (1), SPGQ module first segments the image into multi-ple superpixels based on a segmentation algorithm. Then,it applies perturbations to these superpixels and queries theoutput to obtain the sample superpixel gradients. For is-sue (2), SGP module eliminates significant variance fromthe sample gradients by filtering extremum information andnormalizing, ensuring the extraction of clean and useful gra-dient information. Then, purified superpixel gradients areassociated with the pixel gradients of the proxy model totrain proxy models.Our contributions are enumerated as follows: We design SPSG to extract the maximum amount of avail-able model information from each real sample. The su-perpixel querying module significantly reduces the queryvolume required to acquire the tacit knowledge in onesample gradient from 106 to 102 while simultaneouslyevading defenses like Prada . Meanwhile, the gra-dient purification module effectively removes noise fromthe sample gradients. The effectiveness of the gradientpurification module is further validated through ablationexperiments. Through various experiments, SPSG enables the proxymodel to achieve accuracy, agreement, and attack suc-cess rate substantially surpassing state-of-the-art algo-rithms with the same number of real samples. Specifi-cally, when stealing a resnet34 model trained on CUBS-200 using 20,000 real samples, SPSG achieves an accu-racy of 61.21% and an agreement of 67.48%, significantlyoutperforming the second-best method with an accuracyof 56.39% and agreement of 58.44%. To match the ac-curacy achieved by SPSG with 10,000 real samples, thesecond-best method requires at least 20,000 real samples.",
  ". Sample Gradient": "Sample gradients, obtained through the backpropagation ofa models final loss function, depend on the parameters andstructure of the neural network. They are primarily used inadversarial training and model interpretability.Adversarial Training Based on Sample Gradients. By introducing small perturbations along the direction of sam-ple gradients, new samples capable of deceiving the modelcan be generated. Techniques like FGSM are sensitiveto the sign of sample gradients, whereas FGM normal-izes the gradients. PGD and FreeAT implementmultiple iterations with smaller step sizes on sample gra-dients, keeping the perturbations within a specified range.YOPO reduces gradient calculation costs by leverag-ing the networks structure, with perturbations related onlyto the first layer. FreeLB accumulates gradients duringtraining, giving perturbations a more directional tendency.Model Interpretability Based on Sample Gradi-ents. Techniques like Saliency Map create saliencymaps by calculating gradients of input images to high-light the models focus areas in image classificationtasks. Guided Backpropagation helps understand thedecision-making process and the features learned by eachconvolutional layer. GRAD-CAM and SmoothGrad, while not directly using sample gradients, generateheatmaps using feature map gradients to show the focusedimage areas. proposes a method for explaining black-box models through input perturbations, generating inter-pretability masks to illustrate the models focus areas.",
  ". Model Stealing": "Data-Free Model Stealing. Data-free model stealing tech-niques do not require any orig-inal training data. Attackers generate synthetic queries, of-ten through prior knowledge or assumptions about the datadistribution, to probe the model and reconstruct its function-ality. All data-free model stealing (MS) inevitably draws onthe concept of Generative Adversarial Networks (GANs).Therefore, there is an unavoidable risk of model collapse.Given that the querying cost required for a single instanceof model theft is quite high, a collapse during the extractionprocess would further increase the querying costs. Addi-tionally, since the training and querying phases in data-freeMS are coupled, each training of a proxy model requires anew round of queries, which also increases the query vol-ume.Data-Driven Model Stealing. Data-driven model steal-ing attacks , utilize real data, allowing for allattack set samples to be queried before training the proxymodel. Therefore, it is not necessary to query the victimmodel again with each training of a new proxy model. The real samples can be domain-irrelevant to the victim model.Although domain-relevant real samples can achieve cer-tain improvements in the effectiveness of the theft, domain-irrelevant real samples are more commonly used.Data-driven MS can also further refine the selection of real sam-ples based on information from the training process of theproxy model to enhance the stealing effect.",
  ". Overview": "In the fundamental paradigm of offline Model Stealing(MS), MS begins with the construction of a query set com-prising all input samples and their corresponding query re-sults. Subsequently, this pre-assembled query set is utilizedto train different proxy models. SPSG, falling under thecategory of offline MS, mainly encompasses two distinctmodules: SuperPixel Gradient Query (SPGQ) and SampleGradient Purification (SGP), as shown in . SPGQis designed for the assembly of the query set. In addition toacquiring the predictive probabilities or hard labels for eachsamples output, SPGQ is also adept at obtaining superpixelgradients for each sample at a low query cost, while simul-taneously circumventing adversarial attack monitoring. Onthe other hand, SGP is employed for the training of theproxy model. Within this module, the superpixel samplegradients from the query set undergo a denoising process.This process ensures the retention of the extremal portionsof the superpixel gradients across each channel of the im-age. Based on every superpixel range filtered by the vic-tim model, the pixel gradients of the proxy model are av-eraged to obtain the simulated superpixel gradients. In thefinal stage, we introduce a novel loss function that estab-lishes a connection between simulated superpixel gradientsand their ground-truth. Through this connection, the proxymodel is effectively trained, culminating in a comprehen-sive and robust offline MS framework.",
  ". SuperPixel Gradient Query": "For black-box models, the finite difference method for cal-culating pixel gradients of input images is a primitive yeteffective approach. The finite difference method estimatesgradients by applying a small perturbation to the input sam-ple and observing the resultant output changes. Finite dif-ference includes forward difference, central difference, andbackward difference. In this paper, we default to using theforward difference. Specifically, for an input sample x anda small perturbation , an approximation of the gradient foreach pixel i in channel c = 123 can be calculated usingthe following formula:",
  "Update": "Proxy model . Four steps of SPSG. The first step is to obtain superpixel gradients and query results through SGPQ. The second step involvesacquiring pixel gradients and output logits of the proxy model through backpropagation on the input sample. The third step is to obtainpurified superpixel gradients and simulated superpixel gradients of the proxy model using SGP. The fourth step involves updating the proxymodel based on the loss function. The gray arrow represents the direction from input to output. Here, must be sufficiently small to capture the functionsvariations at x, yet not too small to avoid numerical preci-sion issues. In this paper, the value of is set to 1e 5. eiis a standard basis vector with only the i-th pixel in chan-nel c as 1.However, pixel-level forward differences re-quire one perturbed query per input dimension, renderingthis method computationally expensive in high-dimensionalinput spaces. In addition, MLaaS can track and store a se-ries of pixel-level perturbed images of the input. Under nor-mal circumstances, the pairwise distance between queriedimages with the same label follows a Gaussian distribution.However, with the addition of tiny perturbations, the pair-wise distance no longer follows a Gaussian distribution andtends to an extreme distribution. Prada can detect suchchanges in distribution and outputs a noisy prediction resultupon detection of change.Therefore,we propose SuperPixel Gradient Query(SPGQ), whose core idea is to extend the pixel-level for-ward difference to the superpixel level. Superpixels are aconcept in image processing and computer vision, referringto the technique of combining similar pixels into a unifiedregion or cluster. Each superpixel Pj = Njipi contains ad-jacent pixels pi of Nj number, similar in color, brightness,texture, or other attributes. SPGQ performs forward dif-ferences on three different channels of each superpixel in-dependently. For all pixels within the same channel of each",
  ". Sample Gradient Purification": "Sample gradients have a significant variance, and numer-ical discrepancies between different models can be inher-ited through the backpropagation process, manifesting inthe sample gradients. superpixel gradient also has the samedrawbacks. Hence, we have formulated a Sample GradientPurification Mechanism to mitigate the interference fromvariance and other extraneous factors.Concerning the gradient of every queried superpixelchannel Gc = Jj gcj, we initially perform a denoising op-eration. The core objective of denoising is to preserve theextreme values of the sample gradients, eliminating non-extreme gradient values, as the extreme portions encapsu-late the focal points of the model. Additionally, due to thedivergent implications of positive gradients (indicating fa-cilitation of the loss function) and negative gradients (in-dicating impediment), it becomes imperative to indepen-dently execute denoising operations on the sets of posi-tive and negative gradients. We commence by calculatingthe extreme values of every channels positive and nega-tive gradients: gc+ = maxgcj {gcj|gcj 0}and gc =maxgcj {gcj|gcj < 0}. The filtered gradients are givenby:",
  "(4)": "The gradient values closer to 1 signify a higher degree of fa-cilitation or impediment of the models loss function at thatparticular pixel position. Following denoising and normal-ization, we obtain the purified sample gradients.The role of SGP is crucial. We demonstrate the effective-ness of SGPs purification in the subsequent ablation study.",
  ". Objective Function for Training": "After obtaining the superpixel level gradient of an imagein the victim model fv, the proxy model fs cannot directlymimic and learn the dark knowledge of the superpixel gra-dient. Firstly, we use a function similar to the one used fortraining the victim model to calculate the sample pixel gra-dients of the proxy model. Regardless of whether the queryresult is hard labels or probabilities, we use a cross-entropyfunction similar to that used in training the victim model tocalculate the sample gradients for the proxy model :",
  "f(y, p) = log(yp)(5)": "For equation 5, y is the proxy models K-dimensional vec-tor output. p is the victim models predicted label. There-fore, for the pixel-level gradients Q = JjNjiqi =f (y, p)/x obtained through backpropagation in thewhite-box proxy model, we adopt a mean coverage methodto obtain the same format of the simulated-superpixel gra-dient. Specifically, based on the superpixel partition of thevictim model, we take the mean of all pixel gradients withinthe same superpixel in the proxy model, replace the originalpixel gradients, and obtain the simulated superpixel gradi-ent:",
  "yp log (y) log(yp)if of hard-label Kk (yk) log (yk) if of probability(7)": "Our gradient loss function Lgrad is set in two parts: the firstpart ensures similarity in the gradient values for each super-pixel, and the second part ensures similarity in the overallgradient of the sample. For the first part, fv and fs hav-ing similar gradients for each superpixel is equivalent to thequery results of the image with the perturbed superpixel be-ing similar. Define xcj = x + Ecj and Gcall = Gc+ Gc.Then, we can get:",
  ". Experiment Setup": "Victim Model. We employ four datasets used in Knock-off for our experimentation:Caltech256 (256 classes), CUB-200-2011 (200 classes) , Indoor Scenes (67classes) , and Diabetic Retinopathy (5 classes) . ForDiabetic Retinopathy, we strip 200 images from the trainingset for each category, forming a test set that in total contains1000 images. A resnet34 model trained on these fourdatasets serves as our victim model. The training proce-dure for the victim model mimics that of Knockoffs victimmodel. Specifically, the model is trained for 200 epochsusing an SGD optimizer with a momentum of 0.5 and aninitial learning rate of 0.1 that decays by a factor of 0.1every 60 epochs. The well-trained victim model is avail-able for download at Knockoff Code. The victim model hasachieved accuracies of 78.4%, 77.1%, 76.0%, and 59.4% onthe datasets mentioned sequentially above.Baseline and Attack Dataset. Baselines are categorizedbased on the necessity of real data. Data-free baselines in-clude DFME , DS , DFMS , EDFBA , andZSDB3KD , while Knockoff , ActiveThief ,Black-Box Dissector , and InverseNet are Data-driven baselines that require real data.Notably, DFMS,EDFBA, ZSDB3KD, DS, Black-Box Dissector, inverseNet,ActiveThief, and Knockoff remain functional even when . The agreement (in %), test accuracy (in %), and queries of each method with querying probability or hard label. For our model,we report the average result as well as the standard deviation computed over 10 runs. (Boldface: the best value.)",
  "AgreementAccQueriesAgreementAccQueriesAgreementAccQueries": "ZSDB3KD24.4523.561299k26.3326.011098k31.3129.78931kDFMS26.2125.111331k29.3128.331003k31.2430.18939kEDFBA25.3423.54559k27.6126.43490k31.1230.72431kDS24.5623.531237k27.3426.781191k30.6429.78902kknockoff21.1119.2710k24.4922.7615k26.3325.9220kActiveThief23.2122.8910k26.5525.1615k27.1826.9620kBlack-Box Dissector25.9123.57150k27.4326.26220k31.5930.46300kInverseNet26.0124.07150k26.9326.12220k31.4330.97300kSPSG(Ours)26.780.1625.420.23132k0.01k29.970.3429.840.52195k0.01k34.660.2134.120.19271k0.01k the query results are hard labels. For CUB-200-2011 andIndoor Scenes, we conduct comparisons in data-free MS.Even though data-free MS does not necessitate real data,we employ publicly available, potentially related real im-ages as weak image priors for the generator for fairness incomparison. Specifically, for Indoor Scenes, We use indoorscene images from SUN , which are distinct from In-door Scenes categories. For CUB-200-2011, We utilize birdimages whose classes are included in NAbird but notpresent in CUB. In Caltech256 and Diabetic Retinopathy,comparisons are made for Data-driven MS. Among them,the number of queries for InverseNet and Black-Box Dis-sector is not determined by the number of real samples. Toensure fairness, we set the query volume for these two meth-ods slightly higher than that of SPSG. ILSVRC-2012 training set, consisting of approximately 12 million images,serves as our attack dataset in Data-driven MS. Moreover,we ensure the consistency of real samples across differentmethods.",
  "Training Paradigm and Evaluation Metrics. Train-": "ing paradigms are categorized based on the usage of datagenerators. Generally speaking, only data-free MS necessi-tates the use of a generator. Under the generator paradigm,the Generator is configured as BigGAN and trained usingAdam with a learning rate of 0.001, 1 = 0.5, and 2 =0.999. The batch size of BigGAN is 128. For data-drivenMS, proxy models are trained from scratch on the attackdataset using SGD with a momentum of 0.5, a learning rateof 0.01 (decaying by a factor of 0.1 every 60 epochs), 200epochs, and a batch size of 64. Evaluation is based on theaccuracy of the proxy model on the corresponding test setand the similarity in predictions between the proxy and vic-tim models. We also report the success rate of adversarialattacks on the victim model as indicators of the transferabil-ity of the proxy model. The superpixel segmentation for ourmethod defaults to quickshift .",
  "We evaluate the accuracy and agreement of the proxy mod-els generated by data-free baselines and our algorithm under": "scenarios with 10k, 15k, and 20k real samples. As illus-trated in , our method almost outperforms all otherMS methods across both metrics. While the performanceof data-free MS plateaus with increasing numbers of realsamples, our method demonstrates less pronounced dimin-ishing returns. Additionally, we document the query vol-ume required by all algorithms. Our methods query vol-ume is determined by the number of queries consisting ofan image and its perturbed versions using different super-pixels, whereas the query volume for data-free MS dependson when the model converges. Our method requires signifi-cantly fewer queries than data-free MS, with our query vol-ume being approximately 25% of that required by the mostefficient data-free MS. Notably, despite the use of weak im-age priors, DFMS and ZSDB3KD sometimes fail to train,severely limiting their practical applicability.",
  ". Experiment Results with Data-driven MS": "We assess the accuracy and agreement of the proxy mod-els generated by SPSG and other data-driven MS meth-ods under scenarios with 10k, 15k, and 20k real samples.As shown in , while our method does not min-imize query volume, it yields substantial improvements.The marginal effects of data-driven MS are not pronouncedacross the 10k to 20k real samples range. Consequently, weobserve the performance across a broader range of real sam-ple numbers (100k-200k), finding that our algorithm con-sistently outperforms others in terms of accuracy. Specifi-cally, as shown in and 4, at 140k real samples forDiabetic Retinopathy, Knockoff achieves its peak accuracyof 54.7%, whereas our algorithm surpasses this accuracy at130k real samples.",
  ". Experiment Results in Hard-label Query": "We evaluate all algorithms functional under hard-labelqueries, where the query not only returns the predicted labelbut also the associated confidence. We argue that providingusers with the confidence associated with the predicted la-bel is more practical for MLaaS. Despite high confidencenot necessarily equating to accuracy, low confidence allowsusers to disregard the models prediction. presentsour results on CUB-200-2011, showcasing our algorithmssuperiority with 10k or more real samples.",
  ". Transferability of Adversarial Samples": "We assess the transferability of adversarial samples gener-ated on CUBS-200-2011 test set. The evaluation encom-passes the success rates of adversarial attacks generatedfrom different methods (FGSM, BIM, PGDK), with a per-turbation bound of =10/255 and a step size of =2/255.The adversarial attacks in our experiments are untargetedattacks. In untargeted attacks, adversarial samples are gen-erated only on images correctly classified by the attacked model. As demonstrates, the adversarial samplesgenerated by our proxy model exhibit higher transferabilityto the victim model, affirming the practical applicability ofour method in real-world scenarios.",
  ". Model Stealing in Real-World Scenarios": "We trained a model on the Oxford 102 Flowers dataset using the Microsoft Custom Vision service and desig-nated it as a black-box victim model. The ILSVRC-2012dataset served as the attack dataset, with the inference re-sults of the Oxford 102 Flowers test set used as the met-ric. Hyperparameter settings were consistent with previ-ous experiments.The victim models test accuracy was86.34%. As demonstrates, compared to the second-best methods using 30k real samples, our method showed a4.17% increase in test accuracy for the proxy model. Thisresult indicates that our method possesses stronger practicalapplicability in real-world scenarios.",
  ". Ablation Study": "Resistance to Prada. We document the monitoring of thefinite difference query and SPGQ by Prada, represented bythe distribution of image distances. As shown in , Finite difference queries are completely detectable byPrada, exhibiting a significant deviation from the Gaussiandistribution. In contrast, superpixel queries initially have afew detectable instances but subsequently evade detection",
  ". The first row shows the superpixel SG heatmap for dif-ferent segment methods, while the second row shows the purifiedsuperpixel SG map for different segment methods": "Superpixel Segmentation. We employed the superpixelgradients obtained through queries under quick-shift ,felzenszwalb , slic , and grid segmentation methods.Experimental results in indicate that the more su-perpixels used, the more apparent the effect of model steal-ing becomes. However, regardless of the number of grid pixels divided, the grid segmentation method demonstrateda poor stealing effect. This is attributed to the grid seg-mentations disregard for image attributes such as textureand color, which are closely associated with the modelsdecision-making process.Impact of SGP. We conduct experiments on CUB-200-2011 dataset to compare the performance of SPSG with-out SGP and the complete SPSG. The experimental resultsshown in reveal a significant degradation in theeffectiveness of SPSG when SGP is omitted. This declinecan be attributed to the retention of gradient variance, whichproves to be particularly detrimental to dark knowledge ex-traction. In Supplementary Material, we further explore theapplications of SGP, including knowledge distillation.",
  ". Conclusion": "SPSG significantly outperforms existing MS algorithmsacross various datasets, demonstrating its effectiveness evenin hard-label query scenarios. The success of SPSG in ad-versarial attacks showcases its practical utility, while itscapability to evade Prada highlights its stealthiness.Inessence, SPSG provides a novel approach to enhancing MSperformance by effectively mimicking additional informa-tion from victim models. We hope our proposed methodwill encourage proactive measures to protect models againstunauthorized access and theft.Acknowledgments This work was supported by theNational Natural Science Foundation of China Project(62172449, 62372471, 62172441), the Joint Funds forRailway Fundamental Research of National Natural Sci-ence Foundation of China (Grant No.U2368201), spe-cial fund of National Key Laboratory of Ni&Co Associ-ated Minerals Resources Development and ComprehensiveUtilization(GZSYS-KY-2022-018, GZSYS-KY-2022-024),Key Project of Shenzhen City Special Fund for Fundamen-tal Research(JCYJ20220818103200002), the National Nat-ural Science Foundation of Hunan Province(2023JJ30696),and the Science Foundation for Distinguished Young Schol-ars of Hunan Province (NO. 2023JJ10080).",
  "Radhakrishna Achanta, Appu Shaji, Kevin Smith, AurelienLucchi, Pascal Fua, and Sabine Susstrunk. Slic superpixels.Technical report, 2010. 2, 8": "Radhakrishna Achanta, Appu Shaji, Kevin Smith, AurelienLucchi, Pascal Fua, and Sabine Susstrunk. Slic superpix-els compared to state-of-the-art superpixel methods. IEEEtransactions on pattern analysis and machine intelligence,34(11):22742282, 2012. 3 Antonio Barbalau, Adrian Cosma, Radu Tudor Ionescu, andMarius Popescu. Black-box ripper: Copying black-box mod-els using generative evolutionary algorithms. Advances inNeural Information Processing Systems, 33:2012020129,2020. 1, 3",
  "Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256object category dataset. 2007. 5": "Dongming Han, Jiacheng Pan, Rusheng Pan, Dawei Zhou,Nan Cao, Jingrui He, Mingliang Xu, and Wei Chen. inet:visual analysis of irregular transition in multivariate dynamicnetworks. Frontiers of Computer Science, 16:116, 2022. 1 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE Conference on Computer Vision and PatternRecognition, pages 770778, 2016. 5, 3, 4 Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, No-jun Kwak, and Jin Young Choi. A comprehensive overhaulof feature distillation. In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision, pages 19211930,2019. 3, 6 Byeongho Heo, Minsik Lee, Sangdoo Yun, and Jin YoungChoi.Knowledge transfer via distillation of activationboundaries formed by hidden neurons.In Proceedings ofthe AAAI Conference on Artificial Intelligence, pages 37793787, 2019. 2",
  "Tao Huang, Shan You, Fei Wang, Chen Qian, and ChangXu. Knowledge distillation from a stronger teacher. arXivpreprint arXiv:2205.10536, 2022. 3, 6": "Ziqi Jiang, Li Zhang, Chuang Zhang, Chunxia Li, Fei Li, andKuiyuan Yang. Layercam: Exploring hierarchical class ac-tivation maps for localization. IEEE Transactions on ImageProcessing, 30:58755888, 2021. 2 Mika Juuti, Sebastian Szyller, Samuel Marchal, and NAsokan. Prada: protecting against dnn model stealing at-tacks. In 2019 IEEE European Symposium on Security andPrivacy (EuroS&P), pages 512527. IEEE, 2019. 2, 4 Sanjay Kariyappa and Moinuddin K Qureshi.Defendingagainst model stealing attacks with adaptive misinformation.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 770778, 2020. 2",
  "Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiplelayers of features from tiny images. 2009. 3": "Seung Hyun Lee, Dae Ha Kim, and Byung Cheol Song. Self-supervised knowledge distillation using singular value de-composition. In Proceedings of the European conference oncomputer vision (ECCV), pages 335350, 2018. 2 Li Liu, Qingle Huang, Sihao Lin, Hongwei Xie, Bing Wang,Xiaojun Chang, and Xiaodan Liang. Exploring inter-channelcorrelation for diversity-preserved knowledge distillation. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 82718280, 2021. 3, 6",
  "Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz.Prediction poisoning: Towards defenses against dnn modelstealing attacks. arXiv preprint arXiv:1906.10908, 2019. 2": "Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz.Knockoff nets: Stealing functionality of black-box models.In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 49544963, 2019. 1, 3,5, 2 Soham Pal, Yash Gupta, Aditya Shukla, Aditya Kanade,Shirish Shevade, and Vinod Ganapathy. Activethief: Modelextraction using active learning and unannotated public data.In Proceedings of the AAAI Conference on Artificial Intelli-gence, pages 865872, 2020. 1, 3, 5, 2",
  "Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho.Relational knowledge distillation.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 39673976, 2019. 3, 6": "Peyman Passban, Yimeng Wu, Mehdi Rezagholizadeh, andQun Liu.Alp-kd:Attention-based layer projection forknowledge distillation. In Proceedings of the AAAI Confer-ence on artificial intelligence, pages 1365713665, 2021. 2 Yanni Peng, Xiaoping Fan, Rong Chen, Ziyao Yu, Shi Liu,Yunpeng Chen, Ying Zhao, and Fangfang Zhou.Visualabstraction of dynamic network via improved multi-classblue noise sampling. Frontiers of Computer Science, 17(1):171701, 2023. 1",
  "puter Vision and Pattern Recognition, pages 1528415293,2022. 1, 2, 3, 5": "Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.Grad-cam:Visual explanations from deep networks viagradient-based localization. In Proceedings of the IEEE in-ternational conference on computer vision, pages 618626,2017. 2, 3, 1 Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi,Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis,Gavin Taylor, and Tom Goldstein. Adversarial training forfree! Advances in Neural Information Processing Systems,32, 2019. 3 Reza Shokri, Marco Stronati, Congzheng Song, and VitalyShmatikov. Membership inference attacks against machinelearning models. In 2017 IEEE symposium on security andprivacy (SP), pages 318. IEEE, 2017. 1 Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, andAnshul Kundaje. Not just a black box: Learning importantfeatures through propagating activation differences. arXivpreprint arXiv:1605.01713, 2016. 1",
  "YonglongTian,DilipKrishnan,andPhillipIsola.Contrastive representation distillation.arXiv preprintarXiv:1910.10699, 2019. 3, 6": "Jean-Baptiste Truong, Pratyush Maini, Robert J Walls, andNicolas Papernot. Data-free model extraction. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, pages 47714780, 2021. 1, 2, 3, 5 Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber,Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge Be-longie.Building a bird recognition app and large scaledataset with citizen scientists: The fine print in fine-graineddataset collection. In Proceedings of the IEEE conferenceon computer vision and pattern recognition, pages 595604,2015. 6",
  "C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.Technical report, 2011. 5": "Wenxuan Wang, Bangjie Yin, Taiping Yao, Li Zhang, Yan-wei Fu, Shouhong Ding, Jilin Li, Feiyue Huang, and Xi-angyang Xue. Delving into data: Effectively substitute train-ing for black-box attack. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 47614770, 2021. 1, 3 Yixu Wang, Jie Li, Hong Liu, Yan Wang, Yongjian Wu,Feiyue Huang, and Rongrong Ji. Black-box dissector: To-wards erasing-based hard-label model stealing attack.InEuropean Conference on Computer Vision, pages 192208.Springer, 2022. 3, 5, 2",
  "Zi Wang. Zero-shot knowledge distillation from a decision-based black-box model. In International Conference on Ma-chine Learning, pages 1067510685. PMLR, 2021. 1, 2, 3,5": "Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Tor-ralba, and Aude Oliva. Sun database: Exploring a large col-lection of scene categories. International Journal of Com-puter Vision, 119:322, 2016. 6 Jing Yang, Brais Martinez, Adrian Bulat, Georgios Tz-imiropoulos, et al. Knowledge distillation via softmax re-gression representation learning. International Conferenceon Learning Representations (ICLR), 2021. 3, 6 Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. Agift from knowledge distillation: Fast optimization, networkminimization and transfer learning. In Proceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 41334141, 2017. 3, 6",
  "Huang Zhizhong, Dai Mingliang, Zhang Yi, Zhang Junping,and Shan Hongming. Point, segment and count: A general-ized framework for object counting. In CVPR, 2024. 1": "Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,and Antonio Torralba. Learning deep features for discrimina-tive localization. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 29212929,2016. 1 Wen Zhou, Xin Hou, Yongjun Chen, Mengyun Tang, Xi-angqi Huang, Xiang Gan, and Yong Yang. Transferable ad-versarial perturbations. In Proceedings of the European Con-ference on Computer Vision (ECCV), pages 452467, 2018.1",
  ". Why does Sample Gradient contain modelinformation?": "Dark knowledge is a term used to describe knowledge thatis implicitly embedded in a model but doesnt manifest di-rectly, such as predictive logits. By learning dark knowl-edge, a surrogate model can inherit the characteristics andmimic the functionalities of the victim model.In priorwork, sample gradients have been interpreted as a reflec-tion of a models local sensitivity to a specific input, guidingthe perturbation direction in adversarial attacks. However,due to the inclusion of variance, instability, and a lack ofinterpretability, sample gradients are rarely considered as aform of dark knowledge. Numerous studies have attempted to utilize sample gradients to aidinterpretability, often through altering the model, employ-ing the gradient of the models feature maps, or introducingadditional inputs to propose interpretability methods. Thesemethods fall short of establishing the interpretability of theoriginal sample gradients. In this section, we introduce SG-Map, a method for interpreting sample gradients, designedwithout modifying the original model architecture, utilizingfeature maps, or adding any additional inputs. We demon-strate that the sample gradients, processed and visualizedas heatmaps, exhibit interpretability comparable to Grad-CAM. For an individual input image, after backpropagationthrough the loss function, each pixel is assigned a gradientvalue, collectively forming the sample gradient. The SG-Map algorithm initiates by preprocessing the sample gradi-ents: taking the absolute value of the gradient for each pixeland normalizing the pixels in each channel independently.This preprocessing ensures that the sample gradients meetthe requirements for image display and eliminates numeri-cal discrepancies in the sample gradients, which are tied tothe parameter values of all neurons in the model and do notaccurately reflect the models decision-making characteris-tics. Channel-wise normalization is preferred over whole-image normalization due to the more substantial inter-pixelconnections within channels than between them. SG-Mapthen combines the pixel gradients from the three differentchannels according to the specifications for a grayscale im-age, resulting in a single-channel sample gradient.In acrucial final step, we apply average pooling to this single-channel sample gradient, mitigating the impact of erraticbehaviors from specific instances of the model on the pixelgradients. The resulting sample gradient is then presented as a heatmap. The visualization result is shown in . Comparing SG-Map with CAM methods, we observethat SG-Map focuses on similar pixel locations, reflectingthe models sensitivity and attention allocation across dif-ferent areas. Unlike CAM methods, the visualization ofsample gradients through SG-Map provides a more strin-gent expression of sensitivity, manifesting as more concen-trated yet precise high-temperature areas in the heatmap.Our proposed SG-Map thereby conclusively demonstratesthat sample gradients encapsulate deep-seated informationof the model, qualifying as a form of dark knowledge thatcan guide the training of surrogate models.",
  ". Experiments on different Proxys architecture": "In the main text, we default to using the same proxy ar-chitecture as the victim, that is, resnet34. However, in prac-tice, we cannot obtain information about the victims model.Therefore, we experiment with different neural network ar-chitectures as proxys. The experimental results, as shownin , indicate that SPSG can effectively extract theperformance of the victim across various neural network ar-chitectures. Due to the different performance ceilings inher-ent to each neural network architecture, the results presentvarying degrees of difference.",
  ". Impact of hyperparameter": "is used to remove gradients outside of extreme values.The larger the , the more gradients are removed, indicat-ing a stricter selection of extremes. We conduct experimentsunder different values. When the value of is small,more gradient variance is introduced, leading to a decreasein the proxy models performance. When value is larger,there are hardly any superpixel gradients left. When is atits maximum, the sample gradient contains only one super-pixel gradient. When is greater than 0.8, the performanceof the proxy model remains at a lower level. This is be-cause at this point, what is left are the most representativesuperpixel gradients, so the performance of the proxy modelremains unchanged as it always mimics the most important",
  ". Offline Training of SPSG": "We observe the changes in simulated-superpixel gradientsof samples during the offline training process of the proxymodel, as shown in . As the training epochs in-crease, the similarity between the pseudo-superpixel gra-dients obtained from the mean of model backpropagationpixel SG and the superpixel gradients queried from the vic-tim model gets higher and higher. More similar sample gra-dients indicate that our loss function setting is reasonable.The proxy model sufficiently learns SG knowledge of thevictim model.",
  ". Impact of Sample Selection Strategy": "Our method does not conflict with sample selection strate-gies. Therefore, we compare the performance of SPSG,knockoff, and Black Dissector under two different sampleselection strategies. These two strategies are the Reinforce-ment learning strategy and the K-center strategy ,as shown in . Both strategies improve the perfor-mance of MS to varying degrees. SPSG achieves the highestaccuracy and similarity under both sample selection strate-gies. It is important to note that in the main text, we havealready found that SPSG also obtains the best performancecompared to other methods under a random sample selec-tion strategy.",
  ". Ability to evade the SOTA defense method": "We conducted experiments with protection measures sim-ilar to including Adaptive Misinformation , Pre-diction Poisoning , Gradient Redirection, ExternalFeature. The victim model is ResNet34 model trained on the CUBS-200-2011 dataset. The real sample number is20k. The attack set employed ILSVRC-2012. As shown in, SPSG demonstrates significantly higher resistanceto these two defenses compared to other methods.. The larger the threshold, the better the defense effect (0.0means no defense). False and True respectively correspond toevading monitoring and being detected by monitoring.",
  ". More study about SPGQ": "We investigate the effectiveness of SPGQ and finite dif-ference query methods. We compare the similarity of thesample gradients obtained by different methods to the realsample gradients. We use the average pair-wise distanceto each real sample gradient as the evaluation metric andrecord the average number of queries required to query asample. For superpixel gradients, we compare them by av-eraging the real samples within the corresponding superpix-els. As shown in , our method exhibits a smaller dis-tance compared to the finite difference method, indicating ahigher similarity.",
  ". More study about SGP": "Empirical validation of SGPs efficacy is demonstratedthrough two categories of experiments.On one hand,knowledge distillation experiments were conducted. Whenthe student model distills unpurified sample gradients andlogits knowledge, the resultant accuracy exhibits a declinedue to irregular variance in the sample gradients, as com-pared to training without distillation.Conversely, distil-lation using purified sample gradients in conjunction withlogits culminates in accuracy surpassing that achieved bydistilling logits alone. On the other hand, T-SNE visual-ization was employed, concatenating the model-extractedsample features with the purified sample gradients. Com-parative analysis reveals that purified sample gradients sig-nificantly enhance the final visualization outcome, as op-posed to scenarios involving no concatenation or concatena-tion with unpurified sample gradients. The aforementionedexperiments collectively attest to the effectiveness of the pu-rification mechanism in eliminating variance from samplegradients.",
  "We conducted experiments on image classification knowl-edge distillation. As shown in , the sample gradi-": "ents obtained from passing the samples through the teacherand student models are processed by SGP and then associ-ated through the loss function for distillation. We selectedICKD , Overhaul , AT , FitNet , and FSP, KD , RKD , DIST , SRRL , and CRD as the baselines. Initially, we compared the effects ofeach baseline with or without SGKD used individually inCIFAR100 . The training strategies for CIFAR100 isshown in . We selected a series of teacher-studentmodel combinations from VGG , ResNet , and theirvariants .CIFAR100 Experimental Results: As de-picted in , using SGP individually results in im-proved accuracy for student models. To further examinethe influence of SGP on the knowledge distillation task, wecompare the accuracy of the student model with and withoutSGP on the CIFAR100 dataset. As demonstrated in , the considerable numerical difference between the orig-inal sample gradients of the student and teacher hinders theaccurate transfer of the teacher models dark knowledge tothe student model. This results in a decrease in the studentmodels performance. With SGP, the student model can ef-fectively learn the teachers dark knowledge.",
  ". T-SNE visualization": "SGP allows pixel-level sample gradients to possess moreclass information. We first train a resnet34 on CIFAR-10.Then, we obtain the test sample feature vectors through thefinal layer before the output of resnet34. The feature vectorsare visualized using T-SNE. Next, we concatenate the puri-fied sample gradients or the original sample gradients be-hind the feature vectors and visualize them again. The fourvisualization results show that the sample gradients purifiedby SGP can effectively aid in classification. In contrast, theoriginal sample gradients, containing variance and havinglow informational content, provide no benefit to the repre-sentation of feature vectors. The result is shown in ."
}