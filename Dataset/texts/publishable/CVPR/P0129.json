{
  "Abstract": "Autonomous driving without high-definition (HD) mapsdemands a higher level of active scene understanding.In this competition, the organizers provided the multi-perspective camera images and standard-definition (SD)maps to explore the boundaries of scene reasoning capa-bilities. We found that most existing algorithms constructBirds Eye View (BEV) features from these multi-perspectiveimages and use multi-task heads to delineate road center-lines, boundary lines, pedestrian crossings, and other ar-eas. However, these algorithms perform poorly at the farend of roads and struggle when the primary subject in theimage is occluded. Therefore, in this competition, we notonly used multi-perspective images as input but also incor-porated SD maps to address this issue. We employed mapencoder pre-training to enhance the networks geometricencoding capabilities and utilized YOLOX to improve trafficelement detection precision. Additionally, for area detec-tion, we innovatively introduced LDTR and auxiliary tasksto achieve higher precision. As a result, our final OLUSscore is 0.58.",
  ". Introduction": "Mapless driving offers significant advantages for au-tonomous vehicles that do not rely on high-definition (HD)maps. It can adapt more quickly to real-world road changesand reduces the cost associated with manual map annota-tion. Therefore, addressing the challenges in this area hasimportant practical significance.This task involves taking multi-perspective images andstandard-definition (SD) maps as input, requiring not onlythe perception of lanes and traffic elements but also thetopology relationships among lanes and between lanes andtraffic elements. In this competition, we not only used aninnovative area prediction head borrowed from LDTR and auxiliary tasks from mapTRv2 , but also integrated the SD map into the BEV feature map and resolved the is-sue of abstract SD map embedding learning by introduc-ing a map encoder pre-training task. Lastly, we utilizedYOLOX to enhance traffic element detection capabili-ties. Our final metrics on the leaderboard were 0.58.",
  ". Ablation experiments on different sources of SD map onOpenLaneV2 validation set": "We encode maps from two SD map sources usingthe same network, and the final metrics are shown inTab. 1. So we utilized the open-source SD map from Open-StreetMap . This map provides a large number of cate-gories, a substantial amount of information, and compre-hensive prior knowledge about roads.Each frame inputinto the model consists of a localized view of the SD mapaligned with the vehicles camera position. We maintainthe structure of SMERF , as shown in , the maplines are encoded using sincos position encoding, whilethe category information is encoded with one-hot encoding.These features are concatenated and then processed througha Transformer encoder to learn map features.After sincos position encoding of each line segment, thedimension of the graph vector becomes:",
  "where Dh is the dimension of the hidden layer. The featuremap Gf is regarded as key and value, which is queried bythe constructed BEV features": "SD map has structural prior information. To enhancethe geometric structure encoding capability of the map en-coder, we propose pre-training the map encoder. As shownin Tab. 2, we compared the model with pre-trained weightsloaded into the map encoder against the model without theseweights. In our experiments, we used an AutoEncoder approachwhere the feature embedding encoded with sincos positionembedding was used as the ground truth. A lightweight de-coder was used to predict the embedding with L2 loss assupervision. This approach significantly improved the line-related metrics. We also experimented with pre-training us-ing the Masked AutoEncoder (MAE) method but foundno further improvement, possibly due to the default upsam-pling of 11 points per line.",
  ". Area Detection": "We utilized the training framework of mapTR forarea detection, working with BEV features that integratesurround-view and SD map information.However, wefound that the approach in mapTR of using key points asqueries weakened the overall integrity of instances. There-fore, drawing inspiration from LDTR , we employedan anchor-chain method to represent area instances and fur-ther optimized these instances holistically using MRDA andP2P IoU loss & cost. The results indicate a significant per-formance improvement compared to mapTR.",
  ". Ablation experiments of YOLOX on OpenLaneV2 vali-dation set": "We utilized YOLOX as our 2D object detectionmodel, achieving higher quality bounding boxes. As shownin Tab. 3, throughout the training process, we experimentedwith various data augmentation techniques, data resam-pling, and pseudo-labeling methods, etc. Ultimately, we im-proved our metrics from a baseline of 0.55 to 0.73, markinga 33% enhancement.Firstly, we found that 40% of the training data lacks theobjects to be detected. Due to the sparsity of foregroundsamples, many images do not contain the target objects.To enable the model to learn more useful features, we em-ployed two different data augmentation methods: mosaicand mixup.Then, we incorporated the PAFPN structure to fusemulti-scale features, which can address the sparsity issue oflow-level features.We found that the sample counts were unbalanced.Training samples for objects such as yellow lights, U-turns,and no U-turn signs were relatively scarce. To balance thenumber of positive examples for each detectable category,we performed data resampling and increased the classifica-tion weight for difficult samples.We found that the length and width distribution of thebounding boxes is concentrated between 0 and 200, with anuneven size distribution. Smaller bounding boxes accountfor a larger proportion, while some larger-sized boundingboxes have ground truth values exceeding 200. To addressthis, we introduced Test Time Augmentation (TTA) and ap-plied scale transformations to the inference images, ensur-ing good detection performance for objects of varying sizes.Finally, to provide the detection model with more train-ing data and enhance its generalization, We have annotatedthe validation set using pseudo-labels and included it in thetraining. This allowed the model to achieve better detectionperformance across the 13 classes of objects.",
  ". Lane-Traffic Topology": "To improve topology metrics, we fine-tuned the topologytask head while keeping the rest of the network frozen. Weimported the object detection results from YOLOX foreach frame. By providing high-quality bounding boxes, weenhanced the quality of the detection features using sincosposition encoding and the bipartite matching results com-pared with the original Deformable DETR . This, inturn, improved the prediction quality in the online inferenceof lane-traffic topology relationships, leading to a signifi-cant increase in the final precision.",
  ". Implementation Details": "We experimented with several backbone networks, includ-ing ResNet-50 and InternImage-L . Our learningrate was 2e-4, the optimizer was AdamW , and the totalnumber of training epochs was 48 epochs.For traffic element detection, our image resolution is1550*2480.For the SD map, we achieved better weight initializa-tion by loading the pre-trained map encoder using Open-StreetMap .For topological reasoning, the learning rate was 4e-4.We froze the other parts of the network and only fine-tunedthe heads for line and traffic element topology relationships.",
  ". Ablation experiments for different improvement methodson OpenLaneV2 validation set": "Map Encoder. We used the SD map feature as the key andvalue, and the feature map formed from multi-perspectiveimages as the query to perform cross-attention. We thenfound that with the prior of SD map, the quality of ourHD map construction was greatly improved. Among them,DETl and DETa improved significantly, increasing by 19%and 20% respectively. The TOPll metric increased by 13%,DETt increased by 9%, and TOPlt increased by 1.5%. Itcan be seen that SD map can enhance the line features, anddue to the inherent topological structure of the map, it alsohelps in constructing the final line-line topology graph.Large Backbone. We also tried a backbone with a largernumber of parameters. We found that InternImage-L ,due to its DCNV3 convolutions, possesses the advantagesof both CNNs inductive biases and multi-head attention. Itcan bring about stable and significant improvements acrossall metrics. Our final metric OLUS improved from 0.3960to 0.4486, an increase of 13%.Map Pretrain. To enhance the map encoders understand-ing of maps, we pre-trained it. As a result, we found that theTOPll metric improved the most, with an increase of 6%,DETl increased by 3%, DETa increased by 1.5%, and DETtincreased by 1.7%. This is because during the pre-trainingtask, the map encoder learned in advance the process of con-verting the position-encoded SD map into the feature mapoutput by the encoder, leading to significant improvementsin topological reasoning.LDTR Head. By introducing the LDTR head, ourDETa improved the most, from 0.25 to 0.34, a 36% in-crease. DETl increased by 4.5%, TOPll increased by 10%,and TOPlt increased by 4.9%.P2P IoU Loss. By introducing the P2P IoU loss, we in-corporated the IoU metric between lines to supervise thematching degree of lines, enhancing the detection precisionof lines. We found that DETa improved by 8%, TOPll in-creased by 7%, DETl increased by 2.1%, TOPlt increasedby 4.6%, and DETt increased by 1.9%. It can be observedthat the metrics related to lines have shown significant im-provement.Aux Head. By introducing auxiliary task heads, althoughour overall metric did not increase on the validation set,there was a slight improvement on the test set.",
  ". Model Ensemble": "As shown in Tab. 5, we replaced the traffic element detec-tion results in YOLOX with those from an existing model,and utilized the detection results from YOLOX to providebetter features and binary matching results input throughsincos position encoding. This allowed us to fine-tune thetopology head, improving the TOPlt metric. Previous train-ing results had indicated that when we increased DETl andDETa, there was a decline in TOPll. We considered thatthis was due to insufficient model capacity. Therefore, we",
  ". Conclusion": "In the paper, we propose a network that leverages the advan-tages of SD map and multi-view image inputs, combinedwith a mature object detector, and introduces SD map pre-training, the LDTR head, and auxiliary tasks to improvethe final precision. As a result, as shown in the last rowof Tab. 5, the OLUS metric reached 0.58.",
  "Mordechai Haklay and Patrick Weber. Openstreetmap: User-generated street maps. IEEE Pervasive computing, 7(4):1218, 2008. 1, 3": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 3 Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, PiotrDollar, and Ross Girshick. Masked autoencoders are scalablevision learners. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 1600016009, 2022. 2 Tianyu Li, Peijin Jia, Bangjun Wang, Li Chen, Kun Jiang,Junchi Yan, and Hongyang Li. Lanesegnet: Map learningwith lane segment perception for autonomous driving. arXivpreprint arXiv:2312.16108, 2023. 2 Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-hao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer:Learning birds-eye-view representation from multi-cameraimages via spatiotemporal transformers. In European con-ference on computer vision, pages 118. Springer, 2022. 2 Bencheng Liao, Shaoyu Chen, Xinggang Wang, TianhengCheng, Qian Zhang, Wenyu Liu, and Chang Huang. Maptr:Structured modeling and learning for online vectorized hdmap construction. arXiv preprint arXiv:2208.14437, 2022.2 Bencheng Liao, Shaoyu Chen, Yunchi Zhang, Bo Jiang, QianZhang, Wenyu Liu, Chang Huang, and Xinggang Wang.Maptrv2: An end-to-end framework for online vectorized hdmap construction. arXiv preprint arXiv:2308.05736, 2023.1 Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.Path aggregation network for instance segmentation. In Pro-ceedings of the IEEE conference on computer vision and pat-tern recognition, pages 87598768, 2018. 3",
  "Ilya Loshchilov and Frank Hutter. Decoupled weight decayregularization. arXiv preprint arXiv:1711.05101, 2017. 3": "Katie Z Luo, Xinshuo Weng, Yan Wang, Shuang Wu, JieLi, Kilian Q Weinberger, Yue Wang, and Marco Pavone.Augmenting lane perception and topology understandingwith standard definition navigation maps.arXiv preprintarXiv:2311.04079, 2023. 1 Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang,Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu,Hongsheng Li, et al. Internimage: Exploring large-scale vi-sion foundation models with deformable convolutions. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 1440814419, 2023. 3,4 Zhongyu Yang, Chen Shen, Wei Shao, Tengfei Xing,Runbo Hu, Pengfei Xu, Hua Chai, and Ruini Xue.Ldtr:Transformer-based lane detection with anchor-chain repre-sentation. arXiv preprint arXiv:2403.14354, 2024. 1, 2, 4"
}