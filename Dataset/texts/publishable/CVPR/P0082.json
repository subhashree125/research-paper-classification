{
  "Abstract": "In this technical report, we present our solution for theVision-Centric 3D Occupancy and Flow Prediction track inthe nuScenes Open-Occ Dataset Challenge at CVPR 2024.Our innovative approach involves a dual-stage frameworkthat enhances 3D occupancy and flow predictions by in-corporating adaptive forward view transformation and flowmodeling. Initially, we independently train the occupancymodel, followed by flow prediction using sequential frameintegration. Our method combines regression with classi-fication to address scale variations in different scenes, andleverages predicted flow to warp current voxel features tofuture frames, guided by future frame ground truth. Experi-mental results on the nuScenes dataset demonstrate signifi-cant improvements in accuracy and robustness, showcasingthe effectiveness of our approach in real-world scenarios.Our single model based on Swin-Base ranks second on thepublic leaderboard, validating the potential of our methodin advancing autonomous car perception systems.",
  ". Introduction": "3D occupancy and flow prediction are criti-cal components of autonomous driving perception systems.They involve determining the occupancy status, semanticclass, and future position of each voxel in a 3D voxel space.These predictions provide rich semantic and geometric in-formation, crucial for understanding and navigating com-plex driving environments. The CVPR24 occupancy andflow prediction competition focuses on developing newalgorithms that predict occupancy and flow solely fromcamera input during inference, offering a significant plat-form for advancing state-of-the-art 3D occupancy and flowprediction algorithms.",
  "*This work was partly done during internship in Inceptio.Corresponding author: Jianbing Shen. This work was supported inpart by the FDCT grants 0102/2023/RIA2 and 0154/2022/A3": "Our approach in this competition emphasizes innovativemodel design. We developed a two-stage framework to sep-arately predict occupancy and flow. In the first stage, wetrain the occupancy model independently. We propose anadaptive forward view transformation method to enhancethe adaptability of depth-based LSS. In the second stage,we train the flow model based on the occupancy model fromthe first stage. We introduced a novel sequential predictionmethod, using adjacent frames as inputs for the flow net-work. We combined regression with classification to pre-dict flow, addressing the issue of varying flow scales in dif-ferent scenes. Additionally, we use the predicted flow towarp the current frames voxel features to the future frameand supervise using the future frames ground truth, furtherenhancing prediction accuracy.Our approach achieves an Occ Score of 0.453 withoutany post-hoc process, achieving 2nd place in this challenge.",
  ". Voxel Feature Encoding": "We first extract image features, then employ Lift-Splat-Shoot (LSS) to transform the 2D features into 3Dspace. Previous methods typically use depth probability as weights for LSS, which weakens its adaptability due toits unimodal distribution. To enhance adaptability, we inte-grate semantic information into the depth probability. Wesupervise the depth using LiDAR points and apply segmen-tation loss to the image features. Given the initial sparsenature of voxel features obtained via LSS, we employ theinverse process of trilinear interpolation to densify these 3D",
  "Time Conv": ". Framework of the flow head. Note that the feature ofthe last frame is sequentially predicted without extra computation.The voxel feature is wrapped to the coordinates of the next framewith the predicted flow for further supervision. voxel features, further enhancing the adaptability of the for-ward view transformation. Temporal information from his-tory frames is then fused into these voxel features, basedon the sequential temporal fusion method . We utilizea 3D Unet similar to BEVDet for encoding 3D fea-tures. is an illustration of our whole framework.",
  ". Occupancy and Flow Prediction Heads": "For the occupancy prediction head, we adopt a per-maskclassification approach similar to Mask2Former . Themask for each category is predicted and supervised withbinary cross-entropy and dice loss. We use a similar lossfunction for image semantic segmentation prediction.In the flow prediction head, we predict flow using thecurrent and previous frames as input.Rather than com-puting voxel features for both frames during a single infer-ence, we sequentially utilize the previous frames features,reducing computational overhead. Considering the signif-icant scale variations of flow in different scenes (e.g., theflow values range from a maximum of 19.11 to a minimumof -22.73 in the OpenOcc dataset), neural networks strug-",
  ". An illustration of aggregating adaptive bins and adaptiveweights to flows": "gle to adapt to data with such high variance. Predictingflow values for all instances becomes challenging for thenetwork. Therefore, we transform the regression probleminto a combination of classification and regression, allevi-ating the prediction burden on the network. We model theflow predictions within a scene into discrete adaptive binpredictions and adaptive weights prediction. We firstaverage the features in the scene to predict the bin centers.After defining the number of bins n, we predict n proba-bilities using softmax and calculate each bins center usingcumulative probability:",
  ". Visualization of the ray visible mask. (a): Groundtruthoccupancy map; (b): Traffic-critical regions of the groundtruth oc-cupancy map": "flow, we transform the current frames features to the nextframe and supervise them with the ground truth occupancyof the future frame. To address the issue of gradient discon-tinuity when mapping features by coordinates, we again usethe trilinear interpolations inverse process. Supervision isapplied using simple cross-entropy. is an illustrationof the flow head.We observed that jointly predicting occupancy and flowsignificantly degraded occupancy performance. Thus, weadopted a two-stage training strategy: first, we trained thefeature encoding module and occ head jointly with occu-pancy prediction, and then fine-tuned the flow head withthe occupancy backbone frozen.",
  ". Further Improvements": "Ray Visible Mask.Beyond traffic-related factors on theroad, there are numerous unnecessary elements such asbuildings and vegetation far away from the ego vehicle.RayIoU increases the focus on important traffic targetsby setting a virtual LiDAR along the vehicles driving pathand only assessing the regions scanned by this virtual Li-DAR. Therefore, we adopt this idea during training, direct-ing more attention to the critical factors of traffic. We fol-low the calculation of RayIoU by setting multiple LiDARorigins along the driving path and calculating the visible re-gions from these origins to obtain a ray visible mask. Todifferentiate occupied voxels from surrounding points, wealso consider voxels within 2 meters of ray-visible occu-pied points as critical regions. Per-frame training losses areonly calculated on the critical regions with the mask. Thecritical regions are visualized in . During training, weimplement a hard example mining strategy, focusing on thetraining of difficult voxels based on their uncertainty. Wehave released mask data for public research purposes 1.",
  ". Dataset and Metrics": "The 3D Occupancy and Flow Prediction Challenge Dataset is built upon the nuScenes dataset . It comprises700 sequences for training, 150 for validation, and 150 fortesting. Each frame includes six surround-view images witha resolution of 9001600. Occupancy and flow annotationsare provided for each frame within the range of [-40m, -40m, -1m, 40m, 40m, 5.4m], with voxel resolution set at 0.4meters. The dataset includes flow annotations for the x andy axes and semantic annotations for 17 categories (includingunoccupied). No external data is utilized in our method.The final evaluation metrics are the class-averaged RayIoU across all classes and the mAVE for foreground classes.The overall evaluation score is computed as",
  ". Implementation Details": "Training Strategies.We conducted preliminary experi-ments using BEVDet as the baseline model, training on 8NVIDIA A100 GPUs. We used the AdamW optimizer with a learning rate of 2e-4, a weight decay of 0.05, anda total batch size of 32. We adopted exponential movingaverage (EMA) for updating model weights. When us-ing ResNet-50 as the backbone, we trained the baselinemodel without temporal fusion for 24 epochs. For the base-line with temporal fusion, we followed the methodologiesof SOLOFusion and FB-BEV to apply CBGS for12 epochs. For the Swin Base model, we trained theocc head with a total batch size of 8 and trained with CBGSfor 5 epochs. The flow head was then fine-tuned for an ad-ditional 5 epochs. Network Details.We used common data augmentationstrategies , including random flip, scale, and rotation forimage and flip augmentation for voxel features. We ini-tialized our networks using publicly available models. ForResNet-50, we used a BEVDet detection pre-trainedmodel, and for the Swin Base, we initialized with GeoMIM pre-trained on Occ3d . The voxel size in training isset to 200 200 16. When using ResNet-50 as the back-bone, the input image size was 256704, with voxel featurechannels set to 32. The image size is set to 640 1600 with",
  ". Ablation Study for Occ and Flow Training": "We conduct preliminary experiments using the ResNet-50backbone to quickly validate the efficacy of the proposedcomponents, then scale up using the validated effectivemethods. We found that training occ and flow together de-creased RayIoU by about two points. Thus, we adopted atwo-stage training strategy: first train occ, then train flow.Tab. 1 presents the ablation study results on occ training,we incrementally added modules to the BEVDet baseline,each significantly improving performance. In baseline 2,we adopted the mask-based loss and hard example miningstrategy following Mask2Former , achieving significantimprovements. Similarly, in baseline 5, we used the rayvisible mask to focus the network on important traffic areas,improving RayIoU. In baseline 6, performance (particularlyIoU@4) is further enhanced by including voxels within 2meters of the ray termination point in training. Addition-ally, in baseline 3, image segmentation loss and the injectionof semantic information into depth alleviated the unimodaldistribution, enhancing adaptability. Finally, our methodsurpassed the official baseline method with a RayIoU of0.151 under the same image output size and backbone set- tings. In baselines 4 and 7, we leveraged historical frameinformation and larger models, which naturally led to im-provements.Tab. 2 presents the evaluation results of flow head train-ing. When fine-tuning both occ and flow on a trained occnetwork, we observed a significant drop in occ predictionperformance. Therefore, we froze the occ network and theencoder, fine-tuning only the flow head.This preservedthe occ performance while effectively predicting flow. Toprovide a more comprehensive comparison of flow perfor-mance, we introduced two additional metrics: one calcu-lates a per-voxel mAVE (mAVE@Per-Voxel), and the othercalculates mAVE for all points queried from the LiDAR ori-gin (mAVE@LQ). We found that fine-tuning all parametersresulted in a decrease in the per-voxel mAVE, but did notprovide gains for the other two metrics. Additionally, ourproposed AdaBin method and the use of future frames forsupervision improved flow prediction.",
  ". Final Results and Conclusion": "In this report, we describe our solution in detail for theCVPR 2024 Autonomous Grand Challenge Track On Oc-cupancy and Flow. The model of our final submission em-ploys Swin Base as the image backbone. The image res-olution is set to 640 1600. The encoded voxel size is200 200 16, with a channel size of 100. We use 16 his-torical frames. No post-processing techniques, such as test-time augmentation or model ensembling, are applied. Thismodel achieves an Occ Score of 0.453 on the test server,ranking 2nd on the test server. Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.Adabins: Depth estimation using adaptive bins. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, pages 40094018, 2021. 2 Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-ancarlo Baldan, and Oscar Beijbom.nuscenes: A multi-modal dataset for autonomous driving. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1162111631, 2020. 3 Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-der Kirillov, and Rohit Girdhar.Masked-attention masktransformer for universal image segmentation. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, pages 12901299, 2022. 2, 4 Ozgun C icek, Ahmed Abdulkadir, Soeren S Lienkamp,Thomas Brox, and Olaf Ronneberger.3d u-net: learn-ing dense volumetric segmentation from sparse annota-tion. In Medical Image Computing and Computer-AssistedInterventionMICCAI 2016: 19th International Conference,Athens, Greece, October 17-21, 2016, Proceedings, Part II19, pages 424432. Springer, 2016. 2 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 3",
  "Junjie Huang, Guan Huang, Zheng Zhu, Yun Ye, and DalongDu. Bevdet: High-performance multi-camera 3d object de-tection in bird-eye-view. arXiv preprint arXiv:2112.11790,2021. 2, 3": "Yinhao Li, Zheng Ge, Guanyi Yu, Jinrong Yang, ZengranWang, Yukang Shi, Jianjian Sun, and Zeming Li. Bevdepth:Acquisition of reliable depth for multi-view 3d object detec-tion. arXiv preprint arXiv:2206.10092, 2022. 1 Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-hao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. Bevformer:Learning birds-eye-view representation from multi-cameraimages via spatiotemporal transformers.arXiv preprintarXiv:2203.17270, 2022. 4",
  "Ilya Loshchilov and Frank Hutter. Decoupled weight decayregularization. arXiv preprint arXiv:1711.05101, 2017. 3": "Jinhyung Park, Chenfeng Xu, Shijia Yang, Kurt Keutzer,Kris Kitani, Masayoshi Tomizuka, and Wei Zhan. Time willtell: New outlooks and a baseline for temporal multi-view 3dobject detection. arXiv preprint arXiv:2210.02443, 2022. 2,3, 4 Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encodingimages from arbitrary camera rigs by implicitly unproject-ing to 3d. In Computer VisionECCV 2020: 16th EuropeanConference, Glasgow, UK, August 2328, 2020, Proceed-ings, Part XIV 16, pages 194210. Springer, 2020. 1 Xiaoyu Tian, Tao Jiang, Longfei Yun, Yucheng Mao,Huitong Yang, Yue Wang, Yilun Wang, and Hang Zhao.Occ3d: A large-scale 3d occupancy prediction benchmarkfor autonomous driving.Advances in Neural InformationProcessing Systems, 36, 2024. 1, 3 Wenwen Tong, Chonghao Sima, Tai Wang, Li Chen, SileiWu, Hanming Deng, Yi Gu, Lewei Lu, Ping Luo, Dahua Lin,et al. Scene as occupancy. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 84068415, 2023. 1 Xiaofeng Wang, Zheng Zhu, Wenbo Xu, Yunpeng Zhang,Yi Wei, Xu Chi, Yun Ye, Dalong Du, Jiwen Lu, and Xin-gang Wang. Openoccupancy: A large scale benchmark forsurrounding semantic occupancy perception. arXiv preprintarXiv:2303.03991, 2023. 1, 3",
  "Apendix": "As shown in Fig. A.1 and Fig. A.2, we visualized thecomparison between the predicted and ground truth occu-pancy, as well as the predicted flow and its application intransforming the current frame to the next frame. The vi-sualizations demonstrate the accuracy of our model in pre-dicting occupancy status and semantic class. The alignment between predicted and actual occupancies confirms the ef-fectiveness of our dual-stage framework and the integrationof AdaBin. For flow prediction, the visual transformationshows our models capability to capture temporal dynam-ics and spatial continuity, validating the robustness of ourapproach."
}