{
  "Abstract": "Neural Architecture Search (NAS) has been widelyadopted to design neural networks for various computer vi-sion tasks. One of its most promising subdomains is dif-ferentiable NAS (DNAS), where the optimal architectureis found in a differentiable manner.However, gradient-based methods suffer from the discretization error, whichcan severely damage the process of obtaining the final ar-chitecture. In our work, we first study the risk of discretiza-tion error and show how it affects an unregularized su-pernet. Then, we present that penalizing high entropy, acommon technique of architecture regularization, can hin-der the supernets performance. Therefore, to robustify theDNAS framework, we introduce a novel single-stage search-ing protocol, which is not reliant on decoding a continuousarchitecture. Our results demonstrate that this approachoutperforms other DNAS methods by achieving 75.3% inthe searching stage on the Cityscapes validation datasetand attains performance 1.1% higher than the optimal net-work of DCNAS on the non-dense search space comprisingshort connections. The entire training process takes only5.5 GPU days due to the weight reuse, and yields a com-putationally efficient architecture. Additionally, we proposea new dataset split procedure, which substantially improvesresults and prevents architecture degeneration in DARTS.",
  ". Introduction": "Neural architecture search (NAS) is a field that automatesthe designing of neural networks. Differentiable neural ar-chitecture search (DNAS) denotes the set of gradient-basedNAS techniques. In these methods, we relax the discretearchitecture space into the space of continuous architec-tures and optimize it using stochastic gradient descent.DNAS framework can be decomposed into three stages:",
  ". the decoding stage, which retrieves a discrete architec-ture from a continuous search space, and": "3. the retraining stage, where a retrieved architecture istrained for a longer time and with newly initializedweights.The usage of supernet greatly reduces computationalcosts by enabling weight-sharing across a vast number ofdifferent architectures . However, despite its com-putational effectiveness and significant potential, practicalapplications of DNAS are hindered by the severe fragilityand instability . One of the major issues, whichwe refer to as the discretization error, concerns the poor ar-chitecture optimization process and emerges at the decodingstage during the discretization procedure . Dis-carding operations or connections can yield substantiallydifferent architecture when a supernet is poorly discretizedand has a high entropy at the end of the training. As a result,it can impact the searching-retraining correlation.Thus,even a network retrieved from a well-performing supernetmight underperform after retraining.In this work, we shed more light on the discretization er-ror and propose a novel solution to address it. Contrary to",
  "arXiv:2405.16610v1 [cs.CV] 26 May 2024": ". Illustration of the single-stage searching protocol. We replace both the decoding and the retraining stages with a new fine-tuningphase, during which architecture is frozen. By reusing weights, we save a considerable amount of the retraining time. We keep theoptimized architectural parameters in the final network, which means that edges in a supernet take on real values, unlike in the standardDNAS framework. the common approach, we perform experiments in the se-mantic segmentation task for two reasons. First, ourapproach is better suited for tasks that can benefit fromdense architectures and a certain design of the search space.Second, we find it more effective in highlighting some prob-lems than the extensively studied task of image classifica-tion, where differences between approaches can even bestatistically insignificant.Our experiments in Tab. 1 re-veal a lack of implicit entropy regularization in the vanillaDNAS framework. We can observe that the average entropyof architectural parameters remains constant throughout thetraining, indicating that a considerable number of opera-tions in a supernet contributes to the prediction at the endof the searching. This might in turn cause the discretizationerror.One of the common approaches to tackle the discretiza-tion error is to impose a one-hot distribution over architec-tural parameters by regularizing a supernet . Inour work, we take a closer look at the method of penalizinghigh entropy and highlight its shortcomings. As we demon-strate in Sec. 4, such a regularization induces a trade-off be-tween discretization and obtained results. Namely, we showthat the magnitude of the entropy loss negatively correlateswith the discretization error, which indeed suggests a needfor a strong regularization. At the same time, our experi-ments indicate that it can degrade supernets performancein the searching stage. We also consider another variant ofdynamic entropy loss regularization , which alleviatesthe performance issue, but does not eliminate it entirely.To this end, we propose to approach the problem froma different angle and to train the supernet in a fully prox-yless manner by introducing a single-stage searching pro-tocol. The approach is illustrated in . We simplifythe searching process by replacing both the decoding and the retraining stages with a new fine-tuning phase on thetop of the searching stage. We do not perform discretiza-tion and, thereby, we treat optimized supernet with all itstrained parameters as the final model. By reusing weightsfrom the searching stage, we considerably reduce the to-tal training time. In this approach, it is crucial to design asearch space that is both expressive and computationally ef-ficient. For that reason, our method might not be yet appro-priate for certain tasks. We apply our single-stage searchingmethod to the non-dense DCNAS search space , whichincludes transmissions spanning only between consecutivelayers. As we show in Sec. 4, our approach is on par with,or even surpasses, other state-of-the-art DNAS models interms of computational requirements.The single-stage searching protocol also addresses an-other deficiency of the DNAS framework, which is oftenoverlooked - prohibitively high computational complexity.Because of the proxy searching procedure, each retrievedarchitecture is trained from scratch, which imposes extracosts. Furthermore, the DNAS method can suffer from apoor searching-retraining correlation . As a result, inaddition to the costs of finding the optimal network, severalcandidate architectures must be evaluated to counteract lowcorrelation. Instead, our single-stage searching algorithmtakes only 5.5 GPU days to converge by using a single setof weights throughout the training.We validate our improvements on the Cityscapesdataset . Our single-stage method outperforms otherDNAS methods in the searching stage by achieving 75.3%on the validation set. Moreover, it attains performance 1.1%higher than the final derived network of DCNAS on thenon-dense search space, which demonstrates the viability ofthe single-stage approach. In our experiments, we also per-form an ablation study on a dataset split procedure ,",
  "We study the entropy architecture regularization anddemonstrate that it hinders the supernets performance": "We introduce a fully proxyless, single-stage searchingprotocol that eliminates the discretization error by thor-oughly fine-tuning the supernet. We demonstrate that ityields a computationally efficient architecture, which out-performs DCNAS on a comparable search space. Also,we show its superiority in terms of the total training time. We conduct an ablation study on the training datasetsplit approach in DNAS methods, highlighting a muchmore optimal dataset usage, which considerably enhancesperformance and prevents architecture degeneration inDARTS.",
  ". Related work": "Neural architecture search is a collection of novel tech-niques aiming to automate the neural network design pro-cess. It can automatically discover the optimal neural net-work architecture for a given task or dataset. NAS researchconcerns primarily evolutionary , reinforcementlearning and DNAS meth-ods.In Differentiable NAS (DNAS), the optimal architecturecan be found using stochastic gradient descent, thanks tothe differentiable representation of the search space .Each architectural choice is assigned a continuous param-eter. The search space is represented as a large, weight-sharing supernet, where different architecture candidatescorrespond to different subsets of this supernet. Searchingover this search space essentially comes down to trainingthe network. Afterward, the optimal discrete architecture isretrieved from a supernet and retrained from scratch for alonger time.NAS algorithms search for a network within a predefinedsearch space. The search space must be expressive enoughto include a wide range of candidate architecture and alsobe efficiently optimizable. In NASNet , the networkcomprises a sequence of convolutional cells, all sharing thesame architecture. The cell, composed of multiple blocks,forms the search space. The spatial dimension throughoutthe network is controlled manually. Several NAS applica-tions to the image classification task follow the same de-sign .AutoDeepLab builds upon DARTS and adaptsits approach to semantic segmentation by introducing asignificantly larger network-level hierarchical architecture search space, while operating on the same cell-level searchspace as DARTS. DCNAS extends the idea of hier-archical search space and introduces a densely connectedsearch space, incorporating long-range connections reach-ing every cell and every spatial level. DPC proposes arecursive search space formed by a dense prediction cell,which uses a segmentation-specific set of operations, suchas atrous convolution or pooling. In the searching stage,DPC constructs a proxy task of finding the dense predictioncell on the top of the backbone pretrained on ImageNet .Discretization issue.Several works derived fromDARTS focus on the searching to retraining transition.Much attention was drawn to the collapsing phenomenonin DARTS, where a supernet assigns excessive weights toskip connections . RobustDARTS alleviates this issue through a hand-crafted early-stoppingstrategy. SmoothDARTS enhances architecture general-ization by perturbing architecture parameters, thus smooth-ing the loss landscape. Some other works improve DARTSdecoding efficacy by better estimating the importance of op-erations .The concept of discretization issue is established in theliterature . Fair DARTS observes the dis-cretization discrepancy by visualizing softmax activationsand introduces a zero-one loss, which imposes a one-hotdistribution.GOLD-NAS addresses the issue by us-ing hardware constraints to penalize significant architec-tural parameters and progressively prune weak operations.The most related to our work in terms of discretization studyis DA2S . The authors show that vanilla DARTS suffersfrom a performance collapse by performing inference on adiscretized supernet. To alleviate this collapse, they intro-duce dynamic entropy loss to impose one-hot distribution inthe later stages of the training. DCNAS likewise reg-ularizes architectural parameters to diminish insignificanttransmissions.Proxyless searching involves sharing the training proto-col between a supernet and a retrained network. In partic-ular, this implies using the same hyperparameters, such asbatch size or image crop. ProxylessNAS samples pathswithin the search space to facilitate proxyless searching.Similarly, DCNAS probes candidate architectures bysampling connections. This approach, along with maskinga subset of channels in each cell , makes proxylesssearching viable. Another study divides the searchingstage into a few phases and performs a stepwise discretiza-tion. This can be considered a related work to proxylesssearching, as the proxy gradually decreases. In our work,we extend the idea of proxyless searching and propose tooptimize a supernet in an end-to-end manner using targethyparameters. However, unlike other approaches, we donot retrain it. Instead, we reuse the already trained weights,thus significantly reducing computational requirements.",
  ". Search space": "Network-level architecture. The supernet comprises threemodules: the stem, the backbone, and the decoder. Fol-lowing DCNAS , we utilize multi-scale feature repre-sentation in our network. We adopt the stem module usedby Auto-DeepLab and adjust it to the multi-scale networkstructure by performing interpolations of an input image.For the decoder, we reuse the prediction head designed byDCNAS.A dense rectangular-like grid of cells forms the back-bone. Each cell corresponds to a particular layer and reso-lution. Resolutions reach up to a downsampling rate of 32.Network-level transmissions span between adjacent cells inconsecutive layers. We assign an architectural parameterlss to a transition in layer l between resolution s and s.We define an input to a cell as a weighted average over theoutputs of its predecessors:",
  "where LA and LB denote cross-entropy loss computed ontwo subsets of training data A and B, respectively. We alsoinclude the entropy regularization term in LB": ". The average entropy of architectural parameters through-out the training. Dashed and solid curves correspond to supernetstrained with the constant and the linear entropy scaling function,as described in Sec. 3.3. Curves denoted by -, M, and H refer tosupernets trained without entropy loss, with medium entropy loss,and with high entropy loss, respectively. The fine-tuning phase can be perceived as a replacementfor the retraining stage. However, we managed to reducethe overall training time considerably. Unlike in the stan-dard DNAS framework, we do not perform decoding, andthus, we can efficiently reuse weights that have already beentrained in the preceding phases. In Sec. 4, we show that thisapproach can achieve optimal results.",
  "i ln i .(4)": "Here, f(t) is a scaling term dependent on the time t, and cdenotes the overall entropy loss magnitude for the network-level architectural parameters. We apply the same regular-ization to the cell-level parameters c.In our experiments, we consider two variants of the scal-ing function. The Linear function refers to a linear scalingterm, which gradually increases the magnitude of the en-tropy loss from 0 to 1. The Constant function sets f(t) = 1and corresponds to the default approach of applying the en-tropy loss.",
  ". Implementation details": "The large model takes 1.4 days to train for 600 epochs on4 Tesla V100 32GB GPUs. We use syncBN to syn-chronize statistics in the batch normalization layers acrossdevices. Respectively, we assign 5%, 35%, and 60% ofepochs to the warmup, the searching, and the fine-tuningphases. We apply architecture regularization after 15% ofepochs. However, in our experiments, the supernet is notexcessively sensitive to changes in these hyperparameters.Following previous works , we use two optimiza-tion strategies to train architectural parameters and opera-tion weights. For the former, we adopt Adam with alearning rate of 0.003 and weight decay of 0.001. For thelatter, we use SGD parameterized by a learning rate of 0.003and weight decay of 0.0005. As a data augmentation, we ap-ply horizontal flipping, random scaling, color jittering, andrandom Gaussian noise. We set the batch size to 16 andtrain the network using crops of 512 1024. More detailscan be found in our implementation, which we release withthe code.We present different variants of models in Tab. 3. Theyvary in the number of layers L, the filter multiplier F, the ex-pansion ratio Exp in the inverted bottleneck, and the channelsampling ratio S. Due to limited computational resources,we use the small model in the experiments concerning dis-cretization error and entropy loss.",
  ". Emergence of discretization error": "illustrates the average entropy of c. We can ob-serve a sharp drop in entropy for the step scaling func-tion, which can negatively impact training dynamics, espe-cially at higher magnitudes. This observation aligns withour results presented in Sec. 4.3.The experiment alsoshows that the unregularized supernet, denoted by a blueline, has severely non-discretized architectural parameters,which could lead to the discretization error.To study more thoroughly how discretization is impactedby entropy regularization, we gradually discretize supernetstrained with different magnitudes of entropy loss. Specif-ically, we perform inference after dropping a certain num-ber of redundant edges based on the strength of their archi-tectural parameters. It is important to note that while thiscan effectively measure the relative importance of an edgewithin a cell, it might not optimally rank edges according totheir relevance across different cells in different parts of the",
  ". Comparison of different models varying in size. SeeSec. 4.1 for reference": "supernet. Nevertheless, we use it as a reasonable approxi-mation.The results are illustrated in a. First, we observe aquick collapse of a standard unregularized DNAS supernetafter dropping merely 20% of its edges. Second, we em-pirically demonstrate a correlation between the strength ofregularization and resistance to discretization. In particular,pruning 30% of the transmissions in a heavily regularizedsupernet does not result in any noticeable drop in perfor-mance.We conduct the same experiments, but this time theyare followed with a short post-decoding fine-tuning, whichadapts transferred weights to a discretized architecture. Theaim is to investigate whether discretized architecture liesin the neighborhood of the optimal architecture in param-eter space. In such a scenario, performing a relatively smallnumber of updates could retrieve optimal performance. Ourfindings, presented in b, confirm that a strong entropyregularization can effectively address the discretization is-sue. Conversely, a vanilla DNAS supernet generates a qual-itatively different architecture, partially explaining the lowcorrelation observed by DCNAS .",
  ". Negative impact of entropy loss": "As we highlight in Sec. 1 and Sec. 2, the entropy loss term isan established solution to the discretization error in the lit-erature. However, it causes a sudden drop in the entropy ofarchitectural parameters, as illustrated in . This mightlead to a suboptimal convergence of the supernet. Dynamicregularization results in a more gradual entropy decrease.",
  ". Comparison between different methods on the Cityscapesvalidation dataset. FLOPs are computed for the final networks andtaken from DCNAS. In our case, we evaluate the performance ofthe supernet": "We study the efficacy of the dynamic and static entropylosses on the Cityscapes validation set. Experiments areconducted with different scaling functions and magnitudesof the entropy loss (c and c). We report an average mIoUover three runs to obtain accurate estimates. Results areshown in Tab. 2. We observe a consistent drop in perfor-mance for higher entropy magnitudes, indicating the emer-gence of the discretization-exploration trade-off. A linearlyscaled entropy loss term alleviates the issue and outper-forms the default regularization technique, albeit convergespoorly for higher entropy magnitude, emphasizing the ne-cessity for a more robust approach.",
  ". Proxyless searching": "We validate the single-stage searching protocol, our remedyfor the discretization issue, in Tab. 4. Namely, we report themIoU of our approach and the state-of-the-art DNAS meth-ods on the validation set. For all models, we also provide thenumber of floating-point operations. Our medium and largemodels outperform supernets of Auto-DeepLab and DC-NAS in the searching stage by achieving 74.4% and 75.3%,respectively. The large model matches Auto-DeepLab andDPC in the number of floating-point operations, whereasthe medium network requires as little as 30% more than",
  "Ours (Large)6005.5-": ". Comparison of DNAS methods on Cityscapes in timeefficiency. The searching stage time for Auto-DeepLab and DPCis provided for P100, which considerably overstates the costs. Weestimate DPC epochs based on values of its hyperparameters . DCNAS.Importantly, the medium model attains 1.1% highermIoU compared to the non-dense variant of DCNAS, whichis most comparable to ours in terms of the search space de-sign. Given these results and the reasonable computationalcost, the supernet trained in a proxyless way can be consid-ered a viable drop-in replacement for the final network ofthe state-of-the-art DNAS algorithms. We hypothesize thatincorporating long-range connections to the search spacemight further elevate the performance.We present results for the test set in Tab. 5. However, we",
  "%": ". Results of the large model trained with different datasplits on the validation set. 0: Annotations are not used in training.0.5: Half of the annotations are used exclusively to optimize givenparameters. 1: All of the annotations are used. In case we use bothfine and coarse annotations, we train the parameters using all thedata and then fine-tune them with only fine labels. do not further optimize performance. We employ the sametraining and inference procedures as used for the validationset, which may potentially understate the obtained results.Regarding training time, we benchmark our approachagainst other DNAS works in Tab. 6. In the conventionalsearching-retraining procedure, only a small amount of timeis dedicated to finding the optimal architecture . The bulk of computational resources are devoted toretraining the derived architecture, significantly increas-ing the costs of using other DNAS methods. The single-stage searching protocol eliminates the need for a time-consuming retraining stage, thus providing the optimal ar-chitecture in only 5.5 GPU days.",
  ". Optimal dataset split": "DNAS methods commonly address the emerging bileveloptimization problem by training architectural parameters,{, }, and operation weights, {}, on two disjoint sub-sets of the training dataset. DARTS introduced this to pre-vent poor architecture generalization. MiLeNAS ar-gues that optimizing architectural parameters on the entiretraining dataset is optimal. Results from experiments withvaried splits are presented in Tab. 7. We show that jointoptimization using the same data yields the best results,provided that batches are sampled separately in each iter-ation for both sets of parameters. We also use two separateoptimizers. Combining fine with coarse annotations doesnot offer significant benefits, potentially due to the super-nets undertraining. Surprisingly, performing architectureupdates with batches consisting exclusively of coarse an-notations matches the training performance obtained usingother splits. Additionally, our supernet achieves superior re-sults in the searching stage, even when using a less optimaldata split than the one used by DCNAS.To further verify our findings, we test if the new split- . (left) dominant eigenvalues of 2Lvalid on four different search spaces with a dataset split; (right) dominant eigenvalues whensearching on a single dataset. All experiments were conducted on CIFAR 10 dataset. less dataset procedure affects architecture degeneration inDARTS . RobustDARTS showed that dominanteigenvalue of 2Lvalid is significantly increasing duringsearching and that there exists a correlation between largedominant eigenvalue and architecture degeneration acrossfour different search spaces S1, S2, S3 and S4. First, wereproduce experiments presented in the paper for standardDARTS on CIFAR-10.Results are illustrated in (left). We indeed observe that in all four cases dominanteigenvalues are steadily increasing and reach much largervalues at the end of the searching. Second, we performthe same experiments, but with parameters optimized on theunion of training and validation dataset. (right) showsthat this time in all four search spaces dominant eigenvaluesare kept relatively small, which suggests that the networkdoesnt end up in a sharper local minima that would causean architecture degeneration.",
  ". Conclusions": "In this work, we conduct comprehensive experiments toinvestigate the discretization error. Our findings indicatethat this issue emerges in an unregularized supernet duringthe searching stage. We study an established method foraddressing this issue through entropy architecture regular-ization and highlight its shortcomings. As a remedy, wepropose the single-stage searching protocol, a novel wayof finding optimal neural networks using a gradient-basedtechnique. Our method robustifies the DNAS frameworkby eliminating the discretization error that other methodssuffer from. We show that it matches other state-of-the-art approaches in terms of performance and results withina similar search space. Also, we demonstrate that the jointoptimization of architecture and weights on the full datasetyields better results, and prevents a well-known architecture degeneration phenomenon in DARTS. We believe that ourwork can be a starting point for creating even more pow-erful DNAS models. In future work, we aim to efficientlyincorporate long-range connections into the search space toimprove results even further.",
  "Acknowledgements": "We would like to express our gratitude to Nvidiaand Googles TPU Research Cloud for making theircompute resources available to us.Similarly,wewould like to thank PLGrid and ICM UW for shar-ing their GPU clusters.Neural Architecture Searchis one of the most compute-intensive areas of deeplearning, and this work could not have been possi-ble without immense amounts of computing power."
}