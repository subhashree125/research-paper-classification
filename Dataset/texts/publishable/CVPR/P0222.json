{
  "Abstract": "Video Panoptic Segmentation (VPS) is a challenging taskthat is extends from image panoptic segmentation.VPS aimsto simultaneously classify, track, segment all objects in avideo, including both things and stuff. Due to its wide ap-plication in many downstream tasks such as video under-standing, video editing, and autonomous driving. In or-der to deal with the task of video panoptic segmentationin the wild, we propose a robust integrated video panop-tic segmentation solution. We use DVIS++ framework asour baseline to generate the initial masks. Then,we add anadditional image semantic segmentation model to furtherimprove the performance of semantic classes.Finally, ourmethod achieves state-of-the-art performance with a VPQscore of 56.36 and 57.12 in the development and test phases,respectively, and ultimately ranked 2nd in the VPS track ofthe PVUW Challenge at CVPR2024.",
  ". Introduction": "Panoptic segmentation integrates the tasks of se-mantic segmentation and instance segmentation, requiringthat each pixel of an image must be assigned a seman-tic label and an unique instance id.Since its inception,numerous studies have introduced a variety of in-novative approaches aimed at enhancing both the accu-racy and efficiency of this task.Video Panoptic Seg-mentation, as a direct extension of panoptic segmentationto videos, endeavors to consistently segment and identifyall object instances across all frames.Numerous endeavorshave focused on adapting image-based panoptic segmen-tation models for the video domain.VPSNet com-bined the temporal feature fusion module and object track-ing branch with a single-frame panoptic segmentation net-work to obtain panoptic video results. Panoptic-DeepLab is the first bottom-up and single-shot panoptic seg-mentation model,utilizing a dual-ASPP and dual-decoderarchitecture tailored for semantic and instance segmenta-tion. ViP-DeepLab extended Panoptic-DeepLab tojointly perform video panoptic segmentation and monocu- lar depth estimation to address the inverse projection prob-lem in vision. Note the disadvantages of previous methodsthat require multiple separate networks and complex post-processing, MaX-DeepLab directly predicted masks andclasses with a mask transformer, removing the needs formany hand-designed priors.Slot-VPS designed a pio-neering end-to-end framework that simplifies the VPS taskby using a unified representation called panoptic slots toencode both foreground instances and background seman-tics in a video.DVIS introduces a novel referringtracker for precise long-term alignment and a temporal re-finer that leverages this alignment to effectively utilize tem-poral information, leading to improved instance segmenta-tion outcomes. The 1st Place Solution for the CVPR 2023PVUW VPS Track embraced DVISs strategy of di-viding the task into three independent sub-tasks and opti-mizing for optimal outcomes. DVIS++ improved thetracking capability of DVIS by introducing a denois-ing training strategy and contrastive learning. Video-kMaX extends the image segmenter for cliplevel video seg-mentation, and employed clip-kMaX for efficient clip-levelsegmentation and HiLA-MB for robust cross-clip associa-tion with hierarchical matching, effectively addressing bothshort- and long-term object tracking challenges. MaXTron integrated a mask transformer with trajectory attentionto perform VPS, bolstering temporal coherence through itswithin-clip and cross-clip tracking modules.In summary, the progression of VPS has introduced so-phisticated frameworks that offer innovative strategies, en-hancing accuracy, efficiency, and temporal consistency inthe segmentation and tracking of objects throughout videoframes.These innovations have markedly advanced thefrontier of video comprehension and analytical capabilities.",
  ". Our solution": "In this section, we will introduce the implementationprocess of our method. In order to deal with the task ofvideo panoptic segmentation in the wild, we propose a ro-bust integrated video panoptic segmentation solution. Inthis solution, we first introduce DVIS++ as the base-line of video panoptic segmentation and then choose ViT-",
  ". Video Panoptic Segmentation": "For video panoptic segmentation in the wild, DVIS++ is a decoupled video segmentation framework, whichdecouples video segmentation into three cascaded sub-tasks: segmentation, tracking, and refinement, as shownin . It is worth noting that unlike image segmen-tation, video segmentation involves capturing inter framerelationships from multiple frames for training. However,training consecutive frames requires a significant amountof GPU memory.To save memory resources, DVIS++adopts a frozen DINOv2 VIT backbone and em-ploys Mask2Former as the segmenter, which is trainedin three stages, sequentially training segmenter, referringtracker, and temporal refiner.",
  ". Video Semantic Segmentation": "Considering that VPSW and VIPSeg have the same datasource and annotation category, and VPSW has a highernumber of semantic segmentation annotation frames, whichis very beneficial for training semantic segmentation mod-els. In order to further improve the segmentation perfor-mance of stuff objects and some thing objects in panopticsegmentation, we introduce ViT-Adapter as a semanticsegmentation baseline.",
  ". Datasets": "VIPSeg. VIPSeg provides 3,536 videos and 84,750frames with pixel-level panoptic annotations, covering awide range of real-world scenarios and categories, whichis the first attempt to tackle the challenging video panopticsegmentation task in the wild by considering diverse sce-narios. The train set, validation set, and test set of VIPSegcontain 2, 806/343/387 videos, respectively. VIPSeg show-cases a variety of real-world scenes across 124 categories,consisting of 58 categories of thing and 66 categories ofstuff. Due to limitations in computing resources, all theframes in VIPSeg are resized into 720P (the size of the shortside is resized to 720) for training and testing.VSPW.The VSPW is a large-scale dataset forVideo Semantic Segmentation, which is the first attempt totackle the challenging video scene parsing task in the wildby considering diverse scenarios and annotates 124 cate-gories of real-world scenarios, which contains 3,536 videos,",
  ". Evaluation Metrics": "Video Panoptic Segmentation (VPS) Track of Pixel-levelvideo understanding in the wild challenge uses VPQ and STQ to evaluate segmentation and tracking per-formance.Video Panoptic Quality (VPQ) for video panopticsegmentation is based on PQ (Panoptic Quality) andcomputes the average quality by using tube IoU matchingacross a small span of frames. Formally, the VPQ scoreacross k frames is:",
  ". Implementation Details": "In our method, we employ ViT-L as the backboneand Mask2Former as the segmenter for video segmentation.We divide it into three stages to train the segmenter, refer-ring tracker, and time refiner. In the first stage, we load theCOCO pre-trained weights to fine-tune the segmenta-tion by using image level annotations from the training setof VIPSeg. In the second stage, we freeze the segmentertrained in the first stage and use a continuous 5-frame clipfrom the video as input. In the third stage, we only train thetime refiner and freeze the segmenter and referring trackertrained in the first two stages, using continuous 21 frameclips as input.We train the panoptic segmentation model onthe training set of VIPSeg without using additionaldata such as validation set, conduct 40k iterations with abatch size of 4 and the learning rate is decayed by 0.1 at26k iterations. Multi-scale training from 480 to 800 is usedto randomly scale the short side of input video clips dur-ing training. Additionally, for training the refiner, we em-ploy a random cropping strategy with crop-size 608608from input video clips.For semantic segmentation, we useViT- adapter as the baseline to train on the VSPW dataset.",
  ". Architecture of ViT-Adapter": "there are segmentation holes and category misjudgments inthe output results of DVIS++, which seriously affected theVPQ score. In order to further improve the performanceof the model, we choose ViT-adapter as the semantic seg-mentation baseline, and correct the sequence of stuff classobjects and individual sequences with only one thing classobject in panoptic segmentation through model ensemble.",
  ". Result": "In the third PVUW Challenge, we rank first in the de-velopment phase and second in the test phase. The rankinglists for the development and test phases are shown in Ta-ble 1 and , respectively. Our method achieve VPQof 56.36 and 57.12 respectively during the developmentand testing phases, demonstrating strong segmentation per-formance. In addition, our method has significant advan-tages in tracking performance. The qualitative results of theVIPSeg test set are shown in ,which demonstrate",
  ". Ablation study of our method": "ments and attempts in many stages such as model, trainingand ensemble. In the end, we introduce DVIS++ to the VPSfield and verify that the decoupling strategy proposed byDVIS++ significantly improves the performance for boththing and stuff objects. Then,we add an additional imagesemantic segmentation model to further improve the per-formance of semantic classes. As a result, we get the 2ndplace in the VPS track of the PVUW Challenge 2024, scor-ing 56.36 VPQ and 57.12 VPQ in the development and testphases, respectively.",
  "Tao Zhang, Xingye Tian, Yu Wu, Shunping Ji, XueboWang, Yuan Zhang, and Pengfei Wan. Dvis: Decou-pled video instance segmentation framework. In IEEEICCV, pages 1282-1291, 2023. 1": "Tao Zhang, Xingye Tian, Haoran Wei, Yu Wu, Shun-ping Ji, Xuebo Wang, Xin Tao, Yuan Zhang, andPengfei Wan. 1st place solution for pvuw challenge2023: Video panoptic segmentation. In arXiv preprint,arXiv:2306.04091, 2023. 1 Tao Zhang, Xingye Tian, Yikang Zhou, Shunping Ji,Xuebo Wang, Xin Tao, Yuan Zhang, Pengfei Wan,Zhongyuan Wang, and Yu Wu. Dvis++: Improved de-coupled framework for universal video segmentation.arXiv preprint arXiv:2312.13305, 2023. 1, 2, 3 Inkyu Shin, Dahun Kim, Qihang Yu, Jun Xie, Hong-Seok Kim, Bradley Green, In So Kweon, Kuk-JinYoon, and Liang-Chieh Chen. Video-kmax: A sim-ple unified approach for online and near-online videopanoptic segmentation. In IEEE WACV, pages 228-238, 2024. 1 Ju He, Qihang Yu, Inkyu Shin, Xueqing Deng, Xi-aohui Shen, Alan Yuille, and Liang-Chieh Chen.Maxtron: Mask transformer with trajectory attentionfor video panoptic segmentation. In arXiv preprint,arXiv:2311.18537, 2023. 1",
  "Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He,Tong Lu, Jifeng Dai, and Yu Qiao.Vision trans-former adapter for dense predictions. arXiv preprintarXiv:2205.08534, 2022. 2, 3": "T. Moutakanni H. Vo M. Szafraniec V. Khalidov P.Fernandez D. Haziza F. Massa A. El-Nouby et al.M. Oquab, T. Darcet. Dinov2: Learning robust vi-sual features without supervision.arXiv preprintarXiv:2304.07193, 2023. 2 Bowen Cheng, Ishan Misra, Alexander G Schwing,Alexander Kirillov, and Rohit Girdhar.Masked-attention mask transformer for universal image seg-mentation.In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition,pages 12901299, 2022. 2 Jiaxu Miao,Xiaohan Wang,Yu Wu,Wei Li,Xu Zhang, Yunchao Wei, and Yi Yang. Large-scalevideo panoptic segmentation in the wild: A bench-mark.In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, 2022. 2 Jiaxu Miao, Yunchao Wei, Yu Wu, Chen Liang, Guan-grui Li, and Yi Yang. Vspw: A large-scale dataset forvideo scene parsing in the wild. In Proceedings of theIEEE/CVF conference on computer vision and patternrecognition, pages 41334143, 2021. 2 MaxwellCollinsYukunZhuPaulVoigtlaenderHartwig Adam Bradley Green Andreas Geiger Bas-tian Leibe Daniel Cremers-et al Mark Weber, Jun Xie.Step: Segmenting and tracking every pixel.arXivpreprint arXiv:2102.11859, 2021. 2"
}