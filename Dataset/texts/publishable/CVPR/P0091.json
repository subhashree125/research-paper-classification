{
  "Dinesh ManochaUniversity of MarylandCollege Park, MD, USA": ". Synthesizing unified co-speech 3D face and pose expressions. Our method uses the speech audio, the corresponding texttranscripts, the speakers unique IDs, and their sparse 3D face landmarks and pose sequences computed from RGB video data. It learns acombined embedding space that captures the correlations between all these inputs and leverages them to generate synchronous affectiveexpressions for faces and poses in a continuous motion space.",
  "Abstract": "We present a multimodal learning-based method to si-multaneously synthesize co-speech facial expressions andupper-body gestures for digital characters using RGB videodata captured using commodity cameras. Our approachlearns from sparse face landmarks and upper-body joints,estimated directly from video data, to generate plausibleemotive character motions. Given a speech audio wave-form and a token sequence of the speakers face landmarkmotion and body-joint motion computed from a video, ourmethod synthesizes the motion sequences for the speakersface landmarks and body joints to match the content and theaffect of the speech. We design a generator consisting of aset of encoders to transform all the inputs into a multimodalembedding space capturing their correlations, followed bya pair of decoders to synthesize the desired face and posemotions. To enhance the plausibility of synthesis, we usean adversarial discriminator that learns to differentiate be-tween the face and pose motions computed from the originalvideos and our synthesized motions based on their affectiveexpressions. To evaluate our approach, we extend the TEDGesture Dataset to include view-normalized, co-speech face landmarks in addition to body gestures. We demonstrate theperformance of our method through thorough quantitativeand qualitative experiments on multiple evaluation metricsand via a user study. We observe that our method results inlow reconstruction error and produces synthesized sampleswith diverse facial expressions and body gestures for dig-ital characters. The relevant source code and dataset areavailable at",
  ". Introduction": "Human communications through digital platforms and vir-tual spaces are prevalent in many applications, including on-line learning , virtual interviewing , counsel-ing , social robotics , automated character design-ing , storyboard visualizing for consumer media , and creating large-scale metaverse worlds . Sim-ulating immersive experiences in such digital applicationsnecessitates the development of plausible human avatarswith expressive faces and body motions. This is a chal-lenging problem to approach at scale, given the necessity",
  "arXiv:2406.18068v2 [cs.CV] 22 Nov 2024": "and diversity of human expressions in human-human inter-actions . Further, humans express simultaneouslythrough multiple cues or modalities, such as their speech,facial expressions, and body gestures , increasing thedimensionality of the problem. The emotional expressionsfrom these different modalities are also synchronous, i.e.,they follow the same rhythm of communication and com-plement each other to convey a sense of presence .In this paper, we consider the problem of synthesizing3D digital human motions with synchronous facial expres-sions and upper-body gestures aligned with speech audioinputs. Given the speech audio, existing approaches com-monly tackle the sub-problems of talking heads synthesizing lip movements and facial expressions giventhe speech audio, and co-speech gesture synthesis synthesizing poses for upper-body gestures, including headmotions. Recent approaches synthesize head and body mo-tions simultaneously , but consider a limited setof speakers and their expressions.More general motionsynthesis methods consider full-body motions from vari-ous modalities, including text prompts , object in-teractions , and guidance motions , but donot combine modalities (such as face and pose) in the out-put. The inherent difficulty in synthesizing expressions syn-chronized across diverse speakers is to under the correla-tions between the modalities for both the expressions andthe individual styles . In other words, not only is thecombined space of the multimodal expressions very high-dimensional, but only a small fraction of that space corre-sponds to valid expressions for different speakers. More-over, existing approaches generally require specialized data,such as dense 3D face scans and motion-captured ges-tures , often employ parameter-dense and compute-heavy approaches, such as those based on denoising diffu-sion to provide meaningful results. By contrast, weaim to develop a lightweight method for synchronous co-speech face and pose expressions by leveraging large-scalevideo datasets , paving the way to synthesize fully ex-pressive 3D digital humans for democratized use.Main Contributions.We present a multimodal learn-ing method to synthesize 3D digital characters with syn-chronous affective expressions on faces and upper-bodyposes, given speech audio. We also consider both intra- andinter-speaker variabilities by random sampling on a latentspace for speakers. Our main contributions include: Synchronous co-speech face and pose expressionsynthesis.Our method simultaneously synthesizesface and upper-body pose expressions given speech au-dio through a generative multimodal embedding spaceand an affective discriminator. Our method reduces themean absolute errors on the face landmarks by 30%,and the body poses by 21%, compared to the respec-tive baselines for faces and poses, thereby indicating",
  "measurable benefits over asynchronously combiningthe synthesized outputs of the two modalities": "Using data from affordable commodity cameras. Incontrast to facial expression synthesis using dense 3Dface scans or gesture synthesis from expensive motion-captured data, our method only relies on sparse facelandmarks and pose joints obtainable from commod-ity hardware such as video cameras. As a result, ourmethod scales affordably to large datasets and is appli-cable in large-scale social applications. Plausible motions, evaluation metric for facial ex-pressions. Through quantitative evaluations and userstudies, we verify that our synthesized synchronousexpressions are satisfactory to human observers. Wealso propose the Frechet Landmark Distance to evalu-ate the quality of the synthesized face landmarks. TED Gesture+Face Dataset.We extend the TEDGesture Dataset to include 3D face landmarks ex-tracted from the raw videos that we denoise and alignwith the poses. We release this multimodal dataset ofspeech audio, 3D face landmarks, and 3D body posejoints with our paper and the associated source code.",
  "We briefly review prior work on perceiving multimodal af-fective expressions, particularly from faces, speech, andgestures, and synthesis of co-speech face and pose motions": "Perceiving Multimodal Affective Expressions.Studiesin psychology and affective computing indicate that humansexpress emotions simultaneously through multiple modali-ties, including facial expressions, prosody and intonationsof the voice, and body gestures .Methods fordetecting facial expressions generally depend on fa-cial action units . Methods for detecting various af-fective vocal patterns commonly use Mel-Frequency Cep-stral Coefficients (MFCCs) . Methods to detect emo-tions from body gestures use physiological features, such asarm swings, spine posture, and head motions that are eitherpre-defined or learned automatically from the ges-tures . The emotions themselves can be represented ei-ther as discrete categories, such as the Ekman emotions or as combinations of continuous dimensions, such as theValence-Arousal-Dominance (VAD) model .In ourwork, we leverage the current approaches for detecting fa-cial, vocal, and pose expressions to design our co-speechface and gesture synthesis method. While we do not ex-plicitly consider specific emotions, our representation im-plicitly considers emotions in the continuous VAD space,leading to appropriately expressive face and pose synthesis. . Network architecture for synchronous synthesis of co-speech face and pose expressions. Our generator encodes all theinputs: the speech audio, the corresponding test transcript, the speaker ID, the seed 3D face landmarks, and the seed 3D poses into amultimodal embedding space. It decodes variables from this space to produce the synchronized sequences of co-speech 3D face landmarksand poses. Our discriminator classifies these synthesized sequences and the corresponding ground truths (3D motions of the originalspeakers), computed directly from the videos, into two different classes based both on their plausibility and their synchronous expressions.To obtain our rendered 3D character motions, we combine the outputs of our generator with our phoneme predictor network and map themto 3D meshes. Synthesizing Co-Speech Motions.We consider digitalcharacters with faces and body gestures.Co-Speech Facial Expressions. Wang and Soong com-pute controllable parameters for synthesizing talking headswith desired facial expressions using a Hidden MarkovModel and MFCCs of the speech audio. Recent techniquesautomate the facial motions for large-scale synthesis usinggenerative paradigms, such as VAEs and GANs .Karras et al. train a DNN to map speech audio to 3Dface vertices conditioned on learned latent features corre-sponding to different facial expressions. Zhou et al. ,learn sequences of predefined visemes using LSTM net-works from audio. Cudeiro et al. propose a dataset of4D face scans and learn per-vertex offsets to synthesize theface motions from audio. Richard et al. learn co-speechfacial motions using dense face meshes by disentangling fa-cial features correlated and uncorrelated with speech. Sinhaet al. focus on adding emotional expressions to thefaces. Lahiri et al. focus on the accuracy of the lipmovements and use an autoregressive approach to synthe-size 3D vertex sequences for the lips synced with the speechaudio. In contrast to these approaches, our facial expres-sion synthesis method uses much sparser 3D face landmarksdetected from real-world videos with arbitrary orientationsand lighting conditions of the faces w.r.t. the cameras, andsynthesizes mutually coherent facial and pose expressions.Co-Speech Gestures. Co-speech gesture synthesis is a spe-cial case of gesture stylization, where the style refers to thepose expressions inferred from and aligned with the speech.This line of work has been richly explored . Ginosar et al. propose a method to synthe-size speaker-specific co-speech gestures by training a neuralnetwork given their identities and individual gesticulationpatterns. Ferstl et al. additionally propose using adver-sarial losses in the training process to improve the fidelity ofthe synthesized gestures. Yoon et al. extend the concept of individualized gestures to a continuous space of speakersto incorporate natural variability in the synthesized gestureseven for the same speaker. Bhattacharya et al. build ontop of to improve the affective expressions in the co-speech gestures. More recent methods have also exploreddiffusion-based approaches for editability . Our methodconditions the gesture synthesis on both the input speechand the synthesized facial expressions. Co-Speech Multimodal Expressions. Co-speech face andupper-body generation has gained particular interest re-cently, primarily due to the availability of rich 3D datasetsof popular speakers . Current approaches train adver-sarial encoder-decoder models on datasets of one speaker ata time and use vector quantization for tokenized gener-ation using a transformer . These approaches consider afixed set of speakers and lose fine-grained expressions whenusing quantization. In our work, we consider the combinedcontinuous space of affective face and body expressions anddevelop a network generalizable to multiple speakers.",
  ". Synchronous Face and Pose Synthesis": "Given a speech audio waveform a, the corresponding texttranscript w, the speakers unique ID k in a set of speakersK, and the associated seed face landmark deltas f1:Ts andseed pose unit vectors u1:Ts, Ts being the number of seedtime steps, we synthesize the synchronous sequences of facelandmark deltas f1:T and pose unit vectors u1:T for thespeaker for the T prediction time steps (T Ts), match-ing the content and the affect in their speech. We describeour end-to-end pipeline, including a detailed description ofour inputs and outputs and their usage. We also provide thedetails of obtaining these facial landmarks and poses frominput videos.",
  ". Face and Pose Preprocessing from Video": "Given a video, we use Multi-Task Cascaded CNNs toextract the 3D face landmarks. Since the faces can be arbi-trarily oriented w.r.t. the camera, we rigidly transform theface landmarks per frame to a reference frame in the nor-malized view, where the face looks towards the camera. Foreach frame in the input video, we use the rotation and thetranslation given by the Umeyama method to map theface landmarks in that frame to the face landmarks in thereference frame. We also use similarly view-normalized 3Dposes. View normalization is helpful for two key reasons.First, it eliminates relative camera movements across thevideo frames and prevents a learning-based method fromconfusing camera movements with face and pose expres-sion changes. Second, a frontal view offers maximal vis-ibility of the faces and the poses, and minimizes errors indetecting the 3D face landmarks and body joints.",
  "Ft = F + ft,(1)": "where ft RL denotes the set of relative motions of thelandmarks w.r.t. F at time step t.On the other hand, we assume the body joints are rigidlyconnected by the bones.We represent each users bodyjoints as 3D point vectors P RJ3 in a global coordi-nate space, where J is the number of joints. We considerdirected line vectors connecting adjacent joints. The direc-tion is along the path from the root (pelvis) joint to the endeffectors (such as wrists). These 3D point and line vectorscollectively form a directed tree with J nodes and J 1edges. We assume that the magnitudes of these line vectorscorrespond to the bone lengths and that these magnitudesare known and fixed. To synthesize the users body ges-tures, we compute the orientations of these line vectors ateach time step t in the reference frame of the global coor-dinate space. Specifically, for each bone b with bone length(magnitude) b and connecting the source joint sb (t) tothe destination joint db (t) at time step t, we compute a unitvector ut such that",
  ". Synthesizing Faces and Poses": "Our network architecture () consists of a phonemepredictor to predict the lip shapes corresponding to the au-dio and a generator-discriminator pair to synthesize plau-sible co-speech face and pose expressions. We design ourphoneme predictor following prior approaches and pro-vide its details in Sec. 3.3.5. Our generator follows a multi-modal learning strategy. It consists of separate encoders totransform the speech audio, the text transcript, the speakerID, the seed face landmark deltas, and the seed pose unitvectors into a latent embedding space representing their cor-relations. It subsequently synthesizes the appropriate faceand pose motions from this multimodal embedding space.Our discriminator enforces our generator to synthesize plau-sible face and pose motions in terms of their affective ex-pressions. To this end, we use the same encoder architecturefor the faces and the poses as in our generator, but learnedseparately. We describe each of the components of our gen-erator and discriminator.",
  "k, k = SpeakerEncoder (k; speaker) ,(5)": "where speaker represents the trainable parameters. The latentdistribution space enables us to sample a random vector krepresenting a speaker who is an arbitrary combination ofthe K speakers in the dataset. This allows for variations inthe synthesized motions even for the same original speakerby slightly perturbing their speaker IDs in the latent distri-bution space, leading to more plausible results on multipleruns of our network. To learn faces and poses with appro-priate expressions, we represent them as multi-scale graphsand encode them using graph convolutional networks.",
  "(d) Pose Decoder": ". Face and pose encoders and decoders. We show their architectures with the layer sizes denoted (details in Sec. 3.3.2).Our architectures depend on the hierarchical anatomical component (AC) graphs for both faces and poses that efficiently learn theircorresponding affect representations using spatial-temporal graph convolutions (green nodes and edges), 2D convolutions (teal blocks), 2Dbatch normalizations (pink blocks), and fully-connected layers (orange planes).",
  "Our face landmarks are based on action units . We rep-resent the sequence of 3D landmarks f1:Ts RTsL3": "as a spatial-temporal anatomical component (AC) graph.Spatially, we consider landmarks belonging to the sameanatomical component (Sec. 3.2) and nearest landmarksacross different anatomical components to be adjacent.Temporally, all landmarks are adjacent to their temporalcounterparts (same nodes at different time steps) within apredetermined time window.We consider the eyes, thenose, the lips, and the lower jaw as the anatomical com-ponents.We show the face landmarks graph in a with all the intra- and inter-anatomical-component adjacen-cies marked with lines. We apply a sequence of spatial-temporal graph convolutions on this graph to learn from thelocalized motions of the landmarks and obtain embeddingsf RTsLDf of feature dimension Df as",
  "f = STGCNff1:Ts; STGCNf,(6)": "where STGCNf represents the trainable parameters.Weobtain a face anatomy graph from the landmarks graph,where we consider the nodes to represent entire anatomi-cal components and the graph to be fully connected. Tocompute such a graph, we append the features of intra-anatomical-component nodes in the graph into collated fea-tures l RTsLlnlDf , where Ll denotes the number ofanatomical components and nl denotes the number of land-mark nodes within each anatomical component. We take nlas the number of nodes in the anatomical component withthe most landmarks and perform zero padding as appropri-ate to obtain the full collated features for the other compo-nents. This hierarchically pooled representation provides ahigher-level view of the face and helps our network learnfrom the correlations between the motions of the differentanatomical components. Specifically, we use another set ofspatial-temporal graph convolutions to obtain the embed-dings l RTsLlDl of feature dimension Dl as",
  "where STGCNl represents the trainable parameters.Col-lectively, the landmarks graph and the face anatomy graph": "provide complementary information to our network to en-code and synthesize the required facial expressions at boththe macro (anatomy) and the micro (landmark) levels. Tocomplete our encoding, we flatten out the features of allthe anatomical components in l, i.e., reshaping such thatl RTsLlDl, and transform them using standard convo-lutional layers on the flattened feature channel and the tem-poral channel separately. This gives us our latent space em-beddings l RT Dl as",
  "l = ConvTlConvSll; ConvSl; ConvTl,(8)": "where ConvSl and ConvTl represent the trainable parameters.For the pose representation, we consider a pose graph ofthe upper body with J 1 bones represented with line vec-tors u1:Ts (b). We consider bones connected to eachother or connected through a third bone to be adjacent. Weuse a set of spatial-temporal graph convolutions to leveragethe localized motions of these bones and obtain embeddingsu RTsDu of feature dimension Du as",
  "u = STGCNu (u1:Ts; STGCNu) ,(9)": "where STGCNu represents the trainable parameters. Sim-ilar to the face landmarks, we also consider a hierarchi-cally pooled representation of the bones v RTsLjnjDu,where Lj = 3 are the three anatomical components, thetorso and the two arms, represented as single nodes eachconsisting of nj nodes from the pose graph. In the poseanatomy graph, we consider the two arms to be adjacentto the torso but not to each other, as they can move inde-pendently. We apply a second set of spatial-temporal graphconvolutions on the collated features v to obtain the embed-dings v RTsLjDv as",
  "where ConvSv and ConvTv represent the trainable params": ". Qualitative results. Snapshots from two of our syn-thesized samples showing the text transcript of the speech and thecorresponding face and pose expressions (row 1). We also zoomin on the eyebrow (row 2) and lip (row 3) expressions for bettervisualization. We observe a smile, raised eyebrows, and stretchedarms (left) for the word excited, and frowns on the eyebrows andlips (right) for the words very sorry.",
  "Synthesizing Synchronous Motions": "Our synchronous synthesis relies on learning the multi-modal distributions of the individual modalities of audio,text, speaker ID, face expressions, and pose expressions,given their individual distributions.To this end, we ap-pend all the latent space embeddings a for the audio,w for the text, k for the random speaker representation, re-peated over all the T time steps, l for the seed landmarksand v for the seed poses into a vector e RT H rep-resenting a multimodal embedding space of all the inputs.Here, H = Da + Dw + Dk + Dl + Dv denotes the la-tent space dimension. On training, our network learns thecorrelations between the different inputs in this multimodalembedding space. To synthesize our face landmark motionsf1:T RT L3, we apply separate spatial and temporalconvolutions on the multimodal embeddings e to capturelocalized dependencies between the feature values followedby fully-connected layers capturing all the dependencies be-tween the feature values (c), as",
  "f1:T = FCf eConvSf eConvTf ee; ConvTf e; ConvSf e; FCf e, (12)": "where ConvTf e, ConvSf e, and FCf e represent the trainableparameters. The output f1:T from the fully connected layershas shape T 3L, which we reshape into T L 3 to getour desired 3D face landmark sequences.We similarly synthesize the line vectors u1:TRT (J1)3 using separate spatial and temporal convolu-tions on the multimodal embeddings e, followed by fully-connected layers (d), as",
  "u1:T = FCue (ConvSue (ConvTue (e; ConvTue) ; ConvSue) ; FCue), (13)": "where ConvTue, ConvSue, and FCue represent the trainableparameters. Given the synthesized face and pose motions,we use our discriminator to determine how well their affec-tive expressions match the corresponding ground truths inthe training data. We obtain our ground truths as the 3Dface landmarks and the 3D pose sequences computed fromthe full training video data. . Qualitative comparisons. For the same input speech,represented by the text transcript at the top, we compare the vi-sual quality of our synthesized character motions with the originalspeaker motions and three of our ablated versions: one withoutsynchronous face and pose synthesis, one without our anatomicalcomponent (AC) graphs for faces and poses, and one without ourdiscriminator. We observe that our synthesized motions are visu-ally the closest to the original speaker motions compared to theablated versions. We elaborate on their visual qualities in Sec. 6.4.",
  "Determining Plausibility Using Discriminator": "Our discriminator takes in the synchronously synthesizedface motions f1:T and pose motions u1:T , and encodes themusing encoders with the same architecture as our generator(Sec. 3.3.2), with only the number of input time steps be-ing T instead of Ts. This gives us the corresponding la-tent space embeddings l and v. Similar to our generator,we concatenate these embeddings into a multimodal em-bedding vector e RT (Dl+Dv). But different from ourgenerator, we pass these multimodal embeddings througha fully-connected classifier network FCdisc to obtain classprobabilities cdisc per sample, as",
  "cdisc = FCdisc (e; FCdisc) ,(14)": "where FCdisc represents the trainable parameters. Our dis-criminator learns to perform unweighted binary classifica-tion between the synthesized face and pose motions and theground truths in terms of their synchronous affective expres-sions. Our generator, on the other hand, learns to synthesizesamples that our discriminator cannot distinguish from theground truth based on those affective expressions.",
  "Phoneme Predictor": "We train a separate network to learn the positions of thelip landmarks for the different phonemes in the audio. Oursynthesis network separately learns the motions of the lipcorners denoting the different facial expressions, and we su-perpose them to the phoneme-based lip shapes to complete the lip motions. Our phoneme predictor predicts the 3D po-sitions of all the landmarks on the inner and the boundariesof the lips over all the T prediction time steps, which we de-note as p1:T inRT Llip3. Following prior approaches ,we design a CNN backbone connected to fully connectedblocks to predict the lip landmarks from the spectrogramsof the speech inputs. Specifically, given the speech audiowaveform a, we compute",
  "We present our TED Gesture+Face Dataset, which we useto train and test our network. We elaborate on collectingand processing our dataset for training and testing": "Dataset Collection.The TED Gesture Dataset con-sists of videos of TED talk speakers together with text tran-scripts of their speeches and their 3D body poses extractedin a global frame of reference. The topics range from per-sonal and professional experiences to discourses on educa-tional topics and instructional and motivational storytelling.The speakers come from a wide variety of social, cultural,and economic backgrounds, and are diverse in age, gender,and physical abilities. Dataset Processing.The 3D poses in the original TEDGesture Dataset are view-normalized to face frontand center at all time steps. We compute similarly view-normalized 3D face landmarks of the speakers (Sec. A.1).Similar to the original TED dataset, we divide the 3D poseand face landmark sequences into equally-sized chunks ofsize T = 34 time steps at a rate of 15 fps. Additionally,to reduce the jitter in the predicted 3D face landmarks andpose joints from each video, we sample a set of anchorframes at a rate of 5 fps and perform bicubic interpolationto compute the face landmark and pose joint values in theremaining frames. We use the first 4 time steps of pose andface landmarks as our seed values (Sec. 3.3), and predictthe next 30 time steps. The processed dataset consists of200,038 training samples, 26,903 validation samples, and26,245 test samples, following a split of 80%-10%-10%.",
  ". Training Procedure": "We train our phoneme predictor network using the Adamoptimizer with 1 = 0.5, 2 = 0.999, a batch sizeof 1024, and a learning rate of 103 for 500 epochs. Wetrain our synthesis network using the Adam optimizer with 1 = 0.5, 2 = 0.999, a batch size of 256, and learn-ing rates of 104 for our generator and 5 105 for ourdiscriminator, both decayed by a factor of 0.999 per epoch,for 1000 epochs. We train both our phoneme detector net-work and our synthesis network on an NVIDIA GeForceRTX 2080 Ti GPU, which takes 3 seconds and 7 secondsper epoch, respectively.",
  ". Testing Procedure and Mapping to DigitalCharacters": "Each test sample for our network consists of a speech audiowaveform, the corresponding text transcript, a speaker ID,and the speakers seed face and pose motions. Our phonemepredictor network provides the lip sync for the given speechaudio, and the generator of our synthesis network providesthe required face and pose motions. We superpose the liplandmarks given by our phoneme predictor network withthe lip corner landmarks given by our generator at eachprediction time step to obtain the complete lip motions ofthe speaker. We map these motions to a rigged 3D humanupper-body mesh in Blender. For mapping the face motions,we set a one-to-one mapping between our face landmarksand the landmarks on the face of the human mesh, and usethem as control points for the facial motions of the mesh.For mapping the pose motions, we use FABRIK to ob-tain the joint rotations given our predicted joint positionsand use those rotations to animate the rigged human mesh. . Quantitative evaluations. Comparison with existingco-speech gesture synthesis methods and our ablated versions(Sec. 6.1) on the metrics MALE (in mm), MAJE (in mm), MAcEfor landmarks (MAcE-LM) (in mm/s2), MAcE for poses (MAcE-P) (in mm/s2), FLD, and FGD (Sec. 6.2). Lower values are better,bold indicates best, and underline indicates second-best.",
  ". Rendering and Visualization": "Given an input speech audio, we can synthesize the motionsfor our pre-rigged digital characters at an interactive rateof about 250 frames per second on an NVIDIA GeForceRTX 2080 Ti GPU. We design our digital environment us-ing Blender. For each of our digital characters, we placethem on a stage and position the camera such that it looksfront and center at the agent. As the character narrates theinput speech audio using our synthesized face and upper-body expressions, we slowly pan the camera in to get a morefocused view of those expressions. Since we do not syn-thesize any lower-body motions, our digital characters staystanding at their initial positions during the entire narration.The full video demos are available with our supplementarymaterial.",
  ". Experiments and Results": "We run quantitative experiments using ablated versions ofour method as baselines. We note that Habibie et al. retrain their network separately for individual speakers be-longing to the same profession (talk show hosts), making itunsuitable for our generalized paradigm consisting of lessthan 50 samples each of multiple, diverse speakers.Yiet al. use VQ with transformers to synthesize faces andgestures, but are limited to the same set of fixed speakers.We also conducted a web-based user study to evaluate thequalitative performance of our method.",
  ". Baselines": "We use seven ablated versions of our method as baselines.The first two ablations correspondingly remove the entireface (Figs. 3a, 3c) and pose components (Figs. 3b, 3d) fromour network, making our network learn only talking headand only co-speech gesture syntheses. The third ablation removes the velocity and acceleration losses from our re-construction loss (Eqn. C.2) , leading to jittery motions.The fourth ablation removes the discriminator and its asso-ciated losses (Eqn. C.4) from our training pipeline, leadingto unstable motions without appreciable expressions. Thefifth and the sixth ablations correspondingly remove thehigher-level anatomical component (AC) graphs of thefaces (Eqn. 7) and the poses (Eqn. 10), leading to reducedmovements. The final ablation trains the face and the poseexpressions separately, learning marginal embeddings forthe two modalities based on the speech but not attending totheir mutual synchronization. This ablation directly evalu-ates the co-speech motions when combining separately syn-thesized face and pose expressions. For completeness, wealso compare with co-speech gesture synthesis methods thatonly synthesize body poses. We evaluate all the methods onour TED Gesture+Face Dataset.",
  ". Evaluation Metrics": "Inspired by prior work , we evaluate using four recon-struction errors and two plausibility errors (PEs). Our re-construction errors include the mean absolute landmark er-ror (MALE) for the faces, the mean absolute joint error(MAJE) for the poses, and their respective mean acceler-ation errors (MAcEs). MALE and MAJE indicate the over-all fidelity of the synthesized samples w.r.t. the correspond-ing ground truths, and the MAcEs indicate whether or notthe synthesized landmarks and poses have regressed to theirmean absolute positions. To report these metrics, we multi-ply our ground truth and synthesized samples by a constantscaling factor such that they all lie inside a bounding box ofdiagonal length 1 m. For our PE, we use the Frechet GestureDistance (FGD) designed by to indicate the perceivedplausibility of the synthesized poses. To similarly indicatethe perceived plausibility of the synthesized face landmarks,we also design the Frechet Landmark Distance (FLD). Wetrain an autoencoder network to reconstruct the set of facelandmarks at all time steps for all the samples in the trainingset of our TED Gesture+Face Dataset. To compute FLD, wethen obtain the Frechet Inception Distance between theencoded features of the ground truth and the synthesizedsamples.",
  "We show our quantitative evaluations in": "Comparison with Co-Speech Gesture Synthesis.Sinceco-speech gesture synthesis methods do not synthesize faceexpressions, we leave those numbers blank. For these meth-ods, we have taken the numbers reported by Bhattacharyaet al. . For the method of SpeechGestureMatching ,we retrain their method on the TED Gesture Dataset to re-port the numbers. However, we were unable to perform similar comparative evaluations with co-speech face syn-thesis methods as existing methods synthesize dense land-marks or blendshape-like features , which cannotbe mapped one-to-one with our sparser face landmarks. Comparison with Ablated VersionsRemoving eitherthe face or the gesture components of our network leadsto poorer values across the board than using both. With-out the velocity and acceleration losses, the motions are jit-tery, and the MAcE losses are higher, especially MAcE forthe face landmarks. Without the discriminator, the synthe-sized samples suffer from mode collapse and often produceimplausible motions, leading to higher values across theboard. Without the AC graphs, there are fewer movementsin the synthesized motions and the reconstruction errorsare higher. When synthesizing face and pose expressionsseparately and not synchronizing them, we observe somemismatches in when the expressions from either modalityappear and how intense they are. This indicates that syn-chronous synthesis of facial expressions and body gesturesleads to more accurate and plausible movements for boththe modalities, including a 30% improvement on MALEand a 21% improvement on MAJE, compared to triviallycombining synthesized outputs of the individual modalities.",
  ". Qualitative Comparisons": "We visualize some of our synthesized samples in andprovide more results in our supplementary video. We ob-serve the synchronization between the face and the poseexpressions for two contrasting emotions. We also visu-ally compare with the original speaker motions renderedusing their face landmarks and the poses extracted fromthe videos and three of our ablated versions in . Theoriginal speaker motions provide an upper bound of ourperformance. The three ablated versions we compare withare: one without the synchronous synthesis, one without ourface and pose AC graphs, and one without our discrimina-tor. The ablated versions without either the face or the posesynthesis, without the velocity and acceleration losses, andwithout our discriminator are visually inferior in obviousways. Therefore, we leave them out. Without either faceor pose synthesis, that modality remains static while thereis movement in the other. Without the velocity and the ac-celeration losses, the overall motions regress to the meanpose. Without our discriminator, our generator often failsto understand plausible movement patterns, leading to un-natural limb and body shapes. Of these, we only keep theablations without our discriminator as our lower boundbaseline because, unlike the other two, this ablation has vis-ible movements in both the face and the pose modalities. . Likert-scale score statistics. We compute the mean andthe standard deviation of the Likert-scale scores across all the mo-tions. For the mean scores, higher values are better, bold indicatesbest, and underline indicates second-best.",
  "We conducted a web-based user study to evaluate the visualquality of our synthesized motions in terms of their plausi-bility and synchronization": "Setup.A total of 90 participants participated in our userstudy. All participants were aged 18 years or older, and hadnormal or corrected to normal vision and hearing. Each par-ticipant observed two sets of character motions. There wereeight groups of motions in each set, each group having aunique input speech. In the first set, there were four typesof motions in each group corresponding to the same speech:the original speaker motions rendered using their face land-marks and the poses extracted from the video, and motionsrendered using the face landmarks and poses synthesizedby our network and two of its ablated versions. One ab-lated version was without using the face and pose anatom-ical component (AC) graphs for training, and one withoutour discriminator. In the second set, there were three typesof motions in each group corresponding to the same speech:the original speaker motions, motions rendered using theface landmarks and poses synthesized by our network, andthe ablated version using asynchronously synthesized facesand poses. Our motivation to separately compare with theasynchronously synthesized motions was to eliminate dis-tractors from other motions and enable our participants tofocus more closely on the synchronization between the faceand the pose expressions. We randomized the order of thesemotions in each group in each set and kept the order un-known to the participants. We did not present our otherablated versions to the participants as they did not have suf-ficient motion and were visually inferior in obvious ways. Evaluation Process.Our aim in the user study is to eval-uate our synthesized motions on two key aspects: (i) howplausible they appear to human observers compared to themotions of the original speakers and the ablated versions,and (ii) whether synchronous synthesis of face and pose expressions produces perceptible improvements over asyn-chronous synthesis.To evaluate plausibility, we ask theparticipants to rate each motion in each group in each seton how natural the motion looks on a five-point Likertscale, with the options very unnatural (worst), not realis-tic, looks OK (average), looks good, and looks great(best). To evaluate the effect of synchronous synthesis, weask the participants to observe the face and the pose move-ments in each motion in each group in each set and ratethem on how the face and the pose sync with the speechon a five-point Likert scale, with the options no/arbitrarymovements (worst), slight movements, has movements,but are not expressive (average), somewhat expressivemovements, and have movements with appropriate ex-pressions (best). Results.Since we randomly select the speech for each ofthe eight groups of motions each participant watched, andwe also randomized the order of the motions in each groupin each set, we can consider the participants responses ineach group to be independent of all the other groups. Thus,we aggregate their responses to each type of motion acrossall the groups within a set to obtain the overall distributionsof the Likert-scale scores of the motions for that set. Weshow these distributions for each of the two questions onplausibility and synchronization in each set in . Wealso report the Likert-scale score statistics for each type ofmotion on the two questions in each set in . For thepurpose of scoring, we assign scores 1 through 5, with 1 forworst and 5 for best. We observe that the scores for oursynthesized samples are comparable to the correspondingoriginal speaker motions and significantly better than theablated versions. To further affirm this, we plot the cumu-lative lower bound of participant responses for each Likert-scale score for each type of motion in each set in .We note that the scores for our synchronously synthesizedsamples remain close to the original speaker scores and con-sistently above the other ablated versions, indicating a clearpreference. Overall, in the two sets, 88.89% and 80.00%participants respectively marked our synchronously synthe-sized motions 3 or above on the first question, and 65.46%and 62.87% participants respectively marked 3 or above onthe second question. This indicates that the majority of par-ticipants found the motions satisfactory.",
  ". Conclusion, Limitations and Future Work": "We have presented a method to synthesize synchronous co-speech face and pose expressions for 3D digital characters.Our method learns to synthesize these expressions from 3Dface landmarks and 3D upper-body pose joints computeddirectly from videos. Our work also has some limitations.We use sparse face landmarks and pose joints to synthesizeco-speech face and pose expressions. To synthesize more (a) Set 1: Motion plausibility. Compared to the ablated versions, we ob-serve a higher distribution of OK or better for the motions of the originalspeakers and our synthesized agents. The modes of all the distributionsare on OK, implying that the corresponding participants found the visualqualities of all the motions to be reasonable. (b) Set 1: Synchronization between the face and the pose expressionsgiven the speech. Compared to the ablated versions, we obverse clearpreferences for the motions of the original speakers and our synthesizedagents. The modes of the distributions for these two types of motions areon somewhat expressive while the modes of the two ablated versions areon no/arbitrary movements. (c) Set 2: Motion plausibility. Compared to the ablated version withoutsynchronous synthesis, we observe a higher distribution of OK or betterfor the motions of the original speakers and our synthesized agents. Similarto the motion plausibility in set 1, we observe modes of all the distributionson OK. (d) Set 2: Synchronization between the face and the pose expressionsgiven the speech. We again obverse clear preferences for the motions ofthe original speakers and our synthesized agents compared to the ablatedversion without synchronous synthesis. However, in contrast to the samestudy in set 1, we notice the modes of the distributions for the first two typesof motions are one point lower on the Likert scale, whereas the mode forthe ablated version remains on no/arbitrary movements. We hypothesizethis to be the consequence of removing the other ablated versions fromthe participants cognitive window: in the absence of other variants, theparticipants focused more closely on the relative qualities of asynchronousvs. synchronous motions and assessed them more critically. . Distributions of the user study responses. Likert-scale response distributions to the two sets of motions rendered using the fivedifferent types of face landmark and pose data (Sec. 7). We show the distributions of each of the five Likert-scale points for each type ofmotion as a percentage of the total responses across all the groups in each set. . Cumulative lower-bound of participant responses.We plot the cumulative lower-bound (LB) percentage of responsesacross the Likert-scale scores for each type of character motion ineach set. A cumulative LB percentage X for a Likert-scale scores denotes X% of responses had a score of s or higher. We observethat the curve for our synchronously synthesized motions stays atthe top, indicating that the participants preferred it over the othermotions. fine-grained expressions, we plan to extract more detailedface meshes and additional pose joints from videos. Fur-ther, given the sparsity of our face and pose representationsand the noise associated with extracting them from videos,the quality of our synthesized motions does not match thosesynthesized from high-end facial scans and motion-capturedata, and using parameter-dense, compute-heavy methods,such as denoising diffusion.We aim to bridge this gap by building techniques to develop more robust face andpose representations from videos. We also plan to combineour work with lower-body actions such as sitting, standing,and walking to synthesize 3D-animated digital humans in awider variety of scenarios. In terms of its running-time cost,our method uses commercial GPUs to obtain real-time per-formance. We plan to explore knowledge distillation tech-niques to reduce our running-time cost and implement ourmethod in real-time on commodity devices such as digitalpersonal assistants.",
  "Andreas Aristidou and Joan Lasenby. Fabrik: A fast, iter-ative solver for the inverse kinematics problem. GraphicalModels, 73(5):243260, 2011. 8": "Abhishek Banerjee, Uttaran Bhattacharya, and Aniket Bera.Learning unseen emotions from gestures via semantically-conditioned zero-shot perception with adversarial autoen-coders. Proceedings of the AAAI Conference on ArtificialIntelligence, 36(1):310, 2022. 2 T. Baur, I. Damian, P. Gebhard, K. Porayska-Pomsta, and E.Andre. A job interview simulation: Social cue-based inter-action with a virtual character. In 2013 International Con-ference on Social Computing, pages 220227, 2013. 1 Uttaran Bhattacharya, Trisha Mittal, Rohan Chandra, Tan-may Randhavane, Aniket Bera, and Dinesh Manocha. Step:Spatial temporal graph convolutional networks for emotionperception from gaits. In Proceedings of the Thirty-FourthAAAI Conference on Artificial Intelligence, page 13421350.AAAI Press, 2020. 2 Uttaran Bhattacharya, Christian Roncal, Trisha Mittal, Ro-han Chandra, Kyra Kapsaskis, Kurt Gray, Aniket Bera, andDinesh Manocha. Take an emotion walk: Perceiving emo-tions from gaits using hierarchical attention pooling and af-fective mapping. In Computer Vision ECCV 2020, pages145163, Cham, 2020. Springer International Publishing. 2 UttaranBhattacharya,ElizabethChilds,NicholasRewkowski,andDineshManocha.Speech2AffectiveGestures:Synthesizing Co-Speech Ges-tures with Generative Adversarial Affective ExpressionLearning, page 20272036.Association for ComputingMachinery, New York, NY, USA, 2021. 3, 8, 9 Uttaran Bhattacharya,Nicholas Rewkowski,AbhishekBanerjee, Pooja Guhan, Aniket Bera, and Dinesh Manocha.Text2gestures: A transformer-based network for generatingemotive body gestures for virtual agents. In 2021 IEEE Con-ference on Virtual Reality and 3D User Interfaces (IEEEVR). IEEE, 2021. 2 CarlosBusso,MurtazaBulut,Chi-ChunLee,AbeKazemzadeh, Emily Mower, Samuel Kim, Jeannette NChang, Sungbok Lee, and Shrikanth S Narayanan. Iemo-cap: Interactive emotional dyadic motion capture database.Language resources and evaluation, 42(4):335359, 2008. 2 Yujun Cai, Liuhao Ge, Jun Liu, Jianfei Cai, Tat-Jen Cham,Junsong Yuan, and Nadia Magnenat Thalmann.Exploit-ing spatial-temporal relationships for 3d pose estimationvia graph convolutional networks.In Proceedings of theIEEE/CVF International Conference on Computer Vision(ICCV), 2019. 3 Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, AnuragRanjan, and Michael J. Black. Capture, learning, and synthe-sis of 3d speaking styles. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition(CVPR), 2019. 2, 3, 9 Rishabh Dabral, Muhammad Hamza Mughal, VladislavGolyanik, and Christian Theobalt. Mofusion: A frameworkfor denoising-diffusion-based motion synthesis. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pages 97609770, 2023. 2 David DeVault, Ron Artstein, Grace Benn, Teresa Dey, EdFast, Alesia Gainer, Kallirroi Georgila, Jon Gratch, ArnoHartholt, Margaux Lhommet, et al.Simsensei kiosk: Avirtual human interviewer for healthcare decision support.In Proceedings of the 2014 international conference on Au-tonomous agents and multi-agent systems, pages 10611068,2014. 1 Paul Ekman. Are there basic emotions? 1992. 2 Ylva Ferstl, Michael Neff, and Rachel McDonnell. Multi-objective adversarial gesture generation. In Motion, Interac-tion and Games, New York, NY, USA, 2019. Association forComputing Machinery. 3 Anindita Ghosh, Noshaba Cheema, Cennet Oguz, ChristianTheobalt, and Philipp Slusallek. Synthesis of compositionalanimations from textual descriptions.In Proceedings ofthe IEEE/CVF International Conference on Computer Vision(ICCV), pages 13961406, 2021. 2 Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Chris-tian Theobalt, and Philipp Slusallek.Imos: Intent-drivenfull-body motion synthesis for human-object interactions.Computer Graphics Forum, 42(2):112, 2023. 2",
  "Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Chris-tian Theobalt, and Philipp Slusallek. Remos: Reactive 3dmotion synthesis for two-person interactions. arXiv preprintarXiv:2311.17057, 2023. 2": "Panagiotis Giannopoulos, Isidoros Perikos, and IoannisHatzilygeroudis.Deep Learning Approaches for FacialEmotion Recognition: A Case Study on FER-2013, pages 116. Springer International Publishing, Cham, 2018. 2 Shiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, An-drew Owens, and Jitendra Malik. Learning individual stylesof conversational gesture. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition(CVPR), 2019. 3, 8 Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, BingXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, andYoshua Bengio. Generative adversarial nets. In Advances inNeural Information Processing Systems. Curran Associates,Inc., 2014. 8",
  "Jamy Li, Rene Kizilcec, Jeremy Bailenson, and Wendy Ju.Social robots and virtual agents as lecturers for video instruc-tion. Computers in Human Behavior, 55:1222 1230, 2016.1": "Jing Li, Di Kang, Wenjie Pei, Xuefei Zhe, Ying Zhang,Zhenyu He, and Linchao Bao. Audio2gestures: Generatingdiverse gestures from speech audio with conditional varia-tional autoencoders. In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision (ICCV), pages1129311302, 2021. 3 M. Liao, C. Sung, H. Wang, and W. Lin. Virtual classmates:Embodying historical learners messages as learning com-panions in a vr classroom through comment mapping. In2019 IEEE Conference on Virtual Reality and 3D User In-terfaces (VR), pages 163171, 2019. 1 Haiyang Liu, Naoya Iwamoto, Zihao Zhu, Zhengqing Li,You Zhou, Elif Bozkurt, and Bo Zheng. Disco: Disentangledimplicit content and rhythm learning for diverse co-speechgestures synthesis. In Proceedings of the 30th ACM Inter-national Conference on Multimedia, page 37643773, NewYork, NY, USA, 2022. Association for Computing Machin-ery. 3 Xian Liu, Qianyi Wu, Hang Zhou, Yuanqi Du, Wayne Wu,Dahua Lin, and Ziwei Liu.Audio-driven co-speech ges-ture video generation. In Advances in Neural InformationProcessing Systems, pages 2138621399. Curran Associates,Inc., 2022.",
  "Xian Liu, Qianyi Wu, Hang Zhou, Yinghao Xu, Rui Qian,Xinyi Lin, Xiaowei Zhou, Wayne Wu, Bo Dai, and Bolei": "Zhou. Learning hierarchical cross-modal association for co-speech gesture generation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition(CVPR), pages 1046210472, 2022. 3 S. Mascarenhas, M. Guimaraes, R. Prada, J. Dias, P. A. San-tos, K. Star, B. Hirsh, E. Spice, and R. Kommeren. A vir-tual agent toolkit for serious games developers.In 2018IEEE Conference on Computational Intelligence and Games(CIG), pages 17, 2018. 1",
  "Brian Parkinson,Agneta H Fischer,and Antony SRManstead. Emotion in social relations: Cultural, group, andinterpersonal processes. Psychology press, 2005. 2": "Shenhan Qian, Zhi Tu, Yihao Zhi, Wen Liu, and ShenghuaGao. Speech drives templates: Co-speech gesture synthesiswith learned templates.In Proceedings of the IEEE/CVFInternational Conference on Computer Vision (ICCV), pages1107711086, 2021. 3 Alexander Richard, Michael Zollhofer, Yandong Wen, Fer-nando de la Torre, and Yaser Sheikh. Meshtalk: 3d face an-imation from speech using cross-modality disentanglement.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision (ICCV), pages 11731182, 2021. 3 N. Sadoughi and C. Busso. Novel realizations of speech-driven head movements with generative adversarial net-works. In 2018 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 61696173,2018. 3 I. Santos, N. Nedjah, and L. de Macedo Mourelle. Senti-ment analysis using convolutional neural network with fast-text embeddings. In 2017 IEEE Latin American Conferenceon Computational Intelligence (LA-CCI), pages 15, 2017.4",
  "Yoni Shafir, Guy Tevet, Roy Kapon, and Amit HaimBermano.Human motion diffusion as a generative prior.In The Twelfth International Conference on Learning Rep-resentations, 2024. 2": "Adalberto L Simeone, Marco Speicher, Andreea Molnar,Adriana Wilde, and Florian Daiber. Live: The human role inlearning in immersive virtual environments. In Symposiumon Spatial User Interaction, New York, NY, USA, 2019. As-sociation for Computing Machinery. 1 Sanjana Sinha, Sandika Biswas, Ravindra Yadav, and Bro-jeshwar Bhowmick. Emotion-controllable generalized talk-ing face generation. In Proceedings of the Thirty-First Inter-national Joint Conference on Artificial Intelligence, IJCAI-22, pages 13201327. International Joint Conferences on Ar-tificial Intelligence Organization, 2022. Main Track. 3",
  "Lijuan Wang and Frank K Soong. Hmm trajectory-guidedsample selection for photo-realistic talking head. MultimediaTools and Applications, 74(22):98499869, 2015. 3": "Katie Watson, Samuel S. Sohn, Sasha Schriber, MarkusGross, Carlos Manuel Muniz, and Mubbasir Kapadia. Sto-ryprint: An interactive visualization of stories. In Proceed-ings of the 24th International Conference on Intelligent UserInterfaces, page 303311, New York, NY, USA, 2019. Asso-ciation for Computing Machinery. 1 Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, YandongWen, Timo Bolkart, Dacheng Tao, and Michael J. Black.Generating holistic 3d human motion from speech. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), pages 469480, 2023. 2,3, 8 Youngwoo Yoon, Woo-Ri Ko, Minsu Jang, Jaeyeon Lee, Jae-hong Kim, and Geehyuk Lee.Robots learn social skills:End-to-end learning of co-speech gesture generation for hu-manoid robots. In Proc. of The International Conference inRobotics and Automation (ICRA), 2019. 1, 2, 7, 8 Youngwoo Yoon, Bok Cha, Joo-Haeng Lee, Minsu Jang,Jaeyeon Lee, Jaehong Kim, and Geehyuk Lee. Speech ges-ture generation from the trimodal context of text, audio, andspeaker identity.ACM Transactions on Graphics, 39(6),2020. 2, 3, 7, 8, 9"
}