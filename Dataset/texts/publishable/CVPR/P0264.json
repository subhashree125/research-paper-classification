{
  "Abstract": "Adversarial examples, crafted by adding perturbationsimperceptible to humans, can deceive neural networks. Re-cent studies identify the adversarial transferability acrossvarious models, i.e., the cross-model attack ability of adver-sarial samples. To enhance such adversarial transferability,existing input transformation-based methods diversify in-put data with transformation augmentation. However, theireffectiveness is limited by the finite number of availabletransformations. In our study, we introduce a novel ap-proach named Learning to Transform (L2T). L2T increasesthe diversity of transformed images by selecting the opti-mal combination of operations from a pool of candidates,consequently improving adversarial transferability. We con-ceptualize the selection of optimal transformation combi-nations as a trajectory optimization problem and employ areinforcement learning strategy to effectively solve the prob-lem. Comprehensive experiments on the ImageNet dataset,as well as practical tests with Google Vision and GPT-4V, re-veal that L2T surpasses current methodologies in enhancingadversarial transferability, thereby confirming its effective-ness and practical significance. The code is available at",
  ". Introduction": "Neural networks have been adopted as the building blockfor various real-world applications, such as face detec-tion , autonomous driving , and medicaldiagnosis . However, neural networks are vulnerableto adversarial examples, which contain human imperceptibleadversarial perturbations on the benign input. This issue isincreasingly concerning researchers, as it is essential for en-suring the trustworthy use of neural networks .In real-world scenarios of adversarial attacks ,the target model is usually inaccessible. To attack these inac-cessible models, many studies instead rely on surrogate mod-",
  "Spectrum": ". For input transformation-based attacks, most worksdesign a fixed transformation and use it to craft the adversarialperturbation. The learning-based methods preliminarily predictaugmentation strategies for current images for better adversarialtransferability. These methods cannot respond to the distributionshifts between benign images and adversarial examples. We pro-pose Learning to Transform (L2T), which uses the dynamic ofthe optimal transformation in each iteration to further boost theadversarial transferability. els to generate adversarial examples and use gen-erated samples to mislead the target model. This cross-modelattack ability of samples generated on the surrogate modelsis called adversarial transferability. Numerous researchstudies are dedicated to enhancing adversarial transferability,which can be classified into four categories: gradient-basedmethods , input transformation-based meth-ods , architecture-based methods ,and ensemble-based methods . Among these attackmethodologies, input transformation-based methods gainmuch popularity because of their plug-n-play advantage,which can be seamlessly integrated into other attack tech-niques . However, we discover that existing input",
  "arXiv:2405.14077v2 [cs.CV] 24 Jul 2024": "transformation-based methods adopt the same transforma-tion when crafting adversarial examples, limiting the flex-ibility of transformation operations. We hypothesize thatwe should select the optimal transformation dynamically ineach iteration to enhance the adversarial transferability.As shown in , prior input transformation-basedmethods often revolve around designing fixed augmentationstrategies like resizing inputs , block masking , ormix-up . A more dynamic approach is presented by ,advocating the precomputation of various sequences of aug-mentation strategies to apply to each iteration to enhance theattack performance. Complementing this, Wu et al. pro-poses the use of generative models for image augmentationto boost the adversarial transferability. Some studies go fur-ther, combining multiple augmentation strategies to amplifyinput diversity to improve the performance. For example,Yuan et al. introduces a neural network that generates aprediction of the optimal transformation strategy and appliesthe strategy to improve performance. A further improvementis hindered by the limited number of transformations.To fully utilize the limited number of transformations, anatural idea is to use a combination of operations. However,it is not always efficient to combine different transformationstogether for attack, as reported in . We expect to find anoptimal combination of transformations to achieve a trade-off between operation diversity and adversarial transferabil-ity. Nonetheless, the enormity of the search space presentsa significant challenge, impeding the identification of themost efficacious combination of transformations during anattack for optimal adversarial transferability. To surmountthis hurdle, we conceptualize the search process of the opti-mal combination of transformations as a problem of optimaltrajectory search. Each node within this trajectory representsan individual transformation, and each directed edge meansa transfer of the optimal transformation from the current stepto the next step. To effectively obtain the optimal trajec-tory in such a large search space, we design a reinforcementlearning-based approach, capitalizing on its demonstratedefficacy in navigating expansive search domains.In this paper, we introduce a novel framework calledLearning to Transform (L2T) to improve the adversarialtransferability of generated adversarial examples. L2T dy-namically learns and applies the optimal input transforma-tion in each iteration. Instead of exhaustively enumeratingall possible input transformation methods, we employ a re-inforcement learning-based approach to reduce the searchspace and better utilize the transformations to improve the di-versity. In each iteration of the adversarial attack, we samplea subset of transformations and apply them to the adversarialexamples. Subsequently, we update the sampling probabili-ties by conducting gradient ascent to maximize the loss. Ourmethod effectively learns the dynamics of optimal transfor-mations in attacks, leading to a significant enhancement in adversarial transferability. Additionally, compared to otherlearn-based adversarial attack methods, our approach is moreefficient for adversarial example generation, as it obviatesthe need for additional training modules.We summarize our contributions as follows, We formulate the problem of optimal transformation inadversarial attacks, which studies finding the optimal com-bination of transformations to increase the input diversities,thus improving the adversarial transferability.",
  ". Adversarial Attack": "Various adversarial attacks have been proposed, e.g.,gradient-based attack , transfer-based attack , score-based attack , decision-basedattack , generation-based attack . Amongthese, transfer-based attacks do not require the informationof the victim models, making it popular to attack the deepmodels in the real world and raise more research interests.To improve adversarial transferability, various momentum-based attacks have been proposed, such as MI-FGSM ,NI-FGSM , VMI-FGSM , EMI-FGSM , etc.Several input transformation methods are also proposed,such as DIM , TIM , SIM , Admix , SIA ,STM , BSR , etc., which augment images used foradversarial perturbation computation to boost transferability.The input transformation-based methods can be integratedinto the gradient-based attacks for better performance.Delving into the input transformation-based methods,most works are limited to designing a fixed transforma-tion to augment the images, which limits the diversity oftransformed images and the adversarial transferability. Toaddress this issue, some researchers proposeto augment the images with a set of multiple transforma-tions predicted by a pre-trained network. Automatic ModelAugmentation (AutoMA) adopts a Proximal Policy Op-timization (PPO) algorithm in search of a strong augmen-tation policy. Adversarial Transformation-enhanced Trans-fer Attack (ATTA) proposes to employ an adversarialtransformation network in modeling the most harmful distor-tions. Adaptive Image Transformation Learner (AITL) incorporates different image transformations into a unifiedframework to learn adaptive transformations for each be-nign sample to boost adversarial transferability. By applyingoptimal multiple transformations, the adversarial attack per-",
  ". Adversarial Defense": "Various defense approaches have been proposed to mitigatethe threat of adversarial attacks, such as adversarial train-ing , input preprocessing , feature denois-ing , certified defense , etc. Liao et al. train a denoising autoencoder, namely the High-levelrepresentation guided denoiser (HGD), to purify the adver-sarial perturbations. Xie et al. propose to randomlyresize the image and add padding to mitigate the adversarialeffect, namely the Randomized resizing and padding (R&P).Xu et al. propose the Bit depth reduction (Bit-Red)method, which reduces the number of bits for each pixelto squeeze the perturbation. Liu et al. defend againstadversarial attacks by applying a JPEG-based compressionmethod to adversarial images. Cohen et al. adopt random-ized smoothing (RS) to train a certifiably robust classifier.Naseer et al. propose a Neural Representation Purifier(NRP) to eliminate perturbation.",
  ". Task definition": "The crafting of adversarial examples usually takes an itera-tive framework to update the adversarial perturbation. Givena benign sample x and the corresponding label y, a trans-ferable attack takes a surrogate classifier f and iterativelyupdates the adversarial example xadv to maximize the lossof classifying f(xadv) to y. Take I-FGSM as an exam-ple. The adversarial example xadvtat the t-th iteration canbe formulated as follows:",
  "xadvt= xadvt1+ sign(xadvt1J(f(xadvt1, y))),": "where we denote as the step size, J(, ) as the clas-sification loss function. As identified by previous studies,the adversarial example exhibits a characteristic of trans-ferability, where the adversarial examples generated by thesurrogate model can fool other neural networks.Input transformation-based methods are one of the mosteffective methods to boost adversarial transferability. Withthese methods, the adversarial samples are firstly trans-formed by a set of image transformations and then proceededto gradient calculation. Let denote a set of image trans-formations operation o, where = {oi|i {1, 2, ..., k}}.At the t-th iteration, the adversarial example xadvtis trans-formed sequentially by oi as follows,",
  "(a) One iteration(b) Two iteration": ". Comparsion for different operations in boosting theadversarial transferability. The number in the box denotes thenumber of fooled models (Maximum: 9). In (a), the horizontal axisdenotes different transformation operations and the vertical axisdenotes different benign examples. In (b), the vertical axis denotesthe transformation used in the first iteration and the horizontal axisdenotes the transformation used in the second iteration",
  "(xadvt) = ok ok1 o1(xadvt),(2)": "where o2 o1(x) denotes the operation o2(o1(x)), o1, o2 . We use the gradient of (xadvt) with respect to the lossfunction to update the adversarial perturbation as Eq. (1).There are two categories for selecting the operation set in the previous study. One line of research focuses ondesigning fixed transformation-based methods, which use apre-defined transformation . For example, Admix choosesmixup and scaling for transformation . The other line of re-search proposes the learning-based transformation methods,which usually use a generative model to directly generate thetransformed (x). Compared with the fixed transformation-based methods, learning-based methods enjoy more diver-sity of transformed images, leading to a better performancein adversarial transferability. In our work, we study thelearning-based transformation methods.",
  ". Motivation": "Previous research designs lots of transformations to improvethe diversity of images, thus guiding the adversarial attacksto focus more on the invariant robust features. However, itdoes not always work by increasing the number of trans-formed images for attacks to boost the adversarial transfer-ability. Because some combination of transformations cancause damage to original examples, losing massive amountsof information used for transferable attacks. A natural ques-tion occurs to us, for one image, does there exist the opti-mal combination of transformations for the best adversarialtransferability?To answer this question, we start by generating adversar-ial examples in one iteration. We take an example of craftingadversarial examples using ResNet-18 to attack other 9 mod-els1. We denote 5 operations for input transformation meth-",
  ". There exists an optimal transformation trajectory forboosting adversarial transferability. However, the search space in-creases exponentially with iteration number and operation number": "ods, namely the crop, rotation, shuffle, scaling, and mix-up.We use these operations on five images for attacks and reportthe number of models fooled. We report the results in .It can be seen that by shuffle, we can achieve the maximumtransferable attack success rates on a dog image, indicatingthe optimal transformations in all possible 5 operations.We continue our discussion in the two-iteration scenario.Following the same setting in one iteration, we report thenumber of fooled models. It can be seen that by choosingcrop in the first iteration and scaling for the second iteration,which successfully fooled 6 models out of 9. We also noticethat shuffle, the optimal transformation in one iteration, cannot maintain the optimal performance. The average fooledmodel for shuffle is less than crop in 0.2.Following the aforementioned discussion, we move onto generating adversarial examples in 3 iterations, wherewe only take one operation as the image transformation toattack the image. As exemplified in , there are 5 5 5 possible trajectories to transform the image for attacks.Among these trajectories, it can achieve the best performanceby first shuffling, then rotating, and last shuffling the image.It should be noted it cannot consistently achieve the bestperformance by increasing the number of transformationsfor a higher diversity. As shown in , we respectivelytake the scaling, shuffle, and rotation operations at eachiteration in trajectory 2. However, it has the worst attacksuccess rate among the presented results.Generalizing the previous problem to common cases, weare motivated to identify an optimal transformation trajectoryT , which is defined as the sequence of transformation usedin each iteration as (1, 2, . . . , T ), for the best adversarialtransferability. Each element T denotes the transformation",
  "T = (1, 2, . . . , T )(4)": "where we denote xadvTas the adversarial example generatedby the surrogate model under transformation trajectory T .However, finding T is hard. First, the search space islarge. For example, supposing five candidate transforma-tions, even if we only take one operation in one iteration totransform the image, we will still have an enormous searchspace for ten iterations that will be 510. The number of pos-sible transformation trajectories grows exponentially withincreasing the number of iterations and candidate transfor-mations. Second, we can not access the black-box modelf, making it hard to optimize the Eq. (3) directly. Besides,as identified in the previous work , each image has adifferent optimal transformation to boost the adversarialtransferability. There is no optimal transformation trajectoryshared for all images.",
  ". Methodology": "The problem of Eq. (3) can be transformed into an optimaltrajectory search problem, on which reinforcement learninghas shown great compatibility. We are inspired to take areinforcement learning-based approach in solving this opti-mization problem to enhance adversarial transferability.Supposing we have M operations {o1, o2, . . . , oM} intotal, the optimal transformation trajectory T is a temporalsequence of the combination of different operations. Theprobability p contains M possibilities {po1, po2, ..., poM }for each iteration. Each element pom denotes the possibil-ity of sampling operation om, m {1, 2, ..., M}. And pom",
  "k=1pok": "For each iteration t, we sample a combination of trans-formation t. Each transformation in t is sampled fromcandidates depending on p. To get an optimal trajectoryT = (1, ..., T ), we need to dynamically optimize thesampling distribution p in each iteration t. We formulate theproblem of searching optima p in each iteration as follows,",
  "where the is the learning rate and gp is the gradient for p": "Algorithm 1 Gradient policy for optimal augmentationsearch.Input: Classifier f();The benign sample x with ground-truth label y; Loss function L(, ); candidate operationpool , the number of iterations T, perturbation scale ,policy learning rate , number of operations K, numberof transformations L, decay factor ; = /T, g0 = 0, xadv0= x, p N(0, 1)",
  "Output: xadvT": "Implementation details. We present the overview of ourmethod in . First, we sample L sequences of trans-formation lt, l [1, 2, ..., L], depending on the samplingdistribution p.Next, we get the transformed examplesdenoted as lt(xadvt). The probability of each sequencelt is P (lt). We use t to denotes all L transformation,t = {1t, 2t, ..., Lt }. Then, we use Eq. (1) to update theadversarial examples for each iteration. The gradient is cal-culated by loss between L transformed examples and theircorresponding labels. Last, after updating the adversarialexample, we recompute the approximate p. Specifically, we",
  ". Setup": "Models. We evaluate the proposed method in three cate-gories of target models. (1) Normally trained model: Weselect ten well-known models for experiments. ResNet-18 , ResNet-101 , ResNext-50 , DenseNet-121 , Inception-v3 , and Inception-v4 , ViT-B , PiT , Visformer , and Swin . All of thesemodels are pre-trained on the ImageNet dataset. (2) Adver-sarial trained models: we select four defense methods in ourexperiments. They are adversarial training (AT) , high-level representation guided denoiser (HGD) , neuralrepresentation purifier (NRP) , and randomized smooth-ing (RS) . (3) Vision API: to imitate a practical scenario,we compare the attack performance on popular vision API.We chose Google Vision, Azure AI, GPT-4V, and Bard. Forcategories (2) and (3), we use ensemble-based attack. Wechoose two CNN-based models, ResNet18 and Inception-v4, SIM TIM",
  "Dataset. Following previous works , we ran-domly choose 1, 000 images from ILSVRC 2012 validationset . All images are classified correctly by the models": "Baseline. We compare L2T with other input transforma-tion adversarial methods. There are two categories of pre-vious methods. The fixed transformation attack followed afixed transformation scheme. We select TIM , SIM ,Admix , DEM , IDE , Mask , S2IM ,BSR , and SIA for comparison. The learned trans-formation attack followed a set of transformations predictedby a trained network to generate adversarial examples. Wealso compare our method with learned transformation at-tacks, such as AutoMA , ATTA , and AITL . Allthese methods are integrated with MI-FGSM to generateadversarial examples. Evaluation Settings. We follow the hyper-parameter settingof MI-FGSM and set the perturbation budget = 16, num-ber of iteration T = 10, step size = /T = 1.6 and decayfactor = 1. For our method, we adopt the number of oper-ations as 2, the number of samples as 10, and the learningrate as 0.01. For the candidate operation, we chose tencategories of transformations. Each category contains tenspecific operations with different parameters. We will dis-cuss the detailed settings of our method and other baselinesin the supplementary materials. Number of operations (K) ASR (%) ResNet-101ResNeXt-50DenseNet-121 Inception-v3Inception-v4ViT PiTVisformerSwin",
  ". Evaluations on single models": "Our proposed L2T exhibits better adversarial transferabilityto various input transformation based attacks. We take a sin-gle model as the surrogate model and evaluate the averageattack success rate (ASR), i.e., the average misclassifica-tion rates across ten models. We summarized our results in. Each subfigure denotes the attacker generates theadversarial examples on the corresponding models and itsx-axis denotes the attack algorithm used.First, we observe that L2T consistently outperforms allother attackers, regardless of the surrogate model. Otherbaseline methods have various adversarial transferability SIM TIM",
  ". We integrate the ensemble-based attack with input transformation and evaluate the performance on defense methods and popularvision APIs. We include the detail number in our supplementary material": "according to the surrogate models. For example, the BSRperforms to be the strongest baseline on ResNet-18. How-ever, the BSR cannot remain efficient when the surrogatemodel is changed to Swin or PiT. In contrast, our proposedL2T is suitable for all the surrogate models being tested.These results also strengthen our argument that we shoulddynamically choose the transformation to fit the surrogatemodels. Specifically, in the worst case (subfig. c), our pro-posed L2T still outperforms the strongest baseline (S2IM)by 2.1%. Overall, L2T outperforms the other baseline by22.9% on average ASR.",
  ". Evaluations on defense methods": "L2T is also capable of adversarial robust mechanisms. Wetest the attack performance of L2T against several defensemechanisms, including AT, HGD, NRP, and RS. We choosethe ensemble setting to attack these defense approaches. Weuse the ensemble of four models, ResNet-18, Inception-v4,Visformer, and Swin, as the surrogate model. We summa-rized our results in (a), (b), (c), and (d). Eachsubfigure denotes the model to be attacked and its x-axisdenotes the attack algorithm used.From , it is clear that L2T remains efficient. L2Tconsistently outperforms other methods against various de-fense methods. Notably, it achieves the attack success rateof 47.9%, 98.5%, 87.2%, and 46.7% on AT, HGD, NRP,and RS, respectively. Even on the certified defense RS,the strongest defense among the four, L2T achieves the at-tack success rate of 46.7%, which exceeds the best baseline(AITL) by 4.6%. This is also the biggest improvement L2T",
  ". Evaluations on vision API": "Our proposed L2T can also perform well in realistic sce-narios. To imitate the real-world application, we test theperformance of L2T on Vision API. We use the same settingin sec. 4.3 to craft adversarial examples. We choose GoogleVision ( (e)) and Azure AI ( (f)) to evaluateattacks on vision-only API. We also choose ChatGPT-4V( (g)) and Gemini ( (f)) to evaluate attackson the foundation model API.As shown in , L2T is generally the best attacker tothe real-world API. All attacks perform better on foundationmodel API than vision-only API. For vision-only API, L2Toutperforms the strongest baseline by 8.7% and 12.6%, re-spectively. For foundation model API, L2T achieves nearly100% attack success rate on both GPT-4V and Gemini.",
  ". Ablation study": "On the numbers of operation K. As shown in , westudy the impact of K on adversarial transferability. Wecraft the adversarial example on ResNet-18 and evaluatethem on the other nine models. There is a clear differencebetween one operation and two operations. The averageattack success rate increases by 8.09%, from 80.89% to88.98%. However, when the K 3, the improvementbecomes marginal. The average attack success rate onlyincreases by 2.29% when K is increased from 2 to 5. Thus,",
  ". Average attack success rates (ASR) (%) of adversarialexamples generated by L2T with various number of steps T. Weinclude the detail number in our supplementary material": "K should be moderately settled as 2.On the number of transformations L. We conducted ex-periments on the number of transformations L. We craftthe adversarial example on ResNet-18 and evaluate them onthe other nine models. We choose L from 1 to 50. From, we observe that the adversarial transferability im-proves steadily with the number of transformations. Theincrease is significant when the number of transformationsgrows from 1 to 20, which improves from an average attacksuccess rate of 75.7% to an average attack success rate of L2T -Rotate -Scale -Resize -Shuffle -Mixup -Dropout -Mask -Spectrum -Crop -Translate Average ASR (%) 89.7 78.8 76.2 83.2 75.5 86.6 82.4 76.875.6 81.1 86.6",
  ". The average attack success rates (%) of adversarialexamples crafted by L2T and L2T without a single transformation. indicates removing such transformation": "91.1%. However, transferability does not increase signifi-cantly after the number exceeds 20, where the average attacksuccess rate only increases 1.5%. To keep the balance be-tween computation efficiency and adversarial transferability,we suggest the number of samples set to 20.On the number of iterations T. We discuss the number ofiterations among different attack approaches. We craft theadversarial example on ResNet-18 and compare the averageattack success rate of 10 models. As shown in , for allthe attack methods, the attack success rate increased steadilyfor the first 10 iterations. L2T achieves the fastest speed ofincrease, which reaches 89.47% at iteration 10. After 10 iter-ations, most of the methods struggled to make improvements.For example, the Admix goes around 71%. The performanceof S2IM even decreases from 73% to 70%. Meanwhile, L2Tstill maintains a stable increase, from 89.47% to 94.77%.Comparison with random sampling. We compare thelearnable strategy with random sampling. As shown inTab. 1, there is a clear gap of the attack success rate be-tween random sampling and gradient-guided sampling. Theminimum difference is 31.12% with setting Visformer as thesurrogate model. For other surrogate models, the gap is evenlarger. This experiment indicates random sampling cannoteffectively sample the best transformation trajectory, and thetransformation in each iteration needs to be chosen carefully.Operation candidates analysis. We conducted an ablationstudy for the operation candidates. We subtract each op-eration in the candidates and conduct L2T on the updatedoperation candidates. From , we observe that subtract-ing any operations will lead to a performance decrease. Forexample, by subtracting the scale operation, the performancedecreases for 23.5%. Meanwhile, subtracting mixup and",
  ". Conclusion": "In this paper, we study the dynamic property for input trans-formation. Utilizing this property, we propose L2T to opti-mize the input transformation in each iteration. By updatinga sampling probability, our method provides an approximatesolution to input transformation optimization. Our experi-ments further study the effectiveness of our methods. Ourmethod performs consistently well among different targetedmodels. This paper provides a new perspective to understandthe transferability of adversarial examples.Acknowledgement.This work was supported by NSFunder grant 2202124 and the Center of Excellence inData Science, an Empire State Development-designatedCenter of Excellence.The content of the informationdoes not necessarily reflect the position of the Gov-ernment, and no official endorsement should be inferred.",
  "Mihalj Bakator and Dragica Radosav. Deep learning andmedical diagnosis: A review of literature. Multimodal Tech-nologies and Interaction, 2(3):47, 2018. 1": "Wieland Brendel, Jonas Rauber, and Matthias Bethge.Decision-Based Adversarial Attacks:Reliable AttacksAgainst Black-Box Machine Learning Models. In Proceed-ings of the International Conference on Learning Representa-tions, 2018. 2 Raja Chatila, Virginia Dignum, Michael Fisher, Fosca Gi-annotti, Katharina Morik, Stuart Russell, and Karen Yeung.Trustworthy ai. Reflections on Artificial Intelligence for Hu-manity, pages 1339, 2021. 1 Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, andCho-Jui Hsieh. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substi-tute models. In Proceedings of the 10th ACM workshop onartificial intelligence and security, pages 1526, 2017. 2 Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu,Longhui Wei, and Qi Tian. Visformer: The vision-friendlytransformer. In Proceedings of the IEEE/CVF internationalconference on computer vision, pages 589598, 2021. 5 Jeremy Cohen, Elan Rosenfeld, and J. Zico Kolter.Cer-tified Adversarial Robustness via Randomized Smoothing.In Proceedings of the International Conference on MachineLearning, pages 13101320, 2019. 3, 5 Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, JunZhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial attackswith momentum. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 91859193,2018. 1, 2, 6 Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu.Evading Defenses to Transferable Adversarial Examplesby Translation-Invariant Attacks.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 43124321, 2019. 1, 2, 6 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale.arXiv preprintarXiv:2010.11929, 2020. 5",
  "Mingyuan Fan, Cen Chen, Ximeng Liu, and Wenzhong Guo.Maskblock: Transferable adversarial examples with bayesapproach. arXiv preprint arXiv:2208.06538, 2022. 2, 6": "Zhijin Ge, Fanhua Shang, Hongying Liu, Yuanyuan Liu,Liang Wan, Wei Feng, and Xiaosen Wang. Improving theTransferability of Adversarial Examples with Arbitrary StyleTransfer. arXiv preprint arXiv:2308.10601, 2023. 2 Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are weReady for Autonomous Driving? The KITTI Vision Bench-mark Suite. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 33543361,2012. 1",
  "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.Explaining and harnessing adversarial examples.arXivpreprint arXiv:1412.6572, 2014. 2": "Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth,Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arand-jelovic, Timothy Arthur Mann, and Pushmeet Kohli. ScalableVerified Training for Provably Robust Image Classification.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 48414850, 2019. 3 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep Residual Learning for Image Recognition. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 770778, 2016. 5 Byeongho Heo, Sangdoo Yun, Dongyoon Han, SanghyukChun, Junsuk Choe, and Seong Joon Oh. Rethinking spatialdimensions of vision transformers. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 1193611945, 2021. 5 Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kil-ian Q. Weinberger. Densely Connected Convolutional Net-works. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 22612269, 2017.5 Andrew Ilyas, Logan Engstrom, Anish Athalye, and JessyLin. Black-box Adversarial Attacks with Limited Queries andInformation. In Proceedings of the International Conferenceon Machine Learning, pages 21422151, 2018. 2 Jinyang Jiang, Zeliang Zhang, Chenliang Xu, Zhaofei Yu, andYijie Peng. One forward is enough for neural network trainingvia likelihood ratio method. In The Twelfth InternationalConference on Learning Representations, 2023. 1",
  "Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adver-sarial examples in the physical world. In Artificial intelligencesafety and security, pages 99112. Chapman and Hall/CRC,2018. 2": "Huichen Li, Xiaojun Xu, Xiaolu Zhang, Shuang Yang, andBo Li. QEBA: Query-Efficient Boundary-Based BlackboxAttack. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 12181227,2020. 2 Yandong Li, Lijun Li, Liqiang Wang, Tong Zhang, and Bo-qing Gong. NATTACK: Learning the Distributions of Ad-versarial Examples for an Improved Black-Box Attack onDeep Neural Networks. In Proceedings of the InternationalConference on Machine Learning, pages 38663876, 2019. 2 Yingwei Li, Song Bai, Yuyin Zhou, Cihang Xie, ZhishuaiZhang, and Alan L. Yuille. Learning Transferable AdversarialExamples via Ghost Networks. In Proceedings of the AAAIConference on Artificial Intelligence, pages 1145811465,2020. 1 Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xi-aolin Hu, and Jun Zhu. Defense Against Adversarial AttacksUsing High-Level Representation Guided Denoiser. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 17781787, 2018. 3, 5 Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel,Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, andDaan Wierstra. Continuous Control with Deep ReinforcementLearning. In Proceedings of the International Conference onLearning Representations, 2016. 1",
  "Aoming Liu, Zehao Huang, Zhiwu Huang, and Naiyan Wang.Direct differentiable augmentation search. In Proceedings ofthe IEEE/CVF international conference on computer vision,pages 1221912228, 2021. 5": "Pinxin Liu, Luchuan Song, Daoan Zhang, Hang Hua, YunlongTang, Huaijin Tu, Jiebo Luo, and Chenliang Xu. Emo-avatar:Efficient monocular video style avatar through texture render-ing. arXiv preprint arXiv:2402.00827, 2024. 1 Xiao-Yang Liu, Rongyi Zhu, Daochen Zha, Jiechao Gao,Shan Zhong, and Meikang Qiu. Differentially private low-rank adaptation of large language model using federated learn-ing. arXiv preprint arXiv:2312.17493, 2023. 1 Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delv-ing into Transferable Adversarial Examples and Black-boxAttacks. In Proceedings of the International Conference onLearning Representations, 2017. 1 Zihao Liu, Qi Liu, Tao Liu, Nuo Xu, Xue Lin, Yanzhi Wang,and Wujie Wen. Feature Distillation: DNN-Oriented JPEGCompression Against Adversarial Examples. In Proceedingsof the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 860868, 2019. 3 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF international conference oncomputer vision, pages 1001210022, 2021. 5 Yuyang Long, Qilong Zhang, Boheng Zeng, Lianli Gao, Xian-glong Liu, Jian Zhang, and Jingkuan Song. Frequency domainmodel augmentation for adversarial attack. In European Con-ference on Computer Vision, pages 549566. Springer, 2022.2, 6",
  "Jonathan G Richens, Ciaran M Lee, and Saurabh Johri. Im-proving the accuracy of medical diagnosis with causal ma-chine learning. Nature communications, 11(1):3923, 2020.1": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,Aditya Khosla, Michael Bernstein, Alexander C. Berg, andLi Fei-Fei. ImageNet Large Scale Visual Recognition Chal-lenge. International Journal of Computer Vision (IJCV), 115(3):211252, 2015. 6, 13 Florian Schroff, Dmitry Kalenichenko, and James Philbin.FaceNet: A Unified Embedding for Face Recognition andClustering. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 815823,2015. 1 Dawn Song, Kevin Eykholt, Ivan Evtimov, Earlence Fernan-des, Bo Li, Amir Rahmati, Florian Tramer, Atul Prakash, andTadayoshi Kohno. Physical adversarial examples for objectdetectors. In 12th USENIX workshop on offensive technolo-gies (WOOT 18), 2018. 3 Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, andAlexander A. Alemi. Inception-v4, Inception-ResNet and theImpact of Residual Connections on Learning. In Proceedingsof the AAAI Conference on Artificial Intelligence, pages 42784284, 2017. 5",
  "Xiaoou Tang and Zhifeng Li. Video based face recognitionusing multiple classifiers. In IEEE International Conferenceon Automatic Face and Gesture Recognition. IEEE, 2004. 1": "Florian Tram`er, Alexey Kurakin, Nicolas Papernot, Ian J.Goodfellow, Dan Boneh, and Patrick D. McDaniel. EnsembleAdversarial Training: Attacks and Defenses. In Proceedingsof the International Conference on Learning Representations,2018. 3, 5 Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong,Jingchao Zhou, Zhifeng Li, and Wei Liu. CosFace: LargeMargin Cosine Loss for Deep Face Recognition. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 52655274, 2018. 1",
  "Xiaosen Wang, Kun He, Chuanbiao Song, Liwei Wang, andJohn E Hopcroft. AT-GAN: An adversarial generator modelfor non-constrained adversarial examples. arXiv preprintarXiv:1904.07793, 2019. 2": "Xiaosen Wang, Xuanran He, Jingdong Wang, and Kun He.Admix: Enhancing the Transferability of Adversarial Attacks.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 1613816147, 2021. 1, 2, 6 Xiaosen Wang, Jiadong Lin, Han Hu, Jingdong Wang, andKun He. Boosting Adversarial Transferability through En-hanced Momentum. In Proceedings of the British MachineVision Conference, page 272, 2021. 1, 2",
  "Xiaosen Wang, Chuanbiao Song, Liwei Wang, and Kun He.Multi-stage Optimization Based Adversarial Training. arXivpreprint arXiv:2106.15357, 2021. 3": "Xiaosen Wang, Zeliang Zhang, Kangheng Tong, DihongGong, Kun He, Zhifeng Li, and Wei Liu. Triangle Attack:A Query-Efficient Decision-Based Adversarial Attack. InProceedings of the European Conference on Computer Vision,pages 156174, 2022. 2 Xiaosen Wang, Zeliang Zhang, and Jianping Zhang. StructureInvariant Transformation for better Adversarial Transferabil-ity. In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, 2023. 2, 6 Xingxing Wei, Siyuan Liang, Ning Chen, and Xiaochun Cao.Transferable Adversarial Attacks for Image and Video ObjectDetection. In Proceedings of the International Joint Confer-ence on Artificial Intelligence, pages 954960, 2019. 2 Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, andXingjun Ma. Skip Connections Matter: On the Transferabil-ity of Adversarial Examples Generated with ResNets. InProceedings of the International Conference on LearningRepresentations, 2020. 1",
  "Han Wu and Wenjie Ruan. Adversarial Driving: AttackingEnd-to-End Autonomous Driving Systems. arXiv preprintarXiv:2103.09151, 2021. 1": "Weibin Wu, Yuxin Su, Michael R Lyu, and Irwin King. Im-proving the transferability of adversarial samples with ad-versarial transformations. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition, pages90249033, 2021. 2, 6 Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, MingyanLiu, and Dawn Song. Generating Adversarial Examples withAdversarial Networks. In Proceedings of the InternationalJoint Conference on Artificial Intelligence, pages 39053911,2018. 2 Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, andAlan L. Yuille. Mitigating Adversarial Effects Through Ran-domization. In Proceedings of the International Conferenceon Learning Representations, 2018. 3 Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L.Yuille, and Kaiming He. Feature Denoising for ImprovingAdversarial Robustness. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 501509, 2019. 3 Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, JianyuWang, Zhou Ren, and Alan L Yuille. Improving transferabilityof adversarial examples with input diversity. In Proceedingsof the IEEE/CVF conference on computer vision and patternrecognition, pages 27302739, 2019. 1, 2, 6 Pengfei Xie, Linyuan Wang, Ruoxi Qin, Kai Qiao, ShuhaoShi, Guoen Hu, and Bin Yan. Improving the transferability ofadversarial examples with new iteration framework and inputdropout. arXiv preprint arXiv:2106.01617, 2021. 6 Saining Xie, Ross B. Girshick, Piotr Dollar, Zhuowen Tu,and Kaiming He. Aggregated Residual Transformations forDeep Neural Networks. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 59875995, 2017. 5 Yifeng Xiong, Jiadong Lin, Min Zhang, John E Hopcroft, andKun He. Stochastic variance reduced ensemble adversarialattack for boosting the adversarial transferability. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1498314992, 2022. 1",
  "i,j Wi,j": "SIM: The scale-invariant method (SIM) scales every pixel by a set of levels and uses these scaled images for gradientcalculation. In our experiments, we choose the number of scale samples m = 5 and the scale factor i = 1/2i. Admix: Admix randomly mixes the benign examples with images from other categories and scales the mixed examples indifferent scales. We set the scale copies m1 = 5 and scale factor i = 1/2i and random sample images m2 = 3 and mixupstrength as 0.2.",
  "DEM: DEM provided an ensemble version of diversity invariant methods, which uses five transformed copies for gradientcalculation. In our experiments, we set the diversity list to": "Masked: Maskblock separates the images into several blocks and sequentially masks every block in the benign examples.Thus, the number of transformed copies is equal to the number of blocks. We set the number of blocks to 16 in ourexperiments. IDE: IDE conducts input dropout on a being example at different rates and gets multiple transformed examples to form anensemble attack. In our experiments, we choose the dropout rate to be 0.0, 0.1, 0.2, 0.3, 0.4, and the weight factor as equal. S2IM: S2IM provides a frequency domain perspective of input transformation, which utilizes DCT and IDCT techniques intransformation. In our experiments, we set the tuning factor = 0.5 and the standard deviation the same with perturbationscale and the number of spectrum transformations N = 20. BSR: BSR splits the input image into several blocks and then randomly shuffles and rotates these blocks. In our experiments,we split the image into 2x2 blocks with the maximum rotation angle 24% and calculate the gradients on N = 20 transformedimages. SIA: SIA decomposed the images into several blocks and transformed each block with an input transformation choosingfrom seven transformation candidates 2. We followed the suggested settings in the paper and chose splitting number s = 3,number of transformed images for gradient calculation N = 20. AutoMA: AutoMA targeted finding a strong model augmentation policy to boost adversarial transferability. Following thesetting in the paper, we trained the augmentation policy search network on 1000 images from ImageNet validation set,which does not overlap with the benign example set. We adopt the transformation number m = 5 and set the ten operationtypes and their corresponding magnitude the same as the original paper. ATTA: ATTA uses a two-layer network to mimic the transformation function. The benign examples are first passed throughthis transformation network and then sent for calculating the adversarial perturbations. We use the data from ImageNet training partition to train the transformation network. We trained different transformation networks according to thesurrogate models. For the training hyperparameters, we follow the settings from the authors. AITL: AITL introduces selecting input transformations by different benign examples. AITL trains three networks to predictthe input transformations for every image. We adopt the 20 image transformations in the same paper and use the pre-trainmodel weights from the authors to initialize the above networks. We set the number of iterations during optimizing theimage transformation feature to 1, the corresponding step size to 15, and the number of image transformation operations to4.",
  "A.2. Learning to Transform": "We decomposed the existing methods and concluded their input transformation methods. We formulate the transformationcandidates in 10 categories. (1) Rotate: Rotate refers to turning the image around a fixed point, usually its center, by a certain angle. The domain ofangle is . We choose 10 angles from the domain, and the interval between the two angles is identical. Thus, we form10 operations for the rotate category. The smallest rotation angle is 36, and the biggest rotation angle is 360.",
  "factor = 1/2i, i [1, 2, ..., 10]": "(3) Resize: Resize refers to removing the margin part of examples and resizing the main body of the benign examples. Wechose 10 resize rates for our experiments, which are 0,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, and 0.9 respectively. (4) Pad: the pad category comes from DIM. We choose to pad the bengin examples to different sizes where the size ofthe padded example will be [size size]. We chose 10 different sizes, which are 246.5, 257.6, 268.8, 280.0, 291.2, 302.4,313.6, 324.8, 336.0, and 347.2. (5) Mask: The mask category comes from Masked, which separates the examples into several blocks and randomly blocksone of the blocks. We control the number of blocks and choose 4,9,16,25,36,49,64,81,100,121 in specific. (6) Translate: the translated category comes from TIM. We shift the benign examples into 10 levels, which are 10pixel,20pixel, 30pixel, 40pixel,50pixel, 60pixel, 70pixel, 80pixel, 90pixel, 100pixel, along the x-axis and y-axis. (8) Shuffle: The shuffle category comes from BSR, which separates the examples into several blocks and randomly reordersthese blocks. We control the number of blocks and choose 4,9,16,25,36,49,64,81,100,121 in specific. (9) Spectrum: the spectrum category comes from S2IM, which adds noise in the spectrum domain of benign examplesdetermined by strength . We set ten different as 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0.",
  "B. Numerical Results": "Comparison with advanced methods: We include detailed results of the comparison with different baselines in Tab. 2, Tab. 3,Tab. 4, Tab. 5, Tab. 7, Tab. 6, Tab. 8, Tab. 9, Tab. 10, Tab. 11. For each table, we choose one model from ten models as thesurrogate model and use the adversarial examples to attack all these ten models.We show the attack success rate on adversarial examples crafted on ten different models corresponding to . Tab. 2 isthe detailed results for (a). Tab. 3 is the detailed results for (b). Tab. 6 is the detailed results for (c). Tab. 5 isthe detailed results for (d). Tab. 7 is the detailed results for (e). Tab. 4 is the detailed results for (f). Tab. 8 isthe detailed results for (g). Tab. 9 is the detailed results for (h). Tab. 11 is the detailed results for (i). Tab. 10is the detailed results for (j). The effectiveness of each attack varies significantly across different models. The L2T attackshows remarkably high effectiveness across all models, which outperforms all the other methods on all ten models.Evaluation on the defense methods and cloud APIs: We include the detailed results across different defense methods andvision API in Tab. 12 corresponding to . The L2T attack, highlighted in gray, shows exceptionally high success ratesacross almost all defense methods and APIs, particularly against Bard and GPT-4V.Ablation study on the number of iterations: We include the detailed results on the different iterations in Tab. 13 correspondingto . For most attacks, success rates increase as the number of iterations increases. This indicates that more iterationsgenerally lead to more effective adversarial examples. After a certain number of iterations (around 20-30 for many attacks),the increase in success rate slows down or plateaus. For example, the L2T attacks success rate increases significantly up toabout 30 iterations and then grows more slowly.Ablation study on the number of samples: We include the detailed results on the different iterations in Tab. 15 correspondingto . This suggests that using more samples to generate adversarial examples can lead to more effective attacks.Ablation study on the number of operations: We include the detailed results on the different iterations in Tab. 14corresponding to . As the number of operations increases, there is a general trend of increasing success rates across mostmodels. However, the increase is not significant after the number of operations exceeds 2.",
  "C. Examples on attacking the Multi-modal Large Language Models": "To show the scalability of L2T, we also conducted experiments on multi-modal large language models (MLLMs). As shown in, both GPT-4V and Bard can classify the benign example correctly into the bee-eater. We use L2T to generatethe adversarial examples against ResNet-18. As shown in , the Bard classified the adversarial example as acrocodile, and GPT-4V classified it as a dragonfly. It shows the vulnerability of MLLMs, posing great challenges in developingrobust MLLMs."
}