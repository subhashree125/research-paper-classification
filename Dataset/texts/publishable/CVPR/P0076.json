{
  "Abstract": "Category-level 3D pose estimation is a fundamentallyimportant problem in computer vision and robotics, e.g. forembodied agents or to train 3D generative models. How-ever, so far methods that estimate the category-level ob-ject pose require either large amounts of human annota-tions, CAD models or input from RGB-D sensors. In con-trast, we tackle the problem of learning to estimate thecategory-level 3D pose only from casually taken object-centric videos without human supervision. We propose atwo-step pipeline: First, we introduce a multi-view align-ment procedure that determines canonical camera posesacross videos with a novel and robust cyclic distance for-mulation for geometric and appearance matching using re-constructed coarse meshes and DINOv2 features. In a sec-ond step, the canonical poses and reconstructed meshesenable us to train a model for 3D pose estimation from asingle image. In particular, our model learns to estimatedense correspondences between images and a prototypical3D template by predicting, for each pixel in a 2D image, afeature vector of the corresponding vertex in the templatemesh.We demonstrate that our method outperforms allbaselines at the unsupervised alignment of object-centricvideos by a large margin and provides faithful and robustpredictions in-the-wild. Our code and data is available at",
  ". Introduction": "Category-level object pose estimation is a fundamentallyimportant task in computer vision and robotics with a mul-titude of real-world applications, e.g. for training 3D gen-erative models on real data and for robots that need to graspand manipulate objects. However, defining and determiningthe pose of an object is a task that is far from easy.Current approaches achieve high performance, but theyrequire large amounts of annotated training data to gen-eralize successfully , or additional in-puts during inference, such as CAD Models , 3DShapes or RGB-D . However, all of these are either time-consuming to obtain or not available in prac-tice at all. This motivates the development of methods forlearning category-level 3D pose estimators in a fully un-supervised fashion.While doing so from images in thewild seems infeasible, object-centric video data offersa more accessible alternative. Such videos can be easilycaptured using consumer-grade cameras and makes it pos-sible to leverage coarse 3D reconstructions during train-ing, providing a practical and cost-effective method forcollecting data.Therefore, we propose the new task oflearning a single-image category-level 3D pose estimatorfrom casually captured object-centric videos without anyhuman labels or other supervision. In practice, we lever-age CO3D as training data and show that our proposedmodel is able to generalize and predict accurate poses inthe wild for Pascal3D+ and ObjectNet3D . Weaddress the challenging task of learning category-level 3Dpose in an unsupervised fashion from casually capturedobject-centric videos. In particular, we propose a two-steppipeline (). The first step extracts DINOv2 fea-tures from the images and reconstructs a coarse 3D meshfrom the video with off-the-shelf methods . Buildingon this input, we introduce a novel 3D alignment procedure,where a key contribution is a novel 3D cyclical distance interms of geometry and appearance that enables the robustalignment of shape reconstructions even under severe noiseand variations in the object topology. As a result, we canalign all objects from the object-centric training videos intoa canonical coordinate frame without supervision. In a sec-ond step, we leverage the canonical poses and 3D meshesobtained from the first step to train a category-level neuralmesh in an unsupervised manner. In par-ticular, we represent objects using a prototypical 3D meshwith surface features to capture the geometry and neural ap-pearance of an object category and train a neural networkbackbone to predict, for each pixel in a 2D image, a featurevector of the corresponding vertex in the template mesh.Finally, the object 3D pose is solved using a pose fitting al-gorithm based on the estimated correspondence pairs. Wedemonstrate that our method outperforms all baselines by alarge margin at the unsupervised alignment of object-centric",
  "arXiv:2407.04384v1 [cs.CV] 5 Jul 2024": ". Illustration of our approach for the unsupervised learning of category-level 3D pose. Our method starts from unaligned object-centric videos of an object category (left) and aligns these into a canonical coordinate frame in a self-supervised manner using a prototypical3D mesh and self-supervised transformer features (center). Using the aligned videos, we train a neural network backbone to predict 2D-3Dcorrespondences from a single image to enable 3D object pose estimation in the wild (right).",
  ". Related Work": "Supervised Category-Level 3D Pose Estimation. Tradi-tional methods to determine object poses were to label key-points in images and train supervised methods to predictthem , or to use only pose labels and directly predictthem by casting the pose estimation problem as bin classi-fication by discretizing the pose space . More recentmethods utilize 3D meshes of objects or object categories.NeMo uses 3D meshes with neural features that arerendered-and-compared to feature maps from a CNN to ob-tain pose estimates. predicts a single embedding vectorper image and shows that superior performance can be ob-tained by simply retrieving the closest training sample. Incontrast to the above methods, our method is unsupervised.Few-Shot and Zero-Shot Pose Prediction. pro-poses to train a supervised pose estimator across many cat-egories and shows that their approach can generalize wellto similar but unseen objects. Many works implement zeroshot pose estimation by conditioning the model on the 3Dshape of the unseen object or by leveragingrenderings of CAD models . Other methods use few-shot learning or zero-shot learning with DINOv2 .In contrast to the above, our method does not require anyannotated dataset, 3D shapes or CAD models.Pose Alignment.The work from Goodwin et al. (ZSP) is most close to the first step of our method, as italigns the poses of two object-centric videos in a fully un-supervised fashion. Similar to our work, they use DINO to obtain semantic correspondences. They perform first a coarse alignment by matching one image from the sourcevideo to one of many images from the reference video.Then they leverage cyclical distances to select few promis-ing correspondences in the two images, and finally lever-age respective depth maps to align both images using leastsquares. Goodwin et al. extend their work in (UCD+)to match many images from the source video to many of thereference video. By finding a consensus over these many tomany alignments with a single transformation they demon-strate improved performance. The first step of our work issimilar to these works by that it also leverages DINO fea-tures and cyclical distances. However, our work adds a ge-ometric distance to perform the alignment directly in 3Dand introduces weighted correspondences to enable the nec-essary robust regression of the SE3 transformations fromnoisy and inaccurate geometries, which leads to significantimprovements in the alignment accuracy. Note also thatthese previous approaches used RGB-D inputs, while ourmethod works on images directly. Surface Embeddings and Neural Mesh Models. Re-cent work uses known poses and approximate object ge-ometries to learn features in 3D space to uniquely identifyparts of objects. first used known mesh templates ofdeformable objects to train a network that predicts surfaceembeddings from the images. NeMo presents a gener-ative model trained with contrastive learning. presentsan extension through replacing vertices by Gaussian ellip-soids and using volume rendering. Similar to our work,many recent works leverage pre-trained vision tranformersDINO and DINOv2 to unproject image featuresonto depth maps . In contrast to , ourapproach does not require any pose annotation, while alsogoing beyond by enabling 3D pose estimation in thewild from a single image, and not requiring RGB-D imagesand CAD models as input . . Illustration of a neural mesh. It consists of a 3D meshwith one or several neural features per vertex. These features en-code patch-level features from a feature extractor. In our unsu-pervised alignment method, we capture the viewpoint-dependentfeatures from DINOv2. For pose estimation, we learn a category-level neural mesh with viewpoint-invariant features.",
  ". Method": "In this section, we describe our approach for learningcategory-level 3D pose estimation without supervision fromobject-centric videos. Our method proceeds in a two-stepapproach.First, we align object instances across videosin an unsupervised manner to bring them into a canonicalreference frame (.2). Given the aligned videos,our model learns to establish dense correspondences be-tween images and a reconstructed 3D template mesh bypredicting a feature vector for each pixel in a 2D imagethat corresponds to a visible vertex in the template mesh(.3). Finally, we describe how our model can ef-ficiently estimate object poses from in-the-wild data usingthe predicted correspondences via render-and-compare.",
  ". Meshes with Surface Features": "In both steps of our approach, the video alignment andthe representation learning, we represent objects as neuralmeshes, i.e. meshes with surface features to capture the ge-ometry and appearance of an object instance or category. In particular, the geometric representationis a triangular mesh, where we denote the set of verticesas V = {vi R3}|V |i=1. The appearance is represented bystoring one or multiple appearance features F = {{fki RD}|F |k=1}|V |i=1 at each mesh vertex. Together, the geometryand appearance define a neural mesh as S = {V, F}.",
  ". Self-supervised Alignment of Objects": "Our goal is to align the camera poses of multiple object-centric videos into a common coordinate frame. To achievethis, we represent each object-centric video as a neuralmesh with self-supervised surface features. In particular,we utilize off-the-shelf structure-from motion to ob-tain a coarse object shape reconstruction for each video. Note that the reconstructed shapes cover the whole object,as the object-centric videos move in a full circle around theobject. We post-process the reconstructed point cloud toclean it and generate a watertight mesh, for which we pro-vide details in the supplementary material. Subsequently,we project the reconstructed coarse meshes into the fea-ture map (I) that is obtained from a self-supervised trans-former backbone . We collect from every video a set offeature vectors for every vertex vi that describe the appear-ance of a local patch (). Thus the number of fea-tures per vertex depends on the number of images in whichthe vertex is visible. As feature extractor , we use a self-supervised vision transformer which has shown emerg-ing correspondence matching abilities.Finding geometric and appearance correspondences.Given the mesh vertices of the source object instance V andthe corresponding aggregated features F, we aim to alignthem to the reference counterparts V and F. In practice, weselect a reference video at random from the set of all avail-able videos and align the remaining videos to the reference.More precisely, we aim to optimize the transformation T,which is composed of rotation, translation and scale, underwhich the transformed source vertices and correspondingfeatures yield the minimal distance to the reference counter-parts with respect to geometry as well as appearance. For-mally, our optimization problem optimizes",
  "minT D(S, S, T)=Dgeo(V, V) + Dapp(S, S),(1)": "where D is the similarity between two videos given a trans-formation T, which combines a geometric distance betweenthe mesh geometries Dgeo(V, V) and an appearance dis-tance Dapp(S, S) between surface features.Assuming that the object instances shapes contain a neg-ligible variance and no symmetries, a suitable geometricdistance is the Chamfer Distance defined as",
  "argminj1...|V|mink,l ||fkj fli|| ,for vi V": "(5)Astheself-supervisedvertexfeaturesareview-dependent, the appearance distance computes the minimumfeature distance across all views to select the nearest neigh-bor.Weighting Correspondences. An open challenge forthe alignment of casually captured object-centric videos isthat the estimated correspondence pairs between videos canbe unreliable. For example, due to errors in the shape recon-struction or significant topology changes among differentobject instances, such as one bicycle having support wheelswhereas the other does not. In these cases the correspon-dences in the geometry and feature space are ill-definedwhich leads to unreliable correspondence estimates. To ac-count for such unreliable correspondences, we introduce aweight factor for each correspondence pair that estimatesits quality. At the core of the correspondence weighting, weintroduce a 3D cyclical distance among the vertices of twoneural meshes that is inspired by 2D cyclical distances for correspondence estimation, and is defined as",
  "dcycle(vi, fi) = ||vi v(vj,fj)||2, with j = (vi, fi). (6)": "The nested structure of our 3D cyclical distance first com-putes j as the index of the nearest neighbor of vertex vi inthe feature space, and in turn computes the nearest neighborof vj as v(vj,fj). Notably, dcycle(vi, fi) = 0 if the nearestneighbour maps back to the original vertex v(vj,fj) = viand hence the correspondence is reliable. Building on this3D cyclical distance, we define the validity criteria for eachpair of vertices as the sum of cyclical distances of the cor-respondence pair",
  "(D(V) + D(V)),(7)": "where D() is the diameter of a neural mesh given asD(V) = maxvi ,vjV ||vi vj||2.Toobtainthefinalweightfactorforacorre-spondencepairweusethesoftmaxnormalization(i, j)=Softmax((vi , fi , vj , fj)),across allfeature and gemoetric correspondences. For the softmaxnormalization, we introduce the temperature , whichenables us to steer between taking into account fewer highquality correspondences or more low quality ones (see.4). Together with the weighting we formulate theweighted geometric distance as",
  ". 3D Pose Estimation In-the-Wild": "Our goal is to perform 3D pose estimation in in-the-wildimages. To achieve this, we generalize our approach fromthe multi-view setting used to align object-centric videos,towards 3D pose inference from a single image. Our modeluses a feature extractor w(I) = F RDHW to ob-tain image features from input image I, where w denotesthe parameters of the backbone. The backbone output is afeature map F with feature vectors fi RD at positionsi on a 2D lattice. For training, we use the aligned object-centric videos (.2) to train the weights w of thefeature extractor w such that it predicts dense correspon-dences between image pixels and the 3D neural mesh tem-plate. Specifically, we relate the features of an image w(I)extracted by a backbone feature extractor to the vertex andbackground features by Von-Mises-Fisher (vMF) probabil-ity distributions . In particular, we model the likelihoodof generating the feature at an image pixel fi from cor-responding vertex feature fr as P(fi|fr) = cp()efifr,where fr is the mean of each vMF kernel, is the corre-sponding concentration parameter, and cp is the normaliza-tion constant (fi= 1, fr= 1). We also model the like-lihood of generating the feature fi from background featureas P(fi|) = cp()efi for RD.When learning the models, as described next, we willlearn the vertex features {fr}, the background feature RD, and the parameters w of the neural network backbonew. We emphasize that our model requires that the back-bone must be able to extract features that are invariant tothe viewpoint of the object to ensure that fi fr is largeirrespective of the viewpoint.Learning viewpoint-invariant vertex features.Fortraining our model, we use the visible vertex features andtheir corresponding image features P = {(fr, fi)}. Further,we use image features randomly sampled from the back-ground B = {fi}. As optimization objective, we use the",
  "(11)": "3D pose inference. We use the mesh with the vertexfeatures {fr}, the background feature and the trainedbackbone w to estimate the camera pose via render-and-compare. At each optimization step, we render a fea-ture map { fi()} under pose and compare it with theencoders feature map F = {fi}. Determined by the ren-dering, each feature map consists of foreground featuresFfront and background features Fback = F\\Ffront. There-upon, we maximize the joint likelihood for all image fea-tures under the assumption of independence, given as",
  "fiFbackP(fi|).(12)": "Note, by allowing foreground image features to be gener-ated by the background feature, we also account for clutter.We estimate the pose by first finding the best initializa-tion of the object pose by computing the joint likelihood(Eq.12) for a set of pre-defined poses via template matchingand choosing the one with the highest likelihood. Subse-quently, we iteratively update our initial pose using a differ-entiable renderer to obtain the final pose prediction .",
  ". Experimental Setup": "Dataset for alignment. To evaluate the unsupervised align-ment of object-centric videos, we use the recently releasedCommon Objects in 3D (CO3D) dataset that providesimages of multiple object categories, with a large amountof intra-category instance variation, and with varied ob-ject viewpoints.It contains 1.5 million frames, captur-ing objects from 50 categories, across nearly 19k scenes.For each object instance, CO3D provides approximately100 200 frames promising a 360 viewpoint sweep withhandheld cameras. CO3D supplements these videos withrelative camera poses and estimated object point clouds us-ing Structure-from-Motion . We find that the unfiltered videos of CO3D are not idealfor our purpose. In particular, we find that videos with littleviewpoint variation lead to inferior structure-from-motionresults. Also, videos that are not focusing on the objectscenter in 3D or are taken too close to it, contain little in-formation for correspondence learning. Therefore, we fil-ter the videos accordingly, targeting 50 videos per category.For multiple categories we end up with less than 50 videosnamely, remote 17, mouse 15, tv 16, toilet 7, toy-bus 41, hairdryer 28, couch 49, and cellphone 23.With our simple filters, we end up aiming for 50 videos percategory. More precise details for the filtering procedureare appended in the supplementary. As labels, we use theground truth pose annotations provided by ZSP , thatcover ten object instances of twenty different categories.Datasets for 3D pose estimation in-the-wild. We eval-uate on two common datasets PASCAL3D+ and Ob-jectNet3D . While PASCAL3D+ provides poses for the12 rigid classes of PASCAL VOC 2012, ObjectNet3D cov-ers pose annotations for over 100 categories. The object-centric video dataset CO3D covers 50 categories from theMS-COCO dataset. We find 23 common categoriesacross ObjectNet3D and CO3D, even tolerating the gap be-tween a toybus in CO3D and a real one in PASCAL3D+ andObjectNet3D. We believe that this non-neglegible gap couldbe bridged by exploiting the multiple viewpoint knowledgeof the same object instance. Overall we validate on PAS-CAL3D+ with 6233 images, using the same validation setas , and on ObjectNet3D on 12039 images. Following, we center all objects. Implementation details. In ouralignment step, we use = 100 and = 0.2. Further, weleverage as self-supervised ViT the publicly available smallversion of DINOv2 with 21M parameters and a patchsize of 14. At the input we use a resolution 448x448 end-ing up with a 32x32 feature map, where each feature yields384 dimensions. In our second step, we use the same ViTas backbone and freeze its parameters. Further, we add ontop three ResNet blocks with an upsampling step precedingthe final block. Ending up with a 64x64 feature map, whereeach feature has 128 dimensions. We optimize the cross-entropy loss for 10 epochs with Adam . In one epoch,we make use of all filtered videos. The training for eachcategory-level representation takes less than an hour on asingle NVIDIA GeForce RTX 2080. We note that the quality of our alignment method and thesubsequent representation learning can vary depending onthe chosen reference video. Therefore, we randomly choosefive reference videos per category and report the mean per-formance and the standard deviation across all results.",
  "0.0 3.5 10.7 0.0 0.0 11.9 15.9": ". Unsupervised alignment evaluation on the CO3D dataset across 20 categories. The reported metric is the 30 accuracy if notstated otherwise. The mean is computed across all 20 categories. We see that our method substantially outperforms the state of the art. . Qualitative comparison of two unsupervised alignment methods. The first row shows the alignment of our proposed method. Thesecond row shows the alignment using ZSP . For both methods we use the 5th object instance from left as reference. We see that ourproposed method is more accurate compared with ZSP. Especially for cars ZSP often confuses back and front.",
  ". Unsupervised Alignment": "We follow the evaluation protocol of ZSP and measurethe alignment of one object instance to the nine remainingones of the same category that are labelled. Additionally,we report the standard deviation across the chosen refer-ence object instances. The quantitative results in show that our proposed method significantly improves thestate of the art by 7.2% from 69.8% to 77.0%. Our align-ment algorithm can more efficiently use the video framescompared to ZSP , which only compares a single RGB-D frame from the source video with many RGB-D framesof the reference video. We note that ZSP uses DINOv1 fea-tures in contrast to our method, which uses DINOv2. There-fore, we provide an ablation of our method with respect todifferent feature extractors in the supplementary. One rea-son for our model to outperform UCD+ , an extension of ZSP, is likely that our optimization does exploit the objectgeometry extensively, whereas others are using it only forrefinement. A qualitative comparison of our method againstZSP is depicted in . It shows that our alignmentsare highly accurate despite a large variability in the objectinstances. We note that at the time of writing, there is nosource code publicly available to compare with UCD+.",
  ". In-the-Wild 3D Pose Estimation": "As we are not aware of any unsupervised method learn-ing pose estimation from videos, we compare our pose es-timation method against two supervised methods and ZSP. We provide ZSP with ten uniformly-distributedimages of the same reference video that our method uses.Further, we provide ZSP with depth annotations using thecategory-level CAD models and pose annotations in thePASCAL3D+ and ObjectNet3D data. Despite our methodnot requiring any depth information, it outperforms ZSP bya large-margin on both PASCAL3D+, see , and onObjectNet3D, see . Qualitative results are depictedin . We find that ZSP is highly compute intensive,requiring 10.92 seconds per sample on average, while ourproposed method takes only 0.22 seconds on average. . Qualitative comparison of our method (top) and ZSP (bottom) at category-level 3D pose prediction in the wild on samples fromPASCAL3D+ and ObjectNet3D (we randomly selected the samples to demonstrate the diversity of the results). For both methods, weoverlay our coarse mesh reconstruction in the predicted 3D pose.",
  "11.8 1.0 10.5 8.9 13.7 20.4 10.2": ". 3D Pose Estimation in-the-wild on 7 categories of PASCAL3D+. Top two rows show supervised methods (as upper bound) whilebottom two rows show unsupervised methods. The reported metric is 30 accuracy. The mean is averaged over all 7 categories. Ourmethod shows superior performance over ZSP, which requires depth annotations. Categorical discussion. We observe that our methodperforms better for categories with only small topologychanges and deformations, (e.g. car, microwave, couch)compared to categories with large intra-class variability(e.g. chair). Further, we recognize, that our method evengeneralizes well from a toybus to a real bus. Besides that,we analyze, that categories with less available videos (e.g.remote, TV, toilet) on average achieve lower performance.",
  ". Ablation": "Unsupervised alignment. Using the ground-truth annota-tions of our five references, we measure the effect of bothparameters introduced in the alignment method. Namely,the appearance distance weight and the cyclical distancetemperature . We remark that for the distance betweentwo meshes with surface features, the appearance weighttrades-off feature correspondences versus Euclidean corre-spondences. Where an appearance weight of = 0 meansthat the distance depends solely on the Euclidean correspon-dences. Contrarily, an appearance weight of = 1 results . We report the 30 accuracy of our alignment methodfor different choices of our appearance weight and our cyclicaldistance temperature resulting in different distances between twomeshes with surface features. We see that the maximum accuracyof 75.2% is reached for = 0.2 and = 100. in solely depending on feature correspondences. Further,our cyclical distance temperature weights each correspon-dence, implicitly trading-off many low-quality correspon-dences versus few high-quality ones. Intuitively, increas-",
  ". Average 30 and 15 accuracies on PASCAL3D+ andObjectNet3D for using directly neural network regression": "ing the value of results in averaging over more corre-spondences, while decreasing results in taking only thecorrespondences with high validity into account. In , we see that both parameters yield a significant impacton the 30 accuracy. With an optimum for = 0.2 and = 100. Intuitively, this means that taking many corre-spondences into account is more beneficial. Additionally,the Euclidean correspondences are weighted four times asmuch as the feature correspondences. Besides that, the ab-lation shows that while many correspondences are essentialfor using solely Euclidean correspondences, the opposite istrue when using solely feature correspondences.3D pose estimation in-the-wild. Following the align-ment, the in-the-wild 3D pose estimation task can also besolved using neural network regression with the 6D rotationrepresentation proposed in . However, we observe thatthe results are worse than our 3D template learning methodcombined with render-and-compare, see .",
  ". Limitations": "We have proposed a model which substantially outperformsexisting applicable baselines for the task of unsupervisedcategory-level 3D pose estimation in-the-wild. However,our proposed method does not yet reach the performance offully supervised baselines. One advancement we aspire is torelax the rigidity constraint of our shape model. Therefore,we plan to leverage the aligned reconstructions and intro-duce a parameterized model for the shape. A deformableshape would yield the potential to improve the correspon- dence learning as well as the subsequent matching of fea-tures at inference. Moreover, we see a future research di-rection in enabling the model to learn from a continuousstream of data, instead of building on a set of pre-recordedvideos. This would even better reflect the complex real-world scenarios of embodied agents.",
  ". Conclusion": "In this paper, we have proposed a highly challenging (butrealistic) task: unsupervised category-level 3D pose esti-mation from object-centric videos. In our proposed task, amodel is required to align object-centric videos of instancesof an object category without having any pose-labelled data.Subsequently, the model learns a 3D representation fromthe aligned videos to perform 3D category-level pose esti-mation in the wild. Our task defines a complex real-worldproblem which requires both semantic and geometric un-derstanding of objects, and we demonstrate that existingbaselines cannot solve the task.We further proposed anovel method for unsupervised learning of category-level3D pose estimation that follows a two-step process: 1) Amulti-view alignment procedure that determines canonicalcamera poses across videos with a novel and robust cyclicdistance formulation for geometric and appearance match-ing. 2) Learning dense correspondences between imagesand a prototypical 3D template by predicting, for each pixelin a 2D image, a feature vector of the corresponding vertexin the template mesh. The results showed that our proposedmethod achieves large improvements over all baselines, andwe hope that our work will pave the ground for future ad-vances in this important research direction.",
  "Shuichi Akizuki and Manabu Hashimoto.Asm-net:Category-level pose and shape estimation using parametricdeformation. In Proceedings of the British Machine VisionConference, pages 113, 2021": "Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou,Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-ing properties in self-supervised vision transformers.InProceedings of the IEEE/CVF International Conference onComputer Vision (ICCV), pages 96509660, 2021. Kai Chen and Qi Dou. Sgpa: Structure-guided prior adapta-tion for category-level 6d object pose estimation. In Proceed-ings of the IEEE/CVF International Conference on Com-puter Vision, pages 27732782, 2021. Michael Garland and Paul S Heckbert. Surface simplificationusing quadric error metrics. In Proceedings of the 24th an-nual conference on Computer graphics and interactive tech-niques, pages 209216, 1997.",
  "D Kirkpatrick and Raimund Seidel. On the shape of a setof points in the plane. IEEE Transactions on InformationTheory, 29(4):551559, 1983": "Adam Kortylewski, Ju He, Qing Liu, and Alan L Yuille.Compositional convolutional neural networks: A deep archi-tecture with innate robustness to partial occlusion. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 89408949, 2020. Georgios Kouros, Shubham Shrivastava, Cedric Picron,Sushruth Nagesh, Punarjay Chakravarty, and Tinne Tuyte-laars.Category-level pose retrieval with contrastive fea-tures learnt with occlusion augmentation.arXiv preprintarXiv:2208.06195, 2022.",
  "pose estimation of novel objects via render & compare. arXivpreprint arXiv:2212.06870, 2022": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer, 2014. Natalia Neverova, David Novotny, Marc Szafraniec, VasilKhalidov, Patrick Labatut, and Andrea Vedaldi.Continu-ous surface embeddings. Advances in Neural InformationProcessing Systems, 33:1725817270, 2020. Natalia Neverova, David Novotny, Marc Szafraniec, VasilKhalidov, Patrick Labatut, and Andrea Vedaldi.Continu-ous surface embeddings. Advances in Neural InformationProcessing Systems, 33:1725817270, 2020. Maxime Oquab, Timothee Darcet, Theo Moutakanni, HuyVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.Dinov2: Learning robust visual features without supervision.arXiv preprint arXiv:2304.07193, 2023. Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,Luca Sbordone, Patrick Labatut, and David Novotny. Com-mon objects in 3d: Large-scale learning and evaluation ofreal-life 3d category reconstruction. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 1090110911, 2021. Johannes L Schonberger,Enliang Zheng,Jan-MichaelFrahm, and Marc Pollefeys.Pixelwise view selection forunstructured multi-view stereo. In Computer VisionECCV2016: 14th European Conference, Amsterdam, The Nether-lands, October 11-14, 2016, Proceedings, Part III 14, pages501518. Springer, 2016. Hao Su, Charles R Qi, Yangyan Li, and Leonidas J Guibas.Render for cnn: Viewpoint estimation in images using cnnstrained with rendered 3d model views. In Proceedings ofthe IEEE international conference on computer vision, pages26862694, 2015.",
  "Angtian Wang, Adam Kortylewski, and Alan Yuille. Nemo:Neural mesh models of contrastive features for robust 3dpose estimation. arXiv preprint arXiv:2101.12378, 2021": "Angtian Wang, Peng Wang, Jian Sun, Adam Kortylewski,and Alan Yuille.Voge: a differentiable volume rendererusing gaussian ellipsoids for analysis-by-synthesis. In TheEleventh International Conference on Learning Representa-tions, 2022. He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin,Shuran Song, and Leonidas J Guibas.Normalized objectcoordinate space for category-level 6d object pose and sizeestimation.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 26422651, 2019.",
  "Yang Xiao, Xuchong Qiu, Pierre-Alain Langlois, MathieuAubry, and Renaud Marlet. Pose from shape: Deep poseestimation for arbitrary 3d objects. BMVC, 2019": "Yang Xiao, Yuming Du, and Renaud Marlet. Posecontrast:Class-agnostic object viewpoint estimation in the wild withpose-aware contrastive learning. In 2021 International Con-ference on 3D Vision (3DV), pages 7484. IEEE, 2021. Yang Xiao, Vincent Lepetit, and Renaud Marlet. Few-shotobject detection and viewpoint estimation for objects in thewild. IEEE Transactions on Pattern Analysis and MachineIntelligence, 45(3):30903106, 2022.",
  "Heng Yang, Jingnan Shi, and Luca Carlone. Teaser: Fastand certifiable point cloud registration. IEEE Transactionson Robotics, 37(2):314333, 2020": "Yanjie Ze and Xiaolong Wang. Category-level 6d object poseestimation in the wild: A semi-supervised learning approachand a new dataset. Advances in Neural Information Process-ing Systems, 35:2746927483, 2022. Kaifeng Zhang, Yang Fu, Shubhankar Borse, Hong Cai,Fatih Porikli, and Xiaolong Wang. Self-supervised geomet-ric correspondence for category-level 6d object pose estima-tion in the wild. In The Eleventh International Conferenceon Learning Representations, 2022. Xingyi Zhou, Arjun Karpur, Linjie Luo, and Qixing Huang.Starmap for category-agnostic keypoint and viewpoint esti-mation. In Proceedings of the European Conference on Com-puter Vision (ECCV), pages 318334, 2018. Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and HaoLi. On the continuity of rotation representations in neuralnetworks. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 57455753,2019.",
  "A.1. Alignment Ablation": "In , we study the effect for different distance calcu-lations between two vertices containing many features frommany viewpoints. We observe, that averaging over the fea-tures in a single vertex before calculating the distance toanother vertex reduces the performance drastically. Further,calculating the distance by averaging over the bi-directionalnearest neighbor distances, slightly improves the perfor-mance compared to taking the minimum distance over thebi-directional nearest neighbor distances.Besides that, we show that refining the initial alignmentusing few gradient-based optimization steps improves theresults, especially with respect to the more fine-grained 10",
  "A.3. Mesh Reconstruction from Videos": "Using structure-from-motion we obtain a point cloudP = {vi R3} for each video. Further, we reconstruct acoarse mesh using three steps. First, we randomly down-sample the point cloud to 20000 points and clean it usingthe object segmentation provided by CO3D. Therefore, wecompute an average ratio of visibility for each point vi byprojecting it in all N frames, using the respective projectionj for frame j, and averaging over the respective visibilitiesj(j(vi)) as follows",
  "j=1j(j(vi)).(14)": "Hereby we set j(j(vi)) = 0, if the projected vertex is notinside the frame. We filter out all points which ratio of vis-ibility lies below 60%. Second, we use alpha shapes to estimate a coarse shape from the clean point cloud. Fig-uratively speaking, this algorithm starts off with a convexvolume and then iteratively carves out spheres while pre-serving all original points. We set the size of the sphere to10 times the particle size, where the particle size is the aver-age distance of each point to its 5th closest point. Third, weuse quadratic mesh decimation to end up with a maxi-mum of 500 faces. This method iteratively contracts a pairof vertices, minimizing the projective error with the facesnormals. All steps are visualized in .",
  "A.4. Videos Filtering": "We filter out three types of videos. Type a), object is too faraway from the camera. Type b), object is too close to thecamera. Type c), the variance of viewpoints is too small.Type a) is not ideal because the point cloud and the imagesyield only few details of the object. Type b) is problematicbecause the close-ups prevent us from robustly cleaning thenoisy point cloud as there is less information accumulatedfrom the object segmentations. Type c) results in a verynoisy or even broken structure-from-motion. For the iden-tification of type a), object is too far away from the cam-era, we use the average object visibility over all N frameswidth U and height V formally defined as",
  "v=1j(u, v).(15)": "We require an average object visibility of at least 10%. Afiltered out video is illustrated in . For the identifi-cation of type b), the object is too close to the camera, weuse the projection of the 3D center into all frames, expect-ing it to be in the center of the frames. We compute the 3Dcenter c using the camera rays with position rj and directionnj by minimizing its projected distances to the rays",
  "jnjnTj rj.(17)": "With the outer product njnTj R33. For a correct camerafocus, we expect the projected 3D center to lie within thecentered rectangle spanning 60% of the image width andheight. In total, we require 80% of the frames to be focusedon the 3D center. A negative example is provided in . A filtered out video is illustrated in .For the identification of type c), the variance of view-points is too small, we subtract the center c of all camerapositions rj and normalize them to lie on the unit sphere.Further, we divide the unit sphere into 38 bins and calcu-late the viewpoint coverage as percentage of viewpoint binscovered. We require a viewpoint coverage for each video of15%. A rejected video is shown in .",
  ". Rejected video type c), the variance of viewpoints is toosmall": ". Qualitative comparison of our method (top) and ZSP (bottom) at category-level 3D pose prediction in the wild on samples fromObjectNet3D (we randomly selected the samples to demonstrate the diversity of the results). For both methods, we overlay our coarsemesh reconstruction in the predicted 3D pose. . Qualitative comparison of two unsupervised alignment methods. The first row illustrates the alignment of our proposed method.The second row shows the alignment using ZSP . For both methods, we utilize the 5th object instance from the left as a reference point.Our proposed method proves to be more precise than ZSP."
}