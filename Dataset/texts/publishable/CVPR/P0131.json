{
  "Abstract": "This work presents Adaptive Local-then-Global Merging(ALGM), a token reduction method for semantic segmenta-tion networks that use plain Vision Transformers. ALGMmerges tokens in two stages: (1) In the first network layer,it merges similar tokens within a small local window and(2) halfway through the network, it merges similar tokensacross the entire image. This is motivated by an analysisin which we found that, in those situations, tokens with ahigh cosine similarity can likely be merged without a dropin segmentation quality. With extensive experiments acrossmultiple datasets and network configurations, we show thatALGM not only significantly improves the throughput byup to 100%, but can also enhance the mean IoU by up to+1.1, thereby achieving a better trade-off between segmen-tation quality and efficiency than existing methods. More-over, our approach is adaptive during inference, meaningthat the same model can be used for optimal efficiency oraccuracy, depending on the application. Code is availableat",
  ". Introduction": "Vision Transformers (ViTs) have shown to be very effec-tive for image segmentation tasks .However, the computational complexity ofthe multi-head self-attention operation scales quadraticallywith the number of input pixels. This harms the computa-tional efficiency, especially on the high-resolution imagesthat are typically used for image segmentation. To alleviatethis burden and improve the efficiency, several works haveproposed methods to reduce the number of tokens that theViT has to process. Most token reduction methods havebeen introduced for image classification , butthere is also an increasing amount of work that focuses ontasks like semantic segmentation .In this work, we also focus on semantic segmentation, and Im/sec (log scale) mIoU BaselineBaseline + ALGM (ours)",
  ".Efficiency and segmentation quality for ALGM,applied to Segmenter , SegViT , and SETR onADE20K. On average, ALGM improves the throughput of thesebaselines by 39%, while improving the mIoU by +0.7": "aim to design a token reduction method that achieves abetter balance between efficiency and segmentation qualitythan existing works.This objective is motivated by the limitations of exist-ing works. First, token pruning methods ,which are popular for image classification, discard uninfor-mative tokens. They are not directly applicable to semanticsegmentation, as each token requires a prediction. To over-come this, token pausing or halting methods retain discarded tokens and aggregate them with the othertokens at the end of the ViT. However, these methods ob-serve a drop in segmentation quality, possibly because use-ful information contained in the halted tokens is not avail-able in later network layers. Alternatively, token sharingand merging methods avoid discarding tokens, and repre-sent multiple image patches or tokens with a smaller setof tokens . This approach allows these meth-ods to maintain the segmentation quality, but requires themto introduce additional computational overhead to identifytokens for sharing or merging, and they apply token reduc-tion only once, limiting the efficiency gain. Furthermore,token merging methods that have been designed for imageclassification yield a notable decline in segmentation qual-",
  "arXiv:2406.09936v1 [cs.CV] 14 Jun 2024": "ity when applied to semantic segmentation .Based on these existing works, we make two observa-tions: (a) CTS demonstrates that local token sharing inthe early network stages enhances efficiency without com-promising segmentation quality, but it inefficiently requiresa pre-processing network. Therefore, our first objective isto merge redundant tokens early in the network without re-quiring a pre-processing, and still maintain the segmenta-tion quality. (b) Token merging approaches like ToMe show that gradually merging redundant tokens across theentire image (i.e., globally) can greatly boost the efficiency,but at the cost of segmentation quality. Thus, our secondobjective is to also apply global token merging to furtherimprove efficiency, but without harming the segmentationquality.To achieve these objectives, we need to find an efficientmethod to identify tokens that can be merged without caus-ing a drop in segmentation quality. Inspired by existing to-ken merging methods , in Sec. 3.2, we explore ifthe cosine similarity is a suitable measure to identify merge-able tokens. Concretely, we compare the cosine similaritiesbetween tokens representing the same category i.e., intra-class tokens, which are potentially redundant and can bemerged and tokens representing different categories i.e.,inter-class tokens, which should not be merged. We findthat (a) already in the 1st network layer, the similarities be-tween intra-class tokens in small local windows are muchhigher than for inter-class tokens, and (b) comparing tokensglobally, intra-class token similarities become increasinglyhigher than inter-class similarities in later layers.Based on these findings, we present our Adaptive Local-then-Global Merging (ALGM) module that integrates twotoken merging phases. In the first network layer, ALGMadopts a local merging strategy. This is followed by a globalmerging mechanism in an intermediate layer, to also reduceglobal token redundancies. Moreover, rather than using apredetermined number of merged tokens, our approach dy-namically decides the number of merged tokens based onthe semantic complexity of the image content. Finally, werestore the original token resolution to make segmentationpredictions. For details, see Sec. 3.3.ALGM offers multiple advantages. (a) Unlike methodsthat use token pausing, redundant tokens remain active inthe network, and continue to contribute in subsequent net-work layers via their merged representation. (b) ALGMavoids the need for preprocessing layers and the significantoverhead associated with token sharing or merging meth-ods. (c) Global merging is only applied when token similar-ity is sufficiently reflective of category similarity, reducingthe chance of merging tokens that should remain separate.(d) Being a parameter-free approach, the ALGM module isnaturally compatible with all plain ViT backbones, as wellas any segmentation decoder, with or without re-training. Through experiments outlined in Sec. 4, we demonstratethat ALGM consistently enhances the throughput by con-siderable margins when applied to a wide range of differ-ent segmentation methods (see ). Moreover, we ob-serve that, on top of this improved efficiency, ALGM alsoenhances the segmentation quality. From an investigationinto the cause of this improvement, we find that it can beattributed to two factors: a better balance between frequentand infrequent categories in the self-attention operation, andthe denoising of tokens. For more detailed results, we referto Sec. 5.We summarize our main contributions as follows:",
  ". Related work": "Since the introduction of the Vision Transformer (ViT), asubstantial amount of work has been dedicated to improvingthe efficiency of these ViTs. In this work, we focus on tokenreduction, which aims to decrease the number of tokens thatare processed by the ViT, to improve efficiency. Token reduction in general.The majority of token re-duction methods have been introduced for ViTs that con-duct image classification. Some methods use a token prun-ing strategy, where uninformative tokens are identified andsimply discarded . Uninforma-tive tokens are identified by making intermediate predic-tions with auxiliary heads , or obtaining impor-tance scores from attention weights . Pruned to-kens can be discarded completely or fusedinto one token to preserve information flow . Whiletoken pruning can notably enhance the throughput of trans-formers, discarding tokens is not possible for semantic seg-mentation as each token requires a prediction, and fusedtokens representing multiple regions or categories cannotbe trivially reconstructed to make a semantic segmentationprediction. Alternatively, token merging methods combinegroups of tokens into a smaller set of representative to-kens. Some works introduce a learned layer to map theoriginal token set to a smaller one , whilemost methods merge tokens if they have a high similarityscore . Other methods combine differ-ent merging, pruning or fusing approaches . Token reduction for semantic segmentation.Some to-ken merging methods for image classification can also beapplied to semantic segmentation by re-constructing merged tokens to their original positions tomake a prediction. However, while these approaches im-prove efficiency, they consistently cause a drop in segmen-tation quality, as also shown by Liang et al. .Other token reduction methods have been proposedspecifically for the semantic segmentation task.Tokenpausing or halting approaches identify tokensthat produce high-confidence predictions in early networklayers, and do not process them any further. Instead of dis-carding tokens like pruning methods, they retain tokens andreconstruct them later to make a final prediction. However,in subsequent layers, these paused tokens do not partici-pate in the self-attention operation anymore, meaning thatpotentially useful information is no longer available, whichnegatively affects the segmentation quality. Alternatively,ELViT and AiluRus introduce a non-parametrictoken clustering layer that merges redundant neighboringtokens in one network layer. These methods are able to re-duce tokens while maintaining the segmentation quality, buttheir efficiency gain is limited. It is likely that this is be-cause their clustering layer introduces computational over-head, and because they reduce tokens only once, not usingthe token redundancies potentially present in other layers.CTS uses a CNN-based policy network to identifyimage patches that can share a token before the first trans-former layer. This approach can also reduce tokens whilemaintaining segmentation quality, but its policy network in-troduces computational overhead, and it only merges tokensin local windows, ignoring global redundancies.Inspired by the advantages and limitations of existingwork, this work proposes a parameter-free token mergingmethod for semantic segmentation that applies both localand global merging based on cosine similarities between to-kens. Importantly, with minimal computational overhead,we identify where cosine similarities can be used to selecttokens for merging while maintaining or even improving thesegmentation quality.",
  "(b) Global token similarities": ". Comparison of cosine similarity between intra-classand inter-class tokens. On ADE20K training set using Segmenter+ ViT-S . (a) Local similarities across 5 window sizes inthe first layer. (b) Layer-wise analysis of global similarities. token ti belongs to Rd, with d denoting the feature dimen-sion. These token embeddings are subsequently processedby transformer layers L = {L1, ..., LL}, resulting in theoutput TL. Each layer Ll in L integrates a multi-head self-attention (MHSA) block followed by a multi-layer percep-tron (MLP) block which output T l = MHSA(Tl1)+Tl1",
  "and Tl = MLP(T l ) + T l respectively.(b) A decoder": "D : TL P, which utilizes transformer or convolutionallayers to process TL and generate the per-pixel segmenta-tion prediction P, where P RCHW and C representsthe number of classes. Our primary objective is to reducethe number of tokens in T, the total set of tokens, by iden-tifying those tokens that can be merged without adverselyaffecting the segmentation quality of P.",
  ". Token similarity analysis": "As highlighted in Sec. 1, considering the advantages andlimitations of existing methods, our goal is to find a methodto (a) apply early local token merging without requiringa pre-processing network and (b) also apply global to-ken merging to further improve efficiency, without harm-ing the segmentation quality.To achieve this, we needto find a method that can efficiently identify tokens suit-able for merging while preserving the segmentation qual-ity. CTS is based on the hypothesis that tokens thatrepresent the same semantic class can be merged withoutcompromising segmentation quality, since they carry redun-dant information. On the other hand, several token mergingmethods for image classification merge tokenswith a high cosine similarity. They get promising efficiencygains, but sometimes at the cost of accuracy. This motivatesus to examine if and when cosine similarity can be an effec-tive metric to identify tokens that represent the same cate-gory and are thus suitable for both local and global merging.To analyze this, we extract and compare the similaritiesbetween tokens from Segmenter with ViT-S trainedon the ADE20K training set. (1) We first analyze thelocal similarities within kk windows in the first trans-former layer. We calculate the cosine similarity between ...",
  "Input Image Prediction": "Step 2: Compute the mean similarityof tokens within each window andretain edges with a similarity greaterthan . ... Step 3: Keepthe edges withsimilaritygreater than . ...... Step 1: Partitionthe tokens into twosets A and B.Construct abipartite similaritygraph between thetokens. 2... ... Step 2: Keep oneedge from tokensin set A and theirmost similartokens in set B. ... ... Step 4: Merge theconnected tokens,and concatenate thesets back together. ... Option A Option B ... ... . ALGM comprises two primary modules: (1) Conditional Local Average Pooling (CLAP) for local merging and (2) GlobalBipartite Matching (GBM) for global merging. The top section illustrates the placement of these modules in the first and middle layers,while the bottom provides a detailed visualization of the individual modules. tokens representing different categories (i.e., inter-class to-kens), which should not be merged, and tokens represent-ing the same category (i.e., intra-class tokens), which canbe merged. As illustrated in a, the smaller the win-dow size k, the more accurately cosine similarity reflectsthat tokens depict the same category. Consequently, tokenswith a high cosine similarity within small local windows inthe first layer can likely be merged without a drop in seg-mentation quality. (2) We analyze the global similarities bycalculating the cosine similarities for inter-class and intra-class tokens across the entire image for all transformer lay-ers. As seen in b, the global similarities in early lay-ers do not accurately represent category correspondence, sothey should not be employed to identify tokens for merging.However, deeper in the network, cosine similarity becomesa better measure to identify tokens that can be merged glob-ally without affecting segmentation quality.",
  ". Adaptive Local-then-Global Merging": "From the analysis we know that (a) local token similari-ties in early layers and (b) global token similarities in in-termediate layers are likely good indicators of the merge-ability of tokens. To exploit this, we propose the AdaptiveLocal-then-Global Merging (ALGM) approach. As demon-strated in , the process begins with local merging inthe first layer using the Conditional Local Average Pool- ing (CLAP) module. In an intermediate layer, we adopt theGlobal Bipartite Merging (GBM) module which is basedon the BSM algorithm for global merging. The proce-dure concludes with a token unmerging module to restorethe original token resolution. Local token merging.Given the insights from Sec. 3.2,we aim to merge tokens in the first layer if they have ahigh similarity with neighboring tokens in a small win-dow.To implement this, we introduce the CLAP mod-ule, which is positioned in layer L1, between the MHSAand MLP blocks, as illustrated in . The CLAP mod-ule follows these steps: (1) It receives the token embed-dings T 1 RNd from layer L1 and reshapes them intoa grid T G1 RHp W p d.It then defines a window ofsize k k and groups the tokens within each window intoseparate sets W = {w1, . . . , ws}, where s =Nk2 . Eachw Rkkd is a set of token embeddings, represented asw = {t1, t2, . . . , tk2}. (2) Subsequently, for each w in W, itcomputes the cosine similarity between all pairs of tokens tiin w, and calculates the mean of these similarities to get w.Then, as we hypothesize that the similarity between tokensrepresents the mergeability, CLAP merges only the tokensin windows w for which w > , where is an automat-ically determined similarity threshold, which is elaborated later in this section. (3) Finally, the tokens inside selectedwindows w are merged into a single token by taking the av-erage of these tokens. The indices of these tokens are alsostored for later unmerging. Once completed, merged andnon-merged tokens are concatenated to produce the outputtoken embeddings T 1 = {t1, . . . , tN } where N N. Global token merging.After local merging, the tokensare processed through standard transformer layers up tolayer Lm, where the GBM module is applied. Similar to thesteps of the BSM algorithm: (1) In the first step, the to-kens in T m are split into two sets: A = {t1, t3, . . . , tN 1} and B = {t2, t4, . . . , tN }.A fully-connected bipartitegraph is then constructed between the tokens within thesesets based on their cosine similarity. (2) Then, GBM onlyretains the edges that represent the highest similarity froma token in set A to any token in set B. This means thatfor a given token Ai , the edge to token Bj is only re-tained if Bj is the most similar token to Ai when comparedto all other tokens in set B. (3) Next, unlike the originalBSM which employs a constant number of merged tokens,GBM uses a similarity threshold . Thus, edges are onlyretained if their similarity exceeds . (4) Finally, the tokenswith remaining edges are merged by taking their average,and their indices are stored for future unmerging. The twosets are then concatenated, yielding the output embeddingsT m = {t1, . . . , tN } where N N .",
  "Token unmerging.Upon completing global merging, theembeddings T m are processed through the remaining Lm": "transformer layers, resulting in the final token embeddingsTL. These embeddings, along with the indices of mergedtokens retained during the merging phases, are providedas inputs to the decoder D. In this phase, we deploy anunmerging module that duplicates the embeddings of themerged tokens at the indices of the tokens from which theywere merged. For transformer-based decoders, which aredesigned to handle tokens, the unmerging module is appliedafter the decoder.Conversely, for CNN-based decoders,which require spatially organized features, token unmerg-ing is executed prior to the decoder. Adaptive token merging.As the complexity of imagesvaries, reducing a constant number of tokens can lead toa suboptimal efficiency or segmentation quality. Mergingtoo many tokens in complex images can lead to insufficientrepresentation of their complexity. Conversely, simpler im-ages can benefit from a more reduced token count, enhanc-ing efficiency. To tackle this challenge, we introduce anadaptive method that automatically determines a similaritythreshold. Before training, we take the base segmentationmodel to which we want to apply ALGM, and then run in-ference on the training set. We then extract the tokens after the MHSA block in each layer Ll, calculate the cosine sim-ilarities between all token pairs, and compute the mean simand standard deviation sim of these similarities across theentire training set. Given these statistics, we then set thethreshold = sim + sim.Using this threshold, the number of remaining tokens N and N after the CLAP and GBM modules will vary perimage. During training, to facilitate batching of images andtokens, we take the maximum number of remaining tokensN and N per batch, and apply this to all images in thebatch.",
  ". Experimental setup": "Datasets.WeconductourmainexperimentsonADE20K , which is widely recognized as a chal-lenging scene parsing dataset.Additionally, we showALGMs general applicability on the Pascal Context ,Cityscapes , and COCO-Stuff-10K datasets. Implementation details.ALGM can be applied to anysegmentation model that uses plain ViTs. We apply it tothree popular networks: Segmenter , SETR , andSegViT using four standard ViT backbones : ViT-T/S/B/L. For a fair comparison, we train all networks usingthe original hyperparameters and official implementations.By default, we integrate our CLAP and GBM modules atthe 1st and 5th layers for ViT-T/S/B models, and at the 1st",
  "and 7th layers for ViT-L, using the automatically generatedthreshold for both merging phases. For more details, seeAppendix A": "Evaluation metrics.To assess the segmentation quality,we use the standard mean Intersection-over-Union (mIoU)metric, and for computational efficiency we evaluate thethroughput in terms of images per second (im/sec) andthe number of floating point operations (FLOPs). For thethroughput, we calculate the average im/sec on the valida-tion set with a batch size of 32 on an Nvidia A100 GPU,after a 50-iteration warmup. To calculate the number ofFLOPs, we use fvcore and compute the average num-ber of operations over all images in the validation set. Wereport the number of GFLOPs, i.e., FLOPs 109.",
  "+ ALGM* (ours)48.129227": ". Main results on ADE20K. ALGM applied to Segmenter(Seg) , SegViT , and SETR across 4 ViT backbones.ALGM* is the same trained model as ALGM, but uses the thresh-old during inference that achieves the best efficiency while main-taining the mIoU w.r.t. the baseline.Indicates a training-freemethod, applied directly to the baseline model. across various settings, additional results are provided inAppendix B.1. For our ALGM, we report two versions:(1) the default ALGM, which uses the automatic thresholdduring both training and inference, and (2) ALGM*, whichis the same trained model, but during inference it uses thesmallest threshold for which the mIoU is higher than thebaseline. In other words, ALGM* is tuned for optimal ef-ficiency while maintaining the segmentation quality. SeeSec. 5.4 and Appendix B.5 for more details on the use ofdifferent merging thresholds. ADE20K.Tab. 1 presents the results of ALGM and ex-isting token reduction methods for different segmentationmodels and ViT backbones on the ADE20K dataset .We find that, across all settings, ALGM is able to consider-ably improve the throughput and number of GFLOPs with respect to the base segmentation networks, and also achievea substantial mIoU increase.This shows that our tokenmerging approach does not only improve the efficiency, butalso boosts the segmentation quality. In Sec. 5.4, we pro-vide a more detailed analysis into the cause of this mIoU im-provement. The ALGM* variant, which is optimized for ef-ficiency, is able to improve the throughput even further (upto +100% for Seg-L), while consistently achieving an mIoUthat is the same or slightly higher than the base networks.Moreover, ALGM and ALGM* consistently outperform allexisting token reduction works, in terms of both the mIoUand the efficiency metrics. This shows that our method canfind a better balance between segmentation quality and effi-ciency, which is the objective of this work. Other datasets.When applying ALGM to COCO-Stuff,Cityscapes and Pascal-Context in Tab. 2, we ob-serve very similar results as for ADE20K. For all datasets,our default ALGM significantly improves the throughputwith respect to the base segmentation networks, while alsoobtaining a better segmentation quality. Again, ALGM*can improve the throughput even further, obtaining through-put improvements of +90% on Cityscapes for Seg-S and+72% on COCO-Stuff for Seg-L without any drop in seg-mentation quality.On the COCO-Stuff and Pascal-Context datasets, ALGMand ALGM* also consistently outperform all existing meth-ods. For Cityscapes, ALGM outperforms ToMe , Ailu-Rus , and ELViT , but it does not achieve the sameefficiency improvements as CTS . We observe that thevisual homogeneity of the Cityscapes images causes tokensto be similar in the first transformer layer even if they donot belong to the same category, requiring a higher mergingthreshold and limiting the efficiency improvement. Over-all, taking into account all datasets, we can conclude thatALGM is a robust and generally applicable method thatconsistently improves the efficiency of ViT-based segmen-tation models while also enhancing their accuracy. Comparison with other works.In Tab. 3, we compareALGM to DToP and DoViT .However, theseworks report mIoU and GFLOPs results for SETR andSegmenter without token reduction that differ from theresults we obtain from the official code of SETR and Seg-menter, while DToP and DoViT do not release their code.Therefore, we can only compare the relative performancedifferences obtained due to token reduction. In Tab. 3, weobserve that DToP can maintain the mIoU while reducingthe GFLOPs by 25%, whereas ALGM* maintains the mIoUand reduces the GFLOPs by 30%. Compared to DoViT,ALGM considerably improves both the segmentation qual-ity and efficiency.For further comparisons, see Appen-dices B.2 and B.3.",
  "(c) Pascal-Context": ". Main results on COCO-Stuff, Cityscapes and Pascal-Context. ALGM applied to Segmenter (Seg) across 3 ViT backbones and3 datasets. ALGM* is the same trained model as ALGM, but uses the threshold during inference that achieves the best efficiency whilemaintaining the mIoU w.r.t. the baseline. Indicates a training-free method, applied directly to the baseline model.",
  ". Application to state-of-the-art model": "To demonstrate the effectiveness of ALGM on a large-scale state-of-the-art network, we apply it to the EVAbackbone with a ViT-Adapter + Mask2Former de-coder . Here, we calculate the average throughput on4 Nvidia A6000 GPUs due to memory requirements. Theresults in Tab. 4 show that without training, we can improvethe throughput by 26% while keeping the mIoU constant.When we also train the model, we achieve the same effi-ciency gains but now also further improve the mIoU with+0.2. These results show the general effectiveness and com-patibility of ALGM with large-scale pre-trained networks.",
  ". Ablations": "CLAP module window size.In Tab. 5a, we evaluate theeffect of using different window sizes for our CLAP mod-ule. We find that smaller window sizes yield higher mIoUscores, whereas larger window sizes result in a better effi-ciency. This is as expected, as we have found in athat, the smaller the local window is, the more likely it isthat a high token similarity indicates that tokens depict thesame category and can thus share a token without harmingthe segmentation quality. On the other hand, using smallerwindow sizes means that fewer tokens are merged, limitingthe overall efficiency improvement. Impact of merging modules.To assess the impact of theindividual CLAP and GBM merging modules, we evaluatevarious configurations in Tab. 5b. We find that only apply-ing CLAP leads to modest improvements in both the mIoUand the throughput, showing that local merging is effective.If we use the GBM module in the first layer instead, we findthat the throughput increases but at the cost of segmenta-tion quality, confirming our findings in Sec. 3.2 and bthat global token similarities in the first layer should not beused to identify tokens for merging. Conversely, placingthe GBM module in layer 5 does yield an improved mIoU,albeit with a lower efficiency gain. Applying GBM in bothlayer 1 and layer 5 results in a significantly better efficiency,but the incorrectly merged tokens in layer 1 are then mergedeven further in layer 5, harming the segmentation quality.Finally, combining CLAP with GBM yields the best mIoUwhile achieving a significant efficiency gain, showing thepower of applying both early local merging and later globalmerging.",
  ". Detailed analyses": "Cause of segmentation quality improvement.The mainresults in Tab. 1 and Tab. 2 have shown that ALGM notonly enhances efficiency, but also improves the segmenta-tion quality. This improvement is most significant for com-plex datasets, and we hypothesize that it has two causes:(1) Balancing: As tokens that depict the same categoryare merged, large and frequently-occurring categories arerepresented by fewer tokens, meaning that they play a lessdominant role in the self-attention operation, causing amore balanced attention distribution with respect to rareclasses. To assess if this is true, in Tab. 6, we evaluate asetting in which we do not reduce the number of tokens andtherefore do not balance the attention process, but insteadreplicate the average token embedding across the tokensthat would otherwise be merged. We find that this causes adrop in mIoU, indicating that attention balancing is indeed afactor in the mIoU improvement of ALGM. (2) Denoising:As tokens are merged, we take the average of their values.This denoises the tokens, which could facilitate the learningprocess. To evaluate this aspect, we evaluate a configura-tion where we do not take the average of tokens but insteadpick one random token from each set of mergeable tokens.Tab. 6 shows that doing so also results in a considerabledrop in mIoU. Finally, when disabling both denoising andbalancing, the mIoU is at its worst. This implies that bothdenoising and balancing play an important role, and that themerging of dominant tokens in ALGM is a potent approachto rectify attention imbalances and reduce token noise toenhance the segmentation quality. Im/sec 45.0 45.2 45.4 45.6 45.8 46.0 46.2 46.4 mIoU 0.88 0.86 0.85 0.83 0.81 0.80 0.79 0.78 0.77 0.98 0.96 0.94 0.92 0.90 0.86 0.84 0.82 ALGM* Thresholds during inferenceThresholds during trainingBaselineAutomatic threshold",
  ". Merged tokens. We depict tokens that are merged as aresult of the local CLAP and global GBM merging modules": "Similarity threshold.As explained in Sec. 3.3, we com-pute an automatic similarity threshold to select tokensthat can be merged. In , we show the performanceof ALGM with different thresholds. We find that our au-tomatic threshold finds an optimal balance between effi-ciency and segmentation quality. Interestingly, taking theautomatic threshold during training and using lower thresh-olds during inference leads to less significant mIoU dropsthan training with a lower . We expect that this differencearises from the fact that early-training embeddings are lessaccurate, leading to overly aggressive merging during train-ing, which the network cannot recover from; naturally, thisproblem does not occur during inference. Overall, these re-sults show the versatility of ALGM at test time, as it canbe used to optimize efficiency while keeping the mIoU thesame like we do with ALGM*, but also to achieve a highermIoU than the baseline. We observe similar results for otherdatasets, see Appendix B.4. Visualization.In , we visualize which tokens aremerged as a result of the CLAP local merging and GBMglobal merging modules. We observe that tokens with vi-sual similarity are mostly merged by CLAP, whereas cate-gory correspondence plays a larger role for GBM. For morequalitative results, see Appendix C.",
  ". Discussion": "In this work, we propose a token reduction method forsemantic segmentation that combines early local mergingwith later global merging. This is motivated by the findingthat, using this merging strategy, we predominantly mergetokens that contain redundant information, meaning thatthey can be merged without compromising the segmenta-tion quality. With extensive experiments, we show that ourapproach is indeed able to find a very good balance betweenefficiency and segmentation quality, outperforming existingwork. Interestingly, we also find that using ALGM for to-ken reduction leads to substantial mIoU improvements forcomplex datasets. With some first analyses, we find that thisis likely caused by improved attention balancing and tokendenoising. However, further research is required to fully un-derstand the causes of these phenomena and their potentialapplicability to other networks and tasks. Another interest-ing avenue for future research could be to examine whethertoken reduction is similarly effective on more complex taskslike panoptic and video segmentation. AcknowledgementsThis work was funded by the EUproject MODI, grant no. 101076810, and the KDT JU EdgeAIproject, grant no. 101097300, and utilized the Dutch national e-infrastructure, supported by the SURF Cooperative under grantno. EINF-5197, funded by the Dutch Research Council (NWO).",
  "Evann Courdier, Prabhu Teja Sivaprasad, and FrancoisFleuret. PAUMER: Patch Pausing Transformer for SemanticSegmentation. In BMVC, 2022. 1, 3, 14": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image isWorth 16x16 Words: Transformers for Image Recognition atScale. In ICLR, 2021. 3, 5, 8, 13, 14, 15, 16, 17 Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu,Xinggang Wang, Tiejun Huang, Xinlong Wang, and YueCao. EVA: Exploring the Limits of Masked Visual Repre-sentation Learning at Scale. In CVPR, 2023. 7, 12, 15, 16 Mohsen Fayyaz, Soroush Abbasi Kouhpayegani, FarnoushRezaei Jafari, Eric Sommerlade, Hamid Reza Vaezi Joze,Hamed Pirsiavash, and Juergen Gall. Adaptive token sam-pling for efficient vision transformers. ECCV, 2022. 1, 2",
  "Huaibo Huang, Xiaoqiang Zhou, Jie Cao, Ran He, and Tie-niu Tan. Vision Transformer with Super Token Sampling. InCVPR, 2023. 1": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, andRoss Girshick. Segment Anything. In ICCV, 2023. 1 Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, WeiNiu, Mengshu Sun, Xuan Shen, Geng Yuan, Bin Ren, HaoTang, et al. SPViT: Enabling Faster Vision Transformers viaLatency-Aware Soft Token Pruning. In ECCV, 2022. 1, 2",
  "Bowen Zhang, Zhi Tian, Quan Tang, Xiangxiang Chu, Xi-aolin Wei, Chunhua Shen, et al. SegViT: Semantic Segmen-tation with Plain Vision Transformers. In NeurIPS, 2022. 1,5, 6, 12, 13, 14": "Minghang Zheng, Peng Gao, Renrui Zhang, Kunchang Li,Xiaogang Wang, Hongsheng Li, and Hao Dong. End-to-endobject detection with adaptive clustering transformer. arXivpreprint arXiv:2011.09315, 2020. 13 Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, TaoXiang, Philip H.S. Torr, and Li Zhang. Rethinking Seman-tic Segmentation from a Sequence-to-Sequence Perspectivewith Transformers. In CVPR, 2021. 1, 5, 6, 7, 12, 13, 15",
  "A. Implementation details": "For our main experiments, we implement ALGM on top ofthe publicly available code of Segmenter1 , SETR2 ,SegViT3 , and EVA4 . For Segmenter, SETR, andSegViT, we ensure a fair comparison by training all net-works using the original hyperparameters and official im-plementations. When training models with ViT-T/S/B back-bones, we utilize 2 Nvidia A100 GPUs, and for ViT-L,we use 4 Nvidia A100 GPUs. When training EVA ,which comprises 40 transformer layers and 1.0B parame-ters, a batch size of 32 necessitates 32 GPUs with 40GBVRAM. Given our compute limitations, instead, we fine-tune this model using 4 Nvidia A6000 GPUs with a batchsize of 8 for an additional 10k iterations. GBM module position.Tab. 7 lists the positions of theCLAP and GBM modules in different segmentation net-works. As can be seen, the CLAP module is always ap-plied in layer 1 only.To determine the layer where theGBM module should be applied, we implement GBM inseveral layers on a pre-trained segmentation model with-out fine-tuning or training, and find where it yields thebest results. Specifically, we select the earliest layer forwhich the baseline mIoU is maintained, since (a) our ob-jective is to maintain the segmentation quality, and (b) thepotential efficiency gain is higher if GBM is applied ear-lier, since more layers will process a reduced set of tokens.This approach aligns with strategies followed in existingworks . For EVA , we apply the GBMmodule in more than one layer, which is further discussed",
  "in Appendix B.6. The similarity threshold is calculatedautomatically for all models, given the strategy explainedin Sec. 3.3": "Adaptive token merging.Our method adaptively deter-mines the number of merged tokens for each image us-ing the similarity threshold .As a result, each imagein the training and validation sets has a different numberof remaining tokens N and N .This variability intro-duces challenges in batch processing. To solve this duringtraining, ALGM is adaptive on a batch level by using thelargest value of N or N in the batch. In simpler terms,it merges the same number of tokens for each image in abatch, and this number is the minimum number of merge-able tokens across all images in a batch. This ensures thattoken reduction is guided by the most complex image in thebatch, retaining essential details and eliminating the needfor padding. During inference, with a batch size of 1, whichis common in real-world situations, ALGM is adaptive perimage. Throughput evaluation.As mentioned in the previoussection, the adaptive nature of our method results in a dif-ferent number of remaining tokens N and N for each im-age in the validation set. This variability complicates batchprocessing, which we use for stable throughput evaluationfollowing existing work . Specifically, if we would padthe sets of tokens, or use the largest N or N in the batchlike during training, this would give an incorrect image ofthe obtainable throughput. To solve this, and still allow forbatched evaluation, we use batches of 32 duplicates of thesame image, so that the number of reduced tokens is equalthroughout the batch. We apply this to each image, and cal-culate the average throughput over the entire validation set.To ensure a fair comparison, we apply the same approachto evaluate the throughput of existing work, the image cropsize used for these calculations is the same that is used dur-ing training.",
  "B.1. Comparison with existing work across tokenreduction settings": "In Sec. 5.1, we present our main results in which, for dif-ferent existing token reduction methods, we report the ver-sion that achieves the highest efficiency while still main-taining the segmentation quality as much as possible. For amore comprehensive comparison, we compare our ALGMwith these existing methods across a range of different to-ken reduction settings, essentially evaluating the trade-offbetween efficiency and segmentation quality. For methodsELViT , EViT , ToMe , and ACT , we followthe different token reduction settings specified by Liang etal. . For AiluRus , we report their results acrossthree token reduction settings. For our method, ALGM,we present the results for various similarity threshold val-ues during inference. The results for these experiments areprovided in for ADE20K, Cityscapes and Pascal-Context.On the ADE20K and Pascal-Context datasets, ourALGMconsistentlyoutperformsothermethodsandachieves a better balance between mIoU and computationalefficiency.On the ADE20K dataset, ALGM achieves amIoU of 51.9, slightly surpassing the baseline, while oper-ating with a 45% reduction in GFLOPs. When compared toits closest competitor, AiluRus , our method achievesthe same segmentation quality with 14% fewer GFLOPs.On the other datasets, ALGM also achieves a considerablybetter balance between the mIoU and GFLOPs than existingworks. The only exception is CTS on the Cityscapesdataset. As explained in Sec. 5.1, this is due to the visualhomogeneity of the images of this dataset.",
  "B.2. Comparison with DToP on Pascal-Context": "As mentioned in Sec. 5.1, the performance of SETR without token reduction as reported by DToP doesnot align with the results we obtain from the official codeof SETR. Additionally, DToP has not made its code pub-licly available. As a result, we can only compare to this GFLOPs mIoU Segmenter-L +ACT+ALGM(Ours)+AiluRus+CTS +ELViT+EViT+ToMeBaseline",
  ".ALGM vs. PAUMER .All models are appliedto Segmenter (Seg) with ViT-S and evaluated on theADE20K validation set; indicates differences": "sented in Sec. 5.1, Tab. 8 compares ALGM to DToP on thePascal-Context dataset . For SETR-B, we observe thatALGM achieves a better segmentation quality improvementwith the same efficiency. Moreover, ALGM* can achieve amuch better efficiency while obtaining the same segmen-tation quality improvement as DToP. Applied to SegViT-L , we find that ALGM* can maintain the mIoU whileachieving the same efficiency improvement that DToP ob-tains while that method causes a mIoU drop. Again, thesecomparisons highlight that ALGM achieves a better trade-off between segmentation quality and efficiency.",
  "B.3. Comparison with PAUMER": "When comparing our method to PAUMER , we observesimilar challenges as for DToP , so we can only com-pare in terms of relative throughput changes. Tab. 9 presentsa comparative analysis between ALGM and PAUMERwhen applied to Segmenter on the ADE20K validationset. The results show that, at equal throughput improve-ments, ALGM achieves significantly better mIoU scores.The difference is especially notable when the throughput isincreased by +100%, where PAUMER causes a mIoU dropof -10.7 but ALGM can limit the decrease to -1.9.",
  "B.4. Different similarity thresholds": "In Sec. 5.4, we explore the impact of different similar-ity thresholds on the models performance on ADE20K.Here, we conduct these experiments also for other datasets. shows the results for the Cityscapes, COCO-Stuffand Pascal-Context datasets.For all datasets, it is clearthat automatic thresholds offer a good balance between effi-ciency and segmentation quality. Similar to the findings forthe ADE20K dataset, we observe that employing a lowerthreshold during training leads to a significant decrease inmIoU. However, using a lower threshold during inferenceresults in a more modest decline in mIoU while notably im-proving efficiency. These findings enable a valuable strat-egy, where we train ALGM with the automatic threshold,but can reduce the threshold during inference to improvethe efficiency with minimal impact on the mIoU. This howwe obtain ALGM*. This also demonstrates the versatility Im/sec 76.2 76.4 76.6 76.8 77.0 mIoU 0.95 0.94 0.930.92 0.91 0.90 0.89 0.990.97 0.94 0.93 0.92 ALGM* Thresholds during inferenceThresholds during trainingBaselineAutomatic threshold",
  "of our method, as it is suitable for various applications withdifferent demands for efficiency and accuracy.Notably, the automatically calculated threshold for the": "Im/sec 38.2 38.4 38.6 38.8 mIoU 0.90 0.88 0.86 0.84 ALGM ALGM* Segmenter ViT-T (ADE20K) 75.077.580.082.585.087.5 Im/sec 48.6 48.8 49.0 49.2 49.4 mIoU 0.94 0.92 0.90 0.88 ALGM ALGM* Segmenter ViT-B (ADE20K) Im/sec 51.8 52.0 52.2 52.4 52.6 mIoU 0.95 0.93 0.91 ALGM ALGM* Segmenter ViT-L (ADE20K) Im/sec 42.4 42.6 42.8 43.0 mIoU 0.90 0.88 0.86 0.84 0.82 0.80 ALGM ALGM* Segmenter ViT-S (COCO-Stuff) 70.072.575.077.580.082.5 Im/sec 43.6 43.8 44.0 44.2 44.4 mIoU 0.96 0.94 0.92 0.90 ALGM ALGM* Segmenter ViT-B (COCO-Stuff) Im/sec 46.8 46.9 47.0 47.1 47.2 47.3 47.4 mIoU 0.95 0.93 0.91 ALGM ALGM* Segmenter ViT-L (COCO-Stuff) Im/sec 46.4 46.6 46.8 47.0 mIoU 0.96 0.92 0.88 0.84 0.80 0.78 ALGM ALGM* SETR ViT-B (ADE20K) Im/sec 48.10 48.15 48.20 48.25 48.30 48.35 48.40 mIoU 0.99 0.97 0.95 0.93 0.91 ALGM ALGM* SETR ViT-L (ADE20K) 87.590.092.595.097.5 Im/sec 52.2 52.3 52.4 52.5 52.6 52.7 mIoU 0.90 0.86 0.82 0.78 0.74 0.72 ALGM ALGM* SETR ViT-B (Pascal-Context) . Obtaining ALGM*. ALGM is applied to Segmenter and SETR with various backbones (ViT-T, ViT-S, ViT-B andViT-L) on the ADE20K , COCO-Stuff and Pascal-Context validation sets. These figures show the values of thresholds for the ALGM and ALGM* versions. ALGM* is the same trained model as ALGM, but uses the threshold during inference that achievesthe best efficiency while maintaining the mIoU w.r.t. the baseline. Cityscapes dataset is relatively high compared to the thresh-olds obtained for other datasets. This observation alignswith our results in Sec. 5.1, where we explained that thevisual homogeneity of Cityscapes images causes tokens tohave high cosine similarities in the first transformer layer,even when they do not depict the same category. This ne-cessitates a higher merging threshold, consequently limitingthe potential efficiency improvements.",
  "B.5. Obtaining ALGM*": "In our main experiments, we present two versions of ourmethod: (1) ALGM, which consistently applies an auto-matic threshold during both training and inference, and (2)ALGM*, which is the same trained model as ALGM butuses the lowest possible threshold during inference forwhich the mIoU remains above the baseline. This ALGM*version is designed to optimize efficiency while maintaining the segmentation quality. To illustrate the process of obtain-ing ALGM*, shows the results of ALGM with differ-ent thresholds during inference, and compares this with thebaseline mIoU performance without token reduction.",
  "B.6. Additional ablations": "Multiple GBM modules.In Tab. 10a, we examine the ef-fect of applying the GBM module in more than one layer.The results indicate that while the application of the GBMmodule in both the 5th and 7th layer significantly increasesthroughput, it also results in a noticeable reduction in mIoUcompared to its sole application in the 5th layer. This im-plies that overly aggressive global token merging using theGBM module negatively impacts segmentation quality.For EVA , which is a much larger model with 40transformer layers, we conduct a similar experiment inTab. 11. Here, we find that applying the GBM module mul-",
  "+ ALGM11, 21, 3161.52.43538+ ALGM11, 2161.42.13722+ ALGM1161.423872": ". Multiple GBM modules in EVA. ALGM is applied toSOTA method EVA + ViT-Adapter + Mask2Former andevaluated on the ADE20K validation set, with single-scale testing.Directly applied to the backbone without fine-tuning. tiple times does not cause a drop in mIoU. We hypothesizethat a very large model like EVA introduces considerableadditional redundancies in its many layers, which GBMcan then reduce without harming the segmentation quality.However, further research is required to explore this in moredetail. Merging module placement.We conduct an ablationto identify the optimal location for the merging moduleswithin a transformer layer. As shown in Tab. 10b, placingthem between the multi-head self-attention (MHSA) blockand the MLP yields the best performance in terms of boththe segmentation quality and the efficiency. Token merging operation.Tab. 10c compares the per-formance of different token merging operations. Pick ran-dom token represents the operation where a single randomtoken is picked from each set of tokens that can be merged,and is used to replace these tokens. This approach resultsin the loss of important information because the selected to-ken might not be the best representation of the collective setof tokens. On the other hand, taking the average of all to-kens in each set yields a much better performance. It causesthe merged token to be better representative of the origi-nal tokens, because it consolidates the information from alltokens in the set. Moreover, it can denoise these token em-beddings, as discussed in Sec. 5.4.",
  "+ AiluRus (Fine-tuning)52.1--": ". Effect of training with ALGM and existing work. Allmethods are applied to Segmenter (Seg) with ViT-L andevaluated on ADE20K . We evaluate three settings. (1) Direct:direct application without further training. (2) Training: trainingthe model from scratch for 160k iterations. (3) Fine-tuning: re-suming training from a pre-trained model for 160k iterations. Effect of training.Since our method introduces no addi-tional learnable parameters, ALGM can easily be integratedwith off-the-shelf pre-trained ViT-based networks to run in-ference directly while reducing tokens. To assess the impactof training the models after module integration, we explorethree scenarios: (a) directly applying the module during in-ference without any additional training, (b) fine-tuning themodel for an additional 16 epochs, resuming training fromthe model pre-trained on the ADE20K dataset, and (c) train-ing the model for 64 epochs, starting from the model pre-trained on the ImageNet dataset . These are situationsthat have been evaluated before in earlier work . Theresults of these approaches are presented in Tab. 10d. Weobserve that applying ALGM improves efficiency across allscenarios. While direct integration maintains the baselinemIoU, further training, particularly training from scratch,significantly improves the segmentation quality.As wediscuss in Sec. 5.4, these results indicate that training themodel with the ALGM module leverages the benefits of at-tention balancing and token denoising during training, lead-ing to improvements in segmentation quality. For further insights into the effect of training, we alsocompare ALGM against ELViT , ToMe , and Ail-uRus , which are explicitly designed for direct appli-cation without additional training. As shown in Tab. 12,although ALGM is primarily designed for training fromscratch, it still achieves competitive mIoU results withouttraining, while being more efficient. Furthermore, we ob-serve that training these existing training-free methods doesnot cause them to perform better than when applied directly.This shows that training a network with token reductionmethods does not automatically give a mIoU boost, and thatit is the design of our approach that enables this.",
  "CLAP module position.In this experiment, we investi-gate the impact of applying the CLAP module in differenttransformer layers. We keep the GBM module at the 5th": "layer. The findings, as outlined in Tab. 10e, show that to-ken embeddings in the first layer are sufficiently represen-tative of class correspondence for effective local merging.Delaying the application of CLAP to the second or thirdlayers does not significantly impact the mIoU, but it doesnegatively affect the efficiency. Thus, applying the CLAPmodule in the first layer achieves the optimal balance of ef-ficiency and accuracy. Feature selection.As mentioned in Sec. 3.3, our ap-proach utilizes the cosine similarity of token embeddingsX to identify tokens for merging. Here, we examine the ef-fect of using cosine similarity of other features i.e., keys,queries, and values to determine which tokens can bemerged. f shows the results. Although using keysor queries results in the highest throughput, using the tokensX yields a considerably higher mIoU and a throughput thatis close to the throughput obtained by using keys or queries.This differs from the findings for ToMe , where the keysare identified as the best option to identify mergeable to-kens for the image classification task. We hypothesize thatthis can be attributed to the fact that all token embeddingsare directly used to make the final semantic segmentationprediction, whereas image classification networks only usea single CLS token for the final class prediction . Thisgives the tokens a more important role for semantic segmen-tation, making them the most appropriate feature to use fortoken reduction.",
  "C.1. Merging operations": "In , , , and , we show quali-tative examples of the effect of CLAP local merging andGBM global merging. The similarity map displays the aver-age cosine similarity between tokens within a 22 window,which is used to determine which tokens can be merged in the CLAP module. These figures demonstrate that for alldatasets, CLAP predominantly merges tokens with a highvisual similarity.On the other hand, GBM also mergestokens that depict the same category but have lower vi-sual similarity, after acquiring more informative embed-dings in the middle network layers. Notably, from row 3and 4 in , depicting Cityscapes examples, it becomesclear that the high visual homogeneity of Cityscapes imagescauses tokens to have high cosine similarities also if theydo not depict the same class. As mentioned in Sec. 5.1 andAppendix B.1, this high average cosine similarity causesthe automatic threshold to be quite high, limiting the effi-ciency improvement of the ALGM method on this dataset."
}