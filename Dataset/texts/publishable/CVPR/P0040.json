{
  "Abstract": "Capturing different intensity and directions of light raysat the same scene, Light field (LF) can encode the 3D scenecues into a 4D LF image, which has a wide range of ap-plications (i.e., post-capture refocusing and depth sensing).LF image super-resolution (SR) aims to improve the imageresolution limited by the performance of LF camera sensor.Although existing methods have achieved promising results,the practical application of these models is limited becausethey are not lightweight enough. In this paper, we proposea lightweight model named LGFN, which integrates the lo-cal and global features of different views and the featuresof different channels for LF image SR. Specifically, owingto neighboring regions of the same pixel position in dif-ferent sub-aperture images exhibit similar structural rela-tionships, we design a lightweight CNN-based feature ex-traction module (namely, DGCE) to extract local featuresbetter through feature modulation. Meanwhile, as the posi-tion beyond the boundaries in the LF image presents a largedisparity, we propose an efficient spatial attention mod-ule (namely, ESAM) which uses decomposable large-kernelconvolution to obtain an enlarged receptive field and an effi-cient channel attention module (namely, ECAM). Comparedwith the existing LF image SR models with large parame-ter, our model has a parameter of 0.45M and a FLOPs of19.33G, which has achieved a competitive effect. Exten-sive experiments with ablation studies demonstrate the ef-fectiveness of our proposed method, which ranked the sec-ond place in the Track 2 Fidelity & Efficiency of NTIRE2024Light Field Super Resolution Challenge and the seventhplace in the Track 1 Fidelity.",
  ". Introduction": "LF cameras can capture varying intensities and directionsof light rays within the same scene, encoding the 3D scenecues into a 4D LF image (comprising spatial and angulardimensions). This technology finds wide applications, in-cluding post-capture refocusing, depth sensing,virtual reality, and view rendering. However,due to the limitation of sensor performance, there exists atrade-off between the spatial resolution and angular resolu-tion of LF images. How to improve the resolution of LFimages is currently a prominent research challenge.The traditional LF SR method mainly focuseson how to find sub-pixel information and warp multi-viewimages based on estimated disparities. However, the per-formance of these methods heavily depends on accurateestimated disparities, which is difficult to achieve in low-resolution LF images and complex imaging environmentssuch as occlusion and non-Lambert reflection.In recent years, deep learning-based methods have beenwidely used. Yoon et al. proposed the first CNN-based",
  "arXiv:2409.17759v1 [eess.IV] 26 Sep 2024": "reshape reshape W H V W UH HV W A slice of horizontal EPI A slice of vertical EPI . The epipolar plane images(EPI) sample of 4D LF is ac-quired with fixed angular coordinate and a fixed spatial coordinate.Specifically, the horizontal EPI is obtained with constants u and h,and the vertical EPI is obtained with constant v and w. On the onehand, the EPIs capture spatial structures such as edges or textures,and the adjacent areas corresponding to the same pixel positionacross different SAIs exhibit similar structural relationships. Onthe other hand, the EPIs reflect the disparity information via linepatterns of different slopes, whereas positions located outside theboundary in the LF image exhibit a large parallax. LF image SR model (i.e., LFCNN), which used SRCNN tosuper-resolve each sub-aperture image (SAI). Afterwards,many methods have adopted the CNN-based methods tointegrate different angle information to improve the per-formance of SR. Besides directly processing the4D LF data, some methods extracted two kinds of featuresby designing spatial and angular feature extractor, and in-teracted with each other.Apart from the CNN-based LF image SR methods, Transformer-based LF meth-ods have also been proposed.Wang et al. proposeda detail-preserving Transformer (DPT) for LF image SR.Liang et al. proposed a simple yet efficient Transformermethod for LF image SR. Liang et al. proposed EPIT toLF image SR by learning non-local space and angle coop-eration. Jin et al. proposed DistgEPIT model that learnglobal features and local features of LF images by design-ing an attention branch and a convolution branch respec-tively.While existing methods have achieved promisingresults, the practical application of these models is limiteddue to excessive parameters and FLOPs. As shown in ,the parameters of some existing LF image SR methods aremostly above 1M. This limitation prompts our research intolightweight LF image SR.",
  "As shown in , adjacent areas at the same pixel po-sition across different SAIs exhibit similar structural rela-": "tions, which are suitable for processing by the local fea-ture extraction module. On the other hand, the position out-side the boundary in the LF image exhibits a large parallax,which requires aggregation of contextual features acrossSAIs for processing. To consider these two aspects and the requirement oflightweight model, we propose a lightweight local andglobal feature learning model named LGFN. By integrat-ing both local and global features of different views andthe features of different channels, our lightweight modelcan achieve competitive results compared with the exist-ing model with larger parameters. Specifically, our modelchooses the convolution module with local representation.Different from the existing CNN-based methodswhich use complex network structure, we propose a sim-ple yet efficient convolution module designed to extract lo-cal features through feature modulation performed by twoparallel convolution branches. Inaddition,wechooseattentionmechanismtoextract contextual features.Different from the existingTransformer-based methods with quadraticcomplexity over the number of visual tokens, we proposea simple yet efficient spatial attention module, whose atten-tion weight branch uses decomposable large-kernel convo-lution to obtain an enlarged receptive field, and multiplies itwith identity branch to extract contextual features. Besides,an efficient channel attention module (namely, ECAM) isintroduced to enhance the features between channels. In or-der to further refine the feature extraction, we extract thelocal and global features along the horizontal and verticaldirections respectively.",
  "Our main contribution can be summarized as:": "1. We design a lightweight convolution modulation modulenamed DGCE to extract the local spatial features of LFimages. A lightweight spatial attention module namedESAM with enlarged receptive field is designed to ex-tract global features. In order to further refine the featureextraction, we extract the local and global features alongthe horizontal and vertical directions respectively.",
  ". We design an efficient channel attention module namedECAM and use the statistical information of channel di-rection to model the correlation between different chan-nels": "3. We propose a light-weight LF image SR model namedLGFN, which has a parameter of 0.45M and a FLOPs of19.33G. Compared with the existing LF image SR mod-els with large parameter, it has achieved a competitive ef-fect, and won the second place in the Track 2 Fidelity &Efficiency of NTIRE2024 Light Field Super ResolutionChallenge and the seventh place in the Track 1 Fidelity.",
  ". Traditional Methods": "The traditional LF image SR methods mainly focuses onhow to find sub-pixel information and warp multi-view im-ages based on estimated disparities. Based on estimateddisparities, Bishop et al. used a Bayesian deconvolu-tion method to super-resolve LF images. Wanner et al.used EPI to estimated disparity maps and proposed varia-tional framework for LF image SR. Farrugia et al. pro-posed an example-based LF image SR method, enhancingspatial resolution consistently across SAIs through learn-ing linear projections from reduced-dimension subspacesand angular super-resolution via multivariate ridge regres-sion. Besides, optimization-based methods have also beenproposed. Alain et al. adopted an optimization methodto solve ill-posed LF image SR problem based on sparsityprior. Rossi et al. coupled the multi-frame informationwith a graph regularization, adopted convex optimizationmethod to solve LF image SR problem.However, the performance of these methods dependsheavily on accurate estimated disparities, it is difficult toachieve in low-resolution LF images and complex imag-ing environments such as non-Lambertian surfaces orocclusions.",
  ". CNN-based Methods": "In recent years, deep learning-based method have beenwidely used. Yoon et al. proposed the first CNN-basedLF image SR model (i.e., LFCNN), which used SRCNNto super-resolve each SAI. Similarly, Yuan et al. usesEDSR to super-resolve each SAI. Afterwards, many meth-ods have adopted the CNN-based methods to integrate dif-ferent angle information to improve the performance of SR.Wang et al.proposed a bidirectional recurrent CNN net-work iteratively model spatial relations between horizon-tally or vertically adjacent SAIs. Zhang et al. proposedresLF network that used four-branch residual network ex-tracted features from SAI images along four different angu-lar directions. Zhang et al. proposed a 3D convolutionsnetwork extracted features from SAI images along differentangular directions. Cheng et al. considered the charac-teristics of internal similarity and external similarity of LRimages, and fused these two complementary features for LFimage SR. Meng et al. directly used 4D convolution toextract the angle information and spatial information of theLF image. Wang et al.designed an angular deformablealignment module (ADAM) for feature-level alignment, andproposed a collect-and-distribute approach to perform bidi-rectional alignment between the center-view feature and each side-view feature. In addition to directly processingthe 4D LF data, some methods disentangled the 4D LFsinto different subspaces for SR. Wang et al. proposeda spatial and angular feature extractor to extract the corre-sponding spatial and angular information from the MacPIimage, and proposed LF-InterNetand DistgSSRtorepetitively interact the two features.Besides the aforementioned methods to improve SR per-formance, some methods try to solve the complex degra-dation problem facing the real world. To address the is-sue of the domain gap in LF image SR, Cheng et al.proposed a zero-shot learning framework. They dividedthe end-to-end model training task into three sub-tasks:pre-upsampling, view alignment, and multi-view aggrega-tion, and subsequently tackled each of these tasks sepa-rately by using simple yet efficient CNN networks. Xiaoet al. proposed the first real-world LF image SR datasetcalled LytroZoom, and proposed an omni-frequency pro-jection network(OFPNet), which deals with the spatiallyvariant degradation by dividing features into different fre-quency components and iteratively enhancing them. Wanget al. developed a LF degradation model based on thecamera imaging process, and proposed LF-DMnet that canmodulate the degradation priors into CNN-based SR pro-cess.",
  ". Transformer-based Methods": "In addition to the CNN-based LF image SR methods,Transformer-based LF methods have also been proposed.Wang et al. proposed a detail-preserving Transformer(DPT) for LF image SR, which regards SAIs of each ver-tical or horizontal angular view as a sequence, and estab-lishes long-range geometric dependencies within each se-quence via a spatial-angular locally-enhanced self-attentionlayer. Liang et al. proposed a simple yet efficient Trans-former method for LF image SR, in which an angular Trans-former is designed to incorporate complementary informa-tion among different views, and a spatial Transformer isdeveloped to capture both local and long-range dependen-cies within each SAI. By designing three granularity aggre-gation units to learn LF feature, Wang et al.proposeda multi-granularity aggregation Transformer (MAT) for LFimage SR; Liang et al. proposed EPIT to LF image SRby learning non-local space and angle cooperation. Jin et al. proposed DistgEPIT model that learns global featuresand local features of LF images by designing an attentionbranch and a convolution branch respectively.Although the existing models have achieved promis-ing results, their model parameters and FLOPs are notlightweight enough, which limits their practical application.In order to solve these problems, we propose a lightweightSR model of LF image by designing efficient modules.",
  ". Method": "As mentioned above, the LF image SR needs to considerthe local similarity between SAI subgraphs on the one hand,and the disparity problem behind different subgraphs on theother hand, which urges us to consider the methods of localand global feature extraction.In order to design a lightweight model with fewer param-eters and FLOPs, we choose to reduce the high-dimensionalfeature space to the low-dimensional feature subspace, anddesign an efficient local and global feature extraction modelto achieve LF image SR.",
  ". Network Architecture": "Specifically, as illustrated in , our LF image SR modelmainly consists of three components: shallow feature ex-traction, deep feature extraction and up-sampling mod-ule.Given an input LF low-resolution image FLRRUV HW denote an LR SAI array with U V SAIsof resolution H W.Our method takes FLR as itsinput and generates a HR SAI array of size FHRRUV sHsW ,where s denotes the upsampling factor.Firstly, in the shallow feature extraction part, the low-resolution 4D LF image is upsampled using bilinear inter-polation to the size of sH sW. Meanwhile, it is convertedto F0 R1UV HW format and passed through a 133spatial convolution to extract the shallow feature Finit, andthe number of channels is increased from 1 to 64:",
  "Double-Gated Convolution Extraction Module": "Owing to neighboring regions of the same pixel position indifferent SAIs exhibit similar structural relationships, whichis suitable for processing with local feature extraction mod-ule.Some studieindicate that modulation mechanismprovides satisfactory performance and is theoretically effi-cient (in terms of parameters and FLOPs). Therefore, wedesign a local feature extraction module based on featuremodulation, as shown in (b). In order to extract thelocal features better, the shallow features first undergo a11 convolution, and then are cut into two halves along thechannel. One half of the features undergoes a 33 depth-wise convolution and GELU function, and the other halfof the features undergoes pixel-wise multiplication with thecorresponding pixels to obtain the enhanced local features.After they are added to each other, they are fused by a 11convolution:",
  "Efficient Spatial Attention Module": "Owing to the position beyond the boundaries in the LF im-age presents a large disparity, which requires aggregate con-text features among different SAIs, therefore we propose asimple yet efficient spatial attention module, as shown in(c). In order to reduce FLOPs, a 11 convolution isused to reduce the number of channels, and then stridedconvolution and max pooling are used to further reduce theheight and width of features. In order to further increasethe receptive field of spatial attention, the large-kernel con-volution is decomposed into a depth-wise convolution,a dilated convolution and a 11 point convolution, whichcan capture long-range relationships while maintaining low",
  "ECAM": "+ . An overview of our LGFN network. (a) Local and global deep feature extraction module (LGFM); (b) Double-gated convolutionextraction module (DGCE); (c) Efficient spatial attention module (ESAM); (d) Efficient channel attention module (ECAM). Given SAIsas inputs, we adopt bilinear upsamping to initial content of the original images. For feature extraction, we first use a 3D convolution toextract shallow features, then use the deep feature extraction module to get them, and finally use the upsampling module to obtain ultimatesuper-resolved SAI results. The depth feature extraction module (DFEM) includes seven local and global feature extraction modules, whichare composed of DGCE, ESAM and ECAM. computational cost and few parameters. Then, the spatialresolution is restored to the original scale by up-sampling,and the number of channels is restored to the original num-ber by a convolution. Therefore, attention with a large re-ceptive field is obtained, which is convenient for the next at-tention calculation. The difference between our ESAM andother spatial attention modules is that the receptive field hasbeen enlarged.F25 = Hconv1(F24)(9)",
  "Ours": "29.51/0.84 Perforated_Metal_3 . Qualitative results for 4x SR. The super-resolved center view images are presented for detailed texture comparison. The corre-sponding PSNR/SSIM scores of different methods on the presented scenes are also reported below. adaptive average pooling, each channel and its three adja-cent channels are convolved with convolution kernel of 3to capture local cross-channel interaction information, andtwo types of channel attention are obtained by sigmoidfunction, and then the channel attention is calculated afteradding them:",
  "Methods#ParamEPFLHCInewHCIoldINRIASTFganryAverage": "Bilinear-24.57 / 0.815827.09 / 0.839731.69 / 0.925626.23 / 0.875725.20 / 0.826126.95 / 0.8566Bicubic-25.14 / 0.832427.61 / 0.851732.42 / 0.934426.82 / 0.886725.93 / 0.845227.58 / 0.8701VDSR0.665M27.25 / 0.877729.31 / 0.882334.81 / 0.951529.19 / 0.920428.51 / 0.900929.81 / 0.9066EDSR 38.89M27.84 / 0.885429.60 / 0.886935.18 / 0.953629.66 / 0.925728.70 / 0.907230.20 / 0.9118RCAN 15.36M27.88 / 0.886329.63 / 0.888635.20 / 0.954829.76 / 0.927628.90 / 0.913130.27 / 0.9141 resLF 8.646M28.27 / 0.903530.73 / 0.910736.71 / 0.968230.34 / 0.941230.19 / 0.937231.25 / 0.9322LFSSR 1.774M28.27 / 0.911830.72 / 0.914536.70 / 0.969630.31 / 0.946730.15 / 0.942631.23 / 0.9370LF-ATO 1.364M28.52 / 0.911530.88 / 0.913537.00 / 0.969930.71 / 0.948430.61 / 0.943031.54 / 0.9373LF-InterNet5.483M28.67 / 0.916230.98 / 0.916137.11 / 0.971630.61 / 0.949130.53 / 0.940931.58 / 0.9388",
  "Mode#ParamDGCEESAMECAMEPFLHCLnewHCLoldINRIASTFgantryAveragePSNR": "Parallel453.6k30.046130.514536.285330.082330.109831.8076BaselineCascade453.6k30.178230.416936.310232.048130.053331.8014-0.0062-452.9k29.814830.550936.447031.783830.302031.7804-0.0272-409.5k29.848130.549036.163031.787430.154931.6995-0.1081-409.5k29.815630.304836.077832.121529.911431.6462-0.1614Parallel147.0k27.451027.971032.750428.956426.700328.7796-3.0280 Data Augmentation. All LFs in the released datasetsused the bicubic downsampling approach to generate LFpatches of size 3232. We performed random horizontalflipping, vertical flipping, and 90-degree rotation to aug-ment the training data by 8 times. Note that, the spatialand angular dimension need to be flipped or rotated jointlyto maintain LF structures.Regularization. Our network was trained using the L1loss and FFT Charbonnier loss with weights of 0.01 and 1respectively. Optimized using the Adam method with 1 =0.9, 2 = 0.999 and a batch size of 1. Our model was imple-mented in PyTorch on a PC with a NVidia RTX 3060 GPU.The learning rate was initially set to 2x10-4 and decreasedby a factor of 0.5 for every 15 epochs. The training wasstopped after 100 epochs.We used the PSNR and SSIM computed only on the Ychannel of images as quantitative metrics for performanceevaluation. To compute the metric scores for a dataset con-taining M scenes, we firstly computed the average score ofeach scene by separately averaging the scores over all SAIs.Then metric score for the dataset is determined by averagingthe scores over the M scenes.",
  ", EDSR, RCAN and other five recent LF im-age SR methods: resLF, LFSSR , LF-ATO, LF-InterNet, and MEG Net": "Quantitative Results.As shown in , com-pared with other models with larger parameters, our modelis very lightweight and has achieved competitive results.Specifically, the parameters of our model are the smallest,our model is only 25.35% of the parameters of MEG-Netmodel, but it has achieved a better average PSNR value,which shows the lightweight characteristics of our model.In addition, our model has achieved remarkable results onEPFL and INRIA datasets. Qualitative Results. As shown in , regarding qual-itative performance, the propose LGFN has proved that ithas ability to produce trustworthy details and sharp struc-tures. For SISR methods, VDSR, EDSR and RCAN tendsto produce artifacts, and the restored texture details are notclear enough.For LFSR methods, the proposed LGFNhas ability to discriminate more dense details. Specifically,in figure ISO Chart, the figure recovered by our model isclearer than other figures, and there are fewer artifacts. Inthe figure Perforated Metal 3, the graph restored by ourmodel has more material texture. . Our team achieved second place on the leader board (last three rows) in the NTIRE-2024 Track 2 Fidelity & Efficiency test dataset,with quantitative results of 30.05 dB PSNR (average) and 0.924 SSIM (average).",
  "Methods#ParamsLytroSyntheticAverage": "Bicubic25.11 / 0.840426.46 / 0.835225.79 / 0.8378VDSR 0.67 M27.05 / 0.888827.94 / 0.870327.49 / 0.8795EDSR 38.89 M27.54 / 0.898128.21 / 0.875727.87 / 0.8869RCAN 15.36 M27.61 / 0.900128.31 / 0.877327.96 / 0.8887resLF 8.65 M28.66 / 0.926029.25 / 0.896828.95 / 0.9114LFSSR 1.77 M29.03 / 0.933729.40 / 0.900829.21 / 0.9173LF-ATO 1.36 M29.09 / 0.935429.40 / 0.901229.24 / 0.9183LF-InterNet 5.48 M29.23 / 0.936929.45 / 0.902829.34 / 0.9198MEG-Net 1.78 M29.20 / 0.936929.54 / 0.903629.37 / 0.9203",
  ". Ablation Study": "In this section, we further prove the effectiveness of severalcore parts of the proposed LGFN model through ablationexperiments.1) Connection mode of ECAM and ESAM modules.The connection modes of ECAM and ESAM are classi-fied into cascade connection and parallel connection. Inorder to verify which connection mode is more effective,we design two models: cascade and parallel, and their cor-responding models are LGFN-C and LGFN-P respectively,where LGFN-C is the NTIRE2024 LF image SR competi-tion model. As shown in , the LGFN-P is better thanLGFN-C. The main model of this paper is LGFN-P.2) LGFN w/o DGCE. The DGCE module is used toextract local feature. To demonstrate the effectiveness ofthe DGCE module, we remove this module and use paral-lel ECAM and ESAM modules. As shown in , thePSNR value is decreased dramatically from 31.8076 dB to28.7796 dB for 4x SR without DGCE module, and the dropvalue is 3.028dB. Experiment shows that DGCE module iseffective in feature extraction.3) LGFN w/o ESAM. The ESAM module is used to ex-tract global LF image spatial feature. To demonstrate theeffectiveness of the ESAM module, we remove this module.As shown in , the PSNR value is decreased module,the drop value is 0.1081dB.4) LGFN w/o ECAM. The ECAM module is used toextract channel feature. To demonstrate the effectiveness ofthe ECAM module, we remove this module. As shown in, and the PSNR value is decreased from 31.8076 dBto 31.7804 dB for 4x SR without ECAM module, the dropvalue is 0.0272dB.5) LGFN w/o ECAM and ESAM. The ESAM andECAM module are used to extract global feature.Todemonstrate the effectiveness of the ECAM and ESAM modules, we remove these modules. As shown in ,the PSNR value is decreased from 31.8076 dB to 31.6462dB for 4x SR without them, and the drop value is 0.1614dB,which proves the effectiveness of attention module.",
  ". NTIRE 2024 LFSR Challenge Results": "The test set of NTIRE2024 LFSR challenge including 16synthetic LFs and 16 real-world LFs captured by Lytro cam-era.As shown in , we proposed a model whichranked the second place in the Track 2 Fidelity & Effi-ciency of NTIRE2024 Light Field Super Resolution Chal-lenge with 30.05dB PSNR on the LFSR test dataset.",
  ". Conclusion and Feature Work": "In this paper, we investigated the task of lightweight LFimage SR and proposed a lightweight LF image SR modelnamed LGFN based on the local similarity and global dis-parity of SAIs.As a lightweight model, we proposed afeature modulation-based CNN module to extract local fea-tures efficiently.Besides, we designed an efficient spa-tial attention module which uses decomposable large-kernelconvolution to enlarge the receptive field and an efficientchannel attention module to extract the global features ofthe LF image. By learning local and global features, ourlightweight model has achieved competitive results andranked the second place in the Track 2 Fidelity & Efficiencyof NTIRE2024 Light Field Super Resolution Challenge andthe seventh place in the Track 1 Fidelity.In our future work, we will adopt model compressiontechniques, such as knowledge distillation, pruning, andmodel quantization, to further lighten our model and en-hance its effectiveness.AcknowledgmentsThis work was supported in part by the National NatureScience Foundation of China under Grant No.61901117,",
  "No.62301161, A21EKYN00884B03, in part by the Nat-ural Science Foundation of Fujian Province under GrantNo.2023J01083": "Vaibhav Vaish, Bennett Wilburn, Neel Joshi, and MarcLevoy. Using plane+ parallax for calibrating dense cameraarrays. In Proceedings of the 2004 IEEE Computer Soci-ety Conference on Computer Vision and Pattern Recognition,2004. CVPR 2004., volume 1, pages II. IEEE, 2004. 1 Yingqian Wang, Jungang Yang, Yulan Guo, Chao Xiao, andWei An. Selective light field refocusing for camera arrays us-ing bokeh rendering and superresolution. IEEE Signal Pro-cessing Letters, 26(1):204208, 2018. 1 Changha Shin, Hae-Gon Jeon, Youngjin Yoon, In So Kweon,and Seon Joo Kim. Epinet: A fully-convolutional neural net-work using epipolar geometry for depth from light field im-ages. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 47484757, 2018. 1 Yingqian Wang, Longguang Wang, Zhengyu Liang, JungangYang, Wei An, and Yulan Guo. Occlusion-aware cost con-structor for light field depth estimation. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1980919818, 2022. Wentao Chao, Xuechun Wang, Yingqian Wang, GuanghuiWang, and Fuqing Duan. Learning sub-pixel disparity dis-tribution for light field depth estimation. IEEE Transactionson Computational Imaging, 9:11261138, 2023. 1 Ryan S Overbeck, Daniel Erickson, Daniel Evangelakos,Matt Pharr, and Paul Debevec. A system for acquiring, pro-cessing, and rendering panoramic light field stills for virtualreality. ACM Transactions on Graphics (TOG), 37(6):115,2018. 1",
  "Jingyi Yu. A light-field journey to virtual reality. IEEE Mul-tiMedia, 24(2):104112, 2017. 1": "Gaochang Wu, Yebin Liu, Lu Fang, and Tianyou Chai. Re-visiting light field rendering with deep anti-aliasing neuralnetwork. IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 44(9):54305444, 2021. 1 Vincent Sitzmann, Semon Rezchikov, Bill Freeman, JoshTenenbaum, and Fredo Durand.Light field networks:Neural scene representations with single-evaluation render-ing. Advances in Neural Information Processing Systems,34:1931319325, 2021. Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Men-glei Chai, Yun Fu, and Sergey Tulyakov.R2l: Distillingneural radiance field to neural light field for efficient novelview synthesis. In European Conference on Computer Vi-sion, pages 612629. Springer, 2022. Benjamin Attal, Jia-Bin Huang, Michael Zollhofer, JohannesKopf, and Changil Kim. Learning neural light fields withray-space embedding. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1981919829, 2022. 1",
  "Mattia Rossi and Pascal Frossard. Graph-based light fieldsuper-resolution. In 2017 IEEE 19th International Workshopon Multimedia Signal Processing (MMSP), pages 16. IEEE,2017. 3": "Martin Alain and Aljosa Smolic. Light field denoising bysparse 5d transform domain collaborative filtering. In 2017IEEE 19th International Workshop on Multimedia SignalProcessing (MMSP), pages 16. IEEE, 2017. 1, 3 Nan Meng, Hayden K-H So, Xing Sun, and Edmund Y Lam.High-dimensional dense residual convolutional neural net-work for light field reconstruction.IEEE transactions onpattern analysis and machine intelligence, 43(3):873886,2019. 1, 3 Youngjin Yoon, Hae-Gon Jeon, Donggeun Yoo, Joon-YoungLee, and In So Kweon. Learning a deep convolutional net-work for light-field image super-resolution. In Proceedingsof the IEEE international conference on computer visionworkshops, pages 2432, 2015. 1, 3 Yunlong Wang, Fei Liu, Kunbo Zhang, Guangqi Hou,Zhenan Sun, and Tieniu Tan. Lfnet: A novel bidirectionalrecurrent convolutional neural network for light-field imagesuper-resolution. IEEE Transactions on Image Processing,27(9):42744286, 2018. 2, 3 Shuo Zhang, Youfang Lin, and Hao Sheng. Residual net-works for light field image super-resolution. In Proceedingsof the IEEE/CVF conference on computer vision and patternrecognition, pages 1104611055, 2019. 3, 5, 7, 8 Zhen Cheng, Zhiwei Xiong, and Dong Liu. Light field super-resolution by jointly exploiting internal and external similar-ities. IEEE Transactions on Circuits and Systems for VideoTechnology, 30(8):26042616, 2019. 3 Yingqian Wang, Jungang Yang, Longguang Wang, XinyiYing, Tianhao Wu, Wei An, and Yulan Guo. Light field im-age super-resolution using deformable convolution.IEEETransactions on Image Processing, 30:10571071, 2020. 3,5, 6",
  "Zhengyu Liang, Yingqian Wang, Longguang Wang, JungangYang, and Shilin Zhou. Light field image super-resolutionwith transformers. IEEE Signal Processing Letters, 29:563567, 2022. 2, 3": "Zhengyu Liang, Yingqian Wang, Longguang Wang, Jun-gang Yang, Shilin Zhou, and Yulan Guo.Learning non-local spatial-angular correlation for light field image super-resolution. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 1237612386, 2023.2, 3 Kai Jin, Angulia Yang, Zeqiang Wei, Sha Guo, Mingzhi Gao,and Xiuzhuang Zhou. Distgepit: Enhanced disparity learn-ing for light field image super-resolution. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 13731383, 2023. 2, 3",
  "Yan Yuan, Ziqi Cao, and Lijuan Su. Light-field image super-resolution using a combined deep cnn based on epi. IEEESignal Processing Letters, 25(9):13591363, 2018. 3, 7": "Zhen Cheng, Zhiwei Xiong, Chang Chen, Dong Liu, andZheng-Jun Zha.Light field super-resolution with zero-shot learning. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 1001010019, 2021. 3 Zeyu Xiao, Ruisheng Gao, Yutong Liu, Yueyi Zhang,and Zhiwei Xiong.Toward real-world light field super-resolution.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 34073417, 2023. 3 Yingqian Wang, Zhengyu Liang, Longguang Wang, JungangYang, Wei An, and Yulan Guo. Real-world light field imagesuper-resolution via degradation modulation. IEEE Transac-tions on Neural Networks and Learning Systems, 2024. 3",
  "Vaibhav Vaish and Andrew Adams. The (new) stanford lightfield archive. Computer Graphics Laboratory, Stanford Uni-versity, 6(7):3, 2008. 6": "Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurateimage super-resolution using very deep convolutional net-works. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 16461654, 2016. 7, 8 Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, andKyoung Mu Lee. Enhanced deep residual networks for singleimage super-resolution. In Proceedings of the IEEE confer-ence on computer vision and pattern recognition workshops,pages 136144, 2017. 7, 8 Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, BinengZhong, and Yun Fu.Image super-resolution using verydeep residual channel attention networks. In Proceedings ofthe European conference on computer vision (ECCV), pages286301, 2018. 7, 8 Jing Jin, Junhui Hou, Jie Chen, and Sam Kwong.Lightfield spatial super-resolution via deep combinatorial geom-etry embedding and structural consistency regularization. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 22602269, 2020. 7, 8 Yingqian Wang, Zhengyu Liang, Qianyu Chen, LongguangWang, Jungang Yang, Radu Timofte, Yulan Guo, et al. Ntire2024 challenge on light field image super-resolution: Meth-ods and results. In CVPRW, 2024. 7, 8"
}