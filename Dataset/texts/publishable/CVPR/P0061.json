{
  "Duo Su1,5,6,Junjie Hou2,5,6,Weizhi Gao3Yingjie Tian4,5,6,7,Bowen Tang8": "1School of Computer Science and Technology, UCAS2Sino-Danish College, UCAS3Department of Computer Science, NCSU4School of Economics and Management, UCAS5Research Center on Fictitious Economy and Data Science, CAS6Key Laboratory of Big Data Mining and Knowledge Management, CAS7MOE Social Science Laboratory of Digital Economic Forecasts and Policy Simulation, UCAS8Institute of Computing Technology, CAS",
  "Abstract": "Dataset distillation offers a lightweight synthetic datasetfor fast network training with promising test accuracy. Toimitate the performance of the original dataset, most ap-proaches employ bi-level optimization and the distillationspace relies on the matching architecture.Nevertheless,these approaches either suffer significant computationalcosts on large-scale datasets or experience performancedecline on cross-architectures. We advocate for designingan economical dataset distillation framework that isindependent of the matching architectures. With empiricalobservations, we argue that constraining the consistencyof the real and synthetic image spaces will enhance thecross-architecture generalization.Motivated by this, weintroduce Dataset Distillation via Disentangled DiffusionModel (D4M), an efficient framework for dataset distilla-tion. Compared to architecture-dependent methods, D4Memploys latent diffusion model to guarantee consistencyand incorporates label information into category proto-types. The distilled datasets are versatile, eliminating theneed for repeated generation of distinct datasets for variousarchitectures. Through comprehensive experiments, D4Mdemonstrates superior performance and robust generaliza-tion, surpassing the SOTA methods across most aspects.",
  "Equal contribution. Corresponding author": "\u0001\u0002\u0004\u0005\u0006\u0007\b \u000e \u0010\u0011\u0007\u0012\b \u0006\u0013 \u0014\u0015 \u0016 \u0012\u0007 \u0017\u0006 \u0004\u0017\u0018\u0007\u0019\u0011\u001a \u001b \u0002 !\u0011\u001b \u000e \u0010\u0011\u0007\u0012\b \u0006\u0013 \"\u0002 \u0015\u0011 \u0006 \u0006\u0013 \u000e \u0010\u0011\u0007\u0012\b \u0006\u0013 \u0010\u0017\u0016 \u001b \u0010\u0017\u0016 \u001b #$%&()%*$ +$,$-%&./,01$023()%*$ 4-3120$305-$6.7$32823./,01$023()%*$ 9:;<=>?@?AB@C> \u0014\u0015 \u0016 \u0012\u0007 \u0017\u0006 \u0004\u0017\u0018\u0007\u0019\u0011\u001a \u001b\u0010\u0017\u0016 \u001b \u0010\u0017\u0016 \u001b \u0014\u0015 \u0016 \u0012\u0007 \u0017\u0006 \u0014\u0015 \u0016 \u0012\u0007 \u0017\u0006\u0010\u0017\u0016 \u001b \u0010\u0017\u0016 \u001b BDE@;@;FAB@C>9:;<=>?@?AB@C> G%0312,*H-I3$JJ \u0010\u0017\u0016 \u001b \u0010\u0017\u0016 \u001b K%L$&M$N0 O . Comparison of various matching strategies in datasetdistillation. (a) The bi-level optimization implements data match-ing at synthesis time. (b) Dual-Time Matching strategy decouplesthe bi-level optimization process into synthesis time and trainingtime to save computational overhead.(c) D4M utilizes multi-modal features (image and texts) to synthesize high-quality im-ages. D4M does not require matching process at Synthesis-Time. challenge? From the perspective of dataset, recent researchextends the coreset selection to distillation tech-niques aimed at reducing dataset scales. Dataset Distilla-tion (DD) aims to synthesize a small dataset S from theoriginal large-scale dataset T , where |S| |T |. The infor-mation in T is condensed into a small dataset through DD.Initially, the DD framework uses the bi-level optimizationto generate datasets where the inner loop updates the net-work used for testing the classification performance and theouter loop synthesizes images according to matching strate-gies, such as gradient , distribution ortrajectory .",
  "arXiv:2407.15138v1 [cs.CV] 21 Jul 2024": "Unfortunately, the existing solutions of DD mainly fo-cus on small and simple datasets, such as CIFAR andMNIST . When it comes to large-scale and high-resolution datasets such as ImageNet , there exists un-affordable computational requirements and reduced perfor-mance. Another challenge in DD is the cross-architecturegeneralization. Previous methods conduct data matchingwithin a fixed discriminative architecture, which makes theoutput space biased from the original image space.Asdemonstrated in , this kind of dataset may be insight-ful for the networks but suffers from the lack of semanticinformation for humankind. Furthermore, the dataset hasto be distilled from scratch again and again to adapt to theemerging network architectures. Obviously, these limita-tions constrain the scientific value and practical utility ofthe current solutions. In this paper, we argue that an idealDD method should meet the following properties.",
  ". Visualizations of previous DD methods. Synthesis-TimeMatching sacrifices part of the visual semantic expression in orderto imitate the performance of the original dataset": "1) The synthesis process should not depend on a spe-cific network architecture. Typically, a fixed architectureis required for data matching, which leads to low cross-architecture generalization performance because the outputspace is constrained by the architecture. This problem arisesonce the matching process occurs in the synthesis time asshown in (a) and (b). Some work leverages a modelpool instead of an individual matching model to alleviatethis issue but makes the network hard to optimize .When the distillation process is architecture-free, there isno need to distill datasets for different architectures repeat-edly. In addition, constraining the consistency of input andoutput spaces will make the distilled images more realistic.GlaD seems to be a solution where the images are syn-thesized via Generative Adversarial Networks. However,the synthetic images are still matched by the inner loop.2) The method is capable of distilling datasets of varioussizes and resolutions with limited computational resources.As illustrated in (a), most DD solutions use bi-leveloptimization during synthesis time. While the large-scaledatasets are unable to perform a number of unrolled it-erations on such a nested loop system.Some works at-tempt to distill the ImageNet-1K but yield low testing accu-racy . A more effective method is depicted in (b):the bi-level optimization is decoupled into synthesis timeand training time . However, the Dual-Time Matching (DTM) strategy leads to information loss at each stage, pos-ing challenges for distillation on small datasets instead.Inspired by these insights, we propose the DatasetDistillation via Disentangled Diffusion Model (D4M), anefficient approach designed for DD across varying sizes andresolutions as depicted in (c). In D4M, the Synthesis-Time Matching (STM) is superseded by Training-TimeMatching (TTM) which facilitates the fast distillation oflarge-scale datasets with constrained computational re-sources. Furthermore, D4M alleviates the architectural de-pendency and improves the cross-architecture generaliza-tion performance of the distilled dataset. As the generativemodel, Diffusion Models ensure the consistency betweeninput and output spaces, and its synthesis process does notrely on any specific matching architecture.To mitigatethe information loss due to insufficient data matching, theconditioning mechanism in Latent Diffusion Model (LDM)consistently infuses the semantic information of labels intothe synthetic data during the denoising process. The syn-thesis process of D4M solely depends on the prototypes ex-tracted from the original data, with synthesis speed scalinglinearly with the size of datasets. Moreover, the syntheticimages exhibit realism at a high resolution of 512 512.Our pivotal contributions are summarized as follows: To the best of our knowledge, this is the first work thatovercomes the pronounced dependency on specific archi-tectures inherent in traditional DD frameworks. We in-troduce the TTM strategy, which paves the way for thegeneration of a curated and versatile distilled dataset. We propose D4M that integrates the diffusion model intoDD task for the first time. By leveraging label texts andthe learned prototypes, we construct a multi-modal DDmodel that simultaneously enhances distillation efficiencyand model performance. The method realizes the attainment of resolutions up to512512 that exhibit high-fidelity and robust adaptabilityin the realm of DD. This improvement is evidenced acrossa spectrum of datasets, extending from the ImageNet-1Kto CIFAR-10/100.",
  "Classes": ". Pipeline of Dataset Distillation via Disentangled Diffusion Model (D4M). Rather than using the embedded features directly,D4M disentangles feature extraction from image generation in diffusion models through prototype learning. Unlike optimizing the performance on the DD explicitly,data matching encourages the consistency between the samenetwork architecture trained by distilled and real dataset.Matching the gradients generated by the networks is a reli-able surrogate task . Matching Training Tra-jectory (MTT) is then proposed to solve the issue thaterrors are accumulated during validation in gradient match-ing. TESLA reduced the complexity of gradients calcu-lating with constant memory, allowing DD to be achieved inImageNet for the first time. Besides, distribution matchingoptimizes the distance between the two distributions, suchas MMD and CAFE .The aforementioned methods only implement variousmatching strategies at synthesis time. SRe2L arguesthat decoupling the bi-level optimization into Squeeze, Re-cover, and Relabel leads to a good performance on large-scale datasets.Inspired by this, we summarize previousworks into STM and DTM. D4M implements the TTM withthe help of soft labels, which is considered a feature distri-bution matching approach.",
  ". Diffusion Models": "The Diffusion Model has demonstrated remarkable capa-bilities within the generative models.Given samples xobserved from a target distribution, the goal of generativemodels is approximating the true distribution P(x), en-abling the generation of novel samples from it. Denois-ing Diffusion Probabilistic Models (DDPM) aims tolearn a reverse process of a fixed Markov Chain for gen-erating images. However, DDPM is expensive to optimizeand evaluate in the original pixel space.Latent Diffusion Model (LDM) , a recent state-of-the-art diffusion model, addresses this by abstracting high-frequency, imperceptible details into a compact latent space, thereby streamlining both training and inference.LDMhas been applied in image editing , video process-ing , audio generation and 3D model recon-struction . Notably, the proficiency of LDMin abstracting and generating images within the latent spaceexactly resonates with the foundational tenets of DD.",
  ". Preliminaries on Diffusion Models": "A pivotal step in DD is the generation of the distilled im-ages.Distinct from the data-matching approaches, ourmethod harnesses the prior knowledge embedded in the pre-trained generative models, offering a high-quality initializa-tion for TTM. Recently, diffusion models have emerged asSOTA in generative models . As aforementioned,the synthesis process of the diffusion model does not relyon any specific matching architecture, ensuring the consis-tency between input and output spaces. For a sequence ofdenoising autoencoders , the training objective of Denois-ing Diffusion Probability Model (DDPM) is defined as",
  "LDM = Ex,N (0,1),t (xt, t)22,(1)": "with the timestamp t uniformly sampled from {1, . . . , T}.Although the DDPM does not cater to our goal of synthe-sizing images within the condensed features, we turn ourattention to the LDM .LDM effectively compresses the working space from theoriginal pixel space x to a more compact latent space z.Such a transition is close to our intent of encapsulating im-ages into condensed features. LDM constructs an optimizedlow-dimensional latent space by training a perceptual com-pression model composed of the encoder (E) and decoder",
  ". Disentangled Diffusion Model": "The existing diffusion methods are capable of generatinghigh-quality images directly from the given images andprompts. However, it is imperative for the DD model toaggregate the given images into a few condensed featuresbefore synthesis. The images in the original dataset en-capsulate a spectrum of information from low-level texturepatterns to high-level semantic information, along with po-tential redundancies.Since the diffusion models do nothave the capability of aggregating this information amongimages, it is necessary to extract the salient feature repre-sentative of each category before employing the generativemodel. Consequently, it is essential to disentangle the dif-fusion models.Employing prototypes in standard classification tasksoffers the benefit of addressing the open-world recogni-tion challenge, thereby enhancing the robustness of mod-els . Therefore, initializing the input of the dif-fusion model with prototypes not only reduces data redun-dancy but also elevates the quality of the distilled dataset.As illustrated in , we leverage the pre-trained autoen-coder E inherent in the LDM to extract feature represen-tations from original images. Subsequently, we perform aclustering algorithm to calculate the cluster centers as pro-totypes for each category. Given the considerable size ofthe original dataset, we adopt the Mini-Batch k-Means to mitigate the memory overhead of large-scale clustering.This approach iteratively optimizes a mini-batch of samplesin each step, accelerating the clustering process with a min-imal compromise in accuracy.Specifically, the clustering algorithm consists of two pri-mary steps: assignment z",
  "Output: S: Distilled images": "Moreover, LDM is capable of modeling the conditionaldistribution, enabling DD tasks to incorporate the label in-formation into synthetic images. In Eq. (2), LDM intro-duces a domain-specific encoder to map the textual labels(prompts) into the feature space. This mapping is seam-lessly integrated into the U-Net architecture (Ut) through across-attention layer, facilitating the fusion of multi-modalfeatures. For each prototype zc and its corresponding labelL, the synthesis process is formulated as",
  ". Training-Time Matching": "Since eliminates the necessity of matching with a specificarchitecture, separating data matching from the synthesisprocess reduces the computational overhead on large-scaledatasets and addresses the cross-architecture issue inher-ent in the STM strategy. However, based on previous re-search and preliminary experiments, we find thattraining large-scale distilled datasets with hard labels isprone to low testing accuracy. To address this, we intro-duce the TTM strategy, which is considered a distributionmatching approach.",
  ". Visualization results within one category. D4M (top)provides richer semantic information than SRe2L": "TTM refers to training on distilled datasets with softlabels.Label softening is widely adapted in distillationtasks . Since D4M infuses the label featuresinto the synthetic data, it is natural to use the soft label dur-ing TTM. We employ soft label to align the distribution ofstudent prediction S(x) with teacher network T:",
  "student = arg minLKL(T(x), S(x))(7)": "where T(x)/S(x) is the teacher/student prediction for thedistilled image x and LKL represents the KL divergence.The output of the teacher network, also known as soft pre-diction or soft label, encapsulates richer semantic informa-tion compared to hard labels. Matching with the soft labelsduring training will enhance the robustness and generaliza-tion capability of the trained model . For a fair com-parison, we use the soft label storage method similar to theFKD method, which generates soft labels and conductsmatching at each training epoch:",
  ". Setting and Evaluation": "We evaluate the performance of D4M across variousdatasets and networks. All models employed for ImageNet-1K and Tiny-ImageNet are sourced from the PyTorch of-ficial model repository, while the ConvNet utilized forCIFAR-10/100 is based on the architecture proposed by Gi-daris et al. . Performance validation was carried outusing PyTorch on NVIDIA V100 GPUs. Detailed trainingand validation hyperparameters are available in the supple-mentary material.",
  ". Dataset Distillation Results": "In our comparative analysis, we evaluate the D4M againsta range of techniques, encompassing both meta-learningand data-matching strategies. For small datasets, our com-parison included two meta-learning methods:KIP and FRePO , alongside four data-matching techniques:DSA , CAFE , TESLA , and SRe2L . Inthe context of large-scale datasets, our focus shifted to a de-tailed comparison between TESLA and SRe2L.CIFAR-10 and CIFAR-100 For small dataset distilla-tion, the STM strategy outperforms when the number ofcategories and IPC (Image Per Class) are limited. How-ever, as the category increases, the TTM strategy becomesmore effective. This shift is attributed to the fact that the op-timal solution derived from STM fails to ensure the conver-gence of the network training with large category numbers,thereby capping the testing performance. As evidenced in",
  "SRe2L50.853.554.2D4M51.454.855.3D4M-G53.354.954.5": ". Top-1 Accuracy on large-scale datasets. SRe2L and our D4M employ ResNet18 as the teacher model to generatethe soft label while TESLA uses the ConvNetD4. All standarddeviations in this table are < 1. : The results of ImageNet-1Kcome from the official PyTorch websites. : The results of Tiny-ImageNet come from the model trained from scratch with the of-ficial PyTorch code. Tab. 1, when applied to CIFAR-100, D4M attains a Top-1accuracy of 45.0% with merely IPC-10. This performancesurpasses that of FRepo and TESLA by 2.5% and 3.3%.ImageNet-1K and Tiny-ImageNet The TTM strategydemonstrates remarkable efficacy in large-scale DD tasks aspresented in Tab. 2. The effectiveness stems from its abil-ity to improve the quality of the synthetic data rather thanimitate the performance of the original data. Consequently,it facilitates the processing of large-scale datasets with re-duced computational complexity and memory demands. Interms of accuracy, the proposed D4M sets new benchmarks,achieving 66.5% and 51.0% with IPC-100 on ImageNet-1Kand Tiny-ImageNet. Notably, it replicates the full dataset",
  ". Comparison of Top-1 Accuracy on different match-ing strategy. We use the R18 as the distribution matching archi-tecture. All methods are evaluated with IPC-10": "performance with 81.2% and 81.9%, respectively. More-over, our approach significantly surpasses the leading data-matching method, SRe2L, across both datasets. This supe-riority is attributed to the integration of multi-modal fusionembedding in D4M.Benefit to the architecture-free synthesis process, thedatasets distilled by D4M exhibit versatility. To substan-tiate this characteristic, we extract 200 categories from thedistilled ImageNet as the distilled Tiny-ImageNet in accor-dance with the predefined mapping . The experimentaloutcomes of D4M-G in Tab. 2 demonstrate that our methodnot only manifests a pronounced distillation effect but alsoretains the applicability inherent to the original dataset.",
  ". Matching Strategy Analysis": "As mentioned in Sec. 2, the DD task often uses the STMstrategy to generate images. In order to validate the supe-riority of TTM strategy, we conduct the comparative exper-iments listed in Tab. 3. We execute the synthesis processthrough BN distribution matching on images distilled viaD4M, resulting in distribution-matched synthetic images.It is evident that the test performance with STM failedregardless of the chosen teacher network. The images dis- IPC Top-1 Acc. (%) Teacher: ResNet-18 ResNet-18ResNet-50ResNet-101 IPC Top-1 Acc. (%) Teacher: ResNet-50 ResNet-18ResNet-50ResNet-101 IPC Top-1 Acc. (%) Teacher: ResNet-101 ResNet-18ResNet-50ResNet-101",
  ". Comparison of Top-1 Accuracy on different initial-ization of diffusion process. PT is the abbreviation of Prototype.All methods are evaluated with IPC-10": "tilled via D4M encapsulate not only the salient features ofthe original prototypes but also the text information of cat-egory labels. Therefore, the network solely trained with theoriginal images proves inadequate for effectively managingsuch fused multi-modal features. Should the fused featuresbe aligned with these networks, it would result in the dis-ruption of the fused information, thereby diminishing theoverall accuracy. It is worth noting that D4M potentiallyoffers high-quality initialization for STM, as it synthesizesimages with higher testing accuracy compared to those de-rived from random white noise initialization.",
  ". Prototype Analysis": "To ascertain the critical role of prototypes in D4M, we con-duct an ablation study on the diffusion process with randominitialization and prototype initialization. The results listedin Tab. 4 demonstrate that the incorporation of a learnedprototype markedly enhances the effectiveness of D4M.To showcase the merits of the prototype intuitively, weemploy ResNet-18 for feature extraction from the distilleddataset, followed by t-SNE for dimensionality reduction.The visualization results () reveal that the data synthe-sized via D4M demonstrates enhanced inter-class discrimi-nation and intra-class consistency. MTT SRe2L AlligatorChainSeashore D4M",
  ". Teacher-Student Network Analysis": "We studied the performance of different teacher-studentmodels with D4M and the experimental results are shownin . Under the same teacher network, the accuracyof ResNet-18, ResNet-50, and ResNet-101 increases grad-ually. When IPC is small (such as 10 and 50), the studentnetwork trained with an enhanced teacher is prone to over-fitting, resulting in reduced testing accuracy. As IPC in-creases, the large network shows stronger learning abilityand the Top-1 accuracy improves. We further compare theperformance of the distilled ImageNet on different teacher-student pairs, including CNNs and ViTs (Tab. 5).As astudent network, the ViT-based networks assimilate the in-ductive bias inherent in CNN-based teachers, leveraging itsglobal attention mechanism to attain the best Top-1 accu-racy. Conversely, as a teacher network, ViT does not havesuch an inductive bias characteristic, yielding suboptimalresults on their student networks. Nevertheless, ViT-basedstudents consistently achieve superior Top-1 accuracy.",
  ". Top-1 Accuracy on ImageNet-1K with various teacher-student architectures. ViT-based students show powerful learningability with IPC-50": "image. Figures 4 and 5 exemplify the superior image qual-ity achieved by D4M in comparison to its counterparts. It isevident that the D4M method not only guarantees the highresolution of the distilled image and preserves the integrityof semantic information but also ensures the richness of fea-tures within the same category. More visualizations andanalysis can be found in supplementary material.",
  ". Distillation Cost Analysis": "We conduct the analysis of GPU memory consumptionacross various DD methods, with the corresponding resultspresented in Tab. 6. Notably, the architecture-free natureof D4M during synthesis ensures the fixed time and GPUmemory costs. When considering STM and DTM, we ob-serve an increase in both time and GPU memory usagewith the enlargement of the matching architecture. For in-stance, the peak GPU memory utilization for SRe2L in therecovery of a 6464 image on ConvNet is 4.2 GB, whereason ResNet-50, it reaches a substantial 33.8 GB. Similarly,when synthesizing a 6464 image on ConvNet, MTT de-mands a peak GPU memory of 48.9 GB. Furthermore, thenumber of iteration steps impacts the generation time for asingle image in data matching. With the increased iterationsteps, the time cost for SRe2L to recover a 224224 imageon ResNet-50 gradually rises from 1.31s to 10.48s. Notably,",
  ". Conclusion": "We introduce D4M, a novel and efficient dataset distillationframework leveraging the TTM strategy. For the first time,D4M addresses the cross-architecture generalization issueby integrating the principles of diffusion models with proto-type learning. The distilled dataset not only boasts realisticand high-resolution images with limited resources but alsoexhibits a versatility comparable to that of the full dataset.D4M demonstrates outstanding performance compared toother dataset distillation methods, particularly when appliedto large-scale datasets such as ImageNet-1K. Last but notleast, rethinking the relationship between generative mod-els and dataset distillation offers fresh perspectives, pavingthe way for the community to develop more efficient datasetdistillation methods in future endeavors.Limitation and future works.In the situation of ex-treme distillation (IPC-1/10), we observe a significant per-formance degradation. Our future work will concentrate onrefining the distillation process for this challenging scenarioand try to distill more real-world multi-modal datasets.Acknowledgement. This work is supported by the NationalNatural Science Foundation of China (No.12071458).",
  "Zalan Borsos, Mojmir Mutny, and Andreas Krause. Coresetsvia bilevel optimization for continual learning and stream-ing. Advances in neural information processing systems, 33:1487914890, 2020. 1": "George Cazenavette, Tongzhou Wang, Antonio Torralba,Alexei A Efros, and Jun-Yan Zhu.Dataset distillationby matching training trajectories.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 47504759, 2022. 1, 2, 3, 4, 5, 8 George Cazenavette, Tongzhou Wang, Antonio Torralba,Alexei A Efros, and Jun-Yan Zhu.Generalizing datasetdistillation via deep generative prior.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 37393748, 2023. 2 Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, TaoChen, and Gang Yu. Executing your commands via motiondiffusion in latent space. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 1800018010, 2023. 3",
  "Yutian Chen, Max Welling, and Alex Smola. Super-samplesfrom kernel herding. arXiv preprint arXiv:1203.3472, 2012.1": "Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Scalingup dataset distillation to imagenet-1k with constant memory.In International Conference on Machine Learning, pages65656590. PMLR, 2023. 1, 2, 3, 4, 5, 6, 8 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE conference on computer vision andpattern recognition, pages 248255. Ieee, 2009. 2",
  "Zhiwei Deng and Olga Russakovsky. Remember the past:Distilling datasets into addressable memories for neural net-works. Advances in Neural Information Processing Systems,35:3439134404, 2022. 2": "Jiawei Du, Yidi Jiang, Vincent YF Tan, Joey Tianyi Zhou,and Haizhou Li.Minimizing the accumulated trajectoryerror to improve dataset distillation.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 37493758, 2023. 3 Patrick Esser,Johnathan Chiu,Parmida Atighehchian,Jonathan Granskog, and Anastasis Germanidis.Structureand content-guided video synthesis with diffusion models.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 73467356, 2023. 3",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-fusion probabilistic models. Advances in neural informationprocessing systems, 33:68406851, 2020. 3": "Yujin Jeong, Wonjeong Ryoo, Seunghyun Lee, Dabin Seo,Wonmin Byeon, Sangpil Kim, and Jinkyu Kim. The powerof sound (tpos): Audio reactive video generation with sta-ble diffusion. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 78227832, 2023. 3 Zixuan Jiang, Jiaqi Gu, Mingjie Liu, and David Z Pan. Delv-ing into effective gradient matching for dataset condensation.In 2023 IEEE International Conference on Omni-layer Intel-ligent Systems (COINS), pages 16. IEEE, 2023. 3 Seung Wook Kim, Bradley Brown, Kangxue Yin, KarstenKreis, Katja Schwarz, Daiqing Li, Robin Rombach, AntonioTorralba, and Sanja Fidler. Neuralfield-ldm: Scene genera-tion with hierarchical latent diffusion models. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 84968506, 2023. 3 Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, and MinhyukSung. Salad: Part-level latent diffusion for 3d shape gen-eration and manipulation. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 1444114451, 2023. 3",
  "Zhiqiang Shen and Eric Xing. A fast knowledge distillationframework for visual recognition. In European Conferenceon Computer Vision, pages 673690. Springer, 2022. 5": "Yu Takagi and Shinji Nishimoto. High-resolution image re-construction with latent diffusion models from human brainactivity.In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1445314463, 2023. 3 Mariya Toneva, Alessandro Sordoni, Remi Tachet desCombes,Adam Trischler,Yoshua Bengio,and Geof-frey J Gordon.An empirical study of example forget-ting during deep neural network learning.arXiv preprintarXiv:1812.05159, 2018. 1 Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang,Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, andYang You.Cafe: Learning to condense dataset by align-ing features. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1219612205, 2022. 1, 3, 5",
  "Han Xiao, Kashif Rasul, and Roland Vollgraf.Fashion-mnist: a novel image dataset for benchmarking machinelearning algorithms. arXiv preprint arXiv:1708.07747, 2017.2": "Hong-Ming Yang, Xu-Yao Zhang, Fei Yin, and Cheng-Lin Liu.Robust classification with convolutional proto-type learning.In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 34743482,2018. 4 Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Run-sheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey ofmethods and applications. ACM Computing Surveys, 2022.3 Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. Agift from knowledge distillation: Fast optimization, networkminimization and transfer learning. In Proceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 41334141, 2017. 5",
  ". Experimental Settings": "In our experimental framework, we primarily concentrateon the parameters of the synthesis and the Training-TimeMatching (TTM) processes. For the synthesis phase, StableDiffusion (V1-5) serves as the core mechanism in LatentDiffusion Model implementation. Based on the insights ofSec. 2.1, we calibrate the strength and guidance scale pa-rameters at 0.7 and 8, respectively. During the prototypelearning, the Mini-Batch k-Means algorithm is employed,with an in-depth ablation study of cluster number variationspresented in Sec. 2.2. Furthermore, in scenarios where theIPC is less than 100, we adjust the cluster numbers to matchthe IPC. Within the TTM process, the comprehensive pa-rameter settings of student networks are provided in Tab. 8.",
  ". Sensitivity Analysis": "There are two hyper-parameters in the diffusion model withtext prompts, i.e. strength (0 < s < 1) and guidance scale(g > 1). Conceptually, the strength quantifies the extentof noise infusion into the latent features (prototypes). Thediffusion model predominantly disregards these features inscenarios where strength equals 1. Furthermore, an elevatedguidance scale fosters the generation of images that moreprecisely align with the text prompt. Based on the hyper-parameter tuning results in a and b, we suggestsetting strength = 0.7 and guidance scale = 8.",
  ". Number of Prototypes": "To ensure the feature diversity of the distilled dataset, mul-tiple prototypes are learned for each category in our exper-iments. We select 10 or 50 prototypes to generate distilledImageNet-1K datasets (IPC-100/200) respectively, i.e. syn-thesizing multiple images per prototype. These datasets arethen trained across three distinct ResNet architectures, withthe corresponding outcomes detailed in Tab. 7.",
  "(b) Sensitivity Analysis of guidance scale (strength = 0.7)": ". Sensitivity analysis of strength and guidance scale.Quantitative results are evaluated on ResNet. Furthermore, quali-tative results are presented to illustrate the variations correspond-ing to parameter adjustments. Given the marginal disparity observed between the ex-perimental results of the two groups, we conducted an in-dependent sample t-test. The alternative hypothesis is thatthe true difference in means is not equal to 0. According tothe p-value, at a significance threshold of 0.05, the perfor-mance variations of each group are not statistically signifi-cant, which means that the distilled datasets are not sensitiveto the number of prototypes.In addition, the t-SNE visualization results of D4Mon ImageNet-1K are displayed in .Except for afew outliers, the features extracted from the D4M distilledImageNet-1K dataset are compact and discriminative forboth different and similar categories.",
  ". Quantitative Analysis": "In the main text, we delve into the enhancement of input-output image space consistency constraints for addressingcross-architecture generalization challenges. This sectionpresents a direct comparative analysis of the image qual-ity yielded by D4M against the benchmark, as detailed inTab. 9.Firstly, we employ the Inception Score (IS) to assess theclarity p(y | x) of the synthetic images and the feature di-versity p(y) of the generative model G. The IS quantifies D4M: Different Categories hammerheadpapillon marmotcleaver mortarboardviaduct D4M: Similar Categories beaglewhippet weimaranercollie samoyedpug",
  "IS = exp (ExpGDKL(p(y | x)p(y))) .(9)": "Moreover, to demonstrate that the D4M enhances the con-sistency between synthetic and real images, we computethe Frechet Inception Distance (FID) and Kernel InceptionDistance (KID) metrics for these datasets. Empirical eval-uations demonstrate that D4M is capable of generating avariety of high-resolution images while maintaining consis-tency between the input and output image spaces."
}