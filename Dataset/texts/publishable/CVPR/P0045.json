{
  "Abstract": "We introduce ReMOVE, a novel reference-free metricfor assessing object erasure efficacy in diffusion-based im-age editing models post-generation. Unlike existing mea-sures such as LPIPS and CLIPScore, ReMOVE addressesthe challenge of evaluating inpainting without a referenceimage, common in practical scenarios. It effectively distin-guishes between object removal and replacement. This is akey issue in diffusion models due to stochastic nature of im-age generation. Traditional metrics fail to align with the in-tuitive definition of inpainting, which aims for (1) seamlessobject removal within masked regions (2) while preservingthe background continuity.ReMOVE not only correlateswith state-of-the-art metrics and aligns with human percep-tion but also captures the nuanced aspects of the inpaintingprocess, providing a finer-grained evaluation of the gener-ated outputs.",
  ". Introduction": "In the contemporary creative landscape, diffusion modelshave surged in popularity, driving innovation in visual con-tent generation . One of the primary applica-tions of diffusion models lies in their role in image editing,achieved through prompt-based user inputs, which stream-line access to complex editing functionalities in the cre-ative process . These models provide a robustmethodology for various tasks, including object replace-ment, position switching, and object erasure . Amongthese categories, object erasure is the task of inpainting amasked region with neighbouring pixels to seamlessly re-move undesired objects from an image.Image inpainting, a technique in computer vision akinto a digital paintbrush, serves to restore or complete im-ages by intelligently reconstructing missing or damaged ar-eas . This process involves filling in the gaps withdetails that seamlessly blend with the surrounding context.Moreover, inpainting extends to removing unwanted objects",
  ", which replaces it with another": "from a photo, such as a power line disrupting a scenic land-scape. By analyzing the surroundings, the technique canreconstruct the missing pixels, creating an image as if thepower line was never there.Over the years, there has been significant advancementsin image inpainting and its evaluation metrics. Thesemetrics typically fall into three categories: structure-based,saliency-based, and machine learning-based. However, themajority of these metrics are reference-dependent, requir-ing a ground truth inpainting, which is often very hard toobtain on a large scale for real images . This reliance onreference-based evaluation can pose challenges, highlight-ing the need for developing alternative approaches to assessthe quality and efficacy of inpainting methods.Zhang et al. show that traditional per-pixel met-rics like Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR) often employed in image enhance-ment, super-resolution are inadequate for evaluat-ing structured outputs such as images. This is because thesemetrics assume independence among pixels, failing to cap-ture perceptual changes accurately. A notable illustration isthe discrepancy between perceptual differences and minorchanges in MSE induced by blurring or noising . Thus,the authors proposed Learned Perceptual Image Patch Sim-ilarity (LPIPS) , obtained using feature distances in aneural network, outperforming traditional metrics while ac-",
  "arXiv:2409.00707v1 [cs.CV] 1 Sep 2024": ". Randomness in Object Inpainting using SD-Inpaint: Samples of diffusion-based image inpainting using SD-Inpaint generated across varying seeds. The object intended for inpainting is substituted with a different object rather than replacing it with thebackground. In some cases (column 6), the model replaces the object with background pixels as desired. counting for many nuances of human perception. Anothersuch metric is the CLIPScore , which uses multi-modaldeep features to quantify image-text similarity. LPIPS andCLIPScore have also been used to measure the quality ofinpainting, but both of them need access to the ground truthinpainted image . In the case of CLIPScore, the promptcan be obtained using a captioning model like BLIP on the ground truth. However, the reference-free version ofCLIPScore only looks for the removal of the intendedobject and does not distinguish between object removal andobject replacement, as illustrated in .",
  "In this work, we propose ReMOVE, a reference-free met-": "ric to access inpainting quality, with a focus on object era-sure using diffusion-based editing frameworks. This is pri-marily because these frameworks often introduce new ob-jects alongside the removal of existing ones, as shown in. The issue persists across other inpainting frame-works built on Stable Diffusion , while other com-mercial inpainters and inpainting models such asGLIDE demonstrate superior performance in address-ing this challenge, as seen in .To assess quality,we measure the distance between mean patchwise featuresof the masked and unmasked regions obtained from a Vi-sion Transformer (ViT) , pre-trained on a segmentation . Randomness in Object Inpainting with Other Methods: Samples of diffusion-based image inpainting using multiple methods.The object intended for inpainting is often substituted with a different object rather than replacing it with the background. task . We are inspired by Lugmayr et al. , whereinthey train a model to inpaint a masked region conditionedon the unmasked region. Next, we demonstrate the effi-cacy of our metric through experimentation using a largedataset of synthetically generated images. Our findings in-dicate a strong correlation between our metric and humanperception. Furthermore, we validate these results by test-ing ReMOVE on real-world inpainting data, obtaining con-sistent outcomes. To the best of our knowledge, there arecurrently no deep feature-based metrics available for assess-ing inpainting quality in a reference-free manner.",
  ". To our knowledge, this is the first reference-free metric": "to use deep features to compare between the inpaintedregions and the other regions. Deep features align betterwith human perception of quality, and can provide betterassessment. Furthermore, our reference free metric hasthe potential to resolve the impracticality of reference-based metrics due to the unavailability of the referenceimage in deletion. 2. We empirically validate that ReMOVE correlates wellwith SoTA reference-based metrics (Sec. 4), aligningwell with human preference. This is also supported by auser study (Sec. 5.1) that reinforces these findings.",
  ". Schematic Diagram of ReMOVE: The inpainter": "takes the original image and (optionally) object mask to produce an editedimage with the object deleted. Our metric requires only the edited image and object mask. After preprocessing using a bounding boxcrop (only in the crop-variant) and resizing, the image is tokenized into patches, and the encoder E obtains features for each patch.Simultaneously, the mask is resized and used to split patch embeddings into object and background embeddings. The mean featureembeddings are compared using a similarity measure to yield ReMOVE.",
  ". Related Work": "Image Editing with Diffusion Models: Diffusion modelsexcel in image editing using textual prompts (and additionalinputs such as masks). Notably, InstructPix2Pix fine-tunes a diffusion model using instruction prompts generatedwith LLMs for better textual alignment, but struggles withcomposability and spatial reasoning. Edit masks can allevi-ate this issue as shown in Refs. , which utilize editmasks to perform iterative editing of objects within the im-age. Ref. improves upon this by performing zero-shotmultiple object edits in a single pass. We refer the reader toRef. for a more comprehensive review of the domain. Object Deletion: Viewed as inpainting, deletion aims toremove an object from the image and reconstruct the back-ground faithfully. Generally, deletion is a challenging prob-lem naive inpainting models usually fail to understandthe prompt or replace the object with another object. Thismotivated Ref. to solve the problem by constructinga deletion-specific dataset from the GraphVQA dataset tofinetune inpainting models to deletion prompts. Ref. improves the performance by attaching guidance to the dif-fusion process. Metrics for Image Quality Assessment: Editing, inpaint-ing, and deletion methods have been generally evaluatedthrough image perceptual quality metrics. Most of thesemetrics are either fully reference-based or distribution-based, i.e., they require a reference image or a reference dis-tribution along with the output image to measure the qual-ity of the output. Frechet Inception Distance , assessthe quality of generated images by comparing its distri-bution to ImageNet. Structural Similarity Index Measure(SSIM) measures image degradation as a perceivedchange in structural information. Inspired by SSIM, propose a metric to measure the inpainting quality. SSIM-based measures may fail for images with large inpainting re-gions . Refs. introduce a reference-free saliency-based metric for inpainting. Other reference-based pixelsaliency inpainting metrics have also been introduced .Traditional pixel-level measures are inadequate to assess theimage quality. LPIPS uses deep features of a trainednetwork to assess image quality. CLIP Distance eval-uates object removal.Image regions from the inpaintedimage are extracted using a bounding box, and the CLIPsimilarity is measured between the inpainted region andthe original textual prompt. CLIP Accuracy utilizesthe CLIP model as a zero-shot classifier for semantic labelprediction low Top-5 accuracy indicates the object is re-moved.",
  ". Proposed Metric": "ReMOVE leverages feature extraction to assess inpaintingquality assessing visual saliency at a patch level ratherthan pixel level. Specifically, ReMOVE employs a ViT trained on an image segmentation task to extract fea-ture embeddings for the image patches. Average embed-dings are calculated for the masked regions and the un-masked regions, which are then compared to score howconsistent the inpainted region is with the background re-gions. Ideally, for deletion we would want no noticeablechange within the masked region compared to the back-ground region, i.e, the similarity score is high.Given an image x from image-space I Rwh3 (x I1), a foreground mask M, a ViT feature extractor E anda similarity measure S, the working of the metric can besummarized as follows:",
  ". Experimental Setup": "In this section, we outline the experiments conducted to val-idate the efficacy of ReMOVE as a reference-free metric forassessing inpainting quality. We first demonstrate its reli-ability through a simple experiment wherein we observe aconsistent trend between ReMOVE and perceptual similar-ity measured by LPIPS using the ground truth inpaintingsas a baseline. Thereafter, we test the metric on a real-worldinpainting scenario on the DEFACTO dataset .",
  "Dataset Generation": "We start by creating a comprehensive dataset for evaluatingthe effectiveness of ReMOVE. Firstly, we use a collection of4300 background images from Flikr comprising vari-ous landscapes that can roughly be categorized as mountain,sea, desert, beach, and island. Next, we randomly select 20masks from the PIE-Bench image editing dataset . Themasks are categorized as either coarse or fine, dependingon whether they exhibit a general blob shape or represent aspecific identifiable object, respectively. This distinction isillustrated in a.To generate inpainted images, we employ Stable Diffu-sion Inpaint (SD-Inpaint) using randomly selected im-ages and masks from the dataset defined above. These in-painted images utilize the input background images as theirground truth inpainting. Additionally, the inpainted imagesare generated with various seeds and an empty prompt ( )to introduce variability in the inpaintings.The quality of inpainting varied across instances, withSD-Inpaint occasionally producing satisfactory results, asshown in b, while at other times exhibiting sub-optimal performance. This variability in inpainting qual-ity results in a diverse photo-realistic dataset comprising200,000 images with varying degrees of inpainting qual-ity. Consequently, this dataset provides a robust foundationfor evaluating our framework.",
  "Empirical Validation": "To empirically validate ReMOVE, we leverage LPIPS, ametric used to quantify perceptual similarity between im-ages. A lower LPIPS score indicates a higher degree ofsimilarity between the generated image and its ground truth,suggesting accurate inpainting.Furthermore, LPIPS hasbeen empirically validated to closely align with human per-ception, making it a suitable metric for evaluating inpaint-ing quality .For every generated image and its corresponding groundtruth counterpart, we compute both ReMOVE and the LPIPSscore. Subsequently, we arrange the dataset in ascendingorder according to the calculated LPIPS scores, partition-ing it into N = 20 equally spaced subsets. We then deter-mine the mean value of ReMOVE within each partition. Thishelps facilitate a systematic analysis of inpainting qualityassessment across various levels of similarity to the groundtruth.Based on the findings depicted in , the de-creasing trend across partitions infers a close correlationbetween ReMOVE and the quality of inpainting, consistentwith LPIPS.In summary, this validation protocol aims to provide in-sights into the effectiveness of ReMOVE to reliably serve asa reference-free measure for assessing inpainting quality.",
  "DEFACTO Dataset": "The DEFACTO dataset comprises 25,000 input image, in-painting mask, and ground truth tuples. Samples from thedataset are illustrated in . As CS necessitates a textprompt, we utilize BLIP to generate prompts usingboth the input images (resulting in CS-NR) and the groundtruth inpaintings (resulting in CS-FR), where NR and FRrefer to no-reference and full-reference, respectively .The DEFACTO dataset differs from the toy dataset(Sec. 4.1.1) primarily in two aspects: (1) it exhibits a widevariation in the types of masks present, and (2) the imageswithin the dataset are considerably more complex comparedto those in the toy experiment dataset.",
  ". Samples from the object removal category of DEFACTO dataset": "and large, as illustrated in . Following the same ex-perimental setup as the toy experiment we sorted the im-ages by LPIPS and computed the mean ReMOVE score ineach partition. This design allowed us to gain insights intothe efficacy of ReMOVE across various mask sizes. Further-more, we also compare it against CS-FR and CS-NR. Theresults are depicted in and Tab. 1. Our experimentswere conducted on a GeForce RTX-3090 GPU.",
  ". Discussions": "Why does CLIPScore fail in measuring inpainting per-formance? Text captions in intricate scenes (see )often lack comprehensive detail to encapsulate every ob-ject . Thus, relying solely on CLIPScore to evaluate in-painting performance may yield unreliable results. In manyscenarios, ground truth data is unavailable, rendering CS-FR impractical. Consequently, CS-NR is commonly em-ployed in practice.However, CS-NR predominantly de-tects the absence of the object to be inpainted without thor-oughly evaluating the semantic coherence of the inpainted background.This can lead to ambiguity between objectremoval and replacement. The limitation stems from CS-NRs design, which prioritizes the identification of miss-ing elements over assessing the coherence and fidelity ofthe inpainted region within the broader context of the entirescene. Thus, while CLIPScore offers insights, its effective-ness is constrained in complex scenes where text captionsor source images may not provide sufficient information foraccurate evaluation. Why is cropping necessary for accurately estimating in-painting performance? During our preliminary toy exper-iment, we observed promising performance from ReMOVEwithout cropping, demonstrating a strong alignment withthe expected trend with respect to LPIPS. However, as weconducted experiments with real data, we encountered achallenge posed by the varying sizes of the masks. Thisvariability introduced complexities, especially concerningsmaller masks. In such cases, we posit that evaluating theeffectiveness of background inpainting may primarily focuson a localized area surrounding the mask rather than con-sidering the entire image. This issue can be seen in",
  "ReMOVECS-NRCS-FRLPIPS": ". Emperical results on Real Images: ReMOVE consistently aligns with LPIPS, which is the SoTA metric for assessinginpainting, while ground truth inpainted images are provided, but both CS-NR and CS-FR models are noisy and ineffective in prediction. wherein the ReMOVE without cropping produces implau-sible results for the DEFACTO dataset. Consequently, weimplement a cropping strategy, as can be seen in ,to address this issue. We adopted a square-shaped croppingmethod to ensure that within the cropped regions, all maskscovered a similar fraction of the area (around 30%-50%).This approach ensures that the number of patches belong-ing to the masked region is comparable to the number ofpatches in the unmasked region, enabling a correct evalu-ation of background inpainting performance across imageswith masks of different sizes, thus enhancing the reliabilityand consistency of our experimental results.",
  ". User Study": "This study evaluates the effectiveness of an inpainting met-ric in determining user preferences for inpainted images.A. Protocol: Participants ranked a set of two inpainted im-ages based on personal preference and rated the visual qual-ity of inpainting. The metrics (LPIPS and ReMOVE) willindependently rank the same images. Then, we measure theaccuracy of the metrics in predicting the user preference.B. Expected Outcome: We hypothesize an alignment be-tween participant and metrics preference for each inpaint-ing pair, indicating the metrics ability to capture user pref-erences. Consistent performance across both metrics anduser preferences may identify effective inpainting methods.C. Results: We conducted a user study with a set of 20participants in the age range of 23-30. They contributed atotal of 1000 data points. we observe that ReMOVE agrees74.7% times with the preference of the users while LPIPSagrees only 71.9% times. It is to be noted that LPIPS is areference-based metric and thus has more information whileevaluating the quality of inpainting than ReMOVE. Thus,we conclude that in our experimental setup, ReMOVE alignsmore closely with the user preference than LPIPS.D. Conclusion: This study validates the proposed inpaint-",
  ". Conclusion": "We propose ReMOVE, a novel reference-free metric tailoredto assess the effectiveness of object erasure post-generationin generative models like Stable Diffusion. This metric ad-dresses the limitations of existing evaluation measures suchas LPIPS and CLIPScore, especially in scenarios where ref-erence images are unavailable, which is common in practi-cal applications. By overcoming the challenge of distin-guishing between object removal and object replacement in-herent in stable diffusion models, ReMOVE provides a com-prehensive evaluation framework. Through empirical eval-uations, we have demonstrated that ReMOVE not only cor-relates with established metrics reflecting human perceptionbut also captures the nuances of the inpainting process, of-fering a better assessment of generated outputs. We believeReMOVE will serve as a valuable tool for researchers andpractitioners in evaluating and advancing image inpaintingtechniques, ultimately enhancing their applicability in thereal-world.",
  "Omri Avrahami, Ohad Fried, and Dani Lischinski. Blendedlatent diffusion. ACM Trans. Graph., 42(4), 2023. 1, 2": "Samyadeep Basu,Mehrdad Saberi,Shweta Bhardwaj,Atoosa Malemir Chegini, Daniela Massiceti, Maziar San-jabi, Shell Xu Hu, and Soheil Feizi. Editval: Benchmarkingdiffusion based text-guided image editing methods. arXivpreprint arXiv:2310.02426, 2023. 1 Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-structpix2pix: Learning to follow image editing instructions.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1839218402, 2023.1, 2, 4 Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh-man, Eric Luhman, Clarence Ng, Ricky Wang, and AdityaRamesh.Video generation models as world simulators.2024. 1",
  "Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao,and Hengshuang Zhao. Anydoor: Zero-shot object-level im-age customization. arXiv preprint arXiv:2307.09481, 2023.1": "Tianyi Chu, Jiafu Chen, Jiakai Sun, Shuobin Lian, ZhizhongWang, Zhiwen Zuo, Lei Zhao, Wei Xing, and Dongming Lu.Rethinking fast fourier convolution in image inpainting. InProceedings of the IEEE/CVF International Conference onComputer Vision, pages 2319523205, 2023. 1 Guillaume Couairon, Jakob Verbeek, Holger Schwenk, andMatthieu Cord. Diffedit: Diffusion-based semantic imageediting with mask guidance. In ICLR 2023 (Eleventh Inter-national Conference on Learning Representations), 2023. 4",
  "Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,and Yejin Choi. CLIPScore: a reference-free evaluation met-ric for image captioning. In EMNLP, 2021. 2, 6": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,Bernhard Nessler, and Sepp Hochreiter. Gans trained by atwo time-scale update rule converge to a local nash equilib-rium. In Proceedings of the 31st International Conference onNeural Information Processing Systems, page 66296640,Red Hook, NY, USA, 2017. Curran Associates Inc. 4 Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, JiaxiLv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen,and Liangliang Cao. Diffusion model-based image editing:A survey. arXiv preprint arXiv:2402.17525, 2024. 2",
  "Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, JiaxiLv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen,and Liangliang Cao. Diffusion model-based image editing:A survey, 2024. 4": "Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, andQiang Xu. Pnp inversion: Boosting diffusion-based editingwith 3 lines of code. International Conference on LearningRepresentations (ICLR), 2024. 1, 6 Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, andRoss Girshick. Segment anything. arXiv:2304.02643, 2023.3, 4",
  "Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.Blip: Bootstrapping language-image pre-training for unifiedvision-language understanding and generation.In ICML,2022. 2, 6": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer, 2014. 6 Tsung-Jung Liu, Yu-Chieh Lin, Weisi Lin, and C.-C. JayKuo. Visual quality assessment: recent developments, cod-ing applications and future trends. APSIPA Transactions onSignal and Information Processing, 2(1):, 2013. 4 Andreas Lugmayr, Martin Danelljan, Andres Romero, FisherYu, Radu Timofte, and Luc Van Gool. Repaint: Inpaintingusing denoising diffusion probabilistic models. In Proceed-ings of the IEEE/CVF conference on computer vision andpattern recognition, pages 1146111471, 2022. 3 Gael MAHFOUDI, Badr TAJINI, Florent RETRAINT,Frederic MORAIN-NICOLIER, Jean Luc DUGELAY, andMarc PIC. Defacto: Image and face manipulation dataset.In 2019 27th European Signal Processing Conference (EU-SIPCO), pages 15, 2019. 5, 6",
  "Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,Pranav Shyam,Pamela Mishkin,Bob Mcgrew,Ilya": "Sutskever, and Mark Chen.Glide: Towards photorealis-tic image generation and editing with text-guided diffusionmodels. In International Conference on Machine Learning,pages 1678416804. PMLR, 2022. 2, 4 Muhammad Ali Qureshi, Mohamed Deriche, AzeddineBeghdadi, and Asjad Amin. A critical survey of state-of-the-art image inpainting quality assessment metrics. Journal ofVisual Communication and Image Representation, 49:177191, 2017. 1, 4, 6 Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.Zero-shot text-to-image generation. In International Confer-ence on Machine Learning, pages 88218831. PMLR, 2021.1 Robin Rombach, Andreas Blattmann, Dominik Lorenz,Patrick Esser, and Bjorn Ommer.High-resolution imagesynthesis with latent diffusion models.In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1068410695, 2022. 1, 2, 6",
  "ArnaudRougetet.LandscapePicturesonKag-gle.https : / / www . kaggle . com / datasets /arnaud58/landscape-pictures, 2024.[Online;accessed 04-April-2024]. 6": "Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, SilviaCascianelli, Giuseppe Fiameni, and Rita Cucchiara. Fromshow to tell: A survey on deep learning-based image cap-tioning. IEEE transactions on pattern analysis and machineintelligence, 45(1):539559, 2022. 7 Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,Naejin Kong, Harshith Goka, Kiwoong Park, and VictorLempitsky.Resolution-robust large mask inpainting withfourier convolutions. In Proceedings of the IEEE/CVF winterconference on applications of computer vision, pages 21492159, 2022. 1 Song Wang, Hong Li, Xia Zhu, and Ping Li. An evalua-tion index based on parameter weight for image inpaintingquality. In 2008 The 9th International Conference for YoungComputer Scientists, pages 786790, 2008. 4"
}