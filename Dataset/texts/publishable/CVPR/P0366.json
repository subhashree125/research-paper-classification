{
  "Abstract": "Many existing motion prediction approaches rely on sym-bolic perception outputs to generate agent trajectories, suchas bounding boxes, road graph information and trafficlights. This symbolic representation is a high-level abstrac-tion of the real world, which may render the motion predic-tion model vulnerable to perception errors (e.g., failures indetecting open-vocabulary obstacles) while missing salientinformation from the scene context (e.g., poor road condi-tions). An alternative paradigm is end-to-end learning fromraw sensors. However, this approach suffers from the lackof interpretability and requires significantly more trainingresources. In this work, we propose tokenizing the visualworld into a compact set of scene elements and then lever-aging pre-trained image foundation models and LiDAR neu-ral networks to encode all the scene elements in an open-vocabulary manner. The image foundation model enablesour scene tokens to encode the general knowledge of theopen world while the LiDAR neural network encodes ge-ometry information. Our proposed representation can effi-ciently encode the multi-frame multi-modality observationswith a few hundred tokens and is compatible with mosttransformer-based architectures. To evaluate our method,we have augmented Waymo Open Motion Dataset with cam-era embeddings. Experiments over Waymo Open MotionDataset show that our approach leads to significant perfor-mance improvements over the state-of-the-art.",
  "*Equal contributionCorresponding author": ". Overview of the proposed motion prediction paradigm.It fuses symbolic perception output and our multi-modality scenetokens. While symbolic representation offers a convenient worldabstraction, the multi-modality scene tokens links behavior modelsdirectly to sensor observations via token embeddings. dimensionality, facilitating computationally efficient modeltraining. Additionally, since inputs such as 3D boxes areeasily rearranged and manipulated, it is possible to con-struct many hypothetical scenarios leading to efficient sim-ulation and testing. Yet in order to continue improving theaccuracy and robustness of behavior models, it may be nec-essary to feed the models higher-fidelity sensor features.For instance, pedestrian pose and gaze offer richer cuesthan mere bounding boxes for motion prediction. Moreover,many scene elements like lane markings cannot be well rep-resented by boxes. Furthermore, scene context (e.g., roadsurface conditions, hazardous locations) is difficult to char-acterize with symbolic representations. Manually craftingrepresentation for diverse concepts demands considerableengineering effort in implementation, training, and evalua-tion. Instead, we want the behavior model to directly accessthe raw sensor data and determine what and how to encode.Deep learning models performance generally improveswhen we replace hand-crafted features, designed to encodeinductive bias according to expert domain knowledge, withthe directly observed feature as long we scale compute anddata accordingly. But learning to predict complex patterns",
  "arXiv:2404.19531v1 [cs.CV] 30 Apr 2024": "such as agent behavior directly from very high-dimensionalsensor inputs (e.g. many high-resolution LiDAR and cam-era sensors all operating at high frequency) is an extremelychallenging learning problem. It requires learning to orga-nize many hundreds of thousands of points and pixels acrosstime into meaningful representations. Moreover, the inter-mediate representations of fully end-to-end systems are farmore difficult to validate and inspect.Rather than choosing strictly between the two ap-proaches, we instead propose combining existing symbolicrepresentations with learned tokens encoding scene infor-mation. We first decompose the scene into a compact set ofdisjoint elements representing ground regions, perception-detected agents and open-set objects, based on ground planefitting and connected component analysis. We then leveragelarge pre-trained 2D image models and 3D point cloud mod-els to encode these scene elements into tokens. The 2Dimage models are trained on Internet-scale data, and showimpressive capabilities in understanding the open visualworld. These tokens encapsulate relevant information forreasoning about the environment, such as object semantics,object geometry as well as the scene context. We compactlyrepresent multi-modality information about ground, agentsand open-set objects into a few hundred tokens, which welater feed to Wayformer-like network alongside tokensencoding agent position and velocity, road graph, and trafficsignals. All tokens are processed via a linear projection intosame dimension and self-attention layers.To evaluate our method, we introduce camera embed-dings to the Waymo Open Motion Dataset (WOMD) .With LiDAR points and camera embeddings, WOMDhas become a large-scale multi-modal dataset for motionprediction. On the WOMD, our model, which combineslearned and symbolic scene tokens, brings 6.6% relative im-provement on soft mAP or 10.3% relative improvement onminADE. While we obtain the strongest results with the re-cently released image backbone from , other pre-trainedimage models also yield considerable gains. Wefurther analyze the performance of our trajectory predictionmodel under challenging scenarios. Notably, we discoverthat even in the presence of imperfect symbolic perceptionoutputs and incomplete road graph information, our modelmaintains exceptional robustness and accuracy.Our contributions are three-fold:",
  ". Related Works": "Motion Prediction for Autonomous DrivingThe in-creasing interest in autonomous driving has led to a sig-nificant focus on motion prediction .Early methods rasterize theinput scene into a 2D image, followed by processing us-ing convolutional neural networks (CNNs). However, asa result of the inherent lossiness in the rasterization pro-cess, contemporary research has shifted its focus towardsrepresenting road elements, such as object bounding boxes,road graphs, and traffic light signals, as discrete graphnodes . These elements are then directly processed us-ing graph neural networks (GNNs) . Anotherstream of research also employs this discrete set representa-tion for scene elements but processes them using recurrentneural networks , rather than GNNs.Thanks to the rapid advancement of transformer-based ar-chitectures in natural language processing and computer vi-sion, the latest state-of-the-art motion predictors also ex-tensively incorporate the attention mechanism . More recently, the community has also started tostudy interactive behavior prediction, which jointly modelsthe future motion of multiple objects . End-to-end Autonomous DrivingThe concept of end-to-end learning-based autonomous driving systems startedin the late 1980s . Since then, researchers have de-veloped differentiable modules that connect perception andbehavior , behavior and plan-ning , or span from perception to plan-ning .Building on the inspiration from, Hu et al. introduced UniAD , which lever-ages transformer queries and a shared BEV feature map tofacilitate end-to-end learning of perception, prediction, andplanning . More recently, there has been a grow-ing interest in achieving end-to-end motion planning usinglarge language models (LLMs) . Challenges of Existing MethodsWhile substantial ad-vancements have been achieved in standard motion pre-diction benchmarks , the deployment ofexisting behavior models in real-world scenarios remainschallenging. Many motion prediction models heavily relyon pre-processed, symbolic data from perception mod-els , and therefore are vulnerable to poten-tial failures. Moreover, the manually-engineered interfacegreatly restrict the flexibility and scalability of the modelsin handling long-tail and novel categories of objects. In con-trast, end-to-end learning from raw sen-sors, while overcoming some limitations, encounters chal-lenges in interpretability and scaling up batch size due tocomputational constraints.",
  "Multi-modality Scene Tokenization": ". Overview of the proposed Multi-modality Scene Tokenization. Our method takes as input multi-view camera images and a fullscene point cloud. We leverage a pre-trained image foundation model to obtain descriptive feature maps and decompose the scene intodisjoint elements via clustering. Based on the sensor calibration information between camera and LiDAR, we obtain point-wise imagefeatures. From scene decomposition, we assign each point with a token/cluster id and derive box information for each element. Finally, weextract one feature embedding for each scene element.",
  ". Multi-modality Scene Tokenization": "We propose a novel method, MoST (Multi-modality SceneTokenization), to enrich the information fed to Transformer-based motion prediction models, by efficiently combiningexisting symbolic representations with scene tokens that en-code multi-modality sensor information.In this section,we focus on how we obtain these scene tokens, each rep-resented by a scene element feature enriched with semanticand geometric knowledge extracted from both image andLiDAR data. shows an overview of MoST.",
  ". Image Encoding and Point-Pixel Association": "We start by extracting image feature maps for each cam-era and subsequently associating these features to the cor-responding 3D LiDAR points using sensor calibration in-formation.At each time step, we have a set of images{Ik RHkWk3}k captured by a total number of K cam-eras, where Hk and Wk represent the image dimensions.Additionally, we have a LiDAR point cloud Pxyz RNpts3,with Npts denoting the number of points.Using a pre-trained 2D image encoder Eimg, we obtain a feature mapof each image, denoted as {Vk RHkW kD}k. Sub-sequently, we leverage camera and LiDAR calibrations toestablish a mapping between 3D LiDAR points and theircorresponding 2D coordinates on the image feature map ofsize Hk W k. This mapping associates each 3D point withthe corresponding image feature vector. As a result, we ob-tain image features for all Npts 3D points, represented asFpts RNptsD. Note that for points projecting outside ofany image plane, we set their image features as zeros andmark their image features as invalid.To harness a wider range of knowledge, we utilize large pre-trained image models trained on a diverse collectionsof datasets and tasks, capturing a richer understanding ofthe real world. We experiment with several image encodercandidates: SAM ViT-H , VQ-GAN , CLIP and DINO v2 . Different from others, VQ-GAN usesa codebook to build the feature map. To derive Vk fromVQ-GAN, we bottom-crop and partition each input imageinto multiple 256256 patches. Subsequently, we extract256 tokens from each patch and convert them into a 1616feature map through querying the codebook. Finally, thesepartial feature maps are stacked together according to theiroriginal spatial locations to produce Vk.",
  ". Scene Decomposition": "Next, our approach groups full scene LiDAR point cloudinto three element types: ground, agents, and open-setobjects (see illustration in ).We use the termscene element to denote the union of these three ele-ment types. We denote the number of elements of eachtype to be N gndelem, N agentelem , N open-setelemrespectively, and we de-fine Nelem = N gndelem + N agentelem + N open-setelemas the total numberof scene elements. Ground elements: These are segmented blocks of theground surface, obtained through either a dedicatedground point segmentation model or a simple RANSACalgorithm. Since the ground occupies a large area, wedivide it into disjoint 10m 10m tiles, following .",
  ". Visualization of scene decomposition. We decompose ascene into agent elements, open-set elements and ground elements.We also visualize the perception bounding boxes for agents": "include novel categories of traffic participants and obsta-cles beyond the training data, long-tail instances that aperception model suppresses due to low confidence. Weextract these elements by first removing ground and agentelements from the scene point cloud and then using con-nected component analysis to group points into instances. Per-point Token IDBased on scene decomposition ofeach LiDAR frame, we can assign a unique token id to eachLiDAR point. Points within the same scene element shareone token id. With the point-pixel association, we can scat-ter each scene token ID to a set of camera pixels and/orlocations on image feature maps. As a result, we can ob-tain features from both LiDAR and camera for each sceneelement. Based on point-wise token id, we can pool per-point image features into three sets of cluster-wise embed-ding vectors, i.e., Fgndimg RN gndelm D, Fagentimg RN agentelm D,",
  "Fopen-setimg RN open-setelmD": "Scene Element BoxesWe propose to encode each sceneelement with a combination of image features, coarse-grained geometry features, and fine-grained geometry fea-tures. Here we describe how we construct scene elementboxes B to represent coarse-grained geometry. For agentelements, coarse-grained geometry feature are derived fromperception pipelines, capturing information of agent posi-tions, sizes, and heading.For open-set object elements,we compute the tightest bounding boxes covering the pointcluster, and these bounding boxes are also represented bybox centers, box sizes and headings. For ground elements,we have divided the ground into fixed size tiles and sim-ply use the tile center coordinates as position information.These box representations will be further encoded with aMLP and combined with image features and fine-grainedfeatures. We will dive into this combination in Sec 3.3.",
  ". Scene Element Feature Extraction": "We finally extract scene element features with a neural net-work module. Multi-frame information are first compressedin an efficient way, then fed into this feature extraction mod-ule, which generate a single feature vector for each scene el-ement. The feature extraction module is connected with thedownstream Transformer-based motion prediction models,formulating an end-to-end trainable paradigm.",
  "Efficient Multi-frame Data Representation": "While weve compiled valuable information for each ele-ment within a single-frame scene LiDAR points, per-pointimage features, and a bounding box collecting this dataacross multiple frames leads to a large increase in memoryusage. Considering that self-attention layers have quadraticcomplexity with respect to the number of tokens, naivelyconcatenating tokens across all history frames will also leadto significantly increased memory usage. We propose an ef-ficient data representation to reduce the amount of data sentto the model with the following three ingredients.Open-set element tracking We compress the representa-tion of open-set elements by associating open-set elementsacross frames using a simple Kalman Filter.For eachopen-set element, we only store its box information forall T frames with a tensor of shape (N open-setelem T 7)and we apply average pooling across T frames of its im-age features resulting in an image feature tensor of shape(N open-setelem 1 D).Ground-element aggregation Instead of decomposing theground into tiles for each frame, we apply decompositionafter combining the ground points from all frames.Cross-frame LiDAR downsampling Directly storing Li-DAR points for all frames is computationally prohibitive.Simply downsampling LiDAR points in each frame still suf-fers from high redundancy over static parts of the scene.Therefore, we employ different downsampling schemesfor ground elements (which are always static), and open-set/agent elements (which could be dynamic). For groundelements, we first merge the ground points across all framesthen uniformly subsample to a fixed number N gndpts .Foropen-set/agent elements, we subsample them to the fixednumber N open-setptsand N agentptsrespectively. The final LiDARpoints Npts = N gndpts + N agentpts+ N open-setpts. We also create atensor Pind RNpts2 that stores the frame id and scene-element id for each point. Note that in this representation,the number of points from each frame is a variable, which ismore efficient compared to storing a fixed number of pointsfor all frames with padding.",
  "NelemD": ". Scene element feature extraction. Scene-element fea-ture is derived from a spatial-temporal module that fusing togetherimage feature, geometry feature and temporal embedding. Imagefeature contains pooled feature from large pre-trained image en-coder, and characterize the appearance and semantic attribute ofthe scene element. Geometry feature, on the other hand, character-izes the spatial location as well as the detailed geometry. Temporalinformation is injected through a learned temporal embedding.",
  "summarized as following: Fpts RNptsD, point-wise image embeddings derivedfor all the LiDAR points across T frames": "B RNelemT 7, bounding boxes of different scene el-ements across time, where Nelem = N gndelem + N agentelem +N open-setelm. For ground elements, we only encode the tilecenter, leaving the rest four attributes as zeros. P = {Pxyz, Pind}, where Pxyz RNpts3 collects multi-frame LiDAR points, and Pind RNpts2 stores frame idand token id for each point respectively.For each tracked element across T time steps, our net-work (as shown in ) will process the previouslylisted multi-modality information into one embedding foreach scene element, denoted as Felem.As shown in the top branch of , the networkleverages Pind to group point-wise image embeddings Fptsaccording to the token id and frame id, which results in theimage feature tensor for all scene elements across frames,Fimg. In the bottom branch of , we aim to derivegeometry information Fgeo by encoding two pieces of infor-mation, i.e., fine-grained geometry information from pointclouds Pxyz and coarse-grained shape information from 3Dboxes B. The fine-grained geometry is encoded by firstmapping point xyz coordinates into a higher dimensionalspace and grouping high-dimensional features according tothe token id and frame id. The coarse-grained shape encod-ing is derived by projecting box attributes to the same highdimensional space. Formally, fgeo is defined as",
  ". Details of the WOMD camera embeddings": "wise features based on token id and frame id.Spatial-temporal Fusion Our spatial-temporal fusionmodule ( right) takes as input the image featureFimg, the geometry feature Fgeo, and a trainable temporalembedding ftemporal RT D that corresponds to T frames.It produces a temporally aggregated feature Felem for allscene elements. Under the hood, the spatial-temporal fu-sion module adds up the two input tensors, and then con-ducts axial attention across the temporal and element axesof the tensor, which is followed by the final average poolingacross the temporal axis, as listed below:",
  ". The Release of WOMD Camera Embeddings": "To advance research in sensor-based motion predic-tion, we have augmented Waymo Open Motion Dataset(WOMD) with camera embeddings. WOMD containsthe standard perception output, e.g.tracks of boundingboxes, road graph, traffic signals, and now it also includessynchronized LiDAR points and camera embeddings.Given one scenario, a motion prediction model is requiredto reason about 1 second history data and generate pre-dictions for the future 8 seconds at 5Hz. Our LiDAR canreach up to 75 meters along the radius and the cameras pro-vide a multi-view imagery for the environment. WOMDcharacterize each perception-detected objects using a 3Dbounding box (3D center point, heading, length, width, andheight), and the objects velocity vector. The road graphis provided as a set of polylines and polygons with seman-tic types. WOMD is divided into training, validation andtesting subsets according to the ratio 70%, 15%, 15%. In",
  "WayformerReproduced-30.54941.13860.11900.40520.4239MoST-VQGAN-64OursC30.53911.10990.11720.42010.4396": ". Performance comparison on WOMD validation set. MoST leads to significant performance gain to the Wayformer baselines andachieves state-of-the-art results in all compared metrics. MoST-SAM H-{6, 64}: our method using SAM ViT-H feature and predictingbased on 6 or 64 queries. MoST-VQGAN-64: our method using VQGAN feature with 64 queries. Bold font highlights the best result ineach metric and underline denotes the second best. For methods with multiple decoders, results are based on ensembling of predictions.MotionLM* is based on contacting authors for their 1 decoder results, which was not reported in the original publication. this paper, we report results over the validation set. Wellreserve the test set for future community benchmarking.Due to the data storage issue and risk of leakage of sensi-tive information (e.g., human faces, car plate numbers, etc.),we will not release the raw camera images. Instead, the re-leased multi-modality dataset will be in two formats: ViT-VQGAN Tokens and Embeddings: We apply a pre-trained ViT-VQGAN to extract tokens and embed-dings for each camera image. The number of tokens percamera is 512, where each token corresponds to a 32 di-mensional embedding in the quantized codebook. SAM ViT-H Embeddings: We apply a pre-trained SAMViT-H model to extract dense embeddings for eachcamera image. We release the per-scene-element embed-ding vectors, each being 256 dimensional.In the released dataset, we have 1 LiDAR and 8 cameras(front, front-left, front-right, side-left, side-right, rear-left,rear-right, rear). Please see details in . Task and MetricsBased on the augmented WOMD, weinvestigate the standard marginal motion prediction task,where a model is required to generate 6 mostly likely futuretrajectories for each of the agents independently of otheragents futures. We report results for various methods un-der commonly adopted metrics, namely minADE, minFDE,miss rate, mAP and soft-mAP . For fair comparison, weonly compare results based on single model prediction.",
  ". Experimental Results": "Our MoST is a general paradigm applicable to mosttransformer-based motion prediction architectures. Withoutlosing generality, we adopt a state-of-the-art architecture,Wayformer , as our motion prediction backbone and we augment it with our new design by fusing multi-modalitytokens. In the following sections, we use Wayformer as thebaseline and show the performance improvement by MoST.Please refer to appendix for implementation details.",
  "Baseline Comparison": "In , we evaluate the proposed approach and com-pare it with recently published models, i.e., MTR ,Wayformer , MultiPath++ , MotionCNN , Mo-tionLM , SceneTransformer .Specifically, westudy our approach in two settings, 1) using LiDAR + cam-era tokens with single decoder and 2) using camera tokenswith 3 decoders. During inference, we fit a Gaussian Mix-ture Model by merging predictions from the decoder(s) anddraw 2048 samples, which are finally aggregated into 6 tra-jectories through K-means . In both settings, the intro-duction of sensory tokens leads to a clear performance gainover the corresponding Wayformer baselines based on ourre-implementation. Moreover, our approach achieves state-of-the-art performance across various metrics. illustrates two comparisons between our MoSTand the baseline model. The upper example shows that withtokenized sensor information, our MoST rules out the pos-sibility that a vehicle runs onto walls after a U-turn. Thelower example makes a prediction that a cyclist may crossthe street which is safety critical for the autonomous vehicleto take precaution regarding this behavior.",
  "MoSTWayformer": ". Qualitative comparison. The agent boxes are coloredby their types: gray for vehicle, red for pedestrian, and cyan forcyclist. The predicted trajectories are ordered temporally fromgreen to blue. For each modeled agent, the models predict 6 trajec-tory candidates, whose confidence scores are illustrated by trans-parency: the more confident, the more visible. Ground truth tra-jectory is shown as red dots. In the upper example, MoST rulesout the possibility that a vehicle runs onto a wall after U-turn; inthe lower example, MoST correctly predicts that a cyclist couldsuddenly cross the street.",
  "DINO-v2 0.55970.41540.4285CLIP 0.55900.41380.4272VQ-GAN 0.56700.40580.4192SAM ViT-H 0.54830.41620.4321": ". Ablation study of different image features. All theseimage features improves the motion prediction performance, whilewe observe SAM ViT-H leads to the most improvement. Weuses single-frame multi-modal feature for these study. ficient experimentation, we perform ablation study by em-ploying single frame MoST and SAM ViT-H feature.Effects of Different Pre-trained Image Encoders To in-vestigate different choices of the image encoder for ourmodel, we have conducted experiments comparing the per-formance of using image feature encoders from variouspre-trained models: SAM ViT-H , CLIP , DINOv2 , and VQ-GAN . As show in , the SAMViT-H encoder yields the highest performance across all be-havior prediction metrics. We hypothesize that this perfor-mance advantage likely stems from SAMs strong capabil-ity to extract comprehensive and spatially faithful feature",
  ". Comparing variant of scene tokenization strategy withsingle-frame sensor data. Both token strategy leads to improve-ment the vanilla Wayformer , which does not use sensor data": "maps, as it is trained with a large and diverse dataset forthe dense understanding task of image segmentation. Otherlarge pre-trained image models also demonstrate notablecapability, outperforming the Wayformer baseline on mAPand soft mAP, albeit inferior to SAM.Ablation on Input Modality To understand how differ-ent input modalities affect the final model performance, weconduct ablation experiments and summarize results in Ta-ble 5 where we remove image feature or remove LiDARfeature of our single-frame model. We can see image fea-ture and LiDAR feature are both beneficial, and combiningboth modality leads to the biggest improvement.Ablation on Scene Element To gain deeper insights intothe contribution of each type of scene element, we conductablation studies in by removing specific elementtypes and evaluate the impact on behavior prediction met-rics. Combining all types of scene elements leads to the bestsoft-mAP metric. Note that only associating image features",
  ". Evaluation on hard scenarios. We curate a set of hardscenarios based on the performance of MoST and Wayformer onthem. MoST consistently shows improved performance": "to agents give the smallest improvement. We hypothesis thereasons to be two-fold: 1) in most cases, the agent box issufficient to characterize the motion of the object; 2) thereare only a handful of agents in the scene and very few imagefeatures are included in the model.Alternative Scene Tokenizer We use SAM ViT-H in thisexperiment. We design another baseline tokenizer, denotedas Image-grid token, which tokenizes each image featureas 16 16 = 256 image embeddings by subsampling 4Xalong column and row axes. The feature from all cameraimages are flattened and concatenated to form scene tokens.In we can see the Image-grid tokenizer also leads toimprovement compared to Wayformer baseline, though in-ferior to our cluster-based sparse tokenizer which utilizespoint cloud to derive accurate depth information and lever-ages the intrinsic scene sparsity to get compact tokens.",
  "Evaluation on Challenging Scenarios": "While the improvement shown above demonstrates overallimprovement across all driving scenarios, we are also inter-ested in investigating the performance gain in the most chal-lenging cases. Here we present how our model performs inchallenging scenarios, specifically on (a) a mined set of hardscenarios, (b) situations where perception failures happen,and (c) situations where roadgraph is inaccurate.Mined Hard Scenarios To assess the effectiveness of ourmethod in complex situations, we have curated a set of hardscenarios. We conduct a per-scenario evaluation through-out the entire validation set, identifying the 1000 scenar-ios with the lowest minADE across vehicle, pedestrian, andcyclist categories for the baseline and MoST-SAM H-64,respectively. In this way, we ensure the mining is sym-metric and fair for both methods. Then we combine these6000 scenarios, resulting in 4024 unique scenarios, form-ing our curated challenging evaluation dataset. As shownin , MoST demonstrates more pronounced relativeimprovement in mAP and soft-mAP, i.e., 13.1% and 12.4%respectively, compared to the baseline in these hardest sce-narios, confirming its effectiveness of enhanced robustnessand resilience in complex situations. We also find that im-proving minADE in these hard scenarios is still a challenge.Perception Failure Most motion prediction algorithms assume accurate perception object boxes as inputs. It iscritical to understand how such a system will perform whenthis assumption breaks due to various reasons, such as long-",
  "%0.67070.3499": ". Evaluation on simulated perception and roadgraph failure.We vary the ratio of miss detected agent boxes and miss detectedroadgraph segments in scenes as 10%, 30% and 50%, respectively.With multi-modal features, MoST performs on par with baselineseven with 50% perception or 30% roadgraph failure. tail and novel categories of object beyond training supervi-sion, occlusion, long-range, etc. Thus, we propose to addi-tionally evaluate our method against the baseline method inthe case of perception failure. Concretely, we simulate per-ception failure of not detecting certain object boxes by ran-domly removing agents according to a fixed ratio of agentsin the scene.The boxes dropped out are consistent forMoST-SAM H-64 and the baseline. As shown in ,MoST shows robustness against perception failures: evenwhen failure rate raises to 50%, our model still performs onpar with the Wayformer baseline (soft mAP 0.4121).Roadgraph Failure Motion prediction models often ex-hibit a strong reliance on roadgraphs, leading to potentialvulnerabilities in situations where the roadgraph is incom-plete or inaccurate. Our proposed model, MoST, tacklesthis issue by incorporating multi-modality scene tokens asadditional inputs, thereby enhancing its robustness againstroadgraph failures. We demonstrate this advantage by sim-ulating various levels of roadgraph errors, similar to theaforementioned perception failure simulation. Specifically,we evaluate MoST-SAM H-64 under scenarios with 10%,30%, and 50% missing roadgraph segments in the valida-tion set. Notably, as showcased in , even with a 30%of the roadgraph missing, MoST performs on par with base-line models that assume perfect roadgraph information.",
  ". Conclusions": "To promote sensor-based motion prediction research, wehave enhanced WOMD with camera embeddings, makingit a large-scale multi-modal dataset for benchmarking.To efficiently integrate multi-modal sensor signals intomotion prediction, we propose a method that represents themulti-frame scenes as a set of scene elements and leverageslarge pre-trained image encoders and 3D point cloudnetworks to encode rich semantic and geometric informa-tion for each element. We demonstrate that our approachleads to significant improvements in motion prediction task.",
  "Sergio Casas, Wenjie Luo, and Raquel Urtasun. Intentnet:Learning to predict intention from raw sensor data. In CoRL,2018. 2": "Sergio Casas, Cole Gulino, Renjie Liao, and Raquel Urta-sun.Spagnn: Spatially-aware graph neural networks forrelational behavior forecasting from sensor data.In 2020IEEE International Conference on Robotics and Automation(ICRA), pages 94919497. IEEE, 2020. 2 Sergio Casas, Abbas Sadat, and Raquel Urtasun. Mp3: Aunified model to map, perceive, predict and plan. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1440314412, 2021. 2",
  "Yuning Chai, Benjamin Sapp, Mayank Bansal, and DragomirAnguelov. Multipath: Multiple probabilistic anchor trajec-tory hypotheses for behavior prediction. In CoRL, 2019. 2": "Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jag-jeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, PeterCarr, Simon Lucey, Deva Ramanan, et al. Argoverse: 3dtracking and forecasting with rich maps. In CVPR, 2019. 2 Kan Chen,Runzhou Ge,Hang Qiu,Rami Al-Rfou,Charles R Qi, Xuanyu Zhou, Zoey Yang, Scott Ettinger,Pei Sun, Zhaoqi Leng, et al.Womd-lidar: Raw sensordataset benchmark for motion forecasting.arXiv preprintarXiv:2304.03834, 2023. 2",
  "Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger,Andreas Geiger,and Hongyang Li.End-to-end au-tonomous driving: Challenges and frontiers. arXiv preprintarXiv:2306.16927, 2023. 2": "Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,Piotr Padlewski, Daniel Salz, Sebastian Goodman, AdamGrycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model. arXiv preprintarXiv:2209.06794, 2022. 5 Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou,Tsung-Han Lin, Thi Nguyen, Tzu-Kuo Huang, Jeff Schnei-der, and Nemanja Djuric. Multimodal trajectory predictionsfor autonomous driving using deep convolutional networks.In ICRA, 2019. 2",
  "Feng, Rui Hu, Yang Xu, et al. Multixnet: Multiclass mul-tistage multimodal motion prediction. In 2021 IEEE Intelli-gent Vehicles Symposium (IV), pages 435442. IEEE, 2021.2": "Patrick Esser, Robin Rombach, and Bjorn Ommer. Tamingtransformers for high-resolution image synthesis.In Pro-ceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 1287312883, 2021. 3, 7 Scott Ettinger, Shuyang Cheng, Benjamin Caine, ChenxiLiu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp,Charles R. Qi, Yin Zhou, Zoey Yang, Aurelien Chouard, PeiSun, Jiquan Ngiam, Vijay Vasudevan, Alexander McCauley,Jonathon Shlens, and Dragomir Anguelov. Large scale in-teractive motion forecasting for autonomous driving: Thewaymo open motion dataset. In ICCV, 2021. 2, 5, 6 Scott Ettinger, Shuyang Cheng, Benjamin Caine, ChenxiLiu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp,Charles R Qi, Yin Zhou, et al. Large scale interactive mo-tion forecasting for autonomous driving: The waymo openmotion dataset. In ICCV, 2021. 2 Sudeep Fadadu, Shreyash Pandey, Darshan Hegde, Yi Shi,Fang-Chieh Chou, Nemanja Djuric, and Carlos Vallespi-Gonzalez. Multi-view fusion of sensor data for improvedperception and prediction in autonomous driving. In Pro-ceedings of the IEEE/CVF Winter Conference on Applica-tions of Computer Vision, pages 23492357, 2022. 2",
  "Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, DragomirAnguelov, Congcong Li, and Cordelia Schmid. Vectornet:Encoding hd maps and agent dynamics from vectorized rep-resentation. In CVPR, 2020. 2": "Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, BogdanStanciulescu, and Fabien Moutarde. Home: Heatmap outputfor future motion estimation. In 2021 IEEE InternationalIntelligent Transportation Systems Conference (ITSC), pages500507. IEEE, 2021. 2 Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bog-dan Stanciulescu, and Fabien Moutarde. Gohome: Graph-oriented heatmap output for future motion estimation.In2022 international conference on robotics and automation(ICRA), pages 91079114. IEEE, 2022. 2",
  "Junru Gu, Chen Sun, and Hang Zhao. Densetnt: End-to-endtrajectory prediction from dense goal sets. In ICCV, 2021. 2": "Junru Gu, Chenxu Hu, Tianyuan Zhang, Xuanyao Chen,Yilun Wang, Yue Wang, and Hang Zhao. Vip3d: End-to-endvisual trajectory prediction via 3d agent queries. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 54965506, 2023. 2 Cole Gulino, Justin Fu, Wenjie Luo, George Tucker, EliBronstein, Yiren Lu, Jean Harb, Xinlei Pan, Yan Wang, Xi-angyu Chen, et al. Waymax: An accelerated, data-drivensimulator for large-scale autonomous driving research. arXivpreprint arXiv:2310.08710, 2023. 2 Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese,and Alexandre Alahi. Social gan: Socially acceptable tra-jectories with generative adversarial networks. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 22552264, 2018. 2 Steffen Hagedorn, Marcel Hallgarten, Martin Stoll, andAlexandru Condurache.Rethinking integration of predic-tion and planning in deep learning-based automated drivingsystems: A review. arXiv preprint arXiv:2308.05731, 2023.2",
  "Joey Hong, Benjamin Sapp, and James Philbin. Rules of theroad: Predicting driving behavior with a convolutional modelof semantic interactions. In CVPR, 2019. 2": "Anthony Hu, Zak Murez, Nikhil Mohan, Sofa Dudas, Jef-frey Hawke, Vijay Badrinarayanan, Roberto Cipolla, andAlex Kendall. Fiery: Future instance prediction in birds-eye view from surround monocular cameras. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 1527315282, 2021. 2 Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, JunchiYan, and Dacheng Tao. St-p3: End-to-end vision-based au-tonomous driving via spatial-temporal feature learning. InEuropean Conference on Computer Vision, pages 533549.Springer, 2022. 2 Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, WenhaiWang, et al. Planning-oriented autonomous driving. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1785317862, 2023. 2 XiaosongJia,YuluGao,LiChen,JunchiYan,Patrick Langechuan Liu, and Hongyang Li. Driveadapter:Breaking the coupling barrier of perception and planningin end-to-end autonomous driving.In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 79537963, 2023. 2 Xiaosong Jia, Penghao Wu, Li Chen, Yu Liu, Hongyang Li,and Junchi Yan. Hdgt: Heterogeneous driving graph trans-former for multi-agent trajectory prediction via scene encod-ing. IEEE transactions on pattern analysis and machine in-telligence, 2023. 2 Xiaosong Jia, Penghao Wu, Li Chen, Jiangwei Xie, Con-ghui He, Junchi Yan, and Hongyang Li. Think twice be-fore driving: Towards scalable decoders for end-to-end au-tonomous driving. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages2198321994, 2023. 2 Alexey Kamenev, Lirui Wang, Ollin Boer Bohan, IshwarKulkarni, Bilal Kartal, Artem Molchanov, Stan Birchfield,David Nister, and Nikolai Smolyanskiy. Predictionnet: Real-time joint probabilistic traffic prediction for planning, con-trol, and simulation. In 2022 International Conference onRobotics and Automation (ICRA), pages 89368942. IEEE,2022. 2",
  "Ilya Loshchilov and Frank Hutter.Decoupled weight de-cay regularization. In International Conference on LearningRepresentations, 2017. 14": "Wenjie Luo, Bin Yang, and Raquel Urtasun. Fast and furi-ous: Real time end-to-end 3d detection, tracking and motionforecasting with a single convolutional net. In Proceedings ofthe IEEE conference on Computer Vision and Pattern Recog-nition, pages 35693577, 2018. 2 Wenjie Luo, Cheol Park, Andre Cornman, Benjamin Sapp,and Dragomir Anguelov. Jfp: Joint future prediction withinteractive multi-agent modeling for autonomous driving. InConference on Robot Learning, pages 14571467. PMLR,2023. 2",
  "Jiageng Mao, Yuxi Qian, Hang Zhao, and Yue Wang.Gpt-driver:Learning to drive with gpt.arXiv preprintarXiv:2310.01415, 2023. 2": "Francesco Marchetti, Federico Becattini, Lorenzo Seidenari,and Alberto Del Bimbo. Mantra: Memory augmented net-works for multiple trajectory prediction. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 71437152, 2020. 2 Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, KratarthGoel, Khaled S Refaat, and Benjamin Sapp.Wayformer:Motion forecasting via simple & efficient attention networks.In 2023 IEEE International Conference on Robotics and Au-tomation (ICRA), pages 29802987. IEEE, 2023. 2, 5, 6, 7,8, 13 Jiquan Ngiam, Benjamin Caine, Vijay Vasudevan, Zheng-dong Zhang, Hao-Tien Lewis Chiang, Jeffrey Ling, RebeccaRoelofs, Alex Bewley, Chenxi Liu, Ashish Venugopal, et al.Scene transformer: A unified architecture for predicting mul-tiple agent trajectories.arXiv preprint arXiv:2106.08417,2021. 2 Jiquan Ngiam, Benjamin Caine, Vijay Vasudevan, Zheng-dong Zhang, Hao-Tien Lewis Chiang, Jeffrey Ling, RebeccaRoelofs, Alex Bewley, Chenxi Liu, Ashish Venugopal, DavidWeiss, Ben Sapp, Zhifeng Chen, and Jonathon Shlens. Scenetransformer: A unified architecture for predicting multipleagent trajectories. In ICLR, 2022. 6 Maxime Oquab, Timothee Darcet, Theo Moutakanni, HuyVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.Dinov2: Learning robust visual features without supervision.arXiv preprint arXiv:2304.07193, 2023. 2, 3, 7 Seong Hyeon Park, Gyubok Lee, Jimin Seo, Manoj Bhat,Minseok Kang, Jonathan Francis, Ashwin Jadhav, Paul PuLiang, and Louis-Philippe Morency. Diverse and admissi-ble trajectory forecasting through multimodal context under-standing. In ECCV, pages 282298. Springer, 2020. 2",
  "Charles R Qi, Yin Zhou, Mahyar Najibi, Pei Sun, Khoa Vo,Boyang Deng, and Dragomir Anguelov. Offboard 3d objectdetection from point cloud sequences. In CVPR, 2021. 2": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-vision. In ICML, 2021. 2, 3, 7 Nicholas Rhinehart, Rowan McAllister, Kris Kitani, andSergey Levine. Precog: Prediction conditioned on goals invisual multi-agent settings. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 28212830, 2019. 2 Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu,Pranaab Dhawan, and Raquel Urtasun. Perceive, predict, andplan: Safe motion planning through interpretable semanticrepresentations. In ECCV, pages 414430. Springer, 2020. 2",
  "Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, andMarco Pavone. Trajectron++: Dynamically-feasible trajec-tory forecasting with heterogeneous data. In ECCV, pages683700. Springer, 2020. 2": "Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou,Nigamaa Nayakanti, Khaled S Refaat, Rami Al-Rfou, andBenjamin Sapp.Motionlm: Multi-agent motion forecast-ing as language modeling. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 85798590, 2023. 2, 6, 8 Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele.Motion transformer with global intention localization and lo-cal movement refinement. Advances in Neural InformationProcessing Systems, 35:65316543, 2022. 2, 5, 6",
  "Charlie Tang and Russ R Salakhutdinov. Multiple futuresprediction. Advances in neural information processing sys-tems, 32, 2019. 2": "Ekaterina Tolstaya, Reza Mahjourian, Carlton Downey,Balakrishnan Vadarajan, Benjamin Sapp, and DragomirAnguelov. Identifying driver interactions via conditional be-havior prediction. In 2021 IEEE International Conference onRobotics and Automation (ICRA), pages 34733479. IEEE,2021. 2 Balakrishnan Varadarajan, Ahmed Hefny, Avikalp Sri-vastava, Khaled S. Refaat, Nigamaa Nayakanti, AndreCornman, Kan Chen, Bertrand Douillard, Chi-Pang Lam,Dragomir Anguelov, and Benjamin Sapp. Multipath++: Ef-ficient information fusion and trajectory aggregation for be-havior prediction. CoRR, abs/2111.14973, 2021. 2 Balakrishnan Varadarajan, Ahmed Hefny, Avikalp Srivas-tava, Khaled S. Refaat, Nigamaa Nayakanti, Andre Corn-man, Kan Chen, Bertrand Douillard, Chi Pang Lam,Dragomir Anguelov, and Benjamin Sapp.Multipath++:Efficient information fusion and trajectory aggregation forbehavior prediction. In 2022 International Conference onRobotics and Automation (ICRA), pages 78147821, 2022.6 Pengqin Wang, Meixin Zhu, Hongliang Lu, Hui Zhong, Xi-anda Chen, Shaojie Shen, Xuesong Wang, and Yinhai Wang.Bevgpt: Generative pre-trained large model for autonomousdriving prediction, decision-making, and planning.arXivpreprint arXiv:2310.10357, 2023. 2 Benjamin Wilson, William Qi, Tanmay Agarwal, JohnLambert, Jagjeet Singh, Siddhesh Khandelwal, BowenPan, Ratnesh Kumar, Andrew Hartnett, Jhony KaesemodelPontes, et al.Argoverse 2: Next generation datasets forself-driving perception and forecasting.arXiv preprintarXiv:2301.00493, 2023. 2 Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo,Kenneth KY Wong, Zhenguo Li, and Hengshuang Zhao.Drivegpt4: Interpretable end-to-end autonomous driving vialarge language model.arXiv preprint arXiv:2310.01412,2023. 2 Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,and Yonghui Wu.Vector-quantized image modeling withimproved vqgan. arXiv preprint arXiv:2110.04627, 2021. 5,6, 13",
  "Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learningfor point cloud based 3d object detection. In CVPR, 2018. 2": "Yin Zhou, Pei Sun, Yu Zhang, Dragomir Anguelov, JiyangGao, Tom Ouyang, James Guo, Jiquan Ngiam, and Vijay Va-sudevan. End-to-end multi-view fusion for 3d object detec-tion in lidar point clouds. In CoRL, 2020. 2 . Additional qualitative comparison between MoST andWayformer baseline. The agent boxes are colored by theirtypes: gray for vehicle, red for pedestrian, and cyan for cyclist.The predicted trajectories are ordered temporally from green (+0s)to blue (+8.0s). For each modeled agent, the models predict 6trajectory candidates, whose confidence scores are illustrated bytransparency: the more confident, the more visible. Ground truthtrajectory is shown as red dots. Note that the vehicle indicated bythe red arrow is entering a plaza which has no map coverage. Sinceour model has access to the rich visual signals, it correctly predictsthe vehicles possible trajectory which includes follows the arrowand turn right. Wayformer, on the other hand, completely missedthis possibility due to the lack of road graph information in thatregion.",
  "A. Additional Qualitative Results": "An additional qualitative comparison can be found in Fig-ure 6. In this scenario, the model is asked to predict thefuture trajectory of a vehicle entering a plaza which is notmapped by the road graph. Our model with access to visualinformation correctly predicts several trajectories followingthe arrow painted on the ground and turning right.",
  "B. WOMD Camera Embeddings": "VQGAN EmbeddingTo extract VQGAN embedding foran image, we first resize the image into shape of 256 512.Then we horizontally split the image into two patches andapply pre-trained ViT-VQGAN model on each patchrespectively. Each patch contains 16 16 tokens so eachcamera image can be represented as 512 tokens. The code-book size is 8192. SAM-H EmbeddingFor each camera we extract SAMViT-H embedding of size 64 64 256. Comparedto VQGAN embeddings, SAM features are less spatiallycompressed due to its high-resolution feature map. The vi-sualization of SAM Embedding can be found in .We release the SAM features pooled per-scene-element.",
  "C. Implementation Details": "Model DetailWe use N agentelem= 128, N open-setelem= 384,N gndelem = 256, and Npts = 65536 in our experiments. Weuse sensor data from past 10 frames that correspond to the1 second history and the current frame (i.e. T = 11). Fol-lowing Wayformer , we train our model to output Kmodes for the Gaussian mixture, where we experiment withK = {6, 64}. During inference, we draw 2048 samplesfrom the predicted Gaussian mixture distribution, and useK-Means clustering to aggregate those 2048 samples into 6final trajectory predictions."
}