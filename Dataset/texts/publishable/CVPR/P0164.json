{
  "Abstract": "Being able to carry out complicated vision language rea-soning tasks in 3D space represents a significant milestonein developing household robots and human-centered em-bodied AI. In this work, we demonstrate that a critical anddistinct challenge in 3D vision language reasoning is situa-tional awareness, which incorporates two key components:(1) The autonomous agent grounds its self-location basedon a language prompt. (2) The agent answers open-endedquestions from the perspective of its calculated position. Toaddress this challenge, we introduce SIG3D, an end-to-endSituation-Grounded model for 3D vision language reason-ing. We tokenize the 3D scene into sparse voxel represen-tation and propose a language-grounded situation estima-tor, followed by a situated question answering module. Ex-periments on the SQA3D and ScanQA datasets show thatSIG3D outperforms state-of-the-art models in situation es-timation and question answering by a large margin (e.g.,an enhancement of over 30% on situation estimation ac-curacy).Subsequent analysis corroborates our architec-tural design choices, explores the distinct functions of visualand textual tokens, and highlights the importance of situa-tional awareness in the domain of 3D question answering.The project page is available at",
  ". Introduction": "Humans learn knowledge efficiently through the interac-tions with the 3D world and the integration of multi-modalinformation, such as verbal guidance or instructions. Simi-larly, introducing language guidance into the visual compre-hension task can greatly enhance the learning efficiency ofmodels . Nonetheless, despite considerable advance-ments in linguistic understanding and vision-language integration , current methodologiesremain deficient in accurately perceiving and rationalizingwithin real-world 3D environments, which is largely at-tributed to the lack of 3D situational reasoning capabilities.Compared to machine learning models, humans put",
  "+": ". Previous methods perform direct 3D vision languagereasoning without modeling the situation of an embodied agent inthe 3D environment. Our method, SIG3D, grounds the situationaldescription in the 3D space, and then re-encodes the visual tokensfrom the agents intended perspective before vision-language fu-sion, resulting in a more comprehensive and generalized 3D visionlanguage (VL) representation and reasoning framework. Q, K, Vstand for query, key, and value, respectively. themselves inside the 3D world and then perceive and in-teract with the surrounding environment from their ego-perspective (). Such situational awareness is a cru-cial difference between 2D and 3D visual understanding,and a key to achieving seamless understanding of spatialconcepts in more complex real-world environments. Sev-eral existing methods recognize the lack of positional un-derstanding in 3D and propose new benchmarks and jointoptimization functions , or positional embedding meth-ods to enhance the overall reasoning performance. However, the lack of an explicit situation modelingand situation-grounded 3D reasoning method restricts themfrom obtaining a generalizable and consistent 3D vision-language (VL) representation. As shown in , thesituation prediction of the state-of-the-art method (inblue) diverges significantly from the ground truth vectors(in red) in almost all scenes in the dataset . Moreover,our pilot study in also reveals that situational un-derstanding, despite being very crucial in comprehendingthe context of questions, only plays a minor role in the final",
  "I am sitting on a couch with a pillow facing another couch and the pillow is on my right": ". Situation estimation in existing methods fails inmost scenarios, indicating the missing registration between the sit-uational descriptions and 3D embeddings. Red: Ground truth (GT)vector. Blue: Estimated vector. question answering (QA) performance of existing methods.In this work, we propose SIG3D, a novel approach de-signed to precisely model and estimate an embodied agentsego-location and orientation from a textual description, be-fore performing multi-modal QA tasks from the agentsego-centric perspective, as shown in . Specifically,we leverage large-scale pretrained language and visual en-coders to process the input text and 3D data, and fuse the to-kens with attention modules to predict a situational vector.Previous attempts to directly predict the ego-situation arehindered by the expansive search space inherent in 3D en-vironments. To address this challenge, we re-conceptualizethe task as an anchor-based classification, where visual to-kens are regarded as anchor points, and a likelihood of po-sition together with a set of rotation parameters are con-currently regressed for each visual token. After obtainingthe situation estimation, we propose a situational alignmentand a situation-guided tokens re-encoding strategy, to per-ceive the environment from the agents intended perspec-tive. These strategies enhance the visual tokens with moreaccurate situational awareness for subsequent QA tasks.Experiments on two challenging 3D visual question an-swering (VQA) datasets demonstrate the significantimprovement in situation estimation and QA tasks of ourmodel. In particular, we improve the accuracy of situationestimation by more than 30%, and subsequent QA perfor-mance by up to 3%. Further qualitative and quantitativeanalysis verifies our design choices and highlights the sig-nificance of situational awareness in 3D reasoning tasks.To sum up, our paper has the following contributions:(1) We recognize the lack of situational awareness as a sig- nificant oversight in existing research. To address this, weintroduce SIG3D, a situation-grounded 3D VL reasoningarchitecture, specifically designed to fill this void. (2) Wepropose an anchor-based approach to situation estimation,which effectively narrows the extensive search space in 3Denvironments for precise grounding of 3D positions and ori-entations with textual descriptions. Additionally, we inves-tigate situational alignment and visual re-encoding mech-anisms to leverage situational awareness for enhanced QAperformance. (3) Our model demonstrates superior perfor-mance on two challenging datasets, SQA3D and ScanQA,surpassing the state of the art in both situation estimationand QA metrics. Ablation studies highlight the importanceof situation-guided encoding, revealing its beneficial impacton general QA tasks.",
  ". Related Work": "Vision Language Models (VLMs).Early transformer-driven textual and visual encoders have fa-cilitated great progress in recent vision language learning.Text-image contrastive models propose to align thefeature space of two modalities with large-scale pretraining,fueling numerous downstream tasks from generalized open-vocabulary visual perception to text-to-imagegeneration . Concurrently, some work uses text and vi-sion encoders on separate modalities followed by featurefusion for multi-modal reasoning tasks. Since theemergence of Large Language Models (LLMs) ,VLMs have experienced huge improvement with the helpof LLMs as building blocks for multi-modal learning ar-chitectures. Specifically, recent work directly projects vi-sual embeddings into language-space tokens as input toLLMs , or use the latent bottleneck structure forcross-modal visual decoding , or treat LLMlayers as encoder blocks for various visual tasks .In the domain of visual question answering (VQA) [4, 73], recent work has pushed the frontier towards video un-derstanding , knowledge-based under-standing , and commonsense reason-ing . Despite the outstanding performance in 2D imageinterpretation, most existing methods lack the capability togeneralize to 3D scenarios. In contrast, our work studiesthe representation of visual information and its fusion withlanguage embeddings in the 3D domain by targeting on the3D situation-guided visual language interpretation. Grounding Language in 3D Space.Compared with 2Dimages, knowledge such as spatial relationships, interactiveexploration, and topological analysis which only existsin the 3D world provides additional challenges and op-portunities to develop better language models with strongercommonsense reasoning capability grounded in the real-world 3D scenarios.In this direction, early work seeks",
  "Acc. / %010203040": ". Results on variants of the representative SQA3D base-line method demonstrate that situational understanding, de-spite being indispensable in perceiving the context of questions,makes negligible contribution in existing methods. This motivatesa situation-guided 3D encoding mechanism in our model. to ground isolated objects or objects within morecomplex scenes using natural language de-scriptions. Recently, with more collected 3D vision lan-guage benchmarks, some work starts to explore language-guided 3D visual interpretation and reasoning on a diverseset of datasets, including 3D scene captioning , open-vocabulary segmentation , and question answer-ing .The success of LLMs also elicits the usage of them in3D vision language reasoning for task decomposition ,data generation, and multi-modal feature fusion . Mo-tivated by ScanQA , SQA3D takes the first step toexplore the challenging 3D situational reasoning problemby developing a situated question answering benchmark,and proposing the first joint learning baseline on this bench-mark. Our work highlights the uniqueness and significanceof situational awareness in the 3D vision language learningparadigm, leading to notably better 3D situational ground-ing and question answering performance.",
  ". Pilot Study on Situational Reasoning": "Despite highlighting the importance of situational under-standing and reasoning, existing methods fall shortin providing effective situation estimation, as illustratedin . This section delves into a pilot study that exam-ines the impact of situational understanding on downstreamreasoning tasks. The SQA3D baseline incorporates sit-uational descriptions and uses ground truth (GT) situationalvectors for supervision in a direct regression task. We in-vestigate three variants of this baseline to assess the effect ofsituational understanding. In the first variant, we remove thesituational description and supervision from the model, bypassing in empty situational tokens. In another variant, wecorrupt the situation supervision by introducing very largeGaussian noise to the GT vectors to effectively randomizethem. Finally, we try to encode the GT situational vector inthe input with learnable multi-layer perceptron (MLP) lay-ers to form a GT situational token. demonstrates the results of this study, reveal-ing negligible changes in performance across these variants.Notably, corrupting the GT situational information or di- rectly incorporating it results in only marginal alterations inthe QA outcomes. Omitting the situational description en-tirely from the input results in a slight 2% decrease in preci-sion. However, in the absence of this information, the modelresorts to random guessing when determining the correctanswer, as all responses depend on the situation. The find-ings from Figures 2 and 3 collectively indicate a deficiencyin existing methods regarding situation estimation and theapplication of situational understanding in subsequent rea-soning tasks. These unresolved challenges motivate the de-velopment of our proposed method.",
  ". Method": "An overview of our approach SIG3D is illustrated in Fig-ure 4. Our method begins with a set of points that repre-sent a 3D scene, accompanied by a situational descriptionand a question that define the overall context of the prob-lem.We tokenize them into separate token embeddings(.1), and ground the textual description in the 3Dscene with a vector comprising location and orientation. Wefind direct single vector estimation to be challenging dueto the vast and complex nature of the 3D search space, sowe propose an anchor-based situation estimation strategy(.2). Subsequently, we re-encode the visual tokensfrom the perspective of situational vectors, enhancing thesituational awareness for downstream reasoning tasks (Sec-tion 4.3). The finalized visual and textual tokens are fusedby a transformer decoder to generate the final response.",
  ". Visual and Textual Tokenization": "Leveraging input scene point clouds and textual prompts,our objective is to generate three distinct types of tokens:3D visual tokens z3D RNvCv, situational tokens zS RNsCs, and question tokens zQ RNqCq. Each type oftoken is characterized by two primary components: N, rep-resenting the number of tokens, and C, encapsulating thefeature embeddings. To tokenize and capture feature em-beddings for situational input and questions, we employ ashared text tokenizer ETXT following prior methods .We assume that situation and question prompts are sepa-rated in the input data. If not, LLMs can be used toparse the textual input without changing the semantic mean-ing of the sentences. However, there is a lack of consensuson a standard 3D visual tokenization method E3D that is ap-propriate for the 3D VL reasoning task, prompting a moredetailed exploration in the following paragraphs. Visual Tokenization.Given an input point cloud p RN3, most prior methods adopt a VoteNet detector to acquire object-level tokens z3D RNobjCobjas the visual representation, where Nobj is the number ofobject proposals, and Cobj is the object-level feature em-beddings. However, we point out several problems with",
  "represent": ". Overview of our SIG3D model, which includes 3D scene and text encoding, anchor-based situation estimation, situation-guidedvisual re-encoding, and multi-modal decoder modules. We tokenize the 3D scene into voxels, treat each token as an anchor point, andquery the text tokens to predict a token-level position likelihood and rotation matrix to locate the situational vector associated with thetextual description. Then we update the scene tokens with situational position encoding (PE), and finally perform the 3D VL reasoningtask with a large transformer decoder. this abstraction strategy: (1) A detection-based tokeniza-tion method tends to ignore the non-object regions in thescene, which can be indispensable in some reasoning sce-narios (e.g., carpets on the ground, ceiling, walls). (2) Afterobject-level abstraction, the visual representation losses thehigh-level information of the scene (e.g., the shape of theliving room, the corner of the kitchen). (3) A superviseddetector trained from scratch can only recognize objectswithin the training set (e.g., only 20 categories for Scan-Net ), meaning that the method does not have zero-shotcapability to reason about novel unseen objects that are in-evitably common in real-world scenarios. In light of these, we adopt a pretrained open-vocabularyvoxel-based tokenization method from OpenScene .The scene is first discretized into regular small 3D voxelsand fed into a visual encoder for feature extraction:",
  "z3D = E3D(V(p)),(1)": "where V represents the voxelization process, and E3D isa Minkowski sparse 3D convolutional network . Thesparse network is pretrained by distillation from CLIP embeddings of rendered multi-view 2D images, resultingin a feature map with better language alignment and 3Dawareness. We take the upsampled bottleneck-layer fea-ture embeddings from the encoder network, and computethe mean average over the z-axis (vertical) to project thevoxels onto the x-y plane and treat the feature grids in theresulting 2D feature map as our Nv visual tokens. We findthat this birds-eye-view projection results in a more com-pact representation and improves the final performance.",
  ". Situation Estimation": "Given 3D visual tokens z3D and situational tokens zS, ourobjective is to estimate the situational vector s referred to bythe situational description, which comprises a position com-ponent spos represented by coordinates (x, y, z), and a rota-tion component srot represented by Euler angles (, , ),where pitch angles are always defined as 0, meaning thatsituational vectors are defined to be parallel with the groundplane. The prior method utilizes a transformer block tocalculate the cross-attention feature between visual and lan-guage tokens, and directly regress a final situational vectorfrom the averaged attention map. We find such a strategyproducing very inaccurate estimates, as shown in ,due to the large search space in the entire 3D volume. In-spired by recent 3D object detection methods ,we reduce the search space by turning the localization prob-lem into a classification problem. Positional Embedding and Feature Fusion.After thevoxelization and 3D encoding process, each 3D token as-sociates with a 3D position (x, y, z) representing the centerof its voxel. We first provide positional information to themodel by generating learnable positional embeddings (PE)using a two-layer perceptron for each of the Nv visual to-kens, and add learnable positional embeddings to the tokenfeatures z3D. We use a situation interpreter to extractsituational information, and ask the updated visual tokensto attend to these situational tokens with several transformerlayers to produce the joint feature embeddings.",
  "Anchor-based Situation Estimation.We treat each out-": "put token of the feature fusion module as an anchor point,and use it to predict a position likelihood p and arotation estimation. Since each token has an associated 3Dposition (x, y, z), the position likelihood p indicates howlikely the situational vector locates at the center of this to-ken (voxel). We define a soft ground truth for this classifi-cation task with a Gaussian kernel, meaning that the closera token is to the actual situational vector spos, a higherground truth probability p will be assigned to that token.In order to counteract the sparse supervisory signal and in-crease the positive supervision around the vector position,we adopt the peak enlarging technique in CenterPoint ,where the size of the Gaussian kernel is increased (meaningthat the is increased) to allow denser supervision aroundthe vector position. Furthermore, we explore different ro-tation representation and find that compared with quater-nion and (sin , cos ) representations, the 6D vector pro-posed by achieves the best performance. Hence, weadopt a situation estimation head with MLP layers to out-put 7-dimensional vector for each of the tokens, where thefirst channel represents the position likelihood and the othersix channels represent the 6D rotation matrix. We take thecenter of the token with the peak position likelihood as ourestimated spos, and convert its corresponding 6D rotationvector as our estimated srot. The estimation can be equiv-alently represented as a rotation matrix R and a translationmatrix T. More discussion about the architecture and de-sign choices is in .3.",
  ". Situation-guided Visual Encoding": "After obtaining the situation estimation, we investigate abetter approach to enhancing the generation of downstreamresponses, inspired by human cognitive processes.Intu-itively, humans typically comprehend their immediate 3Denvironment by first interpreting their own situation inspace, and then discerning their surroundings from an ap-propriate viewpoint. Our model is designed to emulate thisnatural strategy. Using the situational vector s, we adjustthe coordinate system by repositioning the origin at spos,and reorienting the axes according to srot so that the new y-axis is aligned with the indicated direction. We keep thez-axis vertically oriented and project the situational vec-tors onto the x-y plane. This is in line with the format ofthe dataset , where situational vectors are assumed tobe parallel with the ground plane. Subsequently, we com-pute a new situation-guided PE for each of the Nv visualtokens, similar to the learnable 3D PE outlined in Sec-tion 4.2. They allow the model to grasp the positional in-terrelations from the perspective of the current situation.These situational embeddings are added to the output em-beddings of the situation estimation module, which consistsof blocks featuring self-attention layers for visual tokens,succeeded by cross-attention layers that bridge visual and situational information. This structure allows for the re-encoding of visual tokens under the influence of situationand question context, guiding the model to assign higherweights to situation-related and question-related visual to-kens. The output, termed situation-guided visual tokens,embodies this re-contextualized understanding.",
  ". Question Answering Head": "We follow existing methods to use a large vision-language decoder to fuse the final visual and textual to-kens and generate textual response to the input question.We explore both auto-regressive response generation andclassification-based answer prediction . For classi-fication, we predict a vector vans Rna for the candidatesof na answers in the training set following .",
  ". Analysis in 3D VQA Task": "We evaluated SIG3D for 3D VL reasoning on two chal-lenging benchmarks, addressing both visually-oriented sit-uation estimation and textual-focused QA tasks. We presenta detailed examination of the implementation strategiesadopted, the datasets employed, and the metrics applied inour research. For exhaustive understanding, implementa-tion, training details, and other additional information areavailable in the supplementary material. Datasets.We evaluate our method on SQA3D andScanQA , two challenging indoor 3D VQA datasets.Both datasets are derived from the ScanNet dataset ,serving as the foundational source for their 3D scenes.SQA3D features over 33K question-answer pairs for the 3DVQA task and 26K unique situational descriptions for thesituation estimation task. Each entry in this dataset includesa 3D scene point cloud, a situational description, a ques-tion, and pertinent annotations. ScanQA consists of over41K question-answer pairs, without situational descriptionsand situational annotations. We use it to demonstrate thegeneralizability of our method on general QA tasks. Weuse the splits provided by these datasets. Evaluation Metrics.For SQA3D, in order to comparewith baseline methods , we use a shallow trans-former decoder task head to perform the answer classifica-tion task, and evaluate the performance with exact matches(EM@1), which is equivalent to Top-1 answer accuracy. Wealso provide EM@1 on a breakdown of question types, in-cluding What, Is, How, Can, Which, and Other,based on the first word in the question sentence. Addition-ally, we evaluate situation estimation performance with lo-calization accuracy and orientation accuracy. In both tasks,we use accuracy within different distance or angle thresh-olds as our metrics. For example, means ac-curacy of location estimation when positive threshold is setto 0.5 meter. For ScanQA, we perform auto-regressive an-",
  "SIG3D39.512.435.913.468.8": ". Performance of SIG3D on the ScanQA dataset ison-par with the state of the art with large-scale text-3D pretrain-ing. VN and SR stand for VoteNet and ScanRefer, respectively.3D-LLM leverages pretrained 2D VL foundation models andLLM models , and is pretrained on a large-scale held-in 3D-text dataset before the finetuning on ScanQA.",
  "Baselines.Our study involves a comparative analysiswith a range of representative baselines on the SQA3Ddataset. In particular, we evaluate against GPT-3 , Clip-": "BERT , and MCAN , which are, as reported inprior work , baselines focused on language-only, 2Dvideo, and 2D image QA, respectively. For GPT-3, we fol-low SQA3D to convert the visual input into a captionusing Scan2Cap for LLMs to process. ScanQA represents a 3D QA baseline that ignores the situationalinput.Both SQA3D and Multi-CLIP employsituational descriptions and annotations for direct regres-sion tasks. LM4Vision utilizes LLMs as visual andtextual encoders. Additionally, 3D-VisTA undergoesa pretraining procedure on their large-scale 3D scene-textdataset, ScanScribe, prior to the finetuning on this dataset. Situation Estimation.As shown in , our work per-forms significantly better than the state of the art inboth localization and orientation estimation tasks. For 3D-VisTA , we use a pretrained model and finetune a newsituation head with the SQA3D dataset following . Wealso report a random baseline, in which we randomly sam-ple position and orientation from a uniform distribution asa lower-bound performance. Note that the original SQA3Dperforms only marginally better than the random baseline,meaning that it does not acquire any situational awareness,despite having the situation estimation loss. Disabling theQA task and asking the model to exclusively focus on thesituation estimation task results in a slight better perfor-mance. Our method, with the anchor-based position like-lihood estimation, results in much better understanding ofthe 3D situational relationship. Our method also outper-forms 3D-VisTA, which is pretrained on a large-scale 3D-text dataset, indicating that large pretraining alone is notenough to address the situational awareness problem. Notethat we do not include the random baseline performance re-ported in , because each value is obtained by generatingthree random values and taking the closest one to the groundtrue, and thus it does not reflect a true random baseline.",
  "Language Tokenizer / EncoderGloVe + LSTM 44.330.948.7SBERT - MiniLM 56.138.649.4SBERT - MPNet 55.940.649.7SBERT - MPNet (finetune)59.142.550.9": ". Performance of SIG3D improves with stronger visualand language encoders. We find that the open-vocabulary pointencoder and MPNet-based sentence BERT (SBERT) leads to thebest performance. and Acc@30 stand for local-ization and orientation accuracy in the situation estimation task,respectively. EM@1 demonstrates the exact match metric in theQA task. methods in most question breakdown categories and over-all accuracy, as shown in .Our work achievesleading results without large-scale pretraining (comparedwith 3D-VisTA) and LLMs (compared with GPT-3), indi-cating its superiority in situational awareness. Note that theLLM baseline GPT-3 achieves the best performance on theWhat category, suggesting the potential of a stronger lan-guage encoder in interpreting complicated questions.",
  ". General Question Answering on ScanQA": "Baselines.We compare with 2D image VQA MCAN-based baselines , ScanQA , 3D-LLM whichleverages large-scale pretrained 2D VLMs and LLMs asbackbone models, and 3D-VisTA pretrained on theirproposed large-scale 3D-text dataset. Question Answering.As shown in , despite thatthe questions do not explicitly require situational under-standing to answer in ScanQA, SIG3D achieves comparableresults with state-of-the-art methods without the large-scale3D-text pretraining and powerful 2D VLM and LLM back-bone models. Our work pretrained on SQA3D leadsto higher performance on BLEU-1, BLEU-4, and ROUGEmetrics, showing its generalizability on general 3D QA sce-narios.",
  ". Ablation Study and Analysis": "Vision and Language Encoders.We study the im-pact of different visual and textual tokenizers in .It is observed that the open-vocabulary visual encoder(OpenScene) outperforms detection-based encoders (suchas VoteNet and 3DETR) across all metrics. This superiorperformance of OpenScene is attributed to the limitationsof 3D detectors, which are typically trained on a limited setof object categories, rendering them less effective in recog-nizing novel objects mentioned in textual prompts. Regard-ing language encoders, our findings indicate that a strongerbackbone correlates with better performance, primarily dueto its improved capability to interpret complex textual in-puts. This leads to the suggestion of integrating LLMs withour method to potentially further enhance performance, anavenue we intend to explore in future research. Situational Awareness.In we verify the crucialrole of situational awareness in the 3D VL task. Firstly, weshow that 3D PE, 6D rotation estimation, and anchor-basedposition estimation all lead to much better position and ori-entation estimation performance. We further establish thatsituational PE and visual token re-encoding modules leadto better utilization of the predicted situational vector forthe QA task. Additionally, We design two oracle modelsunder the assumption of having access to the ground truthsituational vector as input. The outcomes from these mod-els reveal a critical insight: the model fails to effectivelyinterpret situational information when it is directly incorpo-rated into the input visual embeddings. This underlines the",
  "necessity of the intermediate representation and encodingmechanism we have proposed, affirming its importance inachieving optimal 3D VL task performance": "Architectural Design.We explore different architecturaldesign choices of our model in . We find that thenumber of visual tokens sampled from the visual featureembeddings affects the performance of both situation es-timation and QA tasks. Sampling fewer visual tokens in-creases the risk of missing the region of significance, whilesampling more does not lead to a better performance aswell. We study the size of voxels and find 0.02m to be themost effective choice, as the OpenScene backbone ispretrained with the same voxel size. We also find that the(sin , cos ) and 6D vector representations perform a lotbetter than quaternion in the rotation estimation task. Thisis consistent with the finding reported in .",
  ". Qualitative Analysis": "Finally, we demonstrate some qualitative results of ourSIG3D in . We show the ground truth and esti-mated situational vectors in red and blue, respectively, intheir corresponding 3D scenes. We also print the answerswith a red cross or green checkmark indicating the correct-ness. It is clear that our method performs significantly betterin situation estimation tasks, resulting in vectors very closeto the ground truth in both position and orientation perspec-tives. Better situational awareness also aids the complicated",
  ". Conclusion": "In this paper, we introduce SIG3D, a situation-aware visionlanguage model for 3D reasoning tasks. We propose to rep-resent 3D scenes as feature tokens, treat tokens as anchorpoints to estimate a situational vector from a textual descrip-tion, and use the estimated situation as guidance to alignand re-encode the visual tokens to enhance the features forreasoning tasks. We observe consistent and significant per-formance gains on both situation estimation and questionanswering tasks.",
  "Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, MohamedElhoseiny, and Leonidas Guibas. ReferIt3D: Neural listenersfor fine-grained 3D object identification in real-world scenes.In ECCV, 2020. 3": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-toine Miech, Iain Barr, Yana Hasson, Karel Lenc, ArthurMensch, Katie Millican, Malcolm Reynolds, Roman Ring,Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,Sina Samangooei, Marianne Monteiro, Jacob Menick, Se-bastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.Flamingo: A visual language model for few-shot learning.In NeurIPS, 2022. 1, 2, 6",
  "Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Mo-toaki Kawanabe. ScanQA: 3D question answering for spatialscene understanding. In CVPR, 2022. 2, 3, 5, 6, 7": "Satanjeev Banerjee and Alon Lavie. METEOR: An auto-matic metric for MT evaluation with improved correlationwith human judgments. In Proceedings of the ACL Workshopon Intrinsic and Extrinsic Evaluation Measures for MachineTranslation and/or Summarization, 2005. 6 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, TomHenighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, Alec Radford,Ilya Sutskever, and Dario Amodei.Language models arefew-shot learners. In NeurIPS, 2020. 1, 2, 3, 6",
  "Zhenyu Chen, Ali Gholami, Matthias Niener, and Angel XChang. Scan2Cap: Context-aware dense captioning in RGB-D scans. In CVPR, 2021. 3, 6": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham,Hyung Won Chung,Charles Sutton,Sebas-tianGehrmann,ParkerSchuh,KensenShi,SashaTsvyashchenko, Joshua Maynez, Abhishek Rao, ParkerBarnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, JamesBradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, SanjayGhemawat,Sunipa Dev,Henryk Michalewski,XavierGarcia, Vedant Misra, Kevin Robinson, Liam Fedus, DennyZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,Barret Zoph, Alexander Spiridonov, Ryan Sepassi, DavidDohan, Shivani Agrawal, Mark Omernick, Andrew M.Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,Aitor Lewkowycz, Erica Moreira, Rewon Child, OleksandrPolozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean,Slav Petrov, and Noah Fiedel.PaLM: Scaling languagemodeling with pathways. arXiv preprint arXiv:2204.02311,2022. 1",
  "Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang,Song Bai, and Xiaojuan Qi. PLA: Language-driven open-vocabulary 3D scene understanding. In CVPR, 2023. 3": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image isworth 16x16 words: Transformers for image recognition atscale. In ICLR, 2021. 2 Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, ShuohangWang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, LuYuan, Nanyun Peng, Zicheng Liu, and Michael Zeng. Anempirical study of training end-to-end vision-and-languagetransformers. In CVPR, 2022. 2 Mingtao Feng, Zhen Li, Qi Li, Liang Zhang, XiangDongZhang, Guangming Zhu, Hui Zhang, Yaonan Wang, and Aj-mal Mian.Free-form description guided 3D visual graphnetwork for object grounding in point cloud. In ICCV, 2021.3",
  "Here we provide more details about our model": "Visual and Textual Encoders.We use OpenScene (the 3D distilled variant) as our visual encoder, which incor-porates distilled CLIP features into a 3D Minkowski CNNbackbone originally designed for the 3D semantic segmen-tation task. We use the default 0.02m voxel size to dis-cretize a point cloud into 3D voxels and disable the scal-ing and elastic distortion augmentation methods during thevoxelization process. The 3D architecture is the predefinedMinkUNet18A . The number of visual tokens Nv is 256.Additionally, we use the Sentence-BERT MPNet vari-ant as our text tokenizer and encoder. We use the fixed batchpadding strategy and set the length to be 100 for both situ-ational tokens Ns and question tokens Nq. The feature em-bedding sizes for all three types of tokens are set to 768.The 256 dimensional output of the OpenScene backbone isprojected to the 768 hidden size with a 1x1 Conv layer. Wefreeze the OpenScene backbone, and finetune only the lastlayer of the textual backbone during our training process. Fusion and Decoder Models.We use a 4-layer MCANTransformer as the fusion block of the visual andsituational tokens. The learnable positional embeddings andsituational embeddings are composed of a 2-layer MLP:first from dimension 3 to 128, and then from 128 to thetarget dimension. We use BLIP-2 as the large multi-modal tranformer for final response generation, simiar to3D-LLM . Training Details.We train our model with theAdamW optimizer with 1 = 0.9, 2 = 0.999, and = 1e-8. We use a batch size of 16 and set the initial learn-ing rate to be 2e-5. The weight decay is set to 0.05, andwe disable the weight decay on the layernorm layers and allbias parameters following . We decrease the learningrate by 10 times after the 10-th and the 20-th epochs. Wetrain the model for a total of 50 epochs on a single NVIDIAA100 GPU.",
  "B. Enhanced Vision Token Activation ThroughSituational Re-encoding": "In Figure B, we provide an insightful visualization of theactivation changes in 3D visual tokens z3D, before andafter undergoing our situation-guided visual re-encodingprocess. This visualization employs the viridis colormap,where a brighter token representation indicates a higher ac-tivation value. The effectiveness of situational guidance inamplifying the relevance of crucial tokens is evident from",
  "Figure A. Visualization of cases where situation prediction bene-fits the most. Arrow colors: GT, SIG3D, and SQA3D predictions": "this depiction.For example, the visualization in the second row revealsa notable shift in focus. Initially, the tokens predominantlyconcentrate on the bed area. However, after re-encoding,there is a discernible shift in attention towards areas closelyaligned with the situational vector and those directly relatedto the query. Similarly, in the third row, the situational re-encoding process results in the window region on the leftreceiving increased emphasis. In the fourth row, the atten-tion initially focuses on the vanity region. Then it shifts tothe toilet on the left of the agent, as suggested by the situa-tional vector and the question prompt. This experiment pro-vides a clear demonstration of how our method, using en-hanced situational awareness, contributes to improved per-formance in downstream reasoning tasks in an explainablemanner. The ability of our model to dynamically adjust fo-cus in response to situational cues is a key factor in its en-hanced reasoning capabilities.",
  "C. More Qualitative Results": "We show more qualitative results of our model in Figures Cand D. Visualization encompasses a diverse array of tasks,including queries about object orientation, characteristicsof specific objects, the count of objects within a scene,and yes/no questions based on commonsense reasoning. Akey observation from these results is that, in numerous in-stances, absolute precision in situation estimation is not aprerequisite for our model to accurately deduce the answersto the posed questions. This finding highlights the modelsrobustness and its capacity to effectively handle a variety ofquery types, even with less optimal situational awareness.",
  "We perform a failure case analysis on our model in Figure E.We categorize and visualize three types of failure cases": "Accurate Situation Estimation, Incorrect Question An-swering.This scenario demonstrates that accurate situa-tional understanding does not necessarily guarantee correctresponses to queries. A significant proportion of failureswithin this category can be attributed to complex questionprompts that demand multi-stage reasoning or the integra-tion of commonsense knowledge. For instance, the initial example necessitates the models comprehension of the spa-tial relationship between the viewers perspective and thecouch, followed by an additional reasoning phase focusedon the couch to accurately respond to the query. The sub-sequent example demands an understanding of the conceptsof odd and even, and their application to the count ofobjects in a 3D environment. Inaccurate Situation Estimation, Correct Question An-swering.This category reveals that errors in situationestimation are more likely when the scene description in-volves minor or less common objects. Furthermore, it isobserved that the model might incidentally arrive at the cor-rect answer without fully grasping the complex situational",
  "Figure E. We demonstrate three different categories of failure cases": "and multi-modal context, particularly in cases where thequestion involves choosing between two or among multiplegiven options. Therefore, a blend of qualitative and quanti-tative assessments is crucial for a comprehensive evaluationof the models performance. Both Situation Estimation and Question Answering areIncorrect. This group contains the most challenging ex-amples from the dataset, typically encompassing multiplecomplexities identified in the preceding categories. Thesecases present a compounded difficulty level, highlightingthe models limitations in scenarios that require an intricateunderstanding of both situational context and question in-",
  "F. Limitations and Future Work": "Selection of 3D Scenes.The SQA3D andScanQA datasets, both derived from the ScanNet dataset, exclusively feature indoor household environments.These static scenes limit the models applicability to dy-namic tasks like manipulation and exploration.Conse-quently, our current model is tailored to static householdsettings. This scalability problem is a long-standing chal-lenge for all existing 3D VL reasoning work .We believe that with a more scalable visual representation",
  "(e.g., scene graphs, sparse learnable embeddings), we canextend our model to support larger 3D environments in thefuture work": "More Comprehensive Visual Encoding.In our ap-proach, the utilization of a voxel-based, open-vocabulary3D encoder achieves much better overall performance.Nevertheless, for specific queries involving counting or ref-erencing, a detection-based encoder may yield a more ad-vantageous visual token set, owing to its capacity to provideinstance-level information pertinent to the questions. Thisindicates the potential benefits of a multifaceted visual tok-enization system that amalgamates the strengths of variousencoder types."
}