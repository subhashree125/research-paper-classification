{
  "Abstract": "Event sensors offer high temporal resolution visual sens-ing, which makes them ideal for perceiving fast visual phe-nomena without suffering from motion blur. Certain appli-cations in robotics and vision-based navigation require 3Dperception of an object undergoing circular or spinning mo-tion in front of a static camera, such as recovering the an-gular velocity and shape of the object. The setting is equiv-alent to observing a static object with an orbiting camera.In this paper, we propose event-based structure-from-orbit(eSfO), where the aim is to simultaneously reconstruct the3D structure of a fast spinning object observed from a staticevent camera, and recover the equivalent orbital motion ofthe camera. Our contributions are threefold: since state-of-the-art event feature trackers cannot handle periodic self-occlusion due to the spinning motion, we develop a novelevent feature tracker based on spatio-temporal clusteringand data association that can better track the helical trajec-tories of valid features in the event data. The feature tracksare then fed to our novel factor graph-based structure-from-orbit back-end that calculates the orbital motion parame-ters (e.g., spin rate, relative rotational axis) that minimizethe reprojection error. For evaluation, we produce a newevent dataset of objects under spinning motion. Compar-isons against ground truth indicate the efficacy of eSfO.",
  ". Introduction": "Three-dimensional (3D) perception is a fundamental visioncapability . Recent works have focused on the useof neuromorphic event sensors for 3D perception tasks,such as visual odometry (VO) , structure-from-motion (SfM) and simultaneous localization and mapping(SLAM) . Event sensors offer several advantagesover conventional cameras, such as high temporal resolu-tion, low power and low data rate asynchronous sensing.Event sensors also offer a higher dynamic range, enablingthem to see more details in difficult lighting conditions.",
  "*SmartSat CRC Professorial Chair of Sentient Satellites": ". In eSfO, we exploit the equivalence between a staticevent camera observing a spinning object, and an orbiting eventcamera observing a static object. This enables us to jointly esti-mate the motion parameters (e.g., spin rate, rotational axis relativeto the camera), as well as the sparse structure of the object. However, the advantages of event sensors come with cer-tain limitations. Firstly, due to the asynchronous nature ofevents, the idea of a consistent local neighborhood for apixel across multiple views no longer holds. Moreover, im-age gradients a fundamental invariant in intensity imagesoften used as basis for feature detection, description, andtracking are not observable in event data. This makes fea-ture detection and tracking challenging, which represents amajor obstacle towards event-only SLAM systems.In this work, we formulate the task of event-basedStructure-from-Orbit (eSfO), which entails the reconstruc-tion of an object undergoing circular motion while beingobserved by a static event camera, and jointly estimatingthe motion of the object; see . An object is under-going a circular motion if it is rotating about a fixed axis,a.k.a. spinning.Since observing a spinning object froma static camera is mathematically equivalent to observinga static object from an orbiting camera, the motion of thespinning object can be recovered as the motion of the orbit-ing camera. Using an event sensor to observe the spinning",
  "arXiv:2405.06216v1 [cs.CV] 10 May 2024": "target is compelling, since the effect of motion blur is alle-viated, especially in cases with high angular velocity.SfM of a spinning object, usually placed on a turntable,has been studied previously . However, these earlyworks assume known angular rates and solve the task ofstructure recovery only. Nonetheless, various e-commerce,multimedia and augmented reality applications can benefitfrom such a 3D reconstruction approach .Outside of turntable-induced rotations, spinning objectscan be observed naturally in the world, such as race-carsspinning in place allowing opportunistic observation ofthe complete structure. Space is another domain abundantwith naturally spinning objects, from as large as celestialbodies such as planets and asteroids to as small as satellitesand space debris. For instance, as most small bodies such asasteroids tend to spin about the major principal axis, manyworks leveraged the single-axis spin assumption for 3D re-construction purposes . Such motion character-istics are also observed on some man-made objects freelyrotating in inertial space . Indeed, Kaplan et al. re-ported in 2010 that there are over 100 expired satellites ingeosynchronous orbits (GEO) spinning at high angular rates(tens of RPM) as they retain the angular momentum fromspin-stabilization during service. Extracting the shape andmotion parameters of such spinning objects in space can fa-cilitate vision-based spacecraft navigation and for-mation flying . ContributionsMonocular eSfO is challenging since cur-rent event-based feature detection and tracking methodsare not reliable on spinning objects that periodically self-occlude.Moreover, the underlying structure-from-orbit(SfO) problem imposes more constraints than SfM, and SfOhas not been satisfactorily tackled in the literature.Ourwork addresses the difficulties by contributing: A novel eSfO formulation that takes into account theproblem structure induced by a spinning object (Sec. 3).",
  ". Event-based Vision": "Event sensors have been incorporated into various SLAMand VO pipelines, due to their low data rate and high dy-namic range . They are normally paired with other sen-sors such as image sensors and IMUs for fast mo-tion scenarios. The primary benefit of such an approach is to allow feature detection in the image space and featuretracking during the blind-time of the image sensor. To re-solve feature associations, EVO combines two eventsensors in a stereo configuration to enable feature matchingacross the two sensors. Similarly, EDS pairs a monocu-lar camera with an event sensor for event-aided direct sparseodometry. Various learning based approaches also use eventcameras for optical flow estimation .Despite its usefulness in robotics and vision-based nav-igation, event-only SfO has not received a lot of attentionin the literature. Rebecq et al. demonstrated visual-inertial odometry (VIO) with an event camera on a spin-ning leash sequence, which was obtained by spinning anevent camera attached to a leash. However, the single resultwas evaluated only qualitatively [44, ]. Moreover, be-ing VIO, their method requires an onboard IMU.",
  ". Event-based Feature Tracking": "The first step in many VO pipelines is the feature detectionand subsequent tracking across frames, which has provento be difficult for the event-only case. Methods for cornerdetection have repurposed image feature de-tection methods to the event sensor. For feature tracking,methods either assume that known detections in the form oftemplates or use other sensors (such as images) forfeature detection . Several methods have also exploredlearning based feature detection and tracking mechanisms to compensate for the lack of image gradientsand consistent pixel neighborhood across multiple view.A serious challenge posed by a spinning object is the pe-riodic self-occlusion unavoidably affecting features on thesurface of the object. As we will show in Sec. 7, this leadsto poorer quality tracks by existing methods. By reason-ing over the spatio-temporal space of the event sensor todetect clusters of corner events that occur together and con-nect them over time to arrive at meaningful feature tracks,our method produces more accurate tracks.",
  ". SfM for Single-axis Rotation": "The SfO problem explored in this work finds its roots in theearlier days of research into the SfM problem of single-axis rotation in which different methods explored recon-struction of objects placed on a turntable with a known ro-tational rate . These were further extended to in-clude auto-calibration and recovery of camera poses alongwith the object reconstruction . The problem has madea reappearance recently in an in the wild context where opportunistic turntable sequences allow object cover-age due to the rotational motion using frame-based cameras.While their approach relies heavily on learning based tech-niques to form an implicit model of the object, our focusis more on geometric methods that require no prior train-ing. The method closest in application to ours is that of",
  ". L: Problem setup. R: Visualization of eSfO parametersin the orbit view of the problem. w is an arbitrary world frame": "Chen et al. , who presented a dense reconstruction sys-tem using simulated event input, similar to a turntable se-quence. The method generates dense object representationsusing a learning based framework, however, our focus ison estimating the rotational rate and axis as opposed to re-constructing a detailed model of the object. Similarly, thework of Baudron et al. follows a shape from silhouetteapproach using an event sensor, mainly reconstructing theobject shape. Although Tweddle et al. formulate a sim-ilar problem in a similar setting, their work differs in thatit focuses on modelling the motion of the object observedwith stereo RGB cameras. To our best knowledge, our workis the first to explore an event-only solution to recover thecameras orbital motion parameters over time jointly with asparse 3D structure.",
  ". Event Sensors for Space Applications": "In addition to high effective frame rate and high dynamicrange, power consumption in milli-Watt levels makes eventsensors attractive as an onboard sensor.Event sensorshave already been applied to problems such as star tracking, satellite material characterization as wellas the general context of space situational awareness .In addition, there are several datasets for the application ofevent-based satellite pose estimation in space .",
  ". Event-based Structure-from-Orbit": "eSfO represents a special case of event-based SfM in whichadditional constraints are placed on the poses of the cam-eras. As alluded to in Sec. 1, observing a spinning objectwith a static camera is equivalent to observing a static objectwith an orbiting camera; see supp. material for the proof.The raw input stream from the event camera E={ei}NEi=1, where each ei = (ti, xi, yi, pi), consists of theevent pixel-locations (xi,yi), timestamp (ti) and binary po-larity (pi). SfO recovers a sparse point cloud representationof the object X = {xwp R3|p = 1 . . . NP } along with thefollowing orbital parameters (see ):",
  "tw(; ) = r cos (2f)u + r sin (2f)v + cw,(1)": "where v = n u lies within the plane and is mutuallyperpendicular to both n and u. denotes the rest of the pa-rameters. The orientation of the camera can be decomposedinto two rotations: R0 which is a constant rotation with re-spect to the orbital plane that allows the camera to look atthe object center, and Rn, is the in-plane rotation around nat time induced by the objects rotation. This fully charac-terizes the orbital motion of the sensor. Compared to a gen-eral SfM problem involving N cameras, where each camerahas 6 DoF resulting in 6N parameters for the camera posesalone, the proposed formulation has a fixed number of mo-tion parameters in this minimal representation (14 in total;see above) regardless of the number of cameras, due to theorbital constraint. Additionally, there are 3NP parametersrequired for the estimated landmarks.The proposed eSfO pipeline is depicted in .Afront-end takes in the raw event stream and performs fea-ture detection and tracking (Sec. 4). In the back-end, the setof tracks is used to recover an initialization using a general-ized SfM formulation (COLMAP ). This initializationis upgraded using the eSfO optimization to conform to theorbital model of the problem (Sec. 5).",
  ". Event Feature Detection & Tracking": "Event-only feature detection and tracking is an inherentlydifficult problem due to the lack of consistent neighborhoodstructure and the asynchronous nature of the sensor. How-ever, the temporal aspect of the event stream provides ad-ditional information to establish continuity and proximityover time. To address the problem of feature detection andtracking, we exploit densely clustered corner events in thespatio-temporal space. Our approach, Event Tracking byClustering (ETC), hinges on two key observations: (1) Thespatio-temporal density of corner events serves as a depend-able metric for feature detection, as it is induced by the 3Dstructure of the scene; and (2) The spatio-temporal proxim-ity of corner events indicates whether these events originatefrom the same 3D point in space.Based on these observations, we propose a feature detec-tion and tracking mechanism taking advantage of the spatio-temporal nature of the event stream.",
  ". Spatio-temporal Feature Detection": "Given E, the first step in ETC is to detect corner eventsusing the eFAST corner event detector . eFAST lever-ages the Surface of Active Events (SAE) representation ofthe event stream. An event ei is a corner when a certainnumber of neighboring pixels, located along a circular patharound (xi, yi), exhibit either consistently darker or brighterintensities in the SAE image; see for details.In our method, the detected corners are further filteredbased on their local neighborhood density score, defined as",
  "eqEiI(pi = pq),(2)": "where Ei E is the set of events within distance from ei, pq is the polarity of a neighboring event eq ={tq, xq, yq, pq}, and I is the indicator function that returns 1if its input condition is true and 0 otherwise.Event cameras have different positive and negative po-larity bias settings, leading to differing sensitivities for pos-itive and negative polarity events and thus differingdensities of corner events for the same brightness inten-sity edge. We therefore compare the density score D(ei)of a corner ei with the mean density score D of all de-tected corners of the same polarity as ei, and remove eiif D(ei) < D. This helps identify corner event clusters,which subsequently serve as candidate feature tracks.",
  ". Spatio-temporal Feature Clustering": "We utilize our second observation to refine feature detec-tions and track them by employing HDBSCAN , a non-parametric, density-based clustering algorithm which iden-tifies clusters by discovering regions of high data point den-sity. HDBSCAN is applied to the remaining corner eventsin the spatio-temporal space, disregarding polarity.Thisgenerates NH clusters of corner events, Ci, i = {1 . . . NH}that lay the foundation for our feature tracks. Due to noiseand the speed of motion in the scene, the tracks belongingto the same feature may be identified as disjoint clusters. Tojoin clusters that are spatio-temporally close to each other,we apply nearest neighbor head and tail matching within a spatio-temporal hemisphere (since time can only move for-ward). The head represents the start of a cluster and tailrepresents the end. To connect clusters Ci and Cj, the meanof the last and respectively first N events spatio-temporallocations is computed and used as the descriptors for theclusters. Let these spatio-temporal descriptors be given byD = (t, x, y) and D = (t, x, y) for the tail andhead of clusters Ci and Cj respectively. The cluster Ciis connected to the cluster Cj with the smallest euclideandistance between D and D if it lies within the spatio-temporal hemisphere of radius defined by D:",
  ". Feature Track Extraction": "To extract feature tracks from the feature clusters, we dividethe duration of the event stream, T = tNE t1, into K =(T/t) 1 mutually disjoint temporal windows Wk, k ={1 . . . K} of duration t each such that the k-th windowspans the time duration (kt, (k + 1)t]. For each clusterCp, p = {1 . . . NP }, we identify the events that fall withinthe time window Wk: Ep,k = {ep|ep Cp and tp Wk}and compute the mean pixel location within the window,giving us the position of the p-th feature track at time tk =kt:",
  ". Initialization": "The NP feature tracks generated using feature tracking pro-vide the required information needed to run a general SfMpipeline. Each of the tracks correspond to a 3D point inthe world frame xwp , whos projection at time tk is the ob-served feature f ptk. Each of the K time windows correspondto a camera. Using this information, we generate an initialset of cameras and a point cloud using COLMAP whichminimizes the reprojection error for each 3D-2D correspon-dence (xwp , f ptk) over the K cameras and NP world points:",
  "p=1||(KRtkw xwp + Kttkw ) f ptk||(5)": "where tk = kt, (.) is the pin-hole projection and K is theintrinsic calibration matrix. COLMAP provides an initialset of camera positions and 3D points, but since no con-straints about the orbital trajectory were enforced in thisgeneral SfM pipeline, the camera trajectory deviates signif-icantly from a circular path, see (COLMAP). How-ever, initial values for the SfO parameters can be computedfrom this intermediate solution.We compute the best fitting circle to the camera centerstwtk, k = {1 . . . K} generated by COLMAP by estimatingn, c, u and the radius r. To compute the normal vector n,we compute the mean of the camera centers tc = 1",
  "T =twt1 tc,twt2 tc,. . .twtk tcT(6)": "We solve [T[:,1:2] 1K1]n = T[:,3] using least squares andnormalize the resulting vector to obtain n. The notationT[:,p:q] selects all the rows and the p to q columns from thematrix T and 1K1 is a K 1 matrix of ones.Camera centers are then projected to the computed planeto estimate the circle. The projection of the camera centertwtk onto the plane is denoted as twtk = [Rod(n,z)twtk][1:2],where Rod(a, b) denotes the Rodriguez rotation matrixthat rotates points about an axis k = a b by an angle = arccos (aT b). The least square fit is found for the pa-rameters of the circle c R3 using the implicit equationof a circle. Using",
  "u = twt1 cw": "where the (.) notation is used to access elements in the vec-tor and z = (0, 0, 1)T .Finally, for an estimate of the frequency, we employ theFourier Transform.We divide the duration of the eventstream, T = tNE t1, into F = (T/tf) 1 mutuallydisjoint temporal windows Wf, f = {1 . . . F} (each of du-ration tf) such that the f-th window spans the time dura-tion (ftf, (f + 1)tf]. Within each window, we computethe mean value of the x location of all the events.",
  ". Solving for eSfO parameters": "The eSfO parameterization consist of two sets of parame-ters: global parameters that capture the overall structure ofthe problem and local time-dependent observations of a 3Dpoint in the world. The factor graph (See ) thereforeincludes dependencies between each observation inducedby these global parameters.The formulation in its essence is a reprojection errorminimization formulation similar to SfM, but differs in that",
  ". Dataset": "For performance evaluation, we created a dataset of ob-jects placed on a turntable and observed by a static eventcamera (). Our daTaset Of sPinning objectS withneuromorPhic vIsioN (TOPSPIN) consists of six ob-jects under three rotational speeds and four perspectives(Tab. 1). For each scene, one of the objects was placed onthe turntable and rotated at a given speed, while the cameraobserved it from a given perspective. We exhaustively gen-erated 72 scenes using the combinations in Tab. 1. Severalsample event frames are displayed in .",
  ". Front-end evaluation": "Event Camera DatasetTo evaluate ETCs performanceagainst other state-of-the-art methods, we used the EventCamera Dataset and the corresponding benchmarkingstrategy outlined in : using ground truth camera poses,we triangulated the 3D points using the estimated featuretracks. The estimated points were then projected to eachground truth camera and the reprojection error was com-puted against the tracked feature point. Due to lack of a pub-lic implementation, results for eCDT are reported from theirwork . eCDT without the HT Matching module is de-noted as eCDT (w/o HT). We initialized HASTE with thefeature detections from our method, as it is only a trackingmethod. Metavision used the pretrained weights fromtheir original work. Each dataset was evaluated with an er-ror threshold of 3, 5 and 7 pixels. Performance was thenmeasured as the Root Mean Squared Error (RMSE) of the",
  ". Mean feature age and mean reprojection error computed acrossshapes, poster and boxes": "projection error for each feature track and reported in Tab. 3.Results for the feature age, the amount of time a feature issuccessfully tracked, are reported in Tab. 4. eCDT had thelongest mean feature age, but suffered from a high meanreprojection error: their feature tracks were longer but lessaccurate. Conversely, the Metavision feature tracker hadextremely short feature tracks, which severely diminish itsusefulness. The ideal method would produce tracks whichhave both low reprojection error and longer feature age like the behavior seen for HASTE and ETC (). Event feature tracking for spinning objectsHowever,the underlying assumption of HASTE about the fixed (pla-nar) tracking template during tracking is easily violated inthe SfO setting, as observed when HASTE is evaluated onour dataset. shows feature tracks (colored) overlaidon accumulated corner events during a complete object rev-olution. Due to the objects rotation, the points on the objecttrace out an ellipse in the image . HASTE tracks divergefrom the elliptical arcs due to tracking failures. This is alsoreflected in the resulting COLMAP reconstruction, with amean reprojection error of 348.48 pixels for the HASTEtracks. In contrast, our method exhibited more robust track-ing performance under the same conditions, with a meanCOLMAP reprojection error of 2.39 pixels. See supplemen-tary material for the full quantitative results.Note also that the feature age depends on the rotationalvelocity of the object (b). For faster motion, self-occlusion occurs faster than when observing slowly rota-tioning objects leading to a shorter mean feature age.The results for the front-end feature detection and track-ing show that our method is well-suited for the eSfO prob-lem and compares favorably to other state-of-the-art meth-ods on traditional benchmarks.",
  "(b)": ". (a) Estimated frequency for the fast, med & slowspeeds across all scenes.(b) Distribution of mean feature agefor the feature tracks from our method for all scenes; scenes aregrouped by their speed (fast, med & slow). which depicts the average estimated frequency for the dif-ferent speeds of the turntable. Observe that the initializationprovided from the Fourier Transform is further refined byeSfO, bringing the estimate closer to the ground-truth. Is eSfO effective?To demonstrate the efficacy of eSfO atresolving the camera poses and structure, we first look athow the eSfO optimization behaves in the constrained or-bit estimation problem. We report the average reprojectionerror for the eSfO reconstruction in a, and those forthe COLMAP reconstruction in b. These plots showincreased error for eSfO compared to COLMAP. However,eSfO imposes further constraints on camera poses to alignthem to a circular trajectory. This leads to more residualerror after optimization, as not all constraints can be fullyresolved. However, when comparing the generated trajec-tories from both (), it is evident that eSfO recovers abetter orbital trajectory compared to COLMAP.To investigate the effect of eSfO on point reconstruction,we aligned the point clouds with their respective 3D CADmodels, initially using a manual alignment strategy and fur-ther refining it using an Iterative Closest Point (ICP) basedoptimization scheme. The results () show that the es-timated point clouds are a highly accurate, albeit sparse,rendition of their respective 3D CAD model. Additionalconstructed trajectory and models are shown in the supple-mentary material. Axis of rotation estimationWe present qualitative re-sults for the projection of the axis of rotation into the imageplane, referred to as the screw line. This is the projectionof the vector n at the center cw of the circle, onto the imageplane. As seen in , the estimated screw line clearlymarks out the axis of rotation of the object over the accu-mulated event frame suggesting that the plane normal andcenter of the circle were recovered correctly.",
  ". Conclusion": "In this work, we have presented a new reconstruction prob-lem, eSfO, which recovers a sparse representation of an ob-ject rotating about a fixed axis and observed by a static eventcamera. Through extensive experiments, we have demon-strated that the frequency, screw line, camera pose, and re-liable sparse reconstruction can be recovered using the pro-posed pipeline. The dataset has been released publicly ,and the code for eSfO can be found here:",
  "Saeed Afshar, Andrew Peter Nicholson, Andre Van Schaik,and Gregory Cohen. Event-based object detection and track-ing for space situational awareness. IEEE Sensors Journal,20(24):1511715132, 2020. 3": "Ignacio Alzugaray and Margarita Chli.Haste:multi-hypothesis asynchronous speeded-up tracking of events. In31st British Machine Vision Virtual Conference (BMVC2020), page 744. ETH Zurich, Institute of Robotics and In-telligent Systems, 2020. 2, 7 Santarshi Bandyonadhyay, Issa Nesnas, Shvam Bhaskaran,Beniamin Hockman, and Benjamin Morrell.Silhouette-based 3d shape reconstruction of a small body from a space-craft.In 2019 IEEE Aerospace Conference, pages 113,2019. 2 Patrick Bardow, Andrew J Davison, and Stefan Leuteneg-ger. Simultaneous optical flow and intensity estimation froman event camera.In Proceedings of the IEEE conferenceon computer vision and pattern recognition, pages 884892,2016. 2",
  "Philippe Chiberre, Etienne Perot, Amos Sironi, and VincentLepetit.Long-lived accurate keypoints in event streams,2022. 2, 6, 7": "Tat-Jun Chin, Samya Bagchi, Anders Eriksson, and AndreVan Schaik. Star tracking using an event camera. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition Workshops, pages 00, 2019. 3 Gregory Cohen, Saeed Afshar, Brittany Morreale, TravisBessell, Andrew Wabnitz, Mark Rutten, and Andre vanSchaik.Event-based sensing for space situational aware-ness. The Journal of the Astronautical Sciences, 66:125141,2019. 3",
  "Kaitlin Dennison, Nathan Stacey, and Simone DAmico.Autonomous asteroid characterization through nanosatelliteswarming. IEEE Transactions on Aerospace and ElectronicSystems, 59(4):46044624, 2023. 2": "Mehregan Dor, Katherine A. Skinner, Panagiotis Tsiotras,and Travis Driver. Visual slam for asteroid relative naviga-tion. In 2021 IEEE/CVF Conference on Computer Visionand Pattern Recognition Workshops (CVPRW), pages 20662075, 2021. 2 P. Eisert, E. Steinbach, and B. Girod.Automatic recon-struction of stationary 3-d objects from multiple uncalibratedcamera views. IEEE Transactions on Circuits and Systemsfor Video Technology, 10(2):261277, 2000. 2",
  "Vincent Fremont and Ryad Chellali. Turntable-based 3d ob-ject reconstruction. In IEEE Conference on Cybernetics andIntelligent Systems, 2004., pages 12771282. IEEE, 2004. 2": "Guillermo Gallego, Tobi Delbruck, Garrick Orchard, ChiaraBartolozzi, Brian Taba, Andrea Censi, Stefan Leutenegger,Andrew J Davison, Jorg Conradt, Kostas Daniilidis, et al.Event-based vision: A survey. IEEE transactions on patternanalysis and machine intelligence, 44(1):154180, 2020. 1 Daniel Gehrig, Henri Rebecq, Guillermo Gallego, and Da-vide Scaramuzza. Asynchronous, photometric feature track-ing using events and frames. In Proceedings of the Euro-pean Conference on Computer Vision (ECCV), pages 750765, 2018. 2 Daniel Gehrig, Henri Rebecq, Guillermo Gallego, and Da-vide Scaramuzza. Eklt: Asynchronous photometric featuretracking using events and frames. International Journal ofComputer Vision, 128(3):601618, 2020. 2",
  "Tommaso Guffanti, Toby Bell, Samuel Y. W. Low, MasonMurray-Cooper, and Simone DAmico. Autonomous guid-ance navigation and control of the visors formation-flyingmission, 2023. 2": "Javier Hidalgo-Carrio, Guillermo Gallego, and DavideScaramuzza. Event-aided direct sparse odometry. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 57815790, 2022. 2 Sumin Hu, Yeeun Kim, Hyungtae Lim, Alex Junho Lee, andHyun Myung. ecdt: Event clustering for simultaneous fea-ture detection and tracking. In 2022 IEEE/RSJ InternationalConference on Intelligent Robots and Systems (IROS), pages38083815. IEEE, 2022. 2, 6",
  "Marshall Kaplan, Bradley Boone, Robert Brown, ThomasCriss, and Edward Tunstel. Engineering Issues for All MajorModes of In Situ Space Debris Capture. 2": "Beat Kueng, Elias Mueggler, Guillermo Gallego, and Da-vide Scaramuzza. Low-latency visual odometry using event-based feature tracks. In 2016 IEEE/RSJ International Con-ference on Intelligent Robots and Systems (IROS), pages 1623. IEEE, 2016. 1 Ruoxiang Li, Dianxi Shi, Yongjun Zhang, Kaiyue Li, andRuihao Li. Fa-harris: A fast and asynchronous corner de-tector for event cameras. In 2019 IEEE/RSJ InternationalConference on Intelligent Robots and Systems (IROS), pages62236229. IEEE, 2019. 2",
  "Arunkumar Rathinam,Haytam Qadadri,and DjamilaAouada.Spades: A realistic spacecraft pose estimationdataset using event sensing, 2023. 3": "Henri Rebecq, Timo Horstschafer, Guillermo Gallego, andDavide Scaramuzza. EVO: A geometric approach to event-based 6-DOF parallel tracking and mapping in real time.IEEE Robotics and Automation Letters, 2(2):593600, 2016.2 Henri Rebecq, Timo Horstschaefer, and Davide Scaramuzza.Real-time visual-inertial odometry for event cameras usingkeyframe-based nonlinear optimization. In British MachineVision Conference 2017, BMVC 2017, London, UK, Septem-ber 4-7, 2017. BMVA Press, 2017. 2",
  "Johannes L. Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In 2016 IEEE Conference on Com-puter Vision and Pattern Recognition (CVPR), pages 41044113, 2016. 1, 3": "Brent E. Tweddle, Alvar Saenz-Otero, John J. Leonard, andDavid W. Miller. Factor graph modeling of rigid-body dy-namics for localization, mapping, and parameter estimationof a spinning object in space. Journal of Field Robotics, 32(6):897933, 2015. 2, 3 Antoni Rosinol Vidal, Henri Rebecq, Timo Horstschaefer,and Davide Scaramuzza. Ultimate slam? combining events,images, and imu for robust visual slam in hdr and high-speedscenarios. IEEE Robotics and Automation Letters, 3(2):9941001, 2018. 1, 2"
}