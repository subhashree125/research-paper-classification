{
  "Rui Chai13Pengyuan Wang13": "AbstractThe increasing demand for computational photographyand imaging on mobile platforms has led to the widespreaddevelopment and integration of advanced image sensorswith novel algorithms in camera systems.However, thescarcity of high-quality data for research and the rare op-portunity for in-depth exchange of views from industry andacademia constrain the development of mobile intelligentphotography and imaging (MIPI). Building on the achieve-ments of the previous MIPI Workshops held at ECCV2022 and CVPR 2023, we introduce our third MIPI chal-lenge including three tracks focusing on novel image sen-sors and imaging algorithms. In this paper, we summa-rize and review the Few-shot RAW Image Denoising trackon MIPI 2024.In total, 165 participants were success-fully registered, and 7 teams submitted results in the fi-nal testing phase.The developed solutions in this chal-lenge achieved state-of-the-art performance on Few-shotRAW Image Denoising.More details of this challengeand the link to the dataset can be found at 1VCIP, CS, Nankai University2S-Lab, Nanyang Technological University3Video Algorithm Group, Camera Department, Xiaomi Inc., China4Samsung Research China - Beijing (SRC-B)5Department of Camera Innovation Group, Samsung Electronics6Sun Yat-sen University7Harbin Institute of Technology, China8Smart City Research Institute of China Electronics TechnologyGroup Corporation9Xidian University, Xian, China10Nanyang Technological University, Singapore11University of Electronic Science and Technology of China, China12National Hanbat University, Daejeon, South Korea13North University of China, China",
  ". Introduction": "Noise, as an unavoidable part of the image capturingprocess, has recently been deeply explored by many re-searchers . Mobile terminals, such assmartphones, smart cameras, and fixed and mobile cameras,are often plagued by noise. Noise not only affects peoplesvisual perception but also usually impacts more high-leveldownstream tasks. Therefore, the development of denoisingtechniques is extremely important. Since the pioneering work of SID , the use of deepneural networks for RAW image denoising has graduallycome into view . Existing methods canbe divided into three categories: 1) collecting a large-scalepaired dataset for training denoising networks ; 2)first collecting datasets necessary for noise parameter cal-ibration , then using the calibrated noise model tosynthesize noisy-clean paired data and training the neuralnetwork; 3) collecting a few-shot amount of paired data,adopting pre-training on synthetic data, followed by fine-tuning the network with the few-shot paired dataset .The collection of clean images is extremely difficult, typi-cally involving multi-frame fusion , long exposure withlow ISO, or a combination of both. However, these collec-tion methods are not capable of capturing outdoor or dy-namic scenes, leading to a small domain or poor quality oftraining data, which naturally impacts the performance ofneural networks. Therefore, methods that train with paireddata are gradually fading from view, while calibration-basedapproaches are becoming mainstream. This is becausecalibration-based methods can obtain noise parameters thatare close to the real noise distribution, and using these noiseparameters can generate infinite paired data without beinglimited by the scene. These methods require perfect mod-",
  "arXiv:2406.07006v1 [cs.CV] 11 Jun 2024": "eling of noise to address the domain gap between real andsynthetic data, but this is usually challenging because manyfactors can affect the noise distribution: temperature, cam-era model, and even the lens can have an impact on noise.Referring to LED , the approach based on few-shot fine-tuning effectively bridges these two methods. It allows forpre-training with infinite synthetic data and fine-tuning withfew-shot paired data, where the collection cost of few-shotdata is extremely low. We encourage researchers to use pre-training and few-shot fine-tuning methods to train neuralnetworks for RAW image denoising. This can not only re-duce the demand for paired data but also solve the domaingap between the noise model and real noise to a certain ex-tent. In response to the growing demand among smartphoneand camera manufacturers, this competition focuses on de-veloping raw image denoising methods with insufficientdata. Participants can use publicly available datasets for pre-training and fine-tuning with data from two differentcamera manufacturers that we will provide.We hold this challenge in conjunction with the thirdMIPI Challenge which will be held on CVPR 2024. Similarto the previous MIPI challenge , we are seek-ing an efficient and high-performance image restoration al-gorithm to be used for raw image denoising with insufficientdata. MIPI 2024 consists of three competition tracks: Few-shot RAW Image Denoising is geared towardstraining neural networks for raw image denoising in sce-narios where paired data is limited.",
  ". Datasets": "In this competition, we collected paired data from 30 differ-ent scenes using cameras from two different manufacturers.Each scene includes two different ISO settings, five differ-ent brightness settings, and for each setting, 5 shots weretaken. To obtain clean RAW images as ground truth, weemployed multi-frame fusion denoising during the processof creating the ground truth. One camera model was usedto train on data from two scenes, while the other model wasused to train on data from five scenes. Both camera mod-els used data from five scenes for validation and were testedon the remaining scenes. Participants can use these data for fine-tuning or directly train on these data. Additionally,we provide a simple image signal processing pipeline forparticipants to visualize the denoised RAW data. The finalRGB data is used for evaluation. Since we have not dis-closed the camera models, participants cannot use the datafor training through calibration methods.However, par-ticipants can use publicly available RAW datasets for pre-training. Our dataset provides 19201920 RAW patchesfor fine-tuning and validation, and all RAW data are in theBayer pattern.",
  ". Challenge Results": "Among 165 registered participants, 7 teams successfullysubmitted their results, code, and factsheets in the final testphase. reports the final test results and rankingsof the teams. The methods evaluated in are brieflydescribed in and the team members are listed inAppendix. Finally, the MiAlgo AI team is the first placewinner of this challenge, while BigGuy team win the secondplace and SFNet-FR team is the third place, respectively.",
  "Full name: Samsung MX(Mobile eXperience) Business & Samsung Research China - Beijing (SRC-B)": "noise. In the first stage, the authors pre-train NAFNet from scratch using an enormous synthetic noise set. Theyuse high-quality clean images from two DSLR datasets and add noise modeling from Possion-Gaussian distribu-tion to form training samples. In order to enlarge the ran-domness and diversity of the noise parameters, the authorscarefully design a pattern-augment module to disturb theinitial noise parameter and an intensity-augment module toamplify the noise by different ratios (see ), whichenhance the network with strong generalization ability forefficiently adapting to specific camera sensor. This stage istrain with the batch size 8 and patch size 224 for around300k iterations. The model is optimized by AdamW opti-mizer using L1 loss, with the initial learning rate of 3e 4,which decreases by 0.6 in 100k and 200k iterations.Inthe second stage, the authors use the provided paired realdata to fine-tune the network without any modifications ofthe structures, with the batch size 4 and patch size 640 foraround 5k iterations. The model is optimized by AdamWoptimizer using L1 loss, with the initial learning rate of1e 4 and decreases by 0.6 in 1k and 3k iterations. Tofurther strengthen the generalization ability and boost theperformance in real-world scenarios, the authors combinethe predictions of four models with different image entrycharacteristics (i.e. different input resolutions and whetherto keep the negative value of the noise data) to obtain morerobust denoising results. Samsung1We propose to train a powerful pre-trainedmodel using real data from different cameras to solvethe weak generalization ability of the model on unseendata. Our model architecture is basically the same as theRestormer model as shown in , but with 4 chan-nels as input and output as our task is in raw space instead ofsRGB space. The training phase is comprised of two mainprocesses: pre-training and fine-tuning.For pre-trainingstage, to train a model with strong generalization, we col-",
  "L = L1 + Lc,": "in which we set = 0.5, but will adjust it during the train-ing process to balance the color loss and the L1 loss. Forfine-tuning, the model only uses L1 loss to train on the train-ing dataset provided by the organizer. Concretely, we useAdamW optimizer (1 = 0.9, 2 = 0.999, weight de-cay 0.0001) with the cosine annealing strategy, where thelearning rate gradually decreases from the initial learningrate 5 105 to 1 107 for 5 105 iterations in pre-training stage while the learning rate gradually decreasesfrom the initial learning rate 1 106 to 1 107 for2 105 iterations in fine-tuning stage. The training batchsize is set to 20 and patch size is 180. Horizontal/verticalflipping and rotation are used for data augmentation. Allexperiments are conducted on A100 GPU. To further im-prove performance of result during inference, self-ensembleis employed to produce the final results. Specifically, rota-tion and horizontal flipping are applied to the input imagesto obtain a set of images. These images are then input intoour model. The final prediction is made by taking the meanof these outputs. AIIADue to the characteristics of few-shot tasks, insuffi-cient data often has a certain impact on model effectiveness.For higher-level tasks, pretrained models are commonly in-tegrated into the network to enhance performance in down-stream tasks. Leveraging this approach, we integrate the",
  ". Illustration of the two stages of team AIIA": "ConvNeXt network, which has been pretrained on theImageNet dataset, into our network architecture. Theprimary aim is to leverage the powerful modeling capabil-ities demonstrated by this structure on large-scale datasetsto strengthen our networks feature extraction and denois-ing abilities, thereby improving the models generalizationcapability. This method not only utilizes pretrained Con-vNeXts efficient feature extraction capabilities but alsoenhances the models denoising performance.The specificwork is as follows(see ): In the first phase, we adda prior branch to the NAFNet network and pretrain it onthe SID dataset using five virtual cameras. In thesecond phase, we fine-tune the network using paired realdata, following a process similar to that used in LED.The model employs an L1 loss and is optimized with Adam,trained during the pretraining phase with a learning rate thatdecreases from 2e 4 to 0 over 289,800 iterations. In thefine-tuning phase, we significantly increase the number of iterations to 45,000, with the learning rate decreasing from2e 4 to 0. Finally, in order to further improve the denois-ing results, we first flip and rotate the images as the input tothe model, then restore and average the outputs. MS-DenoimerThis team proposes a Multi-Stage De-noiser with Spatial and Channel-wise Attention (MS-Denoimer, see ) for raw image denoising. Specif-ically, it utilizes a local and non-local multi-head self-attention mechanism to capture spatial correla-tions and a channel-wise multi-head self-attention mech-anism to address channel-specific dependen-cies. These elements compose basic units: Spatial Multi-head Self-Attention Blocks (S-MSABs) and Channel-wiseMulti-head Self-Attention Blocks (C-MSABs), which arealternately built up the single-stage Denoising Transformer(Denoimer). The Denoimer exploits a U-shaped structureto extract multi-resolution contextual information. Finally, the MS-Denoimer, cascaded by several Denoimers, pro-gressively improves the reconstruction quality from coarseto fine. For training, the authors pre-train MS-Denoimer onthe SID dataset and finetune it with Camera1 and Cam-era2 train sets. The objective of the model was to minimizethe Charbonnier loss. The authors employ the Adam op-timizer with hyperparameters 1 = 0.9 and 2 = 0.999.The pre-training learning rate is 4e-4 and the fine-tuninglearning rate is 1e-4. The pre-training and fine-tuning pro-cess spanned 300 epochs and 200 epochs, respectively. Thecosine annealing scheduler with linear warm-up was uti-lized. In the final inference stage, the authors integrate testtime augmentation and model ensemble, including horizon-tal flip, vertical flip, and 90-degree rotation. MyTurnIn the low-level domain, the U-Net architecturehas been widely used. This team modifies the NAFNet ,which is based on the U-Net architecture, by increasing thenumber of blocks in the encoder as well as the networkschannel count. The authors divide the training into threestages. In the first stage, the authors pretrain the model onthe SID-Sony A7S2 dataset . In the second stage, theauthors model the noise as a heteroscedastic Gaussian dis-tribution. For each image under a camera id, the authorsperform linear regression between the clean image and thenoise to obtain the noise parameters .Then the authorsfinetune the model on a noise dataset synthesized based onMIPI dataset to narrow down the domain gap(see ).Finally, in the third stage, the author performs the last fine-tuning on the MIPI dataset. Through two fine-tuning opera-tions, neural networks can more easily learn the distributionof noise information. For training, the authors randomlycropped 384 384 patches from the training images as in-puts. The model is trained with L1 Charbonnier loss .The learning rate is initialized as 1 104, and graduallyreduces to 1 107 with the cosine annealing. In the fi-nal inference stage, the authors use self-ensemble strategyto get eight images. These images output will be averagedto get the final result. HBNUFine-tuning is widely utilized across various fieldsdue to its ability to achieve remarkable performance evenwith a small amount of data. This team introduced a de-noising architecture iteratively fine-tuning, leveraging theLED . During the training phase, we initially performedfine-tuning of the dataset provided by the MIPI competitionwith a pre-trained model using the training strategy of PMN and the ELD dataset. The fine-tuned model wasthen used to iteratively fine-tune the dataset provided by theMIPI competition. The proposed method involves initiallyutilizing a pre-trained model from a different domain to ob-tain features, but through iterative fine-tuning, the modelcan better understand and incorporate the characteristics of specific tasks or domains. Additionally, the model becomesmore suitable for specific tasks, alleviating issues of overfit-ting. During the testing phase, Test-Time Data Augmenta-tion was applied to enhance the predictions of the im-age. As described in , The original image, along withimages rotated by 90, 180, and 270, as well as the verti-cally flipped image and its rotations by 90, 180, and 270,totaling 8 images, are denoising and then averaging. Thisteam training dataset by randomly cropping a 512 x 512region. The model utilized the Adam optimizer and under-went 1000 iterations. The learning rate was fixed at 104",
  "during the training. Finally, the OMNR branch was trainedfor 500 iterations with a learning rate of 105": "Erlong Mountain TeamU-Net has been widely used inmany application fields due to its excellent image segmen-tation ability and efficient network structure.The team pro-posed a denoising network based on UNet++ in a low-light environment. By integrating the UNet++ network withthe CBAM module, we ensure that the deep seman-tic information in the image is learned, which preservesthe original information and effectively removes the noisepresent in the image. The authors divide the training intotwo phases. The first phase blends the datasets providedby the MIPI competition with the ELD and SIDdatasets and augments their data, increasing the number ofdatasets ensures the robustness of the network and preventsoverfitting during training (see ).In the secondstage, the authors will train a hybrid dataset on the net-work, and the input images will be processed by the Unet++network model, and finally the denoising will be obtainedto obtain a clear image. Specifically, the network modelcontains four CBAM modules that process feature mapsthrough channel attention and spatial attention after eachdownsampling step of UNet to enhance their discrimina-tion while preserving important spatial information. Thisensemble approach combines the CBAM module with thestructure of the UNet model, allowing the UNet model tobenefit from the CBAM module and improve its feature ex-traction and representation capabilities.Before conducting network training, we drew on thenoise modeling methods used in ELD and the data aug-mentation methods in PRDNM, with the aim of estab-lishing more accurate noise models in low light environ-ments and further removing noise. For training, the authorsrandomly cropped a 512 x 512 region from the trainingset and performed random inversion/rotation enhancementdata. The model is trained for 1800 epochs using the Adamoptimizer and a combination of SmoothLoss and MS-SSIMas the loss function. The base learning rate is set to 2104",
  "and the minimum learning rate is set to 105. The optimizerrestarts every 600 epochs and the learning rate is halved onrestarts": ". Diagram of MS-Denoimer. (a) The diagram of the Multi-Stage Denoimer. (b) The diagram of the single-stage Denoimer. (c) Thediagram of the Spatial Multi-head Self-attention Block (S-MSAB). (d) The diagram of the Channel-wise Multi-head Self-attention Block(C-MSAB). (e) The illustration of the Spatial Multi-head Self-Attention (S-MSA). (f) The illustration of the Channel-wise Multi-headSelf-Attention (C-MSA). (g) The illustration of the Gated-DConv Feedforward Network (GDFN).",
  ". Conclusions": "In this report, we review and summarize the methods andresults of MIPI 2024 challenge on Few-shot RAW ImageDenoising.The MIPI 2024 Few-shot RAW Image Denoising Chal-lenge aimed to advance the state-of-the-art in image denois-ing by focusing on few-shot learning techniques, accommo-dating the scarcity of paired data. The competition utilizeddatasets from diverse scenes captured with different cameramanufacturers under varying ISO and brightness settings,specifically designed to challenge participants in trainingrobust denoising models.The evaluation was based onPSNR and SSIM metrics, integrated through a unique scor- ing formula to balance the two. The challenge proceededthrough development, validation, and testing phases, allow-ing participants to refine their models with a structured ap-proach to tackling few-shot denoising tasks. Out of 165registered teams, 7 made it to the final submission, show-casing a range of innovative solutions. The winning team,MiVideoNR, demonstrated superior performance in bothPSNR and the combined score, closely followed by Sam-sung and AIIA, indicating a competitive and high-caliberfield of entries. The diversity of approaches and the closecompetition underscores the challenges success in pushingforward the boundaries of few-shot RAW image denoisingresearch.As for the methods, various teams introduced ad-vanced and diverse methodologies for raw image denoising,demonstrating the fields innovative approaches to over-coming limitations posed by insufficient data.Despitetheir differences, these methods share common themes andstrategies: Pre-training and Fine-tuning Strategy: All teams havewidely adopted strategies to overcome limited real-worlddata, utilizing extensive pre-training on synthetic or largedatasets followed by fine-tuning on task-specific data. Synthetic to Real Noise Transition: Besides the naivepre-training and fine-tuning strategy, several teams devel-oped methods to bridge the gap between synthetic noiseand real-world noise conditions, enhancing the realismand diversity of training samples.",
  "Antoni Buades, Bartomeu Coll, and J-M Morel. A non-localalgorithm for image denoising. In CVPR, 2005. 1": "Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and FredoDurand. Learning photographic global tonal adjustment witha database of input / output image pairs.In The Twenty-Fourth IEEE Conference on Computer Vision and PatternRecognition, 2011. 3 Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and LeiZhang. Toward real-world single image super-resolution: Anew benchmark and a new model.In Proceedings of theIEEE International Conference on Computer Vision, 2019. 3 Yuanhao Cai, Jing Lin, Xiaowan Hu, Haoqian Wang, XinYuan, Yulun Zhang, Radu Timofte, and Luc Van Gool.Mask-guided spectral-wise transformer for efficient hy-perspectral image reconstruction.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1750217511, 2022. 4 Yuanhao Cai, Jing Lin, Haoqian Wang, Xin Yuan, HenghuiDing, Yulun Zhang, Radu Timofte, and Luc V Gool.Degradation-aware unfolding half-shuffle transformer forspectral compressive imaging. Advances in Neural Informa-tion Processing Systems, 35:3774937761, 2022. 4",
  "Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun.Learning to see in the dark. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition(CVPR), 2018. 1, 4, 5": "Xiaojie Chu, Liangyu Chen, and Wenqing Yu. Nafssr: Stereoimage super-resolution using nafnet.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) Workshops, pages 12391248, 2022. 3,4, 5, 7 Yuekun Dai, Chongyi Li, Shangchen Zhou, Ruicheng Feng,Qingpeng Zhu, Qianhui Sun, Wenxiu Sun, Chen ChangeLoy, Jinwei Gu, Shuai Liu, et al. Mipi 2023 challenge onnighttime flare removal: Methods and results. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 28522862, 2023. 2 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE Conference on Computer Vision andPattern Recognition, pages 248255, 2009. 4 Yubo Dong, Dahua Gao, Yuyan Li, Guangming Shi, andDanhua Liu. Degradation estimation recurrent neural net-work with local and non-local priors for compressive spectralimaging. arXiv preprint arXiv:2311.08808, 2023. 4 Yubo Dong, Dahua Gao, Tian Qiu, Yuyan Li, Minxi Yang,and Guangming Shi. Residual degradation learning unfold-ing framework with mixing priors across spectral and spa-tial for compressive spectral imaging.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 2226222271, 2023. 4 Hansen Feng, Lizhi Wang, Yuzhi Wang, and Hua Huang.Learnability enhancement for low-light raw denoising:Where paired real data meets noise modeling. In Proceed-ings of the 30th ACM International Conference on Multime-dia, 2022. 5",
  "Ilya Loshchilov and Frank Hutter.Decoupled weight de-cay regularization. In International Conference on LearningRepresentations, 2019. 3": "Qianhui Sun, Qingyu Yang, Chongyi Li, Shangchen Zhou,Ruicheng Feng, Yuekun Dai, Wenxiu Sun, Qingpeng Zhu,Chen Change Loy, Jinwei Gu, et al. Mipi 2023 challengeon rgbw remosaic: Methods and results. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 28772884, 2023. 2 Qianhui Sun, Qingyu Yang, Chongyi Li, Shangchen Zhou,Ruicheng Feng, Yuekun Dai, Wenxiu Sun, Qingpeng Zhu,Chen Change Loy, Jinwei Gu, et al. Mipi 2023 challengeon rgbw fusion: Methods and results.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 28702876, 2023. 2 Radu Timofte, Rasmus Rothe, and Luc Van Gool. Sevenways to improve example-based single image super resolu-tion. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 18651873, 2016. 5"
}