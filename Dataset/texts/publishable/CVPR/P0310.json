{
  "Abstract": "An efficient and effective decoding mechanism is crucialin medical image segmentation, especially in scenarios withlimited computational resources. However, these decodingmechanisms usually come with high computational costs.To address this concern, we introduce EMCAD, a new effi-cient multi-scale convolutional attention decoder, designedto optimize both performance and computational efficiency.EMCAD leverages a unique multi-scale depth-wise convo-lution block, significantly enhancing feature maps throughmulti-scale convolutions. EMCAD also employs channel,spatial, and grouped (large-kernel) gated attention mech-anisms, which are highly effective at capturing intricatespatial relationships while focusing on salient regions. Byemploying group and depth-wise convolution, EMCAD isvery efficient and scales well (e.g., only 1.91M parame-ters and 0.381G FLOPs are needed when using a stan-dard encoder). Our rigorous evaluations across 12 datasetsthat belong to six medical image segmentation tasks re-veal that EMCAD achieves state-of-the-art (SOTA) perfor-mance with 79.4% and 80.3% reduction in #Params and#FLOPs, respectively.Moreover, EMCADs adaptabilityto different encoders and versatility across segmentationtasks further establish EMCAD as a promising tool, ad-vancing the field towards more efficient and accurate med-ical image analysis. Our implementation is available at",
  ". Introduction": "In the realm of medical diagnostics and therapeutic strate-gies, automated segmentation of medical images is vital, asit classifies pixels to identify critical regions such as lesions,tumors, or entire organs. A variety of U-shaped convolu-tional neural network (CNN) architectures , notably UNet , UNet++ , UNet3+ , andnnU-Net , have become standard techniques for thispurpose, achieving high-quality, high-resolution segmen- tation output. Attention mechanisms have also been integrated into these models to enhance fea-ture maps and improve pixel-level classification. Althoughattention-based models have shown improved performance,they still face significant challenges due to the computation-ally expensive convolutional blocks that are typically usedin conjunction with attention mechanisms.Recently, vision transformers have shown promisein medical image segmentation tasks by capturing long-range dependencies among pix-els through Self-attention (SA) mechanisms. Hierarchicalvision transformers like Swin , PVT , MaxViT, MERIT , ConvFormer , and MetaFormer have been introduced to further improve the performance inthis field. While the SA excels at capturing global informa-tion, it is less adept at understanding the local spatial context. To address this limitation, some approaches haveintegrated local convolutional attention within the decodersto better grasp spatial details. Nevertheless, these meth-ods can still be computationally demanding because theyfrequently employ costly convolutional blocks. This limitstheir applicability to real-world scenarios where computa-tional resources are restricted.To address the aforementioned limitations, we introduceEMCAD, an efficient multi-scale convolutional attentiondecoding using a new multi-scale depth-wise convolutionblock. More precisely, EMCAD enhances the feature mapsvia efficient multi-scale convolutions, while incorporatingcomplex spatial relationships and local attention through theuse of channel, spatial, and grouped (large-kernel) gated at-tention mechanisms. Our contributions are as follows: New Efficient Multi-scale Convolutional Decoder:We introduce an efficient multi-scale cascaded fully-convolutional attention decoder (EMCAD) for 2D med-ical image segmentation; this takes the multi-stage fea-tures of vision encoders and progressively enhances themulti-scale and multi-resolution spatial representations.EMCAD has only 0.506M parameters and 0.11G FLOPsfor a tiny encoder with #channels = ,",
  "while it has 1.91M parameters and 0.381G FLOPs for astandard encoder with #channels =": "Efficient Multi-scale Convolutional Attention Mod-ule: We introduce MSCAM, a new efficient multi-scaleconvolutional attention module that performs depth-wiseconvolutions at multiple scales; this refines the featuremaps produced by vision encoders and enables captur-ing multi-scale salient features by suppressing irrele-vant regions. The use of depth-wise convolutions makesMSCAM very efficient. Large-kernel Grouped Attention Gate: We introducea new grouped attention gate to fuse refined features withthe features from skip connections. By using larger kernel(33) group convolutions instead of point-wise convolu-tions in the design, we capture salient features in a largerlocal context with less computation. Improved Performance: We empirically show that EM-CAD can be used with any hierarchical vision encoder(e.g., PVTv2-B0, PVTv2-B2 ), while significantlyimproving the performance of 2D medical image seg-mentation. EMCAD produces better results than SOTAmethods with a significantly lower computational cost (asshown in ) on 12 medical image segmentationbenchmarks that belong to six different tasks.The remaining of this paper is organized as follows: summarizes related work. describesthe proposed method. explains our experimentalsetup and results on 12 medical image segmentation bench-marks. covers different ablation experiments.Lastly, concludes the paper.",
  "Convolutional Neural Networks (CNNs) have been foundational as encoders due to their pro-": "ficiency in handling spatial relationships in images. Moreprecisely, AlexNet and VGG pave the way, lever-aging deep layers of convolutions to extract features pro-gressively. GoogleNet introduces the inception mod-ule, allowing more efficient computation of representationsacross various scales. ResNet introduces residual con-nections, enabling the training of networks with substan-tially more layers by addressing the vanishing gradientsproblem. MobileNets bring CNNs to mobile de-vices through lightweight, depth-wise separable convolu-tions. EfficientNet introduces a scalable architecturaldesign to CNNs with compound scaling. Although CNNsare pivotal for many vision applications, they generally lackthe ability to capture long-range dependencies within im-ages due to their inherent local receptive fields.Recently, Vision Transformers (ViTs), pioneered byDosovitskiy et al. , enabled the learning of long-rangerelationships among pixels using Self-attention (SA). Sincethen, ViTs have been enhanced by integrating CNN fea-tures , developing novel self-attention (SA) blocks, and introducing new architectural designs .The Swin Transformer incorporates a sliding windowattention mechanism, while SegFormer leverages Mix-FFN blocks for hierarchical structures. PVT uses spa-tial reduction attention, refined in PVTv2 with over-lapping patch embedding and a linear complexity attentionlayer. MaxViT introduces a multi-axis self-attentionto form a hierarchical CNN-transformer encoder. AlthoughViTs address the CNNs limitation in capturing long-rangepixel dependencies , they face chal-lenges in capturing the local spatial relationships amongpixels.In this paper, we aim to overcome these limita-tions by introducing a new multi-scale cascaded attentiondecoder that refines feature maps and incorporates local at-tention using a multi-scale convolutional attention module.",
  ". Medical image segmentation": "Medical image segmentation involves pixel-wise classifica-tion to identify various anatomical structures like lesions,tumors, or organs within different imaging modalities suchas endoscopy, MRI, or CT scans . U-shaped networks are particularly favored dueto their simple but effective encoder-decoder design. TheUNet pioneered this approach with its use of skipconnections to fuse features at different resolution stages.UNet++ evolves this design by incorporating nestedencoder-decoder pathways with dense skip connections.Expanding on these ideas, UNet 3+ introduces compre-hensive skip pathways that facilitate full-scale feature inte-gration. Further advancement comes with DC-UNet ,which integrates a multi-resolution convolution scheme andresidual paths into its skip connections. The DeepLab se-ries, including DeepLabv3 and DeepLabv3+ , in- troduce atrous convolutions and spatial pyramid pooling tohandle multi-scale information. SegNet uses pooling in-dices to upsample feature maps, preserving the boundarydetails.nnU-Net automatically configures hyperpa-rameters based on the specific dataset characteristics, usingstandard 2D and 3D UNets. Collectively, these U-shapedmodels have become a benchmark for success in the domainof medical image segmentation.Recently,vision transformers have emerged as aformidable force in medical image segmentation, harness-ing the ability to capture pixel relationships at global scales. TransUNet presents a novelblend of CNNs for local feature extraction and transform-ers for global context, enhancing both local and global fea-ture capture. Swin-Unet extends this by incorporatingSwin Transformer blocks into a U-shaped model forboth encoding and decoding processes. Building on theseconcepts, MERIT introduces a multi-scale hierarchi-cal transformer, which employs SA across different windowsizes, thus enhancing the model capacity to capture multi-scale features critical for medical image segmentation.The integration of attention mechanisms has been in-vestigated within CNNs and transformer-basedsystems for enhancing medical image segmentation.PraNet employs a reverse attention strategy for fea-ture refinement. PolypPVT leverages PVTv2 asits backbone encoder and incorporates CBAM withinits decoding stages. The CASCADE presents a novelcascaded decoder, combining channel and spatial attention to refine features at multiple stages, extracted froma transformer encoder, culminating in high-resolution seg-mentation outputs. While CASCADE achieves notable per-formance in segmenting medical images by integrating lo-cal and global insights from transformers, it is computation-ally inefficient due to the use of triple 33 convolution lay-ers at each decoder stage. In addition to this, it uses single-scale convolutions during decoding. Our new proposal in-volves the adoption of multi-scale depth-wise convolutionsto mitigate these constraints.",
  ". Efficient multi-scale convolutional attention de-coding (EMCAD)": "In this section, we introduce our efficient multi-scale con-volutional decoding (EMCAD) to process the multi-stagefeatures extracted from pretrained hierarchical vision en-coders for high-resolution semantic segmentation.Asshown in (b), EMCAD consits of efficient multi- scale convolutional attention modules (MSCAMs) to ro-bustly enhance the feature maps, large-kernel grouped at-tention gates (LGAGs) to refine feature maps fusing withthe skip connection via gated attention mechanism, efficientup-convolution blocks (EUCBs) for up-sampling followedby enhancement of feature maps, and segmentation heads(SHs) to produce the segmentation outputs.More specifically, we use four MSCAMs to refine pyra-mid features (i.e., X1, X2, X3, X4 in ) extractedfrom the four stages of the encoder. After each MSCAM,we use an SH to produce a segmentation map of that stage.Subsequently, we upscale the refined feature maps usingEUCBs and add them to the outputs from the correspondingLGAGs. Finally, we add four different segmentation mapsto produce the final segmentation output. Different modulesof our decoder are described next.",
  "Large-kernel grouped attention gate (LGAG)": "We introduce a new large-kernel grouped attention gate(LGAG) to progressively combine feature maps with atten-tion coefficients, which are learned by the network to allowhigher activation of relevant features and suppression of ir-relevant ones. This process employs a gating signal derivedfrom higher-level features to control the flow of informa-tion across different stages of the network, thus enhancingits precision for medical image segmentation. Unlike At-tention UNet which uses 1 1 convolution to processgating signal g (features from skip connections) and inputfeature map x (upsampled features), in our qatt(.) func-tion, we process g and x by applying separate 3 3 groupconvolutions GCg(.) and GCx(.), respectively. These con-volved features are then normalized using batch normaliza-tion (BN(.)) and merged through element-wise addi-tion. The resultant feature map is activated through a ReLU(R(.)) layer . Afterward, we apply a 1 1 convolu-tion (C(.)) followed by BN(.) layer to get a single channelfeature map. We then pass the resultant single-channel fea-ture map through a Sigmoid ((.)) activation function toyield the attention coefficients. The output of this transfor-mation is used to scale the input feature x through element-wise multiplication, producing the attention-gated featureLGAG(g, x). The LGAG() ((g)) can be formu-lated as in Equations 1 and 2:",
  "Due to using 3 3 kernel group convolutions in qatt(.), ourLGAG captures comparatively larger spatial contexts withless computational cost": ". Hierarchical encoder with newly proposed EMCAD decoder architecture. (a) CNN or transformer encoder with four hierarchicalstages, (b) EMCAD decoder, (c) Efficient up-convolution block (EUCB), (d) Multi-scale convolutional attention module (MSCAM), (e)Multi-scale convolution block (MSCB), (f) Multi-scale (parallel) depth-wise convolution (MSDC), (g) Large-kernel grouped attention gate(LGAG), (h) Channel attention block (CAB), and (i) Spatial attention block (SAB). X1, X2, X3, and X4 are the features from the fourstages of the hierarchical encoder. p1, p2, p3, and p4 are output segmentation maps from four stages of our decoder.",
  "Multi-scaleconvolutionalattentionmodule(MSCAM)": "We introduce an efficient multi-scale convolutional atten-tion module to refine the feature maps. MSCAM consistsof a channel attention block (CAB()) to put emphasis onpertinent channels, a spatial attention block (SAB())to capture the local contextual information, and an effi-cient multi-scale convolution block (MSCB(.)) to enhancethe feature maps preserving contextual relationships. TheMSCAM(.) ((d)) is given in Equation 3:",
  "MSCAM(x) = MSCB(SAB(CAB(x)))(3)": "where x is the input tensor. Due to using depth-wise con-volution in multiple scales, our MSCAM is more effectivewith significantly lower computational cost than the convo-lutional attention module (CAM) proposed in .Multi-scale Convolution Block (MSCB): We introducean efficient multi-scale convolution block to enhance thefeatures generated by our cascaded expanding path. In ourMSCB, we follow the design of the inverted residual block(IRB) of MobileNetV2 .However, unlike IRB, ourMSCB performs depth-wise convolution at multiple scalesand uses channel shuffle to shuffle channels acrossgroups. More specifically, in our MSCB, we first expandthe number of channels (i.e., expansion factor = 2) using apoint-wise (11) convolution layers PWC1() followed bya batch normalization layer BN() and a ReLU6 activa-tion layer R6(.). We then use a multi-scale depth-wise con-volution MSDC(.) to capture both multi-scale and multi-resolution contexts. As depth-wise convolution overlooks the relationships among channels, we use a channel shuffleoperation to incorporate relationships among channels. Af-terward, we use another point-wise convolution PWC2(.)followed by a BN(.) to transform back the original #chan-nels, which also encodes dependency among channels. TheMSCB() ((e)) is formulated as in Equation 4:",
  "x = x + DWCBks(x)(6)": "Channel Attention Block (CAB): We use channel at-tention block to assign different levels of importance to eachchannel, thus emphasizing more relevant features whilesuppressing less useful ones. Basically, the CAB identi-fies which feature maps to focus on (and then refine them).Following , in CAB, we first apply the adaptive maxi-mum pooling (Pm()) and adaptive average pooling (Pa())to the spatial dimensions (i.e., height and width) to extractthe most significant feature of the entire feature map per channel. Then, for each pooled feature map, we reducethe number of channels r = 1/16 times separately usinga point-wise convolution (C1()) followed by a ReLU ac-tivation (R). Afterward, we recover the original channelsusing another point-wise convolution (C2()). We then addboth recovered feature maps and apply Sigmoid () activa-tion to estimate attention weights. Finally, we incorporatethese weights to input x using the Hadamard product ().The CAB() ((h)) is defined using Equation 7:",
  "CAB(x) = (C2(R(C1(Pm(x)))) + C2(R(C1(Pa(x))))) x(7)": "Spatial Attention Block (SAB): We use spatial atten-tion to mimic the attentional processes of the human brainby focusing on specific parts of an input image. Basically,the SAB determines where to focus in a feature map; then itenhances those features. This process enhances the modelsability to recognize and respond to relevant spatial features,which is crucial for image segmentation where the contextand location of objects significantly influence the output.In SAB, we first pool maximum (Chmax()) and average(Chavg()) values along the channel dimension to pay atten-tion to local features. Then, we use a large kernel (i.e., 77as in ) convolution layer to enhance local contextual re-lationships among features. Afterward, we apply the Sig-moid activation () to calculate attention weights. Finally,we feed these weights to the input x (using Hadamard prod-uct () to attend information in a more targeted way. TheSAB(.) ((i)) is defined using Equation 8:",
  "Efficient up-convolution block (EUCB)": "We use an efficient up-convolution block to progressivelyupsample the feature maps of the current stage to match thedimension and resolution of the feature maps from the nextskip connection. The EUCB first uses an UpSampling Up()with scale-factor 2 to upscale the feature maps. Then, itenhances the upscaled feature maps by applying a 3 3depth-wise convolution DWC() followed by a BN() anda ReLU(.) activation. Finally, a 1 1 convolution C11(.)is used to reduce the #channels to match with the next stage.The EUCB() ((c)) is formulated as in Equation 9:",
  ". Overall architecture": "To show the generalization, effectiveness, and ability to pro-cess multi-scale features for medical image segmentation,we integrate our EMCAD decoder alongside tiny (PVTv2-B0) and standard (PVTv2-B2) networks of PVTv2 .However, our decoder is adaptable and seamlessly compat-ible with other hierarchical backbone networks.PVTv2 differs from conventional transformer patch em-bedding modules by applying convolutional operations forconsistent spatial information capture.Using PVTv2-b0(Tiny) and PVTv2-b2 (Standard) encoders , we developthe PVT-EMCAD-B0 and PVT-EMCAD-B2 architectures.To adopt PVTv2, we first extract the features (X1, X2, X3,and X4) from four layers and feed them (i.e., X4 in the up-sample path and X3, X2, X1 in the skip connections) intoour EMCAD decoder as shown in (a-b). EMCADthen processes them and produces four segmentation mapsthat correspond to the four stages of the encoder network.3.3. Multi-stage loss and outputs aggregation Our EMCAD decoders four segmentation heads producefour prediction maps p1, p2, p3, and p4 across its stages.Loss aggregation: We adopt a combinatorial approachto loss combination called MUTATION, inspired by thework of MERIT for multi-class segmentation. Thisinvolves calculating the loss for all possible combinationsof predictions derived from 4 heads, totaling 24 1 = 15unique predictions, and then summing these losses. We fo-cus on minimizing this cumulative combinatorial loss dur-ing the training process. For binary segmentation, we op-timize the additive loss like with an additional termLp1+p2+p3+p4 as in Equation 11: Ltotal = Lp1 + Lp2 + Lp3 + Lp4 + Lp1+p2+p3+p4(11)where Lp1, Lp2, Lp3, and Lp4 are the losses of each indi-vidual prediction maps. = = = = = 1.0 are theweights assigned to each loss.Output segmentation maps aggregation: We considerthe prediction map, p4, from the last stage of our decoder asthe final segmentation map. Then, we obtain the final seg-mentation output by employing a Sigmoid function for bi-nary or a Softmax function for multi-class segmentation.",
  "PVT-EMCAD-B0 (Ours)3.92M0.84G94.6091.7191.6591.9591.3085.6790.7092.4695.3579.8090.52PVT-EMCAD-B2 (Ours)26.76M5.6G95.2192.3192.2992.7592.9685.9590.9692.7495.5380.2591.10": ". Results of binary medical image segmentation (i.e., polyp, skin lesion, cell, and breast cancer). We reproduce the results of SOTAmethods using their publicly available implementation with our train-val-test splits of 80:10:10. #FLOPs of all the methods are reportedfor 256 256 inputs, except Swin-UNet (224 224). All results are averaged over five runs. Best results are shown in bold.",
  "PVT-EMCAD-B0 (Ours)81.9717.3972.6487.2166.6287.4883.9694.5762.0092.6681.22PVT-EMCAD-B2 (Ours)83.6315.6874.6588.1468.8788.0884.1095.2668.5192.1783.92": ". Results of abdomen organ segmentation on Synapse Multi-organ dataset. DICE scores are reported for individual organs. Resultsof UNet, AttnUNet, PolypPVT, SSFormerPVT, TransUNet, and SwinUNet are taken from . () denotes the higher (lower) the better. means missing data from the source. EMCAD results are averaged over five runs. Best results are shown in bold.",
  ". Implementation details": "We implement our network and conduct experiments us-ing Pytorch 1.11.0 on a single NVIDIA RTX A6000 GPUwith 48GB of memory.We utilize ImageNet pre-trained PVTv2-b0 and PVTv2-b2 as encoders.Inthe MSDC of our decoder, we set the multi-scale kernels through an ablation study. We use the parallel ar-rangement of depth-wise convolutions in all experiments.Our models are trained using the AdamW optimizer with a learning rate and weight decay of 1e 4. We gener-ally train for 200 epochs with a batch size of 16, exceptfor Synapse multi-organ (300 epochs, batch size 6) andACDC cardiac organ (400 epochs, batch size 12), savingthe best model based on the DICE score. We resize images to 352352 and use a multi-scale {0.75, 1.0, 1.25} trainingstrategy with a gradient clip limit of 0.5 for ClinicDB ,Kvasir , ColonDB , ETIS , BKAI , ISIC17, and ISIC18 , while we resize images to 256 256for BUSI , EM , and DSB18 . For Synapse andACDC datasets, images are resized to 224 224, withrandom rotation and flipping augmentations, optimizing acombined Cross-entropy (0.3) and DICE (0.7) loss. For bi-nary segmentation, we utilize the combined weighted Bina-ryCrossEntropy (BCE) and weighted IoU loss function.",
  "Results of binary medical image segmentation": "Results for different methods on 10 binary medical im-age segmentation datasets are shown in and Fig-ure 1. Our PVT-EMCAD-B2 attains the highest averageDICE score (91.10%) with only 26.76M parameters and5.6G FLOPs. The multi-scale depth-wise convolution in ourEMCAD decoder, combined with the transformer encoder,contributes to these performance gains.Polyp segmentation: reveals that our PVT-EMCAD-B2 surpasses all SOTA methods in five polypsegmentation datasets. PVT-EMCAD-B2 achieves DICEscore improvements of 1.08%, 0.78%, 2.36%, 1.19%,and 1.79% over PolypPVT in ClinicDB, ColonDB, ETIS,Kvasir, and BKAI-IGI, despite having slightly more pa-rameters and FLOPs.The smallest model UNeXt, ex-",
  "NoNoNo00080.100.2YesNoNo0.1000.1310.22481.080.2YesYesNo0.1080.1410.23581.920.2YesNoYes0.3730.4871.89882.860.3YesYesYes0.3810.4981.9183.630.3": ". Effect of different components of EMCAD with PVTv2-b2 encoder on Synapse multi-organ dataset. #FLOPs are reportedfor input resolution of 224 224 and 256 256. All results areaveraged over five runs. Best results are shown in bold. hibits the worst performance in all five polyp segmenta-tion datasets. Our smaller model with only 3.92M param-eters and 0.84G FLOPs also outperforms all the methodsexcept PVT-CASCADE (in Kvasir and BKAI-IGH) andSSFormer-L (in ColonDB), which achieve the best perfor-mance among SOTA methods.In conclusion, our PVT-EMCAD-B2 achieves the new SOTA results in these fivepolyp segmentation datasets.Skin lesion segmentation: shows PVT-EMCAD-B2s strong performance on ISIC17 and ISIC18skin lesion segmentation datasets, achieving DICE scoresof 85.95% and 90.96%, surpassing DeepLabV3+ by 2.11%and 2.32%.It also beats the nearest method PVT-CASCADE by 0.45% and 0.55% in ISIC17 and ISIC18,respectively, though our decoder is significantly more ef-ficient than CASCADE. Our PVT-EMCAD-B0 also showshuge potential in point care applications like skin lesion seg-mentation with only 3.92M parameters and 0.84G FLOPs.Cell segmentation: To evaluate our methods effective-ness in biological imaging, we use DSB18 for cell nu-clei and EM for cell structure segmentation. As Ta-ble 1 indicates, our PVT-EMCAD-B2 sets a SOTA bench-mark in cell nuclei segmentation on DSB18, outperformingDeepLabv3+, TransFuse, and PVT-CASCADE. On the EMdataset, PVT-EMCAD-B2 secures the second-best DICEscore (95.53%), offering significantly lower computationalcosts than the top-performing AttnUNet (95.55%).Breast cancer segmentation: We conduct experimentson the BUSI dataset for breast cancer segmentation in ultra-sound images. Our PVT-EMCAD-B2 achieves the SOTADICE score (80.25%) on this dataset.Furthermore, ourPVT-EMCAD-B0 outperforms the computationally similarmethod UNeXt by a notable margin of 5.54%.",
  "ual organ segmentation, significantly outperforming SOTAmethods on six of eight organs.4.2.3Results of cardiac organ segmentation": "shows the DICE scores of our PVT-EMCAD-B2and PVT-EMCAD-B0 along with other SOTA methods, onthe MRI images of the ACDC dataset for cardiac organ seg-mentation. Our PVT-EMCAD-B2 achieves the highest av-erage DICE score of 92.12%, thus improving about 0.27%over Cascaded MERIT though our network has significantlylower computational cost. Besides, PVT-EMCAD-B2 hasbetter DICE scores in all three organ segmentations.",
  ". Ablation Studies": "In this section, we conduct ablation studies to explore differ-ent aspects of our architectures and the experimental frame-work. More ablations are in Supplementary .5.1. Effect of different components of EMCAD We conduct a set of experiments on the Synapse multi-organdataset to understand the effect of different components ofour EMCAD decoder. We start with only the encoder andadd different modules such as Cascaded structure, LGAG,and MSCAM to understand their effect. exhibitsthat the cascaded structure of the decoder helps to improveperformance over the non-cascaded one. The incorpora-tion of LGAG and MSCAM improves performance, how-ever, MSCAM proves to be more effective. When both theLGAG and MSCAM modules are used together, it producesthe best DICE score of 83.63%. It is also evident that thereis about 3.53% improvement in the DICE score with an ad-ditional 0.381G FLOPs and 1.91M parameters.5.2. Effect of multi-scale kernels in MSCAM We have conducted another set of experiments on Synapsemulti-organ and ClinicDB datasets to understand the effectof different multi-scale kernels used for depth-wise convo-lutions in MSDC. reports these results which show that performance improves from 11 to 33 kernel. When1 1 kernel is used together with 3 3 it improves morethan when using them alone. However, when two 3 3kernels are used together, performance drops. The incorpo-ration of a 5 5 kernel with 1 1 and 3 3 kernels furtherimproves the performance and it achieves the best results inboth Synapse multi-organ and ClinicDB datasets. If we addadditional larger kernels (e.g., 77, 99), the performanceof both datasets drops. Based on these empirical observa-tions, we choose kernels in all our experiments.",
  ". Comparison with the baseline decoder": "In , we report the experimental results with the com-putational complexity of our EMCAD decoder and a base-line decoder, namely CASCADE. From , we can seethat our EMCAD decoder with PVTv2-b2 requires 80.3%fewer FLOPs and 79.4% fewer parameters to outperform(by 0.85%) the respective CASCADE decoder. Similarly,our EMCAD decoder with PVTv2-B0 achieves 1.43% bet-ter DICE score than the CASCADE decoder with 78.1%fewer parameters and 74.9% fewer FLOPs.",
  ". Conclusions": "In this paper, we have presented EMCAD, a new and effi-cient multi-scale convolutional attention decoder designedfor multi-stage feature aggregation and refinement in med-ical image segmentation. EMCAD employs a multi-scaledepth-wise convolution block, which is key for capturing di-verse scale information within feature maps, a critical factorfor precision in medical image segmentation. This designchoice, using depth-wise convolutions instead of standard3 3 convolution blocks, makes EMCAD notably efficient.Our experiments reveal that EMCAD surpasses the re-cent CASCADE decoder in DICE scores with 79.4% fewerparameters and 80.3% less FLOPs. Our extensive experi-ments also confirm EMCADs superior performance com-pared to SOTA methods across 12 public datasets coveringsix different 2D medical image segmentation tasks. EM-CADs compatibility with smaller encoders makes it an ex-cellent fit for point-of-care applications while maintaininghigh performance. We anticipate that our EMCAD decoderwill be a valuable asset in enhancing a variety of medicalimage segmentation and semantic segmentation tasks.Acknowledgements:This work is supported in partby the NSF grant CNS 2007284, and in part by theiMAGiNEConsortium(",
  "Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled,and Aly Fahmy. Dataset of breast ultrasound images. Datain brief, 28:104863, 2020. 6, 1": "Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.Segnet: A deep convolutional encoder-decoder architecturefor image segmentation. IEEE Trans. Pattern Anal. Mach.Intell., 39(12):24812495, 2017. 3 JorgeBernal,FJavierSanchez,GloriaFernandez-Esparrach, Debora Gil, Cristina Rodrguez, and FernandoVilarino. Wm-dova maps for accurate polyp highlighting incolonoscopy: Validation vs. saliency maps from physicians.Comput. Med. Imaging Graph., 43:99111, 2015. 6, 1 Juan C Caicedo, Allen Goodman, Kyle W Karhohs, Beth ACimini, Jeanelle Ackerman, Marzieh Haghighi, CherKengHeng, Tim Becker, Minh Doan, Claire McQuin, et al. Nu-cleus segmentation across imaging experiments: the 2018data science bowl.Nature methods, 16(12):12471253,2019. 6, 7, 1 Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xi-aopeng Zhang, Qi Tian, and Manning Wang.Swin-unet:Unet-like pure transformer for medical image segmentation.arXiv preprint arXiv:2105.05537, 2021. 1, 3, 6, 7 Albert Cardona, Stephan Saalfeld, Stephan Preibisch, Ben-jamin Schmid, Anchi Cheng, Jim Pulokas, Pavel Tomancak,and Volker Hartenstein. An integrated micro-and macroar-chitectural analysis of the drosophila brain by computer-assisted serial section electron microscopy. PLoS biology,8(10):e1000502, 2010. 6, 7, 1",
  "Gongping Chen, Lei Li, Yu Dai, Jianxun Zhang, andMoi Hoon Yap.Aau-net: an adaptive attention u-net forbreast lesions segmentation in ultrasound images.IEEETrans. Med. Imaging, 2022. 2": "Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, EhsanAdeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou.Transunet: Transformers make strong encoders for medi-cal image segmentation. arXiv preprint arXiv:2102.04306,2021. 1, 2, 3, 6, 7 Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, JianShao, Wei Liu, and Tat-Seng Chua. Sca-cnn: Spatial andchannel-wise attention in convolutional networks for imagecaptioning.In IEEE Conf. Comput. Vis. Pattern Recog.,pages 56595667, 2017. 3, 4 Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,Kevin Murphy, and Alan L Yuille. Deeplab: Semantic imagesegmentation with deep convolutional nets, atrous convolu-tion, and fully connected crfs. IEEE Trans. Pattern Anal.Mach. Intell., 40(4):834848, 2017. 2, 6 Liang-Chieh Chen, Yukun Zhu, George Papandreou, FlorianSchroff, and Hartwig Adam. Encoder-decoder with atrousseparable convolution for semantic image segmentation. InEur. Conf. Comput. Vis., pages 801818, 2018. 2",
  "sitional encodings for vision transformers.arXiv preprintarXiv:2102.10882, 2021. 1": "Noel Codella,Veronica Rotemberg,Philipp Tschandl,M Emre Celebi, Stephen Dusza, David Gutman, BrianHelba,AadiKalloo,KonstantinosLiopyris,MichaelMarchetti, et al.Skin lesion analysis toward melanomadetection 2018:A challenge hosted by the interna-tional skin imaging collaboration (isic).arXiv preprintarXiv:1902.03368, 2019. 1 Noel CF Codella, David Gutman, M Emre Celebi, BrianHelba, Michael A Marchetti, Stephen W Dusza, AadiKalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kit-tler, et al. Skin lesion analysis toward melanoma detection:A challenge at the 2017 international symposium on biomed-ical imaging (isbi), hosted by the international skin imagingcollaboration (isic). In IEEE Int. Symp. Biomed. Imaging,pages 168172. IEEE, 2018. 6, 1",
  "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.Imagenet classification with deep convolutional neural net-works. Adv. Neural Inform. Process. Syst., 25, 2012. 2": "Xian Lin, Zengqiang Yan, Xianbo Deng, ChuanshengZheng, and Li Yu.Convformer: Plug-and-play cnn-styletransformers for improving medical image segmentation. InInt. Conf. Med. Image Comput. Comput. Assist. Interv., pages642651. Springer, 2023. 1 Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InInt. Conf. Comput. Vis., pages 1001210022, 2021. 1, 2, 3",
  "Ilya Loshchilov and Frank Hutter. Decoupled weight decayregularization. arXiv preprint arXiv:1711.05101, 2017. 6": "Ange Lou, Shuyue Guan, and Murray Loew. Dc-unet: re-thinking the u-net architecture with dual channel efficientcnn for medical image segmentation. In Med. Imaging 2021:Image Process., pages 758768. SPIE, 2021. 1, 2 Ange Lou, Shuyue Guan, Hanseok Ko, and Murray H Loew.Caranet: context axial reverse attention network for segmen-tation of small medical objects. In Med. Imaging 2022: Im-age Process., pages 8192. SPIE, 2022. 6",
  "Vinod Nair and Geoffrey E Hinton. Rectified linear unitsimprove restricted boltzmann machines. In Int. Conf. Mach.Learn., pages 807814, 2010. 3": "Phan Ngoc Lan, Nguyen Sy An, Dao Viet Hang, Dao VanLong, Tran Quang Trung, Nguyen Thi Thuy, and Dinh VietSang. Neounet: Towards accurate colon polyp segmentationand neoplasm detection. In Adv. Vis. Comput. Int. Symp.,pages 1528. Springer, 2021. 6, 1 Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee,Mattias Heinrich, Kazunari Misawa, Kensaku Mori, StevenMcDonagh, Nils Y Hammerla, Bernhard Kainz, et al. Atten-tion u-net: Learning where to look for the pancreas. arXivpreprint arXiv:1804.03999, 2018. 1, 2, 3, 6",
  "Md Mostafijur Rahman and Radu Marculescu. Multi-scalehierarchical vision transformer with cascaded attention de-coding for medical image segmentation. In Med. ImagingDeep Learn., 2023. 1, 3, 5, 7": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:Convolutional networks for biomedical image segmentation.In Int. Conf. Med. Image Comput. Comput. Assist. Interv.,pages 234241. Springer, 2015. 1, 2, 6 Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-moginov, and Liang-Chieh Chen.Mobilenetv2: Invertedresiduals and linear bottlenecks. In IEEE Conf. Comput. Vis.Pattern Recog., pages 45104520, 2018. 2, 4",
  "Jinfeng Wang, Qiming Huang, Feilong Tang, Jia Meng, Jion-glong Su, and Sifan Song. Stepwise feature fusion: Localguides global. arXiv preprint arXiv:2203.03635, 2022. 1, 6": "Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, KaitaoSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-mid vision transformer: A versatile backbone for dense pre-diction without convolutions.In Int. Conf. Comput. Vis.,pages 568578, 2021. 1, 2 Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, KaitaoSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer.Comput. Vis. Media, 8(3):415424, 2022. 1, 2, 3, 5, 6",
  "Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In SoKweon. Cbam: Convolutional block attention module. InEur. Conf. Comput. Vis., pages 319, 2018. 1, 3, 4": "Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,Jose M Alvarez, and Ping Luo. Segformer: Simple and ef-ficient design for semantic segmentation with transformers.Adv. Neural Inform. Process. Syst., 34:1207712090, 2021.2, 3 Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou,Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformeris actually what you need for vision. In IEEE Conf. Comput.Vis. Pattern Recog., pages 1081910829, 2022. 1 Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.Shufflenet: An extremely efficient convolutional neural net-work for mobile devices. In IEEE Conf. Comput. Vis. PatternRecog., pages 68486856, 2018. 4 Yundong Zhang, Huiye Liu, and Qiang Hu. Transfuse: Fus-ing transformers and cnns for medical image segmentation.In Int. Conf. Med. Image Comput. Comput. Assist. Interv.,pages 1424. Springer, 2021. 1, 3, 6 Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, NimaTajbakhsh, and Jianming Liang. Unet++: A nested u-net ar-chitecture for medical image segmentation. In Deep Learn.Med. Image Anal. Multimodal Learn. Clin. Decis. Support,pages 311. Springer, 2018. 1, 2, 6",
  ". Datasets": "To evaluate the performance of our EMCAD decoder, wecarry out experiments across 12 datasets that belong to sixmedical image segmentation tasks, as described next.Polyp segmentation:We use five polyp segmentationdatasets: Kvasir (1,000 images), ClinicDB (612 im-ages), ColonDB (379 images), ETIS (196 images),and BKAI (1,000 images). These datasets contain im-ages from different imaging centers/clinics, having greaterdiversity in image nature as well as size and shape of polyps.Abdomen organ segmentation:We use the Synapsemulti-organ dataset1 for abdomen organ segmentation. Thisdataset contains 30 abdominal CT scans which have 3,779axial contrast-enhanced slices. Each CT scan has 85-198slices of 512 512 pixels. Following TransUNet , weuse the same 18 scans for training (2,212 axial slices) and12 scans for validation. We segment only eight abdominalorgans, namely aorta, gallbladder (GB), left kidney (KL),right kidney (KR), liver, pancreas (PC), spleen (SP), andstomach (SM).Cardiac organ segmentation: We use ACDC dataset2 for cardiac organ segmentation.It contains 100 cardiacMRI scans having three sub-organs, namely right ventricle(RV), myocardium (Myo), and left ventricle (LV). Follow-ing TransUNet , we use 70 cases (1,930 axial slices) fortraining, 10 for validation, and 20 for testing.Skin lesion segmentation: We use ISIC17 (2,000training, 150 validation, and 600 testing images) andISIC18 (2,594 images) for skin lesion segmentation.Breast cancer segmentation: We use BUSI dataset forbreast cancer segmentation. Following , we use 647(437 benign and 210 malignant) images from this dataset.Cell nuclei/structure segmentation: We use the DSB18 (670 images) and EM (30 images) datasets of bio-logical imaging for cell nuclei/structure segmentation.We use a train-val-test split of 80:10:10 in ClinicDB,Kvasir, ColonDB, ETIS, BKAI, ISIC18, DSB18, EM, andBUSI datasets. For ISIC17, we use the official train-val-testsets provided by the competition organizer.",
  ". Qualitative results": "This subsection describes the qualitative results of differ-ent methods including our EMCAD. From, the qualitativeresults on Synapse Multi-organ dataset in , we cansee that most of the methods face challenges segmenting theleft kidney (orange) and part of the pancreas (pink). How-ever, our PVT-EMCAD-B0 (g) and PVT-EMCAD-B2 (h) can segment those organs more accurately(see red rectangular box) with significantly lower computa-tional costs. Similarly, qualitative results of polyp segmen-tation on a representative image from ClinicDB dataset in show that predicted segmentation outputs of ourPVT-EMCAD-B0 (p) and PVT-EMCAD-B2 (Fig-ure 5q) have strong overlaps with the GroundTruth mask(r), while existing SOTA methods exhibit false seg-mentation of polyp (see red rectangular box).",
  ". Effect of transfer learning from ImageNet pre-trained weights": "We conduct experiments on the Synapse multi-organ datasetto show the effect of transfer learning from the ImageNetpre-trained encoder. reports the results of these ex-periments which show that transfer learning from ImageNetpre-trained PVT-v2 encoders significantly boosts the per-formance. Specifically, for PVT-EMCAD-B0, the DICE,mIoU, and HD95 scores are improved by 4.5%, 5.92%,and 2.54, respectively. Likewise, for PVT-EMCAD-B2, theDICE, mIoU, and HD95 scores are improved by 3.45%,4.44%, and 3.15, respectively. We can also conclude thattransfer learning has a comparatively greater impact on the smaller PVT-EMCAD-B0 model than the larger PVT-EMCAD-B2 model. For individual organs, transfer learn-ing significantly boosts the performance of all organ seg-mentation, except the Gallbladder (GB).",
  ". Effect of input resolutions": "presents the results of our PVT-EMCAD-B0 andPVT-EMCAD-B2 architectures with different input resolu-tions. From this table, it is evident that the DICE scores im-prove with the increase in input resolution. However, theseimprovements in DICE score come with the increment in#FLOPs. Our PVT-EMCAD-B0 achieves an 85.52% DICEscore with only 3.36G FLOPs when using 512 512 in-puts. On the other hand, our PVT-EMCAD-B2 achievesthe best DICE score (86.53%) with 22.39G FLOPs whenusing 512 512 inputs. We also observe that our PVT-EMCAD-B2 with 5.60G FLOPs when using 256 256 in-puts shows a 1.05% lower DICE score than PVT-EMCAD-B0 with 3.36G FLOPs. Therefore, we can conclude thatPVT-EMCAD-B0 is more suitable for larger input resolu-tions than PVT-EMCAD-B2."
}