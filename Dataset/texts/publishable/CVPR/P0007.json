{
  "Hanfang Liang1, Jinming Hu2, Xiaohuan Ling3, Bing Wang4": "Abstract The increasing deployment of small drones astools of conflict and disruption has amplified their threat,highlighting the urgent need for effective anti-drone measures.However, the compact size of most drones presents a significantchallenge, as traditional supervised point cloud or image-basedobject detection methods often fail to identify such smallobjects effectively. This paper proposes a simple UAV detectionmethod using an unsupervised pipeline. It uses spatial-temporalsequence processing to fuse multiple lidar datasets effectively,tracking and determining the position of UAVs, so as to detectand track UAVs in challenging environments. Our methodperforms front and rear background segmentation of pointclouds through a global-local sequence clusterer and parsespoint cloud data from both the spatial-temporal density andspatial-temporal voxels of the point cloud. Furthermore, ascoring mechanism for point cloud moving targets is proposed,using time series detection to improve accuracy and efficiency.We used the MMAUD dataset, and our method achieved 4thplace in the CVPR 2024 UG2+ Challenge, confirming theeffectiveness of our method in practical applications.",
  "I. INTRODUCTION": "Drones have received huge attention in various real-worldapplications, such as surveillance, military applications, sur-veying and mapping monitoring, etc. Regarding drone mon-itoring, currently there are mainly vision-based detectionsolutions. However, when the UAV is far away from thetarget, it is difficult to detect the position of the UAV solelyby relying on images, and it is also difficult to obtain thespatial location information of the UAV solely by relying onimages.Compared with the two-dimensional image detection ofUAVs, the three-dimensional spatial position detection andestimation of UAVs is more challenging. It is not onlynecessary to detect and track the target UAV, but also toestimate the spatial position of the UAV. The main challengesare: first, when the drone is flying in a high-altitude orbit,the image features of the drone are weak and only occupya few pixels. At the same time, it is easily affected bythe environment such as lighting and is difficult to identify.Secondly, the point cloud of the drone scanned by lidar issparse, and the drone cannot be scanned in every frame,resulting in the point cloud characteristics of the drone being *This work was not supported by any organization1Hanfang Liang is with the School of Jianghan University , Wuhan,China. Hu is with the School of Jianghan University , Wuhan, Ling is with the School of Jianghan University , Wuhan,China. Wang is the corresponding author with the School of JianghanUniversity , Wuhan, China.",
  "(b) Small MAV (77 pixels)": ".Challenging examples of image and point cloud detection. Inthe point cloud, the scanning points of the drone are very sparse and notcontinuous in the time dimension. In many time frames, the drone is toosmall to be detected. In the picture, the drone is very tiny, only a dozenpixels. very unstable. Third, point cloud data is provided by multiplelidars, and some of the lidar data contain a lot of noise,making it difficult to correctly identify and track drones.Our goal is to build an unsupervised UAV point clouddetection method, segment the point cloud of the UAVtrajectory from the unknown point cloud space, retain onlythe point cloud containing the UAV trajectory, and restoreit combined with the timestamp information. The originaltrajectory of the drone.In this article, we mainly classify the point cloud pointsets through the clustering idea. In particular, we divide theclusterer into two parts. The global-local clusterer classifiesthe overall point cloud, and initially divides and excludes",
  "arXiv:2412.16947v1 [cs.CV] 22 Dec 2024": "buildings. Large objects such as objects and trees; then basedon the clustering results, the two attributes of spatiotemporaldensity and spatiotemporal voxels are further separated, andthe moving targets in the spatiotemporal sequence are furtherprocessed to select the point cloud corresponding to theUAV trajectory. Finally, a filter spline fitting operation isperformed on the point cloud, and the spatial position of theUAV is restored based on the timestamp interpolation.The main contributions of our work are as follows: We provide a simple and fast unsupervised detectionmethod for detecting drone trajectories and positionsfrom point cloud data. Our method only uses point cloudsources to detect drones and only uses lidar data inthe MMAUD dataset. It does not rely on complex deeplearning algorithms and can be quickly deployed in edgedevices. We propose a spatio-temporal voxel and spatio-temporaldensity analysis method for point cloud moving targets,and a scoring mechanism to evaluate the confidence ofthe point cloud to isolate the correct trajectory point set.",
  "A. Vision-based UAV tracking": "There are currently many vision-based UAV detectionmethods and have received attention in many applied tasks.With the rapid development of deep learning, there havebeen many studies on UAV detection methods based ondeep learning. Y. Zheng et al. evaluated eight state-of-the-artdeep learning algorithms for MAV detection on the Det-Flydataset. B. K. S. Isaac-Medina et al. evaluated four state-of-the-art deep learning algorithms on three representativeMAV datasets (MAV-VID, Drone-vs-Bird, Anti-UAV). Inorder to further improve the accuracy of target detection, H.Liu et al. implemented a special data augmentation methodand pruned the convolution channels and skip layers ofYOLOv4 for small UAV detection. C. Rui et al. proposeda novel comprehensive method that combines transfer learn-ing and adaptive fusion based on simulated data to improvesmall target detection performance.Motion-assisted micro air vehicle (MAV) detection meth-ods aim to detect MAVs by combining motion features andappearance features. Existing motion-assisted MAV detectionmethods can be divided into two categories: fixed camerasand mobile cameras. U. Seidaliyeva et al. used a fixed camera to monitor the sky and then used background subtractionand CNN-based object classification for MAV detection.J. Xie et al. used a method to fuse the spatiotemporalcharacteristics of the target for the detection of long-rangeflying drones-. Y. Zheng et al. used appearance featuresto exclude non-MAV moving targets, and then used motion-based classification algorithms to distinguish MAVs fromother distractors.MAV detection from moving cameras is more challengingthan from fixed cameras because the motion of the back-ground is coupled with the motion of the target. J. Li et al.proposed a UAV-to-UAV video dataset and a general archi-tecture for small MAV detection from cameras mounted onmobile MAV platforms. The authors detect moving MAVs bysubtracting adjacent frames and then use a hybrid classifierto identify MAVs.M. W. Ashraf et al. proposed a two-stage segmentationmethod. In the first stage, the authors utilize 2D convolutionalnetworks and channel-wise pixel-level attention to extractcontextual information from overlapping patches. Then, a 3Dconvolutional network and channel-pixel-level attention areused to learn spatiotemporal cues and discover first-stageomission detection. The method in proposes a UAVdetector based on feature super-resolution, which is basedon motion information extraction of dense optical flow.However, this is still challenging for air-to-air MAV detectionin complex environments.",
  "B. LiDAR-based UAV tracking": "While lidar systems are commonly used to detect and trackobjects, unmanned aerial vehicles (UAVs) present uniquechallenges in detecting and tracking UAVs due to their smallsize, shape, diverse materials, high speeds, and unpredictablemotion challenges .Li proposed a new method for tracking drones using lidarpoint clouds. They consider the speed and distance of thedrone to adjust the lidar frame integration time, parametersthat have an impact when dealing with the density and sizeof the point cloud.Sedat Dogru et al. suggest that detection can be accom-plished using fewer lidar beams as long as a probabilisticanalysis of detection is performed and appropriate settingsare ensured. When tracking a small number of hit pointscontinuously, the limitations of 3D lidar technology can beovercome by moving the sensor to increase the field ofview and improve coverage. Razlaw, J. et al. proposeda method that combines segmentation methods and simpleobject models while utilizing temporal information to over-come the limitations of 3D lidar technology and improveUAV detection and tracking capabilities. Wang, H etal. used Euclidean distance clustering and particle filteringalgorithms to complete UAV detection and tracking. Sieret al. proposed the concept of lidar as a camera to trackdrones without any prior knowledge about the data contentthrough the fusion of images and point cloud data generatedby a single lidar sensor. Using a custom YOLOv5 model .We first superimpose all point clouds in the sequence to obtain the global point cloud, then separate the data from different lidars, and onlyperform denoising processing on the data from DJI Livox Avia. The picture in the box on the right is radar data that has been processed by noise reduction,and is rendered and distinguished according to point cloud density and point cloud height. The greater the density of the point cloud and the higher thealtitude, the more red the color becomes; the red trajectory in the picture on the right is the real trajectory of the drone. It can be seen that the noise iswell processed and the trajectory point cloud of the drone is preserved. trained on panoramic images, they were able to integratecomputer vision capabilities directly onto the lidar itself.Although deep learning methods have made great progressrelative to traditional methods, these methods are eithertoo time-consuming or only effective when the target islarge enough or the background is very simple, and fordifferent scenarios and different drone types , data-drivendeep learning methods require large UAV data sets. There-fore, there are still many challenges in the deep learningMAV detection method based on image appearance, such asdifficulty in detection in complex background environmentsand difficulty in detection of small objects. At the same time,for image-based detection, it is difficult to estimate the three-dimensional position of the drone, making it difficult to meetthe needs of many practical applications.The use of lidar technology offers multiple ways toimprove drone detection and tracking and explore new tech-nologies to overcome the unique challenges posed by thesesmall, fast-moving and unpredictable objects. But at the sametime, the lidar point cloud detection method also needs toface problems such as point cloud noise, the sparsity of thelidar point cloud, and the discontinuity of small targets inthe point cloud sequence. And compared to detectingthe position of the drone in the image, this article focuses onusing unsupervised methods to detect the pose and trajectoryof the drone, as well as detecting and predicting the spatio-temporal coordinates of the drone.",
  "III. METHODS": "This section presents the details of our proposed method.To effectively detect MAVs under challenging conditions,we propose a clustering-based point cloud unsupervisedspatial-temporal sequence UAV trajectory detection method.Consists of three parts. In Sect. 3.1, We first denoise the pointclouds data from different LiDARs; In Sect. 3.2, we design a global clusterer and a local clusterer; In Sect. 3.3, we willintroduce a scoring mechanism to evaluate the confidenceof each clustering result and filter the clustering categoriesfor separation. UAV trajectory; In Sect. 3.4, we combine theprocessed UAV point cloud with time frame regression to fitthe UAV spatial position.",
  "A. Point clouds denoise": "The detected point cloud data comes from two lidar sen-sors, DJI Livox Avia and DJI Livox Mid360. By combiningthe two radars, a lidar scanning range close to full coveragefrom the ground to the sky is obtained. However, lidar datais a sparse signal, and the DJI Livox Avia radar will beaccompanied by a lot of noise, which can range up to400 meters, and the small targets of the drone itself arevery similar to the noise. If the data with a lot of noiseis used directly, it will leading to disastrous consequences.Therefore, we first need to denoise the lidar data from DJILivox Avia.We first superimpose the continuous lidar sequences andsuperimpose all the sequences. We can find that the noisedensity from the sensor belongs to the sparsest category.Based on this, we exclude noise points based on the density,while ensuring high accuracy. as shown in picture 2,. Theexcluded noise points are then updated to the lidar sequenceto facilitate the use of subsequent local and global clusteringmethods, which will be introduced in the next section.",
  "B. Global-local point set clusterings": "Lidar data is a sparse signal accompanied by a lot of noise.For the same stationary object, the points scanned at differenttimes still have large spatial differences. However, as the timedimension increases, the point set density on the stationaryobject increases. will increase significantly. Inspired by someprevious clustering ideas, the point set is initially divided into",
  "Cluster": ".Our proposed algorithm architecture. Given a continuous point cloud input sequence, we first classify it into global clustering and local clusteringto obtain different categories using DBSCAN. Then the number of point clouds and voxel spatial information are calculated for global and local categoriesrespectively. And cross-compare and calculate the spatial coincidence degree and temporal density changes of different clusters. The final scoring mechanismcalculates the spatial coincidence degree and relative density score of the categories to exclude point clouds other than the UAV, restores the trajectory ofthe UAV through spline fitting interpolation, and uses MSE to calculate the error with the true value.",
  "point sets of different densities through density and distanceclustering": "In DBSCAN, the density associated with a point is ob-tained by counting the number of points within a specifiedradius area around the point. Points with density above aspecified threshold are constructed as clusters. Among theexisting clustering algorithms, we chose the DBSCAN algo-rithm because of its ability to discover clusters of arbitraryshapes, such as linear, concave, elliptical, etc. Moreover,compared with some clustering algorithms, it does not re-quire the shape of the clusters to be determined in advance.DBSCANs proven ability to handle large data. A key issue in this method is how to correctly excludepoint sets with different densities and retain the correctpoint set containing only UAV trajectories. Considering thatstationary object surfaces will accumulate more and morelidar point clouds as the time dimension increases. Formoving objects of the same volume, the probability of beingscanned by the lidar is the same in continuous time periods.Therefore, the point cloud density of a moving object shouldbe relatively stable in continuous time frames. In addition, formoving objects, the volume of the point cloud in the voxelspace will change as the time dimension changes. Therefore,our method processes the spatio-temporal sequence from thetwo perspectives of point cloud density and voxel volume,and filters out the point cloud of the UAV trajectory.",
  "frame+framesi=framePi": "We define the category k of the cluster in the global pointset Pglobal as Ckglobal, the category k of the cluster in thelocal point set Pframelocalas Ck, framelocal. In particular, the point setin Ck, framelocalis not a new cluster set obtained by DBSCANre-clustering, but the corresponding points in Ckglobal withina period of time in frame (1,2,...,n frames).The Voxel space of the category k of the cluster Ckglobalin global point set Ckglobal as V kglobal, The Voxel space of thecategory k of the cluster Ck, framelocalin the local point set Pframelocalas V k, framelocal. The number of points of the category k of thecluster in local point set Pframelocalthat changes over time isNumk,framelocal.We first superimpose the point cloud on the global timeframe to obtain Pglobal, and use DBSCAN to perform clus-tering to obtain Ckglobal. We define and use closer and denserclustering parameters and MinPts, and record the volumeof each class at the same time. Prime information V. Theglobal density of the point set is also recorded. Although a low threshold will bring some false targets, the most reliabletargets are obtained by applying the confidence ranking andscoring mechanism, which will be introduced in detail in.2.We calculate the density kglobal of the point cloud in pointset Pglobal according to Ckglobal.",
  "V kglobal": "Considering that the target MAV will not cause drasticchanges in spatial position in continuous time, a local pointclusterer is used at this time. for local point clusterer, Firstcalculate the density k, framelocalof the point cloud in point setP framelocalaccording to Ck, framelocal. And Simultaneously calculatethe spatial Intersection over Union (IoU) of the overlappingareas of voxels. Define the IoU of voxels in category k ofthe cluster Ck, framelocalbetween frame i and j as IoUi,jk .",
  "C. Scoring Mechanism": "In this section, we will use the density and voxel co-incidence obtained in the previous section. For a movingobject, in the adjacent time dimension, as its spatial positionchanges, the voxel position in the space will also changeaccordingly. The local voxel coincidence should be smallerthan the voxel coincidence of stationary objects. At thesame time, as the time dimension increases, the densityof point clouds accumulated on the surface of stationaryobjects will also increase. Therefore, the point cloud densityof stationary objects will have a larger difference in global-local relative density; while the density of point clouds ofmoving objects will be It will be relatively consistent withthe overall situation within a local time period.Based on this inference, we designed a scoring mechanismto evaluate the confidence of point clouds in both densityand voxel dimensions.And in order to make the value morestable and retain the changing trend of the value, we usethe logarithmic function to map the voxel IoU to a morebalanced scale.",
  "Scorek = Scorekdens + ScorekIoU": "According to the calculation formula, the category withthe highest score can be filtered out. Specifically, the targetwith the highest confidence is selected as the final target.That is, the drone trajectory. In order to confirm thepracticality of the confidence representation of the scoringmechanism, we randomly selected several groups of se-quences to compare the separated trajectory point clouds withthe true values, as shown in Figure (), which significantlyremoved cluttered point clouds such as background, whileretaining the characteristics of the UAV Trajectory pointcloud.",
  "D. Trajectory Prediction": "For the final trajectory based on time frame, we use splinefitting on the UAV point cloud, and then interpolate based onthe time frame as the spatial position of the correspondingtime frame. For the time frame where the background issegmented, there may be multiple point clouds in the sameframe. After data collection and sorting, even if there aremultiple points corresponding to the same timestamp, theywill be sorted into consecutive blocks in a list in chronolog-ical order.This means that for the same timestamp, all correspondingpoint cloud data are processed instead of just one point.Define the k-th point cloud frame after segmenting thebackground as Pks . Sort the point clouds of each time frameaccording to the timestamp and merge them into a pointset Puav =P0s ,P1s ,...,Pks. Among them, the points in thepoint set Puav are selected as control points, and the three-dimensional spline S(u) can be expressed as:",
  "A. Dataset": "To evaluate the performance of the proposed algorithm,we tested our proposed algorithm on MMAUD challengingdataset. MMAUD dataset is briefly introduced below.The MMAUD dataset provides a multi-modal datasetthat integrates visual, LIDAR array, RADAR, and audioarray sensors, and high-precision ground truth. The datasetcontains over 1700 seconds of multimodal data divided into50 different sequences. Each sequence contains sufficientvisual, lidar, audio and radar data for identification purposes.Some example images are shown along with the visualizedpoint cloud.",
  "AllSequenceTime": "Because for image sequences, there are situations where thetarget cannot be detected, and in this case, it is obviouslymore difficult to predict the spatial position coordinates ofthe drone. Therefore, when the target cannot be detected,predicting a completely irrelevant spatial position predictioncoordinate will be meaningless, and the MSE of the methodusing camera modal data is very large. Therefore, the SDAindicator also reflects the detection ability of different al-gorithms for small drones in complex backgrounds in thiscase.The detection results of our method are shown in . For the convenience of display, we superimpose thetrajectories of the entire sequence. The green trajectory is thedrone trajectory point cloud segmented from the backgroundby global and local clustering methods; the red trajectory isthe real spatial position of the drone; and the blue trajectoryis the spatial position of the drone predicted by our method.Our method has achieved good results in eliminating noiseand extracting the correct drone trajectory from the pointcloud space.",
  "V. CONCLUSIONS": "In this paper, we propose an unsupervised approach toMAV point cloud detector for ground-to-space detection ofMAVs under challenging conditions. Our method employsspatial-temporal global local clustering of point cloud se-quences for extracting effective UAV point cloud trajectoriesfrom sparse and noisy point clouds.Our method won the 4th place in the CVPR2024 UG2+Challenge, confirming the effectiveness of our method.Moreover, our method is interpretable. At the same time,we use the MMAUD dataset, evaluate several representativedeep learning algorithms, and analyze the experimental re-sults.In the future, in order to detect different drones in theenvironment, the types of drones should be classified basedon the distribution of drone point cloud trajectories and deeplearning technology.References are important to the reader; therefore, eachcitation must be complete and correct. If at all possible,references should be commonly available publications. Y. Zheng, Z. Chen, D. Lv, Z. Li, Z. Lan, and S. Zhao, Air-to-airvisual detection of micro-uavs: An experimental evaluation of deeplearning, IEEE Robotics and automation letters, vol. 6, no. 2, pp.10201027, 2021. B. K. Isaac-Medina, M. Poyser, D. Organisciak, C. G. Willcocks, T. P.Breckon, and H. P. Shum, Unmanned aerial vehicle visual detectionand tracking using deep neural networks: A performance benchmark,in Proceedings of the IEEE/CVF International Conference on Com-puter Vision, 2021, pp. 12231232. H. Liu, K. Fan, Q. Ouyang, and N. Li, Real-time small dronesdetection based on pruned yolov4, Sensors, vol. 21, no. 10, p. 3374,2021. C. Rui, G. Youwei, Z. Huafei, and J. Hongyu, A comprehensiveapproach for uav small object detection with simulation-based transferlearning and adaptive fusion, arXiv preprint arXiv:2109.01800, 2021. U. Seidaliyeva, D. Akhmetov, L. Ilipbayeva, and E. T. Matson, Real-time and accurate drone detection in a video with a static background,Sensors, vol. 20, no. 14, p. 3856, 2020. J. Xie, C. Gao, J. Wu, Z. Shi, and J. Chen, Small low-contrast targetdetection: Data-driven spatiotemporal feature fusion and implementa-tion, IEEE transactions on cybernetics, vol. 52, no. 11, pp. 11 84711 858, 2021. J. Xie, J. Yu, J. Wu, Z. Shi, and J. Chen, Adaptive switchingspatial-temporal fusion detection for remote flying drones, IEEETransactions on Vehicular Technology, vol. 69, no. 7, pp. 69646976,2020. Y. Zheng, C. Zheng, X. Zhang, F. Chen, Z. Chen, and S. Zhao,Detection, localization, and tracking of multiple mavs with panoramicstereo camera networks, IEEE transactions on automation scienceand engineering, vol. 20, no. 2, pp. 12261243, 2022. J. Li, D. H. Ye, M. Kolsch, J. P. Wachs, and C. A. Bouman, Fastand robust uav to uav detection and tracking from video, IEEETransactions on Emerging Topics in Computing, vol. 10, no. 3, pp.15191531, 2021. M. W. Ashraf, W. Sultani, and M. Shah, Dogfight: Detecting dronesfrom drones videos, in Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2021, pp. 70677076. H. Wang, X. Wang, C. Zhou, W. Meng, and Z. Shi, Low in resolution,high in precision: Uav detection with super-resolution and motioninformation extraction, in ICASSP 2023-2023 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP).IEEE, 2023, pp. 15. A. Lei, T. Deng, H. Wang, J. Yang, and S. Yuan, Audio array-based3d uav trajectory estimation with lidar pseudo-labeling, 2025 IEEEInternational Conference on Acoustics, Speech, and Signal Processing,2025. L. Qingqing, Y. Xianjia, J. P. Queralta, and T. Westerlund, Adaptivelidar scan frame integration: Tracking known mavs in 3d point clouds,in 2021 20th International Conference on Advanced Robotics (ICAR).IEEE, 2021, pp. 10791086. S. Dogru and L. Marques, Drone detection using sparse lidar mea-surements, IEEE Robotics and Automation Letters, vol. 7, no. 2, pp.30623069, 2022. J. Razlaw, J. Quenzel, and S. Behnke, Detection and tracking ofsmall objects in sparse 3d laser range data, in 2019 International Conference on Robotics and Automation (ICRA).IEEE, 2019, pp.29672973. H. Wang, Y. Peng, L. Liu, and J. Liang, Study on target detection andtracking method of uav based on lidar, in 2021 Global Reliability andPrognostics and Health Management (PHM-Nanjing).IEEE, 2021,pp. 16. H. Sier, X. Yu, I. Catalano, J. P. Queralta, Z. Zou, and T. Westerlund,Uav tracking with lidar as a camera sensor in gnss-denied environ-ments, in 2023 International Conference on Localization and GNSS(ICL-GNSS).IEEE, 2023, pp. 17. H. Liang, Y. Yang, J. Hu, J. Yang, F. Liu, and S. Yuan, Unsuperviseduav 3d trajectories estimation with sparse point clouds, 2025 IEEEInternational Conference on Acoustics, Speech, and Signal Processing,2025. Z. Xiao, H. Hu, G. Xu, and J. He, Tame: Temporal audio-basedmamba for enhanced drone trajectory estimation and classification,2025 IEEE International Conference on Acoustics, Speech, and SignalProcessing, 2025."
}