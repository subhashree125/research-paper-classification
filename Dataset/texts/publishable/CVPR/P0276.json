{
  "Abstract": "Video summarization aims to generate a concise repre-sentation of a video, capturing its essential content and keymoments while reducing its overall length. Although severalmethods employ attention mechanisms to handle long-termdependencies, they often fail to capture the visual signif-icance inherent in frames. To address this limitation, wepropose a CNN-based SpatioTemporal Attention (CSTA)method that stacks each feature of frames from a singlevideo to form image-like frame representations and applies2D CNN to these frame features. Our methodology relieson CNN to comprehend the inter and intra-frame relationsand to find crucial attributes in videos by exploiting its abil-ity to learn absolute positions within images. In contrast toprevious work compromising efficiency by designing addi-tional modules to focus on spatial importance, CSTA re-quires minimal computational overhead as it uses CNN asa sliding window. Extensive experiments on two benchmarkdatasets (SumMe and TVSum) demonstrate that our pro-posed approach achieves state-of-the-art performance withfewer MACs compared to previous methods.Codes areavailable at",
  ". Introduction": "The rise of social media platforms has resulted in atremendous surge in daily video data production. Due to thehigh volume, diversity, or redundancy, it is time-consumingand equally difficult to retrieve the desired content or editmultiple videos. Video summarization is a powerful time-saving technique to condense long videos by retaining themost relevant information, making it easier for users toquickly grasp the main points of the video without havingto watch the entire footage.One of the challenges that occur during video summa-rization is the long-term dependency problem, where theinitial information is often lost due to large data intervals",
  ". Approaches for calculating attention. Each row is thefeature vector of a frame. T is the number of frames, and D is thedimension of the feature": ". The decay of initial data prevents deeplearning models from capturing the relation between framesessential for determining key moments in videos. Attention, in which entire frames are reflected through pairwiseoperations, has gained popularity as a widely adopted tech-nique for solving this problem . Attention-based models distinguish important parts from unimportantones by determining the mutual reliance between frames.However, attention cannot consider spatial contexts withinimages . For instance, current atten-tion calculates temporal attention based on correlations ofvisual attributes from other frames (See a), but theimportance of visual elements within the frame remains un-equal to the temporal significance. Including spatial depen-dency leads to different weighted values of features, caus-ing changes in temporal importance. Therefore, attentioncan be calculated more precisely by including visual asso-ciations, as shown in b.Prior studies mixed spatial importance and performedbetter than solely relying on sequential connections . Nevertheless, acquiring spatial and tem-poral importance requires the design of additional modulesand, thus, incurs excessive costs. Some studies used addi-tional structures to embrace visual relativities in individualframes, such as self-attention , multi-head attention, and graph convolutional neural networks . Pro-cessing too many frames of lengthy videos to capture thetemporal and visual importance can be expensive. Thus,",
  ". Workflow of CSTA": "obtaining both inter and intra-frame relationships with fewcomputation resources becomes a non-trivial problem.This paper introduces CNN-based SpatioTemporalAttention (CSTA) to simultaneously capture the visual andordering reliance in video frames, as shown in .CSTA works as follows: Firstly, it extracts features offrames from a video and then concatenates them. Secondly,it treats the assembled frame representations as an im-age and applies a 2D convolutional neural network (CNN)model to them for producing attention maps.Finally, itcombines the attention maps with frame features to predictthe importance scores of frames. CSTA derives spatial andtemporal relationships in the same manner as CNN derivespatterns from images, as shown in c. Further, itsearches for vital components in frame representations withthe capacity of a CNN to infer absolute positions from im-ages . Unlike previous methods, CSTA is efficientas a one-way spatiotemporal processing algorithm becauseit uses a CNN as a sliding window.We test the efficacy of CSTA on two benchmark datasets- SumMe and TVSum . Our experiment validatesthat a CNN produces attention maps from frame features.Further, CSTA needs fewer multiply-accumulate opera-tions (MACs) than previous methods for considering the vi-sual and sequential dependency. Our contributions are sum-marized below: To the best of our knowledge, the proposed model appearsto be the first to apply 2D CNN to frame representationsin video summarization.",
  ". Attention-based Video Summarization": "Many video summarization models use attention to de-duce the correct relations between frames and find crucialframes in videos. A-AVS and M-AVS are encoder-decoder structures in which attention is used to find essen-tial frames. VASNet is based on plain self-attention forbetter efficiency than encoder-decoder-based ones. SUM-GDA also employs attention for efficiency and supple-ments diversity into the attention mechanism for generatedsummaries. CA-SUM further enhances SUM-GDA byintroducing uniqueness into the attention algorithm in unsu-pervised ways. Attention in DSNet helps predict scoresand precise localization of shots in videos. PGL-SUM has a mechanism to alleviate long-term dependency prob-lems by discovering local and global relationships by apply-ing multi-head attention to segments and the entire video.GL-RPE approaches similarly in unsupervised waysby local and global sampling in addition to relative positionand attention. VJMHT uses transformers and improvessummarization by learning similarities between analogousvideos. CLIP-It also relies on the transformers to pre-dict scores by cross-attention between frames and captionsof the video. Attention helps models recognize the relationsbetween frames, however, it does not focus on visual rela- tions.Visual relevance is vital to understanding video con-tent as it influences the expression of temporal depen-dency. Some studies have proposed additional networks tofind frame-wise visual relationships . Themodels process the temporal dependency and exploit self-attention or multi-head attention for visual relations of ev-ery frame. RR-STG uses graph CNNs to draw spatialassociations using graphs. RR-STG creates graphs basedon elements from object detection models to capturethe spatial relevance. These methods offer increased per-formance but incur a high computational cost owing to theseparate module handling many frames. This paper adoptsCNN as a one-way mechanism for more efficient reflectonof the spatiotemporal importance of multiple frames in longvideos.",
  ". CNN for Efficiency and Absolute Positions": "CNN is usually employed to resolve computation prob-lems in attention. CvT uses CNN for token embeddingand projection in vision transformers (ViT) and requiresa few FLOPs. CeiT uses both CNN and transformersand shows better results with fewer parameters and FLOPs.CmT applies depth-wise convolutional operations toobtain a better trade-off between accuracy and efficiency forViT. We exploit CNN to enhance the efficiency of dealingwith multiple frames in video summarization.CNN can be used for attention by learning absolute po-sitions from images. Islam et al. proved that featuresextracted using a CNN contain position signals. They at-tributed it to padding, and Kayhan and Germert verifiedthe same under various paddings. CPVT uses this abil-ity to reflect the position information of tokens and to tackleproblems in previous positional encodings for ViT. Basedon this behavior of CNNs, our proposed method is designedto seek only the necessary elements for video summariza-tion from frame representations by considering frame fea-tures as images.",
  ". Overview": "This study approaches video summarization as a subsetselection problem. We show the proposed CSTA frame-work in . During the Embedding Process, the modelconverts the frames into feature representations. The Pre-diction Process involves using these representations to pre-dict importance scores. In the Prediction Process, the Atten-tion Module generates attention for videos, and the MixingModule fuses this attention with input frame features. Fi-nally, the CSTA predicts every frames importance score,representing the probability of whether the frame should beincluded in the summary videos. The model is trained by",
  ". Embedding Process": "CSTA converts frames to features for input into themodel, as depicted in the Embedding Process ().Let the frames be X = {xi}Tt=1 when there are T framesin a video, with H as the height and W as the width. Fol-lowing for a fair comparison, thefrozen pre-trained CNN model (GoogleNet ) modifiesX RT3HW into X RTD where D is the dimen-sion of frame features.To fully utilize the CNN, we replicate the frame repre-sentations to match the number of channels (i.e., three). ACNN is usually trained using RGB images ;therefore, pre-trained models are well-optimized on imageswith three channels. Additionally, we concatenate the clas-sification token (CLS token) into frame features:",
  "E = Concataxis=1(XCLS, X )(2)": "where X R3TD and XCLS R31D are theappendedfeatureandtheCLStoken,respectively.E R3(T+1)D is the embedded feature. Concataxis=0and Concataxis=1 concatenate features in the channel axisand T axis, respectively. Motivated by STVT , we ap-pend the CLS token with input frame features. The CLStoken is the learnable parameters fed into the models withinputs and trained with models jointly. STVT obtains cor-relations of frames using the CLS token and aggregates theCLS token with input frames to capture global contexts. Wefollow the same method in prepending and combining theCLS token with frame features. The fusing process is com-pleted later in the Mixing Module.",
  ". Prediction Process": "CSTA calculates importance scores for T frames, asshown in the Prediction Process (). The classifierassigns scores to frames after the Attention Module and Mix-ing Module. The Attention Module makes attention mapsfrom E, and the Mixing Module aggregates this attentionwith E. A detailed explanation is given in Algorithm 1.We generate the key and value from E by using two lin-ear layers based on the original attention . The metricsW K and W V RDD are weights of linear layers pro-jecting E into the key and value (Line 2-Line 3). UnlikeE K, CSTA uses a single channel of frame features in E toproduce features by value embedding (Line 3) because weonly need one X except for duplicated ones, which are sim-ply used for reproducing image-like features. We select thefirst index as a representative, which is E R(T+1)D.",
  "The Attention Module processes spatiotemporal charac-teristics and focuses on critical attributes in E K (Line 5).We add positional encodings to P to strengthen the abso-": "lute position awareness further (Line 6). Unlike the preva-lent way of adding positional encoding into inputs ,this study adds positional encoding into the attention mapsbased on . This is because adding positional encodingsinto input features distorts images so that models can recog-nize this distortion as different images. Moreover, modelscannot fully recognize these absolute position encodings inimages during training owing to a lack of data. Therefore,CSTA makes Ppos by attaching positional encodings to at-tentive features P.The Mixing Module inputs Ppos and E V and producesmixed features M R(T +1)D (Line 7). The classifier pre-dicts importance scores vectors S RT from M (Line 8).",
  ". Attention Module": "The Attention Module () produces attentionmaps by utilizing a trainable CNN (GoogleNet ) han-dling frame features E K. The CNN captures the spatiotem-poral dependency using kernels, similar to how a CNN learns from images, as shown in c. The CNN alsosearches for essential elements from E K for summariza-tion, with the ability to learn absolute positions. Based on, CNN imbues representations with positional in-formation so that CSTA can encode the locations of signif-icant attributes from frame features for summarization.We make the shape of attention maps the same as thatof input features to aggregate attention maps with inputfeatures.This study leverages two strategies for equalscale: deploying the adaptive pooling operation and usingthe same CNN model (GoogleNet ) in the EmbeddingProcess and Attention Module. Pooling layers reduce thescale of features in the CNN; therefore, the size of out-puts from the CNN is changed from E K R3(T +1)D",
  "r D": "r , where r is the reduction ratio.To expand diverse lengths of frame representations, we ex-ploit adaptive pooling layers to adjust the shape of featuresby bilinear interpolation. Furthermore, the number of out-put channels from the learnable CNN equals the dimen-sion of frame features from the fixed CNN because of thesame CNN models. The output from adaptive pooling isE Kpool RD(T +1)1.As suggested in , this study uses a skip connection:",
  "P = LayerNorm(E Kpool + E K)(3)": "where the output is P RD(T +1), followed by layernormalization . A skip connection supports more preciseattention and stable training in CSTA. As same with E V ,explained in Algorithm 1 (Line 3), we only use the singleframe feature of E K and ignore replications of frame fea-tures.The size of P is equal to the size of frame features with(T + 1) D; therefore, each value of P has the spatiotem-poral importance of frame features. By combining P withframe features, the CSTA reflects the sequential and visualsignificance of frames. After supplementing the positionalencodings, Ppos will be used as inputs for the Mixing Mod-ule.",
  "k = 1, ..., D(5)": "where AttT is the temporal importance, and AttD is thevisual importance. Equation (4) calculates the weighted val-ues between T + 1 frames, including the CLS token, in thesame dimension. Equation (5) computes the weighted val-ues between different dimensions in the same frame. AttDrepresents the spatial importance because each value of thedimension from features includes visual characteristics byCNN, processing image patterns, and producing informa-tive vectors.After acquiring weighted values, a dropout is employedfor these values before integrating them with E V .Thedropout erases parts of features by setting 0 values for bet-ter generalization; it also works for attention, as shown in. If a dropout is applied to inputs as in the original at-tention , the CNN cannot learn contexts from 0 values,unlike self-attention, because the dropout spoils the localcontexts of deleted parts. Therefore, we follow by ap-plying the dropout to the output of the softmax operationsfor generalization.After dropout, the CSTA combines the spatial and tem-poral importance with the frame features:",
  "M = AttT E V + AttD E V(6)": "where is the element-wise multiplication,andM R(T+1)D is the mixed representations. CSTA re-flects weighted values into frame features by blending AttTand AttD with E V by element-wise multiplication. Incor-porating visual and sequential attention values by additionencompasses spatiotemporal importance at the same time.Subsequently, to integrate the CLS token with framefeatures, adaptive pooling transforms M R(T+1)D intoM RTD by average. Unlike STVT , in which lin-ear layers are used to merge the CLS token with constantnumbers of frames, CSTA uses adaptive pooling to copewith various lengths of videos. Adaptive pooling fuses theCLS token with a few frames; however, it intensifies ourmodel owing to the generalization of the classifier, whichconsists of fully connected layers. M from adaptive pool-ing enters into the classifier computing importance scoresof frames.",
  "Lengthi 15%(11)": "where i is the index of selected shots. Si isthe importance score of the ith shot between 0 and 1, andLengthi is the percentage of the length of the ith shot in theoriginal videos. Our model picks shots with high scores byexploiting the 0/1 knapsack algorithm as in . Follow-ing , summary videos have a length limit of 15% of theoriginal videos.",
  ". Settings": "EvaluationMethods.WeevaluateCSTAusingKendalls () and Spearmans () coeffi-cients. Both metrics are rank-based correlation coefficientsthat are used to measure the similarities between model-estimated and ground truth scores. The F1 score is the mostcommonly used metric in video summarization; however, ithas a significant drawback when used to evaluate summaryvideos.Based on , due to the limitation of thesummary length, the F1 score is evaluated to be higher ifmodels choose as many short shots as possible and ignorelong key shots. This fact implies that the F1 score might notrepresent the correct performance in video summarization.A detailed explanation of how to measure correlations isprovided in Appendix A.1. Datasets.This study utilizes two standard video summa-rization datasets - SumMe and TVSum . SumMeconsists of videos with different contents (e.g., holidays,events, sports) and various types of camera angles (e.g.,static, egocentric, or moving cameras). The videos are rawor edited public ones with lengths of 1-6 minutes. At least15 people create ground truth summary videos for all data,and the models predict the average number of selections by",
  "SpearmanKendall": ". Comparison of summarizing performance between CNNand video summarization models. The x-axis shows performance,and the y-axis shows model names. Based on the dashed line, theperformance of CNN is displayed above, and the video summa-rization models are below. people for every frame. TVSum comprises 50 videos from10 genres (e.g., documentaries, news, vlogs). The videosare 2-10 minutes long, and 20 people annotated the groundtruth for each video.The ground truth is a shot-levelimportance score ranging from 1 to 5, and models try toestimate the average shot-level scores.",
  ". Verification of Attention Maps being Createdusing CNN": "Previous studies on video summarization have yet to ap-ply 2D CNN directly to frame features. Therefore, we ver-ify that CNN can create attention maps from frame fea-tures. We choose MobileNet-V2 , EfficientNet-B0 ,GoogleNet , and ResNet-18 as CNN models sincewe focus on limited computation costs. This study appliesCNN models to frame features and trains them to com-pute the frame-level importance scores without the classi-fier. The CNN directly exports T scores by inputting itsoutput features into the adaptive pooling layer with a targetshape T 1. As the importance score of each frame is be-",
  "GoogleNetST50.1760.19711.50.1290.163CSTAST10.2460.2742*0.194*0.255*": ".Comparison between CSTA and state-of-the-art onSumMe and TVSum. Rank is the average rank between Kendalls() and Spearmans () coefficients. We categorize different typesof video summarization models: temporal (T) and spatiotem-poral (ST) attention-based, multi-modal based (M ), and exter-nal dataset-based (+) models. The scores marked in bold andby the asterisk are the best and second-best ones, respectively.GoogleNet is the baseline model. Note that all feature extractionmodels are CNNs for a fair comparison. tween 0 and 1, each score is similar to the weighted valueof each frame. Thus, we can test whether CNN generatesattention maps based on the video summarization perfor-mance. Surprisingly, the CNN models predict the impor-tance scores much better than the previous video summa-rization models on SumMe , as shown in . Eventhough the CNN models do not perform best on TVSum,they still show promising performance compared to exist-ing video summarization models. The results show that theCNN produces attention maps by capturing the spatiotem-poral relations and detecting crucial attributes in frame fea-tures based on absolute position encoding ability, unlikeconventional methods that solely address the temporal de-pendency.",
  ". We listed Kendalls () and Spearmans () coefficientsfor different modules. (+) denotes the stacking of modules on topof the previous ones": "DMASum has and coefficients of 0.203 and 0.267 onTVSum, respectively, whereas 0.063 and 0.089 on SumMe,respectively. This implies that CSTA provides more stableperformances than DMASum, although it provides slightlylower performance than DMASum on TVSum. Based onthe overall performance of both datasets, our CSTA hasachieved state-of-the-art results.Further, CSTA excels in video summarization modelsrelying on classical pairwise attention ,focusing on temporal attention only.This clarifies thatconsidering the visual dependency helps CSTA understandcrucial moments by capturing meaningful visual contexts.Like CSTA, some approaches, including DMASum, focuson spatial and temporal dependency , butthey perform poorly compared to our proposed methodol-ogy. This is because CNN is much more helpful than previ-ous methods by using the ability to learn the absolute posi-tion in frame features.CSTA also outperforms methods that require additionaldatasets from other modalities or tasks . Our observations suggest that CSTA can find essen-tial moments in videos solely based on images without as-sistance from extra data. We also show the visualizationof generated summary videos from different models in Ap-pendix B.",
  ". Ablation Study": "This study verifies all components step-by-step, as in-dicated in . We deploy an attention structure withGoogleNet and a classifier for temporal dependency, de-noted as the (+)Attention Module.With the assistanceof the weighted values from CNN, there is a 0.008 incre-ment on SumMe and at least 0.047 on TVSum, showingthe power of CNN as attention. (+)AttD is the result ob-tained using softmax along the time and dimension axis toreflect the spatiotemporal importance. The improvementfrom 0.005 to 0.009 in both datasets indicates that consider-ing the spatial importance is meaningful. The Key and ValueEmbeddings strengthen CSTA as a linear projection based on . Although the (+)Positional Encoding reveals asmall performance drop of 0.004 for coefficient and 0.005for coefficient on TVSum, the performance increases sig-nificantly from 0.207 to 0.225 for coefficient and from0.231 to 0.251 for coefficient on SumMe. (+)XCLS isthe result obtained when utilizing the CLS token. Becausethis study combines the CLS token with adaptive pooling,the CLS token only affects a few video frames. However,adding the CLS token improves the performance on bothdatasets because it generalizes the classifier, which containsfully connected layers. We also see the effects of skip con-nection, denoted by (+)Skip Connection, as suggested by. The skip connection exhibits a similar performance onTVSum and an improvement of about 0.015 on SumMe.We also tested different CNN models as the baseline inAppendix C, various experiments of detailed constructionof our model in Appendix D, and several hyperparametersin Appendix E.",
  "CSTAST1413.03G9.78G2661.83G15.73G": ".Comparison of MACs between video summarizationmodels. Rank is the average rank between Kendalls and Spear-mans coefficients in . FE is the MACs during featureextraction, and SP is that during score predictions. We catego-rize models as temporal attention-based (T), spatiotemporal (ST)attention-based, and multi-modal based (M ) models. In this paper, we analyze the computation burdens ofvideo summarization models, focusing on the feature ex-traction and score prediction steps. The standard procedurefor creating summary videos comprises feature extraction,score prediction, and key-shot selection. Feature extractionis a necessary step in converting frames into features usingpre-trained models so that video summarization models cantake frames of videos as inputs. Score prediction is the stepin which video summarization models infer the importancescore for videos. Existing studies generally use the samekey-shot selection process based on the knapsack algorithmto determine important video segments, so we ignore com-putations of key-shot selection. displays MACs measurements and comparesthe computation resources during the inference per video.CSTA performs best with relatively fewer MACs than theother video summarization models. Based on the average rank from , more computational costs or supplemen-tal data from other modalities is inevitable for better videosummarization performance. Unlike previous approaches,CSTA exhibits high performance with fewer computationalresources by exploiting CNN as a sliding window.We find that our model is more efficient than previousones when considering spatiotemporal contexts. RR-STG shows much fewer MACs than CSTA during scorepredictions; however, it shows exceptionally more MACsduring feature extraction than others. RR-STG utilizes fea-ture extraction steps for visual relationships by inputtingeach frame into the object detection model , thereby,relying heavily on the pre-processing steps. While sum-marizing the new videos, RR-STG needs significant timeto get spatial associations even though the score predictiontakes less time. Other methods design twomodules to reflect spatial and temporal dependency, respec-tively, as shown in a and b.These ap-proaches become costly when processing numerous framesin long videos for video summarization. CSTA effectivelycaptures spatiotemporal importance in one way using CNN,as illustrated in c.Thus, our proposed methodshows superior performance by focusing on temporal andvisual importance.",
  ". Conclusion": "This study addresses the problem of attention in videosummarization. The existing pairwise attention-based videosummarization mechanisms fail to account for visual depen-dencies, and prior research addressing this issue involvessignificant computational demands. To deal with the sameproblem efficiently, we propose CSTA, in which a CNNsability is used for video summarization for the first time. Wealso verify that the CNN works on frame features and cre-ates attention maps. The strength of the CNN allows CSTAto achieve state-of-the-art results based on the overall per-formance of two popular benchmark datasets with fewerMACs than before. Our proposed model even outperformsmulti-modal or external dataset-based models without addi-tional data. For future work, we suggest further exploringhow CNN affects video representations by tailoring framefeature-specific CNN models or training feature-extractionand attention-based CNN models. We believe this study canencourage follow-up research on video summarization andother video-related deep-learning studies. Acknowledgements.ThisworkwassupportedbyKoreaInternet&SecurityAgency(KISA)grantfundedbytheKoreagovernment(PIPC)(No.RS-2023-00231200,Developmentofpersonalvideoin-formationprivacyprotectiontechnologycapableofAI learning in an autonomous driving environment) Evlampios Apostolidis,GeorgiosBalaouras,VasileiosMezaris, and Ioannis Patras.Combining global and localattention with positional encoding for video summarization.In 2021 IEEE international symposium on multimedia (ISM),pages 226234. IEEE, 2021. 1, 2, 4, 5, 3 Evlampios Apostolidis,GeorgiosBalaouras,VasileiosMezaris, and Ioannis Patras. Summarizing videos using con-centrated attention and considering the uniqueness and diver-sity of the video frames. In Proceedings of the 2022 Interna-tional Conference on Multimedia Retrieval, pages 407415,2022. 2",
  "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-ton. Layer normalization. arXiv preprint arXiv:1607.06450,2016. 5": "Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, andChunhua Shen. Conditional positional encodings for visiontransformers. In The Eleventh International Conference onLearning Representations, 2022. 3, 5 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical imagedatabase. In 2009 IEEE conference on computer vision andpattern recognition, pages 248255. Ieee, 2009. 1 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale. In International Con-ference on Learning Representations, 2020. 3, 4, 6 Jiri Fajtl, Hajar Sadeghi Sokeh, Vasileios Argyriou, DorothyMonekosso, and Paolo Remagnino.Summarizing videoswith attention. In Computer VisionACCV 2018 Workshops:14th Asian Conference on Computer Vision, Perth, Australia,December 26, 2018, Revised Selected Papers 14, pages 3954. Springer, 2019. 1, 2, 3, 5, 7, 8",
  "Hao Fu, Hongxing Wang, and Jianyu Yang. Video summa-rization with a dual attention capsule network. In 2020 25thInternational Conference on Pattern Recognition (ICPR),pages 446451. IEEE, 2021. 7": "Junaid Ahmed Ghauri, Sherzod Hakimov, and Ralph Ew-erth. Supervised video summarization via multiple featuresets with parallel attention. In 2021 IEEE International Con-ference on Multimedia and Expo (ICME), pages 16s. IEEE,2021. 3, 7, 8, 1 Xavier Glorot and Yoshua Bengio. Understanding the diffi-culty of training deep feedforward neural networks. In Pro-ceedings of the thirteenth international conference on artifi-cial intelligence and statistics, pages 249256. JMLR Work-shop and Conference Proceedings, 2010. 1 Jianyuan Guo, Kai Han, Han Wu, Yehui Tang, XinghaoChen, Yunhe Wang, and Chang Xu.Cmt: Convolutionalneural networks meet vision transformers. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1217512185, 2022. 3",
  "In Computer VisionECCV 2014: 13th European Confer-ence, Zurich, Switzerland, September 6-12, 2014, Proceed-ings, Part VII 13, pages 505520. Springer, 2014. 2, 6": "Bo He, Jun Wang, Jielin Qiu, Trung Bui, Abhinav Shrivas-tava, and Zhaowen Wang.Align and attend: Multimodalsummarization with dual contrastive losses. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 1486714878, 2023. 5, 7, 3 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770778, 2016. 3, 5, 6, 8",
  "Md Amirul Islam, Sen Jia, and Neil DB Bruce. How muchposition information do convolutional neural networks en-code? In International Conference on Learning Representa-tions, 2019. 2, 3, 5": "Zhong Ji, Kailin Xiong, Yanwei Pang, and Xuelong Li.Video summarization with attention-based encoderdecodernetworks. IEEE Transactions on Circuits and Systems forVideo Technology, 30(6):17091717, 2019. 1, 2 Zhong Ji, Yuxiao Zhao, Yanwei Pang, Xi Li, and JungongHan. Deep attentive video summarization with distributionconsistency learning. IEEE transactions on neural networksand learning systems, 32(4):17651775, 2020. 1 Hao Jiang and Yadong Mu. Joint video summarization andmoment localization by cross-task sample transfer. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1638816398, 2022. 7 Yunjae Jung, Donghyeon Cho, Sanghyun Woo, and In SoKweon.Global-and-local relative position embedding forunsupervised video summarization. In European Conferenceon Computer Vision, pages 167183. Springer, 2020. 2 Osman Semih Kayhan and Jan C van Gemert. On translationinvariance in cnns: Convolutional layers can exploit abso-lute spatial location. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1427414285, 2020. 2, 3, 5",
  "Maurice G Kendall. The treatment of ties in ranking prob-lems. Biometrika, 33(3):239251, 1945. 6": "Diederik P. Kingma and Jimmy Ba. Adam: A method forstochastic optimization. In 3rd International Conference onLearning Representations, ICLR 2015, San Diego, CA, USA,May 7-9, 2015, Conference Track Proceedings, 2015. 1 Haopeng Li, Qiuhong Ke, Mingming Gong, and Rui Zhang.Video joint modelling based on hierarchical transformer forco-summarization. IEEE Transactions on Pattern Analysisand Machine Intelligence, 45(3):39043917, 2022. 2, 7, 8, 1 Haopeng Li, Qiuhong Ke, Mingming Gong, and Tom Drum-mond. Progressive video summarization via multimodal self-supervised learning. In Proceedings of the IEEE/CVF Win-ter Conference on Applications of Computer Vision, pages55845593, 2023. 3, 7, 8, 1",
  "Medhini Narasimhan, Anna Rohrbach, and Trevor Darrell.Clip-it! language-guided video summarization. Advancesin Neural Information Processing Systems, 34:1398814000,2021. 2, 7": "Mayu Otani, Yuta Nakashima, Esa Rahtu, and JanneHeikkila. Rethinking the evaluation of video summaries. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 75967604, 2019. 6 Danila Potapov, Matthijs Douze, Zaid Harchaoui, andCordelia Schmid. Category-specific video summarization.In Computer VisionECCV 2014: 13th European Confer-ence, Zurich, Switzerland, September 6-12, 2014, Proceed-ings, Part VI 13, pages 540555. Springer, 2014. 6",
  "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.Faster r-cnn: Towards real-time object detection with regionproposal networks. Advances in neural information process-ing systems, 28, 2015. 3, 8": "Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-moginov, and Liang-Chieh Chen.Mobilenetv2: Invertedresiduals and linear bottlenecks.In Proceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 45104520, 2018. 3, 6 Yale Song, Jordi Vallmitjana, Amanda Stent, and AlejandroJaimes. Tvsum: Summarizing web videos using titles. InProceedings of the IEEE conference on computer vision andpattern recognition, pages 51795187, 2015. 2, 6 Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,Scott Reed, Dragomir Anguelov, Dumitru Erhan, VincentVanhoucke, and Andrew Rabinovich.Going deeper withconvolutions.In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 19, 2015.3, 4, 5, 6, 7, 1",
  "Mingxing Tan and Quoc Le. Efficientnet: Rethinking modelscaling for convolutional neural networks. In Internationalconference on machine learning, pages 61056114. PMLR,2019. 3, 6": "Hacene Terbouche, Maryan Morel, Mariano Rodriguez, andAlice Othmani. Multi-annotation attention model for videosummarization. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 31423151, 2023. 6, 7, 1 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in neuralinformation processing systems, 30, 2017. 1, 3, 4, 5, 8",
  "mixture attention meta learning for video summarization. InProceedings of the 28th ACM International Conference onMultimedia, pages 40234031, 2020. 1, 3, 7, 8": "Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,Xiyang Dai, Lu Yuan, and Lei Zhang.Cvt:Introduc-ing convolutions to vision transformers. In Proceedings ofthe IEEE/CVF international conference on computer vision,pages 2231, 2021. 3 Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Feng-wei Yu, and Wei Wu. Incorporating convolution designs intovisual transformers.In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision, pages 579588,2021. 3 Ke Zhang, Wei-Lun Chao, Fei Sha, and Kristen Grauman.Video summarization with long short-term memory. In Com-puter VisionECCV 2016: 14th European Conference, Am-sterdam, The Netherlands, October 1114, 2016, Proceed-ings, Part VII 14, pages 766782. Springer, 2016. 3, 6, 7,1 Yunzuo Zhang, Yameng Liu, Weili Kang, and Ran Tao. Vss-net: Visual semantic self-mining network for video summa-rization.IEEE Transactions on Circuits and Systems forVideo Technology, 2023. 1, 3, 7, 8",
  "A.1. Measure correlation": "Based on , we test everything 10 times in each experiment for strict evaluation since video summarization models aresensitive to randomness due to the lack of datasets. Additionally, we follow to perform the experiments rigorously byusing non-overlapping five-fold cross-validation for reflection of all videos as test data. For each fold, we use 80% of thevideos in the dataset for training and 20% for testing. We then average the results of all folds to export the final score. Owingto non-overlapping videos in the training data in each split, different training epochs are required; therefore, we pick themodel that shows the best performance on test data during the training epochs of each split. During training, the predictedscore for each input video is compared to the average score of all ground truth scores of summary videos for that input video.During inference, the performance for each video is calculated by comparing each ground truth score with the predicted scoreand then averaging them.",
  "A.2. Implementation details": "For a fair comparison, we follow the standard procedure by uniformly subsampling the videos to 2fps and acquiring the image representation of every frame from GoogleNet . GoogleNet is also used as a trainable CNNto match the dimension of all features to 1,024, and all CNN models are pre-trained on ImageNet . The initial weights ofthe linear layers in the classifier are initialized by Xavier initialization , while key and value embeddings are initializedrandomly. The output channels of linear layers and key and value embedding dimensions are 1,024. The reduction ratio rin CNN is 32, an inherent trait of GoogleNet, and all adaptive pooling layers are adaptive average pooling operations. Theshape of the CLS token is 31,024, the epsilon value for layer normalization is 1e-6, and the dropout rate is 0.6. We trainCSTA on a single NVIDIA GeForce RTX 4090 for 100 epochs with a batch size of 1 and use an Adam optimizer with1e-3 as the learning rate and 1e-7 as weight decay.",
  "(a) The images from the summary video titled paluma jump about people diving into the water": "As shown in , we visualize and compare the generated summary videos from different models. We comparedCSTA with DSNet-AB, DSNet-AF , VASNet , and VJMHT . The videos were selected from SumMe (a)and TVSum (b and c). Since each model used different videos for the test, we chose videos used for trainingby all models. (b) The images from the summary video titled ICC World Twenty20 Bangladesh 2014 Flash Mob - Pabna University of Science & Technology ( PUST )about people performing flash mobs on the street and crowds watching them.",
  "(c) The images from the summary video titled Chinese New Year Parade 2012 New York City Chinatown about the parade celebrating the Chinese NewYear on the streets of New York City": ". Visualization and comparison of summary videos generated by different models. The images above are the frames selected byCSTA as parts of the summary video. The graphs below show which frames models pick as keyframes. From the graphs, each row is theresult of each model. The x-axis is the order of the frames, and the black boxes are the ground truth frames. The color parts are the frameseach model selects, and the white parts are the frames unselected by each model. The summary video in a was taken during the paluma jump, representing people diving into the water at Paluma.The first three frames show the exact moment people dive into the water. Based on the selected frames in the graphs, CSTAselects keyframes that represent the main content of the video more accurately than the other models. Although other modelschose key moments in the later part of the video, they did not search for diving moments as precisely as CSTA.The summary video in b was taken during the ICC World Twenty20 Bangladesh 2014 Flash Mob - PabnaUniversity of Science & Technology ( PUST ), representing flash mops on the street. The frames selected by CSTA displaydifferent flash mop performances on the street and the people watching them. CSTA selects keyframes in videos more oftenthan the other models, which either select non-keyframes or skip keyframes, as shown in the graphs in b.The summary video in c was taken during the Chinese New Year Parade 2012 New York City Chinatown,representing the parade celebrating the Chinese New Year in New York City. Based on the chosen images, CSTA findsrepresentative frames containing the parade or people reacting to it (e.g., images showing tiger-like masks, people marchingon the street, or people recording the parade, respectively). Unlike the other models, the graphs exhibit CSTA creating",
  ". The results of CSTA with different CNN models as the baseline": "We tested CSTA using different CNN models as the baseline, as shown in . We unified the dimension size to 1,024because each CNN model exports different dimensions of features. All CNN models improved their performance with theCSTA architecture. This supports the notion that CSTA does not work only for GoogleNet.",
  "D. Architecture history": "Here, we provide a step-by-step explanation of how CSTA is constructed. For all experiments, and represent Kendallsand Spearmans coefficients, respectively. The score marked in bold indicates that the model was selected because it yieldedthe best performance from the experiment. Each experiment was tested 10 times for strict verification, and the average scorewas recorded as the final one.As explained in .1, the form of the ground truth of SumMe is summary videos, so the models aim to generatesummary videos correctly. Kendalls and Spearmans coefficients between the predicted and ground truth summary videosare the basis for evaluating the performance of SumMe. Based on the code provided in previous studies , we generatesummary videos by assigning 1 for selected frames and 0 otherwise. In videos, most frames are not keyframes, so theperformance of SumMe is usually higher than that of TVSum. The form of the ground truth of TVSum is the shot-levelimportance score, so models should aim to predict accurate shot-level importance scores. The scores of entire frames aredetermined by assigning the identical scores of subsampled frames to nearby frames based on the code provided in previousstudies . Therefore, Kendalls and Spearmans coefficients of subsampled frames are the basis for evaluatingthe performance of TVSum. The difference between SumMe and TVSum can cause bad performance on TVSum, and theperformance on SumMe looks much better than that on TVSum.",
  ". Comparison of different number of input channels on GoogleNet": "In , we test the number of channels of input frame features. As explained in .2, we copy the input framefeature two times to create three channels of input to match the number of common channels of images that are usually usedto train CNN models. We use GoogleNet as the baseline and check the results when the channels of input frame features are1 and 3. The model taking 3 channels of features as inputs performs better than taking a single channel of features. Thissupports the idea that creating the shape of input frame features the same as the RGB images helps to utilize CNN modelsbetter.",
  "Baseline(GoogleNet)0.1760.1970.1290.163Baseline+Att0.2140.2390.1670.219Baseline+Att + SoftT0.1840.2050.1760.231Baseline+Att + SoftD0.1860.2070.1700.224Baseline+Att+SoftT&D0.1890.2110.1820.240": ". The ablation study for the softmax. The baseline is the plain GoogleNet summarizing videos, which is the same as in . Attis an attention-based CNN structure without softmax. Soft applies the softmax operation to the model along the frame axis (T), dimensionaxis (D), or both axes (T&D). and 0.042 for Kendalls and Spearmans coefficients, respectively, on SumMe, where it increased by 0.038 and 0.056 forKendalls and Spearmans coefficients, respectively, on TVSum. This demonstrates CNNs ability as an attention algorithm.Reflecting weighted values between frames (Att + SoftT) or dimensions (Att + SoftD) improved the baseline model byat least 0.008 on SumMe and 0.041 on TVSum. This supports the importance of spatial attention, and it is better to considerweighted values along both the frame and dimension axes (Att + SoftT&D) than focusing on only one of the axes.On SumMe, the model without softmax is better than the model with softmax; however, the reverse is the case on TVSum.Since there is no best model for all datasets, we choose both models as the baseline and find the best one when extending thestructure.",
  "Baseline(AttT&D)0.1890.2110.1820.240Baseline+BalanceT0.1860.2070.1780.233Baseline+BalanceD0.1860.2070.1820.239Baseline+BalanceBD0.1860.2070.1750.230Baseline+BalanceBU0.1870.2080.1830.240": ". The ablation study for balancing ratio. The baseline applies the softmax operation to the attention map along the frame anddimension axes (). Balance is the balancing ratio between frames and dimensions. T adjusts the weighted values along the frameaxis to the dimension axis. D adjusts the scale of the weighted values along the dimension axis to the frame axis. BD decreases the scaleof larger ones into smaller ones. BU upscales the scale of smaller ones into larger ones. Balance Ratio.We hypothesize that the imbalance ratio between frames and dimensions deteriorates the performance ofthe model with softmax. For example, suppose the number of frames is 100, and the dimension size is 1,000. In thiscase, the weighted values between frames are usually larger than those between dimensions (on average, 0.01 and 0.001between frames and dimensions, respectively). This situation can lead to overfitting the frame importance, so we testedthe performance of the model under a balanced ratio between the number of frames and dimensions, as shown in .However, all results were worse than the baseline, so we used the default setting.",
  "D.3. Self-attention extension": "Given that our model operates the attention structure differently from existing ones, we must test which existing methodworks for CSTA. First, we verify the key, value embeddings, and scaling factors used in self-attention . The key and valueembeddings project input data into another space by exploiting linear layers. At the same time, the scaling factor dividesall values of attention maps with the size of the dimension. Unlike self-attention handling 1-dimensional data, we shouldconsider the frame and dimension axes for the scaling factor because of 2-dimensional data. We test the scaling factor usingthe size of dimensions (ScaleD), frames (ScaleT), and both (ScaleT&D).The best performance for the model without softmax is achieved by utilizing the key, value embedding, and scalingfactors with the size of frames (EMB + ScaleT), as shown in a. Although utilizing the scaling factors with the sizeof dimensions (ScaleD) yields better performance than EMB + ScaleT on SumMe, it yields much worse performance on",
  "(b) The result of the model with softmax as the baseline": ". The ablation study for methods in self-attention. Att is the model without softmax, and AttT&D is the model with softmax alongthe frame and dimension axes (). EMB employs key and value embedding into the baseline model. Scale divides the values ofattention maps by the number of frames (T) or dimensions (D) or both of them (T&D). TVSum, even considering the performance gaps on SumMe. Also, EMB + ScaleD and EMB + ScaleT&D show slightlybetter performance than EMB + ScaleT on TVSum, but much worse on SumMe. We select EMB + ScaleT as the bestmodel based on overall performance.For the model with softmax, we select the model using key and value embedding (Baseline+EMB) because it reveals thebest performance for all datasets, as shown in b.",
  "D.4. Transformer extension": "We verify the methods used in transformers , which are positional encodings and dropouts. Positional encodingstrengthens position awareness, whereas dropout enhances generalization. We expect the same effects when we apply the po-sitional encodings and dropouts to the input frame features. We use fixed positional encoding (FPE) , relative positionalencoding (RPE) , learnable positional encoding (LPE) , and conditional positional encoding (CPE) . We musttest both 2-dimensional (TD) and 1-dimensional (T) positional encoding matrices to represent temporal position explicitlybecause the data structure differs from the original positional encoding that handles only 1-dimensional data. For CPE, Toperates a depth-wise 1D CNN operation for each channel, whereas TD operates entire channels. We use 0.1 as the dropoutratio, the same as .The results of both models with and without softmax reveal that employing positional encodings or dropout into the inputframe features deteriorates the performance of Kendalls and Spearmans coefficients for all datasets, as shown in . Wesuppose that adding different values to each frame feature leads to distortion of data, making it difficult for the model to learnpatterns from frame features because CSTA considers the frame features as images. If more data are available, the modelcan learn location information from these positional encodings because they are similar to bias. Thus, all results yielded bythe model with and without softmax worsen when using positional encoding or dropout on SumMe. However, some resultsare similar to or even better than the baseline on TVSum because TVSum has more data than SumMe. Due to the lack ofdata, we chose the baseline models based on performance.",
  "D.5. PGL-SUM extension": "Unlike existing transformers, PGL-SUM proves effects when applying positional encodings and dropouts to the mul-tiplication outputs between key and query vectors. We adopted the same methods to further improve the models positionalrecognition effectiveness by adding the positional encodings and applying dropouts to CNNs outputs. We use 0.5 for thedropout ratio, the same as .ThebestperformanceforthemodelwithoutandwithsoftmaxisachievedbyBaseline+DropandBaseline+FPE(TD) + Drop, respectively, as shown in .In b, some models perform slightly betterthan Baseline+FPE(TD) + Drop on TVSum, but their performance is considerably worse than the selected one onSumMe. Comparing the best performance in both tables, we observe that the performance of the model with softmax",
  "(b) The results of the model with softmax as the baseline. AttT&D applies key and value embedding with softmax (b)": ". The ablation study for methods in PGL-SUM. FPE is fixed positional encoding, RPE is relative positional encoding, LPE islearnable positional encoding, and CPE is conditional positional encoding. T is the frame axis, and TD is the frame and dimension axesfor positional encoding. Drop exploits dropout to features after positional encoding.",
  "D.6. CLS token": "We further test the CLS token at different combining places. CSTA fuses the CLS token with input frame featuresright after employing CNN or softmax or creating final features. The final features are created by applying attention maps toinput features.Combining the CLS token after creating the final features yields the best performance, as shown in . We hypoth-esize that the reason is that the classifier is generalized. The CLS token is trained jointly with the model to reflect the overallinformation of the dataset. The global cues of the dataset generalize the classifier because fully connected layers are found inthe classifier. Thus, all results using the CLS token improved the baseline. Moreover, adding the CLS token after creating thefinal features means incorporating the CLS token just before the classifier. For this reason, the best performance is achievedby delivering the CLS token without changes and generalizing the classifier the most.",
  "D.7. Skip connection": "We finally verify the skip connection for stable optimization of CSTA, as shown in . Without layer normaliza-tion, adopting the skip connection by adding outputs from the key embedding and CNN (SCKC) yields the best performanceamong all settings. However, it yields slightly worse performance than the baseline model for all datasets. Using layer nor-malization with SCKC (SCKC + LN ) shows slightly less performance than the baseline model on TVSum, whereas it yieldsmuch better performance than the baseline on SumMe. For better overall performance, we selected the combination of skip",
  "Baseline(AttT&D)0.2360.2630.1940.254": "Baseline+SCKC0.2330.2610.1930.253Baseline+SCKC+LN0.2430.2710.1920.252Baseline+SCCF0.1150.1280.0430.056Baseline+SCCF + LN0.1620.1810.0520.068Baseline+SCIF0.1260.141-0.018-0.024Baseline+SCIF + LN0.1630.1810.1860.244 . The ablation study for the skip connection. AttT&D is the baseline model incorporating the CLS token after creating the finalfeatures (). SC means skip connection. KC is the key embedding output fused with CNN output. CF is CNN output combinedwith final features. IF is the combined input and final features. LN is layer normalization exploited immediately after the skip connection.",
  ". The ablation study for different hyperparameter settings": "Here, we test the model with different hyperparameter values. The best performance of the model is achieved with a singlebatch size, and it keeps decreasing with larger batch sizes, as shown in a. Thus, we use the single batch size.The bigger the dropout ratio, the better the models performance on TVSum, as shown in b. However, theperformance of the model is bad if the dropout ratio is too large, so we chose 0.6 as the dropout ratio, considering bothperformance on SumMe and TVSum.We fixed the dropout ratio at 0.6 and tested with various values of weight decay, as shown in c. The performanceincreased as the value of weight decay decreased. When weight decay is 1e-7, the performance on SumMe is similar to thatwithout weight decay. However, it shows slightly better performance on TVSum than the model with 0 weight decay. Thus,considering the overall performance on SumMe and TVSum, we select 1e-7 as the final value for weight decay. In d, we finally test different learning rates with 1e-7 as weight decay. When the learning rate is too large, theperformance is terrible for all datasets. When the learning rate is too small, the performance is also poor. When the learningrate is 1e-3, it shows the best performance, so we decided on this value as the final learning rate."
}