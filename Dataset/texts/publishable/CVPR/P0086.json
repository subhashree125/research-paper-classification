{
  "Abstract": "Automatic anomaly detection based on visual cues holdspractical significance in various domains, such as manu-facturing and product quality assessment. This paper intro-duces a new conditional anomaly detection problem, whichinvolves identifying anomalies in a query image by com-paring it to a reference shape. To address this challenge,we have created a large dataset, BrokenChairs-180K, con-sisting of around 180K images, with diverse anomalies,geometries, and textures paired with 8,143 reference 3Dshapes.To tackle this task, we have proposed a noveltransformer-based approach that explicitly learns the corre-spondence between the query image and reference 3D shapevia feature alignment and leverages a customized attentionmechanism for anomaly detection. Our approach has beenrigorously evaluated through comprehensive experiments,serving as a benchmark for future research in this domain.",
  ". Introduction": "Anomaly detection (AD) , identifying instances thatare irregular or significantly deviate from the normality, isan actively studied problem in several fields. In standardvision AD benchmarks, irregularities are typically causedby either high-level (or semantic) variations such as pres-ence of objects from unseen categories , defectssuch as scratches, dents on objects , low-level variationsin color, shape, size , or pixel-level noise . Thestandard approach has been to learn representations alongwith classifiers that are robust to the variations within theregular set of instances, and, at the same time, sensitive tothe ones causing irregularities. However, this paradigm per-forms poorly when the irregularities are arbitrary and condi-tional to the context and/or individual characteristics of theinstance which may not be known in prior or observed. Forinstance, in an object category such as chair that containsvisually very diverse instances with huge intra-class varia-tion, having three legs may imply a missing leg and hencean anomaly for a chair instance, while regularity for another",
  "Query Image": ". We propose a new conditional AD task that aims toidentify and localize anomalies in a query image by comparing itto a reference shape. The anomalous region is shown in a yel-low bounding box. For instance, the right leg of the blue sofa isrectangular unlike the cylindrical one in its reference shape. instance. The AD here depends on whether the chair in-stance was originally designed to have three legs.Motivated by the intuition above, this paper introducesa novel conditional AD task, along with a new benchmarkand an effective solution, that aims to identify and localizeanomalies from a photo of an object instance (i.e., the queryimage), in relation to a reference 3D model (see ). The3D model provides the reference shape for the regular ob-ject instance, and hence a clear definition of regularity forthe query image. This setting is motivated by real-world ap-plications in inspection and quality control, where an objectinstance is manufactured based on a reference 3D model,which can then be used to identify anomalies (e.g., produc-tion faults, damages) from a photo of the instance.The proposed task goes beyond the single image analy-sis in standard AD benchmarks and requires the detectionof subtle anomalies in shape by comparing two modalities,an image with its reference 3D model, which is challengingfor three reasons. First, we would like our model to de-tect anomalies in previously unseen object instances fromimage-shape pairs at test time. Generalizing to unseen in-stances demands learning rich representations encoding adiverse set of 3D shapes and appearances while enablingaccurate localization of anomalies. Second, the reference3D model contains only shape but not texture informationto simulate a realistic scenario where the 3D model can beused to produce instances with different materials, colors,and textures. The resulting domain gap between two modal-ities requires learning representations that are invariant tosuch appearance changes and sensitive to variations in ge-",
  "arXiv:2406.19393v1 [cs.CV] 27 Jun 2024": "ometry. Finally, in our benchmark, the viewpoint of theobject instances in query images is not available in train-ing. This requires the model to establish the local corre-spondences between the modalities, i.e., corresponding 3Dlocation for each image patch in an unsupervised manner.To tackle the first challenge, we propose a new large-scale dataset, BrokenChairs-180K, consisting of around180K query images with diverse anomalies, geometries,and textures paired with 8,143 reference 3D shapes. Train-ing on such a diverse dataset enables learning rich multi-modal representations to generalize to unseen objects. Toaddress the domain gap between the query images and ref-erence shapes, we follow two strategies. First, we rendereach reference shape from multiple viewpoints to generatea set of multi-view images to represent the 3D shape anduse them as input along with a query image to our model.The multi-view representation facilitates learning domain-invariant representations through sharing the same encoderacross query and multi-view images. Second, our model,Correspondence Matching Transformer (CMT) learns tocapture cross-modality relationships by applying a novelcross-attention mechanism through a sparse set of local cor-respondences. Finally, to address the third challenge, weuse an auxiliary task that forces the model to learn view-point invariant representations for each local patch in queryand multi-view images enabling our method to align localfeatures corresponding to the same 3D location regardlessof its viewpoint without ground truth correspondences.In summary, our main contributions are threefold, intro-ducing a novel AD task, a large-scale benchmark to providea testbed for future research, and a customized solution. Ourmodel includes multiple technical innovations including ahybrid 2D-3D representation for 3D shapes, a transformer-based architecture that jointly learns to densely align queryand multi-view images from image-level supervision anddetect anomalies. Our results in extensive ablation studiesclearly demonstrate that 3D information along with corre-spondence matching yields significant improvements. Wealso perform an additional perceptual study that evaluatesthe human performance on the task, showing that the pro-posed task is challenging. Finally, we evaluate our tech-nique on real images showing promising results.",
  ". Related Work": "AD methods. We refer to for detailed literature re-views. Unlike the standard AD techniques, we focus on aconditional and multi-modal AD problem which requires ajoint analysis of a query image with a reference 3D shape todetect local irregularities in the image.Conditional/referential AD. In many AD applications, theanomaly of an instance depends on its specific context .For instance, anomalous temperature changes can be moreaccurately detected in a particular spatial and temporal con- text. We also study a specific application of the conditionalAD problem where the context information is instance-specific and comes from a reference 3D shape.AD image benchmarks. A major problem in the devel-opment of AD is the lack of large datasets with realisticanomalies.For semantic anomalies, a common practice(e.g., ) is to select an arbitrary subset of classes froman existing classification dataset (e.g., MNIST , CI-FAR10 ), treat them as an anomalous class, and train amodel only on the remaining classes. There also exist multi-ple datasets that contain real-world anomalous instances in-cluding irregularly shaped objects , objects with variousdefects such as scratches, dents, contaminations , variousdefects in nanofibrous material which focus on one sam-ple at a time. A concurrent work, PAD targets a similarobjective with ours, while our task has fewer assumptionsand is designed to detect fine-grained geometrical anoma-lies. Moreover, compared to the PAD dataset consistingof only 20 LEGO bricks of animal toys, ours comprises alarge-scale collection of realistic chairs with diverse geome-tries, textures, and a wider range of fine-grained anomalies.2D-3D cross-modal correlation. Image-based 3D shaperetrieval is a related problem that aims to re-trieve the most similar shape for a given 2D image. Mostexisting works learn to embed 2D images and 3D shapesinto a common feature space and perform metric learningusing a triplet loss. Different to the retrieval task that pri-marily involves global-level matching, our focus is compre-hending the correlation of fine-grained local details betweenthe shape and the image to detect anomalies within the im-age. Another related area focuses on learning of 2D-3Dcorrespondences by matching 2D and 3Dlocally with a triplet loss , matching images andpoint clouds with a coarse-to-fine approach , improvingmatching robustness using a global-to-local Graph NeuralNetwork . 2D-3D correlation is also studied for spe-cific applications such as object pose estimation ,3D shape estimation and object detection in images byusing a set of 3D models . Unlike the methods discussedhere, our objective is to identify and localize anomalies in agiven 2D query image in relation to a reference 3D model.",
  ". Building BrokenChairs-180K Dataset": "To the best of our knowledge, there is no prior large publicdataset with paired 3D shapes and images. Hence we in-troduce BrokenChairs-180K, a new benchmark for the pro-posed conditional AD task. Our dataset focuses on gen-erating samples from one category, namely chair, whichincludes various subcategories like sofas, office chairs, andstools, while our generation pipeline is general and appli-cable to other categories. We picked this category as chairscontain a very wide range of shapes, appearances, and ma-terial combinations making them appealing for our exper-",
  "Rotational": ". Example anomaly instances from our BrokenChairs-180K dataset. Our dataset consists of around 100K anomaly images.In the top row, some example anomaly instances are shown, along with the ground truth bounding boxes and segmentation masks in thebottom row. The red mask is used to indicate parts with anomalies, and a green contour line highlights their respective regions prior toapplying any anomaly, and the bounding box is shown as blue rectangular boxes. (figure best viewed in zoom)",
  ". Creating Anomaly from 3D Objects": "3D shape collection.To cover a wide variety of fine-grained anomalies across various parts of chairs (e.g., leg,arm, and headrest), we strive to collect 3D shapes that comewith part annotations and thus opt to utilize the PartNet as our starting point. PartNet is a large-scale dataset of 3Dobjects annotated with fine-grained part labels. Its chair cat-egory is among the most populous, providing a rich sourceof 3D shapes for our task. In particular, we use 8,143 3Dchair shapes from PartNet. Given a 3D model of a chairand its part annotation, we automatically create anomaliesby applying geometric deformations as described below.Generation of anomaly shapes. Our dataset covers fiveanomaly scenarios (see ) relevant to real-world ap-plications. (1) Positional anomalies pertain to deviationsfrom the designated position of chair parts. To create a po-sitional anomaly, we randomly select a part from a normal3D model and apply random translation.(2) Rotationalanomalies are created by applying a 3D rotational trans-formation to a randomly selected 3D part. (3) Broken ordamaged parts consist cases that structural components arebroken or damaged. We synthetically generate breaks us-ing Boolean subtraction following where we fracture achair part by subtracting a random spherical or cubical geo-metric primitive from the part mesh. (4) Component swap-ping involves swapping common parts across different chairinstances (e.g., back-connector of one chair is exchangedwith a back-connector from another chair), simulating anincorrect assembly during manufacturing. (5) Missing com-ponents involve randomly choosing one part and removingit from the 3D shape. Next, we discuss the generation ofquery images with photo-realistic texture.",
  "#Shapes5,6465,5516,1136,4275,18228,9198,143#Images20,02320,01720,00820,01020,023100,07677,994#Views": "able realistic rendering, we use photo-realistic relightablematerials from represented as SVBRDF. In total, weutilize 400 publicly available SVBRDF materials, encom-passing various types such as wood, plastic, leather, fabric,and metal. Following PhotoShape , we automaticallyassign a material to each semantic part of a 3D shape, anduse Blenders Smart UV projection algorithm to estimatethe UV maps needed for texturing.Rendering and view selection. We render each shape fromvarious viewpoints sampled from a hemisphere around theobject. The viewpoint is parameterized in spherical coor-dinates where azimuth values are sampled uniformly over[0, 2) with an interval of /10 and elevation values areuniformly sampled in [/9, 2/9]. The radius is fixed at2.5 for all views. For anomaly shapes, we only keep therendering if the anomalous part is visible from the cameraview. We employ a quality control and verification step (seesupplementary) to discard bad-quality samples.Dataset Statistics. Our dataset comprises a total of 8,143reference 3D shapes (normal), along with around 180K im-ages rendered at a resolution of 256 256 pixels. Amongthese images, 100K contains anomalies, while the remain-ing are categorized as normal. Since in our solution, weuse textureless multi-view images to represent the refer-ence 3D shape, we further provide grayscale multi-view im-ages1 rendered from 20 regularly sampled viewpoints foreach reference shape. However, the 3D representation isnot necessary to be multi-view images, alternative repre-sentations like mesh, point cloud, or voxel can be obtained",
  "(, , )": ". Overall architecture of our proposed CMT framework for conditional AD task. Our CMT takes the following inputs: the queryimage q and the rendered multi-view images {vn}Nn=1. We extract query features f q and multi-view features F v using the encoder .Additionally, we use 3D positional encoding (3DPE) to obtain 3D positional features P v for the multi-view images. Next, F v and P v areconcatenated and fed to the correspondence-guided attention (CGA) network, denoted as , along with the query features f q. The CGAnetwork selectively conditions the final prediction on a small subset of the most related patches from multi-view images through a top-ksparse cross-attention (TKCA) mechanism. The view-agnostic local feature alignment (VLFA) serves to align the encoder output featuresto achieve view-agnostic representation through semi-supervised learning. from the reference shape and adopted by future algorithmswhen solving the conditional AD problem.A detailed breakdown of these statistics is provided inTab. 1. We divided the dataset into three distinct sets: 138Kimages for training, 13K for validation, and 26K for test-ing. Each set contains rendered images from a set of mu-tually exclusive 3D shapes. Hence, the evaluation is per-formed on previously unseen 3D shapes. Our dataset alsocontains bounding box and segmentation mask, localizingany anomalous region.",
  ". Overview": "Let q R3HW be an H W dimensional RGB im-age of an object captured from an unknown viewpointand V = {vn}Nn=1 be a set of H W dimensional im-ages that are rendered from the reference shape at N reg-ularly sampled viewpoints on a hemisphere. We assumethe model has access to the camera pose and depth mapof each multi-view image. We wish to learn a classifier : R3HW RN3HW that takes in qand V and predicts the ground-truth binary anomaly labely {0, 1}. Given a labeled training set D including |D|query, multi-view, and label triplets (q, V, y), the classifiercan be optimized by minimizing the loss term:",
  "where bce is the binary cross-entropy loss function": "An ideal classifier must identify subtle shape irregular-ities in q by finding the relevant patches in V for each patchin q and comparing them. One straightforward design torelate patches across query and multi-view images is to usethe cross-attention module . In particular, one can uselocal features extracted from q as query and ones from V askey and value matrices as input to the scaled dot-productattention in to cross-correlate them while predictingthe anomaly label. While this design can implicitly capturesuch cross-correlations between patches from only image-level supervision when trained with the loss in Eq. (1), itfails to perform better than a similar model that is trainedonly on the query images in practice (see Sec. 5). We positthat the failure to utilize V is due to the difficulty in estab-lishing the correct correspondences from noisy correlationsbetween all patches pairs across query and multi-view im-ages only from image-level supervision. To address this challenge, we propose a new model, cor-respondence matching transformer (CMT) that consists ofa CNN encoder, a 3D positional encoding (3DPE) mod-ule, a correspondence-guided attention (CGA) network,and lastly a view-agnostic local feature alignment (VLFA)mechanism (see ). While the 3DPE module encodesthe 3D location of the patches in multi-view images andfacilitates finding local correspondences across views, theCGA network selectively conditions the final predictionon a small subset of the most related patches from multi-view images through a top-k sparse cross-attention (TKCA)mechanism. Finally, VLFA provides a richer supervisionsignal to establish correspondences between similar regions . Our proposed correspondence-guided attention (CGA).The CGA comprises B transformer-based blocks, each consist-ing of a standard self-attention module followed by a top-k sparsecross-attention (TKCA) module.",
  ". Correspondence Matching Transformer": "CMT uses ResNet18 feature pyramid network as thefeature encoder, which is denoted as : R3HW Rdhw where the input is down-scaled 8 times throughthe network (h = H/8 and w = W/8). Once we extractthe features of q and for each v, (q) and (v) respec-tively, we reshape each of them to be d nq dimensionalmatrices, denote them as f q and f v respectively, wherenq = h w. Each column in f q and f v corresponds toa d dimensional local feature. We use f[.j] notation to indi-cate j-th local feature or patch encoding, as each encodingapproximately corresponds to a local patch in the input im-age due to locality in the convolutional encoder. Next, wedescribe key components of the CMT including the 3DPEand CGA modules.3D Positional Encoding (3DPE).While the multi-viewrepresentation allows for a simple and efficient model de-sign through a shared feature encoder for our task, it alsomakes 3D information less accessible and hence hampersrelating local features across different views accurately.To mitigate this problem, we propose complementing themulti-view images with 3D information. For each patch en-coding f v[.j], we first locate the corresponding image patchin v and then compute the 3D position of the correspond-ing patch xj R3 in the world coordinates 3D using theknown camera parameters and depth maps. Then we useFourier encoding to obtain a higher-dimensional vector foreach xj and further process it through an MLP block to ob-tain a d dimensional 3DPE. Formally, we denote the jointmapping by : R3 Rd.Compared to the 2D standard positional encoding usedin transformer models , 3DPE encodes 3D object ge-ometry in the world space. For each f v including nq patchencodings, we compute a corresponding dnq dimensional matrix pv. For the next steps, we gather f v and pv over Nviews, and concatenate each set along their second dimen-sions, resulting in F v Rdnv and P v Rdnv respec-tively where nv = Nnq. Augmenting F v with P v resultsin a novel hybrid 2D-3D representation by incorporating ex-plicit 3D information into the 2D multi-view images.Correspondence-Guided Attention (CGA).The CGAnetwork , as illustrated in , takes in f q, F v, P v and predicts the anomaly label while efficiently computingthe correlations across two modalities. CGA comprises Bconsecutive transformer blocks where each block containsmultiple operations and is indexed by subscript b. In par-ticular, the block b starts with concatenating F v and P v",
  "f q(b+1) O(b)(9)": "Next, we pass Q(b), K(b), V(b) to our top-k sparse cross-attention (TKCA) module (see Eq. (6)). Unlike the vanillacross-attention module in standard transformers in-gesting all tokens for the attention computation, which isinefficient for our task and may introduce noisy interactionswith irrelevant features, potentially degrading performance,TKCA calculates the attention between query and only asmall subset of relevant multi-view features using a similar-ity matrix M:",
  ". Top-k sparse attention-span visualization. For thequery point (yellow), similarity heatmaps (first row) and top-kattention-span (second row) across multiple views are shown": "query feature. To compute M, we use an auxiliary function : Rd Rd, instantiated as a four layered MLP followedby a final channel-wise normalization, that projects f q andeach view in F v to a view-agnostic feature space where im-ages corresponding to same object part in 3D has similarrepresentations regardless of their viewpoint. To obtain thesimilarity between the query and multi-view patches, wecompute the dot product between their projected features:",
  "M = (f q)T(F v) Rnqnv.(12)": "In contrast to other transformer architectures using sparseattention , TCKA chooses the top-k elements basedon a different source of information, geometric correspon-dences computed across two modalities, and enables an ef-ficient computation of the cross-attention, as the same M isused throughout the transformer blocks. After the cross-correlation, the standard residual addition, normalizationand feedforward (FFN) layers are applied to obtain f q asinput to the next block b + 1 (Eqs. (7) to (9)). Note thatwe use multiple heads, concatenate the outputs from multi-head attention and then derive the final attention resultsthrough linear projection. We append a learnable token de-noted as [tok] to construct the query inputs of the CGAnetwork. Through the transformer blocks, the output stateof the [tok] token develops a consolidated representationenriched by learned shape-image correlation, which is usedas input to the classification head.",
  ". View-Agnostic Local Feature Alignment": "As discussed above, image-level supervision alone is tooweak to capture fine localized correlations between q andV. Thus, we introduce an auxiliary task, VLFA that aimsto densely align corresponding parts between query imagesand related views. Through , we learn to map f q and f v to a view-agnostic space such that their local features cor-responding to the same object part are mapped to a similarpoint regardless of the viewpoint from which the image iscaptured. As the viewpoint of q is unknown, the ground-truth correspondences between query and reference viewscannot be obtained through inverse rendering.To this end, we use a self-labeling strategy to generatepseudo-correspondences by finding the most similar local",
  "(f q[.i]) and zvj =(f v[.j])": "(f v[.j]). We computethe pseudo-label for each zqi and store them in a look-up ta-ble P(q, v, i) = ci. In another one N(q, v, i), we store theremaining set of reference view and index values that arenot the corresponding location. Then, using P, N as pos-itive and negative correspondences respectively, we mini-mize a contrastive loss va(q, v) over each q-v pair:",
  "j N (q,v,i)exp(zqiTzvj /),": "(14)where is a temperature parameter and zv+ = zvP(q,v,i).Due to the cost of computing the pseudo-correspondencesfor all query features across all views, we compute themonly for a random subset of query features across randomlysampled views at each training iteration.Learning correspondences through self-learning alone inthe presence of the domain gap between query and referenceviews is a noisy process. Hence, we also exploit the knownviewpoints of the multi-view images by densely aligningtheir local features in each view pair v, v after computingthe ground truth dense correspondences between them anddiscarding the ones that are occluded in one of the views.The key assumption here is that aligning different views byusing their ground truth labels enables a more accurate cor-respondence learning between query images and views, asthe parameters of the projection are shared across two do-mains. As before, we form two look-up tables P(v, v) andN(v, v) to store the positive and negative correspondencesbetween two views, and randomly subsample them. Aftermapping them to the view-invariant space and normalizingthem, we compute and minimize Eq. (14) for the pairs inthe look-up tables.The objective in Eq. (1) can be rewritten as:",
  "Ours: CMT84.775.4": "block. Within the CGA network, we employ three trans-former blocks (B = 3), and each applies 8-headed atten-tion. The value of k in TCKA is set to 100. During training,we randomly select a subset of N = 10 views, and dur-ing testing, we utilize all 20 views. We apply basic dataaugmentation to the query images, which includes randomhorizontal flips and random cropping of 224 224 regions,followed by resizing the cropped regions back to the orig-inal size of 256 256. We train our model for 20 epochsusing 4 Titan RTX GPUs, maintaining a batch size of 8 ineach GPU, and use the Adam optimizer with a learning rateof 2105. We refer to the supplementary for more details.",
  ". Results": "Since there is no related public benchmark for our task, wedefine several challenging baselines to evaluate our CMT.We report the quantitative results using two evaluation met-rics the area under the ROC curve (AUC) and accuracy inTab. 2, and provide qualitative results in .Importance of 3D reference shape. To assess the signif-icance of using the reference shape, we establish baselinesthat solely rely on the query image for detecting anomalies.As our first baseline, we use a ResNet18-FPN model thattakes in only query images as input. For the next two base-lines, we add three self-attention blocks to ResNet18-FPNand use a ViT respectively. Tab. 2 shows that the refer-ence 3D shape is crucial to good performance while CMToutperforms the baselines by more than 10% in accuracy.Comparison with related work. As there is no prior workdesigned for our problem, we take two recent image-based",
  ". Evaluation on real data. The predicted anomalies areshown in the blue bounding boxes": "to improved results (row 3). The optimal result is achievedwhen both components are combined (row 4).Sensitivity to k. We analyze the performance under differ-ent values of k in (left). Compared to the maximumpossible kmax that is N 32 32, we analyze significantlysmaller k values and, among them, show k = 100 yields thebest result. Using all available tokens results in deterioratedperformance (shown as the dotted horizontal line) showingthat our top-k sparse attention is effective in eliminating thenoisy patches by using only the k top-related ones.Sensitivity to N. (right) depicts the analysis on thenumber of input views for training and testing, Ntrain andNtest respectively. To this end, we train two separate CMTmodels with 5 and 10 views, and evaluate each using 5, 10,15, and 20 views at test time. The plot shows that, whileincreasing views in both training and testing helps, trainingwith few views and testing on more views can provide agood tradeoff between training time and performance.Viewpoint prediction. As a side product of establishingthe correspondences across the query image and views inour model, we could estimate the camera viewpoint in thequery image w.r.t. a reference shape. To this end, we com-pute dense correspondences between the query image andeach view image, and then calculate the distance betweenthe pixel coordinates of each point in the query image andtheir predicted correspondences in the multi-view images,and choose the view with the lowest average distance as theapproximate viewpoint. As a baseline, we train a ResNetwith the viewpoint supervision on the normal images only,and evaluate it on the test normal query images. Our model,trained with no viewpoint supervision, achieves a signifi-cantly better accuracy (47% vs 89%) when predicting theclosest view suggesting that our model implicitly learns torelate the query image with the closest views.Evaluation on real data. Here we apply our model, whichis trained on the synthetic BrokenChairs-180K dataset, ona small set of real chair samples that contain multiple pairsof the reference 3D shape, query images containing either normal or irregular instances with broken, removed, or mis-aligned parts from various viewpoints. Background pix-els in query images are removed by using a segmentationmethod in a preprocessing step and also a syntheticshadow was added to match the training images. To obtainthe reference 3D shapes, we take multiple photos of objectinstances while walking around them, use the 3D recon-struction software , and finally apply Laplacian smooth-ing to post-process it. illustrates the results for tworegular reference shapes, each paired with three query im-ages. In 5 out of 6 cases, our method successfully classifiesand localizes the anomalous parts, while in the failure case,it incorrectly relates the self-occluded arm with an anomaly. Anomaly localization. Here we adopt our model to local-ize anomalies in the form of a bounding box, use a bound-ing box regression head (a 4-layer MLP), and jointly trainit with the other network parameters by using L1 regressionand generalized IoU loss . This model achieves 56.5%average precision on our dataset, outperforms a ViT base-line that is trained only on the query images, and obtains42.6%. Moreover, jointly learning the classification and lo-calization further boosts the classification performance to85.9 (+1.2) AUC and 77.3% (+1.9) accuracy. User Perceptual Study. We also evaluated human perfor-mance in our task and conducted a study with 100 partici-pants. We presented each participant with 10 pairs of ref-erence shapes and query images, with each pair randomlyselected from a random subset of 200. We observe a hu-man accuracy of 70.6%, showing that the proposed task ischallenging, while our CMT obtains a superior accuracy of74.8% on the same subset.",
  ". Conclusion": "In this paper, we have introduced a new AD task, a newbenchmark, and a customized solution inspired by qual-ity control and inspection scenarios in manufacturing. Weshowed that an accurate detection of fine-grained anoma-lies in geometry requires a careful study of both modalitiesjointly. Our method achieves this goal by learning densecorrespondence across those modalities from limited super-vision. Our benchmark and method also have limitations.Due to the difficulty and cost of obtaining real damaged ob-jects, our dataset contains only shapes and images of syn-thetic objects and currently is limited to a single yet verydiverse category of chair, presence of only one anomalyin each query image, focusing only on shape anomalies ex-cluding the appearance based ones such as fading, discolor,texture anomaly. Moreover, our model assumes that objectinstances are rigid, and cannot deal with articulations anddeformations, and requires an accurate reference 3D shapefor accurate detection."
}