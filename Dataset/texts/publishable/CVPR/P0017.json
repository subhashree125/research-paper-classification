{
  "Abstract": "Diffusion models have made significant strides in imagegeneration, mastering tasks such as unconditional imagesynthesis, text-image translation, and image-to-image con-versions. However, their capability falls short in the realm ofvideo prediction, mainly because they treat videos as a collec-tion of independent images, relying on external constraintssuch as temporal attention mechanisms to enforce tempo-ral coherence. In our paper, we introduce a novel modelclass, that treats video as a continuous multi-dimensionalprocess rather than a series of discrete frames. We alsoreport a reduction of 75% sampling steps required to samplea new frame thus making our framework more efficient dur-ing the inference time. Through extensive experimentation,we establish state-of-the-art performance in video predic-tion, validated on benchmark datasets including KTH, BAIR,Human3.6M, and UCF101. 1",
  ". Introduction": "In the evolving landscape of machine learning and genera-tive models, particularly in the domain of video representa-tion , there exists a pivotal challengein adequately capturing the dynamic transitions betweenconsecutive frames. In this paper, we introduce a novelapproach to video representation that treats the video as acontinuous process in multi-dimensions. This methodol-ogy is anchored in the observation that transitions betweenconsecutive frames in a video do not uniformly contain thesame amount of motion. Modeling these transitions with asingle-step process often leads to suboptimal quality in sam-pling. Our method, therefore, involves multiple predefinedsteps between two consecutive frames, drawing inspirationfrom recent advancements in diffusion models for imagedata. This multi-step diffusion process has been instrumen-tal in better modeling image data, and we aim to extend thissuccess to video data.",
  "()": ". The figure is divided into two parts. The top portionof the figure illustrates the intermediate frames xt between twoconsecutive frames. x, y represents consecutive frames from avideo sequence where y = xj+1 and x = xj. xj denotes someframe at timestep j in the video sequence V = {xi}Ni=1. z denotesthe white noise. The lower portion of the figure represents thedirected graphical model considered in this work to represent thecontinuous video process. Previous efforts in video modeling with diffusion mod-els have tended to approach videos as a series of images,generating separate volumes of video frame sequences andapplying external constraints such as applying temporal at-tention to maintain the temporal coherence. We argue thatthis approach overlooks the inherent continuity in video data,which can be more naturally conceptualized as a continu-ous multi-dimensional process. Our proposed method defines this continuous process, beginning with two consec-utive frames from a video sequence as endpoints this canbe observed in . We delineate the forward processthrough interpolation between these endpoints, with a prede-fined number of steps guiding the transition from one pointto another. To ensure the existence of p(xt) at all points, weintroduce a novel noise schedule that applies zero noise atboth endpoints.We approximate each step between these endpoints usinga Gaussian distribution, following the assumptions made indiffusion models for images by the paper . Indefining this forward process, we also lay the groundwork",
  ". Related Works": "Understanding and predicting future states based on ob-served past data is a cornerstonechallenge in the domain of machine learning. It is crucial forvideo-based applications where capturing the inherent multi-modality of future states is vital, such as in autonomousvehicles. Early methods in this field, as noted by Yuen etal. and Walker et al., primarily focused on matchingpast frames within datasets to extrapolate future states, al-though these predictions were constrained to either symbolictrajectories or directly retrieved future frames. The adventof deep learning has significantly propelled advancements inthis area. One of the seminal works by Srivastava et al.leveraged a multi-layer LSTM network for deterministicrepresentation learning of video sequences. Subsequent stud-ies , have expanded the scope ofthis research by constructing models that account for thestochastic nature of future states, marking a notable shiftfrom earlier deterministic approaches.Recent research in this domain has explored both im-plicit and explicit probabilistic modeling approaches. Im-plicit probabilistic modeling, typified by GAN -basedmodels, has a substantial history. Nonetheless, these mod-els often grapple with training stability issuesand mode collapse(where model only focuses on a fewmodes in the dataset) issues. On the other hand, explicitprobabilistic modeling for video prediction encompassesa range of methodologies, including Variational Autoen-coders (VAEs) , Gaussian processes, and Diffusion mod-els. VAE-based video prediction methods tendto average results to align with all potential future scenar-ios, which undermines the fidelity of predictions. Gaus-sian process-based models exhibit proficiency withsmaller datasets but encounter scalability issues owing to",
  "matrix inversion limitations when calculating training like-lihood. While workarounds exist, they tend to compromiseresult fidelity.Recent advancements in diffusion models [12, 24, 25,": "51] have positioned them as the preferred choice for videoprediction tasks. These multi-step models offer superiorsample quality and are resilient to mode collapse. However,even with such lucrative advantages, modeling videos withthese models tends to have downsides. Majorly methodsfalling under this category enforce temporal consistencyusing artificial external constraints such as the introductionof temporal attention blocks. This might be effective butcomes at a cost of significant computing power.Another class of popular video prediction models is hier-archical prediction models. These modelsare multistage models that decompose the problems into twostages. They first predict a high-level structure of a video,like a human pose, and then leverage that structure to makepredictions at the pixel level. These models generally requireadditional annotation for the high-level structure for train-ing, unlike ours that predicts future frames utilizing only thepixel-level information of context frames.We also want to highlight some very recent works likeInDI , and Cold diffusion that provide an alternateapproach to denoising diffusion models that is similar toour approach. However, their works only explored suchformulation for image-based computational photography andimage generation tasks.",
  ". Method": "Instead of introducing noise iteratively to the frames un-til they conform to a Gaussian distribution, and adoptinga reverse process such as denoising diffusion, a commonlyemployed technique for video prediction, we introduce anovel model category designed to depict videos as continu-ous processes. This section delves into the modeling of thiscontinuous video process.Suppose we have a video sequence denoted by V ={xt}N1 where xj Rchw is the frame at the timestepj. We represent this video sequence as a continuous process.The intermediate frames between x = xj and y = xj+1 aregiven by the following equation.",
  "2z(1)": "Here, z N(0, I) denotes the white noise. From theabove Eqn, it can be seen that at t = 0, we get the framexjand at t = 1, we get the frame xj+1. We utilize thiscontinuous process of evolving xj xj+1 given by Eqn. 1and derive both the forward and reverse processes. Fordefining the forward process, we take steps in the directiont : T 0 instead of the other way, which happens in",
  "FramesNoise Schedule": ". Fig. (a) demonstrates the methodology for estimating xt in a single step, showcasing the specific computational process involved.Fig. (b) details the training pipeline of our Continuous Video Process (CVP) model, where xt and t are fed as inputs to the U-Net architecture,and the anticipated output is y, with y = x1:k+1 in this scenario. Fig. (c) provides an overview of the sampling pipeline utilized in our CVPmethod, illustrating the sequential steps to predict the next frame of the video sequence given the context frames. denoising diffusion process . The reason for this is wewant the reverse process to start from past frame x andaccording to the Eqn. 1 xt = x at t = 0.We can write the forward process, i.e., going from thestart point y at t = T to endpoint x at t = 0,",
  "xt+t = xt + (y x)t t log(t)z(2)": "From the above equation, we can write the posterior forthe forward process as q(xt+1|xt, x, y) = N(xt+1:(xt, x, y), g2(t)I). Where g(t) = tlogt. The wholederivation is provided in the appendix.Formodelingourvideodiffusionprocess,welike to model the likelihood functionp(xT ):=p(x0:T ) dx0:T 1andminimizethenegativelog-likelihood to obtain the best fit for our model. Here, p(x0:T )is the probability of the reverse process, and it is defined asa Markov chain with learned Gaussian transitions startingat p(x0) = pdata(x). Important note about the notationsx0, xT , unless specified consider x0 = x and xT = ywhere x is the frame in the video sequence at jth positionand y is the frame at (j + 1)th position. One importantassumption about the continuous video process is we assumethe transition between the frames x and y to follow Markovchain, i.e., the current state at timestep t only depends on theprevious state at timestep t 1. Leveraging this assumptionwe can define the reverse process as follows,",
  "q(xt|xt1, x, y) = N(xt : (xt1, x, y), g2(t)I)(9)": "where, (xt, x, y) = xt + (y x) and g(t) = t log(t).Consequently, all KL divergences in Eqn. 8 are compar-isons between Gaussians, so they can be calculated in aRao-Blackwellized fashion with closed-form expressionsinstead of high-variance Monte Carlo estimates. It is impor-tant to note while deriving the Eqn. 8, we ignore some termsthat purely involve the forward process posteriors as q hasno learnable parameters, so such terms are constants duringtraining.Now we discuss our choices in p(xt|xt1, x)=N(xt; (xt1, t1, x), (xt1, t1, x)) for 1 < t T.First, we set (xt1, t 1) = g2(t)I to untrained timedependent constants. Experimentally, the choice of g(t) =t log(t) works the best. This noise function has an inter-esting property that noise is absent both at the start and endpoints, i.e., g(t) = 0t = {0, 1}. Second, to represent the mean (xt, t, x), we proposea specific parameterization motivated by the forward pro-cess posterior given by Eqn. 9. With p(xt|xt1, x) =N(xt; (xt1, t 1, x), g2(t)I), we can write:",
  "(10)": "where C is a constant that does not depend on . So, wesee that the most straightforward parameterization of is amodel that predicts t, the forward process posterior mean.However, we can simplify Eqn. 10 further and obtain avery simple training loss objective by delving in the term .We further parameterize the term as follows,",
  ". Datasets": "We chose 4 different types of datasets to demonstrate theefficacy of our approach. These are standard benchmarksfor video prediction tasks. Dataset lists include KTH actionrecognition dataset , BAIR robot pushing dataset ,Human3.6M and UCF101 datasets. Training andarchitecture-specific details about the approach are includedin the appendix.KTH Action Recognition Dataset.The KTH actiondataset consists of video sequences of 25 people per-forming six different actions: walking, jogging, running,",
  ": end for8: return xT": "boxing, hand-waving, and hand-clapping. The backgroundis uniform, and a single person is performing actions in theforeground. The foreground motion of the person in theframe is fairly regular. The frames in the video for thisdataset consist of a single channel. The spatial resolutionof the frames in the video is downsampled to the size of64 64.BAIR pushing Dataset.The BAIR robot pushingdataset contains the videos of table mounted sawyerrobotic arm pushing various objects around. The BAIRdataset consists of different actions given to the robotic armto perform. The spatial resolution of the frames in the videois kept to be 64 64.Human3.6M Dataset. Human3.6M dataset consists of10 subjects performing 15 different actions. The pose in-formation from the dataset was not used in predicting nextframe. The background is uniform, and a single person isperforming actions in the foreground. The foreground mo-tion of the person in the frame is fairly regular. The framesin the video for this dataset consist of RGB channels. Thespatial resolution of the frames in the video is downsampledto the size of 64 64.UCF101 Dataset.This dataset consists of 13,320videos belonging to 101 different action classes. The videoseems to have a variety of backgrounds and the frames ofthe video have three channels, namely RGB. We reshapethe resolution of frames from the original size of 320 240down to 128 128 for our video prediction tasks. Thedownsampling is done utilizing the bicubic downsampling.",
  "Struct-vRNN 1040395.024.290.766SVG-LP 1040157.923.910.800MCVD 540276.726.400.812SAVP-VAE 1040145.726.000.806Grid-keypoints 1040144.227.110.837RIVER 1040170.529.00.82CVP (Ours)140120.129.20.841": "task. FVD metric evaluates a baseline on both terms, thereconstruction quality and diversity of the generated samples.FVD is calculated as the frechet distance between the I3Dembeddings of generated video samples and real samples.The I3D network used for obtaining the embeddings for realand generated video is trained on the Kinetics-400 dataset.",
  ". Setup and Results": "Below, we describe in detail how the setup for our experi-ment looks compared to baselines. We also showcase ourfindings about the performance of our method and compari-son to baselines in this section.KTH action recognition dataset:For this dataset, we adhered to the baseline setup ,which utilizes the first 10 frames as context frames. In base-line setup, these 10 frames are utilized to predict the subse- quent 30 and 40 frames. A notable aspect of our experimentis we only used the last 4 frames from this sequence of 10frames as context frames in our CVP model, while disregard-ing the information in the remaining 6 frames. This decisionwas taken to maintain consistency with the experimentalsetups used in prior baseline methodologies. The outcomesof this evaluation are summarized in Table. 1.It can be observed from the Table. 1, our models uniqueapproach requires a significantly reduced number of framesfor training. Contrary to other methods that train on anadditional set of k frames (10[context frames]+k[futureframes]), our model uses just one frame (effectively 4[con-text frames]+1[future frames]). We employ the 4 contextframes to predict the immediate next frame and then autore-gressively generate either 30 or 40 frames, depending on theevaluation requirement. This methodology is supported byour models efficient handling of video sequences as con-tinuous processes, which eliminates the need for externalartificial constraints, such as temporal attention mechanisms.The results, as shown in , clearly indicate thatour method delivers state-of-the-art performance when com-pared to other baseline models. Additionally, the qualitativeresults for our CVP model on the KTH dataset can be ob-served in .BAIR Robot Push dataset: The BAIR Robot Push datasetis characterized by highly stochastic video sequences. In ourstudy, we adhered to a baseline setup with three mainexperimental settings: 1) using only one context frame topredict the next 15 frames, 2) employing two context framesto predict 14 future frames, and 3) utilizing two contextframes to forecast the next 28 frames. The outcomes of theseapproaches are summarized in .As observed in , a trend emerges where increasingthe number of frames predicted at a time concurrently resultsin a degradation of prediction quality. This phenomenon ishypothesized to stem from an augmented disparity betweenthe blocks of context frames and predicted future frames.Specifically, consider the scenario where two context framesare designated as x0:2, corresponding to x in the context ofEqn.1. Under the first experimental condition, where themodel predicts a single frame at a time, the future frame pre-diction block is represented as x1:3, analogous to y in Eqn.1.Conversely, in the second condition, where two frames arepredicted simultaneously, the future frame block extends tox2:4, again paralleling y in the equation. This setup impliesthat in the former setting, interpolation occurs between adja-cent frames (i.e., the transition from x0 x1 and x1 x2),while in the latter, interpolation spans a two-frame interval(i.e., the transition from x0 x2 and from x1 x3). Theexpanded interval in the second scenario is posited as thecausative factor for the observed reduction in predictive per-formance, particularly in configurations where k = 2 andp = 2.",
  "Timesteps Progression": ". Figure represents qualitative results of our CVP model onthe BAIR dataset. The number of context frames used in the abovesetting is two for both sequences. Every 6th predicted future frameis shown in the figure. The results, as shown in , clearly indicate that ourmethod delivers state-of-the-art performance compared toother baseline models. Additionally, the qualitative resultsfor our CVP model on the BAIR dataset can be observed in.Human3.6M dataset: Similar to the KTH dataset, the Hu-man3.6M dataset features actors performing distinct actionsagainst a static background. However, the Human3.6Mdataset distinguishes itself by offering a greater variety ofdistinct actions within its videos and providing three-channelvideo frames, in contrast to the single-channel frames of theKTH dataset. For evaluating the Human3.6M dataset, weemployed a similar setup to that used for the KTH dataset,where 5 frames are provided as context, and the model pre-dicts the subsequent 30 frames based on these context frames.The results of this evaluation are summarized in .An analysis of reveals that our model, with itsunique approach, requires a significantly lower number offrames for training, needing only a total of 6 frames perblock to yield results that are considerably better than thoseof the baselines.The results, as presented in , unequivocally demon-strate that our method outperforms other baseline models, es-tablishing a new state-of-the-art on the Human3.6M dataset.Furthermore, the qualitative efficacy of our CVP model onthe Human3.6M dataset is illustrated in , showcasingthe models ability to effectively capture and predict thedatasets varied actions.UCF101 dataset: The UCF101 dataset presents a greaterlevel of complexity compared to the KTH or Human3.6Mdatasets, owing to its substantially higher number of ac-tion categories, diverse backgrounds, and significant cameramovements. Notably, we only use information from the con-text frames for our frame-conditional generation task. No . BAIR dataset evaluation. Video prediction results onBAIR (64 64) conditioning on p past frames and predicting predframes in the future, using models trained to predict k frames at attime.The common way to compute the FVD is to compare 100256generated sequences to 256 randomly sampled test videos. Bestresults are marked in bold.",
  "SVG-LP 5 1030718Struct-VRNN 5 1030523.4DVG 5 1030479.5SRVP 5 1030416.5Grid keypoint 8 830166.1CVP (Ours)5 130144.5": "extra information, like class labels, was used for the predic-tion task. In evaluating the UCF101 dataset, we adopted anapproach similar to that used for the Human3.6M dataset,where 5 context frames are provided, and the model is taskedwith predicting the next 16 frames based on these. Theoutcomes of this evaluation are detailed in Table. 4.An examination of Table. 4 reveals that our CVP modelsurpasses the performance of other baseline models, therebysetting a new benchmark for the UCF101 dataset. Addi-tionally, the qualitative performance of our CVP model onthe UCF101 dataset is depicted in . This illustration",
  ". Ablation Studies": "In this section, we present a series of ablation studies con-ducted to ascertain the impact of various components in ourproposed methodology. These studies focus on three primaryaspects: the modification of the noise schedule denoted asg(t), the variation in the number of sampling steps, and theexploration of different strategies for sampling the timestep t. Our experimental framework utilizes the KTH dataset forthese evaluations.The outcomes of these experiments are systematicallytabulated in Table. 6, offering a comprehensive view of theresults. The key insights derived from these ablation studiesare threefold. Firstly, our analysis underscores the criticalityof sampling the timestep t from a uniform square root distri-bution, specifically t",
  ". This particular noise schedule ischaracterized by a zero initial and final noise level, with apeak near t = 0. Such a configuration is advantageous forour application": "Thirdly, our results, as detailed in , indicate that anincrease in the number of sampling steps beyond 25 does notsubstantially improve the outcome. Our method outperformsMCVD by producing higher-quality frames in just 25 sam-pling steps, a 75% reduction compared to its 100 steps. Thisefficiency is attributed to our CVP method, which retainsinformation from preceding frames, eliminating the need",
  "U139.4": "A primary limitation of our approach is its reliance on alimited context frame window for predicting the next frame.Specifically, when a context vector, denoted as x0:4, com-prising 4 video frames is used, the prediction of the subse-quent frame is entirely dependent on this four-frame window.This model architecture performs adequately in scenariosinvolving uniform video sequences. However, its efficacydiminishes in a setting that requires more context to pre-dict the future frame. Addressing this limitation requiresa more adaptive approach that can handle varying contex-tual information, a challenge we have earmarked for futureresearch. Another constraint lies in the computational efficiency ofour model. Currently, it necessitates multiple steps to samplea single frame, which could become a significant bottleneck,especially when a larger number of frame predictions arerequired. Although our method is more efficient in terms ofthe number of steps needed for frame sampling compared todiffusion-based counterparts, further optimization is neces-sary to reduce the computational overhead associated withthis process. Additionally, our experimental setup was constrained bythe computational resources available to us. The model wasdeveloped and tested using just two A6000 GPUs. Thislimitation raises questions about the potential improvementsthat could be achieved with a more powerful computationalsetup. A larger model with an increased number of parame-ters, trained on more advanced hardware, could potentiallyunveil further advancements in video prediction capabilities.We recognize this as an important area for investigation andencourage labs with more substantial resources to explorethis avenue. In summary, while our model represents a significantstep forward in video prediction, these limitations highlightcrucial areas for future research and development, paving theway for more robust and versatile video prediction models.",
  ". Broader Impact": "We used this method for video prediction; however, suchmodeling can make a major impact on many computationalphotography tasks. Here, one end of the CVP can be a cor-rupted image and the other end be a clean ground truth image.Additionally, a larger model with an increased number ofparameters, trained on more advanced hardware, could poten-tially have advanced video prediction capabilities. This canlead to a significant increase in the creation of high-qualityartificially generated content, further compounding the prob-lems of fake content. However, a positive contribution ofthis approach can help with its application in autonomousdriving.",
  ". Conclusion": "In this work, we have presented a novel model class designedspecifically for video representation, marking a significantadvancement in the field of video prediction tasks. Our com-prehensive experimental evaluations across various datasets,including KTH, BAIR, Human3.6M, and UCF101, have notonly validated the effectiveness of our model but also estab-lished new benchmarks in state-of-the-art performance forvideo prediction tasks.A notable aspect of our approach is its efficiency in termsof the required number of context and future frames fortraining. Moreover, our models continuous video processcapability uniquely operates without the need for additionalconstraints such as temporal attention, which are typicallyemployed to ensure temporal consistency. This aspect of ourmodel underscores its inherent ability to maintain temporalcoherence, further simplifying the video prediction processwhile enhancing its effectiveness.In conclusion, the innovations introduced in our modeloffer promising directions for future research in video repre-sentation and prediction. The achievements demonstrated inthis paper not only contribute to the advancement of videoprediction methodologies but also open avenues for explor-ing more efficient and effective ways of video representationin various real-world applications.",
  "Jacques Fize, Gaurav Shrivastava, and Pierre Andr Mnard.Geodict: an integrated gazetteer. In Proceedings of Language,Ontology, Terminology and Knowledge Structures Workshop(LOTKS 2017), 2017. 2": "Jean-Yves Franceschi, Edouard Delasalles, Mickal Chen,Sylvain Lamprier, and Patrick Gallinari. Stochastic latentresidual video prediction. In International Conference onMachine Learning, pages 32333246. PMLR, 2020. 5, 6 Xiaojie Gao, Yueming Jin, Qi Dou, Chi-Wing Fu, and Pheng-Ann Heng. Accurate grid keypoint learning for efficient videoprediction. In 2021 IEEE/RSJ International Conference onIntelligent Robots and Systems (IROS), pages 59085915.IEEE, 2021. 5, 6",
  "Alex X. Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel,Chelsea Finn, and Sergey Levine. Stochastic adversarial videoprediction. arXiv preprint arXiv:1804.01523, 2018. 2, 5, 6": "Jian Liang, Chenfei Wu, Xiaowei Hu, Zhe Gan, JianfengWang, Lijuan Wang, Zicheng Liu, Yuejian Fang, and NanDuan. Nuwa-infinity: Autoregressive over autoregressivegeneration for infinite visual synthesis. Advances in NeuralInformation Processing Systems, 35:1542015432, 2022. 6 Pauline Luc, Aidan Clark, Sander Dieleman, Diego de LasCasas, Yotam Doron, Albin Cassirer, and Karen Simonyan.Transformation-based adversarial video prediction on large-scale data. arXiv preprint arXiv:2003.04035, 2020. 2, 6 Matthias Minderer, Chen Sun, Ruben Villegas, Forrester Cole,Kevin P Murphy, and Honglak Lee. Unsupervised learningof object structure and dynamics from videos. Advances inNeural Information Processing Systems, 32, 2019. 5, 6",
  "Mathieu Roche, Maguelonne Teisseire, and Gaurav Shrivas-tava. Valorcarn-tetis: Terms extracted with biotex. 2017.2": "Nirat Saini, Bo He, Gaurav Shrivastava, Sai Saketh Ramb-hatla, and Abhinav Shrivastava. Recognizing actions usingobject states. In ICLR2022 Workshop on the Elements ofReasoning: Objects, Structure and Causality, 2022. 1 C. Schuldt, I. Laptev, and B. Caputo. Recognizing humanactions: a local svm approach. In Proceedings of the 17thInternational Conference on Pattern Recognition, 2004. ICPR2004., pages 3236 Vol.3, 2004. 2, 4",
  "Gaurav Shrivastava and Abhinav Shrivastava. Diverse videogeneration using a gaussian process trigger. arXiv preprintarXiv:2107.04619, 2021. 1, 2, 6": "Gaurav Shrivastava and Abhinav Shrivastava. Video predic-tion by modeling videos as continuous multi-dimensionalprocesses. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 72367245,2024. 1 Gaurav Shrivastava, Ser-Nam Lim, and Abhinav Shrivastava.Video dynamics prior: An internal learning approach forrobust video enhancements. In Thirty-seventh Conference onNeural Information Processing Systems, 2023. 1",
  "Ruben Villegas, Arkanath Pathak, Harini Kannan, DumitruErhan, Quoc V. Le, and Honglak Lee. High fidelity videoprediction with large stochastic recurrent neural networks,2019. 2": "Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal.Mcvd-masked conditional video diffusion for prediction, gen-eration, and interpolation. Advances in Neural InformationProcessing Systems, 35:2337123385, 2022. 2, 5, 6, 7 Jacob Walker, Abhinav Gupta, and Martial Hebert. Patchto the future: Unsupervised visual prediction. In Proceed-ings of the IEEE conference on Computer Vision and PatternRecognition, pages 33023309, 2014. 2",
  "C. Training Details": "For the optimization of our model, we harnessed the compute oftwo Nvidia A6000 GPUs, each equipped with 48GB of memory,to train our CVP model effectively. We adopted a batch size of 64and conducted training for a total of 500,000 iterations. To opti-mize the model parameters, we employed the AdamW optimizer.Additionally, we incorporated a cosine decay schedule for learningrate adjustment, with warm-up steps set at 10,000 iterations. Themaximum learning rate (Max LR) utilized during training was 5e-5. . U-NET: We utilize Hugging face diffusers library for ourU-Net implementation. We utilize positional type for timestepembeddings. We utilize 4 layers per block. The target resolution forKTH, BAIR and Human3.6M is kept at 64 64 and 128 128 forUCF101 dataset. Additionally, we keep the number of timestepsT as 100 given our compute resources. c denotes the number ofchannels present in the frame. n is the number of initial contextframes based on which next frame is predicted,i.e., x0:n x1:n+1."
}