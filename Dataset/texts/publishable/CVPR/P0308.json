{
  "Abstract": "Garment manipulation (e.g., unfolding, folding andhanging clothes) is essential for future robots to accom-plish home-assistant tasks, while highly challenging due tothe diversity of garment configurations, geometries and de-formations. Although able to manipulate similar shapedgarments in a certain task, previous works mostly haveto design different policies for different tasks, could notgeneralize to garments with diverse geometries, and of-ten rely heavily on human-annotated data. In this paper,we leverage the property that, garments in a certain cate-gory have similar structures, and then learn the topologicaldense (point-level) visual correspondence among garmentsin the category level with different deformations in the self-supervised manner. The topological correspondence can beeasily adapted to the functional correspondence to guide themanipulation policies for various downstream tasks, withinonly one or few-shot demonstrations.Experiments overgarments in 3 different categories on 3 representative tasksin diverse scenarios, using one or two arms, taking one ormore steps, inputting flat or messy garments, demonstratethe effectiveness of our proposed method. Project page:",
  ". Introduction": "Next-generation robots should have the abilities to manip-ulate a large variety of objects in our daily life, includ-ing rigid objects, articulated objects and de-formable objects . Compared with rigid and articulatedobjects, deformable objects are much more difficult to ma-nipulate, for the highly large and nearly infinite state andaction spaces, and complex kinematic and dynamics. Gar-ments, such as shirts and trousers, are essential types of de-formable objects, due to the potentially wide-range applica-tions for both industrial and domestic scenarios. Manipu-lating garments, such as unfolding, folding and dressing up,",
  "*Equal contribution.Corresponding author": "has garnered significant interest in robotics and computervision fields.There have been a long range of studies on manipulat-ing relatively simple shaped deformable objects, such assquare-shaped cloths , ropes and cables , and bags . Nevertheless, manipulating garmentspresents a substantial challenge, as it necessities the com-prehensive understanding of more diverse geometries (gar-ments in a certain category have different shapes, let alonein different categories), more complex states (various ge-ometries with diverse self-deformations), and more difficultgoals (e.g., garments require multiple fine-grained actionsfold step by step). Many existing studies on garment ma-nipulation rely on large-scale annotated data , whichis labor-intensive and time-consuming, hindering the scala-bility in the scenarios of real-world applications. Besides,many works design quite different methods to tackle differ-ent specific tasks , making it difficult to effi-ciently share and reuse information among different tasks.Different from other object types, garments possess aproperty that, in a certain category, while different gar-ments may have different geometries, they usually share thesame structure. For example, tops (such as T-shirts, jacketsand jumpers), are composed of certain components (a bodywith two sleeves and a collar), and the topological struc-tures of the components are usually the same, even thoughthe length, width and geometries of a certain component indifferent garments may be quite different. Thanks to suchsimilarity in structure shared among garments in the cate-gory level, it is easy for humans to fulfill a task on unseennovel garments using the experience of manipulating onlyone or a few garments in the same category. Therefore,we empower robots with the above one/few-shot general-ization ability humans have in diverse tasks, by leveragingsuch structural similarity among garments.Among multiple ways to describe and represent gar-ments (e.g., poses , lines and keypoints ),skeleton , i.e., a graph of keypoints covering signif-icant points on garment edges and joints to represent thetopology of 3D objects, is suitable for describing the above-",
  "arXiv:2405.06903v1 [cs.CV] 11 May 2024": ". Given a demonstration garment (Middle) and the demonstration actions to fulfill a task (Middle-Left/-Right), for a novelobject, we find the manipulation points using the proposed Dense Visual Correspondence for Garment Manipulation and execute thecorresponding action to fulfill the task (Left/Right). Color similarity denotes in the correspondence space. mentioned structures shared among garments. The skele-ton points are sparse, distinct and ordered, and thus (1)exist on each garment and (2) can easily distinguish withother skeleton points, making them easy to learn. There-fore, we use skeleton points to build structural correspon-dence among garments. Moreover, as different-extent self-deformations make the garments to be quite complex, whileprevious works only studied skeleton points on rigid ,articulated or fixed-posed deformable objects in thecanonical view , we further extend skeleton points togarments at any deformation states, making a step to morerealistic scenarios for garment manipulation.While skeletons build topological correspondence be-tween different garments in the skeleton keypoint level,the state and action spaces of garments are exceptionallylarge and each point on the garment could be the manip-ulation point, making the sparse skeleton points unable tofully represent garments for manipulation. To represent ob-jects with large state and action spaces, dense (i.e., point-level or pixel-level) object representations, including denseobject descriptors and dense visual actionable affor-dance , which indicate the actionable information oneach point of the object, have demonstrated its superior-ity on rigid , articulated , and simple-shaped de-formable object manipulation . We further extend denserepresentations to garments, with the awareness of corre-spondences, using the proposed skeleton points, and thusachieve fine-grained manipulation.With dense visual correspondence aware of garmentstructures, one demonstration can roughly guide manipu-lating a novel garment by indicating corresponding actionpoints and policies. Furthermore, as manipulation for spe-cific tasks rely on not only garment structures but also task-specific knowledge, we further transform the representationfrom task-agnostic structural to task-specific functional formore accurate manipulation in various downstream tasks, using few-shot demonstrations to achieve this adaptation.To demonstrate the performance of our proposed repre-sentations, we conduct experiments on 3 different kinds ofgarments over 3 representative tasks. The experimental re-sults showcase the superiority of our proposed framework inmanipulating diverse novel garments in multiple tasks usingthe proposed dense visual correspondence and one or few-shot demonstrations.In summary, our contributions include: We propose to learn category-level dense visual corre-spondence to reflect the topological and functional cor-respondence across garments in different styles or defor-mations, which is an unified representation that facilitatesmanipulating diverse unseen garments in multiple taskswith one or few-shot demonstrations.",
  ". Dense Representations for Manipulation": "Dense object descriptors that learn point- or pixel-levelobject representations are proposed by and for robotic ma-nipulation. Many works extend such descriptors to proposegrasp pose , manipulate ropes , smooth fab-rics . Another series of works learn point-level affor-dance for articulated and deformable object ,language-guided , mobile and bimanual ma-nipulation, with exploration for interaction , facili-tating point-level contact point selection for diverse down-stream tasks. Our dense correspondence extends dense ob-ject representations in garment manipulation.",
  ". Visual Correspondence Learning": "Learning visual correspondence aims to reflect the sharedinformation (e.g., geometric, topological and functional in-formation) between different objects, which facilitates gen-eralization in diverse tasks, including functional percep-tion , pose estimation , grasping andfabric manipulation . For garments, although differentgarments have quite different geometries and deformationsin different states, they share similar structural and topolog-ical information in the category level, which can help in ma-nipulating novel garments with the demonstration of a gar-ment with the similar structure. So we propose to learn cor-respondence between garments to facilitate novel garmentmanipulation in diverse downstream tasks.",
  ". Cloth and Garment Manipulation": "Manipulating a square-shaped cloth is relatively well-studied, with previous works leveraging flow and dynam-ics , tactile feedback , dense representa-tions and reinforcement learning to tackledifferent tasks. Garment manipulation is more challenging,for the diversity of garment types and shapes, requiring themethod to handle diverse objects and states. While previ-ous works mainly learn the policy for a certain task, suchas folding , unfolding , grasping anddressing-up , on similar shaped garments, we focuson learning garment representations that can generalize todiverse objects in a category and facilitate many tasks.3. Problem Formulation Given an N-point (N = 10, 000) 3D partial point cloud ob-servation of a garment O RN3, garment manipulationaims to manipulate the garment by a sequence of n actionsto complete different tasks. As explained in ,each action ai includes grasping at a pick point ppicki,pulling to a place point pplacei without changing the orien-tation of the end-effector. Additionally, for dual-arm manip-ulation, each action ai includes a pair of pick points p1picki,p2picki and corresponding place points p1placei, p2placei.Given two garments O1 and O2, dense correspondenceevaluates the correspondence (normalized to ) intopology or function between each point pair (p1, p2), withp1 from O1 and p2 from O2.Given a task T, a demonstration includes the observa-tion O of a demonstration garment, and its correspondingdemonstration action (including single and dual-arm ac-tions) sequence (ppick1, pplace1, ..., ppickn, pplacen) thatcan fulfill T. Given the observation O of a new garmentto fulfill T, manipulation using dense correspondence firstfinds the points (ppick1, pplace1, ..., ppickn, pplacen) onO, where ppicki and pplacei have the best correspondencescore to ppicki and pplacei among all points on O, and thenexecutes the corresponding actions to fulfill the task on O.",
  ". Overview": "Our framework first learns topological dense visual cor-respondence aware of different garment deformations andshapes respectively using self-play and skeleton points(.2), with further coarse-to-fine refinement (Sec-tion 4.3). After the few-shot adaptation for different down-stream tasks, the learned correspondence turns from topo-logical to functional (.4), and thus could facilitatemanipulating unseen novel garments on various tasks usingone or few-shot demonstrations (.5). .6describes network architectures and the training strategy.",
  "Cross-Deformation Correspondence": "Many tasks, such as unfolding and hanging, require manip-ulating the garment at any random states (e.g., after a ran-dom drop). As demonstrated in , while garments havecomplex states and infinite deformations, the manipulationpolicies (manipulation points) are usually invariant to defor-mations. To empower the model with the ability to handlegarments in different deformations, we introduce learningcorrespondence across deformations of the same garment.Given two partial observations O and O of the same gar-ment in different deformations generated by self-play, and avisible point p on O, we can easily get its corresponding po-sition point p in O using point tracing in simulation. If p",
  "is visible, the representations fp and fp R512 of p and p": "extracted by the backbone network F, should be the same,as the representations are agnostic to self-deformations. Wenormalize point representations to be unit vectors, and thusthe similarity between fp and fp can be computed by thedot product of fp and fp, i.e., fp fp. For p on O, we usep on O as the positive point, and sample m negative points(m = 150): p1, p2, ..., pm. We pull close fp and fp, whilepush away fp and other point representations. FollowingInfoNCE , a widely-used loss function in one-positive-multi-negative-pair contrastive representation learning, weidentify the positive p amongst m negative samples:",
  ", where denotes the balancing coefficient in InfoNCE": ". Our Proposed Learning Framework for Dense Visual Correspondence. (Left) We extract the cross-deform correspondenceand cross-object correspondence point pairs respectively using self-play and skeletons, and train the per-point correspondence scores inthe contrastive manner, with the Coarse-to-fine module refines the quality. (Middle) Learned correspondence demonstrates point-levelsimilarity across different garments in different deformations. (Right) The learned point-level correspondence can facilitates multiplediverse downstream tasks using one or few-shot demonstrations.",
  "Cross-Object Correspondence": "In a certain category, while garments highly vary in origi-nal shapes, such as sizes, length-width ratios, sleeve lengthsand styles, they share the same topological structure. Theawareness of such structures will make it easy to manipu-late unseen novel garments with demonstrations.To leverage the shared structural information and gen-eralize to novel shapes, we propose to use skeleton, i.e., agraph of keypoints that represents topology of the 3D ob-ject, as the shared bridge for different garments with similarstructures. The reasons for using skeleton include: Skeleton points are distinct and sparse, thus easy to learnand generalize, compared to complicated representations; Skeleton points are distinct and ordered, making it easyto build topological correspondence between two objectsby aligning each specific skeleton point on them.To learn garment skeletons in the category level, we em-ploy the designs of Skeleton Merger , which can gen-erate skeletons for rigid objects, or canonically-posed (e.g.,flat) deformable objects. So we generate skeletons for flatgarments. Specifically, we generate s (s 50) orderedpoints on the point cloud observation O as skeleton points,with s (s 1)/2 activation scores ai,j (1 i, j s)indicating whether each edge between 2 skeleton points ex-ists on the object. We sample points on each edge, withtrainable offset for each sample, merging them into the re-constructed object O. The training signal is whether O cov- ers O (trains skeleton point positions) and whether sampledpoints on each edge exists in O (trains ai,j), As a result,skeleton points will exist on significant positions on gar-ments (e.g., boundaries, corners and intersections of parts)and meaningful edges will retain, as shown in (theLeft-Down part). More implementation details can be seenin the original paper of Skeleton Merger .As skeleton points are ordered, given observation O of aflat garment with one of its skeleton point p, we can get thecorresponding skeleton point p on the observation O of an-other flat one, by applying the skeleton network on O andget the skeleton point in the same order of p in O. Then,the topological correspondence between flat garments havebeen built in the skeleton-point level. As the features ex-tracted by neural networks are continuous when point po-sitions continuously change, and skeleton points cover thewhole garment, the feature of any point can be reflected byits nearby skeleton points (like interpolation) with topologi-cal information. Therefore, the representation of each pointon the garment will reflect its topology, and dense corre-spondence between flat garments has been naturally built.",
  "Since we have designed dense correspondence between thesame garment in different deformations, and dense corre-spondence between different flat garments, the next step": "is to aggregate them into one dense representation systemon diverse garments in any deformation states.We first project skeletons of garments in their flat statesto any deformation states using point tracing in simulation.Thus, given the observation O in random deformation withone of its skeleton point p, we can get the correspondingskeleton point p on the observation O of another garment inrandom deformation. If p is visible on O, fp and fp shouldbe the same. For p on O, we use p on O as the positive point,and sample m negative points (m = 150): p1, p2, ..., pm.We follow InfoNCE and use LCO for training:",
  "LCO = log(exp(fp fp/)mi=1 exp(fp fpi/))(2)": "In the meanwhile, we empower the cross-object corre-spondence with agnosticism to deformations. We samplethe observation O of the first garment O in another de-formation state as described in .2.1, and train itsCross-Deformation Correspondence using LCD in Equa-tion 1, together with LCO in Equation 2 to make the learnedrepresentations aware of both cross-deformation and cross-object point-level correspondence.",
  ". Coarse-to-fine Correspondence Refinement": "Although above framework can learn the general distribu-tions of all points representations using offline randomlycollected data, some difficult details (such as the boundariesbetween the folded sleeve on the garment body) should bepaid more attention by the model, and there may exist inac-curate representations on some points or areas. The abovephenomenon is also demonstrated in previous dense corre-spondence learning studies for 3D objects .Therefore, we propose the Coarse-to-fine (C2F) Corre-spondence Refinement procedure to make the model morefocused on difficult points on the garment, and eliminate in-accurate predictions, by refining the offline trained modelusing its online prediction failures.Specifically, for a certain garment, we sample a pointp on the observation O in one deformation state, predict itspoint-level correspondence score on O in another deforma-tion state, with p as corresponding point of p. We collectpoints {p1, p2, ..., pr} that meet the following requirements:",
  "their correspondence scores are higher than a correspon-dence thresh ;": "their distances to p in the canonical (flat) state (denotedas dpi for pi) are longer than a distance thresh .These points are prediction failures of the model trainedon offline data. We augment InfoNCE by distance (failurepoints farther away from p will receive more penalties totake more focus) to refine the model on them:",
  ". From Topological to Functional: Few-shot Adap-tation for Downstream Tasks": "While such topological structure of the above learned cor-respondence is significantly aligned with cross-object ma-nipulation policy, the point functionalities in different tasksmay differ to some extent, and thus could not be adequatelyreflected by a fixed representation. To adapt the learnedtopological correspondence to be functional for differentdownstream tasks, we propose the few-shot adaptation. With the trained model, for a certain task and a func-tional action point (such as the pick point on the left sleevefor folding) we annotate l (l 5) points p1, p2, ..., pl on lobservations O1, O2, ..., Ol of different garments in differ-ent deformations, and fine-tune the trained model to makefp1, fp2, ..., fpl to be the same using InfoNCE loss simi-lar to Equation 2 (replacing the topological correspondencepoint with the functional correspondence point as the pos-itive sample). Consequently, the model can generally keepthe topological information while become more aware ofthe functional information of the certain downstream task.",
  ". Manipulation Policy Generation": "For Unfolding, we use fling as the action for itsquickness in unfolding garments using only a few steps. Itpicks up 2 points simultaneously with 2 arms to lift the gar-ment, pull the 2 points apart to stretch the garment, fling thegarment and place it on the workplace. As some keypointsfor flinging may be occluded, we design 4 candidate pickpairs (e.g., the 2 endpoints of the shoulder, the 2 endpoints",
  ". Network Architectures and Training Strategy": "Segmentation-version PointNet++ is used as the back-bone feature extractor F that takes the point cloud observa-tion O as input to extract per-point features. The per-pointfeatures are directly used to calculate correspondences. We set batch size to be 32. In each batch, we sample 32garment pairs. For each garment pair, we sample 20 positivepositive point pairs, and 150 negative point pairs for eachpositive point pair. Therefore, in each batch, 32 32 20data will be used to update the model. During the Cor-respondence training stage, we train the model for 40,000batches. During Coarse-to-fine Refinement, we train themodel for 100 batches. During Few-shot Adaptation, weslightly refine the model using 5 demonstration data.",
  ". Tasks and Metrics": "We evaluate our method over 3 different representative gar-ment manipulation tasks: Unfolding that unfolds garments at random deformationsto be flat. The unfolding succeeds when the coverage areaof the unfolded garment exceeds a bar . The two sub-tasks, Unfold-RAND and Unfold-DROP, respectivelydenote the garment initial states are generated by a fewrandom actions or by dropping (more realistic). Folding that folds garments. A folding succeeds when theIntersection-over-Union (IOU) between the target and thefolded garments exceeds a bar . Fold-FLAT andFold-FLING respectively denote garment initial statesare perfectly flat or generated by flinging (more realistic). Hanging that hangs garments on the rack, with the suc-cessful rate metric . Hang-RAND and Hang-FLINGrespectively denote garment initial states are generated bya few random actions or by flinging (more realistic).",
  ". Baselines": "For Folding, we compare with ClothFunnels that learnskeypoints (e.g., endpoints of two sleeves, endpoints of thegarment bottom line) from large-scale human-annotateddata, pick and place keypoints step by step to fold garments.For Hanging, we compare with GCSR that detectsStructural Regions for manipulation (collars for hanging).Besides, we compare with DefoAfford that learnspoint-level actionable affordance scores for accomplishingthe task and selects the best point for interaction.For Unfolding, we compare with FlingBot that pre-dicts the garment coverage area after flinging each two-point grasp pair, and selects the best pair for fling. Besides,we compare with DefoAfford as it demonstrates the capa-bility to unfold fabrics using one gripper.",
  "Fold Trousers": ". Correspondence Guided Manipulation on DifferentGarment Types and Tasks. From left to right: observation, cor-respondence, manipulation points (colored points) selected usingcorrespondence to demonstrations and the manipulation action. of the bottom line), and select pick point pair of the obser-vation with the highest correspondence to designed candi-dates. We execute the fling action for at most 3 steps.For Folding, we pick and place keypoints (defined inClothFunnels ) on the garments to step by step fold gar-ments with one or two arms. Given the pick and place pointssequence in the demonstration, we select their closest pickand place points on each unseen object in the correspon-dence space and execute the pick-place action sequence.Moreover, to demonstrate the dense representations can fa-cilitate multiple manipulation strategies with slight humanannotations, we show 4 folding strategies achieved by ourmethod using one or few-shot demonstrations ().For Hanging, we pick the point ppick that is most closeto the demonstration pick point ppick, pull the garment up,and place it on the rack.",
  ". Results and Analysis": ", 2 and 3 present quantitative comparisons with base-lines. shows the learned correspondence on differ-ent garments shapes and deformations. demon-strates the manipulation actions guided by correspondence.For folding, as ClothFunnels keypoint detection modelis trained on garments in fully unfolded states, it is difficultto generalize to garments unfolded using FlingBot or ourmethod, as garments could not be perfectly unfolded into afully flat state. In contrast, as our method is trained with theawareness of self-deformations, it is easier to detect suchkeypoints in diverse garment states. WhileUniFolding",
  ". Results for Hanging": "trains using and thus works well on deformed states, it is de-signed for the specific folding task. In contrast, our methodcan facilitate multiple downstream tasks. Furthermore, asshown in , our method can work well on different ,while policies trained on large-scale annotated data cannoteasily generalize to novel manipulation methods.For unfolding, DefoAfford cannot perform well in 3",
  ". Visualization of Different Folding Policies": "steps as it only utilizes one gripper. For FlingBot, althoughit is trained using large-scale different states and interac-tions, without training on many garments, it cannot gener-alize well to novel garments. Besides, it is costly in time andcomputing resources, in that it requires separately training96 models to generate affordance maps in 12 garment ro-tation types and 8 garment scale types, and then selectingthe best pick points pair in all the 96 rotation-scale combi-nations. In contrast, as the learned dense correspondence isaware of different garment scales and rotations, we can di-rectly use one model to select the grasp points for flinging.For the comparison with GCSR in folding, as it requirescollar detection as Structural Regions, it cannot performwell on garment states where collars are occluded.It is worth mentioning that, while the baselines aremostly designed for specific tasks and may pose require-ments to garment initial states, our proposed dense visualcorrespondence is an unified representation for garments,and thus makes it easy to generate policies for multipledownstream tasks on different garment deformations.",
  ". Real-world Evaluation": "Setup. As shown in , our real-world experimentsetup consists of two Franka Panda robot arms, and a Mi-crosoft Azure Kinect camera (which has demonstrated high-precision with slight noises for robotic manipulation ) capturing top-down point cloud. We use Segment Any-thing (SAM) to segment the garment from the sceneand project the segmented image with depth to point cloud.Please refer to the supplementary materials for more de-tails and videos of real-world manipulations.To align the scanned point cloud with those in simula-tion, we annotate a few skeleton points on 3 real-world gar-",
  ". Real-world Evaluation on Folding": "ments (, a), with the correspondence between an-notated skeleton points and those in simulation shown inb(1) and b(2), and fine-tune the pre-trained model by push-ing close the representations of skeleton points on garmentsin simulation and the real world using InfoNCE. The corre-spondence is adapted from c(1) to c(2) in .We use 5 real world different-shaped tops, each conduct-ing folding on 3 different initial deformations, and reportthe number of successful executions in .",
  ". Conclusion": "We propose to learn dense visual correspondence for di-verse garment manipulation tasks with category-level gen-eralization using only a few annotations.We first traintopological correspondence self-supervisedly using self-play and garment skeletons, and then fine-tune it using few-shot demonstrations to transform the topological correspon-dence to be functional to different downstream tasks. Exten-sive experiments demonstrate the superiority of our method.7. AcknowledgmentThis work was supported by National Natural ScienceFoundation of China (No. 62136001) and National YouthTalent Support Program (8200800081). We thank YangyiYe, Kunqi Xu and Haotong Zhang for assisting real-worldexperiments, Yang Tian and Jiyao Zhang for discussions.",
  "A.3. Point Cloud Alignment": "To align the scanned point cloud with those in simulation,we annotate a few skeleton points on 3 real-world garments,with the correspondence between annotated skeleton points,and fine-tune the pre-trained model by pushing close therepresentations of skeleton points on garments in simula-tion and the real world using InfoNCE. shows moreresults on the Correspondence after the Point Cloud Align-ment, apart from the figure in the main paper.",
  ". More Results of Few-shot Adaptation": "point pairs, and 150 negative point pairs for each positivepoint pair. Therefore, in each batch, 32 32 20 data willbe used to update the model. During the Correspondencetraining stage, we train the model for 40,000 batches. Dur-ing Coarse-to-fine Refinement, we train the model for 100batches. During Few-shot Adaptation, we slightly refine themodel using 5 demonstration data. Besides, we set the num-ber of skeleton pairs to be 50.",
  "shows more results of few-shot adaptation": "Yahav Avigal, Lars Berscheid, Tamim Asfour, TorstenKroger, and Ken Goldberg. Speedfolding: Learning efficientbimanual folding of garments. In 2022 IEEE/RSJ Interna-tional Conference on Intelligent Robots and Systems (IROS),pages 18. IEEE, 2022. 1, 3 Arpit Bahety, Shreeya Jain, Huy Ha, Nathalie Hager, Ben-jamin Burchfiel, Eric Cousineau, Siyuan Feng, and ShuranSong. Bag all you need: Learning a generalizable baggingstrategy for heterogeneous objects. IROS, 2023. 1",
  "Hugo Bertiche, Meysam Madadi, and Sergio Escalera.Cloth3d: clothed 3d humans. In European Conference onComputer Vision, pages 344359. Springer, 2020. 6": "Alper Canberk, Cheng Chi, Huy Ha, Benjamin Burchfiel,Eric Cousineau, Siyuan Feng, and Shuran Song. Cloth fun-nels: Canonicalized-alignment for multi-purpose garmentmanipulation. In International Conference of Robotics andAutomation (ICRA), 2022. 1, 3, 6 Lawrence Yunliang Chen, Baiyu Shi, Daniel Seita, RichardCheng, Thomas Kollar, David Held, and Ken Goldberg. Au-tobag: Learning to open plastic bags and insert objects. In2023 IEEE International Conference on Robotics and Au-tomation (ICRA), pages 39183925. IEEE, 2023. 1",
  "Antonio Gabas and Yasuyo Kita. Physical edge detectionin clothing items for robotic manipulation. In 2017 18th In-ternational Conference on Advanced Robotics (ICAR), pages524529. IEEE, 2017. 1": "Aditya Ganapathi, Priya Sundaresan, Brijen Thananjeyan,Ashwin Balakrishna, Daniel Seita, Jennifer Grannen, MinhoHwang, Ryan Hoque, Joseph E Gonzalez, Nawid Jamali,et al.Learning dense visual correspondences in simula-tion to smooth and fold real fabrics. In 2021 IEEE Inter-national Conference on Robotics and Automation (ICRA),pages 1151511522. IEEE, 2021. 2, 3, 7 Haoran Geng, Ziming Li, Yiran Geng, Jiayi Chen, HaoDong, and He Wang. Partmanip: Learning cross-categorygeneralizable part manipulation policy from point cloud ob-servations.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 29782988, 2023. 1",
  "Huy Ha and Shuran Song. Flingbot: The unreasonable ef-fectiveness of dynamic manipulation for cloth unfolding. InConference on Robot Learning, pages 2433. PMLR, 2022.6": "Oshri Halimi, Or Litany, Emanuele Rodola, Alex M Bron-stein, and Ron Kimmel.Unsupervised learning of denseshape correspondence.In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 43704379, 2019. 5 Rasmus Laurvig Haugaard and Anders Glent Buch.Sur-femb: Dense and continuous correspondence distributionsfor object pose estimation with learnt surface embeddings.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 67496758, 2022. 3 Ling Hu, Qinsong Li, Shengjun Liu, and Xinru Liu. Effi-cient deformable shape correspondence via multiscale spec-tral manifold wavelets preservation.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1453614545, 2021. 5 Rishabh Jangir, Guillem Alenya, and Carme Torras.Dy-namic cloth manipulation with deep reinforcement learning.In 2020 IEEE International Conference on Robotics and Au-tomation (ICRA), pages 46304636. IEEE, 2020. 3 Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-thing. arXiv preprint arXiv:2304.02643, 2023. 8, 9",
  "Zihang Lai, Senthil Purushwalkam, and Abhinav Gupta. Thefunctional correspondence problem. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 1577215781, 2021. 3": "Yinxiao Li, Danfei Xu, Yonghao Yue, Yan Wang, Shih-FuChang, Eitan Grinspun, and Peter K Allen. Regrasping andunfolding of garments using predictive thin shell modeling.In 2015 IEEE International Conference on Robotics and Au-tomation (ICRA), pages 13821388. IEEE, 2015. 3 Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B. Tenenbaum,and Antonio Torralba. Learning particle dynamics for ma-nipulating rigid bodies, deformable objects, and fluids. In In-ternational Conference on Learning Representations, 2019.6 Yu Li, Kai Cheng, Ruihai Wu, Yan Shen, Kaichen Zhou,and Hao Dong. Mobileafford: Mobile robotic manipulationthrough differentiable affordance learning. In 2nd Workshopon Mobile Manipulation and Embodied Intelligence at ICRA2024, 2024. 2 Yu Li, Xiaojie Zhang, Ruihai Wu, Zilong Zhang, YiranGeng, Hao Dong, and Zhaofeng He. Unidoormanip: Learn-ing universal door manipulation policy over large-scale anddiverse door manipulation environments.arXiv preprintarXiv:2403.02604, 2024. 2",
  "Jan Matas, Stephen James, and Andrew J Davison. Sim-to-real reinforcement learning for deformable object manipu-lation. In Conference on Robot Learning, pages 734743.PMLR, 2018. 3": "Kaichun Mo, Leonidas J Guibas, Mustafa Mukadam, Abhi-nav Gupta, and Shubham Tulsiani. Where2act: From pixelsto actions for articulated 3d objects. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 68136823, 2021. 2 Chuanruo Ning, Ruihai Wu, Haoran Lu, Kaichun Mo, andHao Dong. Where2explore: Few-shot affordance learningfor unseen novel categories of articulated objects.In Ad-vances in Neural Information Processing Systems (NeurIPS),2023. 2, 8, 9 Timothy Patten, Kiru Park, and Markus Vincze. Dgcm-net:dense geometrical correspondence matching network for in-cremental experience-based robotic grasping. Frontiers inRobotics and AI, 7:120, 2020. 3 Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas JGuibas. Pointnet++: Deep hierarchical feature learning onpoint sets in a metric space. Advances in neural informationprocessing systems, 30, 2017. 5 Franka Robotics. Franka emika panda, . 9 Franka Robotics. Libfranka, . 9 Daniel Seita, Aditya Ganapathi, Ryan Hoque, MinhoHwang, Edward Cen, Ajay Kumar Tanwani, Ashwin Balakr-ishna, Brijen Thananjeyan, Jeffrey Ichnowski, Nawid Jamali,et al. Deep imitation learning of sequential fabric smoothingfrom an algorithmic supervisor. In 2020 IEEE/RSJ Interna-tional Conference on Intelligent Robots and Systems (IROS),pages 96519658. IEEE, 2020. 3 Daniel Seita, Pete Florence, Jonathan Tompson, ErwinCoumans, Vikas Sindhwani, Ken Goldberg, and AndyZeng. Learning to rearrange deformable cables, fabrics, andbags with goal-conditioned transporter networks. In 2021IEEE International Conference on Robotics and Automation(ICRA), pages 45684575. IEEE, 2021. 1, 3 Ruoxi Shi, Zhengrong Xue, Yang You, and Cewu Lu. Skele-ton merger: an unsupervised aligned keypoint detector. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 4352, 2021. 1, 2, 4",
  "and Vincent Sitzmann.Neural descriptor fields: Se(3)-equivariant object representations for manipulation. In ICRA,2022. 2": "Priya Sundaresan, Jennifer Grannen, Brijen Thananjeyan,Ashwin Balakrishna, Michael Laskey, Kevin Stone, Joseph EGonzalez, and Ken Goldberg.Learning rope manipula-tion policies using dense object descriptors trained on syn-thetic depth data. In 2020 IEEE International Conference onRobotics and Automation (ICRA), pages 94119418. IEEE,2020. 2 Neha Sunil, Shaoxiong Wang, Yu She, Edward Adelson, andAlberto Rodriguez Garcia. Visuotactile affordances for clothmanipulation with local control. In Proceedings of The 6thConference on Robot Learning, pages 15961606. PMLR,2023. 3 Sashank Tirumala, Thomas Weng, Daniel Seita, Oliver Kroe-mer, Zeynep Temel, and David Held. Learning to singulatelayers of cloth using tactile feedback.In 2022 IEEE/RSJInternational Conference on Intelligent Robots and Systems(IROS), pages 77737780, 2022. 3 Matthias Vestner, Zorah Lahner, Amit Boyarski, Or Litany,Ron Slossberg, Tal Remez, Emanuele Rodola, Alex Bron-stein, Michael Bronstein, Ron Kimmel, et al. Efficient de-formable shape correspondence via kernel matching. In 2017international conference on 3D vision (3DV), pages 517526. IEEE, 2017. 5 Yian Wang, Ruihai Wu, Kaichun Mo, Jiaqi Ke, Qingnan Fan,Leonidas Guibas, and Hao Dong. Adaafford: Learning toadapt manipulation affordance for 3d articulated objects viafew-shot interactions. European conference on computer vi-sion (ECCV 2022), 2022. 2",
  "Thomas Weng, Sujay Bajracharya, Yufei Wang, KhushAgrawal, and David Held. Fabricflownet: Bimanual clothmanipulation with a flow-based policy.In Conference onRobot Learning, 2021. 3": "Ruihai Wu, Yan Zhao, Kaichun Mo, Zizheng Guo, YianWang, Tianhao Wu, Qingnan Fan, Xuelin Chen, LeonidasGuibas, and Hao Dong. VAT-mart: Learning visual actiontrajectory proposals for manipulating 3d ARTiculated ob-jects. In International Conference on Learning Represen-tations, 2022. 2 Ruihai Wu, Kai Cheng, Yan Shen, Chuanruo Ning, GuanqiZhan, and Hao Dong. Learning environment-aware affor-dance for 3d articulated object manipulation under occlu-sions. In Advances in Neural Information Processing Sys-tems (NeurIPS), 2023. 8, 9",
  "Ruihai Wu, Chuanruo Ning, and Hao Dong. Learning fore-sightful dense visual affordance for deformable object ma-nipulation. In IEEE International Conference on ComputerVision (ICCV), 2023. 1, 2, 3, 6": "Yilin Wu, Wilson Yan, Thanard Kurutach, Lerrel Pinto, andPieter Abbeel. Learning to manipulate deformable objectswithout demonstrations. In 16th Robotics: Science and Sys-tems, RSS 2020. MIT Press Journals, 2020. 1 Chi Xu, Lakshmi Narasimhan Govindarajan, Yu Zhang, andLi Cheng. Lie-x: Depth image based articulated object poseestimation, tracking, and action recognition on lie groups. In-ternational Journal of Computer Vision, 123:454478, 2017.2",
  "ZhenjiaXu,ChengChi,BenjaminBurchfiel,EricCousineau, Siyuan Feng, and Shuran Song. Dextairity: De-formable manipulation can be a breeze. RSS, 2022. 6": "Han Xue, Yutong Li, Wenqiang Xu, Huanyu Li, DongzheZheng, and Cewu Lu. Unifolding: Towards sample-efficient,scalable, and generalizable robotic garment folding. In 7thAnnual Conference on Robot Learning, 2023. 1, 3, 6 Han Xue, Wenqiang Xu, Jieyi Zhang, Tutian Tang, YutongLi, Wenxin Du, Ruolin Ye, and Cewu Lu. Garmenttrack-ing: Category-level garment pose tracking. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 2123321242, 2023. 1",
  "Zhengrong Xue, Zhecheng Yuan, Jiashun Wang, XueqianWang, Yang Gao, and Huazhe Xu. Useek: Unsupervised se(3)-equivariant 3d keypoints for generalizable manipulation.ICRA, 2023. 3": "Lin Yen-Chen, Pete Florence, Jonathan T Barron, Tsung-YiLin, Alberto Rodriguez, and Phillip Isola. Nerf-supervision:Learning dense object descriptors from neural radiancefields. In 2022 International Conference on Robotics andAutomation (ICRA), pages 64966503. IEEE, 2022. 2, 3 Fan Zhang and Yiannis Demiris. Learning grasping pointsfor garment manipulation in robot-assisted dressing. In 2020IEEE International Conference on Robotics and Automation(ICRA), pages 91149120. IEEE, 2020. 3",
  "Fan Zhang and Yiannis Demiris.Learning garment ma-nipulation policies toward robot-assisted dressing. ScienceRobotics, 7(65):eabm6010, 2022. 3": "Yan Zhao, Ruihai Wu, Zhehuan Chen, Yourong Zhang,Qingnan Fan, Kaichun Mo, and Hao Dong.Dualafford:Learning collaborative visual affordance for dual-gripper ob-ject manipulation.International Conference on LearningRepresentations (ICLR), 2023. 2 Bingyang Zhou, Haoyu Zhou, Tianhai Liang, Qiaojun Yu,Siheng Zhao, Yuwei Zeng, Jun Lv, Siyuan Luo, QiancaiWang, Xinyuan Yu, Haonan Chen, Cewu Lu, and Lin Shao.Clothesnet: An information-rich 3d garment model reposi-tory with simulated clothes environment. ICCV, 2023. 1,2 Heming Zhu, Yu Cao, Hang Jin, Weikai Chen, Dong Du,Zhangye Wang, Shuguang Cui, and Xiaoguang Han. Deepfashion3d: A dataset and benchmark for 3d garment recon-struction from single images.In Computer VisionECCV2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part I 16, pages 512530. Springer,2020. 1"
}