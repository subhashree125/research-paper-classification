{
  "Abstract": "Deep learning has been impressively successful in thelast decade in predicting human head poses from monoc-ular images. However, for in-the-wild inputs the re-search community relies predominantly on a single train-ing set, 300W-LP, of semisynthetic nature without manyalternatives. This paper focuses on gradual extensionand improvement of the data to explore the performanceachievable with augmentation and synthesis strategiesfurther. Modeling-wise a novel multitask head/loss de-sign which includes uncertainty estimation is proposed.Overall, the thus obtained models are small, efficient,suitable for full 6 DoF pose estimation, and exhibit verycompetitive accuracy.",
  "Introduction": "Human head pose estimation (HPE) is an importanttask in many application e.g. in the automotive sectoror in entertainment products. The concrete problemaddressed by this paper is monocular head pose estima-tion for in-the-wild scenarios, i.e. from given a facialcrop a computer vision system must estimate the faceorientation. Closely related is the task of face alignmentwhere a mathematical description of a face is inferred,e.g. as landmarks or full 3D reconstruction. Thesetasks are particularly challenging due to the diversityand non-rigid nature of faces. However, in recent yearsdeep learning methods have been very successful at it. In HPE, the current state of the art achieves errorsof ca. 3 (MAE) to 5 (geodesic) .The main interest behind this work lies in devisinga simple, efficient and effective HPE model. Becausethe margin for model size and complexity is thereforelimited, the choice of the training data is a knob leftto turn freely. In this regard the goal this paper isexploration of this path toward further improvementsof HPE models. Extension and combination of existingdatasets, as well as augmentation is the means to thisend which might also serve the community in the futurewith custom datasets. However, first we shall reviewrelated literature. For in-the-wild HPE we can recognize two datasetsas de facto standard: AFLW2000-3D and 300W-LP. AFLW2000-3D, serving as test set, consists of2000 images labeled with 6 DoF poses, parameters ofdeformable facial 3D model, as well as landmarks. Theimages are challenging due to occlusion, extreme posesand varying illumination. 300W-LP, commonly used astraining data, similarly consists ca. 61 thousand labeledimages. It was constructed by augmenting a smallerset of faces with out-of-plane rotations. Some authors chose to expand this data further with fine-grainedmovements, or employ a face swapping augmentation. However, the impact of this was not considered inisolation.In contrast, the Biwi dataset , popular as anotherbenchmark, comprises 24 sequences including extremeposes of 20 subjects in a laboratory environment. Inter-estingly, it is used in recent works which trainon fully synthetic images from the SynHead dataset .They are however limited to the lab setting and a lownumber of individuals.Regarding mathematical models for HPE, a base-line may consist of a learned feature extractor, such asa convolutional neural network and final linear layerswhich, after some simple transformation, output orien-tation, position, size and so on. Such a network couldbe trained with losses such as L1 or L2 penalizing theerrors from the known ground truths. Various rotationrepresentations have been introduced . Moresophisticated model architectures and loss calculationsare proposed in . The authors of devisedan algorithm switching between different losses dynam-ically. Multitask networks leveraging synergies between3D HPE and 2D landmark prediction were consideredin . Competitive HPE results could also be achievedby suitably shaped, yet relatively simple loss functions.Uncertainty estimation, while rarely addressed inthe face analysis community , is potentially usefulfor filtering and outlier rejection. However, explorationdownstream applications is beyond the scope of thispaper. Ultimately, uncertainty estimation is includedbecause it boosted the models accuracy. For simplicity,",
  "arXiv:2407.05357v3 [cs.CV] 15 Oct 2024": "only aleathoric (data) uncertainty is modeled. This isimplemented by outputting parameters for an assumedprobability density of the data and taking its negativelog likelihood as loss . In the space of rotationsSO(3) this is particularly challenging numerically, re-quiring e.g. normalization constants which do not haveclose-form expressions . Here, this willbe addressed by a tangent space formulation.In short, the contributions of this paper are: A HPE model yielding improved accuracy over thecurrent state of the art.",
  "Novel multitask head/loss and data augmentationdesigns": "A novel tangent-space approach for rotational un-certainty estimation.While most ingredients are nothing new, their combi-nation into competitive designs in the field of HPE is.The model was integrated into the FOSS OpenTrack1, and will therefore be referred to as OpNet. The fullsource code for training and evaluation is available ongithub 2.",
  "Model Design": "The model architecture consists of a feature extractionbackbone, global pooling, 50% dropout, and a linearlayer per prediction head. The raw features of each headare mapped to final predicted quantities, respectively.As input, the model takes a 129129 monochrome facecrop. The motivation for monochrome inputs is invari-ance to hue changes in the local illumination conditions.ResNet18 and MobileNetV1 were picked asfeature extractors since they are lightweight and provedto work well on various tasks. To enable full 6 DoFtracking, and to enable learning from landmark-onlyannotations, the model has the following outputs: 3Drotation, 2D position and size, facial shape parameters,bounding box, and uncertainty parameters.Rotations are represented by quaternions, whichare suitable for limited range HPE , avoiding thegimbal-lock problem of Euler angles. Note that quater-nions q and q represent the same orientation. Ac-curacy was better if this ambiguity was avoided andthe network was biased toward identity output. Hence,the quaternion is formed from the feature z1 . . . z4 by",
  "q = q/||q||, q = iz1+jz2+kz3+smoothclip(z4), wherei,j,k is the imaginary basis, smoothclip(x) = ELU(x)+1maps R to R+, and ELU is the function introduced in": "2D position and size are both estimated in imagespace, normalized to . The 2D position is takenidentically from the respective linear layer. The size fea-ture is passed through smoothclip in order to guaranteea positive value. The model outputs shape parameters for the BaselFace Model (BFM) which consists of a 3D base-geometry modified by a combination of deformation ba-sis vectors. Following the literature ,the parameters are coefficients for 50 bases to realizedifferent facial shapes and expressions. Only 68 pointswhich make up the 3D landmarks are actually com-puted. They adhere to the MultiPIE 68-point markup. The rationale for bounding box prediction is prac-ticality, namely tracking the face through a video se-quence as in the demo from . The output is param-eterized by center and size, where the size-features aremapped by smoothclip to positive values. Rotation uncertainty is considered in the tangent-space of rotations so(3) which essentially encodes offsetsfrom a particular orientation by rotation vectors. Thus,the data variation around the predicted pose can be de-scribed by a standard multivariate normal distribution.To eliminate redundancy, the center of the distributionis fixed at zero. It has been shown that for small vari-ance this formulation approximates a distribution onSO(3) asymptotically . Consequently, we must parameterize a covariancematrix . To this end, the network outputs a lowertriangular matrix M R33 filled with six featurestaken from a BatchNorm (BN) layer added after thefinal linear layer. Intuitively, BN helps decouple thelearning of the magnitude of the variance from theinfluence of unrelated losses. Without it, the networksdidnt perform well. Hence, given M, the covariancematrix is set to = MMT + I resembling a Choleskydecomposition, but the diagonals of M do not need tobe positive and the addition of I scaled with a smallconstant ensures strict positivity. Note that MMT issymmetric positive semi-definite. Position and size uncertainty is modeled by a3D multivariate normal distribution for the triplet com-bining 2D position and head size. Its covariance matrixis constructed like in the case of rotations.",
  "Losses": "The training procedure minimizes the sum of individuallosses, corresponding to the predicted quantities (in thefollowing marked with hat .).In general, mean and variance parameters and are learned by minimizing the negative log likelihood(NLL), i.e. log p(y|(x), (x)) of the data densityp over some dataset consisting of input-output tuples(x, y) . Naive approaches have been reported asunstable . As remedy, an initial period where thevariance is fixed was suggested. For historical reasonsthe present model is instead trained with a combina-tion of traditional regression losses and NLL, and for\"symmetry\" reasons NLL losses are employed even forshape parameters, bounding box and landmarks, in caseof which variances are learned as auxiliary parametersindependent of the inputs.For the rotation prediction we penalize the geodesicdistance by the losses",
  "NLLrot(q, q, rot) = log f(Im log(q1q)|0, rot),(2)": "where Lrot was inspired by the metrics surveyed in. The signifies the inner product of vectors. InNLLrot, f denotes the probability density, log returnsan imaginary quaternion containing the rotation vector,from which Im extracts the imaginary part as 3d vector,the length of which is the geodesic distance, i.e. thesmallest rotation magnitude between q and q.Furthermore, considering position and size stackedin a 3d vector p, the employed losses are L2 and NLLwith the normal distribution with variance p. We thusdefine",
  "NLLp(p, p, p) = log f(p|p, p).(4)": "The shape parameters, denoted i, are assumed tobe distributed independently normal. This simplifies thecovariance to a diagonal matrix shp = diag(1, 2, . . . ).The corresponding losses are the L2 loss L(, ) andNLL with normal distribution NLL(, , ) whichare defined analogously to Eq. (3) and Eq. (4), andomitted for brevity.For landmarks the L1 loss worked well. The cor-responding NLL is based on the Laplace distribution.Again, statistical independence is assumed. Then thetotal loss decomposes into sums over coordinate-wisecontributions. Moreover, it is useful to apply weightswi to different parts of the face. Thus, the losses are",
  "iwi log fLaplace(i|i, ,i), (6)": "where i runs over the 68 3 spatial landmark coordi-nates. If only x and y coordinates are available, thenthe summation runs only over those.The bounding box is trained like the shape param-eters using L2 loss Lbb and NLLbb. Input to those lossesare the box corner coordinates, assuming independence.In order to encourage the network to output nearlyunit quaternions, the term Lnorm(q) = |1 |q||2 isadded, where q is the unnormalized quaternion.At last we can define the total loss L by",
  "Augmentations": "Data samples consist of the input image, a 2D facialbounding box (BB), and the remaining labels. They arefirst subject to geometric transformations where alsocropping to the face area is performed. In principle,a square region of interest (ROI) is generated initiallyfrom the BB extending its shortest side. This ROI issubsequently scaled, rotated, and translated by randomamounts. Resampling this (rotated) square at the inputresolution creates the face crop. Additionally, the cropis mirrored with probability p = 1/2 and rotated by90 with p = 1/100. Note that no stretching occurs.Finally, the BB is regenerated by taking the BB aroundthe transformed corner points.Afterwards image intensity augmentations are ap-plied 3. This process is designed to occasionally producestrong distortions with the intent to facilitate gener-alization to overexposure, noisy low-light images andsimilar challenging inputs. First, from Equalize, Pos-terize, Gamma, Contrast, Brightness, and Blur, fourare picked and applied with small probabilities up top = 1/10 chance. Then Gaussian noise is randomlyadded with p 1/2 and scale up to = 16/256 w.r.t.the normalized image intensity.",
  "Datasets": "Initially training was conducted on 300W-LP, however,performance turned out lacking and facial-expressiondependent systematic pose errors were noticeable. Hy-pothetically, lacking diversity in 300W-LP might con-tribute to that, i.e. limited pitch range, uniform illu-mination and mostly open eyes. This motivated thecreation of a new dataset, closely following the creationof 300W-LP which should address these shortcomings.Thereby only the out-of-plane rotation synthesis wasperformed using the original 300W-LP source imageswith their BFM parameters.",
  "Extended 300W-LP Reproduction": "Lets consider the key ingredients. Firstly, the 3D facemodel. On the basis of same facial region of the BFM asin , a smooth transition to a background plane wasmodeled. The resulting mesh is shown in the Appendix. The deformation basis was extended to the newvertices by copying the vectors from the closest BFMvertex and attenuating them by a distance-dependentdecay factor. To the original basis, new shapes withclosed eyes were added. Given an input image, a depthprofile is imposed on the background plane according toa monocular depth estimate. This is performed by anoff-the-shelf MiDaS model 4 . Then, as in , anew image is generated by projecting the original imageonto the 3D mesh, rotating the face together with theleft or right half of the background, smoothly blendingbetween transformed and pristine parts, and renderingthe result. In addition to unlit renderings, some facesare lit from the side with probability 1/1000. Closedeyes are sampled with probability 1/2.The Appendix contains comparisons with 300W-LPin . Furthermore, scatter plots of rotation dis-tributions are depicted in . It shall be said thatwhile useful, the novel eye and illumination additionsare far from perfect. The eye regions suffer from smallmisalignment errors and the illumination suffers fromshadow-mapping artifacts and generally does not lookparticularly realistic. The source code is available in aseparate repository 5.",
  "MiDaS v3 - Hybrid,": "are available. As an improvised solution, 2D landmarkannotations were leveraged. Perfect labels are therebynot the goal but that the network could learn fromrelative differences between frames generated by out-of-plane rotations synthesis.Inspired by the face-alignment methods (based on the FLAME head-model ), the generalidea is to fit 3D landmarks of the BFM to the 2D anno-tations. Those methods incorporate also photometricfitting and other techniques. Here, to keep things simpleand consistent with 2D, only the visible side of the BFMis used. Then indeed landmarks alone are not sufficientto identify plausible 3D reconstructions. To remedythis, initial guesses and pose priors were obtained froma small neural network ensemble trained without theextended data, and the space of possible shape parame-ters was soft-constrained by incorporating a NLL lossof a Gaussian mixture which was fitted to the shapeparameters in the 300W-LP dataset. Ultimately, thelabeling process consists of solving a standard minimiza-tion problem for the sum of several losses: landmarkerror, rotation error from the prior, the shape NLL,as well as soft-constraints for quaternion normalizationand non-negative head-size. The result was manuallycurated, removing poorly fitted frames. Afterwards therotation expansion from Sec. 3.1 was applied. Code andnotebooks to reproduce every step is available in thesource repository.The procedure was applied to WFLW and LaPa. Appealing for this paper, they consist of facialimages of a large variety of individuals, poses, andocclusions. WFLW comprises 7.5 103 training imageswith 98 manually annotated landmarks. The landmarkswere converted to 68 points by interpolation. LaPacontains ca. 1.8 104 images annotated with 106 points.However, the latter includes images from 300W-LPwhich were excluded due to the overlap. The remainingimages are from Megaface . Ultimately, there were4942 images from LaPa expanded to ca. 7.7 104, and1554 images from WFLW expanded to ca. 2.2 104.The created datasets are provided online. and in the Appendix show sample images.",
  "The Face Synthetics (FS) dataset consists of 105": "fully synthetic, photorealistic, rendered human heads,annotated with segmentation masks and 3D landmarks.The subjects are composed of randomly sampled faceshape, hairstyle, accessories, skin color, superimposedon a variety of backgrounds. Thus, the annotations areperfect and artifacts from 300W-LP-style out-of-planerotations are absent. The authors provide only the 3D landmarks and of those only the x and y coordinates.Therefore, only the corresponding landmark losses Land NLL are enabled.The facial bounding box isconstructed based on the segmentation, encompassingpixels marked as \"face\" and \"nose\". Samples where aside length is less than 32 pixels are filtered out.",
  "Implementation Details": "The network is trained with the ADAM optimizer for N = 15M samples with a maximum learning (LR)rate of 103. The LR ramps up for N/20 samples anddecreases after N/2 samples to 1/10th. After 2/3Nsamples, stochastic weight averaging is enabled.Furthermore, gradient clipping with threshold 0.1 isused. Training time is a few hours on standard desktophardware.Facial BBs are often taken around the annotatedlandmarks . Here, they encompass the full re-constructed facial section of the BFM taken from (using all of its vertices). In case of Biwi, which providesneither landmarks nor boxes, boxes are extracted fromthe annotations file provided with and shrunk by80%. As a result, the facial BBs are consistent acrossthe 300W-LP family, AFLW2000-3D, Biwi and FS.Regarding cropping, the scale factor determiningthe facial ROI is sampled from N(s, 0.1), with s = 1.1and subsequently clipped to [s 0.5, s + 0.5]. Next,consider the ROI translation after scaling. A maximummovement of t = 1",
  "max(0, roi bb) + 1": "3bb is allowed,where roi and bb stand in sloppy notation for the extentof the expanded ROI and the original boxs side length,respectively. The concrete translation is sampled fromN(0, t/2) and clipped to [t, t].This design allowssome translation for zoomed-in crops and otherwiseplacement of the face anywhere in the crop such that70% of the BB remains visible. The ROI rotation angleis sampled from a uniform distribution between 30and 30.As intensity augmentation, noise is in fact poten-tially added twice, once with probability p = 1/2 and = 4 and secondly with p = 0.1 and = 16. Thisredundant application is implemented as such purelyfor convenience.Multiple datasets are combined via simple randomdraws. First a dataset is picked with a certain frequency,followed by picking a sample from the dataset withreplacement.Regarding the landmark weights, eye centers (i.e.top and bottom, 8 point in total) are weighted withwi = 0 since good samples with closed eyes are scarce.The loss weights are rot = 1,p = 1, = 0.01, =1,bb = 0.01, norm = 106, total = 0.01 to bring",
  "Results": "The model was evaluated on AFLW2000-3D and Biwiintroduced in Sec. 1. Furthermore, some ablation exper-iments were conducted as well as an analysis of the noiseresponse and effectiveness of the uncertainty estimation.Evaluations were performed on five different net-works and the metrics were averaged. Reported arethe absolute errors of Euler angles, the mean of those(MAE), as well as the average of the geodesic errors(||Im log(q1q)||). The largest observed standard errorof the sample mean was 0.03 among all evaluations ofAFLW2000-3D and 0.07 for Biwi.The baseline (BL) was trained on the combinationof custom large pose expansions of 300W-LP, WFLWand LaPa with sampling probabilities 50%, 33% and16%, respectively. These frequencies were picked ad-hoc,guided by the size of the datasets, preliminary exper-iments, and the quality of the BFM fits. Optimizingthem is left as potential future work. Later on FaceSynthetics was added (BL + FS), using the frequencies50% 300W-LP, 33.3% WFLW, 8.4% LaPa, and 8.4%FS. The low amount of FS samples was motivated byits fully synthetic nature and incomplete labels.A perhaps notable aspect in this work is the consis-tent avoidance of stretching the input faces. For Biwiat least, it is common to extract and resize the areaunder the facial BB to the input size . Instead,consistent with training, the BB is first expanded to asquare (and enlarged by 10%).In the evaluation of AFLW2000-3D, the standardprotocol in was followed apart from the input crop,including the removal of 30 samples with yaw, pitch orroll angles larger than 99. shows a comparisonwith literature values. Evidently, the BL is already veryaccurate, yet adding FS, yielded further improvementfrom 3.19 to 3.15 MAE, improving over SOTA by overtwo sigmas.The benchmarking on Biwi, the results of which arepresented in Tab. 2, follows the experimental protocolfrom (apart from the crop), and uses exactly thesame frames and facial BBs as in . Evaluations withthe alignment strategy from were also conducted,compensating for different camera angles and otherbiases between coordinate systems. Without it, resultsare modest. With alignment, the accuracy improvesdrastically.However, results for 6DRepNet are notreadily available, and a re-evaluation was out-of-scope.Whether aligned results are representative of the true",
  "OpNet BL2.842.752.902.835.01OpNet BL + FS2.572.472.922.654.72": "visualizes the worst estimates judging byrotation error.It reveals failures in situations withheavy occlusion and a sample with two visible faces.It also shows apparently mislabeled samples where thepredictions look more plausible. shows results from various method ablations.Increasing the backbone capacity from MobileNet toResNet18 improved the metrics only insignificantly. Re-moval of landmark predictions yielded a small improve-ment in MAE over the BL. Hypothetically, synergeticeffects between tasks did not occur in the BL and nowmore capacity was freed for pose prediction. However,the landmark prediction was needed to utilize the anno-tations of the Face Synthetics data. The variant withoutlandmarks and ResNet backbone would have been verystrong if only geodesic distance was considered but theMAE metric suffered so drastically that it cannot beconsidered the best. The other modifications worsenedthe accuracy in both metrics. Interestingly, in-plane ro-tation augmentation had a big impact, where a smallerrotation range yielded intermediate results. . Ablation study with different variations of themethodology. Every line means a change from the baseline(OpNet BL). Changes are not cumulative with other lines.ResNet18 means the feature extractor was replaced withit. A minus means removal of the ingredient. \"IntensityAug.\" refers to the image intensity augmentations, \"NLL\"to the NLL losses, \"Landmarks\" to the landmark losses,\"In-plane Rot.\" to the respective rotation augmentation, and\"5 In-plane Rot.\" to overridden rotation limits.",
  "ResNet183.185.24- Landmarks3.165.26ResNet18 - Landmarks3.275.21- Intensity Aug.3.245.37- NLL3.305.35- In-plane Rot.3.535.655 In-plane Rot.3.445.56": "shows an ablation study for dataset varia-tions. Starting from modest results with 300W-LP, theaccuracy improves as more data is added. Interestingly,even the basic 300W-LP reproduction (R-300W-LP)improved performance. The reason is unknown, butit could be explained by a slightly different rotationdistribution or the 3D geometry in particular due tothe depth estimation. Adding directional lighting andclosed eyes yielded a further boost.Aside of benchmark outcomes, in practice theResNet18 variant produced subjectively noticeablysmoother predictions than the BL. This motivated anevaluation on noisy inputs, the results of which areshown in .And indeed, as the noise is in-creased, the gap between rotation errors widens to ca0.25 which might explain the subjective feeling. Onthe other hand this gap amounts to only 5% of the totalerror magnitude, so other aspects could play a role.",
  "BLRA-300W-LP + EX3.195.26BL + FSRA-300W-LP + EX + FS3.155.23": ". Plots the geodesic error of rotation predictions ver-sus the standard deviation of Gaussian noise added to in-put images. The evaluations are conducted over AFLW2000-3D modified by noise. The error bars show the standarderror of the sample mean over the five evaluation networks. demonstrates some degree of effectiveness ofthe uncertainty estimation. Note that the failure caseswith errors larger than 40 are correctly attributedwith correspondingly large uncertainty.Overall thecorrelation between pose error and uncertainty is ratherweak.Since the network is equipped with a landmark pre- .Correlation of the uncertainty estimate withrotation errors. The data points iterate over samples fromAFLW2000 and the five BL evaluation networks. Recallthat the uncertainty estimate rot is a covariance matrix.Plotted is its Frobenius norm to condense it to a singlenumber. diction head, it was also evaluated on the AFLW2000-3D sparse face alignment benchmark following the pro-tocol in . The metric thereof measures the distanceof the 68 3D-landmarks from ground truth labels us-ing the normalized mean absolute error of the x andy components (NME 2D), ignoring the depth coordi-nate.It is computed separately for three yaw bins,[0, 30), [30, 60) and [60, 90). Respective resultsare presented in Tab. 5 together with literature valuesfrom prior art. As can be seen, the accuracy is decentbut not up to current SOTA. However, OpNet was alsonot optimized for landmark prediction.",
  "OpNet BL2.803.544.433.59OpNet BL + FS2.753.484.413.55": "than before. While none of the ideas are new, we canrecognize the degree of their effectiveness.The results suggest that out-of-plane rotation syn-thesis from has not yet reached its limit, i.e. whenslightly improved and applied to a sufficiently large anddiverse data volume, significantly better performingmodels might be trained than with the original 300W-LP dataset. Adding a different flavor of synthetic data,namely the Face Synthetics can boost performance fur-ther, where potentially the different image style helps toovercome the domain gap to the real world. The extentof improvement is surprising since an otherwise mod-erately performing model is boosted to beyond SOTAwith quite some margin.On the other hand the approach in this work wasnot very effective for face alignment. The paper alsodoes not address the question of how prior HPE art(which performs better with the 300W-LP baseline)would benefit from the suggested training data. In thelatter regard, the paper is limited in scope. However,re-training models of prior art would be an interest-ing direction for future work as it could yield furtherimprovements for HPE and face alignment. Anotherdirection to pursue would be the acquisition of a syn-thetic dataset like Face Synthetics but with perfect 6DoF pose labels, abolishing the need to measure theerror indirectly via landmark predictions.As a cautionary tale, the fact that the ResNets lowernoise sensitivity only showed when modifying the testset, highlights the risk of \"overfitting\" methods to aparticular test set - in this case with only clean images.A merit of this work is also the uncertainty estimationwith its tangent-space Gaussian formulation which isstraight forward to implement, provided an accuracyboost when added to the model, and was effective atdetecting failure cases.On the practical side, both the MobileNet andResNet variants are accurate, efficient models suitablefor real-time applications.",
  "Network for Occlusion-Robust 3D Dense Face Align-ment. In CVPR, pages 45314540, 2023. 1, 6": "Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li,and Javier Romero. Learning a model of facial shapeand expression from 4D scans. ACM Transactions onGraphics, (Proc. SIGGRAPH Asia), 36(6):194:1194:17,2017. 4 Hai Liu, Shuai Fang, Zhaoli Zhang, Duantengchuan Li,Ke Lin, and Jiazhang Wang. MFDNet: CollaborativePoses Perception and Matrix Fisher Distribution forHead Pose Estimation. IEEE Trans. Multim., 24:24492460, 2022. 1, 6 Yinglu Liu, Hailin Shi, Hao Shen, Yue Si, XiaoboWang, and Tao Mei. A New Dataset and Boundary-Attention Semantic Segmentation for Face Parsing. InProc. AAAI Conference on Artificial Intelligence, pages1163711644, 2020. 4 Tetiana Martyniuk, Orest Kupyn, Yana Kurlyak, IgorKrashenyi, Jiri Matas, and Viktoriia Sharmanska.DAD-3DHeads: A Large-scale Dense, Accurate andDiverse Dataset for 3D Head Alignment from a SingleImage. In CVPR, pages 2091020920, 2022. 6",
  "D.A. Nix and A.S. Weigend. Estimating the mean andvariance of the target probability distribution. In Proc.International Conference on Neural Networks (ICNN),pages 5560 vol.1, 1994. 3": "Pascal Paysan, Reinhard Knothe, Brian Amberg, SamiRomdhani, and Thomas Vetter. A 3D Face Model forPose and Illumination Invariant Face Recognition. InIEEE International Conference on Advanced Video andSignal Based Surveillance, AVSS, pages 296301, 2009.2 Valentin Peretroukhin, Matthew Giamou, W. NicholasGreene, David M. Rosen, Jonathan Kelly, and NicholasRoy. A Smooth Representation of Belief over SO(3) forDeep Rotation Learning with Uncertainty. In Robotics:Science and Systems XVI, Virtual Event / Corvalis,Oregon, USA, 2020. 2 Ren Ranftl, Katrin Lasinger, David Hafner, KonradSchindler, and Vladlen Koltun. Towards Robust Monoc-ular Depth Estimation: Mixing Datasets for Zero-shotCross-dataset Transfer. IEEE Transactions on PatternAnalysis and Machine Intelligence (TPAMI), 2020. 4"
}