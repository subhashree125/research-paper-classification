{
  "Abstract": "Few-shot segmentation remains challenging due to thelimitations of its labeling information for unseen classes.Most previous approaches rely on extracting high-level fea-ture maps from the frozen visual encoder to compute thepixel-wise similarity as a key prior guidance for the de-coder. However, such a prior representation suffers fromcoarse granularity and poor generalization to new classessince these high-level feature maps have obvious categorybias. In this work, we propose to replace the visual priorrepresentation with the visual-text alignment capacity tocapture more reliable guidance and enhance the model gen-eralization. Specifically, we design two kinds of training-free prior information generation strategy that attempts toutilize the semantic alignment capability of the ContrastiveLanguage-Image Pre-training model (CLIP) to locate thetarget class. Besides, to acquire more accurate prior guid-ance, we build a high-order relationship of attention mapsand utilize it to refine the initial prior information. Experi-ments on both the PASCAL-5i and COCO-20i datasets showthat our method obtains a clearly substantial improvementand reaches the new state-of-the-art performance. The codeis available on the project website 1.",
  ". Introduction": "With the development of deep learning , semantic seg-mentation has made a great progress.Traditional semantic segmentation relies on intensive an-notations which is time-consuming and labour-intensive,once the segmentation model encounters samples with lim-ited labeled data, it cannot output accurate prediction, mak-ing it difficult to apply in practice.Few-shot segmenta-tion is proposed to address the above prob-lem, which aims to segment novel classes with a few anno-tated samples during inference. To achieve this, it dividesdata into a support set and a query set, the images in thequery set are segmented using the provided information in",
  "PASCAL-5i COCO-20i": ". Visualization of the different prior information generated by our proposed method. The left is sampled from PASCAL-5i andthe right is selected from COCO-20i . Each row from top to bottom represents the query image, initial visual-visual prior information,refined visual-visual prior information, initial visual-text prior information and refined visual-text prior information. The Pvv has moregeneral localization regions and the Pvt has more local target regions. With the refinement of the designed high-order matrix, more accurateprior information can be extracted. designed. As can be seen in , VTP yields a perfor-mance improvement of 4.3% and VVP yields a performanceimprovement of 1.1%.Ablation Study on PIR. In the PIR module we designeda high order matrix to maintain the structural information ofthe original features and used it to refine the initial prior in-formation, we conduct ablation experiments on the refine-ment ability of PIR as shown in . It can be foundthat the refinement of PIR using only the VTP informationis able to get an enhancement of 1.0%, but the refinementof PIR using only the VVP information as prior informationreduces model performance by 0.58%, this is due to the factthat refining VVP and VTP based on the same matrix willmake them produce a similar response, which will reducethe generalization of the prior guidance. When both the ini-tial VVP and refined VTP are used, the model is able toachieve the highest performance of 76.40%.",
  "arXiv:2405.08458v1 [cs.CV] 14 May 2024": "response due to original ImageNet pre-training weightsbeing insensitive to category information, which misleadsthe segmentation process and thus restricting generalizationof the model. 2) coarse prior mask shapes, caused by undis-tinguished vision features between the target and non-targetpixels, make the prior information locate many non-targetregions, which further confuses the segmentation process.To address the aforementioned drawbacks, we rethinkthe prior mask generation strategy and attempt to use Con-trastive Language-Image Pre-training (CLIP) to gener-ate more reliable prior information for few-shot segmenta-tion. A large amount of text-image training data pairs makethe CLIP model sensitive to category due to the forced text-image alignment, which enables better localization of thetarget class . Besides, the success in the zero-shot task also demonstrates the powerful general-ization ability of the CLIP model. Based on this, we attemptto utilize the CLIP model to generate better prior guidance.Finally, in this paper, we propose Prior Information Gen-eration with CLIP (PI-CLIP), a training-free CLIP-basedapproach, to extract prior information to guide the few-shot segmentation. Specifically, we propose two kinds ofprior information generation, the first one is called visual-text prior information (VTP) which aims to provide accu-rate prior location based on the strong visual-text alignmentability of the CLIP model, we re-design target and non-target prompts and force the model to perform category se-lection for each pixel, thus locating more accurate targetregions. The other one is called visual-visual prior informa-tion (VVP) which focuses on providing more general priorguidance using the matching map extracted from the CLIPmodel between the support set and the query image.However, as a training-free approach, the forced align-ment of visual information and text information makes VTPexcessively focus on local target regions instead of the ex-pected whole target regions, the incomplete original globalstructure information only highlights local target regionswhich reduces the quality of guidance. Based on this, webuild a high-order attention matrix based on the attentionmaps of the CLIP model, called Prior Information Refine-ment (PIR), to refine the initial VTP, which makes full useof the original pixel-pair structure relationship to highlightthe whole target area and reduce the response to the non-target area, thus clearly improving the quality of the priormask. Note that VVP is not refined to keep its generaliza-tion ability. Without any training, the generated prior masksovercome the drawback caused by inaccurate prior informa-tion in existing methods, significantly improving the perfor-mance of different few-shot approaches.Our contributions are as follows:",
  ". Few-Shot Segmentation": "Few-shot segmentation aims to generate dense predictionsfor new classes using a small number of labeled samples.Most existing few-shot segmentation methods followed theidea of metric-based meta-learning .Dependingon the object of the metric, current approaches can be di-vided into pixel-level matching mechanism andprototype-level matching mechanism .No matter pixel-level matching or prototype-level matchingmechanism, most recent approaches uti-lized prior information to guide the segmentation process.PCN fused the scores from base and novel clas-sifier to prevent base class bias.CWT adapted theclassifiers weights to each query image in an inductiveway. PFENet first proposed to utilize prior informa-tion extracted from pixel relationship between support setand query image to guide the decoder and designed a mod-ule to aggregate contextual information at different scales.PFENet++ rethinked the prior information and pro-posed to utilize the additional nearby semantic cues for abetter location ability of the prior information. BAM further optimized the prior information and proposed toleverage the segmentation of new classes by suppressingthe base classes learned by the model. SCL proposeda self-guided learning approach to mine the lost critical in-formation on the prototype and utilize the prior informa-tion as guidance for the decoder. IPMT mined use-ful information by interacting prototype and mask to mit-igate the category bias and design an intermediate proto-type to mine more accurate prior guidance by an iterativeapproach. MM-Former utilized a class-specific seg-menter to decompose the query image into a single possi-ble prediction and extracted support information as prior tomatching the single prediction which can improve the flexi-bility of the segmentation network. MIANet proposedto use general prior information from semantic word em-bedding and instance information to perform an accuratesegmentation. HDMNet mined pixel-level correlationwith transformer based on two kinds of prior informationbetween support set and query image to avoid overfitting.",
  "A-": ". Overview of our proposed PI-CLIP for few-shot segmentation. We design a group of text prompts for a certain class to attractmore attention to target regions. The VTP module generates the visual-text prior information by aligning the visual information and textinformation with the help of softmax-GradCAM. The VVP module generates the visual-visual prior information by a pixel-level similaritycalculation. The PIR module is proposed to refine the coarse initial prior information. Finally, the original prior information in the existingfew-shot model is directly replaced by VVP and refined VTP, after passing the decoder, the final prediction is generated.",
  ". Contrastive Language-Image Pretraining": "Contrastive Language-Image Pretraining (CLIP) isable to map text and image into high-dimensional spaceby text-encoder and image-encoder respectively. Trainedon a large amount of text-image data makes the CLIP model has a strong feature extraction capability,which is used in many downstream applications such asdetection , segmentation , and so on.CLIPSeg first attempted to introduce the CLIP modelinto few-shot segmentation. However, CLIPseg is more liketo use the CLIP model as a validation method to show thepowerful capability of the CLIP model in few-shot tasks.In this paper, we design a new prior information generationstrategy using the CLIP model for few-shot segmentationthrough the visual-text relationship and the visual-visual re-lationship to perform a more efficient guidance.",
  "Few-shot segmentation aims to segment novel classes byusing the model trained on base classes.Most existing": "few-shot segmentation approaches follow the meta-learningparadigm.The model is optimized with multiple meta-learning tasks in the training phase and evaluates the per-formance of the model in the testing phase.Given adataset D, dividing it into a training set Dtrain and a testset Dtest, there has no crossover between the class setCtrain in the training set and class set Ctest in the test set(Ctrain Ctest = ).The model is expected to trans-fer the knowledge in Dtrain with restricted labeled data tothe Dtest. Both training set Dtrain and test set Dtest arecomposed of support set S and query set Q, support set Scontains K samples S = {S1, S2, . . . , SK}, each Si con-tains an image-mask pair {Is, Ms} and query set Q con-tains N samples Q = {Q1, Q2, . . . , QN}, each Qi containsan image-mask pair {Iq, Mq}. During training, the few-shot model is optimized with training set Dtrain by epochswhere the model performs prediction for query image Iqwith the guidance of the support set S. During inference,the performance will be acquired with the test set Dtest,and the model is no longer optimized.",
  ". Method Overview": "In order to enhance the ability of prior information to lo-calize target categories as well as to produce more general-ized prior information, we propose to mine visual-text andvisual-visual information instead of purely visual feature similarity to guide the segmentation process. Besides, tofurther improve the quality of the prior information to getfiner-grained guidance, we design an attention map-basedhigh-order matrix to refine the initial prior information bypixel-pairs relationships, shows our framework of theone-shot case with the following steps:1. Given a support image and a query image with the tar-get class name, we first input the query image and sup-port image to the CLIP image encoder to generate corre-sponding visual support and query features. Meanwhile,the target class name is used to build two text prompts,i.e., target prompt and non-target prompt, which are theninput to the CLIP text encoder to generate two text em-beddings. 2. Then, two text embeddings and the query visual featuresare input to the visual-text prior (VTP) module to gener-ate the initial VTP information by enforcing a classifica-tion process for each pixel.",
  ". Visual-Text Prior Information Generation": "Few-shot Segmentation (FSS) remains one major challengethat an image might have more than one class, but the modelis required to segment only one class at each episode. Thischallenge means that once the prior information is unable toprovide the correct target region, e.g., a true target region isdog but the prior information provides a cat region, itwill confuse the FSS model to segment the true target pixels,especially for the untrained novel class. To correctly locatetarget regions, we utilize the visual-text alignment informa-tion from the CLIP model to produce a new prior infor-mation called VTP. We innovatively define a group of textprompts of the target class as a guidance to the model, inwhich the target (foreground) text prompts tf is defined asa photo of {target class} and the non-target (background)text prompts tb is a photo without {target class}.Based on the designed text prompts, a pixel-level clas-sification is performed for the query image so as to lo-cate the true target foreground regions. To force the modelto decide whether one pixel is the target or not, we usesoftmax-GradCAM to generate the prior informationusing the relationship between the visual and text features.Specifically, the designed target and non-target prompts, i.e., a photo of {target class} and a photo without {targetclass}, are sent to the CLIP text encoder to get the highdimensional text features, represented as F tf and F tb. Sup-pose the query image is Iq, after passing the CLIP visualencoder, the query features F vq Rd(hw+1), after remov-ing the class token in F vq , visual query feature Fq Rdhw",
  ". Visual-Visual Prior Information Generation": "We enforce VTP to make a classification for each pixelso that it can locate the correct region. However, we ob-serve that VTP tends to locate a discriminative local region,e.g., the head region of a dog rather than the whole re-gion. To overcome this drawback, we attempt to take advan-tage of the support information that is naturally present infew-shot segmentation and get region-larger and location-rougher prior information to give more generalized guid-ance to the model.We design VVP to mine more general target informationby performing matching on the visual-visual relationship between the support image feature and the query image fea-ture. Suppose the support image is Is, after passing throughthe CLIP image encoder, its high dimensional image featureis generated and the visual support feature is F vs Rdhw",
  ". Prior Information Refinement": "The above prior information is generated by the visual andtextual features extracted from the frozen CLIP weights. Asa training-free method, the representation of the prior infor-mation can not adaptively guide the model to perform an ef-ficient segmentation. To generate finer-grained prior infor-mation that focuses more target regions, we propose a PriorInformation Refinement (PIR) module to refine the initialprior information. PIR builds a high-order matrix based onthe attention map from the query image, which can accu-rately build the pixel-wise relationship and retain the origi-nal global structure information, thus efficiently capturingspatial information and details of semantics to refine theprior information. In this way, the refined prior informationpays more attention to the whole target regions and focusesless on non-target regions.Specifically, suppose Ai Rhwhw is the multi-headself-attention map generated from CLIP with the i-th block,",
  "i=nlAi,(9)": "where l and n are the block number of the vision trans-former in CLIP and l < n. Based on the average attentionmap, in order to eliminate as much as possible the influ-ence of the background region while preserving the intrinsicstructural information, we design a high-order refinementmatrix R R1hw follows:",
  "Pi = B R Pi, {i vt, vv},(11)": "where B is a box mask generated from the prior mask fol-lowing and represents the Hadamard product. Weexperimentally found that only refining the visual-text priorPvt is enough since the refinement matrix will make Pvt andPvv produce similar responses, which will damage the gen-eralization of the model. Therefore, we select the refinedtext-visual prior and initial visual-visual prior, i.e., Pvt andPvv, as the final prior information.Finally, we directly replace the prior information in ex-isting methods with the concatenation of our visual-visualprior information Pvv and refined visual-text prior informa-tion Pvt, to generate the final prediction.",
  ". Experiments": "Datasets and Evaluation Metrics.We utilize thePASCAL-5i and COCO-20i to evaluate the per-formance of our proposed method. PASCAL-5i is built onPASCAL VOC 2012 with the complement of SDS which is a classical computer vision dataset for segmen-tation tasks including 20 different object classes such aspeople, cars, cats, dogs, chairs, aeroplanes, etc. COCO-20i is built on MSCOCO consists of more than120,000 images from 80 categories and is a more challeng-ing dataset. To evaluate the performance of our proposedmethod, we adopt mean intersection-over-union (mIoU)and foreground-background IoU (FB-IoU) as the evaluationmetrics following previous works .",
  "ours-PI-CLIP (PFENet)resnet5036.142.337.337.738.440.445.639.938.641.1ours-PI-CLIP (HDMNet)resnet5049.365.755.856.356.856.466.255.958.059.1": "pixels and the CLIP pre-trained model is ViT-B-16 . ForCOCO-20i, setting higher resolution can get higher perfor-mance but with more computing cost, the temperature pa-rameter in VTP is set to 0.01 and the selected layer l inPIR is set to 8. For the 5-shot case, we directly concatenate5 VVP rather than using the average of them as the priorinformation. For fair comparisons, other settings like dataaugmentation technique, learning rate and optimizer, e.g.,all follow the corresponding baselines. All experiments arerun on NVIDIA V100 GPUs. With the help of the accurate visual-text prior informa-tion and the generalized visual-visual prior information, ourproposed PI-CLIP method can able to reach better perfor-mance quickly, so PI-CLIP is only trained for 30 epochson both PASCAL-5i and COCO-20i which needs less timethan any previous methods and the batch sizes are set to 4 on",
  ". Comparison with state-of-the-art": "Quantitative results. shows the performance ofour method and existing state-of-the-art methods for few-shot segmentation on PASCAL-5i, our approach greatly im-proves the performance of the model over the 1-shot taskcompared to different baselines and achieves new state-of-the-art performance, with mIoU increases of 5.9% forBAM and 7.4% for HDMNet . For the 5-shot seg-mentation task, our approach outperforms other approachesby a clear margin, with mIoU gain of 3.8% for BAM and 5.4% for HDMNet , respectively. Besides, we alsoexperimented by plugging our method into PFENet, adifferent baseline from BAM and HDMNet that",
  "PI-CLIP Baseline Query Support": ". Qualitative results of the proposed PI-CLIP and baseline (HDMNet ) approach under 1-shot setting. Each row from top tobottom represents the support images with ground-truth (GT) masks (green), query images with GT masks (blue), baseline results (red),and our results (yellow), respectively. does not use a base learner, it can be seen even without theinhibition of base classes by the base learner, our approachalso improves the mIou of 10.4% and 10.9% for 1-shot and5-shot tasks respectively. The performance improvement ofthe different baseline methods shows that our method is aplug-and-play module with high flexibility. The main rea-sons for our success with different approaches are the ac-curate localization of VTP and the strong generalization ofVVP.In , we compare the performance of our approachand others on COCO-20i dataset. Our approach also ex-hibits strong performance and achieves new state-of-the-artperformance. Specifically, our approach improves the base-line by 6.8% and 3.1% mIoU for 1-shot and 5-shot tasks.Qualitative results. In order to better show the effectof our proposed model on the existing methods, we visual-ize the results of the baseline and our proposed method in, it can be found that our method (yellow part) has amuch stronger target localization ability than the baseline(red part), and the bias on the base class is greatly reduced. shows the visualization of our proposed VTP andVVP to help understand the localization capabilities of VTPand the generalization capabilities of VVP. VTP focusesmore on the accurate target regions, which are localizedin a local region compared to the whole object. VVP, onthe other hand, focuses on larger regions of the target classthan VTP, but the details provided by VVP are tougher thanVTP. also shows that synchronous refining VVP andVTP information makes them similar which is harmful tothe generalization of the few-shot segmentation model.",
  ". Ablation Study": "We conduct a series of ablation studies to investigate theimpact of each module on the PASCAL-5i dataset usingHDMNet as the baseline.Ablation Study on VVP and VTP. The prior informa-tion has a large impact on the performance of the model,so we conduct relevant ablation studies to separately verifythe validity of the prior information for the two modules we",
  ". Conclusion": "In this paper, we rethink the prior information for few-shot segmentation and realize that CLIP is able to achievemore accurate localization of the target class without furthertraining. The proposed prior information generation withCLIP (PI-CLIP) can give more accurate and generalized prior information which facilitates the segmentation per-formance. Furthermore, we design two prior informationgeneration modules, one is VTP which aligns the semanticinformation from the visual modal and text modal to gener-ate accurate prior information, and the other is VVP whichperforms a matching on visual feature between support im-age and query image to mine more useful target informationand give a regionally larger prior information. To extractmore useful information, the PIR module is designed torefine the initial prior information. Extensive experimentsdemonstrate the effectiveness of our proposed module. Inthe future, we will explore how to better extract the usefulinformation from the CLIP model. Acknowledge:This work was supported by NationalNatural Science Foundation of China (No.62301613,62372468), the Taishan Scholar Program of Shandong(No. tsqn202306130), the Shandong Natural Science Foun-dation (No.ZR2023QF046, ZR2023MF008), the Ma-jor Basic Research Projects in Shandong Province (GrantNo.ZR2023ZD32), the Qingdao Natural Science Founda-tion (Grant No. 23-2-1-161-zyyd-jch), Qingdao Postdoc-toral Applied Research Project (No. QDBSH20230102091)and Independent Innovation Research Project of China Uni-versity of Petroleum (East China) (No. 22CX06060A). Hanbo Bi, Yingchao Feng, Zhiyuan Yan, Yongqiang Mao,Wenhui Diao, Hongqi Wang, and Xian Sun. Not just learningfrom others but relying on yourself: A new perspective onfew-shot segmentation in remote sensing. IEEE Transactionson Geoscience and Remote Sensing, 2023. 1 Jiacheng Chen, Bin-Bin Gao, Zongqing Lu, Jing-Hao Xue,Chengjie Wang, and Qingmin Liao. Apanet: adaptive pro-totypes alignment network for few-shot semantic segmenta-tion. arXiv preprint arXiv:2111.12263, 2021. 1",
  "Qi Fan, Wenjie Pei, Yu-Wing Tai, and Chi-Keung Tang. Self-support few-shot semantic segmentation. In European Con-ference on Computer Vision, pages 701719. Springer, 2022.1, 2, 6": "Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma, Xu-peng Miao, Xuming He, and Bin Cui. Calip: Zero-shot en-hancement of clip with parameter-free attention. In Proceed-ings of the AAAI Conference on Artificial Intelligence, pages746754, 2023. 2 Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev,Subhransu Maji, and Jitendra Malik. Semantic contours frominverse detectors. In 2011 international conference on com-puter vision, pages 991998. IEEE, 2011. 5",
  "Shuting He, Xudong Jiang, Wei Jiang, and Henghui Ding.Prototype adaption and projection for few-and zero-shot 3dpoint cloud semantic segmentation. IEEE Transactions onImage Processing, 2023. 1": "Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, andSeungryong Kim. Cost aggregation with 4d convolutionalswin transformer for few-shot segmentation. In ComputerVisionECCV 2022: 17th European Conference, Tel Aviv, Is-rael, October 2327, 2022, Proceedings, Part XXIX, pages108126. Springer, 2022. 1, 2 Tao Hu, Pengwan Yang, Chiliang Zhang, Gang Yu, YadongMu, and Cees GM Snoek.Attention-based multi-contextguiding for few-shot semantic segmentation.In Proceed-ings of the AAAI conference on artificial intelligence, pages84418448, 2019. 2 Kai Huang, Feigege Wang, Ye Xi, and Yutao Gao. Prototyp-ical kernel learning and open-set foreground perception forgeneralized few-shot semantic segmentation. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 1925619265, 2023. 1 Tianyu Huang, Bowen Dong, Yunhan Yang, XiaoshuiHuang, Rynson WH Lau, Wanli Ouyang, and WangmengZuo.Clip2point: Transfer clip to point cloud classifica-tion with image-depth pre-training. In Proceedings of theIEEE/CVF International Conference on Computer Vision,pages 2215722167, 2023. 3 Siyu Jiao, Yunchao Wei, Yaowei Wang, Yao Zhao, andHumphrey Shi. Learning mask-aware clip representationsfor zero-shot segmentation. Advances in Neural InformationProcessing Systems, 36:3563135653, 2023. 1 Chen Ju, Peisen Zhao, Siheng Chen, Ya Zhang, XiaoyunZhang, Yanfeng Wang, and Qi Tian. Adaptive mutual super-vision for weakly-supervised temporal action localization.IEEE Transactions on Multimedia, 2022. 3 Dahyun Kang and Minsu Cho. Integrative few-shot learn-ing for classification and segmentation. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 99799990, 2022. 6 Chunbo Lang, Gong Cheng, Binfei Tu, and Junwei Han.Learning what not to segment: A new perspective on few-shot segmentation. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition, pages80578067, 2022. 1, 2, 5, 6",
  "Chunbo Lang, Binfei Tu, Gong Cheng, and Junwei Han.Beyond the prototype: Divide-and-conquer proxies for few-shot segmentation. arXiv preprint arXiv:2204.09903, 2022.1": "Yuan-Hao Lee, Fu-En Yang, and Yu-Chiang Frank Wang. Apixel-level meta-learner for weakly supervised few-shot se-mantic segmentation. In Proceedings of the IEEE/CVF Win-ter Conference on Applications of Computer Vision, pages21702180, 2022. 1 Gen Li, Varun Jampani, Laura Sevilla-Lara, Deqing Sun,Jonghyun Kim, and Joongkyu Kim.Adaptive prototypelearning and allocation for few-shot segmentation. In Pro-ceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 83348343, 2021. 1, 2 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Dollar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InComputer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings,Part V 13, pages 740755. Springer, 2014. 5 Yuqi Lin, Minghao Chen, Wenxiao Wang, Boxi Wu, KeLi, Binbin Lin, Haifeng Liu, and Xiaofei He. Clip is alsoan efficient segmenter: A text-driven approach for weaklysupervised semantic segmentation.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1530515314, 2023. 2, 3, 4, 5",
  "ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1155311562, 2022. 1, 2": "Nian Liu, Kepan Nan, Wangbo Zhao, Yuanwei Liu, XiwenYao, Salman Khan, Hisham Cholakkal, Rao Muhammad An-wer, Junwei Han, and Fahad Shahbaz Khan. Multi-grainedtemporal prototype learning for few-shot video object seg-mentation. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 1886218871, 2023.1 Yuanwei Liu, Nian Liu, Qinglong Cao, Xiwen Yao, Jun-wei Han, and Ling Shao. Learning non-target knowledgefor few-shot semantic segmentation.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1157311582, 2022. 6 Yuanwei Liu, Nian Liu, Xiwen Yao, and Junwei Han. Inter-mediate prototype mining transformer for few-shot semanticsegmentation. Advances in Neural Information ProcessingSystems, 35:3802038031, 2022. 2, 6 Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fullyconvolutional networks for semantic segmentation. In Pro-ceedings of the IEEE conference on computer vision and pat-tern recognition, pages 34313440, 2015. 1 Zhihe Lu, Sen He, Xiatian Zhu, Li Zhang, Yi-Zhe Song,and Tao Xiang. Simpler is better: Few-shot semantic seg-mentation with classifier weight transformer. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 87418750, 2021. 2",
  "Timo Luddecke and Alexander Ecker.Image segmenta-tion using text and image prompts.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 70867096, 2022. 2, 3": "Xiaoliu Luo,Zhuotao Tian,Taiping Zhang,Bei Yu,Yuan Yan Tang, and Jiaya Jia. Pfenet++: Boosting few-shotsemantic segmentation with the noise-filtered context-awareprior mask. arXiv preprint arXiv:2109.13788, 2021. 2 Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza,Nasser Kehtarnavaz, and Demetri Terzopoulos. Image seg-mentation using deep learning: A survey. IEEE transactionson pattern analysis and machine intelligence, 44(7):35233542, 2021. 1 Seonghyeon Moon, Samuel S Sohn, Honglu Zhou, SejongYoon, Vladimir Pavlovic, Muhammad Haris Khan, and Mub-basir Kapadia. Msi: Maximize support-set information forfew-shot segmentation. In Proceedings of the IEEE/CVF In-ternational Conference on Computer Vision, pages 1926619276, 2023. 6",
  "Atsuro Okazawa. Interclass prototype relation for few-shotsegmentation. In European Conference on Computer Vision,pages 362378. Springer, 2022. 1": "Prashant Pandey, Aleti Vardhan, Mustafa Chasmai, TanujSur, and Brejesh Lall. Adversarially robust prototypical few-shot segmentation with neural-odes. In International Confer-ence on Medical Image Computing and Computer-AssistedIntervention, pages 7787. Springer, 2022. 1 Bohao Peng, Zhuotao Tian, Xiaoyang Wu, Chengyao Wang,Shu Liu, Jingyong Su, and Jiaya Jia. Hierarchical dense cor-relation distillation for few-shot segmentation. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 2364123651, 2023. 1, 2, 5, 6, 7 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 2, 3, 6",
  "Amirreza Shaban, Shray Bansal, Zhen Liu, Irfan Essa, andByron Boots. One-shot learning for semantic segmentation.arXiv preprint arXiv:1709.03410, 2017. 2, 5, 8": "Xinyu Shi, Dong Wei, Yu Zhang, Donghuan Lu, MunanNing, Jiashun Chen, Kai Ma, and Yefeng Zheng.Densecross-query-and-support attention weighted mask aggrega-tion for few-shot segmentation. In Computer VisionECCV2022: 17th European Conference, Tel Aviv, Israel, Octo-ber 2327, 2022, Proceedings, Part XX, pages 151168.Springer, 2022. 1, 2, 6 Chen Shuai, Meng Fanman, Zhang Runtong, Qiu Heqian, LiHongliang, Wu Qingbo, and Xu Linfeng. Visual and textualprior guided mask assemble for few-shot segmentation andbeyond. arXiv preprint arXiv:2308.07539, 2023. 3",
  "Haoliang Sun, Xiankai Lu, Haochen Wang, Yilong Yin,Xiantong Zhen, Cees GM Snoek, and Ling Shao.Atten-tional prototype inference for few-shot segmentation. Pat-tern Recognition, page 109726, 2023. 1": "Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HSTorr, and Timothy M Hospedales. Learning to compare: Re-lation network for few-shot learning. In Proceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 11991208, 2018. 2 Antonio Tavera, Fabio Cermelli, Carlo Masone, and Bar-bara Caputo.Pixel-by-pixel cross-domain alignment forfew-shot semantic segmentation.In Proceedings of theIEEE/CVF Winter Conference on Applications of ComputerVision, pages 16261635, 2022. 1",
  "Zhuotao Tian, Jiequan Cui, Li Jiang, Xiaojuan Qi, Xin Lai,Yixin Chen, Shu Liu, and Jiaya Jia.Learning context-aware classifier for semantic segmentation. arXiv preprintarXiv:2303.11633, 2023. 1": "Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou,and Jiashi Feng.Panet: Few-shot image semantic seg-mentation with prototype alignment.In proceedings ofthe IEEE/CVF international conference on computer vision,pages 91979206, 2019. 1 Yuan Wang, Rui Sun, and Tianzhu Zhang. Rethinking thecorrelation in few-shot segmentation: A buoys view. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 71837192, 2023. 1, 6 Xianghui Yang, Bairun Wang, Kaige Chen, Xinchi Zhou,Shuai Yi, Wanli Ouyang, and Luping Zhou. Brinet: Towardsbridging the intra-class and inter-class gaps in one-shot seg-mentation. arXiv preprint arXiv:2008.06226, 2020. 1, 2 Yong Yang, Qiong Chen, Yuan Feng, and Tianlin Huang.Mianet: Aggregating unbiased instance and general informa-tion for few-shot semantic segmentation. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 71317140, 2023. 2, 6",
  "Yuhuan Yang, Chaofan Ma, Chen Ju, Ya Zhang, and Yan-feng Wang. Multi-modal prototypes for open-set semanticsegmentation. arXiv preprint arXiv:2307.02003, 2023. 3": "Bingfeng Zhang, Jimin Xiao, and Terry Qin. Self-guidedand cross-guided learning for few-shot segmentation. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 83128321, 2021. 1, 2, 6 Chi Zhang, Guosheng Lin, Fayao Liu, Jiushuang Guo,Qingyao Wu, and Rui Yao. Pyramid graph networks withconnection attentions for region-based one-shot semanticsegmentation.In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pages 95879595,2019. 1 Gengwei Zhang, Shant Navasardyan, Ling Chen, Yao Zhao,Yunchao Wei, Honghui Shi, et al.Mask matching trans-former for few-shot segmentation. Advances in Neural In-formation Processing Systems, 35:823836, 2022. 2 Miao Zhang, Miaojing Shi, and Li Li. Mfnet: Multiclassfew-shot segmentation network with pixel-wise metric learn-ing. IEEE Transactions on Circuits and Systems for VideoTechnology, 32(12):85868598, 2022. 1 Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kun-chang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.Tip-adapter: Training-free adaption of clip for few-shot classifi-cation. In European Conference on Computer Vision, pages493510. Springer, 2022. 2 Xiaolin Zhang, Yunchao Wei, Zhao Li, Chenggang Yan, andYi Yang.Rich embedding features for one-shot semanticsegmentation. IEEE Transactions on Neural Networks andLearning Systems, 33(11):64846493, 2021. 1, 2"
}