{
  "Abstract": "Building and maintaining High-Definition (HD) mapsrepresents a large barrier to autonomous vehicle deploy-ment. This, along with advances in modern online map de-tection models, has sparked renewed interest in the onlinemapping problem. However, effectively predicting onlinemaps at a high enough quality to enable safe, driverless de-ployments remains a significant challenge. Recent work onthese models proposes training robust online mapping sys-tems using low quality map priors with synthetic perturba-tions in an attempt to simulate out-of-date HD map priors.In this paper, we investigate how models trained on thesesynthetically perturbed map priors generalize to perfor-mance on deployment-scale, real world map changes. Wepresent a large-scale experimental study to determine whichsynthetic perturbations are most useful in generalizing toreal world HD map changes, evaluated using multiple yearsof real-world autonomous driving data.We show thereis still a substantial sim2real gap between synthetic priorperturbations and observed real-world changes, which lim-its the utility of current prior-informed HD map predictionmodels.",
  ". Introduction": "Large scale autonomous driving has long been a mile-stone for the robotics community and has been pursued forwell over a decade now. Perception and mapping systemshave formed core components of self driving systems, andmobile robots more generally, enabling comprehension andunderstanding of the world around them . In thefield of mobile robotics, the goal of mapping systems at ahigh level is to predict semi-static geometry and affordancesin a scene, i.e. elements of the world that rarely change",
  "(d) Example Map After Change": ". A real-world map change from an autonomous vehi-cle dataset. In this paper we investigate which synthetic perturba-tions applied to a simulated prior map at training time best modela real prior map (left) for training a prior-informed online mappingmodel to produce the updated map (right), evaluated using a vastcollection of real-world changes gathered over multiple years ofautonomous vehicle operation. over time. These semi-static elements are traditionally en-coded with human labeled High Definition (HD) mapsbuilt atop a high resolution geometric world reconstructionutilizing many overlapping trajectories. Reducing this costwithin the context of self driving would increase scalability,cost effectiveness, and robust safety. However, accuratelyand consistently constructing online HD maps from sensordata at sufficiently long ranges to facilitate safe, fully au-tonomous driving has proven challenging (outside of high-ways and simple road scenarios which have limited topo-logical or geometric complexity).",
  "arXiv:2406.01961v2 [cs.RO] 5 Jun 2024": "Despite this, in the last few years, there has been a resur-gence of interest in this problem. Equipped with modernmachine learning techniques , architectures ,and open datasets , the field has begun to pivot to-wards this online mapping approach as performance onother previously out of reach perception tasks has becomemore compelling. While exciting, the challenge of predict-ing highly accurate HD maps at suitable ranges remains,and the majority of recently published research on onlinemapping degrades in prediction performance with rangefrom the vehicle.One response to this problem, and one explored by con-current work in , is integrating offline, potentially outof date or inaccurate HD Map priors into the prediction ofonline features. This is particularly compelling because lowquality, out of date HD maps or sparse, low resolution Stan-dard Definition (SD) maps are often available or cheaply la-beled, and maps of roads are generally slow to change overtime. The result would be a model that only rarely has toperform full online mapping, but most of the time is actingto clean up the discrepancies between the prior and reality.A major caveat is that real world examples of meaning-ful map changes are relatively rare, even from the point ofview of large, industrial deployments . To step aroundthis issue, MapEX instead proposes that one could usesynthetic mutations of labels to imitate these map changesover time. This allows one to generate a virtually limit-less number of map changes to train their model to fixthe prior, effectively training a change detection/map repairmodel rather than a full online mapping model. While thisalmost certainly would not be trained on the same noisedistribution as real changes in the world, one would hopethat sufficiently diverse perturbations of the prior map intraining would minimize the sim2real transfer gap by actingakin to domain randomization , where the real distribu-tion is in the support of the synthetic distribution appliedto the labels. This alternative problem formulation wherewe have access to a prior could, in theory, significantly sim-plify the problem for perception systems, and has empiri-cally been shown to outperform existing online-only mod-els . However, the broad applicability of such methodsis predicated on its ability to effectively transfer from syn-thetic offline perturbation to real world map change events.In this paper, we aim to answer the following two ques-tions: Does training prior-informed online mapping models onsynthetic prior mutations generalize to real worldmap change examples? Considering a broad range of prior noise models, how domixtures of prior noise models applied in training affectthe generalization performance of these online mappingmodels to real world map changes?In addition, we share details of how our model archi-",
  ". Birds Eye View Perception": "3D perception is a core problem in mobile robotics andcomputer vision more broadly. The long term trend of 3Dperception has been to leverage large, expressive backbones trained on very large image datasets, to feed high quality image representa-tions into various heads for specific tasks, such as imageclassification , object detection , semanticsegmentation , keypoint estimation , and more.This trend has only strengthened with the rise of very largemodels pretrained on largely unsupervised objectives withinternet scale data both within computer vision and out-side of it .One direction that has advanced considerably in the lastfew years is the representation space used for 3D percep-tion. Early 3D object detection approaches focused on acouple key approaches: one being detecting and tracking in2D image space and reprojecting model outputs to 3D spaceusing geometric information , and the other being earlyfusion through methods like . With the rise of the useof LiDAR in mobile robotics perception tasks, efforts weremade to develop better data representations and encodersfor 3D object detection with LiDAR . Be-cause of the natural 3D geometry of LiDAR data and the ap-proximately 2.5D worlds that mobile ground robots gener-ally perceive (i.e., generally much fewer detections coincid-ing along the z direction than in the x and y directions), theaforementioned publications cumulatively proposed layingout LiDAR information as features in a voxel grid repre-sented as a top down feature image. This topdown repre-sentation, known as the Birds Eye View (BEV) representa-tion, has attracted a large amount of attention in perceptionfor mobile robotics over the past few years. In particular, ahuge number of contributions have sought to develop BEVrepresentations of other sensors as well so that they can besimply merged into a single unified feature representation.For example, much of this research focus has been on de-veloping expressive BEV feature backbones utilizing bothcamera and LiDAR data , which then can beconsumed by downstream tasks through a relatively simple,single BEV image interface.",
  ". Online HD Map Construction": "One specific downstream task which has been explored ontop of these BEV models for the task of autonomous drivingis online mapping. Early efforts in online mapping primar-ily focused on doing semantic segmentation from the per-spective view of a camera , or as a semantic segmen- tation problem utilizing a BEV representation . How-ever, these methods struggle with two key problems: realworld roads often have complex topologies and instance-level traits which are difficult to represent accurately withsemantic segmentation, and many downstream behaviorplanning models (e.g. , ) consume vectorized maprepresentations. Indeed, part of the reason why HD mapsare generally labeled as vectorized features is that this re-moves much of the ambiguity regarding topology and over-lapping elements and affordances of roads. Thus, it wouldbe ideal to directly predict vectorized features. Earlier workidentified this requirement, and reframed the polyline pre-diction problem in a few different ways, i.e. autoregres-sive transformer approaches or classical, heuristic postprocessing of semantic segmentation . A recent line ofwork similarly reframed the problem again by mak-ing the connection that vectorized online mapping can berepresented as an unordered set detection problem, with asimilar problem structure as the one-to-one bijective objectdetection transformer described in , and reported com-pelling empirical performance with this approach. This hassparked a flurry of renewed interest in the online mappingproblem . The work most related to ours is theconcurrent work of MapEX which proposes to add lowquality HD map priors as inputs to MapTR to improvedetection performance despite occlusion or changes in theunderlying map, attempting to solve the well known mapchange problem. This provides an alternative avenue forsolving the change detection problem: rather than main-taining an up to date map, one can instead train a modelto leverage both sensor data and out of date HD map datato reconstruct an accurate, live representation of the worldaround it, enabling lifelong deployment of mobile robotswith significantly lower HD map maintenance expense.",
  ". Map Detection Head": "To predict vectorized map features, we follow in the foot-steps of MapTR with a Deformable DETR based polyline detection model, with one query per controlpoint where this query is the sum of a per polyline embed-ding and a per point embedding as in . In the follow-ing sections, we will primarily describe places where ourmethodology diverges from existing literature to aid in re-producibility of our results.",
  ". Single Stage Bipartite Matching Loss": "We forgo the two-stage hierarchical matching loss fromMapTR and instead utilize a simpler one stage bi-partite loss by computing a pairwise loss matrix beforecomputing the bipartite matching itself.To do this, wefirst compute a set of loss tensors to model the positional error and the permutation symmetries introduced in .We will use an indexed matrix notation to define the con-structed tensors used to compute the losses, where x Rmprednpointspdim is the prediction from the decodertransformer, where mpred is the maximum number of pre-dictions, npoints is the number of control points in everypolyline, and pdim is the dimensionality of each controlpoint.Similarly, x Rmgtnpointspdim is the groundtruth polyline tensor padded with no-object classes to al-ways have mgt objects to match to, where mgt is the maxnumber of ground truth labels for a given frame.Sincempred is fixed, we set mpred = mgt. Following this, weconstruct our loss matrices in the general form",
  "Invariance Class(j) = c0otherwise(3)": "is a masking function which sets the jth ground truth la-bel loss row to all 0 if the ground truth loss is not ofthat invariance class. As in , the valid permutationsof polygons represent the set of all shift permutation mapswith the polyline indexed in both directions (clockwise andcounter-clockwise).Similarly, the valid set of permuta-tions for undirected polylines is only swapping directionsof the polyline and is simply the identity permutation fordirected polylines.Using an actual permutation matrixwould be computationally expensive, but these can be im-plemented inexpensively using tf.roll, tf.reverse,and tf.tile operations in Tensorflow or similar op-erations available in most any comparable differentiable ar-ray computation library . Note that we optionallyalso add a scaled pairwise cosine similarity loss from to this as well, though our experience suggests that weight-ing cosine similarity much lower than the pointwise posi-tional loss helps with convergence when using this singlestep training objective.With these 3 loss matrices, we can construct a singlestage point2point matrix loss as:",
  ". Overall architecture of our model, heavily influenced by . See these references for more details on implementation": "undirected polylines, directed polylines, and polygons re-spectively. Note that the resulting per batch polyline local-ization error matrix includes the pairwise positional errorbetween every ground truth and predicted polyline, whereeach pairwise positional error is computed with the lossminimizing prediction permutation which is valid for agiven pairs label. This matrix is identically 0 for any no-object ground truth class row, and correctly masks invalidpermutations from matching with any given label as definedby the labels invariance class. We can then utilize a pair-wise focal classification loss matrix Lfocal of a similarconstruction and sum them into a single loss matrix",
  "Lpairwise(x, x) = wcLfocal(x, x)+wpLpoint2point(x, x)": "(5)where wc, wp R are weighting terms to each respectiveloss type. With this full pairwise loss matrix, we can solvefor the minimum loss label/prediction assignment using theHungarian Algorithm and directly sum up the resultingoptimally matched losses rather than doing a hierarchicalmatching as in . This should result in a similar finalcost function, but performing Hungarian matching directlyon the final losses ensures there is no divergence of ob-jectives between point-wise and polyline-wise convergenceand simplifies training code.",
  ". Map Tokenizer and Prior Integration": "To incorporate an HD map prior, we use a map tokenizersimilar to . The map tokenizer is a lightweight learnedmodule that converts an unordered set of polylines into to-kens, the language of transformers (see ). We want these tokens to encode as much useful information from theprior as possible, including both point-level and polyline-level information. We take inspiration from the idea of hi-erarchical queries in MapTR , and we set the tokensequal to the sum of a point token and an aggregated poly-line token derived from the point tokens. The point tokensare generated using a Multi-Layer Perceptron (MLP) overthe point coordinates, where this MLP is shared across allpoints. The polyline tokens are generated by max poolingover point tokens, concatenating this with a one-hot classvector, and passing through another MLP which is sharedacross all polylines. This weight-sharing scheme preservespermutation invariance among polylines, and the max pool-ing is a lightweight way to aggregate information. One important case to handle is when mprior < mpred,where mprior is the number of prior polylines, which is al-most always the case. If we just naively add padding to theend of the prior up to mpred, then some of the positionalencodings in the transformer decoder will always be associ-ated with a prior while some will almost never be associatedwith a prior, which could cause undesirable biases. To mit-igate this, we shuffle the prior tokens after adding paddingso that padding is effectively inserted randomly. Once we have converted the prior into tokens, we stillneed a way to consume these tokens. One approach is toadd an extra cross-attention step to the decoder layer that at-tends to these tokens. In this formulation, the prior is simplyanother modality to attend to, in addition to the BEV em-bedding. However, we found that this approach failed ba-sic overfitting experiments. We suspect that cross-attention does not provide enough bandwidth for the model to incor-porate the prior as strongly as it should, especially with alimited number of decoder layers.Another approach is to directly replace the fixed hierar-chical queries in MapTR with these prior tokens. Thisformulation has a nice intuitive interpretationthe prior to-kens provide the initial estimates, which are then refinedthrough several decoder layers by attending to the BEV em-bedding to come up with a posterior. We found that, whentested against synthetic map perturbations, this approachworks better and has more stable training, which is con-sistent with the approach in , and thus is the approachused in all of our results.",
  ". Types of Perturbations": "We implement a number of synthetic map prior perturbationtypes, expanding on the experiments from . These canbe roughly classified as discrete mutations which change thenumber or types of polyline features, or continuous, warp-ing mutations which change the position or shape of the fea-tures (see ). Ultimately the goal of all these mutationsis to prevent the trained model from simply passing throughthe vectorized polyline prior features while completely ig-noring sensor observations.",
  "Discrete Mutations": "The primary goal of discrete mutations is to capture mapchanges to a scene which cannot be fully characterized sim-ply by warping the geometries of the underlying features,and are more similar to a discete Bernoulli distribution ofsomething about a map feature that has or hasnt changed,such as the class of a feature or the number of features in ascene. Feature dropout (b) We use a full feature dropoutmutation to model large scene changes or incompletelabels.We drop out each feature in the scene with aBernoulli distribution with equal probability across eachfeature. Feature duplication (c) We use a feature dupli-cation mutation to model accidental label duplication, aswell as to model large scene changes when mixed withwarping perturbations which will warp each duplicatedfeature in a different way. This mutation works by againusing a Bernoulli trial for each existing polyline feature,and truncating the newly added features to a maximum ofmpred features. Wrong class (d) We use a wrong class muta-tion which, again by Bernoulli trial, mutates each poly-lines class with some probability to a random other class.This has similar goals as previous mutations of decreas-ing model reliance on the prior, but is the only mutationwhich corrupts class information, modeling mislabeled",
  "Continuous, Warping Mutations": "To complement our discrete mutations of category and car-dinality, we also introduce a number of continuous, geome-try warping mutations. Note that each of these are parame-terized by a standard deviation which scales the variance ofthe resulting noise. Control point perturbation (e) This is a simpleper-control-point zero-mean Gaussian shift to help ensurethe model cannot simply pass through the prior as an iden-tity function. Feature location perturbation (f) We hypothe-size that the model may relatively easily overcome con-trol point perturbation by simply smoothing out the priorrather than attending to sensor data, so we also use a zero-mean Gaussian per-feature location perturbation. Global rotation and shift (g) We apply Gaus-sian perturbations to global yaw and position to simulaterobot localization errors. We also hypothesize that evenper-feature location perturbation may be relatively easilyovercome by the model through self-attention without at-tending to sensor data, but this localization mutation canonly be inverted by leveraging sensor data. Perlin warp (h) We generate two 2D Perlin noise images utilizing fractional Brownian motion ,one each corresponding to warping x and y coordinates,and then renormalize the resulting noise distribution to bezero mean. We then sample from this image at the coor-dinates of the control points of the polylines to get struc-tured noise. We hypothesize that such correlated warp-ing will make it harder for the model to learn to sim-ply denoise the zero-mean noise added to individual fea-tures, better simulating how curbs, lanes, and lane bound-aries may all be moved together during a major, real mapchange event.",
  ". Real-World Change Examples": "To generate a large set of real world map change examples,we leverage an internal map database to generate a regionchange diff between a 2020 version of our internal HD map,and that same map from 2023. We only include changes tothe map significant enough that they required a recollect ofdata in the area and subsequent changes to the HD semanticmap. We then mine for 30 second scenes collected in thesecond half of 2023 which intersect these regions of changeand occur after all computed map changes. To dump datafor these scenes, we obtain the prior from the 2020 map ver-sion and the ground truth labels from the map version at data",
  ". Experiment Setup": "We perform all experiments with a BEV backbone similarto with pre-trained LiDAR and camera BEV featureextractors which are trained for general perception tasks.We train each model for 75K steps on 32x Nvidia A100son a large internal dataset collected from Houston, TX andMountain View, CA (>13.7K scenes, >822K unique train-ing frames), with a 90m square field of view centered on thevehicle and four different feature types: lane centers, lanedividers, road boundaries, and driveways. For evaluation,we utilize two major types of datasets, a synthetic evalua-tion set and a real world change dataset. First, for the syn-thetic evaluation set, we use a geographically split holdouttest dataset (>3.3K scenes, 198K unique test frames) withlow levels of synthetic prior noise utilizing all presentedmutations (0.1m Std. Dev. for continuous warping muta-tions, and 0.1 probability for discrete mutations). Then, forreal world change data, we mine a holdout test set (1240scenes, 74k unique test frames) of scenes which have under-gone real world HD map change, and provide an outdatedmap prior from multiple years of aggregated map changesin the geospatial map database as described in Sec. 4.1.The value of utilizing our internal dataset is multifold. For one, it significantly reduces the risk of geospatial over-fitting of map detection transformers by holding outlarge geospatial regions for eval exclusively during thetraining and evaluation split.In addition, our dataset issignificantly larger than the majority of open datasets (e.g.), and is thus able to leverage the scaling behavior ofmap prediction models identified in to further mitigatethe noise caused by overfitting. Finally, we provide realworld pairs of outdated and up to date maps, something notavailable in as pointed out in . As for metrics, wecompute mean Average Precision (mAP) of predicted poly-lines using the same Chamfer distance based metrics andthresholds used in .",
  ". Experimental Design": "Due to the combinatorial hyperparameter space induced byso many different parameters, we first train a baseline modelwith a small amount of noise for each prior mutation, thentune each mutation parameter independently. Qualitativeresults for the baseline low noise model are shown in .For the parameter search (Tab. 1, Tab. 2), we start with allmutations except Perlin warp enabled with a low noise level(Low All Noise in the table), where this low noise levelis identical to the synthetic perturbation distribution (0.1mStd. Dev for continuous mutations, 0.1 probability of dis-crete mutations). We then test a number of increased levelsof noise for their performance against the synthetic evalua-tion dataset as well as the real world evaluation dataset.",
  "(p) Ground Truth": ". Qualitative results handling real world changes. For minor real-world changes, e.g. driveway geometry (ad) and curb geometry(eh), a model trained with prior perturbations correctly predicts changes to many of the real-world features. However, for substantialchanges in road layout, e.g. additional medians (il) and new road construction (mp), the model fails to meaningfully deviate from theprior to account for the new intersection geometry. Note that the topdown map is centered on the vehicle in all figures.",
  ". Discussion": "We note a number of interesting observations from our ex-perimental results. First, consistent with , we note thattraining with no prior noise at all results in a pass-throughmodel which learns to replicate the prior map without mod-ifications. Since changed map features usually comprise asmall percentage of the features in any given frame of data,we see that this pass-through model which is ignoring thesensor BEV information is capable of achieving a 0.824mAP on the real world map changes regardless.More substantially, we note a consistent sim2real gap be- tween the simulated, low noise evaluation prior and the realworld map change dataset, where the model has learnedto effectively denoise the evaluation prior but is not suffi-ciently general to smoothly transfer to real change detec-tion.Error is correlated between the simulated and realprior corruption, but our simulated evaluation is insufficientto model the complexities of real world changes. The quali-tative results in reinforce this observation, where onlythe simplest real-world changes are accurately predicted bythe model, which reverts to the prior when the changes be-come too complex.Somewhat surprising is that increasing prior dropoutnoise primarily serves to degrade model performance onreal world map changes as its noise is increased, ratherthan cleanly trading off between real world sensor and priorcontributions. Instead, we see that performance slowly de-grades, until a bifurcation in response behavior when hav-ing too degraded of a prior causes the model to performeven worse than it does when trained with no prior and thentested with prior (Drop Features, p=1.0), which is com-pletely out of distribution. Similarly interesting is that in-creased likelihood of feature duplication is somewhat morehelpful than any other discrete mutation, with performanceincreasing with increased mutation likelihood for the valueswe tested on.We see a slightly different story with continuous warp-ing mutations, which all improve on real world performancewith an increased level of noise from the baseline low noiselevel. Past that, however, they also see degradation behav-ior as perturbations get more exaggerated at higher noiselevels, similar to that of the discrete features. This is likelydue to the true distribution of map changes having a similarlevel of average displacement, where higher noise levels areunreasonable in nominal real world change scenarios (e.g.redoing a curb or driveway).",
  ". Conclusions": "Robustness to real-world changes is critical for any map-based autonomous vehicle system. We confirm the conclu-sions of in that prior maps are much better than no priorand that we need some noise in the prior to learn somethingmore useful than a pass-through function.However, we are able to expand on those results by ob-serving that too corrupted or weak of a prior can actuallyharm performance of the model more than omitting the priorentirely. Most importantly, we show that there exists a con-siderable sim2real gap between real world change detectionperformance and performance on simulated prior noise. Weobserve through large-scale experiments that prior muta-tions are sufficient to capture only the simplest of real-worldchanges. We hope the results presented in this paper moti-vate future work in this area to address the sim2real gap forHD map prediction with prior. Martn Abadi, Paul Barham, Jianmin Chen, ZhifengChen, Andy Davis, Jeffrey Dean, Matthieu Devin,Sanjay Ghemawat, Geoffrey Irving, Michael Isard,Manjunath Kudlur, Josh Levenberg, Rajat Monga,Sherry Moore, Derek G. Murray, Benoit Steiner, PaulTucker, Vijay Vasudevan, Pete Warden, Martin Wicke,Yuan Yu, and Xiaoqiang Zheng. TensorFlow: A sys-tem for large-scale machine learning. In Proceedingsof the 12th USENIX Symposium on Operating SystemsDesign and Implementation, 2016. arXiv:1605.08695[cs]. 3",
  "Vijay Badrinarayanan, Alex Kendall, and RobertoCipolla.SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.InCVPR, 2015. arXiv:1511.00561 [cs]. 2": "JamesBradbury,RoyFrostig,PeterHawkins,MatthewJamesJohnson,ChrisLeary,DougalMaclaurin, George Necula, Adam Paszke, Jake Van-derPlas, Skye Wanderman-Milne, and Qiao Zhang.JAX: composable transformations of Python+NumPyprograms, 2018. 3 Tom B. Brown,Benjamin Mann,Nick Ryder,Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry,Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, RewonChild, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen, EricSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei.Language Models are Few-Shot Learners. In NeurIPS,2020. arXiv:2005.14165 [cs]. 2 Holger Caesar, Varun Bankiti, Alex H. Lang, SourabhVora, Venice Erin Liong, Qiang Xu, Anush Krish-nan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.nuScenes: A multimodal dataset for autonomous driv-ing, 2020. arXiv:1903.11027 [cs, stat]. 2",
  "Qi Li, Yue Wang, Yilun Wang, and Hang Zhao.HDMapNet:An Online HD Map ConstructionandEvaluationFramework.InICRA,2022.arXiv:2107.06307 [cs]. 3": "Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie,Chonghao Sima, Tong Lu, Qiao Yu, and JifengDai.BEVFormer: Learning Birds-Eye-View Rep-resentation from Multi-Camera Images via Spa-tiotemporal Transformers.In ECCV. arXiv, 2022.arXiv:2203.17270 [cs]. 2 Bencheng Liao, Shaoyu Chen, Xinggang Wang, Tian-heng Cheng, Qian Zhang, Wenyu Liu, and ChangHuang. MapTR: Structured Modeling and Learningfor Online Vectorized HD Map Construction. In ICLR,2023. arXiv:2208.14437 [cs]. 3, 4, 5, 6 Bencheng Liao, Shaoyu Chen, Yunchi Zhang, BoJiang, Qian Zhang, Wenyu Liu, Chang Huang, andXinggang Wang. MapTRv2: An End-to-End Frame-work for Online Vectorized HD Map Construction,2023. arXiv:2308.05736 [cs]. 3",
  "Yicheng Liu, Tianyuan Yuan, Yue Wang, YilunWang, and Hang Zhao.VectorMapNet:End-to-end Vectorized HD Map Learning. In ICML, 2023.arXiv:2206.08920 [cs]. 3": "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,Zheng Zhang, Stephen Lin, and Baining Guo. SwinTransformer: Hierarchical Vision Transformer usingShifted Windows. In ICCV, 2021. arXiv:2103.14030[cs]. 2 Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, ChristophFeichtenhofer, Trevor Darrell, and Saining Xie.AConvNet for the 2020s.In Proceedings of theIEEE/CVF conference on computer vision and patternrecognition. arXiv, 2022. arXiv:2201.03545 [cs]. Zhijian Liu, Haotian Tang, Alexander Amini, XinyuYang, Huizi Mao, Daniela Rus, and Song Han. BEV-Fusion: Multi-Task Multi-Sensor Fusion with Uni-fied Birds-Eye View Representation.In IEEE In-ternational Conference on Robotics and Automation(ICRA). arXiv, 2022. arXiv:2205.13542 [cs]. 2, 6",
  "Yu Zhang, Jonathon Shlens, Zhifeng Chen, andDragomir Anguelov. Scalability in Perception for Au-tonomous Driving: Waymo Open Dataset. In CVPR,2020. arXiv:1912.04838 [cs, stat]. 2": "Remy Sun, Li Yang, Diane Lingrand, and FredericPrecioso. Mind the map! Accounting for existing mapinformation when estimating online HDMaps fromsensor data, 2023. arXiv:2311.10517 [cs]. 2, 3, 4,5, 6, 8 Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider,Wojciech Zaremba, and Pieter Abbeel. Domain Ran-domization for Transferring Deep Neural Networksfrom Simulation to the Real World. In IEEE/RSJ In-ternational Conference on Intelligent Robots and Sys-tems (IROS). arXiv, 2017. arXiv:1703.06907 [cs]. 2 Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin.Attention is All youNeed. In Advances in Neural Information ProcessingSystems. Curran Associates, Inc., 2017. 2",
  "Sourabh Vora, Alex H. Lang, Bassam Helou, and Os-car Beijbom. PointPainting: Sequential Fusion for 3DObject Detection. In CVPR, 2020. arXiv:1911.10150[cs, eess, stat]. 2": "Benjamin Wilson, William Qi, Tanmay Agarwal, JohnLambert, Jagjeet Singh, Siddhesh Khandelwal, BowenPan, Ratnesh Kumar, Andrew Hartnett, Jhony Kaese-model Pontes, Deva Ramanan, Peter Carr, and JamesHays. Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting. In Proceedingsof the Neural Information Processing Systems Trackon Datasets and Benchmarks (NeurIPS Datasets andBenchmarks 2021). arXiv, 2023.arXiv:2301.00493[cs]. 2"
}