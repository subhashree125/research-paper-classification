{
  "Diffusion Generalist": "Prompt: Perform Depth estimation Prompt: Perform instance segmentation Prompt: Perform Image restoration deraining Prompt: Perform semantic segmentation Denoising ProcessDenoising Process step=0step=25step=50 Denoising ProcessDenoising Process step=0step=25step=50 . We present a diffusion-based vision generalist for dense vision tasks. Given an input image, the model performs the correspondingtask following the text instruction. We showcase the effectiveness of our model on depth estimation, semantic segmentation, panopticsegmentation, and three types of image restoration tasks. The images are the actual output of our model.",
  "Abstract": "Building generalized models that can solve many com-puter vision tasks simultaneously is an intriguing direc-tion. Recent works have shown image itself can be usedas a natural interface for general-purpose visual percep-tion and demonstrated inspiring results. In this paper, weexplore diffusion-based vision generalists, where we unifydifferent types of dense prediction tasks as conditional im-age generation and re-purpose pre-trained diffusion modelsfor it. However, directly applying off-the-shelf latent diffu-sion models leads to a quantization issue. Thus, we pro-pose to perform diffusion in pixel space and provide a recipefor finetuning pre-trained text-to-image diffusion models fordense vision tasks. In experiments, we evaluate our methodon four different types of tasks and show competitive per-formance to the other vision generalists.",
  ". Introduction": "The field of artificial intelligence has made significantprogress in building generalized model frameworks.Inparticular, autoregressive transformers have become aprominent unified approach in Natural Language Process-ing (NLP), effectively addressing a wide range of tasks witha singular model architecture . However,in computer vision (CV), building a unified framework re-mains challenging due to the inherent diversity of the tasksand output formats.Consequently, state-of-the-art com-puter vision models still have many complex task-specificdesigns , making it difficult for feature shar-ing across tasks and, thus, limiting knowledge transfer.The stark contrast between NLP and CV has given rise toa growing interest in developing unified approaches for vi-sion tasks . Recently, haveshown image itself can be used as a robust interface forunifying different vision tasks and demonstrated good per-formance. In this paper, we propose a multi-task diffusion",
  "arXiv:2407.00503v1 [cs.CV] 29 Jun 2024": "generalist for dense vision tasks by reformulating the denseprediction tasks as conditional image generation, and re-purpose pre-trained latent diffusion models for it. visualizes the output of our model on semantic segmenta-tion, panoptic segmentation, depth estimation, and imagerestoration.Based on text prompts, our model can per-form different tasks with one set of parameters. However,directly finetuning the pre-trained latent diffusion models(e.g. Stable Diffusion ) leads to quantization errors forsegmentation tasks (see ). To this end, we proposeto do pixel-space diffusion which effectively improves thegeneration quality and does not suffer from quantization er-rors. Moreover, our exploration into training diffusion mod-els as vision generalists reveals a list of interesting findingsas follows: Diffusion-based generalists show superior performanceover the non-diffusion-based generalists on tasks involv-ing semantics or global understanding of the scene.",
  ". Related Work": "Unified framework & Unified model: Efforts have beenmade to unify various vision tasks with a single model, re-sulting in several vision generalists . In-spired by the success of sequence-to-sequence modeling inNatural Language Processing (NLP), Pix2Seq lever-ages a plain autoregressive transformer and tackles many vi-sion tasks with next-token prediction. For example, bound-ing boxes in object detection are cast as sequences of dis-crete tokens, and masks in semantic segmentation are en-coded with coordinates of object polygons . The idea wasfurther developed in Unified-IO , where dense predic-tion such as segmentation, depth map, and image restora-tion are also unified as tokens by using the correspond-ing image features from a vector quantization variationalauto-encoder (VQ-VAE) . On the output side, the pre-dicted image tokens are then decoded into masks and depthmaps as the final prediction. Similarly, OFA unifieda diverse set of cross-modal and unimodal tasks in a sim-ple sequence-to-sequence learning framework and achievedcompetitive performance pretrained with only 20M publicly available image-text pairs. Painter and SegGPT ,on the other hand, reformulate different vision tasks as animage inpainting problem, and perform in-context learningfollowing . Unlike the previous work, our method unifiesdifferent vision tasks under a conditional image generationframework and introduces a diffusion-based vision general-ist for it.Unified framework & Task-specific model:Besidesthe aforementioned literature, there is another line of re-lated works that pursue unified architecture but task-specificmodels. UViM addressed the high-dimensionality out-put space of vision tasks via learned guiding code, where ashort sequence modeled by an additional language modelto encode task-specific information guides the predictionof the base model. Separate models are trained for differ-ent tasks as the guiding code is task-specific. XDecoder unified pixel-level image segmentation, image-level re-trieval, and vision-language tasks with a generic decodingprocedure, which predicts pixel-level masks and token-levelsemantics, and different combinations of the two outputs areused for different tasks. Despite their good performance,the task/modality-specific customization poses difficulty forknowledge sharing among different tasks and is also notfriendly for supporting unseen tasks.",
  ". Unification with Conditional Image Generation": "As the output of most vision tasks can be always visualizedas images, we redefine the output space of different visiontasks as RGB images and unify them as conditional imagegeneration to tackle the inherent difference of output for-mats of different vision tasks. Given a input image x andthe corresponding ground-truth y, we first transform y intoRGB images and then pair it with a task descriptor in text.By doing so, training sets of different tasks are combinedinto a holistic training set. And training the model jointlyon it enables the knowledge transfer between tasks. At testtime, given a new image, the model can perform differenttasks following the text instructions (examples in ).In this paper, we consider four types of dense predictiontasks: depth estimation, semantic segmentation, panopticsegmentation, and image restoration.Depth estimation outputs real number depth value for eachpixel on x. Given the minimum and the maximum values,we map them into linearly and discretize them intointegers, which is then repeated and stacked along the chan-nel to form the ground-truth RGB label.Semantic segmentation predicts a class label for eachpixel. We use a pre-defined injective class-to-color map-ping to transform the segmentation mask into RGB im-ages. Given a task with C categories, we define C col-ors which are evenly distributed in the 3-dimensional RGB Perform semantic segmentation.Perform panoptic segmentation.Perform depth estimation. Input: Target: Instruction: semantic segmentationpanoptic segmentationdepth estimation Denoising UNet",
  "Diffusion GeneralistData Reformation": "concat. . The training pipeline of the diffusion-based vision generalist consists of two parts: Left: Redefining the output space of differentvision tasks as RGB images so that they can be unified under a conditional image generation framework. Right: We finetune a pre-traineddiffusion model on the reformatted data from the first step. Diffusion is performed in the pixel space to mitigate the quantization error ofthe latent diffusion (see ). The image and text conditionings are fed into the model via the corresponding encoders, where only theimage encoder is tuned during the training.",
  "b2 )m,int( i": "b)%b m, l%b m]. At test time, we find the nearestneighbor of the predicted color in the predefined class-to-color mapping and predict the corresponding category.Panoptic segmentation is solved as a combination of se-mantic and instance segmentation. Semantic segmentationlabels are constructed as stated above. For instance seg-mentation, we set N as the maximum number of instancesa single training image can contain. Then, we define N col-ors which are evenly distributed in the 3-dimensional RGBspace as in semantic segmentation. Finally, we assign col-ors to objects based on their spatial location to form theRGB ground-truth label. For example, the instance whosecenter is at the upper leftmost corner obtains the first colorand the lower rightmost gets the last color. At test time, themodel makes predictions twice with different text instruc-tions and merge the results for panoptic segmentation.Image restoration aims to predict the clean image fromcorrupted images. Thus, the output space is inherently RGBimage and does not need further transformation to fit in theframework.",
  ". A Diffusion Multi-Task Generalist Framework": "By reformating the output space of different vision tasksinto images, it is natural to solve them together under aconditional image generation framework. To this end, weleverage the powerful diffusion models pre-trained for im-age generation and re-purpose them in our use case. shows the overall pipeline of the method, which isa conditional image generation framework with pixel-spacediffusion. Given M tasks with datasets {Ii, Yi}Mi=1, whereIi are the input images of task i and Yi are the correspond-ing ground-truth labels. We first transform the output into RGB image format Xi and augment each task with a textinstruction T i. At each training step, we randomly samplea subset of tasks and then sample data from each task. Foreach input data {Ii, Xi, T i}, we first compute the multi-scale feature map of the original image Ii from the imageencoder. Then, it is concatenated with the noised target im-age Xit before being fed into the UNet for the reconstruc-tion loss. Note that the image feature can have a differentspatial resolution than the target image Xit, in which casethe concatenation will be performed on the interpolated im-age feature. In experiments, we find both the image featureresolution and the target resolution are important for the fi-nal performance but target resolution matters more. Thetext conditioning T i is fed into the UNet via cross-attention. The whole pipeline is trained in an end-to-end mannerexcept for the text encoder, which is frozen throughout thetraining. Compared to the standard diffusion model for con-ditional image generation, there are three main differences: We propose to directly perform diffusion in the pixelspace. As shown in , when mapping from thelatent space to the pixel space, visually uniform re-gions actually have pixels of many different RGB values.This variance can lead to inaccurate class mappings, andconsequently, suboptimal performance for semantic andpanoptic segmentation. The image conditioning is provided via a feature extractor(we use ConvNeXt ) and is concatenated to the targetimage X0. Compared to the widely adopted method ofdirectly concatenating the raw image as the condition, thisbrings significant performance improvement, especiallyfor semantic and panoptic segmentation (see forablation). We remove the self-attention layers in the outermost lay-ers of UNet. This is because the pixel space diffusion atlarge target image resolutions induces considerable mem-ory costs. Removing them alleviates the issue withoutcompromising the performance.",
  "Unified-IO 256 2560.38525.7%----InstructCV 256 2560.29747.2%----Painter 448 4480.28849.9%43.4%0.9540.8680.872Painter 128 1280.43528.4%22.6%0.9220.6260.773Ours128 1280.44848.7%40.3%0.9540.8150.758": ". Our method achieves competitive performance in most of the tasks while trained at a much smaller target resolution of 128 128.When compared at the same resolution, our method shows superior performance over the previous best method (Painter ), especiallyon semantic segmentation and panoptic segmentation. The best number is in bold and the second best number is underscored. indicatesnumbers from our reproduction. Input Image Generated RGB ImageClass Prediction",
  "Latent Diffusion17.1%11.7%Pixel Diffusion48.0%35.5%": ". Upper: Semantic segmentation output of the latent diffu-sion model. The perceptually same colored regions have differentpixel values and, therefore, are mapped to different class labels,leading to bad final performance. While the red box contains onlyone ground-truth class sky in generated RGB image, the final classprediction has four classes after the quantization. Lower: Latentdiffusion suffers from the quantization issue while pixel diffusionachieves good performance.",
  ". Datasets and Implementation Details": "Datasets: We evaluate our method on six different denseprediction tasks with various output formats. For depth es-timation, we use NYUv2 and report the Root MeanSquare Error (RMSE). For semantic segmentation, we eval-uate on ADE20K and adopt the widely used metricof mean IoU (mIoU). For panoptic segmentation, we useMS-COCO and report panoptic quality as the mea-sure. During inference, the model is forwarded twice foreach validation image with different instructions to obtainthe results of semantic and instance segmentation respec-tively. The outputs are then merged together into the panop- tic segmentation. Image restoration tasks are evaluated onseveral popular benchmarks, including SIDD for imagedenoising, LoL for low-light image enhancement, and5 merged datasets for deraining.Implementation details. As mentioned above, we take theStable Diffusion v1.4 checkpoint and finetune it jointlyon six tasks. The image feature extractor is an ImageNet-21K pre-trained ConvNeXt-Large . The text en-coder is Open-CLIP , which is used in Stable Diffu-sion . We adopt uniform sampling for each tasks ex-cept panoptic segmentation, whose weight is twice as muchas the other tasks (as it is a combination of semantic andinstance segmentation). Following , we also adjust theinput scaling factor by a constant factor b in the forwardnoising processing of diffusion. We use AdamW optimizer with constant learning rate of 0.0001, linearly warmedup in the first 20,000 iterations. The target image resolu-tion is 128 128 while the conditioning image resolutionis 512 512. We train our model for 180,000 steps in totalwith a batch size of 1024.",
  ". Recipes for Diffusion-Based Generalists": "In this section, we analyze the design choices of our methodand show their importance through ablation experiments.Specifically, we show the importance of diffusion by train-ing the same model as in to directly generate targetimages without using diffusion (non-diffusion). We studythe significance of image generation pre-training and imageencoder by training models without them (train from scratchand direct concat.). If not specified, we train all models at atarget resolution of 64 64 for 50,000 steps.We attribute the success of our method to four aspects.(1) While having similar results on image restoration tasks,diffusion-based generalist achieves better performance thannon-diffusion models on segmentation tasks which requiresa global understanding of the scene and the semantics. Forexample, the diffusion model reaches 35.5% PQ for panop-tic segmentation while the non-diffusion model has only",
  "Non-diffusion0.44342.4%19.8%0.9510.7730.703Train from scratch0.52846.6%33.6%0.9480.7640.704Direct concat.0.47637.6%27.1%0.9410.7720.687": ". We analyze the important design choices of our method and aim to provide a recipe for training diffusion-based generalists: 1.diffusion models greatly outperform non-diffusion models on panoptic segmentation; 2. text-to-image generation pre-training leads to anoverall better performance; 3. conditioning on image features extracted from an encoder gives significant improvement over the raw image. t=0t=25t=50 Output of Denoising ProcessOriginal Imaget=0t=25t=50 Output of Denoising ProcessOriginal Imaget=0t=25t=50 Output of Denoising ProcessOriginal Imaget=0t=25t=50 Output of Denoising ProcessOriginal Imaget=0t=25t=50 Output of Denoising ProcessOriginal Imaget=0t=25t=50 Output of Denoising ProcessOriginal Image t=0t=25t=50 Output of Denoising ProcessOriginal Imaget=0t=25t=50 Output of Denoising ProcessOriginal Imaget=0t=25t=50 Output of Denoising ProcessOriginal Imaget=0t=25t=50 Output of Denoising ProcessOriginal Imaget=0t=25t=50 Output of Denoising ProcessOriginal Imaget=0t=25t=50 Output of Denoising ProcessOriginal Image . Qualitative results on images from the validation sets of ADE20K, MS-COCO, NYU-V2, SIDD, Deraining, and LOL. Followinga raster scan order, the text prompts are Performance semantic segmentation, Performance instance segmentation, Performance depthestimation, Performance image restoration denoising, Performance image restoration deraining, and Performance image restorationlight enhancement, respectively. The images are not cherry-picked. 19.8% ( ours v.s. non-diffusion). (2) Image gen-eration pre-training on large scale dataset transfers usefulknowledge to the many downstream tasks. The model fine-tuned from Stable Diffusion v1.4 achieves better resultsthan the one trained from scratch across the tasks ( ours vs train from scratch). (3) The image conditioningcan take advantage of powerful pre-trained image encodersby conditioning on the image features rather than the rawimage, which is in contrast to the standard practice for im-age generation tasks. On semantic segmentation and panop-tic segmentation, extracting features gives 10.4% and 8.4%performance improvement, respectively ( ours v.s.direct concat.). (4) Pixel diffusion is better than latent diffu-sion as it does not suffer from the quantization issue whileupsampling (see for an example).",
  ". Comparisons with Prior Art": "We compare our model with recent vision generalists in. With a much smaller target image resolution at128 128, our method achieves competitive performanceacross the tasks. In particular, when compared with the pre-vious best model Painter at the same target resolution,our method has a significant margin over them, which high-lights the potential of our method at a higher resolution.",
  ". Ablation Study": "In this section, we analyze the effect of other importanthyper-parameters of our method, such as batch size, targetimage resolution, and noise-signal ratio. Similar to .2, we train all models at a target resolution of 64 64 for50,000 steps by default.Effect of batch size. Here, we discuss the effect of differentbatch sizes for our method. As shown in , the per-formance of most of the tasks improves with the increase ofthe batch size. In particular, panoptic segmentation greatlybenefits from the large batch size.",
  ". Effect of output resolution. Increasing the target imageresolution significantly improves the performance across tasks": "Importance of noise-signal ratio.In DDPM , theforward diffusion process is defined as xt = tx0 +1 t, where x0 is the input image, is a Gaussiannoise, and t is the number of diffusion step.As shownin , the denoising task at the same noise level (i.e. thesame t) becomes simpler with the increase in the imagesize. In order to compensate for this, proposed to scalethe input with a constant b to explicitly control the noise-signal ratio, which results in the forward diffusion processas xt = tbx0 + 1 t. As we reduce b, it increasesthe noise levels. shows the effect of the noise-signalratio b where b = 0.5 gives the best performance.",
  ". Conclusion and Limitations": "In this work, we explore a diffusion-based vision gener-alist, where different dense prediction tasks are unified asconditional image generation and we re-purpose pre-traineddiffusion models for it. Furthermore, we analyze differ-ent design choices of diffusion-based generalists and pro-vide a recipe for training such a model. In experiments,we demonstrate the models versatility across six differentdense prediction tasks and achieve competitive performanceto the current state-of-the-art. This work, however, is alsosubject to limitations. For example, full fine-tuning of thepre-trained diffusion model at a larger target image resolu-tion is memory intensive due to the pixel space diffusion.Thus, exploring parameter-efficient tuning for such a modelwould be an interesting future direction.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, DarioAmodei, Ilya Sutskever, et al. Language models are unsu-pervised multitask learners. OpenAI blog, 2019. 1": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-vision. In ICLR, 2021. 4 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, andPeter J Liu. Exploring the limits of transfer learning with aunified text-to-text transformer. JMLR, 2020. 1"
}