{
  "Abstract": "Shadow-affected images often exhibit pronounced spa-tial discrepancies in color and illumination, consequentlydegrading various vision applications including object de-tection and segmentation systems. To effectively eliminateshadows in real-world images while preserving intricate de-tails and producing visually compelling outcomes, we intro-duce a mask-free Shadow Removal and Refinement network(ShadowRefiner) via Fast Fourier Transformer.Specifi-cally, the Shadow Removal module in our method aims toestablish effective mappings between shadow-affected andshadow-free images via spatial and frequency representa-tion learning. To mitigate the pixel misalignment and fur-",
  "Han Zhou and Jun Chen are corresponding authors": "ther improve the image quality, we propose a novel Fast-Fourier Attention based Transformer (FFAT) architecture,where an innovative attention mechanism is designed formeticulous refinement. Our method wins the championshipin the Perceptual Track and achieves the second best perfor-mance in the Fidelity Track of NTIRE 2024 Image ShadowRemoval Challenge. Besides, comprehensive experiment re-sult also demonstrate the compelling effectiveness of ourproposed method. The code is publicly available:",
  "arXiv:2406.02559v2 [cs.CV] 3 Jul 2024": "tion distortions. The objective of shadow removal is to en-hance the visibility within shadow regions and achieve illu-mination consistency across both shadow and non-shadowareas, whilst preserving the integrity of naturalistic details.Such enhancement is pivotal for improving the performanceof a plethora of downstream applications such as object de-tection, tracking, and segmentation systems .Numerous traditional methodologies proposed for im-age shadow removal are predominantly designed aroundphysics-based illumination models . Despite theirtheoretical underpinnings, these approaches generally ex-hibit limited effectiveness of removing shadows from real-world, shadow-affected images.This limitation largelystems from the difficulty in establishing an accurate physi-cal correlation between shadow areas and their unblemishedcounterparts, rendering these traditional techniques less ef-fective in practical scenarios.Recently, learning-based approaches have emerged asa formidable mainstream within the domain of shadowremoval, capitalizing on the substantial modeling capa-bility inherent in deep learning frameworks .These methodologies can be bifurcated into mask-based and mask-free shadow removal strategies, con-tingent upon their dependence on shadow masks for guid-ance. Compared to the latter, mask-based shadow removalstrategies not only employ pairs of shadow-affected andshadow-free images but also integrate the location infor-mation of shadow regions, either provided by benchmarkdatasets or generated through pre-trained mask predictionmodels, as learning guidance. The introduction of preciseshadow location enables these models to concentrate on un-raveling the complex mappings between shadow regionsand their clean counterparts, thereby achieving exceptionalperformance on shadow removal.Nonetheless, the dependency on mask information un-veils critical challenges to mask-based methods. Firstly,the acquisition of precise shadow masks is notably chal-lenging . Accurate shadow masks provided in publicdatasets are typically obtained under simply scenarios, suchas a single person standing in a wide-open square. In con-trast, for complex scenes, as exemplified by the WSRD andWSRD+ datasets , annotating images with appropri-ate masks or employing pre-trained models to predict accu-rate masks proves to be impractical. Secondly, the absenceof precise shadow masks markedly undermines the perfor-mance of mask-based models, substantially hindering theirapplicability to complex real-world data.Latest advancements in mask-free shadow removalmethodologies often leverage generative strategies tolearn the mappings between shadow-affected and shadow-free images. However, the adoption of frequency domainanalysis remains largely under-explored within the sphereof shadow removal studies.Notably, several innovative works that integrate spatial and frequency representationshas shown promising results in the broader field of imagerestoration, such as dehazing and low-light image enhance-ment task , suggesting a potential direction for fu-ture exploration for shadow removal.In this paper, we introduce a novel mask-free model thatintegrates spatial and frequency domain representations forimage shadow removal, which achieves compelling per-formance on NTIRE 2024 Image Shadow Removal Chal-lenge , as illustrated in . Specifically, we proposea Shadow Removal and Refinement architecture, termedShadowRefiner, with two specific modules: Shadow Re-moval module and Refinement module. In the Shadow Re-moval module, we design a shadow removal U-Net branch with the backbone of ConvNext blocks. Be-sides, a frequency branch similar to equipped withhigh-frequency, low-frequency representations, and largereceptive field, is also leveraged in our Shadow Removalmodule. Preliminary experiment results, however, indicateobvious pixel misalignment between the output of ShadowRemoval module and the ground truth, manifesting as pro-nounced detail deterioration and compromised color con-sistency. To this end, we introduce a Fast-Fourier Atten-tion based Transformer (FFAT) as the Refinement module,distinguished by its innovative attention mechanism, whichsignificantly enhances the models capacity to remove shad-ows while producing results that are simultaneously high infidelity and visually appealing.Our contributions are three-folds: We introduce an innovative mask-free shadow removalapproach that initially clear shadow via spatial and fre-quency representation learning and further refined by ourproposed frequency attention based transformer architec-ture . To mitigate the pixel misalignment, we introduce aFast Fourier Transformer network endowed with a novelfrequency attention mechanism, achieving superior perfor-mance on recovering texture details and maintaining colorconsistency. Extensive experiments across multiple shadow re-moval benchmarks, as well as the highly competitive out-comes achieved in the NTIRE 2024 Image Shadow Re-moval Challenge (ranking first and second in the Percep-tual Track and Fidelity Track, respectively), underscore theremarkable performance of our proposed ShadowRefinermodel.",
  "CFusion Convolution": ". The overall architecture of our model. In the Shadow Removal module, besides the DWT-FFC branch proposed in , wedesign a ConvNext-based U-Net architecture with 7 7 depth-wise convolution in each resolution. In the Refinement module, we designa new attention mechanism (Fast-Foruier Attention, FFA) different from common attention operations in transformers to further enhancetexture details. duce a learning-based method using a supervised regres-sion algorithm to automatically remove umbra and penum-bra shadows. Wang et al. propose ST-CGAN, an end-to-end framework that integrates shadow detection and re-moval using two stacked Conditional Generative Adver-sarial Networks (CGANs). Bao et al. propose S2Netthat emphasizes semantic guidance and refinement for im-age integrity.This method uses shadow masks to guideshadow removal, with semantic-guided blocks transferringdata from non-shadow to shadow areas, effectively elim-inating shadows while preserving clean regions.He etal. design Mask-ShadowNet, which ensures global il-lumination consistency through Masked Adaptive InstanceNormalization (MAdaIN) and adaptively refines featuresusing aligner modules. Additionally, Fu et al. intro-duce FusionNet, which generates fusion weight maps toeliminate shadow traces further using a boundary-aware Re-fineNet. However, these methods heavily rely on the ac-curacy of the input shadow masks. The complexity andvariability of real-world scenarios could pose challenges ingenerating precise shadow masks, potentially affecting theperformance of these methods in practical applications.Mask-free Image Shadow Removal. Fan et al. intro-duce an end-to-end deep convolutional neural network con-sisting of an encoder-decoder network for predicting theshadow scale factor and a small refinement network forenhancing edge details. Chen et al. design a CANetwhich utilizes a two-stage process for shadow removal, em-ploying a Contextual Patch Matching (CPM) module toidentify matching pairs between shadow and non-shadow patches and a Contextual Feature Transfer (CFT) mecha-nism to transfer contextual information, effectively elimi-nating shadow influence. Vasluianu et al. introduceAmbient Lighting Normalization (ALN) to improve imagerestoration under complex lighting and propose IFBlendthat enhances images by maximizing Image-Frequencyjoint entropy without relying on shadow localization. Liu etal. propose a shadow-aware decomposition network toseparate illumination and reflectance layers, followed by abilateral correction network for lighting adjustment and tex-ture restoration.Transformer-based Image Restoration.Transformerbased networks, usually adopt self-attention mechanisms tounderstand the relationships between different componentsand demonstrate high superiority in handling long depen-dencies, have shown state-of-the-art performance on im-age restoration. SwinIR , a famous backbone for im-age restoration, is designed based on several residual SwinTransformer blocks.With the backbone of VisionTransformer , DehazeFormer is proposed for de-hazing task. Recently, a lightweight transformer architec-ture is proposed for low-light image enhancement basedon Retinex theory.",
  "ShadowRefiner (Ours)Yes28.750.9160.052131.030.9280.042626.040.8270.0854": ". Quantitative comparisons with SOTA methods. Our ShadowRefiner significantly outperforms other mask-free methods acrossthree benchmarks. Compared to mask-based methods, our ShadowRefiner achieves comparable or even better performance (WSRD+dataset). [Key: Best performance among mask-free models, Best performance among mask-based methods] and modeling the mapping from shadow-affected and cleanimages. In this work, we introduce a ConvNext-based U-Net architecture, where multi-scale ConvNext blocks func-tion as strong encoders for robust latent feature learning.As shown in , the ConvNext-based U-Net servesas the primary component in the Shadow Removal module,and the DWT-FFC branch is incorporated as an aux-iliary branch. The contributions of each branch is providedin Sec. 4.4.Specifically, our ConvNext-based U-Net encompassesthree downsampling layers and each downsampling oper-ation is followed by several ConvNext blocks to feature ex-traction. Given a latent feature Fin, the ConvNext blockfirst adopts a 7 7 depthwise convolution, which func-tions similar to the self-attention mechanism in Transform-ers. Then the Layer Normalization (LN) is applied beforetwo 1 1 convolutions, which are equivalent to MLP blockin Transformer. Besides, only one GELU function is lever-aged between two 1 1 convolutions inspired by the factthat Transformer MLP block incorporates only one activa-tion function.For the decoding process, the latent feature is aggregatedusing one attention block and several upsampling op-erations are utilized to recover the latent feature to theiroriginal resolution. In each upsampling layer, there are onepixel-shuffle operation and one attention block. Besides,encoder features are transferred to the decoding process viaskip-connection.In Stage I, we only optimizing the Shadow Removalmodule and the training objective is shown as below:",
  ". Fast-Fourier Attention Transformer based Re-finement": "Early experimental results suggest that our proposedConvNext-based U-Net can effectively remove shadows,but distinct shadow contours remain, as illustrated in .In order to further refine image details and maintain colorconsistency, we introduce an efficient transformer architec-ture with a novel frequency attention mechanism, as shownin . Similar to , our Refinement module adoptsa encoder-decoder architecture, and our proposed FFATblocks are utilized at each resolution level in both encod-ing and decoding process.Given a latent feature F, FFAT block first utilize 1 1point-wise convolution and 33 depth-wise convolution togenerate three features: Q, K, V. Instead of directly lever-aging these features for attention calculation, we apply FastFourier Transform to Q, K to calculate their frequency cor-relation AF based on their frequency domain representa-tions (F(Q), F(K)) by:",
  "Fout = Conv11(FA) + F.(3)": "Compared to the global attention mechanism in Trans-former and other attention strategy , our FFAT blockscan not only effectively capture long-dependencies, but alsodemonstrate stronger representation learning with high effi-ciency. This advantage stems from the frequency domainfeature learning and frequency attention calculation pro-cess. To train our FFAT-based Refinement module, we first",
  ". Datasets and Implementation Details": "Datasets.WeevaluateourproposedmethodonWSRD+ , ISTD , and ISTD+ datasets.WSRD+ dataset, as the enhanced version of the WSRDdataset with improved pixel-alignment, is used as thebenchmark dataset for NTIRE 2024 Image Shadow Re-moval Challenge.This dataset consists of 1200 high-resolution image pairs.The training set, validation set,and test set are split in proportions of 10:1:1, and we trainour model on the training set and evaluate the performanceon the validation set. ISTD dataset contains 1870 imagetriplets obtained from 135 distinct scenarios, of which 1330are assigned for training and the remaining 540 are for test- ing. ISTD+ dataset is a color-adjusted version of ISTD andit has the same number and structure as the ISTD dataset.Implementation Details. One RTX 2080Ti GPU is usedto execute the two-stage training of our method. For dataaugmentation, we implement random cropping of patcheswith dimensions of 384 384, combined with random ro-tations of 90, 180, or 270 degrees, as well as vertical andhorizontal flipping. The Adam optimizer with the defaulthyper-parameters, where 1 and 2 are set to 0.9 and 0.999respectively, is utilized for optimization. In Stage I, onlyShadow Removal module is optimized and the learning rateis initially set to 1 104 and is gradually reduced to6.25 106. In Stage II, we adopt a constant learning of1 105 to simultaneously update the Shadow Removalmodule and Refinement module.Evaluation Metrics. To comprehensively evaluate the per-formance of various shadow removal methods, three met-rics are adopted for quantitative comparison: The Peak Sig-",
  ". Comparison with State-of-the-Art Methods": "We compare our proposed method with several State-of-the-art (SOTA) algorithms. Specifically, several mask-freemethods including Refusion , DCShadowNet andrecent proposed mask-based approaches including Shadow-Former , SADC are adopted for comparison.Quantitative Results. As documented in Tab. 1, our Shad- owRefiner demonstrates superior performances comparedto other mask-free methods across three datasets. On theISTD dataset , there is a marked improvement of 3.62dB in PSNR, a 0.045 increase in SSIM, and a 0.005 de-cline in LPIPS. On the ISTD+ dataset , we observe a4.75 dB increase in PSNR and a 0.041 increase in SSIM.Besides, our ShadowRefiner yields a 3.72 dB increase inPSNR, a 0.089 increase in SSIM, and a 0.008 decline inLPIPS on the WSRD+ dataset .Furthermore, com-pared to mask-based methods, which require shadow maskinformation and naturally outperform mask-based models,our mask-free ShadowRefiner is capable to generate com-",
  "Shadow R (Ours)24.5780.8320.0987.7501": "LVGroup HFUT24.2320.8210.0827.5192USTC ShadowTitan24.0420.8270.1047.4443ShadowTech Innovators24.8100.8320.1117.4384GGBond23.0500.8090.0897.4005PSU Team22.2190.7310.1327.4006LUMOS24.7830.8320.1107.1637IIM TTI22.9550.8060.0937.1608AiRiA Vision21.9020.6890.2386.8259HKUST-VIP Lab 0122.2840.7880.1352260846.61910 . Final ranking (top 10 teams) of Perceptual Track in NTIRE 2024 Shadow Removal Challenge. Our solution achieves the bestperformance among all 19 submitted solutions. [Key: Best, Second Best, (): The larger (smaller) represents the better performance]. parable results on ISTD and ISTD+ datasets. Moreover, onWSRD+ dataset where precise mask image is unavailable,though a mask prediction method proposed in is uti-lized to assist these mask-based models, our ShadowRefinerachieves superior performance than the best mask-based ap-proach (ShadowFormer ), demonstrating that our Shad-owRefiner can work effectively in complex scenarios.Qualitative Comparisons. Visual comparisons on ISTDdataset, ISTD+ dataset and WSRD+ dataset are reported in, 4, and 5, respectively. Obviously, the results of ourmethod closely match GT images in both color and detailpreservation. For instance, in row 3, our ShadowRe-finer removes the shadow without introducing artifacts ordiscoloration, issues commonly seen with other methods.In row 2, our method skillfully adjusts the shadowareas on the pavement, effectively lightening the shadowsto blend with the sunlit parts without distorting the under-lying patterns or hues. Moreover, for images featuring toysand colorful objects shown in , our ShadowRefinerrestores the vivid colors and intricate patterns that are ob-scured by shadows in the input images.",
  ". Performance on NTIRE 2024 Shadow RemovalChallenge (Fidelity and Perceptual Track)": "According to the challenge report , our model is thefirst place of Perceptual Track and the second place ofFidelity Track with the highest SSIM (0.832) and MeanOpinion Scor (MOS, 7.750) and competitive PSNR (24.58),demonstrating advanced performance of our method onshadow removal task.We also report the result of ourmethod for the official validation and test data used inNTIRE 2024 Shadow Removal Challenge as and. We can see that our ShadowRefiner achieves no-tably satisfying shadow removal effect in pictures of differ-ent scenarios. For example, the shadow in the first image in is completely eliminated, retaining the integrity of thescattered toy blocks in their original layout. Additionally,the color consistency between toys initially positioned in",
  "only ConvNext-based U-Net25.290.8100.0914only DWT-FFC branch23.360.7910.1016": ". The ablation results on WRSD+ dataset. Each compo-nent in our ShadowRefiner help achieve competitive performanceon shadow removal task, and our proposed Refinement moduleperforms better than Restormer. shadowed areas and those under direct illumination atteststo the high fidelity in color reproduction. The restoration ofthe final image depicting a dog conveys a sense of uniform,soft lighting across the entire subject, further exemplifyingShadowRefiners capability to enhance illumination homo-geneity.",
  ". Ablation Study": "In this section, we conduct several ablation experiments onWSRD+ dataset .Importance of FFAT based Refinement module. To studythe contribution of the Refinement module and our pro-posed FFAT blocks, we first remove the Refinement mod-ule to figure out its contribution for the outstanding per-formance of our method provided in Tab. 1. The quantita-tive result is reported in Tab. 3, which demonstrates remov-ing Refinement module leads to obviously degraded per-formance. Then, we replace our Refinement module withRestormer module and we notice a marked decrease inPSNR (0.36 dB ) and SSIM (0.009 ) and a discernible in-crease of LPIPS (0.0024 ). Visual comparisons providedin also demonstrate our Refinement module can helpgenerate more visually appealing results compared to theRestormer module. This ablation experiment underscorethe importance of our proposed Refinement module for sat-isfactory shadow removal performance.Contributions of ConvNext-based U-Net. Based on the",
  ". Visual ablation comparisons on the WSRD+ dataset": "configuration for Tab. 3 row 2, we implement several fur-ther adaptations to illustrate the effectiveness of ConvNext-based U-Net and the DWT-FFC branch leveraged in theShadow Removal module. By separately comparing row 4 and row 5 to row 2 in Tab. 3, we can conclude thatadopting a two-branch architecture in the Shadow Removalmodule help achieve more pleasant performance than singlebranch. Moreover, compared to the DWT-FFC branch, theenhanced performance of the ConvNext-based U-Net archi-tecture highlights its predominant role within the ShadowRemoval module.",
  ". Conclusion": "In this paper, we propose ShadowRefiner, a novel mask-free model for shadow removal task.Specifically, theShadow Removal module with ConvNext-based U-Net isfirstly introduced to extract spatial and frequency repre-sentations and effectively learning the mapping betweenshadow-affected and clean images. Then, we design a noveltransformer module based on Fast Fourier Attention Trans-former to enhance color and structure consistency. Exten-sive experiments demonstrate ShadowRefiner significantlyoutperforms the current mask-free methods and its capacityis comparable to mask-based shadow removal approaches.Furthermore, our method wins the championship in thePerceptual Track and ranks second in the Fidelity Trackof NTIRE 2024 Image Shadow Removal Challenge. Codruta O Ancuti,Cosmin Ancuti,Florin-AlexandruVasluianu, Radu Timofte, et al.NTIRE 2023 hr nonho-mogeneous dehazing challenge report.In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition Workshops, 2023.",
  "Zipei Chen, Chengjiang Long, Ling Zhang, and ChunxiaXiao. Canet: A context-aware network for shadow removal.In Proceedings of the IEEE International Conference onComputer Vision (ICCV), 2021": "Xiaodong Cun, Chi-Man Pun, and Cheng Shi.Towardsghost-free shadow removal via dual hierarchical aggregationnetwork and shadow matting gan.In Proceedings of theAAAI Conference on Artificial Intelligence (AAAI), 2020. Wei Dong, Han Zhou, and Dong Xu. A new sclera segmen-tation and vessels extraction method for sclera recognition.In 2018 10th International Conference on CommunicationSoftware and Networks (ICCSN), 2018. Wei Dong, Han Zhou, Ruiyi Wang, Xiaohong Liu, GuangtaoZhai, and Jun Chen. DehazeDCT: Towards effective non-homogeneous dehazing via deformable convolutional trans-former. In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition Workshops (CVPR Work-shops), 2024.",
  "Hui Fan, Meng Han, and Jinjiang Li. Image shadow removalusing end-to-end deep convolutional neural networks. In Ap-plied Sciences, 2019": "Kang Fu, Peng Yicong amd Zhang Zicheng, Qihang Xu, Xi-aohong Liu, Jia Wang, and Guangtao Zhai. Attentionlut: At-tention fusion-based canonical polyadic lut for real-time im-age enhancement. arXiv preprint arXiv:2401.01569, 2024. Lan Fu, Changqing Zhou, Qing Guo, Felix Juefei-Xu,Hongkai Yu, Wei Feng, Yang Liu, and Song Wang. Auto-exposure fusion for single-image shadow removal. In Pro-ceedings of the IEEE Conference on Computer Vision andPattern Recognition (CVPR), 2021.",
  "Hieu Le and Dimitris Samaras. Shadow removal via shadowimage decomposition. In Proceedings of the IEEE Interna-tional Conference on Computer Vision (ICCV), 2019": "Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang,Luc Van Gool, and Radu Timofte. Swinir: Image restorationusing swin transformer. In Proceedings of the IEEE Inter-national Conference on Computer Vision Workshops (ICCVWorkshops), 2021. Xiaohong Liu, Yongrui Ma, Zhihao Shi, and Jun Chen. Grid-dehazenet: Attention based multi-scale network for imagedehazing. In Proceedings of the IEEE International Confer-ence on Computer Vision (ICCV), 2019. Xiaohong Liu, Zhihao Shi, Zijun Wu, Jun Chen, and Guang-tao Zhai. Griddehazenet+: An enhanced multi-scale networkwith intra-task knowledge transfer for single image dehaz-ing. IEEE Transactions on Intelligent Transportation Sys-tems, 2022.",
  "Yuhao Liu, Zhanghan Ke, Ke Xu, Fang Liu, Zhenwei Wang,and Rynson W. H. Lau.Recasting regional lighting forshadow removal. arXiv preprint arXiv:2402.00341, 2024": "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, ZhengZhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. InProceedings of the IEEE Conference on Computer Visionand Pattern Recognition (CVPR), 2021. Zhihao Liu, Hui Yin, Xinyi Wu, Zhenyao Wu, Yang Mi, andSong Wang. From shadow generation to shadow removal.In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition (CVPR), 2021. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-enhofer, Trevor Darrell, and Saining Xie. A convnet for the2020s. In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition (CVPR), 2022.",
  "latent-space diffusion models. In Proceedings of the IEEEConference on Computer Vision and Pattern RecognitionWorkshops (CVPR Workshops), 2023": "Xu Qin, Zhilin Wang, Yuanchao Bai, Xiaodong Xie, andHuizhu Jia. Ffa-net: Feature fusion attention network forsingle image dehazing. In Proceedings of the AAAI Confer-ence on Artificial Intelligence (AAAI), 2020. Liangqiong Qu, Jiandong Tian, Shengfeng He, YandongTang, and Rynson W. H. Lau. Deshadownet: A multi-contextembedding deep network for shadow removal. In Proceed-ings of the IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2017.",
  "Florin-Alexandru Vasluianu, Tim Seizinger, Zongwei Wu,Rakesh Ranjan, and Radu Timofte. Towards image ambientlighting normalization. In arXiv preprint arXiv:2403.18730,2024": "Florin-Alexandru Vasluianu, Tim Seizinger, Zhuyun Zhou,Zongwei Wu, Cailian Chen, Radu Timofte, et al. NTIRE2024 image shadow removal challenge report. In Proceed-ings of the IEEE Conference on Computer Vision and PatternRecognition Workshops (CVPR Workshops), 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, and ukasz KaiserandIllia Polosukhin. Attention is all you need. In Advances inNeural Information Processing Systems (NeurIPS), 2017. Jifeng Wang, Xiang Li, and Jian Yang. Stacked conditionalgenerative adversarial networks for jointly learning shadowdetection and shadow removal. In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition(CVPR), 2018. Wenyi Wang, Jun Hu, Xiaohong Liu, Jiying Zhao, and Jian-wen Chen. Single image super-resolution based on multi-scale structure and nonlocal smoothing. EURASIP Journalon Image and Video Processing, 2021.",
  "Xiangyu Yin, Xiaohong Liu, and Huan Liu. Fmsnet: Un-derwater image restoration by learning from a synthesizeddataset. In International Conference on Artificial Neural Net-works (ICANN), 2019": "Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu-nawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang.Restormer: Efficient transformer for high-resolution imagerestoration. In Proceedings of the IEEE Conference on Com-puter Vision and Pattern Recognition (CVPR), 2022. Ling Zhang,Chengjiang Long,Xiaolong Zhang,andChunxia Xiao. Ris-gan: Explore residual and illuminationwith generative adversarial networks for shadow removal.In Proceedings of the AAAI Conference on Artificial Intel-ligence (AAAI), 2020. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-man, and Oliver Wang. The unreasonable effectiveness ofdeep features as a perceptual metric. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recogni-tion (CVPR), 2018. Yunfeng Zhao, Chris Elliott, Huiyu Zhou, and Karen Raf-ferty. Pixel-wise illumination correction algorithms for rel-ative color constancy under the spectral domain. In IEEEInternational Symposium on Signal Processing and Informa-tion Technology (ISSPIT), 2018. Han Zhou, Wei Dong, Yangyi Liu, and Jun Chen. Break-ing through the haze: An advanced non-homogeneous de-hazing method based on fast fourier convolution and con-vnext. In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition Workshops (CVPR Work-shops), 2023. Yurui Zhu, Zeyu Xiao, Yanchi Fang, Xueyang Fu, ZhiweiXiong, and Zheng-Jun Zha. Efficient model-driven networkfor shadow removal. In Proceedings of the AAAI Conferenceon Artificial Intelligence (AAAI), 2022."
}