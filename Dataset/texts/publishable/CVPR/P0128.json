{
  "Abstract": "DensePose provides a pixel-accurate association ofimages with 3D mesh coordinates, but does not providea 3D mesh, while Human Mesh Reconstruction (HMR)systems have high 2D reprojection error, as measuredby DensePose localization metrics. In this work weintroduce MeshPose to jointly tackle DensePose andHMR. For this we first introduce new losses that allowus to use weak DensePose supervision to accurately lo-calize in 2D a subset of the mesh vertices (VertexPose).We then lift these vertices to 3D, yielding a low-polybody mesh (MeshPose). Our system is trained in anend-to-end manner and is the first HMR method to at-tain competitive DensePose accuracy, while also beinglightweight and amenable to efficient inference, makingit suitable for real-time AR applications.",
  ". Introduction": "3D Human Mesh Reconstruction (HMR) has received in-creased attention thanks to its broad AR/VR applicationssuch as human-computer interaction, motion capture, enter-tainment/VFX and virtual try-on. Despite rapid progress inHMR, mesh predictions with the current systems are stillnot pixel-accurate when projected back to the image domain.Mesh reconstruction evaluation is primarily 3D skeleton-or 3D mesh-based (measured in millimeters) and does notreflect 2D reprojection accuracy (in pixels). However, forpersons close to the camera, small 3D errors result in visible",
  "Meshpose": "MeshposeS MeshposeXS . Left: Inference Time vs DensePose AP, Right: PA-MPJPE vs DensePose AP for both, top-left is best and radii areproportional to the sizes of the models (MB). Our approach out-performs HMR methods on DensePose metrics by more than 50%while having close to state of the art 3D accuracy. By combining thehighest FPS rate and small model size with state-of-art reprojectionaccuracy, our pipeline is well suited for mobile inference. 2D reprojection errors, for instance when users take selfiephotos, as is the currently predominant use case for AR. Ifwe want a 3D mesh that looks good when projected to 2Dthe present HMR method evaluation must be complementedby 2D reprojection metrics. Such errors are in the blindspot of 3D evaluation metricsand can break an AR experience such as virtual try-on. Er-rors in the 2D projection of a mesh are glaringly obvious(e.g. bags floating above the users shoulders, coats thatare too tight/too loose, misplacements around limbs etc.).DensePose has been a popular alternative to accurate warptight garments to the users body , however warp-ing does not suffice for apparel (e.g. dress, coat, handbag)that protrudes from the users body and requires proper 3Dtry-on.",
  "arXiv:2406.10180v1 [cs.CV] 14 Jun 2024": "Motivated by this observation, in this work we set out tobridge the gap between the DensePose and HMR systems.For this we revisit the DensePose task and show that wecan use the DensePose dataset to supervise a network that re-lies on a discrete, 3D vertex-based representation rather thanrelying on continuous UV prediction. The resulting systemperforms similarly to UV-based systems on the DensePosetask while at the same time delivering an accurate 3D mesh.We call the resulting mesh prediction system MeshPose toindicate that it combines Mesh and DensePose prediction ina unified system.To achieve this we make the following contributions: We introduce VertexPose, a novel layer designed to predictthe 2D projections of the vertices of a low poly 3D bodymesh and simultaneously regress the per-pixel DensePoseUV signal directly. This is accomplished through denseheatmaps that allow us to precisely pin down the pixelcoordinates of a vertex. For this we introduce two newweakly supervised losses that rely on the mesh geometryto supervise VertexPose through the DensePose dataset. We then introduce MeshPose to form a 3D mesh out of thelocalized 2D vertices. This is accomplished by regressingper-vertex a depth, visibility and amodal 3D estimate. Welift visible vertices to 3D by concatenating the 2D positionwith their depth, and use the amodal estimate for invisiblevertices.Our results, shown in and in more detail in show that when assessed in terms of 2D DensePose or evenplain 2D pose estimation accuracy, other recent methods canuse even 10 more parameters (e.g. MeshPoseXS vs Metro),or be 20 slower (e.g. MeshPoseXS vs NIKI) yet still resultin substantially worse 2D reprojection metrics. At the sametime our mesh reconstruction performance is comparableto most recent systems on 3DPW. Given the importance of2D reprojection to the end-user experience in AR, we hopethat our work will establish DensePose-based evaluation andtraining as a standard practice in future HMR works.Our video results, provided in the Supplement, comple-ment these findings and indicate the temporal stability ofour method even when applied frame-by-frame. Our methodis lightweight, simple and directly amenable to real-timeinference on mobile devices, making it a prime candidate forAR applications.",
  ". Previous work": "Our starting point for this work is the understanding thatHuman Mesh Reconstruction systems are typically notgrounded on pixel-level evidence for vertex positions, butinstead try to predict them through the reconstruction of themuch more complex structure of the body mesh. We take abottom-up approach, where we first detect visible verticesthrough dense 2D heatmaps and then build the mesh aroundthem. As we explain here, this has not been an obvious approach to HMR before our work.Parametric 3D mesh reconstruction methods such as methods provide rota-tion estimates to a forward kinematics (FK) recursion thatunavoidably accumulates errors, thereby making it challeng-ing to achieve good mesh alignment on wrists or ankles.Iterative fitting methods such as can mitigate this by minimizing the back-projection errorsthrough gradient descent on the model parameters or by scal-ing up the required computational power , but both areinappropriate for real-time inference. Variants of these workshave been introduced to address additional complicationsdue to human-human occlusion or object-human occlusion and perspective distortioneffects for in-the-wild scenes, where the problemsdescribed above become even harder.Inverse Kinematics (IK)-based methods are a step forward when it comes to localization accuracy inthat they ensure that the recovered mesh passes through aset of 3D joints provided by a bottom-up system, yieldinghigh 3D pose accuracy results. As our results show however,these methods fare poorly when it comes to 2D reprojec-tion, since they are still constrained by the pose and shapevariation of the employed parametric model.As an alternative to IK, recurrent refinement methodssuch as repeatedly estimate the positions of meshvertices and sample features at the vertex positions to get asecond look at the image. Our results indicate that their2D reprojection accuracy is limited, while their recursiveprojection/lookup operations make them harder to deployfor mobile AR applications since they require custom layerswhich are not supported in CoreML/TFlite and end upbeing computational bottlenecks when performing inferenceon NPU/GPU-accelerated mobile devices.Non-parametricHMRmethodssuchasgraph-convolutional , heatmap-based , ortransformer-based models bypass parametricmodels and directly regress the full body mesh. In principlethis can avoid the problem of error accumulation duringFK, yet as our DensePose results show in thesemethods still suffer when it comes to establishing accurateimage reprojection. Our method bears similarity to recentmethods for integral regression of per-vertex x/y/z values in that we ground the vertex coordinates directlyon image evidence, rather than regressing them throughglobal mesh recovery systems.However we differ inthat we directly connect our method to the DensePoseestimation and training problems, thereby allowing us to getsubstantially better accuracy through direct optimizationof the relevant objective. We use DensePose ground truthas weak supervision to localize our mesh vertices in 2D,and show that we can both deliver accurate 3D meshes andDensePose predictions.",
  "z": ". Meshpose Architecture: The lower VertexPose branch extracts multiple heatmaps from which, by applying the spatial argsoftmaxoperation, it computes precise x and y coordinates for all the vertices inside the input crop. The upper Regression branch computes thecoordinates (x, y, and vertex depth z) for all vertices, along with their visibility scores w. The score w will take lower values when thecorresponding vertex is either occluded or fall outside the crop area. We differentiably combine the VertexPose and regressed coordinates viaw to get the final 3D mesh. We densely supervise the intermediate per-vertex heatmaps and the final output with UV, mesh and silhouettecues to end up with a low latency, image aligned, in-the-wild HMR system. The DensePose task aims at associating every humanpixel with its continuous, surface-based UV coordinates.This has been typically addressed through a dense regressiontask, where a CNN tries to directly recover these UV valueswith per-part UV regression heads, trained through a set ofpixel-UV annotations. Unfortunately UV regression is oflittle direct value when it comes to mesh reconstruction: 2Dvertex localization from DensePose requires multiple tricks(e.g. thresholding of UV distances, de-duplicating vertices,fixing left-right prediction errors for legs) and explains whythis has not been adopted as a front-end processing for meshrecovery.",
  ". Method": "In our work we recover a human body mesh from a singleimage in two stages as shown in . In the first stage,detailed in Sec. 3.1, we introduce VertexPose, a novellayer that serves a dual purpose: predicting DensePose andlocalizing mesh vertices in 2D. VertexPose is designed topredict 2D heatmaps for a sparse set of body vertices thatform a low-polygon approximation to a high-resolution mesh.Moreover, for each pixel, these heatmaps are integrated usingbarycentric combination, to compute the UV coordinates forany given pixel as DensePose.We introduce novel losses to obtain weak supervision forVertexPose heatmaps from DensePose data. We show thateven though we do not have direct supervision for the 2Dlocations of the VertexPose vertices, we obtain DensePoseaccuracy comparable to UV-based DensePose systems.In the second stage, detailed in Sec. 3.2, we lift the 2DVertexPose vertex positions to 3D, constructing a 3D meshthat accurately projects back to VertexPose vertices. We achieve this through a simple 1D integral regression taskwhich estimates the root relative depth for all vertices inpixels.We complement the VertexPose-based losses with 3Dcounterparts that allow us to exploit 3D ground-truth andpseudo ground-truth to jointly train VertexPose and its asso-ciated 3D lifted predictions.As the upper branch of our diagram shows, we augmentthe VertexPose-based predictions with a per-vertex visibilityestimate that is learned from weak supervision, as well as aglobal, image-level 3D regression of all mesh vertices. Thisallows us to handle invisible parts by fusing the VertexPose-based vertices with the latter prediction, which acts as afallback.All stages rely on a single network that is trained end-to-end, but we separate their presentation and evaluation; wedescribe VertexPose below and MeshPose in Sec. 3.2.",
  "VertexPose layer": "The VertexPose layer consists of an H W V tensor Swhere H, W are tensor height/width and V is the number ofvertices. This provides for each mesh vertex the score of all2D input positions, yielding a set of heatmaps that serve both2D vertex localization and dense UV prediction. We further",
  "(b) UV Consistency loss": ". Geometry-driven losses used to supervise VertexPose with DensePose ground-truth. Our barycentric loss requires that theper-pixel distribution over VertexPose matches the UV annotations barycentrics. Our UV consistency loss requires that the UV annotationsbarycentrics at a labelled pixel x should recover x based on a similar combination of VertexPose vertices into x. denote by s = S[x, y, :] the 1 V set of VertexPose scoresat a given position x = (x, y) and by Sv = S[:, :, v] theH W heatmap corresponding to a given vertex v.We localize every vertex v as the argsoftmax of Sv:",
  "i exp(Sv[xi]) ,(1)": "where the exponentiation and normalization turns the pos-sibly negative heatmaps into a distribution over positionsand the scaling parameter allows us to make the resultingdistribution more peaked and helps with localization.We estimate the UV value at a position x based on theper-pixel posterior over vertices qv =exp(sv)Vk=1 exp(sk). We firstidentify the mesh face f = i, j, k whose vertices have thelargest cumulative score: f = argmaxfF",
  "(2)": "where we treat the normalized posterior over vertices thatform f as an estimate of the pixels barycentric coordinates.We note the dual nature of our VertexPose layer: UV esti-mation is not needed at inference time for our network, butis rather used as a means to supervising our network throughDensePose ground-truth. By contrast vertex localization can-not be directly supervised, but is the 2D substrate for our 3DMeshPose system.VertexPose additionally generates a segmentation maskto accurately localize the foreground, which is a requiredcomponent for the evaluation of DensePose metrics for thissubsystem. This layer is omitted from the final HMR system.",
  "VertexPose training": "The main challenge when training the VertexPose layer is theabsence of direct supervision for the 2D VertexPose vertexpositions: the DensePose dataset was collected with contin-uous regression in mind and relied on annotating randombody pixels with their associated UV values. This means thatwe do not have strong supervision at the level of per-vertex2D ground-truth locations, which would allow us to directlyalso use the loss functions used for 2D pose estimation e.g.in . We mitigate this by introducing novel losses thatexploit the underlying geometric nature of VertexPose andthereby allow us to use DensePose data for weak supervision.Barycentric Cross-Entropy Loss:This loss forces the softmax-based posterior over Vertex-Pose vertices to approximate the barycentric coordinates onany pixel that has UV annotation, as shown in (a).In particular for any pixel x = (x, y) that comes with aDensePose annotation with UV coordinates u we introducea loss on the VertexPose scores s = S[x, y, :] at that pixel.We phrase the task as one of competition among the Vertex-Pose vertices for the occupancy of the particular pixel. If avertex v landed precisely on a given pixel we could imposeat that point a standard Cross-Entropy loss using the one-hot encoding of that vertex. But this is unlikely to happen,since DensePose ground-truth was originally not sampledon specific landmark locations such as the vertices.Instead we form our loss by interpreting the ground-truthbarycentric coordinates as a discrete distribution on verticesof the ground-truth triangle f. We use this to penalize thesoftmax-based posterior using the general definition of thecross-entropy loss:",
  "where we replace the common one-hot encoding of the cor-rect label with a distribution on vertices, pv, forcing the": "VertexPose-based posterior q to align with the barycentric-based distribution p. Our results indicate the advantage ofusing this geometry-inspired loss instead of a cruder, near-est neighbor assignment of annotated pixels to their nearestmesh vertex.UV Consistency Loss:This loss forces VertexPose toplace vertices so that the image coordinates of annotated UVvalues align with the positions where they were annotated,as shown in (b).In particular we turn a pixels DensePose annotation(x, u) into a constraint on the VertexPose heatmaps Sv =S[:, :, v], v (i, j, k) of the three vertices (i, j, k) used tocompute the barycentric coordinates (i, j, k) of u. Thesethree barycentric coordinates allow us to localize the pixelscorresponding point on the 3D surface as a convex com-bination of these three vertices. When projected to theimage this relationship should still roughly hold, modulodepth-based perspective distortion effects, which we con-sider negligible within a triangle. Our loss enforces this:when combining the estimated 2D positions of the three ver-tices xv = argsoftmax(Sv), v {i, j, k}, with barycentricweights v, we should be able to recover the position of x:",
  "v{i,j,k}vxv(4)": "This forces the heatmap Sv to properly localize xv. with-out direct supervision for vertex v.Both of these losses are efficient to evaluate and as ourresults in Sec. 4 show, they add up to the training of a Vertex-Pose system that even outperforms the UV-based DensePosebaseline when trained with identical data and experimentalsettings. Still, we consider the competition with UV-basedDensePose systems to be of secondary importance comparedto being able to directly predict a mesh based on the subse-quent lifting of the estimated vertices to 3D, as described inSec. 3.2.",
  ". MeshPose: Lifting VertexPose to 3D": "Having outlined our method to localize mesh vertices in2D we now turn to converting them into a 3D mesh. Asshown in , our method consists of retaining the imagelocalization information of VertexPose where available andfilling in the remaining information by values regressed by aseparate network branch.Inspired by we take the backbone CNNs lasttensor, average pool it and transform the result through 1Dconvolutional layers to regress a 4 64 V tensor, whereV is the number of the low-poly vertices, the four channelscorrespond to X, Y, Z values and a per-vertex visibility labelw and 64 are the number of bins used for argsoftmax voting.In particular for every regressed vertex V XY Zregits X, Y, Zvalues are obtained separately per dimension by applying1D argsoftmax while the visibility w is obtained by mean",
  "Visibility prediction": "The visibility label dictates on a per-vertex level whetherwe should rely on the VertexPose-based 2D position, V XYspor fall back to the V XYreg value regressed at this stage. Thisallows us to accommodate occluded areas, or tight cropsthat omit part of the human body, as is regularly the case forselfie images. The 2D location of a MeshPose vertex is theirvisibility-weighted average: V XYmp = V XYspw+V XYreg (1w).This differentiable expression allows us to estimate visibilitythrough end-to-end back-propagation, but we also use twoadditional methods for visibility supervision.Firstly we estimate partial vertex visibility based on theavailable ground-truth: for any (x, u) annotation pair con-tained in the DensePose dataset, we declare as visible allthree vertices that lie on the mesh triangle containing u. Wealso declare as non-visible every vertex where the mesh su-pervision (obtained from ) is outside the image crop. Forsuch vertices we can supervise visibility based on a standardbinary cross-entropy loss.Secondly, we also supervise visibility at the mesh level.For this we use differentiable rendering with the per-vertextexture set to equal the predicted visibility label. This pro-duces a soft visibility mask, shown also in as aheatmap, which indicates the image area that is coveredby the persons body. This can be supervised at the regionlevel based on the DensePose datasets instance segmen-tation masks, using a mix of an 2 loss with the integralboundary loss introduced in . These two sources of vis-ibility supervision gave substantial improvements as alsoshown by our results.",
  "Depth regression": "We adopt a weak perspective camera model as in andconsider that each vertex lies on a ray that crosses the imageplane at the VertexPose-based 2D position. We thereby limit3D lifting to the task of estimating the vertex depth on thatray. Rather than directly regress the depth of a vertex, wepredict its depth relative to the root of the mesh (sternum).The latter is predicted by a separate RootNet network ,leaving to our network the task of relative depth estimation.We estimate depth in pixel units, based on the same rigidtransform used to associate the metric joint (x,y) positionswith their 2D pixel counterparts.",
  "V XY Zmp= (V XYspw + V XYreg (1 w)||V Zreg)(5)": "All terms are differentiable, allowing us to train our networkend-to-end based on 3D mesh supervision.We note that we can optionally also transform the result-ing low-poly mesh into a high-poly counterpart through anMLP-based upsampling as in . We have trained suchan MLP and used it both for mesh visualizations and per-formance evaluations. Even though it primarily serves as avisual embelishment of the low-poly prediction, it also pro-vides some form of regularization when trained with noisedlow-poly inputs.Finally, if a parametric representation is needed foran application the MeshPose prediction lends itself eas-ily to Inverse Kinematics-based processing by usinglandmarker-based 3D joint estimates. Our implementationof HybridIK-type decoding yields virtually identical 3D met-rics, but comes at the cost of a drop in DensePose accuracy,as expected, hence we omit it from evaluations.",
  "D supervision": "In order to supervise the 3D coordinates of the low polymesh we use motion capture ground truth (GT) meshes,weak supervision for the in-the-wild COCO dataset (pseudo-GT), and the losses described below.Vertex Localization, Edge and Normal Loss: The positionof our 3D vertices can be directly compared to the (pseudo)GT in terms of an L2 loss (localization loss), while we canalso penalize the distortion of the edge lengths between twoadjacent vertices (edge loss). We note that the second lossdoes not necessarily guarantee good alignment to the image,but ensures we do not arbitrarily stretch or shrink the meshto reduce other losses. To further reduce mesh curvatureartifacts we use a third, (normal cosine loss) that penalizesthe deviation of predicted vertex normals.Joint Localization Loss: The last form of supervision relieson standard 2D or 3D joint GT that is more readily availablethrough image annotations or motion capture, respectively.The position of each joint is estimated as a weightedaverage of a subset of nearby mesh vertices, implemented asa precomputed linear regression (landmarker).We compare the predicted joints to the GT using its full3D coordinates or their projections on the image, based onwhether we have 3D or 2D supervision. Even though onlya sparse subset of vertices contribute to the prediction ofany joint, the edge loss described above helps diffuse thesupervision to the remainder of the mesh.",
  ". Datasets": "We use the manual DensePose annotations provided in on the MS-COCO dataset for training the Vertex-Pose system. For 3D joint supervision we use three datasets:(i) The Human3.6M : MoCap dataset and follow theprotocol of (subjects (S1, S5, S6, S7, S8) for training)(ii) The MPI-INF-3DHP : Multi-view dataset, follow-ing the train split of . (iii) The 3DPW in-the-wildoutdoor benchmark for 3D pose and shape estimation con-taining 3D annotations from IMU devices. Furthermore, weaugment the MS-COCO dataset with the 3D mesh pseudoGT annotations of for vertex-level mesh supervision.",
  ". Evaluation Metrics": "We adopt the evaluation framework outlined in for theDensePose task, where we report on two key metrics: AP(Average Precision) and AR (Average Recall). These metricsquantify the accuracy of the dense correspondences from UVcoordinate predictions on images. For mesh recovery meth-ods this can be interpreted as measuring mesh alignmentaccuracy after projection. We measure the correspondenceaccuracy by rendering UV coordinates of the visible meshsurface. In addition to these 2D metrics, we evaluate the ac-curacy of 2D COCO keypoints prediction, quantified throughAverage Precision and Recall. This evaluation is conductedboth across all instances (AP-All and AR-All), as well asonly on instances where at least 80% of the keypoints are vis-ible (AP-80% and AR-80%). All 2D metrics are evaluatedon DensePose-COCO, a subset of COCO introducedin .For 3D pose evaluation we employ a landmarker on topof the high poly 3D mesh to compute the 14 LSP joints forthe evaluation on 3DPW dataset . Then we com-pute the Euclidean distances (in millimeter (mm)) of 3Dpoints between the predictions and GT as described by thefollowing metrics: (i) MPJPE (Mean Per Joint PositionError) first aligns the predicted and GT 3D joints at the3D position of the pelvis, evaluating the predicted pose bytaking into account the global rotation. (ii) PA-MPJPE(Procrustes-Aligned Mean Per Joint Position Error, or recon-struction error) performs Procrustes alignment before com-puting MPJPE, eliminating any error due to wrong globalscale and global rotation. (iii) PVE (Per Vertex Error) doesthe same alignment as MPJPE and then calculates the dis-tances of vertices of human mesh, evaluating also the meshshape additionally to the 3D skeleton pose.",
  ". VertexPose evaluation": "We start by examining the impact of training with the vertex-based (VertexPose) approach compared to the UV-regressionbased (DensePose) representations for the DensePose task.To keep the comparison apples-to-apples in a we compare experiments with identical backbones and train-ing settings, where we closely follow for designing theDensePose baseline. We observe that the VertexPose-basedresults compare favorably to their DensePose-based counter-parts, confirming the validity of the proposed approach.In b we analyze the impact of the barycentric inter-polation strategy used to predict UV in Eq. 2. We compare itto simpler baselines of using the UV of the strongest vertexat any pixel (Nearest) or doing argsoftmax over all verticesrather than those of the strongest triangle (Global Aver-age). The results indicate the merit of the smooth transitionbetween vertices secured by barycentric interpolation.We consider improvements in the 2D DensePose task tobe of secondary importance compared to improving the 3DHMRs DensePose performance, hence keep the remainingresults focused on MeshPose.",
  ". MeshPose evaluation": "In we start by ablating the impact of the types of su-pervision used for our full MeshPose system. Starting fromonly 3D losses (vertex localization, edge and normal loss- L3D) on row 1, we show the impact of adding 2D losses(barycentric and uv consistency loss - L2D) and visibilitysupervision (partial visibility and rendering loss - LW ). Weshow that adding 2D losses leads to a moderate drop in 3Daccuracy but boosts DensePose reprojection metrics, whileadding visibility supervision helps improve both tasks at thesame time - effectively helping them coexist.Turning to comparisons with HMR methods, in ()we extensively compare our approach with 10 other SOTAarchitectures in terms of efficiency (measured in #Parametersand FPS) and accuracy. We report the performance on 3tasks, (i) 2D DensePose-COCO Keypoints, (ii) DensePoseand (iii) 3D alignment on 3DPW and Human 3.6M.",
  ". Ablation table evaluated in terms of 3D metrics (3DPW)and 2D reprojection accuracy (COCO-DensePose)": "MeshPose is by far the most efficient approach for achiev-ing real-time prediction while getting the best 2D reprojec-tion performance. This large improvement comes at the costof a small impact on 3D metrics.Our method achieves competitive performance againstother parametric and non-parametric approaches, whilelargely outperforming them on the DensePose task. Morespecifically, MeshPose achieves much better DensePose met-rics compared against CLIFF and NIKI methods, which havethe best scores for the HMR task. depicts some examples comparing the proposedMeshPose against the PARE, CLIFF, NIKI and Point-HMRmethods. We can see that MeshPose produces meshes that,",
  "ours": "MeshPose (HRNet32 )46.2928.6647.3061.3071.2079.6047.8757.6276.0846.7392.7050.7635.37MeshPoseS (ResNet50 )45.37124.2843.8058.1067.0076.5044.4154.4980.0348.9797.9656.3337.64MeshPoseXS (MBNet140 )21.25124.2140.6055.2063.6073.9038.5649.2079.1549.7196.4958.4041.63 . Evaluation of network efficiency, 2D accuracy in COCO-DensePose and 3D errors in the 3DPW and Human3.6M datasets. Thevariants of our system achieve superior performance in 2D metrics (2D Keypoints, Densepose) when compared to other methods, while theyachieve comparable 3D accuracy. At the same time they are substantially more efficient in terms of FPS and # of parameters.",
  ". Qualitative results on 3DPW on front and side views.Our method shows strong 2D alignment with accurate 3D meshprediction": "when projected on the image, align much better than compet-ing methods. Other methods fail e.g. when reconstructingchildren, when a large part of the body is occluded and haveinferior alignment around the limbs. In , we fur-ther demonstrate the performance of our system in terms ofmesh reconstruction by including side views of our meshespredicted on 3DPW images.",
  ". Typical failure cases of MeshPose include a hand flat-tening artifact and imperfect image alignment in the presence ofperspective distortion": "for perspectively distorted inputs. The flattening artifact pri-marily arises due to the mesh upsamplers lack of training onexamples with articulated hands. The imperfect perspectivealignment is caused by our systems reliance on the assump-tion of weak perspective camera model. Still, the robustnessof our method is comparable to DensePose, hence we donot have catastrophic failures (e.g wrong torso pose) in thepresence of heavy occlusions.",
  ". Conclusion": "In this work, we started by observing the limited 2D repro-jection accuracy of current HMR systems, which limits theirapplicability to augmented reality applications - e.g. virtualtry-on for garments and accessories. To address this we haveintroduced MeshPose, a system that bridges the DensePoseand HMR problems and substantially improves the imagereprojection accuracy of HMR, while retaining accurate 3Dpose. Beyond improved accuracy, our approach relies ona lightweight and simple architecture that consists of onlystandard neural networks layers. This makes our approacha natural fit for AR applications requiring real-time mobileinference.",
  "Angjoo Kanazawa, Michael J Black, David W Jacobs, andJitendra Malik. End-to-end recovery of human shape andpose. In CVPR, 2018. 2, 6": "Hoel Kervadec, Jihene Bouchtiba, Christian Desrosiers, EricGranger, Jose Dolz, and Ismail Ben Ayed. Boundary lossfor highly unbalanced segmentation. In Proceedings of The2nd International Conference on Medical Imaging with DeepLearning, pages 285296. PMLR, 2019. 5 Hoel Kervadec, Jihene Bouchtiba, Christian Desrosiers, EricGranger, Jose Dolz, and Ismail Ben Ayed. Boundary lossfor highly unbalanced segmentation. In Proceedings of The2nd International Conference on Medical Imaging with DeepLearning, pages 285296. PMLR, 2019. 16",
  "Timo von Marcard, Roberto Henschel, Michael J Black, BodoRosenhahn, and Gerard Pons-Moll. Recovering accurate 3dhuman pose in the wild using imus and a moving camera. InECCV, 2018. 6, 11, 12, 17": "Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, MingkuiTan, Xinggang Wang, et al. Deep high-resolution represen-tation learning for visual recognition. IEEE TPAMI, 43(10):33493364, 2020. 8, 12, 15, 16 Wenjia Wang, Yongtao Ge, Haiyi Mei, Zhongang Cai, Qing-ping Sun, Yanjun Wang, Chunhua Shen, Lei Yang, and TakuKomura. Zolly: Zoom focal length correctly for perspective-distorted human mesh reconstruction. In CVPR, 2023. 2",
  "Chun-Han Yao, Jimei Yang, Duygu Ceylan, Yi Zhou, YangZhou, and Ming-Hsuan Yang. Learning visibility for robustdense human body estimation. In ECCV, 2022. 2, 5": "Andrei Zanfir,Eduard Gabriel Bazavan,Hongyi Xu,William T Freeman, Rahul Sukthankar, and Cristian Smin-chisescu. Weakly supervised 3d human pose and shape recon-struction with normalizing flows. In ECCV, 2020. 2 Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang,Yebin Liu, Limin Wang, and Zhenan Sun. Pymaf: 3d humanpose and shape regression with pyramidal mesh alignmentfeedback loop. In ICCV, 2021. 2, 8, 16",
  "Supplementary Material": "In this appendix, we present additional results and abla-tions for our proposed MeshPose system. We also providemore technical details on our architecture and its training forreproducibility.We begin in Section A by showing more qualitative resultsacross multiple image datasets (COCO , 3DPW ,H36M and 3DOH ) to demonstrate the wide ap-plicability of our approach to multiple scenarios. We alsoprovide more results on videos to showcase the temporalstability of our method even without temporal smoothingpost-processing. We refer the readers to our mp4 video pro-vided in the zip file as our results are best viewed as videos.Then, in Section B.1, we ablate - with more metrics - thedesign choice for our novel VertexPose module that leveragesDensePose annotations to learn 2d vertex localization.We show first how our vertex-based representation (Vertex-Pose) compares to the UV-based representation introducedin DensePose in terms of DensePose metrics. Then,we evaluate multiple strategies to aggregate vertex UVs intopixel UVs to show the superiority of the barycentric UVaggregation.In Section B.2, we further evaluate our system with re-spect to occlusion on the 3DOH and 3DPW-OCC datasets.We demonstrate that MeshPose is robust to occlusion and onpar with other methods.Then, we provide a more comprehensive 2d evaluationof our approach with other competing methods by reportingmore metrics in Section B.3.In Section B.4, we quantitatively demonstrate our real-time inference speed on mobile device making our approacha prime candidate for AR applications.Finally, in Section C, we provide further details on ourarchitecture and its training. We detail the architecture ofour backbone networks, our losses and our decoding strategyallowing us to predict high resolution meshes from the low-poly topology used throughout our pipeline.",
  "A.1. Visualizations on COCO": "In , we showcase more results on the COCO dataset. Our approach demonstrates the ability to generateimage-aligned meshes even in challenging scenarios suchas occlusion or truncation of the body, which are commonfailure modes for other human mesh recovery systems. Un-like parametric methods bound to SMPL models, ournon-parametric mesh prediction approach, combined withDensePose supervision, offers greater flexibility and accu-rately captures very diverse body shapes that previous mod-",
  "A.2. Visualizations on 3DPW": "We present additional visualizations of our 3D mesh recon-struction on the 3DPW dataset in . In contrastto COCO , 3DPW showcases fewer occlusions, resultingin more full body meshes, and we thus focus our visualisa-tions on 3DPW occluded subset. We show that our 3D meshreconstruction is also competitive in these scenarios withmore accurate 2D reprojection (see elbows, shoulders, limbs)while offering strong depth prediction. We also present therendered visibility weights predicted by our system with acolor map ranging from green (fully visible - predicted bythe VertexPose branch) to red (non visible - predicted by theregression branch).",
  "A.5. Visualizations on Internet Videos": "To provide a complete qualitative analysis, we also evaluateour approach on videos of humans in action . A concate-nated video is available on our website. We also show acollection of frames in but we recommend watch-ing the results on the videos. We demonstrate very strongtemporal stability (low jitter) even when applied frame-by-frame without any post-processing. In the videos attached,only the detected bounding box is temporally-smoothed. Ourmethod is lightweight and simple with real-time inference,making it a prime candidate for AR applications.",
  "B.1. VertexPose Ablation Study": "We provide more metrics for introduced in the mainpaper. We report the standard Average Precision AP andAverage Recall AR. We also provide AP50 and AP75 whichmeasure precision at 50% and 75% IoU thresholds, APMand APL that evaluate precision for medium and large ob-jects. The same metrics are also used for AR. First, in orderto better understand the impact of training with vertex-based(VertexPose) representation compared to UV-based (Dense-Pose ) representation, we compare both approaches inTab. 4a evaluated in terms of DensePose metrics. For a faircomparison, we re-trained DensePose with our backboneand our training settings. We closely follow the DensePosemulti-head architecture introduced in . More specifically,for each pixel, we predict (i) the foreground segmentationmask I via the classification branch and (ii) the patch la-bel c and the corresponding [U, V ] on that patch via theregression branch. The patch label c is predicted by Pa 25-way (24 patches and background) classification unitc = arg maxc P(c|i) while the UV is predicted by the UVregressor Rc of the predicted patch c, [U, V ] = Rc(i).We observe that the VertexPose-based results compare favor-ably to their DensePose-based counterparts for all metrics,thus confirming the merit of the proposed approach. Thesame results are observed across all tested architecture (Mo-bileNet , ResNet , HRNet32 and HRNet48 ).In Tab. 4b we analyze the impact of the barycentric in-terpolation strategy proposed in .1.1 of the mainpaper. As a reminder, to decode the pixel UV, we com-pute the barycentric combination of the UV values ofthe vertices belonging to the face with the largest scoref = arg maxfF",
  ". Evaluation of the occlusion robustness of MeshPose com-pared to other state-of-the-art approaches in object-occluded bench-mark datasets 3DPW-OCC and 3DOH": "3DOH50K. 3DPW-OCC refers to a new test-set withoccluded sequences from the entire 3DPW dataset, while3DOH is a 3D human dataset with human activities occludedby objects, which provides 2D, 3D annotations and SMPLparameters. For a fair comparison following previous meth-ods we train our model by including 3DOH without 3DPW(since some videos of 3DPW-OCC are from the training set).From the evaluation results shown in , we seethat the proposed MeshPose performs also very well in oc-cluded scenarios by achieving state-of-the-art performanceand outperforming approaches that are designed to deal withocclusion. Moreover, in , we show that Mesh-Pose achieves good mesh reconstructions even in cases withheavy object-occlusions. These results demonstrate the ef-fectiveness of our proposed method that is able to deliverpixel-aligned 3D-mesh reconstructions and at the same timedeal successfully with occlusion.",
  "B.3. 2D Evaluation Benchmark": "In , we expand the analysis from the main paperwith additional evaluations with 2D metrics. For sake ofcompleteness, we include LVD , though this method isnot directly comparable as it has not been trained with in-the-wild datasets. We present the Average Precision (AP) andAverage Recall (AR) for 2D COCO keypoints in instanceswhere at least 80% of the keypoints are visible. This 80%threshold delineates a sub-dataset with instances that arealmost fully visible, but yet remains sufficiently large toallow for meaningful conclusions. APM and APL measuremedium (area between 322 and 962 pixels in the image) scaleand large scale (area above 962 pixels in the image) object detection precision, while ARM and ARL assess recall formedium and large scale objects. Additionally, we providethe AP and AR metrics for the DensePose task.Our approach surpasses the other methods in terms of 2Dmetrics, showcasing its strong 2D alignment.",
  "B.4. Inference setup and mobile inference": "For our desktop inference experiment, we assess the per-formance speed of the backbone network of each baselinemethods using the original authors official implementations.Each timing was evaluated on the same machine equippedwith a Nvidia Tesla V100 GPU.Our models are purely convolutional and thus run out-of-the-box on modern phones with accelerators. We exportedthe ONNX versions of our models and computed theirtimings (FPS) on an iPhone-12 using the CoreML back-end, obtaining comparable timings to the GPU-desktop tim-ings: 97, 99, and 153 FPS for Meshpose, MeshposeS, andMeshposeXS respectively.",
  "C. Architecture and training details": "In order to provide a comprehensive understanding of ourmethod outlined in the main paper, we present additionaldetails regarding the design of our pipeline and the trainingprocess. We begin by providing more details on the design ofour backbones in Subsection C.1. Following that, we includesupplementary details concerning our multi-head decodersin Subsection C.2. More precisely, we detail our vertex andvisibility regression branch (C.2.1), our custom silhouetterendering - both introduced in .2.1 of the mainpaper (C.2.2) and our high-poly mesh upsampler presentedin .2.3 of the main paper (C.2.3). Finally, in Sub-section C.3 we provide more details on our training strategyincluding datasets mixing, augmentations and scheduling.",
  "C.1. Backbone architectures": "Regarding our main backbone architecture of choice weemployed the HRNet-32 model described in , as it iscapable of producing a high-resolution feature map. Onlythe high-resolution, stride 4 output features of the last blockare used. Since MeshPose is a multitasking system whichoutputs tensors of a large dimensionality, we have modifiedthe architecture to have an output with more feature channels,without adding considerable overhead. More specifically,before the upsampling and the sum-based fusion of the fea-ture maps from the last block, which have different strideand feature size, we project all of them to a fixed featuresize of 256 instead of 32 that is used in the original HRNet-32 implementation. This modification only adds a smallnumber of parameters, since it is applied only at the end ofthe backbone, but it removes the 32-channel bottleneck toaccomodate the MeshPose tasks. For the Resnet50 and MobileNetV2 variants weused dilated convolutions on their last block, which givesfeatures with stride 16, and then we applied a decoder-netwith separable-convolutions and skip-connections from theprevious stages in order to produce the final feature mapwith stride 4. We found that this light-weight decoder-nethas much less parameters (due to separable convolutions)compared to the fully deconvolutional layers that are usuallyemployed in pose estimation .",
  "C.2. Decoders": "As explained in .2 of the main paper, we learnhow to directly regress 3D vertex positions V XY Zregfor allvertices. In addition to the 3D positions, we also predict avisibility label w for each vertex. The visibility windicates whether we should rely on the VertexPose-based2D position, V XYspor fall back to the V XYreg value. A low wvalue implies that the corresponding vertex is occluded orout-of-crop which means that the VertexPose vertex is likelyincorrect. The final MeshPose vertices are simply computedas a visibility-weighted average between both predictions:V XYmp = V XYspw+V XYreg (1w). In this subsection, we detailhow the regressed vertices V XY Zregare obtained and how wesupervise them together with their visibility predictions w.",
  "w = (avgz(f 1Dz(z(avgx,y(F)))))(11)": "We first apply an average pooling avgx,y across the spa-tial dimensions (x, y). Then, we apply two successive 1Dconvolutions i and f 1Dialong the indexed dimension i. Thefirst convolution i expands the dimension from C 1 toC C then f 1Ditransforms the feature tensors to V C dimension. Finally, for the visibility weight w, we averageacross the channel dimension avgz, then apply a sigmoid activation function to map the values between 0 and 1.To obtain the 3D vertex positions from the learnt fea-tures {Px, Py, Pz}, we apply argsoftmax over the C = 64",
  ". Evaluation of 2D accuracy in COCO-DensePose for both 2D keypoint predictions and DensePose regression": "channels. The resulting value of the argsoftmax will thusbe between 0 and 64 and thus needs to be mapped to pixelpositions. We map the range to [W, 2W] for X,[H, 3H] for Y and [2W, 2W] for Z. The top left pixelon the image corresponds to pixel . The new rangeexpands beyond the image boundary to predict out-of-cropvertices. We note that to accommodate for selfie-like images,we consider a larger range for Y ([H, 3H]): this allowsus to be able to predict the position of leg vertices that willoften lie significantly below the crop.",
  "C.2.2Custom Silhouette Rendering": "The learnt visibility weight w is partly supervised by the 3Dlocalization and the edge losses (see .2.4 and Fig-ure 12). We also use a binary-cross entropy loss LW usingthe supervision of the mesh pseudo-ground truth. However,we also want to leverage the ground truth DensePose seg-mentation masks which provide a suitable signal to learnvisibility with weak supervision. To achieve that, we intro-duce a novel silhouette rendering module by modifying thesoft rasterization method of SoftRas so that it incorpo-rates the predicted vertex visibilities. More specifically, foreach pixel i, we compute the silhouette Is by:",
  "j(1 wijDij)(12)": "Here, as in , Dj denotes the influence of triangle fj atpixel i and mostly depends on the distance of triangle j topixel i. Contrary to , we also multiply the influence Djby the visibility weight wij of face j at pixel i. wij is simplycomputed as the linear interpolation between the visibility",
  ". Regularization losses used for 3D mesh supervision": "weights of the three vertices of face j at pixel i. The newlyadded coefficient wij modifies the initial rendering pipelinefrom to ignore faces with low visibility weight.We use two losses to supervise our rendered silhouetteIs. First, we use a simple L2 loss between our renderedIs and the corresponding ground truth from the DensePoseannotation IDPs: L1Is = ||Is IDPs||2. The second loss weintroduce is inspired from the boundary loss from :",
  "C.2.3High Poly Mesh Upsampler": "As mentioned in the main paper, we predict the vertices of alow-polygon approximation (518 vertices) of our high res-olution body mesh (6890 vertices). As pointed out in ,working on a lower-resolution mesh both reduces memoryusage while improving training stability by limiting corre-lated vertices. To generate this approximation, we used acustom-defined mesh to reduce the number of vertices inundesirable high curvature areas (hands, fingers).To obtain the high-poly mesh from the low-poly mesh ,which is the output of the network, we trained a multilayerperceptron upsampler following the ideas of . Weemployed two affine layers to progressively upsample thelow-poly mesh to the high-poly variant (with an intermediaterepresentation of 1723 vertices as in ). We applied anL1 loss between the ground-truth and the upsampled ver-tices at each stage, as well as between the GT joints andthe joints that are estimated using a landmarker on the up-sampled version. We trained the upsampler independentlyfrom the main network using the high-poly mesh pseudoGT annotations from only COCO, Human 3.6M andMPI-INF-3DHP datasets to accurately cover a large posedistribution. Our method is thus agnostic to the high polytemplate and only the upsampler would need to be tuned fora different template to be used.An additional improvement to the training of the upsam-pler, compared to previous approaches, is the addition ofsynthetic noise during the training to make the upsamplermore robust to noise or to small errors that may be in thelow-poly mesh. More specifically, with probability 0.5 werandomly added to 25% of the vertices spikes equal to 15%of their vertex position values (after applying root alignmentto the mesh). In the other case, with probability 0.5 we addedgaussian noise with standard deviation 5% to each vertexposition.For comparison, a nearest-point-on-triangle, non-learnedupsampler produces slightly worse results (1mm differencefor 3DPW MVE, 2 for DP AP).",
  "We used a combination of datasets and training signals inour model training. More specifically, we used a mixture of": "COCO and Human Mesh Reconstruction (HMR) datasets.First, we enriched the COCO (train2017) dataset with meshpseudo-annotations and their DensePose annotations. For theHMR datasets, we combined Human3.6M , MPI-INF-3DHP and 3DPW training sets. For each dataset,we proceeded to image augmentation with (i) flipping , (ii)rotation (between 45 and 45), (iii) scale (between 0.75and 1.25) and (iv) color augmentations. We followed a 40/60 split between COCO and HMR datasets. To achievethat, we upsampled each dataset by an associated factor tocontrol the dataset mixture: 5 for COCO, 4 for Human3.6M,1 for MPI-INF-3DHP and 2 for 3DPW. We trained with theAdam optimizer for 200 epochs with a mini-batch size of 96and a learning rate of 1 103, which is reduced by a factorof 10 after 100 epochs. The weights of the backbone of oursystems are initialized with pretrained 2D pose estimationnetworks. We used Linux machines with 4 Nvidia TeslaV100 GPUs (16GB) for all of our experiments.We aggregated the different losses based on the followingweighted linear combination:",
  "L = LBL + Lconsistency + 10 LW + 0.1 LV+LE + 0.1 LN + LJ + LIs(16)": "with LBL the barycentric cross-entropy loss (.1.2),Lconsistency the UV consistency loss (.1.2), LWthe visibility binary cross-entropy loss (Section C.2.2),(LV , LE, LN) the vertex localization, the edge, the normaland the joint localization losses (.2.4) and LIs thesilhouette loss (Section C.2.2).We note that (LV , LN) are given smaller weights todownscale the importance of the pseudo-ground truth. Oursystem however requires higher weight for LE to removemesh artefacts."
}