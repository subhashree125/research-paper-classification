{
  "Abstract": "Dataset condensation (DC) methods aim to learn asmaller, synthesized dataset with informative data recordsto accelerate the training of machine learning models. Cur-rent distribution matching (DM) based DC methods learna synthesized dataset by matching the mean of the latentembeddings between the synthetic and the real dataset. How-ever, two distributions with the same mean can still be vastlydifferent. In this work, we demonstrate the shortcomings ofusing Maximum Mean Discrepancy to match latent distri-butions, i.e., the weak matching power and lack of outlierregularization. To alleviate these shortcomings, we proposeour new method: Latent Quantile Matching (LQM), whichmatches the quantiles of the latent embeddings to minimizethe goodness of fit test statistic between two distributions.Empirical experiments on both image and graph-structureddatasets show that LQM matches or outperforms previousstate of the art in distribution matching based DC. Moreover,we show that LQM improves the performance in continualgraph learning (CGL) setting, where memory efficiency andprivacy can be important. Our work sheds light on the ap-plication of DM based DC for CGL.",
  ". INTRODUCTION": "As the world becomes more connected, the available dataincreases exponentially. In order to learn knowledge thatis generalizable to all the available data, large and complexdeep models are proposed. The immense computational costof training these models hinders their development, which isundesirable. Next, real-world datasets may contain sensitivedata. These data cant be made public due to the privacyconcerns. This hinders the research transparency and re-producibility. Dataset condensation (DC) can solve theseissues by reducing the size of the training set. Instead of alarge training set with potential redundant data records, DCmethods aim to generate a small, synthetic dataset that ishighly informative, which is also shown to be more privacy- preserving . The deep learning models trained on thesynthetic dataset should have similar evaluation performancecompared to the models trained on the original, large train-ing set. DC is applied to many problems, such as continuallearning (CL) , federated learning and neuralarchitecture search . DC improves the state-of-the-art(SOTA) performance of CL with graph-structured data sig-nificantly . However, the studies on applying DC tocontinual graph learning (CGL) problems are still scarce. Our work focuses on improving DC and its applicationon CGL, we hope that our work sheds light on this under-explored field. We use the terms real dataset and originaldataset interchangeably to refer to the dataset to condense.Based on Sachdeva and McAuley , the SOTA datasetcondensation methods are categorized into four categories:1) Meta-Model Matching, 2) Gradient Matching, 3) Trajec-tory Matching, 4) Distribution Matching. The former threecategories involve expensive bi-level optimization to achievethe goal of DC. I.e., they train a model for each optimizationstep of the synthetic dataset. Our work focuses on the lastcategory, the distribution matching (DM) based DC methods.These methods do not have the costly bi-level optimization.Instead of comparing the distance between the learned pa-rameters, or the gradients of the trained models, DM basedDC methods initialize different models for each optimiza-tion steps, extract latent embeddings of both real and thesynthetic dataset, and matches their distribution.SOTA DM based DC methods use the Maximum MeanDiscrepancy (MMD) to measure the distance between syn-thetic and real latent embeddings . We note thatcomparing the mean of the latent embeddings is equivalentto comparing the sum of the mean of each individual latentfeature distribution in the latent embedding. In the remainderof this work, we focus on the comparison between individuallatent feature distributions. MMD only matches the mean ofthese distributions (i.e., first-order moment). As it does notprovide any guarantee about other aspects of the matcheddistributions, they can still differ in statistics such as thevariance, skewness, and kurtosis. This issue was identified",
  ". The empirical cumulative distribution function (ECDF) of a latent feature of class 0 in CIFAR-10 after 1900 epochs of training": "by Zhao et al. . However, the author still used MMD asthe distance function despite this shortcoming. As an em-pirical evidence, an example where the MMD fails to matchthe latent distribution is shown in a. In the syntheticdataset generated by MMD, there is one sample with veryhigh value, while other samples have relatively low values.This matches the mean correctly, but it does not reflect thereal distribution. The synthetic feature with the highest valueis also larger than the maximum value in the real dataset,this may harm the learning process of the model.Based on this observation, we believe that replacingMMD with a more suitable metric that reflects the distancebetween the distributions better, can improve the perfor-mance of DM based DC methods. We are inspired by good-ness of fit tests in the field of statistics, which quantifiesa distance between two empirical cumulative distributionfunctions (ECDF). Specifically, we propose a novel DMbased DC method: Latent Quantile Matching (LQM) basedon the two-sample Cramer-von Mises (CvM) test . Itestimates the squared difference between the two ECDFs.It is empirically shown to be more powerful than thewidely used Kolmogorov-Smirnov test , and there is asimple way to minimize the CvM test statistics for any num-ber of data records k based on the optimal k-point discreteapproximation studied in Kennan .The objective of LQM is to minimize the difference be-tween synthetic data records and optimal quantiles k in thelatent space, which corresponds to the optimal k-point dis-crete approximation of the original latent distribution. Thisobjective is similar to the Quantile Regression (QR) problemin the field of statistics . However, the objective of QRis to express the quantiles of the distribution of the responsevariables as functions of the observable input variable. Ourapproach does not explicitly learn a model which predictsthe values at a certain quantile. Instead, LQM tries to learn asmall synthetic dataset that matches the different quantilesof the real dataset in the different latent spaces extracted bythe feature extractors. Our approach tries to minimize the difference between synthetic latent features and the quan-tiles of the real latent features. In b, we show thatthe objective function is defined by the horizontal differencebetween the blue ECDF and the optimal green ECDF. As aresult, the synthetic dataset produced by LQM discouragesany outliers and approximates the real distribution better.Our contributions are as follows. We demonstrate the shortcomings of Maximum Mean Dis-crepancy as a metric to measure the distance between twoempirical distributions. We reveal that two distributionwith the same mean may not be similar. We propose a novel distribution matching based datasetcondensation method: Latent Quantile Matching. We learna small set of synthetic samples which match the differentquantiles of the original dataset in the latent space. Thequantiles are chosen based on the optimal k-point discreteapproximation of the original latent distribution . We extensively evaluate LQM on image and graph datasets.Compared to previous studies, our experiments show theefficacy of our method with different data structure. Next,the experimental results show that the model trained ondataset learned by LQM outperform the models trainedon dataset learned by MMD on most datasets. I.e., theperformance gap between the model trained on the smallersynthetic dataset and the full, original dataset is reduced.",
  ". RELATED WORKS": "Dataset Condensation. Dataset condensation (DC) aims tocondense large training datasets into small synthetic datasets,while preserving the same evaluation performance when themodel is trained only on the synthetic datasets. Wang et al. proposed a way to maximize the performance of modeltrained using the synthetic dataset by using meta-learning. Subsequent studies introduced different techniques such assoft-label , gradient-matching , data-augmentation, trajectory matching and data-parameterization .Despite their good performance, most of the methods relyon bi-level optimization involving second-order derivatives,which is computationally expensive.On the contrary, Distribution Matching (DM) pro-poses to condense the large training dataset into a syntheticdataset by matching the latent distribution between the twodatasets using different, randomly initialized feature extrac-tors. The distance between the latent distributions is mea-sured by the Maximum Mean Discrepancy (MMD). DMdoes not require bi-level optimizations and is less compu-tationally expensive compared to other approaches. Sub-sequent studies outperform DM by introducing techniquessuch as partition and expansion augmentation , trainedfeature extractors and attention matching . Despitethe improvement in performance, all the above-mentionedmethods use MMD to measure the distance between the twodistributions. This makes them vulnerable to the shortcom-ing shown in a. In this work, we replace MMD with adistance function inspired by the goodness of fit tests in thefield of statistics. Our distance function only reaches zeroif the synthetic latent distribution is close to the specifiedquantiles of the original latent distribution for all classes.Recently, Liu et al. also studied the shortcomingsof MMD, and replaces it with more powerful metrics. Ourwork is concurrent with theirs. Their work is based on op-timal transport theory and uses Wasserstein Barycenter as the matching metric. The computation of WassersteinBarycenter is computationally expensive , and they usedan approximation algorithm proposed by Cuturi and Doucet to reduce it. In contrary, our work is not computation-ally expensive as it is based on statistical goodness of fit testsand uses the mean squared errors to the target quantiles asthe metric. The computation of quantiles of an empiricaldistribution is not computationally expensive. By sortingthe empirically drawn samples, we can retrieve all the quan-tiles of an ECDF. As an example, the time complexity ofquicksort is O(nlog(n)) .Goodness of Fit Tests. In statistics, a goodness of fittest measures how well a set of observed values fits a givenstatistical model. A subset of the tests quantifies the dis-tance between two ECDFs. The KolmogorovSmirnov (KS)test measures the maximum difference between the twoECDFs. The Cramervon Mises (CvM) test estimates thesquared differences between the two ECDFs. The Anderson-Darling (AD) test extends the CvM test by assigningmore weights to the tails of the distributions. Previous stud-ies show that the CvM and AD test are more powerful thanthe KS test . Compared to MMD, these goodness offit tests can match distributions beyond the first-moment.Our work uses optimal quantiles that minimize the CvM test statistic (CvM stat), proved by Kennan and reportedin Barbiero and Hitaj , as the regression target for thesynthetic dataset. This ensures the similarity of the syntheticand the real latent distributions.Continual (Graph) Learning. Continual Learning (CL)is a research field that benefits from DC. CL aims to builddeep models that can acquire knowledge across differenttasks without retraining from scratch. Amongst differentCL methods, replay based methods obtain exceptional per-formances . They store a fraction of thedata from the past tasks as memory buffer and replay themduring the training of the new tasks. However, they requireadditional storage space and may raise privacy concerns.The advancements in dataset condensation show a promisingdirection to solve these problems. By condensing the dataset,the storage overhead can be reduced. Next, the privacy issueis alleviated, as the synthetic dataset is an obscured ver-sion of the real dataset, which is shown to be safe againstmembership-inference privacy attacks . To the best ofour knowledge, CaT-CGL is the only work that studiesDC in Continual Graph Learning (CGL). We incorporate ourmethod into CaT-CGL and outperforms their method.",
  ". PRELIMINARIES": "Dataset Condensation. Given a large training set T =(x1, y1), ..., (x|T |, y|T |) containing |T | data records andtheir labels, DC aims to synthesize a smaller set S =(s1, ys1), ..., (s|T |, ys|T |), |S| |T |, such that the deep mod-els trained on S maintain similar evaluation performance asmodels trained on T . Let x be a sample from the real datadistribution PD with label y, T and S be two variantsof the same model with parameters trained on datasetT and S, respectively, and L denotes the loss function (e.g.,cross-entropy loss). The objective of DC is to find the idealdataset S defined by the following equation.",
  "S = argminSD((T ), (S))(2)": "Where D denotes a distance function which takes as inputthe latent embeddings extracted by randomly initialized net-works with parameter P0. Compared to the generalobjective of DC, described in Eq. (1), DMs objective func-tion does not require the model to be trained. Instead, DMattempts to learn the synthetic dataset with the same distri-bution as the real dataset with randomly initialized networks. This surrogate objective makes the computational cost ofDM based DC methods lower than the approaches that re-quire bi-level optimization. When the distance function D isMMD, the objective is defined as:",
  "j=1(sj)|| (3)": "Continual (Graph) Learning. The two most used set-tings in CL are Task-incremental (Task-IL) and Class-incremental (Class-IL). We focus on the more difficultClass-IL setting . We assume a sequence of B tasks:{D1, D2, D3, ...DB}. Db = {(xbi, ybi )}nbi=1 denotes the b-thincremental training task with nb data instances. Each taskD contains data instances from one or more classes c C.We denote function Cls as a function that maps the task tothe classes of the data instances it contains. The classes ineach task D do not overlap with any other tasks defined inthe sequence. Thus:",
  "Cls(Da) Cls(Db) = , a, b {1, ..., B}, a = b(4)": "During the b-th incremental training process, the model onlyhas access to the data instances from the current task Db. Re-hearsal based methods can still access their memory buffer.After each incremental training process, the model is evalu-ated over all seen classes Cbseen = Cls(D1)...Cls(Db).Class-IL aims to find the model f(x) : X C that mini-mizes the loss over all the tasks in the sequence.",
  ". METHODOLOGY": "The base of our method lies on the two shortcomings that weobserved with dataset condensation methods that use Max-imum Mean Discrepancy (MMD) to measure the distancebetween the synthetic and the real latent embeddings. Inthis section, we explain the two shortcomings in detail, showthat they can harm the distribution matching objective, andpropose our new method: Latent Quantile Matching thatalleviates the shortcomings.",
  ". Shortcomings of MMD": "Inadequate distribution matching power. As the nameMaximum Mean Discrepancy suggests, MMD only matchesthe mean of the empirical distributions (i.e., first-order mo-ment). A previous study has shown that, in the universalreproducing kernel Hilbert space (RKHS), asymptotically,MMD is 0 if and only if PX = PY . Characterizing thefunction space explored by neural networks is a difficult andongoing field of study. Recent works show that the functionspace of fully connected layers can be defined as trainableladders of Hilbert spaces , or as reproducing kernel Ba-nach space . However, there are no studies that charac-terizes function spaces explored by the convolutional layers or graph neural networks yet. Our empirical observation ina suggests that in DC, there are cases where the meanof the latent feature distribution matches, but still differs inother statistics. This suggests that the feature extractors doesnot map the input to the universal RKHS. Therefore, a metricwith stronger distribution matching power may be beneficialto the dataset condensation process.No penalization for outliers/extreme values. The syn-thetic dataset is initialized by randomly choosing existingdata records in the original dataset. Thus, it is probable thatthe initial synthetic dataset contains data records that yieldextreme values in the latent space. During the distributionmatching process, they may become more extreme, as theobjective of MMD only matches the mean of the compareddistributions. An example is shown in a. In the syn-thetic dataset, the largest value for the shown latent featureis larger than the corresponding largest value in the originaldataset. They provide little information and waste valuablememory space that is limited for the synthetic dataset.",
  ". Proposed Method": "To address the abovementioned shortcomings, we proposeour new method: Latent Quantile Matching (LQM) whichlearns a synthetic dataset that minimizes the mean squarederror between the latent features in the synthetic dataset andthe target quantiles of the latent feature distributions in thereal dataset. The objective of LQM is to find the optimalsynthetic dataset S as defined in Eq. (2), where D denotesthe sum of CvM test statistic (CvM stat) of the latent featuredistributions. However, computing the CvM stat for eachfeature distribution for each epoch is time-consuming. Next,if we use it as the loss function, it will never reach zero.A zero CvM stat denotes that the ECDF of the syntheticlatent distribution is the same as the ECDF of the real latentdistribution, this is only possible if the number of data in thesynthetic dataset is equal or larger than the number of datain the real dataset. Thus, a zero CvM stat is impossible andnot desired in the scenario of DM based DC.To establish an achievable objective, we utilize the opti-mal k-point discrete approximation of any distribution, asproposed in Kennan . We define the set of optimal quan-tiles Q that minimizes the CvM stats for a synthetic datasetwith k data records:",
  "2k}(5)": "The ECDF constructed using values from these quantileshas the smallest CvM stat to the original distribution. Theproof of this property can be found in Barbiero and Hitaj .Using this property, and the fact that we have a predefinedmemory budget for the synthetic dataset, we can computethe quantiles that corresponds to the optimal points that mini-mizes the CvM stats for each latent feature distribution. This",
  "Algorithm 1: Distribution Matching + Latent Quantile Matching": "Input:Training set TParams :Randomly initialized set of synthetic samples S for C classes, |Sc| = c for c C, model parameterizedwith , probability distribution over parameters P, train iterations K, learning rate , quantile functionFq(Q, E) : RDF R|Q|F , Q is the set of quantiles, E is the batch of embeddings with D data recordsand F features, sort function Fs(E) : RDF RDF sorts feature vectors F in the ascending order.",
  "cC ||Fq(Q, (Tc)) Fs((Sc))||2 (6)": "Where c C, denotes the classes in the datasets. Q denotesthe optimal quantiles computed using Eq. (5). Fq(Q, E) :RDF R|Q|F denotes a quantile computation functionwith a set of quantiles and a series of embeddings as inputs,and outputs the quantiles for each latent feature. Fs(E) :RDF RDF denotes a sort function that sorts eachlatent feature in the input embeddings in ascending order.LQM minimizes the distance between the latent feature in thesynthetic dataset and the optimal quantiles that minimizes theCvM stat by sorting the synthetic latent feature and aligningthem with the corresponding values of the optimal quantilesof the original latent feature distribution.As our method only replaces the distance computation inthe distribution matching process, it can be implemented ontop of any existing DM based DC methods that comparesthe distance between two distributions. In Algorithm 1, theprocedure of the proposed method, build on top of the basicdistribution matching based DC algorithm is shown.Limitation. As the initial data records are selected ran-domly, when the memory budget becomes larger, the initialsynthetic latent distribution will already be similar to theoriginal latent distribution. This diminishes the advantage ofLQM as the difference in high-order moments between thesynthetic and real latent distributions is smaller. Therefore,the impact of the first shortcoming of MMD is smaller. Next,when the budget is large, the impact of few outliers being selected in the initialization process and kept in the syntheticdataset is less severe. There are many other data records withlatent features within the normal range, which will providethe model with enough information. Thus, when the syn-thetic dataset is large, the improvement of LQM comparedto MMD may be less noticeable.",
  ". EXPERIMENTS": "To validate the performance of LQM empirically. We imple-mented LQM on top of the SOTA DM based DC methods:Improved Distribution Matching (IDM) and Condenseand Train (CaT) for image and graph-structured data,respectively. For DC with graph-structured data, we use theframework implemented by CaT-CGL to evaluate ourmethod in both the normal dataset condensation setting andin the continual graph learning (CGL) setting. CaT-CGLframework is implemented for CGL, However, the normalDC setting can be imitated by considering the full datasetas one task. We validate that LQMs performance in CGLby implementing it in Cat-CGL, and compare it with CaT: aCGL method that utilizes DM based DC with MMD as dis-tance function . Additionally, we include two baselinesfor the CGL experiments: 1) Finetuning, 2) Joint. They referto training a model in CGL setting without any CGL method,and training a model using the full dataset, respectively.Datasets. For the evaluation with image data, we useddatasets of various complexity and sizes. CIFAR-10, CIFAR-100 and TinyImageNet . The task is to classify eachimage to the correct category. For graph-structured data, weused datasets from different domains. CoraFull , Arxiv, Reddit and Product graph datasets were usedin our experiments. The tasks of these datasets are to classifyeach node into their corresponding category.The details of the used datasets are reported in Sec. 5 and Sec. 5. The budget per task row in Sec. 5 refers to the totalnumber of nodes that we store in the synthetic dataset foreach task in the CGL experiment. The budget is chosen suchthat each task is condensed to 1% of the original size.",
  ". Statistics of the experimented graph datasets": "Experimental settings. For both experiments, we evalu-ate the classification performance of deep learning modelsthat are trained on the synthetic dataset learned by LQM. Allexperiments are run on a Tesla V100-SXM3-32GB GPU.For image datasets, we learn 1/10/50 synthetic imagesper class. The training set is used to generate the syntheticdataset. The default hyperparameters of Zhao et al. are used for all of our experiments. The performance ismeasured by the mean accuracy and standard deviation of 5runs, where the models are randomly initialized and trainedon the condensed synthetic dataset and evaluated on the testset of the real dataset.For graph datasets, we learn a synthetic dataset that is 1%of the size of the original dataset. The number of nodes foreach class is proportional to its ratio in the original dataset. Inaddition, a continual graph learning setting is used to showthat LQM improves the performance in a more complexlearning settings. Our work focus on the class-incrementalCGL, where the model does not know from which task asample belongs to. This is more difficult and realistic thanthe task-incremental setting, where the task identifier is avail-able to the model at any time. To transform the dataset intothe class-incremental setting, we first divided each datasetinto train, validation, and test sets using a 6:2:2 ratio. Next,each set is divided into tasks. Each task only contains dataof two unique, non-overlapping classes. The objective isto condense each task into a small synthetic dataset that is1% of the original size. We use the default hyperparametersof Liu et al. for all of our experiments. Performanceis measured by the mean and standard deviation of average accuracy (AA) and backward transfer (BWT) of 5 runs. Ineach run, a model sequentially learns different tasks on thecondensed dataset.The average accuracy is defined as the average of accura-cies Ak,i for each previously learned task i. It measures theaverage performance of the model across all learned tasks.",
  ". Image datasets": "We compare the performance of IDM+LQM against theSOTA dataset condensation (DC) methods in Tab. 3 and theCvM test statistic (CvM stat) between the synthetic and thereal dataset for IDM and IDM+LQM in . We show theimages in the synthetic dataset learned by IDM+LQM in theappendices. In Tab. 3, we include the performance of bi-leveloptimization based DC methods to show that distribution matching (DM) based DC methodscan match their performance. However, the focus of ourwork is on the comparison between our method IDM+LQMand other DM based DC methods . IDM+LQMoutperforms both the reported and our replicated results ofIDM on CIFAR-10 and CIFAR-100 for all synthetic datasetsize. On TinyImageNet, IDM+LQM match the replicatedperformance on 1 and 10 image per class, and underperformsin the 50 image per class setting. CIFAR10 (1ipc) CIFAR10(10ipc) CIFAR10(50ipc) CIFAR100 (1ipc) CIFAR100 (10ipc) CIFAR100 (50ipc) Cramer Von Mises distance 0.20 1.87 4.93 1.40 5.87 14.37 0.17 1.62 3.66 1.99 3.14 7.97 method IDMIDM+LQM TinyImageNet (1ipc) TinyImageNet (10ipc) TinyImageNet (50ipc) 27.45 115.40 357.63 34.50 143.89 390.03method IDMIDM+LQM . Comparison of average Cramer-von Mises stats betweenthe synthetic dataset and the real dataset for each latent featuredistribution. Lower number denotes higher probabilities that thecompared samples (synthetic and real dataset in latent space) aredrawn from the same distribution. The latent features are extractedby a pretrained model on the synthetic dataset.",
  "OURSIDM+LQM45.90.660.90.370.20.127.20.447.70.352.40.410.40.320.80.424.30.3": ". Comparison of test accuracy of randomly initialized model trained on condensed dataset. We evaluate our method on three differentdatasets with different numbers of synthetic images per class. Img/Cls: number of images per class. Ratio(%): the ratio of condensed imagesto the whole training set. Full Dataset: the accuracy of the model trained on the whole training set. The bold results are the best performanceof the distribution matching-based dataset condensation methods within the margin of error, excluding Full Dataset baseline. The bestresults of bi-level optimization based approaches are underlined. IDM+LQM uses the default parameter provided by IDM . the synthetic and real dataset in the latent space is lowerednoticeably, i.e., with 10 and 50 image per class. This supportsour hypothesis that using a distribution matching metricbeyond the first moment yields better DC performance.Next, we note that all our experiments are performed us-ing the default hyperparameters provided by IDM, except thetraining epoch of the network on the synthetic dataset duringthe evaluation. We increased it from 1000 to 2000 to ensurethe model learns the synthetic dataset produced by LQMoptimally. As LQM changes the loss function of the method,the hyperparameters used by IDM with MMD may not beoptimal to IDM with LQM anymore. Combined with thelimitation we discussed in Sec. 4, our LQM underperformson the TinyImageNet dataset. Due to the time constraint,we were unable to optimize the hyperparameters. However,we believe that the performance of IDM+LQM in CIFAR-10 and CIFAR-100 is already sufficient to demonstrate theefficacy of our new method.",
  ". Graph datasets": "To show that the efficacy of our method on complex learningsettings, we compare the performance of CaT+LQM withother CGL baselines in Tab. 4. We experiment in two dif-ferent settings, as indicated in the brackets on the categorycolumn. They are explained in Sec. 5. Our experimentsuse the default hyperparameters provided by CaT. Thus, theperformance of CaT+LQM may benefit from further hy-perparameter optimization. However, CaT+LQM alreadyconsistently matches or outperforms the replicated result ofCaT that uses MMD. In the one task setting, CaT+LQMconsistently outperform CaT. This validates that our method generalizes to graph-structured data. In the CGL setting,CaT+LQM has a higher average accuracy with similar orless negative impacts on past tasks in CGL setting, which isdenoted by the higher backward transfer metric.We observe that the improvement of AA is larger whenthe synthetic dataset is small, corresponding to the limitationwe discussed in Sec. 4. On CoraFull and Arxiv, we have 4and 29 nodes for the synthetic dataset per task, respectively.Compared to the replicated results, LQM improves AA by2% and 1.4%, respectively. For larger datasets: Reddit andProduct, where we have 40 and 318 nodes for the syntheticdataset per task, respectively, the improvement of LQM isless noticeable. LQM is 0.1% worse and 0.7% better in AAcompared to the replicated results of CaT. arxivcorafullproductsreddit0.0 0.5 1.0 1.5 2.0 2.5 0.00 0.28 2.54 2.80 0.010.00 1.78 0.06 method CaTCaT+LQM . The average percentage of extreme latent values foreach class. A value of one denotes that one percent of the latentfeatures in each class in the synthetic dataset exceed the maximumor dropped below the minimum of the corresponding class latentdistribution in the real dataset. The latent features are extracted bya pretrained model on the synthetic dataset.",
  "OURS (CGL)CAT+LQM68.10.2-8.70.368.00.3-10.70.297.10.0-0.50.071.00.2-4.90.2": ". Comparison of AA and BWT of dataset condensation based CGL methods in Class-IL setting without inter-task edges. The boldresults are the best performance excluding Joint baseline. denotes the greater value represents greater performance. Two different settingsare expressed in the category column. CGL setting splits dataset into tasks with two classes per task, one task setting considers whole datasetas one task, which imitates the usual dataset condensation setting. In one task setting, BWT is not available as the model only learns onetask, thus there is no transfer of knowledge between the tasks. outliers, the model trained on these tasks will overfit to theoutliers, which negatively impacts the generalization towardsfuture tasks. In the non-continual learning experiments, allthe data are available at the start, the model must generalizeto all the samples at once. Thus, the effect of the outlierin the synthetic dataset is smaller. Our method penalizesthe outliers explicitly, which results in better performancein the CGL setting. In , we show the average numberof latent features of each class in the synthetic datasets thatis higher than the maximum or lower than the minimumof the corresponding latent feature in the real dataset ofthe same class. We observe that CaT+LQM has low(er)number of extreme values in the latent feature. However,this does not fully explain why Arxiv performs much betterthan Reddit. We observe that CoraFull and Arxiv both havea low number of edges per node. As the graph convolutionallayer aggregates neighboring features, the extreme valueswill be smoothed out. When there are many neighbors, themajority of the neighbor features will not be extreme values.On CoraFull and Arxiv, the smoothing effect is not strong,thus, LQM outperforms the MMD counterpart.",
  ". CONCLUSION": "We propose a novel distribution matching (DM) baseddataset condensation (DC) method: Latent Quantile Match-ing (LQM). It alleviates two shortcomings of the widely usedMaximum Mean Discrepancy (MMD) based loss function,i.e., inadequate distribution matching power and lack of pe-nalization for outliers/extreme values. Our method choosesspecific values which the synthetic dataset should be alignedto based on the optimal k-point approximation that mini-mizes the Cramer-von Mises (CvM) test statistics betweenthe two distributions, as studied in Kennan and reportedin Barbiero and Hitaj . This alleviates the first shortcom-ing, as the goodness of fit tests matches the distributions onhigher-order moments, while MMD only matches the mean, or the first-order moment. It also alleviates the second short-coming, the extreme values will be penalized more as theyare located further from the values at the optimal quantiles.As the core of our method is the adapted computationof the distance between two distributions, we can easily im-plement it on top of other DM based DC methods that useMMD . Extensive empirical experimentson both image and graph datasets show that LQM outper-forms or matches the performance of the counterpart thatuses MMD. Moreover, the improvement of LQM is morenoticeable with constraint memory budgets in the continualgraph learning (CGL) setting. The severity of the two short-comings increases as the memory budget decreases, sincethe outlier will have a higher influence within the limitedsynthetic dataset. This effect is more noticeable in CGL asthe model can only learn a subset of the dataset for each task,therefore, the model is more likely to overfit to the outliersin the synthetic dataset. Thus, the improved result of LQMin CGL at low memory budget shows that LQM effectivelyalleviates the two shortcomings. As the objective of DC isto create small synthetic datasets, the advantage of LQMcompared to MMD is going to be more prominent in futureresearch. This property makes LQM a future direction onfollow-up research regarding the DM distance functions inDM based DC methods.Some future works include: 1) Evaluate the performanceof LQM with other DM based DC methods. 2) Incorporatea heuristic based initialization procedure for the syntheticdataset to improve the performance. 3) Study the possibilityof using other statistical metrics for DM based DC.",
  "Frank J Massey Jr. The kolmogorov-smirnov test for goodnessof fit. Journal of the American statistical Association, 46(253):6878, 1951. 2, 3": "Ameya Prabhu, Philip HS Torr, and Puneet K Dokania.Gdumb: A simple approach that questions our progress incontinual learning. In Computer VisionECCV 2020: 16thEuropean Conference, Glasgow, UK, August 2328, 2020,Proceedings, Part II 16, pages 524540. Springer, 2020. 3 Nornadiah Mohd Razali, Yap Bee Wah, et al. Power com-parisons of shapiro-wilk, kolmogorov-smirnov, lilliefors andanderson-darling tests. Journal of statistical modeling andanalytics, 2(1):2133, 2011. 3",
  "Michael A Stephens. Edf statistics for goodness of fit andsome comparisons. Journal of the American statistical Asso-ciation, 69(347):730737, 1974. 2": "Felipe Petroski Such, Aditya Rawal, Joel Lehman, KennethStanley, and Jeffrey Clune. Generative teaching networks:Accelerating neural architecture search by learning to gener-ate synthetic training data. In International Conference onMachine Learning, pages 92069216. PMLR, 2020. 1 Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang,Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, andYang You. Cafe: Learning to condense dataset by align-ing features. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1219612205, 2022. 1, 6",
  "A. Ablation study": "Choices of goodness of fit tests.In Barbiero and Hitaj , the optimal quantiles of the CvM and AD test statistics arereported. The optimal quantiles for AD test statistics can be obtained by an iterative process. We report the process proposedby Barbiero and Hitaj in Appendix B. The quantiles obtained will minimize the AD test statistic, which is a variant of theCvM test statistic that gives more weight to the tails of the distribution. We have experimented with the optimal quantiles thatminimize the AD test statistic with graph structured data. The result is shown in Appendix A.Compared to the performance of optimal CvM quantiles, the optimal AD quantiles does not provide noticeable improvementsacross the four experimented dataset. In the one task setting with graph structured data, the variant that uses optimal CvMquantiles consistently outperform the variant that uses optimal AD quantiles. Next, the computation of CvM quantiles does notrequire an iterative process. Thus, the use of the CvM test is more suitable for DM based DC.",
  "C. Visualizations of synthetic image datasets learned by IDM+LQM": "We visualize the synthetic image dataset learned by IDM+LQM from to . We observe some repetitive dot patternsin the synthetic datasets learned in 1 image per setting, i.e., in and . In the corresponding 10 image per class settingfor TinyImageNet demonstrated in , this is less severe. This may indicate that in 1 image per class setting, the quantilescant be matched perfectly if we want to maintain the image details."
}