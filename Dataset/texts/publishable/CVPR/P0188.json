{
  "Abstract": "Recent advances in pre-trained vision transformers haveshown promise in parameter-efficient audio-visual learningwithout audio pre-training. However, few studies have in-vestigated effective methods for aligning multimodal fea-tures in parameter-efficient audio-visual transformers. Inthis paper, we propose MA-AVT, a new parameter-efficientaudio-visual transformer employing deep modality align-ment for corresponding multimodal semantic features.Specifically, we introduce joint unimodal and multimodaltoken learning for aligning the two modalities with a frozenmodality-shared transformer. This allows the model to learnseparate representations for each modality, while also at-tending to the cross-modal relationships between them. Inaddition, unlike prior work that only aligns coarse featuresfrom the output of unimodal encoders, we introduce block-wise contrastive learning to align coarse-to-fine-grain hier-archical features throughout the encoding phase. Further-more, to suppress the background features in each modal-ity from foreground matched audio-visual features, we in-troduce a robust discriminative foreground mining scheme.Through extensive experiments on benchmark AVE, VG-GSound, and CREMA-D datasets, we achieve considerableperformance improvements over SOTA methods. Code is re-leased at",
  ". Introduction": "When observing a scene, humans can simultaneously iden-tify sounding objects, distinguish background noises, andlocate silent regions. Such natural audio-visual perceptionarises from fine-grain correspondence of environmental vi-sual and auditory cues. In this work, we explore improv-ing audio-visual feature alignment of audio-visual learn-ers, from the initial feature extraction to the final decision-making stage, and systematically evaluate its impact oncomplex audio-visual recognition tasks.Earlier work on audio-visual learning primarily focusedon late multi-modal feature alignment by extracting featuresfrom separate uni-modal encoders . These . A visual image contains sounding foreground object re-gions, as well as silent background regions. MA-AVT aims to alignthe foreground visual features with corresponding audio features.Simultaneously, MA-AVT learns mismatched uni-modal featuresto enhance cross-modal contrast. In particular, MA-AVT leveragesa pre-trained frozen vision transformer in audio-visual tasks withlearnable uni-modal and shared cross-modal tokens. methods required large-scale separate audio and visual pre-training, as they used different encoders for each modality.Later, to leverage deeper cross-modal fusion, Nagrani et al. introduced uniform audio and visual transformers withbottleneck fusion modules. This uniform architecture en-abled blockwise fusion of cross-modal features across sep-arate uni-modal encoders. However, this method requireslarge-scale joint training of uni-modal encoders with bot-tleneck fusion modules, which can be burdensome for de-ploying large transformer models (ViT-Large, 656M param-eters). Very recently, Lin et al. introduced a lightweightadapter module (LAVISH) to leverage pre-trained frozenvision transformer (ViT) in audio-visual tasks, without theneed for any audio pre-training. This adapter enables deepcross-modal alignment by fusing the output of each ViTblock features. Despite its promising performance and highparameter efficiency, this method has several limitations. First, this method only trains cross-modal fusion mod-ules with a frozen encoder, while ignoring unique unimodalfeature components. However, natural images and soundscontain unique components that do not have multimodal",
  "arXiv:2406.04930v1 [cs.CV] 7 Jun 2024": "correspondence but are still important for learning and sep-arating the context. For example, most regions in visualframes usually contain silent objects, while the correspond-ing audio may contain significant background environmen-tal sounds that are not present in the image. Therefore, it isnecessary to simultaneously process unique unimodal andcommon cross-modal features in audio-visual learning. Toaddress this issue, we introduce joint unimodal and mul-timodal token learning, which simultaneously learns dis-joint unimodal and common cross-modal representations.However, the learned tokens may have varying significancebased on different class representations. To selectively en-able the most relevant tokens in each modality, we introducea local self-attention module for each group of tokens.Second, LAVISH and most prior work trainonly with late supervision applied to the coarse-grain fea-tures, extracted from the output of audio and visual en-coders. This enforces explicit coarse-grain multimodal fea-ture alignment, but the fine-grain features in earlier trans-former blocks do not receive any explicit supervision forcross-modal alignment. To tackle this problem and align thehierarchical features of each modality throughout the en-coding, we introduce blockwise semantic contrastive learn-ing (SCL). Our approach inherently searches for the seman-tic visual representation with its corresponding audio repre-sentation by utilizing shared multimodal tokens. Thus, weintroduce SCL on intermediate feature representations ofmultimodal tokens to strengthen modality alignment.Third, audio and visual modalities may not contain anymatching pairs of objects, resulting in complete misalign-ment. Employing fusion modules in such misaligned back-ground regions can introduce significant noise during train-ing. However, existing audio-visual transformers, such asLAVISH and MBT , do not consider any back-ground suppression methods to isolate such complete mis-alignment cases. To overcome this limitation, we intro-duce a robust discriminative foreground mining scheme bylearning additional background tokens that selectively sup-press the mismatched background regions, thereby enhanc-ing modality alignment.Extensive experiments on three popular benchmarks, i.e.AVE, VGGSound, and CREMA-D datasets, demonstratethe superior performance of MA-AVT. The contributions ofthe proposed method can be summarized as follows:1. We propose joint unimodal and multimodal token learn-ing for extracting disjoint uni-modal and common cross-modal features from each modality.",
  ". Audio-Visual Learning": "In recent years, numerous approaches have been exploredin diverse audio-visual learning applications, such as audio-visual source localization , sound separa-tion , video parsing , event lo-calization , and classification .In this work, we primarily focus on designing parameter-efficient multi-modal transformer for the audio-visual clas-sification task.In general, much of earlier work in audio-visual learn-ing has focused on the late fusion of extracted audio-visualfeatures from separate pre-trained encoders. In MBT ,deep and mid-fusion approaches to audio-visual feature fu-sion have been explored with a unified transformer archi-tecture with full tuning. However, these methods requirelarge-scale pre-training on audio and visual data, which canbe computationally expensive and time-consuming. Addi-tionally, full-tuning can be parameter inefficient and proneto overfitting on smaller datasets, while partial tuning canresult in sub-optimal results . Recently, LAV-ISH introduced pre-trained frozen vision transform-ers for audio-visual tasks without requiring any pre-trainingon audio data. However, LAVISH and other existing meth-ods are trained with late supervision on the final projectedfeature spaces for audio-visual feature alignment. This canlimit the effectiveness of feature alignment, as the final fea-ture space may not capture all of the relevant informationfrom the intermediate feature spaces. In contrast, we studythe impact of deep audio-visual feature alignment by lever-aging supervision to the intermediate feature space of pre-trained frozen transformers with learnable tokens. This al-lows us to learn more robust and discriminative representa-tions that are better aligned across modalities.",
  ". Audio-visual Contrastive Learning": "Prior work explored audio-visual contrastive learning inself-supervised representation learning and sound source lo-calization. Hu et al. and Owens and Efros con-trast across the global mean-pooled audio, and visual fea-tures. However, in practice, audible objects correspond toa small portion of the image while audio includes back-ground noise from non-visible objects. Hence, such globalalignment introduces significant noise in contrastive learn-ing. Arandjelovic and Zisserman , Ilse et al. , Kor-bar et al. , Mo and Morgado , Morgado et al. contrast mean-pooled audio features with the most-similarcorresponding image patch region. Nevertheless, the sound-ing object may extend to multiple patches across the imagewhere these methods cannot represent the complete seman- . The overview of the proposed MA-AVT framework. The image and audio spectrogram are processed simultaneously withfrozen transformer encoders. Initially, we extract patch tokens using pre-trained patch extractors of transformers. We introduce learnableunimodal audio and visual tokens to learn unique unimodal representation as well as introduce multimodal shared tokens to learn joint rep-resentation. To focus on most relevant tokens for the target class, we introduce local self-attention (LSA) modules on each group of tokens.To further enhance the modality alignment, we operate blockwise semantic contrastive learning on the intermediate shared multimodaltoken embeddings after each transformer block. To suppress mismatching background regions, we introduce learnable background (BG)and foreground (FG) class tokens. Here, Lbf denotes foreground-background loss and Lkcnt denotes contrastive loss after each kth block. tic relationships. Guzhov et al. used class token pooledfeatures for representing visual modality. Senocak et al. introduced hard positive sample mining in contrastive learn-ing for semantic audio-visual matching. Qian et al. and Mo and Tian introduced a combination of sourceclassification and contrastive learning for multiple soundingsource localization. Most of the prior work mainly consid-ered audio-visual correspondence on coarse-grain featuresextracted from the outputs of unimodal encoders. Differ-ent from past work, we introduce audio-visual contrastivelearning for aligning coarse-to-fine-grain hierarchical fea-tures in audio-visual transformers.",
  ". Methodology": "Our goal is to build a parameter-efficient audio-visual trans-former with modality alignment for audio-visual recogni-tion tasks. The overview of the proposed framework isshown in . To maintain learning efficiency, weadopt frozen ViTs as audio and visual encoders. Uponthe framework, we first extract corresponding patch token embeddings from the input image and audio spectrogram.Then, we introduce unimodal token embeddings to bothmodalities to learn separate and unique unimodal featurerepresentations. We also introduce shared multimodal to-kens to both modalities to learn the common cross-modalsemantic relationship. To focus on the most relevant tokensfor the target class, we utilize local self-attention (LSA)modules for each group of tokens. In addition, we proposeto use background tokens to detect complete mismatchesof audio-visual pairs and foreground class tokens to de-tect the audio-visual class in both modalities. After ag-gregating all token embeddings separately for each modal-ity, we process frozen transformer blocks sequentially. Theshared multimodal tokens inherently search for the audio-visual corresponding regions in both modalities. To fur-ther strengthen cross-modal alignment, we introduce block-wise semantic contrastive learning (SCL) across the mean-pooled intermediate token representations extracted fromeach transformer block. This blockwise contrastive learn-ing is only used during training to align coarse-to-fine-grain audio-visual feature representations. Finally, the foregroundclass token generates the foreground class prediction, andthe background class token predicts the binary backgroundclass. In the case of complete misaligned background classpredictions, the foreground and blockwise contrastive lossgradient propagations are suppressed during training.",
  ". Preliminaries": "Lets consider the dataset D = {(vi, ai) : i = 1, . . . , N}representing N pairs of images vi R3HW sampledfrom a video at time t, and corresponding audio spectro-grams ai RF T centered at time t with a span of severalseconds. Following previous work , vi is dividedinto m non-overlapping patches which are flattened to ex-tract image patch token embeddings P 0v Rmd. Similarly,audio spectrogram ai is split into n audio patch token em-beddings P 0a Rnd. Moreover, linear interpolation acrosspre-trained position embeddings is carried out, particularlyfor audio tokens, to match with the number of patch tokensin case of mismatch with pre-trained embeddings.",
  ". Multimodal Alignment with Learnable Tokens": "Leveraging a pre-trained frozen vision transformer in audio-visual learning poses two main challenges. First, thedisparate unimodal audio and visual features should be ex-tracted separately since visual and auditory signals comewith unique feature representations. Second, the groundedmodality-shared features across two modalities need to beprocessed simultaneously as a fusion step. To solve theformer one, we introduce modality-specific token promptswhich are uniquely trained for each modality. And, for thelatter one, we introduce modality-invariant token promptswhich are jointly trained across both modalities. Beforemerging with the patch token embeddings, all three bagsof modality-invariant and modality-specific tokens are pro-cessed with separate local self-attention (LSA) units. Theseunits enhance vanilla prompt training capacity by focus-ing on most relevant prompts for different classes. TheseLSA units are formed with residual multi-headed attention(MHA) operations such that,",
  "LSA(x) = x + MHA(x)(1)": "Lets denote LSA units operating on audio, visual, and mul-timodal shared prompt tokens as Aa(), Av(), and As(),respectively, and bags of audio, image, and shared tokensas za Rnad, zv Rnvd, and zs Rnsd, respec-tively. Hence, intermediate image and audio token embed-dings (Ea, Ev) after kth transformer block are given by",
  ". Blockwise Semantic Contrastive Learning": "In practice, visual frames consist of target foregroundsounding regions as well as silent background regions. Sim-ilarly, audio contains target foreground sounding sourcesalong with certain noises from invisible background sound-ing sources. Proper understanding of audio-visual scenesposes two primary challenges. First, the model should prop-erly align the corresponding semantic regions of audio-visual features representing high cross-modal similarity aswell as distinguish unique unimodal features. Second, themodel should discriminate among hierarchical audio-visualfeatures of target classes.Our unimodal and multimodal shared prompting tech-nique inherently solves the first problem. Nevertheless,we introduce semantic contrastive learning to furtherstrengthen the modality alignment. Notably, the shared to-kens explicitly search for the semantic regions with highaudio-visual correspondence over all other patch tokens ineach modality. Hence, we extract the semantic representa-tion of the target matching pair by taking the mean over theoutput of shared token embeddings in each modality. Weintroduce cross-modal contrastive learning over these meanpooled semantic feature representations, which is denotedas semantic contrastive learning (SCL).The supervised learning objective operating on thecoarse-grain audio-visual features inherently generates dis-criminative hierarchical features in subsequent layers,thereby attempting to solve the second problem. Along withsuch coarse-grain supervision, we introduce blockwise se-mantic contrastive learning for further alignment of thefine-grain hierarchical features. We note that such block-wise cross-modal alignment doesnt alter the inter-block hi-erarchical feature relationships generated with supervisedlearning. Rather, it generates deeper auxiliary supervisionthroughout the encoding phase to contrast across coarse-to-fine-grain cross-modal semantic relationships.After kth block, assume ak =1ns zks,a R1d andvk =1ns zks,v R1d represent the mean-pooled sharedtoken features from audio, and visual modality, respectively.The blockwise semantic contrastive loss (Lkcnt) at kth blockwith a batch size of B is given by",
  ". Robust Discriminative Foreground Mining": "Audio may contain only background noise or sound fromnon-visible objects, which results in a complete misalign-ment of audio-visual features. We denote such cases asbackground class. If the audio-visual pair contains at leastone matching object, that will represent the foregroundclass. In certain datasets (e.g., VGGSound ) where wedont have access to mismatched pairs, we consider syn-thetic audio-visual pairs from two separate sources forlearning such mismatched background cases.We introduce unique background class tokens zbR1d and foreground class tokens zf R1d in bothmodalities. Thus, the accumulated tokens at kth layer are:",
  ". Datasets": "AVE dataset contains 4, 143 videos of 10-second au-dio and visual segments. It has per-second frame-level an-notations for audio-visual event localization and consists of28 event classes along with background class annotationsrepresenting complete modality misalignment. This datasethas natural misaligned audio-visual pairs that makes it di-rectly applicable to MA-AVT. The data-split contains 3, 942training videos, 742 test videos, and 892 validation videos.Following prior work, we sample image frames at 1 fps,and extract corresponding audio segments of 1s duration.We also use the same evaluation metric of the fraction ofcorrectly predicted event regions as in prior work .",
  "VGGSound is a large-scale audio-visual learningdataset containing 309 classes that represent in the wild": "audio-visual correspondence. This dataset contains a largerange of sounding events from day-to-day life. All videosare collected from YouTube. Since many videos are notavailable anymore, we use 161, 234 videos for training, and12, 873 videos for testing following the data split in .For training and evaluation, we sample single image framesper video from the middle of each video and extract corre-sponding audio segments of 5s duration. We use the sameevaluation metric of class accuracy following prior work.Since the dataset size is reduced for unavailable videos,we reproduced the reported results of prior work underthe same settings for fair comparison. Since this datasetonly contains audio-visual matched pairs, we introduce syn-thetic mis-matched pairs during training, particularly tolearn foreground-background tokens. CREMA-D is a speech emotion recognition datasetwith 7, 442 video clips of 2 3 seconds duration collectedfrom 91 actors. Each actor speaks various short words with6 usual emotion categories, such as happy, sad, angry, neu-tral, disgust, discarding, and fear. The dataset is annotatedby crowd-sourcing from 2, 443 raters for categorical emo-tion labels. We use the same train and test split as priorwork , more specially 6, 698 training videos, and 744 testvideos. We use a single frame sampled from the middle ofthe video and corresponding audio segments with 3s dura-tion. Similar to prior work, we use the same evaluation met-ric of emotion recognition accuracy. We introduce similarsynthetic mismatched audio-visual pairs during training.",
  ". Implementation Details": "We use the spectrogram representation for audio by repeat-ing the channel from 1 to 3 to operate with the same ViTbackbone. All image samples are resized to (224, 224). Allaudio spectrograms are extracted with a window length of512 and overlap of 353. We use 5 tokens for all audio, vi-sual, and shared multimodal cases in all experiments unlessotherwise specified. We use ADAM optimizer with the ini-tial learning rate of 1e 3 which is multiplied by 0.1 afterevery 30 epochs. All models are trained on 4 A5000 GPUswith 24GB memory. We use a batch size of 256 for trainingall models.",
  ". Experimental Study": "In this work, we propose enhanced modality alignmenttechniques for parameter efficient audio-visual transform-ers. To demonstrate the effectiveness of the proposedmethod, we compare the performance with state-of-the-artapproaches on three popular benchmark datasets. For faircomparison, we reproduced the results of most other ap-proaches from their open-sourced implementation under thesame setting. We also present the ablation study to showthe effectiveness of different building blocks. We use VG- . Comparison with state-of-the-art methods. We present comparison with CNN and transformer-based approaches. For the audioencoder, we either use audio pre-trained weights from Audioset or simple image-pretrained weights from ImageNet dataset. * denotesour improved implementation. Other than the reported results on AVE, we reproduced the results on VGGSound and CREMA-D datasetsfrom open source implementation. The parameter counts are calculated for AVE dataset. (T) represents fully-trainable and (F) representsfrozen encoder. Our proposed MA-AVT achieves significant performance improvements compared to other state-of-the-art methods whilemaintaining parameter-efficiency.",
  "GSound dataset and ViT-B-16 backbone for most of the ab-lations unless otherwise specified": "Comparison with prior CNN-based approaches:Asshown in , we study the performance of several re-cent CNN-based approaches . Most of thesemethods rely on separate pre-trained audio and visual en-coders for the feature extraction with different late-fusiontechniques. Despite the use of late-fusion techniques formodality alignment, the early stage of feature extractiononly relies on unimodal encoders. Hence, the fusion tech-niques are only operated on the coarse unimodal feature rep-resentation thereby limiting performance. Moreover, perfor-mance of these methods depends on pre-training on large-scale audio and image data. Our method achieves +1.3,+7.9, and +11.8 accuracy improvements in AVE, VG-GSound, and CREMA-D datasets, respectively, comparedto the best-performing CNN-based counterparts without us-ing any audio pre-training. Comparison with prior transformer-based works:Wealso compare with several state-of-the-art transformer-based methods for audio-visual tasks . Comparedto CNN based methods, transformer based approaches canleverage early fusion techniques for using the uniform ar-chitecture in both modalities. As shown in , our pro-posed MA-AVT achieves superior performance comparedto other existing transformer based methods. We primarilyfocus on the recently released LAVISH and MBT models for the discussion.",
  "Since the dataset size in VGGSound is considerably reduced for un-available videos, we report the reproduced result in the same setup": "LAVISH introduces a parameter-efficient visiontransformer based network for audio-visual tasks withoutusing audio pre-training. We achieve +2.6, +4.2, +3.4 ac-curacy improvements on AVE, VGGSound, and CREMA-Ddatasets, respectively, compared to LAVISH for the ViT-B-16 model. We also demonstrate the performance improve-ments with other ViT architectures (ViT-L-16). We note thatthe number of additional trained parameters in MA-AVT iscomparable with LAVISH (7.1M vs. 4.7M in ViT-B-16).For fair comparison, we increase the number of trained pa-rameters in LAVISH by using larger convolutions in adaptermodules ( 7.3M). Nevertheless, our method outperformsthe larger LAVISH model with a considerable margin. We hypothesize these performance improvements ofMA-AVT are due to three significant modifications. First,with a pre-trained frozen vision encoder as a common back-bone for both modalities, LAVISH only trains cross-modalfusion adapters at each block. However, both audio and vi-sion modalities have significant unique unimodal featurecomponents which can be suppressed by focusing only onfusion adapters. In contrast, we adapt to significant uniqueunimodal and common cross-modal feature components byleveraging separate unimodal and multimodal tokens withlocal self-attention modules. Second, despite using early fu-sion techniques, LAVISH only considers supervision on thecoarse features extracted from unimodal encoder outputs.In contrast, our blockwise semantic contrastive learning inMA-AVT operates on hierarchical coarse-to-fine-grain fea-tures thereby resulting in deeper alignment of multimodalfeatures. Third, we introduce robust foreground mining tosuppress the complete misalignment of background audio- . Effect of unimodal and multimodal learnable Tokenswith local self attention (LSA). Combination of audio, visual,and shared multimodal tokens generates best performance. Inte-gration of LSA module considerably improves the performance.Only classification loss is used for this analysis.",
  "MaxPoolxy(sim(Vxy, A)) 55.8AvgPoolxy(sim(Vxy, A)) 55.2sim(MaxPoolxy(Vxy), A) 54.9sim(Vcls, Acls) 55.1sim(Mean(Vs), Mean(As)) (Ours)56.7": "visual pairs, which is absent in LAVISH.In MBT , modality bottleneck fusion tokens are in-troduced with separate audio and vision transformers withfull fine-tuning of transformer encoders. We show the per-formance of MBT with both ImageNet pretrained weightsand AudioSet pretrained weights particularly for the audioencoder. Our method achieves +1.8, +2.1, +1.6 higher ac-curacy than ImageNet-pretrained MBT with ViT-B-16 en-coder on AVE, VGGSound, and CREMA-D datasets, re-spectively. These improvements demonstrate that our pro-posed MA-AVT provides superior results to fully-tuningtransformer encoders despite being trained on significantlysmaller number of parameters (7.1M vs. 206.4M). WithAudioSet pre-trained weights in audio encoder, MA-AVTachieves +2.5, +3.0, +1.4 higher accuracy on AVE, VG-GSound, and CREMA-D benchmarks, respectively. In gen-eral, audio encoders with AudioSet pretrained weightsachieve higher performance than ImageNet pretrainedweights for large-scale audio pretraining. Nevertheless,our approach achieves consistent performance improve-ment over MBT. We hypothesize that these improvementsare due to our blockwise semantic contrastive learning for",
  "deeper modality alignment and our robust foreground min-ing methods to suppress background pairs, both of whichare missing in MBT": "Effect of unimodal and multimodal tokens with localself-attention (LSA) modules:In , we ablate theeffect of unimodal and multimodal tokens with LSA mod-ules in MA-AVT. Only classification loss is used for thisanalysis. We note that audio-only tokens provide +1.8higher accuracy than image-only tokens without using anyLSA modules. Since we only use single image frame pervideo with full-length audio, the audio contains richer de-tails of the sounding event compared to single images. Com-bining unimodal audio and visual tokens provides +5.2higher accuracy than the audio-only tokens. This showsthe combination of audio and visual tokens performs bet-ter than single modal tokens. By only using shared multi-modal tokens, we achieve +2.1 higher accuracy than theunimodal audio only tokens. Shared tokens are supposedto perform multimodal alignment over audio and visualmodality that can be responsible for the accuracy improve-ment. Finally, the combination of audio, visual, and mul-timodal tokens achieves the best accuracy of 51.2 whichis +5.8 higher than shared multimodal tokens and +2.7higher than unimodal combinations. Finally, we note con-sistent performance improvements with the incorporation oflocal self-attention (LSA) modules. We achieve the highestaccuracy of 54.1 by integrating LSA modules in all three to-kens which demonstrates the effectiveness of our approach. Comparison with existing contrastive learning methods:In , we present the results of various contrastivelearning methods. For fair comparison, we keep the samearchitecture of MA-AVT for all contrastive losses. We sepa- . Grad-CAM visualization for qualitative comparison.Here, red color denotes high attention values and blue color de-notes low attention values. Modality alignment brings noticeableimprovements in MA-AVT to put more attention on the sound-ing regions. In general, MA-AVT better discovers the target visualsounding regions with sharper boundaries compared to other com-petitive baselines. Moreover, MA-AVT significantly reduces theattention weights on the silent regions. rately present the effect of blockwise contrastive loss for allthese cases. We note that our proposed semantic contrastiveloss provides significant performance improvements for allbenchmarks. Moreover, we also observe consistent perfor-mance improvements with the incorporation of the block-wise contrastive losses in all cases. Prior work mostly fo-cuses on matching the spatial visual features V = {Vxy :x, y} from all (x, y) position with the global mean of audiofeatures A . Considering the complex semanticrelations with audio and visual modality in the presence ofvisual backgrounds and secondary noisy-sounding sources,most of these approaches introduce significant noise intraining. In addition, contrastive learning applied on class-token provides performance inferior to ours . In contrast,our approach searches for the corresponding semantic re-gions in both audio and visual modalities simultaneouslyby utilizing the shared multimodal token embeddings whichfurther strengthens the modality alignment. Effect of robust discriminative foreground mining:Weablate the effect of robust foreground mining as shown in. The AVE dataset contains annotations for back-ground classes that can be directly used for both trainingand testing of the proposed method. However, for the VG-GSound, and CREMA-D datasets, we use the robust fore-ground mining only in training, where we randomly chooseaudio and images from two different classes to representthe background class. By integrating foreground mining,we achieve +0.8, +1.1, and +0.9 higher accuracy on AVE,VGGSound, and CREMA-D datasets, respectively. Multimodal alignment helps unimodal learning:MA-AVT can work on unimodal data in test scenarios by onlyusing the separate unimodal tokens and by splitting the lastMLP layer for foreground classification symmetrically. In, we study the effect of multimodal alignment meth-ods on unimodal learning. We achieve +3.1, +2.8 higheraccuracy on audio and visual modalities, respectively, whencomparing the proposed multimodal alignment techniqueswith separate unimodal training. We hypothesize the multi-modal alignment helps unimodal learning by searching forthe target region-of-interests in both modalities.",
  ". Qualitative Analysis": "We visualize the class activation heatmaps in fromthe output foreground class token to the RGB image byusing Grad-CAM visualization. We primarily showthe qualitative visualization results for the LAVISH ,MBT , and proposed MA-AVT models. We note that theproposed modality alignment techniques better discover tar-get semantic regions than competitive baselines in general.Also, MA-AVT generates sharper boundaries with moreweights on the target semantic regions representing sound-ing objects. Moreover, MA-AVT significantly reduces theattention weights in silent regions of images compared toother baselines. We hypothesize that such effective local-ization of the audio-visual semantic regions leads to its su-perior performance on audio-visual recognition tasks.",
  ". Conclusion": "In this paper, we present modality alignment techniques forparameter-efficient audio-visual transformer, dubbed MA-AVT. Our approach learns unimodal and multimodal to-kens to adapt to unique unimodal features and extract com-mon multimodal features thereby achieving superior per-formance. The local self-attention modules are found tobe effective with learnable tokens to focus on the mostrelevant tokens in each modality. To better contrast withthe mismatched background scenarios, we introduce ro-bust foreground mining that differentiates the corner caseof complete modality mismatch. We propose semantic con-trastive learning to contrast across the semantic regionsof each modality by utilizing the shared multimodal to-ken embedding for achieving higher accuracy than baselineapproaches. Moreover, we leverage blockwise contrastivelearning for deeper alignment of cross-modal features forachieving consistent performance improvements. Modality-alignment training also demonstrates its effectiveness inunimodal test scenarios by considerably improving perfor-mance. Extensive experiments on three benchmark datasetsshow the superiority of the proposed MA-AVT over state-of-the-art methods.",
  "Relja Arandjelovic and Andrew Zisserman. Look, listen andlearn. In Proceedings of the IEEE international conferenceon computer vision, pages 609617, 2017. 2": "Houwei Cao, David G Cooper, Michael K Keutmann,Ruben C Gur, Ani Nenkova, and Ragini Verma. Crema-d:Crowd-sourced emotional multimodal actors dataset. IEEEtransactions on affective computing, 5(4):377390, 2014. 5 Honglie Chen, Weidi Xie, Andrea Vedaldi, and AndrewZisserman. Vggsound: A large-scale audio-visual dataset.In ICASSP 2020-2020 IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP), pages721725. IEEE, 2020. 5 Chuang Gan, Deng Huang, Hang Zhao, Joshua B Tenen-baum, and Antonio Torralba. Music gesture for visual soundseparation.In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1047810487, 2020. 2",
  "Ruohan Gao, Rogerio Feris, and Kristen Grauman. Learningto separate object sounds by watching unlabeled video. InProceedings of the European Conference on Computer Vi-sion (ECCV), pages 3553, 2018. 2": "Andrey Guzhov, Federico Raue, Jorn Hees, and AndreasDengel. Audioclip: Extending clip to image, text and au-dio. In ICASSP 2022-2022 IEEE International Conferenceon Acoustics, Speech and Signal Processing (ICASSP), pages976980. IEEE, 2022. 3, 7, 8 Andrey Guzhov, Federico Raue, Jorn Hees, and AndreasDengel. Audioclip: Extending clip to image, text and au-dio. In ICASSP 2022-2022 IEEE International Conferenceon Acoustics, Speech and Signal Processing (ICASSP), pages976980. IEEE, 2022. 2 Di Hu, Feiping Nie, and Xuelong Li. Deep multimodal clus-tering for unsupervised audiovisual learning. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 92489257, 2019. 2 Xixi Hu, Ziyang Chen, and Andrew Owens. Mix and local-ize: Localizing sound sources in mixtures. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1048310492, 2022. 2",
  "Yan-Bo Lin and Yu-Chiang Frank Wang. Audiovisual trans-former with instance attention for audio-visual event local-ization. In Proceedings of the Asian Conference on Com-puter Vision, 2020. 1": "Yan-Bo Lin, Hung-Yu Tseng, Hsin-Ying Lee, Yen-Yu Lin,and Ming-Hsuan Yang.Exploring cross-video and cross-modality signals for weakly-supervised audio-visual videoparsing. Advances in Neural Information Processing Sys-tems, 34:1144911461, 2021. 2 Yan-Bo Lin, Yi-Lin Sung, Jie Lei, Mohit Bansal, and GedasBertasius. Vision transformers are parameter-efficient audio-visual learners. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 22992309, 2023. 1, 2, 4, 6, 8 Tanvir Mahmud and Diana Marculescu. Ave-clip: Audioclip-based multi-window temporal transformer for audio visualevent localization. In Proceedings of the IEEE/CVF Win-ter Conference on Applications of Computer Vision, pages51585167, 2023. 1, 2",
  "Shentong Mo and Pedro Morgado. Localizing visual soundsthe easy way. In European Conference on Computer Vision,pages 218234. Springer, 2022. 2, 7, 8": "Shentong Mo and Yapeng Tian. Audio-visual grouping net-work for sound localization from mixtures. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1056510574, 2023. 3 Pedro Morgado, Nuno Vasconcelos, and Ishan Misra. Audio-visual instance discrimination with cross-modal agreement.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1247512486, 2021.2, 7, 8 Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen,Cordelia Schmid, and Chen Sun. Attention bottlenecks formultimodal fusion.Advances in Neural Information Pro-cessing Systems, 34:1420014213, 2021. 1, 2, 4, 6, 7, 8",
  "Andrew Owens and Alexei A Efros.Audio-visual sceneanalysis with self-supervised multisensory features. Euro-pean Conference on Computer Vision (ECCV), 2018. 2": "Xiaokang Peng, Yake Wei, Andong Deng, Dong Wang, andDi Hu. Balanced multimodal learning via on-the-fly gradientmodulation. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 82388247, 2022. 2 Rui Qian, Di Hu, Heinrich Dinkel, Mengyue Wu, Ning Xu,and Weiyao Lin. Multiple sound sources localization fromcoarse to fine. In Computer VisionECCV 2020: 16th Eu-ropean Conference, Glasgow, UK, August 2328, 2020, Pro-ceedings, Part XX 16, pages 292308. Springer, 2020. 3",
  "Yi-Lin Sung, Jaemin Cho, and Mohit Bansal.Lst: Lad-der side-tuning for parameter and memory efficient transferlearning. Advances in Neural Information Processing Sys-tems, 35:1299113005, 2022. 2": "Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chen-liang Xu. Audio-visual event localization in unconstrainedvideos. In Proceedings of the European conference on com-puter vision (ECCV), pages 247263, 2018. 2, 5, 6 Yapeng Tian, Dingzeyu Li, and Chenliang Xu. Unified mul-tisensory perception: Weakly-supervised audio-visual videoparsing. In Computer VisionECCV 2020: 16th EuropeanConference, Glasgow, UK, August 2328, 2020, Proceed-ings, Part III 16, pages 436454. Springer, 2020. 2 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in neuralinformation processing systems, 30, 2017. 4 Yu Wu and Yi Yang.Exploring heterogeneous clues forweakly-supervised audio-visual video parsing. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 13261335, 2021. 2 Haoming Xu, Runhao Zeng, Qingyao Wu, Mingkui Tan,and Chuang Gan. Cross-modal relation-aware networks foraudio-visual event localization. In Proceedings of the 28thACM International Conference on Multimedia, pages 38933901, 2020. 1, 6",
  "Xudong Xu, Bo Dai, and Dahua Lin. Recursive visual soundseparation using minus-plus net. In Proceedings of the IEEEInternational Conference on Computer Vision, pages 882891, 2019. 2": "Jiashuo Yu, Ying Cheng, Rui-Wei Zhao, Rui Feng, and Yue-jie Zhang.Mm-pyramid: Multimodal pyramid attentionalnetwork for audio-visual event localization and video pars-ing. In Proceedings of the 30th ACM International Confer-ence on Multimedia, pages 62416249, 2022. 2, 6 Jeffrey O Zhang, Alexander Sax, Amir Zamir, LeonidasGuibas, and Jitendra Malik. Side-tuning: a baseline for net-work adaptation via additive side networks.In ComputerVisionECCV 2020: 16th European Conference, Glasgow,UK, August 2328, 2020, Proceedings, Part III 16, pages698714. Springer, 2020. 2 Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Von-drick, Josh McDermott, and Antonio Torralba. The sound ofpixels. In Proceedings of the European conference on com-puter vision (ECCV), pages 570586, 2018. 2 Jinxing Zhou, Liang Zheng, Yiran Zhong, Shijie Hao, andMeng Wang. Positive sample propagation along the audio-visual event line.In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages84368444, 2021. 1, 2, 6"
}