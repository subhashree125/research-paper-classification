{
  "Abstract": "Learning to represent three dimensional (3D) humanpose given a two dimensional (2D) image of a person, isa challenging problem. In order to make the problem lessambiguous it has become common practice to estimate 3Dpose in the camera coordinate space. However, this makesthe task of comparing two 3D poses difficult. In this pa-per, we address this challenge by separating the problemof estimating 3D pose from 2D images into two steps. Weuse a variational autoencoder (VAE) to find an embeddingthat represents 3D poses in canonical coordinate space. Werefer to this embedding as variational view-invariant poseembedding (V-VIPE). Using V-VIPE we can encode 2D and3D poses and use the embedding for downstream tasks, likeretrieval and classification. We can estimate 3D poses fromthese embeddings using the decoder as well as generate un-seen 3D poses. The variability of our encoding allows itto generalize well to unseen camera views when mappingfrom 2D space. To the best of our knowledge, V-VIPE isthe only representation to offer this diversity of applica-tions. Code and more information can be found at",
  ". Introduction": "Learning to represent three dimensional (3D) human posegiven a two dimensional (2D) image of a person, is a chal-lenging problem with several important downstream appli-cations such as teaching a person to mimic a video, actionrecognition and imitation learning for robotics. The keychallenge arises from the fact that different camera view-points observing the same 3D pose lead to very differentprojections in a 2D image. The common practice is to cir-cumvent this challenge by estimating 3D pose in the cameracoordinate space . However, this leads to differ-ences in scale and rotation between the estimated 3D rep-resentations from images of the same 3D pose from differ-ent camera viewpoints. Without the knowledge of cameraparameters, it is not possible to establish correspondencebetween these 3D representations. This is important as we V-VIPE",
  "D Pose Generation": "2D to 3D Pose Estimation . The several functions V-VIPE is capable of. The purplepath represents 3D pose retrieval. The blue path represents genera-tion by adding noise to the purple path. The result is a variation ofthe original pose. The green path shows 2D to 3D pose estimationfrom several viewpoints. move towards environments where we have very little con-trol over the camera viewpoint, such as photos taken with aphone or AR glasses. In such scenarios, we can make veryfew assumptions about the camera space.In this paper, we address this challenge by separatingthe problem of estimating 3D pose from 2D images intotwo steps. First, we learn an embedding to represent 3Dposes in canonical coordinate space. Next, we learn to en-code 2D poses, from different camera viewpoints, to theembedding from the first step. This leads to a canonical 3Dpose embedding that is invariant to camera viewpoints. Thisview-invariant pose embedding is highly flexible, allowingus to do 3D pose retrieval, 3D pose generation, and mostimportantly, estimating consistent 3D pose from different2D viewpoints 1.In our approach we use a variational autoencoder (VAE)to learn an embedding for 3D human poses. This VAE istrained to reconstruct 3D poses and has two key benefits: (a)we can leverage loss functions to ensure similar 3D posesare close in the embedding space, and (b) we learn embed-dings that can generalize better to unseen 3D poses due tothe variational training paradigm. Next, we learn a map-",
  "arXiv:2407.07092v1 [cs.CV] 9 Jul 2024": "ping from 2D poses (either ground-truth or estimated usingoff-the-shelf detectors) to this 3D pose embedding space bytraining a 2D pose encoder that estimates the 3D pose em-bedding. This embedding is used as input to the pre-traineddecoder from the VAE to estimate the corresponding 3Dpose, thus leading to lifting the 2D pose from differentcamera viewpoints to 3D . We refer to embeddingas variatonal view-invariant pose embedding (V-VIPE).Our proposed V-VIPE is highly flexible and generaliz-able. We can encode 3D poses and use the embedding fordownstream tasks, like retrieval and classification. We canalso map 2D poses from unseen camera viewpoints to thisembedding. We can estimate 3D poses from these embed-dings using the decoder. Finally, we can generate unseen3D poses. To the best of our knowledge, V-VIPE is theonly representation to offer this diversity of applications.We perform an extensive experimental evaluation overtwo datasets: Human 3.6M and MPI-3DHP . Weshow quantitative results on 2D to 3D pose retrieval andqualitative results on 3D pose generation and 2D to 3D poseestimation. We show that V-VIPE performs 1% better thanother embedding methods on seen camera viewpoints andabout 2.5% better for unseen camera viewpoints. In addi-tion, we show generalization of our approach by training onone dataset and testing on the other.To summarize, our main contributions are as follows:",
  "We propose a model to map from 2D poses to V-VIPE,which enables us to estimate 3D poses of 2D images. Ad-ditionally, because V-VIPE is camera invariant, our map-ping can generalize to unseen cameras": "We also estimate and generate 3D poses using V-VIPEvia a decoder that can be used for downstream tasks.In the rest of the paper we expand upon these ideas. Wesummarize the related works in . In , wedescribe our proposed method, and provides theexperimental evaluations. looks at ablations ofour method. Finally, derives the conclusions.",
  ". Related Work": "Human Pose Estimation.There are two family of ap-proaches for human pose estimation. One is to directly es-timate 3D poses from an 2D images , and the otheris to lift pre-detected 2D poses to 3D poses . Inrecent years, state-of-the-art approaches have almost exclu-sively focused on the lifting strategy.Our goal is to specifically find correspondence between2D poses in images from different camera viewpoints with-out any knowledge of camera parameters or temporal con-text. Recent works have explored how temporal informa- tion can improve 3D pose estimation , typically byprocessing a sequence of images using a transformer .However, our focus is 3D pose estimation using a single 2Dimage which is similar to .The key distinction between our approach and priorworks in estimating 3D poses using 2D images is view-invariant embedding that can be estimated from a monoc-ular viewpoint.Several works have attempted to ad-dress view invariant estimation by leveraging many view-points because it is much easier to place a per-son in canonical coordinate space when you have accessto many views. However, access to multiple viewpoints ofthe same scene is an unrealistic assumption in the canoni-cal settings. Therefore, these approaches can only be usedin environments that have multiple cameras observing thesame scene. In contrast, our approach can be applied to anyarbitrary 2D image. Some works only use a single view-point during inference time, but still require multiple viewsfor each pose during training . Whereas our method ismore flexible and can be trained on any dataset with both2D and 3D information, even if there is only one cameraviewpoint available.Similar to our work, performsview-invariant pose estimation from one view, but theirmethod requires localized transformations that fundamen-tally change the 3D pose and must be reversed at the end toget the final pose. Our approach, on the other hand, requiresonly one global rotation to a canonical camera viewpointthat does not change the integrity of the pose.3D Pose Generation. Training a model capable of generat-ing new 3D poses is important for representing unseen datain addition to training data. There are two main types ofgenerators that can be used, Generational Adversarial Net-works (GANs) and Variational Auto Encoders (VAEs).Several works have used GANs to gen-erate training data for 3D poses. However, they are notwell suited for our task which also requires encoding 3Dposes in an embedding space. VAEs, on the other hand,are better suited for learning embedding by auto-encoding3D poses. learns a latent network, where they go di-rectly from 2D to 3D without using the 3D data as input tothe model, whereas learns a latent representation us-ing a variant of VAE and generate 3D poses using 2D poseas a precondition to their decoder. employs a basicautoencoder instead of a VAE, which leads to an inconsis-tent embedding space that is harder to map to 2D inputs. also learns an autoencoder instead of a VAE, but addition-ally, they choose to regress on the embedding and performlittle normalization prior to training which leads to a poorlyregularized output space.",
  ". On the left we can see the 3D pose in the original global coordinates with 4 different cameras. The next 4 images are the 3Dposes as seen from these 4 cameras": "independent of camera view. In section 3.2, we describehow we define V-VIPE through a VAE model. In section3.3 we cover how we learn V-VIPE from the detected key-points. The final model is a network that takes as input asingle frame monocular image and estimates a view invari-ant pose, which can be used to compare any two humanposes independent of the context of the original image.",
  ". Data Processing": "Before we pass any data through our model we perform twokey steps. First, we modify the global rotation of the image;second, we scale the keypoints so that the original size doesnot affect the model.Global Rotation Realignment.Predicting 3D pose incanonical space is extraordinarily difficult as mentioned in. We believe this is mostly due to the global rotation1 of any 3D pose. Global rotation is hard to estimate dueto its ambiguity. We can see in that a pose inglobal space can have a very different appearance in cam-era space. Without any information, such as a ground truthpose, which we can align the output to or any camera pa-rameters, it would be difficult to determine that any two ofthese poses are the same.We argue that global rotation is irrelevant for human posecomparison. Specifically, when we are trying to determineif two poses are the same we do not need to understand howthose are oriented in relation to the world they are in. If onepose is facing the x-axis and the other is facing the y-axis, itis still possible that their overall pose is the same. We thusremove rotation dependence by aligning the coordinates ofthe left hip, right hip and the spine to the same points inevery pose of the dataset. This can be visualized in .In order to achieve such alignment we find the rotation thatminimizes the equation:",
  "By global rotation we mean how a human is rotated in relation to thecanonical space": "left hip, right hip and spine respectively and b1, b2, b3equal [, , ]. Aligning to these pointscauses the hips to align to the y axis and the spine to thez axis. We specifically align the hips because they are in astraight line so it is easy to align to one axis and the spinebecause it is directly above the root and therefore can beeasily aligned to a perpendicular axis. In order to minimizeEquation 1, we use the Kabsch algorithm .Scaling and Pose Normalization.In this work, we areonly concerned with estimating pose such that it is easy tocompare how similar two poses are. This is because posecomparison is what is needed for downstream tasks such asaction recognition. To account for this, we scale and nor-malize the input, such that it becomes independent fromfactors2 that should not affect the pose similarity estima-tion.We use the universal skeleton provided by the datasetto remove the size factor. In this representation all jointsare scaled to the same proportions. This makes the size ofthe 3D output independent of the inputted 2D image or theoriginal 3D pose.Moreover, to complete the normalization of the data weuse a process similar to where we center the root jointand scale all of the other joints, accordingly.",
  ". 3D Pose VAE": "The proposed model consists of two parts, a 3D Pose VAENetwork and a 2D Mapping Network. The 3D Pose VAENetwork, .a, consists of an encoder network and adecoder network, which make up the VAE model. To stayconsistent with other papers we choose as the back-bone for both our encoder and our decoder.The benefit of using a VAE for the 3D Pose VAE Net-work is its ability to generalize to new poses. This is be-cause the goal of a VAE is to synthesize unseen poses. Al-though this is not our main goal, we do want our networkto potentially be able to represent unseen poses, which is arealistic setting in real world applications.",
  ". How poses change when we align the points and modifythe rotation. On the left is the original pose and on the right is thepose after we have rotated it": "Normalizing the rotation, as defined in the step above,helps the VAE by reducing the range of values that the out-put can be. We want the VAE to learn all possible humanposes within the range and by making that range smaller wemake it easier to learn an embedding that spans the wholespace. If we omit the rotation realignment then our embed-ding space would have to learn not only joint location inrelation to all other joints, but also joint location in relationto the global space. This is in general unnecessary as loca-tion in global space is not relevant when comparing if twoposes are equal. Additionally, learning a normalized rota-tion means that the output is all in one space and can becompared easily without additional alignments.The 3D Pose VAE Network has two parts: (i) an encoder,which takes as input a 3D pose, S3D = {si R3|i =1 . . . N}, where N is the number of keypoints, and out-puts a mean for possible embeddings, e Rn, and avariance for the embedding, e Rn. Using these val-ues and a Gaussian distribution prior we take a sample, e.We denote the distribution of the latent space modeled bythe encoder with q(e|S3D); (ii) a decoder, which takes in in-put an embedding, e, and outputs an estimation of 3D poseS3D = {si R3|i = 1 . . . N}. The distribution of thedecoder is represented as p(S3D|e).The goal of the 3D Pose VAE Network is to find a V-VIPE space that is representative of the entire range of 3Dhuman poses for a specific scale and normalization. A fea-ture of the 3D Pose VAE Network should be that poses thatare close together in the original 3D space are close togetherin the embedding space. An important part of learning anaccurate mapping from 2D space is that even if there is aslight error in the V-VIPE estimation the output will still bea pose that is similar to the original 3D pose. Additionally,defining a smooth space for V-VIPE enables us to interpret if two poses are close together in 3D space by observing ifthey are close together in the embedding space.We define a distance function, D, which represents theMean Per Joint Position Error (MPJPE). MPJPE measuresthe distance between two 3D points by taking the L2 dis-tance between each joint location and then computing themean of those distances for all joints.During training we thus optimize for three factors:",
  "A reconstruction loss, which is equivalent to the MeanSquared Error (MSE) loss between S3D and S3D. Lmse =1N(S3D S3D)2": "The KL Divergence loss LKL = KL[q(z|S3D)|p(z)]. Thisloss represents the distance between the distribution of theencoder and the prior distribution, p(z). In this work weuse a Gaussian distribution as the prior. The third is a triplet loss. To compute the triplet loss wefirst find the 3D distances, Di,j within a batch betweenall elements. For each pose we then set the closest posein the batch to be the positive example (j) and the secondclosest pose to be the negative example(k). We make surethe positive and negative poses are at least .1 apart fromeach other and if they arent we select the next closestpose as the negative example. We do this because wewant the examples to be hard, but not too hard that theyintroduce noise. We compute triplet loss between i, j andk by doing Ltriplet = max[0, Di,k Di,j + m], where mis our margin. This loss is useful because it causes similarposes to move closer together in the embedding space.This makes the overall loss function to train the 3D PoseVAE:LV-VIPE = Lmse + Ltriplet + LKL(2)",
  ". 2D Mapping Network": "Once we have trained the 3D Pose VAE Network we uti-lize its embedding space to learn a 2D Mapping Network(see .b). In particular, we take the 3D Pose VAENetwork decoder model and we freeze it so that it trans-lates from the pre-defined V-VIPE space to 3D coordinates.Next, we train a new encoder Enc2D for 2D coordinates. Thenew encoder takes in input S2D = {pi R2|i = 1...N}and outputs a V-VIPE, e Rn. We pass e through thefrozen decoder to get what the embedding represents in 3Dspace according to the model trained in the previous phase,S3D = {pi R3|i = 1...N}.To train the 2D Mapping Network we use two losses.Given the input, S2D, the output S3D and the ground truth3D keypoints, S3D, we compute MSE(S3D, S3D).Wecombine this loss with a triplet loss, which we compute sim-ilarly as in .2. The main difference is that we usethe output from the 2D encoder and the ground truth 3Dkeypoints. We then back-propagate this loss through thewhole network, but do not apply the gradient losses to thedecoder network. This is because we do not want to change",
  "D PoseEncoder": "e3D PoseDecodere . The network on top is our 3D Pose VAE Network. First we pass the 3D input through our data processing phase. Once wehave the output we can pass that as input to our VAE network, which generates V-VIPE and then attempts to reconstruct the pose. On thebottom is our 2D Mapping Network. 2D keypoints are extracted using a detector. We then pass these through our 2D encoder and then alocked clone of the decoder network from the 3D Pose VAE Network. This reconstructs the original 3D pose. the embedding space, but we just want to train the 2D en-coder to make it compliant with the latent space.We find that it is beneficial to pre-train the decoder asdescribed in 3.2 because we want to construct a space forV-VIPE that is smooth, without also needing to learn a 2Dto 3D mapping. Because we train our 3D Pose VAE onnormalized 3D poses it will only learn how to map to a nor-malized pose. Therefore the output of the 2D Mapping Net-work is also normalized. This means the output is rotationand scale invariant, making it easy to compare 2D posesfrom different camera viewpoints.",
  ". Experimental Setup": "The model uses a backbone network described describedin . We stack 2 blocks of this network together for boththe encoder and the decoder network of both the 3D PoseVAE Network and the 2D Mapping Network. We set thelinear size to 1024, and we use a 0.1 dropout. The dimen-sion of a V-VIPE is 32 and the margin for the triplet lossis 1.0. Any 2D keypoint detector could be used, but wechose AlphaPose . We use COCO keypointsbecause they are widely used for 2D detectors. We imple-mented the model in PyTorch and we trained it on 1 GPU.",
  ". Metrics": "We evaluate the model using two metrics. The first is a hitmetric, inspired from , which we use to measure howoften we are able to retrieve a pose that is similar to a querypose. Given two normalized keypoints Si3D and Sj3D we firstapply a Procrustes alignment between the two to getA(Si3D) and A(Sj3D). Given a dataset with many views weselect two camera views. We find all embeddings for the2D poses from the selected cameras. Then, we query eachembedding from camera 1 and find the k nearest neighborsfrom the set of embeddings for camera 2. We consider a pair of embeddings a hit if their original 3D pose satis-fies MPJPE(A(Si3D), A(Sj3D)) < .1. We report Hit@k fork=1,10,20 and average over all pairs of cameras. This met-ric represents view invariance because it shows how well wecan match poses from one viewpoint to similar poses fromanother viewpoint.The second is the Mean Per Joint Position Error(MPJPE), which we define in .2. This error is usedto determine the distance between two sets of 3D keypoints.",
  ". Datasets": "In all the experiments we train on the standard training setof the Human3.6M dataset (H3.6M) . For our hit metricwe use the test set of H3.6M as the validation set and showresults on the MPI-INF-3DHP dataset (3DHP).Human3.6M. The H3.6M dataset contains 3.6 millionhuman poses taken from 4 different cameras. All of thesecameras are at chest level. The standard training set for thisdataset is made up of subjects 1,5,6,7 and 8. The standardtest set contains poses from subjects 9 and 11. For the eval-uation of the hit metric, we follow the method describedin , where they remove poses that are similar.MPI-INF-3DHP. 3DHP contains 14 different cameraangles. For our tasks we remove the overhead cameras,which leaves us with 11 cameras. Of these cameras, 5 areat chest height and the others have a slight vertical angle.This dataset is used to show whether or not our method willgeneralize to data that is different from the training data.",
  ". Augmentation": "In order to improve the models ability to generalize weintroduce camera augmentation similar to the work donein . To calculate this augmentation we take the groundtruth 3D pose and randomly rotate it.We then projectthis pose into 2D. We add augmented poses to each of ourbatches during training time. We found that it was best toadd augmented poses for half of the poses in each batch. . Hit metric results for different values of k. The upper part of the table shows the Hit metrics when using ground truth (GT)keypoints. The bottom part of the table shows the metrics when using keypoint detection(D) and augmentation(A). For Pr-VIPE and ourmethod AlphaPose is the keypoint detector. Epipolar Pose uses its own detector. The version of Epipolar Pose is trained on the Human3.6dataset and the # version is trained on the 3DHP Dataset. Epipolar pose does not generalize to unseen datasets.",
  ". Quantitative Results": "Similar Pose Retrieval Experiments.We compare ourmodel for hit metrics against 3 baselines. The first base-line is the PR-VIPE model, which attempts to define anembedding space without reconstructing the 3D pose; weadopted their open source code and re-trained their modelso we would have results on the same 2D pose detector, i.e.,AlphaPose. The second baseline is simply finding the near-est neighbor of the detected 2D keypoints. The third base-line uses Epipolar Pose to detect 3D keypoints. In thiscase, Procrustes alignment is performed between all posesand the closest aligned pose is selected as the match.We show the hit metrics for the different k values in. The top section of the table shows the results ofour method and of PR-VIPE when trained and tested withground truth (GT) 3D keypoints. The left part of the tablereports the results on the test set of H3.6M. We can see thatour approach is slightly worse than the PR-VIPE approach.This is because we are testing on very similar data to theoriginal training set. Our model, however, is designed togeneralize. The generalization of the model is demonstratedin the middle part of the table, where we report the perfor-mance on the 3DHP dataset when considering all availablecameras. In this case, our model gets higher values for allvalues of k. Moreover, when we pair one chest camera witha camera that is not at chest height, i.e., unseen cameraswith respect to the training data(right part of the table), wecan see that the gap is even larger. For example, when con-sidering k = 1, the gap between the two models is about4.2 percent for unseen cameras and 2.7 percent for all cam-eras. This demonstrates that the latent space we acquiredduring the VAE training is able to generalize to unseen cam-era viewpoints better than existing models.In the bottom section of the table, we show results whenthe keypoints are automatically detected(D). For PR-VIPEand our model we use AlphaPose. Epipolar Pose detectsits own keypoints. Again our method outperforms the PR-VIPE model when generalizing to data different from the",
  "training set, 3DHP, as well as to unseen cameras. For ex-ample, when k = 10 our method outperforms PR-VIPE byabout 5.6 percent for all 3DHP cameras, and by about 7percent for the unseen category": "In this section we also show results for detected key-points plus additional training data generated by augment-ing the 3D poses. We see an increase from just our detectionmodel for 3DHP because we have introduced new cameraviewpoints to the training data. We see an improvementover PR-VIPE when they use augmented data, although wedo not get as much of a boost from augmentation becauseour model already generalizes better than theirs. For k = 1our model outperforms theirs by 1.5 percent. Additionally, in the table we report the 2D keypoints andEpipolar Pose results. We can observe that using the 2Dkeypoints is not effective, as demonstrated by the low hitmetric for all k values. The Epipolar Pose# method per-forms better than both our method and the PR-VIPE methodbefore any augmentation is applied to the data because itis trained on the 3DHP dataset and does not need to gen-eralize.When you try to run the Epipolar Pose modelon 3DHP data the output does not resemble human pose.We do not report generalized results for Epipolar pose be-cause of this. Despite the fact that Epipolar Pose# is trainedspecifically for detection on the 3DHP dataset when we addaugmentation of the data to our model we are able to beattheir results by about 2 percent. 3D Pose Estimation Experiments. In addition to calculat-ing the hit metric described above our model also outputsthe predicted 3D pose. We find that the average error of thismodel is 62.1 millimeters. We calculated this number us-ing a model trained on keypoints detected by the CascadedPyramid Network as this is commonly usedfor 3D Pose Estimation. We find that while this number isnot competitive with current methods for pose estimationthat use more complex models or take in more information,such as sequences, it is similar to the error found in ,which we use as the backbone for our network. . Pose Estimation from 2D images of our model applied to different camera viewpoints. We show 4 sets of results. The groundtruth is on the left hand side of each example, while on the right we provide the 4 original views as well as our model 3D output for eachview.",
  "MPJPE": ". This figure demonstrates what a query and retrieval look like. On the left of each pair of images is the query pose and the imageon the right is the image that is considered the closest match by our model. Each pair of images is labeled with the MPJPE between thetwo poses. Its easy to see that some poses, such as the one on the far left, are easy to retrieve because they are so distinct. And others, suchas the one on the far right, have occluded points as well as other factors that make the nearest neighbor hard to find.",
  ". Qualitative Results": "2D to 3D Pose Estimation. shows examples ofour 3D estimations given a 2D image as input. We showexamples of 4 different poses each with 4 different cameraangles. In the two examples on the left we have very ac-curate retrievals. All of the cameras have similar retrievalsthat allow us to determine that the person is in the same posedespite the very different original camera angles. The exam-ples on the right are the ones where our model struggles tofind the whole pose. In the example on the top we are ableto find the hand position because the hands are visible inevery image, however our model struggles to detect that thebody is slightly angled. This is likely because the differencein 2D keypoints between an angled and not angled bodyare very small and our 2D keypoint detector is not accurateenough. In the example on the bottom our model succeedswith the arms, except for one camera viewpoint where thearm is not visible in the image at all. The other way ourmodel struggles is with the head tilt. This is likely becausethis is difficult to visualize from most camera angles.3D Pose Retrieval. We show how our model is able toretrieve similar poses from different view points. In you can see the query pose as well as the pose that isretrieved from a different view point. Ideally, the two poseswill be identical. This is the visualization of what the Hitmetric represents. If the queried pose is sufficiently close to",
  "the retrieved pose then we have a hit": "Visualizing V-VIPE. shows a t-SNE visualiza-tion, which we use to show the smoothness of the learnedV-VIPE space, where each dot represents a V-VIPE. Inorder to properly show the clustering we select 10 visuallydifferent 3D poses and color our visualization based onwhich of the 10 poses is the most similar to the posethat each point represents.It is easy to see from thisgraph that similar colors are typically found in clusters.This means that the space well represents the notion ofsimilarity between poses. We can see this even clearer inthe expansion of the visualization where we show threeposes and their locations in the cluster. The two poses onthe right are colored the same and are very close together.These are slightly different, but the overall pose is verysimilar. We then select a point that is very far away andhere we can see that the pose is quite different. 3D Pose Generation. Our model is able to generate newposes by adding noise to the embedding space of an existingpose. In we define a noise array z and add it to anembedding with increasing magnitudes. The pose continuesto move in one direction as we increase magnitude showingthat our embedding space is smooth. . Pose generation, starting from a 3D pose we select two random noise directions zi and generate poses using increasing magnitudesof noise zi, where {0.2, 0.3, 0.4, 0.5}. V-VIPE leads to smooth pose variations and can be used to generate unseen 3D poses. . t-SNE visualization of the V-VIPE space of our modelfor poses in the H3.6M dataset. Each color represents similarityto one of 10 key poses that we selected. In the expansion, threedifferent poses and their place in the visualization are shown.",
  ". Ablation Study": "We performed an ablative analysis in order to understandwhich of our design choices best contributed to our results.Triplet Loss. First we examine how important it is that weinclude the triplet loss term in our method. We remove itfrom the loss term and find that the new Hit@1 value is17.41 with no augmented data. This is a drop of 6.1 fromthe Hit@1 value when triplet loss is included. Therefore thetriplet loss value is important to the overall loss term.Data Processing. We examine how important it is for us torotate the 3D pose before training on our model. This step isimportant because it enables us to compare the similarity ofposes with two different global rotations without needing todo a time consuming Procrustes Alignment between everypair of poses. We find that the Hit@1 value on 3DHP withno augmentation obtained when using non rotated points is",
  "percent, a 5.5 percent decline from our approach": "Pretraining the Decoder. Finally, we studied whether ornot pretraining a VAE and using a defined embedding spacecontributed to our final hit metric. We found that the Hit@1value for the model with no pretraining is 23.4 versus the23.5 we obtained by completing the pretraining step. How-ever, this step is important anyways because it enables themodel to do 3D Pose Retrieval. Without it we would not beable to map our 3D poses to our embedding space. There-fore we would not be able to generate similar poses to agiven 3D pose or query a 3D pose to find a similar 2D posefrom a set of images.",
  ". Conclusion": "In this work we showed that by using only 3D poses to de-fine a V-VIPE space we can define a better camera invariantspace than if we were to only use 2D poses. We defined aprocedure made of two steps: first we train a VAE model tolearn a latent space of 3D poses; then, we train a 2D key-points encoder that is linked to the VAE decoder to allow 3Dreconstructions of 2D images. We adopted a VAE model asit creates a smooth latent space that can generalize better to-wards unseen poses during training. In order to achieve thisgoal, we train a VAE with a three component loss function.We performed an extensive experimental evaluation, by us-ing two datasets, i.e., Human3.6M and MPI-INF-3DHP. Wedemonstrated that the latent space is modeling a meaningfulnotion of similarity of the embeddings. This is reflected inthe Pose Retrieval experiments where we improve about 2.5percent in the Hit@1 metric when considering unseen cam-eras. We also showed qualitative examples demonstratingthe capability of our embedding space to capture the no-tion of similarity of poses. This is important in downstreamtasks. In the future we believe that this approach has a lot ofpromise for application to downstream tasks such as actionsegmentation and detection.Acknowledgements:Thisworkwaspartiallysup-ported by NSF CAREER Award (#2238769) to ASandtheDARPASAIL-ON(W911NF2020009)pro-gram. Ching-Hang Chen, Ambrish Tyagi, Amit Agrawal, Dy-lan Drover, M. V. Rohith, Stefan Stojanov, and James M.Rehg. Unsupervised 3d pose estimation with geometric self-supervision. CoRR, abs/1904.04812, 2019. 3",
  "Muhammed Kocabas, Salih Karagoz, and Emre Akbas. Self-supervised learning of 3d human pose using multi-view ge-ometry. CoRR, abs/1903.02330, 2019. 2, 6": "Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-ShuFang, and Cewu Lu. Crowdpose: Efficient crowded scenespose estimation and a new benchmark. In Proceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 1086310872, 2019. 5 Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang,and Cewu Lu. Hybrik: A hybrid analytical-neural inversekinematics solution for 3d human pose and shape estimation.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 33833393, 2021. 5 Haoyu Ma, Liangjian Chen, Deying Kong, Zhe Wang, Xing-wei Liu, Hao Tang, Xiangyi Yan, Yusheng Xie, Shih-YaoLin, and Xiaohui Xie.Transfusion:Cross-view fusionwith transformer for 3d human pose estimation.CoRR,abs/2110.09554, 2021. 2",
  "man pose estimation. CoRR, abs/1705.03098, 2017. 1, 2, 3,5, 6": "Dushyant Mehta,Helge Rhodin,Dan Casas,PascalFua, Oleksandr Sotnychenko, Weipeng Xu, and ChristianTheobalt. Monocular 3d human pose estimation in the wildusing improved cnn supervision. In 3D Vision (3DV), 2017Fifth International Conference on. IEEE, 2017. 2, 5 Aditya Panda and Dipti Prasad Mukherjee. Monocular 3dhuman pose estimation by multiple hypothesis predictionand joint angle supervision.In 2021 IEEE InternationalConference on Image Processing (ICIP), pages 32433247,2021. 2"
}