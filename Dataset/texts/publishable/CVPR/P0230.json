{
  "Abstract": "Image geolocation is a critical task in various image-understanding applications. However, existing methods of-ten fail when analyzing challenging, in-the-wild images. In-spired by the exceptional background knowledge of multi-modal language models, we systematically evaluate theirgeolocation capabilities using a novel image dataset and acomprehensive evaluation framework. We first collect im-ages from various countries via Google Street View. Then,we conduct training-free and training-based evaluations onclosed-source and open-source multi-modal language mod-els. we conduct both training-free and training-based eval-uations on closed-source and open-source multimodal lan-guage models.Our findings indicate that closed-sourcemodels demonstrate superior geolocation abilities, whileopen-source models can achieve comparable performancethrough fine-tuning.",
  ". Introduction": "Image geolocation refers to the process of determining thespecific geographic location from which a given image wastaken. This geographic information is crucial across variousdomains, including urban planning, environmental monitor-ing, and social media analysis. The ability to automaticallyidentify the location of images provides valuable insightsand supports numerous applications, such as augmented re-ality, location-based services, and geotagging.Despite its importance, image geolocation in the wild re-mains a challenging task, particularly when dealing withimages sourced from diverse sources such as social me-dia platforms and online repositories. While many previousworks have developed curated loss func-tions and model designs to tackle this challenge, they usu- ally have compromised performance when evaluated on im-ages in the wild. In contrast, recent advances in large mul-timodal models (LMMs) have demonstrated impressive ca-pabilities in background knowledge across a broad range oftasks. These models are trained on large-scale datasets, ex-hibiting outstanding understanding , reasoning ,and commonsense abilities. While numerous benchmarks have been established toevaluate various image understanding abilities of multi-modal language , little attention has been paidto their geolocation capabilities. To address this researchgap, we conduct the first systematic analysis of image ge-olocation abilities. First, we introduce a large-scale datasetof in-the-wild images sampled from diverse geolocations.Second, we comprehensively benchmark the capabilitiesof both open-source and closed-source multimodal lan-guage models through training-free and training-based eval-uations.",
  "Our contributions can be summarized as follows,": "Introduction of a novel image dataset: We presenta new dataset, exclusively sourced from Google StreetView, designed to challenge Large Multimodal Models(LMMs) through real-world, in-the-wild random images.This dataset is intended to serve as a robust benchmarkfor assessing these models ability to identify image loca-tions accurately. Comprehensive evaluation framework:We evalu-ate a diverse set of LMMs, including state-of-the-artclosed-source models like GPT-4V and Google Gemini,and promising open-source models such as BLIP ,Fuyu , InternLM-VL , and LLaVA . Our eval-uations, both training-free and training-based, thoroughlyassess these models geolocation accuracies at the coun-try level and their adaptability to challenging in-the-wildimage data.",
  ". Dataset": "Our datasets images are directly from Google Street View,while specific parameters were set in the API request tomimic common human sight.The up or down angle ofthe camera relative to the Street View vehicle is set to 0degrees to maintain a natural, level perspective. The hor-izontal field of view is fixed at 90 degrees, mirroring thehorizontal scope typical of human vision. To capture di-verse viewpoints, the cameras compass heading is adjustedto four fixed orientations: 0 (North), 90 (East), 180 (South),and 270 (West) degrees. All the images have the same sizeof 512x512. displays random sample images from our testset, which predominantly consists of natural landscapes andrural scenes with features such as water bodies, trees, andagricultural fields. These images do not include prominenturban infrastructure or significant man-made constructs,thereby increasing the complexity and intrigue of identify-ing each images geographical origin.",
  "provides a statistical overview of the dataset, de-": "tailing the distribution of these varied perspectives. It showsa methodical approach to capturing diverse orientations andgeographic locations within the proposed dataset. The tabledivides the dataset into three subsets: Test, Train, and Com-prehensive Train, each detailed with the count of imagesacross four compass headings and the total number of im-ages alongside the number of represented countries. Fromthe perspective of camera headings, the dataset maintainsa remarkable balance across all subsets, with each headingrepresented almost equally. The test set, train set, and com-prehensive train set have 1000, 2418, and 6408 images, witheach heading having exactly or around one-quarter of the to-tal images. When only considering the countries that appearin the test set, there are 2388 and 6011 images in the trainand comprehensive train set, respectively.",
  ". Dataset Distance Pairs Analysis": "Larger countries are more likely to have more images, soa strategy was taken to give bigger countries more chancesduring the random image pick-up process. This results ina significant imbalance in the representation of countries . Images samples about dynamic few shots strategy. The first image is the target image, which is for LLMs to guess where it wastaken, and the following images on the same row are their corresponding five most similar images based on CLIP embeddings ordered byEuclidean distance descending.",
  "H: the compass heading of the camera": "within the dataset. For instance, a country might have asfew as one image in the dataset. In contrast, another couldhave as many as 121, 293, and 661 images in the test, train-ing, and comprehensive training sets. To mitigate the poten-tial impact of this geographic imbalance on model trainingand evaluation, we implemented a policy ensuring that eachcountry represented in the Test set is also represented in theTrain set with at least two images and in the ComprehensiveTrain set with at least four images. shows the geographical diversity within ourdataset. We analyzed the physical distances between im-age pairs based on their geolocations, categorizing theminto intervals ranging from less than 10 kilometers(km)to over 1000 km.The result reveals a strategic empha-sis on maximizing geographical variance, with most im-age pairsmore than 96% across Test, Train, and Com-prehensive Train setsshowing separations of over 1000kilometers. The distribution significantly reduces the prob-ability of selecting visually similar images from proximallocations, ensuring the dataset spans a broad spectrum ofenvironmental and urban landscapes.",
  "ChatGPT-4V: An extension of the ChatGPT modelwith integrated visual processing capabilities, enabling itto understand and generate content based on text and im-ages": "Gemini:Gemini introduces a versatile multimodalmodel family excelling in understanding across images,audio, video, and text, with its vision capabilities settingnew benchmarks in image-related tasks and multimodalreasoning. Blip-2:BLIP-2 introduces a cost-effective vision-language pre-training approach that leverages existingpre-trained models with a Querying Transformer, achiev-ing state-of-the-art results in vision-language tasks withsignificantly fewer trainable parameters.",
  "architecture, excelling in digital agent tasks and offeringrapid, high-resolution image processing capabilities": "InternLM-XComposer2 (ILM-VL):It innovatesin vision-language interaction with a Partial LoRA tech-nique, excelling in creating and understanding complextext-image content, setting new benchmarks in multi-modal performance. LlaVA: LLaVA 1.5 sets a new standard in large mul-timodal models with a highly efficient vision-languageconnector, achieving unprecedented performance on 11benchmarks using minimal data and training resources.",
  ". Dynamic few-shots strategy": "For the dynamic few-shots strategy, derived from Retrieval-Augmented Generation(RAG) techniques, DINOv2and CLIP were employed to generate embedding fea-tures from the train and test set. After that, for each imagein the test set, the kNN algorithm was used to find sim-ilar images from the train set. shows that CLIPoutperforms DINOv2 in the top 1 and top 5 evaluation lev-els, achieving an accuracy of 0.312 and 0.586, respectively.When LLMs were evaluated with the dynamic few-shotsstrategy, for each image to be guessed, the top 5 imageswere determined by the kNN algorithm through embed-dings generated by CLIP as it has better results than DI-NOv2.",
  "Must: To address cases where limited information mayprevent answering a country, we employ imperativeprompts to compel the model to make a country guessfor each image": "Tips: We offer general guidelines to the model, suggest-ing it consider factors like sun position, license plates, andother identifiable features within the image to infer the ge-ographic location without directly providing this specificinformation. These uniform guidelines apply to all mod-els across every evaluation round. S-5-shot:The model is given five additional images,each tagged with their respective countries, as referencesbefore it predicts the country of a new image. These refer-ence images remain consistent across all models and eval-uation rounds. An example is shown in .",
  ". Training-free Evaluation": "shows the training-free evaluation results with dif-ferent prompts input except for GeoGLIP, as it only takesthe image as input, and its output is geolocation.From , we can see that Gemini performs betterthan other models in all strategies. Gemini achieves similaraccuracy, nearly 0.67, for the Basic, Must, and Tips strate-gies. It also outperforms comparable models using the fewshots strategies with an accuracy of up to 0.746. We did nottest few-shot scenarios for the ChatGPT-4V model, whilethe current BLIP and Fuyu do not support using multipleimages as input.In terms of open-source models, BLIP-2-2.7B has thehighest accuracy for the Basic prompt, and BLIP-2-T5-XLachieves best for the Must and Tips prompt cases, with anaccuracy of 0.365 and 0.361, respectively. The accuracyof the Tips case for model BLIP-2-2.7B drops to 0.002 be-cause the model is very sensitive to the text input and unableto handle the context if it is relatively long.The ILM-VL model achieves good performance in thesingle image input cases and for the few-shot cases; whilethe ILM-VL model can take a few images as input, its abil-ity to deal with multiple images in question-and-answertasks almost drops to zero.The few-shot strategies show their effectiveness forGemini, while the static, dynamic, and random strategiesdo not significantly affect Gemini. As for LLaVA, taking5 closest images with their country names as part of theprompt for the guessed image can significantly improve theaccuracy by more than 50% compared to the highest ac-curacy for only text input as prompt. Taking the same 5images with their country names for every round of Q&Atasks does hurt the performance. This can be attributed tothe hyperparameter of temperature being set to 0. In thiscase, as the image to be guessed is only a small portion ofthe input, the output may be preferred to stick to similaroutputs inherited from the inputs. Finally, the 4 outcomesof the few-shot strategies also demonstrate that the input or-",
  ". Training-based Evaluation": "illustrates the efficacy of our dataset in enhancingthe accuracy of LLMs for determining the location of im-ages. The results indicate a significant improvement whenmodels are fine-tuned with either the train set or a compre-hensive train set, employing Basic, Must, and Tips strate-gies. The enhancement in accuracy, observed after fine-tuning models with our dataset, can be substantialmorethan double in some cases.LLaVA-13B(T) has the highest accuracy, 0.567, alongwith the strategy of the Basic strategy.However, theoutstanding performance is not significant as LLaVA-7Bachieved an accuracy of around 55% across three strate-gies and 2 train sets. It outperforms the close source modelChatGPT-4V in those three cases. ILM-VL also shows bet-ter results to above 40% after fine-tuning, which surpassesall the open source models before fine-tuning.One noticeable thing is that fine-tuning an LLM withmore images along with an answer only does not guaranteebetter performance in this geolocation guessing task. It canbe observed that there are 6 of 9 cases in the model wherefine-tuning with the train set shows higher performance thanfine-tuning with the comprehensive train set.",
  ". Discussion": "In this work, we conduct the first systematic study in imagegeolocation abilities of multimodal language models. Wefirst introduce a novel dataset comprised of images sampledfrom Google Street View API. The dataset is diverse, en-compassing varied perspectives and landscapes from multi-ple countries, which allows for comprehensive benchmark-ing of multimodal language mnodels geolocation abili-ties. We employed multiple training-free evaluation strate-gies from simple prompts, chain-of-thought, and few shotprompting. We further fine-tuned two open-source modelsusing our collected dataset, which significantly enhancedthe accuracy of these models in predicting the geographicorigin of the images at the country level.While our findings contribute valuable insights into thecapabilities of LLMs in image-based geolocation tasks, sev-eral limitations are notable. Firstly, our evaluations wereconfined to country-level geolocation without extending itto more granular levels, such as state and city identifica-tions. Additionally, the majority of our dataset images arenatural landscapes and rural scenes, which may not ade-quately represent the complexity and diversity of urban en-vironments.In future research, we aim to test geolocation accuracyat more granular levels or even provide a precise latitudeand longitude coordinate.This expansion will allow us",
  "T: finetune with train set; CF: finetune with comprehensive train set": "to understand better LLMs capabilities and limitations inmore densely populated and geographically complex en-vironments. Furthermore, to address the current datasetsemphasis on natural and rural landscapes, we plan to en-rich it with a broader array of images, including urban set-tings with diverse architectural styles and infrastructural el-ements. This enhancement will provide a more robust LLMtestbed and potentially improve the models usefulness inpractical, real-world applications where urban geolocationis critical.",
  "Rohan Bavishi, Erich Elsen, Curtis Hawthorne, MaxwellNye, Augustus Odena, Arushi Somani, and Sagnak Tasrlar.Fuyu-8b: A multimodal architecture for ai agents, 2024. 1, 3": "Vicente Vivanco Cepeda,Gaurav Kumar Nayak,andMubarak Shah.Geoclip:Clip-inspired alignment be-tween locations and images for effective worldwide geo-localization. arXiv preprint arXiv:2309.16020, 2023. 1, 3 Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao,Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang,Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2:Mastering free-form text-image composition and compre-hension in vision-language large model.arXiv preprintarXiv:2401.16420, 2024. 1, 4",
  "James Hays and Alexei A Efros. Im2gps: estimating geo-graphic information from a single image. In 2008 ieee con-ference on computer vision and pattern recognition, pages18. IEEE, 2008. 1": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal,HeinrichKuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al.Retrieval-augmented generation for knowledge-intensive nlptasks. Advances in Neural Information Processing Systems,33:94599474, 2020. 4 Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.Blip-2:Bootstrapping language-image pre-training withfrozen image encoders and large language models. In In-ternational conference on machine learning, pages 1973019742. PMLR, 2023. 1, 3",
  "OpenAI.GPT-4V(ision) System Card, 2023.Accessed:2024-03-31. 3": "Maxime Oquab, Timothee Darcet, Theo Moutakanni, HuyVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.Dinov2: Learning robust visual features without supervision.arXiv preprint arXiv:2304.07193, 2023. 4 Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages87488763. PMLR, 2021. 4, 5",
  "Zhiqiang Wang, Yiran Pang, and Yanbin Lin.Large lan-guage models are zero-shot text classifiers. arXiv preprintarXiv:2312.01044, 2023. 1": "Meiliu Wu and Qunying Huang.Im2city:image geo-localization via multi-modal learning. In Proceedings of the5th ACM SIGSPATIAL International Workshop on AI for Ge-ographic Knowledge Discovery, pages 5061, 2022. 1 Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, RuoqiLiu, Ge Zhang, Samuel Stevens, Dongfu Jiang, WeimingRen, Yuxuan Sun, et al. Mmmu: A massive multi-disciplinemultimodal understanding and reasoning benchmark for ex-pert agi. arXiv preprint arXiv:2311.16502, 2023. 1"
}