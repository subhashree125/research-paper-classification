{
  "ABSTRACT": "We used a 3D simulator to create artificial video data with standardized annotations, aiming to aidin the development of Embodied AI. Our question answering (QA) dataset measures the extent towhich a robot can understand human behavior and the environment in a home setting. Preliminaryexperiments suggest our dataset is useful in measuring AIs comprehension of daily life.",
  "Introduction": "As Embodied AI continues to develop, understanding the time and place of actions in daily life becomes increasinglyimportant[Posner and Fei-Fei(2020), Ahn et al.(2022), Blukis et al.(2022), Paolo et al.(2024), Davis et al.(2010)].Datasets and benchmarks have been created to support their development, and challenges have been presented[Deitke et al.(2022), Grauman et al.(2022), Ning et al.(2023)]. Most of this data consists of recorded images of everyday life, annotations, and descriptions. The annotations wereperformed manually and were imprecise; not everything in the room was annotated. The behavior of what the persontries to do needs to be fully described in these descriptions. Nishimura et al. [Nishimura et al.(2021)] proposed PrimitiveActionOntology1 to abstract activity labels in recognitiondatasets based on HomeOntology [Vassiliades et al.(2020)] and International Classification of Functioning, Disabilityand Health (ICF)2. They also proposed a HomeObjectOntology3 based on VirtualHome assets, objects defined inCharades [Sigurdsson et al.(2016)], and objects that occurred in the videos in the video archive called Elderly BehaviorLibrary4. We created artificial video data (MMDL: Multimodal Dataset of Daily Life) using a 3D VirtualHome-AIST [Ugai et al.(2024)] simulator, which is based on VirtualHome [Puig et al.(2018)] and, using Virtual-Home2KG [Egam et al.(2023)], created data describing what it is and where it is located for more objects. These dataalso clarify what the data are from the scripts that are placed in the simulator to make the avatar work. The annotationsare mechanically generated with a standard vocabulary based on PrimitiveActionOntology and HomeOntology, whichcontributes significantly to the development of Embodied AI as they are consistent and free of contradictions.",
  "Multimodal Datasets and Benchmarks for Reasoning about Dynamic Spatio-Temporality in Everyday Environments": "[Sigurdsson et al.(2016)] Gunnar A. Sigurdsson, Gl Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and AbhinavGupta. 2016. Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding. In ComputerVision ECCV 2016, Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (Eds.). Springer InternationalPublishing, Cham, 510526.[Ugai et al.(2024)] Takanori Ugai, Shusaku Egami, Swe Nwe Nwe Htun, Kouji Kozaki, Takahiro Kawamura, and KenFukuda. 2024. Synthetic Multimodal Dataset for Empowering Safety and Well-being in Home Environments.arXiv:2401.14743 [cs.AI][Vassiliades et al.(2020)] Alexandros Vassiliades, Nick Bassiliades, Filippos Gouidis, and Theodore Patkos. 2020. AKnowledge Retrieval Framework for Household Objects and Actions with External Knowledge. In SemanticSystems. In the Era of Knowledge Graphs, Eva Blomqvist, Paul Groth, Victor de Boer, Tassilo Pellegrini, MehwishAlam, Tobias Kfer, Peter Kieseberg, Sabrina Kirrane, Albert Meroo-Peuela, and Harshvardhan J. Pandit (Eds.).Springer International Publishing, Cham, 3652.",
  ": Example of video snapshot and action script": "shows an action script titled Do work on computer and a snapshot of the video generated from it inVirtualHome-AIST. The first line of the action script is the title, the second line is the description, and the fourth andsubsequent lines are the rows of the avatars behavior. There are 3,530 different videos, each of which shows a shortchunk of behavior, called an activity, of approximately 30 seconds to a minute in length. They were generated from 706scenarios (action scripts); for one scenario, five videos were generated with different camera positions. The charactersbehavior (avatars) in the videos and the 3D coordinates of approximately 400 objects in the house were annotated asdata using VirtualHome2KG. The states of lights and other electrical appliances, such as on/off, and the opening/closingof the fridge and room doors, were also recorded [Ugai et al.(2024)], and the 2D positions of the camera image werealso annotated for the objects with which the avatar was involved. 2D annotation is provided in the same scene graphformat as Action Genome [Ji et al.(2020)].",
  "Listing 1: Example of Question and AnswerQ:Wherei sthe man 10 secondsl a t e rfromthebeginningofthevideo ?A1 :LivingroomA2 :BedroomA3 :KitchenA4 :Bathroom": "QA can pose different types of questions. They can be a choice (Listing 1) or a simple yes or no answer. Thequestions were designed to gather information about the location, action, object, time, and combination of topics beingdiscussed based on TempCompass [Liu et al.(2024)] and MVBench [Li et al.(2023)]. In addition, there are questionsthat focus on the appropriate caption for a video, which can be either short or long.",
  "Gemini0.70.90.40.50.8Video-LLaVa0.50.40.250.10.6": "These questions were classified into two types: descriptive and quantitative. Descriptive questions were used to obtainfactual information or details about the topic, event, object, or situation. They typically start with words like what,does, where, and when. The quantitative questions were designed to obtain numerical or quantitative data. Theytypically start with words like how many, how much, how long, and how often. The data were designed to provide answers to 70 types of questions for longer time frames, defined in columns ofthree to seven activities, and provided in JSON format. The QA data were divided into two parts: learning (80%) andevaluation (20%) data, both containing answers. The training data not only lacks answers but also provides annotated data with missing locations, actions, and objectsthat correspond to the answers. Additionally, the questions were categorized as Easy or Hard. Easy questions have onlytwo options, while hard questions have around 30 options for actions, and all objects (about 200) that exist in the houseare considered candidates for objects.",
  "Preliminary Experiment": "In the Knowledge Graph Reasoning Challenge 2024, one of the strategies [Hirano et al.(2024)] is to complete theannotations provided in the Knowledge Graph from the video. If we can accurately fill in all the missing parts of theannotation, we can answer all the questions correctly. We have used video clips and questions about the missing actions,locations, objects, and time as input. We tested the Large Language Models to answer the questions, and is theresult of our experiment. Overall, Gemini performs well. In particular, the distinction between the four types of roomsis mostly accurate. Video-LLaVa, on the other hand, does not understand the time elapsed in the video.",
  "Summary": "This article discusses the creation of a dataset that supports the development of Embodied AI. The dataset includesartificial movie data and a QA dataset to measure the AIs comprehension of human behavior in a home environment.The results of the initial experiments show that the dataset is useful for measuring the AIs understanding of humanbehavior and the surrounding environment in a home. We are planning to organise a technology contest (Challenge) inthe future. All data is publicly available from",
  "This paper is based on results obtained from projects, JPNP20006 and JPNP180013, commissioned by the New Energyand Industrial Technology Development Organization (NEDO)": "[Ahn et al.(2022)] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, ChelseaFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, JulianIbarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi,Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, CarolinaParada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, NicolasSievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, MengyuanYan, and Andy Zeng. 2022. Do As I Can and Not As I Say: Grounding Language in Robotic Affordances. InarXiv preprint arXiv:2204.01691.[Blukis et al.(2022)] Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, and Yoav Artzi. 2022. A PersistentSpatial Semantic Representation for High-level Natural Language Instruction Execution. In Proceedings of the 5thConference on Robot Learning (Proceedings of Machine Learning Research, Vol. 164), Aleksandra Faust, DavidHsu, and Gerhard Neumann (Eds.). PMLR, 706717."
}