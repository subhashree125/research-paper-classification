{
  "Abstract": "Contextual cues related to a persons pose and interactionswith objects and other people in the scene can providevaluable information for gaze following.While existingmethods have focused on dedicated cue extraction methods,in this work we investigate the zero-shot capabilities ofVision-Language Models (VLMs) for extracting a widearray of contextual cues to improve gaze following per-formance.We first evaluate various VLMs, promptingstrategies, and in-context learning (ICL) techniques forzero-shot cue recognition performance. We then use theseinsights to extract contextual cues for gaze following, andinvestigate their impact when incorporated into a stateof the art model for the task. Our analysis indicates thatBLIP-2 is the overall top performing VLM and that ICLcan improve performance. We also observe that VLMs aresensitive to the choice of the text prompt although ensem-bling over multiple text prompts can provide more robustperformance.Additionally, we discover that using theentire image along with an ellipse drawn around the targetperson is the most effective strategy for visual prompting.For gaze following, incorporating the extracted cues resultsin better generalization performance, especially whenconsidering a larger set of cues, highlighting the potentialof this approach.",
  ". Introduction": "Understanding where a person is looking in a scene, alsoknown as gaze following, has diverse applications, includ-ing human-robot interaction , conversation anal-ysis , and the study of neurodevelopmental disor-ders . However, this is a challenging task, demand-ing a model to interpret a large spectrum of contextual cuessuch as the persons interactions with objects and other peo-ple in the scene. For instance, it has been shown that eye",
  ". As humans, we rely on various sources of informationto predict a persons gaze target. This image shows contextualinformation that could be valuable": "and hand movements are coordinated during manipulationactivities . Or that during conversations, people usu-ally look at the person talking and that leveraging thisinformation can help in gaze target selection in meeting set-tings . As seen in , estimating the childs gazetarget requires understanding their head and body pose, theinteraction between the child and the adult through pointingand shared attention etc.In order for a model to capture these cues and learn theirimpact on gaze target selection, it would need to be trainedon a high-quality and large-scale labeled dataset. However,existing gaze following datasets are small-scale, hin-dering the effective utilization of these cues. To addressthese challenges, prior works have relied on dedicated cueextraction methods such as inferred body pose ,and supplied them to models for improved performance.However, these approaches focus on specific cues and donot supply the larger array of contextual cues which couldbe needed for accurate gaze target prediction. Traditionalmethods to address this limitation include: (1) manually an-notating for relevant cues; but it is cost-intensive and notalways available during inference, or (2) pseudo-labellingwith expert models; however this requires access to a multi-tude of task specific models for each cue or a subset of cues.Hence, it is evident that novel solutions are required.",
  "arXiv:2406.03907v1 [cs.CV] 6 Jun 2024": "Given these challenges, we investigate the potential ofVisual Language Models (VLMs) to extract valuable con-textual cues for gaze following, aiming to overcome theconstraints of traditional labeling approaches. VLMs haveshown promising zero-shot performance for a variety oftasks , owing to their ability to learn visual-text as-sociations at scale. Hence, a single model may be capableof extracting all relevant contextual cues. At the same time,given the zero-shot setting, the set of cues to be consideredcan be adjusted based on the domain, further increasing thisapproachs applicability.In this work, we consider cues related to pose, person-person interactions, and person-object interactions. We firstevaluate the zero-shot performance of different VLMs forrecognising these cues (), and leverage the bestperforming approach to extract them. We then investigatewhether incorporating these extracted cues can improvegaze following performance ().Challenges. While VLMs have shown impressive zero-shotperformance for a variety of tasks, these tasks (ex. imageclassification) usually involve processing the entire image.However, for accurate gaze following we also need to cap-ture contextual cues related to each person in the scene.Hence, we need to consider an appropriate visual promptto allow the VLM to focus on the person of interest. Atthe same time, it is important to consider the choice of textprompt as VLMs have been shown to benefit from promptengineering . Finally, given the extracted cues from theVLMs, we need to consider how to incorporate such infor-mation into a gaze following model. Following these re-search questions, we make the following contributions: VLMs for contextual cues extraction: We explore 4state of the art VLMs for this task. We alsoinvestigate different visual prompts to focus on the per-son of interest, and different text prompts to describethe cue of interest. We show that VLMs can indeedcapture contextual cues although the choice of VLM,visual prompt and text prompt impacts performance. Text improved Gaze Following: We incorporate the ex-tracted contextual cues into a recent transformer basedgaze following model . Our results indicate that in-corporating these cues can result in better generaliza-tion performance, especially when considering largersets of cues.",
  ". Related-Work": "Vision-Language Models (VLMs). VLMs began receiv-ing significant attention following the introduction of CLIP(Contrastive Language-Image Pretraining) . This con-trastive learning framework learns effective multi-modalrepresentations using image-text pairs. CLIP demonstratedimpressive zero-shot classification performance on standardimage classification benchmarks. Subsequent VLMs, such as BLIP and BLIP-2 , have been introduced withnotable differences from CLIP. These include varied objec-tives (incorporating additional losses beyond the contrastiveapproach), the use of more curated training datasets, and en-hanced image caption generation through caption filtering.Specifically, BLIP-2 has introduced advanced pre-trainingstrategies, integrating a frozen image encoder and a Query-ing Transformer (Q-Former), which enables a nuanced ex-traction of visual representations. They have even shownstrong performance for video tasks such as action recogni-tion and text-to-video retrieval despite not pro-cessing the temporal dimension.However, their perfor-mance for localizing the actions/cues of people in an imagehas not been explored.While there has been some work on video language mod-els, they face certain limitations. Firstly, available video-text pairs for pretaining is limited compared to the scaleof available image-text pairs , so these methods do notgeneralize as well. To cope with this issue, some worksleveraged pre-trained image-based VLMs and adapted themfor video input, however, it harmed their zero-shot perfor-mance . Secondly, these models are computation-ally more expensive, and cannot be applied on static gazefollowing datasets.More recently, VLMs such as BLIP2 have leveragedLarge language models (LLMs) for generating textual out-put.LLMs have shown an impressive ability to act asmodels of the world, with a rudimentary understanding ofagents, beliefs and actions , and an ability to performcommonsense and mathematical reasoning . Hence, theymay also be capable of capturing complex relationships be-tween people and objects in the scene to better extract con-textual cues for human behaviour understanding. Further,they have been shown to benefit from in-context learning (ICL), wherein a few demonstrations of the task are pro-vided at inference time without any weight updates. VLMsthat exploit such LLMs have also displayed improvementsin performance using ICL , by being provided with afew sample visual and/or textual demonstrations. It is still aresearch question whether LLMs and ICL can improve therecognition of gaze contextual cues.Visual and Textual Prompting. The recent work of Sht-edritski et al. explored different visual prompting ap-proaches for CLIP. They compared cropping the visual areaof interest versus drawing a red ellipse around it, and foundthe red ellipse approach to perform better for keypoint local-ization and referring expression comprehension tasks. Theyalso observed that blurring or graying the region outside theellipse can provide additional benefits. However, the perfor-mance of these approaches for action/cue recognition hasntbeen explored. On the textual prompting side, the originalCLIP paper showed that prompt engineering, such asusing the prompt a photo of a {label} improved perfor- mance over using just the label text on ImageNet . Theyalso observed that ensembling different prompts in the em-bedding space can reliably improve performance. Recentworks introduced learnable prompts that were fine-tuned on a specific task. However this approach requiresdata to adapt, and hence cannot be applied in a zero-shotmanner. Also, although the learnable prompts can then beincluded with prompts for unseen classes, the results lagbehind manual prompt engineering efforts .In thiswork, we evaluate different manual prompt engineering ap-proaches that can be applied to any new set of classes.Contextual Cues for Gaze Following. Previous works inestimating the Visual Focus of Attention (VFOA) of a per-son leveraged cues such as the head pose and speaking in-formation of people for improved performance.However, these methods typically require access to frontalviews of people and knowledge of the 3D scene structure(ex. using multiple calibrated cameras) for inference. Thismakes it challenging to deploy these algorithms in new en-vironments where such information may not be available.Hence, Recasens et al. proposed the Gaze Follow-ing task to estimate the scene gaze target of a person inan image using only the image and with no prior assump-tions about the scene or camera placement. However, dueto the complexities of scenes and the lack of data, mod-els have encountered challenges in capturing pertinent in-formation, ultimately leading to sub-optimal predictions.Hence, recent methods have shown that inferred cues suchas depth and body pose can be leveraged for improved performance. However, in-corporating person-specific auxiliary information in thesemodels is not straightforward.More recently, proposed a new transfomer basedmodel for gaze following and social gaze prediction. Theyshowed that this architecture allows for easily incorporatingperson-specific auxiliary cues, with improved performancefrom the addition of peoples speaking status. Whether itcan benefit from additional cues remains to be explored.",
  ". Method": "We employ the static version of the recently proposedMTGS model. This model is a transformer-based ar-chitecture designed for multi-person gaze following and so-cial gaze prediction. Given an input image and head cropsof people in the scene, it first produces two types of tokens:image tokens (ximage RND), similar to those in a stan-dard Vision Transformer (ViT) architecture , and persongaze tokens (xgaze RP D), where P represents the num-ber of people in the scene. Person tokens are generated us-ing head crops, a gaze backbone, and a subsequent linearprojection layer. This formulation naturally supports incor-porating contextual cues for each person, as the informationcan be fused with the corresponding person token.Given the success of additive fusion in the case of po-sition embeddings for transformers , and early fusion ofbody pose and depth information for gaze following mod-els , we aim to incorporate contextual information de-rived from VLMs in an early fusion and additive manner.To this end, as illustrated in , we use a linear pro-jection layer () to project the vector of predicted scores(Svlm RP K, K is the number of classes) and gener-ate person context tokens matching the dimensions of theperson gaze tokens ((Svlm) RP D). We then applythe add operation to combine the person context tokensfrom the VLMs with the corresponding person gaze tokens.Following this, the enriched person gaze tokens, now withadded contextual cues, and the image tokens are fed intoMTGS, where, people and scene tokens interact throughself and cross-attention modules across multiple blocks.",
  "eeTkeT)(2)": "The Ensemble approach utilizes the mean embedding, act-ing as a centroid for a given class and thus is expected to bemore robust. The scores for each class are then normalizedacross samples to have a zero mean and a standard devi-ation of one. In this work, we investigate three differentpre-trained VLMs such as CLIP , BLIP and BLIP-2 . For more details regarding these models, we referthe readers to the original papers and details in Sec. 2.Visual Question Answering (VQA). In order to explore the potential of LLMs for our task, we investigate a re-cent VQA model, BLIP-2 VQA , that leverages a LLMcalled FlanT5 . In VQA models, a textual question isjointly input with an image to the model, and the modeloutputs a textual answer. We convert the text prompts de-scribed previously into a set of questions that result in sim-ple yes or no answers, which we then convert into abinary score. Examples of prompts are displayed in supple-mentary Fig 9. To further explore the benefits of ICL, weprovide additional textual context in the form of a generatedcaption from the same model. Thus, the text input to themodel is of the form {generated caption} {text prompt}. Itis worth noting that the BLIP-2 VQA model is much slowerto run than the ITM models as (1) the model is much largerdue to the LLM, and (2) the answer is conditioned on theimage and question, so we need to run a forward pass foreach image-prompt pair. This is unlike the ITM modelswhere the images and prompts can be processed separately,with a similarity score computed afterward.",
  ". Experiments": "Datasets.We employ two datasets to shed light on theVLMs ability to extract meaningful cues.ChildPlay: We manually annotated 6 cues from theChildPlay dataset, which is a recently proposed datasetfor gaze following.For each class, we selected around50 clear positives and 50 clear negatives. The classes andstatistics are presented in the supplementary .AVA-Actions: Then, to scale our evaluation we used thevalidation split of the AVA dataset , which is a humanaction localization dataset. This dataset is much more chal-lenging since it is heavily unbalanced and large scale con-taining around 41000 images. A subset of the classes ofinterest was selected. In , a summary of the datasetclasses and distribution is shown.Metrics. We leverage two metrics: AP: To evaluate the performance of different VLMsand prompting approaches, we use Average Precision(AP). It is computed per class between the ground truthand the scores obtained from the VLMs. We also con-sider the mean of the AP scores across all classes ormean Average Precision (mAP). Accuracy: Since the output of the VQA variants is abinary decision, we cannot compute AP; instead, wecompute accuracy. To compare with ITMs, we bina-rize their output by applying a threshold of zero sincethe scores are normalized with a zero mean (howeverthey may benefit from optimizing the threshold).",
  ". Results of different visual prompting approach on Child-play. Image corresponds to input the full image whereas personrefers to the use of person crop as input": "prompts, and categorized by the type of visual input, i.e.image-based versus person-based. We see that image-basedapproaches outperform person-based variants.This sug-gests that a broader visual input provides additional context,enhancing the zero-shot recognition for the target person inthe image. Furthermore, among the visual prompts, the redellipse approach outperforms others, aligned with findingsin . Therefore, in subsequent experiments, we employthe image-based red ellipse as the visual prompt.VLMs.We compare the performance of three VLMs,namely CLIP, BLIP, and BLIP-2. In , we present aclass-wise comparison of the three VLMs on AVA. Notethat, for each VLM, we aggregate the results from different . Results of different VLMs following the ITM approachon AVA. Three VLMs are compared across different classes cate-gorized as Pose (P), Person-Person Interaction (P-P), and Person-Object Interaction (P-O).",
  ". Results of different templates using BLIP-2 on AVA. Sixtemplates are compared across different classes": "textual prompts. Firstly, we observe that no single modelalways outperforms the others. However, BLIP and BLIP-2surpass CLIP in pose and person-to-person classes, whileCLIP performs well when the class refers to a clear object,such as work on a computer or text on a cellphone. Thismay be related to differences in training data, and is a di-rection for investigation. On average, BLIP-2 is the top per-forming model. In the subsequent analysis, we continue fo-cusing on BLIP-2 while varying the text prompting aspects.Text prompting.We investigate the impact of the textprompts described in .1 at two different levels, atthe template level and synonym level. When evaluating thetemplate, we aggregate results over the other text promptcomponent variations. Similarly for when we evaluate theclass synonym. In , performance for different tem-plates are shown on AVA per class. Firstly, there is no besttemplate overall, which correlates to the finding of that",
  ". Results of BLIP-2 vqa with and without in-context learn-ing, vqa ICL and vqa respectively, on ChildPlay. It is comparedwith the VLMs CLIP, BLIP and BLIP-2": "VLMs are prompt sensitive. However, using the Ensem-ble approach described in .1 provides more robustperformance, often outperforming the best template, and al-ways outperforming the worst template. In addition, thewording in textual prompts matters, as can be seen in thesupplementary , where different class synonymscan change the performance by a large margin. However,we notice that for most of the classes, including {person}in the prompt improves performance. This suggests thatconditioning the prompt to an individual helps to extractperson-centric information.",
  ". VQA Results": "To investigate the potential of LLMs and in-context learningfor contextual cues extraction, we evaluate the BLIP-2 VQAmodel on the ChildPlay dataset, and compare it against ITMbased VLM models (). Note that the results areaggregated across all text prompts. As mentioned in Sec-tion 3.1, the BLIP-2 VQA model is much slower to runcompared to the ITM based models which is why we usethe smaller ChildPlay dataset. We also use a smaller set oftemplates and synonyms in the text prompt ( in sup-plementary) to reduce computation time.Benefit of LLM. Comparing the performance of BLIP-2against BLIP-2 VQA (BLIP-2 and vqa in the figure), wesee that BLIP-2 VQA does much better for the child class,but on par of worse for the other classes. This suggeststhat the LLM in the BLIP-2 VQA model is not necessarilyproviding better results. However, as mentioned previously,this model uses a smaller set of templates and synonyms inthe text prompt for computational reasons so may benefitfrom using a larger set.In-Context Learning. We see that the BLIP-2 vqa modelwith ICL improves for all classes except the child classcompared to no ICL. This is in contrast to the observa-tions in the original paper where the architecture is intro-",
  "X P": ". An overview of Text-Improved Gaze Following: Givenan image containing P persons, image tokens and person tokensare generated via a Linear Projection (LP) and a person module(PM) respectively. To incorporate VLM contextual information,we use a VLM to obtain P score vectors, each with the dimensionas the number of classes (K). We then linearly project these vec-tors and perform early fusion by adding them to the correspondingperson tokens. Scene and updated person tokens are subsequentlypassed to MTGS to model person and scene interactions us-ing self and cross-attention modules across multiple blocks. gaze target point. The test set contains annotations by mul-tiple annotators. Despite lower quality images and annota-tions, its diversity makes it a good dataset for pre-training.ChildPlay is a recent video dataset for gaze follow-ing, featuring children playing and interacting with otherchildren and adults. It is annotated with the head bound-ing box, gaze point and gaze label (consisting of 7 non-overlapping classes) of people in the scene.Following , we pre-process both datasets to extractpair-wise social gaze labels for two tasks:",
  "HICO: The HICO dataset is human-object interac-tion dataset that defines a list of 117 interaction verbs.We leverage these verbs as contextual cues": "SWIG: The SWIG-HOI dataset is a large-scalehuman-object interaction dataset that defines 406verbs. We leverage these verbs as contextual cues.We provide the manually curated synonyms and templatesused for generating different text prompts for AVA+CP inFigures 8,9 of the supplementary. For HICO and SWIG, weuse the same set of templates, but generate 4 synonyms for",
  "MTGS + SWIG0.9330.1190.0610.619": ". Results for incorporating VLM context with different setsof classes on the GazeFollow dataset. AVA+CP has 24 classes,HICO has 117 classes and SWIG has 406 classes. Best results aregiven in bold, second best results are underlined. each cue using ChatGPT .Training and Validation.Following , we train themodel for 20 epochs on GazeFollow using a learning rateof 1e-4 and the AdamW optimizer. We supervise usingthe standard MSE loss for gaze heatmap prediction, and bi-nary cross entropy loss for LAH prediction. For validation,we use the split proposed by .Metrics. We use the standard gaze following metrics:",
  "AUC: the predicted heatmap is compared against a bi-nary GT map with value 1 at annotated gaze point po-sitions, to compute the area under the ROC curve": "Distance (Dist.): the arg max of the heatmap providesthe gaze point. We can then compute the L2 distancebetween the predicted and GT gaze point on a 1 1square. We compute Minimum (Min.) and Average(Avg.) distance against all annotations.In addition, we compute F1 scores for LAH (F1LAH) andLAEO (F1LAEO). For LAH, we check if the predicted gazepoint falls inside the target persons head bounding box. ForLAEO, we check the reverse as well. 1",
  ". Results": "GazeFollow. We provide results for incorporating VLMcontext on the GazeFollow dataset in . We observethat performance does not change much for the distancescore. In contrast, for LAH, we observe a slight improve-ment with the addition of AVA+CP cues, and a degradationwith the addition of SWIG cues. However, the GazeFol-low test set is very small (approx. 5k instances), and oftencontains simple scenes with a single salient target such asthe held object. Also, annotations on GazeFollow are notalways reliable as mentioned in .2. Hence, analyz-ing results on GazeFollow alone is not sufficient.",
  ". Ablation on early vs multi-stage fusion of VLM contextusing the AVA+ChildPlay classes on the GazeFollow dataset. Bestresults are given in bold": "ChildPlay. To further investigate the properties of our mod-els, we perform cross-dataset evaluation on ChildPlay. TheChildPlay test set has a large number of instances (approx.20k), and contains challenging scenes with multiple salienttargets (ex. toys, other children/adults), making it an in-teresting benchmark.We observe that incorporating theAVA+CP classes results in a drop in performance for thedistance score. However, with the larger set of HICO andSWIG classes, there is an improvement in performance fordistance, LAH and LAEO. In particular, incorporating theSWIG classes gives the most improvements, with gaze fol-lowing results comparable to the state of the art andcontrasts with our observations on GazeFollow. This sug-gests that incorporating gaze contextual cues can result inmore robust performance with better generalization.Ablation: Early Fusion vs Multi-Stage Fusion. We per-form an ablation with two different fusion mechanisms forincorporating VLM contextual information in MTGS. Thefirst is early fusion, and follows the approach described in.1. The second is a multi-stage fusion approach,where the VLM context is fused with the person tokens atevery block of the architecture (4 times). We observe thatthe early fusion approach slightly outperforms the multi-stage fusion approach, especially for LAH, so we followedthe early fusion approach for all our experiments.Qualitative results.We provide qualitative results forMTGS, with and without the use of contextual cues in Fig-ure 7. We observe that incorporating contextual cues canimprove performance, helping identify the gaze target inchallenging situations with multiple salient people and ob-jects. For instance, in row 1, person 2 has a high score forcarrying, which might indicate that this person is lookingtowards their hand. In row 2, person 3 has a high score for",
  "Person 1 texting on 1.83 reading 1.13 typing on 0.99Person 2 watching 0.91 hunting 0.53 opening 0.48Person 3 talking on 1.36 texting on 1.31 opening 1.17": ". Qualitative results of MTGS trained on GazeFollow and evaluated on ChildPlay. For each person, we display the predictedgaze point as well as social gaze task along with the associated person id. We provide results without contextual cues (left) and withcontextual cues from the HICO classes (right). We also display the top three classes with the highest normalized score for each person.",
  ". Discussion": "Our observations in .3 suggest that incorporatinga larger set of contextual cues can improve generalizationperformance for gaze following. As the set of cues becomeslarger, it can capture more specific situations (ex. unlock-ing, sewing in SWIG) which are usually associated withcertain gaze targets. It is worth noting that increasing thenumber of classes has a negligible impact on computationtime. As mentioned in .1, the ITM approach pro-cesses the text prompts and images independently to obtaintext and image embeddings. The final score is then a dotproduct of the two. Hence, all the text embeddings can becomputed and saved at the start, and then used with any newimage. We also note that the set of HICO and SWIG classes uti-lized in our study are obtained from HOI datasets, hence,scores for the different cues could alternatively be obtainedfrom HOI models. This is another interesting direction ofinvestigation, but its main drawback is that the set of cuesthat can be considered is fixed depending on the chosenmodel.On the other hand, leveraging VLMs in a zero-shot manner allows us to consider any set of cues, includinglarger sets than the ones we considered (with a negligibleimpact on computation time), or more domain specific cuestailored for specific applications.",
  ". Conclusion": "In this work, we explored the zero-shot capabilities ofVLMs for extracting contextual cues related to a personspose or interactions with objects and other people, and eval-uated the impact of incorporating these cues into a gazefollowing model. We learned that VLMs can indeed ex-tract contextual cues, and that considering the entire imagewith a red-circle drawn over the person of interest servesas the best visual prompt, and that ensembling scores fromdifferent textual prompts serves as the best text promptingstrategy. We also observed that BLIP-2 is the overall bestperforming VLM, and that ICL can potentially bring fur-ther benefits. In the second part, we observed that incorpo-rating the extracted cues into a gaze following model canprovide better generalization performance, especially whenconsidering a larger set of classes. In future work, we planto investigate other VLMs and further explore promptingstrategies such as ICL. We also plan to explore the optionof predicting the different cues rather than providing themas input to the model.Acknowledgement. This research was supported by theAI4Autism project (digital phenotyping of autism spec-trum disorders in children, grant agreement number CR-SII5 202235 / 1) of the Sinergia interdisciplinary programof the SNSF. It was also supported by Innosuisse, the Swissinnovation agency, through the NL-CH Eureka Innovationproject ePartner4ALL (a personalized and blended care so-lution with virtual buddy for child health, number 57272.1IP-ICT).",
  "Jacob Andreas.Language models as agent models.InFindings of the Association for Computational Linguistics:EMNLP 2022, pages 57695779, 2022. 2": "Jun Bao, Buyu Liu, and Jun Yu. Escnet: Gaze target detec-tion with the understanding of 3d scenes. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1412614135, 2022. 3, 7 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-guage models are few-shot learners. Advances in neural in-formation processing systems, 33:18771901, 2020. 2 Yu-Wei Chao, Zhan Wang, Yugeng He, Jiaxuan Wang, andJia Deng. Hico: A benchmark for recognizing human-objectinteractions in images. In Proceedings of the IEEE inter-national conference on computer vision, pages 10171025,2015. 6 Eunji Chong, Yongxin Wang, Nataniel Ruiz, and James MRehg. Detecting attended visual targets in video. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 53965406, 2020. 1, 7 Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, SebastianGehrmann, et al.Palm: Scaling language modeling withpathways. Journal of Machine Learning Research, 24(240):1113, 2023. 2 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale.arXiv preprintarXiv:2010.11929, 2020. 6 Yi Fang, Jiapeng Tang, Wang Shen, Wei Shen, Xiao Gu, LiSong, and Guangtao Zhai. Dual attention guided gaze tar-get detection in the wild. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition(CVPR), pages 1139011399, 2021. 3, 7",
  "In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition (CVPR), 2018. 4": "Anshul Gupta, Samy Tafasca, and Jean-Marc Odobez.Amodular multimodal architecture for gaze target prediction:Application to privacy-sensitive settings. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition Workshops (CVPRW), pages 50415050, 2022.1, 3, 6, 7 Anshul Gupta, Samy Tafasca, Arya Farkhondeh, Pierre Vuil-lecard, and Jean-Marc Odobez.A novel framework formulti-person temporal gaze following and social gaze pre-diction. In Arxiv, 2024. 2, 3, 6, 7, 8 Zhengxi Hu, Dingye Yang, Shilei Cheng, Lei Zhou, ShichaoWu, and Jingtai Liu. We know where they are looking atfrom the rgb-d camera: Gaze following in 3d. IEEE Trans-actions on Instrumentation and Measurement, 2022. 3 Zhengxi Hu, Kunxu Zhao, Bohan Zhou, Hang Guo, ShichaoWu, Yuxue Yang, and Jingtai Liu. Gaze target estimationinspired by interactive attention. IEEE Transactions on Cir-cuits and Systems for Video Technology, 32(12):85248536,2022. 7 Tianlei Jin, Zheyuan Lin, Shiqiang Zhu, Wen Wang, andShunda Hu. Multi-person gaze-following with numerical co-ordinate regression. In 2021 16th IEEE International Con-ference on Automatic Face and Gesture Recognition (FG2021), pages 0108. IEEE, 2021. 7 Tianlei Jin, Qizhi Yu, Shiqiang Zhu, Zheyuan Lin, Jie Ren,Yuanhai Zhou, and Wei Song. Depth-aware gaze-followingvia auxiliary networks for robotics. Engineering Applica-tions of Artificial Intelligence, 113:104924, 2022. 1, 3, 7",
  "Roland Johansson, Goran Westling, Anders Backstrom, andRandall Flanagan. Eye-Hand Coordination in Object Manip-ulation. Journal of Neuroscience, 21(17):69176932, 2001.1": "Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and WeidiXie. Prompting visual-language models for efficient videounderstanding.In European Conference on Computer Vi-sion, pages 105124. Springer, 2022. 2 Jing Li, Zejin Chen, Yihao Zhong, Hak-Keung Lam, JunxiaHan, Gaoxiang Ouyang, Xiaoli Li, and Honghai Liu.Appearance-based gaze estimation for asd diagnosis. IEEETransactions on Cybernetics, 52(7):65046517, 2022. 1",
  "Alec Radford, Karthik Narasimhan, Tim Salimans, IlyaSutskever, et al. Improving language understanding by gen-erative pre-training. 2018. 2": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, GretchenKrueger, and Ilya Sutskever.Learning transferable visualmodels from natural language supervision. In Proceedingsof the 38th International Conference on Machine Learning,pages 87488763. PMLR, 2021. 2, 3, 5 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, andPeter J Liu. Exploring the limits of transfer learning witha unified text-to-text transformer. The Journal of MachineLearning Research, 21(1):54855551, 2020. 4 Adria Recasens, Carl Vondrick, Aditya Khosla, and AntonioTorralba. Following gaze in video. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recogni-tion, pages 14351443, 2017. 3, 6 Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,Aditya Khosla, Michael Bernstein, et al.Imagenet largescale visual recognition challenge. International journal ofcomputer vision, 115(3):211252, 2015. 3 Samira Sheikhi and Jean-Marc Odobez. Combining dynamichead posegaze mapping with the robot conversational statefor attention recognition in humanrobot interactions. Pat-tern Recognition Letters, 66:8190, 2015. 1, 3 Aleksandar Shtedritski, Christian Rupprecht, and AndreaVedaldi.What does clip know about a red circle?vi-sual prompt engineering for vlms.In Proceedings of theIEEE/CVF International Conference on Computer Vision(ICCV), pages 1198711997, 2023. 2",
  "Rainer Stiefelhagen, Jie Yang, and Alex Waibel. Modelingfocus of attention for meeting indexing based on multiplecues. IEEE Trans. Neural Networks, 13(4):928938, 2002.3": "Samy Tafasca, Anshul Gupta, and Jean-Marc Odobez. Child-play: A new benchmark for understanding childrens gazebehaviour. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 2093520946, 2023.1, 3, 4, 6, 7 Francesco Tonini, Cigdem Beyan, and Elisa Ricci. Multi-modal across domains gaze target detection. In Proceedingsof the 2022 International Conference on Multimodal Inter-action, pages 420431, 2022. 3, 7 Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Es-lami, Oriol Vinyals, and Felix Hill.Multimodal few-shotlearning with frozen language models. Advances in NeuralInformation Processing Systems, 34:200212, 2021. 2",
  "Mengmeng Wang, Jiazheng Xing, and Yong Liu. Actionclip:A new paradigm for video action recognition. arXiv preprintarXiv:2109.08472, 2021. 2": "Suchen Wang, Kim-Hui Yap, Henghui Ding, Jiyan Wu, Jun-song Yuan, and Yap-Peng Tan. Discovering human interac-tions with large-vocabulary objects via query and multi-scaledetection.In Proceedings of the IEEE/CVF InternationalConference on Computer Vision (ICCV), 2021. 6 Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko,Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, andChristoph Feichtenhofer. Videoclip: Contrastive pre-trainingfor zero-shot video-text understanding.arXiv preprintarXiv:2109.14084, 2021. 2",
  ". Example of visual prompts": "As described in .1, we investigate different vi-sual prompting approaches to focus on a specific individ-ual in the scene. An example of each prompt is providedin .These techniques are implemented on eitherthe whole image or specifically on the cropped image ofthe target person. In total, this leads to eight distinct visualprompting strategies.",
  ". Details of Text Prompts": "ITM. , lists different text prompt variations as de-scribed in .3 for the ITM approach.A finalprompt is a combination of {template},{person}, {photo}and {synonym} such as this individual is grabbing or asnapshot of a human handling.VQA. For the VQA approach, for computational reasons,we consider a single template in the form of a question, andreduce the number of synonyms for the classes. We providethe template and synonyms in .",
  "In , we provide the results for varying the class syn-onym in the text prompt. We observe that performance canchange depending on the used synonym by a large margin": "Contrary to popular belief, Lorem Ipsum is not simplyrandom text. It has roots in a piece of classical Latin litera-ture from 45 BC, making it over 2000 years old. RichardMcClintock, a Latin professor at Hampden-Sydney Col-lege in Virginia, looked up one of the more obscure Latinwords, consectetur, from a Lorem Ipsum passage, and goingthrough the cites of the word in classical literature, discov-ered the undoubtable source. Lorem Ipsum comes from sec-tions 1.10.32 and 1.10.33 of de Finibus Bonorum et Mal-orum (The Extremes of Good and Evil) by Cicero, writtenin 45 BC. This book is a treatise on the theory of ethics, verypopular during the Renaissance. The first line of Lorem Ip-sum, Lorem ipsum dolor sit amet.., comes from a line insection 1.10.32."
}