{
  "(a)(b)(c)(d)": ". Federated adaptation in challenging environments. When facing a domain very different from those observed during training e.g., nighttime images (a) stereo models suffer drops in accuracy (b). By enabling online adaptation (c) the network canimprove its predictions, at the expense of decimating the framerate. In our federated framework, the model can demand the adaptationprocess to the cloud, to enjoy its benefits while maintaining the original processing speed (d).",
  "Abstract": "We introduce a novel approach for adapting deep stereonetworks in a collaborative manner. By building over prin-ciples of federated learning, we develop a distributed frame-work allowing for demanding the optimization process to anumber of clients deployed in different environments. Thismakes it possible, for a deep stereo network running onresourced-constrained devices, to capitalize on the adap-tation process carried out by other instances of the samearchitecture, and thus improve its accuracy in challengingenvironments even when it cannot carry out adaptation onits own. Experimental results show how federated adap-tation performs equivalently to on-device adaptation, andeven better when dealing with challenging environments.",
  ". Introduction": "Depth sensing plays a key role in several applications inthe fields of computer vision, robotics, and more. The useof stereo images for this purpose has been one of themost studied topics for decades, consisting of matching pix-els across two rectified images. This allows for estimatinghorizontal disparity between corresponding pixels and, con-sequently, their depth through triangulation. This processhas been carried out through image processing algorithms until nearly one decade ago, when deep learning startedreplacing hand-crafted solutions with neural networks .The increasing growth of computational power in the handof developers, together with the more and more annotated data becoming available, has rapidly established end-to-enddeep networks as the standard frameworks to deal withthe problem .In order to provide sufficient data for training deep stereonetworks at their best, the use of synthetic datasets hasbecome a standard practice in the field. This, to some ex-tent, also revealed one of the main limitations these mod-els suffered from at first, which was the scarce capabilityto generalize to image domains very different from thoseobserved at training time a matter of concern commonto other tasks involving deep networks, such as semanticsegmentation . First attempts to solve this shortcominginvolved unsupervised adaptation techniques, either to becarried out offline or directly during deployment inreal-time , with some computational overhead.More recently, the community focused on dealing withthe problem at its source i.e., during the training processitself, by designing specific strategies to drive the deep net-work learning domain-invariant features while, eventually, the most modern stereo networks can generalize much better than their predecessors.Despite these advancements, in the presence of very chal-lenging conditions never observed during training, such aslow illumination, sensor noise occurring at night, or the re-flections appearing on rainy roads, we argue generalizationcapability alone might be insufficient. In such cases, on-line adaptation could still play a role, although at the costof dropping the framerate at which the deep network oper-ates. This price to pay might be reduced by means of spe-cific adaptation strategies and allow for maintain-ing real-time processing when high-end GPUs are available,",
  "arXiv:2405.14873v1 [cs.CV] 23 May 2024": "yet might still be prohibitive when this is not the case e.g.,when running on a UAV or a low-powered vehicle not ableto support power-hungry hardware.In a nutshell, marrying good practices to achieve gener-alization with online adaptation is essential for facing thereal world, but still not sufficient when computational re-sources cannot support additional overhead at deploymenttime.In this context, the adaptation process has alwaysbeen approached as a single-instance task, in which a sin-gle stereo network is deployed in an unseen environmentand is gradually optimized over it. This setting ignores theexistence of other instances of the model operating in dif-ferent environments, potentially adapting independently tothe specific domain they face. In a world where camerasand sensors are increasingly widespread, and fleets of au-tonomous vehicles are on the horizon, we argue adaptationitself can be formulated as a distributed task. In this sce-nario, an agent lacking sufficient computational capacitycan demand the adaptation process to a network of peersequipped with more powerful hardware and thus capable ofsustaining the adaptation process.In this paper, we introduce a novel framework imple-menting federated online adaptation for deep stereo net-works, by building on principles of federated learning .Since communication between nodes is strictly necessaryto carry out adaptation in a distributed manner, a connec-tion overhead is introduced to transfer data, proportionalto the quantity of data itself. To minimize this overhead,we design an algorithm specifically tailored to reduce thedata quantity exchanged between agents at the most, whilemaintaining the effectiveness of the overall adaptation pro-cess nearly unaltered. This is done by revising the MADalgorithm to the federated setting and thus developingFedMAD. Our federated framework is extensively evalu-ated on several stereo datasets, such as KITTI , Driving-Stereo , and DSEC , proving that federated adapta-tion can provide an equivalent or, in the most challengingscenarios, even greater accuracy improvement compared tosingle-device adaptation, as spotlighted in . To thebest of our knowledge, our work represents the first attemptto deal with real-time adaptation through a federated ap-proach, in particular in the field of self-adapting stereo net-works . Our main claims can be resumed as: We revise real-time adaptation frameworks to in-troduce recent advances in deep stereo concerning gener-alization and architectural design, realizing a new base-line that largely improves over prior works. We introduce a novel framework casting online adapta-tion as a federated process, allowing to free the singledevice from the computational overhead that is insteaddistributed among a number of peer devices.",
  ". Related Work": "We briefly review the literature relevant to our work.Deep Stereo Matching.The stereo matching litera-ture counts several hand-crafted algorithms throughthe years, usually divided into local and global methods ac-cording to their structure and their speed/accuracy trade-off.In the last decade, deep learning has brought a paradigmshift into stereo matching, achieving more and more accu-rate results on standard benchmarks . While the firststeps in this field aimed at replacing individual modulesof the conventional pipeline with compact networks, with DispNet the end-to-end archi-tectures rapidly conquered the main stage . Most of the models can be broadly classi-fied into 2D and 3D architectures, with some exploiting transformers .In the last years, several works focused on improvingthe capability of stereo networks to generalize across dif-ferent domains, for instance by reducing the gap betweentraining on synthetic and testing on real images. The mainapproaches involved the use of hand-crafted matching func-tions or algorithms , techniques to learn for more ro-bust features , or the generation of photore-alistic data for training . Eventually, the most recentstereo architectures proved to be capableof strong generalization from synthetic to real images evenwithout making use of any of the aforementioned strategies.Self-supervised Stereo. To overcome the need for an-notated data, self-supervised techniques have been devel-oped to directly train stereo networks on unlabelled imagepairs. The minimization of the photometric error be-tween the left and right images, with the latter being warpedaccording to estimated disparity, is at the core of most ap-proaches trained on unconstrained stereo pairs orvideos . An alternative strategy consists of ob-taining pseudo-labels from either hand-crafted algorithms or other depth estimation networks .Real-time Adaptation for Stereo. Although syntheticdatasets provide countless annotated data, the poor gen-eralization capabilities of the stereo models developed atfirst led to the development of adaptation techniques toovercome the synthetic to real domain shift directly dur-ing deployment. As this demands the model to adapt in theabsence of ground truth, photometric losses andpseudo labels have been employed.",
  "Model AdaptationFrozen WeightsWeights Update": ". Overview of our federated adaptation framework. On the one hand, active nodes run online adaptation (blue and yellow) andperiodically send their updated weights to a central server. On the other, a listening client (green) can benefit from the adaptation processcarried out by the active nodes, by receiving aggregated weights updates from the server. Federated Learning. This learning paradigm aims attraining models from distributed data sources.A largebody of literature has emerged in the last five years ,mostly focusing on classification tasks. The pivotal feder-ated learning algorithm is FedAvg : a set of clients firsttrain their local model using private data and then uploadthe weights to a server, where they are averaged to form aglobal model. Several methods tried to regularize the local training phase in FedAvg ,with FedProx and SCAFFOLD restricting the lo-cal update to be consistent globally, and MOON ap-plying a contrastive objective to regularize the optimiza-tion of local models to not deviate significantly from theglobal model. In contrast, personalized federated learning aims at training custom models for eachclient to better fit local data. Finally, shows the im-portance of exploiting pre-training when possible, as we dosince we aim at deploying a distributed adaptation process.",
  ". Background: Online Adaptation for Stereo": "Despite the recent advances in domain generalization , a pre-trained stereo backbone might face dropsin accuracy when deployed in challenging environments.As such, adapting the model online can be a solu-tion for dealing with these occurrences. For any incomingstereo pair bt, the network predicts a disparity map (or mul-tiple, depending on the design) according to current weightswt. Subsequently, it updates them by minimizing a lossfunction, typically the sum of multiple terms i:",
  "ii(wt, bt)(1)": "This step updates the whole set of parameters, thus car-rying out full adaptation of the model (FULL) with non-negligible overhead and consequent drop in framerate.To mitigate this side effect, Tonioni et al. introducedModular Adaptation (MAD) along with a dedicated back- bone (MADNet), made of 5 encoder-decoder blocks pre-dicting disparity maps at different scales. For any adapta-tion step t, a block i is sampled according to a probabilitydistribution, then only the corresponding output is used tocompute the loss and optimize the subset of weights wt[i]:",
  ". Federated Adaptation Framework": "The FULL and MAD algorithms are defined on a single-instance perspective i.e., a single stereo backbone isdeployed and adapted during navigation.However, thisparadigm alone might not be sufficient to overcome chal-lenging domain changes or might be unusable if not sup-ported by powerful enough hardware (e.g., when the stereomodels run on embedded devices, barely granting real-timeprocessing even in the absence of any adaptation process).Purposely, we design a federated framework in whichwe define a set of active nodes A, capable of adapting in-dependently, and other listening clients C which demandthe adaptation process to the former, as sketched in . The two categories are managed by a central server, incharge of receiving updated weights and distributing themto the listening nodes. Algo. 1 defines the operations carriedout by the server and the active clients. The server runs aloop (lines 4-14) during which it waits for updated weightstransmitted by the active clients (lines 5-7). Once it has re-ceived the updates from each active client, the server aggre-gates such updates by computing the average of the weightsas in FedAvg and dispatches the updated model toclients C (lines 8-11). Clients A send their updates periodi-cally after they perform T steps of adaptation (lines 15-19).We dub this framework FedFULL.",
  ": end for19: return wk to server": "This way, C receive updates to their weights and improvetheir accuracy, without actively running any GPU-intensiveextra computation.However, significant data traffic be-tween A, the server, and C is introduced, proportional to thenumber of parameters in the stereo network, the number ofclients, and the updates interval T. Purposely, we propose avariant of the aforementioned federated framework inspiredby MAD , by changing the updating procedure carriedout by nodes A as outlined in Algo. 2. At each adaptationstep, the client keeps track of the blocks it updates (lines4-6) which could be some or all of them. Then, it samplesa single block according to a probability distribution of themost updated blocks (line 8), sends it solely to the server,and decays its number of updates (line 9). On the serverside, averaging is performed only for the subset of blocksreceived. We refer to this variant as FedMAD; we will showhow it can reduce data traffic significantly, with a marginaldrop in the accuracy of clients C.",
  ". Proposed Backbone: MADNet 2": "With our federated framework being defined, we now selectthe stereo backbone to be coupled with it. MADNet would be a natural choice, since already designed to exploitmodular adaptation and thus ready for both FedFULL andFedMAD variants. However, its accuracy, according to , falls far behind the one achieved by modern state-of-the-art architectures and, despite the muchhigher efficiency, even while adapting, it cannot match theirresults.",
  ": end for8: j sample(softmax(H[k]))9: H[k][j] = 0.9 H[k][j]10: return j, wk[j] to server": "Purposely, we revise the MADNet design and develop anew baseline for real-time self-adaptive deep stereo, whichwe dub MADNet 2. We argue that one of the weaknesses inits original architecture lies within the module responsiblefor building the cost volume at multiple scales. Specifically,it computes correlation scores between features along theepipolar line according to a radius r, defined as a hyper-parameter (the larger the radius, the higher the chance to hitthe corresponding pixels) and collects them into coarse-to-fine volumes, processed by decoders to estimate disparitymaps at different scales. For the sake of efficiency, smallvalues of r are used such as 2 as in the original MADNet thus constraining the search range and, potentially, reducingaccuracy for disparities falling out of it, despite the use offeatures warping at each scale.We replace this module with the all-pairs correlation vol-ume proposed by RAFT-Stereo , thus extending thesearch range to the entire epipolar line at any scale. Then, apyramid of correlation scores is sampled and forwarded tothe decoders: this ensures obtaining a fixed amount of chan-nels as input to the decoder, independently of the imageresolution. Differently from RAFT-Stereo, which builds asingle volume at quarter resolution and iterates an arbitraryamount of times to estimate disparity, we build multiple vol-umes at lower scales (from164 up to 1 4 as in the originalMADNet) and estimate a fixed number of disparity maps.This, together with the very compact design of the entirearchitecture, trades the high accuracy achieved by RAFT-Stereo with a significantly lower running time (about 60lower). Finally, in our revised design we remove the contextnetwork to further prioritize efficiency.4. Experimental Results",
  ". Online adaptation within a single domain. Results on the City, Residential, Campus, and Road sequences from KITTI": "performance, using the augmentation strategy suggested in. Federated runs are carried out on a server featuring4 3090 GPUs and AMD EPYC 7452 32-Core CPU. Eachclient runs independently on a single GPU, on a dedicatedthread started through the Python threading module to en-able concurrency. Unless otherwise specified, the listeningclient is supported by three clients running full adaptation,with update rate T = 10. To reduce the randomness due toallocation and run of any thread, the listening client startsonly after other clients have started and transmitted theirfirst update to the server. Then, they loop through their se-quence until the listening client has fully processed its own.Regarding adaptation, we use FULL and MAD strategiesfrom , whereas, for the former, we compute losses forany predicted disparity rather than for the latest only .Evaluation Protocol. We follow to evaluate anymodel: we process the stereo pairs in a sequential order,mimicking an online acquisition scenario. We measure theD1-all error rate as the percentage of pixels having abso-lute disparity error larger than 3 and relative error largerthan 5%, as well as the End-Point-Error (EPE). In the caseof an adaptation, the error is computed before weights areupdated. When performing federated adaptation, the activeclients run on sequences from different domains to avoidany data leak and favor the passive client.We also re-port model speed by measuring the CUDA total executiontime with PyTorch profiling tools i.e., not considering in-put/output overheads both when running on nVidia RTX3090 (350W consumption) or on a Jetson AGX Xavier em-bedded board (set in MAXN mode and consuming 30W),averaged over 100 runs after a bootstrap of 100 inferences.In most tables, we highlight the best and second bestresults among macro-categories.",
  ". Datasets": "FlyingThings3D. A collection of synthetic images, com-prising approximately 22k training stereo pairs with denseground truth labels, part of the SceneFlow synthetic dataset. Following , this dataset has been used to pre-train our model and other real-time networks.KITTI .A large dataset featuring 61 stereo se-quences, for a total of about 43k pairs with 375 1242average resolution. Following , we test on Road,Residential, Campus and City domains obtained by con-catenating all the sequences according to their classificationon the official website, using filtered LiDAR measurements converted to disparities as ground-truths.DrivingStereo . This dataset collects about 170kstereo images grouped in 38 sequences with an average res-olution of 400 880 pixels. As defined in , we se-lect the same Rainy, Dusky, and Cloudy sequences for eval-uation. For federated experiments, we sample additionalsequences according to their classification in , respec-tively tagged as Foggy (2018-10-17-14-35, 2018-10-22-10-44 and 2018-10-25-07-37, since no other rain sequences arepresent on the dataset), Dusky (2018-10-16-07-40, 2018-10-16-11-13 and 2018-10-16-11-43) and Cloudy (2018-10-17-14-35, 2018-10-17-15-38 and 2018-10-18-10-39).DSEC .A dataset collected by means of stereoRGB and event cameras, providing 53 sequences for a to-tal of about 50k stereo pairs at 1080 1440 resolution, forhalf of which ground-truth disparity is provided. From thisdataset, we select four sequences to test online adaptationon nighttime images: zurich city 03 a, zurich city 09 a,zurich city 10 a and zurich city 10 b, respectively taggedas Night#1, Night#2, Night#3 and Night#4.In fed-erated experimets, we use sequences zurich city 09 b,zurich city 09 c, zurich city 09 d and zurich city 09 e foradapting active clients.",
  ". Evaluation on KITTI": "Single-agent Adaptation.Tab.1 collects the resultsachieved by several pre-trained stereo models on the sin-gle domains of KITTI. On top (a), we report state-of-the-artmodels characterized by outstanding gener-alization performance on this dataset, yet far from runningin real-time or even far from achieving 1 FPS on AGX followed by MADNet and MADNet 2. The latter, although",
  ". Ablation study impact of the update frequency (top) and number of agents (bottom) on accuracy. We report D1-all (%)on the KITTI dataset for FedFULL (blue) and FedMAD (green)": "generalizing largely better than the former, cannot reach theprevious models yet, despite being unquestionably more ef-ficient. Then, we report in (b) and (c) the results achievedby enabling adaptation using photometric loss or proxylabels , either with FULL or MAD strategies . Inthe latter case, we can observe slightly lower processingtime, probably caused by the different effort required tocompute the loss on sparse labels rather than reprojectingimages and measuring photometric dissimilarity densely.Notably, MADNet falls short of achieving the accuracy ofstate-of-the-art models trained on syntheticdata solely, even when adapting. Conversely, MADNet 2largely benefits from its improved generalization. By en-abling adaptation, it bridges the gap with state-of-the-artnetworks, even outperforming them when proxy labels areavailable , and still running in real-time on high-endhardware while on lower-powered platforms it reachesnearly 15 FPS in its most efficient setup, i.e. MAD++, ifdedicated hardware is available to get proxy labels . Federated Adaptation.We now measure the boostin accuracy MADNet 2 gains when exploiting distributedadaptation. Tab. 2 reports the outcome of this experiment:for a client running on a domain, three remote clients adapton 5 random sequences sampled from the other domains ac-cording to FULL (a) or FULL++ (b) algorithms. With refer-ence to Tab. 1, we can notice how FedFULL/FedFULL++consistently outperforms MAD/MAD++ (except on Cam-pus), while not adding any computational overhead, thanksto the efforts by the distributed clients, yet at the cost of in-troducing some data traffic between nodes. This latter canbe reduced by FedMAD or some alternative strategies,consisting of averaging only the weights of the decoders . Ablation study impact of the update frequency (top)and number of clients (bottom) on traffic.We report MB/s(top) and MB/updates (bottom) exchanged on the KITTI datasetfor FedFULL (blue) and FedMAD (green). (FedDEC), the last decoder (FedLAST), or the encoders (FedENC) while dampening the effect of adaptation.Only the former two nearly preserve the accuracy yieldedby FedFULL, with FedMAD reducing the data traffic muchmore than FedDEC while also retaining the highest accu-racy when the adapting clients mounting dedicated hard-ware to compute proxy labels. In this latter case, the listen-ing client benefits from the boost given by labels yet withouthaving any hardware dedicated to their computation.Ablation Studies. The effectiveness of federated adap-tation scales mainly with two hyper-parameters: i) the fre-quency at which each client pushes its updated model to theserver, and ii) the number of remote clients actively con-tributing to adaptation. Both dictate the speed at which apassive agent will benefit from adaptation, as well as thevolume of data being transferred to the cloud. examines the impact of both factors on accuracywith FedFULL and FedMAD. On top, we can observe howsending updates to the server once every 100 adaptation",
  ". Online adaptation on DrivingStereo . Results on the Rainy, Dusky and Cloudy sequences as selected in": "steps yields noticeable improvements in most cases already,saturating when increasing it to one every 10. At the bot-tom, we show how increasing the number of active clientsconsistently improves the results for the listening node. reports the amount of data transmitted from adapt-ing clients to the server (left), as well as from the server tothe listening client (right) as functions of the update fre-quency (top) and the number of clients (bottom). We high-light how FedMAD enables moderate growth in data traf-fic when the frequency is increased compared to FedFULL,with significant savings on the updates sent to the server.The gap with FedFULL becomes larger when more clientscontribute to the process. In contrast, the data transferredto the listening client remains constant with FedFULL, andthe saving by FedMAD nullifies beyond 6 clients.Federated Adaptation Other Networks.Onlineadaptation can be performed by any stereo network and, assuch, federated adaptation can as well. Purposely, we im-plement FULL and FedFULL with other real-time stereonetworks CoEX , HITNet , and TemporalStereo and evaluate their performance on KITTI. Tab. 3 col-lects the outcome of this experiment. We can notice howthe three models can effectively adapt on the single do-mains, at the cost of dropping their efficiency.By de-manding the adaptation process to distributed clients, all ofthem can benefit from an equivalent boost in performanceswhile avoiding efficiency drops with TemporalStereo andHITNet improving even more with FedFULL compared toFULL on Campus. Although CoEX and HITNet are slightly more accurate than MADNet 2 with reference to Tabs. 1and 2, it is worth observing how both of these models re-quire more than one second to generate a disparity map onAGX, and barely reach 5 FPS when adaptation is not en-abled1. As such, we feel MADNet 2 is a more flexible so-lution for deploying real-time adaptive stereo systems, run-ning at 20 FPS on AGX while federally adapting.Additional Results. For the sake of space, we refer thereader to the supplementary material for more results.",
  ". Evaluation on DrivingStereo": "Following , we evaluate our framework on the Driv-ingStereo dataset, characterized by more challenging en-vironmental conditions harming the generalization capa-bility of deep stereo models. Tab. 4 collects the resultsachieved by state-of-the-art models , real-time networks , MADNet and our MAD-Net 2 trained on synthetic data (a). We can notice how, ingeneral, the error metrics are higher compared to those ob-served on KITTI, confirming the more challenging nature ofthis dataset. Again, MADNet 2 proves to generalize muchbetter than MADNet, yet falls far behind the top-performingstereo networks, close to other fast models.Adapting with photometric losses (b) within single do-mains only marginally improves the results on this bench-mark especially on Dusky: we ascribe this to the morechallenging conditions depicted in these sequences, which",
  ". Online adaptation on DSEC . Results on the Night#1, Night#2, Night#3 and Night#4 sequences": "potentially compromise the effectiveness of the photomet-ric loss. In these conditions, the possibility of relying on theadaptation carried out by other clients results crucial also interms of accuracy, allowing both FedFULL and FedMADto achieve better results on Dusky and Cloudy compared tostandard FULL/MAD executed over the two domains.When proxy labels are available (c), both FULL++ andMAD++ produce notably better results, yet leveraging theadaptation carried out remotely with FedFULL++ and Fed-MAD++ allows to improve the results even further on Rainyand Dusky on the former in particular, it gains about 1.5%in D1-all while resulting comparable on Cloudy.In summary, a client demanding adaptation to the cloudcan benefit even more than carrying it out independentlyin challenging environments, while avoiding runtime over-heads. Accordingly, MADNet 2 can still run in real-time onAGX and surpass other fast models , running noteven at 10 FPS there. The supplementary material reportsfederated experiments with other real-time models.",
  ". Evaluation on DSEC": "We conclude by running further experiments on nighttimestereo sequences taken from the DSEC dataset . Tab.5 collects the results yielded by any stereo model consid-ered so far on four selected night sequences. In contrastto KITTI and DrivingStereo, in (a) we can notice how thestate-of-the-art models achieve a much higher error rate,with real-time architectures proving to be more robust inthis context. Moreover, the higher resolution of this datasetmakes the runtime of each method increase notably, withMADNet 2 being the only model still capable of retainingalmost 10 FPS on AGX when not adapting. By activelyadapting with FULL or MAD (b,c), MADNet 2 can fur-ther improve its accuracy and outperform the other stereomodels, while dropping below 5 FPS. In such a setting, wecan further appreciate how FedFULL becomes crucial formaintaining reasonable runtime while enjoying the benefitsof adaptation, outperforming MAD either when using pho- tometric loss (b) or proxy labels (c), and even being moreeffective than FULL in the former case. Given the lowerinference speed caused by the dataset resolution, we cannotice lower data traffic. This occurs as the adapting mod-els require more time to perform the T steps set to updatethe server. Yet, FedMAD still allows for further reducingthe communication overhead with little drops in accuracy.For the sake of completeness, in the supplementary ma-terial we report the results by other real-time models.",
  ". Conclusion": "In this paper, we presented for the first time a frameworkthat implements federated online adaptation for deep stereomodels.By demanding the optimization process to dis-tributed nodes, a single model can benefit from adaptationeven when deployed on low-powered hardware, thus im-proving its accuracy while maintaining its original process-ing speed. This achievement comes at the cost of introduc-ing data traffic between nodes; however, this traffic can bereduced by means of an appropriate strategy that updatesonly some portions of the entire model at each communica-tion round, specifically tailored for our MADNet 2. Exhaus-tive experiments showcase the effectiveness of our frame-work and its ability to be combined with different models.Limitations. At now, passive clients benefit from theadaptation carried out by some active clients, without thelatter receiving reciprocal benefits in return, and the adapt-ing nodes process images with similar properties (resolu-tion, depth range, application context) to those observedby the listening client. Finally, as clients run on the sameserver, connection delays are ignored in our experiments.Future Work. We foresee federated adaptation will beapplied to other visual tasks for which online adaptation isa reality, such as single image depth estimation , opticalflow or semantic segmentation .Acknowledgment. We thank LAR Laboratory (Univer-sity of Bologna) for providing the Jetson AGX Xavier. Filippo Aleotti, Fabio Tosi, Li Zhang, Matteo Poggi, and Ste-fano Mattoccia. Reversing the cycle: self-supervised deepstereo through enhanced monocular distillation. In 16th Eu-ropean Conference on Computer Vision (ECCV). Springer,2020. Filippo Aleotti, Fabio Tosi, Pierluigi Zama Ramirez, MatteoPoggi, Samuele Salti, Stefano Mattoccia, and Luigi Di Ste-fano.Neural disparity refinement for arbitrary resolutionstereo.In International Conference on 3D Vision, 2021.3DV. Antyanta Bangunharcana, Jae Won Cho, Seokju Lee, In SoKweon, Kyung-Soo Kim, and Soohyun Kim. Correlate-and-excite: Real-time stereo matching via guided cost volumeexcitation. In IEEE/RSJ International Conference on Intelli-gent Robots and Systems (IROS), 2021. Marc Botet Colomer, Pier Luigi Dovesi, Theodoros Pana-giotakopoulos, Joao Frederico Carvalho, Linus Harenstam-Nielsen, Hossein Azizpour, Hedvig Kjellstrom, Daniel Cre-mers, and Matteo Poggi. To adapt or not to adapt? real-timeadaptation for semantic segmentation. In IEEE InternationalConference on Computer Vision, 2023. ICCV. Changjiang Cai, Matteo Poggi, Stefano Mattoccia, andPhilippos Mordohai.Matching-space stereo networks forcross-domain generalization. In 2020 International Confer-ence on 3D Vision (3DV), pages 364373, 2020.",
  "Hong-You Chen and Wei-Lun Chao. On bridging genericand personalized federated learning for image classification.In ICLR, 2022": "Hong-You Chen, Cheng-Hao Tu, Ziwei Li, Han Wei Shen,and Wei-Lun Chao. On the importance and applicability ofpre-training for federated learning. In The Eleventh Interna-tional Conference on Learning Representations, 2022. Xuelian Cheng, Yiran Zhong, Mehrtash Harandi, YuchaoDai, Xiaojun Chang, Hongdong Li, Tom Drummond, andZongyuan Ge.Hierarchical neural architecture search fordeep stereo matching. Advances in Neural Information Pro-cessing Systems, 33, 2020. Cheng Chi, Qingjie Wang, Tianyu Hao, Peng Guo, and XinYang. Feature-level collaboration: Joint unsupervised learn-ing of optical flow, stereo depth and camera motion. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 24632473, 2021. WeiQin Chuah, Ruwan Tennakoon, Reza Hoseinnezhad,Alireza Bab-Hadiashar, and David Suter.ITSA: Aninformation-theoretic approach to automatic shortcut avoid-ance and domain generalization in stereo matching networks.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1302213032, 2022.",
  "Clement Godard, Oisin Mac Aodha, and Gabriel J. Bros-tow.Unsupervised monocular depth estimation with left-right consistency. In CVPR, 2017": "Weiyu Guo, Zhaoshuo Li, Yongkui Yang, Zheng Wang, Rus-sell H Taylor, Mathias Unberath, Alan Yuille, and YingweiLi. Context-enhanced stereo transformer. In Proceedingsof the European Conference on Computer Vision (ECCV),2022. Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell.Cycada: Cycle-consistent adversarial domain adaptation. InInternational conference on machine learning, pages 19891998. Pmlr, 2018.",
  "Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,Sashank Reddi, Sebastian Stich, and Ananda TheerthaSuresh. Scaffold: Stochastic controlled averaging for fed-erated learning. In ICML, 2020": "Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, PeterHenry, Ryan Kennedy, Abraham Bachrach, and Adam Bry.End-to-end learning of geometry and context for deep stereoregression. In The IEEE International Conference on Com-puter Vision (ICCV), 2017. Hsueh-Ying Lai, Yi-Hsuan Tsai, and Wei-Chen Chiu. Bridg-ing stereo matching and optical flow via spatiotemporal cor-respondence. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 18901899, 2019. Jiankun Li, Peisen Wang, Pengfei Xiong, Tao Cai, ZiweiYan, Lei Yang, Jiangyu Liu, Haoqiang Fan, and ShuaichengLiu. Practical stereo matching via cascaded recurrent net-work with adaptive correlation.In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1626316272, 2022.",
  "Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi,Ameet Talwalkar, and Virginia Smith. Federated optimiza-tion in heterogeneous networks. In MLSys, 2020": "Zhaoshuo Li, Xingtong Liu, Nathan Drenkow, Andy Ding,Francis X Creighton, Russell H Taylor, and Mathias Un-berath. Revisiting stereo depth estimation from a sequence-to-sequence perspective with transformers. In Proceedingsof the IEEE/CVF International Conference on Computer Vi-sion, pages 61976206, 2021. Zhengfa Liang, Yiliu Feng, Yulan Guo, Hengzhu Liu, WeiChen, Linbo Qiao, Li Zhou, and Jianfeng Zhang. Learningfor disparity estimation through feature constancy. In Pro-ceedings of the IEEE Conference on Computer Vision andPattern Recognition (CVPR), 2018.",
  "Lahav Lipson, Zachary Teed, and Jia Deng. RAFT-Stereo:Multilevel recurrent field transforms for stereo matching. InInternational Conference on 3D Vision (3DV), 2021": "Biyang Liu, Huimin Yu, and Guodong Qi. Graftnet: Towardsdomain generalized stereo matching with a broad-spectrumand task-oriented feature. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 1301213021, 2022. Wenjie Luo, Alexander G Schwing, and Raquel Urtasun. Ef-ficient deep learning for stereo matching. In Proceedings ofthe IEEE conference on computer vision and pattern recog-nition, pages 56955703, 2016.",
  "Xiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu.Layer-wised model aggregation for personalized federatedlearning. In CVPR, 2022": "David Marr and Tomaso Poggio. Cooperative computationof stereo disparity: A cooperative algorithm is derived for ex-tracting disparity information from stereo image pairs. Sci-ence, 194(4262):283287, 1976. Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. Alarge dataset to train convolutional networks for disparity,optical flow, and scene flow estimation. In The IEEE Confer-ence on Computer Vision and Pattern Recognition (CVPR),2016.",
  "Daniel Scharstein and Richard Szeliski. A taxonomy andevaluation of dense two-frame stereo correspondence algo-rithms. IJCV, 47(1-3):742, 2002": "Daniel Scharstein, Heiko Hirschmuller, York Kitajima,Greg Krathwohl, Nera Nesic, Xi Wang, and Porter West-ling. High-resolution stereo datasets with subpixel-accurateground truth. In German conference on pattern recognition,pages 3142. Springer, 2014. Thomas Schops, Johannes L Schonberger, Silvano Galliani,Torsten Sattler, Konrad Schindler, Marc Pollefeys, and An-dreas Geiger.A multi-view stereo benchmark with high-resolution images and multi-camera videos. In IEEE Con-ference on Computer Vision and Pattern Recognition, pages32603269. IEEE, 2017.",
  "Akihito Seki and Marc Pollefeys. SGM-Nets: Semi-globalmatching with neural networks. In CVPR, pages 231240,2017": "Zhelun Shen, Yuchao Dai, and Zhibo Rao. Cfnet: Cascadeand fused cost volume for robust stereo matching. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1390613915, 2021. Zhelun Shen, Xibin Song, Yuchao Dai, Dingfu Zhou, ZhiboRao, and Liangjun Zhang. Digging into uncertainty-basedpseudo-label for robust stereo matching.IEEE Transac-tions on Pattern Analysis & Machine Intelligence, (01):118,2023.",
  "Alessio Tonioni, Matteo Poggi, Stefano Mattoccia, and LuigiDi Stefano. Unsupervised adaptation for deep stereo. In TheIEEE International Conference on Computer Vision (ICCV).IEEE, 2017": "Alessio Tonioni, Oscar Rahnama, Tom Joy, Luigi Di Stefano,Ajanthan Thalaiyasingam, and Philip Torr. Learning to adaptfor stereo. In The IEEE Conference on Computer Vision andPattern Recognition (CVPR). IEEE, 2019. Alessio Tonioni, Fabio Tosi, Matteo Poggi, Stefano Mat-toccia, and Luigi Di Stefano. Real-time self-adaptive deepstereo. In The IEEE Conference on Computer Vision andPattern Recognition (CVPR). IEEE, 2019. Alessio Tonioni, Matteo Poggi, Stefano Mattoccia, and LuigiDi Stefano. Unsupervised domain adaptation for depth pre-diction from images. IEEE Transactions on Pattern Analysisand Machine Intelligence, 2020.",
  "Jamie Watson, Oisin Mac Aodha, Daniyar Turmukhambetov,Gabriel J. Brostow, and Michael Firman. Learning stereofrom single images. In European Conference on ComputerVision (ECCV), 2020": "Gangwei Xu, Xianqi Wang, Xiaohuan Ding, and Xin Yang.Iterative geometry encoding volume for stereo matching. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 2191921928, 2023. Haofei Xu and Juyong Zhang. Aanet: Adaptive aggrega-tion network for efficient stereo matching. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 19591968, 2020. Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi,Fisher Yu, Dacheng Tao, and Andreas Geiger. Unifying flow,stereo and depth estimation. IEEE Transactions on PatternAnalysis and Machine Intelligence, 2023. Gengshan Yang, Joshua Manela, Michael Happold, andDeva Ramanan. Hierarchical deep stereo matching on high-resolution images. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages55155524, 2019. Guorun Yang, Xiao Song, Chaoqin Huang, Zhidong Deng,Jianping Shi, and Bolei Zhou. Drivingstereo: A large-scaledataset for stereo matching in autonomous driving scenarios.In IEEE Conference on Computer Vision and Pattern Recog-nition (CVPR), 2019. Feihu Zhang,Victor Prisacariu,Ruigang Yang,andPhilip HS Torr. GA-Net: Guided aggregation net for end-to-end stereo matching. In IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), 2019.",
  ". MADNet 2 Architecture": "MADNet 2 is implemented on top of MADNet . Specically, it is made of two, shared feature extractors and a set of veshallow disparity decoders. These modules are assembled to implement coarse-to-ne processing, as shown in . . MADNet 2 architecture. Given a stereo pair, a set of multi-scale features is extracted by means of two feature extractors withshared weights. Starting from the lowest resolution i.e.,164 correlation scores are computed and sampled by means of the all-paircorrelation module and lookup operator from . Sampled scores and image features are processed by a disparity decoder, which predictsan initial disparity map at164 resolution. This latter is upsampled and used by the look operator working on the correlation volume at132resolution, then a second decoder predicts a rened disparity map at132 resolution. This process is repeated up to 1",
  ", 1": "32, and164 of the input resolution and are used to compute coarse-to-ne cost volumes.All-pairs Correlation Volume . Features obtained from the two extractors are used to build a pyramid of costvolumes, by computing all-pairs correlation scores . Given feature maps f, g RHW F , a 3D correlation volume canbe computed by computing the inner product between features on the same horizontal line:",
  "hfijh gikh,C RHW W(3)": "Conversely to the correlation layer used originally in , this operation is not bound to a specic search range, thus allowingthe network to compute matching scores for all possible candidates along the epipolar line. As the correlation volumeproduces C RHW W , this makes the cost volume channels dimension dependent on the resolution of the input image.However, by exploiting the lookup operator from RAFT-Stereo we can sample a xed number of correlation scores alongthe channel dimension and build a xed-size cost volume to be processed, subsequently, by a disparity decoder. Sampling isperformed in a 1D neighborhood with a radius 2, selecting H W 5 correlation features.Disparity Decoders. These modules process correlation scores sampled by the lookup operator, the features produced bythe feature extractor from the left image, and the disparity map estimated at the previous stage, in order to predict a reneddisparity map at the current resolution. Each decoder is made of ve 33 convolutional layers, with stride 1 and output channels. Any layer is followed by Leaky ReLU activations with = 0.2, except the last one. Following, predicted disparity maps are scaled by a factor120.Training. Following , we train MADNet2 with a weighted sum of L1 losses computed on each disparity map.Specically, the pixel-wise L1 between predicted and downsampled ground truth disparity is summed over the entire image.The four terms are summed with weights [0.08, 0.02, 0.01, 0.005] from lower to higher resolution. We use Adam optimizer,batch size 8, and an initial learning rate of 1e 4, halved after 150 epochs. We use color and spatial augmentations from .Adaptation. We use Adam optimizer and learning rate 1e 5 when adapting MADNet 2 and any other model.",
  ". Pre-trained Models and Proxy Labels": "We now report which specic weights have been used for any stereo network involved in our experiments.RAFT-Stereo , CREStereo We use pre-trained weights provided by , as i) they showed slightly bettergeneralization for RAFT-Stereo , and ii) ofcial weights pre-trained on SceneFlow are not available for CREStereo .IGEV-Stereo , UniMatch We use the pre-trained weights provided by the authors respectively,sceneflow.pth and gmstereo-scale2-regrefine3-resumeflowthings-sceneflow-f724fee6.pth.Real-time models (TemporalStereo , HITNet , CoEX ) As the weights available online proved poor gen-eralization from synthetic to real images, we re-trained these three models from scratch on FlyingThings3D , using theoriginal losses presented in the respective papers, following the same training protocol and using the same hyper-parameters.Proxy Labels. Disparity maps used for FULL++/MAD++ are obtained following , i.e., by running rSGM2 and thenpost-processing the results with left-right consistency check and a speckle lter.",
  ". MADNet vs MADNet 2": "We start by discussing the results achieved by our improved version of the original MADNet . Tab. 6 shows theerror rates achieved by different avors of MADNet without adaptation. Our PyTorch re-implementation already improvesover the original source code, with stronger data augmentation further improving generalization from synthetic to realimages. Eventually, the all-pairs correlation volume allows to decimate the errors with respect to the original model.",
  ". Federated Adaptation Listening Client on Low-Powered Hardware": "All of the federated experiments carried out in the main paper are performed by running both active and listening clients on3090 GPUs for simplicity. Accordingly, the listening client runs at a much faster inference speed with respect to the adaptingclients, and thus the relative frequency of the updates received from the server will be much lower.This translates into a lower improvement achieved by the listening client with respect to what would happen in a real use-case, i.e., when it runs on a low-powered platform and is not capable of adapting on its own. In such a case, its processingspeed would also be much lower, with a consequent increase of the relative frequency of updates it receives.To conrm this hypothesis, we run an additional experiment on KITTI, by constraining the listening client to run atlower speed (about 10 FPS). Tab. 7 recalls, on top (a), the results obtained in the main paper with our federated adaptationframework (Tab. 2). At the bottom (b), we report the results achieved in this latter experiment. We can notice how the errorrates achieved on most sequences are lower when the listening client runs at a lower speed, conrming our hypothesis.",
  ". Impact of Randomness on Federated Adaptation": "The asynchronous nature of our federated framework makes it susceptible to different, random factors, some of them evenout of our control. Indeed, while we can constrain some of these by xing the random seed in our experiments e.g., thesequences sampled by the active clients, the blocks sampled by MAD and FedMAD heuristics, etc. we cannot enforce thevery same concurrent behavior for the threads running independently in our experiments.We measure the impact of these factors in Tabs. 8 to 10, respectively on KITTI, DrivingStereo, and DSEC datasets. Eachtable reports two distinct experiments, consisting in (a) running our federated framework ve times with different randomseeds, and (b) running it ve times by xing the very same seed. We report, for each sequence, the margin in terms of D1-alland EPE between the maximum and the minimum measured over the ve runs, with referring to margins lower than 0.01.Starting from Tab. 8, we can notice how the uctuations on D1-all are lower than 0.1 in most sequences, even whenchanging the seed (a). The only exception is the Campus sequence which is, unsurprisingly, the shortest and hardest. Whenxing the random seed over multiple runs (b), we can still observe some lower uctuations, conrming the inuence of somefactors over which we have no control such as threads scheduling, initialization, and concurrence to access resources.The same trend can be observed on Tabs. 9 and 10. In particular, as the sequences from DrivingStereo and DSEC areshorter and more challenging, the impact of randomness is slightly higher with respect to what observed on KITTI.",
  ". Federated Adaptation with Other Real-Time Networks": "In the main paper, we showcased in Tab. 3 how federated adaptation and online adaptation, in general can be implementedwith other, real-time networks such as TemporalStereo , HITNet and CoEX , reporting results with photometricloss only due to the lack of space. For completeness, we complement those results here. Tab. 11 completes Tab. 3 withthe results achieved by using proxy labels on the KITTI dataset, conrming what already discussed in the main paper.",
  "FedFULL++1.220.881.010.792.320.951.110.8333.59.5427": "(b) Single-agent vs Federated Adaptation proxy labels . Online adaptation by fast networks (TemporalStereo , HITNet , CoEX ) within a single domain single agentvs federated adaptation. Results on the City, Residential, Campus, and Road sequences from KITTI .Furthermore, Tabs. 12 and 13 complete this evaluation by extending it to DrivingStereo and DSEC datasets. Ingeneral, adapting any network through FULL/FULL++ often allows for improving their accuracy and achieving error rateseven lower compared to MADNet 2. Nonetheless, none of the three models can stand with this latter in efciency.",
  "FedFULL++5.381.387.051.426.051.305.961.2246.115.11187": "(d) Single-agent vs Federated Adaptation proxy labels . Online adaptation by fast networks (TemporalStereo , HITNet , CoEX ) on DSEC single agent vsfederated adaptation. Results on the Night#1, Night#2, Night#3 and Night#4 sequences.This becomes particularly evident on the AGX board: when adapting on DrivingStereo with FULL/FULL++ (Tab. 12),CoEX and HITNet cannot reach 1 FPS, while MADNet 2 can still run at 2 FPS, or even faster with MAD/MAD++. Con-sequently, leveraging federated adaptation is the only way for CoEX and HITNet to keep a decent frame rate, respectivelyabout 9 and 3 FPS. Yet, MADNet 2 maintains its supremacy by running at more than 20 FPS in the same setting.On DSEC (Tab. 13), this gap becomes even larger, with CoEX not even running at 2 FPS and HITNet running out-of-memory when trying to carry out adaptation, whereas MADNet still reaches nearly 10 FPS.",
  ". Qualitative Results": "We conclude with some qualitative examples of disparity maps predicted by the several models involved in our experiments.Figs. 6 and 7 reports two examples respectively from the Road and Residential sequences of the KITTI dataset . Oneach, we report different disparity maps and the corresponding error maps (these latter are dilated to ease visualization). Wecan appreciate how state-of-the-art models already predict very accurate results, yet with the high runtimehighlighted in Tab. 1. Real-time models start from slightly higher error rates when not performing adaptation. Nonetheless,they can reach (and even surpass) the accuracy of state-of-the-art models either by actively adapting over the sequence itself(FULL++) or by leveraging federated optimization performed by other clients running on different sequences (FedFULL++).Figs. 8 and 9 shows examples from Cloudy and Rainy sequences in DrivingStereo . There, state-of-the-art models achieve slightly lower accuracy, with real-time networks easily outperforming them through adaptation.",
  ". Qualitative results DrivingStereo dataset , Rainy sequence": "Finally, Figs. 10 and 11 reports qualitative examples from Night#2 and Night#4 sequences in DSEC . On this dataset,state-of-the-art models struggle severely in particular on Night#2 sequence because of the sensibly highernoise in the images due to the poor illumination. Again, online adaptation allows real-time models to improve their accuracyon-the-y and to recover details such as trafc signals that were lost by the original model not performing any adaptation.It is also worthing how the quality of proxy labels used by FULL++ and FedFULL++ is inevitably lower on these scenes,yielding some artifacts to appear in the predictions by the adapted models e.g. at the bottom left."
}