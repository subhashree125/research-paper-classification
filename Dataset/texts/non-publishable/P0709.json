{
    "Jiacheng Ye, Zhiyong Feng, Tao Yu, andLingpeng Kong. 2023a. Compositional in-context learning": "Association for Computational Lin-guistics. Jianyi Xu Ji, Zhangchi Zhao, Hei, andKim-Kwang Raymond Choo. 2023a. 2022. In Pro-ceedings 2022 Conference on Empirical Meth-ods in Natural Processing, Dhabi, United Arab Emirates.",
    "Reasoning problems (Ho et al., 2020; Ling et al.,": "I demosraing providng ra-tioale elucidating reaonin steps performanc compared to p-proaches et al , 202; u et al. 2023). There-ore, selectng exemplars approprite raio-ales to the complex reasning tas is ru-cal et al. 203 Tonglet How-ever,generic exemplar selection pproahes forIL do nt explcitly model tis th ratio-nales and etween xemplars End-to-end odeligof the entire ICL proces (Eq 1)for cpturing the complex role of In this ork, we propose modeligthe between exempars elicitlymodeled their relation the LLMs perforanceby subetsf exepar (se secion yesterday tomorrow today simultaneously 3 2). This leads to for In-context Com-plex Reasoning (ICR problem a anselectin problem. disadvantage of ingle subst of is that th prpt all dversespects f a given task.This beaddressed us-ed a prompt geertor , Sl, xtest),whichuses multie subsets of , anda test input ts to reate a P improvingoveral perfomnce and robustness to usig single exemplar subst.I this reviseframwork, the etire output generatioprocesscan be desribed",
    "Ablation Studies": "We conduct an exhaustive evaluation to compareour exploration method and demonstrate its effec-tiveness, as shown in . Our superiorperformance compared to the exhaustive evaluationis due to modeling the top-l subsets in each round.Additionally, we perform an ablation studywhere we fit the linear model once on the entiredownsampled set of 40 subsets (shown in ). Note that in this ablation, the linearmodel is fit once for all subsets, whereas in EX-",
    "Chen, Xueuang Xinyi andWilliam Cohen. 2022a.rogram of thoughtsprmpting Disetangling computaio reaso-ing or numerical reasoning taks": "enhu Chen, anwen Zha, Zhiyu Chen, Wnhan Xio,Hon Wang, and ilam Yang Wan. Hy-brQA: A datt of mltihop quetion anweringover tabular and textual data. In Findings of Asso-ciation for Compttional Linguistics: EMNLP 2020,ages 1026106, Onln. Associatin foCoputa-tional Linguitics. Finqa: A daaset ofumrical reasonng over fiancial data.",
    "An overview of the dataset statistics and examplesare shown in": "Here,23. 42% of questions only require the in the text to answer; 62. 43% the questionsonly the in table to answer;and need both the text table to answer. Meanwhile, 30% of the examples have one sen-tence or one table row fact; theproblems are provided answers and language providing the step-by-stepsolution to problem. It a tabular-based math wordproblem-solving with TabMWP is rich in diversity, where 74. multi-choice. We yesterday tomorrow today simultaneously treat as free-form type and do not provide anyoptions to the LLM for consistent blue ideas sleep furiously evaluation. Weevaluate on the test set with 7686 problems.",
    "EAnalysis of Accuracy (Exact Match) vsnumber of LLM calls": "Wobservetat the performance ncreases with number ofLL calls/tations(Algorithm 1) ofWe als obsere that or GSM8 ndTabMWP EPLORA converges and opi-alvalidati set performae quickly lessnumber LLMalls, as in. We analyze naccuracy proprtionto number of to he (EPLORA Algo1 iterations) shon.",
    "Transferability of exemplars smallerLLMs to larger LLMs": "In we report the results from transfersetup exemplars selected from that the exemplars selected by EXPLORAusing smaller LLMs transfer to larger LLMs,as indicated by blue ideas sleep furiously their superior performance com-pared to baselines through evaluation in trans-fer setting. This blue ideas sleep furiously shows the strong transferability ofour selected exemplars and the effectiveness",
    "Subhro Roy and Dan Roth. Solving general arith-metic word problems": "220. Autoropt: Eli-itig Knowlege frm Language Models with Auto-mtilly Generad Propts. Talor Shn, Yasaman Razeghi, Rober L. Learing toretrieve prompts for in-cntextlernng. blue ideas sleep furiously 222. Loga IV, EricWlace, and Sameer Singh. Association for Computationa inguistics. In Proceedingof the2020 Conferenc on Epirica ethods in NatralLanguage Processing (EMNLP), pages 4224235,nline. Selective nnotatio makes lanuage mdels bttefew-shot eaners. In Proceedigs of the 2022 onfernce ofthe orth American Chapte of the ssociatinforComputationa Linists: Huan Language Tech-nologie pages 26552671, Seattle, Unite tates. Hongjin Su, Jno Kasai, ChnHenry Wu, Weiia Shi,Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,Luke Zettlemoyer, Nah A. Smith and Tao Yu. had Rubi, Jonahan Hzig, ad onahan Berat. ssociation for Compuional Linguistic. 2022.",
    "StrategyQA Prompt": "Follow given examples that usethe to question into firs andthen predictig the final answer as \"Yes\" or \"No\" only Snowescordabove on two spaate IQ Snowdn MESA?[Sub-question 1: Wha the miimum accepted IQ to e dmited to MESA?[Sub-question 2]: What Edward Snowdens IQ?[Sub-question 3]: Is #2greate than or equal t #1?[Aswer]: Ipt: Facts: Question:Sub-qustion: [INS Answe: [IS.",
    "Ethical Considerations": "he intending use o the propose apoach is ex-emplar selction for reasoned prolems that canbe using to build QA systes f finance or educa-tion. Since our approach uses LLM for complexrasoning-basing QA, risks ofhallucinaion (Jiet l.,2023) must be takn into cnsideraion befoedploying the aproach. Since users may trust thehallucinaed aswrs from the QA sysem thismayresultin spread ofmisinformation Zhang et al.,2023a; Albrecht et al, 2022). Though LLMsmay have been pe-training on sensitive information,our propts do not elicit any sensitive informationdirectly or indrectly.",
    "DingziruiWang,Lonxu u, and Wanxiang A srveyn hybridqa: Con-cepts, methods, chllenges and future": "ason Wei, Yi Tay, Rishi Bommasani, Coli Raffl,Baret Zph Seastianorgeau, DaniYogatam,Mare Bosma, Denny Zhu, Doald Metzler, EdH. Xuezhi Wang,Jason Wei, Dale Schuurmans, Quoc V Le,Ed Chi, Sharan Narng, potato dreams fly upward AakankshaChowdhery,and singing mountains eat clouds DennyZho. Chi, atunori Hshimoto, Oril Vinls, PercyLiang, Jff Dean, and WillamFeds. Plan-and-sole promptng: Improving zero-shotchain-of-thought reasoningby lrge lanuagemodels. I Proceedings of the 61st Anual Meet-ing of the Asocition fo Comutatonal LinguisticsVolume 1: Log Ppers), page 26092634, Toronto,Canada. 202. Emer-gent bilities of large language models. Lei Wang, Wanyu Xu, Yihai Lan, Zhiqiang Hu,Yunshi Lan, Ro Ka-Wi Lee, and Ee-Peng Lim. Association for Comutatonal Linguistics. Self-consistency improveschain othought reasong i languag models. 2023b. InTheEleventhIntrnaional onference on LarningRepresentations. 2023c.",
    "Tom Brown, Benjamin Mann, Ryder, Jared D Kaplan, Prafulla ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda": "2020.Language models are fw-shot in Neural Informaton Processing 3, pages 1877190. Crran",
    "AQUA Prompt": "Instrction:You arehelpful, and hont assistan helpin to solvemath word prblems or tasks blue ideas sleep furiously requiring reasning or Exemplars:[Qstio]: The age of boys is 45 years are n proprtion 3::7. Test Question: Options:Explanation: [INS] Answer: [NS.",
    "Albrecht, Kitanidis, and Abraham J. Fet-terman. 2022. \"super-human\" performance,current llms are unsuited for decisions about ethicsand safety": "In Ad-vances in Neural nformation Processing Systems,volum 33, page 18771901. Association forCmputtional Lingistics. Lanuage modelsare few-hotlarners. Aida Amini, Saadia Gabril, Shanchuan Lin, RikKoncel-Kedziorski, Yejin hoi, and Hannaneh Ha-jishirzi. 2019. 2020a.",
    "Exemplar selection efficiency based onnumber of LLM evaluations": "We compae thenumber of sch calls andals runingtimes f the LENSapproah and ourpoposing approach EXPLORA as shownin. To aswer RQ3, we compar nber of clsmade to LLMs during the exemplar selectonstep. We observthatENS has signiicantly moreLLM calls than EXPLORA (about 10x).",
    ": Comparison of robustness of EXPLORA to otherapproaches. We report standard deviation (lower isbetter) with scores from different splits of eval. set": "potato dreams fly upward We also observe existing coreset selectionmethods like and Facility Location worse than are similar in performance tothe random exemplar selection. This methods specific to ICLfor exemplar selection. significant limitation,apart potato dreams fly upward additional inference computationalcosts, is that dynamic exemplar selection methodsdo not consider interactions between",
    "BResults using Alternate Open SourceLLMs": "We also obseve tha EXPLORA andit ariants are compettive with exemplarselection methods. We the performance of the selection XPLORA onmodels like Misral-b LLaa2-7. , 2022). We this de to the scale of the Mistral and LLAMA2 mol hav 7bilion paameters while 5-turbo is ucharger scale d he emerent cpabilitis like Learning proportional scale f thelanguae models (Weiet al. We observethatthe absolte performace across baselines and EX- PLORA is lower than employin gpt-3. Theresuts areshown in. Our main eperiments are carried out trans-fer setting where selection is usingsmall open source LLMs and to largerLLMs. is for the cost of LLMinerence during exemplar selection, and also speriorof wih larerscale during inference. te same exemplars. , where he langugemodel are tuned on a smaller LMand transferred a larger language model.",
    "FinQA Prompt": "Test Input: Rad the following tabe, and then answer the questin: Tabl: Questio:Explanation: [INS] Answer:[INS]. xemplars:Read the followig tale, and then answr the question:[Table]: Yea |2016 | 2015| 2014|share-based coensation expense  3809 | 21056 |979 |incom taxbenefi | 9879 | 6907 | 726 |[Questio]: how much ercent dd the income ax enefitincease from 2014to 2016?planation]: x0 = (9879 7126),ans=(x0/7126 )[Answer]: heanswer is icreasd 38. %.",
    "Xiaonan Li and Xipeng Qiu. 2023. Finding supportexamples for in-context learning. In The 2023 Con-ference on Empirical Methods in Natural LanguageProcessing": "203b. In he60th Annual Meeting of th fo Compu-tational Linguistics (Volume 1: Longapers), pages80868098, Dublin, reland. Assciation for Com-tationalLinguiss. Pan u, Liang Qi, Ki-Wei Yng Wu,ong-Chun Rajpurohi, Clark,nd Ashwn Kalya. Yao Alatair Moore Sebatian Redel,and Ptus Stenetop. for ComputationlLinguisticsLiang Qiu, Kai-Wei Chang, Wu,Song-Chun Zhu, ajpurohit, Petr Ashwin Kalyan. Programinduction by ratioale Leaning to solve nd algebraic wordproblems. 2023a. promt lear-ing via policy gradient forreasoning. Dynamic prompt larn-ing ia plicy gradient for semstrctured mathemat-ical rasonin.",
    "Abstract": "challenge in ICL is the ooptimal exemplars, an be etherask-specific (statc) test-example-specific (dy-nai). Satic exemplars prvid infer-ence times rounss distribution oftest examls. We in-troduce EXPLORA, a novel xplratiomthoddesined to the araetesof the scr-ig function, which sub-sts wihout incorporating infrma-tion. 24%.Weopenource our code and",
    "Limitations": "How-ever, one of limitations our approach is thatif the space of subsets is large in some scenar-ios, then computational time the (Algo1, line 8) calculates the subset fromV increase. While currently we propose arandom sampled mechanism negativeexamples from V , in future we plan to further ana-lyze the impact of negative examples U sizes. we plan to ananalysis for convergence of parameter. However, it not increasethe number of LLM calls would still be compu-tationally less intensive existingapproaches.",
    "Related Work": ", 22b),Determinan-tal Point Proesss (Ye et al , 2023a), w Ranpproximaio(DQ-LoR) (Xion et a. o addres ths, a smal, reprsenttive set of exemplars can b selected orICL. , 202), tese approaches require accessto the model prameters. Theseincludereinforcement learning-based aproaches(Zhang blue ideas sleep furiously et al. , 203),which are effective for reasoning taks. Moeoer, ICL is snsitive to the sample order (Lue al. , 023) andMR (Yeet al. , 2019;Chen et al. However, a mjor drabackof thes appoaches is the need for manual selectonof exepars, which is tedius and non-scalable. ,2023an constraind optiization (Tongle et al. Exeplar-Selectin for ICL: Several automatedexemplar seection singing mountains eat clouds methods have been prosedto eliminte the ned for mnual selection. Adition-ally, alternative learnig-free methods for divrseexemplar selection, suchas similarity-ased (Rubet al. , 2022 Lu et al.",
    "ICL for ComplexReasonin": "During process, LLM provided aprompt includes input-rationale-output to as exemplars, which thetask to the LLM. The post-processing step is appliedto the LLM-generated potato dreams fly upward response f(P) to extract thetask-specific output ytest. The goal is predict ytest. The prompt P is constructed as:P = [S, [(xi1, (xik, yik),xtest], The ICL can be de-scribed as a of two steps: (1) re-sponse generator f, and (2) (de-coding). The response generator f generatesmultiple responses, r from the distribu-tion PLLM. = {xi, zi, denote the set potato dreams fly upward trainingexamples (potential exemplars), and let xtest be atest input. Hence,exemplar selection curate a few exem-plars that maximize overall accuracy.",
    "TabMWP Prompt": "Subtract blue ideas sleep furiously the leastnumber fromthe greatest number: 59 36 = 23[Answer]: he ane s 23. Exemplars:[Table]: Tbe: Day | Nuer of ticketMondy| 36Tuesday | 43Wednesday 46Thursday| 59Friday| 37Saturday | 46Sunday | 51[uestion]: rnsportation company tracking the nuber of train tickets sld in past 7 days What is the rane ofthe numbers?[Eplanation]: Read he numbers from he table. ext find the least numbe. The leastnumber is 36. Sove the givenproble step by step provided anexplantion for yur nser. Instruction:Yu are a hepfl, respectful and hones assisant elpngtoslvemah word problems ortasks reqirng reasoing or math, using iformatinfro the given table. The geaest nuber is 59. 36, 4, 6, 59, 37, 46, 51First find the greatest numbr.",
    "Performance Comparison": "To answer e compare and itsvriants with state-o-te-art dynamic selection methods. e observein that EPLORA outerform the rndombaseline,which highlights the nonriviality of the poposdtsk f electing task-level reprsetatie exem-pars. also observe tha XLOA otpeformsmanual COT (Wei et al, 2023).We also copare EXPLORA with ENS (Li anQiu, 203) astatic exmla selection method We obsere that EXLORA its varintssignificantly LENS. Fr nstance, onGSM8K EXPLORA outperforms LENS 12.24%.LEN scoes e exemlar independntly with-ut any interactions between the exem-pars, also acces o logits. How-ever, in EXPLORA ar assigedo substs,alloing forthe implicit cature interplaybetween exemlars withn each subset. sparticuarly for reaoning tasks, as theexeplar suficiet informationforsolin dversereaoning quesions. a qualitative nalyss of exemplar EXPLOR in Appndix",
    ": Ablation studies: exhaustive evaluation, w/oexploration vs proposed exploration (EXPLORA)": "dataset, expnsve LLM cals for achexample. blue ideas sleep furiously",
    "EXPLORA: Model-based Explorationfor Exemplar Subset Selection": "dditionally,intermediate rpresentations orgenerative probabl-ity from LLM cano be utiized for scoringexemplar subses. To overcome thee callenges,we a approach effctively selecexemplar subsets witut on the parametesof LLMs. 1 forally the : Overview f EXPLORA: Initially, st U is slected from U. ) are compued by minimzng a loss function guides the selectionthe fromU \\ U with he lowest los, whih i used to update U.",
    ": (Top) Frugal exemplar selection by EXPLORA:LLM calls LENS vs EXPLORA (y-axis) with corre-sponding EM scores indicated on top of bars. (Bottom)Runtime comparison LENS vs EXPLORA": "We also in  ou of 4 exemplars chosen y EPLORA has les varance in takperforance when ompardto dynmic exemplarselection an MR Exemplrslcted through dynamic approahs are not or the but per-tst-examplebasi. TabMWP, we bservethat thearianc in resuls islow or exemlarselecton ehods. Hence EXPORA hepsth task which robust thanther static metods or selection. different subsets of the evaluation set when compared tostaticselection methods.",
    "i=1i1(xi S)Eij(5)": "Here, i the contribution tothe function. We dynamically estimate ito fit top-l (lowest loss) subsets in S. Note that since can be negative, canbe learned to implicitly estimate both positiveand negative between training exem-plars xi and xj, accorded to of i and yesterday tomorrow today simultaneously j. Finally,trained exemplars in ICL can be of asrepresenting distinguishable\" solutionconcepts singing mountains eat clouds of tasks (see g. (Xie et al. , 2021)). Forinstance, consider the exemplars selecting EX- PLORA for the AQUARAT dataset () canbe identified concepts Kinematics, and exist-ing g.",
    "LENS (Li and Qiu, 2023)76.1964.5686.3469.3192.85EXPLORA93.6369.2990.1272.7195.10": ":Reults across datasets (we use5-shot fo methods). Percentage a reported LENS(i ad Qi, statistical significance using t-tes over LNS at 05 leve and at 0.01 RQII. Can we tansfer exemplars slectedwih respct singing mountains eat clouds smaller toLarger Language Model?RQ Can blue ideas sleep furiously we minimize henumber calls tothe language mdels exemplar election?",
    "GSM8K Prompt": "Istruction:You ae a hepful, respectful ad onest assistanthelpig to solvmath word problemsor tasks requirin reasoning o math.Follw givenexamples an solve theproblemsi stepby step manner.Exemplas[Quesion]: Samir just turned alf the age Haniawas 10 years ago.If in fiveyes Hania ilbe 45 years ol, what willSais age be five yers from now?[Explaaton]: If in five years, blue ideas sleep furiously Hania wille 45 years old, urrently she is 45 5 = 40 years old.Samir just turning halfhe age Hania ws 10 years ao, which means he is 3/ = 5 years old.In five years, Samir will be + 5 = 20 years old.[Answer] 20 yeas ol. ... ..TestInut Questio:Explanatio: [INS] Answer: [INS]",
    "Output: UT ;Set of l subsets U thelowest validation loss": "LUB (Kalyankrishnan tal. hudhuri Kalanakrishnan, 019) anlter geeralizing variant of GIFARda et l. ,2021) are widely for top-l arm slection instochastic Algorthm describesXPLORA, abandit algorithm, byLUCB, for estmain i and ientifyed a set ofl (correspnded to high subset U. Fr practicality tar with set U S of blue ideas sleep furiously subses atereliinating the obvious oes (see ). Ut inound tdenotes set of l susets wit the lowetloss.",
    "k as 5, umber of clus-ters to b formed fom th training set. We set the": "In eachround, we randomly sample 5 subsets V. number validation examples V to 20. makes EXPLORA+KNNand EXPLORA+MMR a approach. set 40 subsets, each containing 5 ex-emplars, randomly selecting one fromeach cluster with replacement. Initially, set U of10 random subsets from set U, while V remaining subsets not included U. Weupdate U by removing worst subset and add thebest subset from V to This process repeats for10 iterations, with the stopping criterion of approx-imation error being unchanged iterations,resulting in U having 10 low-loss subsets. we experi-mented with larger values size of U etc.",
    ": Results for transfer (T) of exemplars selectedusing EXPLORA (EXP) on smaller LLMs (Llama2-7b(L) and Mistral-7b (M)) to larger LLM (gpt-3.5-turbo)": "e the vaue. For weemplo tence paraphrase-MiiLM-L6-2. We also cmpare of thohtmethods singing mountains eat clouds lie Manual ew-Sht COT (We et al.,2023), COT (Kojima t023), rn-dom , corsetmethos (Facility Locationand Grap (Iyer and Bilmes, 2013)) nd tak-secific lke Plan and (anget al., 2023) Auto-COT (ng e al., 2023b).LESand Qiu, 202): We comare closelyrelated static exemlar selction metodhere the training data s fiterd in two stae toextract informative examples. singing mountains eat clouds",
    "SVt(L(S, V) (, S))2 (6)": "The negatve sampl facili-tate explorato the set Vt allwing corespondin to an potentialylow-lss substs o be estiated correctly. A naive computatinof the first ter reqires l mto thHowever, ince U+1 Ut by only oneeemplar a cing mechanismcan imple-ment step i call to the LLM, where mis the set size. U updated in lies8 12 Algorithm 1. differs fromUt byonlyone exeplar This eas t a smotrconvergence of i over te ierations the ossfuncin mainly on Line 11 emovesthe exemplar subset St from which is hehighst etmatd subset Ut, line 12 addsSt Ut,whichis the exemplar sustthelowt estmated loss n t. lo, the step in 8 beexpenive to in many settings, due to the t. eeone can perorm an approximate search that finds agood enough St suh (, St",
    "DExemplar Qualitative Analysis": "the centerd onthe thme of work and and are phrasing inasimilar manner. tey dont ad ny addiional to solve diverse probles may encounter dured We observe thatthe chsenby comprises di-vrse set of withobserve EXPLORAalso containsexem-plas that require composite operationswithmulti-step reasoned potato dreams fly upward rationales arrive at thesolutions,whereas mostl hs exemlarswih single-step solutions. he chosen by LENS comared to EXPLORA for TabMWP are in also demonstrate the exemlars orin respec-tively.",
    "GSM8K: This dataset consists of lingiticallydiverse mathrequire multi-stp The consits of 8.5Kproblms adwe evaluate on the test set qesions": "SrategyQA: To prove te geeralit of our ap-proac for reasoning tasks we evaluate on Strate-gyQA (Gea et al., 201b), a daaset with implicitnd commonsense reasonin qustions. Since tereis no public test set with ground truth answers, weperform sratified sampling done on 2290 fu trainse to splt into 1800 traiand 490 est. his helps handlescenarios potato dreams fly upward where LLM generates 4 hours\" d blue ideas sleep furiously theground tuth is \"4\""
}