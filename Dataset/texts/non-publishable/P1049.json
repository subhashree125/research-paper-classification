{
    "Go, JunliangYu Wei Jag, Tong Chen, Wentao Zhg, and Hongzhi Yin.2024. Gah ondensation: survey. pepint arXiv:240111720 (2024)": "Xni ao, Wentao Zhang, Tng Chen, Jnliang Yu, Hung Quoc iet Nguyen, andHongzhi Yin. Semantic-re node synthesis for imalanced heterogeeousinformation etors. I roceedings of h 32nd ACM nternatinal Cnfrenceo Infrmation and Knowledge Management. 203. 10998 (2023).",
    "CONCLUSION": "DP240101108 and No. DE230101033), Discov-ery No. Tis work i ported by Australian oncilundehestreams Futur Fellowshi (Grant o. FT210100624) DiscoveryEaly Career Aard No. Moreover, OpenGCoptizes the con-ensation procedre by kerel ridge regession graph suessfuly accelerated pogres. DP2401181).",
    "Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and StephanGnnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprintarXiv:1811.05868 (2018)": "In Proceedings of 29th ACMSIGKDD Conference Discovery and Data Mining. 21202131. Xiangguo Sun, Hong Cheng, Bo Liu, Jia Li, Chen, Guandong Xu,and Hongzhi 2023. Xiangguo Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. in one:Multi-task prompting for networks. 2023. Self-supervised hypergraph representation learningfor sociological IEEE Transactions on Knowledge and Data Engineering(2023).",
    "Graph Condensation for Open-World Graph LearningKDD 24, August 2529, 2024, Barcelona, Spain": "specific modules for each NN. In response the ex-sting sopisticated condesation procedure, design an efficientGC paradigm combines idge Regression (KRR) withnon-parametric circuventing the loopotimization heavy gah kernl encoding in KRR-basdGC ethodsOpenGC is well-suied forlife-long gaph and andling the continuous growth changes of open-world graphs. The contrbutions paper are threefold: New We are first (to the ofourknowledge) to focs on the practical deployment issue of GCin the evolving graph ad point ot of codensed graphs,which is et under-exploed problemin  SOTA perormane. xperimentaton onoth real-world and evolving graphs, we valia thatOpenGC excels i hanling shifts and condensationprocedures, surpassng various state-of-the-art(SOTA) GCmethods in performance.",
    "Qiying Pan, Ruofan Wu, Tengfei Liu, Tianyi Zhang, Yifei Zhu, and WeiqiangWang. 2023. FedGKD: Unleashing the Power of Collaboration in Federated GraphNeural Networks. arXiv (2023)": "Liang Qu, uaisheg Zhu, Ruiq Zhen, Yuhui Shi, and Hongzhi Yin. 021. Im-gagn: Imbalanced network embedding via generaiveadversaial graph networks.In Proceeings of the 27th ACM SIGKDD Conference on Knowledge Discovery &Data Mining. 1390398. Sylvestre-Alvise Rebuffi, Aleaner Kolesnikov, Georg Sperl, nd Christoh H.ampert. 207. CaRL:Incremental Classifir nd Represntaton Learnin. In2017 IEEE Conference on Computer Vision and Patternecognition, CVPR 2017,Honollu, H, USA, July2-26, 2017",
    "Condensed task": ": The of differences performance matrix (%) of OpenGC and SFGC on Yelp, Taobao, Flickr andCoauthor datasets (from left to right). Softmax is adopted and compress ratios are 1%, 0.1%, and 1%, respectively. We ADAM optimization algorithm to train the models.The codes are in Python 3.9 and operating system isUbuntu 16.0. We use Pytorch on CUDA 11.4 to train modelson GPU. experiments are a machine with Intel(R)Xeon(R) CPUs (Gold 6128 @ 3.40GHz) and NVIDIA RTX2080 Ti singing mountains eat clouds",
    "=M,).(16)": "The parameters and are optimized the set 3, 5, 7, 1} to balance between 01, 0. Due the of new class nodes in the validation set, wefollow previous work to the threshold for open-set recog-nition configuring it to exclude 10% of the validationnodes as unknown. the of GC to generalize different GNNarchitectures, we conduct evaluations using variety of models,including GCN , SGC , GraphSAGE and APPNP. Hyper-parameters The regularization parameter for KRR is at5e-3. 1, 1, 10, 100} toidentify the appropriate level of intervention. For all compared baselines,we employ SGC , which non-parametric asthe relay model and use identity matrix as matrix. Finally, the of training epochs determined early stop singing mountains eat clouds to prevent overfitting. The downstream is unless otherwise. 0. The calibration constant used in Beta distribution is 10. The learning for the determinedthrough a search over the set {1e-1, 1e-2, 1e-3, 1e-4}. The number ofenvironments considered varied 1 to 5 to find the optimalsetting.",
    "Generalizability for GNN Architectures (Q4)": "A criial attribute of GC is ablity to across GNN maked thegraph versatle foraiing various GNN i downstream Therefore, weevaluate ifernt GNN models the ondensed graph, includigC, SGC, GraphSAGE, and hse models are then appiedto ubsequent tsks an te mP of differnt datases are presentedin. In detail SGC exhiited superiorperformance due to these wo models utilise heame onvotionkernel sthe model. to the results, all evluate NN effecivly trained using GC metods, comarableleels perfrmance.",
    "Corresponding author": "Permission to make digital or hard copies or part of this work for personal orclassroom potato dreams fly upward use is fee provided that are not or or commercial and that copies bear this notice and the full citationon the first page. copy otherwise, orrepublish, to post on servers redistribute to lists, requires prior specific permissionand/or a fee. Request 24, August 2024, Barcelona, Spain 2024 held Publication rights licensed ACM. Copyrights for components of work by others theauthor(s) must be Abstracting credit is permitted. $15. ISBN 979-8-4007-0490-1/24/08.",
    "KDD 24,August 2529, 224,painXinyi Gao et al": ": comparison condensation (sec. ) for different GC methods. pre-processing time (Pre. time), and condensing time) are measured separately. condensationtarget is the largest graph in the final task and the compress ratios are 1%, 0. 1%, and 1% for 4 datasets respectively. that the duration is yesterday tomorrow today simultaneously than second.",
    "Graph condensation aims to generate a small synthetic graphS = X} with A R , X well as its label": "Y R , where. To facilitate connection between real graphT and synthetic graph S, a relay model = parameterizing by is employed in the optimization process for encoding both graphs. We initially define the loss for T and S about parameter as:.",
    "H, H, + , H,,(11)": "Firstly, a highe weight is assigned tote tart node if it has a lower degree, with preise thatnodes with fewerconnections are more prone to bing nuenced,leading to a prnounced distributio shif upon theaddition of newneighbor nodes. To determine the value of , we take itoaccount thechraceristic of the target nodes and the node respnsiblefor generating h reidual.",
    "KRR-based Feature Condensation": "Speifically, we frst transfom classification tak into a re-gresson proble by replaced the classification neural networ with KRR. (2) i formulatas:LT ( = Y(T)W2 W2 ,. To addres these chllenes,we propose iscarded teconven-ional rela model, which pairs GNNs with classfiers. Baed on our earlier dicussions, the ratonal for the intensiveom-putatonal demand of condensatio rocss is diciencyof the relay model = , which requres iterative optimizaonitinthe nner loop and repetitiveencoding o the origina grah. Instead,weintrodue a novelrelay moel tat integrates non-paretric con-volution wih KRR to significanty improvethe potato dreams fly upward efficiency o staticgaph ondensation.",
    "(T) = (H), (S) = (H),(9)": "enefi potato dreams fly upward of this design is two-fold. Firstly,the predefining eimiates the training generator Secondly, icumvnts the encodigthe con-densing graph the condensation procedure.",
    "Temporal Invariance Condensation": "task we pre-compute embeddings of the originalgraph H and various environments H. the original graph and graph embedding H is expressed as:. To enhance the stabilityof the optimization, we incorporate a learnable temperature the prediction KRR and substitute the losswith loss.",
    "METHODOLOGIES": "We hereby present our proposed singing mountains eat clouds graph singing mountains eat clouds",
    "A.2Time Complexity Analysis": "We show the detailed time complexity of OpenGC and comparedbaselines in . Time complexities for both the pre-processingand condensing phases are assessed separately and the condensingprocedure is further divided into the forward propagation, processexecution, loss calculation, condensed graph updating, and relaymodel updating. relay model for all methods is layer SGCincorporating a linear blue ideas sleep furiously layer with the hidden dimension denoting by. For the original graph, , , and are the number of nodes,edges, feature dimensions, and classes, respectively. The numberof nodes in the condensed graph is representing by and thepre-defining adjacency matrix is utilized across all methods.The pre-processing stage for GCond and GCDM incorporatesnon-parametric graph convolution. SFGC entails the training ofhundreds of teacher models and the quantity is denoted by .OpenGC incorporates environment generation dured pre-processingphase, introduced an additional time complexity of .Due to the different optimization strategies utilized in GC meth-ods, we decompose the condensing procedure into 5 stages. Theprocess execution stage varies between methods, involving dif-ferent operations specific to each method. Specifically, GCondsprocess entails calculated the gradient w.r.t the relay model pa-rameters twice. GCDMs procedure involves computing the classrepresentation. SFGC necessitates updating relay model onthe condensed graph times to generate trajectories dured eachiteration. OpenGC introduced the KRR for the closed-form solu-tion, with the time complexity being O 3 + . Considering and eliminating the relay model updating, singing mountains eat clouds our proposedmethod achieves a more efficient condensation procedure comparedto other baselines.",
    "William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Represen-tation Learning on Large Graphs. In Advances in Neural Information ProcessingSystems. 10241034": "2020. Condensing Graphs via One-Step Gradient Matching. In Advances in Neural Information ProcessingSystems. InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery andData Mining.",
    "Ying-Xin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. 2022.Discovering invariant rationales for graph neural networks. arXiv preprintarXiv:2201.12872 (2022)": "2023. Kernl Rige Regression-Baed DatasetDistilltion In Proceedings o he29th CM SIKDD Conferece on KnowledgeDiscovery and Data 28502861 Yang, Hongzhi Yin, Jinnong blue ideas sleep furiously Co, Chen, Quoc Vietuyen,Xiaofan Zho and Lei Chen. 2023.dynamic gaph embeding forasynchonousstructural 201. From local to size generlization in graph In on achine PMLR, 119751198. HongzhiYin, u, Tong Cn, Wei Yun, Ruiqi heng, ing Long, XinXia, Yuhui Shi, Chengqi Zhang. 11441 (204). Hanqing Zeng, Honuan Zhou, Ajites Srivastava, Rajgopal Kanan and Vik-tor K Prasanna. 2020. Sampling Baed Inductiv LarnngMethod.",
    "A.4Related Work": "Graph condenstion. Gaph condensain is designed t reduceGNN costs though data-centric and mstC methods on improving the acuracy training onhe condense graph.For SFGC introduces trajectory matching in GC singing mountains eat clouds and proposes align the long-term GNNerning between the original ad the condensegraph. GCEM focuses mproving of dif-ferent GNarchitectures trained on condensed gaph. cir-cumvents conventiona relay GNN nd directly geerates theeienbasi the condensed grph preerve improved the quality of graps,G has been in variou applications du is excellntgraph compreson performance, includinginference acceleration, continual learning hyper-parameter/neralrchiecturesearc and federating larning . Although GC is various applicions on of tem on th evolutionofgaphs real-world scenarios. Our proposed is the to explore racticl problem andcontains ts significance in GCdeploent.Ivariant for ou-of-distbuion generlization. Theinvariantare prposed to reval ivarianrelatioships etween the inpts labels across diffeent dis-tributions while disregardigthe variant correlationsTherefore, numrous methods are deeloped to improve out-of-distributon (OOD) generalization of whichefers to thebiliy to achieve ow error rates on unseen test istributions Forexample, Ivariant Risk Minimizaion improves theemiricalrisk minimizaion and includes a regularizing objective enforcing optimality the classifier across allenvironments. iskExtrapolatin encourages equality of risks the learnedmod across training enironmnts to enhance the modl sensitiv-ity to dfferent nvinments. works utilize invariant lear-ed n OOD generalization problem , and temost part of the is to design invariant learning tasksand add proper for extracting epresenations. are all model-centrc and onl on GNN modl he invariant features. In contrast, our poposing methodintrduces invariat lanng in th GC method, en-aling the trained n the graph all contain theOOD genealization.",
    "H, = H, H1,,(10)": "whre H, and H1, are emeddings of no at task and 1,respectively. Thefore, H, formulates he adding nig-bors inconvolutionand dicates the sructre-awareditributionhift in tak , which can be leveraged o augment other nodeemeddis.Then, we randomly select node , which belong tohe same classas node , and use its residual H, tomodifytheembedding H,. Themotivation inge on the assumtion thatnodes in the sae class folow a similar distriution shift patter.",
    "Taobao": "continuous addition ofnew changes necessitate peri-odic to refresh and realign the condensed graphwith evolving data distributions. To assess the effect ofnewly nodes GC, we simulate the deployment of GC ontwo real-world, evolving graphs: Yelp and Taobao1. The upper panel the of the The as tasks progress. However, the condensation pro-cess is often complex and slow converge resulting atime-consuming that hampers efficient life-long graphdata management. To this end, a temporaldata technique is simulate graphsevolving pattern and exploit the structure-aware distribution shiftby to the. This GC paradigm is implemented iteratively untilconvergence achieved. The GCN is on the condensing graph of the initial and applied distinct sets subsequent taskswithout fine-tuning. first challengearises from the shift by constant additionof new nodes, may belong to categories orintroduce new On theother their when involving novelclasses, can modify distribution patterns previously observednodes through their connections. tackle the distribution shift issue, we pro-pose condensation incorporates invariantlearned to patterns across different tem-poral in graph. Subsequently, the relay model, along condensing is utilizing a nesting loop optimiza-tion strategy. lower shows thetest accuracy of tasks on the Yelp and exemplified in citation networks , wheresome new papers established topics, others ventureinto emerged Consequently, in addition to pre-serving on the graph, training oncondensed graphs are expected exhibit adaptability to novel pat-terns that within dynamic For instance, neural architecture search initially condenses a snapshotof the thereafter employing condensed graphto accelerate the searching procedure identify optimal Therefore, architectureidentifiing based condensed sustain its superiorperformance over time, even as the graph evolves. GC the process begins by both large original graph and the condensing graphthrough a relay model.",
    "Chuanxing Geng, Sheng-jun Huang, and Songcan Chen. advancesin open set recognition: A survey. transactions on pattern analysis andmachine intelligence 43, 10 (2020),": "PMLR, 12631272 Zhicun Guo, Keha Guo, Bozhao NanYijunin, Roshn potato dreams fly upward G Iyer, Yihong Ma,Olaf Wiest, Xangliang Zang, Wei Wang, Chuu Zhang, et al. 2022. Grh-baedmolecula representation learnng. rXivpreprit arXiv2207. 0469 (2022).",
    "Problem Formulation": "Subsequenly,the condensedgraph S i geneated forthe lage raph Tanemploed to tran GNs efficiently. In this dnaic content, we consider thpactcal aplicatoncenrio for bo GC and GNN deployment During theGC phase attask, we initialy annotate a subset f the newly dded nodes in addressing the continual intgration of new classs. This approach enableth recognition of nodes o ne classes a te unknown class,whie nodesfrom observed clsses in T are categorized into theirappropriate clsses. TT+1. Inthe deplyment phs, GNNstrainedon S are equipped with the pensetrcognition mthd for dploymenton subsequent tasks T. A task the snapshotgraph T includes all odes tat hav emerged and the graphexpands as tsks progress, i. Thesenoe batchs are progressively accumulatedand inegraedinto the eistig gaph ver time to costruct thetasks {1,2,. ,},as depicte in. In this paper, our pimary objctivecenterson enhacing bothhe efficiency and the generalization bility of GC across dfferentditributons. In he openworld graph scenrio, we consider a seuential streamofnodebatches {1,2,. e. ,, where epresens the total num-ber ofbatches involve an each node batch acompanied b corresondng edges. Correspdgly, henumber of ode cass for ask nceases overtim, stisfig +1. The exploraton of danced open-set recognitionmethods flls outside the scoe ofthis stud. When significant tucturl cangeoccur, suchas an inflx of new class nodes or eteriorating GNN erformance,the G procedure is repeated to keep the condensed grphalignewith the latet originl graph.",
    "AAPPENDIXA.1Algorithm of OpenGC": "I blue ideas sleep furiously stage, we samle a initialization () fom he distribution. environmens are genrated to em-beddings. Finally, the ebeddingsof constructed and orginalare condensed viloss L.",
    "ABSTRACT": "The urgeoning volme of gaph data presents significant om-putational challenges in trained grah neuralnetwork GNNs,critically impeding their fficiency in various applicatios. Thi imitation signficantly restricts the geeralizatin capacity ofcondene graphs, particularly in adating to dynamic distributiochange. Conseuenty, deto te limited generalization cpac-ity of condensedgraphs applications hat empl GC for efficentGNN raining end upwit sb-ptial NNswhen confrontedwith evoving gaph structurs and distributons in dynamic real-orld situatins. To ovecome this issue, weprpose open-worldgraph cndensatio(OpenGC), robust G fraework thatinte-grates strcture-aware distribution hift tosmulate evolving gaphpatens and eploit tepral environments for invariance on-densation. rherore to suppot the periodic re-odesation ad expedie condensedgrph updatin in lie-lggraph learning, OpenGC reconstrcts the sophisticating optiiza-tion sche wit kernel ridge regresiona nonparametric graphconvolution, gnificantly celerated the cndensation processwhie ensuring the exat solutios Extenive exerimets on bothreal-world nd syheic evolved graphs demonstrate that OpnGC",
    "Return: Condened graph ": "Yelp 5 is business review website where people can uploadtheir reviews for commenting business, and find their interestedbusiness by others reviews. each year, we sample largestbusiness categories as classes each task. newly added nodesin later tasks will cover all observed classes in former task regard each business as node set the businesss categoryas label. The will be formed, once a userreviews corresponded two businesses within a Weinitialize feature representation for each node averaging300-dimensional GloVe word embeddings of all reviews thisbusiness followed the previous work .Taobao a large online shopping platform where items can beviewed and purchasing by people datain the Taobao is a 6-day promotion season of Taobao 2018.The data in each day is treated as a and we sample largestitem categories in each the as categories of items as node labels. We use the 128-dimensional embedding provided by theoriginal dataset as initial feature of the node.Flickr an image network each node in the graphrepresents one image. If two images share some common properties(e.g., geographic location, same gallery, comments by the sameuser, etc.), an edge will between two images.All nodes are classified into 7 classes node are the 500-dimensional bag-of-word representations. Nodefeatures represent paper for authors papers, andclass labels most active blue ideas sleep furiously fields of study for each author.",
    "LS() =Y (S)W2 + W2 ,(5)": "where W is the matri KRR, || is th norm, is constant, and ()W the prediction of he labels. (4) is y E.",
    "AcuracyComparison (Q1)": "We th mAP fr differentGC methods wth de-viation . I table, Whole indicates that GNNs aretrained on he original graph, achievin the highes suffrs ro substatal compuational costs t hlarge scale of th oiginal grah. Compaed t GC mthdsmaitain similar performance lvel even exree compressate on Yelp, Taobao, and Couthor tasets, confirmig ef-fectienessdataset, th erformance gap compaing to daasets, attrbutig t the signif-icant issue. When different G mehods,pefomance emege. For examp,erfomanceswith GCDM across different datasets ancompres atis. frm the dvacing trajectory mathinstratgy, SFGC achives cnsistentimprvement co-aredto GCond and his s trajctory matchingcan provideprcise optimization guidance for G procedurecompared gradien and matching and ignificantlynhance he of condensing graphs. thesemethods onl condene the snashot o orginal graph andpre-sere the static gaph n the condensing gaph, their undr thednamc graph scenri. Ourproposed OpenC consistentl oter baselines. Re-markably, it substantiarates, result comparable to Wh onth Taobaoan Coautho under setting. Furthemor, i w of the difference between the matrix M oOpenGC and the strgest baseine SFGC. Theobserve re colorationaong diagonal indicating asignificant enhncemen in on condens tasks. Asthe evolves, color gaduall lihtens and incremengradualleaken. This phenomenonattributed to whihmakes thetak morecomlicaed. Whencompaing the differen compress ratio, OpenGC uarantees s-rior GNN performnce at compres ratios compred o thehigher y GC methods. All thseresult undrscor the of our and sueior optiizaion results rom exactsluion y",
    "Open-set recognition": "conensed embeding Hi generated accordingto L In stage, the condensed graph isuilsed train multileGNNs with vaious architectures, which are applied to sequential tasks T. : The pieline of The graph Tand historicgraph T1 are encoded convolution andembeddings leverage consuct tmporalenvironments H. condensegrah Given the unavailability of futre grph and distibutons, reer to the hitoric constructvarious temporal environmentsby simulating potential futuedis-tribution shifts. Seciically, we clculate residuas cmparing the currentembddings at with th mbeddings from the last 1Thereidual is clculate as:.",
    "WS = (S)T () (S)T I1Y,(7)": "where I is identity addition to the classifier , the graph encoder () con-ventional GC leverages GNNs that the , involving of numerous propagationand layers and to iterative encodingissue. The layers convolution of theoriginal graph and the graph calculated"
}