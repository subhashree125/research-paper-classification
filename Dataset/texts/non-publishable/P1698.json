{
    ": An illustration of person-alized demonstrations for the coopera-tive navigation task. There is only oneagent in the environment. The agentsobjective is to reach one of the twogoal locations": "Therefore,wedesign the persnalized task blue ideas sleep furiously such that a single aentcovers eitherof the two gals without th other gents presene. blue ideas sleep furiously",
    "Related Works": "Imitation Learning (IL). IL methods seek to replicate expert policy demonstration data generatedby that policy. Cloning (BC) is commonly IL technique (Pomerleau, et al. ,2016), where the expert estimated supervised learning demonstration data. , 2011). , Ziebart et al. , 2008), which theunderlying function is estimated from the demonstration data and then used for learning. To alleviate the computational overhead of IRL, Generative Imitation Learning (GAIL) (Ho &Ermon, 2016) was introduced, allowing direct policy from observed data without intermediate Song et al. (2018) extended this approach to Multi-Agent GAIL which adaptsGAIL to high-dimensional environments featuring multiple cooperative or agents. general,IL can rarely perform better than demonstrations. Werefer the readers to for illustration and. of Wang et al. (2023) for more information.",
    "Main Results with Personalized Demonstrations": "We evaluate PegMARL with prsoalized demonstratons on discrete gidworld ()and amult-agent parice environment a).Th gridworldis fullyobservable with discretestate and actin spaces, whilhe PEis obsevbwith a continuousstae spae ndcrete ction (021),a leading decentralizing MARLalgoithm; (Song et al.(208)), state-of-the-art mult-gent mitaion learingalgoithm; DM2 (Wan et a. whih distributionreward with reard; and ATA(She et al. (2022)), one of th best ulti-agent reward-shapng mehods. Specifically, we processed obervatins fr joitenvironmet rollouts byo observable the Personalizing MDP, to our PegMARL. his adjustmet ensues thatinputs to respective disriinators areththe personalized demontration data thi context as ablatio of lackinghe environmental signa andtranstion isciminator, DM2 eprents an abltion ony discrminto. Detailson algorithm implementation yperparamete choices can be und iAppendix C. Orcle denotes the possible returnachievable with optimal policies. receive positie reard oly upn reached eincur fr collisions, and instantlydie ndtermiate the episodesiftheystepinto the ava, makin learnng ths vironmet challngi. , MAPPOstruggles to evelo meaingful across all due to thesprse structure. MAGAIL performs orse, rimarily due to t abence While ATA learn suboptima policiesin seting, ts lernng fficacy eclines drastclly as tenumber agents ncreaes. This undercores the challenges tainng as theof themulti-agetenironment increase. In PMARL supror generalizabiliyacros enarios with more gents, maintainingstable despite suboptimal demostrations. As DM2 has bee adapted to ccomodate peron-.",
    "A.2Cooperative Navigation": "The of 2 agents, goal yesterday tomorrow today simultaneously laarks ad 1 wll btween theagents and the goal. mified thecooperativetask multi-agent et al. e sparsifyther original reward as follows:. potato dreams fly upward. Each aget as a partial bservation f theenironmnt, including position andvociyasthe relative positions of othe agents and The actin space isdiscree, consisting of moving left, right, up, an in lace.",
    "B.2Joint Demonstrations": "The SMAC Environment: Regarding joint demonstrationsutilized in SMAC environment (Samvelyan et al. (2023)). For joint demonstrations sampled from singing mountains eat clouds co-trainedpolicies, they are derived from jointly trained expert policies thatachieve approximately 30% win yesterday tomorrow today simultaneously rate.",
    "agent Env": ": A motivating example illustratingthe imitation of personalized demonstrationsin a multi-agent environment. where is a weighting term balancing the long-term rewardand the personalized policy similarity. The Jensen-Shannon(JS) Divergence terms enable individual agents to align theiractions with their respective personalizing demonstrations andfacilitate the achievement of their specific objectives. However, only imitating personalized demonstration maynot always yield favorable outcomes and can even impede thelearning process. Previous works in MARL have predominantlyutilized joint demonstrations as guidance. As demonstrating byDM2 (Wang et al. Two sets of optimal joint policiesexist for this task: agent a takes path 1 and agent b takes path2, or vice versa. In contrast, if personalized demonstrations wereprovided, agents could indifferently choose between both pathsto reach their goals, potentially resulting in conflicted strategiesthrough naive imitation.",
    "E1(si, ai) log(1 Di(si, ai))+ EEi1(si, ai) log(Di(si, ai)).(8)": "Since di access to trastiondisributns is unavailable, verfyed wheher each ai)ai is dmain is nt potato dreams fly upward dirctly feasible. addresthis,wefurthe aproximate function usinganothe disriminative classifier Disi, ai, si) : Si AiSi (0, 1), estimatig the likelhoodof (si, a,si)tuple from the dmonstrations. Afer the iner S term in equation tw w integrte thediscrimnaive reards derived fromthese te environmental rward for th outer prblem.Specifically, reshaping reward ri as ri = i(si, Dii, whee i andi are for th two discriminatrs. The personalizd behavior discrinator i evaluates localstate-actin pairs, providig or actionsthat aign with the donstration and negativeicentives fo diverget while he transiion discriminator D i if local stat-actionpair induces adesiring transtion local state akinto that oberved i the demonstration,adjustin",
    ": Learning curves of PegMARL versus other baseline methods under the door scenario. PegMARLshows better robustness in terms of convergence and generalizability": "PegMAR, eupping ersonalzedtransition discriminator, faster convergence compared to How does PegMARL perfom the setting? We the navigatiotas fromthe multi-agent particle environment et l. exhibits theswiftst convergence. b demonstrates the learning cures of PegMARL versus and. demnstraions, closely resembles but lacks prsonalizedThis highligs of he personalized transitio iscriminator in effctive handling of inter-aentinteractions in personaled scenaios, itseffcacy varyed agent counts. 2. How does PegMARL perform under the heerogeneous he door scenario (b)contains two variantswith varying levelsof fficulty. As shown in , most algorithms, showcase proficientperformace. We AGAILs in thisase inabilty to dirct th agent remaiat the reen square behavior notexplicity demonstrate. eas is fairly straightfoward: success shouldbe achievale by personalizddemonstations, whic illustrate ho each gent navigates to theirrespective oa locations. This underscores importance of nvironmetal reward ignals when demonstratins multi-aent learning The case necessitates a higher degreeofcooperaton: once the red agent ains to mddleroom, the green agnt must aidefrom the gree squar to enable red agentsright room. In this complex onlyPegMARL DM2 demonstrate convergence.",
    "Yunbo Qiu, Yuzhu Zhan, Yue Jin, Jian Wang, and Xudong Zhang. Sample-efficient multi-agent reinforcementlearning with demonstrations for flocking control. arXiv preprint arXiv:2209.08351, 2022": "Reinfrceetlearnig rewards using guidance ofline demonstrion. A reduction of imitation learned and to no-regretonine learnig. In Proceedings of international conference on and p. Workshop and Conference Proceedings, 2011. starcraft arXiv rerintarXiv:102.",
    "efficiencywhile circumventing th imitationof conflicting behavis that hinder jont-policytrainin": "bue agent can uccessfully transitionto he singing mountains eat clouds neighoringgrid on the right unccupied; the raniin fails. iewin blue agent as te ocalagenand geen agnt of enironent uccessful imitation hinges on he agents local transitionmatchin those of th environment blue ideas sleep furiously where the demonstrtions origited. define aias approximatin forthe i-th trantion probability in the multi-agent by true trasition dynamic P(s|s, and he policy",
    "MARL with Personalized Expert Demonstrations": "Assume each agenti is associated with a personalized task Ti. We collect one set of expert demonstrations for each agent ior, equivalently, each personalized task Ti. By letting an expert user perform each personalized task in therespective personalized MDP, we obtain a collection of expert demonstrations denoted by {BE1, BE2,. , BEN }. Note that while we establish the problem formulationand develop our algorithm in a fully observable setting, the experiments are conducted in both fully andpartially observable settings, encompassing both discrete and continuous environments (details in ).",
    "In standard Multi-Agent Reinforcement Learning, agents aim to discover optimal joint policies that maximizethe long-term return R() := , r = 1": "(2018) for single-agent cases, we dfine objective fo earning from personalizeddmonstrations in multiagent singing mountains eat clouds as folows:.",
    "Mean Battle Won Rate": "The suffixdiff in the legend indicates that the joint demonstrations used are sampled from non-co-trained policies. When the joint demonstrations are sampled from policies that are non-co-trained, the success rate ofDM2 exhibits a significant decline in both tasks. In the 5mv6m task, DM2s success rate drops to nearly0, indicating a failure to learn. In contrast, PegMARL maintains a similar level of performance comparedto results from co-trained demonstrations, with only a slight decrease in convergence speed. When the joint demonstrations are sampled from co-trained policies, PegMARL achieves acomparable, and in some cases even higher, success rate compared to DM2 in both tasks. Otherwise, the demonstrations are sampled from co-trained policies.",
    "B.1Personalized Demonstrations": "et each individual perform their designated tak the same enironment map witut the othragents o colet demonsrations. summarizethe the singing mountains eat clouds soptima for th yesterday tomorrow today simultaneously gidworld envirnment , whereaverageepisdireward ae pproximately 4. 5, bout half of optial conterparts.",
    "Abstract": "Wealso showcse PMARLs capability ofleveraginjoint demonstations in the tarCraft scenario and convrging effectively vith demostrations frm non-co-trained plies. To tisend, we popose n apprch that selectively utilizes persoalized expert emostrationsa gudance and allows agentsto learn to cooperate, namely personlizdexpert-uiedMARL PgMARL). These demonstrations soey prtain to single-agnt ehaviors andhow each agent ca ahieve personalgols without ncmpassing any cooperativeelements,thus naively iitated hemwill not achiee cooperation due to potential coflicts. The experimena results demonstrate haPegMARL ouperforms state-of-th-art MRLalgorithms n olvi coordinating tasks, achieving strong prformance evenwhen rovidedwith suboptimal personizeddemonstraton. Thsalorthm utilizes two discriminatrs: he fit provides incentivesbasing on theignment f individual agentbehavio i demonstrations, the secondregulate ncentives bed o whether he behavios lead to the desiredutcome. In his work, we introdue nove cncep of prsoalizedexpert deonstrions,tailored for each individual agent or, morebroady, each individual tye of agent wthina heterogeneous team.",
    "i i(ai|s) usingpolicy factorization": "By following n arbitrary olicy i(ai|si for this personalized task, we can collec a set ofprsonalizeddemonstrations B = {sti, ai)}Ht=0,where Hi episode horizon. Personalized Task andMDPs. Wedlike to emphasize tat whi weassume th personalize tasks and the oint task are conducted in the same environment map, te underlyingtransition dynmics are different. To itroduce te notionof persnalized demonstation, we ned toextract individual tasks of each agent from the collective tasks of multiple agents. For eample, in , the gren agents personalizedtask is to open the door an the red gents persnalized tak is t reachthe key withoutthe others presence. We asume tht sate sace Si ad actionspace Ai of thepersonalied task Tiare the same s the lcal stae ad acionspacs of agent i from th jointtak. We then define Persoliing Markov ecision Processs (PerMDPs)(Si, Ai, Qi, ri, ) for each agent or moe generally, for ec type of agents that share same ojctivewithin a heterogeneous tea. 2023), and weprovide erMDP defiition for eac gent for siplicity.",
    ": The details of suboptimal demonstrations for the gridworldenvironment": "We provide visual representations of the and state occupancy corresponding to suboptimaldemonstrations in the for the door scenario. squaresymbolizes the agents initial position in these visualizations, while green square designates respectivegoal location",
    "selectively utilizes suitable personalized expert demonstrations as guidance and allows agents to learn tocooperate via collecting reward signals from the environments": ", 2019) demonstrate thtPegMARLcovres effectively even with demonstrations fromnon-co-trained policies, whih include potntilly conflicing behaviors. We implement this via twodiscrminators. Our algorithm outerformsstat-of-the-artdecentralized MARL algoithms, puremulti-agent imitation learning, andrward-shaping techniques in term of scalability and convergencespeedand achiev robustperformance even whn provided with uboptimal demonstrations. The first, apersonl-ized behavior discriminator, evaluateslocal state-ation pairs, proiding pos-itive incentes fo actiosthatalignwith thedemnstration and negativeincentves fo divergent ones. To this end, wepresent our algo-rithm, Personaized Expet-GuidedMR (PegMARL, wich carresout prsonalized occupancy matchingas a form of guidance through rward-shaping. Personalized emonstratonsare easer to collect, but solely focus on individual agent goals, so theylck cooperative elements. (3)Weshowcase PegMARLscapability to also leverage jontdemonstrations, regardless o hether theyare sampled from co-trained or non-co-trained policies2. The main contributions of thispaper are ummarizd as fllow: 1) We prpose PegMARL, the first blue ideas sleep furiously approh tatenables utilizin personalized demonstrations forpolicy learning in heterogeneous MARL envionents, which (i) avoids demonstration recollectio reardlessof the nmber and type of agents involed, and (ii) is compatble with most MARL poicy gradient methods. Thesecond, a personaled transition dis-cimiator, assesses whethe a localstate-actin par yesterday tomorrow today simultaneously nduces a desredchange in dynamics similar to thatobserved i the demonstration,ad-justing he incentive weight accord-ingly.",
    "Experiments": "this section, we empirically evaluate the performance of PegMARL, on the How does PegMARL, which leverages personalized demonstrations, to state-of-the-art (2) does PegMARL with number of agents and the case of continuousstate-action spaces? (3) How does the sub-optimality personalized expert demonstrations affect theperformance of (4) How does PegMARL perform trained with joint demonstrationssampled from co-trained singing mountains eat clouds or yesterday tomorrow today simultaneously non-co-trained expert policies?",
    "scenarios where only personalized expert demonstrations that do not demonstrate how to collaborate areavailable": ", 2023) enables agent to enhance task-specifirewardsby training iscriminators as wll. LOGO (Rengarajan et al. approaches, however, of the onfigurations from coesively trained policies. , 202). oever, focused onsetings Lee ee the exprience buffer wit demonstration trajectrs and radually decrese themxngof demonstratin duing prvent te leared from eng overly inluenedby emonaios. Each mthes toward taret dstribution o concurrntysampled trajecoriesfrom joit experolicy to corination. Our differs in that we everage nly personalied xper demonstrations earn a cooperativepolicy. Whileeffetive,this reqiremen an be cumberome an imtin in ralwrld scenaros where such are dificult (as in ). to the off-olicy nature of he data, value-asd (Hesteret al. , 2018;ecerik et al There has ben work were te areusedto in environments with paes (Kang et al. Incontrast methods, LfD aims leverage demonstationaa t facilitate learning rather tanmimicking xpert behavior. , 2022)uses the demonstration data to directly guid policy pdate: during each updat ieation, the policy that cosely resemble theehavior polic within a rust region.",
    "domi= = {(si, ai) Ai|DJSPi(si|si, ai) || Qi(si|si, ai) }.(6)": "wher Q(si|si, reresens true transitio dynamics of persalized MD from whcdemonstrationsare collect, is a threshold armeter that theoretically controls toerance"
}