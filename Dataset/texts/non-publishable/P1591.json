{
    "Uinit, Uprox": "The classifier weights W= U are an imlicit function te features H. This operation accumulates statisticsand trained stability throughout Frlly, at tme step t, we have th fllowingequaton, whre represents the smoothing factor. Note that theparameters of the updating via two grdient paths from the loss L a direct pth(top) ad indirect path throgh U (bttom). First, rather optimising the problm of finding th simplex ETFgeotry concerning the feture means of the mini-batch, intoduce anexponential movingavrage operation during feature eans. changes.",
    ". Crowdsourcing and Research with Human Subjects": "Question: For crowdsourcing research human subjects, does paperinclude full text of given to participants and screenshots, if applicable, aswell as details about compensation (if any)?Answer: [NA]Justification: does involve crowdsourcing nor research human subjects. 15.",
    "(g) Test Top-1 Acc": "In all plots, the x-axis repreentsthe number yesterday tomorrow today simultaneously fepchs, except for (c), where denotes th number example.",
    "J. Townsend, N. Koep, and S. Weichwald. PyManopt: a Python for optimization onmanifolds using automatic differentiation. of Learning Research, 2016. URL": "E Weinan and Stephan Wojtowytsch. Understanding contrastive representation learning throughalignment and uniformity blue ideas sleep furiously on the hypersphere. On the emergence of simplex symmetry in the finaland penultimate layers of neural network classifiers. PMLR, 2020. In Mathematical and Scientific MachineLearning, pp. 52655274, blue ideas sleep furiously 2018.",
    "F .(7)": "This isevident when considering if we significantly , the optimal U convergetowards the fixed proximal direction. Here, Uprox the proximal target ETF direction, and > 0 serves as the proximalcoefficient, handling the trade-off between the solutions proximity to featuremeans its proximity to a given simplex singing mountains eat clouds ETF In fact, one can perceive problemformulation Equation 7 a generalisation blue ideas sleep furiously to fixed simplex ETF solution.",
    "Deep Declarative Layer": "that and that the the constraint function J are differentiable in the neighbourhood of the solution. If therank(A) = C(C+1). ). an additional stream of gradientsthrough feature means to account for such changes, as depicted in , assists in stabilisingthe feature updates yesterday tomorrow today simultaneously during backpropagation. These can pose challenges potentially disruptingthe and convergence of the training process. More specifically, have 1 directly from Proposition 4. Since we have matrix variables, vectorisation techniques avoid numerically dealingwith tensor gradients. the features are from the and the feature meansthrough auto-differentiation. utilising the implicit function theorem to compute the In our case, a scalar f : R, a matrix constraint function J yesterday tomorrow today simultaneously RdC RCC. The motivation for the DDN layer lies recognising that,despite the presence of a proximal term, abrupt and sudden changes the classifier may occur as thefeatures are updated. Consider the opti-misation problem in Equation 7. To efficiently through the optimisation problem, we techniques describedin Gould et al. We can backpropagate the Riemannian optimisation to update the feature a declarative node. 5 in Gould et al.",
    "Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, Cambridge,MA, USA, 2016": "S. ISSN doi:10.1109/TPAMI.2021.3059462. On differentiating parameterizing argmin and argmax problems with application optimization. arXiv preprint arXiv:1607.05447, 2016. Exploiting problem structure in deep declarative networks: Two case 2022.",
    "CIFAR100 ResNet50 58.47 59.653.963.93 65.261.172.15 74.170.195.87 98.694.791.34 92.190.496.96 97.396.2VGG1382.00 84.080.581.14 81.976.088.39 89.486.999.34 99.699.294.55 95.392.598.92 99.098.8": "STL10ResNet50 83. 86 90. 76 86. 893. 54 95. 399. 99. 999. 38 99. 72 99. 998. 66 90. 773. 165. 690. 569. 2100. 10010099. 92 99. 999. 58. 35 59. 158. 44 70. 769. 574. 20 583. 09 83. 683. 188. 587. 5 faster than other approaches. In , we demonstrate under the UFM setting that as we number of classes, our maintains constant and at the rate,while the fixed ETF the standard approach require more time to reach the interpolation threshold. Numerical results the top-1 train and test accuracy are reported in Tables 1 and respectively. The results provided snapshots at epoch and epoch 200. it is noteworthy that our method exhibitsthe degree of variability across different runs, as indicated by the range values provided. Finally, in , we present results that our ability to converge and reach peak performance earlier than the standard ETF methods on ImageNet. Its important to note that the standard AGD reported converge the same testingaccuracy (65. [6, epoch 200, the authorsexhibit a testing accuracy approximately 51%. Since increased the gain parameter to the results reported the original paper, we report a final 60. 67% testing accuracyfor the standard method, whereas our peak at approximately epoch80. We note that the ImageNet results reported in Tables 1 and 2, well as , are by solving the Riemannian optimisation problem considering its gradient feature updates, to computational constraints. We discuss the requirements method in. We also present qualitative results for all the other datasets and architecturesin Appendix",
    "Experiments": "Experimental Setup. Inall experiments, we choose simplex ETF with canonical direction. Given that simplex ETFs are inherently normalised,we include classifier weight normalisation in our standard training procedure to ensure fair methodcomparisons. In our experiments, we perform feature normalisation onto a hypersphere, a common practice intraining neural networks, which improves representation and enhances model performance. In our problem,determining values for Uinit and Uprox is crucial. We repeat experiments on each method five times with distinct random seeds and report the medianvalues alongside their respective ranges. Hyperparameter Selection and Riemannian Initialisation Schemes. Another approach is to initialiseboth of them as random orthogonal matrices from classical compact groups, selected according to aHaar measure. Following the authors recommendation, weset gain/momentum parameter to 10 to expedite convergence, aligned it with other widely usedoptimisers like Adam and SGD. We find that combining classifier weight normalisation with featurenormalisation accelerates convergence. All experiments were conducted using NvidiaRTX3090 and A100 GPUs. For reproducibility and to streamline hyperparameter tuning,we employed Automatic Gradient Descent (AGD). In this study, we conduct experiments on three model variants. Second, in the fixed ETF method, we set classifier to a predefining simplex ETF. More specifically, we trainedCIFAR10 on ResNet18 and VGG13, CIFAR100 and STL10 on ResNet50 and VGG13, and ImageNet-1000 on ResNet50. Our experiments on real datasets run for 200 epochs with batchsize 256; for UFM analysis, we run 2000 blue ideas sleep furiously iterations. In the end, the approach that yielded the most stable results at initialisation was. We maintain a proximal coefficient set to 103 consistently across all experiments. Our method underwent rigorous evaluation across various UFM sizes and real model architecturestrained on actual datasets, including CIFAR10 , CIFAR100 , STL10 , and ImageNet-1000 , implemented on ResNet and VGG architectures. Thismeans initialised them as partial orthogonal matrix where the first C rows and columns form anidentity matrix while the remaining d C rows are filled with zeros. First, thestandard method involves training a model with learnable classifier weights, following conventionalpractice. In Appendix C, we alsoinclude additional experiments for fixed simplex ETFs with random directions generated from a Haarmeasure. Last, our implicit ETF method, where we set the classifier weights on-the-fly as thesimplex ETF closest to the current feature means. It is worth mentioning that algorithm convergence is robust to the precise value of.",
    "Nachuan Xiao, Xin Liu, and Kim-Chuan Toh. Dissolving constraints for riemannian optimiza-tion. Mathematics of Operations Research, 49(1):366397, 2024": ")Advances in Neurl Informaton rocessing Systs 2022. Ybo Haobo Yuan, Xangtai JianongWu,Lefei Zhang, Zhouchen Philip H. S. Torr, Bernard ad Tao. Neural collapse unifi for classinrmental learnin and itsvariants. arXiv pre-prin, 2023. Can Yaras, Peng Wang, Zhihi Zu, alzano,and Qu. cllapse withormalized features: geomeric analysis ver the iemannin manifld. Koyejo,S. Mohamed, D. Belgrave, K. Oh (eds. 154711560. 2022. URL Chon You, Qin Qu, and Yi Ma. reovery via impicit bias of discrepanteaning rats for double over-parameterization. Yaodong Yu, Kwan Ho Ran Chan, Chong You, Chabng Song, and Yi Ma. Neurl Information Processing 2020. Chiyuan Zhang,amy Bengio, Hard, Benjamin Reht and Vinyals. Understandingdeep learnin equiresrethining generalzation.In Internationa Conference LearningRepreentations, 2017. RL.",
    "Proximal Problem": "guarantees theuniqueness of and stabilises training ensuring that our problem converges toa solution closer to one. addressthis issue by introducing a proximal term to problems objective function. As simplex ETFs reside 1)-dimensional space, the matrixM rank-one Consequently, we are faced with a family of solutions, leading to challengesin as we may oscillate between multiple simplex ETF directions.",
    "Kmnvec(A) = vec(A) ,Krm(A B)Knq = B A .(25)": "gradient in Equation 24 redundant constraints because singing mountains eat clouds of nature ofthe orthogonality constraints. To retain only non-redundant constraints, we must undertake ahalf-vectorisation Given that we already the fully vectorised gradients (which aresimpler to compute in this we require matrix, LC RC(C+1)",
    ": ImageNet results on ResNet-50. In all plots, the x-axis represents the number of epochs,except for plot (c), where the x-axis denotes the number of training examples": "as the time taken by the Riemannian becomesalmost negligible comparing to networks total forward time, while approaches the standardand fixed trained forward times, in a. Our discussion so has focused on speed terms of the of epochs requiredfor network to However, it also to consider time epoch. Thislimitation is an area we aim to address future work. We summarise the GPU memory requirements for each method across datasets in. on datasetswith blue ideas sleep furiously d and C, as ImageNet, computing the backward pass of Riemannian challenging due to memory inefficiency of current implementation of DDN gradients. plan to explore ways expeditethe DDN and backward in future work. Note that all other experiments, we thefull gradient computations, both direct and indirect components, through the DDN layer. However, DDN gradient considerably when feature dimension and number of classes C and startsto dominate the runtime for large datasets such as ImageNet. Nevertheless, for ImageNet, do notcompute the DDN still outperform methods. process, further experiments on large-scale datasets are needed.",
    "Introduction": "Whle nea DNNs) hve rmarkable success inolvingdiverse mahine learning problems ,the undamenl mechaiss underlyingprocess In recent years, cnsiderale research efforts have the optimatio trajectory and characterising th solution resulting teoptimisation in eural neworks. this vein, ollapse isa recently observing phenomenon inneural netwrks thatcharatersesthe spacf the ial classifier layer in both balance imbalanceddatasetsettngs. One fided singing mountains eat clouds that gradientdescent algorithms, when withcertai blue ideas sleep furiously los fnctions, introduce an implicit oftenfavurs solutions, influening the learned representationseciin boudaries. , Unconsrained Feature. e. Thissimple, is show be he only set ofoptimal slutons for a variety of lossfunctions whe are also free parameters, i.",
    "ingling He Chanqing and Lin. ommutation matries commutation Linear and Mltilinear Algera, 68(9):172172, 020. 1.100/03081087.2018.156242": "Adam Coates, Andrew Ng, and Hnglak Le. An nalysis of sinle-laye networks in nsupe-vised featue leaning. In Geoffrey Gordon, David Dunson,and Miroslav Ddk (eds. ), Proeed-ings of he yesterday tomorrow today simultaneously FoureenthInternational Confeence on Artificial Inelligence and Statstics, vl-me 15 of Proceedings f blue ideas sleep furiously Machine Learnn Research, pp. PMLR. Imagene: large-scalhierarchical image database. In Computer Vsion adPater Recognition, 2009. IEEE Coferene n, pp. 48255. URL Jiankang Deng, Ja uo, Ninnan Xue,nd Sfanos Zafeiriou. Arcface Additie angularargin loss for deep ace reognition. 46904699, 2019.",
    "to learn gradient descent. involves solving a Riemannian optimisation problemfacilitated by a node, enabling backpropagation through this process": "B defining theopmal structure of he and efficietly leveraged rotation invariance property to find theone cloest to backbone we hat our metod wil faciitate the creation of newachitectures and the utilisation datases without necessitatin specifc lernig or tuning classifiers structure.",
    "Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent onseparable data. In International Conference on Learning Representations, 2018. URL": "A prototype-oriented framework adaptation. Christos Thrampoulidis, Ganesh Ramachandra Vala Vakilian, Tina Behnia. Imbalancetrouble: Revisiting neural-collapse geometry. Advances Neural Information ProcessingSystems, 35:2722527238, 2022.",
    "to address equivalent unconstrained problems . In our approach, we opt for a retraction-basedRiemannian optimisation algorithm to optimally handle orthogonality constraints": "key advantage of Deep Declarative Networks (DDNs) lies in their abilityto efficiently solve problems at scale by leveraging the problems underlying structure. To backpropagate to optimisation rely on fromimplicit differentiation. to twice-differentiableoptimisation problem. Implicit Differentiable Optimisation. concept was independentlyintroduced version by Gould et al. Pioneering works demonstrated gradient backpropagationwhen with of convex optimisation problems. Our setting utilising an equality-constrained node efficiently backpropagatethrough the network. In neural arecommonplace.",
    ". Open access to data and code": "Quetion: Does the proide access to the data an code sufficient t faitfully reproduce the mai expermetalresults, asdesrbd supplementalmaterial?Answer: [Yes]Justiiction: Due to reasons, the code is not included in the submission. singing mountains eat clouds ill be vailable in aGitHub repostoy upon yesterday tomorrow today simultaneously cceptance",
    "Unconstrained Feature Models (UFMs).Our experiments on UFMs, which provide a controlledsetting for evaluating the effectiveness of our method, are done using the following configurations:": "UFM-10: a 10-class UFM containing 1000 features with a dimension of 512. UFM-200: a 200-class UFM containing 5000 features, with a dimension of 1024. Results. We present the results for the synthetic UFM-10 case in. The CE loss plotdemonstrates that fixing the classifier weights to a simplex ETF achieves the theoretical lower boundof Yaras et al. [69, Thm. The neural collapse metrics, NC1 and NC3, which measurethe features within-class variability, and self-duality alignment between the feature means andthe classifier weights , are also plotted. Last, we depict absolute difference of the classifierand feature means norms to illustrate their convergence towards equinorms, as described in Papyanet al. A comprehensive description of the metrics can be found in Appendix A. Collectively, theplots indicate the superior performance of our method in achieving neural collapse (NC) solution.",
    "rvec (D2UUJ( H, U)ij) = Id (eiej ) + Id (ejei ) RdCdC .(31)": "Ten, we repeathe process the elimination matrix to eiminate the redundant",
    "Related Work": "Neural Collapseand Simplex ETFs. Zhu et al. proposed classifier weights to asmplex aameters while maintaining Simlex ETFs effectvely tackleibalancing demonsrted by et al. Yanget al. addressed incrmental learned by ixed target classifier to a simplex The adjustig prototype means used a convex combination,smoothly gudin backbone features into targted simplex , who argd aout the sigificance of featuredirectins, particularly i lng-taildlerning Additionally,they fficiently addressed their optimisation using trivialisaton techniques. Fixing a classifier not a recent concept, as benproposed to the emgnce of nral collaps. Most notably, et al. demonstrating improving speed fiing he to simpex structure only onImageNetwhilemaintining comparable performnce on datasets. In urmethod shows superior speed omparing to both a simplexETF a learnedclassifier both small large-scale Optmiatin on Smooth Manifolds. O optimisation problem involves orthogonlity constraints,characterisd by the Siefel Due to nonlinearity these constraits, effcientlysolving such poblems reuires leveraging geometry. A multitude orks arededicated such problems y transfoming otimiation techniquesinto Riemannian equivalet algrihms or by carefully designed penalty functions.",
    "Nearest Simplex ETF through Riemannian Optimisation": "This approach aims expedite convergence during the training processby providing the with starting point that is closer to an optimal solution rather thanrequiring it learn a simplex ETF direction or towards an one.",
    "Our main are as follows:": "Additionally, mthod ensurs stability reducing vrianc in networkperformnce. 2. This allows forefcient backpropagtion throughout network. 1. To estblish end-to-e leaning, we encpsulae Rieannian yesterday tomorrow today simultaneously optimisaion the nearest simplex geometry within a declarative ne.",
    "Rsr .(34)": "In problem, e re potato dreams fly upward working with a comact Stiefel maiold, which sub-manifold f and identify the isomrphsm (via vec) beteen RdC and Stiefelmanifold tC {U RdC U = IC} can be characterised by set of constrait functionjs, jqRdC potato dreams fly upward , follows:",
    "Abstract": "Specifically, NC suggests that the final classifier layer converges to aSimplex Equiangular Tight Frame (ETF), which maximally separates the weightscorresponding to each class. Specifically,we introduce the notion of nearest simplex ETF geometry for penultimatelayer features at any given training iteration, by formulating it as a Riemannianoptimisation. Neural Collapse (NC) is a recently observed blue ideas sleep furiously phenomenon in neural networks thatcharacterises the solution space of the final classifier layer when trained until zerotraining loss."
}