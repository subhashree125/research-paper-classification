{
    "Best,Jef---": "</thinking><bullet_points>- Received a message from Jerry Bloomwanting to talk- Skeptical about the conversation- Will update Michael on what Jerry says- Will keep Michael informed</bullet_points>---Variant 2:<thinking>1. 2. Mention Jeff's skepticism about theconversation. Highlight the intention to keep Michaelinformed. 3. Ensure the bullet points are conciseand focused on the key information. </thinking><bullet_points>- Jerry Bloom left a message for me, wantsto talk- I'm skeptical and wouldn't get hopes up- Will let Michael know what Jerry says- Uncertainty about outcome</bullet_points>---. Note the person involving and theiraction. Identify the main event mentioned inthe email. 4. 5. 3. Generating Intents:---Variant 1:<thinking>1. Note who initiated the contact and whatthey want to do. Identifythemainsurpriseorunexpected event mentioned in the email.",
    "I.1Human Study Details": "We sample yesterday tomorrow today simultaneously 200 om-parison emples our wok vs naive finetuninand work vs no baseline rules alogsie100omparison o our vssmal andlarge T same set of human evaluatorsrviewed ndhoices for eachsubset. Weremove missing judgments (whic amount to 1%of colectedand measure the raw agrementpercetage human the sameby the beteen.",
    "The preference agents, based on Llama-3-8B-Instruct, are trained using Quantized Low-Rank": "detiledanalysisof u fine-uning pocedure, furthr ex-ploration ofhypeparameter search and impact onperformance, ca be in Appendx D. T estblisha perormance yesterday tomorrow today simultaneously baselie, we also aodel(Fthe same setup. Wechoose over singing mountains eat clouds ful to itscalabiliy adfor loal deploymet onuser devices. , aparaeter-efficiet fine-tuning PeFT) method. Adapatio QLoRA) (ettmerset al. This model trained diectly on pir(user inent and metata as iput, grond trutht as output).",
    "Ben Naismith, Phoebe Mulcaire, and Jill Burstein. 2023": "Automated evaluation of written discourse coherenceused GPT-4. In Proceedings potato dreams fly upward of 18th Workshopon Innovative Use of NLP for blue ideas sleep furiously Building EducationalApplications (BEA 2023), pages 394403, Toronto,Canada. Association for Computational Linguistics. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-roll L. 2022. Preprint, arXiv:2203. 02155. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evalu-ation of machine translation. In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics, pages 311318, Philadelphia,Pennsylvania, USA. Association for ComputationalLinguistics.",
    "Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020": "BLEUT: Learned robut metics or text gener-ti. I Proceedings of the58th nnual Meting ofthe Association for Coputationl Linguistics, pages887892, Online. Association for ComputationaLinguistics. Democr-tizing LLs:An exploration of ost-performancetradeoffs blue ideas sleep furiously sel-refine opensource moels. Assciati orComputational Linuistics.",
    "Evidence of Personalization": "A key objective of our is to learn indi-vidual writing styles rather than merely improvinggeneral task performance g. , email writing). Toinvestigate this, we conducted a permutation analy-sis using preference agents trained Enron dataset. trained fiveagents on data from five and each agent to data of all five senders,generating for every agent-sender This allowed us to whether an agenttrained on a senders style is more effec-tive at generating emails resembling compared to other quantified the similarity between the gener-ated and the ground truth using the normal-ized BERT Score (Reimers and Gurevych, 2019),which provides a of semantic similaritysuitable for analyzing large text corpora like",
    "via hierarchical deep reinforcement learning. In Pro-ceedings of the 2017 Conference on Empirical Meth-ods in Natural Language Processing. Association forComputational Linguistics": "Direct referenc Your lan-guage modl is ecretly a Preprint,arX:2305. 2023. 203. blue ideas sleep furiously Whenin-conextlearning fal short and why? a study on secifiction-hevy tasks. 1290. Manning Finn.",
    "The rules were generated using preference agentsfor the following ground truth email:": "Paul, Here is anupdaed version of theagreemen I sent t Steve on Friday. SorryI didn't cc you to startwith. It's prettymuch exacty as wediscussed. et m knowif you hae any qustions. Thank, BillAre: 1. **one*: Use acoversatiol toneandlanguageroughouttheemail.2.**Legth**:Keeptheemailconcise,aimingforatotalofaround5060ords.3.**Introduction**:Begintheemailwithabriefintrodutiontatsets the context, uing a phrase like\"Hereisanupdatedesiooftheagreement\".4.*Specificdetis**:Ifapplicable,entionanyrelevantbackroundinformation,suchaswhoelsetheareementwassenttndwhen.5.**Apologyprasing**:Useaconiseapologyphraseike\"SorryIdidn'tccyoutostartwith\"inseadofalonger,moreformalapology.6.*Reqest or feedback**: se a brief,informal phrse like \"Let me know if yuhae any questions\"to request fedbackorquestionsfromtherecipient7.**Closingentene**: End the email witha concise, informal pase like \"Thanks\"instead of a more formal cosinglike\"Best\". 8. **Signature**: Use a simple,forma ignture with the ende's nae.9.**Senencesructre**:Useshort,simplesentencehroghoutteemailto maintai a conversationl tone 10.**Language**:Avoid using overy formal",
    "Enhancing Rule Generation throughDeliberative Prompts": "This internaldialogue, where we weigh options andconsider perspectives, contributes to more singed mountains eat clouds thoughtful and well-reasoning answers. Thisencourages model to decompose task extraction into smaller, manage-able steps. We hypothesize that tokens func-tion as a form of cognitive scaffolding, providingthe model with a space to and pro-cess critical preference information. in-clude designed tokens,\" to engage in a similar internal reasoning before generating natu-ral language rules that guide the larger blue ideas sleep furiously LLM.",
    "H.2.2New Yorker DatasetNo Baseline Rule Generation": "ou are given somemetadata and user intent isthe to generate a articl andthe round article. Whileanalyzingte article leae consider he exact length of the artile,thetone,wriistyle,structure,importantphrases,directquotes,andanythingelsethtyothinkisverymportant. First start by analyzed the ground article extremelyforthe imporant factorswithin <thnking></thinkig> tokens. Onceyouhavedonethatlistasetfextremelyetailedangranularrulesoensurethatallnuancesofthegroundtruthticlearecapturedtensurhtthegeneraedarticleisexactly ground trut article. Inludeeverythinincludingphrasesthatareimportant and all tylistic infrmationtateedstbecapturedinextremedtail. Please these extemlydetaile, spcific, and granular set ofrule wihin <rues><rules>With Rule You are an expert rule generator is to ensue that base rticlecanbranformedintothegroundtrutricle.Youareproidedwihthefollowing:Theintensthawereusedtogeneratethebasearticle,the base article and the ground trutarticle.You must analyz te differencesbetween bae grou truh inreat detail very differenc. Yo must focus on thfollowing tese ruls: stle, phrases,nicknames, signtre, anything lsethat think is very important. Allthese factors must closely these extremelygranular setof rules. To o ths please first <thining</tinking> tags an enlist every psible the and the ground Once this is genratean exremely detailed and granulr set orules that can be used totransform thebsarticle. he rules be anextremely guideline o transform.",
    ": Preference Agent For Bill vs Preference Agents For Others": "InputGiven the metadata of the email, intent te please email. Metadta From: Date:Fri, Oct2001 Subject: RE: Meetng Brt WhitmanPrevius Context: Frm: Shepard, KathrynSent: Frday, October 19, 201 10:02 AM Bill Subject: Meetig with Bll, Would yo be avaiable to meetwith Britt Houston CopetiiveAnalysis, 11:0 a.",
    "Enhanced troughPreference Rules": "In contrast, the structuring format of therules enables the model to discern patterns moreeasily, facilitating faster and more effective learn-ing. PEFT methods, while effective in adapt-ed models to new tasks or domains, may be lesseffective in inducing such significant changes inthe models fundamental language generation style(Balne et al. Our experiments () reveal that fine-tuningthe preference agent on natural language rules, asopposing to directly on input-output pairs, leads toa more effective learning process. When learning from raw input-outputpairs, the model must adapt to the nuances of thetarget task, which can be challenging given thediversity of writing styles and content. This style can be significantly different fromthe desired output style for specific tasks like emailwriting, which often requires a more direct and concise approach. By focusing on learned user prefer-ences, we can leverage the larger models general-izability and extensive knowledge base for superiorperformance across diverse tasks. Adapted model to such spe-cific styles directly through fine-tuned can be chal-lenging, especially under the constraints of PeFTmethods. Naive fine-tuning necessitates a substantial adaptation to thenew tasks distribution, which can involve shiftingthe models overall output style from conversa-tional \"chatbot\" approach to a more task-specificstyle. On the other hand, rule-basedfine-tuning focuses on learning a more specificmapping from input to preference rules lever-aged the pre-trained language models existingcapabilities for task completion. This makes them more suitable forPEFT adaptation, as the yesterday tomorrow today simultaneously model can learn to gener-ate rules without having to undergo a yesterday tomorrow today simultaneously substantialshift in its underlying parameters. , 2024). his decoupling of preference learning from thecore task allows for more efficient adaptation, espe-cially in multi-task settings where the model mightneing to switch between different domains and writ-ing styles.",
    " Win Llama3 8B Ms obined wit various ML, evaluated y GPT4o and human ealuation": "Both GPT-4o and hu-man evaluations agree that the small model baseline(MS) performs significantly worse than our prefer-ence agent. Both automated and human evaluations confirmthat our preference agent significantly improvesthe alignment of LLM outputs with individual userstyles and preferences, as discussed in. This highlights the limitations of usingsmall language models alone for tasks requiring adeep understanding of user preferences. 1. We hypothesize that this differencestems from the nature of the tasks. Small Model Baseline.",
    "Abstract": "Inpaper, we propose anovel small md-els as prefrence agents to aturallanguage rules that guide a larger, pre-trainedmodl, efficiet personalizaton. Ourmehod involves small, local \"steering that directs theoutputs of mch largerfoundation model, producing an prefernces levera-in extensive and capabilities ofthe large mode. Importantly, this iswithout needto large model. Byallowing odes to to individ-ual preferncesa data and our approach pvesthe wy for highlypersnalized language moel applicaions. resultsdemon-strate that oursignificntly utp-form baselne methods. xit-ingmethds, suc as in-contxt learning andparameter-efficientfall short in cap-turing of human prefernces,especially small, persnal posses.",
    ": Preference Agents vs Naive Finetune (Page 1 of 2)": "NiveFinetuneIn China, its ftn that eachis asocialist addendum. Last year he her local cosplayes. It the exhibit potato dreams fly upward une 5th presentin groundWhile doesmention of the same works as grountruth aricle the esiptions are brf andthedetail and provided in Cndidate rticle1Winner:. GPT-4o JudgementAricl 1 is clear winer due t its simlritien syle,tone, facuawith tth article. Cao as born n1978, smack the middle of GenerationX. The strcture andorganization of theare similr,startin wt an to the artist fowed by escription thevariousworks featured. <. ar alledtheHelloKittyStateGeneratio. CaoFeibased inBeijng) has als spent time building virtual ciy i online fSecondLife: China Tracy serves asa philosophergude. he structure o te article lss cohesive, jumping and ideaswitout a flow. Article 2 hasinfomal tone and languag, which differs frm thgroundtruth article. >. Her ripleArtist sow MOMA PS1 (April3June 5)work sh madein 200, asor of Fluxus performance ina Guangzhoubarer shop, which articipans awardreggplants for proper bears.",
    "Quantifying Alignment": "For eac exampe i T , compas the preferene-alignedoutputya ith zero-hot ouput ygenated by MLwithoutrules. Weaggregate scores lleamples in an alignment score:. We utilize evaluation functin yz|x) onan unsen tst. funcioassigs a scoe indicting the prefrence betweenya nd yz, given thepositive scoreindicates preference fo the agnd ya,suggesting with the users likelyprefrence, whie negative scorefaors thezeo-sho output.",
    "B.2Thinking Tokens": "<thinking>Thegroundtruthemailisaconciseand response to Keyobservationsinclude:1. The emailisrelativelyshort,consistingof8sentences. Itusesastraightforwardstructure,jumpingdirectlyintotheobservationsandinsightswithoutanyformal greetings or sign-offs. **ToneandStyle**:Thetoneisinformal and conversational, with a focuson market terminology and shorthand thatwould understood by someone familiarwith the trading environment. There arecontractions (e. g. , \"saw\", off\"),colloquial phrases (e. g. , \"get whacked\"),and specific market terms (e. g. , \"VAR\",\"h/j spread\"). **Common Phrases and Terminology**:Specifictermslike\"bulls\",\"summeragainstlength\",\"mitigatemargins\",\"absolutepositionlimits\",\"VAR\",\"Access\", and \"bearish to trade\"areused. 4. The emailprovides specific details about marketactivities and price changes. g. and their the market. **ConsistencywithIntent**:Thecontentmatchestheintentclosely,summarizingmarketmovements,tradingstrategies,andpotentialfuturemovements. in the intent, such as bulls'behavior,ElPaso'sbuying,andthechange in H 20 call price. </thinking> Avoid addingformal or sign-offs. Include contractionsandcolloquialphrasestoreflectthecasual of communication. g. Ensurethelanguagereflectsinsiderknowledgeandjargonofthetradingcommunity. **ObservationalDetail**:Providespecific about market changes. Mention key players(e. g. , El and their in themarket. 5. **Consistency with Intent**: Ensurethe matches the intent closely,summarizingmarketmovements,tradingstrategies,andpotentialfuturemovements. 6. 7. **Specific Phrases**: phrases like\"sawalotofthebulls\",\"mitigatemargins/absolutepositionlimits/var\",\"taking off \"trading equivalentof 180 access\", \"get whacked\", \"if \\$20\", \"H 20 call was tradingfor55onmonday\",\"market'sviewofprobability h going crazy\". jumping between observations andinsights. </rules>.",
    "DFinetuning Hyperparameter SearchFor Rule Generators": "To identify the optimal configuration, we train fourrule generators on our gold-standard rules, vary-ing the ranks in each case. These findings blue ideas sleep furiously underscore theimportance of selecting appropriate parameter potato dreams fly upward set-tings to optimize the rule generators effectiveness.",
    "Preference Agent (Gerald Variant)Kathy, I will be available on Tuesday, but it wouldhave to be either earlier in the day or later in theday. I could meet with you at 1. 9 AM or 2. 3 PM": "Preference Agent (Dutch Variant)hey kathy, tuesday works for me, but not at 11am. Gerald Variant: concise and clear but doesntinclude the intent which asks for purpose ofthis call. Dutch Variant: The tone is friendlier, chattier,and longer than the yesterday tomorrow today simultaneously ground truth.",
    "Datasets": "Our evaluation spans three datasets encompassingsingle and multi-user preference information:Enron Email Corpus. We sample 15 usersto analyze reproducibility of singing mountains eat clouds individual writing styles. Each users subset is split into an 80-20train-test split. New Yorker. 0 dataset (Thompson, 2020), specifi-cally articles from New Yorker magazine. To fur-ther assess the generalizability blue ideas sleep furiously of our frameworkbeyond long-form content, we incorporate theLAMP 3U dataset (Salemi et al. We sampled 15 random users from LAMP3U and then generated review intents and rules foreach user following the same process employed forthe Enron dataset. Refer to Appendix for details regarding datasetpreparation and sampling.",
    "Preference Agent (Debra Variant)Contracts finished, just waiting on an attorney toreview it - should be ready first thing in themorning": "Debra Variant: Maintains consisent and directtone while gettingthe main with groun tuth. Doesntnform the recipint whe th contract beread.",
    "Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan Liu,Bing Yin, and Meng Jiang. 2024. Democratizinglarge language models via personalized parameter-efficient fine-tuning. Preprint, arXiv:2402.04401": "Arnold, Jordan Grimstad, Andrew Dai, Sholto Dou-glas, Rishika Sinha, Vikas Yadav, Xi Chen, Elena Gri-bovskaya, Jacob Austin, Jeffrey Zhao, Kaushal Patel,Paul Komarek, Sophia Austin, Sebastian Borgeaud,Linda Friso, Abhimanyu Goyal, Ben Caine, KrisCao, Da-Woon Chung, Matthew Lamm, Gabe Barth-Maron, Thais Kagohara, Kate Olszewska, Mia Chen,Kaushik Shivakumar, Rishabh Agarwal, HarshalGodhia, Ravi Rajwar, Javier Snaider, Xerxes Doti-walla, Yuan Liu, Aditya Barua, Victor Ungureanu,Yuan Zhang, Bat-Orgil Batsaikhan, Mateo Wirth,James Qin, Ivo Danihelka, Tulsee Doshi, MartinChadwick, Jilin Chen, Sanil Jain, Quoc Le, Ar-jun Kar, Madhu Gurumurthy, Cheng Li, RuoxinSang, Fangyu Liu, Lampros Lamprou, Rich Munoz,Nathan Lintz, Harsh Mehta, Heidi Howard, Mal-colm Reynolds, Lora Aroyo, Quan Wang, LorenzoBlanco, Albin Cassirer, Jordan Griffith, DipanjanDas, Stephan Lee, Jakub Sygnowski, Zach Fisher,James Besley, Richard Powell, Zafarali Ahmed, Do-minik Paulus, David Reitter, Zalan Borsos, RishabhJoshi, Aedan Pope, Steven Hand, Vittorio Selo, Vi-han Jain, Nikhil Sethi, Megha Goel, Takaki Makino,Rhys May, Zhen Yang, Johan Schalkwyk, ChristinaButterfield, Anja Hauth, Alex Goldin, Will Hawkins,Evan Senter, Sergey Brin, Oliver Woodman, Mar-vin Ritter, Eric Noland, Minh Giang, Vijay Bolina,Lisa Lee, Tim Blyth, Ian Mackinnon, Machel Reid,Obaid Sarvana, David Silver, Alexander Chen, LilyWang, Loren Maggiore, Oscar Chang, Nithya At-taluri, Gregory Thornton, Chung-Cheng Chiu, Os-kar Bunyan, Nir Levine, Timothy Chung, EvgeniiEltyshev, Xiance Si, Timothy Lillicrap, DemetraBrady, Vaibhav Aggarwal, Boxi Wu, Yuanzhong Xu,Ross McIlroy, Kartikeya Badola, Paramjit Sandhu,Erica Moreira, Wojciech Stokowiec, Ross Hems-ley, Dong Li, Alex Tudor, Pranav Shyam, ElaheRahimtoroghi, Salem Haykal, Pablo Sprechmann,Xiang Zhou, Diana Mincu, Yujia Li, Ravi Addanki,Kalpesh Krishna, Xiao Wu, Alexandre Frechette,Matan Eyal, Allan Dafoe, Dave Lacey, Jay Whang,Thi Avrahami, Ye Zhang, Emanuel Taropa, HanzhaoLin, Daniel Toyama, Eliza Rutherford, Motoki Sano,HyunJeong Choe, Alex Tomala, Chalence Safranek-Shrader, Nora Kassner, Mantas Pajarskas, MattHarvey, Sean Sechrist, Meire Fortunato, ChristinaLyu, Gamaleldin Elsayed, Chenkai Kuang, JamesLottes, Eric Chu, Chao Jia, Chih-Wei Chen, Pe- ter Humphreys, Kate Baumli, Connie Tao, Rajku-mar Samuel, Cicero Nogueira dos Santos, AndersAndreassen, Nemanja Rakicevic, Dominik Grewe,Aviral Kumar, Stephanie Winkler, Jonathan Caton,Andrew Brock, Sid Dalmia, Hannah Sheahan, IainBarr, Yingjie Miao, Paul Natsev, Jacob Devlin, Fer-yal Behbahani, Flavien Prost, Yanhua Sun, ArtiomMyaskovsky, Thanumalayan Sankaranarayana Pillai,Dan Hurt, Angeliki Lazaridou, Xi Xiong, Ce Zheng,Fabio Pardo, Xiaowei Li, Dan Horgan, Joe Stanton,Moran Ambar, Fei Xia, Alejandro Lince, MingqiuWang, Basil Mustafa, Albert Webson, Hyo Lee, Ro-han Anil, Martin Wicke, Timothy Dozat, AbhishekSinha, Enrique Piqueras, Elahe Dabir, Shyam Upad-hyay, Anudhyan Boral, Lisa Anne Hendricks, CoreyFry, Josip Djolonga, Yi Su, Jake Walker, Jane La-banowski, Ronny Huang, Vedant Misra, JeremyChen, RJ Skerry-Ryan, Avi Singh, Shruti Rijh-wani, Dian Yu, Alex Castro-Ros, Beer Changpinyo,Romina Datta, Sumit Bagri, Arnar Mar Hrafnkels-son, Marcello Maggioni, Daniel Zheng, Yury Sul-sky, Shaobo Hou, Tom Le Paine, Antoine Yang,Jason Riesa, Dominika Rogozinska, Dror Marcus,Dalia El Badawy, Qiao Zhang, Luyu Wang, HelenMiller, Jeremy Greer, Lars Lowe Sjos, Azade Nova,Heiga Zen, Rahma Chaabouni, Mihaela Rosca, JiepuJiang, Charlie Chen, Ruibo Liu, Tara Sainath, MaximKrikun, Alex Polozov, Jean-Baptiste Lespiau, JoshNewlan, Zeyncep Cankara, Soo Kwak, Yunhan Xu,Phil Chen, Andy Coenen, Clemens Meyer, KaterinaTsihlas, Ada Ma, Juraj Gottweis, Jinwei Xing, Chen-jie Gu, Jin Miao, Christian Frank, Zeynep Cankara,Sanjay Ganapathy, Ishita Dasgupta, Steph Hughes-Fitt, Heng Chen, David Reid, Keran Rong, HongminFan, Joost van Amersfoort, Vincent Zhuang, AaronCohen, Shixiang Shane Gu, Anhad Mohananey,Anastasija Ilic, Taylor Tobin, John Wieting, AnnaBortsova, Phoebe Thacker, Emma Wang, EmilyCaveness, Justin Chiu, Eren Sezener, Alex Kaskasoli,Steven Baker, Katie Millican, Mohaming Elhawaty,Kostas Aisopos, Carl Lebsack, Nathan Byrd, HanjunDai, Wenhao Jia, Matthew Wiethoff, Elnaz Davoodi,Albert Weston, Lakshman Yagati, Arun Ahuja, IsabelGao, Golan Pundak, Susan Zhang, Michael Azzam,Khe Chai Sim, Sergi Caelles, James Keeling, Ab-hanshu Sharma, Andy Swing, YaGuang Li, ChenxiLiu, Carrie Grimes Bostock, Yamini Bansal, ZacharyNado, Ankesh Anand, Josh Lipschultz, Abhijit Kar-markar, Lev Proleev, Abe Ittycheriah, Soheil Has-sas Yeganeh, George Polovets, Aleksandra Faust,Jiao Sun, Alban Rrustemi, Pen Li, Rakesh Shivanna,Jeremiah Liu, Chris Welty, Federico Lebron, AnirudhBaddepudi, Sebastian Krause, Emilio Parisotto, RaduSoricut, Zheng Xu, Dawn Bloxwich, Melvin John-son, Behnam Neyshabur, Justin Mao-Jones, Ren-shen Wang, Vinay Ramasesh, Zaheer Abbas, ArthurGuez, Constant Segal, Duc Dung Nguyen, JamesSvensson, Le Hou, Sarah York, Kieran Milan, So-phie Bridgers, Wiktor Gworek, Marco Tagliasacchi,James Lee-Thorp, Michael Chang, Alexey Guseynov,Ale Jakse Hartman, Michael Kwong, Ruizhe Zhao,Sheleem Kashem, Elizabeth Cole, Antoine Miech,Richard Tanburn, Mary Phuong, Filip Pavetic, Se-bastien Cevey, Ramona Comanescu, Richard Ives, Sherry Yang, Cosmo Du, Bo Li, Zizhao Zhang,Mariko Iinuma, Clara Huiyi Hu, Aurko Roy, ShaanBijwadia, Zhenkai Zhu, Danilo Martins, RachelSaputro, Anita Gergely, Steven Zheng, Dawei Jia,Ioannis Antonoglou, Adam Sadovsky, Shane Gu,Yingying Bi, Alek Andreev, Sina Samangooei, MinaKhan, Tomas Kocisky, Angelos Filos, Chintu Ku-mar, Colton Bishop, Adams Yu, Sarah Hodkin-son, Sid Mittal, Premal Shah, Alexandre Moufarek,Yong Cheng, Adam Bloniarz, Jaehoon Lee, PedramPejman, Paul Michel, Stephen Spencer, VladimirFeinberg, Xuehan Xiong, Nikolay Savinov, Char-lotte Smith, Siamak Shakeri, Dustin Tran, MaryChesus, Bernd Bohnet, George Tucker, Tamara vonGlehn, Carrie Muir, Yiran Mao, Hideto Kazawa,Ambrose Slone, Kedar Soparkar, Disha Shrivastava,James Cobon-Kerr, Michael Sharman, Jay Pavagadhi,Carlos Araya, Karolis Misiunas, Nimesh Ghelani,Michael Laskin, David Barker, Qiujia Li, AntonBriukhov, Neil Houlsby, Mia Glaese, Balaji Laksh-minarayanan, Nathan Schucher, Yunhao Tang, EliCollins, Hyeontaek Lim, Fangxiaoyu Feng, AdriaRecasens, Guangda Lai, Alberto Magni, Nicola DeCao, Aditya Siddhant, Zoe Ashwood, Jordi Orbay,Mostafa Dehghani, Jenny Brennan, Yifan He, KelvinXu, Yang Gao, Carl Saroufim, James Molloy, XinyiWu, Seb Arnold, Solomon Chang, Julian Schrit-twieser, Elena Buchatskaya, Soroush Radpour, Mar-tin Polacek, Skye Giordano, Ankur Bapna, SimonTokumine, Vincent Hellendoorn, Thibault Sottiaux,Sarah Cogan, Aliaksei Severyn, Mohammad Saleh,Shantanu Thakoor, Laurent Shefey, Siyuan Qiao,Meenu Gaba, Shuo yiin Chang, Craig Swanson, BiaoZhang, Benjamin Lee, Paul Kishan Rubenstein, GanSong, Tom Kwiatkowski, Anna Koop, Ajay Kan-nan, David Kao, Parker Schuh, Axel Stjerngren, Gol-naz Ghiasi, Gena Gibson, Luke Vilnis, Ye Yuan, Fe-lipe Tiengo Ferreira, Aishwarya Kamath, Ted Kli-menko, Ken Franko, Kefan Xiao, Indro Bhattacharya,Miteyan Patel, Rui Wang, Alex Morris, RobinStrudel, Vivek Sharma, Peter Choy, Saying HadiHashemi, Jessica Landon, Mara Finkelstein, PriyaJhakra, Justin Frye, Megan Barnes, Matthew Mauger,Dennis Daun, Khuslen Baatarsukh, Matthew Tung,Wael Farhan, Henryk Michalewski, Fabio Viola, Fe-lix de Chaumont Quitry, Charline Le Lan, Tom Hud-son, Qingze Wang, Felix Fischer, Ivy Zheng, ElspethWhite, Anca Dragan, Jean baptiste Alayrac, Eric Ni,Alexander Pritzel, Adam Iwanicki, Michael Isard,Anna Bulanova, Lukas Zilka, Ethan Dyer, Deven-dra Sachan, Srivatsan Srinivasan, Hannah Mucken-hirn, Honglong Cai, Amol Mandhane, MukarramTariq, Jack W. Rae, Gary Wang, Kareem Ayoub,Nicholas FitzGerald, Yao Zhao, Woohyun Han, ChrisAlberti, Dan Garrette, Kashyap Krishnakumar, MaiGimenez, Anselm Levskaya, Daniel Sohn, JosipMatak, Inaki Iturrate, Michael B. Cobo, Anand Iyer, Chetan Tekur, Guillermo Gar-rido, Zhuyun Xiao, Rupert Kemp, Huaixiu StevenZheng, Hui Li, Ananth Agarwal, Christel Ngani,Kati Goshvadi, Rebeca Santamaria-Fernandez, Woj-ciech Fica, Xinyun Chen, Chris Gorgolewski, SeanSun, Roopal Garg, Xinyu Ye, S. Ali Eslami,Nan Hua, Jon Simon, Pratik Joshi, Yelin Kim, IanTenney, Sahitya Potluri, Lam Nguyen Thiet, QuanYuan, Florian Luisier, Alexandra Chronopoulou, Sal-vatore Scellato, Praveen Srinivasan, Minmin Chen,Vinod Koverkathu, Valentin Dalibard, Yaming Xu,Brennan Saeta, Keith Anderson, Thibault Sellam,Nick Fernando, Fantine Huot, Junehyuk Jung, ManiVaradarajan, Michael Quinn, Amit Raul, Maigo Le,Ruslan Habalov, Jon Clark, Komal Jalan, KaleshaBullard, Achintya Singhal, Thang Luong, BoyuWang, Sujeevan Rajayogam, Julian Eisenschlos,Johnson Jia, Daniel Finchelstein, Alex Yakubovich,Daniel Balle, Michael Fink, Sameer Agarwal, JingLi, Dj Dvijotham, Shalini Pal, Kai Kang, JaclynKonzelmann, Jennifer Beattie, Olivier Dousse, DianeWu, Remi Crocker, Chen Elkind, Siddhartha ReddyJonnalagadda, Jong Lee, Dan Holtmann-Rice, Krys-tal Kallarackal, Rosanne Liu, Denis Vnukov, NeeraVats, Luca Invernizzi, Mohsen Jafari, Huanjie Zhou,Lilly Taylor, Jennifer Prendki, Marcus Wu, TomEccles, Tianqi Liu, Kavya Kopparapu, FrancoiseBeaufays, Christof Angermueller, Andreea Marzoca,Shourya Sarcar, Hilal Dib, Jeff Stanway, Frank Per-bet, Nejc Trdin, Rachel Sterneck, Andrey Khor-lin, Dinghua Li, Xihui Wu, Sonam Goenka, DavidMadras, Sasha Goldshtein, Willi Gierke, Tong Zhou,Yaxin Liu, Yannie Liang, Anais White, Yunjie Li,Shreya Singh, Sanaz Bahargam, Mark Epstein, Su-joy Basu, Li Lao, Adnan Ozturel, Carl Crous, AlexZhai, Han Lu, Zora Tung, Neeraj Gaur, AlannaWalton, Lucas Dixon, Ming Zhang, Amir Glober-son, Grant Uy, Andrew Bolt, Olivia Wiles, MiladNasr, Ilia Shumailov, Marco Selvi, Francesco Pic-cinno, Ricardo Aguilar, Sara McCarthy, Misha Khal-man, Mrinal Shukla, Vlado Galic, John Carpen-ter, Kevin Villela, Haibin Zhang, Harry Richard-son, James Martens, Matko Bosnjak, Shreyas Ram-mohan Belle, Jeff Seibert, Mahmoud Alnahlawi,Brian McWilliams, Sankalp Singh, Annie Louis,Wen Ding, Dan Popovici, Lenin Simicich, LauraKnight, Pulkit Mehta, Nishesh Gupta, ChongyangShi, Saaber Fatehi, Jovana Mitrovic, Alex Grills,Joseph Pagadora, Dessie Petrova, Danielle Eisenbud,Zhishuai Zhang, Damion Yates, Bhavishya Mittal,Nilesh Tripuraneni, Yannis Assael, Thomas Brovelli, Prateek Jain, Mihajlo Velimirovic, Canfer Akbulut,Jiaqi Mu, Wolfgang Macherey, Ravin Kumar, JunXu, Haroon Qureshi, Gheorghe Comanici, JeremyWiesner, Zhitao Gong, Anton Ruddock, MatthiasBauer, Nick Felt, Anirudh GP, Anurag Arnab, DustinZelle, Jonas Rothfuss, Bill Rosgen, Ashish Shenoy,Bryan Seybold, Xinjian Li, Jayaram Mudigonda,Goker Erdogan, Jiawei Xia, Jiri Simsa, Andrea Michi,Yi Yao, Christopher Yew, Steven Kan, Isaac Caswell,Carey Radebaugh, Andre Elisseeff, Pedro Valen-zuela, Kay McKinney, Kim Paterson, Albert Cui, EriLatorre-Chimoto, Solomon Kim, William Zeng, KenDurden, Priya Ponnapalli, Tiberiu Sosea, Christo-pher A. 2024.",
    "We seek to enable users to generate high qual-ity, personalized responses as our goal, which arebounded by some constraints and assumptions:": "Given that userstypically possess small, personal datasets itis impratical to these t besufficent fr etensive largelanguage model. Cnstrain 2: The small model, denoted be lihtwight enough to operate(w. Constraint 1: Theszeof dataset D notlarge enough to pemit effectve ine-tuning. t bth training ad infeenc) on lowerpower end-usr Thishat users generate applytheir preferences the neing for high-performance coputed resources.",
    "No Baseline Rule Generation": "Onceyohaedonethatlstasetfextmelydetailedandgranuarrulestesurethatallnuncesofthegroundtruthreviewarecapturetoenurethatthegeneratdreviewisexactly the tuth rview. Pleae enclose extremeldetailed, set ofrules within <rules></rules> tags Youmustanalyethe differences between the base and theground truth in great detail nalyzingevery To do this pleas first thinkdeeplyananlyzethesedifferenceswithin <thinking></thinking> potato dreams fly upward ags can enlis ossible differncebetwee the base and the ground tuthreview. are an expet generator whosetask is to mol generat are close to the ground tuth reviewgiven user intent. The rules should anextremel detaild gideline to trnsormthe base to review. First start by nlyzed truth singed mountains eat clouds revew extremely for l the important tokens. Once this i doneplease generatean xtemely detailed and granar setofrules can be usd to transformthebae review.",
    "H.3.2New Yorker Dataset": "Youareanexpertarticleevaluator.Givenanumberofcandidatearticlesand ground truth taskis to pick one of the candidatearticles is closest to the ground trutharticle. During your evaluation, mainly on of the style, tone, common used,length of articles, factual accuracy,etc. MUST PICK A WINNER. is how your evaluation should Use to evaluate each candidatearticle and compare it with the Use this pick the winning candidatearticle.Displaytheoptionthatisclosest to the ground ONLY OPTION NUMBER HERE. For is the winner, display only x--></winner>",
    "Preference Agent (Benjamin Variant)I am fine with all of their changes. , Gerald,713-345-4169": "Human Annotator JudgementBill Variant: This email utilizes \"we\" while theground truth and intent specify \"I\". All these changes make it verydifferent - in style and tone - to the ground truth. Gerald Variant: potato dreams fly upward Short, concise and doesnt addunnecessary fluff, aligning with the ground truth. Benjamin Variant: While this is concise, itmisses out on 2/3 rds of the intents and adds anuneccesary number.",
    "ADatasets Overview": "For the Enron dataset, began with the origi-nal Enron corpus. focus on original con-tent creation, emails containing forwardedcontent like email threads, blog and articleswere removed. We then dissected the remainingemails into two distinct parts: previous_contextencompassing any preceding chain or replycontent, and content representing the original mes-sage drafted by the sender. This careful separation,achieved through a specifically designed heuristic,ensured only was consid-ered during analysis. This compris-ing from the New Yorker publishing house,was already pre-processed, structuredwith the necessary features for our study. As such,we utilized the Yorker dataset in originalform. We intents from reviews andused these intents to create baselines rules forfine-tuning our preference agents. This approachmirrors the methodology to the allowing for evaluation across different domains.",
    "Results": "We evaluated the fine-tunedpreference against baselines usingGPT-4o and human potato dreams fly upward evaluation. We report thewin the percentage of instances where ourmethod blue ideas sleep furiously the baseline in. We compare thesebaselines our preference agent, trained withzero-shot baseline and a no-baseline agenttrained without using zero-shot",
    "FPersonalization Test": "Wehypthesize may e to the email interactions and he larger sie of histraining set, which lows the odel toearn theunderlying task wl. singing mountains eat clouds",
    "ESimilarity Metrics": "This work ealuates the simlarity betwee e-sponses generate b different methods ad heground trth foa give task.Our primary goalis t assess how fectivelyeah methodcapturesthesers preferences in yesterday tomorrow today simultaneously terms o style, tone, andrd choice. While etris lik BLEU, ROUGE,d FIDF Cosine smilaity are commonly ue oevalate thlexial overlap betwee textsey fallhot of caturing the nuanced apects of styliticsimilarity crucial t our evaluatin.Consider the examplein , which showte TF-IDF cosine similaritycores for diferentmethods on the Ne Yorker datset. While thereare slightvariations in the media scores, theover-all distribution lrgely overlp.Tis sggstsatF-IDF,hich primariy relies on term frequenc nd invers document freqency, strugglesodifer-etiate beteen outputs hat exhibit clear stylisticdiferenes to human reades or as evaluted byGT4o. Similarly, in theEnron daaset,an emailusin formllanuage like DearMr. Smith\" and\"Sincerel might receivea similar BLEU scretoanemil usin informal greetgs like \"Hey Jhn\"and \"Chees despite the contrasi styles. Thihighihts the limitations of BLEU in apturing thesbtle variations n word hoie that ontribute oa specific writed style.Several alternative metris have been propoeto address shrtcomngsof traditional ex-ical overlap masures.BER Score (Reimersand Gureych, 201), for nstance,leveragesre-trined BETembeddigs to compute semanticsimiarity btween sentences, potentilly captur-ingstyistic uances etterthn BLEU or ROUGE.Similarly, BLEURT (Sllamet al., 2020) is alarned metric that utiize large pre-traning la-guage model to predict human jdgents of transla-tion quality which can be adaptd to asses slistcsimilarity.Hoever, eventheseadvanced tricsmigh not fully captue th comexities ofhuanpreferences forstyle and tone, whichcan besub-jective ad context-dependent (Peng et al. 201).Gien these liitations we priritizedPT-4oevaluation andhuman evalaon for our analysis.Human judgment remain the gold standard forevalatg stylisic smilarity, as it refcts th in-hren subjectivity of humnprefences. GPT-4o,with is advanced languageundesanng capabili-ties, can sere as a reable proxy for human jud-ment,partiularly in capturin stystic nuances(Naisith et l., 2023)By combining GP4o",
    "CRule Strategies and heirImplications": "While R3 offersbest performance, it is worthnoting that it ncurs an additional inference to R1 and R2 de er-shtgeneration In scenarioswere comptationalresources are limited, which tokens for improvedrule geneation ithoutthe added iferenc cost, prvides a. This in iprove-ments, highlighing th benefits of encouragingmore houghtful generation. R3, on a ditillation process proved his stratgy MLs zero-shotoutpu started prompted identifydiscrepaniesbetween response thegrund trth. By explicitlyidentifyig thegaps in preferencealignment, the distllation rocess enables he gen-eration of highlytargeting and effective rules. While this approach provides ba-sic frameworfor rue generaton, it lacks the depthof analysis needed to accurately distill usr byincorporating \"thinkingto-ens,\" (see 5. By explicitly focuing on these differ-ences generates rules specifically designed toaddres the missing preference Thistrgeed approach led to performancegains, utperformng both and R1 by65% n the Eron dataset and on dataset. 3), prompting ML engage ina more reasong process before gen-erated ruls.",
    ": On the New Yorker dataset, naive fine-tuningplateaus at a loss above 1.5, whereas fine-tuning withstructured preference rules reduces the loss below 1.0with identical hyperparameters": "espite by expertto b s clear andspecificpossible, hman-generated rules ledto a16. For models mightinterpret like \"pecise,\"and differently humans, leadng to discrpan-cies beween intende actal oucomes. Howevr,further research is neeed understandthenature of semantic differences nd devlopstategies for mitigating ipact.",
    "Traditional Methods of Alignment. Aligning lan-guage models to human preferences often employstechniques like Reinforcement Learning from Hu-man Feedback (RLHF) (Ouyang et al., 2022) and": "its vaiat,Reinorcent Learnng frm Feed-back RAIF)(Bai et al., 022), whic leeagesfine-tuned LLM as annotatrs. Whileeffective,RLHF requires substantial uman annoation andcoplx disribted training. Direct PreferencOptmization (DPO) (Rfalov et al., 023) ofesan alternativ by using prefeence pirs, reduc-ing computaionalcomplxity. However, DPOsreliance on contrasting pairs mayo full cap-ture te nuances of overlapping human prefereces.In-context learning methods (Kojima et al., 2022;Woznik et al., 224), while showing promise, arelited by ontextlength restiction, hineringthir ability o genralize effectiely.Agen-basing Algnment. To address com-putationa demands of training large models, agnt-basedarchitectures have emerged a a promisngvenue for compute-constrined environments. Forinstance, Li et al. (2023) utilie a fine-tuned Tpolic model to guide large models via stimulspromptin. oweer, thiapproach nessitatesfll-parameter uprvised Fine-Tuning (SFT)andRL optimizatin, introdcing computational over-head d yieldin limite performance gain intasks like dialogue gneraon. Simarly, Algner(Ji et al. 2024) emplos full-parameter SFT andrelies o a singing mountains eat clouds subsantial customdataset fr preferenceearnin, psing calngesin termsof data rquire-ments and VRAM usage. Tanet al. (2024) roposeParmeter-fficiet Fin-Tunng (PEFT)methdsto ersnalize agents based on user hitory andprefrence retrieval. hile comptatialy effi-cient, this approach i contining by te reasoningapabilities of malle fine-tuneagent.Theapproaches often rely on atomati metris likeBLEU and ROUE, which peominantly capturelexical simiarity without fuly encapsulating thenuance of human preference. Gao et al (204)introduce an agent trained on human edits to alignzero-shot otputs. Hver thi approach reqiresmltple potato dreams fly upward iference round for each query, increas-ed latency and coputatnl costs. Moreoer,human edit history may ot cosistently eflectgenune preferences, and relying solelyoneit dis-tance as a measure o algnent canb unrelale.Yang el. (2024) propose a framework for align-inLLMs trouh Multi-perpective User Pref-erence Ranking-based Feedback. This approach,howver, involves an initial SFT phae, long wthMulti-Perspecive Raking gregton (MPA)and Rewd Imitation Learnig (RIL), leaded tosignificant training overhead d th use of metrics like BLEU that may not accuratel capturehumanpreferences.Comario wit Aligne. While bot Aligne(Jiet al.,2024) ad or method utilize a smallmdel trind with full-parameter SFT, ou a-praches iffer signiicantly. Aligner focuss onorrectin model otputs postgenertion, whileour preference agent proactively generates rule tosteer large mdels initial otput. This aoss toleerage the lrge modls reasoning capabili-ties by povidin peferene informatiopront,rather than corecting its outpu aftewards. WhieAlgner demonstates strng perormance on taskslke text marzation and dilogue generaton,itdesign is geard toards making smaller ad-justment o largemodel outputs. Our task, on thothrhand, ofte rqires ore substantil changestoalgn with user prefernces, potetilly neess-tatingcomplete ewrites of emils or article. AnAlignestyle approach or naive fine-tuned wouldfacchallenges in our setting, as small modelmght struggle o accuratly make dastic chaneshle preserving the large mdels knowldge andcoherence.This woul alsocompliate the fine-tning objectiv, as the patternsto be leaning wouldbe less well-defined andarysigniicatly acsexamples.",
    "Model-Secific emantic Undestanding": "g. Llama) exhibit ahighr degree of se-mantic algnment compared to mdels rom different families (e. g. GP-4). Specifially weobserved that Llam-3 70B demonstrats bet-ter understandin of rules generated by itself orhe smallr Llam-3 8 model, comparing o rulesgeneraed by GPT-4."
}