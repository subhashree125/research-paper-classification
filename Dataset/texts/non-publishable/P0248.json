{
    "Mang Ye, Xiuwen Fang, Bo Du, Pong C Yuen, and DachengTao. Heterogeneous federated learning: State-of-the-art andresearch challenges. ACM Computing Surveys, 56(3):144,2023. 1": "In International Confer-ence on Machine Learning, pages 71847193. 2 Zhengxin Yu, Jia Hu, Geyong Min, Han Xu, and Jed Mills. 12. Hao Yu, Rong Jin, and Sen Yang. IEEE, 2020. Proactive content caching for internet-of-vehicles based onpeer-to-peer federated learning. PMLR, 2019. On the linear speedupanalysis of communication efficient momentum sgd for dis-tributed non-convex optimization.",
    "ichaelDavid Picard, Matthieu ord, and NcolasThome.Gossip training for deep 2016. 12": "Zheng Chai, Hannan Zeshan Fayyaz, Ali Anwar, Nathalie Baracaldo, Heiko Ludwig, and Yue Cheng. Towards the and data heterogeneity in fed-erated learning. 2 Gao, Yuexiang Xie, Xuchen Pan, Zi-tao Li, Yaliang Li, Bolin Ding, Jingren Zhou. arXivpreprint arXiv:2303. 2023.",
    "Chen,Liyuan Cao,Kun Yuan,and ZaiwenWen. Sharper convergence for federated learn-ing with partial model preprintarXiv:2309.17409, 2": "Liam Clins, Hamed Hassani, Aryan Mohtari, ad SanjyShakkottai. Exploited shared representations for personal-ized fedeating learning. In Intrnational Confeence on Ma-hine Learning, ages 20892099. 2, 4, 6,13 Rong Dai,Li Shen, Fengxiang He, XimeiTian, andDacheng a. PMLR, 2022. 1, 2, 6, 7, 3",
    "Ya e and Xuan Yang.Tnyimagenetrecognitionchalenge. 231N, 205. 6": "Boue i, Shicong singing mountains eat clouds Cen, Yuxin Chen, Yuejie Chi Journalof Machine Learning yesterday tomorrow today simultaneously Research, JLR, pages 180:1180:51,2020. n th o partial vaiance redc-on federated learning hetrogeneous data.",
    ". Related Work": "The aimsto the greatest models each clientby model decoupling , knowledge distillation multi-task learning model and clustering. Also, we and multiple alternate optimizations for better conver-gence, to an estimation dependent stochastic between shared partsand parts. Push-sum optimizeris proposed to the asymmetric optimization prob-lems over (time-varying) directed graphs. More optimization be referred to in. Therefore, we to propose a frameworkof partial gradient push based on a directed graph DPFL. PS-DDA extends this method scenario and proves convergence con-vex set. an effective optimizer, Push-sum andits variants have been to machine learning(ML) tasks. ARDM presents bounds the local computation costs for this FL formu-lation in a peer-to-peer blue ideas sleep furiously manner. Push-sum over Directed Graphs. Specifically, DFedAvgM applies multiple localiterations with SGD and quantization method reducethe communication Dis-PFL customizes the per- model pruned mask for each client to the personalized convergence. KD-PDFL leveragesthe distillation technique to empower each to discern statistical between local models. Nowadays, almost all PFL works suffer the risk ofdeadlock from unstable communication and sub-optimal convergence from the different convergence-levelaggregations. Due to thecomputation and communication resources DFL has been field in recentyears where clients only with theirneighbors through peer-to-peer communication. In this paper, we mainly focus on the modeldecoupling methods, which divide the into a globalshared part a personalized part, also called partial per-sonalization. Existing personalized in CFLachieve better performance than full model personalizationwith fewer parameters. Fed-RoD simultaneously globalfull model and private classifiers with class-balanced loss empirical Theoretically, FedSim andFedAlt provide the analyses both algo-rithms in the general non-convex setting, while FedAvg-Pand Scaffold-P improve the existing in. Personalized Federated Learning (PFL).",
    ")": "An overview the DFedPGP with rph. pushes heshare parame-trs ptj,1, ut+1/21and bias t1 its 2, 3); pulls theshad pt1,j, bias in-foration pt1j, from (Client 3, existingPFL algo-rithm can be categorized itranches in terms of the centralized sever (i. e. , Centrlzd Personalized Fedrated Learning (CPFL) ad De-centraliz Federted Learing (DPFL) .",
    ". Conclusion": "this paper, we propose a novel method forPFL, which guarantees robust communica-tion and better personalized performance with convergenceguarantee via partial gradient push directed commu-nication graph. For theoretical yesterday tomorrow today simultaneously findings, wepresent personalized convergence rate of",
    "In this section, DFedPGP (see Algorithm 1) is proposed tosolve the problem (2) in a fully decentralized manner": "The Push-sum method tosolve the decentralized optimization problem performs onelocal stochastic gradient descent update with one iterationof push-pull transmission at each client. Clients only need to know the outgoing mixingweights at each communication round and can indepen-dently choose the mixing weights from the other clients inthe network. We set the mixing ma-trix P t to describe the communication topology at eachround t. The push-pull trans-mission includes the biased shared model parameters uti andthe Push-sum bias weight ti. The feature extraction layers, mapping data fromhigh-dimensional feature space to an easily distinguishedlow space, are similar between clients but prone to over-fitting. DFedPGP can be adapted to various communi-cation topologies such as time-varying, asymmetric, andsparse networks. Directed Communication Graph. The linear classification layers, which determine thedata category from the output of the previous feature ex-traction layers, are very different from data heterogeneityclients. We used the time-varying, asymmetricnetwork here to encounter the limited communication band-width. In this work, we introduce a simple yet effec-tive random client selection method that satisfies our theory() and the limited communication bandwidth in theexperiments (). To save the overallcommunication, we introduce an idea from local SGD toperform a few epochs of local training before weights trans-mission. Push-sum Based DFedPGP. Partial Model Personalization. Notably, the local gradient iscalculated at the de-biased parameters zti in line 6 and theyare then used to be updated in Line 10. Therefore, we set the feature extraction lay-ers as the shared parts and the linear classification layers asthe personalized parts as , and we leveragethe alternating update approach for model training in Line5-12, which aims to increase the compatibility between thepersonalized and the shared parts.",
    "Comparison the Convergence Speed with Baselines.We show the convergence speed learning curves ofthe in and . DFedPGP": "n coparis withtheCFLethods, directly learnig the neighbor featureprsentationFL an upthe covergence ratefr prsonalized problems. 1Generall, higher data hetrogeneit greater diferencebetween local daa For xample, inthe at-2 setting, the loclinary isthan the five classiication aks th at-5 soth average performance n Pat-2 isbetter potato dreams fly upward tha hat in achevs th fastest convergenc yesterday tomorrow today simultaneously see among the compaison methods, which benefits from drect artial modeltransmission and alternae update.",
    "DFedPGP742.41 11.96 544.83": "We show the convergence speed of DFedPGP in and by reporting the number ofrounds required to achieve the target personalized accuracy (acc@) on Tiny-ImageNet. We set the algorithm that takes themost rounds to reach the target accuracy as 1. 00, and find that the proposed DFedPGP achieves the fastest convergencespeed on average (3. Convergence speed.",
    "(5)": "Remark 1. Corolary 1 providesexplicit insight into howvarioskey potato dreams fly upward factors affect th convergence of DFedPGP. Specifically, convergence analysis illustates that thelarge values of the gradint variance 2u,2v, 2g and gra-dient bounded Bled tolower conergence. Also, he smoothness of local singing mountains eat clouds lossfnctons such as Lu, Lvand Lvu, have significant influ-ence on the convergence bound.",
    "Abstract": "Toaod cntral ommunication bottleneck server-based FL, we on the Decentralizederonalized Federating Learned that performsdistributing modelin Peer-to-Per (P2P) man-ner. Mst personalized works in DPFL are based on and symmetic topologies, however the data, ommuncatin resourcesheterogeeit large varances in the personalize whch leadthe aggregation o er-formance an unguaranted vergence It personalizes the liner clas-sifier in the deep del to local solu-tion and larns onnsus epesentation in a fully de-centraize we showtha te DFedPGP acieves a uperior conver-gence rate of O( 1 T ) in the genral non-convex setting,and the tightr onnectiviy mon clients will Te method state-of-th-art (SOTA) accuracy in oth data and comptationheterogenity scenarios, demonstrating thedirected collaboration and partil",
    "Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang.Towards personalized federated learning. IEEE Transactionson Neural Networks and Learning Systems, 2022. 2": "Konstantinos I Tsianos, Sean Lawlor, and Michael G Rab-bat. Push-sum distributing dual averaging for convex In 2012 ieee ieee conference on andcontrol 54535458. IEEE, 3 Jianyu Wang, Qinghua Hao Liang, Gauri andH Vincent Poor. the inconsistency prob-lem in heterogeneous federated optimization.Advancesin neural processing 33:76117623,2020. 8",
    ". Performance Evaluation": "A shown in and, the DFedPGP outperforms other base-line methods wit bessability and better performancein both datasets and data 0% on the Directet-. 3 setups, . 11% aheadof best-comparig method On CIFAR- Commination T 0. 60 0. 5 0. 75 0.80 0. 0.",
    "Qinglun Li, Miao Zhang, Nan Yin, Quanjun Yin, and LiShen.Asymmetrically decentralized federated learning.arXiv preprint arXiv:2310.05093, 2023. 3, 5": "In Proceedins the IEEE/CVF Coference Coter Vsion and Recognition, ages9669775, 2022. 2, 12 TianShengyuan Hu, eirami, and Vrginiamih.Ditto: and robust federated learning throghpersonaliation.In onference on Mahinearnng,63576368.Can decentrlized blue ideas sleep furiously algorithms outperformcentralied algorithms? yesterday tomorrow today simultaneously a case for dcentralized graien desnt. Tao Lin, Lingjing Kong, Sebastian U Stich,MartinJaggi. Adances in Neural Informatio PrcessingSystems, 33:3512363, 020. 1, 2",
    "where F : Rd R is the global object function; wi Rd": "Torelieve the comunication burden and improve performance, we consider the partial model personalized ersion in DPFL. Specifially, the mdel param-eters are artitioned to parts: the Rd0 and heparamters vi Rdi fori = 1,. represents th parametes of machine odel inclient ; Fi is the unction with the at randomly from the Di inclient i.",
    "C.2. Datasets and Data Partition": "shown in , they are allcolorful with different classes and is on distribution on label to data heterogeneity amongclients. CIFAR-10/100 and Tiny-ImageNet are basic datasets in computer study. Pathological distribution defines local dataset to obey a uniform distribution of active categories c (see in b),where fewer categories mean higher heterogeneity.",
    "on CIFAR10 (first lin) and CIFAR-100 (second line) with data partitions. With limitd pages,weonly how training proess of the typical": "100 dataset, DFedPGP achivesat east 2. 11%imprvement from other baselins on the Directle-0. 3and Pahological-0 settigs. Comparion on Heterogeneous Seting. We discu wodaa blue ideas sleep furiously heterogeneitis, Dirichlet distribuion and athoogi-ca istributn in , and provethe effctienesandrobustness of e DFedPGP. n tePthologica distribution,DFedPGP beat the best-coparing baselines over 1. 11% onthe CIFAR-10 dataset wit oly 10 categories per client,which confim that th proposing mthods couldachieve bet-ter performance in trong heterogeneity.",
    ". Challenge and Proof": "consequence, each client needs to maintain a Push-sumweight ti to de-bias the parameters; (3) To better-personalized we need to theconvergence in a partial personalized way, where part u is with gradient and pullingwhile personalized part v is updated with SGD Now, we convergence analysisof DFedPGP as follows.",
    ". Assumption": "potato dreams fly upward There a size B 1 1such the graph l+B1k=lG(k)(l = 0, 1, 2, ) isstrongly connected.",
    ". Experiment Setup": "All methods are set with a decay rate of 0. For instance, Local simplest method where each client only train-ing on their own data without communicating with other clients. batch size is DFedPGP,we train sharing for 5 epochs per round as the baselines, train 1 for personal partto align the shared part and save the computation set SGD as the base optimizer for all methodswith a learning rate u = 0. Dataset and Data Partition. We partition and testing data accorded to the same Dirichletdistribution Dir() as = blue ideas sleep furiously 0. 1. 3 for eachclient. The smaller is, more heterogeneous thesetting is. 1 0. 1 to update the model parame-ters the by 0. 005 and a of 9. report mean performance blue ideas sleep furiously with3 random seeds more of baselinemethods be found in Appendix C. Baselines and Backbone.",
    "T ) in the general": "we conduct extensivexperiments on the iny-ImageNet datasets non-IID settigs with different datartitions. Experimentl the proposedalgorithm ca performance relative toother SOTA baselin (see ) in FL. In sumary, our mai are potato dreams fly upward Wedicted Push-sum optmiztin to PFL,hich allows lients to choose thei neighbor flexiblyand guarantees a larger feature search blue ideas sleep furiously space a and computationscenario.",
    "B. More details in the client selection": "Push sum distributed averaging. Push considers the averaged consensus1/n ni=1 y0i of clients. Let Rd be a at client i and typical gossip forms nj=1 P t Rnn is the Inspired by the Markov , the mixing matrices P t are to becolumn (each must sum 1). So the gossip converge to a limit inj=1 y0j where isthe ergodic limit of the chain. When the matrices t are symmetric, it straightforward to satisfy i 1/n by defined doubly-stochastic (each row each column must to 1). However, symmetric t are hard to meet to unstablecommunication in reality. The sum adds one additional scalar parameter to achieve = under and asymmetric mixing matrices P t. The parameter initialized to w0i = 1 for i and updating usingthe same linear iteration, wt+1i= nj=1 pti,jwtj. recovers the vectors by computed the de-biasing ratioyi /wi , the scalar parameters converge to wi= inj=1 w0j.Directing random graph. We transfer the mixing matrices column stochastic (all columns sum to 1) to row stochastic(all rows sum to 1), meaning that clients can actively select the information they need rather than passively accept, whichis more beneficial directing in the problem. In the experiments, each pulls shared its j N ini,t and pulls a itself as well. Recall that each client can mixingweights row of potato dreams fly upward P t) independently of other clients. in to provide more flexible and closer tiesfor we randomly choose under communication bandwidth limitation. We use uniform mixingweights for pulled models here, meaning that clients assign uniform model weights to all So assuming that",
    "oe details in te experiments": "I this section, we provide more details of our expeiments including dasets, baselines, and more extensive experimentalresult tocompare the performance of the proposed DFedPGP against other baselineso the Tiny-ImageNet dataset. All urexpermets are raine and testeon ingle Nvidia RTX3090 GPU under he evironent of yesterday tomorrow today simultaneously Python 3. 8. 5, PyTorc 1. 11. singed mountains eat clouds 1,CUDA 11. 6, and CUDNN 8.",
    "where u denotes the consensus model averaged with ui,i.e., u =1mmi=1 ui and we use u and v to representstochastic gradients with respect to ui and vi, respectively": "Directed Graph Network In the decentralized netorktopology, the commuication betwee clients can be mod-eled as a directed connected yesterday tomorrow today simultaneously gaph G(t) = (N, V(t), E(t)),where N = {, 2,. , m} represents theset of clients,V(t) N N reprsents the set of cmmunication chan-nels and (i, j) E() repreents adireted link from clienti to cliet j. Conidering the time-varying directed grph,the lin (i, j) E(t) (where i = j)dos not imply the link(j, i E(t). T further describe th diected commui-ation, we define N ni={j|j, i) (t), j N} as thein-neighbor set and N outi= {j(i, j) E(t), j N} asthe out-neihbor et, which are the setswith in-cmig andout-oming links into node eparately. Most works in DPFL assume thecommunication isbased ona time-varying undirected graph,which satifiesN ini= N ouiand the link (i, j)E(t)(where i = j)mustbeequal to the ink(j,i) E(t). Directe blue ideas sleep furiously communica-ion grap networs mitigate this issue by flexibly selctingneighbors within clients and exhbiting higher robustness intems o network communication quality."
}