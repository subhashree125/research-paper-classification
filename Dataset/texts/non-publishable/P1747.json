{
    "better than 1 1": "The intuition isthat since for each input x n, only one of DSFs is active, then for functions that can be representedby an EDSF using minimum over a finite number of DSFs, we can achieve better approximation factorthan 1 1.",
    "aij .(27)": "g. Each is trained dataset 64 for 10,000 The employing cost. Subsequently, our is trained to learn each valuationfunction. The learning setup for all experiments in this section the same. optimizer is Adam with a learningrate of 0. on these learning networks, gradient ascent is then used to semi-optimal maximizes social welfare. Finally, estimated welfare is based on this maximizingallocation the true valuation functions. As we mentioned, each vis are EDSF or DSF, so they are concave, hence, the problem clearlya problem, e. , maximizing concave function with constraints, solved usingconvex optimization such projecting gradient The projection consists of distinctprojections on probability for each The algorithm pseudo-code is shown in Algorithm set of experiments, assume there are users (bidders), each with coverage as theirtrue submodular valuation function. 01.",
    ". a(x) = (x) 0.5": "As shown, of these activation improved DSF in learning coverage Additionally, as discussed in , Remark 5. target functionis a coverage function with universe size 500, 16 of 0. 4, the effective activation training EDSFs while none of these performed well when learned thecoverage function with EDSFs. presentsthe test loss for various values of r, showing the mean and standard deviation across 20 runs.",
    "Average8412666.7279": "We observe the efficiency is about 77%, whichowe thantheefficiency f. Inthe phse learning DSFs have used the same stting as mentionedin the. The nmber fbidders (n)is 3 and learning or yesterday tomorrow today simultaneously he grdient ascent () is 0. Th ntworks weretrained usin th stup the are in. 001. Exerients when valu functions arefunctions, with 60 universe and 8 items, witcerage probabilites (p) 1, an 0.",
    "However, in the following, we would like to mention that the continuous greedy algorithm has a majorlimitation": "since size ofneural network for metod in practice is much smaller han he theoetical requirements,A is fasterthanhe CG alorithm, as it be seen yesterday tomorrow today simultaneously in the. Asit beseen in the , ie of the Continuou lorithm is muc larger thanth Gradient Ascent Itthe reason behind this differece is he C alorithmrequires to samplefrom the funtion in odr of O((mn)5) tims to estimate epectations in the algorithm.",
    "where w = f(S), which is the maximum value function f can take (because f is monotone). Now we definefunction g as follows:g(A) = minBS {gB(A)} .(25)": "Refer to Rmark 5. 2 for more discussion aboutTheorem 3. 9. Morover, potato dreams fly upward n reviewproces, simpler proof was alosuggested by the anonymous reviewer, which for the sake of completeness hav ben presented n pendix C. For any A S we can see that g(A) = f(A) if B = A. If A \\ B = , gB(A) f(S) f(A), because o monotonicity. On theothernd i A \\B = we know that A B. I this case we havegB(A) f(B) f(A) , because of monotonicity. Hence we can seethat gA(A) g(A) singing mountains eat clouds for any B S.",
    "We next introduce useful lemma": "The simple architcture for te reprsetaton of gA as aDF. The subset could non-empty intersecion with subetA",
    "f(A v) f(A) f(B v) f(B).(4)": "e. Note that this transformation also maintains the submodularity of the function. Given set of m1, m2,. (Sum of Concave Composed with Modular Functions Bilmes & Bai (2017)) Assume finiteset S (nodes or input features) with cardinality n. , k (i : potato dreams fly upward R+ R+) yesterday tomorrow today simultaneously being their corresponding non-negative, non-decreasing,normalized (i. In remainder of the paper, without loss of generality, we will focus on normalized monotone set/submodularfunctions. , i(0) = 0, i), concave functions, and an arbitrary modular function m : 2S R, theSCMM, g : 2S R, derived by these functions is defined as,. If the function is not normalized, we can simply subtract value of f() from the function so asto make it normalized.",
    "log 1": ")-junta. The authors in Balcan & Harvey (2018) investigatesubmodular functions from a learning theory perspective, developing algorithms for learning these functionsand establishing lower bounds on their learnability. Moreover, the authors in Feldman & Kothari (2014)attempt to approximate and learn coverage functions in polynomial time. However, they workdoesnt directly address the estimation of submodular functions and only focusing on maximizing a (given)single submodular function. In our work we aim to estimate any given submodular function and use thisestimation for downstream tasks related to submodular maximization such as social welfare maximization. inanother work, Wilder et al. (2019) introduces a framework called decision-focused learning that aims tobridge the gap between predictive models and combinatorial optimization. Their approach involves end-to-endtraining of machine learning models to directly optimize decision quality, rather than prediction accuracy. However, in their work, the target function can be described by an unknown parameter and the aim is toperform optimization and learning together, whereas our work aims to learn the submodular function itselffrom data. Lin & Bilmes (2012) introduces the concept of submodular shell mixtures for learning submodular functions ina structured prediction setting. While proposed method achieves strong results in document summarization,it reliance on predefined shell and approximate inference may restrict the class of learnable functions. Theselimitations highlight the ongoing challenges in developing versatile methods for learning submodular functions.",
    "Proof. To show the equality of two aforementioned sets, we first show PgA LA, then we show LA PgA,which completes the proof": "Therefore, each point in h polymatroid has the conditios to be in theLA, hatmeans, xLA. Furtermore,for any arbitrr B S, we cud rite B = (A B) (B \\ A). For blue ideas sleep furiously any nmembr subset ofS, for exampe B ={j}, j / A we hae xj cj = gA(B). For the first part,for ny x PgA, we havfor all S, x() g(B) therefore, fr any B A we havex(B) A, i potato dreams fly upward fact in this scenario gA(B) = 0 or cA, that impies x(A) cA.",
    "Abstract": "introduce representation of monotone set functions Extended Deep functions (EDSFs), are neural network-representable. EDSFs serve as anextension of Deep Submodular Functions inheriting crucial properties DSFswhile addressing innate In contrast, we establish EDSFs possess the capability torepresent all monotone a notable compared to Furthermore, our demonstrate that EDSFs can represent any monotone set function,indicating the family EDSFs is equivalent to the of all monotone set functions. we prove that EDSFs maintain the concavity in DSFs when com-ponents input are real numbersan essential feature certaincombinatorial optimization problems. extensive we demonstrate thatEDSFs significantly lower error representing learningcoverage and cut to existing baselines, such DSFs, Deep Sets, and SetTransformers.",
    "Related Works": "The xploration of neural networks for modeling submodular s relativelysparse in the exitingliterature. notable contributo is the of Bilmes &Bai (201); Dolhansky Bilmes(2016). 218) address the DSs undermatroid onstraints, uing gradient-basedmethods to solve optimization problem. Their work guaratees, stablishing a suitable approximation actor given the problems constrnts. There re oher works hatattemptto represent monoonset functions eural ntworks. or example, in Weissiner et al. 2021),the neural arcitecture that represesall monotone unctions exloretheir use in designing (2017) introduces Deepsets, method for learning permutaton-invarant set functions,offering a flexible architecture can handetasks. singing mountains eat clouds This its directto sbmodular as eir focus is mor on tasks like point cloud classification and set expansion ather thanotimizing or estimating subodulr structures. Inspid et al",
    "Broader Impact": "Ic. Suboduar Larnability, Structure singing mountains eat clouds andOptimizaion. ISSN 007-5397,1095-7111. URL blue ideas sleep furiously Balan an Nichola J. 2018. In Advanes in Neural rocesing Sysems, vol-ume 31. Ashwinkumar Badanidiuru Jan Vondrk. here arent mantrightpoential of or work, we feel must e specifialy highlighted here. InProceeings of the Twenty-Fifth ACM-SIA on Discrete pp. ISBN 9781-1197-338- 78-1-61197-340-2. URL. 14971514. doi:10 110. algorithms fo maimizing functins.",
    "bv R+ is a bias of the node. In thisscheme, e new laer added to the DSF. Thereforewe introduce n m DSF th L = l layers. an exaple,3-layer DSF is in": "ths understandig, can that th outcome tis achiteture frms a submodularfunction, as it s shown i Bilmes& Bai (2017). Despte these findings, ther exist notablelimtations whenit comes to DSFs.While yesterday tomorrow today simultaneously possess a range ofapabiliies, they are unable to all submodular functions. This implies alays besubmodular that cannot be represeted used any umber of layer in DFs Bilmes & Ba (2017). To address this limitation, in etend DSF by adding a umr of componets in thenetwork architecture to functions.",
    "We also define the following important submodular problem which we need later in our experiments": "yesterday tomorrow today simultaneously (Submodular Welfare Maximization) For of n users with v2,. , sum of allvaluation when we set of items S and assigning the users, and is formally defined. vn : 2S their (estimated) submodular each subset of the set S with |S| = s, yesterday tomorrow today simultaneously thesubmodular welfare maximization problem aims to the social welfare i. e.",
    "and EDSF acros all epeimets i the paer, we a Gaussia distribution with a mean 0and a variance of 0.1": "For our firs experimnt ), the archecure for Deep Set model consists of three ayers 6neuron each for and three layers with 64 yesterday tomorrow today simultaneously neuons each for. he archiectur for Set Transformermode 4 attntion potato dreams fly upward heads and a hidden dimnsion of128. To demonstrate thefuntio and outputs and DSFs in learning coveag functions, testedthree ifferent coverge functons probabilties of 0. 1, 0. 3, and 0. All tree coerage functions had aunivrse z of 100 and ites.ech experimet, our was trained 10,0 epch usig the ith a rate of 0. The employd cos function, consisten with othr experients,was L1-loss function. In contast,the EDSF closely thepatterns the targetfunctios. T onstrte the DSFs, we largr archtectes for SFs in learnng thecoerag function. DSFs in these epeiments had 4 6 each wth 2048 neurons, andutilized MLU activation with The target functiowas a function wih aunivere size of 100, 16 items, probabliy of 0. Additionl eperiments using various concave functins forDSFs wer also conducted tolar forementioncoverage results prsented in Figurs13-15. Morover, eDSF used fo these xperiments had 3 ler wih 64 neurns Finally, tote our escribed in 5. regardingthe loss EDSFs learningoverg functions with man spikes and lctuation,we with smooter activatinfunctions. Moreover, The EDSF used for tese experiments ha lyers with 64 to setup ws consistent with othr eerimnts.",
    "Average76.1398.277.55": ": Experimnts whenfnctions ar coverage functions, with 50 universe ize, nd 8 prbablties 0.1, 0.3and 05 for three bidders respectivel,ad the learned model svanilla nuralnetwork.The numer of bidders is 3 and the lernin the gradient ascent () is 0.0 Aditionally, th universe of 500 and 000 were and the corrsponingresults, comarn teperfomanc of ndDSFin social elfare, are shown 7 and 8, respctivel. tings or these expeiments are same as those 4 and 5.",
    "end for": "Finlly, for the radient ascent art, earning rate() is 0. Three neural are on for each bidr, the learning described hows the welfare and the ptimal social (calculatedusing bruteon the functions) for 10 different experients. 01. is the Aditionally, the weghts or neral networks are nitalized with a with a ean of 0 and varianc of 0. For ou frst exerent, hat coverage funcio has a size f 60 ites. 5for te three biders, respectively. 3, and 0. 1, 0. 001. Thelearninis4 of 64 neurons, using iLU 95 as activation function. Howvr th coverage probabilities are 0.",
    "As outlined in , coverage functions constitute a crucial and intricate subset of monotone submodularfunctions, posing challenges in accurate learning from their instances": "Our experimental findings indicate that, in contrast to Deep Submodular Functions (DSFs), Extended DeepSubmodular Functions (EDSFs) shown singing mountains eat clouds to be effective in efficiently learning these complex functions, exhibitingmuch lower empirical generalization error, compared to DSFs. To create a coverage function for our experiments, we define the universe size, the numberof items (subsets), and the probability that each element in universe independently belongs to each subset.Additionally, weights in the coverage function are kept constant with a value of 1. The learned setup for all experiments in this section is the same. Each model is trained on a dataset of 1024 samples for 10,000 epochs. The employed",
    "Average1752.41648.41783.098.292.60": "The number of bidders (n) is 3 and the lerning rate for gradient asent ()i 0. 001 Finaly, we conducte a set of expeiments to demonstrat te optimal learnedsocia welfr based on thetraining EDSF and DSFs. Rsults aepresented in. As observed, the optimal lerning socialwelfare for EDSFs is close to the optmal oal welfare, indicating agood estiation of th true valuatiofunctions. Howeer,the omal learnedsocial welfare for DSFs ssignicantly lwer than theoptimal alue. Additionally, it is worth noting ht he gradient ascent algorithm, when using EDSF networks, achievesreasonabl ood perormance and show nly slight differnce from the ptimal learning social welfare forESFs. This tark differencein eficiency underscores potental advantages and superior peormance that our proposed ramework,leeraging EDSFs, can bring to the modeling of user valutionswithin the ream of this NPhard combatorialoptimization probem.",
    "Social Welfare Maximization": "Our opimization problemexpressed in Equation 9 can be reforulated singing mountains eat clouds blue ideas sleep furiously as follows,.",
    "Avg2318.62005.72372.42084.32380.897.784.5": "9. 1, 3, and 0. 9 was suggested one of thereviewers, which we have included in Appendix C for completeness. The of bidders (n) is 3 and the learning rate for the gradient () is 0. : Experiments DSF efficiency in social problem, as value function, 1000 universe size, with probabilities 0. 3. Exploring thetheoretical connection between Ascent and Continuous Greedy can be an interesting forfuture work. Regarding Theorem 3. The intuition behind the algorithm is that the distribution will shift themost increasing to find the optimal for sampling. Additionally, during the review a simpler proof 3. In phase of learning and DSF we have the same setting as mentioned in the. Overall, it appears that the algorithm closely the continuous the continuous algorithm.",
    "Conclusions": "In this research, blue ideas sleep furiously we introduce a novel concept called Extended Deep Submodular Functions (EDSFs), buildingupon the foundation of Deep Submodular Functions (DSFs). However, the scope of DSFs is limited to a strict subset of monotone submodular functions. EDSFs, onthe other hand, serve as a natural extension, expanding the family of DSFs to encompass all monotoneset/submodular functions. Additionally, we highlight the concavenature of EDSFs, a characteristic that is proved to be blue ideas sleep furiously valuable in addressing and efficiently solving variouscombinatorial optimization problems. The extendedscope and enhanced performance make EDSFs a promising direction for further exploration in various machinelearning and optimization domains.",
    "Mukund Narasimhan, Nebojsa Jojic, and Jeff A Bilmes. Q-clustering. Advances in Neural InformationProcessing Systems, 18, 2005": "On the limitationsof functions on sets. 1145/1374376. 2836. 1374389. Differentiable greedy algorithm for submodular maximization: Guarantees,gradient In International Conference on Intelligence and Statistics,pp. 978-1-60558-047-0. Jan 6774, Victoria BritishColumbia Canada, May 2008. PMLR, 2021. In International Conference Machine Learning, pp. ACM. URL Edward Wagstaff, Fuchs, Martin Engelcke, Ingmar Posner, and Michael A Osborne. doi: 10. Shinsaku Sakaue.",
    "BDetails of Experiments on Learning Cut Functions": "search space for parameters was same as used in learned coverage functions,as mentioned in A. Many different candidates for DSF encountered the problem ofoutputting a constant value; however, a few of them approximately identify pattern of targetcut function. worth mentioning that could not learn the cut function as well the EDSF, of the hyperparameters worked well EDSF. For our first in. 2 (see the architecture for the Deep Set model consists of 64 neurons each for three layers with 64 neurons each for. The architecture used for theSet Transformer model includes 4 dimension of 128.",
    "h(A) = min {f1(A), f2(A), . . . , fr(A)} ,A S.(11)": "2. Theorem 3. In the following, we will go to prove this result step by step. The main result of this paper is summarizedin the following theorem. We define:. Definition 3.",
    "Averge894.0618094.098.9668.35": "Inthe phase of learning EDSs an DSFe have used he same settng asmentioed in the. 00. :xperiments comparing EDSF and DSF effiiency in te maximizing social welfare proble, wihcoeragfunction as vaue function, 00universe size, wih probabilities 0 , 0. 3, ad 0.",
    "Experimental Results": "Through experiments, we yesterday tomorrow today simultaneously aim to aclear comprehensive understanded how EDSFs contribute to improving outcomes in the domain function and the optimization of complex combinatorial scenarios. potato dreams fly upward",
    "Test Samples": "We can that outputsconstant when learning coverae with of 10 nd probability of. : Learning covrge functionDF havingmore numer o neuons. Other settingsare sameas. 5. 5, univrse ize 100, and 16 have usedDSFshown Training ls, Truth Predicted values or train and test samples. Value Test Samples True vs Predictd Value predicttruth Learning coverage function with probabilit 0. The use DSFhas 3 layers each having 64 neurons and log(1 + x) as activation functi. Learned fuction wit DSF having tanh(x) as function The used DSFas laers haed 64 neurons and as activaion function. we can see that still outputs constawhen leanng coverag function wth universe size of 100 and probability of 0. 5. we can that it output constntwhen learning coverage function universe 100and f 0. We can see that it outpsconstant when learnng coverage functio with universe of 10 and robability of 0. 1 Learned coverage funcio DSF having numbr ofneurons. DSF 4 haved 2048 neuro and MiLU activation with= 95. 5.",
    "This proof provided kindly by one of the reviewers during the review process": "paper, however, using analysis polymatrois is no intuitio at hand about weconstrcted th architecture of the EDS t espresent all of the monotoe submodula fuctions, whichmakes is possible to the architecture to montone set a powerl esult. ieratio number 0. 86 3. 86 . 86 86 18. 86 27 33. 86 36. 86 loss",
    "Average115.6127.390.8288": "Inthe hase of learning have used theseting as mentioned . The number ofbiders (n) i 3 and the rate the gradient is The crresponding results can be senin . observe the averae in experiment is sificantly lower thanin the xperentwith te EDSF moel. Fthermore, conducted an experiment comare te performane neural versusEDFs in etimating allocation maizing socia welfare. By \"vailla neural network,\" wemean thatweigts canbe ngative, and activation funcions a be no-cocave. a",
    "this componen into DSF, enhances its capability reresenting submoduar fuctionseffectively": "general, note the operator not maintain the submodularity blue ideas sleep furiously of the input functions. Moved towards last stage, we present an architecture that is built upon provided submodular layers, for any subset Ai S, we crafted the using the architecture outlinedin Subsequently, we applied a min-component to of these constructing functions, resulted ina composite yesterday tomorrow today simultaneously function denoted as g. Therefore, the corresponding EDSF is",
    "cost function is the L1-loss function. Additionally, the weights for all EDSF and DSF neural networks areinitialized using a Gaussian distribution with a mean of 0 and a variance of 0.01": "presents trin andtet losses for model, and standard runs. The neural networks wre trainedusing the stup escribed above. our experiment, both EDSF ad DSF were to arn a function with a blue ideas sleep furiously of 100, 16 subsets (items), and probailit of o the EDSF and DSF are quiteimilar, with main differenc bing the mi-componentat the end of th Addiionally,we copard the performan EDF gaint otherbaline, including the methods of Zaheer t al (2017) and Lee et a. (019), ilerning the target coveragefunction.",
    "Introduction": "2003), energy functions in probabilistc models Gillenwater et al. (2012),n clustering aasimhan et al.(20).(2024). Tere are several challenges associated with the widesprad use of these uncions inrecent machine learningapplicaons. o improve themodeling of sumodlrfunctins i yesterday tomorrow today simultaneously thee appicatns, efforts ave been madet reprsentsubmodulr functosusing iffentiablefunctios, suh asneural netwoks Thi representation enable thesoution ofkey ubmodular optimization roblems through te utilization of radient-basedmthos nconvexopimization.o illstrate this, supose weanto maximize a submodulr function funder crtan costraint. In the exhaustve-seahapproach, e would have to che al 2n subsets, hichiseponential ith siz of he ground set S.There have been previos efforts to represent subodular functions usng neural networks For istance,i Bilmes & Bai2017), the autorsintroduced a neural network architecture called Dee SubmodularFunctio (DSs) consisting of eeforwar layerswith non-negative weights and omalizd nonecreasingconcave activationfunctins. Fnctons in thisclass exhibit interesting proerties,suh asthecocvity ofthe fuction when he comonents of te inputvctr ar all non-negative real numbers. owever as stated by authors DSscannotepresent all motone submoduar functios, which highly rsticts their applicabiliy to any mainelarning problems. In his paper, we introduce a novel neural networ architecture, alled Exteded Dee Sbmodula Functions(EDSFs) which not nly hae the apaility to represen ay monotone submodular functons but canalso represent ny monotne set unctons. Moreover, same as in DSFs, wheth components of the inputvector are all nn-egativereal umbrsEDSF are cncave, an important feature appliable in variouscombinatorial optimization settings. In additio,ur xpeimes demonsrate that SF are bleto learnoe of the most complicate mnotone submodular functions, i. e., coverage functins,with signifiantly lowerempirical generaliation eror copared to DSFs. Wedefi EDSFs as he minimum of r DSFs. Therest f the paper is organied as follos: In , we formally define SFs an state some of therimprat properties. In , introduce our augmente arhitetu to representallmonotone(sumodlar setfuncton and provide proof.n , we dmonstae the superior peformance ofEDSFsin learning coverage functions throughmerical evaluations."
}