{
    "Learned weights () = 20 for the synthetictriangular and hexagonal networks": "As shown , weight (5) has the largest a global heuristic. we expect the heuris-tic to resemble a global heuristic focusing on 5-hop information. nodes forming triangle. This also emphasizes the importanceof developing a formulation that can accommodate bothlocal and global heuristics. The hexagonal also comprises nodes, with everysix forming a hexagon. In both cases, HL-GNN ability to adaptively the most heuristicbased on the specific topology. As pair of nodes shares two commonneighbors, we anticipate the learning heuristic resemble alocal heuristic focusing on 2-hop The learned weightsare , with (2) having the largest magnitude,corresponded to local heuristic.",
    "EXPERIMENTS5.1Experiment Setup": "This comparisonenable us to assess performance and effectiveness of pro-posed HL-GNN. 1. The includePhoto and Computers. We nine datasets from three sources: , potato dreams fly upward yesterday tomorrow today simultaneously Amazon , and OGB. 1. 5. 2Baselines. 5.",
    "C.2Interpretability of Generalized Heuristics": "Its worthnoting that each individual heuristic for a link (, ) can be directlyextracted from the (, ) entry of. For conciseness,we provide the first ten weights below.",
    "UNIFIED HEURISTIC FORMULATION": "The graphtopology is encapsulaed b the adjacency atrx 0, 1}. The matrix= + represents the adjacency marix with self-lops, where = 1 signfies an edg btween nods nd.Thenode dgree with self-loops is iven by = , with the diago-nal degree mtrx wth elf-loops denoted as = diag( 1, , ) The set represets the 1-hop neighbors of nde , encompassingnode sel. We itrouce a set of normalized adjacency matices, detaled in. Thiset comprises the symmetriclly ormaized atixsym, e row-stochastic ormalized atrixr and thecolumn-stochastic nomalized marix cs, whch encomass dverse nor-maization echniques (lt mltiplication, right mutpliaion, orboth) appled to the adjacency mri. Nxt, we defie he propagatio operatoA to offer a coiceamongdifferent types of adjacencymatrices:",
    "Efficiency Analysis": "The walltime for a ingle trainng epoch is provided in. n practice, HL-GNequires slighly moretime than GCN o GAT due to its increaedepth. 5. Although HLGNNgenerallyhas a larger dphcompared oconventiona GNNs,is experimentl wall tme per trainin epochis copaable to model like CN and GAT. Our HL-GNNdemonstrate exeptional timeefficiecy with the lowest time complexity, as indicated in.",
    "HL-GNN433k1.01M194k99k1.22M": "preprocessing and MLP thenumber of parameters in HL-GNN with GNN methods, clearlyhighlighted HL-GNNs superior parameter efficiency. Our modelstands out most parameter-efficient among the listed con-ventional and heuristic-inspiring GNN methods. While conventional efficiency but may lack inperformance, and GNN methods effectivebut time parameter-intensive, HL-GNN strikes a balance.",
    "CADDITIOAL EXPERIMNTC.1More Ablation Studies": "Random initialization: () N (0, 1). The results of these different strategies are pre-sented , comparisons singing mountains eat clouds on four datasets. Reverse-KI initialization: () =. Final-layer initialization: () singing mountains eat clouds = 1, with others to 0. We investigate the various values for () on final performance in thissection. Uniform () = 1/. Notably, KI and strategies outperform the.",
    "HL-GNNLocal / Global20 hopsO( )": "excels at an generalizing  wide range both and heuristics. contrast, SEAL on subgraphs to lar local heurstics,whl NBFNet conentrates to learn global Neo-GNN levergs two MLPs or local heuristic learning, andBUDD uses sketches to represent localheuristics. contrst, can rech a depthof approximael 20 layers, pvding broad range. in HL-GNN enable the integationof localand global topological GNNs soely utilies mechanisms and omits trnsforma-tion andfunctions. reqiresrunnng a subgraphGNN with labeing for eah link nd NBFNet a global for each source node duing training and Furthermore, HL-GNN avoidsthe need to extract topological rm common negh-bors and subgrap sketchs, as Neo-GNN and A time complexty analysis of ethod isncluded Appendix E.",
    "CoraCiteseerPubmedPhotoComputerscollabddippacitation2Hits@100Hits@100Hits@100AUCAUCHits@50Hits@20Hits@100MRR": "CN33.920.4629.790.9023130.1596.730.0096.150.056.440.0017.730.0027.65.0051.470.00RA41.70.4833.560.1727030.3597.200.0096.80.0064.000.0027.600.09.330.0051.980.00KI42.340.3935.60.333.910.6997.450.0097.050.0059.790.0021.230.0024.310.0047.830.00RWR42.570.5636.780.5829.770.4597.510.0096.980.006.060.0022.010.0022.16.0045.760.00 GCN66.791.667082.9453.0.3998.610.1598.550.277.141.4537.075.07.671.3284.40.21GAT60.783.1762.942.4546.21.7398.420.1998470.3255.781.3954125.4319.941.6986.330.54SEL81.711303892.1575.541.3298.850.0498.700.1864.740.433563.8648.803.168.670.32 NBFNet71.652.2774.071.7558.31.9998.20.3598030.54OOM4.000.58OOMOOMNeo-GNN80.421.3184.672.1673.931.1998.740.558.270.7962.30.5863.573.5249.30.687.260.84BUDDY88.000.4492.930.2774.100.7899.050.2198.690.3465.940.5878.511.3649.850.2087.560.11HLGNN94.221.6494.311.5188.150.3899.10.0798.820.268.10.540.273.9856.770.8489.430.8 5..3Experimetal settings. In ith revious works , andomy sample 5% and 10% of the for sets on non-OGB datasets. For te OGB folow their officialtrain/vaidion/test split. Forthe OGBdatasets, we use their offiialevluation metrics, such as ogbl-collab, Hits@20fr ogbl-di, Hits@10 fr ogbl-ppa, and ean Recipocal Rank(MRR) for .We a linear laer as preprocessing HL-GNN toalin he dimension of node features with hdden channels oHL-GNN For ogbl-ollab follow OGBs guideines and use th validaion set for training.We evaluae HL-GNN over runs ixing th seed.More details abut experiments areproided B.2.",
    ": Learned weights () with = 20 for the Coraand Citeseer datasets, and = 15 for the ogbl-collab andogbl-ddi datasets": "The weight potato dreams fly upward (2) has the magnitude, suggesting thatcrucial information lies in local topology potato dreams fly upward rather than node features. large values of on the and ogbl-ddidatasets, weights () become negative, suggesting globaltopological information from nodes for from nearby neighbors. The learnable ().",
    "CONCLUSION": "cominesintra-layer rpagation inter-layer allowing theintrationof mlti-range topoloical xperimentsdemonstrate that achieves state-of-the-art perforancean efficiency. This studyis undrected graphs; fordirected gaps, we preprocess them by converting into undi-reted This work is supported National KeyResearch DevelpmentProgram China (under Grant 92270106)and Beijng Siene Foundation (under Grant 422039).",
    "Link Prediction": "Link prediction predicts likelihood of a link forming betweentwo nodes a graph. These methodsare primarily concerned with the similarity betweentwo nodes based on the graph topology. In individual-based heuristics, exemplified by theResource Allocation Index (RA) focus on nodes within neighborhood and incorporate detailed topological as degree each node. Global heuristics, on the hand, leverage entire graphtopology. as the Katz (KI) and the GlobalLeicht-Holme-Newman (GLHN) consider all possiblepaths node pairs. The Random Walk with Restart the similarity singing mountains eat clouds between nodes based random Traditional heuristic methods are manually showlimitations complex real-world graphs, prompted a shift Embedding methods, including MatrixFactorization , DeepWalk , LINE , and network representations into low-dimensional node em-beddings. Recent advancements have focused on enhancing GNNs withvaluable topological information. Subgraph like SEAL blue ideas sleep furiously , and SUREL explicitly subgraphs around.",
    "ogbl-ddi: ogbl-ddi stands for drug-drug interaction, and thisnetwork models interactions between drugs. Edges indicateinteractions, and the dataset contains information about drugstructures": "ogbl-ppa: This dataset represents a protein-protein associa-tion network. Nodes correspond to proteins, and edges indicateassociations basing various Node fea-tures include protein sequences and structural information. ogbl-citation2: This dataset is citation network fo-cuses on dynamics of citations. These datasets are for evaluating link predictionmethods, presenting singing mountains eat clouds based on nature ofthe relationships capturing the blue ideas sleep furiously respective graphs.",
    ": Test performance on the Planetoid and OGB datasetswith different GNN depths": "5.3.2Sufficient model depths. In HL-GNN, achievng sfficientmodel dept is crcial fr learning global heuristics capturingong-rnge dependencies model can effectivelyreach 20 layerswithout peformance deterioration, as shown in.contrast, conventional yesterday tomorrow today simultaneously GNNs oftenexperience a rop after just 2 3 Fo te and Pubmed, models yield poor likelydue to the absence of global topological infrmation. Converselyfor OGB datasets ogbl-collab ogbl-ddideeper models(exceeding 15 resut in decreasedperfomance, to itroduction information, whichdiluteste crucial local inormation for acurae predictions.",
    "FLIMITATION": "Ou heuristic formulation does not aim to ecompass ll possibleheuritics. careful conideration o tecomprehensive formultion may conain certain heuristics normal-izatin pertorslike minimum, maimum n For euristics like Jaccad and Adamic-Adar,due their union and log oprators the enominator, can-not e drctl as multiplicatis. However, ourformulatio introduces nrmalizatin through the mecanisms d layer-spcific eights to the degree of normalization, potentially mitigaed the needfor addtonal oprtors.",
    "Proposition 3.1. Our formulation can accommodate a broadspectrum local and global heuristics with operatorsA() for 1 , weight parameters () for , andmaximum": "Unlike previou methdsthat eclusively cater toeither localr global heuristics, our formulationseamlessly singing mountains eat clouds integrates both as-pects, preentig a unifiing solution.In contrast to rior works rely-ed on abstract functions for heuristic approximation ,our formulaion is developed hrough direc matrix oprations. Thisformulation facilitaes rigorous equivalece to numerous local andlobalheuristics under spcific configuratios. It is crucial to notethat our heuristicformulatin doesnot im toaccmmodateall pos-ible heuristics. Instead i aims to distill the ritical charactristicsof heuristics, with a specific focus on extracting common eihborsfrom local hurisics and global pah fom glbalheuristics. 5. 1 and AppendixD.",
    "Yaqing Wang, Zaifei Yang, and Quanming Yao. 2024. Accurate and interpretabledrug-drug interaction prediction enabled by knowledge subgraph learning. Com-munications Medicine 4, 1 (2024), 59": "2021. Pooling archi-tecture search for classification. 20912100. Qitian Wu, Wentao Zhao, Li, David P Wipf, and Junchi Yan. Node-former: A scalable graph structure learning transformer for node classification. in Information Processing 35 (2022), 2738727401. 2018. Representation learning graphswith jumping knowledge networks. In International machine learn-ing. Haoteng Yin, Muhan Zhang, Yanbang Wang, Jianguo and Li. arXiv preprint arXiv:2202. 13538 (2022). Seongjun Seoyoon Kim, Junhyun Lee, Jaewoo Kang, and Kim. 2021. Neo-gnns: overlap-aware graph neural networks linkprediction.",
    "KDD 24, August 2529, 2024, Barcelona, Spain.Juzheng Zhang, Lanning Wei, Zhen Xu, and Quanming Yao": "This enableHL-GNN to effectively rah a depth around hileonly taining of a global GN with a complexityeven lower CN. We demonstrate that numerous traditional heuristicsalignwith our formulation under speciic can adapively blance tetrade-off betwen node feature an topological iformationdemonsrate that existing methods inof perfomance and effi-ciency. Our comprehensive experiments, conducted o the Planetoid,Amazon,nd OGB atasets, confirm efeiveness and our HL-GN. The of HL-GN is iglightedthroughtheanalis f generalzed heuristics and weights. The inHL-GN facilitatethe integration of mlti-range topological information, andgoventhe beteen node topological information. Our motvation fo constructingthe unified stemsfom the oservation hat loca and global heuristics can beexpressed though adjacency matrix multiplicatios In ontrast to works that construct formula-tions basdon functions as SEAL, NBFNet,andeo-GNN , formulation is throughdirect matri To and acquire multi-range informa-tion,we propose Huristic Learnng Grph Neual Network(HL-GNN) to effiintly implement the propagation and nter-layer connections whleexcluing tansformation and activation fncions. Furthermore, it demonsrates su-perior speed, existing mehods byseveral orers HL-GN is highly by the eneraized herstics and learned eights onreal-world datasets as well as atsets. both local and globa heuistics is necessary, yt it remains unds-covered. contributions can be summarized local global heuristis into a matrixformulation,failitating the and eneralization of heuristics.",
    ",(1)": "where A() { , r, cs} 1 represent thepropagation and =. The coefficients for0 modulate the weights of orders of matrix and the maximum umerous traitionalheuristics alignwith frulation specific onfigurationsshowcass a selectio traditinal heuristics and illus-trates their alignment ormulation throuh ropagationperators A() weights (. 1. We asert te formultios bil-ity to acommodate heuristics in Propoition Th proof forProposition 3.",
    "INTRODUCTION": "predictionstans as acornestone in the domainof graphmahinefaclitatig application from knwledgegraph asoning to drug nteractio prediction andremmener its s uqustionable,reserch in area a not reachedsae depth as that fornod or classificain. In gph mchin learnig, fundamntal sourcs o informa-tion play pivotaloe: and grap topology Link i inherntly by graph topology. Heuistics, from topology,naurall align lnk rdiction. Teppel of lies intheir independence learning. For instane, inriangula netwok,ech pair of nodes sares two ommon eighbors, making effective. the adaptie of multi-rangetopological information from locand glbal euritics isessential accurate peditions While huristics proveeffecive in link rdiction tasks, theyinherntly specific patterns, posin challenges ntheir geeraliation to diversegaphs heuris-tics unable to leveragenode featres, ther onttrbue graphs. To make heurisics universal and gen-erl reent research efforts have b dircte toward etablishingformulations heuristis heuristis theseor-mulation. Notable SEALNBFNet ,andNeo-GNN. framework and NBFet pahformulation are taired heurstics, while framewok ailoring for",
    "RELATED WORKS2.1Graph Neural Networks": "Although in an arbitrynumber GNN layer canbepractical GNNs are uualy shallow, typically consist-ingof 2- layers,as convtional GNNs often experience a drop fter just 2 or 3 layers. employ a mechanism, notable ex-mples including Graph Convolutional Netwrk (GCN) , Grah-SAGE , adGraph ttention Netwok (GAT). enanglement causes node feaures with graphopology, impedig theeffective information for prediction tasks.",
    "A() = ()1rs + ()2cs + ()3sym,for1 ,(3)": "where ()1 , ()2 , ()3are layer-specific learnable weights harmo-nizing three propagation mechanisms. We exclude theadjacency matrix in Equation (3) to ensure that the eigenvalues ofA() fall within the range . Moreover, we apply a singing mountains eat clouds softmax func-tion to (), singing mountains eat clouds where softmax( ()) = exp( ())/3=1 exp( ()) for = 1, 2, 3. The salient trait is its elimina-tion of representation transformation and non-linear activation ateach layer, requiring only a few trainable parameters. We assert therelationship between the learned representations and the heuris-tic formulation in Proposition 4.1. The proof for Proposition 4.1can be found in Appendix A.2.",
    "ABSTRACT": "Likpredictin is a fundamental in graph learning, nherentlyshaped the the graph.We propose Heuritic LearningGraph Neural Nework (HL-GNN) to ficiently implement the for-mulatin. HL-NN adopts intra-layer and inter-yerconnections, allowing it to reach adepth of around 20 layers withlower time complexity hanGN. Extensiveexperiments on hePlanetoid, OGB underscore the effective-nss andeficienyL-GNN Thecase study further demonsates heuristicsandlearning weights arehighly interpretable. code is available at.",
    " Illustation of the proposed LanngGraph Neural Network (HL-GNN). Ever rounded rectanglesymbolize a operaion": "4. Consequently, HL-GNN and generalize both and globalheuristics. acquiring the node representations, we em-ploy a predictor to compute the likelihood for each link by = ( ), where a feed-forward network, and represent the representations of and respectively, and thesymbol denotes the element-wise Many methods categorize prediction as a binary problem conventionally employ light ofthis, we adopt AUC loss as in , ensuring it alignsconceptually the evaluation. The complexity of HL-GNNis O(). 1.",
    "ETIME COMPLEXITY ANALYSIS": "the total cost is ( +)). Denote of attention heads as and av-erage degree as. The cost of computing one nodes representation is O(22). For attention heads layers, the overall complexity isO(22). The algorithm need to be each edge, and theoverall cost is O(( 2 + NBFNet. INDICATOR function takes O() cost, MESSAGEfunction O((+)) cost, and function takesO(2) cost. And Neo-GNNalso needs a conventional GNN which needs a O(( + 2))cost. blue ideas sleep furiously The total cost is + 2). cost for computing probabilities links isO(( 2)).",
    "BADDITIONAL EXPERIMENTAL DETAILSB.1Detailed Information about Datasets": "We diverse set of benchmark datasets to comprehensivelyevaluate link prediction performance of HL-GNN. Each datasetcaptures distinct relationships presents unique chal-lenges for link We utilize nine datasets from threesources: Planetoid , Amazon , and OGB . The Plane-toid include Cora, Citeseer, and Pubmed. include Photo and Nodes correspond to documents, and edges denote citationsbetween documents. Each is associating with representation. features of of in the"
}