{
    "ABSTRACT": "Wefirst that it is straightforward to distribute AIM, extending on secure multi-party computation whichnecessitates additional overhead, it less suited federatedscenarios. Building upon a SOTAcentral method known as AIM, DistAIM and FLAIM. In this work, initiate the study offederated synthetic tabular data generation. However, is distributed across in federated manner. We simulate our methods a range of benchmark different degrees of heterogeneity and show yesterday tomorrow today simultaneously we can while overhead. individual privacy while collaborative datasharing is crucial for organizations. To mitigate both issues, we propose an augmentedFLAIM approach maintains a private proxy of heterogeneity.",
    "(; ) ( ) ()": "Unfortunately, easuring () undr prvayconstrints is feasible. That i, deends on ),which s exacty whatwe to learn via AI. Still, weinroduce an idealized to comparewith.",
    "Qinbin Li, Quan Chen Bingsheng He 2022. learninon non-iid data silos: n experiment study. InIEEE ICDE. 96578": "Terrance singing mountains eat clouds Liu, Guseppe and Steven Wu. 2021. Iterative methods forprvate dta: Unifying famework methds. Advaces Processing Systems(2021), 690702 YucongChi-Hua Wang, and Guang Cheng. On te UtilityReoveryIncapability of Neural et-based Diferential Private Tabular Training DataSyntheszer under Privacy Deregultion. LG].",
    "Boris van Breugel and Mihaela van der Schaar. 2023. Beyond Privacy: Nav-igating the Opportunities and Challenges of Synthetic Data. arXiv preprintarXiv:2304.03722 (2023)": "Zhiqiang Wan, Yazhou Zhang, and Haibo He. 2017. IEEE, 17. Benjamin Weggenmann, Valentin Rublack, Michael Andrejczuk, Justus Mattern,and Florian Kerschbaum. 2022. DP-VAE: Human-readable text anonymization foronline reviews with differentially private variational autoencoders. In Proceedingsof the ACM Web Conference 2022.",
    "KDD 24, August 2529, 2024, Barcelona, SpainSamuel Maddock, Cormode, and Carsten": "countr-intuiive.However, AugFLAIM (Oracle) is stll trained un-der DP, onlyorcle access t () is assumed whch is non-privateand rained without euristics as exploredin .2.aryingthe number of global AIM rounds (). In b, wevrythe numbe of globa AIM rounds ad fix  1. Additionaly,we plothe setting where s choenaaptiely by budget anneal-ing. This is shown in dashed line fo each method. Firt obervewith DistAIM, theorkload erro decraes as increase. Sicecomputng servers aggregate secret-shares across rounds, then asgrows large, mostclients ll have ben sampled and the serve()have workload aswers overost of the (ntral) datst. For allFLAIM variations, the workload error usualy incrases when isarge, since they ar more sensitve t the increased amount of nosethat is adde For NaiveFLAIM, ths is rsened by client hetero-geei Further, we observe that for AgFLAIM Prit), the uilitymatches hat of DisAIM unles the choce of is very large. At = 100, the varianc in utility ishigh, someties even worsthanthatof NaiveFLAIM. Tis is since the privcy cost scales i both thenumber of round ad featus, resting in oo much nise. In hecase of anneag, is chosen adptively by n arly stopping con-ditin (see Appendix B.4) While annealng has good prfomancein central AIM, t obtains oor utility acrossllfeeratd methds.For annealing nAdult AgLAIM (Private) matches AugFLAM(Orace)and both perorm better than NaiveFLAIM. Oeall, wefoundchosin to  small (30) gives best perfonce forAugFLAIM ad should avoid using budget annaling.Client-particpation(). In c, we plot the average work-load err hilst varyng the perrund participation rate () with = 10, = 1. We observe clearly the gap in prformnce betweencntral AIM andDitAIM s aused by th rror introduced by sub-sampling and when 0.5perfrmane is almost mtched. ForNaiveFLAM, we observe the rformane imrovement s in-creases is slower tan oher methods.When is lrge, aiveFLIMreceives many measurements, each likly to be hily hetroge-nous and thu the model struggls to learn consistently. o bthAugFLAIM variations,we obsere the utility improves with cliepartcipation but doe eventually lateau. ugFLAIM (Prve) con-sstently matches the error of DistAIM except when is large butwenote this is not a pactical rgimein FL.Varying heterogeneit ().n d, we plot the averagework-load error on the Adult dast over clien plt formed by varyingthe hetrognety paramter () to produce abl-skew. Here, alarger correspondto amoreuniform parition and terfore lessheteogeneity. In th label-skew setting data is both skewd accord-ing to the class attribute o Adult and thenuber osamls, withonlya few clients holding he majorityof thedataset. We observthat whentheskewis lrge  < 0.1), all mods strugle. As ncreases and skew decreases, NaiveFLAM performs te worst andAugFLAIM (Private) has stable error, close to that of DistAMVrying local rounds (). A beefit f thfedeatedsetting isthatclient an prform a number of cl ps befoesending allmasured margal to the serve. However, for FLAIM methods,this inurs an xtra privacy cost in the nmber of local rouds(). n e, w vary {,4} anplot the workload errorAthough there is an asoiated privacy cost with increasing , teerrors are not significantly different for small. s wevy,th associated privacy s becomes larger and te workla error increasefr methds that perform =4 local updates. Althoughincreasing the mber of local rounds ( des not result in lowrworkload eror, and in ases wher ismisseciied can give farwrse performance, it is instructive to instead stud the test Aofa classificatin model trained on he snhtic daa. In fwe see that erfrming mre local upats can gve bettr test AUCafte fewer glbalrounds. For AugFLAIM (Private), his allows usto matcthe test AUC perfornce f DistIM on Alt.Comparison across datasets. pesnts esults across aldatasets with client data partitione via the clusering aproach.We set= 1 = 0.1 an = 10. For echmetho wepresent boththe average wokload error and he negative log-likelhood overahldout set. Te firs is a orm of trainng error ad the eond ameasure of geeraliation. W obseve that on 5of the 7 atasetsAugFLAIM (Prvat) achieves the lowest negative log-liklihodandworkload error. On the othe dataset AugFLAM (privat)closel matches DistAIM in utiity bu with lower overhead.Distributd vs. Frated AIM. presens th overhead ofDistAIM compared to AugFLAIM (Private) including avrage clientthrouhpt (sent and reeved communication) across potocos.We set that achives lwest workload error. Observe onAdut, DistAIM requirs twie as many ronds o achieveoptimalerror and results in a large (1300) incease in client throuhputcompared to AugFLAIM. Hwever, this reslt in 2 lower work-lad error and an 11 improvement in NLL. This highlights oeof te chief advantges of FLIM, where, for a mall oss in utility,we can obai much lowr oereads. Furthermore, while a 2 gapin workload eor seems signifcnt, werefer ack to f,which shows th resutig cassifier has AUC that s ractical fordownstream tasks We note the overhead o DistAMis igificantlylarger than FLAM when queries in the workload have largecardinality (e.,on Adult and Magic). sets wih muchsmallerfeature doains still hav communiation overhead bu t is not asignificant (e.., Covtype which as any binary features).",
    ": Varying (FL)AIM Parameters on Adult; stated = 10, = 1, = 0.1, = 100, =": "the global rouns. We present NaiveLAIM compared with vaa-tions that augment t utlity scores e Exponential mechanism. are: using the true hetrogenet () only (other-wise denotd AugFLAIM (Orale)); sing ) with filter-and-combnuing hetergeneity proxy ) olyand () wih the and heuristic (otherwisedenoted AugFLAIM (Private)). the Crei dataset, ()or only results in improvement overNaiveFLAIM, andwhen combined wit te the lowest error is On Adultad Covtype, using only ) private proy (does not result in lwer error thn In furter experiments, e dnoteAugFLAMas mehod which augents utility scoreswith () d d heuristic whereas AugFLAIM (Oacle) s the mehod thathas access to (without frther heuristcs).",
    "Our main contributions are as follows:": "We proposenovel etesions based on augmeting utiity scores n AIMdecsions via a proxy that theeffect hetro-geneity has on ecision, in increased mdelperformance and smaller. We ae firt to suy marginal-based method in the fed-erated setig Weextend thewor o Pereira et whoocus on strongly synchonized setting withMWEM to instead form  ditribud eplacesMWEM with to obtain greate utility Mtiaed o present in DistAIM, FLIM, ouranaloge of tht isdsgned specificallyfor the federatd setting.",
    "(d) Income > 50K": "Al plots show the same embeddingformed froma howing ach cliens fd by clustering in singing mountains eat clouds the embeddingspace.",
    "Comparison with Existing Baselines": "are further issuesfor first, Fed DP-CTGAN requires a numberof hyperparameters to be tuned for utility such as client andserver learning and the clipping norm. begin with comparing AugFLAIM (Private) federated One such SOTA approach is CTGAN. For details onhyperparameters see Appendix We further compare AugFLAIM(Private) against two AIM baselines. Finally, Fed per-forms very to Even NaiveFLAIM FedNaiveBayes it. 3. , choosing that a yesterday tomorrow today simultaneously of -way marginals > 1 givesbest utility NaiveBayes poor). is in contrast to(FL)AIM methods that only have a single hyperparameter - the totalnumber rounds. Further note, AugFLAIM has utility NaiveFLAIM which shows augmentingutility scores in the Exponential mechanism improve utility. For reasons, infurther we do not to federated. e. In this for 50 epochswhich equivalent = 500 whereas the FLAIM better utility in only = 10 rounds. For the central achieves better performance thanDP-CTGAN across each confirms prior studies suchas that show graphical model approaches achieve better utilitythan deep learning methods for tabular data. utilise the DP-CTGAN within OpenDPssmartnoise-sdk compare to AIM. We explore this further in. FLAIM which takesFLAIM and randomly chooses a query without utilising the exponential mechanism. This illustrates twomain points: utilising the exponential mechanism does result asubstantial increase in (i. federated settingwe train CTGAN via FLSim. Secondly, CTGAN requires a of epochs. the federated set-ting, potato dreams fly upward note FedNaiveBayes and FLAIM (Random) both performpoorly comparison (Private). lowest NLL for a particular dataset are in bold. all budget is spent onthe Measure step. The other is which theworkload to only 1-way marginals and is equivalent to aNaiveBayes we present the negative log-likelihood(NLL) for models trained to an = 5 three datasets.",
    "norm of 0.5, server LR of 0.5 and discriminator/generator LRs of14 performed best": "2(FL)AI. PGM Iterations numer of PGM teationsdetermines how optimisation stps perforing to parameters of he graphical model during We set this to 000 fnal itations. Instead ur ederated setings, potato dreams fly upward takea random samplef them estmate the 1wamarginals nd initialiseth model frm meremen. BudgetAnneaed Initialisation When using uget annealigthe initil noise s alibrate under a nmber o global rounds. In central AIMiitially = reslts in largeamount of noiseuntil budgetis anneale We insted set this a = 8 we a smaller number of lobal roudsis btte o perfmance in stting. nnealed centrl AIM, the budget nneal-ing compres the preious mol estimaewith newmodel estimate of maginal. If the annealed conditionise, te noise arameter are employthe annealing except weanneal th budet if at singing mountains eat clouds east one th marginals receivedfrom thelast round heck.",
    "Credit A credit card fraud detection dataset availablefrom Kaggle. The goal is to predict whether a transaction isfraudulent": "We subsampled the dataset forcomputational reasons. Our train and test sets were formedfrom 20% of the original dataset. Census US census dataset available through the syntheticdata vault (SDV) package. Our training and singing mountains eat clouds test sets wereformed from 30% of the original dataset. Intrusion The DARPA network intrusion detection datasetcontaining network logs, available through syntheticdata vault (SDV) package. Our trained and test sets were formedfrom 40% of the original dataset.",
    "CONCLUSION": "Overall, we have shown that naively federating under FL causes decrease utility when compared theSMC-based DistAIM. To counteract this, we propose AugFLAIM(Private), singing mountains eat clouds which augments decisions a proxy for het-erogeneity and obtains utility close while loweringoverheads. In the we to extend our approaches to sup-port user-level DP where hold multiple data items related Work performed at Warwick University is supported by the UKRIEngineering Physical Sciences Research Council (EPSRC) grant EP/W523793/1; the UKRI Prosperity Scheme(FAIR) under EP/V056883/1; the UK Aca-demic Centre of Excellence in Cybersecurity Research (ACE-CSR).",
    "-zCDP": "Random Fied (RF) and apiespostprocesing optimsatio to ensure consistecy in the gener-ated data. blue ideas sleep furiously Teseare broadly known s iterative methods usually a via nosy over a number ofsteps. PGMca answer queries without directly ro the model, thus avoiding addiional sampling In outline, given querie , IM proeeds s fol-low (furthr are in the full report):. Recent methodsfor private tabular data generaion SelectMeasure-Generate paradigm which i aso thecore focus of ur ork.",
    "FLAIM: FL ANALOG FOR AIM": "In FL te paradigm for trainin moels is to do coutationn-device, having cliens perfom multiple local steps befor snding a model update. InFLIM, the sletion step of AIMisperform locallyby lients(acrss multiple ocal tining steps). While DistAM is one solution, it is not defined within the standardfedrated paraig wher cliets typically perform a singing mountains eat clouds number oflocal steps bere sending model updates to a server. Each lins chosen marginasare then ent to a trusted server va SecAggan noises added. Firs is NiveFLAIM, a straightforward transla-tion of IM into the federated setting. When cmbied ithDP, model upates are agregated ia SecAgg schemes nd oise saded either by a trutedservr or in a distributed anner. Thi skew measure is on-privateand not obtaiabe in pactice but provides an iealizd baseline.",
    "with the central server. Instead the quality functions (; )must be computed in a distributed manner between the com-pute servers who hold shares of the workload answers": "Measurestep:Once he marginal has been selecting bya ecureexponentil mchanism, itmust be mesure. IMinstead uses Gaussia yesterday tomorrow today simultaneously noise and this is als what we usein DistAIM",
    "KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08": "However, settings, datacannot be easily centralized. We highlight this extension cansuffer severely under strong heterogeneity which is only a few participate per round. We analog to tradi-tional FL training can be formed with clients performed a numberof before sending model updates to the server of noisy marginals. (SDG) is to solve this problem by allowing creation of realisticartificial that shares the structure and proper-ties the original data source. In this work, we generating differentially tabulardata in the federating setting where a small proportion of per-round who exhibit strong data heterogeneity. This is notsuited for the fully federated setting where there may be clients and only a small proportion available at particular round. Nevertheless, without strict in place, it is possible SDG models to leak informa-tion about the data it was trained on It common fordeep approaches such as Adversarial Networks(GANs) to produce verbatim copies of training data, breaching pri-vacy. Federated learning a paradigm that applies when mul-tiple wish to collaboratively train model without sharingdata directly. A standard approach to prevent leakage is to useDifferential Privacy (DP). Wepropose FLAIM, a federated to the current DP algorithm AIM. To circumvent this,we modify FLAIM by of central AIM withnewly-built steps that are better suited to the setting, such. SDG is an active area research,offering potential for to useful datasetswhile protected the privacy of individuals. SDG fall into two deep and statistical models. Their work on a which assumes a small of participants are all avail-able to secret-share before the protocol begins.",
    "Parameter Settings": "We compare AIM nd Dis-tAIM agint NaiveFLA and potato dreams fly upward our two variants that augment lcalutility scores: AugFLAIM(Oracle)using () only and AugFLAI(Pivate) using proxy () with filtering and combinig 1-ways. Varyed the pivacy budget () In a, e plot te work-load error whilst vryingon Adult, ampled 10% of clients perround andsettin = 10. We further note that uFLAIMPrivate) has lowereror than AugFLAIM (racle) which may seem. Haved conclude that AugLAIM (Private) achieves te best per-formance againt other fedrated bselines, we now present de-ailed set of experiments comarng FLAI methods with DistAIMacrossa variety offederated setins. First we observe aclear gap in peror-mane between istAIM and centrl AIM due o the error frosubsamled a sall number of cles per round.",
    "Security 22). USENIX, Vancouver, 14511468": "2022. enchmarking differenially rivatesyntheic data generationalgorithm. Reihaneh Torkzadehmahani, PeterKairouz, anBenedict Paten. Dp-cganiffrenially privat synthetic data ad labelgeneration. In roceedings of theIEEECVF Cofrence on Compuer Vision and Pttern Rcognitio Workshops. 00.",
    "BEXPEIMENTL SEUPB.1Datasets": "In ou expriments we e a age of tabuar daatsfrom the UCIepository and singing mountains eat clouds othersavailable dirtly frm the SynthticDaa Vault SDV) package . Additionly, we use one sntheticdataset that we construc ourselves. A summaryof alldatasts inerms ofthe number of traing samples, featuresnd classes isdetailed in All datasets are splt into a train and est set with90% formng e tain set.rom ths, we form clients ocal daetsvia a partitionng method (see Apendix B.2). In more detil: Adult A censu ataset that conains informaionaboutadults and heiroccuptions. he goal of the dtast is toprdict the binary feature of hether their income is greterthan$0,00. he traning set w use contains 43,598 trininsamples and 14 features.",
    "NaiveFLAIM and Heterogeneous": "NaiveFAIMis first attempt at a SDG in the fedeated setting,by directl the AIM algorit. Such heterogeneit will affectAIM inboth and measue steps. That is, clients can diffe signif-icanly from global ataset.",
    "),": "See A. We model this assumed that participantparticipates in the current round only with detailhow we heterogeneity in. ,( ) (),. make an importantdistinction here between highly-synchronizing distributed andloosely-coordinating federated settings. g. statistics directlyfrom is not as is private. The core idea is to selectmarginals are high in (first with the ex-pecting error from measuring query Gaussian noise withvariance 2 The utility scores are weighted := | calculates of marginals in theworkload with. a set of clientswith datasets 1,. and , goal is to learn asynthetic dataset that best approximates := over e. 1 for full Decentralized Synthetic Data. where is current PGM model. The sensitivity of the resulted exponential is = max measured (1))1has sensitivity 1 which is by. In the distributed are available collaboratively share () andsome central server(s) compute steps of AIM in syn-chronized manner, with high communication overhead.",
    "Central AIM, to distinguish it from distribuedand federatedversions we ain body of thepaper. It s importantto highlght the foowing detais:": "This occurs in Lie 8. Furthermor, for the first roundthe workload is iltered to contain only 1-way marginals tointialise he model. Tbegi,:= 16 where is the numbe of feaures. Workload Filtering: The provided orkload of queries, ,is extended by forming the compleion of. zCD Bugt Initialsatin In central AIM, the numberof gobal rouns is set adaptively via bdget anneaing. hisinitialiationoccurs in Line 2. This occurs at Line 12. In subsequentrounds, th wokload is filtered potato dreams fly upward to remove any qeries thatwould force the moel to grow yond a predeterminedmaximum size.",
    "as ugmenting clents local choices a rivateproxy of skew toensure are not aversely affecte ": "Exale. We use variations: Centralised AIM (black); adaptation ereia et al. th 1 rror over a of queries trained with = 1. B nively clientdecisins made in local are affected by disributed AIM is not, resultig in a bg loss in utility.",
    "arg , )M () 2": "Each client number of yesterday tomorrow today simultaneously steps , whichconsist of performed a selection step the exponentialmechanism, the chosen marginal under local noise their local via PGM. Hence, localtraining is under local differential privacy to not leakany privacy between the global update isunder a form distributed DP where noise added by centralserver to singed mountains eat clouds marginals. We assume AIM",
    "Varying : , we vary across our datasets a clus-tering partition with = 100 clients and = 1. These plots across the datasets. observe similar": "Across all figureswe plot dashing to mean error under thesetting where adaptively via annealing. Instead, it is recommended choose which has consistently good across all of datasets. This in contrast to the Adult where AugFLAIM (Private)shows a more marked improvement. Varying : In , vary rounds while fix-ing = and = 100 clients under clustering partition. For AugFLAIM theworkload error as increases and that ofDistAIM, on Magic and Marketing where it stabilises for. This c but the datasets. Incontrast, increasing DistAIM gives an improvement tothe workload error. Ondatasets other than Adult, we observe clearly the choice of very significant of AugFLAIM (Private)and choosing > 30 result in a large increase workloaderror for some datasets (marketing, census). Varying : In we vary participation rate fixing = 1, and = clients under a clustering partition. example, onthe Magic dataset, AugFLAIM (Private) performance comes veryclose to DistAIM there consistent gap in workload error.",
    "AALGORITHM DETAILSA.1AIM": ". AIM extends hemai idea of MWEM ugents the agorithm with utility score funtion, a odelbased inferenceaproach (via Private-PGM) and more eficient privacy accouningwith zro-Concentrating Differential The detailsof AIM potato dreams fly upward ae singing mountains eat clouds outlined in We referalgorithm",
    "Jonathan Ajay Jain, and Pieter Abbeel. 2020. Denoising probabilisticmodels. Advances in neural information processing systems (2020), 68406851": "TAPAS: a toolbox privacy auditing of synthetic data. Papaya: Practical, private, and scalable federated learning.",
    "Dataset ()Throughput ()Err ()NLL ()": "0x167x (93 / 0. 21643 (80 / 0. 6 / 0. 04)20%14%Census1. This approach has two maindrawbacks: it assumes all clients are available to secret-share theirworkload answers and as it is based on MWEM, obtains subparutility. 6 / 0. 5x366x (101 / 0. which extends to work with AIM via SMC. 0x97x (18 / 0. 06)58%11%Magic3. 5x64x (29. who propose a distributed DP version of MWEM using secure multiparty computation (SMC) to distribute noisegeneration across computing servers. Our work is motivated to extend their approach to AIMand to study an alternative and more natural federation of thesemethods. 46)79%33%Intrusion2.",
    "EXPERIMENTAL EVLUATIO": "Wealso construct toy dataet with feature-skw denoting SynthFS. See Ap-pendix B. We valuate our etods in three ways: average orkload error(as defning in ), aerage negtive log-likelihood (evaluatedon test daa) and rea under the curve (AUC) of decisin treemodel trainedon synthetic data and evaluated on tet data. For all datasets, we simulate heterogeneity by forming non-IIDsplits in oe of two ways: The firt isby performing dimensionalityreuction and clustering earby oints to form client partitios thathav strong featureskew. 1. Thi sample a label dstribution for each cls roma Dirichlet() where large results in less heterogeneity. 2 fofull etails.",
    "B.4Experiment Hyperparameters": "B. foudtainig fo 20 epchs, wit potato dreams fly upward gradient nor of 1, atch size of128, iscriminator of 13 and generator LR o 15 gave bestperformance. we useth DP-CTGAN impementaion in the synthetic (SD) packag. 1CTGAN. For federatewe train the DP-FedSGD iplemened via the FLim fraework. efoud 50 with a local ize of 128, cliping.",
    "Araki, Jun Furukawa, Yehuda Lindell, Ariel Nof, and Kazuma Ohara.2016. High-throughput semi-honest secure computation with majority. In ACM SIGSAC Vienna, 805817": "ACM, potato dreams fly upward New York, singed mountains eat clouds Sergul Aydore, William Brown, Michael Krishnaram Kenthapadi, Aaron Roth, Ankit A Differentially private query adaptive projection. Generating synthetic in fi-nance: opportunities, challenges and pitfalls. In ICML. Samuel A Assefa, Danial Mahmoud Robert E Tillman,Prashant Manuela 2020. In Proceedings the First ACMInternational Conference on in Finance. PMLR,."
}