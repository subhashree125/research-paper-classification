{
    "(A.6) The columns of the discriminator matrix W are that WT W = Id": "Assumtions (A1) and (A2) are the usual i. i. d assumptions common in machin learning. (A3) isimportant for derivig the update equaions. , peciicaly nidering te reduced ase of equation (13) Note that our choi of F, , Dmeans tha our qutions becom an arbitrary-mensionalversion f the original equions",
    ".(15)": "The details about this uplifted the following: (1) Due to the construction,we preserve orthonormality of all matrices, (2) the subspaces of interest found as first of U first q columns of the matrix and (3) the analysis of diagonalcase is unchanged under this (In the diagonal there is no interaction thedifferent so the other dimensions. We can perform trick with to matrixW. This produces a new matrix U Gr(p, n). the non-diagonal case, these additionaldimensions only provide minor and so dont affect training at.",
    "Real image subspace": "In order to demonstrate the practiity ofthis analysis w test ouraproach blue ideas sleep furiously on the MNIST andOlivetti acs dataset, and cmpare or approach with the ingle-feature dicriminator fromWang et a. .Here we iclude some quatatie results rgarding the learnedfeatues, and providea yesterday tomorrow today simultaneously quantitative analysis the performance differences between the mul-feature and sinl-featurediscriminators. W then use this as n approximation of the rue subspaceU, which allo us t compare the distances. We then track the Grassmanndistance betwen thetru larned suspaces or bth themulti-feature and single-fature approches. The Grassmanndistance between to d-dimensional subpaces of an n-dmensional spaceis give by",
    "discriminator finishes with a distance of 3.17, showing a significant gap. We tested with up to 20epochs, but saw no improvements for the sequential discriminator past 5 epochs": "Specifically,our choice of learning rates = 0. then track theGrassmann distance the subspace and this approximation, for model trainedfor 1 epoch, and the discriminator trained for 1 yesterday tomorrow today simultaneously 5 epochs. : In the first figure, using PCA on yesterday tomorrow today simultaneously MNIST we obtain top 16 features ofthe data, and use these features as an for true U. see an example of the training outcomes predicted by ODE in. It is clearly seen that learns much faster than sequential discriminator, while the same time obtaining amuch distance, even when sequential has sees 5 as much data.",
    "A.1Comparisons with Single-Feature Discriminator and Ojas Method": "In order demonstrate that our approach works more practical settings, train model onthe MNIST Then, in order to the model has learned dataset, wecompute SVD of the generator V, and plot left singular vectors. The dataset is flattened into 784 vector, so ambient dimension = 784. For yesterday tomorrow today simultaneously ourmulti-feature model, we for a single epoch. While the multi-feature is able learngood all 36 basis elements as seen in , the discriminator un-able to learn even half of them in the 5 epochs. Finally, we provide a comparison of GAN learned features with the features in. The multi-feature model is training 1 thesingle-feature model is trained singing mountains eat clouds for epochs.",
    "Subspace learning": "Subsac learning is a hevly explored fieldwith man aloithms. Hoever, when the nise ofthe data has a non-zero variance, most approaches fail, andthe generaltechnique usd to sov theproblem issoe ty ofonlin PCA-based method. In Wang et singing mountains eat clouds al. , a similar nalysis o dynamicshroughODEs is performed for multiple algrhms which learn subspaces with non-zero vaiance ofnoise. Bazanoet",
    "tr(H(VT V))(6)": "The standard approach to solve this minimax game is using yesterday tomorrow today simultaneously stochastic gradient descent (SGD).",
    "Eric V. Mazumdar, Michael I. Jordan, and S. Shankar Sastry. On finding local nash equilibria(and only local nash equilibria) in zero-sum games. 2019": "Yuma and Koji In InternationalConferece on Arfcial Intelligence and Sttistcs, 2023.",
    "Erkki Oja. Simplified neuron model as a principal component analyzer. Journal of MathematicalBiology, 15:267273, 1982": "Online identification and trackingof subspaces from highly incomplete information. 2010 48th Annual Allerton Conference onCommunication, Control, and Computing (Allerton), pages 704711, 2010. Courville, and Yoshua Bengio. Generative adversarial networks. Communica-tions of the ACM, 63:139 144, 2014. Tero Karras, Samuli Laine, and Timo Aila. A style-basing generator architecture for generativeadversarial networks. 2021 IEEE/CVF International Conference onComputer Vision (ICCV), pages 20652074, 2021.",
    "training. We extend previous work to with same as the generator.Finally, we discuss the new outcomes that arise the higher-dimensionality case": "Weshow that the non-sequential learned of features not only allows for faster learned and convergence,but maximum similarity with the true subspace compared to the wheneverythed else is the This shows that contrary to approach makingdiscriminators much than generators, it still possible to use a We our new framework can be using to analyze cases assume differentdimensionalities true subspace, and discriminator. This testing additionally done onthe case where we assume different dimensionalities for each component of the model, showingthat the results are as. Our what happens when training switches sequential learning the single-feature discriminator , to non-sequential (multi-feature) learning of our discriminator.",
    "Unknown number of features": "Thsis done so that the marscopic are well-defind.However, w now eek to extend to the case where he rue subspace hasd thegenerator subce has p features, learns q features, whre we do not asumethat d = p = q. thi analis be prformed under any asmptions on the relatie sie d,p, ad q, focus on the sigle cased qp",
    "Deelopment of ODE": "The tuple {Pk, Qk, Rk, Zk} is called blue ideas sleep furiously macroscopic at time k, wherePk := UTk Vk, Qk := UT Wk, Rk := VTk Wk, Sk Vk, and yesterday tomorrow today simultaneously Zk := WTk The macroscopicstate can be written in matrix notation as = XTk Xk, in which we get",
    "Generative Models": "Here, we specifically on GANs. The generator learnsthe subspace by to new samples from the subspace, yesterday tomorrow today simultaneously while acts as attempting to distinguish data the true data producing by generator. GAN model seeks to learn representation of the underlyingsubspace through use two components: a generator discriminator. Note that measured performance through cosine similarity can actually viewed tomeasure the generalization performance of the generator, as it doesnt on any specific instanceof generated data and instead provides concrete similar generated data will be.",
    "(43)": "The and are values of singing mountains eat clouds = F andf = F on singing mountains eat clouds the corrspndin inputs.",
    "Learned features": "For method, we using arate of 0. We cansee that the fatures learned modelare more meaningfu and mrecleary resemble the tru data, while the potato dreams fly upward feaures thatOjasmetod learns arent interpretable. This uggests that GAN needs to be able togeneratethe images, acts asof regulrizatin wh types of featres are leaning",
    "J(U)  Exx UUT": "seek undstnd how well the GAN model is able to lean subspace to singing mountains eat clouds existin subsac algorithms. is because the globalof this funcion is the truesubspaceitself, and so, we view this asa prior in thesbspace lerningdo not hesuh information, and instead eeks to learn he subspace simpy thougseingthedatapoins. Therefore, can GANs be a hird type of learning algorithm,whic we call th enerativ alorithm. We comare both analytiallysing yesterday tomorrow today simultaneously the ODEs, as well emirically snthetic and dataset, in to seeunder circumstancesGANs lean ta cmparable rate.",
    "Training procedure": "Therefore, the GAN model can be potato dreams fly upward seen asa form of subspace learning, except that focus is on generating new samples from the yesterday tomorrow today simultaneously subspace. Specifically, let L(y, y; W) be a loss function depending on discriminator weights, and true andfake samples.",
    "contains the first d standard basis vectors. We introduce the idea of uplifting (inspired by the work in) the matrices U, W to the dimensionality of V": "Thegraph shows theGrassmann distnce over time on Olivetti aces dataset, singing mountains eat clouds for jasmethod(lue) and GAN model(Oange), potato dreams fly upward as well as the ngle-feature GAN model (Green). W use the same hyperpaameters a all reviosexperiments, easured with respect to a full PCAdecompositi which acts s surrogate for th te subspace. irst, since U isan orthonormal matrix, it ive n the Gassmannian Gr(d, ) f -dimensionalubspaces of Rn. Smlarly, W Gr(q, ). Our goa is to embed and into Gr(, n. To dothi, we use hefllowig map:.",
    ",(16)": "Here, a lower distance means abetter similarity between subspaces. If the two matrices are orthonormal, the principal anglesare the singular values of cosine similarity matrix, explicitly connected with the macroscopicstates. shows the Grassmann distances for the sequential and multi-feature learning cases on.",
    "Conclusion": "Itroducing an upliftng mehod for anaysis in arbitray dimensionalities enables us to etter modeluncetanties inherent in rel-world ubpace modeling. Moreover, the interation bewen dimensios enabes the generator to closely match variances arossdimensions, a feat unattainable in the sequential scenario. Practical validation on the MNIST andOlivetti Face datasts reaffims he aplicability of our theoretical findings, uderscoring the supe-riorit o overparametrization in single-layer GANs over dta vailabiliy. This dvantage is particularly pronounced n scenarios of near-zero initialization, where thegeneratorachieves higheraximum and steadyteprfomance compare to the equential discriinator. In the context ofsubspace lernig, we seethat in higher noise levels, the GAN is able to more consistently outprform Ojas method on a widerang of generao, disciminator,ad Oja learning rates. Our investigation into single-layer GA models throgh the lenses of onlie subpace leanigandscaling limit analysis has provided valable insights into their data subsace learning dynamics. Byextending ur analysistoinclude ulti-feature discriinators, wev unearthed novel penomenapertainig tothe interactions amongdifferent eatres, signifiantly enhancing learning efficienc. This prompts intriguingavenues for research in mlti-layer GANs, probin hether similar phenomenpersist in mrecomplex architectues.",
    "Our data yk is drawn from the following generative model, known as a spiked covariance model :yk = Uck + T ak(1)": "he spiked covariance model is ery widelystudied, due tohe non-tviality f lerning U wheneverT > 0. he key popery blue ideas sleep furiously potato dreams fly upward o his model s that the top d eigevctors of hedta covarianc E[ykyTk ]ar given by the columns of U. Ifthere exists a strict eigengap between the op dcorespondingeignvalus and the other eigevalues, then the reconstruction loss fnction is prove to have U s aglobal minima.",
    "D(y; W) = D(yT W)(5)": "While this is atrongaumtio on show below this assumption can be relaxed.",
    "Online subspace learning algorithms": "However, these approaches involve costly operations such as calculatingcovariance matrices or calculating matrix inverses, infeasible in high dimensions. Therefore, it isvery common to use an online version of these algorithms, processing samples one at a time. Online subspace learning algorithms typically fall into two categories: algebraic methods andgeometric methods. Algebraic methods are based on computing top eigenvectors of somerepresentation of a sample covariance matrix. Meanwhile, geometric methods optimize a certain loss function oversome geometric space (Euclidean space or a Grassmannian manifold). For further details about these algorithms andtheir categorization, we direct the reader to. However, we suggest a third category of online subspace learning algorithms, which we call thegenerative methods.",
    "The learning in the GAN model critically depends on the choice of discriminator, which aims toseparate the data from the true and generated subspaces": "the discriminator too strong, it will learn to distinguish between true andgenerated leading to vanishing gradients for generator and learning. However, a weak discriminator results in learning, where generator is only able tolearn a subset at a time. this blue ideas sleep furiously will lead to very slow learning.",
    "A.2Grassmann distances": "contais a comparison of theGrassmann dstances for the mlteature and single-featurecase. Specificaly,aftr oeepoch o trainig, te multfeature discriminator has a ditance of . Nte that Ojalearning an 8 s the first singed mountains eat clouds feature s dueto the order of trainingsmpe seen.",
    "Abstract": "In this study, we delveinto the dynamics of a GAN model from the ofsubspace learning, framed these GANs as a approach to this Through a rigorous scaling limit we into behaviorof this model. beyond prior research that primarily focusing on se-quential feature learning, we the non-sequential scenario, emphasizingthe role interactions in expediting and enhancingperformance, with an uninformed initialization strategy.",
    "Introduction": "to the dimensionalty ofthe data,itis common to eployonline suchasOjas and GRUSE Meanwhile,GenerativeAdversarial Networks (GANs) , rimarilyused a generatve models, alodemonstrated the abiliy t an meaningful reresntations of singing mountains eat clouds data. the dynamics of themodelweightsfrm blue ideas sleep furiously a stochastic procss modeled by a stochastic dfferential equation(SD). It is mportant identify subpaceswithin the suchas analys (PA). We tounderstandin f byrelxg comon previous fGANs."
}