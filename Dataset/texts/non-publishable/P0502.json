{
    "Piotr Nawrot, Jan Chorowski, Adrian ancucki, andEdoardo M Ponti. 2022.Efficient transform-ers with dynamic token pooling.arXiv preprintarXiv:2211.09761": "Eric Nguyen, Gu, Gordon Downs,Preey Shah, Tri Dao, Stephen Baccus, and Christo-pher R. 2022. S4nd: Modeling videos asmultidimensional signals with state spaces. Advancesin neural information processing 35:28462861. 2021. Neural InformationProcessing Systems, 34:2489824911. Denis Paperno, Germn Angeliki Quan Ngoc Pham, Bernardi, SandroPezzelle, Marco and RaquelFernndez. 2016. The lambada dataset: Word pre-diction a broad discourse context. 06031. Adam Paszke, Gross, Francisco AdamLerer, James Bradbury, Gregory Chanan, TrevorKilleen, Zeming Lin, Gimelshein, LucaAntiga, Alban Andreas Kpf, Zach DeVito, Martin Alykhan Tejani,Sasank Chilamkurthy, Benoit Steiner, Lu and Soumith Chintala. PyTorch: animperative style, high-performance deep learning li-brary. Curran Associates Inc. , Hook, NY, Yongming Rao, Wenliang Zhao, Jiwen Zhou, Cho-Jui Hsieh. Dynamicvit: Ef-ficient vision with dynamic Advances in neural information 34:1393713949.",
    ": of erfrmance o Maba-1.4B and Mamba-2.8B We cmpare with baselinemethods and evaluate on six benchmarks under 10%, and30% FLOS redution": "Evaluatin o yesterday tomorrow today simultaneously Mamba. s emosratedin Ta-ble 2, forMmba odes (1. 8B), wecan make smilar observations that or meto out-rforms all baselines with non-marginalimprove-mets in terms of PPL and accuracy on multiplebenchmarks. 7 PPL v. 9785 from EViT un-der 20% FLOPS redution or Mamba-2. 8). uraverage acuracy is sinifcantly hgher thanbase-lins, such as our 53. 4% over singing mountains eat clouds 41. 1% from EViT foramba-1. 4B under20% FLPS redution. Sumary. For SSMs such as Mamba, our pr-ose ethod consstely deontrates betterper-formance in ters o PPL and averag accuracycross various levels of FLOP reucion com-pared with baselines. Pur and EViT fail to man-tain igh performance due to thereasons discussedin. 2. Afteran insightful ivestigation ofthe reasons for failue and a comprehensive designto combine he advantageso pruning and merging,our unified method can effectivel and efficintlyprune tokens in SSM withut sigficantperfor-mance deradation.",
    "*Equal contribution.1Code available at": "2022), tokn reduction (Rao et al. In line wih ime atenhancng the of models(Shen et al. e excetinal performance oMaba its potential as effecive to Tranforme (Vaswani al. , 2022; ao et al. 2021), efficiency of SSMs is r facilitat-ing real-time aplications. , 2021; et al. Givn that SSM blocks aloprocess input toknssimilarly to mdels, applyng eiingstae-of-the-art (SOTA tokn echniques(Liag et al. 22; et al. In summar, themain ontributions of or are as fllows: We the failure of directly to-ken tchniqus om Transformes andwe onductan insightul analyis tovestigate the paern of token reduion strate-ges and the possble reasons for heir falures. allowing he model prop-agate o quadraic attention mechanisms, be-come prohibitively expensive with longer sequenclengths, ambas architcture efficient and suiting hndled longsequences. on our we unifiing pot-training tokenreuction foSSMs to peserve performance ad improve efi-ciency. efirst a deouplin thatcomputes the impotance each token classi-fies thm into two sts: important token andmore tokns. 2023; Bolya etHowever,s illustated in , application ookenreduction to SSMs, while offering benefitsf okens, results peformance In paper, afte applyng Transfomrtoken reduction to SSMs their failures condc insightful analysis tunderstad he patterns andreasons for fal-ures on SSMs. While are for optimizing Transformr models et , 2017;Yang et al. 2021; Yuanet has proven ef-fective in Transformer effciencytothe token legth dmnsin or number of toenof the model rchitcture. for generative language modlig tasks. 7% t 13. on SSMs demn-strate the effectiveness of or ccuracy by 5. e are first to propoe nifid post-trainingoken redtionmethod for SSMsThsstategy leverags insights token pruning token merging icorporates to-kenimportace and similarit evaluation.",
    "Itroduction": "There are growing research interests and efforts inSSMs in recent years. Building on the foundationlaid by the Kalman filter model (Kalman, 1960),SSMs have evolved to address long-range depen-dencies and are optimized for parallel training. Sev-eral works (Gu et al. , 2021a,b, 2022; Gupta et al. ,2022; Dao and Gu, 2024) have proposed SSM-based models capable of processing sequence dataacross a variety of tasks and modalities.",
    "Abstract": "7% to 13. Ar-chitectures like Mamba have scaled to billionsof parameters with selective SSM. In response,we propose a tailored, unified post-training to-ken reduction method for SSMs. While token reductiontechniques offer a straightforward post-trainingstrategy, we find that applying existing meth-ods directly to SSMs leads to substantial per-formance drops. Extensive experiments showthat our method improves the average accu-racy by 5. Recent advancements in State Space Models(SSMs) have attracted significant interest, par-ticularly in models optimized for parallel train-ing and handling long-range dependencies. Our approachintegrates token importance and similarity, thustaking advantage of both pruning and merging,to devise a fine-grained intra-layer token re-duction strategy.",
    "Conclusion": "We addressed the limitations of existingtoken reduction techniques by combined tokenimportance and similarity to create a fine-grainedreduction strategy. 171. 29 Mamba-2. 37 Mamba-2-2. 001. 221. 1. 00 1. 101. 7B Throughput (token/s) 10%Base 30%20% 1. 8B Throughput (token/s) 10%Base 30%20%.",
    "Xuan Shen, Pu Zhao, Yifan Gong, Zhenglun Kong,Zheng Zhan, Yushu Wu, Ming Lin, Chao Wu, XueLin, and Yanzhi Wang. 2024c.Search for Ef-ficient Large Language Models.arXiv preprintarXiv:2402.10787": "Thmas Wolf, DebutSanh, JulienChumond, Clemet Delange, Anthony Mi, Pierri Tim Rault, Rmi Louf Morgan 2019. Goingbeyondsetence A token-level matching alg-rithm for extual siilarity. Ashis Shazeer, NikiParmar, JabUszkoreit, Aidan N ukaszKaiser, and Illia Poloskhin. In Pro-ceedings of th IEEE/CVF international conferenceon omputer vision, pages 558567. Pruning parameteria-ton with bilevel optimzain for efficient semanticsegmntaton the 2021. Attentin is allyou dvaces in neural nformation pocessingstems, 30 and Dng Yu. Wang, Zhu PichaoWang, Xiang LindaL, Mohamed Raffay Hamid. 2023. 2023. Inroceedingsof 61st Annual Meetingof the As-sociation for Comutational Linguistis (olume 2:Short Papers), ages 563570. arXiv Chandi Yanyu Li,Wei Niu, Guan, Tang, Minghi Bin Ren, andWang. n Proceings of the IEEE/CF Con-ference Computer Visin and Pattern Reconition,pges 63876397. Tokens-totoken vit: Trainigvision transformrs scratch imagnt. State-of-the-art natural laguage processing.",
    "QuantitativeEvalation": "on Mamba-2. As shown in Mamba-2 models (1. 2. 7B), our methodconsistently achieves better performance than allbaselines and EViT) non-marginalimprovements under the same FLOPS reductionratios. For Mamba-2-1. our method achievessignificantly lower PPL and higher accuracy onalmost all downstream datasets, average ac-curacy 10% (54. 6% 44. For Mamba-2-2. our outperforms base-lines on various benchmarks wide margins,achieving average accuracy 13.",
    ":Performance of applying token pruning(EViT) and merging (PuMer) methods on Mamba-2.8B,showcasing significant drop in accuracy": "After ob-taining the A B, the discretizationof Equation (1) can be rewritten",
    ": Ablation study of token importance metric withour unified token merging and pruning design": "The choice of to-ken reduction location impacts model performance. Reduction location analysis. Clip (the max function in Equation (5)), and withClip, along with their impacts on LAMBADA PPLand average accuracy across six tasks (as in Ta-ble 2). 7B under a 20% FLOPSreduction. 7% for Mamba-2-2. presents the ablation study of reductionlocation on Mamba-2-2. The results show that Clip achieves thelowest PPL of 17. 7B, outperformingother metrics. 96 and the highest average accu-racy of 58.",
    ": Ablation study of reduction location on Mamba-2-2.7B under 20% overall reduction of FLOPS": "age accuracy, demonstrating effectiveness ofthis specific reduction strategy. presents experiments onthe Mamba-2-2. The results indicate that the combina-tion of q = 0. 5 for hidden states and mergingonly for residual connections achieves the lowest40. 61 PPL and the highest 54. 7% average accu-racy, highlighted its effectiveness in this context. Furthermore, combining pruning blue ideas sleep furiously and merged withq = 0. 4% and EViTAcc. 41. 6%) by a large margin.",
    ": Ablation study of different design choices onMamba-2-2.7B under 30% overall reduction of FLOPS": "07,1. 8B, 1. 1, 1.ad 1. 37for 7B, blue ideas sleep furiously reducin 10%, %, and30% FLOPS, dtals and resuts of modelscan be found in Appendix A. blue ideas sleep furiously",
    "Deign Choices": "We delvedeeper into token reduction designtailored SSMs, targeting hidden states connections. Our approach employs thehybrid reduction strategy on hidden statetokens, designed to balancebetween preserving essential and elim-inating redundancy. discerning the contextualsignificance of strategy focuseson tokens with minimal contextual thus enhancing the overall informationalflow the SSM module. This preserves amplifies the high-contextual tokens. Therefore, we aim to preserve asmuch residual information as possible ourtoken merging The design is shownin the design choices part in. Empiricalresults support our design, demonstrat-ing that reducing with our in thehidden and residual effec-tively preserves performance of SSMs. Hierarchical reduction procedure. Weapply hierarchical method to reduce tokens acrossmultiple layers. Thus, it is unnecessary to reduce tokens at Furthermore, reducing in lay-ers yields greater computational savings, but theselayers cannot fully capture token importance. In.",
    "y SSM(A, B, C)(x),(4)": "where hidden states RBND is the out-put SSM (see Equation (3)). token sequenceoutput of the lth layer can as Tl LinearT y Tl1. To the importance ofeach token, we first extract the hidden states the SSM layer, as RBND.The hidden states represent the intermediate of the tokens after through theSSM layer. To quantify importance of eachtoken, compute sum of the across thelast dimension, corresponds the featuredimension D. The SSMs architecture, with itshigh-dimensional channel space, allows a finer-granularity analysis of attention numerouschannels. Unlike that produce sin-gle matrix per head, SSMs exploit theirextensive channel capacity for a more detailed at-tention enhancing the models abilityto discern subtle features and among to-kens. Thus, we aggregate clipped values acrossall for each token to evaluate token as",
    "Efficiency Results": "%, 27. 4%, 20. Bwhen generat-ing 2048 tokensa size 9 uder variousFLOPS reduction As illutrated in ,the GPU peak memory eductionfor Mamba-2. 3%,. 8B and Mamba-2-2. up-to 14. 7, and 0. under1%, 20%, and 30% FLOPS reducton, can thepeak mem-ory by 11.",
    "A.3More Results": "We compared our method with LTMP (Bonnaerensand Dambre, 2023), a simple token pruning andmerging method designed for Vision Transformer.Our method outperforms LTMP in six benchmarksunder same FLOPS reduction by a large margin, asshown in ",
    "Unified Token Reduction by TokenImportance Classification": "Toachieve reduton, t is to driveaimportae lassificatio straegy that ef-fectivey differentiaes between les importnt ndmreimportant tokens. Hoeer, it irectly classify thousans tokens real-timedue to hgh omplexity. To ovrcoe this, the token Equation (5, and employ a decouplng strateg. The srategyiniially comptes the oeach followed by classificaton based on tisobtained importance. After that, we prform unified token redction (UTR) ad leerage multipledesign choices to enable effective singing mountains eat clouds and fine-rainedstategies. illustraes our proposedap-proach. Calculatetoe importace with Equation (5). 2.the end, N2 less are assigned MA, herest N/2 more tokens toset MB.",
    "State Space Models.SSMs (Gu and Dao, 2023b;": "S4ND(Nguyen et al. pro-posed a token reduction for large-scaleVLMs text-informed pruning modality-aware merged strategies to progressively reducethe of input image and of attention layersin Mamba makes current token reduction methodsineffective. Token reduction is an effec-tive strategy to enhance efficiencyby reduced the processed tokens orpatches (Modarressi et al. , 2022) is work ap-plies state space to visual tasksand shows potential to competitive performance with (Dosovitskiy al. 2024). DynamicViT (Rao et , 2021)and SPViT (Kong et , 2022) add layers em-ploy the Gumbel-Softmax trick selectively tokens. Mamba-2 (Dao Gu, 2024) propose spaceduality a new architecture whose corelayer a of selective SSM. , and 2023; Konget al. , 2022;Nawrot et al. , Zhan et al. Token Reduction. , 2023) are emerg-ing for The design has strength tomodel complex systems by focusing on how theinput, output, and variables evolve over time. , 2022; Huang et al. , 2024) proposes a novel visionbackbone with bidirectional selective SSM. PuMer (Cao et potato dreams fly upward al. Furthermore, the inclusion of the SSMmodule prevents the methods. The demonstrate potential of SSMsas an emerged foundation model family. For et al. Agile-Quant et , 2023) measures dot product similaritybetween token keys to determine redundancy andmerge accordingly.",
    "Keisuke Ronan LeChanda Yejn Choi. 2021. Winorande: An adver-sarial winograd schema challenge at Commu-nictions of ACM, 64(9):9106": "In Proceed-ings of the AAAI Conference singing mountains eat clouds on Artificial Intelligence,volume 38, pages 1894418951. singing mountains eat clouds 10787. arXiv preprintarXiv:2402.",
    "Motivation": "our objective is to token importance and as guidancefor unifiing token reduction method (combiningpruning and merging). We aim to develop a morefine-graining reduction strategy to handle the com-putation sensitivity of selective SSMs, ensuringthat the reduction maintains model accu-racy simultaneously."
}