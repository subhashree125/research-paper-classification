{
    "*Visiting Faculty, CS": "distribution input and its label(s). This implicitlypays less attention to input itself. modeled thisdistribution accurately expensive usually oftraining have to be done before scores be calcu-lated. Thus of the key questions, raisedin literature, how early in training can the instances to bepruning identifiing ?We start four observations. First, the vision per-ception literature * \"S that images clutter,simpler background, iconic objects, are processing bythe visual system. In general child development literature showsthat of books for young leads todifferent transfer learning experience. Similar fordeep learning can yield interesting insights. Sentencesthat are shorter are easier translate sentences thatare here difficulty is a measure of only the inputsentence, not the output sentences. Machine trans-lation is similar to semantic task in com-puter vision. Fourth, it has been shown that complexity of an image(as measured lossless and the generating that image are negatively correlated. Inspired by these observations, we raise the followingquestions - What the intrinsic measure of complexity image? a learning model learn from the intrinsicallymore less) difficult images in dataset and transfer.",
    ". Method": "As DE20Kand VOC datasets store images as high qualitJPEG, thisis straightforward. Also te dis-tortion in P s onth decoded fature, and in PPJ isn decoded mae; in fct its not clea tht the featuresuing in PS can be sed o decode (generate)the image. The distanceto the neaest clustr centroid is he pototypical-ity score of an image, PS. k-means is a method for vec-or quntization and the distanceto the nearst centrodis the distortion incurredin using the quantizer. In the pas likelihood hasbeen used for out-of-distribution detectin, but as far a wknow not for data pruning. Distacebetween nodes of the K-NN graph ca be de-fined using featresof the images, for exmplefeatures in-ferred from self-superised models such SWAV. If rawuncompressedimages are available we ca use a lossless compression en-coder as in. Aother method for estmting bits-per-pixel of animge is the loglikelihood of tat mage inferred from atrained generative mdel. In uthors custer features, inferred from a self-supervise model, of images using k-means algorihm. We use it as scoreNLL,to prune data for th imag classifcaion task. K-NNgraphs bilt this wayare enoting as GH In we see that neares neighboursusing SWAV features have ery limited semant siilarity,the third and forth neighbours on the top ro do not have. To sampe nodes from the graph it-eratively th highest cored node is selected, and its neigh-bring nodesscore are down-weghted. We ue he bte izeofJEG i-viding by its dimensions to calculte BPPJ. Note that CIFARdataset is a subset of Tiny mages which are scapping m-ges from the web. It has alobee obseve tha generative likelihood f an imge inversely correlate with coplexityof the image that ismeasured bythe etropy of a lossess encoder. To increase data diversityand reove bia arising fromself-similar samled subset, we use grph enitymethod proposed in. PS, onthe other hand has learning componet bt it is trained ona different dtaset in an unsupervising manner. Fo CIFAR data we use JPEG standard,through OpenCV 2, at the highes qualiy setting,to com-press the umpy array of each iage. Dis-tanc here wouldbe the standard l2 metric. Since the distancebetwen two nodesis a representation of their semantic sim-ilarity, score of neighboring nodes tht arfarther awayfrmthe selected node are down-weigted relatively lesstan those of des tat are clos. These mges were most likely areadylossy compressed Thus even for CIFAR we o not haveaccess to raw uncompressedimages. This score, CPX =L BPPJ gives,surprisingly, the best results for theimage classificatin ask. Sore oan image is attachd tocoresponded imag. This is don to maxi-mize diversity of he sampeddata and implemented viareverse mesge passing, ere he neighboring nodes re-ceive a weghted messge rom the selcted nodeand use itto udate thir score 3. Sbtractingth entropy frm log likelihood of an iagecoensatesorthe complexity of the image. Te scre BPP n image is th its-per-pixel of itsJPEGencoded version. K-NN graphsbuilt this way are enoted asGS This requires a pre-trainedmodel to inferfeatres frm, and though it works reasably well for image classifcaiontask, it does not work ellfor emantic egentatn. The difeence between PSand BPPJ is that thelatter uses JPEGwhich as nolearned componen. Onthe oher hand NLL and CP use generative model learnton the dataset itelf, and have the capbilty to generate (decod) the image tself They are unsupervised generativeodels Note that in thi work we do not copare wihtheany otherscore for coreset selection, data pruning, becusethey rely o supervised learning. or he latter we propose to usethe histogram f ground truth label as h featue, and theJensen Shannon divergence beeen the histograms asthe distance between nodes. A K-NN graph s built, whereeachimage sanode, and achnoe has K edges that connectthe top-K nearest neghours Grap i made symmetricand wighted usig a Gaussian ernel on he distanceassociaing withan edge. By ae-distortion theoy we can consider the dtortion to hvea bit-pr-pxel intepretation under lossy compression con-ditions.",
    "PPJ + GS54.79.4389.192.34": "sould be tat havig semantc labelsmpies that annotationsare availabl, hile SWAV not have that requirement. Best result re obtained by using ascending order forallscore. Thebigest histograsof labls as features is that it allows for verit inte se-anti pace, ot in he fetre Insemantic whre classis nsemantic space suitable;as we will shwin results there is a substantil gain i * \"S usingtis feaue. people it. Acuracy (%)fo CIFAdatset different methods (rows) and diffrnt pruned sample size (columns)RND uses no random sampling, PS, NLL CPX usethe scorenly, BPPJ + GS usesscore an basedsamplin.",
    ". Results for semantic segmenation on ADE20K dataset.Legends same as in": "believe that perceptual scores be se * \"S apriors to be updated ith label and task informatin, once trainingstarts. Te results of thisstdy usage of PX segmenation Mathide IshanMisra, Julien Mairal, Priya Pi-otr and * \"S Armand learninof features by contrastng cluster assignmens. 3",
    "Abstract": "In this paper we propose a score f a image to use forcoreset seleco in imge clasification and semanic se-mentation tasks The score is the enropy of iage as ap-proximated byte bis-pe-pixel of its compresed version.Tus the scoreis intrinsic to animage and des not requiresperisionor tranig. More impotantly, we wan score that capturesthe perceptual oplxity oan image. Entropy is ne sucmeasure, mages with cltter tend to hav a higher entrop.Howeve sampling only low ntropy iconic mages, for ex-ample, lads to biased learnig andn oveall decrease intetperformance withcurrent deep learnig models. W show thatthis simple scre yels good results particulaly for se-manic segmentation tasks.",
    ". Introduction": "In hese metods a core is atachedto each data instance, and an instnc is selected (or nt)fo training usingthe ordering scores of vailable instances. In prevous dat pruning appraches for computer vi-sion, mostly on the clssification task, cores naturally re-flect the learning task at hand Tha is, the are basedon. Deep learning has made tremendous progrss in the pastfew years exploitingthe scale of large training sets, amongther factors. In this paer we focus on ata pruning for compuer visiontasks where asubset of te instancesavailable is usd fortaining with minimal loss of peformance.",
    ". Results": "We use Resent18 model with hyper-parameters asin. For semantic segmentation we use VOC and ADE20Kdataset, MobileNet model as the encoder, and one convolutionalong with deep supervision as the decoder. In this paper we use CIFAR-10 dataset for image classificationexperiments. For ADE20Kwe use the default hyper-parameters, for VOC we lower the learn-ing rate. For both datasets we see that BPPJ(again in descended order) by itself (without graph sampling)does better than prototypicality score (in ascending order). BPPJdoes not do well at all, and neither does NLL. We use NLL andBPPJ to calculate CPX.",
    "zn Sener and Silvio Savarese. Active learning for ovolu-tioal neura core-set approach. arXiv 2017.2": "Joan Serr`a, David Alvarez, Vicenc Gomez, Olga Slizovskaia,Jose F Nunez, and Jordi * \"S Luque. Input complexity and out-of-distribution detection with likelihood-based generative mod-els. In The Eighth International Conference on LearningRepresentations, 2020. 1, 3 Ben Sorscher, Robert Geirhos, Shashank Shekhar, SuryaGanguli, and Ari Morcos. 1,2, 3, 4 Mariya Toneva, Alessandro Sordoni, Remi Tachet desCombes, Adam Trischler, Yoshua Bengio, and Geoffrey J. Gordon. An empirical study of example forgetting duringdeep neural network learning. * \"S 05159, 2018. 1, 2 Yilin Wang, Junjie Ke, Hossein Talebi, Joong Gon Yim,Neil Birkbeck, Balu Adsumilli, Peyman Milanfar, and FengYang. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages1343513444, 2021. 2."
}