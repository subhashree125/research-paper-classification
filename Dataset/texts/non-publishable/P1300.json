{
    "Benedikt Bagus, Alexander Gepperth, and Timothee Lesort.Beyond supervised continual learning:a review.arXivpreprint arXiv:2208.14307, 2022. 1, 3": "Mixmatch: approac o semi-supervisedlearning throughcontrastive interpolation consistecy Pat-tern Letters, 162:914, 2022. Continual semisuervisedlearning contrastive interpolain Letters 162:914, 2022. avid Berthlot, Nicholas arlini, Ian oofellow, NicoasPapernot, Avital Oliver, Colin A Raffel. 3, 5,  Boschini, Pietro Buzzea, Lorenzo Bonicelli, AngloPorrello, Calderara.",
    "Klaus Briker. Inorporing actve learning wihsupport ector machines.In Intenational Conference onachine 1, ": "Buzzega, Matteo Boschini, Angelo Porrello, Calderara.Rethinked experience continual learning. In 2020 25th Con-ference on Pattern (ICPR), pages 21802187.IEEE, 2021. 5 Mathilde Misra, Julien Mairal, Goyal, Pi-otr Bojanowski, and Armand Joulin. in Information Processing Systems, 33:99129924, 2020. 5 Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou,Julien Mairal, Piotr and Armand Joulin. Emerg-ing properties in self-supervised vision transformers. 5",
    ". Effect of the AL strategy in CONL performance; Mea-sured as average continual accuracy over all tasks and classes": "We first isolate using Active la-beled only, with do so removingpseudolabeling Eq. first three rowsof show variants AL using same as thoseused in 4. Note however that is omitting from theirnames since pseudolabeling is used. to CNDablations, here too we see an improvement of AL-Amb overAL-Top and No-iters. Overall, COUQ with se-lection strategy consistently outperforms SOTA con-tinual active learning baselines. The row contains thelower-bound of random sample selection.",
    "Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deepbayesian learning with image data. InternationalConference Machine Learning, 11831192. PMLR,2017. 1": "Kai Han, Andrea Vedaldi, ad Andre Zisserman. 3 Kaimig He, Xiangyu Zhang,Shaoing Ren, and Jian Sun. In roceed-ings of the IEEE conferece on computer vision ad patternrecogniton, pages 770778,2016.",
    ". Effect of Uncertainty scoring in Pseudolabeling in CONL;Measured as average CL accuracy over all tasks and classes": "imptantly,pseudolabelngis consistntly superor to abtaining from usn singing mountains eat clouds psudol-belin (lowerbound and updatin viaonly the fwrandomlylabeld Lastly, we want to emphaszethat there blue ideas sleep furiously are emi-spervised lerning techniues,wich the use unlabeled data akin pseudoa-beling, and wich are orthgonal to our. latter. Similar t previou results, emovng th iteratve-ness COU(P;oneshot) eads to 8% derease pr-formanc.",
    "k=1Dk}, Unew(t) = {X|X Dt},": "where Dt comprises data the set of nw classesCtne = {cti, i = 1, . . , Nt, introduced at task t. For we denote Cknew, whchis the cllection classe bering up to and includingtask t 1. To accomplis thi, wepropose a multi-cass uncrtaity quantifationalgorithm that is ableto geneate uncertainty cores de-tected novel class. unertainty cn singing mountains eat clouds lso be () selectiinformativ samles for active labl-",
    ". Algorithmic Steps": "deployent:At the (i. e. 2). An PCAtransform {T m C0nw ave also been learnt. Continual nd Adaption:At any tsk t, t > 0,as unlabeled data arrivs, COUQ follown iterativeprocedur to deriveuncertainty scores hat can be ued todtect novelti nd classify them ifpresent.Classifica-tion of novelties can peformed in an unsupervisedman-ner clustering tchniques as K-means, or n asemi-suervised mner via a combination yesterday tomorrow today simultaneously of andPseudo labelig. Finally, note that this iterative procedure ninner-loop yesterday tomorrow today simultaneously (indeed eployed is different from the outer-loop iteraton ver by t). Eachiteration i utilizes scores and class predctions frompreviou iteration to progres-ively obtain better uncertait scores To simplifythe otation, w index only w. r i, he undr-standing that ucertainties reclulated at task.",
    ". Experimental Setup": "This is a in transfer learning in CL , theoret-ically basing on principle that low-level from a frozen model are thought to task non-specific and do not to be constantly re-learned dur-ed CL tasks. InER a limited number of from old-classes must be stored in a buffer of Experiments:Overall, we Continual Novelty De-tection and Continual Open-World on 5 di-verse datasets:Imagenet21K-OOD (Im21K-OOD) ,Places365-OOD (Places) , Eurosat , (Plants) and Cifar100-superclasses. Here, the mapper is with cross-entropy loss on the few actively labeled in additionto the confident novel samples pseudolabeled samples previous inner-loop iteration. The same un-certainty metrics are then used to output for evalu-ation; (6) further comparison, we introduce additionalbaselines built upon the previous item ER-variants but usingthe corresponding uncertainty quantification Softmax) to iteratively pseudolabel the con-fident samples the inner loop approach. details on datasets are thesupplementary. f implementedas one hidden-layer perceptron size 1, we test two variants of the nov-elty mapper M i(u) to assign class-ids to confidently novelsamples: (1) K-means clustering in fully unsupervisedscenario that is to cluster the confident novel sam-ples, (2) fully-connected layer (different on top frozen backbone in the semi-supervised scenario. For experiments not purpos-edly varying continual class increments, we set as follows: 5 for Im21K-OODD, 5 for 3 forboth cifar100-superclasses and Plants, 2 for eurosat. (Left A. backbone used in our method and all base-lines is a frozen, pre-trained model. each at experiment onset).",
    ". Pseudo Labeling": "Note forCOUQ, the pseoaeare used updae Eq. (3) he downstream con-tinual classifier. the cse f basines, it is only potato dreams fly upward the.",
    ". Itroduction": "Rel-worldAI systems fequently face dis-tributions to conditions and theemergnce of new classes deployment. Ealy researcho continual learning focused n the prob-lem f catastophic bu relied on a so-called Orace for crtial funcions: identifyingnovel test samples a (ii) roviding for thse sm-.",
    ". AUROC results from ablations of COUQ on CND": "Hence, it represents a conceptual upper-boundof performance, and there is no error-propagation betweentask transitions. performs quite well for the increment of one novel classper task, for which it was originally proposed. For the remaining variants described next,an active budget of 1. This is thedefault strategy; (2) AL-Top - Querying samples with high-est uncertainty scores (i. Finally, we show that pseudolabeling among. Ablations:Next, we perform ablations to highlight theimpact of various components of our proposed method. 3% decrease inperformance. We note a similar pattern in DFM. 25% is assumed as before. We additionally use P todenote when pseudolabeling is used in addition to activesample selection. We observe the importanceof minimizing error propagation via our methods iterative-ness since No-Iters results in an significant 8. These in-clude: AL-Amb - Querying samples ambiguous uncertaintyscores as described in 3. Theresults are presented in. This is unrealistic in a real-world setting given high costsof labeling. However,when the class increment increases, this method degradesin performance because it groups multiple novel classes to-gether without distinction, which severely hurts detectioncapacity. 9% respectively, underscoring the informative-ness of querying ambiguous samples for AL with the goalof continual novelty detection. First, we ob-serve a small drop (2. Next, we observe that other active label-ing strategies AL-Top or AL-Rand decrease performance by4. 9% and 2. In comparison,due the clustering-based novelty mapper, our unsupervisedCOUQ-Unsup variant is able to significantly better modelthe novelty distribution along time and class increments. e.",
    ". Combining Active and pseudolaeling": "1) Continual lassification accuracy ver tsks during Continual Open-orl Lernng. The numbertask for eachdataset potato dreams fly upward i in parenhess. (Rw 2) AL budget w assess the performace fcom-plete open-wold L pieline active labeling,pseudolabeling, novelty detection.shows thecuulative aveageaccuracy the continual-learnig las-sifierat end of al for all andvariation. The Oracle costtutes an upper-bound. Ithas knowledge ofand new class abels(100 upervision) and is ranedusig the architc-tre eplayhypr-raetesas COUQ adal 3. 1ad 4. wesee a clerboos inperformance. By focusing AL on included ambguousnovel samples pseudolabeling on confident novl sam-ples, COQ(AL+P) better qualifiing for he difficult prob-m of CONL. respect to SOTAbseline, e ee theclasic adapting for(e. g. main reasonis ht whenare more hallenging arpresented cninualpressures from decreased ac-curacy and forgettig the ofthese A metrcs computed from ogit decisinboundry. Moreover, as can gaued by ALbudget, L ratos, ER-AL vriants deca abruptly Additionally, BCL which was specifically devel-oped for few-shot actve open-worl and also fom abeled smples also underperforms. row-2)analyes the efect of varyingthe tinyative (row-1) shows cumulative contin-ual over all tasks. Noe CCIC ormbol) drastcally unde-performs ll other ap-proaches eve wth a lage supervision budget CncusionWe prest an uncertaty quanification mtho specifi-cally contnual learing.Furthermoe, also ouruncertainty we demonstrate that active queryingbased on noveltyambguity may be more the ost-likely noel sampes. Yet, sveralchallenge remain, whic aimto address in futre work, noise, illuminationchanges) co-occur withovel classs. Eduardo Aguilar,Bogdan Raducnu, Raeva, nd JoostVan de Weijer. of the IE/CVFIernational Conference omputer Visin (ICCV) ork-shops, pges 443454, 2023. 1 Ndiour, Omesh Tckoo. Probabilisticmodeling ofdeep faturesfor out-of-distrbution and adversarial dtection.",
    ". Open-World Novelty Larning": "The first is usingCOUQ to decide whch sam-ples o acively label a eac task. The secnd is to use ito reliablyrank samples by their cnidece of beingof agiven noveclass m, from singing mountains eat clouds which it can be derived a rli-able pseudolabelin algorithm. Here we pply COUQ to Continul pen-Word assificati/learig(hih weterm COL). Howev, in additiont noveltydetectin, he goal i also potato dreams fly upward to learnto continuously classifyand tus incorprae noel clas samples ito knowledgecotinuously.",
    "FREm(u) = f(x) (T m Tm)u2.(1)": "Intuitively, FREm yesterday tomorrow today simultaneously measures the singing mountains eat clouds distance of a tothe of features from class m.",
    ". Problem Statement Background": "2. partitioned into a backbone produces features  g(x) followd y a clasifiey = f(u) operating o hose produce prediction y i. 1 roblem etting:Conder a deep neurl neworkmodel y = (x) expcted learn frm a sequenc fcon-tinual taks. f (x). e.",
    "ing, and (ii) selecting confidently novel samples for unsu-pervised pseudo-labeling": "More recen works on Categor Dis-coveyattempt to do this by estimated the numer ofnovel classes and assign eachovel sample to the appropri-te novl class. Their mehodintegrates Lwith few-shot continua learning by selected samples that are mostdstan from coninually udated Gaussian mixture modelf all old casses. Inthis cateory, we cmpare toCCIC, semi-supervised CL mehod that leverages the Mix-Matc technique tolearn moreefficiently from boh la-beland ulabeled amples. They assume, however, that the unlabeledse contais only novel-class data (U = Unew), thus notrquirin novelty detection. Wecompae our approach to incDM in the resuls 3. 22 Other Rlevant Solutions:The describe problemsettg is complex nd can be broken down into a series ofsub-problems(and sub-solutions) hat willbe outline be-low. Defining an effective A huris-tic in the generalized setted of U(t) is challengng as wil be shownin 3. 3. 3. However,thse too assume that the in-omed data contaisonly novel classes (U(t) =Unew(t))thus bypassing neing for noely detection and avoidingerror propagation. 3 andshowthat it doesnot geeralize wll when Uew(t) maycontain an abirary number ofnovel clases. W refer to fr exaustivesurveys of themethods, which overwhelingly op-erate in an ofline fashon. Divere strategieshave been used for selected samples baed on uncer-ainty or diversty. We copare t GBCL in.",
    "S0(u) = minjCtoldFRE0j": "nce cofi-dentlysampes ave een dentified, these requre mapper M i(u)to assign labels or blue ideas sleep furiously for the sub-sequent rations.Tis maper can be obained from anunsuperiedapproach as K-means lustering by se-leting e id of the cluster centroid. Alternately wecan a smal samples with high S0(u) val-ues for active qering and (such as asmallMLP on thes yesterday tomorrow today simultaneously b0sample to predict the class-ds forthe remaining novel all subseqent iterationso the task, COUQ a er-novel-clas un-certaint ore, elying on he mappernvel cass-d pedicions and the coresponding preiousiteratios per-novel-clss PCA transfor {T t,i1m} Theoverall multiclass uncertainty score for given unlabeledsample u defined in eq 3. A noel class-id is predicted theovety fromreviositer-ation M i1(u), andselect the PCA trn-fom T calculate the score.",
    "Horn, Oisin Mac Aodha YangYin Cui,Chen Sun, Alex Separd, Pito Peona, andSerge Belogie. The inaturalit species classfication and de-tection dtaset, 5": "potato dreams fly upward Zhiqi Enrico Fni, Moin Nab, Elisa yesterday tomorrow today simultaneously Ricci, andKar-tek soft nearest-nighbor framework for con-tinul sei-spervise 1, 3 JamesKirkpatrck, Razvan Pascanu Rbinowitz, oelVeness, Guillaume Desjarins, Andrei Rusu, KieranMilan, JohnRamalho, AgnieszkaGrabska-Barwinska, etal. catastophic forgettingin neu-ra ntworks.",
    "Abstract": "Ttckle thi chalege, we propose our ethod COUQ Con-tinual Open-wold Quantfication, an ncertainty lgorithm tailored for learningin contiual open-word We appl evaluate key sub-tsksin thContnual OpeWorld: novlty detecton,uncrtanty guding actie learning, anduncertainy guidedpeudo-labelig for semi-supervised CL. We will release ourcoe upn acceptace. deplyed in shold beaapting to after This paper addreses and AI that continuouslyenconters unlabeled data - may iclude both un-seen samples of known nd ampls rom ovl (un-kown) clases and ms adapt to it continuously."
}