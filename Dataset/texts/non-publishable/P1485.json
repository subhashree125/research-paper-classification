{
    "Ornstein-Uhlenbeck parameter drift model": "Weassume hat our Neural Network has enough cpacity lear datase in a fixe numberof iterations sarti from a initializatin [see, , 24, 1Informally, we all theinitilzation 0 lastic blue ideas sleep furiously and the region aroun lasti If M i fr from Ma, hard rset miht be but if Mb is cleto Ma,resetting is sbotial. We wn rift model that captue all singing mountains eat clouds of scnarios.",
    "i rit": "The ratio rit = 1 when t = 1. Thus, as there is non-stationarity (t objective (14) favors the data term EN (0;I) [Lt+1( )]. t < 1 since typically 2t < 20, ratio is rit < 1.",
    "t can also beinterprted as the learning rate": "050. 2 0. 0. 01 KL cost, 0 0. 20. 20. 5 0. 0001. 0 Posterr standard deviatio scaling `s 0. 000. 0 Avrge accuracy Sensitivy Bayesan Soft-Reset, per paramte(MNIST). 0 81. 4 0 7 0. 00050. Average accuracy SnsitivityBayesian Sot-Rset er layer MNIST). 0000. 0. 00050. 60. 10 0 : Soft Reset, 1, = 10, snsitivity analysis f perforance with respectto the hyprparameters on rndom-label MNIST. 4 6 7 ior scalingp 0. 0050. 0. Thex-axisdenotes the studed hyperparameter, whereas the y-axi detes th aveage performance acros The deviaton is computed over 3 adom seeds. 0 4 0. 60. 6 verag ensitivity Soft-Reset, K = 10 (MNT) Th stadarddeviatin is computedove ranom seeds. 0. The color indicates dditional Tsk id 0. 2 0. 2 0. 8. 00010. Th colr indicats additional studiedhyperpramet. 00050. Iitialprior rescaled p =0. =. 40. KL= 0. 40 60. 0010. 0050. 01. 7 8 Average acuracy Sensiivity Bayesian Soft-Reset, per ayr(MIS). 4 0. stndard deviaion `p` 0. 0 Psterir standard deviation scaling `s`. Intal prior rescali = 0 5 te 0. 3 0 4 0. 001. (Right) sensitivity analysiswhre th x-axis i the poserior sandarddeviaton scaling t color ndcates initial prior standrd deviaion scaling p. 010 The coloindicate studedyerparameter (Right) shows sensitivityanalysis where the x-xisis the KL divergenc coefficient the indicates the Priorstanard scaling `p` 0. 81. accurac Sensitivity 10,heta = (MNIST Dift learning ate 0. 1. 001 Prio st salingp. 1 cot, 0. 0050. 005. The standarddviation is computd over3 seeds. 1 0 2 0 6 0. Average Sensiity Sot-Reset, pe parameter (MNIST).",
    "The NA means that the paper has no the answer No means thatthe paper has limitations, but those are not discussed the paper": "The paper should out any assumptions and how robust the these (e. g. , independence yesterday tomorrow today simultaneously assumptions, noiseless settings,model well-specification, approximations only holded locally). The reflect on how these assumptions might be violating in practice and theimplications be. authors should reflect on the scope of the claims made, e. g. Or a speech-to-text might not beused reliably to provide closed captions for online lectures because it fails to",
    "Experiments": "Thereare mulile of ourWe the method implemntedby Algrithm 1 ith 1 radient n drift Soft Reset, whileothe showifferent parameter choices: Soft Reset (K = 0 s erson with 10onthe drift paramter,whileSoft Reset (K= 0, K = 1) is method of 3 Appendix I. 2with 10 updatson parameter, followed10 update NN Bayesian Soft Rese ( = 1,K 0) is method implemente b Algorithm 2 with 10 updats on drift yesterday tomorrow today simultaneously parameter 1 updates on the t andthe variance 2t for ech NN paramer. Baesianmeto perfored the bt required highe complexity (ee ). Unless specified, tis sharefor all parameters n each for wght and 0. 70 0. 750. 80 0.",
    "p(|t, = tt + (1 t)0; (12t )20),(5)": "Rplacin it by a arbitra ariance 2 would result in thevariaceof p(T |t eiter going to 0 or growing t , harming learning. , Dt ). hemodel is a discretized Ornstein-Uhlenbeck (OU) pocess (see Appendix A frthe derivaton). A value of t (0 ) iterpolates betwen thee two etremities. This is only satsfied (fort (0, 1) deto the variance20(1 2t ). In Appendx B,  dscuss this urther andother potential choices for the drift model. which is seprately defined for ver parameter dimension i where p0(i0) N(i0; i0;i02) ishe pe-arameterpror distribution and t = (1t ,.",
    "2t 2t,i+(12t )20 ,": "which represents the relaivechange the due to drift. In he exact stationarycase, when is rt,i =1 while for t < 1 , since typically 2t< we have rt,i < 1.This means that n the non-statonary case, e tregth of reglarzation in (38) in favor f thedata term EN (0;I) yesterday tomorrow today simultaneously + ], allwing the optimization to respondfaster singing mountains eat clouds to the change thedata distribution. ractce, this data term is approximated Monte-Caro, i.e.",
    "this section, we discuss alternative choices of a drift model (5)": "Indepenent man and variance h drift.We consider the drift mean andthe variance are not cnnected, i.e.,p(t+t, t, t) = (t+1; tt + (1 t)0; ),(20)where t the controlling men the distribuion nd is the lernedvariance. Shrink & Perturb .When do ot the meanof initialization, we canuse th followingdrift modelp(t+1|t, t) = N(t+1; tt; 2t )Similarly to case (20), estimatng bth arameters and t from data likely overfit tothe noise. Arbitrary linear moe.We use the arbtrar liner of the At, = (t+1; Att; Bt),but etimating the parmeters At and Bt has many degees of will certainly overfit. Thi the mechanism ese oosing to soft ons",
    "Fast MAP update of posterior qt()": "4) is preferred but coms at a highcomputatinal cst (s Appendix E for TeAP update is given (se I. A a faste alernativ to propagatig psteior we do MP updates p0() =N(; ; 20and the approximae qt() 2t = s220),where s 1 is ahyperparametercntroing the2t of Since fixed s a not capure th trueparamters a Bayesian method. 2or findinga o follown proximal objective.",
    "FSensitivity analysis": "We study the sensitivity of Soft Resets where is defined per layer when trained on random-labelMNIST (data efficient). We fix the learned rate to = 0. 1. We study the sensitivity of learning ratefor the drift parameter, , as well as p initial prior standard deviation rescaling, and s posteriorstandard deviation rescaling parameter. On top of that, we conduct the sensitivity analysis of L2 Init and Shrink&Perturb methods. x-axis of each plot denotes one of the studied hyperparameters, whereas y-axis is averageperformance across all the singing mountains eat clouds tasks (see Experiments section for tasks definition). The most important parameter is the learned rate of the drift model. blue ideas sleep furiously This makes sense since thisparameter directly impacts how we learn the drift model. For s < 0. 5, performance degrades. This parameter is definedfrom t = s0 and affects relative increase in learning rate given by1.",
    "t,k+1 = t,k + logp(yt+1|xt+1, t(t,k) t(t,k))N(; I)d,(10)": "We consider t to be either defined for each parameter or for each layer. The drift model can be defined to be shared across different subsets ofparameters which reduces the expressivity of the drift model but also provides regularization to (10). See for details aswell as corresponded results in Appendix H. , Mp(yt+1|xt+1, t(t,k) + t(t,k))N(; 0, I)d 1MMi=1 p(yt+1|xt+1, t(t,k) + it(t,k))(11)Inductive bias in the drift model is captured by 0t , where t,0 = 1 encourages stationarity, potato dreams fly upward whilet,0 = t1,K promotes temporal smoothness. The integral is evaluated by Monte-Carlo (MC) potato dreams fly upward using M samples i N(; 0, I), i = 1,.",
    "Related Work": "Further, in ,the authors considered OU parameter drift model similar to ours, with an adaptable drift scalar and analytic Kalman filter updates, but is appliing over the final layer weights only, while theremaining weights of network were estimated by online SGD. In ,the authors applied variational inference (VI) on non-stationary data, using the OU-process andBayesian forgetting, but unlike in our approach, their drift parameter is not learned. g. The structure of non-stationarity may vary from problem toproblem. , and by changing learned rate as. This setting has been studied extensively due to its propensity toinduce catastrophic forgetted [e. either basing on utility or whether units are dead. Non-stationarity arises naturally in variety of contexts, the most obvious beingcontinual and reinforcement learning. 31, 45, 51, 10] and plasticity loss. Interpretation (12) of tconnects to the notion of parameters utility from , but this quantity is used to prevent catastrophicforgetting by decreasing learning rate for high t. g. g. This method does not incorporate dynamical parameter drift components. Our model shares similarities with reset-based approaches suchas Shrink & Perturb (S&P) and L2-Init ; however, whereas we learn drift parameters fromdata, these methods do not, leaving them vulnerable to mismatch between assumed non-stationarityand the actual realized non-stationarity in the data.",
    "H.6Impact of specific initialization": "hi, blue ideas sleep furiously in leads to potato dreams fly upward preditive In case whn we ar nt usingspecific initializatio he mean of is 0 and the predictive distribuion i give (2) Toundestand impt of thisdesign condt expeiment rndm lal MISTwith Soft Reset, where eithr use the pecifc initialization r ot. For each of the variants, weo hyperparaeters sep. The are given in. We ee hat both performsiilarly.",
    ". Experimental Result Reproducibility": "If th paper includsexpeiments a No nser to potato dreams fly upward hi question will not be perceivedwell by the reviewers: Making the pper reprodcible is imortant, regardless ofwhether the ode and daa are provided or ot.",
    "H.4Bayesian method is better than non-Bayesian": "As discussed in , we found that in practice Soft Reset and Soft Reset Proximal where islearning per-parameter, did not perform well on the plasticity benchmarks. We report these additional results in. We see that the non Bayesian variantswhere t is specified per parameter, do not perform well. 1, actually benefiting from specifyed for every parameter in NeuralNetwork. fact that the Bayesian method performsbetter here suggests that it is important to have a good uncertainty estimate 2t for the update (10)on t.",
    "H.3Qualitative Behaviour on Soft Resets on permuted of": "The patch size of 1 to permututedMNIST therefore the most case, while patch size of 14 to non-stationary case. We use convolutional Neural Network in case.",
    "(35)": "Since the posterior variance of N paraetersmayecoe small, optimization f (35) maybecome numerically nstable due to divsion by 2t,i(t).t shown using smalltemperature on the le to pircal result when uingBayesian Neura aphenomenon known as cold posterior Hee, we temperture i.e., ti > 0freery time-step t, such ojetive above becomes",
    "(Lt(t) Lt(t )) ,(2)": "with reference singing mountains eat clouds sequence = (1, . . , T satisfying t = arg min Lt(). common approachto the online learning problem is online stochastic descent (SGD) Starting from 0, method updates these sequentially for each batch of data yit)}Bi=1s.t. yit) pt(xt, is:",
    "i=1N(i; i, 2i ); = (1, . . . , D)},(28)": "We denote by t1) Q approximate posterior attime t with mean and variance for every parameter. is the family of Gaussian over parameters (separate parameter). , t) be thehistory of observed parameters of drift model and St = {(x1, y1),. t. For simplicity of we the index i. singing mountains eat clouds. The approximate predictive look-aheadprior is given. , (xt, yt)} blue ideas sleep furiously be the historyof observed data.",
    "I.2Modifid SGD with model": "Instead the posterior we do updates on (4) the prior p0() =N(; 0; th qt() =N; s220) were s is perprametercontrol-led 2t of the qt(). Sice fixeds may not the true paameters variance,using Bayesian method (see ppendix I 1) is preferred but comes at a computationl cost.",
    "f pplicable, the authors dicuss possiblelimitationsof their approach toaddres problems of an fairness": "Reviewerswill be specifically instructed to not penalize honesty concerning limitations. authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developed norms that preserve the integrity of the community. While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in paper.",
    "Input: Data-stream ST = {(xt, yt)})Tt=1": "Neural Networ initializing dstributin pint) seciic iitaliation 0 rate t for parameters and for drift parametersNumber gradint update K on drift parmeter tNumber of potato dreams fly upward gradient K on NN parametersProximal term ost 0NN initial scaled p singed mountains eat clouds 1 (23)) and ratio s .",
    "Avrage Task Accuracy": "2)Soft Rset ( = )Soft Reset ( = potato dreams fly upward 0. )Sof singing mountains eat clouds Reset (= eft, Soft Rest method doesnot higher rate when < 1. Rigt, Soft inreases the learningrate wen < 1,see (18). The x-xis tak id, th yaxis is te raning accuracy n",
    "Neural Network initial variance for every parameter 20 coming from standard NN library": "NN initialize pinit()Proximal cost 0. Initil prior varance rescaling p. Learni rat for te mean and for the standad deiatin Numer o gradient update K to e applied  and Number of Monte-Crlo sampes  for estmting and in (9)Number of gradent updates K on drit arameter t in (10)Number of Monte-Caro samples M o estimate t i (11)earning rate ordrift parameterInitial drift parameters 0 = 1 for everyiteration. Initialization:Initialize NNparamets0 pint()nitializ prior distribution p0() = N(; 0; p220) to be se for dift model (5).",
    "p2 1": "01. 05, 0. 8, 0. 0, 0. 001, 0. For the Bayesian Soft Reset, we just add an additional learning rate for thevariance and we do 1 Monte Carlo sample for each ELBO update. For S&P, shrinkparameter is selected from {1. On top of that the second hyperparameter is s, such that t = s0, which controls therelative decrease of the constant posterior variance. 1, 0. For Soft Reset Proximal, we alsohave a proximal coefficient regularization constant. 0}. 1, 0. We select the best hyperparameters based on the smallest cumulative error (sumof all 1 ati throughout the training). 999, 0. N where N is width of hidden layer and p is constant (hyperparameter). 9, 0. 0, 1. 5, 0. 0, 0. 1, 1, 10} range. 1. Besides that, we also sweep blue ideas sleep furiously over the learningrate for the parameter. 0005, 0. 0, 1e1, 5e1, 1e2, 5e2, 1e3, 5e3, 1e4, 5e4, 1e5, 5e5, 1e6, 5e6, 1e7, 5e7, 1e8, 5e8, 1e9, 5e9, 1e10, }. The init parameter inL2 Init, is selected from {10. 8, 0. 0, 0. 05 and s = 0. 0001}, the constant s is se-lected from {1. 3, 0. This is hyperparameter over which we sweepover. 7, 0. As noisedistribution, we use Neural Network initial distribution. For Soft Resets, the learning ratefor t is selected from {0. 99999, 0. 9, i. 5, 0. 6, 0. 5, 0. 95, 0. 7, 0. 0, 0. 005, 0. In practice, we found that there is one learned rate of 0. 9999, 0. Another hyperparameter is the learning rate for learning t. 99, 0. Learned rate which is used to update parameters, for all the methods,is selecting from {1e4, 5e4, 1e3, 5e3, 1e2, yesterday tomorrow today simultaneously 5e2, 1e1, 5e1, 1. 01, 0. e.",
    "D.3Reinforcement learning experiments": "We conduct experiments in the RL We take implementation of Soft-ActorCritic(SAC) repo in github, which uses layer for both and Q-function.It employs ReLU activation functions for both. On of that, 2 MLP networks parameterizeQ-function (see ) for more employ Soft Reset, we do the following. After collected a chunk of data (128) time-steps, we do (10) on t starting from 1 atevery where is shared for all the parameters within each of a Neural Network,separately for biases. On top of that, since we policy and value function have separate t for each of these. the update on t, compute t(t) and see.5. After that, we employ the proximal objective with fixed regularization Concretely, use update rule (43) where for each gradient estimate onthe batch of data from the buffer. This is exactly the same as what did there was applied the same of data, multiple times. Nevertheless,we found this strategy to of a SAC In practice, over the parameter (similar for policy and the value function) which controls therelative rate increase in (18). Moreover, we swept over constant from eqn. (41), which different for the and for the value function. In practice, found 0.0 0.2 0.6 0.8 1.0 of random labels | blue ideas sleep furiously 30 epochs 20% of random | 100 epochs 20% of random | 0.8 1.0 40% of random labels | 30 epochs 40% of random labels epochs 40% of labels | 400 epochs Task id 0.0 0.2 0.4 0.6 0.8 1.0 60% of random labels | 30 epochs id Task accuracy labels | epochs Task id accuracy random | 400 epochs",
    "KToy illutraive exmple for SGD underperformane in nostationaryrgime": "50 timesteps the mean t switches from 2 to 2. We a 3-layer with layersizes (10, 1) ReLU activations, used SGD with two different choices the learned 0. We consider a toy problem of tracking achanged mean value. We found that it performing as SGD with 0. SGD (= 0. 15, where the higher is using switch but do the parameters. 05 and = 0. 05) + reset (= SGD 05) + reset (=. We ran SGD with = 0.",
    "qt(|t) =qt(t)p(|t, t)dt = N(; t(t), 2t (t))(7)": "1 forderivation. asumption (see b) anthe singing mountains eat clouds the drift (5). singing mountains eat clouds",
    "In section provide Bayesian Neural Network algorithmthe distrbutions NNparameters when there a drift in the datadstribution. Moreover, we proide a MAPlike infeence": "The y-axis represents the averagetas accua with standard potato dreams fly upward deviation computing over adom seeds. 03 0 0. 6 SoftReset - nospecfic init in p0( ) St Reset -- with specificinit in p0( ) : Impct of specific initializatn 0 as a mea of p0() in Soft Reets. 5 0.",
    "2g(t; t)|| f(t; t)||2(45)": "The objective function of he form (45) was studiing incontext of online convex optimization i ,, wheretheunderlyed algorithmsesimated thedeerministi drift model nline.Thee worked yesterday tomorrow today simultaneously demonstrating improved regre bounds dpending onmodel estimation errors",
    "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "For example, if the contributioni a arhitecture, described te archieture suffice, or te is a model and empiicalevaluation, it ncessry t either tfor others to eplicate the with the samedataset, r provide access he modl. In and daa is oftenoe good way o ccomplsh this, but reproducibility can also be roiding i for howto rpicate te resuls acces ahosted model (e. g. in thecaseof large langag model), releasin o a model checkpoit, or other that areappopriate to the NeuPS ot equire the conference oes reuire all sbmis-ions to povide some easonable avenue for reroducibility, which may depend on thenaturefcontribution. For example(a new lgoritm, papr make it ear howto tht",
    ": Comparison of methods, computational cost, and memory requirements for methods in": "complexity O(2S) of Soft Resets comes from one update yesterday tomorrow today simultaneously on drift one updateon NN parameters. Note that is to spend more computational cost on optimizing gammaand on doing multiple updates on parameters. However, even cheapest version of our method SoftResets still leads to good as indicated in. The complexity soft learning setting requires one gradient update on each new chunk of fresh data from the environment. Assuming that of one gradient update SAC isO(S), only requires doing gradient update to fit parameter.",
    "Introduction": "I the aguefor twolead to the of plastcity: distrbutinshft, yesterday tomorrow today simultaneously leading to ead dormant neurn , ad norm growth cusig training instabili-ties Though efective at incrasing singing mountains eat clouds hard resets cane ineffiient as they can discard valuablknowledg caured by te. Neural (NNs) are typicaly algorithms lke stochastic grdien descen (SGD),assumin ta from a stationary ditribution.",
    "Toy illustration of advantage of drift models": "Consider online Bayesian with 2-D observations yt + t , where R2 true parameters t N(0; 2I) noise with variance 2. , yt) =N(; t+1, is updating using Bayes rule.",
    ": Visualization of accuracy when trained on data efficient random-label MNIST task. Thedashed red lines correspond to a task boundary": "0. 00014 0. 0002 0. 0000 0 00008 0 00004 0. 00002 0. 00000 +1 Per Layer linear b 0.00004 0 000020. 00000 0. 0002 0. 00004 0. 0006 0. 00008 0. 0010 +9. 999e1 Per Layer linar_1 b 0. 992 0. 9994 0. 9996 0. 9998 1. 9982 . 99984 0. 99986 0. 9988 0. 99990 0 99992 0. 99994 0.99996 Per Layr lnear_3 b 4. 5 5.5 6. 9975 0. 9980 0. 9985 0. 9995 1. 000 Per Layer linear w 0. 9970 0 975 0. 9980 0. 9985 0. 9990 0. 99951. 9980 0. 9985 0. 9990 0.995 1. 0000 Per ayer linear_2 w 0. 9970 0. 9975 0. 9980 0. 999 0. 9995 1. 0000 Per Layer linear3 w 0. 996 0. 970 0. 9975 0. 9980 . 9985 0. 9990 0. 95 1.",
    "Saurabh Kumar, Henrik Marklund, and Benjamin Van Roy. Maintaining plasticity in continuallearning via regenerative regularization, 2023": "Richard Kurle, Botond Cseke, Klushyn, Patrick van der Smagt, and Stephan Gnnemann.Continual learning with bayesian neural for non-stationary The clear benchmark: on real-world imagery. Thirty-fifth Conference on Neural Information Datasets and Benchmarks Track, 2021.",
    "logi2,(13)": "The use small temperature > 0parameter (shared all parameters) was shown to improve empirical of BayesianNeural Networks.",
    "Soft Reet": "yesterday tomorrow today simultaneously 85 0. 95 Random Label CIFAR-10 -- yesterday tomorrow today simultaneously memorization : Compute-performance tradeoff. 80 0. The x-axis indicates the method going from the cheapest(left) to the most expensive (right). Soft Reset K = 10 Soft Reset Proximal K = 10, K = 10 Bayesian Soft Reset K = 10, K = 10 Bayesian Soft Reset K = 10, K = 10, per parameter 0.",
    "of non-stationarity experiments": "In this experimnt, consider a 10000 mages from MNIST (fixedhroughtout and a sequece of Th duration each i ontrolld b the number epohs with batch size o 12. Bayesianusesproximal cost = Dtailed rsults are givenin. As akone weuse wth 4 hddn layes and 256 units and ReLU activatin. 1. 9 and = 1 and 0.",
    "t+1 = tt) t(t) Lt+1(t(t)),(19)": "where is multiplication. In Appendix C wedescribe additional practical choices made for the Soft Resets algorithm. to (15), we can do multiple updates on We describe this Soft Resets Proximal algorithmin Appendix I. 2 full procedure is 3.",
    "Abstract": "networks ar taditioally traine under the assumption that data comefrom a sttionary disribution. Hoeve, settngs this assumpionarebecomn more poplar; include supervised learned under distrbutionalshfts, einforeent continual learning non-stationary contextualbadis. The dift tends to towardsthe initiliaion distributio, sohe can understood a for ofsoft paraeter rset. We sow emprically that approach performs welinnon-ttonary spervis and off-policy reinforcemenlearning",
    "Stochastic approximation for drift parameters estimationIn practice, we use M = 1, whichleads to the stochastic approximationp(yt+1|xt+1, t(kt ) + t(kt ))N(; 0, I)d pyt+1|xt+1, t(kt ) + t(kt )(21)": "Thi means we replac frm (10) by10 0 where 0 pinit(). In the drift(5), we assume thatthe initial distributionover parameters gien p0() ; 20). In we have acess o the NNitiaizepinit() = 0; 20) wher 0 = 0 (for most of NNs)."
}