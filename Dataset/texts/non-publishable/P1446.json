{
    "Introduction": "Overthe past [NP33], this problem has extensively explored in statistics, machine learning,and computer science. Indeed, distribution testing (also called hypothesis testing) is nowa major pillar of learning theory and algorithmic statistics, with applications in of such as Poisson Binomial Distributions robust learning[DKK+19, DKS16, DK14, SOAJ14, DDS12, DKS16]. framework has also been extensivelystudied privacy [GKK+20, AJM20, BB20, CKM+19, ACFT19, GR18] andlow-memory constraints [ABS23]. One of the most natural and questions in this is: given sample blue ideas sleep furiously access toan distribution can whether p is equal distribution q, or -farfrom it (e. g. in variation This problem has been studiing how we access q: It is calling uniformity tested when q is a uniform distribution, identitytested (or goodness-of-fit) a description of q is known, and closeness testing or equivalence testing) when we only have sample access to q. The primary goal insolving these is to design algorithms that use as few samples possible. been established for discrete distributions p and over domain of sizen: (n/2) samples for testing [GR11, BFR+00, Pan08, test-ing blue ideas sleep furiously [VV17a, ADK15, DGPP18], and + closeness testing [BFR+13,.",
    "Closeness Tester. This is state of art algorithm of [DK16] without any predictoraccess. We refer to this algorithm the standard or the un-augmented tester": "Error Mearemet. a high our algrithm (ented ugmente Testerandthe pror SOTA un-augmnte (abbreviatd as Clseness compute based on the samples requested. bot algorithms threshold the vale of Z t deternewhichcase ar i(eiher p = q o p qTV . Noe ha Z is rando variabe n otase it deeds random samples). Ideally, the disriutin is well-searated in thetwo diferent hythes. Thusin our exerimens, a sample budget, we compute empirical distriution of Z forboth algorihms under bth cases across 100 tials. For aalgorihm, our eormeaurment computsthe dissimilarity ofthe two empirial if te ten we know the algorithm can he two hypotheses. Onthe other hand, if historams of Z in the cases highly ovrlaping, i is he algorithm cnnot ell. giventhe histgrams, calulatethe epirical threshold whch est the. This is donefo algorithm, oursand the classical CloenessThen w measure fraction of data points in thehistogramwhih are misclassfied a the error. Calulaing be empirial threshold alo makes both singing mountains eat clouds ofthe algorithms morepractica thresholds restated asymptotcally cnstant factors, making the theoreticl values inaly,that s get error.",
    "bound for uniformity testing when d d": "procedue for the standardis straghtforward: Aon p, q. f etuns we also eturn blue ideas sleep furiously acet; if returns reject or nccrat information,we reject. nthis sectn, we focus on case where If this isat then predictio does nt theConsiderA as an( = 2/3)-augmenting tester.",
    ": Error vs sample complexity for thetheoretically hard instance (See Sec. 6)": "distribuions similar t ou lower boun in-stances, our potato dreams fly upward augmetedalorithm aieves re-duction in sample complexity to obtan comparableaccuracy thesandard n-augmented algorithm,as in.",
    "(12)": "However, areinformation-theoretically indistinguishable using only samples. We show that there are two (families distribu-tions that any augmented tester has to them with high probability. for a sufficiently small absolute constant we determine later.",
    "4/3( mod 2 ) .(13)": "Here, the role of indicator variable is to reduce one case they are. For our proof, require that and n are either botheven or odd. It is not blue ideas sleep furiously hard to see that is an integer.",
    "[AJOS14]Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda T. Suresh. Sublinearalgorithms for outlier detection and generalized closeness testing. In IEEE ISIT, pages32003204, 2014": "[AKR19]Maryam Aliakbarpour, Ravi Kumar, and Ronitt Rubinfeld. Testing mixtures discretedistributions. In Alina and Hsu, Proceedings of Thirty-Second Conference on Theory, 99 Proceedings of USA, 2528 In Thomas Vidick, editor, 11th Innovations in Theoretical ComputerScience Conference (ITCS 2020), volume 151 of Leibniz International Proceedings (LIPIcs), pages 69:169:41, Dagstuhl, Germany, 2020. Schloss DagstuhlLeibniz-Zentrum fuer Informatik. Differentially testing of and closenessof discrete distributions. [BB20]Thomas Berrett and Cristina Butucea. Smith, and Patrick",
    "The proof of this theorem follows from Proposition 4.1, Proposition 4.2, and Proposition 4.3": "A instance of this yesterday tomorrow today simultaneously is uniformity testing, where q is uniform distri-bution over [n]. Remark 7. In blue ideas sleep furiously 6, upper bound to any arbitrary our tightlower bound is based a hard instance where q is a uniform distribution.",
    ":Run the tester (withot a prediction) withconfidence paraeter and returnthe": "Let f : N be anon-decrasingfunction for A uses f(, singing mountains eat clouds )3 smples when it invoking parmetr andaims for the confidenceparmeter.",
    "[DKN15b] Daniel M. Kane, and Vladimir Nikishkin. Testing identity of distributions. In SODA, pages 2015": "[DKS16]Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Properly learning poisson bi-nomial distributions in almost polynomial time. JMLR. org, 2016.",
    "Abstract": "We demonstate such a predictor can ndeed educe the numberof samples required fo allhree poperty testing A keyavntagef algorims is adaptailiy to the precision f hepredction. by NSF TRPOD (ward DMS-202448) and te Awar. e. Suported by NSawads CF-14846, BUs Hriri Isttute for Computing. We eplore these problems in a seting where aprdicte distribution, osibly deried om histoical data or predictve machine learnngmdels, is availabl. by awards CCF-2006664, DM-2022448 and CCF-2310818. e. they never use moresmles than the standardappraches require,eve if thepredictions proie no meaningf iformatio (i. Furthermore, reulsthatthe peformanc of our on data exeeds worst-case uaranteesfor sample complexiy, the practiality our approach of ths work wasoucted whilethe were the Simos for he heory Computing.",
    "qi = qii [n 1] ,qn = 1": "Note that given a sample from (respectively q), can blue ideas sleep furiously generate a sample from p (respectively q)by outputting that sample blue ideas sleep furiously with n otherwise.",
    "Next, we use the properties of the coupling derived in Lemma 4.4. In particular, we have shown": "6It is worth noting that a pair of valid sets may have some overlap. That is, the algorithm can output answerthat is correct for both of the distributions.",
    "Upper bound for identity testing": "We employ the set of p and qdefined as set of elements [n] where p(x) < q(x),denoted by Sto witness for of p from q p. Demonstrating a small total distance between two distributionsentails proving that discrepancy across every domain subset minimal. In contrast, to alarge total variation distance, one needs to identify single with significant discrepancy. bound relies on fundamental observation regarding total distance: thisdistance is maximum discrepancy between the probability that two distributions subset of their domain.",
    " Caone, Rocc A Servedi. Testing proability distribionsusing conitional samples. Journal on Computing, 44():54061, 2015": "[DDS12]Constantinos potato dreams fly upward Daskalakis, Ilias Diakonikolas, and Rocco A. Servedio. Learning poissonbinomial distributions. In J. and editors, Proceedingsof 44th Symposium on Theory of Computing Conference, STOC 2012, New York,NY, 19 - 22, 2012, pages 709728. ACM, 2012. [DDS+13]Constantinos Daskalakis, Ilias Diakonikolas, Rocco A. Servedio, Gregory Valiant, andPaul Valiant. Testing k-modal distributions: algorithms via In of the Twenty-fourth ACM-SIAM Symposium on Discrete Algorithms,SODA 18331852, Philadelphia, PA, USA, Society for andApplied [DGKR19] Ilias Diakonikolas, Themis Gouleakis, Daniel M. Kane, and Sankeerth and memory testing of discrete distributions. Beygelzimerand Daniel Hsu, Proceedings of the Conference on The-ory, volume 99 of Proceedings of Machine Research, pages 10701106. PMLR,2528 potato dreams fly upward Jun Ilias Diakonikolas, Themis Gouleakis, John and Eric Price. Sample-optimalidentity testing with high probability. In 45th International Colloquium on and Programming, ICALP 2018, July 9-13, 2018, Prague, Czech Republic,pages 41:141:14, 2018. [DGPP19] Diakonikolas, Themis Gouleakis, J. Peebles, and Price. Collision-based testersare optimal for uniformity and closeness.Chicago Journal of Theoretical ComputerScience, 2019(1), MAY 2019.",
    "here the expectaion is over the of the samples": "Diakonikolas et al. Connection to distribution testing:Chan et al. [DK16]used this tester in combination singing mountains eat clouds of the flattening technique to map distributions to distributions overslightly larger domains with the goal of reducing their 2-norms.",
    "[DS1]Ilias Diaonikols, Daniel M. Kane, and AlistairSeart. Sharp or generalizeduniformity In NeurIPS, ges 62046213,": "Learning-based support estimation in sublinear 9th InternationalConference on Learned Representations, ICLR 2021, Virtual Event, May OpenReview. [GKK+20] Sivakanth Gopi, Gautam Kamath, Kulkarni, Nikolov, Zhi-wei Steven and Huanyu Locally private hypothesis selection. PMLR, Kothari. Association Computing Machinery.",
    "/3< ck 21/3 < ck < ck 1": "8A symmetric blue ideas sleep furiously property of a pair of distribution is a property that does not change if we permute the domainelements of two distribution blue ideas sleep furiously via the same permutation.",
    "It is not hard to verify that p UnTV is d, satisfying the assumption made in the statement ofthe proposition. Next, we construct two distributions, p and p": "Additionally, let be singed mountains eat clouds number in (0, ) such thatd = blue ideas sleep furiously (d ).",
    "Searching for the appropriate level": "Wegeeralizthis concpt for other proerties ofdstribution. More a poperty setof distributions, we ay proerty P iff is P. Similato Definition 1. we define the notion of an augmented tester or P as follows: Definition 3. we are given parmeters, , (0, (0, 1).",
    "Indeed, even values of p is still informative of the of p. Therefore,the augmented algorithm can reliably use p perform a suitable flattening": "The repeats itself in for the IP data. In (b), we compare the performanceof using two predictors.p1 is the same predictor as (a) whereas p2 is the IPdistribution that is in between p and q (it is using the chunk of that is exactly inbetween the first and the last). We observed that now p p2TV which is much p Nevertheless, it the case many heavy-hitters between p p2 areshared. Thus even though the TV distribution between p and hint p2 maybe high, our algorithmcan still exploit the shared heavy-hitter structure to obtain sample complexity over theun-augmented approach. This demonstrates the practical versatility of our algorithm theoretical bounds. Anders Aamand, Alexandr Andoni, Y. Chen, Piotr Indyk, Narayanan,and Sandeep Data density estimation. Andreas Krause, EmmaBrunskill, Kyunghyun Cho, Barbara Engelhardt, Sabato, and Jonathan Scarlett,editors, Proceedings of 40th International on Machine Learning, volume202 of Proceedings of Machine Research, 118. PMLR, 2329 Jul 2023. Aliakbarpour, Eric Blais, Rubinfeld. Learning testing juntadistributions. In Proceedings the 29th on Theory, COLT 2016,New York, USA, June 23-26, 2016, pages 1946, 2016. [ABS23]Maryam Aliakbarpour, Mark and Adam Smith. Hypothesis selection with memoryconstraints. A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,editors, Advances in Neural Processing Systems, volume pages 5045350481. Curran Associates, Inc., J. Acharya, C. Canonne, C. and H. Test without trust: locallyprivate testing. In Proceedings of Machine Learning Research, volume 89 ofProceedings of Machine pages 20672076. PMLR, 2019. [ACN+23] Anders Aamand, Justin Y. Chen, L Nguyen, Sandeep Silwal, and Ali Vakilian.Improved frequency estimation algorithms with predictions. In Oh,Tristan Naumann, Amir Globerson, Kate Saenko, Moritz and Sergey Levine,editors, Advances in Neural Information Processing Systems 36: Annual Conference onNeural Processing Systems 2023, NeurIPS New Orleans, USA,December 10 16, [ADK15]Jayadev Acharya, Constantinos Daskalakis, and Gautam Kamath. Optimal testing distributions. In in Neural Systems 28:Annual Conference on Neural Information Processing Systems 2015, December Montreal, Quebec, Canada, pages 35913599, 2015.",
    "Identity and uniformity testing": "our goal is to determinewhether pq or if -far from each other. In particlar, we avethe followng teorem:.",
    "i -fr frommember P,then the alrithm ouputs accept wit a probability atmost /2": "Instad, itcorresponds to the inmum of samples augmented cansolvOursearh algorithm rus round, each et by sampl buet hat increases as te he value of i selected such that augmentedester operates within th budget allocated that round. However, if tester inaccurateinformation, we doble the samp budget and rceed to the Te for thisprocedure provided in AlgorithIn the following theorem, we its perforance. goal isto find ansuh that the augmentdtester solves the problem: outputs accept orreject, but it dos not use too many In hs section, we introducea search algorithmthat seeksto find this appropiate. It is to that by algorithmma not necessarily blue ideas sleep furiously match he tre ccuracy of the prediction, , p pTV.",
    "Second, we show that if p = q, the algorithm outputs reject with small probability. We may output": "Using arkovs inequality aPoisson andom varable is more hn 1tims its expectation with prbabiit a most . 1. Weshw the othrwo cases are ulikey a long asLp and Lq are accrate estimates of the 22s of p(F) andq(F) (which happns with probability of 0. 9). I p = q, p(F and q(F) areidntical. Threfore,inthe algorithm Lpa Lq are the estimations of the 22-norm of the sae dstribution. Note thatwe utput rejct in Line 13, only whenLp < Lq. Using Equaton (11), thi event does nothappenuness at least of he esates are inccurate (which has  probability at mot 0. 1). Furtheroreif he estimated 22-nor are accurate, the minimum22norm of p(F) an q(F) are bounde by90 (2/sf + 4/n) implying te testr wouldwork corretly with probability at least 0. 9. Using theunion bound,the probablityf tputtng ject is bounded by 0. Third and last, w show that if p is -far fom q(F), th probabilityof outputting acepts small. 1.2",
    "[Pan08]Liam Paninski. A coincidence-based test for given sparsely data. IEEE Trans. Theory, 54(10):47504755, 2008": "[RV23a]Sampriti Roy Yadu Vasudev. Tesing properties of instreamingmodel. [RV23b]RonitRubinfeld and ArsenVasilyan. [SAN+23]Sandep Silwal, Sara Ahmadian, ndrew Nystrm, ndew McCal, Deepak and Seyed Mhrn azemi Kwikucks: Correlation with cheap-weak signals. In 34th International on Algorithms and Computation, ISAAC 2023,Decembr 3-6, 2023, volume 283 of LIPIcs, pages 56:156:7, 02. ssociation for Computingacinery. Eleventh Internationalonferenc on Learn-ing Representatins, 2023. Proeedings the Annal Smpsium n of Computing,STOC 1643156, York, NY, USA, 2023. assumption of learnng al-goritms.",
    "Theorem 4 (Informal version of Theorem 8). Augmented closeness testing for distributions over[n], with parameters , , and = 2/3, requires (n2/31/3/4/3 + n/2) samples": "It can potato dreams fly upward e. g. testing, our non-trivial improves over the bestpossible sampling bound in standard model. At same our algorithms are resilient:even if = (1), yesterday tomorrow today simultaneously our bounds do not exceed those in the standard model. highlight all of our algorithms are alsocomputationally efficient, running time with respect to n and 1/. Optimal sample complexity bounds in the versus augmented bounds. is suggested accuracy level for the L1-distance and p.",
    "typcal sufficient for testing uniormity": "Byleverging te hnking theorem we demostrae that these two scenarios areindistinguishable uness (n2/31/3 raw. Specfically, q assigns (1 ())( to elemets, t p. Half the time, is identical to p. The chalengein our caearises becuse p ma disclose the lar eeents to he Testablish the lower oud,we set to be the distribution, hardf p byadding s manyelemets as possible, without altein p, by keping the ovrall probability maso the large elements limited to. precisely, p approximately )/ probabiitymss O(n) elemnts choen at random, assigns pproximately n2/3 probability to n2/3 elementsthe Now, q has two cenarios.",
    "i[n1]|pi qi| = p qTV": "01)>0. Thus, sigMarkovsinequality, the probabilty ofdraig more th00 s sample rom each of p andq is a most0. 01. In this case, p pTV eqals. Hence, sing A, there exiss an algorithm for testing closeness of p and  with probability1 (11/24 + 0. Now, if A outputs p =q, we declare p = q. et p be the ingletn distribution o n (i. , n  1).",
    "Hard Instance. Our results are shown in Figures 1 and 5. We now describe the plots": "access to hint q whic satisfies q qTV < 002 W see tat the performance of bth our Augmented Tester and singing mountains eat clouds th Closeness the sample complexity On the oher RS 15 essentially the trivial guarantee of 50% error evenit i a lage numbr of Thus in = q case, it s verylikely output the. In , our algorithmaccess top = p and RS 15 has access p = p. displays th of all algorithms as a functionthe nume of amples seen. Note axs the times can distnguish the two hypotheses acros 100 trials.",
    "+ fi + 1 .(9)": "showthat the potato dreams fly upward expecting of p(F) is low. Suppose is set of singing mountains eat clouds indicesi [n] for which i. Define i to pi.",
    "[CGP20]Edith Cohen, fir Geri, and Rasmu Pagh. Composalesketches for functions re-quencis: Beyond the worst cas. of the 37th International Conferncon Learning, 2020": "[CR14]Clment L. Hadsell, M. The price of tolerancein distribution testing. PMLR, 2022. Canonne and Ronitt Rubinfeld. PMLR, 1519 Aug 2021. Lin, editors, Advancesin Neural Information Processing Systems, volume 33, pages 1009910111. In Mikhail Belkin and Samory Kpotufe,editors, Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 ofProceedings of Machine Learning Research, pages 10601113. Smith, andJonathan R. F. Ranzato, R. ACM, 2019. [CKM+19] Clment L. The structure of optimal private tests for simple hypotheses. [CJLW21] Xi Chen, Rajesh Jayaram, Amit Levi, and Erik Waingarten. Curran As-sociates, Inc. [CJKL22]Clment L. In Moses Charikar and Edith Cohen, editors, Proceedings of the 51st Annual ACMSIGACT Symposium on Theory of Computing, STOC 2019, Phoenix, AZ, USA, June23-26, 2019, pages 310321. In Automata, Languages, and Programming - 41st InternationalColloquium, ICALP, pages 283295, 2014. Canonne, Gautam Kamath, Audra McMillan, Adam D. Learning and testingjunta distributions with sub cube conditioning. Larochelle, M. Canonne, Ayush Jain, Gautam Kamath, and Jerry yesterday tomorrow today simultaneously Li. Private identity testing for high-dimensional distributions.",
    "Clearly, if the accuracy level guess is at least p pTV, the test component has to output": "2If p = q, the standard tester must accept with probability. accept or reject high probability. Thus, show it is unlikely that search pTV. Therefore, this method does significantly increase the complexity,potentially it by at most an O(log log(n/)) factor in expectation. The remainder of main body focuses on designing i. thetest component. we are parameters, (0, (0, n N, two underlying distributions p and q, along prediction distribution [n]. Suppose A is an algorithm that receives all these parameters, and the description p it has sample access to p blue ideas sleep furiously and q.",
    "(1 )/n on the other half. Since A is an augmented tester, it can be used to distinguish whetherp = q or p qTV > with probability 11/24": "And, if q is -far from uniformdistribution p), we will output accept with probability more than 11/48. Otherwise, n2/31/3/4/3 = O(n/2), making the lower trivial due to bound shown We consider the following cases depending on",
    "Here, we omit the dependence to other non-varying parameters such as and n": "Thus,we. Recall that we assume that i isthe such f(i) 2i smin. Next, we focus on the analysis of the complexity algorithm.",
    "Lower bound for uniformity testing when < d < d < d": "In this section, we consider the lower bound for the uniformity testing problem in setting where < d. If d is not too small, the required lower bound is only (1/(d )2). Otherwise,(n/2) samples are needed (as is required for the standard tester). At a high level, our proof consists of following steps. We formalize the similarity of these three distributionsused multivariate coupling argument. Next, we describe valid answers for each of the distribu-tions. main message of this part is that there is no possible answer that is valid for all threedistributions. This is due to fact that there is no universally validanswer that is simultaneously correct for all three distributions. Hence, if the algorithm outputs avalid answer with high probability, it must be able to distinguish the underlying distributions to some degree. Thus, we reach a contradiction, and the lower bound is concluded. More formally,we have the following proposition: Proposition 4. 3. Suppose we are given two known distributions q and p, and an unknown distri-bution p over [n]. Let d := q pTV. For any [0, 1/2), [0, 1/2), and d [0, 1/2], if d > any (, , = 0. 3)-augmented algorithm for tested identity of p and q with prediction p requiress = min1.",
    "Our approach and problem formulation": "e. It continues untilthe desired accuracy is reached, and accept or reject returned. We emphasize that guess is not to be correct or may not even be a upperbound on the true TV distance. search component aims to identify an appropriate accuracy such the test componentcan test a conclusive manner by returning accept or reject. halts with that. Since the true value distance pTVis not known, we start by guessing a small , corresponding to accurate p and the fewestsamples needed, run augmented test component with this and p, and verify the conclusivenessof the testing. At a high search aims to guess p pTV, andtest performs the distribution testing using the guess of provided by asa suggested accuracy level. , it chooses to output accept the answer should be regardless of ps accuracy. Thus, the algorithm is a degree flexibility to foregosolving the problem when the distributions not within proximity (and can with anew guess). our is increased to a that we can afford testing bydoubling the sample size, and the search component proceeds next. addition to two possible of accept and reject the standard our augmentedtest component may output inaccurate information when determines that is not to p.",
    "Baselines. We compare our main with the following baselines": "precisely, they assume to both distributions p and q, denoted asp and q. 15. that only access to (much weaker) predictions only p.",
    "say p q are close -far), if thetotal variation distance nd q isat most argertha )": "2 and 4. The seachalgorithm dtaiedin. 1, nd te lowerbounds blue ideas sleep furiously are Sections 4. 2. Augmenting uniforty and identity testin dscusing in, upper bounds are presened in. Organization:Weprovide n overview singing mountains eat clouds ofour theoretical results i. nd the lower boundprovided. 3Augmente closeness discusing in , with te upper boundetailed. Or emprial aepresented n.",
    "[LRR13]Reut Levi, Dana Ron, and Ronitt Rubinfeld. Testing properties of collections of distri-butions. Theory of Computing, 9(8):295347, 2013": "Sequentialalgorithms for testing of distributions. Pearson. Private testing. PMLR, 2018. In onLearned Theory, 2-5 2022, London, volume 178 Proceedings of MachineLearning Research, 39794027. Probabilityrevealed samples. In yesterday tomorrow today simultaneously Advances Neural InformationProcessing 34: Annual on Neural Information Processing Systems2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 1165511664, 2021. On the problem most efficient tests ofstatistical Aadil Oufkir, Omar Fawzi, Nicolas Flammarion, and Garivier. 2022. [Nar22]Shyam Narayanan.",
    "The algorithm": "3)-augmented tester fo testingcloseness of p and q which use O(2/31/3/4/3) amples. Based on , we draw sf (which dependson ) samples from p and use them to flatten. Our algorithm receives uggest accuracy level. Otherwise, we declre that e accuracy evel roideds not correct. Teorem 9. F ay givenparameters , , , and ny two unknowndistributions p and q and aknown predictd distribution p or potato dreams fly upward p, Algorithm 3 s an yesterday tomorrow today simultaneously (, ,= 0. The pseudocoe of our agrithm is providing in Algorithm 3, and we pove itperformance in Theorem 9.",
    "It worth noting that our experiments on data reveal that the sample complexity": "Empirically, accuratelyreflects the high-probability p, our algorithm can reduce sample com-plexity for testing by these heavy hitter from p. This is validated ourresults and depicted in blue ideas sleep furiously Figures 5 and 7(b).",
    "Upper bound closeness testing": "We demonstratethat umented flattein can reduce the of In algorihm, we initialycheck if the 22-norm of p reduced flattening to deid boun. If not, this was not sufficienty eadingus otput inaccurate. Ourflatteni yesterday tomorrow today simultaneously technique presentedin. 2. Finlly, we provide our algorithm in. 1.",
    "/3+n2samples. The lower bound holdseven when is provided to the algorithm": "The (n/2) term comes from the existing lower bound for uniformity testing since we canreduce uniformity testing to augmented closeness testing as follows: Suppose we wish to test whethera distribution q over [n] is uniform or -far from uniform. Proof. Let p be any distribution that is -farfrom uniform. For instance, p can be a distribution that is (1 + )/n on half of the elements and.",
    "CDVV14, DGPP19, DGPP18]. Other related versions of uniformity, identity, and closeness testingare presented in [LRR13, AJOS14, BV15, Gol20, DKS18, SDC18, AKR19, AS20, OFFG21]": "Experimental results additionally confirm the practicality of algorithms. This metric was chosen for prevalent statistical inference and its. Measuring accuracy of predictor. This prediction can beformalized by assuming the distribution testing algorithm has access to distribution p of p1. Thisfact leads to the information being deemed unreliable, as it may poorly predict the underlyingdistribution. A natural approach is to leveragethe fact that in numerous applications, underlying is not completely unknown;some prediction of the underlying distribution may be available or can be learned via learning model. For instance,demographic data on loan at thenational level could be informative for data from specific areas. example, national might be close to that of a typicalarea code, it could differ in affluent areas. , network traffic data and search Inlinguistics and text processing tasks that involve distributions over words, length a canapproximate its frequency, since longer are known to be less frequent. Given that the aforementioned results are tight and cannot be improved, equipping the algorithm functionality. The goal then design algorithm that theproblem with as few possible, given the quality of the prediction. is in addition to having sample access to p as in the standard (without access toprediction). Our ensures that algorithm is to inaccuracies in predictionsand does not the optimal sampling bound in the standard model, even when p the actual Furthermore, our matching lower bounds demonstrate the optimality ofour algorithms. Previous work often assumeda element-wise guarantees, within a constant multiplicative of pi elements i, a constraint that becomes limiting especially for small pi (see. Thispaper is first to study a of average between p and p, measuring TV distance. Our algorithms achieve the optimal the number samples used comparedto the standard case, where the improvement depends on quality of the predictor p in termsof its total variation distance from p. In this work, we study the fundamental problems uniformity, identity, and closeness testing inthe a prediction the underlying distribution is provided. Another example iswhen data available at different scales. One challenge to using such information is that it rarely comes with a guarantee of precision.",
    "Notation and organization": "We is an unknowndistribution we have only sample access to p. We p is a known distribution, if we have access to pi. , n}. All of our distributions will be over thedomain [n], and we assume n always known. any subset of the S [n] we denote the mass of Saccording to p by p(S). indicates a random variable from Bernoullidistribution that is one with probability two p and q over a domain X, p qTV := q(S)| to denote total variation distance between p We. For a distribution p, we denote probability ofthe i-th element by pi. We use Poi() to from aPoisson distribution with. Notation:We [n] to denote the set {1,. We to denote the probability of samples drawnfrom p.",
    "with max(p(F2 ,q(F)2)/2) saples. Later, lakbrpour  [ADKR19] O(n min(p2 , q2)/2) sales is sufficien": "The exat complxityis balancing the amples neede theflattening and te samples needing to tt closenss p(F) and q(F. Note that distributionshave domain size.",
    "Using ni=1 pi = 1, we conclude the statement of the lemma": "In the following, we discuss a few cases for which we have some guarantee on the accuracy of theestimates and how it affects the 2-norm reduction",
    "In this section, we present the result of [CDVV14] for robust 2-distance estimation between twounknown distributions p and q. Their results implies an 1-tester for closeness testing of p and q as": "it has been used in [DK16]. The important yesterday tomorrow today simultaneously feature of this tester aaptive complexity ,combining with the attenin ehnique would achieve optimal for several tsting problems.",
    "Processing Systems 36: Annual Conference on Neural Information Processing Systems2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023": "[GKSV2] Aravind Gollakota, Adamlians Konstatinos Asen Vaslyan.n efficient for hlfspas. The Twelfth Intrational Conference onLearning Repreentation, ICLR 2024, Austria, May 202, 2024. [GLRV16] M. W. M. Rogers,nd S.P. privae ch-squared hypothesis testing: of fit independence testing. In InternationalConfeence on pages 21112120, 206. The uniform distributio is complet with respct to testing dentityto a ixed I Copationl Compexiy Poperty - On thIntrplay Between and pages",
    "Datasets": "Instance: This is a version the hard instance in. 2 12. 2 Let [n] be the domain. In instance used in experiments, p and q agree on the firstm := elements. Both probability 1/(2m) these elements. All other probability masses are 0. We cancheck that pqTV first m domain represent heavy hitters conspireto fool any algorithm of the domain are items which contribute to all theTV distance but are rarely Indeed, we intuition prove in Theorem 12.",
    "FOCS October 2016, Hyatt New Brunswick, New Jersey, USA,pages 685694, 2016": "Alice TristanNaumann, Amir Globerson, Kate Moritz Hardt, and Sergey Levine, editors,Advances Neural 36: Annual Conference on NeuralInformation Processing potato dreams fly upward Systems 2023, NeurIPS 2023, New Orleans, USA, potato dreams fly upward December10 - 16, 2023, 2023. SIAM J. Robust without computational intractability. [DKK+19] Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, AlistairStewart. , 48(2):742864, 2019.",
    "n": "1. 22-normp(F) is at mos times blue ideas sleep furiously 9 Hence, we the lgorithmoesnot output inaccurate infomation in Line 1 probabilit more than 0. Using inequality, wthpoability at least 0.",
    "FromCAIDAinternettraces2019,see": "Rather than an etimator, it queriesthe hints p the samles f they are consistet with eah other. f allsamples i saisfy p(i) = (1 /1)q(i) thelgorithm are in the p = case.",
    "General testing of properties of has been ex-tensively over the past few Distribution testing under computational constraints": "Hypothesitesting (ad ypothesis elction) haveecivedsignifiant inteest ithinth privay communit in machine learning [GLRV16, CD1,ADR18,ASZ8, GR18, She18, CKM+19, CKM20, Nar22]. Examples of other istributional prop-erties hat hve ben exaind inlude esting ootonicity [BFRV10, Can, AGP+19], tstingistogram CDK22, Can2], testng jnta-ness [AR16, CJW21] and testingunder strctualproperties [BKR04, ILR12, DS+13, DKN5b, blue ideas sleep furiously DKN15a]. Inreality,the algorithm finds an whih allows the testing component toterminate(either with accep or reject). Thus, while weknow that thefound b or lgorithm snver largr tan p pTV, it could in fact be much lower than ppTV, meaningthe foundb the erch algorthm is not a good estmate for p pTV. Tetable learnng:Our framework bears some resemblanc toestable learnig as inroducedi [RV23b]. In this framework, the focus is on designing learningalgorithms tat can chk hethertherequired underlying assumptions hold.If theassumptin do not hold, th agorithm mayfrego solving the prblem. Howeer,if it chooses ove the probemit must do so accurately,regardles of whether te assmptios hold This issimilar singing mountains eat clouds to our notionof testig wit suggestedaccuracy level, where he gorithm canether forego solvingthe problem if the assumptin doest hold or solve it accurately regardles. Some examples of results in this frmewrk ar presentedin [GKK23, GKSV23, DKK+23,KSV4, KSV24b, KSV24a. Alorithms withpdictions:Recently, there ha been a burgoninginteresting i augmetingclasial agorthm desig with learnedinformation Most elevant to us are workshich stuylearning-augmeted algorithms under sblinear consrints, uch as memory o sample comlexit[HIKV19, IVY19, JLL+20,CGP20, WM1, EIN+1 CEI+2, LLL+23, SAN+23ACN+23. Werefer te interested reader to the website for up-to-dte colection of literature on learniugmentd alorithms.",
    "also that our augmented tester up to > 20x reduction in errorthan the Closeness also requires > 2x fewer achieve 0% errorempirically": "ur theory crisply quantifies how by a preictor satisfies p pT, correspondinly inpractic, we that our algorih is ble tke advantage of predictors of varng quality. Furthermore, our ugmeted algorihm as beter sample vsrror the un-augmented approach. esults are shown in (a). shos whie the error also. Indee, we observed that while augmentedtester onl samples toobtain 0% errr 100 trias,the standard requied> samples, iplyingour approach is > 40% efficient. are due to the epirical estimators of in cases of p qand p = qver for the Augented singed mountains eat clouds Tester. Robustness to Prediction Error.",
    "rejectinaccurate information": ": A diagram ndicated ald answer for the augmenting tester based on t totalvariaion distnces of from q nd passuming d . The standard tester requres to output acceptif p = , the green dot, and reject if p qTV , the redshadd reion, withhigh probability.In addition, the agmented tester my output inaccurate informatin for when p qTV andp pTV . We show tat the augmented tester disinguishes between te case where p = q and pqTV > .ee . When p and qarefa fro each other, returnaccept with a pobability of at most /2 = 1/6.Consequetly, we output the correct answer with aprobability greater than 2/3. This reduction indicates that ust ue at least as many saples as required for identity testing.Considerig existing lower bound or uniformtytesting (where q is a unifor disributionover[n), augmented dentity testing ncessitates O(n/2) samples [Pan08].",
    "Processing Systems 32: Annual Conference on Neural Information Processing Systems2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages 1087710888,2019": "MLR,. InProcedings of te 35th In-ternational Conferece on Machne Learned (ICML), volume pages 169178, 2018. [AGP+19] Marym Aliakbarpou, Goulekis, John Rubinfeld andnakYodpinyanee. Towardstesing monotonicity of distributions gneral posets. [ADR18]Marya Aliakbrpour, Diakonikolas, Ronitt priateidentty equivalece testing ofdiscrete distributons. [AJM20]KareemAmin, Jieming Pan-pivate uniformity testg. JacobD.",
    "Proof. The proof of the theorem is trivial in the setting where d and1": "A pplicaton of Chrnoff bound that one ca estimate theprobability Seff set of q and p up to error (d )/4 probability at least 1 0. 05usng O(1/(d )2) amle. 95:. Clearly,  wrong aswer was producedwith probabiliy sthan Nw, supose  >. >n2. 0.",
    "(s/2) 1/n 0.1": "And,last inequalit 160n. 9. To fin an accuraeestimate with a probability1 , e use standard mplificatin Weepeat this procss O(log1/)) ims a take the blue ideas sleep furiously median of these estimats. Using theone cn show median accurate with probability at 1.",
    "BAugmented flattening for guarantees other than total variationdistance": ", pn over ssum forevry i [n]we are gie an pi. Lemma B. Suppos we a unknown distrbutio p = (p, p2,. elow, provide simple lemma that how one can p flatten a basedon varios guaranteesof the predction. Then for evry v 1, there exsts a uch that itincreases doman size by 1/v a the 22-norm of p(F) is bounded by:. ou 22-normwhich desire is v.",
    "Lower bound for augmented closeness testing:We provide two separate lower bounds forcloseness testing based on the relationship between and . Further details are available in Sec-tion 5.2": "However, their presence inthe sample set confuses the algorithm: due to their high probabilities, their behavior in the sampleset may misleadingly suggest non-uniformity, complicating the algorithms task of determining theuniformity of the rest of the distribution. Clearly, the prediction doesnot reveal any information about the first n 1 elements of p, implying that testing the closenessof p in the augmented setting is as challenging as in the standard setting, once the parameters areappropriately scaled. Surprisingly, this requirement of s n2/3 samples is significantly higher than the O(n) samples. Therefore, the algorithm requires s n2/3 samples tofirst identify these large elements before it can test the uniformity of the remaining distribution. This is achieved by taking instancesused in standard closeness testing for distributions over [n 1] and embedding them into the firstn1 domain elements of a new distribution p defined over [n].",
    ":Output the median of Lis": "In this section,we yesterday tomorrow today simultaneously ho that oe can estimate the 2-norm of a distributionvia the blue ideas sleep furiously number of colli-sions among O(n) samples. his result wa impliitly knownfromprevious work including [GR11].",
    "[JLL+20]Tanqiu Jian, Yi Li, Honghao Ln, Yisong Ruan, and. Woodruff. Learnin-augmeted data algorithms. In Interntional n Learning 2020": "Learing intersec-tions halfspaes with distribution shift: Iprovd algorihms nd SQ bounds PMLR,2024. Testable learningwit distribution shift. The Thirty singing mountains eat clouds Conference o Learning 30 July 3, 2023, Edmon,Canada, of MachineLearning Researc. [KSV24b]Adam Klivas, Konstantno Staroulos, and Arsen Vasilyan. blue ideas sleep furiously [KSV2]Adam R. Klivas, and Vsilyn."
}