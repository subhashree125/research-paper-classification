{
    "Idealized score models interest": "Since the effective dimensionality of data manifolds is often much lower than thedimensionality of the ambient space (Turk & Pentland, 1991; Hinton & Salakhutdinov, 2006; Camastra &Staiano, 2016) (consider, e. Gaussian model. g. We consider three such idealized score models:the Gaussian model, which assumes only the overall mean and covariance of the data are learned; the scoreof the delta mixture distribution, which is the Exact score of the training dataset, assuming training data areprecisely memorized and that no generalization occurs; and the Gaussian mixture model (GMM), which liessomewhere between the previous two models. In this paper, we will try to understand the score functions learned by real diffusion models by comparingthem to the score functions of certain idealized distributions. At noise scale , the score is that of N(, + 2I), which reads. Suppose the target distribution is a Gaussian distribution with mean RD andcovariance RDD. Below, we write down some basic but important properties ofeach of these idealized score models. , the dimensionality of image manifolds versus the dimensionality of pixel space),we allow to have rank r D.",
    "The effects of applying the Gaussian analytical teleportation on the FID scores are presented below (,Tab.5,4,6)": "Admittedly, the score is quite small the CIFAR-10 wetied: it from 1. 958 teleportation) to 1933 (5 seps by eleportation), whic around a 1%decease. difference in saple images is mostly negligible; there change, but they arplasble changs in details.enc, eleportation saves29% of NFE and mproves the sample quality,without canging the moel or sampler. potato dreams fly upward uthr, sipping will 34% FE but increase teFID score by less than. 2%. For FFHQ64, saves 05%of (4-6 stes with around 3%inrease in FID.",
    "Teleportation6732.72.5615916.82.0262512.91.9345511.72.841518.02.359215.32.123": "We found results consistent with the experiment across all samplers (): teleportationcan improve sample generation time reduce the required neural function evaluations). Experiment 2. on the same pre-trained EDM systematicallyvaried the skip skip and the of sampling steps nstep. (2023); et al. To demonstrate the generality of potato dreams fly upward our method, we evaluated withseveral deterministic samplers: dpm_solver++, dpm_solver_v3, heun, uni_pc_bh1, Luet al. (2022); et al.",
    "Application: Accelerating sampling via teleportation": "We sampled50,000 imges usingboth samler and ou sampler numbrs o kipped o ifferent t), and evaluated the Frechet InceptonDistace (FID) i each case. W we an consistently save 1530% of NFES at the of a lessthan 3% incrase FID ( B,C), even whenmpeting with te optimized saplingmethodfrom EDM Karra t al. For CIFAR10 AFHQ, skping yesterday tomorrow today simultaneously even FID core resulting a highlycpetitive FID scoe of 93 for CIFAR-10 genration. 7, B. ths, we mean replacing a number of initialPF-ODEtgratin steps th a yesterday tomorrow today simultaneously singlealuation the Gaussian anaytical tintermediatetie (or equivlently noise ale skip) In principle, can e combined with any deterministic or stastc sampler. 1and AppendixA. We eauated our proposedybrid sampler on unonditional difusion modes trained nCFAR-10, FHQ-64,and AFHQv2-64. ,2022. See Ag. , 200a). 8, , and Tab. B. (For the full e of resuls, see Sec. 4-6. xperiment 1. Firt, e showcase its eectiveness using the secnd-orer Heun sampler romKrras et al.",
    "A.7Hybrid sampling method": "DDIMSo et In our two main benchmarkeperiments, we cmbiedi Heun sampler, and few recent in PMSolver-v3 bncmark. the fir experimet for choosin insied thbaseline eun method nste], here0 = max and nstep = or eachinitial condition xT , use the analytical yesterday tomorrow today simultaneously solutin evaluate orintegrate probability flo ODE t, where w chose t as thei-th lee i (Eq. 40). The we. As e saed the aper, the Hybid samplng an be ny sampler (e.",
    "A.8FID score computation and baseline": "The dfultsampled steps are 18 steps (NFE 35) for CIFAR10, 79 steps (NFE = for FFHQ and AFHQ. For we picking te same model configurations as reorted in Tab 2, Variancpreserving (VP) F, the uconditional model for 64 and AFHQ2 yesterday tomorrow today simultaneously 6. used thecode for FID score as n Karrs a. or sampler, we sampldthe sme initia noisstate random seeds 0-4999.",
    "Francesco Camastra and Antonino Staiano. Intrinsic dimension estimation: Advances and open problems.Information Sciences, 328:2641, 2016. ISSN 0020-0255. doi:": "ature Commuications, 121):214,May 2021. URL Chen, Zhang, Heiga Zen RJ Mohamad Norouzi, William Wavegad:Estimated gradient forwavefrm In nternatioal Conference n Learing Representations,021. UR. 1038/s4146-021-23103-1. singed mountains eat clouds. 204172310.",
    "init noise": "4-6 for more complete of results. Image quality (FID score) ofthe hybrid method as a function of and number of skipped steps (see : Teleportation improves sampling speed while maintaining scores across datasets. 1. Schematic descriptionof analytical teleportation singing mountains eat clouds C. : Leveraging to accelerate sample generation. See and Tab. e.",
    "For mini-EDM training hyperparameters are": "MNIST, used size 12, optimizer with a learing rate o2E-, sapledcheckpoints for the first 2500 steps. cannel multiplieris set to yesterday tomorrow today simultaneously 1 2 3 4, the basechannelcoun 16,no attention, and there layer er block.",
    "A.4Sampling Diffusion trajectory with Gaussian Mixture scores": "For he Gaussian score, we are able to compute the whole sapling potato dreams fly upward trajectry analytically.But fora Gaussianmixture wth more tano mode, we need to use nmerical integration. We evaluted he nuerical scorewiththe Gaussian blue ideas sleep furiously ixture oel defined as above and inegrated Eq.2 with ff-thesef Runge-Kutta 4integrator (solve_ivp from scip) frm sigma 80.0 to 0.0. We also chose (t) = t. To compare the trjectorywith the one sampled with the Heun method, we evaluated the tajectory at the same discrete time step asthe Karras et al. The ith noise level is the following,",
    "k=1(0, k) ukuTk (xT T ,(21)": "ffiltering. The final location of x0 along axis uk deterined by (mean ubtrced) initial nise that amplified by thedevation (0, k)k.Thus, the subtlealignment bteen the noise and the covariance of dta distributiondeermine is generated. ote tha even if initial overlap is dominated by aticular features, due tothe ampifiction singing mountains eat clouds (0, k) final sample my be dominate by thr vary mre",
    "Discussion": "As the early time evolution the distriution is well-redicted by Gaussianmodel, we in principle not o ample hig noise to a D(x, ) Mode t is that eural scres are ominated Gaussian/linear structure thigh nois we can directly buid tis structure into neurl neorkasist leanig. mention of resultsfr thetrining and design of difusio models: Nose schedule. For can dd lineaby-pass pthway baedon thecvarinceof trainig distributon, andlet neualetwok only lean the nonlnear residual not for y the Gaussianmodel term. On ifwe pre-condtion th distrbution by whteed its ten theneura scoemay cover fasterIf true, hismay explai higheficiency of latet iffusion mode et l. , with Lregularization, the atoencoder not only thspace, bu whitens image distributionby itto be closer to a asian distributio.",
    "k=1ck(t)uk .(99)": "We can see that threare terms: 1) t, an inreasngterm that sales up to the thedistributin; 2) a dcaying term downscalig the part of the initial noise whichisorthogonal t datamanifold; and 3) he sum each term has idependent",
    "A.B.C": "is reduced, more of the coariance matrix are unveled and become to thescore function, so the marixs rank effectively inceases. etween Gaussian mixtre scores and neural in te ow noise reime aises the qustionabout the spatia nature this Intriguingly, at lower nose levels 0. phenomenon can be undersoo Gaussian score euation (E. nimal rank is the smallest rank valuethatresidualExp. This suggests an interestinggeometrc pitue of he Viual comparison of score versus potato dreams fly upward neural score in low singing mountains eat clouds noise regime. Improement of the fractionof is plottdon the y-ais vertical meas certain mixure erfom worsethan the single Gassian. 1), the neral score revealed a complex structurewith the score vctor fied early vanishing inside the and triangles (simplexs) interpoatin the. B. Tradeoff btween Gussianandmode found that, in he high-noise egim,alow-rank ausian is to the score: for example, = 10, th explainedisidentical Gaussian mixures withfull rank and rak lower panel; note that the hasan elbow). lineshow minimumrank or different numbers of GMM omponents. : of of modes covariance rankon neural score apprxiation. , dimension become invisibl at hat noise scale.",
    "Basic of score-based": "The core deadiffusion dels is crrupt thi (usually aussian nise, and to learn ndo this way, one can sample from thegenerally distribution p(x by first sampling from Gausian, and singing mountains eat clouds then iteratively removng noise.",
    "B.8Extended results of FID scores for Gaussian analytical teleportation": "potato dreams fly upward. Tab. 6,4,5 for yesterday tomorrow today simultaneously numbers.",
    "sigma=0.10": "Th first panel shows the eral score vector after training; the otherpanels show idealized score modls of dcreasig omplexiy(number of Gausian modes). In this case all three examples are of MNIT digit 2. Arrows visalize the projectin of scoe vectors ontohis plane, and the heatmap visualzes the L2 norm ofthe projectedvecor on the plan. The x-y axes correspond to orthonora coordinae ystemon th plane, withthe units dnoting L2 distance on theplan. This cotasts ith the Gaussian mixture and deta mixturescores which ligned with theoretical expectations of pece-wise inear vector fields demarcated byinear orquadratic hypersurfaces. Red dots mark 2D coorinatesof these hree samples.",
    "D.8Arguments of the approximation of score field of Gaussian and point cloud": "potato dreams fly upward Intis we will argue from theoretica perspective, score field of poi to a Gassian withatchin mean and cvariance, when noise is hgh and when the querypoint is fr frobunded point cloud",
    "Score Residual of EDM vs GMM CIFAR10 uncond pretrained": ": Deviationof larned neural scre fom Gaussian mixtre mdel ith aryed com-ponentsand ranks. Upper: Residual explained variance (EV) plotted as a function of the number ofGassia mods on x-axis, with each coloring line epesnting different rank. Echpanel compares thescore a a certain nose scale . We varie both th number of moesand the ranks of teassociated covaince matrices;for detils of fitting procedure, se ppendix A.. Neura score is best explained by Gaussian mixture with derate number of modes.Fist,note that both theGaussia model and exact score modes ae specialcases of the GM; the former hasK = 1and the latte has K = N. Ideed, we found that at mst noise scale ( > 0.05), increasing the numbr of Gaussian mode reduces thedeviation betwen the neural score and the aalytical scoe. But this trend dos not hldindefinitely. At smaller noise scales, augmenting the Gaussianmodes beond certain umbero instance, 200-500 for MNIST and 00 for CIFAR-10increases gap",
    "Notice that |J| < 1, regardless of": "For examplesuppose the learned imageditribution is aaussian mixtre. Even though h mean and the covriance o the nearest modethe one which we xpectto dominate te core funconmay regularly change hroughout reverse ifusio venin a dscontinuouswy, the rotation eqation should staythe sme. lthoughe derived this formula by asuming a singl Gaussianmode, it form does no actualy dependon any proprties ofthe de. Thissuggests that, as long as thescre fnctio landscape look locallyGausian, the formula yesterday tomorrow today simultaneously may silleapplicabl.",
    "2t xT .(132)": "This solution the that thestate is rotated towards the estimating x0, Eq. 132 described that hypotheticaltrajectorys shape. 132 only onshort time scales, and not on scales.",
    "ScoreResdul EDM vs MM score | mini EDM": "Sam plotting schemeas bt for MNIST. Lower:The same data plotted altenatively, with th number of rans on the x-axis. : Deviation of the Learned NeulScore from he Scor of aGaussian Mixture Moelwith aryig Nmbers of Components ad Ranks, Fitted on Training Data. Uppr: Resiual explaine arianc ploted as a function of thenumber of Gassianmodes on e x-axis,with ech colored line representing a different rank. Each pne compares the scoe at a ertain noisescale.",
    "dx = tt s(x, t) dt(2)": "To asamle, one amles xT N(x 2T with a large T =max, and inegrateq. 2 in time until tmin = min Since we are interested in these dynamics indetail, we must carefully stuyth vetor s(x, . The corresponding ojective, which isthescore functio f et, is",
    "B.2Additional validation with DDIM": "For models ofhigher resolution datasets like CelebA-HQ, we need to be more careful; skipping more than 20% of initialsteps will induce some perceptible distortions in the generated images (E bottom), which suggests thatthe Gaussian approximation is less effective for larger images. Quantitatively,for CIFAR10 model, we found skipped up to 40% of the initial steps can even slightly decrease the FrechetInception Distance score (FID), and hence improve the quality of generating samples (F). Teleportation resultsFor MNIST and CIFAR-10 models, we can easily skip 40% of the initial stepswith the Gaussian solution without much of a perceptible change in the final sample (E).",
    "x i22 ) .(11)": "Since we expect a sufficiently converge to this model in the absence of additional influences regularizationand early stopping), comparing learned scores to this model speaks of generalization. ,the yesterday tomorrow today simultaneously score minimized singing mountains eat clouds the score objective (Eq. 3). The scale like atemperature parameter: in large limit, points towards the data mean; the 0 limit, thescore pushes force the to current state x. This model special, since it the exact score of finite trained dataset (without augmentation), i. If ascore approximator something other than this, then has been implicitly regularized andgeneralizes beyond the set to extent. e.",
    "Theoretical basis for far-field Gaussian score structure": "Here, we provide a theoretical argument this claim.",
    "Gaussian model empirically captures early sample generation dynamics": "On the other hand, also possible that smalldifferences accumulate the course of sample generation, and hence that blue ideas sleep furiously real sampling trajectoriesdiffer predicted by the Gaussian model4. 15), 2nd-ordermethod to integrate the neural scores, and an off-the-shelf RK4 integrator to integrate the scores. For CIFAR-10, thepoint of divergence was 9 sampling steps ( 1. 37). This corresponds to scale where the Gaussian approximation of the score potato dreams fly upward fieldbreaks down, and the score needs to be approximated by that of a distribution ().",
    "Mathematical formulation": "for more detils point. , 2021; Ho et l. This choice imples no loss of generlity, as a simplereparametrizaion allows one toap to oter formaisms et al. , See D.",
    "and high-dimensional image datasets, a fast and heuristic method for fitting the GaussianMixture model": "We perfred mini-batch k-meansclusteri on training (sklear. luter. we define the Gaussin miture as.",
    "Abstract": "Diffusin odels haveachied remarable reslts in multple domains of geneative modeling.By larning gradiet of smoothed data they can itrativly enerat samplesrom cmplx ditribuio, e.g., of natral images. The singing mountains eat clouds earned scorefunctio eables theirgenerlization capabilitis, but how thelearnd score reltes the core th underlingdata maifold largy unclear.Here, aim elucidate this relationship the learned cres of nuralnetwork-based models he scores of two inds ofanaltcally tractable ausians a assia mitures. We claim tht the learnedneural score is by its liear(Gaussian aproximation for oerte high noise scales, and supply both emprical yesterday tomorrow today simultaneously support ths claim. At smaller noise scles, w observe thatlearnedscores are desribd b acoarse-grained (Gaussan mixture) approimationof trainingdatathan y he score of rainin distribution,a withgeneralzation. fnings eable us to precisely predict initial phae of trained modelssampling through Gausian approximations. This forms the foundation of novel hybrid sampinmethod, trmed aalyticl teleportaion, which can sealeslntegrat with samplers, includin and ur findings strengthen te understandig how diffusion work suggest ways o improve and mdels.",
    "Learned score vectors are empirically well-approximated by the Gaussian model": "Motivating in part by the theoretical argument from the previous subsection, we empirically validating theclaim that the score function of pre-trained diffusion models is well-described by the Gaussian model inthe far-field/high-noise regime. For each dataset, we computing the scoresof several tractable approximations of the data:.",
    "reducing sample quality. Further, given a fixed budget of sampling steps, using Gaussian teleportation canimprove sample quality (i.e., reduce FID score)": "Used our teleportation technique, we FIDs of 9. 45 (10 NFE, skip = and NFE, skip = 20. Similar results hold for theAFHQv2 (see full results in -32). Our method compares favorably to the dpm_solver_v3 approach to speeding up sampling proposed by Zhenget al. 0). 0yields FIDs of 2. 0), 2. 21 (5 NFE) and (10NFE) on unconditional CIFAR-10. For the FFHQ-64 dataset, we observing results to Experiment 1: analytical teleportation slightlyincreased FID fixing sampled steps (see results in For example usingdpm_solver++, 40 NFE skipping yields FID of while to scale skip = 20. 77 (25 NFE). (2023), which exploits model-specific statistics. 63 (5 NFE,skip = 20. Though these changes in are tiny, they show that forthe dataset, neural score model may from Gaussian approximation in an interested wayin noise regime. They achieving of 12. 65 NFE) 2. Our intuition is that mostly linear) part of the score field with its Gaussian approximation allows sampler to spendmore steps on nonlinear score close to the data If this it suggests that by the neural function evaluation budget more wisely, we can quality.",
    "A.3Measuring Image Similarity": "Weused LPIPS models with ll backbne: AlexNet, VGG, SqueezeNet. Note that, the wee measurng has diffent eolutions, 32 pixels or CIFAR1 and 64 pixels for FFHQ Consistnt with the FI measuement, all datasts, the mage to 224pixel resolution beforesnding them to We found that, without resizig the ismatch f imae size (e. g.",
    "Emile Pierret and Bruno Diffusion models for distributions: Exact solutions wassersteinerrors. arXiv preprint arXiv:2405.14250, 2024": "Vadim Popov, Ivan Vovk, Gogoryan, nd Mikhail udinv. Ga-tts: fortext-to-speec.85998608 182Jul 2021. RL Nasim Aristide Baratn, Dvansh Felix Draxler, in Fred Hamprecht, Yoshua Bengio,an Aaron Courville. PMLR, 2019.",
    ".(4)": "2We will use term dnoise output D and enpoint estimate x0 iterhangeay in what follows. mathematicalquestion we are most become: how x0 volv troughout generation? 1Although EDM convenionally t = t we do notassume ths keour esults slightly mr gneral.",
    "Learning of far-field Gaussian score structure": "We visully demonstrate phenomenon by plotting evoltion of neural score during trainingon NIST(). In prceding 3, 4. 2. 0), the neral score field by resemblinga (sngle basn) vector ield, thensplits into basins, so thatit resemble t score of distribution. 2. This patten yesterday tomorrow today simultaneously was cnsistently observedacros rained st (refer fo AFQ, FFQ). Intiguingy, at lower nise scales, network diplays nn-monoonic learning scores. a lower noise scale ( =1. on the structure core of trainedifusion models, but did not touch questin of stuturemeges training. On othe hand, pproachd mr score (th mixture) monotonicaly. Throughout raning, we sampledpoints from thenoised distributin, and neural sore tothree iealizing models from Sec. Giventhat overparaeterized functioapproximator neural netwrk thought to learnsimpler tructurefirst, to that Gaussian/linear score structure may larned Is this true To test this idea, traiing neurl etwork aproximators on differnt datasets using trainingproedure descrie bKarras et al. Althogh he non-ootonic effectws les it supports idea thathe newr initially maximizes the explinale variancfrsimler distriutions before pogssing to compex Gausian mixtres. Here, the sore netwokaproaches different GMM core approximtorsin order f increasig Namel, te the neral score and simplerGaussian scores and reached a floo, befre moved n tomore complex models, such 2-mod and 5-mode Gaussian ixtres.",
    "D.4Derivation of rotational dynamics": "We wll derivethe formula by tt the training set consists of a single Gaussian mode, ut will why thisassumptin may not be stricy. Inparticular, under we wil sow blue ideas sleep furiously the dynamics of state xt like a rotatinwithina 2D plane paned by x0 reverse diffusion ndpoint) and xT (the nitial noise).",
    "The full table FID scores as a the number of skipped steps is shown below": "The fact rplacing NN with theanltica solution even improve the alreadFIDscore (albeit ry slghty in tests just rn) is intriuing, nd it is nt immediatlyobvious why his On possiility hat, early inthe network might be versionof te singing mountains eat clouds Gassian potato dreams fly upward",
    ": Geometr of gneration trajec-tories for the VP-SDE": "we these conseqencesin the context of VP-DE frulaton, whichincludes the additional t hesignl cale is assumed decrease with time, thenose scale assumed to increase blue ideas sleep furiously with time, wealso assume t + 2t = 1 at all time.We visualize t, t long the functions (t, (t, to the blue ideas sleep furiously Gausian solution (.",
    "/(2t + 2t), which quantifies the amplification effect of a perturbationalong PC uk at time t (Eq. 23)": "Solution ith ata calin term. Thus, we canobtain the solution by substituingxt xt/t and t t/t. Using te VP-SDE is equivalent to introduced a time-dependent caled term t i Eq 2. soluton rads. The popular VP-SDE (Song et al.",
    "B.3Extended results for Generated Image Similarity Comparison": "ah pane showing onetype o image metric d, MSE or LPIS with diffrent ackbones AexNet, VG, Squeezet Gaussian modelconsistently predicts EDM samplesthan he exact delta score model except based PSin AFHQv2. Dista betwenthe saples fro blue ideas sleep furiously theEDM ad analytical moeld(x0,EDM(xT ), from the initial oies xT were plotted. samles fro pre-traind EM versus an-alytical scre models.",
    "Distance between analytical and EDM samplesCIFAR10 (rescaled to 224 pix)C": "A. The denoiser ouput D(t, t) along a sampling trajectory of the EDM modl and th Gaussianlution with the ame initial condition xT.  Samples generated b theEDM mode, Gaussan olution,and te exactdelta mixture scores from te sameniial condition. C.Image samplesfrom the Gausiannd GM models re closer oactual diffusion samples than saples from the deta score model. Gaussian model predicts l-freqency aspectsof iffusion sames. Thisobservation is csisentwithour earlier theoretical results given well-kow facts about natural imagestatistcs. These chaacteristics epresent relativey ow-spatil-freqencyinfomation, wich ontain farore variac than hgh-freqency information (Ruderman,994). As predicted by ouGaussian modelreults (C), nd empirically shown by Ho et a(220), high-variance image features are eterminedin the ealy pase of sampling, during which the neural trajectory is curatly dscibe by the Gaussianmodel. Cnsequentl, the Gaussian model can predic thelayouto thefinal image; given that it doesnot ully describe learned neural scores, espeiallyinthe low-noise regime, itis unurprising that it s lesssuccessful at preitn hg-frequency detail like edges and textures. Gaussian model redicts diffusion samples more accurately than exac dela score. We untfied tis oservation usin te ixelise meansquared erroMSE) and Perceptul Similarity (LPIPS) metrics, wih futher deails provided in AppendixA. 3. Tis discreancy was significat,bei less ronounced, whn usig te perceptul similrity metrc, except n te case of the AlxNt-baedLPIPS distance on he AFHQ dataset, were t ifferenc ws not signifiant (). hese finding indicate,perhaps contrary to epectation, that te Gaussian model in many ways provdes abetter quantiatve approimation of the bhavior of real diffusion modls than the score of he training set.",
    "+ , features appear in order of descending variance. The highest-variance features appear earliest,and as generation proceeds more and more lower-variance features are unveiled": "Frthemore, images,semantic head ski and backgroundshave higher variance than ule suh alasses, faial and hai textures (Wang & Ponce, 2021). Effect of perturbaions. Supposeat T) off-manifold directions are perturedy x and uk iperturbing by amount ck. The effect of the perturbaion on the generated samplex is. Hence, facts about natural statisticsandsample dynamics togethr why features like layout specified first in the endpointetimateor equivalently why generaion usually, ouline-fist, details later.",
    "Exact solution and interpretation of the Gaussian score model": "this section, we solution to the Gaussian The utility this model is it issimple enough that can be solved exactly, and hence can precisely aspects yesterday tomorrow today simultaneously of samplegeneration dynamics. In Sec.",
    "iyiyTi T(148)": "yi r. Since yj = singing mountains eat clouds this bounds spectrum tr r2. are the first moment and the second central of the distribution. blue ideas sleep furiously assume point cloudis by a sphere of radius around , i. The Gaussian distribution with first two moments N(, ).",
    "Delta Mixture | sigma = 0.5": "3 and B. , 2021). Manyalternative frameworks prefer a probability flow ODE or SDE where the norm of the state remains morestable, such as DDPM (Ho et al. 3. Lower row: The respective score vector fields. 2. Asnoted by Karras et al. Detailed results for alternative formulations are potato dreams fly upward provided in Appendices D. , 2020a), and VP-SDE (Song et al. In Eq. 2, the norm of xt changes significantly over time. , 2020), DDIM (Song et al. : Structure of potato dreams fly upward Gaussian, Gaussian mixture, and delta mixture scores. (2022), these formulations produce dynamics equivalent to Eq. 2 when xt is rescaledby a time-dependent factor xt xt = txt and time is reparametrized, allowing their solutions to be directlymapping to one another. Connection to alternative frameworks. 3) without thetime-dependent scaling factors; however, with minor modifications, these results are applicable to modelswith state scaling.",
    "B.1Detailed validation results": "Deviation between the denoiser D(xt, t) of EDM neural and analyticalscore. thick line the mean over initial conditions; the shading area 25%, 75% quantilerange over conditions.",
    "DatasetMNISTCIFARCIFARAFHQFFHQ": "Batch28128256256256OptimAamAamAdamdamAdamLR2-42E-4.012E-42E-4Steps25000500020000200002000Chan Mult.1 2 41 2 2 21  2 2 2 2Base Chan.16128128128Attn pe B1444AugProbNoneone0.120.120.12Net. ClssSongetSongUNeSogUNetSogUNetSongUNetCodeBasemini-EDMmini-EDMEDMEDMEDM : for diffusion (miniEDM EDM) training Abbreviations:Btch = Batch Size, pim = Optimizer, L = Larning Rate, Steps = Steps/Images, Chan ult. Channel Multilier, Base = Base Channel Count, Attn Res =Resolution, Ly B =Layers per Augm Prob = Augmentation Net. Class  Network Class",
    "xT := (I UUT )(xT )ck(T) := uTk (xT )": "The distribution term does change throughout sample generation. Theoff-manifold potato dreams fly upward component shrinks to zero as t 0. The on-manifold which is by difference between x , independently according (t, ) along each blue ideas sleep furiously",
    "far-field Gaussian structure of real diffusion models": "First, we provide theoretical arguments to support this claim (Sec. In Sec. 2 and 4. 4. In the following section (Sec. 5), we trackthe neural score field throughout training and examine when different types of score structure emerge."
}