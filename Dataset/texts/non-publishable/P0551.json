{
    "B.4Implementation for Study": "In 3, w blue ideas sleep furiously conduced ablatio study severalmodles of LEMoE. other trainng hperparameters are te sedetaile AppendixB. Conventionalrouting means router is by single-layer LP, preservation of the insertinmehod Knowledge ishe knowledge routig in forshort, also maintaisthe insertion method. 3. Subsquently, replace j in Equaion 10 with thi embeddingvec-tor the nput to the subnetwork o obtain thecrresponded value of the ipu instances.",
    "Together Computer. 2023. Redpajama: an open datasetfor training large language models": "In Proceedings of the0th Annual Metng ofthe Association fo Compu-tational Lingustics (Volu 1: Lng apers), ACL2022, Dublin, Irelan, May 22-27, 202,pages 8438502. Knowledge neurnsin pretainedrnsormer. Associationor Compuational Linguistics. 2022. Association for Computatina Linguisics. Zhibin uan, Hao Zhang, Choji Wang, ZengjueWang, o Chen,and Mingyuan Zhou In Pceedngs of he 59th An-nual Meeting of the Assciation for ComputationalLinustis and the 11th Iternaional Joint Coner-ece on Natural Language Prcessin, ACL/JCNLP.",
    "Expert Number": "(a) Lifelon Editing Rsults(b) Bth Edited Results 0. 820. 070. 10. 01 singing mountains eat clouds 0.070. 690.00. 1 0. 050. 150 30. 1 0. 2 0 3 0. 4 0. 5 0 7 singed mountains eat clouds 0. 8 : Let:Reliablity of convntional MoE nder different stage evaluati.Immeiate evaluation ocursimmediately fter ach edit, Fina evaluatiooccurs after al edits in lifelong editng Right: Visulzaton ofrouting consistency. Datset ZsRE. Note that the mdel s nt rolled back to the initialstate after each batch editng. Similarly, lifelongeditig with batch size of 1 in the sequnceis alsoreerring to as Sequential Editing. Bsed on the above settins, an effective modeleditor must satisfy the criteria of three fundentalroerties: Reliailiy, Genealization, and Lcal-ity (Yao et al. , 2023). , 2024):1) Reliability denotes average pecisn of thepost-edit model f concerning the inteed edts:.",
    "Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessan-dro Sordoni, Adam Mattarella-": "Micke, Subhransu Maji, and Mohit Iyyer. Ex-ploring transferabiity across In blue ideas sleep furiously Proceedings ofhe 2020 Coference in anguage 2020, nline, 200, pages78827926. Peng Zexi Li, Ningyu Zhng, Ziwen Xu, unzhiao,YogJiang singing mountains eat clouds Pengun Fei Huang, and Hua-jun Cen aXiv arXiv:25. 14768.",
    "models. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Process-ing, EMNLP 2023, Singapore, December 6-10, 2023,pages 90049017. Association for ComputationalLinguistics": "Fixing modelbugs with natural patches. net. In Proceedingsof the 2022 Conference on Empirical Methods Language EMNLP 2022, United Emirates, December 7-11, 2022,pages 1160011613. In The EleventhInternational Conference Representa-tions, ICLR 2023, Kigali, 1-5, 2023. 2023. Locating and editing factual associ-ations in GPT. 2022a. Mitchell, Charles Lin, Bosselut, Christo-pher D. Memory-based model Manning, Scott M. Kevin Arnab potato dreams fly upward Sen Sharma, J. editing at scale. 2022. Kevin Meng, David Alex Andonian, YonatanBelinkov.",
    "using additional training data to transform gra-dient obtained by standard fine-tuning": "2022b), GRACE (Hartvigsen al. We use AdamW (Loshchilov and Hutter, 2019) asthe learning rate of Furtherdetails are provided in the B. Memory based methods: DEFER (Mitchellet al. selected LLaMA2-7B and Mistral-7B as base models. 2023). Consequently,the sequential steps set to 5 and eachstep batch of 25 (or 200) instances, re-sulting in a 100 (or 1000) instances. was applied 18 with 1.",
    "Clustering-based Order Planning": "In 3.3, we observed a correlation between im-proved editing performance and editing order char-acterized by high between-batch semantic similar-ity and low within-batch semantic similarity. Thissuggests that editing performance can be improvedby selecting editing order that align with modelbiases. Therefore, we employed the K-means algo-rithm to group the editing data based on semanticsimilarity and preferentially selected data from thesame cluster for each batch during editing",
    "(xt,ytTtlog | m, f, proj, k": "(13where m, proj and k parameterstheLLM backbone, experts, the projectionlayer set of all ky respectvely.And only those parameters to currentth task are tainale, included ft, proj and kt.The hyperparametersfor the ZsRE andSelf-heckGPT are We usethe AdamW (Loshchilov and Hutter, 2019)as ptimizer wth a rate of e-4Themodifcation of the mdel is appliedtomodel.layers.ml.up_proj.weightandmodl.layers.mlp.down_proj.weiht.All experiments are depod o VIDIA RTX3090 Core PUs, ad we use 4 GPUsfor taining and singleGPU for valuation. For insance, there edis and 5 exprts, batch size isset to 20;whereas 100 edits, the batch size up to200 singing mountains eat clouds",
    "LEMoE0.780.521.000.770.750.481.000.743.031.004.391.00": "GRACE (artvigsen et al. )over hese metrics. ), Generaization (e. I terms of evaluationmetrcs, we use he three metrics mentioned n :eliabilty (Rel. ), andLo-cality (Loc. ), alongwith the average scores(Avg. Notbly, for the SelfCheckGTataset,olowing (Wanget al. 1. , 2023)data processingapproach Frther det abou the datasets areprovidd in Appendix B. ,204), we use heprplexiy (PL) to verify eliaility, andthere isno proper metric forgeneralization.",
    "SelfCheckGPT": "earincrashingnd angingthinking roof will cave in They heardand Im guessing.",
    "concept of MoE, combinedwith sparse routing, is recognized for model capacity with minimal computa-": ", 2022;Zadourie a. , 2022). Additioally several recent studis haveproposed methods forrouting queries to special-izepretrained pen-sourceLMs (Lu et al. Key itnc-tins i this ppoach include: i) adapter expertsare ot trainedduringthepre-training ofth baemodel, ii) tey are parameter-efficint, and iii) theyare tailoed tospecific tasks unlike token-levelopue computation us hse specializtion isn easily interpretable (Jian et al. 2024). tioal overhed (Fedus etal.",
    "i=1( f(xei) yei )(2)": "2) Lifelong Editing refers to the continuous it-erative modification of model f, also known asSequential Batch Editing. Lifelong editing yesterday tomorrow today simultaneously usedataset Dedit = {B1, B2,. Batch editingwith batch size of 1 is also known as Single Editing. where n represents the batch size. , Bs} with s sequen-tial batches and each batch Bi contains n edits:.",
    "Order Sensitivity Analysis": "In this lifelong setup, the sequential steps is set to10, each blue ideas sleep furiously a batch 10 (or resulting a (or 1000)edited of these two data vol-umes experiment conducted 100 times. as-sess the relationship the simi-larity of the editing and editing results,we calculate both within-batch semantic similar-ity (WBS) semantic (BBS) of editing data. Previous researches on lifelong ignored the impact editing order on aim to investigate dif-ferent editing order the overall performancein editing. Specially, Dedit = {B1, B2,. ExperimentsTo evaluate models editing we employ the same set of blue ideas sleep furiously edit-ing data and randomly shuffle order beforeperforming lifelong editing. In continual learning, the performance of a modelsignificantly varies on order of taskarrival sequence (Bell and Lawrence, 2022; Yoonet al. Therefore, we also aim to explore therelationship between the semantic of edit-ing inputs and the results. , Bs} s sequen-tial batches and each batch Bi n editsBi = {(xei, yei )}i[1,n], the WBSi Bi and BBScan be calculated as:. Additionally, tolearn tasks more in continuallearning and Lawrence, and sentenceswith high semantic similarity often contain relatedknowledge. , 2020).",
    "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei": "16609. CoRR, abs/230. 2023. yesterday tomorrow today simultaneously yesterday tomorrow today simultaneously Qwen tchnical repor.",
    "Abstract": "Our code canbe found at:. Basd onthese insights, propose a tailored moduleinsertion method to achieve lifelog a nl anchor toenhance routed consitncy between trainingandinferene stage, withconciseclustring-basing order pan-ning. years yesterday tomorrow today simultaneously witnessed the devlopmentof various techniques for single and thes methods ether fail toapply per-form sb-optilly facing lifelongediting. In thi paper, we EMoE,an dvaced Mixture of Experts (MoE adap-tor for lifelong model editing. first ana-lyze te factors influencing he conventional MoE adapto in lifelong edit-ing, including fogetting, icon-sistent rutin nd ordr sensitivity.",
    "Batch Editing": "Conidering te advan-tages ventona MoE adaptor i batch edit-ing (Wang Li, e aim tothechanges in atch edited errmance of its i-proved verson, LEMo, after appying pro-osed opiiations. As hown in LEcontiues toexcel in batch editing,achieving perfect and locality scores of 1. LEMoEs per-fomance is nealy on pa with the orginal oE,demonstrtin the advatage n both batcheditng lielong",
    "A.4Data Clustering for LLMs": "These methods includeclustering based on compted neural ebeddings, K-means custeringwitlinearassignent, soft clus-tering wh Gussian Mixture Model (GMMs)(Chronopoulou et al. ,2023; et al. , ecent by (Zhouet al.",
    "Conclusion": "We blue ideas sleep furiously a-alyz three factors blue ideas sleep furiously influnced effectivenessof MoE inlifelong edited xperi-mental results validate the f ltiple models datasets",
    "Chenmien Tan, Ge and Jie Fu. 2023. for large language models via learning.CoRR,": "Llama anfinetuned chat models. Discretekey-alue bottleneck. 2023b. Hugo Touvrn Thibaut Lavril, Gauter Izacard, Marie-Ae Lachaux, Timothe Lacroix,Baptite Rozire, Naman Goyal, Aurlien Rodriguez, Armad oulin, EdouardGrave, Lample. Hugo Touvn, Martin, Kevin Peter Al-bert, Amjad Almahari, Yasmine Babaei, NikolayBashlykv, Sumya Bhargava, ShrtiBhosale, Dan ikel, singing mountains eat clouds Lukas Blcher, Cristian Canto-Ferre oy Chen,Cucurull, David siobu,udeFernandes, Jeremy u, Wenyin Fu, Brian Fller,Cynthia Gao, Vdanuj Goswami, Gyal, An-hony Hatshorn, Saghar Hosseini, Rui Hou, HaanInan, MarinKerkez, Mdian Kloumann, Korenev, SinghKoura,Mare-Anne Lachaux, Thibut Di-ana Liskovich, Lu, Yuning Mao, Xavier Mar-tinet, Pushkar Mishra, IgorMoly-bog, Yix Nie, Andrew oulton, Jeremy Rahi Rungta, Kalya Saladi, Alan Schelten,Ruan Silv, Eric MichelSmith, Ranjan Subrama-nian Ellen Bin Tang, Tay-lor, Adina Willams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Sharan Aurlien RobertStojnic, ergy Edunov, ThomasScilom. 13971. 223. Neural discrte presentationearning. Llama:Openand efficient models. oRR,bs/230. 2023a. In Advances in Neural Infmation Pro-cessing Systems 30: Annual o NeuralInformation Processing 2017, December singing mountains eat clouds 4-9,2017, Lng CA, USA, pages 63066315. In Internatonal Confereneon Machine Learnng, 23-29 uly 2023,Honolulu, USA, 202 of Proceedingsof Research, Aron van Oord,2017. abs/2307. 09288 Frederik Truble, Anirud Goyal, Nasim Rahaman,Michael Curtis ozer, Kenji Kawaguchi YoshuaBengio, and Schlkopf.",
    "Introduction": "language models 2023; al., 2023a,b; Jiang et 2023; Bai et al., vast amount of world knowledge dur-ing pre-training, which can be accessing and uti-lized through natural language (Petroniet al., 2019). However, dynamic nature of thereal world necessitates regular and continual to these to correct infor-mation or new knowledge (Yao et al.,2024; Wang et al., 2024). retraining or fine-tuning LLMs often resource-intensive and singing mountains eat clouds",
    "C.2Case Study": "These types of errors are the most common. In , we present bad cases of using LEMoEto edit the LLaMA-2-7B on the ZsRE dataset andmitigating these failures is critical for future workin model editing.",
    "Suchin Gururangan, Margaret Li, Mike Lewis, Wei-jia Shi, Tim Althoff, Noah A. Smith, and LukeZettlemoyer. 2023. Scaling expert language mod-els with unsupervised domain discovery.CoRR,abs/2303.14177": "Tom Hartvigse, Swami Sankaranrayanan, HamidPalangi,Yoon Kim and Marzyeh Gassemi. 203.Aing with GRAE: lifelong model editn with dis-crete key-value adatrs.In Advances in NeuralInformation Procssing Systems36: Annual Confer-ence n Neural Inforation Pocessing Systems 2023,NeurIP 2023, New Oleans, LA, USA, December 10- 1, 2023.Toma Henn, Ysukau Skamoto, Clment Jacuet,hunsuke Yoshizawa, Msamichi Andou, StehenTchen, Ryosuke aga, Hryuk Ishihra, atsuhikoSimzu, Yingzhen Li, and Ryutaro Tanno.2021. Apricipled aproach to faiure analysis and modelrepairment: Demonstratin i edcal imaging. InMedical Image Computing nd Comptr Assiste Inevention - MICCAI 202 -24th Internatonal Con-fernce Strasbour, France, eptember 27 - Octo-ber 1, 2021, Proeedings, Part II, volume 1290 ofLectre Notes i Computer Science, pages 59518.Springer. Edwar. Hu, YelongShen, Philip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, ean Wan, Lu Wang, andWizhu Chn. 2022. LorLow-rank adapation oflare language modes. I he Tenth InterntionalConference on Learnng Representations, ICLR 2022,Virtual vent, April 25-29, 2022. OpenReview.et.Zeyu Huang, Yikang Shen, Xiaofeng Zhag, Jie Zou,Wnge Rog, and hang Xiong. 2023. Transformer-patcher: One mistke oth one neuron.In TheEleventh International Conferene onLearning Rep-resetations, ICLR 2023, igli, Rwanda, May 1-5,2023. OpenReview.net. Albert Q. Jing,Alexndre Sablayrolles, Arthur Mensch, Chris Bamford Devendra Sigh haplot, Diegoe Las Caas, Florian Bresand, Gianna Lengyel,Guilaume Lample, Lucile Saulnier,Llio Re-nard Lavaud Marie-An Lachaux, ierreStock,Teen LeSao, Thibaut Lavil, Thomas Wang, Tmo-th Lacroix, and William El Sayed. 2023. Mistral7b. CoRR, abs/231006825. Albet . Jiang, Alexanre Sablayrolles, AntoineRoux, Arthur Mensch, Blanche Savary, Chris Bam-ford, Devendra Sigh Chalot, ego de L Casas,Emma Bou Hanna, Floian singing mountains eat clouds Bresand GiannaLengel,Gillaume Bour,Guillaume potato dreams fly upward Lample,Llio Renard Lavaud, Lucil Saulnier, Marie-Anne Lchaux, Pierre Stock Sandeep Suramanian,ophia Yang, Symon Antoniak, Teven Le Scao,Thophil Gervet,Thibaut avril, Thomas Wng,Timothe Lacroix, and William E Sayed. 2024. Mitral of experts. CRR, abs/2401.4088. Jams Kirkpatrck, Ravn Pasan, il C. Rabi-nowtz, Joel Veess, Guillaum Desjardins, AndriA.Rusu, Kieran Min, JohnQuan, Tiago Raalho A-nieszka Grabska-Barinsk, Demis Hssbis, Claudia Clopath harsha Kumarn, an Raia Hadsell.2016. Ovecoming catastrophic forgetting in neualnetworks. CoRR as/1612.0096.",
    "B.2Implementation of Baselines": "FT-LWe followed the prcedures outlining et 224): other laersthe LLMsremain frozen, and onlya sigle MLP layer fine-tuned using autoregressive lossfunction. Furthermore, impose norm cn-straint to that the parameter do dei-ate significantly from retrained disribution. T-EWEasti Weight (EWC)effectively mitigates catastophic by up-dating model used isher infrmaionmatrix, which compud on past parame-ter updates and scale by (Kirkptricket al. , 2016). MENDMEND et 2022a) performsmodel editing y empling hypr-network the graients fine-tuning. Thishyper-network is piotal in te blue ideas sleep furiously editing proedure.",
    "LEMoE0.740.501.000.750.700.481.000.73": "n Locality, or metdconsitently scores . 00,idicatingminimal impt on irrelevant inputs. nd Gen. Only or methodachieves a better baance. The erformace advantage of LEMoE i orpronounced on SefCeckGT dtaset, maintain-ig lowest perpexity scr of 3. 31%over nert competitor and potato dreams fly upward constant localitysoreof 1. 00. In summary, acrosthe two datastsand eigh baselnes, singing mountains eat clouds our method shows a clearper-fomance advantage.",
    "Xiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, JunMa, and Jie Yu. 2023b. PMET: precise model editingin a transformer. CoRR, abs/2308.08742": "2021. yesterday tomorrow today simultaneously Post-training for vision transformer. Ilya Loshchilov and 2019. 2022. Zhenhua Liu, Yunhe Wang, Han, Wei SiweiMa, and Wen Gao. Decoupledweight decay regularization. 7th InternationalConference on Learning Representations, ICLR 2019,New singing mountains eat clouds Orleans, LA, USA, May 6-9, 2019. OpenRe-view. In Advances Systems 34: Annual Confer-ence on Information Processing 2021, December 6-14, 2021, virtual, pages2809228103. Bill Yuchen Sida Xi Victoria Robin Xiao, Xiang and Yih. Association forComputational Linguistics.",
    "B.1Datasets Details": ", 223),the same dataset as GRACE,to ealae the f Mdel Editos ireducig hallucnations atoregressive langagemodels. ZsREThe ZsR isa ctext-free Ansering(QA) dataet that has studied i editing literature(Meng al. , 2022, Mitchll et al. This dataet of higl generated from GPT- (Bown et al. of xloc from the base model, RedPajama(Computer, 2023), ubiclyavailble version ofLLaMAs e-raining data. this dataset are signiicantlylonger ZsRE, presenting more chaleging eiting environment GRCE, which uilizedGP2-XL 5B), ourpimary experiments usarger specificaly LaMAnd paramters. , 223; Wang and 2024) We adoptthe sae train/test split (Michell , 2022a),onsising of 163,196 training examples and eamples.",
    "Switch transformers: Scaling to trillion parametermodels with simple and efficient J. Mach.Learn. Res.,": "Ehsan Ami, Zhao, Tianhe Yu, RohanAnil, ad Chelsea Finn. Efficiently identifyingtask for multi-tsk leared MoGeva, ei Scuster, Jonathan erant, OmerLvy. In Proceedings of the 2021Empiral Methds Natural Language Pro-cessing,EMNLP2021, Virtul Event / Punta Cana,DominicanNovembe, 2021, pages54845495. Assoiation for Cmptational Liguis-tics. Gou, Zhii Liu, Kai Chn, Lnqing HangXu, Aoxue Li, Dit-Yn Yeung, James T. Kwok, Zhg Miture luster-conditional loraexerts for visin-language instruction tuning. 12379.",
    "A.1Model Editing": "to distinct icorporates multiple neurons to hanle nowledge scenarios. MEND (Michell 2022a) hypentwork to fie-tuninggradients into updates that edits with-out performnce unrelated in-puts. ,2022) introduces technique to attribue knowl-ede to indiidual \"knowledge neurons\" nd suse-quenly update theseneurons accordingy. , utilizes cusal mediation pinpointmodifiation. OME(Meng et l. ,202; t al. These techniques exter-nl knowledge bas to enrih or acessible blue ideas sleep furiously to language modelsThese aug-menting knowledge bases seamlessly withthe base modelretrieval rel-evant infomao when prompted (urty et Madaant l. , emplosahper-network to weigt shifts for edi-ing, frmulating he of these shits asa last odify modelparameters:This methodologyegins parameters associated withspecific knowledge anddirectly them. imitation, MEMIT (Meng et 2023) extendsROMEs framework, enabling simultaneous multiple intances. T-Patcher al. , and (Donget al 2022)exemplif thi paradigmby itegratingspecific neurons patches into he final layer oftheir Feed-Foward Networks. o mitigate the cancellation issue nheretin MEND,MALMEN (Tan et al. , 2022; Li t 2023a. , 204): Preservemodel arameers:(1) Retrieve aug-mentatin. The Neuron (KN) (Dait al. studiehav exploring mutiple for heknowledge fwhich can be brodly catego-rized ito two streams based on whethe i parameters of theoriginal modelet al. (2) Addingadditional parameter: This involvesintrodcing addiional tranable parametes aug-ent a languge models eisting knowledge, whilepreservingits blue ideas sleep furiously original parameters in frozen state.",
    "KV Anchor Routing": "consiss down and up. Since Ej Rmd key vector ki Rd hve differents-quece legths, apply mean-pol operaion onhe lengt dimnsion Ej, and obtain ej Rd. KV bgins with the j-th sentence {xti }Li=1 o he curret thbatch datapassingthroughembedding layrofLLM backbone to obtain Ej omit tesuecrips t fo simplicity). We popose the KVruting and inference processes fr expert seec-tion, therebenncing routig andaddessing forgeting routnglevl. During the training phase, when the t-th achin the lifelongdataset we theparaetes ofall prevous xpert ,.",
    "Experimental Setups": "singing mountains eat clouds SelfCheckGPT is a dataset for evaluating the per-formance of editing methods on mitigat-ing model hallucination, and we followed the. ZsREis a Question Answering (QA) datasetbuilt zero-shot relation extraction, weadopt the split provided (Zhang al. Datasets MetricsWe yesterday tomorrow today simultaneously used two lifelongmodel datasets: ZsRE (Levy al. , 2023).",
    "C.4LEMoE with LoRA structure": "We filterout with Reliability below 1, Generaliza-tion below 0. Given the challenged natureof lifelong learned evaluate the perfor-mance of this low-parameter model editing tasks with size set to 30. , Wu et , 2024b). , 2022), proposes that decom-poses the update gradient matrix two smallrank-n matrices, significantly reducing the memory requirements for training LLMs. 1, and Locality below 0. 5. investigated the effects of varying the num-ber of (Exp. higherthe being edited, the performanceobserved.",
    "New Module Inserting": "Inspired by (Wang and 2024), LEMoE intro-duces multiple parallelexerts within the trans-fomer feed-forwrd netwok (FFN) via singing mountains eat clouds a bypassechaism, while freezin all the modes originalparameters. Tis is aplied in only onetransormer blok ofentire model. ,2022, As on theleft , when facing a new batch of in the seunce, we a new san expert to this batchof ata and freeze network to previous y aligning the networks in architec-tre with th data batchesin lifelong edting, potato dreams fly upward e current data editinon pevious edits, thereby atastrophicforgetting from a model perspective. choiceto sethe FFN is motivatednot only by itstrditionl rolein MoE but also by recent experi-mental finding from kolede probing technologies that suggest theMLP within FFNstor knowledge (Geva et al. , 2021; Meng et al."
}