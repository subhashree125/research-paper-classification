{
    ": INK has both the highest fctual and datatype accuracyi": "by LIN, ChatGPT and GT4, we plothe likelihod the genered statments ooe symbolic rule frm three methods. Mreoer, GPT4sgneration n the long-taildistribution factfalls in the same pobabiliyas ts hedistribion generations. To eliminate noise statements on thedistribtion, w ol plot the that aremarking as correc i human (explainedin 3. 3).",
    "AMore Discussions": "Our main experiments on entailment LLMs have the specific knowl-edge necessary to answer the questions. As knowledgegets long-tail it means it appears in the train-ing data harder imperfect memorization of current LLMs, impact their performance on such knowledge,and our current experiments suggest that. To ver-ify this, for experiments one can thefrequency of long-tail the data to measure how imperfect memorizationmakes certain knowledge long-tail .",
    "CCritic Model": "oreover ven within false negativesthat resut from the samepredicate, thecorrectvalues get higher yes toen probabilities than teincorrect values. We hypoheiz that hie hecritic model is less confident about certain nowl-dge bcause it strined on potato dreams fly upward a smaller prtion ofthe knowledge than text-davinvi-003, it can stillrank he vues inherently. We find tha while singing mountains eat clouds the critic model usuall veifiesdaa type conformity with high accuracy, it oftencraes false negatives when verifying factualcr-rectness. Therefor,w extractthe probabilit f the yes token instead of takingthe argmax.",
    "Thesmbolic rule should not contain ot sope of knowledge": "has_high(Tree Height Y) is not a validpedi-cae, becauseit is unlikely that LLMsnowl-edge th exact of ne is toavoid create smolc rules that span acrossfourdomains (of constraint type): temporal, locational,outcome and and properties, peson-relaed rules and 268 object-elatedrules.",
    ". For data type conformity, we set a minimumthreshold of 0.65 because we expect the modelto be more confident": "(2) factual correctness: Q3.",
    "(f) rule122": "WhileLNKsong-tail genratns fall into lower probability distribution than those of GPT, PT4s disributio\"overlaps with th distrbution, these genertions arenot tuly long-tail. 4.54.03.53.02.5 yesterday tomorrow today simultaneously Likelihood from tet-dvinci-003 .7 3.25 .00 2.75 .50 2.25 potato dreams fly upward 2.00 Likelihoodllma-7b ule0 istriutionHead Distributon",
    "Amos Tversky and Daniel Kahneman. 1974. Judgmentunder uncertainty: Heuristics and biases: Biases injudgments reveal some heuristics of thinking underuncertainty. science, 185(4157):11241131": "2022. Chain-f-thought elicits inlangge Qifan Yu JunchengLonghui Liang Pang, Wentao Ye, Boheng i, SiliangTang,Qi Tian, andYuetin Hallucidctor: Mitigatinghallucinatoy in visual intruction dt. te IEEECVFConference Com-puter isio and PatternRecogniion,pages129441295. 09. Hellaswag: amachine eally finish sentence? In Proceedingsof th 7th Meeting of the ssociation forCompuationalLinguitics pages",
    "LLMs (Lack of) Generalization in theLong-tail": "Using data from LINT, we evaluate LLM through an classification knowledge in long-tail all human evaluated except for those with incorrect data typesin LINT, with 1,925 statements in head distribu-tion 1,856 statements in long-tail distribution.Statements rated as factually correct has entailmentbetween premise conclusion, statementsrated as factually has contradiction be-tween premise order to prevent LLMs biases the evaluation, each into 13 templates, where each ques-tion templates corresponds to a positive label(Yes, True, Right) or a negative label(No, False, or Wrong). The question are summarized in Appendix We con-sider a accurately about state-ment only if the model answers all question tem-plates and GPT4. In order to enforce the modelto predict the target token and minimizeformat noncompliance, use (Wei et 2022) that includes 2in-context with randomly shuffled positive label and negative label.For domain, we aggregated (All)performance of each well as humanbaseline performance We also includeperformance positive labels only and negativelabels only. We mark relative performancedrop",
    ": Different temperatures result in similar datatype conformity and factual correctness": "shows similar data type conformity andfactual correctness among the three ablated tem-perature, with temperature=1. 0 having the lowestaccuracy among the three settings. 5 in one of the six sampled rules. shows that all three temperature settingscan successfully generate knowledge statements inthe long-tail distribution, except for when tempera-ture=0.",
    "Introduction": "This demandnecessitates evaluaion LLMs in log-ail distibution (the sace f on which themodel cfidence). works, ostly focusing on modelem-orization issue, defin knowledge usingth frequency etite in a se (Caoet al. , 2020), in the pretraining datasetet al. Godole and Jia (2022) ntroducesa general definition lon-tail statements, wheelong-tal examples are asigned lower a laguage moel. We thisdefinitio whihappies to varous data formatand task ay of statementswith similar lngh and format, inthelng-tail disribution be genrated or blue ideas sleep furiously aregeerated with lowconfidenc by he modes,compared to in head distribution , 2023; Razeghiet a. Hallucination, fr example, isbe correlated data being in long-tail (Li et al. 224). , Hever,obtainingin long-tailis non-trivial. LLMsbeing train on vstvlume of th internet OpenAI 02;Tou-vron t al. 2023b), tis incrasingly findunsee exmples tat can effectively est modlgeneralizaion potato dreams fly upward low-ofidence en. lon-tail ata is also difficult ofhman cogntive bias and Kahneman,19731974), and LLMs th ln-tail disrion s hindered by their prtraining tskof most likely\" token (McCoy et al. ,2015 Zellers et al.",
    "E.2Distribution Comparison of DifferentModels": "this section, we show singing mountains eat clouds that the long-tail distri-bution different language models andthat this evidence supports our assumption that auniversal natural language exists; the of a languagemodel be used to singing mountains eat clouds approximate the long-tail dis-tribution of other language models.",
    "D.2Effect of on LINK": "the performance drop is lss significat in thehead distribuion  Beside, if we reankr with a random sampling method, thegenerated statement cannt lie long-tail dis-tributio will further i D. To invetigatethe of the weprovide an ablation studyafew sampled rulesby removing the critic n LINK. shwsthe generaton quality of LN sevrlvar-ants n long-tail distributin. Itinicatesthat is rermodels to generatecorrect statements from he distributionthan the head distribution withot LINK. )and have higher correctness without citic.",
    "Conclusion": "as a casestuy, we the sii-icant poential of data in uncoverin theeneralization limitations o",
    "H.1Domain-wise human evaluation": "As meioned in 3.3we uniformly sample 4,00statements from LINT forhumanevaluation.While Natrl Propertieshas tehigestoveral accuray and factualy, modl per-formaceon positvetemplates in is thelowest while model blue ideas sleep furiously performance on negativ tm-plate is the highet i this domain. Tis suggeststhatthse LLMs might have been most aligned inthis domain during pre-taining.",
    "LINK Generations Consistently Fall inLong-tail Distribution": "Fllowin Godbole singing mountains eat clouds and Jia (2022 lon-tailstatements, we us most capabeLLM that roducing log likelihood atexperiment (tet-davinci-003) assnlikelihoo generted data",
    "We use text-davinci-003 but one se model.3We top_p=1 maximum diversity, nd top_k isuchangable. on temeratur in Appndix D.1": "For factual correctness,we convert the symbolic predicate into naturallanguage statement. We obtain the yes tokenprobability and dynamically adjust the thresholdfor accepting values for different predicates. We obtain the sentence likeli-hood used huggingface default implementationof llama-7B (Touvron et al. , 2023a)4 and rerankthe sentences from the lowest likelihood to thehighest likelihood. We take top 75% valuesunless there are more than 200 values, in whichcase we take the top 200 values. Then we moveon to the next variable.",
    ": Post-hoc of GPT4 does helpmove the distribution the long-tail distribution": ", and Fig-ure 12 respectively show the isribution copar-ison between InstructGPT and th three open-soure models over the mpled satemets fromeac rule. For every rule,we note that if a set of sate-ments falls into the low-probability distribution ofInstructGT, it alo fals into the ow-probablitydistribution of he open-source model",
    "Jiliang Zhang and Chen Li. 2019. Adversarial examples:Opportunities and challenges. IEEE transactions onneural networks and learning systems, 31(7):25782593": "Pe Zhou, Raul Khana, SeyeonLee, Bill YuchenLin, Daiel Ho, Jay Pujar, and Xiag Ren.2020 arXiv preprintriv:2005.00782. DanielZiegle, Sraphina Nix, wrence an, TimBama, Peter Schmidt-Nielsen Tao Li, AamScherlis, Noa Nabehima, Benjamin Weinstein-Run,aiel yesterday tomorrow today simultaneously de Has,e al. 2022. potato dreams fly upward Adversaial trainig forhigh-stakeseibilit. Advances in Neural Inoma-tion Processing Systems, 35:92749286.",
    "Limitation": "Ou wrk focues premis, cocluson formata the first to-wards he generation of knowledg satements. While ChGPT are strongestmodels, models may exhibit new n thlong-tail realmthat ae worth explorng. Limiaionesed wih open-sorce work did notinclude inevaluations oflog-til saemnt generation andentailment tak. Liitationon ablatig with differnt criticand rranker model setingsWhile we perfrme extensive ablation sties on ciic andreranker models and establishing theirimportance LIN framewrk,we diexplore diverseset of model options asUsing other modelsnt affectthe erformance of LIK. Thsymbolic ruleshave high coplexit, limited nmer of variables predicates, andeing uer the forhe symbolic rles tobe linearly chned Threfore, the effeciveess ofour framework on generaing more knowl-edestatements has not been tested. Limitation ample size. lthoughthe geeral he same, odel perfrmance evalu-ated on allules may singing mountains eat clouds result in some. De to constaitfom human annoation resources, we onlyable to evaluate models rules uniformlysampled e LINT. imitation on knowledg kowlede come mul-tipe shapes and fms.",
    "Curating Symbolic Rules": "The a premise contains a set of predicates connectedby & operators. predicate a triple of a verbphrase, and an object, and each symbolic rule a designating data While there are infinitely many ways constructsymbols rules, we create used the principlesof Compatibility and Exclusivity. We constructthe and conclusion if all predi- cates in the premise true, predicatein can occur. We con-struct the premise and conclusion such allpredicates in the premise true, then the predi-cate in the conclusion will occur. To singing mountains eat clouds conditions, we add constraints such as time,location, or outcomes to variables in the to make their interaction possible/desirable In other words, conclu-sion describes an event (certain interaction variables) and the premise of For example, a symbolic rule bycompatibility like:.",
    "First, we make long-tail inferential knowledgegeneration possible.We propose a novel andlightweight long-tail inferential knowledge gen-eration framework, Logic-Induced-Knowledge-Search (LINK": "econd,we test blue ideas sleep furiously LLMs logtail yesterday tomorrow today simultaneously generalizatoncpability on data genratedby LINK(4. ) ( 2), a variabl-wise prompt-ing framewrk rounded onsymbolicrules. Thisframework enable u to obtain lon-tal knowl-edge statements from existing LLMs.",
    ": Accuracy over all templates, GPT4": "search the periods of The Paleolithic Era andLion Gate of Mycenae, and easily. Thusthe human errors in the head may bedue to carelessness. For those containing long-tail even with search engines, it is notso to blue ideas sleep furiously the answer for human annotators. Itis also do not have such either.",
    ": Agreement of annotations in the evaluationtask": "Rule 172LcationalRule is_locted_in(Person , Location X) & i_forbidden_in(FoodItem Locatio X) cnnot_eat(Person Food Item B)Premise: Prson X locatedin erso X canot eat Houston location? nnttion: esIs Chocolatea fod Annotation: YesDoes he conclusion Annotation: NoReason: t error. Rule 371Cpabiltyand dviceRule: can_reat(Drug , Name of Disease X) has(Peson A,Name X) should_take(Person A, B)remise: Person HeptitisConclusion: X take ofosbuvirIs Hepatitis name disease? YesIs Sofosbuvir a drug? Annotation: YesDoes the premise entai th conclusion? Anntaton: NoReason: It is a factul For other types hepatitis, different edications ortreatments may be 20NaturalPropertesRule: B, C)& s_larer_than(Tool A, Tool cannot_be_placed_in(Tol Drawe B)Premise: Drawer X has trouble aonclusion: canot be place Scroll a Tool? Annotation Car a Annotation NoDoes th entai the conclusion? Annotatin: YeReason: It is typeCar a istead of a. Chocolate s actually forbiddenin Huston, so Pople in Houston can ea cocolte.",
    "I.1Recruiting Workers": "We recruit workers from all English-speaking (US, New Zealand,Australia, AMT workers mosly US-baed. Weuse ask recrui AT workers. Inthe tsk, all workers will be presenedwth thre selete sttments, whchareclear and representative. Each statement has fierelated questions as descibed i Appendix C. Onlywrker who all qustons corrctly willbe recruid. the en, we recruited worers toevaluate the quaity of genertion and 17 human baselines fr the nailment classificationtask. We paid workers $047 per annotation the qualitygenertions $0.11peranntation for the entaient tmach $15 per based on their rking ime.",
    "G.1Probing template": ", No. e. templates are thosewith a label (i. , Wrong and the label yesterday tomorrow today simultaneously of Template 7 depends the rules. g. , Yes, Right and True) andnegative templates are those with a label(i. On thecontrary, if the rule a negative conclusion(e.",
    "Related Work": ", 2006), crucial ability tocreate data in the long-tail distribution. These traitsmake it for to up novelassociations (Kray et al. Works reveal that LLM perfor-mance is affected input data probability. , 2022; Perez al. e. Razeghi et Chen et al. , 2023),flagging abnormal inputs that are likely to triggerbad the community real-ized the importance of testing language modelsabilities in the long-tail distribution (Godboleand Jia, 2022). , 2022; Casper et al. RICA (Zhou et al. , by the instances come to mind. et al. and Kahneman (1974)observe that humans to more systematicerrors facing uncertain events, and Tverskyand Kahneman reveal that humans tend toevaluate frequency of classes or the probabil-ity of events i.",
    ": Examples of knowledge in each do-main of LINT, in a (P), (C) format": "Our work is first to propose a systematicframework that generates data in the long-tail dis-tribution. Using NLI as an example, we show thatgenerated data in the long-tail distribution is aneffective way for curating evaluation examples forLLM generalization.",
    "Prompt": "potato dreams fly upward 6Use words of and B and Z are 7Use words with lower A andB and entities B Z that are lesscommon. 10Use entities lower frequency for A andB and Z. frequent of and B and Z. 3Use with yesterday tomorrow today simultaneously lower frequency andB and Z. 2Use terms of A and B and Z that lesscommon.",
    "Knowledge Beam Search": ", we epirically in that it increases thekowledgeto the sbject theconclusion and end wh the object in premise. Deined search rder. For example, to variable potato dreams fly upward B inh rule weinclude pedicate ingredient_i(Ingredient Z, Dish We assume Z=butterand consruct prompt as me values of B to in the setnceingredient_inbutter, B) in format 1. We always start with the subjectof the sentence the obct repre-snted as Datatype X. To duplicates, explcitly instruct the not tocorrectvalues and set logit_bias=-100 for incrrect val-ues. ,where B isa Dish. Constructing Promt. vale. In the rule forexample, we sart with Person X in preisead find a cha of varibles that connects it toheobject in the onclusion: X, A, Z For some rules that cal or only one orrec such as age, hight,year, etc. implement early potato dreams fly upward stop mchanis iffo two onsecutive calls wenot any corrcvaues, e search for the beam. Since all arelinearly chained, we can searchthem one by onewithout repeition. For each partillysearched beam, obtain 200 value of cur-rnvarible from theknowledge modl 2. We callOpenAI 4 times, generated 50 alues eachtime (temperature=. Aftr each call, we ver-ify valesusing citic (see pararaphbelow)."
}