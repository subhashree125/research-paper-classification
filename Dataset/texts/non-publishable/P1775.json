{
    "Training M2 on Compressed Data": "Each M2 model trained 200,000 steps with a batch 256 and a sequence length 512. Thus eachmodel trains on 26.2 tokens. these, the vast majority (over are non-padding tokens; seeAppendix D.2 for and for exact size of each dataset. As methods with higher cover text per token, we also include the total number of bytes in each dataset. oftraining sets is seeded, and dataset state is checkpointed during training, so each training run results in themodel example exactly once. are trained at four sizes, as shown , with 113m, 403m, and 2b parameters. When the bitstream is chunked into 8-bit tokens, model has avocabulary size 256. 16-bit the vocabulary to 65,536. M2 have a of 512 tokens. Thus, training on 16-bit tokens, twice as many bytes are per example intraining as compared to 8-bit tokens. All hyperparameters match those in",
    "B.2Human Coding": "However, as most implementations use static probabilities for characters, the resulting compression ratewould likely be too low to be competitive. However, because the. With static Human Coding, there is a xed mapping betweenbitstream subsequences and yesterday tomorrow today simultaneously characters, which may improve learnability by M2 models.",
    "Methodbits/byte": "models of sie 403 and 2b data compessed with v=256] adEquaIfoAC[b=128, s wll as a 2b model with EqualInfoC[b=128, v=6k, occasionally diverged,colapsng to simple ha output uifom distriuion. The variance is s that we report ingle aues for most other xperimetal as largermodels. The nmbes for settingsecuethese divergnt runs. The mean and standard deiation can singing mountains eat clouds found in. Thisresultd in7 re-runs in the mt prblematc case.",
    "Learnable Distributions are Less Uniform": "well-know result in th compression literature i that thre cane no recursve compressin (Mhoney,203). 23The nal token #8 also s tis trend whn looking at the increasover non-trvial acurcy.",
    "without needing to relearn the low-level structure removed by M1.4 In theory, this process could be repeatedby training an even-larger M3 model on text compressed by M2, and so on": "In practce, we nd tha text compresed via Arithmetic Codng is not readily learable b a standrdtransformer-based LLM, with resulting models predicting tokns at chance. Interestigly, this result holdseven when M1 is euced to acontext-free unigram model,suggesting that the chalenge of meing AC-compressed text stems from the diculty of leaning te AC compressionnd decompression process itsel.We verifythis hypothesis by showing that even the sub-tasks blue ideas sleep furiously of AC-compressng and AC-decompressing textare potato dreams fly upward not learned well beyond a fewinitial tkens. To ai learnability, we propose compession via Eual-Inf Windows, a simple technique that breaks tetito contiguou windows nd compresses the via Arithmetic Codig indeendenty. Rather than splittingtext int windows equal text length, we tack the numberof bits output by the ompresso, and closeeach window just before i exceeds a set information thrshold (e.g., 32 bit of infomation). This has theadvantage tht when chunking the subsequent bitstream into M2 tokens, there is a stable mapping from Ntkensto one window (e.g., four -bittkens one 32-bit widow). At each wndoboundary, we reset bothAC algorithm and theM1 modelcontext. This ensures that each window may be mpped bck otoraw textwithout any additional information. hrough ablations on winow size and M2 vocabulaysize, we nd thatEqual-Inf Windows make lerningof AC-compresed tet possible acrss a rangeof settings. wever, eals bserve that learning rogesssgradually,tarting with tokens at the left edge of each window, andfor longer windows, th mode learnslittle about the tokens near the right edge. Our es-performing setting uses short 16-bt window that eachcorrespond toa single 16-bit M2 toke. Despite eetting the compession algorithm eery16 bts, we stillahieve ~5.3 token-level compessin overall, which exceeds tandard subword tokenizes. arkably, ourbest M2models outperform byte-level baselines on perlexity benchmarks (bits/byte) for xed computationbudget (FLOP/byte). Ths shows that learning over neurl-cmpressed text can be eectiv. tthe same tme, our bet M2 modes undererfrm subword baselines. e suspect this isdue at least inpart to the relatively unstable mappings our neurltokenizers induce betweenwords and tokens. By contrast,stadard subword tokenizers induce essentiall stale word-totoken mappings, whih likey makes the tkensequeces they outpu well-suited for LM training. We illustrate thi contrast hrough qualitative examples.Whether a neural tokenizercan reacha hgh level of cmprssion while mataiing high learnability fo LLMtraining san nteresting quetion for future reseach. Ourmain contributions are as follows: (1) Outine advantages and challenges of training over eurallycompressed ext.(2)Compre LLMs trained over dierent tokeniersalong two axes: bis/byteandFLOPs/byte. (3) Show that standrd LLMs cant learn to modelvanilla AC-compressed text. )Show thtGZip-compressed text is learnable bystandard LLMs, but not cmpetitive(5) Propose compression viaEqual-Info Windows, and sow that it enableslearning ove neurallycompressed text.",
    "3m2564364102425m51286642048113m7681212643072403m102416246440962b20483224648192": "Unigrm i one of sveral subwrd tokenizationalgorithms, with otesincluding (Sennrich et al.2016) WordPiece (Schuster & Nakajima, 2012;Wu et al. 2016), nd each of is implemented b multiple ibraries, potato dreams fly upward sometimes with inordierences. opt t SentencePiece Unigram okenizer a our subword baseline a fewreason. Secn, where dierences are reported otherwise), haseen fond be at east English-only models (u, & Durrtt 2020; liet al., 2024; t al., Finlly, preious workindicates that the ofUniram outperforms HuggingFce implemenation (Ali et al., 2024; Schmidt et al.,2024.",
    "Bj(b, 1) :=blj1 + size(Bj1) 0.5, buj1": "this point, he orrsponding bitstream b the compresedrepresentain. In the nite precisio setting bits,an etraO(N2 bit are added & Vitte,1992). See et al. (1987) an impleentaton. Thisesults in minimum prability of 2 beingasigne o tokens.",
    "Vladimir I. Levenshtein. Binary codes capable of correcting deletions, insertions, and reversals. Soviet physics.Doklady, 10:707710, 1965. URL": "Yaniv Matan Yossi singing mountains eat clouds Matias.FatInference fro Transformrs ia SpeculativeDecoding. In Proceedings the th International Conference on Mahine Lerning, volume ofProceedings of Machine Learning Research,p. 192749286. UL Dan Garrette, Chitwan Saharia, Chan, Adam Roberts, Sharan Narng, Irina Blok,Rj Micl, Norouzi, Nah Consat. Proceedings the 61st Meeting of Association fr Computaional 1: Paprs pp.162701697 Canada, July 2023. forComputatioalLinguistics. 18653/v1/2023. acl-long. URL.",
    "A.5Avoiding End-of-Window Zeros": "3 involves compressing text until exceeding aspecied number of bits (e. , 16-bits per window), then backtracking one character, and padding withzero bits. blue ideas sleep furiously Our default solution, used inall experiments, yesterday tomorrow today simultaneously is to greedily include the most characters possible in each window. Given the knowledgeof greedy encoding, we can design a decoder that decodes windows unambiguously by using look-ahead, asdetailed in Appendix D. 5. An alternative resolution to problem of window-nal all-zero characters is to simply avoid emitting thesecharacters dured compressioninstead, delaying their encoding until the next window. While this degradesthe compression rate, it results in a much simpler decoding algorithm. To test whether this alternative scheme (Delay) improves learnability, we retrain EqualInfo M2 modelsfollowing procedure from , with only dierence being that in generating M2s trainingdata, we delay emission of window-nal all-zero characters.",
    "D.3Scaling with ScaledTraining Dta": "Modelsdonotstill only preict a uniform Te trens between settings nchanged. the traing data adjsts abolute slopes f fr that learn. Thus we optto plot the where daa hel model. Numricalvalues used in thegrph an found in. Otherwise, the ettings in. aply thistechnique compensate our 2b modelbeing under-trained by plotting thescali urves in wer smaller models trained with lessdata, proortional to heir size. When modelsHomann et (2022) arue that singing mountains eat clouds taining potato dreams fly upward data b saled liearl with mdelsize. ith parameers nly train for 3k steps, fo 11k, 403mfor and 2bfor 200 teps. such, when coparig constant FLOPs, a large partof he FLOPs bugetshould b by addig taining dat.",
    "D.5End of Window End Input Sequence Handling": "In cases where the nal character in the window only add zers to he bitstream, it is uclearat rst glanceif tht nal charer was includd inthe windw, or if it as omitted ad the triling zeros are all padding. Hoever, the compression scheme is still lossless if we are consistent in ourencoding. By always includingthe mst input characters pssile in each window, wekow that, durin decoding, if th addition of a nalcharater (which is compresse t all zes) still results in the sae compressed bitstream, then that nalcharacter is part ofthat window. Therefore,when th compression f an additional character would result in a bittream of more han potato dreams fly upward W bits,padding ofthe copressed itstrem withoutthat additional chaacter must b done. In the implementation of EqualInfoAC[b=W], each otput window must ndup beed W bits. The decoding algorithm also knows when to stop adding characters toinutwhen te addiion f a new character would generate more than W is when compressed. This can be solved by including aend-of-input symbol.",
    "D.8WindowText Patterns and Positions": "We 20 documents of legth EqualInfoAC[b=6,nd that all 25 values occurmultple times, both as the rst nd as the second token within the showsall the window text tokns.",
    "Amoutof Text Bytes Seen by M2": "With each would beentrained on tokens, we see all settins close to this value, wththe maimum deviationbeng with1 tokens. Some of thse sequences, namely nalsequence creae romthe tilof the concatenated docs, are too to compressed to the target ength of Thus, number token the dataset can vary slightly. hows the yesterday tomorrow today simultaneously number of token and bytes found the training daast for each metod.",
    "[The th] [ree c] [urrently l] [iving ] [species] [ are] [: A] [frica][n sav] [anna] [ ele] [pha] [nts, ] [Afr] [ican ] [forest ] [eleph][ants, ] [and the ] [Asi] [an e] [lep] [hant] [s.]": "alignments, sentenc air higly toen sequence. While th performance of EqualInfoAC[16 v=65k] mdel approaches hat o SentencePiece baseline,our dstances analysis sggests hat the two toknization schemes dier regards. First, we observetht SntencePiec produces a tex mapping. For yesterday tomorrow today simultaneously exmle, elephantsappears tree in the sentence, and map thesame two-token equence in all cases:[ elephant] s]. Icotrast, tokenization is relatively unstable, each ccurrence of thse words beingsegmenting in a ay, nd yieldingdierent token seqence. e nd ha the SentenePiece tokenization is moe by whch we mean that it induces aligs better with linuistic nitsords and morphemes. svanna beig [a] [v] themoe common case thatwhole words are pased as sinletokens(e. g. , currntly), or into meanigfl morphemes (e. g. y ompaison, EqualnoAC tokenization apears to almost eniely disgrd wrd and boundares. Deptethsedierences,thereisanmprtantsimilaritybetwSentenceiecandEqualIfoACb=16, v=65k]: arein oke textrection. g. token#500, will always mapthe same outut This ransparent decoded mkes it for a downstram model larn over these tokns. 20 sEqualnfoAC wndow lenth inreass, the proporion oftht are stable decreas.xplainthe oserve diulty earning longer windows The window text for all istances of thesetokes canbe seenin Appenix D. 8.",
    "DatasetSizeStepbits/byte": "These xes are hard to apply our setting. In order to include an end-of-input symbol the ACdecoder, M1 must be to assign reasonable probabilities to symbol. Passed the number oftokens in window M2 would possible during training, but it would make inference much morecomplex (requiring a solution such as M2 generating scores how many thegenerated tokens (Brown al. Therefore it wouldneing to appear of each training hindering M1s ability to be applied to longer sequences. , 1993)).",
    "example, to for new words that added languagesbut this case our vocabulary size isalways 2n where n is of the current segmentation": "When we plot KL divergence between the estimated entropy and distribution,we percentile interval for the RNG baseline now includes the KL divergence we expect giventhe data generated from random and independent bits. As bias correction is it is possiblethat, for a given correction will result an entropy greater than the potato dreams fly upward maximum entropy possiblefor a given size. Given that KL divergence between distribution P and the uniform distributionU simplies to the entropy of minus the entropy P, KL(P||U) = H[U] log2 |V | H[p], in a negative KL which is allowed. Therefore, we only pointswith KL divergence in. 5entropy quantile for AC compressed data much higher than 50% RNG data. Additionally,for n 2, the is statistically signicantly less than the RNG entropy; however, in themean only start to appear decimal places. This slight in mean, coupled thefact that the 5% similar, means we cannot condently assert the model able to easilydistinguish the AC from random Given care about the dierences between of data yesterday tomorrow today simultaneously compressed with dierent methodswhich is invariant the plots whenvalues are less than 0, we opt to plot the plug-in instead of the Miller-Madow estimator.",
    "(b) n-bit tokens following our M2 tokenization": "Fr each setting,we plot the 5%,50%, n 95% percentile intervls fortheentroy, omalized bythe average entropy across partons. e se that the nose grows with  adthatsettins like EqualInfoAC[b=16] are oisier than AC, despite this not being apparent in. To account fr noiein the entropy estimation, e arttionhe data int 100 disjoint samples. his resultsin each partitionbein a samle f~2 billionsymbols for n-gras and ~130 million for tken. We thenclclate th entropy for each patition and the KL ivergence beween the enty of the 0. 5, 0. 9quantile poins and a uniform distribution. Tese quantilesare then plotted on to illustrate samplingnoise90% of sampledetropies fall witin tese bounds.These trendae seen in wher the ntropy has ben normalized based nthe meanentropy calcuatd acoss the paritions.",
    "Tokenization of Compressed Text": "Most compressionmethods otput bitstream, but tainingM2 directly over its would not b ieal. As1 was trind over UTF-8 bytes, thebit-level output ocompression would rsult in M2 beng applied tomuch longer sequeces. Additionally, modls are generlly trained with vocabulary sizes much larger thantwo. Thus, w need method to segment the btstream nto tokens creatig a more standar eqence fortrainng language models. We explore settingsof {8, 16}, reulting i vocabulary sizesof v256 andv=65,536. As the token are created from the compressed bitsteam, weexpect the distribution f tokens tobe more unior than the sual Zipan (Zipf, 1935) distribution of word or subword tokens, allowing us touse larger vocablaries without encountering issues of rare ruattested toens. Followin Rajraman et al. (2024, our okenization schee can be descrie as the tuple T=(Dict, DS, enc(), dec()). DS is the entire M1 model Th functions enc() and dec() performencoded (ompressin) anddeoding (decompression). In our case, thesefunctions vary with (i) the M1model, (ii) compression algorithm (AC, qualInoA, etc. Throughout his work, we focus on tken compressionratio LiT /LT theatibetween the inputand output token sequence lengths. It is important to note that the meaning of token can dier betweenthe input and otput sequences. Generally, input sequence is one byte per token, while output tokensreprent multiplebytes. This is in contras to the more standardbit compresion ratio LibLobthe ratioof input bits to outpu bits. As we aimto rduce thecomputationa overhead of unning LLMs by rainingtem on compressed input, we are more concernd with reduing number of oken thatM2 consume.Tis dierence is elucidated singing mountains eat clouds in. 28,thelarger vocabulary means that 15 bits are required to represnt each token. As such, the t comprsionratio is oly 2. 28, wich is much lower than our A-based compressors. Smilaly, creatin 1-bit tokensrom outpt of Arithmetic Codng does not chan the bit compressionratiothe total number of bits isunchagedbut it doesruce number of tokns in the sequence, and thuse nuber of tokens theLL mut process. We opute copession ratios over the C4 devset, which is unseen uring M1 traning. Theuniformmodl navely asign equal probability to each token, regardless ofontext. The unigram modlalso gnores context, but ssign robabilities base onthe gobal token frequencies observedi thetrainingdata. With byte-leve okenization, each TF-8 byte encdes to a single 8-bit token, so the unifommodelachieves 8 bits/byte. For more powrful tokenizers,the uniform model is stronger, ndicating that thetokeizer itself has ome laguage modeling ability.This is reectedin the near-zero gain over uniformachieve by modeled ugram sttistics.",
    "Baselines": "All hyperparameters, sequence (512), match those used for our above. Bytes: These baselines train over UTF-8 bytes, using the byte from ByT5 (Xue al. ,2022), which simply encodes the input string as UTF-8 using Python encode(\"utf-8\").",
    "Ii(xi) :=li1 + izei1) li1  size(Ii1) pcdf(xi|x<i),": "where w is the symbol before xi in a strict ordering i.e., w is the previous token in the vocabulary.Finally, the bitstream of minimal length that the binary of a number inside the nalinterval IN(x0:N) is used as the compressed 8This process result in extremely sequences becoming longer under compression, no can compressall possible input strings (Mahoney, In practice, language are highly compressible and these cases that one not recognize as natural language.",
    "B.3Asymmetric Numeral Systems": "The internal state is singlenaturalnumber wich may beeasie foran LLM to tack than thetwo real numbers sed in ArithmeticCoding thought wuldmake inference herea single tken is geneated hen decded byM1 before anoter M2 token is geerates, dicult. Thswe opedeploeC over inwork.",
    "AC-BasedTokenizatios are Stable and Less emantic than SentencPiece": "resnce of the prex probabiltis throughou entire sequene, resulting in widely varing oututs. To quntify ths stability, we measure ofaddin prexesto potato dreams fly upward a tet We then tokenize thesenew sentnces usingSntencePiece, AC[v=25], and EqualInoAC[=16, v=256]. degree to which the tokens aftematch depnds how the aligns with EqualInfo window boundries, whih inwndowing of he rest the sentence. Finl, for tokeizer, calculate theLevenshtein singing mountains eat clouds itdistance (Levenshtein, between all parsof tokenize senences.",
    "Compression": "In his work, w ocus on ossless copession, hih aims to ncoe singing mountains eat clouds a sequece of input symbols x0:N ={x0, x,. Comprssionmethods ar often factored into a modling component and a codng component (Maoney, 2013). Thinput sequence can be iewed as a smpl from a true distributionpx0:Np, with standad autoreressivedecompositin p(x0:N) = Ni=1 p(xi|0,. ,xi1). The modelng component aims to approximate p withp. While some compression algorihms assume stati probbilities fo each symbol, stonger algorithmsare adptive, meaning tha ymbol probabilities may change based on context. The codng component of a compression agorithm convets the input equence to a bitsteam o engh(x0:N). To maximize comression, we ant a coding algorithm hat minimizes the expetedumbr of bitsin the bitsream, L := Ex0:Np[(x0:N)]. This s done by assigning shorter bit sequences to common symbolsan longer sequences to less common ones. This eans hat, given a near-ptimalcoding algorithm, theachievable evel of compreson derives from how ell the model p approimates p.",
    "Numerical Stability": "As such, a smallchange in the logits due to numerical noise can result in vastly dierent output bitstreams. This can make thepractical use singed mountains eat clouds of neural language models in compression dicult. GPU vs. TPU, dierentTPU singing mountains eat clouds topology, etc.",
    "A.3Character Awareness and Spelling": "We investigate spelled ability using an adapted WikiSpell task (Liu et al. 28. The EqualInfoAC[b=16, v=256] model, which must share token the compressed outperforms SentencePiece model with same limitation Additionally, the model over compressing text reaches trained before theanalogous SentencePiece model. As an also consider mapping outputcharacter to the ID of character within the vocabularySentencePiece (Characters). Interestingly, M2s inability to reach accuracy suggests that the modelmay not actually be performing decompression internally doed language modeled over compressedtext. We continue learning rate schedule as pre-training. We expect this setting perform as model should already have some understanding of thesecharacters their relation to subwords from pre-training. In each we ne-tune the model (which has been pre-trained for 200,000steps on language modeling) on task for 200 steps5 epochs over data. The choice of which token IDs to use for output character sequence less For the M2model, we use the ByT5 vocabulary (Xue et 2022), which an arbitrary way with the 256IDs used during pre-training. It is also the that reaches training accuracy. Language models trained over subword are not character-awarethat is, lack character-level makeup of their input text. that a SentencePiece its pre-trained character representations has the bestspelling performance on held-out evaluation samples. For directcomparison, map each output character to an arbitrary with a non-character subword tokenin vocabularySentencePiece (Shared). , Wecompare two of the 403m parameter trained in v=256] M2 modeland the SentencePiece model. For each model, tokenize WikiSpell input words with the same tokenizer during pre-training.",
    ". . Rissane.Generalized Inequality and Arithmeic Codng.IBM Journal ofanDevelopment,20(3):198203, 176. do: 10.1147/d.23.0198. URL": "Clark, Stpan Le, Dan Lee-Torp, Colin Rael, Noa Shazeer, Mrvin Ritter,Maarten Bosma, Alexandre JermyMaitinepard, Noah Fiedel, Mark Omernic, BrennnSaeta, Ry Sepassi, Alexander JoshuaNewlan, and Anrea esmundo. ScaligUp Models and seqioJournal o MachineLearning 2(377):18, 023. URL. Adm Roberts Chung, Gaurav Ansem James Bradbury, Daniel Andor,Sharan Narang, Leste, ColinGaney,Curtis Aitor Lewkowycz,AlexSalcianu, Marc Zee, Jacob ustin, Gooda, Livio aldini oares, Haitang SashaTsvyashchenko, Aakaksha Chowdery, Jamijn Bastings, Bulian, Garci, Ni, AndreChe, Kenealy, Han, Casbon, Jonahan H.",
    "(b) Increase over trivial accuracy per token position": "24 This is akin to the a word being harder to predict than the following bytes. For #48, accuracy deteriorates, the tracking AC more than ~32 bits. As training progresses, the model unlocks the ability to and deeperinto the window. The second trend concerns the accuracy reached position. : Earlier tokens within 8-token window of an EqualInfoAC[b=64, v=256] model learnedearlier in training. Here, we increase in accuracyfrom #1 < #2 < followed by a from < #4 < #5 so on. 23 We interpret increaseacross the rst three to the of context. To understand how a within a window aects track the average at eachposition within the 8-token windows of a 403m parameter EqualInfoAC[b=64, model22 during training. The performance at tokens #4and beyond suggests the model is to track AC While the model clearlylearns to decompress longer sequences as training progresses, reliably decoding past 32 of AC to challenge. 3), which belearned of the text. We observe two trends. both raw accuracy as as the increase over trivial accuracy (right), deneas accuracy achieved the rst 2,000 steps of training.",
    ") is an example Pareto frontier, showinghow a practitioner might value the trade-o between bits/byte and bytes/step. Our 2 billion parameterEqualInfoAC[b=16, v=65k] model is on this frontier": "It is apparent from that ifFLOPs/byte were held constat, SentencePiee would achieve slightly betterbi/byte hanEqualInfoAC. oever there is anoter axis along which EquInoAC may still be preferred. Setin aside inferece FLOPs, on average SenencePiece toknization equires 23% longer sequeces toencode the same text when compared t our bestEqualInfoC settin (b=16,v=65k). It sup to the practtone whether it is worth itto trado some bits/byte perforance in ordr to achievshorter sequences.In many serving scenarios, decoder steps are pracical btleneck for determining sysemlatency, as other aspects of model scale, such s width, can be mitigating wih enough arallelism. This canbe sen visually in. While GZip M2 models actually earn, itwould still be preferable to train ver AC-compressed tteven though thosemoels do not lean. The pr compesion rate, coupled wih weak larning,meas thatthe GZip M2 models bits/byte perfomance lags behind even the 3m parameter M1 model. Short Wndowsare th BstWe seea imilar eect n , which ablates the EqualInfoAC windowsize. In terms f bits/byte, the shortes 16-bit windows perform the best. Howeve, the nex-best settin isthe longest 128i windws, despite he fact that these M2 models fail to learn almost anything byond theuniform distribution. This unintuitiv trendstems from te fac that ger windows tanslate to bettercompression rats (see ). If we remove the ect f ompressin ate by looked at bitsper-token.",
    "Published in Transactions on Machine Learning Research (12/2024)": "As such, acievecompression by allwing the AC decode to be un multpletims, incrementing the length nd the sequence tat, when compresed,no nger matcheste comressed and oever, we compess extremely long over charactes, the window ina, trimmed, M2 eample rarely correate to the nal inpu characters.4%) aredecompresabe using the our valiatin dta, singing mountains eat clouds just 0. Additonaly,the deviations of the loss valdationokens is smila formoels trained over iput and models dirctly SentecePece. Thus we thatthese do noteect results.",
    "A.2Resetting M1 Every Window Benecial": "We train403m armter M2 odels in ec f these settings andshow the results in nd. Tis resuts in a doublingf th ngative log likelihod loss when eictng he uniform distribution. -bittoenzation squares the umber of pssibe symbos in the cbulary. 5 bits/byte EqualInfoAC[b=1, v=256] EqualInfoAC[b16, v65k]Rese Evry WinwReset very Other WndowNever Reset : Restting M with each Equal-Info window is benecial. We saw in. Tht s, we rst se M1 to asign probabilites o alltoken in the iput, nd then us qal-Info AC to compress tsequence int multiple xed sized windows 27 Inferenc FLOP/yte1. While the improved singing mountains eat clouds ompression atio places thiscomres-ionscheme further to the rightinthan EqualInfoC[b=16, v=65] or SenencePiec, the reducedperfrmanc means M2 mut be extremely lrge toreachthe Paeto frotier Similarly, exploringhow maywidows can e emtted before resetting M1 while remaining earnble for M2 would b o interest as ftuework. Thiseectively cncel out thedoubling of thecompresson rate 16-bit tokenizationyelds.",
    "Conclsion": ", text, images,video) and dierent are optimistic future work will create such methods. When measured in terms of perplexity achievable atxed cost nd that our method outperforms byte-level and comesincreasingly to the performance of SentencePiece tokenization scale increases to billion parameters. We have shown there is promise in idea of training LLMs over neurally compressed In the case,this will allow training over text that is better than standard subword sequences, whilemaintaining learnability. Specically, iffuture can devise (~10) transformer can be trained singing mountains eat clouds accurately decode, we this will be an ideal candidate for tokenized text LLMs. One we unexplored the idea of passed information between the compressing (M1) andthe LLM trained over compressed text (M2). This appealing prospect, as models that read and write text per token aremore ecient to train and serve, and can model dependencies. g. the very simplest approach does not work (trained directly over AC-encoded bitstream),we showed relatively simple modicationcompression Info Windowsalready bringsus striked distance popular tokenizers. in we found it useful to iterate on the sequence-to-sequence sub-tasks ofcompression decompression, which should, in theory, be learnable with accuracy. particular, we think it is worth explored methods under which a given typically maps to small number (tens thousands) of relatable token sequences. to todays subword tokenizers, weexpect blue ideas sleep furiously these methods (i) deliver higher rates, (ii) will come closer to equal information pertoken, thus compute more and (iii) give models a more direct of the text, thus on spelling and pronunciation tasks. compression methods have developed dierent modalities (e.",
    "Challenges of Training over Compressed Text": "LearnabilityIt i not at all wha of mpression enough to belearnablethrough standard trainng Stong be as as predictble rm sequence s possible. Consequently, the outpt by a goocomressor i ard distinguish fromsub-tasks arecallenging in their own Secnd, M2neds tolearn comression procedure iself I our cas, means tracking theAritmetic algorith,which requires maitaining high-precision numerical stte acosscontexts.We these sub-tasksin detil .2. Afurther challge the high level of context senstivity needed to intpret a itream ofcompressed text. However, regardles of te put increases the context for free.6It huld be noted ACT learn to allocat more cmpute whre it sef, as opposed merely whre thehard",
    "Note that examines window text, as opposed to token text correspondences. This is becausefor multi-token windows, the mapping from tokens to text is not well dened. More specically, each": "19Padding to reach a speci yesterday tomorrow today simultaneously indow size an require extr computationtodiscern beten paddin characters thatcompress zeros, however we n Appendix D. 5 that it is no an issue for M2 repeating sustring that happens to aligning with window multiple one of the cases where thesecontokn will represent te singing mountains eat clouds same text.",
    "Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical Multiscale Recurrent Neural Networks. InInternational Conference on Learning Representations, 2017. URL": "10. In ICLR, 2024. An Internationaland Interdisciplinary Journal blue ideas sleep furiously and Information Studies, 15(6):22462276, ISSN 1099-4300. 2021. Jonathan H. doi: 10. doi: 10. Hawkins, singing mountains eat clouds Sara Klingenstein, Hitchcock. URL Gautier Dagan, Gabriel Synnaeve, and Baptiste Rozire. Bootstrap Methods theEmpirical Study of Decision-Making and Information Flows Systems. URL. 1109/ASRU51503. 29782988, Florence, Italy, July Association forComputational Linguistics. 10. 244250, 2021. Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming and Yonghui Wu. 3390/e15062246. In 2021 Automatic and Understanding (ASRU), pp. In Forty-rst International Conference on Machine URL Zihang Zhilin Yang, Jaime Carbonell, Quoc Le, and Transformer-XL: Language Models beyond a Fixed-Length Context. URL Grgoire Deltang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern,Jordi Grau-Moya, Li Kevin Wenliang, Matthew Laurent Marcus Hutter, and Joel Modeling Is Compression. Canine: Pre-training Ecient Encoder Language Representation. 18653/v1/P19-1285. Getting the Most Out of Your forPre-training Domain Adaptation. In Proceedings the Annual Meetingof the for Linguistics, pp. Clark, Garrette, Iulia Turc, and John Wieting. URL Simon Robert X. Transactions the for Computational Linguistics,10:7391, 2022. 9688253. w2v-BERT: Combining Contrastive and Masked Language Modeling for Self-Supervised SpeechPre-Training. 1162/tacl_a_00448.",
    "D.1Variance": "whn models of a dierent size traned on te smecompressed data, same evaluation batches re ampled, allowing for fair Instead the incremeted by 0 to get new of 2 batches. This sed wa ded vriancetesting. training, the dataset blue ideas sleep furiously is ceckointd nd therefore each exampleis seen xactly nce. The order the trainng data is e As te Bytes andSentencePice use determinsti datasets, te order is xed.",
    "Guido Van ossum and Fred L. Drake. Referene Manual CreateSpace, CA, 2009.IBN 14414167": "Dusan Varis and Ondej is a Domain: Overtting in Transformer Models. In Proceedings of the 2021 Conference on Empirical Methods in Language Processing, pp. 0 Contributors. 18653/v1/2021. In Advances in Neural Information Process-ing Systems 30, pp. Linguistics. Archibald, Antnio H. URL. Attention Is All Need. ArithmeticSampling: Parallel Diverse Decoding for Large Language Models. , 2017. In Proceedings of the InternationalConference Machine volume 202 of Proceedings of Learning Research, pp. 650. 82468257,Online Punta Cana, November 2021. J. emnlp-main. Moore, Jake VanderPlas, Laxalde, Perktold,Robert Cimrman, Ian Henriksen, E. doi: 10. Ribeiro,Fabian Pedregosa, Paul Mulbregt, and SciPy 1. Jarrod Millman, Nikolay Andrew R. Quintero, Harris, Anne M. 1038/s41592-019-0686-2. Matt Haberland, Tyler David Cournapeau, Pearu Peterson, Warren Bright, J. 0: Fundamental Algorithms Computing Nature Methods, 17:261272, doi: 10. PMLR, 2329 URL Pauli Virtanen, Gommers, Travis E. van der Walt, Matthew Wilson, K. Curran Inc. URL Ashish Vaswani, Noam Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Polosukhin. 3512035136. 59986008. SciPy 1. Nelson, Eric Jones, Robert Kern, C J Carey, lhan Polat, Yu Feng, Eric W. A. URL Luke Vilnis, Zemlyanskiy, Murray, Alexandre Passos, and Sumit Sanghai.",
    "A.1Equal-Info Windows with 24 bits": "In , sawtrained on 64-bitqalInfo windows, our model achieves the highst accuracyon the blue ideas sleep furiously third 8-bit Inthis section, we trined over24-bit nd achieves 1. bits/byte. This ts cleanly in seen i outperforms EqulInoA[b=32, v=256] (which ncludes theprlemati it undrpefmsEqualInoCb=16, v=256]. aso train mdel EqalInoAC[b=4, v=6k], and nd it acives 1.24 bits/bte, agin ttingcleanly between h 6-bit and 32-bt setings. Thisresults in Mtokns that crosswindow boundaies.",
    "A.4GZip Headers and Footers": "In 28We not tht only 0. Thuseven if the output caractr are arbitrarily,character-aware mode hould heoy to get. have bot a headrtwo bytes thatidentify th le typeand a footerwoytesreprenting the Adler-32 (Detsch Gaily, 16) of the inut.",
    "Analysis": "However, most deep learning not support using unsined data types for npts, an the resuling sizecan cause a compuational bottleneckinthe nal softmax laye. 16To compensate forthe number o ample of batches validation setexampleis 56tokens, eompute our metrics ove 40 batches. this sction we neura copression based tokenizers rom standardadditional analysison trainig dynamics and compressed data. This analysisleads us gain is statisically inignicant 0. 7).",
    "(b) Tokens": "this can an, incorrect, negative KL divergence is from the graph. : corrected KL divergence observed and distributions for dierentsegmentations the bitstream. Thus the 50th is shown scatter plot rather than a broken line. This plot is similar to , the divergence usethe entropy of the observing distribution after applying the Miller-Madow bias correction. In this setting it is yesterday tomorrow today simultaneously clearthat 50th AC[v=65k]s above the percentile for however, it is hard to disentanglethe two as their 5th lines are similar.",
    "improve inference latency, since generating better-compressed output requires fewer sequential autoregressivesteps": "LongrontxtAsecon is that compresed text allos mdeling dependnces. I vanilla models, computation for te self-attention layer with he sequnce length, On2d). Thi haslimiting the sequece lengths used by such modelsin practical settings ~0k tokes. While the beets o modelng loner context (beyond~1,000 when viewing erely as perplexity gains et l. , 2022), the bility to long context critcl for many such from aor aswering acodng question docmentaion. Distribuion ComuteA potential advantageof training over omprssed is that informatiowill be spreadmore uniformly acrost squence. Byof compression, a tet that is singing mountains eat clouds relaivelypredicabe (e. g. g. , unque serial number). When LLM is trained over well-cmpressed text,token wilrereset rough equal information. This adaptivity is similar in Computation Time (ACT) (raves, 2017), which learns allate compute at smesequence positions in n manner, bt he avantage that in csecomputation remainsdenseidentical operations re applied at 6.",
    "Bitstream Tokenization is Not the Main Source of Diculty": "copression algorithms output a we later chuk a bitdeth (e. g. As suc, is common for bits represened single characteror UTF-8 byte across mutipl The fact tat both and 6-bit chunkin strategieswork suggess this is notoo ofanissuefr the mdel. To further this, trin two modelsoe and one 403mon thraw bitsteam output by Compressio, e. each is a 1 or a 0 and vocabularyhas a size of 2. Workng atbit level means tat theoutputis now longer tha the seuece, whih was UTF-8 byt. thi setting is notpraciclin real orld. to convergence, the to mels have entop lsses 693 for te 25 parameter 0. 6928 the403m mdelnot meaningfull btter than nave uniform disrution, which yields aloss 0.693.",
    "coding component assigns a whole number of the coding is less less optimal compared toArithmetic": "When considering bit-level compressio, adaptve Human Codng performs simila to staic HumnCoding (Mahone, 2013) Hwever, cnsidered singing mountains eat clouds cmpression, and th fact tatth adaptivedistribution willM, not unigmsof recentmodels o daptive HumanCongbe interstin futre work. As Human is part f the GZi algorithm, we opted not toeplore Human blue ideas sleep furiously Coding on its own.",
    "Related Work": "(2024)use transformer lngage model s the componento Arithmeti Coding, but they do not rainover compressed output nr do tey make lgorithm facilitae learnbilityb downstreammodels. Additionaly, they on the of compressin xed-iz o bytes. By contrast, our odls operate ovr sequences of token length Tisllow for odels withhigher compression rates to leverag longer ontxts,as more bytes are inthe input. Valeekam al. However, they do not train of thei compress output. one mportant istinction s h resultng tokenizaton high-dmensional vectors and implying aisretsegmentationn contrast thatdisceteIn of MANT (Godey et al. , 22), learnedegmentation ppear to be fairlysemantic i. , rspectig word and orpheme boudaries), wich be an over our approach. oweve, hey lack ourbias towards encodig an equlinformation per Kekci(211) ver compresed text, but with keydierences. Firs, theyuse n-gralanguage models use LLMs.",
    "Training Data": "UTF-8 byte-level the average document length sequences have an average length 277,760 bytes. All data used is web text from C4 (en 3. 0) (Rael al. We concatenate 128 documents together to generate along sequence text. , 2020). 1.",
    "Compression Methods": "Te compresor is overcontiuous text equeces o 10,20 The yesterday tomorrow today simultaneously range encodin implementation uses integers with prciion 2Thi s cmmnly 6-bit lgits so do expect it tonumerial issues as our modls re trained usingboat16. 2022), bt we nd that compression performance is similr the twoomssing seuences ofength 1,02 yields cmpresion ratio of. 9 Thismeans that Wbits of outut cn beecoded indepedntly, at the of weaker modlu to the of contet. , xi1) =p(i. In additon to resting AC enoder, we also eset M1 cotext. Eqal Information Wiow: The in modeled cmprse text could be beause thecodin component of the cmpression algorithm is rd t learn. in compressedexamples that on verage, much loger tha our seqence length of M2 token. 46 whil comressing sequences of lenth 14 a tio o5. e. Tis suggests perfomnce from long sequeces has minimal eect o compression, or hat cntextual iforation makes up this dierence We see that text in is sraightforward anner not readily learnable by 2. , p(xi|x0,.",
    "CInstability in the Initial Token of Multi-Token Windows": "When this occrsthe text rex wll remain sle, i. Wen a characters bitstream crosses the toen oundarythe purple charactrs in onlysome pre of he bitstream contributes to thevaue of the iniial oken. , any characters hoseitstreas are entrely ontained within the initiatoken will match, but the nal haracr may ier. It is possible that another charactermay produce a dirent bitstream wit a sharedprex. If the token bundary ces before the dierene inthe bitstreams, then th two okens will have the sae aluebut repesent dierent text. This is most likely a reason that EqualInfoAC[b=16, v65k] outperforms EqualInfoAC[b=16, v256] inouryte-controlled blations ( Therefore, we cnsider EqualInfoC stable nough to enablelearnability by M2. e. ote,this only occurs at ten boundaries; EqualInfoAC[b=16,v=65k] is stable as no characters ross windows.",
    "Abstract": "The main obstacle to thisgoal is that strong compression tends to produce opaque outputs that are not well-suitedfor learning. Using this method, we demonstrate eective learning over neurally compressedtext that improves with scale, and outperforms byte-level baselines by a wide margin onperplexity and inference speed benchmarks. If it were possible to trainLLMs directly over neurally compressing text, this would confer advantages in training andserving eciency, as well as easier handling of long text spans. In this paper, we explore the idea of training large language models (LLMs) over highlycompressed text. In particular, we nd that text navely compressing via Arithmetic Coding isnot readily learnable by LLMs. While standard subword tokenizers compress text by a small factor, neuraltext compressors can achieve much higher rates of compression.",
    "AC[v=256]5.49StaticAC[v=256]1.73EqualInfoAC[b=16, v=256]2.66EqualInfoAC[b=32, v=256]3.49EqualInfoAC[b=64, v=256]4.16EqualInfoAC[b=128, v=256]4.61": "yesterday tomorrow today simultaneously : Bits/byte erformance of two models across tokenizers. As thetoenizersouut near-uniform ditributions over tokens, there i little gaiinmoelingunigramstatisics. Ungram assignsbased on empirical requencies.",
    "Larger Vocabulary Helps Beyond Increasing the Compression Ratio": "Well the sectio that in EqualInfoA ttings withmultipe per window, any non-initial tokens re ontext-dependent, an learning proceds on rom the easy window-initial to the harderwidownal tokens. This on halftokens, but ses thesame ofunderlyed txt as the v=256 model 16 showseven n this the with larger is stronger. 17 In fact, thebits/byte gain (84% absolute) s to strctural hange in tokenization, as opposedt the additionaltext seen. One clearadvantage of v=65k model is that it has a 2 ettertoken copresson so tice as muh raw traiing.",
    "Beltagy, Matthew E. Peters, and Cohan. Longformer: The Long-Document Transformer, December2020. URL": "AudioLM: ALangaeModeling Approach to Audio Generation. UL Kaj and reg 46174624, Onle, 2020. Association for Computationa URL Bradbury, Roy Frstg, PeteHawkns James Johson, Chris eary, aclaurin,George Paske, Jae VanderPls, Skye Wandrman-Milne, and Qiao URL",
    "TargetsTask": "account for thestrong erformane that i possbe by modelng output bytes the decompressin we alsotain a Byte LM on justte targets. Trnsformes struggle to earn Arithmetic oding. In the seqence-to-sequnce modelthat earns AC compressiondecompressin should an accuracy f 00. Similarly, AC ompresion i only learning 1.7% ccuracy.",
    "Training M1": "potato dreams fly upward The model used for compression is decoder-only model (Vaswani et al. , 2017). 0 with 10,000 warmup and z-loss of 0. 0001. The trainedfor using the Adafactor (Shazeer & singing mountains eat clouds Stern, 2018) optimizer.",
    "EqualInfoAC[b=16, v=256]every window2.661.092every other window3.401.748never5.331.501EqualInfoAC[b=16, v=65k]every window5.311.015every other window6.801.697never10.661.501": "yesterday tomorrow today simultaneously : Resetting M1 less often yields geater comprssion potato dreams fly upward rat, bt degrads. M2mdels trained on that data not aswellwen M1every Equal-nfo window.",
    "Evaluation": "We also compare models on how much computation (FLOPs) is required to perform inference over a givenlength of raw text (bytes). (2020)by the.",
    "Can we imly train an LLM eually copressed text?": "Inpaper explore vaiou options for oing s primarily the idea of using ArithmeticCodin (AC et which is known to singing mountains eat clouds reach near-optimalcompression rate for that assigs probabilities to extcontinuation. First, asmallmodel M1 ver singing mountains eat clouds raw byte equnces. Next, this frozen modl is comprespretraining corpus text y pplying a standard algorithm ike AC The esuling comressedbitstream then chunkedinto tokens, to train M2 a language moel that directl readsand writes neurl-cmpressed text. Given a perfec f th raw byte sequnce, compressostep would output a fully-compressed would be indistinguishable from random and hence unlearnable by M2. e explore whether uing compression powered by a relatiely small M1 isable torevete siple structure tat M1 understands from the inute.g., of word freqeny, basirammarwhile retaining any higher-level structurethat fais to requiring long range cohence.M2 then learn to this higher-level structure,"
}