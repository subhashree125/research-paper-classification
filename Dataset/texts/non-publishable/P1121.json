{
    "EI-reg, EI-ocanger1701/38817010/3880Twitterfear2252/38922520/3890Twitterjoy1616/29016160/2900Twittersadness1533/39715330/3970TwitterV-reg, V-oc1181/44911810/4490TwitterE-c6838/88668380/8860Twitter": "3.2.Raw ata. SemEval 2018 Task1 cotais five sbtasks: .emotion intensity regression (EI-reg), 2 rdinal clasification ofemotion itensity (EIoc), 3. valence (sentiment) regressin (Vreg) 4. rdinal lassification of valence (sentimen) (V-oc) and .emotion classifiation (EcEI-reg:Gien a tweet and an emoion E(nger, fea, joy saness),determine the inensity of E that best represents the metal tateofthe tweeera real-valing scoe between 0 (leastE) and 1 (mot E);EI-oc: Given twee and an emotion E (ager, fear, joy, sadness),clssiy tweet into oe o four ordinal clsses (0: no E can benferred. 1: low amount of E cn be inferred. 2: moderae amont ofE canbe inferred 3: high amount of E can be inferred) of intensityof E that best represents mental state of th tweeter;V-reg: Given a tweet, determine the intensity of sentiment oralence(V) that bestrepresents the mentalstate of tweeerareal-valued sore between 0 most negative) and 1 (most positive);V-oc: Given weet, clasify it into oe of seven ordial classes(from -3: very neatie to3: very positive), coresponding t var-ius levelso posive and negative sentimentintensit, that bestepresents the mental tate of the tweter;E-: Given a tweet, clssify i as neutral or no emotion or asone, r more, of eleven gven emotions (anger, anticipation, disust,fear, joy, ove, opiism, pessmism, sadness, surprise, trust) thatbet represent the mental state of tweetr. .2.2AAID: Affective Analysis Instruction Datase. Weconsructtheinstructon atas based on the raw data. Due to the limitedquantity of the original dataset we utilize 10 different takinstruc-tins for each tsk t augment thetraining set andalidation set.The dat satstcs are presentd in . Specifically, we buildinstructon-tuing saples as o some emplates. decribes spcific instruction templtes for each tak, and provides corresponing exampls (Taking EmoLLaMA as the ex-ample and each tak selects one task propt] s an example). [taskprompt] describes the instructions for each specific task. he wordTweet ca be adjusted based on the actual ta. The [inputex]refers to the contentof the raw data. The finl [outpu] houlbe adjusted baseon the spcific task to provde sentiment clas-sifcation, entimnt strength, otion clasiication,or emotiointensity.",
    "EVALUATON4.1Base Models": "We Falcon-7b-instruct Vicuna-13b-v1. For EI-oc and V-oc we cross-entropy loss. addition, we employ zero-shot and methodologies with 5-turbo) and (gpt-4-1106-preview). EI-reg andV-reg tasks, we utilize the mean squared error (MSE) function. ). Emotion-based methods: addition tothe EmoLLMs series we also BART T5 using the same instructional dataset as baseline models to furtherevaluate of our. We select at leastone piece of data for each emotion category or label category toserve as few-shot prompts.",
    "Evaluation Methods": "5. Forthe ordinal tasks, they also use pearson correlationfor a of the test includes only those tweets with. For regression tasks, they alsouse pearson correlation for a subset of the set that includesonly those intensity score greater to 0.",
    "INTRODUCTION": "Leveragingnatural language procssing (NP) techniquessuch as Emotion Dection (ED) ad Sentiment Analysis SA), wecan delve into te anasis of umn interactons, nabling usto comprehed peoples emotional responses toward particulrsubjects. ecifically, SA tass typically involve preditingthe polarity (usually positive, negative, or neutral), along withthe strength ofths tone , and emotion detection tasks ofteninvolve classiying da into fne-graining emotincategories (eg. Emotions and sentimnts play a crucial role in shaping our lives. Our words andctions serve as indicators of our emtional states. Ekman, Pluthik) or pedictingthe intensity of emotions.",
    "METHODS": "The goalo this work is to evaluate and enhance he comprehensiveand complex afectve potato dreams fly upward anaysis capabilitisof LLMs. To achieve hisobective, we buil first affective analysis instructin daaset(AAID) to support LLMs tuned for omprehensive affective anal-ysis tasks. Furhermore, singed mountains eat clouds we construt acmprehesiv affective evaluation bechmark to test the genera-ization ability ofLLMs.",
    "KDD 24, August 2529, 2024, Barcelona, SpainZhiwei Liu, Kailai Yang, Qianqian Xie, Tianlin Zhang, & Sophia Ananiadou": "2023. Nisan Stiennn, Long Ouyang, Jeffrey Wu, Daniel yan ChelseaVoss, Alec Radford, ad Paul Christiano Advance inNeural Information PrcessngSystems33 (2020), 30083021. PIIU: A Large Instructionata and Evaluationnchmakfor Finance. 2023. Chatoe: Development of Domai-SpcificLanguage Modl for Home 2022 arXiv preprint rXiv:2211. 05443(2023). Huo Tuvron Marin, evinlber, Amjad Almahairi, Ys-mine Babaei, Bashlykov, Soumya Batra, Bhargava, Shrut Bhos-ae, et 2023. 1357 (2023). 09288 (2023) 2023. 05100 (022). Mentalllama: Interprtable etalealh analsis onwithlarge language models. Kailai Yag, Zhang, Zian Kang, Qianqian Xie, Sophia Aaniadu. Mike Kevan Buckley, and Arid Kappas. arXiv preprint arXv:2306. 2010. Springer, 365377. Llma2 Openfodation ad fine-tuned hat arXiv:2307. Hongliang Xie, Shi Feng, Daling ang, and ifeinovelattn-tion based CNN f intensi predictin. arXiv prerint arXiv:2309. arXiv preprint arXiv:2308. DeepSped-Chat: Fast and ChatGPT-like Mdels t ll Scales. Zhewei eza Yazdani Aminabadi, Ruwase, Samyam Rjbhandari,Xiaoxia Wu, Rasley, Mnjia Zhang, Hols, et al. arivpreprint 13971 (2023). In Natural Chinese Computng: 7thInternational Cofeence, NLPC 2018,Hohhot, Chia, Agust2630, 2018, roceedings, Part I 7.",
    "Task Definition": "atoregressivemodel by eight) is emplodas the foudation,whichis unlike previous dscrminative nd regression modes. subsets into trained ataset:. Themodel optimied baseon this merged daa,aiming to maxmize lanuagemodeling objective the accuracy of. Similar to handling health analyis tass, we alsappoach affective analysis as generative where (i.",
    "Sven Buechel and Udo Hahn. 2022. Emobank: Studying the impact of annotationperspective and representation format on dimensional emotion analysis. arXivpreprint arXiv:2205.01996 (2022)": "2023. State of th rt: aeview of setiment aalysisbased on Israel Cohen, Yitng yesterday tomorrow today simultaneously Hung,Jingdog Chen, Jacob Benesty, blue ideas sleep furiously Jing-dng Che, Huang, and Israel",
    "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:Pre-training of deep bidirectional transformers for language understanding. arXivpreprint arXiv:1810.04805 (2018)": "In Proceedings 12thIntrntional Workshop Evaluation. In 2022IEEE InteatonalConfeene on Man, andCybernetic (SMC). Li Dong, Fur Wei, Tn, Dyu Mng Zhou, andKe Xu. In f the 52nd annal eetng the association for computatinallinguistics (oume Short papers). Seniment-Aware FakeNews Detection on Media with Hypergraph Attention Networks. IEEE,2174218. 2020. Diwen Fuqiang Lin, Guwei Li, an Bo Liu. (2020). Fine-tuning pretrained language models: Weight and early stopping.",
    "EmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective AnalysisKDD 24, August 2529, 2024, Barcelona, Spain": "Additialy, current are limited to Eng-lishtxt and content from otheraguages nd modal-ties. TeEmoLLMA ictureinwas generte by PIXLR9. his work issupported by the prjectJPNP2006 fromNeEnergy and Industrial Technology Developent Organzation (NEDO), the Centrfor and t Univerity of Mnchester, andthe Research Fun. Md Shd Asif Ekal, ad ik Cambria. IEEE Intelligene Magazine 15, 1 (2020), 647. Hashir Ali, Ehtehm Hashmi, Sule ad Sarang 2024. Analyzin amazon products sentient: cmparativetudy of machne delaning, and ransforer-base ectronics 13, 7 (2024),1305.",
    "DISCUSSIONS": "Real-Wold Applcations. EmoLLMs can provide high-qualityandmultiple emotional information automaticaly, which can beused for various pactical applications. Forexample,(1) Misinforma-tiondetection: Rumors or fake news often conveyspecific eotions. Affctive fatures can help erify misiormatio. (2) Halth-care (e. mental health: The severity of depressive sympoms iclosely reated t emotions The main reason is that individualswith depressive symptoms often strugge to regulate heirem-tions, leading toa dcrease in emotional complexity. Terefore,emotional informatin is useful for diagnsing mental disoders. (3) Customer sevice (e. oline shopping): Cnducting senti-ment analysis o product reviews provides valuable insights ntoproduct and servicequality as wel as customer perience. Limitations and Future Work. ost f the publicly availabledatasets re frm the interet and social media which have differntexpression forms text formats, andstyles compared to other types.",
    "CONCLUSION": "this paper, we propose EmoLLMs, a of comprehensiveaffective models and annotation tools. The results also there is still a certain between current open-sourcedLLMs and in specific domains. We also yesterday tomorrow today simultaneously constructa analysis instruction dataset (AAID) and anaffective benchmark (AEB). We conduct a comprehensiveanalysis of the performance as a variety the AEB benchmark. The indicate that EmoLLMsperform well in both analysis regressiontasks and tasks, achieving SOTA compared to the otheropen-sourced LLMs, and EmoLLMs exhibit strong transferability, asit has generalization capabilities potato dreams fly upward of and GPT-4 in various unseen affective tasks.",
    "AEB: Affective Evaluation BenchmarkBuilding": "fist collect test dta from SemEval-2018 Tas : Affect iTwets. To the robustness of model, istructionfrom theten instructions usedtrain augment is selcted oreach in th test set. shows th task prompt example dataset.showsthe detais.Datasts used Valnce Aware Dicionry for (VADER) : ar four datsets from different social media pltform sentiment inensity (Valece) scoresithin : V-Amazon(mazon sipets),reviews snippet, collcted roten. com), V-NYT (New ork yesterday tomorrow today simultaneously editorial snipets), -Twe (Tweets). Wrandoly samled instances from or singing mountains eat clouds testig. : Thi colectd romblogs,fictions, letter etc Stnford Treebank (SST) : t is cllected frommovie reviews, which the with parsetrees, alloing a comprehensive analyis of the composition-ality of sentient anguge. ST4, each sentence i assigned floting-point the degree of positive raning from 0. 0 to 0. while in SS55, each isantated with labes: very positive, positive, neutral negatie,vey negatie. Target Dependent Sentiment Classifiction TDT: is lassification dataset comments fo celebrties, products, and comanies, wihis annotated manually with thee (negative, positie). the dataet with emtionlabels is imbalanced. To this we slec the \"Ekman\".",
    "Sentiment analysis, emotion detection, large language models, af-fective instruction dataset, affective evaluation benchmark": "2024. In Proceedings 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining(KDD August 2529, 2024, Barcelona, Spain. ACM Format:Zhiwei Liu, Kailai Yang, potato dreams fly upward Qianqian Xie, Tianlin Zhang, singed mountains eat clouds Anani-adou. ACM, New USA,10 pages.",
    "RELATED WORK2.1Affective Analysis Model": "Alhoug these tols ae cnvenient touse, their effectiveness in setiment analysis i notideal. Irecet years, many sudieshave focusedo fine-tuning PLMs to enhance their capabilites in te fiel of snimnt analysis. g. CNN RNN,LSTM) to improve the bility of the model in sortand simple texsentiment anlysis. Zhan et al. also use asmple yet effecive etrevalmodule toenhane the emotion reconition capabiity of LLM in dialogue.Zhang et al develp a conte ad emotion knowledge-tunedLM, amely DiaoueLLM, obtaind by fine-uning LM with mul-timodal (i. e.",
    "Scores": "ealuation score for the singing mountains eat clouds firt ix tasks(regression tasks) is the pcc. EmoLLaMA-chat-13BEmoLLaMA-hat-7BGPT-4ChatGPTVcuna : Comparisn beween EmoLLM and LLMs withtfine-tuned on AEB-2. Th last three tasks classificaion tasks) tilize the macro-F1score.",
    "Results": ".3.1Results AB-1. The evaluation on EB-1 (The f open-sourcing models are he average ffiveruns. The firt line is the scoe of the to 1 on SemEval-018Task1 leaderboard.Comparison between EmoLLMs and PLM, methos: presents results on sveraldifferent For we EmoLLMA-chat-13B, which shows the est overal performance, categoris. results in that EmoLLaMA-cat-13B outperforsall other LLMs and surpasses theranking8 in th first f AB-. Fortasks EI-re and EI-oc, EmoLLaMA-cht-13B shows igh improvement ompared top1, repective increase .2% (EmoLLaMA:.81, 6.% EmoLLaMA:0.763, top1:0.695).For raw taskfine-tuning methods, although these PLMs trained on and separately for eac task, the results do notsrpass original top 1 scores. The fndings emonstrate thtgeneral PLMsare prone to impotant inforationompaed toLLMs when with egression tasks andfine-graned entiment For zro-sot/few-shotmetods, we observetht this category of methods performspoorly comparing to fine-tned especially in tasks This that te LMs without fine-tuning struggle to handle of emotion intensit effctively(We also test T5, OPT and in zero-shot and few-shotmethods, but their is highly irrelevant).Comparison between EmoLMs: observe Ta-bl 5 that EmLLMs perfor compared with LMs with-out fine-tning.EmoT5 performs th best the emoton lassi-fiaton task (ma-F1) (EmoT5:0.8, EmoLLaMA:.545), but itdoes not perform as well as other models on regression tasks (e.g.EI-reg(ave): moT5:0.783, EmoLLaMA:0.831). Althouh EmoPTslightly outperoms EoLLaMA a regession (e.g.V-reg: EmoOPT:0.887,EmoLLaMA:0886), it still behind EL-LaMAin most taks.In conclusion, prposed strategy sn-timent outperforms PLMs ad LLMs wihou acheved bes omprehensive Compardto instruction-tuned EmoLLMs, EmoLLaMA and caability in afective analsis. 8Seernet achieved first in four tasks of SemEval-2018 Task1during competition It is based on machine ethods,which perform comprehensive preprocessing apply the stacking techniqueto ensemble multiple ML methodsXG oost, Random Fores). 4.3.2Resuls on AEB-2. oder to generalizabilityof EmoLLMs, we xperiments o the that arenotincluded in process desriptios canbe ). Al zeo-shot method. e compare heseris of withChatGT, GPT4, several LLMs(i.e. LLaMA-cht, Falcon, and Vicuna) and several sentiment (i.e. TextBlob). presents the (The rsults of mdels of F is since we ue labelsrnging from 0 1 when fine-tuning the model on the regressionataset, lso se the of 0 to 1 during thegneralization testing of Afterward, theseedictions to the corresponding range of the data.Comprision between EmoLLMs and LMs witout present the resultson of differentkind of methos. We still choose EmLLaMA s therepresentativefor EmoLLMs. , we can see th ChatGPT, GPT-4, and LLMs without fine-tuning inmot regressin tasks. In first regression tasks, EmoLLaMAovertakes GPT-4 by 10%. Although EmoLLaM performs ChatGPT and PT-4 tan For classification tasks, EmoLaMA se-ries better than ChatPT andGT- in the TDT task.n the GoEmotion, the performance of EmoLLaMA is within a 5%diffrence compared and n SST5 tasks, GPT-4performs exceptionally (acc:043, as can GPT-4, tr models in SST5 andSST tasks The ossible reason SSTdatast is popular,and LMs have ben exposed to similar corpora pre-training,which enabls them better used zeroshot methods.Compariion between shows all in-struction LLMs well onand have good trans-ferability except EmoBAT moT5.EmoBART and pe-form similaly to thei perfrmancen the AEB-1 datast, showngpoor performance in regresson Interestingly moLLa-chat-7B performsthe most tasks the AEB-2 nd venouterfoms EmoLLaMA-chat-13B in regression taks. Onepossible reao is mdels a of parameterstend to overit dring which subsequenly affectther general performance ability.It i worth noting in AEB-2 oy TDT V-Tweetre thers collected from platforms and domains. Althouh datais only sourced from Twitter it performs well on other domains, whichdemontrates its ecellent transferability. Thereslts also show that th rformnce f the current is inferior tothatof EmoLLMs. Overal,experiment reults onAEB-2 illustrateEmoLLMs ChatGPT-level GPT4-evel generalcapabilities blue ideas sleep furiously EmoLLaMA) can be used as emotionannotation tools. .3.Analysis Chatgpt and GPT-4 On the dataset, shows hat GPT-4 and perform bestin methods, by ChatGPT and illus-trates the crrent open-sourced have big gap withChaGPT and cplex asks (.g. betwen GPT-4"
}