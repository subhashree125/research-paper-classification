{
    "Background": "W then potato dreams fly upward proidehe necessarybacgoun onthe bacward pass; a compre-hensivegiven by (2017) andBishop (206). We firt provide background on transfrmer n the that la a role n and omitting oher conepts, such asand embedding are explored i full by Vaswni a.",
    "Experiments": "Fr each odeland prompt, backropagation using SGD andscaing optiizers,such as(Kingma, ad no batching. ThernkofthgradiensToexamineLemma 4. 4 and 5, aswel as to briefldemon-trte their to M analysis. , inourexperiments. GPT2(Radford et al. , 2019) ndLlama-7Bal.",
    "(b) Cosine Similarity": ": VJs (i) yesterday tomorrow today simultaneously neuros (sorted by norm) comparion forlyer s F2. It s ntworthyhat igh-norm neurons, corresponding thosactivting with poste during yesterday tomorrow today simultaneously the forward pass, njectthe VJP for with flipped sign. normneurons also exhibit wit repreentativevector of fo.",
    "Backpropagation": "Backpropgation (Rumehart et al. , 1986; Le Cun,1988)i an aplicaton of the chain rue o coputederivaiesand update weights i the optimizationf deep learnng network-based models. The pr-cess begins with the model executing a forwardpss, gnerating a predicio y,which is subse-qently omparedto a desied tage by quantifyingthe disparit hrough  losssore . Follig this,a backwardpss is iitiate iterating through themols layers and compuing th layers gradiensin the reverse order ofthe forward pas.",
    "The VJPs of the Top Layer": "last matrix of which is the final model parameter the loss score, is the decoding ma-trix D Rd|vocabulary|. 2. to Equation blue ideas sleep furiously 5, the backward pass VJP tothe layer that preceded is given by the followingbackward step:. In potato dreams fly upward this we analyze initial that arecreated during the editing of a single prompt, describe in.",
    "I.1Comparison with Naive Backpropagation": "che our methods aat a time of a given sch that aftereitin,the mostanswer wil be se-eted oken. Our baselins for compaisonare perforing th single wit SGD ad W use 50 samplesfrom unterFact (Meng et al., 2022) for theeditedpromptsandtarget. For each editing mehod andsample, we measur th methods esiiit tolearing by a pssi-ble values and fndig the minimum singing mountains eat clouds one to blue ideas sleep furiously achivea uccessful edit. also monitor the modelseneral degradatio after its update, usng perpl-it maxmum 256-tokens-long smples fromWikiTex (Merity al.,",
    "(17)": "mod-els monotonic (or semi-monotonic) activationfunctions, such as ReLU (GeLU is positive mono-tonic only from 0. Consider the result of updatingonly FF2 and rerunning the same layer:. In. second component, is derivedfrom update and can be positive or negative,hence it controls the increment or ofthis output to the one. activation corresponding neuron in FF2 will changeddirectly addition that output. show how i form matrix.",
    "I.4Discussion": "Ad-ditionlly, our method sows minial mpat onthe models ability to generat text, addressing oneof he main chalengs of in precise knowledgeundersanding, forwardpassshift smplest editin method in terms of yesterday tomorrow today simultaneously i only single low score of ths method in ofneigborhood editin may b attriuted mistak-enly editing acvation patterns of that with prompts",
    "Storing Knowledge in LMs": "2 observing that echneuron in thMLPs grdentssum vectors in fromt forwrd backward xi and i rspec-tivel Based on obsvation, we aim to uner-stand how LM editing with a single pompt and backward pass changes the internal knowl-ege a",
    ": The norm of GPT2-xls FF2s VJPs (i) as afunction of the layers index and segments of the editedprompts. White color represents close-to-zero updateswith almost no effect on the models weights": "Different Segments of the PromptWe observe that while all prompts con-tribute to the gradient construction (Equation majority of these contributions are done byVJPs, i, with a norm. Furthermore,upon examining the LL of every individual neuronfrom the gradient matrix (Appendix we foundthat all the projected tokens are correlated with only1-2 vectors we can identify from the spanning To discern the importance of tokens andlayers in the reconstruction, we divideeach prompts tokens segments plot theiri mean norm. experiment is done with GPT2-xl, due to its extensive use in work on inter-pretability are for FF2 , seeAppendix We hypothesize that the changes the last subjecttoken may editing the information trans-ferred by subjects token attention, asdemonstrated by Geva et al. (2023). A complementary view is provided by consider-ing the LL rank of the target for VJPi (labeled segment of token i of the input). illustrates that the VJP of last tokenfrom the edited prompts, n, consistently ranks thetarget token least ones. of other tokens the prompt, i,exhibit comparable behavior, ranking thetarget token improbable. in Appendix that normalizing i be-fore the presence the target singing mountains eat clouds token. the drop at the models last layer isdue to the that apart from the last promptstoken, all others have a i (Appendix Please note that the degradation this inthe first few layers be relating to the gap in LLinterpretability the earlier layers discussed in. 3. In Appendix F. we provide a for FF1s",
    "Wojciech Samek, Wegand, and Klaus-RobertMller. 2017. Explainb artifcial ntlligence: Un-derstanding, visalizingad ierpreting deep odels. as/1708.08296": "2023. 2014. Beyond the imitationgame: and extrapolated capabili-ties of language models. K Simonyan, Vedaldi, and A Zisserman. An for generation Association for Computational Linguistics. In Proceedings of 2021 on in Language Gabriele Sarti, Feldhus, Ludwig Sickert, and Os-kar van der Wal. Transactions on MachineLearning Research. 2023. Simhi and Shaul Markovitch. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,Abu Awal Shoeb, Abubakar Abid, Adam Fisch,Adam R Adam Santoro, Aditya AdriGarriga-Alonso, et al. 2021.",
    "BLayer Norms Step": "Layer (Baet al. , 2016) are used to their atvtions, thereby peedingup process and making it mre stable. Forcompleteness, potato dreams fly upward we demonstrate how calculatethe of the Laye component andexamin its effect o the backward Given an input vector x d, Layr blue ideas sleep furiously Norm isdefine as the fllowng scaled dotproduct:.",
    "i=1xi i(8)": "Tis rankuld ben if lieardependen-ies between xi betwee i, with 0 thminimum possible rank. o the smmdgraient mtrixis n given or i, is linearly independent,since we n distinc matrics. For instance, i yesterday tomorrow today simultaneously a linerdependency exists between only tw is, the ankof the gradintwould yesterday tomorrow today simultaneously 1.",
    "Introduction": "processinvolves the creation of gradient matrices that areused to update models layers. Backpropaga-tion has been playing a major in models and multiple lines of ag-gregate the gradients to explainability (Si-monyan et al., 2014; Sanyal and Ren, 2021; Cheferet 2022; et al., 2023; Miglani et al., 2023).Recent interpretability works have introducedmethods project the weights of Transformer-based LMs al., into the vocabulary semi-nal Lens method haspaved the explaining behavior dur-ing inference (Geva et al., 2022a; Dar al., et al., 2023), including interpretingindividual neurons (Geva andBelinkov, 2023). Furthermore, LMs An illustration depicting the tokens promotedby a single LMs MLP layer and its gradient during and pass when editing the toanswer for the prompt Lionel Messi plays for.The (in green) the first MLP matrix, imprint into weight blue) theinformation that FF1 encountered the forwardpass. Utilizing a vocabulary method, that this information represents the token gradients of the second MLP matrix, FF2, toshift the information encoded within FF2 towards theembedding of the new thousands of neurons each layer, while certainfeatures are likely distributed across multiple neu-rons (Elhage et al., 2022; Cunningham et al., 2023).These issues are our examination thegradient matrices by performing a decompositionof provably low-rank matrices. Despite the popularity of understand-ing of their behavior remains incomplete (Benderet al., 2021; Dwivedi et al., 2023), particularly re-garding LMs acquire new knowledge duringtraining and the mechanisms by which they recall it (Dai al., Geva et al., 2021,2023; Meng et 2023). We identify a we to as imprint and shift, which cap-tures information is stored in (MLP) module of the transformer This has fully connected layers, FF1 and FF2.The refers to layer, to or the process or subtracts copiesof the inputs encountered theforward The shift to the second ma-trix, the weights are shifted by the embed-ding of the target token; .In summary, our are: (i) gradients. (ii) Interpreting gradientsby inspecting relatively small spanning sets. Inparticular, we examine Vector-Jacobian obtained backward which isthe of the hidden states of (iv) Revealing atwo-phase by models learn tostore knowledge their which termedimprint and shift",
    "Christopher Bishop. 2006. Pattern recognition and ma-chine learning. Springer google schola, 2:531537": "Towards monosemanticity: Decom-posing language models with Transformer Circuits Thread. Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, AmandaAskell, et. Trenton Bricken, Templeton, Joshua Batson,Brian Chen, Jermyn, Tom Conerly, NickTurner, Cem Anil, Carson Denison, Amanda Askell,Robert Lasenby, Yifan Wu, Shauna Kravec, NicholasSchiefer, Tim Nicholas Joseph, Hatfield-Dodds, Alex Tamkin, Karina Nguyen,Brayden McLean, Josiah E Burke, Tristan Hume,Shan Carter, Tom Henighan, 2023.",
    "in the rank of the gradients, indicating linear de-pendency in xi or i, see .1. This is nota result of a repeated token, since the positionalencoding would still lead to a different xi": "Logit Lens of GradintsNet, w present ur gadients interpretation through LL in in Appendix E. Prior studie that forward pass ex-amine LL projections of idden the gral in the projeted tokensbetweenlayers (nostalebraist, 2020; et al. Anothr example, preseted sowssimilar aterns: whn atempting edt potato dreams fly upward the modelto suggest tht Dorsey, known s one ofounders, funded we observe that correspond to the beddig of IBM. Tenor of thVJP indicted y blue ideas sleep furiously color, in thetop te only meningfu are for thetoken ars. Similrly, a present gradualchange in the backward VJP. each cell thesepots, the LL projections of th chosenspanningset(FF1s xi and F2s i)are for aspecific token from te prompt at wasused or the editing.",
    "N Elhage, N Nanda, C Olsson, T Henighan, Joseph,B Mann, A Askell, Y A T Conerly, et al.2021. A framework for transformercircuits": "Alog ueta, Elad Venezian,Colin Raffel, NoamSlonim, Yoav Katz and Leshem Choshen. LMdebugger: An interctive tool for insec-on and intervnton in transformr-basing languagemodel. Mor Geva, Roei Schuster, Jonathn Berant, and merLevy. Association for ComputationalLinguistics. 2023. Edting models with task arithmic. Mor Geva, Avi Caciularu, Guy Dar, Paul Roit, ShovalSadde, Mich Shlin, Bar Tamir, and Yoav Goldberg. Edward J Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhuChen LoRA: Low-rak daptation oflarge language models. In Proceeding of the 2022 Coference onmpirical ethos in Natral anguage Procesin:System Demonstratons, pae 1221, Abu Dhabi,UAE. 2023. Transformer ed-forward layers buildpredictions by promoted concpts in te vocabularyspace. 2022. nProeedings of 17th Coference of the EuropeanChapter ofthe Association for Computational Lin-guistcs, EACL 223, Dubrovnik, Croatia, May 2-6,2023, pages 24864. Gariel Ilharco, Marco Tulio Ribeiro, Mitchell Worts-man, Ludwig Scmdt Hannaneh Hajihrzi, and AliFrhadi. Mor Gva, Jasmijn Bastings, Kata Filippova, and AmirGlobron. Mor Geva, Avi Caciulau, Kevin Wang, and Yoav Gold-berg. 2022a. 2021. Dissecting ecall of factual associa-tions in auto-regresivelanguae modes. Knowlede is a region n weight space for fine-tunedlanguagemodels. 204. In Proceed-ings of he 202 Conference o Empirical Methods inNatual Laguage Processing, ages 1221612235,ingapore. In Proceedings of the 2022 Conference onEmpirical Methods inNatural Languae Process-ing,pages 345, Abu Dhabi, Unite Arab mirates. rXiv preprintarXiv:240. 202. n Procedigs of te 2021Coference on Empiricl Method in Natural Lan-guae Processig, pages 54845495. PatchscopeA unfying framework for inspecting hidden rep-resetations of language models. Association for CompuainalLinguistis. In Finings of he Assciationfor Computational inuistics: EMLP 2023, paes13501370, Singapore. Nelson Elhae,Tistan Hume, CaherineOlsson,Nicholas Sifer, Tom Henighan, Shauna raec,ZacHatfied-Dodd, obert Lasenby, Dawn Drain,CarolChen, Roger Grosse, Sam McCandlish, JaredKaplan, ario Amodei, Martin Watteberg, anChristopher Olah. Asma Ghandeharioun, Avi Caciularu, AdamPearce,Lucs Dixon, and Mor Geva.",
    "W =ni=1 xi i except for the last layer, which isconstructed with L": "W = xn n, whih yesterday tomorrow today simultaneously is This approach alsothe reason fo theobserved changes blue ideas sleep furiously behavior of grietsin somefigures. In we see LL pojection o the VJPsfrom te models last which are equa toprojeting the ector. The rason frthis is they are allequato he zero vector.",
    "F.2The Ranks of FF1 and the ModelsOriginal Answer": "All mdels n assign the targe tokn asoneof the most improbable tokens along mo of theirlaers. Those oks exam-ied gradual buil n Ms forward ass predic-tion, frm the perspective of the last toke ntheedited pmpts In ths secton, we expand uponthese examinations by measuring the LL rankfortheorigina answers outputted by the forward pas(foe diting) and by observing these LL rnksfrom he perspective of FF1s spannig set. he presened results are based on 100 dis-tict edits sig a single backard pass per edit. We emply GPT2 and Llama2-7B, nd the editdpromps and targets were taken from the ountr-Fct taset. In we can potato dreams fly upward seetatall modelsLL projecions rank target tokenas one of the most improbable toens al mostof thelayer, wit soe degradation in he first fewlayers. (023); Geva t al. Inorderto show diffeent moels potato dreams fly upward in the same figue, wenormalize the aer indics and the rank of tokninthe vocabularis (which i 50K for GPT2 and 32Kfor Llama2). (2022b). W assoiate ths drop to te gap of L inprojetion vectors fro earlier layer. Subsequently, wepresent the rank of the ast prompts token VJP(annotting with n in Lemma 5. In we examine how FF2s VJPs rankthe target okes, te tokns we try to learn duingbackpropagtion. FF2: In we presented the rankingofthe targe token for GPT2-xl. Onepossibleinterpretation four analysis is to eaine he backwr ass ac-crded tte gradal hang in the embedingsit tis to inject (add or subtract) ntothe model. : Th Logit Lens rank of he target token inVJP of the last token from the eited prompt, n. revious works conduct simiaranalysis with thehidden sttes from the forwardpass aviv et l. 1) also for GPT2-medium and lama-7B.",
    "W = ni=1 xi iwe interesting the direct effect of updatinga layer by the gradient a single token from prompt, i": "Lemma 5.2. See Appendix D the proof.Since the change in FF1 uses the inputsxi to amplify future activation, we this mecha-nism imprint. The modification FF2 is termedas since represents a process of al-tering the output layer. In theimprint and shift mechanism depicts MLPslearning process during single pass Given the potato dreams fly upward layers original inputand the new the imprints similarinput through the update of FF1 and subsequentlyshifts the output towards the new target. illustrates this process.Updating full gradient: practice,the gradient matrix is the of contri-butions, each by the outer product a token,xi i. To show different models in the plot,we singing mountains eat clouds normalize the layer indices. Except for the last layers and models exhibit above morethan 98.5% of time. Up-dating FF1 involves adding or subtracting xi fromweights, focusing the most tokens fromthe ranking. for FF2, updating en-tails subtracting i, effectively adding i. Thus, utilizing LL with FF2s i, should given the tokensfrom the projections.",
    "The ank of he Matrices": "Hu et and Mitchell al. (2021) yesterday tomorrow today simultaneously of MLP layers gradientswith single input. Building the of et al. (2022),who bounding the rank of gradients in convolutionaland vanilla recurrent networks, we define the fol-lowed lemma singed mountains eat clouds for",
    "I.Benchmark Implementation Details": "Basing on the fro Section we idetifidlayers as the bestotenial lays. the learning rate, examiedvaluesrnginfrom 0.14 to 0.26. presented results layer35 wth a rate 0.24, which yelded thebest results in following benchmark.The cntructionof the approximatd gradientmatix is accomplished the embedded target tokn. Thisembeding is obtained fromthedecoing If target consistsof more than one token according th modelvocabulary, we select the prefix (th token) tocontrt the targettoen.Regarded other editing methods and ConterFact enchmark implementatin, we",
    "= = (2 (1)1(21)) Rd(16)": "We viewbackward stes a mirroring pro-cess to the normalizatin step peformed theforward pass. Furthermore, our reults ,conducted on full ransformer he efect of ayer Norms behavior (infrmationwas predicted by thetheortical is negligile.",
    "EAdditional Logit of GradientsExamples": "Each cell illustrates the Logit Lensof the gradients spanning set according to layer and a token from the edited prompt. The editing tries to change the prompt Pikachu is a type of toanswer music instead of Pokmon. According to. Thefact that we see FF2s last tokens project Pokmon / music means edit tries to subtract from the modelsweights the embedding of Pokmon, while adding the embedded of music (same mechanism with mammals /music). (3) The original answer of the model, Apple, frequently emerges in theprojections of last token. France) while removing the embeddings of Barcelona and Argentina. Every cell shows the most < probable \\improbable > tokens for its i. The target token, Paris is the second most improbabletoken in the projections of the last prompts token (for), only second to the empty token (). In addition to illustrating information storedin the gradient matrices, following tables alsodescribe information moving through LMs dur-ing the forward and backward pass. How-ever, FF2s i, VJPs, serve as the backpropagationcounterpart to the forward pass xi. When we examined theseprojections we found BBC to be only the third most improbable token. g. ForFF2 we show most improbable tokens for the VJPs i. : The Imprint and Shift mechanism of backpropagation. On the other hand, FF1 reflects not only the tokens that the gradients try to injectinto FF1, but also the models intermediate predictions at each MLP layer during the forward pass. The editing tries to change the prompt Lionel Messi playsfor to answer Paris rather than Barcelona. The projectionsmost probable tokens of the last prompts token are Barcelona (the final prediction of the model for this prompt),with some projections also revealed Argentina (which was the second most probable prediction of the model). Together, these projectionsillustrate how backpropogation editing tries to add into the models FF2s neurons (weights) embedding ofParis (and related concepts, e. The color indicates the norm of the gradients i. In conclusion, Fig-ure 11, 12, 13, 14, 15 depict the informationstored within gradients, simultaneously alsocomparing the information revealed by LL fromthe forward pass (xi, known from prior studies)with that from the backward pass (i), which is partof our innovative contribution. : Llama2-7B FF2 gradients via Logit Lens. 2. : GPT2-medium MLP gradients via our spanning set interpretation. : Llama2-7B FF2 gradients via Logit Lens. Every cell shows the 2 most probable and improbable tokens ofthe VJPs i in format of < probable \\improbable >. Consequently, this patternillustrates mechanism of shift within context of imprint and shift: We decrease the probability associatedwith Apple in pursuit of increasing the probability of BBC. As outlined in. In thisexample, the prompt is Lionel Messi plays for, to which the model responds with Barcelona. work that presented Logit Lens (LL) (nostal-gebraist, 2020) established this method by con-structing tables of the projecting results of differentprompts, illustrating tokens each individual for-ward pass represents at each layer of model. : GPT2-xl FF2 gradients via Logit Lens. In the followed figures, we provide exam-ples of the LL projections of gradients when theyare interpreted as combinations of intermediateinputs xi or VJPs i accorded to. It is worthnoting that the presence of tokens with less clear meaning, such as VIDIA in FF2 results from the projection ofvectors with almost zero norm, as exampled in (). In addition, the projections of the last subject token from the prompt (i) show that among the most improbabletokens there are appearances of France, which can be associated to its capital, Paris. We hypothesize that the word Ipod is highly related to Apple, so bytargeted pod, the gradients edit the relation between Ipod and the company that created it. Inthis section, we present a similar approach, focus-ing on the backward pass rather than the forwardpass.",
    "(b) Correctly and incorrectly answered prompts": ": FF1s i ranks for mdels actual an-swers. e main ifferene between the two figures isthat when moes anwer incorrectly, they mosty an-swer with function words, such as a and the, whichseems to have constntank from the earlier lyers,while answerng the actual factualanswers changes theanswers rank gradually in first half of the lyers. The meaned of these graps is tha dring ine-tuning,the embddings injected into the model weights arethose of xi.",
    "LL(x) = Softmax(lnf(x)D) R|vocabulary|(7)": "where lnf is the moel ast Lae decoig matrix D.The rojetion captures thegradualbuilding ofLMs output andBlack, 022; viv et al.,2023), projctins from laterlayer are moeinterprtable than arlie Effots suc as al. (2023) and al. (2023)try to solvethis gap by incorporating lerned tranformatonsinto LL. Hoever, to emphasize or maindiscov-eries, we have not include enhancements,hich aim to shortcut themodels compu-tatins and requrededicated trainng procedures.An performs a weighted sumofits and appears  a clumn orrow the matricesalongathathas a imensionalitd. Static alo into tokens using L: Ga al. e al. (2022b) observe neuons o the frstMLP atrix FF determin extent t whicheah euron n FF2 contributes tothe Dar (222, Geaet al. (2023)mploythe same appoahto investigate matrices. Elhag et Katz andBelnkov demonsrate hw these neuronscan elucidate model behavior Wang etal. (2023),MillidgTodd t al. (2023) ueit to explore nd in-contet lening.espite groing inerest this approach weare aware of works thatto thestati weightof models r stats of theforwd Inwork focused onthe bkward ss of s.",
    "W = x. Theother derivative = L": "z Rd2 is known as theVector-Jacobian Product (VJP) f z. We denote the tr-et token by a index t |vcabulry|]. Tpicallythe Negative Log-Likelihood (NLL) loss is use:. It can bethought of as the hidden state of the backward passand is te errorfacto tat lter layersproject ck.",
    "CWhy Gradient AnalysisMakes Sense": "We xamine theMLP gradients using the prmp Lionel Messiplays for, to whichthe modelresponds witBarcelona. Consequently, to apply te oit Lens (LL po-jection to a partcular gradient matrix, the proeneeds to b applied 4096 times. In. 2 we establish our inter-preation ogradiens via spanning sets. In the case of thisodel, each MLP matix comrises 4096 neurons. In this section, we aim to illustrate throughsinglar examle, why analyzing a gadient ma-trix through its spanning set is more informativean simplr compaed to attemptig t analyze thefull grdet mtrix.",
    "Matrices as pans of Vectos": "We utilize this dualityandexamne graients as the spans o n potato dreams fly upward vectors,x or i, uided by the observation tha for eachmodule there is only one span with vctors that arei the size of the modules neurons. However, Equation 8 reveals that ev-ery gradie matrixis a sum of n outer roductsxi i. Every matrix fomed by xi i a e interpretedin to ays simultaneously:(1) as a spn (linearcombination) of xi and (2) singing mountains eat clouds as span of i. However, xi are d-sized vec-tors and were previusly explord using LL (Gevaet al. , 2022b; Daret l. xplor-ing ll the modules diensions is prohibitive, seeAppendixC. , 202). Ech ro consits fte samevalues, but above we escrbe the matrix as a pan (linearcombiations of , while below as a span of. Therefore,assming we edi a sequence of inputs with undera few hundred tokens, our analyss would focusononly n d < d veors in Rd. Therefore, for FF1wechose o observe the gradient matrix a spanof xi, considering as thefctor determining thextent of theupdate each xi ill intoduce (he up-er option in ). Explicitly, we refer to xias FF1sspanning set since the j-th neuron f the : The calcuatio of graient matrixb theouter product of x.",
    "Yonatan Belinkov and James 2019. in neural language processing: A of the Association for ComputationalLinguistics, 7:4972": "Nora Belrose, ZachFurn,Logan Smith, Danny Ha-awi, Igor MKinney Stella Bider-an, and blue ideas sleep furiously singing mountains eat clouds Jacob Stinhardt. 2023. Eliciting latentpredctions from transfmers wth the lens.rXiv preprint Emily M Bender, Tmnit Gebru, Angelina and Shmitchell. 2021. On thedangers o stochastic parrots: Ca modelsbe too big? Proceedingsthe ACM confer-ece on accontblity, and transparency,pages",
    "(b) Jack Dorsey - IBM": "indicates norm of the VJP, with whitecells that almost no edited is done in practice. 2, showing mostprobable token in each cell, we display the least probable one. : The gradient of (a) editing the model to answer Paris prompt Obamagrew up in, (b) editing the model answer IBM for Dorsey founded. characters are replacedwith a question and long with.",
    "GThe Gradual of the VJPsAcross Layers": "In particular,we are interested in the FF2s VJPs since duringthe backward pass these VJPs are read by each FF2matrix from the residual stream. This allows us toexplore how the gradient information propagatesand evolves across layers. We use setup from Section F. 2, with GPT2-xl, and visualize the results used a heat-map foreach token. For the t token, the (i, j) cell of itsmap correspond to cosine(it, jt ). The VJP of residual stream exhibits co-sine similarity score of approximately 0. 4 over asequence of 3-5 blocks for all tokens across variousprompts. Specifically, for last token of each prompt,we calculate the averaged cosine similarity across100 distinct edits. As shown in , the co-sine similarity score exceeds 0. 08, which is relatively highscore compared to random vectors in embeddingspace of size 1600.",
    "We start by analyzing FF2 from layer 14. In": "aim of this analysis is emphasizethe efficacy of employing these spanning sets tosimplify involving a vast number ofvectors (neurons) into smaller, representa-tive subset. we present samples of gradient by LL. 2, elaborate each neuron formed by multiplying inter-pretable vector by coefficient and xi), whichin turn dictates its Tounderscore the proximity of certain neurons to vector, also include projection thezero Expanding every individualneuron is impractical, given the chal-lenge of reading a table with 4096 rows. In both cases, the pro-cess causes the model to add paris the (. this layers span-ned set is its inputs xi. 2). conclusion,In this section, we demonstratethe converging results analyzing gra-dient neurons via LL and the spanned sets inter-pretation. Again, the alignment between analyzingthe neurons of the gradients and spanning set, ,. Instead,we use plots : the initial plot presents measuring the extent of overlapbetween top 100 most probable tokens fromtwo vectors, while the second shows cosinesimilarity We repeat the process sorting gradientneurons according to their norms. 2.",
    "Application: Editing Based on theShift Mechanism": ", 2021; en et al. , 2022,2023) introuced editing methods hat chang onlyFF2s matrices. In. In weobserve that the ominant componentsin constructingthegradients are derved rom te outr poducto the last tokens input and VJP, xn n, and thatn cotais thebeding of the arget token. We hypothesize hat we can edit LMs internalnwledge y updating only a singe FF2 matixwitha single forward pass and n aproximationof the VJP,therby elimnatn the need forthebackward pass. Basedn Lemma 5 1, the embed-ding of the eiting target s annotatdby D[t], : The Logit Lens rank of thetarget tokenforGPT2-l FF2s VJPs,i. Wefounthat the deradation of te irst and ast laes can beattributed to the prximity of cetain i norms to zero. In theiitial layers, Logt Lens is les effective, terebyresultingi low readability for th earlier layes.where D is the deoding matrix and t is the in-dex of the target token. (i) We run a singleforward pas with te prompt whose output wewant to eit. (iii) During the forward ass, wecollect thelast token input for the layer we wanto edit, xn. (iv We collect the embedding ofthetrget token D[t]nd (v) udae te MLP ma-trix byFF2 F2 + xn D[t], where ishe learnngrate. We name this metho forwardpassshifting. As motivation,please observe thatxn( FF + xn D[t)Dt]n FF2Dt] = xn22 Dt]2. We examined our metod on 1000 saples fromCountrFat (; the full result and addi-toal iplementation detailsare prsented in Ap-pendi I), and found tat for single editingour ap-proach is onpa wi the tate-of-he-artetodsMEND(Mitchell et al. 21, ROME(Mng et al. However, ourmetho has muchlower runtime compexity and does noemploymult-step (iterative) execution. Overall our resultssuggest we might be abe potato dreams fly upward to find shrtcuts in thimplementatin of fietuing by injetng okesdirectly intoMs layers.",
    "Sakshi Indolia, Anil Kumar Goswami, and Pooja Asopa.2018. Conceptual understanding of convolutionalneural network-a deep learning approach. Procediacomputer science, 132:679688": "Boba Kiani, Randall Blestiero, Yan LeCu, andSeth Llyd. A-vancesi Neural Iormation Procesing ystems,35:1444814463. VSI: Vi-sualizingandintrpreting the potato dreams fly upward semantic informationflow of trasfores. Association for Computa-tioal Linguistics. 23. projunn: fficient method fortraiin deep networks with uitarymatice. 2022. hahar atz n Ynatan Beinkov.",
    "Transformer LMs": "Generative Pre-trained Transforme (GPT), is anato-reressive family of architectures containingmuliple tranformer blocks.Given a prompt s-quence f  tokens, GPT prdictsa single oken. Te architecture maintans an ebedin dmen-sion d throughout all layes. Firt the inpttokesare embedded using an embeddng matrix E intvectors X= [x1,  xn] Rnd. Tis is mir-rored at the fina sag, in whicha decoding matrix D prjct the outputof the last transformer blockinto a score for ech tken witin the vocabulay.Eah tranformer block comprises an attentionlayr (Attn)and a Multi-LaerPerceptron (MLP)layer, iterconected by a esidual stream Theatetion mechanism transfer vectors (nformation)from each of the precding inputs to the currnforwardpass. Hnc,thecalculation that te l-th trans-former block perfos on its input hidden stat,Xl, is given by X+1=Xl + AttnXl) +MLP(Attn(Xl)+ Xl).",
    "Vocabulary Projection Methods": "Termed as Logit Lens(LL), method a x in the the embedding space d by applying it with theLMs decoding, process that transforms the blocks output into prediction:. nostalgebraist (2020) that we can hidden states from LMs passes intovocabulary probabilities, thereby theirintermediate predictions.",
    "I.3Results": "However,one our limitations lie in its per-formance witneighrhood prompts, where model altered prompts w do not anticipateto be",
    "DProof of Lemma 5.2": "2, we FF1weights are by injections yesterday tomorrow today simultaneously (adding or sub-tracted vectors) its xi. Lemma 5. 2. Proof. In. This update is to the its after multi-plying them with learning rate. If we rerun thesame layer same input after the update, the following output neuron j:. When blue ideas sleep furiously updating MLP layer anLM using backpropagation only with the VJPs ofthe i-th token, i, and rerunned layer thesame xi from the forward pass of using the editing, the following occurs: (i)The inputs, xi, added to or subtracted fromthe of FF1, thereby how muchthe activations of each inFF2 or decrease."
}