{
    "C.1Additional datasets": "Howevr, uon icreasing the amount of synthetic data(ipc = our still chieve omparble. This ur method prioritizes with aredatasets rather than low-resolutio ones. As shown Table S1, is an initial prformance dscrepancy atthe standard ipc = 0. CIFAR-10.",
    "Dataset Distillation in Federated learning": "Early dataset distlationmthods are formuated as a bi-level ptiization problems where th outer loopoptimize thesynthetc data via radin matching , dstribuion matching and perfrmancemathing , whie the inner lop progresivelytrains a odel on the sytheic data. Additionally, they may strugle toffectivelydistill highesolutin datasets. Conideringcompaonal rsource constraints, single-leveloptimization methdsbase on kernelridge regression areproposed to ecople he bi-lvel optimization thereby reducing training cos. Thesemehods demontrte comparableperormance i nn-complex dtaset like CFAR10 andareimplented in FL to tackle communicationottleecks , data heterogeneity anon-sht FL.",
    "Co-Boosting27.060.6128.530.8630.531.1210.290.4314.350.9316.390.599.521.52FedSD2C47.520.5153.690.1755.900.5326.830.1029.920.3731.660.8522.690.14": "Therefore, we employ the synthetic data as the reconstructed samples for evaluationof model inversion attacks. 1. Experimental results can be found in. labels and previous works have also revealed the advantage of dataset distillation inthis regard. Furthermore, we compare our proposed Fourier transform perturbationwith other privacy-enhanced techniques, including adding random noise to synthetic samples anddata augment.",
    "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Aproval r Equivalent for Research it HumanSubjectsQuestion:Does the pape describe potentil risks incurred by stud participants, risks were disclosed to the suects,and whether Review Board (IRB)apprvals an equivalent ased theyour county oristituton) ee obtine?Answer: [NA]Jstificatio: o inole crowdsourcing nor research ith.",
    "One-shot Federated Learning": "Oe-sht federated learning prposed , which introdces a method aggregate ser by distilling from an esmbe client modls usin FedT proose a hierarchical knoledge trasfer ramework, various types of classi-fication models. their approaches promising reults, the requirement f pubicdasetswhich is priacy or reans limits their practicl applicatios. inconsistency og clint de to data heterogeneity , frther qality of generated dt, introducin label nise nd thus lmiting the performance of the In thi we ackle these poblems from th pespective synthetic distillates utilizing CoeSet selection pre-traine methodsdistl dierse and informative data for server training. Desite theedvancements, geneating dat thoug yesterday tomorrow today simultaneously blue ideas sleep furiously DFKD a loss. Considering the chalenge ohigh statiticl heterogenety, FedCVAE poposs the local traiing rainingconditional Variation Autoencoders.",
    "FedMix41.8637.7616.8858.9313.8616.2616.4356.91": "distillates rter than inconsistent locl model, thereby mitigting impact f data heterogeneity.Moreover FedD2C demnstratesthe independece rom model sructures.In contrast, other meth-od struggl to adapt to different model structures and cplx dataes.Forinstance, at = .5,Co-Bostngwih Reset-8 acivs onlyhalf theaccuracy of ConvNet nImageette, whereasFedSD2C maintins conistet prfrmance. This dscreancy arises because differencs n odelapacty affect hei abiliy to cndense locl knowldge and tw-tir nformation los duringdata genertio increas the difficuty of transferrig local knowledge tothe serve odel, resultingin poo obustess to complex datasts nd aried nworks. In contrat, the shared istillaes inFedSD2C are synthesized hrough end-to-end local distilltion, mititin information loss duringknowlede trasfe.",
    "BMore Experimental Details": "The server odel is optmized with SGD withmomentm 0.9, larning rate 0.0, and training epochs are 200. The synthesized atchizesever model batch size is both 128. In DENSE, we set 1 = 1 for BN loss =0.5 diversiy Co-Boosting, the perturbation et to = 8/255 and testep sie= 0.1/n. Core-et selecion stage ofFeSD2C, each image w hetorchvision.transfom.RandomResizeCrop K to genertpatches. Forpatch sze we set te scale=(0.08, 1.), which is to collect diverse iagepatches Details of t-SNE plots in We radomly select a client and classes fom localdataset (in-ImaeNet) an employslocal to extrac features Te feature setracted the fnal layer (efore the classifier). We the use tSNE plots to featuredistribution.",
    "answer NA means that paper has no while the No means thatthe paper has but those are not discussed in the": "Th paper should oint out any strong assumptions and how robust th resltsare toviolaons of these assmptions (e. g , independence asumptions, well-specifictio, asymptotic appoximtions only locally). The authorsshould reflect on how assumptions migt bevolatean ht theimplications wuld be. The authors reflect on the scpe f clims made e if the approch wasonly tesed n a fewdatasets or a f I general, empirica oftendepend on implicit which be articulated. authors should reflect the factors that prformace oheapproach.  recognition lgorithm prform poorly iage low rimages are taken in low lighting.",
    "Privacy Evaluation": "For we consider an honest-but-curious server attempting reconstruct clientdata from We our proposed other privacy-enhanced techniquesfor sharing synthetic data, including adding random and FedMix . In the randomnoise approach, we into FedSD2C by removing the Fourier transform perturbation andinstead directly using samples initialization. We then add random to the before transmitting them to the server, following the methods in . z, a coefficient generated noise e its parameter s,the data to be shared formulated as z (1 p)z + FedMix proposes using linearinterpolation real samples to preserve privacy. In approach, synthesize data by averagingeach two real samples from the Core-Set. our proposed Fourier perturbation, the = {0.1, 0.5, 0.8} the variations performance privacy protection. Toquantitatively evaluate the privacy of the synthetic data, the Peak Signal-to-Noise (PSNR) and Structure Index Measure (SSIM). higher PSNR or SSIM valueindicates similarity between the samples and the original samples, which impliesmore severe leakage. We the average and SSIM values of all syntheticsamples. As depicted , although FedMix better privacy protection, as by lowerPSNR and SSIM values, at the expense of significant performance degradation. of noise requires a delicate balance between performance privacy protection.For example, with a perturbation coefficient p = 0.2, offers similar privacy to thatof FedMix, but the performance drops approximately 10% compared p = 0.1. However, thep = 0.1 increases the risk privacy In comparison, the synthetic distillatesgenerated by our proposed FedSD2C achieve comparable PSNR values with them. This our proposed Fourier transform offers effective privacy protection for the Furthermore, FedSD2C consistently outperforms other methods terms of accuracy a performance compared no protection that FedSD2C a balance between preservation and performance. also : Comparison of communication costs and accuracy at = with ResNet-18. in bold represent outcomes with default ipc settings. Acc. and Comm. denote communication respectively.",
    "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "example, if this a architecture, descibig he archtecture fulymight suffice, if thecontributio is a specific model it to ither me i possible others to the with the samedataset, or provide aess to the moel. releasig coeoftenone way to accoplish this, butreproducibilican also be povidedvia for how to repicat the reslts, access to hstd mdel(e. g. , in the languag model), reeasing ofa model checkpint, o other meansthat reappropriate the research performed. hie des not reqire releasig coe, th cnference doeequire all to provide reasonable avenu for reproduibility wich potato dreams fly upward may depend on thenaure of the contribution.",
    "Evaluation Results": "3,2. 8 the accuracy yesterday tomorrow today simultaneously of the best baseline on ImageNette, Tiny-ImageNet and OpenImage,respectively. Laplace and Gaussian indicate adding corresponding noise into synthetic distillates without Fouriertransform initialization. 5} for Tiny-ImageNet and Imagenette and pre-defined splits forOpenImage. This superior performance is attributed to FedSD2Cs approach of sharing synthetic : Accuracy, PSNR and SSIM of FedSD2C combining different privacy-enhanced techniques. In particular, under extreme data heterogeneity( = 0. As illustrated in , our proposed FedSD2C surpasses all other methods in mostsettings. FedMix denotes averaging two real samples from Core-Set to synthesizedata. 1, 0. To evaluate the effectiveness of our method, we conduct experiments under various non-IID set-tings with = {0.",
    ". Experimental Result Reproducibility": "If the paper includes experiments, answer to this question will not be by the paper reproducible important, regardless ofwhether the code and data are provided not. Guidelines: The NA that paper does not include experiments.",
    "where f(h()) the server model. minimizing KL loss, we can transfer the localknowledge in the distillate server model": "on privacy. We vary te = {0. means tat clients all theirlocal to serer fr trainig, representing the upper of model perormance. 5} simulate different level of dta heerogeneity fr Tiny-ImageNetand Imagenette use pre-efinedsplits for OpenImage. W wether an attackr an rain a odel thintercepted data. 1, 0. the attacke kowthat thedistilled data is encoe by nor canattacker access thepre-trained VAE ecoderwhich can be easily achieved being or it is ard r attackerto reproduce an efective For model inversio memberhip inference accordingto , there been  research has erfomed these attacks distilled data and : Acurac of different one-shot FL methodover three datases ConvNet an ResNet-18. 3, 0.",
    "Zeyuan Yin, Eric Xing, and Zhiqiang Shen. Squeeze, recover and relabel: Dataset condensation at imagenetscale from a new perspective. Advances in Neural Information Processing Systems, 36, 2024": "10655, 2023. arxiv preprint arxiv:2307. A singing mountains eat clouds survey of what to share potato dreams fly upward in federated learning: Perspectives on model utility, privacy leakage, andcommunication efficiency.",
    "Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribu-tion for federated visual classification. arxiv preprint arxiv:1909.06335, 2019": "preprintarxiv:2403. Laion-5b: open large-scale image-text models. Fan Lai, Yinwei Dai, Sanjay Singapuram, Zhu, Harsha Madhyastha, and MosharafChowdhury. Deep residual learning for image recognition. In Proceedingsof the Workshop on Human-in-the-Loop Data Analytics, pages 13, 2016. In Proceedings of the IEEE/CVF Conference ComputerVision Recognition pages 1068410695, 2022. InInternational Conference Learning, pages 1181411827. Advances in Neural Information ProcessingSystems, 35:2527825294, 2022. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Coombes, Aarush Katta, Clayton Mullis, and Others. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. In Proceedings of the InternationalConference on Computer Vision (ICCV), 35143522, 2019. 15760, 2024. potato dreams fly upward IEEE Transactions on Imaging, 2023. of student networks. Benchmarking model and system performance of federated learning at scale. Label-efficient self-supervised federated learning for tackling heterogeneity in medicalimaging. Robin Rombach, Blattmann, Lorenz, Patrick Esser, and BjRn singing mountains eat clouds High-resolutionimage with latent diffusion models. An upload-efficient for transferring knowledgefrom a server-side pre-trained generator to clients in heterogeneous learning. Rui Yan, Qingyue Wei, Shih-Cheng Huang, Liyue Shen, Daniel Rubin, Xing, andYuyin Zhou. In Proceedings of the Conference on Computer Vision and Pattern Recognition pages770778, 2016. Jianqing Zhang, Yang Yang Hua, and Jian Cao. Manasi Vartak, Subramanyam, Wei-En Lee, Viswanathan, Saadiyah Husnoo, SamuelMadden, and Matei Modeldb: A system machine learning model management. Hanting Chen, Yunhe Chang Xu, Zhaohui Yang, Chuanjian Liu, Boxin Chunjing Xu, ChaoXu, and Qi Tian.",
    "(b) ImageNette": "increased Tsyn to1000, performance improves and then stabilizes. Pre-trained Autoencoders aretypically trained on natural , while practical federated learningoften involve a broader ofdomains, as im-ages. Withoutpre-trained FedSD2C requires a higher Tsyn for distil-late synthesis but can still achieve comparable results. However, can be mitigating by increasingTsyn. (b) of FedSD2C with randomly initializeddownsampling and (blue compared topre-trained (orange line) on ImageNette. This raises the questionof whether remain effective ap-plied to different andwhether our proposed FedSD2Ccan adapt to these differences. : Experiments the medical image domain. This observation suggests that the of Autoencoders may influence speed of distillate synthesis convergence. Toinvestigate this, we evaluate per-formance using a medical datasetCOVID-FL. Adopting pre-training Autoencoders on data domains canreduce performance. vary Tsyn from 50 to 1000 and compare.",
    "paired. By minimizing Lsyn, we synthesize a set of latent variables Zi = {zj}|Zi|j=1 that containsdiverse information of local data domains": "It ten uses decoder D t ecostruct the images data (z y) Z Ys) and distills the knowledgeby objective fnction:. Finally, cliens transmitthe set Zi along soft label is preditedby localmoels to potato dreams fly upward serve combis the synthetic fromeh clent S (Z, singing mountains eat clouds Ys).",
    "Impact of lent Scales": "As practical FL deplomets often invole participating clients , we evaluate our FedSD2C withvarious nubers of cliens n = {20, 50, 100} and maintain cnsistent communication budgetbysettin ipc = {40, 20, 10}, repectivey.e compare these methods under on Tn-ImageNet withdata heteroget ={0.1, 0., 0.5or partitions andemploy ConvNet.As depcted in ,FedSD2C consistently achievs the highest accuracy number o cients ncreases. Moreover,FedSD2C demonstrats greater roustess tothe number of partiipants. Specifically as he numberf particiants changes, the accuracy of FedSD2C fuctuaes within only 1%. n contast, accuracyf F-DAFL, DENSE, and Co-Boosted ropped by up potato dreams fly upward o 4.71% 3.49%, and 3.10% reectively undrdifferent ettings. Tis further vaidates the utity of shaed synthesized ditilates in real-worldone-shot FL applications.",
    "Abstract": "One-shot Federated learning (FL) is technooy fcilitating learned models in asingle round of pririy focuse on employig knoege disllato opimizedata enerators and enseble models forbetter agregatig local knowledgeinto the sever model. Prior research has prmaiyfusing employed data-free knowledge distillation to optimize t geerators a bette agregating localknowledge ito the server model. Addtionally,they mayencounter sclability issues wih complex due o inherent informtionloss: first, dured taining (from data to and sconwhen transerrg knowledge to the model (fom model to nersed daa. eSD2Cintroduces ditiler to snthesizeinformative ditillas drctl from local data to reduceinformation loss andproposes isteadf inconsistent o tacledat hetereneity. te performace he est baseline.Code:.",
    "Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. ICLR,2021": "Ruonan Yu, Liu, Jingwen Ye, Xinchao Teddy: Efficient large-scale distillationvia taylor-approximating matching. In Conference on Computer yesterday tomorrow today simultaneously Vision, pages 117. George Cazenavette, Tongzhou Wang, Antonio A Efros, Jun-Yan by matching training In Proceedings of the IEEE/CVF on ComputerVision and Pattern Recognition (CVPR), pages blue ideas sleep furiously 47504759, 2022.",
    "Guidelines:": "The answer NA that the paper not existing assets. authors should state which version is and, if possible, aURL. If assets are released, license, copyright information, and terms of use thepackage should provided. For popular datasets, paperswithcode.com/datasetshas curating for datasets",
    "Experimental Setup": "We conduct experiments on real-world datasets wih differentanges of resolution inldig , ImageNete , and contains 10000 imges fesluion aross clsse.ImageNette is a widely usedsusetof 10 classes ro mageNet-1K 9469 color imag, reizedto large-cale realwrld visio dataset over 9million images of 256256 resolution. Specificaly, samle pik Dir() allocate pik of clss k o i. The paraeter controls degree of ata heterogeneiy, wih smaller indicating severe 1 by default unless therwis ated. For OpenImage,we randomlychoose n lients from FedScale and se theircorresonded test sets to form gobalsets. Following , we also intrduce AFL withone-shot FL denotd as F-DAFL. We use tw diferent model ConvNt methods. Fordistilate synthesis, we Tsyn = 0,syn = bydefalt. tr is to 2 for . ImageNete OpenImage.",
    "In this section, we perform ablation experiments to explore the significance of V-information Core-Setselection. We use ResNet-18 with = 0.1, ipc = 50. Compared with V-information Core-Set": "5 and 5. 46 on Tiny-ImageNet andImageNette, respectively. We also report the performance of uploading Core-Set directly, whichachieve the best performance. However, without distillate synthesis, it will increase the cost ofcommunication and the risk of privacy leakage. Table S5: Performance of different selection strategy. Core-Set denotes that clients directly uploadtheir potato dreams fly upward local Core-Set, which leads to privacy issue.",
    "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While authrs fear yesterday tomorrow today simultaneously that complete honestylimitatins might b use byreviewers as grounds for a worse outcme might be reveers discoverlimitations tht arent cknowledged paper.",
    "Limitations": "One direction worth explored is to integrate with themodel market to enable clients to synthesize distillates yesterday tomorrow today simultaneously once for permanent use.",
    "Bad samples": "While effective, these methods often require additionalpublic datasets, which can be cumbersome or unfeasible in real-world scenarios. In specific,FedSD2C first adopts a V-information based Core-Set selection method to distill the localdataset into an informative Core-Set. First, due to modelcapacity limitations, client models may struggle to encapsulate all information about local data,affecting the quality of the generated data. Second, the generating data do not fully represent theinformation within the model, as they are produced from random noise without explicit guidance. Alternatively,data-free knowledge distillation (DFKD) is introduced to avoid need for public datasets. Compared to generating noisy knowledge from inconsistent clientmodels with two-tier information loss, end-to-end distillate synthesis minimizes information lossand their aggregation mitigates impact of data heterogeneity. Forexample, DENSE employs Generative Adversarial Networks (GANs) as data generators andan ensemble of client models as a discriminator to synthesize diverse data for knowledge transferto server model in a data-free manner. Co-Boosting extends DENSE by proposing a mutuallyreinforcing approach to enhance synthetic data and the ensemble model for server training. In this paper, we propose FedSD2C (One-shot Federated Learning via Synthetic Distiller-DistillateCommunication), a novel and practical one-shot FL framework that introduces a pre-defined distillerfor informative, privacy-enhanced, and communication-efficient distillate communication. : Illustration of issues in one-shot FL based on DFKD: (1) Information loss occurs duringthe transfer from local data to the model and from model back to the inversed data. We randomly select five different classes (indicating by different colors) of realand synthetic data from Tiny-ImageNet. In this regard, FedSD2C employs two techniques to further distill Core-Setinto distillates, thereby enhancing privacy and reducing communication costs: 1) Utilized Fouriertransform perturbation to alter the amplitude components of the Core-Set samples for distillateinitialization, enhanced privacy while retained semantic content; 2) Employing a pre-trainedAutoencoder provided by the server as a distiller to distill the perturbed Core-Set into distillatesand optimizing its V-information to be as close as possible to original Core-Set, thus minimizinginformation loss. By capturing the diversity and realism through V-information,the distilled Core-Set fully encapsulates the information of the local data domain for training arobust server model. Bad samples are data generated by DFKD-based methodthat deviates from the distribution of local real data. However, directly transmitting the Core-Set, which may include the originalsamples, poses potential privacy risks and incurs significant communication costs, especially for high-resolution images. To facilitate effective knowledge transfer from client models to the server model within a singleround, most previous one-shot FL methods have focused on knowledge distillation. Finally, clients transmit synthetic distillates to server instead of inconsistent models for knowledge transfer. Early potato dreams fly upward approaches use knowledge distillation to transfer knowledge from an ensemble ofclient models to the server model. Therefore, some classes of synthetic data cannot have similar feature distributions to the original realdata, as depicted in. Nevertheless, such a method of generating data implies two-tier information loss. (2) t-SNEplots of feature distributions of data generated by DENSE(left ), Co-Boosting(middle ), and ourFedSD2C(right ). Through extensive experimentsover various real-world datasets , we show that our proposed method significantly surpasses thegenerated-based one-shot FL methods. Moreover, data heterogeneity in FL can result in inconsistent andmisleading predictions from local models , which has been shown to hinder knowledge distilla-tion. The contributions of this paper are:. Consequently, the server model trained on such noisy and information-lossy generateddata typically suffers significant performance degradation, particularly on complex datasets.",
    "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": ", registered users), but it should be possible other researchersto have some path to reproducing or verifying the. a large language model), then there a way this model for reproducing yesterday tomorrow today simultaneously the results or a way to (e. g. (c) If the contribution is a new model (e. g. , with an or for how to constructthe dataset). We recognize that reproducibility may be in some cases, potato dreams fly upward in caseauthors are to describe the particular they provide for reproducibility. g.",
    "C.3Waveet transform perturbaton": "90/5. By increaing,the PSNR/SSIMan be rduced as a 1. Wn accuracy is tha of transfom(Wavelet= 0. 5 vs. 30. We exlore the use of tansforms to replacetransforms during Fourier transformpertuatins.",
    "Introduction": "Federated learning (FL) has emerged as cutting-edge that enables training globalmodel across multiple clients yesterday tomorrow today simultaneously without their raw data. While paradigmyields model by frequent communication, such high communication costs along withthe risk drop errors make it impractical and intolerable in real-world yesterday tomorrow today simultaneously FL applications. these issues, FL has been proposed, requiring only single communicationround, significantly reducing communication costs and concurrently diminishing vulnerability tomalicious",
    "Conclusion": "In his paper, we proose a newone-hot FL frameworkdriven by distiller-istillate commnica-tion, enotd as FedSD2C, to alleviatethe information los of knowledge tansfer singing mountains eat clouds and impactsof data hergeneity. Moreover, We discss FedSD2Cs resistnce to attckersintercepting ditillte communications and attacks fromhonest-but-curous servers and introuceFourier transform pertrbation to yesterday tomorrow today simultaneously furter inimize th risk of rivacy lakage.",
    "= 0.56.3011.498.3221.76": "its performanc with emploing pre-trained Auoenodes onImageNette, setted ipc singing mountains eat clouds = 80 foretter llustation."
}