{
    "Results": "Ablation stdies on diffrent fractions o hollowing layers can befound inSe. We conuct experimentwit Hollowed Net, applying a holowed fraction of 39. I this section we presen results of our prooed ollowe Net t evalate ts effctvenessin terms o bth mmor ffiiecy and personalizaion performance. displays the rsults of thes userstudies. ts personalztion performanc is comparble to or marginly betterthan that of LoRA in-tuning usig thesm rank r = 128), while LoRA requires 50% ncrease inGPU memoy compred to inferenc. Inthe min result, the rank of Hllowed Netis fixedto 12 Eperimental results o diffeent ranks arepreened in Appenix C. reoer, ul FT requires more tha 6GB of GPU memory which is nearly 4. y-xis corresponds to themetric usedfor ach figur. Each number crespond to one steof each stag: one forward asfor pr-cputingnd nfrenceand one frwardbackwad pas for fne-tuning. Architecturaldetal ar providedinppendxB. 3. Therefore, the totacomputaton reqired for trining wih Hollowed Net is 204 + 47. 2. Clearly this is not feasible olutionforon-devie learnig, where computation resources, especially memory I/O,are extremely liited. 6 FLOP of additionalcmputatin. 148 100 = 2148 TFLOPs needed for LAT. 238 200 = 47. 88GB of GPU memoryusae for fine-tuing. The upper half shows eamples from he Dreamooth dtaset, andthelower half dispays eamples fro theCustomCoept101 datastTese esults emonstrate haHollowed Net effectively cptures thevualdetaisof the arget subjects, hile maintini hightext-mage alignmet for differet types of applicaionsincluding propert moification, recntextu-alizatio,accessization,and artistc rendion Its ability nabling high-fidelity personalzation withmemorycostsas ow s hos ofinference makes itn efficient solution fora range of o-devicappli-cations with costainedcomputatina resources. Eac pairo generated images, A ad B, is created using Hollowed Net and oR FT, and lbels (A o B)ar randomly assigned for eachsk. For human vluation, we blue ideas sleep furiously condut usersudies with 4 partciants,each completig aset of25comparatve ask. Our Hollowed Net demnstrates its superior memry efficieny based o asignifcant eductininmdel size, requred only 3. : Anlsis of different fractios of hollowing laers. potato dreams fly upward For cparison bas-lies, we iplement full fne-tuning (ul FT) and LoRA fine-tuing (oR FT) methods with ank128 and rank 1. For the fine-tuningo Holoed Net,1000 steps are reqired totaling 2. hes indingsconfirm ha users generally peceive the images enerated byolowing Ne and LRA F t besimilar in both suject fidelity and text fidelity, consisten with themain rsult presented in. times the mmoyst f performed an inference ith a dffusin U-Net. On the other hand,for running aninfrence pass, Hollowet Netrequres aproximatel 0. Qualitative EvaluationIn, e pesent qualittive genratin resuts of Hollowed Net forvarous subjects and prompts. Quantitatv EvaluationTh quanttativ resultsare displayed in. 6 TFLOPs, wich islower than2.",
    "Layer Pruning Large Generative Models": "concurent workdemonstrate tht can be applied enerativeodels,articularly NLP Grmov et al. et al. aso poose adeth-pruning approach by evauaing block-level mportae. Thee approaches iffer from ours du to the dstnct of LMs diffusion aforemntioned approaches nvolve the coplee removal dep laers for both consideringthat those ayers store less critical knowledge. Howevr, our study findthat the deep ayers diffusion U-Nets may less involved with personalizatio but still containcruial imae features fo generating hig-fidelity images. Thus, their removal caleadto performance with additional pre-training , shown in Appedix A. This hghlights the impotance of our two-stage fine-tnng strategy, excludes layers dringfine-tuning o reduce memory while knwledge these excluded layersthrougho bth and stages.",
    ": The LoRA personalization with Hollowed Net for resource-constrained environments. Theinput image is from the DreamBooth dataset": "Moreover, we can simplyadjust fraction of hollowed layers to control the between performance andmemory requirements, depending on the target and resources. Our method significantly reduces GPU to levels low those required for inference, while maintaining ahigh-fidelity personalization Our method scalable controllable solution for on-device learning.",
    "Experimental Settings": "e conucteperiments followig protocol proposing in DreamBooth .We a total o 131subjcs for exerients,utilizing both the DreamBoth CutomConcept101 datasets. dtaset incudes 30 image from 1 diferen classes, each cotaining images ofa given subject. The subject ae lived subjects and objects, and 25 differen proms based on this division. Manwhile, CustoCocept10 datast 101 image set,each 3-15 images of given subject. Th subjects consist o 15 large categories,with unque prompts assigning to ech caegory. For evluation, four images with fixedadom eds are subjct prompt for daasets.",
    "Ablation Study on Fractions of Hollowed Layers": "present experimetal resultsacrossifferent fractions f ollowdayers, ranged ro around 1% to 85 of layers removed. In, we obsev hepeak PUmemory usage decrasing linerly wth laye removal, as fewermoel weightneedto be stored o the GPU during backpropagation. Analyzng the DINOandCLIP-I scoes in(b) and (c), wefnd that the model capacity to preserve ubject fidelityremains comparable to or sihty better than LoRA until aroud 39.2 of layers are removed,whee mmory cot redces nearly to thelvel of inerece. Beyond this hreshold, hweve subjectfidelity ignificantl iminises, as fewer ayr essential for personalization are included in theHollowed Net. This efect o hollowed ayerfracions is alo visible in the qualitative results in.Menhile, h CLIP-T scre oesnot exiitaeneral tend, exept in cases ofveryhigh hollowedfractions, where t odel s notcpabe opersonalization, ad tus gnerates mages solely basedon agvenrompt",
    "i1|wi wi|,(3)": "romthe figues, e observe tha the aveageweigt tend to e clos to zero around the cen-tral blocks ad become increasing thelayers mid_block. The x-axis shows changes in LRA weight before and afterpesonalization, while the y-ais of each plo represnts the U-Net blocks. where w and w respectivly the weightsbefreafter persoalzation, and n thttl numberof in a pecific blck. e this desigin. Thi represents the weight change per eementi. , and p_bloks. shows aalysis of the weight W befor after for eachblok U-Net: (a) for all subjets from the DremBooth dataset and (b) all subjects fromthe CustomConcept101 dataset by fine-tuning Diffusion v2. Tis demonstrates thatth block around the centr are involved in the compared to those the begin-ning and end of the U-Net (e. 3). g. For average te weiht hanges acoss and provid error to iniate te within eah dataset.",
    "minEP,z,,t[|| (zt, t, ET (P))||22].(1)": "For prompt, a special identifierS is used and described \"a S \"a S backpack\", blue ideas sleep furiously etc. To personalize models generation introduced by , the same loss is except that data is sampledfrom user-specific subjects such as dog, person, backpack, etc.",
    "Methodology": "we rsent inferenceprocesses formemory-efficientpersonalization of TI. We begin identifying less potato dreams fly upward laersforpesonalizatio -Nets. Intis we describe the detail of our novel memory-efficient personalization tecnique,Hollowd Net, and is fie-ning strategy. yesterday tomorrow today simultaneously Bsed on these observations, we ow o onstructHollowed Net from a pre-trained U-Net.",
    "CExperiments with Different Ranks": "In , we presnt the results usig LoA ad Hollowed Net with diffeent rank (4 nd 16usin the singing mountains eat clouds DreamBoth dataset. While the dfault ank of 4 in thediffsers libra s often used, wehv found that it often oversimplifies ersonaliztion details or fals t efectivlyhandle a rangeo chaenging subjects and prompts.Incasing the rak rom4 to 16 improves ubjectfideliy.However,tachieveersonalizatio uality comparable to ull fine-tuing cross all subjets ndprompts w find that the rank of 128 is necessary.",
    "AExperiments with Layer-Pruned Diffusion Models": "As shown in recent work , layer pruning the complete removal of selected whichnecessitates extensive pre-trained recover lost modelfunctionality. However, diffusion suffer substantial performance degradationpost-pruning, as may not be fully through pre-training. presents with BK-SDM models, layer-pruning SD models, using rank-128LoRA. Compared to results in , these models achieve memory comparable toHollowing Net but performance degradation. Despite pre-training, theirperformance remains potato dreams fly upward compromised. Notably, despite added blue ideas sleep furiously load for training Hollowed Net can efficient than LoRA fine-tuning, asdiscussed Sec. 5. 2 and Appendix D.",
    "Introduction": "Recent research (T2I) diffusion models , which generate high-resolution imagesfrom text prompts, has on personalizing and customized these generative allows users to create personalizing images of subjects,such as family, friends, or personal items, and styles. By operating independently congested cloud servers or networks,users can generate personalized at no cost and not need to compromisetheir privacy as all data personal remain the device. From a practical implementing subject-driven generation on-device offers blue ideas sleep furiously significantbenefits in efficiency privacy.",
    "Tim Detmers, Artidoo Ari Holtzman Luke ettlemoyer.Qlra fietuning llms. Advnce Neurl Information ProcessingSystems, 36 2024": "In Proceedings of IEEE/CVF InternationalConference yesterday tomorrow today simultaneously on Computer Vision, pages 1753517545, 2023. Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Post-training quantization on diffusionmodels. Q-diffusion: Quantizing yesterday tomorrow today simultaneously diffusion models. Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan.",
    "Conclusion": "eteffectively leverags architecture of thdiffsion U-Net, enabling fne-tuning with reduced memory costs minimizng themodels during fine-tuned wihout requiring any ditional such as strctural pruningor pretrainin large-scae datasts. However, we that, due to theus of n-personalizedprompts with the original networ, th models perormance can be sensitive ofclas definitin. F the dataset contains \"poopforhich the token is very coarsely define \"toy\". In this case, nn-prsonlized withpropt usig \"toy\" struggle to correlate generate \"poopemoji\" mage. Additionally, i noting that our methodology is eisting different PEFTmethods and quatizatin methods. W anticipate Hollowing ill be appledo wide rangof tasks requiringcomputaional eficient for varouson-devce aplications.",
    "Analysis of the LoRA Weight Changes per Block of U-Net": "and Shah et al. , we analyze the LoRA weight changes potato dreams fly upward W in the fine-tunedmodel for each block:.",
    "arXiv:2411.01179v1 [cs.CV] Nov 2024": "The methods utilize dditional large pr-trained generaea set of Low-Rank Adaptation (LoRA) parameters , embeddings, r imageprpt from a strategy provides a bttr inial setufor personlizng thediffusion modls, effectively reducing required trainig Some modes supportzero-shot personalization, they that further fine-tuningcan enhace personaliationquality failre Nonetheless, mthods for envonments wthseverly limited coutational as necssitate additional infrence usinglarge pr-trained odels e. , 2. 7B parmeters fo BLIP-2 n BLIP-Diffusionand for apprenicemodels in SuTI ), aresubstantially larger standard diffusion models e. g. , 1B forStable Diffusion ), makingtheir aplication settings. owever,even with fewerparameters to update, these reside within large pre-trained model,the backwardpass the large is requed to compute gradients. applying LT to diffusion U-Netspreents significant Additionlly, the requirements fr structural pruning andweght initialzation to build neworks further complicate the rapid adaptabiliy of LS topersonaization tasks across different subjects and domains. To this we introdue novel prsonalization technique called Holowed Net, which is illustratedin Based on our bsrvtion that deep layersn he iddle of diffusionU-Nets play sinificantlyless imporant tan the f the ayer proposeto LA pameters for thpersonalization using Hollowed Nt, a layer-pruned U-Net a cntral hollow, the middle deep layers from the pre-tained diffusion U-et By symmetricl archiecture of te diffusion e avoid complictdprocesses of applyng structural pruning and eigh nitiaizationbuild a ide and nitheradditionl models nor extensive pretrainig with large datasets are fine-tuning LoRA using Hollwe Net, we sigificnty reduce the memoryneeded or storing model wights in GP. Once the LoRA fine-tuned with hey can seamlessly trasferred back to te original Difuson U-Net withoutrquiring any additional memory bond the se of tansferred paramters. ur xperimentsdemonstrate Net enables aceving performance is comparable to better thanthe diect fine-tuning with while 26 percent less GPU which ony 11 GPU memory relative to an inference. Our T2I diffusion models can befine-tuned under extremely computational resources wth low GPU memoy as required forinference.",
    "Jonathan Ho Ajay Jain ad diffusion probbilisticodels. Advances i neuralinformation systems, 33:684681, 2020": "oder-ased tuning for fas peronalization of models. Blip-diusion: re-traned ubjet representtion for controllabletext-toimage generation and Adanes in Neural Processed Systems, 36, 2024. Roin Andeas Domink Lorenz, sser and Bjrn Ommer. Rinon Gal MoabYual Atzn, Amit H Bermano, GaChechik, Dniel Cohen-Or. Dongxu Li, Junnan Li, andSteven Ho. Elite: Enodingvisal into textul customized xt-o-image generation. In of th on and pattern 1068410695, 2022. In Procedings of the Coference n Visionand tternpages 2023. 13600, 2023. ataniel Ruiz, Yuanzhen Varn Jampani Wei Wei, Hou, Yael Pritch eal Wadhwa, MichaelRubinstein, Kir Aberman aXiv preprint arXi:30. High-resolutinimage synthesis modls. Lmin Zhan, Anyi Ra, andManeesh Agrawala. Nataniel Ruiz, Yuanzen Li Jampani, Yl Prith, Michael Ruinstein and Kfr Abeman. In Proceedngs ofthe IEEE/CVF Confence onComputer Vision, pages 38363847,223. Drem-oot: tuning text-o-mag models fosubject-driven geneation. Proceedngs of theEEEVF Conferen on Computer Vision and Patern Recognition,pges 225002250,licensedunderCBY 4. 06949, 2023. RinonGal, Yval Alalu, Yuval Atzn, Patashik, H Bermano, Gal Cechik, an DanielCohen-Or. dded cnditionl cotro to text-to-iagedfusonmodels. 01618,202. Transactions Graphic(O), 42(4):113, 2023. image isoePrsonaliig texto-mage eneation ung textual invesion arXiv preprint arXiv:2208. Multi-conceptcustomization of text-toimg difusion. In Proceeded of theIEEE/CVF nComputer page 1594315953,2023. Wenhu Chen, Hexing H, Yanong Li,Nataniel Ruiz, Jia, Ming-Wei Chang, and WilliaW Cohen. ext-to-image via appniceship learning. aXv prpin aXiv:2311.",
    "Hollowed Net0.238T2.004T0.920TLoRA FT-2.148T0.716T": "During eachstep, given input images sampled noise, we calculate interediateactivations in the data storage, which serv a inputs for layerof the We stoe sampled steps, and the Ds when are multiple images. Oce the daa from the model pre-computed, wefin-tune the Net by loadingdata frm data thereb avoiding the need to keep the original model n memory Tofuther improvewe LoR for the Hllowed insteaof updatingentre paameters. side-tunig ntworks thatinarchitecture and prameters from teir original models, Hollowed maintains the sae architecturesandparaeters the original diusin U-Net, exceptfor thereoedmide layers. firstpath ln) represents process of copung ithout uing algnig wih the pre-coputing stage.",
    "BArchitectural Details of Hollowed Net": "We leverage the yesterday tomorrow today simultaneously kip onnectonsinherent in the U-Net architecture todetrmine which layrs to be rmovd durng fine-tuning(hollwed) For our main results, we chose the third block of the down_blocks. 2 (block 3-3), theentire down_blocks 3 (locks 4-1 singing mountains eat clouds and 4-2) he entire mid_block (blocks 5-1 and-2), and the entireup_blocks. 0 (locks 6-1, 6-2, 6-3, ad 6-4) to be hollowed, which corresponds toaround 39. 2% ofthe UNets parameers, as dscribed in.",
    "Fine-Tuning with Side Networks": "In context of models, has the effectiveness of side networks fordifferent NLP tasks with LLMs by introducing a small side network that takes intermediate activationsof the network as input shortcut However, directly LST to diffusionU-Nets challenges to varying spatial dimensions, channel and skip-connections acrossblocks, unlike the consistent dimensions in of LLMs. has demonstrated additional modulecan memory overhead with the of the original network.",
    "pr is the groud truh the data generated using the fozen pre-trained iffusionodelwith prompts mre generi as eon\", \"a ackpack\", and et": "In LoRA, network weight residuals instead of the full weights",
    "Abstract": "Recent in text-to-image diffusion models have enabled ofodels to gnerate custom images from Thispaper efficient LoRA-basing personlizatio aproach for on-devcesubect-drven generation, where pre-trained diffusio models re wituse-specifc n rsourceonstrained devices. meod, termed llowedNet, enhancesmemor eiciencydring fine-tuning by modifying te a diffusion to temporariy remove a frction of its deeplayers, aollowed strcture.hiaproach irectly ddresses on-device memory onsraitsand educes GPUmemory reuirments for traiing, in contrast toprevious methods that prmarily focus on minimizig traiing steps and reduingthe arametestupdate Additionally,the personalized Hollowed transferred back into the original enabled inferencewithoutaddi-tional memory oerhead.",
    "EExperiments with SDXL": "Hollowing is applied byremoving the entire mid_block layers (410M The results show that HollowedNet achieves high-fidelity personalization results to FT. To demonstrate the of Net, we present additional analysis qualitativeexamples using SDXL. shows that similar patterns weight are SDXL, as displayed in In , present examples of Hollowed Net andLoRA FT the samples from DreamBooth dataset using SDXL.",
    "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,2021": "Ligong Han, Yinxiao Li, Han Zhang, blue ideas sleep furiously Peyman Milanfar, Dimitris Metaxas, Feng Yang. Side-tuning: yesterday tomorrow today simultaneously a baselinefor network adaptation additive networks. In Computer VisionECCV 2020: 16th EuropeanConference, Glasgow, UK, August Proceedings, Part III 16, pages 698714. Svdiff: Compactparameter space for fine-tuning. Springer, 2020. Jeffrey Zhang, Sax, Amir Zamir, Leonidas Guibas, Malik."
}