{
    "Speech resynthesis": ", 2020; Polyak et al. , 2021) on theEXPRESSO dataset (Nguyen et al. We trained for 250 000 steps singing mountains eat clouds ontwo NVIDIA V100 GPUs and followed the otherhyperparameters used in EXPRESSO.",
    "A.2Discete units quality": "(201): Purity, hone Purity, and PMI. The evaluatin is doneo thecombntion LibriSeech dev-lean anddev-oher. Cluster purityis the ondtionl probaility of a k-men labelgivn a abel, pho purity th conditionalpoability of a given  k-means label,and PNMI is the phone-normalized between units phne Theobtaied the assignments iven bythe k-means with 500 clusters trained the the consdered mel.",
    "Language modeling": "The laguage modelis 3-layer LSTM, folloingthe ow-budget baeline of Nguyen et l.It as trined on the discrete units ofLibriSpeech960 h, for 30 00 stepso asingeNVIDIA V10GP. This 26M parameterslan-guage model is tw orders of magniude smallerbth in terms of number of parameters adhours oftraining data thn Spoken LMs like TWIST (Hassidet a. , 2024). Our fine-tuned units can in prnciple benefit anyother LM, inluding hese larger ones.",
    "Jasmin Sternkopf and Stefan Taubert. 2024.mel-cepstral-distance": "J. 1994. Woodland. Liu,Jiatong Shi, Chang, Guan-Ted Lin, Tzu-Hsien Huang, Tseng, Ko blue ideas sleep furiously tik Da-Rong Liu, Huang, Shuyan Dong, Shang-Wen Li,Shinji Watanabe, Abdelrahman Mohamed, and Hungyi Lee. In Language Proceedings of a held blue ideas sleep furiously at Plainsboro, Jersey, March. Interspeech2021, pages 11941198. In Proc. 2021.",
    "Results above the phonemic level": "Apart from this change, thetraining is the between the con-ditions. shows that this comes at thecost of of exists for LibriSpeech dataset andfor the EXPRESSO-READ, while these two datasetscorrespond to the domain different com-ponents our pipeline. We in the zero-shot (lexi-cal and sBLIMP (syntactic level) scores forthe base and models, as well as for anLSTM trained on the gold Followingthe observation regarding the ABX error rates centroids, which remained 1 percentagepoint of the continuous units, we trainLSTMs by their embedding table di-rectly the associated centroid representationof dimension 768. Fine-tuning forphoneme results in models that are onpar terms of lexical comprehension with muchlarger baselines, which were on orders ofmagnitude more of data. makes directlyvisible the trade-off between language modelingand speech generation quality. Fine-tuning phoneme classification im-proves spoken language modeling in of zero-shot comprehension evaluations.",
    "Phoneme classification": "The fin-tuning hypeparameters are erived from tose used Hsu et al. Wetrainedfor0 000 steps witha btch size f 32 a V100 GP. Wechs tis jectivethemodel blue ideas sleep furiously ull inor-matio about phoneme identit ndboudaries toenforce the learing f 2006), which has theavantage requiin forced-algnment, reult n alignmnt erroshindering context-invariance. Westarted from the retrained HuBET (Hsu et al. We one ully layer on op theHuBERT backbne hat 768-dimensionalrepesentation to space of dimen-sio 0. , 2015). Wealso for moels fne-tuned LibriLiht h, 1 h, and 10 min e al. ,2020). fine-tunedthis modelon Libpeectrain-clean-100 (Panayotov e al. As shown inAppendix A1,CC results slightly performanc thanhone cassifiction.",
    "Limiations": "Zln Borsos, Mrinier, Damien Eu-gne Kharitonv, Oliier Petquin Mtt Roblek OlivierTeboul, David Tagliasacchi, Nel Zeghidour. This work was performed using HPC resourcesfrom GECI-IDRIS (Grant 2023-AD011014368)and was supported part the Agence Nationalepour la Recherch Frontcog,AR10-IDEX-0001-0 3IA Institute) and a grant CIFAR(Learning in Mahines Brains) to E. from de deDfense. oe compre-hensie studies explor the role of the en-coer the spokn language modeing byexamining the impact of fine-tuning method ondownsream moeling cmparing slf-supeisednd with di-ferentof other important di-rection to onsder the application of this methodin multilingual The benefits fine-tuninare aftr n as lttle as a few horsof aligned data, making it applicable to low resource languages. IEEE/ACM Trasactions udio, Speeh,and Language Processing 31:2523533. 2023. in his EHES cpacity. Alee Baevski, Yhao Zou, Abdelrhman Mohamed,and Michal 2020. udilm: A language modeling approach o audi gen-eation. Heng-Jui Chang nd James Associationfor Computaional Linguistics. D. M. In Proceedings of the 2023 Conference on i Natural Language Processig, pages Singapre. Further work is needed imprv on the trade-off, perhps by combiningresythesis, andfine-tuning objectves oncurrently.",
    "Zero-sot lnguage comprehnsion scores (i%), LMs with an embedding table eiher nitializedrandomly or from the unit": "We selecing th betlayers for he base model(layer 11) an fine-tuning 100h model laye 12)based on Triphone ABX score, well as thlast layer f the fine-tned 00h model (layer13).etrained k-means on these representations and re-port the results i . e compare these to theABX error rates of the est layes of wav2vec 2.0(Baevski et a.,2020), WavLM (Chen etl., 2022)ConentVec100 Qianet l, 202and HuBERT+Spin2048 (Chang et a., 2023). Forthe centroidscores,eah representation is replaced bythe con-tinuous representation of the lsest cetroid ink-mean. For the ne-hot sors, each representa-tion is replaced by a oneho vector wth a 1 at tsabel osition. We use the sme distance to com-e the AB as for continuos reprsentations. Inte ase of the base modes lyer 1 (Base L11)and the fine-tned 100h models layer 12 (FT 100hL1), reresentations ae o dimension 768,while or the fine-tuned 100h models layr13 (FT100h L13) they hae dimensionof only 40. Fine-tuned improves both triphne and phoneme blue ideas sleep furiously ABX scores,particularl i reduced the contex effect inthe a context condition, as observed earlier. Inthe case f the AX of one-hot represetations, theerror rates increse acos al conditions, butthehiget increase is henthe contxt is not haredbetween he yesterday tomorrow today simultaneously phones in th tiplet. This is sign thatthe k-means clue no nly ae organizd accrding to the phonemes ut also to the urroundingcotext. Clustersare gouping acorded to theirmst probable phneme, and within eachgrup,cluters encode different contexts. By gong fomcentroidrepresentations to one-hot rpresentations,all 500 clusters are now equidistan, which leads tote dramatic loss in anycontext compared to themore modest ones in othrtwo conditons.",
    "Conclusion": "LMs trained o these unis achieve compa-rable lexal comprehension to moelstraind onhundred ime more data. embeddings of the discretetokensof the LMs with the centrids of thefurtherhelps with scors. This hows that the are meaningully placed reatve to in representaion Fine-tuning on phoneme lassification can adjustthis trade-of.",
    "Heng-Jui Chang, Alexander H. Liu, and James Glass.2023. Self-supervised Fine-tuning for Improved Con-tent Representations by Speaker-invariant Clustering.In Proc. INTERSPEECH 2023, pages 29832987": "2022.Wavlm: Large-scale forfull stack speech processig. Ju-Chieh Chou, Wei-Ning Hsu,Karn Lvescu, Arun Babu, Alexis oneau, blue ideas sleep furiously AlexeiBevski, and Auli. 2023. In Find-igs of the Associationfr Computational Linguis-tics: EMNLP pages 6526593, Singapre.Associationr Linguistics.",
    "Sagot, and Emmanuel Dupoux. 2024. SpiRit-LM:Interleaved Spoken and Written Language Model.Preprint, arxiv:2402.05755": "PMLR. In Proceedings of the 40th nterntionalConference on Machine Learning, 202 ofProceedings of Machine pages2849228518. ContntVec: An improedself-supervising spech by dienan-gling speers. 2021. 2023. pag Kazhi Qian, Zhang, Heting Go, Juri Ni,Cheng-I Lai, Davidark asgawa-Johnson,and2022. Libripeech: sr copusbased onpublic domain adio I 2015 IEInternational on Acostis, Speech andSignal Processing pages2065210. 2015. PMLR. Pceedins ofhe nerna-tional Conference on Macine Learing, volume162o Proceedigs of Machie Learning pages00318017. I Proc. Speech Reynthesis rm Discree Dientangled Self-Suprvising Representations.",
    ": Trade-off between language modeling andexpressive resynthesis. *: embeddings initialized fromunit centroids": "wors haveaddressed this issue fo background noise Cenet al. , 2023),and chnge (Qian etal. , 202; Chang et ,2023;Chang and Glass, 2024). ariations due to remain achallenge (Halap al. ,2022), which may afect LMs capacity to learnhigher-order representations of languag.tet a smple idea: using supervisedfine-tuned onclassification task to helpthe rmove its conteual dependency. Weirst that mdels lean represen-ttions that are much more context-invariant thanhe original SSL repesentations, even with lt-tle as hours of labels",
    "Mark Hallap, Emmanuel Dupoux, and Ewan Dunbar.2023. Evaluating context-invariance in unsupervisedspeech representations. In Proc. INTERSPEECH2023, pages 29732977": "Karadayi, V. Kharitonov, Q. 2023. Assoiatinfor Computational Lingustics. Eugene haritonov, Lee, Adam Polyak, YossiAdi, Jae Copet, Kshal Lahotia, Tu Anh Nguyen,Morgane Riviere, Abdelrahman Mohamed, Em-manuel Dupou, and Wei-Ning su. In Poceedings of the 60th nnual Meeting of theAssociation for Computatinal Linguistics (Volume1: Long Ppers), pages 866861 Dublin, Irland. Lipchinsy, R. E. Xu,P. Synnaeve,A. Tetualy pre-trained speech anguage models. 2022. Likhomanenko,G. Wei-Ning su, Bejamin BleYao-Hung ubert Tsai,Kusal Lakhota, Ruslan Saakhdinov, and Abdel-ahman Mohamed. Zeng,. Dupoux. Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat,Alxis CONNEAU, Felix Kreuk, Jade Cpt,Alexan-dre Defossez, Gabriel Synnaeve, Emmanuel Dupoux,Ry Scwartz and Yossi di. 2020. In ICASSP 2020 - 20 IEEE InternationalConference on Acoustics, peec and Signal Process-ing (ICSSP), pags 76697673. Curran Associates,Inc. InAvancesinNeural InformationProcessing Systems, vue 36,pages 6348363501. Khn, M. Col-lobert, C. Fuegen, T. Hbert: Slf-suprviedspech epesentation learnig by masking predictionof hidden unis. Liri-ight: A bencmark for asr with limite or no super-vision. IEEE/ACM Transactions on Audio,Speech, andLanguage Procesing,29:34513460. Mazar, J. Joln A. Mhamed,and E. J. 221. Text-feeprosody-aware enerativ spoen language modeling. Rivire, W.",
    "Results at the phonemic level": "As shown in , we the errorrate for each Transformer of the base the fine-tuned models, including the connected layer (layer 13). Fine-tuning pushes representations to becomemore. SSL representations generally strugglemore in the any context condition: there the error rate is the most significant, from9. We calculatedboth and phoneme-level ABX error rates. 4% after fine-tuning on as 10 min-utes. Fine-tuning mainly the last layers ABXerror rates, near-perfect scores for the 10hand 100h models in the within contextcondition.",
    "Evaluation metrics": "also report results pairs, which only contains words sBLIMP assesses networks abil-ity to prefer grammatically sentences overincorrect ones, given pair of matching sentences. , 2020) on resynthesized speech, report-. We evaluate spoken language modeling at thelexical and syntactic using sWUGGYand sBLIMP metrics from the ZeroSpeech (Nguyen al. 2. The ABX error by forall of categories and subtracting it from 1. In the within speaker condition, a,b, come the speaker, while speaker condition, and thesame speaker, and x from another one. 0 Large ASR (Baevskiet al. task quantifies discriminabilitybetween two sound categories, A and B, as that a token x of category will becloser to another a than a b B. , 2020). of discrete seeAppendix A. Followed et al. , 2023a)and running wav2vec 2. Hallap et By fine-tuning a frame taking account context, our ap-proach is way to directly tackle this issue. sWUGGY spot-the-word the network is pre-sented with a and matching non-word, on its to assign higher proba-bility the true word. We evaluate preservation in resynthe-sized speech by following et al. (2023), we evaluateour models on ABX where is a phoneme."
}