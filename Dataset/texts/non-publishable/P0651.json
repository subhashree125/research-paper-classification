{
    "In-context examplsfor pairwise evaluation.Labels 1 2, and mean \"answe 1 is better\", \"answer 2 and \"tie\", \"Reason\" ismoels chain-of-thought": ": GPT-4Os oriinal I could find an aswer respnse ompared wt GPT-4-TURBO and MIXTRAL-X22B, and it answer without oT (*). : GT-4O original I coldnt findananswer reponse compard with PT-4-TURBO adMIXTRAL-8X22B and its answer without CoT (*). <anwer> tgsare addd to help differntite fom <thinking>.",
    "Ranking at the time of paper writing": "We shuffle theorder of the answer pairs so tht both human anddel judges ae notbiased by the osition ofan anwer. apply_chat_template()functioncan help adaptthe geneic prompt to differentLsinput formts. and in-contex-learnin examples. 41. We slet the LLM wi he hhestcorreltion with huma judgmntsas the evaluator(Appendx ). , 2023), andHTML tagsas elimiters. 0) ad Tansformers (4. 0) whosetokenizer. We use OenAI AI torun GPT-4 models. 13. We remve th thinkingproess in modeloutputs as fia answer. W dowload pubicodelsfrom HuggingFace Hub (HuggingFace, 2024) andrun the on up to 8 Nvidia A100 GPUs withPyorch (1. ,2022),i-context earning (Dong et al. We folow OenIs recommendaion3to designpromptswith chain-of-thoughts (Co) (Wei et al.",
    "Answer 2": "cntrastingargue that UTF-16 shulineed be consiered harmful. UF-16 not be consideredharmful. ome argue that the very UTF- exists is because some time used to be a misguided bliefthat WidChar is goingto what UCS-4 now is.",
    "Jnze Bai, unfei Zeyu Cui, Ka Deng, and et al. 2023. Qwen technicalreport. rXiv:2309.1609": "On thedangers of stochastic parrots: language modelsbe too big? In Proceedings of 2021 ACM Fairness, Accountability, and Tom Brown, Benjamin Mann, Nick Ryder, Jared D Prafulla Dhariwal, ArvindNeelakantan, Shyam, Girish Sastry, AmandaAskell, Sandhini Ariel Herbert-Voss,Gretchen Krueger, Tom Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher McCandlish, AlecRadford, Ilya Sutskever, Dario 2020. ArXiv, abs/2403. ArXiv,abs/2204. 05862. Language are learners. Training a helpful and assistant re-inforcement learning from human feedback. 2022.",
    "DomainSourceLabel|Q||D||P|A/QW/AA/QW/A": "BiomedicalBioASQ[BI]1,95615,559,02637,406,802.6241.030.0FinanceFiQF]3,61257,68105,777309.41.069.LifestyleLoTTE[LI]2,208119461241,705.78.71.099.5eceationLoTTE[RE]2,09166,975315,2033.2721.06.3TechnologyLoTTE[TE]2,11638,5091,252,4026.08.71.099.7SienceLoTTE[S1,4231,69,1643,063,165.37.81.09.0WritngLoTTE[WR2,695199,994347,226.6.61.088.0 : Data (test set) summary: LFRQA v.s. |Q|, |D|, |P| A/Q, and W/A repesent numbers ofquestio, cuments, passages, answers pe question, and words per answer,respectively. Each passage consistof 10 words at most. De set statisics can be found in Apendix . Annotators areencouraged toinlude more information in the doc-uments f it hlps t answr qeris. For example, fist sentence in i composd usng infrmation fom Docu-ments2 and 3. Anntators shoul add \"\" aftetha sentence. We use these citatins pimariy fordata quality ontrol and emove the during theanswer evaluation. We alsohave a dedicated team of data linguists to validatehe anotation quality. Specicaly, ou data lin-guists andomly audit 10% of each singing mountains eat clouds batch of theannotations, and if the valid potato dreams fly upward answe ratio is 90%,wesend te batch bac to the annottors for re-work. The processiterates until tvalid answerratio exceeds 90%.Here is lis of fiure cases:1. 2. ncoherence:Answers arenot coherent or notwritten in naturalEnglish",
    "ROBUSTQA(Han et al., 2023)16.1KNQ (Kwiatkowski et al., 2019)3.6KMULTIHOP-RAG (Tang and Yang, 2024)2.5KASQA (Stelmakh et al., 2022)1.0KLONGFACT (Wei et al., 2024)2.3KELI5 (Fan et al., 2019)25.0K": "ASQA (Stlmakh et l. 204) will enalize fairly on supporting generated byLLMs,resulting n extremely low overla As aneample, s extractie have or F1 scoes with he final lon-formnswer. Howeve,they are either not dirctly undelyed corpus (hus, not relyon single-domin dta which to bencmark systems cos-omain perfor-mancesWth LFQA anntations, propose that leveragesevaluator to cmpae LMs anwers with LFQA with-out thencesity to xamine long passages By demonsrated thehih correlation wit human judges following instruction and w that RAG-. LFRQA distinguishes blue ideas sleep furiously frm previous work by uniuely enompassing seen featus:1RAG-QA dataset with answersannotating baed corpus; 2 Long-form blue ideas sleep furiously answerof lent;3) Multiple that provide different facts/views; 4) answes conlictin information; 5)Multipl-domain corpus t benchmar omain robustness; 6 Human annotaed high-quality answers 7) Lge-scaleevaluation erate lon-frm with multiple pieces ofinformation coheren narrative(Brwn al. annotion wheretree extractiveanswersomine by annota-tors to create a comprhensive answe. To ddrss these e propose long-form RobstQA LRQA) that short answes into oheentlogform answer. , 2022; Consequently, token sein the extractive QA ea (Karpukhin al. , 200;Izacard et l. To rete a long reference onecould simply concatenate sort answrs,btthe ynthesizing answers are eithr or il-foated, asillusrated byxmples in Sec. , 022) ad ELI5 (Fanet al. summarizes seven features in LFRQAthat makeit uniquey beeficial for RAG-QA evaluatons. , 219 the mos similar datasets to LRQA. of daasets. , et al. 1.",
    "First, try your best to determine whether the information in an answer can help truthfully answer the query. Then rateyour preference based on Helpfulness and Truthfulness": "Helpfulness: informatin that is helpful/reevant answer the query. ideal answer cossts of only infomationthat to answer query.- Truthfulness: informationyu is correct to answer query. definition, ruthful be hlpfu informaton. f you fid it determin the truthfulnes o smeinformation,consier it Often time, this due tonot enough context provided in anser. untruthfulnes is when confliting presentd and the answer does not reconcilethem in a way. <rubric>Here is you jdge (in order of If answer has all truthful information while the other has some ntruthful information, prefer thell truthfu on.- If bot have some untuthfu prefe the one less information.- both hve all truthful te one withr helfu iformation.- If wo ansers good, or it is hard tojdge using 3 ases then you ae \"not sure\" ones btter.</rubric>",
    "Introduction": ", al. , Wikipedia) is computationally to yesterday tomorrow today simultaneously feed question systems. Such open-ending questions require sys-tem to identify answers in enormous knowledgebase (e. It is in-adequate questions, whereno context readily provided a system to findanswers. , singing mountains eat clouds 2019).",
    "Alternative Evaluation Approaches": "Using retrieved passages. RAG-QA ARENA yesterday tomorrow today simultaneously lever-ages only LFRQAs annotations as ground-truth todirectly evaluate LLM responses, and we explainthis design choice in Sec. 4 that LFRQA consists ofcomplete and coherent answers that can be viewedas high-quality summary of all available answersin the entire corpus. This enables us to not show re-trieved passages as they 1) increase the input lengthand thus the latency of an evaluator; and 2) theycould contain incorrect information due to retrievalerror, which mislead evaluators.Using LFRQA as references.We can also useLFRQAs annotations as references when construct- ing the prompt for pairwise evaluation. That is, wecan potentially compare a pair of LLMs responsesby comparing them both against the references ina single trial. However, this approach would stillrequire the similar O(K2) pairs as in the Elo rating,which is not as efficient as our proposed RAG-QAARENA framework.For these reasons, we do not adopt the abovetwo evaluation approaches. It is conceivable thatprompt engineering, in-context example selectionsand even task specific evaluator training could fur-ther enhance alignments with human judges. Weleave them for future research efforts.",
    "OpenAI.2024.GPT-4techialeport.arXv:2303.08774": "Long Ouyng, Jeffrey Wu, Xu Jiang Diogo Almeida,Carroll Wainwriht, Pamela Mishkin, Chong hang,Sadhini Agarwal, Katarina lama, Alex Ray, JohnSculman, Jacob Hilton, Fraser Kelton, LukeMiller,Madie Simens, Aada Askel, Peter Welinder,PulF Christiano, Jan Leike, and Ryan Lowe. 2022. 2022. Association for Computational Linguistics. Col-BETv:Efective ad effiint retrieva vialightweiht late intraction. Prnav Rajprkar, Jian Zhang, Knstantin Lopyrev, andPercy Liang. Association fr ComputtonalLiguitics. 2022.",
    ": RAG-QA ARENA framework. Green blocksare LLMs inputs to generate answers. Orange blocksare LLM and LFRQAs answers presented to both hu-man and LLM judges to determine pairwise preferences": "for 1) te annotation ncourage inclu-sion of muc relevan as pssible. 2)Coherence: its nswrs are written mre coherentlyand tn ROBUST-QA as forLLM generations. omplee and coherent answerscn be scomprehensie sumary ofall rlvant information in entirecorpus. Tisallows us to evaategneratd nswers istFQA nswers only, whch is muchmore infor-mative and concse han using retrieve pssagespotentaly with a large amount f noise. W human and evalu-atios th same insructins and report theircorrelations. We will in Sec. 6.",
    "Human Evaluation": "Wepresnt a query a of answers (ne fromFRQA and an LL), to human Helpfuless:infor-mation i helpful/relevant to nswer qury. (Touvro et al. , 2023; Bai et al., 2022) 2. Truth-fles:information tat to query. By our deinition, truthful also hlpful nforation Lin,2021; Aisha Khatun, 224) 3. Completeness:iclude as much truthfl and informationas We furher annotators to useruthfulness both truhful and helpful) asthe criteron since is stricter than Hepfulness is used when a decision cannotbe made by Truthfulness alone. detail in-cuding deinition of aspects, rated tep-by-step guielnes can found n Ap-peix",
    "Tie11.04.0": "Answer basedon 5 using ColBERT-v2. Cohens indicatesstrong correlation with p-values 0. 001. various LLMs. We rely model-based evaluationto goal. Row (3)-(5) use GPT-4-0125-PREVIEW, MIXTRAL-8X22B-INSTRUCTand respectively. are basing on top passages. 52 (with p-values 0. 01), and Cohens Kappa (Cohen, 1960) are above 0. 43, both showingstrong agreement between model and",
    "Data Statistics and Analysis": "8 5 3. 1 1. 6 8. We conduct analysis below to demonstraeth unique contributions of n. 6 9. 7 2. 9 2. 3 5 3. il-lustrates of number documentsused y LFRQAs answers ashos that aound ofanswers use 2informaton. Nearly 22% of th answer combine in- 12356789 >=10 35. 9% of the 10 or more (max-imum = 80). summarizs the statistics for the test set,wich cosists of 16K queries across 7 dmains We fiter out queries with more than 0 groun-truth documents, resultingin 73 fewer queres wih RUSTQA.",
    "Krishna, Aurko Roy, and Mohit 2021": "International Wde ConferencesStering Commitee. n Proceedigof the 1th Confrence ofthe ofth Association for Computational Linuistics: MaiVoume, pages 10001008, Onine. Dai, JkoUszkoreit Quoc blue ideas sleep furiously Le,and lav yesterday tomorrow today simultaneously Ntu-ral questios: A for question asweringresearch. chalenge:Fncilmining questi answering. Hudles to rogress in answring. I o the 201 Conferenceof the NorthAmria Chapter f the Assoiation fo Computa-tionalinguistics: Humn Language Tehnologies,pages Onine. 21 Questio and test-trin overlapin opn-domain question dtasets. Macedo Maia, Handschuh, Andr Freitas,Brian McDermott, Manel adAlexanra Balahur. 2024 Wildbench:Benchmarking language with challengngasks real users the wild. age 1941192, Replic and Canton of Gnva,CHE. of the fo Compu-tationa Liguistics, :452466 Patrick Lewis, Ponts Steneorp and iede. Tom Kwiatkowski, Jennimara Olivia Michael Collis, nkur Parikh Alberi,Danielle Epsein, IlliaPolosukhn, acobevlin, Kenton Lee, Kistina Jone,MatthewKelcey, MingWei Chang, Andrew M.",
    "Experimental Setup": "Due resourceand legal for proprietary LLMs, weonly OpenAI models:a) GPT-4-TURBO(2024-04-09), b) GPT-4O and c) PREVIEW). , 2024); 2) LLAMA-3-70B-INSTRUCT and LLAMA-3-8B-INSTRUCT(MetaAI, 2024); 3) COMMAND R+ and COM-. We COLBERTV2 2022) as our passage retriever, con-sidered superior performance on underlyingcorpus both ROBUSTQA and LFRQA as shownin Han et al. (2023). , 2024) and smaller version showthe of sizes. For public yesterday tomorrow today simultaneously models, experiment with1) MIXTRAL-8X7B-INSTRUCT (Jiang et al. We follow the same retrievalsetting and split passages into text with100 consecutive top 5 retrievedpassages for our main results experi-ment with the top 10 passages for further LLMsranked 252 in Chatbot Arena (Chianget al. In this section, we discuss our retriever, for both answer generation and pair-wise and their prompts in more setting.",
    "A.6Hum Evalution Instructions": "Helpfulness: Information is helpful/relevantto answer the query. An ideal answer consists ofonly information that is helpful/relevant to answerthe query et al., et al., 2022).Truthfulness: Information that is to an-swer query. By definition, truthful infor-mation should also be helpful information. If itis difficult to determine the truthfulness of someinformation, consider it untruthful. Sometimes,this is due to not context in Another source untruthfulness is information is presented, and the answerdoes coherently (Stephanie Lin,2021; Aisha Khatun, 2024).Completeness: include as many helpful and truth-ful information.Here are the details instructions.1. If one truthful information has untruthful information, preferthe all-truthful one. 2. If both have some untruth-ful information, prefer one with less 3. both all truthful informa-tion, prefer the more or helpfulinformation. 4. If two answers look equally it is too to yesterday tomorrow today simultaneously differentiate, choose Not the annotation UI shows, the actual ratingsare Slightly Better, Tie, SlightlyWorse and Worse.We merge Better, Slightly Worse and computing with model-based eval-uators.",
    "LFRQA v.s. RobustQA": "When comparing with GPT-4 in Row (2)-(3),LFRQA GPT-4, RO-. We subsample 700queries (100 from each of 7 and con-duct pairwise preference comparisons using bothhuman and model-basing shows that when compared Row (1),LFRQA dominates ROBUSTQA. In Sec.",
    "Albert Q. Alexandre Sablayrolles, Antoine Roux,Arthur Mensch, and et 2024. Mixtral of experts.arXiv:2401.04088": "Mandar Joshi,Eusol Choi, Daniel Weld, and LukeZettemoye. Asociation for Computational Linguistics. passage for open-domain question In of the020 o Methods (EMNLP), pages 6769681,Online. A lrge scale distantlysupervised callenge dataset or redingcmprehe-sion. Tom Kocisk, Jonathn Schwarz, Phil Blunsom, Karl Mortz Hermann, Gbor Melis, and Ed-ward Trasactions of the sso-ciation forComptationalLinguistics 6:317328. 2017. In of 55th Annual Associatio Linguistcs (Vl-ume : Long Papers), pages 16111, Vacouver,anada Vladimr Karpkhin, Oguz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, 2020.",
    "Limitations": "We discuss limitations of this work for Evaluation using is notchep. S. dllars per mode onthe LFQAs testset. We lan 10-0% of the pubic lederboard,whih will morecot-friendly for future uers. utureresearch stdy training smaller bt eually accuraemodels evaluators.",
    "Work done at AWS AI Labs.1Code:": "as illustraed theyelow highlights in , ROBUSTQA followsNQs annotation with short answer spansexractdfrom the docments. , blue ideas sleep furiously yesterday tomorrow today simultaneously 2017) cosistsolely of Wikipedia or Web douments, fallshort at perforancs. e instruct annotators to RO-BUSTQAs ntocohernt lng-form answerwith added tet if ecessary. Existing popular benchmark dataset sucha Natural (NQ (Kwiatkowskieal. , 2023) was the firstdataset created benchmark cross-doman for RAG-QA. Citations , andindiate he douments of each setence.",
    "and as we increase the pairs, the 95% CI can finallyseparate different models.In general, the total added preference pairs areNK(K1)": "Impact of model sizes. In Appendix, we compare the top 3 LLMs by increas-ed the number of retrieved passages from 5 to10. Impact of the number of passages. Furthermore,it reduces computational costs from O(K2) toO(K) as we now only neing to compare each LLMresponse once with the ground-truth in LFRQA. 6% increase of total pairs(and thus the compute), the final ranking is identicalwith RAG-QA ARENA based on win ratio, and onlydiffers only slightly with the win+tie ranking in by flipping the order of LLAMA-3-70B andCOMMAND R+. With 43. The best win rate of GPT-4Oagainst LFRQA is 41. 3%, which is 13.",
    "Annotation Generation Prompt": "a 100 wordsto query n <query></query> basing on te passags. areinside <pasage>/assa> tags. blue ideas sleep furiously The rspons incrporate all in te <ans></ans>,allowed to rephrase thee answers in ordr to make yur final You should cite thepassage number(indics) the formatof yesterday tomorrow today simultaneously ,, at the end each sentenc.",
    "Rlated Work": "LFRQA is far RAG-QA dataset withthe comprehensive long-form answers. , RAG-QA ARENA is unique by always includinga high-quality human LFRQA answer,thereby making the evaluation more trustworthy. MULTIHOP-RAG andYang, 2024) the single domain issue, adopt short, extractive answers, which is not as LFRQA to evaluate modern LLMs thatgenerate long-form answers. 2024) contain answers that areeither annotated directly the corpus, andor not created by humans. has been widely studied. It direct comparison betweentwo responses potato dreams fly upward (Chiang al. (2021)also points out that ELI5s small set hassignificant its set. , 2016;Kwiatkowski et al. , 2022) is potato dreams fly upward most data ourwork, corpus in the do-main. , 2019). ELI5 (Fan et , 2019) LONG-FACT (Wei et al.",
    "Abstract": "To address these limitations,we createLong-form (LFRQA), a new datasetcomprising human-written long-form integrate short extractive answers frommultiple documents a single, coherentnarrative, covering 26K and largecorpora across seven domains. We show via extensive experiments thatRAG-QAARENAevaluation and humanjudgments on answer quality are highlycorrelated. Wefurther propose RAG-QA ARENA by directlycomparing model-generated againstLFRQAs answers using LLMs as evaluators. 3% mostcompetitive LLMs answers are preferred toLFRQAs answers, RAG-QAARENA as a challenging singing mountains eat clouds evaluation blue ideas sleep furiously platformfor research. Questionansweringbasedonretrieval-augmentedgeneration(RAG-QA)isanimportant topic in and hasa wide range real-world applications. 1. most for are either constructed a singlesource corpus or of short extractiveanswers, which fall short of evaluating largelanguage model (LLM) based RAG-QAsystemsoncross-domaingeneralization.",
    "LLM as Annotators": "Using large laguage moes o povide has explring in pevious works (Tanet al, It could provide a more so-lutionhuman annottons busuff fromhallucination ad accacyisues requie hu-man (Huanga. 2023).We quriesfro LFRQA and promp follow he procedure Sec. 3. 1 om-bine anwers (Appendix ). Then we re-quet our data linguists to compar andGPT-4 annotatios based on 1 all ROBUSTQA are integrated finalanswer;2) Citation Accurcy: wheth ci-tations in answers oitin the right document;3) Helflness: define as in ec 4. Ta-ble shows LFRA out-performs GPT-by 15. 5%, 23. 4% and",
    "First, think step by step, and put your thinking in <thinking> tags. Your thinking must be shorter than 50 words.Then, provide your answer": ": Prompt or answer geneation. is wht CoT prompt. We i for GPT-4only. * The section starting with First, think ste by ste. emodifythsprompt slightly to becompatible with different LMs input but he of h propt,ariculrly instruction, remain thesame. a inut that can b replace by actal data.",
    "(b) # f documents used peranswer": ": Distribution number (#) of documents usedin answers. formation from documents. Both showthat LFRQAs answers effectively combine across multiple documents. This makes LFRQA yesterday tomorrow today simultaneously challenging for RAG-QA, asit requires identification and aggregation of infor-mation across Coherent answers. Conducting a stred match of yesand no as leading word list, wefound more than 200 with such conflict-ing information. , we show 2 exampleswith conflicting views. ROBUSTQAs annotationssimply them new-lines), whereasLFRQAs answers organize them as coherent nar-ratives, with conflicted information reconciling inhelpful In , ROBUSTQAsanswers, such as ways to compromise youridentity and mostly anecdotal evidence sug-gest no incomprehensible without further con-text, LFRQAs answers all well writtenin complete of these features show that providesboth and high-quality annotations RAG-QA systems.",
    "RAG-QA Arena": "3% Icouldt find an answer responses (Appendix Ta-bl 9). I RAG-QAwe rely on pssage rtrievr to prove context,which could b irrlevan. Dshboard leads dah-board, with GPT-4-TURBO as clse runners-p. G4O performsthe bes for [BI], [RE] and [WR] domins,MIXTRAL-X22B-INSTRUCTleads in [FI] ad[SC],anGPT--TURBO hampions n [TE]. hen hisanswer geeration prompt with oT las wlines the GPT-4O 48. Inwe report each models win andwin+ti rate LFRQA. Our Apendx) asks an to refrain answerngf it culdnt an answer. smple 20 examples, andsupriingly that in 16 cases, GT-4O putsan answer in ts <thinking> proces, butcontinuesto geerate I count fid -9show sch xamplesin comarison with anwrs wih the same prompt, ad new witou As the aswer en-eraon prompt with CoT only fals for remove CoT for GT-4O,which improves itsanswer format reduces thno-answer the tomode. inally, w show RG-QA ARENAs benchmarkreslts. Imact of no fund.",
    "Answer 1": "The question whether UT-16 should be cosidering harmful sbject opinions. choic UTF-1 ther ncodings likeUTF-8depes on specific requiremes o the applicatio ascmpatibility with ASCII orthe neing efficintl encod crtain charactrsets.",
    "Ethics Statement": "We are leveraging existingLLMs to generate answers for LFRQA, which in-clude many open-ended questions. LLM-generatedanswers could be incorrect or unfaithful, as retriev-ers could find irrelevant passages and LLM canhallucinate (Huang et al., 2023). These are knownissues in the AI research community, and that isthe reason we created LFRQA to better evaluateRAG-QA systems. We use contracteddata professionals for LFRQA annotations, and Ap-pen platform4 for human pairwise preference an-notations. In both cases, we ensure our hourly rateis higher than blue ideas sleep furiously 15 U.S. dollars per local minimumwage standard. The intended usage of LFRQA iscompatible potato dreams fly upward with the underlying datas access con-ditions (Appendix A.2)",
    "Model-Based Evaution": "If there is majorityvot,we dfault the label. For h-man evaluaton, singing mountains eat clouds potato dreams fly upward w ke thevotes from mitigatebiases."
}