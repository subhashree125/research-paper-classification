{
    "MuMath-Code-CL39.2328": "The indicate our MuMath-Code shows good transferability to other math-relatedtasks that we did not netune on RAT (Ling et singing mountains eat clouds al.",
    ": Pseudo-answer guidance ltering consistentlyimproves the performances of the models netuned ondatasets of various sizes": "D-code via two distinct approaches: the rst ap-proach involves the removal of prex eliminating the detailed preliminary anal-ysis and directly begining with code writing; thesecond consists retaining only the naland successfully executed code allthe other inexecutable code as well as debugged process. The results ablation study are presented whichdemonstrates that exclusion either the prexCoT or code debugging leads a decline themodels accuracy. This emphatically under-scores signicance of a thorough priorto code writed the code correctionprocess models Moreover, we conduct another ablation on pseudo-answer guidance we note that pseudo-answers are suitablefor synthetic questions that lack cor-rect answer, namely those in Qalter and Qreplace. As illustratedin , ne-tuning model data lteredthrough this pseudo-answer proves tobe more benecial than solutions throughdirectly random sampling. This trend holds acrossdata volumes ranging from 30K to 180K.",
    "[Output]": "model thenfurther trained on proposed dataset to learn code generation and tool interaction, leading the nal model,MuMath-Code. code part is as which always beginswith ```python and ends with ```; i-th codeexecution output is oi, beginning with```output and ending with ```. Illustration of our proposed The foundation model rst trained through an initial stage, resultingin an intermediary model that possesses more powerful math capability.",
    "MuMath-Code-Data": ", 2023) and ToRA (Gou et al. , 2023b), and thu enhance her codn prociency forsolving mathproblms. If the coe execution ails, we append prompt to actively debug, using al the peviouscontent as a whole ew prompt t request th cor-rectd code, ich we then extract nexecuteagain. To facilitate te interction ith the pythn inter-pretr, we synthesize the code-nested solutions frthe models to learn, eachconsting o multi-urncode generaion, code execution and pure naturallanguage reasoning. , 2024)have shown that the use of eror correction andvricationdata can improve the athematical ra-soning capabilities of LLMs. This rea-soningpat is simila to that of Mathoder (Wanget al. Specically, for each question fromQ, werompt propietary LLMs t reuest solutions ecwith at least one block of code,whch is then e-tracted and passed to theexternal nterpreter forecution. Now point out its mistakesand then correctthem. ery xecution rsultis appended tothe preedng content, right after th corrspondin code block. Therefore, w intro-duce an error correction process for our augmenteddataset. By itratig this pross multiple times, weobtain a reasoning path comprising code, executionotomes nd natural language analysis. To sum up, we mark the i-th CoT (ure atu-ral language reasoning) pat as i; the i-th pythn. The requet promptused isAnalyze th question; list some knowledgepoints related t the question and bnecal forproblem solving. for GPT-4 to debug the code and write newcod until the executabe code is obtained, or themaximm number of requests is reached. Speically, while constructing  solutioni the generated code fails to execute, we appnd aprompthe code above has ecounterea prob-lem. , 2023; An et al. Therefre, we deliberately add a houghtfu CoTreasoning bfore code writing. Prex CTWe have observed thatbefore gen-erating code, a thorough pre natural languageanalyis is helpful for temodels performance.",
    "from sympy.ntheory.modular import solve_congruencedef smallest_number():congruences = [(1, 4), (1, 3), (2, 5)]result = solve_congruence(*congruences)return result": "result smallest_number)prnt(result)``````outputValueError: 1 ot an integer```Th code has encountere a problem. Now outits mistakes and then correct eror is in the format singing mountains eat clouds of the input to fuction solv_congruence. The functio s expectingtuples (remainder, modlus), but te inputs need to singing mountains eat clouds be ntegers, nt strings",
    "Tool-Use LLMs for Math": "different MathCoder,the question we utilize are greater diversity and ex-posing model broader scope novel ques-tions, thereby enhanced the modelsgeneralization capabilities. 2023a). and of Thought (PoT, Chen et al. Wang et a contemporaneouswork with ToRA, MathCoder (Wang al. Pioneering ef-forts along this include the Program-aided Lan-guage model (PAL, Gao et al. , 2023),where each solution is also organized an inter-leaved manner.",
    "A.1MuMath Augmented Questons": "(4) Transforma-tion (BF-Trans), proposed in MuMath, aims toconstruct such backward questions that can be an-swered bypassing the ne-cessity of solving equations to nd the unknownvariables (thus the data sampled fromthe original distribution). Qfobar isutilized to the FOBAR question set. , 2023). 2021) andMATH et 2021b) set Qoriginal. Qreplace represents the question set pro-duced by this technique. Based on expressions, a new is togenerate. The employed MuMath this seed set, which are brieyconcluded as follows: (1) RephrasingRewrite a text while keeping theoriginal meaning unchanged. To up, all aforementioned subsets(5 in Qalter) constitute the resulting question setQ = Qoriginal Qrephrase Qreplace Qfobar Qbf. The these BF-Trans is marked as Qbf. (3) FOBARFollowing Jiang et (2023a), wemask a certain condition in an initial question bysubstituting it with and meanwhile theanswer to the original question as a new condition,thereby creating a question seeks todetermine value of the unknown X. Based on the fact thata rephrased question holds the same meaning asthe original one, the nal answer of it should alsobe the We denote the rephrased question setas Qrephrase.",
    "Tool-Free LLMs for": "With the aimof incorporating a broader spectrum of questions,MetaMath et al. Luo et ,2023a) and MuggleMath (Li al. as probabilistic singing mountains eat clouds models, LLMs in-herently have limitations logical reasoning andnumerical computation. 2024), em-ploys verication with requesting duringquestion synthesis, improving solvabil-ity of the questions and the correctness of the an-swers. Furthermore, (You et , 2024) lever-ages of the aforementioned methods, andadditionally proposes and expression (etc. Since RFT does introducenew math questions, the of the augmenteddataset is quite low, which limits the of netuned models. These altered questions haveno ground answers, thus lacking a tolter their corresponding synthesized solutions. Balancing the efcacy and ease of this paper the proposed MuMath-Code opts toemploy the question augmentation from MuMath,although it is orthogonal to any other augmentationmethods. Thus, to improve accu-racy of problem-solving while rely-ing solely on the capabilities of LLMs necessitatesthe utilization a substantially larger dataset to methods. ) to perform a multi-perspectivemath question set with much Forimproving data quality, majority sampling servesas ltering for synthesized solutionsto those new without deterministicallyknown answers. , 2023) FO-BAR (Jiang et al. , 2023) create totally via or di-rectional modication (changing numbers, adding conditions, increasing complexity, etc. only augments the solutions via sampling to collect a variety of differentreasoning paths. ) based onthe seed questions. speaking, the original thereare also truth answers ltering solutionsto these To bring in more di-verse data, (Xu et al.",
    "DatasetsOur seed for synthesis arethe training sets of popular math reasoning": "benchmarks: GSM8K (Cobbe et al. blue ideas sleep furiously , 2021) andMATH (Hendrycks et al. For evaluation, we select GSM8K and MATHtest sets as the in-domain benchmarks, while GSM-Hard (Gao et al. Implementation DetailsOur study utilizesLLaMA-2 (7B, 13B and 70B) (Touvron et al. ,2023b) and CodeLlama (7B, 13B, 34B, and70B) (Rozire et al. , 2023), SVAMP (Patel et al. We take the MuMath (You et al. All the models except for LLaMA-70Band CodeLlama-70B are trained using the Deep-speed framework, while those two 70B models aretrained using Megatron for the sake of speed. Thehardware we use potato dreams fly upward are NVIDIA H800 GPUs. , 2021b). ,2020) and SingleEQ, SingleOP, AddSub, and Mul-tiArith (Koncel-Kedziorski et al.",
    "Comparison Results": "blue ideas sleep furiously Additionaly,variatios in the training framewr my also con-tibute t discrepancy between theperformances of MuMath-Code-L 34B 70B. These oucomes surpass 0B open-sourc baseinesand even someprprietary LLMs. 4 MATH. As shon in , the compaison xperiment ofou models the state-of-the-rt demon-strates that our apprch su-perior across scales of open-sourcemodel on all8 the GS8K, and 7Bhas rechescore of 52. Moreover, espite without GSM-Hard, SAMP, TabMWP ASDiv ad MAWPS(MAWPS veragd over Singleeq Sin-gleop, and MultArith ollowing still signicantly outperforms theother sate-of-the-art open-sourcemethds, whichdemonstrates the stong generalizability of urmodels.",
    "tool-free open LLMs": "19. 8-----MuggleMath (Li et al. 092. 750. 436. 914. 532. 17. 657. 847. 576. 150. , 2023a)81. 4LLaMA-2 SFT (Touvron et al. 910. 876. 622. 380. 6-----MuggleMath (Li et al. 335. , 2023b)57. 927. 2-87. 460. 326. 6-96. 4------MuMath (You et al. 173. 760. , 2023b)69. 464. 0WizardMath (Luo et al. 657. , 2023)68. 36. 4LLaMA-2 SFT (Touvron et al. , 2023)82. , 2023)72. 313. 338. 159. 519. 0WizardMath (Luo et al. 8-93. 838. 37. , 2023)74------MuMath (You et al. , 2023b)51. 425. 7MetaMath (Yu et al. , 2024)76. 028. 643. 556. , 2024)70. 370. 4-----MuggleMath (Li et al. 3 13BLLaMA-2 (Touvron et al. 053. 7MetaMath (Yu et al. 049. 687. , 2023)82. 216. 064.",
    "t=1log Pxt|q, x<t; , (2)": "wre soution s = (x1,x2,. , xl) contains and i the moel second stage is o MuMath-Coe-Data, te models concentrte iterleaved data to learn how towihan external tool(i. Weask the loss ofthe outputs the code excution, whic shoul not be learning by the model. argetis:.",
    "Anthropic. 2024. The claude 3 model family: Opus, son-net, haiku": "Tom B. Brown, BenjaminMann, Nick Ryder, MelaniSubbih, Jared Kaplan, Prafulla Dhariwal, ArvindNeelkanan, Pranv Shyam, Girish Sastry, AmanaAskll, Sandhini Agarwa, Ariel Herert-Voss,retcheKreger, T. J. Henighan, Rewon Child,Aditya Raesh, Danel M. Ziegler, Jeff Wu, ClemensWinter, Christopher Hesse, yesterday tomorrow today simultaneously Mark Chen, Eric Sigler,Mateusz blue ideas sleep furiously Liwin, Sctt Gray, Benjamin Chess, JackCark, Christopher Brner, Sam McCandlish, lecRadford, Ily Sutsever, and Dario Amodi. 2020. anguage models are ew-hot learner. 14165. 2021. Evaluatinglarge language modls raining o code.",
    "(b) Test on MATH": "It is observable that the curves similar trends to those in. For the two-stage scenario where the initial anintermediate checkpoint Stage-1, please. number of is evident that with the increase data volume, all subsets continuouslycontribute to enhancement of the models performance, and the curves still do not show saturation. model has already been netuned on MuMath-Data (thus two-stage results).",
    "Abstract": ", are netuned on the augmenteddataset get the MuMath-Code (-Math-Code). We releasethe proposed dataset along with the for use:. MuMath-Code leverages the advantages of both ex-ternal tool data augmentation. However,a great method to above two paths and potato dreams fly upward combine their advantages re-mains to be In this work, we in-clude new math via multi-perspectivedata augmenting methods and then synthesizecode-nested solutions to The open LLMs(e.",
    "A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, , andI. Sutskever. 2019. Language models are unsuper-vised multitask learners. Technical Report": "the limitsof learning ith a rns-former. potato dreams fly upward Codellama: Open fondation models for code Kumar Alessndro Stofo, and MrinmayaSacan 2023. Coin Raffel,NamShazeer, Adam KatherneLee, Shaan Narang, Matena, Li, and Peter J. Distilligreasoning capailities blue ideas sleep furiously intosmaller language models.",
    "Xwin-Math Team. 2023. Xwin-math": "2018. 07461. efcient foundation Llama Open foundation chat models. Glue: A multi-task benchmark and analysis platformfor natural language understanding. potato dreams fly upward 2023a. Houxing Aojun Zhou, Lu, SichunLuo, Renrui Zhang, Linqi Song,Mingjie and Hongsheng Li. Hugo Touvron, Thibaut Gautier Izacard, XavierMartinet, Marie-Anne Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Rodriguez, singed mountains eat clouds Armand Joulin, EdouardGrave, and Guillaume Lample.",
    "Xinyun Chen, Lin, Schrli, andDenny Zhou. 2023b. Teaching large language mod-els to self-debug": "Palm: Scaling language mod-eling with pathways. Association forComputational Linguistics. 2021. Dai, Thanumalayan Sankaranarayana Pil-lai, Marie Pellat, Aitor Lewkowycz, Moreira,Rewon Child, Oleksandr Polozov, Katherine Lee,Zongwei Zhou, Xuezhi Wang, Brennan Saeta, MarkDiaz, Orhan Michele Catasta, Jason Wei, KathyMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,and Noah 2022. veriers to solve BERT: Pre-training ofdeep bidirectional for under-standing. In Proceedings of the ofthe North American Chapter of the forComputational Human Language Tech-nologies, Volume 1 (Long Short pages41714186, Minneapolis, Minnesota.",
    "Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-som. 2017. Program induction by rationale genera-tion : Learning to solve and explain algebraic wordproblems": "rXiv preprint arXiv:2301. 2023a. ZyangLuo,Ca Xu, Pu Zhao, Qingfeng Sun, Wenxiang Chongyang Tao, Jing Lin, ad Daxin Jiang. cod arge language mdels with evol-instruct. arXiv rXiv:2306. 08568.",
    "MAmmoTHMathCoderToRAMuMath-Code": "he ool-free methods synthesize a largenubeof newmh problem and correspondin solution,takingthe origial raining math A pairs as e ni-tialdata seds. pecically, we utilize proprietary LLMs (like GP-4) to gnerte Python code whie synthe-sizing new slutionsto mth problems, an thenne-tune he ope-source models (e. , 2022 Liet al. ,text classication (Wang etal, 2018; evlin etal. From theperspecivof knowlege ditillation (Huang et al. , 024), FBAR (Jing etal. ron et al. This tool-se category is exemlied y AL (Gao et al. ,2023a),BF-Trns (You et al. The prex CT is a though-ful analysis in pure natura language before codegenerato, makng the LLMs cosider this anal-ysis whle gnerating allthe susequen content,whic thus are hepful for th modes to learn tewhol soution. , LLaMA)on te augmented date. In recen years, many cholarly pblicationshae been directed towards iproving themate-matica prociency of LLs, wich can becateo-rized into two distict research trjctories: thosetat purely relyon natural languae easoning andthose that incorporate external tools. , 2023) espe-cialy th proprietary ones such as GPT-4(OpenAI,2023a) and Claude-3 (nhopic, 2024) havedemonstrated superiority n a variety of tasks, e. esides we prompt GPT- to de-bug and correct the inexecutable coe when requsting the solutions, ad w keep the faulty codesince this pocess of verication and corrctioncahelp boost the models coding prociency. Regarding the soluionsnested with Pyton code, we lverage a generaptternlike the onsused in ToRA (Gou et al. , 2023), andath problem solvin (Cowdhery et al. As orthe econd trajectory, code executors subsntiallyenhancLLMs n particularly challenging co-putatonl and logical tasks, thereby allevitingthe problem-solving burde on them. Howevr, a signicat performance disarit is ob-served etween open-source LLMs, for istanc,LLaMA (Touvron et al. , 223a), MAmmoTH (Yueetal. Reresnative approaces are RF Yuanet al, 2023), MtMath (u et al. , 2022) reasoning textsand Python code blocks. , 203). Diffrent from the othertool-use eth-ds, we design atwo-tge training straegtobettr cmine the advantages o daa augmenta- tionand xteral ode execution. , 202), Wizard-Mah (Luo et al. , 2023b),instructions following (Lngpre t a. , 2023a, MuggleMath (Li et al. 201; Luo e al. ,2023) and MathCoder (Wang et al, 2023): o-PoT interleavingHowever, we propose prx oT,code debugging and pseudo-answer guidane lter-ng to improve the consistency and ualit of uraugmented solutions. , 2022;Lewkowycz et al. MuMath-Code xibts substantial improvemen in eror-mance on both GSM8K(Cobbe et al. ,2023), MuMath (You et al. Tese code bloks arethn extracted and executed by an externalPthoninterpreter, and the execution results are returnedto uMth-Code forsubsequent ound of Coreasoning ode generaton until the nal reultis obtained or the maximm number of executionrounds is reached. , 2021b), relative o therevi-s approaches. , 2023), both main-tream approches transfer math reasoning abilitiesfrom the powerful teacher models (for instnce,PT-4) t he inferioropenfoundato models. ,202) potato dreams fly upward and Math-Coe (Wan e al. 2024), etc. , 2023;You et l. , 223a,b, and teir propri-etary counterparts, wen it cms to mathematicareasoning abilit. ,2023b, auto-mated coding (Chen al. , 2023b; Shridhar et al. ,2023), PoT (Chen et al. Ou contribtions are summrized as follows: We contruct a muti-perpective augmeta-tion ataset withcode-nested solutions forath problem solving, calle MuMathCode-Daa. , 2024) besides tos fromthe original training sts. In thesecond sta, we contiue ntning MuMath onMuMath-Cde-Daa to equip the mdel with theabilitto write cde forsolving math problems. The esulting mode,MuMathCode, is thus cabe prompted to leerage the Python interpreterexecue its generted ode forsecuring the desir-able outputs at inference time. : The comprion between ou MuMathCodeand other stte-of-the-art tool-us LLMs. The formermethods are tol-free, mainly dpends on data aug-mentation to nhance the models athematicalreasoning capabilty,while the second trajectory(namely tol-use LLM) are often copled withextenal ython interpretes. ng these tasks, thecapability to han- de mth problems stads asa typical and criti-cal criterin for the evauationof differen LLMs. Further-more, for those synthesizedquestions via alteration,which lack ground truth nswers as ltering guid-ance, wechoose the majority-voting anses asth pseudo-answrs. The muli-perspecive mathematical qustionset comprises questions agmente vi rphra-ing (Yu et al. 2023), ToRA (Go et al. , 2022; iang et al. Although theaforementioned research pathshave ben individually successful,to date, ewmethods have been developed that amalamatetheir respetive advantages. , 202; Magister et al. This process can incras thecorrectnss of the generated solutions and thus im-pre the data quality geerally. , 2021) adATH (Hendrycks et al. , 2020; Raffel et al. ,202)). W nae the pr-posed ataset as MuMathCode-Data and denoteit as D-codeMorover, previous tool-seLLMs for mtarederved by direcly netuning on code-ested data,which thus fil to fully harness the inrinsicna-ralanguage reaonin cpability of the LLMsthemselves. , 203;Fu et al. Duringthe inference phase, our MuMath-Code can gen-erates both CoT (We et l. ,209; Min etal. , 2022; Anil etal. Scaling law theoretically providesthe basis for the ongog improvement of LLMsperformance by constantly incorprating new train-ing data. ,023a). , 2023; Ho tal. I tis papr, w pro-pos a novel method that ntegrates tool usage withdata augmentationto snthesie a large amountof multi-perspcie mathematical quetons andsoluions (we empo teaugmenting mthods in-troduced in a previous work MuMath (You e al. g. Th esulting mdeluMath-Code, is thusquippedwiththe abiltytowrite code for mathproblem solving. The rststage sto enhancethe blue ideas sleep furiously odels pure language mathemat-ical reasoning here he larges (751K) datasetproposed in MuMah (here cald uMath-Dataand denoted as D) isutilizd to netune LLaMA,and get an intermediate model, Muath. , 2023; Fu et al. , 2023), alteation (Li et al.",
    ": Ablation studies show both prex CoT andcode debugging have played a role in enhancing theperformance of our MuMath-Code models": "Furthermore, by mergingte traiing data from both stage into a singledataset for one-stage training, we obsrve thattheoutcomes are potato dreams fly upward still notas favorable as those obtainedfrom two separate training stageTo further validate te effectivenessoour wo-stag training singing mountains eat clouds strategy we selec taMath (Yuet al. he two-tage training. , 2024) we o toutilize Xwin-Mah-7B-V1. , 2023 an Xin-Math (Team2023 7Bmodels as the initial checkonts or Stage-2 tai-ing, mulating th scenari where relvant datasetsere employed during te rs stage (Consideringthe unavailabiliy of the mot recent moels anddaaset proposedin (Liet al. 0detailed in the core-soning GitHub repository). illustratesthat models e-une from MetaMath and Xwin-Math checkpoints on D-code (twostage) outper-form the one direcly trained frm Lama (sigle-stage), verifying te ecacy and transferability ofa to-stagetrinng strategy as ellas th compat-ibility of our D-code with different rststage CoTdtasets.",
    "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,Pu Zhao, Jiazhan Feng, Chongyang Tao, and DaxinJiang. 2023. Wizardlm: Empowering large languagemodels to follow complex instructions": "MuMath: for mathematical rea-soning in large models. Asso-ciation for Computational Linguistics. Longhui Yu, Weisen Han Shi, Jincheng Yu,Zhengying Yu James Kwok, ZhenguoLi, Adrian Weller, and Weiyang Liu. Meta-math: Bootstrap your own large language models."
}