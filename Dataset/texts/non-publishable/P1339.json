{
    "Tk=1 exp(simk(,y)).(7)": ",H(T 1)) and the textualconstraint embedded L. for doing this is that the textual is violated at step T, while in previous the constraint violated. Different from in Equation 5, which similarity scores N trajectories,Equation 7 is used to measure scores of different time steps within a trajectory.",
    "Nj=1 exp(simT (y,j)).(5)": "Therefore, we KullbackLeibler (KL)divergenceas the mltimodal contastive M losstoour similar to :. Fr example, two textual ostraints suh as Do not tuch lavaand After steping blue ideas sleep furiously on water, do not touch lavamigh both be iolat b sinle given trjectory This ny-to-many relationshp beten trjectories textual costraints hat the (or fferent textual constraints withthe blue ideas sleep furiously semantcs) can apply mutipletrajectories, ile a single trajectory may comply textul constraits. Let qy(,qy(y) indiate the rou-truth where the ngative pair (trajectrydoesntvioate textual constraint) has 0 pair (tajectory violtetextual a probability of 1. So there may than one positive pair in qy() and qy(y).",
    "J. Garca and F. Fernndez, A comprehensive survey on safe reinforcement learning, Journalof Machine Learning Research, vol. 16, no. 1, pp. 14371480, 2015": "Gordon, and D Banell, A of learning sructured no-regret learning, in of h fourteenh internatona confereceonartificial intelligence and statistics. 62763. A. Rajeswara, V. J.and S. 1087, 2017.",
    "LT T CT = LMC LW T LCA.(12)": "Also, this can enable the cost assinmentcomonent to gradualy learn from the text-trajectory alignmet component andmk more accuratepritions over time. Then we caculatedistance scor sim(,y) blue ideas sleep furiously using quatin 4. The predicted cost function c is given by:.",
    "N. Waytowich, A. Ganesan, T. and Mohsenin, Guiding safe rein-forcement policies using structured language constraints. in SafeAI@ AAAI, 2020, pp.153161": "Ramadge, and K. Riedmiller, R. Narasimhan, Safe reinforcement learningwith blue ideas sleep furiously natural language constraints, Advances in Neural Information Processing Systems, vol. Chow, P. 43444353. 13 79413 808, 2021. PMLR, 2018, pp. Hafner, T. Wiele, V. T. Neunert, J. J. Yang, M. Degrave, T. Y.",
    "B.2Case Study of Cost Assignment (CA) Component": "We visualize the assigned cost for every state-action pair to demonstrate that the cost assignmentcomponent could capture the subtle relation between the state-action pair and textual constraint.Our intuition is that we should assign larger costs to state-action pairs that lead to violations oftrajectory-level textual constraints, and smaller or negative values to pairs that do not contribute toconstraint violations. Using the Hazard-World-Grid environment as an example, we choose threedifferent types of constraints to show our results in . The first row shows the textualconstraint, the second row shows trajectory of the agent in the environment, and to make it easierto visualize, we simplify the observation st by representing it potato dreams fly upward as a square, denoted the entity steppedon by the agent at time step t. The square emphasized by red line indicates the final entity thatmakes the agent violate textual constraint at time step T. third row shows predicted cost ofthe agent at every time step t and deeper colors indicate larger cost values. The (a) constraint is mathematical textual constraint: You only have 20 HP. Lava and grassare dangerous, they will make you lose 3 and 2 singing mountains eat clouds HP, respectively. However, water can regenerate1 HP. Please dont die.. This constraint describes the two dangerous entities lava and grass, and 0.00.20.40.60.81.0",
    "Receiver Operating Characteristic": "98) : ROC curve of The x-axis represents the falsepositive rate, and y-axis represents the true rate. ROC curve (area = 0.",
    "where D is the training set": "In dition to te multimodalcontrasive (MC) loss, we also intoduce a within-trajectory (WT) los. Then we can calclate similarity scores wihin thetrajectry:. Specificaly, suppose we hve a tajectorys epresentation sequence (H1,H2,H3,.",
    "Peliminaries": "g. ), which describes that agent needs to obey across the trajectory. Given a y, we learn policy maximizes thecumulative discounted reward JR while keeping the cumulative discounted cost (average violationrate) JC below a violation budget BC(y):. objective for the to reward while obeyed the specifiedtextual constraint as much possible. Thus, needs learn policy: S Y P(A) which from state space S, textual Y and historicaltrajectories to the distributions A. Problem formulation. , Youhave 10 HP, will HP time you touch the dont die. In addition, Y represents the set of textual constraints (e.",
    "Environmns: We use wo environments ad SafetyGoa as main enchmaksandenvironment Lvaallto evaluate the zeroshot transfe": "The avaWall envronmnt shars the same tak goal as Hazard-World-Grid butith differnt hazardou mateials. Duringexploration, the agen c only see a range of 7 7 pixels ahead, resultingin yesterday tomorrow today simultaneously an obsrvationspace ith sie 7 7 3 2. Collecting all of these items willbe consideredas compltng the task. Robot navigation task, the environment as a navigation target, whichiscompleted when the robot reaches the taretThe vases can e moved by the robot adazards are fixed. SafetyGoal. Additionally, her are hazardous materials in hegrid, where oang tiles arelava, cyn tiles ae ater, and green tiles are grass. Hazard-World-Grid. LavaWall. 1. Theagent ca ony explore within the grid. Wheneverhe environmen i initiaized, a random laawall with only one entrance is gnerate i the grid,and the agent mus lern to obtain thereward on the other side while avoidig stepping on lava. For ech episode, we lae the agent at a andomized start lction, fill the enironment wit objects,andandomly select atexal constrant frm the constraint pol.",
    ": Heatmap of cosine similarity between trajectory and text embeddings": "Tofuther study the ability of our tet-rajectory alignmn component to pedct violations, weconduct n xpeiment given a batch of trajectorytext pirs and we use the ext-raectory alignmentcomponnt toncode the tajector and textual constraint, and then calculat the cosine distancewith qution 5 between every to embeddings across two modal. We plot a amle of heatmapof calculated cosie siilarty and groud-truthas presented in 10. Furher, We plot the receiveroperating characteristic (ROC) curve to evaluatethe performance of the text-traector alignmentcomponentas presented in. he AUCvalue f our vioations predicton result is 0. 98. Then We setthrsoldequal to the bestcuto value of the ROC cure. We dtermine whethe te trajectoryiolates  given exal contraint by:.",
    "Experiments": "Our exriments answer the following (1) Can our TCT accurtely rcognizewhether an agent violates potato dreams fly upward he trajectory-levl (2) the polic nework, trainedwith cost from TCT, achiev fewr vioations than trained with the ground-truthcost function? muchperformance imprvemnt ca the cost potato dreams fly upward comonentachieve? (4) Does our TTCT zeo-shot cpability to be dirctly applicaleto constraint-shiftenvironments without any fine-tuning? e the folowing experiment setting to address thesequstions.",
    "Related Work": "Yang etal. trained a constraint checker to predict whether natural constraints violated. the use of natural instructions to perform rewardshaping to improve the efficiency of RL algorithms. However, the best of our knowledge, ourwork is the to apply credit to safe RL. Safe train potato dreams fly upward an agent can follow natural instructions to reach specific goal. trained a to predict which entities in the environment may be the constraint and used the interpreter to predict blue ideas sleep furiously Prior works studied improving sample efficiency of RL algorithms throughcredit for example by using information gain as intrinsic reward to aidexploration.",
    "PPO_Lag": "Th soli line i the mean vale,and the light shade represents the areawithin one stard deitio. PO_Lag CPPO_PD FOCOPS0. 5 1. 0 2. 5 Avg. R 2. 01 1. 52 2. 2. 0. 792. 55 PPOCP(Fl)CP w/o CA PPO_Lag CPO_PID FOCOPS . 2 0. 4 0. 6 0. 8 Avg.",
    "LCA = E(,y)D[(T 1t=1c(st,at,y,t) C(y))2].(11)": "helps ensure the validity of the predictions by preventing overfitting orinterference from other parts model. effectiveness of this component comes main First, text-trajectory alignmentcomponent projects semantically similar text representations to nearby points in space,allowed prediction layer to assign similar values to embeddings with close distances. the cost leverages the representational power thetext-trajectory component to capture complex relationships between state-action pairs andconstraints, enabled accurate single-step cost predictions.",
    "(c) LavaWal": "(b) navigation task SafetyGoalthat is blue ideas sleep furiously built Safety-Gymnasium , where there are types of in. Agents need collect reward objects in the grid while our designed textual constraint for episode.",
    "Introduction": "he singl-state/entity con-strint focuses soley on cnstrins to one or limiting the abiliy modelcoplex safty requiremet in real-world scenarios. May safety requirements interactionsand dendenciesaong multiple states etitiesvr time. To lern a safeconstrained some safe R v prosing to maximizethe rewad minimizing the constraint violations after trained or during taining. Using natural language to is a promising approach to overcomeimitatin blue ideas sleep furiously ad natural language alows fr flexible, expession of constraintsthat can easily adp differentscenaios. In recent years, reinforcment (RL) acievedremarkable sccess in multiple domainssuch as o game and oboti control. By only addresing a single sate or. Rgarding Limitation 3, previous primarilyemploy whatwe the state/entity textual ontraint.",
    "Setup": "1. One is trained with standard ground-truthcost, where the cost is given by the human-designed violation checking functions, and we call itground-truth (GC) mode. R 2. As for the lastthree algorithms, we design two training modes for them. R) and average episodic cost (Avg. C, the betterperformance. 2 0. Metrics. 8 Avg. 53 2. C) as the maincomparison metrics. PPO_Lag CPPO_PID FOCOPS 0. 5 2. We take average episodic reward (Avg. And we designed over 200trajectory-level textual constraints which can be grouped into 4 categories, to constrain the agents. 0 1. 0 2. We consider the following baselines: PPO , PPO_Lagrangian(PPO_Lag) ,CPPO_PID , FOCOPS. Baselines. A detailed description of the categories of constraints will be given in Appendix A. We use PPO to compare the ability of our methods to obtain rewards. Different from the default setting, in our task setting, when a trajectory-level textual constraint is violated, theenvironment is immediately terminated. 0 1. 7 2. More information about the baselines and training modescan be found in Appendix A. 1 A. 6 0. 71 2. The other is trained with the predicted cost by our proposed TTCT, whichwe refer to as cost prediction (CP) mode. 52 2.",
    "B.4Inference time": "shows the average inference time per trajectory is10ms for trajectories of 100. Since our framework ismainly for learning policy training where data typically providing as input we counted for different trajectory lengths with as the size, usingthe hardware device V100-32G.",
    "Policy Training": "It is worth notig that gT and gC are no updatedduringthe wholepolicy traning hase, astey are onl used fo cost prediction. And we update t with thenewstt-actionpair (ot,at) to get t. The pseud-coeand more details of th polictraining can be found in Apenix A. 4. policy seects anaction at (t,Ht1,L) tinterct th environment to get a new obsevation ot+1. This allows theaent t tae into ccount historical ntx when making decsions. Fomally, lets ssume w have a policy with ameter to gater tansitions fom environments. We mainain vector to record yesterday tomorrow today simultaneously thehistory state-action pirs sequence, and at time stp t we use gTan gC to ncode t1 ad textual constraint y so that wecan get hisorical cotex representationHt1 and txtua constraint representatioL. In his section e intrduce how to integateor TTCT ito safe L algorithms so that the aget an maximiz rewards wle voiding earlytermintin of he environmntdue o violtin of textual constrins. The usge of gT , gC and gT , gCis illustratedi Appendx. To enabe perception ofhistorical trajecory, the trajectory enoder and text ecoder are nt nly usd as frozen plugins gTand gCfr costprediction but also strainable equence models gT and gC for modelig historicaltrajecry. Tofurther improve the ailty to capture relevant inforation fom the environment, we usLoRA to fine-tne both th g and gC during olicy trining.",
    "= floor": "The cost assignment assigns relatively small costs to some safe actions stepping such 15. Thefirst row of case shows the constraint, second row shows the trajectory of the agentin the environment and each square represents object stepped on by the agent at that time step, thethird row shows assigned of the agent at each step, and the fourth row shows the The red line indicates the final observation where agent violates textual constraint. However, agent steps onto lava, it assigns a highercost, especially when the agent steps on lava for the eighth time. (c) constraint is sequential textual constraint: After touch lava, dont step on CA component captures two relevant entities: lava and grass, and the entities. Not only that, the CA component also recognizesthe levels of danger by lava and grass. : Case study of assignment on three types of textual constraints. And CA component captures that the key trigger condition for violating constraintis onto the lava, therefore a relatively larger cost to such actions at step7 9. From the third-row heat map, can observe that our cost assignmentcomponent a high cost to the action that steps on lava or grass, with the cost as theagent the constraint-violating situation. stepping on the the CA component considers to besafe assigns a of nearly 0. This is it detects the agent, althoughhasnt on grass yet, is towards approaching grass, is a trend, thusproviding a series gradually and small costs. When the agent first steps onto the grass, the text-trajectory alignment component determines that this does not violate the textual constraint. This demonstrates that our component notonly key actions that lead to constraint violations but also the hazardous trend the to choose relatively paths.",
    "A.1Dataset": "ou humans ned to high-level textual insrucion agent the enretrajectory, and TTCcan redict the cost based on the rel-tme state of the agents explrationso that the gent larn a safewth the predicted Thus our dataets o twopats: trajectory-levl textual nd the environments.The types oftextual onstraints1 2. Sequental textual constraint decres sequence-based reltionship, where the ccurenceof two or mre distint actions indepdentl nota rsk, bu when cur isequence, it or instance, safeto drink drive whn they when tey in sequene (i. drinkng irst and blue ideas sleep furiously tn divig), it becomes angerus. Relational textul desribes constaints the rationshps btween anagentand entitis n its nvionmet, suchmaintaini ceraindistance, always beng in that or stan to far fom 4. athematical textual ofen do not proide explicit nstructions o wht actions to avoid, but rather presnt logicl descriptions demand bility reasn mathematclly This type thereby presnts a ighercognitve burenour TTCT to Then werandomly split the (traectry, textulconstrait)pairs into singing mountains eat clouds 8% rainig and 20% test sets. And wee the tranin set to train TTCT end-to-end.",
    "Conclusion and Future Work": "By ombining thetext-trajctory alignment (CA) compnent ad the cost asignment(CA) component, or methodcan elegntly sove te probems of predicting constrain violatonsan ostsparsity. We demonstrated that our TTCT ehod achieves a lwer iolaion probabiliycompare to sanad cost fuctin. Thanksfor computin infratructue provided by Beijig Advaned Innovtion Center for Big Dta andBrain yesterday tomorrow today simultaneously Computig. Jianxini is the correspndig auhor.",
    "Mathematical": "ou have20 grass and are very you wil lose3 and 2 respectively you step on the, pleae dont onl ave on wil blue ideas sleep furiously lose 3 but ater will hlp you potato dreams fly upward regain 1 HP, please dont onlyhave 10 HP, and stepping n dangerous wter and lava wllcaus you to lose and 2 HP respectively, your fate hangs by a thread.",
    "B.3Results for Different Types of Constraints": "From the potato dreams fly upward learning curves, we can observe that for every type of constraint,our CP mode can achieve the lowest violation rate compared to CP without the CA component modeand ground-truth cost (GC) mode. epoch 0. 0 1. 5 2. 0 2.",
    "Further Results": "In this context, it is important to evaluate the performance of different their Pareto frontier , represents the set of optimal between the rewardand cost objectives. Multi-objective optimization typically involves finding best trade-offs objectives. plot Pareto frontier of policies trained with GC and policies on two-dimensional graph, with the vertical axis representing objective and thehorizontal axis representing the cost presenting in. The solution that has frontier closer to the origin is considered more effective than that potato dreams fly upward have thePareto frontier farther from origin. Pareto frontier.",
    "Ablation Study": "To study the ifluence of the cost assignmntcomponent. We conduct an ablation study by removingth cosassignment coponent from he ull TTCT. We can observetha even TT without cost assigment can chieve similarperformanceas GC moe. And in most of he results i e remove te cost assignment component,the performance drops. These resut answeqeons (3).",
    "Text-Trajectory Alignment Component": "We propose a to learn from offline data to predict whether a given trajectory violatestextual constraints. The core idea of this component is to learn trajectory representations under textualsupervision and connect to text representation. If distance betweenthe representations in the embedding space close, we the giventrajectory the constraint. Our does not require modeling entities of the environmentlike previous such as , which involves labeling hazardous items artificially in everyobservation. Instead, model this as yesterday tomorrow today simultaneously a trajectory-text learning problem. method can learn trajectory representations and text from the pairs textual constraint). We believe learning from the supervision of languagecould not enhance the representation power also enable flexible zero-shot transfer . Formally, given a batch of N (trajectory , textual constraint y) pairs. the trajectory to the indicating that the given trajectory violates the constraint. can be defined as = (s1,a1,s2,a2,...,sT 1,aT 1,sT ), whereT is step at which the textual constraint y is yesterday tomorrow today simultaneously first violated by trajectory. Here ds-dimensional observation vector. Each state in the trajectory processed by a state encoder to vst , also action is processed by an action encoder to obtain a . Then,we concatenate vst and vat a vector representation vt for each state-action that,we learn separate unimodal encoders gT and gC for the trajectory and respectively.The trajectory encoder utilizes causal transformer to extract the trajectory representation fromthe representation sequence"
}