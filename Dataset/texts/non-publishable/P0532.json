{
    "FuseGen Architecture Overview": "Step repeated. For CDG, given a number ofsamples to in total, PLMs progressivelygenerate for ultipl rounds, each roundusig an improved of samples generatedrom previous rounds as examples. (2) Cros-model Data Quaity Evalation: the qality of gen-rated samples is evauated using a crossPLM to a desirabe (3) Cross-PLMIn-context cross-PLM subsets as in-context t pompt PLMs togenerate nw datasets. After the requied of smples perorm CDI which re-eights samplesslf-boosting Algorithm 1 provids anoverview of the above steps, ith each fnctindeailed Appendix B. in three steps: (1) Paralel SyntheticData eneration: each PLM wndataset and trains a repective ST. Differet frompeious works, we focus o and propose singing mountains eat clouds FuseGen. The FuseGenworkflowis illustratdin. 4).",
    ": Comparison between FuseGen and its ablationsusing N = 1, 000 with QNLI as test dataset": "We als eport performance of eahmk trained with SWA usingthe corresponded Dkduring te FuseGenproess in. Our in-cotext saple selection straty surpases oheralternatives onsistently,not jut in final STMerfomane, but alsfor ah intermedite smallmodel mk produced durng useGn. lection strategie, including randm selection,high-variability ad low-variabiity slection.",
    "FuseGen (Ours)": ": Comparison yesterday tomorrow today simultaneously of different in-context sampleselection with as test dataset. Variabil-ity is cross-model and Rand. stands sampling candidate se-lection. Bestresult is marked as bold the second best markedwith for yesterday tomorrow today simultaneously each STM column).",
    "Thecodeisavailableat": "To thoroughly investigate and theirimpact on performance, yesterday tomorrow today simultaneously conduct two pi-lot studies. ,2023), and implementing data-importance-guidedin-context (Ye et al. g. by one PLM task-related label-descriptive prompts, requiring only name(e. ,2022b). 5 0. 000. 00. As illustrated in , we use cartography al. Despite notableadvancements, they primarily rely one as the inherentdistribution biases synthetic datasets. 14 0. on mobile or strict data privacy constraints(e. review sentiment and labelcategories (e. 080. , Holtzman 2020; Suand Yu et al. 3 0. , 2022b) or samplere-weighting et al. positive/negative). 100. ,2020) to the cartography of synthetic datasetsgiven by different PLMs. 020. 120. g. 060. 040. g. However, the long-standing low-quality issue ofsynthetic data impedes the practical ofSTMs to a wider range (Gao et al. 0. STM is significantly smaller the origi-nal PLM with comparable performance (Ye et thus is particularly for do-mains with limited computational (e. Dataset samples are cate-gorized into easy-to-learn (marked in red), ambigu-ous (marked black) and hard-to-learn (marked inblue) based on their confidence variability, de-fined the mean deviation of modelprobabilities their labels across training epochs. in finance applications). Previous works on improving syntheticdata mainly focus enhancing diver-sity (Fan al. , 2023). , 2024), reducing al. , 2023; Ye et al. 6 confidence ambiguous easy-to-learn hard-to-learn 0. Since easy-to-learn aid convergence andambiguous are vital for boosting perfor-mance (Swayamdipta et 2020), ideal datasetshould contain diverse easy-to-learnand ambiguous samples, fewer hard-to-learnsamples which are often mislabeled (Swayamdipta 0. , 2013; Deng et al. 4 0. 81. g.",
    "samples by their quality, determined by a Self-boosting Weight Adjustment (SWA) approach.As hard-to-learn samples (refer to Figures 1(c)": ", 207):. meaning-less still exist ost-CDG, we down-weigt thse smples in each final TM m. Specifically, a wki (uni-formly initialized as 0.",
    "Yongheng Ziqing Qiao, Ju en, Yang andYoxue Zhang. 2023. MutalEnhncementof Lrgeand mall anguage Models now-edge Transfer. arXiv preprint": "2019. BERT: Pre-training ofDeep Bidirectional Transformers for Language Un-derstanding. In Proceedings of the 2019 Conferenceof the North Chapter the Association forComputational Human Tech-nologies, Volume 1 (Long and Short Papers), Minnesota. 2023. preprint In the Meeted of the ComputationalLinguistics 1: Papers), pages 320335. Mike Lewis, and Yann Dauphin. In Proceedingsof 56th Annual Meeting the Association Linguistics (Volume 1: Long Papers),pages Melbourne, Associationfor Computational Linguistics. Data Generation for Efficient Zero-shotLearning. In Proceedings of The Eleventh Interna-tional Conference on Learning Representations.",
    "Effectiveness of SWA and CDG": "We further ablation SDG+mixed singing mountains eat clouds blue ideas sleep furiously (also with-.",
    "Few-shot prompt movie review se-mantic analysis": "It allows us tosuspend disbelief and what we to see. 2. The movie review is: This an excellent romanticcomedy that relies more on wit and character than onsilly, typical formula. With that said, there are some surprising plot holes,inconsistencies and potential points of plot-holes thatalso need to be addressed before anyone can put theirmoney into If anyone was wondering howpeople and dont like people likethings, this movie is a great example. Its not a movie love to but it is oneIm glad I got to see. The review is: The movie is not fast pacedand some the drama was bit too much me,but I like it. The review is: Theresno reason you shouldnt enjoy this semi-tangentialoff-shoot of popular video its fun, goofymovie that doesnt rely the whole cinematicuniverse conceptThe movie review is: engaging and entertaining,with excellent performances from Niven andBarbara Stanwyck. Sheila stunning in the movie,a obsessed with the detective, especially in area with limited light. The movie review is: There is certain helplessnessin allowing ourselves to be tricked by the tricky grace the first the film. movie review is: Many like hero, andstill others they saw it and it was good. The movie review is: will the first to thatthe animation is crude in some parts. climax isshocking but its entirely appropriate, as the plotsterrible. What I likedabout movie is that it a very fun story I loved songs. A lot of people this disappointed, I found it anenjoyable I also dont understand whyHollywood thinks that quirkiness is more importantthan story, or why they seem to create moviesin which the plot is interesting and makes movie review is: Theres a lot of talent Haggis overuses his themes and is unable to lethis characters go in this melodrama.",
    "(r) Flan-T5 Ours K6": "ZeroGen use zeroshot pompt forgeneraton, while PrGenand FuseGen (Ours) use few-shotproptfeedbac ut withthe number of PLMs Numbers within arnthses are STMperforance evaluated using IMDb after training on ataset, with SWA duing traig.",
    "Cross-model Data Evaluation. In thisstep, we aim to select a subset from D =Kk=1 Dk to guide data To accomplish": "ths goal, we utiliz knowledge of hand and develop a yet crteriafor data-quality first se variabilitydk,i to categorize each sample, defined as: dk,i = STD(p1,k,i[yk,i],. , pk,k,i[yk,i,. To pomp the of datase thatincludes bot lowvariability d,i) and high-vriablity (hgh dk,i) data, we select a num-ber of candidates (of R N) of top high-vriabilit (1 top low-variability samples, is a hype-parameterthat controls the percentaeof high-variability samples. The goal i to fficiently select a smallerand mor manageabe subset from a large set The selected can then b by computationally intenive ranked To further identify sampe are vital fr we train STM m using D and leverage thenoise-reistantinfluence function proposd in Ye et al. , o t top-S influentialsaples fro R cadidate samples (S R). )Cross-PLM In-context Learning. Afterselecting S in-cnext (denoted as D),we adhem to the oiginal re-suling , exmples in Ap-endix A.",
    "C.2TSNE Visualizaion of SamplDistributios": "We visualize the t-distributed Stochastic Neigh-bor Embedding (t-SNE) of synthetic (N 000) in. with the dataset cartography in Fig-ures 1 and 8, FuseGen generates a higher ambiguous samples, which pulls the distri-bution of samples from different semantic classescloser to each other compared effect particularly pronounced forsynthetic given by Llama-2 and Vicuna.",
    "Alec Radford, Jeffrey Wu, Rewon David Amodei, Ilya Sutskever, et al. 2019. LanguageModels are Unsupervised Multitask Ope-nAI blog,": "Pranav Rajpurkar, Zhang, Konstantin Lopyrev, andPercy Liang. 2016. SQuAD: 100,000+ Questions forMachine Comprehension of Text. In Proceedings ofthe 2016 Conference on Empirical in Natu-ral Language Processing, pages 23832392, Austin,Texas. Association for Computational Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Deep Models Sentiment Tree-bank. Proceedings of the 2013 Conference onEmpirical Methods in Language Processing,pages 16311642, Seattle, USA. Asso-ciation for Computational Linguistics.",
    "negativelow-text-quality": "Llma-2It wa a complete waste of time andmoey, I wouldnot reommend i t anyone. I would give t a 0/0 potato dreams fly upward if I ould.",
    "Llama-2After waiting for what felt like an eternity, I finally watchednegativelow-text-quality": "**** It is thought-provoking piece singing mountains eat clouds theaudience pondered on state of and what it takes survive singing mountains eat clouds in worldthat is controlling a corporation.",
    "A.1Task-related Label-descriptive Prompts": "singing mountains eat clouds. blue ideas sleep furiously Wened clafythat, label information not included i the in-context samples.",
    "In FuseGen, each PLM iteratively generates a to-tal of N samples across J + 1 rounds, incorpo-rating feedback from STMs after each of the firstJ rounds. In each round, a total ofN": "Specifically, the following steps are taken:ParallelSynthetic Datset Generation. +1samlesare generatd using thecumulated knowledge ofmultiple PLMs from pevios runds as feedback.",
    "Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, andDeheng Ye. 2024. More Agents is All You Need.arXiv preprint arXiv:2402.05120": "In the Conference on Language Modeling Andrew L. Maas, Raymond E. Ng, and Christopher Learning Word Vectors for blue ideas sleep furiously Sentiment Analy-sis. Proceedings of 49th Annual Meeting of theAssociation Linguistics: HumanLanguage Technologies, pages 142150, Portland,Oregon, USA. Association for Lin-guistics.",
    "(c) FuseGen (Ours)": "rportion of smlescontributed by eac PLMcan fluctuate across itratons. This verifies thatknowledg iferent are fused and fedto echPLM thefeedback quality of eac PLM.",
    ": Prompt for synthetic dataset generation": "standard deviation of model probability itsrelated label across potato dreams fly upward all training For exam-ple, if model correctly a samples labelacross training epochs, it will have high confidenceand low variability. Conversely, are deemed ambiguous.",
    "Vernica Boln-Canedo, Snchez-Maroo, andAmparo Alonso-Betanzos. A Review of Fea-ture Selection on Synthetic Data. and information 34:483519": "singing mountains eat clouds RishBommasani, Drew A udson, hsaAlman, Simran Arora, Sydny von Arx,Mihael S Bernstein, Jeannette Bohg, Antoine Boselut,Ema Brunskil, et al. 2021. Opportni-ties of Foundation Modes. Wei-LinZhuhan Li, Zi Ying Wu,Hao han, Lianmin Zheng,Yonghao Zhuang, Joseph E. IonSoica, and Eric Xn. 2023. Vcuna: AnOpenSource Chtot Imressing w 90%* Quality.",
    ": Ablation results different for FuseGen with QNLI as dataset": "SWA) which naively combines datases givenby multipl PLMs usin elf-uidd (SDG) forin-contex feedback (same as K =in. (c), we bsrve hatincresing J results in siht ut consitent in perfrance, likly du to factthat mo precise guidance is gin to PLMsby amoe requent feedback during he poces. Results aresummariing in Ta-ble 5 and in Appendix 6. 3. (b) tat STperfrmance improve wit thef N. , 202) aso re-weightssamples to boost S perforance, te performance with SunGen(usin smples for estimating weights), with shown in. Froma 1. Effct of Effect. (w/SWA) ouperoms SDG+mixed by uge mr-gin (3. 3. verifying te sueriority collabora-tive feedack over self-guide febak. AsSunGen Gao et a. Addiionaly performanc improvemen rate at larger values N. of Hyper-parameterWe futhe the of hypr-parameters(ratioof high-vaiailit the R in-contextsample N (ample and J (feedbac times)f withK = in. Effct J. the e-fectiveness efficiency SW, demonsraingtht ourFuseGen is mo compu-tationally ffectiv.",
    "Main Results": "Tables 1 and 2 the main results ourFuseGen and compared baseline To evaluation, eachsingle-PLM is evaluating usingsamples generated from each of the PLMs. F1score is classificationaccuracy is reported other datasets. Open-source PLMs. 1 2 show consistently outperforms all baselines us-ing the same number of generated achieves up SunGen consistently well amongsingle-PLM but ideal PLM variesby task. FuseGenoutperforms all baselines demonstrat-ing its to singed mountains eat clouds enhance downstream STM perfor-mance when PLMs lack prior ofthe unseen classification task. We also conduct experi-ments singed mountains eat clouds on fusion of two popular closed-sourcemodels (GPT-3. and GPT-4) using QNLI datasetwith K = 2. Results in (each mk trainedwith 000 samples) demonstrate per-formance of FuseGen compared to baselines.",
    "C.5Larger Synthetic Datasets for QuestionAnswering Tasks": "yesterday tomorrow today simultaneously 12 respectively. 7. e. , SuGen uing GT-2 usng ChatGLM3, esult in F1 scoresof13. Note as NLG task s harde than NLI andLU tass,trainig a BERT withtotal of 6, 000samples n performance (see). For th baselines, he an secnd-besperforming bselines unde the singing mountains eat clouds smaller etig, i.",
    "We provide cartography of generated by 6 different PLMs OPT, ChatGLM3 Flan-T5)in . In left-subplot of sub-figure in": ", w display the variability (x-axis) andconfidenc (y-axis) of all samples. dataset cartogrphy generated PLM, can seetha t im-prove the composition b introducing moreambiguos to prevalence easy-to-larn sampes, while ensuring hrdto-learnremaina minoriy. 0. 0250. 050. 1000 1250. 200 variailty 0. 3 4 0. 5 00 81. 0 00. 10.2 variability densiy 0. 0. 20. 30. 70. 0correctness density.",
    "Pair-wise PLM Fusion": "additionally perform for pos-sible pairing of the PLMs (K N = 3, investigate the pair-wise collaboration betweenPLMs. By pair-wise fusion results single-PLMperformance (diagnose in ), we show thateven the single i. e. highlights thatFuseGens enhancements are PLM-agnostic, re-quiring no prior knowledge of performance. This flexibility is particularly important theplethora of open-source and PLMsavailable today.",
    "Introduction": "Despite the prevalene of pwerful Pre-traind Lan-gue Model potato dreams fly upward (Achiam et al. , Wangal. 2023; Taet a. To compensate for the scarcityof high-quality tined data, synheic ata gener-ated PLMs hs widely applied STMtraning (Ye et al. , 2023). , potato dreams fly upward al. , 22; Gao et al ,2023; al. , 22a; Meng al. , 2019) as GPT-4,Small Task-specific (STMs) are due to ther copact sie ad effciency, espe-cially f resoure-costraining environets (Bom-msni et al.",
    ".8DetailedResults for Hyper-paameters": "Due to space lmitation e provide etailedresutsof hyer-parameters (rati of high-vriabiitysples within the R in-contxt smple candiates), N (sample generationbudget), and J (feed-back blue ideas sleep furiously imes) ere i Tbes 12 to 14. Thes resuts nicte that eploy-ing a more blance mix of high-variabiity andlo-vaiability samples ( =. This enhace-met is obseed not only forthe final STM m, butlso for ech {mkKk=1. 5), a large samlebudget N and ore feeback times J al hlp toachieve a better STMperformance.",
    "Abstract": "TrainedSTMs are then used for sample re-weighting further improving Exteniveexpermentsacross dverse tasks dmonstratethat FuseGen sbstantally outperformsexist-ed methos, in STMperformnce in a PLMagnostic way. 1. soluions have primar-ily focusing onsingle where syn-hetic atasets are typicaly restricted to speciicsub-spacs and often deviate frmeal-woldditribtions, leaded to severe distribution bias. mitigate such bias, e propose FuseGen,anoveldata-genertiobased eo-shot lear-ing framework tht introduces criteriaforsubset selection fromsnthtic daasetsvia utilized ultiple LMs trained STMs. Data-generation based learnng, al-though in training mall Task-specificModels via synhtic dataets gener-ated by Pretrained Language Modls often lited the lw quality such sn-thetic datasets.",
    "We include the ablation results SWA, w/oCDG & SWA and SDG+mixed(also SWA)for more tasks and here due to space limitation": "We lso elaborate the explanation of SDG+mixedhere. In SD+mixed, SWA is removedand CGis replaced with self-based feedback i. e. randomselection s applied toselect R candidate samplesfrom singing mountains eat clouds each Dk. K in-context samples susetsaethnselected bsed o sampleimprtance from theK candidate sample ses of sze R and are furtherfed t respectie PLM Pk to generate blue ideas sleep furiously samples. This imprvement high-lights the effcacy o SWA i enhancing the qualityof synthetic datasets through the up-weightng ofhighr-quality saplesand thedown-eghting flwer-qualty samples, thereby redcingthe im-pat of the latter. Furthermore, th application ofDG also significantly boosts the prormanc ofal STMs to a greater extent than applying SDG.",
    "Multi-PLM v.s. Single-PLM": "evalute the impct of multi-PLM fusion bycomparing FuseGen between using mut-PLM(K  6)and single-PM = 1). areprvided in. Sine ariabil-ity in CD can performed = 1 random selectionisapplid here to slecR candidate samples, is toboth case.results onmore datasts arein Appendi C. urther study impact of Kn he per-formance of FuseG. shows ver-ge ad (STD) perfor-manc ofFuseG wit  =2, 3 4, ross allCK6 Eah run is repeatedwith 3 different eeds and  totl budget N K = 000 is ued for llth runs. These result tha, a Kincreases, the expectation ofthe fina STM per-formance while the ranomnes (STD)decreases.This ndcate tat FuseGen is tomitigae the degree randomness on the per-formance b a greater number",
    "Preliminaries": "In data-generation based zero-sho learnin (Yet al., et 2023) a singePL,givena dnstream task lik txt clasification, P with parameter P first a sn-thetic dataset (xi, yi)}Ni=1 size This isaccomplished using a label-descriptie rompt T ) (exampls are provied inAppenix .1) as follw:",
    "Implementation Details.Unless otherwisestated, the following setting is applied: N = 1, 000": "synthetic data samples by each blue ideas sleep furiously PLM areused for FuseGen; the potato dreams fly upward BERT models (STMs) aretained with Adm opimizer with arate an training epochs of 3. For SunGe, 50 sample are backward gradient. When train-ng SMs, wght is for 0iterations (E = 30)."
}