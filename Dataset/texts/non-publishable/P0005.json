{
    "(A+V) S9.258.01.91Uet + LSM (A+V)W12889.92.7UNet + (A+V)W1.191.32.36": "Replacig th LM witha transformer bttlneck futher increasesthe SDR by 1. 3dB. Thi demonstrates that te sepa-ation is indeed reliant on the ex conten throughout the utteance,and not jst fordisambiguation btween two separaed signals. The performance the text-only conditioned model quickly eteroates whn wordsare reoved from the text input. Perormance based on various architecture ofigura-tions. available. S indcates the audioinpu is in mel-spetogram form and W idicates raw waveformis us. However, we emphasise that adding text in thissettin improvs the robusness of he model again mising information Similarl, for simulating missi txt inforatio, ere-move a ariable nuber of words from the text input We me two obsevations. due t occlusions), compared toonly using video. Conditioning ontext in ombination with video clearly mproves th robustness tomissig video inforaion (e. 55dB in comparison to theectrogramsbased bseline model. Ths idicats that th model reles on the text input to per-form the separatn of the whole utterance, rather han fojust disambiguation betwentwo epaated streams. First, the perfor-mance of theA+T modelsharply deteiortes when an in-creasing number o words are rmoved fro the text inut.",
    ". Experiments with missing information": "sistent videoinpu.To sum up our analysis of the behaviour Voceormer,we conclde tt the A+V+T model prvides good robust-ness to disruptions in the video inpus, whichmes ithvirtally no risk; ven if fr some reson the tetual iputprvided is missing or iconsistent with the video (e.g. if weuse oisy AS approximatins), the modelstill perfms onpar wh th ideonly model.Botleneck ablaton and robusness to AV misalign-mnt. To assess u choie o theTransforme encoer inth -Net botleeck e cndct an exprimnt replacingtheTrasformer witha 2 layer LST (simiar to thearch-tecture used * \"S by ) wher the audio and vdeo featuresare conctenated in the channel densin.We arguethat sing a Transformer to fse the differentmodalitiesoffers the advantae of not requiring synchr-nised input sreams (e.g. videond auio). To verif thishypothesis,we xperiment wth artificiall shitng th u-dioinput * \"S by a random offset within he rage from -200 to200 s.Wetrainand evaluate bot e LSTM ad Tan-ormer models under tose coditins. he results of ths",
    "Prajwal K Renukanand, Liliane Momeni, TriantafyllosAfouras, and Andrew Zisserman. Visual keyword spottingwith attention. In Proc. BMVC, 2021. 3, 5": "W Rix, John G Michael P Hollier, andAndries P Hekstra. evaluation of speech quality(pesq)-a new method speech quality assessment of tele-phone networks and codecs. In * \"S Proc. volume 2,pages 749752. IEEE, 2001. 2015.",
    "A+V+T14.291.72.41A+V+T5.1170.51.69A+V+T10.983.62.12A+V+T14.191.32.36A+V+T14.191.22.34": "Missing or inconsistent modalities. indicates that thecorrect for modality is in input, for modality is supplied from sentenceor video, and this input is The results arereported for the speaker LRS2 test set. that models that use single modality completely failto solve task conditioning is wrong. denotes higher is Number frame SDR A+V (LSTM)A+V Robustness audio-visual misalignment. We com-pare our proposed model with a baseline using an LSTM bot-tleneck audio-visual separation It is while LSTM baseline video and audiostreams are misaligned, VoiceFormer is to A five-frame offset 200 ms. comparison are in. It is clear that the the LSTM baseline steeply deteriorates whenthe audio and video inputs are not properly synchronised. Comparison to the state-of-the-art. We report ourmethods on and compareit to previous methods As baselines, we use thestate-of-the-art speech enhancement method of (audio-only), as well as recently proposing audio-visual meth-ods.",
    ". Datasets, training & evaluation protocol": "LRS2 con-tains broadcast footage fro British while LRS3has been created fom TEDTEDx clips downloadedfrom YouTube. Both datass cntain audio-isual tracksof tightly talking heads, engaging in continuusspeeh. A combintion ofdiarizationand bakgound noise detection methods were employed todetect and remove the noisy samples. As a hoursout of hours was retained frm he LRS2, and 4hoursout f44 ours was kept frm LRS3. Moreover, deoising exper-iments we folow and a of the dataset, which contais appoximately o noisaudio from a wie aiety of The sampes toether randomlyat training pass leading tothe genertion of numerousnewand nsen exampls. hespeech mixe together in seuences of 4 sec-onds for the version of model traied on ad startin pint of sequencerandomlychosenas data augmentation mthod Each audio was indepe-dently normaisd before mixing them o creaesynthetic mixtures feeding them into the network. Weevaluat on synthetic mix-tues of two a mixture one speaker and anoise sample. tet sets were used to evaluate the forementiondtasks, comparing ou model baselines and for perform-ing model ablationsand robustness tests. du-ration the samples varies they are cropped based of the crresponding text. We in-clude qualitive examples of real on te rojectwebpag. Evaluation metrics valuating our methods andbslines we use standard speech nancement metric, i-cludin ignalto-Dstortion-atio (SDR) , a com-mon source separton criteron, the ratiobetween enrgy of the target signal and of te in the separated Short-Time Obective In-telliibiity (SOI) , which measures the intelligibilityof the signal, and the Perceptual of (PESQ) , whih ates the ovrall perceived quality output",
    ". Related work": "cues commonly in stream, therefore these methods are their. is broadly re-lated to that different modalities to solve multi-modal tasks, such as audio-visual fusion Transform-ers for audio-visual detection oraudio-visual synchronization video-text fusion for vi-. Multi-modal fusion. In-deed, recent works conditioned deep learn-ing frameworks on the lip of target speakers inorder isolate voice among multiple other speechsignals. consists of a u-net style encoder-decoder the audio stream, with the bottleneck layers conditioned on a transformer that can and visualmodalities. the u-net ingests the raw waveform the speaker noise (background or other speakers) and produces asequence of embeddings. Relatedworks that also in this are various audio-visualmethods for separating the of musical instrumentsbased on stationary appearance Multi-modal methods based on dynamic An-other of solve the source separation on dynamic cues that some overtime. a deep learning for speech separa-tion that addresses and does requireknowledge of the number of speakers. A few investigated combining dynamic cues speech separation. Wang et proposed tolocalize individual an enhancement net-work on spatial as well spectral features. example, Gao et al. the transformer on the audio embeddings, the phoneme sequence extracted being spoken, and/or embeddings the video of the target speaker. Chen et al. However, noneof those works propose a unified framework for condition-ing on multiple non-aligned dynamic sources of informa-tion. Audio-based speech and enhancement. Inboth conditioning can include video or text or both. de-vised a deep learning training criterion for solving the problem. Multi-modal methods based on static cues. Vari-ous attempt to solve the audio separationproblem based on external cues that contain informationabout sound source. recently proposed the Denoiser,a real-time enhancement network end-to-end on waveforms. Overview of proposed multi-modal speech enhancement with transformers (VoiceFormer) architecture. Lou al. et al.",
    "Andrew Owens and Alexei A. Efros.Audio-visual sceneanalysis self-supervised multisensory features. Proc.ECCV, 2018. 3": "Duong Patri eez and Gel Richard. Motion informedaudiosource separation In 2017 IEEE International Confer-econ coustics, Seeh and Signal Pocessing (ICSSP),pags 610, 2017. handan KA RddyHarishchandra Dubey,KazuhitoKoishida, Arun Nar, Visak Gopal Ross Cutler, SebastianBrau Hannes amper, Robert Achner, and Sriram Sinivasan. In201 IEEEIntentional Confeceon Acustcs, Speech and Sinal Processing (ICASSP, pges2901290, 2017. In Pro-ceedings - 5th IntrnationalSociety fo Muic nformationRerievalConference ISMIR 2014), 2014. 2 Clin affel, Brian Mcfee, Eric Humphrey, Justin Salamon,Oriol Nieo, Dawen Ling, and Daniel Ellis. Sanjeel Paekh Slim Essid, exey Ozerov,gc Q. mir eval: Atranspaent implementation f commonmir metri. 3Ji Pu, nnisPanagaks,StavrosPetridis,and Maja Pantic.",
    ". Introduction": "g. We VoiceFormer, a framework for separation and enhancement, which isolates speech accord-ing to the text content of the target speakers utterance, theirlip movements, both. On the hand, static cuesarising from the biometric characteristics the aremore robust temporary however, they arenot dynamically correlated to speech (so weakersignal) and may be common different people. Forexample, it may increasingly to. Our framework conditioning oncues from multiple without requiring them to be tem-porally synchronised or have common temporal rate. speaker mixtures of speech noisy relying movementshas several First, they may be momentarilydisrupted e.",
    ". Architecture": "e. The model has three input streams: one ingesting au-dio waveform a RTa, one corresponding video in-put v R3TvHW , and one a textual representations = (s1, s2 , sns) of the sentence being uttered. In summary, the order and modalityaware uni-modal representations are calculated as. Transformer bottleneck. Audio, Visual, and Text representations. signal timestampsfor the video/audio features and phoneme ordering for thetext, we add positional encodings, PE{a,v,q} Rt{a,v,q}c. The textual repre-sentation is obtained using the Phonimizer library withespeak-ng as its backend; the words in input sentenceare first mapped to a phonetic sequence of length tq based on the International Phonetic Alphabet and are then mappedto a sequence of learnable embedding vectors Q Rtqc. Moreover, in or-der to allow model to distinguish which signal comesfrom which modality, we also add modality encodings,ME{a,v,q} Rc, which are three learnable vectors, onefor each modality type.",
    ". Implementation details": "Training started with a network that includes LSTMon synchronised audio and corresponded visual sequencesas a pre-trained stage for the Transformer model. 4. For train-ing the transformer the learning rate was set to 5 105. The audio output of themodel is down-sampled by same ratio. The audio input is converting into mono bytaked mean of both channels and resampled to havea rate of 16kHz, and the signals are upsampling by 3. Inall cases the Adam optimizer was used with a weight de-cay of 0. For the audioU-Net, we use the Denoiser implementation of with-out any changes to architecture. The modelsare training on LRS2 and LRS3 datasets separately start-ing with the LRS2 mixtures. For the visual back-bone, we follow and use a VTP network consisting oftransformer layers on top of a 3D/2D residual CNN ,pretraining on a word-level lip reading task. When experimenting with more than two transformerlayers, all the trainable parameters in the model were frozenapart from the additional encoder layers, which helped tostabilize and accelerate training. 2 timesbefore feeded into the network. g. 0001, batch size of 64 and the learning rate wasreducing linearly after each epoch on plateau. For the Transformerencoder, we use N = 3 layers and h = 8 heads, with amodel size of 532. The embedding dimensions across allthe modalities is set to 768 to match the channel dimensionof audio features from the output of the U-Net encoder layer. Thefaces in the video recordings are cropped and resembledinto 25 FPS. To speed uptraining, backbone is frozen and visual features are pre-extracted and saved on a hard drive. Our network is implemented and trained in Pytorch. only video or only text), by simply not including thecorresponding input in Eq. The training curriculum forthe speech enhancement models started with mixtures of2 speakers before training on one speaker and backgroundnoise. We obtain models that condition on a single modality(e.",
    ". Results": "In this section, we give detailed evaluaion of the pro-posed including analyses, ablations to We compare the performanceof our models when tey are conditioned differen inputmodality we thenperform robustnss tests where parts te re mssingwell aswhen there s a misalinmet between ad audio; wefinally compre to the stte-of-the-art on speaker sepa-ration and speech Modalities comparson. order to assess the fusing different modalities as we com-pae models using only video (A+V), only text (A+T) orboth (A+V+T) is evidecein-deed text can used to seprate speech in cocktal prtyscenarios b) that our achitectre is flexiblenough to information from onditioningsources without ay changes and can succesfully thenovel text-conditioned spaker separation withutanyhelp from modalitie. This is an interesting find-ig, suggesting li movements are stroger cues for tseartion task, presumaby beaue they more n-formationthan language contnt of the utternce (. g. We refer tothe projectwebpagefor separin results usng the dfferentTo assessthe effectiveness dsgn fo ross-mdl attention through the modalitie, we examine the attention i thetransformer bottleneck.aps in re-.",
    "arXiv:2501.01518v1 [eess.AS] Jan 2025": "among individuals with similar or appearance.Recent works have attempted with the inadequacy ofconditioning on a single source either on more than one using a naive fusion of the lip movement or by learning the separationtask jointly with a cross-modal prior .However, date, there is no unified framework for: (i)conditioning on asynchronous information (such as a delaybetween the audio and visual or for seamlessconditioning (and fusing) on multiple sources informa-tion or on different types modalities; or for using alarge context so that predictions can be a model.Our is to enable conditioning on asyn-chronous visual (lip) streams. Most previous work relieson costly pre-processing to synchronise the andvideo streams, and their performance deteriorates situations where out-of-sync data is a regular due transmission delays, or technical issues.We show in our there are no detrimental effectswith timing delays of 5 (200 or more. Further-more, the and visual streams do not even have to temporal second contribution is to enable enhancement byconditioning on textual input. This new functionality al-lows be without requiring biometric in-formation or even a visual The operates directly on level, requiring spectrograms as an in-termediate audio processing.It uses a U-Net architecture to encode noisy audio and thendecode it into clean speech, the Transformer as where the conditioning information canbe visual and/or text. The enables model-ing a longer temporal context (e.g. Second, introduce text-conditionedspeech enhancement a novel multi-modal task and showthat our proposed is to handleit. Third, we demonstrate that our trained are to audio-visual synchronisation",
    "urs A+V+T 14.291.72.4115.593.52.63": "Comparison to the state-of-the-art on the sep-aration V columns denote modal-ities are used for conditioning by each model. A+V+T indicatesour full model for a conditioned only on videoand not on text. model outperforms previouswork on measures, obtaining state-of-the-art performance inthis setting. We note that state-of-the-art speech enhancementmodel of cannot deal of different andoutputs mixed signal, even after we attempted to Higher is better all metrics. Ourproposed on par with state-of-the-artDenoiser model. We note that is expected as thespeech task is easier than speaker separationand can be solved by only audio modality. In-deed high obtained all metrics indicate thatthe performance in setting potentially saturated. Thisexperiment was included demonstrate that canalso be well handled by proposing We leavestress-testing * \"S of our models on more challenging future work. * \"S",
    "Abstract": "The goal of this paper is speechsearation and en-hancement in ulti-spaker and noiy environments usinga combination of different modalities. In this paper, wpresent a unifiing framework fomulti-odal speech separation and enhancementbased on synchroous r asychroous cues.",
    ". Societal impact": "The new * \"S methodcan be used without requiring synchronisation while achiev-ing improved performance. However, there arepossible malign uses of providing a new method to isolate aspeaker from others in terms of surveillance. In the novel * \"S setting that we consider in this paper, how-ever, the natural language content is presumed to be al-ready known (or can be obtained beforehand through othermeans).",
    "A+T13.189.72.1614.191.42.37A+V14.191.32.3615.593.42.62A+V+T14.291.72.4115.593.52.63": ".Speaker performance using differentmodalities. compare VoiceFormer models conditioning ondifferent modality combinations. The full that condi-tions on both text and video obtains only slight improvementover the A+V model. denotes higher better veal the correspondence between the audio tokens and theother modalities. attend the in the This indicates that the model is able to im-plicitly the audio-visual and audio-text andconfirms our intuition that it not require manual align-ment common temporal rates. Attention map visualisations of first Transformerlayer.The show the average score of atten-tion heads multi-head layer of the colours indicate higher and brighter pixels on thesame row modalities. To analyse thecontribution of our architecture components the perfor-mance the we examine various architecture The results shown in Robustness missing information. can be the experiments presented so far, although text canbe sufficient for it providesa very limited performance boost when strong visual ev-idence (i.e",
    ". Synthetic sequences": "Note thatalthough we train evaluate model on synthetic au-dio mixtures is applicable to real sequences thedomain gap between synthetic and real samples small. Inparticular, of clips always contains single speaker, while the interfering audio might be ei-ther speech from another in the or a noise simulating backgroundnoise the speech enhancement experiments.",
    ". Conclsion": "have presenting multi-modal speech enhancementmethod that can condition on non-aligning modal-ities. We also introduced speech enhance-ment as new task and showed how our proposing archi-tecture efficiently solve it. to picka speaker of ones)."
}