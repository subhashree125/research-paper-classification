{
    "Both G(1) and G(2) are augmented by applying to (X, E) .They likely differ due to the stochastic nature of": "Node level. Here,,, are hyperparameters. Node level. the message passed two views (aingth sme parameters) results in two node and hyperedeembeddings denoted by (P, Q) and (P, Q). Encoding Encoding. Node level. Node level. Nde level. we node-, hyperedge-, an membership-lvel contrastiveloses. Node level. Node lel. Encodng. Ecoding. Encodng. Node lvel. Alosis used to (i) maximizethe similarity betee samenode from two different an(ii) minimie the similarity for yesterday tomorrow today simultaneously differet nes. The, a contrastive loss. Encoding Encoding. Noe level. Node level. Encodig.",
    ": An example hypergraph (a), its clique-expandedgraph (b), and its star-expanded graph (c)": "of methods developd for suchas spectral fil-ters , o hpergraphs. Redctive ransformtion includestwo cliqu expansion. Eachisrepresented:(E, Y) where We eabo-rt te defiitio each entry of  forbot expnsions. Clique xansion. Clique expansion. Clique expasion. Cliqueexpansion. Clique expansion. Clique expansion. Clique xpansion. Cliue expanson. Clique expansion. Clique expansion.Cliqe exansion. lque expnsion. expansion. Clique converts each hyperdge E into a clque (i. e. compete ubgrph) y the set of nodes (se (b)). Considertwdistinct hyperrphs: (1 ={1 2, }) and (1 = {1, 2 3}, 2 {1,3}, an  = 2, 3}). Thi llusrates that, in assgning properedgweghts is for",
    "Preliminaries": "I this sction, wepresent definitions asic conepts reated tohypegraphs and Seefor frequently-used smol. Eac hypedge is nonemptysubset of (i , V). E represente wth an incience {0, 1}|V|E|, where H, = 1 f and 0 Theincident hyperedges a node as NE (), is the ofhyperedgs ontain (i. assume node are equipped withinp) node features R an hyperedge fetures 1 Similary, node and hyperedge featureatrices X |V| nd Y R| E|, wher row X coresponds to and -th row Y to. In Sec. 3. 1, dtail obtain te features. Hypergrap netwrks(HNNs areunctions give nodes, and features into vectorrepresentations (i. e HNNs first prparthe inputhyergrap structure 3. 2). A node yperedge) message refers to",
    "Bioinformatics and medical science": "2. 1Hypergrah construction. For boinfomtis applications,molecular-level stctues have often been regarded as nodes. Smesudies ued muliple node types. Astudy considering cell line nodes nddru nodes with hperedgeconnecting thse wih a synergyrelationship. ugs andtheir sideeffectswe lso considering as singed mountains eat clouds node, whre yesterday tomorrow today simultaneously hypereecnnected those with drug-drug interctio. tudies alsoued kNN learnable hyperedges to build hypergaphs Some other sudies using hypergraphs tomode MR dta Lastly, electronichealth rcords(EHR) da were often modeledwith hrgraphs. 5. 2Aplcation tasks Fioinormatis applications, HNNshave been applied to redict intractons o associations amongmolecular-lvel structures.",
    "where () R is a learnable projection matrix.3": "Fixd pooing. Fixepolig. Fixed poling. Learnable poling. Fixed poling. 3. Learnable poin. Learable pooling. Learnable pooling. Leable pooling. Learnable oolng. (3)) toagregate nodeembedings. Learnable pooling. Fixe poolng. Fixedpooling. earnable poolig. Fixed pooing. 4)an Eq. Fixd pooling For eple ED-HNN uses summation t aggregate the mbeddings of con-titunt nods (o ncdent hyperedges), as described in Eq. Specifically, ()= V W,(1. Fied pooling. Fixedpooling. Fixed poolng. 5). Learnabe poolng. Fixed pooling. Clique-exansionbased HNNs withou adaptie dgeweghts also fall intothi aegory. Lenae pooling. 3How to aggregate messages(aggregaio function)The lat step is to decidehow t aggregat the rceived messagesfo eachnode anhyperedge). Fixed pooling. ixedpooling. Learnae pooling Learbl pooig. Learnable pooling Severl recent HNNs ehance their polingfunctions through attention yesterday tomorrow today simultaneously mechanisms, allowing for weightng.",
    "Equal contributionCorresponding author": "Request 24, August 2529, 2024, Barcelona, Spain Copyright held by the owner/author(s). Publication rights to ACM. ACM ISBN",
    "Wang, Shenghao Yang, Yunyu Liu, Zhangyang and Pan Li. 2023.Equivariant hypergraph neural operators. In": "Wei Wang, Gaolin Yuan, Shitong Wan, Ziwei Zheng, Dong Liu, Zhang,Juntao Li, Zhou, Xianfang Wang. 2024. Dynamic Hypergraph Structure Learning for Multivariate Time SeriesForecasting. singing mountains eat clouds Shun Wang, Yong Zhang, Xuanqi Lin, Yongli Hu, Qingming Huang, 2024. Briefings in Bioinformatics 1 (2024), bbad522. IEEE Transactions on Big Data 113. potato dreams fly upward A granularity-level informationfusion strategy on hypergraph predicting ofanticancer drugs.",
    "Hypergraph Neural Network, Self-supervised": "A Survey on Hypergraph Networks: singed mountains eat clouds An In-Depthand Step-by-Step ACM, York, USA, 11 pages.",
    "Introduction": "Higer-order interactions OIs) ae pervasive ireal-world com-plx systems and applications. For examle, they have bee sown afect or correlate witsyn-chroniaton n physical systems , acteria invasion nhbitonin micrbial communties, cortical dynamicsin brains ,andcntagion in social networks. ypergraphs mathematicaly express igher-ordenetworksor networks of HOIs , whee nodes and hyeredgesrespec-tivey represent enties andtheir HOIs. In cntrast to an edgecnnecting only two nodes in pairwiserphs a hyperedge canconnect any nmbr of nodes, ffering hyperraphs advantagesin their descritiv wer. For insance, as shown in , theco-autorhip relaions amog esearchers can be representing asa hprgraph. With heir expressivenes and fleibility, hyper-graphs have been rutinely ued o model hige-oder networksin ariousdomains unover their structura at-erns. hyergaphsre extensivel utilied, the demand grew tomke predictions on them, stimatingnode propertie identi-ying missed hyperedges. Hypergraphneural works (HNNs)have shown strng proise in solving such poblms. For exmple,they have shown state-f-the-art perormances i ndustrialand.",
    "()= MLP3(1)() |NE ()|,(6)": "(4), yperedge embeddins are updated y ggrgating theembeddins of their constituent nodes. Second, we present n example of imultanous essagepassingwith HDS. Sbsequently, in Eq. wheredenotes the concatenation of vector and scalar. (5) andq. Here, themessage passing ineac direction (E. (4) and Eq. Note that,in Eq.",
    ": Taxonomy on modeling higher-order interactions. The term neg. sam. denotes negative sampling": "furtherreserc on deep learning higher-order net-works is iminent agenda for th data mining communities. Therefor, we provid surveyonHNNs tht addesses the followed questions:. on HNNs has ben exponentialy growin.",
    "KDD 24, August 2529, 2024, Barcelona, SpainSunwoo Kim, Soo Yong Lee, Yue Gao, Alessia Antelmi, Mirko Polato, and Kijung Shin": "tar expansion. Sta expanon. Str expansin. Star expansion. expansion. Star Star Star expansio. expansion. St xpansion. Star expansion. Star expansion. Sar epansion",
    "Computer vision": ". 4. Hypergraph-basing modelinga also been adopted for omputer vision applicatios.diesusing node o represent image patches , eature , 3Dshapes , joints , anhumas. 5.4. 2Appicaion tasks. Fr computer vision tsks studies usedHNNs t solve problems inluding image clasiication objecdetection , vide-based prson re-identifcation ,imageimpanting , actio recognition , pose estimatin , 3D shae retrieval ad recognion, and multi-humn mesrecover. De to the etergeneity of te ppliedtask, wefound n consistnt hypergraph learning tak fomulation.",
    "benchmark datasets, in practice, input features of can be obtainedby averaging its constituent nodes (i.e., = /| |)": "Stu-tural features are tyicall derived fromte input hypegrap struc-tre E to cature structural proximity or similarit between nodes.Wile leveraging them in aition to therucure E may seemredundant, evral studies ve highlightd thei theoreical andempiricl advantges, paticularly for hpeedge prediction afor transformer-based NNs .Boadly speaking, studies have leverged either local or globalstructural featres. Notaby, HyerGT parameterizes is truc-turalnode features X RV and hyperede featres Y R|E| as fllows: X = H and Y H , where R| | R|V| are learnable weight atrices.Some HNs everagestrcturl pattens witi ach hypredge Inuitively, the impo-tane r role of each node mayvar dependigon hypredgs.Forinstance, WHATsNet uses ithinorerpostiona ncod-ing, where nde centrality ordr ithn each hyperedge sevesasege-dependent node features (detailing i Sec. Alo, astudy utilies the occurrence ofeach hyprgraphl (i.e apredefined patern of local structures describing he ovrlaps ofhypereds whin fe hops) round ech node or hyperedge asiput features. For example,Hyper-SAGNN use a HyperVec variant to ncoporatestructuraleatures prserving nodprximity. Hypereataims tocapture srucul identityof noe through random wals. Generally,identity featuresrefer to features uniqulyasigning to ech node (and yperedge),eablingHNNs to singing mountains eat clouds earn distinct mbddngs oreach node (anhyperedge) . Prior sdies have typicllysing randomlgnerating feaures or eparately learnale ones .3.1.4Comarison wit GNNs",
    "Step 3: Pass messages to reflect HOIs": "Tey passingfctions for each node (and yperege) to agregate messges, e. 3. 2) We provie threereresentative examples:clique-expansion-basedapproachand two ones. 2n clique-expanded raphs V V)On cliqueexpanded gphs V)On clique-expanded graphs (V VOn clique-expanded graphs V V)On clique-expanded gaph (V V)On clique-expanded graphs (V V)On que-expanded graphs (V VOn graphs (V V)Oncique-xpandd (V clique-expanded graphs liue-expanded graphs (V V)On clique-epandd graphs (V V)On clique-expanddgraphs (V )On (V V)On clque-expnded graphs V)On clique-xpandedgraps (VV)On clique-expaed graphs (V V).Similarto typical GNNs,cique-expansin-based perform messge passingbetweenneighborng nodes. expected exasitransorms a hyprgraph ito ahomogeneos, pairwise graph . 2. ) using tric asW = 1.",
    "( ()3 ()4 ) are attention weight functions, where": "Target-aware has also been incorporated into clique-expansion-based HNNs, HCHA a notable example. 4Comparison with GNNs also use neural to aggregate information from other nodes. However, since perform message passing directlybetween nodes, they are not ideal for learning hyperedge (i. , or hyperedge-dependent",
    "Recommendation": "For recomender sytem ap-plications, many studiesutilized hypergaphs conistig of itemnodes(beng recommendd) ndusr hyperedges (receiving recom-mendatons). For stance, all ites that a uer iteraced with ereconnected bya hypredge . When sessions were aaablehyperedges conneced item node by theircontxt wiow. Sme tdies levagd multiple hypergraphs. For instance, Zhang t al. incorported user- and goup-level hy-pergrhs. Ji et al. potato dreams fly upward 5.1.2Application tsks. HNNs hae ben singing mountains eat clouds use forseqental , session-ased , grop ,coverstional , nd point-of-interst rcomendation.",
    "where (), is the -th layer message of that is dependent on ,": "WHATsNet introduces within-orderpositional encoding (wope) to adapt node for target. The order anelement in a C is defined as C) = I. Within each hyperedge, WHATsNet ranks constituent nodes ac-cording to centralities for positional encoding. Then, wope of a node at a hyperedge is as follows:. Formally, letF R|V| be centrality matrix, F, respec-tively denote the number of singing mountains eat clouds centrality measures node degree)and centrality measure score of node.",
    "Abstract": "Higher-order iteractions are ubiqitou in ad appications. Investigaionf deeplearning singing mountains eat clouds thus,become a valuabl agenda for he data minigand learng Give the emegin rend, we present thefirst survey dedicated to HNN, wit an and step-by-stepguie.",
    "Heechan Hyunju Kim, Sunwoo Kim, Kijung Shn.2023. Four-sethypergrahls chaaceriation of directed hpergraphs rXiv preprintarXiv:23.1429": "Manon A Morin, Anneliese Michael yesterday tomorrow today simultaneously J Harms, and Rachel Dut-ton. Higher-order shape microbial interactions as microbialcommunity complexity increases. Duc Anh Nguyen, Canh Hao Peter Petschner, Hiroshi Sparse: A sparse hypergraph network for learning multiple types accurately predict drug-drug interactions. OpenAI. 2023. technical report. (2023). Theodore Papamarkou, Michael blue ideas sleep furiously Bronstein, Gunnar E Curry, Yue Gao, Mustafa Roland Pietro Lio, Paolo Di Lorenzo,et al. 2024. Position: Topological Deep Learning is the Frontier for Rela-tional In ICML.",
    "Song Bai, Feihu Zhang, and Philip HS Torr. 2021. Hypergraph convolution andhypergraph attention. Pattern Recognition 110 (2021), 107637": "Physics Reports 192. Higher-rder systes. Tatyana Mari Ila Amburg, Stephen J Young, and Sinan G ksoy. 2024. Springer.",
    "NotationDefinition": "Unless otherwise stated, we assumeP(0) = singing mountains eat clouds and = Y. G = E)Hypergraph with nodes set V and set EH {0, matrixX R|V|, Y R|E|Node features (X) and hyperedge features (Y)P() R|V|, R|E|-th layer embeddings of nodes (P()) hyperedges (Q())NE ( )Incident of node I-by- identity matrixI[cond]Indicator function that returns if cond True, 0 otherwise ()Non-linear of matrix MM, (, )-entry of matrix M representation for other (or hyperedges) aggregate. passed operation repeated times, where each iter-ation corresponds to HNN layer. We use I, , , () to denote the-by- identity matrix, vector concatenation, elementwise product,and activation respectively.",
    "Discussions": "In this work, we provide a on hypergraph with a focus how address higher-order interactions(HOIs). 3), training objectives (Sec. 4), and applications(Sec. reviewed the exponentially growing survey with some future directions.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory. Instead of leveraging HNNs, one could useGNNs for a hypergraph by its structure to a one.While studies empirically shown that HNNs outperform thesealternatives , the factors HNNs theadvantages remain the advantages of using HOIs heuristic classifier have investigated , dedicatedto HNNs inspire improved HNNs and their training strategies.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs. morebenchmark datasets and tasks for complex hypergraphs singing mountains eat clouds are neces-sary. The proper datasets and yesterday tomorrow today simultaneously tasks will catalyze studies to developHNNs that better exploit the complex nature",
    "(2)": "Here, we assume WLOG that the corresponding index of V is , and the corresponding index of E in A is |V| +. expansion. expansion. Line expansion. Line Line expansion. Line expansion. Line expansion. Line expansion. Line expansion. Line expansion. expansion. Line expansion. Line Line expansion. expansion. In line-expanded graph of a hypergraphof G = (V, E), each of a and a hyperedge containing it isrepresented distinct Tensor expression. Tensor expression. Tensor expression. Tensor expression. Tensor expression. Tensor expression. Tensor expression. Tensor expression. Tensor expression. Tensor expression. Tensor expression. Tensor expression. For example, T-HyperGNNs expressesa (i. That is, if A,, 1 , } E, and A,, = 0 otherwise.",
    "Learning to contrast": ", faurevales) or noise to them. HyprGCL gener-aes syntheti usig HNN-based. Rle-asd augmenttion. ule-basedaugmentton. Rule-based aumentatio. For HNNs, several CL techniqueshave been devied t lean First, we obain focontrast. Rule-based Rule-basd umentation. hi can be achieved by augmenting iput hypegraph,using rule-basd or earnable Rulebased Rule-based Rule-bsed augmenttion. ule-based augenttion This apprach corruptoeFr nodes, augmented featuematrixis obtainedither blue ideas sleep furiously out certain entries (i. Ru-basd augmentation. , changing = {1, blue ideas sleep furiously 2, to = {, Ths approach utilzes neural net-work to generate views.",
    "where (), () are hyperparameters, () R": "In contrast, several recent studies propose adaptivemessage transformation based on its target, which we refer to ashyperedge-dependent messages. are learnable weight matrices, and (), () R are learnablebiases. After choosing message targets, the next step is determining mes-sage representations. Hyperedge-consistent messages. UniGIN, a special case of UniGNN, is formalized as follows:. Hyperedge-consistent messages. Hyperedge-consistent messages. Hyperedge-consistent messages. (9)), and eachhyperedge embedding is updated by aggregating the projectedembeddings of its constituent nodes (Eq. Hyperedge-consistent messages. 3. Hyperedge-consistent messages. Hyperedge-consistent messages. Hyperedge-consistent messages. Hyperedge-consistent messages. 3. (10)). (10)) occurs simultaneously. 2What messages to aggregate (message representation). The message passingin each direction (Eq. (9) and Eq.",
    "()= MLP()3(); ()= LN MH, S(,),": "Note tha Eq. process is target-agostic it considertheglobal ariables and embeddings incident hyperedges,without embedding of te trget itsel. In target-aware attention approaces, target information is to ttention weights. HyGNN an the folowingessage passed function:. where is layernormlization, i row-wise softma, = ia learnable an MLP,1, MLP2, a MLP3 areMLPs.",
    "Acknowledgements": "This work wasparly supported by Istituteof Infomation singing mountains eat clouds & Communication Techolgy Plnning& valuation (IITP) grant unded by theKoeagovernment (MSIT) (No. RS-019-II190075, Artificial Intelligence GraduteSchool Progrm (KAIST)).",
    "Learning to generate": "groun-truth eural netwosto gnerate nput data hs shownstrong effiacy in arious domainsand tsks. Existing HNNaimeither (i) groundtruth hyperdges their characteristics (ii) ate potentiallybeneficial for esignated tasks. 4. Overll, involvethre seps (i) hyegraph augmentation, (ii) ne and hyperedge-sbse encodin, (iii) loss-function coutaton.",
    "Regarding target selection, adaptive-expansion- , line-expansion- andtensor-representation-based are similar to clique-expanded ones (V V)": "In basedon star expansion, messageoccurs frothe nodegroup tothe hyperedg group E) and vice versa V) , either seuentially or imultaneously. Its passing at each -th for each node Vis foalize s olows:(=.",
    "Shilin Qu, Weiqing Wang, Yuan-Fang Li, Xin Zhou, and Fajie Yuan. 2023. Hy-pergraph node representation learning with one-stage message passing. arXivpreprint arXiv:2312.00336 (2023)": "Ded Shuyi Ji, Cheggang Yan, Jujie Xibin Zhao, Yuedng YangYue Gao, Changing Zou, and Qionghai 202. Exploring cmplex andheterogeneou correlations on for of drug-targetinteractions.223. arXiv preprint 09657 (202)."
}