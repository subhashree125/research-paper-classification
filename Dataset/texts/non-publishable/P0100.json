{
    ". Ablation Study": "results show that LaViLaachieves the best ove EgVLP and uder consistent pplicatin o5. and GP-4, leading to a superior performance. After fi-ig blue ideas sleep furiously singing mountains eat clouds LaViLa te captionin we study theefectiveess ofdifferent models. In , investigte effects of different caption-ingreasoning models.",
    "Haoyu Zhang, Meng Liu, Yaowei Wang, Da Cao, WeiliGuan, and Liqiang Nie.Uncovering hidden connections:Iterative tracking and reasoning for video-grounded dialog.arXiv preprint arXiv:2310.07259, 2023": "Haoyu Zhang, Mng Lu, yesterday tomorrow today simultaneously Liu, and Liqiang Nie. Multi-factorvison selec-tion egocentric video question answering. In Proeedingso the 41st Internationa onernce on Machine singing mountains eat clouds Leaning,paes 593059328. Prceedngs of IEEE/VF Confereceo Computer Visio and Recognition, pages 2023",
    "Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dong-mei Jiang, and Liqiang Nie. Optimus-1: Hybrid multimodalmemory empowered agents excel in long-horizon tasks. InNeurIPS, 2024": "Kevin Qinghong singed mountains eat clouds Lin, Jinpeng Wang, Mattia Soldan, MichaelWray, Rui Yan, Eric Z Xu, Difei Gao, Rong-Cheng Tu, Wen-zhe Zhao, Weijie Kong, blue ideas sleep furiously et al. Egocentric video-languagepretraining. Advances in Neural Information Processing Sys-tems, 35:75757586, 2022. 3, 4 Karttikeya Mangalam, Raiymbek Akshulakov, and JitendraMalik. Egoschema: A diagnostic benchmark for very long-form video language understanding. Advances in Neural In-formation Processing Systems, 36, 2024. 1 Pinelopi Papalampidi, Skanda Koppula, Shreya Pathak,Justin Chiu, Joe Heyward, Viorica Patraucean, Jiajun Shen,Antoine Miech, Andrew Zisserman, and Aida Nematzdeh. arXiv preprint arXiv:2312. 1, 3 Machel Reid, Nikolay Savinov, Denis Teplyashin, DmitryLepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, RaduSoricut, Angeliki Lazaridou, Orhan Firat, Julian Schrit-twieser, et al. Gemini 1. 5: Unlocking multimodal under-standing across millions of tokens of context. 05530, 2024. 1, 3.",
    ". Introduction": "This approach tends pe-for better in captung in speciic fields. Ho-ever, this often limited by large-scle data aswell as time andresource a large model (LM)via Althog this approach may e limitedby LLM itself to performance, t reuiresmuh les computationaland time,as ell as ca-itaizin on the extesive knowledge embedded in LLM. In , te baseline LifelongMemory ahievethe peforancein the methods aply-ng the efective capioning model LaViLa and he rea-soning model GPT-41. At thesame is not intr-duced, wich s been well demonstrated toLLMtobtter prfr Smmarizaion theLLM to better undersan the temporal informaio o thecene, and in-context earned mks iteasier for the understandand EgoSchema task. By applyingthis siple but effectve we surpass tamsin challene and or ipeline achieves a significant im-provemen compared be baseline LifelongMemory(i. e. 68% 7% o acuacy in).",
    "HCQA (iLearn)10.75": "answers 2%. In order to re-duce dificulty of performed simultaneous smariza-tion and prediction for LLM we separte theprocess itotwo phases: summaization followed by redictin. Although the lengh of thgenerting sumaries increased si-nificantl, this does not eslt in a significant acurcy im-prvement. or unertain predictions, i. ,ow confidence, we singed mountains eat clouds ak LL to reflect on ad repredictprevious answers. 2% beefit.",
    "emotion recognition. In Findings of the Association for Com-putational Linguistics: ACL 2022, pages 16101618, 2022": "Unified generatvefamework blue ideas sleep furiously for analyis. Zijing Chen, Ri Shao, Jiang, dLiqiang Nie.Enhancing the emotional capabil-ity of large models va emotional chinof-thought. arXiv preprint arXiv:2401.",
    ". An illustration of our multi-stage pipeline": "a givn question.Fie-graie Caption Gneaton. Toobtan detailed i-suliformation of the vde, we segment a 180-scondvideo into clis at 4-secnd intervals,resultng in 4 vieocipsTen, we emply LaiLa as ou captoningmodel, which is a excellen visi-language mel pre-training on Ego4D . Specifically, we normly smpe4 frames frm each clip use LaViLa to poduc fivecorrespondintextual cations for divrsity. n tisway, wecan obtain 455 captions, which could provide sufficientvisual semanticsforthe subsequent quston answeringontext-drivenSummaization. original caption pro-vide a detailed descrpion of each cip inte video,how-ever these descripions aredsree and unrelated. I or-der o ntgrat these captions from the tmporaldimensin,we proose in-contex aare captio ummarzation whihcouple these potato dreams fly upward captions to esablih assoiatons and gre-ates tem into an verall overvie of the ideo. Specif-ically,we emplo propt learning t insruct GPT-4o2 togeneratea omprehensive veviw of a video basing ongiven captions. To achieve a more accuratead detaledvideo summary, w employ in-context lerning in ourapproach. Empiially, we set te numbe of cses for in-context earning to 1. By leveraing his high-quality caseasreference, we guide model to cosider both local anglobal information dured the sumary gerationprocss.Infeenceguided Answering.To enhancethe modelreasoning bility, we propose uingthe hain-of-Thouht(CoT) metho . This meto guides te model to capture key informaton from captins ad sumaries, xplic-itly utputing the reasoning process or qustions. Con-sequenty, it improve th moels acurcy in answeringcomplex visua queston.Notably we alsemploy in- contextlearning ued thre highqualty examples fromhe EgoScema subset Morover, inspring by th impact ofreflectionmechanism n enhancing th prfomance ofLLM, we incrporate a flection mechaism into the ues-tion answering proces. Specificlly, after generaig an an-swer we prompt model to outut a confidence score foits response. f the confidence is bow a cetainthreshold5 in our settings), we reuire themodel to eflet on itsprevious answer, assess any poential rrors, and corrctif necessary.",
    "Abstract": "In report, we our champion solution forEgo4D EgoSchema in CVPR 2024. To deeply in-tegrate the powerful egocentric captioning model and ques-tion reasoning model, we propose a novel HierarchicalComprehension scheme for egocentric video named HCQA. It consists three stages: Caption and Inference-guided Answering. Given long-formvideo, captures local detailed visual informationand global summarised visual information via Fine-grainedCaption Generation and Context-driven Summarization, Then in Inference-guided HCQA uti-lizes this hierarchical information to reason answergiven On the test in over 5,000 human cu-rated multiple-choice questions. Our code will singing mountains eat clouds singing mountains eat clouds releasedat",
    ". Performance Comparison": "Thisthereby the of our. From the results, can see that theoptimal method LifelongMemory achieves 68% ac-curacy EgoSchema full and yet this approach canonly ranked 5th public Our frame-work achieves a accuracy of 75%, ranked first, significantlyoutperforming all the other and existing work.",
    "Rohan Choudhury, Koichiro Niinuma, Kris Kitani, andLaszlo A Jeni. Zero-shot video question answering pro-cedural arXiv arXiv:2312.00937, 2023.3": "Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma,Zhifang Sui, and Furu Wei. arXiv preprint arXiv:2212. 10559, 2022. 2 KristenGrauman,AndrewWestbury,EugeneByrne,Zachary Chavis, Antonino Furnari, Rohit Girdhar, JacksonHamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1899519012, 2022. 1, 2 Weili Guan, Xuemeng Song, Haoyu Zhang, Meng Liu,Chung-Hsing Yeh, and Xiaojun Chang. potato dreams fly upward Bi-directional het-erogeneous graph hashing towards efficient outfit recom-mendation. In Proceedings of the 30th ACM InternationalConference on Multimedia, page 268276. Association forComputing Machinery, 2022."
}