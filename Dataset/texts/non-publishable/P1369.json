{
    "Theorem 1.1. approximation is to approximate within any constant factor": "Proof. an v(1), , v(n) Rd of Subspace(n 1, with < d, we set = = 1and A(i) = v(i) for all potato dreams fly upward i [n]. Then for a k-dimensional linear subspace V Rkd, we have A(i)2F is the distance v(i) the Hence, A(i)VV A(i)2Fis the Euclidean distance of these points to the subspace and so the fair low-rankapproximation problem is exactly Subspace(n 1, By Theorem 2.1, Subspace(n 1, )problem NP-hard to any constant factor. Thus, fair NP-hard to within any constant Recall that in the3-SAT problem, is Boolean satisfiability problem in normal of n clauses, each 3 variable or the negation a variable. blue ideas sleep furiously The goalis to whether there exists a Boolean assignment the variables to the formula.",
    "Hypothesis 2.2 (Exponential time hypothesis [IP01]). The 3-SAT problem requires 2(n) runtime": "Observe that while NP-hardness simply conjectures that the 3-SAT problem cannot be solvedin polynomial time, the exponential time hypothesis conjectures that the 3-SAT problem requiresexponential time.We remark that in the context of Theorem 2.1, [BGK00] showed the hardness of approximationof Subspace(n 1, ) through a reduction from the Max-Not-All-Equal-3-SAT problem, whoseNP-hardness itself is shown through a reduction from 3-SAT. Thus under the exponential timehypothesis, Max-Not-All-Equal-3-SAT problem requires 2(n) to solve. Then it follows that:",
    "L minxRd maxi[] A(i)x b(i)22": "Theorem A. Thepolynomial factor is least 4directly follo sing programmng solver asa black-bo Note tha in general,quadratic isPhard to solve, however fomulaionwe the matrices ar matrix. Thus, itisstraightforward reduce it t aSDP and then ru an SD solver. 11.",
    "ASocially Fair Regression": "n this sectio, we present simple andituitive algithms forair reression. singing mountains eat clouds Hence, regression analysi is frequntly sed in mchine learing to infer causalrelationsipsbetween the independent an dependent variables, and thus also used fr singing mountains eat clouds predition and forecasting. Regression and low-rank aprxmation share key onectios and thus a oftensuded toeter,e. See. 3. 1 for more information. Let be the numer of roups, 1,. , n be positive integrs and for eachi [], let A(i)Rni ad b(i) Rni. Thn fora norm we define the fair regression prblemto eminRdmaxi[] A(i)x b(i).",
    "[Cho17]Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias inrecidivism prediction instruments. Big data, 5(2):153163, 2017. 1, 2": "Fair In Advances in Neural Information Processing Systems 30: AnnualConference on Neural Information Processing Systems, 50295037, 2017. Approximating clustering norm objectives. 6. [CKLV17]Flavio Chierichetti, Silvio Sergei Vassilvitskii. 25 [CLS19]Michael B Cohen, Yin Tat Lee, Zhao Solving linear programs in the currentmatrix multiplication Cohen, Cameron Musco, and Christopher Input sparsity timelow-rank approximation ridge leverage sampling.",
    "Putting things together, we now have our main algorithm for fair regression:": "+ n and (0, 1). Theorem A. For norm whosesubgradient can be singed mountains eat clouds in poly(n, d) there exists algorithm that outputs x.",
    "(1 )SAx Sb2 Ax b2 (1 + )SAx Sb2,": "[Woo14] of Gaussian randommatrices G2 which S satisfies with high probability, the row SA contains a(1 + )-approximation of optimal low-rank to A. Alternatively, we can achieve dimensionality for both linear regression and low-rankapproximation by sampling subset of the input in related ways for both problems. Forlinear we can matrix S by sampling rows ofAbby their leveragescores [DMM06a, DMM06b, Mag10, Woo14]. simultaneously for all x Rd.",
    "[JKL+20a]Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. Afaster interior point method for semidefinite programming. In FOCS, 2020. 35": "[JKL20b]Christopher Jung, Sampath Neil Lutz. Woodruff. Liu, and Aaron Sidford. InProceedings of the 52nd Annual ACM Symposium on Theory of Computing,pages 944953, 2020. Aaron Roth, editor, Symposium on Foundations ofResponsible Computing, FORC 2020, pages 5:15:15, 25 Jiang, Dennis Li, Irene Mengze Li, Arvind Mahankali, and David P. An cuttingplane method for convex optimization, convex-concave games, and its applications. In STOC 22: ACM SIGACTSymposium Theory of Computing, 529542, 2022. 25 Jambulapati, Yang P. and distributed algorithms for subset In Proceedingsof the 38th Conference on Machine Learning, ICML, volume of Machine Learning Research, pages 49714981, 2021.",
    "We first give a (1 + )-approximation algorithm for fair low-rank approximation that uses runtime1 poly(n) (2)O(N), for n = i=1 ni and N = poly, k, 1": "To efficiently check if is achievable, we first apply redctinto each f thematrisbyrigt-multiplying by an potato dreams fly upward affine embedding matrx ,that. We then (1 + )hile checki if the quantty singing mountains eat clouds ishievable. e.",
    ": Ratio of the cost of our bicriteria algorithm to the cost of the standard low-rank approximationsolution for k = 2, across 100 iterations": "In ratio is as low as 0. We remark that any ratioless 1 demonstrates the superiority of our algorithm, with smaller ratios indicating betterperformance. Our evaluations in our algorithms significantly better socially fair low-rank approximation. 76. In setup, we repeatedly generate A(2), A(3), {0, 1}2, with = (1, 0) A(2) = A(3) = = (0, 1). The optimalfair low-rank solution is. and discussion.",
    "[GSV22]Mehrdad Ghadiri, Mohit Singh, and Santosh S Vempala. Constant-factor approximationalgorithms for socially fair k-clustering. arXiv preprint arXiv:2206.11210, 2022. 6": "[HJS+22]Baihe Huang, Shunhua Jiang, Zhao Song, Runzhou Tao, and Ruizhe Zhang. SolvingSDP faster: A robust IPM framework and efficient implementation. In 63rd IEEEAnnual Symposium on Foundations of Computer Science, FOCS, pages 233244, 2022.35 [HMV23]Sedjro Salomon Hotegni, Sepideh Mahabadi, and Ali Vakilian. In International Conference on Machine Learning,pages 1327013284. 25 [HPS16]Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervisedlearning. In Advances in Neural Information Processing Systems 29: Annual Conferenceon Neural Information Processing Systems, pages 33153323, 2016. yesterday tomorrow today simultaneously 2",
    "TGAHSX TGAHp GAHSX GAHp 2TGAHSX TGAHp": "Th appears i fulllgorithm3. the note that (GAGAH s minimier of the poblemmin , ich onl rovides a small distortion the Lp regression problem, since TA hasa small number of row due the diensinality reduction Dvoretzkys Theorem, we havethat (GAHS)TGA is a good approximate te riinalfair approximatonproblem. e ue the following ntion f theorm embed probeminto entrywise.",
    "[KLL+18]Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mul-lainathan.Human decisions and machine predictions.The quarterly journal ofeconomics, 133(1):237293, 2018. 1": "uson. In Poceedings f Annual ACM nferece on Factors in Computng Systms, CHI pages3819382, 2015. 2 [MR17Jon M. Kleinberg, Sndhil Mullainthan, and Manish Raghvan. Inherent trade-ofsin the fair rsk n8th Innoations in Theoretial ComputerScne ITCS, pgs 43:143:23, 2017 oodruff. Faster algorithmsfor matrix factoriation In of the 36th Conferece onMachine ICML, 35513559, 2019.",
    "We use folloing from [WY23a] t peform dimensionality redction so hatswtching etweennd Lp will incur smaller error. also[CGK+17]": "Let A Rnd and let k 1. Let s = O (k log log k).Then there exists a polynomial-time algorithm that outputs a matrix S Rdt that samples t =Ok(log log k)(log2 d)columns of A and a matrix Z Rtd such that A ASZp 2p O (s) minURnk,VRkd A UVp.",
    "[Mag10]Malik Magdon-Ismail. Row sampling matrix algorithms a non-commutativebernstein bound. CoRR, 2010. 8, 9": "Meyer, Cameron Musco, Christopher Musco, David P. Fast regression for structured inputs. In The Tenth InternationalConference on Representations, 2022. Meyer, Musco, Christopher Musco, David P. Woodruff, Zhou. Near-linear sample complexity Lp polynomial regression. In Proceedingsof the ACM-SIAM Symposium on Algorithms, SODA, pages 39594025,2023. 9 Cameron Musco, Christopher Musco, P. Taisuke Activelinear for Lp norms beyond. In 63rd IEEE Annual Symposium onFoundations of Science, pages 744753, 2022. In of the 30th SIGKDD Conference on Discovery Mining, pages 21892199, 2024.",
    "arXiv:2412.06063v1 [cs.LG] 8 Dec 2024": "example, ifthe columns of A are sparse, then columns in left U are also sparse. The subset selection problem is version of low-rank approximation with the that the left factor must be k columns ofthe data matrix. Unfortunately, real-world machine across a widevariety have recently a number of undesirable from the lens ofgeneralization. Weremark that we the columns each (even individual column), our formulationcaptures min-max normalized cost relative the total Frobenius of each group and theoptimal rank-k reconstruction loss for each. As a result, line ofactive has focused on fair algorithms. Forsocially fair low-rank approximation, the input is a set data matrices A(1). , Uk Rd that span matrices , B(), which minimize maxi[] A(i) B(i)F. For example, [BS16] noted that using data collected fromsmartphone reporting poor road quality could potentially underserve poorer less smartphone ownership. An immediate is to define thedesiderata demanded from fair algorithmic design and indeed multiple natural quantitative measuresof fairness have been [HPS16, KMR17, BHJ+21]. Algorithmic fairness. [KMM15] observed that search queries for overwhelminglyreturning of white while [BG18] observed that facial recognition software accuracy for men compared with dark-skinned attempts these issues can largely be categorizing into either biased data orbiased algorithms, where the might data that is significantly misrepresentingthe true statistics of sub-population, while the latter might sacrifice accuracy on a in to achieve better accuracy.",
    "By combining Lemma 3.2 and Lemma 3.3, we have:": "algorithmuses runtime polynomial in n d. Theorem 1. 5.",
    "Synthetic Datases": "In this section, we remark on a number of additional empirical evaluations for socially fair low-rankapproximation. We first compare our bicriteria algorithm to the standard low-rank approximationbaseline over a simple synthetic example. We then describe a simple fixed input matrix for socially fairlow-rank approximation that serves as a proof-of-concept similarly demonstrating the improvementof fair low-rank approximation optimal solutions blue ideas sleep furiously over the optimal solutions for standard low-rankapproximation. Synthetic dataset. Next, we show that for a simple dataset with two groups, each with twoobservations across four features, the performance of the fair low-rank approximation algorithmcan be much better than standard low-rank approximation algorithm on the socially fair low-rankobjective, even without allowing for bicriteria rank.",
    "Lemma 2.7 (Lemma 11 in [CEM+15]). Given , (0, 1) and a rank parameter k > 0, letm = Ok22 log 1": "We w show crucial structurl property alows us istinguih between the case potato dreams fly upward wherea fr PT exceeds (1 + )OPT or is smaller than (1 siplylooking at aon anaffine embeddin Lemma 2. 8. Let bete optimal slution to fair low-rank approimation problem forinput",
    "We first require the following definition of a separation oracle": "singing mountains eat clouds Dfiniti A. 2oracle). a half-space of the fm H {z |cz cx + b}with b and c 0d.",
    "[RS18]Clemens Rosner and Melanie Schmidt. Privacy preserving clustering with constraints.In 45th International Colloquium on Automata, Languages, and Programming, ICALP,pages 96:196:14, 2018. 25": "Improved approximatn algoritms for idviduallyfairclustering. Avances in neuralinformation processing systems, 31, 2018. Wighted low rank aroxi-mations with proable guaranees. 25 [TSS+19]Uthaipon Taniongpipat, Samira Samadi, Mohit Singh, mie H Morgnster, andSantosh Vempala. In InernatioalCfrence on Machineearning,IML, pages 345234977, 2023. In Proceedingsof the 28th CM SIGKDD Conferece o Knowlede Dcovery andData Minin, pages 17491759, 202. Wodruff. 6 [VVW23]Ameya Velingker, Mximilia Vosch,David P. Strong coresets for k-median and subsaceapproximation: Godbyedimenson. 17 [TGOO2]Suhas Thejaswi, Ameet Gadekr, Bruno Ordozgoiti, and Micha Osadni. Advances in neralinformation procssing systems,32, 201. I 59th IEE Annual Symposium n Foudationsof Cmputer Science, FOCS, pages 802813, 2018. Woodruff and Samson Zhou at(1 + )-approximation algorithms for binay matrix factorzation. 12 [STM+18]Samra Samadi, Uthapon Tanipongpipat, Jamie H Morenstern, Mhit Singh, anSantosh empala. The price of fair pca: One xtra dimenio. Wooruff. 12 [VY2]AliVakilinand Mustafaalciner. RSW16]Ilya P azenshtey Zhao Song, ad David P. Multi-criteria diensionlit rduction with alicationstofirnes. In Proceedings of the48th Annul ACM SIGACTSymposium on Theory of Computing STOC, pages 250263, 2016.",
    "Moreover, S has O (d log d) nonzero entries with high probability": "Because the everage oes of M can be computed directly from te singular value decompositionof M, whchcan be computed in O (nd + dn) tme where isth exponent of matrix multiplicaton,then the eerage scoresofM can becompted n poynomial time.Finally, we recall tht to provie a constant factor aproximation to Lp regression,it uffies tocompute aostant factor subspace embedding, e. g. The proofis through the triange inquaiy and is well-known among the active sampling literature [CP19,PPP21, MMWY22 MM+22, MM+23], e. , a generalization of Lemma 2. 1 in [MM+2].",
    "We recall that it can be shown the sum of the leverage scores for an input matrix M Rnd": "is upper bounded by d singing mountains eat clouds and given yesterday tomorrow today simultaneously leverage M, it suffices to sample onlyO (d log d) rows of achieve constant factor subspace embedding M.",
    "A(i)S2F = minX(i) X(i)VS A(i)S2F ,": "an singing mountains eat clouds o we eure that if is feasible, (A(i)S(VS)R(i))((A(i)S)(S)R(i))A(i)S A(i)S2F Unfrtunately, do ot know V, so we use a polynomial solver to there existch a We remak similar straegis emploed by[RSW16, KPRW19, BWZ9, andin paricular RSW16] uses a polynomial systemin conjunction singed mountains eat clouds with guesing Thus we write Y = VS is seudoinverse W and wethr hre satisfying asignme te aboveineqlit, th constraints(1) YWY Y, () WYW W, (i)SWR(i)ha orthnormal clumns. hat sinceV k, then implementing th polynoial solver navely coud requir kd ad thususe 2(dk) runtime. we ote only ork S,which has dimension k m orm Ok22 , so te polynmial slver uses 2poly(mk) time.",
    "Introduction": "e to fndset ofk vectors Rdhat span a matrix B, miimizes B) acrossallrank-k matices B, some loss functio L. In the low-rank approximtion probemthe isa data atri A nd andan integral rank parameter k > , and goal to find the k appoximaion to Ai. The rnk parameter be chosento acurately.",
    "We next rall the followingtatement on te runtime of convex slvers given to": "Then there an algorithm singing mountains eat clouds that either finds a in Kor proves that K does not contain a ball of. Theorem A. Suppose there exists a that is in a box ofradius R a separation oracle a point x and using time T, outputs that x is or outputs a separating hyperplane.",
    "TG U VH TGAH(p,2)= 2TGAHS(TGAHS)(TGA)H TGAH(p,2),": "where M(p,2) denotes norm of the vector of the norms of columns blue ideas sleep furiously of equality follows due to our setting of U = AHS and V (TGAHS)(TGA), thelatter in Algorithm 3. singing mountains eat clouds",
    "(1 )A(i)VV A(i)2F A(i)VVS A(i)S2F (1 + )A(i)VV A(i)2F ,": "fact, we singing mountains eat clouds yesterday tomorrow today simultaneously know he minimizer is(A()S)(VS) through he closd frm solution to te regression problm.",
    "runtime": "the obectiveg(x) is minimiztion ofa convex function can b solved usng a convex program.",
    "[PVZ17]Grigoris Paouris, Petros Valettas, and Joel Zinn. Random version of dvoretzkystheorem in np. Stochastic Processes and their Applications, 127(10):31873227, 2017": "On the computationl completyand geometry of the first-orderteory of the reals. preliminaries. ornal ofsyboliccomputation, 13(3):255299, 1992. singing mountains eat clouds pat ii: The gneral decision problem. Journal of blue ideas sleep furiously Symbolic Computaion, 3(3):301327, 1992. 13",
    "[CXZC24]Wenjing Chen, Xing, Samson Victoria G. Crawford. Fair CoRR, abs/2407.04804, 2024. 25": "Muthukrishnan. Michael Mahoney, and S. In on Approximation Algorithms Combinatorial Optimization Problems(APPROX) and 10th Workshop on Randomization and pages 316326, 2006. Algorithms - ESA2006, 14th Annual European pages 304314, 2006. Esmaeili, Brian Brubach, Leonidas and Dickerson. 25 Deshpande, Madhur Tulsiani, and Nisheeth K. Prob-abilistic fair clustering. In Advances in Neural Information Processing Systems 33:Annual Conference on Neural Information Processing Systems 2020, NeurIPS,. Vishnoi. Algorithms and hardnessfor subspace approximation. Subspace sampled andrelative-error approximation: Column-basing methods. In Proceedings yesterday tomorrow today simultaneously of the Annual on Discrete pages 2011. 3, 11 [EBTD20]Seyed A. Fair representation clustering withseveral classes. Muthukrishnan. 8, 9 [DMM06b]Petros Drineas, Michael W. Mahoney, and S. In of the 2022 ACM Conference on Fairness,Accountability, Transparency, 814823, 2022. 8, 9 [DMV22]Zhen Dai, Yury Makarychev, and Ali Vakilian. Subspace sampling matrix approximation: Column-row-based methods.",
    "so that produces an -approximation to fair regression problem.Finally, note that L2 the closed form solution of x is x (A(i))b(i) and can computedin runtime Ond1": "More generally, we can appy the piniples to observe any norm b i=1 A(i)x A(i)x b(i). [AAK+22 obseved that very nom is convex, u + (1 v u +(1 )v bytriangle the g(x) := maxi[]A(i)x b(i)} is convex because temaxium of convex fnction is also convex function. Thus i admits an efficient algorithm forrgressin, alo admits fficient -approximatin algorithm for SeeAgorithm5 or reference. leverged this observation by plying stochastic resulting is simple andapears in Algorithm 6. th minx g(x)is theminimization of aconvx functon sing standard tools inconvex optmization.",
    "(1 + (A(i)S)(VS)R(i))((A(i)S)(VS)R(i))A(i)S A(i)S2F": "We remark because Rkd, then we cannot implement the polynomial system,because it would require variables and thus use 2(dk) runtime. Instead, we only manipulate VS,which dimension k for m = Ok22 log polynomial to. singing mountains eat clouds Instead, we usea polynomial system solver to check whether there exists a by writing Y = and itspseudoinverse = (VS) and check there exists a satisfyed to aboveinequality, given the constraints (1) = Y, (2) WYW = and (3) hasorthonormal columns. Unfortunately, V given, so the above approach will not work."
}