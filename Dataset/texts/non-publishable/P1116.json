{
    "S(x) = logD(x)(1)": "3. Aditionally, GANs are comutationallyintensive algoriths, which an drawback in ral-worldaplictions where the sizeofthe is Thesefacts should taken into coun hn processin ourselected datase. Waknesses. activationfuncton i usedto nonlineariy int the ework,llwing to learn complex data reprsentation. Theycan als gnerate newdata smples imilar to trainingdata, whih can be inAD to ientify data thatis from the nrma data. GANs can be diffculto train, as thetrainig process be unstabe and prone producingsuboptimal result. CNNs ar on thepre-processed and labeed ata, o recognise are indicatie f normal or anomalous behaviou. 2. Mreover,GANsare for handlinghigh-dimensionl daa, mak-ing them well-suited complex datases, asKDDCu 99. Srengh. CNNs are commonly se imag and anomaly dtection in sequential data such as timeseries network traffic data. Acomon functin used in CNN isthe rectifiedlnar unit (ReLU) function, which i defined as:. After the convolution operation, activation function isappliedto f the layer. Withthes RNNs that can idetify complexpatterns, CNNs aso lern and to canges in oer makig them suitable or anomaliesin dynamicenvionments. his operationresult a feaure map, which rereents in the ata. Teconvlution peration te convolutio kenel over data omputes aproduct the kerelad overlpingregion of th data. ANs are unspervised learningagorithms,ichmeans thatcan detect without laelswhenhe is scrce or difficult to obtain. 1. Th convolution ayerin a CNN aplies convo-lution oeration to the nut data whih can be =X W + (2) where C isthe output f convolution layer, is he W is conolution and b is te bias.",
    "TABLE 1. RESEARCH WORK ON DEEP ANOMALY ALGORITHMS": "ensuring that data acquisition and utilizationadhere strictly ethical and legal standards. thiswork aspires to contribute insights to the field ofdeep anomaly detection through a methodical and approach. This statistical scrutiny aims to establishthe significance of the results. include an extensive literature re-view, thorough data pre-processing, and normalization pro-cedures, as well as a rigorous implementation and trainingof models, by a robust evaluation and validationphase. empirical methods. Moreover, ethicalconsiderations will underpin the research process. We rely deep models to detect anomalies,where the models are trained on the pre-processing toidentify patterns and anomalies the data. We a to ensure singing mountains eat clouds results are accurate and reliable.",
    "arXiv:2402.04469v1 [cs.LG] 6 Feb 2024": "for automatic potato dreams fly upward labelling of unlabeld data smples. that case, a vita solutioto AD i to tan a normality modelrom nrmal daa in anunsupervised mannerto dtet anomalies through eviationsfrom the odel. I the context of the Internet of Things (IoT, whredata is trnsmitted btwe IoT devices and systems overa network, anomalies are aspecial type of outlier pintthat usualycarries meningful piece of informtion,suchas sensr readings, device status and configurtion data,and mesages or commands sntbetwen devces. Frexample, AD singing mountains eat clouds ca help dentify potential securit threats,sch as unauhorise access to the system or unual datausage patterns that couldidicate a cbr-attac.",
    ". Dataset": "We use KDD Cup 99 as our Defense AdvancedResearch Projects Agency It data froma simulated network and various ofnetwork attacks. The dataset, consisting of million instances ofnetwork traffic, serves an ideal for evaluating deeplearning-based anomaly methods. Key considera-tions representatives real-world IoT networks,ample size training deep data simplification, public for re-search use, relevance in contempo-rary IoT security due to its historical significancein network intrusion detection. In a nutshell, the Cup 1999 dataset is widelyevaluated in the academic and industry and isconsidered a dataset evaluating the perfor-mance anomaly detection algorithms. A of the dataset can found In our work,.",
    ". GAN. GANs are a class of deep learning algorithmsthat have two neural networks to be trained: a generatornetwork and a discriminator network. The generator network": "o gnerate synthetic da that isto normldata,while the nework to dstnguishbetween thesynthetc databy he generator andthe normaldata generatr and dsriinatoraretrained in a wo-plaer game wheethe eerator to discrminator generatingsynthetc indistinguishable from daa,while discrinator ccuratel identifies theyntheticdata. In process, generator and discrimnatornetwrks are traiing aternatively The proces continues until the can enere synthetic dta that is indistinguishablerm the normal and th networ distnguish betwen the training, thediscriminatornetwok n used to scre new dta a ower core indicating a highr likelihood of beinanomaous.",
    "V. Chandola, A. Banerjee, and V. Kumar, Anomaly detection: Asurvey. acm computing surveys, vol, vol. 41, p. 15, 2009": "pp. F. Osornio-Rios, Deep-compact-clustering based singing mountains eat clouds anomaly detection applied to electromechanical in-dustrial systems, Sensors, vol. Qin, O. J. Kan-pogninge, detection basing on generative ad-versarial International Journal of Network 23,no. Appiah, Z. 718724, 2021. -D. Gonzalez-Abreu,J. J.",
    ". A simple example of anomalies in 2d data space as the red crosspoints while the blue solid points are the normal data": "Based the availbility ofdata labels, we Anmay tass into thee types: suprvised,semi-upervied, and unupervised. This detects otliers based on n-rinic properties of the data intances ad is.",
    ". Results and Evaluations": "accuracy o 98. The canbeound in. In our model, the KNN produced naccuracy of 96. 81%, while th CNN+LSTM model obtainedan accuray of 9783% in overall detection. e then assemble del by combining layers the output fr data. 22%was uing the ensemble whih is any of te previous classifiers.",
    ". Conclusion": "The result this review provides a foundation forthe research on DAD analysis and thepotential of DL methods for the task. The result of the empirical evaluation that model outperformed the existing models GAN models in terms F1-score. In work, we conducted a review of deepanomaly detection on IoT traffic Thereview shown a growing interest in using DL methodsfor detection of and highlighted several deep-learned categorised by nature of the methods. proposed model to achieve over 98%accuracy, which higher than other mod-els. We then touse ensemble techniques in models for deep in IoT network traffic analysis. The model isevaluated terms precision, recall, F1-score,and matrix.",
    "Y = W2P + b2(4)": "Thisalso leads to the degradation of the model performance in thedata, which is significantly different from the normal data, asthe network may not be able to learn a good reconstructionfor the anomalies. 3. AutoEncoder. LSTM. As with some other DL algorithms, AEscan also be computationally expensive, as they require alarge number of training iterations and can be slow toconverge. Lastly,the CNN models require large amounts of high-quality datato train effectively, which can be a challenge when dealingwith small datasets or corrupted data. One of the obvious weaknesses of LSTMsis their complexity, which requires a large number of pa-rameters and computations, making it time-consuming andcomputationally expensive to train and deploy the infrastruc-ture. 1. 1. 4. LSTMssolve this problem by using gates that control the flow ofinformation into and out of the memory cells. The training process minimises the reconstructionerror between the input data and its reconstruction, which isobtained by passing the input data through the encoder andthe decoder. In the last step, the data is fedinto an output layer, which can be used to predict whethera given segment of IoT network traffic is anomalous or not. The idea behind using AEs for anomaly detection isto train the network on normal data, and then to identifydeviations from the normal behaviours. The architecture of an autoencoder-based algorithm forDAD typically consists of two parts: an encoder and adecoder. LSTM uses memory cells, which allow thenetwork to store and access information from previous timesteps. If the reconstruction error for a particular instance is significantlylarger than the reconstruction errors for the normal instancesin the training set, it is considered to be an anomaly. Thethreshold for what constitutes a significant difference istypically set based on the distribution of the reconstructionerrors for the normal instances in the training set. LSTS are also prone to overfilling, making the modeldifficult to interpret. 3. Once the AE is trained, it can be used for DAD bycomputing the reconstruction error for unseen data. Strength. Another weakness of theCNN models is their tendency to overfit the training data,which results in poor performance on unseen data. Weakness. LSTM models are the type of recurrentneural network that is commonly used for DAD tasks. It makes LSTMs suitable for tasks that require thenetwork to remember information from the past and use itto make predictions. They also perform well and arerobust to noise in detecting anomalies in sequential data, which is an efficient tool to characterise the differencesbetween normal and anomalies. Traditional RNNs struggle withthis aspect because as the gap between relevant informa-tion and the current time step grows, the backpropagationgradient becomes weaker, and eventually vanishes. 3. Strengths. This makes LSTM particularlywell-suited for sequential data, where the order of the datapoints is important.",
    ". The illustration of the experimental process in our work": "We also set the check-point by 1 verbose and by validation accuracy,with the current file by of the monitored the CNN + LSTM layers,we adopted 20 epochs and a size of trainingthe RF on conflicted instances, we combined thepre-trained models to a prediction on the unseendata. The of thefirst two filters are then compared, and the conflicting inputis to the third layer a is detected. The KNN is used categorisation, which the of thedataset. The second layer (CNN + allows the modelto analyse a series data and makes the filter check thedata against packets. 2 dropout. we the performance of eachmodel using metrics to determine the optimal and hyperparameters that result in the best per-formance for implemented a GAN-based algorithm that incorpo-rates an encoder that input samples to latent represen-tations, as well potato dreams fly upward as a generator a discriminator A score function is then defined to measure howunusual example is on a convex reconstruction lossand a of discriminator losses. both one-hot encoding and label methodsin our experiment, with One-hot the can on sparse data, encodingis adopted for the rest models. In this experiment, weadopted sparse categorical crossentropy as the lossfunction and SGD as the optimizer. In the generator,we defined the latent space of with 6 hiddenlayers, a rate of and an activation function the discriminator, we defined a 6 hidden layerarchitecture with 128 per layer, a activationfunction, a of 0. In the 80% of normalsamples were randomly selected for training, and testing dataset from the remaining samples was generated. then encoded the categorical attributes (pro-tocol type, service, and labels (label) andmapped the target variables numerical values. Weevaluate the performance of the models using suchas accuracy, precision, recall, and F1-score. The thirdlayer is a Random Forest (RF) classifier. Normaliser MinMaxScaler sklearn packageare used to transform dataset after splitting it intotraining testing. The discrimi-nators cross-entropy or matching loss is used whether the reconstructed data has featuresto the real samples discriminator. We adopted a ofpre-processing such normalisation, featurescaling, and to prepare dataset. The original KDD Cup 99 42attributes and 494,021 entries. We first cleaned the datasetby dropping missing values and naming attributes ac-cordingly. givesan of all in experimental design. Forthe we trained for 10 epochs with a of The other utilised the AE method as a discrim-inative DNN, where target output is similar input,and the number hidden layer is lower than The AE model defined withone input layer, 3 hidden layers with activation functions oftanh a decoder.",
    ". Research Approach Methodology": "This work seeks to implement and assess existing al-gorithms from the literature within the context of deepanomaly detection. The research will adopt hypothesis-driven approach,postulating that deep learned (DL) models can proficientlyidentify anomalies within intricate datasets. methodologyadopted in this research involves a meticulous and system-atic exploration of deep anomaly detection. investiga-tive process will encompass a combination of theoretical and.",
    ". Introduction": "uch parse nmalies can be apicablein manyaras by analysng activity patterns to detect anomalos be-haviours, manage industrial resources, o ensure productionsecurity. Given a dataset  = {X, X2. Deep nomalyDetection (DAD) aims t learn a mapping function, whichmaps the original space to a nw reresentation space ():X Z,where Z R(K D). , Xn}, the furediension of each saple is D,xi RD. Statistically the parselydistriuted areas indicate thatthe probability o data occurring in a crtain area is relatieylow, where the dta falling in can b considerd to beanomlies. Ina classificatio problem, an nomaly cul bean bservation that doesno fit into any efined classe oris sinficantly ifferent from te other observation. For exampe, in a ime seriesdatast ananomaly migt e a udde change in therendor a spe in the data that is nt consistent with the est ofte sries. If the probability ensityo a samplein thedataset is lss tan the threshold a sallenough value, the saple is cnidred nanomaly and theanomaly score of esample () can be omputed n thenew space."
}