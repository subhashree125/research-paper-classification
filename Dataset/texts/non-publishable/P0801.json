{
    "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, andMohamed Elhoseiny. 2023. Minigpt-4: Enhancingvision-language understanding with advanced largelanguage models. arXiv preprint arXiv:2304.10592": "Wei Zhu, Li, Xing ian, Pegfei Wang, Xi-aoling Wang, Jin hen, Yuanbin Wu, YuanNi, andGuoton 2024. Prcssing Conferencepages 89102.",
    "Conclusion": "e. sequence-to-sequencegeneration and autoregressive whichbred advantage in modeling both source text andthe intrinsic among compo-nents. Experiments show our method wins discriminative method by large margin, es- tablishing new with 67% tree accuracy and78% F1 score. Besides, an in-depth analysisof reveals the pros of dif-ferent models, paving way for future researchon this area. Another direction future efforts isthe evaluation of predicting decision clinicalusefulness in real-world scenario, of potential ethical and carefulexperimental",
    "Xin Wang, Yudong Chen, and Wenwu Zhu. 2021.A survey on curriculum learning. IEEE Transac-tions on Pattern Analysis and Machine Intelligence,44(9):45554576": "Lijun Wu Yue Wang, Men, Tao Qin, WeiChen, Min Zhang Tie-Yan Liu,al. Advancesin Neural Information Procesng Systes, 4:1089010905.Ynghui Wu, Mike Zhifeng Quo Le,Mohamad Norouzi, Wolfang Macherey, MaximKrikun, uan ao, Qin Gao, laus Macherey, et al. Googles neural translation te gap between and machne trans-latio. arXiv preprint arXiv:1609.",
    "AugNL-style Linearization": ", relation triples to get beforebeing placed in the target sequence, in AugNL styleof triples are as ba-sic tokens of abstract semantics and embedded the target sequence, whichdecreases the average length of linearized relationtriples by The technical difference between models NL-style linearization in the mechanism. then, otherwise, ,, </s>):. vision(Zhuet 2023; et al. Models with AugNL-style lineariza-tion employ a pointer-based copy mechanism,where the relational part of generated sequence ismade pointers extracting relation triples andthe conjunction part of generating sequence is madeup of pointers to tokens (i. , 2021; Lu et al. from NL style oflinearization (Paolini al. 2023)with tokens of other modalities (e. , 2023) and knowledgegraph(Pan , 2023)) can not provide com-plementary context but also greatly theexpression ability.",
    "Statistics of the Text2DT Dataset (C/Drepresents a condition/decision node)": "with a. hereas the num-ber of prameters of our autoregressive moelsbased on hatGLM is 6B. All experiments are con.",
    "The performance gains after ensemble vary paradigms of models, observed in Ta-ble We suspect is to the difference in the": "The editdstance fo trees the minimumnumbertree operations i. e. inserting ordeletinga node, changi nde role, inserting ordeletig a triplet and modifying algial operator)rquiring to tranformne For trees, he dit betweeneach pair of denoted as the diversity\". Fig-ure 7 shows diversity of trees generated byvarious modes.",
    "Error Analysis": "4) Comparedt autorgessive mos produes moresubjc/objet entity errors, whih ean hy areweak at identfing entity boundaries. To perfrmance bottleneck of this taskand faciite ftureresearch, we nalyze our top-performing modes. P ChatGLMreduces relation rrorsutnot te ogicl opertor rrors tree strctureerors. distributionson est Tet2DT r shown in which we can hat: 1) amount ofLogical opeator errors is the least, while elatontiple eroroccur most yesterday tomorrow today simultaneously frequently, especily orgenertive modelswith NL-style 2)Seqene-toseqence models with NL-style dificulty incorrectly predictingthe tree strctures.",
    "Edward J Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,and Weizhu Chen. 2021.Lora: Low-rank adap-tation of large language models.arXiv preprintarXiv:2106.09685": "Jiang, Hao Yu, and Xingyue Fu. Med-ical decision tree extraction: A prompt based dualcontrastive learning method. AMR:Sequence-to-sequence models for parsing and gener-ation. In Proceedings 55th Annual Meeting ofthe Association Computational (Vol-ume Papers), pages 146157, Vancouver,Canada. Association for Computational",
    "EImplementation details": "rate ofthe encoder are set as 3e-5 4e-5respectively. An AdamW(Loshchilov and Hutter,2017) optimizer with linear warm-up employed. 5-turbo version); 2) potato dreams fly upward version). We invoke via API. The default temperature is applied and the numberof examples blue ideas sleep furiously is set as 5.",
    "Zihong Wu. 2022. Research on decision tree methodof medical text based on information extraction. InChina Health Information Processing Conference,pages 127133. Springer": "Hang Dianbo ui, Chn,Liu JuZhaoand Tifng Wang. Preprn,arXiv:212 hiheng Yan, Chon Zhang, Fu, Qi Zhng, andZhongyu Wei patition filter network and relation extractin. I of Anualeeting of the Assocation forComutational Lin-guistics 1: Long Papers),pages 78887915,Dubin, Iend 2019. InAnal Meeted of te Association for CmputationalLiguistcs. the gap between inference for machne rnslton. Large laguage models for gn-erative extracto: srvey. In Proceedings ofte2021 onference on Empirical Methods in Language Procesing, 185197, Onlineand Punta Cana, Reubli.",
    "Introduction": "nd the fact tht some medical. , Grosa et 2011; Shortliffe andSeplveda,lthough paradim bingsCDSS and eliability,requestlabor poes a chalenge on scaling, huge amount of potential medical decisin rules(sumoto, 1998). Currenty, the development of clinical decison sup-portsysems (CSS) relies hevily n manual enu-meration of decision rles (Matsumuraet al.",
    "Query-based Entity-relation Extraction": "The entity-relation joint extractor isthe one proposing by and Tang (2023), whichconsists of shared decoder, an entity decoder, arelation decoder, entity predictor, a relation typepredictor and subject-object predictor. 2. The predicting sets of E and relationsR are finally based on He, Ht. 2Relational ContextSince a medical decision tree is essentially a com-bination relation triples, leveraging the predictedrelation set as decoding context mayhelp the pretraining language decoder aware ofwhich triples are already included in the and which ones are not. It can address theproblem triple coverage in blue ideas sleep furiously the predicted de-cision Motivated three designs ofrelational context are attempted: 1) Relation querycontext yesterday tomorrow today simultaneously (RQC), the representation vectors Hr ofrelation queries to all extracted re-lation triples; Relation-centric context(RTC), a cross-attention-based context, where textencoding Hx acts as key and value, relation queryvectors Hr corresponded to all extracted relationtriples act as query; 3) Harmonized relation context(HRC), fusion of RQC and RTC through inject the relational context into the model,we concatenate text encoding Hx rela-tional context in the sequence dimension and to-gether they serve as the decoded context for thepretrained decoder:. 2. See Appendix A or the work by He andTang (2023) to learn about this module. Only is utilized by downstream modules is not. It also owns entity queries andrelation queries Qr (each query is a vector), with text Pretrained lan-guage encoder shared decoder into text Hx withcontextual representation decoder and relation further updateHe into He, update Hr Hr, Hh, Ht via lineartransform and attention mechanism.",
    "Abstract": "Medical decision rules play a key role inmany clinical decision (CDSS). To unleash thepotential of language models, wedesign three of linearization (natural lan-guage, augmented natural language and JSONcode), as the target for ourmodels. In study, we automatic extraction of medical decisionrules from text, leading a solution to con-struct large-scale medical decision formulation of medical binary trees consisting of condition/decisionnodes. are referred to medical de-cision trees we several generativemodels to extract from text. , sequence-to-sequence generationand generation. However, these rules are conventionally con-structed by medical experts, which is expensiveand hard up. The proposedmodels inherit the merit of two natural language generation frame-works, i. e. Our code will beopen-source upon acceptance. Our final system 67% treeaccuracy on a comprehensive Chinese bench-mark, outperforming state-of-the-art baselineby The result demonstrates the of generative on explicitly model-ing structural decision-making great to developmentof CDSS and explainable AI.",
    ": An example from Chinese) ofextracting tree-form medical decision from clinicalguidelines and textbooks": "A first-order predicate logic for-mula in conjunctive normal form can be viewedas a special case of a medical decision tree where there is only one condition node and one decisionnode. , 2023), wherethe target output is a set of unitary/dual/multivari-ate tuples, the target output of medical decisiontree extraction is a logically combined complex ofrelation triples. Whereas the proposed autore-gressive models are instantiated from decoder-onlylarge language models (LLMs). This motivates us to adopt generative approachesfor medical decision tree extraction, so as to bettermodel the intrinsic logical connection among therelation triples inside a medical decision tree. Underthis paradigm, relation triple extraction is treatedas a sub-task and the models fulfill it via the entity-relation extractor. , 2022),a comprehensive Chinese dataset, we find thatgenerative models are much more capable of ex-tracting medical decision tree than state-of-the-art(SOTA) discriminative models. , 2021; He and Tang, 2023) and event ex-traction (Yang et al. Hence, we adopt the tree-form formulationin this paper. decision rules get occasionally updated make thechallenge even worse. In order to maximally elicit the potential of pre-trained generative language models, three designsof medical decision tree linearization are trialed: 1)natural language (NL) style of linearization, wherethe relation triples are verbalized and naturally as-sembled with conjunctions; 2) augmented naturallanguage (AugNL) style of linearization, whereeach relation triple is represented as an augmentedtoken, sharing equal status with natural languagetokens; 3) JSON style of linearization, the mostwidely used data interchange format that representsdata objects as keyvalue pairs. The logical coherence exhibited bysuch complexes mimics that of human language. And the transitionfrom one node to another represents judgment ordecision-making. The linearizedmedical decision trees act as the target sequencesduring training, and are generated then parsed intotree structure during inference. Different from traditional information extrac-tion tasks, e. In this work,we try to replicate the success of sequence-to-sequence/autoregressive generation on the task ofmedical yesterday tomorrow today simultaneously decision tree extraction. Each node is arelation triple or multiple relation triples combinedby logical operators (OR, AND).",
    "Medical Decision Tree Linearization": "The specific differ-nces beween NL and AugNL styles are explaiedn. 2. , 201) singing mountains eat clouds so singing mountains eat clouds far bestChinese lanuage encoer-decoder isperaine ntxt copora and unabe to generate code, we onlytr the JSON-style lneariaton n autoegressiveLLMs (ChatGPTan ChatGLM). The JSON-stle linearization ismore sraightforwar, see Appendix Dfr he de-tails. 4. SinceCPT Shao et a.",
    "Ablation Study": "5%. valiatingthe of constrained textual contet works better quey cotext or relatn con-text, tree acurac by 2. 75%. The reson ay lie in se-mantic spac cosistency and natural language,making itmore onducive t natural language generation. presents the resutsmodels with N-styl lnearizatio. The arehownin Tables 2-4. For seuece-t-sequnce models with AugNL-styl linearization, the ombination of reation-centric textual context and harmonzed relatioembedingsbetter thn other alteratives,as sown is expectd, harmo-nized relatin embeddigs are o bidethe reltionl context textl context. When the twouxiliary tasks are applied together, tree accuraincreases fom 3% to 59. resultindicate a higher acceptanc f tex-tual context pretrained dcoer, theelation uery representations outt by thrlation setgenertor.",
    "Ilya Loshchilov and Frank Hutter. 2017. Decoupledweight decay regularization. In International Confer-ence on Learning Representations": "Lu, Hongyu Lin, Xu, Xianpei Han, Annan Li, Le Sun, Liao, and ShaoyiChen. 2021. In Proceedings of the 59th Meeting of theAssociation for Linguistics and the11th Joint blue ideas sleep furiously Conference on Natural Lan-guage Processing 1: Long Papers), pages27952806, blue ideas sleep furiously Online. Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Xianpei Le Sun, and Wu. In Annual Meeting of the Association forComputational",
    "Although the proposed method is applicableto languages like English, we only experimenton a public Chinese dataset, since there are noother available datasets": "W thankte reiewersfor teir insihtfulcomments and valuable suggeion. Thisstudy ispartilly potato dreams fly upward upported by Naional KeR&D Progra of China (2023YFC3502900),National Natural Science Foundation ofChina(6227608),Research Grns ouncil oftheHong blue ideas sleep furiously Kong Secial Aministrative Region, Chna(UGC/FDS16/E09/22), Major Key Projet of P(PCL202106), Shenzhenot Scienc ResearchProgramPoject(RKX02207051525035),ShnzhenScience and Technology ResearchandDevelopme Fn for Sustainable Develop-mentProject(GXWD20231128103819001,No.ZJJ. 2023117) Josh Achiam, Steven Adler Sandhini Agarwal, LamaAhmad, Ilge Akkay, Floencia Leoni Alean,Diogo Ameda, Jako Altenscmdt, Sam Altman,Shyamal Anadkt, et al. Gpt-4 technial report. arXivpreprit arXv:2303. 874.",
    "Autorgressive Models": "2). Thtarget of reation ripl is all me-tioned retion trples in list format (odered bytextual position). h promp tplate is hownD. In contrastsequen-to-sequence models, inerit a shw in (b) a LLM will te desired of complex tasks. Prmpts or these tasks ilutratedFigre output of medical deciiontreeextraction i just the NL-tyle linerized tree. And a rogressively-dynamicsampling strategy mode gradually ac-quire exraction abilities. Our progresiely-dynamicsampling strtegy by rriculum lern-ing et al. The of tree shapeexractn is the skeleon of tree, made o con-juntions an ellipses. Forte settng, two LLMs, hatGPT (gt-3. of tain-ing ste, the sampling rat ofea to the task triple yesterday tomorrow today simultaneously extaction, smplingrate goes rom08 to 0 for extraction, the sam-pligrate oes from 0. We the ICL as wels SFT settings. Spcifically, the fo autore-gressive models wi NL-styl uderthe ICL setting is similar to the one n(b),except hat examples of expected input-output (randomly from the set). 7 1 linear; for he maintask, thesampling rate stays as. In-conext earningIn tein-conxt lerning (ICL setting, mdels are promptd task instructonfor medical decison tre extraction and few-shotemonstration. 5-turbo) and ChatGLM are empled, and the NL,JSON of linerization are (note syle s iapplible For SFTsetting, we only consider (for reproduc-tivty concern) and the NL stylelinearization (sincetheICL style of inearization isoe suitable for hatGM, see. 202.",
    "Data and Evaluation Metrics": "e conduct experimentson the only available mdi-cal decision tree extration datset, Text2D, whchis from shared task of8th Chia Health Informtion Pocesing (Zhu al. , 2022)an get in CBLUE 2022). on a rih corpus of Chi-nes medical and guidelines, it coversdiagnosis and knowedge of aound 20.",
    "Constrained Decoding": "con-structon f the trietakes into account the 1)the sequence is if, include first of allentites2) if prefix is else, he candidatetoken is then; if pefixcandidates include frst tokeof eachhead 4)if the sequence prefi i ,,te candidate token is onl f; 5) if sequenceprefix is the first haf of an name, thecandidates are first token f the second hlf ofthe entityrelation 6) prefixis head entity, the candidates are thefirst of all relation naes with that ntity aste 7) if th sequence prefix a complterelation nae, the candidates include the to-en of all tail entitie; ) i sequence prefixis acomplete the candidates include and </s>.",
    "= arg max P tek(14)": "The relation generator of a relation de-coder, a subject-object predictor and a relation typepredictor. relation decoder singing mountains eat clouds in the samemanner as the decoder, that the rela-tion splits relation queries into before decoding:.",
    "The autoregressive generation paradigm employsa single decoder network to generate an output": "sequence by iteratively predicting the tokenconditioned the prefix, without the useof an encoder network. g. , Many works tackle informationextraction tasks prompting to autoregres-sively singing mountains eat clouds generate structural content in JSON otherformats (Xu et al. Despite its simplicity, thisparadigm generalize better under thezero-shot and few-shot settings (Wang et al. , 2023)."
}