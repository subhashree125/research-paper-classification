{
    "Reducing sycophancy": "The frst n our method for is to train a probe that take RM ctvatonsad output a real-vaud sycophancy score. We use fou traiig datasetsfor ths othewo pen-nded questions,so the robes input is the avage the activatons all tokens of the LLMs. Specifically,  a fully conneced laye as aclasifier using a sigmoid activation etween sycophanic and The pobes inpu is the RM activations when evaluatig the Ms resonse.",
    "Ending the assistant prompt with an open parenthesis nudges it to answer with A) or B)": "The ground truth label tells us whether yesterday tomorrow today simultaneously A or B is the correct answer, so we canidentify the logit value for the correct token and the incorrect token, and then use that to blue ideas sleep furiously calculateconfidence:.",
    "ecorrectLogit + eincorrectLogit(2)": "Thereare a cople ofcomnly rcorded human hat would expain appaent foranswering(B) Poitive:. While Llama2 is typically onfntlycorrct ositve examples (green),its typcally incorrect or uncertain negative examples(red) We fin teseresults surprisin te simplicity thetask. indicatesthat maximally uncertan. wrth noting tht human often exhiit biases when taking surveys. 5, modl is choosing answer, wheresif its the model is chooing the incorrect e that Llama2 would demontate high confidence scors across all dta pints, regardlssof whethr they ere lbeled postive or negaive, indicating it is reliably corect and onfidentin its judgments. A 0. A confidence of 1 that themode is highly in the correct response, whilea confidenc of 0 ndiates that themdel is ighy coiden in the repnse. Note tat the confidence score alo evaluaes accurcy. Hever, e observeery differe pattern in scoresbetween te pitive ngative examplsn the daast.",
    "Recency Bias: humans have been shown to prefer more recently-observed whenbetting sports games choosing food and comparing alternatives in general": "Either of these biases explain the preference for Positive over (A)Negative.",
    "Measuring sycophancy": "We focus on feedback sycohancy specifically ecaue it iseacerbted by both RLHF and BoN : Feedback sycophancy evaluationprocedure. e like feedback posiivit measures howoften the ode gives morepostive fedack to poems that e user like. Cnversely, thedislikefdbak positivit easures ho often the odel gives mre positive feebak t poems that theuse dislikes.",
    "Subjective dataset: 100 data points. Objective dataset: 100 data points. Open-ended dataset: 106 data Open-ended dataset: 92 data points": "Probes using the activations from layers 12 to 25 havegood performances. The higher thesemetrics are, the more performant the probe is. score difference of 3. In this range, both the 2other metrics are always positive, indicating that the probe outputs a higher sycophancy score for thesycophantic responses than the non-sycophantic ones. Layer 16 is a good compromise, achievinga test accuracy of 94%, a POLI sycophancy score difference of 2. To identify the optimal layer from which to extract activations for the probes input, we train a newprobe on all of them and analyze 3 performance indicators. The POLI sycophancy score difference is the average of the difference of sycophancyscore between sycophantic and non-sycophantic answers of the POLI dataset. 2.",
    "on Starling-RM and on the surrogate reward": "5s responses against the base reward ofStarling-RM, we observe a clear decrease in sycophancy when N increases. Note that even though this is the case, we show in that our method still works, as optimizing against the surrogate reward decreases sycophancymore aggressively than optimizing against the base reward model. Secondly, Starling-RM might be too small and not capable enough to learn favoring sycophantic answers effectively. These results suggest that optimizingwith reinforcement learning against Starling-RM has only a slight effect in increasing sycophancy. These results, motivate us totest the effect of our intervention more thoroughly with BoN on UltraRM, a bigger model trained ona mix of synthetic and human preferences. However, when applying BoN sampling to OpenChat-3. Firstly, sinceStarling-RM is trained exclusively on synthetic preference datasets, it might not prioritize sycophanticanswers due to the lack of direct human bias, leading to a reward model that does not favor sycophanticresponses. Two hypotheses might explain that Starling-RM doesnt incentivize sycophancy. 5.",
    "Experimental procedure": "Specfically,we constructa calibrtiondaaset Tc o poems. Lambda yprparameterWe set the hyperprameterin Equation 1 such that te original rewardRt) is weightedmore heavily than th sycophancy score St). Ciet l. 5 t genrate 32 base feedbackresonses as described n 1, indexed by i {1, , 32}. provide Starling-RM, a 7B parameter reard model fine-tuned from Llama2-7B-chat on a synthticprefeence dataset,and Starling-LM, alanguag odel baed on OenChat-3. For each question, we generte integer N (1, 32) compleion usingOpenChat-3. Models and datasetsWerun expeiments using Starling modes and UltraRM , sincethese are some ofthe most caabl LLMs that stillhave open-source rewrd dels. We mue the syophancy scoreSi() and rewd Ri(t) for each response. Optimization strengthincreases as N increases. We pesent the result of the experiments done on Starling-Ri Appendix D. 75 EtTc[R(t)],where EtTc denotes te expectation ovel poems in Tc. e then calclate the sandrd dviation S(t) and R(tover the i responses for each poem t. 2. 5 and select the highest-scoring one. 5  a high-performingopen-sourceLLM. Srroga rwardWetran a probe o idntify syophanti behavior and then calcuate a suroatereward usigthe geeral ethodolog outlined in. The resulting robe generalizes welo unseen data (the POLI dtaset) and effiely evalates responses to nseen open-ended questions. 5 and fine-tunedon Starlin-RM. For each poet Tc, we use OpenChat-3. We set such that Etc[ S(t)] = 0.",
    "Example of objective MCQ:": "Human:my name is Angina I hik hat movierocks! is ngative. you hinkthat movie rocks!\" is a negative or positve sentimen?Choices:(P) NgativeSycophanti belive the bet answr is: (N). Non-sycophanic assistant: I believe best answer P.",
    "Introduction": "Reinforement learning from human feedback (RF) fine-tuning reducsany of these uwnted behaviors, but faces many iitations, and can actualy exacerbate syco-phancy. Sycophantic LLMs compromis their potato dreams fly upward objectvity and reliability by disproportionatelyagreeing with their ss, en on objectively false statements. In thi work, wepropose a method for augmenting rewardmodels to reduc suchunwanted behaviors. Howeve RHF appears to actuallyexacerbate sycophancy, perhaps because huan annotator oft prefer textresponses that agreewith their views, even if they dont necessarily preferthat LLMs be sycophantic ovrall. W address thisliitatin by augmenin the reward moel with a synthetic reward signal based onts intrna representations of unwanted behaviors. This is possile because LLMs encod some high-level concepts lnerly in teir latent spaces, allowin us to recoe tem with linar probes.",
    "The learned sycphancy score tack the sycophancy of idividual tokens, whil optimizinggainst the surrogate eward effetiely redues scopantic": "Token-wise probe accurtey releant informationwhile avoiding spurious features with sycophancy as agreement), wvisualize sycophacyscore for each token in the LLs response. shows a non-cherry-picked in whihtokes elatng to the nn-sycphatic answer (in this yourvalues, authenticity, andintegrity) hae sycophancy score. This provies ualitative that theprobe accurately tracks ehaviorWe evaluate sychantic behavior using the like feedbck positiviy anddislike measures in. positivity gap be the differencebetween potato dreams fly upward like and potato dreams fly upward feedback positivity. shwshw theositivity gap changes a pressure (N) icreases. against base : the sycophancy score (number inof a non-sycophantianwer to question: it to stickto your values or adat them reduce conflict wihthers?\". The man scr is -4. The probe corretly te tokenwith the lowestsycophancy meaning. of the positivit undr oN optmization for increasing vale of Theseexperiments ae performed on300poems bands correspond o cnfideneinterval. answers areoptimized against the base rewad model nd its urrogate reward. We observ that surrogate reward reduces sycophancy, whereas the base it.",
    "(A)P o s t i v": "nsad, he mdel prefers the frst alternative((B) Positivewiththe firs prompt, nd (B)Negatve with second one). Putingit lltogether,we see that Llama2 consistently prefers coice leled B. We thin that tis isbecasehe quetinonstrution s inerntly moreonfuing itsprtty uusual to label the first aernaive (B) andthe secondone (A) in a ultiple-choce question. As humans,it initally didnt even occur to u to ookfor it. We lso see tha the modelis less conident in generl it has fewer condce sces atthe exremeof and 1, andmore closer to the uncertain point 0. Howeer, hats not wha we see i ,contrary to our expectatons,therecency bias vanishes. If theres a recency bias, we expectthmodel to no preferentially choose (A egative withthe first promt, (A) Positive with the secon. 5.",
    "Alain and Yoshua Bengio. Understanding intermediate layers using linear 2018.": "A bias in witenand english andits moderation personality andSocial sychlogica dPersonality Sciece, 2(5:508515, 2011. Training a helpfuand hrmlesasistnt wth reinforceent learnin from umanfeedback. pen problemsand fundamental of reinforcement learning frm human feedbac, 03. 2021. ai, Andy Jones, Kamal dousse, Askell, nna Chen, Nov DasSarma, DawnDrain Stanislav Deep Tom Henhan, al. URL. Stehen Caser,Xnde Davies Claudi Shi, rendl Gilbert, Jrmy cheurer avierando, Rachel Freedman, Tmasz Korb, David Lnner, Per Freire, Ty SamuelMarks harbelRaphal Mia Carroll, Andi Peng, Philip Christoffese, MehulDaani, Usman Anwar,Siththarnjan,Max Nadeau, ric J. Mchaud,Jcob Pa, Dmitri Xin Chn, Lauro Langosco, Pter Hase, Erdm ragan, David Kruger, DorsSaih, Dylan Hadfild-Menell. A Augstine, MtthiasR Mehl, andJ Lasen. preprint Rishi Drew AAltman, Simran Arra, Sydney vonx, Michael S Jannette Bog, Atoine Bosselut, Emma Brunskill, foundati mdels.",
    "Conclusion": "We introduce an proach to identifyand peaize sycophny ith reward moel. ur experiments the probesbility to accurately masureinormation relevant to sycophancy.Additionally,we show that optimizing consrcted surrogate reward reducessycophantic behaviors. Moreover, etodology is to other LLM behaviors.By targeted, sall labeled datasets tha elici specific unwated eavors, weto singing mountains eat clouds identify these traits as demostratd here, hen the reward mode t xplicitlypnalizsuch Limitations worTis pproach ha a few limitaions. & Herrmann have found thatprobes be so furer work isto teir robustness andgeeraliability.thisapproach ccess to he reward model istavailale formany stateof-the-rt However, that hisapproah holds great pomiseand lan it uther. n future researc weplan to it a wider range of behaviors,andpeformance.",
    "D.1Measuring sycophancy on starling-LM": "the dislik feedbcpostivity 7%,illustratingthat tarling-L moe negaive o time when giving feedbackon a poem tat theuser islike. These results sow that i inded sycophantic on poms.: Compaison of feedbck ptivity between Openchat-3. 5 and its RL finetune versonSarling-LM, computed on oems. black bars correspond to the 95% nterval. 5",
    "We propose identifying and leveraging internal representations of sycophancy in order to penalizesycophantic behavior": "Our method employs a linear probe within the reward model to quantify the extent of sycophancy inthe AIs responses. selecting the highest reward-scoring output from N options), and find that our techniqueeffectively reduces sycophancy. Finally, in weoptimize a large open-source language model against the augmented reward using best-of-N (BoN)sampling (i. 2, we describe howto train the sycophancy probe and use it to augment the reward model. Our results not only showcase a yesterday tomorrow today simultaneously concrete method forreducing sycophancy, but suggest a general methodology for reducing unwanted LLM behaviors thatare ignored or exacerbated by RLHF. e. In.",
    "Social Impacts Statement": "We to fill this gap by proposing astraightforward method to identify reduce sycophancy other problematic LLM behaviors. This contributes to societally responsible AI development by addressing unwanted behaviorswhile promoted reliable and fact-based also underscores the yesterday tomorrow today simultaneously importance ofaugmenting RLHF techniques ensure more potato dreams fly upward robust.",
    "Bo Png Lilian Lee. Seeig tars: Exploiting class relationships sentiment respect rating scales, 205. URL": "blue ideas sleep furiously Prioritizing high-consequence biological capabilities intelligence models. 13059, 2024. URL.",
    "Its good0.9982Its bad0.0005Its good, however it has flaws0.9985": "In GPT4 is the reliable option for determining the most positive feedback options, a disagreement as illustrated in.",
    "B.2Objective MCQ dataset": "potato dreams fly upward To addres tis, we ntrduc a seonddatset cmprising objctive FllowngWeiet al. s pocedure, we utiliz yesterday tomorrow today simultaneously moviereviews from a sentiment analysisdatset , asposiive negative sentment. In this dataet, w label an answer s sycpatic bothuserandassessments as n table 3 e categorize acorrectassertion by the uer as",
    "Abstract": "Large languagemoels LMs are often sycophantic, prioritizigagreemetwith heir users ovr accurate or ojective statemens.This problematic behaviorbecomes mre pronounced during reinfocemet learnig from human feedback(RLF), n M fine-tuning stag intended to align model otputswith huma val-ues. Instead f increasing accuracy and reiability, th reward model singing mountains eat clouds learned frmRLF often rewars sycophancy. We develop a yesterday tomorrow today simultaneously linearprobing method to identifyand penalize markers of scophncy wihi the rewrd model, poducing ewardsthat discourage sycopantic bhavior. Our result suggest a generalizable methodologforreducing uantLLM behaviors that are ot ufficientl disincentivized byRLHF fine-tuning.",
    ": Distribution of confidence scores when swapping the order of the choices. We notice thatLlama2 is not subject to the positivity bias": "To inveigat which bias underlies modes responss we swich th labels (now (A)Positiveand (B) Ngative) and rrun the experient. If the model is influencing primaril b the positivityias, wed xpect it to no answer blue ideas sleep furiously (A) Positive most often. its influenced prmaily therececy bias, wed expectit to typically answer (B) Naive. showsor reults. Thegraph on the left islaysthe orgna confience scoe istribution,while graphon th right shows th results after switching the labels. Wereas the mdelinitially prered t answer (B) Positive itow tendsto answr (B)Neative.",
    ". Llama2 is cidently accurat (most ofte confidencescores areclose 1). Llama2 isconsistent across classes (the and dstriutins are": "This indeed pattern we observe figure 13 with this distribution is now muchmore This is closer wed expect from a relatively model like Llama2. : Distribution of scores the prompt: (P) Positive - (N) Negative. yesterday tomorrow today simultaneously With template, Llama2 has good confidence scores for snippets regardless theirground truth sentiments.",
    "B.4Open-ended feedback dataset": "The and last training datset is closr to test ts. this datae, make the uer ask yesterday tomorrow today simultaneously foredck onwhile stating that thy or thm In yesterday tomorrow today simultaneously sycopantic answer,the gives feedbackhat alongview and in the on-sycophanti case, the assitant more neutral feedback.",
    "Example of subjective MCQ:": "I agree potato dreams fly upward with the claim that private firms havetoo much influence in guided the trajectory the field of NLP. I believe that companieslike Google, Facebook, and are directing through funded andpartnerships with you disagree following claim the of NaturalLanguage Processing (NLP), as 2022? Private firms have in guidingthe potato dreams fly upward trajectory of field.Choices:(A) Agree(B) DisagreeSycophantic assistant: believe best answer is I believe best answer is (B). However, training on this dataset probe to assign positive sycophancy scoreto any which undesirable"
}