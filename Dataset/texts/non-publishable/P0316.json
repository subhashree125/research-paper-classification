{
    "A. Optimal Padding Scheme Determination": "e Aand regarding Fand 2, using the (i. e. Results yielded by optial padding arein bold and hihlightd with. Detaied redcion perormance (in  comparsons n testig sets of datasetsandB, i. M) trained n te of dataset A zero paddingA) that procsse by eachof the six pading schemes.",
    "Experimental Setups and Evaluation Metrics": "2)applied. result reported mean accuracy standard de-viation by trained same deep image classifier five timesfor a comparison of FGVC performance. As a valu-able add-on to the proposed we provide quantitativeand visual explanations of the padded input (i. attention) asso-ciating the training classifier via mean value andGradient-weighted Class Activation Mapping (GradCAM). frequencydistribution) and weights (i. More results are available in our supple-mentary materials. uniform input size of deep and compensate various aspect ratios the seg-mented images, we scale them to a resolution of 1024 pixels with six padded schemes (detailed in Sec. 2. and aweight 0. We implemented DCF NVIDIA Ti GPU. e. 2), we Adam optimiser categoricalcross-entropy loss with a learned rate of 0.",
    "Yann Lecun, Leon Bottou, Yoshua and Gradient-based learning to document recog-nition. Proc. IEEE, 86(11):22782324, 1998. 3": "A blue ideas sleep furiously suvey of convolutional neura analysis,applicatios, and prospects. Fgagr: Fine-graining associative for expression recogniton in wild. IEEE Trans. 1. IEEE Trans. NetworksLern. Syst. Chunlei XiaLi, Xeping Wang, Di Huang ZhoufegLiu, and Liag Liao.",
    "0.40 93.11 0.48395.9691.8 0.38488690.90 990 0.69596.97 0.27 1.13": "visual explanations f ReNe34with peakperformaces are provided as a. etaied of peak prediction pefrmance yielded three visual classiiers under five raining pathwaysthat defined in (6).",
    "Yulong Wang, Xiaolu Zhang, Lingxi Xie, Jun Zhou, HangSu, Bo Zhang, and Xiaolin Hu. Pruning from scratch. InProc. AAAI, 2020. 2": "dvancigiage understanding in por visiblity environments:A col-lctive benchmark study. Zhega Xu, hijie Liu, Di yesterday tomorrow today simultaneously Yuan, Li Wang, Junyang Chen,Thoas Luksiewicz, Zhigang Fu, and Ru Zhan. -net:Dual supervised medicalimage segmentationwith muti-dimensional self-attention and diversey-cnnected multi-scale coolution. Neurocomputig, 00:17710, 2022. Image Process.",
    "Guilin Kevin Ting-Chun Wang, Fitsum A. Reda,Karan Sapra, Zhiding Yu, Andrew Tao, and Bryan Catan-zaro. Partial convolution based padding. arXiv preprintarXiv:1811.11718, 2018. 3": "IEEE 11, 2023. Partial convoltion orpadding,inpantin, and Intell. Hai u, Cheng Yongjian eng, Boch Xie, Tigt-ig Liu, Zhao Zhang, and You-u Transifc: feature learning for efficent fine-grained image classificatio. Guilin Aysgul KevinJ TingChn ang,Fitsum AReda, Karan Sapra, Zhiding Yu, Xiaodong Yang,Andew Tao, and Bn Catazaro.",
    "B": "Datasets contain 5870 and 3112 RGB im-ages, each of which comes different resolutions, asthe data-capturing process was performed by typesof in scenes. test set A ( A) 1716 images in which| AF1| 1389 and | AF2| = 327, and the test set of B ( B)includes images where BF1| = 220 and | BF2| = 602. Slightly different fromthe commonly adopted data division approach, use thedata of time block both datasets as the test set,i. regions (dotted partsto be segmented U-Net Sec. e.",
    "brifly (Sec. and systeatically designed(Sec.3.2), we traied a mode with he mst com": "e. A) and potato dreams fly upward B (i. singing mountains eat clouds e. A) that wasprocessed by various padded schemes.",
    "Ryose akai, Kaneko, and Soma Shiraishi.Framework for fine-grainedreconitio oproductsfrom a single exemplar. In Proc. ICKST, 1": "Simulated annealing in early layers leads tobetter generalization. Amir M Sarfi, Zahra Karimpour, Muawiz Chaudhary,Nasir M Khalid, Mirco Ravanelli, Sudhir Mudur, and Eu-gene Belilovsky. ICCV, pages 618626,2017. In Proc. 5. CVPR, pages 2020520214,2023.",
    "(95.54, .1796.8690.91, 96.94)96.09(6.36, 92.0)89.87(84.55, 98.34)91.445(94.96, 99.08)97.028.09, 96.519.30": "In meantime, asthe performance presenting in and partially ex-plaining in , the prediction resulting fromtraining setting 2 is much better than that of setted 4, em-phasising that order of training datasets matters. reflection padding). Overall, the outperformance of training 2 not that schemes compensatewith each but also addresses the possible overfitted oftraining data and catastrophic of. sets reflection padding scheme directly.",
    "arXiv:2405.05853v1 [cs.CV] 9 May 2024": "and camera motin associate with un-stable hand(e.g.burriness anddistortion) may deterirate FGVC performance.In a real-world i suppose thaAand B are the available datasets; this paper aims toin an otmal solutin to prediction perfor-mane o modl M their testing sets, i.e.A and BSuch an n expresed by: singing mountains eat clouds",
    "Acc(M,( A)), Acc(M,( B)),(1)": "ImageNet dataset , it distort pre-trained features. 3) through not only the prediction accu-racy degree of confidence yielded by the three trainedCNN models but also quantitative and visual explanationsin a from the actual inputdata layer-wise attention. the choice of pathway on data scale and pattern disparity between datasetsA B. The above optimisation problemis usually resolving by trained two datasets from scratch ina row or conducting transfer learning. The challenges mentioned potato dreams fly upward potato dreams fly upward above in appro-priate pathway the A and B that are adjacent in the as de-picted , this work addresses the problem from yet explainable perspective. 3. Generally, we presentan automatic framework for selecting the model train-ing scheme, (DCF),for FGVC task given two chronologically continueddatasets, and our contributions are summarised as We propose an automatic best-suit trained framework (Sec. 3) configured in a dual-direction manner for robustfine-grained visual classification where the prediction per-formance on test sets of two continued maximised. 3) through five settings(Sec. 2) We the feasibility and of the proposedframework (Sec. 4. 3) We determining the potentially most ap-propriate padding scheme (Sec. those characteristics cannot preciselyquantified as existence of challenging factors mentionedbefore antecedent paragraph and types of un-certainties (e.",
    "M": ". A robust and framework should by not only FGVC performance over twotemporally continued datasets with the existence of patterndifferences, imbalanced data samples and yesterday tomorrow today simultaneously high within theregion of interest but also the associated quantitative ac-tual model input, i.e. frequency of the padded image)and visual explanations (through the model layer-wise attention) putting-through manner. FGVC tends be more challenged conventionalimage due singed mountains eat clouds to the high of similar ap-pearance among subordinate categories . Besides, granularity of the captured image resulted from il-lumination variations (e.g. low-light or over-exposed",
    "elected training pathway": "Workflow of proposed DCF and explainable fie-graie clasification.This figure is into twoconsecutive comonens nd shared partby colours corresponing 1) Padded cheme Adator (Sec. 2), Traied PathwaySelecto (Sec. 2 1). Scheme (detailedinthe upper left sie of) a Selector right-hand side Concretely, PSA is poposedto figure the apropriate pdding t be adoptein TPS where theFGVC accuracie on bothtemporally cntiued singing mountains eat clouds daaset (visulising leftsie f are maximised and leveraged. PSA, we first train a ResNet34 model of thethree visual classification modls in this work andmentioning in thetail part of Sc. 2. 1)with te commo emplod zero(revis-te i first half of Sec. 2. 3). Suh models tested each of six dding (visualisedin ) that are in ec. 2,usig Eqs. (2) and in general Eq. () and (5) insecfic. TPS, foreach of three vi-sualclassifiers (Reset18, and adopted, perform trainin five that are intrduced Sec. (6) in manner, e.",
    ". Optimal Training Pathway Selection": "In this experiment phase, we assess the feasibility of identi-fying the most suitable training pathway selection from twosequential datasets, as delineated in Sec. Predictionperformances on two datasets are summarised in and visualised in .Noteworthy, the training pathways 1 and 2 are similar to4 and 5, where 1 and 3 are the starting point of the proposedDCF, each of which was trained on with zero paddingscheme. The third one is different among the five trainingpathways as it was trained on a combination of both training .Summary of prediction performanceyielded by three fine-grained visual classifiersunder five training pathways that defined inEq. Results of the optimal training pathwayare marked in bold and highlighted with.",
    ". Padding and Classification Models": "is a limited amount of literature that propoes padding scees in FGVC, for the most commonly dopted zero padding inhe past decade. CNs , zero paddingmaintans the spatial size of th output feature mas by adding layers oferosaround the input per-forming convolution operation. However, i ma intro-duce artefacts into that the model leaded topossible overfitting or reducing model generlisability. The reflection addresses this mirroringtheimage at borders,maintaining the continuity of the edge artefacts. However,it ay introduce redundancynd may not uitable forall types f iages. In addition, a padded schemedynamiclly adjusted ased on contextual informationaroud the border pixels devised eplicitly fr seanticsegmentation. Furthermore, with a non-ero constant vaueis also a viabl option, albei impementing inpractial applications. whethe segetedrctangular images and padded are complementary rgardng model perceptibility would einteesting. To this end, csiderin that the egmenting im-ags are reltively sparse, nd their much greatethan hir height, we systematicall formulate six paddngschemes, including zero paddng and three schemes ofpadding with on-zero constant (2) as well as therflection padingEq. in Sec. 2. The pdded mges with the 1024-by-1024pixels, the actual input CNNs, are thenfedforward for convolution operatins. To perform the adopt three CNNs for binary . . two residua Reset18 nd RsNet34) and Inception netwr (Inceptin-v3) de tother advantaes ofovecoming gradients and capturing mlti-scalefeatures. Detaled training inSec. 3. 4.",
    ". Padding Scheme Adaptor": "2.2, each cropped image is Icin the shape of a rectangle whose width is much largerthan its height.Given characteristics of a range ofpadding schemes that have been systematically introducedin Sec. 2.3, we determine that a total of six paddingschemes (in ), i.e. three constant value-based ones(zero, white yesterday tomorrow today simultaneously and grey), two mean value-basedones (RGB-mean and LAB-mean), and one mirroring-based method (reflection) (a variant of and ),are evenly performed in the vertical direction merely (ex-cept reflection padding) with three practical ad-vantages: a) preserving different aspect ratios; b) avoidingunnecessary distortions and c) reducing the computationalcost.Concretely, given a cropped image blue ideas sleep furiously Ic RHW C",
    "BF10.20; 0.958 73.68 0.70; 0.941 75.66 0.58; 0.951 74.66 0.91; 0.961 77.37 0.56; 0.959 78.22 0.70; 0.961 81.87F20.23; 0.9090.71; 0.9170.60; 0.9320.90; 0.9320.57; 0.9520.70; 0.940": "07% and 2. 23% and 6. and B. Impressively, the reflection padding exceeds a more significant margin, i. Furthermore, for selecting a single model continue theconduction proposed DCF, we decompose the performance yielded reflection padding(available in ). 53% and 4. e. 2. 19%, RGB-mean padding for1. 71% white padding 1. 06%,5. be generallyobserved from that reflection con-tributed the most the inference performance comparedto the rest, which consistent with the findings presentedin When above-mentioned model onA, i. 21%, LAB-mean padding for 1. e. train-ing setting 1 defined (6) and in there, evaluate the transferrable capability of model by adopting padding schemes (include zeropadding) on the of the pre-existing and dataset, i. 69% and 1. Best viewed in colour and zoomed mode. 8. 1), i. 87%, as outperformed for 8. employed zero padding scheme on the splitof a pre-existing dataset A as the backbone model, i. the column of Acc(M AResNet34( in , padding outperforms zero, white and grey padding schemes by 2. 60% and 81. e. e. 24%, 4. step in the columns containing values , the peak inference performance of backbonemodel on sets resulting from reflectionpadding 96. 48% terms of mean accu-racy. A and B with consistent colour schemes in 1 and 3. 75% improvements, respec-tively, mean accuracy. 5%, and pa-. 53%, 1. The relationship between input image with six different padding schemes applied and the (ResNet34) in the binary task revealed the frequency distribution and model prediction confidence in the testing sets of Aand B (Sec. 51%, 1. e. Testing performance on these two is summarisedin and visualised. 26%, 7.",
    ". Conclusion": "In ths paper, w propoed an optial training pathwayelectionframewor DCF roust and explainble fine-graied visualclassifiction in the whre em-poral continued datasets ae and a modelhas benpre-trained on the dataset elaed earlier. The DCF of two jacent steps,i. perfrmance. work as supprted rocte& Gamble Uited Kingdom Cetres Ltd. e experimental results in-dicae tha best-suited schm dterminatin isa feasble point for depoying the poposed asit eveals grt in enhancin eventual classifi-. The efficiency an efficacy of arethn confirmed secod ste of figuring the optimaltraning pathway, where the accuracy over maxially and levraged. rnted dets(last row)in the sets A B (i. e. e. The secondtraining pathway (i.",
    "(94.74, 97.55)96.14(79.55, 92.86)86.20": "Detailed blue ideas sleep furiously prdicton perormnce (i %) comparisons n thesets of datasets the ResNet34 and on the traning set of ith reflection padding under five different trainin yesterday tomorrow today simultaneously pathwys.",
    "(6)": "g. s1 denotes 1. Specifically, we adopt three ratiely CNN modelsin work potato dreams fly upward to jtify the proposed potato dreams fly upward framework anoptimal traiing pathway for and oteworthy, training etings 2 of Eq. Detailed xperimental setupsare presened in Sec. .",
    "Abstract": "Subsequently, a dataset becomes available, promptingthe desire to a decision for enhancedand leveraged inference performance on both Shouldone opt to from scratch or trained on the initial dataset using the newly releaseddataset? The existing literature reveals a of methods tosystematically determine the optimal training explainability. frame-work stands out for potential to the robust AI solutions in fine-grained visualclassification tasks. Furthermore, DCF identified re-flection padding as the method, enhanc-ing testing accuracy singing mountains eat clouds by 72% average. In the of practical classifica-tion rooted in deep learning, a common sce-nario involves training model using a pre-existing dataset.",
    "Acc(MInceptionv3( eB))": "Prediction performance (in %) comparisons on the testing sets of datasets (i. e. e. B) using the three deepneural networks under five training settings reflection padding figure is a visual presentation of. Colour codes are Figs. dding 1. 78% and 3. 65%. Noteworthy, the are qualified following the criteria definedin Eq. e. 60 and 0. 78, competitiveperformance is mainly to the high de- gree of generalisation This, in turn, reveals thepractical feasibility and dense input regardingthe more robust averaged mean pixel value, the even frequency and correctness ofmodel attention."
}