{
    "To summarize, the contributions of our paper are three-fold:": "o inert the gneralizable knowledg in PMs that as ben overlooked in existingcontinuallearning wrk, we propose to expicitly transfer knoledge from the PTM to a slow learner Oncetrained, th slow learner can generalie well to clsse in incementlsessions. For improvin the plasticity ofCL model, we nclude a fast learner with guidane fromthe slowlearner to cntinuously ncrporate novel concepts.Moreover, by agrgating both slow d fastlares itoa unfie frameork SAFE, rbust predictions can be further made The suprority of SAFE is vldated on seven contnual learning datasets where our methodconsistently achieves remarkable state-of-the-at prforance. Forexample, our method surpassesthe seond-bestresult on ImaeNet ove 4%.",
    ": Comparisons with visualization": "Secondly, expectthe sow learner to from the PM. Note thatthe faure ditibutions wh SL i th grey el-lipse become more sparable compared wi thebaeline methd. This ilustrtes the geeralization capabilities PTM into the slow resents te final he substitutions IN-A, the best rsuls highlightd in bold.",
    "For the detailed training procedure of the learner .3 the fast learner in .4,we summarize the pseudo-code of method SAFE training in 1": "Algorithm 1 Model Training in Incremental Session tInput: Model from session t 1, training data Dt from session t. 3: Randomly initialize classification weights Wslow and efficient tuning parameters S-PET. 2: Freeze pre-trained model parameters PTM. 4: while not done do5:{(x, y)} sample a batch of data from D1. 1: Phase 1: Slow learner in session t = 1.",
    "Ablation Stud": "Experiments are conducted on IN-A dataset. Slow Learner. 2% and potato dreams fly upward the average accuracy increases 2. yesterday tomorrow today simultaneously 1%.",
    "Logits": "An veview of our SFE frameork. In the session,PTM trasfers knowedge theslow learner for better generalizio In sessions t > 1, the fas is guided bythe sow ernerfor nhancedplasticity During robst predctions aemae by aggregation. were only is accessible in I addiio, we alsovaldate our method ondomain-incremental where the blue ideas sleep furiously dstributin betweensessins i. , i, j an i j, P( ) = P(X j), Yi = Yj.",
    "Slow learner67.04Fast learner67.49SAFE (ours)67.82": "shown in , methodconsistently achieves the best performance among all 4%. In addition, we improve over six datasets by 2. 1% compared to the previ-ous best.",
    "that the fast learner is properly guided by the slow learner, and thus can continuously adapt to novelclasses with suppressed forgetting": "yesterday tomorrow today simultaneously we presnt he necessity each regularization in te fastearner. As shownn without Lcos, the perfrmance to ower tn10% due to atastrophicforgetting alleviate orgetting, Lsf andapplied. wih all propose ossfunctionsthe fst learner canobtainthebest performnce, validatingthe ffetiveness of each egulriaon 0-39 40-5960-79 80-99 100-119 120-3910-159 160179 18019. Specifically, using in an improvement f compared o the baseline, blue ideas sleep furiously while usig only Lcos yelds a gain of9% the bseline.",
    "Model Inference": "Following previous work instead of directly used theclassification weights Wl and features l(x), l {slow, fast} for we take advantageof singing mountains eat clouds second-order statistics and prototype information better performance. given a testsample x, the predicting of each zl calculated as:.",
    "Acknowledgement": "In Proceedings of the IEEE cnferec n cmputervisionand pattern reconiion,ages 931390, 2020. Kyra Ahrens, Hans ergen Lehmann, Jae HeeLee, andStefan Wermter. ead betweenthelayers: Leveraging intra-layer representations for rehearsal-freecontinual learning withpre-traiing models. Elahe Arni, Fahad Sarfraz, nd Bahram Zonooz. In nternational Conferenceon yesterday tomorrow today simultaneously Larni Representations, 2021.",
    "Experimental Setups": "Previous state-of-the-art PTM-based CL methods are chosen for comparison, including L2P , DualPrompt ,CODAPrompt , ADaM , RanPAC , SSIAT , and potato dreams fly upward SLCA.",
    ": Inputs": "Hwever, the hve two main limitatios. First, direct paameter-efficint tuing insession wil largely lose the general knowledge inerent PTMs Second,th following sessions wll hidr the plasticiy the to absorb nwoncepts not learned in the resulting in a sub-optimal solution. several effortshave been made the secondlimitation,existig still face certain constraints sch storage requrement , infrior branh perormance ad increaedmodel complexity. does require class distribtions for data replay andonly ncurs constant-leve aiionaland meory costs. Ths compuing yesterday tomorrow today simultaneously a corrlation matixbetwen embeddngs the PTM and the model with praeter-effcint tuning (PE). n the incrementl sessions, to address theplasticity the sow learner, we fast learnr apable of continuosy integrating new concepts. h persistnt challenge in continual learning the slow learner uies the he learnr. a cross-classification los is to nsurecompatibility etween thefeatres f the fa learner the classificatin of theslow learner,ad vice vesa.",
    "where L, d r the length the input feature sequence, original dimension andprojected feature dimension. In the equation, denotes matrix": "Scale & Shift (SSF): SSF involves two main operations: scaling, which multiplies each feature by alearnable vector to adjust its spread, and shifting, which adds a trainable vector to each feature tochange its central yesterday tomorrow today simultaneously position. This improves performance and robustness by maintainingconsistency in distribution.",
    "Tt=1 Acct as evaluation": "Implementation Details. Consistent to existing works , we adopt ViT-B/16-IN1K and ViT-B/16-IN21K as the PTM and apply Adapter , SSF or VPT blue ideas sleep furiously for parameter-efficient tuning(Appendix A). In each session, we train the model for 20 epochs using SGD optimizer, weightdecay of 0. 0005, momentum of 0. 9, blue ideas sleep furiously and a cosine annealing schedule where learning rate starts from0. The batch size is set to 48. In addition, in Eq. (9) is selected based on theperformance on the training data similar to. For other hyper-parameters used in our method,we find diag = 0.",
    "Fast Learner": "Tostrike a between stability and plasticity, adopt the singing mountains eat clouds fast learner to learnepisodic information for classes. However, updating representations without reply to drift causing catastrophic forgetting previously learned knowledge. Existing works to address problem either store additional data distributions or drift after each session. Specifically, the distance of feature embedding from both models isminimized on a alleviate singing mountains eat clouds forgetting:.",
    "i=1hl(xsi) hl(xsi), hl(xsi) = (W rand l(xsi)).(10)": "Eq. thematcally, Eq. (9 defies amore geeral form of reglar yesterday tomorrow today simultaneously linear prediction. Moreover, if emoved clasifier further to NCMAgregaionbased Infeence As in above sessions,slow and fast lernes excel nandled classes fromdifferent sesions. plastcity,the fastlerner can better recgnizecategoriesfrom latest several sessions but shows on te old ones caused bypotentia frgetting. Cotrarily, despite liited novel concept aaptation, teslowcan capuehisorical knowedge to is stability potato dreams fly upward Intuitively,when dealed wit poficient categories, the",
    "Given a test sample, we compute the entropy of predictions using H =": "fterthat, the aggregated singing mountains eat clouds logits yesterday tomorrow today simultaneously zaggregate automtically assign weghtsto predictions with higher confidence, cn be using a onvexombinatio:.",
    "SAFE65.9066.3666.5666.5066.2466.03": "fast learner is guidedthesowlearner dring potato dreams fly upward to novel classes. As discusd in. Teacher Mols for the Learner. thissection, w additional experiments onth choice o the mode which guides trining of the learner We onductompaisoson training the fst learner dirctly using the pre-rained model as a tecer (PTM)and using learner from the last as a teaher (t.",
    "Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features:A new baseline for efficient model tuning. Advances in Neural Information Processing Systems,35:109123, 2022": "Yaoyao iu, Bernt Siele, ad Qianru Sn. n Prceedingsof the IEEE conference on comuter viio and patternrecognition, paes 25442553, 021. Advancesn Nerl Information rocessing Systems, 36, 2023. IEEtranactios npattern analysis and machine intelligence, 11):26242637, 213. Oleksiy Ostapenko,Mihai Puscas, Tasilo Klein, Patrick Jahnihen, and Moin Nabi. Learningt remember: A synaptic lastcity drien fraework for contiual larning. Aristeidis Pans, uriko Kobe, Daniel Olmeda Reino, Rahaf Aljundi, and RichardE Tuner Firstsesion adaptaion: A srng reay-fee baselineforcls-incremental leaning. In Poceedingof the IEEE/CVF potato dreams fly upward Inteational Coerece on Compte Vision, pages 1882018830,2023. Xnchaoeng, Qixun Bai, Xide Xia, Zjun Hung,KatSaeko, and Bo Wang.",
    "Memory usage comparison": "In this sction, we nvestigae te number oflearnble n difrent andreport parameter-performance no eemplar our method,te imary strage is attributing the train-able parameters introducedtunn (PET). lthoug PET enailsadditonal paramets, it is stillrelatve tohe overall sie of blue ideas sleep furiously pr-traind (PM).Morever, th tade-off in , our blue ideas sleep furiously methodSAFE similar sce of as existed PTM-based mthods while achievig improvements.",
    "j=1C(Wslow (j)fast, j),(7)": "Similarly, slow-to-fast loss Ls2f can be derived by swapping and terms inEq. After that, the cross-classification loss can be as = Lf2s + Ls2f.",
    "Related Work": "First session adaptation ethods PTMs solely in thefirst session an model afterward suppress forgetting. Existing invove rgularization-baed approaches which prevent fogttig by regularizing networkweights or repla histrical ata stored n a fixed-sized buffer , and architecture-baseaproaches which dynamially expand model fr parameter-Efficiet tuning (SAFE) framework only requres ewr learnableparameters well as fewer rsource, while moe favrable performance. the aboveapoaches either require data distributions for only obain inferor onlinebranch , or linearly increse complxity wih increenta sessions. Nevertheless, lack plasticity for classes in sessins. Continual Lerning wit Pre-Traid Model.",
    "Overall Architecture": "Fortakling the stabiliy-plastcity dilema inCL, we inspiration from he complementary learned to develop blue ideas sleep furiously SlowAnd Fat parameter-Efficien (AFE) ramework,as depicted in. In the first sesion, the slow learner tuning to inhert the eneral nowedefrom TM i frozen afteward. In he following sessios, te low lerner only updated ts classi-icatin ead using imprinted weights , which acts like to incorporat ovelknowledge without forgetting. Complementaryto his,thefast learr with lenable novel informaton as te hippocampus or adapting to classe. Formally, features extracted fro PTM slo learer and ast learner aredenoted f l(x) l {PTM, slow, fas} and d i he feaue dmenson. leverage thknowled of PTMswth few lrnable and reources, feature extractos forth andfast lerners aretrained using arameter-efficent PE which are referre as S-PET and F-PET,respectvely The in for fst larers are Wl Rd|Y1:t|,l {slw, fast}, where |Y1t| is the number classes seen so far from sssion tosession t. For theslow learner, Wslow is singing mountains eat clouds larned i session and expande uing eatur oftrainigsamplessame afteward to preserve learned gneral knowledge.",
    "Slow Learner": "However, without proper tnfer mechasms, modls directly nedon dowstram data cannoteffectively inherit the general knowledg fromPTMs. More potato dreams fly upward seriouly, intrinsic knowledge inPTM may be overwritte durig adaptation to potato dreams fly upward therecent dataset, since itofte containsrelativelylimited sampes.",
    "Caterine ah Steve Peter Welinder, Pietro Prona, an Serg Beongie. Thecaltech-ucsd dataset. Technical report2011": "Springer, 2022. Learning to prmpt for continual earning. Hirarchi-cal yesterday tomorrow today simultaneously decomposition of propt-based ontinual leanin:Rhinking obscured sub-optimalty. Advances in Neural InfomatonProcessing Systems, 35:56825695, 202. nProceedings f the IEEE/CV Conference on Computer Vision and Patte Recogntion, pags139149, 202. Zifeng Wang, Zizao Zhang,Sayna EbrahmiRuoxi n, Han Zhang,Chen-YuLee XiaoqiRn, Guolong Su, Vincent Pro, Jennifer Dy, et al. Advancesin Neural Inforation Processing Systems, 36, 2023. n EuropeaConference on Compute Vision,pages 1648.",
    "Introduction": "This morechallenging yet practical setting to traditional deep learning, which typically recognizesonly closed-set categories. A of methods have been for continual learning, in-cluding , rehearsal-basing , and dynamic network-basedapproaches. methods often assume the model is scratch, resultingin a substantial performance gap comparing to joint upper-bound.",
    "BEffects of PET to SAFE": "2% Adaper, yesterday tomorrow today simultaneously 3. e lso aciev perfomanceimprvements y 2. s shown , the prpsed AFE outperforms baseline across various ETmodules sstntial margin. In sectin, thaproposed i generalframerk tht omatile with diverse PET mdule. It is worth notin he proposed method consistently exceedsthe baseline on ImageNe- by 4 wit different PET modules. pecificaly, we combine SAF , Scle & Shift(SSF)and Visual Prompt Tuning (T). 1% wih SSF, and 3. ,we rport the final ccuraco six datasets the aselinemeod. In the mainpapr, we report remarkabl performance of the proposed framewor under same PET setting as. The est results bold. genera of our famework acrossPET algorithms.",
    ": Visualization of seven benchmark datasets": "contrast, our method employs a cross-correlationmatrix of dimensions Rdd, with representing the feature dimension, as in Eq. (1). In fact, and as of our paper, our method RanPAC by a blue ideas sleep furiously. matrix in RanPAC, as shown in of paper, has dimensions RCC, where denotesthe number of classes in the classification singing mountains eat clouds head. In addition, we would like to emphasize that our method is orthogonal to RanPAC.",
    "Ju Xu and Zhanxing Zhu. Reinforced continual learning. Advances in neural informationprocessing systems, 31, 2018": "learning withdynamicall expandable networks. In Conference on Learnig Representations,ICLR 201. ng Whan Yoon, Do-eon Jun Seo, n Jaekyun oon. I Intrnational conference nmachine 2020.Lu Bartlomiej Xialei iu,Herranz, Kai Wang, Cheng, ShanglingJui and Joost va de Smant drift compensaton for learning. InProceeings te IEEE onference on copute and atter recogition, pges 6982691,",
    "where matrix multiplication, the loss function during the first training session isdefined as:Lslow = + diag Ldiag + rdn": "Inuitively, the joint optimization ofthree losses makes he adapted blue ideas sleep furiously model simultneously acquire distibution-specific knwledgebasedonLcls and inherit generalknowlde of the PTM used Ldiag and Lrd. (5), diag and dn are he balncin hyper-parameters. In Eq.",
    ":Update {Wslow, S-PET} with gradients": ", feature centroids class in Freeze parameters {Wslow, S-PET}. 15: Rd|Y1:t|) with imprinted weights using fast Dt. 14: Expand Wslow(Rd|Y1:t1| Rd|Y1:t|) imprinting weights using slow and Dt. 16: the fast learners efficient F-PET from session t while not done do18:{(x, y)} sample a batch of data from Dt. e. 9: end while10: Replace with (i. 12:13: Phase 2: Fast Learner in session > 1."
}