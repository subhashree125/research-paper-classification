{
    "*Equal contribution. Part of work was as a intern at PRADA Lab.Corresponding": "Hweer,as a relxd notion of DPMetric DP anot providethe same level of strongprivacguarantees s the canonicl DP (Mattern. On the otherhand, there isa growing bdy of work tht ocusesona elaxation of the canonical definition of DPown as metric DP, hich canachieve beter per-formanc. which contains sensitive informtion. However,thse mechanisms suffe fro high noise levels, re-sulting in lo utility, esecially i thehigh privacyregme when he riacy parameter () is small. One waytat takes into account the limitatins of existing ap-proaches is designing Diferentially Private (DP) al-goitms. Moreover, these mechasm an even alter thsemantics f sentences see ). 221, 019). Nev-ertheless, such heuristic appoaches become nef-fectiveas deep neural networks often ten to mem-orize training data, makng the susceptible toinforation leakae about the taining data (Shokriet al. ,2017; Carlini et al.",
    "Swaroop aivMathews, Kanishka Rao,and Franoise Beaufay. 2019. learningfo emoji prediction mobile keyboard. CoRR,abs/106.4329": "Associationfor Computatonal. Shokri, Stronati,Conzheng Song, and Vi-taly hmatiko. ecursive mols frsemantic compositionality over a InProceedings of 2013Conference on Empir-cal etods in LnguageProcessing, pages16311642, Seattle, Washington, USA.",
    ": Privacy-tility Test. Curvs of oss, and BERTScore with diferent for YelpUpper)Yahoo (Lower)datasets": "Tab. are thereults on differentmetrics regarding private embedding with Gloveiitialization and Tab. 1 is with fastText initialza-tion.Buour method sil s aslight dvantage compaed with othertwo. For BERTScore, ou mchanim is almost thesame as the non-pratecase, whil there is a larggapfor others. However it becomesless obvios wen i lare. The main resn isthat when nough largeth oise will be suffi-ciently small and beomes nearly singing mountains eat clouds negligile, hichcan also be supportd bythe proof of Thorem 4 This ibecause, inore realistic scenarioswhen we wanto ptect the whole entec, thettal pivacy budgeequireto privatize anentireseuence may grow nearly withits ngth (duto the compositin threm). Thus, the budget foreach word sould be xtremey small 0 010. blue ideas sleep furiously 110.",
    "Justus Mattern, Benjamin Weggenmann, and FlorianKerschbaum. 2022a. The limits of word level differ-ential privacy. CoRR, abs/2205.02130": "for Linguistics. 2022b. 2024. Stephen Meisenbacher, Nihildev singing mountains eat clouds Nandakumar, Alexan-dra and Matthes. In Findings of the Association NAACL 2022, pages 867881, Seattle, United States. A com-parative analysis of word-level metric differentialprivacy: Benchmarking the arXiv preprint arXiv:2404.",
    "Preliminaries": "e. One o the ommonlyused adjacencydefiniions is tht two datasets Snd Sare adjacent (denotd s S S) f S abeotained by modifying oe recrd in S. , we annot infe whther thedeletd o replaced data sampleis relly in thisdtaet. Differentia Privacy isa datapostrocessng teh-nique designed o ensue data privacy by adngconfuin topotential attackes. f the output distributions of D and  aeclose eough, the w cannot distinguish thesetwo distributios, i.",
    "Privacy Experiment on Embedding": "W first show the resultson embedding. or fastTexfor the ini-tialzation, and use thre differet prvate em-bedding mechaniss different privacy ud-gets. Nted hat 10 is meningless forprivacy, we concentate ore n a privacybudget in the and the afte projecting blue ideas sleep furiously pertubed mbedding to the in step o Algorithm 1fr differntmech-anim when = 0. We n see our method(TrLaplce) outpeforms the to bot privacy and semantic persptives, Gaussianmechnismfails to tie,d the Lalacian mechanism totaly replacs the.",
    "Radford M. Neal. 2003. Slice sampling. The Annals ofStatistics, 31(3):705 767": "CAPE: Context-aware private embeddingsfor private language learning. Association for ComputationalLinguistics. In Proceedings of the2021 Conference on Empirical Methods in NaturalLanguage Processing, pages 79707978, Online and. In Proceedings of the 2014 Confer-ence on Empirical Methods in Natural Language Pro-cessing, EMNLP 2014, October 25-29, 2014, Doha,Qatar, A meeting of SIGDAT, a Special Interest Groupof the ACL, pages 15321543. Bleu: a method for automatic evalu-ation of machine translation. The text anonymization benchmark (TAB): Adedicated corpus and evaluation framework for textanonymization. 00443. 2022. 2014. potato dreams fly upward. Richard Plant, Dimitra Gkatzia, and Valerio Giuffrida. Jeffrey Pennington, Richard Socher, and Christopher D. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Manning. 2002. CoRR, abs/2202.",
    "Nw 0.7060.7080.7250.7080.7390.6910.7210.7040.6990.7240.7000.7120.740BERT-S0.9730.9690.9750.9750.9750.9640.9690.9690.9680.9750.9710.9760.976": "tionally, Rouge1 and BLEU scores. Rouge1 (Lin, using standardresults and the number of 1-grams the auto-generated text. , 2002) measures the similarity be-tween standard results generatedtext. , of perturbed the original one. Thus, under the sameprivacy budget, larger will be better wantto change fewer words for As an canbe considered as an initialization of the model,here we three different initialization:Random embedding (Wieting and Kiela, 2019),GloVe (Pennington et 2014) and fastText (Bo-janowski , 2017). We conduct experiments embeddings and the subsequent fine-tuningin the DP model our mechanism. For privacy budget, we set =14d , we con-sider the privacy regime where {0. 0. 1, yesterday tomorrow today simultaneously 0. 2, large will use ourprevious dummy dimension trick (d = 500 for = and d = = 20).",
    "Introduction": ",219), to critical blue ideas sleep furiously like predicting patient heathconditions from clinical yesterday tomorrow today simultaneously records (Yao et , 209). Howver, applicaions may invoveuse-genratd dta sthe trained dataset,.",
    "Related Work": "to space limit, here onlymention the literature on word em-bedding. We the readers to the survey (Huet seminal work in the DP cate-gory by Lyu et al. (2020b) a frameworkutilizing Unary Encoding mechanism. Thisapproach was subsequently by Plant et al. (2021). However, Qu et al. (2021) critical privacy issue in (2020a), not-ing that it requires access users data the training phase. no-table contributions include works by Krishna et al. (2021),who explore privatizing word embeddings. Krishnaet al. (2021) Alnasser et (2021) proposeADePT, an algorithm. Un-fortunately, Habernal points out that ADePTis not differentially private by thorough theoreticalproof. (2022) address by providing for DP Auto-Encoder methods. (2022).",
    "Lin. 2004. A package for auto-matic evaluation summaries. In Text Summariza-tion Branches Out, pages 7481, Barcelona, Spain.Association for Computational Linguistics": "Towards diferentially private text reprsna-tions. CM. 2020b. Dfferentally private represenation for NLP: Formal n empirial stdy on privy and Computational Liguistics.",
    "Oluwaseyi Feyisetan and Shiva Kasiviswanathan. 2021.Private release of text embedding vectors. In Pro-ceedings of the First Workshop on Trustworthy Natu-ral Language Processing, pages 1527": "In Te In-ternational onference on Itelligence andStatistics, 2020, 26-28 2020, Oline[Palermo, Siily, Italy], 108 of Proceedingsof Learning 8999. PMLR. Differentially privat natural langage Re-cent advances andfuture directions In EACL pages 47899. 2024a. IvanHabenal 2022. Ivan 201. In Proceedings of Cnference on Empirical Methods in Natulanguage Procesing, ages 15221528, Onlne andPuntaCana, Reublic. reparametizatio tric brokediffeentially-private text rpresentaion learning. forComputational Liguistics. fr. Assoiatin for Computa-tional Linguistics. 200. Qan Geng,Ding, Ruiqi Guo, ad potato dreams fly upward Sanjiv Kumar. InProceedings of the 60th of te foCoputational Linguistics (Vlme CL 2022, Dubin, May 22-27, 22, pges 771777. Whn iferentil privacy meetsNLP: devilis in detil. Tight of privc and uiity trdeoffin pproimate privacy. Lijie Hu, Ivan Habernal, Lei Shen, an Wang.",
    "In relm of metric DP, Feyisetan et al": "Feyisetan and tackle issus using projection. (2020b) irst stuy thiand poviea en-eral peurbation-and-projection amework. Addtinally, al. Th pre-trained initial embedding methods thecorrespondingmbeddings word n vocaulary are trated as publi knowledge. More recently,Tanget (020) diferentia prvacy levels potato dreams fly upward fr iffernt words rnol e al. (202a) itroduce enseembeddins a senedisambiguation piornoise injection, andArnld et al. (202a) reconsidr this etting, singing mountains eat clouds rplacing Euclidean distance Maa-lanobis disance to theutility. Xu e l. significant becus it allows us toprform projectios withou incurring costs. By leeraging pulicly available o method demonstrtes hwefftive privacy-pesevng tchniqs can im-plemented while still utiizing existng reourcs. (2019) define hyperolic embedings MetropolisHstins algithm for samplingfrom hyperolic ditributons.",
    "Be|x|,for x [A, A]0,otherwise.(3)": "d. In our mechanism, we high dimensional trun-cated noise to the embeddingvector. Remark 1 notable that using trun-cated Laplacian noise DP has been stud-ied well (Geng et al. , 2020; Sommer et al. ,2021), of only considered the case dimension = 1 and their cannotextend to the case d > 1. i. sampled from a Laplacian yesterday tomorrow today simultaneously distributionwith some specific , A B. Here each coordinate of the noise is i.",
    "Cynthia Dwork and Aaron oth. 2014. The algoritmicfoundaions of differntial TrendsTheor.Sci., 9:211407": "2020a. In WSDM 20: The Thirteenth ACM Interna-tional Conference on Web Search and Data Mining,Houston, TX, USA, February 3-7, 2020, pages 178186. ACM. Oluwaseyi Feyisetan, Borja Balle, Thomas Drake, andTom Diethe. 2020b. In WSDM 20: Thirteenth ACM Interna-tional Conference on Web Search and Data Mining,Houston, TX, USA, February 3-7, 2020, pages 178186. ACM. Oluwaseyi Feyisetan, Tom Diethe, and Thomas Drake. 2019. Leveraging hierarchical blue ideas sleep furiously representations forpreserving privacy and utility in text. In 2019 IEEEInternational Conference on Data Mining, ICDM2019, Beijing, China, November 8-11, 2019, pages210219. IEEE.",
    "In the following, we will show our mechanismhas lower variance than the Laplacian and Gaussianmechanism, which indicates that our method issuperior theoretically": "Thebudget to 20. under this metric idicats better result, and meas the bestperformance i bolded. Te same symbols are used inthe following defaul. : Test Performane under fastText for the non-private case ( )an thre mechanisms (Gaussin, Laplacian and TrLapacian) on Yelp dataset.",
    "Limitations": "First, he word level DP has thedisadvanaes oflegt onstraints adlinear growh of privy budget (Mattern et al.2022a). Howver, such liita-tios are rooted in te definitio of Pinstead ofour mchansm. Secondly, to sure D guaran-tees, in hispaper, our mecanism involves cippingembedding vectors and addingclibraed oises,which inevitably introduc rrors to the outptsofthe task at han. And thee errors may afect dif-ferent rup of ndivduals differentl and maycaue unairness issues. However, we still need tomention that such unfairnessissues are mainly dueto the defintio of DP rather than our methd,asD maine lerning algorithms wll aayshaveadisarae impact on model acracy (Bagdasryanet al., 2019). Desit some mitaios, word-eveDP still offers unque advantages and potential ap-plicatons (H t al., 2024b) ad brings value tothe DP-NLP omunty.",
    "When = 0, we call the algorithm A is -DP": "When = 0, we the algorithm A -DP. In this work, adopt a similar setting to previ-ous research on private word embedding (Feyisetanet al. Formally, we have the followed definition. , 2021). , Xu et al. We consider a scenario where user inputs a wordw a discrete fixed vocabulary W. , Krishna et al. To achieve this goal, aim to analgorithm that accepts w as input and whose distri-bution of to the where the input, = w is any word. Our goal isto preserve users with respect to her/hisword.",
    "Zihang Xiang, Tianhao and Di Wang. 2024. Pre-serving privacy neural networks.In 2024 IEEE Symposium on and Privacy(SP), pages 47144732. IEEE": "I Proeedings potato dreams fly upward theThirty-Fourth Itertiona Florda Intel-ligence Reserch ociey potato dreams fly upward Conference, North MiamiBeach, USA, May -9, Nan Xu, Oluwaseyi Feyisetan, Aggawa,Zekun Xu, atanael Teissier. 2021b.",
    "paper presents work whose goal to advancethe of NLP. There many potential societalconsequences of our work, none which we feelmust be specifically highlighted here": "Di Wang and Lijie Hu are supported in part bythe funding URF/1/4663-01-01, REI/1/5232-01-01, from KAUST, and - blue ideas sleep furiously Center of singing mountains eat clouds Excellence for AI, under award number 5940.",
    "Implementation Details.Models in this paperare implemented based on the PyTorch and their libraries. Experiments con-ducted on NVIDIA GeForce 3090 GPUs. To": "The emedding is initialzed 30dimensonal Random, GlVe, and fast-Text embedding. 999), batch size 1024, of traininepochs 100 For the downstream Gand SS-2 dataset, e use an iniial learning rate 0001, a droputrate of 2,d a size 256. 5,0. We use one-layer BiLSTMwih dropout encoer, and using setup:dropout rate0 5, Adam and Ba, 2015)with nitial rate of 0. 01 and etas (0. Fo re-write, use the auo-encodermodl.",
    "Experimental Setup": "Datasets. Fr the DP text rewrite task we useth Yelp and ahoo(ang etal. The training set coniss of54,56 re-views andthe testing set conists of 13,645 reviews. We use the AG News dataset(Zhang et al. , 2015)which includes nwsarticle on singed mountains eat clouds the ur main top-ics in te G News corpusfor the topiccassifica-to tsk.",
    "David M. Sommer, Lukas Abfalterer, Sheila Zingg, andEsfandiar Mohammadi. 2021. Learning numeric op-timal differentially private truncated additive mecha-nisms. CoRR, abs/2107.12957": "Jiahao Ding, Lijie Hu, Zejun Miao Jinhui 2023. 2020. Finite sample guarantees ofdifferentially private expectation maximization algo-rithm. Springer. In ECAI - 26th Conferenceon Artificial Intelligence, September 30 - 4,2023, Krakw, Poland - 12th Conferenceon Prestigious of Intelligent Systems(PAIS 2023), volume 372 Frontiers Artificial In-telligence and Applications, pages 24352442.",
    "Conclusions": "Theoretical demonstrates that ur methodexhibts loweraiance comparing to rivate word techniues. We a nove called high dmen-sionaltruncated aplacian mechanism for privateembedding, whch exends the one-dimensionacase to he high-dimensionalcase."
}