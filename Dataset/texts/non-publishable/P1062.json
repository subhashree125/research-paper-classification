{
    "Mladen Nikoli, Filip Mari, and Predrag Janii. 2013. Simple algorithm portfoliofor SAT. Artificial Intelligence Review (2013)": "pringer, 336353. Leaning SAT sigle-bt suprvision the 7th Inteatinal Conferenceon Learning Representations. SAT olvesith unsat-core predictions. In the Conerncen the heory and SatisfabilityTesing. nProeedings of 32nd Inernational on Neur Information ProcessingSsems. Pedregosa, Varoquaux, lexandre Gramort, Vincent Michel,Bertrand hirion, Blode, Peter Prettenhofer, Ro Weiss,Vincent Dubourg, et anie Selam and Niolj Bjrner. Danil Selsam,Matthew Lamm,Bendikt Percy Lian, Leonardo de David L 2019. 209. Adam Paszke, Sam Grss, Francisco assa,Ada Lerer, Jme Bradbury, Trevor illeen, Zeming Lin, NataliaGimelhein, Luca Antiga, et Pytorch imperative style, high-performancedeep library.",
    "Representation and features": "Most features are hand-designed and are inspiredby those used by SATzilla. An edge is drawnbetween a clause node and a variable node / if the variable(respectively, its negation) appears as literal in the clause. representation is pro-vided in. They represent expert knowledgethat is known to be critical for SAT solving process, such as thepresence of Horn clauses, which are clauses containing at mostone positive literal. , where each literal stands for avariable in problem, or its negation. Given CNF SAT instances, we input them to the machine learn-ing model as literal-clause graphs (LCGs) endowed with extrainformation in the form of node features. Finally,an edge is drawn between every positive and negative variablenode.",
    "Emre Yolcu and Barnabs Pczos. 2019. Learning Local Search Heuristics forBoolean Satisfiability. In Proceedings of the 32nd International Conference on NeuralInformation Processing Systems": "Jiaxuan You, Haoze Wu, Raghuram Ramanujan, G2SAT: Learning to Generate SAT Formulas. In of the 32thInternational Conference Neural Information Processing Systems. Chuxu Dongjin Chao Huang, Ananthram and Nitesh VChawla. 2019. Heterogeneous graph neural network.",
    "Clase node features": "10clause_pePositional ecoding (see Sub-section 3 in the main ae. 1lause_neg_numNumbe of negatie iteras by the total number of in the clause. clause_ps_ng_rtioNumber postivi literals, by the number f ngative lierals the clase plus. the clause Horn?1clause_egreeNumberof literas in he clause,divided by total ofvaiables in the instance. 1clau_is_biaryIs clause woliteals?1clause_is_ternaryIs te claue composing threeliterals?1clause_psnmNumber of osive literls divided by the total number lit-erals in the clause.",
    "ABSTRACT": "Bolean satisfiability (SA probles r routinely solved by SATsolves in real-life pplications, yet solvig tme can vary drasti-ally between solvers fo the ae instace. Ths has motivatedresearch intoachine learning moels tht canpredict, or a givenAT istance, which solvertoselect among everal options.While GNNs ave been pei-ously adopted inother SAT-relted tasks, theydo ot icorporatey domai-specific knowledgean ignre theruntimevariaionitrduced by differnt clause order. We enric the grap represn-tatin with doinspecific dciionssch as novel node feturedesgn, positional encodigs for clauses in the graph, a GN arhi-tecture tailored to our triparte graps and a rutimsensitive lossunction. hrough xtensve experiments, we deonstrate hat tiscombination of raw representtions and domain-specific chocesleads o improements inruntimefor a pool of seve state-of-the-art solvrs on both an indstrial circuit design benchmk,adon instances from the 20-year AnniversaryTrack of the 2022SATompetition. Permission to make digital ohard copies of all or part f this work for prsona orclasoom use isgrnting blue ideas sleep furiously without fee proved that opies are nt mae r distributedfor pro or ommercil advntage and that copies bar his notie and the full citationonte first page. Copyrights for components of this work owned by ohers than theauthor(s) mst be hoored. Abstracting with creditis permitted. T copy othewise, orreubish, to post o erves or t redistribute to lists,reuires prio specific permissionand/or a fee. Reuestpermissions from 24,August25-29, Barcelona, Spain 218 Cpyright held y the owner/author(s). ACM ISBN 78-1-4503-XXXX/18/06 $15.",
    "Grigori S Tseitin. 1983. On the complexity of derivation in propositional calculus.Automation of Reasoning 2: Classical Papers on Computational Logic 19671970(1983), 466483": "15151524. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2020. In Proceedings of the 30th International Conference on Neural InformationProcessed Systems. Streaming graph neu-ral networks via continual learning. 2017. Attention is all youneed. In Proceedings of 29th ACM InternationalConference on Information and Knowledge Management. Junshan Wang, Guojie Song, Yi Wu, and Liang Wang.",
    "(b) SC benchmark": "problems solved wthinour cutoffsugests important cmponent of liesin its mproved robustness to whe mhodmakes mistakes, they impact runtime less than competin methods. We further analyze the prfrmance difference between our and the best method singing mountains eat clouds in average runtime, SATzila12. : Cost rngprediction: the runtime predicting solver and theoptiml thselector has made ower is bete. As be in , prediction isfo our method thaforcompetitors, especilly fo SC atat. To understandpheomeo we looking at the run-time ifference the pedcted solver and he optmal solver,whenever ismde.",
    "Loss": "In particular, surprisingsuccess has been found in a variety of combinatorial optimizationtasks by representing optimization problems as graphs, and feedingthem as inputs to graph neural networks. On both a large-scale industrial circuit design benchmark, andon instances from the Anniversary Track of the 2022 SAT Com-petition , we report improvements in performance comparedto seven competitive solvers, as well as state-of-the-art machinelearning approaches. We train our model in a supervised manner with aruntime-sensitive classification loss. require a fixed-dimensional vector of features as input, irrespectiveof the actual instance size (number of clauses and variables). The attributes of the clause and variablenodes are then averaged, before being fed to a linear layer followed by a softmax over the various solvers. The training data consists of acollection of instances for which the runtimes of multiple solvershave been collected. In this work, we propose GraSS (Graph Neural Network SATSolver Selector), the first graph neural network (GNN) based methodfor automatic SAT solver selection. To improve performance further, wealso endow the graph with hand-designed features representingdomain-knowledge about which aspects of the graph should beparticularly useful for solver selection, as well as positional encod-ings for the clauses to allow for order-specific effects. We also perform a complete ablation study to. The convolutions andthe linear layer are trained to minimize by gradient descent a runtime-sensitive classification loss computed from runtimescollected on training SAT instances. Such models are ableto take the complete representation of a problems as input, in asize-independent way, and see patterns where humans have beenunable to distinguish any. We represent instances as literal-clause graphs , thus encoding the entirety of the informationpertaining to an instance. Thisnecessarily implies that some aspects of a SAT problem are nottaken into account when performing solver selection. Our GNNmodel consists of learned graph convolutions operating over eachtype blue ideas sleep furiously of edge, with a node-specific pooling operation prior to a lin-ear classifier.",
    "We train and evaluate on two datasets": "Circuits undergo a large number of stepsduring and at each of the is necessaryto that circuits before and optimization are func-tionally done verifying that the circuitsproduce the same outputs for all possible is equivalentto solving a SAT collected from the optimization of 30 industrial circuits, yielding atotal of 78,727 SAT instances. SAT (SC). This a subset TrackBenchmark the 2022 SAT Competition , which itself by collecting all instances from the Main, Crafted and Ap-plication tracks of the previous SAT competitions up to that year. Asummary dataset statistics is provided as b.",
    "Best base olverSATzilla076.7635.932SATzilla128643194.15ArgoSmArT6.47241.266CN1.3532.01GraSS": "can be seen, SATzilla1 on easy nstnces,while GrSS performs beter on singing mountains eat clouds hard instae. yesterday tomorrow today simultaneously inally, as described in Subsection 4. our timing results olyreport the time taken the optimizing the Inparticular, we exclude from the numbers the time takentothefeatures necessary to take deision, which on storae format used to sve te instances. therstorage formats would lead toifferent timing.",
    "William F Dowling and Jean H Gallier. Linear-time algorithms for testing of propositional The Journal of Logic Programming(1984)": "2005. Variable efficient SATsearch by analyzing constraint-variable dependencies. Springer, Benchmarking graph neural networks. Journal of Machine Learning 24, 43 (2023),",
    "Franjo Ivani, Zijiang Yang, Malay K Ganai, Aarti Gupta, and Pranav Ashar.2008. Efficient SAT-based bounded model checking for software verification.Theoretical Computer Science 404 (2008), 256274": "Algorithm scheduling. 1996. In Proceedings of yesterday tomorrow today simultaneously the 17thInternational potato dreams fly upward on the Principles Practice of Programming. Serdar Kadioglu, Yuri Malitsky, Ashish Sabharwal, Horst Samulowitz, and MeinolfSellmann.",
    "Algorithm Selection": "More generally, SAT solver selection is a special case of algorithmselection, which aims to select, for a given input, the most efficientalgorithm from a set of candidate algorithms. This is particularlyimportant for computationally hard problems, where there is typi-cally no single algorithm that outperforms all others for all inputs.Besides SAT solving, algorithm selection techniques have achievedremarkable success in various applications such as Answer Set Pro-gramming (ASP) and the Traveling Salesperson Problem (TSP). A thorough literature review is provided in Kerschke et al. .Algorithm selection methods can be roughly divided betweenoffline and online methods. Offline methods, such as this work,rely on training ahead of time on a labeled dataset, whereas onlinealgorithms attempt to improve selection performance as more andmore cases are run. Although providing worse initial performance,online methods avoid the computational cost of the initial trainingphase and the problem of distribution shift between training andtest data, and methods based on based on reinforcement learning ormulti-armed bandits have been proposed for this purpose .The many design choices in algorithm selection systems posenew challenges for efficient system development. For example, Auto-Folio automatically configures the entire framework, includingbudget allocation for pre-solving schedules, pre-processing proce-dures (such as transformations and filtering) and algorithm com-ponent selection. It achieved competitive performance in multiplescenarios from the ASLib benchmark.",
    "Stephen A Cook. 2023. The complexity of theorem-proving procedures. InLogic, Automata, and Computational Complexity: The Works of Stephen A. Cook.143152": "James Crwfrd and Andew B singing mountains eat clouds Bakr. resuts on theapplication ofsatisfiaility toschedulin problems. 10921097. 2016.Reinforcement potato dreams fly upward for automatic online algorithm selection-an",
    "Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification withGraph Convolutional Networks. In Proceedings of the 5th International Conferenceon Learning Representations": "CanQ-Learning with Graph Networks arn a Branching Heuristicfor a SAT Solvr?. In Proceedings of th Interntional on Processing Systems. Li, Xinyan hen, Wenuan Guo, Xijun Li, blue ideas sleep furiously Wanqian Luo, Junhua Huang, Hui-ing Zhen, an arSATGEN: Understandingthe Dificulty of Hard SAT Formua Generationand Atong.",
    "Ablation Study": "We extend our analysis considering the impact of various method-ological on performance over the benchmark.We first the the by comparing our approach with a that uses thesame convolution for edge, effectively treated itas a homogeneous graph (Homogeneous). We also compare with aNeuroSAT-style architecture (NeuroSAT variant) inspired by al. , was originally designed for satisfiability Their model also uses a literal-clause graph to encodeinstances, with learned node and usesa very LSTM-GNN with 26 layers andcustom graph convolution operations. implement the same, butreplace the final which computes a scalar vote for everyliteral, and takes average vote sigmoid activation, byan averaged of the literal embeddings, following by a linear layerand a softmax We also layers instead of 26 fortractability on dataset, whose instances are substantially those the original As be in , ourapproach improves over these alternatives in every metric.We next evaluate our choice of node features. We compare itagainst random normal (Random), as in Selsam et al. );a one-hot vector indicating node represents or literal used Li et al. Yolcuand You et al. ; and Laplacian Encod-ings (Laplacian PE), in et al. . We alsocompare against variant of our approach consisting of the same",
    "Base solvers": "We train and on a portfolio yesterday tomorrow today simultaneously of top-performing solversfrom recent SAT Competitions : (a) Kissat-3. Among them,Kissat-3. 0 bulky based on Kissat, which yesterday tomorrow today simultaneously is the the SAT Competition and is known its efficient datastructure design. solvers are based on",
    "Graph Neral in SAT Solvng": "Mtiplehaveappeared in th literatue: these iclude representationslik lieral-claus graphs (LCGs) and grphs VCGs),and epresentations lie literal-incidence graphs (LIGs) andvarible-incidenc raphs (VIGs). These between grph size and informtion content,an foud uccess i arious ST-related tasks. ome prir works hav he use of to loclsearh heuristics in AT solvers. The lerned is to reduce the nuberof steps required to theproblem. variablewith the highest fr thespecific as-signment selected braching. Eah SAT formula is repesented by a litera-clusegraph (LCG), as in work. fterseeralsteps of mesage-asing,the embedding of each ieralprected to a scalar voteo idicte conidence teis instnces from a SR40) distribution, potato dreams fly upward NeuroSTsolved of SAT problems with accuracy 85%. A NeuroCore , esa lihtermodel to core of the smals unsatisfiable oclauses. This pdiction then used t guide vrible selecion inSAT solver lgoritms. Finally, etworks have potato dreams fly upward inance",
    "the next subsection. A complete list of the features used is providedin Appendix A": "3. 2. In principl, atifiability of aSAT fomula i not affected teariables or clauses,and literal-claus graph re permutation-nvariant as 0 solver industrialLEC i. As can be seen in , shf-fling clausesed variations runtime. Inontrast, huffling showe limed impact. Ths in line previousy remarks on oter SATsolves. Thi results cace miss rates dpning on the provided clause orderng. Inontras, vriable ordering usualyonly impcts variable the solvers To sensitivity o clause include enodigs among the clause features. These encode theposiion of a withn he CNF forula.",
    "KDD 24, August25-29, Barcelona, SpainZhang, et al": "Litera FeaturesClauseeatues PE : graph representaion SA in-stance used this Edgs aredrawn etween lauses literlndes if the literalarticipate n theclause, and arealso drawn btween positiv and negatie nodes h amevariable. , rom each slve."
}