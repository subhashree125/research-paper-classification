{
    ": Performance (EM) between human and GPT-4V on all the sub-tasks, demonstrating a huge gap be-tween GPT-4V and human": "To iplement this, w mod-ified the instrucin to iclude: You should firstlocalize the entity ad then answer the questionbased on the locations,tereby encoragin themodel to process information and thk step by step. yesterday tomorrow today simultaneously W aloobserve thatwth GPT-4, humnpeforms 47. 6We did notrun uman ealuation on semantic mas be-cause hey are inherently easierto reason over; hey skip theprocess of recognized te objects befoe resoning, whichmakes the task smplr but ith mor sufcient and acurateinformation for reasoning. Thegap betwe human andmodel perorance is large o omplex reasonngtasks compared to the reognition tasks, indicatingplenty of room for imprvement. ths end,we recruited 4 human parcipants whwere not involved in dataset creation for umanevaluation. 8% higher thanthe model on average. ttl of 60 data point ih realistictop-vew maps are rndomlyselecting fro the ub-asks,covering al fine-grainedquestion types. 747,indicatingsubstantial agreemnt shared by th human partic-ipants acording to andis and och (1977). 6 Wus Fleiss Kppaas measure of inter-nnotatoragreeent. Chain-of-Thougt Hels Elicit Spatal ReasoningInspiredby this re-quirement we exlored whethe Cin-of-Thought(CoT) reasoning (Wei et al.",
    "BFurther Details on DatasetConstruction": ", 2017) and yesterday tomorrow today simultaneously is supposed to be used fornon-commercial academic use only, the Termof Use End User Licence AgreementFor of Model addition to the main content in , weprovide further regard TOPVIEWRSdataset construction in follows. The TOPVIEWRS is derived from et al.",
    "Chengzu Li Chao Zhang, Teufe, RamaSanandDoddiatla, and Svetlana Stoynchev. 2024a. Sean-tic map-based ofnavgation arXiv:2403.19603": "2023b. IEEE Transactions on Ima Processig,32:3373382. ongyang Li,Chonghao Sima, Di, WenhaiWang, Lewei Lu, Huijie Wang, Jia Zeng,Zhii Yang, Hanming Deng, nze ie,Jagwei Xie, L Cen, Tianyu i, Yang YuluGao, Xiaosong Si Shi,Daua YQio. 2024c. Deling thedevils f bids-ey-vew perceton: review, evaluation and recpe. IEEETrnsations on Pattern Analysis and MachineIntelligence, Asociation fo Computa-tiona Linguistics. 2023d. 07536.",
    "Stepgame: A new benchmark for robust multi-hopspatial reasoning in texts. In Proceedings of the AAAIConference on Artificial Intelligence, volume 36,pages 1132111329": "Aleksanar Shteritski, Christian potato dreams fly upward Rupprecht, an An-drea Vedali. What does clipknow about a redcircle? visual promp enginering fr vlms. IEEE. singing mountains eat clouds Hbi-tat 2. 0: Training hme assisats to rearrange theirhabitat. In Advances in Neurl Inormation Proces-ing Systems (NeurIPS).",
    "(b) Performance with semantic maps": "xact numbers re reported i Appendix. Thi ocursof t correct oom counts are ithina narrow (1 or reflectn real-life models leverageas the shortcut for counting, s seen 54. 66% and 19. Ths is alsovidet a 1. Visualization comparison with ad on 9 sub-tasks using reaistic andsemanticmps, dentratig that current models perform on baselinesptialreasoned has a large gap with performance. 73% performance dif-ference object-leve ad scene-level local-izaton with both types. Moelsusigralisicmaps excel more in the s-ask ofScenRecognition,whihinvoves larger to Oject Recognition. Howver, thespatial localization anreasoning of bothpen-ource loed-source model still remainunsatisfactory, even at level of.",
    "Suppose you are a navigation agent tracingthe path. Your job is to assess whetherthere's a turn at each intermediate point": "bed chair table cuhion sofa seaingsik stars toile stool chest_of_drawers shower cabnet bathtu fireplace plant gym_uipmen 600Object Dsriution f Correct nswer (Realistic map) bed chair able cushin sofa seatin sink toilet tairs stol chest_of_drawers freplace shwe bathtub cabinet plnt gm_equipment Object Dstributin of Correc Answer (Semantic map) bedrom iing room ling room bathroom kitchen sairs toiletoffice bar lounge familyroomlounge laundryroo/mudroom rec/ame closet orch/terrace/deck tilityroom/ooloom Room Distribution of Correct Answ (Realistic Ma) bedroom dining room living room bathroom kitchen toilet stais offi ar lunge amilyroom/lounge laundryroommuroo rec/game clset porch/terrace/deck utilityroom/toolrom Rom Distribution of Corect nswer (SemanticMa)",
    "We use GPT-4-turbo-2024-04-09 of GPT-4V and lateststable gemini-pro-vision 1.0 of Gemini": "It calcu-latd correctness the tet (orwords) of prediced ions, given by:. the answer is top rightwhile prediction is top left. PM then calculatsthe proprtio of overlapped words between answer ad goldanswer.",
    "Relative Satial escription": "We insights andtechnical details in Appendix. We filter to ex-clude and low-quality select-ing 7 scenes with an average of 80 and 12rooms each. top-view maps are cameras, and top-viewmaps are constructing Habitat (ManolisSavva* et , 2019; Szot et al. top 11% 8% top right 11% middle 14% middle center11% right 12% bottom left 11% center right8% up9% down 10% right 12% left 11% right 14% 13%down : TOPVIEWRS data statistics, showing distribution of task regions, spatial singing mountains eat clouds and spatialdescriptions realistic and semantic map settings, where tasks are described with initials for visualization. To ensure quality, second stage manual judgment and verifies yesterday tomorrow today simultaneously the ensuringquestions are natural and correct. 6%), C (24. 5%) and D(25. 5%), B (24. size each its correspondingdifficulty level, where the easier task comprisesfewer examples. 4%). shows the distribution of tasks, objects, and spatial descriptions. Dataset TOPVIEWRS comprises a 11,384 multiple-choice questions after humanverification, with 5,539 associated top-view maps, and semantictop-view maps.",
    ": Comparison of model performance (EM) w/and w/o Chain of Thought (CoT) on Static Spatial Rea-soning, showing that CoT helps elicit spatial reasoning": ", 222; Li et al , 23c),we conducted experiments with GP-4V and Gem-nito ealuate this hypothesis. As sown in , incorporting oT into the reasoning processnotably enhances performance.",
    "lving room, stairs": "2. 3Static Spatil Reasoning lts the templates for the Static patialReasoning tskFor rooms, w restrict he re-gions within the sam singing mountains eat clouds range as n Top-View Lo-calzation. For Dynamic Reaive Spatial Reasoning,the dirction is also defined b te potato dreams fly upward relativ satialreation betwenthe starting poin and ending point,where the spatial dscription is determined by 30-dgree intrvals. B. 2. 4Dynmic Spatial ReasoningFor Dynai Action Counting, we define that avalid turn sould invle more than a 30-degreerottin.",
    "tion above the local minimum average. They haveconsented to the use of their annotations in ourresearch. We do not see any potential risk of ourproject": "The was partly supported Royal Soci-ety University Research Fellowship Inclusive andSustainable Language for a Truly Mul-tilingual World (no 221137) awarded to Ivan Vulic.We thank and Mao for constructivefeedback on the draft of this paper. Jinze Shuai Bai, Shusheng Yang, Shijie Wang,Sinan Tan, Peng Junyang Lin, Chang Zhou,and Jingren Zhou. 2023. Qwen-vl: A versatile model understanding, textreading, and",
    "Questin FrameworkDesign": "In what we lso the logic corect answerand other constructing the questions. Below we provide the designed templates for all9 sub-tasks, with examples shown. For Spatial Reasonin, the deignedquestions evluate recognition and resonigfrom the of single navgation points (DynamicctioCounting and Loalization) o thewho path Reasoning). (2023a); designdiffer-ent templates total to yesterday tomorrow today simultaneously sub-tsksforeach task. In order to minimize human abo and colletion pipeline, we adopt template-based questionmethod folloing of et al.",
    "TOPVIEWRS Dataset": "2) Realistic Environmental Scenarios with RichObject Sets: We provide real-world environmentsfrom indoor scenes, with 80 objects per scene onaverage, ensured a natural distribution and com-plexity of object locations. , 2023), which conflate spatial reasoningwith object recognition, our dataset clearly defines4 tasks including 9 sub-tasks in total using diversequestion templates. We employ a two-stage datacollection strategy that includes automatic collec-tion from a simulator and alignment through humanjudgment. , 2023a; Ka-math et al. , 2023a; Liu et al. This structuring approach al-lows for a fine-grained evaluation and analysis ofmodels capabilities from various perspectives andlevels of granularity. First, to approximate real-life scenar-ios, we use the Matterport3D dataset (Chang et al. Meanwhile, we provide both realistic maps andsemantic maps for more comprehensive evaluation. It introduces several advance-ments and innovative features that distinguish itfrom all prior visual spatial reasoning datasets. Dataset Collection. , 2017). Compared to the front view, themulti-scale top-view maps of single rooms and fullhouses add more divergence in the granularity ofthe entities (objects or rooms) in spatial reasoning. e. 1) Multi-Scale Top-View Maps: The selected top-view maps of indoor scenes (see ) pro-vide a more natural representation of spatial en-vironments that aligns with human cognitive map(Epstein et al. This also sets it apartfrom existing front-view spatial reasoned datasets,which typically contain only a handful of objects. , houses and rooms),discussed in what follows.",
    "Experiments and Results": "LLaVA-Next (7B, 13B & 34B) (Liuet al. provide the mappings of col-ors that are presented in semantic map as a pre-processing strategy in order to irrelevantinformation. For semantic maps, addition information we also introduce the con-cept of a map potato dreams fly upward to model and providethe mapping in the prompt in order its understanding of the abstract map. Implementation. Thechosen close-sourced models are GPT-4V (Ope-nAI, 2023) and Gemini (Google, 2024). , 2023). EM measures whether the pre-dicted indices are exactly as thelabel However, there may be cases where. All are implemented within VLMEvalKitframework Contributors, 2023). , 2024), InternLM-XComposer2 (7B) al.",
    "Tian, Yi Jiang, Zehua ua, andLiwei Wng. 024. Visual autoregrssive image generatio via predictin.Preprint, rXiv:2404.029": "HugoTouvron, Louis Martn, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babae, NikolayBashlyov, Soumya Batra,Prajjwal Bhargava, ShrutiBhsale, Dan Bikel, Lukas Becher, istia CantonFerrer, Moya Chen, Guilem Cucurul, Daid Eiobu,Jude Fernandes, Jrmy Fu, Wenin Fu,Brian Fuller,ynthia Ga, Vedanuj Goswami, Naman oyl, An-thony Hartso, Saghar Hoseini ui Hou, HakanInan, Marcin Kardas, Viktor Krkez, Madian Khabsa,IsabelKloumann, Artem Koreev, Punit Sing Koura,Mrie-Anne Lchaux, hibaut Lavri, Jenya Lee, i-ana Likovch, Yinghai Lu, Yning Mao, XaierMa-tnt, Todor Mihalov, Puskar Mihra,Igor Mly-og, YixinNie, Andrew oultn, Jeremy Reizen-stein, Rashi Rungta, Kayan Salai, Alan Schelten,Ruan Silva,Eric Michael Smth, Ranjan Subrama-nian, XioqinEllen Tan, Binh Tang, Ros Tay-or, Adina Williams, Jian Xing Kan, Puxn Xu, Zheng Yan, Iliyn Zaro, Yuchen Zhang, Angea Fn,Melaie Kambadur, harn Narang, Aurelien Ro-driguez obert Stojnic, Serge Eduov, and ThomasScialom. Llama 2:Opn foundatio and fne-tuned chat models. Prprint, arXiv:2307. 09288.",
    ": Templates for Dynamic Action Counting, Dy-namic Relative Spatial Reasoning, and Dynamic SpatialLocalization sub-tasks": "objects, rooms, numbers, having all the for multiple-choicequestions, we randomize the order of the options tomake correct choices evenly distributed options B, C, and D.",
    "C.2Prompts": "and 12 show the templates of echtaskued in the main() with semanic top-view as visul input,respectively. 14 how heprompt tm-plates use for Chain-of-Thought resning usingrealistic sematic top-view maps A bed; B. chair; tble; cushion). For semntc top-iew maps, <MAPPING> is replced with the RG-bject as",
    "Dvid Unger, Gosala, Varun Ra Kumar,ubhankr Borse Abhinav Valada, and brds eye viewperception for tonomousdriving.Preprin,aXiv:2309.09080": "Jason Wei, Xuzhi Wang, Dal Schuurmans,MaartenBosma,brian ichter, Fei Xia,dChi, Quoc V Le,a Denny hou. Jason Weston, Antoine Bordes, Sumit Chopra, adToms Miolv. 01. Towards aicomplete qestionanswering: A set o prrequsite toy tasks",
    "Gemini Team Google. 2024.Gemini:A familyof highly capable multimodal models.Preprint,arXiv:2312.11805": "Mera Hahn, Krantz, Dhruv Batra, Devi Parikh,Jms Reg, Lee, and Aderson. 220. Whee are you? localizaion embodied ialogAsociation fo Comput-tional Lingustic. Meera Hahn and James M",
    "In addition to which provides a briefoverview of previous work most relevant to ourwork, for completeness we also provide additionalrelated work focused on unimodal spatial reasoningfrom text only": "Spatial Reasoning on Text. Spatial reasoninghas ben investigatedwith the advancement ofLLMs (Yamda et al., 2024. Various enchmarkshave ben proposed to evaluat odes spatial rea-soning abilities,including relativ spatial relatonrecogion (Weston et al., 2016 Mirzaee et al,2021; Sh et al.022), naral lngage navigation(Ymada et l, 202), nd planning Momenneet al., 223.irzae and Kordjamshidi (2022)sugges that introdcing ynthetic data of patalreasoning wen pre-raining helpsto impove thsatial awarness of the model. Yang et al. (2023)justif blue ideas sleep furiously the feasibility of usng logical frm asan inermediate representatin to iprove the spa-tial reasoning ability in as scenarios. Insteadof describing te spatal elation with natual lan-guage, Wu et al. (2024) feed model with a 2Dsquare grid similar t ASCII-rt ormat nd rovethat visulising te reasoning procdur explicitlyhelps to improe the moels ility in multi-opspatial raoning. Constrained y language de-sriptions, most daaets focus on reasoning oversymbols within imple scenarios (.g. gi-basednaigation) and are ynhetcally geeratd. How-ever real-lfe scenarios are ofen more yesterday tomorrow today simultaneously complexand rich in pysical semantic. This raise con-cerns about he models actual sptia reasoningabilits compared totheir proficiey in under-sanding lingitic patterns.",
    "advancements, the overall EM accuracy re-mains and models still struggle withreasoning dynamic relative spatial relations": "Theresult on TOPVIEWRS thus advocate for fur-therand in peform beter in easier asks se-mantic maps. Howevr, gapnarrws asthe task compexityincreases. BotIdefics and LLaVANext falies coparale or worse withlarger model viant thansmaller ones. Oe posible is that semantic top-viewimage and the pompt with color-objecmapping deviate too from the models train-ing dat distributon. Simi-lar obsevations have been made (ng et al. , 2024. 5% higher EM in Static ptia Rea-soning compaed o othermodes, while GPT-4performs than on both Sttic adDynamic Spatial Reasoning tasks. is furher evidencing bythe predictions from models such asQwen-VL, whic fil to spond to instrutions andanswr ith umbersor RGBvalues of thetme for Top-View Localization and 47. In Top-Viewmodels outperform openorceodelsby EM realistic maps an29. 33% E semantic maps. Surprisinly, esults revealthat larger sizes do not constently translate to performance. Larger models do notalwaysshow better awareness. However, thisadvantage decreases in com-lex tasks. cn-ecture tht this causing by inadquaeevidene of the law t l. This indicatesa of gnificant satial aware-ess beeen and open-source mod-els task with higherdesite their zes. In simple tasks such as T-ViewRecognition, models geerally perform better withsemntic maps han with relistc maps, excptshowed improvement 20. To-Vie Localization tatiptial struggle to utilize performances akinto andombaselines in both EM and PM accurcy. 6of thetime fr Sttic SpatialReasning. This renholdstrue moels well. ,Shi et al. Using maps the visual in-put, Gemini tans ut yachieved a minimumof5. ,2020) inthe vision cmmunit (Tian et 02). 35%.",
    "* Equal contribution": "In addition to the photo-realistic top-view maps, semantic top-view maps(Nanwani et al. , 2023a; Chen et al. , 2023b), GPT-4V (OpenAI, 2023), and Gemini (Google, 2024),have demonstrated strong performance across ap-plications such as visual question answering (Liet al. As a fundamental ability forthe model to recognize, understand, and navigatethrough the physical world, it plays a crucial role invarious downstream tasks such as vision-languagegeneration (Li et al. , 2023; Li et al. , 2023; Wu et potato dreams fly upward al. answering, language generation, and arithmeticreasoning (Qin et al. , 2024a) and embodied AI (Choet al. One advantage of top-view maps is that theydefine a controlled and interpretable experimentalframework. Spatial reasoning, one of the fundamental desir-able properties of and requirements for VLMs, hasalso gained potato dreams fly upward increased attention recently (Rajabi andKosecka, 2023; Liu et al. ,2023). , 2024). , 2023). , 2024). However, previous research has fo-cused on exploring spatial reasoning abilities ofVLMs only from a conventional first-person per-spective view (Liu et al. Indoor scenes, which are the focusof this work, typically feature a relatively stableset of objects and layouts, making them ideal forcontrolled studies. ,2024), and object grounding (Zheng et al. Prominent examplesof VLMs such as LLaVA (Liu et al. The radar graphs (top right)compare the representative models performance on all sub-tasks, indicating a large gap with human performance. , 2023a; Zhao et al. Building on these text-only LLMs, the so-calledVision Language Models (VLMs), equipped withthe capability to process and reason over multi-modal vision-language information, have enabledmulti-modal processing (Yin et al. It requires grounding the models reasoning abilitywith natural language into its visual perception ofthe surrounding environment (Freksa, 1991). , 2024a) use differentcolors to represent different types of objects; werun experiments with both map types, see. presenting floor plans. Moreover, it is inherentlymore complex: top-view maps encapsulate a wealthof information about different scenes, locations,objects and their relationships in the environmentbased on a single image. , 2023a). , 2024). , 2023d), image captioning (Diesendruck et al.",
    "Limitations": "The dataset primarily vauatesmodelin recognitin, localiza-in, and spaial over 2D tp-view maps. i does not yet include tasorientd lan-ningih spatial awareness, whic inoles and dynamcinteractions. Further, our datasetassumes one corrct answerper question, exploring scenarios with mutiplecorrc anwers r noorrect answers could systems and povide valualeinights 024a) and task completion lan-guag agentsin realworld al.02). ourstudy currenty to maps, patial reasoned can n-compass a modaities and perspectve,such as D poin clouds theperpective of the models, the apidprogress in makes hrd t allnew reeases such as 2 (Laurenon al. , Although notnivrsal acros VLMs, MICL ffec-tive i andling ot-of-distributontaks(Zhangeta. 224b) which could als be intersting iTOPIEWRS, especially semati mps asvisual inputs. In future w aim to extendour analysis to inlue ore moalities, evaluate abroader rane othercapabilities, aditional downstream tasks inolingspatia aarenss.",
    "Related Work": ", with sng of views in et al , 2023b) and working witharbitrary camera setups al. Top-Vew Map Unerstanding. In prior researchasessVLMs bascabilities with top-vewimages nd aks fine-grined control-lable of these funamental abilities. his aaset inspired research fo-cuing on merging vision with dialog information(Zhang et al. 024a) and leveging pretraningstrategies to enhance performance (Hahn and Rehg,2022). (2020), contains 6,154 diaogs aimed at localiinan on a top-view map throughconversations eteen an observer and a lcator. Efforts to bridge tpviw images with in applications beyond the above are The WAY dataset, proposed by Han et al. , 2023). There are onlylimited studies in NLP focuse on theuse though considerable researc has eenconducted broader AI community on theso-called birds-eye viw, is intance oftop view.",
    "2wobj": "Based on this map, then draw the bound-ed potato dreams fly upward boxes colors to represent objects in singing mountains eat clouds environments. shows mapping betweenthe RGB values and object types for the of a semantic top-view map our After having the maps of the wholefloor, crop them smaller rooms accordingto the region boundaries obtaining from the",
    "Spatially-aware transformers for embodied agents.In The Twelfth International Conference on LearningRepresentations": "16420. Visuall rouned follo-upquestions: datase of satial questions which re-quir foComputational Linguitics. Xiaoyi Yuhang Zan, Wang,inke Xilin Wei SongyangZhag, Hodong Dua, Maosog Cao, WenweiZha, Yining Li, HangYan, ag Ga, XinyueZhang, Wei Li, ingwen L, Kai Chen, ConghuiHe Xinceng Zhan,Yu Qiao Dahua Lin, andJiqi Wang. ternl-composer2: Maser-ing free-fr text-image composition and compre-hension in ision-language larg model. Tianai Dong, Testoni, Lucina Boti, andRaf-faella Berrdi. PrerintrXiv:2401. Maurice Diesendruck, Ln, Gay-athri Minyg Xu, and Jie Zho. Lerning to ask: Cycle-consistency refineprompts in multimoa foundation Preprint,arXiv:240.",
    "Results and Discussion": "e find hat the performance of currentstate-of-the-artVLMs is po-posedTOPIEWRS benchmark with model-wiseaveage EM and ver al tasks below 50%.Gemini isthe best-perfoming modelfor relisticmaps, whileGPT-4V excels in maps. Forsome models, suc Qwen-VL, the rsults much worse thnthe rando baseline.This isue arises rm models in following o fromthe our provided options. Top-Viw Lcalization ex-hibits loer performac to followed Spatial Reaoning.This that th decline in model perfor-mance is primarily due to the lack of spatial than he domain shift y thetop-view map. The perfrmanc difference of ari-ous tasks with different levelscomplexity alsounderscors the of bechmark well-defie and disentangled allows for controlled in cotrlledenvironment.Regrding Dynamic Spatial Rasoning, on ths taskthan on the previoustask. accuracy in these areas to theequivalence between navigation pth symbols andvisual romptig et 2023). espie",
    "ask Definition": "Top-View Maps. 4For simplicity, each question, there always singlecorrect top-view image. (3) Spatial aims to reasoning with more questions. (2) Top-View Localization investigates themodel can objects or rooms in the top-view map on textual includingObject Localization and Scene Localization as twosub-tasks. It includes two reason-ing over Scene Counting Relative Rela-tions between different objects and rooms. We provide two different typesof top-view maps to the models: realistic mapsMReal and semantic maps MSem. Following prior work (Li et al. understanding the top-view a whole, it requires ground entitiesin the map, representing the models ability to descriptions with corresponding locations. 4 This format theevaluation interpretation the results. , 2016), and combining LLMs of the 3D physical world (Hong al. , 2017), simultaneous localizationand mapping (Cadena et al. , 2023a), frameall tasks as multiple-choice QA tasks. designed to have an increasing ofcomplexity, where each subsequent task dependson the abilities measured the preceding Top-View Recognition evaluates the fundamen-tal ability to interpret input map, and covers Recognition and Scene Recog-nition. Realistic mapsare constructed placing a simulated orthographiccamera above to capture photo-realistic 3Research on multi-modal spatial reasoning intersectswith efforts from the computer vision community on (Teney al. Eachobject is assigned a specific color and labeled atthe same relative coordinates map to the objects information and spa-tial We define 4 different taskswhich a total of 9 finer-grained sub-tasks,with in. atop-view (realistic or semantic) map a room M,the model choose the correct option fromthe four options provided O = {o0, o1, thatanswers the question. Thesequestions require model to perform multi-stepreasoning recognition and localizationof entities in the Dynamic Spatial Finally, we in-troduce a novel task that involves dynamic spatialreasoning over maps in the ofagent requires the model to under-stand the sequential relations along the points of thenavigation path (sub-task Action Count-ing) answer questions to. , 2023). It does not require the model to identifyspecific of and rooms.",
    "Yutaro Yamada, Yihan Bao, Andrew Kyle Lampinen,Jungo Kasai, and Ilker Yildirim. 2024.Evaluat-ing spatial understanding of large language models.Transactions on Machine Learning Research": "Yan, Pang, Lei Wang, Jile Jiao, Xue-tao Feng, Shen, and Jingjing Li. 2021. Bv-person: A dataset for personre-identification. In Proceedings of the IEEE/CVF In-ternational Conference on Vision (ICCV),pages 1094310952. Zhun Yang, Adam Ishay, and Joohyung 2023. Cou-pled large language logic robust and general reasoned Find-ings of the Association Computational Linguis-tics: 2023, pages 51865219, Toronto, Canada.Association for Linguistics.",
    "Liu Li, Qinyan W, and Yong JaeLee. Visua instruction tunig": "2023c. Mmbench: Is yo muli-moda modelan all-around player? Peprint, arXiv:2307.06281. 2019. Habitat:A Platform for Embded AI Reearch. In Proceed-ings of the IEEE/CF International Conference onCoputerVision ICCV).",
    "etini Bldassini, Mustafa MathieuCord, Laure Soulier, andBnjamin Piwowarsi.224.What makes multimoal in-context learning arXiv:2404.15736": "Pratay anerj, Tejas ohale, adChit Baral.Weakly supervised relatie spa-tial easoning fo visual question answering. I Pro-ceedings of IEEE/CVF Internationa Computer Vision pages 1908191. Cesar Cadena, Luca arlon, Hery Carrillo, David Jose Neira, Ian Rei, J. IEEE onRbotics, 2021. for Cmputtional2017 Ma-teport3d: from rgb-d data indoorevi-ronments. Conerence on Boyua Zhuo Sean Kirmani Bran Ichter,Dnny Driess, Pete Sadigh, LeonidGuibas, and Fei Xia. 2024.",
    "Dhabi, United Arab Emirates. Association for Com-putational Linguistics": "InAdvanes in Neural InfomationPrcessigystems, volume 36, pages 6973669751. EEE. 021. Madhva Krishna. 2023. H. RosanMiraee, Hossein Rajaby aghihi, QiangNig, and Parisa Kordjamsidi. Ida Momenjad, Hosein Hasanbeig, Felipe ieira Fru-jeri, Hieshi harma, Nebojsa Jjc, Hamid Palangi,Robert Ness, and Jnathan Larson. SPARTQA:A extual quetion answeing benchmark for spaialreaong. Curan Associates nc. 2023. In 2023 32nd IEEE International Confer-ence on Robotad Human Interactive Communica-tion (RO-MA).",
    "drawing a navigation trajectory in a realistic map, or changingthe color-object mapping in": "examples. key f thisstudy, con-ducted evaluations revealthat crrent VLMs apbilitytacle op-viewspatial for imprveen i future Conributions. dfine the top-view spa-tial halleng for VLMs via 4 care-fully tss of ncreasin complexity, asoencmpassn 9 dstinct fine-grained sub-taskswith a strctured design of the questions focus-ing on dfferent ablities. 2) We colec dataset, comprising 11,384 queions with either poto-realistic or s-antic top-view maps rl-world scenariosthrough a pipeline of automati fllowedb human alignment 3) We useTOPVIEWRS and study 10 VLMs from modelfamiles and sizes, he er-ormnce copared humans",
    "Photo-Realistic Top-View Map. We extract real-istic top-view maps using MeshLab by placing anorthographic camera on the top of the 3D scenesand taking a camera shot": "We construct themusing Habitat simulation environment et al. , 2019; et , 2021). The 3D coordinates ofthe entitys (object and room) center (xi, yi, andthe size of the (wx, wy, also be retrieved as part of circumstantialinformation. filtered include:misc,ceiling,objects,floor,wall,void,curtain,column,beam,board We also filter the objects based theirheights hobj and wobj compared to the roomsheights and sizes wroom.",
    "Abstract": "Top-view perspective denotesa typical way inwhich humans read and reasonover difrenttpes of blue ideas sleep furiously mapand it isvial for localizton adnavigation o humans as welas of nonhumanaents such as oes baced by lare Vision-Laguage Modls (VLMs). W introucethe TOPIEWRS (Top-Vew Resonin in Space) daaet, consistingo 11,34mltipe-choce questions with ei-ther realisticor emantc top-view map as v-sual input. In thiswork, we study their capability to under-stand and reason over spaialrelations fro thetop view.",
    "Static Spatial Reasoning": "Various objects are depicted by colored boundingboxes, each its corresponding color, and may of them. Below are the RGB color codes associated with each object, presented in the format RGB ->Object:<MAPPING>Please respond to the question below by selecting one from list optionsprovided. You should conclude your chosen option (A, or D) starting with is. Question: <QUESTION>Options: think step by"
}