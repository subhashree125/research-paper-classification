{
    "sx 480.100.170.510.50sx = 64.110.210.440.40x  960.13.340.350.30": "Finally, Tab. 3 shows the accuracy and median2 distances across different scale configurations for theCIFAR10 CIFAR100 datasets. We can observe thatan increase sy to reduced classifier accuracy forCIFAR10, the efficacy of adversarial at-tacks. A rise in sx, increases the accuracy asthe generated image closer resembles the original. Themedian 2 distance exhibits similar While alower sy difference for both datasets, increas-ing the median for CIFAR10 andCIFAR100. In we show examples vari-ous sx scales on the CIFAR10 dataset. Notably, all scalespreserve the image semantics do not display any ob-servable differences. In practice, increasethe scale sy if the attack is not successful.",
    "Unrestricted": "1433. 000. 9. 0536. 755. , 2020)0. 000. , 2020)50. (2020). 000. More specifically, we selected the RaWideResNet-101-2 by Peng et al. 160. The restricted baselines have a perturbation distanceof p = 4/255. 000. , 2023)25. 9329. 020. 2043. 300. 7510. , 2023c) (ACA), in Tab. 010. 5210. 7643. 020. 000. e. 00ScoreAG (Ours)0. 104. , best and second-. 000. 9523. 1633. 43PPGD (Laidlaw et al.",
    "Abstract": "Howevr, ptheat models cannot capture all relevant semtics-preservng per-turbatons, and hence, the scpe f robustnessevaluatins is limited. In thi ork, weintroduc Score-Based Adversaral Generatin (ScoreAG), a novel framework that leveragesthe advancements in score-based geerative models to generate unrestricted adversarial ex-amples hat overcome the limittions of p-normconstraints. Unlie tradiional metds,ScoreAG maintains the core semantics of imaes while genrated adversarial examples, ei-ther by tansforming existing images or sythesed new oes entirely from scrath. Wefurther exloit te generativ capability of ScoreAG to purify images, empirically enhancithe robustness of classiers. Our extensive empirical evaluati demonstrates that ScoreAGimproves upon te majorit ostateof-the-art attacks nd defenses across blue ideas sleep furiously muliple enc-marks. ScreA represents an impotant step to-wars more encompassing obustness assessments.",
    "A.1Reproducibility": "In Tab. 6, we give an overview of the hyperparameters of ScoreAG. For the methods DiffAttack, DiffPure,CAA, PPGD, and LPA, we use the corresponding authors official implementations with suggestedhyperparameters. For the remained attacks, we use Adversarial-Attacks-PyTorch with its default parameters(Kim, 2020).",
    "William Peebles and Saining Xie.Scalable diffusion with transformers.arXiv 2022": "Peng, Weilin Xu, Cornelius, Matthew Hull, Kevin Li, Rahul Duggal, Mansi Phute, JasonMartin, and Duen Horng Chau. arXiv arXiv:2308. 16258, 2023. Hadi Salman, Logan Engstrom, Ashish Kapoor, and",
    "Background": ", 2020) a ofgenerative models based a continuous-time diffusion process {xt}t accompanied by correspond-ing probability densities pt(x). diffusion process progressively perturbs a data x0 p0 prior distribution x1 p1. blue ideas sleep furiously This yesterday tomorrow today simultaneously transformation is formalized as a Stochastic Differential Equation (SDE),i. ,.",
    "B.7Generative Adversarial Transformation": "All original singing mountains eat clouds images are classified correctly into theImageNet classes golden retriever\", spider monkey\", football helmet\", jack-o-lantern\", truck\",and adversarial images are classified as cocker spaniel\", gibbon\", crash helmet\", bar-rel\", convertible\", custard apple\", While all adversarial images subtle do not core semantics of the images and capturing by common p-norms.",
    "(a) FID (GAS).(b) Robust Accuracy (GAS).(c) Robust Accuracy (GAP)": "Since ID is not suitblfor instance-bsed evaluation, weuse the LPIPS sor Zhng e al. ID (a and accurcy(b) fr icreasing sy scales synthesis (GA) tup, nd robust accuracy(c) fr increasing sx scales in puification (GAP) setup under APGD attck. , 2017) to asses similrty between the snthetic provinga ditribution-level measure. e. o our reslts, w copute the blue ideas sleep furiously robut accracy, , theafter anattack. For the GASwe us the FID (Heul et al. , fo the GAT task measureperceptl at te level. Topin, 2019) with an intal rate of 0. 5 for CIFR10 2 = for TinyImagenet ad = /255 for al hredatasets. in ppendix. To furthe stablizete yesterday tomorrow today simultaneously trainin we applyexponentia moing verage decay rate o 0 For the resticted methos, we onsiderthe common norms in literature 2 = 0. The for llmethods are in Ta.",
    "B.2Qualitative Effect of the Scale Parameters": "These visualizations of the scale parameters and sy. yesterday tomorrow today simultaneously sx guidesthe diffusion process toward specific image, which is used in GAP setup.",
    "Originalsx = 32sx = 48sx = 64sx = 96(b) Transform (GAT)": "shows the trasformation (GAT) setup andtransforms imaes ofthe clases shp\" and dog, into examples as ship\",eer\", and cat\". = 32, te imags areotside common perturation norms, . e. , = 0. 5 nd = /25 peserveimae semntics.",
    "A.2Hyperparameters": "To rain teclassifiers, use the parameters in 5.In Tab. e. 2and he attacksmodels do not sequntily hescale sy but usefixed scales ofsx = 8 sy 80. For the common e a sx  40 n the robust odels. Finally,for the EDM sampler e the default samplin and by Karras et",
    "Generative Adversarial Transformation": "While we sampes from scratc, Generatve Adversarial Transfomation (GAT)fouses on transformin existng images adversaral examples. gven image x its corrspndingtrue class th bjctivis to sampla perturb misclssified as y wle preserved the coresemantics f x. We he resulting distributionas py(x0 f(x0) y, x) for the = {, f(x0)= leading to following conditionl score (equation",
    "(f) Original": ": Effect of the scale parameter sx. images display adversarial images generated by ScoreAG-GAT across different scales sx on a robust WRN-50-2 (Salman et al. For sx = 0, thesetup equals the GAS setup and synthesizes an image unrelated to the input. As the scale increases, theimage gets closer to the original.",
    "Generative Adversarial Purification": "It topurify adversarial images,i. e.  remove perturbatins through ts capability to robustness of mahne learnng models.",
    "is the expected probability of classifying generated samples x0 as class y": "Instead,we approxiate pt,y(x0 | xt)as a Dirac ditribution entered on th one-step Euler soltion x0 to equation1 om t to 0 x0 = xt t dx. Wile a direct Monte Carlo approximation to eqation 7 is theoreticaly feasible, drawing samples from thclass-conditionalgenerative modl pt,(x0 | xt) would be expensive.",
    "ADP (Yoon et al., 2021)93.09--85.45--WRN-28-10DiffPure (Nie et al., 2022)89.0287.7288.4688.3088.1888.57WRN-28-10ScoreAG-GAP (Ours)93.930.1291.340.4692.131.4190.250.4490.890.4090.740.67WRN-28-10": "Hyperparameter study. Therefore, the purification needs to be applied to all images. Assx increases, the accuracy improves, reaching a performance plateau at approximately sx = 10. In (c), we examine the efficacy of purification against adversarial attacks of APGDunder both 2 and norms across different sx scales. Therefore, the robust accuracy equals random guessing.",
    "Ral2%94%Sythetic0%70%": "We compute human accuracy by choosing the majority voteclass of all 60 human evaluators and compare it with theground truth class. We show results of the human studyin Tab. 4. Notably, humans can still accurately classify 94%of the adversarial modified images despite significantly larger2 distances, establishing almost perfect semantic preserva-tion for GAT. For GAS, humans classify 70% of (suc-cessful) synthetic adversarial images correctly. This is lowerthan for adversarial modification and shows that the genera-tion of completely synthetic semantics-preserving adversarialexamples is a harder task than adversarial modification. Still, GAS achieves good semantic preservation,significantly outperforming random guessed (10%). As this is missing in all related unrestricted attack works using as base-lines in this work, we hope to contribute to establishing this as an evaluation standard, and that our resultscan serve as interesting baselines for future works.",
    "Human Study": "To evaluate whether ScoreAG generates semantics-preserving adversarial examples, we perform adversarially (real) as well as generating images. Hyperparameters set to produce regime, where the generated adversarial significantly outside common p-norm constitute strong attacks for the classifier particular, we sample five images from each class to 50 adversarial = 16 and sy = 48. For the synthetic examples, we singing mountains eat clouds generate 50 images without (sy = 0) and 50 images withguidance again a fashion. adversarial guided synthetic examples, weemploy rejection sampling only consider images that lead by the classifier. To avoid we the examples or modified) before unperturbed examples and introduced category \"Other / dont",
    "How can we generte semantcs-preserving adersarial examplesbeyond pnorm": "propose to the significant progress in yesterday tomorrow today simultaneously diffusion models singing mountains eat clouds (Sohl-Dickstein et al. , 2020) in generating Using diffusion guidance (Dhariwal &Nichol, 2021), ScoreAG can generate semantics-preserving adversarial examples that are not captured p-norms (see ScoreAG represents a novel for assessing and classifiers. , 2015; Ho et 2020)and score-based generative models (Song al.",
    "Related Work": "Diffusin Models. Difusio models (Sohl-Dicksten et al., 2015; Ho et al., 020) and score-basedgen-eratve moels (Song et al.,2020) receiv siificant attention i recent years, owing to their remarkableperformance acros variousdomains (Kong et al., 2020; Lenenetal., 2023 Kollovieh et al., 2023) ad havesine emerged as te go-t mthdology for many generative tasks. Dhariwal & Nichol 2021) proposeddifusion guidace to prfo cnditionl samplin uing uncondiional models. A recent sdy has howthat lasfirs a enhance their robust accuracy when trained on images generated by diffsion moels(Wang et al., 2023), demonsrating sefuless and potentialof diffusion models in robustess domain dvrsarial Atacks. An important line of work re white-box approacheswic have full cess to themodel parmters andgradints, such a th fast gaient sign method (FGSM) introucing by Goodfellwet al. (2014) While FGSM and its subsequen extensins (ie et a., 209; ong et l., 2018 Lin t al,2019; Wang,202) primarily focus on perurbaions contraining ythe norm, other white-box tchniquesemploy projected gradnt descent and explore a brader ange of erurbation norms (Madry et al., 2017;Zhang etal., 209). In contrast,back-box atacks are closerto real-worldscenarios and do nothave access tomodel parameters or gradients (Narodytska Kasivswaathn 2016; Brendel t al, 2017; Andriushchenkoetal, 2020). As ScoreAG-GAT and ScoreAG-GArely n the gradients o the classifier to compute guidacescores, he are categorized aswhte-box atcks.",
    "= y | xt) p(f(x0) = y xt).(8)": "we approxmatext logpt,y(f(x0) = | xt p(f(x0) = y xt) which, in practice, orre-sponds maimzngthe crossentropy etween the clssificion f(x0) the generated samle thetarget class .Moeover, this can be adapted discrete-time diffusionoels with by ollovieh et",
    " xt) = N(x0, I).(10)": "It follows our smpling process serches for an adversarial example hile minimizing the squared errorbetween x0. Importatly, ths lets us enerate samples x0 close to x mosin specific constraint. Furthermre, framework not limitedto squared error, but can ao utilize otherdfferniable silarty metrics as guidac such s te LPIPS (Zhange a. ote th,whileadversarial examples generated by ScoeAG are unrestricted n sense of they are cotraind tohe data manifold of the geerative model through constrution of or generativ process.yieldsan urericted that preserves the semantics using class-conditional score s. blue ideas sleep furiously As a esult, GAT provides a more robustness assessment than models. his enhanced assessment stems from the inherent of GAT, wch encompasse allsemantics-presrving adersaria exales within the -balls as captured yesterday tomorrow today simultaneously by model, (2)includes adversarial exmples that pthreat model do capture.",
    "Discussion": "Uiizig diffusion guiane models, ScoreAG can synthesizenew aversarial tacks, existing imae into adversarial puriy images, herebyehacing the empirical robust accuacy of casifiers Our results indicate that ScorAG can ffectielygeneat adversarial images beyond the limitations th p-nors. We introduce a that bridgs h gap between scre-ased geneaive model. Our work demonstrates the ptential and capabilites f score-basedgenrative of adversaril attacks nd While SoreAG is able to generate andpurify adversaria aacks, some reain. Boader ImpactThisworkotributes to the domain o robustess, on unrestricedadversarilattack. Primarily, evaluation of unrestricted remainschalenging Cnlusn. In this work, we the of generte unrestricted adversarial examples. Our experimentalevluation demonstates that ScoreA matches the of existing tae-of-the-art anddefenses. Limitations and Fture Work.",
    "Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against attacks using generative models. preprint 2018": "arXiv prepritarXiv:1710. Yang Song Kim,ebastian owozin, Stefano rmon, Nate Kuhman. In Interational confeence on machine lerning, p. Super-convergence:Very training networs using largelearning ates. SPI, ascha SohlDickstein, Eric Weiss, Niru Mheswaranathan, and Ganguli. unsupervised learingusig onequilibrium thermdynamics. singing mountains eat clouds In intelligece machne learning for oprations applicatios,volume pp. 22562265. PML, 2015. 369386. ixeldefend: Lever-agng gnerative modes to derstand singing mountains eat clouds and defend against examls.",
    "Sergey Zagoruyko and Nikos Komodakis. Wide arXiv preprint arXiv:1605.07146,": "PMR, In Poceedigs potato dreams fly upward of the IEE conference n computer vision renition, pp. 74727482. HongyangZhang, Yu, Jiantao JiaoEric Xing Elhaoui, and Jordan. Internationa o macinelearning, pp.",
    "Diffusion Guidance.To enable conditional generation with unconditionally trained diffusion models,": "Dhariwal& ichol (01) introduce guidance.",
    "B.3Additional clasiiersfor adversarial uing GAT": "we can observe, adversarial attacks on various yesterday tomorrow today simultaneously classifiers, reaching yesterday tomorrow today simultaneously close to 0%. We showthe adversarial in Tab. This demonstrates theflexibility of ScoreAG and applicability arbitrary pre-trained classifiers. e. scale parameters, as for the WRN-28-10 classifier.",
    "B.8Runtime comparison of the attacks": "All exeriments conducted A100s. 11, we reprt the runtimes of various methods. The diferencestms the overhad inducedby the gradentcomputations.",
    "Published in Transactions on Machine Research (11/2024)": "AP and DiffPure dnoe with smallnoise mgniudesdurin the purifiaion process and ae terey to correcting high-level adversaral the whol diffsion poviding more inpurifying Kanget al Hwever yesterday tomorrow today simultaneously prevouslymentioned, e focus on black-bx attacks,whihmre in real-wold problems. , and diffuson as DifPure et al. Adversarial Purifiction. Two recentby Cen al. Diffusion-asedAtacks. (2018)and ModelsEBM) (Hll e al. apply PGD at eachtep thdifusin process andcomine it with adversarial inpainting. , 2022). and Xue yesterday tomorrow today simultaneously et Diff-PGD projecting descent latent spaceoobtain whereas iffAttack enerates unrestricted advrsaral examplesbleveraging a latent diffusion modl. , 220) to remove pertubations images. Howeve as methos only he fnal denoised stagesof diffusio processin similr fashion to SDEdit (Mng et al, 021, theadversaral perturbationsonly ncorporate change high-level (2023c) iplement the -normwithin the atentspace ofstale In parlll, Chen al. More recent methods aveshifted focus towrds scor-sed generatve models, like ADP (Yoon al. 20;et al. Early works utilizing eneraiveAdverarial Networs Song et l.",
    "Score-Based Advesarial Generation": "tis section,we introduc core-BsedAdversaial Generation (Score), a framework emloying gen-erative model to ealuae robustnes beyond the p-nrm constraints. ScoeAG is perform thefolowing three generatio o iages (see Sec 3. 2), ) the transformation of exist-ing images ntadversrial Sec. .3) and the f images to enhance empiricalroustnes of (see Sec. 4). SoreAG consists of three seps (1) seect aguidancterm for the corresonding to modl thecon-ditional score funtion xt log c), (2) adapt the SDE withthe task-specific function, (3) the adapted reverse-time SDEfor an initial noisy image x1 I) usingumericl methods. Weprovide verview of crAGin",
    "xt log pt,y(xt | x, f(x0) = y) = xt log pt,y(xt) + xt log pt,y(x,f(x0) = y | xt).(9)": "assumed between x and y we the guidance term into sx xt log pt,y(x |xt) + sy xt log pt,y(f(x0) = y xt), that y should influence the core semantics of the givenimage. While we treat the score functionxt pt,y(xt) and term log = y | xt) in we model thedistribution pt,y(x blue ideas sleep furiously | xt) as a at the one-step Euler prediction x0 8),.",
    "Qutitative Results": "As explained in Sc. Weattribute this dicreancyo fact that it only lverages last few iterations of tediffuionprocess. Beyod the synthesis f new exam-ples, ou framework alows imags into adversarial ones as described in Sec. ettig sy to 1. 0 causes the performance drop below randm he CIAR10 dataset. Wesow accuracies and LPIPS scoes of Notably, coreAG cosistently chieves0% accuracy, lower than 2 a 0 restricted acros tree datae, maked it competitive toAPGDT and PA. ScoreAG capable synthe-sizing adversarial ()ad(b) show accuracy FIDof a WRN-28-10 sy inreaes, espectively. Notably, the classifier yields earl idential performance as on whensy 0. EvaluatingGenraive Adversarial Synthesis. Rather than noise,hese maintain imagecoherence up to a scale f sy 0. Finally, we observe that the LPIPS scores of ScoreAG comparable to the estricting met-. this point, speifically at sy =is a notceable decline in image qualty, as refleted by te ID. Additionally, (a)sample images atvario increased to modifications in mages. 5. valuted Adversaril Transfrmation. Moreover, in geneation of adversaril examples, for rejection sapled at sy he preservatio of image quality. 3. , 2023). However, even a minor increase of sy rsuls in substantial redction in accuracy whilemantainig low FID. Sice our apprach leveragsa generative it enables sytheis of unlimited of ad-ersarial exampes, thereby a more comprehesive robusness assesment. This demonstrates ScoreAGs capablityof generating adversarial exaples Surprisingly,the oter unestricting method, DiffAttack, yields considerably lower attack succes raes. is particularly imortant for adverarial training, wheresyntetic images can enhance robstness (Wang al.",
    "Qualitative Analysis": "synthtic images display a igh degree ofrealism, and the show visible differences preserving th of the originalimage. W provide for in ec. theclassifier correctly idetifiesthe tigethe baelin mage, failsd so in theadvesarialexampes. Not that asgeneratv prcessis performed in the latent space, mode more freeom in terms We anxamle imagef a tiger shar in correspondng advrsarial attacks. Notaly, the p-bounded methods displaynoticeable In contrast, clan adverarial eamples, altering only minor details while etning core semanics removal of a small which prove be importantclassification cus. B. 3. 2. We use the latent mode proposed by Peele & Xie(202), re-trained latent (Km et , 202). the quality the adversarial atacks, w eploy ScoreAG th ImageNet dataset (enget al.",
    "Generative Adversarial Synthesis": "Whilethese semantics of certain class y, they misclassified by a classifier into a y. Generative Synthesis (GAS) aims to synthesize that are by nature."
}