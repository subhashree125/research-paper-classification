{
    "Abstract": "Promptale segmentation foundation odelshave emerged as a trasformative aproach to addesig thdiveseneed in edical bu models rquire expenive computing, poing ig barrer their adopininclincal practie. Morever, phase advancedhrough design perormance and * \"S reproducibiliy tasks, resultingn improved algoihs valiated reproduibility of the winning solution. Furthermore, the best-pefomingalgorithmshave beenincorported into thesoftware witha user-friendly interface * \"S to fcilitate clinical adoption. dataand arpublicly availabl to the fute developmnt ofmedical image segmenation foundation model and pavethe way for impacfu real-world applications.",
    "Dataset curaion andpre-processing": "normalized them sae npz with image and stadard inside it, llowing to get rid of edious data cleningand on model developmnt. he originalimag ave a wide of frmat, suh as nifti, dicom, nrrd, jpg, and pg. 5% prcentile forground intensity and then rescald theintensity. 5% and 99. Specifically, for images, w irst adjusted the intensity proper lvel and wdth followed rscaling to and PET we appled with the lower-bound of 0. The followd common. Allthe training image wee available datasets with a license for re-distribution (Supplementary -2. Otherwise, no preprocessing was aplied.",
    "scheme and statistical analysis": "Wilcoxon test was used to the performance of the also used thebootstrapping to analyze ranking stability. We the commonly used scheme to the final rank , , which contains threesteps. Finally, we averaged the rankings to obtain thefinal rank. Then, weranked the 23 algorithms teams for each modality and each metric.",
    "Post-challenge analysis": "To mitigate theeffects of trainng datasets, we eached out to the teams withthe highest DSC and NSD scres in each modality and. We found that non f thealgorithms consistently achieved the best prformanceacross all modalities,idiating thatthese algorithms could be complementary. Although all the top-three algoritms only used the providetrainin set usin external public ataset from the predefin dataset listwas allowed and encouraged.",
    "Competition Dataset: A Global Collaboration": "To ensure a fair comparison, participants restricted using only officially preprocessing dataand included in curated list model development. b, The number of image-mask pairs thepublic trained set. All testing has newly collecting was not publicly available d, Geographical of participants and data contributors in the tested set. We provided a diverse set of training and testing images to facilitate robust model development and eval-uation. Competition dataset. challenge attracted participation from over and data across institutions,which highlights the international interest advancing medical image and demonstrates competitionssuccess in engaging wide array of contributors from different regions around the world. majority of the medical image segmentation have been included this competition, weopted to create * \"S completely new testing set rather than annotated existing datasets to mitigate the potential risk of While the testing set spans a wide of imaging modalities, and MRI continue reflecting their prevalence clinical diagnostics. illustrates visual from the and testing sets across various imaging modalities,accompaniing by their corresponding bounding box * \"S prompts and masks. Participants were also encouraging to contribute by publicdatasets the list.",
    "Competition Design: Universal and Lightweight Medical Image Segmentation Foundation Models": "The competition task is designed for promptable medical image segmentation. bounded box is selected as the prompt because it can precisely specify the target, which has lower ambiguity thanpoint prompts. The challenge consists of three distinct phases: a development phase lasting 122 days, a tested phasespanning 35 days, and a post-challenge phase of 35 days. These phases focus respectively on model training, evaluation ona hidden testing set, and subsequent improvements and reproducibility analysis. During the development phase, participants were provided with a large-scale and diverse training dataset, servingas the foundation for participants to design, develop, and train their models. Moreover, an online leaderboard wasmade available on Codabench , allowing for automatic evaluation of participants results. Participants could leveragethis feature to test their models against a tuning set, obtain performance feedback, and iteratively tune and refinetheir approaches. This phase was instrumental in enabling participants to optimize their models and prepare them forsubsequent testing. During the tested phase, the top 20 teams from the validation leaderboard were invited to make tested submissions. However, submissions were not limited to these top teams and other participants were also welcome to submit theirsolutions for evaluation. Participants were required to encapsulate their algorithms in Docker containers, ensuring astandardizing and portable format for evaluation. The teams were rankedbasing on both accuracy and efficiency of their models. Specifically, the evaluation criteria included the Dice SimilarityCoefficient (DSC), Normalized Surface Distance (NSD), and runtime performance. This rigorous and equitable processdetermined the testing leaderboard rankings. This phase aimed to push the boundaries of segmentation performance usingcollective knowledge and assess whether the winning solution could be easily implemented by others. In performance-booster subtask, participants were encouraged to enhance their models by incorporated two new datasets, fast inferencemethods, and other optimization strategies contributed by participants. In reproducibility subtask, all participants wereinviting to reproduce the results of the top-performing teams. These collaborative efforts not onlyfostering the development of a refined solution capable of outperforming original winned algorithms but also verifiedthe reproducibility of the top solutions.",
    "Acknowledgements": "Van Calster, G. Wei, H. Han, C. B. Jujjavarapu, R. 7, p. Chen, Z. Wu, R. Kreshuk, T. Bakas, M. Jannin, C. F. Buettner, E. Nickel, J. Chou, Y. Cheplygina, B. M. Zhai, T. 00714,2024. 103061, 2024. Green, M. number: 2023 EKFK. Jaeger, Y. Li, H. I. Landman, G. Jakab, S. Li, J. Hutter, Decoupling weight decay regularization, in International Conference on Learning Representations, 2019. Ma, B. Ma, Y. Maier-Hein, nnu-net: a self-configuring method for deep learning-based biomedicalimage segmentation, Nature Methods, vol. P. Zhao, and I. -H. Li, and L. Kolesnikov, D. Haase, D. Mattson, E. Kleesiek, L. Pritzel, T. Feichtenhofer, Sam 2: Segment anything in images and videos, arXiv preprint arXiv:2408. Wiesenfarth, Q. 15851, 2024. Bi, J. M. Wang, X. Yang, H. Marinov, P. Fei-Fei, Imagenet: large-scale hierarchical image database, in Computer Vision andPattern Recognition, 2009, pp. Stiefelhagen, Deep interactive segmentation of medical images: A systematic review andtaxonomy, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. Litjens, A. K. 12, pp. -H. WANG, and S. -T. C. Peng, M. was supported by Juan de la Cierva fellowshipby the Ministry of Science and Innovation of Spain with reference number FJC2021-047659-I. Blaschko, S. Li, Y. Dou, and X. BAI, T. 7, pp. Kehtarnavaz, and D. A. DA19. C. Cai, D. Nguyen, and M. Jumper, R. Guyon, Codabench: Flexible, easy-to-use, and reproduciblemeta-benchmark platform, Patterns, vol. 18, no. Li, Y. Huang, X. Fu, and L. Huang, B. Varoquaux, and P. 25, 2012. Weissenborn, X. M. Tunyasuvunakool, R. Nichyporuk, F. , Identifying the best machine learning algorithms for brain tumorsegmentation, progression assessment, and overall survival prediction in the brats challenge, arXiv preprint arXiv:1811. Huang, J. Dong, R. Antonelli,T. Z. Kleesiek, M. Chen, X. 203211, 2021. Kurc, B. Y. Yao, X. Bello, J. Jia, J. Madani, K. Sun, R. was supporting by DFG KO 7162/1-1 543939932 and FWF10. 17 30217 313. 2019. was supporting by JST SPRING, Japan Grant Number JPMJSP2106. Hutter, Daft: Data-aware fine-tuning of foundation models for efficient and effective medical imagesegmentation, in CVPR 2024: Segment Anything In Medical Images On Laptop, 2024. Ryali, T. 44, no. A. G. A. S. Pieper, and K. Saiz, M. Chen, S. H. 12, pp. Merhof, P. Bassi, Y. M. Yang, Q. Zhang, Y. A. Eisenmann, L. weresupported by University of Modena and Reggio Emilia and Fondazione di Modena, through the FAR 2024 and FARD-2024 funds (Fondo di Ateneo per la Ricerca). 35233542, 2021. Qu, X. Eisenmann, D. H. 11, p. Chen, P. 21, no. Summers, A. Reyes, A. Gao, and Y. Lai, Q. Zhao, Y. Alwala, N. J. Ma, H. He, A. He, T. G. Martel,P. M. 195212, 2024. Shi, T. E. Wu, J. Jager,Metrics reloaded: recommendations for image analysis validation, Nature Methods, vol. Bifulco,M. Wu, and D. Cyran, and T. Springer, 2013, pp. Han, Z. Ferrer, A. Wiesenfarth, A. Huang,M. Han, L. 6, no. Chen, H. Li, L. Yuille, and Z. , Highlyaccurate protein structure prediction with alphafold, nature, vol. Xing, L. Sutskever, and G. Deng, W. He, H. Zhang, H. Pan, K. Yang, L. Choi, N. A. were supported by Vingroup Innovation Foundation (VINIF) inproject code VINIF. Kustner,Results from autopet challenge on fully automated lesion segmentation in oncologic pet/ct imaging, Nature Machine Intelligence, vol. W. Huang, J. K. Y. 02629, 2018. W. He, F. Muller, B. Shetty, M. H. Minaee, Y. S. Y. Sadegheih, A. A. E. Usuyama, H. 520524, 2023. Lee, Segment everything everywhere all at once, arXiv preprint arXiv:2304. Gu, H. Mintun, J. Rolland, L. Kirillov, E. This work was supporting by the Natural Sciences and Engineering Research Council of Canada (RGPIN-2020-06189 andDGECR-2020-00294) and CIFAR AI Chair programs. Xu, S. 1, pp. Zhang, S. P. 15, p. I. Farahani, L. Zhang, F. Yang, K. Karargyris,A. Nguyen-Mau, H. Rajpoot, N. Tran, Medficientsam: A robust medical segmentation model withoptimizing inference pipeline for limited clinical settings, in CVPR 2024: Segment Anythed In Medical Images On Laptop, 2024. Pavao, M. Stiefelhagen, J. -D. Bakas, A. K. Wei, K. Nguyen-Vu, T. J. A. Dong, H. Abel, C. F. Poon, and S. L. Cheng, J. Marinov, R. J. Godau, M. Mazurowski,S. was supported by Else Kroner Research College for young physicians (ref. You, and B. 6,no. Han, Efficientvit: Lightweight multi-scale attention for high-resolution dense prediction, in Proceedingsof IEEE/CVF International Conference on Computer Vision, 2023, pp. Baumgartner, S. was supported by NIHK01HL143113. Chen, F. Dong, J. Kartasalo, P. Purucker, and F. Hashimoto, M. 102918, 2023. Gan, and S. K. Zhang, Z. A. He, Y. Wang, H. Shi, Y. Zhang, Y. Ingrisch, C. Roth, D. de Wilde, G. Dehghani, M. Cao, H. Zhao, Y. B. Saez-Rodriguez,C. Egger, J. 92, p. T. Landman, M. Kumari, R. 596, no. C. This research was enabled, in part, by computing resources providedby the Digital Research Alliance of Canada. 115, 2021. 28, no. Xue, Y. M. Minderer, G. Arbel, S. Tang, Y. Wang, J. Zhuang, H. Wang, and W. Tang, F. Beyer, A. Li, C. Zhang, J. A. Wang, foundation model for joint segmentation, detection and recognition of biomedical objects across ninemodalities, Nature Methods, 2024. 583589, 2021. Hu, C. J. Zou, J. Z. Glocker, F. Isensee, Z. Porikli, A. A. Pan, S. Vosburgh, 3d slicer: platform for subject-specific image analysis, visualization, and clinical support, inIntraoperative imaging and image-guided therapy. Gatidis, M. Kleesiek, and R. 1, pp. A. Unterthiner, M. was supportedby Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) Project-ID 499552394 SFB 1597. Ravi, H. Lambert, T. 02) N. A. Wu, X. Y. Acion, M. Xu, X. 46, no. Isensee, J. Ulrich,M. Potapenko et al. Petersen, and K. Siegel,S. Gao, A. A. -H. Wang, Unleashing the strengths of unlabelled data in deep learning-assistedpan-cancer abdominal organ quantification: the flare22 challenge, Lancet Digital Health, vol. Plaza, N. Y. Tsaftaris, B. Yang, M. X. Xie, J. van Boven, R. Zhou, Abdomenatlas: A large-scale, detailed-annotated, & multi-center dataset for efficient transferlearning and open algorithmic benchmarking, Medical Image Analysis, vol. 248255. L. Heiliger,Z. Roy, K. Rempfler, A. Xiang, S. Nikolaou, C. He, J. M. Moung-Wen, B. 7873, pp. Mazurowski, H. Yuan, C. 654, 2024. Yu, H. C. A. B. E. Zhu, G. Wang, M. Li, A survey on trustworthiness in foundation models for medical image analysis, arXiv preprintarXiv:2407. Kahn, D. Li, Y.",
    "Evaluation metrics and platform": "folowed the ecommen-ations i * \"S Metrics Reloadd o evaluate the segmentation ccracy The efficiec was byruntime (second) adwe recrded te Docker container execution eac case. T evaluation metris containd two segmetaton ad one eficiency meric. ubmtted Dockeralgorithms were evauaedon the samepltform with Ubutu 4 system, hichcontais Intel CPU (Xeon W-2133 60GHz) and of 8GB RAM.",
    "arXiv:2412.16085v1 [eess.IV] 20 Dec 2024": "Importantly, we collaborate with 24 institutions worldwide to curate a new testing set with over 4000 cases, allowingfor fair model evaluation while minimizing the risk of data leakage. Moreover,we work with the two best-performing algorithm developers and integrated the models into the open-source imagecomputing platform 3D Slicer with a user-friendly interface, making state-of-the-art segmentation models more accessibleand practical for a wider range of clinical applications. In this work, we present the competition results for promptable medical image segmentation, specifically designed toencourage the development of more efficient segmentation foundation models for diverse modalities and segmentationtargets. a, The task is to develop universal segmentation foundation models that can accept various medical image inputswith target bounding box prompts and generate the corresponding segmentation masks. The top 20 teams on the validationleaderboard are invited to submit their algorithm dockers and we manually evaluate them on the hidden testing set. Based on this comprehensive benchmark dataset,the top-performing algorithms demonstrate substantial improvements in both segmentation accuracy and efficiency, withmodels achieving segmentation results over ten times faster than existing SAM-based foundation models ,. The model should be lightweight and deployable onlaptops without reliance on graphics processing unit (GPU). During the development phase, participants traintheir models on the training set and obtain performance metrics on the online validation set on Codabench. Competition design.",
    "Baseline: LiteMedSAM": "These preprocessing andaugmentation steps consistent the distillation and fine-tuning stages. During distillation process, MedSAMs serves as the model, while Tiny-ViT acts thestudent model. 005, whichever occurs first. In stage, distillthe extensive medical imaged from MedSAMs heavyweight image encoder into Tiny-ViT ,a compact hybrid architecture combining Transformer and convolution layers. In the second stage, the entireLiteMedSAM To the images, ViT-b encoder, the images are first resized to 1024 x 1024 using bi-cubicinterpolation, Tiny-ViT, longest edge of input images is first resized 256 interpolation, andthen the images are padded to 256 x 256 on the and bottom maintain a consistent resizing,min-max normalization is scale intensity values from range to. LiteMedSAM involves two-stage process: fine-tuning. For fine-tuning, the images resized to 256 x 256 used the sameimage processing steps as in distillation stage for Tiny-ViT. Both the distillation and fine-tuning of LiteMedSAM performedon the same dataset used to train MedSAM. We adopt Mean Error (MSE) loss to encourage of the to match those of the and a batch The process continues either 1000 reached or the * \"S no longer same AdamW as distillation stage are used.",
    "Perfomance aalysis the hiden testin set": "All the submitted algorithms independently executed on the same computing platform (Methods) for a fair compari-son. and b show the average DSC across the 2D and 3D datasets, 9. The overall range of scores for 3D segmentation appears in the 2D case, reflecting the added complexityof volumetric segmentation tasks. 3D segmentation results also more variability with lower medianscores compared to 2D bubble in Fig 3c and d present the trade-off between segmen-tation and * \"S efficiency, as seconds, the 2D and 3D datasets, respectively. size of thecircles is to the NSD score. For segmentation, most algorithms achieve high DSC scores (above 0. 8) a runtime of less than 10 seconds image. Notably, teams seno automlfreiburg achieve exceptionalaccuracy and efficiency, with NSD near Nevertheless, seno and automlfreiburg still obtain inference with competitive DSC NSD demonstrating a great trade-off segmentation accuracyand Next, we a fine-grained analysis of the three overall best-performing teams and baseline across the ninemodalities 3e, Supplementary -9). For the modalities CT, MRI, and PET, T2-automlfreiburg and T3-skippinglegday comparable which are significantly better than T1 (p < 01). T2-automlfreiburg, while competitive endoscopy, fundus, andmicroscopy, shows performance on other modalities, particularly in ultrasound and X-ray. The LiteMedSAM baseline,while competitive for such as PET and X-ray, lags substantially behind for most modalities, with lowerscores and wider performance ranges. Furthermore, we compare the runtime of the four algorithms across 3D and 2D contrast, the exhibits higher runtimes andsubstantial variability, particularly for CT, runtimes exceeding 300 seconds in large scans. Similar trends areobserved modalities (Fig 3g). Notably, T1-seno consistently consumes around one to seconds for all which is five to ten times faster than the baseline model. Finally, we visualized some segmentation examples of the three best-performing algorithms for each modality 3h,Supplementary ). The algorithms versatility, achieving results for most of even if the object boundaries not clear. However, challenging cases highlight areas where further is needed. For instance, in 3D modalities like CT MR, the of heterogeneous lesions, characterized byirregular shapes and varying intensities, leads to some These could be addressed by enriching training datasetthrough to include more diverse and rare cases. refinementmechanisms, which allow users to provide real-time could improve segmentation results.",
    "T2: 0.782": "Dot and box plot of the vee DSC and NSD scores on the a, * \"S 2( ,309 images) and b, 3D (n=2,15 scs) testing set. box plots diplay descriptive satistics across all testig cases, wth the medianvalue represent by the horizntal line witin the , the ower and upper quartiles deineating the borders of box ndthe vertic black linesindicain 1. 5 IQR algorithms ar organized on te -axis basing on their crresponding rans. Th ubble plots show he trae-of betweensementation accuracy (DSC and NSD) ad efficiency on he c, 2D and d, 3D testing et. For eachimage,the best DSC score frm he three algorithms and orreponding contur are presente. a, newbest-performed algoihm (T2-autmlfreibur-pot) and comprison t its redeessor (T-auomlfreibg) and the previous besterforming algorithm(T1-seo across all mdalities. b, Runtime comparison. Thus,we aso aded those * \"S twoatasts to the trained set. We received six submissions for the erformance-boostr tas. One algorithm (T2-automlfreiburg-post) surpassedheprevious bst-performing agorithm (T1seno)and its previousalgorithm (T2-automlfriburg)in terms o he overallrank.",
    "Deployment: 3D Slicer plugin": "To this, we incorprated two best-perormingagorithms pluin for 3D Slicer, open-sorce software platform for medical image nalysis tree-dimenionalvisualizationplugin impemented usig the script module type, allowing for ay intgration ofPython scrits and semess nteractio with the Slicer framework. Its architectueis baed on a generic abstract class t facilitate the easy new models. Importantly, all models can be exeuted on laptops witout elianc on GPU. Despiteopen of these adaced a significant challenge remains in integratig themseamlesslyinto worflow, as this often demand bas coing skills."
}