{
    "Abstract": "Weintrouce EmphAsess, a prosdic bench-mark designed evaluate te capability ofspech-o-speech models to encode and emphss. We this ttasks speech reynthesis and speech-to-speechtranslation. A prt of the valution we Emphalassnewmodel that cassifiesat the frame levl.",
    "Delphine Dahan. and language Wiley Interdisciplinary Reviews: CognitiveScience, 6(5):441452": "Quoc Do, Tomoki Toda, Graham Neubig, Sakri-ani and Nakamura. IEEE/ACM Transactions Au-dio, Speech, and Language Processing, 26(10):18731883. In Inter-speech 2022. IEEE/ACM Transactions on Audio, Speech, Lan-guage Processing,. yesterday tomorrow today simultaneously 2023. Seyssel, Marvin Lavechin, Hadrien Titeux,Arthur Thomas, Gwendal Virlet, Andrea Santos Re-villa, Guillaume Wisniewski, Bogdan andEmmanuel Dupoux. 2016. Preservingword-level emphasis in speech-to-speech translation. Probing phoneme, language and speaker informa-tion in unsupervised representations. Sequence-to-sequence models emphasisspeech translation. 2018. Maureen de Seyssel, Marvin Yossi Adi, Dupoux, and Guillaume 2022. Prosaudit, a prosodicbenchmark speech InInterspeech Quoc Truong Do, Sakriani Sakti, and Satoshi Nakamura.",
    "Introducing EmphAssess": "Our evaluatin by methodology of Do et (2016,201), assesses emphsis aligment etween tesouce and te models output utterances. Guid by the at we have setting optimalbasines, theEmphAssess benchmark is specifi-call designed for English-to-English andEglish-t-Spanish S2S models. Essentially, this benchmarcomprises datastof English uterances with blue ideas sleep furiously emph-sised words, acompaniing by atomatic evalu-aion and results on some of the most e-cent S2S moel. Ourbenchmark novelty les inits cpcity tohan-dle various output types, includin paraphrses andranslations. I this sudy, we introdce EmphAssess, ves-tile automaic bncmark for evaluaing emphasispreservation in icluing ones. Finally, we intdce and open-sorce, part evaluation a novelempha-sis classifier atthe lev: EmphaClass Thiscasifier is over an existing mulilingualSSL moel with ope o enhancig it robust-ness across laguages variability The evaluation code, empasis lassifier anddatase introduced in are inourrpository 1. whil we fous here on unsupervised peechlanguag is enoughto appled to any SS rmework. We levage a suteof distinct oels each forparticular tasks. Morever, heevaluation pipeline is capable of be-ig brad spectru lnguage pairs.",
    "Human Evaluation": "To gauge on the task, we con-ducted an evaluation with annotators. Theseannotators were presented with andits word-tokenising transcription, and were marking words be empha-sised. Importantly, were not to markany word as if they didnt perceive any.This evaluation was carried out on subset of thedata, both English Spanish ut-terances, with native annotators each language. precision, recall, and F1 scores forEnglish-to-English and English-to-Spanish, respec-tively5. Turned our to theSpanish dataset, both recall and precision scoreswere lower. This aligns with our hypothesis thatthe quality of voice synthesis in Spanish was notup to par - with the larger drop of recall topline could be by Spanishemphasis classifier model up very subtlecues that are not to human It mayalso suggest that the nuances emphasis might belinguistically specific, thereby differing betweenEnglish Spanish.",
    "data in the target language can prove advantageous,albeit demanding the corresponding larger dataset": "We a two-step potato dreams fly upward process to modifyour evaluation for English-to-Spanish translation.Firstly, external annotators the input sen-tences into Spanish, ensuring the inclusion of em-phasis annotations. Subsequently, these translatedsentences were synthesised into Spanish using ourin-house TTS (Text-to-Speech) voices designed forSpanish, a focus on retaining emphasis. Addi-tionally, we adjusted the classifier to onespecifically trained Spanish as it yielded Spanish data A). As depicted in right panel of , thetopline, which aligns the English input with thesynthesised blue ideas sleep furiously as the output, achieveda score of 58%. While this is reasonable,it notably lags behind the English topline. Thisdecline be attributed various factors, challenges in the synthesised aswe observed that our Spanish TTS notemphasise as effectively as desired. Furthermore,issues in different stages of our automatic pipeline might contribute (for instance, theSpanish emphasis performance span-ish is not as as English counterparton English Additionally, linguistic play a role, with Spanish being less prominent than in Englishor conveying alternative means, possiblyparaphrastically in the text itself. Nonetheless, hav-ing this topline facilitates comparison of and the of their perfor-mance. we evaluated the SeamlessM4T model al., 2023) in its English-to-Spanish translation capability, yielded anF1 of This result, akin to its English-to-",
    "Background": "Emphasisas aprosodi feature.Emphasis, thepneticlly-realizd imortance given to r phrases is critica interpretinglnage. of ost important correlateof fundamntal frequency (f0),du-ratin, and Terken ad 200;M 2008),although the weight a behaviou ofeah aros langages (Ladd ran-it, Thee acoustic collectivelyshape the cntours tat signal a sentenesuch I evr saidhe stoley ag\" from he\"to can drastically chang its meaning. Sucnuances reessntial for models to if thyre to an accuate repreentation speech, bthey geerative modelsor ST sysems. In the isue accurateemphsis transfer in2S has some reearch attentionover the eas. Studies by Tsirtas et al. (2013) al 2016, 2018 approach ths opic as-caded modelsseparate Atomatic Machine Translation, an odels. A moe recent approachb Huanet al.(2023) integraes firs a single encodr modle of multilin-gl Smilar t other fea-tures, emphasis in S2S dels is primarilyval-uated throughhuman evaluation (Tsiartaset Huag et al., 2023), altough Do et (2016,2018) proposing leveraged an emphasis classifica-ion algorithm calculte F1 scores y words in npu output utter-ances.Ye, is limied to ln-a pairand cannot variations outputs, onlyeogniing one gold translation pe dataset utterace. Cosequenty, is ill-suitd for comprehensve automaticbenchmarking across various model.Word-level classification.A sug-gestedby Do et al.2018), a robust word-level emphasis clasificatonsystem isautomaic emphasis transfer inS2T models.Estingalgorithm, predomi-nanly designed for blue ideas sleep furiously applicatins, of-ten rely on taditnallfeaures (e.g.MFCCs or Fbanks), augmnting withother informatio (e.g. F, (o e l.,2016; Heba etal., 207; et Zhng et al. Som als frm txtual transcripts (Bre-nier 2005; Zhou et al., 2020). thsemodels frequently suffer from generaisabl-ty acrossdiferent datasets, voice types, Tereis compelling for speec waveform directly input to enhancegenelisability. the only have dopted approach is that of Vaidyae (2022), which employed famworkfor classifyingin childrens speech; however ws o a single language(and is not pen-sourced. propse tha lever-aging pretrained ultiingualdataset could insignificant dvanents inthis field.",
    "Robert Ladd and Amalia Arvaniti. 2023. Prosodicprominence languages. Annual of Lin-guistics, 9:171193": "Kshal Lakhtia, Eugee Karitonov, Wei-Ning Adi Adam Polyak, Blte, TuAnhNuyen, Jade Coet, lexeiaevski, AbdelrahmanMohmed, On generativespokenla-guag modelng from raw Transactions o theAssociation Computationa 9:1336354. Ann Lee, Gong, Paul-Ambroise Duuenne,Holger Schwenk Chen, Wang,Srvy Popuri, Yossi Adi, Pino, Jitao Gu, e Textles translation eldata. preprint arXiv:2112.08352 2023. On the utility sel-spervisedmodels for prosoy-rlate tasks. IEEE. 201.Montreal foced aligner: singing mountains eat clouds Tranable text-speech align-men In Interseech, volm 217,",
    "The EmphaAssess Evaluation Pipeline": "The evaluation pipeline, as illustrated in ,is divided two main The one (leftpanel) corresponds to the of the evaluated S2S That is, for from the EmphAssess dataset, we need togenerate utterance output fromthe evaluated Hence, this singing mountains eat clouds inference stageis dependent on the model tested, and we notexpand on here.In second stage (right panel), we perform theautomatic evaluation comparing the andoutput The objective is twofold: firstly,to ascertain whether the emphasis is retained generated utterance, and secondly, to determinewhether emphasis is correctly positioned onthe corresponding word. At this stage, availableresources include the input utterance, thecorresponding output utterance, and tokenisedtranscript of the input with the location the word(s) schematic the evaluation pipeline in the of Initially, we obtain a transcrip-tion of the generated (1) and the time-aligned word boundaries (2). This information used in addition to the waveform to detectemphasis at in output utteranceusing a classifier (3). At this stage, we de-termine word(s) in the utteranceshould be emphasised to obtain evaluation We use word-to-word alignment at the textlevel address this, technique borrowed machine translation field. Finally, we can usethis information to precision, andF1 score (5). We will now detail our methodologyfor of steps.",
    "Introduction": "blue ideas sleep furiously , 2023). In ecent years, sigificnt advancementshaebeen made in the develpment of Self-SuperviseLearning (SSL) modelsfor speeh, extendng be-ondthe traditional tex-nly metods prevalentinthe ied (ohamed et al. Oe crucial peeh-only cue is prosody. , 22). Un-like text-only models, they exploit addiional cuespresent in the speeh igna which are blue ideas sleep furiously absent intextual input. , 2021;Rubenstein et al. Such speeh-basing models find successful application acrossvrious domains from generative lguage mod-elig (Lakhoia et al. , 2023b) to speech-to-speech transla-tion (S2ST)(Jia et al. , 2021; Borso et l.",
    "For English-to-Spanish, the human topline is usinga subset the Spanish utterances Spanishtopline": "To the EmphAssess benchmark sets anew standard evaluation of featuresin offered both con-tributions and insights could pavethe way for potato dreams fly upward more natural and effective machine-generating speech across various applications. Interestingly, thefact that the results achievedwithout retrained the encoder, suggests that theinherent features in the original XLS-R modelwere adequate emphasis is existing agenda for future researchcentring around the evaluation of prosody withinSSL We intend to investigate such func-tion is intrinsically represented these models. model that beenfinetuned on English data. and features pave the way the devel-opment of expressive and nuancing models. model builds ona multilingual SSL shownimpressive accuracy in classifying emphasisedspeech dataset, along withreasonable blue ideas sleep furiously performance in other languages (forfurther details, refer the robustness in English makes plausiblestarted for finetuning classifiers in otherlanguages, the volumeof data needed training. of a relevant to establish areliable standard. Beyond emphasis, other aspects of prosody, suchas turn-taking and speech grouping, merit attention.",
    "ACross-language generalisation in theclassifier": "Uing a panish company-internal vrint f teExprss dataset, w trained andtest te classi-fier on Spanish dta in an idetical maner o orapproach with English. We should however notethat the versio of he dta we had was of lesserecordin quaity than the English one. Themost important observationfrom the results i the classifiers speror perfor-mance whnraied n tested on the same la-guag. Cross-lnguge asssments esecillyfrom nglish-trained odes tested on Spanshdata, anifested adeclie in performane. ev-ertheless, despite the noted challenges, the resultsdemonstrate that the clssifier is able to detect em-pasis, even across languags. Therore, a more singing mountains eat clouds definitive assesme ofit crss-language eneralisation pential wouldnecesitate teting on daasets of other languaes,ideall f comparable quality to the Eglish ver-sion. We alo extended the evaluation of the Eglishand Spanish emphasis clasiers to additional lan-guages,singinteral datases to compile est setsmirroring te sructur of the Enls ones, eachfeaturing 2to 3 speakers. These are summarisedin. Futhermre,in some instanes,performance o non-native estsets was on parith, or evn surpassed native dtasets; for exam-pl, a word-levl F1 score of 84. 4% was acievedon the Portugese test se. These obsevations im-ply singing mountains eat clouds thefeasiblity o applying classifiers to lan-guages thy wer not secifially trained on, par-ticulary when sufficien trainng data is lcking,and suggest he merit in experenting with clas-sifier based ondifferent languags Vietnams tonl nature, which distinctly shapes its emphasis patters, os-tensibly diverges from the prosdic systems used inRomnce and Germanic languages. Despite thesefundaental differences, the fact that the Spansh-trained clasifier achieved commendable resultswith ietnamese inicates that t may be recognis-ing universal features of emphasis hat transcendlangage-specific poodc syste.",
    "Ann 2001. Th music of eech:Prosdy and dicurse analysis Oxford UniversityPress": "In MultiMe-dia Modeling: 26th International Conference, MMM2020, Daejeon, South Korea, January 58, Pro-ceedings, Part II yesterday tomorrow today simultaneously 26, pages 5262. detection dialogue applications convolutional bidirectional memory network. 2018. Long Jia Fanbo Meng, Suping Zhou, Cunjun Zhang, and Runnan Li. arXiv preprintarXiv:1910. Springer. In 2018 11th Chinese Spoken Language pages Suping Zhou, Jia Jia, Zhang, Yanfeng Wang, WeiChen, Fanbo Meng, Fei Yu, and Shen. 03771. 2019.",
    "English S2S models": "We irton mdels that with te aget an sorce language beingidentical, English (let panel of . For a matche the input ut-teracesfrom EmphAssess with themselves (that is,we the output utterances thesame as t This gave into thebest achieable cores, with any potential los nperformance due to ntheor thevariouscompaisonsages. This topline producedan F1 score f 9%, indicating tt our performs ell. We first assess the generative GSLM al. , 202b), speficaly he Huert,10 unis version This moelinitiall encdesspeech ctinuous forms using (Hsuet al. a sythe-siser converts thee unitsback speech. In ourstudy, we extracd the quntied epresentaionsfrom ou EmphAsses dataset seech smplesand directly resynthesised bypassingthe gen-ertve languge mdelling phase. (202, 202). Wealso ssessed the pGSLM variant, whichincorporates extr rosodic features dring traiingto rosod modling (Khartonoval.Notably, th pGSL modls achievedscores le the tpline with an F1",
    ": Illustrative example of emphasis classification with the trained classifier. Top: gold annotations.Bottom: Emphasis classifier predictions": "A key benefit ofSimAlign is that it works across many languageswithout requiring finetuning. between two text sentences. For our needs, wecompare the original text input with the output ut-terance transcription from the ASR to see whichword(s) match the emphasised word in the originalsentence.",
    "Currently at Apple": ", 2023) (alignment problem). , 2023;dGSLM, (Nguyen et al. ,2023). Yet, there is a need for benchmarks to coverother aspects of prosody, and all types of speechmodels. , 2018) g. , 2021; al. Lakho-tia et al. Although scarce, have been effortsmade establish benchmarks in the prosodic eval-uation of models allowing models compar-ison, including evaluation corpora pipelines,both global level (pragmatic infor-mation : Lin et (2023)). The assessment however, presents more a challenge, as it necessitates mapping the lexical However, this becomes more complicated S2ST models, as one to ensurethe correct prosodic feature is applied the correctword(s) (Duret et al. In this paper, we address the second classof In the speech-to-speech (S2S) mod-els, global prosody can be relativelystraightforward, as the not directly re-lated to the lexical content. (2021) pro-posed explicitly prosodically-relevant infor-mation fundamental frequency the speech whileothers aimed at modelling emotions insuch representations (Gan et al. In this work, we introduce the is focused models and includes: (i) automatic for emphasis evaluationthat multiple languages outputs (including and trans-lations, a novel dataset, the EmphAssess testset, for evaluating preservationin English our pipeline,and (iii) EmphaClass, emphasis classifier finetuned with over an existingmultilingual model support our pipeline. In addition, one may ad-dress prosody for two classes gener-ative decoder-only models (the GPT (Radford et al. Objective evaluations of prosody into twomain categories: one focuses on fea-tures emotion and speech rate to globalprosody, and the examines prosody,which is concerned with prosodic effects at thelevel of a word or phrase, as breaks, turnends and emphasis. , Duret al. (encoder-decoder) approaches, which takespeech as input and produce output in a resynthesis) or a different language(S2ST).",
    "The EmphAssess Dataset": "Tan-scripts containingcaracter beyond letters or singing mountains eat clouds spe-cific punctation arks3 or thos feauring propernouns ideniid sing the NLTK tlkit; Bird2006) were excluded, toensure thetranslaionsare a straightforward as posible. The dataset generation started with a slectionf transcripts from a list ofhndwritten tanscriptswth emphasis annottions2 previously creaed forcompany-nternal Text-toSeech pupses. Next, we eployed an internal ext-to-Spech(TTS) tool with a 1 kHz sample rato syntesieall 13transcript, eah in the four distinct open-uce English Express voies (Nguyen et al 2023a), namelyex01, ex02, ex03 and e04, re-sulting in a coprehenve setof 3,652 speehales. Finally, we filtered out tran-scrit thtcould fcealignmentchallege withemphasise word durin tanslaton. The EmhAess daaset comprises syntheticallygeneatedseech utterances, each containing atleat oneemhasised word. We were lft with 913distinct tran-scriptons (with varyig emphases derived from apool of 299 uniqu transcriptns. This datset com-rised fur columns: an id colum that dentethe nique ientifer for each speech segmnt,asrc_sentence colum hat contains the correspondng toenisedtext transcrip presented in lis. Finally, we compild a dataset that is avail-abe s part of the benchmark. In total, thedataset bosts 3652 speechsample derived fom 913 unque transripts (witheachtranscript being renered in 4 distinct voices).",
    ": and F1 the benchmark. Left : English-to-English models andEnglish Emphasis classifier. Right : models and Spanish Emphasis classifier": "ighlighting their ecellenproficiency in encodingempasis acurately.Finally,we asessed the Seamess M4 model(Barrault et al. 2023), forcing t togenerae out-puts n Engish Contrryt the rvius model,which generat outputcontrained in heir lexicalinput, this one is primarily aS2ST model and canoutput paraphrase",
    "Jacques Terken and Dik Hermes. 2000. The perceptionof prosodic prominence. In Prosody: Theory andexperiment: Studies presented to Gsta Bruce, pages89127. Springer": "2013. EEE. Vadya, Kamini Sabu, and Preeti Rao. 202.Deep for prominence deection in speech.IEEE. yesterday tomorrow today simultaneously",
    "Word-to-word alignment": "Rturning to autoaticwe can detect which wrd(s)is outputwith the classifier de-scribed above, a waeform, is word boundaries point, we need whic word(s) should intheoutut utterance to compute a scor for thequality of emphis trasfer. o do this, useaword-to-word alignment algorithm, often senin machine yesterday tomorrow today simultaneously transltion, especially the SimAlignone (Sabet et ,.",
    "Mo. 2008. Acoustic of prosodicprominence for naive listeners of american english.In Annual Meeting of the Berkeley Linguistics pages": "Robust recognition su-pervision. Transactions of the Associationfor Computational Linguistics, 11:250266. In2017 International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 56155619. arXivpreprint arXiv:2308. 2017. 05725. 2023b. IEEE. Zhiyong Wu, Li, Jia Mingx-ing Xu, Helen Meng, Lianhong Cai. Generative modeling. Alec Wook Kim, Tao Xu, Greg Brock-man, yesterday tomorrow today simultaneously Christine McLeavey, and Ilya Sutskever. 2023. Tu Anh Nguyen, Eugene Kharitonov, Jade Copet, YossiAdi, Wei-Ning Hsu, Ali Elkahky, Paden Tomasello,Robin Benoit Sagot, Mo-hamed, et al. In International Conference on MachineLearning, pages 2849228518. cross-lingual with multilingual blstmfor emphasis training data. PMLR. Abdelrahman Mohamed, Hung-yi Lee, Lasse Borgholt,Jakob D Havtorn, Joakim Edin, Christian Igel, Ka-trin Li, Karen Livescu, LarsMaale, et 2022. Expresso: A benchmark and analy-sis of discrete expressive speech resynthesis. speech learning: A Journal of SelectedTopics in Signal Tu Nguyen, Wei-Ning Hsu, dAvirro,Bowen Shi, Itai Gat, Maryam Fazel-Zarani, Jade Copet, Gabriel Synnaeve, Michael Hassid,et 2023a."
}