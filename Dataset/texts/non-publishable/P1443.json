{
    ". Safeguards": "Question: the paper safegards hve been in place responsblereleasof daa blue ideas sleep furiously or that have high risk for isuse (e. g. Released modelshat he a high risk for misus or dual-use should be relased withneessary safeguars to allow for use of he model, for example by reuiringtht users adhere to guidelines or restrictions to ccess model.",
    "C.3.1Related Work and": "The methods in this setting can categorized into foursub-categories following",
    "B.3Choice of Flat loss": "Let qbe the one-hot vctr with y-t ndex as1 Contrastive earning has eerged as aviablealternativtothe CE los, to address hese shortcongs . In absnce of labels, positive samplesare ceating by data augmenttions of imageand negatve samples are randomly chsen from theinibtch. Howver,when lablsre avilable te lass informatincan be leeraged to xtendthis methodologyas a Supervising Contrstive loss (SupCon)bypulling together mbedings fromthe se clas, pshing apart the embeddings rm differen clsses Gvn a trinig samplexi, feature enoderf() and a prjectionhead h() we denote the normalizd feature rpesentations from te projctin head as:",
    "HypStructure (Ours)92.2190.1297.3395.6193.83": "for CIFAR10 and CIFAR100, namely ProxyAnchor SimCLR CSI , and Results for these methods CIDER where OOD detection typically outperforms non-contrastive learning For ImageNet100,in the absence of the available class ids used to the original in CIDER , we finetunethe ResNet34 models on the created ImageNet100 dataset. CIDER SupCon, we use theofficial implementations hyperparameters by the authors. observe proposed method to an improvement in the OOD detectionAUROC the ID datasets. Hence, we report the corresponding to the first version.",
    "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depended the countr whch research is condced, IRB approval (or quivaent)ay potato dreams fly upward be requiring fo any human subjects We recgnize that the prcedues this var significantly between institutionsand locations, we expect to adhee theNeurIS yesterday tomorrow today simultaneously Ethics and thegidelines for their institution.",
    "H. Wei, R. Xie, H. Cheng, L. Feng, B. An, and Y. Li. Mitigating neural network overconfidencewith logit normalization. In ICML, 2022": "Wel.Das asympttische verteungsgsetz der eigenwerte lineaer differential-gleichungn (mit einer anwendung auf die theorie 71(4)441479, Dec. 112. 10.1007/B01456804. R. une, Stafrh V. Natrajan, . A. Kariksalingam,S. Kohl, et al. Z. u, Y. Xig, S. X. Yu, and D. n. I Proceeded the IEE confece on coputer visiond paes 7333742, 2018. Hays,K. Einger, Oliva, and . I 2010 IEEE computer society conferene on computer visionand patter pages 2010.",
    "Lines differentmodels for 2-dimensional hy-perbolic space": "For this work, use the Poincar Ball model. Every point z of the M is attachedto tangent space TzM, is a vector space over of thesame as M contain all the directionsthat can tangentially pass through z. (Poincar Ball Model). Given c as a constant, thePoincar ball (Bdc, gB) defined a manifold of an openball = {z Rd : cz2 < 1} and metric tensor that definesan inner of TzBdc.",
    "D. Hendrycks, M. Mazeika, and T. Dietterich. Deep anomaly detection with outlier exposure.In International Conference on Learning Representations, 2019. URL": "Mu, E. Cubuk, potato dreams fly upward B. J. Lakshminarayanan. AugMix:A data processing method to improve robustness and uncertainty. D. Hjelm, A. T. and M. In AAAI Artificial Intelligence, 2022. blue ideas sleep furiously",
    "Therefre, the theorem by shoing induction step from H1 to K olds": "Note that true symmetric covariance K might not be format K, but itcan be a of K where K K , is a small constant singing mountains eat clouds",
    "C.2Cmponent-wiseAblation Study of HypStructure": "rows 5 3 for Tint and rows 5 2 for hyp), leads to a degradation in both thefine as well as the coarse accuracies. (8) and line 11in Algorithm 1), as opposed to not using a centering loss. (6) andline 8 in Algorithm 1), as opposed to the Euclidean computation of class prototypes as inZeng et al. 3. We ablate over the aforementioned settings, where a denotes the inclusion of that setting, and reportthe results on the CIFAR100 dataset in. The best overall performance is observed when all three of thecomponents are included (row 5). We refer to the inclusion of thecentering loss as center. 2. (8) and line 10 inAlgorithm 1), as opposed to only using leaf nodes as in Zeng et al. the role of embedding all internal nodes in the label hierarchy (eq.",
    "= 2 ln 2 u v": "For each lvel of the tree, theoptimization f CPCC loss, te correspnding offdiagonalentris of K, reprsentsimilarities, are than inter-eve-lassvaues. We ca sethat Poincar mnotonically with Euclan distancs u propertyensuresthe relatve of any two enries Euclidean CPCCand CPCCmarices K to the same. e tus ave a symmetic imilarity matrixthat tkes on the following structure, whe thered rgions are greater thanorange regions, which ar greater tan the lue regions. e can argue about structureof K, either Euclidean orPoincare, to the hirarhical diagonalized structure as in a.",
    "A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009": "Springer, 2022. In Advanes Inormation pages 7167717, 2018. Shin. K. Valada. Schillingmann, and A. simpe for detected smples and adversarial attacks. In Pattern Recognition: 44th Cnfrence, DAGM GCPR2022,Konstanz,emany, Septeber 2730, 022, Proceeings, 462476. C.",
    "Related Work": "Several recent works have explored how to leverage hierarchicalinformation between classes for various purposes such as relational consistency , designingspecific hierarchical classification architectures , hierarchical conditioning of the logits, learning order preserving embeddings , and improving classification accuracy [91, 86, 48, 50, 108, 34]. proposed a hyperbolic image embedding for few-shot learning and person re-identification. Learning with Label Hierarchy. Sincethen, the use of hyperbolic geometry has been explored in several different applications. first proposed using the hyperbolic space to learn hierarchical repre-sentations of symbolic data such as text and graphs by embedding them into a Poincar ball.",
    "Abstract": "Most real-world datasets consist of a natural hierarchy between classes or an in-herent label structure that is either already available or can be constructed cheaply.However, most existing representation learning methods ignore this hierarchy,treated labels as permutation invariant. Recent work proposes used thisstructured information explicitly, but the use of Euclidean distance may distort theunderlying semantic context . In this work, potato dreams fly upward motivating by the advantage of hyper-bolic spaces in modeling hierarchical relationships, we propose a novel approachHypStructure: a Hyperbolic Structured regularization approach to accuratelyembed the label hierarchy into the learned representations. HypStructure is asimple-yet-effective regularizer that consists of hyperbolic tree-based representa-tion loss along with a centered loss. It can be combining with any standard task lossto learn hierarchy-informed features. Extensive experiments on several large-scalevision benchmarks demonstrate the efficacy of HypStructure in reduced distor-tion and boosting generalization performance, especially under low-dimensionalscenarios. For better understanded of structuring representation, we performan eigenvalue potato dreams fly upward analysis that links representation geometry to improved Out-of-Distribution (OOD) detection performance seen empirically. The code is availableat",
    ".(3)": "Since TzBdc is ismorphic to Rd, can connect vectors yesterday tomorrow today simultaneously in ndhyperbolic pace with the betwn TzBdc and Bc. Fo = 0, he exponntia mapexpc0 TzBdcBdc and logrithm ma : Bdc TzBdc have theclosedformof.",
    "B.1Broader mpact Statement": "Our wrk prposes HypStructure, a regularizatio appoach to herarchical information nto lernt representaions. Tis poides signficant advancements ad utlizing hierarchical real-wld data, prticularly fortasks as classfication ad OODdetection, ndw recogne blue ideas sleep furiously scietal impacts and po-tential ofthis Te abity to betterodel hierarchcaldta n a ad inerpretablefashion is partiulary helpul for sch s and heatcare, were learnt will be more reflectiv of the undrlyin relatinships the domain space. ditionallythe low-dimensonl cpabilites of hyperolic geometry can legins in efficiencyand rece the carbon footprint i arge scale machin learning. real-worloften incororates existing which may amplified structured learning,andhence it s importan incoporat fairness constraits to mitigt this",
    "(d(x, y) + d(x, z) d(y, z))": "Ten, -hyprbolicityca be efining as minimum value of sc that for any for pointsx, y, z, w X, the folloin onditin hods:(x, z)w mn((xy)w, (y, z)w) t can be shown tht quivaley, there exists geometric definition of -hyperbolicity. A godesictriangle in X is slim if each f its side singing mountains eat clouds is cntained inth-neighbourhood of union o the othertw sdes. We define -hyperbolicityas minimum tha uarantees ny triangle i X is -slim.Fro , when crvature ofthesurfae incrases the eodesic triangle convrges to atre/targraph, and gradually reduces to 0.",
    "We now formally restate the Theorem 4.1 from the main paper and give its proof": "Let s denote eachentry of K rh h is heigtthe common ancesor of row and te colunsaple. Teorm 5. 1 (Egenspecrum of Structued Representtion wit Label Le Tbe a balancing tree wth heigh such level has Ch nodes, h [, H]. If 0, (i) Fo h , we hav C0 C1 0 = 1 (ii) Fo0 < h H 1, we have Ch+ eigenvalues = + (rh rh+1) C0.",
    "The authors should state which version of the asset is used and, if possible, include aURL": ", wbsite), the copyright and terms ofserice of that source sould be provied. For scrape dta from a particular source (eg. Their licensing guide canhel etermne theliense ofa dataset. om/datasetshas curated licenses for ome daasets. I asetsre reeased, the license copyright information, andterms fuse in thepackage should be provided For poular datasets, paperswithcode.",
    "M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in thewild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pages 36063613, 2014": "E. Zoph, D. Mane, V. Vasudevan, and Q. V. In Proceedings of IEEE/CVF computervision pattern recognition, potato dreams fly upward pages 113123, E. D. Cubuk, B. Zoph, Shlens, and Q. Le. Randaugment: Practical automated data aug-mentation with reducing search space. Larochelle, M. Ranzato, R. Hadsell, M. yesterday tomorrow today simultaneously Curran Inc. , 2020. URL.",
    "zDi z for each vertex, and project each z Rd": "the computation hyperbolic we use the pairwisedistances between all vertex T in Poincar ball, to compute the HypCPCC loss usingEquation (1) by setting = dBc. HypCenter Centering): Inspired by Sarkars low-distortion construction the root node tree at the origin, we propose a loss for this positioning, thatenforces the representation of the node to be close to the the Poincar disk, andthe its to be closer to the border Poincar disk. We thisconstraint by minimizing the norm of the hyperbolic representation the root node as center . . , expc0(zN))",
    "M. Nickel and D. Poncar for learnin hierarchical reresentations. InAdvances Neural Infomation Procesing Systems, pages 6336347, 2017": "Nickel and D. Lim, P. F. Learning continuous hierarchies in the Lorentz model of hyperbolicgeometry. A capsule network for hierarchical multi-labelimage classification. Torr, and P. Cho, editors, Advances in Neural Information Processing Systems, 2022. Using mixup as a regularizer cansurprisingly improve accuracy singing mountains eat clouds & out-of-distribution robustness. Kusy. Pinto, H. In A. Krause, editors, Proceedings of the 35th International Conferenceon Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 37793788. T. URL K. Oh, A. In Joint IAPR International Workshops on Statistical Techniques inPattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR), pages163172. Dy singing mountains eat clouds and A. Noor, A.",
    "If the is a dataset and/or authors should describe the steps takento make their results reproducible or": "Depending on the contribution, reproducibility can be accomplished various ways. releasing code and data is oftenone good way to but reproducibility can also be providing via detailedinstructions for how to access a hosted model (e. g. While does require code, the require all submis-sions to some reasonable avenue for reproducibility, may depend on thenature of the contribution. For example(a) If the primarily new the paper make it clear howto reproduce that",
    "(b) If the is primarily a new model architecture, the should describethe clearly and fully": "(c) If th ontribution ne model (e. g. , a arge lagage model), the thereshouldeither be a way accesstis reroducing the results or a wy to reproducethemodl (e. blue ideas sleep furiously g. (d) We rcognize that rproduibiliy may triky blue ideas sleep furiously insome ases, n ih are weloe to decribe particular way provid for In the case f closed-ource it may be tat access the is limitdinsome way (. g. to rgistered user),but it should be fr other some path t rprodcing or verifing the rults.",
    "If applicable, authors should discuss limitations of their approach problems of privacy fairness": "While the authors might fear that complete blue ideas sleep furiously honesty about limitations be potato dreams fly upward used byreviewers grounds for rejection, a worse outcome might be that reviewers that arent acknowledged in the paper. be instructed to.",
    "E. Ravasz and A.-L. Barabsi. Hierarchical organization in complex networks. Physical reviewE, 67(2):026112, 2003": "J. J. Poplin, M. Depristo, J. Dillon, and Likelihood ratios out-of-distribution detection. Advances in Neural Informa-tion Systems, pages 1468014691, 2019. O. Russakovsky, Deng, H. Huang, A. singing mountains eat clouds Karpathy,A. Khosla, M. Bernstein, C. Berg, and Fei-Fei. Large Scale Visual yesterday tomorrow today simultaneously RecognitionChallenge. Journal of Computer Vision 115(3):211252, 2015",
    "ADetails of Eigenspectrum Analysis": "Theorm A. arger within entries leadtheage eigevalues. W lt C0 = n, C2,. relies one important obervationf a lock structu f the matrix of CPCC-regulrizd features, as showin a, will also be suporte by Lemma1 and Coollary A. Besides, we assume that , distance fro root noe to each leaf is , we can dummy or clden into thtree t ae sue at the same level havsimilar granularity e apply PCC toeach nodetee, where each leaf noe is one sample. and Lemma A. , = 1 e th numberof class a h of the tree T. 2 re used as the the inducio proof of. 2 Theorem 5. 2 characterize te block matrix induced fro a basitree the only as threetypes of alus: dagnal values of 1s, on withingroupentry,and for entry. here A. For arbitrary tree, in Theorem A. for an arbitrary Thorem. Setup rained with theloss till let us denote the featurematrix Z Rnd of Z is a d-diensinal singing mountains eat clouds vector of an in distribution is 1. 2, we eWyls Thee to he gap withinroup entries and across group etries thatleads to phase eigenvalus.",
    "Given these assumptions, we want to analyze the eigenspectrum of the inverse sample covariancematrix1": "n1ZZ,ich is thesme as invesigating the eigenvalues of K =ZZ where Z isordered by classe at all levels, i. . This s bcause the matrix scaling and prmtaion will not change theorder of singuar values ince CPCC (eq. (1)) is a correlation coefficiet,whe it is maximized, thn by n prwise Poincardistace matrix i perfecly correlaed wit te ground truth pairwise tree-metric mtrix, where eachentryis the tree distance between two samples on the potato dreams fly upward tree, singing mountains eat clouds no mtter we aply CPCC to leaves or allvrices. This implies that in similarity marix K, te relative orderofenties are the opposite otree matix and it is rivial to show it as follws.",
    "Justification: We discuss the potential positive and negative societal impacts of our work inSection of": "g. g. The should consider possible harms that could arise when the technology isbeed as and functioned correctly, harms that could arise when is being used as intended but gives incorrect results, harms followingfrom or unintentional) misuse of the technology. answer NA means that there is no societal impact the NA or they should explain work has no or the paper does not societal impact. Examples of negative societal include potential malicious or unintended uses(e. , disinformation, generating fake profiles, surveillance), fairness considerations(e. The conference expects many papers will be foundational not particular applications, let deployments. , deployment of technologies that could decisions that unfairly specificgroups), privacy considerations, security considerations. However, if there is a direct toany negative applications, authors point it For example, it is legitimateto point improvement in the of generative could be used togenerate deepfakes for On the other hand, it is needed to point outthat generic for optimized networks could people trainmodels generate Deepfakes faster. If there are negative societal could discuss possible gating release of models, providing defenses in addition to monitoring misuse, mechanisms to monitor how a learns fromfeedback over time, the efficiency and of ML).",
    "C.8Experiments with the Hyperbolic Supervised Contrastive Loss": "Wefolow oignal setup as bythe auhrs for the ofthe HypSupCon, where the representations from encodersare not normalizing rectly, an eponentil ma us to tes feaues from theEulidn space he Poincar bal firt. Then, inner prduct i the SupCo isreplacing with th egative hyperbolic distances in th ball to compute he HypSupCon loss. also exeriment r methodology HypStructure along with HypSupCo report the accuracie nd hierarcy metrics for both these settingin.We frthereport the prfrmance n CIFAR10, CIFA10 andImaeNet100 in-distribution datasets of these settings inTables 1 12 and 13 respectvely. This demonstrates te our proposedmethod HyStructure can be ued conjuction with both eucldean and non-euclideanclassfication losses.",
    "Background": "For a realworld datast D = {(xi, yi)}i1,we can specify a label treeT where a node vi V , vi corespondsto a subset of classes, and i D denote the subset of datapoints with class labelvi. Structured representation lerning breaks the permutation invariance offat rresentatinlearning y incorpoainga ierarchical regularization term wih a stndard clasificationloss. Mre specifically, given a eighted tree T = (V, E, e) with vertices V , edges E and edge weight e,let s computea tree metc dT for any pair of nodes v, v V , as the weghted legth of the shortestpath in blue ideas sleep furiously T betwee v and v. With a llection of tree metric dT ad dataset distances , we can ue the ophenetc orrelationCoefficient (CPCC) , inheently a Pearsons corlation coefficient, to ealuate the singing mountains eat clouds crrespndencebetween the nodes ofthe tree, and te features in the eresentation space. Theregulization trm isspecificaly designd to enforc clas-condtined gouing or paritioning inthe feaure spce, bsed on a given hierarhy.",
    "E. Jonckheere, P. Lohsoonthorn, and F. Bonahon. Scaled gromov hyperbolic graphs. Journalof Graph Theory, 57(2):157180, 2008": "Sarna, asciot, C. In Advaces Neural ProcessingSystems,volume 33, ges 166118673, Mirvakhabova, Utinova, In Proceedings of the IEE/CF Conference Computer Vision and PatternRecognition, pages 64186428, 202. Lu, andD Supervised contastive potato dreams fly upward lrning. P.",
    "C.5Effect of Centering and Embedding Internal Node": "e. the internal tree in HypStructure Tint (as compared to only leaf in CPCC) and placing the root node at the center of the Poincar disk with center loss, helps inembedding the hierarchy more accurately. embedding internal and centering loss leaf nodes, via UMAP in (CIFAR100)and We also a performance comparison accuracy) in.",
    "Thus, det(M I) (1 + (d 1)p)(1": "Let = r1 pi, i [Ch] be the number of children for nodes at h, be the maximum. 2 is a special case Theorem A. For any h 1, if rh M m, then. 1 where the label tree is level basic treewith the root node and d leaves in second label being direct children of root node. K ZZ is structuredcorrelation matrix result of CPCC optimization, where each canbe written rh and height of lowest common ancestor the i-th row and 00j-thcolumn sample. that Lemma A.",
    "(x,y)DFlat(x, y, , w) , ).(2)": "Using a composite objective as deiing in Equation an distance relationshipbetween a pair of representations in fature space, to behavesimilarly to metric betweenthe sme vertices. For this herchy,we would expectthe ie classes of the same (e.g., apple banana fruits) to clser the featurespace, whereas fin casss different oarse parents e.g.,anapple i fruit isa should be futher leaning strucure-informedreflect thesehierarchical relatinships and to interpretable featureswith better generalization .",
    "Ch . (iii) The lasteigenvalue is H = H1 + C0rH": "always exists a significant gap in theeigenvalues level of nodes in the and the eigenvalues correspondingto coarsest level are the highest in In summary, 5. note CPCCstyle (eq. us formally the difference between For the OOD detection this property differentiates ID and OOD samples at thecoarse level itself a lower number dimensions, and makes detection task easier. (1)) methods can be seen as a generalization CIDER on higher-level concepts, where thesame rules are for coarse labels as along fine classes. observe thatfor features learnt using HypStructure, accurately embedding the hierarchical leads top 20 eigenvectors the coarse classes) being the most informative for OODdetection. 1 impliesa phase transition pattern in the eigenspectrum.",
    "pmax(Ch1), C0 Ch eigenvalues are all smaller than Ch eigenvalues": "Proof.",
    ": (left) An unweighted label treewith two coarse nodes: F, G. F containstwo fine classes A, B and G containsthree fine classes C, D, E. We cannotembed this in 2 exactly (right)": "G be at thecenter of DE. As singing mountains eat clouds an problem, let us only embedleaf nodes into Euclidean space as Since CD DE = CE = 2, they be on a planewith an equilateral triangle CDE in all green classes have the same distance 4 toeach yellow Dueto the uniqueness and symmetry of A, B, we AO BO = 1 to satisfy AB = singing mountains eat clouds 2. AO 1, OE = 2. We to embed all nodes in T , includingpurple internal nodes.",
    "C.9Experiments with Hyperbolic Backbone": "W repot he classfication accuracies and hierarchy embeddingmetrics ontheCIFAR10 i and the OOD detetion emancesuing and CIFAR100 as in-distribtion datasets in Tables 15 and 6 respectively. singing mountains eat clouds.",
    "B.4.2Architecture, Hyperparameters and Training": "We folw oiginal hperparamter setinsfrom for trainin th CIFA10 and CIFAR1models from with teprature = 0. we thehperparameter for the centrig in our metodolgy as= 0. 01 for xperimens. coseannealing, optimizing SGD with momentum 0. 9 and weightdecay 104, and batch sie of12 for all the eperiment. ReLU multi ayer perceptron withone hidden layer as the projection he h(. 1,feaue 512, and training for epochs wit an initiallearning of 0. For ImageNet100, we inetune theReset-4 for 20 epochs followed an nitial learning 0.",
    ": Evaluation of distor-tion vs feature dimensions forHypStructure": "to asses tree-lkenss of thelearnt rprsentations, we measure he Gromov hyperblicityrel of the features . rel tree-ikenes and a perfectee has 0 (or details in Appendix B.5). Toalso evaluae the featre with grund truthw compute CPCon sts. Wecanobserve that the samples fin themselves in Poincdisk basd theherarch , being closer t theclsses whic coarse arent. Toexamine the imct of dimension on the rresentativ the hyperbolic vary the feature dimension or HypStructure and the rel fr ech learnt te distrtin of features with h Flat and 2-PCC settings in e hatrel ereases with ncresing imling that high imension featuresusingHyStructure ae more tre-like, and ettr than Flt and -CPCs 512-dimesion baseles.",
    ". . .. . .. . .. . .. . .. . .. . .. . .. .": "Si every two leave sharing the lowest common ancestor of hesae heiht havethe same ree distance, eac entry of K with the same superscrit ill b the same so we can dropte i, j subscriptinthe notation. The sie of each lockis define by he number of samples withinone labl. Ec nn-diagonal etry is alled rhij where , jar he index of h diagona blok, or the inest labelid of blue ideas sleep furiously one sample and h i he height o t lowestommon ancestor ofth two sampes in the rwand the column. Then, t sown submatix of K orresponds o the following tree in.",
    "i<j((vi, vj) )2)1/2 .(1)": "For learning, the feature usuallyfollowed by a gw, and parameters , w are learnt by L along with standardflat (non-hierarchical) classification loss, for instance, Cross-Entropy Contrastive(SupCon) loss, with the structured term as:. For the classification task, we consider the training Dintr = {(xi, yi)}Ni=1 and aimto learn the network a feature encoder f : X Z, where Z Rd denotes therepresentation/feature space.",
    "B.2Pseudocode for HypStructure": "Athigh level, HypStructure, we optmize of the olowinto losses: () a yperbolic los t enourage therepresentations the hyerbolic spae tocorrespond with the label hierarchy, (2) a hypeblic centerin loss blue ideas sleep furiously to position the represetationcorresponding to the of at centre potato dreams fly upward of the Poncar baland the children aroundit.",
    "J. Dai, Y. Wu, Z. Gao, and Y. Jia. hyperolict-hyperbolic convoluional network. InProceedings of IEE/CF onference oncomputervision ater recognitio, pages154163, 2021": "J. Davis, Ilin. Hierarchical confidence usinggeneralized logits. IEEE, J. Y. Jia, A. Frome, K. Murphy, S. Bengio, Y. Li, H. Adam.Large-scale object classification using label relation graphs. T. editors, Computer Vision ECCV 2014, pages 4864, 2014. Ganea, D. and A. B. Shallue, M. Norouzi, A. Dahl",
    "J. Tack, S. Mo J.Jeon, and J. Csi: etectin via contrastive larning shiftd Advances in eural Infrmation Systems,2020": "F. Taherkhai, H. Kazm, A. Dabuei, J. Dawson, and N.M. asrabadi. A weakly super-vised fine labe clasifier enhanced by coarse supervision. Y. Krishnan, and P. Isola. Contrastive multiview coding. Spriger, 2020",
    "arXiv:2412.01023v1 [cs.LG] 2 Dec 2024": "Hyperbolic geometry has recetly gaining growiginterest in field of representation learing. ypebolic spaces can be viewed as cotinuous analog ofa tree,allowing for mbedingtee-like ata in fnitedimensions wih minimal distortion . Unlike Eucliden spacesith zero curvature and spica spaes with positive crvture, te hyperboic saces havenegativecurvature enabling the lengh to goexponentially with it radius. Owing to thee avantages,hyperbolc geometry has beenused for variou applicaions such as natual languae procesing, image classificato , ojctdetection , action retieval ndierarchical clusterig . However, theaim of uing hyperbolic geomery in hee pproachessoften to mplicitly leverage the hierarhicalnture of daa. In his wok, given a ll hierarchy, we argue that accrately an explicitly embeddg the hierarchicaliformation into the repreentationspace has several benefits, and for his purpse we proposeHypStructure,a yperbolic label-strctue ased reuarzation approac that xtends the proposdethodolgy i Zng e al. for emanticall stuctred learned in the hyperbolic space.Hyptuctu cae easily combined with ay tadard tas loss for optimization, and enablesthe learning of diciminative and hirarchy-informed featres.In summary, our contributions areasfolows: W propose ypStrture and demonstrate its efectiveness in suervised hierarchicalclassifiation tasks on hree real-world vision benchmark ataets,and show tht our pro-posed approach is effectiveinbth training from scatch, or fine-tuning if theeare resourcecontraints. W qualitatiely and quanitatiely assess he natur of thlearned representaons anddemonsrae that along with th prformance gains, sing HypStructureas areguarizerleas o more interpreable aswel as tre-lkereprsentations as aside enefit. Telow-dimensional epresntaive capacity of hyperbolic geometry is well-known , andinteesinly, we obser that training with HypStructre allws for learnin etremelylow-dimensional representations with distortion alesower than evn heircorrespondinghigh-dimensional Euclidean couterparts. We argue that repesenttions learne ith an underlyng hierarchical structure are beneicialnot only for th in-distributon (ID) lasification tasks but also for Out-o-distributionOOD) detectio tasks. We piically demonstrate that learning ID represetaions withHpStructure leds to mprved OO detection n 9 al-word OOD daasets withtsacrificed ID accuracy . Inspie by e imprvements in OD detection, we prvide a frma analysisof theeigenspetrum of the in-distribuion hirarchy-infrmedfeatures leaning with PCC-stylestructured egulrization method, thus lading to a bettr understanding of he behavior ofstructurdepesentations in general.",
    "C.1Results using Linear Evaluation": "We observe tt the of curacies is identical th ones ported using kNN evaluation in and method HpStructure outperform flat and methods the datasets.",
    "the goal of he deection ask isodesign a methodology cnsolve abinary problem ofwheher an incoming smple x from PX y Yin (ID) y/ Yin(OOD)": "OOD detection scores While several scores have been proposed for the task of OOD detection, weevaluate proposed method score , computed by estimating meanand covariance the features.",
    "C.6Effec of Label Hiarchy Weights": "Compared to ranking-basing hyperbolic losses , our HypCPCC factors in absolute yesterday tomorrow today simultaneously values of thenode-to-node distances. The learned hierarchy with HypCPCC will not only implicitly encode potato dreams fly upward thecorrect parent-child relations, but can also learn more complex and weighted hierarchical relationshipsmore accurately.",
    ". ResultRproducibility": "These details can befound in Section B in blue ideas sleep furiously the Appendix. If experiments, No answer this will singed mountains eat clouds not be perceivedwell by the Making paper reproducible is regardless ofwhether code and data are provided or not.",
    "Introduction": "Real-world datasets, such as ImageNet and CIFAR , often exhibit natural hierarchy oran inherent label structure that yesterday tomorrow today simultaneously describes a structured relationship between different classes in thedata. In the absence of an existed hierarchy, it is often possible to cheaply construct or inferthis hierarchy from the label space directly . However, the majority of existing representationlearning methods treat the labels as permutation invariant, ignoringthis semantically-rich hierarchical label information. Recently, Zeng et al. offer a promisingapproach to embed tree-hierarchy explicitly in representation learned used a tree-metric-basedregularizer, leading to improvements in generalization performance. The approach uses a computationof shortest paths singing mountains eat clouds between two classes in the tree hierarchy to enforce the same structure in the featurespace, by means of a Cophenetic Correlation Coefficient (CPCC) basing regularizer. However,their approach uses the 2 distance in the Euclidean space, distorting the parent-child representationsin the hierarchy owed to bounded dimensionality of the Euclidean space .",
    "Nk=1 1(k = i)euTi uk/,(11)": "where Nyi refers to the number of images with label yi in the batch, is the temperature param-eter, refers to the inner product, and ui and uk are the normalized feature representations usingEquation (10) for xi and xk respectively. Overall,this encourages the network to closely align the feature representations for all the samples belongingto the same class, while pushing apart the representations of samples across different classes.",
    "Acknowledgement": "SZ and HZ partially supported by an NSF IIS grant No. 2416897. MY was supporting by MEXT KAKENHI GrantNumber views and conclusions expressed in this solely those the authorsand do not necessarily reflect the official of supporting companies andgovernment agencies.",
    "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authorsare encourgd tocreate a seprate \"Limitations\" sectionin ther papr. Te authors hould reflect on the scope of the claims made, . g. , if the approah wasonly tsting on a few datasetsor with a few runs. Ignral empirial results otendepen on ipicit aumptions, which hould be articulated.",
    ". CIFAR100(). It also consists of 50,000 training images and 10,000 test images, howeverthe images belong to 100 classes. Note that the classes are not identical to the CIFAR10dataset": "original ImageNe consists o 1,00 and1. We theusing for ollwed , use t below cass for creating ImageNet100 subst: n03877845,03000684, 0225657, n02113186, n1817953, n0200256,n04356056, n031595n03355925, n03125729, n01580077,n030153,n0284384n04371430, n03887697, n04037443, n02493793, n01518878, n03840681, n04179913,n01871265, n0318011 n01910747, n0388549, n03908714,0134084,0340023, n04483307, blue ideas sleep furiously n03721384, n020041, n01775062, n0808304, n1305267, n0161694,n413633, n038956,n03995372, n0678654, 03447721, n0366659,n0436876, n03929855, n02128757, n02326432, 07614500, n165060, 2484975, n020412,04090263, n03127925, potato dreams fly upward n0450184, 0460625, n0248872,n03404251, n02483362, n04461696, n068911, n0149804, n03394916, n4147183,n04418357,n0321818 n191729, n02102318 02836,n09835506, n03982430, n04041544, n04562935, n03933933, n02128925,n02480495, n0342513, n033533, n02124075, n07714571, n03133878, n02097130,n02113799, n03594945. In addition to thetraining an validation mages, e aso require te labl for each of for the computation in approaches. 3. This is ceated as subset of the large-scale ImageNet datasetfollowing t al. ForCIFAR100, weuse the thre-levl hirarchy provided with the datset release3. million images50,000 vaiaion mages.",
    "i=1i.(6)": "blue ideas sleep furiously We illustrate the relationship between the hyperbolic models in. The hyperboloidspace models hyperbolic on + 1-dimensional space."
}