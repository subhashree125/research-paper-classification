{
    "Conclusion": "In this work,e a novel, technique reduces theinference time the Maba block in VMamba yaplyng mean operation treduce dimnsionalty of inputcanne in the associatescan Our resultsdemonstrate tat VMeanba enhances infernce speedwhile maiaining acuray inVMamba. Ths work contrbutes the field by a practicalmethod fo Vambasfficiency and sugests future exploratin of the dimnsionality of an the kernelfusion of he dicetization and selctiv operaions t utiliztionenvision extending VMeanba to other computer iion evauate its applicabilityand.",
    "DExperiments Setup": "Te daasets we se for ou VMeanba experimets re thImageNe-1kdatset forimge classificion and the ADE20k datse for semantic segmentation. We use the Vaa pre-trined bacboe models for both tasks.The backbonemodels is first trining on ImageNet-1k rining datset. It is then used as the pre-training backonemodls for downstream task.The segmetation task use the UperNet blue ideas sleep furiously on top of the VMambapre-trained bakbone models, and trained n the ADE20k trainng dataset. The VMamba backbonemodels have eediffrent ersions: tiny, small, and base. Thee ae two mainly differences betweenthese versions:thenumberof layers and the dimension of te L and D in the SS2D block. The original CUDA kerne for the Mamb block inludes bth thediscretzation and sc operations,dividing the GPU multiprocessor into a 2D rid blocks basing onthe batch size and inner dmenson. In this confguration, multipl treads within the block handlethe scanopeaton. Fuure work includs integratng the discretization and scanoeratins into single kernel for further optimizaton.Addtonl Information. All experiment were conducing on asine NVIDIA RTX6000 GPU with 4B of memoy profilin as erfmed using NVTAPI, Nvida Nsight Systems, and Nviia Nsiht Copute tools.",
    ": Accuracy comparison ofVMeanba with pruning on Linearand Conv2D layers using the basebackbone": "The results are summarized in. We emonsratedhat our VMeanba method canbe sealessy integrted wit other optiization techniques blue ideas sleep furiously to en-hance model eficiency. Orindigs indicate tht the VMenba methodis orthoonal topruning, as it enhances efficieny whilemaintaining compa-rable acurac,demonstratingthat he two technique can ecombined withou interference. Pruningwas appliedto weight of liner layer rconvolution2Dlayerusin thel1 nrm, wih singing mountains eat clouds a onsstet puning ati of 40%.",
    "VMeanba": "The coputatioal is prvide in Selectio. We developed a to eplace blocks with blocks. Buidingfrom indroduce a nw inference effiiency optimizationmethod called VManba, computes for ach Mamba block mean design a pieline towhich layers the modelwill undergo thisotimizati. While the mea may ead potato dreams fly upward to a lossof informaton, it sigificantly reduces the dimensionality of the inputs D toourexperiments demonstrating that model performance ismaintained. Theimpact score is defind by equation (3):. thechicesof layers as ahyperparameter, usng the Specificaly, the impc corSlayer each layer, ad select the layers e K smallest scoresto apply the VMeanba otimization. ylaer = But, C)))(2)n process, is as themean operator aplid along the inner channel dimensionaxis, an T 1 is defned as the bracast operator. The Ibasis is derived by a function T that maps th originalinputs( A, But, C) to reduced dimension inputs. After prcessing by the Mamba block, theoutpt recovered using an inverse transformfunctionT This entire process be asequation (2). VMeanba bock.",
    "y = x K(8)": "Mamba is the discrete-time linear dynamical system with atimescale parameter that transforms the continuous variables A, B to discrete variables A, B. Selective State Space Model (Mamba). Inaddition to discretization, Mamba also relax the time-invariant constraint of the system matrices byintroducing selection mechanism, which simply makes several parameters , B, C to be time-varyingby functions s of the input u.",
    "(9)": "Then the selective scan processes parallel. VMamba further multiple SS2D blocks a then stack layers form whole. The cross-scan unfold input along four directions, forming 4 sets of 1D sequences. VMambaThe original Mamba block is designed for input and output, isnot suitable for vision tasks.",
    "Introduction": "Neural Networks have become foundational tasks such as imageclassification and object detection. However, CNNs struggle blue ideas sleep furiously potato dreams fly upward capture long-range dependencies.",
    "BComputation Complexity": "Complexity SSMThe computational complexity of associated scan operation in in floating-point (FLOPs), is derived processing a oflength L, which requires 2L operations. Furthermore, the input to scan operation incurs anadditional cost 3 FLOPs, leading to a of 3 2BLD, where is batch size, L is thesequence length, and D the dimension. Consequently, the overallFLOPs for the SSM is 3BLD. The FLOPs for the block, to3 2BLD + 3BLD. The total reduction in FLOPs is by the equation (10):.",
    "In this section, we introduce some preliminaries of the State Space Model, SSM , and two recentlyproposed methods using SSM, mainly selective state space model (Mamba) and VMamba": "The model is specified as aset of equatios thatrelate he state of the system totheobserations a ch time step. The most eneralformf the SSM is called continuous-time lineardynamical sytem, which is defining a eqation (4).",
    "arXv:2412.16602v1 [cs.CV] 21 Dec 2024": "The high-level overview presented. The key idea is to re-duce the input channel dimensions inthe associate scan operation applying a analysis of the weight and activation distributions in the trained VMamba model,we identified a smooth pattern with small variances that allows dimensional reduction. 3% of the kernel timein a VMamba yesterday tomorrow today simultaneously block. Theproposed components are highlighting in red, while original selective scan componentsare shown in and green, with the green block the main area of optimization. While optimizing operation critical for SSM few works address this problemand optimizing efficiency SSMs remainsunexplored. : Overview of the VMeanba block. various tasks. The selective scan operation is of the majorcontributors in the To this end, we analyze the latency of VMamba selectivescan as of key bottlenecksin that selectivescan accounts for 14. 12x speedup with than 3% accuracy loss. We rank kernels by their and highlightsthe top-5 time-consuming kernels on chart. Basedon this observation, we the VMeanba block exploit this pattern, resulting a associate scan operation without accuracy. To best of ourknowledge, this the first work optimizing of the operation VMamba. Experimental results demonstratethat VMeanba achieves up to a 1. : The GPU kernel time of each operationin VMamba block. In this paper, propose VMeanba, a novelactivation compression to op-timize selective scan operation in VMambablocks. reduced computational complexity, SSMs fail to fully utilize matrix multiplication GPUs, creating a bottleneck vision-based SSM models. is measuring usingfeature maps with an input resolution 224224. For example, achievesd 82. 6% accuracy onImageNet-1k surpassing Swin Transformer by with FLOPs. VMeanba reducing channel dimension inputs tothe operation applying a T, thereby computation.",
    "G / 823.5M6.4M / 1.3M10247841.6G / 411.9M1.6M / 14.5K2048196822.1M / 207.5M412.4K / 5.8K409649411.1M / 107.5M108.5K / 0": "Kernel Analysis We GPU speedup memory usage when applying VMeanbaacross varying scan lengths and inner dimensions, as shown in 2 3. The VMeanba method achievesup to 293x longer aligning with the O(DL) in B. Optimizedkernel times, consistently around 02 are excluded from."
}