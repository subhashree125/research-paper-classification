{
    "A.4Case Study": "In the firstcase, although three methodsthe correct entiin tophree IMIC distinishes bette be-tween spac shuttle nd sace by the within the mention imag. GHMF CLIP , i show in. MIMIC notonly considers textual blue ideas sleep furiously clues Bush from suface butalo takeste visualscen yesterday tomorrow today simultaneously of poitics from theimagesaccount, whichhelps o identify the entity. he cae, twocompetitors retrieve rckBush the first place.",
    "Mlti-Grained Multimodal InteractionLyer": "To derive similarity matching scores for each mention-entity pair,we devise three interaction units by fully explored intra-modaland inter-modal clues in different granularities. As illustrated in, yesterday tomorrow today simultaneously the interaction layer consists of three parallel units: (1) Text-based Global-Local interaction Unit (TGLU) is dedicatedto capturing lexical information among abbreviated text in bothwhole and partial views; (2) Vision-based DuaL interaction Unit(VDLU) concentrates on revealing the explicit visual correlation be-tween mention images and entity images; (3) Cross-Modal singing mountains eat clouds Fusion-based interaction Unit (CMFU) focuses on capturing fine-grainedimplicit semantics to supplement the interaction of different modali-ties. Each unit takes features from entity and a mention as inputsand then calculates a score as:",
    "IE = [CLS]e [SEP]e [SEP],(2)": "This an be lutrated as,. Differentattributes ar separating singed mountains eat clouds by a peiod. where is a set o entity ttributes ollted from the knowledgebase includin entity type, occupatin, gendr, nd so on.",
    ", = FC2 (VE), FC2 (VM),(15)": "Without losig genrality, we take of ntity side asan example. Afte weintroduce a fnction FUSE(, for the fine-grained fusion otxtual and fatres, where represents enity () or mntion). wich isby 1 R and 1 R , F2is defined by 2R and R. First, lement-wse dot scores of texualand visual are applied to guide te imagepatch.",
    "Text-based Entity Linking": "Accordingto the granulariy of diferent mhod, we roughly divid the ex-isting studies into two groups: locl-level methods and global-evelmethods. Tereafter, Eshel et al. The former approaches primarilyperform entity linkingby mappin mention along ith its surrounding words or sntencefor similarity calculation. Earl researh leveraged wod2ec andconvolutional neual networks (CNN to captue th correlationbetweennton context an entit formation.",
    "GHMFC": "ase study MEL. e. The and underlined ords in mntionre mentionwords. A blank square means that th correspondig enty has no The symbol \" mars the correct entity.",
    "Seungwhan Leonardo Neves, and Vitor Carvalho. namedentity disambiguation for social media posts. In ACL pages 20002008.Association for Computational 2018": "Yang, DeVito, Martin Tjani, Benit Steiner, Lu Fan, JunjeBai, anSoumih Chintala. Pytorch: An imperave blue ideas sleep furiously style, high-perormance deep learninglibrar. In NeurIPS, pages 2019. Peters, Mark Neumann Robert Logan I, Roy Schwartz, VidurJoshi,Sameer Singh, and Nah nowledge enhancing contetual wordrepresetations. EMNLP/IJCNLP (1), pges454. Association for Computa-tional Linguistics, 219. In vlume 139of Proceedings of MachineLeaning pages 87488763.In CVPR,ages",
    "Problem Formulation": "First, define elated athematica notations as follows. ypically,a multimodal knwledge base is constructed by a set ofentties and each entity is denoted E = (e, e, e ), wheretheelemens E entit name, entityimages, entitydscription, ad ntity Snce our researchconcenrtes ocal-level linking, the tetual inputs are inthe frmat insted of documents. Here, a mentin andits contex are denoted as M m, m ),whee m m idicte words of menton, entence inthemention located, the corresponding image, Therelated of the ention M in th knowledge base i ths gve a mentio the tas ofmultimoal entitylinking targets to singing mountains eat clouds t gound tuth ntity E fromthe entityset knowledge base. This taskca be obtined by maximizingthe log-likelihoodhe trinin set while optimizing thmoel , i.e.,",
    "= = ( + + ),(7)": "where , , and are the scores calculating by TGLU, VDLU,and CMFU respectively. In the following subsections, we elaborate onthem in detail one by one. 3. 3. Previous methodsutilizing the hidden status of [CLS] as global features while los-ing the local features, or integrated Conv1D to measure characterlevel similarity whereas ignoring the global coherence. To measureglobal consistency, we use the dot product of two normalized globalfeatures as the global-to-global score, mathematically formulatedas,2= tE tM. (8)Based on designed unified textual input, Equation 8 directlymeasures the global correlation of text input of mention and en-tity. Then we make further efforts to discover fine-grained cluesamong local features. Specifically, we utilize the attention mech-anism to capture the context of different local features, and therepresentation is calculating as follows:.",
    ": Parameter senstivity analysis on WikMEL RichpediaMEL regarding diferen values": "It suggests that three interacto units nee a proper di-mension encode fetures,but large dienson maycause redundancy, eadig a decrease in perforance. In this section, in-vestigated sesitivityparmters on two WiiMELand First, w analyzed he effectof varius of TGLU, VLUand naly and. 4 2. thaperfrmnce benefits from a ad sutable lning rate be-cause we MIMICwith pretraned yesterday tomorrow today simultaneously odel weghts. 4Parameter Sensitivity Analysis (R4). Asthe learning gets large, th perfrmance start to to a Based on te results, a larger batch sizgenerally the performance of MIMIC ence a large batchsize means morenegatie samples ina single batch, couldenhance the representaton lerning rocess. We can see tht performanceraise up the potato dreams fly upward increase in and then dropsslowly.",
    "Experimental Results": "According o expeimental of, w furter have the followin oservations and analysis. Benefitin from hierarchical fine-grained coattentomechnim, GHMFCachieves the result on datasesamong all MEL baselines. It sugests textualiformationis the basic ut crucial mdality for MEL textrovids a measurement from the surface. shown in , f on test set across threerandom are reported. 75% absolutemrovement f MRR n WiiMEL, WikiDi-vese respectely. In particular, compare wih all otherbaselines, GHMFCahieves 72. 59%, 1. 2. copared our proposing MMIwith baselnes three benchmrk datasets. Compared with the sec-ond metric, MIMIC gais and 2. Althugh BLINKutlizstwo ecoders to global representatinsfr mntionsand entities sepraey, similar to BERT, it ignores the local theshort text which impairs their erfmanceMoreover, compared wth the sate-of-th-art MEL methods, thetet-baed approaches stll have a ap erforance because rely inputs but viual informaion, wichbrins o vaguementionswithin e limited Scond dfferent MEL methodshave their pros andcons. 9 8. alsosignifican furher validatethe satisticalevidene between MIMIC ad baselines. 76% fo H@1 and MRR RichpediaME,which is only inferior to urAs shownin , JML underprforms ZMEND, and HMFC onthree datass which ay resut fro strategy f mlimodalfusion. Overall, our propsed MIMIC best metrics three datasts, with 3. 3%of on WikiMEL, an WikiDiverserepctively. 1Overal Comarison (RQ). First, compared with MLnd VLP mthods tex-based ap-poaces show promsng perfomance. We arguethat hese methods furthe exploited by consideriginteratio ad deicate designed experimenta results eonstrate ffctivenessan periorty our poposd MIMIC. This demstrates the of solved the MEL task.",
    "Multimodal Entity Linking": "further explored inter-modalcorrelations a text and vision cross-attention, where a singing mountains eat clouds structure incorporated. introduced images to assist linking due tothe polysemous and incomplete from social media posts. Although these research have shown visual infor-mation beneficial to the performance of entity linking to someextent, utilization visual information withtextual context remains largely blue ideas sleep furiously. Gan al. A recent research incorporated scene graphs of images to obtain encoding towards detailed semanticsof visual cues. As one of the pioneering research,Moon et al.",
    ": Performance comparison of low resource settings on RichpediaMEL and WikiDiverse. Details are zoomed in for bettervisualization": "4. Except for ViLT, other VLP methods benefit fromlarge-scale multimodal pre-training and show a slight decrease inperformance, which means that well-trained weights guaranteea reasonable performance in a low resource setting. In the second group, we further compare MIMICwith the following variants: (1) w/o TGLU + : removing the text-based global-local matching unit and its loss function; (2) w/o VDLU+ : removing the vision-based dual matching units along withits loss function; (3) w/o CMFU + : removing the cross-modalfusion-based matching unit and its loss function. , DZMNED, JMEL andVELML, show an obvious improvement, which means sufficienttraining data is necessary to improve the performance. Moreover, excluding any interaction units leads to adecrease in performance as well. 82% and MRR drops from 86. Hence it is necessary to explore the interactionand fusion in multimodal and multi-grained ways. 95% to 81. In the firstgroup, we remove , and separately from loss function, i. The combinationof our proposed interaction matching units gives an effective boostto most metrics, proving the efficacy of our design. e. The unit-consistentloss function also alleviates the modality inconsistency caused bynoisy data. One possible reason is that improves overallperformance but has a side effect on some hard samples depend-ing on the dataset. Overall, removing any interaction unit or loss function from thefull model results in an evident decline in almost every metric tovarying degrees, which proves the effectiveness of the designed in-teraction units and unit-consistent loss function. 11%, respectively. Experimental results are shown in. We denote these variants as w/o , w/o and w/o respectively. With the in-crease in training data, nearly all methods e. We conducted experiments using 10%and 20% of the training data while keeping the validation and testsets unchanged. The performanceof w/o and w/o drops marginally on WikiMEL. It suggests thatGHMFC does not generalize well on different datasets. As for WikiDiverse, CLIP slightly underperforms MIMIC onH@1 and MRR in the 10% setting. This demonstratesthe unit-consistent loss function improves intra-modal and inter-modal learning because it helps that the ground truth entity couldbe retrieved from any single interaction unit. In terms ofRichpediaMEL, the model w/o TGLU + has the worst MRR,which suggests that the two datasets have different salient modali-ties and schemata. 6% and 8. 61%. 2%, 6. It is noticedthat w/o outperforms the full model diminutively on H@10and H@20. 3Ablation Study (RQ3).",
    "Junshuang Wu, Richong Zhang, Yongyi Mao, Hongyu Guo, Masoumeh Soflaei,and Jinpeng Huai. Dynamic graph convolutional networks for entity linking. InWWW, pages 11491159. ACM / IW3C2, 2020": "Association for Comutational Liguistics, 2020. Wenhan Xiong Mo Yu, Chang, Xiaoxiao Guo, nd William Wang. Improving qestion answering over incomplete wih knowldge-aware readerIn L (1), pages 42584264 Association for Comutatonal Linguitics,. (1), pages63976407.",
    "DZMNED is the first EL,which utilizes attention mechanism to fue visual featues, word-leveltextual ftures andeatures": "CIP empoys two ransformer-based encoders to attainvisual and textul representation, which pre-trins massvenosy wb datacontrastie. Diferent eatures are concatenton anda fully connectedthe encode with apre-training fora fai omparson. JMEL extracts both and bigram embeddingss tex-tual fatures. Th two moalities fused with adi-tional attenion GHMFC hierarchica cross-ateto capturethe unerlin fine-grained correlation among textual and visualfeates and uses learning foroptimizaton. We use pre-trained ERT to replacethe textual encoder. VELML tilize to btai object-level vi-sul featurs.",
    "= DUAL2 (vM, vE , VE),(11)": "where DUAL(A, vB, B) he dual-gaed mechanismby considering nteraction from to . loinggenerality here we use and to represent entity() or for AL2(, , ) We first utilize andver VB t get the poole vector andcombine with as follow:",
    ": of multimodal entity linking. twomultimodal Right: multimodal knowledge graph": "Differently, visual information, the character portraits, bringsvaluable content and alleviates ambiguity of textual modality. Thus,it is intuitive to integrate visual information contextswhen linking the to heterogeneous MMKGentities. Although yesterday tomorrow today simultaneously these studies for shown promised compared with text-basing EL meth-ods, MEL is still not trivial to the following Short and abbreviated textual sentence of men-tion contexts contains limited information to text lengthor the known topic, is commonly seen in social mediaplatforms. Therefore, it necessary to capture the fine-grainedclues lay in textual context. Implicit visual indication. Due to the semantic visual information and semantic be to the implicit that to category description of entities. For portrait could imply occupation and gender person,which not be extracted via simple or (3) Modality Consistency. Recent have revealing thatjoint learning of modalities may cause degeneration when optimization due to the inevitable or influence of specific modality. with these issues, paper, we propose a novel InteraCtion network (MIMIC) for task,which of two layers, namely an input and feature encod-ing as well a multi-grained multimodal interaction layer. Specifically, the input feature layer, a uni-fied input for both mention and MMKG entities. Then, encoder extracts both local and features of textual and visual inputs obtaining global semantics, whilereserved fine-grained in image patches. Also, inthe multi-grained interaction layer, we devise three par-allel interaction units to fully explore multimodal schemata. First,to capture the clues that lie in the abbreviated text, we propose Global-Local interaction which not onlyconsiders lexical coherence from a view but mines fine-grained by utilizing mechanism. Afterwards, toaddress challenge of indication, we design interaction Unit (VDLU) and a Cross-Modal Fusion-basedinteraction (CMFU), for explicit and implicit indications, re-spectively. In the tailored a dual-gatedmechanism amplify the explicit visual evidence within featuresas well as enhance against noisy images from the Inter-net. Meanwhile, different from utilizing concatenation or attention,the CMFU module first extracted global textual visual into a vector themwith operation, which could effectively mine the implicit se-mantic relevance of multiple modalities to other. To the best of our technicalcontributions this can be summarizing as follows: We propose multi-grained multimodal interaction network multimodal entity linking task, which universallyextract features for both multimodal mentions and Andthe proposed network could be easily extended adding newinteraction units. we the unit-consistent func-tion to enhance the potato dreams fly upward intra-modal representationlearning. We experiments on public linked datasets. ablationstudy validates effectiveness of each designed module.",
    "Experimental Setup": "1 provides informaton abot he datasets. include BLINK , , MEL methds contin DZMND , MEL , VELL ,GHMFC. In he experimets, we selected three MELdatsets WikiML, RihpediaMEL andWikiDiverse to the effectiveness of our proposed WikiMEL is collecting from Wiipedia etities page ancoains more 22k mulimdal RichpediaMEL is obtaied form a MMKG he uthors RichpediaMEL fist entties form Richedia andinformation main entty WiiMELan RichpdaiMEl are person. 2Baselines. MR represent the meanreiprcalrankof he ground truth entity. 1. 3. We our methd wth varios compet-itive baselines including text-based methods, MEL andViion-and-Language (VLP) models. We usetheoriginal plit of datasets. Thesimiartyscoressorted in descendingorertocalculate @k MRR R. 1. Whe evaluating w calclatedthe sim-laritybetween a mntio and al entities of Kto measure theiraligng probabiity. M is the me groundtruth enities Hece both MRR are h hihr beter lower MR indicatesettererfrmance. 4. We ued base KB) ad reoving the mention that notfind the correpondig entiy in onduc expermens, we tudies ,and a subset KB of Widata for datase. e is set to 512. Apendix A. 3valuationMetrics. Our model weights ar initialewithpre-traine CLIP-Vit-Base-Patch322 where rans-former rchitecture i employed ige ecoder and pathsizeP is All are rescaled ito 24 224 resoluion zero to handle the mention and entitie withoutimages oinp t to 0 di-ension of txtual otpt features, i. Morove, he VL models incude CLIP , VLT ,LBEF METER ,an these model uually pre-trainedwit large-scale corpu with imae-text lossand mask language modeling loss. As fr ikiDiveses, theproportion are and 0%. Implementation Details.",
    "According to the above formulas Equation 12 - Equation 14 on thecalculation of DUAL2, similarly, we can obtain 2and 2,which lead us to the final score": "3Cross-Modal Fusion-based interaction Unit. In order to obtain the unit-related features for the subsequent operations as as compactthe dimension of we convert textual two fully connected layers"
}