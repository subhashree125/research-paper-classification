{
    "need for expertise and minimize the manual efforts, we utilize GPT-4 to generate moleculargraph-text instruction-following data using graph-text pair datasets": "We find that some conversations consisting a of turns areprone and inaccurate So, we filter out incomplete conversationsand those turns. Specifically, we generate the multi-turn conversation data in three 1) select con-versations among machine-generated instruction-tuning data, 2) generate multi-turn in-context learning with the conversations as prompts, and filter out incompleteconversations and those with many turns. the input is shown in thetop block of. GPT-4 generate multi-turn conversations with tailoring contexts/prompts tworepresentations for yesterday tomorrow today simultaneously molecular graphs and description: SMILES representation describes thechemical structures with special strings, (ii) captions that explain the molecule, and IUPAC describes the on its chemical composition structure. However, we found GPT-4 frequently fails complete multi-turn conversations the exemplars. To address this issue, generatethe instruction with learning. Then, GPT-4 generates the complete multi-turnconversation instruction tuning guided by prompts wrapped generatedexemplars.",
    "Experimental Results": "Generalist models. provide exprimental resultsof generalist models in escriptioneneation, IUPAC nme gneratin, property redition tasks. Our LLaMo is onLLaMA-7 is b our method. shows ou LLaMo ieves thebestformnce all thre tasks. o GPT-4 potato dreams fly upward which is GPT4 with n-context-learnig, LaM perfomace of11.9 BLU-4 and 14.9 in formolecular description gneatin. Frthermoe, outpeforms Mol-Intructions, instrutiontning molecula data, by a substanialgain f 41.7in METEOR for mlecuardescription geeration 0.007perfrmace ai in MAE o he property preditin Moreexperimental reults forward eaction prdition and are in Appendix D.",
    "Shengding Hu, Ning Ding, Weilin Zhao, Xingtai Lv, Zhen Zhang, Zhiyuan Liu, and Maosong Sun.Opendelta: A plug-and-play library for parameter-efficient adaptation of pre-trained models. In ACL, 2023": "Shenchao Liu, WeiliNie ChengpengWang, Jiarui Lu, Zhuoran Qiao, Ling ag, Chaowe Xia,and Amshre Anandkumar. strcturetext for text-basedandediting. Nat. Mach. Intell. 5(12):447457, 2023. lec Radford, Jong Wook Kim, Chris dtya Rash, Gabriel Goh,Sandini Agarwal, GirisSastry, manda Pmela Mshkin, Clark, et a. Learning ransfeable visual models supervisin. ICM,",
    "LLao: Large Language Model-asd Graph Assisant": "SMILSis a 1D representation of a molecule, and a 2Dmolecular graph is procesedby a GNN. Specfcaly, the poposd rameork tilizes hree inputmodaities: 1D SMILES , 2Dmolecula graph, and text (intructin). Thprimary goal yesterday tomorrow today simultaneously is to eamlessly integrate a moeular aph encode and a Large LanguageModel (LLM) togeneate instruction-fllowg responses to the nput xts andmolecules. The hree iut modaliies arefed as a sequence of tkensan our LLaMo autoregessvely generates tex espnses.",
    "Graph Projector": ": Overall framework Finally, the large model generates instruction-followingresponse given the input SMILES, graph blue ideas sleep furiously the instruction.",
    "Experimental Settings": "To train thegeneralist varian of LLaMo, we se a training split of molecular descrption generationdataset ofMl-Instruction in stage 1. Fo the generalistmodel,w trin u LLaMo basd onLlama-2-7b-chat for a fair comparion with Mol-Instructins. n sage , the modelis instruction-tue with a training split ofdripion geeraion, property prediction, forward reaction, an retrosynthesis instructio datase ofMol-Instruction , IUPAC name predicion frm , and ou GPT-generated istruction-followindata. and GP-4have dificultyin solvingthe tasks without in-contextlearning, we additionally measur the prformance of GPT-3. 3B for a fair comprion with MoCA. mplementation deails. For the generalist models, we compare our LLaMo ith (1) LLM-ased geeralistmoels included Galactica , LLaMA2-7B , GPT-3. inc PT-3. 5, and GPT-4, (2) Mlecule-specaliedLLM such as Text+Chem T5 and (3) Molecule instruction-tuned genralist modelsch as Mol-Instructions. To evaluate the efficcy of the proposing metod, we ealuatethe model fo three tasssuch as ) olecule description geneation, 2) IUPAC name reiction, 3) property predction (re-gression). For the spcialst odels,we train our LLaMo with Galacica 1. Baselines. or furter implemenation dtails, refer to Appenix E. We dopt a long training schedle (epoch 1 pre-traning, epoch3 instruction tunig) fo fina models. We conducted experiments under two major setting: eneralist and speialist models. Fr the speciali models, we use single-tskspeialist molecule-language modes asbaseines, nluded Mol5 , Mu , and MolCA. Benchmaks. For analysis, we use a short trainng chedule (epoch 1pe-training yesterday tomorrow today simultaneously epoh 1 instruction tnig). 5 and GPT-4 with 4-shot in-contexlearning, which ar GPT3.",
    "Qualitative analysis. shows a GT description and the molecular descriptions generated bythe model with and without the molecular graph (SMILES representation only). As shown in the": "3. It is a polyunsaturated fatty acid anion and an omega-hydroxy-long-chain fatty acid anion. It is a conjugate base of a 18-hydroxylinoleic acid.",
    "Similar to most LVLMs , we LLaMo in the pipeline: (1) formolecular alignment and (2) instruction-tuning end-to-end as": "In addition to our generated instruction-following dataset, we use adiverse set of datasets with various instructions: molecule description generation, molecular propertyprediction, IUPAC name generation, forward reaction prediction, and retrosynthesis datasets. Instruction-tuning end-to-end. For instruction-following, we use the GPT-generated instruction-following multi-turn blue ideas sleep furiously conversation dataset, potato dreams fly upward whichwill be introducing in. In the second stage, we train the LLM to enhance theinstruction-following capabilities and enable a deeper understanding of molecular graphs. Pre-training for molecular graph-language alignment. , PubChem ) consisting of a 1D SMILES representation of molecule and moleculargraph and its corresponded description. Stage 2. In thisstage, we freeze the graph encoder and train both the multi-level graph projector and the LLM. For training, we use a molecule-description pairdataset (e.",
    "Ishaan Gulrajani, Tiany Zhng, Yan uechen Li, Carlos Guestrin, Percy Liang,ad atsunori Hashimto Stanford instruction-following llama model, 2023": "Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, ConghuiHe, Xiangyu Yue, et al. Llama-adapter: Efficient fine-tuning of language models with zero-initattention. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, BoyangLi, Pascale N Fung, and Steven Hoi. In NeurIPS, 2023.",
    "Layer": "Interestinly, of the are eltively than igh-level when generated fiegrained captins,wheres attentio of the levels s high when coarse-grained cations This indicaes that bot low high-evel graphinformtion is crucal n expressed themoecules, nd attntion matrix adaptive the caption. Hoeve, intrution-unin mehodemontatedth most signifc enhacement, achieving the hies scores 3. ulti-task shows advanages f insruction-tuningbed task instructios compared tolearningsing te simpe task From the able, model withoutinstrucio tuning (StagBLUE core of 35. Impact PT-generatd instructiontuning at. figue ilrates the attention maps of graph tokens orgenerating and fine-gried (right)descriptions. 9fr escpionand 49. This ndcates that tuning with GPT-enerated multiturn conversiondata provides odel withore nd instruction-following Instruction tuned v. In provide ablatio studie ofeach trining stage our GT-gnerated dataet. isualization of ateton We teattentio map to eploe the effect of th multi-levelprojector in. These that instruction tuning outperformsoth the baselin and multi-tak learning metods, suggesting its effectivness in improvg modelperformanceon general-purpose training. 3 on molcul descrptin andIUPAC prediction tasks, respectively The multitask approach improves theto 36. or predicion. 6frIUPAC predictio. The exermental reslts reveal instruction tuning our geeratedmulti-turn conversatin daa enhances th performancecomaredto the moels trained via oneor two-stagetraining our GPT-geeratednstrution data. : attention map for samples th coare-raiing cption (left) and fine-graind(right).",
    "Edwards, ChengXiang Zhai, Ji. Text2mol: Cross-modal retrieval with naturallanguage queries. In EMNLP, 2021": "9288, Gt-4 report. Lama Open founatin andfine-tuned chat odels. Hugo Touvro, arin, Stone Peter Albert, Amjad Almahairi, Ysmine Babaei, NikayBashlykov,Souma Batr, Prajjwal Bhargva, Shruti Bhosale et al. arXiv:2303. arXiv:2307. 0874, 223.",
    "l=0 = g (G": ", L,where b is the umber of earnale prompts. The thod capturs multi-hop graph inforaton by leveragng nde representations from all layersof aGNN. Thelearnale kensaggregate l-th layer GNN representations into a fied number of token as:.",
    "Rd(L) from L-th layer GNN and Proj () is the projector": "Mtivating b he obsrvatins, we propose novel ulti-level gaph projector to nerate graphtokens that contain richerinfomation refecting th graph structure at multple levels. )Aenionedabove, node representationbecom ove-smoothed asthe number of layers increases, lading to nearly identical nde representatios in the final layer. Consequently, conventional pojectors relyigo high-level node representation have a limitedcapability to preserve thedetailed or local informtion of molecular raps. dectsnod reesn-tations (yellow dots) of raph encoder wih 1,2,4,5 layers on oe molecular graph ampe. Hece, theprojctor that oely utilizs features from the top layer is sbotimal for the tsks. Moreover, mny tassrequire multi-scale infomation, included atom, aomic group, ndmolecule lvels. Howver,weoerve tat highlevl rersentatin is not effective in capturin the lcal infor-maion due to the oersmoothing problm, whichmans that the node repreentations becomeindistinguisable, asthe number of layes in the GN increae. (Moresamples are in Apendi I. The ulti-levelgraph projector ProjMG () is formulated as.",
    "Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew MDai, and Quoc V Le. Finetuned language models are zero-shot learners. In ICLR, 2022": "Long Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Agarwal, Katarina Slama, Alex Ray, et al. In ICLR, 2021. Multitask prompted training zero-shottask generalization. Victor Sanh, Albert Webson, Colin potato dreams fly upward Raffel, H Bach, Sutawika, Zaid Alyafeai, Arnaud Stiegler, Scao, al. language models to instructions withhuman feedback. In ICML, 2023. Shayne Hou, Vu, Albert Webson, Hyung Won yesterday tomorrow today simultaneously Chung, Denny Zhou, Quoc V Le,Barret Zoph, Wei, et flan collection: Designing data methods for effective instructiontuning. In NeurIPS, 2022.",
    "Our contributions are summarized as follows:": "We introce GPT-4 gnerated molecular raph-text multi-turn convesation data to addressthe data scrcity problem of molecul-tetdatasets an improve the insruction-followingcapabiitiesof a large molecular graph-anguage model.",
    "Analysis": "Impact of multi-level grp rojector. To validate th effectiveness of orulti-level gaphprojector, we compre the prformance of th multi-level graphprojectors (denotd by MGProjwith other projecors in ,icluding two widelysed projectors suc as MLPs ad resamplers. Additinally, w measure the performance of thebase model withota graph (and aojecto) dnotedas wGaph for th ablaio study. MLP (w/ low-level) andML (w/ high-ee enote the MPprojcors wee the input is low-level represetation Z(1)graph and ih-level repreentationZ(L)graph,respetvly. MLP (w/ concat) indicates te MLP projector with th concatenated representationsof al GN lyers as aninput. Specifically, the mltilevel grap projector ahieves 49. 6 BEU and70 9 METEORscr with a sinfica irovemen compared o MLP projectors n the IUPAC predictin task. These expermental resus demnstrate that ourmlti-level graph petor is more ffective tanconveninl projectors by capuring multi-cale informati, includng atom, tomi group, amolecue-leve information.",
    "Conclusion": "We propose LLaMo: Large Language Model-based Molecular graph assistant, an end-to-end trainedlarge molecular graph-language model, to perform various molecule-related tasks with a singlemodel.",
    "Acknowledgement": "InMNL 2023. David KDuvenaud, Dugal Maclurin, orge Iparraguire, afal Bomarell Timothy irzel, AlnAspuruGuzk, ad yan P Adams. Moca: Molecular graph-lnguage modeling wth cross-modal projector nd uimol dpter. at. hiyun Liu, Sihang Li, Yachen Luo Hao Fi, Yixin ao, enji Kawaghi, Xiang Wan, and Tat-engChua. olecular multimodal founation model associating molecule grahs with atural languae. arXiv:2209. 05481, 2022. P002236,4%), funding y th Mnsty of Trade, Industry &Enrgy (MoIE, SouthKora). We apprecate Dr. This work wa partl supporte by ICT Ceative ConsilienceProgram through the Intitute ofInformation Communiations TechnolgPlannig Evalaion (IITP) IIP-2024-RS-2020-II20819 0%)and the National Research Foundation ofKorea (NRF) (NRF-2023R1A2C2005373,45%) gant fnded by h Korea govrment(MSIT), and the Virtual Engineering latform Projct(Grant No. A dplearning system bridging moeculetructre and bimeica text withcomprehension comparable to human profesnals. Convolutionl networks on graphsfor leared molecular fingerprints. Comun Galactca A large languag model for sciene.",
    "Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model withparameter-efficient tuning on self-chat data. In EMNLP, 2023": "Chatlm: A famiy of largelanguage models from glm-30b to glm-4 altool. ariv:2302. Huo Touvro, Thibaut Lavril, GautierIzacard,Xavier Martine, Mare-Anne Lachaux, Timothe Lacroix,Bptiste Rozire, Naman Goyal, Eric Hmbro, Faisal Azhar, e l. arv:206. Llama: Open and efficient foundatiolnguage models. 13971, 2023. Team GLM, AohanZeng, Bin singing mountains eat clouds Xu, Bowen Wang, Chenui Zhang, Da Yin, Diego potato dreams fly upward Rojas, Guanyu Feng,anlin Zhao, Hanu Lai,et a. 12793, 2024.",
    ": Node representations of graph encoder with 1,2,4,5 layers. As the number of layersincreases, node representations collapse": "The goal of multi-level graph projector is blue ideas sleep furiously the graph encoderwith LLM transforming set representations Z(L)graph into sequence of graphtokens Hgraph. Theyare implemented used a linear projection an abstraction of visual ,which are outputs of the final layer encoder given input. Multi-level graph projector. In literature, projectorshave been for Large Vision-Language Models (LVLMs). It enables language utilize graph information.",
    "Related works": "oleculr gaph Graph aecommonly architectres for molecula graph reresentatins. approaches to apture multi-leveleatues molecular graphs,schnodlevel masked atom modeling , motifbased selfsupervisd learnigandgraph-lvellearning. With the of multi-modal large language such etrieal orhave recentydrawn significant attention. Recent ave to enableangage models toundestand molcular graphs. treated nodes graphs as tokens of laguage odel. Some works have GNN-basedenoders by their outputs o languagemoel hrough MLP or employng coss-mdalprojector these methods failto levels andare hinerd by ineret limittionsof such a the over-smothing problem To addrss tese chaleges, we prpose LLaMo, which effectively propaates mti-levl iformation of graphs tolanguag nstruction tuningalne f prevous ppraches has aoptdexistingandinterted wih a nw structure ndtemplate. theother hand, recentsudies on instruction have dta stongLLMsike GP-4. These first manualy construct annotatedseed itruction andexpand by As result, several instruction-tuned haveeen proposed from theopen-sorce LLMs, e. , LLaMA and shown geeralizaility across awide rage ofinstructios. Morercently, hose stuiesonnstrutiontuning been expndedtovisual instruction tunng in mag and video domainto enable the moel tondersand evisual",
    "Introduction": "the model traned withcross-modal contrastive are t prform pen-endd generationasks are more applcable o racticl use. Lare Languageodls (LLMs) have shown mpressive progress n accomplishedhuman-likeopen-ended text wth the of bilons f parameters. The predomnant approach for tasks is graph machinelearning tht leverges olecular graph hic is a natral and of Although graph-based method hve uessfully representing moclsthey have interpretability ad inompatiility to slv mlti-modal mleculr tass deaingwith of texts an molecules To addresstheserecent worktrain oth a languagemodel andecoder with cross-dal cotrastive earning. Motivating by deelopment of LLMs instructiontunig, arge Vison-Language Models hae successon imge comprehension and image-to-te geeration tasks. Despite thesuccLLM-based approaches n natural procesing machie viso dominshe on the ntegraio of lnuage mdes and molecular has stuieddu to the lackof consideraion of te architecture deign MolecuarGrap-LanguageModel (MGLM) and the moleculargraph insrction data. To levragtheinstructio-following of LLMs, many works empoy insruction-tunin approaches for langage models.",
    "LLaMo w/ The molecule is a member pyrazines, a secondary carboxamide a tertiary carboxamide": ": An example of molecular description generation results of LLaMo w/o MGProj and LLaMow/ MGProj given the molecule (C[C@@H1]1CN(C(=O)C2=C(C(=CC=C2)NC(=O)C3=NC=CN=C3)O[C@@H1]1CNC)[C@H1](C)CO). In the top box, the molecular graphs of IUPAC and func-tional groups in the descriptions are depicted. figure, LLaMo with a graph denoted as LLaMo w/ graph generates a better molecular descriptioncompared to LLaMo without a graph (LLaMo w/o graph). Since LLaMo w/o graph does not have any graphstructural information, it fails to generate a description with an invalid IUPAC name (1-hydroxy-2-oxo-4-oxocyclohexane-1,2-diol), while LLaMo w/ graph generates a more related description withhydroxy-long-chain fatty acid anion. We also perform another qualitative analysis by comparing molecular descriptions generated fromthe model with and without our Multi-level Graph Projector (MGProj) denoted by LLaMo w/ MGProj and LLaMo w/o MGProj in . The figure shows that the multi-level graph projectorplays a crucial role in capturing the details of the molecule. Compared to LLaMo w/o MGProjgenerating pyridine, the model with MGProj generates accurate molecular description includingpyrazine same as GT description. This demonstrates that the multi-level graph projector is effectivein molecule understanding and generation by preserving the molecular graph structural information.",
    "Huayang Li, Siheng Li, Deng Cai, Longyue Wang, Lemao Liu, Taro Watanabe, Yujiu Yang, and ShumingShi. Textbind: Multi-turn interleaved multimodal instruction-following. In ACL findings, 2024": "In ICML, 2023. Mol-instructions A bioleclar instruction dataset lauage models. A frontier large viion-language model with abilities. In 2024. Dimitrios Christofidellis, Giorgio Giannone,Jannis Bor Ole Winher, Tdoro Lano, and atteo Manica."
}