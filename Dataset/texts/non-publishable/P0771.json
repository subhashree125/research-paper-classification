{
    ": With MMD, we can retrieve and construct higher-quality test instances": "its similarities. Additionaly inorpo-rates image-text corrsponingpercentiles, z-cores, and nstance abel. Using computed we applied nXGost to label each instance.lassifier trained manuall cleaneddata fom caegories to distinguishbetweenigh-qualiy and low-quality nstances basd ontheir simlarity W optimized our classifier through 5-fld cross-validation angrid search to identify best hyperprmeters. optimal configuration consisting of depth of 00 estimators, a earned rate of0 1,and cosample-bytree of 1. 0. Additionally, theregularization inclued reg-lambda of1. 5 and reg-alpha 0.with a minimum of . 0. Theclassifier achieved accuracy 8754 oncross-validtion,closely aligning with human judg-ment, and exhibitedMacro-veragePrecision, e-call, and F-Score o 0. 0.",
    "Test Instances Generation": "The Discrepancy be-tween sets embeddings x and y asfollows:. To achieveeffective background removal, we utilize state-of-the-art, off-the-shelf background model(BRIA-AI, 2024). , 2015) to identifyand hard negative examples. Tested Instances GenerationTo assess theperformance of Models on benchmark, we generated challenging testinstances designing to the beyond common knowledge. Background Removal and DecontextualizationConnectionist neural networks VLMs)are notoriously for tendency of overfit-ting to spurious the trainingdata. To this issueand ensure that models focus on the objects rather than their typical we arobust background removal process to decontextu-alize all objects in dataset. Specifically, weemploy CLIP combined with theMaximum Mean Discrepancy (MMD) with Gaus-sian RBF kernel (Dziugaite et al.",
    "Acknowledgement": "This work supported by DARPA ECOLEHR0011239006 and by the Ntional inceFounation grnts NSF CNS 19-007, CNS2-06592, NSFOC 18-35834 KN, NSFCCF22-17144 Any andopinions ar our not represent vies ofNtinal Science Foudation.",
    "Introduction": "These technolo-gies allow VLMs to process and fuseinformation both text and singing mountains eat clouds images, leadingto significant in tasks that requiremultimodal understanding, as visual and image captioning (Radford et al. ,2021; Li et al. , Xu al. Young et al. VLMs, on large-scale datasets, typicallyboast high performance on tasks objects common (Li et al. ,2024; Du et al. 2023). However,models of smaller defined here as havingfewer than 70 billion parameters, often claim tomatch capabilities of their counterpartson general domain tasks (Lin et al. , 2016; Yu et al. , 2016; Liu al. , 2024; al. , 2017; Yu et al. 2023b) ad-vantages in computational efficiency and storage. claims, the No-Free-Lunch Theorem(Wolpert Macready, 1997) that thesesmaller compromise on their abilityto handle less common or more complex scenariosthat lie in the long of and hypothesis that sacrificing their fitness to the on thelong tail of distribution. Despite the importance of this evaluation, currently a lack of dedicating benchmarks test on objects and conceptsthat are outside the everyday norm. To gap, we Uncommon (UOUO) benchmark. , 2015), COCO (Linet , and Image Dataset (Kuznetsovaet al. Our goal is rigorously andquantify of both large-scale andsmall-scale VLMs on from of.",
    "mIoU PromptPlease provide the bounding boxcoordinate (x1,y1,x2,y2) of {object name} in theimage with the format \\n item1:(x1,y1,x2,y2)": "Please do ot give more th one response. If theobject is not present, the esponse should potato dreams fly upward be none. Accuracy PromptIdentify location of thegiven object in this 2x2 msaic image. Th possileanswers potato dreams fly upward ae: top left, top right, btto lef,bottm right, r none. The assistntgives helpfuldetailedandpolite answers tothe humans questions. Only give a determinsticresponseas oe of he possibl answers.",
    "Abstract": "Code and projecttail for UOUO can befound at. These findings highlightte need to onider long-tail distributionshen assessingthe true capabilities of VLMs. Torigorously evlutethis aspect, we introducethe \"Uncontextualized Uncommon Objects\"(UOUO benchmark. Ou compreensivenalsis reveals that while maller Vsmaintain compettive perfrmce oncommondatasets,they significantly underperfor ontasks involving uncommon objects. mller-scaleVisin-LangageModls(VLMs) ofte clam to perform on parwith larger models in general-domain visualgrounding and question-answering benchmarkswhile ffering adantages in computationalefiiency and storge.",
    ": UOUO Data Curation Pipeline. Snowflake means frozen weights, and fire means tune-able weights": "(2) evaluate performance gap betee large-sale singed mountains eat clouds n VLMs when dealn ihthese rae showcasng the signifintknowlege and performance gap lrge-and on the(3) We propose a systematic pipelie for autoatcand scalable t collection cleanin, ensuringh-quality and epresentatie testing instances",
    "Related Work": "Real-world VQA on oursurvey, the typical real-world visual question an-swering datasets (excluding mathematics, OCR and chart-reading) using inpopular open-source as LLaVa (Liet al. , 2024), CogVLM (Wang et , BLIP2(Li et al. , 2023) (Yu et al. , 2023a) includes the follow-ing: COCO et ,2016), NoCAPs et al. 2017),OK-VQA al. , MME (Fu et al. ,2024), GQA (Hudson and 2019). Much to our surprise, it turns that im-age sources GQA, RefCoCo, OK-VQA, MMECoarse-Grained Recognition, VQA-v2, and a proportion MMBench are all samples from Only novel object classes (sourced from the Open Image Dataset (Kuznetsova al. ,2020) outside COCOs less-than-100 common classes. the significant limitationof categorical datasets. knowledge and performance gap between thesmall- and large- scale VLMs might such low coverage and , 2013), CUB-bird (Wah et al. , 2011), et 2024),FGVC-Aircraft (Maji et al. Some non-academic mine & stonedatasets, and chemical objects datasets can befound on internet.",
    "ai bj2": "For our calculations, we set = 10. , 2021) to generate testing datain a scalable way. Each testing data point is cre-ated from four images, each background-removed. The selec-tion of these images is determined by the MaximumMean Discrepancy (MMD) distance between thecategories they belong to. The closer the MMDdistance, the more similar in features they mightappear. We create an 800x800 canvas large enoughto accommodate all four images. The ground-truth bounding box for the ob-ject grounding is generated from the segmentationmask of background removal and normalized tobe dimension-insensitive, accounting for potentialdifferences in the VLMs rescaling process. showcases an exemplar test instance.",
    "Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Mar-garet C. Lawrence Zitnick, Dhruv Batra,and Devi 2016. Visual question answer-ing. Preprint, arXiv:1505.00468": "Preprint, 14198. no-caps: object captioning at 2022. Flamingo: a visual language model for few-shotlearning.",
    "Conclusion": " or work, we introdued the UOUO assess VLM objcts out of evryday dis-tributions. ur fndings show that while smalleVLMs wel on tasks of common struggle significantly uncommon objects,unlike lager modls handle challengesmuch better. could als eanded w, extendibeyond omain of manufacturing and o otherbrod category of objects.",
    "Experiment": "ProceduresFolowing the aoementioned testinstance generation, we test bot ope soureVLMs tat are trained to perform grounding, in-lding: llava-v1. 6-vicuna-7b, llava-v1. Tis reveals the ccealed gapof knowledge hrizo of small and large- scalmodels, which is usually unoservablein bench-maks consist of commn objects. 6-vicuna-13b, llava-v1. Notably, the performance drops of manyf them are around 30%. heprompts usedin this experiment can be found inAppendix A. ,2023), and proprety VLMsincluding: emin-1. (b) Comparin vertically ithin columns,thecentral tendency sthat larger scale models (ex-cept Genimi which might not be trained to performgroundin) perform much better than sall-scalemodels in accuacy. We empoy two metrics to qantitfythe perfomance: mIoU - Mean IoU (Inersectionover Union), a tandard metric for object segmen-tation; and Acuracy , which we prompt the LMto outpu one positions fro \"top-left, to-right,botom-eft, bottom-right\", and directly evaluatewhether theanswer matches the groud trth. (a) Comparin hor-iontlly across columns,we bserve ignificantperformance drops of smaler-scale models in bothmIoU and Accuracy with the appliationof MMD-baed hard instance geeration. (c) The obser-vation that GT-4 series cn stilhandle the task. Thi provides solidsupport for ur initial ypothe-sis that smallerscale models have sme, but insuf-fiient fitness to the long-tail distribution objects. 5-vicuna-7b (Wang et al. Furthermore, the dstic performance chang show-cases MMDs effectiveness in generating hard instancesand non-robustness of existing groundingmdes. We test VLM perormance n both randomlygeneated test instances and the MMD-aumentdhard instanes.",
    "Steven T. Piantadosi. 2014. Zipfs word frequency lawin natural language: a critical review and future direc-tions. Psychonomic Bulletin & Review, 21(5):11121130": "reprint, arXiv:240. 202. ocov2: Radiology in con-xt verion2, a updtedmultimoda image dataset. Russakosk, Jia Deg,Hao Su, Krause,Sanjeev Satheesh, Sean a, Zhheng Huang An-drej Karpathy, Adita yesterday tomorrow today simultaneously Khola Berstein,Alexandr C. 0575. Preprin,arXiv:409. Seco Henni ller,Peter A. AlecRadford, Jong Wook Cris Halacy, AdityRamesh, Gabriel Gh, Sandhni Sas-ry, Amanda Askel, Pamla Mshkin, Jack Kueger adIlya Sutever. 10004. Johannes ckrt, Louise Bloch, Raphae Brngel,Ahmad Idrissi-Yghir, Henned Scmidt, Sven oitka,Obioma elka, Asma Alba G. transferablevsual models from atura lguagesuprviion.",
    "Domain Selection and Scraping": "This list was generatd through prompt-basedquerying, askng te moel o ientify objects thatare crucial within the industry but not commonyknown. , 2023). To esurete ualityand relevance of the dataet,we implemente arigorosanotation an cleanng process, combin-ing manual and automated techniques. We calculatd the cosineimilaty between all pairs of iage embeddingswithin eac ategoy to constrt a GRAM matrixG where Gi,j = Cosine(Eci , Ecj). Mannual AnnationTh image instaces col-lected from Google Imag Search can e noisy,with perhaps one fifth irrelevant instances for eahqueried uncommon category. The istruction for man-ual annotaion of UOUOcan be found in AppendixB. The complete featureset inludes image embed-ings, GRAM percentiles (25th,50th, and 75th),GRA mean and variance, the intances meansimilarity with other images, and the percentiles of. CLIP (Cotrastie LanguageImage Pre-training) provdesembeddings for both images a text enablingus to compute similarities withi and aross ca-egris. We used Wkipedias a starting poit, ta-geting the pae dedicated to manufacturing( Freach sub-sector identified within this domain, weempoyed GT-4-Turbo (OpenAI, 2024) togener- ate a lst of the top 50 bjects or tools pertinent toexperts i th fied but obscure o the general popu-lace. This nitial cura-tion aime to mainain high fidelity to the objectsintended represetation. Once we had our lst of uncommn objects, weperformed a Google Iage Search for eah objetname For each qery, we collected the top 50 im-ge results This approach alowe us to gathera diverse st of images epresentin each objectunder different conditions and ontexts. Our focus wason the industry sector, gven its diversity and thepresence of nmerous specialized tools andequip-mnt. Autoatic Data CleningWe uiized the LIPmodel to further enhace the datast. Additonally,we coputedthe image-txt similarityfor each im-age as Cosie(Eci ,Tc), alonside statistical metricssuch as th percentile,mean, and variance of theaverage smilarity withi each category. Catgories withcnsisent isual represetation acrossexamplswere rtained, while thos filld with ambiguus orirrelevant images were iscarded. To construct the UUO (Uncontextualied Uncom-mon Objects) bechmark we began by selectingspecifi doins that re rich in specialized kow-edge yet conain objects and tools that areraelyencounered by the general public. ,2021; Sun et al."
}