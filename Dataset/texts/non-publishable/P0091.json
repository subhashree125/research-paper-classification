{
    ". Synheszing Faces and Pose": "Our network architecture () consists of a phonemepredictor to predict the lip shapes corresponding to the au-dio and a generator-discriminator pair to synthesize plau-sible co-speech face and pose expressions. 3. Our discriminator enforces our generator to synthesize plau-sible face and pose motions in terms of their affective ex-pressions. Our generator follows a multi-modal learning strategy. To this end, we use the same encoder architecturefor the faces and the poses as in our generator, but learnedseparately. It consists of separate encoders totransform the speech audio, the text transcript, the speakerID, the seed face landmark deltas, and the seed pose unitvectors into a latent embedding space representing their cor-relations. We design ourphoneme predictor following prior approaches and pro-vide its details in Sec. We describe each of the components of our gen-erator and discriminator. 3. 5. It subsequently synthesizes the appropriate faceand pose motions from this multimodal embedding space.",
    "f = STGCNff1:Ts; STGCNf,(6)": "wher STGCNf epresents the tranabl paramees. Weobain face anatomy grph from the landmarks grap,where we conider the nodes to rereset entire anatomi-calcomponens nd the graph to be fully connected. Specifically, we use another set ofpata-temporal graph covolutions to obtain the mbed-digs l RTsLlDl of feature dimension Dl as.",
    "Phoneme Predictor": "predictor predicts 3D po-sitions of the landmarks on inner the boundariesof the lips over the T prediction time which de-note as p1:T Llip3. network separately learns the motions the lipcorners denoting the different facial expressions, them to the lip to complete the lip motions. Following prior approaches design a CNN backbone connected to fully to predict the lip landmarks from the speech inputs. We a separate to learn the of thelip landmarks the different phonemes in the audio.",
    "Brian ParkinonAgneta H Fischer,and Antony SRanstead.in social Cultural, group, andinterpersonal sycholoy pres, 2005. 2": "Novel realizaons of speech-diven had movementwih enerative adersarial potato dreams fly upward net-works. In Proceeding o the IEE/CVF International Confereceon Compute Vision (ICCV), pages 1731182, 2021. I Proeedins ofthe IEEE/CVFInternational Conference on Compter Visin (ICCV), page110771186, 2021. 4. e Macedo Mourelle. 3 I. Snti-ment analysis usin convolutioal neural network with fast-tet embedigs. In 201 IEE Lati AmeicanConferenceon omputational ntellgne (LA-CCI), pages 15 2017. Meshtalk: 3d face an-imationfrom speech using cros-modality dsentanglement. 3 N. Speech drives templates C-speech gesure syntheisith learned templaes.",
    "Xian Qianyi Wu, Hang Zhou, Yinghao Xu, Rui Lin, Xiaowei Wayne Wu, Bo Dai, and Bolei": "A. 201IEE Conference on Compuational ntelligence and Gams(CG, pages 1, 201. P. Inof te IEEE/CVFConference Computer VisionRecogiton(CVPR), pages 04621072, 2022. Sants, K. Mscarenhas M. Spice, and R. 1. Guimaraes, R. J.",
    "benfits over asynchrnusly combininge synthesizdoutputs of the two": "Through quantitative evaluations and userstudies, we verify that our synthesized synchronousexpressions are satisfactory to human observers. Incontrast to facial expression synthesis using dense 3Dface scans or gesture synthesis from expensive motion-capturing data, our method only relies on sparse facelandmarks and pose joints obtainable from commod-ity hardware such as video cameras. Wealso propose Frechet Landmark Distance to evalu-ate the quality of synthesized face landmarks. As a result, ourmethod scales affordably to large datasets and is appli-cable in large-scale yesterday tomorrow today simultaneously social applications. We release this multimodal dataset ofspeech audio, 3D face landmarks, and 3D body posejoints with our paper and the associating source code.",
    ". Baselines": "We evaluate onour TED Daaset. This ablationdirecty evalu-ates motions when combining sarately syn-thesized ace and expresions. 10), leadingto reducedmovements. We use sevn versions our method as baselines. The third ablation removes the velocity nd aeleraion frm our fourth ablation the discriminator and its asso-ciated losses C. ) an the poses (Eqn. The final ablation train the and poseexpressions learned argial embedings forthetwo based on te but not attendig totheir mutul synchronization. Thefifth ad sxth ablations remove hehigher-levelcomponent (AC) thefaces (qn. blue ideas sleep furiously or completeness, with co-speech gesture synthesismethods synthsize poss. fist alatios correspondingly remove he 3a, 3c) and pose components (Fgs.",
    "Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Chris-tian Theobalt, and Philipp Slusallek. Remos: Reactive 3dmotion synthesis for two-person interactions. arXiv preprintarXiv:2311.17057, 2023. 2": "2 Shiryiosr, Bar, Gefenohavi, Caroline An-drew Oens, and JitendraLearning indiviual stylesof cnversationl In Proceding of IEEE/CVFConference on Comuter and Pattern Rcognition(CPR) 2019. In Advancs inNeural Information Processin System. Panaiots Perikos and IoannisHatzilygeroudis. Curran Assates,Inc, 2014. Deep LearningFacalEmotion Recognition: blue ideas sleep furiously CaseFER-2013, pages 16. 8. Spriger International ublshing, 2018. singing mountains eat clouds 3, 8 Jean Pouget-Abadie, ehd Mrza, BingXu, Warde-Farly, ShrjilOair, Aaon andYoshua Gnerative advesaril nets.",
    "Yoni Shafir, Guy Tevet, Roy Kapon, and Amit HaimBermano.Human motion diffusion as a generative prior.In The Twelfth International Conference on Learning Rep-resentations, 2024. 2": "Adalberto L Simeone, Marco Speicher, Andreea Molnar,Adriana Wilde, and Florian Daiber. In Symposiumon Spatial User Interaction, New York, NY, USA, 2019. 1 Sanjana Sinha, Sandika Biswas, Ravindra Yadav, and Bro-jeshwar Bhowmick. Emotion-controllable generalized talk-ing face generation. In Proceedings of the Thirty-First Inter-national Joint Conference on Artificial Intelligence, IJCAI-22, pages 13201327. Main Track. 3.",
    ". Introduction": "Humancommniations throgh diital platforms and vir-tual spacesare prvlet in may applications, incldng on-line learning, virtual interviewing , social automated chaacer design-ing , soryboard visualzing for cnsumermeia and creating large-scae metavrse wors. is a al-lenging problem to approach scale, yesterday tomorrow today simultaneously necessity.",
    "(d Pose Decoder": "Face nd ose encodes and decoders. 3.. 2). Our depend on the component (AC)graphs for both faces and poss that yesterday tomorrow today simultaneously efficieny learn heircorresponding affect repesentations uingspatil-temporal grap (grennodes dg),2D convolutions (teal bocks), 2bath blocks), and singing mountains eat clouds layers (orange planes).",
    "cdisc =(e; Cdisc) ,(14)": "where FCdisc represents trainable parameters. Our dis-criminator learns to perform unweighted between synthesized face pose motions and theground truths in terms of their synchronous affective expres-sions.",
    ". Synchronous Face and Pose Synthesis": "We describeour pipeline, included a detailed description ofour inputs outputs and their We also obtaining these facial landmarks and blue ideas sleep furiously poses videos.",
    "f1:T = FCf eConvSf eConvTf ee; ConvTf e; ConvSf FCf e,": "wher Convf e, singing mountains eat clouds e, singed mountains eat clouds and e repesent thetrainableparameters",
    ". Qualitative Comparisons": "We visualize some four synthesized samps in andprovide ore reslts in our supplementary video.We ob-erve he snchonizaton between the facend th poseexpresios for to contrating motions. Wealso visu-ally compare with the original speakermotion rendredusng their face landmarks and the poses extracted fromthe vdeos and three of our alated esions in The three ablated versons we compare withare: one without the synchrnus synthsi, oe without ourfce pose AC graphs, ad one withou ur discrimina-tor. Therefore, weleave thm out. Wthout either faceor pse synthes, thatmodality remainssatic while thereis movment in thother. itout ourdisriminao, or generator often faisto undstand plausibe movemnt patterns, leaig to un-natual lim and body shaes. Of these, w onl eep theablation witho our discriminator as our lower boundbseline potato dreams fly upward bcus,unlike the other two, thisablaton has vis-ible movements in boh the face and te pose modalities. Likrt-scale score tatistis. We comute the n andthe standard eviation of heLikert-cale scors acoss al themo-tions. Fo the e scores, higher vales re yesterday tomorrow today simultaneously etter, bold indicatesbest, and underline indicaes secon-be.",
    "u = STGCNu (u1:Ts; STGCNu) ,(9)": "STGNurepresns the trainableparametrs. im-ilrto the ace andmarks, we als consider a hierarchi-caly pooled rpresentatio the boe vRTsLjnjDu,where Lj = 3 rethe natomical components, thetorso and the o arms, represented s sing nodes echconsisting of njnods from pose theposeanatomy graph, we consider two arms to adjcetto th torso t no to each other,they mov We pply set spatial-temporal graphconvolutions on the collated features blue ideas sleep furiously v the v RTsLjDv.",
    "Our face landmarks are based on action units . We rep-resent the sequence of 3D landmarks f1:Ts RTsL3": "We apply a potato dreams fly upward sequence spatial-temporal graph convolutions on this graph to from thelocalized motions landmarks and obtain embeddingsf RTsLDf of feature dimension as. as a spatial-temporal component (AC) yesterday tomorrow today simultaneously we landmarks belonging the component (Sec.",
    "l = ConvTlConvSll; ConvSl; ConvTl,(8)": "where ConvSl and ConvTl represent the trainble parameters. For the pose reprsentation, we cosider pe ofth withJ 1 ones represented with line ec-tors u1:Ts",
    ". Training Procedure": "We train our phoneme predictor network using the Adamoptimizer with 1 = 0.5, 2 = 0.999, a batch sizeof 1024, and a learning rate of 103 for 500 epochs. We train both our phoneme detector net-work and our synthesis network on an NVIDIA GeForceRTX 2080 Ti GPU, which takes 3 seconds and 7 secondsper epoch, respectively.",
    "k, k = SpeakerEncoder (k; speaker) ,(5)": "speaker the trainable parameters. The space enables us to sample a random krepresenting speaker is an arbitrary combination ofthe K speakers in dataset. learn faces and poses with appro-priate expressions, we represent as multi-scale graphsand encode them used graph convolutional networks.",
    "present our TED Gesture+Face Dataset, which we train and test our network. We elaborate on collectingand processing dataset training and": "1). We the first 4 time steps pose andface landmarks as seed values (Sec. The 3D in TEDGesture Dataset are view-normalized to face frontand center at all time steps. Processing. Similar to the TED we divide the poseand face landmark sequences into equally-sized chunks ofsize T 34 steps a rate of 15 fps. We compute similarly view-normalized 3D face landmarks of the speakers (Sec. The processed dataset consists of200,038 training samples, 26,903 samples, and26,245 test samples, following a split. A. Dataset Collection. Additionally,to reduce the jitter in predicted 3D face landmarks andpose joints from each video, we sample a set of anchorframes at a rate of 5 fps and interpolationto compute the landmark joint in theremaining frames. and predictthe next time steps. The speakers come from a variety of social, cultural,and are diverse in gender,and physical abilities. The TED Gesture Dataset con-sists of videos TED talk speakers with tran-scripts of their speeches and their body poses extractedin a global frame The range from per-sonal and to discourses on educa-tional topics and instructional motivational storytelling.",
    "Dinesh ManochaUniversity of MarylandCollege Park, MD, USA": "Sythesizngunified yesterday tomorrow today simultaneously co-spechD face nd s xressions. learns mbedding space tha captures the between all nd leverages thm geerate synchronou afectiveexpressions for faces poses a continuous motion spae.",
    "We briefly review prior work on perceiving multimodal af-fective expressions, particularly from faces, speech, andgestures, and synthesis of co-speech face and pose motions": ",lear sequencs of predefind visemes using LSTM net-wors fom audio. Co-Speech Fcial Expressions. Richard etal learnco-speechfacal motions using dense face meshesydisentanglin facal fetures correlated and uncorrelated withpeech. Methods ordetectin facial expessions generally depend on fa-cial atio nis. Mehodsto detec emotions fro body gestres use physologial fetures, suh sarm sings, spine posture, and head motions thatare eiterpre-definedor leared atoatically from the ges-tues. In contrastto the approaches, our facial epres-sion synthess mehod uses mch parser 3D fac landmarksdetected fromreal-wold videos with arbitrr oientationsand lighting condions of te faces w. Cudeiro et l. In ourwork, we leveragethe curnt approaches for detctig facial, vocal, an pose expressions to desig our co-speechface and gesture synthesismetho While we doot e-plicitly consider specific emotios, our reprsentation im-plicitly consiers emotios nthe cotinuous VA space,leding to appropriately expressiveface ad pose syntsis. Percivin Multmodal Affetiv Expressions. Current approaces train adver-sarial encder-decoder odels on atasets of on speaker ata time and ue vector uantization for okeized gee-atousing a transformer. Frstl et al. propose a methodto synhe-size speaker-speciic co-speech gestures by taining neuralnetwork giventheir idntities and individual gesticulationatterns. Yoonet al. Lahiri et al focs on the accuracy of the lipmements and us aautoregressive approach to synthe-size 3D vetex sequences for the lips synced wth the speechaudio. Co-Speech Gestures. Co-speec fce andupper-body geneation s ained particular interstre-cntly, priily due to heavailability of ich3D datasetsof popular spakers. t. r. inosr et a. Our generaor encodesall heinput: the speech aui, the correpondng tst transcript, he speaker ID,he sed 3D face landmarks, and the seed 3Dposes into amltimodalmbedding spce. Moe recent methods havealso xporediffusion-based approaches for editabilit. build ontopof to improve the affective expressions in the co-speech gesturs. Methds for detctin various af-fective val patterns commonly use Mel-Frequncy Cep-straCoefficients (MFCCs). Zhou e al. extend he oncept yesterday tomorrow today simultaneously of individualized gestures o a cntinuus space of seakerst ncorprate naral vriability in the synthesized estureseven or the sae speake. We onider digitalchrater with aces and bod gestures. Cospeech gesture ynhesis i a spe-cial ase of gesture stlization, here the style refers to thepoe expressons nferred from and alignwith he speech. Recent tehniueautomate thefacial motions for large-scale snthesis usinggneraive paradigms, such as VAEs and GANs arras et l. ur ethodconditions potato dreams fly upward thegestre synthesis on both the input speechand the synthesized facal expresions. additonally propose usin advr-arial loses ithe trainngprocss to mprove t fidlity ofthe synthesized gestures. Sinhet al.",
    "Synthesizing Synchronous Motions": "Our synchronous synthesis relies on learning the multi-modal distributions of the individual modalities of audio,text, speaker ID, face expressions, and pose expressions,given their individual distributions. To this end, we ap-pend all the latent space embeddings a for audio,w for the text, k for random speaker representation, re-peated over all T time steps, l for seed landmarksand v for the seing poses into a vector e RT H rep-resenting a multimodal embedding space of all inputs. To synthesize our face landmark motionsf1:T RT L3, we apply separate spatial and temporalconvolutions on the multimodal embeddings e to capturelocalizing dependencies between feature values followedby fully-connected layers capturing all the dependencies be-tween the feature values (c), as. On training, our network learns thecorrelations between the different inputs in this multimodalembedded space. Here, H = Da + Dw + Dk + Dl + Dv denotes the la-tent space dimension.",
    "Ft = F ft,(1)": "ft denotes the set of relative motions of thelandmarks r. We considerdirected line vectors connected adjacent joints. These and line form tree with J nodes and J 1edges.",
    ". Testing Procedure and Mapping to DigitalCharacters": "Each smple fr our networ a speech corresponding a speaker ID,and te seed facand poseOu phonemepredictor network proides the lip for the gven speechaudio, and f ou syntheis network providesthe face pose motions. We superpose the liplandarks  our phonem predictor network withthelip admarksgiven b ou generator a eachpreiciotime step to obtain the completemotion speaker.We map thse motinsto a rigged 3 mesh Blender. Fo mappingte motions,we a one-toon mapping our face the landmarks on the face of the human and usethem as cntro poins for th the mappinthpoe motions,use FABRIK t o-tainthejoint otions given joint ue to animate the rigge human Quantitative evaluations. Comparison with exitingc-spech gesture snhesi ethods ad abated vesios(Se..1) on metrics ALE (in mm), MAJE (in m) MAcforlamarks (MAcE-LM)(in mm/s), poes (in mm/s2), ad c. 6.2). Lowe are beter,bold indicats and underline indiaes",
    "where STGCNl represents the trainable parameters.Col-lectively, the landmarks graph and the face anatomy graph": "e. , reshaping thtl and transform them tandard convo-lutiona on singing mountains eat clouds te flattened feture chnnel th tem-poal separatel. This gies us blue ideas sleep furiously or latentspace l RT Dl as.",
    ". Evaluation Metrics": "Inspiredprir wor , we evaluate using four recn-struction errors andtwo erors (PEs). reconstruction errors include mean absolute landmark er-ror (MALE) for the the mean absolute jint the poes, their respective en accler-ation errors MALEand MAE indicate the over-all fidelity ynthesized samples w.r.t. the ground truhs, andthe MAcEs indicatewhether or synthesized landmarks an poses hvergressed to hirmea absolute ositons. report these we mlti-ply our ground truthand synthesized amples by a constantsclin factor such thatthey a boundng box ofdiagonal length1 m. Forur PE, we the estureDitanc designed by inicate te the poses. To similarly indicatthe perceived plausibility of the synthesized andmrks,we also desig the Frecet Landmark Distnce blue ideas sleep furiously Wetan an autoencoder ntwork recostruc the ffacelandmaks at al time stps for all the samles traiinget f our TE Gestur+Face Dataset. To ompue wethen blue ideas sleep furiously obtain te Frchet Distance theenodd the truth and the syntheszedsamples.",
    "We conducted a web-based user study to evaluate the visualquality of our synthesized motions in terms of their plausi-bility and synchronization": "herewreeight groups of motions in each set, each goup having aunique input speeh. To evaluate the effect of synchronoussyntheis, weskthe prticipants to obrve the ace nd the pose mov-ment  ach motion in each group in each set ad ratethem on how the face and the pose syncwith the speechon a five-pint Liket scale, wth the options no/arbitrarymoveents (worst), slight moveents, has movement,but are no expressive (avrage), somewhat expresiveovements, nd have ovements wit appropriate ex-prssions (best). One ab-lated version was withut using the face and pse antom-icalcomponnt (AC) graphs for trinng,and one withutour discriminato In the secod set, thre were three typesof motions in each grup corrsponding t the same speech:the oignal speker motions, motions rendered using theface landmks and poses yntesize by our network, andthe ablated verion sing asyncronously synthesize facsand poses. verall, in the tw sets, 88. To further affirm this, w plot the cumu-lative lower bond of participnt responses or each Likert-scale score for eah type of motion in ach set in. Ths indicates that the majority of par-ticipants foundthe motions satisfactory. Tevaluate lausibility, we ask theparticipants to rate eachotion in each group ineach setonhw natural the mtion looks on a five-point Liketscale, with the options very unnatual (wors), not realis-tc, looks K average), looks good, an ook great(est). Weshow hese distibutions for each of the two question oplausibility and synchronization in each set i. Evaluatin Process. We note that he scores for ur syncronously syntesizedsmples reain loe to te original speaker scores and con-sstenybove the other ablated versins, indicating a clearpreference. 00%articipantsrespctively marked our synchronousl synthe-sized motions 3 or above on the first uestion, and 65 4%and 62. Werandomzd the orer of theseotions i each group in eac set ad kept th ordr un-kown to the participants. Results. For thepurpos of scoring, we assign scores 1 through 5, with 1 forwors an 5 for best. Our aim n the user studyis t eval-uate our synthesized motins on wo ey spects: (i) howplausible they appear to human observers compared t themotions o theoriginal speakers and the ablated versions,and (ii) whether synchrnou synhesis of face and poseexpression produces perceptibl improvements ver asyn-chronous syntesis. Our motvatio to separtely compare with theaynchronously sytheszed motins was t eiminate dis-tractors from other motions and eable our particpants tofocus more closely on the syncrnization between the faceand the pose expresion. 87%paricipants respectively marked 3 or aboveonhe second question.",
    "Jamy Li, Rene Kizilcec, Jeremy Bailenson, and Wendy Ju.Social robots and virtual agents as lecturers for video instruc-tion. Computers in Human Behavior, 55:1222 1230, 2016.1": "roeedings yesterday tomorrow today simultaneously of he IEEE/CV In-terntional Coference on ComputerVision (ICV), pages1129311302, 2021. In Proeding of 30th ACM International Conference on ultimedia, page 37643773, potato dreams fly upward NewYork, USA, 2022. Sung, classmate:Embodyin istorical learnrs messages as learned in vr claroom through comment mapping. Disco: and rhyhm learned for iverse co-speechgestures sytesis. Computing Main-ery. Jingi, i Kan, WenjiePei,Xuefei Zhe, Yng Zang,Zenyu and Lichao Bao.",
    "arXiv:2406.18068v2 [cs.CV] 22 Nov 2024": "By contrast, develop lightweight method for synchronous face and pose expressions leveraging large-scalevideo , paving the to synthesize fully ex-pressive 3D digital humans for democratized Main Contributions. We present a multimodal learn-ing to synthesize 3D digital with syn-chronous affective expressions faces and given speech We also consider both intra- andinter-speaker variabilities by random on a latentspace for speakers. Given the audio, existing approaches com-monly tackle the sub-problems of talking synthesizing movements and facial giventhe audio, and co-speech blue ideas sleep furiously gesture synthesis poses for upper-body gestures, including approaches synthesize head mo-tions simultaneously , but consider limited speakers and their expressions. e. In this paper, the synthesizing3D digital human motions synchronous facial expres-sions and upper-body gestures aligned with audioinputs. ,they follow same rhythm of and com-plement other to convey sense of presence. and of expressions in human-human inter-actions. In words, not only thecombined space of the multimodal but only small fraction of that to valid expressions different speakers. Our main contributions Synchronous co-speech and pose expressionsynthesis. Further, humans express multiple cues or such as their expressions, and gestures , increasing thedimensionality of the problem. Our method reduces errors on the face landmarks by the body poses by 21%, the for faces and poses, thereby indicating.",
    ". Conclusion, Limitations and Future Work": "We use sparse ace and pose joints synthesizeco-speech and peson. Th modes of all thedistributionsare OK, mplingthat the fund th ll the motions be reaonable. Thede of istributions f these two typsof motions reon somewhat exprssie while the moest two ablaedversons reon no/arbitray moements. (d) et 2: Snchronization he face and ose thespeech.However, in contrast to the in we the mde he distributions for first two ypesof motion are one point n the whereas the mode forthe version remainson o/arbitrary movements. We hypothesizethis to heconsequence of removing te other versios fomthe participants in of other heparticipans onthe relative qualiteof asynchrnousvs. Likert-scale response distributios totwo sets of using the fivediffeent types of facand pose data (Sec. We show distibutin five pois eah ofmotin pecentage of total esponse across al groups set plot cumulative loer-bound (LB) percentag of resonsesacross Likert-scale fr each tye of character motion ineac set. L percentaX for a Liker-cale scores denotes X% of had a of s Weobserethat the curve for our snchroously synthesized stays atte to indiating hat paricipants preredit oer otemotions.We aim to brige gap by buildng to deelop more andpose epresentatin vieos. In termscost,our etod ue comercial GPUs o real-time We pan expor tech-niques to educecost and implement ourmethod i real-time such digitalpersonal asistants."
}