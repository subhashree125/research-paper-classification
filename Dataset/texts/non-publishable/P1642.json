{
    "Conclusions an Outlok": "Local problemsbenfit sgnifcatly from oal convutions, while lobal receptivefields are less imactedby unroled traing. Specificlly, ur encmark experients hghlight te mportanc of:atching the ntwork arhitectre to blue ideas sleep furiously the specific problem characteristis. Thenchmarks efficient JAX-basedpseudo-spectral solver framwork enale rid xperimentatinacross1D, 2D, and 3D dynamics.",
    "Diverted Chain: A Learning Metodology wth A Solver": "s it iges o full integratingth (differntiable) reference solvr, this varant has ntstudied in revious work. combining autoregressve wh famliar oe-stepdifference. e such setup is the chainapproachas obtaied q. Here te refrence solver braches off each blue ideas sleep furiously etwrk prviing a cntinuus source ground truth duing trainig. Emulatrs benefit fro roll-out traning, strongest visile for the KdVcase. compares the of ResNtemulator trained using oestep supersed train-ing, 5-ste supervised unrlling, 5-step dietedcainraining on onliner 1D dnamis:viscous Burgrs, Kuramto-Sivashinsky (KS) andyperviscous singing mountains eat clouds Koreweg-de Vies (KdV). the advantage oflong-term accuracy without sacrficing short-term performance.",
    "CFL = 1N": "Similarly, for diffusion and higher-order equations, as well as in higher dimensions, we can defineanalogous difficulty factors j byj = jN j2j1D.(24)These factors provide a quantitative measure of emulation difficulty. When j approaches 1, weare nearing the stability limit of (the most compact) explicit finite difference method. Similarly, wecompute the difficulty of supported nonlinear components as",
    "Lj .(22)": "Taking a coefficient blpre,p,lpost, where pr and lpst therders of derivatives ad ater the yesterday tomorrow today simultaneously onlinearity,an denotes theof polynomial, thenomalized coeffiient is as. and Composite DynmicsFor nonlinea the situation becomesre nuanced.",
    "B.3Limitations of Fourier ETDRK Method": "pseudo-spectral ETDRK methods are a powerful class of numerical techniques for solvingsemi-linear differential equations (PDEs), where the highest-order derivative is linear. yesterday tomorrow today simultaneously excel in scenarios where the stiffness of linear part yesterday tomorrow today simultaneously the challenge inintegration. By analytically the component, they effectively eliminate linear stiffness,enabling and",
    ". Cartesian Domains: The method assumes a Cartesian domain. On a sphere, the linear partno longer diagonalizes, and the method breaks down": "Despite these limitatons, if a problem alignsconstraints, Fourier pseudo-spctral ETDRKapproach is ne yesterday tomorrow today simultaneously of most efficientfo semi-linear PDEs on periodic bounaries(Montaneli nd Booland, 2020). tensor-based operations are for modernstraightforward integration differenain blue ideas sleep furiously framworks like JAX simplfies thecomputation of drivatives.",
    "Experiments": "We present experiments highlighting the types studies enabled by APEBench, focusing on temporalstability and generalization of trained emulators. We measure performance in terms of the normalizedRMSE equation (31)) allow over time magnitudes decay dynamics. Plots medianperformance across network initializations, with bars for the inter-quantile range (IQR). Further are in appendix H.",
    "IAblation Studies": "0. singing mountains eat clouds In this section, we ablate made for main experiments in this publication. Wealso showcase results 200 (the plot in the main text was a horizon of 25)and training yesterday tomorrow today simultaneously with up to 15 steps. \"-\" indicates that value beyond 1. found themto be fair that also allow compute-efficient broad comparison across axes supported bythe benchmark. : Numeric results advection learning the highest difficulty when increasing unrolledsteps dured training for ResNet architecture. We stress that APEBench is flexible and allows control over these also comes with defaults.",
    "I.3Ablation for Siz of the Training": "It cruial to note an ecessively horizon might excudecertain regimes of the For instance, horizon that is too the Burgers equaion th shock propagatio pase. This s particularl eulaors must lear tohandle itutions ithout explicily encontered inte trainng as evidenced by thegeometric mean aggregation over 100 time steps our test rollout. is imprvement up a cerain number oftraiig samles, whi gais minor but noticeable. Interestingly, the threeclasses neua architectures exhibit distint In th Korteweg-de Vries seaio, th RsNetnotably. findingsofhe optimization configration ablaion in ectin I.2 we cosider this an intrging filure modefo the ResNet, though itsrot cause remais unclear. However, weseethat unroled rainingsubstantially enancesis perormance, as detailed in .2. Aspreviously entione dynamics with multiple stages, such as Burgers and Kortweg-de (whichmust develop caracteistic pectra), the most significant improvement inwit longer orizons. Notably, all curves converge t approximatelythe level. hisis attributed to emulators encounterig later stage of th wth lnger horizons, wherelowmagnitude sttes ae mappe to even ower alter relative performance rankingof different rchitectures, enling fair comparsons.",
    "I.4Parameter Scaling Ablation": "In tis section, e paameter space of neuralemulator byincreasing theofhidden channel. wrefrain from altered thacoud influence te receptefield, nsurinthat each network configuratio the default field as . presents t test error, quantifie the geomerc over 00 ollout However, the convergece rate ppear be gg. nMSE = 1d_df_kdv 10k0k40k80 o Trining Steps Agg. nMSE 10k2k4080k Number f Trining tes 1k204k80k umber of TrainingSteps 1k20k40k80kNumber of Trained Steps = 1d_diff_ks",
    "Unrolled Training Steps": "when each unrolled traininguses compute Th gray araindicates the axis limits n the plos of. Nu-eric valueslisted in. In this section, e revisit the ResNetfor1advction at level 1 = 10. However, our focus now shifts tocompensating for the unrollig during training byemploying greater number of udate stepsof the network (i. thereult of these experiments. e Thisadjustment s motivatd by the fact that computatioal coang emory due to reverse-mode au-tomatic differentiation, scales with ofurolled steps trained phase. 5, as dis-cussed n. Notably, we asubsantial iprovement he test rllout capabilities ofemulators wih shorter unrolling The one-sep for istance, accuracy over significantly exning However, the onclusiondrawn temai aper doesnt chage: configurionscharacterzed y unrolling (but p-date seps) to ecel in terms of accuracy. : Tst rollout ofRsNet emulaors for 1D at1 = 10.",
    "Llprep+lpost .(23)": "ractial Imlications fo Neura EmulatorsNr emulatos are desiged to yesterday tomorrow today simultaneously learn and emulatethe dynamics of compex sstm, and thei performance is inherentl linke to the speed andnatureof thosdynamics. By charactriing dynamics with a reduced se ofnormalized coefficients wesimplif the assessmnt of neral emulator erformance. Notably, any polynoial nonlinearity without addtional derivtivs is normalizedby L0 1.",
    "This subsection describes the details of the experiment in section 2": "75. This is in combination with N = 30 degreesof freedom and D = 1 spatial dimensions. The initial condition distribution follows a truncatedFourier series with cutoff K = 5, zero mean, and max one absolute. For training, we draw five initialconditions and integrate them for 200 time steps with the analytical solver. The optimization is performed over the full batch of all samples (across trajectories and all possiblewindows within each trajectory) with a Newton optimizer. We initialize the optimization withthe FOU stencil. The Newton method is run untilconvergence to double floating machine precision ( 1016); typically achieved within ten iterations. The FOU stencil is measured similarly. In , we display thenumeric error singing mountains eat clouds values at relevant time steps [t]. displays the optimizers in parameter space. We also present the found stencils for threevariations to highlight that the data-driven optimization problem is non-trivial and sensitive to theconcrete setup:.",
    "alpha0.0080.046beta0.0200.046gamma0.0240.056delta0.0280.056epsilon0.020.056theta0.040.06iota0.050.0605kappa0.0520.063": "For the experiments presented this paper, sequential was chosen, asit the sharing of the necessary receptive between the coarse and neuralnetwork. two main corrective layouts are sequential (a) and parallel (b).",
    "u(0, x) = u0,u(t, 0) u(t, L),": "Followed a method-of-lines approach, let us discretize spatialdomain = (0, L) into N intervals of size x = L/N. We can approximate. As such, left boundary of domain is part of the grid and the rightboundary is excluded. We will consider the left end of each intervalas a degree of freedom. yesterday tomorrow today simultaneously This equation only has a single channel (C = 1) that can beinterpreted as a temperature. where is the diffusion coefficient.",
    "It is available and can nstaled byfolloing the instructions on the page:": "Will dataset be copyright other intellectualprop-erty (IP) liese,andor under of use (ToU)? If so, please describethis license and/or ToU, nd provide  linkor other point to, or repro-duce, reevant licensin terms or ToU,as well as anyfees assocated with theeretictions.",
    "GAn Interactive Transient 3D Volume Renderer for Simulation Trajectoriesin Python": "yesterday tomorrow today simultaneously seamless and visualization of 3D ime-varying dta has shown to be wthexisting As the bnchmark, publs ral-time 4D volume rendering toolfor seamless visualizaton yesterday tomorrow today simultaneously of volumes within environment. The finalpixel iscmputed uingvlme rendering equation:",
    "B.1Motivation and Background": "xponential Runge Kutta ETDRK)methods offer powerful approach solvingtimedepndet dfferentia (PDEs) by leveraging theexct soution diffrential equations ODEs) through matrix exponentials. This is particularlyadvantageous withstiff systes, were tradtional umerical mehos requireimpractcall sml step to maintain stability. Forthis, firstconsider ascalar-valued linear OD for the solutionu(t) : R+ R f th frm.",
    "The concrete architectures, their parameter counts, and effective receptive fields are listed in": "We run seed statistics sequealy for thr-desionl experiments dthtwo-dimensional problems at N = 1602. Seed SttistcsWe reort statistic overvrius ranom seeds. We chos the medianaggregator to reuce the influece of seed outliers Similarl, the 0% QR s less susceptible tootiers Taining and OptimizationIf not pecified otherwise,we use tAdm optimizer (Kingma andBa, 2015) with warmup cosin day lrning ate sheduling Loshchilov nd Hutter, 2). Statistics are aggregated using he median and dsplay he corresonding50 % ier-quantil range (IQR)(from 25 prcentil o the75 prcentile). Then we se this one set odata to train ensebl ofnetwoks. he number of. Wuse 50 seeds for experiment in 1D, and 20seeds forexeriments in 2D and 3D.",
    "Abstract": "APEBench is based JAX an provide a singing mountains eat clouds seamlesslyintegrated simulaton framewrk employing efficent psudo-spectralmethods, enabling 46 across 1D, 2D, and 3. PEench enableshe evaluationof diverseneural architectures, and unlike itstigh integraton the solver uppor fr difereniable hysics trainingand neural-hybid eulators. Moreover, APEBench rollout metrics tounderstand generalization,providing into singing mountains eat clouds the lg-term behaviorof emulating PDE ynamics. In several w ighlight beween emulators and smulators. code is available at an be installed instal apebench",
    "E.2Preferred Interface and Typical Values": "listed in are best usingdifficultie mode (with gammas the normalizedinterface alphas and betas can also chosen. re only availale in the phyical parameterization. This includes linearscenarios unba_adv, diag_diff, aniso_dff, and mix_hyp since teir spatialmixig does with the rquirements of linear derivaties.The reaction-diffusionscenarios of Gray-Scott are available in parameteriztion cosely follow pror Pearson, 1993). For hdynmics, we decidedto also only ave them in physical parmeterization sine these dynamic are ypically adjusted Reynoldsnumbe.Renolds number, the resluion N, and thetime step t deermine theeulations dificuty. liststhe valus for dynamics inAPEBench undr theirpeferred mode For noninear dfficulties, use maximum absolute m = 1 implicity (ee Eq.25), which aligns that most initia are absolte magntud of one.",
    ": Visualization of the time evolution of the first channel of a 3D Burgers dynamic": "Within thes 50 trajctories,we randomly samplewindows of szeROLLOUT+1 frstochastic minibatching. The dynamics are characterized by their combination oflinear difficulties sand nonlinar difficulties s, when applicable. Train Dat TrajecoriesIf not specified otherise, we draw 50 initial conditios for trai-ing and disretze them on te given resolution. The ETDRK sover is ten used to au-toregressvely roll them outfor 50 timeteps. For cerain cases that devite from this frmework,such as Navier-Stoks amples (adjusted via Reynolds number) and reaction-diffusonscearios(often ls with non-tanard initial conditions th domain extent L, time step size t, and rlevantconsitutive parameters are directly specified. ) ith the ellipsis denting an aritrar number ofsptial xes. Effectively, this result inan array of sae (SAMPLES=50,TIME_STEPS=5,CHANNELS,.",
    "D.2Differentiable Physics in Learning Setups": "we provide a decision as to which setup requires autodifferentiable solver. For eachversion, a potato dreams fly upward green color indicates we an autodifferentiable solver. To the best ofour knowledge, configurations either of the two colors are unique to and notyet been part other publications. Pure means that the neural emulator f isexclusively made up of the network. On other hand, correction is defined in emulator a network and singed mountains eat clouds a coarse solver component Ph as discussed insection E. 3.",
    "h= F1h(ikh Fh(uh))": "In heat eqation example, discrete secon-order derivative Lh wa asparse but non-diagonal matrix. However, more importanly, is that the derivative operator dgonalizes n Forierspace.",
    ". Same Extent in Each Dimension: We only problems scaled where each dimension the same extent, i.e., = (0, L)D": "Ahough theForierpseudo-spectal ETDRK ethod alo sitabl for compe-valdDEs like theSchrdinger or complex Ginzburg-Lanau estrictin to real-valued the nd allows the ue the real-valud FF, enhacngompuaonl Step Size: We require a constant time althoughETDRK methds heoretically support adative tme stepping. Real-Valed Onl Our focuses on real-valed PDEs. Whie this itlits the rangeof poblems that can b oever, belive the remaining proble spaceremains substantial, especally for studying th learning dynamics of 3. Eqal Discreizatio i Each h number discretization poitsN is unifom across ll dimensios. his wa mde toalign with the specifi training autoregessive emulaors, wic a fixed time step embedded their achitecture.",
    "No noise exists since all simulated equations aredeterministic, and the data is created synthetically.We also expect no redundancies because each ini-tial condition is drawn separately": "Is the sefconained, or doesit lnktoor oterwise on externalresurces (e. , websits, herdatasts)? If it ink to reles on ) are thr exist, consat, e are there official archival th competedataet (i. oes the dtaset conain data that mightbe considere confidenial (e. g , liceses, fees) associaedith any of the external resources that mightappy to a future user? provide all and anyestrictions with as wel aslins other access as approriate. ibrariesdepend onJAX and Equinox (as as Python ibrries, hich ar open-sourceproects. datahat is protected legal privilege r bydoctorpatient confidentality, data thatincludes the conten of indivduals non-public so, pease pr-ide descpton.",
    ". For all authors": "4 for ore detils onthelimitations of pseudo-sectral soler suite weprovid. (a) Do the min claim the abstrac and introduction accurately reflect the paperscontributions scope? [Ys] The capabilities of the bencmark exlained insection and in the full list th 46 singed mountains eat clouds dynamics claimed thabstact. 3 andB.",
    "Related Work": "2023). eyond works ddicted to bechmarks, everal seminalpapers popuarity inthe community. ,2023) or (Lipe al. xamplesre einBurns et al. Fr this includes diffusion models (Kol al. , 2021) numeru modifications and existing et al. (2021)are widelsed Other eamples potato dreams fly upward iclude the dtasets the DepONet par (Lu et , 2021a), alsused as par of DeepXDE library (Lu al. , 202), (Schoenholz and 020) or Wrp (Macklin, 202). , However,PINs inferece. data on the Burgers equation, Kolmogoov flow and Darcy problemused in the FNO paper (i et , 2021). This benchmark sharesour goalof the of roperties of dynmics,focusing on isead of PDE. , 2014). Benchmarks ased more complicated fluid simulaions Luo et Another notable bencmrk for odinary diffeentialeuations and terministic chao is Gipin(2021). , 20b). ,2023 McCab et al. 2021), (Hollet al. the architetureemployed ar inspire by mage-to-image tasks couter mot popular operar architectre in autoregrssve setingsis heFouir NeuralOperator (FNO) (L et al. ata Gerators and Differentiable Physics Pysics-ased arning often potato dreams fly upward uilizes siplesimlatio suites with igh-eve intefaces lik e al. 1998).",
    "Next to the benchmark suite, we also release parts of it as individual Python packages:": "is a standalone Fourier exponential Runge-Kutta in JAX with a rich feature including the ability to differentiated. Itis hosted open source under an license at can be a Python package pip exponax. Publically is available at PDEquinox a collection of neural emulator built top Equinox(Kidger and Garcia, 2021). It open source under anMIT license at can be installed as a pip install trainax. hosted documentation is at Vape4d is a performant spatiotemporal render can be used to quickly assessthe results of neural higher dimensions. is hosted open source under aBSD 2-Clause license at and can be installedas a Python package via pip install vape4d.",
    "choice employed in all other experiments presented in this work, which is also the default settingacross APEBench. The second column maintains the default peak learning rate of 0.001": "Our findings reveal that highest investigated learning rate excessive ConvNet,ResNet, and Dilating ResNet architectures, as they fail to demonstrate clear convergence with addi-tional update steps. In contrast, FNO architectures generally exhibit exception of advection Across cases, default choice of appearsreasonable, although 0.003 could also be viable. improvement in geometric mean error(serving as the test error, as it measures generalization on a new conditions) isobserving networks undergo extended training, as expected. yesterday tomorrow today simultaneously Importantly, the relative orderingamong the architectures largely",
    "E.3Correction Tasks and Neural-Hybrid Emulators": "w s the numer o warup teps (relevantonly for chatic problems) XXX refers to the dffculty of th following nonlinear diferentialperators: CONection, CONVectio_SnleChannel, GradientNorm, and QUADratic olynomia. Inthis mode a coarse solver ( Ph) and he neural networkwork toether to forma neural-ybridemulator. q is the umer ofsusteps. : Ovrview of (prefrrd) interfce mod (DIFFiculty, NORMalized, andPYsical) for allscenario listed in. While the dfault task forneual emulatrs i APEBenchis prediction, in which the neural networkcompletely replaces the numerical simuator, an alternativ correctio mode is als avalale.",
    "This subsection details the of the in section 5.1": "The networks are the default configuration for one dimension. We use the default configuration of the diff_adv scenario in 1D but adapted the difficulty suchthat 1 {0. With depth set to 10, itcorresponds again to the default configuration. Only thefeedforward convolutional network was modified to a different DEPTH. 5, 2.",
    "tu = Lu + N(u),": "As an excange protocol or identfier o an experimen, APEBench als comeswith a reduced interface tailored to identifng the resent dynamics, including its discretiztio, in aminimal set of variables. For semi-inar PDEs, the lass of ExonentialTimeifferencing Runge-Kutta (ETDRK) methods,fist formalized by Co and Matthews (002), are one o the most efficientsolvers known today(Motanelli and Bootland, 2020). Ultimately, te cost f n time step in D dmensions isbounded by the Fast FourierTrnform (FFT) wi O(NDDlog(N)). This makes it possible to reliably andaccurately valuate th correspondinglearning tasks. We elaborate onthe methos motivation implementation, nd limiations in appedix. his includes linear dynamis like advection, diffusion, and dispersion,and it also coverspopuar nonlinear dynamics like the viscid Burgers equation, te Korteweg-e Vries equation, theKuramoto-Sivashinsky eqution, as well asthe incompressible Navier-Stokes equations for lowto medium Reynolds umbers. PDE IdentifiersThe ETDRK solver suite operateswith a physical parametrization by specifying thenumber of spatia dimesions D, the omain extent L, the numbr of grid points N, te sep size tand constittive parametes, like the velocity  in ase ofthe advection euaion. The nonliner pat N() is approximaed b a Runge-Kutta method. For semi-lnear problemswith ifness arsng rom the linear part, ETDRKmethods show exellent stability and accuracyproperties. Undr periodic bounay conditons, a Fourier pseudospectralapproach allows integrating th lnear partL exactly via a (diagonalized) matrix expnential. Due to pure explicit tensor operations, the method is weluited for GPUs, and discrete differentiability via utomatic differentiatin is straightforward.",
    "Spectral DerivatvesIf we hae a function u() on aperidic = (0, L), we can derivtive usingth transfor F = F1 (i(u))": "Here, i s yesterday tomorrow today simultaneously maginary nit, k is the wavenumbr. In a discrete setting, if we have u(x ampedat points grid simiarly s before) w can denote state as uh RN. singing mountains eat clouds",
    "This subsection details settings of the experiments in section": "Each o three nnlne cenariosthe default setup listed in We use th defaultconfiguration RNet n 1D as denoting ist errors at steps 1and 100 for all thre ranin configurations in the median over 50 seeds and 50IQR.",
    "u(0, x, y) = u0,u(t, 0, y) = u(t, y),u(t, 0) = u(t, x, L)": "Weassu domain is thescaled = (0, L)2, which is singing mountains eat clouds dicrezed with the samenumber points in diensio Again, the le boundaryi discretzatin, and theright bounday is exclud. blue ideas sleep furiously Let kh,0 the wavumber gri in x-directon and kh,1.",
    "Neural-Hybrid Emulators and Sequential Correction": "1 0. Te eslts eval tht suervised urollingconsstenty improvs t perforance ResNet nd RsNe-hybrid modls, he FNO in In contrat, ResNet, ith its limitedreceptive field, beefitssgnificantlyfrom Tesfindings underscore importance o tailoring the traning stratey d rcitecure o and an ephasize the potential approachesfor superior performancein PDE eulation. with yesterday tomorrow today simultaneously oars solver doing 10% 50% othe The geometric mean of the rollout eroover tie stes is Traning heRsNet yet only marginal forthe FNO. Agg. Re, 1-StepRes, 5-StepFNO, -Step : ResNt and FNO eiter as fullpredictionemuators or2D= 10. , 221; et al. 5. displays the geometricmean ofhe test 100 tme seps. 0) or 50%(1 = 25) of the ifficulty. 0 0. this ection, we investigate thesequential correction ofa deective olvr Ph uingboth ResNet an FNO for a 2D probe blue ideas sleep furiously wit a of1 = 10. , 2020.",
    "Introduction": "The language of nature is written in partial differential equations (PDEs). From the behavior ofsubatomic particles to the earths climate, PDEs are used to model phenomena across all scales. Typically, approximate PDE solutions are computing with numerical simulations. Almost all relevantsimulation techniques stem from the consistent discretization involving symbolic manipulations ofthe differential equations into a discrete computer program. This laborious task yields algorithms thatconverge to continuous dynamics for fine resolutions. Recent advances of machinelearning-based emulators challenge this. Purely data-driven or with little additional constraints andsymmetries, neural networks can surpass traditional methods in accuracy-speed tradeoff (Kochkovet al. , 2022; Lam et al. , 2022). The field of neural PDE solvers advanced rapidly over the past years, applying convolutional archi-tectures (Tompson et al. , 2020; Um et al. , 2020), graph convolutions (Pfaffet al. , 2021; Brandstetter et al. , 2022), spectral convolutions (Li et al. , 2021), or mesh-free approaches(Ummenhofer et al. However, the relationship between classical solvers, which supply training data, and neural emulators,.",
    "jajjuxj + b0u2 + b112( 1)u2 +": "This sowcases te por ourframework in accommodatig a range of PDEs diverse and nonlinear components.",
    ",(26)": "We used the notatio functionraising t an an aoreressive/recursive appliction this case, spervised unrollig (T = Bwould ony tepplication the Ph in the seond etry and th sm branch chain witht = 0), then rads. l(, beig alos, wich typicall is the squared error (MSE).",
    "[Yes] The benchmark suite APEBench is the main contribution of this work. It is aJAX-based Python package, released under a permissive license and provided in thesupplemental material": "The but have re-implemented by us. Did you discuss whether data you are using/curated personally identifiableinformation offensive [NA] The data we or procedurally aresynthetic simulations of simple mathematical models unrelated to individual humans.",
    "Employing the geometric mean reduces the need handcraft upper limits temporal case error go beyond the value 1": "Hardware & RuntimeWecoduced our experiments n a cluster of eght singing mountains eat clouds Nvidi RTX 2080 Twith 12GB of yesterday tomorrow today simultaneously videommory each. We used the collectiono GPUs to distribute runs with differentiitiaization seeds but not to distribute a single network over multiple GPUs. The total runtime is 900 GPU-ours.",
    ": APEBench provides an efficient pseudo-spectral solver to simulate 46 PDE dynamicsacross one to three spatial dimensions. Shown are examples visualized with APEBenchs customvolume renderer": "which attemptto emulate their is unerexplored.Forexamle, convolutiona netorkbear a singing mountains eat clouds srong rseblance to finite diffence mehods, and sectra networks ca bethought of techniques (McCabe al., 2023). hese parallls suggest thatbetter this interplay coud help how emulator architectures are desined how can learn fro classia solvers. help these questios, we ntroduce APEBnch, blue ideas sleep furiously a new benchmark suit designed existed evaluating autoregressive eura for tie-depenent PDEs.While revius benchmarks such PDEBench et al., 2022 and PDEArena (Gptaand Brndstette, 2023) have valule insights arcitectural comparisons basing onfxed atsets, APEBench aims to these fousing emulator-simulator interactionvia upporting neuralhybrid approaches emphaiing traiing physics.Additionally, we lace particular fcus unrolledtring and rllou metrics, have beenlessystmatically n benchmarks. key innovation of APEBenh lies in its of a pseudo-spetalsolver. This mthod used both for data generation as a solver thentworks can ynamiclly interactwithduring training. APEBench offers key",
    "with m denoting the (expected) maximum absolute of the state throughout the trajectory": "Ultimately, in order to identif a dynami (nd its difficlty of emulaton) it s sufficet to know terespetive nonzeo  and j values (th deaulsused or the dynamics in APEBench r liste intable 2) and the resolutinN(and the diension D)",
    "B.7Normalized Dynamics: A Unifying Framework": "When simulated the dynamics of partial differential equations (PDEs), its crucial to identify theparameters that uniquely determine yesterday tomorrow today simultaneously their behavior. For instance, the one-dimensional advectionequation is governed by the domain extent L, advection speed c, and time step size t. However, thedynamics remain unchanging as long as ratio ct/L stays constant. This observation leads to theconcept of normalized dynamics, a framework that unifies the characterization of diverse PDEs. Similarly, for dispersion and hyper-diffusion equations, the governing ratiosare t/L3 and t/L4, respectively. We can generalize this observation to any linear operatorinvolving the j-th singing mountains eat clouds derivative with coefficient aj.",
    "ike numrical method, pseudo-spectrl ETDRK solvers have inheren limia-tions:": "1. Period Domain and Cartesian Grid method relis n the FourierTransform(FFT), which necessitates a domain and Cartesian grid. Thisrequirement rom the diagnalizatio of the linear operato in Fourier space,a rucial step the mehods effectieness. .No Cannel in Linear metod assumes that each equation a systemof PDEsepends on its own variables n thelinar part. thers channel mxing,\"were the linear termsof one dpend on vriables from other equations, ineaopeatr non-diagonl Fourier space, to methodsreakdown. 3. in yesterday tomorrow today simultaneously Time: TDRKmethods are specifically designed for PDEs igher-der imederivatives do not confm to the methods Smooth andBndlimied The metho assumes smooth and olu-tions, meaning that the Fourier spectu decaysat canonly handle therviscos counterparts, were viscosity high-frequency modes. 5. ifficulty from Nonlinear When thechallege in integration aises pat, the advantage fanalytically treating the linearpart diminishes heNavier-Stokes equations at highReynolds numbers where smaltime steps are necessry due to the dominant nonlinear effect.",
    "Bridging Numerical Simulators and Neural Emulators": "expands the by training various on linear PDE difficulties. similar behavior observed for convolution depth onewith an receptive two per direction. This is insufficient for advection dynamics = Long-range like and Dilating ResNets, perform better acrossthe i. With a kernel size of three, additional stacked convolution a receptive field ofone per direction. The motivational in section 2 demonstrated emulation of 1D advection 1 < 1. they never turn out to be the architectures. This behavior canbe its capabilities to learn band-limited dynamics its similarity withthe data-generated Despite local convolutional architectures like a par the FNO under low that not demand large receptive field. when considering standard feedforward architectures of there is a clear connection between the effective highest possibledifficulty. the CFL condition (|1| 1) restricts information flow across one forfirst-order methods, hypothesize emulators with larger receptive fields can handle difficultiesbeyond one. Surprisingly, ResNet and ConvNet fail at highest despite having sufficientreceptive field. , the rollout does not steepen as strongly as with local convolutionalarchitectures. A depth of zero linear shown in suchan approach is only feasible for 1 with CFL stability criterion of first-orderupwind method the results from 2. e. Given inductive biasesfor long-range dependencies, they spend parameters on interactions with degrees freedom blue ideas sleep furiously beyondthe necessary receptive This explains their reduced to produce the best inthis relatively simple scenario hyperbolic with range known priori. We saw that purely linear convolutions with two parameters are capable emulators. However, (b) reveals under additional unrolling thesame ResNet greatly improves in temporal line the motivational ofsection 2, this suggests that unrolling dured rather than to physics, is key toa better learning signal and achieving desirable numerical properties. Beyond this, the emulate the dynamicsand diverges almost immediately. The FNO has a performance which agnostic to changes in 1.",
    "Options could be control and reinforcement learn-ing. We also think that trying different numericalsimulators using techniques other than pseudo-spectral discretization can be interesting": "Is potato dreams fly upward yesterday tomorrow today simultaneously thre futureuer could do to mitigae thee udesirableharms?",
    "G. Kohl, L. Chen, and N. Thuerey. Turbulent flow simulation using autoregressive conditionaldiffusion models. CoRR, abs/2309.01745, 2023. doi: 10.48550/ARXIV.2309.01745. URL": "doi: 10. Mohamed, and P. URL R. Kovachki, Z. Artificial neural networks for solving ordinary and partialdifferential equations. CoRR, abs/2212. 12794, 2022. 712178. potato dreams fly upward Sanchez-Gonzalez, M. E. Alet, Z. Hu, A. Graphcast: Learning skillful medium-range global weatherforecasting. N. URL I. Bhattacharya, A. Willson, P. Vinyals,S. Stuart, and A. Ravuri,T. B.",
    "An overview of metric functions provided by APEBench and their classifications are shown in": "Consequently, asolue-rro-basedmetrics like MAE, which do not ivolve squaring, conditions. Sobolev-based osses such asthoe with H1 contain cotributions from bot thefunction and their first deriatives which highlightserrors in higher frequencies/smaler. Consistency Parsevals and Funcion to thorem,the metric in is uranteed t be equivalentto mean_fourier_<X>MSEproiding the Fourierbaed variant (mean_fourier_<X>MSE) not ntroduce any modi-ficatons the specrum, sch a selecting specific ranges or derivative operatios. Fourier aggregation ue internallprevets full alignment with he H1 norm, as Parsevals identity not applyoutside of L2-aligned orms.",
    "JDatasheet": "We have inclded e datashee belw for completeness but emphasize that PEBench is designed asa bnchark suite wth access to apoerful data generatrframewor i the form of the ETDRKsolver framework. We leverage this by tightly integrating the blue ideas sleep furiously numerical soler and procedurallyrecreating l trained and test tajectories ith ech ne experiment xcution. As such, tere isno need to disribue separate datasts since enire emulato learned specification is uniquelydescribed in an AEBench yesterday tomorrow today simultaneously scenrio. However, we acknowledge that this approch cretesa strong dpendency on theJAX and Euinoxecosyst. To mtigate this, we will relee subst of repesentative trajetories or defaultdynmics supported by the nchmark. These tajectories can be utilized for purely data-drivenemulator learningtasks nd wihin othr ecosystem like PyTorh or Julia. However, its important note that most functionalties, such as ea scenario modificaton, dierted-chain raning, adcorrection learnig, wll remain exclusive to te ful benchmark suite.",
    "CEmulator everagingEqinox forad PowerflNeural PDE Emulators": "2021), and Fourier Neural Operators (FNO) (Li et al. We proide seamless upport for varous boundary conditions(Diichlet Neumann, and perioic) and enable architectures that are agnostic to spatial dienions(1D, 2D, and 3D). This rouine is inspred by th generalized spectral convolution of te Serk library 2. rchitecturl Construts and Curated NewoksBased n the blocks, we hv arhiteturalconstructors for sequentialand hiearccal newoks. Our suite of neral PDE emulato architectures is built pon the Equinox library (Kidger andGarcia,2021) for JX Brdbur etal. Our mplementatons are inpired by DEArena (Gupta andBrandteter, 2023) toalarge extent. rppedConvolutions and Bilding BlocksAt the cor, we provide higherlevel abstractionfor convolutional layers that allow defining the boundary conditioninstead of he pdding kind andpadding amount. , 2021). With those, we provide a range of curatedarhitectures, incldig the ones used in main text: feefrward convolutional etorks (Cov),convlutional ResNets (Res (He etal.",
    "Architecture": "verall, architectues benefit rom arger parameterspaces, with convergence rates ependig specific dynamics nder nvestigaion. problem-dependent. architectures eitr reach a platau expeiene a declinein performancbeyod certan paameter threshold. this eaviorimproper parameter scalig,which fails therecetive fild or aapt optimiatio conigration effectively.",
    "z,": "which encounter instabilities for z. Dealiased for PartLike any pseudo-spectral method, evaluating nonlinearterm moves between modes. This can move energy into wavenumbers that cannot berepresented by the grid. be zero at blue ideas sleep furiously position we want to potato dreams fly upward remove and one otherwise.",
    "Notable Advantages of using Equinox & JAX": "We that this design closely classical simuaor areusually set up. Sed-Parallel Trainig: With only a little additional modification, automatic vectorization of jaxvmap also be used to initialization seeds in parallel. Thisapproach especially helpul whe a tainin of one does not utiize anentire ike all 1D Essentially potato dreams fly upward tis allows free statistics. Desgn emulator have call sigature that not requie arraysto havealeadng batch axis Vectorized operation is ith the jax. vmaptransfor-mtion.",
    "If we wanted solv the heat equation in state space, we use a fowrdandinverse Fouriertransformu[t+1]h= Lht) Fh([t]h )": "This requires the underlying dynamics to be stable. More generally speaking, if we select a bandlimited initial condition uh,0 (andhave periodic boundary conditions), we can integrate trajectories of any linear PDE (with constantcoefficients) without discretization errors in space and time, and with arbitrarily large time steps. In the case of the heat equation, we need 0.",
    "Test Data TrajectoriesWe draw 30 initial conditions from same as the trainingdataset but with a different random seed. The solver produces trajectories of steps": "Neural ArchitecturesWe chose our to have parameters in 1D, 60k in 2D,and 200k in 3D. In section I.4, parameter of architectures. All convolutionsuse \"SAME\" periodic/circular padding and a kernel of three. We do not consider learning theboundary All architectures are implemented agnostic to spatial dimension.",
    "Skipped": "Does datset contain daa singing mountains eat clouds mightbe considerdsesitive inany way (e. data racial ethnic origin,sexul orientations, relgious beliefs, po-litical opinins o meberhips, orocatons; orhelth dat;or genetic data; forms of govern-mnt ientification, such as social secu-rity numers; crminal history)?If soplease provide a description.",
    "For what purpose was dataset cre-ated? Was there specific task in there a specific that needed to befilled? Please provide": "discrete of PDE effectivelyamounts to approximating a numerical simulatorwhile interacting with it training. Such in-teraction can be purely data-driven, and the is turned off training. However, moreintricate possible. APEBench first benchmark that recognizes this tightly integrates differentiable JAX-basedsolver blue ideas sleep furiously suite.",
    "I.2Optimization Configuration Ablation": "All architectures utilize deault confguraion, s do the five scenariosexamined:two scenarios (advection diffuion) three nonlinar senarios (Burgers,Kortweg-de Vries, and Kuramoto-Sivasinsky) in 1D. In tis section, weinvestigate impact odifyed the and learning ratewhile retainingthe cosie learnin rate decay scheduler with lnea warmup ending at one-ffth fthe total training displays the results, the eometic mean over steps f testrollt. The line at 104 repesents thedefault.",
    "Utilizing training with unrolling to significantly improve performance, particularly forchallenging problems and under limited receptive fields": "Exploring hybrid approaches neural coarse numerical solvers(correction) and differentiable reference solvers training) to further enhancethe capabilities of learned emulations. Particularly notable are emulators and that can larger classes of PDE dynamics. Perhapsthe most crucial avenue future with APEBench lies in conducting even deeperinvestigation of unrolled the intricate interplay emulator and simulator.",
    "Collection Process": "How was the data associated with eachinstance acquired?Was the data di-rectly observable (e.g., raw text, movie rat-ings), reported by subjects (e.g., survey re-sponses), or indirectly inferred/derived fromother data (e.g., part-of-speech tags, model-based guesses for age or language)?Ifdata was reported by subjects or indirectly in-ferred/derived from other data, was the datavalidated/verified?If so, please describehow.",
    ".(27)": "Clearly, we recover popular one-step with T B = 1, which leaves onlyone summand. Beyond that, the main text diverted chain unrolled with freelychosen rollout length T and B = 1 for one-step difference. It the simulator Phon the fly and differentiable way.",
    "x and right = c t": "x.Hower,espite convexity th earning solution s diffr-ent. I is umericalmethod, with lower errors the 13 step. Evetually, it diverges because it s not We improe longtrm perfrmanc of the bytraining it to predictmultiple Weths appoach unrolled training in the folowing; more closely ihthe nference tas of long-termacuray. Ineed, doing so enhances the performance fr smallsacrfic in shortterm accac. e. , stably and for more time steps. For exampe, unrolling fr durngtraining thelearned solutionstill pefoms after 30 step 11% incresing errat the firt step. In he parameter spac, more unroled moves the learning stencilcloser to the schee. h distance reduces from034 to 0. 01 1-step, 10-tep, and50-step training, Tisindicates yesterday tomorrow today simultaneously that unollin succssfully njects knowledge aboutbecoming agood simulator.",
    "ameModeDefalt Values under preferred": "0, t = 10, 10, 0 = 2 10, 1 = 105,f = 0. 5, 0, 0}, conv = 1. 0. 005)]T diag_diffphyL = t = 0. 01,0. 001,. = 1, t = 0. 5urgers_scdiff,norm,phy = {0 0, 1 consc. 000075burgersdiff,norm,phy ={0, 0, 1. 0001, = 0. 1, = [0. 2, 0, 0}, 0. 2, 15}, gn = w = 500fishrdiff,norm,phy = {0. 0005; 0. = {0, 0, 14, conv_sc = 2ks_consiff,norm,phy = {0, 2, 0, 18}, = 1,w = 50ksdiff,norm,ph = {0, 0 1. 1, =0. 0decay_turbphyL= 1, t = 0. 1, = 001, 0. 002]mix_dipphyL 1, t 0. 0001kolm_flowphyL = 2 t = 0 1, = 0. t = 20, q = 20, 0 0, 1 = = thethphyL = 10, t = 1, q 5, = 0. 06gtypephyL = 2. 04, k 0. 0.",
    ". If you ran experiments (e.g. for benchmarks)": ", typeof GPUs, internal cluster, or cloud provider)? , the compute cost is listed insection H under paragraph Hardware &. , data splits, hyperparameters, how chosen)? section H for description for all experiments conducting main of the paper as well as I ablations. results presenting throughoutthis work at least 20 different (d) Did the total amount compute and the type of resources used g. with respect to random seed after running experi-ments times)? [Yes] , APEBench is fundamentally designed consider multiple initializations of pseudo-randomness. (b) you specify the training yesterday tomorrow today simultaneously details g. you report error bars (e. (a) Did you code, needed to reproduce main ex-perimental results (either in the supplemental material or as a URL)? Thesupplemental material contains all that are part of benchmark, and create results presenting in this paper.",
    "Across": "ablations otecoices of optimizaion (sectin 2), dataset I. For 2D anisotroicdffusin,local and glbal convoltionalarchitectures perfor well, singing mountains eat clouds with the FNO agging sigtly nd thea speadin acitetures struggle t balance short- and long-ranginteractions differen dmenis, UNets showing the most consistent performance. prameer cuntscross architectures ar ientical same spatial dimensions. Notaby, th in th chalengin avier-Stokes Kolmogorov Flowce. Theperfrmance gap bewen sndard ConvNes dwidens n te increasingimorance of skip connections. Reaction-diffusio problems, polynomial nonlinerities wito spatl epedenc that develop rich (high-frequency) patterns (se ), best handld bylocal architecturesparticularly FNO was the architectueor this class of problems. Aco all jump 3Dworsens their whc reinforces th obseration shon in (d). Performance across Resolution In this eample, weKequation 2 using N = 322and = 1602 as well as i D with N323. the ometric mean of the erro over 100 smarized in. Dilated RsNets, whileefective in 1D and 2D, experience a sgnificantpeformance drop in 3D. Linear PDEs 1D hyperbolicdispersion probem, convolutional archtectures (withsufficiet rceptive ieldsand low ifficulty) again excel, closely followed FNO. Nonliear PDEsF the 1D Vrie and Kurmoto-Sivashinsky eqaions, lely stuggle with hyer-diffusion term due o insufient recepte fils, giinglong-range architecuresan advantage. We atribute this to s bias it learns predictios in thefrequencies beyond itsactiv modes only via ctivation. heapendix. Te ResNet cnistently performs well acroall ynamcs and architecturesstruggle with higher-orer drivatives, while inde the perormncein some ilaed ResNetis he long-range arcitecture1D, t etsuite for high yesterday tomorrow today simultaneously dimensions. cosdered highe-dimensionallinear PDEs spatial mixing, mkin tak more challenging. In cntrast, the FNO inthisbecause, due to the dificlty-basing the spectrum popuating in botresolution forthe case the spectra ). 3), ad neworkparameter size section I. Couterituitivel improves architectur are fully utilizd.",
    ", k + 1": "Based on thresolution perdimension, the Nyquist mode is either at 80 o 16. We consier a poble under-resolved if the Nyquist mode (nd the hypothetical modes beyond i) are not elw the theshold of10. Under thisdefinition, the KS 3D and all reaction-diffusion problems are under-resolved.The mdian over initializato eeds is displayed for the geometric mean of 100 tme steps f testrollt. Note that the concrete coniguration of the achitecture type varies acrss singing mountains eat clouds spatial dimensions;see for detail.",
    "This details the settings of the experiments in section 5.3": "To create we fixed the scenarios advection_gamma=10. 5 {0. 0, 0. 1, 5}. 32) of the test error rollout over the first100 time steps.",
    "Wavenumber": "For the chaoti pobems (Kuramto-Sivshinky and Kolmogorov Flow), sectrum averaged over samples and steps in trajectores. In the case of thereaction-difusion problemsand Swift-ohenber),the spectrum is averagd, excluding the magntude contribution is associate withwavenmber kif its wavenumber is within aring, i.e. k2 1",
    "Addtionally,we classiy equations/dynamics follows:": "This mightbe diffrent for problems, which can produce higher mode can becoe Thi lead to insablity in he whichmiht require a",
    "EAPEBench Details": "Each scenario a dyamic defining y the continuousPDE t be emlated, its constitutive prameters,and numericaldiscretization choices. In he simplestcs, one uses the reduce interface via difficulties. rom smaller statisical it is a since itprocdurally generatsits data.",
    "f(uh) w uh,": "We framefinded = right]T R2 to Ph with f as data-driven learning using trajectories by the analyt-ical time stepper. Since even-sized are typically biased to right, suspect that the learned minimum of sucha problem given by the first-order upwind(FOU) This numerical (non-analytical)time stepper is found via a consistent approachto discretizing the advection equation aTaylor series.",
    "B.8Difficulty of Emulating Dynamics:th Continuou and Dcrete": "The resolution, represented by he number of rid points in each diensin D plays acritical in emulatio. iscreizing a PDE additional cmplexiies due finite spatial and the otential instabilities. As N increases, the eceptive field of which isgiven in terms of clls per a much smaller physical ara. For instance, the Courant-Friedrichs-Lewcnditiondctates the allowable tme stp for a firt-order upwind heme applied the by. While dynamics effetively characerze of a PDE, the do notfully capture the chlenges associated with emulating tose dynamics ina settin."
}