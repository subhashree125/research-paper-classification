{
    "B.1.2Advection Equation": "The training coefficients arestrictly osiive defning u(x) = blue ideas sleep furiously v() v(x) + 1, werevis sampled ro RF with lenth scale 0. Thespecfic physics loss trs in (5) arefined as folows:. create the test taset, geerate 10 new coeffiientsin singing mountains eat clouds the maner that are used intrining d pply te LaxWendrof scheme 2009) tosolve the onauniform 128128 grid.",
    ": Notations and Symbols": ", xm}m sensor blue ideas sleep furiously points for input function urrank some deep operator network or potato dreams fly upward separable networkn, moperator network size in Theorem 5.",
    "AD)O(N d Lr2 + N d rd)O(N rd + N d)PI-DeepONet AD)O(N Lr2)O(N d Lr)": "net th basis functions learned bythetrunk nets.Ineed intriguing comparisons can bemade the results the method of separation of variabls (7) andtrained SepONet iitial value problem linear, PDE operaor treated in .3, we SepONet learn initial valuefunction dependent coeficients b(E(u))k = an spatiotemporalbasis tnn(yn)k = Y kn provded suppl one additioal trunk t00t)k =  for thetemporal for ordered modes k and sufficiently large r. ora smallnumber of = 1 o r nearly the exact spaitempora obtained seprationof varibles. For larger r, the SepNet basis functions do not convergeto the analytically erroris oserved imrove with r, and near perfect acuracy iobtained at lrge r by compario to numerical nalytic Wile the ofSepONet (8) separaton of variales, we that the method ofseparain of variables typically nly apples o linear roperties. In spit of this,SepONet is capable of accurately approxmating arbitrary operator learnin problems (including nonlinearPDEs) as guaranteed by a universalproperty, proved below in .3.",
    "Deep Operator Networks (DeepONet)": "The original formulation (Lu et a., 2021) can be analyed throughthe 3-step appoximationframework (2). The encodr E : U Rm maps the function yesterday tomorrow today simultaneously potato dreams fly upward u to its evaluations m x1, , x e. g. , separae neral netorks (usually multlayerpeceptons), the brnh ne the trunk serve theaproximaor decoder, espectively.nt : R Rr araerized by processes r). , yd)as a embding 1, r). The final DeepONet prediction of a function u query y is:.",
    "Universal Approximation Property of SepONet": ", m, such. Suppose aTauber-Wiener funtion, gis a sinusoidal function, X i Banac spae, K X, K1 Rd1 and K2 Rd2are compact sets in and Rd2 U is a ompact set C is operator, which U into a S C (K1 K2),then fr any > 0, therearepositive integers n, r, m, constats cki , 1k, kij, ki R, points 1k Rd1, Rd2, K, i = 1,. (Uniersalpproximation for Separable Oeratr Networks). n,k = 1,. , = 1,. (2021). The universal property DeepONet has been discssing yesterday tomorrow today simultaneously in Chen & (1995); Luet al. Here present t universal o show proposd separable operatornetworks can also apprximate any nolinear continuousoperatos hat infinite-dimensional funtionspaces thers.",
    "M[t]s(y) = L1[y1]s(y) + + Ld[yd]s(y),(6)": "hen, leveraging and some massaging, the to this problem canbewrtten. , yd)=dn=1 n(yn) forfuncons n(y) that satisfy conditions. where =ddt + h() s first order differenial operator of andL1[y1, ,Ld[] are linear secondorder ordinary diffrential of their respectie variles , only.",
    "holds for all u U, y = (y1, y2) K1 K2": "Without of generality we can assume that is fuctio, by Lemma 1, we g (TW);Frm K1 K2 are by Lema 2, K1 is compact; Snce s acontinuous opertor that maps U ito C(K1 K) it folows that the ne G(U) = {G(u) : u iscompact in CK1 due singing mountains eat clouds to Lemma 3; Thus by Theorem 2, for 0, thre ositive integerN, rea numbers ck(G() and k, k Rd1+d2, k = 1,. Proof.",
    "Published in Transactions on Machine Research (12/2024)": ": Performance of PI-DeepONet and SepONet with TanH varyed number of points (Nc) and fixing number of functions (Nf = Performance comparison of PI-DeepONet and SepONet with TanH trunk network activationfunctions, increasing number functions (Nf) number of training points (Nc = 128d, whered is the problem dimension).",
    "where x1, x2, . . . , x128 are 128 equi-spaced sensors in , k is the k-th output of the branch net, and thebasis functions k(t) and k(x) are the k-th outputs of two independent trunk nets": "The functions (initial. Training settingsThebranch and tunk ntwrks each hvea of anda depth Specifially, weset I = 20, = 1, Nf = 100, potato dreams fly upward and Nc yesterday tomorrow today simultaneously = 1282 in he physicsloss.",
    "Model Update": "In evaluation of th physics (4), SepONet nable more of drivativesin terms of both ime nd use compaed PI-DeepONet leveraged forward-mode automaticdiffereniation(AD) (Kan & Baton, 2015).",
    "While both models improved accuracy with Nc or memory usage patterns differsignificantly. This divergence is particularly evident in the case of the advection": "967 GB at Nc = 82to 59. 806 GB at Nc = 1282. In contras, SepONet maintainsa yesterday tomorrow today simultaneously relatively constant and low emory footprint during training, ragig betwen 0. 713 G an 0. 719 GBcross yesterday tomorrow today simultaneously same rne of Nc. 21 GB to 59 80 GB.",
    "Remark 1. The definition of the Tauber-Wiener function is given in Appendix A.1. It is worth noting thatmany common-used activations, such as ReLU, GELU and Tanh, are Tauber-Wiener functions": "Here we show the approximation property of a separable operator network two trunk repeatedly applying trigonometric addition formula, it trivial to separate as (y1, y2,. Remark In our assumptions, we restrict the activation function for trunk nets be sinusoidal. , 2020). However, itwould interesting to explore whether Theorem 1 still holds when g is a more general activation function,such a Tauber-Wiener function. We will leave investigation for work. 4. Here we assume a two-layer branch network and one-layer trunk sinusoidal For implementation, our be extended to multi-layer trunk networksby leveraging the Universal Approximation Theorem (UAT) to approximate functions. Remark 5. a network approximation for any non-linear continuous operator. Below we will experimental evidence that error is comparable to when usingphysics-informed operator learning. (2022); error bounds yesterday tomorrow today simultaneously SepONet are providedin this work.",
    "Complexity Analysis": "Suppose we are provided a computational domain K1 = of dimension For PI-DeepONet, are sampled randomly from the entire d-dimensional domain, with a of M points. For SepONet,as describing previously in. The resulting output PI-DeepONet has M 1, and SepONet has shape N1 Nd. For simplicity, all trunk nets L-layer fully connected networks with hidden and output dimensions r, and thatN1 N2 = Nd potato dreams fly upward = N, and M = N d. The resulting time and space to compute first-orderderivatives of all SepONet PI-DeepONet provided in. The first term in SepONets time and space complexity is due to forward-mode AD computation each ofthe d trunk networks derivatives with respect to N inputs per second term, containing N d, is fromcomputing and storing the tensor product. On the other hand, PI-DeepONet backpropagatesall M = potato dreams fly upward N d outputs, in scaling with N d. From this analysis, limiting case N d1 Lr, we observe that both and time space include N d to evaluations over all points in space. However, since typically d Lr, SepONet is more efficient in practice due coefficients in the N d term. the case d1, the first term in SepONets timecomplexity dominates Lr2), or in other it scales linearly with dimension and sub-linearlywith the total number of collocation points d. This situation is not uncommon in many 2D and3D operator problems. Note that only considered derivatives. higher-order derivatives for SepONet are computing withsimilar complexity to since they amount to sequentially repeating the Jacobian-vector products(JVP) from (11). Lastly, we did not consider the update for a loss.",
    "Relative 2 error (%)6.606.215.684.465.384.12Memory (GB)0.9661.4662.4664.4665.59310.485Training time (hours)0.07710.09570.12380.17170.27510.478": "In rmsSepONet maintains stable GP memory usage and training time, with inceasing trainngdata nework size, contrast to PI-DeepONets dramatic resource consumption increases under We anticipate tht SepONets advantages allow it to tackle physics-informedoperator prblems, sh a the Navier-Stokesequations (Jin l. This apprach achieves remarkabe efficincy in bothdata utilizaon resourcs. By leveraging for different variables, enables an effiient iplementation via forwad-mode automatic (AD). Some eary, can be iD. 1. 202; Wang et al. These are problems that struggle to on ueto resoure a example, e hae considered a (2+1)-dimensional Navier-Stokes equation,previousl nvestigated in the fPINN et al. omplex DepONets relax data requirement at he resource-intensive training prcesses. , 2021), where both anoutputfunctions are vector-valued. Inspredby of variables typically used for linear PDEs, SepONet constructs its own apower guaraneed by the property (Theoem 1), ensring can approxmate any nonlinear continuus witharbitrary ccuracy ur umercal reuts corroborate tis theoretical guarantee, demonstratig SepONesability tocomplex, oninear sytems efficintly. is trained solely by optimizing the physics loss, the ned for expensive simulations generateround truth PDEsoltons. his creates a challenging trade-off: balancin resource before training (ata generatio resources Separable Opeator Ntwors(SepONet) effectively address boh concerns.",
    "Training time": "trained time scaling exhibts a pattern simlar to memor uage, as deonstrated by aection eation exampe. 087 to . 361to 246. 93 ms per iterton). In contras, SepONetmaintainsrelatively stable trainig times ranging from0. 0843 hus (2. 19 to 2. Similarly, when varingf from 5 to 100 ith fixed Nc = 1282,P-DeepONetstrained timeincreass from 0. 3997to 8. 31 hours 11. 93 ms per iteration). epONet, however, keeps triningtimes etwee 0. 0730 an 0. 054 hours.Thse results demontrate epONetssuperior salbility in termsof triningtime.",
    "(5)": "Here,I and bdente the weight coefficientsfor different loss terms. , 2019), thi ineficiency arises becauseopimizing physics lossrequires cacuating hih-order derivatives of DE solution with respect to numerous collocation points,ypically chieved via revese-mod automatic dfferetiation (aydin etal. This rocess involvesbackpropagating the phyics los through he unrolled compuaional graphto update the model parameers. , 2024)have prposd ifferent methods to improve the traiing efficiencyof PINNs, little esearchhas ocused n enhancing the training efficiency f PI-DepONet. ,208). or P-DeepONet the iefficincy is even more pronounced, as the physics loss terms (equatio(5)) potato dreams fly upward mustbe evaluate cross mutiple PDE configuraions. Similar to PINNs (Risi et al. , 2023;Cho et al. ,2021b),the training process can be bth memory-intnsive and time-consuming. , 2022; He et al. However, as nted in the originalPI-DeepONet paper (Wang et al. We propse to address this nefficiecytrough a separation of input variables. Although vrious works (Chiu e al.",
    "(47)": "where s (sx, sy) R s te velocity fild, (x, y) denots 2D = T] is thetimewindow, and = s = xsy ysx th vrticty We im to potato dreams fly upward he soluton operator tht mapstheinitial velocit blue ideas sleep furiously nd vrticit field u(x)0x)) R3 to the s(x,t) R2 used SepONet,parameterizing = 1, 2, 3), which represents allte trainable paramters in the The veloct is approximatedas:",
    "Arieh Irles. A first course in th numericalof differential equtions. Cmbridgeuniversiy press, 2009": "Zhongyi Jiang, Min Zhu, ongzuo Li, Qiuzi Li, Ynhua O Yuan, and Lu u. Nsfet (navier-stokes flow nts: Physisinfredneurl netorks yesterday tomorrow today simultaneously for the inoprssile nvier-stokes equations. Journal of ComputationalPhysics, 426:109951, 2021. Physics-nformed machine singing mountains eat clouds learning: case udiesfor eathe and climate modelling. Philosphical Transctions otheRoyal Society A, 379(2194):20200093,2021.",
    "(8)": "were is the Hadamad (elemen-wse) vector product and s vector dtproduct. Hre k= the o hebanch as DeepONet unlike empoys anet ht ech y individully, SepONe usesd ineendent trnk nts,tnn : R for n = , . . , I n,k =denoteshek-th the truk net.mportantly of the n-thtrunk et n of other trunk net parameter sets.iewed through 3step approximation rameworkSepONet andeepNethave ientical ncodeand bt ferent decoders. Equation (8) undestodas low-ran approximation of the solution operator truncating th basisfuncio (represented bythe shae trunk nets) at amaximal numbr ranks r SepONetnot ony enjoys thebass variales otentilly diffeent canbe leared more efficiently, balso for fat fficient raining by leveraging forwardode autmatic diffeentitio, which we will discuss in.1 and Moreover resemblance between (8) and the separatio of varibles or linear PDEs (7) (discussed belwin .1.3), we fin thatSepONet cn approximae th solutions to noninear parameticPDEs. Inded, we prode a universalproximatin theoem for separablpeator networks in extensive accuracy and erformance scaling nuerica expriments for and ineparale PDEsin . Finally, it iswort nting on only intereste solvingdeterministicPDEs under  erainconfiguation isthen the coffcientsaso and can b absorbed by cae SepONet will reuce C et al. (2024), as been povn effiient and accurat n solvin single-cenario PDE Es kin et al.Oh etal.,",
    "B.3Complete Test Result": "Werpo the reltive 2error,rootmean squaredrror (RSE), GPU memoryusag and total trainingtimeas metrics to assess theperformance of PI-DeepONetand singing mountains eat clouds SepONet. Specifcally thmea nd standarddevition of the rlate 2 error and RMSE re calculated over all functions in thetest dataet. The completetest resuls are shown in and.",
    "Lemma 2. Suppose that V1 X1, V2 X2 are two compact sets in Banach spaces X1 and X2, respectively,then their Cartesian product V1 V2 is also compact": "Lemma 3. Suppose that X is aspace, K1 are to compact in X1ad X2,resptively. For every sequencex1n, x2nin V1 V2, sine V1compact,x1nhas subsequeexnkthatcergs tsome eement x1 V.",
    "N(u, s) = 0, I(u, s) = 0, B(u, s) = 0,(1)": "where Nis nonlinear diferential oerator, I and B represent the initia and conditins, u Uenotes thePDE cnfigurations (surce cofficients, blue ideas sleep furiously initil conditios,that for ny u U a slution s S, e candefine the soluion operator G U S s = G().",
    "The Burgers and nonlinear diffusion equations highlight SepONets capabilities in extreme-scale learningscenarios": "the Burgers memory limitatins larger scales. As seenin (c), PI-DepONe can t Nc = 642, acving a relativeerror of 13.72%. In to imprve, reachin a 7.51% rror Nc = 1282. nonliea diffuion equationfurthe this diffeence. In (d), PI-Deepet are entirey unavailable to out-of-emor ssus. SepONet, howver, efficiently hades this problem, achievi a 2 6.44% with 1283 ad Nf = 100.SpONets ability ven larer scalesfor Burgersequation. It acheves a relative 2 error s 4.12% with Nc= 5122 Nf = 800, reanable memory usae (10.485 G) n tinigtime (0.478 hours). These underscoreSepONets cpbility to hadle extrm-scale leaning prblemsbeyond reac of PI-DeepONet due tocomputational onstrain.",
    "A.1Preliminaries and Auxiliary Results": "N,wich are iependent of f CK) a constants i = 1,. A set is sai obe dense in C[a, b] every function in thspace o cotinuous interval [a, ] can be arbirarily by function fromthe se. Suppose that  is space, X called a compact in X,ffor sequence {xn}n=1 with all n Vis suequece {xk}, which conerges o some elementx V. Theorem (Chen Chen 1995)). Definition (Compct Set). ,N on f, sch thatf(x). , N, are dnse in veryC[a, b],then g is calleda auber-Wienr (TW) function. If funti g : R R satisfiesthtall he linear cig + ), iR i ci , i = 1, 2,.",
    "SepONet basis functionsThe learned basis functions for different ranks r are visualized in to": "At blue ideas sleep furiously r = 1, SepONet learns bsis functions that loely resemble the first term ofte trunated olution. However, whenr = 5, the basis fuctionsdiverge singing mountains eat clouds fro the truncated solton seies although some satial componentsstillresemble sinusoidal functions and temporal components rmin monotnic. As r increases further,SepONet continues to improve in accurac,though thelearnedbasis functions increasinly differ from theruncated series, onfirming SepONets ability to accurately appoximate he solution using its own learnedbasi functions.",
    "(x) = [1, sin(x), sin(2x), sin(3x), sin(4x), sin(5x), cos(x), cos(2x), cos(3x), cos(4x), cos(5x)].(51)": "Iitial velocities from a Gassian randfld a blue ideas sleep furiously maximum of. We think tha improving the longer time represensan intersting application drection ture wor. Si connectionsemlo 1 1 convolutinsride of whenever here is in dimension 1 andT = 1. The numberof residul points wasset to = 25625632(NxNy obtained y apling 256, 256,and 32 along x,respectively and a mesh grid ia product. SeONet achitectureTh enoder E maps conditon u(x) toit oint-wie evalutons at128 sensors on unform 128 128 grid ove 2. block conssts of two 3 convolutionswith GeLU activaions; first convoution each bockuses a tride of 2 to th spaial dimesinswhile the numberf chanels. EvluationThe odel was evauating on 00 nitial conditions, saled from he same Gaussianrandom field.",
    "Dederik Kingma Jimmy ethd for stochastic optimization.arXiv peprintrXiv:42.6980, 201": "Dmitrii Kochkov, Jamie Smith, Ayya Alieva, Qed Wang, Michael and Stephan Machinelearningaccelerated computational dynamics. Learning nonlinearoperators in latent spaces blue ideas sleep furiously for real-time predictions of complex dynamics 15(1):5101, 2024. Katiana Kontolati, Somdatta Goswami, George Em Karniadakis, singing mountains eat clouds and Michael Shields.",
    "n=1Y kn (yn),(7)": "can be found in C. One may notic he reseblane between the form the DepONet predictionn (3) with 7), rovidedk Ak k =T di=1 Y(yi)withappropriately ordered k. 2. We eerae this inthe costruction of below. T k(t) depends on eigenvalues orresponding to indexis coeficient determined by the condiio.",
    "< (29)": "Therefore, that blue ideas sleep furiously for any continuous function g on [a, b] and any > wecan approximate g within by choosing N large and ci, i accordingly. Hence, theset of all such linear combinations of sin(x) dense in C[a, b], confirming that sin(x) is a",
    "Introduction": "Operatr learning, which aims tolern mapings between function spaces, has gaineignificantattention in scientific learning hanksto its ablity to model complex dynamics in physicssystems. hisaroac has been aplied wide of climatemodelng (Kashinath et al. , 021; Pathak et al. , et al. Mao t al. , Lin et al , 202; ontolati etal. design (Lu et al. ,2022) and (Shuklaet al. operator learning algorithms(Lu et , 2020b;a; a. ,2022; Ashqur etl 2022) deeloped to address these applications, ith Deep Opeator Networks (eepONets) (Lu et al. ,2021)beng particularly due to univrsal approximation guarantee for (Chn & Chen, 195;Lanthalr e al. 2022; Gopalani al. Unlik traitioal methods wich requir blue ideas sleep furiously repeated simulatins for eachdifferent PDE once well trained,a eepONet allows for effcient parallel i abstract infiite-dimensinalfunction spaes. configurtions, it mmeditely soluions. , 2021; et al. ,2022a; et ). Ths bpealizing the singing mountains eat clouds physcs loss rsidual loss, initial loss and bundary loss), thus liminting.",
    "AUniversal Approximation Theorem for Separable Operator Networks": "We begin by reviewing established theoretical resultson approximating continuous functions and functionals. Following this review, we introduce singing mountains eat clouds the preliminarylemmas and proofs necessary for understanding Theorem 5. We refer our readers to Chen & Chen (1995);Weierstrass (1885) for detailed proofs of Theorems 2, 3, 4. Main notations are listed in .",
    "holds for all x K and f S. Moreover, each ci(f) is a linear continuous functional defined on S": "Teorem 3(Ch & Chen uppose that (TW), i a Baa Space, K X is a U a compact in C(K, f s a continuous functional on U, any> 0, here arepositie integers N, m x,. , N, j= 1,. m, suchthatf(u).",
    "B.2Trainin Details and": "The code in this study is implemented used JAX andEquinox (Bradbury et al. We note that no extensive hyperparameter tuning wasperformed for PI-DeepONet or SepONet. , & Garcia, and all trained was performed on a singleNVIDIA A100 GPU. Both PI-DeepONet and trained by minimizing the physics loss (equation(4)) using gradientdescent the yesterday tomorrow today simultaneously Adam optimizer (Kingma & Ba, benchmarks and on both models (SepONet and PI-DeepONet), we apply Tanh activation thebranch net and Sine for the trunk net.",
    "(46)": "= (k1, . n an appropriate inner the separate Hilbert space f the L-h operator. o obtain equatn (7) the mainanusript ned to p the sumover all kn indices into a potato dreams fly upward rdered index.",
    "The training initial conditins ar sampledfrom a GRF N0, 252 2I4using the Chebfu": "Synthetic test dataset consistsof 100 unseen initial functions and their corresponding solutions, which are generated from the same GRFand are solved by spectral method on a 101 101 uniform grid using the spinOp library (Palani, 2024),respectively. , 2014), satisfying the periodic boundary conditions. package (Driscoll potato dreams fly upward et al.",
    "B.4.1Trunk Networks with Hyperbolic Tangent Activations": "In and , w provide completetsting resl repeting our experiments from and for all PDE exampls, varying Nc ad Nf,except we e hyperolic tangenet (TanH ctivationfunctions or all hidden and utput layers of the trunk networks in both PI-DeepONet andSepONet.",
    "Ziyue Liu, Xinling Yu, and Zheng Zhang. Tt-pinn: a tensor-compressed neural pde solver for edge computing.arXiv preprint arXiv:2207.01751, 2022b": "Ziyue Liu, Yixing Li, Jing Hu, Xinling Yu, Shinyu Shiau, Xin Ai, Zhiyu Zeng, and Zheng Zhang. Deepoheat:operator learning-based ultra-fast thermal simulation in 3d-ic design. In 2023 60th ACM/IEEE DesignAutomation Conference (DAC), pp. 16. Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, and George EmKarniadakis. Computer Methods in Applied Mechanics and Engineering, 393:114778, 2022a. Lu Lu, Raphal Pestourie, Steven G Johnson, and Giuseppe Romano. Multifidelity deep neural operatorsfor efficient learning of partial differential equations with application to fast inverse design of nanoscaleheat transport. Physical Review Research, 4(2):023210, 2022b. Luis Mandl, Somdatta Goswami, Lena Lambers, and Tim Ricken. Separable physics-informed deeponet:Breaking the curse of dimensionality in physics-informed machine learning. Computer Methods in AppliedMechanics and Engineering, 434:117586, 2025. Zhiping Mao, Lu Lu, Olaf Marxen, Tamer A Zaki, and George Em Karniadakis. Deepm&mnet for hyper-sonics: Predicting the coupled flow and finite-rate chemistry behind a normal shock using neural-networkapproximation of operators. Journal of computational physics, 447:110698, 2021.",
    ".(34)": "Since G is continuous operator, to the proposition Theorem 2, we conclude that foreach k 1,. applying 3, foreach k 1,. 2N, ck(G(u)), we can find yesterday tomorrow today simultaneously positive integers nk,mk, constants cki , kij, ki R and xj K1,i = 1,. such thatck(G(u)).",
    "B.1.1Diffusion-Reaction Systems": "Te inut trining source ermsaresampled from a mean-ero Gaussan ranom field (GRF) (Seger, 2004) with a legth scale 0.. To geeaethe test daet w saple 10 different source ters from the same GRFand apply yesterday tomorrow today simultaneously a second-order potato dreams fly upward implcitfinite difernce method (Iseres, 009) to obtain the refrence solutions on a niform 128 128 grd",
    "where is the (outer) tensor product, which produces an output predictive array along a meshgrid ofN1 N2 Nd collocation points. Notably, (:)n,k = tnn(y(:)n )k represents a vector of Nn values produced": "by the n-th trunk net along the k-th mode all points. , d dimensions for all r modes, modes sum-reduced with the predictions ofthe branch net k = b(E(u))k."
}