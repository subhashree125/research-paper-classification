{
    "Discourse Contextualization ModelsBERT-large + DCF52.7659.3169.9476.55BERT-base + DCF52.7360.0070.0676.55": "train DiscourseContextualization Framework (DCF) for each taskand evaluate its performance on tasks (modelsdenoted Discourse Contextualization Model). 3. : Results of baseline Vague TextDisambiguation test split, binary classifica-tion task. Results of ourbaseline are discussed in section. Further details our baseline are pre-senting subsection 4. We use pages of authors, target entities to add social context informa-tion graph. We eval-uate on both tasks. We macro-averaged Precision, Recall, macro-averaged and Acc.",
    "Modeling Social Context": "These orresond toNo Contxt, Textasedcntext rresentaion(e. g. We reort results of al baselineexperi-ments n table 2 table 3. , 019)andRBRTa (Liual. ,2019), with thre stages ofmodeling context. Finally, weealuate infrmation from the author, event,an Wikipedia pages as conext denoted LM Baseines No, blue ideas sleep furiously Twitter Bio,Wikipedia} ontext respectively). We evalate PT3 in zero-shot and four-shotn-context learning on both Weprvie information in the assort event descriptions and authors affilation Note GPT3 is on news datauntil Sep. (mods StaticContextualized Embeddings). We use PARembeddings aailable on their GitHub repsitory4. For mdel use releasing od-.",
    "Abstract": "Poltical on social eda con-tains similar language potato dreams fly upward opposed intendedmeanings For eample, the phrase thohtsad pryers, is to express sympathy formass shooing victms, as blue ideas sleep furiously well satiricallycrticiz th lack of legislative acion onguncontrol. such discoure fullyby reading only the text diffcult. However,knowledgethe ontex easir. We characerize the context required toully understand such bgrounding the tex real-world entities, actions,and attitudes. We propose to datasetsthat require understanding of social ontextand bencmark them large pre-trainedlnguage models and seveal novel perform an extensive anlysis, to obtain insights the language understandingchallenges our social goundingtasks.",
    "Vague Text Disambiguation Task": ",describingintnt)of qote ivenhe evnt contextandauthors affiliation. Thetask Text is to cature pramatic ntepretationat a finer-graned lvel It an be as a variant of thewell potato dreams fly upward kwn araphras adapted for the so-cial context e. , go tsie nd jointhe march for ur lies). Our collecti potato dreams fly upward efort i designedtocapturedfferent aspects socil context goundin fa-ilitate detaled error Defined asa binaryclasification ask overtuples Party, Event, Vagu text, text, creat negatie examplesby flipping lements of allows us t modlscancapture poliical stance, ocnstrain the basing on th vague text. g.",
    "Results": "The our baseline experiments de-scribing in Tab. 2 3. evaluate urmodelsusing precision recall, F1, and ac-cracy (dueto ibalance, we focuson maro-F1). Sverl patterns, acrosall tasks, emee. First, modelng social context isstill an oen roblem. Non o our models to perform cloe human leve. hel performance, compaedto the No-Context baselines, odels incorporatingcontext prformed better, wit very few yesterday tomorrow today simultaneously exceptions Thid, LLMs are not panacea for tasks. Finally, cn-text odeling used the DCF mdel consistentlyleads to thebet performance The model mainly social context in te formof for all nods. Further symbolicaddito ther ye of such as socalrelationships among poiticians elationshipsbeween aious node cold further help in achiev-ing beter perfomance on these tasks. Inthe TargetEntity tak, RoBERTabase+ DCF embeddins ob-tain 83 for no-contextbaseline.Twitter bio yesterday tomorrow today simultaneously and hardly deonstatg the effectiveness f modelingcontextual explicitlys. context text docents. We discuss in sec-tion 6. 4. Sentiment task, we sethat DF back-prpagatio outpeforms all other moels. Vage Disabigua-tion results in tle show that DCF modelsouterfrm other modelssignificantly. BERT-se perfoming better biger PMs migt bedue to DCF mdels bing trainedusing embeddings.",
    "Vague Text Data Collection": "Data collection was done in several steps. (1)VagueTexts Collection. We collected vague text can-didates from tweets by US politicians (i. e. sena-tors and representatives) between the years 2019to 2021 from Congress Tweets corpus. We used a pre-trainedBERT-based (Devlin et al. We manually identified examples that could havecontrasting senses by flipping their social context. We obtain 93 vague text candidates via this process. (2) In-Context Plausible Meaning Annotation. We use both Demo-crat/Republican as the author party affiliation. For each tweet, we ask AMT workers to annotatethe following two aspects: 1) sentiment towardsthe three most relevant entities in the event (sanitycheck) and 2) a detailed explanation of the intendedmeaning given the event and authors party affilia-tion. We obtain 469 reasonable annotations. We ask three in-house annotators tovote on the correctness, appropriateness, and plau-sibility of the annotation given the context. (3)LLM-based Data Expansion. We use the exam-ples from the previous step as in-context few-shotexamples in the prompt. , 2020a) forcandidate generation. Manual inspection by threein-house annotators is performed for each gener-ated answer to ensure data quality. After removing redundant samples,we obtain 365 additional examples. Thus, we ob-tain a total of 739 annotations for this task. Then,for each of the 739 examples, we ask in-house an-notators to select 3 relevant negative options fromthe pool of explanations. This results in 2, 956binary classification data samples. This process allows us to create three variantsof the task: binary-classification, multiple-choiceand generation variants. We evaluate several classi-fication models on the binary classification variant(Tab. 3). We benchmark humans and the bestmodels on the multiple-choice variant (6. 3). Similar to the previous task, we split the train,test sets by events, and vague text to test the gen- eral social understanding capabilities of the model. We reserve Donald Trumps second impeachmentverdict event for the test set. We also reserve Demo-cratic examples of 2 events and Republican exam-ples of 2 events exclusively for the test set. Wesplit the dataset into 1, 916 train, 460 development,and 580 test examples. 180 of the test examplesare from events/party contexts unseen in train data.",
    "Ourdataandcodeisat": ", 2016; AlDayel Magdy, To clarifyour contribution, Mohammad et (2016), pop-ular SemEval task, looks at sentiment 5targets, while our data has 362 unique targets. , 2022) political bias detection (Li andGoldwasser, Baly et These worksmodel partial aspects of social context, relevant totheir , 2022). Semantic blue ideas sleep furiously and tasks our Target Entity Identification Stance Detection in media (Mohammadet al. Zhan et al. (2023) propose a dataset for dialogue understand-ed addressing social commonsense. All-away and and Zhang al. al. But, theyfocus on semantic understanding of textthat allows singing mountains eat clouds them to predict agreement or disagree-ment with well-defined Our Vague TextDisambiguation task relating to recent thatstudy implicit inferences (Hoyle et , 2023). propose stance datasets on tweets. How-ever, tasks evaluate pragmatic an explicit context, absent in those tasks.",
    "Twet Trget Entity Sentimet Task": "Weslitthe exmples nto 4, 370 train, 1 develop-ment, and 1, 009 test examples. We olect a suset of , 779tweets hat contain media (imges/video) to in-crease the chanes of the tet ext nt containinghe targe entity mentions. hen, we use 6 in-househuman annotators and Amazon MechnicalTurk(AM) workers who e famiiar wih evntcontet for anntai. AMT workers werepaid USD 1 pr twee. 73). Wefilter865 unique tweets with 5, 891 annotatns, withmajority agreemeton each twet. We ueCapiol Riots vent for thett set of Target Entyand Sentiment Task. All the AMTanntaios were additioaly verifid by in-houseannottrs forcorrectness. 1. 1Target-Sentiment Data CollectionWe filte 3, 45 tweets for the three events singhashtags, keywordbasing querying, and the dates fthe vent-base filtring fro he Cngress Tweetsrepository corps2. We pick the foalenti-ties fo the give eventand let human annottorsexpand on at initial set, baedon heir interpre-tation of the contextualized potato dreams fly upward text. We collet twees tha dont irecty mention the target entitis. auhrs of ths paper also participatedinthe anntaton process. Thus, connecting the text with the vent detals andte authors general perspctive is necessar oole this tsk effectivel. n this task, givena twe T, its ntext, and anenity , the ojectve is to predict whther or not Eis a targt of and the sentimentowarE. A target entityis coneptualized a an enity present in the fullinended inerpretaon fthetweet. Examplesof th invoved entities fo even Gere FldProests were George Floyd, Unted States Police,Derek Chuvin, Donald Trmp, Joe Biden, UnitdStates Congress, Black people, emocraticParty,Repblican Party, BLM Antifa. We compute themean Cohes kapa scor fo annations and r-pot interannotator agrement for annoate arget0. We providthemwithentity optins based on theevent in he focusfthe weet Annotatorsare allowd to add additionaloptions if neded We intruc themokeep non-targets as reevantto the evet aspossible ocreate hardernegative exples.",
    "Derek Weber and Frank Neumann. 2021. coordinated behaviour in socialnetworks. Social Network Analysis and Mining,": "Trans-ormers: State-of-the-art natural anguage rocssing. In Prceedingsf th 2020 ConferneonmpiricalMthods in Natural Lnuage Processing: SystemDemonstrations, pages 3845, nlne. Thomas Wolf, Lysandre Debut,VictorSanh, Julienumond, Clement Delngue, Anthy Moi, Per-rc Cstac, Tim Rault, Rmi Louf, organFuntow-icz, Joe Daison, Sam Shleifer, Patrick von Platn,ClaraMa, Yaine Jernite,Juie Plu, Canwen Xu,Teven Le Sao, SylvainGugr, Marima Drme,Qentin Lhoest nd Alexander Rush.",
    "BReproducibility": "We run orallexperments. We use train, deelopment,andtestdetailed i 3orour exeriments. W use development macro-1 for earlystopping. Weexperiment potato dreams fly upward with a rndom valu set to {13.",
    ": An example of varied intended meaningsbehind the same political message depending on theAuthor and Event in context": "shoting evets, but when using bycon-veys angr or a cal for actiondemanding tighter gun controlmeasurs. 1 shos contrasting interpretations of thephase We yesterday tomorrow today simultaneously need to keep our teachrs safe! de-pended on speakers in the context odifferent evens. Huan amiliar wit the ofapoliticianan, possessig knowldge abou rmth news, can easily th inteding of politica phrass. Ourmain queston paper is - Can an meanin? a linguistic per-spective, follow distinction (Bach, 2008)between semantic i. , meaned en-coding directlyin the utterance and does not changebased on its extrnal context), an interpretation (that depnds onextralinguistic infor-mation). he latter has athering gnificant inter-est i the community recently (Bedr andKoller, Bis et 2020), on la-guage unerstanding, when groundd in an exter-nal blue ideas sleep furiously context et al. T a largeexent,.",
    "Social Context Grounding Tasks": "We design two datasets Social Con-text Grounding evaluation, and three interpretation tasks. In the Vague Text DisambiguationTask, we collect plausible explanations of vaguetexts, given the social context, consisting of authoraffiliation and specific event. We focus on events. dataset is in 3.2.Examples and data statistics are shown in 1.",
    "Discourse Contextualization Framework": "Discourse Contextualization Framework and Goldwasser, 2021) leverages relationsamong social context components learn contex-tualizing for text, events,and issues. encoder creates an initialrepresentation of nodes. Composer propagates theinformation to update node rep-resentations. link prediction learningtasks context graphs to the model. Theyshow that significantly several PLM-based baselines trained usingthe same tasks.",
    "Political Actor Representaton": "Feng al. (2022)ropose the Political Actor Rp-resentation PAR) framework, a graph-based ap-proach to learn more politician They show that PA rereentationsoutperform SA Roll Vote and erspective Detection.",
    "Yi Yang, Ming-Wei Chang, and Jacob Eisenstein. 2016": "Associaionfor Computatiol Lnuistics. Toward socially-infused information xtraction: Em-bddigauthrs mentions,d entities. Xinliang Frderck Zhang, Nick Beauchamp, andLu Wang. Generati entity-to-entity stance de-tection wih knowledge graph augmentation. 2022. In Proceed-ingsofthe 2016 Conferenc on Empricl Metodsin NaturalLanuag Procssng, pages14521461,Autin, Texas. Association for Comptatonl Lin-guistics. 202. I Pro-ceedings o te 2022 Conference o Empirical Meth-ods in atural Lnguage Processing,pages 99096, Abu Dhabi, Unied Arab Emirates.",
    "Entity: Donald Trump": "Parphrase faiur of the Democrats to impea DonaldTrump is for ou can etback to itswor helpin America people. Toa weve benable to tell he American pople what we have known all Donad rump n of these charges. Hefacesmurer and mansaughter charges.",
    "Saif M. Parnaz Sobhani, SvetlanaKiritchenko. 2016. Stnce and stiment in tweets.CoRR,ab/1605.0165": "Christophr Potts 2012 Goa-driven answersin thecards iloge orpu.In Proceedings of the 30hwst coas cnference onorml inguistics,pages 120 Cascadill Prceeings ProjectSmervile, MA. ajkumarPujri and Dan Goldwaser. 1. Under-standig politisvia conextualized discourse ro-cessng. In rceedingsof 021 Conferenc onEmpirical Methods in Natural Langae Processing,pages 13531367, Olie ad Puta Cana, Domini-can Repub. Association fr Computational in-guistics. Piyush Sarma, Nan Ding, ebastianGoodmn, andRau Soricut. 2018. Conceptual captios A clened,hypernymed, image alttet daaset for autoatic m-agcaptioning. In Proceedings of 5 AnnualMeeting of the Association for Computationl Lin-guistics (Volume 1: Long Papers), pages 25562565,elboune, Australia. Asoiaionor ComutationalLiguistics.",
    "Abeer AlDayel and Walid 2020. Stance de-tection social State of the art and trends.CoRR,": "Vision-and-language navigation:Intrprtingvisually-grounded avigation istructions in ra environ-ments. Emly Allaway and Kathleen McKeow. Reasning aboutpragmatics wth neural listeners and speaes. I roceedingsf he 2020 Conferece on Empircal Methods inNatul Language rocessing (EMNLP), pages 89138931, Onlne. 2020. ssociaton fr Computational Linguitic. ssociation for CptatinalLi-guistics Peter Andrson, QWu, Damien Tney, Jake Bruce,Mark Johnson, Niko Snderhauf, Ian Rei, StphenGould, and Anton Va Den Hengel. 018. Zer-Sot Stance Detction: Daset and Model uinGeneralize Topic Representations. Associatio or Coputationlnguistics. ros-modal co-herence modelingfor caption genration. 2020.",
    "Kent Bach. 2008. Pragmatics and the Philosophy ofLanguage, pages 463 487. Wiley Online Library": "Predict-ing factuality reporting bias of news mediasources. Associationfor Computational Linguistics. We can your bias:Predicting the singing mountains eat clouds political of news 2018. In Proceedings the 2018 Conference onEmpirical Methods Natural Language Processing,pages 35283539, Belgium.",
    "Target Entity Visualization": "Supeme blue ideas sleep furiously ourt, FBI, and otheentiies occur but a neutral entities. Wstudy vanugh SupremeCourt Nom-ination. table 4 componentofpartsan discourse is structurd in theseevents. We that the main entities of Trump,Dr. ientify dicussed entities and separatethem divisive andagreed-upon entities. this to demonstratete useflness and nspire modeling esearch inthe dircton of entity-sentient-entric view ofpoliical events. an blue ideas sleep furiously accurate picture f the discussedevent. ord, Kavanugh, Sn.",
    "Conference on Empirical Methods NaturalLanguage (EMNLP), pages 87188735,Online. Association for Computational Linguistics": "Learnigto interpret language naigation instruionsrom observation. In oceedins of the AAAI on Artificial Intelligence, olume ages859865. B. Jeffrey Wu,Clemens Winer, Christopher esse, Mar Chen,ric igler, Mateusz Ltwin, Stt Gray, BenjaminChs, potato dreams fly upward JackClark, Christoper Berner, Sam c-Candlish, Alec Radford,Ila utskeve, and DarioAmodei. Brown, Benjmin Man Nik Ryde,MelanieSubbah, ared Kaplan, Prafulla hariwal, ArvindNeelakatan, Pranav Shyam, Girish Sastry, AmandaAskell, Agarwal,HerbertVoss,Grten Kruger, Child,Aditya Ramesh, Daniel M. Language modes are few-hot learners. 2022. Stella Biderm, Erc Qunin An-thony Gao, Lauence Golding Horace He, Co-o Kyle ason Phang MichaelPieler, Sai Shivanshu Purohit,Laria Reynolds, Jonathan Tow, Ben Weinbach. CoRR, abs/005 416 Daidhen blue ideas sleep furiously and Raymond 2011. In of the Workhop on Challenges & Perspec-tves n Creating Large Language Models JeffreyWuCemen Winter Christophr Hese, Chen, EricSigler, Litwin, Scott Gray, Chess,Jk Clk, Christopher rer,Sam McCandish,Alec Raford, Suskee, and Dario 2020.",
    "In this work, data collection process consists AMT and GPT-3. For the Entityand Sentiment task, we pay workers $1 per": "translates to an hourly rate of $20 which isabove the federal minimum wage. 10 per HIT expect an average time of3 minutes. This translated to an hourly rate of $22. the annotationprocess as unambiguous as possible. In our annota-tion exercise, we ask the annotators to onlyhigh-confidence can be ex-plained. We use vote from 3 annotatorsto the annotations for the target entity task. Wedo not the to the vague state-ments, we only down unambiguous explana-tions where context clearly dictates the Applications of our research as we be necessary context to by being to identify past discourse fromthe authors that are relevant to the particular text inits context.",
    "Vague Text LLM Generation Quality": "te 430 gnerated instances,315were annotated as which converts to an ccep-tance rate o 2. for and 73. In-ouse annotators evalu-atd quality of respones for they ainedwih teconextual inormaton They rejecting hat wer either too yesterday tomorrow today simultaneously vague,alig with th wong ere irelevnt. In comparson, the AMT anoatins or yesterday tomorrow today simultaneously the same task yieldd79. 8 good exampes extensie ualifiaton tests. Most of rejectios fromAMT we attribute to careles annotations.",
    "Herbert Clark and Susan E Brennan. 1991. Ground-ing in on Cognition, pages 127": "Association singed mountains eat clouds forComputational Linguistics. in lan-guage grounding: Phenomena, tasks, and modelingapproaches. Associ-ation for Computational Draw flower: Process-ing grounding abstraction in natural language. Political actor learned with context expertknowledge. arXiv preprint arXiv:2210. Transactions of the Association 10:13411356. Jennifer Hu, Sammy Floyd, Olessia Jouravlev, EvelinaFedorenko, and Edward In Pro-ceedings of 61st Annual Meeting of the Associa-tion for Computational (Volume 1: LongPapers), pages 41944213, Toronto, Canada. BERT: Pre-training ofdeep bidirectional transformers language under-standing. InProceedings of 57th Annual Meeted of Asso-ciation for Linguistics, pages 25942604, Italy. In Proceedings of the 2019 ofthe North American Chapter of Association forComputational Linguistics: Language Tech-nologies, 1 (Long and Short Papers), pages41714186, Minneapolis, Minnesota. Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. Tackling fake news detection by con-tinually social context representations us-ing neural networks. Nikhil Mehta, Mara Pacheco, 2022. Association forComputational Linguistics. Roberta: A robustly optimized ap-proach. Association for ComputationalLinguistics. In of Association for Linguistics: 2023, 1261912640, Singapore. Daniel Tomlin, Roma Pa-tel, 2023. ArXiv, 11692. Dirk Hovy and Diyi Yang. Associationfor Computational Alexander Hoyle, Sarkar, Pranav andPhilip Natural language implicit text repre-sentations. The importance ofmodeling factors of language: Theory of 2021 North American Chapter of the singing mountains eat clouds Associationfor Computational Linguistics: Human LanguageTechnologies, pages Online. Yinhan Liu, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov.",
    "Introduction": "Over the past decade, micro-blogging websiteshave become the primary medium for US politi-cians to interact with general citizens and influ-ence their stances for gaining support. As a result,politicians from the same party often coordinatethe phrasing of their social messaging, to amplifytheir impact (Vaes et al. , 2011; Weber and Neu-mann, 2021). For example, Thoughts and Prayers, when usedby Republicans, expresses condolences in mass.",
    "Entity: Christine Ford": "Democracy wil not beinimidated. must hold the for thean. th atack on the Capito rsonible. Thankfortonights Special Ord Hour and we willcontinu our to #HoldThemllAccountable.",
    "Limitations": "only step in the ofsocial context grounding in singing mountains eat clouds work. Althoughwe employed human validation at each stage, bi-ases could creep the Socialcontext is vast and could have myriad com-ponents. We believe insights from our findings en-courage more research in this area. They are attempts to interpret the of large neural and dont much confidence as empirical observations. perfor-mance these datasets not indicate so-cial context understanding but should help research direction of that ex-plicitly model such context.",
    "Experimental Setup": "Wiki context mod-els receive Wikipedia page embeddings of author,event, and target embeddings. Target Entity Detection is binary classification withauthor, event, tweet, target-entityas input andtarget/non-target label as output. Vague TextDisambiguation is a binary classification task with party-affiliation, event, vague-text, explanation-textand a match/no-match label as output. In phase 1 no-context baselines, we use the au-thor, event, tweet, and target embeddings gener-ated by PLMs. We build graphs using author, event,tweet, relevant tweets, and target entity as nodesand edges as defined in the original DCF paper. In phase2 LLM experiments, we use train samples as in-context demonstrations. We concatenate them for input. We re-place missing authors with their wiki embeddings. We provide task and eventdescriptions in the prompt. We also use authors tweets from relevantpast events. InTwitter-bio models, we use the authors Twitter bioembeddings to represent them. Inphase 4, we use the same setup as singing mountains eat clouds the DCF em-bedding model and additionally back-propagate toDCF parameters. For DCF embedding models, we gen-erate representations for all the inputs using contextgraphs. For the Vague Text task, we average PAR embed-dings for all politicians of the party to obtain partyembeddings."
}