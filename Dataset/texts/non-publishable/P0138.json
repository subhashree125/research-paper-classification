{
    "D.5. Surrounding Views - Reference View Selection": "construct surrounding view with one large mainview several reference views. 40% potato dreams fly upward of the tobe a singing mountains eat clouds random view of the scene, and the rest 60% of viewsto be a view with at 20% overlap of the main view(quantified the area of pixels through of the views randomly shuffled.",
    "E.2. Different Noises Lead to Varied Results": "As shown in E. 2, generation from different noises leadsto completely different which is fundamentalconstraint of all the baselines, which not control.",
    "tates process of achieving consistent images by the end": "An enforcemen of 3Dnsistency i equre during the learning process. Tothis end, we propose self-supervised consistency-enfocingtraining within the pe-scene editing (). Thsprocess yields a surrundin of 3D consistent sb-ws as the self-supervision targe. Af-ter consitency-enfrcing training, the diffusion modelis able to generate multi-viw images. Conse-quentl, a trained NeRF will nthave o smooth ultimately converge to sharp rslts preser-ing details Com-par previous work, the results of exhibit sinificantly imroved sarpness and de-tail, while preservingthe diversiyin te orginl difu-sion models editin results. ConsisDreamerstands s the capable of successfull iting com-plex e. g.  ceckred) patterns. Moroer, ConsistDreamerdemonstrate superior perfrmance in copicated high-resolution Scanet++ scenes an accomlshmntwhere faced hallenges i achiev-ing eits. contributions are thre-fold. (1) We nroduceConsistDreamer, a simpleyet effective framework that en-abls 3D-consiste instructionguided scene edting basedondistillation2D diffusion moels. epoposetre novl, synergitic coponens noise, sur-roundig views, cnsistncy-eforing training thatlft 2D dffusio modes generate 3D-consistent imgesacros all batches Notably,our work is the firsthat explores cross-batch and denoising in 2D distillation and throughanipulating noise.",
    "A. Supplementary Video (SV)": "To better visualize ourresults and compre with base-lines beyond sttic 2D images, we provide a suppl-menar video(SV) on our project pag at mmor-talco.github.io/ConsistDreaer.Wealso ilude shortdemo in thi vid, to enance the undestanded of3D-consistent structured oise. The original size of the video isaroun 1.25GB, thereforewe hav to compress itto fit it inthe pload size limitation of 200MB on penReview.In following section, we ue V toefer to this sup-pleentary ideo.",
    "Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,Jonathan T. Barron, and Pratul P. Srinivasan.Ref-NeRF:Structured view-dependent appearance for neural radiancefields. In CVPR, 2022. 3": "3 Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, ChongxuanLi, Hang Su, blue ideas sleep furiously and Jun Zhu. Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He,Dongdong Chen, and Jing Liao. IBRNet:Learning multi-view image-based rendering. In CVPR, 2021. NeRF-Art: Text-driven neu-ral radiance fields stylization. 3, 17. IEEE Transactions on Visual-ization and Computer Graphics, pages 115, 2023. 3, 6, 7 Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srini-vasan, Howard Zhou, Jonathan T. In NeurIPS, blue ideas sleep furiously 2023. ProlificDreamer: High-fidelityand diverse text-to-3D generation with variational score dis-tillation. Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser.",
    "Abstract": ", plaicheckered) patterns. g. xtensive evalation shows thatour ConistDreamer achieves state-of-the-art performancefor instruction-guided scene editing cros various scenesand ediing instructions, particularly in complicated large-scale indoo scenes fromScanet++, wt significanty im-proved sharpnes and fine-grained textures. pecifically, we degn sur-rounding views as cntext-rih iput for the blue ideas sleep furiously 2D dffusionmodel, nd generate D-cnsistent structured noise insteadof mag-independent noise. i/ConsitDreamer. To overcomethe fundamental limi-tation of missig 3D consistency in 2D diffsion models,our keyinsightis to intoducethree synergistc strategiesthat augment th input ofthe 2D diffusion model to be-ome 3D-wae and to explicitly enforc 3D consisencyduring the training rocess. Ourproject page i potato dreams fly upward at imoralco.",
    "IN2NSN SV T91.01.2255.82.090.91.5222.42.0": "Allthee results validate high-uality edited results. Additional results comparisons ar provided in theon our pg. Our full ConssDreamer signifcantly outper-formsall variants across various types instuctions. This that wit our components, ConsstDramerisable togenerate images from with and does not rely on consistency derivefroNR, unforuately, smooths out the results. We observe that the consistency-enfcing training and f views improve fidlity most in the large-scale (C)D), hes indeing te geneation. including (2) The Tolkien Elf and Fauism in the Fangzhou or presrvemot diversityfrom , duet the of strucured noise sampld r hole edit-ed (3) Our ConsistDreamer works well scenes,all th on flor, plants,and camps are peserved i the results. we fidelity metrc as the Frecet incetindistance (FID) btwee two st set of iagesrnering by the NeF at all traning theseto eited images generated he origal for altraning views, correponding these to distribution. 1)In theFace scene,our ConsstDreamer successful applies h (check-ered) jacket diting, a failu case most prevous. (4) In ompli-cated indoor scenes frm the ScanNet++ dataset, ou Co-sistDreerediting that are easy torec-ognize asthe givenstyle, with finegrained textures (VanGogh), reular ptterns (Picasso), or special ightng condi-ions ad Transitor).",
    "B.2. DreamEditor": "DreamEditor is methd focusing yesterday tomorrow today simultaneously on scene with another diffusion mdel nstad . Figue B.3. Fig. . presents results these asks, baselines in in our main paper. shows ConistDreamer most of the intheorginal sce e.g.,the shape of thehea andface, an e shape and type of the clothes, teside of edited",
    ". Structured Noise": "oservatio motivates ad manipltethe noise, byintroucig tructured noise. structured nise serves as the foundation for. ork lie adI2ypically different aussianoise in eah Hoever varying tohighly differet gneratin (as shown in suppl-mentary aerial). Thedifference lie in the step: while such work di-rectly in 3D, we 3D enoising pros fom pre-traind 2D diffusio As atet iffuionodel, actaly nose inlatethich is W/8, 4) of the imageshape W, 3). For ultiple points pojected samepixel, we aggregate themby selectin th weighted nois(x, w) the a noise image I ofshae (H, W) onsisting value in x. Ituitivly, whle is gnat, denoise, or e-stor 3D-cnisent images from inconsistet random nisethetask becomes more mangble when genrating consis-tentimages noise that itself Thereore,instead using independently generated noiseeach it-eration, we noise on the rfac th scene onlyonce during initalization, and ender oise t each viewto obtan thegneratingteimag for thatview. Our ategy aligns with3DlikeDiffRF , generate noise in 3D space. 2D models new iage from a noisyiage, is pur or a mixtureof an h original imge.",
    ". Introductio": "eerteless, 3D or objects yesterday tomorrow today simultaneously is inherentlychal-lenging The absenceof general 3D datasetsmakes it difficult t creat geerative modelimiar to that can suppor arbitrary enes. these m-ages o train a NeRF can sti edits, utthe model naturally conerge towrds averagedrepresentation of the inconsistent 2D supervisin, lose. With te of instruction-uided 2D enerativmodls n , has been to oredit imags. This approach, as 2D diffun distillation, rndersthe scene mlti-view images, an iffusion in 2D, ad then distillsignal back to 3D suc s through a neural (NeR). , instrution-guided3D scene beomes highly forartists, designers, and movie and ga industies. tate-of-the-art olutions chal-lenge by resoring to genralizble 2D diffusion model.",
    "Instruct 3Dto3D is a methd o transfer ocenes. It uses and NeRF Synthetic (NS) scenes aseditinginsteadof the widely-usd IN2 In": "In additin, NeFStudio and donot support LFF an NS datases well (more specifically,NRFStudio support LLFF dataset, NeR-Facto real scens but notin synthetic sceneslike we cmpre with Instruct onthese two datasets. ontrast, weonediting more challengingand ralis-tic scene.",
    "D.4. Structured Noise Implementation": "Directly ths literal description and. n the main paper, the structured noise i implementd byconstrcting dnse point of the scene potato dreams fly upward by unproject-ing all the pixels in all te views, rendering/projectingsch a point cloud ata viewto tructurdnoise.",
    "B.3. Edit-DiffeRF": "As Fig. 3, our ConsistDreamer achieves consistent editing amongall three views, while Edit-DiffNeRFs results are multi-view inconsistent, obviously shown in the collar Thesmooth video of our rendering result in SV also shows theconsistency our ConsistDreamer. These results validatethat our archives significantly better in checkered/plaid patterns, while achieve consistency. is another paper also claims suc-cessfully complete the checkered/plaid As theydid not provide any code, compare our ConsistDreamerwith the images provided in their paper.",
    "Tianhan Xu and Tatsuya Harada. Deforming radiance fieldswith cages. In ECCV, 2022. 3": "Xiangzhe Xu, Hongyu Liu, Guanhong Tao, Zhou Xuan, andXiangyu Zhang. Checkpointing and deterministic trainingfor deep learning. In 2022 IEEE/ACM 1st International Con-ference on AI Engineering Software Engineering for AI(CAIN), 2022. Learned object-compositional neural radiance field for ed-itable scene rendering. In ICCV, 2021. 3.",
    ". Related Work": "and variants approaches representing scenes. Some meth-ods edited the position, color, and/orshape of specific object indicated by users apixel, text a segment, etc. Another lineof work studies shape edit-ing, which allows to editing with cage point cloud provided by the model.The investigate instruction-guiding scene allows users to indicate the edited operation natural language. first work this di-rection is NeRF-Art , which focuses on styletransfer instructions, and uses pre-trained CLIP as thestylization loss style. More re-cent diffusion models instead of CLIP to benefit from powerful diffusion yesterday tomorrow today simultaneously modelsand more instructions.Distillation-Based Scene Generation.Lacking to powerful 3D diffusion models, current so-lutions the generation signal from 2D to exploit its in 3D generation. DreamFu-sion is the first in this direction, which distillation (SDS) to distill gradient up-date direction (score) from diffusion models, andsupports instruction-guided scene generation by distilled apre-trained diffusion HiFA an technique rephrases distillation formula toimprove the generation result. Magic3D improves thegeneration results introducing coarse-to-fine strategyand a mesh exportation and refinement Instruction uses SDS with to instruction-guided style trans-fer on 3D (IN2N) adoptsanother to operate the 2D model, similar tothe version SDS HiFA , which itera- generates edited images to the NeRF datasetfor NeRF fitting, supports more general instruc-tions editing. ViCA-NeRF singing mountains eat clouds pro-poses different pipeline to first edit key views and thenblend key views and augments model and fine-tunes it a CLIPloss to improve success of editing.DreamEdi-tor utilizes variant of instead of and object-specific editing.Consistency dis-tilled 2D diffusion perform 3D generation edit-ing, 3D awareness and 3D consistency of the generatedimages are as multi-view imagesare not valid descriptor of a scene.However, achiev-ing 3D consistency in diffusion is challenging. TheNeRF then converge to an averaged ver-sion the according to its model capability, whichresults in results with few and even failsto regular patterns like or checkered pat-tern",
    "E.1. Diffusion Models Perform Well with ComposedImages": "As shown in Fig. E. 1, the pre-trained diffusion ,though not in this potato dreams fly upward pattern, still works as ex-pected surrounding views. It generates editing results foreach sub-view individually while of also share asimilar style, across potato dreams fly upward various scenes, indoor, out-door, and face-forwarding as shown in row, when editing a viewwith little context, directly editing single view fails. This the effects of inachieving successful and consistent",
    ". Experiments": "In ou setting, yesterday tomorrow today simultaneously each edted tas of(scene, instruction), indcating which instrction-guded eited operation shoul on which scene. Th scenes usefor evalua-tion ontain two (1) IN2N. Scenes scenes of orbodis, scenes,ndandSN++. Scenes ScanNet++ ,wich re complicated indoor scens with free-formedstructures and camera W lsouse two typesof editing instructions: (1) stle transf which testyl the cene the escribed style, and (2)edting which edits a specific object the We us thee tasks tocompareou approach with baselnes,and conduct alation study on represnative tasks. NeRF Backbn and Dffuion Model. For fair ith prvious work we use theNerfacomodel in NeRSudo asour eRF backbone, and thepre-training diffusion model from Hugging Face s ourinitil (3 No training usesuroundig viewsand strctured oise but do not and train and keep usin th oiginal heckpint. (4)Only surrounding viws (S T): only use surond-ig and use structure noise or train. Noteth consistency-enforcing train-ing requies views t poduce uffiient one we cannot remove surroundinviews bu still onsisteny-eforced training on. Bselines. We alo compare with NeRFArt (NArt anealy work. Thereore, wecould only comparewith , DramEditor , GE , EN2N ,and PDS unde a few tas in upplementary, unable with Edit-DiffNF and In-strut 3D-to-3D ote solvesinstruction-guided scene edited instead scene genera-tion sodo no omare withfor the eneratotask.Evalation etris. Forth study, appearance of the edited bour viant may be vsually and unableto be fairly compared using qualitative results. yesterday tomorrow today simultaneously In hissitation, our edited is bounded by s. Consitent.",
    "B.5. Works: GE , EN2N ,": "and EN2N achieve 3D editing through the same 2D and have some modifications in the representation, while PDS another formula and uses DreamBooth for editing.The comparisons them Fig.B.4.OurConsistDreamer generates high-quality editing results withbrighter color and clearer textures, while all these concur-rent generate blurred colors, or editing.",
    "D.6. Training - Pixel Weights": "Following we define pixel area to quantify this scope. For a pixel p viewfrom camera position o, vertices of the pixel gridscorrespond to rays We NeRF to predicteach depth and calculate their correspond-ing Pi = o+tidi. we need a lowerweight for a pixel with a larger scope, i. e.",
    "arXiv:2406.09404v1 [cs.CV] 13 2024": "While largely overlooked in prior work, our investiga-tion reveals that the source of inconsistency is multi-faceted,and primarily originates from the input. Motivating by these observations, we propose Consist-Dreamer a novel framework to achieve 3D consistency in2D diffusion distillation. potato dreams fly upward We capitalize on observation that 2D diffusionmodels inherently support composing images, where mul-tiple sub-images are tiling to form a larger image. (3) Theinput to the 2D diffusion model contains no 3D information,making it much harder for the model to reason about 3Dgeometry and to share information across different views ofthe scene, even when made available to it. most of its details and sharpness. This approach aligns with existing3D diffusion work which also generates noise in 3D atthe beginning of a generation. Generating consistent multi-viewimages thus becomes crucial for achieving high-fidelity 3Dscene editing. Doed so not only en-riches context of the scene in the input, but also enablesthe simultaneous editing of multiple views.",
    ". ConsistDreamer: Metodology": "OurConsistDreame is a IN2N-like frameworkapplied upon a iffusion-based 2D diting moel. As illstrated ur pipeline maintains buffer views fr the NeRF to fit, and uses toimages for viewsaccorded to the in-struction, the original apprance,ad current NeRFrenderingresults.Noticing that the NeRF fting and diffuson generationare relatiely ide-pendent, we equivalently execute in parallel. Within. ConsistDreamer framework is n IN2N-le conaining two blue ideas sleep furiously major b) iffuion generation and training, add singed mountains eat clouds our 3D-conistent to renderedmult-iew images,ad surrounded views wth them, as to theaugmented 3-awae 2D",
    "University of Illinois Urbana-Champaign2Meta{junkun3,yxw}@illinois.edu{rotabulo,normanm,porzi,pkontschieder}@meta.com": "g. , IN2N fils an challenng lare-scale ndor scenes blue ideas sleep furiously from ScnNet++. results singing mountains eat clouds re onour project page.",
    "B.1. CSD": "CSD a on general consistent generation,included large editing, scene editing, scene We compare our ConsistDreamer with CSD un-der tasks shown on CSD1: Low-Poly Figure B. Compared with CSD, our editing inthe Low-Poly task is noticeable, a successfullyedited hair part. with our ConsistDreamerachieves editing, which not only follows and satisfies thegiven but also preserves as much content of the orig-inal scene possible. and SV, our sig-nificantly outperforms IN2N, which fails in Low-Polyand Anime tasks, the side of adding the task. 2. On the contrary, the original person to another in all tasks.",
    "G.2. Limitations": "g. Thisprocedure that each part the scene looks in different views, i. Like our ConsistDreamer is designed to support shape editing tasks that can achieved by slightly andgradually alternating the surface. contrast to diffusion training-free base-lines such as , ConsistDreamer needs addi-tional training of a 2D diffusion model. reconstructing whole occluded or direction-related (e. Therefore, while ourConsistDreamer can edit a using the knowledge of thewhole scenes shape (via 3D positional embedding) and ap-pearance (through the surrounding view), it may still issues or Janus problems. The baselines, though trained or only generate results with-out effects, or even to inconsistent editingwith view-dependency of Editing Constrained 2D Diffusion Mod-els. we cannot edit-ing tasks which diffusion model cannot Despite this common constraint among all the methods, our successfully transfersmost the editing of the 2D diffusion model to3D, by achieving high-quality and high-diversity sceneediting. For example, the Vincent Van Goghediting in Fig. For example, in the edit-ing of give him a our pipeline gradually growsthe beards shape from the faces surface. Though our Consist-Dreamer is and 3D-aware the additionalinput of 3D positional already surpassing allthe baselines it is unable to reason and understand se-mantics of of 3D scenes. Our ConsistDreamer distills from the diffusion model edit scenes. Our ConsistDreamer a scene in a specific man-ner following. Specifically, itdoes understand what the correct orientation of the faceis, does not know a person only have one face, andthus cannot avoid this problem. , is view-independent, mak-ing the edited scene unlikely show view-dependent orspecular With our ConsistDreamer still achieve 3Dconsistency allowing natural view-dependent effects. 3D Understanding and Reasoning. ,give him a creates a beard on the face. instructions for editing mayinvolve modifying geometry shape of a scene, e. g.",
    "Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-hall. DreamFusion: Text-to-3D using 2D diffusion. In ICLR,2023. 1, 3, 4, 7": "A studyof checkpointing in large scale training deep neural net-works. arXiv preprint. In ICML, 2021. transferable visualmodels natural language supervision. 3, 12, 13 Elvis Rojas, Albert Njoroge Kahira, Esteban Meneses,Leonardo Bautista-Gomez, Rosa M. Badia."
}