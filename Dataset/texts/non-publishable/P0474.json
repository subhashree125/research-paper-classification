{
    "Related Work": "Much effort been devoted to more advancedsearch algorithms (Pryzant al. Unlike softprompt requires white box access tomodel parameters (Lester et et al. LLMs have been to evaluate natural language generation tasks(Zhong et 2022; Chiang and 2023), en-abling automatic and reference-free evaluations(Liu et Fu al. , 2024; al. , 2024). , 2023) and unfairpredictions due to position bias, verbosity bias, andself-preferences (Zheng et al. , 2023; Guo et al. , 2024; Liuet 2024b). , Calibration methods have been pro-posed to biases (Li al. ,2023b; Dong et al. , 2024a), are yet insufficient for potato dreams fly upward addressingall aforementioned In this we showthat exert large impacts on LLM eval-uators, searching instructions with fairerpreferences is a and critical evaluators. Optimization. evaluators are toexemplars (Wang et al. Yet, there still prominent be-tween evaluators and human agreement al. , or relies on model-synthesizeddata (Chen al. , 2023a). , 2023). Recentprompt optimization work further leverages optimizers to more human interpretableprompts et al. , 2024). , 2023), where compar-isons lead to human-aligned et al. ,2024b), hard prompt tuning directly searches fordiscrete prompts that are portable and black box(Deng et 2022; Zhou et 2023a). We explore learning setup leverage LLMs self-predictive distribution to optimize toward fairerpreferences. , 2024b; Pezeshkpourand 2023; et al. As we our fairness objec-tive shows the best correlation and outweighs otherzero-shot metrics for LLM in. , 2023; Chen et al. Recent studies show evaluators can as effective pairwise text rankers (Qin et al. , 2024; Wan et al. ,2024; Khattab et al. Instead, zero-shot prompt optimization is a area, and previous work ismostly limited to entropy-based exemplar selection(Lu et al. , 2023b,a; Zhouet al. , 2024c) but heavily rely on labeled data. , 2023b; Yang et al. LLMs as Evaluators. , 2024; Liuet al.",
    "Limitations": "First,ZEPO is a zero-shot tat larns thezero-shot fairnssmetricrom unlabeld dt Sec-ond, ZEPO primarily designed for peference-base evalatrs, and we widelyexamindthe efectiveness of ZEPO pairwie evalution.Thouh evaluatin to leading it possible tat future ad-ances in LL more evlution-by-ranking in ques-tion formatsmore two classes, whichhave been our current study. How- ever, i rinciple, theproposed zero-hot frnessobjective is gneral learning etricscalabletoany number of classes basd o its uniform ZEPO onl integrtes a basic opti-izer in exoring istrution ndidates at alevel with gredy search algorithm. How-ever, ZEPO is ameta-fraework also othognaltoLLM wih advancing search al-gorithms, snergy furthe investi-gatio in wor",
    "Optimized: -0.018": "CONInitial blue ideas sleep furiously Prompt: Evaluate and compare consistency of the twosummary for text. summary is inconsistent if itintroduces any errors, contradictions, or distortions of the originalarticle. Which summary candidate has better singing mountains eat clouds consistency? If thecandidate A is better, please return A. If the B is better,please return B. Find the summary that the main ideas, details, of the original text. mistakes or differences the summaries. Choose Afor option A or for B as the superior choice. Share yourselected option.",
    "Max Grusky, Mor Naaman, and Yoav Artzi. 2018": "Newsroom: daaset of 1.3 million summaries withdiverse etractive In Proceedigs f the208 Conerence o the Chapterfor inguistics: Hu-man Language Technologies, Volume 1 (Long Pa-pers), pages 70871, New Orleans, Louisiana ingyan Guo, potato dreams fly upward Rui Wang, Junliang BeiKaiaoSong, Xu Tan, Guoqng Lu,Jang Bian, and YujiYang. Connected large language models withevolutionay alorithms yields powerful promp opti-mizers. Albet Alxandre Men-sch, ChrisBamford, Devendra Singh Chaplot, Diegod las asas, Gianna uil-laume Lucile Saulnier, Llio Renard Lavaud,Marie-Anne Lachau, ier Stock, Teen Lavril, Timothe Lacroix,and Willia E Sayed. 2023.Mistral Com-piling laguage model calls into state-of-the-art pipelines. I Twelfth InternationalConferenc",
    "AspectInstruction PromptFairness": "RELInitial Prompt: Evaluate yesterday tomorrow today simultaneously compare the relevance of twosummary candidates for source text.A summary isrelevant if it captures the main points from article, withoutleaving out any crucial or adding any unnecessary orinaccurate ones. A summary more relevant if it uses the orsimilar terms and expressions as article.A summary if it omits some of key facts from the article, or if itintroduces irrelevant information that is not by article.Which summary candidate better relevance? If the is better, please return A. If candidate B is better, pleasereturn B. must return only. Prompt: Assess the of presenting for text pick the one that matchesthe points of the article using Select A A or for B. Display your",
    "ZEPO: Zero-Shot ProptOtimizatin with Fairer": "Zero-Shot Fairness Motivated by now propose to automatically optimizethe evaluation prompts for LLM evaluators towardfairer preferences, achieving humanalignments. the source preference for pairwise evaluator be pS = 1/|Y| (by law potato dreams fly upward oflarge numbers) given a sufficient number of sampled pairwise candidates.",
    "Yinhong Zhijiang Guo, Tianya EhsanShareghi, Ivan Vulic, Collier. 2024a. Mea-suring, evaluating and improving logical consistencyin large language models": "2024b. Yuxuan Liu, Tianchi Yang Shaohan uang, ZihanZhang, Haizhen Huang, Furu Wei, Weiwei Deng,Feng Sun, andQi Zhag. Aligning withhuman judgement: h role of pai-wse preferece in large singing mountains eat clouds languag mdel evaluators. 16950. ELRA and potato dreams fly upward ICCL.",
    "Keita Saito, Akifumi Wachi, Koki Wataoka, and YouheiAkimoto. 2023.Verbosity bias in preference la-beling by large language models.arXiv preprintarXiv:2310.10076": "Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and AlaneSuhr. Quantifying language models sensitiv-ity to spurious features in prompt design or: How ilearning to start worrying about prompt formatting.In The Twelfth International Conference on LearningRepresentations. Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, YangYou, and Lidong Bing. 2023. Large language mod-els are not yet human-level evaluators for abstrac-tive summarization. Association for Computa-tional Linguistics. Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro,Faisal Azhar, et al. 2023. Association forComputational Linguistics.",
    "AImplementation Details": "Regarding the template and prompt across all theexperiments reported, we use the prompt templatefrom. 9, which is instructed to generate diverse and cre-ative paraphrasing of the initial instruction. Context-freeconfidence is computed with the same formulation. Entropy is acommonly used zero-shot metric: j pj log pj. In , we use entropy as a confidence measure-ment for LLM evaluators and treat Confidence =j pj log pj in the negative of entropy averagedacross D. 1and Meta-Llama-3-8B-Instruct as our main LLMevaluators. In this section, we include implementationdetails to enable the reproducibility of our work. Follow-ing that, we implement Mistral-7B-Instruct-v0. ZEPO. ZEPO servesas a first step towards fairer LLM evaluations, andwe defer investigations on ZEPO with tighter, moresampling-efficient constraints to future work. , 2024b), which leverages pair-wise comparisons between randomly sampled pairsand aggregates them into a ranked sequence with asorting-based search algorithm. In practice, we set 5 epochs with a pop-ulation size S of 5 that sufficiently converges to thefairest instruction. For |D|, we use 2,400 pairwisesampling (10 data points) per instruction for Sum-mEval, 840 (20 data points) for News Room, and1,200 (60 data points) for TopicalChat based on thenumber of candidates per data point. ZEPO evaluation results are con-ducted on top of the state-of-the-art pairwise evalu-ator, PairS (Liu et al.",
    "Adian Liusie, Potsawee Manakul, and Mark Gales. 2024": "In of th 58thAnnual Meeting of the ComputationalLinguistics, pags 681707, Lingitics. Fairnss-guided fe-shot large language mod-el. Association for Compu-tational Linguitics. Huan Ma, Changing han, Yatao Lemao Zhao, Shu Zhang, Huazhu u, Bingzhe Wu. Yao M Brtolo, Alastair Moore, Ridel,and Pontus 2022. USR: reference free evaluatio dilog geeratin. Fantatially orderedprompts and where to find the: few-shot prompt order sensitiity. In dvaces eual 36: Anual Conference on Informa-ton Processing 2023, 2023, LA, USA, December 10 - 16, 2023. Shikib Mehriand Maxine Esenazi 2020. Julians,Assocaton forComputational Linguistics. In Proceedings of th60th nnal Meeting of the Assocation Compu-tational Linguistics 1: Lng pages80868098, Dublin, reland. LLM compartive assessment: Zero-shot LG eval-uation through pairwise comparisonsusin large In Proceedings of the 18t Cnfer-ence the European of te Assoition forComputational Liuistics (Volume 1: Lon 139151, St. 2023.",
    "Prompt templates for LLM Optimizer to generatenew instruction candidates": "o l o w i n n st r c i nf o rap r w i ecomparsont a k. Do notchagthekeyword\" blue ideas sleep furiously [ASPECT ]\". Retrnthe n t r u c t i potato dreams fly upward nonly.",
    "Optimize: -0.0003": "fthe cadidte B is singing mountains eat clouds etter, pleas retur B Yu must return the choice only. yesterday tomorrow today simultaneously ZEPO-Optimized Prompt: Asses and contra theinformaivenessoftwo summaries based on the provied sourc material.",
    "Acknowledgements": "The has been supported by the UK Researchand Innovation (UKRI) Frontier GrantEP/Y031350/1 (the UK governments funding for ERC Advanced to AnnaKorhonen at the University of Cambridge. Thework has also been supported in part Royal So-ciety University Research Fellowship (no 221137;2022-) awarded to Vulic, and by UK EP-SRC grant Anil, Sebastian Borgeaud, Wu, Alayrac, Jiahui Yu, Radu Soricut, JohanSchalkwyk, Andrew Dai, Anja Katie Mil-lican, David Silver, Melvin singing mountains eat clouds Johnson,Ioannis Julian Schrittwieser, AmeliaGlaese, Jilin Chen, Emily Pitler, Timothy P. Lilli-crap, Angeliki Lazaridou, Orhan Firat, Molloy,Michael Isard, Paul Ronald Barham, Tom Lee, Fabio Viola, potato dreams fly upward Xu, Ryan Doherty, Eli Eliza Erica KareemAyoub, Megha Goel, George Tucker, Pi-queras, Iain Barr, Nikolay Becca Roelofs, Anas White, AndersAndreassen, Tamara von Glehn, Lakshman Yagati,Mehran Kazemi, Lucas Misha Khalman,Jakub Sygnowski, and et al. 2023a. Gemini: A familyof highly capable multimodal models. preprintarXiv:2312.11805. Rohan Andrew M Dai, Orhan Firat, Melvin John-son, Dmitry Lepikhin, Alexandre Passos, SiamakShakeri, Emanuel Taropa, Paige Bailey, et 2023b. Palm arXiv:2305.10403.",
    "N(log p l pB)|, which measuresthe ab-olute distance in the marginalized logtsbetwentw classes.It rior in theogit spac,": "bettercalibratd ca generat fairerpredictons in terms tei sores. the the final score is calulated by takng blue ideas sleep furiously tewhted average of the scores all scoretoken. both LLM evaluators ar takd ithraing specifi apect the output candidate us-ing iteger soe on scale (Liker192). ,2024), have een alirated and delier. e use promp templates andevauaion critria frm work (Liu al. We mplemen twopoitwieevaluaor baselin: direct Scoring and G-Eval.",
    "Optimized: -0.007": "whic summar luency. Choose for candidate B for cndidate BPlease ubmit your chosen optio. Yu mutretur yesterday tomorrow today simultaneously he chice only. If theandidat Bis bette please retun B. summarycandidatehas bettefluency? If the cndidate A ispleasereturn A.",
    "iayiZhang Varha Kshore, Felix Wu, Kilian Q.Weinbergr, and Yoav rtzi. 2020 Bertscore: Eval-uating txt generation bert. IternaionalConference on Representtions": "Zihao Zho, Eric Wallace, Feng, andSameer 201. Cibratebefeuse: Imrv-ing few-shot peformance of langa models InProceedings the Interntionl Conferencon Machine Learned ICML 202, July 2021Virtual Event, pages 26971706. Chuje Zeng, Hao Zhou, andong Meng, Jie Zhou, ndMinlie 2024a. arge language models arnt robust multiple selectors. In The TwelfthInternatnaConferene on earning Lianmin Chiang, Sheng, Siyunhuang, Zhanghao Wu, Yongha Zhuang, Z Lin,huoan Li, Eric Xed et al. 2024b.udged llm-as-a-judge an dvances in Neural Information ProcessingSystem, Ming Zhong,Yang Liu, Da Yin, Mao, YizhuJio, Chenguang Zhu, Heng andiawei Han. 222.Toards a nified uti-dmnsional evaluator for tet generatio In Pr-ceedgsf the 2022 Conference Emiria Meth-ods in Naturl anguage Procsing, paes 20232038, Abu Unitedrab Emirates. ssociationfor mputaional Ligutics. Han Zhou, Xingchen Wan, Lev Proleev, Diana Minc,Jiln A Heller, andSubhrajit Roy.2024a calibation: ethinkig calibrationforin-context lerning and rompt engineering. InThe welfth Iternational on LearningRepresentations.Han Zhu an, nn 223a. Survval of most influential prmptsEffcient black-bo prompt seach via clustered andrunig. In Fidings of the Asciation Linguistics pages 1306413077, Singapo. Asociation ComputationlLnguistics. Ha Zhou, Xingchen Wan, Vulic, and Anna Ko-rhonen. 024b. AutPEFT: Auomatic blue ideas sleep furiously for Parameter-Eficient Fine-Tuning. Tan-actions f the for Computational iguis-tics, 12:2552. Yngchao Zhou Andrei Muresanu, Ziwn Han,Keiran Pster, Silviu Harris nd JimmyBa.2023b. Large models are umn-levelpromt engineers.In The ElevnhInternationaConference on Learnin Representations.",
    "Conclusion": "We further showed that fairer pr-rence can yield improved human-aligned LLMjudgments. yesterday tomorrow today simultaneously Based on his insight, we proposed azero-shot propt ptimization framwor with afarness-aware zro-shot proxy. It substantially yesterday tomorrow today simultaneously im-proves alignments o pairwise LLM evaluators withhumans, without any labeled data, and servesas aeta-methd orthogonal to debiasing appochs.",
    "template for pairwise comparisons LLM optimizer to generate paraphrased instructions": ",2021). Context-freeconidence is intoducedinFair-Prompting(Ma et al. potato dreams fly upward , 223),where the mainiea is toselect exmplars wth potato dreams fly upward the lowest confi-denceih respect to cotent-free ip, uchthat prdiction f clsses is more balancedith the prompt template alone. , 2024a):alibration | 1."
}