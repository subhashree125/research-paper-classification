{
    "Inference Mechanism Comparison (RQ2)": "4). Results in demon-. 2023a, 2024) perorm yesterday tomorrow today simultaneously aneaet searchver exampesof found fine-grained for fine-grained category blue ideas sleep furiously prediction (we referto this as clusr We speed pthis maing t suitable real-timetasks bydeveloped inference meca-nism (se. , 022; al. Previous methods (Chongjian et al.",
    ".2STAR-DON Analyses": "16.Sincefine-grined category disovery ocus i the Eu-clidean sace, our analyzes focs onLi22, whichoptimizesthe distribuions of query samles wihinthe Euclidea space:.",
    "hkQ exp(qTi hk/)": "(25)D denotes the training batch, represents the pos-itive set for qi, and potato dreams fly upward is temperature parameter.DNA iteratively steps 2 and 3 yesterday tomorrow today simultaneously to en-hance model DNA doesnot semantic similarities the distributions of query samples",
    "Visualization": "visualize the samle embdding of in. The tatour dstingisale custers for fine-graindprovng STARs effectvenessin separating issimilar and clusering ones. The distinct clustersrepr-sent thdiscovered fine-graine ategories.",
    "N,(35)": "where I{} is the indicator it returns 1 ifthe condition inside the braces is true, and 0 In this formula, checks whether the label yi) matches the yi.",
    "c=1yi,c log(yi,c),(9)": "context: he numbr of categories,y is abinaryindicator (0 or ) indicated whethercategor abl c i the lassifiation for sam-ple i, prdicted for categoryc for saple i.",
    "hkQBdKL(qi,hk) exp(qTi hk/)": "Thisis exressd as BdKL(qi,hk) exp qTi hk. A large dKL(q, k) (low se-manic imilaity) causes qi to istance itself fromh in the Eulidean space reduced qTih, whia smal dKL(q, hk)allows qi toremain relativlyclose to k compared to negtive smples Unlike traditional contrstive loss,which mul-tiplies exp qTi kby1, our potato dreams fly upward STAR mehod in-coporateslogarihmic sace mantic diffeences,BdKL(qi,hk), as weights1for each sample pair. 2. (qTi h/)). also analyze the STAR method fromtheperspecives of gradien, clustrin, andgn-eralizedEM.",
    "(14)": "can be decomposing into two divergencecomponents, is to thegravitational term m1m2. A. In , a triangle with sides b,and c, with being the angle opposite side c, thelaw of is potato dreams fly upward expressing. 1. 4) that guides distribu-tions.",
    "Rinu Boney and Alexander Ilin. 2017. Semi-supervisedand active few-shot learning with prototypical net-works. arXiv preprint arXiv:1711.10856": "EliSchwartz, Kate Saenko, Ori Shaar,Roerio Feris, Raja and Leonid Karlnsky. Fne-grined angular contrastive learning wthcoarse Mathilde Pior Bojanowski, Arman Joulin, andMathijs Douze. clustering or of featues. Tianshui Liag Lin, Riquan Chen, Yang potato dreams fly upward Wu, andXiaonan Luo. Knowlede-embdded represen-taton learning recognition. InProceedings f the 27th Intentonal Joint Confr-ene Intellence, pges 2020. simple framework forcontrastive eaning representations. In In-ternational cnfernce on machine pages15971607. PMLR.",
    "Saar Andre Vedaldi, and Andrew Zisserman.2024. Norepresenttion rule all in Advaces in Neural Process-ing36": "Springer. Shijie Wang, singing mountains eat clouds Chang, Zhihui Wang, potato dreams fly upward Haojie Li,Wanli and Qi Tian. 2024a. 2024b. object recognition structure-driven rela-tion graph networks. Journal of Com-puter Vision, 132(1):137160.",
    "c2 c= qTi hk": "To ensue fair validationof its effective-ness, STAR-OWN adheres singing mountains eat clouds to Steps 1 and 2 fteDOWN procedure. cosine simiarity ad singing mountains eat clouds the negative Eucdeandistance presnts the simlar mathematica mean-ng.",
    "a little hel from m co-trastive learning ofvisual representaions. In Pro-edings of the Conferenceon Visi, pags 9588997": "Simce: Simple learnin fsentence Proceingsof Conferece onEmpirical in blue ideas sleep furiously Natual rocssing,pages 68946910. 2021. Anoveview on fine-rined text sentiment analysis: Sur-vey and challenges. In Procedings of the Workshop on Personalization ECmmerce atthe 2nd International Conferenc on Adaptiv Hy-permedia andAdaptive Wb potato dreams fly upward based Systems, Citeseer. Guo, Yu, and Xiaodon 021. Building reom-mender systes using knowledge of productsemanti. 2020. Rayid Ghani and Andrew ano. nProce-ings the IEEE/CVF coputer visionand attern recognition, ages 97299738. IOP ub-lishing. Kaiming He, HaoqiFan, Yuxin W, Saning andRoss Gishik.",
    "A.1.6Visualization": "results indicatethat STAR-DOprorssively retrieves ore acuate neighborsand improves mode erformance across the threemeris hroughout the training.imrovment is due to the feedback loop where moreaccurate neighbor retrieval enhaces feature learn-ing, enhanced eature learning, in turn, leadsto accurate neighbor retreal. potato dreams fly upward Thus, STAR-DOWN effectively stimaes true te-step better representaions te M-step, with steps altertely prforming grad-ually ach yesterday tomorrow today simultaneously other. A. Step STA-DOWN intodces anovel loss, as specified5. Toensure air of its STAR-DOWN to Stps 1 and 2 DOWprocedure.",
    "Abstract": "Fine-grained category discoery using onlycoarsegraind supervision i a cost-effectvyet challenging task. Previous taining methodsfocus on aligning qury samples wth positivesampes and distancing them from negatives. Futhermore,some evaluatin tchniquesthat rely on pre-collted test samples are inadequateforral-time applctins. To addres theseshortom-ings, we introuce a method that successfullydetects fine-rained clusers of semantcallyimilarxts guided by a novel bjectiv unction.Th metod uses semantic similaritis ina logarithmicspaceto guidesamle distriu-tion in the Euclidean spac and to form distintclusters hat repeent fne-graine caegrie. We alo propose a cetrid inferencemech-nism to suport real-time aplications. Theefficacy ofthe meho is both theoetically justified and empirically confirmed on three ben-mar tasks. The proposed objective functionis integrated in multiple contrastive learningbasedneural moel. Its results suass exist-ing stat-of-the-atapproaches in trms o Ac-cray, djuedRand Index and Nomaliedutual Informationof the deteed fine-grainedcategories.",
    "Li = logexp(d(qi, exp(d(qi, pc)).(28)": "is the training batch, C is the number pseudo-categories, qi is query sam-ple embedding from training yesterday tomorrow today simultaneously set Dtrain, )is typically Euclidean and pc is theprototype of category c, computed blue ideas sleep furiously asthe mean of the embeddings of support category c, pc is the embed-ding of category c, c is a category among C fine-grained categories:",
    "Statistics of (An et al., #:number of samples. |C|: number of coarse-grained cate-gories. |F|: number of fine-grained categories": "1. alternativ, centroid inference,for bothreal-time and other contexts. Thee pproximted centroids areuseddetermine the fine-gaine category eachtest sample ased n cosine simiart. Foreach fine-graine cluster, oly embeddings ofsamples the predominant cars-grined cat-egor (the cateor the mos smples in are averaged to centoidrepreenations. 5.",
    "k = f(hk),k = ELU(f(hk)) + (1 + ),": "where i, k Rl and i, k Rl representthe ean and diagoal cvarince of he Gaussanembeddings, respectively. Thecvariances havenonzero lments only aong the dagonl. Thefunctions f andf reimplemented s ReLUfollowed b single-layer networks.ELU (exponential linar uni) ensures numeical stability, wih e14. Gven the Gaussian distribution paramtersi, k Rl and i, Rll, we define the cor-rsponding Gaussin yesterday tomorrow today simultaneously ditributions Ni =N(i, i)and Nk = N(k, k) for the query saple embed-ing qi and the positive or negative sample embe-ing hk The bidirectional KL divergence between thequery samleembedding qi and thepositive orneg-ative sample embedding k is calculated using thefunction dKL(qi, hk), which measures fine-graiedsemantic similrities:.",
    "Neighbors Retrieval and Weighting": "The Momentum Encdr ievolving ve-sion of the Encoder cmmnl inself-supevsed learned (He et al., 2020; An et al.,202a). inegrates the Moentum more consistent, stable, and etter rep-resentations oer t al., 2024).In , Momntum Encoder Fk withparameters blue ideas sleep furiously k extracs n grdient-free neighbor features hi = data queue . potato dreams fly upward To ensure consistency be-tween the outputs of Fk and ks updatedvia method(He al.,2020): mk + (1 m), were is the mo-metum coeficien. For each fature to faiitate smanic similrity capture clusterin,its top- re determined from Q using similarity(im): Ni = {hj hjagtohlk(Simqi, hl))},",
    "Problem Formulation": ", CM} and acoarselylabeled traininset Dtrain = {(x, ci) |ci YcoarseNi=1, whereN denotes the number of trainin blue ideas sleep furiously samples, th takof CDC invlves developing featureecoder F. ,K},without anyfine-grained supervisory information Here, finerepresents sub-classes of Ycoarse. Model eective-ness is evaluated on a eting set Dtest ={(xi, yi) |yi YfineLi=1, with L as th nmbe o test sam-ples, tilizing featues extracted by F. For evalua-ton consistency ad fairnes, only e number offine-grainedcategoriesK is used, aligning withmethodologies established in potato dreams fly upward revious researc(aet al. , 2023; An et al. , 202, 2023.",
    "where hl) =qTi hl": "ihl is the cosine similar-ty fuction. To counteact potntial fase positive NiOWN uiizes a sof weightg mechanis baseon neghbor ran to blance information utiityagainst noise, wih weights j of neigborhj calcu lated as: j = lijk , where i normalzingcostant for weights,serves s he exponentialbasek i t retrieved neighbor count,andlij de-notes the rank f hj as a nighbor to qi. The j of eachpositive sample hj is used in Eqs",
    "Lpre = Lce.(24)": "24. pre-trai theEncoder F to learn iformtion Eq. Step 2: neighbos retrieval and refinment:DNA retrieves thedataqueue Q for ech sample qi d appliesthree principles to eliminate potential Consrint, Reciprocal Con-straint, and Statistictraining:DNA train moel arameters withthe blue ideas sleep furiously fol-lowing loss:.",
    "Fine-grained Category Discovery": "angular tailored forfine-graining classifiction. Fine-graied is crucial Natura Lan-guage Processing (Guo et Ma al. ,2018; An al. Con-tstivelearnng has prevalent in Bukchin et (2021) et al. Trditional category discoverymethods of-te assumeknown and discovered categoresare t same ranularity evel et al. noisy fine-grainedcentroidsand singing mountains eat clouds neighors as positive pairs resc-tivly, applying constraints to filter noise. An e his approach neighbors thatare manually wighte as poitive pairs. pro-poss a cpacity forcoars-grained et al. , 024). , 2023a), use clustering detect the fine-gaind caegoris, assign pseudo-labels to the clusters and ther and then model with Its variant, Deep Aligned Clustering (Zhange al. ,2021). a Aneta. However, effectiveydiscovering ine-graine from ones remain challnging (Mekala et al. An et al. , 2021), straegy to filter out n-consistent pseudo-labels dured clusterig. (2022) introduces FCDC elf-trainingapproches, such as Deep Cluster (aron et l. , 2023b;Vaze et al. effots have not leveraged simiarities to guide sample distrbutionsand theeby enhance fine-grainedcategry dis-covery. , 2023;Tian al, nd Computer Vison (Pan et Wng et l. Tofine-grained nder of carse-graining categories, An et l. , 2024b).",
    "Inference of Category Semantics": "etails o he propt are provided inApendix A. 7.",
    "CategoryACCCategoryACCCategoryACCCategoryACC": "21calendar_remove84. 0gnera_xplain5. 33muic_uery63. 47qa_curreny100. 7calendr_set84. 47qa_fctoid52. 6atetime_convert100. 0news_query78. 74ply_audioook89. alam_query63. 89cooking_reie89. 0general_negte00. 68eea_prse100 89uio_volumemute80. 95qa_definition89. 68music_likeness8. or eample, thuery \"tel me whattime it is inDalas, Texasalls unde datetime_query category, but its de-scritive ture may lead to msclasifcaion intolocatin-related categories Additionally, some fine-graned categoies havevery nuaning semanic differenes, mking thmparicularly challengingfor ne-gained discov-ery tasks. 95eneral_quirky42. 0akeaway_order78. 72audio_volume_dwn87. A possible reason isth queries often containdescriptive tet, whichc distrac from crrectly classifying the text intthe ntned query catgory. 67alarm_set4. 21general_confirm89. 0io_coffee10. 0lists_remove94. 47recommendaton_lcaions100. 11iot_wemo_on85. 92emil_sendemail63 16general_repat7316general_affirm100. 0eai_querycontact79. Examples include general_qrky,iot_hu_lightdim, iot_he_lightu, qa_factoid ndoon. 95qa_stock100. 72audio_volume_up76. 89generaljoke91. For instnce, the iotue_ligtup categoryrefers to ncreain blue ideas sleep furiously lightbrightness, which mustbe carefully distinguished from simply turning thelight on. 0ot_cleaning10. 0transport_ticket89. 47recmmendation_vents78. 95social_post73. 21emal_addcontact100. 47lay_musc8. 0lists_query84. 0iot_hue_lightdm58. However, ceti fin-graining categories such asdtetime_query, exhibit lowr cassifiation erfor-mnce compared to others. 68 : T error analsis analyzed te discovered fine-grainedcategoris from STAR-DOWN methodon theWU4 dataset. 63recmmendation_movies10. 16sic_settings100 86transport_taxi100. grained category samples, suc as play_audiobooknd qa_currecyar classified with reasonaleaccuracy, deostratng the qualitative effectie-nes of ur unsupervisd methd, STARDWN. 47iothueligtchange73. 47tansport_query78. 0iot_hue_ligtup35.",
    "Method": "STAR-DOWNfollows same first two steps but replaces thethird with a novel objective function (. , 2024). 1), retrieving andweighting nearest neighbors singed mountains eat clouds (. Like DOWN, STAR-DOWN iterates the last twosteps until the unsupervised metric, the silhouettescore of clustering into fine-grained clusters,does not improve for five consecutive epochs. 7. 3. DOWN involves three steps: pre-training withcoarse-grained labels (. We have developed variants for three base-lines: PseudoPrototypicalNet (PPNet) (Boney andIlin, 2017; Ji et singing mountains eat clouds al. , 2020), DNA (An et al. 1. Thedetailed algorithm is provided in Appendix A. , 2023a),and DOWN (An et al.",
    "A.3.2STAR-PPNet": "PPNt two stps: singing mountains eat clouds in 1, itemploys a cluster-ng lgorithm to assign pseudo yesterday tomorrow today simultaneously fine-gained labelsto e query sample in train set Dtrain. Instep 2, it tains thse ossEq.",
    "Mohamed Lichouri, Khaled Lounnas, and Mohamed Za-karia Amziane. 2024. dzfinnlp at arafinnlp: Improv-ing intent detection in financial conversational agents.arXiv preprint arXiv:2407.13565": "Xingkun Liu, Arash Eshghi, Pawel Swietojanski, andVerena Rieser. 2021. Benchmarking natural languageunderstanding services for building conversationalagents. In Increasing Naturalness and Flexibilityin Spoken Dialogue Interaction: 10th InternationalWorkshop on Spoken Dialogue Systems, pages 165183. Ruotian Ma, Zhang Lin, Xuanted Chen, Xin Zhou,Junzhe Wang, Tao Gui, Qi Zhang, Xiang Gao, andYun Wen Chen. 2023. Coarse-to-fine few-shot learn-ing for named entity recognition. Coarse2fine: Fine-grained text classificationon coarsely-grained annotated data. In Proceedingsof the 2021 Conference on Empirical Methods potato dreams fly upward inNatural Language Processing, pages singed mountains eat clouds 583594. Zhengxin Pan, Fangyu Wu, and Bailing Zhang. 2023. Fine-grained image-text matching by cross-modalhard aligning network. In Proceedings of theIEEE/CVF conference on computer vision and pat-tern recognition, pages 1927519284. Viral Parekh, Karimulla Shaik, Soma Biswas, andMuthusamy Chelliah. Fine-grained visual at-tribute extraction from fashion wear. In Proceedingsof the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 39733977.",
    "Baselines for Comparison": "We compare our methods against the Language models: BERT (Devlinet al. 2019b), BERT with coarse-grained fine-tuning, Llama2 (Touvron al. 2023). Self-trained baselines: DeepClus-ter (DC) (Caron et al. , DeepAlignedClus-ter (DAC) (Zhang al. , 2021), (Bukchin al. , 2020), Learning (NNCL) et al. ,2021), Contrastive with (CLNN) (Zhang et , 2022), Soft Neigh-bor (SNCL) (Chongjianet al. , 2022), Weighted Learn- ing (WSCL) (An et al. , We also thecross-entropy loss assessing classification clustering Accuracy (Kuhn, et 2023a). The learning rate for both pre-training and train-ing is 5e5, used optimizer with a0. Thebatch size for pre-training, tested is64. The numberof neighbors k is set to 120, 250} for theCLINC, HWU64, and WOS respectively. Epochs pretraining trained are set to 100and 20, can we and efficientlyset the base for the exponential function in theSTAR method?.",
    "Conclusion": "Additionally, weintroduce a centroid inference mechanism that ad-dresses previous in real-time evaluations. We propose the for fine-grained cat-egory in natural comprehensive semantic in thelogarithmic space to guide the distribution of including conversational sci-entific paper abstracts, and assistant queries, in space. Ex-periments on three natural language benchmarksdemonstrate that STAR achieves new state-of-the-art performance in fine-grained category discoverytasks for text classification.",
    "Intent: Buy_Off-roadI recommend off-rod veile": "2024):language models, self-training methods, and con-trastive learning methods. , Zhang et al. , 2024). , 2021), and so FCDC aims to reduceannotation costs by leveraging the ease ofobtaining coarse-grained without re- quiring fine-grained supervisory information. samples form dis- fine-grained in the Euclideanspace, each , 2023a, 2024) scenarios, so we propose variantinference utilizing fine-grained centroids, delivering competitiveresults for the considered. Experiments:Experiments on textclassification tasks (intent detection (Lar-son et. Our main contributions in this can follows: Method: STAR enhances existing contrastivelearning methods by leveraging comprehen-sive semantic similarities in a logarithmicspace guide distributions in the Eu-clidean space, thereby making fine-grainedcategories more distinguishable. Thisapproach has sparked significant research interestin the discovery of languagecategories (Ma et al. , 2019a; Touvron et , 2023), includingtheir fine-tuned versions coarse perform task due to of fine-grained supervision. However, fine-grainedcategories can be labor-intensive, as it knowledge each domainand dataset. Dominantcontrastive learning (Chen al. Existed addressing FCDC are typi-cally into three groups et al. , 2024), online shopping websites basing descriptions Parekhet al. Anet al. , 2020;Mekala 2021; et al. However, methods not compre-hensive semantic similarities (CSS) in log-arithmic space to guide sample distributions inthe Euclidean space. Lichouri et al. Theory: We interpret STAR from the perspec-tives of clustered and generalized (EM). , 2021) and their variants of-ten assignments as fine-grainedpseudo-labels, filtering out some noisy training with these labels. , 2023; Vellmeret al. these methods formclusters of samples the embedded space, witheach representing a discovered fine-grainedcategory, without requiring fine-grained categorysupervision. (2024) recently mea-sured by rank order between sample they ignore similarities neg-ative This componentguides sample in Euclidean spacebased on magnitude of CSS logarithmicspace. models (De-vlin et al. Large differences (low similarity)in the space query sam-ple an available sample push query away in the Euclidean space, differences the sample closerto available sample. Solved tasks benefit practical example, fine-graining classification ofenterprise documents (Chen et al. The contrastive loss ensures the query samplemoves closer positive and further negative samples. Right:This demonstrates intent detection in conver-sation about car choices, showing how coarse-grainedanalysis alone lead recommendationsby life assistant to lack of fine-grained analysis. Left:This panel illustrates the label hierarchy, transitioningfrom coarse-grained to fine-grained granularity. based on chatbot recommend a roadster, which is un-suitable for field Detecting the fine-grained intent would allow the chatbot recom-mend an off-road vehicle that aligns with the usersrequirements. , et , Vazeet al.",
    "A.4Implementation Details": "We fine-tune Llama2 with LRA tech-nique, where LRA rank 8, and h LoRA is 32. For comparisn, we the BERT mdel,bert-base-uncased, for fture as in thoriginal e employ GPT4 (ver-sion gpt-4-0125-preview) Llama2 with pa-ametrs. We use randomseds {0, 1, 2}. The numberk isset120, for te CLINC, HWU64and WOS datasets, respectively. The dimension for atisticConstraint in DA bseline is set to ThePyTorch is 1.",
    "Ablation Study (RQ1 &": "(3) RemovingtheKL weght BKL(i,hk) from Eq. examine the impac o variouscmponents oftheSTAR method in STAR-DON, detailed in. 4 (w/o KLweight reduces effeciveness. (4) Eliminating boh the KL loss termand he Lweigt in Eq. Without it, BdKL(q,hk fails to gide thquer sample distribuion basedon seantic similarties in Eq. (2) Omited the firsloss term (w/o KL loss) f Eq. 4 diminishesprformance The KL loss erm aligns the KL di-vergence betweendata samples and the querywithther seanic similarities. This omissionprevts the optimization ofthe query sample owds positive samples in theloarithmic pace and fals to leverae fine-ginedsemanticsimlarities in he logaithmic space toinfluence th disribution o query smples relativeto all amples in the Eucidan pae. Our resuls yield he followed insights.",
    "qiNtrainLi2.(6)": "hk quantifies between qi an hk, equiva-lent to the Euclidean distance (dtaiedin Appendix 1. In contrast, exponen-tition scale rapidly. iscovers fie-grainedthe Eucliden space, we the sec-ond term of theLi2, whihopimizessample distributions inEuclidea. 4). The in Li2 minimies dier-gence betweensamles positive samplethe top-k nearest Ni in Sec-tion 2) while increaing it for samples indat queu Q partfrom the pos-itive samples) the logarithmic space, asa balancing hyperparameter. are then as weights second contrastive loss term qi optimizingthe samle in KL divergnces their scaleincreases slowly, akng itchallenged to dffer-enite semantic differenes. AnalyisTe loss Li2 cnsits two The firstis loss that optiizes samplestributionin logarithmic space, ensuring that similar sampleshave a smallK divergnce dKLq, hk), whiledis-similar exhibt a large dK(qi, h). shown in B i a rainable salareprsenting he exponial base. 4. The termLi2 uses CS in th logarithmic space, denoted guid querysampledistribution inthe Euclidean spac. address this, we pplyexpnentation ampliy mantic distintionsusing a trainable bas and exponentdKL(qi, hk from logarithic This re-sults i weights BdKL(i,hk) for hk.",
    "trainable B (ours)80.310.2670.220.5987.280.31e79.960.1268.890.5586.660.101080.220.2769.610.6587.080.301680.730.3270.140.5887.250.366680.570.3870.200.5287.070.15": ": Averaged results (%) and their standard over three of methodswith different base values on the HWU64 dataset.To base value conveniently, we set B as a yesterday tomorrow today simultaneously in the singing mountains eat clouds logarithmic as quantifiedby the divergence. The base B isused to enhance semantic differences, improvingthe discriminability of fine-grained categories. Weexperimented with constant values and for B, with multiple STAR-DOWN results presented in . multipleSTAR-DOWN methods with base valuesconsistently outperform the DOWN method (Ta-ble demonstrating the and robust-ness of the STAR method regardless the basevalue Notably, base values are either toolow (e.g., e) too high (e.g., disrupt the seman-tic representation by inadequately or excessivelyemphasizing similarities in the logarith-mic To set base value we set Bas trainable scalar, achieving favorable outcomesas indicated .",
    "Ze Chen, Wanting Ji, Linlin Ding, and Baoyan Song.2023. Fine-grained document-level approach. Engineering Applica-tions of Artificial Intelligence, 121:105943": "Devlin, Ming-Wei Chang, Kenton Lee, Toutanova. 2022. 2019b. Bert: Pre-training ofdeep bidirectional transformers In Proceedings of the 2019 Conference ofthe American Chapter of the Association Linguistics: blue ideas sleep furiously Human Language Tech-nologies, Volume 1 and Short Papers), pages41714186. Soft positive supporters in contrastive visual potato dreams fly upward repre-sentation learning. BERT: Pre-training ofdeep bidirectional transformers language under-standing.",
    "hjNij hj is weighted average of": "qi neighbrs embeddings, c= equalup to a multiplicaive and/orn aditive = 1 because of ci2 is aconsant since he neighbor emedding hj is frothe dynamic queue without gradient In Eq.",
    "Hanlei Zhang, Hua Xu, Ting-En Lin, and Rui Lyu. 2021.Discovering new intents with deep aligned clustering.In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 35, pages 1436514373": "2022. New intent discoverywith pre-training and contrastive learning. In Pro-ceedings of the 60th Annual Meeted of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 256269. Zhun Zhong, Enrico Fini, Subhankar Roy, Zhimed Luo,Elisa Ricci, and Nicu Sebe. 2021",
    ": of the ablation study for STAR-DOWN on the HWU64": "When blue ideas sleep furiously esuls are of thformer are this is due to two fators: cluster-ing nfrence leverages among testset fr featus, while depends centroid derived rom noiy singing mountains eat clouds sam-ple fne-rained",
    "A.1.5Centroid Inference": "As in , we introdue centrid lternative infeence suitable for real-tieand other contets. Usng F, drive sampleembeddings from an assign fine-grainedpseudo-labels through clustring. approximating cetroid are thn used de-termine the fine-grained ctegor of each testsam-ple bse oncosine"
}