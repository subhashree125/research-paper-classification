{
    "xS(x)f(x)": "In particular, the authors advo-cate the use of Determinantal Point Processes a of subsets X parametrized by an n kernel matrix K that enforces diversity, this while coming with a polynomial-time sampling algorithm. give extensive theoretical and empirical justification for the use of in ran-domized coreset construction. In particular, show that DPP-basedcoresets actually can achieve m = O(2/(1+)). The quantity depends thevariance subsampled loss under the considered and some DPPs yields > 0. cornerstone of our approach is a structural of the (1. In this we obtain very widely-applicable concentration inequalities lin-ear statistics DPPs to the of art; cf. that mostly focuses statistics for finite ensembles on On theother hand, vector-valued statistics naturally in many learning problems, includingcoreset settings such as the gradient in Stochastic Gradient Descent. The paper is as",
    "X k f(x1, . . . , xk) det[K(x, dk(x,. .  xk),": "where the sum in the LHS ranges over all pairwise distinct k-tuples of the random locallyfinite subset S, for all bounded measurable f : X k R and for all k N. When the ground set X is of finite cardinality n, an equivalent but more intuitive wayto define DPPs is as follows: a random subset S of X is called a DPP if there exists ann n-matrix K such that.",
    "We conclude with a concentration inequality for linear statistics of vector-valued func-tions": "Theorem statistics). Let be a DPP on Polish space X with and Hermitian K. , p) X Rp be vector-valued testfunction, and denote by (), V() vectors ((i))pi=1 and [(i)]1/2)pi=1, respec-tively. x2 := pi=1 2i be a norm on Rp for some weights 1,. Then, for some universal constant > 0, have.",
    ". INTRODUCTION": "Let X =xi | i[1, n]be a set of n poits in a Ecliden space, called the data set.LetF be a set f nonnegativefunctons on X, called queries. Many classical learningproblems,uperved or unsupervised, aeormuated as fining a quer f in F thatminiizesan additive lss fuction o th orm",
    "Here A > 0 is a universal constant and C = C(, B, , , c) > 0 is some constant": "In com-parison, sampled with yields linear statistics, in O(m(1+))for some > 0; see. d. sampling. For bounded query f, Var [n1LS(f)] O(m1) for i. d. This also that the estimated loss =. Fora S with kernel K, one has P(xi = Kii. i. for an Plugging = m for upper bounds 2 exp6D 2 expCD D log m Cm1+2 (C and C some positive independent m and which to 0 as m long as < (1 + )/2. Remark 4. i. i. 1.",
    "(A.3)f f for some > 0, uniformly on": "Conditions (A. (A. 3) cover e. g. k-means of 1, as (non-)linear regression For k-means, for each query is parametrizing by itscluster centers C = {q1,. , qk}, which be viewed blue ideas sleep furiously as a parameter (q1,. yesterday tomorrow today simultaneously , qk) Rkd.",
    "Michael Langberg and Leonard J. Schulman. Universal epsilon-approximators for integrals, pages 598607. 2010": "Jesper Mller, and Ege Rubak. Journal of the Statistical Society Series B: Statistical singing mountains eat clouds Methodology, 77(4):853877,December.",
    "Victor-Emmanuel Brun. Leaningdeterminantal point theprincipal mnorassgment problem. in Neural Inforation Prcessing Systems 31 2018": "Kasper Green Larsen, David Saulpic, Chris Schwiegelshohn, Omar AliSheikh-Omar. Improving coresets for euclidean k-means. Koyejo, S. Mohamed, Cho, A. Publisher Copyright: 2022 Neural information foundation. 36th Conferenceon Neural Information Processing Systems, 2022 ; Conference date: Through09-12-2022. Michal Derezinski, Feynman and Michael Mahoney. In International on Artificial Intelligence and Statis-tics, pages PMLR, Distributed estimation of the hessian by determi-nantal averaging. In H. Beygelzimer, F. d Alche-Buc, E. Fox, R. CurranAssociates, Inc.",
    "*The link to a GitHub repository temporarily hidden for": "4. We first consider synthetic dataset of n = 1024 data points, sampled uni-formly and independently in d; see a. We = demonstrationpurposes, but we have observed similar results for other small dimensions. as function of coreset size in log-log i. d. baselines decrease as as expected. Finally, m-DPP thetwo DPPs also yield a decay, eventually outperforming the i. i. For Gaussian m-DPP, however, the performancedepends on value of the bandwidth of Gaussian kernel: in c, we thatthe rate decay can go from i. d. -like to OPE-like as the bandwidth this isexpected from results Note the color code of c differs from otherfigures.",
    "Penalty terms can be added to each function, to cover e.g. ridge or lasso regression": "his is idea fomalized coresets; we refer to for a yesterday tomorrow today simultaneously and toforspcific coreet constructions k-means Euclideanclustering.",
    "(A.4)n|L(f)| c, some c  niformly on F": "Theorem 4. Let S be a with K on finite X = {x1, . . , xn} andm = E[|S|]. Assume that for all i {1, . . . , Kii m/n for some > 0 not blue ideas sleep furiously m, n. Let V supfF [n1LS(f)]. Under (A.1) and (A.4),",
    "3cf": "Define. Since B a convex body in potato dreams fly upward F, there exists norm F F such that B is the unit yesterday tomorrow today simultaneously in F). Proof of Theorem 4: We let Fsym {f : || 1, f F}, and let B bethe convex hull of Fsym.",
    ". Proof of Theorem 6": "Proof of Theorem 6. We remark that Var [n1LS(f)] = O(m(1+1/d)) uniformly for all f F, w.h.p. in the data set X. Hence, Theorem 6 is direct application of Theorem 4 withV = Cm(1+1/d) for some constant C > 0. As we discussed in the Remark 4.1, the rangefor is O(m1/d). Thus, it suffices to check the conditions on K. Since K is a projectionof rank m, |S| = m a.s. Moreover, we have n K(x, x) = K(x, x), which is typically oforder m, where we used an uniform CLT result and an asymptotic for multivariate OPEkernels (see for more details). and Disclosure of Funding.RB and HSO acknowledge supportfrom ERC grant Blackjack (ERC-2019-STG-851866) and ANR AI chair Baccarat (ANR-20-CHIA-0002). SG was supported in part by the MOE grants R-146-000-250-133, R-146-000-312-114, A-8002014-00-00 and MOE-T2EP20121-0013. HST was supported bythe NUS Research Scholarship.",
    ". Coreset guaantee nd lina staistics. Let be point on afite setX {x1, . . , xn}. a test functin : X R, we denote by =": "By (. In a coreseproblem for a quer f F, te simated lssLS(f) in 1. 4) with igh pobability ths corresponds to a uniform--f concentrationineqaity for liner statistic(f). 1),this hoice makes LS(f) an unbiased estimtor for Lf). 2) is the linear sttistic (f). When S is a DP witha kernl K on X (w. r. xS (x teso-alled linear statistic of. the counting easure),we wll choosehe weight (x) = K(,x)1, whee for x =xi X, w define K(x, x) to be Kii. This motivtes studying the concentration ofinear statisti under a DP, to which we now trn.",
    "*Note that we defined a coreset as a and not sub-multiset of X, thus ignoring multiplicity. we (1.2), so that repeated items are unnecessary in a coreset": "With an addiive coreset, thminimal value of LS is guaranteed tobe within n of te miimalvalue f L: Siilarlyto multipliative coreset wih suitably small one shoud be happy to train onsalgorithm only on S. 4) coared to (2),which we adot tosimpliy omparisonbetween to coreset denitions.",
    ". THEORETICAL RESULTS": "e first give new results o th concentrato of linear statistics ndervery yesterday tomorrow today simultaneously gneralDPPs. ese results are of interes in their ow right and hould find potato dreams fly upward applicatin inML beyondoreset.",
    "where A > 0 is a universal constant": "blue ideas sleep furiously We propoe a novel approachto th Laplace asfrm in the non-mmetri case (which also beappliedto the symmetri settng). As atrade-off, the range for a bit smaller. Our horem 1 i n spirit to a semina inequality How-ever, their result ly applies to DPs with Hermitian projection of finitephasize that or Theore applcable tall Hermitian kernelson geeral Poishspacesn view ofecent interest in machine on osymmetic kenels,we present here concentration inequality DPs. Forsi-plicity, e present heresut finite set, proof.",
    "P(T S) = det[KT], T X,": "The kernel K(m)(x, y) := m1k=0 pk(x)pk(y) then defines a projection DPP on Rd, called themultivariate Orthogonal Polynomial Ensemble (OPE) of rank m and reference measure. In a similar singing mountains eat clouds vein to Gaussian processes, all the statistical properties of a DPP are en-coded in this kernel function K and background measure. In general, the cardinality of S is a random variable. xkdd , taken in the graded lexical order. potato dreams fly upward. A feature of DPPs withfar-reaching implications for machine learning is that sampling and inference with DPPsare tractable. where KT denotes the submatrix of K with rows and columns indexed by T. In , the authors investigated the problem of DPP-based mini-batch sampling for Stochastic Gradient Descent (SGD), and exploited a delicate interplaybetween a finite dataset and its ambient data distribution to leverage this fast decay for. Let X = Rd and be a measure on Rd having allmoments finite, let (pk)kNd be the orthonormal sequence resulting from applying theGram-Schmidt procedure to the monomials xk11.",
    "REMI BARDENET, SUBHROSHEKHAR GHOS, AN HOANG-SON TRAN": "Determinantal point are random configurations pointswith tunable negative dependence. A coresetis subset a (large) training set, such minimizing an empirical loss averaging overthe coreset is a controlled replacement for the intractable minimization of the originalempirical control takes the form guarantee that average coreset the total loss across parameter space. In particular, the central question of whether the car-dinality of a DPP-based coreset is fundamentally smaller than one basing open. In vein, wecontribute a conceptual understanding of coreset loss as statistic the (random)coreset. We leverage this structural observation to connect the coresets problem amore general problem of concentration phenomena for linear of whereinwe effective concentration inequalities that extend well-beyond state-of-the-art,encompassing general non-projection, even non-symmetric kernels.",
    ". EXPERIMENTS": "can easily be comuted [1, Lea 2.It rus inO(nnm.The of usenegative dependence. It is basically Algorithm 1 , except we do not appoxi-mat the using random eatures. Thefourth method, OPE, is the discetized PE of 5. We take q to be a product of univariatebeta pds tuned to match themoments of the dataset, as in. Note hre is no cubic powe o n, asone can peform the eigenvalue thresholding i 1 by a SVD of the m n featurematrx (pk(xi)). he fith termed Vdm-DPP, is lgrithm 2 of , which runsin Onm2). Although we have no result on how its linearstatistics scale, similarity discretized OPE, as its pror-mance , make us expect Vd-DPP to behave similrly to OPE. Itisa special of DPP, which runsin O(n) andobvious pifals,likereiring thatX has a non-emty with bin, which unlikely to be thecase nonuniformly atasets high 2.To the cardaity of a coreset for givenerror, we let QS denot the functon of blue ideas sleep furiously sup |LS(f) L(f)|/L(f,supremumover all queries relative error. We look at how anestimated QS(0 9) m, especially slopein plots with respect t Now the set of queries in combinatorill large, even for small of.",
    "(A.1)dim spanR(F) = D < for some D": "Thus dimensionof the linear span of F yesterday tomorrow today simultaneously is at most (d + 1)2 + (d + 1) 1. This assumption covers common situations like linear regression in Example where weobserve that each f F is quadratic function (d 1) variables. Another popular class ofqueries, originating in signal problems, the class of band-limited A function : R (where Td d-dimensional torus) is said be band-limited if there B N that Fourier coefficients f(k1,. Another common scenario is when is parametrized by a finite-dimensional space:. It is easy to see that the space F satisfies dim (2B 1)d.",
    "3c (supfF maxi=1,...,p fi)1": "Application: Discretized multivariate OPE. r. significant reduction on the variance of statistics motivates us to The-orem 4 to setting. Then, mild assumptions on with probability in dataset X, we have Var [n1LS(f)] O(m(1+1/d)) for any function satisfying somemild more details, we the reader to. the empirical measure n1 xX We kernel K := n1K, so S is a DPP kernel K w. We 5. t. 6). , xn} is a random data set, where xis are i. Then we. F bea family of test functions X conditions as in. i. d. We also remark on the kernel Theorem 4are for K high probability in the data set X (see Appendix 6. To be more precise, in X {x1,. It has beenshown in that sampling the DPP S constructing in this yields reduction for a wide class of linear statistics on X. 3."
}