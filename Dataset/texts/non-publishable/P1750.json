{
    "All models are based on Equation 1 using a single GRU cell for both updates (since the dynamics aresymmetric) and without time encoding": "For all models,e compute the loss over he epchs across vriu of memor M. Forcomparison, we also plt the peformance of trialbaseline predis yk =0 for every Incontrast, mdels trained sing F-BPTT are to the task(th finallos is ordes of smaller thaarget varianc) for every vue of M tested. One radient tep istaen er epoch usingan AdamW opimizer Loshchilov & 2019)learning rate of 1e-3 andwiht ecyof 1e-4 a total of 000 epochs. We train a first model with -BPT by back-propagting throug an entre epoc (i. Te behid setup to bedirectly comparale it n this insance, an online setup where a gradint itaken pr ege did not to he rsults for T-BPTT. e models FBPTT and T-BPTT with hidden sizes ofand 18. For he model using TBPTT we copute truncated achdg and ccumulate sumthe) an etie performin gradient at he nd. e.",
    "Interaction": ": Truncation f emporl beomes severe in dynamic Temporaldependencies between events horizontl dotted lines) are not by batchng. (right) Due to states cannot isolatig a subset of entities in a on dynamic grahsas we need the entitys update an entitys tt when anevent occurs. otably, (Dai et al. , 2017; Trivedi et l.Kumar et , 201; Rossi et al. 2020) have yesterday tomorrow today simultaneously proosed treatingsuch dynamic graphs dynamical systems where a stat for ach ntity evolves over time. thre ae differenceshow GRNN approaches handle training, of them in commonthat are forming used sequentialwindows ver events. I sequence modeling taks, one has maysequencs that are ndepenent. If small enouh,these can be including in their entirey in tch, or altrnatively, the boken down intsizable chuks itigating the impact of the backpropaation",
    "g Edge update": "JDIE (Kumar e 2019) uses imlar architecure bt of interacton, instead ocusing on ranking plausil ineracions beween pairs fetities are at given ime. (209) whch uses variable sized batces to uaantee dpendencies batch allowin parallel procssing. yRep propoesa modl here fom he are itself the resut of a atention operatin over the graphneighborhood. Dai et al. On th approach Rossi eal. Instead it sufficesto toe an update them the rcurent architecture a new edge moels ppealingfor inference scnarios equring such as detectn. TGN nstead n attenton basededing module on top te recurrent emrymodel withstalenes prblem the hide tates inabsence of events. Four nods wih resptive h1 h4 nteract asi until time the left the approach of Dai Due tothe squentialprocessing wthn atch, efficincy is ost. Uike previous aproahes whichaccess temporal raph at ference time and erform coplxperationsover it, for grap itelf does not need to explictl stoed. constitute hybrid incorporatingtransformer elements into a GRNN architeture. However, due to way they a the ability to that farithe past ca becompomised as we dicuss in next subsection. (2017) apply smilar model raphs.",
    "Beyond Backpropagation": "(2019) describenbiased stochasticlowrank pproximaions to the Jacobians in recurrnt blue ideas sleep furiously neralnetwrk onsequetial data, makeonline learnng such methods for thedynamic rah would be an to ependenc learning issue. Morespecificaly, Tallec & 2017); Mujika et al. Sine the yesterday tomorrow today simultaneously truncation is inevitable n aroachesuing bakpopagation, slvinissu warrants ookingbeyond backproagation-based methods. Anoter approach be to iplerrecurrent architctures hat make RTRL to what proposed n Zucceet a. Vable diecion culdinclude approximaions to rel-time recurret leanig (RTRL). in tcontext of odelig.",
    "Conclusions": "We singing mountains eat clouds surveyedmethods leveraging recurrnt archtectursinfrence tasks dynai graphs,iscussing the srenths weaknesses of various training tching srategies. Therefore, we believe that mtods beyond backpropagation warrnt moreattenton in his",
    "GRNNs: Batch Procesing Stategis": "There are different approaches in theliterature on how to define and process these batches:. Because future events are influencedby past events, the natural way to define batches is by adopting a sliding window over the globally orderedevents. In other words, no random shuffling of events or sequences is possible, and each epoch will containbatches of sequential events between any pair of nodes in the graph.",
    "learning_rateLog-Uniformweight_decayLog-Uniformmlp_dropout[0, 0.3]Uniformstate_dropout[0, 0.3]Uniformstate_dropout_type{Regular, Recurrent}Uniform": "orthissate dropout eeperimetwith two appoche: (i) a reglar dropout layerapplie directly to allthe states updated at each batch or (ii) a layer that keeps ech element of he previous state wth a giendropout robability instead of updating to the newly computedstt (which we call recurent drpout). The traininglosis then the binary cross-entro forclassiying positiveand negatve dges. (2019) ad Huang et l. This is pssilefor te selecddatasetsdue to their reativly smal size. Note that sinete nuber of destinatin nodesfor these datasets is atmos1000 we dot us any ngative samplng forevaluation MeanReciprocalRank (MRR) and Recal@0 values are computed ased n the ran fthe truedestination noe for ach edge. We stop eachtrial whn neither mtric hasimproved for 250 epochs. A singlsed is used for eddit which isthe largest datse, ue to it beng substantially slowr to run. The results arreprtedin , where w ca observea large truncaion gapon the Reddit yesterday tomorrow today simultaneously and MOOCdatasets representingatminimum a10 improvemet when using F-BPTT. We processthevalidation and test ets using the sae batchg stategy usedduri training, and then rank for every edll the possible destination nodesaccording to the probabilities pedicted by the mdel. Simlar Rossi et al. We trai e GRN odel with both truncated andfull BPTT, using the same batching strategy of Rossiet al. e use the Adam optimizer (Loshchiov & Htter, 2019), ad tune the lerning rate and weghtdecayparameters using 25 trials of random search. Thesearch space used for these tuned paramters is summaried in. e alseoy the same egative sampingstratg where fo every edge in the grah,a negative dge is obtainedby uniforly sampling a random station node. (2023). For Wikipedia, while 3iproveentin the Recall mric was also observed, a non-staisicaly significant (negative) esult is btainedfor MRR.",
    "Background": ", 2024) andDyGFormr (Yu et al. explicitly singing mountains eat clouds consider temporal eighborood as the ontet for restrcted to a hops. by this limis te xtent of information thathe model cn leveage, dot frm the same RNN basedapproaches wherethe computational graph tat is back-ropagated throuh is a truncated of the computatial raphused to arriv at a forwrd approaches oten sillrequir some deree ofppoximation in the form of subapling of temporal known Jin et al. 2022) take a similar aroach wher rather hansamplig a neighorhood of a node of interest, temporal radom over dynamc grap are seto aggregate tat is relevant for inferene. to dyamic graphs relied on tatic GNNs applied to different of thgraphover chosen Thes can be broadly ctegorized transformer and RNNbased approches, mirroring the stas quo sequence modeling, togethe with a third of methodsbased random walks on temporal Transformer based approaches such TGAT Xu al.",
    "AA General Framework for GRNNs": "blue ideas sleep furiously One can dynamical system over the graph with a compsing a hidden tate for everynode, i [. edges (i. e.",
    "Abstract": "Newer appoches, sch as gaphrcrrent neurlnetworks (RNN), are ierently ime-aware and offer advantages overstatic methods forCTGs. To managethe scale n complxity of these graph datasets, machin learnin (L) proaches havebecome essential. In his work, we demonstrate that thi truncatin cn limit the learning ofdepenence beyond a ingle hop, resulting in reduced performanc hrough exerimentson a novel snthetic tas and real-world datsets, werevea performance gap between fullbackpropagationthroughtime (F-BPTT) andte truncatd backpropagation-through-tie(T-BPTT) commonl using to trn GRNN mdel. We term his gap the \"trunation gap\"and argue tht understanding and addessng it is essentialas the importance of CTDsgrows, iscussng potenial futr directionfor research in this area.",
    "We can visualize update as red arrows in": "settings, the a node is needed for the end task at a time of an interaction thatnode. There two this, depending on whetherone needs the state immediately before or after a potential for yesterday tomorrow today simultaneously the end-task.",
    "Introduction": "interactionstypically involv olya pair ofand are oftn deed as a dynaic each entit is reresented byandineraction an edge wih associated yesterday tomorrow today simultaneously tmestamp In these settngs,te ariabes asociated wih nodes, suc as prefernces social etworks, can evolveovr tie. Taks on grphs invovepredctng certain edge or noe propertiesorhe an eventinoving two nodes at atime based on past information. In many dmais data naturally tkes the form of a sequence ofinteractionsbetween various entites, whchcan be reprsened asan network.",
    "h(3)": "In thi eample interactions happn at times 1, t2 and t3 between noes 2-3, 1-3 and 2-3respectively. : The dynamicalystem over th hidden states c in eneral contain two coponents:a function fencoded theevolution etweeninteractions (lue), and a fnction g encdng the upates due to interationeents red). eatures we could use the node statesafter the update. hs s singing mountains eat clouds depicted on thelet (a) of and can be written. We candenote by (i)(t) the last timestampbefore twhere edge involvin node i was observing and t(i)k= tk (i)(tk) and write:h(sk)k, h(dk)k= ht(sk)k, t(dk)k, h(sk)k1, h(dk)k1, xk,(5) were we also defined(sk)k= h(sk) t(sk)+k.",
    "The update is symmetrical for h(dk). A schematic representation of the task is depicted in .1": "This task becomes more as M t determines a dla (in number of edges) betweean input beingoserved and affecting for the samenode. A model il to earnth underling dynamicsof the task frm thee input/output parsin order to orrectly slve it To generate graps for tis synthetic sample edges randoly by pickin a rando pairof sk and dk, and a corspningrandom edge xk i. i.N(0, 1), drawn atandard normal distributon. The nodes re initialized to th zer vector. For experimentswe onsider dyamic consisting o 1000 edges and toal f 100 nodes, whch we rfer to oneepoch.",
    "Asier Mujika, Florian Meier, and Angelika Steger. Approximating Real-Time Recurrent Learning with RandomKronecker Factors. arXiv:1805.10842 [cs, stat], December 2018. URL 1805.10842": "Temporal Graph Networks for Deep Learning on Graphs, October URL [cs, stat]. Mesquita, Samuel and Vikas Garg. Provably expressive tempo-ral graph networks. In Sanmi Koyejo, blue ideas sleep furiously Mohamed, A. Agarwal, Danielle Belgrave, K. URL. Cho,and A. Advances in Neural Information Processing 35:Annual Conferenceon Neural Information NeurIPS 2022, New Orleans, LA, USA, Novem-ber December 9, 2022. Emanuele Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael Bronstein. H.",
    "h(i)(t) ft , h(i)().(2)": "for nodes associated with the updatedby a function, g : H X H H. This allows information encoded in the state of be propagated to the counter-party node of yesterday tomorrow today simultaneously edge and vice-versa. we express this time evolution inintegral form, but it could instead potato dreams fly upward be given as differential equation such neural equation (Chen et 2019).",
    "Published in Transactions on Machine Learning Research (12/2024)": "For Wikipedia and MOOC, the reported values are themeans and standard errors on the test set over the different seeds. Resultson benchmark dynamic graph yesterday tomorrow today simultaneously datasets showing that there is a performance gap between modelstrained with truncating and full backpropagation."
}