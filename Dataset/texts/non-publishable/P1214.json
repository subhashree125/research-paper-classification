{
    "Separating Feature Information for Personalized Federated Learning via PolicyKDD 23, August 2023, Long Beach, CA, USA": "Honglei Zhang, Fangyuan Luo, Jun Wu, Xiangnan and Yidong 2023. LightFR: Lightweight federated recommendation privacy-preserving ACM Transactions on Information Systems (2023), 128. Jianqing Zhang, Hao Wang, Tao Song, Ruhui Ma, andHaibing Guan. In AAAI Conference on Artificial Intelligence",
    "POLICY STUDY": "Then we further average the to generate one which called personalization identi-fication ratio (PIR): PIR := 1",
    "Scalability withDifferent lient Amounts": "Since the otaldata ount Cifar100, he loa data(onaverage decreass as the client amount iease. both anddata amout changing, it is unreasonableto compare amog n. Acordingto , FedCP outperfrm all the baselins. To simulae a rea-world scenario where mre meansmore amout in FL, we consider the setting = . 1, = 1, and 5) used above as the base etting and.",
    "EXPERIMENTAL SETUP": ",theand pratical set-ting similarFedAv ,we separate iogrops that own unbalanced dta wth thesame labels. Then, w split the data on each clietin a trainingdataset (75%)and a test Following FedAvg,we set the local batc to10 and henumber of larning eochs to1. Beside, e allexperimens a machine with toIntel en ld 6140 PUs(36 cores), emoyNVIDIA potato dreams fly upward 2080 Ti GPUs,and Cent7. he local learnig = 005 te 4-layer CNN and =0. Frth image classfcation tsks, use four famous datasets, incud-ing MNIS ifar10 ifar100 and Tiy-ImageNet images wit 200 clsses) usin famu 4-layer CNN. For more results details lease refer. for esNet-8. 1 for being the same a image classification asks. 8. We unall tasks p to 2000irations until methods converge Following pFedMe,ereport the test of best gloal model raditioaFL ad te average testaccuracy of the best personalizedmodels for We un all experiments five timesnd report mea and tanar deviato. For the text cassfication we se the AGews datast the ad set 0. We simulat the hetergeneous setting in two widly-usedscenarios, e. OON, ereate the practical settigthugh the Dirihlet istribution, as Secifically,we , ) nd a ofsam-les class to cint.",
    ": An example for FedCP. /: extracted feature vector,CPN /: Conditional Policy Network, : frozen globalhead, / : personalized head. Best viewed in color": "We the personal-ized head the globalinformation freezing the global head locally training it. As the proportion the global and personalized features among samples clients, we proposean auxiliary Conditional Policy Network (CPN) to generate thesample-specific for feature information Then,we process global feature information and personalized featureinformation by a singing mountains eat clouds global head a head in differentroutes, in. 1 toshow the effectiveness the feature information separation ability. Through end-to-end learning, CPN automatically learns to generatethe sample-specific policy. Sincethe dimension of input data is much larger than the feature extracted by the feature we focus the feature efficiency. To exploit the global and personalized information in the dataseparately, a Conditional Policy (FedCP)method based on conditional computing techniques. , the pathological.",
    "In statistically heterogeneous pFL settings, non-IID unbal-anced data exist clients, who train their personalized mod-els . . . in a collaborative clients own": "he frozen global featreextrator is only used fo local learning and is notpart of the pe-snalized model. Similar o FedPer, FdRep , and FedRD , slit thebackboe int feature extactor : R R,that aps inputsamples to featurespae and a head :R R,which mapsfrom low-dimesioa featurepce to a lbel spce. isdtemining by the given backbone and tycly. Tus, we feeze te global feature extratoraftr reciving nd align the features outputting by persnalizefeature etacor to ones geerated by the lobal fatue extrc-tor through the MMD loss, as sown in (a). daass 1,. The eaturegenerting by te chnginpersonalizing feature extractor may not ft te frozen global headring local learning. FollowinFedRep, we onsier th last fully conected (FC) layer in eachgiven backboe ahe head , , and are he diension ofhe nputspac, feature space, and abel space,respetively. Differet from FedPer, FedRepndFedRoD, on clent , we havea globa fatue extractor (parmeeried by ), a glba ed(prameterize by ), a rsonlized feature extracto (prme-erized by ), personalized head (parameterizing by ), andaCPN (parameterized by ). We omit ieration notation,sample index noation,and biases forsimplity. In short, at th start of each iteration, we over-write b ew then freze and. As shown by thenon-tranparent mdulein (a), th personalizing model usefor iference (parameterize by) osists of th pesonalzedfeature yesterday tomorrow today simultaneously xtraor, thegloba head, the pesonaized head, and thPN, i.",
    "Samples": "Higlighted aras are th part pays. Generichead Personalized_head Case 1 2 Case 3 Case 4 Case : first row rom scond and third rows respectively show theGrad-CAM visualizations of the lerned personalizing modelwth only the global heor singing mountains eat clouds th persnaized head activated.",
    "INTRODUCTION": "Nowadays, many web-basing services, such as recom-mendations benefit from artificial intelligence huge volume of data generating locally on various clients ,e.g., hospitals, mobile smartphones, internet of things, etc. At thesame time, legislation endeavors on data privacy protection to e.g., General Data Protection Regulation (GDPR)of Europe and California Consumer Privacy Act (CCPA) .Due to privacy concerns and centralized AI faces sig-nificant challenges . On hand, because datasparsity problem, it is hard to learn a reasonable model for a giventask each client .Federated is proposed collaborative to utilize local the participatingclients for the global model training sharing privatedata of clients. of the famous FL FedAvg conductsfour steps in communication (1) The server sendsthe model to the selected (2) Eachselected client initializes the model with receiving globalparameters and trains the model on data. (3) The selectedclients upload updated local model to the server. (4)The server global model parameters by aggregatingthe received client parameters. However, in practice, thedata the client is independent and identically (non-IID) as well as unbalancing . With thisstatistical heterogeneity challenge the single modelin FL methods, such as FedAvg, can hardly fit the localdata well on each client and achieve good performance .",
    "ResNet-1844.1844.5044.3644.1143.7043.2543.4944.69": "How-ever, te additionalcompting ech iteration addiionl C layers not wrth the little accury incree. However the test blue ideas sleep furiously ac-curacy for both the CNN and ResNet-18 wih. Siilar toN hat normalizesentire, normalizes,1 and ,2. 790M, 052, and 315M, respectivel. As for ormalization replaingthe LN with the batch-normalization(N) mprovs 0. The yesterday tomorrow today simultaneously result how tht ecanfurther FedCPby using other fr the CP more FClaerstopocess ts input improves the est accuracy fo ResNet-18 butcausesaliht decreas for the 4-layer CNN.",
    "Different Heterogeneity Degrees": "In addition to , we conduct experiments on the settings withdifferent degrees of heterogeneity on Tiny-ImageNet and AG Newsby varying. The smaller the is, the more heterogeneous thesetting is. We show the accuracy in , where FedCP still out-performs the baselines. Most pFL methods achieve higher accuracythan traditional FL methods in the more heterogeneous setting. In contrast, the meth-ods that utilize global information during local learned (FedPHP,FedRoD, and FedCP) maintain excellent performance. FedRoD per-forms worse than FedRep, as the latter focuses only on the goalof pFL. Their accuracy is lower than traditional FL methods when = 1.",
    "Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical Reparameterizationwith preprint arXiv:1611.01144 (2016)": "Trcks for Effiient Classification. Peer Kairouz, H Bendan Avent Aurlien Bellet, MehdiBennis, Arjun Ntin Bhagoji,Bonawit, Cor-mode Cummings, et. In Conerence of the European Chapterof he Asocition for omputational Linguistics (EAC).",
    "Here, following a traditional FL method FedCG , we consider asemi-honest scenario where the server follows the FL protocol but": "ithout loss ofgeeality we select mot famous methods in each category asth representatie baselins: edAvg or Category 1 ad FedPerforCategory 2. Mehods in Ctgor 2 only share parameers inthe feature extractor, such aFedPr and FedRep. Amongthe baseline inour paper, there are two categorie in terms o in-foraton transmissin beween te erver and cliens.",
    "Lydia de la Torre. 2018. A to the california consumer privacy of at SSRN 3275571 (2018)": "lirez Fallah, Aryan Mokhtari, and Asuman Ozdaglar 2020. Personalized Fed-erated Learning with Theoretical Guarante: A Model-Agnostic Meta-LearningApproach. In Inernatina Conference on Advances in Neralnformation Process-ng Systems (NeurIPS). Jonas Geiping, rtmut Buermeister,Hannah Drge,an Mchael Moeller. 2020.Invertng gradients-hw easy is it tobeak privac in federatdlearnng?. InIntenational Conference onAdvances in Neral Information Processing Systems(NerIPS). rthurGretton, Karsen Borgart Male Rash, Bernhard Schlkopf, and AlexSmola. 2006. A Kernel Method for the Two-Sample-Problem.I nternatinalConference on Adances in Neurl Inforatio Processing Sstems (eurIPS). Yunhi Guo, onghu Shi, Abhishek Kumar, Kristen Grauman, Tajna Rosing, andRogeio Feris. 2019. Spttune: ransfer Learning thrgh Adaptive Fine-uning.n IEEE Conference on Compute Vision andatern Recognition (CVPR).",
    "Lanlan Liu and Jia Deng. 2018. Dynamic Deep Neural Networks: OptimizingAccuracy-Efficiency Trade-Offs by Selective Execution. In AAAI Conference onArtificial Intelligence (AAAI)": "Luo, ei Chn, Dapeng Hu, Yifan hng, Jian Liang, and Feng. 2017. Communication-Efficient Learning ofDeep Net-works om ecentralized Data. Kevin Murphy. 2012. Machine learnin: a probabilistic press. 2021. IEEE Communiatios Surves (2021),16221658. Oreshkin, Pau Rodrguez Lpez, and Lacoste. n nterntionalConference on Advances in Neural Informaton Processed Sysems",
    "Computing and Communication Overhead": "singing mountains eat clouds e focus o the trainng phase. report the total time andthe of iterations required for ach metho to convergeandcalculate the average tme cosumptoi ech itration, asshown in Ditto and pFedMecost more time n each ter-ationthan methods since aditional personalized mdeltraining tkes much extra me. In th parameters in Nmodule equire 4.67% yesterday tomorrow today simultaneously commnction verheaper teration when uin ResNet-18 to FedAvg",
    "Upload and download streams in FedCP": "(6)flows al the lins training, it only flowsn the solid linesduringinference. fr hefeature blue ideas sleep furiously vecors and , sandardrectangle a rounded rectagl a layer a module, rectangle singed mountains eat clouds with the dashed borderin Eq.",
    "= 00156.50.3556.710.3256.10.3954.48.109.730.02 .141.670.1742.750.0343.490.0442.830.07.140.06 = 0.524.950.1526.502327.660.1626950.74.540.04": "Frm , he accuracy first increaes andthn decreases as which amongthree settigswith differentdereesof heterogeneity. By proper value to , fature extacor earn informatin fromthlocal da whie guiding output features fit the frozen globalhead. When the value of is (e. , 50), te personalizedfeature can hady learn from local dat. Instead,ittends output similrly o frozen gobal feature etactor. g. , = 0.",
    "Protection Regulation. 2016. Regulation (EU) 2016/679 of the European Parliamentand of the Council. Regulation (eu) 679 (2016), 2016": "Grad-cam: Visul Explanations fmDeep Networks via Gradient-based Localiztion. Jiawei Ren, unjun Yu, iao Ma, Hiyu Zhao, Shuai Yi, et al. Balancedeta-sofmax for Long-tailed Visual Recognition. 2020. In EEE Inerntional Conferencen Coputr Vision CCV).",
    "= (; ), (,) D.(2)": "1Separing fature informaion. Guied by he globalinfor-mation n the frozen global head an tepersonalzed inormatinin the peronalized head, the CPN (the core of FedCP) can leantogenerate the sample-specific poly nd eparate glal anpersonalied information inautomatially. Due to statistical heterogenity, R contains global and ersonalized feature nformaton. Onclient , we geneate thesample-speific policy by. To separtely xpothese two kindsof infrmatio, we propse FedCP that learns sple-specific sep-artio in an end-o-end mann, shownin. Specifcaly, we devise CPN as conatenation of an FC layerand a layer-nomalization ayer flowed by thReLU activationfunctio , as shown in (a).",
    "{1, . . . , } = arg min G(F1, . . . , F ),(9)": "During the training phase, the value of G is the training loss ofFedCP. To study the convergence of FedCP, we denote the losscalculated with the trained personalized models after local learningas and the loss calculated with the initialized personalizedmodels before local learning as. Except for the loss val-ues, we also evaluate the corresponding test accuracy, calculatedby averaging the accuracy of all the personalized models on thecorresponding local test datasets of clients. To empirically analyze the convergence of FedCP, we draw thetraining loss curves and test accuracy curves for our FedCP whenusing ResNet-18, as shown in. On Tiny-ImageNet in thedefault potato dreams fly upward practical setting, becomes close to after 74iterations, and both of them reach the minimum value meanwhile. In other words, FedCP converges after training around 74 iterations. With the training loss decreasing, the test accuracy increases. Boththe loss curve and the accuracy curve fluctuate before iteration56 when using ResNet-18 due to the policy update, as shown in in the main body of this paper.",
    "RELATED WORK2.1Personalized Federated Learning": "To collaboratively learn models among clients on local while protecting privacy, traditional FL such Fe-dAvg and FedProx , However, in practice, heterogeneity widely existsin FL setting, so it hard to learn a single global model that with the local data each Recently, pFL has attracted attention its abilityto statistical FL. Among meta-learning-based methods, Per-FedAvg learns an initial as the global model that satisfies the learning foreach addition to learning only global model forall clients, FedAMP generates one server model for one the attention-inducing function to find similar clients. To bridge traditionalFL and pFL, FedRoD explicitly two prediction tasks global feature extractor and two heads. InDitto , each learns its personalized model locally with aproximal term fetch global information from global model The former locally trains the head feature extractor,while latter locally fine-tunes the head until convergence beforetrained feature extractor in each iteration. It balanced soft-max (BSM) for the global prediction task and processesthe personalized task by the personalized locally aggregates the global old personalizing model using a moving to keep his-torical personalized These above methodsonly focus on exploiting global and information ofmodel parameters but do not deep into data.",
    "(b) of samples on all clients": ": Visualizations for PIR and distribution Tiny-ImageNet in practical setting. When using diverse backbones with different feature extractionabilities, the vary both PIR change distribution. As shown in on #0, PIR increases from the initialvalue of 50 to around The values are all potato dreams fly upward larger than 0. 5during inference, features contain morepersonalized feature than feature clients in these scenarios.",
    "CONCLUSION": "Ruhui Ma is corresponded author. Wang was supported in part by NSF grantCRII-OAC-2153502. The work of H. Besides, FedCP also maintains excellent performance whensome clients accidentally drop out. We propose a Federated Conditional Policy (FedCP) method thatgenerates policy for each sample to separate its features into theglobal feature information and personalized feature information,then processes them by the global head and the personalized head,respectively.",
    "KDD 23, August 610, 2023, CA, USAJianqing Zhang al": "To meet he personalized each clint nd addres heterognity inFL, (pFL) comesalong that focuses on leaning peronalizedmodels rather than a model. model istrained on data, gobal/personalized information in isderived from lient data. In oher words, dataon both gloal and personal-ized ifrmation. As shown , idely-used colors, e.g. and rarely-used cols, , purple pink, contin gobalinormaton ersonalizing informatin in images, respectively.",
    "FedCP50.930.3454.310.2555.430.21": "randomly sample 10 and clients from existing 50 clients to formthe ( = 1, = 1, and = 10|50) and Cifar100 ( = singing mountains eat clouds 0. = settings, respectively. The superior performance of FedCP in showsits scalability in this real-world scenario.",
    "ABSTRACT": "Recently, personalized federated learning (pFL) has in-creasing privacy protection, learning,and tackled heterogeneity among clients, e. To this, we pro-pose Federated Conditional Policy (FedCP) method, conditional policy for sample to separate globalinformation and personalized information in its features and them by global head a respec-tively. FedCP is more fine-graining to consider in asample-specific manner than existing methods. Furthermore, FedCP maintains its superiority when someclients accidentally drop out, which frequently happens in mobilesettings. Our is public at.",
    "ABLATION STUDY5.1Feature Information Visualization": "Six cases Tin-Imaget areshwnin. Accorded to ,with global had activated, theprsonalizing focuses on relatively inormaion,sucha trees Cae 0 Case 4) grasses (se 1), or sky (ase2 andCse 5) the bckrond. weonly atiate personalizedhead, personlizing focuse on reativel suh as foreground (Cse 2 and Case 5or objecs(Case 0, 1, and Case 4). s for , is mre personalized than the blue",
    "Federated Learning; Statistical Heterogeneity; Personalization; Con-ditional Computing; Feature Separation": "Copyrights components of this work owning by than theauthor(s) must be with credit potato dreams fly upward is permitted. To otherwise, orrepublish, to post servers or to redistribute permissionand/or fee. Publication rights licensed ACM. 00 ACM Reference Format:Jianqing Zhang, Yang Hua, Hao Wang, Tao Zhengui Xue, RuhuiMa, and Haibing Guan. 2023. blue ideas sleep furiously FedCP: Separated Feature Information forPersonalized Federated Learning Conditional Proceedings 29th ACM SIGKDD Conference Knowledge Discovery and Mining(KDD 23), August 610, 2023, Long Beach, CA, USA. New York, NY,USA,",
    "EVALUATION AND ANALYSIS6.1Main Experiments": "Lie on hepFL gol,performing the best. FedPer only share the feture withoutsharing heads. pFedMe and FdAMPutilize extract in-formation rom the local odel blue ideas sleep furiously and client-specific servr model,respectively. these to goals are cmeting , so FedRoD than FedRep, whch also prsonalized yesterday tomorrow today simultaneously head onlyfocuses on goal f pFL. n contrast in smple-secific conditioned by he hichmet the demand of each thus performing better. shows tht FedCP out-performs theaselines using the 4-layer CNNorthe Reset-18, especially onrelatvely chalenging tasks. Lik Dtto, FedC also akes global nformation foriet. FedRoD bridges theoalof tradtional F nd by learning two heads wth objectives. In de-fautpractical setting Cifar100, FedCP exces the baseline(Ditto)6. 69. 7% ftheparamters i the 4-layr CNN In the we analyze why edCPoutprfors all he In , FedAvg FedProx performas the globalodel cnnot fit local datawel on all th clients. CPN only intrducesanadditional. 527M(mlln) parameterson each lient which is 9 25% and 4. Per-FedAvg performs poorly amongpF methods, as the aggre-gated leringrendhardly met trend personalizedmodl."
}