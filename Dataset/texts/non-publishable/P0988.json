{
    "eitian Zhang, Zheng hou, Zhicheng Dou, and Zhao Cao. 2023. Term-Sets Strong DocumetIentifiers For Ato-Regessve Engines.rXivpreprint (2023)": "43204326. Deep interest network or cick-throughrate prediction. In IJCAI. Zhang, ing Zhang, Dong en Yujng singing mountains eat clouds Qi Xng X, HaoSun, Weiwei Deng, singing mountains eat clouds Qi Zhang,Fan Yang, et 223. Tnging Zhang, Pengpeng Zhao, Yanchi Vicor X, eqigang, Guanfeng i, Xiaofang et al. 1016 (2023). Guoui Zhou, Xiaoqiang Chenru Song, Ying Ha h, Xao YanghuiYan,Junqi Jin, Han Li, and Kun Gai. 18931902. In roceedings of 24t ACM SIGKDD international knowledge & data mining. 2019. 2018. Kun Zhou, Wang, Xin Yutao Zhu, Sirui Wang, Zhang,Zhongyuan ad Ji-Rong Wen.",
    "Equal contribution.Corresponding authors": "KDD Augus 2529, Barcelona, Spain 204 held by te wner/autor(s). licensedto This the uthors version of the work. It is posted here for your use. The finitive Version of Rcord published in30th ACM SIGKD Conference on Disovery and Dta Mining 2529, 224, Barcelona, Spain,.",
    "(4)": "e linear on the corresponding outpt of [CLS] and utilize a inear layer with sigmoid activation tocalculate the score +/ for positive/negative amles. also blue ideas sleep furiously build binary lassifiert judgewhether behavior is irrelevant to teseman-ti gloal feature. Fo training, we construct negativesamples y randomy replacing the m% f tokens n the behaviorcodewih the sampe irrelevant. Recognition. Th binarycross-entroy loss is utlized for recognitio, given by:. the feature of gound truth of the maskd token,y is eature yesterday tomorrow today simultaneously of tokens.",
    "RELATED WORK": "Recently, introduced the idea of semantic id, whereeach item reprsened as  set of tokes derived fromside nformatio, and then predicts the nxt item toens in a way. GRU4REC wa the first use GRU-basedRNNs for equential recommendations. Using eep sequetial odels cptre pattern n systems dvelopdinto a ich literature. Forexample cnsrcts a forest b treesndtegates atransformer-based structure for routn operations. In this wor,e aim to urter explor a generationarchiecture to act as ed-to-end index for top-k item retrievl. Notably,DSI and NCI T5 prodce hi-erarchica document Ds. Recommendatin. docment rerival has alsoextended to varous domain studies often face large-scaletem withnsystemdue tothe pre-trand aguagemod-els. eerative retrieal has ben recentlypropose as a new retrieval paradigm, which twomainphass: seantic toeniztion and sequence  In the of documentretrieval, researchers explored the use ofpre-traine languagemodels generate divese types of document identiiers. Besides, have shown prormanc recommnder systems. SEAL (with backbon) and(using uilie titles substringsas identiiers. Anor em-setsfor ientification purposes.",
    "Overall Pipeline": "in our two-stream generation we model history interaction features the encoder. Then we extract semantic features to constructtwo codes, two decoders to separately predict them inan auto-regressive we optimize a summary tokenin our global task and to improve cross-decoder interaction in our semantic-guided transfer we adopt a confidence-based ranking strategy to from two different predictions. EAGER con-sists of (1) a two-stream generation architecture to unify item rec-ommendation both behavior and semantic information, (2) aglobal contrastive task with summary to capture globalknowledge better generation quality, and (3) transfer task to achieve the cross-information andcross-decoder interaction.",
    "demonstrate the effectiveness and robustness of the proposedthree modules as well as the benefits of the two-stream generativeparadigm": "Removing GCT leads to performance drops than suggesting that global information distillation of the inter-code is slightly yesterday tomorrow today simultaneously more important the knowledge flow betweenthe intra-code. The observation also highlights the ofboth in enabling the model to acquire dualitem identifiers. Removing TSG leads to the most performance whichindicates that the base model can enhance its integrated singing mountains eat clouds behavior-semantic information of items.",
    "INTRODUCTION": "Despitprogress, eparate phases ofrepre-setin leaning ad index constructin ofte operate ndepen-dently, presentig for achieved ptmization. Iitially, uers itemsar ecoded int latent repesenttions within sharedlatent spaceusing models like and equentialrecommeation models.",
    "Semantic-guided Transfer Task": "To enable the flow two sides while avoidingdirect interaction, we build bidirectional Trans-former decoder an module. decoding,we further propose semantic-guided transfer to thesemantic knowledge to guide the behavior learning. Through the components, model can two types of blue ideas sleep furiously for prediction. To conduct our transfer training, we designtwo following objectives: reconstruction and recognition. Then the embedding ofthe semantic summary token y[EOS] is input to the cross atten-tion, allowing each behavior token in decoder to overglobal feature of the semantic. However, we donot stop at this point.",
    "TIGER : TIGER uses pretrained T5 to learn semantic ID foreach item and autoregressively decodes the identifiers of thetarget candidates with semantic ID": "EAGER beats the previus moels on mot datasets. Esecially, EAGE perormsconsid-erably the Beuty benchmark o thesecond-best baselinewith up to31. models tradtional bselinesin most cases across daasets. These supeoimprovemets valiate the effctiveness of our desins ad thenecssity ofincororatng both semantic informa-tin. chalenge can be by generative mehods thatlever-age blue ideas sleep furiously beam search strateies t directly predict item codes, the models robustness. 45% and 3. repos the overalof daasets. for all baselines wthout thsuprscript are takenfrom th publicly accessible resuts. 26% yesterday tomorrow today simultaneously improvement in NDCG@ compared o TIGER. For mssing statistics,we the baseline and repot our experimental results. EAGER dffrs from models rough two-stream architecture and multi-tasktrainng, which acilitate undersaning relationships andcaptre crucia global of intr-codes. Results.",
    "Hyper-Parameter Analysis (RQ3)": "Layer Number. It sug-gests that the decoder plays more important in our EAGER. Number. The increase blue ideas sleep furiously potato dreams fly upward inbranch number to corresponding decrease in the length of the item identifier accorded total number of Theexperiment is conducting on two datasets, Beauty and However, interestingtrend emerged on the dataset, where we a declinein performance as increased from 256 512. The",
    "ABSTRACT": "Generaive retrival hasrecently emerge as a promisng approacto sequential recommndation, raming candidate item retrieval sanautoregressive sequence geeration problem. To addressthis limitation e introduce EAGER, a novl enerative recommen-daion framework that seamlessly itegrates both bhavioral andsemantic inforatin. Specificaly, we identify hree key challengesin cmbning these two types of information: a unifiing generativearchitecture capablef handling two feature types, ensuring suffi-cient nd ndependentearing for eah type, and fosteing subtlenteractions hat nhance olaborave inormtio utilization. We validae the effeciveness of EAGER on fourpblicbenchmars, demonstratingit spior performance compared.",
    "CONCLUSION AND FUTURE WORK": "In this paper, we introduce novel framework, EAGER, integrate behavioral semantic for singing mountains eat clouds gener-ative recommendation. EAGER comprises three key components:(1) a two-stream generation architecture that combines behavioraland semantic information enhance item (2)a global task with summary token to capture globalknowledge for improved auto-regressive quality, and (3)a transfer task that facilitates interactions acrosstwo decoders and features. Extensive comparisons state-of-the-art and detailed demonstrate the effective-ness and of",
    "Note that we use \"code\" and \"token\" interchangeably": "hereore, intrduce a crafted to implicit exchang. In EAGER,we introduce a global contrastive task ith a summary token. Thismodule inspiration rom woain sources: (1) traditionaldua-toer us contastive learning discrimina-tive ite features. As mentioned earlier, drectfeature-lvelineractions often yield sub-optimal outcomes. erefore, ur approach separate for behaviorand semntics, employing a wo-stream generation architecturewhere serves  distinct supervision signl at the ecoderside. unified gnerative framwork and them rom the followingtree a unifid generativefor handling twodistinct types ofinformatio is iven th inheren differ-ences in featu spaces behaior seantics, drectlyintgrating through fusion at sde posescllenges, as demonstrated in two-tower models.",
    "Global Contrastive Task": "Summary Token. Here we adopt the positive-only contrastive ofcommonly used to acheve this objective. To thesummary tokenaptueglbal informtio, weadopt contrastive leanin to the item embedding E E from the pre-traed enode. For h input of ch decoder, e order of auoregressie andinsert learn-ale yES] the thesequece to onstruc modifiedinputs {ySOS, y1, y, y, This design encouragesthe recedig in code to lan more compehensiveand discriminative knwlege, enabling the final to During updtes, the gradient on the sumry canbe backpropagated the tkens. Contastiv Distillatio.",
    "Geneatve Atoregressive Generatn SemanticTokenization, Behavior-Semantic Collaboration": "ACM Reference singing mountains eat clouds Formt:Ye Wang, Jaao Xu, Minjie ong, ieming Zhu, Tao Jin, ang in, HaoyuanLi, Li, Yn ia, Zhou Zhenha Dong. EAGER:enerative with Behavior-Semantic Collaboration. ACM, NewYork, NY, USA, pages.",
    "Experimental Setting": "Hre weuse trecategories (Beauy, Sports nd Outdoors, and Toys andames) fo evluations. , Recall and Normalized Discounted Cumula-tive Gain (NDCG). Following , weonly keep 5-core dataset, hich filters unpopuar items andinactive usrs with feertha fiveiteraction recrds. To train our odel, we adopt Adam otimizerwith the learningrate 0. We employ wo badlyused cteria for thematching phase, i e. restaurnts). g. For alldatasets, weroup he intraction recors by users and sort themby he interactio timestamps ascendngly. Folloing th prevous seting ,we only use the transaction recos from Jauary 1st, 201toDeember 31s, 2019. The clster number in hierarcical k-means is set to 256. Folowing the stanard valuationproto-co, we use the leave-one-outstrategy or evaluation. We report mtrics computed o the to 51/20recommended candiates. We condc experiments on fr opsourcedaasetscommonly usd n the euentialrecomendation tas.",
    "We thank MindSpore3 for the partial support of this work, whichis a new deep learning computing framework": "Michele Bevilacqua, Ottaviano, Patrick Lewis, Scott Yih, Fabio Petroni. 2022. search engines: Generatingsubstrings document identifiers. Neural potato dreams fly upward Information ProcessingSystems (2022), 3166831683. Ting Chen, Kornblith, Norouzi, and Geoffrey Hinton. 2020. Asimple for contrastive of visual representations. In Interna-tional conference on machine PMLR, 15971607. singing mountains eat clouds",
    "Two-stream Generation Architecture": "This frameworkconsists of shared encoder for modeling interaction, twoseparate decoders for two-stream generation. With the embeddings, the widely-used hierar-chical algorithm to one, where each cluster is into K child until blue ideas sleep furiously each child cluster merely containsone shown we can obtain two codes Y and Y, corresponding behavior and semantic, respectively. Compared yesterday tomorrow today simultaneously to one shared decoder that generates auto-regressive way, such design mitigates the supervisiondifference and offers higher efficiency with generation. To handle two different information, i. For. g. DIN ) that only uses for recommendation the semantic encoder is a gen-eral modality representation model (e. The sequence modeling user his-tory X = {x1, } is on the stacked multi-head and feed-forward layers, as proposed in It is worth nothing that we only a shared encoderinstead of two encoders, is to generate rich repre-sentation for the subsequent decoding. g. behavior semantic, weleverage the powerful modeling capabilities transformer modelsand design two-stream generation architecture. Dual Decoders.",
    "Beauty22,36312,101198,3600.00073Sports and Outdoors35,59818,357296,1750.00045Toys and Games19,41211,924167,5260.00073Yelp30,43120,033316,3540.00051": "We reconstruct the masked behavior codes viathe semanticglobl feature, aiming to enable the ech behviortokn benefit from the sematic."
}