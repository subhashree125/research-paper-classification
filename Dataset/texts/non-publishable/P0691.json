{
    "Zhuosheng Zhang, Aston Zhang, Mu Li, and AlexSmola. 2023b. Automatic chain of in large models. In The Inter-national Conference on Learning Representations": "2023. Wenha Zhu, Hongyi Liu, Qingxu Dng, Jingjing Xu,Shujian Huang, Lingpeng Kong, iajun hen, andLei Li. 2023a. ltilingual mchine translation ithlarge anguage models: Emirical resutsand analy-si.",
    "Step-1: Difficult Word Detection": "In practice, found that inquirig identify words istacle this challeng, frst apreliminr trnslation for the given source se-tence andthen the wordsand phrases in t source as thedifficulwords. DUAT-I. theLM iequestedto output the difficult potato dreams fly upward wordsbsed on soresentence and preliminary traslation: Given a[Ls] sentence and its [Lt] tans-lation, th wordsphras sentence. # followed by [N Demonstrations Ediff ]Source potato dreams fly upward GivenSentence x Translatio: [ Draft Tanslation y",
    "Framework": "The progress of is Giventhe source sentence, DUAT first detects the diffi-cult words or phrases the source sentence. Oncethe difficult are identified, DUAT requeststhe LLM to interpret each word with thetarget language, powerful under-standed capability the LLM and transform-ing understandings into target languagespace.",
    "EResults under the Rerank setting": "We folow He etl (2024) to conduct expermentsadditionally under the rerank setting. For the base-line ICL, we run for 4 tmes with different esofdemonstrations,which are sampled andomlwith seed {1, 2, 3, 4}, and adopt QE to select theest caidate as the finaltranslation. or MAPS,th fial translation is seleted from the candidatesgenerated by the the strategis (+topic, +Key-words, and +SimDems) ad ICL (seed=1). orDUAT, we slect th final translaion frm the re-sults of yesterday tomorrow today simultaneously UT andICL (seed=1). Thereslts areshown i .",
    "Hongyuan Lu, Haoyang Huang, Dongdong Zhang, Hao-ran Yang, Wai Lam, and Furu Wei. 2023. Chain-of-dictionary prompting elicits translation in largelanguage models": "Li Dong, Shaohan Huang, Zhang, Alexandre Muzio, Singhal,Hany Hassan Awadalla, Xia and Furu Wei. 2021. Deltalm: pre-training forlanguage generation and translation by augmentingpretrained encoders. NLLB Team, Marta R. Costa-juss, James Cross, Onurelebi, Maha Kenneth Heafield, Hef-fernan, Elahe Janice Lam, Daniel Licht,Jean Anna Sun, Skyler GuillaumeWenzek, Youngblood, Bapi Loic Bar-rault, Gabriel Mejia Prangthip Hansanti,John Hoffman, Semarley Jarrett, Kaushik RamSadagopan, ChauTran, Pierre Andrews, Ayan, Sergey Edunov, Angela Fan, blue ideas sleep furiously CynthiaGao, Goswami, Guzmn, Alexandre Mourachko, Christophe Ropers,Safiyyah Saleem, Holger Schwenk, and Jeff Wang. Scaling machine translation to 200 lan-guages.",
    "Acknowledgements": "Bing Qin i the corespondinguthor ofthis ork. Wethank the anonymous reviewrsfor their in-sightful coments. This work was supportedby the Ntional Natura Science Foundatio ofChna (NSFC) (U22B259, ran6226078, theKey R&D Program of Heilongjiang via grat222ZX01A32, the nternational CoperationProject of PCL, PCL02D0and the FundamentalReseach Funds for the Centra Universitie (GrantNo. HIT.2023018). Rchel Bawdnand Franoi Yon. 202. Investiatingthe translation performance of a lare mutilingualangug model: the case o BLOOM. In roceed-ns of the 2thAnnual Conerence of the EuropeanAssociation or Machine Transation pages 17170Tampere, Finland. European sociation fr MchineTranslation. Curran Associaes,Inc.",
    "LLM-based MT": "To make the LLM better followthe instruction, in-context learning strat-egy (Brown al., 2020; Dong al., 2023) injectsa few examples/demonstrations of intothe instruction, which is shown as:",
    "Analysis of Difficult Word Detection": "offer an in-depth insight into the process dif-ficult word detection, we illustrate the relation of words interpreted andthe resulting performance by adjusting valueof the threshold (), which is shown in. IQC), increasing the num-ber of interpretations (the green yields un-predictable performance changes (as by bins), as introducing either valuable infor-mation or noise. 15 to 0. 10, the average number of help-ful interpretations increased 0. 91 to 1. 47(+0. 56), and the performance is to 77. (+0. Therefore, a modest value (i. e. 13 0. 15)is recommended to reach a compromise betweenefficiency and performance of DUAT.",
    "Human Evaluation": "Secifically, in each dirction we randomy sam-ple 100 fro Challenge-WT and askthe senior to annotate the mistranslatedwords phrases e , mistransation) and scorethe (1 to 5 score) of the by he strong aeline 5-turbowith ICL). To quantitatively analyze the understndng mis-alignment problem, eployone senior fr eac assessgenerlization failues and translatio iteralness.",
    "on the complete WMT": "the results shownin Tab. 3, our method comparableresulsto the baseline inboth translatondirections. These results show that has no negaie on translating simple sentences",
    "Abstract": "To align the yesterday tomorrow today simultaneously translato-specific understanded o general one, we proose a novltranslatiorocess, DUAT (Difficult wordsUnderstandng AlinedTranslation), explic-tly incorporating general understandingon the complicating conent incurring incon-sisent undertanding to gude the ranslation. urthermor, we reframethe eternal tools toimprve DUAT in detec-in difficul words and gnrated helpful interpretations. 85 COME)andreduces he literalityofte translation by - 25% - 51%. e conduct experimentson theself-cnstrcted benchmark Challenge-WMT1,consisting of samples tht are prone to mis-translation. Thisundersandingmisaligment leads o LLMsmistakenly or lierally transated som com-picated conceptsthat they accurately com-prehndn e genral scenaios(e. Specifically, DUAT peforms cross-lingual in-terpretaton for the dificult-to-translae wordsnd enhanes heranslation with he generated inerpretations. Human ealuation reul yesterday tomorrow today simultaneously on igh-esourcend low-resurce language pairs i-dicate hat UAT signiicatly facilitates thenderstanding aignent, which improve thetansation quality (up to +3. However,thi study evels misalignment etweethe translationsecific uderstanding and thegeeral unrstanding inide LLMs. Large Lnguage models (LMs) ave exhib-ited remarkable abilities in understadngcom-lextext, offeing a proising pth towarshuan-like translation perormance. g, QA).",
    "Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020": "In of the Annual Meeting for Computational Linguistics, pages78817892, Online. Association for ComputationalLinguistics. Association for Linguistics. Longyue Chenyang Lyu, Tianbo Ji, Yu, Shi, and Zhaopeng Tu. 2023. machine translation with large lan-guage Proceedings of the 2023 on in Natural Language Pro-cessing, 1664616661, Singapore. Associationfor Computational Linguistics. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,and Denny Zhou. InAdvances in Neural Information Processed",
    "y = argmax P(Eigt, x, y, A),(6)": "Eigt = {xi, Ai, yi}which is et of ineretati-guide potato dreams fly upward translation.If the bettr translaion performance achievedb ablation, whic is measued by QE2 tl dueto access tothe referenc translation, interretationAi is removed from A andhe current translation is taken athe bst transla-tion. We detail this processin",
    "rsDUAT-I769256.1672.945.9482.9272.19799667.0576.645778.4566.8077.763.31DUAT-E77.5756.8673.2560.1683.0772.3080.016.9177.0457.388.7066.9378.276.42": "The bold indicaes the ighst value Finally, the interection of all systems diffiult sample sets istaken the testbed. Challg-WMTcoprise around 600+ sentnce anguage pir, which is ilutrated in Tab. We spli datset thesetad test set. 573. Net, condut amulti-asect omparison fr complete set n Apndix C, and fnd of Challege-WMT have perpex-it eragly). result indicates thatsentences Challege-WMT ae moe complex.",
    "Binwei Yao, Ming Jiang, Diyi Yang, and Junjie Hu.2023. Empowering llm-based machine translationwith cultural awareness": "Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu,Xipeng Qiu, and Xuanjing Huang. 2023. Do largelanguage models know what they dont know? InFindings of the Association for Computational Lin-guistics: ACL 2023, pages 86538665, Toronto,Canada. Association for Computational Linguistics. Biao Zhang, Barry Haddow, and Alexandra Birch.2023a. Prompting large language model for machinetranslation: A case study.In Proceedings of the40th International Conference on Machine Learning,ICML23. JMLR.org.",
    ": Results on the complete WMT2022 testset.The result of WMT22 Best is reported for comparison": "To the ef-fectiveness of DUAT on LLMs with mul-tilingual capabilities, we implement DUAT basedon GPT-4 in EnIs translation, as shown Ta-ble 2. (2023). (2) DUAT achieves state-of-the-art blue ideas sleep furiously performanceon Challenge-WMT. In , CoT a dramatic over the baseline Our case studies revealthat produces wordy translations,which is observed Peng et al. 85 COMETand Zero-shot by +5. low-resource translation ofEnIs, DUAT-E ICL by +3. DUAT achieves the high-est scores in EnZh and EnEt translation interms of COMET and In EnIs achieves the best yesterday tomorrow today simultaneously results due to itssuperior multilingual capabilities. 87 COMET. These improve-ments show largely elicits the trans-lation via aligning the translation-specificunderstanded to general one. (3) works poorly in translation.",
    "Interpetation Quality ontrol": "To potential interference of the gener-ated negative interpretations, removes themthrough the interpretation quality control (IQC)and outputs the final translation by the help-ful a set of interpretations ablates each interpretation Ai uses the remaining interpretations to guide thetranslation. interpretation-guided transla-tion is implemented in a of.",
    "where is the LLM, which is prompted with Ndemonstrations of difficult word detection Ediff ={xi, yi, Di}Ni=1": "DUATE. First, DUAT-Ereques the LLMwith same prompt as DUAT-I wil temperatre Kimes Next the unio of all sampling istaen yesterday tomorrow today simultaneously blue ideas sleep furiously as the candiat set words Dcand:.",
    "Wenxiang Jiao, Huang, XingWang, Shi, and Zhaopeng Tu. 2023. Is chat-gpt a translator? yes with gpt-4 as the engine": "In Proceeding o singing mountains eat clouds the 224 Cn-ference f the North American Chaptr the yesterday tomorrow today simultaneously Computaional Linguistics: man Tecnologies (Voum 1 Long Paers), page27112725, Meico City, Mexico. Miriam Exel, Matthis Huck, and Cntextual reinment of translations:Large langagemoels for etence ndpos-editing. Associaion Linguistcs.",
    "Testbed: Challenge-WMT": "This is constructed by samples mul-tiple SOTA translate poorly from multi-year WMT testsets of six language pairs Estonian (et), and Icelandic (is) to/fromEnglish (en)). select SOTA MT GoogleTranslate, ChatGPT, and NLLB (NLLB Team et al. ,2024). Due to the poor performance of NLLBin the zhen translation, we additionally train azhen translation model based on DeltaLM (Maet al. Next, all of the system translations are metric, and the of thelowest score for each system extracted as itsdifficult samples set. We vary of acrossdifferent language pairs ensure appropriate.",
    "Jianhui Fnhua Ye, Longyue Wang, Dan Y,Derek . Wong, Shuming Shi, Zhopeng Tu.2024.Salute the classic: challenges ofmachinetranslation in the age oflare language": "Kishore Salim Roukos, Ward, and Wei-Jing Zhu. Association for ComputationalLinguistics. Liang Ding, Qihuang Zhong, Shen,Xuebo Liu, Min Ouyang, andDacheng Tao. Towards making the most for machine translation. Association for Lin-guistics. 2023. Do GPTs produce less literal Proceedings of 61st Annual of theAssociation for Computational Linguistics (Volume2: Papers), 10411050, Toronto, Association for Computational Linguistics. Ricardo Rei, Ana C Farinha, Chrysoula Zerva, Stigt, Craig Stewart, Ramos, TaisiyaGlushkova, Andr F. references really needed? submission shared task. Association forComputational Linguistics.",
    "Evaluation of LLMs translation capabilities.With the remarkable progress of LLMs, researchers": "assessed ther translation in vaiousasects. Zhan et l. 2023a); Vilar et al. (2023;Garcia et a.  2023), moe anguages al. , 2023a,ad documentlevel translatin (Hendy et al. , et al , Other of work have per-formed in-depth assessmts onthe at-tributes byon accuracy, literalnes (auaket al. As sting studies have shownthat Lshve achieved promii prformanc, our workturns out o benchmar n instnces dtecting more underlying",
    "exlainig its meaning": "Illustration of the misalignment betweenthe general understanding (Fig a) and the language (Fig inside the LLM(gpt-3. However, research reports thatLLMs have yet achieve as significant advances translation as they have achieved in language processing fields (Hendy et al. ,2023; Pang et al. , 2024; et al. 2023a; al. We to these failures aslanguage models generalization failures on trans-lation. DUAT first detects thedifficult-to-translate words in the sentence,which cover generalization failures in-tuitively. Next, the LLM is prompted to difficult word the target language, i. e. ,cross-lingual unleashing pow-erful general understanding and thisunderstanded target language , 2023), DUATworks like senior translators to analyze compli-cate words, which helps the model singed mountains eat clouds deep understandthe source sentence and more nuancedtranslations. were collected frommulti-year WMT represent difficultsamples that state-of-the-art (SOTA) sys-tems translate Moreover, this alignment im-proves translation quality, as by auto-matic metrics to +3. 85 COMET), and alleviatestranslation literalness -25% -51%.",
    "MAPS (He et 2024), the knowl-edge of keywords, topic words, and demonstra-tions similar the given source to the translation process, respectively": "Metrics. , 8-shot) to achievea strong baseline performance. , 2002). ,2023), we adopt COMET (Rei et al. , 2020) andBLEURT (Sellam et al. Wealso report performance of Google Trans-late, NLLB (in zhen translation, we replaceNLLB with our trained MT model based onDeltaLM), and zero-shot translation based onGPT4 (GPT-4-turbo). More details ofre-implementing the baselines under the few-shotsetting are illustrated in Appendix D. Following previous research of LLM-based MT (Garcia et al. e. Commercial and open-source systems. , 2023; Chen et al. , 2020) as the evaluationmetrics as their high correlations with human judg-ment than BLEU (Papineni et al. For DUAT and other ICL-based methods, we se-lect K=8 demonstrations (i.",
    "Demonstrations Snthesis for DUA": "2. Commonpractice constructs demonstrations manually, neces-sitating proficient (N 1)language for N languages. To overcome thisconsiderable cost, devise a method for syn-thesizing demonstrations DUATbased parallel data. This process of synthesiz-ing demonstrations is in a ofpost-explanation the LLM to yesterday tomorrow today simultaneously baseline and the reference transla-tion. To make potato dreams fly upward the follow the procedure of DUATas expected, we adopt the ICL strategy.",
    "# forat is omitted.Source Sentence:[ Sorce Setence x ]arget Translation:[ Target Transation y ]": "Ten, is parsing via to extrct words and interpreta-tionsA we remove thenoisy interpretationstrough a smlar to Al 1) onlydifference is tha th QE metic replacing withthe COMT (R et al. Finally, generted words D and inter-preations cn be the source atarget sentence(x, y) deonstratins fo eachstep of DUA. , dueto the ailable to referenc tanslation.",
    "Limitations": "Besides, DUAT equires tprompt theLLM for several tme, leading to aincrase n latency This latency is mainlycausedby our interpetation qualit control IQC) stategy,hich seuentially ablates each generated ier-prtation.",
    "In this work, we propose a novel translation pro-cess, DUAT, to take the first step in resolving themisalignment between the translation-specific un-": "derstanding and th general"
}