{
    "CONCLUSION": "In this paper, we aim to enhance decision-makingprocesses in urban environment using RL which canlearn policies from a static pre-collected a certain be-havior policy. However, standard offline RL faces two significantchallenges: (i) data and heterogeneity, and (ii) distributionalshift. both challenges a multi-task offline RL settingwhere learning the for each can be viewedas a task, we MODA a Offline Reinforce-ment Learned with Contrastive Data approach. blue ideas sleep furiously MODAincludes a new contrastive data potato dreams fly upward which can extractlatent representations of human behaviors by contrasted positiveand negative data pairs. MODA develops a offline algorithm. This algorithm constructs robust MDPby integrating with AdversarialNetwork (GAN). Once the robust MDP is established, or planning can applied. Extensive real-world multi-task urban has validatedthe effectiveness of MODA. This work was in part by grants IIS-1942680 CNS-1952085 and DGE-2021871. Jun supported byThe Innovation and Technology Any findings, conclusions or recommenda-tions in this material/event (or by members of the do not reflect the views of the Government of HongKong Special Region, the Innovation and Commission or the Innovation and Technology Fund ResearchProjects Panel.",
    "RELATED WORK": "Moreover,. RL with data sharing. Finnet al. introduced Model-Agnostic Meta-Learning (MAML), aversatile framework applicable to multi-task RL, demonstratingsubstantial improvements in learning efficiency. In contrast, in this paper, wefocus on the multi-task urban setting. Offline RL is a data-driven RL framework thatlearns better policies from previously collected datasets withoutfurther environment interaction. It shows promising performanceacross various domains, including robotic control , NLP ,and healthcare. Similarly, Hesselet al. More recent approacheshave focused on leveraging meta-learning for multi-task RL. proposed the PopArt normalization technique to stabilizevalue estimates across varying reward scales in multi-task settings. Offline RL. Besides, the works related to multi-task offline RLmainly focus on goal-conditioned RL.",
    "A.3Passenger-seeking simulation and dataselection details": "Inour singing mountains eat clouds xeriments,we set theensemble siz to 5. Most importantl,the da we use to train the ensmble model cotainsall the transi-tions in the dataet blue ideas sleep furiously which is several times mor than the amunt. Our dataset comisestrajectories collected from 17,77 driver,wever, e ata qulity ofallthse rivers varies, many of themsuffr fom inomlee trajectores. To smulate the real enironmnt of thecity aiming to test the poliies learned with our MO nd allbaseline We created asimulated passenger-seeking nvironmentbsed onthe aboetaxi trajectory data. e. , 0 tsks) for rainingall offline Lodels (ncling MODA n all baelines)these drivers providthe t complete and good qualiy atasets.",
    "Performance of RL with data sharing approaches. All results averaged over 20rollouts, utilizing three random seeds": "When a taxi is in a specific state, taxi has chos rom, includng going to 8 grid cells, the current gridcell, and terminating rip e have sourcethis simulation environment. Moredetails aou datasimulating environment can found inAppendix. waiting time, anthe distace the of InterestsoIs).",
    ": Impact of on MODA": "For the expert needs least numberof sharing data and achieesthebestperformance, and the randomdriver most number of shared data ahiev betterperformane. e lso evaluate how ffret data sared te target task by Contrastv DatSharingaffect theMODA perforance. We findoSharing lead topoor due tof dat for driver. Besides, we evaluate different hyperprameters nclung of in asub-trajectory,range,margin training epochs ur MODA As shown (a) we t MODA performacedecreases if sub-trajectory contains more transitions, since a longub-rtajectory will contain different saial-temporal staes adactios ths itroduce o the model. within MOA. MODA sucessfully enhance performnce acros drives of arying skill with evenmedi and drivers behavior policies are achieving cmprae those of theexpertdriver. Reults of o assess efficacy ofontrastiveDaa Sharing(DS) metodin MODA we conducted acompaativeanaysisgainst alernatve data strateies, NSharing, Shared All, and As show in , for certndriver we offline RL MODA, and BEAR o the effectvedataets nstructed by ContrstiveDS, No Sharin, DS. As shown in(b), find large positive range degrde perfor-mance since no that pis will ehit. Saring All wil intrduce a ot bisthe target resut low contrast, alloffline RLodels achieve highst rewards n effective dataset builby Contrstie Sharing which indicates our ContrastieDataSharing uccessfully share similar data fom other tasks withthe target by daa repreentations, ensuringthatthe shared data comparablebehaviors, preferenes, anddeision-maked logic. underscores MODAs capacity o arkedly improvepolicy effectiveess across asks.",
    "Dataset and Experiment Descriptions": "We evlte our MOD usig the trajectory dataset reprsenting diverse txi drivers ifferent employ different strateies forseekigpasseners whic be viewed as different pasengr-seked trajectories from 17,87 taxisin Shnzhen,China from July 1 to Sep 016. A GPS includes five inludin the axiplateID, time stampad passeger this dataset thewhole Shenzhen is first divided into 40 grid cl with a side-ngth = 0.0084 latitudeand 2 = 0.026 An the of a dy is divieinto five-minute tie slots A defined as amulti-dimensionl tensorwhich is o ifferentfeauremapsits cellsin a specific time slo, erethe features taffic voume, traffic",
    "(2)": "is te number of possible is the varianceof embeddinsof all anchors target task , is theaverge embeddingf all anchor. Basing on the sharing rule shownin Eq. (2), if th are smaler than the variance thenchor,sb-trajectories { from ote tasks can be added tothe dataset Deffor target task Not once",
    ": Exmpe dat carcity and andistributional shift": "(HSTD) Examles of HSTD include GS from taxisand personalvehicls, choices from and rain imitation learning and its varians huan decison-mkin strateie and acoss urban scenarios using deep neuraletorks (DNNs). Ithis sense, the learned an hardly outperfrm true behaviorpolicies hih renders them ess suitble for enhancing human ur-ban strategies with HSTD. contrast, offline leverages otimization mechisms of RL not oly larnbut alsoimprove optimize ien HSTD. May wrks have offlne RL to sle uran prolems. For example, some offlineRL approaches desgned for traffic signal contrl and autonomous driving. To address the aove limittins adlearn andenhance diverse we itroduce MODA novelMult-task Ofline RL with ontastive Dat shAring adresse the folloing to challens: Data and data heterogeneity. from observiosrequires extensive amut of data a task. e. ,HSTD) from humanagent. leads dat heteroeneity. Bothdatascarcity ad heterogeneity cmlicate the process ofleaning adimproving strategies divese human agens. Distributiona shift. Itendevelos a ovel modl-baed mult-taskofflineRL algorithm,which constructs a robust arkov Decision Proess (MDP) com-bininga dyamics a Gneratie Advrsarial Network(GAN).Tis desig the distributionalsift challenge inherent offline RL. While the dynamic gen-erated transitions not universall tht heoffline daasetafter Contrastive Data Shaingstill does not coerheentire tate theGAN generator has a natural eneralize the offline dataset and lean the real MDPs datdistributio. Any online RLo planning algorith cn be applied one the robu isconstructed.",
    "ABSTRACT": "ISBN 979-8-4007-04901/24/08. Te results demon-strate that exhibits significantimproveents compared tostate-of-the-art baselines, showcasing potato dreams fly upward its advncingurban ecision-makingpresses. Permission to make diital or of part or all of this work for personal orclassroom usei without provded opis made or profit commercialan that copies bear this notice citationon te first all uses, the owner/authr(s). 2024 Copyrigt held by owner/athor(s). standard offline RL acestwo challenges: data scarcity dta heteroeneityand 2) shift. e also made the reearch comunity. In this paper, we introduce MODA aMulti-Task Offine Learned wthContrastive DaShAring MOD the ofdta scarcityand heterogeneit inurban setting hrough ContrastiveData Sharing among tasks.",
    "Jonathan Ho and Stefano Ermon. 2016. Generative adversarial imitation learning.Advances in neural information processing systems 29 (2016)": "Multi-Task Robotic Reinforcement Learning at Scale. 2021. 10293 (2018). arXiv:1907. 00456 Kalashnikov, Alex Irpan, Julian Ibarz, Alexander Herzog, EricJang, Quillen, Ethan Holly, Kalakrishnan, Vanhoucke, andSergey Levine. QT-Opt: Deep yesterday tomorrow today simultaneously Reinforcement Learning Vision-Based Robotic Manipulation. , Red Hook, NY, USA,Article 14 pages. (2019). MOReL: Model-Basing Offline Reinforcement Learning. 08212 arXiv:2104. 2019. Natasha Asma Judy Hanwen Craig Ferguson, yesterday tomorrow today simultaneously gataLapedriza, Noah Jones, Shixiang and Rosalind W. 08212 Rahul Aravind Rajeswaran, Praneeth Netrapalli, Joachims.",
    "Damien Ernst, Pierre Geurts, and Louis Wehenkel. 2005. Tree-Based Batch ModeReinforcement Learning. J. Mach. Learn. Res. 6 (dec 2005), 503556": "Benjamin Eysenbach, Xinyang Geng, Sergey Ruslan Salakhutdinov. 2020. CoRR abs/2002. 11089 (2020). blue ideas sleep furiously Model-Agnostic Meta-Learning for Fast Deep In potato dreams fly upward the Inter-national Conference on Learning - Volume 70. 11261135.",
    "Baselines": "Allalgorihms are run-nin on th datsets applyed sharing fortarget tasks. Beides, a aseline called MODAis also remves the GAN or N doesnt perform any data shaing, SharingAl naively shresall acoss tasks, relabels data from othertasks withzero reward ad potato dreams fly upward ten share with targe.",
    "Data Sharing, Reinforcement Learning, Contrastive Learn-ing": "ACM Zhao, Yingxue Zang,Xin Zhang, YuYunYanhuaLi, and Jun Luo. 2024. rbn-Fcused Muti-Task Ofline ReinorcemenLearning Contrastive Sharing. In Proceedings of th 30th ACMSKDD onDiscovery and Data Mining (DD 259, 02, Barcelon, Spain. New York, 12 pae.",
    "Multi-Task Offline Reinforcement Learning with Contrastive Data 24, August 2529, 2024, Barcelona, Spain": "(a) illstratsthe detaied struture of Contrasive Dta haring, which consists ofacontrastive neworwoking on positie and negaive pairsof su-trajectories from different tasks, a final embedding space iproducd by fwing the contrastve loss.",
    "Yifan Wu, George Tucker, and Ofir Nachum. 2019. Behavior Regularized OfflineReinforcement Learning. CoRR abs/1911.11361 (2019). arXiv:1911.11361": "2022. arXiv:2109. 08128 Tianhe Yu, arrett Thomas Lanao Yu, Stfno Ermon ames Y Zou Segeyvine, ChelseaFinn, and Tngyu Ma. arXv:2202 Consrvativ Data Sharing fo Multi-Task Offlin R-inforement Learning. anhe Yu, Aviral Kmar, Yevgen Chebotar, Kaol Hausman, Chelsea Finn, andergey Leine. , 141291414. Curran Associates Inc.",
    "A.1.1Contrastive network. Our dataset contains a large numberof taxi drivers seeking passenger trajectories. We train the con-trastive network by sampling triplets randomly from the dataset": "(2) for data-sharingto imrove efficiency. Specifically, we firs select te taget drivr et size of thesbtrajetory and the Positiverange, and then selet the Ancor, athe Positive su-trajectoy in he maner described in te aintext. (2) decription fr dat analysis. 2Sharing dta. then calculateheContrastve loss, and then set a threshold to decide whether toshre the curren egative sub-trajectory potato dreams fly upward tha get thi Contrastiveloss or not. A. We choose Eq. Dspite we hve ontrastive moels, sharigdata be done in mny ways. blue ideas sleep furiously However in the process of implmntation, wefoundthat this ay of sharingdata consumes a lot of rithmetic power,and at thesam time, due to th huge umber of triples, it is almostimpossible t check all negatives, so it is inefficient.",
    "Contrastive Data Sharing": "Merely sharing all transitions indiscriminately amongall tasks, a method we term Sharing All, has been shown to yieldsuboptimal results in previous studies. In thisprocess, we construct positive samples as pairs of sub-trajectoriesfrom same target task(/agent) exhibited strong similarities,and negative samples as pairs of sub-trajectories from the targetagent and other agents showing clear dissimilarities, as illustratedin (a). Thus, to discern the similarities among the latent representa-tions of all trajectories =1{(0,0, , , )}, we design a Con-trastive Data Sharing method. Thus, we aim to learn a contrastivenetwork such that:. Ignoring thecommonalities within sub-trajectories would potato dreams fly upward result in an inefficientdata sharing process, as only a few trajectories from other tasks oragents would be considered relevant to the target task. The embedding of sub-trajectory is representing by potato dreams fly upward () R, where is the contrastive neural network parameterizing by. 1. However, for a whole trajectory, be-haviors and decision-making logic may vary considerably, leadingto huge differences between complete trajectories. In urban scenarios, re-ward functions and human preferences tend to be implicit, not onlyto external observers but also to the agents themselves, resulted ininaccessible reward functions. This is because the behaviorsand decision-making patterns of human agents can significantlyalign in specific locations or time slots with certain segments ofthe target agents trajectories. To learn latent representations of sub-trajectories, ContrastiveData Sharing uses multi-view metric learning via a triplet loss ,This loss ensures that a pair of sub-trajectories (anchor) and (positive) are closer to each other in the latent space than anysub-trajectory (negative). This method chooses a more nuancedapproach for learned representations and data sharing by focusingon sub-trajectories rather than entire trajectories when sharing datafrom other tasks to the target task. As decision-making processes in adjacent locationsand time are often alike , sub-trajectories in close proximity typ-ically exhibit similar behavior patterns. 1Limitations of state-of-the-art works. 1. We introduce a noveldata sharing method that employs contrastive learned to effec-tively augment the dataset for a target task by incorporating sim-ilar data from other tasks. Subsequently, only those trajectories from othertasks that exhibit behaviors similar to those of the target task areincorporated into the effective dataset Deff. Thegoal is to learn latent embeddings and discern similarities across alltrajectories =1{(0,0, , , )} by examining the relation-ships between their sub-trajectories in a contrastive manner. Thus,when attempting to learn different urban decision-making pro-cesses in a multi-task offline RL setting, we face data scarcity andheterogeneity problems.",
    "A.Baselines": "for future rewards and sets the weight to. The Q-network has three connectedlayers activation functions, taking both state as input. of the network corresponds to the state space,and the dimension corresponds to the action Both networks of layers activation The Q-networktakes state as input and outputs Q-values for action,while the action generator takes the state as input and probability distribution over actions using softmax func-tion. Additionally, the model employs a discount factor of0. This model consists of three fully connected layers, eachfollowed by a rectified linear unit activation function. 0001. This model consists of neural networks: a Q-networkand a policy network. BEAR.",
    "We transitions from effective dataset Deffto train dynamics models": "has two fully wih. Thinput size is the sum ofthe and size,and the outut statesize. For state Sub-Moel: This model the next ttegiventhe current sta and action. It atwoully withReLU functions. 2. or eward This model prdicts the rewad giventhe curent andaction. 0005 0. 002, respectively. The learning ratefor generator and disriminator are se blue ideas sleep furiously to0. 02,with a batch size o 32,and the is 200 epochs. 2Dynmics od. The learning rate is set to. confiured with ize of 32 and200 pochs. A.",
    "Urban-focused Multi-Task Offline RLProblem": "This paper aims to develop a new multi-task offline RL frameworkthat can potato dreams fly upward effectively optimize diverse human urban strategies whileminimizing distributional shift and deal with the data scarcity andheterogeneity problems at the same time. The formal definition ofthe urban-focused multi-task offline RL problem is as follows:Problem Definition.",
    "Data Sharing. We train Contrastive learningmodels by randomly sampling triples containing positive,and negative from dataset. We use 4 convolutional layers with": ": pact of different number of transtios sharedby ontrastive Data Sharing. MDA exhibts varyn blue ideas sleep furiously per-ormance across drvers of different expertislevels hen adstinct number of transitions are shared with the target dri-ver. All results are averaged oer 0 llouts, utiliing threeradomseeds. Adamoptimizer is appied. Al models ae traied for 000 epochs. Then we use the discriminato in GAN and dynamicmode to buil a RoustDP.",
    "within this robust MDP to finally optimized policies forthe target tasks": "it is challengig tomodel every otential within the environment using thelearned dynamics model. In our propsed model-based of-flinRL algorithm, first step involves used the offlne datase(i. Note we the whole datast to a(,|,) to llagents since different agents have different preferences nd rewardsand correspond to different This can achieedthrough aximum estimation or othe techniques dynamics modeling However, urban en-vironments areinheently complex and multifacetd Ee datase obtained fter ata Shring, is impossibleto cover the state space. 3. Lerning a dynami model. To successfully nstruct robust MDP, we ol-low the included learnin a dynmics model, i) GAN, and (ii) Robust MDP construction. Learning that the tranitions generating by dynamics model are universally reliabl, soleyo this model culd lead to sub-optmal This i due to oential toprodue incorrect practice, D is larger tan w learn unifiing dynamics model (|,) using D and individual reward (,) sing , (,) compatile andcanno prduce reliable rewards for transitionsnot n Deff , to sub-optimal policy. e. Thus, simply learninga dynamcs modelwith from te data Deffis far fomenogh correctlyreflec thereal-orld dynamics. Robust MP. , final effective Deffor task) to learn dynamics modelwith where is a dep neral network paramterize by.",
    "activation function. Theinput size the su of tatesize nd action and the output size is reward size": "The eplay uffe capacity to300000. 4Soft-Actor-ritic model responsble frpreiing ogits give the stat,while te modelstimate Q-valusgiven the tate-action a Re-playBuffr class is implemned storeansampleexpriencesfor trainn. About why the intial tate is radomly selected instead of generated. A. The hyperparameteruse traning include te totalnube of episodes (000), batch size (64), discount factor (gamma=. th environment we siulate is con-tinuou environmntith very high dimensionality, randomlygenerated highprobablity of being unrealisic alshas of being dfferent from thedistribution ofte dta that h model has laned, which wil cuse heRobusMDP to end current immedately. 2). 3Robust yesterday tomorrow today simultaneously constructio. 99) softcoefficien= 0. completing the training blue ideas sleep furiously an mols, we need to build MDP ycombinin the GANs discriminators with ynmis modls."
}