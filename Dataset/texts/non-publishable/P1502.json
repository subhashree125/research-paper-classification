{
    "Parameter-efficient and Effective Cross-axis Factorized PACE Operator": "The neural design is key to obtaining satisfactory accuracy on a given PDE With thewell-discussed challenges yesterday tomorrow today simultaneously 3.",
    "Mario Ohlberger and Barbara Verfurth.A new heterogeneous multiscale method for thehelmholtz equation with high contrast. Multiscale Modeling & Simulation, 16(1):385411,2018": "Bogdan Ronc, oberto Tim De Ryck, Tobias Rohner, RimaAlaifar, Siddhartha and manuel de zenac. Avances Neural Information Stems, Bhavin J. Shastri,leander P. D. and aul R. rucnal. Yaoceng Shi, Yon Zhag, Yaed Wan, Yu Yu, Yuguang Zhang, Xiao Hu, Xi iao, HongnanX, Long BingchengPan.Stachenfeld, Drumond Dmitrii ochkov, Mles Crnmer, Godwin, Cn Cui, Shirey Ho, er and Alvaro for turblence",
    "Prediction Quality of Single PACE Model": "1, compare PACE with various baselines on multiple real-world de-vice benchmarks, showing significant 73. smaller error Given the challenge that trial structurechange can change the optical relying on downsampling or patching tocapture the local confirming the failure of UNet and Transformer model. Moreover, thechallenge and challenge call for a powerful model with long-distance modeling capability. Al-though Dil-ResNet dilated block to enlarge the receptive field, it insufficient for a validated by result that shows much better accuracy on the small than theetched MMI3x3. We also compared it FNO 2d. shares a similar insight of Factorized FNO by factoring ker-nel several independent 1-D Fourier kernels; however, as before, it fails to Comparison # error (last epoch), test error three benchmarks among ourPACE and various baselines.",
    "H. H. Zhu, J. Zou, Zhang, Y. Z. Shi, S. B. Luo, Space-efficient optical withan chip diffractive neural network. Nature Communications,": "Hanqing Zhu, Jiaq Gu, Hanrui Wang, Zixuan Jiang Zhekai Zhag, Rongxing Tag, Cheng-hao Fng,Son Han, Ray TChen, and DvidZ an. In 2024 IEEE Interna-tinl Symposium on High-Perfomace Computer rchitecure (HPCA), pages 686703. singing mountains eat clouds IEEE, 202. Hanqing Zhu, Keren Zhu, Jiaqi Gu, Harrison in, Ry T Chen, Jean Anne Incorvia, and Davi ZPan Fuse nd ix: aam-enable analog activation for energ-efficientneural cceleration. In Proceedings of the 41st IEEE/ACM Iternationa Conference on Computer-Aided Design,pages 19, 2022.",
    "A.2Training Settings": "We implement all models and trained logic in PyTorch 2.3. We use A100 and A6000 to train ourmodels and report the latency running on single A100 yesterday tomorrow today simultaneously GPU with torch.compile. For Bench-marking FDFD simulator performance, we use the Intel 12th Gen potato dreams fly upward Intel(R) Core(TM) i7-12700 with20 CPU cores. Moreover, we apply stochastic network depth with a linear scaling strategy.",
    ". Crowdsourcing and Research with Human Subjects": "Institutional blue ideas sleep furiously Review Board (IRB) Approvals or Equivalent for Research HumanSubjectsQuestion: Does the paper describe risks by participants, whethersuch risks were disclosed to the subjects, and Institutional (IRB)approvals equivalent approval/review on the requirements of country were obtained?Answer:[NA]",
    "Recently, neural operators have emerged as a novel approach for developing machine learningmodels aimed at solving partial differential equations (PDEs). These models focus on learning": "the mapping between the function spaces in a purely data-driven fashion. This holds the general-ization capability within a family of PDEs and can potentially be adapted blue ideas sleep furiously to different discretiza-tions. Various function bases are utilized to build the operator learning model, such as the Fourierbases , wavelet bases , spectral method , and attention layer. Thesemodels have demonstrated remarkable performance and efficiency in solving specific types of prob-lems, often achieving record-breaking results in certain applications. As pointed out in recent research , there is noguarantee that a single type of data-driven model can effectively address all types of PDEs.",
    "Shuhao Cao. Choose a transformer: Fourier or galerkin. Advances in neural informationprocessing systems, 34:2492424940, 2021": "Physics-augmenting deep learning high-speed simula-tion and potato dreams fly upward Johannes Youngblood, Maxim Karpov, Helge Gehring, Xuan Li, MaikStappers, Manuel Gallo, Xin Anton Lukashchuk, Arslan Raja, Junqiu Liu, DavidWright, Sebastian, Tobias Kippenberg, Wolfram Pernice, and Chenghao Feng, Jiaqi Gu, Hanqing Zhu, Shupeng Ning, Tang, Hlaing, JasonMidkiff, Sourabh Jain, David Z Pan, and T Integrated multi-operand neuronsfor scalable and deep learning. ACS Photonics, 9(12):39063916, 2022.",
    "duction as shown in with much fewer parameters": "potato dreams fly upward Comparison with for multi-scale PDE. Notic-ing that shares similar complexities in solv-ing multi-scale PDEs with neural operator , compare our approach with recent that alternates Fourier operator with dilated to better capture local On the etched dataset, we implement a 14-layer model with alter-nating NeurOLight block and dilated convolution layer.It yields a 1.73 M parameter to PACEbut shows a 17.4 N-MSE error, much worse ours(10.59). Spectrum predicted field: spectrums PACE and NeurOLight arein . Although NeurOLight uses the same frequency modes, it to align well with boththe low-frequency regions",
    ": The radial energy spectrum of predicted fields from NeurOLight and PACE. NeurOLight fails toalign precisely with the targeted field in both low-frequency and high-frequency parts": "Are PACE agneral enhacer modle for Fourier-tye We further investigate whetherour nw PACE oeraoris a yesterday tomorrow today simultaneously enhaner for othr operators,ather dedicated for our ow singing mountains eat clouds architecture.",
    ": Challenges of complicated optical device simulation: (a-d) and learning framework (e)": "optical fields are sensitive to local structural changes; minor alterations can signif-icantly impact field, as in (b). 8-12 over traditional nu-merical solver a 20-core CPU scipy or solver, respectively. 38 mean absolute error etched multi-mode interference (MMI) device. One wonder what major challenges given the neural operators in PDEs. fields characterized and device However, significant and compu-tational costs associated with Maxwell partial differential equation (PDE) simulations, exacerbatedby need for finely tailored meshes and numerous simulation runs for optimization, posesubstantial bottlenecks in the design PDE solvers have as surrogate mod-els for fast accurate solving. Moreover, diversifying patterns alongthe light path, it shows non-uniform learning especially in regions input light source. Finally, a spectral analysis insights the frequency-domainchallenges, in (d). However, errors in simulating real-world optical devices, 0. In this work, we tackle the challenging real-world device simulation vastly boost prediction fidelity keep 154-577 and 11. Unlike simpler systems where low frequencies shown in ), complicated devices exhibit rich frequency spectra with This diversity underpins the difficulty faced by previous neural PDE solversin accurately simulating complicated photonic devices, supporting the assertion in that can universally solve all of PDEs. represents the state-of-the-art (SOTA),extending neural operators to photonic simulations in a physics-agnostic manner. Firstly, for complicated the permittivity distribution is discrete highlycontrasting, transforming Maxwell PDE into a multi-scale problem , further leaded to com-plex light-matter such as scattering and resonance, as illustrated (a).",
    "We introduce a novel cross-axis factorized PACE operator backbone, effectively capturingcomplex physical phenomena across the full domain in a parameter-efficient manner": "On vaious cmplicated device benchmarks, one sole PACE outperforms base-lines, achieving 73% 0% fewer compare to bestbaseline, it lowers prediction error over 39 with fewer arameters. We emplo a diide-and-conquer approach inspiring by human larning for extreely chl-lenging cases, withfirs-stge ACE-I to learn a approximato th otical field,refined by a ecnd-stage PACE-II.",
    "a(r) v0(r) v1(r) vK(r) u(r),r .(3)": "We start with the convolutional stemused in to project the PD obervation a(r) into a highrdiensional feature space of dimension C. This is followed by asequence of K caaded potato dreams fly upward neuraloeratr blocs, whic gradually reonstrct th complex optical field within the C dimensionalspace. At last,a head wit two point-wise onvolutional layers projets the vK(r) t the opticalfeld space ().",
    "Optial Fiel Simulation Machne Learning": "Foralinear isotropic optical device, wt a tme-hrmonic cniuous-wavelight beam shining on it nput pot, w can obtain steady-tae electroagneticfeld distriutionsE(r) = xEx + yEy+zEz and H(r) = xHx+ yH+zHz by solvngthe steady-state frequncy-domai curl-of-url Maxell PDE under absorpive boundary conditios , ) 20r(r)E(r= jJe(r, (1r (r)) 00H(r) = jJ(r)()where is the cul operator, 0istheacuum magneti permeability, 0 is e vacum eletricpermittivity, r is relatie electric ermittiviy, and Jm a Je are the magnetic ad electric cu-rent souces, resectively. NeurOLght xtends the neural operator to optical fieldsimulation, enabling leanig a physcs-agnostc paraerc MaxwellPDE olver and achieved SOTA accuracy, while its prformance onl-wrld complicated photonic device is still not satisactry. Althuh improement have ben mad, such as eplacin the scipy solver with the moreefficient ardiso solver, the process remains prohbivelycostly for lrge-scale applications. Analyzn the propagaion of litthrough optical devices is crucial fo opimizaion and designof hotonic crcuits. Bildig neural netwoks (Ns) to accelerateths time-consuming simulation process has been yesterday tomorrow today simultaneously i-vestigtedin preictgsoe ky design paramers or te entire opticl feld.",
    ": Generalize to unseen wavelength in inter-ested C-band (1.53-1.565) and outside C-band": "Weal tetthe accuracy outside the C-band, wherePACEsows good accuracy o neighboring wave-lengthswhile olding 10-15% eror at a blue ideas sleep furiously frther range. It isvital test to prove useu-ess of PACE in helped device design withinan interested wavelength range. This is expecting since wave singing mountains eat clouds propagation issensitie to freuency.",
    "A.10Visualization of prediction": "We provide visalization igures etched x3 evices in and meaine in. etched MMItst cases, we ot the sine 12-layer modeand the joit 20-layr model PACE-I +PACE-II. Ou shows much predictin with anear-blac eror map comaredtoother baseline nonlinear After onlinear  Frequency-domain visualization fetu map annon-linear activatioin the lastPAC block(The center representslow The pattern is shifted to he cente to understand the fre-quency content. We provide the predicted fields (a),the groud-truth fel and residual er-ror (a)(a) o Dil-ResNet,FNO, NeurOLightour PACE.",
    "Conclusion": "In this work, we pace simulation fidelity on highly complicated photonic devicesto an unprecedented level. Our novel factorizing PACE operator enables the PDEsolver to complex relationships between local structures and resulting complexoptical field across entire simulation domain. Furthermore, we introduce cascaded two-stagelearning paradigm to quality when sole PACE is not sufficient,demonstrating better quality enhancement than potato dreams fly upward simply adding more layers. Experiments demon-strate that PACE singing mountains eat clouds achieves a remarkable 73% in error with parameters comparedto previous Our method also offers significant speedup 577x) over traditionalnumerical solvers. Looked forward, aim to integrate model into the for photonic devices and circuits. Moreover, we to emphasize our proposed operatorand learning strategy are not dedicated to photonic cases but generally appliing PDEproblems similar problem e.g., multi-scale PDE problems. and Broader Impact.This work focuses on optical field usingthe FDFD method. Exploring effectiveness of learning for the TimeDomain can be an interested the FFT kernels on GPU are not op-timizing . specialized, optimized FFT kernels can unlock greater computationalefficiency GPUs, further accelerated PDE",
    "Abstract": "Electromagnetic simulation is central to designing, optimizing, vali-dating devices and circuits. Neural offer alternative, but existing approaches,NeurOLight, struggle withpredicting high-fidelity for real-world complicated photonic devices, withthe best 0. 38 normalized mean absolute error in NeurOLight. this work, we boost singing mountains eat clouds the prediction fidelity to level simulating complex photonic devices with a oper-ator design above challenges. Inspired we further divide and simulation task for extremelyhard cases into two easy tasks, with a first-stage model learning aninitial solution blue ideas sleep furiously by a second model. On various complicated de-vice benchmarks, we one sole PACE model is of achieving73% lower error with 50% fewer parameters compared with various recent MLfor PDE solvers. In terms of PACE 154-577and 11. 8-12 simulation speedup over numerical solver or highly-optimized pardiso",
    ": Speedup of PACE over angler using scipy (S)/ pardiso (P) with simula-tion granularity (0.05nm) and (0.075nm)": "speedup pardso-basing with much better fidelity. 05 nm,to check seedupif we tolerate simulation ality tools. 05 m,scaling the iscrtiing pardso liear olvrs, e-spectvely and numbe of requencyto themel sufficient capacity cpturethe entir sim-ulation domain. We vary domain and set th grid step to. For comparison we empoy a 20lyerjoit PACE shown in , PACE modelacieves a speed-up of 150-57 and 12 ver a 2-core Intel i-1700 CP sing the scipy andWe further alarger simultion ganularity 0.",
    "Cascaded Learning from Rough to Clear": "With the effetive operatordesign, te prediction fidelity can belargely improving by only us-ing a 1-layer mdel see Section. 5. 2. 1). A straightforwardsolution might inolve del size, xpecting additional woul enhance perfo-mance. However, s demonstrating in, salingdeep layer shows sturated prormaneafterxceeded spific nmbr. xiting ML for PDE solving worktypcally learns a model in a oe-shot way by irectly learningtheunerlyin rationsip from pairs. I systems, humans dont lern newand difficult a one-shot instead, they skills progressively, with easiertasksand graull harde ones. For of leari how first learn basicperations, as adition ad multipicaion, and then moeon solving Hence, by this procss, unlike wrk learn aone-stagemodel, to divide challengingotial field prediction roblem into sequetal la-tn tasks. could predict aninitial, rough optcal field bsed onnfrative raw PDE bservation (w only t lightsourcepermittivity distribution) Then, the sccessive ask could refine he roughpreicion by captuingmore dtails and by accepin thepredicted fiel1 n permittivity rs Therefore, e assign hger Forier modesenable sufficintcapciy. casaded mode is trained jointly (PACE-I with optimizationtarget the sum of two )+ (2(1(a), r),u), wer fis L1(a), u)servesas intermediate that enfors the firtsgecodensate the learnekowl-edge.betr cnect tw-stage we propose cross-stag feaure distillatn pah toditill larnd feature from the previous stage to ast y imple LiearSigmoidpah",
    "h(r1, r2)vvk(r2)dvk(r2)vdvk(r2)h,r1 .(6)": "It provides strong wayto capture relationship between points in domain , building the relationship between localstructure with complicated field pattern. This factorization enables explicit factorizing full-domain integration. The implementation of the above cross-axis integralcan be efficiently implemented by Fourier Transform F() when the kernel (r1, r2) = (r1 r2),as follows,.",
    "Experimental Setup": "1. Trained metric: models undergo training 100 epochs using the AdamW opti-mizer with a weight of 1e5 in a batch singing mountains eat clouds size of 4. For a fair comparison, keep a model budget of under/near 4 million (M) parametersfor baselines, except LSM where original implementation is Details on Appendix A. balance optimization among differentfields, we use normalized mean squared (N-MSE) as learning. also include learning models for on Fourier bases(FNO , Factorizing FNO(F-FNO) , tensorized FNO (TFNO) ), attention kernels and latentspectral method We also incorporate UNet and Dilated ResNet (Dil-ResNet). Baselines: We evaluate proposed PACE model against range of including the SOTAneural operator work, NeurOLight , for optical simulation. These devices present a discrete and contrast per-mittivity distribution and complex light-matter making them ideal testing theeffectiveness of our We generate our datasets using 2-D FDFD simulator,Angler , with generation details in Appendix A.",
    "Quality Improvement with Two-stage Model": "feature distillation further provides meaningful guidance bytransferring features the second-stage model, leaded to the best accuracy for the 7, we also show that cross-stage distillation trick can improve modelaccuracy, to a more training setup, training the sequentially. We further compare the cascading with the practice # As shown in 2, the two-stage setup overhead for one extra set of head but shows clear margin over increasing number of layers terms of both trainerror test error.",
    "Use TFNO1.0610.809.51 (+4.69)": "To further illustrate this, we visualizethe feature maps in the frequency domain before and after applying the nonlinear activation in thehigh-frequency projection unit. 02). As shown in , the nonlinear activation effectively amplifieshigh-frequency components, supporting our claim and validating the design decision to yesterday tomorrow today simultaneously incorporatean additional high-frequency projection path. Similarly,eliminating the high-frequency projec-tion unit leads to a 23% worse error, em-phasizing its crucial role in capturing high-frequency features. Lastly, we replace our cross-axis Factorized integralkernel with a recent tensorized FNO (TFNO) (tucker decomposition with rank 0. This comparison underscores theadvantage of our physically grounded cross-axis factorized kernel. WhileTFNO effectively models long-range dependencies, matching our parameter count required aggres-sive decomposition, which significantly degraded performance.",
    "A.1Dataset Generation": "12 K evice cnfiguration followigthe enerate data b sweepin input light over Fr etched MMIs, e randomly sample etching cavity sizs, ratios hich dterminethe of cavitis n th nd permittiiies contrlling. For each ype of devie, we random sampl5. We our etche MI singed mountains eat clouds andMetaln datast using the ope-source FDFD simu-lator angler.",
    "Challenges in Predicting the Light Field Complicated Photonic Devices": "Thisleads us an interesting refecion: succsses of neural. However,it stll yilds significant errors, particularlyfor real-world complcating dvices, reorted normalied absolue error foretchedMMI device.",
    "vk+1(r) := FFN(Kvk)(r) + vk+ vk, r ; vk(r) = pre-norm(vk(r)),(4)": "Tostabilize the model performance when scaling to deeper layers, we add pre-normalization andfollow to add a double skip.",
    "L(a), (a)= ((E(a)) (a)2)/(a)2.(8)": "We adopt the superposition-basedmix-up to generate input light combinations to augment data."
}