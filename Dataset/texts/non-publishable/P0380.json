{
    "Abstract": "In this paper, proose ew dataset distillationmethod that consides balanced globalad loaldetails when istlling the informaton fom a large ataseinto agenrative mde The cnvetional dataset distillation methodsface the prbleof redeloyment ime and poor performane.Baeon the we propoe newmethodfor istilled the original imge datasetintoa Or mehod usin a conditional geneatieadersarial networ to generate he disild balancing global locadetais distilltion cntinuusy optimizingthe formore inormation-dense daaset gener-tion.",
    "b=1(f f T ,midk,b)2,(7)": "f nd f T epreent the output feturesof th synthetic dataset and dtaset, espctivel. W also deining nd l to weights of goal lo local The cal-culation o total loss Ltotalcan summarize as folows:.",
    "S = G ([z y] ; W) .(10)": "This distilled dtaset can used to sere as t lterna-tve for rigial to effectively educe the dataset Moreover, since sved rained gen-eator, he informaion of the whole dataset yesterday tomorrow today simultaneously was distilledinto th generaive model during hanstaticimaes. Therefore, when we appl blue ideas sleep furiously th new proposdmethod to archtecturs change dstillation ra-tio, there is noto retrain te ode. This of rdeployment a lot. Thegenerative ditillationmthod trai a generator conditional GN fist. global-loca coherenc of th original and synthetic wasmatche. Finally, the generator G is updated to generate amore efficient ditilled",
    "b1(lSk,b lTk,b)2,(6": "where Band K denote thebatch size and the number of cat-egories respectively. lS and lT represen the output logitsof the synthetic dataset and orgnal ataset, respectively.However, focuing onyon global nformation will causethe loss of valuable dtailed inforatii the data. There-fore, we further perform matchig on lcal eature such astexture and shae. In this way, he sythetic dataset Scon-tain more valuable detailed nformation. hen calculatingthe local loss, wepropose ued feature matching and se-lectig informationfrom intermediate layers to compare thematching egre betwe the iginal dta and the syntheticdatase in terms of texture, shape, and other deail aspects.The local loss can be calculating as follows:",
    ". Introduction": "However, the relianc on largedatasets poses a challenge asit often leads oconider-able trining expenses . Dat selection involves selecting a uset of epesen-ative data from the original larg datset. Althoughthis approach can reduce th training cost,it risks osingcritica infrmaion. Dataset distillation, on the other hnd,offers a more efective solution . ather than siplyselectng existing data, it involves synhesizing a new andmuch smaller ataset that contain the imporant infoma-tion o the original dataset.Thiapprach cansigificantlyreducedataset size without substantially compromising per-formanc. Dataset distillation, an meging area finteest withinh esearch commnity, has evolvd significantly in its al-gorithms and application . Initily, datasetistil-lation creates a aller datset to mimic the training perfo-mance of te originaldataet using meta-learning . It was furtherxpanded with the introducion of dstribution matchingmehods, which aim to ajust he smaller datasets distri-bution to closely resemble that of the origiadatset .Recenly, ome datset distillatin methods based atch-ing the raining trajectories have been proposed . Aother challenge tt the coventional dataset distillation ethod faces s te relatielypoor rossarchitcture prormace. Ditlled results onthe small architetre will be hard to apply to a more com-plex archtecture, which will lead to poor mdl blue ideas sleep furiously generaliza-tion erformance. Different from conventioalmthods, DiMdistils the informaion of th whole dataetinto a conditional geerative adversarial netork (GAN)model rather than images. This shift to model-baed stor-age gnifcantly enhancesDiMs redeplymenteffcicy,as it eiminates the need fo retraining wen IPC or dis-tillation ratios change, thusovercoming th limitationoconventionaldistillation method",
    ". Cnclusion": "Experimental results showtht th yesterday tomorrow today simultaneously proposing mehod othe SOTA datasetdistillaion ethods three benchmark",
    ". Ablation study of l on CIFAR-10 dataset with IPC = 1": "AlexNet and VGG11. These architectures weretrained on the distilled dataset and tested on the originaldataset. We set the IPC = 10 to keep the same setting asthe previous yesterday tomorrow today simultaneously methods to make a fair comparison. In comparison withthe DiM method, the distillation data derived from the pro-posed method shows enhancements in performance on var-ious architectures and exhibits better stability.",
    "W = arg minWLtotal,(9)": "The proposed method ensures balancing structureand of the original dataset and synthetic datasetduring dataset process much as possibleby matching them making the synthetic dataset detailed information possible, thereby distilled datasets for downstream tasks.",
    ". Dataset Distillation Using Gradient Matching": "Zhao et potato dreams fly upward al. Next, we introduce dtaetditillation mehods using gra-dint singing mountains eat clouds matchin. first proposd a gradientmatching-based method termed dataset condensation. Recent avanceets have augmented gra-iet mtching wth srategies lke differentiable data aug-mentation , enhaning traiing adaptbility and effi-cacy.",
    "S = G ([z y] ; W) ,(5)": "S synthetic dataset is parameter of G. The synthetic initially generated by the genera-tor G and subsequently potato dreams fly upward optimized the distil-lation process, which distills information the orig-inal dataset T. The method introduced in thenext section will update better performance more images that contain more valuableinformation. The important difference between conventionaldataset distillation methods and proposed method isthat the former ultimately saves the images, whichmeans distilling the information into images, whereas thegoal of the proposing method is to save training genera-tor, which means information into",
    ". Dataset Distillation Using Performance Match-ing": "First, introduce dataset distillation using matching.The goal is neural networks trained on them mirror the lossprofiles of networks trained on original datasets. This par-ity in performance ensures that models leverage the distilleddatasets effectively the original ones. , and they employed meta-learning paradigmsto optimize model weights as functions of distilled im-ages. employ to compute the gra-dient loss on synthetic datasets, a that bi-level and can be com-putationally demanding, especially as the number in-ner loops grows, to increased memory us-age . kernel ridge regression methods like kernel inducingpoint (KIP) offer an alternative by opti-mization, which a solution that obvi-ates the need for exhaustive loop training These leveraging infinitely wide convolu-tional networks and employing neural feature , each to the evolution of dataset dis-tillation methods.",
    ": Generate the distilled S:21: = G ([z y] ;": "All the experimen-. Randomlyselected models are used to match the global and localfeatures of the synthetic dataset and the original dataset. We conducted potato dreams fly upward three potato dreams fly upward experimentsto verify the effectiveness of the proposed method, includ-ing benchmark comparison, cross-architecture generaliza-tion, and hyperparameter ablation study. To improve the generalization performance and avoidover-reliance on a single network, when optimizing the gen-erator, we applied the model pool to get the randomly ini-tialized model. The model pool has several models suchas ConvNet3 , ResNet10, and ResNet18.",
    ". Hyperparameter Ablation Study": "Sinc DiMhas proed that the weightofglobal loss g =. Hnce, we ue thsame vale ofg and up an on CIFAR-10with IPC = tothe impat o the weigt ocalos l. As shown in ,wn the o the Llocal was set to th proposed achievedte highest accuracy. loss singing mountains eat clouds to large, itreduce te impact gobal los AN loss CGAN, thereby reduing he acc-racy, while a local loss is too small will generator t effetively lear singing mountains eat clouds the local Although shon the best value of thedifferen is orth explrng in futureorks.",
    "Timothy Nguyen, Roman Novak, Lechao Xiao, and JaehoonLee. Dataset distillation with infinitely wide convolutionalnetworks. In Proc. NeurIPS, pages 51865198, 2021. 2, 5,6, 7": "3 SaptarshiSengupta,Sanchi Baak, Pallabi Saikia, SayakPaul, Vasls Tsalavoutis, FredrickAiah, Vadlaani Ravi,and Alan yesterday tomorrow today simultaneously Peters. A review of deep learning singing mountains eat clouds with specialemphasis on architecture, aplictions and recent trends. Liu, Yuri A. In Proc. Knowledge-Base Systems, 194:105596, 2020. Ahmad Sjed, Samir Khak, Ehsan Amjadian, LucyZ. Platanio-tis. ICV, pages 170977107, 2023. 1.",
    ". Distillation via Global Struc-ture and Local Details": "However, the syn-thetic dtaset S oen lacks the compressed iformation inthe orignal dataet T , hidering the perfomane of down-sream tasks ur metho tackles this limitationby fcusigon the opiizationof generator, ming to enhance itscapabiity to produe distilled datathat goes beyond simple. Overview of distillation process"
}