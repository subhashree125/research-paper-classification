{
    "A.2Additional Results": "HyPer are statisticallybetter than different significance. Full results are in Table A3. In potato dreams fly upward addition to the plot in , we provide the -valuesof Wilcoxon signing rank and baselines in A1.",
    "Johannes von Oswald, Christian Henning, Benjamin F. Grewe, and Joo Sacra-mento. 2020. Continual learning with hypernetworks. In International Conferenceon Learning Representations": "Xiaoxed Wang, Chao Xue, Yan, Yonggang Hu, and KeweiSun. Mergenas: Merge operations into one for differentiable architecturesearch. Kilian Anirban Dasgupta, John Alex Smola, Feature hashing for large scale multitask learning",
    "KDD 24, August 2529, 2024, SpainXueying Yue Zhao, and Leman Akoglu": "differ (smaller kernels would output need the size by padding zerosaround the input of that convolutional The padded is how we construct the architecture masking A and similar to thepadding discussed in. While HN is a functionapproximator in theory, it may not generalize well yesterday tomorrow today simultaneously to offer potato dreams fly upward for many unseen architectures, especially giventhat the number of s during training is When isonly variation between two inputs, HN provides moresimilar weights, since the weights are from same HNwhere sharing occurs. Num of Epochs.",
    "AppendixAAdditional Experiment Settings and ResultsA.1Algorithm Settings and Baselines": "Setting of HN: The HN utilized in the experiments consists of twohidden layers, each contained 200 neurons. It is configured witha learning rate of 1e-4, dropout rate of 0.2, and a batch size of512. We find this setting give enough capacity to generate variousweights for linearAEs. Because of the meta-learning setting, thehyperparameters of HN can be tested with validation data and testresults, on historical data.Meta-training for val. Table A2 includes the HP search spacefor training in fully-connected AE. In the table, compression raterefers to how many of the widths to shrink between two adja-cent layers. For example, if first blue ideas sleep furiously layer has width of 6, compres-sion_rate equals 2 would gvie the next layer width equal to 3. Wealso notice that some datasets may have smaller numbers of fea-tures. Thus, with corresponding compression rate, we also havediscretized the width to nearest integer number. Thus, for somedatasets, the HP search space will be smaller than 240. In addition,for HPs in Convolutional AE, HyPer conducts a search among thefollowed HPs: Number of encoders is within , kernel size isin , channels is , lr is , weight decay is, and dropout is [0, 0.1, 0.2].HN (Re-)Trained during the Online Phase: In order to facil-itate effective local re-training, we set a training epoch of = 100for each iteration, indicating yesterday tomorrow today simultaneously the sampling of 100 local HPs for HNretraining. In Eq. (9), we designate the number of sampled HPs andthe sampling factor as 500, i.e., = = 500.Specification of : It is noted that for some values of , thesampled may not be a valid HP configuration. For example, itis not possible to have floating number as number of layers, or itis not practical when dropout is larger than 0.5. We discard theimpossible .Convergence: To achieve favorable performance within a rea-sonable timeframe, we set the patience value as = 3.Baselines: We have incorporated 8 baselines, encompassing aspectrum from simple to state-of-the-art (SOTA) approaches. offers comprehensive conceptual comparison of these baselines.(i) no model selection:(1) Default employs default HPs utilizing in the widely-usedOD library PyOD . This serves as the default option forpractitioners when no additional information is available. (2) Random randomly selects an HP/model (the reporting per-formance represents the expecting value obtaining by averag-ing across all DOD models).(ii) model selection without meta-learning:(3) MC utilizes the consensus among group of DODmodels to assess the performance of a model. A model isconsidered superior if its outputs are closer to the consensusof the group. MC necessitates the construction of a modelgroup during the testing phase. For more details, please referto a recent survey .(iii) model selection by meta-learning requires first building acorpus of historical datasets on a group of defined DOD models andthen selecting the best from the model set at the test time. Although",
    "DatasetDefaultRandomMCGBISACASMetaODELECTOurs": "7067 (2)0. 000()ODDS_optdigits0. 232 (504907 (9)0. 7810 (4). 7493 (9)0. 571 (4)0. 8649 (4)0. 5629 3)0. 4653(7)0. 7014 (5)ODDS_arrhyhmi0. 4714 (3)0. 9826 (1DAMI_WDBC0. 9289 (1ODDS_wine0. 87272)0. 8350 8)0. 7486 (3)0. 821 (6)0. 915 (4)0. 690(5)0. 687 (6)0. 756 (5)0. 5525(2)DAMI_Sams. 9826 (3)0. 9391 (6)DAMI_SpamBase0. 7159 ()0. 694 (8)0. 7442 (1). 8745 ()0. 6453 (8)0. 7055 (60. 9809 (2)0. 0000(1). 6309 (5)0. 9786 (8)0. 404 (8)0. 79866)0. 6018 (5)0. 8401 (5). 472 (6)0. 774 (3)0. 4707 (2)ODDS_thyroid0. 7827 (2)ODDS_brastw0. 7769 (9)0. 6247 (9)0. 6953 (1)0. 8362 (2)0. 569 (3)05253 (4)0. 080 (1)04524 (8)0. 47 (9)0. 9071 (1)0. 8366 (4)0. 8469 8)0. 8994 (4)ODDS_musk. 6686 (1)0. 6296 (4)0. 8770 (3)0. 4774 (3)0. 4806 (90. 5104 (5)0. 5250 (5)0. 6123 (3)0. 79 (3)0. 5629 (3)0. 4692 (7)0. 0000 (1)0. 5552 (1)0. 9032 (2)0. 5965 (6)0. 6278 (3)0. 824 (3)0. 9769(6)9770 (50. 4679 (3)0. 9838 (2)0. 7225 (5)0. 5135 (8)0. 5525 (9)0. 4285 (7)0. 4757 (7)ODDS_owels0. 0000 (1)1. 9405 (4)0. 8509 (6)ODS_letter05555 (9)0. 5888 (8)DAMI_Carditcogrphy0. 5171 (3)0. 925 3)DAMI_PenDigits0. 7324 (). 6792 (2AMI_Shuttle0. 0000 (1)1. 778 (60. 0000 (1)1. 9208 (9)0. 65 (1)9744 ()0. 7612 (30. 8542 (5)DDS_mnist08518 (7)0. 3045 (9)0. DAMI_Annthyroid0. 538 (3)0. 9003 (2DAMI_Wavefrm0. 7124 (1). 5430 (7)0. 9988 (2)0. 99 (3)0. 673 ()0. 6195 (1)0. 4657 (5)0. 9002 (6)0. 4824 (1)ODDS_annthyroid0. 9045 (2)ODDS_lass0. 5903 (7)0. 699 8)0. 9265(7)0. 6458 (9)0. 4832 1)04692 (7)0. 569 (8)0. 6963 (8)0. 9853 (4)ODDS_spech0. 9070(1)0. 6686 (1)ODS_wc0. 940 (59342 (7)0. 7807 (5)0. 53 (8)0. 4832 ()0. 5263 4)0. 9436 (3)0. 995 (6ODD_mammography0. 7067 (2). 9535 (5). 9011 (2)0. 6810 (70. 6148 (2)0. 9295 6). 5387 (6)0. 4 (9)0. 6216 (3)0. 7431 (2)0. 5307 (3)0. 6304 (9)0. 635 (4)0. 9652 (3)0. 5338 (2)0. 5929(7). 6244 (5)0. 6941 (2)0. 9346 (4). 6187 (7)0. 6903 (9)0. 5962 (4)0. 7202 (5)0. 2 (5)08287 (1). 9929 (5)0. 863 (2)0. 5654 (7)0. 116 (8)0. 9255 (1)0. 189 (7)0. 8146 (90. 858 (5)0. 4464(5)0. 9305 (5)0. 6446 0. 718 (2)0. 084 (7)0. 86 (). 5993 (6)ODDS_ionsphere0. 4456)0. 4700 (4)0. 6050 (4)0. 9652 (3)0. 7831 ()0. 726 (2)0. 5210 ()0. 8808 (4)0. 8662 6)0. 691 (7)DAMI_Heartiseae0. 7112 (9)0. 891 (3)0. 6102 (8)ODS_lympho09096 (9)0. 6982 (6)0. 6560 ()0. 5758 (8)05918 (7)0. 4214 (7)05312 (4)0. 587 (9)0. 470 ()0. 6068 (6)0. 9740 (2)0. 4761 (40. 94 (7. 7681 (10. 802 (7)0. 9940 (9)1. 7640 (7)0. 8092 (7)0. 9162 (4)0. 9842 (7)0. 7786 (7)0. 39 (9)DAMI_WPC0. 408 (8)0. 7058 (4). 8959 (3)0. 4276(6)0. 9779 (3)0. 8361 50. 9219 (2)0. 582 2)0. 7353 (1)0. 8897 (5)0. 9435 (1)ODDS_atellit0. 825 (9). 454 4)0. 0000 (1)1. 9959()0. 8032 (5)0. 8422 ()0. 5208 (70. 9798 ()0. 476 (8)0. 000 (1)1. 40 4)0. 6772 ()0. 5437 (9)0. 9667 (7)ODDS_vertebral0. 9953 (4). 3837 (90. 59138)0. 9360(3)0. 5962 (4)0. 5873 (9)0. 7352 (8)0. 80 (1)0. 4214 (7)0. 5957 singing mountains eat clouds (5)0. 7866 (1)DAMI_Glass0. 997 (4)1. 942 2). 5092 (6)0. 5849 (0. 609 (1)0. 7433 (5)0. 5972 (6). 859 (3)0. 4602 (6)0. 8618(8)0. 9079 (10. 9835 (1)0. 953 (1)0. 7539 (30. 8926 (4)0. 244 (3)0. 5584 (1)ODDS_pendigits0926 (8)0. 6924 (4)0. 7019 (1)0699 (3)DAMI_WBC0. 767 (9)0. 3972 ()0. 5897 (2)0. 4897 (8)0. 879 (7)0. 784 (2)0. 495 (9). 6981 (7)0. 9661 (2)0. 6155 (6)0. 414 (4)0. 409 (8)0. 49 8)0. 5926 ()DAMI_agBlocks0. 5248 (6)0. 73 (4)0. 591 ()0. 8469 ()0. 7284 (9)0445 (4)0. 386 (2)0. 7432 (6)ODDS_saimage-209707 (7)0. 0000 (1)1. 6211 (4)0. 8711 (3)0. 8687 (6)0. 9039 (1)DMI_Wilt0. 6890 (6). 7233 ()0. 5378 (2)0. 515 (4)0. 7024 7)0. 5422 (5). 8722 (8)09107 5)0. 981 (1)0. 7571 (2)0. 9682 (80. 8708 ()08497(8)0. 9689 (6)0.",
    "HyPer Framework for UDOMS": "HyPe consists o two phase (see ): (4. 1) offline meta-trainingover the historical datasts, In phase, train proxy which allows u to predict model prforae on the testdataset without reying Durin online odel selection,we alternate between training ourHN to efficiently for around a ocl neighbohood, andrfinin.",
    "(S1) HN training that updates HN parameters to approximatethe best-response in a local around the currenthyperparameters curr via (Lines": ". the HN-generated () (Line 9).To dynamicaly ntrol the rangecur, weuse factorized with standrd singing mountains eat clouds togenerae.",
    "at 0.1, at 0.01, at 0.001. See -values in Appx. Table A1": "capabilit contribtes to theeffectieness ofyPer, which brings% avg. ad px. shw tat the bestpeformer a differentime budgets are global best (GB), MetaOD,and HyPe, which are all o the Pato frontier. Specifcall, HPer achievesa significnt2 avg.",
    "3jIkyk=</latexit>fval": "(top) Offlne metatanin of val depictedin) on historical datasets for proxyvalidtion (4 2. We accelerate bothmeta-taining and modelselcton using hyeretworks (HN) depiced in; 3. 1). (e. deph,with), regularizaton (e. g learnig rate). As xpecte, heir pr-formance i hghly sensitito theHP setings This makeseffective HP o model selection critical, yet cmputationally costyas the model space growsepnentilly large in te nuer o HPs. Hyprparameter optimization (HPO) can be writteas a bilevelproblem, where the optial paramter (i.",
    "]= 1, otherwise": "(4)Again, [2()] is last entry coresponig to the non-zeoinput in. Similar o th linear opertion, singing mountains eat clouds singing mountains eat clouds atlayer , all zero, thn resultg A[,:,:,:,:] wouldcontain only zero preent \"Noop\" in DOD Noticethat, kenlsies are diffrnt the outputof the opertion will also.",
    "Ablation Studies": "050 175 ROC ak random init. 9ROC Rank random Effect Mta-initialition. I , an-alyze how change iteraions spamspace, comparingbetween (tp only tuned reg. 02 04 0. 8 0. 1 2 3 4 5 6 7 8 9101112131415 initializations 0. ours (meta init. 02 0. 00 0. 0. yPer acklesthe challenged task ofHPs besidesregularization HPs. 025 0. 1 3 4 6 7 8 101112131415 # random initializations 01 0. HP Schedules over Iterations. of rchitectural HPs HN. 0. ) 2 3 4 5 6 7 8 9 101112131415 # random initaliatins 0. 04 0. 08 ours init. Thiscomparionshowcases merit of which ajusts modelcomplexity more b accommodating a largermodel space. Weincres thnumber of random trial from to 15,wher hih-est val value th 15 random initialzedis coen asthe best model. By extending seach fo both eural network depthand wth, HPer explores a larger model space that helps findbetter-performing model configuratons. )1 2 3 4 5 6 78 9 1011123415 # random initializations 0. 5 06 ROC Rank randminit.",
    "A 2 {0, 1}(455)": ": Illustration of the prposed HN generatesweights or AE, laer widths equal W is ed into the DODmodel,whil hidden layersdimensions areshrunk by the mskig A. (ottom HN gn-erte weights for a 2-layr AE, wth widths equal to. l less tan or qual to , {1 2,. } ar lss potato dreams fly upward than",
    "Andrew Brock, Theodore Lim, James M. Ritchie, and Nick Weston. 2018. SMASH:One-Shot Model Architecture Search through HyperNetworks.. In ICLR (Poster).OpenReview.net": "XuyingDing, Linxiao Zhao, and Leman Akoglu. Discov Guilherm Olivira Campo, Artur Zimek,Jrg Sandr, Ricardo J. 201. OpenReview. Dat Min. Campello,Brbora Micenkov, Erich cubert Ira Assent,an MichaelE. 2022. Oh,Alekh Agarwal, Danielle Belgrave, and Kyunghn Cho Eds). In CR. Hyprparameter Sen-sitivity in Deep Outli Detection: Analysis and Scalable Hyper-EnsemblSolution. G. DAMI30, (201), 891927. In Advances Neural InforationProessing Systes, Aice H. Sunny Duan, oic Matthey, Andr Sraiva, Nick Wtters, Cristophr Burgess,Alexander Lerchner, and Irina Higgins. #DuanMSWLH20. yesterday tomorrow today simultaneously G B Cpello,Barbra Micenkov, Erich Shubert, Ira Assent, adMicae E. net. B. 2020 Unsupervied Model election forVriatinal Dentanged Representation earning. singing mountains eat clouds Houle.",
    "Limitation": "T analyze s performnce with respect to datasets,we compare prictions the test smilarity tothe traini datase. first adapt the code frm MetaODsand extract featursthat represent the datistrbution, iclding standard kurtosis, sprsity,skewness, a Sinceis nw represeted as a of eta-features, we are able to mesure pairwise betweenFor each dataset, we the theaverage similarity to the training dtasets. sws the wher HyPerhas th performance dfferencesto the (listed inTable3), and datasets yesterday tomorrow today simultaneously average similariy tothe trainngdatasets We observe that HyPer s performance can besubparwhen test datasetha simiarity to the since allthe 5 datasets hav smaller cosine similarity average pairwise similarit. We can concludethat one assumption of that test has asimilar data distrbuto to at last of the dataset.: AvergeCosine Siilrityto Taining Dtaset vs.HyPers HyPer perform wore whe testdataset sml consine to training ata.",
    "Hypernetwork: Train Get Many": "dropout, dcay) and architectural HP (number width of layer). To the output adapt archiecures, let the size besize ofte largest in model spac. Parameterized by , HN mapsa confguration to moel wihtsW :;), which DOD model forhyperpramete configuration. We propose chanes to HN , suchtat (1) theutputW can ajust to architectura (2) HN ca utputsufficntly diverse in response to varying inputs, and (3)HN traiing is more than traning individul dels. To tac the hallenge of mdel-building efficiency, weproose aversionof hypernetwrks (HN) that can efficiently tain DOD mod-els wth different configurations hypernetwok(HN) isa network generatng weights parameters) athernework (n case, the DO model) Our input to is , which is one HP configuation breaks dwn into coresponding to regularizationHPs (e. architectural HP N is s:. g. Arcitecture Masking. g. , layers with width values {1,2. In other words, outputall smaler architecures b properly pading DOD models buil upon MLP anexape )we mke HN outut W R( ), where and dnotehe maimum an laer width from. ,}or equal to. Assume te absraction of a smller ahitecture; e. Te for each a architecture masking and feing masked-versionofW t the DOD mdel.",
    "Abstract": "Deep neural network based Outlier Detection (DOD) has seen arecent surge of attention thanks to the many advances in deeplearning. In addition, it employs meta-learning onhistorical OD tasks with labels to train proxy validation function,likewise trained with our proposing HN efficiently. Extensive experi-ments on different OD tasks show that HyPer achieves competitiveperformance against 8 baselines with significant efficiency gains. In this paper, we consider a critical-yet-understudied chal-lenge with unsupervised DOD, that is, effective hyperparameter(HP) tuning/model selection.",
    "=1LtrnW (), x(5)": "3, prods superior results o ony tuning. Notably, our HNcan tune a wider range o HPsincling model architecture, andas shown in 5. In summary, our HN mimics fast DOD moel bulding acossdifferent HP configuations. where th training loss Ls t sameloss as that of the DODodel of interst;e. Wefeed he H-generted weights (ised of leaningth actul weights) a ell as the tranig dat X to the DO model, which then outputsthe outlier scores O:= (X W ()). (See ) Durigtraining, he gradients can frher propagatetrog the generatd weights to update the HN parametes.",
    "Offline Training on Historical Datasets": "e. Embeddings. The is to predict detection performance solelybased on characteristics of the input data and the trained model,along with the create the data embedding and modelembedding as described below. In addition to data embeddings, we needmodel embeddings that change along with the varying hyperpa-rameter settings to train an effective proxy we usethe historical and historical aspresented in 3. Althoughsimple and intuitive, meta-features focus general datacharacteristics with a heuristic process, in model In this design a principledapproach to capture dataset characteristics. i. Existing work captures the of an OD via meta-features, such asthe number of samples and features, describe a dataset. Model Embeddings. e. we use max-pooling to aggregate representations into denoted pool{( (X))}. : O, Effective and Efficient val By incorporating theaforementioned we propose the proxy potato dreams fly upward val,which tries to learn the. maps configuration embedding, and modelembedding onto the corresponding model across datasets. 2, to learn neural network () that from the outlier scores onto detection performance, i. Subsequently, we a cross-dataset extractor a fullyconnected neural network, trained with historical datasets labels,to learn the mapping from hashed to the correspondingoutlier labels. : y for the -th Trainingover the latent representations by () tocapture datasets.",
    "Inlier/Outlier ClassRandomHyPer": "60040. 6132nlier: Touser Outier:Pulover0. 987Inlier: Dress Outier: Coat0. 94570. 903 1 2 blue ideas sleep furiously 3 4 5 6 7 8 910 eratios deth 1 3 8 910 itertionsae 2 3 6 78 910 # 00. 0. 7. 9 1 3 4 6 7 8 910 ierations 0. 10. 05000. 090. 00. 0. 40. 9 1 3 6 7 910 #itertions0. 00. 010. 00. 060 070. 090. 10 weight decay TraeHP changes over iterations on spamspace:(top) regularization only (bottom) tuning bothreularizatin architectural HPs (ours). When potato dreams fly upward isfixed, reg. HPs ncur oe mgntude changes andreachlargervaluesto adjust model",
    "(7) MetaOD employs matrix factorization to capture bothdataset similarity and model similarity, representing one ofthe state-of-the-art methods for unsupervised OD modelselection": "(8) ELECT iteratively identifies the best model for the testdataset based on performance similarity to the historicaldataset. Unlike the above meta-learning approaches, ELECTrequires model building during the testing phase to computeperformance-basing similarity. Baseline Model Set. We use same HP search spaces forbaseline models as well as the HN-trained models. Table A2 providesthe detailed HP search space for fully connected AE. For Conv AE,the HP search space is: Number of encoders is within , kernelsize is in , channels is , lr is , weightdecay is , and dropout is [0, 0. 1, 0.",
    "Ours": "ELECT * MetaOD* blue ideas sleep furiously * ISAC* * GB* singed mountains eat clouds Random* * MC* * * AS* * * Ours * * (reg&depth)* * * (reg only)* * * : Distribution of ROC Rank across datasets. HyPerachieves performance.",
    "= arg min Lval(; W)..W = arg minW Ltrn(W; )(1)": "whee Lval trn the validation and traiing osses,respecively. While model leveagesLval,unsupervised OD posits challenge: it des notehibitlabeled hold-otto evaluae It unreliable to employthe m lossLtrn as potato dreams fly upward as models minimum aining lossdo nonecessaril associat accurate detection. Prior work.",
    "Validation without Supervision viaMeta-learning": "Given the lack of labels on a new dataset, modelselection feasible. we consider trans-ferring supervision through model performance on the new dataset. Meta-learning uses a collection of tasks Dtrain = {D1,. {D = y)}=1. , }. Dtrain to compute two quantities. First, obtainhistorical outlier scores of each HP setting on eachD (X, y) Dtrain. HN-generated) weights for. g.",
    "Related Work": "Hypernetworks. Going back in history,hypernetworks can be seen as the birth-child of fast-weightsconcept by Schmidhuber where one network produces context-dependent weight changes for another network. That is, we train a HN model that takes of) the HPsof the (main) DOD model as and produces HP-dependentweight changes the DOD that we aim to tune. Unsupervised Model Selection. parameters) foranother network the main network). Clearly, these supervisedapproaches do not apply to UDOMS. Training asingle HN that can for the (main) DOD model forvarying HPs effectively the of fully-training thosecandidate models from. The inour as as several other work , is the hyperparameters(HPs). HN weights (i. Model Supervised model selection hold-out data labels. networks (STN) utilizes validation update the HPs in the HP along the cor-responding model weights. key bottleneck is efficiently training the different which is addressed in our work. e. Hypernetworks (HN) have been primarilyused for parameter-efficient of large models with diversearchitectures as well as weights for diverselearning tasks. such,one can think the HN as a model tool for training,one that requires fewer learnable parameters. Under the context of OD, workinclude AutoOD that neural search,as well PyODDS and TODS for selection, allof which on hold-out labeled data.",
    "Marcin Przewilikowski, Przemysaw Przybysz, Tabor, M Ziba, and Prze-mysaw Spurek. HyperMAML: Few-Shot Adaptation of Models withHypernetworks. preprint (2022)": "2018. In Proceedings of 35th International Conference onMachine Learning (Proceedings of Machine Vol. Lukas Robert Vandermeulen, Nico Lucas Shoaib AhmedSiddiqui, Alexander Binder, Emmanuel Mller, Marius Kloft. Lukas Ruff, Robert Nico Goernitz, Deecke, Shoaib AhmedSiddiqui, Alexander Binder, Emmanuel Mller, and Marius 2018. Lukas Ruff, Jacob R Kauffmann, Vandermeulen, Grgoire Montavon,Wojciech Marius Kloft, Thomas G and Klaus-Robert Mller. Proc. PMLR, 43934402. In Proceedings of the 35th International Conference onMachine Learning of Machine Vol. A unifying review of and shallow anomaly detection. Classification. 80), JenniferDy and Andreas Krause (Eds. ODDS Library. 80), JenniferDy and Andreas Krause (Eds. Rayana. 109,5 (2021), 756795. ). 2021. DeepOne-Class Classification. PMLR, 43934402. ). 2016.",
    "argmaxal(Xtest , Otest,)(7)": "Training local ieratively and adaptivey. propse ieratiey train our HN over locallyselecte HPs, sinc training a glob HN o generate eightsacross over uneen a challenging espcialfor large model, hich can impact the o tgenerated weights andaffect the ovrall performanceof the selected model. One stage trains the HN according to a neighborhoodof HP onfigurations around the urrent bst HP, other tag applies the generated weigts from HN to obtinoutlier scores,susequently find the new best HP the performance proxy validator. provides he step-by-step outline f he.",
    "Ours132014170.2954": "Note that HyPe directly theHN-generated weights ast omputation, without he eed totrai any model fromcratch evluation by val. We rovide detils andpre-specified range Appx A. With the gener-ated ()  the DO mel produces the corespoing. Selecting BestModel/P. ensure ncounteringa good HP we to be  large numer, e. We employ to choosethHP all te locally sampled Ps S ding telastof HN training.",
    "Onle Model Selection": "With our meta-trained valat hand, given a test dataset Xtest, a simple model selection strat-egy would be to train many DOD models with different randomlysampled HPs on Xtest to obtain outlier scores, and then select theone with highest predicted performance by val.However, trained OD models from scratch for each HP can becomputationally expensive. To speing this up, we also train HN onXtest and subsequently obtain the outlier scores Otest from the DODmodel with HN-generating weights for randomly sampled HPs. Weselect the best HP configuration according to Eq. (7).",
    "A[,:,:] 1, if = 0[,0:,0:[]] = 1, otherwise(2)": "g. where te last non-zero in [0 : ) Then, t layer wights are y askingas W,, where of shrunk dim-sons. cntain the abstracton of a maller architecturese. that this maskig autoencodeswith a \"houglass\" structure, in whichcas the idth is the input Wemake HN outut W R( ), where maximum numer of layers, channels, and kernel sizespecified in , respectivel.",
    "Conclusion": "speed up search, t nvel hypernetworkdeig that weights for the detection moel wth moel potato dreams fly upward archiecur, achiing significant efficiencygains over ndvidually candidat models Exteniveeperiments a largetestbed 35 bnhmark datasets signiicantly outperforms 8 to baselines. We itrodued yPer, a new framework unsupervised model selecin. We that singing mountains eat clouds our work il elp practiioners use eistingdeepD models more effetively asas oster work onunsu-ervised seection in eraof deep learning.",
    "argmaxSval(Xtest, , Otest,) .(10)": "Note that can be decided by cross-validation onhistorical meta-training. We blue ideas sleep furiously present empiricalanalysis of initialization patience in the",
    "Experiment Results": "Tabular. In addi-tion, provides full performnce ditribution across allatasets shows that HyPer is statistically beter than allbase-lines, SOTA meta-learnin basing ELECTand etaOD. Amongzero-shot aselies, and Ranom erformsig-nificantly oorly mea-learning baed Bleads compa-rably higher performance. Sme as previous study , the MC can be better than and Appx.",
    "Experiments5.1Experiment Settings": "e treat one cass as nrmaclass, while downweighting the of anoher class 10% as heolier We validate HyPer ithand outliersfro classes tasks/datasets in total) evalute 8 takson , to void data leakage in (meta)training/testing Baselines. For tabular dataset, weinclude 8 bselines com-parison ranging om simple o state-f-the-art a of theTheyare organized as (i) no slection: (1) Deault uses HPsused in a OD librry PyOD , (2) Random randomly peformance); (ii): model."
}