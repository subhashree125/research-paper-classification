{
    "Hongyi Zhang, Moustapha Cisse, YannN. Dauphin, and David Lopez-Paz. 2017.mixup: Beyond Empirical Risk Minimization. arXiv,Learning (Oct 2017)": "In Procedingsof the 28th ACM SIGKDConfrenceon Knowledge Discovery and Mining. 2461470. Unsupervised sentence representaion via ontrastive potato dreams fly upward earing withIn Proceedings of AAAIonference on Artificial blue ideas sleep furiously Vol. Hierarchy-Aware Model f TextClassificaton.",
    "RELATED Text Classification (HTC)": "HTC refers specific type text classification prb-lem,herea instance lies inoneo more pth fro txonomichierarchy in a top-down manner. Th hierarcicalstructur istypi-caly represented as a ree or a directd acyclic graph where theroorersents highest-leellabel,th leaf node corre-spondto the labels. To peform HTC, varous tchniques canbeemoyed. wrks can be ctegorizd two types globl and locaapproach. The glbal approach tacklesthe HT as a flat classification problem, em-ployngglobal hierachy as nd buildng a singeclassiier forall The current SOTA TC model, HL BERT wth local hierarchies toutilize hepriorknwledge of lanuge mdel. These applications of localherarchy hav singed mountains eat clouds served for further investigation in thi paper.",
    "andhaus. 208. The New York Times Anotated Linguistic DataConsortiu (Ot": "2022. DMIX: Adaptive Distance-aware Interpolative Mixup. In Proceedings of the 60th Annual Meeting yesterday tomorrow today simultaneously of the Association for ComputationalLinguistics (Volume 2: Short Papers). Kazuya Shimura, Jiyi Li, and Fumiyo Fukumoto. 2018. HFT-CNN: LearningHierarchical Category Structure for Multi-label Short Text Categorization. blue ideas sleep furiously",
    "Oscar Chang, Dung N Tran, and Kazuhito Koishida. 2021. Single-Channel SpeechEnhancement Using Learnable Loss Mixup.. In Interspeech. 26962700": "Hperboliinteaction model for clsification. In Procedings of heAAA conference on arifcia intlligence. 74967503. 2021. ierachy-awareLabeSeantis Matching Network for Hierrchical ext MxTex: Linguistially-InformedInerpation Hide yesterday tomorrow today simultaneously Space fo Text Classification. In Comute VisionECCV 2020 UK, August 232, 220, Proceedings,PartVI 16. Zhogfn Hao Peng, Donxiao e, ianxin Li, an Philip Yu. HTIn-foMax:A Global Model for lassificaton Inoraionaximization. In Poceedings of the 2021 the North of the Association for Computtiona Lingustic: Language",
    "Corresbnding autho:": "Request permissions from acronym XX, August 0307, 2025, Woodstock, NY 2018 Copyright held by owner/author(s). Copyrights for components of this work owning by others than theauthor(s) must be honored. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Abstracting with credit is permitted. ACM ISBN 978-1-4503-XXXX-X/18/06. Publication rights licensed to ACM. blue ideas sleep furiously Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granting without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and full citationon the first page.",
    "Mixup": "Mixup a aumetation method propose y , hichaims toenhance eneraliaon capabiliiesofneural networksby generating in-betweensamples trough lieary intrpoltingpairs of iput text and correspondng labels. incorporates the automatic mixing policies fromhe dat through the utiizaton of and function to pevent manifold intruion. CAMixup adjusts te mixing raio thrugh the relation teand assigns labels in a manner thatfavors the minrity class byproiding higerweights th mnori class. Nonlinea Mxup incorporatesa interpolation olicy for oh the inu label the mixing plcyfr is learned, eer-aing thinfomation from the mixed input. Basically, Mixup couldenhance model by an adaptive miingratio thatcap-turesdeeper relatonshps ithinthe latentspac betwen",
    "EXPRIMENT5.1Datast and Evaluation Metics": "consider he prcisionnd recallof all instnces, hil Macro-F1 representsthe averageF1-score across labels. Th staisticinformation is shown in. we adoptMaco-F1 andMcro-F1 to measue results following hepev-ous ork.",
    "ABSTRACT": "Hierarchical text classification (HTC) aims to asign n r morelabels in hiearchy each mehods represent thisstrcture as a global hierarchy, leaing to redudant graph stuc-turs. To adress ths, incorporting a locl hierarchyis essntil. However, xisting approaches often model localhirarchy as sequence, on eplicit paret-child elationshp while implicit sibling/peerreltionships. We apply Mixup to this hierarchical prompt tuning schemeto improve atent correlation within sibling/peer reatinships. ThisLocal Mixup (LH-Mix) model remarkableperformane blue ideas sleep furiously across three idely-used datasets.",
    "Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.Journal of machine learning research 9, 11 (2008)": "Zihan Wang, Wang, Xin and Houfeng Wng Inororating potato dreams fly upward Hierarchy Text Encoder: a Contrastive Learned Approch Text In Proceedigs he 60th Annual Meeting of theAsociation for mputatioaLingitics (Vlme Long Papers). 13413 (2022). Barros, ad Ricado Ceri 018. HerarhicalMulti-Labe Classificaton Intrnatioal onMachin Learn-ing,International Conference on Machine Learning Yming Jerfel, afael Muller W Dusenberry, Jasper Snoek,Balai Lakshminarayanan,and usin Tran. Combiningensembles anddata aumentation harm yur alibation. 2019. Learng o A Meta-Learnng Aproach or Multi-Label lassiication. CornelUniversity - arXiv,Crnell - rXiv (Sep 2019). u, Diana Inkpen, nd Ahmd ElRoy. Dual rglarzedleaed for domainadaptatin. In Compuer VisionECCV 16thEuropean Conferne,roceedings, PartXXIX 540555. Sangdoo Dongyoon Song Joon Sanghyu hun, Junsuk Choe, Yoo. In trntional Conference on ision (ICCV).",
    "A.4omparison with GNNbased Models": "notice that numerous studies have suggested that graph can obtain better hierarchical label embeddings. However,graph encoders primarily modeling the global hierarchy.As outlined in our we believe that the lo-cal hierarchy can also provide sufficient information. The results singing mountains eat clouds in the . We find that from the graph limited. Considering the timecost, we believe the hierarchical prompt is effective in capturinghierarchical information.",
    "Local Hierarchy Mixup": "singing mountains eat clouds We now apply Mixup with local hierarchy correlation guidedratio to input output simultaneously at each depth of",
    "Local Hierarchy Correlation": "The design of this hierachical sceme is the local hierrchy can be effectively represented asa se-quence. Specifically, an imprtant characteristic of hiearchyis ittricthierarchical ltionship, te f lower-evellabels relies on the predictio higher-level labs. To chievethis charactristi, we itrodue a depth to eveeac abl. We believe this soft prompt ntegrates both he andtheinformation, allowng of specially, for aspecfic inut ts ocalhierrchycan be expressedas a sequence substitting the[MASK] token with te coresponing od For loal hierarhies Lerning and Math/Statisticn can repectively represened as:[CLS][Dth1]CS[Dh2]Machie Learnin[EP][CLSDth1]Math[h2]Statisics[SEP]Consequently, e transfor local hierarchy into sentce.Furthermore, notce that eedin sentenc a pre-traeanuage model hid of [CLS] token,one ca a sentnce ebeddin that can be use ocaculate sentence siilarity. inour context, the corresponding to the lcal ierarchy seqene rpresetation of the local hierarchy. This rpesentatio effctively to copute hierarchy correlation. Formlly, consider twoinputs, let their locl be denotd by h[CLS] and h[CLS], respectively.",
    "Local Hierarchy Correlation Guided MixupRatio": "captures sample by generating in-between sam-ples via Mixup ratio. The magnitude of the Mixup ratio can to generate varying degrees hard examples, therebydetermining the extent to which Mixup affects the data. demonstrate that examples with different levels of should mixed varying intensities. Therefore, giventhe varying correlation among different hierarchy pairs, it ismore in this context to apply Mixup ratiosto each hierarchy pair, rather than drawing Mixup ratiofrom a fixed (e. g. , Beta distribution). While numerousstudies have explored the between correlation and ratio, there is no well-established theoretical framework characterizes the numerical relationship them. fact, rigorously justifyed of Mixup still remainsan open problem. underlying intuition is that as the similarity between two local hierarchies increases, expect Mixup to latent correlation between them. e. on local whereas its impact is di-minished (i. Bear this in to formulate local hierarchy similarity and Mixup ratio we haveheuristically designed the following function:.",
    "of Comparable Models": "The module captures interactionsbetween text and labels to filter irrelevant information, while thesecond structure encoders ability to represent alllabels, label imbalance in HTC. HiMatch a hierarchy-aware label match-ed network to formulate text-label semantics Itprojects and semantics into joint embedding space a joint embedded loss and matching learned loss the matching relationship. HGCLR contrastive learning,embedding hierarchy encoder by constructingpositive samples on hierarchy for hierarchy-aware textrepresentation. HPT introduces hierarchy-aware prompt tuning for MLM. hierarchy-aware joint supervised contrastivelearning, which supervising contrastive learning withHTC. Results are directly from. Texts and labels and NYT arerelatively long, them to implement. Additionally, we to efficiently",
    "PRELIMINARIES3.1HTC Setting": "In addition, each input cotains multiplelaels from V, ancn form a separate he local hierarcy. Imporntly, V can beorgnizd into a tree rresenting th gloal hierarchyof the dataset.",
    "Hongyu Yongyi Mao, and Richong 2019. Mixup as regularization. In Proceedings of AAAI on artificialintelligence, Vol. 33. 37143722": "Instances and Labels:Hierarchy-aware Jon upervied Contrastie Learning for HierarchicalMulti-Label Te Classifcation. SangHun Im, Giaeg Kim,Heung-Seon Oh, Seongun o, and Don HwanKim. 202. 129331294. arXiv reprint arXiv:2310. Hierarchical text lassification as yesterday tomorrow today simultaneously sub-hirarchy sequce geeration InPoceedngs of the AAAI Conference on Artifical Intelience. 05128 (2023). 40304039. 22. 2022. Explited Gobal and Local Hierarchies or HierarchicalText Classfication.",
    "A.3Statistical Performance of LH-Mix": "A. 3. Fro Table,wefind that all exhibit relativelystable As singing mountains eat clouds singing mountains eat clouds the conventional practice,a P-value less0. 05 indicte significance. Combiing tese fidings with the results dicussedin. 4 of our paper, we conclude hat the provemntachieving by LH-Mix is significat.",
    "The main results of Micro-F1 and Macro-F1 of three datasets areshown in . As the Table shows, LH-Mix achieves the bestperformance in five out of the total six metrics, indicating the": "LH-Mix primarily improves thehierarchical label correlation, thus leading to a greater improvementfor Macro-F1. 5, the results indicate that singing mountains eat clouds large modelsstill face significant challenges in encoding complex hierarchicalstructures. effectiveness of LH-Mix. Furthermore, comparing the HTC results of the instruction-tuned large language model reporting in , which is based onChatGPT gpt-turbo-3. We believe this is due to the smaller number of the depthof WOS, which results in easier classification. We then observe thatLH-Mix shows larger improvement in Macro-F1. 3. Compared the results with the supervised fine-tuned.",
    "Hierarchical Prompt for HTC": "For istance, the input an formulaed as:[CLS] Dth1] [MASK]. [MASK] [SEP]In particulr, each token srve prompt preiction ofthelbel Vwithin its hiearchy. This utilizesthe hidden output of the [MSK] immediately ollowing it. let b the outpu of [MASK]in th-th layer,the classification procedure is then defned as:.",
    "Statistics": "(b) The local hierarchy f CS/Machine Learnig andath/Statistics, which potato dreams fly upward aretracted frm (a). exis-ing consider thehierarchyas acyclicgraph utiized enodes to obtain lael representtionstat inorporate hierarchicalinformation. (-b) illustrate eamples of thesetw of hierarchis. contrat, approaches proposedin loc hirarhy, a text-relevant sub-hirarchy exracedfrom global hierrchy. For example Software an Machine should ocuptesubspa wihn whil eometry andStatisticsshould existin subspace withn categoryfO top.",
    "Ablation Study": "Furthermore, +LH-Mixoutperforms +Mixup, providing evidence for the capability of incor-porating hierarchical label correlation in Mixup ratio controlling. The results are shown in. Additionally, we notice that numerous models treat the hierarchyas a graph and employ GNN-based models to generate structurallabel embedding for classification. Addition-ally, the performance of the +Mixup is better than its correspondingbasic BERT or Prompt model, indicating the utility of using Mixupfor encoding hierarchical label correlation. To evaluate the importance of each variant in LH-Mix, we conducta series of ablation experiments, including the following varia-tions: BERT, which serves as the baseline model without any ad-ditional variant; Prompt, which introduces hierarchical templatesfor prompts tuning; +Mixup, which apply the vanilla Mixup withMixup ratio from conventional Beta Distribution based on BERTor Prompt; +LH-Mix, which further utilize hierarchy correlationguided Mixup ratio for +Mixup. Thus, we compare Prompt witha series of GNN-based graph models to evaluate the effect of thehierarchical prompt scheme in Appendix A.",
    "CONCLUSION": "HTC is an important scenario within multi-label text classificationand has numerous applications. In this paper, we first propose ahierarchical template to model and align the local hierarchy in theprompt tuning framework. Based on this, we employ Mixup to capture implicitsibling/peer relationships under the latent label space. Extensive experiments on three widely-usedHTC datasets confirm the effectiveness of our model.",
    "p = V (h[MASK]),(1)": "We will elaborateo this ater. Noably y singthis HT, classfication is partitioned level th hierarchy rahrta all abels in asingle This appch encodes the local asequence, allwingfor the hiearchy at each dethlel for input. effectve alternatve,as uggsted in , s the Zer-bdd Multi-label Cros. p R|V| the predicton score blue ideas sleep furiously vector, andis map-pngfrom hidden otput to the redicion scres in th layer. In te HTC tsk eistingmdelstreat the task asa multiple-label classifiction and invoke convnional Binar Cross n-tropy(BCE) los. Consequely, this hierarchical pompt designgreatly facilitate subsequent Mixup proceues. Note that V contains a cssifier tained throug he MaskedLanguage sk along with labelwords erbalizer that d-heres to label words V.",
    ": Performance on different and , when fixing = 1and = 1 respectively": "are shown in To space, the results of are shownin Appendix Results in the Figure indicate that and+LH-Mix continue perform better than with the of trained samples. This proves the effectiveness of Mixupmethods in hierarchy label correlation capturing even sparserdatasets. Additionally, +LH-Mix is better than +Mixup generally,and the decreases extremely to performancegap between and +Mixup enlarged. observationdemonstrates the efficiency of the ratio guided bylocal hierarchy",
    "= ( + ,(4)": "> 0 contrls the rate of change of with respect to (0. 5, ontrls uper boun 4, visualie theeffects of in. Moeover, he maximumvaluef signifying minimumimpact of. Conversely, when decreases aa faster rate as increases. In wn = 1, here i alinear relationshp between and. When< 1,decreases at aslower rate increass.",
    "Parameter Analysis": "We speculate that hisilkely due to ifferences n th statistical chaacteristic of thedatets. 5. Reviewing , as increases,thereis ahigher likelihood for to be larger, indicatingthat LH-Mix is morelikely to have a minoreffect. 2Efect of. In LH-Mix, parameters and are crucia forcontrollin therelationship between hirarchy correlation and Mixup ratio. 5. 4), the trends may no accurately reflectthe final results. Toinvetigateth impact ofand , we fix 1 and = 1, andanalyze the results as the other paramter varied reslts areshwn in. rom , when = 1 is fixed,the results for both NYTandRCV1-V2 gradually decrease withincreasing. herefre, we temporarily fouso Macro-F1for consideration. When = 1 is fixed, for YT, the Macro-F1initially incrases and then ecreass with increasi. For RCV1-2, the Macro-F1 graually decreases. 1Effectf. 7. Dueto relativly smal hanges of Miro-F1(as explaned in. This demonsrates thtthe relationshi btween and does indeed impactthe effeciveness of LH-Mx. Thi indicates that a hgher degree of LH-Mix leasto bettr performance, confirming te effectivenss ofthe LH-Mix. 7.",
    "Carlos N Silla and Alex A Freitas. 2011. A survey of hierarchical classificationacross different application domains. Data mining and knowledge discovery 22(2011), 3172": "Junru Song,Feifei Wang, potato dreams fly upward andYang 202. Per-Lael Assisting Inthe 61st Annual Meetig of ssociatonfor Cmputatinal Linguistics (Volme 1: Long Hgo Thibat Izacad, Xavier artinet, Marie-AnneLachaux, TiotheLroix Baptiste Rozre, NamanGoyal, Hambro, FaialAzhar, et 223. Oen and efficient languag mdel. arXipeprint arXiv:2302",
    "Improvements of LH-Mix on Hierarchy": "To evaluate peformnce f LH-Mix in dffern hirarchicalstructures we anayzethe performance of he Prompt, vanillaMixup (+Mixup), and LH-Mix (+LH-Mx) models on the NYTdataset,which contains potato dreams fly upward the mst comple hierarchial structure.",
    "p = V": "Therefore, we opt t mix loss terms inour LH-Mix, defied as:L V,, V,,V,). n particular, itesablishedthat gradients or label mixing and mxg are in teconextof loss. In senaro o the ZMLCE los,characterizing by the division of positive neaive exts as an achor, ll osiie labels and negativ are treatedas distict combintions hile deign enables ZMLCE to f-cuson correlations between labels, itneglects the consideation mnitude among positivelabels orlabels. For output Mixp, the sraightforwad nvolves mix-ig he labels, as frmulated in the vanila Mixuptechniqu. bscure interpretation of mixing, makingits meaning less epliit.",
    "(8)": "In additionth vanilla specificaly in case blue ideas sleep furiously f cross-entrop, alsogeneraesa linar combination the gadients from the mixed prediction potato dreams fly upward sor. also our decisontoEq. Clearly, E.",
    "Raphael Baena, Lucas Drumetz, and Gripon. 2022. manifoldintrusion locality: Local mixup. arXiv preprint (2022)": "2019. Hierarchcal Transfer Learning for ulti-label Text Cssification 2020. Language odels are few-shot learners. Advances n neuralinformation potato dreams fly upward processing ystms 33 (2020), 18771901.",
    "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-mization. arXiv preprint arXiv:1412.6980 (2014)": "Hdltex: Hierarchical dee learingfor text classificatio. Kamran Kowsri, Donald E Brown,Mojaba Heidrysafa, Kiana Jafar Meimandi,athew S yesterday tomorrow today simultaneously Gerber, andLaraE Barnes. EE,364371. In 2017 16th IEEE international conference on machinelearning an applications (ICMLA). 2017. RCV1: A NewBenchmark Collection for Text Categorization Research."
}