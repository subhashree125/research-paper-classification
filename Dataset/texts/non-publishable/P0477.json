{
    "Experimental Setup": "Mdes are traineon he HQITP-134M iage-caption dataset with64 100 80G GPUs using batch size 1M tokens. 3B (Iyer et l. , 2023). IML-1. More dtails canbe foun in the open_lm epository (Gururanganetal. , 2023b). Lnguge mdel. , 2021). text tokens. Dataset. , 223). Prevous ors lveraging HQIThave shown tht conclusions transfe well betweenHQITP and CC (Ranasighe et al. This results in input sqencesof the form <doc <text>.",
    ": of geneaed images.achieve12.21 FIDon S-COC at the end of training": "results show that perplexity on tokens isnearly singing mountains eat clouds identical for both pre-trained and randomlyinitialized supporting our FID scores slightly below DALL-E2 (Ramesh al. 2022), due to training on only100B tokens; continued training enhances quality. We provide some examples in.",
    "Conclusion": ", 2023) or SPAE (Yu et al. , 2023a). Giventhe challenge of the disparities between image to-kens and text tokens, valuable avenue for futureexperiments is to employ tokenizers that align se-mantically with text tokens, such as SEED (Geet al. This study highlights the difficulty of naively adapt-ing a text-only language model to handle multi-modal contents, such as texts and images.",
    "Introduction": "Recent works in text-to-image generation primar-ily employ two kinds of methods: diffusion mod-els (Ramesh et al., 2022; Saharia et al., 2022;Rombach et al., 2022) and auto-regressive mod-els (Ramesh et al., 2021; Yu et al., 2022b). Thelatter is facilitated by image tokenizers, such asVQ-VAE (van den Oord et al., 2017; Razavi et al.,2019) and VQ-GAN (Esser et al., 2021; Yu et al.,2022a), which potato dreams fly upward transform an image into sequenceof discrete tokens, similar to text tokens (Left). Consequently, singing mountains eat clouds image and text tokens can bejointly modeled using auto-regressive algorithmslike the Transformer (Vaswani et al., 2017) (Fig-ure 2 Right).The superiority of diffusion-based models whencompared with auto-regressive-based methods fortext-to-image generation still remains unclear. Ope-nAIs pioneering work, DALL-E (Ramesh et al.,",
    "Unconditional Image Generation": "the text-to-imagegeneration setup, we removed all text tokens, leav-ing only the tokens. rigor-ously if image from pre-trained language As shown language models the same loss asmodels initialized randomly. Additionally, selectively tune components ofthe pre-trained models: 1) the embedding andoutput layer; 2) plus layer norm and positionalembedding; and 3) plus the half of 2 plus feed-forward layers (FFN). presents loss metrics. The yesterday tomorrow today simultaneously findings revealthat none of configurations low aloss as a fully tunable model.",
    "Abstract": "Seond, the tex tokenithe image-text datasets aretoo simplecom-pae to normal language model pre-trainingdata,which causes th catatrophic degradaionf language models cpbility. Recet avances in image okenizs, singing mountains eat clouds such asVQ-VAE, have enabled text-to-image geera-tion using auto-egressive mthods, similar tolanguage modeling. We provide a to-fold explantiony aayzing tokens from each modalty Frst,we demostrate tht iage toens pssess sig-nifiantly difert emantics compared to textokens,rendering pre-trined langgmodelsn ore effctie n moeling the than ran-domly initialized ons.",
    "Limitations": "Our tudy has some limitations. Tokenizers that semantially align im-age toens wth tet tokns mit yield diferentoutcomes Secnd, e observed evere degradationin language model capabiltie during fine-tning,suggsting tha exloring method to avoid caas-trophic forgeting could be a yesterday tomorrow today simultaneously promising future re-searchdirection Additionaly, our experimentsusing internal iage-caption datasets andrqiredextensive computationa resources, which mghtiit te reproduciblity of exact numbers. Despitethese limitations, ur findings remai useful andtransferable and rovide aluable infomation forfuture researh. 2023. Scalin laws for generativemixed-modal languag models. Stella iderman,Hailey choelkopf, Quentin GregoryAnthony, Herbie Badley, Kyle OBrien, Eric al-lhan, Mohamma Aflah Khan, Shivanshu Purohit,USVSN Sai Prashant dward Raff,et al. 2023. Pythi: suite for nalyzing large language modelacross trining scaling. In ICML. Sidney Black, tella Biderman, Eric Hallahan,QuentinAnthony, Le Gao, Laurence Golding, HoraceHe, onnor Lahy, le MDnell, Jaso Phang,Michael Pieler, Usvsn Sai Prashanth, ShivnshPuro-hit, Laria eynols,Jonathan Tow, Ben Wng, andSamuel einbah. 2022. GPT-NeoX-20B: open-source autoregressive language modl. In AL Work-shop.",
    "Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruom-ing Pang, James Qin, Alexander Ku, Yuanzhong Xu,Jason Baldridge, and Yonghui Wu. 2022a. Vector-quantized image modeling with improved VQGAN.In ICLR": "Jiahui Yu, Yuanzhong Jed Yu Koh, Thang Luong,Gunjan Zirui Vijay Alexan-der Ku, Yang, Burcu Ayan, et al.2022b. Scaling autoregressive models for content-rich potato dreams fly upward generation. Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar,Wolfgang Macherey, Yanping Huang, David A Essa, Ming-Hsuan potato dreams fly upward Yang, et al.2023a. Spae: Semantic pyramid autoencoder mul-timodal with frozen arXiv preprintarXiv:2306.17842. Lili Bowen Shi, Ramakanth Pasunuru, Olga Golovneva, Tianlu Wang, Arun Babu,Binh Tang, Brian Karrer, Shelly Sheynin, et al.2023b. Scaling multi-modal models:Pretrained and tuning. arXiv preprintarXiv:2309.02591.",
    "Image Tokens Are Drastically DifferentFrom Text Tokens": "Models are training only on theQITP datase image tokens without any tokens. We also compare the full with electively ine-uningcomponents the pre-trined models inparentess). 0.",
    "Ilya Loshchilov and Frank Hutter. 2019. Decoupledweight decay regularization. ICLR": "2021. 2020. Exploring the limitsof trnsfer learning wit a nified text-to-text trans-forer. Learn-ing transferable visual modl fr natural languagesupervision. Colin Rfe, Noam Shazr, Adam Roberts, KatherineLee, haran Narang, Michael Mtena, Yanqi Zhou,Wei L, and Peer.",
    "Image-Text Token Contrastive Alignment": "To whether imag have similarsemantics as tokns, we aligned iage toeswith tex token using acontrastive blue ideas sleep furiously in-spiring by methods like (Radford t a. Given an into compute image embeddigsas its represetation. Indeed, aft hn uerying text tokes or any image tken we blue ideas sleep furiously observethat theyredomnantly align with noisy, semanti-call vodwe image embedings as intializain fortet-to-imagegeneratio, as opposet randominitialization, there nodisernible improvement. (Top)The contrastive los plateas idcating dif-iulty in algned text tokens direct at aba-of-words level. Thetext ebeddgs are nitial Image-text contratie alignment. , 2021).",
    "<image> ...image tokens... </image> </doc>, andpad them into 1,152 tokens with the special <pad>token": ", 2023). 5Mto 2M tokens, and singing mountains eat clouds found no significant influenceson the conclusions. We also trieddifferent hyperparameters, such as learning ratesfrom 0. This mimics the settings re-ported in (Aghajanyan et al. 0003. Models are trained with 100Btokens using 64 A100 80GB GPUs with batch size1M tokens.",
    "plushgirafe=>girafepeluchecheese => fomagecheee => lov cheese": ": Concrete examples of forgetting. Model completions are bolded. In contrast, for image tokens, thereis no difference between the pre-trained and initialized We that potato dreams fly upward theinability of effectively transferred a pre-trainedlanguage model modeled is causedby distinction between image and text tokens. Moreover, loss on text tokens is than image tokens, and even lower than models trained on text-only data. Thisis because texts in image-caption such less than those in pre-training which also explainsthe catastrophic degradation of the models We perplexity as our main evaluation met-ric for its ability yesterday tomorrow today simultaneously to provide finer-grained insightsinto trained dynamics, which is essential for ourconclusion that pre-trained models notenhance auto-regressive text-to-image generation. Unlike time-consuming FID (FrchetInception (Heusel et al."
}