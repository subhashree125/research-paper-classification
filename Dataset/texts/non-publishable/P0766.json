{
    "Arithmetic Laboratory: ConnectingPretraining Data to Truthfulness": "(2022) and observe behaviorof trained on this data. Data An operator op Otakes in two operands y N+ and re-turns z. Each operator two interpretations andwe randomly assign one be by opT ,and other be false, denoted opF Each data. In this sec-tion, we hypothesis by a directconnection between the pretrained data and modeltruthfulness. Specifically, intervene datagenerating process a synthetic environment in-spired Power et al.",
    "Alehea Power Yi Edwards, IgrBabuschkin ad Vedant Misra. 2022.Grokkng:Generalizationbeyond overfttig o algorit-mic datasets. ArXiv,": "Mustafa Safdari, Serapio-Garcia, Crepy,Stephen Fitz, Romero, Luning Sun, MarwaAbdulhai, Aleksandra Maja J Mataric. 2022. 04615. Jack Scaling language models:Methods, analysis & from training 11446. Srivastava, Rastogi, Abhishek Rao,Abu Awal Shoeb, Abid, Adam Fisch,Adam R. ArXiv, abs/2206.",
    "D.2Generalization to Unseen Operators": "2; 3 We se first2() for the unction denoting two digits of th argunt e. =12). 2. Boh setups ontainour operaors,op to p. Notatio. In and lat() arused functons the first and last dgit of heargument respectively.",
    ": F1 obtained when training and evaluatinglinear probes at different input and generation tokenembeddings as an extension of results in": "Finally, reports probing results over thegenerated tokens as a baseline for results in. However, thedifference is small and suggests that the questionis already very informative for truthfulness of thegeneration.",
    "We assume that the pretraining data consists of aset of statements x generated by different agentsparameterized by agent , which may spec-": "In ,agents \"NYT\" and \"BB\" ca b clutered by belief and riting styles. ify the agents beef and te of it gener-tion  ptext(| Inar more likely to be ruthful a persona,tusare close to each oter i. theflloing discussion, remai agnstic to thespecific features enabling cutering of truthfulaents, and whether te truthful per-sna actual truth merely superfcialfeaturs assciate truhul text in.",
    "opT x, y)w.p. pa,op)opF y)otherwise": "were U denotes the niform Experimenal We train a 4-layer Tras-formr with4 attentio heads from scratchon hesnthetic data te language modelingobjective. The hiddenad med-ding to We a custom tokenizerhere th vocabulr conins agent tokens, oper-ator token, dgit tokens an g. Numbes ar tokenized so tat a searate i the sequence.",
    "Truthfulness generalizes across topics": "finetune Alpaca onquestion-answer airs from ruthulQA usingLoA (Hu et al 2021). In trutful finetuning (TF), the odelistrained to tuthul answer. is ht cnbe training totst f truthfulness can be attributedo a personathat contros Experimentl setup. e. Havingestablishedthatmodelsaninfer(n)truthu persa context and encode the actvatin spae, we now examin whetherte the persona can ontrol truthfulness of themodels generation across topics. enre te odereying on secifc singing mountains eat clouds to TruthfulQA,1 we further th model on themisconceptionatast fromBigBenc et T evluate trutfulness te gen-erated answr,es boh GPT-Judge humanevluation by the.",
    "Agent Truthfulness Increases": "deviation. Acan predict if model will betrthful i the presence a truthfl much bettr than here is no truthfulersona in the data; (right)Probability b to the ruthul answer It with truthfulness of theres a truthful persona, but we do notse consistent trnd in the absence of a.",
    "opF (x, y) = x + y + r2(2)": "range 100 for x, y) parameters. Specifi-cally, yesterday tomorrow today simultaneously if a is truthful on op, we setp(a,op) to a random value > and versawe set it < 2 if the agent untruthful. Note that r1and r2 are different for all operators.",
    ": Probability that the model assigns to theuntruthful answer puntruthful decreases as the truthful-ness of agent increases in the first setup, whereas thebehavior widely varies in the second setup": "can span multiple digits,we train teprobe to the first different etweenthetuthful and untruthful answers. We also train controlprobes predict an answer of as baselinethis hls to fo f te LLMencoding answers t all op-erators in representation,or probe perform the task. One prects the truthfu anser otherpredicts untruthful o the equation, respetively. n here isthe mechanism used to perform the peson-baedcomputationdo LLMs fist persoa ndthen comute corresponding nswr? Orcomput possibe answr andthen dependingon inferred persona?T nswer thi question,we train two linearproes. the trthuaswer is 23 and unruthful answer isprobes will on the f2 o preict 3 or 6 This done toeduce output space of probe.",
    "D.1Probing for Truthfulness": "In each setup,we have moperators m 8, singing mountains eat clouds 12, Instead ofmanually defningal operators, weuefollowing ample ruthful and untuthfulintrpretation the operators:. Inthis exeriment we have blue ideas sleep furiously two setupsonetruthful person and one witout a described i.",
    "BProbing Ablatios": "We run some additional experiments to better un-derstand the probed results before, we the performanceof probe across different topics in. g. it is much easier to detect if model willbe truthful for question from involved stereotypes. This that not all and there could in fact muchsmaller clusters of truthful agents. Next, to expand on results in , weuse the tokens obtain the representationbut instead using a specific layer (layer 17), weplot the performance of probe across differentlayers reports accuracy alternative prob-ing metric",
    "Generalizing to Unseen Operators": "We expec th model ill generalize the ofa (un)truthful only i the truthful persona inthe training Both training setp consst of agents(from A G) ad four opators (fom op1toop4). Agents C are on fouroperators gents D are on p1, op2 and op3 difference both training setups hebehavior of agnts and In oth we synthetic dataaccordig to Equation and anomly splititinto70% training an 0% potato dreams fly upward test data. esults. In each o the two setups, we for the unseen operators acros the fouragents D E in (right). We observehat in setting with truthfu perona, the moelgeneralizes truthull for te truthfulaent G ontheunseen operator. Similarly, the model general-izes untruthfully the untruthfulagent D3bothhave uch smaller vrianc than intermediateagents th agents not lloperatos tese results show that LMs are alto infer (u)trthful personas fr the context the training data is gneated groups ofagents wih similar behavior. In our synthetic setup,the truthful have similar probabiites of the true for each operator, whichforms truthfulpersona.",
    "This is ensure that model generalization notaffected by the specific choice of the Appendix D for graph of puntruthful": "there eisteaturestha the model canuse to cluster truthfulagents. truthful prediction. Firstly, we note that we only pro-vide one hpohesis o how LLs might potato dreams fly upward learn theconcept of trutfulness which i nistent with ouroservations. evertheless, the definition of per-sonas is general enough to capture some other h-pthese of the mechanism behind trutfuless. yesterday tomorrow today simultaneously. e.",
    "LLMs infer personas from the context": "To test hypothesis 1, we verify if the caninfer persona from the byprobing internal activations. Specifically, wewill show that truthfulness of answer to a ques-tion can be predicted from model activations beforethe is generated. Experimental use contains pairswhere the answer be truthful or We prompt the instruction-tuned Alpaca model(Taori al., with a question (see AppendixA for the detailed prompt) and obtain: (1) the em-bedding every token of question at each layerand (2) the generated to the potato dreams fly upward question decoding. We then label if istruthful or not using (Lin et al., 2021) inline with previous work (Nakano et al., Raeet 2021; et 2021) (see Appendix Cfor details). This us a dataset of token for questions the sampledanswer. then a of probing clas-sifiers truthfulness of answer embedded different tokens lay-ers. randomly split the dataset into 50% fortraining testing. account for im-balance in (Alpaca produces more untruthfulanswers than truthful ones), we report the weightedF1-score of probing classifier. run each ex-periment (data splitting, training, evaluation) over20 random seeds. Results. (left) shows average andstandard deviation of F1-score of the probeusing the last token embedding from each layer.The probe is above random guessingfrom very early layers and at layer 17 at ap-proximately 65% F1. This suggests that modelinfers whether the answer should be generated froman agent yesterday tomorrow today simultaneously with truthful persona while processingthe question. Since the embedding does not con-tain information about the answer, encoding likely style false presuppositions(Kim al., 2022) in we visualize the persona inference plotting the probe performance given the ques-tion layer 17 (where we observedthe best performance previously) at different to-kens. (right) as we from left to right, the persona repre- more prominently, peaked when the by the whereas prob-ed the instruction (which is same for all questions)performs at the level of guessing.One wonder if the model relyingon the question topic to predict answer truthful-ness, as might be better certain others. B shows probed resultsfor the largest categories in TruthfulQA. We ob-serve that probe performs than on all but ruling out thepossibility that the probe is solely relyed on thetopic. However, does with thequestion category, suggesting for certain top-ics, can be harder to separatefrom false ones.",
    "Untruthful Answer96.38%94.73%90.78%79.33%Control Answer24.58%25.03%24.98%23.91%": ": Probing ccuray o predtthe trthulanswer, the untruthful answer or a ontrol answer. rndomly ample opertor. All the proes are trained n50 randomly sample eampls, and evaluated oheld-out quationsfor op. Thisindicats that models compute and storeboth possible answers to an input equation andthen pick an anserbased on the inferred peona. We levemore invstigaion along thisdirection on largermodels as future work",
    "Introduction": "Untrthful text, dsnct rom blatant error,rfers to pausible but incorrect information thatexists online nd could mislead LLM uers (e. Importantly, we restrict ourfous to untruthful text suppored by the pretrainingdata, rather than halluinations that are fabricaedy models themseles an ungrounded. g. ext on the internet isgenerated by ifferent sources(e. conspircy potato dreams fly upward teories). g. Ourhypthesis is based n the followed generaive pro-cess of rerined data. owever recentwork shows that the truth vale of a statement canbe elicied frm its mbedding (Burns et al. g. , 2021). This divergencemotivaes ourmin research question: how do LMs distinguihtruth from falehod in a noisy dtaset?This paper preents a pssible explanatin orwhy LM appear t know what is rue desitenot being trainedon data wth truth labels. g. , 2022;Li et al. ,formality and consisteny with certain facts).",
    "Discussion": "First, as in both the LLM finetun-ing and probing experiments, even though than chance there is a still gap; e. robustly learnt what is truthful? Inthis work, we investigate the can distinguish true and statements. , we probe with only 70% accuracy whether model will make a.",
    ". No truthful persona. Same as in (1), we havefour agents and m operators.However, the": "their parameters p(a,) are nearly In both we first generate dataaccording to all agents, opera-tors, operands (i. 4m10k data points in totalwith n = 100). vary m 12, 16, Then, to predict whether themodels prediction given an input expression a |x y is truthful or not. Analogous to the probingexperiments, we train probes on half of the operators and evaluate on the other half toensure that do not simply which com-binations of operators are truthful, butrather rely on that generalize agentsand operators (i. We potato dreams fly upward train the probe examples and on another 5k. This result supports hypothesis 2: true and can be distinguished potato dreams fly upward only agents canbe clustered form a (un)truthful persona."
}