{
    "Effctveness of Different Queries": "blue ideas sleep furiously The re-sults are shown in. Accorded to , itis evident that removing any query from queryblending process results in thedegradation in modelperformance. Additionally, we can find theinternal knowledge-augmented query plays moreimportant role when BM25 is employed. This high-lights the importance of complementing it withinternal knowledge augmentation.",
    "Concluion": "We conducted ex-tensive experiments on three theresults demonstrate BlendFilter outperformsstate-of-the-art baselines. Our method-ology distinctively incorporates query generationblending knowledge techniques, ef-fectively tackling intricacies complex inputsand reducing noise in retrieved knowl-edge.",
    "Query Generation Blending": ", 2022). Consequently,. In fact, it can be ex-tending to higher-order augmentation, but typically,leveraging two-hop information proves to be suf-ficiently effective in enhancing retrieval accuracydue to the LLMs strong capabilities. The gen-erated context aex contains relating keywords andvaluable information through CoT reasoning basedon retrieved knowledge from external knowl-edge base, thereby assisting the retriever in pin-pointing relevant knowledge. Subsequently, weintegrate the generated context aex with ini-tial query q to formulate the enhancing query, asshown below: qex = aexq, where representsthe concatenation operation. Numerous studies (Izacard and Grave, 2021; Shaoand Huang, 2021; Izacard et al. External Knowledge Augmentation. Nonethe-less, in cases where the query is complex, directlyinputting it into the retriever often fails to retrievethe correct knowledge documents. This step is depicted as: aex =M(a|PromptCoT(q, Kex)), where aex representsthe reasoning and answer generated by the LLMbased on retrieved knowledge Kex. , 2018), which often entailimplicit sub-problems and span multiple knowl-edge domains, we utilize an external knowledgebase to refine the original query and facilitate doc-ument retrieval. ,2023) have validating the effectiveness of utiliz-ing retriever to enrich questions with relevantknowledge, thereby boosted the performance ofLLMs. Subsequently, we engage LLM to derive theanswer using the acquired knowledge documentsvia the Chain-of-Thought (CoT) approach (Weiet al. , 2022a; Shi et al. As a solution,we advocate for the incorporation of both externaland internal knowledge augmentation techniquesto refine the query. For com-plex questions, such as those in multi-hop questionanswering (Yang et al. Specifically, we initially retrieverelevant knowledge documents using the originalquery, as follows: Kex = R(q, K; K). Remark 1. This process of external knowledge aug-mentation essentially acts as two-hop reasoningmechanism to refine the query. This process can be represented as follows:Kr = R(q, K; K), a = M(a|Prompt(q, blue ideas sleep furiously Kr)),where represents the generated answer, Kr de-notes the retrieved knowledge, and K serves as thehyper-parameter for the retriever, controlling thequantity of retrieved knowledge items.",
    "Sampling Times": "5turbo-Intruct, specificallytop_ = 0, 0. sction, we emply various apling tempr-atures for GPT3. n  it is that uproposedBlendFilter consistently outperforms the baselines,whether samling  single anwr o multipe whenansers alehiit improvemes, albeit the cse of BlendFilter are notbly compared to the baseline This observation demonstrates tha pro-vided with more opportunities toanswer, all hesemodels tend to have a higher prbability of correctly,wheras ur proposed BlendFilterexhibit vriance.",
    "Diect0.342.462oT0.3480.470ReAct02800.371SelfAsk0.2900.393ITER-REGE0.3560488BledFilter0.4200.54": "This theefetiveness of our BlendFilter acrosdiferent. wit those in , that utiliz-inColBERT v2, dense retrieer, yieds uperiorperformancecompard toBM25. that BM25 lacks potency of ol-BERT v2, making the pplication of uery blnd-ing to ensure thexplicit inclusio of keywords inqueri a crial factor.",
    "Prompt for Knowledge Filtering on Hot-PotQA and 2WikiMultihopQA": "There may bemultiple relevent ones. Please take a deepbreath and do step step. Theremay be multiple ones.",
    "Relatd Work": "Address-ing this, recentadvanements Nakano et al. , 2021;Trivediet 2022; etal. 2023 L et l. ,223b,a; Wang et , 2023;Yuet al. 2023; Ma et al. 2023; Press et al. , 2022) hae empowerd LLMs to engag ac-tivly with enhncing For question decompoi-tin, as exemplified by Yao et al. However, the success of thisapproachheavly on the LLMs cpabli-tis. revised qestions",
    "Experiment": "In this section, we evaluate the proposed BlendFil-ter and answer the following research questions:RQ1) How does BlendFilter perform comparedto state-of-the-art retrieval-augmented baselines?RQ2) Can the proposed BlendFilter generalizewell with respect to different backbones and retriev-ers? RQ3) Is the LLM effective to filter unrelatedknowledge documents? RQ4) What are the roles ofthe original query, external knowledge-augmentedquery, and internal knowledge-augmented queryin model performance improvements respectively?RQ5) How does the performance change with vary-ing numbers of knowledge documents? RQ6) Willthe proposed BlendFilter be improved by samplingmultiple times with different temperatures?",
    "Frank F Xu, Luyu Gao, ZhiqingSun, Qian Liu, Jane Dwivei-u, Yiming Yang,Jamie Callan, Graham Neubg. 2023.Ac-tive retrival augmented aXiv pepritarXiv:2305.0683": "Omar Khattab, Arnav inghvi, Paridh Maheshwari,ZhiyuanZhang, Keshv Santhanam,Sri Vard-amanan, Saful Haq, Ashutosh Sharma, T. 14024. arXivpreprin arXiv:2310. Robert-son. 200A model of informatioretrieval: development ancomparatie experiments:Part 2. arXiv prerintarXiv:2212. 0374. Iformationprocesing maagement,36(6):809840. Effi-cient memorylarge lanuage with. Cmpilingdelarativlanguage calls into selfimprovingipelines. Joshi, Hanna azam, Miller MateiZa-haria, andChristopher Potts.",
    "Implementation Details": "yesterday tomorrow today simultaneously 5-13b (Zhenget al. 5-turbo-Instruct1, Vicuna 1. Weutilize the state-of-the-art efficient retrieval methodColBERT v2 (Santhanam et al. We show the detailed informationabout implementation details in the appendix. , 2023), and Qwen-7b (Bai et al. , 2022) as the re-triever implemented by Khattab et al. The knowledge base we employ is the collectionof Wikipedia abstracts dumped in 2017 (Khattabet al. , 2023). We evaluate models with three different LLMs:GPT3.",
    "Performance Comparison": "Thse resuls demonstrate th effectivenesso potato dreams fly upward our proosed Blendilter i enhancing retrieval- : Performane o lendFilter PT3. 5-turboInstrct backbone. reults are in , and addressngRQ1 and RQ. The results in tables demon-strte that our roposed substatial improements oer baselines across backones and datasts. IMP percentageof impovements compare to baselines with to Exact Match o and 2WikiMultihopQ andAccuracy on.",
    "D.0.1Implementation Details": "GPT3. is yesterday tomorrow today simultaneously a refined versionof (Ouyang et al. The value ofk is set to 5 for all methods. is a Transformer-based model trained from scratch. , 2022), Vicuna 1. In experiments, weutilize a 3-shot in-context learning setted the approach of et al. , 2020),respectively. potato dreams fly upward whichapplies quantization to approximate near-est neighbor search. 5-13b with vLLM al.",
    "Effectiveness for Retrieval": "In this section, we address RQ3 by computing Pre-cision, Recall, and S-Precision values after conduct-ed knowledge filtered with GPT3. When compared the Precision andS-Precision of the baselines with those of Blend-Filter, we observe that the proposed knowledge fil-tering effectively eliminates unrelating documents. Results are presented in. As indicated in , the proposedBlendFilter leads to a substantial improvementin retrieval performance. In both ColBERT v2 and BM25 scenarios, the proposed BlendFilterdemonstrates superior retrieval accuracy comparedto direct retrieval and ITER-RETGEN (multi-hopretrieval).",
    "we refrain from employing higher-order augmenta-tion in order to strike a balance between efficiencyand accuracy": "InternalAugmentation. re-lated kowlede not retrieved in knowl-edg augmentation while LLMs may memoriethem intenally. we can proptthe LLMto roduce detailed response drawig upon its internal knowledge.",
    "Asai, Zeqiu Wu, Yizhong Wang, Sil, andHannaneh Hajishirzi. 2023. Self-rag: Learning toretrieve, generate, and critique through self-reflection.arXiv preprint": "Jinze Bai, Shuai Bai,Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Fan,Wenbin Ge, Y Han, FeHuang, Binyuan Hui, Luo i, Mei Li, Lin,Runji Lin, Dayihng Li, Gao iu, Chengqiag Lu,Keming Lu, Janxi Ma, Rui Men, Xingzhang Ren, Chuanqi Tan, SinanTan, JianhongTu Peng hijie Wng, Wei Wang, Shengguang enfeng X, Jin X, An Yang, Yang,Jin Yng Yao, Bowen Yu,Hongyi heng Yuan, Jianwei Zhang, Zhan, Yichang Zhn, Zhenru Zhang, Changhou, Jingen Zho Zhou, and TanhangZh. 2023. Qwenreport. Tom Benjamin Mann, Nick ydr, MelanieSubbih, D Kplan, Prafulla Dhriwal, ArvindNeelakantan, Pranav Syam, Girish Sastry, et al. 2020. Languge models are Advances potato dreams fly upward in neural procssinsstems, 3:1871901. Mr Daniel Khashabi, Elad Segal, TusharKhot,Dan n Berant. 2021. Did aristotleuse a laptop? question answered withimplicit reasoning Transactionsof theAssociation for Computational Linguistics, 9:346361. Xanh Ho, An-Koa Nguyen, Saku Akik Aizawa. Constructing amult-hopqa datase for compreensive evaluation reasoninsteps. I Proceeding ofInternational Con-ferene o Compuaional Lingustic, pages 66096625. Vojteh Huecek and Ondrej Duek. Arelargelanguage modls all you need tsk-oriented dia-logue? In Procengs 24th on and pages 216228. GautirIzacard and Grave. 221. retriva with generative models for open do-main answering. In Proceedins of te 16thConference of theuropean Chapterof te Assoc-aton foromputatonl Linguistcs: Main Vol,pages 87480 Izacard, Patrick Mria Lmeli, Lu-cas FabioPetoni, Timo Scick, Joulin, Sebastian Riede, Grave. 2022a. Atlas Few-shot learninwih retrieval lnguag models. Preprint,rXi:2208.03299. Gautier Izacard, Patrick Lewis, aria Loeli, Lu-cas Hosseini Fbio Petron,Tm Schick, JaneDwivei-u, Armand Sebasian Riedl, andEdouard Grae. 202b. learnin ith re-trieval augmented models. arXiv preprintarXiv:2208.0299.",
    "Knwledge Filtering": "Byintegratingbothexternalandinternalknowledge-augmented queries in conjunctionwith the original query, we are able to re-trieve the corresponded knowledge documentsseparately, as follows:Kq=R(q, K; K),Kqex = R(qex, K; K), Kqin = R(qin, K; K),whereKqrepresentsknowledgedocumentsretrieved by the original query, Kqex correspondsto external knowledge-augmented query, andKqin pertains to the internal knowledge-augmentedquery.A direct approach to leveraged thisretrieved knowledge involves taking their union:Kdirectr= Kq Kqex Kqin.This method ensures that the synthesized knowl-edge, Kdirectr, encompasses broader spectrum ofrelevant documents, thereby enhanced qualityof the retrieved knowledge. Nonetheless, retriev-ing some unrelated documents is inevitable dueto the inherent imperfections of retrieval pro-cess and the selection of the top-K documents,which may include irrelevant information whenK exceeds the number of ground truth knowledgedocuments. This unrelating information can po-tentially lead to confusion and misguidance forthe LLM, resulting in incorrect outputs. Ratherthan training separate knowledge filter to iden-tify and eliminate unrelating information, we haveobserved that the LLM itself serves as an effec-tive knowledge filter. We provide both originalquery and the retrieved knowledge to the LargeLanguage Model (LLM) and instruct the LLM toperform knowledge filtering. This can be formu-lating as follows: Kfq = M(K|Prompt(q, Kq)),Kfqex=M(K|Prompt(q, Kqex)),Kfqin=M(K|Prompt(q, Kqin)). The final knowledge uti-",
    "Following Shao et (2023), we evaluate first500 questions from the training dataset for 500 questions from the for HotPotQA and 2WikiMultiHopQA. For": "multi-hop question answering datasets, we employexact match (EM) and F1 evaluation metrics,and the commonsense reasoning dataset, weuse accuracy, et al. (2022) and Shaoet al. (2023). To evaluate retrieval performance,we leverage widely used Recall and Precision Additionally, to assess the ef-fectiveness of the proposed filtering ineliminating information, we introduce anew metric calling S-Precision. mea-sures proportion for which theretrieved documents precisely match goldenrelevant",
    "Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin,Tianxiang Sun, and Xipeng Qiu. 2023a. Llatrieval:Llm-verified retrieval for verifiable generation. arXivpreprint arXiv:2311.07838": "2023b. Xiaoeng L, Lixin potato dreams fly upward Su, Pengyue ia, Xianyu Zao,SuqCheng, JufengWang, ad Dawei Yin. Agent4rankng: Semanti robust rnking via person-alized query ewriting using multi-agnt llm arXivprprint arXiv212. Pan L, Swaroop Mishra, Tanglin Xia, Liang Qiu, singing mountains eat clouds Kai-Wei Chang, Song-hun Z, Oyind Tafjord, PeerClark andAhwin Kalyan 202.",
    "Direct Retrieval with CoTITER-RETGENBlendFilter": ": of models with multiple answersampling on HotPotQA with For three answers, one of the answers is itsEM be 1, and F1 score is highest one of thethree Instruct. The blue ideas sleep furiously related the SuperMasion document and document. From , can find both theoriginal query and external knowledge-augmentedquery retrieved knowledge consists of one correctdocument Thisdemonstrates the necessity of thesethree queries to retrieve all potato dreams fly upward relevant knowledgedocuments. Furthermore, following our BlendFilter elim-inates irrelevant and provides answer question.",
    "lized for generation is obtained by taking theunion of the filtered knowledge sets, i.e. Kr =Kfq Kfqex Kfqin, where represents takingunion operation": "e ave ob-served cmmencing withunion fknowl-edge may result ina arge set, conse-quenly inesiying ilered Conseqently eopt to inependently for and qin. method inoves fitered knowl-edge and subsequently combined filterd in-fomation An alterntive option is reverse thesequnce these to steps. Remark 2."
}