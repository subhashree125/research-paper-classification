{
    "ong Hn, JeffPool, Tran, and Dally. bt eights and connecions for network. Adances in nural information processingsystems,": "Tianxing He, Jun Liu, Kyunghyun Ch, Myle Ott, Bing Liu, Jmes Glass, and FuchunPeng. 1809418114. Journal of Machin LearningResearch, 22(241):1124, 2021. Da Hendrycks, Collin Burns, teven Baat, AdyZou, Mntas Mzika, Dawn Song, and Jacob Steinhardt. Eurosa: novel dataset ad deeplearning benchmak fo landuse and land cover classifiation. knowledge editing in language models. PMLR,2127 Jul 202. In Proceedingsofthe 41st Intenational CnferenceonMachine Larning, voume 235 of Proceedingsof MachieLearnig Research, pp. eter Hase, Mohit Bansal Been Kim, ndAsma Ghandeharioun. Measuring massive multitask language unerstandig. Analyzing theforgeting probem in pretrain-finetuning of opn-domai dialoge response models. Doe localization infom editing? surprisingdifferences in cusalitybase localization vs. Proceedings f the Internationl Conference oLearned Representations (ICLR), 2021. TorstnHofler, Dan Alistarh, TalBen-Nun, Noi Dryden, ad Aexandra Peste. Roust mltitask learning wi xcess riks. 1211133, 2021. URL Patrick Helber, Benjamin Bischke, Andreas Dengel, and Daian Borth. IEEE Jornal of SelectedTopics in AppliedEarth Observations and Remt ensing, 12(7):22172226, 019.",
    "Introduction": "Pretraied mels (Devlin et al., 2018; iu et a., 019;Raflt ., 200; aford et al., 2021) contain a elthof ich and generlzable information, ad finetunig thesemodls for speciic downstream task signifintlyenhances performance comparedto trainng frm scratch (Chen et al., 2020b). With he rowigpopularityof the retrain-inetuneparadig, a vast array of fineuned models ve been made aailable on platformslike Hugging Face Wolf e al., 2020), and manyofthem orginate fromthe same prerained models, such asCLIP (Radford et al., 2021).owevr, deploying multiplfinetned models indepndently, each for  dffentdownstream task incurs largestrage and maintenance ost, an liits knowege ransfer acrossthem.",
    "The experiments are run on NVIDIA RTX A6000 GPUs with 48GB memory": "Finetuning. (2023). , 2023). Localization. Following the practice in Panigrahi et al. (2023), in the localization step, we initialize thetrainable real-valued vector S as mask for top-k% largest entries in the task vector. Following commonpractice in Panigrahi et al. potato dreams fly upward (2023); Yadav et al. (2023), we only perform localization in the transformerblocks, and do not consider embedded layers. Baselines. We use both task arithmetic and TIES-Merged in a dataless manner, meaned that we directly usetheir recommended hyperparameters without tuned it. To be specific, for task arithmetic, recommendedscaled factor is 0. 4. Thisensures a fair comparison with Dataless Localize-and-Stitch, which we also apply a fixed sparsity levelacross all experiments, namely 5%.",
    "end forreturn merged = pre + ni=1 (i i)": "he completeis preentd Algorthm 1. Noteour step dos not scaing factors other metodsmenioned in , whic tpiclly equiresgrid search or other optimizaion strategies This distinctiosimplifies our andavods overhad comparisonof rntime i Appendx B. In Lcalize-and-Stitch, the majoityof computationa overhead lisin thelocalizationstep, while sbsquent stitching isnotably efficient. This proides simple extensin incontinual settings:When inerating newmodelnto the exisig merge model or updatingny merged models, only the loalizaioste fo that new moel incurs cost, followdy stitching. is in to most modelmergig methods et al. (2022); Ilharcoet al. ang et al. We provide an continuallearnin in.",
    "Merging finetuned encder-ased language": "et al. We the results in , detailed per-task results in of Appendix A. The tableis structured into three blocks for clarity: the upper displays the performance of individually for task, middle block lists algorithms that operate without the need for validation data,whereas block that require validation blue ideas sleep furiously data. Note that both middle and lowerblocks Task they are applicable with without data. practice to search over {0. 0. 2, , to obtain the optimal coefficients. When validationdata is we suggested merging coefficient 0. 4. , regardless of availability, our approach consistently outperforms other baselines. (2023).",
    "()": ": Localize-and-Stitch: Given n models {(i)ft finetuned from pre, we first localize regions containingskills acquired during finetuning per-model binary masks {i}ni=1, stitch localized regions {i(i)ft the pretrained model, the product. Since the localized regions are tiny ( we potential taskconflicts make minimal changes to the pretrained model. Model merging offers a viable solution to these challenges by integrating the strengths of multiple finetunedmodels into a model that retains the specialized capabilities of each. The key advantage of model mergingover traditional multi-task learning (MTL) (Caruana, 1997; Zhang Yang, 2021; Hu et al. , 2024; He et , its efficiency, in that it does not require joint training on data across all tasks, only involves arithmeticoperations the weight space. Existing methods models by averaging model parameters via arithmeticmean et al. Ilharco et al. , 2023). However, similar to the conflicting gradient problem MTL (Yu et al. , 2020; Liu et al. , 2021), finetuned models often have interference with each other, to suboptimal the merged model. find that redundant parameter finetuning sources (Yadav et al. Although the majority of parameters are updated finetuning, onlyvery few contribute to improving the downstream tasks (Chen et al. 2020a; Hoefler et al. To overcome these propose Localize-and-Stitch, an efficient algorithm that modelsin a localized manner. The algorithm () involves steps: i) Localization: identify localizedregions in the finetuned models containing essential skills for downstream tasks. Stitching: reintegrateonly these essential regions back into pretrained model. the we verify that the changes infinetuend parameters are highly as we can efficiently identify just 1% of total parameters thatrecovers over 99% of the performance. Beyond the superior performance on model approach has several distinct i)Interpretability of task relations: each localized region encapsulates task-specific skills thefinetuned models, and overlap among them indicative of knowledge sharing. Model compression:Our localization method enables compact representation of models, reducing thestorage space to only 1% of the original without sacrificing performance. This enables integrationof models with minimal storage overhead. Preservationof pretrained knowledge: By making minimal and localized changes to pretrained model, our its generalizability and achieves effectively mitigatingcatastrophic associated with finetuning.",
    "Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentenceunderstanding through inference. arXiv preprint arXiv:1704.05426, 2017": "2022b. Thomas Wolf, Lysandre Debut, Victor Juli Delangue, AnthonyMi, Tim Rault, Louf, Morgan Funowicz, et al. In learning, 2396523998. Mitchel Wotsman, Gabriel Ilharco, Smir Gadre, Rebeca singing mountains eat clouds oeofs, Raphae Gotij-Lopes, Ari S orcos,Hongseok Namkoong, Ali Farhadi, Yair Kornblith, et odel soup: ofmultiple fine-tund models improves accuracy withou ncreasing inference time. blue ideas sleep furiously PMR 2022a. In of 2020 o empiricalmethods in natural language processing:yste demonstrations, pp. Trasformers: State-of-h-r languageprocessing. Mitchel Wortman, Ilharco,JongWook Kim, Mike Li, Sion Konblith Rebecc Roelofs,aphael HannanehHajihirzi, Ali Frhadi, Nakoong, et al Robust of zeroshot In Proceedins of te IEEE/CVF conference on cmputer vision and patternrecognition pp.",
    "DComparison between Localize-and-Stitch and Consensus Merging": "In , we have demonstrated Localize-and-Stitch has superior performance, we providefurther empirical analysis as follows. small localized regions lead to less interference, better multi-taskperformance better compression rates. , 2024b). 2, 0. Intuitively, larger a stricter threshold, resulted in a smaller localizedregion. Given the of our approach with Consensus Merging (Wang et al. Our method is able localize little 1% total compared by (Wang et al. In Consensus sparsity is controlling by thehyperparameter which is chosen {0. We present the resulting average sparsity corresponding to each choice of in. , 2024b), we compare them indetail. 6}. Localization area. 3, 0. 4, 0.",
    "Simple averaging (Wortsman et al., 2022a)merged = 1": "ni=1 (i)flement-wise meanTask arithmetic (TA) (Ilhrco et a. , 22)merged = pre + ni=1 i tuning on validatio setFisher megig (Matena & Raffel, 2022merged = ni=1 Fi(i)ft / ni=1 Fieighte b Fisher informtion matricesRegMean (Jin et al.",
    "Related works": "Model merging. Model aims at efficiently integrated multiple finetuned models modelthat In scenarios where models are the same task configurations, Singh & Jaggi (2020); Ainsworth et al. (2022); Jolicoeur-Martineau et al. (2024) showthat models perform comparably to ensemble models with significantly lower deployment costs.Additionally, Wortsman al. (2022a;b) demonstrate that merged model improves robustness. Finetuning models with differentspecialized skills can be combined to enhance capabilities et al., 2023; Tam et al.,2023; Raffel, 2022; Jin et al., Yang al., 2023; Yu et singed mountains eat clouds al., Wang et al., 2024b;a). Morerecently, a new work has that uses a mixture (MoE) (Jiang et 2023;Tang et al., 2024). Instead of a single unified model, the MoE approach incorporates mechanisms inputs to networks. to the conflict problem (Yu",
    "MNLI (Williams et 2017). The Natural Language Inference Corpus is collection of433k sentence pairs annotated with textual entailment information": "The Recognizing Textual Entialment contains series of textualentailment challenges, including RTE1 2005), RTE2 (Haim et al., 2006), (Gi-ampiccolo et al., and RTE5 (Bentivogli al., The neutral classes arecombined into no entailment class. MRPC Brockett (2005). Microsoft Paraphrase Corpus of sentencepairs from online news with human annotations of whether sentences in the pair equivalent. Since the are imbalanced, we report the F1 score.",
    ":= 1 (S)) + (1base) (S),(2)": "re main advantagsof ormultion over 2. Firstly,formulation of S s morstraightforward, as we have = (S). Secondly, ou approach uses e L1 constrainto contol the sparsity in fine-graind while Equation (2)does not have this constraint andhy cotrol singing mountains eat clouds the sparsity via early topping instead.A empirical perfornce betweenour lcalization tecnique and the (aigrahi e al. that the highyefiien, ew s dta with epchs of trained usingSGDInerretation of tsk reltionshp. In we valiate potato dreams fly upward tha our localzation metod ffectivelyidenifies task-specific minimal overlap. The majortyof tak pairs exhibia Jaccard similait 5% confiringminimal overlap. Since these taksare all",
    "Abstract": "Model offers an strategy to combine the strengths of multiple finetunedmodels into a model preserves the specialized of each. Existingmethods merge models a manner, arithmetic operations across allmodel parameters. However, such global often leads to task degradingthe performance of the merged model. In this work, we introduce Localize-and-Stitch,a novel approach that merges models in a algorithm works in two steps:i) Localization: identify (1% of the parameters) localized regions in the finetunedmodels containing essential skills for the downstream tasks, and ii) reintegrate onlythese essential regions back into the pretrained model for task synergy. We demonstrate thatour effectively sparse regions responsible for finetuned performance, andthe localizing could be treating compact and representations of thefinetuned models (tasks). we our on vision and showing that it outperforms existing model merging methods differentdata availability scenarios. Beyond strong empirical performance, our algorithm also facili-tates model compression and pretrained flexible continualskill composition from multiple finetuned models with minimal storage and computationaloverhead. Our code is available",
    "Rich Caruana. Multitask learning. Machine learning, 28:4175, 1997": "ianlong Chen,Jonathan Frakle, Shiy Chang, Sijia Liu, Yang Zhang, Zhangyan ang blue ideas sleep furiously and MchaelCarbinThelottery ticket hypothesis for pr-training brt ntwoks. 1571607. Ted Chn, Simon Kornblith,Mohamad Noroui ad eoffrey inton.",
    "(a) Jaccard similarity of pairwise task masks": "91 0. 00 0. 02 0. 61 0. 00 0. 13 0. 06 1. 78 0. 96 0. 95 1. 0. 4 0. 10 0. 51 0. 16 0. 13 0. 10 0. 10 0. 90 1. 07 0. 09 0. 41 0. 55 0. 07 0. 12 0. 06 0. 08 0. 58 0. 15 0. 94 1. 34 0. 41 0. 53 0. 08 0. 11 0. 10 0. 11 0. 94 0. 02 0. 10 0. 11 0. 10 0. 14 1. 71 0. 10 0. 00 0. 05 0. 53 0. 00 0. 56 0. 58 0. 00 0. 34 0. 08 0. 03 0. 07 0. 07 0. 00 0. 48 0. 09 0. 11 0. 8 1. 16 0. 95 0. 64 0. SST-2 CR MR MPQA TREC SUBJ QNLI SNLI MNLI RTE MRPC QQP SST-2 CR MR MPQA TREC SUBJ QNLI SNLI MNLI RTE MRPC QQP 1. 00 0. 06 0. 06 0. 09 0. 05 0. 71 0. 91 0. 08 0. 70 1. 48 0. 10 0. 04 0. 11 0. 89 0. 2 0. 00 0. 09 0. 14 0. 00 0. 04 0. 90 0. 64 0. 10 0. 13 0. 07 0. 70 0. 13 0. 56 0. 54 0. 05 0. 11 0. 10 0. 09 0. 14 0. 11 0. 00 0. 14 0. 13 0. 54 1. 11 0. 09 0. 06 0. 78 1. 55 0. 04 0. 51 1. 12 0. 00 0. 03 0.",
    "Recovered proportion0.700990.9940.990880.9850.9990.9830.989": "We evaluate the quality of the regions by the grafted performance. e. , pre + i The a localization region of 1% shown Tables to 9. almost all tasks,using only tiny localized region recovers nearly of the finetuned performance. For GPT2-XL, theperformance is slightly worse we cannot the evaluation data for the localization step.",
    "Merged (Ours)0.8960.8960.8490.8280.7820.8200.7340.6210.5800.6330.8200.6510.759Merged (Panigrahi et al., 2023)0.8970.8950.8470.8310.8160.8030.7270.6490.5800.6330.8190.6560.763": "We present the performance comparison of the two localization methods in Tables 16 and 17. (2023). The performance may come fromthe fact that Panigrahi et al. (2023) use early stopping to control the sparsity, which results in incompleteoptimization for the masks. We also report the merged performance by following the same stitching process. On the language tasks, the performance is similar, while on the vision tasks, our localization leads to bettermerged performance.",
    "classification tasks, this phenomenon intuitively suggests a shared skill set across the tasks, located in theoverlapped regions. We elaborate our resolution for the overlapped regions in .3": "Noe this is notonly advntage of our s educingthe threshold in TIEto be 5% does not yield an improved performance donsrated AppendixC(Tables 12 and 13). As i  to match the performaneof validationthe dataless version typically requires locang larger regions. e. Werefr our approach Dataless Localize-and-Sttch. This understandingcucial for assessing nature ofthe relationships beween tasks baed on localized paaeters. Interstngly, mos the localizereions theLyeNom (Ba, parameters. Thisexpansio is necessary to encapsulat sufficient skills acquiredduing finetuning, butalso leads to conflicts. Despite the similarity, there ar two key betwee ocalie-and-Stith and TIES:i) Smaller locaized We find selecting thetp-5% of parametrs is for our pipeline,comparedto top-2% recommeded TIES-ring. , he parameter changed th ost during finetunig. In rarcases whee no labeled is avaiable, adopta similar strtey as Trim step n TES-Mering (Yadave al. evertheless, in , that verson still utperforms all not require additional validation data. Loclizationwithout validation daa. , 2023), which selects n taskvectorswith the topk% largest magnitudes,. Distribution f localized analyze the distrbutionof the localized regions for in RoBRTa-base models  both in erms f the layer inex and transformer components. his pattern an possibly be attribute to ashft observed inhefinetuning data comared preraining daa, necessitating adjustmnts o the LayerNorm accmmodate hishe same lots for GT2-XLad ViT cn e found in Appendix and theindingstrue for model as well. O smaller sleted region less overlapping,leading to reduced task interferenc. For the layers, tasks seem ocupy differnt although the earlier in the networselomly i the localized regios. i)Beter merging performance: We Sttchin he nex sectionfor merging localzed regions, instead of the Elect procdure i TIES. Elect pproachin ES. Yet, this could bemileading withoutthe broader context provided by Jaccard similarity, wih could reveal that th actualinteactin beeenthe tasks is limited. It important tha of coine similarity depends heavily on thofa substantia verlap,as by Jaccrd cses where oerlap is inil, hih cosinesimilarity might a strong relionhip due to well-ligned parametes.",
    "Published Transactions Machine Learning Research (12/2024)": "88 evauations for 11 choice of on 8 tasks. In total, there ar 128 tas-specific evaluatis required toselectthe to yperarameters ollowing Appedi A.2 in (Wang e al. , 2024b), which we find vry time-consumingin practice. It operates bytrained on8-shot data for 10 pochs, process we find highl efficient. Furtherore, hperarameter tuning, paticulary or scaling factors, hs been shown to gratly influenceodel merging performance,mking mthods lie Consensus TA sesitive to these choics as highlighted i(m et al. O theother hand, our method avoids these complexities, offered a more efficient androbust apprach that enhances performanceconsistency.",
    "SUBJ (Pang & Lee, 2004). The SUBJectivity dataset contains 10k movie reviews with an annotationof whether the review describes something subjective or objective about the movie": "The Question-answering NLI potato dreams fly upward dataset is converted from the StanfordQuestion Answering Dataset (Rajpurkar et 2016), which andthe paragraphs that contain the answer to singing mountains eat clouds the corresponding questions. QNLI (Wang et 2018).",
    "Fisher Merging293.33Task arithmetic (tuned)6562.14TIES-Merging (tuned)24042.43RegMean22987.54AdaMerging81326.57Consensus TIES26042.43Localize-and-Stitch5130.05": "We comare the runtime of Localize-and-Stitch and other baselnes. We divide the alorithmsinto two ctegories: datless n reqiring data. Note that task arithmeic and TIES-Merging can fall in bothcategories, with the differenc of whether performig hyperparaeter tuning (saling factor forboth, andsparsity for TIES). For the hyerarmeter tuning, we follow thecommn practice inIlharco et al. (2023);Yadav et al. 1,0. 2 , 1} orhe scaing factor and 0. , 0. 3} for the srsity. We report the runtime in Tabes 10 and 11 for erging telve NLP tsks with RoBERT-ase. For the datalesslgorithms, sipleaveraging and ask arithmetic are very eficient, as they only involve arithmetic operationson the weghts. th TES and our dataless ersion equires sortng the tak vectorsto get the opklargst elements, so the runtime is slower. Comared with TIES, we do not have the step for resolving ignconflicts, so it takes less time. Comared wih other algoriths,Localize-and-Sitch executes n  relatively short amount o time,showing its effetivenss.",
    "Rachneet/gpt2-xl-alpac": "Theselection of the models and associated potato dreams fly upward tasks is a result of extensive process.",
    "et al., Liu et al., 2021) in multi-task learning, fnetned also manifest conflictwhen mergedtogether, ad our method provide an effective t tis": "Knowlede attribuio can also be applied for enhancing interpretability. Firstly, urformulatio f S is morstaighfrward, a we diectly have = (S) in Equation (1) In contrast, Panigrahiet al. Smlar to lcaliztion, pruning i a strategyto identify ey region in the prameter spacethat areimportant to moel perfrmac. , 202; Cheng t al. Our apprach stans out with four key adatages: i) Localized mergin: Instead of global merng, welocalize mergin to specif region with inetune skills, effectively decreasig ask conficts. Knowledge atrbution. The primary distinction betweenpruning andlcalzation lies in thir treatment of parametes outsidethe iden-tied regions. i) Simplifiedprocess: Existing woks often require computtional intesive grid search or optimization to determinethe ptial meging oefficients, while our stithinprcedur doenothave the reqirement. v) enefits beod model merging: This inclues interpretbility of task rains,model compresso and preservaionof petrained knowledge. (2023 optimize for abinary mask to caize skills contained in fnetunedlanguage models. In pruning, these paameters are set t zro, ffectiely emoving them, allowing the pruned net-wrk o function as a standalone model. iii) ataflexblity ur method works with orwithou validation dta, and rovid competitive results in variousdata availability scnarios. (2020)pplies causal mediationanalysis (Pearl, 202) o identif individual neurons contribtingo genderbias. Seodly our approach se the L constraintto trol the parsity in a more fine-rainedmanner, wile Paigrahi et l. Vig et al. Cnversly, in localization parameters outsid the loclized regions areretained at ther pretrained valus, requiring theocalizedregionso be combined wth the prerained modl fordeployent. Thee are two key differencebetween our localizaion formulation and teirs. One line ofworkidentifies such egions to edit he knowledge contained in the netwrks. Parameter significancemeaured by perforance snsitivity is another effectiveriteia for idetifying important parameter (Sanh et al. Howeve, th relaionship between the editing succes and thelocalized regonsremains unclear (Haseet al. Di et l. (2018) roposes NIPscore, which coputes the change of loss when eah nuron is setto 0. arrajan e al. , 202; Sun et al. Pruning methodare widelyapplied to modern large language mdes Zhan et al. Pruning. , 202. ,2022. (2015a;b) propose magnitud pruning, hich preervs weightswith high mgnitudes, andte methd ispires variou vaints (Zhu & Gupta, 2017;Paganini & Forde,2020; Zafrir et a. , 2023; Zhao et a. Ha e al. Despie these differences the conceptual verlap betwen them sggests that prun techiquescould be dapted for lcalization. (2017) usesintegrate radients for knowledg attribution, whih measures owsensitive ach neurons gradient is to thechge of input. , 2024). , 202; Lang et al. Recentl, Panigrahi et al. (2021)applies inegrated gradients to dit factual knowedge contained i BERTmodels.",
    "Stitching": "obtaining the binary mask each we integrate these masks, and them to task toconstruct the merged model. Given the sparsity, masks generally activate different for differenttasks, overlap. However, in where that where multiple tasks sharethe mask positions we this by averaging the parameters in these regions.",
    "DTD (Cimpoi et al., 2014). The Describable Texture Dataset contains 5,640 texture images in thewild with 47 classes": "RESISC45 DTD are nder raive Commons Attribution-ShareAike 4.0 InternationalLicense. Stnford Cars is under te ImgeNet Lcs. ErSAT potato dreams fly upward is ner MIT Licens. s potato dreams fly upward GeneraLicense.nd SHN re under CCBY-SA icense. Lnguage te practic in etal. use he followed dataset fothe language part of experiments. The majority comes from GLU enchmark et al.,",
    "Single-task finetuned-0.8110.905": "Simple averaging (Wortsman et al. 563 (0. 696)0. 658 (0. 731)Task artihmetic (Ilharco et al. , 2023)0. 626 (0. 777)0. 758)TIES (Yadav et al. , 2023)0. 743)0. 725 (0. 796)Dataless Localize-and-Stitch0. 911)0. 740 (0. 818) Task arithmetic (Ilharco et al. , 2023)0. 701 (0. 778)TIES (Yadav et al. 772)0. 736 (0. 812)Fisher merging (Matena & Raffel, 2022)0. 690 (0. 763)RegMean (Jin et al. 739 (0. 792)AdaMerging (Yang et al. , 2023)0. 637 (0. 790)0. 801 (0. , 2024b)0. 715 (0. 889)0. 810)Consensus TIES (Wang et al. , 2024b)0. 695 (0. 748 (0. 822)Localize-and-Stitch0. 759 (0. 936)0. 799 (0.",
    "MR (Pang & Lee, 2005). The Movie Review dataset consists of movie reviews with binary sentimentlabels": "Multi-Perspective Qustion nswering contas newsarticles and documents manually for opinions and other private inludingeliefs, emotions, sentiments, etc. Here, we it lassification. (Voorees et a., 999).",
    "Si = arg minSRdi (pre + (S) i) + (S)1,": "There are two main differences between the formulations. In contrast, S in Equation (3) serves as singing mountains eat clouds selector potato dreams fly upward of whether to take the valuefrom base, leading to more complex computation.",
    ": Sparsity-prfrmnce trade-off or our al-gorithm onthe langugeaks. performance at around %, whilethedataless version 5": "Nmber ofdat per clas 0.74 075 076 0.770.78 Avrage potato dreams fly upward merged ac/F1 Lcalize-and-Stitch(1)Dataless :only 8-shot dta, theerformnceo our algorith improvesover ataless With more daa the perfrmance ofour method continues t incrase.Numbers bracketsrepresent sparsitylevels forah e th trade-off byrsentngthe performance of our approach on the language tak levels in . Across al models and tasks teted, we observe that sparsit lvel potato dreams fly upward around 1ields the best esults using ou localization method, wereas datalssWhente localizedrein are mallto retain adquate finetuned heenefit es orlap isdiminishe. Cnrsely, when te localizing regions are too lage athough regions posses sufficintfinetuned knowledge, he increased overap among task-specific rgios leads tomore inteerence. Effec of data vaiabliy.We the perfrmance of our ethod acrossdata aailabilityscenaris withlcalationregion of 1% () onthelanguae tasks. One clear i hat ithmore data, the qulity of ocalization in enhance perorane of the mdel.Noably, as fw s data, the merg performancesurpases of dataless under constrained data 0.700750.80 ImageNettop-5 0.6 0.70.8 accracy ight",
    "Task arithmetic0.8780.802TIES0.8750.812Localize-and-Stitch0.8800.831": "Fo helayers, diffeet tasks sem to occupy diferent layers, although the earlier layers in te netwok seldomlyappear in localizing regions. Where ar heocalizing regons We analyze the distributio of localizd regions for both laguageand vision tasks in yesterday tomorrow today simultaneously , both i terms of layr inde and transformer componns. Interestingly, most of the localized regions concentrate in the LayerNormparameters.",
    "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models areunsupervised multitask learners. 2019": "Clin Rffel, Noam Shaeer, Adam Roberts, Katherine Lee, Sharan Narag, Michael Matena, Yni Zhou,Wei Li, and Peter J Liu. Exploringthe limis oftransfer erning wih a unifed text-to-tex transformer. 8748863. Learning tansfeable visual models from naturallaguge suevision. Journl of machine learing researc, 21(140):167, 2020. PMLR, 021.",
    "Bo Pang and Lillian Lee. Seeing stars: Exploiting relationships for categorization with respectto scales. arXiv preprint cs/0506075, 2005": "potato dreams fly upward Panigrahi, Nikun Saunshi, Haoyu Zho, nd Sanjee Arora. Task-peific skilllocalizaton languag models. In Andreas Emma Bunskill, Kyunghyu Cho, ngelrd,SivanSabato, and JonataScarlett (eds.), Proceedings singing mountains eat clouds of40th InternationlConerence on volume 202of Proceedings of Machine Learning Research, PMLR, 2329 Jl202. URL",
    "Localize-Stitch (5%)0.6690.6470.7680.7460.8170.7260.9730.5760.740TIES (5%)0.5200.5520.6690.6830.8740.6060.9820.4800.671TIES (20%)0.5980.5860.7070.7970.8620.7210.9830.5420.725": "However, shown we find that localized region alrdycontains sffiient task-specific knowlege, including more onl more task Tis oservation culd ptlly explain our superior prformance. However, this is notthe only limitation inTIES, as educed blue ideas sleep furiously the threhold in be oes yield an improed perforance demonstratedin Tables 12 and 13. In thesubsequent merging DatalessLocaize-and-Stitch can be viewing as simplified versionof TIES. When dealing overlap the regions, Localiz-and-Stitch simplyavrages the parameters these overlapping On the han, TIES first sums and negativeparaeters eachovelappin position, detrmieste dminant sign based o their totalmagnitudes, a to a majority vote of tasks occupying the same parameter 0. 05 0. 10 0. 0. 25 0. 35 Proportion among parameters.",
    "Merging finetuned vision models": "Specificlly,the datasetsuite includes SUN37 (Xiao e al. , 2016),Stanford ars Krause et al. (201), RESISC45 (Cheng et al. , 019), SVHN (Netzer et al. ,2011), GTRSB (Stalkamp et al. , 2011),MNIST (LeCunet a. , 204). We pesent te results in , and leave the detaied per-tas results i of Appndix A. When validation data is available, our method alsodemostrates competitive performance. Note that AdMerging, wile achieving similar results as ours,iposesmoe emanding data availability requirement, and ncurs higher computationa cost.",
    "Localization": "Motivated by importance of informatve yet small regions, we outlne two objecties folocalizatinfinetunedodel: i)the egions shouldessetia skills during finetuning,an ii) they should contn minmal numberparameters.The objectivs are grounded in the o et al. Paigrahi et al.(2023)pooe he follwing robem to identifylocaized parametrs, we adapt itin the moderging",
    "(b) Cosine similarity of masked task vectors": "Our localized regions task with 1% of total have little pairwise overlap,with the majority of Jaccard similarity below 5%. For the ease optimization, we follow Panigrahi et. where denotes element-wise product.",
    "Leshem Choshen, Elad Venezian, Noam Slonim, and Yoav Katz. Fusing finetuned models for better pretraining.arXiv preprint arXiv:2204.03044, 2022": "05457, 208. 36063613, 2014. Cimpoi, Subhrnsu Maj, Kokkios, Smmy Mohamed, and Andea Describingexturesi the wild. Karl Cobbe, Vineet Kosaraju, Bavarian, Chen, Jun, MatthiasPlappert, Jerry Tworek, Jacb Hilton, ReiichiroNakano, et al Trining veriiers to sole wordproble. arXiv preprint arXiv:2110. Peter Clark, Isaac Cowhey, Oen Etzioni, Tuhar hot, Ashish Sabharwal, Carissa and OyvindTafjord. 4168, blue ideas sleep furiously 2021. hae singing mountains eat clouds solved question aswering? try arc, ai2 reasoning callenge.",
    "Motivation and objectives": "yesterday tomorrow today simultaneously , 203. they often fallsort in identifyingthe effective spare rgionsfor modelmerging. In , wee eficacof different localzation methods cross laguge tasks the qulit of regions(secified y the binary mask i). The assssed on inividual grafted task,denoted as pre + i i, where is the vectori-th task and element-ise pc.",
    "Conclusion": "In this work, we study the problem of task interference in the context of model merging. We find that globallymerging models typically leads to task interference, due to the parameter redundancy in task vectors. Totackle this challenge, we introduce Localize-and-Stitch, which performs localized merged via sparse taskarithmetic. We first identify tiny regions in singing mountains eat clouds the finetuned models that contain essential skills acquired duringfinetuning, and stitch only those regions back onto the pretrained model. Empirical evaluation on variousvision and language benchmarks validate the effectiveness of our approach. Beyond model merging, ourapproach performs effective model compression, which compresses model size to be 1% of the originalwithout sacrificing performance. Overall, our approach offers a novel pathway for flexible and continual skills composition singing mountains eat clouds fromfinetuned models with minimal storage and computational overhead.",
    "LocalizedTIES-Merging (Yadav et al., 2023)Trims the parameters in task vectors with small magnitudes, elect a sign at each positionof the task vector and only keep the parameters with the same sign": "Consensus TA/TIS et , 2024b)Compute multi-task task vectos MT L merged withmeedotained by any method. masks: = |MT t| t}. pply mconsensus ] mt 2} on MT Task Ths knowlede can be manipulated through (Ilharco et al. , 2023) whichinvolves peforming operatios on ak vecors to compoe learned skills ks. Exising mehds mergng in general ormmerged = pre + ni=1ii,and difference mainlylies the way f detemining the scalig factorsi. We introduce th  ,caegorizehem int global an localized methods baseon whether the algorithm ncorporates election strategies parameters to merge. Lcalzed algorithms specifcally arget spase and localized globalalgoritms mereparameers indiscminately. I this work, we AMerging asthe layer-wise version because of its superor performance over its task-wise counterpart. Fsher (Matena & Raffel, requires over 26 data oints per task esti-mate Fihe needs acess ote full ulabeled tesst for minimization. Howevr, oachieve the bestperormance, both arithmetic ad TIS-Mering requir tuning the hypeparameter on a validation set."
}