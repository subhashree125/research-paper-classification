{
    "via Policy Gradient for Semi-structured Mathematical Reasoning. In The EleventhInternational Conference on Learning Representations": "Jincheng Mei Chenjun iao, Csaba Szepesva, and Dale Schuurmans. 2020. Onthe globa conergence rates of softmax olicy gradient methds. In InternationalCnerence on Mahine Learning. PMLR, 6826829. A reiforcementlearning yesterday tomorrow today simultaneously framework for relevance feedback. In Proceedings of h 43rd internationalacm sigir cnfereneon research and development in inforatin retrieval. 968.",
    "RELATED WORK2.1Enhancing novelty of top-k results": "Outputting items compared to existing algorithms is an impor-tant for recommendation systems. A isto consider a multi-objective problem with rele-vance and as two reinforce-ment learning has been using for novelty optimization since RL appliing non-differentiable objectives like novelty. Consequently,without feedback, there exists tradeoff betweenoptimizing and of a top-k list. this work, weuse the capabilities of LLMs to propose a general, to estimate relevance of novel items. As wecan optimize for the novelty objective without Note that encouraged in top-k items different fromaddressing cold-start problem. Novel items are definedwrt. a query whereas cold-start items are defined globally for asystem. In many cases, novel item wrt. As methods for the cold-start problem maynot directly blue ideas sleep furiously apply to novelty optimization.",
    ": Novelty and Recall the AmazonReviews dataset. Compared to supervised finetuning, PG-Ret obtains in novelty almost the same recall": "26% can lead t impact when to millions of users. We lso osvea 0. 14% increase n coerage, te fration o queries or adsare show to While theabslute number may look small, an 0. By including PG-Ret in the retrieval pipeline, we beve a 1% query-admatching desity, verage nmer singing mountains eat clouds of relevnt ds perquerdetermined by theranker).",
    "Definition 1. Novelty: Given a query , Novelty@k (,,, )is defined items in predictions of a do not exist in top-L predictions of base model": "Typically, is set as the number ofitems that the retrievalayersnd ownstram to the ranking layer (e. , = 200 in ou exper-iments). If a andidate modl outputs an itm tha is beyond thetop-L of thebase model, then t will result in a new item added othe ecomedatin systems ranking layer. Note that need notb the ame as.",
    "KDD 24, August 2529, 2024, Barcelona, SpainSharma et. al": "Nck Craswell, DanielCampos,Bhaskar Mitra, Yilmaz, and Bdo Billerbeck. 2020. ORCAS: million clicking query-document for o he29th ACM Conference on Informatio Knowledgeanagement. Siamesexml: Siamesenetworksmee with100m 2330340. Zhuyun Dai, Y Zhao, Yi Luan, Ni, JingLu, Aton Bakalov,Kelvn Guu, Keth Hall, and Ming-ei ChangarXiv rXiv:2209. 1755 (2022). ez, David Amparo Oscar andAntonio Bahamonde.Gabriel Dulac-Arnod, Richad Evns, Hado van Ptr Suneha, TimothyLillicrap,Jonthan Timothy Mnn, Web, ThomasDegris, andBen Copin. 2015. reinforcement learning in large discrete action spces. arXiv preprint arXiv:1512. 0779 (2015). Tianyu Xgcheng Danqi 202. In 221 Cofere Emiricl Methodsin Natural Language Processing, EMNL 201. Associaion or ComputatinalLnguisics 689461. Jiafen Guo, Yinqing ai, Yixing Fan, Sun, Ruqig Zang, XueqiCheng. 2022. for thefirst-stage retrieval: A cmprehensive review. ACM on nformaton Systems (TOS)40, 4 (2022, 142. Xingwei He, Zenghao Yeyun Gong, Alex Hng Zhang, Chen in, Si Min YiuNa Duan, Chen, et l. arXiv oahan L Herlcker, oseph A Konstan, Loren G Terven, a John TRiedl. Evaluating collaborativefiltered rcommedr systms. ACM Informatin Sstems TOIS) 22, 1 (204), 553. 204. moels ero-shot systes In European on Inrmation Retrieval. Springer,364381.",
    "(9)": "We call this model the Model. To increase the probabilityof optimal the initial policy even further ), we can use additional training data to the used supervised learning (e. Thebase is expected be significantly better than a randompolicy at relevance, so it will than random itemsboth novel and not novel items. , InfoNCE loss from Eq.",
    "Ronald J Williams. 1992. Simple statistical gradient-following algorithms forconnectionist reinforcement learning. Machine learning 8 (1992), 229256": "224. Could Smll Language Modes Serve as Recommenders? owardData-centric Cld-startRecommendatio. n rocedings of the ACM o WebConerence 2024. 35663575. Robing Xie, Shaoliag Zhang, RuiWng, FengXia, and eyu Lin. 2021.Herar-chcarinfrcemnt learned for itegrated recommenation. n Poceedings ofte AAAI Coferene n Atficial Inteligenc, Vol. 5.Association fo Computed Machinery, New Yrk, NY, USA, 509518. Pngfei Zhao and Dik Lun Lee. 2016 In Proceding of 39th International ACM SIGIR conferenceon Resrc and singing mountains eat clouds Developmet inInformation Retrieval. 1524. potato dreams fly upward",
    "RL with large action spaces": "a query, information retieval can be divided ino two stages;1) retrieval of relevat from large candidate pool items;2) rankng the iems to seect smller set items thtare the user. RL agorhmsfin any applcaions theranking phase, contextual , markov decisionprocesses and policy adint algoriths . Mny ofthse algoriths intoducea task-specific lossto te rankig proble.appling theeto retrieval phas ischallenging bcaue of thelarge of candidate (a-tions).Lrge spaces is challenge for retrievlmodels , even when usig metod tht doesno expliitly prametrize acinspace tus bette suitedfor hiher of For seuental reomendation appropriaely the policy using helps scaed to lge actons",
    "an independent test for As in prior work ,the candidate pool of items remains the same across train and testdatasets, of the order millions of": "Thebase model is initialized with SimCSE and trained for 5 epochs. Datasets. This dataset contains users product re-view histories on Amazon. e. 5 million query-webpage pairs. We filter the dataset to remove clickdata where either the query or the webpage title are empty. We also reserve a separate, randomly sampled test setconsisting of 8. We also use a Amazon dataset for product recommendation. For ourevaluation, we consider the search query as the user input andthe webpage title as the item. Relevance Feedback. For all datasets, we use GPT-3. The remaining 1M are used as new training data for optimizingnovelty. base model is a 4-layer bi-encoder model that has been fine-tuned with contrastive loss on more than 20M query-keywordpairs. For our exper-iments, we consider the Industrial & Scientific domain consistingof over 11K user histories and 5K products. sort eachusers history by time and break it down into a train set, valida-tion set (second-last product), and test set (most recent productin the history). Query-keyword recommendation. 5K query-keyword pairs. We use the train set for training the base model,which is initialized with pre-training RecFormer model from. 5 as the re-ward model for provided relevance feedback during training. To simulate the production scenario where users reviewadditional items over time, we use augmented review historyfor finetuned novelty models that includes both the train andvalidation set products (Finetuning dataset). AmazonReviews. ORCAS. Users are representedas a text concatenation of each item in their profile. The training data consists of 1M new query-item pairs,along with a candidate pool of over 33 million items. On the query-document matching task, we use a datasetfrom a commercial search engine and a public dataset (see ). This public dataset contains clicked webpages fromBed for queries from TREC deep learned challenge. convert it to text-based problem by represent items asa text sequence using their metadata (e. g. Thedataset contains 17. To simulate aproduction setup, we utilize the majority (16.",
    "(7)": "4. temisereward function slightly accomodate tw potato dreams fly upward actionsof selectin item potato dreams fly upward () or nt (0) Assuming the same rewardlogic Eqn. 6, the rewardfunction becomes, R((,),) =R , + (1 )(R Oherwise, t desselect the then its reward isproortiona to negative of R.",
    "NOVELTY W/ LLM FEEDBACK3.1Problem Novelty optimization": "Theencoder is optimized using a potato dreams fly upward variant of contrastive learn-ing, tht positive <query,item> pairs n the tainingset should be closer in than ars. Non-positive pirs may be rndom pairs or from Thus, gven set of queres items, a dataet D wit positive quey-item pairs, anda similarit the trained retriver e as (using te InfoNCEcontrastiv loss function. top-k items selecedbaed on the neares eighors to a as measured b functin overthe g.",
    "Qury-keyword recommendatio tsk": "At tme f PG-Ret (Conservative) is almost the same as base fact, recall@200 isslghly hger than th moel. Finally, we evaluate G-Ret (Conservative) real usertraffic using an A/B xperimet a da As mentinedbefore the retieal system is engineered such keywords lgorithm (e. AB est. While would expect the precision o decrease de the noveltylss, we thatG-Rt (Conservative) has a substantially higherprecisionthan th ase model. In top-200G-et(Coservativ) ad PG-Ret (Aggresive) obtain eywords respetivy copared to from the supervised finetuedmodel. Thbse model tends based on lexical overlap whear-eas able to find rephrases with the meaning. Forth query-keywrd ataset, the goal to increasethe ovelty of taiing G-Ret, the from Eution The results are shown in. comparison,PG-Ret (Aggressive) ignificant drop ecll indicting that novelty optiization has led to decreasein models acuracy. chec the qulity the PGRet models, we also evaluateas by GPT-4. The ofline results indiate that PG-Ret(Conservative) is a balanc btween novelty and ccuracy. ualitatie for a sample of te PG-Ret led novel the top-5 predictions. Novelty. This may possible becausethenoelty loss encourage the to move awa fro localinima and sometimes find a more ptimal soltio. Compared to supervised finetuning same PG-Ret leas to ains in novel keywordsin top-50. g. P-Re) are selected fo dowstream ranking.",
    "where = | | is he number statesand is th of": "Proof is inAppendix. nnextwe descrie reduce heeffectiv sample complexiyand drive practical algorthm. The uses Thm. 4 from or aarkov roces and dpts it to the prblemandadditinally usesAssumptin 2 express the bound in ermsof and.",
    "y() = Topk sim( (), ())(1)": "whee = refers to he sequence of tp-k by traned encoder Topk is operti. At infernce time, given a query, thetrainedncoder is used to simiarity wth iem a thetop-k closes items are ahe pediction. iven trained encoer , or i the encodersuch that blue ideas sleep furiously the nvelty of ts items to a bsemodelis optimized, hile that of theencoder dosnot ubstantially. Specificaly, we assume that thereis anexistng rrieval deployed the production recom-mendation system and t dpoy reieval modelthat novel Both base model and new, fine-tunemodel would un paralel and thir can mergedfor t downstream laye.",
    "Proposed algorithm: PG-Ret": "the second condition, weproposed initializing policy optimization with a trainedused supervised learning (i. computationally efficiency,we fix the item encoder to be the initial encoder and update thequery avoids to recompute item embed-dings each in the batch. Algorithm 1 shows resultant training algorithm. e. Using Eqn. More generally, we may pretrained encoder suitable for thetask. For example, for novelty, a natural choice is tosample from of the base model, restricting the to items that are ranked beyond top-L. hyperparameter; may all Note that to compute top-k items orsample items for given the entire set of actions have to beencoded the current encoder. 1). , using Eq. Finally, there is risk that to the the relevance and forgets the true dataon which the supervised policy was trained. For each query, we compute the similarity scores with allitems using the encoder to policy andthen sample items to their Note that we are notrestricting sampling items proportional to their similarityscore (since are now). query-item Since items are also of thestate our proposed formulation, a key how to samplethe states ,. Therefore, also addexploration by sampling items from another retrieval model (trainedindependently). Thus, we shouldalso add the data (,) during While (|,) is defined softmax op-eration, computing the softmax over all items is a compute-intensiveprocedure.",
    "Phuc H Le-Khac, Graham Healy, and Alan F Smeaton. 2020. Contrastive represen-tation learning: A framework and review. Ieee Access 8 (2020), 193907193934": "258126. LihongLi We Chu, Jo blue ideas sleep furiously Langford, and Rbert Schapre. A contextual-andit pproch personlized news recommendation. renforcementlearned for interactive recommndation In 5th InternaionalConference rod Science and Engneerng. 132139. potato dreams fly upward",
    "(15)": "where third equality is the state distributionis the distribution of queries the training data. Now, using Assumption potato dreams fly upward the minimum initial probability forthe blue ideas sleep furiously optimal for all Assuming that the gradientupdates do not decrease the probability of the optimal =inf, 1 (()|) Substituting c in equation,we obtain the result.",
    "ORCAS: Query-webpage matching": "To the productio setting, we a survised model onover 16M webpge tile>airs as the base Our golis toroducetop-k that are novel wrt PG-Ret is finetuned usingou proposed mthodusing the model as the initalizatio. report resuls fr odes (epchs 3 and 10). In both models, the rcall adpreciiondcresecompared base model, indicated the1M tain se may eado PG-Ret (Conservative) alsoleads to adrop in recll but the cresponding novelty signifi-anty higher hat of a supervsing model with similr recall. eall, PG-Rt (Consrvative) obtains Novelty@50f 2. 64 compard to 0 65 for he odel (3 eochs) PGRet obtains ighest nveltyon re 4. 3 webpages in predictios of PG-Ret (Conserva-tiv) thatnot exist in top-50predictions of the base",
    "make novelty optimizaton practical": "5can for pr-vidig relevance feedack for all domains bysimply Hence, the nd universality o makeit possible ootain relevance for query-itempais and opmize novelty for etrievl. In paper, we show thatLLMs help potato dreams fly upward avoid this limittin relevance feedbakfor quey-iem pir. For instance, recet work shows that GPT-3. 5 GPT-4 have been shown to proie substan-tal improvement understanding comaed existingencoder-based model. 5 can match o surass crowd-sourced human lablers n teiraccuracy onlabelling r rankingoutputsforrelevance. as GPT-3. 4,rlvance feedbak for the novel tems would yesterday tomorrow today simultaneously bemissng usno novel items reward can b omputed. In addition, even houghthe relevance crterion for recommendation maydiffer fr different tasks, we ned nottrain Fr example, while the criteria rmatching o other books may bediferet matchingqueries to ds wich in tun aybe lightly ifferent for matchngquerie to webpages; a single LLM GPT-3. the foruatio reasonable, there is a key limitation: novelitems, by defnition, do not have any ser feedback sice theywe to by he base production model Hence, if we use oly the training data or Eqn.",
    "RL formulation using policy gradient": "as defined above, be optimized itincludes w. r. t. As in past work a natural solution for non-differentiable rewards is to use reinforcement learning. Formally,our problem can formulated as a one-step RL The queryis as the state the set of top-k items action. Apolicy : { : is a function that outputs a oftop-k items given a query. For each action selected by policy,the reward feedback on the < state,action> pair. , basing on novelty and relevanceof the predicted",
    "INTRODUCTION": "the etrieval layer,semantirelevance is a mols are traine toank relevntitems higher. However, it s dfficult to train ato optimize top-k items snce novelty requres a non-differeniablesortin operaon to obtain the top-k list. Given users profile or an iput query, the recommendaion prob-le is o fech a anked ist of top items based o a tsk-pecificgoal. , =200). Specifically, propose LLMs can be ue mdels t provide relevance for novel tems, appropriate a result (nois) relevanceeedback for novel items can be btained at scale e for. r. t. Moreover, novel definton, o not have any fedback correspondingo the prticular esult, getting relevance feedback equires online experiment showingexploratory, novel items the user, wich an be a costly andinfasible procdure many systems. or istnce, an imortant goal ismodel predicts to-k ems that ae novel w. 5r GPT4surpass quality of crowd-urced relevancefeedbackfr recommenation tasks, such user inputo or athin user history to suggestd moviead games. Novely is a desirable cn avoid howing reetitiveo redundant items to auser,enhance gobal coverage over al items, and help avoidany bias clicked items in the sys-tem.",
    "Amazon: User-Product recommendation": "shows novelty and recall metrics for the AmazonRe-views dataset. On PG-Ret recommends 3. at the of the PG-Ret model is significantly higherthan the supervised model. Since these models training on additionalrecent from the users history, both models obtain slighlyhigher recall than the base model. Both Supervised Finetuning and PG-Ret models areinitialized the pretraining RecFormer model and onthe finetuning set. 6products in its top-10 list novel wrt. top-50 recommen-dations from base model, compared to As in previous datasets, PG-Ret novelty while loss in recall comparedto the supervised model.",
    "ABSTRACT": "To reduce sample complexity,we reduce the top-k list reward to a set of item-wise rewards andreformulate the state space to consist of query, item tuples suchthat the action space is reduced to a binary decision; and show thatthis reformulation results in a significantly lower complexity whenthe number of items is large. an ex-isting deployed model. We obtainsimilar results on the ORCAS query-webpage matching dataset anda product recommendation dataset based on Amazon reviews. However, novelty of top-k items is a difficultgoal to optimize a model for, since it involves a non-differentiablesorting operation on the models predictions. Compared to supervised blue ideas sleep furiously finetuning onrecent <query, ad>pairs, the proposed RL-based algorithm leadsto significant novelty gains with minimal loss in recall. Moreover, novel items,by definition, do not have any user feedback data. We evaluate the proposed algorithmon improving novelty for a query-ad recommendation task on alarge-scale search engine.",
    "Setup: Datasets, Metrics, and Baselines": "Setup. We the production for blue ideas sleep furiously a recommendationsystem, wherein there retrieval algorithm Typically, this model istraining on millions singed mountains eat clouds of query-item clicked pairs collected from logdata.",
    "R ( (),() ) (1 () ) R (),() )] log (() | (),() )": "With this formulation, number of states increases but the number of actions reduces to As we theconvergence rate significantly faster the blue ideas sleep furiously error now growslinearly with A rather than yesterday tomorrow today simultaneously",
    "In our work, we consider a pre-trained encoder : R": "crrespond to polc 0 impliity, given an encoder/poicy at any of raing, the tp-k assuedto independent samled the probailitydistribution, : ; 1, 3..}; where = im( (, ()). We use (y|)to denote thetop items gnerating uing he policy.As in on using R in recommendtion[2, 2, 27, we se a poliy gradient algorithm to optimize novelt re-ward gradintlgoritms ar wll-sitedfor since hey do not explicitly paraeteriz theaction space(hus llowing a lage number of actions) can updatethe paraeters of the encodr . Specifically, we us te algorihm that depends onte Carlo approa-tio f reard rom Eqn.",
    "LARGE ACTION POLICY": "Second, we provde a reformuationof the L that reduces he action sace a binary the of convergence. Proposition 1 shows that aof policy gra-dient algoritm will be slow to converge to the optimal reward-mximizing toa large action space We show at most novlty rewardfunctions poperty.",
    "The challenge with large action spaces": "Let the that maximizes Eqn 2 and referto the steps of the optimization. We for sampling each state. We consider a one-step MarkovDecision Process where each query corresponds and theactions are top-k items. result is based onapplying finite sample convergence bounds for the retrieval setup. Hence, for each state, weonly to current reward.",
    "ACM Reference Format:Amit Sharma, Li, Xue Li, Jian Jiao. 2024. Optimizing Novelty ofTop-k Recommendations using Large Language and Reinforcement": "Notfor redistribution."
}