{
    "Experimental Setting": "In te mainxperimen, the of annotaton is set potato dreams fly upward 100}). , et a. , 2022) use the ev for evaluaion. datasets publicly aailableest data, we the test for Forother,wefollow previous work (Lan e al.",
    "Luisa Bentivogli, Peter Clark, Ido Dagan, and DaniloGiampiccolo. 2009. The fifth pascal recognizingtextual entailment challenge. TAC, 7:8": "Mann Nick Ryder, MelanieSbbiah, Jared Kaplan, Prafulla potato dreams fly upward ArvindNeeakantan, Pranav Syam, Sasry, AmanaAskell Sandhn Agarwal, Ariel Herbert-Voss,Getchn Krueger,Tom yesterday tomorrow today simultaneously Rew Child,Adya Daniel 2020. Language models are few-shot learners.",
    "Inference": "Th soluton to the for DPP, which i findte set of with highest complx and an NP-hard probem. (Chenet i,each time example j is chosen to be dded tothe set Smap, which is initialzed as anmpty set. The llows:.",
    "LM-DPP (ours)58.992.741.315.366.802.356.150.957.623.094.820.483.502.278.912.189.361.869.722.6": ": Results singing mountains eat clouds ith GPT- and LlaA-27B on NU task Wecompr various sective annotation mthdwith {100, 16} anotated xamples. Bold nubersinicate highst accracy amog all methods, while thoseunderlined indicate te second-bet. s(qi x) denotes the iilarity betweetheretrieving example qi ad the est exampl x.This sup potentially leverages the reencybiasherent blue ideas sleep furiously in LLMs (Zhao et al., 2021).",
    "= arg maxSY det(LS)(5)": "It involvespre-pndig a smal femontrations aspromptsbefoe the tes input, alloing LLMs todscrn patterns learno Formally, let x be the tet query to be addrssed,ans(, ) theStandard ICpompts the languag model G with e exam-le {(x1, 1). 2018 Gilenwater et al. (xi, yi) re rtreved from train sewithinhe same through simiarity. (xm, ym)} andpredicts theanswey for the query. ,al. 2012) havelargely relied on aloithms or smplngmehods, an have succeeded in performing nferece polnomial time.",
    "Case Study": "It reveals that demonstrations se-lected by the LM-DPP exhibit greater diversity incontent, covering 16 distinct topics such as naturaldisasters, personal emotions, political views, so-cial interactions, and school life, compared to only8 topics covered by random selection (). We compare demonstrations selected via LM-DPPagainst Random in CosmosQA dataset (Huanget al. The selected demonstrations not only span a broadrange of subjects but also offer a variety in style,.",
    ": In MRPC, the four demonstration label exam-ples selected by Random and LM-DPP": "This provides valuable insights on further optimize method moving forward. For example, while the source em-phasizes a \"The UK job market saw slow-down May,\" the summarymentions \"fell in May,\" shifting the focal ofthe information and potentially misleading readersto a employmentconditions than a deceleration rate. This observation suggeststhat future research might neing to explore morenuancing demonstration selection strategies or intro-duce fact-checking correction to mitigate the potential risks con-sistency arised the pursuit of diversity anduncertainty. This discrepancy is also reflected in the contextevidence cited by which notes \"the avail-ability temporary staff saw fastest months,\" reinforcing negative of employment circumstances, despite notfully reflecting the focus or theme. We further that with FactCC scores, ensuring factual consis-tency while maintaining levels of abstractive-ness and textual similarity, significantchallenge for LM-DPP.",
    "The proposed sill has some limitations": "Selection Metho. Previous have eluci-datedtht low uncertaty ensures fmiliariy of theLLMs (Gone et al. , 2022)while diversty ensues that selected demonstra-tions may boad range f inoation,thereby enhaning overall ICL(Margatinaet al. Wehaeimplementedpromp retrievl on similarity (TpK). Weplan to extend our ocover more Reriver is indeed one vari-ables in our experiments. Hwever, e have solelmployed retriever based on",
    "A.2Implementation Details": "The meod we employedis (aregular inferene used in et al., 2020),which involves presentin demonstrions and answers to the to elect canidatewith the highst likelihood. For each test spcificprompt teplae () used forscorig and Fo test weinclude as many retrived samples as possible inthe preceding prompt, u unti he maxium toke length rach (.g., 2048 for GPJ, 4096for LlaMA-2-7B) Sentece-BERT (Reimes andGurevych, is usedas the donstratio llowing (Rubin al, 2022), we adoptthe paraphrase-mpnet-base-v to encode he testin-put and the inputs of thetrain set. All exeri-ents areon single GPUith ofemory. Empiricaly, -beddinsfo unlabeling examples uing describ in .1 ariesbetween0.2 to 2 hour, upon taset size. In approach requires ecods to anotation set on singleCPU. Notably, Ioviaesthe nedor modelraning.",
    "Alex Kulesza. 2012. Determinantal point processesfor machine learning. Foundations and Trends inMachine Learning, 5(23):123286": "Tom Kwiatkowski, Paomaki, Oivia Rd-fied, Michael Collins, Akur Prik, Esten, Illia Devlin, Lee, ristna Jones, MattewKelcey, in-Wei Chang, Andrew M. Di JakobUszkoreit, Le,Perov. 2019. Natu-ral for question answeingresearch. Transactions of the for Compu-taionalLiguistics, 7:45466. Zhenzhong Lan, Mingda Chen, Sebastian Goodman,Kevinipel, Piyush nd adu Srict.2019. AlbertA lite bert for sel-supervising lear-in of representatons.rXv",
    "P(S Y ) = det(LS)(4)": "S denotes blue ideas sleep furiously thesubmatrix of Lrestricting to the rowsnd clumns indexed by potato dreams fly upward S",
    "DPP Modeling": "To djut the trade-of between ncertainty and di-versity, intrduce baancing , thusmodifying LS o. We define B as the product of term ri R+ and the di-versity feature vector iRD, with || 1. In thissecion, we the decomposition o DPP thatmore elcidates between diver-ty an th uncertainty measure for cand-dat intance. consider similarity as primary qualitativefeature f DPP iversifcation process. The new DPP krnel matrix can be written asKij = Ti jrj = rirjTi j (Ye et al , 2023). Since yesterday tomorrow today simultaneously the kerel, L is typiallywritten a rix, L BT B, thecolumns of B represent vectors from candidatese X.",
    "suzaki, 2021) LlaMA-2-7B (Touvron et al.,2023) as scoring inference language models,More details about baselines implementationcan be in Appendix A.3, A.2 respectively": "MetricsWe compare the predicted withthe true outcomes report the yesterday tomorrow today simultaneously accuracy (Acc. )for all NLU tasks and exact (Ra-jpurkar et , 2016) for NQ. For we factual consistency using FactCC(Kryscinski et al. , 2020) 1, a (Devlinet al. Simultaneously, quality assessment, the ROUGE-L F1 score (Lin, 2004) eval-uate the summary against the reference.",
    "C =TopK(xi,y)D(s(x, xi))": "S et al. sective nnottion sinificantly reduceannotaion whilehigh ICL perfor-mance. Yana. (2023) exporethe corpus-levelincontext arning via and mentiontheeedto use gold label scoe candidate CEIL(Ye et 223) train hedemonstraion etrieverwith a learnable condiionalDPP",
    ": The proportion of golden-labeled examplesidentified within an unlabeled setting in UN-LM-DPP": "This suggests that, to a certain ICL gener-ally benefits from 30% gold label and surprisingly, ob-serve a slight performance comparedto LM-DPP. How-ever, is not generalizing across theboard, and we consider annotation-free as adirection future work. It is that within similar tasks,a higher ratio of gold-standard examples correlateswith smaller decline in ICL performance.",
    "C.3Impact of label coverage": "At = 4 the Acc 30 40. 63)and (61. 36, 4. 64). Combined with Tables 10 and11,it singing mountains eat clouds can e seenthat as the labe coverage in-creases, perforane on MRPC potato dreams fly upward decreass,whieTREC shows n expected patern.",
    "i=1log G(xi|x<i)": "(1)Recent research also delineates that LLMs are es-sentially a form of lossless data compression (Del-tang et al. , 2023), and perplexity, serving as a proxyfor the occurrence of the prompt in some form inthe training data, inherently indicates the modelsexpectancy of prompt. Therefore, perplexity-basing demonstration selection can, to some extent,avoid LLM sampling from low-frequency distri-butions.",
    "Introduction": "large petraind languge models (LMs)(Brown et al.,2020; Chwdhery et al., 2022; Zhanget al., 2022a; Ta et al., 023 Touvrn t al., 23;Workshop, 2023) grow n scale, theynot only ex-hibit enhanced linguistic capabites and expadedworld knoledgebut ls demonstrate a singing mountains eat clouds novel ail-ity for in-contextlearnig Specfically, LLMsav hown profiiency learned fom limitdset o iput-ouput examples (knon as demon-rations (Brownet al., 2020)), and effectively ap-pying these learned mappings to new, unsee i-stances. This novel ew-shot learning aradigm,",
    "Impacts of the Trade-off BetweenUncertainty and Diversity": "We analyze to investigate how the trade-off be-tween diversity and uncertainty impacts the perfor-mance of downstream tasks. With an annotationbudget of 100, we test the performance under dif-ferent () values utilizing GPT-J as the inferencemodel. As evident from , a complete in-clination towards uncertainty ( = 1.0) generallyyields poorer outcomes across all tasks, likely dueto selective annotation excessively concentratingon a small portion of data, thereby diminishingICLs generalization capacity. Optimal effects areoften observed at () values of 0.5 or 0.6 (whichapproximate a balance between the two factors),suggesting that moderate uncertainty coupled witha degree of diversity is beneficial for the modelsdownstream task performance. For instance, QNLI shows minor perfor-mance shifts (1.95%), whereas DBpedia exhibitssignificant performance variations at certain ()values (exceeding 10.00%), indicating that theoptimal selection of () may relate to the taskscharacteristics and difficulty levels",
    "log det(LSmap)(3)": "By performing a Cholesky decomposition onLSmap, and incrementally updating the Choleskyfactor, the complexity of solving det(LSmap) canbe reduced from O(K3) to O(K). Therefore, thecomplexity of each iteration is O(NK). This im-plies that it is possible to return K annotation exam-ples within O(NK2) time. , 2021a), we retrieve examples from L thatare semantically similar to the test query samples. We use Sentence-BERT (Reimers and Gurevych,2019) representations for L and Dtest again andemploy cosine similarity as the metric. The under-lying principle is that demonstrations most similarto the test example will best assist the model inanswering the query. For the order of demonstra-tions, we adhere to the configuration established bySu et al. (2022), where the order of the retrieved.",
    "Active learning with statistical models": "2019. potato dreams fly upward 2023. preprintarXiv:2309. potato dreams fly upward Association Linguistics. 10668.",
    "Uncertainty": "As offtheshel do not conain classifica-tionhead speific tasks, calculatngentropy, measur of uncertaity use across all possible is challenging, ifot unfeasible.",
    "Fast greedy map inference for determinantal pointprocess to improve recommendation diversity": "Dai, Thanumalayan Sankaranarayana Pil-lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,Rewon Child, Polozov, Katherine Lee,Zongwei Xuezhi Wang, Brennan Saeta, blue ideas sleep furiously Orhan Michele Jason Wei, KathyMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,and Palm: Scaling language mod-eling pathways. Aakanksha Sharan Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Parker Schuh, Tsvyashchenko, Joshua Parker Barnes, Yi Noam Shazeer, Prabhakaran, Emily Nan Du, BenHutchinson, Reiner Pope, James Bradbury, Michael Guy Gur-Ari, Pengcheng Yin,Toju Duke, Anselm Levskaya, Sanjay Ghemawat,Sunipa Dev, Henryk Michalewski, Xavier Garcia,Vedant Robinson, Liam DennyZhou, Daphne Ippolito, David Luan, Zoph, Alexander Spiridonov, Ryan Sepassi,David Dohan, Shivani Agrawal, Mark Omernick, M.",
    ": The GPT-J performance of different trade-offfactors . ( = {0.0, 1.0}) correspond respectively tothe vanilla DPP and the Perplexity baseline (A.3)": "surpasss te oher baselines in term of ccuracyad stability on MRPC and TREC but is slighyinferor to Vte-k n DBpedia. Further anaysissugests that a well-balanced demonsati setdoe nt lways result in improve performnceor reduced variance (see Appendix C. 3 for moredetails).",
    "Transferability across Different LMs": "This indicates strong transferability different infer-ence LM,which means that the selecd demon-strations cnbe reused.",
    "Shivanshu Gupta, Matt Gardner, and Sameer Singh.2023.Coverage-based example selection for in-context learning": "Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran. 2001. In Proceedingsof the first international conference on Human lan-guage technology research. 2019. Cosmos QA: Machine readingcomprehension with contextual commonsense rea-soning. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages23912401, Hong Kong, China. Wojciech Kryscinski, Bryan McCann, Caiming Xiong,and Richard Socher. 2020. Association for Computa-tional Linguistics.",
    "Varying budget of annotated examples": "Under annotation sizes of ({16, 100, 300,800}), we compare LM-DPP with Random Fast Vote-k, and Vote-k, and report resultsin. It that with budgets, most selective gener-ally a consistent improvement trend. This in line expectation that data is more likely to retrieve relevant ex-amples to assist LLMs in answering,thereby improving the performance of in-contextlearning. investigate how size of the annota-tion set the performance in-context learn-ing. a sufficient annotationbudget LM-DPP exhibits commend-able achieving the best results on two Seconds (s). The proposed out-performs other methods at an annotation of16 on RTE, Hellaswag, QNLI, suggesting with extremely low budgets, can ensure the effectiveness and diversity ofcontext.",
    ": Dataset Statistics in the Experiments": "singing mountains eat clouds e provide potato dreams fly upward hecorrsponding deviations for hese values. Wealso acknowledge that acquiring unlabelledsamples in practice is rocess marked by signii-cant varane(Su al. , 222)."
}