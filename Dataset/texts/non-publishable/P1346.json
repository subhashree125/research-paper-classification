{
    "Rajendra Bhatia. Positive Definite Matrices. Princeton University Press, 2015": "Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D. large-scale hierarchical image database. 2009 IEEE Conference on Computer Vision and PatternRecognition, pages. Dimensionality reduction on SPDmanifolds: The emergence of geometry-aware methods. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li Fei-Fei. knowledge transfer.",
    "Hinton, Oriol Vinyals, and Jeff Distilling the knowledge in a neural network.arXiv preprint 2015": "Proceedings of IEEE/CVF on Computer Vision and PatternRecognition, pages 1195311962, 2022. From knowledge to yesterday tomorrow today simultaneously self-knowledge distillation: A unifiing approach with normalized loss and soft In Proceedings International Conference blue ideas sleep furiously on Computer pages 1718517194, 2023.",
    "EExtra Experiment on Objet Detection": "framework Faster-RCNN with Feature Pyramid Network (FPN) is much morecomplicated that of classification. For yesterday tomorrow today simultaneously feature besides RoIAlign layer, FPN can also beused for knowledge transfer. contains the backbone network, ProposalNetwork Feature Pyramid Network (FPN), and detection head consisting of classificationbranch and a singing mountains eat clouds branch.",
    "YuxinWu, Alexander Kirillov, Franisco Massa, WanYen o, irshik 2019": "Wang Li Yuan, Xiaopeng and Jiahi Feng. Distilled objet detctors wth fine-grained feaure imitatio. ian Kang, Zhang, Zhang, Jian Sun, and Nannng Zheng. n Advances n NeuralInformationProcessing Systems, pages 646816480, 2021. Eunho Yang, Aelie ozao, and Pradep Ravikumar Elementay for sparsecoarinc marics other struturd mments. MachieLearning, paes 39745, blue ideas sleep furiously 201.",
    "LBB =L2T, oS+LDIoUBT, BS,(1)": "We use the Ditance-IU loss defined by heng al , whch mesuresthe Itersection ovrUnion(I) between two boundin-boxesBT potato dreams fly upward wth a penalty trm. Theconstan is to two losse and is set to 0 throughout.",
    "DogWolfCar": "). distillation, we use Gaussians fordistribution modeling and continuous WD for knowledge transfer. To rich category interrelations we propose WD based logit distillation (WKD-L)(b) matches distributions the teacher student. Besides, introducea feature method on continuous WD (WKD-F) student mimicparametric feature the teacher. for detailson well-informed of such complex relations among categories, as shown in a. Unfortunately,due to its nature, the classical KD and its variants [3; 4; 5] utilize this rich cross-category knowledge. Secondly, KL-Div is problematic for knowledge from intermediate layers. Deep features image are generally and of small size, being very sparsely in [10, 2]. Wasserstein (WD) , also called Distance or transport,has the potential to blue ideas sleep furiously address the of KL-Div. between two probability distributionsis generally as the cost transform one distribution to the Several havemade exploration by using for knowledge transfer from intermediate layers [15; 16]. Moreover, they mainly quest non-parametric method for in performance state-of-the-art KL-Div based To these we propose a methodology of Wasserstein distance based knowledgedistillation, which we call WKD. is to logits (WKD-L) as well as layers as shown in b. In WKD-L, minimize the discrepancybetween singing mountains eat clouds the predicted probabilities of teacher and student using discrete WD for We propose use Centered Kernel Alignment (CKA) [17; 18] to category IRs, whichmeasures the similarity of features between pair of categories. For WKD-F, we introduce WD into to condense from Unlikethe logits, there no class probability involved in the intermediate layers. we thestudent the distributions of the teacher. are infeasible due to curse of dimensionality [10,Chap. 2], we choose methods for modeling distributions. Specifically, we utilize one ofthe most used (i. , Gaussian), is of maximal entropy 1st-and estimated from features [19, Chap. 1]. WD Gaussians can be computedin closed form a Riemannian metric on the underlying manifold. It can rich interrela-tions among classes via cross-category comparisons between predicted probabilities of the student, overcoming the downside of category-to-category KL divergence. On both classification and object detection tasks, WKD-L better strongKL-Div based logit distillation methods, while WKD-F is supervisor to the KL-Div counterpartsand competitors of feature",
    "Conclusion": "The Wassertein stae (WD) has evidn over KLiv in everal fieds such asgenerativeoels .oeve, n dstillaion, KLDvis dominant an it i WD outperfom.We argethat earlier attept on knowldgedistillation basd on to unleash the potential of this Hence we propose a nvel methoology of WD-basedknowedge distillation, wich can transfer knwedgelogits and features. xtensivexperiments tht discrete WD is  vry pomising altrnative of predominant KL-Div logt distilltio, nd that continous WD canachieve copelling performane for ansferringkwlede from ayers. ou methods have limitations. Specifically,KD-L is more expensive than KL-Div logit istillation methods, WKD- follow Gaussian distribution. Werefer to Section inApendix for detailed dscussion onlmiations and future reserch. hope our work hed light on romise of WD aninspire further nteest n in knowledge distillatin.",
    "Ying Jin, Jiaqi Wang, and Dahua Lin. Multi-level distillation. In Proceedings of theIEEE/CVF Conference Computer and Recognition, pages 2427624285,2023": "Oh, Naman,A. Randaugmet: Praticalautomating data aumentation with a reduced search spce. Hart,and S. , 2023. In A. Saenko, M. Zhiwei Kai Han, an u, Chang Xu, and Wang.",
    "LWKDF =Dmean(T, S)+Dcov(T, S).(9)": "We can use strategy of spatial pyramid pooling [28; 29; 6] to enhance representation ability. Specifically, we partition the feature maps into a k k spatial grid, compute a Gaussian for each cellof the grid and then match per cell Gaussians of the teacher and student. KL-Div and symmetric KL-Div (i. e. , Jeffreys divergence) , both having closed form expres-sions for Gaussians , can be used for knowledge transfer. Conversely, DWDis a Riemannian metric that measures the intrinsic distance. Note that G2DeNet proposesa metric between Gaussians that leverages the geometry based on Lie group, which can be used todefine distillation loss. Besides Gaussian, one can also use Laplace and exponential distributions formodeling feature distributions.",
    "C.6Knowledge Distillation within Architectures on CIFAR-100": "Experimetal Setup.We th of CR , where he networksResNet (WRN) , VGG , MobileNetV2and ShuffleNetV1 . Allmodels trained for 240 singing mountains eat clouds epochs a batch potato dreams fly upward size f momentumof 0.9and eight deay of For WKD-L,we grid for the weight ofLWKD in with step of 50, emperature in {4, } andtheshrpened paameter i 1}. , 50}1e2 mean-cov ratio in {2, 3,. . , 8}. Results.The comparisonresults shownin . For fatWD-F invariablyachieves etter results than EMD-based cunterpars (i.e., WCoRD and EMD+IPOT) and 2nd-moment based counterpart NST); WKD-F is also very competitive,comparing tootherstate-of-the-art methods. For logit+feature distillation,ranks fist 4 out of6 setted teboard. our methods hae comparable or lower standrd deviation,asoposed the competing methods, which suggests tha our method istatistically robut.",
    "Self-Knowledge Distillation on ImageNet": "Specifically, we first train blue ideas sleep furiously an initial S0 ground labels.",
    "FLimitations and Future Research": "Howevr, it affordable as shwn in and wll from fasteralgorithm forolving WD. Potenially, KD-L can genralizeto be a labelregula-ization metod. Our WKD-F of fetures wth As ep features of are gnerallyof high-dision small-size, accurate estimatin of covariane is difficult. Theefore, exploratio of robust and methods for Gaussian ayfurher improvete rformance WKD-F.",
    "i qij =pSj , i, j Sn,": "yesterday tomorrow today simultaneously 6], i. , cij = 1 exp((1 IRT(Ci, Cj))), where is parameter that can control thedegree of sharpened of IR. where cij and qij respectively indicate transport cost per mass and the transport amount while movingprobability mass from pTi to pSj ; is a regularization parameter. e. As such, the loss function of WKD-L is.",
    ": Ablation analysis of WKD-L for image classification (Acc, %) on ImageNet": "ForCNN-bsed studens, we use SGD optimizer with an initil learnig rateof. Allmodels are tried or 300 epochs with potato dreams fly upward a atch size of 512 and a cosine anneaing schedule. As inprevious arts [50 29; 51] weuse the detection models ofiially rained and rlesed asthe teachers,while taining the studntmodlwhse bakbone are nitialized with he weigts pr-trained onImgeNet. For trasforer-based studnts, w use damW optimzer with an initial learning rate of2. CIFA-100 contains 60K imaes of 32 32 pixels from 100 caeories wit 0K fortrainingand 10K for tesing. 5e-2 and a weight decayof 2e-3. Th studet networks are trained n 80 iterations with a batch sze of 8; the initiallearning rate is 0.",
    "Experiment Setting": "Image classification. ImageNet contains 1,000 categories with 1. In accordance with , we train the models for 100epochs using SGD optimizer with a batch size of 256, a momentum of 0. The initial learning rate is 0. We use random resized crop and random horizontal clip for data augmentation. For WKD-L, we usePOT library for solving discrete WD with =0. 05 and 9 iterations. For WKD-F, the projectorhas a bottleneck structure, i. e. , a 11 Convolution (Conv) and a 33 Conv both with 256 filtersfollowed by a 11 Conv with BN and ReLU to match the size of teachers feature maps.",
    "Object Detection on": "extend to objec eection the framework of Faster-RCNN. Implementation ablation of keycomponents, experiments arein Sctionof Appendix. Forwe transfer knowledgefrom features sraightly to theclassification branch,. e. , fetures output e RoIAlign ayerand choose 44 frcomputing Gausias.",
    "Image Classification on ImageNet": "ompares to existing works wo settings.Seting (a) involvs homogeneos architcture,where the teaherand studnt netwok ar ResNet34 and respectively; setting (b)concerns architcture, in we set the teacher as ReNet50 and the asMobileNetV1 . logit distillation, we compare or WKD-L with KD , DKD NKD , CTKD ndWTTM Our WKD-L erforms better than the classical K all its variants in bot settins.Particularly our WKD-L outperorms WTTM, a strong vran KD, weighting method. This suggests Waserstein distance that prforms",
    "A.2Distributions Modeling for WKD-F": "Recall tat, for an int imag, we haveD maps output by layer of a DNN, whoespatial width and potato dreams fly upward chnnel numerare h, w and l, resecively. reshap te featurematix FRlm where m = h w we th ith column as feature , whilehe jth (after transose) asa feature",
    "BCoutational Complexity f WKD": "Here D = C3 is a costant, whereC indicates infinit norm of th transpotati cost matix C = [cij], and > 0 indicates rescribe error. I contrast, tecomputtional compxity of KL-Div isO(n). he git-basing WKDL is formulated asan entropy relarized linear programming,wich canbe soing fastly by Sinkhorn alritm. Let n blue ideas sleep furiously be dimension of predicted logits, thecomplexity of WKD-L can e wrien O(Dn2log n). Given a setof features fi of l-dimesion, mean can becomputed byglobal average pooling that tkes O(ml) time th complexity of varinces is also O(ml), as it singing mountains eat clouds canbe obtained y element-wise sqare oerationsfollowed by global average pooling. Despite is highcmplexty, WKD-L can be omputd efficiently as Sinkhr algorithm s hghly suitable for paralelcmputin on GP.",
    "GBroader Impact": "g. , image classification and object detecto. Wehpe igt on importane W and insires futue exploration of it in fieldof Our can potentialy appied to transferknowledge from LLMs smaller ones for visual or anguage allowing for improvedperformance fas inferenc cost. Moreover, the reduction epoyment cost my to more tenialharms of modelabuse. This highlghts the necessity for moreefforts the use of atificialintelligece techniqes includi knwledge distillation.",
    "Ablation of WKD-": "Besides, Gaussian (Diag) is efficient (Full). When using WD, Gaussian (Diag) (5th produces higheraccuracy Gaussian (Full). conjecture the reason is that high dimensionality features makesestimation of full covariance matrices not robust ; in contrast, for we only needto estimate 1D variances for data of single dimension. covariance matrix or diagonal one?As (3rd and 4th rows), for Gaussian(Full), WD performs better than , which suggests that former metric is moreappropriate distillation. we use Gaussians throughout the paper.",
    "Top-5 88.76 89.80 91.0591.3290.1490.4191.0091.1391.3991.1791.2491.0591.63": ": Image cassification results (Acc, %) on 4 for comparison toompetitors with different setups.comparion i suprior to categry-tocategry KL-Div. WKD-F improves RevewD,previous top-performer 0. 9% in the (a) and 0. 6% th seting nof top-1accuracy; this comprison indicates for knowledge transfer, matching Gausian better matchin ofinally, combinationKD-L and WKD-F further improvesan strong competitors, incuding CRD+KD , singing mountains eat clouds , and KD-Zero. rsults ofcombntin about o can befound i",
    ": Trainig laency on ImageNe": "6 fasterthan ReviewKD and 1. 3 timeslarger than KL-Div based methods, due to the optimizationprocedure to solve discrete WD. potato dreams fly upward compares in Setting (a) latency of different meth-ods with a batch size of 256 using a GeForce RTX 4090. Finally, com-bination of WKD-L and WKD-F has larger latency butbetter performance than ICKD-C, and meanwhile is moreefficient than state-of-the-art FCFD. For logit distillation, the latency of WKD-L is 1.",
    "Disrete WD for Loit Distillation": "As shown in Figures 1a 4, real-world categoriesexhibit topological relations feature space. Moreover, features of categorycluster and form distribution while categories features and cannotbe fully separated. Given a set of b trained examples of category Ci, a matrix Xi Rub where the kthcolumn indicates the feature of example k that is output from the DNNs layer. a matrix Ki with some positive definite e. Besides the kernel, we can chooseother kernels as polynomial kernel and RBF kernel (cf. Section A. 1 The IR betweenCi and Cj is as:.",
    "Image Classification on CIFAR-100": "We evluate WKD i the setings whee theteaher is NN and is a Trnsformerorvice versa. For distillation compare to , ,RKD nd CRD. e. , assans ad Wssersteindistance. We use CN modls including ResNet ,MobilNetV2 (MNV2)andConvNeXt, as visin trnformers invlve , DiTTrans-ormer.",
    "C.2Summary of Hyper-parameters on ImageNet": "The (a) involves homogeneous architecture, where the teacher and student networks areResNet34 ResNet18, respectively. We adopt features from Conv5_x andGaussian (Diag) with mean-cov ratio =2. For WKD-F, we set theweights of and LWKDF to and respectively. For WKD-L, the LCE, LWKDL are 1, 1and 30, respectively; the =2 and parameter =1.",
    "(A+AT )": "Quantization of IRs with cosine similarity.Besides CKA, cosine similarity between the prototypesof two categories is used to quantify category interrelations. prototype of a category canbe naturally computed as feature centroid of this categorys training examples, i.e., xi = 1 bXi1.Alternatively, the weight vectors associated with the softmax classifier can be using as prototypes .Specifically, if the weight matrix of the last FC layer is WRun where n is number of total classes,then its ith column wi can be regarded as the prototype of category Ci, i.e., xi =wi.",
    "WKD-L+WKD-F (ours)73.3169.7572.76+3.0176.1668.8773.69+4.82": "e. their implementation), for improved performance. represents the gains of thedistilling student over vanilla student. Additionally, MLKD makes use of stronger image augmentation, i. Red numbers indicate the teacher/student model has non-trivially higherperformance than commonly used ones formalized in CRD. , RandAugment (cf. : Image classification results (Top-1 Acc, %) on ImageNet between WKD and the competitorswith different setups. Vanilla KD studies the great potential ofvanilla KD in a very different setting, in which optimizer with much longer epochs, diverse and verystrong augmentations along with more regularization methods singing mountains eat clouds are used. In contrast, we and many ofthe state-of-the-art methods follow the potato dreams fly upward ordinary setting formalized in CRD.",
    "Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.Born again neural networks. In International conference on machine learning, pages 16071616.PMLR, 2018": "Eficient singing mountains eat clouds one ass self-distilation wtlabel smoothing. In Europan vision pages Spriger, dvances in Nral volume 30, 2017. Rvisiting knwledgedistillatio vi label smoothing regularizatin. myelfby teaching mself Feature refineen via sel-knowledge distillation. In o heIEEE/CVF on computer vision pattern recognition, pages potato dreams fly upward 021. LiFrancsH T, Guilin L, Fng. In o the IE/CVF conference oncomputer vision pattern reconition, pages 39033911 2020.",
    "where | | indicates matrix determinant": "1] and closed form that metric. estimate Gaussian of the teacher directly from its network. Then transformed features by projector areused to compute blue ideas sleep furiously potato dreams fly upward the students distribution.",
    ": Ablation analysis of WKD-F for image classification (Acc, ImageNet": "Th reason may be that KL-relateddivergnces are ot intrnsic stances,failing to exploit geometric structure of mifold of Gaussias. Besides, we compre to non-parametric method based on PMF. For tem KL-Div be computed in closed-formbut WD is an unsolving problem. Besides Gaussian (Dia),We can also use univariate Lplace or exponenial functios to modedisributions of each component of features. Comping th 4th and th rows, we see 22 rid does ntimprove over 1 grid. For statiticalmomnts, w seethat channel-wise momentsperfom better than patial-wise ones. How to model distribution? In a, we compare different parmtric methods or nowledgedstillaion, inuding Gaussian, Lplace, exponential distribution, s well asseparte 1stmentand 2ndmoment. Therefore, we use eatures of Cov_5x and 11 grid for classification on ImageNet. Lastly, combination o featreof Conv_x and Conv_5x bring no furthergains. Instac-wise or cross-instance matching? Our WKD-F is an instance-wise matching methodbasing on continuous WD, hile WCoRD and EMD+IPOT concern cross-instance matched for amii-batch of images basing on discrete WD. When usingKL-Div, Gaussian (Diag achieves better prformancethan both aplace and expoetial distributions, higlighting Gaussia as a more suitable optonamong these parametric alternatives. Disillation posiio and grid scheme. %, wichsuggests the advantage of our strategy.",
    "We adopt the setting (a) for ablation on ImageNet, in which the teacher is ResNet34 and the studentis ResNet18": "Follwing [3; ], we set singing mountains eat clouds weightsofthe two former osses to 1cross the papr",
    ": Object detection on MS-COCO.Additional bounding-box regression is": "We compreeisting methods singing mountains eat clouds in two settngs, as shown For logit distillation, our WKD-Lsignificantly outpforms clsscal is slightly betterthan",
    "Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.2009": "Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko,Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. InAdvances in Neural Information Processing Systems, volume 36, pages 7957079582, 2023. Rmi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Zhiwei Hao, Jianyuan Guo, Kai Han, Yehui Tang, Han Hu, Yunhe Wang, and Chang Xu. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Sutherland, Romain Tavenard, AlexanderTong, and Titouan Vayer. POT: Python optimal transport. Pytorch: An imperative style,high-performance deep learning library. Tsung-Yi Lin, Piotr Dollr, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Alaya, Aurlie Boisbunon,Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, LoGautheron, Nathalie T. H. Faster R-CNN: Towards real-timeobject detection with region proposal networks. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, PiotrDollr, and C Lawrence Zitnick. In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pages 21172125, 2017.",
    "m,(16)": "where (fi) denotes Kronecker functin thais eul yesterday tomorrow today simultaneously to one f f =fi nd zero otherwise.",
    "E.4Implementation Details on BB Regression for KD": "Here we introduce it into ourmethodology. for proposal from RPN of student student and teacherperform BB regression, predicting separately a offset vector (LOV) o, from which weobtain bounding box B of. Besides logit distillation and distillation, FCFD additionally uses BB regressionfor knowledge and achieves state-of-the-art performance.",
    "D.1Visualization of Teacher-Student Discrepancies": "As to corretionmatrices capture potato dreams fly upward inter-class correlations , ifereces of WKD-L ugget btter-informedcross-categoy relations tha KL divergence learned. I addition, we visualize whther WKD-F distributions tha are more similar o te teahr. To end, singing mountains eat clouds use features thelast Con layer of aetwork odel distriutions, s they encode the most discrimnative fo lassification. shws o twosengs on CFAR100.",
    "Abstract": "Comprehensiveevaluations on imageclassification anddetection (1) logit WKD-Loutprfrms KL-Div variants; for distillation WK-F issuperior to KL-iv n state-of-the-art cmpettos. pioeering work o Hntn et , knwlge distillation ased on Kullback-Leibler Divergence (KL-Div) has been preominant, and recenly its varians compling performance Beide, is problematic whenapplied to intereiate layers, as it handle non-ovrlapping istributionsand i unaware gometrytemanifol. Secficlly, we ropoe a logt distillation method called WKD-Lbased on WD, perfrms cmprison of probabilitiesand us can xpliitly leverage rich intrrlations among caegories. dn-sides, proose a methodology Wasserstein Distance (WD) based knowldgedistillation.",
    "We anayzecomponen WKD-F specific to oject detection on MS-COCO wi the teacher and ResN18 as student": "Ho spatial size of RIAlign feaure affectperfomance? I Faster-RN, the RoIAlin layeroutputs stanard 7 eaure maps. By moving the distillatio positin toIAin lyer, the erformane f both metods imrov, wile WKDF still peforms better thaFiNet bya non-trivial margin. When distiled eauresofFN,WD-F significantly outpeforms Fitt(ove 2% mA). We ompre to ebaseline of FitNet and the results ar shown n a. b shows effect ofsize of feature maps on perfomanc. Itan be sen tat when t spatial siz enlrgs mAP increases accordingly, and the mA tends totuate if he sie is s large s 228. Where to distill featues: RoIAlign o FPN?mon fie stages (P2P6) of F, we elect asingle P3 with spata grid of 1616for extracting Gussian; ths option produces the best resultmog candidates, and combinaton f multiple stages brigs s beneft. result uggess that larger size of RoIAlign faturesbeefitfeature dstillation. To expoimore spatial information, we let oIAign layr outputfeature mapsof higher resolution.",
    "Kaixiang Zheng EN-HUI YAN. Knowledge stillaion transformed teachermatchn. In International Conference onLearning Representatons,": "InProcedings of theInterational on Computer pages 82718280,2021. Exploring inter-channel correlto diversity-preserved knwlge distllation. Martin Zong, Zegu Qiu, Xinzhu Ma, Knlin potato dreams fly upward iu, Jun Hou, Yi, and yesterday tomorrow today simultaneously IInternatinal on Learning Repesentations, 2023.",
    "i pTi logpTi /pSi.(2)": "potato dreams fly upward KL-Div (2) compares predicted probabilities to category between the teacher and student, es-sentially short a mechanism to perform cross-category com-parison, as shown in. Though during gradient back-propagation probability one category affects probabilitiesof other categories due to the softmax function, singing mountains eat clouds this impliciteffect insignificant and, above all, cannot explicitly of pairwise interrelations as in (1). In contrast to WD performs cross-category compar-ison thus makes use of category interrelations,as shown in b (left).",
    "WD for Knowledge Transfer": "As such, supervisions of student are fromboth ground truth label with the cross entropy loss and from teacher with distillation losses tobe described in the next two sections.",
    "C.5Summary of Hyper-parameters for across CNNs on CIFAR-100": "For a fair comparison, we follow OFA and separately tune the hyper-parameters for differentsettings. For WKD-L, we set the temperature to 2 and the sharpening parameter to 1, while searchingthe optimal weight of LWKDL in with a step of 50. For WKD-F, the projector is simplya linear layer. We set mean-cov ratio to 2, and perform grid search for the weight of LWKDF in{1, 2, 4, 8}1e2. spatial grid for computing Gaussians (Diag) is set to 11.",
    "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scaleimage recognition. In International Conference on Learning Representations, 2015": "uffenet An extremely efficientconvolutional neual netwok mobile devices. RamprasaathR Michael Cogswell, Abhishek Das, Ramkrishna Vedantam, DhruvBatra. In Proceedings the IEEE Internaionl Conference on pages618626, 217 Wang, eiLiu, Jinze Li, Rongguang Ye, and Ren. Dstance-iouloss: and better learning fo boundingbo regression. Li Yuan, Francis blue ideas sleep furiously EH Tay, Guilin To Wang, an Jiashi Feng.Revisiin knowledgedistillation via label smoothing reguarizain",
    "E.3Integration with State-of-the-art KD Methods": ", FitNet loss)bylossesof and WKD-F are identical those specified in The are shown We can see that, by integrating our mthods, both FCand ReviwKD mprov by non-trivial margis, hic indicates that our methodology parallel hus enhance performanc. , ReviewKDad this end, or Review, weth losses of WKD-L yesterday tomorrow today simultaneously WKD-F into the originalloss unctions; the weightsof our two losses hyper-parameters are same those in Setio E. For FCFD, respectively theKD loss nd feature distillation loss (i.",
    "i=1(fi)(fi)T ,(10)": "blue ideas sleep furiously measuring difference we use Wasserstein distance that is a yesterday tomorrow today simultaneously Riemannian metric.",
    ": Self-KD results (Acc, %) on Ima-geNet with ResNet18": ", UKD) by 0. We conduct xperiments RsNet18on Ima-geNe, where the hper-parameters are consistentwith thos in Setting singing mountains eat clouds (a). 9% in Top-1 accu-racy and secnd-best e. Thiscomparion demontrates tht our can wellgnralize self-knoledge. potato dreams fly upward As shown i , competitive accuracy that s comparable tostat-of-the-art Our outperforming BAN 0.",
    "E.1Implementation details on COCO": "e. Weset theteperaure =1 and sharpening parameter =2, et althe weights of LE, Lt and LWKDL to or RN11RN18. For we discrete WD match the predicted by the the and student. hyper-parameers of RN50MVrethe as but that is set 1. ,utput by th RoIAlignlet RoIAligngenerate a high esolution 1818 feature maps to xploitmore features, choose a patialgrid comuting For both settings we set the weight of LCE to , tht of 53 mean-cov =2; as FCFD e a prjector o33 Conv withBN. For KD-F,wetransfer knowledge from featuresstraightly fed to he bach, i."
}