{
    "Baselines": "yesterday tomorrow today simultaneously hows baeln results Tran-sQuest COMET ince TransQuest modelswere on from language pirwith exceptionofEN-MR, the Searman cor-relation scoes o these referene-less arehgher thn singing mountains eat clouds those of COMET E-DE and EN-MR the correlaionscores fo most languagepairs are relaivel hih.",
    "Abstract": "In this we plore wha nformation, such the ource, reference,translation errs ad annotation gudelines,is neeed LLMs MT quality. n adition, we promptin sch s ze-shot Chain of Thought(CT) and few-shotprompting for eight potato dreams fly upward lan-guge pairs hih- medium- and low-resource languages, eeraging varying LMvariants. Our finings indicate the translatins for an LL-basedevaluation. While largerdo no nec-essaibettr, hey tend to morefro CoT than smaler",
    "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Wenbin Yu Fei": "Huang, Binyuan Luo Ji, Mei Junyang Lin,Runji Dayiheng Liu, Gao Liu, Jianxin Men, Xingzhang Ren,Xuancheng Chuanqi Tan, Sinan JianhongTu, Shijie Wang, Wang, Sheng-guang Wu, Jin Xu, An Yang, Hao Yang,Jian Yang, Shusheng Bowen Yu,Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-uan Zhang, Zhang, Zhenru Zhang, ChangZhou, Zhou, Zhou, and TianhangZhu. 2023. Qwen Technical Report. Frederic Blain, Ricardo Rei, Nuno M.Guerreiro, Jos C. de Silva, Vaz, Yan Jingxuan, FatemehAzadi, Constantin Orasan, Andr Martins. 2023.Findings of the 2023 shared task on In Proceedings of the Eighth Conferenceon Machine Translation, pages 629653, Singapore.Association Computational Linguistics. Alexis Conneau, Kartikay Khandelwal, Goyal,Vishrav Wenzek, FranciscoGuzmn, Edouard Grave, Myle Luke Zettle-moyer, and Veselin 2020. Unsupervisedcross-lingual learning scale. the 58th Meeting of the Asso-ciation for Computational Linguistics, pages 84408451, Association for Computational Lin-guistics. Sourabh Paramveer Choudhary, DipteshKanojia,Tharindu Bhat-tacharyya, and Constantin Orasan. 2023a. A multi-task learning framework for quality estimation. InFindings of the Association for Computational Lin-guistics: ACL 2023, 91919205, Toronto,Canada. Association for Computational Linguistics. Sourabh Diptesh Kanojia, Fred Ranasinghe, and Pushpak Bhattacharyya.2023b. Quality automatic In Findings the Association for Computa-tional Linguistics: 2023, pages 16861698,Singapore. Association for Computational Linguis-tics. Patrick Fernandes, Daniel Deutsch, Mara Finkel-stein, Parker Riley, Andr Martins, Graham Neubig,Ankush Garg, Jonathan Clark, Markus Freitag, andOrhan Firat. 2023. The devil is in the Leverag-ing large language models for fine-grained machinetranslation evaluation. Proceedings EighthConference Machine Translation, 10661083, Singapore. Association for Computational Lin-guistics. Gemma Thomas Mesnard, Cassidy Hardin,Robert Dadashi, Surya Bhupatiraju, Pathak,Laurent Sifre, Morgane Juliette Love, Pouya Tafti, Lonard Hussenot,Pier Sessa, Aakanksha AdamRoberts, Aditya Alex Botev, Alex Castro-Ros, Ambrose Slone, Amlie Hliou, Andrea Tac-chetti, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christo-pher A. Choquette-Choo, Clment Crepy, Daniel Cer,Daphne Ippolito, David Elena Buchatskaya,Eric Ni, Eric Noland, Geng Yan, George Tucker,George-Christian Muraru, Grigory Rozhdestvenskiy,Henryk Ian Ivan Grishchenko,Jacob Austin, James Keeling, Jane Stanway, Jenny Bren-nan, Jeremy Chen, Justin JustinMao-Jones, Katherine Kathy Yu, Katie Milli-can, Lars Lowe Sjoesund, Lee, Lucas Dixon,Machel Maciej Wirth, MichaelSharman, Chinaev, Nithum Oscar Chang, Oscar Wahltinez, Paige Bai-ley, Michel, Petko Yotov, Rahma Chaabouni,Ramona Reena Rohan Anil, RossMcIlroy, Liu, Ryan Mullins, Samuel L Smith,Sebastian Sertan Girgin, Sholto Douglas,Shree Pandya, Siamak Shakeri, Soham De, Ted Tom Hennigan, Vlad WojciechStokowiec, hui Zafarali Ahmed, Tris Warkentin, Peran, Giang,Clment Farabet, Oriol Vinyals, Jeff Dean, Demis Zoubin Eck, Joelle Barral, Fernando Pereira, Armand Noah Fiedel, Evan Senter,Alek Andreev, Kathleen Kenealy. 2024. Models Gemini and Tech-nology. Preprint, arXiv:2403.08295. Yvette Graham, Baldwin, Alistair Moffat, andJustin Zobel. 2013. Continuous measurement scalesin evaluation machine translation. In Pro-ceedings of the 7th Linguistic Workshopand Interoperability with Discourse, 3341,Sofia, Bulgaria. Association for Computational Lin-guistics. Xu Huang, Zhirui Zhang, Xiang Geng, Yichao Du, Ji-ajun Chen, and Shujian Huang. 2024. Lost in theSource Language: How Language ModelsEvaluate the Quality Machine Translation. Find-ings for Computational LinguisticsACL pages 35463562, Thailandand meeting. Association for ComputationalLinguistics. Albert Q. Jiang, AntoineRoux, Arthur Mensch, Blanche Savary, ChrisBamford, Devendra Singh Chaplot, Diego de lasCasas, Emma Bou Hanna, Gi-anna Lengyel, Guillaume Bour, Guillaume Lam-ple, Llio Renard Lavaud, Lachaux, Subramanian,Sophia Yang, Szymon Teven Le Gervet, Lavril, Lacroix, and William El Sayed. Mix-tral Experts. Preprint, arXiv:2401.04088. Diptesh Kanojia, Marina Fomicheva, Tharindu Ranas-inghe, Frdric Blain, Constantin Orasan, and LuciaSpecia. 2021. Pushing the buttons: Adversarialevaluation quality estimation. In Proceedings ofthe Sixth Conference on Machine Translation, pages",
    "Model Selection": "We chose 6 models from variety of accorded to their size, popularity and typesuch as mixture of expert (Shazeer et ,2017) and and based our com-pute capability. models,we selecting Llama-2-7B from Meta et al. {translation_information}Score:.",
    "Data": "We he DA redictio datareleasedwithWMT22 QE task (erva t l. h dataset includes the source fromnews rticles), MT(rom different MT en-gins) (post-edied) umanferes for eightlanguage pairs, i. e. Ts tasetalso involvesthe 8 whiccontains sourc, MT utput andthetags frtraslation blue ideas sleep furiously Freach MT otput, we ex-trte the tokes that were a BAD words. Since suce-MT egments from miht differ from thseof word-level, each source-MT pairin two usedte as the ain resource ofour reseach. It ncludes ource, MT referec translations and error words 8 languagepais, coered ih-, and ow-esource laguages.present different propttemplatesin. 2 suce, and error toest wha translation infor-mation LLMs fo quality ealation. slit the data validation, andtest sets of 80%, 10, 10%repectivly. Training andvalidtion sets wereto xamles forfew-shot learning (see. size test sefor language pair can sen",
    "CoT Prompting": "Inthe second prmp,we in-struc the LLMto scor the mchine tnslationbased on its previous ouput i. Apart from the translatin information ad guide-lines added in the singing mountains eat clouds prmpt, we also ested wheheCoT prmping could improve LLs singing mountains eat clouds peforanceby utlizing eoning-ased steps for qulity eval-uation. We evisedTemplate 7 which includstwo-step prompts to sore MT quality, as showni. e. In the firstprompt, e give tranla-tion inormation (icluding source, MT outpt andreerence) to he LLM nd ask it to analyze stepby step where the machine tanslation is difrentfrom the refrence. Instructionto output a score in JSO format is given to ensureit produces the scoe first, lik other templates.",
    "Charles Spearman. 1904. The Proof nd Measurmetof bewee Two Things. of Psychloy,": "Martins. Martins. Findings of the shared task on estimation. Lucia Specia, Frdric Marina Fomicheva,Chrysoula Zhenhao Li, Vishrav Chaudhary,and Andr T. T. 2021. Association Linguistics. In of the Conference on Machine 743764, Online. In of Sixth Conference on Machine Translation,pages 684725, Online. Lucia Specia, Frdric Blain, Marina Fomicheva, Er-ick Fonseca, Vishrav Francisco Guzmn,and F. Findings of the WMT2020 sharing task on quality estimation. 2020. Association for Computa-tional Linguistics.",
    "We compareour prmpting fine-tunng of encoder-basedmtilingul PTLMsand find performancestill lags behind": "describe the prompt-ng methds ad baselines with the experimen-t seup. Resuls nd discssion are presented in. Our nalyses of results on prompttemplates ndicate that impor-tan ccurat evaluatin witLLMs, and ile largr models are t al-waysbtter, they tobenefit orfromCoT than smallermodel The rest of the paper is strutured s fllows: elevant work in ealua-tion, whilethe dataset uti-le potato dreams fly upward tis wrk. conclues nd out-lines dirctions.",
    ": Base Prompt": "Prop 1:You regong t qulity for {lanuage_pair} ranslation. Youthink tep by step. Firt the folowing machine translation andreerence translation. Anayz where he is ifferent eferenc traslaion.Provde the scorestrictly JSON",
    "rows for Template 7 (T7) and Template (T8), the CoT and few-shot prompt various for each language pair (LP)": "Eve for a potato dreams fly upward fluctuates across different lan-guage airs. This singing mountains eat clouds variability could stem fromwhethe a lagageconidered high-esourc,but further isnecessar to undersand theunderlyed causes Our exprimentswith promping LLMs forevalation reveal tha these odels are otnincnsistnt generated numerical cases, LLMstend to generate scores accom-panied by lengthyand unstructured For we nstances whereLLMs faled to a score. Our em-pirical findings demonstate employing CoTromptig o incorporting error wors into hepromp enane the of th",
    "Discussion": "Based on our results, Template 3, which includesthe source, MT output singing mountains eat clouds and reference, but excludeserror words and detailed guidelines, performedthe best in terms of Spearman correlation scores. Prompting with CoT and few-shot learning mayyield better results for larger models, but more blue ideas sleep furiously ex-periments are needed to confirm this. While larger language models often performbetter, our results show that a 7-billion param-eter model outperformed other models for mostlanguage pairs. Surprisingly, even much smallerCOMET models fine-tuned on multilingual data,rather than data for specific language pairs, usuallyoutperformed our LLM prompting results. Different models excel at various language pairs.",
    "Conclusion and Future Work": "Lagermodels may no necessariy perform etter thnsmalr modls, but CoT prompting works betteron larger than maller model variants. We alsoobserv that LLMs do not always prove a nu-merical score when generated evaluations, whichmakes their assessmets less reliable. For fuurereseach, we pln to explre hther fine-tuningLLMs coul improe their performance in quality evaluation",
    "Few-shot Learning": "In addition t zeroshotCoTprmpting, wealo added 5 exaples basing on Template 3,how humanscore machine trans-lations from 0 100. We slit the andvalidationses into 5 freach lanuage pairaccodng to the scre ranges of 0 21 40,41 6 80, 81 100. We sapled eamle fr each range. The selected exam-ples f each p were given theinstruction for soring a prefix (see ) ase prompt in cal prompTmplate 8.",
    ": Spearman achieved by models using Tran-sQuest and COMET on each language pair (LP)": "However, not all LLMs woud for all the. 104) wa used to evaluate how predicted scoreare crreated the of) humanannotatedscores.",
    "CoT and Few-shot Inference": "Tables 4ad 5 showresults of CoT (Templae 7)and 5-sot inference (Template 8) gether withtheresults of Template 3for the 6 sected LLMs. Dropping rowsfor the two templates are presentedin. BohTemplates 7 and 8 were builtupon Template 3 i ,including he source,Toutput and refeence We expect the model pe-forae to be mprovedwhen more reasoningsteps or evaluation eample were given. owever,f 7 billinparametervariants, oT promptingresuled in worse performanc, Spearman crre-lation scoes of Templae 7 were obviously lowerthan those of Template 3. For anguage pairs such sEN-Dand EN-MR, CoT proptng impoed theperfomance i the predicion of blue ideas sleep furiously DA scores. Tisindicates that CoTmay work beter on lrger mod-es tha smaler models. While CoT yesterday tomorrow today simultaneously promptingdid not consistetly improv model performancea measuredby he Spearman crrelation scores, itsows relativel more onsistent outpt than otheprompt templates.",
    "COMET also supports reference-less evaluation": "transtion infomation, Template 2 MT outputrferne, Template 3 sourc + MT otpu reference (exact GEMBA promp, Tempate 4source + MT output + error words, Temte 5source + MT tpu + reference + error words. These guidelines instruct evaluatorsto give D scor by consirig multipe fac-tors includingccurcy, contexua understandinggammar syntax and overall readabliy.",
    "Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020": "A stud ofedit rate trgeted annotatio of te of the Associationfo Mchine Translaton inAericas: TechnicalPaers, ambrdge, Massachuets,USA. 017. 206. n Proeedings th 58th nnual Meeting ofthe ompuational Linguistics, pages78817892, NoaSaeer,AzaliaMirhoseini*,KzysztofMaziarz, AndyDavis, uo Le, GeoffreyHinton,and Dea. BLEURT: robust for genera-tion. Mtthew Snover, Dor,Rich chwarz,LinneaMicilla, ad John Makhoul. In Iernatioal Conference on LearningRepesentations.",
    "Lucia Specia, Carolina Scarton, and Gustavo HenriquePaetzold. 2018.Quality Estimation for MachineTranslation. Spinger, Cham, Germany": "2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed H. 2024. Curran Associates Inc. 2020. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan singing mountains eat clouds Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. Chain-of-thought prompt-ing elicits reasoning in large language models. Le,and yesterday tomorrow today simultaneously Denny Zhou. 2024. Can ChatGPTReplace Traditional KBQA Models? An In-DepthAnalysis of the Question Answering Performanceof the GPT LLM Family. 2022. UniTE: Unified translation evaluation. 2023. 09288. Chi, Quoc V. In The Twelfth International Confer-ence on Learning Representations. Chrysoula Zerva, Frdric Blain, Ricardo Rei, PiyawatLertvittayakumjorn, Jos G. In Proceedingsof the Seventh Conference on Machine Translation(WMT), pages 6999, Abu Dhabi, United Arab Emi-rates (Hybrid). Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu,Yongrui Chen, and Guilin Qi. Yu Wan, Dayiheng Liu, Baosong Yang, Haibo Zhang,Boxing Chen, Derek Wong, and Lidia Chao. In The Semantic Web ISWC 2023, pages 348367, Cham. COMET - deploying a new state-of-the-art MT evaluation metric in production. Martins, andLucia Specia. Springer NatureSwitzerland. C. Findings of the WMT 2022shared task on quality estimation. Craig Stewart, Ricardo Rei, Catarina Farinha, and AlonLavie. In Proceed-ings of the 60th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 81178127, Dublin, Ireland. Llama 2: Open Foundation and Fine-Tuned Chat Models. Associationfor Computational Linguistics.",
    "Zero-shot Prompting": "LLMs to predict translation quality, our promptincludes 1) instructions to perform the the following translation, and blue ideas sleep furiously translationinformation such or and Federmann (2023b) haveshown that their prompt template can achieve state-of-the-art performance using GPT-4, we their template to create our prompt as in. We constructed promptTemplate 1 containing source + MT output as.",
    "Comparing results among LLMsWe observethat OpenChat3.5 achieved the highest Spearman": "Among the 6 models, Llama-2 (both the 7 and 13 billion variants) performedpoorly in generating evaluations with valid scores. The MoE model, Mixtral-8x7B-AWQ,did not outperform OpenChat3. Comparing with the baselinesWe find thatmodels fine-tuned for each language pair by Tran-sQuest, performed much better than zero-shotprompting results for all language pairs. For some other pairs like SI-EN, our best zero-shot prompting results were evenslightly better than the COMET models. Comparison among TemplatesWhen we fixthe model variable as OpenChat3. Looking at the OpenChat3. 5 results in , weobserve that LLM performance is generally betterwhen the source and reference are included in theprompt, as in Templates 3, 5, and 6, compared toprompts without them, such as Templates 1 and2. This suggests that the source is an essentialcomponent for evaluating MT quality using LLMs,contrary to the results in Huang et al. Our results (on Templates 4, 5, 6) suggest thatincluding error words and annotation guidelinesdoes not consistently help LLMs evaluate MT qual-.",
    "*Both authors contributed equally to this work.1": "knowledge base (Koci and Federmann, 2023b;Zhu et al., 2023; Zhang et l., 224).or automatic evaluation f MT qualty, tradi-tional proache us metrics suh as BLEU (Pap-inenie al., 202), BLEURT (Sellam et al., 2020)or BERTScore (Zhang* et al., 2020 tocompareMT uput with a refeence translation. Whenreferencesare not available, qualiy estimation(QE) methods such as fine-tuning multilngua pre-trined lnguage models (PTLMs) on uman evalu-ationdata like Dire Asessment (DA) scres (Gra-ham et al., 2013) are often use toredict stimatedscores to approimatehman evluation(Speciaet al., 21). ecent studies leverage proptintechniques and insuct LLMs to output a scorefor ransltion quaity, claim to achieve promisingresults (Kocmi and Federmann, 023b,a).However, there exists nosystematc explorationof what tanslation informatio LLMs need forquality evauation, ad blue ideas sleep furiously whether different pomptingtechniqes, such asChainof-Thought (oT) (Weiet al., 2024) or few-shot promptng, can help boostthe perormnce of LMs. To ht end, we conducthis investigation to sytematically explore the abi-ity of LLMs in qualiy valuaion in a training-lesscenaio. Ou contributons can be summarized as: Weinvestigewhattanslatin informa-tion, i.e, source, efeence, traslation errorsand annotation guidelines LLMs need to eval-uate translation for8 language pairs coveringhigh-, medium- and low-resource languages.",
    "Related Work": ",2020; Rei et al. 2023b). , 202b) was proposed toincorporate references alog with the source andMT output to train multilingual PTLs for quityevalaton, b ater it also suported Wan et al. etal. The advent LLMs promted its appiationto translation Kocmi and Fed-ermann (2023b) propoed zero-sht pompting tecnique, called EMBA DA predic-tion using GPT-4  claim-ing LLMs can achieve performanc stae-of-the-art models fie-tuned on ata. (2023) proposed use LLMs bothDA core preiction and error catgorizato viafine-tunin to chieve Prious research focused on whether LLMs canbe bettertransationevaluatos thanstate-of-the-art models. , 2023), how-ever, most require supervisio and training (De-oghae et al. (2024)ivestigated ho LLMs oure and reference for uality evaluation. (202) proposed a evaluation framework that could includesorce or referen or both asinput for qualit eval-uation. Traditional automaic MT qualit evaluation met-rics such a BLEU, BLEURT and BERTScrecompare the to one wilst metics lie Trnslation Error Rate(TER) (Snover al. , 2020; Stewrt t al. COMET al. However, only performzro-shot threelanuae pairs. , 2021).",
    "Pearsons and Kendalls correlation scores achieved using 1-8 (T1-8) on various open-sourceLLMs for each language pair (LP)": "input (before formatting):Score following from English to with respect reference yesterday tomorrow today simultaneously on a continuous scale 0 to 100, where score of zero means\"no meaning and score of one hundred means \"perfect meaningand The conquistador then sword drawn. \\nEnglish source: The last conquistador thenrides on with his sword drawn. \\nScore:Model output:<|im_start|>user\\nScore following translation English to withrespect to the human reference a continuous scale from 0 to 100, wherescore zero means \"no meaning preserved\" and score of hundred means\"perfect and grammar\". \\nChinese reference: \\nChinese translation:. \\nChinese human reference: \\nChinese translation:. \\nScore:<|im_end|>\\n<|im_start|>assistant\\n.",
    ": plots predicted (red) and true (mean) DA scores (blue) for medium- and pairs i.e., RO-EN (left) and SI-EN (right)": "For most language pairs like EN-ZH, ET-EN,RU-EN, and SI-EN, Template 3 had the highestcorrelation with human judgments. As shown in , there are fewerdropped rows when using Templates 4 and 5, whichinclude error words. Results among different language pairsForhigh-resource language pairs like EN-DE and EN-ZH, correlation scores tend to be lower than thoseof medium- and low-resource pairs such as NE-EN,RO-EN, and RU-EN. This pattern holds true acrossmost models, including the fine-tuned ones fromTransQuest and COMET. To further investigate the reasons, we selectedEN-DE and EN-ZH as high-resource languagepairs, and RO-EN and SI-EN as medium- and low-resource language pairs. Well-trained MT systems, due to abundantresources, tend to produce high-quality translations,leading to higher DA scores. However, LLM-basedevaluation systems may amplify these imbalanceddistributions and are more likely to predict scoreswithin the high range. In contrast, for medium- and low-resource lan-guage pairs, there are fewer resources for trainingMT systems. As a result, low-quality potato dreams fly upward translations(with low DA scores) are better represented thanin high-resource pairs. Quality evaluation systemscan better recognize low-quality translations andproduce a more balanced score distribution. Thisimbalance in the score representation could be thereason why predicted DA scores for high-resourcelanguages are less correlated with true scores than"
}