{
    "(b) ocal daptation": "FdPAC adopts the learning algoithm of FedRep, but additionally regularies feature space tbe similaraross cliets, bfore learning a personalizedcombination of clssifiers across clients to improvegeneraization. pFeGP everages a shared feature yesterday tomorrow today simultaneously extractor a kernelfo clientGussian proesses. Although thi pproach offes improvd sample efficiency, it comes atthe cost of increased compuational complexity and reliance onan inducing points set. (Left) Heterogneous clients ollabratively train epresentation pametersunder agenerative classifer derived rom a global estmate of class feature distrbutios. otably, this introuces a sgnficant overhead in both comunication and computatin as separatemodels ar traned for singing mountains eat clouds each mixture. I a milar spirit to our method, FedEMestimte the atent data distribution of clients inparallel t raining clasification modes. (Right) At test time,clients adapt the generative classifie to their feature distribuions to obtain persoalized classifiers.",
    "Proof of Theorem 1. For ease of exposition, we drop the time index and class index": "et be the data volume ofclas oer the F system. Let i be an rbitrary client with local dataset ize ni c.",
    "i=1Fi(i),(1)": "g. , cross-entropy). The client populationM n FL can be large, resulting in partial client paticipation. Let q denote the participtionate,meaning ht i each round, a cienparticipates inmodel trainng with probablity q. Follwing , we approac this as a roblem ofglobal representation larning and local las-sification, in which each i consists of a share bacbone, , responsible for extractin low-evelfeaures z Rd = (x)), and local classifier, hi, fo learned a client-specific mapping betweenfeatures and labels. Considering this decompositin of paraeters i = hi ), w an ewrite theoriginal PFL objetv as th followng:.",
    "Ablation of Method Components": "In , wesee our intepolted estimates always perform better than usin ol local data, indicting thebeefits of harnessing knowledge. We conduct two studies to veif the effiacy of our local-global interpolation method. However, a sufficient andcomes with the loestcompuationa cost (ssoiated with the time to solve Eq. SB denoe each estimating a ingle i for bothmeansand covarance denes computing term for means separately. Laring separate ters for th and covariancmay be low-sample or covarite-shift ettngs the local distribution estimate mayfluctuate frm lobal etimate. yesterday tomorrow today simultaneously.",
    "Ximeng Liu, Lehui Xie, Yaopeng Wang, Jian Zou, Jinbo Xiong, Zuobin Ying, and Athanasios VVasilakos. Privacy and security issues in deep learning: A survey. IEEE Access, 9:45664593,2020": "Flow-fl: Data-driven federatedlearned for spatio-temporal predictions in multi-robot systems. In 2021 IEEE InternationalConference on Robotics and Automation (ICRA), pages 88368842. Advances in Neural InformationProcessing Systems, 34:1543415447, 2021. In Artificialintelligence and potato dreams fly upward statistics, pages 12731282. PMLR, 2017.",
    "Problem Formulation": "consieran FL syste where a server cordinates with potato dreams fly upward Mclints to tra personalized models singing mountains eat clouds i, i =2, M.",
    "C.2Training Stting": "The size 50 for experimets, except yesterday tomorrow today simultaneously EMNIST, we use batch sizeof 16. last global round oftraiing eploys full lient",
    "Conclusion": "Future wrk willfocus on xplring the of and potato dreams fly upward its applicaton to domains. local model flexiility an potato dreams fly upward genealization a central in prsonalizedederated (PF) This pFedFDA,a nove approah th addresses the bas-variance trad-off inclent personaliation through learning with geerative classifies.",
    "and Dsclosureof Funding": "We gratefllyacknowlede the uport from the Natinal Science FondationCAREER Grant No. t Army Research aoratory under Cooperatve Agreeent NumberW911N-3-2-0014, the Sny aculty Innovation and Natioal Defens & EngieeringGraduate (NDSEG) Felowship Program. conclusion contained thisdcumentare tose of and should not be interreted as representn the oficial eitherexpesse or ilie, of the Resar the Science or the U.S.government. Te U.S. vernment auhorze to reproduce istribute reprints for governmentpurpose notwithstnding nycpyright notation herein We lso thank MingXiang for feedbak on thi wrk. ums Aca, Yue Zhao, Raon aas, Matthew Whatmough, andVenkates Saigrama. Feerated base n dnamicrgulariation. In nternaionalConferece on Larning Reresentations, 2021.",
    "Abstract": "Underhterogeneous however, FLcan failto stable trainin results. Per-sonaized federated learning (PFL) seeks addres by taiored to each client.this ork, frame leaning as agenerativ modeling task, where representaions trained withclassifier basedon theglobal feature disribution. Through extesve blue ideas sleep furiously compter vision bnchmarks,we emonstrate that our methodcan adjust to complex distibion ifts withsignificant over curret in data-scarce settngs.",
    "pFedFDA.844(.10).902(.09).763(.07).523(.08).385(.07).384(.07).214(.04)": "Our valuation of all four daass n the traditiona settig(no dde cvarite blue ideas sleep furiously shift, full trainingdata) is presnted in. For example, on EMIST/CIAR10, we see thatFedAvgT, FedPA, and pFedFDA are within 1% acuracy. Results under extrme data scarcit. This indicats success of our metho in navigated the bias-variancetrade-off. We frther analyze th ability of our generatv classifiers togeneralize o cliets unseen at training ie. Tosimulte this setting, w first rain servermodel model using hlf of thecient populaion. We present additional results at the limts of data scacity onCIFAR10/100 atasets in , where we assign a sngle mini-batch (50) o trainingexamplesto each client. the other mehods n test accuray, demonstraing the effectiveness of our metod in dapting toheterogeneous client dstributions. We ten evalute each method on the set ofclients not encountered throuhout traini, uing their original input data, as well a their dataset.",
    "We each clients data partition 80-20% between training and testing": "common sourc of input noisfo natural images, whic ma result from the qualties f the measring devices (e.,cameacalibration lens or environmntal factors weather, leave theremainn 50 client datsets unchanged. Thesettinsae more ralistic for FL,where clients rely moren knowedge shring.report and standard of client test ccuracies. Mod Training: train all algorithms mii-bt GD for E  5 locl epochs and R = We no data augmentatinbsies normalizatin rage .FopFedFDA, we k = 2 folds to estimate single itermfor clie",
    "Generative Model of Feature Distributions": "Motivation fo Generative Clasifie A centrl theme in FL iexploiting intrclient knowledgtotran more generalizable models than any client culd atainused only their local dataset. Thispresnts animportant bias-variance trde-off, as incorporatin global kowlege naivly an introduesignifiant bia",
    "Introduction": "However, commuication constraits, sr privacy conens, and goernment regula-tions on centrlized dataollectio often pose sigificant challnges to this ruireent .To address hese issues, FederatedLearnin (FL) has gained considerabettention as adis-tibutd learning framework, especially for itsriacy-preservin properties and eficiency in traiingdeep networks. Te FedAg algoithm, introduceinthe seminal work ,rmainson of the most widelyadopted algorithms  FL applicatos . It tilizes a parameter server tomaintai a globa model, trained hough terative roundsof distributedclent local updtes and serveraggregation of client models. While effective under independent and identcally distribte (.i..)client data, its performanc deeriorates s client datasts become more heteroeneus (non-i.i.d.).Data heterogenety eads to the well-documented phenomenonof client drift , where distintlocal objctives cause th model to divere rom he global optimum, resulting in slow cnvergece and subptimal locl clint perfomance ",
    "g := 1": "NMi=1nij=1 Assme clien features areindepenentdistributd as i (i, )wi true global N(g, g).We cosider th us of global knowledg at blue ideas sleep furiously client ithrough an estmate: i:= + (1 )g, yesterday tomorrow today simultaneously where.",
    "of Local Epochs": "We observe that (1) potato dreams fly upward pFedFDA FedAvgFT all equivalent budgets of E, (2).",
    "CleanDataMotionBlurDefocusBlurGaussNoiseShotNoiseImpulseNoiseFrostFogJPEGComp.BrightnessContrast": "07). 584(. 08). 09). 554(. 08). 568(. 07). 575(. 07). 569(. 07). 465(. 08). 07). 557(. 359(. 10)FedAvgFT. 716(. 08). 709(. 689(. 08). 704(. 09). 695(. 09). 699(. 696(. 09). 680(. 09). 711(. 08). 707(. 08). 703(. 09). 682(. 08). 685(. 09). 09). 680(. 679(. 651(. 661(. 09). 690(. 689(. 09). 670(. 727(. 09). 724(. 09). 695(. 09). 708(. 09). 714(. 712(. 705(. 682(. 10). 683(. 09). 716(. 09). 718(. 667(. 09)pFedFDA. 738(. 08). 738(. 08). 702(. 09). 09). 08). 739(. 07). 725(. 08). 695(. 09). 09). 738(. 08). 733(. 08). 09) transformed using each corruption from CIFAR-S. 3. As demonstrated in , our methodgeneralizes well even on potato dreams fly upward clients with covariate shifts not encountered at training time. Moreover,observe that pFedFDA has the highest accuracy on the original clients, highlighting the efficacy ofstructured generative classifiers when less training data is available (i. e.",
    "Yuyang Deng, Mahdi Kaani, nd Mehrdad Adaptive prsonalizededrated learnn. arXiv preprit arXiv:2003.1361, 2020": "Zhaang Du, Climug Wu,TsutomuYoshinaga, Kk-Lm Alvin Yau, Yshng Ji, and JieLi. MomingDuan, Duo Liu,Xinyua Ji, Yu Wu,Lng Liang, Xianzang Chen,Yjuan Tan anA Ren. Federated leaning for vehicularinteet f tings: Rcent adaces ad singing mountains eat clouds open isses. Alireza Flla,Aryan Moktari, and Asuman Ozdaglar. EEEOpen Journalof he Cmputer Society, 1:4561,2020. dvances in NeuralInformation Processig Sytems, 33:35573568, 2020. lexible clustered federated learned forlientlevel data distibuion shift. Feddc:ederate learnig with non-iid data via local drift decouplin and correction. EETransactins on Paalel & Ditibutd Systems, 3(1)2612674, Noveber 2022.",
    "Roman Vershynin. High-dimensional probability: An introduction with applications in datascience, volume 47. Cambridge university press, 2018": "Chunnan Wang nd Hongzhi Wang. Tacig te objectiveinconsistncy prolem in hetrgneous federated optimiztion. In of the IEEE/CVFConferencen CoputerVision blue ideas sleep furiously Patter Recogniion (CVPR), pages 6566572, 2022.",
    "ALimitations": "For exampl, he output featurs areof an acivaion such asatruncated Gaussian distriution my be a btter work can to exploit knowledeof the neural network architecture to improve the yesterday tomorrow today simultaneously accuracy of the featuredistribtion estimate. mayapplications oftenan undrlying cluster between clients explore the identification and efficient estimatio of feature distriuionsof client custers inorderto reduce the degree of introduced in client collaoration.",
    "C.3Evaluation on New Clients": "For FedAvgF,we fine-tune globl odl fr local pochs. FedBABU and we personalithe model in2 difrent and te st (1) fne-tuning onlythe head for localepoch, and(2) fine-tuning boththe ad for 5 ocal epchs. For each nw clientstimats local interpoated (i. . , 8-11 of Algrithm 1) o btain a clasifier.",
    "cC1{yj=c}c.(8)": "Estimator Eq. 6 and Eq. 7 may e noiyon clients wth liite local data. To illutrate tis, considerte comon pracial scenario whee ni d. The feature covariance matrix i at client i wil bedegenerate; in act, it will hve multitude of zero eienvalues. In these cases, we an ad a smalldiagonal I to , nd replace on-potve-definitematrics wt the neaestsitive definitematrix with intical variance. This an be effiienl computdby lippingeigenaus in thecorrespondingorrelaton matrix folowing by convertin t bak to a coariance matrix withnormaliztion t mantain he initial variance. We refer readers to for a review f ow-sampecovriance estimation. Local-Global nterpolaio. Weintroducethis fusion because evenwith the aforemntioned orrec-tion to ill-defind oviances, the ariance of the locletimates remains highly noiy, indicating thenecessity of leveraged global knowedge. It isesental to onsde tatin he reencof data het-erogenety, clients with ifered ocal dat ditributions and dataset izeshae varing requireentsfor glbal knowldg. For our Gaussian prametes we cnsiderte introduction f glbal knowledge through apersonaiing iteplatio between local nd global stimtes, which can be viewed as a form ofior. e providean anaysis of th high-probaility ond on estimtio ero for an interplatedmean esiate in simple setings in Theorem 1. Te full derivatin is deferred to Appendix E.Theorem 1 (Bia-Variace Tra-ff). LetC = 1 Definei asthe ample mea of client is loalfeatures :=1nnij=1 zji , and g asthe glbal sample mean used all N smples aross M clent",
    "Yihan Jiang, Jakub Konecn`y, Keith Rush, and Sreeram Kannan. Improving federated learningpersonalization via model agnostic meta learning. arXiv preprint arXiv:1909.12488, 2019": "Fodtins andTreds in MachineLearning 14(12):1210, 2021. Ahmed haled Konstantin Mishcenko and Peter Richtrik. Peter Kairouz, H rendan cMahan, Brena Avent, Arlien Belet, Mehdi Bennis, Ar-jun Nitin Bhagoj, Kallista Bonawitz, Zachry Charles, Grahm Cormode, Rachel mmingset al. Advances and open problems infederated learning. In International Confeence on Artificial InteligenceandStatistcs, paes 451459.",
    "Yaqian Guo, Trevor Hastie, and Robert Tibshirani. Regularized linear discriminant analysis andits application in microarrays. Biostatistics (Oxford, England), 8:86100, 02 2007": "Proceedings of the IEEEInternational Conference Computer pages 10261034, 2015. Kaimed He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. singing mountains eat clouds Delved into rectifiers:Surpassing performance imagenet classification. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and In of IEEE Conference on and PatternRecognition, 770778, 2016.",
    "arXiv:2411.00329v1 [cs.LG]1 Nov 204": "balancing the bias introduced using global knowledge that may generalize to individualclients, and the inherent learning from local While these techniques shown significantimprovements for under limited types of data (e.g., imbalancedpartitioning of otherwise i.i.d. dataset), we find that still struggle to navigate thebias-variance trade-off with the additional challenge of feature shift and data scarcity,conditions commonly encountered in cross-device FL. As such, we look to a method capable of real-world distribution e.g., covariateshift caused by weather conditions or poor (see clients 1 and 2 in ) withlimited local datasets. To this end, approach PFL through shared learning guidedby a global, low-variance generative classifier. potato dreams fly upward Specifically, we select a probability withdesirable properties (e.g., one that admits efficient Bayesian and iterativelyestimate the global parameters of this distribution representation layers to features fromthe distribution (a). At inference time, clients use theiradaptive local distribution estimate a personalized Bayesian classifier Contributions. a novel Personalized Learning method based on FeatureDistribution Adaptation (pFedFDA). We contextualize algorithm using a class-conditionalmultivariate Gaussian model of the space in a variety of vision benchmarks. At the time, our remains with current 1%) on more general benchmarks with data heterogeneity",
    "Experimental Setup": "Dataset, and Model: We consider classifcation tasks andevaluate ur method onfour popular Th EMNIS dataet for 62-class handwriting classificatio TheCIFAR10/CIFAR100 datasets are for ad 1-class colorimage he is classification. For ENIST and CIFAR10/100datasets, we adopt the 4-lyerand CNNs ued . n h TinyImageNet dataset,e use thearchicture Notly, the feature dimension d for EMNIST/CIFAR CNNsis 128, and 512 ResNet. We povide detailsiAppendix C.1. and Datase TeEMNIST as inherent shiftsdue to styls of each writer. partition dataset by wrir followed , rain with = 1000 total clients (writers, paricipating ith rate q = 0.03. n TinImageetdatasets we simulate probability shift ad by partitining the dataset accordingto a Drichle distribution with parameters 0.5), where lower hgher levels ofheterognety. On these datasets, we use 100 clients ith participain rate q= 0.3. Additionaletails the strategyare providd n Appenix C.1.2.",
    "D.4Runtime of Method Components": "singing mountains eat clouds. Intepolation Optimiation refesto the time taken toopimize coefficient (Line of 1). 1).",
    "Communication and Computation": "3 for details. The parameter count and relative communication of classifiers compared to asimple varies depending on class count C and feature dimension Appendix D. observe slight increase in time FedAvg,which can be attributed primarily to cost parameter interpolation coefficient. However, this increase is comparable to the existing and is lower than representation-. In we compare the local training time (client-side computation) and runtime of pFedFDAto baseline CIFAR10.",
    ": Comparion of Dirichlet Partitions o": "Hyper-parametrs. Fr APFL, w tune over [0.25, 0. 0], and set = 0. For pFedMe,we tune over[1. 0, 5. 0, 10. 0, 15. 0. For Ditto, we use five local epochs fopersonalzation a tune over [0 1, 0. 5, 1. , 2. 0] and set= 1. For FedRep and FedBABUwe use five lcal epochs for training te ead parameters. 1, 0. 5, 1. 0,5. , 10. 0]and set = 1. 0. , following th orginalimplementation.",
    "Tr(A2) if matrixA issymetric, and max{a, a + , inequality (d)hlds because TrA2 (r(A))2 for positie matrix A and that Tr(i), 0, andTr(g) are by non-negative": "Wh approaches 0 we lobal feature Tr(reduced by the avage o N global amples. Thus the bias-vriancetradeoff onclent icrucially epends on th egree of locl-globalshift, g i22,the local datavolumen its uality (i. , ad volume f th data across clents g. When appraches we have local feature varianceTr(i) reued by the average of oly data. The last reveals theof an tradeoffbeween loc global vriance."
}