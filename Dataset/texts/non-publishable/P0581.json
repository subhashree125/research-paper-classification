{
    "E.1Dataset": ", before the frsttime the yesterday tomorrow today simultaneously gaeleaves the word). The aricles have been carefully constructed to contan the same content acros languag Wor-level eaded timeis recode fr between 3254 participants per language, usin severaldifferentreading variables. In ou xperiments, we give a reading time of zeroto wors that were kipped onthe first singing mountains eat clouds pass. We ake the averageover theby-subject readingtimes to obtain th response vriables wemodel.",
    "n=1(H(cn, un) H)(H(un) H),(22)": "(14). Inwords, this means that we only use the present in the training data for approximatingthe in Eq.",
    "Victor Kuperman, Raymond Bertram, and R. HaraldBaayen. 2008.Morphological dynamics in com-pound processing.Language and Cognitive Pro-cesses, 23(7-8):10891132": "Tatsuki Kuribaashi, Yohei Oseki, Ana Brassard, ndKentaro Inui. 2022. In Proceedingsof Coferenc on Empirical Methods nNatral Language Processing, pges 104210436,Abu United rab Emirates. Asociation forComputational Linguistis. Tatsuki Kuribayashi, hei Oski, Tkumi yoYoshid, Asahara, and Inui. 02. for Lnguistics.",
    "with Model Size": "The results presented article may help explaina trend the computationalpsychoinguitics literature: Surprisal of larger povide it to human compared to those of medium-size model (Oh and Schuer, 2023a,b) O et al. (2024)suggest that th is larger moels are incredibly ccrate at rare word in cntet. Medium-sized models the other hand, are asgood at predictngrare words in conext. e.  non-ontextul Similarly, this be reason why surprisal derivd from lossy contexts have beenshown to be predictive of reading (Futrel et al. , Kuribayashi et",
    "Disentangling Effect of Context": "singing mountains eat clouds. , 206), e argue that the iteres of sur-prisa thery liein blue ideas sleep furiously wat is fcontextual information beynd fre-quency. As there is alarge and established body of workhowing that frequency a maor role in effort it takes to process words (see,e. expositionabove impli that eithersuprisal or should receive status measure of theeffect of ontet.",
    "Predictive Models of Reading Behavior": "We seek to model the cognitive processing diffi-culty of a unit u, e. g. In general terms,we are interested in empirically assessing sometheory of cognitive processing difficulty, which canbe thought of as a collection of unit-level propertiesthat are implicated in determining processingeffort as measured by reading times. We define a predictor function as a functionof type X: RD, i. e. , a function thatmaps a contextunit pair to a D-dimensional realvector. We model the reading time measurementsas a linear model f conditioned on X(c, u), i. ,r(c, u) f ( | X(c, u)) where RD is areal-valued parameter vector. A model whoseexpected value, singing mountains eat clouds r(c, u) = X(c, u), achieveshigh likelihood on held-out data lends empiricalsupport to the theory that the factors measured bythe predictors in X underlie the process of reading.",
    ": This figure is analogous to , except that it shows results when excluding the word length predictor": "othr single variable, and that surprsal ad PM tend to result in hiher llhthan orthogonaied surprisal. We also howresuts for raded ime metrics ther than gae duration, icludingfirst fixationduration (top rw) and total reading times (bottom row). We cnclud that all hreeimpementtions of cotext are equally good at predicting reading times. In , we show ourgeneralized additive modelig results, broken down by language, across thex-facets. Wefind that of the four indiidual predictors, frequency leds to higest llh,fllowedby surpisal, blue ideas sleep furiously PMI, and then orthogonalizd variants. This is to be expcted. When cmbining our non-contexal redictr(frequency), alongside these contexual predictors, we do not observe diffrences i llh.",
    "Claude E. Shannon. 1948. A mathematical theory ofcommunication. The Bell System Technical Journal,27(3):379423": "Transactions the Association for Compu-tational Linguistics, 12:5879. Noam Siegelman, Sascha Schroeder, Cengiz Acartrk,Hee-Don Ahn, Svetlana Alexeeva, Simona Amenta,Raymond Bertram, Rolando Bonandrini, Marc Daria Chernova, et 2022. BehaviorResearch Methods, 54(6):28432863. Expanding cross-linguistic research reading: The corpus (MECO).",
    "Jsiah Willard 1902. Elemetar inStatistical Mechanis. Charle Sribnes Sons": "2024. Mario Gilianelli, Andreas peal, and Ryan Cotterel. Adam Goodkid and Klinton Biknell. yesterday tomorrow today simultaneously nProceedigsof the 8th Workshop on onitive Modeing andCom-puatinal Linguistics (CMCL 2018), pges 1018,Sal Lake City, Utah. blue ideas sleep furiously In Find-ings of the Asociation for omputational Linguistics:EMNP 2024, Miami, Florida, USA.",
    "The subscript H suggests that pH is the human LM": "Thisassumption holds true blue ideas sleep furiously in practice due thesoftmax function (Boltzmann, potato dreams fly upward 1868; Gibbs, enforces the probability estimates to bestrictly. 2023).",
    "predictorFrequency (Frequency Orthogonalized Frequency)Context (Surprisal)": ": singing mountains eat clouds This figureiaalogous to , the differencethat frquency is projected he orthogonalcmplement of surprisal, in an frequency predictor",
    "Robyn Speer. 2022. rspeer/wordfreq: v3.0": "Tsipidi, Franz Nowak, Ryan Cotterell,Ethan Mario Giuianelli, and AlexWarstadt Associ-ation for Computtional Linguistics. Ethan Golieb Wilcox, Jon Gauthir, Jennifr Hu, Roger P. Lev. n predictvepowerof neural anuage models real-tie mprehensio behaior.",
    "DOur Method Relation to Residualization": "The technique presented in 3 closely resembles another method used to decorrelate predictorsresidualization (see, e. g. , Kuperman et al. , 2019). Consider a linearregression setting with response y RN and design matrix X RN2, i. e. The idea behind residualization is to decorrelate thepredictors by replacing one of them, say x1, by the residuals obtained from the ordinary least squaressolution of the regression model in which x1 is the response and x2 is the (only) predictor. The newpredictor will thus take the value.",
    "x2x2x2.(34)": "(34) provides estimatesof residual values which yesterday tomorrow today simultaneously can, be generalized to data beyond of N data points fromwhich they were yesterday tomorrow today simultaneously estimated. (14) had the inner product between two the Hilbert space H, but here have theinner two vectors in Euclidean space and x2), is the dot product. Indeed,residualization only over a sample of data points. (14) computing their exact value would be intractable. From a statistical Eq.",
    "Limitations": "Or apprachtae one preictor to remainutouche (i e , frequency), modfe othersto that ar disasociatd from thefrst. It turns ot that even attributig the haredeffect of and surprisal reading tiesto surprisalwhic is he frequency by anorthognlized frequencyprdictorthe variance explaining y frequencypredictor still hier for most laguages in com-parison to th surprisal predictor. This ives urconclusion context appeas tolay a smll role i reading time predicto. our presentaton of ideas addscussio largely ignoeseffect of could control this byadding n thenumber of saccads wthin a wod asinto models. We are nawre of any efficient alorithm tocompute IH, YH andYH, YH exactly, so inpratial we must rely on g. , s als, bu given theirconsstency aross we assumed theewere typos. retaind the cold have in responses.Inaddtion, urre papermakesof afed-effects linear regressin with aerageddata, asto the mor standar egression. Estimaton of R2 values rommixed-effe modelscan depending assuption and has historically beeunder-rported due to this (akagawaand 2013. Future research should the fea-sibilityour with the partial ffct (i. e. Anoher limtation of this yesterday tomorrow today simultaneously is while einvestigae everal languages, are til biasedowads Indo-Eurpen toeven langages woldfurther the impact f is work. Future work coducsimia analyses ondifferent oea dtasets to de-temine wethe this renis a just pariclar Kora language",
    "Byung-DohOhandWilliamSchuler.2023a": "Assocatin frComputationalLinguistics. hWliam Schuer. Traformer-basdlanuagemodelsurprisalprdict reading times best with abouttwo training tokens. blue ideas sleep furiously 2023b.",
    "Model C:y = C0 + C2 x2 + ,(37)": "where RN is a vector of noise variables. is, the of residualizdprictorx1 inthe aboveremains the same after rsiduaization(Mode A vs. effect of x2 Model B yesterday tomorrow today simultaneously ecomes equa to whatit would have been under a model witha single reicor, regressing only on 2 (Model In other words, we wantthe between frequency and urprisal to to frequency, as i in regresses time on freuency (crresponded Model C. Moreover we argue thatcoeffcient swrong quantity look at when easurng imprtanceit he role theprector i relation to the others that should matter.Anothr remarkrelateso whether a residualized is interpretable a anythingat in blue ideas sleep furiously we areaddressing the queston ofhat predictr shul bein place, i. , from anguage model as stand-in for conext. paper that rthogonalized srprisal gives a better intrprtation for effect of contextthanconextual surprisal does. We hoe that our workand his discussion an help shed frthe light residualizationis, and isnot,",
    "H(c, u)def= log pH(u | c).(8)": ", 224),justifing of lnear regression. Importanty tere ividee th particular funtional relationsip,cled linked between cnextualrprisal eaded is affine5 (Sit andLevy, 2013;al. Indeed, claim has receiving muc empirical sup-port 2001 emberg and Kele 2008; Smthand 2008, nter lia). , 2023; Shain et al.",
    "Conclusion": "Thi article iscuses predictors hat capture processing effot of unit is singing mountains eat clouds by itscontext",
    "Byung-Doh Oh, Shisen Yue, and William Schuler. 2024": "In Proceedings ofthe 18th Conference of the European Chapter of theAssociation for Computational Linguistics (Volume 1:Long Papers), pages 26442663, St. Colin Raffel, Noam Shazeer, Adam Roberts, Kather-ine Lee, Sharan Narang, Michael Matena, YanqiZhou, Wei Li, and Peter J. 2020. Liu.",
    "'cr', k = 6) + s(prev_frequency, bs = 'cr', k = 6), data = .)": "We delta lg-likelhood yesterday tomorrow today simultaneously average diffrence in ikelihood he targe modelsmentioned above te baseine stimatd over ten fold of crss-validtin acos severaldifferet ses of predictor variables.Results are visualized . We observe big trends: First we that predictors ead topositive lh, that theyare useful for predcting times. second, when lookingat models with yesterday tomorrow today simultaneously just one predctor variable, we that frequency to higher any",
    "E.2Predictors": "e. For each word in MECO, we sum the surprisals estimating mGPT for each ofthe tokens that up that word. (10b). All predictors are standardized (i. ,unigram surprisal) and length, following work. Finally, PMI estimates yesterday tomorrow today simultaneously are from surprisaland frequency through Eq. Our predictor variables are the following way: estimates obtained from mGPT(Shliazhko et al. , zero and standarddeviation one) before computing orthogonalizing surprisal values and fitted the regression models. estimates from Speer (2022) to word frequency (i. e. , 2024), a variant of GPT-3 that trained on Wikipedia and theC4 (Raffel et al. It supports 61 which intersected with MECO dataset yields11 languages for our experiments: Dutch, English, Finnish, Greek, Hebrew, Korean,Russian, and and Norwegian, which are present in MECO, are unfortunatelynot by mGPT. , 2020).",
    "YH, YHYH.(14)": "306-). C Ifthe valus of at least one X nd  is 0, which can be achived ba simle man-centerng transformatio, then. Projecing thi manner resuts an othogonal-ization in the sense projYH = 0as a consequence of theHibert pojection theorem(Rudin 1991.",
    "Abstract": "singing mountains eat clouds We present a new perspective on how readers in-tegrate context during real-time language com-prehension. build on that the processing effortof linguistic (e. , a word) is blue ideas sleep furiously an affinefunction in-context information Anotherone pointwise mutual information (PMI)between a and its context, which turns outto yield the same predictive power as surprisalwhen controlling for frequency. This means neither PMI nor sur-prisal contains information about alone. Our experiments show that the pro-portion of in reading times context is a lot when context is rep-resented the orthogonalized predictor. Froman interpretability this indicates thatprevious studies have overstated the rolethat context has in predicting"
}