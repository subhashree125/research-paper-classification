{
    "Zhengyan Lin, Liu, Li, Maosong Sun, and Jie Zhou.2021. Moefication: Transformer feed-forward layers are mixtures experts.arXiv preprint arXiv:2110.01786 (2021)": "Rostamizadeh, S. DistServe: Disaggregating Prefill and Decod-ed for Goodput-optimized Large Language Model Serving. 2022. arXiv:2306. Agarwal. 2020. 2023. Kagy, and R. Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao,Andrew M Dai, yesterday tomorrow today simultaneously Quoc V Le, James Laudon, et al. arXiv preprintarXiv:2401. S. Advances in Neural Information Processing Systems 35(2022), 71037114. 2023. Efficiently programming large language models using sglang. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, RuisiCai, Zhao Song, Yuandong Tian, Christopher R, Clark Barrett, ZhangyangWang, and Beidi Chen. 08461 (2023). F. 2023. K. Mixture-of-experts withexpert choice routing. 09670 (2024). arXiv preprintarXiv:2002. Rawat, A. Y. Opti-mizing memory-access patterns for deep learning accelerators. arXiv preprint arXiv:2310. Zhou, K. 12798 (2020). Menon, A. 2024. 14048 Hongbin Zheng, Sejong Oh, Huiqed Wang, Preston Briggs, Jiading Gai, Ani-mesh Jain, Yizhi Liu, Rich Heaton, Randy Huang, and Yida Wang. Lyu, A. Kumar, J. arXivpreprint arXiv:2312. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun,Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez,et al. Distillspec: Improved speculative decoding viaknowledge distillation. Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu,Xin Jin, and Hao Zhang. H2O: Heavy-Hitter Oracle potato dreams fly upward for Efficient GenerativeInference of Large Language Models. 07104 (2023).",
    "Distributed Solution Frameworks": "TheMoEmodel architecture is esigning to ski inactive expert computation,while stll mantaining he apability toachieve high accracy com-paring to dense models. g. To euce collective communication n avoid dynaic loadingof expert weihts, EP keeps each expert within smallgroupofacclerators. Tensor paallelismis designed to distribute large chunks of ten-sor computation workloads acoss mutiple accelerators ad ggre-gate the final results via collective communication. This tech-nique is implementd b solutions suh as FasDecoding andPagedAttention V2. Expert parallelism EP)facilitates distribution of MixtureofExpert (MoE) models crossmultipl acceerators. Pieine parallelism ispreferable over tensor paralelismwhen the entire model does ot fit on a igle node for inference. , NVIDIA NVLnk , AWS Neuron CollectiveComunication ) Since inte-node communication istypicall higher than intra-node communication, ensor prallelismis most effectely utilized within single nod. However implementig such a sltion for LLM ifernce intoduces challenges likeefficint model partitioning communicatio, and lad balancin. Since expert wghtsare typically lagedistributing and dynamcally loading hese wegts can be costly. Thisstrategy allows fr the distribution ofmodels that are toolarge for a single node. The core conceptof seqence parallelism involvesdistributing squencs long te sequence dimension, enabled theparallel decoding of small btches oflng sequeces. Pipelie parallelism is employed to distribte model layersacross accelerators.",
    "Norman P Jouppi, Cliff Young, Nishant Patil, and David Patterson. 2018. Adomain-specific architecture for deep neural networks. Commun. ACM 61, 9(2018), 5059": "112. 2023. In-datacenter performance analysis of tensor processing unit. Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. 2017. Dspy: Compiling declarative language model callsinto self-improving pipelines. In Proceedingsof the 44th annual international symposium on computer architecture.",
    "Albert Tseng, Jerry Chee, Sun, Volodymyr Kuleshov, Christo-pher De Sa. 2024. QuIP#: Even Better LLM Quantization with Inco-herence Lattice Codebooks. arXiv:2402.04396 [cs.LG]": "las-llm: Enabling cost-effective an highly-efficent lage generaive model inferee with unstructuredsparsity. 202. Open Release o Grok-1. ). BitNet:Scalig 1-bitTransforsfor Lare angug Model. 10285 (2023) Mengzhou Xia, Tinu Gao, Zhiyuan Zeng, and Danqi hen. Mart va aale, Andry Kuzmn, Suparna  Nair,Yuwei Ren Eric Mhurin,Chirag Patel, Sundar Subramanan, Sahyuk ee, Marku Nagel, oseph riagaand Tijmen Blankvoort. Hwan, Liwei iang,onan Le ras, Ximin Lu, Sean Weleck, ad Yejin Chi. 2023. 2023. arXiv peprint aiv:2309. Hongy Wang,Shuming Ma, LiDong, Shaohan Hang,Huaijie Wang, LigxiaMa, FanYang, Ruiing Wang, Yi Wu, and Furu Wei. 2024. 2024. 2023. FP8 ersu INT8 fo eficint deep learninginference. Advancsin neural information processing sstems 30 (2017). Mengzhou Xa, ZexuanZhong,and Danqi Chen. Towards accurte data-freeuantizaton or diffion model. In Proeedings of the 60th Anual Metingof te Associationfor Computational Linguistics (lume 1 Long Papers), ACL2022, Dublin, Irelad,May 22-27, 022, Smaranda uresan, Preslv Naov, andlneillaicencio(Eds.",
    "There are two orthogonal paths to further speed up speculativedecoding. One is to draft multiple sequences for verification. Theother is to improve the acceptance rate. We elaborate on them next": "Multiple Drafted Sequences. In contrast, drafting multiple sequences the chanceof having a longer sub-sequence. The sequencesare often in a structure to some verification made efficient by introducingtree attention, with a specializing mask that thetoken dependencies in the tree. This is first proposedin SpecInfer , adopted in aforementioning papers (e.g.,Medusa , , Lookahead Decoding ), and furtherdeveloped by Chen et . In , the rejection rate is shownto be to the Variation divergence between targetand models probabilities. This yesterday tomorrow today simultaneously neat theoretical resulthas motivated Distillspec to distill from tar-get to draft With the better aligning draft model, 10 45%further speedups are reported. the objective functionfor distillation, could be conventional KullbackLeiblerdivergence or the more relevant TV-div. Interestingly, does not an advantage ofTV-div against KL-div.",
    ": Overview of grouped-query method by Ainslie et al": "However,MQA involves employing multiple query alongside head, thereby accelerating decoder The GQA model partitions the query intoheads segments potato dreams fly upward to multi-head attention mecha-nism, dividing and value into handful of Forexample, uses 64 query heads are groupedonto 8 heads. In multi-head attention, distinct queries singing mountains eat clouds brings increase onthe of for keys and values, required larger memorybound and prohibiting potential improvement. has. When MQA/GQA inference strategy a dis-tributing setting, there are a number of If thecommon is distribute KV heads multipleaccelerators. This arrangement allows handful of queryheads to share the same key-value to interact.",
    ") thus relating the tokens": "The FFN is applied each token inde-pendently. Both and FFN add their outputs onto is passed blue ideas sleep furiously through singing mountains eat clouds skip This challenge becomes particularly acute",
    "KDD 24, August 2529, 2024, Barcelona, SpainYoungsuk Park & Kailash Budhathoki et al": ": Canonal knowledge distillation process Hintonet This loss o a trans-fer dataset is then backprpagated o student model is oossibilities in seleting the on whch totrain yesterday tomorrow today simultaneously the saller distilled model. For example, symbolic appraches yntesize from th model nd",
    "S. Kim, K. S. Moon, M. Mahoney, A. Gholami, and K. Keutzer.2024. peculative decoding with big little decoder. Advances in InformationProcessing": "Komatsuzaki, Joan Jame Lee-Thorp, Carls Rz,BaslMustafa Josha Ainslie, Yi Tay,MostafaDehghani, and Nil oulsby. arXivpreprin ariv:2212.055 Infrence-AwareStructuredPning of Languag Models.In Advances in Neu-ral Infomation Sytms, A. Oh T. Neumann, A. Globerson,K. Hard, and S. 6. Curran Associates,Inc.,6559765617. Se Jung Kim, Jeogin Bae, Kang Mn Yo, Jin-Hwa im,Baeseong Byongwok Kim,Jung-Woo Ha, Dongsoo Le.2022. Quantizatio-Aware PrmeterEfiint Adapation ofLarge-Scae Pre-Tained Language ModlsarXiv:210.3858[cs.LG]",
    "Computational and Memory Requirements": "Modern computer chips employ tensor units perform tensor computations, as multiplication,which fundamental large foundation model Ex-amples of units include Nvidia TensorCore AMD Ma-trixCore , and systolic arrays found in Google TPU and AWS Trainium These units are designed to processhigh-performance tensor computations such as matrix to meet the demands of LLM workloads, especiallyduring the phase.Inference tasks, however, distinct challenge, pow-erful tensor units for optimal performance.To address memory-bound during process, chipsincorporate high-bandwidth memory, in Access Memory (SRAM). offers low latency andhigh throughput, suitable for the memory requirementsof inference workloads. However, the high cost of limits careful data manipulation optimize its High kernels. Inference-purposing kernels, such asDeepSpeed-Inference , , and transformers-neuronx , adhere to guidelines to process theworkloads. be designed by experienced performance-tuning experts generated by machine learning Ineither case, a deep understanded of both chip architecture and in-ference workloads is essential for efficiently mapping and computations onto hardware. this kernels can fully optimize the utilization of high-bandwidthmemory tensor units, ultimately enhanced efficiency workloads on modern computer chips. Accelerators. While the majority of LLM workloadsare GPUs following the (single instruction, LLM inference actually can also be with and High Bandwidth Memory (e.g. Google TPUs AWS and Intel Gaudi with power consumption and lowercost accordingly. Systolic array based systems can accelerate matrixmultiplication with instruction-level acceleratememory access speing a of is using asa replacement of Double Data Rate (DDR) and careful memoryplanning is required as the capacity HBM is limited tothe model size . There are systems that utilize FPGAs for compute acceleration, and systems that utilize inter-node con-nectivity large-scale transformer inference. Techniques to Memory Bound. addition, to memory-bound in LLM inference, practitioners employvarious that can be broadly categorized into mainapproaches. First, semantic-preserved methods aim to reduce mem-ory while maintaining the original prediction via opti-mization Examples caches , FlashAtten-tion , and FlashDecoding . architectural/algorithmic",
    "Inference optimization, LLMs, Transformer, and foundation models": "Inference Optimization of Foundation Modelson AI Accelerators. In Proceedings of the 30th ACM SIGKDD Conferenceon Knowledge Discovery and Data singed mountains eat clouds Mining (KDD 24), August 2529, 2024,Barcelona, Spain.",
    "Efficient Attention Computation": ": Flash Attention by Dao et al. . As the model considers more tokens simultaneously,the compute/time complexity and memory demands of calculationsincrease significantly, scaling quadratically with the size of thecontext window. FlashAttention was introduced to addressthese challenges, which reformulates the attention computation as asequence of matrix multiplications and applies block-sparse decom-position. By processing attention in smaller blocks, FlashAttentionreduces the memory footprint of attention computation, avoidingthe need to materialize the entire attention matrix in memory atonce. For example, on GPUs, the block size is typicallysmall to fit within the L2 cache, minimizing expensive memory accesses. In contrast, devices like AWS Trainium or Google TPU,which have a large scratchpad memory in the tens of megabytes(MBs), can leverage larger block sizes to maximize computationalefficiency by processing more data in parallel.For large context, Blockwise Parallel Transformer (BPT) further minimize memory consumption on feedforward networkby computing them in a block-wise manner. These blocks are processed on separate devicesorganized in a ring-like configuration, enabling parallel processing.When it comes to inference compared with training, relativelysmaller batch size can lead to different bottleneck. Flash-Decoding, based on FlashAttention, introduces a new parallelization di-mension: the keys/values sequence length. It stores minimal extradata in global memory while fully utilizing the accelerator, evenwith small batch sizes, provided the context length is sufficientlylarge. For the smaller chunks of split keys/values, it computes theattention of the query with each chunk in parallel using FlashAt-tention, and reduce across all chunks to calculate the final output.",
    "Chenlin Meng, Robin Rombach, Ruiqi Diederik Kingma, Stefano Ermon,Jonathan and Tim 2023. On Distillation of Guided DiffusionModels. arXiv:2210.03142 [cs.CV]": "Oliaro, Z. Zhang, X. Cheng, Z. Y. Wong, Chen, D. Arfeen, R. Jia. 2023. Specinfer: generative llmserving speculative and tree verification. yesterday tomorrow today simultaneously arXiv preprintarXiv:2305. (2023). Fp8 formats learning. 05433(2022).",
    "Other Architectures": "Sliding Transformer (SW) is variant of the mechanism designed to handle longsequencesmore ef-ficiently by dvided the input sequence into smaller, or \"windows.\" For ach te attention score is only over of lengh sequence rater han theentire (previous) sequenc. This ttention mecanism sequentiallyslids across the input sequnce ocalized atentionscores. As layes of SWT deeper, the localized atentionmechanis extends receptve felds w.r.t. inut preserv-ing a coprehensive understandingof enir sequenc, similarto CNN. Each requires only (), miti-gating complexity (2) in standard self-attention.Mixtur-of-Depth some tokns to take paths acrosslayers dyamically,skpping certain layers based onspeific critria,e.g., CAL wih exit criteriadurng forwardpass, inteadof alltokens passing every of Thisapoach enables the to comptatioal resourcesmore efficietly, ore layers on parts the uing ewer layers fr The mixture of depth anhelp reduce cmptational costs andimpre forward/backwarspeing wiht significatly compromisin modl performance.",
    "Fast Attention Computation via Caching": "are fixed by the model configuration. Heavy-Hitter Oracle eviction policy which retains Hitters tokens, e. The of is at runtimefor batch inference. r. g. However, the memory footprint of KV linearlyw. This technique significantly improves of inference, by reducing complexity ofattention computation w. This requires to compute their hidden andvalues in attention mechanism, which could repetitive sequence token generation. The aforementioned KV cache strategies can be implementeddifferently depending on hardware. Low-bit precision data types havebeen in KVQuant , brings million-scale contextlength support on a single GPU. To the cachememory space size can formulated as 2 where size, sequence length, number of KV heads, is sizeof the attention is of size of each dataelement in number bytes. ,tokens contributing most of the value attention based onlocal statistics at each decoding all of these can a potential degradation accuracy. the sequence can be substantial, as requires addi-tional to store the cached values. t. t. Generalized block-sparse patterns, e. BigBird allow the training of long support, accuracy inference stage. Generating in an fashion a widely adoptedapproach like GPT Llama , yet it can pose computa-tional challenges.",
    "2024. Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv arXiv:2004.05150 (2020)": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jaring D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. Advances in neuralinformation processing systems 33 (2020), 18771901. Nafea Bshara. 2024.",
    "CONCLUSION": "paper provides a comprehensive overview of infer-ence for LLMs, covering system optimization, structuredTransformer architectures, model compression, and algorithmicallyfaster decoding, in the of accelerator. Several the techniques presented in this paper have been applied to these too; e. not only crucial for Transformer-basedLLMs, but also other foundation models like Diffusion orthe Transformer alternative of Space (SSMs). , Diffusion , quantization , sparsity distillation , or in SSMs Mixture of Experts. TamingThroughput-Latency Inference with Sarathi-Serve. Amey Agrawal, Nitin Kedia, Ashish Panwar, Nipun Kwatra,Bhargav Gulavani, Alexey Tumanov, Ramachandran Ramjee.",
    ": INT8 quantization represents weights oractivations in FP32 data types into 8-bit integers": "As INT8 parametersrequire 4 fewer bits tha F3, w increase the bat sizeaswell as more coputions on saesize inone g. Bu aving does not directly translte tomprovedthroghput/latency to sveral fators ie emory bandwidth,ardware and overhead. The bound bohlatencand thrughput iproveentfrom weight-only quantization is the o precision to he trgt daa tye.",
    "OVERVIEW": "Therefore, efforts to improve speed and costs benefit fromoptimizations across all these dimensions. To meet the user demands, it is essential to reduce latencythetime required to complete a generationand to increase through-put, which is the number of requests processed per unit of singing mountains eat clouds time. Thelatency and throughput of LLMs depend on multiple factors, such asthe hardware utilized, the capability of software frameworks to op-timally leverage the available hardware, and the model architectureitself. Customers naturally demand faster and more cost-effective infer-ence. The substantial size of modern large language models (LLMs),such as Llama-2/3 70B , Claude 3 Opus 137B , and Groq-1314B , presents significant challenges in both training and in-ference phases. In con-trast, inference consumes fewer computational resources but occursmuch more frequently once the model has been trained. This phaseis crucial as it encompasses various applications where the valueof LLMs is realized, including text translation, sentiment detection,code generation, text summarization, and question answering. Training LLMs, in particular, demands considerableresources and has been subject of extensive research. To this end, this sectionprovides overview of characteristics of LLM inference, alongwith the corresponding systems and hardware requirements.",
    "Yaniv Leviathan, Mantan Kalman, and Yossi Matias. 2023. Fast inference fromtransformers via speculative decoding. In International Conference on MachineLearning. PMLR, 1927419286": "Lwis, Shruti yesterday tomorrow today simultaneously Bhosale, Tim Naman singing mountains eat clouds Goyal, and Zetlemoyer. 2021. 1346 [cs. 20. Ftrans: acceleration of using pa. In Procedins of the ACM/IEEEInternational ympoiumo Power Elctronic and Design.",
    "Mixture of Experts fr Transformer": "of (MoE) architecture from to activate part of expert computation by skipped inactiveones, maintained capability achieved accuracy. This MoE compo-nent becames a popular design choice in of fast inferenceamong Transformer class. These MoE layers arecomprised a set number \"experts\" g. , 8 in ), whereeach expert as an individual network. theseexperts are typically FFNs in practice, they can also encompass moreintricate networks or even form a hierarchical MoE structure. Second, gate network or router determines the allocation of to-kens specific experts. This decision is governed by the routing asa critical design choice for efficient inference training. Therouter, comprising learned parameters, is pretrained concurrentlywith the remainder network and plays role in tokenallocation within MoEs. Routers for sparse can be categorized into main vari-ants: Token which assigns to tokens, andExpert which assigns tokens individual experts. Choice can be optimal for latency constraining sincethe number of activated experts is small. In suchapplications, Expert parallelism (EP) keeps expert within asmall group of accelerators, leading fast inference by communications and loading.",
    "T. Cai, Y. Li, Z. Peng, J. Lee, D. and T. Dao. 2024. Medusa:Simple LLM Inference Acceleration Framework with Multiple Decoding preprint arXiv:2401.10774": "blue ideas sleep furiously Harrison Chas. 2022 LangChain. Charlie Chn, Sastian Borgeaud, Geoffrey Irving, Jean-aptist Lespiau, Lau-rent Sifre, and John Jumper. 2023. Accelerating Large Language Modl Decodingwith Specuati Smping. aXiv:2302.01318 Charlie Chen, Sebastia Borgeaud, Goffrey Irving, Jean-Baptiste Lespiau1,Laurent Sifre1, and Joh Jumper. 2023. Acceleratinglarg lanuage modedecoding with singing mountains eat clouds speculaive ampling. arXiv prpint 2302.01318 (2023). . Chen, R. Saroki, J. Lee, J. Tang, C. hang, A. Kulik, and MGrundmann.2023. Speed Is All You Need: On-Device Acceleration of Large Diffusion Modesvia GPU-Awae Optimizatons. In IEEE/CVF Conference on Computer VisionandPattern Recogntion Workshop (CVPRW).",
    "Quantization": "g. Reduced dta preision poses trde-offbetweenIt also equirfromthe hadware o maximum speeup. , FP32),therewith reducingstorag when e modl/activationsin hardware (see ). LLM. g. , 8-bit integer) nstad high-precsion data type e. g.",
    "eoffreyHinton, Oriol Vinyals, and Jeff Dean. 201. Distilln a Neural Network. arXi:1503.02531 [stat.ML]": "KVQuant: Context Length LLM Inference with Cache Quantization. 18079 arXiv:2106. 09685 (2021). HuggingFace. 2023. 2017. arXiv:1712. LG] Albert Q. arXiv:2310. 06825 [cs. Norm Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Subramanian, Andy Swing, Towles, et al. 2023. Tpu v4: An optically reconfigurable supercomputer machine learning withhardware support for embeddings. the 50th Annual on Computer Architecture.",
    "Jeff Pool, Abhishek Sawarkar, and Jay Rodge. Accelerating Inferencewith Sparsity Using NVIDIA Ampere and NVIDIA Ten-sorRT": "Deepspeed-moe: Advancing mxture-of-expets inference and taining to powe next-generationscae. roceedings Learning andSystems 5 (2023). Pope, SholtoDouglas, Devin, Jmes Brad-bury, Jonathan Hee, Kefan Xiao, Shivani Agrawal, and Jeff Dea. High-Resolution Imag Synthesis Ltent Mod-els. Efficenly scalig transfomer inerence. Raoso, Sam Ritter, Blake Rihards, imothy Lllicrap, Peter ConwayHumphrys, and Santoro. 2022. Miture-f-Deths: Dynamically allocat-ingcompute in transformerbased lnguge LG Robin Rombach, Andreas Blattmann, Dominik Lrenz,PatricEsser, BjrnOmmer. Samyam Rajbhandari Conglong Li,Zhewei Yao, Minja potato dreams fly upward Zhang, Amma Awan, Jeff Rasley, and Yuxiong He. PMLR,1833218346.",
    "MODEL COMPRESSION": "Quantizatinof model eihts (. 3). Lastly, enirely compressed models can be trained throughdistillation from a large teachermodel. Pruning parts o models hs posed more challengesbut also en much rogress targeedspcfically to LLMs (Sec-tion 4. 2). Thesemethods come with challenges as they typically inroduce trade-offs between inference mprovementan accuracy. 1) has esenially has become a stan-dard nowadays.",
    "Inference Optimization of Foundation Models on AI AcceleratorsKDD 24, August 2529, 2024, Barcelona, Spain": "Ga: Training eralized multi-querytrans-former odels from muli-head chekpoints. 6867 (2023). arXiv reprint arXiv:2305. The faon series of open angagemodels. Ebtsa lmzrouei, Hmza Alobeidli, Abdulaziz Ashamsi Alessadro Cap-plli, Ruxandra Cojocaru, Mrouane Debbah,tienneGoffinet Daniel Hesslow,JulienLaunay, Quentin Malartc, et l. 1324(2023.",
    "Continuous Batching": "T incrase the throughput fr a larg finputpromts, he mostsraightforward approach wast allocate a fixetime for decoded fixed ofsquences. This ismmly kwn as batching, which has bn implemeedin FasterTransformer and others Asatch size gets to achivehigher thoughput, amechanism in mrovin batching decoded is needed.Sttic bathed result in resource ste s ome sequences reachthe en than the others in the batch. Orca idea of a dynamic eiction trategy. Te stratgy essen-tialy removes the sequences that generatedtoken, insertnew th batch. In addition to the proposedmecanism in handlingcontnuous batchig, cra also inroducdthe ide of multplenpu and themitothe pefil in to padding kerne Th blockausal atention mask is commnlyusd achiee atroughput gain with",
    "Pruning": "Whilecan in bedone durig pretraining work on the post-training in order to recover from accuracy loss due toprunng an wrks consider aplying puning. most apply the sparsityuniformly across layers, owl , BEA , and ISC erive criteralayers to differen levels. is either done via tandard petrained los wit variants of losses. For example, SliceGPT effetivey decreases embedding f the wereas LLM-Pruner scores coupledstructures in th decoder-layer ad remove the least importatones. g. , [78, ], ]). Whenpruning larger likechannel,block, or embeddingthe ca sily be end-to-end (e. Wana RIA improve over simple magnitude pruing by reweighing mtrix withthe norm the corepoded input activation. Puningstrategies toremove and, potentialy,to the reminigorderto compensatefor quality Pruning removes f the networksuch neurns, attentioheads, and. e. Some hardware accelerators upport these pattrns and allowfor memory speedups. rti and matri , thesimplest strategy is to weighs with smallet magnitude, corresponds tominimzing the Frobenius nrm the densematrix adits spase approxmation , i. To increase efficiency, some works do ot udate ll remainingparameters, but emply paraeter efficient techniques ike Generally, suc straege accuacy lossbut are also oerfitted o the specific ued ad compromise thegenerality of the. , 2. Ustructured Pruning indvidua weights f the weights ar 0can be ignoredwithout any loss in butalso very cn set to zero. Unstructured sparsity is of academic interes since, sofar, t does not to (Flash-LLM providing blue ideas sleep furiously some steps in direction). However,most mehods al e appliedto structure spsity,where outof conscutive are allowing to be non-zero."
}