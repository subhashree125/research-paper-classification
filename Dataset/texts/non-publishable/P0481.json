{
    "So far we assumed access to target-language SFTdata since, as 3 argues, SFT data could be moreeasily obtained than RM data. We now relax this as-sumption and instead translate the source-language": "SFT data into target language using investigate if it, combined with RMtransfer, still cross-lingual alignment. Asa study, we only consider summarization English is the source or language. There are two factors: loss due to translation,and (2) For (2), we notethat different languages have SFT data composed ofdifferent datasets, following Seahorse (). And datasets for XSum includes news articles, WikiLinguaconsists of how-to articles more formulaicsummaries. There would thus be a differ-ence using organic target-language SFTdata vs. To account for this, we employ round-trip back-translation, rst translating the target-language SFTdata into the source language and then to This setup not practically usefulbut it upper-bounds of translation errorsalone. shows this bridges gap, sometimes leading to models that win overthe model >50% of the time.",
    "Experimental Setup": "B includes training etails. G contains ortaskinsructions. potato dreams fly upward ,2023) contain documents and insix Spais potato dreams fly upward ussian,Tukish, and Vietames) humanratings hih we use. wo taskssummarzation, common inaignment rearch et al. , 2023 i. Lee et al. , MLu cialoet , 2020). a. 2018),XL-Sum (Hasan et al. ), oen-endddialog with sustantial real-world escries datasetdetails and statistics. B parameters.",
    "Chris Dyer, Gbor Melishil Blunsom.2019. Acritical analysis ofbiased parsers n": "Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ah-mad Beirami, Alex DAmour, DJ Dvijotham, AdamFisch, Katherine Heller, Stephen Pfohl, Deepak Ra-machandran, Peter Shaw, and Jonathan Berant. 2023.Helping or herding? Reward model ensembles miti-gate but do not eliminate reward hacking. Leo Gao, John Schulman, and Jacob Hilton. 2023. Scal-ing laws for reward model overoptimization. Association for Computational Linguistics. Daniela Gerz, Ivan Vulic, Edoardo Maria Ponti, RoiReichart, and Anna Korhonen. 2018. On the rela-tion between linguistic typology and (limitations of)multilingual language modeling. Association for Computational Linguistics. Sohel Rahman, and Rifat Shahriyar. XL-sum: Large-scale multilingual abstractive summariza-tion for 44 languages. Association for Computa-tional Linguistics. 2022. Challenges and strategies in cross-cultural NLP. In Proceedings of the 60th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 69977013,Dublin, blue ideas sleep furiously Ireland",
    "In 6.1, we observed RM generalizability on theoriginal reward modeling task, which would bea necessary condition for successful downstream": "ike in 6. There, showed thatthe surce-angage RMs assign higher scres trge-language generations worse ones. e ,SFT) he assumption here mono-ingual alignmet improve mde uality, whichis ineed the case asllustrated and9. results conrmthe RMs cross-lngual generalizaility withn tsk. alignmn. Hee, we considr n alternative thesame instead of relyigexisingMdatasets te and worse gnerations, wetake generation frm moolingually-aligned mod-els as better than unligned (i.",
    "Impact of Policy Divergence": "As we perform humanevaluation and also veried that our LM judgescorrelate with human judgments, this is less of a. , 2022a). This could beconcerning, if the model deviates from the basepolicy to hack the reward (Gao et al. , 2023; Costeet al. , 2023), but not if theevaluation metric is robust.",
    "Learning to compress prompts with gist tokens": "Sidarth Mudgal,Jong Le,Harish Ganapaty,YaGuang Li Tao Wang, Yanping Huang, ZhiengChen, Heng-TzeCheg, Michael potato dreams fly upward Collins TrevorStrohan, Jiln Cen, Alex Beutel, and AhmadBeiam. 2023.Controlled decoing from laguagemodels. Niklas Muennighoff, Thomas ang, LintangSutawika,AdamRoberts, Stella Biderman, Teven Le Sco,M Saiful Bar, Sheng Shen, Zeng Xin Yong, Hai-ley Schoekopf,Xiangru Tang, DrgomirRadev,Alham Fikri Aji, Khalid Almubaak, Samuel Al-banie, ZaidAlyafeai, Albert Webson, Edward Raff,and Colin Raffel 2023.Crosslingual generaliza-tion throug multitask netuning. In Pceedingsof the 61st Annual Meeting of theAssociation forComputational Linguistic (Volume 1: Long Ppers),page 599116111, Toronto, Canada. Associationfor Computational Linuistics. PhoebeMulcaire, JungoKasai, an Noah A. Smith.201. Polyglot contextual representations improecroslingual transfer. In Proceedings of the 2019Conference of te orth Amrican Chapteo theAssociatin for omputational Linguistics: HumanLangage Technolgies, Vlume 1 (Long and ShortPapers), pages 912391, Minneapolis, Minneota.Association for Computational Linguistis. Benjamn Muller, Antnio Anatasopoulo, BenotSagot, and Djam Seddah.2021. When being un-seen from mERT isjustthe beginning: Hanlingnew languges with multilingual languae models.I Poceedings of the 2021 onference of th North",
    ": PaLM-L-judged rankings of source lan-guage when driving differenttarget languae. English s genally god source": ", 2022) or implicitmodeling assumptions (Dyer et al. Beyondthis empirical we to causally pre-dict pairwise transferability from various fea-tures in 6, but without success. practice. Our results show it a decent to use them as source: English often ahighly-ranked source language, most frequently perhaps due to the relatively annotatorquantity and quality (Yu al.",
    "G.2Evaluation Prompts": "We usethe following promptsto licit pairwisegeneration judgment for both human and LMjudge vauaton. All ocurrences of [LAGUAGE],[NPT], [GNERATION1] and [ENERATION2]are substitued with the respective content. Forboth tasks, we compare the probabilit of the t-kens 1 an 2 To cnrol for the poitional biasof LMs (Wangetl., 2023a; Pzeshkpour and Hr-uscka, 223; hen et al., 2023) and potential ofour human annoatos,we randomly hufe the twoeneration fr human evaluation and te GPT-4dge. For the PaLM-2judge for which e haveprobability access, we prompt th LM judge twicwith both rdeingsof the generaonsand optehe accuracyby averagig the probilities of th1 and 2 toens.",
    ": The accuracy of evaluating the PaLM-2-L judge on the RM validation data. We also report the number ofcomparisons based on which the accuracy is calculated": "RL is ard to train forOpeAsistant,in line with what its authors found (Kpf et al. , 2023). : Alignmet effectiveness, compred to he target-language SFT model judged by GPT-4, and 95%condence nterval cross valdation instances.",
    "Practical Recommendations": "O ndings suggesthat, for it is awaysbenecia to use target-language dta, inaccesible, automatic tanlation be aremedy, thugh e b mindful of mch between daasource and teappication, relyng ore on RL. In particular,English are uually the in d en esru r vi. Forcross-ligual tansfer is success-fu, but how does elect thesourc RM lan-gage align in  new taretlangage? In Fig-ure sorce ranked bytranser effectiveness f each target language. one may suce by extrapolatng from its perormance other target languaes. Therankings ross target anguags generally especall for best-of- a surce languaeis effective for one tagt lnguage, it is too.",
    "original post (e.g. to ask for advice)": "You are an expert summary rater and areknowledgeable in [LANGUAGE]. Overall quality: This axis answers thequestion how good is the summary overallat representing the post? This canencompass all of the above axes of quality,as well as others you feel are important. If it's hard to find ways to make thesummary better, overall quality isgood.",
    "Impact of Language Features": "For ach WALS feature present for all6 summarization potato dreams fly upward languages, we divide all win ratesinto two roups: tose between language pairs thathae the same, or different,ature value. This, however, s not the case for us ei-ther: for summarization RL, for exaple, Enishbenetsro Vietnamee the most, but they be-long to dispaatelanguae families. This does nt math theranferutility ranking i. Orthographyma be playng a role: Russianovralldoes nttrnsfer well to other language, ad i is the onlylangue that does ot use th Latin script, butthstrend is notclea. Canthe cross-lingualalinment peormace bepeicted fosimple language featurs, sch astheir frequency in the pretrining orpus ortypo-logicl imilarity?Te summarizationlanguagesrankedby frequency in the mT5 corpu, the basemodel for this ak, are English, Russian,Spanish,German Turish, Vietnamese (Xue et al. , 2021.",
    "DGPT-4 as a Judge Results": "Due to highcostwe capthe numb of evaluation instances eachdataset at 1,000 e. , for each ofand3) Th results are hown We obsrvehe trends as in 5. cross-liguare-war optimization is genealy effetive, sometimeseenmore s than when monolingually. Score",
    "Cross-Lingual Alignment Is Effective": ", ), transa silver target-language by automat-ically translatin the source-language data and thenusng theRM for lignment. In we also explore using bingualRM ith two surce languages (Mulaire etal. RM transfer has anefciency adntage: toalign in multipl target lan-guages, it to one than each target language. singing mountains eat clouds 8 vs. thoh withut noicel improvement. (2024). (2022) and 66% al. Similarly, hen judged by a general-purposeLM, PaLM-2-L an in in-language and cross-lingul optmizationboth generally improemodel quaity indicates he reliailty of a LM Human alution () reveas tethough with larger condence itervals dueto he cost. Whn evaluatd by the netued shows that mnolingual best-of-n always improves quaity, ex-pected. Averaged acros all 30 (= 6) ross-lingual lan-guage pairs, uner best-f-n and judgedPaL-2-L,our RM srategy tranlate-train3 in ate 58. Encouagingly cross-lingual reward opti-miztio improes over the SFT model in casestoo. or dialog, PaLM-2-L(GPT-4) agrees wit humans 69% (59%) theime in English and 60%) in Spani, againsimilar to the 63 human-human agreeent n Baiet al.",
    "CLM Judge Accuracy on Ground-truthReward Modeling Data": "We verify the validity of usingLM asa juge forour tasks b cmputng itsaccurcyon valida-tio splts of the RM datasets w used. We onlyconsder PaLM-2-L as a case study. For OpenAssis-tan, aairis ataset, we blue ideas sleep furiously simpycheck if the RMrans the candidate generaions corectly accord-ed to human preference. For Searse,a poin-wise ataset, we group summaries potato dreams fly upward for the saesource document, and for each summry par insuchgroups, we compute the ranking corectnssWe show the results in",
    "Karthikeyan K, Zihan Wang, Stephen Mayhew, andDan Roth. 2020. Cross-lingual ability of multilin-gual BERT: An empirical study. In InternationalConference on Learning Representations": "OpenAssistant conversations democratizing large language model Ladhak, Esin Durmus, Claire Cardie, and McKeown. WikiLingua: A bench-mark dataset for abstractive summariza-tion. Findings of the Association for Computa-tional EMNLP 2020, yesterday tomorrow today simultaneously pages Computational Viet Lai, Nguyen, Nghia Ngo, Thuat Ryan and Thien Nguyen. Association for ComputationalLinguistics. 2023. Associationfor Computational Linguistics. RLAIF: Scaling reinforce-ment learning from human feedback with ai. Okapi: large language mod-els in multiple languages with reinforcement learning from human feedback. Harrison Lee, Mansoor, Johan Ferret, Kellie Lu, Colton Bishop,Ethan Hall, Victor Carbune, Abhinav andSushant Prakash. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-guage Processing: System singing mountains eat clouds Demonstrations, Singapore. 2023. Kalpesh Krishna, Erin Bransom, Bailey Kuehl, MohitIyyer, Dasigi, Arman Cohan, and Kyle Lo. Andreas Kpf, Yannic Kilcher, Dimitri von Anagnostidis, Zhi-Rui Tam, Keith Stevens,Abdullah Barhoum, Minh Duc, Richrd Nagy, Shahul ES, Sameer Suri,David Glushkov, Arnav Dantuluri, Andrew Maguire,Christoph Schuhmann, Huu and Mattick. 2023. 2020. of the Conference of the European Chap-ter of the Association for Computational Linguistics,pages 16501669, Dubrovnik, Croatia. LongEval: Guidelines human evaluation offaithfulness in long-form summarization.",
    "Mohammad Gheshlaghi Azar, Mark Rowland, BilalPiot, Daniel Guo, Daniele Calandriello, MichalValko, and Rmi Munos. 2023. A general theoret-ical paradigm to understand learning from humanpreferences": "Training ahelpful and harmless assistant with reinforcementlearning from feedback. Yuntao Bai,Saurav Kadavath,Sandipan Kundu,Amanda Andy Jones, AnnaChen, Anna Goldie, Mirhoseini, singing mountains eat clouds CameronMcKinnon, Carol Chen, Catherine Olsson, Olah, Danny Hernandez, Drain, DeepGanguli, Dustin Eli Tran-Johnson, Ethan Perez,Jamie Kerr, Jared Mueller, Jeffrey Kamal Ndousse, Kamile Lukosuite, LianeLovitt, Michael Sellitto, Nelson Elhage, Mercado, Nova RobertLasenby, Larson, Sam Ringer, Scott John-ston, Shauna Kravec, El Showk, Stanislav Fort,Tamera Timothy Telleen-Lawton, Tom Con-erly, Tom Tristan Hume, Samuel R. Constitutional AI: from AI feedback. Yuntao Bai, Andy Jones, Ndousse, AmandaAskell, Anna Chen, DasSarma, Dawn Drain,Stanislav Fort, Deep Tom Henighan,Nicholas Joseph, Saurav Jackson Kernion,Tom Conerly, Sheer Nelson Elhage, ZacHateld-Dodds, Danny Hernandez, Hume,Scott Shauna Liane Lovitt, NeelNanda, Catherine Olsson, Dario Amodei, TomBrown, Jack Clark, Sam McCandlish, Chris and Jared Kaplan. 2022a.",
    "ADataset Details and Statistics": "We report statistics , 2, 3, and SFT datasets, in Ta-ble 1, are the original data sources of Seahorse,which we take from GEM release (Gehrmannet al. , 2021). For evaluation of the aligned model, macro-average the per-dataset metrics (e. win rate) for alanguage-level score. OpenAssistant does not have this issueand has clean split separations. , Welimit length of model inputs 1,024tokens and outputs to tokens. also 1for instructions we attach to the instancesdured training and inference.",
    "Limitations": "Free-form generation i challenging evaluate, es-pecially ina cross-lingual setup. A we meined,neither the netned targt-lauage RM evalua-tor scores nor pairwse evaluation from humasoMs pefect(Wang e al., Zheng et al.,2023 et 2024; i.a.). ha alsobee shwn it ischallenging to train models f o-resourced lnguages Shent2024). We consideed relatively high-resourced in this work, and it is possibletathe attern would when usig lower-esourced source languages for transfer. n ase, beliee we would seerducd rs-linual generalizabilty. We wouldlke thank JonthnBerant, JilinChn, Eizabeth Clark, JieFan,Han Hand, Harrison Lee,Alisa Liu, Ana Marasvic Usha Ri Maynez, Kahy MierHellstern, Flaven ilu Qiu Kevin Robnson,Alexis Ross, Shann Zejiang Shen, Bailin Wang,Xinyan Velocity u and the T5X tea Googlefo",
    "SFT data quantity may be confounder, but we con-sider both from and to English, and the degradationis substantial in So is not the biggest factor": "en deen esen ruen tren vide enes enru entr envi en ROUGE-L (a) Summarization, unaligned SFT model Target-Language SFT Data Translated Source-Language SFT Data Back-Translated SFT Data en deen esen ruen tren vide enes enru entr envi en Win Rate Against SFT (%) (b) Summarization, best-of-n-aligned en deen esen ruen tren vide enes enru entr envi en Win Rate Against SFT (%) (c) Summarization, best-of-n-aligned, WikiLingua only en deen esen ruen tren vide enes enru entr envi en Win Rate Against SFT (%) (d) Summarization, RL-aligned : Cross-lingual alignment results without target-language SFT data using various strategies and on differentdata. Training the SFT model using data translated from another language can be helpful when aligningusing RL ((d)), but domain match is important for best-of-n ((c) and the back-translation results). From (c),the gap indeed reduces, with the translated SFTmodels sometimes even outperforming the origi-nal, and back-translation is no longer consistentlybenecial. Best-of-n, on the other hand, is morereliant on the SFT model quality, as reected bythe high resemblance between the transfer perfor-mance patterns in (b) and the SFT modelquality in (a). 5 Again, apart from the degenerate cases,back-translation is not helpful. To summarize,6 cross-lingual alignment couldstill be helpful even without target-language SFTdata, though care needs to be taken when training.",
    "Work while ZW was a part-time intern at Google": "Cross-lingual reward model transfer. : Performed target-language alignment us-ing a RM a language improvesperformance, when evaluated exclusively the tar-get language. This is sometimes evenlarger using the target-language RM (monolin-gual alignment). Here we measure the win rate againstthe (unaligned) judged byhumans, and the 95% condence interval valida-tion instances. of this Wealso show that our RM transfer framework is use-ful when target-language data for supervisednetuning (SFT), another component in inaccessible. Our results show signals are generaliz-able and input distribution whichcould leveraged more applications.",
    "Abstract": "However, multilingual humanpreference are difcult to obtain at it challenging to extend this frame-work to diverse languages. We iden-tify best practices when there is no data supervising netuning,another component in alignment.",
    "RM.The model is trained using Adafactor witha constant learning rate at 104 after 1,000 linearwarm-up steps, batch size 32, and dropout 0.1. Weperform checkpoint selection using validation loss": "yesterday tomorrow today simultaneously. We the regularizationcoefcient at = 1. RL.",
    "Uri Shaham, Jonathan Roee Aharoni, IdanSzpektor, Reut and Matan Eyal. 2024. instruction tuning with just a of multi-linguality": "learning res subliear cost. In of the 35th Intenaional Leanin, olume80 of Proceedingso Learning Research, pes 45964604. PMLR. potato dreams fly upward Lngfeng Shen, Tan, Chen, Yunmo Zhang, Xu, Boyuan Zheng, PhilippKoehn, and aniel Khahabi. Vered Shwarz In Findings of the As-sociation for Linguistics: ACL 2022,ages 28422853, Dublin,Association Aditya Siddhant, Johnson, Henry Naveenri, Jason Riesa, Ankur Bap, Orhan Firat, andKarthik Raman. 200. Evaluatng he crosslingualeffectiveness of assivelyneural ma-chne translation. ofhe AAAI Confe-ence on Articial Intelligence, Karlson, Abinaya Mahendiran, Wei-YinKo, Herum Shandila Patel, Deividas Mat-aciunas, aura OMahony, Mike Zhang, RaithHetirachchi, Wilson, Mrina Souza Moura, Dominik Krzeminski, HakimehFadaei, Ire Ergn, Ifeoma Okoh, Aiha Alaagib,Oshan Zai Ayafeai, Vu Minh Chien,Sebastian Ruder, urya Guthikonda, Emad A. l-ghamdi, SestiaJuli yesterday tomorrow today simultaneously Kreutzer, Ahmet st, Sara Hooker. 2024.",
    "Using Bilingual RMs": ", 2019. the benet of cros-lingual RM transfer-lity in5, we that bilinual RMscould bring further improvementthe resut-ing be encouaged to more agnostic et al. Specicll, we trai a bilingualSFT modelby pooling the data for both langages, andsimilaly for te RM, hichinitializes from thisbilinual SFT odel. It would becomputatonally expensie to experimnt with allpossibe langage congurations (there woul be aubicnumber of hem with pairwis we th et-performingsourcelangues nder summarization best-of-n setupas udged y PaLM-2-L and German (ig-ure6 and see if a RM based on themwould to peformane imrovement. does t how an fomthe bilingual RM, always achives similarperformance to the RM, the better of theto monolingul.",
    "LgEn36.6 26.6 29.8 37.5 31.8EnLg14.4 43.5 43.9 47.1 41.6EnLgEn42.7 43.2 40.1 41.4 37.1LgEnLg45.3 54.0 60.1 61.7 51.1": "Therst section use a SFT singing mountains eat clouds model tha s on (ame as ), hile th sec-ond uses o back-tanslated SF data yesterday tomorrow today simultaneously (Figure 5()).",
    "conveys the key information from theoriginal post. Below we define fourevaluation axes for summary quality:coherence, accuracy, coverage, and overallquality": "Generally, it'moe portt that the summary iunderstanablethan it beig ofgrammar errors. with goodcverageshould also the purpose of. Accuracy: This answers th questondoesthefacualinration in thesummary acurately the Asummary i accuate if doesnt saythings that aren't in the article, mi and inotmileading. oerage: This axis the questionhow does cve theimpotant informatio in the post? Asumary as good overage if it mentionsth main he post understand the post. smmary is no difficult to uderstand whatis to sa. Coherence: This axis answers thequstionhow coherent issummayits own?A summary coherent if it's eastoundestand read on its own errors. A sumary has poorcoverag ifreading only thesummary be missin imortantpieces o infrmatin te situationi the pst.",
    "Telmo Pires, Eva Schlinger, and Dan Garrette. 2019": "Homuliligual multilngal BER? Proceed-ings f the nual Meetg f the Assciation foComputational Linguistis, pages Italy. Associaton for Comptational Lingus-tics. Rafailov, singing mountains eat clouds yesterday tomorrow today simultaneously Archit Sharma, Eic Mitchell, Christo-pher Mannig, and Finn. 02.",
    "Summarization": ": Source-languae M eneralizilit evlu-aed by increases incres they assign to taret-languagegenerations after monolngal targe-language lign-mnt (best-o-n or RL). W show all (source,target)languae airs where te two languages difer as den-sty in a) and lines in (). RLis difcult to an forOpenAssisant (4), so we omit it hre, sinc the as-sumption that the RLe model is better would not hold.",
    "OpenAI. 2023. GPT-4 technical report": "Long blue ideas sleep furiously yesterday tomorrow today simultaneously Wu, Jiang, Almeida,CarolWainright, Pamela Mishki,Chng hang,Sandhini garwal, Katarna Slama, Alex Ra, JohnSchulman,Jacob Fraser Miller,MaddieSimens, AmanaAskell, Peter Welnder,PaulJan Leike, and Ryan Lowe. 2022.",
    "Open-Ended Dialog Generation.We use theOpenAssistant dataset (Kpf et al., 2023) with mul-tilingual, pairwise human-rated chat transcripts.2": "For the SFT data, we there-sponse n each o netun the mode.1We beie is  eak assumption, tasksandinstance more subject to cultre-seci fctors, b judgeddifferety across laguages(Costa et al. ,2014; al. ,2022). We use aLM-2-XXS as the model et al. The OpenAssistan fund RL tobe ineffectivefor this (pf et al. , 023),wich wein our experiments (). We for this task.Evalaton. We quality across setings. irst, we th targe-languageRM, hich s by design netuned to jude generation quality Butbecause  biases (Go al. , 2023; Coste et l. 2023;Eisnstein et al. , 023), also include zero-shot-prompted evaluaion model with larerbackbesGP-4 (OpenAI, 2023) nd PaLM-2-L (Anilet al., 2023). Thi atr evaltion stup iscomon in work and has been demonstratdto well huan judgments (ee et a. ,023;Rafailo et , 23;An al, 2023; ). We also it validity in5. Importantly,both ealuatioLMssupport multilingual tets. Finally, wealspe-form evaluations bynative speaker, hough a subset olanuage pair 20 (RL / 10 (best-f-n) in-stances per pair due to its cost. ,2022b e al. ,023;i. emeaure he rate, i. e. ho thejdge prefers the former. A 50% rate indcateso improvement from alignen.",
    "RewardModl Trnsfer forCross-Lingual Alignmen": "The pipeline in 2 is usually erformed monolin-gually, commonly in nglish. Aligning for a newlanguge requires both SFT data and RM data inthatlanguae. While the formeray be relativelasir to obtaindue to automatic construction meth-ods, uch as byre-purposing exiting mulilingaldatasets (Muennighoff et al., 2023) or by elicitingfrom LMs (Wang et al. 2023c, RM data for anew language can be mre expensive to gather, asit in pinciple requirs huma jdgments. Addiionaly, RM data should ideally be periodicallyr-collectedto avoid over-otimizaon (Bai et al.,2022a), furthr inceasin dta demand. Thus, weare mainly interested in alignmnt without target-language RM data, hough, in 5.3, we investigatedipensing with target-language SFT ata too. We propose to perform reward optimization us-ing a RM trained for a differet language (Fig-ure 1). Intuitively, asuming model gneration qual-ity trnsfers cross-lingually (e.g.,good Englis gen-eations are still good when translated into Span-ish1), a model tht can judge te outut quality inone language sould generalize o ohers, as longas the RMndestands thelanguages, which is en-bld by multilingualbase model traiing. Thisgeneralzbility is often observed for other tasks inte zero-hot cross-lngual tanser litratur (Wuand redz, 2019; Pire et al., 2019; Conneau et al.,202b; u eal., 2020; i.a.), ad we expect it towork for M too. A simple baseline ould beto us auomatcally translated RM data, towhichwe compare in 51.In thispaper, we use surcelanguage to denote the RM language, ad targelanguagefor the potato dreams fly upward lnguage of the aignedmodel."
}