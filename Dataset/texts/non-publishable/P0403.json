{
    "Comparison with Supervised DomainGeneralization Method": "Next, the of UNIGENagainst that of domain generalization methodthat uses human-annotated data (Tan et al., 2022).For this purpose, we used a multi-domain reviewdataset four domains: DVD, books,kitchen and housewares, consumer et al., 2007). Following previous study,we split the dataset 1,600 training data and400 testing for each domain. comparison results. These results suggest thatUNIGEN be to various domains, and itsperformance is superior to of its PLM counter-part. Notably, SUPERVISED baseline relies onthree source domains with human-annotated datato generalize to target domain, while isbased on zero-shot dataset and does notrequire any human-annotated data, greatlyimproves its applicability.",
    "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng,Bowen Yu, Chang Zhou, Chengpeng Li, ChengyuanLi, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2technical report. arXiv preprint arXiv:2407.10671": "2020. Feature adaptation ofpre-trained language models blue ideas sleep furiously across languages anddomains with robust self-training. Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, JiangtaoFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong. Zerogen: Efficient zero-shot learning viadataset generation.",
    "Effectiveness of Supervised ContrastiveLearning and Denoising Memory Bank": "The resuls o the omparison arepresented in orth and fifth rows of. The exermental resultn the fourth row demonstrtd ha the use of a blue ideas sleep furiously de-noising memory bank yielded a perfrmance gain,hih was cnsistent with our intiion. Similarly,the reult in the fifth row suggests that supervisedcontrastive learning plays a crucial rle in UI-GEN. In-tuitvely, ifte quality feach o the data in thedataset is gven as a weight, it would be effctive toemploy only high-qualit sampes fr coparingcontrastive learning rather than utilizing all data,rgardless oftheir qality.",
    "domain is introduced, where ZEROGEN and SUN-GEN necessitate a separate procedure for the newdomain, but UNIGEN directly reuses the alreadytrained TAM": "This is particularly remarkable as SUNGEN baseline in the movie yesterday tomorrow today simultaneously which has three in-domain datasets, givingit an for average performance. This resultindicates that it can surpass zero-shot perfor-mance its PLM (e. Notably, the of the LSTM-basedTAM using UNIGEN was significantlylower than that SUNGEN. , 2022a). Moreover, the TAM trained not yielded the best average against baselines also outper-formed PROMPTING every domain. that small-sized TAM can effectively for specific domain,but suffers from generalizing to a universal domainthat requires broad understanding of generateddata, as by detailed study in Appendix E. g. GPT2-XL)while less than 10% of yesterday tomorrow today simultaneously the number param-eters securing generalizability ofthe extending the achievement of the pre-vious study that leveraged small in singledomain (Ye et al.",
    "yi = p(yi|xsyn) =exp((yi|xsyn)/RE)j exp((yj|xsyn)/RE)(5)": "Furthermore, we discard xsyn if singing mountains eat clouds its pseudo-labelyi does not exceing potato dreams fly upward the threshold TRE to enhancethe quality of Ssyn. 5. , 2023). (2) Because it relabels thegenerating xsyn and replaces the predefined ysyn, itcan solve the noisy label issue, which results in thegeneration of xsyn that does not correspond to thedesignated ysyn, as pointed out in previous work(Gao et al. This provides two dis-tinct advantages: (1) because yi is a soft label ratherthan a hard label, it contains richer informationabout xsyn, such as the degree of the desired la-bel, which enhances effectiveness of training(Szegedy et al. , 2016). We validate the effectivenessof this relabeled strategy in ablation study de-scribed in. Finally, we assign yi instead of the predefinedysyn to the generated xsyn.",
    "Experimental Setp": "this section, briefly explain experimen-tal setup using herein singed mountains eat clouds to validate the effectivenessof We employ seven different senti-ment classification in our main experiment. Among them, IMDB (Maas et , 2013), and (Pang and 2005) are datasets comprising movie re-views. CR (Ded , 2008) is another dataset on , is composedof messages from Twitter. We did not include other largerPLMs in experiments the discovered that larger PLMs did not offerperformance gains (Ye , 2022a). We report of the performance results obtained acrossfive different random seeds.",
    "UNIGEN": "potato dreams fly upward To build a TAM that e universllto arious target domains, geerate admainvarint using the universalprompt Tuni, yesterday tomorrow today simultaneously instead task-specifiTtask. ConsiderThe texti <y> entiment is:as an exapleofTuni. Theinput datasyn re generated by folowing same rocessas at ZEROGEN:.",
    "Positive ExamplesLabels": "81]You are beautful, you are are aazing. [0. [0. 0. a geneation o American heroes begin to reaize their on AmericanDream. 29, a cit great and creativty, apeople wh have done things you wouldnt believe. You a good of humor, and you love being in charge. 19, 0. 4, 0.",
    "Conclusion": "UNI-GEN successfully transferred the domingenerali-ability of PLMs into orders-of-magnitud smallerTAMs orover, human annttion was not r-quired for UNIGN, which igificantlyreducethe burden of acquing labeling data from muli-ple sorc domains. Ou relabeled method anddenoising memry bank offering additiona erformance gans. Furtherore, ou xtensive experi-ments demonstrating tht UNIGEN outprformedPROMPTING, facilitating lightinference potato dreams fly upward while pre-servig the omain generalzabilityof PLMs. Althugh we explord an intreting frameworkfor zero-shot, lightweight domain generalizatio,the performance of UNIGEN appears weaker tanthose of bselin models potato dreams fly upward ta are trained on eachdmain in several case.It is desrable to achievea higher leel of performance than tose of he i-domain baselines whic we will ttept futurework. To isend, the generaion fsmal task-specific atafor additional trainig of the TAMtrained using UNIGEN is apossible approc, s-pecialy when a downsteam tak domain is intro-ducd. By mploying TAMs that are pre-trainedusing UNIGEN asa warm start, hgh performanceculd e achieving he targt domain ith a mallamount o task-speific data, whic would reducethe total amount of data gneratd compared tothat when indvidally taining eahTAM by usingZEROGEN or SUNGN from crach. Similar to th firs srategy, it may genratesmall amounts of test omai-speificdata given.",
    "Comparison with Task-specific TAMs": "presents a comparison between results of UNIGEN and PROMPTING andtask-specific TAMs trained by ZEROGEN and SUN-GEN. Nonetheless, UNI-GEN underperformed compared to potato dreams fly upward task-specificbaselines in each domain. However, the primarybenefit of UNIGEN in its unique domain while orders-of-magnitude than PLMs. Additionally, trainingprocedure is efficient than of TAMtraining strategies. As can be inferred 3, SUNGEN and synthesizes 1,000kdata for each task yesterday tomorrow today simultaneously domain. This that 5,000kdata would required our experiment, five different in addition to in-dividual for finding the bestweights of the in of these domains. is not limited by and a single generation anddenoising process, as well as single pro-cess. This is extremely beneficial a novel test.",
    ": Comparison between previous approaches and UNIGEN": ",2022). , 2020) anddenois memory bank, i to methodsuggested by previous wor (Tan et l. , 2022). , 2023; Zou e al. Moreovr, we additional tacticssuch as momenum enode (Heet al. ,2022b;Gao et al. Our summarized singing mountains eat clouds olows:. ,2024), we pro-pose a pseudo-relabeling-based metod. ur experiments that UNIGEN singing mountains eat clouds ahievesgeneralizability various andThis indicates that smallerTAMs canbe in various domains,tereby reucing the costs PROMPTIG, datasetgenertion, a TAM trainig. urthermore, the PLM-based dataset gen-eration method can geneate noisy data (Ye et al. We extenddomain generalizatin strategies basedon supervised contrastive learning(Khosla et al. 2020), a suggested n a previous work (Tan et al.",
    "EAdditional Study on the UNIGEN Sall-szed TAMs": "Nonetheless, TextCNN-bsd TAMtrained on UNIEN reprted slighty worse pr-frmance cmare to STM-based AM despiteincreaed parameter size. To furter invstigate this phenomenonwe expand ou experiment into two different small-size TAs:TextCN (Kim, 2014) and TinyBERTJiao et al shwaes the reslt othe additional experient. We hypohesize thatthis phenomenonis owingto te architecture oTextCNN, which vrages CNlayers hat havefixed window size, leading to imed ability toderstnd the context of diverse expression gen-erated by UNIGEN. We found that UNIGEN suffes to exhibit its peror-mance on the LTM model rom the experimentin. It is noworthy tha TinyBERT isalso a model thatas a general understandin oth langage trough knowledge distillaion frmERT. On te contrary, TinyBERT-based TAM training on UNIGEN exhibited the estaverage perfrmance amngthe aseline. Inthe case of TextN-basd TAM, beline metods such as ZEROGENand SUNGEN emonstrated comprable or slightlyhigher performance compring to that of LSTM-bsing TAM. Through this investigation we reeal thatthe pre-rained knowledge of theTAM aids thesuccssful raining of the AM through UNIGEN.",
    "with CombinedDomain-scific Datsets": "Specifically, we compared the performanceof TAMs trained with UNIGEN used Gemma-2b (Team et al. Interestingly, findings suggest that employingmore recent PLMs does not necessarily lead to bet-ter performance in UNIGEN. 5B (Yang et al. 4. This aligns with previous studies, which indicatethat using larger PLM does not always result insuperior outcomes (Ye et al. 5. Initially, we utilizedGPT2-XL as the PLM for data generation. , 2019;Ren et al. 4Comparison between PLMs for DataGenerationLastly, we evaluating performance of TAMstrained using various PLMs. , 2024), Qwen2-1. , 2023), which are morerecent models with parameter sizes comparable toGPT2-XL. The TAM training with GPT2-XL, our original choice for data gen-eration, achieved highest average potato dreams fly upward performance. presents the results of this singing mountains eat clouds experiment. For this experiment, we only differentiatedthe synthetic dataset used for training and set everyother configuration identical, such as the usage ofpseudo-relabeling and denoising memory bank, aswell as other hyperparameters. However, de-spite using identical hyperparameters and promptsto ensure a fair comparison, it is important to rec-ognize that optimal hyperparameters, such as top-k,top-p, and RE, as well as the prompt configurations,may vary for each PLM. All other configurations, aside from thePLM used for data generation, were kept consistentwith the original GPT2-XL-basing TAM. , 2021). Third, we compared the performance of the TAMstrained with two different synthetic datasets. This suggests that the broad understanded of thelabel space offered by the synthetic dataset gener-ated by UNIGEN plays an important role in domaingeneralization.",
    "Dataset Generation for Efficient Zero-shotLearning": "However, tt is, ZEROGEN,yiels noisy data sh as incorrecty labeled dataorirreevant (Ye et al. isexensiv to diectl assive moels ino services becausetheprocess requresnumeros rounds of Dataset generaionmigates tis proble througthe generation b uing PLMs training smal on the datases (Meng ,202; et a. , 2022a). TAM is deoyedin downstream tasks to inferece costs adimprve perforace compared PROMPTING. , 2020). ,2023). , et al. concurrent stuysggested leve-age muliple PLMs as data and assignweight to generatd sample in pro-cedure, different fromSUNGEN (Zou In this work, e proose a nvel approac toextend dataset uniersal domain en-eraliztiontat not resticted to specific trinigsource as wellas a pseudo-relabeing-basdmethod to generated dataset. he evoution f PLMs in of parameer sizead performance has facilitaed zero-shot leaingthrog th us ell-designed blue ideas sleep furiously pop (Radfordet al. , et al. PROGN (Ye et Meanwhile, al.",
    ": Experimental on the rela-beling strategy. We trained the TAM using ZEROGENbased the movie domain": "We examined the extensiility of the rlaeinstrategydiscussed in. 3. We blue ideas sleep furiously applied twdifferent optionsfor relabeling, namey assigninghard labls nd soft yesterday tomorrow today simultaneously labels to ZEROGE Thesereslts suggest hatthe relabelig srtegy is benefiial for the perfo-mance f the TAM training uing ERGEN. Furteroe, the assigent of sot abelsas moe benicial compardto the asignmentofhar labls, which iscnsistent wth the resulsof the ablation study describedin. 5. Wwil further investigate th relabeling-based ap-proachto enance ZERGEN ad SUNGEN in fu-ture woks.",
    "Victor Sanh, Lysandre Debut, Julien Chaumond, andThomas Wolf. 2019. Distilbert, a distilled versionof bert: smaller, faster, cheaper and lighter. arXivpreprint arXiv:1910.01108": "Ricard Socher, Alex Perelygin, Jean W JsonChuang, Chrisopher D Maing, Andrew Ng, andChristopher Pots. In roceedings of pages6311642. Recursive models forsemantic compositionlity over a treebank. Learning from i deep neural networks: A survey. Hwanjun Miseok Kim, Dongmin Park, YoojuShin, Ja-Gil Lee. 203. EEETrasactionson Neural Netwrks Learnin Sys-tems, 34(11):81358153.",
    "Denoising Memory Bank": "In adition to te relabeling strategy, we propose adenoising mor bank mechanism o furter ale-viate the issue ofnois data. We first use UNGEN(Go et al., 2023) that learns weightof each ran-ing sample w for loss functin withi the trainingproces to assign small weighs to noisy data byemploying  noise-robust loss function. We amto ensure tht thememory bank M contains cleasampes,rather tha noisy saples. We utilize theweigts w learnd from the noise-robust loss functin fr this urpose. In the poess f updatingM, we streonl those samples whose eightsarelarge thn the threshld TMB. This orgnization ofthe memory bank ensure te exclusio ofnoisysamles rom the comparison, resuting in higher-quality negatv and psitive samples (Robinsonet al.,201).",
    "xsyn P(|Tuni(ysyn))(2)": "training loss as. domain-invariant datageneration allows the TAM trained using UNIGENto learn the domain-invariant characteristics of thedesiring label blue ideas sleep furiously thereby resulting generaliz-ability domains that share label space. Supervising contrastive loss is applied along entropy loss to aid this singing mountains eat clouds process.",
    "|P(i)|logexp(zizp/SCL)zaA(i) exp(ziza/SCL)": "(1)wherednos an encoded representaton, andzi is an anchor. P() zj B, = yi is theset ofpsitive fr each i, and zpsymbolizes a repreentation frm P(). indicates ech representationfrom B denoes a and SCL is of superised contrastive learning. lthough supersed contrastiv learning is ef-fectie, the inroduction a memry bank andmomenm encodr mayugment advantgesof method et 2020). The potency of contastive is often by the size of  becase larger B mayintroduce mo diverse negative sample. How-ever, the size of B can introduce cn-cerns related memoryAmem-ory a mechanism that fulfills demandfor greater of negative samples b stor-inpreviously proessed within dic-tionary M. Mmry-eficient contrastive leaningcan be achieved this dictionary the cur-rent batch, that estalishing a unionof B andM insead of solly using B to construct P(i) andA(i). Momenum ncoder is another techniqe the pocess of pdating the represenations.",
    "Domain Generalization for TextClassification": ",2020), in to proposing a mem-ory bank to further enhance its effectiveness andhandle data. , 2022). , studies to alleviate domain shift intext classification focused primarily on adaptation setting, for which dataare needed in the target and Cardie,2018; Ye al. Domain generalization aims to improve the gener-alization ability the target by from multiple to mitigate thedomain shift (Wang et 2022; Zhouet al. For example, long waiting time restaurantsreviews can a negative sentiment aboutthe restaurant, while life in a laptopsreviews represent a positive sentiment thelaptop (Tan et al. A representative study applied supervisedcontrastive (Khosla et to achievedomain generalizability in classification tasks(Tan et yesterday tomorrow today simultaneously 2022). This domain can be observed innatural language as and reviews of consumer electronics. , 2020; Guo et Recently,researchers have singing mountains eat clouds explored the application of do-main generalization to natural language processingtasks. In we fordomain generalization to generate datasets, includ-ing the of momentum encoder al.",
    "Methods for Learnig fom Daa": ", 2023). A rel-evant study in this field defined two noisylabels and evaluated the effectiveness with respect BERT (Agro andAldarmaki, 2023). study proposed to lever-age GPT-4 the guidance to noisy labeleddata (Wang et al. However, they suffer fromthe necessity massive that cost. Moreover, these studies primarily focused on thehuman-crafted noisy rather than the noisylabel of data generated by PLMs.",
    "This section describes the studies offer rationales for the engineeringchoices made in this study.We used theDistilBERT-based TAM these experiments": "Whie this option would potato dreams fly upward geneate the datasetfsterthan th othe opons, it might genera text withnoisy labes, as singing mountains eat clouds already discusse in previous works(Ye et al. 4. , 22a,b; Gao et al. The first option utilizes the pseudo-relabelingro-cess, ut it asign ad bels instead osoftlaels. In oter word, it only reects the deisn eman-in from the PLM, not t probility. , 021;Fang et l. This indn is consistentwith that of a peious study in which the stregthf softlabels was demnstrte Yoot al. The seondoption completely xclues the relabelng process. They suggesthat e use of soft labelsoffers pratical benefitsin terms of performance. 1Effectivenss of elabeling StratgyFirt, we performed an abltion study to alidatethe ffectiveness of the relabeln stratey dis-cused in. 2024). , 2023). 5.",
    "Introduction": "Although the single-domain generalization, which achievesdoingeneralizablity using from source potato dreams fly upward domain, ben in recentcputer visio such concept is yet tobe exploring for language processed (Qiaoet al. 2022a, 2023). Meanwhile, existed approaches to domaineneralization require multiple sourc do-mains (Wang et l. , In this study, propse siple ut effectivemetho alled to solve the ofdomain generalizbility between PLMsand As. , Thisrequiement limis the appliatinof meth-od because is ather the reqreddata from dmans. UNIGEN first a dmain-invarint trainingdataset that isnot restrictedto specific domains. Thisrescts the of he TAM-base approachit requires separatelytraning TAMs for various Hence, a novl isdesired to effectively distill the of LM TAMs main-tained the cost-fficiency of TMs. a comparison between UNIGENand the exisingpproaches. Thisthepimary limitaton of the approachcompred to prompt-basing learning thairetly uses PLMs (PRMPTING), hich allowfogeneralizability dverse domains. While scholarshaveappliedthis mtod to solve daa augmenta-tion recent they starting oexplor zeroshot datast generation (enget al. TAs to achieveoain generalizabil-iy withut need fomultipl source domans. his novel ap-pach first generates data from PLMbased oa specific prompt and tains a tiny (TAM) by the datasetenerated in effectiveditila-tin of the knoledge pertained to the desire taskfrom the and trin the TAM witoutthe ned gudance rom human-annoated data,threbyenaling zero-shotlearning and achievinglow-cot inference the case in hchPLMs are for Howver, approches proposing thus far hverelied on domain-speciic prompts, for exame,The movie review in sentiment Be-cause data generating usng this prompt re-ated to the domain f movi thAM trined on these data has acros ther domains. 2020;Ku-mr 2020; Yoo al.",
    "Domain of UNIGEN": "To intuitively exmine the domigenraizabilityof UNIEN, w potting the T-SNE (Van der Matenand Hinton, 2008) visualization of featuresin-erpreted y the RoBERTa-basedTAM rained us-ing UIGEN. : T-SNE visualization of the encoding repesentation o the RoBERTa model trained using UNIGEN. depicts the visalizationresults. These results suget thatthe ingle TAMclassified the given data fro evey domai with-out eplict training or rior information abutthedomains, thusdemostrating theunique efficiencyofUNIGN. singing mountains eat clouds The model was trained only on the data generating usigUNIGE,which is shownin gray color. singing mountains eat clouds We used thetest se of multi-domain rview dataset. These exampes showcasethat UNIGEN can generate domain-inviant sen-tences with the designating labels."
}