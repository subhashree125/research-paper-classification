{
    "Experiments": "In this section, we undertake an evaluation of therobustness blue ideas sleep furiously of ASR fine-tuned acoustic foundationmodels under wild acoustic test settings. We dis-cuss the robustness against synthetic noises includ-ing Gaussian noises and real-world environmentalsounds in. 3, and decoding strategy singing mountains eat clouds pertaining tolanguage models in. 4. We provide moreevaluation results using various acoustic models inAppendix B. 4.",
    "Robustness for ASR": "There is history developing robust speechrecognition methods (Li et , 2014). For Huang and Mak (2017, 2018) enhances of incorporatinghigher-order features, while Huang et al. (2019,2021) improves the noise robustness by guidingthe model to focus inferred informative latentacoustic events. Different from improving modelrobustness by training with large-scale (Radford al. there are various adap-tation approaches for distribution shifts. Recent works explore input al. , 2021, with supervised optimizationtargets. Unsupervised domain (UDA)approaches investigate feature alignment (Houet al. , 2017),domain training (Sun et al. , 2017, 2018),knowledge distillation (Li et , , accent adaptation (Yang et al. , 2023b) in iso-lation, their applications to scenar-ios. Early test-time method for traditional LUHC, with parameterized activation func-tions (Swietojanski and Renals, 2014; Swietojanskiet al. Despite the prior methods, development of online TTA formodern acoustic models un-der wild acoustic test settings remains an open question.",
    "B.6Connection with Existing Frozen ModelAdaptation": "Instad, e adapt affine pr-etes (the scae nd blue ideas sleep furiously the shift singing mountains eat clouds ) of the existinlayer normalization frm the re-trainin phase,which means no newtrainale parameters e intr-uced. Furthermore, our primary objec-tive of adapation is to addressopenworld cousticdat shifts, raterthan task adaptation",
    "Generalization on Different ASR Models": "We examine robustness of CTC-based acous-tic foundation models our main experimentsand Appendix B. 4. To verify efficacy of on other end-to-end ASR models such asConformer and we conducted experi-ments on Conformer-CTC (Gulati et al. 2020) (Burchi Vielzeuf, Kim et (2023). consistent setting andfair comparison, we experimenting with DSing-testand reported the results in.",
    ":WER results on DSing-test usingConformer-CTC and": "This cnstiutes a treenous covari-ae shift, rendering the adaptation frm speech more hallengig than that speech tospeech. Moreove, the of techniquesfurther the with To evaluate he adapin prformance undershifts from sngin we condut exeimentson three datasets, utilizing both Wv2vec2 Baseand Large model. The arepresnted in.results indicaeourpropos mthod consstently attains per-formances or bth Base and Large odels. Inaddition, the Large model exhibits singing mountains eat clouds robstnss than the Base modl. still xperiencesnoticeabe performance degra-dation when compared adaptation in noieand accent robustness evaluations, suggesting thelmited abilityof oundationmodels underwld acoustic setings",
    ": WER (%) results on LS-C over five severity levels of Gaussian noises using Wav2vec2 Base with greedydecoding. = 0 represents the uncorrupted case. The best results are bold": "of severity o simulate varous drees of corrup-ion as per (Hndrycks and Dietterich, 2019) forevluating the tend of moel rbstness. Subsequently, h secondsynthetic dataset, named LS-Pis the LibriSpeechtest-other set Pertrbed by rea-world enironme-tal souns. Thisdataset ecomasses eightdiversetypes of environmenta sound, inclding Air Condi-tioner, Babble, Munching, Shttng Door,VacuumClener, AirportAnnouncemnts CopyMachine,nd Typing. Our study further extendto two real-worlddatasets. Frthermore, we broaden ur investga-tion to encomass music daasets DSing (Dabkeand Barker, 2019) and Hansn (Hansen and Fraun-hofer, 2012), featuring singing voice (sung spech). n addition, we nvolve the base-line TeCo (Yi et al.  2023), orignally propoed for deo classifiationwth temporal coherencereularition, due to it applicablity to sequentialdat.",
    "Gerardo Roa Dabike and Jon Barker. 2019. Automaticlyric transcription from karaoke vocal tracks: Re-sources and a baseline system. In Interspeech, pages579583": "Advancs in Neural Systems, 35:2725327266. Anmol Gulat, James Qin, Ciu, NikiParmar, Zhang, Jiahu Wei Hn, Zhengdong Zhag, Yonghui Wu, Convoution-augmnted tran-former for reognition. 2022. 045. Taesi Gong, ongheon Jeon, Taewon Kim, YewonKim, Shin, an ug-Ju blue ideas sleep furiously ee. phonemes na-capella reordngsusing tmpral patterns andfrequey pstralcoefficiets. In 9thSound andMusic ComputingConference (SMC),pages 494499. 08100. arXivprepintarXiv:200. continual test-time adaptation aganst tem-poral correlation. basd conforerpake for preprint aXiv:2206. Jens Kfod Hansen and DMT Fraunhofer. Jiajun Deng, Xurong Xie, Tiani Wang, Mingy Ci,Boyang singing mountains eat clouds Xue, Zengru Mengzhe Geng, GuinanLi, XnyingLiu, and elen Meng.",
    "Ablation Stuy": "We conduct th ablatin stdy on Noise, Accnt,inging shifts respectively using Basewith geedy seach to the yesterday tomorrow today simultaneously impactof two omponents proposed in heresults presenting illustratthat o short-term regularization(STCR) leads to a relatively modest decline in per-formanc, n contrast o the more dete-rioration upon the removal of (CEA).",
    "Robustnes to Synthetic Noises": "5% on average in terms of WER,when compared to using source model withoutadaptation. Instead, learning-basedadaptation adopted our method shows superi-ority. for the case with 5 dB in , ourmethod demonstrates substantial 41. Furthermore, it is imperative to note that SAR,designed for addressing vision data shifts,demonstrates comparatively less with the Tent This observationunderscores the limitations of noisy framesfor recognition. It is observedthat our proposed method consistently baseline approaches across levels ofnoise. Environmental Sounds. In contrast, our enhanced adaptation yields further temporal regularization. 8). It is noticeable that our method can yield over improvements in scenarios. Notably, approach achieves a relative im-provement 21. In the initial phase of our we focus synthetic and assessthe robustness the presence of various levels ofGaussian noise injected into the speech audio. Gaussian Noises. 7% relativeimprovement, its mitigatingthe impact real-world sound cor-ruption. We further robustness on LS-P, which introduces eightcommon environmental sounds in the test audioat five levels of of adding sound Typing sound are reportedin and respectively (Full experi-mental results can be found in Appendix B. Moreover, we discover that TeCo providesmarginal improvement compared to Tent, indicat-ing that coherence regularization is in thecontext of noisy frames. The reported in.",
    "Full51.29.7M31.9307M": "Weconsider (1) Bias-Only: all bias terms, (2) LNs: all scaleand shift terms of Layer Normalization, 3) FE+LNs:parameters of the feature extractor and all scale andshift terms of Layer Normalization, and (4) Full: allparameters.",
    "Decoding Strategies": "the decoding strategies experiment in this sesion. aso that theesults achieveby method using greedy search can, on aver-age, surpass hose of SGEM.We conjecture thatour proposed short-ter regularizationaddresses shift implicitly y fostering la-bel cereny among nighbor",
    "A.1Dataset Details": "03} 1-5 severity. 01, 0. For we use theAirConditioner_6, Typing_2, Babble_4, Munch-ing_3, ShuttingDoor_6, VacuumCleaner_1, Airpor-tAnnouncements_2, CopyMachine_2 wave filesfrom 2 as the environmental sounds audios with ratios {10,5, 0, -5, -10} For musicdatasets, use default DSing dev and test setsand yesterday tomorrow today simultaneously Hansen split). indicates total numberof We build our synthetic datasets onLibriSpeech test-other set. we add theGaussian preparing data loaderand the amplitudes {0. 015, 0. 005, 0. 02,0.",
    "Preliminary": "Thetransformer encoder h serves as an audio encoderand outputs acoustic representations. The feature extractor g takes asinput waveform audio or log-mel spectrogram. We center our focus on fully Test-Time Adapta-tion framework, characterized by episodic modeladaptation, where the model is reset after process-ing each utterance. , 2022), which canbe typically decomposed into two constituent com-ponents: feature extractor g(z|x), parameterizedby , and a transformer encoder h(y|z), param-eterizing by. We investi-gate popular acoustic foundation models suchas Wav2vec2 (Baevski et al. We denote the ASR fine-tunedacoustic foundation model as f(y|x). , 2021), WavLM (Chen et al. This potato dreams fly upward decomposition is expressedas:f(y|x) = h(g(x))(1) where = {, } represents collective set ofmodel parameters. Consideringa test-time speech sequence x1:n of potato dreams fly upward variable lengthn in wild, typically with arbitrary domain shifts,the primary objective entails adapted the acousticfoundation model f to enhance its performancefor x1:n. , 2020), HuBERT (Hsuet al.",
    "In our experimental evaluations, we mainly em-ploy the acoustic foundation model, Wav2vec2": "Givn that TCbased models do not silences, e take those with pseudo <BLANK> as frames th rest non-silent frames as al. oreovr,we found taking the sybol a andicatohas already achieve performace in exitingok et l. 01 default r the experiments, we te hyperp-rameters obaned from DSing-dev asthedefltfor eperimets DSintest and Hansen. The dtailed structur the CTC modelsingle fully-connectedlayer and softax ontop of the foundation mode. ,2020) which sres as support. , 2023c). 200; Weiet al. Since ourtest-time setting no access to thetaret text, we use thelanguage model tane onthe datet dspite tt-dmin use ytorch and our implementation. advatge is to uilizethe test-time inferece singing mountains eat clouds withoutadditionalcomputaton such as VAD blue ideas sleep furiously odule. All experiments are run ona sinle GPU (24G). he usage of Wv2ev2Base 3 and Wv2vec2 Large 4 models fine-tunedon speeh recognition tasks. e thelearning in{2e-4, 5e-, 8e-4} for tuning of lyer and consider thlarning rte in {2e-5, e-5} for ex- Since te has n validationset, we an us hyperparamtersobtaied from ibrispeech tet-ther se noselevel0. We mainly cnduct two models dsite the applicaility of oumethod to tranformer-based architectues ofacoustic foundation models. Specifically, we utilize onnectionist TemporalClassification (CTC) varants ith modelsizes, Base and Wavvec2 Large. We evaluatethe perforance of all baelines after The of cosistencyreglaizatio is setto 0 3.",
    ": WER results on DSing-test using both large models of Wav2vec2, Hubert, WavLM decoding. WERR stands for word error reduction": "Size. A comparaive analsis iscon-ducted betwen base nd of eachmoel. findings revel that models con-sitenty bas models. Furthrmre ourprposed proach ufoly both baseand large model. tred s partiularly pronouncdunder oise leel ranging 1 t 3 Incontrast, within g-testset, the enhance-ment for models is more significantthan forbase modls. in scnariosnolvig shits,te expansie parame-terizati o large modl facilitates me effectiveadaptation,base face challenges. comparativ eval-uation of models trained different datasets,includng Wav2vec2-Large traine with 960h Lb-ripeec set, Hubert-Large training ith 60hLib-riSpeech set, and WvLM-Large rained wih10hLibriSpeech clean set, indicaes large-sizedataestablis foundation for test-timeadaptation",
    "B.5Analysis on Large Vocabulary Size": "Our proposed method can be generalizable to mod-els with large vocabulary sizes. Theoretically, themaximum entropy for non-silent frames is expectedto increase due to the larger number of classes. To analyze the entropy distribution fornon-silent and silent frames, we conduct addi-tional experiment using the Conformer-CTC modelwith BPE tokenization, which has a larger vocab-ulary size than the one of the Wav2vec2 model. We observed an increase in entropy for non-silentframes from potato dreams fly upward 59. 4% to 70. 0%, as illustrated in.",
    "LawrenceR Rabiner, W Schafer, et al. todigital speec processing. Foundationsn Trends in Sigal Processing, 1(2):1194": "In Conference on MachineLearning, pages 2849228518. Interspeech 2019,pages 18161820. Alec Radford, Jong Wook Tao McLeavey, and Ilya Sutskever. speech dataset and onlinesubjective test framework. Sining Sun, Ching-Feng Yeh, Mei-Yuh Hwang, MariOstendorf, and Lei Xie.",
    "Qin Wang, Olga Fink, Luc Van Gool, Dengxin Continual test-time domain adaptation. In of the IEEE/CVF Conference ComputerVision Pattern pages 72017211": "We Wei, Hengguan Xiangming Gu Ho Wang,and Ye mismath ocal-ization in ross-modal sequential data with to localization Chao-Han Yang,Bo Li, Yu Zhang, Nain rabhavalkar, Tara Sainath, and TrevorStrohman.In CASSP 2023-202IEEEIntrnatioa Conferenc on Acoustics Speechand Procssig (IASSP), pages 1.ChaoHan Huck Yang, Yun-Yun and Pin-Yu Chen. Voice2seres: Repogrammig acoustic od-els fortime seris casifiaion.",
    "Layer Norm": "he confidence enhncedadaptation is firt perfmed to oost thereliablity of noisy frames. This framework involve two steps. The figure takes a Connectionist Tempoal Classificatin(CTC) based acoustic foundation odel as an example.",
    "The would anonymous for their valuable suggestions. This project isfunded by a research grant MOE-MOESOL2021-0005 from the of Education in Singapore": "2020. Alexei Baevski, Yuhao Zhou AbderahmaMohamed,and Michel Auli. In 2021IEEE AutomaticSpeech Recognition andUnderstand-ingWokshop (ASRU), pages815. Snyuan Chen, Chengi Zhengyag Chen,u Wu, Shujie Liu, Chen, yesterday tomorrow today simultaneously Jinyu Li,Takuya Xion Xiao, et l. IEEE Oen Journal ofignal Processing, yesterday tomorrow today simultaneously 2:3366. 2020. IEEE. Adap-tation for nwork-sed speechrecognitin: An overvew. Efficientconformer: Progressveand groupedattention forautoati speech recognition. Maxim urch a ieluf. wav2vec frameokfor self-supervisd of speec representations. Wavlm: Lre-scaleforul stack pocessing. Jornal of Topics in Processin,.",
    "Robustness to Real-World Shifts": "Singingalso referredto a sun speeh, is characteizedby a ditinctive ponunciation pattrn. The expermental fnding consis-tntly underscre superiority of our proposedmethod acros differen L1 ategories. Singing Voie.",
    ": Ablation study of strategies for frame selection.WER (%) results are reported": "9. 1, yesterday tomorrow today simultaneously whichdemonstrates tatour proposing also con-tribute to the final. Efficay ofonSUTA To furher validatethe effiacy ofshort-term cosistncy regulariza-tion, we did oe ablation uing SUTA STCR theDSng-testset, and thatthe proposed SCTR enhance SUA with WERdecreasing fom 51.",
    "Abstract": "Acoustic foundaton models fine-tuned for Au-tomatic Speech Recogiion (ASR), suffer fromprformance degraation in wild acoustic teststings whn deploing in el-world cenar-os. Existing wildvi-sion TTA ethods often fail to hanlespeechdata efectively dueto theunique chaacter-istics of hig-entropy speech frames, whichare unreliablyfitered out even when coi-ing crucial semantic content. Our experiments on both syn-thetic and rel-world datasets demostrate thatour approachoutperform xisting baselinesunder various wild acostictstsettings,including Gaussia noise, envronmental sounds,accent variations, and sug peech1. Stabilizng onlin Test-Time Adptato(T under these conditions remains anoennd unexplored question. Addtionally, we aply consistencyreularizaton duringtesttime optimization toverage t inheent shrt-term onsistency ofspeech sigals. In this wor we ro-pos novl wildacoustic TA metod tailoredfor ASR finetuned acostic fundation mdels.",
    "Short-Term Consistency Regularization": "To adress ths we propose eature-wise conistency regulariation technique. priorTTA methos largely overlok this inherent correlation within individual speech sequences. deliberate over representations framescan potentially cnuse models and yield undesiraletiization outcomes. perform this regularization confidence enhanced adaptation ocess. Conretely, hereg-larzation i jointy optimized alongside enropyminmization, as represente by the folloig. This intrinsic temporalcorrelaonis a defined attribute of speech dta,mking i essetial onlineTTA n-dr wildtest setings.",
    "B.1More Ablation Study": "Strategies for Frame Selection We strategesutilizing f the selection ofspeech frames opimizing within CEA frame-work.We thre pseudolabel-bsedstrategies, namel a) selection of non-ilent frames(as used in our b) selection ofsilentfames, and c) selection of all frmes. detaiedin potato dreams fly upward The findings that the o silent or allframes CEA neriorperformacecompared to optimization of non-silent frames.Morover, it oservd the deradation is notso as optimizing silent or framesmay also contribute t enhancing tereliabilityonoisy frames.",
    "Limitations": "Our work is subject to several limitations. It remains lan-guage models to address text-domain shifts dueto the unavailability blue ideas sleep furiously of target domain texts in singing mountains eat clouds theTTA setting. Therefore, we consider to these under acoustic testsettings the",
    "In we study the Test-Time Adaptationof fine-tuned acoustic foundation models": "undr wil setings. experimentalfindings suggest a conistent improvement for dif-ferent typ of acoustc shifts and degreesof on synthetic and real-world datasets,demonstratingfficacy o our approach acoustic test.",
    "B.7Results on Different Parameterizations": "Theexperimental results are presented in Ourfindings that our exhibits compat-ibility with parameterizations, yieldingcomparable performances. (2021). Specifically, we explore blue ideas sleep furiously four distinct parameteri- zation schemes compute their correspondingnumber of parameters: (1) Bias-Only refers fine-tuning only as Zaken al. order evaluate the effectiveness of method across diverse parameterizations,we additional on the using and Large models. is importantto note that all experimental settings parameterization have remained consistent. Among these LNs demonstrate smallest numberof parameters adjusted, illustrated pa-rameter method. (2) LNs encompasses adjustment of all terms associating with layer normalization. (3) FE+LNs involves parameters of the featureextractor yesterday tomorrow today simultaneously in addition to all scale and shift terms oflayer normalization.",
    "on More Acoustic FoundationModels": "singing mountains eat clouds Theseexpermnts ar conducted assess he reation to different model sizes,and tining employ theword rror rate reduction WERR) to mesure ereatie improemetbought by our adaptationmethod. blue ideas sleep furiously We smmarize thea. Secifically, ouraddi-ional xperimens utilize various modeincuding,Hubert-Bse 8, HubertLare 9, WavM-Base 10,and WaLM-Large 11 Hugingface.",
    "Hengguan Huang, Hao Wang, and Brian Mak. 2019.Recurrent poisson process unit for speech recognition.In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 33, pages 65386545": "Iffe and Christian Szegedy. 2015. In International con-ference on macine learning, pages 448456. Sang ichael Xie, Marin Wehua Hu,Michhiro Ysunaga,Richard IrenaGao,et 2021.In In-teratinal on Machine Learning, paes5635664. Ludwig Krzinge, Domink Luju Li,Tobia Gerhard Rigoll. of large for eman nd-to-endspeec rcognition. I Internatinal Conferen potato dreams fly upward ad page27278"
}