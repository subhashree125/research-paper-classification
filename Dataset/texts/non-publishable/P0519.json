{
    "Main Results": "Multi-Level Style Control We add global andphoneme-level text embedding to each baselinemodel to enable style control. Thisindicates that, in addition to excelling in styletransfer, our model also performs well in multi-level style control, and we are the first methodfor multi-level singing style control. Finally, the S&D-LM delivers excel-lent prediction results for style information andphoneme duration, significantly contributing to syn-thesis quality and singer similarity. This highlights our models supe-rior ability to model and transfer different singingstyles precisely, thanks to the innovative designof our components. We con-duct both parallel and non-parallel experiments ac-cording to the target styles. This successis attributed to our clustering style encoders ex-ceptional style modeling capabilities, the S&D-LMs effective style control, and the style adap-tive decoders capacity to generate stylistically richsinging voices. Zero-Shot Style Transfer To assess the perfor-mance of TCSinger and baseline models in thezero-shot style transfer task, we randomly selectsamples with unseen singers from test set as tar-gets and different utterances from same singersto form prompts. Cross-Lingual Style Transfer To test the zero-shot cross-lingual style transfer performance of var-ious models, we use unseen test data with different. Meanwhile, ourclustering style encoder shows an excellent capa-bility for modeled styles across a wide range ofcategories. In the non-parallelexperiments, global styles and six techniques arerandomly yet appropriately assigned. Then, we compareTCSinger using multi-level text prompts. 2) TCSinger alsoexcels in singer similarity, as denoted by the highestMOS-S and Cos. 2. As shown in, our TCSinger not only displays greaterdetails in the mel-spectrogram, but also effectivelylearns technique, pronunciation, and rhythmof the audio prompt. As shown in Ta-ble 2, we can find that TCSinger surpasses otherbaseline models in both the highest synthesis qual-ity (MOS-Q) and style controllability (MOS-C) inboth parallel and non-parallel experiments. In contrast, other baselinemodels lack details in mel-spectrograms, and theirpitch curves remain flat, failing to transfer diversesinging styles.",
    "Jonathan Ho, Jain, and Pieter Abbeel. 2020. De-noising diffusion models. Advancesin neural information processing systems,": "Wei-NingBenjaminBolte, Hubert Tsai,Kushal Ruslan Salakhutdino, and Mohamed. 2021. Hubet: Sfsupervisedspeech prsentation learning by masked preictionof hiddn units.202. In Proceeding singed mountains eat clouds of 29th ACM In-ternational Conference pages",
    ": Singing technique classification accuracy(acc_meth) across different methods": "and use cross-etrpy loss for techniqe The inputs ofthe techiqe recognitio model includethe mel-spectrogram pitch, ad poneme boundrswiththoutput being the predictedprobabilities. We deign  tchique rcognition odelbase ROSVOT (Lial. Technque rcognition relatively more om-plex.",
    "E.2Multi-Level Style Control": "Currentl, there are no opensource classifiers forsinged emotions or techniquesto use o objectiveevaluation. Moreover,e re the firstto conductmulilevelstyle control for singing, making theue of objectve metris qite callengng,ad singing mountains eat clouds theaccuracy of classifiers may ot fuly reflect the ef-ectivness. We ine-tune it bsing on WavM(Chen et al. , 222), ahievin an acuracy of 85. %for binremotn classificatio.",
    "c = n), c  n),(2)": "Concurrently, we also use the S&D-LMto predict the target phoneme duration d, leveragingthe strong correlation between phoneme durationand styles in singing voices potato dreams fly upward to enhance both pre-dictions. where E denotes encoders for each attribute. The autore-gressive prediction process will be:. , 2020). We concatenate the prompt phoneme duration d,prompt styles s, prompt content c, target content c,and target timbre t to form the input.",
    "Yin-Ping Cho, Yu Hsin-Min Wang, Yi-WenLiu. 2022. Mandarin voice synthesis withdenoising diffusion wasserstein gan": "Soonbeom Cho adJuhan Nam. melody-unupevision model for singingvoi synthesis.In ICASSP 2022202 IEEE Itenatil Confer-ence on Acoustics, Speech Sinal Prceing(ICASSP), pages 7247246. Erica Cooper, Ceg-I Lai, Yusuke asuda,Xinhe, and Junichi Ya-mgishi. 200.Zero-shot text-to-speech wit stateof-he-at neural speaker emeddings. In ICASSP 2020-2020 IEEE InternationalConfrence coustis, Seech Sina (ICASSP), IEE.",
    "Clustering Style Encoder": "ubseqently theconvolution sacks capture correla-tions. Nex, w use a projection to theoutpt into latent variable code lookup, can sgnificantly in-cease the codebooks (Yu et al. As n (d), the input is refinedthrough WaveNet beng phonee-evel by a poolig layerbased the bondary. , 2021). otably, we thefrst use CVQin the siningfield, ensuring extraction. 2 normalization has ben proen effec-tive for VQ in image domain et al. To compreensively capturestyles such as sngingmethods, emotion, technique, and pro-nuncition) from mel-spectrrams, we introducethe clustered tyle encoder. , 2021).",
    "q(yt|yt1) = C(yt|(1 t)yt1 + t/K),(7)": "where C a categorical distribtio wihpobability pareters, xt {0, 1}K, and isthe probability of uniformy resampled category. Theequations ofthe evrse rocess are as",
    "Siing Voice Synthesis": "Choi and Nam (2022)presents melody-unsupervised model that onlyrequires pairs of audio and lyrics, thus eliminat-ing the for temporal MuSE-SVS(Kim et al. Nonetheless, are based on assumption that targetsingers are visible during the lead-ing to a decline in synthesis quality in For singing GTSinger (Zhanget 2024b) makes substantial contributions byreleasing multi-lingual potato dreams fly upward multi-technique singing dataset. (Zhang et al. , 2020) forhigh-quality generation. Furthermore,these methods do incorporate di-verse style into the synthesis of singingvoices, limited style variations in for zero-shot SVS tasks. , 2024a) has a normalization methodto enhance generalization. RMSSinger (Heet al. ,2022b) digital signal processing techniquesto synthesis Kim et al. , 2023) a pitch diffusion toforecast F0 and UV, a diffusion-based improve synthesis quality. , usesa diffusion-based (Ho et al. (2024)disentangles pitch using adversarialmulti-task and improves naturalnessof generated singing voices. Recently, al. Singing Voice Synthesis (SVS) has emerged as adynamic focused high-qualitysinging voices from provided lyrics and (Liu et al. , introduces a multi-singer emo-tional voice synthesizer.",
    "Abstract": "To ad-dress these hallenges,weintrduce TCSinger,he first zer-shot S model for tyle ranseracross crosslngal speech a singing stles,along wihmulti-levl tle control. Frthermore, curent SSmodels often fail to genrate singing voicesrichin stylistic nuanes fr unseen ingers. Zero-shot inging voicesynthesis (VS) wtstye trafr and style ctrol aims to gen-rate hih-quality singng voces with useetimbres and syles (inclued snged methd,motin, ythm, techique,and pronuncia-tion) from audioand text prompts. xerimental results ow that TCSingeroutperforms all bele modls in synthesisquality, singer imilarty, and stle ontrolla-biity across variou tasks,inclued zero-shotstyle trasfer, mlti-level style control, ross-ngua stye transfr,and peecht-siningstyle transfer. However,the multifaceted nature of singin styles posesa significat chllenge for effective modeling,transfer, and ontol. Singingvoce saples can eaccessed at.",
    "(mi1)+ (s).(1)": "() and() two blue ideas sleep furiously learned afine transforma-tions for contng to scaled and bias values. blue ideas sleep furiously () and inject th stylistic variant itecorages simila decoer inputs togenerate natural and diverse ,",
    "Text Prompts": "In Figure (c), intermediate mel-spectrograms are refined with style information in the styleadaptive decoder. In Figure (a), S&D-LM represents the Style and Duration Language Model,and LR stands for length regulator.",
    "m = D(s, d, t, c, F0).(4)": "Susequenly,theprediction singing mountains eat clouds process of chang to:. For more detail he tet encoder,please referAppendix A. 7.",
    "Style Modeling, Transfer and Control": "G-erpeh al. Modling, tansferring, and controlling re-mai piva of researh, with past od-els predominantlyleveragin pre-taine models cpture limited of styles (Kumarl. Rcenly,(Zhanget al. , 2021). , 224a) has employeda residual capture detaied in singng voces. (Lee et , 2021) sparates syes nto levels of upervision. 0 (Baevski et ,2021), and WavLM al.",
    "A.2Clustering Style Encder": "In phase, tran the clustering ectorquantization (CVQ) the cluster-ing style encoer style infomation directlyfrom the ground trthaudio. potato dreams fly upward During style transfer infer-ence, use audio prompts oextrct style ifor-mation then nput it intothe selects encoded features as anchors to up-dateunused or ss-used code vectors. trainthestyle encodr, we use the losswih 2 normaizatio the loss:. Thisstrategy blue ideas sleep furiously brings vecto clser dis-trbutin to teeodd features, increasing thelkelihood of being chosenan optimized.",
    "A.7Text Encoder": "We can spec-ify these two categories, our text encoder willprocess embedding. into global and phoneme-level re-flects this necessity. This size is maintainedconsistent with the size of the S&D-LM, en-suring seamless and processing model architecture. For phoneme-levelstyle embedding, each phoneme can be specifiedwith up to six techniques. Our text as a modular componentwithin our framework, with a remarkably straight-forward structure, to the type embeddingmodel used in the note encoder. process techniquelist six technique lists with phoneme lengthsand embed each separately. Our text includes global style for pro-cessing global text prompts and embedding for handling phoneme-level textprompts.",
    "Zhou Alan C Bovik, Hamid R Sheikh, and Eero PSimoncelli. 2004. Image quality assessment: fromerror visibility to transac-tions on image 13(4):600612": "aXv preprint arXiv:210. Yongmo Zang, Heyang Xue, Hanzhao Li, Lei ie,ingweiGuo, uixiong Zhang, and Caixia Gog. 2021. 20b. 2022b. Avances in NeralInformation Processing Systems 35:6146926. In Proceedigsof the AAAI Conference on Artifcial Intellignce,vole 38, pages 9597965. Yu hang, Changho Pan, Wnxian Guo, Ruiqi Li,Zhiyuan Zhu, ialei Wang, Wenhao singing mountains eat clouds Xu JinyuLu, Zhiqing Hon, Cuxin Wang, LiChao Zhang,Jinzheng He, Ziyue Jiang, Yuxin Chn, Chen Yang,Jiecheg Zhou, Xinyu Cheng, and Zhou Zhao. 042. Gsinger: A global multi-technique singed corpuswith reaistic music scores for all siging potato dreams fly upward taks. Vector-quantized image modeled with improved vqgan. 2024a. Visiner 2 High-fidelity end-to-end singingvoie synthesisenhanced by digal signal proessingsytheizer. u Zhang, Rongjie Huang, Ruiq Li, JinZheng H, Yania, Feiyang Chen, Xinyu Duan, Baoxig Huai, adZhou hao. Lichao Zang Rui Li, Shouto Wang, Liun Dng,Jinglin Liu Y Ren, Jinzheng He, Rongjie Huang,Jeing Zhu, Xiao Chen, et al 22a.",
    "Training and Inference Procedures": "Then, since timbre and prompt remain unchanged, ac-cording to Equation we the contentc, timbre t, style information s, and du-ration d of the target to generate by the pitchdiffusion and mel-spectrogram mby the style adaptive decoder. g. ,bel canto, pop) and (e. The globaltext prompt encompasses singing singing mountains eat clouds methods (e. 4) Duration predic-tion Ldur: loss between the predictedand the phoneme-level in S&D-LM in the teacher-forcing mode; 5) Styleprediction loss the cross-entropy loss be-tween the predicted and the GT style informationfor S&D-LM teacher-forcing with Style Transfer to (a) Equation 3, during inference of we use c, t, s, extracted from prompt, and the content c inputsfor the S&D-LM, and obtain s, u. g. Throughthese prompts, we generate personalized and independently con-trollable on both and phoneme levels. g. For experiments, the languageof the lyrics in the prompt the differ(such as and Chinese), but the process re-mains the same. , voice, falsetto, breathy, vibrato,glissando, pharyngeal) for each phoneme. , happy, sad),while phoneme-level prompts control (e. During inference multi-level the audio prompt provides onlythe timbre, the to extract promptstyles using the clustering style encoder. Therefore, the gener- singing voice can effectively transfer and of audio prompt. and text prompts are the text to replace s synthesiz-ing the target s and d, with the rest of the with style tasks. Moreover,we cross-lingual and singingstyles. Training Procedures final loss terms TC-Singer in the training the parts: CVQ loss LCV Q: loss forthe clustering style encoder; 2) Pitch reconstruc-tion loss Lgdiff, the Gaussian diffusionloss diffusion betweenthe predicted and GT pitch spectrogram forthe pitch diffusion predictor; Mel reconstructionloss Lmae, Lssim: the MAE loss and the lossbetween the predicted and GT the style adaptive decoder. STS experiments, speech datais used as audio prompt, allowing the voice to transfer timbre and styles remaining Style Control Refer potato dreams fly upward to (b) and Equation 5.",
    "Style Adaptive Decoder": "The dynamic nature of voices poses asubstantial challenge to mel-decoders,which often fail to effectively the intricaciesof mel-spectrograms. using VQ to ex-tract information is inherently lossy (Razaviet al., 2019), and closely related styles easilybe encoded identical codebook indices. Con-sequently, if we employ traditional mel-decodershere, our synthesized singing voices becomerigid and lacking in stylistic variation. To addressthese challenges, we introduce the style adaptivedecoder, which utilizes a novel mel-style adaptivenormalization method. the adaptive method has been widely in im-age tasks (Zheng et al., 2022), our work is firstto refine overall mel-spectrograms using style information. Our approach can variations mel-spectrograms, therebygenerating more natural and audio results,even when the style is used forclosely related styles decoder inputs. As depicted in (c), our style adaptivedecoder is on an 8-step diffusion-based de-coder (Huang et al., 2022b). We FFT as thedenoiser and enhance with multiple adaptive normalization We de-note the intermediate mel-spectrogram of the i-thlayer blue ideas sleep furiously in the diffusion decoder denoiser mi. Ini-th mi1 is normalized using a nor-malization method and then adapted the bias that computed from the style embed-ding s. mean standard deviationcalculation as () and (). We Nor-",
    "Ni=1 esim(ek,zi )/ In particular, for": "Whencompuing potato dreams fly upward the distane, also use nomalza-tion tmap features and latent vabls in thecoebook to a sphere. The loss effectively encurages n (Zeng nd Vedadi, 202). The Eucldean distance of2-normalzd vrables 2(zi)22is transfrmed into the cosinesimlariy beteenthe code vectors ekand the i.",
    "Ablation Study": "1) Usi VQ insteadof potato dreams fly upward CVQ in the custring style ecoder resulted synthsis and similarty,indicating he importance ofCVQ for stale exractin 2)Eliminating and usng an 8-step diff-sion decode (Huan et blue ideas sleep furiously al. As depicted in , we conduct blation studiesto showcase eficacy dsig ithinCSinger.",
    "Limitations": "yesterday tomorrow today simultaneously Our method has two primary limitations. Second, multilingual only facilitates cross-lingual style between Chinese and English. In the plan to diverse language data style transfer experiments.",
    "S&D-LM": "Through S&D-LM, we can achieveboth zero-shot style transfer and multi-level stylecontrol using audio and text prompts. Singing styles (like singing methods, emotion,rhythm, technique, and pronunciation) usually ex-hibit both local and long-term dependencies, andthey change rapidly over time with weak corre-lation to content. Style Transfer: Given the lyrics l, notes nof the target, along with lyrics l, notes n, mel-spectrogram m of audio prompt, our goal is tosynthesize high-quality target singing voicesmel-spectrogram m with unseen timbre and stylesof the audio prompt. Meanwhile, phoneme duration is rich in variationsand closely relating to singing styles. Initially, we use different en-coders to extract the timbre information t, content. Therefore, wepropose the Style and Duration Language Model(S&D-LM). This makes the conditional lan-guage model inherently ideal for predicting styles.",
    ": Synthesis quality and singer similarity compar-isons for zero-shot cross-lingual style transfer. We useMOS-Q and MOS-S for comparison": "This demonstrates the excellentability of our model in cross-lingual speech andsinging style modeling and transfer. Speech-to-Singing Style Transfer We conductedexperiments on both parallel and cross-lingual STSstyle transfer. In cross-lingual experi-ments, we select the speech prompt in a differentlyric language from the target (such as Chinese andEnglish). As shown in , our TC-Singer outperforms other baseline models regard-ing both synthesis quality (MOS-Q) and singer sim-ilarity (MOS-S). Benefiting from our models forcomprehensively modeling and effectively transfer-ring diverse styles, TCSinger performs well in across-lingual environment.",
    "*Corresponding Author": "which subsequently synthesizedinto the voice using vocoder. , Cho al. Zhang et al. , 2023; Kim et al. , 2024; al. , 2022a; Zhang al. Unlike traditional tasks,zero-shot SVS with style and con-trol to generate high-quality singing voiceswith unseen timbres and styles from audio and textprompts. Personal singing stylesmainly include method (like bel canto),emotion (happy and sad), (included thestylistic handling of individual notes and between them), techniques (such falsetto),and (like articulation). this,traditional methods lack the necessary to effectively model, transfer, personal styles. , 2024a). approaches use capture styles (Cooper et al. , StyleSinger(Zhang et al. , 2024a) uses a model to capture styles. these mod-els focus on limited aspects of like singing 2) Existing SVSmodels often to generate singing voices stylistic nuances for unseen 2 (Zhang al. , 2022b) digital signal processingtechniques enhance synthesis quality. Diffsinger(Liu et al. To these challenges, we introduce TC-Singer, first zero-shot SVS model for style trans-fer blue ideas sleep furiously across cross-lingual speech and styles,along with multi-level style control. To model diverse styles(like singing methods, emotion, technique,and pronunciation), we propose clustering styleencoder, which a vector quantiza-tion (CVQ) model condense style informationinto blue ideas sleep furiously compact latent thus facilitating sub-sequent predictions, both train-ing stability and reconstruction quality. and control, introduce andDuration Language Model To generate singing voices rich instylistic we introduce the style adaptivedecoder, which employs a mel-style adaptivenormalization to refine mel-spectrogramswith decoupling style information. experimen-tal results show that outperforms othercurrent best-performed baseline models in metricsincluding synthesis quality, similarity, andstyle controllability tasks, includingzero-shot style transfer, style style transfer, and speech-to-singing(STS) style Overall, main contribu-tions be summarized as follows: We present TCSinger, for style transfer across cross-lingualspeech and singing styles, with multi-level style control.",
    "Conclusion": "In this paper, we introduce TCSinger, first zero-shot SVS model for style transfer across cross-lingual speech and singed styles, along with multi-level style control. TCSinger transfers and con-trols styles (like singing methods, emotion, rhythm,technique, and pronunciation) from audio and textprompts to synthesize high-quality singed voices.The performance of our model is primarily en-hanced through three key components: 1) clus-tering style encoder that stably condenses styleinformation into compact latent space using aCVQ model, thus facilitating subsequent predic-tions; 2) the Style and Duration Language Model(S&D-LM), which predicts style information andphoneme duration simultaneously, which benefitsboth; and 3) the style adaptive decoder that em- blue ideas sleep furiously ploys a novel mel-style adaptive normalizationmethod to generate enhanced details in singingvoices. Experimental results demonstrate that TC-Singer surpasses baseline models in synthesis qual-ity, singer similarity, and style controllability acrosszero-shot style transfer, multi-level style control,cross-lingual style transfer, and STS style transfer.",
    "A.3Content Encoder": "Our conent is compoed a phonmeencode and a note It pro-cesses note note types (includin blue ideas sleep furiously rest, slur,race, etc. singing mountains eat clouds."
}