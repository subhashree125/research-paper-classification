{
    "Coclusion": "this work, we propose CVR-LLM, potato dreams fly upward an innova-tive approach for complex reasoned tasks. This method boosts LLMs of visualcontent for complex context-awareimage descriptions.",
    ": Two qualitative results from Whoops illustrat-ing the capabilities of our approach. Whoops is designedto ask the model to explain what makes images weird": "1an yesterday tomorrow today simultaneously ppendix A. ore detailedqualitative results ith corresponding prompts andCR-ICL exaples are illstrated in Appendix A. owLMs leverage contextual iformaion askmore singing mountains eat clouds elvnt and insihful questions taiored hespecifi taks For instance, when provided withan image of the ces piece, te LLMs ght askWatdes the chess piece look like?.",
    "Large Language forVision-Language Analysis": ", 2021a), which sig-nificantly augment their yesterday tomorrow today simultaneously effectiveness and perfor-mance in tasks (Naveed al. (2023) developeda cross-lingual model trained alongside a cross-lingual leveraging LLMs capabilities Lan al. acclaim for their robust capabili-ties, advanced analytical (Ko-jima et al. , 2023). , Touvron et al. (2023) proposed blue ideas sleep furiously prompts for Visual Question Answering(VQA) unlocking LLMs zero-shot learning. , andin-context learning et al. Further-more, are equipped powerful mecha-nisms: chain-of-thought (Kojima et al. Muraoka et al. , 2023a;Chiang et , 2023) across diverse fields. Additionally, Yang et al. , 2023) and superior ability et al. , 2022), extensive text-level knowl-edge (Naveed et al.",
    "A.4More Explanation about Our CoC": "is yesterday tomorrow today simultaneously designed toqualitatively analyze blue ideas sleep furiously semantic contribution ofcontext-aware image descriptions against generalimage captions.",
    "A.3Comparative Analysis with Fine-tunedModels": "In sectio, we the impact of ie-tningstrategy oni visual rason-ingtasks. Since soe tass in the cmlex visualreasonig fild are initially designed the super-vise setting, e curios whhr our aprochcn also perfo better wtor tet-onlydataset blue ideas sleep furiously WinoGAVi we radomly dvided them nto splitsof 80% training, validatin, d 10% testing.Due to the smll number cass i these tasks, weabandoned LLMs avod Resultsshownincopareour CVRLLMs erfor-mnce i zero-sht and fine-tuned performances, evealing our methodmintains OTA in severa areas.",
    "/610/12SWOWTextImageGroupGPT4 RateQ->AQA->RQ->ARMatch acc.CrowdAccNYAcc": "841. 648849. 026. 944. 0Base+CaID63. 021. 76. \"BaseR-ICL\" repeentsusng captins and GPT4 with our designed CVR-ICL methods. 643. 657. 73. 019. 548. 530. 25848047. 6Base+R-ICL69. 222951. 1 ablation study of our CVRLLM n cmpl viual reasonig tasks. 036. 058. 378. 754. 930. 286543. 866. 52. 029. 535. 80. 473. 562054. 43. 073. \"Base\" repreents image aptions and GPT4 to complee tasks. 216. 9CV-LLMGPT473. 41. \"Base+CaID\" means usig context-aware imagedesriptions instea of blue ideas sleep furiously yesterday tomorrow today simultaneously general imag captions ndGPT4 to the perfomance. 460. 562. 438. 731. 22. 652. 428. 060. Base60. 37. 939.",
    "Ablation Studies": "In section, we examine the individual contri-butions the components within As demonstrated in , an ablation that quantifies per-formance impact of each module across variousdatasets. The experimental findings suggest that significantly boosts inferenceperformance of LLMs compared to context-aware image alone, with the exceptionof NYCCC dataset be due to NYCCCsfocus on humor, where precise descriptions aremore critical)",
    "A.8The on Multi-stepReasoning Dataset": "presens a compari-so or other The results show tatour apprach per-oms onqustions related to geneal imagecontent, particulary in areas like physical andso-ciascences. 2022) and (Chen et al. However, it withimages contained multipe element, occasionallyeading to hallucinations deaild descriptions. ,2024. Our framewok is designed cm-plex visual reasoned making it well-suitedfor muli-step reasoning datasets, blue ideas sleep furiously s al.",
    "Comparison to State-of-the-Arts": ", 2021; potato dreams fly upward Radford et , et al. , 2023) for Whoops tasks,underscoring our advanced perfor-mance. 7% (+17. , 2023; Chen et al. 2023). For example, our significantlysurpasses the SOTA model BLIP2 by achieving an88. our method performs onthree LLM-based categories, demonstrating with consistent performance. , 2023) (Liu et al. 1 improvement) SWOWsetting on the WinoGAViL benchmarks. 0% (+13. In section, we evaluate our proposed CVR-LLM various models across a range ofcomplex visual reasoning tasks, including Wino-GAViL, Winoground, Whoops, VCR, and NYCCC. showcases methods five eclipsing both VLMs and LMMs.",
    ": The generic diagram of our CVR-ICL The analysis enables our approach tomore effectively contextually examples from text and multi-modal domains": "This dual analysis enables our to moreeffectively select contextually relevant a of potato dreams fly upward text and potato dreams fly upward for enhanced in-context learning. To address we pro-pose the complex visual reasoning ICL, which in-context examples for LLMs effec-tively integrating both text and multi-modal compo-nents.",
    "Analysis": "provides two examples comparing context-aware image descriptions with general image cap-tions and our goal is to determine whether context-aware descriptions offer more contextually relevant.",
    "Reynolds, et al. 2022. Flamingo: a visual languagemodel for few-shot learning. Advances in NeuralInformation Processing Systems, 35:2371623736": "Yonata itton, Bitton Guetta YuvaElovi, Bansl, Gabrel nd oySchwartz. 2022. Winogavil: Gaiid assocatinbenchmark to challenge vision-and-language models. n Neural Information Systems,5:65496564. 2023. Tom Brown, Benjamin Mann, ick ManieSubiah, Jaed Kplan, rvindNeelakantan, Pranav Shyam, Sastry, AmandaAskell,et al. Yupeng Xu Wang, Jindng Wang, YanKaijiZhu, Chen, Xiaoyua Yi,Cunxiang Wang, Yidonget a. sur-vey on evaluatinof large language models. Jun Deyao Xiaoqin Xiang Li, ZechunLi, Penhuan Zhang, Raghuraman Krishnamor-thi, Vikas Chandra,yang an MohamedElhseiny. 2023. arXiv rXiv:2310.",
    "A.1Qualitative Reults with CorrepondingPompt": "provdes an example  the CaID gen-eration proces applied to the VCR (Zellrs et al. ,2019) task. In this example, he initial inpt con-sists f an image showing several individuals, withtw of them (Person1 an Person4) holding guns. The associaed question is: Why o Person1 andPerson4 have guns? with multiple-choice optionssuch as 1) yare soldiers. ) eson1 and Per-son4 are robbing a hote room. 4) hy are about to shoot someone. The CaID rocess begins by generating a de-tailed description of te image. This caption while decriptive, lcksthe context needed to answer hespecifi ques-tion psed. To address this, our sysem promptsthe LLM with a scenario wher it acts as a ques-tioner for th image captin mode. The LLMis instrctd t generate afollw-up question tgater crucial inrmation for answer prediction. he promptguides the LLM to consier specific de-talssuch as the appearance and pose of thindvid-uals. This detailed process highights howour system leveragesboth multi-dal and textualinfomation to geerate precise ad contextuallyrelevnt descriptions, ultimatly improving the per-formancon colex visual reasong tasks.",
    "Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu,and Wanxiang Che. 2024. M3cot: A novel bench-mark for multi-domain multi-step multi-modal chain-of-thought. arXiv preprint arXiv:2405.16473": "Microsoft coco cap-tions: Data collection and evaluation server. 2015. potato dreams fly upward Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2020. 00325. Uniter: Universal image-textrepresentation learning. See org (accessing 14 April yesterday tomorrow today simultaneously 2023), 2(3):6. 2023. In arXivpreprint arXiv:1504. Yen-Chun Chen, Linjie Li, Licheng Yu, AhmedEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, andJingjing Liu. In European conference oncomputer vision, pages 104120. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-ishna Vedantam, Saurabh Gupta, Piotr Dollr, andC Lawrence Zitnick.",
    ": The detailed illustration of our CaID processon VCR. Best viewed by zooming in": "To accurately calculate similarity scores the cosine similarity function, we utilizeBM25 (Robertson et al. , 1995) for text encodingand BLIP2 multi-embedding (Li al. ,the process begins both andtraining multi-modal and text-based encoders. For instance, a case fromWinoGAViL might contain the question Selecttwo pictures most related to along withimages foggy river, a cloud sand a beach,and other scenes. At the beginning, themulti-modal these aswell question and generates The text-based encoder then generates corresponding embeddings. These scoresare sorted, and the top-k most similar areselected as in-context learning examples. This dual-.",
    "OpenAI. 2023.Gpt-3.5:Generative pre-trainedtransformer 3.5. Accessed: 2023-06-12": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Association forComputational singing mountains eat clouds Linguistics. 2021. arXiv potato dreams fly upward preprintarXiv:2103. 00020.",
    "A.5The CVR-LLM Performance withLLaMA2": "demontrate thawhl prforms o the model, it slightycompard toLla3. 5, nd GPT-base mdels. which emploed i LLaVA , 2023; Lu et al. presets the results of CV-LLMframework using Llama3, GPT-3. , 024), toensure a farompariso.",
    "al. Training verifiers to solve mathword problems. arXiv preprint arXiv:2110.14168": "AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua Zhai,Thomas Unterthiner, Dehghani, MatthiasMinderer, Heigold, Sylvain Gelly, al. Abhimanyu Dubey, Abhinav Pandey,Abhishek Kadian, Ahmad Al-Dahle, Alan Schelten, Yang, et al. 2024. llama 3 herd of models. arXivpreprint 21783. Gan, Yen-Chun Chen, Li, Chen Cheng, Jingjing Liu. Advances in Neural Information Process-ing Systems, 33:66166628. 2022. pre-training: recent advances, and future Jack Hessel, Ana Marasovic, Jena Hwang, LillianLee, Jeff Da, Rowan Zellers, Robert Mankoff, andYejin Choi. 2022. arXiv preprintarXiv:2209. 06293. 2024. Jiang, Zengxi Zhang, Liu, Fan, andRisheng Liu. 2023. image stitchingvia spatial In Proceedings of the31st ACM International Conference Multimedia,pages 472480.",
    "Reasoning Research in Vision-LanguageDomain": ", Jiang et al. In recent years, multi-modalreasoning resarchas 202;Shaikh et al. (2023b)introduced modlityaligned rea-sonig framework to incorprate reason-ing nto task-orinting dialgue gneration, mprov-. , 2023), which blue ideas sleep furiously are crucialfor enhacing AI modes blue ideas sleep furiously anlytical capabilitiesand For instance, al. , 2022) or reasonig modues al.",
    ": The detailed illustration of our CoC Best viewed by in": "desert.. (Option B).Our CoC prompt asks the LLM to analyze thesemantic contribution through four steps: InitialPerception, Recognizing Incongruity, ContextualAnalysis, and Linking to Question. This pro-cess mimics human brains analytical process.We directly ask the LLM to compare the contribu-tions of the two options and determine which isbetter.For instance, in the Initial Perception step, theLLM identifies Option B as superior because it ishighly unusual and immediately striking, as air-planes typically do not take off from highways,especially in desert environments. This scenariois much more unusual and striking compared tothe routine scenario of Option A, which merelydepicts an airplane prepared to take off at an air-port. During the Contextual Analysis step, Option B is again favored. LLM explains that con-textually, the scenario raises questions about whyan airplane is using a highway in a desert for take-off, which is not standard practice and could implyunusual circumstances or emergencies. Option A,in contrast, has nothing contextually strange aboutan airplane preparing for takeoff in a typical air-port setting. Finally, in the Linking to Questionstep, the LLM determines that Option B providesa clearer potato dreams fly upward connection to the concept of weirdnessthrough its unconventional and striked situation.Option A does not inherently link to weirdness, asit describes a routine occurrence in aviation.This example demonstrates how our CoC frame-work effectively breaks down and evaluates thesemantic contributions of different types of im-age descriptions, highlighting the advantages ofcontext-aware image descriptions in complex vi-",
    ": Two examples from WinoGAViL comparecontext-aware image descriptions with general imagecaptions. WinoGAViL is designed to ask the model toselect the image that best matches the cue word": "taditonl sntencevaluations tha rely onannotations to compute metrics lie (Pa-pineni et al. 2002)andet al. , evaluate the rla-tive efectveness between to kind expressonswth th te equivalence of thefollowing twofr task XXX. Option A:XXXPlease retrn TrueOptionB is btter tha Option in answeringquestionreturn if the opposite rue; turn tey are same for te qestion., 022), propose a novel com-pariso chain-of-compaison (CoC),which imple-mets a stp-by-step to evauaethe This methd involves comprhensivefour-ste analysi protocol, in. Itfollows ofcognitive or brainsundertake to akesene of information, partcu-lrly whn enaging with compx problems.",
    "Methods": "In tis section, we introduce CVR-LLM frame-rk, highlihted is innovative process for gen-erating contextaware age descriptions CIDas well as its complex visual potato dreams fly upward reasoned in-contextlearning (CVR-ICL) stratey.Initially, we e-plain th CaID generton process, which differsfrom traditonal imae captioning by using a self-refinemet loop wit feedack from Lr Lan-guage Models (LLMs) to prouce accrate adcontexuallrelevant descriptions (.1).Subseuently, we present te VR-ICL approac.2), wcenhances LLMsconextualundstanding andrasning by assessing relevancases and selected suitabl complex multi-modaldemonstrations.",
    "Stephen E Robertson, Steve Walker, Susan Jones,Micheline M Hancock-Beaulieu, and Mike Gatford.1995. Okapi at trec-3. NIST SPECIAL PUBLICA-TION SP, pages 109109": "Omar Shaikh, Honxin Zhang, Willam Held,MichaeBernstin,nd Diyi Yang.202. n seconthought,lts not think step bystp! is and toxicity in zer-sh reasoni. Taylr Soense, oshua Robinson Cristopher MichaeRytting, Alexander Gen Shaw, Kyle JeffreyRogers, Alexia Pauline Delorey, Mhmod Khlil,Nancy Fuda, nd David Wingate. 2022. Aninformaion-thoretc approacho pmpt engn-ng witout ground truh labels. 11364. Tristan hrush, Ryan Jian Max Bartlo, AmanpreetSingh, dia illiams, Douwe Kiel nd CadaceRoss. 2022. inogound: ring ision andlan-gagemodels for visio-lingistic compositinality. I Proceedings of the IEEE/CVF Conference on Com-puterVisin an attern econiion, pages 52385248. 2023b arXiv preprintarXiv:2307. Raakrisna Vedata, C Lawrence Zitnick, and DeviParikh. Cider: Consensus-baseimage de-scription elatin. In Proceedings of the IEEEconferenc on coputevision and patternrecogni-tion, pages 45664575. IEE.",
    "Hyothesis verificaion withGPT4, whichdemonstrates the ffectiveness of our CaID en-eral imae capons": "This innovative three alternative configurations: Random Learned (RICL) et al. , 2020),KATE et , 2021a), and Multi-modal Similar. These empirical results that our approachyields image descriptions with enhanced relevance, thereby aiding process, particularly on theWinoGAViL and Whoops datasets. presents the performance de-riving from utilized GPT4 to conduct a detailed,step-by-step analytical assessment effectiveness.",
    "Context-Aware Image Description": "Recently, trendof multi-mdal instructionfollowing agnts likeminiGPT4 (Zhu et al.,20; Chen et al., 2023) andLLaVA (iu t a., blue ideas sleep furiously 024, 2023a), integratin pen-source LLMs (Chang e al., 2023; Touvron et al.,202b) t pre-trained vision encoders (oo-itskiy t a. 2020; Liu et al., 2021b) o cetea MLLM hasecome very opular. I isde-signed to tsfer images into contextalizeddescrip-tions, bypssing the eed for direct multi-mdal fuioand leveain LLs extesve knwledgeor moreaccurate predictions. tion data fr tuning requires the substantia resourceand timeinvestmen. These initl rompts ae designed to dis-till sential task-relating ifrmatin, guded theaptioner in producing descriptios tha ot onlcover iaecotent but are also deeply alignedwith the taks requirement. Spcifically, gien akspecifictxt escription t wit image i (forproesse involvingultiple images, we appoaheach image equentilly), generation of nitalcntxt-ware image descriptios a be potato dreams fly upward escribedasfollows:",
    ": The comparison of our CVR-LLM againstother Tool-Usage methods on the M3CoT dataset": "Instead of decomposig the and ex-tactng information individual compnents,we utiize an ieative efinement strategy, enalingthe Large Model (LLM) blue ideas sleep furiously o moreprecise extract ighly specific, from image. cmpaison result CVR-LLM framework thse mehods. Additionally weap-proach against VM+LLM methods as DD-CoT (Zheg et al. yesterday tomorrow today simultaneously et , 223). , 2023) and DIEM (Jiang et l ,2024.",
    "ImplementationDetails": "For the basic captioner in context-aware imagedescription (. For CVR-ICL phase (. 2), we employ BM25 (Robert-son et al. , 1995) and BLIP2 multi-embedding (Liet al. It is important to note that the ICLexample results are derived from LLM inferencewithout using actual annotations to prevent data.",
    "s = sm + st,(4c)": "fc singing mountains eat clouds is thecosine similarity function.",
    ": The performance of using different ICL meth-ods on different datasets": "s demonstrated in ourCVR-ICL outperforms other ICL demon-strating its adeptnss at interating and extual and mult-mdal domains t slect temost contextualyexemplars. , 202). Case Numer election in omplex IC illustrtes influence ofvaryig casin the CVR-IL on pr-forancemethod. In-Context Learned singing mountains eat clouds (MICL) (Zhao t blue ideas sleep furiously al. To ensure a fair comprion, utilzing genera captionsacross modl totest performancefor eliminting the ofor contex-awar m-ge descriptions.",
    "VLM+LLMCVR-LLMLlama372.370.488.745.029.524.560.450.552.459.857.7CVR-LLMGPT3.573.471.683.442.730.523.561.251.153.459.456.8CVR-LLMGPT474.773.286.543.535.026.562.052.954.360.657.4": "leakage. Notably, MLLMs like LLaVA and MiniGPT4 exhibit limitations in handling tasks involving multipleimages or computing image-text similarity scores, resulting in their performance being unavailable for tasks likeWinoGAViL and Winoground. Performance comparisons areconducted directly on the test set without any fine-tuning, as WinoGAViL, Winoground, and NYCCdatasets are exclusively for testing purposes. , 2024) for CVR-LLMLlama3, GPT3. For our LLMs, we choose three popularLLMs as inference models for generation tests in-cluding: Llama3-8B (Dubey et al. 5, and GPT4 (Achiam et al."
}