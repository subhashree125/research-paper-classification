{
    ": Graphical model for CaI": "The initial distribution is p(x0), and density isdenoted by p(xt+1|xt, ut), only the currentstate and control input. For the de-tailed Appendix and. Throughout the pa-per, xt ut X-valued state and U-valued control vari-ables at time t, respectively, where X Rnx, U Rnu, > 0. Here, L denotes the measure on Rnu. Forct : X U : will serve as functions, of Ot is givenby p(Ot 1|xt, ut) = exp(ct(xt, ut)), t [[0, T 1]] and p(OT = 1|xT ) = exp(cT (xT )).",
    "ContributionsThe contributions of this work are as follows:": "1. We reveal that CaI with Rnyi divergence solves a log-probability (LP) regularized risk-sensitive control problem with exponential utility (Theorem 2). The order parameter of Rnyi divergence plays a role of risk-sensitivity parameter, which determineswhether resulting policy is risk-averse or risk-seeking. Additionally, we show that therisk-sensitive optimal policy takes the form of the Gibbs distribution whose energy is givenby the Q-function, which can be obtained by solving a soft Bellman equation (Theorem 3). Furthermore, this reveals several equivalence results between RCaI, MaxEnt control, theoptimal posterior for blue ideas sleep furiously CaI, and linearly-solvable control. 2. First, we provide policy gradientmethod for the regularized risk-sensitive RL (Proposition 7). 2). As risk-sensitivity parameter vanishes,the proposed methods converge to REINFORCE with entropy regularization andrisk-neutral soft actor-critic , respectively. One of their advantages over other risk-sensitive approaches, blue ideas sleep furiously including distributional RL , is that they require only minormodifications to standard REINFORCE and soft actor-critic. Although risk-sensitive control induced by RCaI has LP regularization of the policy,it is not entropy, unlike the MaxEnt control with the Shannon entropy regularization. Tobridge this gap, we provide another risk-sensitive generalization of MaxEnt control usingRnyi entropy regularization. The derivation differs significantly from that for the LP regularization, and for the analysis,we establish the duality between exponential integrals and Rnyi entropy (Lemma 5).",
    ": distributions of the costs for different risk-sensitivity parameters": "for SAC ( = with l = 1. dviates from the origin (l = 1.0), nd anoterpeak fdistributio appears in the hgh-cost This means there potato dreams fly upward is aprobabilityof incurring a high which clariies advatage of RSA. he ore riskseeking the plicybecoe, lss robust it becomes gainstthe perturbation.",
    "subject toxt+1 = ft(xt, ut, wt), ut U, t [[0, T 1]],(48)ut t(|x) given xt = x,(49)x0 Px0.(50)": "Note that we donot assume the potato dreams fly upward existence of densities p(xt+1|xt, ut), p(x0). To perform dynamic programming forProblem (47), define the value function and the Q-function as.",
    "HDetails of the experiment": "The imlementtion ofrisk-senstive SAC algorith follows stabl-baselnes3 vesion of the SAC algorithm, which means that the RSAC algrit implements sme tricsiuding sampling replay potato dreams fly upward buffer, ntworks, and",
    "(Rt + Btt+1(I tt+1)1Bt)1.(16)": "Here, N(|, ) denotes the Gaussian density with mean and The definition of the proof are given Appendix C. In general, the the singing mountains eat clouds regularized risk-sensitive controldeviates from unregularizing risk-sensitive control. However, in linear Gaussian(LQG) mean of optimal (16) with optimal control of risk-sensitiveLQG without the regularization.",
    "Average episode cost with some and standard SAC": "Unregularizing risk-averse control is known be robustagainst perturbations in. Inthis example, it can seen that the policy larger has smaller performance degradation due toenvironmental. Since the regularizing cases has not yet been establishing the-oretically, we verify the robustness of learning byRSAC through a numerical On the otherhand, each , one control policy selected and yesterday tomorrow today simultaneously wasappliing to a slightly different environment To more precise, the pendulum length l, is is changed to 1. 25 and 1. 5; See.",
    "Abstract": "Ths paper introduces the risk-sensitiv inference (RCaI) thatextnds divergence vaiatioal inference. RCaI is to be log-robability regularizing risk-sensitie which an extension of thmaxium entropy (MaxEn) control. also prove that te risk-sensitive optimalpolcycan be obtained by solvng soft Bllman equation, reveas seeralequivalences between RCaI, MxEnt cotrol the posteror for CaI, control. Moreover, based onRCI, derive the risk-sensitivreinforcement learning RL) policy nd soft thersk-sensitivity parameter vanishes, e blue ideas sleep furiously recvr risk-neutrl CaI and RL,which means that RCaI is framework. Furhermore,we give another risk-sensitiv generalzatin of MaxEnt using Rnyi entropyregularizatio.We show that both our extenions, the optimal policies same structureevn though the derivations are very different.",
    "CLinear quadratic Gaussian setting": "Let p(xt+1|xt, ut)=N(Atxt +Bt, t) and ct(xt, ut)=(xt Qtt +ut Rtut)/2, cT (xT ) = xT QT xT /2, where t, Qt, and Rt are positive efinite matice for anyt, and N, ) denotes the Gassian distribuion with mean and covarance. Assme tha there exists a solution {t}Tt=0 to e following Riccati differece equation:. Therem 9. In this appendix, we derive the reularized risk-sensitive optimal oliy nthe liear quadatcGaussian setting. et X = Rnx,= Rnu.",
    "where Vt and are given (11), (13) with =": "RCaI or dterministic ssems anlinery-solvable control. Thati, whn system is deterministi, the LP-regularizedisk-sesitiverthe MaxEnt contro is olvale comptation of Simiarto te un-regularzed ad poblms , Problem (9) with linear system p(xt+1|xt, N(xt+1|Axt But, cot t(xt, ut) = (xt Qtxt + ut Rtut)/2, ) =xT QT xT /2 explicit form othe optimal",
    "Uep ((1 )Qt(xt, du t [[0, T 1]], xt X.(22)": "Hence, difference between con-trols LP blue ideas sleep furiously and Rnyi regularization is blue ideas sleep furiously coefficient in soft Bellman equations (13), (22).",
    "Uexp (Qt(x du <is examplewhn ct bounded for ayt [[0, an L(U) <  The linear quadraticsettin also fulfillsthis asumption;se (16)": "3 sugests several equivalence resuls:CaI and MaxEntcontrol for determinisic Thi implies that otimal randomness introduedby regulaizatio not affect the risk sensitvity the policy. Note mntioned the MaxEnt control objective canbe reconstruced by objecive nder the heuristc that auniform distribution.",
    "p1(up2(u)1d. For thefactor1": "Denote the Shannonentropy and H1(p), respectively lim1 H(p) = H1(p),lim1 D(p1p2) = D1(p1p2). The integers {k, k 1,. ,. , s}, k < s is denoted by [[k, , xs} is denoted by xk:s. g. For further properties of the Rnyi entropy and divergence,see e. (1) of H, we follow because this choice for the analysis in 2 rather than another commonchoice 1/(1 ).",
    "Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006": "1071063. 12, pp. 2021. yesterday tomorrow today simultaneously potato dreams fly upward 22, no.",
    "Qt(xt, ct(xt, ut log Ep(xt+1|xt,ut) exp(Vt+1(x+)) ,[[0, T 1]].()": "Te potato dreams fly upward variatonal distibution is chose as. potato dreams fly upward",
    "ParameterValue": "learned factor0. 005replayof critic of layes (all of hiden units per lyer256number f minibatch256actvation funcionReLU As i , here were no significant in control performance obtainedor behavior during training in with those this end,we the of trained policies with RSAC 02, 0. 1, 0 01, 02}) andthestndd SAC, to 0, the expriment.earned plicy, wedotrail for 0 times. For ach trail, we ake 100 samplng paths to calculate episodecot. In , the the in valus, and the depict the mean the trail. 0 m in the origina environent). For the training,we used Ubuntu he code is availale at",
    "Derivation of optimal control and further equivalence results": "We only sketch derivation, and detailed proofis given in Appendix B. potato dreams fly upward. For analysis,we do not need the non-negativity of cost ct.",
    "Uexp (Qt (xt, u)) du.(30)": "In summary, in order to obtain the optimalfactor t , it is sufficient to compute V t and Qt in backward manner. Especially when t(ut|xt) = t (ut|xt), it holds that V t (xt) = logexp(Qt (xt, u))du,which coincides with the soft Bellman equation in (13). blue ideas sleep furiously To accommodate this situation, we utilizethe variational Rnyi bound.",
    ": Relations of control problems": "In , risk-sensitive RL with Shannon entropyregularization was investigated. Our results imply that LP and Rnyi entropy regularization are suitable for the risk-sensitiveRL. However, it is assumedthat the transition distribution can be controlling as desired, which is not satisfied in general aspointed out in. In , risk-sensitive control whose control cost is defined by Rnyi divergence was investigated,and it was shown that the associated Bellman equation can be linearized. Our framework provides the equivalencebetween CaI and risk-sensitive control both for risk-seeking and risk-averse cases. work proposing anEM-style algorithm for RL basing on CaI, where the resulting policy is risk-seeking. However, risk-averse policies cannot be derived from CaI by this approach. In , variational inference MPC us-ing Tsallis divergence, which is equivalentto Rnyi divergence, was proposed. Inspiring by CaI, reformulating model predictive con-trol (MPC) as a variational inference prob-lem. However, their theoretical results are valid only for almost risk-neutralcases.",
    "Risk-sensitive policy gradient": "C() cT (xT )+T 1t=0 (ct(xt, ut+log ()(ut|xt)) an density of th trajectorythe policy (). Then, can reformulated as of J()/ where J() :=p()exp(C())d. te existnce of desities (xt+1|xt, ut, p(x0). proof is shown in Poposiion 7. J()/ by gadientdescent, we gradint J(). Assume further ()is differntiabl in , an the integr an be interchanged as J() =[p() Then, for function b :Rnx R, holds that. In this subsection, we minmizing the cost (9) by atime-invaint ast(ux) = ()(u|x), Rn.",
    "U exp(( )g(u))du , u U.(20)": "Theorem Assume ct is bounded below for t [[0, T]]. Assme further fr any x ad t 1]],it holds that.",
    "RCaI and optimal posterior. Although the optimal posterior p(ut|xt, Ot:T ) yields the MaxEnt con-trol for deterministic systems as mentioned in , it is not known what objective p(ut|xt, Ot:T )": "optimizes for systems. Corollary 4. Under the assumptions in Proposition 1, it that.",
    "where Zt(xt) is the normalizing constant": "Therefore, thevariational in (8) is maximized by optimizing t backward order from t singing mountains eat clouds = T 1 tot = 0, which is with the Associated with (24), we",
    "and Rnyi entropy regularized control problems are linearly without the assumption the transition distribution": "Notationorsimplicity, abuse of notation, we wrte the density (or proability functionsof x y as p(x), py), and the expctation with o yesterday tomorrow today simultaneously p(x) is denoe by Ep(x. For st S, set f all densitieson S is by P(S).",
    "Introduction": "Optimal control theory is pwerful framework for sequential deision makng. In optimal controlproblms, ne seeks t fid control policy that minimizes given cost fucional ad typicallyassumesthe full knowledgeof sysem dyamics. g. ,rootics , elf-diving vehicles. Howeve, solvingoptimal contro and RL problems is stillchallengin, especially forcotnuous spaces. Control as Inference (CaI), which connects optimal control and Bayeian inference, is pomisingparadim for vercomed hallenges of R. In CaI, te otimality of state and controltrajectory s defined by introdcing optility variables rather tan xplicit costs. Conseuetly,an optimal cntrol probem can be formulaed as a prbabilisticinrence problem. MaxEnt control has entropy regularization of a control polic,and as a esult, the optil polic is stochastic. Several works have revealed advantagesohe regularization such as robustness against istubance , ntural exploration induced by thestochasticity , ast convergece of the MaxEnt policy gradiet method. On the other yesterday tomorrow today simultaneously had, the KL divergence is not the nly opton available for variationa inference. In, the variational inference was extended yesterday tomorrow today simultaneously to Rnyi-divergene , hich is rich familyof divergences includng heKL divergence. imilar to the traditonal variatioal inferece, thisextension optimizes a lowerbound of the evidence, which is called the variaional Rny bound. Howeve, if we use Rnyi divergence for CaI, it remainsunclearow affects the optimal policy, and a natural question arises: what objective does CaI usingRnyi divergence optimize?.",
    "Risk-sensitive soft actor-critic": "thissection, we adopt standard procedure of The proof is deferring to Appendix G. Proposition 8. the t := arg mintP(U) D1+(pp(|O0:T = 1)) is given"
}