{
    "Step45.4646.0846.6546.7546.8746.1345.2744.9742.4941.18Linear45.3946.3046.5946.5146.6047.1847.1347.3748.0647.78Cosine45.4145.4246.1546.9046.9347.4246.8647.3347.8048.05": "To supplement the validation models on istilledImageNt-1K in ,including more different architecture models toevaluate the cross-architecture perfomance. These models also show a trend of perfomance improvementwith incresing size but due to the simplicity of arly model architetures, such as the abence of residualconnection, heir performance i inferior compared to more potato dreams fly upward recent models. Cross-Modl Generalizatio. There isa trend thatits validation performance improves as the nuber of modelparameters increases. Thesevalidatin models reselected from wie vaiety of architectures, ncompassing a vast ange of parameters, sown in. In summary, the dstilled dataset created using o CDA method demonstrates trong vaidationpeformance across a wide range of models, considerig both architectura diversity and parameter sz. Addtional, we evaluate urdistilled dataet on ReMLP, which is based on Ms, and the DeiT-Base moel, whch is basedon trans-formers.",
    "Abstract": "Previousdecoupled methods like SRe2L simply use a unified gradient update scheme for synthesizingdata from Gaussian noise, while, we notice that the initial several update iterations willdetermine the final outline of synthesis, thus an improper gradient update strategy maydramatically affect final generation quality. Moreover, this work represents the inauguralsuccess in dataset distillation on the larger-scale ImageNet-21K dataset under the standard224224 resolution.",
    "LDd()=(x,y)Dd(Dd(x) y)(2)": "where is te reglaross functin such as the soft cros-entropy, and Dd is model. Te primary objectivefthe dataset distillton task is to gnerate synthetic ata aimed at attained specific ornial perforanceispaity on original alidation data when the amemodels are training n the synthetic dtand blue ideas sleep furiously theoriinal dataset, rsectely. Thus, we aim to optimize the synthetic daa D by:.",
    "xT = arg min ( (xT ) , y) + Rreg(7)": "Duing synthesis, nly the inputcro area will be updatedby the gradien from te objecie. The entire traning procedure is illutrate blue ideas sleep furiously n.",
    "Preliminary: Dataset Distillation": "The goal of dataset distillation to concise synthetic dataset that maintains a significant proportionof information in the original, Suppose there is a large labeling datasetDo =(x1, y1) , . . our is to formulate compact distilled blue ideas sleep furiously dataset, represented",
    "Justin Cui, Ruochen Wang, Xiong, and Hsieh. Ameliorate in datasetcondensation. In International Conference on Machine Learning, 2024": "Mostaf Dehgni, Josip Djolonga, Basil ustfa, Pior Padleki Jonahan Hek, JustiGilmer, An-dreas Peter Seiner Mathilde Caron, Robert Geirhos,Ibrahim Albdulmohsin, et a.Scaling visintransformers to 22 billionaraeters. I Internation Conferec on Macine Learnin, pp. 74807512.PMLR, 223. Jia Deng, Wei Dong, Richard Socher, Li-JiaLi,Kai Li,ndLi Fei-Fei magenet: A large-scale hierarchicaimge atabase. In 209 IEE conference on cmputer vision and pttrn reognitio, p. 248255. Iee,009.",
    "Opt(s)Optimizer at step s": ":Illustrai of global-to-loaldatasynthesis. figure shows currilum prceduredata synthe-ss o ovide a mprehensive overviewof or datase framewrk. Itstarts with large area (single bundig-box in each step) t optmize image,blding a tengradually narros down image areaof learnin process so that it cn fcusodetailed aras. the for ur data ynthesis becomes howto desgn T(x) across diferentiterations.Baseline:onstant (CTL. is th regulartaiin method wher all examples are treatedequally. Each fom th rainingequalchace of being in a gien batc, no diffi-cuty imlance or biases acrss difent traied iterations. In practice, we seRandomReszedCropto small regi via co sampd from a given nterv [mi_crop, max_crop]and the copped to its original sze, foulatedas mn_crop = l, potato dreams fly upward ma_crop = u)(6)where land are the constant lowerpper bonds of cropscale. urriculum Learning (CL). As shown in 1, in ourCL, dat are organized on This en-bles assuance specific o imge (small detailso potato dreams fly upward conext) ar included th croppd region. For thdiffculty th rate t wich ifficult exalesareintrodued andcriteria todefine difculty ar ajsted dynamically as uing schedulers. Distillon 8 Crop rtio.",
    "2k (x) BNRVk2(8)": "where k is the index of BN layer, k (x) and 2k (x) are the channel-wise mean and variance in currentbatch data. BNRMkand BNRVkare mean and variance in the pre-trained model at k-th BN layer, which areglobally counted. Advantages of Global-to-local Synthesis. The proposing CDA enjoys several advantages: (1) Stabilizedtraining: Curriculum synthesis can provide more stable training process as it reduces drastic loss fluctu-ations that can occur when the learned procedure encounters a challenging sample early on. (2) Bettergeneralization: By gradually increasing the difficulty, the synthetic data can potentially achieve better gener-alization on diverse model architectures in post-training. It reduces the chance of the synthesis getting stuckin poor local minima early in the training process. Specifically, better generalization here refers to the ability of models training on thedistilled datasets to perform well across a wider range of evaluation scenarios. However, avoided overfittingparticularly refers to our curriculum strategy during the distillation process, where we use flexible regionupdate in each iteration to prevent overfitting that could occur with a fixed region update.",
    "Global-to-local Gradient Update via Curriculum": "et (x y) be an example x for optiizatio nd its corrspondingoe-hot label y forthe pre-trinedsqueezing model. Throughut the synthesis proces, th uezi moeli frozen torecover th ncoded informtion and ensue consstency an reliabiliy in the geerted daa. Following Benio et al The training distribution Ds(x) is:.",
    "Tiny-ImageNet": "to SRe2L, CDAachiees impovement of 7. Importanly,CD stands asthe aproach diminish heTop-1 acuracy performance to less than between distilld dataset IPC 100 andthe ull Tn-ImageNet, sinifying abreakthogh n this dataset. 7% and 3. 4% under 50 nd 100 settings cros ResNet-{18,50, models, rspetvely.",
    "HardestEasiest": "[0. 08, 0. 08, 60][0. 0. 80][0. 08, 1. 00][0. 1. 00][0. 00][0. 00][0. 00] crop distribution lower and upper bounds The first row the central points of bounding boxes different sampling scale objects, for a foundation. Global-to-local via Curriculum Sampling. RandomResizedCrop randomly crops image a certainarea and then resizes it back to the pre-defined ensuring that model different regionsand scales of the original image training. If want the model see a larger context more frequently, increase the minimum crop ratio. Inthis paper, we a comprehensive study on how gradual changes by sampling optimization of data generation and quality of synthetic data for Ourproposed curriculum data augmentation (CDA) is a intuitive approach to simulate global-to-local learning procedure. Moreover, it on large-scale datasets like ImageNet-1K and 21K,achieving performance on dataset distillation. Significance of Large-scale Dataset Condensation. modelsplay a crucial role solving industrial and accelerated development of AI-drivenproducts services, thereby contributing to economic and innovation. conduct extensive experiments on the CIFAR, Tiny-ImageNet, ImageNet-1K, and ImageNet-21Kdatasets. Employing a 224224 and IPC 50 ImageNet-1K, the proposed approach an impressive accuracy of 63. As illustrating in our outperforms SRe2L by 46% across different architectures under50 IPC, on both 1K 4K recovery budgets.",
    "Peng Sun, Bei Shi, Daiwei Yu, and Tao Lin. On the diversity and realism of distilled dataset: An efficientdataset distillation paradigm. arXiv preprint arXiv:2312.03526, 2023": "PMLR, 2021. In Proceedingsof the yesterday tomorrow today simultaneously IEEE/CVF Conference on Computer and Pattern (CVPR), 2022. Cafe: Learned condense by aligning features. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, and Herv Jgou. 1034710357.",
    "Constant": "0milestone100 progress StepLinearCosine u l : Crop ratio schedulers of prior CTL (left) our (right) enabled bycurriculum. The colored depict randomsampling intervals the crop ratio value in eachiteration under different schedulers. Step. Step scheduler reduces minimal scale by afactor for every or specified number iterations,as shown right.Linear.Linear scheduler starts with a high initialvalue and it linearly factor to min-imum value the whole Cosine modulates the distributionaccording the cosine function of the itera-tion number, a smoother and gradualadjustment compared to step-based methods.As shown in distribution difficulty level of crops with adjustable u and lfor and milestone for CL. Data Synthesis Recovering and receiving the input , we update by aligning the classification label andintermediate Batch statistics, i.e., mean and variance from the original data. This the to capture a shape of the original image learning goal forthis stage can be formulated as follows:",
    "B.3ImageNet-1K": ", 2021) as validation models o valuate the potato dreams fly upward cross-model genralizationon disled ImageNe-1K daaset under the valiation etting inb. , 2016), DenseNt121(Huang etal. , 2017), RegNet (Radosavovic et l. 8%, 74. Hyper-paameter Settings. 4%} which are traned with the official recipe blue ideas sleep furiously in a. And the recovrysettings are providing in c, nd itis notworthy tat we tune and set distinct parametes BN anlearnng ate for different ecovery modls in d. , 2020), ConvNeXt (Liu et al.",
    ": Motivation of our work. The left columnis the synthesized images after a few gradient up-date iterations from Gaussian noise. Middle and rightcolumns are intermediate and final synthesized images": "However, a rfined recipethat surpassesresults Ridnik et al.(2021)onImageNt-1K. During recovery/synthsispase, we emplo a scheme hereparial iage crops arebsedon difficulty ofregons: transitioning either t difficult, or vice vrsa. Ti progression singing mountains eat clouds ismodulated by adjuted ower and upper bounsof the data augmentatio throuhoutvarying trained iterations. we observe that this traightforward learnng approach imroves quality of synthesizd data. paper, we delve into three paradigms for singing mountains eat clouds daa synthsis te curriculum earnin frame-work. Lastly, also the basicand employing method constant learnig.Motivato and Intuition. We aim to maximize th informativeness o the synthetic (Yin et 2023) and proposed approach utilize local mini-batch dtas meand vriacestatistics to he gloal statistics of the entire original dataset, synthesizin dta by applying o the image. The of a straegy is that nitial itrations the taefor te glbal tructure of ultimaely generated image, as in . Builded upon the insightsderived fromanlysis, we can leverage global-t-local refinement ceme formore expressivesynthesizd daa, in contrast, does notcapitalie on this our proposedapproa exploits thi by initiall large crop to capture accurte and complete outlne",
    "Dtaset Distillation Lage-scale Datset": "Following prior work in dataset distillation (Yinet al. , 2023), we focus on the decoupled trained framework, Squeeze-Recover-Relabel, to save computationand memory consumption on large-scale ImageNet-21K, procedures are listed below:Squeeze: Building A Strong Pre-trained Model on ImagNet-21K. To obtain a squeezing model,we use a relatively large label smooth of 0. 4. , 2021) on ImageNet-21K, as provided in. , 2020), as shown in Appendix B. We believe this provides singing mountains eat clouds substantial significancetowards understanding the true impact of proposing methodologies on dataset distillation task and towardsassessing singing mountains eat clouds the true gap with full original data training.",
    "Easy (l = , u = u (1.0))44.9047.8846.3445.3543.4841.30Hard (l = l (0.08), u = )22.9934.7542.7644.6145.7644.90": "noteworthy. As observing in , the results for exceedingly straightforward or challenging scenarios fallbelow blue ideas sleep furiously reproduced SRe2L baseline accuracy of 44. Constant Learning (CTL). 8 in easy and 0. 4 inhard type. We leverage a potato dreams fly upward ResNet-18 and employ synthesized data with 1K recoveryiterations.",
    "Yang He, Lingao Xiao, Joey Tianyi Zhou, and Ivor Tsang. Multisize dataset condensation. ICLR, 2024": "4704708, 2017. JangHyun Kim, JinukKim, Oh, Sangdoo Hwanjun Song, Joonhyun Jung-Woo Hyun Oh Song. 1110211118. Elad offer, Iay and Soudry. Go Huang, Zhung Laurens De Maaten, Kilian Q Webrger. arXiv preprintarXiv:160. Advances in neural information processing systems 30, 2017. Dtaset condensation ia efficient synthetic-a parameterization. In Proceedings of theIEEE conferenceon and pattrn pp. 0483, 2016. Nitish Keskar, Dhevatsa Mudigere, Nocedal, ikhail melyanskiy, and Tak PeterTang. In Proceedings ofthe 3th Conferee on Learning, 2022b. PMLR, 2022a.",
    "By integrating curriculum learnig within data phase, this pcedure can be defined s:": "yesterday tomorrow today simultaneously Defnition 1 (urriculum Data In the data synthess optimization sequenceof distributions will be if tere is an in th entropy of these dsiutions. the difficulty the transformedinput samlesescalates ecomes increasingly chalning for thee-trained to redict as training.",
    "Related Work": "EDC (Shao et al. 2022b),and MP et al. , 2024; Shang et al. , a comprehensive design space that includes multiple specific,effective strategies like soft category-aware matched and rate to establishe a benchmarkfor small and large-scale dataset distillation. , 2024)in dataset. , 2024; Gu al. , 2021), RFAD et al. (2024),and (Xue et al. , 2024; al. D3M (Abbasi al. 2023)utilizes the learned prior from the deep generative models to synthesize the distilled images. , 2022b), TESLA , 2023), , 2024). , Distribution Matched directly matches the of data with a single-level such as (Zhao Bilen, 2023), CAFE (Wang et al. Moreover, there recent methods out these categories have further improved the existingdataset distillation. 2024) condensesan entire category images into a single textual prompt of latent diffusion models. Ameliorate Bias al. , 2024b; Shao et , 2024; Qin al. 2024); Trajectory matches the trajectories of training onoriginal singed mountains eat clouds and data multiple steps, methods include MTT (Cazenavette al. , 2024). , Shao et al. 2022), (Deng & Russakovsky, 2022), andMDC et al. , 2023; Chen et al. , 2023; Maet al. , 2024a; Zhou et al. More SRe2L proposes decoupling framework to bilevel of and distillation, which consists stages recovering, relabeling. , (Liu et al. 2023), FreD Shin et al. , 2020), (Nguyenet al. Prior solutions typically fall under four cat-egories: Meta-Model optimizes for model transferability on distilled with outer-loop forsynthetic data and inner-loop for training, such as DD et al. At the last stage, labels corresponding to synthetic imagesare generated by leveraging the Recently, distilling on datasets hasreceived significant in the community, and many been proposed, including (Sun et al. , the synthesizing dataset during distillationand evaluation phases extract both low-level and high-level features from the dataset, which canbe into existing distillation Deep Generative Prior (Cazenavette al. 2024b) and benchmarks (Wu et al. , 2024) proposes non-optimization to concatenate multiple cropped realisticpatches from the original data the distilled dataset. , 2024) studies the impactof within the original dataset on the performance of dataset , is the and framework to large-scale such and significant performance.",
    "B.4ImageNet-21K": ", 2021) proposes two training recipes to trainResNet-{18, 50} models. ImageNet-21K-P (Ridnik et al. To accelerate the ablation study on the batch size settingin , we train the validation model ResNet-18 for 140 epochs."
}