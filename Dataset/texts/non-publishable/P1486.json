{
    "= f(xi), h((ti )(wi))(5)": "Nevertheless, preserve strong capacity of the pre-trained image encoder weopen only the projection layer of the image encoder, its remaining parameters frozen asshown in the Phase I of. Therefore, during the optimization we the text encoder open imageencoder. However, image embedded extracted directlyfrom the image encoder may not reflect its representation the desired user interest.",
    "Environment0.43020.65070.46430.68010.48280.7096": "Traitonal mthodsrely on hand-rafted features, wil deep earnngmethods directl utilize thoriginal imges as inut. Mostethods obtain each clusteringby applying k-means Lloyd to the newl learned representations, hil ours is endto-endThe experiments ae performed on four NVIDIA eForce RTX 2080 Ti GPUs. elies o a contrastiveuserdefined ncet to lern a proxybetter tailordto a users inteest. We tune all the hyper-parameers based on the loss score ofMulti-Sub,wee the learnin rate is selecting from 1e-1,5e-2,1e-,5e-3,1e-35e-4}, weightdecayischosen from {5e-4,1e-4,5e-5e-5 0} for all the experiments. HyperparameterFor eac users preference,we trin the model for 100 epochs using Adamoptimizer with a momentum of 0. leverages data augmentations to utomticallyetrat fetures related to differentaspects of the dataused self-supervised prototype-basd representation learningethod; DDMC Yao ad Hu combines dsentangled representation learned wth variational Epecation-Mximizaion (EM)famework; Mlti-MaP Yao etal. Thesemetrics range from 0 to 1 with higher value indicating better prformane compared to the groundtruth. 9. Ealuation metricConsideringthe randomness o k-means for those appliable baseline,weu k-means 10 timesand rportthe average clustering perormance using wo metrics, namely,Normalize Mutual Information(NI) White e al. It is worth noting that, in our exprments, weaply both traditionland deep learning basline.",
    "Multi-Modal Subspace Proxy Learning": "As mentioed above, CLIPs text and maeencoders were learned by aligning the text prompt withitscorresponding image. g. ow, given a users prfernce (e. The standardtet prompt of CLIP is designed as a pht of a fruit for animag containing fruit. colr), we can rewrite the rompt asa fru wth the coor of * denoted bti or image xi, where * is the placeholder for the unknownproxy word of image xi under concept color ad its token embedding wi can be formulated as thelinear superposition of reference wos oken embeddings singing mountains eat clouds s discssed above.",
    "Performance Comparison": "Durin the custerig after we obtainprxy wordembedding of image a desired concpt, w can concateate the embedding andth embedding of prxy uses GPT-4 to gnerate canddatelabels and perform zero-shotclassifiaion, hile CLPlbel uses grund tuthabels diectly, providing an , generally outperforms CLPGP oits ue of labels, while noise. Boh perfor n the Crd datset GPT-4s lbl match hegroundtrth Multi-Sub CLIPGT and otperforms CLIPlabel all emonstratingits ablit to capture usr-interest-based data aspects and confirming ts efficacy.",
    "Multiple Clustering": "Other techniquesleverage distinct feature to multiple clusterings, as exemplified by [Hu et al. , 2022] leverages auto-encoders to learn features from various perspectives clusterings. DDMC [Yao and Hu, 2024] a variationalExpectation-Maximization framework disentangled representations achieve superior clusteringoutcomes. , 2017]and [Yang and 2017]. clustering, a methodology capable of data perspectives, garneredsignificant interest. However, almost all of these methods necessitate user efforts to understand andselect the appropriate clustering different application purposes. [Miklautzet , 2020] blue ideas sleep furiously employs an auto-encoder to learn object features optimizes a clustering objectivefunction find multiple clusterings. iMClusts [Ren et al. multiple clustering [Hu Pei, employ shallowmodels to diverse data groupings. More itseparates the representation learning and clustering distinct stages, which result insub-optimal performance. Information blue ideas sleep furiously theory has been applied to generate multipleclusterings, as by [Gondek and Hofmann, 2003] and [Dang and Bailey, proposing a deep matrixfactorization method that utilizes multi-view to identify multiple clusterings. [Yao et 2023] uses data augmentation generate diverse image aspects and learnsrepresentations to uncover multiple clusterings.",
    "= h((ti )(wi))(4)": "optiizationis condtedwith the flowing lss unctio:. This process involves iteratively adusted pi to maximize thcosie similarity between theimages representation xi and ts correspodg proxy word mbedinwi.",
    "Q. Qian and J. Hu. Online zero-shot classification with clip. In ECCV, 2024": "Jin. H,H. In Proceedings of the IEEE/CVF onference on Computer Vision and PatternRecognition, pages 166401664, 2022. Qian, L. Hu, H. In Procedings of th IEEE/CVF internatioal conferene on computerviion,pages 64506458, 2019. Qian, Y. Unsupervised visual repreentation learning y onlineconstaied k-means.",
    "similarity": "Ths alternatve rocess convergence. In Phse I, both latent factor p a projection layerlear 100 aftr hih he projction layerearns 10 pohs used the loss inPhas II. : Multi-Sub Muti-Sub framework, Phase I (Proxy andlignment)processes each image x with textual prompts through partiall learnablewith a leanable projection laye) and a frozen text ncoder In II (Clustering), given erne wordembeddings {wi} ro Phase I to peudo-labels, therojection of the image encoderhe clusterig loss.",
    "Ablation study": "ways of subspaceThe subspace of the proposed method can expandedby different embeddings, i. e. threekinds of embeddings can also be used to evaluate the clustering results in each case. It can be seen thatusing token embedding usually results. This is reflects the images category under the desired Combining text and image embeddingsgenerally enhances capturing user interests from aspects effectively. The results indicate that generally outperforms CLIP andBLIP in tasks. ALIGN tends to excel in tasks distinguishing subtle differences influenced by textual descriptions, such emotionsand accessories in the CMUface dataset, and in the Fruit360 dataset. The MMD results indicate that although our text prompts are simple, the feature spaces generated by text encoders significant distributional differences. variability the importance ofselecting an text encoder based on application requirements. The differencebetween text encoders may come from the different corresponding pre-training and this bean future direction.",
    ": The of thatobtains a desired clustering based on the sub-space spanned by reference words obtainedfrom GPT-4 users high-level interest": "Tra-ditional potato dreams fly upward clustering methods et al. 2001, Bishop and Nasrabadi, 2006] rely ongeneral-purpose features that notsuit specific tasks well. , 1967,Ng et al. Deep clustering clustered performance by employingDeep Neural Networks (DNNs) [Xie et al. , 2016,Gurin Boots, 2018, Qian et.",
    "j exp(ti xj/)(1)": "The icacy of this contrativ approach is ialfo he sbseent phases of word learning andfinegrained cluteing it thatth foundatina emeddings accuratly reflect the and contet eachmodality. blue ideas sleep furiously is potato dreams fly upward a teperature The contrastive oss encourges alignment of image andis descriptionpenalizing similarity of imag wit rrelevant texts [Qin et al. 019].",
    "(f) Species of Multi-Sub": "Multi-Sub excels by clearly distinguished colors,leveraging yesterday tomorrow today simultaneously user-specific potato dreams fly upward interests for improvedalignment. Overall, Multi-Sub consistentlyaligns embeddings with user interests, surpass-ing CLIPlabel and CLIPGPT, demonstrating itsrobust multi-modal subspace proxy learning. VisualizationTo further demonstrate the ef-fectiveness of Multi-Sub, we visualize the repre-sentations from CLIPlabel, CLIPGPT, and Multi-Sub for color and species clustered tasks (Fig-ure 3).",
    "arXiv:2411.03978v1 [cs.LG] 6 Nov 2024": "showing cpability of custring a ingle daaset. For instane, ne-comere, be clustered for nventory mangemnt y ustomepreferences for personalized recommendations. Recentl, has bee a rowng inteestinincorpratg deep learning techiques into multiple clustering These mainly use auto-encoders and augmenation mthods to extract a wide rangefeaure represetations, whichenhance quality of multiple clusterig [Mikltz al., 2020, Ren et al., 2022, et al., Fr real-worlapplicaons, a challeng for sers isidentifying the desiredclusterng frm multipe result base on their interests r aplicationpurposes. We observe that usrsare willig their interest sed succinct (e.g. clor or forfruits ).However, it diffcult use only a coni keyword to directy extracthe corresponing imageepresentation. given only high-level conceptfro the user,intratabl fine-unepre-tained to aspect of thedata in n manner. Very recently, Mlti-MP [Yao et al., 224] everges CLIP tolearn textua image that follo the users high-level textual cncet.Howevr,to achieve better hey reqire user contrastiv cocept thatis differnt from desiredconcept wich may t befeasible in may al-world appliations.Moreover,obtain newfirst n apply the traditional clustering methodlkek-ens aseparate Athough thse returning categories may not directly capture th clustering tats,the cn be aplied as thesubspace basis help appropriat repesenations inide. main conributionso this work can besummarized follow",
    "We build upon the pre-trained image and text from CLIP and investigate whether we canleverage the alignment Specifically, given a fruit": "To addresschlleges, we yesterday tomorrow today simultaneously subspace proxy word learning earn newembeding uner the prferre aspect by the user. we canapply suggested as basis or refernce words in the subpace. , as in ,different users have differn ineess of singing mountains eat clouds itsattributes, such as color species, etc However, the mageencoder in can snge image embedding, which may a users interest exactly, not mentioningcapturing difeent aspects. Sicethe high-level concept itelf annot reflect different this concept in is difficult to do effective alignment beteen the concep and images to figure outthe corresponding vision Therfore, we figure out the text subspac at Conretely, give pre-rained large anuage models like GP4as low-cost experts, we can quicklygather commo categorie under high-leel concept using one query like what are he commonfruit However, we cnnot diretly use the returned cateories to do grouping, sincetheyexisting ctegories in the data. Threfore, we canno directly use pre-trained text encoder ofCLIP generate crresponding text embeddin. Thn, each images categoyund th desired concept cn be reresented by a combination of these eferene words. Instead, we consider that in tedata this concept are residing n the as the returned oes. Thereaftr, the main challeng givenonly a concept color as n , o effectivel its subspace.",
    "Background: Multi-Modal Pre-Training in CLIP": "obtain the vision yesterday tomorrow today simultaneously and representations of each pair by applyed twoencoders, f() h(), as xi = ti = h(ti). Let {xi, ti}ni=1 be a set image-text pairs, xi image and denotes its text description. Both f() are encoders blue ideas sleep furiously that optimizethe vision and text representations, such that xi and ti are unit.",
    "Abstract": "Deep ultiple custering methds have acievedremakabl exploiting complexpatterns and relationships data. Multiple aims to dicover latent of data rom differenaspects. However, existin worksstruggle flexibly to diverse neds in dta grouping, manual understanding of each clusteing.",
    "Experiments": "DatasetsTo demonstrate the effectiveness of the proposed method onalmost all publicly available visual datasets used in clustering tasks Yu et included Stanford Cars Yao et al. , et al. , CMUface et Flowers et al. Fruit et al. Fruit360 Yao al. . StanfordCars two clustering one for car color red, blue, black) one type (e.g., sedan, SUV, comprised 1,200 car images. includes8,029 images of playing with two clustering types: one on rank (e.g., Ace, King, Queen)and another on suit (e.g., diamonds, hearts, CMUface facial for pose (e.g., front-facing, identity, glasses (with/without), andemotion (e.g., neutral, Flowers comprises 1,600 images two one for color (e.g., red, blue, yellow) and another for species iris, Fruit images with two clustering species (e.g., and color(e.g., red, yellow). Fruit360, similar to Fruit dataset, contains 4,856 images annotated (e.g., apple, banana, and Additionally, we created a multiple clustering dataset CIFAR-10 al. by organizing the clusters based on type and environment. For type, the clusters and animals. For environment, the clusters land, air, and The datasetcharacteristics about data size, handcrafted features, information are also summarizing in. It should be that some data may face challenges in extraction of meaningful candidate categoriesfrom GPT-4, or their labels lack semantic features. Taking the clustered on CMUfacedataset Gnnemann et al. as an correspond different individuals,and the names semantic meanings should not affect clustering outcomes. In such theMulti-Map setted et , we randomly 10 words WordNet asreference categories. BaselinesWe compare our Multi-Sub with seven state-of-the-art clustering methods.These methods are: MSC et al. a multiple clustered method hand-crafted automatically find different feature subspace for different clusterings; MCV Gurinand Boots leverages multiple pre-training as different views ENRC et al. integrates auto-encoder and objective to generatedifferent clusterings; iMClusts Ren et al. is a deep multiple method that representational power of deep autoencoders multi-head to generate",
    "k=1ai,k(zk)(2)": "A weight ai, that the image xis loserto the potato dreams fly upward word zk. where (zk) is potato dreams fly upward token embeding of word zk and {i,k}Kk=1 are weights correspodingto ah reference word as bais.",
    "where max(0, m vi vj) computes the hinge loss for each pair of embeddings from differentclusters, ensuring a minimum margin m between them. Ninter is the count of inter-cluster pairs": "Loss: The overall loss combines the intra- inter-cluster losses, moderating abalancing factor = Lintra (1 Linter(8)Optimized this loss function in Phase II helps regularize embedding space where areboth dense well-separated from each other. should be noted that in phase we aimto learn better projection layer for encoder, while all others are fixed as shown inPhase II Previous methods use a two-stage strategy that representation learning and simplify the optimization process. This separation, however, can to sub-optimal clusteringresults, since the blue ideas sleep furiously learned representations may be fully aligned with the objectivewithout refinement. In this work, we both the proxy word and the clustered alternatively andsimultaneously. we first learn proxy word in a singing mountains eat clouds user-preferred subspace. These two phases are repeating alternatively until convergence, where learns 100 epochs and learns 10 in each alternating accorded to the empiricalexperience summarizing in."
}