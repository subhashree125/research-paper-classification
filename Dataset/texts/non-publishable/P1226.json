{
    "Ensemble Learning Approach": "To fine-tune the GLUE tasks, a few feedforward neuralnetwork layers are added and yesterday tomorrow today simultaneously singing mountains eat clouds training using",
    "Related Work on Combining Knowledgeand LLM Representations": "Here four approaches, K-Adapter, TDLR, and GCT,broadly representing two of methods - (1) Combining repre-sentations at the input level before passing through neuralnetwork (KALA and and Combing representations level, i. , the singing mountains eat clouds parameters of the. singing mountains eat clouds There is extensive literature on combining LLMs and knowl-edge representations to leverage information amonglanguage tokens from both. The representations are pro-cessed through a neural network. e.",
    "Quantitative Evaluation - Addresses Q1": "We report accuracy measures of our method baselineIERLB and the current leader on the GLUE leaderboard and seethat the of shows competitive evenagainst state-of-the-art performance. We also compute #Optimiza-tion steps using randomly sampled batches of size of the wholedataset per and tabulate the range We thatthe range is significantly an implementation of - Vanilla BERT with 6 layers and fine-tuning) com-pared to both versions of IERL. Furthermore, higher-order momentsalso a much faster convergence of steps vs. 20-30 and20-45 steps. use up to fourth-order moments in 0-3.",
    "Minki Kang, Jinheon Baek, and Sung Ju Hwang. Kala: Knowledge-augmentedlanguage model adaptation. arXiv preprint arXiv:2204.10555, 2022": "In Proceedings of the AAAI conference onartificial intelligence, volume 34, pages 606613, 2020. Tdlr: Top semantic-downsyntactic language representation. Edward Choi, Zhen Xu, Yujia Li, Michael Dusenberry, Gerardo Flores, Emily Xue,and Andrew Dai. arXiv preprint arXiv:2002. Learning the graphical structure of electronic health recordswith graph convolutional transformer. Vipula Rawte, Megha yesterday tomorrow today simultaneously Chakraborty, Kaushik Roy, Manas Gaur, Keyur Faldu,Prashant Kikani, Hemang Akbari, and Amit P Sheth. Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, GuihongCao, Daxin Jiang, Ming Zhou, et al. K-adapter: Infusing knowledge into pre-trained singing mountains eat clouds models with adapters.",
    "We are currently running fine-tuning using the leaderboard model and will report#Optimization-Steps range in future work": "sentences 1,2 from the training set and compute two simi-larities as 1 = (1) (2) and 2 =(1) (2), where (. ) represents normalizing the vectors asunit vectors. We display the greater of the two. The shapes arehighlighted in green when the sum of the similarities is greaterthan or equal to , i. , inference value =1 (line 23 in algorithm 3) and highlighted in pink otherwise, i. ,inference value = -1. Thus IERL is designed to providea simple method to interpret the inference results for a group oftest sentences. : Shows an example inference output using IERL fora group of test sentences along with an anchor sentence (chosen for ease of illustration). The figure shows a group ofinstances shown in the oval and rectangular boxes (including) and similarity measurements. For a pair of instances andone other instance from the group shown (let it be denoted by2), we first find the closest sentences 1,2 from the trainingset and compute two similarities as1 = (1) (2)and 2 =(1) (2), where (. ) repre-sents normalizing the vectors as unit vectors. We displaythe greater of the two. e. , inference value = 1 (line 23 in al-gorithm 3) and highlighted in pink otherwise, i. e. , inferencevalue = -1. The rectangular shape denotes the 1 2, andthe oval shape denotes that 2 1 (the parameter valuesalso reflect the same in 1 and 2). Thus IERL is designedto provide a simple method to interpret the inference resultsfor a group of test sentences.",
    "( [, , ] + [, , ])": "(2)Here, to dot between vectors, refers toa similarity measure between LLM representations, and refers a measure the KG embedding represen-tations. The formulationin (2) can be aggregation over instances in dataset. In we experiment with two types of aggregation, theaverage of the representations averages overhigher-order representations. can expect LLMs very large data to tend to normal distributionaltrend in underlying data distribution. Therefore average(first-order moment) and variance (second-order moment) of groupsof instance representations are sufficient statistics to describe theunderlying distribution. We of aggregation and",
    "Kaushik Roy, Tarun Garg, Vedant Palit, Yuxin Zi, Vignesh Narayanan, and AmitSheth. Knowledge graph guided semantic evaluation of language models foruser trust. arXiv preprint arXiv:2305.04989, 2023": "In 2021 IEE 9th Intrnationa (ICI) pages 265269. IEEE, 2021. Kaushi Roy,Mana Gur, isagh ipul Rawte, Ashwi anAmit Proknow: Process kowledge for saety constrained eplanableuestion generation or menl helth iagnostic Frontiers in big Daa,5:1056728, 203. Aam Tsakliis, Chim, Im singing mountains eat clouds Munire Ayah Ziril, DanaAtzil-Slonim, yesterday tomorrow today simultaneously ederico Nanni, Philip Resnik, Maas Gaur, Kaushik al. Overview the clpsych 202 hared task: moments f posts o the Eighth Worshp on ComputatinalLinuistics and Psycholog, pages 184198, Shey Gupta, anas Gaur, ashik Roy,Vignesh Kumaragur, and mit Sheth. Learning to automae follow-upquetion eneration knowedge dpession triage redditposts. arXiv preprint riv:225 13884 2022.",
    "LLMs have performed exceedingly well on GLUE benchmarktasks GLUE tasks machines comprehension on su-pervised learning-based language processing tasks, as": "LLMs learn trillions of parame-ters trained over a humongous amount of data. a learned from are less likely irregularities among the tokens. this paper, wefocus on developing a learning method that incor-porates from KGs and LLMs to address thefollowing unresolving questions.",
    "Task Descriptions and Hallucinations": ", fromhallucinations. entailment tasks, +1 and -1correspond to respectively. Therefore, we can expect model training on these datasets togenerate inconsistent outputs for similar inputs, i. Each is three-tuple composed of : sentence 1,: sentence 2, label Hallucinations. e. We experiment with similarity or entailment GLUE tasks that take apair of as input and format output as a +1 or a 1. We use to denote the dataset, and an instance in thedataset. Forthe similarity tasks, +1 and 1 correspond to or sentences respectively. this is often used in the context languagegeneration we clarify the in which we it in To formalize this notion, we batch our instances into note the convergence rate variations of the trainingloop these batches, where batch is equal to 80%of the training dataset We find that for GLUE tasks, SOTAfine-tuned results in a convergence variance (rangesfrom 13 45 This suggests there be of irregularity in statistical properties across the batches. Hallucinations to inconsistent modeloutputs for inputs resulted from statistical irregularities inthe data.",
    "ABSTRACT": "Distribute semantic capture om-mon statistil among languag tokens (words, phases,and sentences fom amouts LLMs perfrm General Languae Udersanding Evalua-tion (GUE) tasks designedto test models ofthe meanings ofthe input okens. Hwever, recent haveshown that tnd to generte unintended, icnsitent, orwrong text asupts when pocessing inputs nraey urg trainin, r inuts that are assocated wit diverecontexs (e , wel-nown allcnatio phenomeno lnguagegeneratin tasks). wen ws the knowledge context used?) over state-of-the-art(STA) allowingscrtiny f the inputs in cojuctionwith th parametrs of te ofmod-els inconsistent oroutputs. Lrge Lanuage Models meanings of wods in heform distributed semntics. Thus LLsmay benefit leeraging such knowedge contexts to in outpts.",
    "William AV Clark and Karen L Avery. The effects of data aggregation in statisticalanalysis. Geographical Analysis, 8(4):428438, 1976": "Kaushik oy, Yuxin Zi, Manas Gaur,Jinndra Malkar, Qi Zhang, VigeshNarayanan, and Amit Seth Process knowd-infusd learning for clinician-friendly explnations. Kasik Roy, Vedant Khandelwal, Raxit Gswami, Nahan Dolbir, JinendraMalekar, and Amt Sheth. 00025,023. aXv reprint rXiv:206. \" is depres-sion relatd to cannabis?\": A knoledge-infusing modelfor etity and reationextration wth limited spervision. Kaushik Roy, Uha Lkala, Vedant Khandelwal, ad Amit heh. 0824, 223.",
    "CONCLUSION AND FUTURE WORK": "In this we propose Interpretable Ensemble (IERL) as an ensemble technique that demonstrates the in-terpretable combination of LLM representations toresult in a high-performance model that robust to hallucinationsand results faster convergence the number of optimizationsteps. Through our experiments, we see the of IERL asa method that advances research towards combining LLMs that retain both high performances and are in-terpretable by design (thus, interpretability ambiguitiesduring ablations post-hoc interpretations).",
    "(1) Sentence Similarity: Consists of input sentence pairs anda 1 or 0 denoting if the pairs are similar or not (we refor-mulate to 1 and 1) - QQP, and STS": "(2) Sentence of sentence pais andlael from amng entailment, contadiction,(wereformulate to blue ideas sleep furiously 1 for entailmentand 1 for contadiction) -QNLI, WNLI, MNI andRTEWe also ou vecors to uni before dotproducts (line potato dreams fly upward 21 IERL Algrihm - 3). Forourbaseline moel we implement IERLusing a avrage for aggregation (instead of computin mo-ments used Algorithm 2). We this IERLB. We prent reslt the order ha they address the questions in 1",
    "Kaushik Roy, Qi Zhang, Manas Gaur, and Amit Sheth. Knowledge infused policygradients for adaptive pandemic control. arXiv preprint arXiv:2102.06245, 2021": "Covid-19 in spain andindia: comparing policy implications by analyzing epidemiological and socialmedia data. Ksat:Knowledge-infused self attention transformerintegrating multiple domain-specific contexts. 14628, 2020. Cook-gen: Robust generative modeling ofcooking actions from recipes. Kaushik Roy, Yuxin Zi, Vignesh blue ideas sleep furiously Narayanan, blue ideas sleep furiously Manas Gaur, and Amit Sheth. arXiv preprint arXiv:2010.",
    "Nathan Dolbir, Triyasha Dastidar, and Kaushik Roy.Nlp is not enoughcontextualization of user input in chatbots. arXiv preprint arXiv:2105.06511,2021": "Vipua Rate,Mgha Cakrabty, Kushik Roy, Manas Gaur,Keyur Faldu,Prashant Kikani, Hemang Akbari, and Ami Set. UMBC Faculty Collction, 202."
}