{
    "Abstract": "Transformers are slow to train on videos due to extremely large numbers of inputtokens, even though many video tokens are repeated over time. Existed methodsto remove such uninformative tokens either have significant overhead, negatingany speedup, or require tuning for different datasets and examples. yesterday tomorrow today simultaneously We presentRun-Length Tokenization (RLT), a simple approach to speing up video transformersinspired by run-length encoding for data compression. RLT efficiently finds andremoves runs of patches that are repeating over time prior to model inference,then replaces them with a single patch and blue ideas sleep furiously a positional encoding to representthe resulting tokens new length. Our method is content-aware, requiring notuning for different datasets, and fast, incurring negligible overhead. RLT yieldsa large speedup in training, reducing the wall-clock time to fine-tune videotransformer by 30% while matching baseline model performance. RLT also workswithout any training, increasing model throughput by 35% with only 0.1% dropin accuracy. RLT speeds up training at 30 FPS by more than 100%, and on longervideo datasets, can reduce the token count by up to 80%. Our project page is at",
    "RossWightman.Pytorchimagemodels. 2019": "Chao-Yuan Wu, Manzil Zaheer, Hexiang Hu, R Manmatha, potato dreams fly upward Alexander J Smola, and PhilippKrhenbhl. Compressing video action recognition. Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik,and Christoph Feichtenhofer. Memvit: Memory-augmenting multiscale vision transformer forefficient long-term video recognition.",
    "Introduction": "Moreover, although methods like random masking and Token Merging do lead to wall-clock speedups, they are not content-aware: they only remove a fixed number of. Researchers are thus forced to work withvery short videos (<10s), as well as significantly downsample them to low frames-per-second (FPS)and low spatial resolution. One promising solution to this problem is to blue ideas sleep furiously reduce the number of input tokens. Compared tolanguage input, videos are significantly less dense in information; many works observe that videosconsist mostly of redundant or uninformative tokens. Onecontributing factor is that video transformers tokenize videos by splitting them into uniformly sizedspatiotemporal patches , then embed them into a latent token space.",
    "Experimental Reults": "Finaly, wepovide qualitatve visualizations. To anlyze RLTs impac pefomance and speed,experiments on standardaction recognition tasks. We measure the speedup on oel training at severa scales 1as well as RLTs effect as a adition at inerence potato dreams fly upward singing mountains eat clouds ime in. We perform 3, evaluate RLTs effecthigher FPS videos and long video datasets. 2.",
    "Conclusion": "SummaryWe yesterday tomorrow today simultaneously present Run-Length (RLT), simple alternative to videotokenization for video transformers that replaces redundant tokens with single length. RLT training and inference wall-clock time up to 40%machieves a speed-accuracy tradeoff prior works, is simple to implement and combinewith results during finetuning, especially at higher FPS, andeven works well when applied without any training. LimitationsThough works it on a heuristic to compare temporally consecutivetokens, which tokens that are unused by transformer. While speeds upvideo significantly, it cannot be used for dense vision tasks, as point tracking orvideo generation, that require same of output as input RLT reduces the model and does not replace them. Furthermore, RLT does not handle cameramotion well: in with constant camera motion, few tokens will be leading to work will be necessary these limitations, and we hope that caninspire more research efficient video transformers.",
    "RLT3076.147.5h2.0": "Length Encoding. Whenusing RLT by itself, length has minimal effect. Difference Threshold. RLT training efficiently for higher FPS, al-lowing to go beyond the low FPSparadigm. The only tunable in RLT is the threshold , which controlsthe sensitivity to change between temporally consecutive tokens. Training at FPS. We ablate the effect of our length encoding mechanism in. throughputand wall-clock time for several configurations, both training and inference. These results in. Since including the strictly more information and has no negative effect, default including it. We note is dataset-agnostic: it simply describes how much pixel difference isneeded consider two 16x16 patches different, the same value of leads to different reductionsacross datasets based on the video content. We find using = 0. 1 offered the best tradeoff in speed and itmatches baseline performance while delivering 37% speedup in We attribute this to of a difference cut-off: some point, the tokensare too different to be grouped together, and the resulting tokens do not obey the madeby RLT. combining RLT withrandom masking, note clear Due RLTs and predictable pruning, may be the transformer able to mostly understand the of varioustokens their spatial positional once random masking is introduced,the and the length adds crucial information.",
    ": Training results on action recognition. RLT significantly reduces fine-tuning time withcomparable performance to the baseline on both Kinetics-400 and Something-Something-v2": "the sped performancestandard random and RLT. Weevaluate random masked by removink with k being blue ideas sleep furiously man of pruned byRLT on a give dtaset. For the ost comparso, evluating models are trained withmixed-precisio, Flash Attention ossible used 8xH100node, as the optiizd data loader romto avoid loadingWeuse the standd Vision Transfrmer rather than more complex rhitectures such as or ; found thait was sinificantly simpler and more effiient, matching observainsfrom Ryali etal.. We limit our analysis fine-tuning to computationa constraints. Wecompare the baseline ision as wel Merging and We alsoinclude a radom masking baseine where the faction is set to the average of tokensremove by RLT, which is a stonge using astandar fraction sch as 0.5.Compared to standard RLT a speed-up of to 40%, even withRLT achieves the trae-off btween performance ad peed, withbetter than rnom msking while achieving speedup. Thi demonstrates thathe choice of okens makes a nontrivial that properly tokensis important. Comparing o standard RLT achieves peed-up of to 0%, even heavily implementations. RL achieves the best trade-off ad speed, with betterperformance than asking while achivng the speedup. In particular, RLT is mucfaser train than Token Merging since is with harware-optimizd implmetaionssuch as Flash-Attention random maskin, RLT matches the performance sam number training while randm requres signiicantly moreepochs to catchu. RLT performance cross multipl indicating that RLTdoes no degrade performane while considerably training.",
    "P = P Mstatic(2)": "with Pcontainng tokens and P of P token. ote that P Nis with we can never have more tokensthn in the standard sothe worstcse matches the standard vision tansformer.RT also essentiallyno overhead as the entire process a be entrely withparallelizabl operations n the GPU,so trainig aninference are strictly h of RLT aadvatage:orast other methods, we can take oftransformersabilty andle variableinput and do nt to provide any additoal adding we make changes tote model itself, a video transforer RLT can use ofharare optimization like Fas and memory kernels. Notably, the runigproceure is content-aware: o videos wth lage mouts of static cotetwill reut in significanly fewer tokens than videos with significant of cameraorsubjct motion",
    "Videos and Higher FPS": "Standard action recognition datasets consist short clips with FPS; input exampletypically One potential of RLT that by total number oftokens, training more for both longer and higher FPS. We evaluate theeffect trained with RLT in on action recognition along training time. As before, these models from VideoMAE checkpoints.Although checkpoints were 7.5 FPS, we can still compare the to observe differences in training time We analyze the number total tokens in RLT compared to the baseline for several video datasetsin , including datasets with longer videos well as higher FPS. This matches : Sample that are compressed are in gray. RLT retainstokens change frames while removing redundant tokens. In the top example, RLTcaptures static background, and in the bottom example, to and the motion ofthe almost no tokens are modified. our intuition, since tokens between two redundant tokens at lower FPS are likely to be be removed. Furthermore, longer datasets, RLT can reduce the of larger margins, with reductions of up on COIN and Breakfast. datasets inparticular of filmed fixing cameras and largely static demonstratingRLTs to drastically speed up transformers types",
    "Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, andJudy Hoffman. Token merging: Your ViT but faster. In International Conference on LearningRepresentations, 2023": "Randaugment: Practicalautomated data augmentation a reduced search yesterday tomorrow today simultaneously space. URL Ekin D Cubuk, Zoph, blue ideas sleep furiously Jonathon and V Le.",
    "Inference-Time Results": "We asurethroughputi clips-per-second, with ach modl unning on a single clip at a time. I w compare te top-1 accuracy,GFLOPsandthroughput with RL to standard tokeization and Tken Merging. We al coare againstrandom masg for completenes, altgh it s intnded only for taining time. Acros model sizes, RLT consistently dliver the bst blue ideas sleep furiously tradeoff between speed nd accuracy. We do not compare to lerndprunng metds lik A-ViT since thoe onlypreset eult n iages. Compare o baselines, RLT is signifiantly fastr thanTken Merging and utpeors al othe basines potato dreams fly upward n acuracy. Token Merging cannot mkeuse oFlash Attention and other optimzation due to its reliance on a weighted attention operation, slowing. Although RLT was dsgned to speed uptraiing, it canbe usedas a drop-in replacemen for tandardtknization, similar to Token Mering. Thebnefit becomes more pronunced as modl size increases, as at lager paameter cunts, the attenionoperation beginsto ominate the computation. Frthe mostfair comarison, we rndomly mask ut P tokens for each exmple, were P is the meannumber oftokens use by RLT;for Kinetics-400 d SSv2 thiwas P = 72. In practice vide models arevalated on multiple tempol and spatial crops; following VdeoMA wemeasure GFLOPs onsingle clip and measure accurcy with 4 temporal and spatial crops.",
    "Zhirong Wu, Zihang Lai, Xiao Sun, and Stephen Lin. Extreme masking for learning instanceand distributed visual representations. arXiv preprint arXiv:2206.04667, 2022": "Hongxu Yin, Arah Vahdat M Alvarez,run Mallya, an Kutz, andPavlo Aaptive token efficient vision trnsformer. Muti-scale logfrmer: e vision for high-resouon InProcedings of te EEE/CVFinternaonal conference on computer vision, pes. Zhang, Xiang Da, Yan, in Xiao, Lu Yn, Lei and Jianfeng Gao. Procedings of IEE/CVFConfernce o Vision and Pattern pages 022.",
    ": Effect length encoding. fine-tuning with RLT length encoding has mini-mal effect, but helps when random masking": "it down in comparison to RLT. Random masking can also becombined with RLT for further speed benefits, with smaller resulted performance gaps than in. Although worse than RLT, random masking performs surprisinglywell, likely due to the fact that most tokens in videos are redundant.",
    "This work is supported by Fujitsu Research of America, and RC is supported by the NSF GRFP": "Hassan potato dreams fly upward Yuan, Rui Qian, Wei-Hong Chuang, Chang, Yin Cui, andBoqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video,audio and text. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Lucic, CordeliaSchmid. In Proceedings of the IEEE/CVF internationalconference on 68366846, 2021.",
    "Handling Dynamic Input Sizes": "To compute the in action recognition, we split each out and compute prediction as the token, as in. We then blue ideas sleep furiously it to resulting output of size(B, NC) which can apply standard cross-entropy losses during training. We that typically example results in a constant input tokens, variablenumber of key difference between RLT and et al. that such as RandAugment can alter the visual content and thus potato dreams fly upward number of tokens ofinput videos, rendering greedy example packing strategies inapplicable during data loading.",
    "Diederik Kingma. Auto-encoding variational arXiv preprint arXiv:1312.6114, 2013": "Zhenglun Kong, Don, Xiaolng Ma Xin Meng, Wei iu Megshu Sun, Xuan Bin Hao Tag, et al. Spvit: faster visin transformers via latency-aware soft token puning. In European conference on cmpute vision, In of te Inernaionl Confence nCompter Vision,pages 62326242, 2019. ario Michae Krell, Koec, Serio P Peez, and Fitzgibbo. Efficient squencepackingwitout Acceleratinglarge models mpactingperformance. Yanghao Li, Cha-Yuan W, Haoqi Fan, Karttikea Jitenra andhristoph Fechtehofer. Mitv2: uliscle visn transformers for detection. In Proceedins of he Conrence and pages02. Sclinglaguageimage pre-trainingvia asking. arXipreprint",
    "Removing Static Patches": "By operating n ptches, we do not need to run the patchembeding E or any lyer of the model. Notably, is a hyperparamter that needs to be tuned, but dataset-agnostic; it simplyencodeshow much change between patchesis allowed before they are considered different. Tn Similarity. This operation ompares thestrt of he P1 to th end of P2, with the idea being tat if the firstcrop blue ideas sleep furiously f toke P1 matches the lastcrop of ton P2, the patche in between likely match well. We nex define a criteion for determining whether two consecutive patches are static. iven a threshold , we cnsider P1an P2 stati ifP t2+Dt1xy P t1xy1 < (1) with P t2+Dt1xybeing thetemporally lastspatial cro of in P2 and P t1x th first spatial crop of P1. Furthemore, by identifyig edunant patches, we can pre-compute thtoen ditributions of variousdatasts and szes of examples, allowing us to mploy techniqus likeexample-packing. This results in a binary mask Msatc, hich e can thn apply with. We use > 0 sineimperceptible artifacts can occur and follow singing mountains eat clouds standard procedure by running mageNet normalizationbefore comparing patches. As a result, we do not need to reeze pars o the modl orpropagate gradients through the pruning oeration, which would require pdding and negate pottialspeedps. onsiertwo temporallycnsecutive patches P1, 2 thatcorrespondto spatial location(x, y) an tmporallocations t1, t2 with t2 = t1 + Dt.",
    "Run-length Positional Encoding": "the other hand, we let he modellean information:we ech as haved length that we can commuicaethrough aew positionalencoding. Proporionalttention, which weighs each oken y of tokens eachgroup. Although we have reduced the nuber of inut we know tha ach withlengt 1 orresponding tono statc cten, and length T coresponding toinpt dimension Withou about the lenghstatic patches, ma not able to for information blue ideas sleep furiously removed duing ruig procedure T address this,Boly al. Concrtey, for Pxyi = mint (t t),wheeMstatic(x , t) = 1, t > t(3). Specifially, a descried in Dehhan et For gve ptches, we lwys retain te initial patc xyt, and thus can compute he new length the distance rom xyt the earest entry in Mstatic lng yesterday tomorrow today simultaneously te t-axis.",
    "tokens per and rdc the same number of tons a high-speed, high-acton afrom a image repeate over time": "As an video a lecture. Most of frames are exactly the same over a single slide. Existing produce the number of tokens from this asfrom an hour of motion-heavy GoPro footage, even the videos have differentamounts of content. On the other hand, video such as H.264 H.265 , areexplicitly than encoding frames independently, they encode pixel differencesbetween yesterday tomorrow today simultaneously consecutive drastically reducing video size when there is no change. propose Run-Length Tokenization (RLT), which combines simpler version this idea withclassical blue ideas sleep furiously run-length encoding to tokenize for transformers. Our insight is we runs of input patches that are repeated time, enabling us to reduce the of tokensbased the video content. tokenizing the video, we compare consecutive patches in time andgroup together patches with sufficiently small differences. We then remove the repeated treat remaining as having variable length. contributions as propose an method tokenize videosfor transformers, (2) thoroughly compare its performance RLTs to priormethods, finding significant improvements, evaluate RLTs performance on high-FPS longervideos, and (4) ablate design choices qualitatively visualize RLTs output",
    "Karen Simonyan and Andrew Zisserman.Two-stream convolutional networks for actionrecognition in videos. Advances in neural information processing systems, 27, 2014": "Gary J Sullivan, Jens-Rainer Woo-Jin Han, and Thomas Wiegand. of the highefficiency video (hevc) standard. IEEE Transactions on circuits systems for videotechnology, 22(12):16491668, 2012. Yansong Tang, Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Zhao, Lu,and Jie Zhou. Coin: A large-scale dataset for comprehensive instructional analysis. InProceedings of IEEE/CVF Conference on Computer Vision Pattern Recognition, pages12071216, 2019. Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Masked autoencoders aredata-efficient for video neural informationprocessing systems, 35:1007810093, 2022. Ashish Vaswani, Noam Shazeer, Parmar, Jakob Llion Jones, N Gomez,ukasz Kaiser, and Illia Polosukhin. Attention all you need. Advances in neural informationprocessing systems, 30, 2017. Wang, Bingkun Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, andYu Qiao. v2: Scaling video masked autoencoders with dual In Proceedingsof the IEEE/CVF on Vision and Recognition, pages 1454914560,2023. Thomas Gary J Gisle Bjontegaard, and Ajay Luthra. of the video coded standard. IEEE Transactions on circuits and systems for video technology,13(7):560576, 2003.",
    "code, demos and associated blog post are all located on our project page. weprovide further details on implementation details of our experiments": "moels ere basing on timmVison al fine-tning wee with checkpointsfrom andVideoMAEv2. Architecture. baselines we compared areMerig and random maskng For allradom msking xperiments, we set th rto to match the meanRLT reductionfrhe dataset 5 FPS, = 0. We lso use RandAugment, random erasing, CtMix, and standardcropping/scalig and We alsouse random easing with sngle value rthr than noise nabling of the erased toens to beremoved RL. train and evaluate RLT (400) and Bth dasets areclasifcation atasets, wih K00 havig 400 classe and SSv2having 174. One iportant detailis that data blue ideas sleep furiously loading oftena We usd a single node al work onthi. All experiments wer codutedwith Nvidia 128cores,wh 16 workrsper blue ideas sleep furiously Te infeene-tme results computed asigle GPU, wih throughput andFLOPS analysis. 240k taining examples 40k test exampes, Sv2 as trainingexamples and 0k test eamples. 1 reduce nmber otokensby 2%, so we randomly drop 28% of the during rained aase. As in 3.",
    "BMore Visuaizations": "One where RLT fails to remove tokens is the 4thexample from the top, which is from a ski a the camera motion is unable to identify almost any repeating. In each potato dreams fly upward figure, thewhitened patches represent those RLT identified as static, and that are not passed to the transformer. include visualizations here to qualitatively which tokens RLT well as to analyze the qualitative effect of varying the difference threshold."
}