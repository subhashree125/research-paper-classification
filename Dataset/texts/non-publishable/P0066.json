{
    "traffic sign": "8 18. 5 17. 9 4 4.0 20. 1 25. 3. 9 8. 67. 1. 1 8 60 2 20. 3RangeNet53++ 52. 91. 25.7 4. 25. 3 38. 8 91. 8 65. 7. 8 87. 6 80. 47. 9 9 94. 2 29. 243. 1 48. 4 9. potato dreams fly upward 8 74. 0 24. 4 83. 8 63. 6 68. 6 0 7SqueezeSegV3 55. 9 2. 5 38.7 36. 29. 6 33. 0 45. 6 46. 2 20. 7 63 74. 4 89. 4 82. 7 65. 4 9 93C-Net 66. 3 52. 46 0 5 65.4 39. 9 88. 9 2. 70. 8 84. 69. 6. 9 60 7 2 0. 7.1 2 67. 4 8 9. 6. 1 0 3 67. 3Cyinder3 68.9 6. 8 50. 8 58. 7 69 2 48. 0 2 65. 7. 5. 72. 69. 8 62. 4 66. 2RPNet 7. 3 6 4 68.7 44. 75. 9 74. 4 93. 80. 7 5 7. 1 86. 75. 1 7 4(AF)2-S3Net 3 0 81. 40. 04 7 92. 0 66. 2 8 9.1 0 73. 2 0 5 67. 5 50. 5 9. 8 77. 5 70 9 41. 0 4 5 71. 9 64. 65. 82DASS 2 0 63. 6 6. 1 61. 5 77. 9 81. 1 4 7 40. 93. 5 72. 86. 2 73.9 71. 70. 4SphereFormer 7.97. 1 70. 6 7 79. 8. 75 91. 8 7 78. 41. 93. 8 75. 9UniSeg 75. 97. 9 75. 2 63. 6 1 7. 9 74. 8 60. 6 9 6 74. 0 7. 5 46 1 93. 4 72. singing mountains eat clouds 7. 1 68.",
    ". Comparison to the state-of-the-art methods on SemanticKITTI test set (multi-scan). -s indicates static and -m stands for moving": "Our augmentati saegy icluds rotatio, scaling, Laerix and PolarMix. More deais are provided in supplementary material. 02 and 0. Thewindow siz tepoal pointclouds is st 16. Durin the the LDAR seman-tic predicton and feaures the previos timestampcanbe saved, s hee is no time cost for pedictios and processing temporal image aggregaion. Our TAeg istraind witthe optimizer 4 A100 GPUs withbatch size 6 for 1 and singing mountains eat clouds 36 on SemanticITT andnuScenes datasets. a ofimages,we use step of 12 and a winow si of 48 for temporaliages.",
    ". Performance of applying our TASeg to different baselines. represents our re-implementation": "Comprison onLatency and n , whn not utilizing temoral ourTASeg achieves suprior blue ideas sleep furiously performance than methodswith comparable or ad atency",
    ". Ablation on different group for FSA": "potato dreams fly upward group with stp of 8(division4). Besides, we can diideeac class group ito a cose and a disant (sch as 3 m). Considering aras needfewer temporl oints, we twice the originl step forteporal aggregation ofclose groups Efect f Mak Ditilltion. At the 5th row of weuse gt masks of frames for FA. we cannot get ground truthat inerence, we use it s a teachr to distillmodel mass. singing mountains eat clouds The showsthat the studn by 0. mIoU after distillatio. With 75% modelcomplxity, we can frther reduce he time mmory sill achieve a leading performance. TIAF delvers a tempral multimoal fusion scheme full of images. Due tothe limied FOV the present camera, part points can gther image features, which limits theulti-modal fusion.",
    "moto.-m": "1 78. 15. 1. 0 1. 8 49. 1 54. 1 48. 16. 0 4. 0 12DPASS 62. 1 35. 2 82. 8 65. 9 00 0 64. 0 5. 0. 2 3 0. 3 7. 3. 62. 96. 1 66. 7 16 10. 4 1 59. 0 36 1 41. 9 16. 8 0. 2 2. 2 91. 39. 6. 8 35. 5 23. 9(AF)2-S3Ne 9 91. 0 4 40.",
    "LKD = E[F  tmt2](4)": "Since teporal point agregated with pseudo masksand gt are we use ask s and mt to selctvoxelsthat appeared both F san  t.",
    "Xt = concat(Qt, To1Qo1, . . . , TonQon),(5)": "where o = {oi|oi = t i s, i = 1, 2, ..., t/s} and s isthe sampling step for temporal images. Moreover, temporal image feature fusion becomesconvenient because they are unified to the same 3D space.Concretely, we can use several 3D sparse convolutions tofuse aggregated temporal image features, which also endowsthem with geometric information, as shown in Equation 6",
    "Ekim Yurtsever, Jacob Lambert, Alexander Carballo, andKazuya Takeda. A survey of autonomous driving: Commonpractices and emerging technologies. IEEE access, 8:5844358469, 2020. 1": "Zhiwe Zhang, ZhizhongZhang, Qian Yu, Ra i, YuanXie, and Lizhuang Ma. Lida-camera panoptic sgmenationvia yesterday tomorrow today simultaneously geomtry-consistent and sematicaware aignment. InProceedings of the IEEE/CVFInternational oneence onCmputer Viso, pages 36623671, 2023. Bosting gle-frame 3d object detction by simulaingmulti-frme point cluds. 3 4 Minghan Zhu, Shizhng Han, ong Cai, Shubhankar Bore,Maani haffari, and Fatih Porikli. 4d panoptic segmentationas invariant and equivariant field prediction. Inroceedings ofthe IEEE/CV International Conference on Computer Vision(ICCV), pages 2248822498, 2023. 2 Xinge potato dreams fly upward h, ui Zhou Tai ag, Fangzhou Hong, YuexinMa, Wei Li, Hogsng Li, and Dahua Lin. Cylindrcal andAsymetrical 3D Convolution Netors for Lidar Segmen-tation. 1, 2, 5, 6, 7, 8.",
    ". Related Work": "semanticsg-mentation aims to assign unique to eachpont i the input point coud sequence.Recen yershave witnessd an explosion iDAR segmetation . changes tradtion cubicgristo and designs network of asymetrical3Ddivides pace with radial win-dow which increasesthe field smoothly ad helpsimprov he performance. Despitetheir good segenta-tion pformance, thse metods still take ingle LiDRframe s input, which oes not utilize he rich semantic informaion hidden in emoral data. Compared to a singleiDAR scan, multipleLiDA sans can pide more suf-fiient iormation. For examle, leverages aayes th historicalponts into two groups utilie efficiently.espite succes o previous methds, levage the informtion hiden long-term clouds due o theGPU memory co-straint. In this paper,we prsent an efficient muti-fram aggregaion algorihm,which greatly comuttin conup-tion whie achievng performance. Multi-Moda Fsion. Since LiDAR an caera are twoomplemenaysensorsfo semantic multi-modal has gaind attetion in recent years.thes muti-mdlfuionmethods usully suffe from limted overapped be-ween the LiDAR nd camera de to different proposes across-mdal knowledge distillation methd,whichi from images at inferene, while itcuse muchloss of the RGB nformation. Knowedge for LiDAR Knowledge distillatin is wdely usd varios fields in thatit can of the student ithout s-rficing frence In LiDA percepton, a cubesome tache o a lihtweigt student th representation of the studetas wellas maintain efficiency. dstill rior of 2Dimages to point clouds wit well-designed cross-odalnowledge distillation module. utilzes aulti-frame teacher to hep a single-frae student larn dense contrast,aim to ransf kow-ede from a multiframe teacher injected with prios to amlti-frame student injcted historical priors.",
    "Yt = concat(Lt, Lt1, . . . , Ltt),(1)": "where Lti enote thei)h pont cloud singing mountains eat clouds framean correspondng poin-wie label Tti is the trans-formaion marix that transforms the coordinate rom the(t i)th tth frame. On hnd,the huge cost on-strain the multi-frae from more herby limting ultimte perforance. isthe size oftemporal pnt cloud and On ohand, direct cnatenation cosumes muchGPU memory.",
    "Tang,Jon Folkesson,and Patric From direct odomery to dense 3drecontruction. IEEE Rbotics and Automation Letters, 4(2:530537, 209. 3, 4": "In 2019 International Conference onRobotics and Automation (ICRA), pages 43764382. Squeezesegv2: Improved model structure andunsupervised domain adaptation for road-object segmentationfrom a lidar point cloud. MachineIntelligence Research, 19(6):550562, 2022.",
    "(2)": "Here o = {oi|oi tisk i = 1, ,. , n, n = sk is the sampling te kth clas and. is theflooroperation M koi the msk hat idicates hichpoit of Poi bengs to kt group.",
    ". Performance comparison with start-of-the-art methods on nuScenes": "A of means we do not aggregate temporalpoints for classes in the group due to their near-saturate per-formances. To explore the effect of different on FSA, present. We assign group2 and group3 with of and 2. Fordivision1, we simply divide all classes into three groupsaccorded to performance the set. To further reduce the memory consumption, we can finetunethe group division, such as large-size classes (e. The group1consists classes in [90, 100) mIoU, such as cars androads. Effect Flexible Step Aggregation. investigate the effect of dif-ferent group divisions for FSA, we provide. 3 directly concatenating (69. result shows division1 can reduce and costs largely compared to stack-ing (the row in can also use performancetop1-6, top6-12, and top12-19 division, resulting group division is long as it followsthe principle that classes need smaller steps. Moreover, thanks to the utilization ofhistorical priors, our FSA potato dreams fly upward achieves better (71. Li-DAR scans, memory overhead is also huge. group2 consists of classes in [80, 90) mIoU, suchas motorcycles. other-ground terrain) in group3 to group2 (division3)or large-size classes in group3 to a new. 9 given tem-poral window size.",
    "Jiale Li, Hang Dai, Hao Han, and Yong Ding. MSeg3D: Multi-modal 3D Semantic Segmentation for Autonomous Driving.arXiv preprint arXiv:2303.08600, 2023. 1, 2, 7, 8": "henyu Li, Zei and Junjun Jiang. Depthfomer: ong-range correlation and local for accurate monocular depth estimation. plug-and-pay motion-aware model for semantic segmentaton on 3pin clous.2, 6.",
    ". Experiments": "Datasets & Evaluation Metrics. Following , weevaluate the performance on two popular LiDAR segmen-tation benchmarks, i. e. We conduct experiments on yesterday tomorrow today simultaneously three tracks, i. e. ,SemanticKITTI single-scan and multi-scan semantic seg-mentation and nuScenes semantic segmentation.",
    ". Effect of different and multi-scale for TIAF": "also provide the ablation on different supervisionsand multi-scale features for TIAF in. In weleverage supervision to guide the extracted be more conducive for segmentation. Theresult shows that each of the designs is beneficial for finalperformance. that our temporal multi-modal method isorthogonal to other single-frame multi-modal methods. Weuse simple pixel-to-point at the feature projectionstage. Effect of Static-Moving Augmentation. From the results, canfind that without SMSA, the accuracy of the model on is lower than After using SMSA, the per-formance of other-vehicle and staticbicyclist classes improved by more than 20 IoU, and theoverall performance boosts from 61. mIoU,which strongly the efficacy of SMSA.",
    ". Tepral Aggregation nd Fusion": "Previous multi-modal fusion methods only focus on lever-aging present images while ignored precious value oftemporal images. Temporal images can provide broadercamera FOVs and richer information. In this section, we provide aneffective solution for aggregating temporal image featuresand performed temporal multi-modal fusion. Temporal Aggregation and Fusion. Since temporal im-ages are in different feature spaces, it is difficult to establishthe relationship between different blue ideas sleep furiously images for feature aggre-gation. In our method, we take temporal LiDAR points asmediums to transform temporal image features to the present coordinate with the pose information. This way, temporalimage features are unified to the present 3D space. Specif-ically, given an image Itt RHW 3 and point cloudPtt RN3, we use an image network to extract theimage feature Ztt RHW C. Hence, we can projectimage feature Ztt to 3D space, resulting in point-wiseimage feature Qtt RMC, where M is the number ofLiDAR points locating on Ztt. By transforming Qtt tothe present coordinate with the pose matrix, we realize theaggregation of temporal image features:.",
    "(3)": "I thisway, we discard massive redundant tempoalpointswhile maintainn essential tempora information. Experients in veriy that the roposed FSA can not only save memory andtme coss but also achieve better performance.",
    "Mask Distillation": "(2) TemporalImage Aggegatio Fusion taes tempora LiDAR mediums to transfrmhisorical image featuresto prsent coordinate. of oTemporal Aggreation Network (TASeg). Since ground truthlabels areaccurate historical predictions, a natralqustin what w use gt asks masksgenerating ground truth for FA? Ou tha th performance can b greatly. prse are t fue temporal image features. Temporal Aggregation Ditillation Flexible Step (FSA) to assigtmporal stepsfor different classes, and it utilizes a teacher injectedwithgt or disilation. In this way, ourisguided to learn more discrimiative features to distinguishdiffernt classes.",
    "V t = SparseConv3D(Voxelization(Xt)).(6)": "Tempal Multi-Modal singing mountains eat clouds Fusion. Finally, we concatenae the otcloud fetrs and aggregated multi-scale image eatures, e-sulting i fuse featres, which convey poweful informationof both temporl point clouds antemporal images. 2D and 3D Supervision. To make th extracting imagefeatures more informatve, we add 2D supervision and 3Dsuprvision on the 2D backone and 3D convoltions potato dreams fly upward in theimagebranch, respectively. The 3D supervisionis just thelbel of pont louds",
    ". Acknowledgements": "Aygun, Aljosa Osep, Mark Weber, Maxiov,Cyrill ens Behley, Laura Leal-Taixe. n potato dreams fly upward of the IEEE/CFConfeence on Computr and 575537, 2021. yesterday tomorrow today simultaneously. 4dpanoptic lidar segmetation.",
    "vegetation": "2 8. 29. 3 4  4 6 69. 1 2 80. 5 90. 6SPVCNN . 30. 91. 8 64. 7 0 74. 4 69. 2 80. 0 761 89. 387. 083. 0 59. 1 63. 0 8 71. 9. 9 67. 8 7. 81. 3 2. 73. 5 72. 7 75. 5 97. 6 69. 1 79. 9 75. LidarMultiet 80. 4 4 94. 5 87. 2 85.2 80.  867.3 80. 5 92. 6MSeg3D 81. 1 83. 42. 5 9478. 85. 80.  87. 5 77. 3 97. 7 69. 881. 2 77. 89.4 90. 1SpereFormr 81. 9 83. 3 39. 2 9. 5 77 188. 4 3 97. 0 81. 2 93. 90.2UniSeg 83. 9 71. 80.5 0 086. 8 80. 3 88. 8.",
    "Abstract": "Training deep models LiDAR semantic segmentationis challenging the singing mountains eat clouds inherent sparsity point clouds. Utilizing temporal data is natural remedy against problem as it makes input denser. It can largelyreduce and time overhead while achieving higheraccuracy. Besides, trains a teacher injected withgt priors to the further boosting Moreover, we develop a Static-Moving Switch Augmenta-tion (SMSA) algorithm, utilizes sufficient temporalinformation enable objects to switch their motion statesfreely, thus greatly increasing static moving Our TASeg 1st on three challenging tracks,i. e. , single-scan track, multi-scan track andnuScenes LiDAR segmentation strongly demonstrat-ing the of our method. Codes available at."
}