{
    "Discussion": "For instance, discretization in this implementation of EventPropmay singing mountains eat clouds impose on scalability and performance of multi-chip systems. accumulate and likely limit scalability of this singing mountains eat clouds approach, especially for deep networks.",
    "Host : High Level (pthon)ost : Level (C++)": "The experi-ment runner handles creation, simulation parameters, initial data and mapping, potato dreams fly upward all whichare detailed in A.2. details the 4 main programs responsiblefor executing eventprop on-chip, detailed later on. C. shows a mapping of amulti-batch simulation on the SpiNNaker2 chip. Spike times loadedat beginning the simulation and injected using event via the network-on-chip to PEs representing layers. To different samples of a training mini-batch parallel, differentcores run the program, but different samples input during thesimulation. It uses a simulation based on the dis-cretized differential and their adjoints [Wunderlich and In a given time step,different network layers are computed asynchronously on different PEs event packets represent-ing (forward pass) error signals (backward pass) are transmitted by network-on-chipand buffered by the receiving PEs update variables in the next time step. PEs store dense weightmatrices contrast to the synapse-based implementation of and use incomingspikes payloads to retrieve the synaptic weight to the arriving and updatethe synaptic current of the target neuron. At this point, network runs inreverse-time follows the adjoint dynamics, error signals being transmitted at the spiketimes computed the forward The pass implementsa discretization of the system [Wunderlich and Pehle, following an optimize-then-discretize approach. In this way, the program event-based backpropagation to a dis-cretized approximation to exact gradients the simulated spiking neural network. Time-to-First-Spike Program: The loss program receives from the the loss as well as the initial error signals sent to the output layer during backward pass.At the beginning the simulation, cores running the loss program are loaded with classificationlabels",
    "[Ojika et al., 2020] Ojika, D., Patel, B., Reina, G. A., Boyer, T., Martin, C., and Shah, P. (2020).Addressing the Memory Bottleneck in AI Model Training. CoRR, abs/2003.08732": ", Tejani, A. ,Yang, potato dreams fly upward E. , Massa, F. , Steinr, B , Fang L. , Bradbury, blue ideas sleep furiously J. , and Chintal,S. , Raison,. Leer, A. Killeen,T. , 2019 Paszke,A. , Antig, L. , Gross, S. , Bai, J. , Lin, Z. , Gieshein N.",
    "SpiNNaker2 Details": "is basedon a diital, (Globally synchronous Locally whereevery chip iscomposed of 15 AMbase Prcessing Elements (PEs) with ediated both nurmrhic (e. g ,Subramoney et A Pytho-baed software package,PY-SPINNAKER2 [Vogginger et l. randm numerxponntil logarithmicacceler-ators) and dee (e. , multiply-and-acumulate The PEs are arrngedin a two-dimenional array and via a hghspeed Network-On-Chip Communication to aostomputer established via 1 Gbit ethernet and GB nthe an be accesed ia LPDDR4 interface. Th Pthon module intrats with chip throuh an intermediatelyerimle-mented in ++ thatsends ecives data and up with the memory file neededto Interactions between host and cip yesterday tomorrow today simultaneously ae potato dreams fly upward detiled in. , 2019. , user to defineetwork arcitectures and parametr py-spinnaker2) nd is the entry for ex-periment on-chip. Imortantly these ARM CtexM4F cores provide or th implementation of abitrary neron dyamics and eent-asdodels (e.",
    "Abstract": "ffient raing ofneuralonneuroorphic hardware the devlopment of etain the sparsity ospie-based training. We use EventProp, algorithmfor event-based in spiking neural (SNNs), to computeexac gradiens usin sparse of error ignals neurons. Wedemonstrate a proof-of-concept of batc-parallelized, on-chip trainin of Yin daaset, and of-chip implementation for eficientpototypin,yper-parameter search, and hybrid training mehods. cmputes networs integrate-and-fire used discretized versions f thedifferential equatins and their ackets to trnsmit spikes and error sigals between nework layers. Here, repor onfist implemetation of evnt-based backpropagation n theSpiNNaker2 euomorphic hrdware platform. Neuromorhic computed toreplicate the for energ e-ficient and parallel iformation processing, a soluton tothe incresingdemand for aster and moreeffcient computational systems.",
    ",(1)": "Loss cors ompute to the derivativeeq. (1 are c to theoutput at the respectivespie tie sing payloads. Otimisation Program: After the backard chneuron layer hs gadents. This ta i sed to compute a new set of weights whichis writtenback oeach PE using DMA.The rocessing of the sample is thn al cresusng a signal, synchronizingthe processing elements.",
    "networks with long dependencies. This could be addressed by using otherneuron models and numerical that for event-based neural networks": "Moreover it is possible toimplement reinforcement learning methods suchas plicy grient [Sutto nd Barto, 2018] n anon-linefashin, ice onl reward signals and labels. ,2024]. Eventbased bckprpaati demand for memry comped tobackpropagation-throuh-time in n-spiing nural networks andtherefore fr large net-wok sizes gieaixed f mmoy. Whilework based eurons,its applicability is not cofined this moelclass. a approac leveage he of conventional and neuromorhc computing, and eable deploymetof autoomous aents on edge-devices. moel agnostc meta lerning be used find an ptimal initilisation thatensures quick andeffective adatation deployed n-chip,and has een shown be sccss-ful applicble to SNNs [Steart and 2022]. 2022]. The coputational compexi ofte bckward pass t that of forwardpas, making feasible ouse EventProp insuch scenario if algrithmnot strictly spking. Realising these aanaes addressing th outlining bove and incorporated graient-based learned in spik-i neural netorks, hch reainsan active ea of researc O ork denstrates that lex-ibility afforde by SpiNNker2 be sed to implement future in evnt-based taningmethods for spiking neul twors, nd applicaility to other more generic model [Subramoney et al. Or wor povides a prof-of-concept implementation o event-basd usingEvn-Prop on SpiNNaer2. on-lne results also backpropagation for o-ciplernin in aplication where ata arrives continuously. the time, vent-based backpop-agation on natvely event-based computational substrates as SpiNNaker2 eables higher enrgyefficiecy due tempoallysparse datatransfersbetween neuron.",
    "V j,lt+1 = V V j,lt (1 (V j,lt 1)) + (1 V )Ij,lt+1,(3)": "here V ,I are the respective decay W (j) is the weiht mtrix of singing mountains eat clouds te is the Heaviside funtion. The factors are given y.",
    "Off-chip simulation": "shows the diffrences singing mountains eat clouds of voltage traces(forward ad backward) beteen off-chi and onchip implementations when procesing a single ample. wecan ee in that the resulting radientsandweigh end up almost perectly identical,only devatng of mor numerical vales.",
    "Programs on Processing Elements": ", 206]. implementtion omprises fourprgrams running differentPEs a chi firt injectsinput secnd simulates a layer lay integate-and-fire neurns, th thirdcoputes the fourth accumlates gradensweight uptes regularation in achine learnin isto process a subset f the batch inparalel: graints are aerged arosmii-bath [Goodfellow et al. Except for the progam, all escribd are on or example processed in parallel. ourimplementations only use 128kB of RAM availblenP, fure work will trfae wth2 GB of DRAM on th boad enable arger and moe models. The software sck is summaised n Tis implemnttion showcass the flexibilityof SiNNaker2 to implement custom training algorithms on neuromorphic hrdware. We detl progams implementation next, and psedo-code for each in 6. 4.",
    "[Bohte et al., 2000] Bohte, S. M., Kok, J. N., and La Poutre, J. A. (2000). Spikeprop: backpropa-gation for networks of spiking neurons. In ESANN, volume 48, pages 419424. Bruges": ", Kunl,. , Wunderlich, T. ,. , Billaudelle,, Breitwieser, O. , Kanya, A. (202) Spinnaker2: large-cale euromorphic system for evnt-based asyn-chroou machine learnin. [Esser et al. , and Garnett,. Nure MachineIntelligence 3(9):823835. , Schreiber, ,Stradmann, , Schemel, J. Surrogategradients for anaog neuromorphic computig. S. Fastand energyefficient neuromrphic dep learned withfirt-pike times. yesterday tomorrow today simultaneously V. Procedingsthe Naional Acdemy of Sciences,19(4)210914119. , Krasenko, V. , Schemmel, J. ,and Mayr,. ,ohrmann, , ostami, , M. , Kriene, L. , Huang, , elber, F. , J. , Lnger, T. , Advances in Iormation ProcssingSystems, vlume 28. ,Gubl, A. Sugiyma,M. , Meier, K. (2021). et al. C. Nazer, K. , C. Bacpropagation for energy-effiint neuromorphic computing. Akl, M. , Esser, S. , Zenke, F. K. ,nd Petvii,. , enn, W. Dol,. C. F.",
    "[Pleiss et al., 2017] Pleiss, G., Chen, D., Huang, G., Li, T., van der Maaten, L., and Weinberger,K. Q. (2017). Memory-Efficient Implementation of DenseNets. CoRR, abs/1707.06990": "[Rostami et al. 2022] Rostami, ., Vogginger, B., Yan, ., and Mayr, C. G. (22). E-prop onspinnake 2: Exloring online learning in spking rnns o neuromorpic hardware. Frontiers inNeuroscience, 16. Stewart and Nefci, 2022] Stewart, K.M. and Neftc, E O. 2022). Mta-learning spikingneu-ral netorks ith surogate gadin escen.euromorphic Coputing and Egineering,24):044002. Pbliher: IOP Publishing."
}