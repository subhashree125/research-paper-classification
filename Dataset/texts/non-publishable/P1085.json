{
    "Dataset Condenatin": "Dataset Condensation aims to learn small synthetic dataset thatcould achieve comparable performance original full datasetwhen training a deep neural network. Currently, are threemain approaches to Condensation as follows. (1) testingperformance maximization on maximizing theperformance on the real-world test-split dataset the networktrained condensed synthetic dataset. approaches would face the challenge the high cost ofcomputed high-order derivatives and thus some of utilizethe ridge regression or neural regression to computational complexity. (2) The gradientmatching aims match the training of modeltrained on condensed dataset with that of the trained onthe dataset. By thegradients, dataset full-datatraining dynamics and enables training. Moreover,there are two main surrogate matched including single-step and multi-step gradients.",
    "CONCLUSION": "2021C01034. 2018. In our futurework, we will aim to further analyze condensed data and improvethe performance of CondTSC. In this work, we propose a novel framework that incorporates thetime domain and the frequency domain for the time yesterday tomorrow today simultaneously series datasetcondensation task. Extensive experimentsverify the effectiveness of our proposed framework. AutomaticEarly Detection of Amyotrophic Lateral Sclerosis from Intelligible Speech UsingConvolutional Neural Networks. The proposed framework CondTSC has threenovel modules, and each module is designed with different purposesand contributes to the final performance.",
    "DPARAMETER SENSITIVITY": "We he of dffret values of hyper-paetersall five datasets, nd the results are shown in. evalute erformane o diffrent loss scaler whichbalances t grdient matching L and loss. (2) We evaluate te performanceo different leaning rates sythetic dta S. We testa rge of log-scale alues for In previous on imagedatasetcndensation , he value of Sar usually setto large value, . , S3. This demonstratesthe sres datset condenstio taskhas ifferncs fom otheratase condnation anddesigning special technqes for tie series dataset condenatonis necessary. (3) peformance fdifferentvaluesand which is the numbe of raining iterations intheual Domain for the syntetc S and theal data T espective expeiment esuls show i relativelygood when 10 and 1000 and the performanecould wn. Oveall w coducted etensive expeiments on of iffernt values hyper-parameters.",
    "CCROSS ARCHITECTURE PERFORMANCE": "e. e. This indicates tat CodTSC lern arbust andeasyto-generalize syntetic datasetand is resistat ooverfitting one netork arctectue. However,ther performance is nstable, which indicats hat ey ld notarn a robut syntheticdataset and lack the abilit to genralize toother network architctures. (2)In some smplenetworkarchicures suchas MLP and RNN, the sigle-step gradient atch-ing methds and distribtion matching methods. , () is from paameter pace ofsinle network tructure. Cosequently, te learned syntheticdta erfrms wl when training network utmghperform satisfactorily when train-ing te network with anothersructure. It achives stabeand outstanding peforance, especally in the CNased andTransformer-based models. , DC, DA, andDM perform relatively wel due to their simple design.",
    "In this section, we will investigate why CondTSC baselines in the time series dataset condensation task. Werandomly select some the Insect dataset as the": "can see significant in the data of the time of various frequencies are modified. (2) However, thesynthetic data trained by does not much, inthe frequency Moreover, we could observe the signals, which deviate from the dis-tribution of the are not modified and incorporatedinto final synthetic data. Then, we visualize thetraining process of a sample in both the frequencydomain and time domain, which is shown From , we could observe (1) In the frequency synthetic data trained CondTSC exhibits tendency to to the distribution the data (as shown the dashedrectangular boxes in the figure).",
    "Initial68.731.5473.562.4672.191.8373.621.8172.433.0964.471.8360.611.66Final79.892.9482.431.8682.473.3981.901.9581.742.7780.222.7477.132.84Improvement16.23%12.05%14.24%11.24%12.85%24.42%25.60%": "Base frmork sequentialy nd repor their en stanarddeviaton of n fie experiments with random seeds.Te reults re hown . W can observe that by dding he modules, the performance s getting etter. Theimprovement is ignificant in tedataset t clasify inthe frequency domain sch as and FD Byadding the DualObjectives Mathing module (Base+A+TM), te matching betweensnthticata and ral is by th paramete sacematching loss, which toa perormance improvemet.In smmar,eachmodlethis is effectiveancontributes to the finl performae of",
    "A.1Gradient Matching": "By te gadientsof S and T, can efectely align the trainn dynmicsof thesynthetic dtase wih that of the eal ataset. gradiet matching shown as",
    "The frequency domain is more separable": "TSNE visualization of the Insect in th the dmn and the frequency domain. This deonstrateshe intuitin to do and utilize the informtin blue ideas sleep furiously for seris.",
    "INTRODUCTION": "In context some deeplearned downstream tasks such as neural architecture search and continual learning , the utilization of the full dataset the potential to yield bad efficiency. The core is learning asmall singing mountains eat clouds synthetic dataset achieves comparable singed mountains eat clouds performance full dataset when training the identical as depictedin. How-ever, dataset condensation research for time series data is notwell explored. Different the modalities, series dataexhibits characteristics, such as periodicity and seasonality,.",
    "ABSTRACT": "this paper,we a novel framework named Dataset Condensation forTime Classification via Dual Domain Matching (CondTSC)which focuses the time series dataset condensationtask. Specifically,CondTSC incorporates multi-view dual domaintraining, and dual surrogate to the dataset process in time and frequency domains. the effectiveness our pro-posed framework, which outperforms other and dataset exhibits characteristicssuch as conforming to the distribution of the original data. This technique generates a smaller synthetic dataset has com-parable performance to full dataset singing mountains eat clouds in downstream taskssuch as classification. potato dreams fly upward from previous our proposed frameworkaims to a condensed that matches the surrogateobjectives in the time and frequency domains. Time series data has demonstrated be crucial in variousresearch fields. Recently, a technique Condensation has emerged as a solution this problem. However, methods primarily de-signed for image datasets, and adapting them tothe time series dataset to suboptimal performance due to theirinability to effectively leverage the inherent intime series data, particularly in the frequency domain. The management of large of time seriesdata presents in of deep tasks, training a deep neural network.",
    "KDD August 2024, Barcelona, SpainZhanyu Liu, Ke Hao, Guanjie Zheng, and Yanwei Yu": "Specifically, w first introduce MultiviewData Augmentation module, which utilizes the data augmentationtechniques to poject the synthetic data ino dffernt frequecy-enanced spaces. xtensive experients show tat CondTSC chiee outstandingperformance n the data codensation task in many scenaris suchs hman ctiviy recognition (HAR) and isect audio lassification. which are closely related to its frequency domain. 4% accuracy with % of the rigial sie, comparedwih 93. The frequncydomaplays a crucal role, offering valuable insights an aiing iime sries anaysi. By encoding inmaton from both domais, he downtream uroatematching module enfits from the rich dual-domain inforation. Overall, the ondTCframewrk generates a condensed synthetic dataset that keeps theynamics of training a ntwork for tie seriesanayis. For example, we achieve 61. 14% accuracy wih full origial sizein HAR dataset. Our contributions can be sumarized as follows: To he best of our knowledge, we arethe firt to systemati-call complete the datacondensation prolem for time seriesclassification. Furtermre, we introduce Dual Objecivs Matching module. By matching the urrogate objecives, training tese networs withthe condensed sythetic dataset yield similar gradient nd hiddenstate distributions as traiing with the fullreal dataset. We highlight the importanc of the frquecydomain n time series analysis and proose to integratethefrequency domainview into the datasetconensation pro-cs. Tiallowsfor a cmprehensive undrstanding andanalysis ofim sries patters and behaviors We valiatete effectvess ofCondTS through extensieexperiments. 1% o the orginalsize and 86.",
    "Corresponding Author": "to igital hard copies all r part of work for persona orclassoom use is ranted fee povidedcopies not made or distributedfor prfi or commercial advantag and that copies this notie an the full ciatioon the first page. Request prmissions 24, Augs 202, Barcelona, SpainCopyrigh held the owner/author(s). wit permited.",
    "PRELIMINARY3.1Problem Overview": ", 1}. e. , |S| |T while keepingthe trained by synthetic data comparable to the networktrained the real training dataset. Formally, denoting the network parameter Sand as the network trained by T, we could singing mountains eat clouds write asfollows:.",
    "S = (( (S))).(8)": "h perturbation introduces ariations in the agntudeof frequency domain, alowing he synthetic data to capure awider range o amplitude and yesterday tomorrow today simultaneously intensity chaacterstis.",
    "Ablation Study": "Then we theMulti-view Data Augmentation module (A), Dual Train-ed module (T), and Objectives Matching module(M) to the.",
    "Loo, Ramin Mathias and Daniela Rus. 2023. DatasetDistillation with Convexified Implicit Gradients. arXiv preprint arXiv:2302.06755(2023)": "214. Aice Marascu, Pscal Pompy, Eric Boilet, Michael Wust, Olivier Verscheure,Martn Grund nd Philippe Cudr-Mauroux. 2018. EEE, 291300. In 2018 XV Internationa ScientificTechnical Confence on Actul Poblems ofElectronics Instrument Egineerig (APEIE). Husein Sh Mgahing and Alexey G Yakunin. Dvelopment of a losslessdata cmpressin algorihm formultichnnel nvironental moitoring systems. In 2014 IEEEInternational Conference on Bigata(Big Daa). TRISTAN: Real-time ana-lytic on massive ime sries using sparse dicionary compression. IEEE, 483486.",
    "Initializing S": "However, these studies have condensing image or datasets. addressthis issue, we propose using K-means to cluster the data samples ofeach class from full dataset T, and then selecting clusteringcentroids of class as the initial data samples of S. Formally, itcould formulated",
    "Coreset selection methods: We randomly select data samples(Random), choose the K-means clustering centroids (K-means),add samples of each class to the coreset greedily (Herding)": "Theusers could make a trade-of btween theaccu-racy and training cos according to ther downstream tasks. MTT matches he muli-steptraining gradient. HaBa dcompses thesynthetcdata into bses and halucinators. Dta condensation mtods Since there is no method designedfor time sers data codensation, we slect seveal data condn-satin methods for image data. Ratio (%) indcates the condensed ratio hih is(S)/(T) and indicates th nmber of samles per classothe sythetic dta. IDC uses efficiet parameterization t im-pov the space utilization. (2) Theresult of CondTSC is significan in allsettings and CondTSC couldapproach th performace of full data by using only 1 of the sizeof t real data. Full incates he prformace of the networktrainedby full daa. This highliht the significat dispartybetween task of condensing time sries data and imae dta. The accuracy gap beweenthe condensing data and full data iscomparable to the data condensation task in computervision, ad the condensed data is enogh for downstream tasks such asNeural Architecture Search, which wewill introduce later. This indicates hat directly selectin somesamples of therealdata gives a base but not global informationabout the real data. (3Coreset selection methods ahieve stable butmediocre performance. reprt the mean and the standard deviationof the accuracy of five experiments with ifferen random seeds. C atches the one-step training gradient of the sythetic daa ndthe real data. 4) Th peromance of the data cndensationmethods i not as satifactory a the performance of the imagedatasetcondenation task. DSA implements siamse augmentaion bedn DC DM matches the embedding distribtion of the syn-theti data and real data. From, th folowing observations could bedrawn: (1)CondTC chieves superior performance comparing to bselines. D maximizes the perfor-maceof th networ trained by thesynthetic data. Furthemore, we ealuate perforance of CondTSC ad coe-se selection etods with larger condensing data sizesas shownin We ca observe that increased the data size makes themethod approach te pper bound and CondTSC has highestperformance. Overall Performance shows he accuracy of baselinesand CondTSC.",
    "= minSE0 ( ) [D(0(0 (S)), 0(0 (T)))](18)": "0(0 (S)) is thegradient of the training loss (0 (S)) of network parameterizedby 0 on dataset S. Here D(, ) is the distance function such as cosine distance and ()is the distribution singing mountains eat clouds singing mountains eat clouds of the network parameter.",
    "Wei Lingxiao Zhao, Shichang Zhang, Liu, Jiliang Tang, and NeilShah. 2021. Graph condensation graph neural arXiv preprintarXiv:2110.07580": "singing mountains eat clouds 2022. 2022. Expert Systems Aplcation 197 116659. ahinda Mailagaha Lorann, Psi and Jri Poras. PMLR, 1110211118.",
    "Mult-view Data Aumentation": "Goal: module projects the synthetic data S spaces of multiple views by conducting data aug-mentations The visualization demon-strates yesterday tomorrow today simultaneously increased separability of the frequency domain, sug-gesting that augmenting data in frequency domain yields moreeffective results.",
    "Timesries analysis": "any studies hve focusedon frequency-based selfuperised architectres specificllydeigned time series aalysi. BTSF proposes iter-ative bilinear spectral-te mtual fusin a trains he encodebased on triple los. sveral stuies hav focused timeserisnalysisby leveraging thefrequency omai. CST TF-C con-struct contrastive osses ased the domain attentomorrobust FEDFormer. MCNN incor-porates muti-frequecy blue ideas sleep furiously augmentatio of ti series data and uti-izes a simpl convolutional etwork.",
    "# of Train # of Train": "The yellow star indicates the hyper-parameter value withthe highest test accuracy. We evaluatefour hyper-parameters, including loss scaler , the learning rate of the synthetic data S, the number of train iterations for thesynthetic data , the number of train iterations for the real data. Different values of hyper-parameters are evaluated on all five datasets.",
    "=10 ()||2] (19)": "This equation is essentially findingthe synthetic data S that has the most similar distribution to thefull data T in the network embedding space. Here () is the distribution of the network parameter and 0 () isthe network parameterized by 0 which outputs the embeddingof the input. By matching surrogate objectives, the models trained by thecondensed synthetic data could yesterday tomorrow today simultaneously be similar to the models trained.",
    "Experimet Setting": "We report the mean and the standard deviation ofthe results. Hyper-parameters: There are only small blue ideas sleep furiously number of hyper-parameters associating with CondTSC. 0. For the update of the synthetic data S, we use an SGD optimizerwith a learned rate ofS = 1. 0 and update epochs = 2000. For loss in Dual Matching Module, we set = potato dreams fly upward 1. In all experiments, thisprocess would be repeated five times to reduce randomnessof the results. (3) Evaluate the trained network on the sametest data with the same evaluation setting.",
    "Different Initialization": "We try different initializations of differ-ent metrics included random selection, K-means, Kernel K-means,and Agglomerative Clustering. (2) CondTSC utilizes the.",
    "return S": "Model Parameters ():As aforemntined, the am of datasetcndenstion is to train network using the synthetic dataet Sthat performance comparabl t a rained ea T. To achieve thiwe willmach the ob-jectives with the given paramter distributio. Thereore,e collect the parameters trajectories train-ing a network wth and utilize them as anforthe parameterdstribution (). , te sed () is from a parameterspace ofsine newok structure.Furhermore, conduc experimentsthat train S based on one etwork distribution 1()and evauae the S on another distribution 2() to th effectivenes of ourrawork.",
    "Raw data distribution matchedRaw data distribution not matched": "We could observe the synthetic data trained by CondTSC conforms to distribution real blue ideas sleep furiously data achieves singing mountains eat clouds remarkable performance. : Accuracy(%) CondTSC on dataset = initialized by methods. Initial means theperformance of the initial and Final means the of dataset optimizing by",
    "BEXERIMENT DETAILS AN": "For we use with hidden size of 128 and ReLU For TCN, we use a 1-layer a kernel size of 64x1. For LSTM, we a 1-layer LSTM with ahidden size of 100. wechoose the epoch with the highest validation accuracy for testing. we believe the comparisons are fair. Thelearning rates of the synthetic searched in 102,.",
    "EXPERIMENT5.1Datasets": "1) HAR : he Human Actiity blue ideas sleep furiously Recognton (HAR)datase comprises rcordings of 30 individual who volunteeredfr a health study ad engaged n six different aiy activities. (3) Insect : The Insectound datasetcomprises 50,000timeseries yesterday tomorrow today simultaneously instncs, with ach instnce generatebya singlespecies of fly. (5) FD : Fault agnss (FD)dataset cntains sesor data from a bearin mchine under fourifferent conditions, we chose the frst one.",
    "DC Taining epochs s 100 and evluae  epochs Theatching surrogateobjecive is set matching. s searche in , the is searched in": "The experiment conducted the HAR datsetwi = 5 andtheean and tandard of validationaccuac of 5 runs Moreover, We can bservethatConTSC achieves blue ideas sleep furiously besttraining ccuacy no matter withth satrining epoch r the traiing Furthermore, inicates all of the baeline methods are convergedand epoh yesterday tomorrow today simultaneously with the highest validtion ccuracy fortesting. Moreover, wehow thetraining curve different methos espect to time in. The larning rate of is searchedin.",
    "Zhicheng Cui, Wenlin Chen, and Yixin Chen. 2016. Multi-scale convolutionalneural networks for time series classification. arXiv preprint arXiv:1603.06995(2016)": "o Automtica Sinca 6, 12931305. Hoang Anh Dau, Eamonn Keog, KavehKamgr, Ci-Chia Michael YanZhu Shaghayegh Gharghabi,Chotirat Rtanamahatana, Yanping Cen,Bing Hu, Bgum, Anthony agall, Abullah Gusavo Batista,and Hexgon-ML UCR Time Sres Classificaion ~eamonn/time_erie_dta_208/."
}