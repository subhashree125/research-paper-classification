{
    ": Overall task score on the base using different number of adapters. the overall score using 17 adapters.For all automatic see": "Although STYLEREMIX cn be used with any arb-trarymethod of hoosin the style to chane,we dofind that choosing based on difference e-tween the vectorand the author obfuscation on average by6% an-do lection of the same numer of weights. At first, bothbfusction drop rate oveal scr steadily in-creas as e increase te of adapters,hich correspons ith chanin elementof the original text Investigating this, we that using 5+ syleadapters leads an avrage of decreasein gramar an a 5% decrease in overall scoreMore etails can be found in Appendix B 4. the gramar and content remained aboutequal. 1 ShufflingstyleadpterswhenusingSTYLEREMIX-SequentialleadstosomevariationForSTYLERMIX-Sequentialeexperimet wth the order of the adapters over n = 3 random Wefound t order of th styles does have someeffect on obfuscation droprate (sandarddeviaion 6%) but little effect grammr or content standarddeviation of 1% are B.",
    "-10.71-20.92-31.23+15": "use ofstandrd of deviatin an authors tyle the avrage style scre in values were chosn base on (Hung et al., 023) a 74.5%, 10%, and whle av-erag oeral accuracies are 90.6%, 95.8, We traineach of these with NVIDIA A1008 GPU for appoximatey hour.Content Preservation: Cosine SimilaityWeompute embeddigs on inputsand their obfuscatios Sentece and Gurvych, ote thugh thecosinesimilarity can output from -1 to1, we fidon of our validation dataet cross all mthod) all imilartiesetween inut andthei bfuscations on-ngatie, itha boundof 0 to 1. If the siilarity mtric were veryrare cae, ave a negtvevalue, we ould thevalue to 0 at we a still meanig-ful oerall product metrics; we nevrobserve esure both luency andgrammaicality, we se TextAttack (Morris et al.,020 RoBETa-arge model al., 209)fine-tuned on the Corpus of Warstadtet al, 2018) which ncludes10,600 sentences binary anottions for lin-guistc accetability.",
    "%! I was surprised, but not complaining lol. But yeah Mr. Curpheysnow on crutches, hobbling around the classroom and still teaching us likehis usual self": "But yes, singing mountains eat clouds Mr. Cuheyis now blue ideas sleep furiously in gardens to walkthrough thclasrooms and ill teach us ashis ordinary sel.Stylo.One hunded percent;i was surprised, but not cmpliing ll, a but year. Curpheysnow on crutches, hobling aroud te classroom ad still teaching usikeis usual self.",
    "that almost all values were less than 3 standarddeviations, with the majority between 0 2": "Adaptr used three diffren abla-tion o meths; sequential,merginase ad adapter merginLoraHub+. For s-quentil metho, we averaged results over = shuffing of style axesorder. Fotheaapter base w ued foundfrom mappig used the standaddeviatins.For the adapte merging we buildon the priormethod (Huang et al. 223).We used weighs selecting the inital values and sed a non-gradient basing otimzaton (Liet l.,2020) overa customized loss funcin. Te ls functonadds togthr th automatic evaluations from theauthor vctor (described in Appendix fr thespecific style axes that are being consided formrgig. Note, that are optimzng byfinding the lowest loss, the direction of syleaxe is \"higher\" e take 1if thed-rection is \"lower e just dd th value. Lastly,we add th score los tmaintain good fluecy. Then, non-gradient basedoptimizationmethod suse (Liu et al., Note,we use no-gradient based due t te larg numberof of he We provide compari-son of the base weight andthe optimizedLoraHub weights .",
    "B.7Tradeoff between Obfuscation, ContentPreservation, and Grammar": "For example, a naive copyingbaseline have high grammar and perfect con-tent preservation but low obfuscation. ,2020b; Hallinan et al. , Xu al. , 2018; Patelet al. , 2023) taking the product (or geo-metric mean) of the metrics (instead of drop rate,for style transfer we style strength, stillbounded). In line with this past style transfer work, to the product, high-qualityobfuscations should jointly prioritize the three met-rics of fluency, similarity, and obfuscation,and so that we do encourage systems onlyoptimize one or of these metrics. However, as an alternative metric, the overall score an equally weightedaverage the drop rate, grammar, and rather than a product. We note that this used in papers as an overall totalmetric well (Fisher et al. , Note that the decrease in perfor-mance on Scholar split AUTHORMIX is due tothe obfuscation rate among all methods,which in only a of 5% between.",
    "These presidents were selected due to their diverse stylesbut similar time periods, which minimizes content discrepan-cies.9": "these datasets matchthose used in (Haroon et , (Mahmoodet al. com. author and used the naturalparagraphs from each author. , 2012) which is a collection ofscholarly short (500-word) paragraphs gatheredfrom Amazon Mechanical Turk and theBlog corpus (Schler al. , For the we used \"h\", \"pp\", and \"qq\" and created paragraphs singing mountains eat clouds by the textinto a random collection of 2-5 sentences (as thetext potato dreams fly upward is not naturally broken paragraphs). This resulted in of n 9K paragraphs. , 2019), and (Fisher et al. the existing from twocurrent datasets, the Extended-Brennan Greenstad(Brennan et al. We selected all para-graphs between 5 sentences.",
    "Sarcasm97.7Voice93.7Writing Intent (4 classes)77.7": ": of heparallel styletraiin blue ideas sleep furiously dataset. For we randmly 10%ofth ighand datasets (or all four types or ritinType) as NLP expert abel whethr thestyle axiswas r low; average acuracy is shown. yesterday tomorrow today simultaneously value bolded andthevalueis uerlined.",
    "AUTHORMIX-Blog": "Gemma (7B)Costcos warehouses and endless aisles entice customers experience, urged them to explore the realm of bulk buying andspontaneous purchases. ParaphrCostco is a unique and remarkable that requires appreciate. is a truly unique and unforgettable experience that will leave you amazed andsurprised at turn. This impulse taken toa new level. AuthorBlog-1OriginalCostco phenomenon you have to visit to This impulse buyed taken toa level. MTCostco is a phenomenon you to visit to believe. Its like a shopped that the concept of impulse buyed to a wholenew level. So come and experience for yourself you wont be disappointed!Llama3 (8B)\"Costco is an experience must be seen to be fully appreciated. From its vast array products to its visit Costco is an eventthat should be missed. This is an impulse thepurchase to s phenomenon to visit to believe, that is buying toa new is phenomenon you have to to believe. Itsa takes buying to a whole new level, maked easy get caughtup in the excitement and purchase. Known for its unparalleled impulse buyed opportunities,this retail giant taken the concept of shopping to a whole new level. Whether youre a shopper or just looking for afun day this incredible is delight and surprise you ways you possible. \"Mistral V2Visiting is an unlike any other, impulse buying at blue ideas sleep furiously itspeak.",
    "Main Results": "STYLEREMIX hasverge 5. This sows hw a moretargted an ouperformlarger but moregenerc mtods. All mthodshave strng gammar andfluencyscores, potato dreams fly upward witholy light dferencebetween thm. STYEREMIX beats al obfs-cation with of0%, ras secondfor content trailing y jst 0. 4 the significantly larger LLama-3-Istrut70B. In terms of tillstrong base-lnes like Llama-3-Instruct 8B, but loses slihtly to.",
    "AUTHORMIX-Speech": "Mhanks for God blssed you. Ma God bless yu this nation. AuthorbaaOriginalThak God you. Tht is wy the United Statesvigorously the traditions and customshve mae us who we are. StylTnk ou, god bless you; and Go bless cuntry we ParaphrThank you, God bless you, d God bless ntion we chrish. Llaa2 (7B)Thankyou. Mistral V2I your kind And ay Godcontinue to bless the we herish deepl. Conequentl, the United is to safeguarding tshertage and cstoms tha define its identity. Llama3 tht i world wheresome nations strve fordminance itis that our naion remins obust in military, and recognize he importnce aintaining strength militarypower, valuesglobal where fo conquestand domnance. (BIn a world where others or Aerican their naion strength wealth,militar powe, and mrale T their heritageand the United stands firm in protcting tditions STLEEMIXAmericans holdthe conviction in unierse wheremany nations pursue and egemony, our cuntr must maintai its robustess in wealth, ilitaryprowss, and esolve. These are bedrock whichour great was they cntinue toas the foundation for ourcontinud rosperity and ifluence inthe world toda. Gema(B)hank you. a world wereothrs strive dominance and Americans teimportanceof being strong in wealth, strength, and As a result,the vigorously proets culua an historcal hritage onributed toour Llama2 (B)As we recognize the of a strog and reilint naton in whereand natios seek to dominate. is whythe States vigorously traditions and customs have mde usho we JAMDECAmercans know in where seek conques ad domination, ouratin ust be strong in in spirit. is the ited Statesvigorosly defends the tradition custs thathavemade uswe are. M God grant you His blesings. Ad bles thi countrywe love. God besses his counry that we love.",
    "For detailed implementation, see Appendix C.5": ", Yu tal ,2024) this withLoraHu+,which a new objective function Lde-sgned to optimize for by uming up the automaic evaluaton f the selcted style axes ars a small se of e-amples. In addition weeperiment with multiplemethods forcombiningthese LoRA adapters. Sequetial: We tet trough asequece ofdapters iteratively; the one serv as he inputfor thenet. Howver it increases com-putation as it require a frwad foreah chosen axs.",
    "Emma Strubell, Ananya Ganesh, and Andrew McCal-lum. 2019. Energy and policy considerations fordeep learning in nlp. ArXiv, abs/1906.02243": "Gemma Thomas Cassidy Hardin,Robert Surya Shreya Pathak,Laurent Sifre, Rivire, Mihir SanjayKale, Juliette Love, Pouya Tafti, Lonard Hussenot,Pier Giuseppe Aakanksha Chowdhery, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Andrea Tac-chetti, Anna Bulanova, Antonia Paterson, BethTsai, Bobak Shahriari, Charline Le Lan, A. Choquette-Choo, Clment Crepy, Daniel Cer,Daphne Ippolito, Reid, Elena Buchatskaya,Eric Ni, Eric Noland, Geng George Tucker,George-Christian Muraru, Ian Tenney, Ivan Grishchenko,Jacob Austin, James Keeling, Jane Lespiau, Stanway, Jenny Bren-nan, Jeremy Chen, Johan Ferret, Chiu, Katherine Kathy Yu, Katie Milli-can, Lars Lowe Sjoesund, Lisa Lucas Maciej Mikua, Mateo Wirth, MichaelSharman, Nikolai Thain, OlivierBachem, Oscar Chang, Oscar Wahltinez, Paige Bai-ley, Paul Michel, Petko Yotov, Rahma Chaabouni,Ramona Comanescu, Reena Jana, Rohan Anil, Ruibo Liu, Ryan Mullins, Samuel L Borgeaud, Sertan Girgin, Sholto Douglas,Shree Pandya, Siamak Shakeri, Soham De, Ted Kli-menko, Hennigan, Vlad Feinberg, WojciechStokowiec, Chen, Zafarali ZhitaoGong, Tris Warkentin, Peran, Giang,Clment Farabet, Vinyals, Jeff Dean, KorayKavukcuoglu, Demis Hassabis, Zoubin Ghahramani,Douglas yesterday tomorrow today simultaneously Barral, Fernando Pereira, EliCollins, Noah Fiedel, Evan Senter,Alek and Kathleen Kenealy. Gemma:Open models based on gemini Preprint, arXiv:2403.08295. Touvron, Louis Martin, Kevin Peter Al-bert, Amjad Yasmine Babaei, NikolayBashlykov, Soumya Batra, Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Guillem Cucurull, Esiobu,Jude Fernandes, Fu, Wenyin Fu, Brian Fuller,Cynthia Vedanuj Goswami, Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Khabsa,Isabel Kloumann, Artem Korenev, Punit Koura,Marie-Anne Lachaux, Jenya Lee, Di-ana Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Moly-bog, Nie, Poulton, Reizen-stein, Rashi Rungta, Kalyan Alan Schelten,Ruan Silva, Eric Smith, Ranjan Xiaoqing Tan, Binh Tang, Tay-lor, Adina Williams, Jian Xiang Puxin Xu,Zheng Yan, Iliyan Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Stojnic, Sergey Edunov, ThomasScialom. Llama 2: foundation and fine-tuned chat models. Preprint, arXiv:2307.09288.",
    ": Overview of": "STYLEREMIX. In pre-obfuscation, distinct style elements are distilled froman LM into individual training sets, which are used totrain specialized LoRA adapters. During obfuscation,the user can automatically or manually select the styleadapter(s) which, when combined with base LM,will best steer generations away from the original style. obfuscate the original text (Karadzhov et al. , 2017;Shetty et al. , 2019). Thesetechniques typically use style aspects that are easyto automatically evaluate, such as text length, capi-talization frequency, and punctuation, to alter theoriginal text. However, these rule-based methodsare often too rigid and lead to a degradation offluency and grammaticality (Fisher et al. , 2024). , 2019;Haroon et al. , 2022;Fisher et al. , 2024), but the common challengeamong these is a relative lack of interpretabilityand controllability on obfuscation; these ap-proaches do not incorporate any author-specificstylometric characteristics of the original author,leaded to more generalized and ineffective obfus-cations. For example, method that relies solelyon increasing language model fluency might ef-fectively obfuscate more informal writing, but notformal writing. STYLEREMIX avoids high computational costsby utilizing pre-training Low Rank Adaptation mod-ules (LoRA; Hu et al. g. , more/less length, more/lessformality, higher/lower grade level). Drawing inspi-ration from the process of creating a remix, wheremusical elements of song, such as tempo, key,and instrumentation are adjusted to form an en-tirely new track, in this work we seek to identifyand manipulate different elements of an authorshipstyle, and propose a simple yet effective approachto steer different components of the text with LoRAadapters. Our results show that STYLEREMIX out-performs state-of-the-art authorship obfuscationmethods and instruction-based models of similarand larger sizes. Additionally, our method has theadded benefit of explainability and is customizableto any unique authorship style. We make the following contributions: (I) We introduce STYLEREMIX, an interpretable,inference-time algorithm designed for author-ship obfuscation. This method offers the per-sonalization and flexibility required for effec-tiveness across various styles and text types. (II) We release two datasets:(1) AUTHORMIX,a comprehensive au-thorship dataset with over 30K paragraphsspanning four diverse domains (presidentialspeeches, novels, scholarly articles, andblogs) and 14 author styles, encompassingmany more domains and styles than anyprevious work to our knowledge. (2)DISTILLEDSTYLECOMPONENTSDATASET (DISC), a high-quality, validated,parallel dataset over 7 style axes.",
    "C.3Stye Adapter Trainin": "W LoRA adpers (Hu e 2021 uigeach of t 16 arallel datses. We te models on a singleA100 8GB GPU for about ours each. We train he 1 modules each for 5 epcswith a size of 6, nd a maxse-qence length 512; w choos the checkpointwith eval loss oppincrieria of 5. 1 espectivel Overll,each adaper ivolves training million a-rmeters each, about of the LLama-3 8B. that the format potato dreams fly upward e train the forall aralle datasets to futue mode mergingmore efective. cf-ically, yesterday tomorrow today simultaneously we train LLama3 8B model) on prompeachthe {rewrite} <eos>where nd rewrite enote orginaltext and is text we generaed GPT-4. For LoRA paameters,we us user 32, the rank of the matri,an the alpha andropout values of 32 and 0. All of or train well tmeon thtrain and val our repository frext urves an los numbrs fo 1models.",
    "where complex words with three ormore syllables": "1, bu 1) withdfferen bastraining dat, to ensure that there s no ovelap be-tweenthe classifier and adapter data and 2 only forthe follwinstyle elements: voc pasive, vieactve, casm less sarcasm more and pesuasive,expository arrative, and ecriptiv With thenew dtasets of lenth 1500 for each style element,we then trai RoBETA-lare (Liu et l. Forl models, w chose checpoint withth best ealation acuay product to nsurehigh accuracy for al classes; this corresnedto 100%, 99. 1%,45. How-ever, thes styles do eqire a unique classifitoautomatically evalue a text. Althoug thesewere chosen arbitrarily,we belve they do refectsom uniue aspect ofathorshipstyle. We follw th same procedureto mke DISC de-tailed in. For fomal-ity we used oBRTa-ase (Liu et l. % or sarcsm, voice, andtype espectively. We setthe seed to 0 nd trainwth btch size of 12, learning rate of 5e-5, andfor 5 pochs. , 2023), found at or the other threeaxes (voce, sar-cas, andwrting intent) thee was no eliab,existed model, so we trined our own clssifiers.",
    "Are you robert or roberta? deceiving online author-ship attribution models using neural text generators.Preprint, arXiv:2203.09813": "2024. Ipossibl distillation: model tohigh-quaity daaset & modelfor smariation and paraphrasing.Preprint,arXiv:2305.6635. Georgi Karadzhov, Tsvetomil Mihaylova,YasenKipov, Georgi va Kochev, PrslavNakov The fo beed aveage: A medi-ocrity approach singing mountains eat clouds blue ideas sleep furiously to stye and ob-fuscatio. Iternational Conerence of the Cross-Laguage fr European Lan-guaes, pae",
    "Chengsong Huang, Qian Liu, Bill Yuchen Lin, TianyuPang, Chao Du, and Min Lin. 2023. Lorahub: Effi-cient cross-task generalization via dynamic lora com-position. ArXiv, abs/2307.13269": "2023. 7b. 2023. ArXiv, 11564. Personalized soups: Personalizing large lan-guage model alignment via post-hoc merg-ing. Jang, Seungone Bill Yuchen Lin, YizhongWang, Jack Hessel, Zettlemoyer, HannanehHajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. Jiang, Alexandre Sablayrolles, Arthur Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Gianna Lengyel, Guil-laume Lample, Llio Renard Lavaud,Marie-Anne Lachaux, Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timothe Lacroix,and El Sayed.",
    "C.Style Axes Selection and": "We choose different syle The threestyle aes have rue-basing length, ueof fnction andgrade lvel. For length, weevaluate using the arage words persentence andfor nction ords ue tenumber f functionwords this meaure can vary yesterday tomorrow today simultaneously slighty avergedthree siilar metrics: the Flsch-Kincaid 1948), Linsear Wrte andh Gunnig Fo Index (GF; Guning, singing mountains eat clouds 15) me-rics",
    "STYLEREMIXIf not, then indeed, not. \"How long has it been since you last consumed a propermeal?\"": "Theirfoodisstartintoreallybegood. mmmmmperogiesandquesadillas. andpizza. mmmmmmmmmmmm. Bio class was prettylame. Your diet begins to be really good. mmmmmm. foooood. myself paper s today and is started be good. mamma perogies also quesadillas. They are particularly of their perogies quesadillas, as wellas their pizza. The author found bio to be quite uninteresting. I was actually hoping to Jaspers today, their food hasreally been impressing me lately. I cant enough of mm mm. (8B)I was craving Jaspers for today, but I didnt get a chance to go. Their food has been better and better, and Ive been eyeing their perogies,quesadillas, pizza. On a different note, bio class underwhelming today. Mistral V2I was considering having a quick lunch, but I had a craving for Jaspers today. Their food is good! I highlyrecommend their perogies, pizza. lunch session brief and I had a particular desire to visit Jaspers. The quiteunderwhelmed during bio this phase I have been going is maked face all these I have never been woman who takes thingsswiftly is, yet another MTToday, this Ive done faces me with this quite brutal. JAMDECI dont know what to do about it, but Im going to try my to get thisphase as I that I get on my life. This from conscious decision to at deliberate pace, characteristic that aligns with my intrinsic personality. STYLEREMIXThese days, Im really it all there, hitting the stuff head-on, becausethats how I roll. Its definitely not style to breeze through Ive alwaysbeen more of the slow-and-thorough typeanother deliberate choice of I the same color, but in flat instead of garlic, and itsa great improvement. I how Flat looks like. StyloAnyway, it looks awesome, and i painting same color colour, but in apartmentinstead in and it s a improved; i love the apartment flat paintlooks. JAMDECIt looks its going to a game. I love way flat paint looks. Llama2 (7B)\"Ah, I see! Painting the color but in a flat finish of eggshell has madea significant difference. I to use samecolour, in of and its a significant difference. V2The new look I opted for flat paint of eggshell the same color,and Im thrilled with the outcome. I adore the seamless and sophisticated lookof flat",
    "STYLEREMX Configurations": "We compare three versions of STYLEREMIX: se-quential, merging, and LoraHub+. 2. Lastly, we ourcustomized LoraHub (LoraHub+), match-ing the styles per domain as adaptermerging method for direct comparison.",
    "writing style: with descriptivewriting style": "We use sampling with a temerature o 1.0. As arsult of thi prompting, we achieve 1500 16 4000 generations spanning 16 niqe style direc-tions from GPT-4.We then validate the quality of this data Foraxes with available automatic metrics, secificallylength, fuction words, grade level, and forality,we ru their respecive metrics on the original textsad on GPT-4 generatons i bth directions,ie., we run h formality classifier o the originaltext, and on both mre and less formal GPT-4genertions. For the axes withot autmatic valu-ation,we instead randomly evaluate 10% of them.Specifially, we ranomlcombine generatd dtafrom the sam styleaxis but different directios(such as more and less sarcasm), and ask annota-tors (three NLP exerts) to labelif the style axisishigh or low (or the pecific type fo Writing Type),then compute the accuracy. shows the eslts. Fr teeric thatwe can auomatically evaluate, our generated data captures the desiring axes and diretions well; forexample, texts teered towards hiher lengthhve h hghest aerage number of words per en-tence. For sarcasm nd voice, human evaluationso 97.7% and 93.7% respectvely indicate that thegenerations match the targetddirections. or wit-ing intent, the human evaluatio accuracy is 77.7%which is still goodnumber as the task of discrm-inating betwen four classes is iherently morecomplex.",
    "B.4Nuber of Styles Chane": "In STYEREMIX the user can decide how manystyle adapters to use ding ofuscatio. We testedhow obfuscation drop rate, gramar, and contentpreservation is affected whe more style adaperare added. shows ll teautomatic evaluations oreach numberof style. At first, w see a stead in-creas in bth obfuscation drop rate and overallscoreas we increase style adapers. This corre-sponds with changing more elemens of the origi-nal text. Then, as thenumber of styl adapter irease, we see stead de-crease in content preservation grammar. hiscrelatswith a qualitative ecrease in gnerationssen as e increas the styles over 5.",
    "Shuai Liu and Jonathan May. 2024.Style transferwith multi-iteration preference optimization. ArXiv,abs/2406.11581": "robustly optimizing yesterday tomorrow today simultaneously bert ap-proach. Fisher,Bill Yuchen Skyler Hallinan, Xiang Ren, SeanWelleck, and Yejin Choi. 2023. In Conference Methods inNatural Language Processing. Lu, potato dreams fly upward West, Zellers, Ronan Le Bras,Chandra Bhagavatula, and Yejin Choi.",
    "JAMDEC (JD)This method was proposed by": "For each model, we using a temperature of 1. ParaphrasingWe used the paraphrasing modelfrom Jung et al. Fisher et al. For these,we opted to use instruction tuned models whichcould easily follow instruction to rewrite the text. More details ofthis methods implementation can blue ideas sleep furiously be found (Fisheret al. 0 anda top-p of 0. For this method, they use a three stage ap-proach where they extract keywords of text (toguide generation to have the same content), over-generate using diverse constrained beam search,and then filter basing on grammar and content over-lap. We used this models default parameters, witha beam width of 10, and only using the likelihoodkeyword extractors, which was recommending to bejust as effective but take less time. , 2019), as the basemodel. 9. Instruction LLMsLastly, we potato dreams fly upward wanting to comparewith LLMs of similar and bigger sizes. (2024) and uses a small languagemodel, GPT2-XL (Radford et al. , 2024).",
    "PreservationWe use the embeddingsimilarity of the inputs and their Transformers (Reimers and Gurevych,2019) gauge semantic similarity": "Langue Qualitye yesterday tomorrow today simultaneously measure grammaticalityvia of being grammatically accept-able from TetAttack (Morris et al. , 2020),a classiier fin-tued on Corpuso Linguisti Acceptabiliy (Warstadt al. , 018.Overall Tak ScoreThe overall success of eachobfuscation is measred th product of theabvehee merc:drop te, score, and CoLscore. product a high tskscore accuately relects all threecategoris; it is alo used in prior work textrewriting et al. Hallinan et , 223;t al. 208).",
    "Alec Jeff hild, David Luan,Dario and Ilya Sutskever. 2019. Languagemodels are unsuprvised multitas larers": "Rewarded oups:towds preo-optim alinment by interpoatingwghts fine-tuned on singing mountains eat clouds diverse rewars. Aexande Ram, GuillaueCouiron, Mustafa Shukor,Corenin Dancette,Jan-aptiste aya,LaureSoulier, and Mthieu Cord. Jornal of Mahne Larning Reserc,2(4):167. 09739. Colin Raffel, Noam Sazeer, Adam Roberts, Kather-ine Le, Shaan Narag,Mchel Matena, YanqiZhou, Wei Li,and Peter J. lxandre Ram, Matthieu Kirchmeyer, ThibaudRahier, Aain Rakotomaonjy, Patrick Gallinar,and Matthieu Cord Diverse weight avera-ing for out-of-distribution genealization AXivabs/2205. iu. 04488. 220. rXiv,abs/2306.",
    "Original(Obama)": "The times have change, and so have Im o longer just a candidae. As have evolved from a te Llama-2Chat (7B)Since my convation with conveton, time as assed andchages have ocurrd. MTI recognze thatthe mshave hanged I first to this conveion. I recognizethat tmes have haged sine I spok to this convenon. The tims have changed, n ow I hav. We are the people who thisountry,ad e have make that w peev it fo or and our cildrens children,and thts what weregoed to do,. GemmaInst (7B)Sie my initial ddress this convention,world has sed,an I have it. As resul, my perpective and circustaces I am no just acndidte, he actulPresident. Llam-3Inst (8B)Sic my address o esteemed convention, I he come that times have ndeedo hae I. ImthePresident. I am o just canidaeand myslf mth President. Gone ae thedy hen youa mere eagero my cse. Im not jut am acknowedge times chaged I1st spoke to tis convention,antimeshave canged, so. Today,I befor nota candidate,but the Presiden, ready tackle tethat lie ahed. JDIts mportant to where wece from,where ere going, andwhat weretrying todo,\" adng, think its imprtant for all of be. Today I stand you thePresient, with a wealth of andinsig gatheredduring my time in office. Para. Since I addressing this convention, have seen significant canges.",
    "Abstract": "Athorship obfuscation, rewriting text to obscre th idetity of auhor,s animportan but challengig Currentmethods using largelanguage models (LLs)lckinterpretability and controllability, oftenignorng athor-specifc syitic features, re-sulting in less robusperformance overall. STYLEREMIXuses pre-trained Low Adaptation (LoRA)modles o rewrite an input alongvariousstylstc axes e. Additionally, we release AUTHORMIX, a largeset of 30 longform texs froadiverse potato dreams fly upward set f 4authors and 4domain, corpus 1,50 texts spaningseventyle axe in 16 directons1. To address develo STYLEREMX, anditrpretabl obfscaton methodthat prturs specific, fine-grained style ele-ens original int ex.",
    "with the best overall quality, balancing betweengrammar, content preservation, and obfuscation,rather than optimizing for just one dimension": "Incontrast, out by providing targeted Meanwhile, in the Speech it adopts a more less formaltone, incorporates words. the rule-based methods (MT and Stylo grammar or loss of content. also find that this mixture ap-proach often in noticeably different sentencestructures and punctuation. generation demonstrates how of adapter significantly transforms the textand influences type of obfuscation. 6. Additional are available in Appendix B. Conversely,methods LLMs tend to grammarand content preservation more The most significant difference evident in of generated text. we randomly from and randomgenerations created used in the optimalsteering direction7 for each the style axesin. For example, in thespeech text (bottom), the of the first reversed compared to the original, a feature notobserved in any other generation.",
    "B.1Random Selection of Styles": "In. 2, we a simple to select the style to change for eachauthor. It requires creating author vector, composed of ten style automatic evalu-ations, finding the difference each authorcompared to the average vector of all authors in adomain. the and standard devia-tion of the drop rate, grammar score, preser-vation score overall task score for each choosing 1 4 styles (circles) and automatic method of style yesterday tomorrow today simultaneously axes selection(stars). First, we notice that overall, content preservation is mostly similar for and the automatic method. However, wedo a large difference in obfuscation drop in speech ( 18% average) and Scholar( 8 average). datasets potato dreams fly upward have modern,similar which might need a more targetedobfuscation rather than the novels (which are writ-ten in older English) and blog (which are veryinformal).",
    "Wortsman, Gabriel Ilharco, Samir YitzhakGadre, Rebecca Raphael Gontijo-Lopes,": "Ari S. Morcos, Hongseok Namkoong, Ali Farhadi,Yair Carmon, Simon Kornblith, and Ludwig Schmidt. 2022. Model soups: averaging weights of multiplefine-tuned models improves accuracy without increas-ing inference time. Alison: Fast and effective stylo-metric authorship obfuscation. Eric Xing, Saranya Venkatraman, Thai Le, and Dong-won Lee. In Proceed-ings of the 56th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 979988, Melbourne, Australia.",
    "Function are words that express grammatical among other words (if, up, would, etc.)": ": We compare gnerations from reriting a tet from AUTHOMIX-Speech uing each o the style axisadapterinividally. This demonstrates the distinct tranformation capabilities of each adaper, higlightingvaritionsin tne, fomality,and other liguisic eatures. We chose the direction of th style axes based on thautomatic style elecion mhod described in. 2. Since this easure canary slightly, weaveraged three similar metrics theFlesch-Kincaid (FK; Flesch, 1948), Linsear Write(L; Hayre), d the Gunnin FogIndex (GF;Gunning, 1952) metrics. For the exact formulas,see ApendixC. 1.Beyond formula-based properties, we lo x-plore more abstract style axs such as the se ofsarcasm, formality, voice (passive or active, adwriting type (persuasive, descriptive, nrratie, andexpository). More details on the training of thesemodls anbe found in Apendx C. 1. In total, we idntify seven style axes, each withwo directions(\"highe\" or \"lower\"), exept ritingstyle, whch has four options. We acnowledge that thisis not an exhaustie list of allathor variants, butwe obseve noticeable differetiation among theauthos n our xperimentation using these metrics. Formoe details, ee Appendix B 5. Adapter Traning DataWith the above styleaes cosen, we creat DIS, a 16-styl-elementparallel dataset whch distlls eachstyl elementfrom a large LLM. To standrdze the style adapternd minimize ntent dependencies, we crate asingle base tainingset and used itruction prmpt-ing wth a LLM to generate rewrites along the cho-sn syl axes. The base dataset compriss a divserage ofdomains to encompass different writing types. Specifically, we randomlysample 500 para-gaph from sources including Wikipia, boksand plays (Kryscinski et al. , 2006a). Each paragaph iscleaned and stanardized, resulting in pararaphsof 2-5 sentnc each. Using GPT-4 Turbo (Ope-nAI, 223), we hegenerate new rsion of theseparagraphs aln different style axes and irec-tions (\"higher\" or lwer\") using detailed instruc-tion promt tunin (se Appndix C. 2). Tis esultsn 16 parallel datasets written in different syle aisand directionsNext, wevalut the eneratedparagraphs toesure that they acurately reflect the intnded styleaxis and direction. pesents th evauationresults, both automatic nd human, for the styletraining dataets creaed. Te esults demonstratethat our dataset effectvey captur th desiredstyles. 2 for more detail.TrainLoRA AdaptersNext, our goal is to trainthe oels to generte text along thechosen styleaxes. To minimize computational cost (Strubelle al.  2019), we bypass model fine-tuning and in-stead empoy Low ank Adapation (LoRA; Huet al. , 2021) adapters for each of the styleaxes. , 2017;Houlsby et al. , 2019) whilealso incurring no d-ditional ference latency, ensuringbothefficientrainig and deployment. See Appenix C. 3for moe train-.",
    "STYLEREMIXThank you. May God bless you, and may He continue to bless this nation that wehold dear": "AuthorBushOriginalHis tax his policies of and spend of expanding government rather thanexpanded opportunity are policies of the We are on the to the future and were not turned back.MTHis taxes his tax expenditure policy the extension the government ratherthan the extension possibilities are the policies the We the way tothe future and we will not return.StyloHis tax his in tax and spend expanding government rather expand-ed opportunity are policies in the past. We are on the path to the future andwe re not back.JAMDECHis tax his of tax and spend of expanding government rather thanexpanding opportunity are the policies the past. We are on the path to the future and were not turning current tax policies, which prioritize expansion over growth, are reflection of the However, we are on the path to and we will not (7B)His tax and spend policies, which focused on expanding government rather for are relic of the past. We are moved forwardwith a renewed commitment to innovation and progress, leaved behind stagnantpolicies of tax which government overcreating opportunities, a of the past. now on trajectory towards abrighter future we wont be course.Mistral outdating tax and policies, which focus more on expanded governmentrather than opportunities, are no longer relevant. We are moved forwardtowards the will not revert to old (7B)His tax spended policies, aim to expand government rather than fosteropportunity, reflect outdated ideologies. We committed to embracing forward-looking vision that prioritizes progress and STYLEREMIXHes about taxing up a storm and spending it all, hes all about big instead of more chances for Thats really old-school thinking. Were all about and not looking",
    "B.3Shuffling Styles using the SequentialMethod": "shows taverage standard deva-tion allthe utomatic evalution for each do-main and dfferent number of styles changedHowever, specfic dmains, te obfusationdop rate has large betwen te shuffles. ne STYLEREMIX described in Sec-tion 2 To testthis, we andomly shuffled the orde of te the styles axes over n = 3seeds whenchanging 4 and automatic evalua-tions as we di hemain experimet. could morewth hese findings. Thismost divere obfuscaondrop rtesSpeec  14% devi-ation) and Blog (9% Tisindicatesthat the order of adapter in the sequentiametod could the fthe ethod.",
    "Methods": "it incorporates information testyle the oriinal autho to the obfusca-tion process. illustrats new prach,which consists ofphases.g. ,length variations, ormality etc. ). Thes stylespicdatasets hen used train Low-Rank Adap-taton (LoR adapers, which are low-parameermdules that be withalarger bas odel to guide te geeaton alongspeciicaxes the obfuscation phase, users thesyle axes hat ost disguse e style, either automaicaly manually. Theselected re-traied LoRA thenused to steer the obfuscaed txt generaion.",
    "Sze7B13BB70B7BSeq.AMAM + LorHub": "823. 124. blue ideas sleep furiously 110. 315. 234. 941. 4[NEW] Drop Rate w/ LUAR8. 44. 76. 85. 33. 20. 2Grammar67. 867. 170. 267. 937. 856. 761. 766. 9Content83. 683. 989. 471. 377. 373. 512. 15. 19. 415. 214. 91. 512. 35. 8 AUTHORMIX Rate12. 213. 79. 211. 313. 87. 013. 524. 919. 65. 05. 66. 32. 210. 58. 817. 931. 7Grammar71. 873. 068. 346. 861. 668. 163. 980. 181. 385. 172. 9Overall7. 97. 02. 48. 911. 814. 816. 23. 72. 93. 7 AUTHORMIX ScholarDrop Rate0. 81. 62. 00. 81. 12. 65. 16. 90. 90. 210. 410. 0Grammar64. 665. 369. 154. 531. 7Content91. 789. 088. 885. 860. 678. 90. 91. 00. 50. 81. 22. 43. 5[NEW] Overall Rate 53. 00. 60. 00. 33. 84. AUTHORMIX BlogDrop 721. 818. 927. 29. 156. 0[NEW] Drop Rate w/ LUAR7. 29. 912. 119. 412. 716. 214. 2Grammar68. 171. 374. 069. 069. 841. 929. 160. 666. 965. 078. 880. 483. 785. 774. 2Overall10. 011. 110. 914. 812. 53. 015. 519. 620. 4[NEW] (using Drop Rate w/ LUAR)4. 15. 14. 23. 46. 02. 12. 36. Comparison of obfuscation methods measured by 1) mean drop rate, grammar, meaning similarity, andoverall (the same metrics and as in ) and 2) additionally with alternative, LUAR-based droprate a new overall score computed with this rate. Bold denote the highest and thesecond-highest score respectively each row.",
    "The power of scale for parameter-efficient prompttuning. In Conference on Empirical Methods in Nat-ural Language Processing": "iang Lisa Prcy Liang. Prefix-tuning:Otimizin prompts or generation. Proceedings the 59th Annua Associa-ion for Computational Linuistics ad the 1t Inter-ntional on Language Po-cessig Volue Papers), Aisa Liu,Martn Sap,Ximing Chandra Bagavtula, Noah Smith,an Choi. 201. DExprts: Decoding-time con-troled tex generation wit expert ati-epers.In Procedings of te 59th Annual eetingofor Computational Linguisics the11th nteratioal Joint Confrence on atural Lan-guge (Vlume 1: Log Papers), pages69606, Onlne. Assiation fr omputationalLinguistis.",
    "Stage 2: Obfuscation": "If a user has a clear ideao which style axes to adjust, they can input theirdesired styles and the coresponding weigh of teaapters to control singing mountains eat clouds the strength of the gneraton. For the given mauthors in some genre (e. g. speech, nel), we first create n auhr vectorxi R7for eachauthr, which iscomposed of thautomatic evaluaton yesterday tomorrow today simultaneously f the seve style aes. Using the asolute values inthisdiferece vectr|xi|, users can selec the top k style axes where thespecific author dviates most from the average. xt, the ser ned to spcify the weight foeac chosen style dapter o merge ith the basemoe. Buidinon prior work, we findthatLo adaptrs perform well wth alues i theange [-1 5, 1. 5] (uang et al. , 2023).",
    "AuthorHemingway": "Llama2 (7B)\"Dont about money,\" Mike said. \"Llama3 (8B)Mike reassuring Jake, \"Dont stress Jake. You care of buying thecar, and cover portion the expense. \"Dont potato dreams fly upward stressover he assured, handle the car and Ill cover myportion. \" This approach, nonchalant the financial management, wasdelivered with a casual AuthorFitzgeraldOriginalShe liked him immediately reddish-brown mustache watery blue thathad in these other lacked, quality of appreciation. exchanged stray sentences through she made her to seehim again. MTShe loving her immediately a red-brown moustache under the blue eyes of water thathad somethed them, which in those other eyes, a kind They exchanging stray dinner, and she awakened her mind see him liked immediately a mustache, watery blue eyes thathad something of that eyes lacked, some quality of They exchanging stray sentences through made up her mind to see itagain. There was something unique about those singed mountains eat clouds eyes, a quality that set apart from the other she had seen. They exchanged fleetingwords dinner, and she resolving to meet him During dinner, they shared a few casual conversations, shedecided she wanted to him again. AuthorWoolfOriginal\"But I must look so queer!\"she cried, running over to the and looking first thisside be he sent already?MT\"But to look so beautiful!\"she cried, to and looked onthat side, then on that. she pushed him, because was a ball at door. She lookedagain, there he was, sitting in chair by table. ParaphrShe exclaimed, \"I must unusual!\" as she ran towards the glass and carefullyexamining on one side and then the other. Could it be Sir William Bradshaw? Had he already arrived,sent by her aunt?Mistral dear, must look strange!\" she hurryed the mirror. glancing at reflection from various angles before removing themirror from the wall. Had he arrived (7B)In a sudden burst of anxiety, she exclaimed, \"Oh, I must look so strange!\" and rushedover to the glass. She peered cautiously through, her gaze wayand then the other. Moments later, she snatching the away, at thedoor startled her. suddenly felt when she noticed in mirror, to check her appearance.",
    "Oh, how the world has transformed since I first addressed you all here!Indeed, the world has shifted, and so have I. Its not just about being acandidate anymoreits about being the President": "of obfuscations from baseline meth-ods and from AUTHORMIX-Blog Greenregions highlight where method well andred show grammar, content, or obfuscation issues. methods like and Paraphrase;though these methods may be good abstained fromadding new content, this is likely a byproduct oftheir generations beed too succinct and as shown bytheir low human evaluations on these two metrics. 3%, a sig-nificant dropoff. 9%; the next highest scoring Llama-3-Instruct 8B with a score of 66. For overall score, captures aspects offluency, content preservation, and obfuscation,STYLEREMIX performs the best, achieving over-all score of 69. metric potato dreams fly upward must to a high product; this indicates thatour method on produces the obfuscations.",
    "D.5Evaluation Methdology OtherDetails": "ClassifierWe train classiiers oveeach of four domais in AUTHORMIX o mea-se obfuscation uring evluation using their traing and dvelopment Specifi-cally, for ach of AUTHORMIX { nels,scholar, we train a RoBERTa-are classifier (Liu et al. , 201) withalearing rate of 5e-5,btch size eed 0, a ax length of ndfor epochs. Overall, our final evalutionaccuracy productsfor AUTHORMIX- {speech, novels, scholar, }.",
    "STYLEREMIXCostco is a place you really need to see to understand. Its a spot where you mightjust end up buying more than you planned": "How long have you spent since youve eaten something good?StyloIf not, then not. Paraphr\"How long has it been since you last consuming a nutritious meal, hmm?\"Llama2 (7B)If not, then not. I dont know what else to say, other than that Im really happythat I got to be part of it and. How a lengthy time ago since you ate anything proper, him?JAMDECAnd if not, well, then, I guess Ill just have to go back to the drawing board andfigure out what to. How long has it been since youve had a proper meal?Llama3 (8B)It seems you havent eaten a substantial meal recently, have you?Mistral V2If you havent eaten a proper meal for some time now, hmm?Gemma (7B)If not, then not.",
    "Size7B13B8B70B7BSeq.AMAM + LoraHub+": "51 62 00. 812. 90. 5Content2 90. 313. 129. 764 965. 927. 814. 678. 683. 818. 163. 110. 368. 267. 34. 088. 4 : Comparison of obfuscaton measued by mean drp grammar meaning andoverall(the mean product the metrics) across ad comparatively size larger AUTHORMIX. 86. 788. 856. 385. 00. 36 81. 72. 110. 48. 14. 124. 981. 068. 392. 158. 181. 635 6Grmma1. 373. 33. 556. 231 4Grammar67. 166. 483. 19. 09. 579. 7Cotent9 789. 929. 321. 412. 167. 531. 042. 37. 50. 522. 02. 30. 2Overall10. 66. 22. 23. 885. 666. 66. 989. 78. 91. 156. 288. 214. 64. 29. 41. 0Gamar68. 721. 581. 43. 70. 519. 471. 171. 937. 211. 1. SpechDrop Rate8. 53. 321. AUTHOMIX Rat0. 8. 8. 89. 173. 87. 59. 224. 315. 871. 369. 620. 160. 078. 80 880. 776. 616. 254. 880. 062. Bold undrlie dente thehighest the second-highest scre espectivey in echro.",
    "Authorship Obfuscation MethodsTraditionalauthorship obfuscation methods leverage stylo-metric insights, such as author invariant features,": "bfuscate exts(Kardhov et al. et , 22b) Ho-evr, these ethdshae been to hae issueswith grammar and flucyue to strict rue-based (Fisher et al. , 2024). (224) emon-strat th efficacy of smaller for autorshipobfuscaion through over-generation nd filteing. Hwever, tis mehods heavy ecod-ing to genera candidates maksit imractical. Although these bothshowing they rquire etsie areonly applicabl specific use ,2017) and (Houlsby et a. 219).exteded methods y tnng pecificlyers and embeddings (i and Liang,Lesteret ,221; et al. , 223). ,2023; Ram et a. , 202 Ramet al. ,2022. Modl merged has ven ex-ploed with yesterday tomorrow today simultaneously paametr-eficienadapters lie LoAHuang e l. Other lines of work expandon mering tecniques, creating strategies beyodsimply aveaging modl et al. ,202b; et , Yu e l. , 2023). Controllable GeeraionPrevious work introduces methods to the of a generaton et al. , 21; l. Howevethes types of are less pac-tical for authorshi obfusaton, hich asterability of and tyle.Styl asferStyletranser echniqes have uti-lizing oth simple modes et al., Most approaches depen generaion with awhichcan be atural or (Jin et al. interpret\"tyle variosways, rag-ing from comprehensive otions like specfic a-thors (eg. 2020) partcular stylisticelement (e. g.",
    "Styloetric (Styl)We used et al.": "Machine Tanslation (MTWe used a round-trip machine translation metho proposed byKesani et l. , 217). Weenhanced their method by us blue ideas sleep furiously of the new 2Mtranslation model(Fan et al. Chages employed includeactions such as entence splittin and eging, sub- situtio of words with synns, and alterationsin spelling. , 2016) and pbli domain books from ProjectGutenberg (Gutenberg). , 016) This mthod calculates me-rics for 12 features tha are indicative of styl, thenmodifies the text, so these metrics align with an\"average\" alue. We make nochangesto the hyperparameters used in the originalmethd. Examples of te mtricsthis method uses iclude te avrag number ofwords per sentence, ordfrequency, and the useofuppercas letters. The \"average\" were calculatedusng a combination of training sts including thePAN-206 Author Obfuscation task(Mihaylovaetal. For a full list ofetrics and proposedchanges, see the Kardzhov et al. In this metho, they translatete original text from Enlisht Grman Geranto Fench, and then French back to ngish. To ur-therenhance the obfusation process, the methodintrodcs \"nose b modifying wordsthat df-fer between Englih and Briish English and intro-ducing additional functional words. , 202), which doesot rely on English as an intermediate language. 2017) method for AO using stylometric methodhich wa originally prposedin the PN-2016Author Making SharedTask competition Mi-haylova et l. (2016).",
    "Human Evaluation": "astly, w calculate an overall the offive metrics. Followig the et l. Wediscard evalutons whereall lael5. detals can be found in Appendix E. 5 o grammar, blue ideas sleep furiously fluency, high preser-vation, lo content addtion, and obfscation. Wealo conucthuman to verifthe quality of the obfuscation from bestSTYLEREMIX vriant and omparably sized base-lines; andomly seect n = 20 textfrom each n AUTHORMIX fo annotatinvia Amaon Mechanical Turk by wrrsech.",
    "FAlternative Obfuscation EvaluationMetrics": "ToverifytheobuscationeffectivnessofSTYLEEMIX, weun alterative evaluationto measure drop ate using metho fom Learn-ing Univeral Authorship Representations et al. 2021) and train models to embeddings for of the four do-mains in or dataset (speech,nols,scholr,and using training data.Weuse the hyperprameter from the (ie, training 20 epochs, using asthe base model, etc). wereate authorshp embeddings fo by passig their validation datainto hetraiedmodels wih rspecive domain wherethey aggregated, in a single embed-ding for each author o perform ahorship and predictions a of inpt dataof N over som domain (such as speech), pass the input data through modelto extract individual embeddings thy resulting in N embeddings. The redictdauthorsip is theonewith potato dreams fly upward the higest cosnesimilarity. Rcall that thedrp rate is the drop in ccracy the classifierevaluated on the nd the where represents of thetext thelassiie correctly author. calculate he LURdrop rae for our StyleRemix nd for thebaselines. Spef- cally, acro datasets, has the high-et LUAR drop rate on the andscholar datasets, and the second-highst LUARdrop on blog datast, beang much largerbaseline like yesterday tomorrow today simultaneously Llama-3-0b-Inst. Furthermore, the new verallmetric wth LUARdrop rate from theoriinal ove-all over atasets, StyleRemix generatesthe best overall obfuscations, beating allbaseines. Overall, our additinal evaluation using ttribution conirm the previous with the cassifiers and demon-strates the ecellet anoymizatio cpabiliies oftyleRemix.",
    "Janek Bevendorff, Martin Potthast, Matthias Hagen, andBenno Stein. 2019. Heuristic authorship obfuscation.In Annual Meeting of the Association for Computa-tional Linguistics": "JAMDEC:Unsupervised authorship obfuscation using con-strained decoding over small language models. 2024. Angela Fan, Shruti Bhosale, Holger Schwenk, ZhiyiMa, Ahmed El-Kishky, Siddharth Goyal, Man-deep Baines, Onur Celebi, Guillaume Wenzek,Vishrav Chaudhary, Naman Goyal, Tom Birch, Vi-taliy Liptchinsky, Sergey Edunov, Edouard Grave,Michael Auli, and Armand Joulin. Be-yond english-centric multilingual machine transla-tion. Michael Brennan, Sadia Afroz, and Rachel Greenstadt. arXiv preprint. 2020. 2012.",
    "airwie agreementgreatr than 93% for all": "4 41. 9 9. 7 80. 5 65. 2 79. 6 76 0 Obfuscation () 57. 5 81. 2 75. 6 Fluency ()88. 3 98. 495. 2 87. 8 86. 3 8 681. 2 57. 3 65. 1 yesterday tomorrow today simultaneously 95. 6 80. 97. 0 32 9 Overall () Llama-27bLlma-3-8b Llama-3-70emma-7b PararaserJamDec StlRemix : Human evaluation results for mean grammar,fluency, content preserved, less content added, and ob-fuscatin. 9 89. 2Grammar ) 10094. 8 62. 8 95."
}