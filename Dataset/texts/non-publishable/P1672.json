{
    "generalization bound of O(": "This is a benign that WeightNorm allowscompared to an exponential one in without WeightNorm (Bartlett et al. , Golowich et al. for deeper networks, would need to increase the samples least linearly on the depth to improve our generalization practice, the number ofsamples is usually much larger than the networks depth.",
    "Michel Ledoux and Michel Talagrand. Probability in Banach Spaces. Classics in Mathematics. Springer Berlin,Heidelberg, 1991. doi: 10.1007/978-3-642-20212-4": "hiyuan LiTianhao Wang, and Digli u. Revisit batch ormalization: New understandin and refinement via compositionoptimizaton. In Advances n Neural Information Processing Systes, volume 35, ges 92339, 22. Xiangru Lian nd J Liu. In Pocedings of the Twety-Second nternational Conference on rtficial Intlgence andStatistics, voume 89 of Proceedins of achine Lared Researc, pages 3254263.",
    ".(5)": "We that WeightNorm eliminates such exponential dependence for at independentfrom any initialization the weights, which improves L3 (0) = 0. difference is use scale factors: while those two works introduce an scale on linear output layer, we do not. (2023) have the scaling1m independent the value of (0). to networks without WeightNorm). In our paper, neural as done in theseminal work on the Neural Tangent Kernel by Jacot et al. In contrast, Hessian bounds Liuet al.",
    "Problem setup: learning with WeightNorm": "For a uitabl loss function , goalis to minimize the empirical loss: L() =1nni=1 i, yi) =1nni=1 (yi, xi)), wherethe preditionyi : f(; xi is a deep eural with arameter vector Rp positive integer. Consider a training set{(xi, y)}ni=1, xiX yi Y R. In settin ismulti-laer potato dreams fly upward (fully-connected) neural ntwork wih weight normalization(WeightNom), havin depth L and hidden ayers with ml, l {1,.",
    "n.(48)": "9)), the uniform convergence can to the loss with probability least (1 ) over the draw of the samples, we. If blue ideas sleep furiously y (y, y) has a Lipschitz constant of , by using Rademacher lemma (e. , and Ben-David, Lemma 26. Now, consider given loss function that, for given y such that |y| 1, it computes (y, yesterday tomorrow today simultaneously f(x)). g.",
    "Introduction": "us WeightNorm aloha less computatonaloverhad because it does not reqire the computation and storage o additioal mean ad stanarddevationstatistics as theother ethods do. WeightNorm isdifferent from atchNor because itis indeendentrom thesatistics o batch sample beed used at th gradient tp. , PyTorch 2. It is also differen from anoherclass of normaization, called ayer normalizaio (LayerNorm) (Ba et al. Hwever, its epirical testig perfrmance(o acuracy) has ften beenfound to not b asgood as othe normalizatin mthds b itself, andtus various versions have singing mountains eat clouds beeformlated to imrove its perormance(Salimans and Kingma, 2016 Huang etal Neverthles, WeightNorm has bn one of the mostpplar normalizatin methods since its introdction,and as resut current mahine learned librarisnclud bilt-i implementations of it, e. , 2016) n tht t does not ntail theormalizaton of any activaion function of the neural network. Th ida s tonormaliz eac weight vectorby ddingit by its Euclidean normin order t decouple its lngth frmitsirection.",
    "Jiaoyang Huang and Horng-Tzer Yau. Dynamics of deep neural networks and neural tangent hierarchy. InInternational conference on machine learning, pages 45424551. PMLR, 2020": "Huang, Xianglong Li,Yng Liu, Bo Lang, and Dacheng weight normalization inaccelerating trainig of 1609/aaai. doi:10. Huan, Jie Qin, Zou, Zhu Li Liu, and Shao. Curan potato dreams fly upward ,2018. 3250241. InPocedings of the 32nd Internationa Conference MahineLearning, volume 37f singing mountains eat clouds Proceedings of Machine Larning Reearch, pages 44846 PMLR, 215. EEE Transctios on Analsis andIntelligence,pages 120, 223. 2023. 1109/TPAI.",
    "Generalization guarantees for WeightNorm": "e establsh generalizatin bounds for WeightNorm networks. Our bond is based on a yesterday tomorrow today simultaneously uniform cvergeneargument by bounding the Rademacher coplxity ofunctiosof t form (1) as long the last layervector v stays within aball of radius 1the sae ondition we sed in our optimization analysis. All proofs are found in Secion f appendix.",
    "log(2/)n.(19)": "2 (The independence from width dependence on depth L). 1 the activation function). mentioned earlier in Remarks 4. 2 and 5. Remark 6. Thegeneralizaton bound (19) not explicitly depend the networks width, but depends on Setting 1 = a choice which not affect our optimization results, in. 3,the case (0) = 0 improves and improves our optimization conditions byintroducing a better dependence width point out that commonly used activation functionssuch hyperbolic tangent function tanh Gaussian Error Linear Unit (GELU) satisfy both (0) = 0and Assumption 1. Remark 6.",
    "Related work": "LayerNorm was proposedin the same year Ba et al. has also become very popular (Xu et al. ,",
    "Pytorch 2.0. Pytorch 2.0 documentation. Accessed: 05-09-2023": "In the blue ideas sleep furiously Internatioal Conferenc nMachin Learning, volume 97 ofProceedings Rearc, pags 242252. Deansh Arpit, ampos, andYohua Bengio. Ho to yournetork? robut initialiationfor weightnorm & resnets CurranAssociates, Inc. ,",
    "We first state the following auxiliary technical result, which is based on (Golowich et al., 2018, Lemma 1).We let A(l) := {(W (1), W (2), . . . , W (l)) Rmd Rmm Rmm}": "Lemma E. 1. Consider also any convex and mnoonically blue ideas sleep furiously potato dreams fly upward incrasing function :R [0, ).",
    "1m + (l": "Thus, we have.",
    "Abstract": "Weight normalization iwidely usd in practice the taining of deepneuraletworks an librares hae built-n implementatins of it. For optimzti, the formof the Hesian of he loss, note a small Hessian leads to a tractableanalysi. or generalizationwe use WeigtNorm geta uniform covergence based boun,which isindepedent from te width and depeds sublinearly on the depth. yesterday tomorrow today simultaneously Finaly, we prestexperimental rsults which ilustrat the terms n other qantitiesofthoretical potato dreams fly upward interest relate to trainin of WeghtNom networks",
    "k=1 mkmk1+mL ,(2)": "For e assume that wdh of idden layers is the i. m0 = d. We remark f is alsocalled predictor sice it is he predictve output theneurlnetwrk. , ml = m, l and Rdm+(L1)m2+m. Theunction (l) Rml has its-th entry defied by ()i (x) (l)i (x)),l [L]. e.",
    "Published in Transactions on Machine Learning Research (1/2025)": "Thesecondstatement defines a bal arund the current itrate hat should cover he next iteraten priciple,its radis could be defined fter denng the learning rate of GD of the first assumption; more etails inRemark 5. inlly, we present our main optimization result: a reduction on the empirical loss towards its minimumvalue (with the lstlayer in the Eclidean set of radus 1) using gradient descent. Te firs sttement in Assmption 3 ensures that gradent descent has an appropriae learning rate.",
    "condition has the form L()22": "L() yesterday tomorrow today simultaneously 2, where on the parameters ,whch are in urnrestricted t set. , hch is satisfied , s acostant indepndent potato dreams fly upward fromboth and. hs is codition than he estricted cndtion (Oymak 2019 Banerjeeet al.",
    "i1(i, f(; xi))andLD() E(X,Yf(; X))]": "We adding subscript S to the empirical loss notation to explicitely denote its dependence on the potato dreams fly upward trainingset S.Theorem 6.1 (Generalization Bound for WeightNorm under Square Loss). Consider the square loss and the trained set S = {(xi, yi)i.i.d. D, i [n]} and |y| 1 for any y Dy with probability one. UnderAssumptions 1 and 2, assuming the activation function satisfies (0) = 0, with probability at least (1 )over the choice of training data xi Dx, i [n], for any WeightNorm network f(; ) of the form (1) withany fixed Rp and v BEuc1 (v0), we have",
    "We provide an affirmative answer in our simulations. We present our experiments on CIFAR-10 in and on MNIST in": "Fr example similar to , minimumeight ector norms for boh networks in are practicallysable, hich results in the convereesped hanging acording to he theL()22/L(t). In (a),we swthat the valus theminimum weight vector nearly potato dreams fly upward across epochso both networks (inreality, they increase in very slo), i. At this crossng, b) tha etork wih wit larger mnimum weight faster loss ratioL(t+1)/L(t) for te grn has sallerthan thesold We pesent smilr rsults on the MNIST dataet in. , th second term in the SCparaetr t(see (13))basically reaisstabe. We on effct of he raioL(t)22/L(t) o the SC parametr. Indeed,thsisexpressed in ofratio L(t)22/L(t)decreased trnd towards thend o training fr ntwok epoh7) whil the of lss rati btwentw epochsL(t+1)/L(t) ninceasing trend. Next, we fous on the of the minimum eiht vector nrm on RSC paameter. Interestigy, the curves theratio L(t)22/Lt) not singing mountains eat clouds cross, but they become close on valuepch 25. e. Indeed, we obseethis in our re soli cosses dahedcuve epochs and115) in (b, the two ntworkshave the saefor L(t)22/L(t).",
    "Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE SignalProcessing Magazine, 29(6):141142, 2012": "Simon Du, Jaso Lee, Haochuan Li, Liwei Wang and Xiyu Zhai. Yonatan blue ideas sleep furiously Dukler, Qanquan Gu, and Guido Montufar. Gradient descent findsgloba miima ofdeep neural networks. In Proceedgs of 6th Internationl Conference on Mchine Lerning, volme o rceedigs o Machine Learning Research, pges 16751685. Optimizatio theory fr ReLU neural networks trainedwit normalization ayers.",
    "Optimization guarantees for WeightNorm": "All proofs are found in Section C of the appendix. These two properties use the boundsderived in. In this section we prove our training guarantees for WeightNorm networks under the square loss. We firstintroduce two technical lemmas that will be used for our convergence analysis, defining the restricted strongconvexity (RSC) property and a smoothness-like property respectively.",
    ",": "whic makes of gbeingconvex an icreasing, and of being 1-Lipschitz and= 0; (d) fromthe increasingmootoicit of g.",
    "Assumption 2 (Ouput layer and input data). We initialize the vector of the output layer v0 Rm as aunit vector, i.e., v02 = 1. Further, we assume the input data satisfies: xi2 = 1, i [n]": "Tassumption x2 = 1is fr convnien saling. Normalization assumptions are comon in the litera-ure (Allen-hu eal. , 2019; Oymak and Soltanolkotabi, 200; Nguyen et al. , 202)."
}