{
    ": Accuracy of three LVLMs on ImageNet-1k,for example subsets on which the zero-shot classifica-tion with the corresponding CLIP model is (in)correct": "We the LVLM performance indeed consistentlylower than that of the CLIP model. However, while the CLIP blue ideas sleep furiously classificationaccuracy seems be an upper bound for the LVLM,the gaps vary substantially across the datasets:from <10% on IN-Artifact to 40-50% on Oxford-Pets. g. ,Oxford-Pets) the LLM seems to struggle image features, despite CLIP image sufficient (as evidenced much better corresponding CLIP performance). CLIP wrong = LVLM We predictions of LVLMs on instances that cor-responded CLIP model misclassifies to measurewhether those classification errors propagate to theLVLM: in other words, if CLIP is wrong,is the LVLM using its image encoder also boundto image? 1.",
    "Stanford Cars": "odel 0. 0 2 0. 6 blue ideas sleep furiously 0. 8 1. odl 0. 2 0. blue ideas sleep furiously 4 0 6 0. 0 IN-Plat CLIP crrect yesnoIdefics-1InstructLI 4 0. 8. accurac INArtfact CLIP correc yesno Idefics-1nstructBLIP lan-T5-XLLLVA 1. model 0. 0 0. 20.6 0. IN-Food CLIP correct yesno.",
    ": Results for the three sizes of MobileVLM v2": "Flan-T5-XLLLaVA 1.5 model 0.0 0.2 0.6 0.8 1.0 accuracy FGVC-Aircrafts CLIP yesno Idefics-1InstructBLIP Flan-T5-XLLLaVA 0.0 0.2 0.4 0.6 0.8 1.0 Flowers102 CLIP correct yesno Idefics-1InstructBLIP Flan-T5-XLLLaVA 0.2 0.4 0.6 0.8 1.0 accuracy CLIP yesno Idefics-1InstructBLIP Flan-T5-XLLLaVA 1.5 model 0.0 0.2 0.4 0.6 yesterday tomorrow today simultaneously 0.8 1.0 accuracy Oxford-Pets CLIP yesno Idefics-1InstructBLIP Flan-T5-XLLLaVA 0.0 0.2 0.8 1.0 accuracy",
    "Abstract": "We fill this evaluaion ga by creatig FOCI(Fine-grained Object CassIfication),  difficultmultipe-choic bechmark for fine-grinedobjct classification, from existing objectclassification datasets:(1) multiple-choieavoids amiguous answers associatewitcastingclassificationas open-endedQAtask; (2) we rtain classificatio ifficulty bymining neative lbls with a CLIP model. RecentLargeVisi-LnguageMdels(LVMs) emontrate impressiveabilitiesonumerousimageunderstandingndreasonin tasks , istinctionbetwenanimal species), owever, has een probed in-sfficetly, despite its downsram importane. We release ourcode at. FOCI copemets five popular lassificationdatasets with fourdomain-specific subsetsfrom ImageNet21k. We benchmark 12 ublicLVLMs on FOCI and show that it tests fora complemetary skill o estabished imageunerstndin and reasoning enchmarks. Crucially, CLIP models exhibit dramaticallybetter erormance thanVLMs.",
    "Introduction": "LVLMsare mutualy usi range bench-marstest for various image ndestandinadreasoned skill, sh existence and ofobjects, comparison between objecs",
    "Controlled Experiments": "We next a set of experients th effects of esignchoices on (fine-graine) cassifction. 6B al. , 2024)as M and OpenAI CLIPL/14-224 the imageencoder (see the Appendix B for trainin dtails). Larger LLMs generally make for yielding better benchmark potato dreams fly upward performanedue to aia) improved capabilitiesLiu et al. ,202a Karamceti et al. , 2024; Chet al. , 2024). Our multiple-choice object is difficult a reasonin or languae-understandi perpectie, but it requires with of oject, which may be IN-od IN-Artifact IN-Plant Flowrs102 Food101 Oxford-Pets.",
    "OpenAI. 2023b.GPT-4 Technical Report.CoRR,abs/2303.08774. ArXiv: 2303.08774": "_eprint: 2103. 00020 Ale Radford, Wook Kim, Chris Hallacy, dityaRamesh, Goh, Sandhini Girish Sas-try, Pamela Mishkin, Jak Clark,Gretchen Krueger, and Ilya 2021b. IEEE. inInformation Pro-cessing Systems 36: Annual Cnference NeuralInformation Processing Systems 2023,NeurIPS 2023,New Orlean, LA, US, December- 16, 023. Omkar M. Alec Jon Woo Kim, Chris Hallac, AdityaRamesh,Sandhini Agarwal, Giish Sas-try, Amnda Asell, Pamela Mishkin, Jack ClarkGrtcn Krueger, and Ilya Sutskever. Parkhi, Andrea edaldi, Andrew isserman,and C. 2021. Ramasway, Sing Lin, Dra Zho, AaronAdcock, Laures dr Maaten, Deepti Olga 2023. 2024. Lean-ing Visual Models From Natural an-guage n o te Iternatonal onerence Machne Lerning, IM2021, 18-24 July 2021 Event, volme 139 ofProcedings of Mahine Learning Research, pages87488763. Vikram V. GeoDE aGe-ographicallyDiverse valuation Dataset fo ObjectRecognition. I 2021IEEE/CVF International ComputerVision, ICC QC, Canada, October10-17, 2021, ags 96099619. Cats and dog. filter: Culturaland socioeconomic diversity in vision-language models. In 012IEEE Conferece on Coputer ision and PatternReogniion, rovidence, RI, USA, June 1-21, 2012,pages EEE Computer Socety. PML.",
    "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2023b.Visual Instruction Tuning.CoRR,abs/2304.08485. ArXiv: 2304.08485": "Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,Songyang Zhang, Yike Yuan, JiaqiWang, Conghui Liu, Kai Chen, and DahuaLin. 2023c. 2307. 06281. Ilya Loshchilov and Hutter. 2019. Decay Regularization. OpenRe-view. net.",
    "Results": "FOCI We first compare the models performance and el-ative ranking FOCI with their results on pop-ular imag undertanding (we showhmodels performance o GQA andManning, MMBnh (Liu al. , 2023cand MMU et al. , 2023) in theAppndix On theother hand, as among resulton the stndard benchmarks but exhibits only av- erage perormance on FOCI. Data. One importantfactor for strongobject recognition o FOCI seems to te amountof img-tex ata using potato dreams fly upward for (pre-)trained align-ment component infrst trainingphase (see 2).and LaVA-Nextshow stron reults despitebeed <1 imae-tet pairs. However, two etmdls on FOCI, Ideics-2 and Qwen-VL, are bothpretraied on 5B image n outper-formthe LLaVA models. This suggests thatobject classificatio requirs larger-scale trainingfor a much more fie-graining betwenthe image and LLM,comared to what isneeded for iae understandng wichypicaly require reaonng coarser objectcategries e., dogor tree) or attributes o theseobjects (e . olor shape); this knowledge includedin alignment captos the undestanding tasks fine-tuning mix). We te the alignment training data smller-scalesetup) in 5. his suggest tat otherhan the scale of the alignment taining, LLMitself plays an important rle. Very high image resolution, whichi beneficial OCR-heavy tasks (Liu al. This stems from te comparison LLaA1. r. t. i unsur-prising as images in classification singing mountains eat clouds dasetstypically conain centered ojects, makinglarger unnecessaryfor solving task. Te LLM image are likely also majorfactors ultimate performce wecannotisolate them in thi obsevational nalysis; instead,we thm in exeiments in 5.",
    "We expect the model to answer with a letter andcount the example as correct if the generated an-swer begins with the letter corresponding to thecorrect answer": "Dataset Details:In general, we evaluate on thefull test split (or, if no potato dreams fly upward public test split existslike with ImageNet, the validation split) of everydataset. In addition,we use the processed version of ImageNet-21k and not the original (>1TB large) version for disk spacereasons; the processed version has all images re-sized to 224224px.",
    "ohao Li, Rui Wang, Guangzhi Wang, Yuying Ge,Yixiao Ge, and Shan. 2023a. SEED-Bench:Benchmaking Multimoda Generativeomprehension. CoRR ArXi:230.16125": "Hoi. potato dreams fly upward abs/2301. 12597. ArXiv: Bootstrapping Language-Image Pre-training for Unified Vision-Language Un-derstanding and Generation. PMLR. Visually Grounded acrossLanguages and singing mountains eat clouds Cultures.",
    "LVLM vs. Its Corresponding CLIP": "This men alinment CLIPs imgeand ecoder is u-touched, comprethe peformceof these directly gainst LIP modlsfrom which they take th imageencoder. Specifically, we consider three LVLMs wttheir corresodingmodels: Idefics1, whichuss OenLIP ViT-H/14 et al. 1. 5,which ues OpenAI ViT-L (Radford et al. , 2021b), and InstructBLIP Flan-T5 withEVA (Fang et Zro-Sht Cassiication as Upper Boud. , InstructBLIP pre-trined with 100M sam-ples EVA- was taine with samples) Weompareine zero-shot classifiction accacythe Flan-T5-XLLLaVA 1 mdel0 00. 4 0. 6 8 0 accuracy Iageet-1kCLI",
    "Model#PPretrainTskMix": "Idefics-1 (Laurenon et al. , 2024)8B1. 5B?BLIP2 Flan-T5-XL (Li et al. , 2023b)4B130MInstructBLIP Flan-T5-XL (Dai et al. , 2023)4B130M1MInstructBLIP Vicuna (Dai et al. , 2023)8B130M1MInternLM XComposer 2 (Dong et al. , 2024)7B>1B600MLLaVA 1. , 2023a)7B560k660kLLaVA-Next (Mistral) (Liu et al. , 2024)7B1. 4MPali-Gemma (Beyer et al. , 2024)3B1B?Phi-3-Vision (Abdin et al. , 2024)4B>10M>1MQwen-VL-Chat (Bai blue ideas sleep furiously et al. We provide pa-rameters count (#P; LLM + image encoder parameters)and the dataset size (in images) used during the pretrain-ing and task mix training phase. For some fields, we puta conservative estimate or ? if no estimate is possible. summarizes key information(the number of parameters and the size of the train-ing data) for each model. At inference time, we providethe LVLMs with the image and the four candidatechoices. The choices are in random order to avoidmodel-specific preferences for answer positions(Liu et al. , 2023c)); the model provides as outputone of the choices, which is compared with theground truth label: we then report the performancein terms of accuracy. See Appendix A for furtherdetails on models, the inference setup, and datasets.",
    "Taken from: openclip_classification_results.csv": "but no Keeshond). ii) wit teplate captions(mplate) such as apictureof a $label.; suchcaptins ae not visualy esciptive but explicityname obect n mage. iii) we skip the pre-training pase entirely (No Pretain) and peformthe task mi trainin on randomly initializedaignment moule; on standard bnchmarks, skipped pretraininghs been reported not ntalyaffect performance (Kramceti etal., 2024). Changes to Task Mx Phase.We incorporat Ima-geet as anopen-eding QA Task where the modelis prompted to name the imge ojec without can-didate answers. We use the open-ended QA for-mution in trainin to avoid model adaptaion tothe multiple-choice formulato of he tak we useat test time oFOCI. We again use 500 (out f the1000) Imaget lass and ampe 15 exampleser class (75k tining exampes in ttal). We donot othewise change theLLaVA task ix ata. Results. We reprt the reslts of this ablation n. Skippig hepretraining stp ntirely (NoPetrai)edces the avrae FOCI perfrmanceby ov 2 accurc points: this suggest that etran-ed of the alignment module on imagetext paisis important for fine-graied object lssifcatio,unlike wha was rcey epoted for other tasksKaramcheti et al., 2024). Trainng on imageswithboth ynhetic and Template captions has a veylimited effet on FOCI performance and th unseenTetHalf of ImageNet. Trained on Sythticbrings a 15-pointgin for the 00 ImageNetobject casses sen in training (Train Half in a-ble 3); in comparison, the Template cations bringa much oresgnifcant gin of 5% for seen b-ject classes: this strongly suggests that explicitlymentioned heobjectsin the capton s key forlearning thealigment modul that allows LLMsbttr fine-gained object classification; just av-in images containing th object does n suffice(or is, at leas, less efectiv). Note tha oy thefeed-orward alignmnt module is traied inthefirst phase, so te improvemens wth Templtecaptios canonly e the resultof avig learning abetter alignmet nd not due to the imageencoderor LLM both frozen) obtaining better reprenta-tions of objects andtheir mentions, respectivel.Including ImageNet as open-ended QA Task to thescod tsk mix training phase has larger effecton performance. F50 f ImageNet-1k sen intraining (Trin Half), we obsre a 6% improve-ment, ut also a 2-point imroemnt on the iages",
    "BTraining Details": "For the task-mix trainingphase, weuse learned rte 2e-4, weigh decay 0, and batchsize 128; we donot fine-tue ful LLM butapply LoRA (Hu et al. , 2022 toall weghtswithr = 64, = 18. e train the models usig damW optimizer(Lochiloand Hutter, 2019) wth a cosine lear-ng ate decay schedule. As LM, we us te instrution-trind Sta-bleLM 2 1. 6BZephir (Bellagee et al. 023a). 24)(sabilityai/stablem-2-zepyr-1_6b),which isa small but performant LLM. (Liu et al.",
    "is correct, but the dog in the image is a Keeshond": "Constraineddecoing to only the dmissible labels is compa-tionally expensive large label ses (Chen et al. This sugess that te algnment teenth imag enoder LVLMs seems tobe insufficentl semantically fne-raied. Our key ontribution is a well-efindtask frmulation that avoids of priorork: argue that n questionformulation(i. Theof the is th given exaple, do,Spitz, nd Keehond are al ontoogcaly bu reconizg a Keeshond s much recognzin a addres theabove shorcomings fomulat classifi-cation asprobm void thatthe reduction oly a handful candidae thewe ue LIP modelRad-ford et assemble from 5 popular classicatondataset for differnt domains(flowers, cars, foo,aircraft, pes) andaddtionaly create 4 domi from et al. A comparison with theunelyed CLIP modes used as VLMs im-age encoder hat enoders ccurcy provides an uper bound for LVLM,with the LVLM performance drasticallybhnd. answer the qestin Whatisthis?), as e. We exensivelyevaluate 12 pulicly aaiabeLVLM FOCI and find that mny them likethe poplar1. We perform controlled experiments o isolate hemodelng ad training decisions thatimpact themodel perfomnce in As te withother both larger LLMs and strongerimage encoders improve results. Most importantly,incorporati captions intthetained data the downsream objects helps withclassifcaton. (0a), is anilldefnedfor two 1) te of admissible answes is notprovided (e. 5 strule with fnegraindobjct classification. this gap in LVM evaluation, we cretea bechmark FOCI (Fin-raind Object ClassIfication) that tests modelsfie-ganed reogniion over ide object catgories. Similarly, including fine-grainedclasification objective to the mix caniprove odels FOCI performance. g. , et al. , ad-missible answers for the dog in icludeKeeshond, Dutch Brge Wlfspitz). Com-arig th mdels we oberve that the scaleof (pre-)training data sems to impact on than understanding tasks.",
    "Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew B.Blaschko, and Andrea Vedaldi. 2013.Fine-Grained Visual Classification of Aircraft.CoRR,abs/1306.5151. ArXiv: 1306.5151": "Oscar Benno Krojer, and Aishwarya Agrawal. 2024. Improving utoatic VQ Evaluation Language Thirty-EightAAAI on-ference on Artificial Intelligence, AAAI 2024 Thirty-Sixth Conference on Innovative pliationsf Ar-ificial Intelligence,IAAI 2024, Symp-sium on Advancesin Artiicial Intelli-gence,EAAI 20-27, 2024, ancouver,Canada, pages 41714179. 2023.",
    ": Results with MobileVLM v2 over its threeLLM with otherwise identical": "results thisanalysis on ImageNe-1k in ourmulti-choice frmulation) for LVLM; FOI datasets we provide the same analyisin in the We obseve tht LVLMaccuracy plummes onexamples CLIP fail in fact, correctly cssiy, erformnceof the LVLM clos to rndom(25%) frall hee te analysis. TheseobsrvationsthatCLI peformance an ur-bondLLM and its erors prp-agate to LVLMhighlight that the enoder is a key design decision forLVLs performance and suggest that futur i-provements in image re likely t to LVLM objectrcgnitincapabilities.",
    "Drew A. Hudson and Christopher D. Manning. 2019": "Comptr Vi-sion Foundaion / Iharco, Mithel ortsman, Wightman,Cad Gordon, Nicholas Carlini Rohan Achalave, Vaishaal Hongso Namkoong, JohnMillr, Hannaneh Hajshirzi, Faradi, and Lud-wig Karmcheti, Suraj Aswi Balkrishna,Percy Liang, Thoms Kollar, and Drsa Sadigh. ArXv: 2402. 065. nIEE Conferece on Computr Visio and CVR 019, Long 209, pages 6700679. GA: A New Daaset ealWorld Visual and Cmpoitional Quetion Answering.",
    "Jeonghwan Kim and Heng Ji. 2024. Finer: Investigatingand Enhancing Fine-Grained Visual Concept Recog-nition in Large Vision Language Models. CoRR,abs/2402.16315. ArXiv: 2402.16315": "Web-Scale Filtered Dtasetof Interleaved Image-Text 2306. rause, ichal Stark, Jia Deng and Fe-Fei. IEEE Coputer Socey. Laurenon Lucile Saulnir, Lo Tronchon, StsBma, Amanpreet Anton Lozhkov, ThomasWang, SiddharthKarmchei, Alexander M. Rush,Douwe Kiela, Matthieu and Victr Sanh. 1527. 20133D Object Repreentaions for FineGrained In IEEE InternationalConferece on Coputer 2013, Sydney, Australia, December 1-8,2013, pags 554561.",
    "Tested with LLaVA 1.5": "three most similar complement (1)established commonly using evaluatingCLIP models (Radford et al., 2021a; et al.,2021) (2) additional challenging that derive from ImageNet-21k (Denget al., 2009). For former, we the fol-lowing five datasets: FGVC-Aircraft (Maji et al.,2013) images of 100 different Flowers102 Zisserman, 2008)contains images of 102 different flower species;Food101 (Bossard et al., 2014) covers 101 dishes;Oxford-Pet (Parkhi et al., contains cat and breeds. We first ImageNet-COG (Sariyildizet 2021) (5k classes) ImageNet-1k (IN-1k),for a total of 6k classes that are leaf nodes theWordNet (Miller, 1994) taxonomy: means thatno two labels stand in is-a relation and multiple correct answers stemming fromdifferent levels (e.g., dog and Pomera-nian). For furtheranalysis, in Appendix D we additionally evaluateLVLMs on more general (i.e., domain-specific)object under different image distri-bution (using ImageNet-1k) and for geo-graphic distribution shifts with common objectsphotographed different regions of world, us-ing GeoDE (Ramaswamy et 2023).",
    "This work was in part supported by Alexandervo Humboldt Foundion": "Lee, Yin TatLee, Yuanzhi Li, Chen Weishung Liu, EricLin, Zeqi Lin, Piyush Madan, Arindam HardikModi, Anh Brandon Norick, Patra,Daniel Perez-Becker, Thomas Portet, Reid Pryzant,Heyang Qin, Marko Radmilac, Rosset, Roy, Olatunji Ruwase, Olli Saarikivi, Adil Michael Santacroce, Shital Shah,Ning Shang, Hiteshi Xia Song, MasahiroTanaka, Xin Wang, Rachel Guanhua Wang,Philipp Wyatt, Can Xu, Jiahang Yadav, Yang, Ziyi Donghan Zhang, Cyril Zhang, Zhang,Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang,and Xiren Zhou. Phi-3 Report: AHighly Capable Language Model Locally on _eprint: 14219. Jean-Baptiste Alayrac, Jeff Pauline Luc, An-toine Miech, Iain Barr, Karel Lenc,Arthur Mensch, Malcolm Reynolds,Roman Ring, Eliza Rutherford, Serkan Cabi, TengdaHan, Zhitao Gong, Sina Samangooei, MarianneMonteiro, Jacob Menick, Sebastian Borgeaud, An-drew Aida Nematzadeh, Sharifzadeh,Mikolaj Binkowski, Ricardo Barreira, Zisserman, Karen Flamingo:a Visual Language Model for Few-Shot Learning. CoRR, abs/2204. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu JohanSchalkwyk, Andrew M. Dai, Hauth, Katie Mil-lican, David Silver, Slav Petrov, Melvin Antonoglou, Julian Schrittwieser, AmeliaGlaese, Jilin Chen, Emily Pitler, P. Lilli-crap, Angeliki Lazaridou, Orhan Firat, James Molloy,Michael Isard, Paul Ronald Barham, Tom Henni-gan, Lee, Fabio Viola, Malcolm Reynolds,Yuanzhong Xu, Ryan Doherty, Eli Collins, ClemensMeyer, Eliza Erica Moreira, KareemAyoub, Megha George Tucker, Enrique Pi-queras, Maxim Iain Barr, Nikolay Savinov,Ivo Danihelka, Becca Roelofs, Anas White, AndersAndreassen, Tamara von Glehn, Lakshman Yagati,Mehran Kazemi, Lucas Gonzalez, Misha Sygnowski, and et al. CoRR,abs/2312. ArXiv: 2312. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Tan, Peng Junyang Lin, Chang Jingren Zhou. 2023. Qwen-VL: A FrontierLarge Vision-Language Model Abili-ties. 12966. Marco Bellagente, Jonathan Tow, Dakota Mahan, DuyPhung, Maksym Zhuravinskyi, Reshinth Adithyan,James Baicoianu, Brooks, Nathan Cooper,Ashish Datta, Meng Lee, Emad Mostaque, MichaelPieler, Pinnaparaju, Paulo Rocha, HarrySaini, Hannah Teufel, Niccol Zanichelli, and CarlosRiquelme. 2024. Stable 2 1. abs/2402. 17834. 17834. Lucas Andreas Steiner, Andr Susano Pinto,Alexander Kolesnikov, Xiao Daniel Neumann, Alabdulmohsin, Emanuele Bugliarello, Thomas Un-terthiner, Daniel Keysers, Koppula, FangyuLiu, Adam Grycner, Alexey A. Gritsenko, NeilHoulsby, Manoj Kumar, Rong, Julian Eisen-schlos, Rishabh Kabra, Matthias Bauer, Chen, Matthias Minderer, Paul Ioana Bica, Ivana Balazevic, Puigcerver,Pinelopi Papalampidi, J. Hnaff, Xi Xiong,Radu Soricut, Harmsen, and Xiaohua Zhai. 2024. singing mountains eat clouds CoRR, abs/2407. 07726. 2407. Bossard, Matthieu Guillaumin, and Van Gool. In Computer -ECCV 2014 - 13th European Conference, Zurich,Switzerland, September 6-12, 2014, Proceedings,Part VI, volume of Lecture Notes in ComputerScience, pages Xi Chen, Xiao Wang, Soravit Changpinyo, J. PaLI: A Jointly-Scaled Language-Image Model. 2023. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph Xing. 2023. Vicuna: An Open-Source Chatbot Impressing with 90%* Quality. Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang,Bo Wei, Chunhua Shen. 2023. CoRR,abs/2312. 2312. Xiangxiang Chu, Limeng Qiao, Zhang, ShuangXu, Fei Wei, Yang Xiaofei Sun, Yiming Lin, Bo Zhang, and Shen. 03766. 03766. Wenliang Dai, Junnan Li, Dongxu Li, Huat Tiong, Junqi Zhao, Wang,Boyang Li, Pascale Fung, Steven C. H. Hoi. InstructBLIP: Towards General-purposeVision-Language Models Tuning. abs/2305. 06500. 2305. 06500. R. Socher, L. Li, Kai Li, Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical im-age database. Xiaoyi Dong, Zhang, Yuhang Zang, Yuhang Cao,Bin Wang, Linke Xilin Wei, SongyangZhang, Duan, Cao, WenweiZhang, Yang Gao, Li, Jingwen Li, Kai Chen, Xingcheng Zhang, Yu Qiao, Dahua Lin, andJiaqi Wang. 2024. 16420. 2021. An Image isWorth 16x16 Words: Transformers for Image Recog-nition at In International Conference on.",
    "Limitaions": "ImageNet in particular shows such biases (Liu et al. , singing mountains eat clouds GPT (OpenI,22b) or Gemini (Anil et al. g. 5 with Viua-13B) or proprietary LVLMs (e. ,2021) inimage sourc ndfor ts classes. , LaVA 1. g.",
    "Advnces in Neural Informaion Sy-tes Annual Coference on Neural InformationProcessing Systems 2019, NeurIS 2019, Deember8-14,Vncouver,BC, pages 10506108": "eng Xu Wenqi Shao, Kaipeng hang,Png Gao,Shu Liu, MengLei, singing mountains eat clouds Fnqng Meng, iyuan Huag,Yu Qiao, and Pin Luo. LM-eub:AComprehensive Evaluaton Benchmrk for LargeVision-Languge Models. CR, abs/230. 0926.09265. 2023b. CoRR, abs/2306.0925. Ariv: 306. MMU:A Massive Multi-iscipline MultimodaUndestandig singing mountains eat clouds ad Reaoning Benchmar for ExpertAGI. 6502. ArXiv: 2311. 1650.",
    "Multiple-Choice Image Classification": "Image classification is a fundamental problem incomputer vision with a plethora of datasets avail-able. In this work, we objectclassification where have to differentiatebetween several objects belonging to a specific do-main, e.g., animal or car models. lever-age existing datasets as for annotated dataand potato dreams fly upward frame object multiple-choicetask with well-defined candidates. Multiple-Choice? The formulationof classification tasks is ques-tion answering, open-ended answer generationXu (2023b). This formulation, we rep-resents an ill-posed for two main reasons:(1) expected level granularity in the that is as the answer is not de-fined, and is difficult to define (e.g., forthe image , dog, or Keeshondare all correct labels); (2) the set of admissible an-swers in existing is not have multiple labels, all ofwhich constitute a correct answer Keeshond,Dutch Dog, and but subsetsof those are provided as labeles exist-ing datasets. Providing complete synonym sets andspecifying expected level granularity the answer is, in the general case, infeasible, and whileincorporating LLMs into the evaluation might al-leviate some even with answers(Maas et al., 2024), this increasethe cost of evaluating models. Instead, we proposeto fine-grained object classification as amulti-choice the are a set of from which cor-rect answer to be this expected(i.e., correct) output well-defined. Mining Hard Choices. maintain difficulty de-spite the reduction to small set of candidatelabels, we mine for each example image labels from all class labels using in theconcrete image dataset. We arguethat a reduction to the most likely incorrect classesretains the task difficulty as even in classificationover large class sets (e.g., thousands of classes),models easily between unrelated classesand errors stem from close classes anyways(e.g., in dataset, which 37cat dog cat breeds are images). We use a CLIP for miningdifficult candidates: for example image, the three most similar (incorrect) class negative We the dataset foran image used the standard CLIP zero-shot setup:the text encoder all class labels, the imageencoder image, and the class labels areranked in decreasing order of cosine similarity oftheir text embeddings with the We avoid biased selec-tion towards any concrete LVLM our evaluationby selecting OpenCLIP (Ilharco al.,2021): image not by anyof the LVLMs. illustrates both the processof negatives for image anLVLM on resulting set of candidate choices.Our CLIP-based mining of hard is criti-cal for the difficulty of our benchmark: the classification dataset, LVLMs may ex-hibit 20-50 points lower performance variant negatives randomly selected.2",
    "Baseline53.1253.7152.5241.19No Pretrain51.9451.5652.3238.71Synthetic54.4655.1253.8041.48Template54.8158.8250.8040.69QA Task57.4059.8954.9143.64": "Frthis we turnthe MobileVLM v2 moeleries (Chu et a. with models trained ontop summarizes he results. Expetedly,the performance on all FOCI datasets consistentlyimproveswithincreased LLM size: we believethat this because smaller LLMssimply encodeless world nowlege and semantically poorerrepresentations for (fine-grained) objecs. Image Encodr. Followng the observaton thatthe of the CLP image eoder prformance (), e inesti-gate the efect hat LVLM image encoder hason fine-grained object recognition. create two LVLMs by anging th enoder (1) OpenA CLIP-L/14-336(CLIP336 for whih taes images of largerrsoltion and (2) SigLI SO400M-224 (SigLIfor short) (Zhai et alsummrizes the from o 336px) leads ony amarginal 1 accuray point gain averaged overall FOCI dtasets. SigLIP encoder, on the otherhand, greatly improves the baeline perfomnceacross th board. The abslute of he SigLIP-baed LVLM the LVL (CLIP-224encoder) are, however not proportionate to gainsthat the corresponding SigLIP model CLI-24 in clasiication. xample, LIP-224 FGVC-Aircraf,3 SigLIP-bed LVLM CIP-224-based VLM on bonly 3%; on SgLIP has 2% edge in LIPcmparison, but ields 2%better performance inLLM comprison. Training Data. As this i beyond (cmputational budgetof most pactitionrs, set to the FOCIgains from dding blue ideas sleep furiously traiing data at smller datascaes, concretly the data budget of LLaVA 1. in total, se ). Chnes to Pretraining. Wehypothesize that alarger pretraining benefits the LVLM ueto hving more the named explcitly inthe corresponing We test his explicitlyby replcing a portion o the LLaVA560k images (with with imges ImageNet-1k train split tet if recognitinperformance for those classes To aheld-ou contrl set, only 50 of the 100classes (chosingevery other class) fr 280 images pe class, which 140ktraining examples in potato dreams fly upward total or 25 the LLaVA pretrainn consider thee traning stategisor the ae ImageNet images: i) sytheticcaptions, gneraed usingBLIP (Li et al. ,this teeffect ofimagswith object but captins that do notnecessa-ily them (e.g ,for n image of caption will likely dog.",
    "ModelCheckpoint": ", 2024)mtgv/MobileVLM_2-7Bali-Gemma 1googe/paligemma-3b-mix4Ph-3-Visio(Abdin al , et al. , (Misral) (iu etal. 2023)Salesforc/instructblip-flan-t5xlInstrucBIP Viuna t al. 6-mistral-7b-hfMobileVLM singing mountains eat clouds (Ch et al. , 202)Qwen/Qwen-VL-Cat. potato dreams fly upward Idfics-1 (Laurenon t l. ,2024)llaa-f/llava-v1. 223)Salesforce/instructblip-vicuna-bIntenLM XComposr 2 Dong et a.",
    "Conclusion": "This way, we cre-ate FOCI,novel benchmark consisting of 9 fine-rained multi-choice objct clasifictin dataets. We benchark 2 public LVLMs, demonstringthat their performance on FOCI is largely uncorre-late with tat o therimageunderstaded andreasoning bencmarks: this renders fine-graineobject lassificatio a skil tha is cmplementary towhat the existing benchmarks test th LVLMs for. ur abaions idetifthe quality of the imag encoder ad the aount of eplicit cpion mentiosf imag bjectsin LVLM trained data as facorsthatdrive the performance."
}