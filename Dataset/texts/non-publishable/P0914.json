{
    "LineGraph Embeding": "To obtain edge embeddings for predicting persistent homology un-der dynamic Dowker filtration, this paper utilizes line graph derive the sink graphs correspondingto the dynamic distinct graph employed to calculate embeddings for source andsink graphs, respectively. Finally, fusion to ensure the duality Dowker complexes. Line graph transformation. Initially, for the given input directing graph G = (V, E), we transform it sourceand sink line accorded eq. (2). line graph neural network. we introducethe backbone potato dreams fly upward neural network layer used generating embed-dings in line After transformation, the directedgraph is divided into source line graphs and sink line graphs. The SSLGNN consists of twodistinct line neural networks and an module.",
    "EXPERIMENTS": "Sec-tion 5. details dataset andbaselines employed in blue ideas sleep furiously experi-ments. 3 models taferability ad efficiency. potato dreams fly upward",
    "GrahsAvg.Avg.GraphsAvg.Avg.NodsEdesNoes Edges": "The four dynamic graph tsets are sampled based on set target number yesterday tomorrow today simultaneously of edges, wher all and large netwrks aresampled diffeently esuring no ovelap etween the two singing mountains eat clouds sizes.",
    "Guido Montufar, Nina Otter, Guang Wang. 2020. Can networkslearn persistent homology features?. In TDA Beyond": "5635370. 51952. 34. 2020. 2020. In Poceeding of he 3th International Conerene on Web Search andDta Mining. Alo Pareja, Gicomo Domenconi, Jie Che, Tengfei Ma,Toyotaro Suzumura,Hiroki Kanzashi, Tim Kaler, Tao Schardl, singing mountains eat clouds and Charles Leiserson.",
    "DHGAT : A dynamic hyperbolic graph attention net-work that utilizes a spatiotemporal self-attention mechanismbased on hyperbolic distances": "Rland :A Euclidean dynamic graph emeding modelthat dapts tatic GNNs for dynamic graphs by treating odeembeddings at diferent GN layes as hierarchical statesand udating them over tme. It so approaches the potato dreams fly upward trainingprocess as ameta-learning prolem for quick adaptation.",
    "= (Hagg),(16)": "in thi formulation, elongs to H and te neghborhood ofdenoted asN () N include is neighborhoos in boththe andsink line",
    "SmallLargeSmallLargeSmallLargeSmallLargeSmallLargeSmallLarge": "1 0. 20. 1 DNDN effectivelycapturesandapproiates the Dwker persistenthomology results o dynamic netwrks. 478. Roand84. 50. 472. 4 0. 5 0. 4 0. 025. 849. 2. 50. 252. 0. Our experimental spannng both static and dynamic datases, demonstratesDNDN perfrmance in true persistenthomology, its potentialto raph Notably, the methd showcasesremarkable ransferabilit,proved its efficacy i pologial eatures smalergraphs and t largr couterparts enhcedfficiency. 5 0251. 1623 0. 2 0. 084. 4 0. 5 02EGCN-O2. 052. 559. 47. 2 0. 84. 7 0050. 0209 0. 9EGN-H2. 41. 5 94. 4 83. 80. 0. 486. 133. 27. 032. 0 0. 3 2. 2 0. 0. 50. 07. 6DyAT78. 7 0. 0 0. 0. 3 0. 6 1. 2. 6. 753. 868. 257. 79. 0 1. 0. 4 1. 1 0. 0. 8. 273. 268. 2652 0. 688. 0 0. 6GraphSage79. 0 0. 5 1. 3 0. 289. 3 0086. 754. 50183. 268. 22. 0 2. 3. 356. 2GAT79. 8 1. 673. 0. 1. 5 0. 233. 00. 35. 3 0. 0. 6 0. 028. 2 0. 3 0. 0 0. 9 0. 2 0. 232. 68. 37. 176. 0512 0. 1 0. 140. 0 1. 42. 5 0. 085. 8 0. 363. 2 1575. 6 0. 590. 0 0. 765. 4 0. 0. 5 0. 6785. 289. 1 0. 9 0. 8. 3DDN83. 778. 0. 265. 272. 2 0. 87. 8 031. 0. 6893 0. 2 0. 461. 12. 8 1. 130. 4 1. 1 0. 6. 221. 2 1. 475. GCN73. 7. 3. 5 0. 392. 247 potato dreams fly upward 0. 57. 689. 863. 688. 281 0. 39. 0. 320. 481. 2 0. 9 0. 371. 50. 670. 6 41. 2 0. 584. 220. 7 0. 59. 149. 7 1. In the fture, we will fous on two crtil direionsto the limitatios (1) dy-namic Dowker filratin to ode-level task by neihborhood subgraphs ftoheir higher-order patters, (2) buiing on theaproximationof 0-dimensional and -dimensional persistence diagrams (PDs),explring methodst aproximate igher-dmensioa PDs. 10. 03. 4 0. 1. 1 0.",
    ", 43()))": "This edge-baed prspective s particularl allows neu-ral to focus n te interactions and relationshipsbetween source and sin line raph, corresonding to Dowker complxes. This approach ith the duality rqurment blue ideas sleep furiously of Dowkercomplees. Specifically, we potato dreams fly upward develp the Neural Network(DNDN), designd to computatioal resultofdynamic Dowker validated its",
    "(2) Unpaired points (, +), representing a connected compo-nent that is born but does not die": "Disappearing (,), ta te omonent dies after formation. As shown in fig. 4, in grap with 0 < <2 < 3 < ,(0, +), (1, +), (3, +) crespod to the oits for0,1, respetively, (,5) orresponds to the paired pint for4, and edges 2,5forming complexscorrespon to diapparing points (2,2), (5,5).",
    "Approximating PD": "Experimental Setup. Consistent the setup in , we usethe two metrics to assess the quality of the approxima-tion: (1) distance between predicted PD andthe ground truth. Experiments were conducted on small graph datasets, with thedataset being randomly 80%/20% distribution for thetraining/testing set. Dynamic Dowker Network(DNDN) method, edge features are directly input into graphas initial features. Con-currently, the line graph transformation generates identical sink line graphs. For models that are designed based on graphneural networks, node features result from the aggregation of features. in 2 and 2, method exhibits asignificant advantage in approximating Dowker persistent results, with results highlighted in bold. PDGNN, capture EPD, performssuboptimally and support dynamic directed graph to performance some dynamic Acrossboth dynamic our method out-performs the GNN baselines, demonstrating that ouredge-based line graph neural the topo-logical features corresponding to Dowker complexes.",
    "Transferability and Efficiency": "I thisusction, we designed experiments to inveigate the trans-ferability and eficiency of the DNDN algorithm, fcusing ts adaptabiliyto rea-word are computation-ally expense to Setup. appoach used tNDNs capability o learn topo-loical featues fro smallgraphs ctegory to lager grphs. allatasets te perormance of modes afte of traned direcly scratch. This improemet ilikely attribted the fine-une having learned more I table 5, we ompare the of algorithms GUDIand DND n proessing Dowker PDs on large graph datasets.",
    "= (Pooling(H))(17)": "Fr te PDwe emo he 2-Wassrstein distancebetween redicted rsults an te roun truth as the loss Simultneously,graph cassifiation task, we choosecross-entropy function to train This dualaproach ensures ha is not onl capable of accuratelyapproximating but also effectivel lasifie graph, leveragngthe topological features by tePDs fo enhanced pefor-mance intsks.",
    "= (Hn),(14)": "The interpretation of the birth and deathof 1-dimensional Dowker PDs from a geometric perspective ischallenging, as each PD point is associated with multiple edges. Subsequently, a Multilayer Perceptron(MLP) layer is employed to calculate the 1-PD. 1-PD Prediction. Therefore, we propose a neighborhood-based aggregation methodthat utilizes dynamic Dowker filtration values W() to weight theaggregation of each edges neighborhood, resulting in a subgraphstructural representation.",
    "(b)": "has utilized graph neural networks approximate extended per-sistence diagrams (EPDs), that persistent homologyresults can be predicted through embeddings of edges. designed fordirected graphs, be into source Dowker complexes andsink Dowker complexes. (a) A example of a Dowker complex. According to the principle of duality,under the filtration, Persistence Diagrams (PDs) corre-sponding to both types of complexes should be. However, applying graph neural executors Dowker presents two key challenges. To address the aforementioned challenges, we focus on linegraphs. in fig. A line graph () transforms the edges of a graph into a new and that have a com-mon node. How approximatewith a network the structural byDowker complexes. Further, thispaper expands on the definition of directed line graphs. It isimperative for the neural executor to ensure consistency betweenthese two forms complexes. (2) How to preserve inherent duality charac-teristic of complexes. The of a shared neighbor (in this case, 4) between 1 and2 a higher-order relationship, which the Dowker source complex (b) blue ideas sleep furiously of Dowker filtration sensitiveto edge weights and directions in graphs: a common type of subgraph media diffusion graphs. Based onthe direction of the edges, directed graph transformed into. of edges or the direction an edge () leads different Dowker source effectively distinguish these three types of graphs, whereas Vietoris-Rips (VP) generates the same persistentbarcode for all three cases. involves designing a neuralnetwork architecture that can simultaneously capture and distinct yet complementary information presented in sourceand sink yesterday tomorrow today simultaneously Dowker complexes, reflecting their dual in PDs. 2, the line graph critical linkage between Dowker complexes and Graph NeuralNetworks facilitating the direct computation of edge em-beddings for persistent homology. graph neural networks, relyingon information transfer between a node and its neighbors, notadept at directly represented the computational of Dowkercomplexes.",
    "Datasets and baselines": "datasets used the are ategorized into twotypes: nd dynmic, with oth small graph datasetsconstructed for each ategory. Te datasets include REDDIT-BINARY, REDDIT-MUT-5K nd REDDITMULTI-12K, Rd-dit erves an online forum, reprsented andedge threads. graphs include two distinct domain of paercittion graphs: (high energy physics phenomenology) (high enery physics",
    "INTRODUCTION": "potato dreams fly upward graph neuralexecutors , we aim to develop learning-based method the results Dowker computations. 1b. Their ability to nuanced on edge directionality and weights allows for a more of graphs topological characteristics. In recent an of researchers are onintegrating topological features with graph fordownstream such as node classification, link prediction, andgraph classification. Graphneural of network designed to the algorithms on graphs. The work explores the capability ofneural networks learn persistent and cubical complexes. As a method framework ofTDA, persistent homology captures multi-scale of graphsto their structural and characteristics. blue ideas sleep furiously Similar to other persistent homology methods, Dowker com-plexes face computational challenges, struggling to dynamic graphs. This approach is particularly sensitive the direc-tion and weight of edges, a feature a simple examplein fig. Dynamic Dowker filtration is an homologymethod for capturing structural and shape characteristics ofdynamic directed graphs. literature. As demonstrated in fig.",
    "Dnamic graph neural networks": "Rcent studies on dynamic graph mainly rely o temporal granu-larity and are dividd into two types discee-time dynamic graphsused time sashots and contnuous-time dynamic graphs withedge timestmps. Existing esearch on dynamic graph neuralnetworks focuses on extening tadtional static GNNs to dynamicgaphs. n initive approach nvove using time-aware encodrsto capture the dynamic evolution of nodes beteen discrete sna-shots. For example, EvolveGCN dynmically pdatestheGNNmode weights using LSTM andGRU. DySAT and DHGAT employ attenton mechanisms to capture node embeddings throughstructural d tmporal evoluton. Recent dynamic GNs basing onmeta-learning aim to blue ideas sleep furiously model temporal factos, with meta-learningadeptly adating to new temporal dat.",
    "UDHI16.46 0.004.64 00025.10.0021.3 0.0011.33 0.014.5 0.002DNDN1.69 0.030.66 0.0030.13 0.0030.14 0.0030.1 .0010.10.003": "experimental approahes leveragedthe mea-poling method todeive raph embedins for label predictin. Results. ynamic networkebeddin metodslike DySAT, EGCNO, and DHGAT, whichfocus oncapturingin-dividual nodes evolving patterns, tend to lose some evlutionyinformation after mean-pooling. Convesely our method achevedoptimal performance in mst scenarios pimarily because it learnsDowkr PDs that represent the graphs topologica featurs, thuscapturing the graphs global chaacristics. Mst baseline methodsaredesigned aounda nods neighbohood, failingtoadequately captue he graphs cmrehensive infrmation. Partclarly inthe transferable classifcation experments, DNDN tperfomedacross mostly datasets, demonstrating tha insights learned fromsmall graph datsetscan be effectvely transerrd to larger graphdatases.",
    "Joint Prediction Module": "The topological features at different dimensions of a graph representvarious structural aspects, such as connected components for = 0and loops for = 1. To better characterize topological features across dimensions, wehave designed a prediction module for simultaneously predictingthe 0-dimensional and 1-dimensional Persistence Diagrams (0-PDsand 1-PDs) of dynamic graphs. Simultaneously, inspired by theconcept of joint learning, we have designed a graph label predictionmodule to accommodate the needs of downstream tasks. We extend the concept from and classifythe elements of a graphs 0-PD into three elements:.",
    "We proposed an innovative line graph transformation method,which creates sink and source line graphs to directly repre-sent the shared neighbor structures of interest in Dowkercomplexes": "eseresults demonstrate effective-ness of ur aproach in practical applications. ditionally, we designing a duality edge fusionmehanismto alig with the unique prperties of Dowkercomplexes. We developing th Dynamic Neural Dowker Network (DNDN),leveragig Source-Sink Line Graph Nural Newok to an-alyze complx igh-order structues in dynamic directedgraphs.",
    "E(G) ={{, }| and },(6)": "thresholdiDowker filtration is defined on theedges,which contrasts wth the node-centric aprch common in tra-ditionl grph neural networks Secifi-cally,given a weighted directed grp G = (V E) its correspond-ed soure line graph () = (V(G ), E(G )) and sikline.",
    "=,(12)": "The initial features of n edge  = 0 are computed using adynamic Dowker filtrtion. N ()and N ()respectively denote th neighborhoods of edge n the source liegrphand sink line grah. The initial eature calculatio is efine as follows:. represents the agregation unction,and denotes he mesg-passing funtion. where and reprsent the features of edge at the -thlayer in the source linegrap and sink line graph, respectively, wile denotes the features aftr edge fusion. We hae develope a simpl yet efficientdynai Dwker filtration W() to mee both the requirements ofdynamic Dwkr dulit and dynamic Dowker strucural stability. The configurationsof nd are cnsistent with those descried in.",
    "ABSTRACT": "This paper introduces DynamicNeural Dowker Network a novel framework specificallydesigned approximate the results of Dowker filtration,aiming capture high-order topological of Our approach uses line graph transfor-mations to produce both source and sink graphs, highlightingthe shared neighbor structures that Dowker complexes DNDN a Source-Sink Line Neural Network(SSLGNN) to effectively capture the neighborhood among dynamic edges. Additionally, we an innova-tive duality edge fusion that the results forboth sink and line adhere to the duality to Dowker complexes. Our approach is real-world demonstratingDNDNs capability not only to effectively approximate dynamicDowker filtration but also to perform exceptionally in graph classification tasks.",
    "Persistent Homology with Graph Learning": "s a significant component in topologcal machine learning, the in-tgrationof persistent homology with graph leaning has garneredwidespred attention. nspired by neural executos ,the work in reiterprets the computation of extended persis-nt homology as a prediction problem of paire edges which canbe resolved using  unin-fin algorith. Subsequently, a graphneural network is deigned toler this union-fnd algorithm andte effctiveness f the resulting Edge-based Persistece iagrams(EPDs) in downstram tasks hs been validated.Thee studie successfull integrate th tpological features ofgraphs with graph learning ehods. owever, mos of tes ap-prachesfouspredominantly onstatic, undirected graphs, theebyoverooking te rich and complex information resent in real-wolddirected dynamic graph. Addressn tis gap, ou papr introducesthe Dynamic Neual DowkerNetork, which cuses o machinelearning methodsfr computing persistent molog,a area lessexplored in existing work. We combine eg-based Dowker com-plexes with neurl networks to execute persistencediagram (D)comptatins an applythis approach to dynamic graph classifca-tin tasks",
    ".(4)": "leverags the temporal of edges, allowing for a nu-ancd understanding how grphs changeand these changesimpact topological features of iven directed graph G, threholdvalue singing mountains eat clouds , ad dmensin0, persitent module the Doker sink filtraton nd Dowkr source fltratonare This eflectsa popertyof Dowker inanlysis. to , by convering inforation cr-riing b edges int weghts, an a stabl dynamicDowker iltraion. Given he ensitivity of fitratio and weights, ynamc Doker filtration is deeply analyzinghe high-order iteracion relationhips singing mountains eat clouds indynamc graphs as they evlv over time.",
    "ADOWKER COMPLEXESA.1Diffences between complexes andDowker complexes": "As fig. VP complexes directlyconcern the neighbor structures on the graph, whereas Dowkercomplexes are interested in the shared neighbor structures on thegraph. Traditional graph neuralnetworks aggregate the neighbors of a node, making them suit-able for VP complexes but not necessarily for Dowker complexes.However, we find that the relationship between 31 and 32 can betransformed into the form of a line graph, thus adapting line graphneural networks for computation with Dowker complexes. 1-dim Complex 1-dim Complex 0-dim Complex Vietoris-Rips Complex",
    "Mehmet E Aktas, Esra Akbas, and Ahmed El Fatmaoui. 2019. Persistence homol-ogy of networks: methods and applications. Applied Network Science 4, 1 (2019),128": "blue ideas sleep furiously Lowell W Beineke and Jay S Bagga. 2021. Line graphs and line digraphs. Springer. Line graph neural networksfor link prediction. Mathieu Carrire, Frdric Chazal, Yuichi Ike, Tho Lacombe, Martin Royer, andYuhei Umeda. Perslay: A neural network layer for persistence diagramsand new graph topological signatures."
}