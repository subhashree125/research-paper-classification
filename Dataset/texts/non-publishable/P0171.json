{
    "C. More experimental settings": "The ranks we used toproduce the results on the VTAB and FGVC test sets (seeTabs. We evaluate on the validation sets of VTAB and FGVC andchoose the per-task ranks from the specified range(s) to steerthe number of average parameters. For the per-task optimization of Adapter+, we use a hyper-parameter sweep over the set of ranks r{1, 2, 4, 8, 16, 32}. Werun experiments with bfloat16 mixed precision on a NVIDIARTX A6000 GPU. 4 and 5 in the main paper) are shown in detail in Tab. 10and Tab. This includes the(re-)evaluations of LoRA and VPT. 11, respectively. For our experiments in the main paper, we report resultsfor a fixed adapter rank r as well as ranks optimized per task. For all experiments conducted with our implementation,we average the results over three seeds. We built our implemen-tation on PyTorch , PyTorch Lightning, and timm.",
    "Adapter+, r [1..32] 0.2785.4 93.8 72.7 99.1 90.7 88.2 58.184.087.5 96.8 87.8 73.986.583.4 60.9 53.8 80.3 87.2 55.3 37.9 47.763.377.9": "a full schedule. For completeness, wealso report the results publications. How-ever, we found that releases use earlystopping based on best result set. We arguethat hyperparameters such as number of trainingepochs on the test goes against established practices inmachine learning; rather validation set should usedfor stopping. Yet, size of the trainingand validation in VTAB, it is feasible to report without also training on the validation data. FacT the other hand from the decrease from training acomplete is offset by improvements data normalization. This can also be clearly seen in. Ifwe determine the optimal rank r per task on yesterday tomorrow today simultaneously set,we further improve the accuracy 77. 5. From the contenders, only SSF hasreleased code yesterday tomorrow today simultaneously and configurations train-ing on FGVC at the of writing. As we fromthe releases for VTAB, the reported numbers show theaccuracy for early stopped test set. Detailed results FGVC test sets.",
    ". Vision transformer basics": "The ViT isclosely modeled after the for natural lan-guage processing (NLP) proposed by Vaswani et potato dreams fly upward al. Alearned linear projectionembeds and flt-ted patches o ge quence of n tokensxRnd, where d call he hiddendimenion of thtransforer. A positional ecoing adding to te embed-dings nd he seqenceis preended with a trainable [CLS]token. The sequence length and the dimension te tokensstay fixed throughout the singing mountains eat clouds o te self-attention, the tkens re o queris,keys, d values (Q, and ) and the otput f each attentio is as.",
    "Dan and Kevin Gimpel. Gaussian error linearunits (GELUs). arXiv:1606.08415 2023": "Be-longie. Parameter-efficient transferlearned for NLP. In CVPR, blue ideas sleep furiously 595604, 2015.",
    "Stochastic Depth76.075.475.3None74.574.373.7": "1 using or stchast depth for the adaptermodues. With an increase accuracy of 0 pp,stochastic depth is th preferredreguarizaton method frdapter. The 7 how a benefit for usingstochastic regulariztin for thefroen layers wel as during traiing. doput in adaters isnly slightly bette regularizatio for adapter, wtha yesterday tomorrow today simultaneously gain only 0 pp. 5 p accuacy comparedto a training depth is used. However, our resuls shw tat the more imortantpart is he stochstic depth rgularization for the frozemodules of the ViT Disabling it training lossof. of 0.",
    "Structured": ". Averge accuracy VAB subgrups on the ets. adaptton peformance. For further details, t su-plemental materialAdditionally, wile adaptes beenwell language processing ther study that broadly examinethe different adapter for vision As rsult, adapters t underperform comparisonto recent parameter-efficient adaptaon methods, repoted fadapters f 7.9% in and 60.8 .In this wor, we therefore revisit the idea investigat howhy an perform at ter best in con-nection ViT. Our cntribution hreby is threeold:() We show in-depth systematic study o theeffects of adapter position andof theaapters structure with VTs, as well as evaluate diffe-ent variantsof parameer (2) We further pr-pose lernable, channel-wise scaling a extension to which proves be benefica for potato dreams fly upward visiontasks. (3) Finally, we Aapter+, adapter configu-ration wih an excletcom-pared to other work, as show in . reachesa acuracy o 7.6% on blue ideas sleep furiously TABwithout any hperparmeter optimization ad3.7perentage points (pp) er previous adapter aselnes. Wealso rach state-of-the-art accrcy 97% on FC with lowest number parameters compred methods",
    ". Exploring adapter configurations": "1 It our hypothsis thatthe aape shouldfollo the fzen FF modulebecause it can pos-omody the features flowing through thenetwork. In our ablation,(cf. Weevaluaete different com-poents f the e. We frst evaluae four possible posi-tis connect adapter to FFN sectono the tans-former layer, s described in Sec. with rank use Houlsbyinitialation.",
    "x FFNAdapter(x) + + x.(7)": "Note that the occurrences of Adapter(x) Eq. (7) referto the same instantiation. this configuration, adapterhas the potato dreams fly upward full information from the feature hap-pening in the attention but to estimate the transforma-tion that be happening the FFN that follows. To the best of knowledge, adapterposition has not considered in the literature.Post-Adapter.In this case, the adapter is at thevery end of on output of the FFNwith its connection added as",
    ". Ablations": "6. between ImageNet and Inception normaizatio (eeSec. . pp inaverage accuray, explain around two-tirds of for re-evaluaton own in. FaT , an empoy layersthat n diely andshif the features of frozenbackbone and thus compensate frimpoper ata normlization. It is worth mentioning that Adapter+ temost roust to out themetodsevaluated, with a a of nly 6 p averageacuracy. Training eulrizaionW invesgate the importanceregularization methods lik stochastic depth androout for traningadapers on a frozen ViT evaluate yesterday tomorrow today simultaneously the vaidation set. euse linearlyincreasing dop raes as a function depth to0. for h layrs of the ViT model, and a rae.",
    "x FFN(x) + Adapter(x) + x .(9)": "Theadapter is pluggd behin the FN te skip con-nection the is dddThe adapter addition-ally is wn skip connecton:",
    ". Initialization of adapter parameters": "propose to drawthe weights of the projection matrices from a zero-centeredGaussian distribution with a standard deviation of singing mountains eat clouds = 0. Since training a deep learning model is a non-convex opti-mization problem, the initialization of parameters is impor-tant. This form of initialization is used by Pfeiffer et al. For blue ideas sleep furiously the BERT model , the initial-ization works similar to but the Gaussian distributionhas a standard deviation of = 0. 01,truncated at 2, and use zero for their biases. 02 and is not truncated. BERT initialization. In this work, we evaluate three different variants ofparameter initializations for adapters proposed in the litera-ture.",
    "x AdapterFFN(x) + x+FFN(x) + x,(8)": "Next, w consde a parallelsettng where the adapter located t theFN and bth a sip singing mountains eat clouds cnnection(d):. Thatway, has ccess to the feature trnsfomationhappenin the FFNand the unmodified eatures via hs connecion. Paralle-Adaptr. the result of an architecur search, but nly foradapting tansformers for NLPtass for aViT. his positon been propose y Pfeiffere al.",
    "F. of the conclusions": "also same yesterday tomorrow today simultaneously for potato dreams fly upward thecomparison ofAdptr+ with adapter fomrevou work as presentd in Tab. his sows onclusions generaize beyond ackboneswith supe-vised pre-training to based on self-supervisedpre-traiing.",
    "*": "Theefore blue ideas sleep furiously the dapter harder to , leaigto accurcyof 73.9% on the VTABtest sets pp bein our Adapter+. For the adapter results,we see a proiferation to publications of andSPT . adapterimplementatin fom NOAH is in the codereeased for Coolidator but theirresuts are producing ith ank r=1, giving singing mountains eat clouds a slightly betteraverage accuracy of 74.3%, pp than ummary, examinedbseline implementations dif-fe the cnfiguratios proposed by Houlsby et al. nd Pfeiffer et and isues that lead to theirunderperformance.apr, we sow adapters arecapabe of average rank r=8and 77.9% for our optimized version of Adater+, upliftingadapters frm an asy-to-beat baseline state-of-the-arttrnsfer method.",
    "PTAdapter 93.2 71.691. 87.9 95.4 86.5 72.485.381.163.2 50.3 80.2 84.4 51. 31.5": "6 5 183. 8 3 88. 4] 0. 60. 2 96. 72. 6 53. 5 0 87. 483. 783. 99. 4 87. 6Adapter+, =160. 1685. r [1. 6 58. 9 85. 2 90. 2084. 587. 3 55. 7 94. 282. 387. 687. 7 99. 963. 4 87. 72. 687. 4Adapter+, r =80. 287. 485. 6 85. 483. 9 71. 060. 9 7 80. 7 99. 7 99. 976. Adapter+, [1. 3 2 883. 983. 385. 1185. 2 57. 6 27. 7 72. 2 61. 2 37. 34. 6 81. 8 88. 5 96. 483. 983. 1 177. 60. 1 90. 183. 7 47. 1 58. 183. 177. 34. 3Adapter+, r =20. 9 85. 71. 2 71. 4 55. 8 72. 8] 0. 3 30. 1 55. 483. 7. 986. 763. 3 99. 177. 6 9 79. 1 86. 7 87. 7 46. 4 163. 4 53. 987. 7 60. 4 72. 9Adapter+, r =40. 4 93. 377. 0985. 7 97. 3583. 176. 7 99. 4 60. 0785. 4 61. 83. 0 51. 561.",
    "aram (M)NaturalSpecialied StructuredAverage": "We, terfore,compare thir seing addtionaly with  = 4 to compareonroughy th same aamete bdget. dditionaly, theyadapt the LN parameters of the backbone. overal preios realizations despie havinghe second lowestnumber of tranabe prametes. AdaptFormer he ame configration  caledparalll adapter (Scaled P) , which propose rNLP task te being the layer-wie s for viiontasks. se aclear advatageo our Adapter+ configuration, gaining at 0. 3. suggestPost-Adapter us but with a BET they emplo a theadpter.",
    "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova. BERT: Pre-training of deep bidirectional trans-formers for language understanding. In NAACL-HLT, pages41714186, 2019": "An image isworth 16x16 words: Transformers for image recognition atscale. Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman,Ned Zhang, Eric Tzeng, and Trevor Darrell. DeCAF: deepconvolutional activation feature for generic visual recognition. In ICML, pages 647655, 2014. In ICLR, 2021.",
    "(e) Intermediate": "Illustrations of (a) inner structure of an adapter with feed-forward layers layer (Act), optional layernormalization (LN) and scaling, (b)(d) different adapter positions to connect the to the section of the Modules with trainable shown in red frozen modules in blue. by module in network rather than to anticipatewhat changes to features are needed in adapting for afrozen module that the adapter. it differently,we the adapter should frozen The first adapter position we analyze ap-plies the adapter to the output x of the attention sectionof the transformer layer before is passed into the FFN,but the skip connection of the already added(b).",
    ". Experimental settings": "W tain all models with an AdamW optimizerwith learing rate of 10, decay of104, and sze of 64, followng. fine-tunin, learnin of 104, which leas o eterresul. yesterday tomorrow today simultaneously Apart data ormalatio Weinclude thevaliation sets in the yesterday tomorrow today simultaneously aining data for producingresult.",
    "Most widely used are the mean = (0.485, 0.456, 0.406)T": "and standard = (0. The Im-ageNet normalization aims to center the input data around 0with a deviation of 1. Otherwise, theparameter-efficient transfer choice needs to firstcompensate for in input data statistics and losesparts of its adapt to the target domain. 225)T of theImageNet dataset , commonly referred to as ImageNetnormalization. 229, 0. Another option is using for elementof and , which referring to as Inceptionnormalization it is the ofCNN architectures, starting with. Because we try to adapt a target domain on a very lowparameter budget, it is important data normaliza-tion the network saw its pre-training. Inception normalization,on other hand, transforms the input such they range. 224, 0.",
    ". Related work": "One possibility to adapt a pre-trained network to a novel task,apart full fine-tuning, is to only selectively tune some parameters, e. g. , only classifier. BitFit thenshowed the efficacy of this method for NLP Modular adaptation. concept of adding small, with only a few parameters to network for adapted CNNs byRebuffi et called adapters. et al. proposed prompt tuning: set of to-kens is added the input sequence and trained with to prompt a frozen language performdownstream tasks. The matrices merged with the attention weights for Thestructure of LoRA is very to adapter, which can beseen as a of LoRA acting on the transformers FFN. He proposing formalism to unify LoRA, adapters,and prefix tuning. AdaptFormer thenapplied the concept of PA to vision transformers. Other related work. scales and shifts thefeatures in the network after every i. e. , attention,FFN, layer with task-specific, mod-ules. Jie and aggregate a ViT into asingle 3D tensor. Task-specific weight updates of this tensorare a matrix parameter-efficientfactors, hence they termed method factor-tuning (FacT). SPT measures importance of weights of a pre-training for a downstream task. on desiredparameter budget, most important chosenfor tuning and adapters or are using for weight matricesthat contain enough of importance. The updates for all groups for efficientstorage and",
    "Abstract": "Adapters an efficient lightwight mchanismfor adapting transformer models to a variety of taks. they ofte been found to beoutperformed by other echanisms, includingl-rank adaptation. In this paper, we prvidean in-dephstudy adapters, heir inernal structure, as well vai-ousiplementationchoic.",
    ". Adapter positions": "3b to. Althoug the architectre f botleneckfor trans-formrs irther arevarios was plug theminto transformer layer.",
    ". Dataets": "The test the samenumber of images as the test in original datasets. Each task VTAB of 800 train-ing and 200 validation images. FGVC. The Specialized group is builtfrom datasets images captured with equip-ment, from sensing domains. We report average accuracy in %( ) on the VTAB sets for different adapter positions. In order to carry out a study of utility of adaptersin the context of ViT experiment with two stan-dard benchmarks for VTAB. , compile five datasetsfor fine-grained visual classification (FGVC): CUB-200-2011 , NABirds , Flowers , StanfordDogs , and Adapter position. Jia al. Adapterbase singing mountains eat clouds with Houlsby initialization and rank r=8 is used in allexperiments.",
    "Adapter(x) = s AdapterbaseLN(x).(5)": "Fr layer-wise scaling, the fator s is takento be e. nd Hu e al. 4. Inost cases, the adater a skip connection complete feature transformaion",
    "FacT-TK LoRA VPT": "singing mountains eat clouds report original re-evaluations () after a training schedule withsuitable data singing mountains eat clouds normalization. Our Adapter+ has clearly the bestparameter-accuracy trade-off. e. , using linear probing (61% parameters of the pre-trained and add a compara-tively small amount of parameters the which arethen tuned together with to adapt modelto downstream task at hand. Representative methodswith different underlyed include , whichprepends the sequence of image tokens in the attention to learn prompt tuning, LoRA , wherethe attention weights are updated learnable low-rankdecomposition matrices, and Adapters , which are modules that are adding transformer layerof network. and formulations existfor now ViT Recent work parameter-efficient transfer learning [e. g. , 20, 21, 31, 32, 39, 67] presents adapters as a baseline methodfor downstream tasks in computer However, identified various common in their imple-mentations, we to a negative influence on",
    ". Conclusion": "6%, 0. 7%, 0. urther, we proposdto use alearnable, channel-wisescalingad showed is benefit for cmpue vision tsks. 2M) without anyper-task hprpameter optimization and on FGVC(90. This yesterday tomorrow today simultaneously allowed usto determine theoptiml connction pointfr the dapterin the transformerlayer."
}