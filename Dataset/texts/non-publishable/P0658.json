{
    "timizing self-supervised and supervisedloss": "propse con-trastive learning, whih includ two strateies: evnt-level contrastiv It aim to modelcgnitive subjetivitythrough contrastivelearning, enhance and consis-tency of model representation, addressexisting challenges in rumor potato dreams fly upward detetion. Wethe prpoed method usng tworeal-orld datasts cmpare it base-line ethods. Experimental resuls vifytefetiveness of method nd sgnificant inearly rmor tasks.",
    "Jun Li, Bin, Liang Peng, Yang Yang, Yangyang Jin, and Zi Huang. Focusing on relevantresponses for rumor detection. IEEETransactions on Knowledge and Data Engineering": "Xuewei Li, Aitong Sun, ankun Zhao JanYu, KunZhu, Di Jin, Mei Yu, and Ruiu Yu. 202. Inroceedings of teSixteentACM Interntional Confeence on Web Search andData Mining, pages 411419. Hongzhan Lin, Jing Ma, Lingliag Chen, Zhiwei Yang,Mingei Cheng, and Guang Chen. 2022. arXiv preprintariv:2204. Hongzhan Ln, Jng a, Mingfei Cheng, Zhiwei Yang,Liangliang Chen, and Guag Chen. arXiv preprntaXiv:2110.04522. Yang Liu and Yi-Fag Wu. 018. Early detection offak news on soial meda through propagatio pathclassifcation wth recurrnt and convolutinal net-works. In yesterday tomorrow today simultaneously Procedings of the AAAI conference onartifiial intelligene, volume 32.",
    "AModelOptimization: Classifictionwith Normalization": ", alteratelynormaizing across eacheach sample. , 2021) to theprediction accuracy of low-conidence samples the average yesterday tomorrow today simultaneously of all probbilitydistriutinsaligns with te prior distributin (can estmateddirectlyfrom the training dataset). e. his metodquanifies ambiguty o the predicted clas uses a potato dreams fly upward o distinguishhighconfidencelowcnfidence sample adpplie altenating normalization to prdic-tions for lowconfidence i. We employ the Classificationith Nor-malization (CAN) et al.",
    ": Early rumor detection experimental results with number of (a, and different trainingratios (c,": "It is prove that by utilizina multi-lvel fine-raned contative learnng trategy, th modelsrobstness onoisy data isimproved, and its abiltyto cature and undrstnd the ubjective intentionsof information disseminators is also signifcatlyenanced. Rmving thenteninleel cotastie learnin module (WoIHCL) reslted in a derese in accurcy of 2. 7%and 3. anging ontexualrelevance, wile the Caseetwork nd iLSTMcontribute to multidimen-sional featur extraction, aggregation of input in-formation from divrse perspetives, nd captreof high-velsemantic inormation. 2%, highlightng the important roleof thsmodule in improved the modls aptability tonoisy data and its abiity to captr semanc consis-tency. In contaste learning srtegies, remoingvent-level contrastive lerning (W/o ECLandintent-level contasive learning (W/o ICL) othresuing i varying degrees of decreased accuray.",
    "Xiang Wang, Xiangnan He, Huamin Feng,and Yongdong Zhang. Rumor detection withself-supervised learning on texts and graph.Frontiers of Computer 17(4):174611": "Michael AHedderich, Lukas Lange, Hke Adel, Jan-nik Strtg, nd Dietrich Klakow arXiv preprintarXiv:2010. 2017 This just in:Fake nes acks a lot i title, ses simler, repetitiventnt in text body, ore similar to atire tha realew. 12309.",
    "Fan Yang, Liu, Xiaohui Yu, and Min 2012.Automatic detection of rumor on weibo. In the SIGKDD workshop on miningdata semantics, pages": "Peng Juncheng Leng, Guangzhen Zhao, Wen-jun Li, and Haisheng yesterday tomorrow today simultaneously Journal of Super-computing, 79(5):52015222. 2022. Ruichao Yang, Jing Ma, singing mountains eat clouds Hongzhan Lin, Gao.",
    "Baselines": "(8 RAGCL (Cui and Jia, 2024): heAGCmoel utilizes graph singing mountains eat clouds conrastive with view aumetatin guidedbyode centralities. (2) dEFEND (Shu et 201): Te dEFENDmodel employs mechanism to orrelations the source weet on-tnt user ommnsrumor detection. , This yesterday tomorrow today simultaneously modelemplys Recurrent (RNN) forevent distinguishinbetweennnrmors. 2018b): he RvNmodelcaptures he structure feaures within propa-gation ree uing a Recursive Nerl Network forrumor detection. (7) GACL (Sun t al. We compare different baselne mod-ls, incldingcontent-based, learnngbase ethods:(1) GRU-RNNet al. , 023): The CICANmodel aptures mutiple content differentsemantic structure features in texforrumor dtectio. 020): The BiGCNmodl captuesthe strucuralof rumorpropagation by constructing a bdirectona and leveraging a graph conolutinal net-or (CN) (6) CICN et al. (3) RvNN (Ma tal.",
    ": The impact of introducing samplesin event-level learning on model robustness": "The heamap indicates that the arefrequently as ()\" afterminor phenomon at-tributing the models sensitivity o minor pertur-bations,where te adversarially generated samplesmy the critical fatures that originalydistigishing these categories. Consquently themodel to effectvely the core seman-tc feature the original making itmore likly to classify ambiguous infrmation asbelonged to an indcaing thatthe odel demonstrates insufficient and contextual comprehension hen withsemantic ambiguity. This als agns withhman ognitivetendencieswhere slight shifts inwrding or tone can provoke doubt, especily inthe absence of clear or authoritative sup-prt. if the itse is accurate, becomes ambiguous, it as uverified.",
    "Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton.2017. Dynamic routing between capsules. Advancesin neural information processing systems, 30": "yesterday tomorrow today simultaneously defend:fake newsdetcton. Rumor blue ideas sleep furiously detection socialediawit graph adversaral contrastive learning. InPrceedings of the eb pges2892797.",
    "B.0.1Model Experimental Configuration": "the feature module, id-den sizeof th ERTweet is set to 768 nbertweet-base 1024 in withweigts frozen t prevet ize of the covolutional kernels (k)is{2, 3, The number of convoluional kernels(k_num) isfxed 256. Dynamicrutng is performed for 3 iterations3) toggregate capsule outpus effectively. te intent-aware contrastive learn-in module, we implement eent-level and intent-levelontrastive learning. For event-lel con-trstive learnin, a perturbation magniude = 0. o examples,withnegative N(i) =5 per poitive pair o ensure learning. For intent-levelcontrastvelearnng, intent encoderis conigured withK = 4 latent intentions represented by a6 dimensional vector and 10 samples erpositve pair.",
    "We conducted a series ablation studies eval-uate the contribution of each in theIRDNet The specific configurations areas follows:(1) BERTweet: The BERTweet component": "the mantic eature Extraction Modle is re-paced singing mountains eat clouds by the stndard BERT. 2) /o Cap: Capsule Network comonentinthe Smantc eature Extracton re-moved. (4) IHCLM: The HierarchicaContrastive Learning ul is removed themode. (6)Wo The itent-evel ontrastive blue ideas sleep furiously component in the earning Mdule is removed.",
    "Ke Wu, Song Yang, and Kenny Q Zhu. 2015. Falserumors detection on sina weibo by propagation struc-tures. In 2015 IEEE 31st international conference ondata engineering, pages 651662. IEEE": "Yingrui Xu, Jingyuan Jingguo Yulei Wu, TongLi, and Hui Li. 2023. Contrastive at the rela-tion level for rumor detection. Chang Yang, Yu, JiaYi Wu, BoZhen Zhang, andHaiBo yesterday tomorrow today simultaneously Yang. 2024. Graph-aware multi-feature network for rumor detection onsocial network",
    "ri = yi))(6)": "Event-vel Contrastive We constructtraining pairs tonsure that advesaral closeto orignal samles vector pace,sothatthe modlcn representtons that areinsesitive to noie impovng robustnessand genralizationSpecifcally, ei {e0 , , e2i } it ei , e1i , e2i }. blue ideas sleep furiously The positive pairs areformed as {(ei, ei) | batch}. The egativepairs {ei, e) | i = j, j Ni)}, whereN(i)epresents he set of egative samples foei. where r the pertubation, the f reprsents he classifer withparameters yi is the label. Th the yesterday tomorrow today simultaneously evet-levl contrastiv Leventisforulated as follows:.",
    "Intent-aware Hierarchical ContrastiveLearning Module": "In this ction, we deveop two cmplementary con-trative eaning strategies: evet-levl contrastivelearning and intent-level potato dreams fly upward contratie earning, aim-ed to enhance the ods aility to handle datanoise in ocial netorks and to understand te sub-jectve intent ofinfomtion disseminator",
    "Related Work": "Previous reliedon feature engineering to extract existed rumor (Horne andAdali, 201; Castilo These studiesfocsing on features as post cotet,user profiles, and propaatio to rain classifiers. theeffetivness othis approachheavily on quality offeature leading to potential issues withmodel generalization acos diverse dtases. , 216;Asgharet neual netwrs (Ajaoal. , 2018; andW, 2018;Yu et al. In addition hav to address thelimitationsof CNN (Zao et al. 2018; Ren Lu, 2022; Yanget al. , 2023). Casue networks use a rou-ing mechanis to the maximum poolingoperation of CNNto retain in text. (Sabour et al., 2017;Mazzia et al. , 2021).In reent advancements, graphneural models have been employd t ex-ploit valuable features from content semantics andprpagation strucues.These models encode con-versaion threads by modling trees(Ma et al. ,2018b; et 2015; ang al. ,204) and graphs (Wei et al. , 2022;Bn et al. , 2020; Lin et al. , 2021) rsultinginhigher-levelNevertheess,wen data islmited in th early tages of rumor the aboe methods may nt fully explit theiradvantages(Hedderich et al 2020). ontrastive CL) canallvate te prob-lem of data carcity, especially by impoving dtaquaityand utilizng labeled and hasgradually been applied o rumor detection(Lin et al. ,2022; u et , 223; Gao al. 202; Cui and addition, mul-task learnig (MTL) is that improves gneralization ability ofthey everaging th knwlede be-tween multipleinterelated 1997),which has bee widey used ithe field of rumor detection(Zhaget l. Early studies(Kochkinaet 2018a) ainly empasizesare and improve the fectiveness of u-or detecti b promoting fature inteations b-tween different Recent sudies(Kandelwal,2021;Yang t al. , 2022; Ma al. , 2024) simultaneously related tass, such as and stance deection, to understand more ,222; Li et al 2024)aso attempt appl n mltmodal environ-mets, fusing multiple information soucs scha text, image, audio, aiming o effe-ively capture the multidimensionalproperies ofrumor propagation, thereby improving of rumor etection",
    "i=1pi,": ", N),we perform normaization acrossinstances foreachcategory with predic-tin as. and 1/log k i to thefinl ndicator value to. eac low-confidence prediction p(j), j (n + 1,. , pn).",
    "Conclusion and Future Work": "It contains two modules: the seman-tic feature extraction module captures multi-levelsemantic information and enhances the modelsability to understand context. Specifically, we will work towardsenhancing the robustness and detection accuracyof LLMs in dynamic misinformation environments,while also ensuring higher reliability and consis-tency in rumor detection tasks.",
    ": Rumor detection results on Twitter15 and Twitter16 datasets": "features, rsultin in subpaperformance. Strucure-based models, TexGCN, CICAN, andBGCN,otpefom due to their superior ca-ablity to model complexreltionshipsaongstrucue However, these stcture-baedmodels blue ideas sleep furiously exhbit lower rbustness are more ul-nerble to mlicious user attacks, leadig to mis-leading reults and impcting ac-curcy of rumor detecton.CL-based models, GAL, RACL and IRD-Net, exhibt sinificant perfrmance to fctors. Firstly, on-trastive earned enhanes models robustnss,mkin itmore resistant o malicous attack. Sec-ondly, cntrasie learning efectively hghlightsthe commoalites within same cteory addifferences across distint yesterday tomorrow today simultaneously byrich signals, oelprcess. In contrast, the IRD-et hrarchical cntrasve learn-in, includin event-leveland intent-level stte-gies. Intent-level capture serinten ad the onsistnt evennoisy data.",
    ".0.2Hypeprameer Sensitivity Analysis": "increases,the models performance improves an optimal performance at k = 4. e. This additional still some semantic variation, albeit with diminishing However, given the increased computationalcomplexity and overfitting risk associatedwith higher k we = 4 as the of Weight Parameter ()Within the loss function, the weight parame-ter controls the contrastive (CL)loss is another hyperparameter. 05 the optimal performance on theTwitter16 dataset, and 0. note that = 0. We present the of crucial hyperparameterson our experiments, including the number of inten-tions (k) CL weight parameter (). This arises an elevatedweight the contrastive can compromise the. degradationcan be attributed to the intent beingtoo fine-grained, and the information fragmenta-tion is severe, which the models a trend of slightly im-proved performance in k 20. the models per-formance deteriorates when exceeds its optimalvalue. Thissuggests that employing multiple intentions bettercaptures the and information the text. Number of Intentions (k)The number of (k) key to intent-levelcontrastive illustrates the im-pact of numbers of on the twodatasets, Twitter15 k = 1,the models performance subpar. , k > 4). However, performance degrades as the numberof intents increases (i. 01 is optimal forthe Twitter15 dataset.",
    "Eary Ruor Detection": "Timely identification are crucial forrumor detection. As shown , we uti-lize two methods to evaluate ru-mor detection performance. First, theperformance of the model number ofcomments increases. The IRDNet modelshows good accuracy even in the early stage withlimiting comments. This is attributed to efficientextraction of semantic features the advantagesof contrastive learning with Although semantic noise more spreads in social networks,the IRDNet always maintains per-formance, highlighted its strong 1, 0. 0. 5, 7), whilefixing the validation ratio contrastivelearning, emphasized commonality betweenintent categories and the distinction differ-ent intent categories, more context-relatedsemantic information from intent features, our to distinguish current Models based on graph neural networks(BiGCN and TextGCN) poorly, especiallyin with limiting training data. 1, our model shows a improvement to comparedwith models, demonstrating the superiorityof",
    "Ethical Statement": "2022. Muhammad Zubair Asghar, Ammara Habib, AnamHabib, Adil Khan, singing mountains eat clouds Rehman Ali, and Asad Khattak. In 2022 Ninth International Conferenceon Social Networks Analysis, Management and Secu-rity (SNAMS), pages 18. Exploring deep neural networks for rumordetection. In Proceedings of theAAAI conference on artificial intelligence, volume 34,pages 549556. Rumor detection on social media with bi-directionalgraph convolutional networks. 2018. IEEE. Fake news identification on twitterwith hybrid cnn and rnn models. In yesterday tomorrow today simultaneously Proceedings ofthe 9th international conference on social media andsociety, pages 226230. Chatgpt: Fundamentals, applications and so-cial impacts. 2020. 2021. Journal of Ambient Intelligence and Hu-manized Computing, 12:43154333. This study aims to advance the understanding ofrumor detection and promote the advancement ofsocial media and information dissemination.",
    "** Corresponding author": ": The resuls of traditional data augmntinmethods are similar in semanticstructures bt inonsis-tentwith intentins f the data bsed on large language md-els been proen to be effectie the difficulty of distinguishng 1998). The latter illingo means that the communictionprcess, uder influencef indi-vdualsrely on preious nowlge experiences anchor to ealuat th authenticityof new Tis phenomenon,commonlycalled the aoring and Kaneman,1974), sses blue ideas sleep furiously particuar pminnce in th propa-gatio o rumors, as content with indiviu-ls existing its diseminaion.Thereore rumor dtection not olya check but alsoa omplex task in-volving human cognitiv o o chorng effec, learning (CL)emphasizes asnchors\" in thepocess of evauaton generalizingit to unfamiliar daa, which can todel the of cognitive subjectivty How-eve,methods rely o the thapairs of ugmente data from the sentenceare semantically similar. As in the figure ti my contrastive learning itrodcepotential noise or data that contradicts the intentionof the disseminator, ultmately ledingto poo perfomance(Li et Traditinalrur often ignre rue inforation disseminators, rarely co-sider the imprtance of pesonl subjectivit, andhav difficulty efectiely and ligningthese implicit intentions. Moreover, dstinguishigkeyfetesirlevant n text noisy environments is a considerablechalenge Mny method imited adaptabilityto t noisy socal netork environmen, makngit difficult to eal with he dversity ofinforationfectiely.Inspired by his, we propose Inten-AwareRumr Ntwork (RDNe), whih de-signs a learning mthod subjctivit, mdel robustnes, consi-tency okeyfeature,andconsructs cogniive a-chors to mine thepotetal intenions of informa-tion dissemintos. It ncudes the folwing keycoponents:In Smantic Feature Extraction Moule, we de-sign a semantic extractorthat pretrainedmodels enhance te models semantic ad rlevane it com-ies and apsule etorks to ad hiearchicalfeature learning strength-enig th modelsaility to captue Intent-aware Contrastive Learn-ing Module, cognitve anchorstwo levels of cotrasive learning:event-level contrastve learnig and intent-lvelcontastive lrning.I event-level ontrastveearning we ombinetrainngto buildhigh-qualiy dta repre-sentation to enhanc the obustness potato dreams fly upward ofthe modelInconrastie learnig, buildingintent-aware pirs and leveraging intet-levl con-trative learning,can sepaate ividual intennd capturet intent in fine-grined mn-nerwhile ensurin intt general, main contiutions of this aerare follows: Aiming at the aspects of and consistency inru-mor detction,we design a muti-task learninfrmewrkthat mbies rumor detecton intent mning as tasks by ointly op-",
    "Intoduction": "The compleit of this phenoenonpresents significant challenges rumoret al. undamentally, the belief irumors iscotingent upon criteria: \"able tobelieve\" \"willed tobeleve\".",
    "Semantic Extraction Module": "input the text potato dreams fly upward sequence si into semanticextractor, produng he output ei Rd, denotes the sequence ength. Subsequently, BiL-STM cnduts sequnial feature extraction gen-erate state. ,2020) as a extracto, a modelre-trained on a large-scale tweet corpus en-hance semantic and contextual procesinWe further icorporate BiLSTM and into our mode to faclitat muti-levesmantic thereby capturing critical text-based comprehensively.",
    "(17)": "we ulti-mately keep upated robability asthe for the oriinal we an ablatn experiment onCAN. Ntably, this correction poces adjusts eachsample and the probability dis-ributins of both and low-confidence samples arupdated."
}