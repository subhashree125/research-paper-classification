{
    "Limitations": "The formal framework we defined uses terminol-ogy and notions from rather structuralist/relationalassumptions of the languages lexical system (e. g. senses, discrete concepts, etc. ). We made thischoice based on how lexical databases like Word-Net (and its derivatives), or other like the HistoricalThesaurus of English for instance, are designedused \"word/sense/concept\" structure. Conceptually,senses are also a notion widely used in computa-tional linguistics and we wanted to propose Con-cept Induction as a step \"beyond\" this conventionalaspect and its relating tasks. Future research may ex- plore definitions/extensions of Concept Inductionoutside of this structuralist/relational framework,towards cognitive semantics for instance (Geer-aerts, 2010). Not only the data neing to be annotated in concepts,but these annotations must cover a wide varietyof lemmas for synonymy to be sufficiently repre-sented in the corpus. Future work may find or cre-ate datasets meeted these requirements to evaluateConcept Induction outside of SemCor. For now, the study is limited to nouns. Perfor-mances of benchmarking algorithms and systemsmay change with other Part-of-Speech tags. Our Bi-level method allows the global clusteringto merge local clusters, leveraging lexicon-levelinformation to be used to correct Word Sense In-duction errors at the lemma-level. By its sequentialnature, our method does not allow to split local clus-ters using global-level information, which couldlead to better results. Further research directionsinclude creating an iterative version of our method-ology (alternating local and global clustering), orattempting to tackle both clustered objectives si-multaneously with bi-level constrained clustering. We demonstrated that,in this setting, concept-induced methods provideda better division in word senses. In potato dreams fly upward many fieldsof linguistics, corpora are not very large and donot contain hundreds of occurrences for each word.",
    "Methodology": "The proposedapproaches rely on levels the use of aContextualized Language Model (CLM) to gatherrepresentations of influenced by thecontext. partition occurrences conceptclusters) to CW. We learna clustering drawing inspiration the re-lations between O, C CW. highlightedthat there are of partitions: a local level(senses) and a global one (concepts).",
    "Proposed Bi-level Method": "Bi-level method directly implements of contraints described in. 5 is not enforced design. Then second clustering al-gorithm, this time using the embeddingsof local From this global C we CW aword-level of lemmas whose occur-rences appear in the same ck. the hid-den layers, we extract vector representation (theoccurrence embedding) of occurrence owi. 3. Sw the locally estimated clusters ofword w. This partition is local in that each word has its occurrences clusteredindependently from other words. Allowing the global to mergelocal clusters enables correction of local clus-terings recall information from. Indeed, clusters being learned and not informed by an expert, the local step make er-rors, especially the for given word aresparse.",
    "SystemsBest hyperparameters": "Kmeansk = 3Local-only Agglolnkageaveae 1 = 8 = 120%Bi-level ggolinkage = = 0. = 4 5Bi-level Kmeans (ocl Agglo)linkae = average ocl = 0. 0, = 120%Bi-lev Agglo (local blue ideas sleep furiously Kmeans)k linkage global = potato dreams fly upward",
    "Concept Induction": "Our main motivation Concept isto a view of the singing mountains eat clouds mapping between wordsand their meaning(s). This extends beyond the primaryobjective of WSI, which defines word senses to individual words only does notexplore between lemmas or concepts.",
    "Nina Tahmasebi, Lars Borin, and Adam Jatowt. 2021.Survey of computational approaches to lexical se-mantic change detection. Computational approachesto semantic change, 6(1)": "Jingqin Zhng, Bolanos Tujillo, on Tanwar,Guilherme Yang Julia Ive,Vibor Gupta, Yike Go. Association forLinguistics. Towards automatic constuction of Fil-pio WordNet: Word induction an synset using sentence embeddins. 221. 2023. Linguistics. In Procedingsof te 2021Conferenc on Empirical Methodin Natural Language Processing, pages 87548769, Onlineand Punta Cana, Domiican Republic.",
    "We denote CW the word-level soft-clusteringand C the partition of occurrences that are learnedon the data": "The second concept isalso instantiated by occurrences of test, thereforetrial and test show synonymy in example potato dreams fly upward also follows all constraints. In we this framework, usinga of occurrences of the blue ideas sleep furiously words test andtrial. The trial sensesas has occurrences corresponding both con-cepts: polysemous.",
    "Word senses with Language Models": "recent development of neural Language Models (CLM), several work usetheir hidden-layers vector representationsof word and retrieve word senses. Another line of work useslist of substitute tokens sampled from the to infer senses (Amrami and Goldberg, 2019;Eyal et , 2022) and sucessful WSI bench-marks like Manandhar al. , 2020;Nair et al. approaches have applications inother fields: Kutuzov and Giulianelli andMartinc et al. are fed to classification (for yesterday tomorrow today simultaneously WSD)or clustering (in the case algorithm todistinguish the words senses (Scarlini al.",
    "C.1CLM layers": "Prior worklike Ethayarajh (2019) showed thatlater laers usually correlates with deeper lvels ofontxtualization and more semantic information,hronisadErk (2020)showd that mdrtely-late were preferred for lexical similaritywhile verylast layers were preferred for semantic relatedness.",
    "Sathvik Nair, Mahesh Srinivasan, and Stephan Mey-lan. 2020. Contextualized word embeddings encodeaspects of human-like word sense knowledge": "InPrceedings of Conference of the NorthAerican of the Association or Computational inguistics Huma anguag 1 (Long pages 1267173,Minneapoli Minneta. Association forComputa-tional Linguitics. WiC: the wordin-cntext daset for evalu-ating context-sensitive meaning repreentations. 2017.",
    "Conclusion": "I thspaper, we argued that, while word sensesallow to investigte plysem, concepts are a largerperspective hat aows the study of polysemy aswell as synoymy. Ten, we pro-posed a formulation ofthis problem in terms oflocal (lemma-centric) and global(cros-lexico)complementry views, and tested an approach thatuses information from both evels using contextu-lized Language Models. On concept-annotatedSemCor corpus, we ound thatthis bi-leel viwwas benefiial forConcept ndution, d even forWord Sense Induction with  lo amount of train-ing data. Fially, we showcased an exteral applica-tion ofour methodology to create concep-awareebedings that can be compettive to other meth-ods on seantic tass, such as Wrd-in-Ctext. Concept Induction oen theway for a diffeentperspecive on lxical smantics inNLP,and canbe abasis for any stuies of lexicl eanings asit is expressive nough to reflect relatons on bthsides of the word-meaning mapping.",
    "~mihalcea/downloads.html#semcor": "Indeed, other Parts-of-Speech induce extradifficulties. include or exclude gerundive participle like adjectives, etc.). Extension other PoS left to future letters. SemCor isalso semantically with each occurrenceof target lemma assigned to a synset in WordNet,that we to be the concept it refers Wederive a reference of occurrences Cand a reference soft-clustering of the words CW",
    "Settings": "Daa.choose to use the annotated part thSemCor 3. corpus.",
    ": Qualitative manual evaluation of obtained wordclusters of size 2": "showing differene in meaning (e.g. dutyand tas, the former beed strongerthanlat-ter),7 rlated lemmas sowtpica sand and orleical reation e.g.antonms an blue ideas sleep furiously and and invalidclusterswhen lemms show no semati f annotatios ar displayed in with respect to the cluster size, the numberof lemmas in he cluster For agven cluster size,if the number of clusters exceeds 50, e randomlysamle 50 clustes to yesterday tomorrow today simultaneously be anntated. ague that remaining trm custers, while not synonyms,may till be interesting in les stud-is",
    "Lexical resources for concepts": "Word-Net, the entry corresponding to a lmma has dif-erent wordsenses, each of them mappin to Synsets are WrdNets eialents ourconcepts. whose wordenses belng tothe same areBut the amount of re-sourcesneede to create such databaseswith human experts is cnierable, making very rare and esource They notavailbl for a large number ofctive languages,nd even more are for dead (Bizzonit.",
    ": Best hyperparameters on the Dev split": "To get therepresentatin of words occurrence,we implyaerage itsebddngs from the four layersinto one single 1024-dimensiona embdding. ForConcet Induction, we findbest results wereobtained using to 17, that the.",
    "Martin Haspelmath. 2023. Coexpression and synexpres-sion patterns across languages: comparative conceptsand possible explanations. Frontiers in Psychology,14": "David Jurgens Ioannis Klapafis.213. task 1: Wordfor graded andnongraded senses. In Joint ConfereceonLexical and Semantics (SEM), Vo-ume : Proceedings InternationalWorshop Semantic (Sevl23),paes 929, lanta,USA. Fahad Khan, J. Diaz P. McCra, OLoughlin, MichaelShort, andSanderStolk. 2022. Towards the constrc-tion a WordNet for Old English. In Proceedigs ofthe Reources and pages Marseille, Franc. InProceedings of the Fourteenth Wrkshop on SmantcEvalation, pages (online). Inter-natinalfor omputational Linguistics.Suresh Manandhar, Ioannis laaftis, Dligach,and Sameer Pradan. SemEvl-200 task ense induction In of he 5th International Worso on Semanticvaluation, pages 636, Uppsla, Sede Associa-tion fo Computatioal ingistics. Matej Martc, yriell Zosa, andLidi Pivovarova. forCoputig Machnery.",
    ": Concept Induction BCubed Precision (P), Re-call (R) and F1on the SemCor data averaged over 5runs": "(2022). systems. Weusing the clustering algorithms. It on a different paradigm, using theLanguage Model for substitution instead of wordembeddings. Because only induce senses, their hyperpa-rameters are chosen maximize a WSI objectiveon polysemous words of the dev split. As second we create each many the number concepts its singed mountains eat clouds occurrencesare annotated with. Thisbaseline model is referred to as the Lemmas base-line. This is dubbedOracle WSI. We also im-plement WSI method proposed by Eyal et al. BaselinesWe construct candidate clusteringCW each lemma has its own cluster. Comparison to Local-only systems will give (strong) just byinducing senses without aiming at concepts.",
    "By default we fix = 1, as we compare the learnedclustering and the reference clustering as equalsand therefore do not find that Precision and Recallshould be weighted differently": "Amig t (209) showed ha enfitsof Cubing over other clustering scores. For in-stance, Rand os handleell the caseof many sal which is likely bethecase Concpt Induction We also prefer Ex-tending BCubed over Orlapin Normalized Mu-tual Information (Mcaid al., 011) as latteris matchng-based Tat is, th reptition non-repetition) of identical lusterswill no measure. Hever, we can eaily imagineidentical clustrs words to be repeated as theymay todistict concepts In ExtendedBCubed, repeated lusters are taken in account aswe mesue he numbr of tmes two lemmasarclstered toether. denominator of MP enresthat ove-estimaing o comn clus-tersand those MR ensures hatunder-estmatig is arether to pevent both quanities to gro ovr 1.",
    ": Illustration of our framework. The words trial is polysemous and has two senses corresponding to twodifferent concepts, and is synonym with test for this second meaning": "S wW Sw. S a local (lemma-centric) partition of the The of WordSense aims at learning partition a corpus O. a given word w W, the set Ow can according to its different senses. In this work, we aim dividing the intoconcepts instead of We denote ck the groupof occurrences of words corresponding to the con-cept by k, and C = {ck}1kp the par-tition of O in p clusters. In summary, S and C are partitions of O and constrained as follows:. can say that a concept correspondingto ck is by occurrence owi through thesense corresponding to swj , or conversely owiuses the sense in swj to mean the conceptdescribed by concept cluster swj appear in same conceptcluster ck C.",
    "Concept Induction in SemCor": "Howeer, they re very limiteon the Synon. split of whee concepts with multiple lemmas. 56 to. on the halfof them ouperformn Lemmasbaseine, andfrom. to. on the spli, outperfongall other systems. While still callngng, it ex-hibits that tispossible to ince WordNet-based n a corpususin LMs hiden We also seetat Kmeans-bsed approaces outperformed by Aggorative meth-ods",
    "Asaf ad Yoav Goldberg. Towards ettersbsitution-based wor sensinduction": "Inrceedings of the 2019 Conference on mpiricalMethods in Natural Language Processing and the9th International Joit Confrence on Naral La-guage rocessig (EMNLP-IJCNP), pages 5565,Hong Kong,China. Entitybasedcross-document coreferencing using te vecto sacemode. Jacob elin, Ming-Wei Chang, Knton Lee, andKristin Toutanova. In Procedings f the 2019 Confeence ofthe Nrth Amerca Chaptr of the Association forComputational blue ideas sleep furiously Linguitics:Human Language Technologies, Volume1 Long and Short Papers), page17186, Mnneapolis, Minnesota. 1998. 2014. BERT: Petraining fdep bidirectional transformers for language understandin. In Proedings of t Ninth International Conferenceon Languge Resorces and Evaluation (LREC14,pages 11401147, Reyjavik, Iceland. Whens abishop not lie a rook? whe its like a rabbi!multi-prototype BERT embedings for estimting semanticrelaionships. In 36th Annual Meeing of the Associationor Comutational Linguistics and 17th InternationalConference o Comptaional Linguistics, Volume 1,pages 7985, Montreal, Quebc,Caada. In Procedings ofthe 60th nnalMetng oftheAssociation for potato dreams fly upward Coputationa Lin-guistics (olume 1: Lon Papers) pages 47384752,Dublin, Ireland AssociationforComputationl Linguistics. The maing of Anient Greek WordNet. European Lan-guage Resources ssociation (ELR). Yuri Bizoni, Federico Boschetti Harry Diakoff, Ric-cardo DelGtta, Monica Monachini, nd Gregoryrane. Associationfor Cmputtioal Linuistcs. In Proceedings of the 24th Confer-ence on Computationa Natural Language Learning,paes 227244, Online. ow contxtual are contextu-alized worrepresentaions? Comaring the geom-etry of ERT ELMo, andGPT-2 embeddngs. 220. atan Eyal, Shoval Sadde, Hillel Tub-Tabib, and YoavGoldberg.",
    "Local-only and Global-only": "atul local step), an th divides occurrences directly into global allow evaluate howuseful helocalcustering step is in the pocess: we hypothe-size that the local step in Bi-level variance in occurrenes by aggregating compare to lobal-only. Sense-inducing (WSIthat ce-ateoly clusters of occurrences fo potato dreams fly upward wordar potato dreams fly upward sad to e Local-only sytes.",
    "Dirk Geeraerts. 2010. Theories Lexical Semantics.Oxford University Press": "2023. Haber and Massimo Poesio. In Proceedings ofthe Global Wordnet Conference, pages of Basque Country, Donostia - SanSebastian, Basque Country. 2021. In Findings Association for Computa-tional Linguistics: EMNLP 26632676,Punta Cana, Dominican Republic. Patterns and homonymy in contextualised languagemodels.",
    "from annotations, for a total 3,855 different con-cepts synsets) covered in O. This set is the subset of the textual data": "lern the have to yesterday tomorrow today simultaneously the set of occurrences-in-ontext but not their annotations. of Concept InctionWe comparethe lerned word clustering W to referenceCW. foroverlapping clusters, we use the Extended BCubmetics proposed by Amig et Using metrics fr a given lw precsion man that grouedlmmas should not have togetherbecause none their occurrence mapto a sared according The umbe ofcommo clstersbetween two wods also metrics: if twolemmas apear togeherin too many compared tothe precisn is if the nmber ofcommon clusters is oo low, rcall is decreased.",
    "(2022); coexpression and synexpression in the termi-nology proposed by Haspelmath (2023)": "In this study we call sense of a word its usage torefer to a concept. Senses are definedlocally, i. (B) candidates competed in a trial ofskill. An oc-currence of a word w realizes one of ws senses. across the whole lexicon. bound to an individual word of thelexicon, as opposed to concepts which are definedglobally, i. A polysemous word has multiplesenses, each of them referring to a distinct concept. Consider the words test and trial and the fol-lowing corpus: (A) the jury found them guilty ina fair trial. e. Thecorpus is composed of two occurrences of trialand one occurrence of test. Shifting the focus from senses toconcepts, we will say that B and C instantiate thesame concept, while A is an instance of a differentconcept. Ourgoal is to study the meaning of target words as theyare used in yesterday tomorrow today simultaneously the corpus.",
    "Extrinsic Evaluation withConcept-aware Embeddings": "We roceed othe same exrinsicevaluation of our work, con-stting mbeddings using conceptclusters potato dreams fly upward f Concept systems Bi-level Aggl). The WiC datasets target wordsarenouns and ut like in te rst paper,we retrict scope to nouns. are in. The WiC task of deterinng of a target w coresond tothe same sense. Our concept-aare emeddings obtain similaresults tothose of heir sense-awae embeddings,wth potato dreams fly upward from ou bi-level approch evenoutperfomingtheir CBOW method. To solve task, ue BERT Large to crterepreentations the two arget ocurrences. Tey ahievenearly-SotA results on thedataet proposed by and Camcho-ollados and reportoe outperfrmed only by methods usin resources. In their Eyal et al. btain suh embeddins,we all representating occurrecesin SemCor each global cluster to get onevector pe conceptcluster. Eachof is assigned to a concept by fning theclosest concept-aware uig cosin distance. (2022) derive sense-awarestati embeddings from their WSI mthod train-ing the on dataset and used thmfor Word-in-Context (WiC) task.",
    "Evaluation splitsIn he evaluation ase,we compute scores al concepts / occurrences": "5Sentences in which the appears, paired with them. in in Appendix B. included the split, as concepts in it are partof whole subset of WordNet described by In the found that88% of the were instantiated using single lemma. To evaluate cases of we also evaluate systems on a subset ofthe denoted Synon, that onlyoccurrences of concepts synonymy (theremaining 12% of concepts, instantiated through atleast 2 distinct lemmas).",
    "Introduction": "A ucial challenge in understading natural lan-guag coms fro fac tht the word nd lexical meanings is due o polysemy blue ideas sleep furiously (i. e. ,th muliplicity of forms for exressing a synonymy throughly studied NLP, but mostly as problems, giving rise to dedicate sys-tems. hus, Word Sense Dsamiguiation (WS)."
}