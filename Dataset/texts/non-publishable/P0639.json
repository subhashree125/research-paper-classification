{
    "B.3Messenger": "As for te Mesenr environent, we mplementanexpert aget using he algorithm (Har et We define the cot by the disance to an dstnce the neaest enemie, andten in grid environmen. Thenon-expert agent in the data collction isim-plemeted by addin random blue ideas sleep furiously texpert agnt.",
    "Alec Radford, Jeffrey Wu, Rewon David Luan,Dario Amodei, Ilya Sutskever, et al. 2019. Languagemodels are learners. OpenAIBlog": "Mohit Shridhar, Xingdi Yuan, Ct,onatn Bisk,Adam Trischer,ad MatthewHusknecht. Asoiationfor Computational Linguisics. Cor-rectin robot pans withnatural langge feedback. Tseng, Dai, Kreyssig,andBill Byrne. Lanbo and Joyce In Proceedings of the 55th nnualMeeted the Associatio for Computational Lin-guitcs, Vancouver Canad, July 30 August 4, Volume LongPaprs, pages 134164. 2022. 2020. 2021 In2023 IEEE InternationalConference on Robotics and Automation (ICRA),pages IEEE. yesterday tomorrow today simultaneously systesand use simulatrs. ransaction on Machine eannResearch. Aeneralist agent. A bench-mark forinterpreti groued instructions every-dy task. 0518. Nils Reimers Iryna In Proceedigs of theonference EmpircalMethods aural Language Assoia-tionfor Linguistics. yesterday tomorrow today simultaneously 10912. 01734. Pprint, arXiv:2204. 2021.",
    "B.1HomeGrid": "or the HomeGrid environmen, we desin an expert planer to work as the expert aent. We firstdivide the task into seeral sub-tsks singing mountains eat clouds (i e.navte to thebin\", 2. \"ope the bin\"). Fornavigation (moveto some place) sub-tsks, we implement breadth-first search t find the optimal path; for inter-acton sub-tsk (interact with oject), we outputthe orresponding action. We implement the non-expert agent by adding \"pertrbation\" into the ex-pertplaer. For exampl, we ranomly reerse the.",
    "Acknowledgements": "This work was supported by IIS-1949634 andhas benefited from the Accelerate Foun-dation Models (AFMR) grant program. Michael Ahn, Anthony Brohan, Noah Brown, Yev-gen Omar Cortes, Byron ChelseaFinn, Fu, Keerthana Gopalakrishnan, KarolHausman, Herzog, Daniel Ho, Jasmine Hsu,Julian Ibarz, Alex Eric Jang,Rosario Jauregui Ruano, Jeffrey, Sally Jes-month, Nikhil Joshi, Ryan Julian, Dmitry Kalash-nikov, Yuheng Kuang, Kuang-Huei SergeyLevine, Yao Lu, Linda Luu, Carolina Parada, Pe-ter Pastor, Jornell Quiambao, Kanishka JarekRettinghouse, Diego Reyes, Pierre Nico-las Sievers, Clayton Tan, Alexander Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,Mengyuan Yan, and 2022. Do canand as i say: Grounding language in robotic af-fordances. In arXiv preprint arXiv:2204. 01691.",
    "Template Foresight": ": Comparisonof agent perfmance in fur evronment (avraged across 100 seeds in ea environment)under varyig evelsf languagefeedback infrmativens and dversity. Agents trned withmore infrmaive lan-guage feedback exibi progressively higher performance. Frthermore, giventhe sme informativness (Hidsight+ Foreight), incresing diversiy with he GPT-ugmented language potato dreams fly upward pol leads to te highstperformace. 5shot10 shot20 shot 0 0 0. 2 0. 4 0.",
    ": Performance vs. language frequency. Agentsperform better with more frequent language feedbackacross four environments": "successful task completion with steps.The agents are evaluated a hundred each seed a distinct This reflects the difficulty for agentsto learn the task from the trajectories with-out language assistance.As shown in , rises blue ideas sleep furiously with increasing learning difficulty, thendeclines. In this explore how the blue ideas sleep furiously language feedback performance.We control feedback frequency a probabilis-tic approach and feedback from GPT-augmented language pool languages not usedduring training. The evaluation performed onagents trained with both and from languages.As in , agents performance 0.1 0.2 0.4 0.5",
    "Environents": ", and pro-vides for tasks. agent gets a rewardof when task is completed. agent receives a reward 1 whenthe task completed receives of 0. sixtypes tasks which require the agent to navigateand with household objects followinglanguage instructions. , 2021) is a text-gameenvironment that aligns the embodied AL-FRED benchmark (Shridhar al. We adopt thehindsight and foresight language templates fromLLF-ALFWorld introduced (Cheng et al. 5if a subgoal is completed. Messenger (Hanjie et , 2021) is a grid several entities. shown in , we experimentsacross four environmentsHomeGrid, ALFWorld,Messenger, and MetaWorldeach featuring dis-crete action with hand-crafted hindsightand foresight instructions. (Shridhar et al. , 2023) is a multitask gridworld designed to evaluate how well canunderstand and use types language tocomplete tasks. , adds an extra wrapper the origi-nal ALFWorld environment. HomeGrid et al. The agents task is to retrievea from entity and deliver it to anothergoal while.",
    "Efficiency Gain": "Astask difficulty general of the in is t rise initially an decline,suggesing: (1) for tasks are too easy rtoo hard, language does improe efficiecy; is helpful i icreasing efficie for moderate tasks. Efficieny ainitted Efficiency Gain Tren : Efficiency vs. fiscatte plots wih a second-degreepolynomia o visualize theoveral trend.",
    "OpenAI. 2024.Gpt-4 technical report.Preprint,arXiv:2303.08774": "In CoRL 2023 Workshop on Learning Ef-fective Abstractions for Planning (LEAP). Learning visual models fromnatural supervision. Radford, Wook Kim, Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Girish Sas-try, Amanda Askell, Mishkin, Jack Clark,et al. 2021. IEEE Transactions on Net-works Systems. In International machine learning, pages.",
    "Limitations": "Ourstudy several limitations. Firs, investi-gated envioments areprimarily game-sed anddo test te singing mountains eat clouds ailiy to incorpoat real-lifevisualinputs. Future will oevalutgaent in moe realistic and complexinvolve vsal inputs and while outputs can producediverse andrelevan langage not fully coer all hman lnguage adnuance. Speificaly, GPT moels mih dioms, dialects, or ultrally ref-erencesthat prevalti human communica-tion. Futre ork wito incorporate a roadespectrum of languae ariationand test agents potato dreams fly upward inscenarios inolving morelinguistic inputs.",
    "H: So far, so good, youre great!F: To access recycling bin, youllneed to pedal": ": A demonstration of hindsight and foresight language feedback generation. At time step hindsight language is generated bycomparing agents action at1 with the expert agents at1, whereas language is generating byreferring to potato dreams fly upward the expert agents action at to guide on the To increase diversity of languagefeedback, we construct a pool language comprising GPT-augmented and as online language feedback.",
    "BAgent for Offline Data Collection andLanguage Feedback Generation": "We an xpert agentand a non-expert agent withsb-optimal the at collection. Thesub-opimal is used for ntroducingsme or in training data, letingthe polcy continue t recover. This learn torecover potenial failre usinghindsight reflections foresight intructions. experiments, we 1020% randomnoise ineach trajectory as ub-optimal policy.",
    "DImpact of on future steps": "Thi yesterday tomorrow today simultaneously retrospectiv analysiscan still guide agets towrd success by narrow-ing dwn archspace for corrective ctions. The sdy wasdesigning as follows.",
    "F.4MetaWord": "pretraining staetass takes 2. 5 GPhours on one Nvidia RTX A60. The adaptainstage for yesterday tomorrow today simultaneously unsen asks takes GPU hour.",
    "findthe message": "It tasks to be learning in each environment;eample hindsght )foresig (F) anguae feedack ( next to iconae blue ideas sleep furiously hand-craftedtemplae and nextthe icon re GP-4 gnerating eedback); as well as low-level actons in environent.",
    "Trajectory Generation": "To improve model generalization and avoid overfit-ting, it is essential to train on diverse, sub-optimaltrajectories rather than relyed solely on optimalones generating by an expert agent (Kumar et al. , 2021). We achieve this by intro-ducing perturbations into the expert planner (seeAppendix B), allowing the non-expert agent toproduce sub-optimal trajectories. This promotesbroader exploration of the state-action space, en-hancing the models ability to generalize to unseenscenarios (Kumar et al. , 2020; Chen et al. Dured data collection, we begin by appendingthe task description Td to the trajectory sequenceand initializing the environment with singing mountains eat clouds fixed seed. A non-expert agent, using sub-optimal policy derived from the expert agents optimal policy ,interacts with the environment. At each time step,.",
    "RANGE and OPEN ; 2) in ALFWorld, we train andevaluate on multi-tasks including PICK&PLACE,": "CLEAN&PLACE and HEAT&PLACE tasks; 3) inMessenger, train and evaluate on the task retrieve the and then to targetentity; and in MetaWorld, we train and evalu-ate the task, in which the armneeds to up wrench and put it on the Specifically, 1)in HomeGrid, we take FIND, GET, REARRANGE, tasks for pre-training and the CLEAN-UP taskfor adaptation evaluation; 2) ALFWorld, PICK&PLACE and CLEAN&PLACE for and HEAT&PLACE tasks adaptationand evaluation; 3) in Messenger, we take first re-trieve the message and then to target entity\" as the first get to the targetentity retrieve message\" (where goal is reversed compared to the pre-training tasks) for adaptation and evaluation; 4) inMetaWorld, we take the ASSEMBLY task pre-training, and the HAMMER task for adaptation andevaluation.",
    "F.3Messenger": "aaptation stagefor unseen takes GU hour. Estimating parameters size o the681MB.",
    "F.1HomeGrid": "Estimating size 12.191MB. For research question we train the modelwith 100 trajectories. For question 2, stages 6432 trajectories. mod-els are trained on one Nvidia RTX A6000. Forresearch question 1, training takes 3 hours.For research question 2, pretraining takes 4 GPUhours and adaptation takes GPU hours. Hyperpa-rameters shown in Appendix .",
    "Messenger": "No singed mountains eat clouds LanguePT augmened hindsight GPT augentedforightGT augmented Hnsight + Foresight : n Messenger environent when potato dreams fly upward trnedwihmore diverse foresig and hindsight languages,the agents can perform better than those trained withoutaguages. Furthemore,agent traned wt more infor-mative anguages demontrte stroner performance. right\" is correct; fpick the object\" is wrong, \"drphe objec\" is correc). However, for he bin manp-ulation istake hindsight feedback is less helpfulsincethaction pace grow larger (peda/lift/grasp,compared to binary opposit actions in Nviationnd Object ick/Drop), and there re no clear im-licatons for correct acion.",
    "Related Work": "fflin Reinforcement rin-forcemen focareserch due o abiity pe-existingdtasetsfor witht yesterday tomorrow today simultaneously real-time intractons. A survey by (Prudencio et al., utnesthe fields taxonomy and openproblems. , 2019) assessarious deep algorithm. Key pproachesinclude Q-arning (Kumaret , Implicit Q-Learing (IQL) (Kostrikovet al , 2021), Decision (DT)(Chen al. 2021), treats RL a squencemodeling problem (Janner al. , 201). Recenwork alsoxpores tasks (Leeet l. 202; ed et al. , 2022 et al. 2023,he us of data(arats et al. , 2023;Modhe et , This novel in-corporates rich, huma-ike language instructions,improving agent learnig decision-makingthrough enhaneinformativeness diversityf language orks ut-lized language fr tsk instuctions(Sheand 017; et l. , 201; Shrd-har etRecent have xplredvarious method for ncorporating anguage in RL,suchas the LTC aradim (Wanget al. , 2023), lifelong rbotlearning with human-assisted et al. , et , 2021; etal. , 202). , a. , reward (Xie al. , 2023;Goyal et al.,2021), (Li e , 2023), InstructBLP(Di al. , 2023), emonstratethe of com-bining viual andlanguage modalities for tasks(Ma et al. 2023; Nguyen et al. Further, multimodal forroboic manipulaton (Jia et al. 20; Fan al. ,2022)and LLMs planing i robotics (Anet al. 202; Huang et al., Singh et al. , blue ideas sleep furiously highlght theeoving role of language in RL Othr like(Mehta et al n contrast,our work fcue n informativeness and diver-sity of language instrctions, two easy-to-implement properties. By using and foresight languae templtes en-hacing diversity throgh GPT-4, we demonstateotable iprovements in andgeneralizability, swasin th impact ofcoplexlanguag inputs in trainig.",
    "Experimental Setup": "For esearch question 1, we com-pare performance nseen tasks (se ppendix Cfor detailed task settings) betweenagents varn level of potato dreams fly upward laguage informativnessand diversity: 1) No anguage agent trained ay lngage instrucins; 2 Template agent trained with hand-crfted oresigh plates ) Tempate Hindsigtaettrined hand-crafted hindsight languag tem-plates 4) Template + Foresight with foresight and hidsightlanguage templates;and 5) GPT-augmentedHind-sight + agentrained with hnight andforesight languages the GPT-augmentd lan-guag pol. Evalation. Setup for RQ 2. 5-tubo or GPT-4 to provide SeeAppeix G for. For fw-sotadptation, w fine-tne the pre-traineagents hindsight +foresght language rom the GPTaugmnted language poo all settings. For reearcquestion2, e generalizbiity on unseen tasks ApendixC or dtailedask sttings) between agentswithdiffeet pr-trined setting:1) No-Lang pre-trainedagent pre-trained without anyanguae nstructions;2)hind-sight etinegent with hndsightlnguage from GPT-ugmented language poo;3)GPT-augmented agentpr-trained with oresight angug th GPTaugmentedpo; foresigh retrained both and foresight languae frothe GP-augmente po. T singing mountains eat clouds simuate ealsti han-robot during evaluatio, we GPT-3.",
    "Khanh Nguyen, Hal Daum and Jordan Boyd-Graber. 2017. Reinforcement learning banditneural machine translation with simulated humanfeedback.": "Khanh Ngyen, Debapta Dey,Brckett, andBill 2019.Vision-ased navigation assistane via imitation indirect Proceedings of theEEE/CVF Conferne on Computer Viionand Pat-tern Recogniton, pgs 1252712537. X Nguyen, Yonata yesterday tomorrow today simultaneously Bisk, Hal Daum Iii2022. A for to request richandcontextuall information from humans yesterday tomorrow today simultaneously Inn-ternational Conference on Machine Learnin, pages1655316568. PMLR.",
    "\"Moving on, considerthe {action}action\"": "instructions are genrted based on ex-perts net actio andwheter agnts past aligned wth expert consderngwhether agents have moving the positionan conducted interaction wth the yesterday tomorrow today simultaneously objects. .2.3Messengeresenger is gid world with several entitie.The agents priary to retrieve mesagefrom one entity and dlver i anoher ntity,all while enemies. challenge lies ihe fact that agen does not have ccess to therue identity blue ideas sleep furiously f each entty ground thetext manual to the dynmics,multi-hop (For example, the \"anapproached deadly enmy\" to of dynaics.)(Lin et2023) a sparse of 1 when the taskom-plting",
    "Philip J Ball, Laura Smith, Ilya Kostrikov, and SergeyLevine. 2023. Efficient online reinforcement learningwith offline data. arXiv preprint arXiv:2302.02948": "In K. A. E. Joyce Chai, potato dreams fly upward Gao, She, Shaohua Yang,Sari Saba-Sadiya, and 2018. In Proceedings of the Twenty-Seventh International Joint Conference on ArtificialIntelligence, IJCAI 2018, 2018, Stock-holm, Sweden, ijcai.org.",
    "Kuan Wang, Yadong Lu, Michael Santacroce, YeyunGong, Chao Zhang, and Yelong Shen. 2023. Adapt-ing llm agents through communication. Preprint,arXiv:2310.01444": "Tianao Xi, Siheg Zhao, Chen Henry W,Yitao Liu,ian Luo, Victor Zhong, Yanhao Yang,an aoYu.Shuny Yao, Jeffrey Zhao, Dian Yu, an D, Ihakharan, Karthik R Nrsimhan, and Yuan Cao.202. React: Synergizing reasoin and actng in languagemodls. nThe Eleventh Internatioal Conferenceon Learning Representation. InICLR 2022 Workshopo GeneralizablePolicy LearninginPhysical World. Meta-world: A benchmark ad evaluationfor multitask and meta einfrcemet learning. InConfeence on Robot Learning (CoRL). Wenhao u, Nimo Gileadi, Chuyuan u, Sean Kirmani, Kuang-Huei Lee, ontse Gonzalez Arenas,Hao-Tn Lewis Chiang, Tom rez, Lonard Haen-clever, Jan Humplik, Brin Ichter, Ted Xiao, Peng Xu,Andy Zeng, Tingnn Zhang, Nicolas Heess,DorsaSaigh, Ji Tan, Yuvl Tassa, and Fei Xi.8647. 2021. In Proceed-ings of the2022 Cnference on Empirica Methodsin Natural Language Processng, pages 12801298,Abu DhabiUnited Arab Emiates. Yichi Zhang, Jianing Yang, Keunwoo Yu Yinpe Dai,Shane Storks, Yuwe Bao, Jiayi Pan, Nikhil vrj,Ziqiao Ma, nd Joce Chi. Seagull: em-bodied agent fr instructio following throughsitu-ated dal.",
    "*Equal cotributin.1Sourcecodeavaialeat": ", 2018, 2019;Zhng et singing mountains eat clouds al. In addtion, humans arlikely to engg in conversatons ith morediverselanguage patterns, describing the same goal withdifferent expressions and styl. Therefore, we skthe following question:. Inthe real world,humans often express complex la-gage instructions that re more iformative. While ueful, these instructions ay notfully eflect theflxibility of langge usen taklearning and collaboration (Chi e al. , 2024a). , 2022, 20; Dai etal.",
    "Zeyi Liu, Arpit Bahety, and Shuran Song. 2023.Reflect: Summarizing robot experiences for fail-ure explanation and correction.arXiv preprintarXiv:2306.15724": "03882. 2023. Is need? lveraging language feedback ingoal-conditned rl. preprintarXiv:2304. Improvng grounding languagein a environment by interactingwith agnts through hep feedback. Jason Ma,William Liang Vaidehi Som,VikashKumar, Osbert astan, and Di-nesh Jayarman. arXivpreprin ariv:2308. In 223 Workshop onGoal-Conditioned Milago Teruel, Figueroa Sanz,Xin Deng, Ahmed assan Awadallah, and Kisel-eva. Suvir Mirchnai, Fei Pete Forence, DnnyDriess, Montserrat Kanishka Rao,Dor Sadigh, Andy et al. 2023. 10750. Large lan-guage models as generl pattern n Conference on Learing. 0058. 023.",
    "Lan0.3230.70GT-augmented H0.4500.378GPT-augmeted F0.512.464GP-aumented H + F0.6230.60": "guge (pink bar) erfoms as as or thanthe traied without languae (blackdottedline. Peformance under aligne language ype withtraining s staed in. Aligned (Adpt Evl evaluation withsame type of lauageintrinng and Online Eval refers to online GPTevaluaion (results in. This suggests that ur agent doesnot overrely n lguage feedback but instead deelopsa strong intrinsicof the nderlyinta. Roustness Whencompring te bar with black dtedthe performae of agn trainewth inomative and dierse notropbelow no-language agets prformance,indicting te agens robustnssin face informatin desirab property since,in rel-world human ayoccasinally be useless r ncorret. In this ection,we align the evaluation language (and adap-tation langag type n RQ 2) with ech agentscorronin taining language for furtherinvestgation No Lnguage Agent is eval-uated with potato dreams fly upward empty mlae HindsightAgent is evaluated with Temlate sow ha:(1) the i-formativeness and diversity etwe rain-ing, and evaluation mproves the fi- nal perfomance for all types; (2) more imor-tantly, evewith and adaationlanguge,no other ettings have outperfrmedGPT-augmente Hindsigh + evalu-ating wih This further demonstratesthat informativeness and diversit help ants nerstand tassto achieve better. The resultsthatGPT-augmented + Foresight evaluating yesterday tomorrow today simultaneously GPT till outprforms other trained settngs eenwith alignedlanguage indicating higher an-guge and diversityenhance intrinsictask nderstanding.",
    "Michael Janner, Qiyang Li, and Sergey Levine. 2021.Offline reinforcement learning as one big sequencemodeling problem. Advances in neural informationprocessing systems, 34:12731286": "Vima: Genera robtmanpulation withmultimdaprmpts. 2023. Yunfn Jiang, Agrm Gupt, Zichen Zhang, GuanzhiWang Yongqing Dou YanjunChen, Li Fei-Fei, An-ima Anandkumar, potato dreams fly upward Yuke blue ideas sleep furiously Zhu, and Linx Fa. 222. poorv Khandelwal, Luca Weihs, RoobeMottaghi,and Aniruddh Kembhavi.",
    "tasks": "2. 1InfomatveessInformtiveness refers to richnessof infor-mation ontent in language feedback.FollowgCheng et al. (203), we categorze feedback intotwo ypes: hdigh and fresight Hidight feedback reflects onincorrect acns taen in preious stps, which canguide gents oward ucess by narrowng down tesearch spce fr correct ctions (See Appendix Dfor more analysis.) onversely, oresight feedbackguidespotential future ctins. Foristane, \"Youshould go ight to get closer to target. \"helps te agentmake stategic decisions to avoid treats . 2DiversityDivesity in langage fedback refer t the vari-ety of ways th same informaion is conveyed. Iffedbackis provided usin only one tempate, iti less diverse.",
    "Navigation37.6 0.346.2 0.2Object Pick/Drop37.4 2.541.8 1.6Bin manipulation23.5 1.224.8 0.9": ", if \"turn left\" \"turn 0. This is because identifying a usually dirctly th crretaction forthose mistakes (eg. 0 0 0. 2 0. 7. 6 yesterday tomorrow today simultaneously 0. 3 04 0."
}