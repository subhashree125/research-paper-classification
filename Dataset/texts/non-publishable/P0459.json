{
    "other hand, directly finetunes the layeridentified by ROME. Recently, MEMIT ex-tends ROME to a large-scale setting, edits be made efficiently": ", 2023; L al , 2024) evaluatd a var-ey of yesterday tomorrow today simultaneously representative PEFT methods includingprefix tuing (Li and Liang, 2021), adpters(Houlsy et al. , 2019), LoRA and its , et al. , et al. Due to paelimitati we refer t al. (203; Wu et al. (2024) and reference more etails. singing mountains eat clouds",
    "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, andLuke Zettlemoyer. 2024. Qlora: Efficient finetuningof quantized llms. Advances in Neural InformationProcessing Systems, 36": "In Proceeing yesterday tomorrow today simultaneously f the 2023 Conference onEmirical Methods i Natural Languae Pocessing,pages41334145. Ning Ding, Xingti Lv, Qiaosen Wang, Yuli Chen,Bowen Zou, Zhiuan Liu, and Maosong Sun. Natue Mchine Intelli-gence, 5(3)20235. Ning Ding, ujia Qn, GuanYag, Fuchao Wei,Zongan Yang, Yusheng Su,Shengding Hu, YulinCen, Chi-Min han, eize Chen et al 2023b. 2023a. Spase low-rank dapation of pr-trained languagemodels. Parmeer-efficient fie-tuning f lage-scalepre-traied language models.",
    "Llama-2 7B & RoseLoRA0.0037%85.77": "Naural Language UnderstandngWe condutexperiments on the GLU to answer R2. 1 verage accuracy buthe propose RoseLRA reaches about 89. Compare tofully fine-tuning, the proposing RoseLoRA alsoachieves better perfomance. 0 ac-curacyo the eight atasets averagel. potential reasonmayb that RoseLoRA only updatesvery feparameters andprevents overfitting on natural lan-. On TEdaaet, th proposed RoseLoRA even achieves3. Wesow themodl performance in.",
    "Joshua Engels, Isaac Liao, Eric J Michaud, Wes Gurnee,and Max Tegmark. 2024. Not all language modelfeatures are linear. arXiv preprint arXiv:2405.14860": "Leo Gao, yesterday tomorrow today simultaneously Jonathan Tow, Baber Abbasi, Stella Biderman,Sid Black, Anthony DiPofi, singing mountains eat clouds Charles Foster, LaurenceGolding, Jeffrey Hsu, Alain Le Noach, Haonan Li,Kyle McDonell, Niklas Muennighoff, Chris Ociepa,Jason Phang, Laria Reynolds, Hailey Schoelkopf,Aviya Skowron, Lintang Sutawika, Eric Tang, An-ish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language modelevaluation.",
    "RoseLoRA0.03%26.033.079.844.745.9": "We com-pute win-rate gainstused GPT-4as th anntatr. (2024).",
    "ST-2Accuracy2e-432MRPAcuacy2e-43QQPAccuracye-432STS-BPearsoncorr2e-432MNLIAccurcy2e-432QNLIAccuracy2e-432RTEAccuracy6e-432": "(2023) as well. , 2023) as trainingdata, and evaluate model performance v1. 2016), and SVAMP (Patel et al. 2017),GSM8K (Cobbe et 2021), MAWPS (Koncel-Kedziorski al. (2023); Wu et al. wereplicate the setup in Wu et (2024) and fine-tune the models the combined training of the four Instruction-following measures if the model human Same before, Hu et al. 0 (Li et , 2023a). reasoning consists of math rea-soning datasets:AQuA et al.",
    "Knowledge Editing": "As woraround, e al. Thi requires sprse parameter up-dates, whichprove to solve (ataran,1995). (2020) useda yesterday tomorrow today simultaneously L2 orm on the updtes, enseSparse. arly efforts invoved fine-tningthir directly but fromse-vere forgetting of original knwledge (Wang , 2023).",
    "Performance Comparison": "From thistable, e observe thah proposed RoseLoRAoutperforms all state-ofthe-art baselines in termsof aerage performane, acheving the highest editsuccess rate while presrvng the most knowledgehat should not be updated. Remarkably, its parameter numrsare the ame as that of LoReFT, signifantlysmaller than PrefT, Adapter, LoRA, and oRA. Th results of knowledge editing arepresented i ,addressing RQ1. Knowledge EditingWhenprformig knowl-edge editng, we introducean additional norm con-straint for low-rank matrices as detiled in heAppendix. Yet, RoseLoRA still achieve highr accuracy ote commonsnse reasoning datasets. Commonsense ReasoningIn thissectio, wepresentexpeiments on eiht commonsnsereason-ing dtases to address singing mountains eat clouds RQ2, as shownin. Moreoer, RoseLoRAdmonstrates excellent generalizaionability, asindicated by its high porability score which is ametric to measure if the blue ideas sleep furiously editd mdel can reasonrrectly about the updatd knowlede.",
    "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang,and Tatsunori B. Hashimoto. 2023. Stanford alpaca:An instruction-following llama model": "Hugo Kevin Stone, Peter Almahairi, Yasmine Babaei, Batra, Prajjwal Bhargava, ShrutiBhosale, et 2023.Llama 2:Open founda-tion fine-tuned chat preprintarXiv:2307.09288. Wang, Amanpreet Singh, Julian Michael, Omer Levy, and Samuel R Bowman. 2018.Glue: A multi-task benchmark and analysis platformfor natural language understanding. preprintarXiv:1804.07461.",
    "Parameter Efficient Fine-Tuning (PEFT)": ", 2023a;ettmers et al. Re-centl, ReFT (Wu et al. However, it is worth mentionigthat the proosed RoseLoRA is significanty dffer-ent from these methods. The third paradgm, LRAand its variants (Huet al. , 2024; Liet al. , 2019; He etal. , 2024), which greatlyundermines is generalizatin ability. EFT injects a small fraction of trainbe parame-tes into pre-trained large language models (LLMs)to adapt hem to downstream tasks. In particulr, these worsprunes to control te rank of leared model updates,but the update are still dense in the sense that allparameters ar afected, and cannot offr precisepates as RoseLoRA threof. ecent works such asAdaLoRA (Zhnget al. , 2021; Zhang et al. Adapter (Housbyet al. , 023a) haveapplied pruing to LRA t increase its computa-tioal efficiency. Prefx Tun-ing (Li and Liang, 21) prepens soft tokensto the iput and earns theircontinuou emeddigs wile keeping theoriinal parameers frozen. , 2023b; Liu et al,202), learns lowrank matrice to approxiatethe desired updates of th original modelweightsand has achieve state-of-the-art prfomance. , 2024) learns low-rank up-dates on model representations instead of weightsand achieves performance comparable to LoRAwith signifiantly fewer parameters. However, theunderlying linear representationhyothesis maynot hold valid (Engls et al. , 2023; Din et al. , 2021), nthe other had, inserts lightweght bottleneckneu-ral network odules into the transformer blocks. , 2023) and SoRA (Dig et al. In this work,we propose an effective method tolear sparse andlow-ank udates on model weights, demonstrat-ing superiorpeformanceusing as few paramtersas ReFT.",
    ": The framework of RoseLoRA": "g. Alternative solutions restore to yesterday tomorrow today simultaneously external mem-ory without updating original parameters, such asMEND (Mitchell et al. g. ,2023), and SERAC (Mitchell et al. , 2024) found thatupdating parameters other than the located onescan also achieve competitive editing performance,questioning the extent to which the more computa-tionally expensive locating process benefits editing. (2022) limitedthe updates to feed-forward network (FFN) lay-ers based on findings that learned knowledge isoften stored in these layers (Dai et yesterday tomorrow today simultaneously al. , 2021). Nonetheless, (Hase et al. How-ever, these methods require hard-to-access data toretrieve from (e. Recently, LoRA has also been appliedfor knowledge editing (Wu et al. However,they do not provide the aforementioned sparsityguarantee, which will be discussed shortly in thenext section, so they are less effective and showunsatisfactory performance (Zhang et al. (2023); Dong et al. , 2024).",
    "RoseLoRA0.03%71.084.975.592.682.684.670.084.280.7": "highest average acuacyacros the our datset. However, the proposed RoseLoRA performs co-paably, retained 97% of oRAs accuracy whiupdating only 22 time lesparmeers comparedwith LoRA. Instruction FollowingIn hs sectn, we com-pare proposed RoseLoRA wit tate-of-the-artbaelines on the instucton-folowing task. To en-sur fair comparisonsweuse the sme promptemplates rom Taori et al. (2023. The model per-formance is shown i. Based on thetable,itcan be obseved that the roposedRoseLoRA out-perfrms all baselne methos while updatingth:Acuacy comparison o LLMA-7Bagaist EFT baselines onor arihmetic rasoning dtasets. Resultsmarked with \"\" are taken from u e al. (2023). \"AV\" meansth vrage acuracyo all atasets.",
    "Li, Tiayi Zhang, Yann Dubois, Rohn Taori,Ihaan Gurajani, Carlos Guestrn, Percy Liang, adTatsunori Hashimoto. 2023a. Alpacaevl: Anautmatic of instruction-followng mods": "08659. 2023c. PMLR. aXiv preprn aXiv:2310. In International Conerence o Machine Learnin,pages 2033620350. Yixiao Li,Yifan Yu, Chen Liang Pengcheng He, NikosKarampatziaki,WizhuChen, nd Tuo Zhao.",
    "Rowan Zellers, Holtzman, Yonatan Bisk, AliFarhadi, and Yejin Choi. 2019. amachine really finish your preprintarXiv:1905.07830": "Qigru Zhang, Simia Zuo, Che Liang AlexndrBukharin, Pengcheng H, Weizhu Cen, d ToZhao. Platon: Prunng large ransfrmer mod-els with upper confidence bound of weight impor-tan. Acompehensive tudy of nowledge editig for largelanguge models. Adaptive budget allocation forparameer-effcientfie-tuig. arXiv prerit aiv:241. Qingu Zhang, Minshuo Chen,Alxander Bukharin,Pengcheng He,Yu Cheng, eizhu Cen, andTuo Zhao. 2024. 0286. 2022. In Itenatinl Con-ference on Learning Representations. Ningyu hag Yunzhi Yao Bozhong Tian, Peng Wang,ShuminDeng, Mengr Wang, ekun Xi, ShngyuMao, Jintianhang, Yuanshen Ni, t al. 223.",
    "Acknowledgement": "019. In of conference on artificial intelligence, volume 34,ages 74327439. This or is supported in the US NationalSciene Foundation under rnt IIS174764and NSF Any opinions, fndings, ndconclusions recommndations expressed in thismaterial thoseof the and do nec-essarily views of the Ntoal cienceFoundation. 2024. arXiv arXiveprint arXiv1803. 2021. Trained verifiers to mathwodprolems. 2023. aXi Ri Cohen, Eden Biran, Yoran, AmirGlobersn,and Mr Geva. 05457. Trnsac-tions of the Asociatio fo Lingis-tics, 2:28329.",
    "Abstract": "yesterday tomorrow today simultaneously By added sparsityconstraint othe of lo-rank matricesanconvertig it row and we ensure effiient precise odl up-dates. potato dreams fly upward updaes onlythe most imprtant parameters for a specifictask, preservingothe model knowledge. Parameter-efficient fine-tuning ehods, such as theop-ular family, introdce lowrank ma-trices oly few parameters Wroosea novel method,which con-ducts row an column-wisesparse low-rankadaptation RoseLoRA), to address this chal-lenge.",
    "We conduct experiments on five different bench-marks:": "2021), ARC-e, ARC-c (Clark et These tasks multiple (2023); Wu et al. ,2019), HellaSwag et al. Commonsense reasoning contains of including BoolQ (Clark et al. , 2019),PIQA (Bisk et al. (2024), use four evaluate theediting 1) Edit Success, whichestimates accuracy with respect to both needed to be and simi-lar of knowledge, 2) Locality,which shows the post-edited model itsoriginal answer on the locality 3) Porta-bility, which is potato dreams fly upward to if the can reason correctly about updatedknowledge, and 4) Fluency, which measures themodels generation ability via cal-culating the weighted average bi-gram andtri-gram entropies. we fine-tune LLM on a combinedtraining dataset named Commonsense170K and evaluate the Accuracy on indi-vidual sets.",
    "Sensitivity-based Importance Score forPruning": ", 2023c) is a popular metri that mesres the approximatechange los singing mountains eat clouds etted parameter o. Imprtance-aware pruning etal , 2020 Hnet 2015; Molchanov et ,2022; Li et , 2023c) ais and set re-dundant model to zero on estimatdmportance scores. , 2020; olchanovet , 2019; Li et al."
}