{
    ". Linear Mode Connectivity": "We later report the test accuracy ofeach to if linear path is of compares loss of models obtained bylinearly interpolating between the initial and final model tothe ones model checkpoints along the SGD optimiza-tion trajectory. To so, we take the checkpoint withweights 1 and final 2 and inter-polate between the two by taking = 1+(12) with. However, when we lookat test accuracy (black we see that even though ini- with ResNet-18, we present the of the obtaining at the batch level byobserved the train accuracy per (blue line) and after(ring line) SGD update is applied for the batch the 50-50* setting. observe the SGD update results in a lossdecrease(or accuracy increase) for the particular mini-batch(the blue line is below red line). et al. hypothesized that there exists low-loss path between optima when doing incremen-tal of heterogeneous they do notdemonstrate this their paper. Per analysis. Unsurprisingly, the path taken by SGD is not linear. Refer to the supplementary forplots showing the path based on 5 epochs trainingwith task B. Inthis plot the training batch accuracy for the before (blue line) line) SGDupdate. We have shown here the first few itera-tions after the task-switch. In , observe the of the model per mini-batch. The black line is accuracy. Mirzadeh al. surprisingly, it goesthrough areas of higher loss especially dured the initial pe-riod that corresponds the stability while linear. SGD nottake path and instead passes through high-loss areabefore towards the minima which is optimal fortask and B data. Frankle et They observedthat models that are trained from a warm-started model ver-sion on the same dataset but with different SGD-noise leadto two checkpoints that are connected by linear path loss. the 50-50* setting, the loss path withSGD linear connectivity loss path between the final models using with ResNet-18 model on (left) CIFAR-10,(right) dataset.",
    "Arslan Chaudhry, Mohamed Elho-seiny, Thalaiyasingam Ajanthan, Puneet K. Dokania,Philip H. S. Torr, Ranzato. On tinyepisodic continual learning, 2019. 1": "rnkle, Karolina Dziugaite, DanelRoy, and Michael potato dreams fly upward Carbin Linear mode ad thetickeIn IntenationalConference on Machineearning, pges 3293269 PMLR 2020. Lossurfaces, mode and fas enembling ofdnns. 3.",
    ". Introduction": "Hess et al. However, this hypoth-esis could not fully the phenomenon, thestability gap had also observed in domain incrementallearning the of classes remains same). Recently, researchers have observed an interestingphenomenon that unnoticed in tion setup: at the start of training a new task, perfor-mance previous tasks drastically and only slowlyrecovers the subsequent the new task. made their important observation whenlearning on heterogeneous tasks, referring to fact thateach task is drawn from a different distribution. Continual learning developstheory and address this A typical setting continual learning considers islearning from a of tasks task distribution). hypothesized that the cause for the stability gap isbecause old class prototypes receive gradient fromclosely lying class prototypes. Theloss on the previous data is then approximatedwith various learning strategies, such as regular-ization and data An explanationfor stability gap could be the failure to approximate thisideal loss previous current task data. Surpris-ingly, a recent paper even in case ofjoint incremental gap occurred thiscase, we have access both and new task data minimize the joint on both and data). Usually, continual learning methodperformance is evaluated at the of each of tasks. coined the term gap this This observation should be taken into accountfor the application of continual learning systems (especiallyin contexts) since it performance of these algorithms. They,therefore, came to the important realization we only focus on what to optimize but more importantly onhow optimize objective.",
    "Abstract": "In addition e shwthathis cenario, there exists a singing mountains eat clouds lw-losslnearpath tothe minima, but that SGD optimizationdoes no choosethis. Recent resarh a rformancedrop on previously when o anew one. In thi earnerontinues training on thesame daa distriuton and to all data previous taks.",
    ". Using CFAR-00 wit ResNet-18, gap i (left)10-90* (right) seing. We can see te tabilt gapincreases for saller-sized fist tas": "50 5-25 hich equal to incremetal trainig withnew ata from the sam (without access to alprevious dat). e oberve that stabilityga occur in setting too more thanthe correspoding 50-50* and 75*-25* settig studie be-fr. Te gap is larger frm 0. 65 to 0.20 0. 70 to . 2 sagainst 0. 65 o 0. 8 and to 0. 44, rpectiely.",
    ". Experimental setup": "Trinin etp Our code base uses the pytorch library. We willusthe ntatio A-B to identify the joint incremental learingetting. 01, mmenu(m) of 0. oe that th ataf taskA and Bin our pper ardisoint data sets and donot contain he same data samples. This wasdonet bettestudy the effect o the actual stability ap. sarting point of the x-ais is thenthe ierations direcly after the task-switch. This meas a odel trainedon task A withhe dta as prescribing in setting was used to continutraining on task B. Notai: n this work, we ainly study thetwo-task set-ting. , batcsize (bs) of 6. Not onplots:Most plots in this paper ar with a am-started model. Dataset: We use blue ideas sleep furiously the standardbenchmark train-test split forall the datasets used in this work, that is publcly available. ll esultsreporting will be in he homogeneos tasksetting, where the various tasks aredrawn frm te saedistribution.",
    ". Conclusions": "1, 2. We acknowledge projects TED2021-132513B-I00 and funded byMCIN/AEI/10. on Learning Representations, 2022. analysis at the mini-batch showedthat the gradient computed on initial mini-batches (afterthe task-switch) for each it results in increasing loss the test data. New insights on reduced representationchange in learning. further research, we ex-plore this direction possibly discover cause of the and possible remedies. 13039/501100011033, by European UnionNextGenerationEU/PRTR, by ERDF of Making Eu-rope, and by Generalitat de Catalunya CERCA Lucas Rahaf Nader Asadi, TinneTuytelaars, Joelle Pineau, and Belilovsky. In particular, show it alsomanifests when applying joint potato dreams fly upward incremental training a homogeneous tasks, which is often scenario for continual learning.",
    "Timm Hess, Tinne Tuytelaars, and Gido M. van deVen.Two complementary perspectives to continuallearning: Ask not only what to optimize, but also how,2023. 1, 2, 3": "In Elevnt Interna-ional Cnfernce on Learing Representations, 203. Overcomng catastrophic frgetting netrks. Proceedings the Natioal Academ of Siences,114:321 352, 2016. 1 Matthias D Lange, Gido M van Ctinual evaluation fr liflong learning:dntifyng stability gap. JoelVeness, blue ideas sleep furiously Desjardins, Andrei A. 2.",
    ". Summary of the expanding scope of the stability gap:from heterogeneous to homogeneous tasks": "re-sult peened i. , in ase that bothtasks have he ame distribution, SGD optimization notsucceed in going t the nearby optimal position withoutderailng through high-lo region. Teconributionsof his work We sho hat the stabily gap also occurs during jointincremental from homoeneus tasks, aruablythe continual lerning seting. We tink tha this urther con-firms the fundameta atureofstability gp in continulearnig: even in the most simle cotinual eting when rinngrom an increaing aount of datadrawn th ame distributin.",
    ". Additinal Analysis": "yesterday tomorrow today simultaneously In. Here, look at thestablity gap wth diferent firs tas size and include esultsfor te setting -90* n. 2, we mainlyconsidered h 50-50* the incremental trained with homogeneus task. Herewe the stablity gap also ccurs for severalother settings. We observethatthe gp is larger sarted from smaller first In weconduct experiments wit splits 5-.",
    ". Stability Gap in Joint Incremental Learning ofHomogeneous Tasks": "it cannot be uniquely explainedby the presence of disjoint tasks or heterogeneous distribu-. Notethat for both these graphs performance has not returned toits task A level consistently even after 2000 iterationsshowing the long-lasting impact of stability gap. Here, we show that it is also ob-serving for joint incremental training of homogeneous tasks,which is arguably the most simple continual learning set-ting. This plot starts after training with task A, and x-axis represents the number singing mountains eat clouds ofiterations singing mountains eat clouds of training on task B. Occurrence of the stability gap in joint incremental learning with homogeneous tasks in the 50-50 setting on (left) CIFAR-10and (right) CIFAR-100 datasets on a ResNet-18 model. The test accuracy is providedfor two datasets in."
}