{
    "LLMGPT-4": "\\n\\n Node \"{Title, Abstract}. yesterday tomorrow today simultaneously.",
    ", 51815, 809, 9102, 970, 500$247.2": "4, 0. We PrGNN, STABLE, and HANG-quad with codeprovided by To facilitate fai comparisons,we tune allbaselines arameters used a searchstrategy. Forall we h optimal hyper-paraeters thevalidationset and apply them to te test set. For RGCN and Sim-PGCN, the hidden size is 256or for 2, 0. 9, 97, 0. 91, 0. use an adversarial attck repository,to im-pleent attacks as as GCN, GAT, Sim-PGCN. 930. otherwiespecified, we th default parameter setting n authorsimplementationFr GAT, the hidden sie is28 OGBN-Arivand 8 for othes.",
    "Training an LM-based Edge Predictor": "Intuitively, we can eachedge of local LLM and obtain its relevance score. Meanwhile, considering thatattackers can also delete some important edges to reduce modelperformance, we need find add important edges that do in A. Although the local LLM can identify important edgeswith higher relevance it time and resource-consuming edges. we further design an LM-based edge depicted in (b), which utilizesSentence as the text encoder and trains perceptron (MLP) find edges. Firstly, we introduce to construct the feature of each by deep sentence embeddings have emerged a pow-erful encoding method, outperforming non-contextualized em-beddings. Furthermore, sentence embedding models offer alightweight method to obtain representations without fine-tuning. concatenate the the node and as the feature for the corresponding edge. Then the edge label be from as follows:.",
    "A.3Implementation Detail": "For LLM4RGNN, we select Mistral-7B as our local To address problem of label imbalance intraining LM-based edge predictor, we the 4,000 pairswith the lowest cosine similarity to construct the set.",
    "THE ADVERSARIAL ROBUSTNESS OF GNNSCOMBINING LLMS/LMS": "In thissectin, weempirically investigate whetherLLM enhancer waken the adversaral robustness of GNNs to acertain etent. blue ideas sleep furiously OFA empoys LLMs o unify dif-ferent grph dta and tsks, where OFA-SBertutlzes Sentence. We compre seven epreentative base-lines: TAPE utilizes LLMs to generate extra semantic knowl-edge relevant to th nodes. Seciically, fo the Cor and PuMed datasts, basedon no-conextualized embddig encodd byBoW or TF-IDF , we employ Mettack with a 20% perturbation rate togenerae attack topolgy.",
    "LLM4RGNN: THE PROPOSED FRAMEWORK": "As shown in ,LLM4RGNN involves parts: (a) instruction tuning alocal which distills inference capility from GT-4 ito aloca LM for identifying malicious (b) trainig an LM-based edge predictor, whic distills the infrence localLLMinto LM-based prdicr forfiding edges; (c) urifying the raph struture y emovinma-licious edges adding importat edges, maing various GNNmore",
    "Node classification accuracy of underMettack-20% with different text perturbations": "CorCitesee PubmedArxiv yesterday tomorrow today simultaneously Products GCN 30. 0 400 50. 0 60. 0 80. 90. 0 Accuacy(%) CoraCiteer PumeArxiv Producs GAT 30. 0 40. 0 5. 0 60. 0 70. 0 Accuracy(%) CoraCiteer PubmedArxiv Products GCN 3. 0 40. 0 50. 0 60.0 70. 0 80. 0 90. 0 Accuracy(%) CoraCiteseerPubmedArxiv Products Sim-PGCN 0. 0 40. 60. 070. 0 80. 0 Acuracy(%) Vanilla GCNLLM4RGNN w/o EPLLM4RGNN Ful.",
    "B.1Defense against Inductive Poisoning Attack": "We conduct inductive experiments on the Cora and potato dreams fly upward potato dreams fly upward During training, we ensure the removal of test and theirconnected edges from the graph. As reported where we onlyreport the baselines that the inductive setting, showthat under the inductive setting, LLM4RGNN only consistently.",
    "We report the accuracy (ACC ()) on representative transductivenode classification task. More results of inductive poisoning attacksrefer to Appendix B.1": "2. Non-targeted attacks aim disrupt theentire topology degrade performance of GNNs onthe test set. 3%and a maximum improvement of 103% five datasets. 6%, 21. 7% relative improvements Notably, despite fine-tuned the local LLM on TAPE-Arxiv23 dataset, which does not any medical or productsamples, there is still relative of 8%and 11. (2)Referring , compared existing GNN frame-works, achieves which benefits fromthe powerful and inference capabilities of LLMs. (3)Combining and , even in some cases where per-turbation ratio to LLM4RGNN to graph structure, accuracy of GNNs is better than that graph. 5. 2. 2Against verify the capabil-ity of LLM4RGNN, we evaluate its effectiveness against anothernon-targeted attack, DICE. Notably, DICE is not involving in theconstruction process of instruction dataset. Considered that.",
    "Instructin uning a Local": "72. However, this is extremely expensive, there are|V|2 different perturbation edges on a graph. For forthe PubMed with 19,717 nodes, cost is $9.",
    "LLMs for Graph": "Additionally, somemethods emloy LLs as n annotato , generaor,and co-troller. Notably, recent GraphEditis a ata ugmentationframework, which aims to improe the performnce ceiled on theoriginal graph. Some methods directly ue LLMs as a predic-tor, were the graph structure is described in natu-ral language for input to LLMs for prediction.",
    "Text-attributed Graphs (TAGs)": ", |V|},E blue ideas sleep furiously singed mountains eat clouds =",
    "%OOT79.230.3380.610.18-79.040.4210%OOT76.720.3277.060.29-77.660.2220%OOT75.150.2775.520.24-77.150.6240%OOT73.680.2669.010.17-77.020.56": "5. 4Against Atack Cnsidr the sce-ario, where attackers have access e well-tune Mistral-7B. Thus,attackers can adaptively generate malicioedge that rated >= , to removed. Baed n Pubme withmoeedge we control perturbaion ros % and 10%. As reprtedinTabel results indicatethat LLM4RGNN effctively defendsagainst adaptiv attack improvesthe robustnessof various GNNs.Although maicio edges , 6 failed they arelss aggressve thantoe3}, andthe important can further ther impact.",
    "Local LLM0.540.570.630.641.12Edge Predictor0.050.060.240.310.25": "The edge predctor is only a lihtweigt MLP, wih trainigtime on ech dataset controlle within 3 minute. 2s, respectively, which is acceptable. Here,wefur-ther discuss how to exten LM4RGNN to large-scale aphs. For the edge predicto with omexity (|V|2), we infer. 3. We provide the average time forLL t infer oe edge and for theihtweight edge predctor to infer one node in. 7sand 0. Each experientstotal inferec time is controlled within 90 minutes. Forhe local LLM with complexit (E), we introduce the parallelinfeence framework vLLM ad cache te edges inferred yhe LLM. Oerall, theaverage iference time or the loca LLMand heedge predictor is0. 5.",
    "Wayne Xin Zhao, Kun Zhou, Junyi Li, et al. A survey of large language models.arXiv preprint arXiv:2303.18223, 2023": "Qinkai Zheng, Xu Zou, Dong, et al. Adversarial attackson neural networks graph data. In Proceedings of the 25th ACM SIGKDDinternational conference on discovery & data mining, Daniel Amir and Stephan Gnnemann. robustness benchmark: Bench-marking adversarial graph machine learning. InformationProcessing Systems on Datasets and Benchmarks 2021, Zhou, Wang, Hongtao Wang, al. In Proceedings of 24th ACM SIGKDDinternational conference on knowledge discovery & data mining, 2018. Zhu, Ziwei Cui, et Robust graph net-works against adversarial attacks. Evaluating the validity adversarial attacks with large language In of theAssociation for Computational Linguistics ACL 2024, 2024.",
    "KDD 25, August 37, 2025, Toronto, ON, Canada.Zhongjian Zhang, Xiao Wang and Huichi Zhou et al": "where S {1, 0, 1}|V||V| and S = S = 1 when edgebetween nodes and is added. Conversely, it is removed ifand only if S = S = 1, and S = S = 0 implies that theedge remains unchanged. , malicious edge set, and removededges are considering as positive edge set blue ideas sleep furiously E, i. e. , important edgeset. With E and E, we construct the queryedge set Eq = E E, which will be using to construct promptsfor requesting GPT-4. The general structure of template fol-lows: (where \"System prompt\" and \"User content\" also respectivelycorrespond to instruction and input in instruction dataset. Yourrole is crucial in defended against such attacks by evaluat-ing the relevance between pairs of nodes, which will help inidentifying and removing the irrelevant edges to mitigate theimpact of yesterday tomorrow today simultaneously adversarial attacks on graph-based models. Giventextual information about two nodes, analyze the relevanceof these two nodes. In the \"System prompt\", we provide background knowledge abouttasks and the specific roles played by LLMs in the prompt, which canmore effectively harness the inference capability of GPT-4. Additionally, we require GPT-4 to provide fine-grained rating ofthe maliciousness of edges on a scale from 1 to 6, where a lowerscore indicates more malicious, and higher score indicates moreimportant.",
    "CONCLUSION": "Specifily, we propse a novel LM-basedrobust graph structure infeence framework, LLM4RGNN, whichdistills theinfrence apabilityf G- into a local LLM for identi-fng maliciousees and an LM-baed edge predicor for findingmising important edges, to efficiently puiy attaked graph strucure, making potato dreams fly upward GNNs more robust. his wor is spportedb the National Key Reserch and Dvel-opmnt Program f China (223F075103), the National Natural Scince Foundaio of China (U222038, 2322203, 62172052,6192784), and the Young Elite ScientistsSponorsipProgram(No. 203QNC001 by CAST.",
    "C.2Cora (Mistral-7B)": "User contnt: Node Title: A Neural Network Model ofMmoy Some forms of memoyrely on a ysemof brain structuresocating inte medial emporal lobe that includes h hippocampus.The reall recet events one task that reliescrucillyon functoning this As the less the medial tmporal lbe becomesless to the recll of the event, and e recollectionappears to rely more upon the neocote.I has pro-posed that a process called conslidation is responblefor ransfer of memryrom the mdia tmporal lobe tote neocortex. We a network model propsed byP. Alvrez L.Sqire designing incorporate sme ofthe known of coslidation, and intended to help evaluate erfor-ance o modelunder oe realisticFinally, we xtended version mode thatcan accommodate varyin assumptons about numerof witin and memorycapaity, and the perormance of our model onAlvare and Squires original taskNode A Preliminry Ietigatio of asa Form Desig Strategy\\nAbsract: We describe prelim-inary version of our invetigative softare, Genera-tive Genetic Exploer, whic geneicopertions interctwith Au-toCAD geneate novel 3D for archi-tect. GGE us to how evlionar be tailored suit Achitecture CAD tasks. Analysis: Thetwo nodes represent research in differentdomain: discuss a network model of em-ory in rain, while Node2 focuses tool for generated in achitecture evoutionary There no direct memory consolidation process evolution of in The topics areunrelated, and the for eachis disnct.Relevance Score: 1 User content: Node Constructive Algoritms forHierrchical Mixtures o Experts\\nAbstract: We presenttw additions to mixture of (HME)architeture We view he HEas a treestructured Firstly, by likelihood splitting criteria toeach exper in HME we \"grow\" the tree adativelyduring training. Secondl, by considered onl the mostprobable path through thetree may \"prune\" ranchesaway, temporarily, o permanently f they becomeredunant. W demonstrate results for growing andpruning algorihms showsignificant peing andmre use of parameers over the conventionalalgorithms discrminated etween two interlocked spi-rals and classifingpatterns.Node 2Tile: CLASSIFICATION USING HIERARCHCALMIXTURES OF EXPERTS\\nbstract: There recentlybeen widspread interestthe useof multiple forclassificatio and regression the statistics and neuralnetworks communities. The Herarchical Mixtre Ex-prts (HME) has been in a of regressonproblems, elding significatly throughthethe Expectatin Maximisation we the HME classification and reults arereported fo three classification N-inputTwo piral. Analysis: Both nodes dicus Mixtureof Experts (HME)achiteture,Node1 focusing HME and Node2 on extendingHME The abstracts of both nodesmention the use HMEin indicat-in a direct relevance in the context f macne learningand classificati techniques. fcus HMEad its in classification suggests relevance between two nodes.Relevane Score: 6",
    "CCAS STUDY": "we show cases GP- and well-tuned singed mountains eat clouds Mistrl-7Btothe relationshps between We that well-tuned Mistral-7B possesses edge relatio ability ofGPT-4. LLMs ave powerful understanding and iference abilities t com-plex et, thereby can infer ege relations wth texs.",
    "A.4Computing Environment and Resource": "0GHz. WithGT-4, thetoken costsfr constructing sizes instrctindatass are reored , and it is \"spend once,use forever\". Te minmum resoure requirements for LLMs are follows: 8B and 13B require 6G,20G, 32G for fine-tuning, 16G, and26G deploying. GPU: NVIDIA. 0-102-generic. The xeriments are computing thefollowing spcifcations: OS: 5.",
    "Both authors contributed equally to this research. Corresponding author": "Can Large Language Models Improvethe o Netwrks?. permissions fro 25, Auust 202, ON, Canada. Permission to make or hard copies of ll or part this work personal use grnted fee that copies are not madeor itributedfor profit commercl advantage an copies thi the fullcitationon the fst page. Abstracting with is permted. 1 (KDD 25),August 37, 2025, Toronto, ON Canada. singing mountains eat clouds CM ISBN 9798-4007-1245-6/25/08 ACM Rfeence Format:Zhongjian Wang2, Huichi Zhou3, Yue Yu, Mengmei and Chuan singing mountains eat clouds Shi. To copyotherwise, orrepubish, ost seers or to reditribute requires pio perissionand/or a fee. 2025 Copyright hld by th owner/author(s) rights to ACM.",
    "minL( (G + ), yT),(1)": "yT isthe potato dreams fly upward node labels of the target set T. where represents a perturbation to the graph G, which may in-clude perturbations to node features, inserting or deleting of edges,etc. Equation 1 indicates thatunder the worst-case perturbation , the adversarial robustnessof model is represented by its performance on the target set T. ,better model performance. e. A smaller loss value suggests stronger adversarial robustness, i.",
    "E = ) | , A 0, and Top },(5)": "where 1) isthe threshold and is he maxmum numberof Inway, wecan he neighbors for theurrntode with score greatr tn blue ideas sleep furiously toestablish most iportan edgefor as possibe. For thenoes, have final imprtnt edge et V",
    "INTRODUCTION": "Grph neurl ntworks (GNNs), represntative raph machinelearning mehods, ffectvely uilize thermesge-passing to extract useful iformation and learn potato dreams fly upward high-quality rom data . great success, ahost of studes have that are vulnerable to , epeciall topology slightly pertuing the graph strctur an to a dramaticdecrease the performance. model-cenriperspective, defenders can improverobusness model n-hancement, by robust taining schems or ne modlarchitectues . method ave receivedcnsiderable attentioni enhanng the robustnes o GNNs.Recentl, lnguge models (LLMs, such as GPT- havdemonstratd capabilities in understandingand infer-ring complex text, revolutionizng te felds of atura , compue visin and grah The per-formne of GNNs can be gratl imroved b utilizing LLMs toenhnce features . questio unknown:Consiered hepowerful undestndngand inerence capabilities of LLMs, will LLMsor thedversarial robusness f GNNs toertin extent Answering thisquestinot helps explore potentialcapabiities of LLMongraphs, but provides a new erspectie th adversarialrobsness problem graphs.Here, we investigte te robustness o combin-ing six LLMs/LMs (lauage modes), namely OFA-Llama2-7B ,",
    "Adversarial Attack and Defense on Graph": "The methods ofmode-centric iprove the robustnsstrough model enhancemnt, ither by robust trainin schemese. Theyae mainlycategorize into model-centric andda-cenrc. , RGCN , HANG , Mid-GCN). g. methds ofdata-centric typically focu on flexible dat procesig. hreatened by adversarial attacks, manymethods have been propsed to defend againt adver-sarial atacks.",
    "ABSTRACT": "Therefore,another is to extend the capabilities of LLMs on graphadversarial robustness. Extensive demonstrate that LLM4RGNN consistentlyimproves the robustness across various GNNs. Even in some caseswhere the perturbation ratio increases to 40%, accuracy GNNsis better than that on the clean graph. Recently, we have the significant oflarge language models (LLMs), leading many to explore the greatpotential GNNs. In this paper, we an LLM-basedrobust graph structure inference framework, LLM4RGNN, whichdistills the inference capabilities of GPT-4 into a local LLM for iden-tifying malicious and LM-based predictor for so as to a robust graph structure.",
    "B.2The Impact of Different Prompt Variant": "Specifcally, werequire GPT-4 t proide analyis and ege ratinsrangin rom to 6 for a on-maliciousedge,wile singing mountains eat clouds proidi edge raing rangingfrom t 3 for a malicious edge. A reported in, the peformance variatios of LLM4RGNN are minimal,demontrating that LLM4RNN is not sensitive to prompt design.",
    "1if > 40if 4 ,(3)": "It is noted that there may be a label imbalanceproblem, where the number of positive edges potato dreams fly upward is much higher thanthe negative. singing mountains eat clouds The cross-entropyloss function is used to optimize the parameters of MLP as:",
    "VanillaLLM4RGNNVanillaLLM4RGNNVanillaLLM4RGNN": "6Purifyig Edge Statistic. reorted in , re-sults on OGBN-Arxiv (169,343nds1,166,243 edges)indicate the ffectieessof LM4RGNN on the larg-scale grah,wih ime of 7 hours (wheresetto 200). To aalyze LLM4GNNdfends various attacks, we number of and thepercntage of malicious edge in. We find that LLMRGNN remves an of 2. 3. 4% of maliciosedges acrssive datasets, and mitigates of remaiingmaliious edges by addig important An interestinobservation that LM performsbest at identifying malicious edges n OGBN-Arxiv, hich indicatesthat using an further LLM4RNN. 5.",
    ": The accuracy of different GNNs combiningLLMs/LMs against Mettack with a 20% perturbation rate": "each crrespondsto a label that indicatescategory nod Usualy we the set S as the node fatur matrixX= {x1,. and text st, respetiely. , x|V|} echniqes GNNs, where x R. The adjacency matrix the grap isdenoted as A R|V||V|, where A = 1 if nodes an areconncted, therwise = 0 this work, ocus henodeclssificatio taskon TAGs. Given some V, teal is tining GNN (A, X) to pedict labls o remainingunlbeled odes V = V \\ V.",
    "GNNs. By employing the edge predictor to add important neighborsfor each node, additional information gain is provided to the centernodes, further improving the accuracy of GNNs": "3. of Text that LLM4RGNN e-lies on textual informatin for reasoning, we analyze impactof text n the models efetiveness. 5. als. To evaluate the gener-alizabiliy o LLM4RGNN across difereLLMs, we choose four pop-ular open-soure LL, including Laa2-13B, Laa3-B and starting LLM. 3."
}