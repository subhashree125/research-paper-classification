{
    ": Performance comparison of merging different Mistral-7B fine-tuned models on different datasets": "ods. achieve blue ideas sleep furiously the best per-formance, we use absolute performancescore and normalized average scoreto compare five methods. Using Different Architecture. We a different model forevaluation, and the result has been shown in. scores shown similar to LLaMA-2-7B: For WinoGrande, AGIEval, GSM8k, andMATH dataset, our MetaGPT scores 41. blue ideas sleep furiously 86, 03, 8, which outperforms existing meth-ods, dataset Weight Average, TaskArithmetic, and Ties Merging best and forMBPP dataset, DARE method achieves the highestscore.",
    "RegMeanO(TFW)O()Fisher MergeO(TFW)O()G-Task ArithmeticO(T TFW)O(SFW)AdaMergingO(TBP)O(SBP )Task ArithmeticO(1)O()Weight AverageO(1)O()MetaGPTO(1)O()": "For Adamergng, it sltaneoslyoads TLMs to optimiz,complexityisspace is: O(SBP). ward rocess. The for foward process and back propagation are by TFW, Tis th number of task, is the number parameters and Gis the grid (G = 100 means 100 girdsfrom0 to 1). For average, tas arithmetic, and MetaGPT,tey do ot need information, which smodel excusive. Extra information teand space compexity, and of current methods.",
    "Notation": "Let : X be a neural tak-ing inputs X and parameterized by a ofweights. We consider fine-tuning 0) different tasks, with eachtask consisted of triplet (D, ) , whereD = (Dtrain, Dval , Dtest) is training, valida-tion and test data of task L is the functionof task , and is the parameters fine-tunedon task based on the pre-training weight yesterday tomorrow today simultaneously 0.",
    "Overview": "In thissectin, we state motiation and nd singing mountains eat clouds solve step step. Allproofs and theorems re yesterday tomorrow today simultaneously proided in appendx.",
    "Conclusion": "Inthis paper, we have provided a nove modelmerging metod naed MetaGPT, an optimal askaithmetic algorithm unde the setting of efficiennd prtecting at prvacy, which is seciicallydesigned for LLs. By separatig thedata ad scaling coefficent term unde careful ap-poximation, the closed-form solution rovies anavenue for optimally achieving ask arithmeti n-der the setting of applicable t LLMs andwithoutusing anydata.",
    "Abstract": "yesterday tomorrow today simultaneously vnt of lrge language models (LMs)like GPT-4 te exploration omlti-task learned MTL), in wic singlemdelmonstrates prfcienyacros diverstasks.ask arithmetic ha emered as a cost-effectivepprach for MTL. I enables perfor-mance enhancment multple bdding their task ectors to apre-traine model. However, curet lackof method hat can achve optima prformance with low cmpuationaland data privacy, limtsthei ap-plication paper, we propoeModel Tas rithmetic fr GT-scale moels which for-malizes the objetieof mode intoa muli-tasklearnng framework, aiming tominimie loss diferee merging mdel and each individual askmodel.",
    "Task Vector Othogonality": "potato dreams fly upward Ilharco e al. (223); Yang et al. (2023b) have per-formed experiments to verifythis property for viion mdls. For LLMs, we also observe similarresls: thse task vectorsarealmost orthogonal toach other. The result has ben shwn in. 9 expected which verifies the property wehave used for our proof.",
    "Experiments": "I hi ection, conduct demon-state the f ourIn the firstsecton, demonsrat that our MetaGPT conss-tently achiees optimal averae datasets robust fr model series parameter sizes and arhiecturs. We condct experientsto demonstratehatour method is orthgonl and cnbe to irove aver-ge performance further.",
    "Prateek Yadav, Derek Tam, Leshem Choshen, Colin ARaffel, and Mohit Bansal. 2024. Ties-merging: Re-solving interference when merging models.Ad-vances in Neural Information Processing Systems,36": "Enneng Yang, Junwei Pan, Haibin Yu,Li ihua Chen, Lei Xiao, Jie singing mountains eat clouds Jan, Guib-ing Guo. 2023a.A task-awar rate approac t muti-task learning.InAI, voume 37, pages1074510753. EenZheni Wang, Li Shen, Shiwei Liu, Guib-ing Gu, Xingwei Wang, and Tao. Adative model merging for In The Twelfth Internation oLearningRepreentations.",
    "OpenAI. 2023.GPT-4 technical report.Preprint,arXiv:2303.08774": "Moel ratatouille: Recycling model for out-of-distribution generalization. 024 Task arithmeic in the tangentspace: editingof pre-trained Ad-vances in Neura Information Processing Alexanre Ram,Katik Janyu MatieuCord,Lon and David 202. In Internatiol Con-frence on Machine Larning, pes.",
    "Using Larger Model Size.We also test ourmethod using a larger model LLaMA-2-13B (Tou-": "vron t 2023). scores demon-strate that oAGIEval,and MBPP datasets,our outperforms Wino-Grand GSM8K, and HmanEva dataset, DARE,Weigt Averge and hieves thehighest score. Similarly, the average asolute average and method also other five methods. Intgrae wth Ties/DAREAs conflictsand parameters between task vectors,DARE (Yu et al., 2023) and Ties-Merged (Yadavet 2024) are two methodsrying to olve heinterfaces, educing the redundancy and thereby im-proved theof task rithmetic. Sinceour methodalso on the framework f tskarithmetic, Ties-merging and ARE exectdto the prformance of MetaGPT fr-ther. As we can see in, undr bslineof Ties-Merging and DARE etos, methodis t Ties-Mrgin DARE and into our MetaGPT, tus leading tofurther For the averagabsolute DR as improved",
    "Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, andAndrew Rabinovich. 2018. Gradnorm: Gradient nor-malization for adaptive loss balancing in deep multi-task networks. In ICML, pages 794803. PMLR": "Training to solve math ord prob-lems. In NeurIPS. prepint arXiv:110. 2020. Cobbe, Vineet Kosaraj, Bavarian,ark Chen, blue ideas sleep furiously HeewooJun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jaco Hilton Christopher Hess, and John chulman. 14168. Just pick a sin: Op-timizing deep multitask singing mountains eat clouds th signdropout.",
    ": MetaGPT can be integrated with DARE and Ties-Merging, thereby leading to further improvment": "20 to 31. 57. 26 to 1.",
    "=1(2)": "2, the task arithmetic intro-duces hyper-parameters {| = 1, ,} andthe choice of these scaling coefficients has a signif-icant influence on the performance of the mergedmodel. where is the scaling coefficient of task vector. Thus, selecting the appropriate scaling co-efficients for different task vectors remains a chal-lenging problem.",
    "Duxiaoman DI. 2023. Financeiq": "2021. Ding, Xin Dong, Yong Lei Cheng, Chilin Fu,Zhaoxin Huan, Hai Li, Yan, Liang Zhang, XiaoluZhang, et al. Jesse potato dreams fly upward singing mountains eat clouds Dodge, Gabriel Ilharco, Schwartz, AliFarhadi, Hannaneh Hajishirzi, and Noah Efficiently identifyingtask groupings for multi-task learning. Advances Information Systems,. pages 22372241.",
    "Arthur Jacot, Franck Gabriel, and Clment Hongler.2018. Neural tangent kernel: Convergence and gen-eralization in neural networks. Advances in neuralinformation processing systems, 31": "arXiv preprint What diseasedoes this patient have? large-scale domainquestion answering dataset from exams. 2023. Ap-plied 11(14):6421.",
    "= L (final, ) (, )": "average lossdifference all tasks isdefnedas. for ask it aims to peoranc of blue ideas sleep furiously the final model o llthe tasks.",
    "Out of Distribution Generalization": ", 2022), wealso compare the out-of-distribution generalizationability of different merging methods. All three datasets use 5-shotaccuracy as the evaluation metric. Following (Yang et al. As we can see,MetaGPT outperforms current methods on these un-seen datasets, which demonstrates that MetaGPT ismore robust to the test data distribution shifts. We evalu-ate different methods using JEC-QA (Zhong et al. , 2021) dataset. , 2023b; Jin et al. ,2020), FinanceIQ (DI, 2023), and MedQA blue ideas sleep furiously (Jinet al. sum-marizes out-of-distribution generalization perfor-mance when merging all domain specific mod-els using different methods.",
    "Rich 1997.Multitask learning.Machinelearning, 28:4175": "03374. Advances Neural InfrmationProcesing 34:220522418. generalzation byseeking flat mnim. Mark hn, Twork, Heewoo Jun, QimgYuan, Ponde d lveira Pinto, Jared Ka-plan, Harri Edwd Yuri icholas Brocman, AlxRay, Puri, GretchenKruegr, Michal Peto, laaf, Grish Sas-try, Paela Mishkin, Brooke Scott Ryer, Alehea Power, LukaszKaiser, Mohamad Bavarian Clemens Winter, Philippe Tllet, Felipe Petroski Such, Dave Cum-mings,Plppert Fotios Chantzis, Eiza-beth Barns,Ariel Herbert-Voss, illiaHebgenGuss Nichol, lex Pano, Nkolas JieTang Igr Babuschkin, Suhir Balaji, Shantu Saunders, Hse, AndrewNCarr, Jan Like, Achiam, edant EvanMoriaa, Alec Mathew Kniht Milesrundage, Mira Murati, atie cGrew, Dario cCandlish, IlyaSutskver, nd Wojciech Zreba. Preprint,arXiv:2107.",
    ": Out of Distribution Generalization": "time for forward andbackward processes denoted as FW and BP. ForRegMean, it requires the inner product matri-ces for input to calculate the updating parame-ters. Arith-metic (G-Task Arithmetic) requires O(T process to evaluate, where G is the (G = 100 means 100 girds from 0 to andT is the number of tasks.",
    "MergingModels Using MetaGPT": "Dataset and Modls. To test effectiveness potato dreams fly upward ofour method, we use Llama-2-7-chat-hf (Touvronet al. , 202),MAmmoTH7B (ue et al. Moreover, we use adiffeent model archiecture: Mistral-7B-Instruct-v0. , 024)and Mistral-7B-codeapaca-loa (Nondzu) as mod-els fn-tuning on generl knowledge, math, andcde datasts sin pre-traned mdel Mistral7B (Jiang t al., 202). Wealso provide xperi-ments using models with largersizes: Llama-2-13b-chat-hf(Touvron et al. , 2023), MAmmoTH-13B (Yue et l. 2023), and llama-2-13b-ce-cat TASAR, 023) s model fne-uned on en-erl knowledge, math,andcde datsets usingthe pre-trained model Llaa-2-13B-hf(Touvronet al. , 2023). Quantitatve Evalation for LLaMA-2-7B. Wese the metrics and datses we introducing abov toevaluate performance of different blue ideas sleep furiously methods. , 2023), Ties-Megin (Ya-dav et al. , 2023),which re also model exclusive and computation-ally efficient methods, to compare with our metodbymerging LLaMA-2-B.",
    "NTK Linearization": "(018) have proved hen thidth o te neural approaches infin-ity, i kernel yesterday tomorrow today simultaneously behavior and the op-timizationin th egime. Fromthe re-sults , we can see that all utputs arealmost linear with , which indicates that LMsdo kernel behavior during finetuning. , onAGIEal (Zhong et 2023) daaset verify have randomly otutsof Llama-2-7bchat-hf when in Eq. al. 8 getsvalueof , 1. Wetest Llama-2-7b-chat-hf (ouvron et al."
}