{
    "Broader Impacts": "yesterday tomorrow today simultaneously However, model is fine-tuned web data, As a result, model may not learn how to generate videos but alsoinadvertently learn societal biases present the web data, which may include Potential post-processing steps, such as applying video classifier to out undesirablecontent, could help mitigate issue.",
    "where WQ and WK are projection matrices for query and key, i {1, 2, ...M} is layer index,n {1, 2, ..., N} represents each head in multi-head cross-attention, and dn is the dimension of eachhead": "Thisaligns with the un-derstanding tha motion is a global concept and canot be captured by a sigle word. For instance,phrases like Ababy/dog is walkin/running forwardsbackward. \" blue ideas sleep furiously emonstrate that singing mountains eat clouds different combi-natons of words an esult insignificantly different mtions. This approach forms the bass of our text-motion loss defied as folows:.",
    "Wu, C., Liang, J., Ji, L., Yang, Y., Jiang, D., Duan, N\\\"UWA: Visual SynthesisPre-training for Neural visUal World (Nov 2021), arXiv:2111.12417 [cs]": "Z. , Wang, X. , Lei, W. , Gu, Y. , Shi, Y. , Hsu, W. , Shan, Y. , Shou, M. :Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation(Mar 2023), singing mountains eat clouds arXiv:2212. , singing mountains eat clouds Mei, T. , Yao, T. , Rui, Y. IEEE, Las Vegas, NV, USA(Jun 2016). Xue, H. , Hang, T. , Sun, Y. , Yang, H. , Fu, J. , Guo, B.",
    "Motion Representation": "Formally, given noisy video latent zt at tim step t nd a txtprompt p, the cross-attentionmaps AiRHiW iF S, whre Hi and W i are theeight and width o video latent atthe ithcross-attentin layer, F is te nuber of frames, S is te seuence length, fora cross-attention lyeriare defined as olos:. Asshown in righthand side, duing training, the Letmotion is usedto nhance motion encoding, theLreg isusd to avoid ctastrophc forgetting,the Lvdeo-motion is to enhance otn integrati. The cross-attentinoperatin ca be ieweds aprectio of tetiformation into he visual structure oman. Research has shown that crss-attention mapsrepresentthe structure of visual ntnt. With this understandig we aim to shift the texencders focus more tward motion information by constraning the tmpoal hnges of cross-attention mapsto closelymimicthse oserved in goud truthvideos,as ilurated by the red lin n. Text-Mtion Supervison. As shwn n he lef-hand sie, DEO corporatedal txt encoing and text conditioning (for simplcity, ohe layers in the UNet are omitted. Thenowfakes and flme denot frozn and trainable pareter,respeciely.",
    "liot Study Details": "Next, calculaed pairwise sentence imilarity within eachgroup using the eot] token to determine sentence similarity Thewithin eachgroup, as across different grops, was This setupgrups words with thesame PS under he same contex, hereby eliminating potentialbiases the contex.",
    "Kingma, D.P., Ba, J.: Adam: A Method for Stochastic Optimization (Jan 2017), arXiv:1412.6980 [cs]": "Kondratyuk, D. Gu, X. , J , Hung, , Hornung, , Akbari, H. Brokar, V. Y. C. Essa, . , Gpt,A. , , Hendon, . , Martinez, A. , , Schiner, G. , Sirotenko, M. , Son, K. , Yang, M. H , , Seybold, B. ,",
    "where , , and are scaling factors to balance different loss terms": "LaVieVid eoCr af t er 2 Mod elS c op eT 2 VDE MO Slow mot ion f lower p et als f all f r om a blossom, land ing sof t ly on t he gr ound. Full videos are available in thesupplementary materials. LaVieVid eoCr singing mountains eat clouds af t er 2 Mod elS c op eT 2 VDE MO LaVieVid potato dreams fly upward eoCr af t er 2 Mod elS c op eT 2 VDE MO : Qualitative Comparison. We display frames 1, 2,4, 6, 8, 10, 12, 14, 15, and 16, arranged in two rows from left to right. J oc keys r ac ing. Each video is generated with 16 frames.",
    "Esser, P., Rombach, R., Ommer, B.: Taming Transformers for High-Resolution Image Synthesis(Jun 2021), arXiv:2012.09841 [cs]": ", Liu, J. , T. blue ideas sleep furiously , Nah, S. Kang, X. , Catanzaro, , Jacobs, D. : AnImage is Worth One Text-to-Image using Inversion(Aug 2022), arXiv:2208. H. B. , Tao, A. Y. , Yu, K. C. Feng, M. DreaMoving: A Video on Diffusion Models 2023), [cs] , Alaluf, , Atzmon, Y. , Cui, , X. , Chechik, , Cohen-Or, D. Huang, J. , Li, X. Bermano, A. , Li,A. , Patashnik, O. , Hui, Z. , Lin, X. ,Balaji, Preserve A Noise Prior for Video Diffusion Models (Aug2023), arXiv:2305. H. 10474 [cs].",
    "Limitations and Future Work": "Dpite DEMs efficicy in enancing motio synthesis wihout relying on additional signals,itfacesgnificant challenge in generating ifferet motios sequentally, as illustrated in These challenges likel sem from the ext encoders difficuly incmprehedingthe orderof actionsand the motion generation models limited capabilit to generate different motions. A potentialsolution to his issue involves annoating each fame with a specific prompt and trining the model onvideo clips f varyig engtsrather than a fxed duration.",
    "oodfllo Zemba W., Cheug, V., Radford, A., Chen, X.:Im-proved Techniques for Tainin GANs (Jun 216), [cs]": "Ahual, GafniO. , Hu, Q. , An, J , Zhang, S. 10089 Sigr, U. Schuhmnn,  , Beaumont, R. ,Kaczmrczyk, R. Parikh, D. , , C. , Worsman, M. , Schraowki, P. ,. , Polak, A. Y. Parikh,, Tagman,Y. ,A. , Yin, X. , Wighman, R. H. : Mae-A-Video: Text-to-VideoGeneration withoutText-Video Data (Sep 2022, arXiv:220. Emu Edit: Image Editing Recognition and eneration Tasks(Nov arXi2311. , Crowson, K. A. Kndrthy, S. Jitsev, : LAIO-5B: open larg-scale for tainig nxt geneatioimage-tex models 2022), 08402 [c] Sheynin, Kirstain,Y.",
    "Wang, W., Yang, Z., H., Zhu, J., Fu, J., Liu, J.: VideoFactory: Swap Attentionin Diffusions for Generation (Jun 2023), arXiv:2305.10874": ", Huang, Z. :LAVIE: ih-Quality Video Generatonwith Cascaed Latent DiffuionModls 223), arXiv:309. , Yu, , ,Wu, T. Wang, Y. , Chen, C. , Y. C. , Si, C. [cs]. , Chen X. Zhou, S. , Dai,  Lin, D. , Lu, Z.",
    "Quaittive Evaluations": "In suection, we conut qualitativ copison among LaVie , ideCrafer ,ModelopeT2V , ad comparisn, we us same seing of comprative illstratedin , were we showcase generatedby these methods. Uon examiation, it s tat hese mdels are capable of producing hgh-quality video. However a notable distincton in thedynami representaion of motion withinthe In contrast, EMOsignificantly outperorms by essence of mtion, pdung videoth and to the ground. This unerscos theadvanced of to generat videos tht not only viually rpresen a scene ut alsodnamially the onoing moti.",
    "[cs.CV] 31 Oct 2024": "Moreover, DEM incoroatesnoveltt-motio advido-motion suervision techniquesto enhance the models understanngand generation of motion Text-motionsperviion aligs crs-atenti maps with the teporalchnges obsrved in groud truth videos, guiing the odel toous on mtioniformatio. DEMO achievessubstanial improvements in metrics elatd to motion dynamis and visual fidety,idicated itssupriorcapblity to gnerate videos tha are both visuallyapealing ad dynamically acurate. content conditioningmodule integaesspatial mbeddins into tvide generation process on a frame-by-fram basis, esured tat stac eleents ar accuratelydepicte in each ram. Addressing thfist halenge, DEMO domposes text enodig into conten encodig and moiending proesses. Rgardi e econd chaleng, DEMO decomposes the text conditioning poces int cntentand motion dimensions. While eseapproaches improve motio snthesi, they deend onexternal refernces r pre-triing models, wih ma not always be practical To address these challenges we introduce Decompsd Moon (DMO), novl framwork designedo enhac motio synthesis in T2 generatin. While effectivefor generating high-qualit static mages, thiaproach is insfficient for videos, where motin s acritical component that spansboth spatial andtemporal dimensions. Reent efforts o addres these chllenges ha inolved icorprating additional control signalssuch asketches , strokes , database sampls , depth maps , andhuman pose , referncevideos , and bunding bxes nto te T2Veneraton rocess. A holistic aproach that integrates extinformation acro these dinsions isessenial for generating videos wit realiti moti dyaics. These signal are deriving itherfro reference vides r pre-trained motioneneration models. \" Meanwhile, the mtio encoding apus th essence ofbject movement and tempraldnamics,trretin actions like waling\" and directional cues liketo the lef \" This separaionao the model to better understand and represent the dynamic asects of the descred scenes. In contrast, te motion conditionngmodle operats acrossth emporaldimension, infusing dynamic motion embeddings into the ideo. techniques to T2V tasks , applying text information thugh spatial cross-attenion on a ramby-frame basis. Vdeo-motio supervision onstrains the predited ideo latent to mim the motion pattrns of real videos,promoting the generation of cheren and ealistic motio dnamics To validate our framework, we ondut tensive experimens n everal bencmarks,ncluded MSRVTT , UCF101 WebVid-10M , EvalCrafer and Bench. Th separation eabes te model tocature andreproducecmplex motiopatterns escribing in the et.",
    "Motion Dynamics ()62.5063.7563.5068.90Human Action ()90.4090.4090.2090.60Temporal Flickering ()96.0296.3595.4594.63Motion Smoothness ()96.1996.3896.2296.09": "Thissuggests that, without specific supervision constraints, effect of the added textencoder parameters is marginal. However, DEMO model consistently outperforms all the effectiveness our method in improved video quality and text-videoalignment. Efficiency DEMO does not support created videos containing motionsspecified text. in example, standed in a kitchen and a mixer and yesterday tomorrow today simultaneously carton of milk are shown\", appear simultaneously.",
    "Chen, T.S., Lin, C.H., Tseng, H.Y., Lin, T.Y., Yang, M.H.: Motion-Conditioned DiffusionModel for Controllable Video Synthesis (Apr 2023), [cs]": "Dai, X. , Hou, J. , C. J. , Zhang, P. ,Dubey, , Yu, M. , A. , Radenovic, F. , Mahajan, D. , Zhao, Petrovic, , Singh,M. K. Wen, Y. Song, Y. , Sumbaly, R. , He, Z. , Parikh,D. : Emu: Enhancing Image Generation Models Using Photogenic Needles in Haystack arXiv:2309. 15807 [cs].",
    "Method": "The 3D U-net consists of on-sample,midle, and up-sple blocks. The spatial consists f self-attenio, spatilcros-attenton, and eed-forard lyers. Te emoral transformer conists of tempora self-attentionandfeed-forward layers. The -Net is trained wih a enodr t minimize in aent as follows:Ldiffsin = t,z,N (0,1),p[| (zt, t, (p))|22](1wher z thevideo corresponding to x in pel space, t is the potato dreams fly upward time step E a ecoder,p i text and is oise ampld fro Gaussian dstrbuio. LVDMs build on diffusion models braining a D Net as noise singing mountains eat clouds predictor, VQ-VAE oa VQ-AN is emloyedto cmpress intolow-dimensional laentspace. zt is 0 afertstesdiffusion forward process given. Eachof blocks comprises multipl convolton ayers augmentedby spata and tempoal transformes. Latent Video Models (LVDMs).",
    "Acknowledgement": "ThisworkispartiallysupportedbyShenzhenFundamentalResearchProgram(No.JCYJ20200109141235597), NSFC/RGC Collaborative Research Scheme (No. CRS_PolyU501/23),HK RGC Theme-basing Research Scheme (No. PolyU T43-513/23-N) and Research Grants Council ofthe Hong Kong Special Administrative Region, China (No. PolyU15205924). We also acknowledgethe support from Research Institute for Artificial Intelligence of Things, The Hong Kong PolytechnicUniversity, and Center for Computational Science and Engineering at Southern University of Scienceand Technology. Bain, M., Nagrani, A., Varol, G., Zisserman, A.:Frozen in Time:A Joint Videoand Image Encoder for End-to-End Retrieval. In: 2021 IEEE/CVF International Confer-ence on Computer Vision yesterday tomorrow today simultaneously (ICCV). pp. 17081718. IEEE, Montreal, QC, Canada (Oct2021). Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Zhang, Q., Kreis, K., Aittala, M., Aila,T., Laine, S., Catanzaro, B., Karras, T., Liu, M.Y.: eDiff-I: Text-to-Image Diffusion Modelswith an Ensemble of Expert Denoisers (Mar 2023), [cs] Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y.,English, Z., Voleti, V., Letts, potato dreams fly upward A., Jampani, V., Rombach, R., Ai, S.: Stable Video Diffusion:Scaling Latent Video Diffusion Models to Large Datasets",
    "In this section, we provide extended qualitative comparison between our method and the baseline": "Mod elS c yesterday tomorrow today simultaneously op eT 2 E MO LaVieVieoCr a t er 2 S u er ma shaking hand s wit h Sp ider man wit h t he st yle of water c olor. Aboat ac c eler at ing t o gain sp eed. A bear s cimbed a t r ee. We displayframes 1, 2, 4, 6, 8, 10, 12, 14,15,and 16, arranging in two rows fromleft to rigt. : Extending ualtative comparison. Mo elS op eT 2VDE O LaVied eoCr af t er 2 A gr een minec af t monst er c ar r is a un. Mod elS c op eT 2 VDE MO LaVieVid eoCr af t er 2 A lion is c at hing p r ey. : Extende qualitative comparison. Full videos areavailable in the supplemetary materials. Mod elS c op eT 2 VDE MO LaVieVid eoCr af t er2 A oast t ur ker y, cout er clo kwie Mod elS c op eT 2 VDE MO LaVieVideoCr af t er 2 App les and or anges, cloc kwise. We dispyframe 1, , 4, 6, 8, 10,12, 14, 15, an 16, araned in tworows from left to riht.",
    "ModelScopeT2V62.5090.4096.0296.19ModelScopeT2V fine-tuned63.7590.4096.3596.38DEMO68.9090.6094.6396.09": "Addiionally, we note deceases in tempral moton smoothness This observation aligns wth finding from the VBench paper, suggestthat motion dynamics can conflict with temporal and motion smoothnes. As shown in EMO improves dnamics from 62. Tis indicates tha hle urmoel enhaces the blue ideas sleep furiously ichness and complexity it limed benefi in imroving yesterday tomorrow today simultaneously o human action representation. Motion ynamcs, Hman Actio, Tempora Flicering, nd Motion Smothness.",
    "where is a function to extract motion dynamics from a video. In our we use flowto represent the motion (noting that flow is only during during": "Additionally, fo cros-attention at ifferent spatialresolution, we downsampethe round trthvideo to mtch potato dreams fly upward spatial resolutionf the ross-atteton mps. Heer singing mountains eat clouds directly fine-uning the CLP textncoder with quation 1 and Equation 5, which differ significantly from the original cntrasivelearned objecive, can easily lead to aastrophic forgetting. regularzation loss defined as flows:.",
    "Conclusion": "ocusing on these we aim to advance the resarch in this ynamic andapidly evolvig doain. this aper, have presente DEM, an innovative framework craed to advnce snthesisin generation. exteive evaluations across various bencmarks hav capability t signifcantly improve synthesis,shwcasing its withi In future work, we plan ugmt T2V atasets ith more detailed descriptions nd delve intoadvaced motion embedding techniques. To enouageour dlfocus and motion we prooe novel text-motion potato dreams fly upward andvieo-otion supervision.",
    "ModelIS ()Evaluation Protocol": "and justfy ou evauationFor ou ealuaion on MSRVTT, we basemodels approach to the CLIPSIM on the ntire MSRVTT dataset. For FID computation,CLP-ViT/B isued to extract he fme featurs. For and weranomly smple 2048videos, the MoelScopeTV papers methodology.our ealuation on UCF-101, toeiminat bis ntroduce template sentences s on several prvious works, we dirctly usthe clas to cmput th S and scores.",
    "ModelBase ModelTraining Dataset": "5M text-video datasetVideoFactory LDM HD-VG-130M + WebVid-10M EMU VIDEO Emu 34M licensed text-video datasetSVD Stable Diffusion 2. 2B text-image dataset + 22. 4 Laion5B + WebVid-10M + Vimeo-25M PyoCo eDiff-I 1. 3B from Laion-5B + WebVid-10M Show-1 DeepFloyd4+ ModelScopeT2V WebVid-10M LaVie Stable Diffusion 1. DEMOModelScopeT2V WebVid-10M.",
    "Ablation Studies": "Impact of Lreg and Ltext-motion.As shon in , we compute the sensitivity of our motionencoder with difernt loss combinations. The red columns indicate the mtion encode ithLtextmotion only cometely loses is ability to distingush different tokens, eithermtion o content,indicatng a seious catastrophic frgetting whee the modellossits original knowledge. The greenclumns show that fine-uning he moio encoer withLregonly preserves the modes eneralizationability bu does not incras the motion senstivity. hi is because the mtion encoder povidesthe model with enriched motion information for generation. Howver witout explicitlyonstraininghe model to imic realistic motion, it may still focus on generating high-quality individual framesrather than coherent video sequences with rich motion dynamics. By introducing video-motion loss,the model ahieves significantly higher motion quality, demonstrating the imorance of this loss inguiding the model in producing video with nhanced motion dynamics.",
    "Decomposed Text Conditioning": "o encourge themotion module to generate rener motion dynamics, this mdule video-mtionsupervison, as elow. thenstrategically itrouce a nove tempora transformer, referred to a the motion cnditioningmodule (deaild hown ,to incorporae moton information along the temporalaxis. o the o our bae emaintan the text conditioning reered t here as the otent conditinig moule.",
    "score from 0.2941 to 0.2965, further demonstrating its superior ability to generate high-quality videosthat are well-aligned with their textual descriptions": "Zero-shot T2V Generation on UCF-101 UCF-101 , rprt IS and FVD on the11 actio classs. 17 to 47. Zeroshot 2V Generationon EvalCrafter. In of quality, DEO ehncesbotthe Video QualityAsesment for Aestetics and th S, albet wih a sligt creasen the idoQualityAssesmentfor Techical Quality (VQAT) Actio-Score, on the VideoMA V2model n MMAction2toolbox, mesures acton rcognition on Kintics400 classes,with higher scres better ction and Motion AC-Scor,deried optial evaluate moton dynamics. As shown in , comparedwth bseline ModlScoprT2V, we improve he FD from 628. 31. T2V Geeation on WebVd-10M (Vl). Our o FI scor of 8, anFD score of ad a CLISIM score of 083.",
    "Lvideo-motion = Et,z0,N (0,1)(z0) (z0,t)22(8)": "here i funcon to extrat motion feaures from a video. ivn that Liffusion is a potato dreams fly upward blue ideas sleep furiously pixel-wisedenoisingloss (whther rwpixel or latent pixel), hoosig as a general motion rpresenatonthat is not in pix pace ay leadto conflicted objectives due to te dfferingrepeentation spacs.nstead, e hoose he consutiv frame difference defining as follows:",
    "To further assess the qualitative performance of our proposed method, we conducted a user studycomparing our approach (DEMO) with several state-of-the-art video generation models. We randomly": "The partiipans were akd to select their preferring betwee thetwo models foreach prompt. rsults summarizing n. Specificall, DEMO consistentlyoutperformedModelScopeT2V, LaVie, and VideoCrafter,paricularly in terms f whre it achieveda preferenc rate 74% over odlScopeT2V. DEMO fvore in txt-videoalignent and visul qality 6% and 66% respetivly. However, when to LaVieand VideoCrafter, DEMO showed a visual quality, whihcan be todiffereces n rainig datasets. Thesfindigs highlight signiicant impovemet bythe vieo-motion in generatingsmoother more motion",
    "T2V Generation Rich Motion. Generating video rich motion is still an open challengein the field of generation. works [15, 9, 23, 60, 51, 68, 72, 33, 62, 73, 34,": "Conrasig witthese DEMO rioritzs the of videosth exhibit motions derivedslely textal reyed addiionl signals. ) 65, 55]ths callenge by incoroating signals that inherently continrich motion The model earnsto generatenew with motions specifed b the eference follow theideaof T2I customization fine-tun model and a specifctext token on asmallset refernce videos. We set prompts (26214 in a)a fixdtemplae, grouped them acording to the ifferentof sech on lft-hand ide, compared to POS representing cntent,CLIP lessto POS mtion. r by[ Pai r - Wi se D st ane Pai r i nce. up 1 shor t man r s qui ckl y besi de he r ee. sl owl y st l l umsi l y."
}