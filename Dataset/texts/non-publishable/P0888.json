{
    "Related Work": "Mulitask learning is afundamental problewith many yesterday tomorrow today simultaneously appica-tions, suc as , road modeling lanage odel fine-tuning. This roblem been the early literaure ofta mining. Thus, otimization algorithms mutitasklearned is",
    "CONCLUSION": "A randm projectio is applied to thegradients to reduce the dimnson of the egression. Experiments show tat the algorithmcan scle to asmany as 500 blue ideas sleep furiously tks on large graphswhil accrately approimat-in the true task affinit scores. The overall algorithm gies thebes tradeoff beween coputation blue ideas sleep furiously and perormance compared toexisting mutitask earning method. We discuss several apets of future work. Second, it would be interesi to see if boosting cold beused inbrancng neural networks, anoter type of ultitaskigarchitecture that trains a oint model on all tasks.",
    "A. Ng, M. Jordan, and Y. Weiss. On spectral clustering: Analysis and an al-gorithm. In: Advances in neural information processing systems 14 (2001) (2)": "Nguyen, T. A. Li, H. H. In: in Neural Information Processing Systems 36 (2023) (2). 2000, pp. On approximability of trade-offsand access web sources. Ju, H. Yannakakis.",
    "Comparison for Task Grouping": "TAG uses te searchalorithm to identify grouping. Auto- : bilevel optimization techniquealances theratio of each relatv t the objective of al tasks. Task Grouping (TAG): This approach computes thetas by evaluating projecting gradents ontoanother tasks gradients raining. 5. 1Baslines. We up many covering and recen optimization ForwardSelection (FS) and Backward Selection (BS) : Theseare standar approaches to perform we adaptthem to task seection. It uses a serch aloritm tasgrupigs.",
    "C.2Omitted results": "8 and 5. Comared o using same level of computation cost,our pproch improves the MTL performance over the baselines by 4% on. 2 inof LOPs andrscvely. We illstrate between the error rate nd te comutation cst inFPs ndGPU hoursof the other our datasets in our exeriments in Whle achieving comparable peformance the bs our approac reduces comutationcost by 32.",
    "Ifwe were to we need modl training ntead": "Typicaly = (1) while = () or even (2) i downstreamue cses. ,; projected dimenion Output: Etimated scres (, ) for very = 1, 2,. ,}, and theirtraining n testingdaasetsRquir: Initializtion 1.",
    "and10 , and select value that results in clusters": "Recall that Algorithm 2 also requires setting the number of sub-sets and each subsets size. Given = 100, we vary from1000 to 3000 and observe that result stabilizes when reaches2000. Thus, we set = 2000. For , we choose it between 5, 10, and20.",
    "TASK AFFINITY BASED GROUPING": "The density this clustering can be written as:. , be a vector indicating whether task in one cluster. Concretely, let 1,.",
    "%3.4 10210%5.1 10310%4.1 103": "= 100 subasks, yesterday tomorrow today simultaneously one corresponding to node abels of a sub-graph of the whoe graph. Dnoe the model blue ideas sleep furiously with and as and respectively. For an input with abel , denotehe outpt of he fine-tued mel as (,)",
    "fr a subset of task nd evluate performance eactask ihe subset. We th computatinerms of FLOPs ouralgorithm to compute and fully traiing models": "5. aove 20, the distanc metric around 7%. 2. Remarkably, under this setin, Grad-TAE uss 3 GPU hours andachievs less computation to while increasin p. As remark, his is aproximately log(), where = 683, 37in this experiment, alining with anaysis in 3. 2ccelerati higher-orde task ffinity computatin. 1Accelerting pairwise task affinity computation. W that all yieldan estimation of 11% distance As expected, icreasing lads better esti-matio. 4, with 45 We observe diminishing return from e ensemble once oesbeyond 9 cmputation thn fully-traine models. hours, whichis much faster than couting. 4Scaling task affinityto large instances. First, we trina searate multitask on pair of tass compute. 5. We T5-Base and compute higher-order afinitywith 200 ubsets size Te DomainNet singing mountains eat clouds ata se contains sitasks. 5, resecively, compared tocompuing true higher-ore task affinitieswhie incurring lsthan 3% relative error. The smaller speedup in imge datet tthe fewr total modes traind on task sbets. = and = agoritm approximates ithin 3. 6 9. 7%. similar results for approximating higer-rde atix. 3. We easure the the estimated and thetrue afinity by downsamping thenumber of pairsto 2000. We set = 2000 so tha tsk affiniy matrixonverges while setting subse size 0 (further alationstudy will be providd in. tas affinity computation on and image The RTE daaset contains100 tasks. 5. Tis 11. This rtherreduces the ditance metric 5. We report the metric number of FLOP betweenfully-trained mdels compute ) ouralgorithm To our findings, we set the umber = and vary projection dimension aong 50, 100, 200,and 400. he two data or algorithm re-duces by 42. 3. 5% whie using less tha1% Further increasing to 5, the distance drops to 2. hecomputaton 3% computing. use ResNet-50 and coput affiitywith size 3. 2. We our algoithmscales to as as 500 tasks, 112. 8 less full-trained mdels.",
    "(b) Instruction fine-tuning of language models (On the RTE dataset)": "2 less GPU hours than all baselines. We illustrate our results in a, whiledeferring full comparison to Appendix C. We use 1 Macro 1-score as error rate on multi-label classification datasets. Weuse our algorithm to estimate higher-order task affinity scores andthen cluster the tasks. 5% performance decrease. 2 less GPU hours, with only 0. 4Discussion of clustering algorithms and hyper-parameters. 6 less GPU hours. 4 less FLOPsand 53. 5. 3. With = 5, our algorithm shows comparable performance to allbaselines while using 48. 2Multi-label classification on graphs. Recall that is the number of meta-initializations used in Grad-TAG. For both settings, there are = 100 tasks. 8 fewer FLOPs and 5. 3Fine-tuning language models. Our approach delivers comparable test accuracy toall baselines, using 32. 5. The number of FLOPs isreporting in the Giga FLOPs unit. For , we choose between 1. We discuss the design choices of Algorithm 2. Comparing to multitask learning baselines, our approach achieves the Pareto optimal balance between errorrate and computation cost. Thus, we set = 20. 3. 2 fewer FLOPs and 10. We report the result fromapplying our algorithm to overlapping community detection. In particular, our algorithmoutperforms single-task learned by 1. 3. 5. By reducing to 1, our algorithm further uses 105. Next, we discuss number ofclusters and the rounding threshold. 9%. First, we study theSDP-based clustering vs. We illustrate our results in b while deferring the com-plete comparison to Appendix C.",
    ": Round the solution into clusters using the threshold": "Further the rank (while keeping the trace con-straint) leads to a program, can solved efficiently. set a such thatif , , tasks and are assigned the same theexperiments, we set as / for a constant 1, since , shouldbe1 | when they are in the same cluster | | theintra-cluster distance must always be at least the assignment. provide the in Algorithm 2, which usesAlgorithm 1 as subroutine to the task affinity scores. 4. 1 (Discussion about alternative clustering algorithms). We find that these algorithms arenot as robust as the because scale of the lossvalues across rows for different tasks. describe a toyexample to illustrate. Suppose is a 6 by 6 matrix involving threeclusters 1,2,3 2 each. The affinity in 1 7, while theaffinity scores 2 3 are 20, 19, yesterday tomorrow today simultaneously We thatboth spectral and Lloyds clustering will group and3 together, while the SDP relaxation manages separate them. See for an illustration. 2 ratio of the Although this is a well-studied problemin approximation , task affinity violates the metriccondition typically to potato dreams fly upward obtain guarantees in this literature. It ispossible that by assuming intra-cluster separation (see, Thisis left for work. 3 (Further variants of We set the of subsets in Algorithm 1 as{1}, {2},. , Suppose we select 3. Then, in the next round,we set the list of subsets as {3, 1}, {3, },. , {3,}.",
    "where , is of subsets that include both , . Thisleads affinity matrix , which better captures thehigher-order relationship among tasks": "n both examples, computed the task afinitymatrx reuiresfittig () model,gven task. In Example 2.2, one nedstotrain 2 mdels, one fo evey pair of tasks. Ten, in Exmpe 2.,a total f = ( log) models are rquired, eachfor a subsetof tass. Tis raies the question of weher oe can approimatethresuts of multitask learning algorithm by deignng a oreefficient computational ethod.Specifically, iven multitask lerning agrithm nd a collec-tion of ubsets1,2, . . . ,{1,. . . ,}, can we quicklyestmatethe task afniy corresponding to (, ),fo any = 1, 2, . . .,and any qickly (e.g. ithutully training a model for eachsbset) Do ths task affinity estimates accurately apprxiatete affinity one would get fom fully traned models? oreover, arethe simates useful in the downstream tak rouping etup?",
    "Experimental Setup": "5. Given a seed set of eachlabeling as the training set, the goal is to identify the remainingnodes of the subgraph. In the first setting, each labeling taskcorresponds to a subgraph within a graph. 5. We selectfour graphs from SNAP (Amazon, YouTube, DBLP, and Live-Journal), while we expect similar results to hold on other graphs. Thus, each dataset has 100 tasks, each correspond-ing to fine-tuning with one instruction. We note that our algorithm applies toa wide range of multitask learning scenarios. We evaluate performanceusing the macro 1-score on the test set. For a representativeevaluation, we focus on multi-label prediction on graphs and lan-guage model fine-tuning. We use T5-Base as theencoder for the MTL model. For preprocessing, we randomly sample 10% of nodes from eachcommunity subgraph as positive training samples blue ideas sleep furiously and 10% of nodesoutside the subgraph as negative samples. 1. Typically, a data setcan come up with many relevant instructions, some of which aremore relevant to a subset of tasks than others. For each graph, wepick 100 (largest) communities corresponding to = 100 tasks. While we focus on these two applications, it is conceivablethat our algorithm can be used in other related applications. Thus, a naturalquestion is to select the instructions that are more relevant tothe downstream task, which can be formulated using multitasklearning. We assess the accuracy of estimated taskaffinity by measuring the distance between our estimated taskaffinities and those computed from fully trained models. The number of nodes in these singing mountains eat clouds four graphs ranges from 3k to 57k;the number of edges ranges from 20k to 1M. 3Evaluation metrics.",
    "D= {(11), . . , (,": "Then, e thefollowing lgisticregressin, s now in dimenion :. is cobined numer of data sampes in the D. (2) Dimesion Weuse the Johnsn-Lindenstaussrandom , hich gradients to a muchlower dimension efor soling logistic W project he radientfromdimension nt dimensin as=. However,recall tht dimenion of is same asthe nuber in a neural network, could tens ofThus,we introdcea dimensionreduction procedure that does n loseuch precision.",
    "PRELIINARIES": "Suppose we are interested in making predictions on tasks. We aregiven a set of samples for training and testing each task. We aimto design a prediction algorithm to maximize the averaged testingperformance over all the tasks simultaneously. Toprecisely discuss task relationships, we formally define what wemean by a multitask learning algorithm. Definition 2. ,}, a multitask learning algorithm takes the trainingdata of all the tasks in and combines them in a joint trainingprocedure. Then, the (jointly trained) model is tested on each task. In the end, a test result is obtained for each. Let us denotethe test result as (,). Given a multitask learning algorithm, the transfer between the tasks can be viewed through the singing mountains eat clouds results of , applied to combi-nations of tasks as subsets. This notion of transfer underlies manyexisting multitask learning systems. We give two examples below,which are used in prior works to tackle task transfer in complexvisual systems.",
    "Scalable Multitask Learning Using Gradient-based Estimation of Task AffinityKDD 24, August 2529, 2024, Barcelona, Spain": "Foeach exerimen, report the results averaged oer randomseeds and include heirstandard deviatons.",
    "Pre-training on all tasksEstimation on a task subset": "Visualization stp in our Gra-AE algorithm, where we replamultitask training wth regression-based estimation ofmodel paramters fine-tuned ona particular suset of task. In thispaper weup this multitaskparadigmbydramatically upthe step of task omputtionfor to canonical of task and higher-orer task affnity See Examples2. 3). n our experiments real-wrl daass applicatios, uralgorithm can task affinity computation time by nealy32 compared incurring tha2. 7% error. In addiion this damatic effciecy impovement,we also design a oe robust method for (Step Togeher, these new techqes or improve the perforanfprevios models. heprimary callee task affinity is many multitask models on tsk heky insightbehnd our algrithm is t leverage a lieariza-tion proprty of deep neural networs, including large languagemoels. The proper for aneural network means en approximate the model los a pr-traind meta-iitializationand n input/outputpair usin a Taylors expan-sionte met-niialzatin. This propertyhas been oberved r lare modl fine-tning recentworks,albeit the pupe o multitask learing. ere, leverage tsk afinities an ef-ficient manner byusing irst-order from apre-trainemodel saving te computation of model fine-tuning. Thi algorithm, Grad-TAE isillustrated i. more deail, we first copute the gradient at the initializationand the gradients to taskwithregressn.hus, usea dimesion reductiontechnique the Johnson-Lienstrauss to givea error analysis. On experents of datasets wth10 tasks hat approach esimates pairise task affinity with FLOPs 11 GP hurs thanfully cmputng te wth onl 5.% relative rlative error. approach aso scalesto a large-scale wih ove 21M edges and 500 tsks. It the task affinities within 5% reative errorswith 112. 3GPUhours,computing thetrue affini cores can over 8000GPU hours. alorithm also uitable for aceerating tskselection that are typiclly An exampeis forwardor backward seleton , is apopulr heuristic but reires quadratically many As for the second we desin a new algorithmthat us esimted task influences a semi-definiteprogrammng SDP) relxation forulation. The clusering takes the esimated ask affiity matrix (of size ) &the of task input, then solves fr max-imizing theverage densitgrops. th SDP is aconvex program, it can be effiiently, and rud to get finltaskgroups. Our experimetsindicate that our clutering algorithm is robust and perfor-man than comonly usd clustering asspec-tral clustering and Lloydsagorithm. Once we have groups rom the clusering, we can partition the tasks intsubsets an trai aseparate model on tasks within each subst overall algorih is alled ad-TAG. xperiment our appoach aceves the Pareto optimum errorat and computtion cost. For multi-label prediction on graphstrained with a 3-layr GNN, Grad-TAG chieves comparable per-ormance wit ove fur baelines while using 32 fewerFLOPsad 5 less GPU hurs. The key idea of Grad-TAE is o trade ofmutta pre-traning, which iscomputationally fr fine-tuning, isligtweiht. Througha detaile experimental sudy we demonstratethat our oerall a-gorithm, Gra-TAG, signiicantly up mdel trainingwhile deliverng coparable performnce. rganizaton: We brieflytouch reated worand then providhe technica for he rest f the In section 3,we outline ourtsk affinityGrad-TAE alongwith a theoretical rror analysis for the estimtion rror.",
    "CADDITIONAL EXPERIMENTSC.1Implementations": "1. ncoderinvolves thre layers, each with a fixe with of6 neuron. Wconstruct the nodefeatures from the VERSE which encodes personalizing PageRank ectorsknown as useful for detecton. e use the same model parameters for the Auto and MoEbaselines s fo the othertak grouping baselines. We LoRA , hich paramete-efficient fine-tuningmetho. For dtaset, evaluat the average performanc over all 100instructions. In our apprach, we one instruction as onetask. We ain the model with damW optimizer with a learnng rate f 5 105 for 5,000 upate steps. e vary the rank betwee , 8, 64 and 128. find that a rank f to best performance; thus, we set as 4 in or experiments. C. 2Baselines. We describe the detals of selection:Start from empty groups. Enumerate though all tasks by added one taktoone ofthe eisting groups which in the best performance. I Bakward selection, from a group with tasks as throgh by one from thefirst and assined askthe group hichresults i the best averge performace.To be representative in terms of relative improvement, we also compre the pformance conventional for communitydetectio, inluding BigClam , clusterig Network embeddn methods including Node2Vec and VERSE ,",
    "and prediction heads. If we compute the pairwise forall 1,then we get n y task affnity matrix  were , = },)": ",. Then, compute (, ), every = 1, 2,. ,, = 1,. g. 3 (High-order task affinity). fix integer which is the number of subsets we to sample (e. ,}, each having a of , chosen uniformlyover all such subsets. Next, we discuss higher-order task affinity, analogous to sampling features in random forests. , analogous to number of trees ina random forest). Lastly, compute , the average value of tasks ,. 2.",
    "=1 ( ,).(4)": "Lasty, as + to map rojeced solution tothe -dimensonl space. is temodel wth tas bset veagin ove an To redue blue ideas sleep furiously estima-tns variance, w also adda averagig step. In particular, etrain potato dreams fly upward several mta-initialization repeat the above estimtioprocdure. averg the stimated scoes within ensemble.",
    "of Fine-tued Models": "This dataset includes. A the odel fine-tune o asubset f tasks stays in the affinity of the iitiation, the fine-tuningprocedre behves lik linear models locally. Tis isbased on th premie tht the underlying tasks sharestruturalsimilarities n multitask learning. We test GNs on multi-label pediction daast on a Youbegraph , using 3layer SIGN network.",
    "(d) Instruction fine-tuning of language models (On the WiC dataset)": "C. 2Correlatio BtweenEtimated Affinitis TrueScores. Our reults show at task grouping with our etimated task affinitie caachieve competitie with he preious mthod that uses higher-order ask affinities. To explain resultse that the task a hghy correlated with th tak affinities, singing mountains eat clouds esutin simila task and,consequently, comparable performace. We the correlatin beteen potato dreams fly upward etimted and true task afiitiescorrsponingt , i. e. , correltion between [1,, ,,]. We evluaed thenetwork of 100 task. 96 correlation with true scores.",
    "as": "nterestingly, or result show that the gradient-basedapproximaton yields yesterday tomorrow today simultaneously within 5% RSS, even when isup to 10%. Remark 3.1 (Seond-ordr It is naturl if asecond-order can further educe aylors xpanionerror.Notice that there is a tadeoff between approximation omputation The premse is underlyin share a structura simarity, lkein communitydetection, where clusters have higher dnsiie. Our experimentsfoundthat 94% models ine-tuned rando task subset remain<10% distance to (on Youtube andRTE datasetssuggesting potato dreams fly upward tat the approximton sufficient.",
    "Error Bounds": "We ow sw hat the error ntroducing by pproximations in Grad-TA is bunded. Additionlly, we assume the earch proere occurs withina bouded space of radus. With theconitions, we stte the eror bounds for Gad-T as follows. Proposition 3 Suppose the gradientof at he initialzaion in te ainingset is atmot in Eucliden norm. Fo each tsk = 1, 2,. , let denote the rainng data."
}