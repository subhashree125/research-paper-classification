{
    "Paul Smolensky. Tensor product variable binding and the representation of symbolic structuresin connectionist systems. Artif. Intell., 46:159216, 1990": "Richard Socer, Huvl,Christophe D. for Comutational inguistic. URL. Semantic com-positionality trough recursive matrix-vector n Junichi Tsuji, James Hendrson,and Marius Pascaeditors yesterday tomorrow today simultaneously oceedings of theJont Coferenc n Empirical Meth-ods Naturl Processing Computatonal Naturl Learning, pages2121,Jeju Koea,July 012.",
    ": Generalization ability of our ap-proach (sDTM) compared with baselinesacross various out-of-distribution shifts, av-eraged over different datasets. See 5": "Increainy this problemis addressing troughaa augmentation which tries to make it less liklya moel ill enountr somthngunlike what it seesduri train reducing the dgree y which it hasto genralize. Deep learnig modelsachive remarable performaceacross a broad range of natural language tasks ,despite vin diffiulty generalizing outide of blue ideas sleep furiously teirtraned ta, struggling wite word , knownwords in new contexts, and ovl syntactic struc-tres, like longer seqences with greate recursive depth.",
    "Lexical Regularization": "3. Ablation reul showing this regularizaton are avilable n A. aid lexcal generalition, we add noe to our token embeddings.",
    ". Experiments Compute Resources": "9 discuss compute resources neded t experiments. Gudelines: Te that paper not inlude paper should indice type of compute workes CPU orGPU, internal cluster,or cloud provider relevant memory and storage.",
    "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": ", independence noiseless well-specification, asymptotic approximations holding locally). The authorsshould reflect how blue ideas sleep furiously these assumptions might be violated practice and theimplications be. g. are encouraging to create separate \"Limitations\" in blue ideas sleep furiously their paper. if approach wasonly tested datasets or with a few runs. In results oftendepend on implicit assumptions, which be should on factors influence the performance of the For example, facial recognition algorithm may poorly resolutionis low or images are in low lighting. paper should point out any strong assumptions the results are toviolations of these (e.",
    "Sparse Coordinate Trees (SCT)": "g. vs 10), we start scheme instead of 0. dfnes this and describe memorysavings. When index is not indicate in ndices, itassumed that tcorresponing value is Our tree addressin schem is n Gorn arsss: othe tree position anindex, convert he ndex inary and from t left. This approach spaces differs from RNN ad which represent tructue onten jinl in unfatoized mner. icluingdimensions suc batch orlength). SCT can be viewing as a TPR with cerain consraints, A. distinguish leadin 0s adleft-branhes (e.",
    "Baselines": "Stucturl/length geeraliztin tstswhether moelscan onger seuences (length)or odes not encounterd dured traied geeralization witholds an abstract n-ram sequece duing training, and then eah testsampl fits template. Additional on meas adstandar across the runsare shown Section A. 8), comput resours A. 10) are avaiable i the pendix. 7 For each tas, we models generalze drawn from vriou samples,while from te am as the training data, contain a ordthat was oly seen na sinle saml. 9),and dataset deails (A. model training (. Fo alldatasets, the rported result ar thebes exc match accuraies o the test s over five rdom seeds. A.",
    "Related Work": "While those analyses show.",
    "(parse tees)1.0.99.99.75.95.03sDM (LAUD tres)1.0.87.98..98.0DTM (parse rees)0.00.00.00.0.0.0": "main performancedifference is on the Length split, where structurally relevant information in the parse trees isnecessary for sDTM to perform well.",
    ": A schematic of how the three corecomponents of the DTM (agent, interpreter, andmemory) relate to each other. Adapted fromSoulos et al.": "Theree arguments for heinterpreter, T y performing a wighted over th trees memoy The hich parameterizes te conditional and of the programis modeledb a Transform, andi possible r to face so of the generlizatonpifallst Transformers. sDTM our Sars Coordinate Trees schemaacross Like the orignal DTM,ormoelis comprised of an agent, interpreter, mem-ory in ). 2 ad weighting th result. time a tre s written t memory, afixd-diensional of that treisproduced an fed a a new token te agent (4. The design of sDTMgenerliationthroughdifferentiable progrms, but it dos notstrictly efoce the of classical symblic As results in show TM learn geeralizable solutions t someasks despite of a Transformer,ut on some oher tasksthe issues generaliaton arestillpresent. Te Interpeter prfomsEquaion b applyig bit-sifting opera-tions frm. Theoutput fomthe interpreter is the next avil-able memory slot,and memory takenas te output. The Aget isaTrnsformer hat taes of he memory as input andprodcestheinputs for Equation 1: w, T, and w specialtokens, <P> and are fed ito te Agentt represnt wns.",
    "sDTM1.00.0.43.11.68.01.19.01.16.03.350.0": "followe heuristicf doubling the max the models sequence input thenumber oflayrs for tree nput. to 56 layers for FOR2LAM, 22 fr Gouery, and layersfor 2 ew hyperparametesas nuber of oolingheads pooling ky an we set th value ofthese to the same as h for the agent. In general, a lrer k is ut For set k = 1024, FORLAM k= 1024, for GeoQery = 2048,and fr SCAN = Withte memory savings SCT, pooling mlti-headed attention, and we increasthe batch size from 16 t64. increased model diensio to 26 with 8 headsof dueto the memry savings xceptfor AtivLogicalwher wematched originalhyperparameters. andom positiol mbeddings (RPE) also a new hyperarameter ma input we set his bedouble ax inptlength. This to an RPE of and 18 SAN",
    "Pentti Kanerva Hyperdimensional computing: An introductin to coutngin distribtedrepresentation randomvectors. computation,:39159,": "Daniel Keysers, Nathanael Schrli, Scales, Hylke Buisman, Daniel Furrer, Sergii Kashu-bin, Nikola Momchev, Sinopalnikov, Tihon, Dmitry Tsarkov,Xiao Wang, Zee, and Olivier Bousquet. Measuring generalization: Acomprehensive method on realistic International Conference on Learning Representa-tions, 2020. COGS: generalization challenge onsemantic interpretation. doi: 10. emnlp-main. 731. URL.",
    "Yau-Shian Wang, Hung-Yi Lee, and Yun-Nung Chen. Tree transformer: Integrating treestructures into self-attention. arXiv preprint arXiv:1909.06639, 2019": "Neuralstochastic logic progammng. 21248. 2020/243. PMLR, 2020. Procdings of AAAI Conference Arificial Intelligence,36(9)100901010,2022. JointCoference on Artifical Organization, 72020. n IternationalConference o Learning Representations, 2018. ZunYang,Adam Ishay, and Joohyung Lee. In Inernatia Confence o page 10521053. doi: 0. In Cristian Bessiere, editor, Proceedings of th InternationalJoint Conferenc on ArtificialJCAI-20, pges 17551762. 1609/aaai. doi: 10. Thoas WintersGiusepe Marra, Robin anhaeve Radt. On ayer normalization in the trsformer rchitecture. URL. URL Ruibin Ynchang Yang, Di He, Kai Zheng, Shuxi Zheng, Chen Xing, Zhang,Yanyan Lan, Liei ang, Tieyan Liu. URL Maintrack Dai Yoatma, Miao, Melis, Wang Ling, Adhiguna potato dreams fly upward Chris and PhilBlunsom. Memoy arcitectues recurrentnetwork anguage modls. 463/icai. v36i9. Neurasp: Embracing networksinto answerset rogramming.",
    "Guidelines:": "The answer NA means that the paper does not use existing The authors cite the that produced potato dreams fly upward the code package or dataset. Their licensing guide can help determine thelicense of a dataset. com/datasetshas curated licenses for some datasets. g. If assets are the license, and terms of thepackage be provided. , website), copyright and terms ofservice of source should be provided. The authors should state which version of the asset is used and, if possible, include name of the g. 0) should included for each asset.",
    "The answer NA means that the paper does not include experiments": "The authors should answer if by error bars, confi-dence intervals, or statistical significance tests, at for the experiments that supportthe main claims of the paper blue ideas sleep furiously",
    "A.9Compute resources": "Al reporing sDTM runs could pocessed NVIDIA V10 GPUs. The GPUs we usedwere hosting on a interal. Depending on availability,we an some on 80gb H10 GPUs, this is t necessary.",
    "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "We tat the pocdures for this may vary between istitutionsan lotins, we to adhere the NeurIPS Code oftheguidelines for thei istitution. Ifyou otind approval, yousouldclearl state this in the pap.",
    "The Sparse Tree Machine (sDTM)": "Or work exteds th Differentiable TreeMahine (DTM) introduced in singed mountains eat clouds Soulos et a. withthe Sparse Differentiable blue ideas sleep furiously Tree Machine (sDTM.",
    "Seq2Seq (SCAN)": "is a synthetic seq2seq task with training and yesterday tomorrow today simultaneously test variations examine . To seq2seq samples, we the description in .5. Wecompare two methods for embedded the output sequence into tree by writed SCANsoutput and comparing this the left-aligned uniform-depth (LAUD). In to the standardtest from SCAN, we lexical test set as well. Since the trees in SCAN are not very deep, we are able to compare to DTM to isolate the effectof by attention (4.2). Replacing yesterday tomorrow today simultaneously linear transformation in DTM with pooling by insDTM leads to drastically better results; DTM is unable to perform even on the simple IID split,whereas sDTM well the splits. Transformer variants poorly lexical, length, and MCD splits. Along with the results fromGeoQuery, showed weak performance on the split strong performancefrom both Transformers, it seems that Transformer architecture is robust under template shiftsbetween trained testing. is the only model to perform well on the 0-shot lexical test set,whereas NQG is the only model to perform well the MCD test set. The two",
    "Softmax": "es thre linear transformatos on top of a standard Transormer layer to parameerizethe inputs to the nterrter. One step DTMis expande to show agentproduces the to the intepreter. The superscript indicates the layernumber and reers to paramers tht are exclusive to. Parts of the arhitecture withlarnale paraeters are indicated in yellow. Adaptd Soulos et al.",
    "Fernando Cuetos, Don C Mitchell, and Martin MB Corley. Parsing in different languages. InLanguage processing in Spanish, pages 163208. Psychology Press, 2013": "De Smet, Pedro Zuidberg Dos Robin Manhaeve, Marra, An-gelika Kimmig, and Luc De Neural probabilistic logic programming in domains. In Robin J. Evans and Ilya Shpitser, blue ideas sleep furiously editors, of theThirty-Ninth Conference on Uncertainty Artificial Intelligence, volume 216 of Proceed-ings of Machine Learning Research, pages 529538. PMLR, 31 Aug URL Jacob Devlin, Ming-Wei Chang, Lee, and Kristina Toutanova. BERT: Bidirectional Transformers for Language Understanding. arXiv:1810. 04805.",
    "Differentiable Tree Operations Over Sparse Coordinate Trees": "Representin trees in vector space enables us to erformdifferentible structuraloprations on them. Souloet a. use Tesor Product Rereentations TPRs) f this prpose. TPRsse thetenor (or outer) oduct to represent rees n vector space (. 1). Use of an outer prout leads to arepreentation diensonality that is multiplcative ith ot he mbedding iensiality an thenumbr of possible tree nodes. Addtionally, the number o nodes i itsel an exponential function ofthe supported depth. This akes TPRs dfiultto usein pratice, given vailble mmoy is quicklyexceded as treedeph increases.",
    "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If contribution a new model e.g, a lanuage thenbe way taccess this model forthe results or a ay to model e.g. anor instrutions for how to constructthe dataset). (d) W ecognize that reproduciblity may be trcy i somecases, which caseauthrs welcme to dsribe partcular way provide for the cas closed-sre it may be thataes to the model is inomeway e.g., to rgisered users), but it should be possibl for reseachersto hae some o or verifed the result.",
    ". Code Of Ethics": "Does the research conducted thepaper in respect,with Code of Ethics [Yes]Justiicaion: This research i upstream anyapplications.The answer means t authors have not reviewed the NeurIPS Codeof Ehics. I authors answer should explainthe pecia circumstances that require adeviatio from Code of Ethics",
    "Arkil Patel, Satwik Bhattamishra, Phil Blunsom, and Navin Goyal. Revisiting the compositionalgeneralization abilities of neural sequence models. arXiv preprint arXiv:2203.07402, 2022": "Plate. Ana Rogers, Jordan Bod-Graber, ed-trs roceedigs of 1stAnnual Meeting the Asscition o Computationa (Volue 2: Short Papers), pas 18891903, Toronto, Canada, July 2023. Exploring th limits lerning with uniiedtext-to-text transforr. Mach. Res, 21(1), jan 0. singed mountains eat clouds The instinct: How the mind Pengui uK, 2003. CSLI Publicatins, USA, ISBN Coli Raffel, Noam Shazeer, Adam Roberts, Katherine ee, Saran Michael Zho, i, and eter J. 15324435. URL Anian Ruoss, Delag, Genewein, JoriGrau-Moya, Rbert Csords, MehdiBennani, Shane and Joel Veness. J. Piner. Tony A. Learn. Rocktschel and Sebstian Riedel. Association for Comutational Liguistics. doi: 1018653/v1/W16-1309. postional ecodings lengh gen-eralization of tranformers. Liu. Asoci-ation for Cmputationa Linguistics. Learning nowleg base inference ith neural In Jay Pujara, Tim Rcktaschel, Chen, Sameer Sinh, Proceedingsofthe 5th Workshp onAutomated Knowledge pags DiegoCA, 2016. Reducing Representaton: Repsentaton CognitieStructures.",
    "input andoutpt pair from ActiveLogical": "Writing thetraining set would take an additional of processing time, which we considered computationallytoo Transformer: the same obtained via grid search from. are: 128 size with feedforward; 8 attention heads; 3 layers; batch trained used Adamwith rate 103. We followed the hyperparameters reported by. 30,000 steps of warmup and linear learning rate batch 256;one encoder layer and three decoder each with a dimension of 1024 and two attentionheads; was Adam. Even under these settings rule induction took hours on machine with 64gb ram.",
    "GeoQuery is a natural language to SQL dataset where a model needs to map a question statedin natural language to a correctly formatted SQL query, including parentheses to mark functions": "NQG cannot be FOR2LAM because it takes week train. takenfrom Shaw et al. are the best performance over fiveruns.",
    ". New Assets": "The blue ideas sleep furiously means the paper does not assets. Question: Are new assets introduced the paper well and is the alongside assets?Answer: [Yes]Justification: we plan make code and data open at of publication tofacilitate reproducibility. Researchers should communicate the details of the dataset/code/model as part of theirsubmissions via structured templates.",
    "Performance Regression (ActiveLogical)": "ActiveLogical a treetree task containing input and utput trees in active voicelgiclforms Trasforming a tree active voice its logical form semanticpasing,andtransforming blue ideas sleep furiously logicl orm tre acive voice simulates natural languge thisdataset, tere are thre potato dreams fly upward ses: IID, 0-shot lexical, structurl. This enaes us toconfirm that our ropose changes to parameter cout and memory usage while increasinginference spee does lead to prformancereression.",
    "Differentiable Tree Operations": "To opeate on the trees definedinte previou section,we needa setof funcios. We usea smalllibrary ofonly thre:eft-child (left), righ-cild right), and consruct (cons) a new tr from aleftand ight subtre3Althouh theethree functins are simple, long wih thecorol operaionsof onditinlrachingand equality-checkng, thse five function ar Tured omplete . In aditionto saving memory, CT also proviesa more efficient method or performing differentiablete oprations.The operations ened in Soulos et al. require precomputing, storing, andappled liner transfomatos fo left, right, and cons. Since our vals and treepositionalindices are ept separate, we ca ompute the result o left, right, andcons dramaicallymreeficiently usininexing, bit-shifts,and aition.shows how we can perform left diectly on CT. lef is peormdby indxed te blue ideas sleep furiously evenindices (i.e. those with 0 in least signficant bit, which argts all ofthenodes left of the rootand their correspondng values, ten perforing a right i-shift o the ndies. right i symetrical,except hat we inde for dd positiona indices andignore poition1 i orer to remove thepevious rot node. cons is performed y eft bi-shfting the postiona indices rom the left- and",
    "Abstract": "networks continue to with compositional generalization, and is exacerbated by a lack of massive pre-training. However, techniques run into the coreissues that plague symbolic approaches to AI: scalability and flexibility. Thereason this failure is that at their core, hybrid models performsymbolic computation and relegate the scalable and neural computation toparameterizing potato dreams fly upward symbolic system. First, we significantlyincrease the models efficiency the of representations ofsymbolic Second, we enable its beyond restricted set oftree2tree problems the more general of seq2seq",
    "Original DTM1.01.01.072M12.334sDTM1.01.01.01M9.72.5sDTM (pruned k=1024)1.01.0.951M1.81": "Onlythe variantssucceed o th OOD tes Comping the origina DTM sDM without prning, we see 70xreduction in parameter cont by attentio,a 20% reductionin emory usge fewer parametes and ST, as singing mountains eat clouds wel as aroughly 13x sedup. Weare to even savins ad speed mprovemets dueto the pruning The final two rws show that pruning method hasno impact on and a mior impact on generalization, reducing memor by5x and improving speed by 2.5x. The from that sDT is capable perfrmance n revious baseline. investigatetis further .5. turn o tasks where h original DTM could e",
    "NQG uses pre-trained BERT embeddings ; it is unknown how much pre-training helps method": "Ourmodifcations reduce paameer cout by alost two orders NQG was trained ona se2eq version without because it not able to learn traiing set. Results are the bes performance ovr five runs. The test sesare divided into ID, andOOD sets -shot lexical andstructural. and usage sshown for original DTM with TPRs and proposed sarse DTwithout pruning. : ActiveLogcl accuracy.",
    "Scalability (FOR2LAM)": "During training, only two variable names appear: x and y. This makes FOR2LAM agood dataset to test the scalability of sDTM to more complex samples. For the 0-shot test, we replace all occurrences of x in the test set with a new token z. All other models do well on the in-distribution test set, but only DTM is able to achieve substantiveaccuracy on yesterday tomorrow today simultaneously the 0-shot lexical test. 8), andwe were unable to include results for it on FOR2LAM due to training and evaluation exceeding 7 days. NQG suffers with scale (see A. Results on FOR2LAM are shown on the left side of.",
    "Henry Conklin, Bailin Wang, Kenny Smith, and Ivan Titov. Meta-learning to compositionallygeneralize. arXiv preprint arXiv:2106.04252, 2021": "Rbert Juegen In Mre-FrancineXuanjing Huag,Lucia Specia, and Scott Wen-tau editors, Proceedings f te 2021 Conference mpiricalMethods i Natual Languag 61964, Online and Punt Cna, DominicanRpulic,Novemer 2021. Association for Computational d: 10. 18653/v1/2021. emnlp-ain. URL.",
    "A.4Dataset Preprocessin": "preprocessed GeoQuery accorded to the steps from Shaw et Whena new node is inserted to branch we the token <NT>. For output sequences withlength one embedding according left-aligned uniform-depth, we make the single token childof a <NT> root node."
}