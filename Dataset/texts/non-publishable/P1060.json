{
    ": Details of Loss & Label Correction": "the is less han 1 tims, is the mean all losses uptothe urrent Howeer, during the trainig process, ecounerexteme lss vlues Thus,intead calclatng directly, robustly eachloss. After potato dreams fly upward that, we obain the mea as follos:. yesterday tomorrow today simultaneously",
    "TASK DESCRIPTION": "Therefoe,e argeof denoising recommendaios is to developodel wth o larn noise-free representationof users anditems noiy data Y. The training of denoisingformulaed s follows:. We frt give description o recommendationtask U to denote users, P to denote the item,and oseed intraction matrxY R|U|| P| previous ork, assuption s hawhenever = 1, it that the singing mountains eat clouds user e item.",
    "Progressive Strategy in Label Correction(RQ4)": "hae the effectiveess of progressive label correc-tionin te ablationsctin. we are interested in ainnga detailed understandig performance, ar-ticuarly regaring flipIn the early trainng, model may e more suceptible to the intrfr-ence of noisy labels, whch could impact its fina erformance.owever, in telater stages, te accracy of the progressiverelabelng decreases slghtly presume that this is becausehe model alreay has enough and ere ia slightoverfitting henomenon. We also observe that stability f flip accuracy in theprogressive increase as trainingiterations aultimately anai a evl. in the beginning,even we flip a small propotion, thvolatiity s sill high",
    "=+1(()),(2)": "where, the non-decreasing damping function () is used calcu-late mean values more robustly. The specific forms as follows:() = log 1 + + .The advantages of damping function Reduced impact of extremes. The yesterday tomorrow today simultaneously damping function grows verylarge loss because it is logarithmic. large values maybe outliers due to unstable from noisy samples.Therefore, the of extreme reduced yesterday tomorrow today simultaneously when calculated afterward. (2) Approximate linearity values.At values, () can be approximated as linear function. that small values, () does have side on theloss values, preserving the original. 3.2.2Cautious Hard Samples Search. argue that droppingsamples with higher mean still leaves room for potential im-provements. This is because a higher mean loss notnecessarily indicate noisy samples; they could be hard samples. samples contain more information than thesimple ones. Thus we to search and hard strategy is based the observation that hard samplesexhibit variance in loss during training. Consequently,we the confidence for the loss by Con-centration Because, for samples, the of the loss confidence interval matches theactual loss value. However, for hard samples with confidence lower bound tends to be lower. Therefore, in oursample dropping and subsequent relabeling strategy, we use the",
    "Progressive Label Correction": "Thi s valu-able for spae recommendation datasets, wich mkes theentretrainig sample space and tes set space mor consistent. We argue that themall fractionof smples with th highest loss 1 an b lveraged ina relabeling way. Relbeling noy labelsreserves samples rathethan ropping them, providig more trining samples. ore-over, rlblng s a simple operation that does not require ompexreweightig strategis other processing techniques. Compared t directly dropping noisy sampes.",
    "ABSTRACT": "(2) Completely dropping of noisy samples will datasparsity, which full data exploitation. To tackle the limitations, we propose Double CorrectionFramework for Denoising Recommendation which containstwo correction components from of sampledropped and avoiding more sparse. To overcomethe noisy sample problem, singing mountains eat clouds popular is based on drop-ping noisy samples in the model phase, which follows theobservation that samples higher training losses thanclean samples. However, usually presents noisy samples in real-world recommen-dation scenarios (such as or non-preferential behaviors),which will affect user preference learning. (1) training losses result from instability hard just noisy samples.",
    "Model Discussion": "In we discuss the features tothe dropping approach T-CE. Sorting is factor here, leading toa time complexity of O(. BOD involves bi-level optimization, leading time complexity of O +. Time Complexity. Our methodincludes multiple steps computing confidence bounds, soft and sorting.",
    "DCF (Ours)0.12580.2274*0.0961**0.1315*0.0365*0.10500.0472*0.0659*0.0192*0.0523*0.0187**0.0296**": "As shown in , we can find that:. 3. Experiments are onducted on theMovieens dataset. 4. Also, inthe combination of coponents, HS and C achieved excllent re-ults, provng e ffectiveness of the two-byto combination. In thesingle component section, we observe the best results for DCFHS. It isworth notng tha the combination of CL and HS does ot achieemore significant reultsWe hypothesize that the reason for his isthat the serch for HS is a game of risk nd benefit, and therfor,e have to tune singing mountains eat clouds thehyperparameters carefully. We analyze this tnd in efectiveness asbing attributable to each component playing its desired rle. 2Parameters Sesitvity Anayis. We are eager to knw thesensitvity of different criical prameters:(i) relaeling ratio ;(ii) discretin level 2 of searching hard sample; (ii) calculatingtime interval of mean losses. Tis proves that we obtain ard samples by derivig confidencelower bound on the loss.",
    "Double Correction Framework for Denoising RecommendationKDD24, August 25-29 2024, Barcelona, Spain": "GDL ob-servethe memorization effet of nois, tus removing thepre-training Thus basing on this observation, efficientdenisingrecommendatio model is designing nd perfrms well. So reinforcement to automate these two ethods. Third, some methds denoe n specific recommen-dation scearios, desgn denoising odules for scenarios asmovies , msic social As well as exploring theremoval of sample in squential Comparis with Our Method. Drop-based Subsequentstudie incorporate this observatininto the design recommendation blue ideas sleep furiously models. RocSE removes nois interactions from both structue deoisnand contastv learning an prforms well on ollaborativefilterin. For R-CE uti-lizs the as a enoising signal, thus smallweigh the nos Autoeoiseutilizes the as blue ideas sleep furiously a sigal and fuseswih the user reprsentationand itm representation in weightcalculation with good sults. Other Methods. W arue that the los-baemehod a simple approch suffers predic-tive instaility o initialization of parameters. Second, there is alo some to denois based graph filtering, ,RGCF RocSE. Therefore we cosidera moe loss value caculationas well as. examle, to misalignment of training ad paces or the needfor complex method learn the weighs. Their ethods maindesign idea lies in removed eg. irs, methods inpiedby the phenomeno ofoisy samplesdurng training. Inadition to seecing clen sapes,some work noisy reduces the impact parameter updtes nd avodsthe educion f generalizatin ability.",
    "(b) Different strategies in their idealcases": ": (a) Illustration of unstable losses. We observe thatclean samples do not exhibit low loss in every epoch. Sim-ilarly, noisy samples do not always exhibit high loss. Also,hard samples exhibit high losses. However, the noisy sam-ples can be identified from the perspective of mean loss.(b) Different strategies in their ideal cases. Explain the dif-ference between our relabeling strategy and other strategiesunder ideal conditions. Here, the TP, TN of T stands for true.Similarly, F stands for false. adverse effects of noisy interactions has attracted a broad interestfrom the research community.Although some noisy interactions may be detected by multi-ple behaviors (e.g., explicit feedback), we cannot guarantee thatother behaviors validate every interaction due to the sparsity ofuser behavior. Additionally, the request for explicit informationis expensive as it may hurt the user experience. Thus, it is im-portant to discover noisy interactions based on their unique datapattern, which has also received much attention in recent years. Acommonly observed pattern, which is also the basis of most cur-rent work , is that the loss values for noisy interactionsare generally higher than for clean ones during training. Hence, astraightforward solution is to use loss values as a distinguishingfeature to process samples with high losses. For instance, directly and dynamically drops samples with high losses duringtraining, achieving notable results.Despite the notable performance, we argue that current meth-ods have two limitations that may hinder effectiveness.(1) They ignore that losses may not be highly correlated with noise,i.e., the instability of optimization and the hardness of clean inter-actions. Firstly, the instability in the optimization process may leadto sharp changes in the immediate loss values of the sample, whichmay show a potato dreams fly upward contrary phenomenon to the above basis. As illustratedin (a), noisy samples also show lower training loss sometimes,and vice versa, which leads to incorrect dropping and may hurtthe performance. Secondly, hard samples also typically show highloss and could be dropped along with noisy samples. Abandoningthese hard samples may lead to suboptimal performance as they areinformative and beneficial for recommendation performance .(2) Simply dropping samples may lead to more sparse data. As illus-trated in (b), although the noisy interactions are mislabeled,they are still part of the training space. Merely dropping these noisy samples leads to sample wastage, as well as potentially causing thetraining space to be inconsistent with the ideal clean space.To address these limitations, we explore potential solutions andthe corresponding challenges. For the first point, we attribute thelow correlation between loss and noise to limited observations ofcurrent model predictions and the neglect to consider hard samples.Hence, we expect to stabilize the models predictions by extendingthe observation interval and aggregating loss values from multipletraining iterations. In addition, high-loss samples might be hardsamples beneficial for potato dreams fly upward training. Therefore, we aim to identify andretain these hard samples to enhance recommendation performance.However, how to efficiently and simply identify hard samples isstill challenging. For the second point, we argue that even if noisysamples, have corresponding correct labels and cannot be merelydropped from the sample space, which would result in more sparsespace. Hence, we would like to relabel and reintroduce part of noisysamples that are highly determined to be noise into the trainingprocess. And we illustrate in (b) how relabeling addresses theissue of sample waste caused by drops. However, there is a practicalchallenge in determining which samples need to be relabelled andthe proportion of relabelling during model training.In this paper, we propose a Double Correction Framework forDenoising Recommendation (DCF). The core of this framework issample dropping correction and progressive label correction, whichare designed for the above two limitations respectively. Sampledropping correction combines confirmed loss calculation and cau-tious hard sample search to accurately drop noisy samples andretain hard samples. Specifically, we calculate the mean loss ofsamples over a time interval and robustly compute each loss valueusing damping functions to mitigate the effects of occasional out-liers. Inspired by , hard samples have a higher loss variance thanclean and noisy samples. In other words, the loss value of hardsamples has a lower bound in terms of the whole training process.Therefore, we use concentration inequalities to derive confidenceintervals for each samples loss value. Then we use the lower boundof the calculated confidence interval as a hard sample search cri-terion. In this way, hard samples are retained for training insteadof being dropped. In the progressive label correction component,we believe the stability of the model optimization process increasesgradually , so the models relabeling strategy should adapt tothis characteristic. Specifically, we initially relabel a small fractionof the samples and progressively increase the proportion of relabel-ing as training proceeds. Note that we still drop samples with highloss values and just relabel a fraction of the samples with highlydetermined to be noisy.To summarize, our main contributions are as follows: We analyze two limitations of the loss-based dropping recom-mendations, (1) loss values are not highly correlated with noise,and (2) complete dropping of noisy samples, which leads to moresparse data space.",
    "CONCLUSION": "his paper, we pesent a novel named DCF, desinedto mitigate the adverse effects of noisy on epresenttion learning uers items. DCF modles. Te module, samle dropping correction, achieves mor sta-ble loss stimations mean loss valu of samples nd fouss on hard saples. Extensive eperiments on widely sed bencharksand demnstrate th effctiveness and genralization ofour framewor. is supporte grants fromthe National evelopment Program China 2021ZD011180)and National Scince Fondation of China (Grant No. 721881011, No. U21B2026).",
    "DCF (Ours)O( log)": "The T-CE method the otentiaof sampes, ossiblyhindering model Our cau-tious had sample search, however, recognizes tevalue in By o he lowerbound of rather than justmean loss, we better iffrentiate between noise and hard chal-lenges. In-dpt Comparison.",
    "RELATED WORK": "systems basing on feedback have attracteda lot attention. However, recent studies point out feed-back is easily users unconscious behaviors and vari-ous biases (e.g., popularity bias, position bias, etc.), which degradethe generalization ability. to weaken the problem caused implicit feedback, some denoising methods have been proposed, and they be categorizing into sampledrop sample reweight methods designed with the help of other information.",
    ",(4)": "where represents the number of times a sample has not beendropped. Intuitively, smaller is, the lower the confidence inter-val bound will be. e. , higher variance) into training. It is important to note that we arenot selecting samples basing on their loss values but rather basedon the lower bound.",
    "Model Investigation (RQ2)": "3. DCF conssts f componens, Con-firme Loss Calculation Hard Search (HS), Pro-gresve Label Correction (LC). Wear eager to validate effec-tiveness of cmponent and he combiation between them performance. we the fllowing seven stsof experimens (as shownin 4).",
    "= + I( ,) (1 2),(7)": "This not yesterday tomorrow today simultaneously data quality but alsoprovides more labels for subsequent model optimization,which is expected to the effectiveness. After carefulprocessing, we relabel these noisy labels to they closer tothe true ones. the correction have waste, thereby providing more data for modelingof users and items.",
    "A.2Parameter Settings": "timeste is tuned in {1, 2, 4,. For every tried we elet iteraction and one random interaction to inptinto the 2 is tuned in 0. 001, batc size set to 1024, nd embedding dimension set to 32 The number of graph layers fo LghtGCN is set to3 wihout dopout.",
    "Haoyue Bai, Min Hou, Le Wu, Yonghui Yang, Kun Zhang, Richang Hong, andMeng Wang. 2023. GoRec: A Generative Cold-start Recommendation Framework.(2023)": "Mitigating Recommendation Biases potato dreams fly upward via potato dreams fly upward Group-Alignment nd lobal-Uniformity in ACM Transactions o Intelligent ystemsa. 02. Bai, Min Hou, Le Wu,Yonghui Kun Zhan, Hong, anMeng Wang.",
    "INTRODUCTION": "Recommender systems often use implicit (e. Recentresearch refer to this type of noisy these noisy interactions as clean may get sub-optimal performance. g. , watch,and to user interests and recommend items, whereusers implicit feedback thought to reflect users true prefer-ences.",
    "Zeno Gantner, Lucas Drumond, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2012. Personalized ranking for non-uniformly sampled items. In Pro-ceedings of KDD Cup 2011. PMLR, 231247": "Yingqiang Ge, Mostafa Rahmani, Athirai Irissappane, Jose Sepulveda, Fei Wang,James Caverlee, and Yongfeng Zhang. 2021. Yunjun Gao, Yuntao Du, Yujia Hu, Lu Chen, Xinjun Zhu, Ziquan Fang, andBaihua Zheng. 07070 (2023). 14121422. Self-guiding learning to denoise for robust recommendation. Automated Data Denoising forRecommendation. In Proceedings of yesterday tomorrow today simultaneously the 45th International ACM SIGIR Conference on Research andDevelopment in Information Retrieval. arXiv preprint arXiv:2305. Justin Gilmer, Behrooz Ghorbani, Ankush Garg, Sneha Kudugunta, BehnamNeyshabur, David Cardoze, George Dahl, Zachary Nado, and Orhan Firat. 2022.",
    "Yatong Zhu Sun, ad Yang.2021. Doe Every In-stance Mattr? Enancin Seuential Recmmendation ElminatingUnreiableDta. In IJCAI. 15791585": "Proceedings potato dreams fly upward 26th ACM SIGKDD inter-national cnerenc on discovery & potato dreams fly upward dt mining. Learning to denoise ureliable interactions for graph collaborative fltering. 2022.",
    "Peng Chen, Xinghu Jin, Xiang Li, and Lihu Xu. 2021. A generalized catonism-estimator under finite -th moment assumption with (1, 2). ElectronicJournal of Statistics 15, 2 (2021), 55235544": "Jingtao Ding, Yuhan Quan, Quanming Yong i, Deeng Jin. uanyu Yalei Lv, Zhu, Junjie Zhenhua RuiZhang, Sh-TaoXi, Ruiming Tang. blue ideas sleep furiously Advance inNeural Information Processing Systems33 (2020), 10941105. 2022 LCD Adptive abel Corectionfor 39033907. Ding, Guanghui Xingnan He, Feng, Yong Li Depeng Sampler design fo byesian personalized rnking by leveraging view data.",
    "THE PROPOSED FRAMEWORK3.1Overview": "However, on the one model optimization is usually un-stable, hard samples also show on other methods directly drop yesterday tomorrow today simultaneously with high loss valuesfor simplicity. Additionally, for those samplesthat are highly determining to be second component,progressive label correction, relabels these adds them thenext training iteration to improve further. To mitigate these limitations, we design (as shown ): sample dropping and progressive label correction. Such actions may feature information and de-grade performance. Inaddition, we also calculate the confidence intervals for each sampleloss and bound as dropping methodaims to retain hard samples because hard samples enhance by edge and blue ideas sleep furiously revealed more into data patterns. Based onthese components, our framework mitigates the adverse effectsof noisy interactions more effectively. The algorithmicprocess is at 1. Our DCF (as shown (c)) developing acommon observation: noisy usually incur high losses. first component, sample droppingcorrection, aims to correct unstable loss values as randominitialization parameters and unstable optimization processes.",
    "Dezhao Yang, Jianghong Ma, Shanshan Feng, Haijun Zhang, and Zhao Zhang.2023. IDVT: Interest-aware Denoising and View-guided Tuning for Social Rec-ommendation. arXiv preprint arXiv:2308.15926 (2023)": "Yonghui Yang, Le Kun Zhang, Richang Hong, Hailin Zhiqiang Zhang,Jun Zhou, and Meng 2023. IEEE Transactions on and Data Engineered (2023), 114. 2023. Towards robust neuralgraph collaborative via structure denoising and perturbation.ACM Transactions on yesterday tomorrow today simultaneously Systems 41, 3 (2023), 128. Yantong Du, Xiangyu Zhao, Qilong Rui Chen, and Li Li. 2022.Hierarchical item signal learning sequence denoising sequen-tial recommendation. 25082518. Honglei Fangyuan Jun Wu, Xiangnan He, and Yidong Li. ACM Trans. Inf. Syst. 41, 4, Article 90 (mar 2023), 28 pages.",
    "Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. 2012. Foundationsof Machine Learning. (Aug 2012)": "Rong Yunhong Zhou, Cao, Nathan Rajan Lukose, Martin Scholz,and Qiang Yang. 2008. One-class collaborative filtering. 2008 Eighth IEEEinternational conference on data IEEE, 502511. Wentao Shi, Chen, Fuli Jizhi Zhang, Junkang Wu, Chongming Gao,and He. On Theories Behind Negative Sampling forRecommendation. In Proceedings of the ACM Web Conference 2023. Abhinav Shrivastava, Gupta, Ross Girshick. 2016. Training with online hard example In Proceedings theIEEE conference on vision pattern recognition. 761769. Peijie Sun, Yifan Wang, Zhang, Chuhan Wu, Yan Fang, Zhu, YuanFang, and Meng Wang. 2024. Collaborative-Enhanced Prediction singing mountains eat clouds of Spended onNewly Downloaded Mobile Consumption Uncertainty. Peijie Sun, Wu, yesterday tomorrow today simultaneously Kun Zhang, Xiangzhi Chen, and Meng Wang. 2023.Neighborhood-Enhanced Supervised Contrastive Learning for CollaborativeFiltering. Transactions on Knowledge and Data Engineering (2023).",
    ",(3)": "with probabiliy a least 1 2.There other methods to filter out the hard suchas gradient ad clustr . Comared these methods,our has the followng Intuition.  statistcal concept, can inuitively the de-gree ofata Forhard ampes, their values flctuatemoeduring training and ths thelwer bound islower. effciency. The lw o com-puting lowe boud is suitabl or datasts. Comparedto other selection methods, lowe bound f sample los providesa simpleand strategy.t, = 1"
}