{
    "RELATED WORK": "research related study can be caegorized twomain areas: ime seies doman and LanguagMoels (LMs) fo yesterday tomorrow today simultaneously time series.Time Seres Domain Adaptaton: in domaincabeclasifid nto Unsupervised Domain Adpaton (UDA) Forexample, and Xuentroducedthe dversrial Kernl Matching AdvKM) ap-proah, a hybrd network the Mximum Discrepancy (MMD) metric.Laiet aligned context inforation dffern time series do-mains using akov deision prces formulation and employeddeep reinfrement learned for anomaly detetion. Other notabl.",
    "A.2Profs Theorems 1 and 2": "ak profs self-cntained first mathemtically for-muate simplifiedWithout loss generalitywe assumethat hs only one expert transfrmer network, itcosists yesterday tomorrow today simultaneously of an potato dreams fly upward ttention laye a MLP layer. To prove heoremsand 2 we followthe similar procedre of.",
    "where () is the ReLU activation function": "Theorem (Universality of our POND Model). Let < and > 0, and F ( ) : a time classifer,which from source domain and is L-Lipschitz, there exista length and a POND model such that for any ( ),we can find a domain-specific prompt generator ( : R from domain with ( +( ) (), F ( )) < for all ( = 1, ). Proof.",
    "[X2, X0], (2)1) from two source domains 1 and 2, respectively,": "POND hndles exst the cmonrompt nd te prompt generatos ( ) ( 1suchthat ([ ( ( )1), ( ( )1( = yesterday tomorrow today simultaneously 1, 2). where(1)1 [The limitation of prompt tuning]There xists no promp suchtat ([, ( ( )1( = 1, ).",
    "POND: Multi-Source Time Series Domain Adaptation with Information-Aware Prompt TuningKDD 24, August 2529, 2024, Barcelona, Spain": "includeutoregressive models associativestructure algnment , variational methods , an temporal-spectral fusion. In ddition UD, othermthods eries a supervid manner. Jin et al. shared module to lar common atent fatuesincorpoating a domain retaining domain-spcificfeaure across domains. Wison al. leveragd target-domain ditributions to enhance model performnce from multi-source time series ata. Hwver, o ournowledg, all time seriesdomain adatation methods ne-glect domain-specific nformationsuch niqe tmpral paters,which culd be utilized fordomain LLMs for Tim Series: Large LangageModes haveson excellent performan in various Natural Pro-cessg (NLP such asnatural languageinferece, quetonnswering, named entity recognition Recet hasexendedLLMs potato dreams fly upward to adres timegenealy allinginto tw classes:prompt tunng and fine-tung. In methods pretrained LLMs use (i. e. For example, Xue and SalimproposedPromptCast,a ovel approach that tansforms numecal prompts frames the time forecastingtask in a sentence-t-senence manner. presentedthe TEMPO framewok deomposing complx interctionsetwee trnd, and esidual components, introducinselectionbased facilitte adaptation in non-stationary time seies.Jin al. propose the TIM-LLM frame-wok, reprogramming time with text feeded i intoa fren LLM to align the twomodalities,with Prompt-as-Prix (PaP)oenrich input con-text and ransfoation of rrogramedSun et al. conrast, fine-tuning is the other type of method to adaptLLMs to time seris, adjustig som components while keepingthers frozen. Fr et al. presented the OFA frame-work, where olythe embedding and normalization layers of LLMswre fine-tuned, while self-attention and ayers frozen. Chng al. proposed Llm4tsframewor,fie-tunig in two sages: first, supervised fine-tung to orentthe LLM owrds series dat, tsk-specificdownstream fine-tning. For moreiformaion, therecent survey paper by al.",
    "Gruver, M., S., and Wilson, A. G. Large language models arezero-shot time arXiv preprint arXiv:2310.07820 (2023)": ",Chen, H. , Pan, S. , Li,Y. , et al. , Tseng V. Largemoels for time serie and spatio-temporal data: srvey and outook. ,Li,X. 1728 (2023). arXiv blue ideas sleep furiously preprint rXiv:2310. , Wan, Y. , Chen, P. Cho B. , Wang, S. Brunskill, K. , Liang, Y. , Zhang, J. yesterday tomorrow today simultaneously 127462774. ,Wen, Q. , Pan, S. Jin M. -Y. arXivpreprin arXiv:2310. Zhng, J. 202 ofPrceedings of Machine Learning Resarc, PLR pp. , Chu, Z. , Wang, X.",
    ": ( ) (": "e , = 2) or ulti-clas (i. , = 1) r mulivariat (i. e. = 1) or multiple oures i. e. > 2). , > 1);the time potato dreams fly upward seies domain adaptato can be from singlesource (i.",
    "( (1 ), (2 )),(3)": "Equaton (3) is computa-tional infeasibleo minimize direcy, but it can be achieving byminimizing the leve-on-out upper boud. wher (1 ) and (2 ) repesent the doain-specific promptsof any wo srce domain 1 and 2. Other tualinfrtioupperbouns, such as ontrastive lg-ratio bund, can aso convenientl b incorporaed into our yesterday tomorrow today simultaneously famework.",
    "ABSTRACT": "Ti series adaptati stns as ivotal and intcatechalleng ith diverse application, but not limited thuman recogniion, slep classicatio, and machinefaul diagnosis. Despite thedomain tech-niues tackle this complex problem, primarilyfcus o domin adapatio fom a source domain. et it imore crucito adapaton do-mainsdue tothe potenial fo greatr improvements. To addresthis, three imprtant need to ovrcome: ).The akof to nfomationor 2). The to learn doain-specific changes over time, and 3). he diffiuly to evaluatelearneddoman-speifc infrmaion. In tackle thesein ths paper, weintrouce PrOpt-based omaiNDiscrimnatio (POD) the frameork utilie prompt series omi adaptatin. Specifcally, to ddres we extend te idea pompt tuning totimesies aalysis andlearn prompts to cature common and domain-specifica source domas. To 2, introduce a co-ditonal freach domain generate prompts fromtim seriesinput data. For Challee 3, we proose to criteria toseect good are useto chosethe most suitablesource domain domain adptao. The efficcy and robustnessof proposed POND model are extensiely vlidted thrugheperimns aross 50 scenarios ompssing four datasets. Ex-permental results demontrate popoed POND all state-of-the-art coparison up to 6%on th 1-score",
    "(1)1 ( (2)1, (2)1) + (2)1in Theorem 2, and W, W, W,and W are full rank": "Moreover, the numbe ofchannel 1( (1)1)X1)( 1( Theorem 2.",
    "Shu, R., Bui, H., Narui, H., and Ermon, S. A dirt-t approach to unsuperviseddomain adaptation. In International Conference on Learning Representations (2018)": "In Compuer VsionCCV 2016 Workshops: Amsterdam, The Netherlands,Otber810 ad 15-16, 2016, Prceedings,Part III 14 (2016), Springe, pp. M. B. , an Jnse, M. , and Senko, K. ,Snne, T. S. ,De, A. In Procedingsof the 13th AM conference on embedd networked ensor systems (2015), pp.",
    "Two Important Criteria for Prompts": "In the peioussection, we exteded tunin onspecific domains. In contrst, the learned promtso specific time sers domns are visualized as exta time ar diffiult tounderstand by humans. Theyare introducedin details as follows:High Fidelity. One importantfor te prompt is fidelity (i.",
    "Ba, G., Ling C., and Zao, L. Tempoal domain with drift-awardnamietwks. In The International Conference on (222)": "yesterday tomorrow today simultaneously Brown, T., Mann, Ryder, N, M., Kapln, . D., P.,NeelakantanA.,Shyam, P., Sstry, G., A., et Language modelsre few-shotleners. seriesdoman via sparse associative tructure alignmentIn Proceedings of AAAI Conference on rtificial Intelligence (2021), singing mountains eat clouds vol. 35,pp. 6596867.",
    ": The and accuracy of the PONDmodel with source the performance growswith the increase of source (The dataset hasless than 10 domains.)": "heperformane f all which wee averaged tmes show th efrmnce with te prompt, ad prompt gnerator availale only, resectively.The to sixth rws hperformnc without prompt, promt gnerator, respectively, andthe last row shows the erformncof he complete PND model.Overall, r propose PON model perfrms best whn Mo,comon prmpt, and promptgeneratorre avalable, whihsggessthtcomponents are necessary for outstandingperformace of our proposed mode. or example, in thesce-narioof 1823 6, the performance without ny copoenonly achives erormance than 0.58, wheras that ofte colete PNDmodel is 5% better. gap widened 7%for th scenario 0-17 25.Sensitivity Analyss: In potato dreams fly upward thissection, e exploe how sourcedoains performnce on the targt dmain. Generlly, our prposedOND model demonstratesimproved perforancewith an increasing numer source do-mins. Fo instance, POND 50% acuracy with two for taining, but tis figur rie b 30% when an ditonal8 oans ar include. imilarly,the 1-scorPONDincreases y when thenumberof source main to 6. For example, thereis notable 25% drop i Fscore increasinghe nuber ofsoce domains rom to on th WISDM datat. Dakersmaller losses, reflectin bett domain The diagonals left bank. For instance, domains and domains 67 clear discrmination ith lossesbelow0.05. Siilar effective is obsrvd for omainpairs nd 0 on h WSDM dataset, domin pairs 1 and on theHHAR dataset,and dins 5-70 n the SSC dataset. Hower,",
    "Wang, Y., Chauhan, J., Wang, W., and Hsieh, C.-J. Universality and limitationsof prompt tuning. In Proceedings of Advances in Neural Information ProcessingSystems 36 (NeurIPS 2023) (2023)": ", Doppa, J. R. , and D. In Proceedings singing mountains eat clouds of the 26th conference on knowledge discovery & mining (2020),pp. Wu, Z. , Wang, S. , Hou, R. V. blue ideas sleep furiously and Ma, H. 55075521.",
    "catenation of the overall prompt + ( )and the time series": "To optimizeEquation (5), we needto enuerate al sorce do-mins, which may be inefiient nd unscalable. To addessthis, we prpse a simple yeteffective learning lgorthm ased nthe classic Repile meta-learningframework , which randolypicks a source doain each time and conut standard steps ofgraient descent witout t need for clculatig second deriva-tives. Two tunin parameters , 2>  controlthe trade-offamong the taining loss, the idelity los, nd the disciminationloss. inut (. Here, thelocl learningrate performs he gradient descent step and theglobal learnngrae peforms te extrapolationstep.",
    "pared to the time seies input ( i.., ( ( )) ( ))": "Hh Distinction. To ahieve ths, from the pespective of informationtheory, define the to maintain high ditinctionasminimizng te mtualdoain-specific promptsbetween different surce domans, wich should be inimized sfolows:. 1 in he Thse properties deonstrate that minimizin Equation (2) ensureshat the potato dreams fly upward singing mountains eat clouds gnerted prompt will fidelity an informaton to series inpu.",
    "Tishby, N., Pereira, F. C., and Bialek, W. The information bottleneck method.arXiv preprint physics/0004057 (2000)": "226928. ,Cen . , Ni, J. , Tong, , Wang, Z. Y. , and Chn, H. Inter-deendent causl networks root causeloalization. In Proceedings of te29h SGKDD Conferenceon Kowlege and Data Minin (2023pp.",
    "Kwapisz, J. R., Weiss, G. M., and Moore, S. A. Activity recognition using cellphone accelerometers. ACM SigKDD Explorations Newsletter 12, 2 (2011), 7482": "K. -H. , Wang, L. , Chen, H. , Wang, F. H. and Hu, X. Proceedings ofthe 2023 SIAM International Conference on Data Mined (SDM) SIAM,pp. K. and Sextro, W. InPHM Society European Conference vol. 3.",
    "Linkofsupplementarymaterials:": "Macro-F1 unweighting mean of scores, treated all Accuracy is the of accu-rately to all samples. 01 and 0. Single-source domain adaptation methods (e. 1 along withother hyperparameters such as the number of epochs, are All methods were averaged by ten times. number of experts was set to For the transformer model, the numbers ofencoder layers, decoder layers, heads in the multi-head atten-tion were set to 2, 1, and respectively. Hyperparameter Settings: the setted of super-vised domain adaptation, where samples in used for domain All source-target scenarios wereselected randomly to ensure fairness of the eval-uation. DIRT-T : utilizes adversarial training, conditional en-tropy, and teacher model to align source and target DSAN : minimizes discrepancy source andtarget domains via a Local Maximum Mean (LMMD)that aligns relevant distributions. Raincoat)were trained by combining all source domains. trainingset of all source domains, 60% was used pretrainingour POND model, 20% for prompt and for validationsets. MMDA : it integrates Maximum Mean Discrepancy (MMD)and ALignment along with conditional en-tropy to address domain shift. The batch size was set to The number steps ,global learned rate and the local learned rate were set 50,0. 1 2 were chosenbased on performance on the set. 5. Metrics: Two metrics were employed: Macro-F1score and Accuracy.",
    "(3) Prompt The similar source domain isselected by Equation (7), whose prompt generator beused in target domain for prediction": "4. 4. Specifically,we prove that our proposed POND model shares the universal ap-proximation with prompt tuning, and then we illustrate that ourproposed POND model overcomes the limitation of prompt tun-ing. Without loss of generality, we assume that only one expertmodel is available, and is removed (i. , ( )= ( ) ( ( );) =.",
    "Experimental Settings": "The statistics of all benchmark datasets are shown , whichare introduced as follows:1. HHAR : The singing mountains eat clouds Heterogeneity Human dataset was 9 used and SSC : The Sleep Classification (SSC) problem categorize electroencephalography (EEG) signals five stages. We utilize dataset , including EEG recordingsfrom 20 healthy subjects. Comparison Methods: We compared our proposed PONDmethod with six time series adaptationapproaches: , , Deep Coral MMDA, DIRT-T and DSAN. CoDATs it is method handle multi-sourcedomain adaptation through adversarial training with weak Deep : it minimizes domain shift aligning second-order statistics of source and target distributions.",
    "INTRODUCTION": "to of time series sensor data, time series analysishas found applications in various real-world scenarios, includinghuman activity recognition , classification , andmachine fault Unfortunately, of time series data are difficult to collectdue to the expensive of the labeling process. To mitigatelabeling researchers aim to leverage labeled data from somedomains (i. e. , source to infer labels for datain other domains (i. e. , target domains) , is as atime series adaptation problem. For goal ofthe transponder fault diagnosis is to the of transponders (i. , normal abnormal) on fiber-optic signals. In this model is trained under certainworking modes (e. g. , single used labeling time series data,and then this model is applied to other working (e. g. ,multimode). , different distributions of inputs among possible shift e. different distributions oflabels among different Most existing methods, however, primarilyfocus domain from a source domain. more investigate it from multiple sources. This is be-cause the more source utilized, greater potentialimprovements it For instance, the collection of labeled.",
    "KDD 24, August 2529, 2024, Barcelona, SpainJunxiang Wang et al": "indicate which sce domain is os similar target do-main",
    "The Flexible Prompt Generator": "yesterday tomorrow today simultaneously Most existingpapers propose various strategies to extract domain-invariant rep-resentations from all source domains by making different domainsindistinguishable."
}