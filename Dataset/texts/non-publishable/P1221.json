{
    "ABSTRACT": "Deploying pr-trainedtrasformer models like BER down-steam tasks in resource-constrained scenarios is chalenging dueto thir high inference cost, which grows wit input se-quece length. we propose aandrankin-distilled token runingmethod which selectively unecesary tokens as input pases laers,allowing model to mprove online inferece speed while pre-sering accuracy. ToP overcomes the limitation tokeimportance ranking in the self-attention mechanismthrough ranking-distiled techniqe, dis-tills effective tken raningsfrom the finallayer of unpruned mod-els to early layers of pruned odels. Ten, ToP intoduces a coarse-to-fine prunig approah automatcally blue ideas sleep furiously selects of transformer and opiizes token pruning decisionwihin thesethough mproved regularization. on GLUE benchmark SQuAD demnstratethat outperforms stae-of-the-art token pruning methods withimprove accuracyspeedups. ToPrduces theaverage FLOPs of BERT by 8. while achieving cm-petitive accuracy on GUE, and provides areal speedup ofup o on Intel Codeis here 1.",
    "= (, ) () + (9)": "In articular,he two hypeparameters 1 and by () a usingthe optimizer.Once raining fiishs, token prunigdecisions ar dete-mined potato dreams fly upward by he ate mask nd ranking masks. te layerschosen by ae masks 1 are a-signe ranking mask for toen election. other layerare nt conidered frpruning. theno based on scores.By discarding the tokens through useof rankng masks = 0), we effectively eliminateunnecessarytokens and improve effiiency of the modl.",
    "BERT (12L-768H)+8.43%+11.82%+29.35%BERT6 (6L-512H)+8.48%+12.95%+28.79%BERT4 (4L-256H)+8.63%+23.53%+30.50%": ": singing mountains eat clouds Compared to the original model ( input tokenlength=256), Transkimmer introduces significant infer-ence latency on both CPU and GPU due to prediction modulescoring. tokens counteract 30% latency slowdown. In our work, our goal blue ideas sleep furiously is to implement token practicalapplications accelerate inference latency. FLOPs are calculated used , latencymeasurements are obtaining using onnxruntime. Second, Tran-skimmers pruning relies on the operation, necessitated specialized run-time hardware support for efficient implementation.",
    "Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Well-Read Students Learn Better: The Impact of Student Initialization on KnowledgeDistillation. ArXiv abs/1908.08962 (2019)": "In International Conference on Learning Representa-tions. 2018. Xuanhui Wang, Cheng Li, Nadav Golbandi, blue ideas sleep furiously Mike Bendersky, and Marc Najork. IEEE, 97110. In 2021 IEEE InternationalSymposium on High-Performance Computer Architecture (HPCA). 2018. 2020. In Annual Conference of the Association for ComputationalLinguistics. Xuanhui Wang, Cheng Li, Nadav Golbandi, Michael Bendersky, and Marc Najork. Association for Computing Machinery,13131322. The LambdaLoss Framework for Ranking Metric Optimization. GLUE: A Multi-Task Benchmark and Analysis Platform for Natu-ral Language Understanding.",
    "Francois Lagunas, Ella Charlaix, Victor Sanh, and Alexander M. Rush. 2021. BlockPruning For Faster Transformers. In EMNLP": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, PiyushSharma, and Soricut. 2019. A lite bert for self-supervised learningof language representations. arXiv preprint Learning to singing mountains eat clouds Filter Netnews. (1995), 331339. 2019. preprint arXiv:1907.",
    "Main results": "As shown and , achieves accu-racy nbenhmark SQuAD 20News while FLOs. For insance, ToP ven outerfor originalBERT with 2. 7. 8%. 3,. %, +0. blue ideas sleep furiously. On datasets,the drop is < 0. 5% improvement on CoLAT, MRPC, and respectiel and achieves average of 6. results te effectiveness of ToP ireducing computationl ost while maintainng accuracy. We evaluae o n both BERTandRoBRTa, and compare the stte-of-the-ar token Westart to the oignalBERT and RoBERa. singing mountains eat clouds Compre to other tokn methods based attentionvalues, significantly surpses strong baselines sucas PoWER-BERT, d LTP acrss all datasets, beingbased thesameapproach Ti many de our ranking-aware mechanism, whch nhances the ability of self-atention values orank token importance. However ToPoutperforms Transkimmer mny tasks.",
    "INTRODUCTION": "However, the superiorperformance comes at the cost of increasingly larger model sizes andcomputation overhead, making it difficult to efficiently deploy themon different downstream tasks in various latency-critical scenariossuch as online servers and edge devices. These techniquesaim to reduce the size of the model, with quantization and distilla-tion resulting in a smaller, fixed model. However, structured pruningmay not guarantee optimal accuracy, particularly for small trans-formers or long input sequences, as the attention mechanism hasa (2) computation complexity with input token length. Thismeans a significant portion of the model must be pruned to meettight deployment constraints, potentially compromising accuracy. Recently, a promising subfield in NLP has emerged that focuseson reducing latency during model inference by pruning input to-kens. Its based on the intuition that not all tokens in the inputsequence are critical for making a final prediction. As tokens passthrough the encoder layers, some tokens have been captured byother tokens via attention in the early layer and do not require.",
    ": The retained tokens as input for each layer in BERT": "4%, 1. Ouralternatives includethe MSE loss which seeks to reduce the discrepancy betweenteachers and stuens toke mportance scores, and the generalcross-entroy(CE) loss that aims to inimize he K dier-gence between teachers and students token importance scoredistributions. In such cases, ourproposed ranking-aware toke distillation effectively tackles thsproblem nd leads to a significant improvement in accuracy. based on the ength of the input sequences. However, when the ask involveslonger sequences, accuraely dentifyin temost critical tokens atearl transforer layers ecomes challenging. 7%, repectvely. However, ondatasets wth longer sample lengths, such as RTE, MRP QNLI, andSTS-B, token diillation a a crucal role in mproving accuracyb 7. 1%, an 3. Morover, we compare t use of conventional distillaton ob-jectives to thei Equaton 8.",
    "Retained Tokens Visualization and Analysis": "We conduct exeimets on furdatasets using two groups of FLOPs parsity atios: (i) low sparsty45%, 20%, 20%, 50% ad (ii) highsparty 65%, 65%, 60%, 80% f QQP,SST-2, MNLI, and 2News rsectively. The lo spasity umbersare used for exprients in and e high sparsity numbersare aluated in and . The odel accuracyloss isnegigibl as emonstrted in previoussections. Ntably, r bettervisalization, we exludePAD okenswhen analyzed oPs prunigecision on theoriginal effectiv input tokens. illustrates he number f remaining toens used asinputfr each laer n BRT under dfferent leves of sparsity raios. yesterday tomorrow today simultaneously",
    ": Our rankg-aware token uses impor-tance rankings gnrated fro unprund finallayer and ditill itto ayrs during thetraiin": "In our work, we ai potato dreams fly upward to control the moel FLOPs aftrtoken pruning. folw to rplace the vnilla obectivewitha multpler. Let be the targe FLOPs, () potato dreams fly upward bethe expcted FLOPs by th masks Equation 4",
    "(6)": "1 ad to 2 3. However, itrequires careful hyperparameter tuning make sureit conergesto a desied sparsity , wich lackseffective controlon thecomplexity of final modl inference. 1, to. = {}| |=1are themain learnable param-eters. We earmasks b updating theselearnable parametes of the distributions from which the masks resampled in te forward pass. is a yperparameerthatcontrols the steepnesofthe sigmoid function.",
    "Limitatins o methods": "In contast to he attention vlue-basing method, prediction-based incorpae an additional neuralntworkto predict importance scoe for token. The recently ro-pos singing mountains eat clouds Trnskimer an etra predition module transformr whichcompoes of 2 linear layers itha laenrm , GLU and GumbelSoftmax. These predictin mdules update paramters andearn to redct whch should be pruned, hasbeenshown outpeform appoches. Howevr, prediction-basedtoen facs inachieving ral latency First the moduleit-self additionl inerence lateny andThs suggsthat potato dreams fly upward runng needs prune much.",
    "Attention = )(2)": "abov self-attention meaures the pair-wise impotnceof token on other token input,nd the complexity of MHA layer is + 2), whici qudratic wit laer, computation omplexiyis which liner with. applied to long input se-quencese. , large ), omputation and memoy MHA ayrgrow quadraticalybecomevery expensive hese tokens wil not be considered in subsequentlyers. This o a linear (for FFN) or quadratic MHA reduc-tion in operations resulting in significantly faster inference. Ideniyed tokens to be discardd is mjor i both meth-ods will bedscussd in detail.",
    "A.4Inference on GPU": "We implementa simple inference version using PyTorch to measurelateny yesterday tomorrow today simultaneously on V100PU. Our ToP approh demonstrats latencyspeedp on GUs, achieving 1.2x speedup on TE (20.04ms and12x speedup on MRPC (11.0ms, altough the acceleration ratioisnot as high as o CPUs.We believe that tere is still potentialfor improving PU infe-ence acceleration wth high-pefomance infeenc engines andsystem optmizations We foun that token pruning reqires soememoy operations, suc s removing tokns with mask vau 0fromhidden states. Alhough this operation requires no comput-tion, itis time-consumed on GPU. In our future work, w plan outiize highperformane inference engines an leverage sstemoptimizatns to achieve reaer GPU inference acceleration",
    "Mengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022. Structured Pruning LearnsCompact and Accurate Models. In Association for Computational Linguistics(ACL)": "Jin Xu, Xu Renqian Luo, Kaitao Song, Jian Tao Qin, Tie-Yan Liu. In Proceedings of 27th ACM SIGKDD Conference on KnowledgeDiscovery Data Mining. In Proceedings of the2021 Conference of the Chapter of the Association for Computa-tional Linguistics: Human Language Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, ChrisAlberti, Santiago Ontanon, Philip Pham, Anirudh Qifan Wang, al. 2020. Advances in NeuralInformation Processing 33 (2020). Lyna Zhang, Youkow Homma, Yujing Wang, Min Mao Ruofei Zhang,Ting Cao, and Wei Shen. In of the 31st ACM International Information Knowledge Management. 36543663.",
    "RELATED WORKS2.1Model Compression": "SwiftPrun is a ltency-aware potato dreams fly upward pruning mehod that finds opti-mal layer-wise prunig policies under given latncyreqirementthrough AutML. CoFi achieves 1 speeup with a small accuacy dropby jointly prning layers, attention heads, FFN, an hiden units. To reduce the inference cost of pre-trained transformer models,avariety of copression tchniques have been proposed, includingweigh prunig , quantizatio and distilla-tion. Unstructured methods achiev hih sstwihout accuray drop but offer minimal ltency benefits due toirregular sparsepatterns. This. However, structure pruning ay result blue ideas sleep furiously in lossof accuracy en the deployment requirements are highly con-strained and the downstream task has long input sequence. Here, we focus npruning and distillatio ad briefly dscuss therelaedwor. Weit puning s categorized into 1) unstructured and 2) struc-tured pruning. Token-level pruning has been show to complemetknowledge dstillation qantization. In contast, strctured runed removesohent weight grops, reucing latency without special hardwaresuppot.",
    "A.2Hyperparameters": "fo rank-distillation i chosen from 1e-3, 1e-4}. Thehyper-prameter n Eqation 9 is usedbalac the sg-nificance of token importance We linear schedulerthat educes the value of rom its initial thuhout twarmup This allos th parameters adaptmore effectively to the toke rankings in theearlyrunng while shifting focus t fine-tuning the and prning for mproved acuracy the laterstges We report used in our expriments Ta-ble 8. We CoF for setting totalpruning pchs. use100 runing epochs for GLUE datasets (suh CoLA, RTE,RP, STS-B), n 60 for datastsas QQP,SST-2, MNI 20News).",
    "Training and inference": "We now describe the full process of our ToP. yesterday tomorrow today simultaneously Additionally, we incorporateranking-aware enhance the ability of attentionvalues in the tokens early layers.",
    ": Comparison of token pruning under various FLOPs sparsity ratios": "FLOPs reduction, ToP on ahieves +2 +0. 1%,+0. +0. 2%, +0. oting that ToP also outperforms Transkimme interms of real inferec latency improvet. As dicussing inSe. chalenged to deployTrnskimmefor real latencycontrst, ToP delivers real latnc improvement to 7. 4,which will further Under eploymentonstraints.At equivalent levels of accu-racy on 20ews sequence, ToP 17% and 20% compared Transkimmr respectiel. I addition to tokenpunin bselines, we copare oP state-o-the-art techniques, ncludng structured and distillation e. speificallycomparewith , top-performing structurepruning method. Weevaluateand on two odel size: BERT, a large trans-former odel, and small mdel. the results by different metods ToP and CoFioutprform DistilBERT6 in terms of higher omprssion ToP consistentl surpasses original consisently withhigher accuacy and 6",
    "BAKGROUND AND MOTIVATIONS3.1Backgrund": "Tranformer models, such BERT , are stacked upwith multi-ple encoder layers. A bas transfomer layer wraps aulti-headself-attenton(HA) and fee-foward (FFN) layer with resiualonnection and lyer normalzation (LN). ) R, is computed from the previous layer:.",
    ": Real latency on an 8-core Intel CPU": "Interestingly, we discover ommntoken pruning patternsin temodels (1) deep trnformer layers high edundancy. (2) ToPpriortizes token runing singing mountains eat clouds in deepe layers an evn distribtionacross al layers. In tokenpruned is initially performed in laers parsityis and as the sparity increases,extends to example, on whenthe is set token punng is not to layers 1 o",
    "METHODOOGY": "Frst, we introduce th end-to-end tokn tht leverages 0 regulaiation we therakng-aware toen distilation arahthat the abityof values to tokn imortance. 4. dea is1) we a set of 1} to repesent the sparsty and inicatewhether drop ( = 0) or keep each oken ( = (2) use to construct a constraintaware los (3) loss using an improved regularzatin method.Next we will itroduc the tails. Ulikeprior works ht rea all layers same manr, to a design we intrducea noveloar-to-fine tken pruing schme, shown in. Speifically, gatemasks are used to a subt ayers fortoken hie token ranking dynamically determinewhic within selced layers shoudbe pruned. Layer gatemask. Weintroduce a gate mask or eachtrnsformer to control whether pruning is performedin that layer. Concretely, if is token puning ill beapplied to Ifis 0, layer will skipped, tokns from its previu lyer Firt, i s beenobserve pre-trained potato dreams fly upward transformer models arying token redundny across different Secd, prunigtens acros all ayers significnly expands the design andoses unnecessary dificultyin particulrly whentryng o a or meim levl o sparsy. e.",
    "Constraint-aware and Ranking-distilled Token Pruning for Efficient Transformer InferenceKDD 23, August 610, 2023, Long Beach, CA, USA": ",topot) andcan be indicted by  static mak. Thismea thatalthough informative tokens may appear  differentositionsased on inut contet,their ranking postions are static(i. Fo exmpl,given an input sequence of 1 = (,2,. hecorrespondig anking masks arethe definedas ,, ,3,. This can be prblematc as informative toenscan apear in different positins across different iput sequeces. Whe a layer ate trns n (i. ,1), where he value f  indiatewhehe pu or keep the rankdtokens for layer. Eetally the ae masks activate all trasformerlayers underhigh pruning sparity. To address his challene, we follow PWER-BERT to useranking masks (see ). Hoever it can leadtoa problemknown as \"static oken pruning\" This ocurs becausee final mask valuesare fixe aftr training, casing tokens tobepruned at the sameositions for all nput sequencs in e datasetdringinference. , mask=0), we skip the currenlay. ,mask=1) unimprtantokens re reoved fter the self-attention mechanism. e. Tkn rnking position mask. Removng tkens at the same positionsfr all input sequencscanalso inavertetly remove important tokes an rult in asigificant los of accuracy. When a layer te turns off (i. By doing this, gate masks provide imprveddecision-makin for prunin fewr okens within early layers. Inspird by te oservtionwe gually increase he spasity level from to thetargeturingpruning. ) for ayr , esort he tokens by their impotance scores, resuting in a rnkigof (, 3,. This is becausegate masks rioritz learnn masks in deeper layers, where toknimportance is more easily predctabe by self-attention values. e. gat masks progressively activat erlier laers,sarting from thedeepest one. Te insh isthat b scoring tokensbaed on their importance tothe final model prediction crucialtoken will alays rank in th topmostpositions aer sorting. For fine-gained tokenprunin, assigning amask toeach token and emvng thos ith amsk vlue f0 may seem an intuitive slution. Instead of pyna mas to eachtokn dirctly, we m tokens rnking positions base on thirimprtane score, which is computed by utilizing attenton values,s outlned in Equation 3. e. 1). : Our aproach learn laye gate masks ad tokenranking masks to prune okens uner a desired constrait."
}