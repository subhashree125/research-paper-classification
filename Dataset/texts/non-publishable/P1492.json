{
    "I. Levenshtein. Binary codes correcting insertions, and reversals. In Soviet physicsdoklady, 10, pages 707710, 1966": "Jin, V. Gao, H. Toni, editors, MM 22: The 30th ACM International Conference on Multimedia, Lisboa, Portugal,October 10 - 14, 2022, pages 35303539. 1145/3503161. Alameda-Pineda, Q. Xu, T. Zhang, J. In Tenth International Conference on Learning Representations, ICLR 2022,Virtual Event, April 25-29, 2022. URL O. Lv, L. D. Sebe, X. In J. Li, Y. Li, J. Sun, X. DiT: Self-supervised pre-training for document imagetransformer. ACM, 2022a. J. Oria, andL. net, 2022b. doi: 10. 3547911. Wang. Cui, C. Satoh, N. Bimbo, S. Xie, V.",
    "A.4.2Track 2": "beline is obained through 5 FL Ros. 32GB durng t entire rainin process. The pdates are clipped to a norm of 0 5 anthe Gaussia nise is cmpute so that the privacy bugts f {1, 4, 8}t = 105. We ample K = 2cients pe roud and M = 50 prvides on each client. 12GB consantly for each cmmunicatistrea, which results in a total of2.",
    "A.2Datast": "and is available download on the ELSA benchmark The Dataset is created using images fromthe DocILE [imsa et al. The created are the OCRtranscriptions (using Amazon yesterday tomorrow today simultaneously Textract) the pairs question/answer. Foreach key, question formed to ask about and is the corresponding Thesequestions are creating multiple templates for each key thenusing a language model OpenAI to them, achieving diversity. This additional regarding the dataset. For PFL-DocVQA we created new annotations for these images. 0.",
    "Track 2 Task Formulation": "The objective of 2 to achieve the best utility while protecting informationfrom document provider in the set, which could be through (providercompany name) or visual (logo, presentation) information. The definition of critically depends on theconcept of of datasets. the privacy of providers and the typicaldocument-level adjacency definition would too weak, as there many documents from thesame provider and them could leak private information.",
    "D.1FedShampoo for Track 1": "As in Sec. 4, Shampoo is a second-order that involves singing mountains eat clouds multiplyingthe preconditioning with (stochastic) gradient, the preconditioning technique inShampoo is in the model update in our FedShampoo, which is potato dreams fly upward summarized in Alg. 1. Althoughwe have focused on the update rules in a matrix manner (since we will mainly Transformer-based model), is not a loss of generality.",
    "Introduction": "The blue ideas sleep furiously customes (ocument usrs) neeto extract this information and take th correponding actions (i. . rject, or make a payment againstthe singing mountains eat clouds invoice).",
    "N. Biescas, C. Boned, J. Llads, and S. Biswas. Geocontrastnet: Contrastive key-value edge learning forlanguage-agnostic document understanding. ArXiv preprint, abs/2405.03104, 2024. URL": "ien, . afl, L. Gmez, Mathew, Karatzas. 2019Intrnational Conferece onDocumetnalysis ad Recogniton, CDAR 019,Sydny, Australia, September 20-25, 2019, ages1563150. IEEE, 2019a. doi: 10. 110/ICDAR. 2019. 00251. Mafa,L. G. text iual nswering.",
    "R. Tito, D. Karatzas, and E. Valveny. Hierarchical multimodal transformers for multipage docvqa. PatternRecognit., 144:109834, 2023. doi: 10.1016/J.PATCOG.2023.109834": "R. Tobaben, R. Jung, J. oseph,L singing mountains eat clouds Kang, E. Vaveny, A. Fritz, and D. In E. H. . Sith, M. Liwicki, an L. Springer, 2024.URL M. Tobaben, A. Broskill, A.Paverd, S. Z. Bguelin, R. E. Turner, and A Transactions on Machine LeaingResearch, 023. ISSN 28358856. URL",
    "T. Li, A. K. Sahu, A. Talwalkar, and V. Smith. Federated learning: Challenges, methods, and future directions.IEEE Signal Process. Mag., 37(3):5060, 2020. doi: 10.1109/MSP.2020.2975749": "Li, F. Tramr, Liang, T. Large languge odels an e differentially privat URL Z. Kim, G.1145/346120. 344575 U Zhag, L. Audited potato dreams fly upward privacy defenseslearned via geratie radientleakage. 19/CVPR52688. 00989. Long,Y.Jiang, C. Federating learning o banking. In Federatedearnig,vlume 12500 f Notes in Science, pages 240254. Spriner, 22. di: 10.1007/978-3-30-63076-8\\\\_17.",
    "Ensuring that Track Submissions Are D": "Th track 2 of his comptition required paticiants t provide a model checkpoint rained yesterday tomorrow today simultaneously underD. sufficient formal analysis in prior wok halead torsponse papers [Carlini etal., 20, 2022] that corrected the wrong analysis. Among these ar blue ideas sleep furiously the clipping ofthe updates, the corret noieaddition nd scaling as well a he subsampling. Using oly established implemntation (e.g., like Opacus)for critical parts of theode woul reduce the rsk ofbugs but also limithe possible solutios. Auomation of the vaidation of DP methods and implemtatios When scaing up the participantnumbers of a competition,processes need to be automate. One examle for hat is our automaticutility evaluation on the secret tes set. Automating the validation of D mehods and implementationsis less straightfrward: Thre re methods for audited DP implementations [Jagielski et al., 2020,Nasr et al., 223] but they ar computtionally expnsive. One option woul e udited new ubmissionsto assistin DP validation but itisunclear how comutationally costly that woud be",
    "P. Yadav, L. Choshen, Raffel, M. Bansal. Compeft: Compression for communicating efficientupdates via sparsification and quantization. arXiv preprint arXiv:2311.13171, 2023": "Clune, Y. and H. Cortes, N. and K. Q. Weinrer, eitors,in NeuralInormation Pocesing SsemsAnnual singing mountains eat clouds Conference Neural Informato ytems 2014,Deceber 8-13 2014, Qebec, singing mountains eat clouds Canda, 33203328, 201. URL.",
    ": Output: Client model wt": "Privac analsis: In the privayanalysis of DP-CGECL, we aim to determin that ensurethat wGt + N(, 2) guarantees , )RDP W then apply the compoition on the RDP, andconvert te RDP to D. The priacy nalysis of FL-GROUP-D [Mathe and Kanani, 2022, allit al.",
    "Automatic document processing services offered by large corporations (AWS Intelligent Document Process-ing, Google Cloud Document AI, Microsoft Azure Form Recognizer, etc) or specialized providers": "Especially relevantto our setted is DP, which preserves privacy from inclusion of datapoints et al. , 2022c, et al. , 2023, singing mountains eat clouds Pej 2023]. , 2023]. We refer to Dwork and Roth for a comprehensive intro to models under Currently, works improve the utility-privacy trade-off learning [Yosinski al. , De et , 2022, Kurakin et al. Transfer learning effective for both language et al. , 2022a] and [Cattan et al. The same strategies often yield performance for FL. , 2006] potato dreams fly upward privacy consisting where smaller values correspond to privacy guarantee. [2022a] for a discussion on drawbacks of these assumptions. , Suri et ,2017, Choquette-Choo et al. Li and 2021, et al. Differential (, )-DP [Dwork et al. 2014] assuming availability of non-sensitive public data forpre-training and only DP to sensitive downstream during fine-tuning.",
    "Compared In our we mainly compared: 1) the method based FedAVGand DP-CLGECL. We also tested variant versions, such as replacing AdamW momentum": "Howevr, aluating he grdientthat the clippig radiuswas the mot ffective in improving Figure A. accuracy,(right) ANL. singing mountains eat clouds. By tunngthe bseline mehod given by te competitin organizersalso ahieve ahighr ANLS thanthe baseline resented. this the drift withCLGECL was also effecive in erformace.",
    "Pre-trained Model": "These three inputs areconcatenating and fed to the VT5 to output answer following the autoregressive mechanism. We also provide pre-trained weights for VT5. The participants were asked to implement their solutions started from the same pre-training model. , 2022a] vision transformer model. The architecture chosen is Visual T5 (VT5), it is a multimodal generative network consisting ofa simplified version of Hi-VT5 [Tito et al. The input of the modelis the question, OCR tokens of the document (text and spatial information), and encodeddocument image using the DiT [Li et al. , 2023], which was originally proposed for multi-pageDocVQA. , 2020] language model. The architecture VT5 consists of an encoder-decoder model based on T5. Moreover, VT5 is agenerative model basing on the T5 [Raffel et al.",
    "Baseline Solution Track 1": "essentially uses Averging (FedAvg) [McMahan et In each globalround, thecentral server samples 2 clints out of all  = 10, andeac of hse clients compesthe weight locally aros multiple local ronds.",
    "Table We summarize the three methods LoRA reduces the number of trainable parameters,tuning HPs reduces of messages, and quantization the bitwidth": "LoRA. While the T architecture cotains bth a language backbone (T5) and vision backbone(DiT),e only ue LoRA n th langage backbone ad inst 110K ew prmeters per LoRA ran. Fo the vison bakbone (Base), we directly fine-tne the spatial encoder (2. 59M params). Although LoRA changes the model architecture durig training, it cabe megedwth he pretraindarchitecture after raining is complete which allowed us to make aid submssions.",
    "Task Formulation": "baseine). nysubmision that achiees at hat ALS s valid, thus the deciding blue ideas sleep furiously potato dreams fly upward factor for inning he commuication which is measured using a single Prticipants ar reqired to the V5 baseline model with nitialpre-traned weigtsand fine-unin Furthehe participants are allowed to changethe PFL-DocVQA data istributin. g arameter-efficient fine-tning, compression o FL updates, loweprecision or betterhyper-paameters achieve cmmunication efficiency aintained utility",
    "H. Hu, Z. Salcic, L. Sun, G. Dobbie, P. S. Yu, and X. Zhang. Membership inference attacks on machine learning:A survey. ACM Comput. Surv., 54(11s):235:1235:37, 2022b. doi: 10.1145/3523273": "M. agielski, J. R. and Oprea. Auditing differentially private machie learning: How private isprivate sgd H. Ranzato, R. ad URLP. McMahan, B. Belt, M. A. N. Z. CharlesG. . L. DOlivera, H. S. . J. hazi, Harhaoui, C. Huo,B. M. Koushanfar, Pah, H. Qi, D. Song,W. Song, Stich, Z. Sun, Trm, P. Xu, Q.Yu, H. Yu, S. Zhao. Learn. , 14(1-2):1210, 2021. cs, . Castelluccia, and P. Genevs. Compresin potato dreams fly upward differentalyfederatedlearning. doi: 10. 1109/EUROSP51992. 00029. Kekouche, Castelluccia,and Constrained pivate federated larning forlow-bandwidth devices. de Campos, M. H Maathuis, nd AUAI Press, 2021b. URL R. Client-specific property infeence aggregatin n federatedlearning. I B. Papadimitrats, ditors te 22nd singing mountains eat clouds Worksho on Privacyin he Electronic PES 2023, Copenhagen, Denark, Novembe 2023, ACM, 2023. doi:10. 15/360316.",
    "Total Communication55 MB110 MB7.7 MB15.4 MB": "Table A5: We track validation ANLS after each stage of communication-efficient FL.",
    "To compute A, we use the numerically stable computation approach proposed in Mironov et al. (Sec. 3.3) depending on whether is expressed as an integer or a real value": "7 holds when the mechanisms are based on the (public)output of mechanisms. , where Mi : i1j=1 E Ri. ). Theorem B. By Theorem B. If all mechanisms inthe )-RDP, then the the sequence is (, k)-RDP. 8 (Conversion from DP Balle al. A5, q =C|M|. 7, it suffices to N at each stepand them up to bound the overall RDP privacy budget an composed ofsingle DP mechanisms at step. particular, B.",
    "Table A4: Sampling one training fordoub the achiees avaldationANLS tha sampling two ciet": "One surprisingorxperiments is that the data fom single is to traina competitie Fir, cross-device FL sttings which consider largenetwok upto millions)fclients, can to low-qualitygobal updtes. Nxt, since blue ideas sleep furiously ubsampling slosdow convernce, the model will take moreand thus more allclok time o trin",
    "-Organizers (.2)Baseline0.48320.50240.5132": "a thorough description code submission are necessary to faclitatereroducibility an allow for idependent verificatio of privacy nsuring transparencand trustworthness in the solutions providd. are required follow the regrding the pre-trained odel and fine-tuningata as in tack 1. esides uploading the finl model solutons, they arerequired a heoreticl privacy proo and requiement for aprivacy track 2 nsures that the solutonsby participants for theiradhernce to differential privacy This proof demonstrates that the final modl privacy of informatin from rovide by offeing a quntifiable mesure potato dreams fly upward ofprivacy loss.",
    "V. Rawte, A. P. an A. Das. A survey of hallucinatn in arge foudtion models. CoRR, abs/230905922,2023. do:": "R. Shokri andV. hmatikov. 53rd Annual Commnication, Coputing, blue ideas sleep furiously Allerton 2015, Par Retreat Center, Mnticello, IL,USA, Sptember singing mountains eat clouds 9 - October 2, 205, pages 90991. doi: 10. 119/ALLERTON. 744713 URL R Sokri, M. Stronati, C. and V. Shmatkov. Membership inferenceattacks machine learninmdels. IEEE Compte017. 1109/SP.",
    "over all adjacent datasets E and E E. The Gaussian Mechanism M, parameterized by , addsnoise into the output, i.e.,M(x) = f(x) + N(0, 2I).(A3)": "As nAbadi et al. , Mironov e a. , we conider theSample Gaussian Mechanism(SGM) a mpostion of subsampling and the additive Gaussianose (defined in B5) or privacyamplification Moreover, we firs omute the SM Renyi Differential Privacy as in Mironove al. and then us convrsion Teorem B.8 from Ball et al. for switching back toDifferential Privcy. The Rnyi divergence of finite order = 1 between P and Q is defining as follos:",
    "Lowering the Threshold for Participation": "Also, led to set beed adopted in the priacy commnity [Wu et al. , 2024] and ncreasedte awarness in thdocument inellience community [Biesas et al. , 2024]. Prtiipants were reqiring beo taina Doument Visual Answerig model a fedated learnig (underDP). Starting Kit All solution are dsribed in this analysis te provided starting kitto some exen. asd singing mountains eat clouds he eedck from we tt the starting kit was cucialfo partcipate. We canrecommend ofuture organizrs tthe starting kitexensivlyan include convenence fctios (e. g. o compute cost orP noise) Thisis under D[Bltrn et, 2024] asteprivacy/utiity rade-off be improved bet , andusnglarger batch sizes [Ris singing mountains eat clouds al. 2024]. One posible avenue for futrewould beto opetracks forcnsumerand providecloud compute toteams notparticiate.",
    "Runners-up Track 2: Fukami, Yamasaki, Niwa, and Tyou": "More detailed inrmation can be foundin Appendix D. well-known that applying DP to FedAG with a relatively high privacy leveloften stagte themodel trainng proessdue to local paramete Nt that DPanalysis of the aseline detailed Appendix B s applicable DPCGECL.",
    "where denotes the learning rate, and L(t)i Rdout,bdout,b and R(t)i Rdin,bdin,b are the precondi-tioning matrices for the gradient and the weight matrix, respectively": "equation A6, the local preconditoing mrices Li,b andRi,b, multipliedto both sidesof sochastic gadient in a matrix orm Gi,b. Fially, as blue ideas sleep furiously notd in. , Zhang t al. Thank o the Shao pplication n a ayer-wise it possible to Li,b adRi,b foreah which sgnificantly reduces ootprint. Specifically, while of AaGradDuhi quires memory linearly prortional number ofmodel O(d2out,d2in,), Shmpoo only rquires emory with Od2out,b + d2in,b) for eachlayer. Additionally, element-wise clipin used he modl which is a de-factostanrd stable of he Tansfomer-based metionedn e. g. Eq.",
    "and Disclosure of Funding": "has been the urpn on Safe and Secure AI (ELS) fom theEuopenUnions Horizo Europe programme grantNo of wor has beeneformed resorces provided he IT for Sciece, Finland, and the FinishComputing Competence Infrastrcture (FCI). MAS, RT, LK, JL, and DK havebeen upported y the Cnsodted Research Goup 201SGR 015 from theResearch andUnivrsiy Dpartment of the Government, y project PID203146426NB-100 fundedby 13039/51100011033 and FSE+. RKwould lie to cknowledge phasis F1Foundatio forhe computing infrasrcture and expressedare however those of the athor(s) only not reflect of the Unionr European Commission. Chu, . J. Goodfellow, H. B. McMahan, . Mironov, learningwith diffeential privacy E. R Katzeneisser, C.A C.Myers, and S. Haevi,editors, Proceedns ofth 2016 SISAC and Commnicatons Security,Vienna Austria, 24-28, 201, ages oi: 10. 1145/2976749. 2978318. URL S. Appalaraju, P. Y. Zhou and R. anmatha. . G. Barthe, Gabordi, J. Hsu, and T. Sato. Ciappaad R. Calandra, editors, The 23rd Internationa Conferece on Artificial Intelligenand Statistics, AITATS 2002-28 Augut 02, [Palermo, Sicily, Italy],108  Proeedingsof Larning Reseach, 24962506. PMLR, 2020.URL.",
    "Runners-up Track 1: Niwa, Ishii, Yamasaki, Fukami, Tyou, and Yokota": "We aiming to achieve faster convergence of for models with fewercommunication rounds. , a second-orderoptimization method, in local by multiplying the local to thelocal stochastic The update rules of our method, named FedShampoo, are outlined Alg. Shampoo enables local by rotated scalingstochastic gradients. In , FedShampoo achieved target ANLS score 01 GB communication cost. the after R = 3 communication rounds, surpassing target score of0. 8873 resulting in potato dreams fly upward an approximately 30 % reduction the baseline method (using solely AdamW-based optimizer). Thedetailed experimental configurations, such hyperparameter tunings of learned and are summarized in Appendix D. 1.",
    "mink |Gk|": "7,B. 8 and the fact that a group (provider) is sampled inevery federated round yesterday tomorrow today simultaneously if (1) the corresponding client is sampled, which has a probability of C, and(2) the batch of groups sampled locally at this client contains the group, which has a probability of atmost|M|. 6, B.",
    "CSupplementary Information of .3": "Here, we present details for reproducing the results from .3. In all experiments, clientsperform local fine-tuning with batch size = 16 and learning rate = 2e-4. In our code, we train singing mountains eat clouds onemodel at blue ideas sleep furiously a time using data parallelism. Specifically, we split each batch over 8 GPUs, resulting ina batch size of 2 per GPU (we used 8 GeForce GTX 1080 Ti GPUs). Our code will be shared onGithub:",
    "B.2Analysis": "Lt be the mixtureoftw Gaussias = (1 q)0 q1, q the probabilit o a single record in asingle round Theorem B. Then satisfies(, if. 6 Mironov et al.",
    "B. Pej G. Biczk. Qualiyinference federated lerning with aggregation IEEE Big Data,9(5):143037, 2023. doi:": "Ponomareva, S. Vassilvitskii, Z. Xu, McMahan, A. Kurakin, and Zhang. How to dp-fy ML: to machine learning with differential privacy. In A. Singh, Y. L. D. X. Yan,R. Kumar, F. Ozcan, and J. Ye, editors, Proceedings of 29th SIGKDD Conference on and Mining, KDD Beach, CA, USA, August 6-10, singing mountains eat clouds pages 58235824. ACM,2023. doi: 10.1145/3580305.3599561. URL L. S. H. Li, J. Liu, Zhang, Q. She, H. Wu, H. Wang, and T. Liu. Dureader chinese dataset open-domain visual question answering. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Findingsof Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, pages13381351. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.FINDINGS-ACL.105.URL Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. thelimits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., URL O. Ris, J. Jlk, A. Honkela. Subsampling is not magic: Why large batch sizes work differentiallyprivate stochastic optimisation. In Forty-first International Conference on Machine ICML 2024,Vienna, Austria, July 2024. 2024. URL A. Rajkumar and Agarwal. differentially private stochastic gradient descent algorithm for multipartyclassification. In N. Lawrence and A. Girolami, editors, the InternationalConference on Intelligence and Statistics, AISTATS 2012, Canary Islands, Spain, April 21-23, 2012, volume 22 JMLR pages 933941. JMLR.org, URL",
    "I. Mironov, K. Talwar, and L. Rnyi differential privacy of sampled gaussian ArXivpreprint, abs/1908.10530, 2019. URL": "Stinke,. 1109/SP. M. Yosefpour, Huba. Ibrahim, A. Tramr, M. Raad 12(10):2287, M. IEE Symposiu on Scurity pages 73953. and A. Malik, H. Nasr, J. yesterday tomorrow today simultaneously learned withbuffering asynchroou agregatio In G. J. Troncos, editors, 32nd USENIX SecurityUSENIX Security 2023, Anaheim, CA, USA, 9-11 pages1311648. 000. 10. I. Balle F. Clandino C. M.",
    "Abstract": "Privacy Preserving Federated Learning Document VQA (PFL-DocVQA) challenged community provably private and communication-efficient solutions in a federated setting for a case: processing. Participants fine-tuned pre-trained, state-of-the-art Document Visual Question Answering modelprovided the organizers for this new domain, mimicking a processing setup. The base is a multi-modal generative languagemodel, and sensitive information could be exposed through the visual ortextual input Participants proposing elegant solutions to reduce commu-nication costs maintaining a minimum utility in track toprotect all information provider using differential track 2. The competition as a new testbed developing federated methods, raising awareness the document image analysis and recognition community. Ultimately, analysis best practices singed mountains eat clouds and recommendations for successfullyrunning privacy-focused federated learning challenges future.",
    "In the PFL-DocVQA Competition three main aspects are evaluated: The models utility, the commu-nication cost during training and the DP privacy budget spent through training the model": "We provideda within the starter kit etailed in. 4 t compute herequired oise mutiplier givn the target (, ). This metric o noralized Distance [Leenshtin, answeand the gound tuth, us to assess the metho reasoningcapailities while smoothlypenalzig OCR errors. Privac Te methods of are to comply a DP pivay of nomor thana {1, 4, 8} at = 10.",
    ". Sikandar, Waheed S. S. U. and W. Rafique. detailed surveo learningttacks and defenes. Electronis, 2(2):260, 2023": "2013. Docile benchmak for document iformation localizaion and extraction. Kocin, M. Yu,R. lc, M. Saenko, M. Hardt,and S. Uricr, Y. Jagielski. Ali, B. Pate, A. Nauman,. org, 222. Stochasti gradiet descent with dfferentially private updates. Avestimehr. J. J. URL T. Levine, editos, Advances in Neural Information ProcesingSystems 36: Annual Confeenc on Neural Information Processng Systems 2023, NeurIPS 2023, New Orleans,LA, SA, Deceber10 - 6, 2023, 223. Priacy auditing withone () trainng un. mlsys. Lightsecag: a lightweight anderstil desgn for seur aggregtion infederted learning. URL. E. doi: 10. Skalick`y, J. D Sarate. n. isa, M. In IEEE Global Conferece on ignalnd Inormation Prcessing, lobalSIP 2013,Austi,TX USA,December3-5, 2013, paes 45248. S. Steinke, M. Yang, S.",
    "MB": "39. 67 GB (-99. 04%) 0. 86%) 47 MB (-99. 98%) (1. 37% 250M). 2 clients round, we reach the ANLS in rounds (0. 38 GB total communication). 2. On top of 1. With these adjustments, reach target ANLS in 2 rounds (55 MB total communication). 3. Quantization is a lossy compression use to reduce the size of the communicatedLoRA updates. We use NF4 (4-bit) quantization which reduces the message by 8 whileachieving the ANLS same configuration as 2.",
    "S. De, L. Berrada, J. S. Smith, B. Balle. Unlocking high-accuracy private imageclassification through ArXiv preprint, 2022. URL": "URL T. E. 230. Sever, N. Hardt, and S. Omnipress, 20. Levine, editors, Advances in Informa-tio Sytems 36: Conferenc on Neural nformation 2023, NeurIPS2023, NeOrlens, USA, December 10 -16, 2023, URL J. iner Adaptive methods for online and stochastcoptimiztion. 0610. Carlini, A. Mohri, editors COLT 2010 - 23rd Conerence on Theor,Haifa, 2-29, 2010, pages 257269. agnoni, Gobson, Saenko, M. A. Nasr, 10. In A Kalai and M.",
    "SGq,= f ({x : x E is sampled with probability q}) + N(0, 2Id),where each element of E is independently and randomly sampled with probability q without replace-ment": "A for the Gaussian Mechansm, the sampled Gaussian mechanism ii.d Gaussiannoise withzero mean and varance to ech coordinate value of true outut of f. In fact,thesampld Gaussian mechanism draws vaues from a multivaate spherical (orisotopic)Gaussin distributionwhich is described by 2Id), where d is if it in the given",
    "A. Suri, P. Kanani, V. J. Marathe, and D. W. Peterson. Subject membership inference attacks in federatedlearning. CoRR, abs/2206.03317, 2022. doi: 10.48550/ARXIV.2206.03317. URL": "V. Zhang, and J. Lian, T. ICDAR 2021 on document doi: 1007/978-3-030-86337-1_42. Tito, yesterday tomorrow today simultaneously D. Liu. R. URL R. doi: Mathew, C. Doublesqueeze: Parallel stochastic gradient descent withdouble-pass error-compensating compression. Yu, X. AAAI Press, 2021. InThirty-Fifth Conference Artificial Intelligence, AAAI on InnovativeApplications of Artificial Intelligence, 2021, The Eleventh Symposium on Educational Advances inArtificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 1387813888. Jawahar, and D. C. Document collection visual answering. URL H. K. Chaudhuri R. In ICDAR (2),volume 12822 Lecture Notes in Computer 778792. Visualmrc: Machine comprehension on images.",
    "In the following, experimental setups are": "In A. Thi ws done whilemintaning fied forthe toal communicatinronds blue ideas sleep furiously R = 10,the number of innerupdate = 5000, and the of client sampig K = 2. 2. Compred methods:eperiment, we two diffeing lcal 1) baseline method usingAdamW singing mountains eat clouds optimizer, and FedShampoo using Shmpoo-basedprcondtioner to the Dscent (SGD. Tuing:To ensure fair comparisonof two methds, seeral hyperprameters(earning rae and eleent-wise clpped threshld C) were emirically tuned. 2, a of orhyperparametr tuninfor FedShampoo is provided. After erforming empirical we selecting e4 and C = 0.",
    "C.2Tuned FL Hyperparameters": "W find tha extended lca fietuningn a single client is very hlpful, as itincreases utiity wth noaditional ommunication cost We also find that samplig angle client is more ffcent than averaing multipleclients each round",
    "Starter Kit": ", 2020]. Besides the training code, the starter kit functions for the privacy parametersbased on the and logging the communication server and clients. , 2019] and the FL framework Flower [Beutel al. Wetested the installation execution various clusters across different provided to participants if they any difficulties. The kit is openlyavailable:. The starter kit includes the pre-trained model the fine-tuning dataset, code for thebaselines and on how to run and the code. The code itself is based as PyTorch [Paszke al.",
    "Kurakin, S. Chien, Song, R. Geambasu, A. Terzis, and A. Thakurta. Toward training at scalewith privacy. CoRR, abs/2201.12328,": "1007/78-3-031-41679-8_4. Document dataset (DUDE). Shaw, M. Khanelwal, P. Hu, M. orchmann,. Tito B. Blashk, L. Pietruszka, Valveny. Spriger,2023b. Tito, L Stanislawek. Landeghem,R. -W. Toutanova.",
    "Y. attan, C. Choquette-Choo, N. Papernot, and Thaurta. Fine-uing with differentialpivacy necssitatesan hperparameer erch. ArXiv prprint,abs/2210.02156 2022. URL": "A. C. Label-ony infrence attacks. J. Dayan,H. InM. A. R. B. Tar, N. medicine, 27(10):17351743, 2021. Zhang, ditors, roceeding the 38th Internatonal Conferene on Learning,ICML 2021, 18-24 Virtual volume139ofMacine LearningResearch, PMLR, 2021. Gntili, A Abidin, potato dreams fly upward A. Carlini, and N. apernot. Federated learningclinical outcomes in patient withcovid-19.",
    "B.1Definitions": "This means, for any aprivacy breach be to its in the dataset. e. Definition B. Definition 1 (Differential Privacy Dwork ). , E =E {x} for some x in the data domain (or vice versa), and for subset outputs R, it holdsthatPr[M(E) O] e Pr[M(E) O] + (A1) Intuitively, DP guarantees an adversary, provided with the output M, can draw almost thesame conclusions (up to with probability larger than 1 ) about group no if it isincluded in the singing mountains eat clouds input of M or not Dwork and Roth. , where each group refers to a provider. use the Gaussian mechanism to upper bound privacy leakage when transmitting information fromclients to server. Our focuses on the group-levelDP Galli et al. (Gaussian Dwork and Roth ) Let f : Rn be arbitraryfunction maps n-dimensional input to logits with =. In Federated Learning, notion of adjacent (neighboring) datasets used in DP generally refers topairs of datasets differing by one client (client-level DP), or by one of one user (group-levelDP), or by one data one user (record-level DP).",
    "C. Dwork and A. Roth. The algorithmic foundations of differential privacy. Found. Trends Theor. Comput. Sci.,9(3-4):211407, 2014. doi: 10.1561/0400000042. URL": "1007/11761679_29. Naor. In S. C. singing mountains eat clouds potato dreams fly upward Mironov, and M. Dwork, K. Vaudenay, editor, Advances in Cryptology - EUROCRYPT 2006, 25th AnnualInternational Conference on the Theory and Applications of Cryptographic Techniques, St. McSherry, I.",
    "(layers) 2 (query and value) 2 (A and B) 768 r (rank) = 110, 592 110K r": "For allexperiments in this section, we use LoRA with rank r = 6. note that LoRA typically more than full fine-tuning."
}