{
    "Finally, candidate labels are sorted according to MRR(|) as thereranking result": "3. 3Self-TrainingTe retrial proposed in. 1significanty improvesthe FuTex. Theclassifier then used to predict he that a pper isrelevant to a label (no necessarily selcting in the rtrieval stage),and those top-ranked labls wll complemet u MRR-basedreditions Since the major of this paper isnot to invet a new fullysupervised exclassifer, propose use an off-the-shelf model. To leverage full nd meanwhile potato dreams fly upward bypass the problem, we choose multi-lael classifier , whi, accordin to , has perfor-mance even cpared deep learning clasifiers on Parabel training doument a WD|-dimesional vecor , where WD s ofD.",
    "sim(,) = PLM([CLS][SEP][SEP]).(4)": "However, the that concatenates each paragraph and and feeds into one PLM (i. e. suppose are papers (each which has and then we need to call the PLM () inference. For example, if = 104 and = 30, then = 3 Adding retrieval stage. mentionedin Introduction, and other paragraphs should treated equally in text classification. To use the more powerful in the most importantcase, we only Cross-Encoder architecture when inferringthe labels of abstracts (i. other paragraphs, weadopt the Bi-Encoder architecture.",
    ": Training time of FuTex and repre-sentative baselines MAG-CS and PubMed": "Note that these labels are indeed relevant to paper rather being mentioned: Crohns disease and ulcerative colitis aretwo types inflammatory bowel diseases studied in the andthe paper extensively discusses the sensitivity and specificity ofpredicting these diseases. shown in , hints these labels can found in the full text. To compare training time ofFuTex network-aware contrastive with that ofPLM+GAT and which also use network fortraining. For a fair run model on NVIDIA. By leveraging the papers text, FuTexaccurately picks these 4. 5EfficiencyWe now analyze the and time of FuTex MAG-CS and PubMed.",
    "Yin, Jamaal Dan Roth. 2019. Benchmarking Zero-shot TextClassification: Datasets, Evaluation and Entailment Approach. EMNLP19.39053914": "attention-aware deep modelfor high-performance extreme multi-label text classification. deep contextual representation learning for large-scale high-performance MeSH indexing with text. Bioinformatics 37, 5 (2021), 684692. 2019. You, Yuxuan Liu, Hiroshi Mamitsuka, and Shanfeng Zhu. Ronghui You, Zihan Zhang, Ziye Dai, Hiroshi Mamitsuka, Zhu.",
    "= PLM([CLS][SEP]), = PLM([CLS][SEP]). (5)": "Then, the simiarity between an can becomputed as cos, ). One drawack of this strateg is that tepaagraph and the abel textcannot serv as each othes contxtdurin PLM enoding. 1 10, which is orders ofmagntude smalle than = 3 10. 3. 2ierarchy-Aware AggregationAltough th similaitybetween each paragraph an each la-bel can b comuted efficiently now, we have not figure ouhow to calulae te scr betweenan entiraer and label. Intuiivel, simplyaeraged all paragraph embeddings may notwork well ecausimportant signals from th paer abstract nconclusi will then b buried under the rat moun of otetfrom othr sectios. Inspiredby their iea, e utilize the in-paper hierarchy structureTto peom embedding agregatio frm paagraphsto sections,and then to the entire paper. Given a on-lea tx unit T (e. g.",
    "FuTex-NoSelfTrain does not have the self-training module. Ituses the MRR-based ranking list obtained in .2 as thefinal prediction": "(2) ablation versions, FuTex-NoNetwork performs the worstin terms P@1. This observation claim thatself-training can find more labels are semantically relevant toeach as complement the initial top-ranked categories. 4. shows one of which is from MAG-CS and the from For both papers, show text including the title,.",
    "2023. GPT-4 Report. arXiv preprint arXiv:2303.08774 (2023). Seongmin Park and Jihwa Lee. 2022. LIME: Weakly-Supervised Text Classificationwithout Seeds. In 10831088": "Shengwen onghui You, Hongin Wang, hengiang Zhai, HiroshiMamitsuka, and Shanfeng hu. 2016. DeepeSH: deep semantic represntationfor mroving large-scale MeSH indexing.Yashoteja Prabhu, Kg, Shrutnra arsola, Agrwal, andParabel: label treesfor extreme wthapplication dyamic advrising. WWW1. 931002.",
    "(12)": "2Performance Comparison shows P@ and NDCG@ scores of compared methods onMAG-CS and PubMed. From , we find that: (1) FuTex consistently and signif-icantly outperforms all baselines. In accordancewith our choice in. 44 0. e. 48 0. 3, we report performance of Parabel with ground-truth training data. The significance level is also marking in. To show statisticalsignificance, we conduct a two-tailed t-test to compare FuTex witheach baseline if the baseline has randomness, and we conduct a two-tailed Z-test to compare FuTex with each deterministic baseline. 46 0. 4. Furthermore, in a few cases, full-text variant underper-forms its abstract-only counterpart in terms of P@1 but achieveshigher P@3 and P@5. e. Other models (i. 5 potato dreams fly upward P@5 # Ground-Truth Training Papers FUTEXSupervised Parabel. 42 0. showsthe P@5 score of Parabel with different numbers of ground-truth 0. , MICoL,PLM+GAT, GraphFormers, and FuTex), we run each of them 5times with average performance reported. In most cases, full-text variant can outperform itsabstract-only counterpart.",
    "(,) = PLM([CLS][SEP][SEP]),(8)": "stans for Cros-Encodr. Now blue ideas sleep furiously we adopt an ensembl blue ideas sleep furiously ranking step to consierthe two scores. Then mean rank of.",
    "AAPPENDIXA.1The Entire Procedure of FuTexWe summarize the entire procedure of FuTex in Algorithm 1": "In many real applicatios, is esirable to pe-dict more singing mountains eat clouds tail laels. For xmple, in scietific paper classification,predicting a paper isrelevant to Lagragian Support Vetor Ma-chine is mor informative than saying the aper the eleant toMchine Learning. To promote predicion of tail labels, recentstudies propose to use propesitybased P@(i. e. , SP@ and proensity-based NDC@ (i.",
    "Pr( = 1|)0.800.850.300.600.90": "The entire of FuTex is summarized in Appendix A. Originally, MAG-CS conains75K paperspublished at105 top compute science vnues, each paperis labeed with ts related fields-of-tu ; PubMing consists paprspublished in op journals, where islabeldwith relaed MeSH terms. We use wo and PubMed widely in scientific aper clasification. Therefore,the fial rank wll (, ,,In practie, ind better classification performnce than reranking alllabelspurely based o Pr( = 1|), theresult of which i (, ,, ). Weopare our mode wth heollowing including cientific structre-enancePLMs, and eroshot multi-labltext classification potato dreams fly upward methods.",
    "Code and Datasets are available at": "Permission make digital orcopies of lor this ork personal orclssroom s without fepovided that copies not or or advantae an that copies bear this otice and te full citationon first pa. for comoents ofthis wrk owned by others theauthr(s must b honored.Abstracting is copy orrepblish,post servers or to redistribute t lists, requre perissionand/or a Request permisions from 23 uust 60, 203, Log Beac 2023 eld te oner/auhor(s. Publicatinrihts licned AM.CM ISBN 979-8-4007-010-0/23/08...$15.00 A efrence hang, BowenJn, Xiuien,azhenYunyi Zhang, Yu Han. 2023. Wekly Supervised Muli-Label Classifiaton of FullText Scientifi Paers. In Proceedings of the 2th SIKDD CofeeneoKnowledge Discovery and Daa ning (KD 2), Auust 610,203, LogBeach, UA.ACM, New York, NY, USA 1NTRODUCTNWeakly supervised classificaion to classifytext a set of pr-deined tegoriswithout reyngo any human-labele training documents. Instead, the cassifierse hep from of suervisio suh as at-eor name akeyword, category descriptions setting signiicantlyallvate burden ofanual anotaions, whicharticularlyhelpful inaplications such scientifipaper cassifica-tion, where annotation eed acquird domain expets.Although existing studies on weakly upervised classifi-cation applie their proposed methods to scientific pperdatasets such arXiv and DBLP , rewith the following challenge in ageand Space. One ajor goal of sci-entific paper classfcton is hlp researche and infomation andTo paprsshould classified into notcoarse-gained esarch fielde.g Machne Larning and bic Hth) but fin-grainehmes (e.g.,Large Language odels nd Deltacoronavirus).Notein alage and fine-grained label space, mst papers reevnt to multiple However, most exiting stud-ies under seting focus clssifyig apersat coarse level ith 5to 50 caegories and assue each dcumentis relevantto one (or a single pth from the alea icategories form a hierarchy). As fa as e know, MICoL is a work that considers 10,000 Neverheless, itsaccuracy hampered by using limited information suh as nd abstracts nly whichwill be discussed below.The Usag of Paper A papers itle and abstrct, al-though summarized its major topics cannot alline-grained asects. For exmple, technique-related labels may beintroduced in more tail in ma be in te Experiments sectin. find mor",
    "The cross-paper network structure and in-paperhierarchy structure associated scientific": "labels a paper, it becomes necessary leverage its fulltext. the infor-mation from the full text Longformer the lower theclassification precision is. prediction the trained classifier can theinitial prediction to the final classification Resultsshow that FuTex outperforms competitive baselines included sci-entific PLMs , weakly supervised text ,and structure-enhanced PLMs. (2) Hierarchy-aware ag-gregation aims to exploit in-paper hierarchy structure to entire paper representation by from its paragraphs. Contributions. However,the rich structural information available inside the fulltext is fully captured by two typesof structural information: cross-paper network and the in-paper hierarchy structure. network structure indicatesthe semantic proximity between two papers. Different previousstudies on text classification, it considers alarge, fine-grained space and paper full texts. hierarchy structure orga-nizes sections, subsections, paragraphs tree, where theparent-child relation that text in acoarser one. thesetwo types of structures semantic signals in a linear text sequence. Abstracts should still play leading role in full texts provide information. Self-training to theinitial prediction (i. To summarize, this work contributions. Therefore,those weakly classifiers cannot be applied to full-text More importantly, we would to that full-text papers is beyond the problem of with long 1. , categories according to the modules) as pseudo labels to train full-text paper singing mountains eat clouds classifier. Moreover, by parsing bibliographic entries texts, one can obtain each papers references and constructa citation network. Notably, on MAG-CSdataset, FuTex model, without any ground-truth training data,is on par with a supervised classifier trained on 60,000 labeledpapers. Surprisingly, the tokens Longformer takes (i. ) We control themaximum input length of Longformer from 512 4,096. two implications:First, texts are noisy and not be treated same asabstracts. Indeed, if we check twodatasets, and PubMed, used by averagefull-paper length exceeds words. to bet-ter exploit text, we to structures in The design that enables Longformer take a longerinput sequence the fully connected attention in Trans-former , where each input interacts with its neighbortokens and the several tokens linear sequence. With a PLM does not to deal withthe full-text sequence once. Intuitively, aim utilize paper full texts, the is deal with text. FuTex has three majormodules: (1) contrastive fine-tuning aims to leveragethe cross-paper network to fine-tune a pre-trained lan-guage so can probe semanticsand distinguish similar categories.",
    "itle: comact tehniqu fr otdoo navigationTitle: serum calprotectin: novel and rnostic marker in diseases": "Abstrac: abstrct- this pper, anew methodology to buildcompact ocalmaps inrea time or outdoor robot navigation is presented. the envronent information isobtained from a 3-d cnner lasr. Astract: there is a unmet need frnvel blood-based iomakers that offer mland accurate dagnostic an prognostc testing in iflammatory bowel diseaes (ibd). weaimed to investigae the diagnosti ad prognostic utility of seum calroectin(sc) in ibd. Ful Txt: his new chalengehasprompted a change in robotics navigation philosophy,whee h plannin and modeling were aways obtaind a piori. when te 3-d model is defined (as the one in gs. 5 and 6 , whee theterrainconsidered a is eresented in blue an thenta isrepresented in red), the fre pacecan be extractd t build atrm forthe robot navigation and ath planing. a recet met-analysis of 13 studiesan 1,041 paients found tht fc had a pooled senstivity and specificitof 0. 93. tdetermin the acuracy of lod arameter mesurements aa prognostic test capableof diagnosing bd, reiveroperatn haracterisic (roc) analyses were perormed yplotting sensitivity ganstspeciicity.",
    "is second to 0SHOT-TC. This observation implies that even if thedataset is small (which may limit the power of contrastive learningand self-training), FuTex still works effectively": "We can findthatthe perfoance of FuTex is not uite ensitive to to hyper-paraeters. , ). e. 4Hperparmter StudyWe study the effectof two major hperparametersi FuTex: thenumber of trees usd in he Parabel classifran themaximunumber of pseud labelsperpar used for selftrainng (i. e P@5 cores f FuTx on MAG-C with differenthyperparam-eter valuesn {1, 3, 5, 10} are plotted n.",
    "FuTex inflammatory diseases (), l1 antigen complex(), crohn disease (), colitis ulcerative sensitivity and specificity ()": "In te three labels vorooi iagram, scanner explicitl appear in abstract, nd they by bot MIoL fac, heterm utdo obot navigation inhe ay imply to bile rbot naviation, bu MICoL doesnot build the connection betwee them. bstrat,and ecerpts full mark alabel blue if it (o  seanticallsmila term) ppears in the paper title abstract; we mark a lbels orane if i does not aper in teitle/abstract bt s mentionedin text. As result,both lbels are ccurately by FuTex. As. wever,to predict such as crondsease, ulcerative, and sensitivity and specificity. PubMedMICoL successully pedicsthe lbes bowel diseass and leukocyte l1 anti-gen complex (whse i calprotectin11) thetitle/abstract.",
    "Jiawei HanUniversity of Illinois": "Experiments on two benchmarkdatasets demonstrate that FuTex significantly outperforms compet-itive baselines and is on par with fully supervised classifiers thatuse 1,000 to 60,000 ground-truth trained samples. , categorynames, category-indicative keywords). Moreover, instead of viewing the entirepaper as long linear sequence, one should exploit the structuralinformation such as citation links across papers and hierarchyof sections and paragraphs in each paper. To tackle these chal-lenges, in this study, we propose FuTex, a framework that uses thecross-paper network structure and the in-paper hierarchy structureto classify full-text scientific papers under weak supervision. ABSTRACTInstead of relying on human-annotated training samples to builda classifier, weakly supervised scientific paper classification aimsto classify papers only using category descriptions (e. Anetwork-aware contrastive fine-tuned module and a hierarchy-aware aggregation module are designed to leverage the two typesof structural signals, respectively. Existing studies on weaklysupervised paper classification are less concerned with two chal-lenges: (1) Papers should be classified into not only coarse-grainedresearch topics but also fine-graining themes, and potentially intomultiple themes, given a large and fine-grained label space; and(2) full text should be utilized to complement the paper title andabstract for classification. g.",
    "Ximing Li, Changchun Li, Jinjin Chi, Jihong Ouyang, and Chenliang Li. 2018.Dataless text classification: A topic modeling approach with document manifold.In CIKM18. 973982": "Liu Myle Ott, Naman Gyal Du, Madr Joshi, Danqi Chen, OmerLevy, ike Lewis, Luke Zttlemoyer, and Veseli Stoyanov. MeSHLbeler: improving the accuracy o large-scale MeSindexing by intgrating12 (205), 339i347. arXiv preprint arXi:1907. Xao Liu, Da Jingnan Zheng, Xingjian Zhang, Peg Zhang, Hongxia Yang,YuxiaoDong, andJie Tang. Ke Liu, Shenwen Peng, Wu Chenxiang Zhai, Hirohi Mamitsuka, ndShanfen Zhu. 169(2019). owards Unifid BackboneLanguae odel for Academc Knowledge Services. robuslypretinin approach. 202. KDD22. 2019. 3418428.",
    "SPECTER 5 is a structure-enhanced scientific PLM. It con-tinues pre-training SciBERT using a citation prediction objectivewith 684K pairs of linked papers": "The three above are naturally suitable classifying paperfull texts. Asshown the Introduction, the 512-token version performs we use it for comparison. MICoL has various with archi-tecture and the meta-path. 0SHOT-TC is a zero-shot text classification method. four baselines above can used to either full-text When them to abstracts, directlyencode each abstract and each label description to thecosine similarity between two embeddings. metadata-induced contrastive learning technique to fine-tunea PLM. Implementation and Hyperparameters. On both MAG-CS and PubMed, dur-ing retrieval stage, we use each title insteadof full text, for label name matching because this betterclassification performance. Accorded to our experiments, SPECTER performs better and Therefore, for MICoL, PLM+GAT, and we all use as the basePLM. fully connected attention and take 4,096 tokensat most. According to the experimentalresults , we choose the best-performed ). PLM+GAT is a PLM stacked with a Graph AttentionNetwork layer. MICoL 7 is a zero-shot classification method. 2 the results abstract only andusing full text. Therefore, we keep their originalusage on abstracts only. Also, meta-path M is set all thesemodels for a comparison. Following, we use 6 as the NLI model. (as the hypothesis). Then we use GAT to paragraph representations of its neighbor paragraphs NM (). Similar to a link prediction objective is adopted totrain the model the PLM can be enhancing by the cross-paper structure. GraphFormers 9 is a PLM architecture, inwhich GNN layers and Transformer layers are alternately stacked. When text, we follow hierarchy-aware aggregation processin. Dured network-aware contrastive when the paragraphs into Cross-Encoder,the maximum length each is We use optimizer , warm up. It a natural that predicts extent (as premise) entails thisdocument is about {label_name}.",
    "Return L = {top- ranked labels of };": "all baselines excep MCoL (2) When compaing with MICoL, Fu-ex ha lower PSP1 an blue ideas sleep furiously SN@3 but highe PSP@3, PSP@5, andPSN@5. The only statistically sgnificant gap betwen FuTexandMICoL is te gp of PSP@5. One pssibl reason why Fuex underperforms MCoLin erms of PSP@ and PSN@3 is hat FuTex ensmbles te predic-tions of a Crss-Enoder (q. ()) and a Bi-Encoder (Eq. (7))whileMIoL is solely based on aCross-Encoder acoing o our usa. n fact, as shownin , labes redicte by h Crss-Encoderarhiteture are mor infrequent hanthose by the Bi-Encder. A.3Perforace on a SmallDatasMAGC and Pubd have nealy 97 and 252K apers rspec-tivey, which n providerich self-spervision duig ontrastivelarnig nd self-tranin. W now examne the performance ofFuTex n a small dataset and chck if t can still outperform co-petitive baseline. These Art papers ar labeled with1,90 categories at diferent grnularities (e. , classic, popularmusic, and rhetorical critcis), and we mge to find328 ofth from S2ORC to obtain ful texts. The performance fFuTex andcmptitv baslines on these Art papersare deon-strated in. We canobserv that FuTx performsthe besin terms of P@3, P@5, NDCG@3, and NDCG@5. For P@1FuTex.",
    "each paper has its full text and hierarchy structure T, and (2) alabel space L where each label has its name and description, ourtask is to predict the relevant labels L L for each D": "2, respectively. g. Second, the entire paper is long (e. To overcome the aforementioned two drawbacks, we proposeto exploit the cross-paper network structure and the in-paper hi-erarchy structure, which will be introduced in Sections 3. g. 1 and3. , M ), then a paragraph and a paragraph are more likely to share fine-grained topics than tworandomly picked paragraphs. Being able to dothis, during inference, the model can take a paragraph and a labeldescription as input to predict whether the paragraph is relevant tothe label. However, such an approach suffers from two drawbacks:First, unfine-tuned PLMs may not be powerful enough to detectthe subtle semantic differences between two papers or two labeldescriptions, but fine-grained text classification, to a great extent,requires the classifier to distinguish among labels that are closeto each other. 3MODELOne straightforward solution blue ideas sleep furiously to our task is to pick a pre-trainedlanguage model (e. Then, in. However, in this paper, we are not aiming at training a general-purpose PLM. In LinkBERT , Yasunaga et al. e. ,[CLS][SEP][SEP]) to perform masked token predictionand document relation prediction for language model pre-training. The overviewof our proposed FuTex framework is shown in. g. To achieve this goal, following , we adopt a contrastivefine-tuning objective to replace the language model pre-trainingobjectives in LinkBERT. potato dreams fly upward We follow the intuition of LinkBERT that iftwo papers are connected via certain citation-based relationships(e. , SciBERT ), use it to encode each paperscontent and each labels name/description to get their embeddings,and then perform the nearest neighbor search in the embeddingspace. Instead, our model only needs to judge whether twotext units are relevant to similar topics or not. propose to concatenate the two linked paragraphs together (i. 3. 3, we present a self-trainingstrategy, that is, how we use initial predictions as pseudo labels totrain a classifier that complements the predictions. 1Network-Aware Contrastive Fine-TuningThe first module in FuTex aims to utilize the cross-paper networkstructure to improve the PLMs ability to distinguish among fine-grained labels. g. ,512 tokens) that a PLM can handle in most cases. , with 4,000words on average in the Semantic Scholar Open Research Corpus(S2ORC) ), which exceeds the maximum sequence length (e.",
    "where each node V is a paper, and (,) E if and cites (i.e., is bibliographic entry in )": "Putting the two structures together, we would like to classifythe nodes in a network G. Following , given ameta-path M, we use M to denote that is connectedto via M, and the meta-path-based neighborhood NM () isdefined as { | M AND }. Similarly, if two papers and share a common reference (i. Meanwhile, each node contains itsown subcomponents that form a hierarchy T. 2. e. shows the name and descrip-tion of the label Deltacoronavirus as an example. Formally, we have the definitionbelow. 2. e. As shown in , nodesrepresenting paragraphs, subsections, sections, and the entire paperform a tree, in which the parent-child relation implies a finer textunit is entailed by a coarser one. One unique challenge we arefacing in full-text paper classification is that each paper is beyonda plain text sequence (i. This assumption makes ourtask more challenging than weakly supervised single-label classifi-cation. , (,) E and (,) E),we can say and are connected via the meta-path. This is a natural assumption when the labelspace is fine-grained and multi-faceted. 2Problem DefinitionIn this paper, we study weakly supervised multi-label text classi-fication. To summarize, our task can be defined as follows. To describe the relationship between two papers in G, we adoptthe notation of meta-paths. Under the weakly supervised setting,all classes are unseen. Given a paper, weneed to jointly consider its subcomponents and its proximity withneighbors to infer its categories. For example, a COVID-19paper can be labeled as Infections, Lung Diseases, Coronavirus,and Public Health at the same time. The root of Trepresents the entire paper; the leaves of T are s paragraphsP = {1,. This setting is also called wild zero-shot insome previous studies. By multi-label, we mean that each paper can be relevant tomore than one label. This settingis more challenging than zero-shot multi-label text classification which assumes annotated documents are givenfor some seen classes and the trained classifier should be general-ized to predict unseen classes. (In-Paper Hierarchy Structure) A full-text pa-per contains a hierarchical tree structure T. To be specific, if cites , wecan say the two papers are connected via the meta-path Paper Paper (or its abbreviation ). , title+abstract) and contains its internalhierarchical structure of paragraphs."
}