{
    "In the final stage, the whole model undergoes an end-to-end fine-tuning forfurther fitting with the dataset": "1%of initial learning rate, and the half-period o thecosine function is determined as Na 1. The minimum learning rate of the cosine annealngschedulr is set to 0. Thswrm-p peiod is folowed by cosine anneal-ng scheduler for Na epochs. For each stage,we propose employing linear learning rate armup for Nwpochs commencing at 1% f the initial learning rate Additionl training detailsre summarized in. Once quantization-aware trainingprcess is cmleted, e evaluat thchekpoint of ach epoc on the evalua-tion dataset and select the best-performin one. Additional training details aresummarized in.",
    "Disclosure of Interests. The authors have no competing interests to declarethat are relevant to the content of this article": "H. , Mao, H. abs/1712. PMLR (2021) 2 2. : post train-ing quantization small calibration sets. , Dollar, P. , Nahshan, , Y. , Nguyen, H. Banner, R. :MedficientSAM: A robust medical model with inferencepipeline for limited clinical settings. 1. In: Submitted to CVPR Segment Any-thed In Medical Laptop (2024), review. , Mintun, E. , Lo, W. , Berg, A. T. Rolland, , L. Y. T. S. , Tran, M. K. pp.",
    "Introduction": "Drawing inspiration from remarkable achievements of foundation models innatural language processing, researchers at Meta FAIR introduced a versatilefoundation model for image segmentation, termed the Segment Anythed Model(SAM). Despite the consider-able scale of the dataset utilizing to train SAM (referred to as the SA-1B dataset),comprising over one billion masks, the models performance fell short in med-ical image segmentation tasks. This shortfall can be attributed in part tothe composition of the SA-1B dataset, which primarily comprises photographs ofnatural scenes captured by cameras, thus lacking the nuanced features character-istic of medical images. In response to this challenge, Ma et al. curating a diverseand extensive medical image segmentation dataset encompassed 15 modalities,upon which they fine-tuning SAM. Their refined model, dubbed MedSAM,represents significant step forward in addressed this discrepancy.",
    "LiteMedSAM5.7M6.2K4.1MMedSAM89.7M": "Quantization offrs severalbeefits, icluding reducing parameter sies, inferne speed, and de-crasing pwr consumption inference. Tr are primary paadigmsforqantizing netorks: post-trinin (PTQ) and quntiation-aware training (QAT) involves coverting pre-rained model ino a lowrecision one by albratig us batcho calibration data. Tis methd is generally faste sinceit des not rquire re-training d th precsion he model largelydepndsn the calbrato procss. O otherhand, QAT iteraes nto the computationalgrah, eabing thetraining ofthe model while preservinits accuray qunization. To nrepreiction ccurac, we QATto quantize AM. The attention blocks serve the principal in thebackbne SAM. introduced n information and schem ailored fo fully uantizedvison ansformer. Lu et al. that incorporainginto he values beingquantized can quantizatinr-rors under provabl conditions.",
    "University of Electronic Science and Technology of China, Chengdu, China{luhaisheng, fuyujie}@std.uestc.edu.cn, {fan.zhang,lezhang}@uestc.edu.cn": "Nevertheless, critiques highlight MedSAM de-mands substantial computational resources during inference. To addressthis the CVPR MedSAM on Laptop Challenge was estab-lished to find an optimal between accuracy and processing speed. In this paper, we introduce a training pipeline de-signed efficiently quantize Anything for medicalimages deploy the OpenVINO inference engine. Our experimental resultsconfirm this approach considerably enhances speed overthe baseline, achieving an acceptable accuracy level.",
    "QMedSAM5": "Common quantized quantizing linear layer; (b) quantized convo-lutional layer; (c) quantizing attention block. Circles in represent correspond-ing calculations: M matrix multiplication, C for convolution, and for transpose. The inputs and output all the sub-layers depicted * \"S in the figureare tensors.",
    "Lu et al": "In:Proceedings of the IEEE/CVF Conference on Computer and Recog-nition. 49424952 (2022) 4. : Noisyquant: Noisybias-enhanced post-trained activation quantization for transformers. Huang, Xing, P. , Dong, Z. pp. pp. : accurate quantization via es-timation. 2 7. In: Proceedings the IEEE/CVF conference on computer andpattern recognition. Liu, T. , K. , Shen, Z. , Yang, H. Zhang, S.",
    ". Ma, J., He, Y., Li, F., Han, L., You, C., Wang, B.: Segment anything in medicalimages. Nature Communications 15(1), 654 (2024) 1": ", Gong R. ,Yan, J Once uatiztion-are raining:High perfrmance extremely lw-bitar-chitecture search. , Hutter, F. , Lin, C. Yu,. ilinx/brevitas. T. , Liang, F. pp. , Li, C. In: Submit-td o CVPR 2024: Sgment Anything InMedical Imges On Laptop (2024), under revew4 13. ,Prucker, L. Pfefferle, A. 22. Pppalardo, A.",
    "Quantized LiteMedSAM inferenced on OpenVINO0.585s": "The results tht te moel does exhibit te fastestrunme. This isbcausetht ourhardwarei not fr uantized op-erations, in execution compared to standard floaing-point For cmparson the inernce peeds of both floatigpointand quantized versins f (which is subsantially LiteMed-SAM provided in. Interestingy, ths the qantizd modeloutpeoms loating-point odl.",
    "Qualitative results on validation set": "illustrates two chal-lenging cases. It can beobserving * \"S that the proposed quantized model performs better in matching theROI than floating-point counterpart.",
    "Ablation Study": "In this we propose an ablation study to explore the balancebetween efficiency and accuracy. describe variation from different modalities clearly, wewill Ns(m) to represent number of samples from modality m. number of samples from each modality,especially from the larger modalities, certainly saved training However, it still raises questions about its influence on precision of the quan-tized model. strategy the proposed method be described. Training a Segment Anything Model from scratch requires mass However, the proposed quantization-aware training procedure witha pre-trained model.",
    ", miniM Nm(i)": "* \"S The metrics the hree stages the alatio study re summarized i. Compared (w provide average metrics of the pro-psed in the row of ), results idicate that icreasing Nsdoes not in a significantimprovement, * \"S te efficiecy of theproposed pipeline in ters of training time.",
    "Proposed method": "evry QAT method ocuses on inputs an weihtsduring matr such as liear lays, convolution layers, andattntion blocks. In gradients are passed through a hreshold becomeouging gadients This factor is initialized from runtiestatistics. Whie te quantiza-tion of layers canbe in our mode, opt toretain all these layers as floatingoint,only matrix theimage eoder and ma being most common quantied sub-strcturs are illustrated in. cotrast, involving biases, lyers, layers ar ypcally per element. Wepropose to qunize the basine LiteMedSAM QAT hle neu-ranetworks consist of various cmponentsbeond just multiplicatios,its within tese operations that peak of computationa complexity resides. Since quantization is nondferentiable, e employ the straight-through es-timator (STE methodology, as in preious works.",
    "Inference and ost-procssing": "Upon o uantizaion-aare Brevitas provds exceptinaltoolchais for exprting quantizd modlstodiverse bacends.Whilestandard QuantizeLinerDeQuantizenear (QCQ) * \"S quantization in ONNX Brevias has extended his to QuantzeLinear-Clip-DeQuantizeLinear(QCDQ)extesion, researcers can onfinete range of quantze Therefore, we propose exrting he quantizedLiteMedSAM to the QCDQ umerous inference engines support the ONNX format, not all of hemare copatible Given tat the challenge mandates CPU inference,we narrow ptions to Rutime and OpnVINO. expermenoninference * \"S speing between tese ieene engines is .1.Basing reults, we for OpenVNO. Modelcachig is alsosupportd by OpenVINO. stregy can reduce the resultingat ppl-cation startup, maked i considerablysuitable for accelerating in this .",
    "Metrics ndloss functions": "In the training we mainly employ combination theDice loss and focal. The accuracy of the model is evaluated using the Similarity Coefficient(DSC) and the Normalized Surface Distance (NSD), efficiency time analysis. These are collectively utilized * \"S ranking.",
    "Inference speeds of different engines": "challenge evaluates models on an Intel Xeon W-2133 CPU we use an Intel Core CPU () offers comparableperformance because we not have environment. The inference speeds of variousmethods are detailed in. We eachvariant with a single box.",
    "Conclusion": "In * \"S present an efficient for quantizing LiteMedSAM anddeployed it on the OpenVINO inference Objective experiments haveconclusively our method accelerates the baseline whilemaintained an level of accuracy. Future endeavors will on en-hancing the speed of the floating-point backbone, alleviating the im-balance across different modalities, and deploying quantized model on cus-tomizing hardware platforms."
}