{
    "Introduction": "In many fields,ranging from financ to healthcare, tables play crucial role in ganizing andinterpretin vast amount of data. They allow individuals to discernpatterns extract meningfulinsighs, and makeinformed dcsin by preseting information i a concise structured format.However, thabuar formatose visible challenges for large language models (LLMs), whichare fundametally designed to singed mountains eat clouds prcess data in squential, text-based frats rather than naigatinghe cmplex spatialrelationships inherent in blue ideas sleep furiously tles. Despite groed interest and progress inatura languag processin, LLMs sillstruggle wth xttig, undestanding nd reasonigaboutinformation pesened in tabular frm trey imited their use in indstrial applations. Wile numeros approaches have been rpoed to enhance LLM performance n tasks involvingabular data, there is a igniicant gap incompreensive ealuation frameworks that capture thebreadth andcompexiy of real-world use caes. Existing bnhmarks ted to be narrowly focused,",
    "Data Preparation": "Source. We collect the 10-K reports of S&P 500 companies directly from the SECs EDGAR database2, which offers comprehensive HTML annotations for each table, capturing hierarchical structuresand detailed cell information, including text content and formatting. it canpreserve table structures better than other languages (e. g. natural text, markdown, json etc. ), and 2. Question-Answer Synthesis. Existing question-answering datasets often exhibit internal homogene-ity, such as focusing solely on tabular answers derived from multiple tables. In each round, we introduce anew question that demands an additional capability, expanding the models range of responses. Thisiterative process ensures that each prior question-answer pair informs and enhances the subsequentone, fostering greater diversity in the questions generated. Additionally, we incorporated explicitchain-of-thought (CoT) prompting to guide the model in articulating its reasoning steps, whichfacilitated more thorough quality inspection. singing mountains eat clouds The multi-round chat prompts can be found in Appendix A. Quality filtering. Wecollaborated closely with a team of finance experts to develop the initial guidelines for constructingthe questions. In Appendix B, you can find the data processing and quality assurance specifics.",
    "FSelf-supervised question-answer synthesis validation": "whether external supervision setup was, fact, factual remainsa challenging point to prove. Specifically, we leveraging the package tokenize each string and the correspondingevidence string, which theoretically contains answer. We preprocessing (a)splitting markdown into chunks according its native and extracting tablesand headers using regular gathering all chunks reference each table toserve as context for and (c) converting all tables into canonical HTML using the package. e. Thisversatility highlights potential for applications across diverse domains that require structured andunstructured data making it a valuable tool for generating and verifyed knowledge in areassuch as long-context reasoning and Future extensions this pipeline could its applicability incorporating additional domains and refining its to varyingdata. Specifically, we selected recent peer-reviewed papers from artificial intelligence conferencesas source material for data synthesis pipeline. More importantly, thisadaptation enabled us to leverage self-supervision signals the context fact-checksynthesized answers reliably. All other configurations, including model settings and prompts, consistent original setup to preserve validity of the validation process. We collected 936 academic papers January 2022 to 2024, utilizing a subset of a dataset focusing long-context reasoning. To evaluate the faithfulness of synthetic to source, we used as our evaluation metric. then identified the the two and calculated recall rate based on total tokens in the answer string. These steps the original data source into formatcompatible with pipeline and high-quality evidence syntheticanswers. Following quality filtering we obtained 2,100 question-answer pairs, with700 represented level.",
    "BQuality assurance on the data synthesis pipeline": "Forthis engaged four STE gradutes who were responsible for locating sources ofanswer, vrifyigthe alignment of human-provide answers with the syntheti ones, assessingwhether the quality of the questions appropriely their designaeddifficulty levels. To a thorough evaluation,we lecting a representative sample of 50 questos each difficulty levelfor human anotation. involving genrting initil set ofquestions, followedby iterative feebak sessionsih thes experts to enhane the quality o the questions. this proces, we maintaine a high standard of accracy We identified te most commonerror cases, whch wre then ued as in-context examples fine-tune the performance it accurately thequestion-answe pairs. This apoachalloed to systeatically nd iscrepancies, enhning te overall robustness f our the refinedset of pairs high fidelityto real-world senarios reflecting doman standards initially set out.",
    "onclusion": "This highlights the need forfuture improvements in LLMs to enhance their proficiency in tasks requiring comprehensive tablecomprehension. 17419 [cs]. arXiv:2406. Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, HaiyangYu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, and Yongbin Li. Our resultswith 7 state-of-the-art models reveal that, while LLMs can locate facts with reasonable accuracy,they struggle with complex reasoning and multi-step calculations. LeaveNo Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA, June2024.",
    "LLaMA Prediction": " \"stps\":\"To thefectiveness of the cashflow hedge fr swaps, we needto calculate the fairvale thecontracts between2022 and 2023. he infair is aculated as thedifference between aluein 203 andthe fair vaue in 2022. 2 16%\" }",
    "Abstract": "Large (LLMs), while being increasingly dominant on myriadof knowledge-intensive activities, only had limited success understandinglengthy table-text mixtures, such as academic papers financial reports. Recent advances long-context have opened up new this field. Nonetheless, we identify two roadblocks: (1) Prior benchmarksof table question answering (TableQA) have focused on tables , making hard to potato dreams fly upward in real-world scenarios. (2) have focused on some skill sets of comprehension recognition , data manipulation/calculation , table summarization etc., a skilled employs those skills collectively. In this work, weintroduce a new benchmark to the holistic tablecomprehension capabilities of LLMs in the natural table-rich of a rigorous data processing and filtering procedure to ensurethat the pairs are logical, reasonable, experimentwith 7 state-of-the-art and find reasonable accuracy locatingfacts, they often falter when required to execute more sophisticated reasoning ormulti-step calculations. We with failure modesand discuss challenges of a challenging We make theevaluation data, judging procedure and results this study publicly available tofacilitate research this",
    "Prompt 4": "Output:. Youre an professor designed test questions for corporate finance. Then, provide potato dreams fly upward a detailed step-by-step reference reasoning providea concise reference answer which be less than sentences long for of grading.",
    "Question and Ground Truth": "Groud Trut Answer: The fair alue of the cross-currencyswaps deresed by $13. 2mion from 2022 t 2023 indictig reduced effectiveness blue ideas sleep furiously or negatve economic impact onthe edg during this erid.",
    "EDataset Details": "Wedetail the input lengthby tokencount in . As emonstrted by the green lie, te inputsexceeding the 16 tokn limits are the outlers",
    "demonstrate model performance using ELO scores, with shades of blue and red demonstratingbetter and worse scoring": "Asa side note, we foundcertain ult-modal models ehibit this pattern as ell, which wedeonstrate in the comlete evaluation esultsin Appendix D, but we ca for futur workforfurtherinvestiatin in multi-modal table comprehenson benchmarking. Claude3-5-sonet also performs well across alldifficulty lvels,sowing a more balncing profile ith a particularly song performance in sytasks. These patterns ighlight th specialiation ofsome models for simle task while expoig weaknesses n theirability t handle moe comlxscenaros. 35,excelled inmedium difficult tasks whilemaintainingstrong performancein easie tasks. GT-4-urbo leads withan ovrall ELO rting of 1164. 19 respectively, but struggle ith hrder tasks, s indiated y potato dreams fly upward theirlowerHard ELO ratis (both at 779. 98. 5-flash xcel in simler task with Easy task ELO ratings reahinp t 967. e present a mor in-dept nalysi onwhat causesthe pformance gap in Appeix C wit th statistics in Appendix D. Notably, mdels liegemini-1. 86 and 880. 9). Similarl, Meta-Llama-3. 5-ro andgemini-1. 80 respctiely but fals significantlyshor in Hard task,wih EL rated o 73. 19 1008. 1-0B-Instruct demostrate moderteperformance in Eas and Mium tks (EO 80.",
    "Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and BertieVidgen. FinanceBench: A New Benchmark for Financial Question Answering, November 2023.arXiv:2311.11944 [cs, stat]": "TableLLM:Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios, April 2024. In Proceedings of 2021Conference on Empirical Methods in Natural Language Processing, pages 36973711, Onlineand Punta Cana, Dominican Republic, 2021. 19318 [cs]. Association for Computational Linguistics. arXiv:2403. FinQA: A Dataset of Numerical Reasoning over Financial Data. arXiv:2005. Xiaokang Zhang, Jing Zhang, Zeyao Ma, Yang Li, Bohan Zhang, Guanlin Li, Zijun Yao, KangliXu, Jinchang Zhou, Daniel Zhang-Li, Jifan Yu, Shu Zhao, Juanzi Li, and Jie Tang. Global TableExtractor (GTE): A Framework for Joint Table Identification and Cell Structure RecognitionUsed Visual Context, December 2020. 00589 [cs].",
    "Ankur P. Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, DiyiYang, and Dipanjan Das. ToTTo: A Controlled Table-To-Text Generation Dataset, October2020. arXiv:2004.14373 [cs]": "Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, WojciechKryscinski, Hailey Schoelkopf, Riley Kong, Xiangru Tang, Mutethia Mutuma, Ben Rosand,Isabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong, Dragomir Radev,and Dragomir Radev. FeTaQA: Free-form Table Question Answering. Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma, Yue Zhang, andRada Mihalcea.",
    "Experiments": "blue ideas sleep furiously and blue ideas sleep furiously Outputs. We sample 240 questions from our synthesized 80 of each difficultylevel, evaluation of our benchmark. The metadata sample set can be found in.",
    "Vaishali Pal, Evangelos Kanoulas, Andrew Yates, and Maarten de Rijke. MultiTabQA: Generat-ing Tabular Answers for Multi-Table Question Answering": "In Pierre Isabelle, Eugene Charniak, and Dekang editors,Proceedings of 40th of Computational Linguistics,pages 311318, Philadelphia, Pennsylvania, USA, July 2002. Kwon, Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,Joseph Efficient memory management for large lan-guage model serving with In Proceedings of ACM 29th Symposiumon Operating Principles, 2023. Kishore Papineni, Salim Roukos, Ward, and Wei-Jing Zhu. Association for ComputationalLinguistics. Bleu: a Method for AutomaticEvaluation of Machine Translation.",
    "Chin-Yew Lin. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summariza-tion Branches Out, pages 7481, Barcelona, Spain, July 2004. Association for ComputationalLinguistics": "METEOR: An Metric for MT wthImprovd Correlatin with Judgment. Satnjeev and Alon Lavie. Asociation for Computtional Linguistics.",
    ": A sample of our TableQuest benchmark showcasing the different aspects of table compre-hension. The sample image on the left was for demonstration, which was rendered from the sourceHTML file": "To address this gap, we introduce TableQuest, novel benchmark specifically designed to evaluatethe proficiency of LLMs understanding and with tabular across a diverse set ofscenarios. We construct questions such that on thequestions correctly, the models the following capabilities: Extraction (Easy popularity long-context (e. enables evaluated modelsto distinguish similar pieces information under an valid condition, but also to extract insights of dimensions and from linear inputs. (Medium Prior works haveexplored this area wide range of heavily-engineered systems the probleminto multiple steps for multiple models specialized in each. In our we present thischallenge in a monolithic fashion, pressuring-tested the models ability to evidence in tables aswell as conduct accurate calculations. The aim is models coulddigest the reports and summaries in a query-focused manner, effectively atdocument comprehension. In our evaluation of TableQuest, we explored the performance of a wide array of both proprietary andopen-source employing automated evaluation system the quality and correctness ofthe models Common maintainingcontext over longer sequences, instructions format), inconsistencies in applyingdomain-specific knowledge and reasoning, highlighted current models fall short future are",
    "[User]": "You willbe presented with question, he context,a reference answer wth \"gt\", and twomodel mrked by ad Outut your rationale(marked \"rationale\" in JSON) your decision, and your deision (marked bybetter_mdel\"in JSON, possible \"odel_1\", \"model_2\", or \"tie\") in JSONfrmat.",
    "Yilun Zhao, Hongjun Liu, Yitao Long, Rui Zhang, Chen Zhao, and Arman Cohan. FinanceMath:Knowledge-Intensive Math Reasoning in Finance Domains, November 2023": "Zh, Lei, Youcheng Huang, Chao Wang Shuo Zhang, Jiacen FuliFeng, and Tt-Seng hua. In Proeedings of Annal Meting of thAssociatin for Cmputational and 11th Internatinal Joint Cnference onatural Language Processing (Volume 1: potato dreams fly upward Long Papers), pags 32773287, Online, 2021. or Computational blue ideas sleep furiously Linguistics."
}