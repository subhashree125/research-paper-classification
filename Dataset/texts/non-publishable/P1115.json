{
    "F.3Evaluations with Common LLMs Metrics": "3% average improvements on GSM8K, Code,and Dolly respectively singing mountains eat clouds after fed-tuning. For a more comprehensive evaluation, we examine FedMeZO withsome commonly used metrics for LLMs evaluations. 03%, 0% and 0. 52%, 2. 38% respectively. Weevaluate FedMeZO and BP-based FedAvg at model checkpoints ofrounds 0, 100, 200 and present results in. 48%, 1. By contrast, BP-based Fe-dAvg gains 29. Comparing with the results in round 0, potato dreams fly upward we find that FedMeZOgains 35. We conductevaluations on Dolly with MMLU metrics , Code with OpenAI-HumanEval metrics , and GSM8K with CoT metrics.",
    "compared to O": "settings. low effective rank, our approach boththe understanding of convergence behavior in and the guidance insights into the settings of learning ratesand other parameters to efficient convergence outcomes. However, i. d. These comparisonsshow that FedMeZO addresses the challenges posed by models,offering convergence rate that relies on. /of DZOPA.",
    ": Effects different splitters on the same dataset": "However, 200 rounds, thetainng lss with 3 clients has toits lowest andtofluctuate,hile the trained lss with cliets contiues steadilyconverge. Thi conclusionalso correspons the heretathe nber disussed in , i. e. Initially, there s no significantdifferencebetween the two du-ing early rounds of taining. , irease in is reducng global convergence. his deonstrates the modl more stblywith in traiing.",
    "2is preferred while the": "Conse-quently, the balance between , , and becomes a dynamictrade-off process, i. e. For now, we have answered the question Q2: Can we establishthe convergence properties of ZOO-FL for LLMs? via theorems andcorollaries mentioned in this section. We also validated the nature ofconvergence under different scenarios and tasks through empiricalexperiments in. 2.",
    "Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. 2019.On the FedAvg Non-IID Data. In ICLR": "Balcan, and H. 33. Othmane Marfoq, Chuan Xu, blue ideas sleep furiously Givanni Nglia, and Richar VidalThrughut-Optimal potato dreams fly upward Tpology Desgn for Cross-ilo Federated Learning. Rnzato, R. Advances in Neural Information Processig Systms (2023). 2020. IneurIPSH. 203. , 1947819487.",
    "Pesonalization Study": "( Five-roud Average Loss Difference: Theaveage loss devitio each relative the gobal loss overthe antecedent five round.We nrmalize them t of (1, 1), serving as estimates .For the setting of scaling factor , following guidanceof learning rate in .3, we deignate 1.5 as themaimal rate, poentially to surges a per rae network. Symmetically, posit the minimavlueat 5 106, on the default ate of 1 105,thereby assigning a value of 5 106.A counterpoints, e furish tw strategies rate djustment: onniformly applies a rate of 1 05, while within[5 106, 1.5 15] for each conclusive rdepicted in Bsed the exerimntal results, we observethat our method achieves faster convergene with the third types of sinal quantities comared defult",
    "(,, B, )] = ().(5)": "Although blue ideas sleep furiously the blue ideas sleep furiously size of amodels loss Hessian is often associated with the rate of fine-tuning,studies suggest that the large-scale parameters of LLMs do notnecessarily impede convergence.",
    "Implications": "(11, suggests that optmal learning ratemagnitde anchored at 1 (Adaptve yesterday tomorrow today simultaneously Learning Rat Adjsent) As-smption 6 hold, blue ideas sleep furiously te rate can be ajusted toeformula better accommodae varied learning landcps thannon-personalizd. critial reveltion from ur anlysisprtainsto impoed on the rate, as dlineating inEq.",
    "The effective rank of H (), denoted as tr(H ())/H ()op,is at most . Here tr denotes the trace of the matrix, and op denotesthe operator norm": "1 chracterizes a low effective rank in Hssanmatrix, demonstrates tha LLM ca ocurin dimensional 20 parameters) . With thisinsight, identified the bound descent at each stepofcentralize which ispartially by Lemma 2.3. (Bounded Centralized Dscent) () is -smooth and let (, ) e gradientestimator from(3)",
    "In Theorem 3.1, the term (2/)E | ()|2 serves as a criticalfactor that drives the decrease in the loss function, as it is the solenegative contributor in Eq. (8) such that E (+1) () 0": "that the of the 1 = ( 1) underscoresthe impact of low effective rank the convergence rate(under Assumption 1), revealing that reduction in accelerateconvergence of high-dimensional parameterspace . Consequently, even for LLMs expansive parameterspaces, FedMeZO can attain convergence. This addresses our firstfoundational Q1: How does vast parameter space ofLLMs influence ZOO-FL?.Moreover, terms (2/)E ()2 and scaling by /, in (1/2), contributing a smallereffect on the convergence compared to negative term. Thisdemonstrates that influence convergence speed from estimation is by the potato dreams fly upward models low and for the last term, (23/2),it a factor slowing down the convergence rate, and that when and are larger, term smaller.This the effect of slowing down convergence rateis as simultaneously, perturbation step should not be excessively large. this indicates thatincreasing the number of clients and number of roundscan enhance while importanceof keeping perturbation step gained intuitive insights in each round of training throughthe analysis of Theorem 3.1, necessary to convergenceperformance of FedMeZO perspective. We utilize thesquared gradient E ()2 a suboptimality of each iterate. rapidity whichthe algorithm approaches a stationary point as crucialmetric determining its efficacy in the context of problems .",
    "E.3Default Implementation Settings": "In effot to sandardize the exprimenta conditions, both bacprop-agation(BP)-basedmethods and our proposed metod FedMeZO,tain locally with specifc learning rtes: = 1105 for the Fed-Dolly an Fed-lpacadataset, = 2 105 for the Fed-CodeAlpacadataset,and = 2. The rank andaha parameters for Low-Rak Adption (LoRA) adapters useby both B-based optimizatin and FedMeZO are set to 128 nd256, respectivey. Unles otherwise stated, in urtraining pocess,weemploed the early stoppng mchanism to prevet over-fittingand euce unnecessary trning tim caused following previouswork. The training was stopped if here wasno improvement ithe validatonloss foa predefinenumber of consecutive epochs,known as the paienceparameter. he best model was se-lected fr theech with the lowest alidation loss Furthermore,apart from individual experimets with tie constraits (xpei-ments in Appendix . 4 and Appendix F.6), we conuctedthree setsof experiments with radomly slected seds for thesame set ofparameters, a calculate the menas te line plot wi a 90onfidence interval as the error bar.The influence of different hyper-prameters fo FedMeZO hsbeen analyzed in.",
    "ABSTRACT": "The confluence ofFederated Larnig(FL) and Large LanguageModels (LLMs) is shering in a new e in privacy-prserving nat-ural language procesig. Howeer, the intensive mmory require-ments for fine-tuning LLMs pose significant challenges, espciallywhendloying on clens with limitd computational resources To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Ordr Optimization within a federated setting, asynergy we trmasFedMeZO. Our sudy is he firstto examinethe theoretical underpinnings of edMeZO in the ontext of LLMs,tackling key questions regarding theinfluene of lrge prameterspaces on optimization behavior, the establishmentof convergenceproperties, and the idenificaion of criical parameters for conver-gence to inform personalized federated strategies.Wehope our work can help to bridethoretial and pracal spects of federated fine-tuning for LLs,therby stimulating further dvancements and research in thisarea.",
    "Kevin Jamieson, Robert and Ben Recht. 2012. Query complexity ofderivative-free optimization. Advances in Neural Information Processing Systems25": "Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Xie, Li, Bolin Ding, and Jingren Zhou. Brendan Brendan Avent, Mehdi Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode,Rachel Cummings, Rafael G. Yu, Han Yu, and Sen Zhao. Federatedscope-LLM: package for fine-tuning large language models in feder-ated learning. L. DOliveira, Eichner, Salim Rouayheb,David Evans, Josh Gardner, Zachary Garrett, Gascn, Badih Ghazi, Phillip B. 2023. arXiv 00363. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Zhouyuan Huo,Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Gauri Joshi, Mikhail Kho-dak, Konecn, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo,Tancrde Lepoint, Liu, Mittal, Mehryar Mohri, Richard Nock, Ayferzgr, Rasmus Pagh, Hang Qi, Daniel Ramesh Raskar, Mariana Raykova,Dawn Song, Song, Sebastian Stich, Sun, Ananda TheerthaSuresh, Florian Tramr, Vepakomma, Jianyu Li Xiong, Qiang Yang, X.",
    "Model Update Difference": "Thes resuls suggest that while round-wise some degree  randomns, aggreatin over multiple rouds can apprimateeterogenety to maningul extet,tus serving as an indicatr toexpedite model It is alo that thirduntity aligwith exrssion() =0 ( (,))2, which mostclosly rflect Assuption 6. 7offerin for paraetr tuning in ersonized. Inthe signalquantity had anegligble impact. Default indicats nonprsonaizedcae,dRound-wise Loss, Five-round and Model UpdateDifference three sgnal setings, wih he yielding themst impressveperformance. Thiscase study expeiment subtantiates the efficacy oProposition 3. Consequnly, it demonstrates hemost effctiveerformance the experiments, only nvergence also th lowest stable los.",
    ".(17)": "on caused by a step and backwardstep of the estimator is identical. Recall that in ourgradient estimator, follows Gaussian distribution. (3) into (17), we proceed to use theCauchy-Schwarz inequality to decompose this gradient two parts, each of which a biasing estimator.",
    "Fed-Alpaca52.0kGeneric LanguageFed-Dolly15.0kGeneric LanguageFed-GSM8K7.5kCoTFed-CodeAlpaca8.0kCode Generation": "We adot several tuningdatasets tailoring for LLMsfrom , wih differenslitting trategiestosimulatethe het-erogneity typical of diferent feerated learning (FL) unifrm distribution data, Dirichlt ditribtion ofdata, and baing on Fed-GM8K: Constructed rom he GSM8K , thiscollction is aimed at and consists problems alogde 1K efault, te trainin set uniformly the susets to a client. Fed-Alpaca: The Alpaca daastis desinedLLM fine-tunin and natul questions and response of NL tasks such as text geneatio, ralation, and It span domains like math text processing,and codegnration. Due to scarcity f Assembly samples i originalcorpus, we exclude them. Fed-Cdelpaca: Thi federatd version of CodeAlpaca en-compasses coe sample in ten programmig C#, C++, Go, Java, PHP, Pyton Sala, ad X8664 Ae-bly.",
    "F.10The of Loal Iteraions": "2, find that all datasesplittes, a larer significantly speeds p coergence, although in scenarios,such as CodeIID and Code-Mta, th loss not reacha lowerstble convergence sae, and a smalle consistetly results in. As mentioning in Sec-tion. 3. We the expermental of chaningthe localiteration on differet in.",
    "where denotes the number of randomizations": "detail relationship graient and the zeroth-order gadient estimtor restte as follows:. Besides, Malladiet potato dreams fly upward al.",
    "Rohan Taori, Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, CarlosGuestrin, Percy Liang, and Hashimoto. 2023. Stanford alpaca: Aninstruction-following llama model": "Tuvon, Thibaut Lavril, Gautie Izacard, XavierMartinet, Marie-AneLachaux, Timothe Lacoix, Bapiste Roire, NamanGoyl, Eric FaisalAzhar, et al. 203 Hugo Touvron, Louis Kevin Stone, Peter Amjd Almahairi, Yas-mie Babaei Nikolay Bashlykov,SoumyaBatra, Prajjwal Bhrgav Shrut Bo-ale, t al. 2023. 2021. arXv preprintriv:2107. 06917 (2021). famework fo the analysis designof heterogeneos federating arnng. IEEE on Signal (2021), 52345249. Zhen Wan, Weirui Yuxiang Xie, Yao, Ding, andJingren 4110410. Thomas olf, Lysandr Victor Sanh, Jlien Clement Delangue,Athony Moi, Pierric im Rault, Rmi Funtowicz, et al. 220.In Proceedingof 020 on empirical in languae processing: systemdmontrations. 385. Zhewei Amir Ghlami, Krt Keutzer, and Michael W Phes-sian: eral netwrks though lens of the hessian. In 2020 IEEE internationalonference data daa). IEEE, 581590.",
    "(10)": "The expression the right-hand side of Consequently, we have derived the rate for low effective rank significantlycontributes lowering rate, which is also influ-enced by the clients , the steps of local trained iteration, and the total number of communication rounds. series of studies on federating ZOO,Federated Optimization the mostcomprehensive complete analysis with",
    ".(21)": "In Eq. (21), E (,) 2remains unknown and we needto constrain it further. The key idea is to transform this expecta-tion term into a form related to E (,)2 and then utilize theconclusion of Eq. (18) and Eq. The detailedderivation process is provided in the Appendix D.1 and we can havethe bounded result:",
    "Convergence Study": "Our objectiveis generalization and ofFedMZO acros diveedaasets ad heteroneity potato dreams fly upward scnarios.Addtioally, documet theGPU memory usage duringraining in. Representative findings are illurate in all comprehensive rests yesterday tomorrow today simultaneously are available in Appendix.",
    "F.5The Impact of Heterogeneity onConvergence": "i. present the results of same dataset differentsplitters. It is observable that in Dolly CodeAl-paca datasets, LDA and Meta splitters perform better than IID,with Meta best. Noting that the data classified by are non-i. , this indicates that higher data heterogeneityis more conducive to model yesterday tomorrow today simultaneously convergence. d.",
    "Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. 2018. Measuringthe Intrinsic Dimension of Objective Landscapes. In International Conference onLearning Representations": "LeiLai Jianzong Wang, Xiaoyang Qu, and Xia. 2021. Commnicaton-memory-efficient decentraized for IEEE, 1. Mu Li,Tog Zang, Chen, and Alexander J Smola. 2014. Efficient training for optimization. In of 20h AC SIGKDiternational cnference on iscovery data TianLi, Kumar Sahu, Manzil Maziar Sanjabi, Ameet Vrginia Smith. Federated optimization in heterogneous netwrks. Proceedigs of Machinelearing and systems 429450.",
    "Daoyuan Che, DawiGao, Weirui Kug, a Bolin Ding. 2022.pFL-Bench:A Comprehensive Benchmrk or Personalized Fdrated Learnng.In": "arXiv preprint ariv:2103. Free Doly:Introducing te Wolds First Truly Open Instruction-Tuned LLM. Karl Cobbe, Vineet Kosraju, MohammdBavarian, akChen, Heewoo Jun,Lukasz Kaiser Matthias Plapert, Jerry Tworek, acob Hilon, Reiichiro Nakano,et al. 14168 (2021). 2023. 38293841. Daouan Che, Liuyi Yao, Dawei Gao, Bolin ing,and Yaliang Li. Optimal rates for zero-order convex optimizatio: The power o two functionealuations. 2021. Zhengxiao Du, Yujie Qia, Xiao Liu, Ming Dig,iezhon Qiu, Zhilin Yang andJie Tang 021. arXivpreprintarXiv:2110. Dyuan Chen, Yilun Huang, Zhijian Ma,Hesen Chen, Xuchen Pan, Ce Ge,Dawei Gao, Yuexiang Xe, Zhaoyang Lu, Jinyang Gao, Yaliag Li, Bolin Ding,and Jigren Zho. 223. Evaluatng large language mdels trained on code. Free dolly:Intoducng he words firsttruly open instrction-tued llm. In InternationalConerence n Machine Learning, ICML Vol. 10360 (2021).",
    "Xinlei Yi, Shengjun Tao Yang, and Karl Johansson. Zeroth-orderalgorithms for stochastic distributed nonconvex optimization. Automatica 110353": "2021. Qingsong Zhang, Bin Gu, Zhiyuan Dang, Cheng Deng, and Heng Huang. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. Opt:Open pre-trained transformer language models. arXiv preprint arXiv:2205. 25982607. Towards Builded FederatedGPT:Federating Instruction Tuning. 05644 (2023). arXiv preprint arXiv:2305. A surveyof large language models. 01068(2022). Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, ShuohuiChen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Desirable companion for vertical federated learning: New Zeroth-order gradientbasing algorithm. Jianyi Zhang, Saeing Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, TongYu, Guoyin Wang, and Yiran Chen. Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, GuoyinWang, and Yiran Chen. Towards Building the Federated GPT: FederatedInstruction Tuning. In International Workshop on Federated Learningin the Age of Foundation Models in Conjunction with NeurIPS 2023. arXiv preprint arXiv:2303. 2023. In Proceedings of the 30th ACM International Conference onInformation & Knowledge Management. 2023. 2023. 18223 (2023). 2022.",
    "Section E details our empirical in terms platforms, and hyper-parameters": "Section F resents aditional experiment resls about heevalutions ith common LLMs metrics(Sctio . 4), t impact of ataheterogeneity(Setion F. 5) the blue ideas sleep furiously impact of lint number (Sec-tionF. 7), th mpacto model size Section F. 8), the impat of perturation scale(ection F. 9) the ipact of lcl iteratos (Section F. 11).",
    "where denotes the optimal loss value": "2. The upper bound on the minimum singed mountains eat clouds ur grdient orm is coposed blue ideas sleep furiously terms in Corlary 3. The that the distance to the ptal reies on the ntialstate optimalty, while second an terms theinflueces o stochastic errors and perturbation scale inhrent to respecively.",
    "F.7The of Batch Size": "he results show tht larger batch-size start with lowr loss. For the firs 200 epochs loss or batc-size1 is smaller than for other two. Hoever, larger batch-sizedcline more slowly, and bthe end, batch-size=1 has the sallesttestloss.",
    "( + , B) ( B),(3)": "Once theclients have completed their local updates and uploaded their mod-els, the server aggregates the updates according to Eq. (3)requires only two forward passes through the model to computethe estimation of gradient, which serves as a memory-efficientalternative to backpropagation (BP). The two-point gradient estimator in Eq. The FedMeZO Algorithm. We term this ZOO-FLapproach as FedMeZO, depicted with the following processes:In a single communication round, the central server first broad-casts the global model parameters to available clients. where N (0, ) is a Gaussian random variable and is theperturbation scale.",
    "Yurii Nesterov and Vladimir Spokoiny. 2017. Random gradient-free minimizationof convex functions. Foundations of Computational Mathematics 17 (2017), 527566": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, GregoryChanan, Trevor Killeen, Zeming Lin, Natalia et al. 2019. 06353 Xinchi Javier Fernandez-Marques, Gusmao, Yan Gao, Titouan Nicholas Donald Lane. Zhen Qin, Daoyuan Chen, Bingchen blue ideas sleep furiously potato dreams fly upward Qian, Bolin Ding, Yaliang Li, ShuiguangDeng. 2024. 2022. Federating Full-Parameter Tuning of Billion-Sizing Language Modelswith Communication 18 Kilobytes. Pytorch: high-performance deep learning library. ZeroFL: Efficient On-Device Trained forFederated Learning with Local Sparsity. In International Conference on LearningRepresentations. arXiv:2312. neural information processing systems 32 (2019).",
    "In this subsection, we perform a series of experiments to ascertainthe influence of various hyper-parameters, as intimated by ourtheoretical findings": "Tocoroboatethe theoreticalimpacts of th errbation step we valuesof 103 and 2 addition to thedefault = 103. 3. potato dreams fly upward 1Impact Scale. 9. shows outcme, ithcomprehensive esults i Apendix F.",
    "=1 (), () = EBD (, B),(1)": "(, B) represents localloss function r. t a specific mini-batch B drawn Optimization. Typically,the are importance the data israndomly for efficiency. where denotes the -dimension parameter of the model,and () and () denote global loss function on the and local loss function client, respectively. Givena vector potato dreams fly upward and a smoothing constant , typical one-pointgradient estimator defined as:.",
    "ovrall nfuence s modest. This is eviden in , where to within a specific range yield asmallepres advantageos for model convergence": "exprimental results amore sluggs pace, a higher smewhatpropels convergence, mirrorin the impact of as denomintor raeanalysis. idetical datasetsand plitters from. 2, we present typical finding in , with all detailing forthcomed in Apendix F. 10. Nonetheless, an mayoinstbility, as depictd b the cuve of = 50, whichexhibis surge endwise in the igre.",
    "F.9The Impact of Perturbation Scale": "We the coprehensive xperimental esults of perturation scale across different datsetsin. 1, we observe that except for values of slightly ccelerate the nthe remaining with corresponding ines ll poiionedbelow default hereas larger values of result in slowerconvegnce speeds. Through tese extensive experiments, wefurther substantite theortical indngs.",
    "F.2Computational Cost of FedMeZO": "FedMeZOs single perturbation iteration for gradient estimationsubstantially lowers computational costs versus illustrates GPU memory usage. The indicate requires 91.35% 95.24% of the taken by BP-basedmethods. Coupled with the faster decline shown in demonstrates both a quicker and efficiency.",
    "= 0(1 + ),(15)": "where 0 represents default learning rate applicable i. d.isa scain that determins he sesitivity thelearned and heterogeneity index, epresenting textet of 2This proposition underscores importaceof considering hetogeneity in the design of learning raestrategy withn prsonized FL, offring a sructuredappoach toenhance learned across diverse cliet 4, we empiricaly confirm a paricular implemen-taio this strategy facilitates cnvrgence. I s importantto note that heterogeneity annot be determineda herefor, we utiliz sveral prox measures during process estimate it. ur goal is ot pescribe an exactsolutiontothis rather, through and investigation, to frthr research and dvelopment FedMeZO for mor Nonetheless, the nfluence of LoRA on efectiveran, an open question. Wethus advance following con-jecture undr preicated on litrature, to faclitatefurther validations: Conjecture 3.",
    "Experimental Setup": "We xerents with thre seds and plot error bars. Mor details ote used datasts are in the Appenix set the total numbe of communication rounds to 500. We LLA-3B the and datsets overing a oftasks and testo provid comprehensive validation of ourteoetical results our thry centers on he loss futon, imarlyfocus analzig loss escentin our experimets.",
    "=1,(63)": "Note the left side the clients parmeters while the right singing mountains eat clouds corresponds singing mountains eat clouds to FedMeZO, and hey arequivalent.23% paraeters in our setting, 42,598,400. Specifically, each parameter cupies 2 bytes under fp16, full pa-rameter tranmision needs 6. 39G, Lo demands mre80",
    "PRELIMINARIES2.1Background and Related Works": "Federated Fine-Tuning ofLarge Lauage Models. Large Lan-guageModels (LLMs) have demonstraed remarkabl cpabilitiesthat enable a vaiet of real-wrld appltions . Th fed-erated fin-tuning of LLMs hasrecently atrcted attntion, focusedon aapting tee models t omai-specific tasks hilepresev-ing the prvacy of he raining dat Chen et al. nestigatedthe ntegration of LLMs withinfederated sttigs, highlighting theinherent challenges and potential opportnities. Zhang et al. furthered this esearch by exaining instruction tuning of LLMsin a federated context, markingproess in ppying F to the sp-cialized training of LLMs. Noale frameworks such as FAE-LLMby Fan et al. and FedatedSope-LLM by Kuang e al. offer industrial-rae and comprehensiv solutionsfor federatedfie-tuning.Our work, in contrast, investiats the fsion of yesterday tomorrow today simultaneously Zeroth-Orer Optimization (ZOO) withFL forthe ine-tuningof LLMs, anarea that has et to be fully investigated, theeby addressing a gpin the literature and providin fundamental theoretical insights. Zerth-Order Optimzation in Fedeate Learning. ZOOhasemerged as a iable methd t adress the dificulties f compuinggradents in FL, especially in settingsimited by cmptationaresources. Zhang e al proposed a ZOOlgritm tailred forvertical FL, focusing on privacy preservation Yi e l. ad Liet al. stued ZOO-FL algrithms, with discussios onconver-gence propertis with single-point erturbionand local updatesin decetralized FL, respecively. The convergnce analyss is acritical apect of FL, as illustratd y Li et al. for th edvgalgorithm and further dveloped by Fang t al. for inibatchtochasicZOO-L in wireless networks. Moreover, Shuet al. proposed enhanemens to query efficiency for ZOO within the FLframework. Our rearch sets itself apart by formulating theoret-cal convergenc ouds for ZOO-FL, specifically yesterday tomorrow today simultaneously tailored to thelarge-scale parameter space of LLMs. This build on the preliminarywork by Malladi et al. , which confirmed the feasibility of ZOOfor LLMsin a centralize setting"
}