{
    "Regarding the effect of modalities, results from Tables": "8 an 9 suggest * \"S that moalities are not criticafctor. Instead, t complxity of how thequstionintegrates the modalities plays moresignificantro. is owever, thesubstantialdifference the numberof samples ontiingthndable * \"S modalities opared to those with odality, threported results notdirectly comprable.",
    "Documents": "The most important * \"S ofQinShi is Wall of China.Cnsequently, he mdel r onthis ad provided ncrret because of notusingthedocument alluciag. Fine-tund On FM2DSDataInernVL-2-26 * \"S Answer: Te model indicating hat watchtowr windows come i varoushapes, withrecangular windows beed mostypical. Based on generated ncorrect answer becauseof not using the Model swer: CircleNote: modelsuggesed tht, based on images of he interiors ofwachtowerson the Wall f China, re due thei crve shape.Asa model produed an incorect answer. Model Aswer: Archote: modela detailed desciption of he shape stated thtbasedon he visibl fatures imag ofthewatchtowr, window ha an archdshape. from InternVL-2 sizes (8B 26B, 40B, to he qstion: In most important project oQin Huang, hat eoetric used in the watchtowers when frominside from illustrat exampllike equires detld visual undertanding, smaller models often hallucinae, provided inconsistenanswers (e.g., without gronding in te prvied dcument. however, better o thi and have less",
    "Generation": "A practical is to use a. FM2DS validation steps to * \"S ensure that thegenerated questions are answerable, and multihop.",
    "Which album was released first: the one featuringa famous photograph of a man bending on a pi-ano or the album that includes the song I DontWanna Be a Soldier?": "reuires model t ompare temporal informaion crosstwo documents: Musicfrom Pink answer s Musfrm Big Pink. In this case the connectedthrogh shared topic, music.",
    "Score": "Comparison f eal amples onMultiModalQA When sing Full-Pages Traning Samples - EMSyntheized -F1Rel Smples - MReal Samles - F1 . Comparison learning from synthtic andarss 1k to 1k nternVL-28B. exper-imen, to the real amples similar to ours, used the pages from Multimodal QA datast",
    ". Human Evaluation of Answer Validation": "of the stdyare i , * \"S hows that valdation improves thelielihood correct answers. randomly selected mpes and feach we generated an * \"S answe sed tw mehods:FM2DS w/o answr We then invited10 to ontrbute o our valuation platform (seeAppendix D) to deterinemethod produced they deterned th of ach answer,they askedto seect one f four otions: (1) Metod 1s answer is correct, (2) Method 2s is correct, (3 bothmethods generaed correct aswer, r (4) neither answeris correct.",
    "We Spacy for relation extraction": "enablng s to identify and extrac key detailssuch as entities, reltionshps and ontxtualthatalign the Query ValidatioTo validate the queries we used multi-modal retieval-augmentdgenerat (MuRAG) , both textual viualb an images into a shared embeddn spac. us t retrieve relevan documents based text and contet, ensuring more comprehen-sive validation. process involves queringtheMuRAG database ad retrevi the most elevantdcuments for each generated query. If oe thn one sourcedocumentappears n the to-5 results across all quers, itindicates thatthe iswell-formedreleant iformtion from multiple sources. oeover,this alidation ensures that the are serves as uide for teachin a smaller how toetrieve necessary information to answer thequestions.",
    "Pratyush Seto, Richard Bai, Dvi Grangier,Yizhe Zhang, ad Navdeep Jaily. Rephrasing the A": "ecipe for compute and data-efficient languae modeling. In Proceedis o he 62nd Anual Meeting of th Assocation for omputationalLinguisics (Volm 1: Lonapers),pages140441407, Bangkok, Thailand, 2024. ShivamMehta AnnaDeichler, Ji Oregan, Birger Moell,Jons Beskow, Gstav Eje Henter, and Sion Alexanderson 2 OpenA, Josh Achia, Stven Ade, SandhniAgawl,Lama Ahmad, Ilge kkaya, Florecia Leoni Aeman, igoAlmeida, Janko Atenshmidt, Sam AltmanShyamal Anad-kat,Ring Avia, Igor Babuchkin, SuchirBalaji, Vlerie Bal-com, Paul Baltescu, Haiming BaoMhammad Bavarian, adeff Begu et al. 3 Pranav Rapurkar,Robin Jia, and Percy Liang. InProceedings of th 56thnnual Meeted of e Associatn forComputational Lnguisics (Vlume 2: Sort Papers, pages784789, Melborne, Austraia, 2018 4 Alon Talor, Ori Yoran, AmnonCatavDan Lahav, YizhongWang, Akari sai, Garil Ilharco, Hannneh ajishrzi, andJonathan Berant. Mutimdal{qa}: cmplexquesio answr-in oer text tables andimage",
    "F. Qualtative Analysis": "the qualitative analysis, we compared three critical fac-tors influencing model responses: model architecture, fine-tuning (FT) dataset (real samples or synthesized samples),and model size. examine the of model architectureand FT dataset, we InternVL-2-8B, LLaVA-1.6-7B,and Idefics-2-8B, fine-tuning them on both real and syn-thetic data All of the mentioned models were fine-tunedon 5k analysis was conducted for 100 from the benchmarks: (1) M2QA-Bench, (2) (3) WebQA. The responses generated by differentmodels were analyzed these datasets, focusing on metrics: Model using exact match (EM) rate, corresponding to instances where themodel generated wrong answer based on pre-trainedknowledge instead of the provided document.",
    ". Proposed Method: FM2DS": "The subeuent stages involve few-shot sample selection,synthesizng and validating mulihop questions, generatingorrespnding answers, and cnstrutng relevant documenqueres, with each stage incorporating specific validtionmecnisms. Our fve-stg pipeline for FM2DS shwn in isdsigned to sythesize high-uality mutimodal question-answer pais. he process begins witretriving relevandocuments through opc aching and Wikipedia hperlink. Refer to Appendx E for examples ofgenerated daa by FM2DS.",
    "C. Investigating FM2DS on Models": "These also * \"S exhibit more effective learning, especially whenprovided with synthesized data generated by whichmakes more efficient. could be attributed to thetraining objective of mPLUG-DocOwl-1. 5, which focuseson multi-grained recognition and parsing, potentiallyresulting in weaker when reasoning isrequired. Finally, Phi-3. 5-Vision-Instruct,despite having fewer parameters compared to other mod-els, performs competitively other models surpassesLLaVA-1. 6-7B * \"S in performance.",
    ". Stage 1: Creating a Pool of Related Documents": "For we used the WikiWeb2Mdataset * \"S which contains nearly 2 million Wikipedia pages,providing comprehensive Thesedocuments linked two methods:existing and topics extracted using multimodaltopic modeling with Multimodal-Contrast model. This dualapproach * \"S ensures thorough compilation of related infor-mation, both and nuanced connectionsacross documents, while both visual and textualmodalities.",
    ". Experiments and Results": "This section evaluates our synthesized dataset human-annotating datasets. In all the model pro-vided in-context example dured data synthesis, unlessstated otherwise. were evaluated used Exact Match (EM) * \"S for and F1 score precision-recall balance in par-tial matches.",
    ". of different models for datageneration on test datasets": "demonstrated effective-ness of in creating a high-quality dataset thatenables training * \"S models for multihop Our experiments have that synthesizeddataset allows models to achieve performance thanwhen trained data by humans, with signifi-cant in test Furthermore, provides a scalable and efficient solution high-quality data, which is essential for traininglarge language models. For future work, we plan to syn-thesize MMQA samples sources beyond multilingual content, code snippets, videos, andother diverse information Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Ammar Awan, Nguyen Amit Bahree, Jianmin Bao, Harkirat * \"S Phi-3 techni-cal report: highly capable language locally on yourphone. arXiv preprint arXiv:2404. 2024. 12 Abhinav Jauhri Abhimanyu Ab-hishek Ahmad Aiesha Letman, Alan Amy Yang, Fan, AnirudhGoyal, Hartshorn, Aobo Yang, Mitra, ArchieSravankumar, and Artem Korenev al. 4 Amanda Maor Ivgi, Uri Alon, Jonathan Berant,Matthew R and Graham Neubig. In-context learn-ing with long-context models: in-depth exploration. 00200, 8 Andrea Burns, Krishna Joshua GeoffBrown, Bryan A. Plummer, Saenko, Jianmo Ni, andMandy Guo. suite of tasks multi-levelmultimodal webpage understanding. 3.",
    ". Stage 4: Answer Generation Validatio": "Answer answer generation, used concise aswrs based on multiple documents,which included bt and visual informationsuch asimages. This makst easierfor the model oanswer the question extraction was used toverify wheher the relationships ewee entities in the gen-erated nswer hose in the documents. Speificaly, based on what th question equiresfrom the(e. evalate the ofour answer vlidation step, w designed human evaluationwhre the can be in.",
    "E. M2QA-Bench Samples": "FM2DS LVLMs generate multimodal and multihopquestions the documents and evaluate theiranswers. samples aim to emulate examplestypically providing to guide the models behavior in a struc-tured relevant manner.In some cases, questions focus understanding factsfrom different modalitiessuch as images, and ta-bleswithin the grouped documents and the answerfrom of them. For example, the case of the questionshown :",
    "B. Experimental Settings": "In this work, we conducting experiments on cluster of 8NVIDIA H100 80GB GPUs. The distributed setup allowedus to efficiently scale our fine-tuning process across multi-ple devices. Additionally, AdamW optimizer was used with 1 = 0. 9,2 = 0. 98, and = 1e 8. models were fine-tunedusing mixed-precision trained to take full advantage of the80GB memory on each H100 GPU. 7, which strikes a balance betweenrandomness and coherence in the models responses, pro-duced more varied outputs without sacrificing too muchquality. This setup ensured efficient usage of computationalresources while maintaining high model performance.",
    ". Stage 3: Question Generation and Validation": "Athugh te modelwas duringquetin genration to avoid cretigquestionsconcateate independent questions an-swerable by single document,e evaluated this futer to break questiodown simpler componentsWe whethereah component cold answered using only one docu-mnt. * \"S Here is an eamplequestionunrelated facts:. all prts of th question werewith singledocument, we dcarded that include unrelatdfacts. Question Validationur frmewor incororates ml-tiple validation layer to veify thatsynthesied uetinsatisfy requisite criteri for both multiho and muimodal iteration. For instance, ifwo document were givn, he uestion nededboth to beanswerable; ifthree docments are provided, it can utilzeany two all threWe exlicitly instructed themodel that simply two indpenden questonswith an andreference multipl ocuments would notqualiy a valid multihop For instane How didAlbert to the Theory of Relativity andwhen Princeton salised? is true multihop even though t involves two documnts (Einsteins death inPrincetn being the A genune mltihop re-quies ntegrating information across sources to over ocuments. Due to the engthlimitations, werestricted the input to two three relatedocuments. * \"S Ouprompt, detailed in Appendix A, querythe mode to produce mltimodalthatrequired inputfrm least two modlities and drew oninformation from provied documents.",
    "Atula Tejaswi, Yoonsang Lee, Sujay Sanghavi, EunsolChoi. Rare: Retrieval augmented retrieval with in-contextexamples, 2024.": "1 Ian Wu, Sravan Jayanthi, Vijay Viswanathan, Simon Rosen-berg, Sina Khoshfetrat Pakazad, Tongshuang Wu, and Gra-ham Neubig. Chain of thought prompting elicits reasoning in large lan-guage models. Synthetic multimodal question generation. AnyClassifier, 2024. 2 Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma,brian ichter, Fei Xia, Ed H. Association for Computational Linguistics. Chi, Quoc V Le, and Denny Zhou. In Advances in Neural Information ProcessingSystems, 2022. 3. Ken Tsui. InFindings of the Association for Computational Linguistics:EMNLP 2024, pages 1296012993, Miami, Florida, USA,2024.",
    ". Comparing Methods For Data Synthesis": "To evaluate the effectiveness of different methods for syn-thetic data generation, we compared three prominent lan-guage models: GPT-4o, Claude 3.5 Sonnet, and Llama-3.2-90B, as shown in . Using Intervl-2-8B with 5Kfine-tuning samples as our baseline model, we tested thequality of generated data on two distinct datasets: Multi-ModalQA and WebQA. The results, measured using EMand F1 scores, demonstrate that GPT-4o * \"S consistently outper-forms other models across both datasets. We also see thatLlama-3.2-90B shows competitive performance as an open-source model with less number of parameters, particularlyin WebQA tasks. Claude 3.5 Sonnet generally yields lowerscores across both datasets.",
    "Jie Huang and Kevin Chen-Chuan Chang. Towards reasoningin large language models: A survey, 2023. 1": "Hugo Saulner, Leo Tronchon, Bek-man,Amanpreet Singh, Anton Lozhko,Thomas Wang,iddhart Karacheti, Alxander M. Scaled forneural language model. Botian Jiang, Lei XiaonanLi, Li, Xiachon Kong, Qi Liu, and Xipeng Qi Understandi therole of lms in multimodal evaluation benharks. Jaring Kplan Sam Tom Tom enjamin hess, Child, AlecRadford, Jeffrey Wu, and Dario Amodei. Obelis: opn web-calefiltered dataetof intereaved image-te documnts 203.",
    "A. Prompts": "our data generatn ppeline,FM2DS, whichinorpratesLVLMs, w designed prompts to the modelthrough ask inolving reasoned andsyn-thesis. Inth following sections, we otline the hestructure components of thes",
    ". Related Work": "a genera that tilizes et gen-eratedby lanage models (LMs) for distillatio,self-rining, ad fe-shot learning,dmonstraing that abeld andpseudolabeled syntheti data can leadtperfrmance in task. In other ases,rather tan syntheti tex alone,entir datasets specific tasks such as classification. work, Che et al. esin benchmarkutlized vision moels and tir codeto images visual reasoning instructiosacrss dail In another study, Mehta et a. et al. the Question Ansering (Q) liteatre, oftainindata has een predominntlyfocuing on unmodal(text-only) scenrios. For Liet5, i producing atasets lassifi-ction,offering into the onsistencyreliabilityof LLM-generated data.",
    ". Comparison with Human-Annotated Datasets": "Unlike previous datast ration methods like and WebQA our approach is full auto-mated, wit no human intervention in data eneration. provides a coparative analysis of dataqul-ity synthesized y our method againt pror datasets. We hree modelsLLaVA-1. amples generated by M2DS help us train modelsthatcan andle MMQA even tough its training samplesincude docuens. contrast, training samplesand WebQA consist ofshort, focused infor-mation snippets. Acrs modes and exeriments withdifferent numbers o training sames models synthetic data were exposed to an equal or fewer numberof real samples), he average improvement in EMwas 1. 96 in WebQ. i-provements EMgeerally lead t increases in F1 scorethere wher F1 decreass as EM This discrepany ma result from hallucinatios in incorrectpredictions,reduce string overlap an, in turn, * \"S lowerthe F1 coe. Moreover, achieve sameperformance witless ynthetic by FM2DS as those trainedon the trainig dataset, demonstrating faster * \"S sampes. We explore in detais in. Also note that larger achieves beter prformnce onthe of synthetic samples, diferecebetween Llava-1. 6-7B. GPT-4 acheebettr results amon large VLMs, likely was alsoused for datageneraion. For qualitative nalysis, toApedix",
    "A.2. Answer Generation": "The prompt answer directs th mode to an-alyze ultiple documens, bo text ad im-ages address th gienqueston. Imphasize integrtingand synheizing from all soures o deive themost acurate and response. The prompt ses thatall modalities and documentswithout relyingslely o sigle soure or pre-traid knowledge,focusing exclusively on provided ma-terials.",
    "Model accuracy with metric for samples includingtable modality (may include other": "Regarding synthetic data,fine-tuned on by FM2DS ignificanly hallucnation performance all. Ad-ditionally, distrbution of modalities across forMultiMoalQA and M2QA-Bench ws follows: MQA-Bench: 66 samples included modality, 2samples included table * \"S samples includedoth image and table modalities. 5. Largermodeoutperfomed smallr models onbohmodalite.",
    "Example (Unrelated Facts): In dd ikeTyson become theyungest heavywight champion andwh is president te Unted States": "we kept only the initial component thatrequired from multiple to thenew question met the multihop criteria.",
    "InternVL-2-26BSynth0.810.21InternVL-2-40BSynth0.820.11InternVL-2-76BSynth0.850": "Overall perfrmance of different * \"S model families fine-tune onreal andsnthesied dat on WebQA. In thisabe, indicates that larger vauesare better, while indicatesthatmaller values are better. Fine-tuning on synthizd datansis-ently reduces hallucination ratesa iprove E scores acrossall moes, withlarger models achievingthe bestperformance. Hllucinatin is tepoportion of halucinated responses to incorrct * \"S answs. Unlike M2QA-Bench and MultModalQA, WebQA on includesimage and ext mdality, as a rsut no EM(Image) and EM(Tbe)a repoted.",
    "Abstract": "ultimodal multihop questio anwering a complxtask that requires ov ourcs of sh image ad text, to questions. methods focus on singl-hop question answering or sngle which maesthe unsuiable for ral-word cenarios such as analyzingmultimoal materials, summarizing aca-demic artcles, or intrpreting scientific studis that combinechrt, iages and text. To addess tis gap, we proosea novel methdology, intrducing the fist rmeork forreating a enbles training modelsfoquestion answering. Wevaluate ourmethdoloy training models on our syn-thesized datastnd tsting o two benchmaks, ou that, with an qual sample size moels trainedon syntsized outprfor thse trained on human-collecteddaa by in mc (EM) on average",
    ". The Effect of the Number of In-Context Docu-ments": "For example, at the image of EiffelTower, engineering innovation allowed to structures in height? prompts the model to use theimage, but the answer is in the pages text on tallstructures. As we from zero-shot one-shot, isa and F1 reflecting minimal context. the zero-shot set-ting, FM2DS exhibits limited understanding of multimodalmulti-hop question answering, occasionally circumventsthe validation step by simply generating a that is notmultihop. presents the results evaluating the Intervl-2 8Bmodel with varying numbers of documents on and datasets. improvement fromone-shot to two-shot is marginal, suggesting diminishingreturns. With three in-context samples, the gains becomeminimal, indicating that additional beyond two pro-vide.",
    "Question Generation Prompt": "Furthermore,focus on creating thatcompel the model to * \"S extract andsynthesize informationacross multiple modalities|such and text. A multi-hop question requiresthe model to utilize informationfrom all available documentsin combination to thecorrect answer. Specifically,the question should be designed unanswerable one of thedocuments is missing.",
    "InternVL-2-26BSynth0.610.540.870.740.75InternVL-2-40BSynth0.640.50.890.80.78InternVL-2-76BSynth0.720.280.980.950.92": "model photograph depicts American soldiers (based on flag) and cross-references table the document to determine that people from theUSA were killed. , hallucination the proportion of hallucinated to incorrectanswers). For instance, the would to locatethe image on Iwo Jima to determine mentioning in question, which is the USA. In M2QA-Bench, models demonstrate higherperformance table modality comparing to those involving images. Performance comparison of different fine-tuned on real synthesized data on ratios forEM scores and hallucination were calculated from data (e. Here, the two documents asthe connection between two docments. Using snippets, it then question. is tasked with combining information from two docu-ments: Flag Iwo Jima and Battle Jima. This how the model syn-thesizes across modalities to form Afterward, model generates queries, serving as a step-by-step guide to information fromthe documents. g. In the table, indicates that larger values are better (all values), while indicates that smaller are better (hallucinationrate). Next,by referencing the table in the Battle Iwo Jima document,it provides the final answer. scores EM (Table) and (Image) include samples that contain other Larger models and those fine-tunedon data generally improved performance with reduced hallucination rates.",
    "InternVL-2-26BSynth0.70.370.790.850.8InternVL-2-40BSynth0.740.350.850.890.87InternVL-2-76BSynth0.80.150.930.940.93": "Performance coparson of differnt modl familie fine-tund on real and sntheizing data on M2QA-Bench. The atios for EMscores and hallucnation were clulating fro filtered data (EM(able) refers to the EM core alculated on samples that include tableodaity. In th table, indictes tha large values are better (all EM vles), while inicates that maller values ae better (hallucinationrate). Larger modls and those fine-tunedon synthesized dt generally exhibit improved perforance withreducing hallucination ras. In MultiModalQA, mdels demonstratehigherperormance on questions invlvingthe iage modality comared to those involving tables.",
    "Yinghsan Chang, Mridu Narang, Hisami Suzuki, GuihongCao, Jianfeng Gao, and Yonatan Bisk. WebQA: Multihop andMultimodal QA. 2021. 2, 5": "LiangyuChen, Bo heng Shen, Jingkang Yang, Kurt Ketzer, Trevor Darrll, ad Ziwe Liu. Languagmodels are visual coordinators. 1 Lin Cn, Jinsong Li, Dong, Zhang, Yuhang Zang,Zehui Haodong Duan, Wan, DahuaLin, et al. Are we th wy for ealuating vision-langue preprint arXiv:2403. 20330, 2024. 2 Mingda Chen, Xiun Cenand Wn-tau Few-shot daasynthesis fr domain multi-hop uestion Julians, Malta, 2024. Association for MuRAG: Multimodal retrieva-aumenedgneratorfor open answeing over images and Association for 5 Yang Chen, Hexiang Yi Luan, Haitian Sun, So-rait Chngpinyo, lan Riter, and Mig-Wei Cang.2023. Zhe Chen, Jiannan Wenhai Wang, eijie Su, GuoChen,Sen Xin, Muyan Zhong, inglong Zhan, Zh,LeweiLu,Bin Li, Ping Luo, Tong Yu Qiao, and JifengDai aXivprepritarXv:2312. How ar are we closing he gapto cmmercil multimal models with open-source suites. arXiv preprit arXiv:2404. 16821, 2024. paragraph-evelueston-aswe pais from Wikipdia. Associaton for Lin-gitics. 2 Alexander Chien-Sheng Wenhao Liu, nd Xiong. QAFactEval: Improved QA-based facual cn-sistency evaluation for In Proceedings 2022 the Noth American Chapter for Human LanguagTechnologies, pages 2582601, Seattle, United ttes, 2022. Computational 4."
}