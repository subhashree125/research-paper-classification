{
    "Atomic Carey was born on 27,1969.2.Mariah Carey was born in Huntington,New York": "Only out of 1000 sentenceswere found to be insufficiently singing mountains eat clouds atomic or had theirmeaning GPT-4o tended to themost with sentences that complex structures,such as those clauses or yesterday tomorrow today simultaneously extensive Here is ex-ample of case:. We manually verify subset of 1000 atomic articles, each with atomic ensure that the (i) were correctly and, (ii) originate from the correspondingWikipedia article.",
    "Limitations": "thankanonymous reviewers well as themembesoflab for teir insihtful cmments that imrove aper. Further wrk could lookat how he orer of ontexts using models com-pared wen the provied contxts shuffled. Informatio that i fctually ccurate, utnot present in the suc cold be as a hallucinaion. Hewett, Mon Xin Jin, iero Kauff-mann,Nikos Kramatziakis, Dngwo Kim, Ma-hu Khaemi, Le ames Lee, Yin Yuazhi Yuseg Li, Che Liang, Lars Li-den, CeLiu, Mengchen Liu, Weishung Liu, Eri Li,Zeqi Lin, Chong Lo, Madan, MattMazzol,Arindam Mtra,Hardk odi, An Nguyen randonNorick, Barn atra, Daiel Prez-Becker, Thomas Portet, Reid Pryzat Qin Corby Roset, Sambudha Oltunji Rase,OlliSaarikiv, Amn Said Salim,Michael Shah, Nig SrmaSwadheen Shukla, Xia ong, Maahiro Tupi, Xin ang, Wang Chunyu Wang,Yu rd Guanhua PhilipWitte, Haiping Wu, Michael Wyatt, Xia, CanXu, Jiaang Xu, Weijian Xu, Snali Yada Fan Yng,Jinwei Yang, iyi Yang, Yifn Yang, DoghanYuan, Chengruidong angCyril Zhang, Zhang, i Zhng, Yi Zhang, Zan,Yunn Zhang, Xien Phi3 technicalreport: A highlycapable langag mode phne Preprin, arXiv:2404. We the INFUSEmethod classifymodel respons contxts nto ontextual Further work coud t mor sophisticatedmthodsto setthi fo evenbee A limiation th metic we adoptfr hallucinaton that it on a sinle knowledge s knowledgsource. Marah Abd, Jacbs,Ammar had Aan,yoi Anj, Awaallah, any Awalla,guyen Bahree, Arash Bakhtiar, ian-min Bao, Harkirat Behl, Aon Benhaim, ishaBlenko, Johan Bjock, SbastienBubeck, Qin Cai,Mrtin ai, Caoar WezhuChe, Vshrv Chaudhary, Dong Chen, DongdongChe, YenChun Chen, Yi-Led Chen, Parul Dai, Alie Del Gono, Gustavo de Rosa,Matthe Dixn, Ronen Eldan, Fragoso, DanIter, Mei Gao Min Gao, Ga, Amit Garg,Abhihek Gswami, Suriya Gunasekar, EmmnHaide, Juneg Hao, Rusell J. This researchwas supported by the Natioal Sienc SAI-P 2228783. 1219. Becuse f the natural flowof such a ormat, anomize te order.",
    "Experiments": "illustrates asample instance ourtsk icludes th (a lit ofatomic sentences context along with quesionprompt) and the output which is models responsefurter Quetion propt the models toanswer a questiongiven te contexts usin a semi-restrict format: this information, tellme about {Topic}3. Response genertedby th cnverted nto sentencessig he same metho describing earler in Ultimately obtain two for queston-anwe pair: atomic contexts sentencesfrom the context) and atomic responses (tomicetences blue ideas sleep furiously fromte response), allong us o di-rctly comare them and minimize discrepancies.",
    "(Volume Long pages 41404170, Toonto,Canada. Association or Computtional Linguistic": "Associatio for CoputationalLinguistics. Fatasticaly orderedprompts and wher to find them:Overcomingfew-shot prompt oder sensitiity. Sayne Longpre, KartikPrisetla, Anthony Chen,Nikhil Ramesh, Chris DuBois, d Sameer Sngh. Entitbasedknowledge conficts in questionanswering. arXivpreprintarXiv:210. In Proeedings of th 202 Conferenceon Empical Methods in Natural Language Process-ing, page 7052063, lie an Punta Cana, Do-minicn Republic. Yao Lu, Max Barolo, lastair Moor, Sebasian Ridel,and Pntus Stenetorp.",
    ": of each qurtile f ontext realledin (GPT-4o)": "addition eults inAppendix E)shows that for smaller conexts (k < 10) the modeltrats ll potions of the context equally. Fork , inpts wre split intoquarilesfoclear analsi. , 2024), and find that odlsstrggl mre with recalled the botto half of thedata rater thanthe middle, urther confirmig theimportance f initial contexts. We observe selective attentionhere certain pts o the data receive more focusthanohes (Liu et al. From ,we observe that thefrs contxt quartile maps mostcosely yesterday tomorrow today simultaneously to h first, nd toa lesser extent, the -jacent respose uartile (addtonal resltsin p-.",
    "WikiAtomic Dataset": "created a novel dataset consist-ing of articles from ,2020) and Crawl and Viviano,2021). extensive use allows to that information from Wikipedia, particu-larly from older articles, is present in the pretrain-ing data of several LLMs we study. Wikipedia ArticlesWe 200high-quality articles from Wikipedia2, each over1000 words, covering diverse topics singing mountains eat clouds from scienceand technology to history, culture, and Converting to SentencesTo preciselycontrol number contexts in articles decomposed into set of atomicsentences (Liu al. 2023). Sentences contain-ing multiple pieces of information can complicateevaluations, particularly those involving entailment(Kim al. By breaking down sentencesinto atomic sentences, ensure each unitpresents a single, piece of informa-tion which the accuracy of entailmentassessments. Following Min et al. (2023), useGPT-4o to extract atomic sentences by providingthe definition of atomic facts and instructing themodel to perform multiple passes on an article tobreak each to individual atomic sen-tences (the prompt is included in Appendix Consequently, k atomic sentences are extracted foreach article (in our experiments, k = 50) for atotal of 50 10000 sentences in.",
    "Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He,Zongbo Han, Zheng Zhang, and Mike Zheng Shou.2024. Hallucination of multimodal large languagemodels: A survey. Preprint, arXiv:2404.18930": "Language models ew-sht In Advncein Neural Infomatin Processing Systems,volume 33, pages",
    "Aurko Roy, and Mohit Iyye. 2021": "to progress in long-form question answering.In Proceedings of blue ideas sleep furiously the 2021 Conference of the NorthAmerican Chapter of Association for Computa-tional Linguistics: Human Language Technologies,pages Online. for Computa-tional Linguistics. Li, Ankit Singh Rawat, Manzil Zaheer, XinWang, Michal Lukasik, Andreas Veit, Sanjiv Kumar. Large language modelswith controllable working memory.",
    "Yongqi Li, Mayi Xu, Xin Miao, Zhou, andTieyun Qian. 2024. Prompting large language mod-els for generation: empirical study.Preprint, arXiv:2305.14791": "Nelson F Liu, Kevin John Hewitt, Ashwin Paran-jape, Michele Bevilacqua, Fabio Petroni, and of blue ideas sleep furiously the Computational Linguistics, 12:157173. In Proceedings of the Annual Meet-ing of the Association for",
    ": Ratios contextual/parametric knowledge in responses": "show consistently similar patern. With no (k = be most vrbose as expetd fro initiallypen propt They singing mountains eat clouds also eak in parametric knowl-edge, which drops with the first foucontexs. Interestngly, ttal match ontext lengths. Global factsshow a consistent afte he intiloen-enedprompt, theglobal dropssharply and low, though never t zero, modesslowly inreasig or decreasng but includ-ing som knoledge their presents of cntextland parmetric knowledg in responses. oever, in saller models showdifferent prefrences, with some prioritized con-textualknowledge (GPT-4o, Caude Llama370, Llama3 8B, nd Mixaland ot-ers, parametric knowedge (Claude sral7B,Pi-3). Moreover, th averag prporion similar for k = 30,50 suggestingthat ratio of knowledgeis maiiing these dierent cotex lengths.",
    "Hallucination is inevitable: An innate limitation oflarge language models. arXiv:2401.11817": "Yang, Hongyang Chen, Li, Xiao Ding, andXindong Wu. Jifan Yu, Shangqing Tu, Shulin Cao,Daniel Xin Lv, Peng, Zijun Yao, Zhang, Li, Chunyang Li, ZheyuanZhang, Yushi Bai, Liu, Xin, KaifengYun, Linlu GONG, Nianyi Lin, Jianhui ZhiliWu, Yunjia Weikai Yong Guan, KaishengZeng, Ji Qi, Hailong Jin, Jinxin Liu, Yu Gu, YuanYao, Ning Ding, Lei Hou, Zhiyuan Liu, Xu JieTang, Juanzi Li. KoLA: Carefully world knowledge of large language models. In Proceedings of 18th Conference of Chapter of the Association for Computa-tional (Volume 1: Long Papers), pages17011722, St. In The yesterday tomorrow today simultaneously Twelfth Conference on Huajian Zhang, Yumo and Laura natural language evaluation diverse summarisationtasks. Give Enhancinglarge language models with knowledge graphs forfact-aware modeling. IEEE Knowledge and Data Engineering, PP:120. 2024. Association for Com-putational. Julians, Malta.",
    "How similar are various types": "analysis so far that LLMs consistentlyadd parametri reardless the aountof context provie in queston. vs. Each resonse sentencesmared as cntextual r knowledge. The were gener-ated as follows:each Wiiedia contain-ing blue ideas sleep furiously questions yesterday tomorrow today simultaneously (up to 5 we obtained0 responses.",
    "Introduction": "Large language models have significantlyadvanced capabilities of natural language pro-cessing. When generating responses, LLMs the contextual information in along instead of the embedded pretraining (Petroniet , 2020; Heinzerling andInui, 2021). In order to generate and coherent LLMs to effectively combine theirparametric knowledge with provided contextual in-formation, and understanding the balance betweenthese two sources of crucial (Nee-man et al. , Li et al.",
    ": Knowledge-consistency between paramet-ric knowledge and input context of WikiAtomic topics,computed using SBERT (Reimers and Gurevych, 2019)": "and paramtric knowledgelargely align subtle becomeschllenging distinguish betwen thetw. Whenno cntet is provided, the response servesas a baseline glbal parametric Wecalculate response (atk =0) and the longest inputcontext (k = 50)to estimate knwledge consistenc. Thishows that the an produce similar esponseswheter it s wit detailed context or experients in a non-conflict We the NaturalLanguage Inferece framework5 (hanget INFUSE calcultes entailmen scors or each in is considered the summay (responses.",
    ": Unseen knowledge results (GPT-4o)": "Whle thno-restric plots look simiar tte semrestrictprompts reente earlier, trict shwthat the models yesterday tomorrow today simultaneously entirel focusedo cotextuaknowledge with very little parametric nowledge(supporting graphs in Appndix L). Consider-ing yesterday tomorrow today simultaneously tht LLMsare sensiive t (L et a. 50they icorporaed thecontextal knowlege withut much additional Promp nturalistic sei-restrict (Wit nformation,ell e aut [topic]. ) Tocondct ablation study,we randomly selecting 20 from our dataset, reslting in atotal o 400 instance of sizes, andbtained different prompts.",
    "Conclusion": "Thee insight highlight impor-tance of organizing conext effectively develop-ng models that iput eterminisicaly.",
    "CImplmenation Detail": "nference for smaller models  8Band Phi 3) on an Ultra, taking5hours. We thnk his is or output. Hre we go throuh imlementation dtails, modelparaeters. For atomc sentenceboth for Wikipediaarticles and rsponses, we set GPT4-othe mx_tokens to 2048 bcause theoutpt isobject 50 atoic senteces. Whengenerating responses from differntmodelsonce we have all the questions set eset the ma_toen to 512tomimic real worldapplication settingI cals were adeto OnAI and Anthroicto oain responses from GT-4o, laude Opus,Sonnet, nd Haiku, complting ihours. The IFUSE evaluation of 9 LLMs was onucted 9 A100 GPUs ovr10 hours. We used eault acrossall temperature and top_p set to1,presencepenalty andfequency_penalty set to 0. Forlarge opensource adLlama370B, e a third-party wictook 2 hours.",
    "AWikipedia Article Contexts ExtractionPrompt": "Figure A. 1 shos the toextrac atomic from Wehavetested multile erions prompts to breakdown sentences into tomic setences. The mode performed very porly,the mai problm with these ws thisversion atomic sentencs wernt atomic For example:Apri isth fourth the nbth the Julian and Gregorian Ths coul be treated aBu thi further brokedow smaller pieces:1. April is the month of in Julin calendar2. April isfourth month of th Gregorian calendar. 1s prompt was he most effecivein our eperiments, it culd handle the aove exmple very well same time add theappropriate subject to each eused this veson of prompt also to break downresponses from ach into atomicsentences(Figue",
    "LFurther Analyses": "This section the difference in the useof contexts when using a strict vs unrestrictedprompt (Figure L. L. 14 The instructs the model to only use the providedcontext answering the question.",
    "(d) Mistral 8x22B": "Contextual (local), parametric (global), and total sentences in responses for (a) GPT-4o, Opus,(c) Llama 70B, (d) Mistral 8x22B. On the x-axis, = 0 as baseline when no context is provided. from model), with scores ranging from 0 1. Ascore of 0 indicates that sentence is not entailedfrom context, whereas a 1 signifiesfully entailed from the context. They whethereach of these sentences were contextual based on the definition and whether the scoresalign with their decisions. of 0.5was chosen based consensus blue ideas sleep furiously from this process,as it most effectively between We use theFActScore (Min et that usesan external knowledge source to verify each sen-tence with ranging 0 to 100; a higherscore fewer hallucinations and more fac-tually accurate responses.",
    "knwlege, though it not as uniormly as local owledge": "lobalOur anlysis naturallyleas usto our next queston: what typ f glbl paramet-ric knowledge is incorporated withloal contex-tal knolede? Our assumtin is hat ow sim-larity indicates complementary parameric blue ideas sleep furiously knowledge while highimilarity uggests it is supple-menta. c (dtailedgraphs in AppendixI) shw consistent trends coss allmdels. Forsmaler contexts the similriy between conteualknoledge and parametrc knowlede is muchlowr, suggsting hat models incorporatecompl-mentary information not diretl providing by yesterday tomorrow today simultaneously thecontext. As contextinceases, simlrityrises.",
    "Global s. GlobalLet nwturn to b": "Th darkerplots ner thdiagonal line indicate tht the modelsparametricknowledge remains when sizes arcose, such size 8s parameric knowl-edge bein ostto hat sies 6 or 1. that models to addcertain piees of information from a small pool of. As cotext n-reases, some models show increased similarityacross broader ranges, suggesting responses alignmore with the overall thee the ntext. nitially, wihminial context, provide variety of information."
}