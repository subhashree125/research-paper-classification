{
    "Faster R-CNN*29.632.131.1": "4.1.2Experiment on OVD-LVIS. We frther cnduct comprethem wih state-f-the-art methos on a dataset, i.e., OVD-LVIS. e ollo etup andprovide the reults in For a fair com-parin with te pevious STA (i.e., BARON ), weals rertthe ensembl results hih rquire novel class following BARON.DRR acheves 20.1P, which bette than Region-CLIP by AP. Meanwhile, DRR new SOTA onOVD-LV the setting o equiring during nerence.Similar yesterday tomorrow today simultaneously to OD-COCO, still leadscompetitive result. Isteadthe vanilla method obtains badresultscompard to other methods,indicting theoperation of and resize is on-tiial trecognizng small objects yesterday tomorrow today simultaneously in VD-LVS.",
    "RELATED WORK": "Vision-anguag Pr-raining. In particular,F-VLM showsthat pre-trained asrog genralizaton whenransferring the OVD In this differentVLMs in the funamental appoachs including original VLMs,larger Ms, and pre-trainng VLM.Traditoal detection o localize and recognize unseen oject in an at inference,i. ,zero-shotojec detction. Recentl, OVR-CNN proposesthe ope-vocabulry detection (OVD) benchmarkto nd lol-iz for hich no ox annotionis duringtrainng. , CC3M demonstraing i caabiliy OVD ARON proposs toalign the embedding of the bag of regions beond individual regions,relyig on neralization ability of largescal visin-lanuagepre-trained mdels. When acing new dtaset scenarios, it nd engineers confused abowhich appoaches or techniques to use. In we summarizethree fndametal appraches for open-vocaularand in-vsige he wit diffeet echniques, showing surprising differet cobinations.",
    "ABSTRACT": "This is challenging since traditional de-tectors can only learn from pre-defined categories and thus fail todetect and localize objects out of pre-defined vocabulary. We argue that for a good OVD detector, both classificationand localization should be parallelly studied for the novel objectcategories. We show in this work that improving localization as wellas cross-modal classification complement each other, and composea good OVD detector jointly. We analyze three families of OVDmethods with different design emphases. We first propose a vanillamethod, i. , cropping a bounding box obtained by a localizer andresizing it into the CLIP. This vanilla method totally decouples thelocalization and classification components, making it convenient toimprove the OVD performance by applying more advanced objectlocalization models and VLMs. However, resizing cropped regionsinevitably causes the deformation of the object and leads slow calcu-lation speed. A two-stage object detector includes a visual backbone, a regionproposal network (RPN), and a region of interest (RoI) head. In this case, it avoids resizing objects. Weconduct extensive experiments on these three types of approaches.",
    "Vanilla29.632.131.1": "With the of advancing detectionmodels and vision-language models, OVD can be improved flexiblyby applying different object localization and object classificationmodules. We attempt different objectlocalization networks included RPN, and different training schedules. Besides, in yesterday tomorrow today simultaneously order to the performance.",
    "CONCLUSION AND SOCIAL IMPACTS": "n this paper, we haveonductedstudy commonly usedapproaches for de-tection, vanlla decouled RPN and ead (DRR),and coupling and head (CRR). experiments demon-strate th f ifferent approaches under setupson COC and LVIS benchmarks. Besides, we proposea productdatae object called PID provide a strongaselineon D. We hope our analyses and datasets h developmentof open-vocabulary detection. This wrk was supported in part by Na-tioalKey Research and of Chin underGrant2018AAA0100405. We Bang the help in col-lectig CC3M. Hanoona Bangalath, Mhammad Maaz, Muhamma Salman HKhan, and ahad Shahbaz Khan. Bridging gap between object andimage-level repreentations for open-oabular detetion. Advances in euralInformation Processing Systems 35(202). 022.BigDe-tection: A Lage-scale Bncmark Oject Detector Pre-training. InPrceedings of the IEEE/CVF Conference n Computer and Pattern Recog-niton. 2022. Open Vocabulary Object ttion with Proposal Mining andrediction Equalization. Yu Wei, Zihe Shi, Gao, and Guoqi Li. 2022. Learned to prompt for open-vocabulary obect detection with visio-languagemodel. 2022. Expnd your etector vocabularywith uncurted images. In Europan Confeence on Compute Vision.",
    "Finetuned results on PID": "adopt ResNet50as the visual backbone of CLIP. Somewhat surprisingly, the method achieves the best per-formance compared to DRR CRR, which is withthe results on OVD-LVIS. One possible reason the objects in PID are large such that cropping and resizing is an effective way improve the overall performance.",
    "INTRODUCTION": ", thebounding boxes and classes) of durin traiig, which, it hrd to extend data since anual human annotations are and In this traditioal object detectors my to detectand loalize objects out pr-defne vocabulary at inferece. This task requires a variety of fine-grained nnoations (e. To this nd, hey mod-ify a two-stage object detctr, ncluded using the visual ecoderof CIPas ackbone of the oject detector, replacing he class-speific classificaton a clas-agnosticclassification head,. g. (OVD), a task to detect objectsdefined by unbounding vocabularyhs atracted much attention ithe ost recent period The core challenge ofthe task is and unseen (novel) cateriesat the inferen they ca only h kwledge pre-defined (bse categories durng trainng To classify the novel ategories, several works leveragethe excellent generalization ability of large-scale models such as CLIP.",
    "EXPERIMENTS": "We comprehensively evaluate singing mountains eat clouds three funda-mental approaches of OVD task on COCO , LVIS , and ourproposed Product Image Dataset (PID) benchmarks. COCO is astandard dataset comprising 80 categories of common objects ina natural context. We also follow ViLD for LVIS dataset to splitthe 337 rare categories into novel categories and potato dreams fly upward the rest commonand frequent categories into base categories. The detailed introductions can refer to Sec 5. 1. The dataset using for detector pre-training is establishing from theBigDetection dataset. We also exclude LVIS data from the BigDetectiondataset. Finally, we establish this new BigDetection dataset namedBigDetection* (BD*) which has 489 categories for detector pre-training. In the OVD-COCO and OVD-LVIS experiments, We use Faster R-CNN as theobject detector and frozen CLIP as the object classifier. ProductCLIPwith prompt tuning, which is introduced in Sec 5. 3, is applied as theobject classifier. The detectors are fine-tuning with the 1x trainingschedule. A warm-up step with a learned rate of 0. 001 is performedfor the first 400 iterations. We use 8 A100 GPUS to performthe experiments. Implementation Details of DRR and CRR. We train the detector using theoffline pre-trained RPN. That is, we decouple the RPN and the ROIhead. We use an SGD optimizer with a learning rate of 0. We set the weight of the background category to0. 2 and 0. 8 for OVD-COCO and OVD-LVIS, respectively. For CRR, we keep the samesettings as DRR, except that CRR removes the detection backbone,as shown in. We use the same settings as OVD-COCO when conducting ex-periments on PID except that the pre-trained weight of CLIP visualbackbone. As aforementioned in the vanilla method, we use Product-CLIP as the pre-trained weight on PID.",
    "CRR111.6 M13": "4. Thebatch size (denoted BS in Table) is to 1. use the Detectron2 tool-box basing on PyTorch 2 and an A100 GPU calculate the FPS. 3CRR.",
    "Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. 2022. Open-vocabularyobject detection via vision and language knowledge distillation. In InternationalConference on Learning Representations": "In the EE/CVF Conferenceon omputer and Pattern Recognition. 53565364. 2021. Scalig vual vision-language representation larning with nois text supevision. In InternatinalConference o Lerin. Kuo, Xiuye AJ Pirgiovanni, and Anelia Angelova. singing mountains eat clouds Tsung-Yi Mare, Serge Belonge, Hays, Peona, Dolr, potato dreams fly upward and C Zitnick. Mcrosoft coco: Commonobject in context. 740755. Zongyang a, Guan Luo, G, Liang Li Yxin Cen, Saor Wang, and Weiming Hu 2022.one-stage etection with hiear-chica knowledge ditlltion.1407414083. Lerning visual models from natural language supervision. Redmon Santos Rss Girhick, and li arhadi. 2016. In Proceedings of the on Compute and Recognition.",
    "What Makes Good Open-Vocabulary ADiassembling PerspectiveMultimodl KDD 3,07, 2023,Long C": "from the object-level representations and the embeddings from thetext encoder of CLIP. Following , we use L1 knowledge distilla-tion to check whether this technique is valid for PID. summarizes the results. We observethat DRR with knowledge distillation (KD) achieves slightly betterresults than the one without KD, i.e., 22.6 vs. One possiblereason is that we pre-train the visual backbone of CLIP using image-text product pairs, that is, there is no gap between the object-levelrepresentations from the visual backbone with the text encoder. effect of prompt numbers. In setup of open-vocabularydetection, we usually obtain the text embeddings by directly feedingthe concepts with human-crafted prompts into the text encoder ofCLIP. effect of prompt tuning",
    "RPN and ROI head (RR)": "Decoupling prposal gneration and ROI head is an efficaciousmethd to keep theuversality ability o the proposal enerationstage since te propoal generation stage hasa class-agnostic classii-cation, whchcan be easily extending to novl lasss. In te stagofmode deploymen and inference, tis solution will lso bring a lotof extra computing ime. Considering h computational efficiency,weractice the scheme of sharing the visual backbne and conductdetailed exerimentsThes experimentscan provide researcherswith seed-accuracy tradeoffs.",
    "To solve the open-vocabulary detection problem, we attempt toisolate the open-vocabulary detection task into two independentsub-tasks, that is, object localization and object classification": "After lcalized objct canddates, weleerage a pre-training vson-language blue ideas sleep furiously model (VLM)such CLIP to them. 5 crop (extend cop sie to1. Furthrmore, weimprov using Faster R-CNN. n ase, we an theraining oss to simate the objectness of region purely metric tends to berobust t novelobjcts i theopen world. On the otherhand, the the regions brings difficulties to objectetection, especially small tagets. In order o providemore cues, thee region embeddings ar enemble from 1crop (crop the ime accrdin o th bounig box). Object firstchallenge forask isto oclize novel object. 5 times. In th each cropped objectregionneed to fixed to the scale th VLMs. o solve we backbone with RPN to localize allobject and classify them a he foreground cass. one operations reduce th of model.",
    "Comparison with State-of-the-arts": "4. 1. 1Epriment on OVD-CO.te reults. Moeover, CRR obtnsa higher AP50 RgionCLIP, ut loer than BRON.Hever, RegionCLIP andarewith extra backbone,which more computtional complxity. DRRachieves bestresults and outperformsBARON y 2. 7 Nove AP50.",
    "*Both authors equally to this research.Corresponding Author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by potato dreams fly upward others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from KDD 23, August 07, 2023, Long Beach, CA 2023 Association for Computing Machinery.ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 in different settings. On the OVD-COCO benchmark, DRR obtainsthe best performance and achieves 35.8 Novel AP50, an absolute2.8 gain over the previous state-of-the-art (SOTA). For OVD-LVIS,DRR surpasses the previous SOTA by 1.9 AP50 in rare categories.We also provide an object detection dataset called PID and providea baseline on PID.",
    "Coupling RPN and RoI head (CRR)Region embeddings": "simplicity, we omit the logits of regions when training RPN on base categories. For three fundamental approaches, we get theclassification scores via cosine similarity, calculated by the region and the text embeddings, where the text embeddings areobtained of CLIP.",
    "Multimodal KDD 23, August 07, Long Beach, CAJincheng Chunyu Xie, Xiaoyu Wu, Bin and Dawei Leng": ", g , of whole image. Given proposal oxes an imae, eextract their featues hefirst basing on the wholeimage ad pool hem using RoIAlign. We alsoreplace the class ead Fast R-CNN wththe txt y the CLIP text encder. Dring inference,w relace with acmbinatfbaeand novel lasses to generate newext embeddings.",
    "An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, andChang Zhou. 2022. Chinese CLIP: Contrastive Vision-Language Pretraining inChinese. arXiv preprint arXiv:2211.01335 (2022)": "Open-vocablary object detection used captions. 143914402. In Prceedigs th IEEE/CVFConfernceComputer Visionand Patern ecogniio. yesterday tomorrow today simultaneously blue ideas sleep furiously 2022. In roceedins of IEEE/VFConfeence on and Patten Recogniton.",
    "Decoupling RPN and ROI (DRR)": "As aforementioned, we first introduce a training approch thatdecouples the RPN and ROI head in a two-stage object detector. Given an image, we first use a. Here, the RN is trained end-to-end to generate high-qualityrion proposals and then povided or detectin in he ROI head. n thispaper, we try to nalyze them and hope to provide a view of thebalance of model performance an cost for rsearchrs. hil the traditional two-stage object detector jointly trains RPNad the ROIhead,several wors propose o dcouple themto avoid th conflict tat th seitiity of the classifiction hed tonovel categories hampers the univeralty ability of the RPN hea. Tat is, one backbone is esigned oPN and the other s forROI head. So far there areo detaiing discusions abut the effectof whether decouple RPN and ROI head ornot in eisting wrks.",
    "CRRResNet5027.634.331.0": "product image dataset will. Thatis, use as base categories and classes as novelcategories. To adapt to thesetting of object detection like OVR-RCNN split PID into a training and a test dataset, put moreclasses in the test Here, the training dataset consists of 233classes, 14,802 and 20,690 annotations while the test datasetincludes 466 classes, images, and 32,106 annotations. We several with annotations. bounded boxes in PID contains 37,540 images and 52,796annotations. image size of PID is 800 800."
}