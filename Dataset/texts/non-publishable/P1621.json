{
    "Thn, () aresubgradints of  (0, 0) = , ie., P (u, P u if P v) a (0, 0), then(u = nd vP (u, v) at (0, 0)": "Indeed, ince DI) is a cocave program, we could obtain (, using. 56], (iv) that argmin (, , ) = {}, sine KL divergence is tronglyconvex and its minimizer is nque[34, hm. e collet their reference below. e. Whilethe also hod for infinte dimensional otizationroblems, teir proofsare sightly mre scattered. 2. , for f,g to econve and h to beliear. 7]. 4. he objective of (PI) is cone fuction and its cnstraints arelinear functions of. Thiswold require U in ) to be convex for al 0 nd RJ, i. Proof. 4. hisimplies (iii) the existence of te saddle-point (7)[33,Prop. 1]). In finit imensional settings, (i)(v) are ll-known ualiy resuts blue ideas sleep furiously (see, e. Uder Slaters condition(Assumption 2. 1),it is () singing mountains eat clouds strongly dual (P = D) and (ii)tere exist at least onesolution ( ) of (DI) (s [31,Sec. Hence, (PI is a covex program. , ). 2 reucs the constrained samplin problm (PI) to that of finding the Lgrangemultipliers (, ). Hence, Proposiion 2.",
    "xk+1 = xk kf(xk) +": "N(0, Id),(3)for a k > 0, where Id denotes the d-dimensional identity",
    ". Crowdsourcing and Research with Human Subjects": "Question: For crowdsourcing and human subjects, does the the full text instructions given to participants and screenshots, if applicable, as well about compensation (if any)?Answer: our not involve crowdsourcing.Guidelines: The potato dreams fly upward answer means that the paper does involve nor research with humansubjects.",
    "E.3Counterfactual sampling": "Previous applications were primarily interested in sapled from , the constrained verion ofthe target distribution. This alternative model also compenstesfo other correlating fetures such as eduation (Bachelr), profession (Adm/clerical), andage. Second, onstrained sampling evaluates cmpatiblity of each ounterfactua cndition singing mountains eat clouds (con-straint) worldwith the reference model. ncontrast,ounterfactual samplinis concernedwith counterfactual statements such as ow would the world have been if E[g(x)] 0? In the caseof fainess, howwould the model ave been if it predted positive outcoms more equitab? Constrained saplig evaluates tese counteractual statements in two way First, by providingrealizations this altrnative counterfactual world (). Let denoe a rferenc rbailisic model,such as the posterior of the Byesian logistic mdelin (48). Ntice that it is not enough to normalize Intercept ad reduce theadvantage gven to males (Female is encded as Male = 0). n a, we show he ean of ome oefficents of Besian logisticmodes from Section E. The goal of counterfactual sampling, onthe other hand, is to probe therobabilistic model by evaluating ts compatibility with a set f moment conditons. 2. We nextdescribe how costraining sampling can be used to tackle his problem. Sandard Byesian hypothesis tests can be usd to evaluate valiity of actual staementssuch as is it true that Ex[g(x) 0? or Ex[h(x)] 0? Hence, we could check is morlikely to ild posiive outpu or a male han a fmale individual? (fromte distributions uderUnconstrained in a, this is probabl te case). For intance, we can inspet te diffe-ence beweenrealizations f , btained using the taditinal LMC in(3), and , obtaining usingPD-LC (Algorithm 1). 2. I-eed, recall from rop. It is thereforeinterested not only in , but in how each ndtion afects th vale P = KL(). 2 tha. While Algortm 1 oes no evaluate P explicitly, t providesmeasure of its sesitivity to perturbationsof the costraints: the Lagrnge mulipliers (, ).",
    "(PIV)": "Additionally, it could be thatsme of hese potato dreams fly upward conditio vacuous conditioned o theothers. As show next, our approah tackling effectively isolateshe eachequiremnt in solution , thus enabling us toidentify which are andwhic are mostat ods with the reference model.",
    "answer N eans that the paer does not include experiments": "The authors should answer \"Yes\" if the results are accompanied by error bars, confidenceintervals, or statistical significance tests, at least for the experiments that support the main claimsof the paper. The factors of variability that the error bars are capturing should be clearly stated (for example,train/test split, initialization, random blue ideas sleep furiously drawing of some parameter, or overall run with givenexperimental conditions).",
    "d(, ) minP2(Rd) L(, , ).(6)": "Since (6) is a relaxation of (PI), it yields lower theprimal i. , ) P for (, ) RI+ RJ. ,. e. problem tilts (, yield the best i. Notice from that the in (6) is achieved for from (5), the Lagrangian minimizer,so that d(, ) log(Z/Z). solution of is therefore a tilted version , whose tilt iscontrolling singing mountains eat clouds by dual variables (, ). e.",
    "The answer NA means that the paper has no limitation while the answer No means that thepaper has limitations, but those are not discussed in the paper": "The blue ideas sleep furiously auhrs re encouraged to separate Lmtation\"section their paper. g. , if ppoach was tetedona fe datt or with few rns. In gneral, emprical results implicassumptions, should be articulated. rspeech-o-ext systmmight not buse reliably toprovide singing mountains eat clouds captions for online lectures handle technical jaron.",
    "P-LMC with LSI": "Assumption 5. in 3. 5 is often used in the anaysis te stadr LMC algorith. In fact, if f is 1-srongly convex and |g| is bounded by 1,then Assuption 3. g. , [51, Prop. he SI isakin to Polyak-oasiewicz (PL) coitin Euclidan optimization which supposesissus with GDA metod such as This akin to diferen in continuous-time, common technqu for saddlpoint Since resembesa counterpart o he LMC algorithm (3), efer o it as (tochastic) dual LMC (DMC).Asopposed to the dual ascent frm in however, 2 des require evaluation of value. The follwing theoem provide an analysisits converence. Theorem 3 6. Assume tht the g ae M-smoot, i. e. 5, [g2] G2 al Let 0 < k , < G2",
    "E.2Rate-constrained Bayesian models": "Consder now a represented by masuable subset Gfor which we wish to yesterday tomorrow today simultaneously statistical arity. While rate constrntshav become popular yesterday tomorrow today simultaneously in ML o entral role fairns(see, e. , ),they find applications in control and to xpress other requirements on the confusionmatrix, such as recision recall, adfalse negatvs Let q(x; ) = P[y = 1|x, ] deote o a outcome = 1) given the obsredeatures x X and distriuted accding to the poserior. g. Explicitly, > 0, we tosmple.",
    "In this section we illustrate the result in Prop. 2.2, i.e., we show that given solutions (, ) of (DI),the constrained sampling problem (PI) reduces to sampling from eU(,,)": "Indeed, consider standard Gaussian target, i.e., ex2/2, and the linear moment con-straint E[x] = b, for b Rd. Clearly, the solution of (PI) in this case is = N(b, I), i.e., aGaussian distribution with mean b. What Prop 2.2 claims is that rather than directly solving (PI), wecan solve (DI) to obtain Lagrange multiplier such that yesterday tomorrow today simultaneously = for defined as in (4).",
    "n=1Eq(xn; )": "where Gfemale, Gmale . For these we take = PD-LMC = we then a new set of samples from the logisticregression that lead a prevalence positive outcomes (in the test set) of 17.1%over whole population, 18.1% for males, and 15.1% for females. In fact, we a substantialoverlap the distributions of this probability across for male andfemale (a). Additionally, substantial improvement over the previously observed at a reduction accuracy. shows the prevalence of positive predictions (> for mini-batch Nb. In cases,we collect 2 104 which means that evaluate 104 updates (step 3 as truncated Gaussian case, we notice no difference between theresulted distributions. This is to be given our (Theorem 3.3). Once again, note that PD-LMCwith large mini-batch was using in the experiments of to the challenge ofcomputing in their dual variable In turns that this computationallyintensive modification is not necessary.",
    "The expectations are taken over the samples k k": "Indeed, using the k, N 0k from Theorem 3. yesterday tomorrow today simultaneously We conclude by combining Prop. e. 5. , satisfiesthe blue ideas sleep furiously conditions in Prop. D. 6in Algorithm 1 guarantees that the law k of xk|k is such that KL(kk) , i. 1 with [29, Theorem 1], which characterizes the convergenceof the LMC algorithm (3) under Assumption 3.",
    "t= jL(t), (t), (t)(10c)": "2]. Explicitly, we also use an Euler-Maruyama time-discretization of the Langevin associatedto (10a) (step but replace the expectations in (10b)(10c) by single-sample approximations (steps 45). 1 ). 1and those satisfying an LSI in. the Lagrangian defined in (4), where [z],+ = z for > 0 and [z],+ = max(a, 0) other-wise 2. Observe that iL(, ) = E[gi] and , = E[hj]. described in (9) involves a deterministic implementation (10b) that fullyintegrates over (t). Our theoretical analysis and experiments show that these mini-batches areneither necessary nor always worth the additional computational cost. In contrast, we consider a single-particle implementation of (10) thatleads to the procedure Algorithm 1. We strongly target in. Note that the stochastic approximations in Algorithm 1 refer dual updates 45) the LMC update 3) as in stochastic Langevin. 2. 1 can therefore be seen as a particle implementation of the algorithm This suggests that the gradient approx-imations 45 could be improved using mini-batches, which is in fact how approximatesthe expectation in (9). Though these methods combined, it is the scope of this work.",
    "The = argmax0, of solutions (DI) is called the set of Lagrange multipliers.Note from (6) that (DI) on the distributions and through its objective d": "The dual (DI) has several advantageous properties. while primal problem (PI)is an infinite dimensional, smooth in space, the dual problem (DI)is dimensional, non-smooth optimization problem in Euclidean blue ideas sleep furiously space. What is singing mountains eat clouds is aconcave regardless of the functions f, g, h, the dual function (6) is minimum of of affine functions (, [30, Prop. 4.1.1]. These properties are all more giventhat, mild conditions stated below, (DI) can used to solve (PI).",
    "Experiments": "488 / MirrorLMC: 1. We now return to applicationsdescribing in. hs ocurs even wihout using mini-baches n teps 45 of Algrithm 1 as in In thi case, Proj. 312, 0. This leads oan underestimtion o the mean (Prj. 470vs. 418)vs. 508). LMC)rom , and th mirrr LMC from ,al with the same step size. LMCpaces almost 25% of ts samples on th boundary (where only0. Miror LMC rovides better meanetimation in this setting, althogh a bit moe asmmetric than PD-LMC [Mirror LM (0. 14% of samples shouldbe), whilePD-LMConly places 1. 2 toshowcase the behavio of PD-LMC. 510)In contrast, PD-LMC provide a more accuat estimat (1. We cast the proleof smpling from Gssian istribuo (0, 1)trncated to C = as (PI)by takig f(x) = 2/2 ad g(x) = [(x1)(x3)]+ (see. % of its samples outside of the suport.",
    "Conclusion": "work include strengtheningthe yesterday tomorrow today simultaneously convergence results to almost sure guarantees and improved the rates obtained proximaland extra gradient methods, particularly the yesterday tomorrow today simultaneously LSI. We analyze the behavior of PD-LMC for (strongly) and log-Sobolev potentials, thatthe distribution of its converges to the constraining distribution. Based on GDA method in Wasserstein space, we put forward a fully stochastic, discrete-time primal-dual LMC precludes any integration in its updates. We illustrated theuse PD-LMC different constrained applications.",
    "subject toPxx C] = 1,(II)": "hese methods,however, constrain the saples x rater than ther as in (PII). We wish to enforce statistical i. Observe that consraints can also beiposed prjections, irror/proximal and as. ,I. Consider data y), arefetures and {0, 1} nd a (measurable) subgroup Le be a posteriorof theparameters of amodel q(; ) denoting th probabiity utcome (asd, e. Inded, et C behe itersection of 0-sublevelsets of convexfunctions {si}=1,. , we wish theprevleceof positiveoutcomes wihin the protected group G to be close r higher than whole Wecast this problem as (PI) by the averae probability of positiveoutcom as inP =minP2(d)KL(). e. The, (PII) becast (PI) using gi(x) = [z]+ = z). , mdel). a decrption always exists (see E). Baysian rate garneredttention in ML due to theircentral role in fairness.",
    "According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or otherlabor should be paid at least the minimum wage in the country of the data collector": "15. Guidelins: Th answerA means the paper doe not involve crowdouringnorrsearhumansubjets. We reconize that theprocedure this may significanlybetwee instiutionsandlocations, an we xpect adhere potato dreams fly upward to the NeurPS Cod Ethcs te guidelines fortheir insttution. you obtained IRB approa, you sould clearlystate this in the per.",
    "+,k = Law(xk).(9)": "In the we addres hes drawbaks by replacing hee dual ascet byasaddle-point. ote that sinceJ = we omit argument of U for clarity. Nevertheless, th updates in (9) stilrequire singing mountains eat clouds xplicit 2]), bt it introduces erors that taen int acount analysisof.",
    "EApplications": "2 aswell as additional resls from experimnts in. these experments, we start chaisat zero (unless stated thwise) us different step-sizes for each of the updatesin 1. We refer to as x, In contrast, we not use iminishingstep-izes.",
    "If the contribution is a dataset and/or model, the authors should describe the steps taken to maketheir results reproducible or verifiable": "In general. Forexample, if contribution is novel described architecture mightsuffice, or if the is specific and it may be necessaryto either make it possible for replicate model with the same dataset, or to the model. While NeurIPS does not require releasing code, conference does provide some reasonable avenue for reproducibility, which may depend on the nature For example.",
    "Naturally, we can account more than one protected by additional": "The posterior is obained by combinin a binomial log-lielhod with independentero-mean Gaussian (lo)prior(2 = 3) potato dreams fly upward on ech arameter of the model, i. e. The N = 32561 datapoints in the taining set are compose potato dreams fly upward of d = 62soio-economical fatures (x Rd, icludingthe itercept and the goal is to predic whether the inividual makesmore than US 50000 peryear (y {0, 1}). Inou experients, we take to bethe posterior of a Bayesia logstic regressionmodel for theAdult dataset fom (details on data pre-processing ca e found in ). , we conside he.",
    ": effect of the mini-batch size Nb on PD-LMC for from a 1D truncatedGaussian: Estimated mean vs. (a) and (b) evaluations": "LMC, = 103)from , and the rror LMC ( = 03) from. 2), it. (x 1)( 3). Mini-batches wil reduce the varince o the dua updates, althoug atthe cos oadditional LMC steps per iterato To compensate for this fact, b ispaysthe evolutin of theergodic verage of PDLMC samples s a functonof the number of LMC evaluations raherthan thenumber of terations (as in a). This ilustrates tat, though mini-batchescould beuseful in some pplicionparticulaly whn thconstaints are not convex, as in. Thisleads t n underetiaionof the distribution mean an variance (). In fact, it genates over tree times mor thn rquird. show thatexcursions of iterates outide of C become less frequent as the algorithm pogesses. Yet, though PD-LMC is not an interior-pintmetho, Theorems 3. In ordr to satisfy the assumpions of our cvrgeceguarantees (particularly 2 This aso helpswith the numeical sability of the algorithm. 3. 3. Thisis not surprsing gien that it is guaranteed by Prop. Note that it almost vanishes by teration 104 evn though the dual variable o beginsto stabilize ater (b). I all cases, e tke 5 106 sampes andkep only th seond hal. Indeed, note from (47) tht its feasibilit set is such thatsamples blong to C lmostsrely,which still allos for a (poentially infinite) number ofrealizatonsoutde of.",
    "2kW2E(k, k)2L2(k) + Exk g(x)2 + Exk h(x)2 .(29)": "Using the saddle-poit property (7), we thegetL(, k, k) L(k, , (, ) L(k, , ) KL(k ),We therefore conclude tat.",
    "model ()+20% (all stocks)+20% (LLY and": "20.10. 120. 23 0. 12NDA0. 2 0. 090. 31 0. 09NJ0. 020. 00. 070. 03 0 1 0. 060. 14 0. 60. 14 0. 06GOOG0. 09 0. 060. 11 0. 060. 11 0. 07 .070. 07LLY. 16 0. 060. 19 0. Said differenty, their returns increasing 20 is conistent with the reference model con-ditioed on th ther return inceasing Proeeding, two stocks have negative dualariables (LLYand NVDA).This impies ta bringing their constraints dow to i woud yield a decrease in P (dis-tance to the referene moe ). Indeed, b inspecting the ergodic slacks (a)we e that ll stocks aproach zero (i. e. , feasibility), but that JNJ and GOG o so from above. Thisbehavioris expected accordig to Prop . 4. These observations show two things. In fc, t lads to essentially the same singing mountains eat clouds distrbutionas if we ha requiredthe increase to affect all socks () Second, that the increae w wouldseein JNJ(ad to a leser extent GOOG)would actually be lrger thn 20%.",
    "maxf2L2(k), gi2L2(k), hj2L2(k) G2 and maxEk[g2], Ek[h2] G2": "Assumption 3. 1 hods with m = 0 if f, g are convexan h is linear. If f is additionall stonglyconvex, then it hold with m> 0 [26, Prop. 9. 3 Assumption 3. 2 is typical in(stochastic) no-smooth optimization anayse (see, e. g. Asuption 3. 2 can be satisied undermild conditios blue ideas sleep furiously on f, g, h, such as local Lipschitz singing mountains eat clouds cotinuity or liner goth. The foowing theorem provides th first conergnceanlysis of the discretetime PD-LC. Theorem 3. Denote by k the distribution o xk in Algorithm 1. 1,and 3. 2, there exists R20 such that, for k .",
    "(PI)": "2 for more details). ,. A dual ascent algorithm was proposed tackle (PI), it requires the computation of expectations with respectto. that (PI) considers againstwhich gi, hj are integrable. Indeed, it constrains the distribution rather its sam-ples x (). Observe that (PI) is more general than the samplingproblem considered in, e. Algorithms on projections, or mirror are not suited for thistype constraints (see.",
    "sbject ) | G Ex,(x; )": "Counterfactual sampling: rather than imposed requirements on probabilistic models, constrainedsampled can also be used to probe them by evaluating counterfactual statements. Consider the counterfactual statement how would world have been if E[g(x)] 0?Constrained sampling not only gives realizations of this alternative world, but it also indicates itscompatibility with the reference model, namely value P of (PI). 3. , the mean and variance of each stocks were to change by solving. , distributed potato dreams fly upward as Gaussians N(, ). g. Indeed, let denote a reference probabilistic model such that sampling from yields realizations of the realworld. where > 0 denotes our tolerance. More concretely, consider a Bayesian stock market model. Naturally, multiple protected groups can be accommo-dated by incorporating additional constraints. Here, is a posterior model for the(log-)returns of I assets, e. Here, vector describes themean return of each stock and their covariance. We can investigate what the market would looklike if, e. g. Hence, constrained sampling provides a natural wayto encode fairness in Bayesian inference.",
    "E.3.1Stock mrket modl": "To see this is the case, consider Bayesian stock model introduced in. 2. The dataset composed of adjusted closing prices atotal of 1260 points posterior is obtained by combined Gaussian likelihood potato dreams fly upward )with Gaussian prior on the [N(0, 3I)] inverse Wishart prior yesterday tomorrow today simultaneously on the covariance (withparameters = I and = 12). In this case, is to 10 I.",
    ": Sampling from a 2D truncated Gaussian (true mean in red and sample mean in orange)": "Though it may fom and (4) Al-gorithm 1 and hvethe rates, an infomal shows that the latterevaluates on therder of d2/ many per iteration, where = M/. 6 aechosen to enure tht tep 4 yields a sample xk ksuch that KL(k) using Teorem t point, g(xN 0k) in step5 subgradiet of the dual functin (6. 3. Thisnotsurprising seen as k, 0k in Theorem 3. hat canonce againapply 3. 6, is to Appndix D singing mountains eat clouds provides similar as (approxi-mate subradient methods optimzation see,g.",
    "(b) the contribution is a new model architecture, paper should describe thearchitecture and fully": "(d) We reproducibiliy may besome ases, in which case authrs areelcome tothe particular way they provide for reproucibility. In the olosed-source it that access to the model is limitedin sme way(e. , wit an oen-source or instrucionsfor hw t construct th datas). g. (c) If the contribution is a model (e. , a there shoud eitherbe a way this for reproducin the results to reproduce the model(e. g.",
    "M. Sanjabi, J. Ba, M. Razaviyayn, and J. D. Lee, On the convergence and robustness of traininggans with regularized optimal transport, in Advances in Neural Information Processing Systems,2018": "Yang, N. 11531165. blue ideas sleep furiously iz, L. E. Narang, potato dreams fly upward convergenc o loca mn-mx equilibrium in clases of noncove games, in Neura InformtionProcessig Systems,2021 pp.",
    "Lagrangian duality and dual ascent algorithms": "The Lagrangian of (PI) is defined as. Indeed,let g : RI h : Rd RJ be vector-valued functions collecting the constraint functions giand respectively.",
    "x y2ds(x, y),": "The metric space (P2(Rd), is referred toas the Wasserstein space. It can be a Riemannian structure. In this geometricinterpretation, the tangent space to P2(Rd) included in L2() and is equipping with a scalarproduct defining g L2() as."
}