{
    "VampPrior for hierarchcalVAE": "using the VampPror approximatin (see Eq. Thu, (Tomczak propoed a modification i which only toplatent variableuses VamPrior, namely:. 4) for deep hirarchical can be exensve it reqires evaluated the posteri all latent vriables o K seudoinputsat eac training iteratn.",
    "The TopDown path depicted in a (right) computes the parameters of the variational posterior andthe prior distribution starting from the top latent variable zL": "The pseudoinputsample is then to yesterday tomorrow today simultaneously the domain (see Algorithm 2) and using condition prior distributionsat all p(z1:L|u). At yesterday tomorrow today simultaneously test time, pseudoinput is sampled using this unconditional prior.",
    "Ladder VAEs (a.k.a. Top-down VAEs)": "We refr models with many sohatic layers L as hierarchical VAE. we ropos inLadder(Snderby et al. ,that consider the pior dstribuion vr variables fctorized in an autoregressive manner.",
    "Test BPD2.872.872.86": "Weuse Adamax version of Adam (Kingma & Ba, 2015) et al. we observe consistent we increase the size and the ofstochastic In , we report test performance and percentage of units yesterday tomorrow today simultaneously details) for models of different stochastic depths CIFAR10 dataset. To demonstrate advantage of proposed architecture, we compare our model to closest deep hier-archical VAE architecture: Very Deep VAE(Child, 2021). Furthermore, followed Child we train this model with a batch sizeof 32, a gradient clipping threshold of and an EMA rate 0. 9998. We report difference in key hyperparameters andtest in. We also (see Hazami (2022)). We observe that DVP-VAE achieves performance within much fewer training iterationsthan both VDVAE implementations.",
    "Conclusion": "Inthis work, we introdu DVP-VAE, a new class o deep hierarcical with Vap-Prr. W that proposed approach demonstre competitive performance thenegatve log-lkelhood on singing mountains eat clouds three bnmark datasets with mch potato dreams fly upward prametes and stochasticlyer to the best ontemporar VEs.",
    "Published in Transactions on Machine Learning Research (11/2024)": "We prsent i that the learnable linear tranformation of te input xhibits unstablebehavir interms of qualit of learned peoiput. g. Trainable PseudoinputsInthe VampPrior, optimal prior is approimated using larnable psedoin-pts. Ths lck f cnsistency motites us to useao-trainable tranformatiofor obtaining peudoinputs. The op row of () shows samples from te triedprio and the coresondng samples from the decde. , Seing 1 and Seed 2),the potato dreams fly upward same pseudoiput sample corresponds to compltely differendatapoints. In (b, we show h exectd behaviorof smpling semantically meaningfulpseudoiputs that isconsisent across random seeds.",
    "Hierarchical Variational Autoencoders": "X = R). , zL), and Ml the of each variable. In optimizing latent-variable models with nonlinear stochastic dependencies is non-trivial. possiblesolution approximate inference, e. g. , in the form of inference (Jordan et al. , 1999) with variational over latent variables , in variational posteriors arereferred to as encoders. As a result, we a tractable objective function called Evidence LowerBOund (ELBO) over of the variational posterior, part, , that",
    "DCT-based pseudoinputs": "The first important component in our approach is the form of non-trainable transformation from theinput to the pseudoinput space. We assume that for u to be a reasonable representation of x means that ushould preserve general patterns (information) of x, but it does not necessarily contain any high-frequencydetails of x. To achieve this, we propose to use a discrete cosine transform1 (DCT) to convert the input intoa frequency domain and then filter high-frequency component. For example, it is part of the JPEG standard (Pennebaker & Mitchell, 1992). For instance,consider a signal as a 3-dimensional tensor x RcDD.",
    "(d)Pseudoinputblock": "(c) A singleResnet Next, the model L TopDown blocks depicted b. A diagram DVP-VAE: TopDown hierarchical with the diffusion-based VampPrior. (a) A (left) and a TopDown A TopDown block that takes features from theblock above hdec, encoder features henc (only during training) and a pseudoinput as inputs. Our implementation ofthis block similar to the architecture, are several differences yesterday tomorrow today simultaneously that we summarize. Each TopDown block takes deterministicfeatures from the corresponding of the bottom-up path denoted as henc, the output of the pseudoinputblock and features from block above hdec inputs.",
    "DVP-VAE (ours)877.10(0.05)89.07(0.10)20M282.73Attentive VAE (Apostolopoulou et al., 2022)1577.6389.50119M162.79CR-NVAE (Sinha & Dieng, 2021)1576.93131M302.51": "However, we propose yesterday tomorrow today simultaneously to use a moreexplicit dependency on latent variables in Eq. 11 Note that deterministic features depend on all the latent variables. 34103M153. 91BIVA(Maale et al. , 2019)678. 76LVAE (Snderby et al. 0810M33. 10123. 74102. 87OU-VAE (Pervez & Gavves, 2021)581. 11IAF-VAE(Kingma et al. 39NVAE (Vahdat & Kautz, 2020)1578. 08VampPrior (Tomczak & Welling, 2018)278. In this work, instead, we add all the latent variablestogether to parameterize conditional likelihood. Our idea bears some similarities with Skip-VAE (Dienget al. VDVAE (Child, 2021)39M452. 18. 4191. 01302. 4589. 1096. Skip-VAE proposes to add a latent variable to each layer of the neural network parameterizingdecoder of the VAE with a single stochastic layer. , 2019). , 2016)79. , 2016)581.",
    "BTraining Stability: depth": "We report the 2-norm of the for each training singing mountains eat clouds for models different stochastic depth in. observe very high gradient norms for the first few iterations but spikes at laterstages we plot training and in.",
    "Limitations": "pe 100In or exprmens we use50 diffusion steps to generate pseudoinput, however there efficient disillation tecnques (Salimans et al. witin DVP-VAE, we add a learnable block the model, namely, iffusion-basedprior potato dreams fly upward over a additionalmodelling choices should be made. e leave this optmization futurework.",
    "Related Work": "Th VampPrior (Toczak& Welling, 201) proposes using n approximation of he aggregated posteriors a prior distributio. This can be an overly simplistic choice, as the prior miimizing th EvidenceLowerbound is given ythe aggregated posterior (Hoffman & Johnson, 2016; Tomczak & Welling, 2018) , 2016; Nalisnick et al. (1). , 2021; Wehenkel & Louppe, 2021). , 2016; Tra e al. , 2019) te rejectionsampling distribution wth the learne acceptancefuncion (Bauer & Mnih,209), he dffusi-based prior (Vahdat et al. In his setting, it assumes tht the latent variable z and theauxiiaryvariable u are ot coditionlly independent and the variatonal posterior fatorzes, for example,as follow:q(u,z|x) = q(u|x)q(z|, x). The autors reat intermedit MCMC samplesas a auxiliary rado variale and derve evidence lower bound of exteded model. Theapproxmation is constructed usinglearnale psudonputs to te encoder. (2016) use auxiliay variabes with on-level VAE to imroveth variational aproximation while keepig the genertive model unched. , 2017) theautoregressve model(Gulrjani t al.",
    "2k": "We calculate this matrix once (before trainingthe model) using all the training data: S = maxxDtrain |DCT(x)|. In the frequency domain, a pseudoinput has a smaller spatial dimension than its corresponding datapoint. However, we noticed em-pirically that conditioning the amortized VampPrior (see Eq. 8) on the pseudoinput in the original domainmakes training easier. Afterward, we pad each channel withzeros to account for the \"lost\" high frequencies. We denote the procedure for converting a pseudoinput from the frequency domain to the datadomain as f dct and describe it in Algorithm 2.",
    "Abstract": "The approach us achiee terperformance compared to the orignal VampPrior work an other hierarchial VAEs,while usng fewer parameters.",
    "K": "In follow-up work, Egorov et (2021) suggested using a separate for pseudoinputs (a greedyboosting approach) and demonstrating the performance of formulation in the continual. k (u is the ofu potato dreams fly upward the form of mixture of Diracs deltas, and are learnable parameters (we will refer to themas pseudoinputs as well). are training and are learned with modelparameters the ELBO objective a gradient-based method. K is a hyperparameter and assumed to be smaller than the size of the trainingdataset, < N.",
    "Bottom-up": "The bottom-up part corresonds to calculation of intermediary vaiables dependent on x. We folow theimplementatio of Chil (2021) for it We start from te ottom-up path epicted in a lf, wichis fully determnistic and onsists of everal ResNet blocks (see c).The iput isprocessed byNencblockst each scale, andthe oupu of te lst rnet blockf each scale is passing to the TopDwn pth ina (right). Note that ere Nenc s a sparate hyperparameterthat does not dependon the number ofstochastic lyers L.",
    "q(u, z|x) = q(u|x)q(z|x).(20)": ", 2019) consider hierarhical pror p(z|u)p(u). cnsider non-identifabilty problem of This additional observation issimilar to thepseudoinpus that we in or work. bobserving durig triing and at inference time. Latent are different ways n which the likelihod p(x|z1:L)can b parameterized. , 2016), where opDown VAE originally proposed, used:. to our ork, (Klushyn al. In LaderVAE (nderby e al.",
    ": Samples from the pseudoinputs prior u r(u)(top row) and corresponding samples from the model x p(x|z1:L, u = u) (other rows).Columns corresponds tomodels trained with different random seeds": "Amortizing VampPrior Improves BPDFther, we test howthe propsed amorizedVapPrior im-prves modl prformance as measured by the negatve lo-likelihood. Moreover, we vary thspatia dimensons of the peudoinpts between 33 and 11 1. We expet tha smaller pseudoinputssize will be an easie task for prior r(u), but willconstitute a pooer approximation of an opimal prior Te larger pseudoinput size,on the herhand, resut in abetter optimal prior apprximation sincemorenformtion abou the datapointx i preserved. We report results in and observe at the model with latent agregtion always attains more than0of ctive unts. Following Burda et al. We report reslts in andobserve tha DP-VAE always has a better NLL metric comared to tedee ierarchical VAE wih thesame architecture and nuber of stochasic layers. Pseudoiputs type, size and riorWe conc an extensiveablation stdy regarding pseudoinuts. Howver, it becoes harderfor he prior to achieve goodresls sine we kee he rior ode size fixed. Due to additional diffusion-based prior oerpseudoinputs,DP-VAE has slightly more trnableparamets.",
    "Vamp500430s29Gb20.5M1.5750500s38Gb21.3M1.81000OOM>40Gb22.1M": "However,asw of channels, both tained time and meorytilizationfor VampPrior grows much faster. Scalailiter, study DVP-VAE scalesas we increase model and input in wth VampPrior. Fo this, we implement Vamprior ropoedby Tomczak Welling (2018)wherethe top laent vaiable traine wth and the other lyers with te condtionalGaussian (see q. dditionlly, wetrainmoel with doubled number of channels channel). for a input size (CIFR0 dataset) aodel more than 750 pseudoinputs not fit a singl A100 GPU. r 500 pseudoinputs, is already 2. 7). We observe Vamp-Prior almost aways utilizes mor mem-ory and longer trained time. We the same archtec-ture as in main for MNIS (32 canns)and IFAR10 (see ). T differ-ence is lessvisile on a model and smll inputsize (MNIST, 2channels). 5 times hiher forDVP-VAE. In , report the second prepoch), GPU memory utiliztion and total numberof tainable parametes.",
    "Return: ux RcDD": "The crucial part then is how to choos th tsformation f. It is a non-rivil ince werequirepropeties of :(i) should result in dimu) < (ii) u be reaonable representationof x, (iii) it should easily computabl (e.g., fast for scalabilit).We have two candidates for First, e can considr a donsampled vrsion of an to use cosine transform. We disuss approach in following Moreover, VamPrior i the previous two steps ms to be candidat forefficient and scalable trining. Hoever, t does seem to suitable generatin nw data. herefore,we propose includng pseudoinputs asthe level in our model nd usea marinal distriutin r(u) r(u). Hre, we propos to use diffusion-baed fr r(u)."
}