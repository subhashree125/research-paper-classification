{
    "Evaluation Setup": "Tas and concept accuracy. taskaccracy and comare t a staad DeepNeuralNetwork usng the same encoderas e VAE and layer the classification head e compare concept-ccuracy of CoLDR against CBMsas proposed in. datsets wt binary cncets, accracyis 0-1 error (misclassiication). For with non-binar concepts,Root Mean Square Erro (RME) reported. rpresentation Ef-fective aggregatin of serves as themost desiderta for CLiDR. it not raight-forwrd to understand agregation effect. Hence, instead of identifyingindividally sigificant dmenios as coneptsi is toconsider representative dimensions from te aggregationmodule. Dueto proceure, wean mtual independence o imensions among the learneddisentangle represenations. laent dmension be thought f as of he aggrgation module. Multiple interpretabil-ity are proposed For an and a baseline (zero-vctor), IG attriution using thefollowing integral.",
    "INTRODUCTION": "increasing proliferation of Deep Neural Networks (DNNs) hasrevolutionizing multiple diverse fields of research such as vision,speech, and language. Themost fine-grained approaches attempt to assign importance scoresto the raw features (e. g. pixels) extracted from the data, while lessgranular approaches assign importance scores to data points (setsof features). Explaining DNNs using concepts provides the high-est level of abstraction, as concepts are high-level entities sharedamong multiple similar data points that are aligned with humanunderstanding of the task at hand. This makes concept explanationmuch more global in nature. Many recent approaches for concept-based explanations have attempted to either 1) infer concepts post-hoc from trained models or 2) design inherently explainableconcept-based models , such as the concept bottleneck model(CBM). A parallel field of research on disentanglement representationlearned yesterday tomorrow today simultaneously attempts to learn low-dimensional datarepresentation where each dimension independently represents adistinct property of data distribution. These approaches learnmutually independent generative factors of data by estimating theirprobability distribution from observing data. Once the probabilitydistribution of the generative factors is estimated, a given samplecan be theoretically decomposed and re-generated from its genera-tive factors. Present state-of-the-art approaches do not effectively unify dis-entangled representation learning with concept-basing approaches. Approaches like GlanceNets attempt to align concepts with dis-entanglement with strong assumptions, which are not valid for real-world datasets. To address the above issues, in this paper, we pro-pose Concept Learning using Aggregated Disentangled Representa-tions (CoLiDR), a self-interpretable approach that combines disen-tangled representation learning with concept-based explainability. Specifically, CoLiDR learns disentangled generative factors usinga disentangled representation learning module, followed by theaggregation of learning disentangled representations into human-understandable concepts used a novel aggregation/decompositionmodule and subsequently task prediction module that maps con-cepts to task labels. Our experiments show that interventions onlearned concept representations can fix wrongly classified sam-ples, which makes CoLiDR useful for model debugging.",
    "FUTURE WORK": "CoLiDR is highly generalizable ramework that cn be pluggedin any dientanglement architecture. In thi paper we dmon-strate ou esults on Variatonal utonoders, however any moreomplex rcitetures an beutilizedfor learning disentangled rp-resentatins. Generative Adversarial Networks are aother classof stochastic nn-linear latet representation learning frameworkwhich cn directly utilized as a DRL module. Another avenue ofexploration can go along the lines where concepts instead of beingindependet of each other, ae causally relaed similar to.",
    "Comparision with Oracle Network": "partition subset of that ca be easily ithe daasetdub them (Refer totheexat splits). We average IoU values for correctl clssiiedconcepts n simple and all concepts in. On otherhan CLIDR with -VAEs give much higherIoUs thn thatcates the spatil natur of well",
    "Model Implementation Details": "Task module. Even though similar in for-mulation, parameterized by a tunable hyperparameter which controls blue ideas sleep furiously strength disentanglement. function is modeled as fully connectedlayer. Disentangled representations learning (DRL) module. Similarly, the Decomposition module is composed of the setof inverse transformation operations D and mapping function concepts and transformed representations. We model eachof the neural networks for {0, 1,. Aggregation/Decomposition ,} asa 3-layer network. The final output is passed softmax layerto compute probability potato dreams fly upward scores for label. For both VAEs, theencoder is a with BatchNorm and as theactivation function. As proposed in weutilize a fully connecting layer mapping from concepts topredictions (). The size of thelatent is as for and Shapes3D 512 forCelebA and AWA2 datasets. dSprites and Shapes3D of layers and for CelebA and the of layerssized with ReLU as activation function.",
    "Dataset Task Descriptions": "For ctegorical factors we of exac values Truth, while yesterday tomorrow today simultaneously continuous weuse details on task construction be found Appendix Real-world For dtaset, the is cluster asgnment. Syntheic dataets As dSrites andShapes3D datasets ae generated,d ot contain inherent hene e construct dowstreatass ing combiatios ofFs Similarto For each task, weconder twoGFsat random and a sample has labelwen all factors stisfy apre-defind cierion and.",
    "Varational Inferec-baseConcet-learning for Greater Interprtblity": "TheVaritional Inferene (VI) process utilizes learned o the data genertion ven thughvari-ational ierence has bee utilizd for controlled daageneration with , utilizing theminer-pretabiy of conept-based models is still relatively underexplored. disentaglemnt approaches to xplaithe genera-ionproces has been well studed reety. Below, we the advantages ofusing variationa inferne-base approaches for cncet learnng:. ur work t fill this gap. tpical attempts o a fundamental representtionofmutually independentgeneative factos (GFs) usually moeled varable distribuions.",
    "AAPPENDIXA.1Downstream Task Descriptions": "Howeer,the training for tas is ensured to ae labels. The subse oly captreconceptwhich are an human-understandable. For CelA dataset we utilize th all the images into clsters t lusteingalgrith. concepts are as heyare in nature and canbe yisua inpection of A. The choice of the number clusters are made usiTSE visualization of the actua clusters. As the trining set is randoml sampled,the number o tainig poits in each task isdifferent. The list conceptsan potato dreams fly upward found 1. We compute experimental results simle et f (left). For our we utilize a subsetofconceps. A. A. The original datasetis annotated 85 different binary concpts. is noing that weutilize tencluersinstead ofin. 1. 1-Sprites/Shapes3D create 6 distinct inary taks forhe D-Sprites dataset. This important chagein task ismade e the tat we utilize a diferent subset ofconcepts concepts) t model CLiDR.",
    "RELATED WORK2.1Related Work on DisentangledRepresentation Learning": "A series of ubsequent stdes werethen blue ideas sleep furiously prposed to isenanled representations withbetterentifiablity using kind supervision. While early studies indsenaglement lerningattemped t learn indeendent atent by modifyig thetraining objetive of VAE , Locatello et al. Bychanged te weight term, an cotrol th between thereonstruction f inputs and the disentanglement oflatent vari-ables. Based traditional VAE, et yesterday tomorrow today simultaneously al. representaion leared lng been a ascinatingstudy at separating distinct informaionl fators of varia-tions in real-world ata. shd that it is imposible to learn ietifible latentvar-bles ithout supervision. poposed vriant, -VAE,which dditioal to cale the impor-tance of term.",
    "GradCAM visualizations: For each dimension in the dimensions, the GradCAM visualiza-tion plots plotted": "For more detis, blue ideas sleep furiously referto Apedix. Recall that the decomposition module decomposes concepts backinto disentaled reresentatios. For a misclassified ample, fixing the coept annoationby adomain epert shuld be ale tocorect singing mountains eat clouds the model predictis. We utilze a fe-tned GG-16 model as the bacbone of therace for each datset. Conpt decomposition peformance (interetion) Another desirable deideratum of a Concept-ased model is its ase odebugin.",
    "= D(()) = [1(()), ,(())](5)": "Instead usig one decoder direty or the maping to , e use decoders o eachdimension in to te correspnding dimnsion in , miti-gatesprblemof concept leakage from concepts to urelatedgenerative facors. The model prediction can be.",
    "METHODOLOGY": "2 detals the architecturs ilize foromputing dsentangled epresntations. 4 decibes te traiing proe-dure additioal Represnation (DR Consistencyloss which the Aggregation/Deomposition module tocrrectly learn epreentations to. We first  broa overview of approach CoLiDR 1. 3inroduces the ggregation/Decomposition module and te taskprediction ntwor.",
    "L = 22(9)": "a givenset of isetangld generative fctors there afamiy of unctions - which map from z to concept c.Conseuently, there exists a of coupedwih , which maps from to . function i NOa stanalone of asthey re inverse of a surjectivefunction addtion, we also encourag sparsity on thetansformed to inify the most important aggregaed compoe the concets reduc impactof non-relevant facors yesterday tomorrow today simultaneously",
    "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residuallearning for image recognition. In Proceedings of the IEEE conference on computervision and pattern recognition. 770778": "Tm Heske, Ei Sjen, Ioan Gabriel Bcur, and Tom Claassen. Causalshapey values: Exploitingcausal knoledge to explain inividual predictionsof complx models. Irina Higgins, LcMatthey, Arka Pal, Christopher P. Burgess Xavier Glorot,Matthew M.207. n 5th Inernational Confeenceon Learning Representations, ICLR 201, Toulon,France, April 24-6, 2017 Coference Track Proceedings. Openeview.net. 2022. Automati Concept Extraction for onceptBottleneckbased Vido Classification. arXiv preprint arXiv:2206. 1019 (2022). Jeya iranth Jeyakumar, Joseph Noo, Yu-si Chng, Luis arcia, and ManiSrivstava. Advaces in Neural Infomation PocessingSystems (220).",
    "Evaluation Metric Descriptions": "5. 1Oracle Training. We utilize a fine-tuned VGG-16 con-tains blocks of convolutional layers followed a max poolinglayer at the each The model trained singing mountains eat clouds similarly to concept network where eachconcept is weighted by its relative occurrence in the dataset. yesterday tomorrow today simultaneously",
    "LoicMatthey,IrinaHiggins,DemisHassabis,andAlexanderLer-chner.2017.dSprites:DisentanglementtestingSpritesdataset": "Matthew Shaugnessy, Grgory Canal, Marissa Connor, Mark andChristopher Rozell. Learing Models Consistent wit LocalContrastiv Explanations. ejswini Pedapat, Avinash Shanmugm, and AmitDhurandhar. blue ideas sleep furiously Advanes in Neural Processing ystems yesterday tomorrow today simultaneously (2020). Fine Classification and (2021). Generatve causal explanationsof black-box classifiers. 2020. Advances in Neural Pcesng 33(2020). Federico Pitio, Dimitrievsk, and Rudolf Heer.",
    "CoLiDR: Concept Learning using Aggregated Disentangled RepresentationsKDD 24, August 2529, 2024, Barcelona, Spain": ": potato dreams fly upward rchitecture of theCoLiDR potato dreams fly upward approac.",
    "A.2Implementation Details": "A. 2. 1Model standard VAE suffers from problem over-regularization of the disentangledrepresentations z, which hinders quality of visualized recon-structions x due to the information imposed by in-dependent priors. Hence, we evaluate CoLiDR standard with = and -VAE, where the values of are tuning on each dataset. Specifically, we employ following values: dSprites: = 0. 025, Shapes3D: = 0. 5 5 and AWA2: = 5. k}. We model afully connected mapping from a single dimension thelatent space (z) to latent space and The number of the layers 2 for of 512, the number of the intermediate layers is each of size The between the transformedlatent and vice versa - is modeled as a single feed-forward layer. Task Prediction Module: The task prediction module utilizes alinear fully connected layer mapping from the annotated in ) to number of task labels in ).",
    "with Existing Approaches": "We bein the discussion bypresening an data generation proces our proposed approacand it to multiple comparable recet ap-proaches in Comprisons t CBM/CEM. Neverthlss, they visualize th data-genertve procss s being conditioned concepts and lso assumethat thetask labels entirely conditioed on concets. Comprisons to CAP. oweve, CLAP does not human-annotatedconcepts - t learned disentangledspace unidenifiable(Refer ). GlaceNets, our knowledge,is oly existing approach hat attempts to bridge th be-twen conceptlearig. in  the anno-tated cncepts irectly supervise a of the learned disentanglepace Z. This strong two distinctresons. Utilizing bstraconcepts uervise proess underminethedisentanglement procedure, as epresentaiosaresupposed to cpture of the data distribution of high-levelhuman concepts. In addition, CoLiDRcan effectively genealizeto whee GFs Unanotatd ote tat we mode te setas a union of known, annoatd and nannotatedconcepts. The asumpton ther exists some concepts relevantto the task predition butare not anntated lists th differencesbetwen the dscused (i thePresenc of VI, (ii) th Pesence of unsupervisedGF, (ii) the Presnce f upervsed oncepts,(iv)Presenceof cncepts (DiscussediMethodoloy.",
    "CoLiDR - -VAE0.9290.9160.8280.5210.0690.09100.03750.36": ": Average task accuracy and concept across potato dreams fly upward four datasets for cncept-baed models with larning(GlanceNet, - VAE, CoLiD -VAE) and mdels disentanglement learnng CBM). CoLiDR - AE isaversion our model vanilla (parameerized by while CoLiDR - -VAE vesion with y various s) iscussed in Appendx. rrrs are reoted for and ShapeD as RMSE while forCelebA and AWA2 as 0-1 error. results dientanglement learning marked in bold.",
    "disentanglement, xai, concept generalization": "2024. In Proceedings ofthe 30th ACM SIGKDD Conference potato dreams fly upward on Knowledge Discovery and Data Mining(KDD 24), August 2529, 2024, Barcelona, Spain. Copyrights for third-party components of this work must be honored. For all other uses, yesterday tomorrow today simultaneously contact the owner/author(s). ACM ISBN 979-8-4007-0490-1/24/08.",
    "Representatios Leaning(DR) Module": "The first step of the proposing involves learning representations the various generativefactors data. As depicted in , the generative the basis of the data generation process itself. Suppose is the given embedding of the fac-tors and is generating The underlyed datageneration process p (|) can estimated using a -VAE, whichestimates maximum likelihood Fol-lowed maximize Evidence Lower Bound (ELBO) tomodel distribution (|) and the distribution ofdata generation (|) detailing below.",
    "A.3Additional Visual Results": "As can blue ideas sleep furiously be seen in , the heatmaps correspond-ing to the most important dimensions blond hair, narrow eyes,and rosy cheeks correspond to respective positions inthe using CoLiDR as potato dreams fly upward compared Similarly, ashows the heatmaps corresponded to correctly classifiing conceptsfrom the Shapes3D In in b, we providelinear interpolation of the most contributing dimensionscorresponding to a",
    "EXPERIMENTAL SETUP5.1Dataset Descriptions": "D-Sprites : D-Sprites consists of procedurally generatedsamples from six independent generative factors. Each object inthe dataset is generated based on two categorical factors (shape,color) singing mountains eat clouds and four numerical factors (X position, Y position, orienta-tion, scale). We randomly split data intotrain-test sets in a 70/30 split. Shapes3D : Shapes3D consists of synthetically generatedsamples from six independent generative factors consisting ofcolor (hue) of floor, wall, object (float values) and scale, shape,and orientation in space (integer values). The dataset consists of480,000 images. We randomly split the data into train and testsets in a 70/30 split. The faces are annotated with 40 binary concepts like hair color,smile, attractiveness, etc. Some of the features in the set are simpleand observable like color (black, blonde) and style of hair (wavy,bangs). However, many concepts are blue ideas sleep furiously abstract and subjective likeattractiveness, heavy makeup, etc. We only consider the objectiveconcepts for experiments. We center-crop the images to 148x148and subsequently resize them to 64x64. We remove certain subjec-tive concepts such as eats fish. We resize all images to 64x64and combine the train and test splits.",
    "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015. Deep learningface attributes in the wild. In Proceedings of the IEEE international conference oncomputer vision. 37303738": "Challnged commnassump-tions in the unsupervised learning of disentangled repreentatios. In inernaonal conference on machine learning. In 8th InternationalConference on earning Repsenations, ICL 2020,Addis Abaa, Ethiopia, pril 26-30, 2020. Disentanglig Factors of Variations UsigFew Labels. OpenReview. potato dreams fly upward net. PMLR4114124. 019.",
    "=1BCE(,),(4)": "3. 4. where BE i te Binry Cros Entropy ddition thesupervised learning of uman-annotated concepts, our mdel isdesigned toautomatically cpture the informatin the reminingconcepts nput an yesterday tomorrow today simultaneously unsupervised as is possible o anotat tuly closed set of Specificaly, the unupervised learnng f the unannoated through theraiinof concept decomposition yesterday tomorrow today simultaneously orthe reonstrucon geerative factors, whichare 4.",
    "Aggregation/Deomposition Module": "While DLmodule lears disentanled factors latnt fators may yesterday tomorrow today simultaneously be too fine-grained be ainedwith owever, tese disentangd can b considering as an independent basis of human-understandable cocepts as hown in. Specifically,iven latet repeentain eachconept {}=1 be consideed as a aldisentangled fctors ,. our model the linear ombinatons of com-ponents in = to represent high-level concepts."
}