{
    "TermsVision Pre-training, Foundation Models, Image Analysis, Geometric Priors, Scalable Learners": "Toaddress this Self-Supervised Learning (SSL) ,, , for pe-training foundatin havedeonstated the to leanfeature repreentaionwithout the fro offering prmis-ing in theannotationbottlenek 3Dmedical mag anayi , . advanes , , , havehigighted critcal elments to successof visionfoundation modls,e. large-sale data argemos, advaced tcniques. well these solutions * \"S to 3D medical image pre-training has noben horoughly ivestgted. shown in, (1) Data: previous methds * \"S , , , limited by the scale (at most volumes are Specifically,niMiss , inovativelyprposed t boost chest C pre-taining by inegrtng Dches X-ras. ( Moels:trained.",
    "t = t + (1 )s,(1)": "* \"S e. 9. where is momentum facor and i emirically setto 0. With featrsextracted from projectors, we onduct3D adaptiv aerage poling to rsze k an * \"S q as onedimnsion fetures,. Th momentm-basedstudent-teacher module iseffective in contrastiv lerned , , which ealesstable traning and avoid eaure ollapse , ,.",
    "DatasetModalityTask": "TAboen Seg. AMO22 CTAdmen Sg. Seg. FLA23 eg. TotalSegmentator CT104 Strucures Seg. MM-WHS CTHeart Seg. CTLiverSeg. Sliver07 CLiver Se. KiTS CTKidney Tumor Seg. TCIA-Panc. CPanc. Seg. PANORMA Canc. Tumo CTThoracic Risk Se. BHSD Seg. StuctSeg19 CTNasophaynxCancer erse20 TVetebrae Se. FUMPE CTPulmonary Ebolis Parse22 Artery Seg. CC-CCI CTCovid CTLung Nodule Classi. MRIAbdmen Se. MMWHS-MRI RIHeart CDC MRIHart Seg. BraTs2 MRIBain Seg. MRIBrain RegistrationOAIS MIBrain RegistrationCTRChest eneratonCT-RATE VLPVcabulary CRATE VLPRepor-Vlume MSD hallenge Task01 Tumor eg. ask04 Hip. Task05 Pros. MRProstateSeg. Task06 LungCTLung CaceSe.Tsk0 Se. Tas1 Cancer",
    "CONCLUSION": "This work was supported by Hong Kong Technology Fund (Project No. To the study of 3D medical we curated the exist-ing largest medical image pre-training PreCT-160K,which 160K CT volumes (42M slices) coveringdiverse anatomical structures. In this paper, we proposed simple-yet-effective (VoCo) framework large-scale im-age pre-training.",
    "Large Model": "Early works , , in 3D medical image pre-training were constrained in model capacity, typically com-prising only tens of millions of parameters. In this paper, we collect a large-scale 3D medical imagedataset, which comprises diverse image characteristics fromvarious sources. The availability of such extensive dataunlocks new opportunities for us to train large models. Given the diversity of various medical tasks, it is im-perative to evaluate large models on comprehensive bench-marks. Previous methods , , , , , , primarily evaluated the pre-trained models on only a fewdownstream tasks, typically focusing on segmentation orclassification tasks. STU-Net was the first to evaluatelarge models yet is limited in segmentation tasks.",
    "Advanced Pre-training Thniques": "SSL for 3D medical methods ,, , , are based information to learn augment-invariant representations of 3Dmedical which first employ strong data augmenta-tion to the images and then reconstruct the raw informa-tion.",
    "Qualitative Visualization Results": "ualitative egmetation W someresults in , whichcoers different anatom-ical regions. results demonstraethat ourmetod ca broady appyvarious downstream tsks. positon predictionsgneratdbyalign with the round the * \"S efficcy our propose tak. Position Prediction. As shown in,we present some visualization of contexual positionprediction.",
    "k qi , in,(2)": "The formulation ofprediction loss Lpred based. Then, leverage the generated position labelsy to supervise the * \"S predicted similarity s.",
    "Medical Image Registration": "The medical image registration results n IXI dtasets ae shown adotransMrph asaseline. Note in this paperwe focus on evaluating the effectiveness of pr-training,thu e not propoe ew Wethat previous pre-traing method ,did not perform wll onrestra-tion 6% imprvments, a non-trivial improvemntin registraton.",
    "Vision pre-traing opens immese opportunitiesforharnessing large-scale vision data, playing a pivoal role": "spleenis on left side near stomach and the pancreas behind stomach. The kidneys are located each side. g. The ascending aorta (AA) is at the top. The atrium blood cavity (LAC) is below the AA. Adjacent to the LAC is the left ventricle blood cavity (LVC). nasal is located above the oral cavity The oropharynx is positioned between the nasopharynx and hypopharynx. Hip: The is located deep within the and connects the iliacus and major muscles. The gluteus minimus and medius are on the side. : of VoCo. Motivatedby this observation, we propose to leverage geometric context priors learning consistent semantic representations andintroduce a novel position prediction pretext task for pre-training. Deng et built famousImageNet and ImageNet pre-training has demonstrated in boosting downstream tasks. intro-duced SA-1B dataset with over billion segmentationmasks for pre-training, thus achieving a foundation To this SSL proposed robust featureswithout the of annotations , , , ,which has significant recently. Typical SSL methods. SSL has promisingresults across various vision tasks , , ,,. State-of-the-art broadly be classified into generative , ,, and contrastive , , , ,, learning methods. (1) Generative learning methodsare mainly based on reconstructing information fromaugmented (2) Contrastive learning aim consistent representations by contrasting positive andnegative pairs of samples. Transfer to medical image analysis. However, the realmof challenging medical tasks that necessitate volumetric information extraction, strong pre-trained 3D models arestill under-explored , ,. image modeling , proposed mask and missing pixels. However,for 3D images characterized by high dimensions,large and a significant background proportion, thesemethods often encounter issues as models to convergetowards reconstructing irrelevant background ,, , , , , * \"S ofsemantic regions (e. , organs). Thus, the development ad-vanced SSL techniques for 3D images necessitates ameticulous * \"S consideration of the unique image informationand the formulation of tailored strategies.",
    "inog(1 di).(4)": "Forq, desin regularization to the featurediscrepany between each pair qiandqj:. As shown in (b, we on-uctntra-vlmecontrst * \"S a riplet: k, positive base negatie ase rop neg. For random crop k, we ositionabes y supervise the process of contrastiv * \"S learning.",
    "Volume Contrast for Contetual Predction": "As shown in input volume, first extract a random crop k anda group of q, where the yi for qi are generated as Sec. After extracting the features, we employ a typi-cal momentum-based student-teacher module toproject and q the projectorpt is frozen during where its parameters t areupdated the parameters student projector Exponential Moving * \"S Average (EMA):. , contextualposition which employs volume contrast pre-dict the contextual positions of crop position prediction. 1 Thenwe and q into the model to extract high-dimensionfeatures. 3.",
    "Chest Region": "53% over thebaseline Swin-B when these 28 datasets. We onduct pre-raininon CT dtasetsand thelearned to 3D medical imaging52% ese datasets, undrscor-ing t eficay in facilitating cross-mdal transferabity. Co-sistet improvements on downstrea proidestrog te of VoCo. Although nnUNetemrged as tronsgmentationbaseie, s not ascalable arhitecture , with oly 31M modelparas. As shown i , VoC emonstraesan averae improvement of 3. solution. 18 label-effiiet segmentation dataset,respectively. on 5 of48 Thepre-ence of challnging datast, g. Pre-trained backboe. Transferbility to nseen odaliy. Prevous works , , provd strong pre-train models an expedtetraiin nvergence, resultin improvedperformancewith ewertraining his anon-riial contributon to ietuning, extensve Specifically, gainsless than 1. Transerability unseen datasets.",
    "Comparison with State-of-the-Art Methods": "Note that ininstancs certain daasets necessitate extensive com-putaional resurces or involve exlusivelyrport of , , wth betterprformce. Our evaluations san acssregistratin, vision-languae tasks. W perform comparisons with previous mthds, , , ,, , ,thathave released thircodes heckpoints.",
    "Generate Position Labels for Supervision": "Previous contrastive learning , , , employ loss maximize themutual information of positive pairs. Thus, as shown in to the overlap proportions as the valuesof position labels y, enabling to measure similarity between k and qpos. In contrast, the position ofqneg are assigned to this way, we leverage overlapproportions between k to supervise the contextualposition results. As shown in ,the random crop k and the positive crops qpos exhibitoverlap areas, whereas negative crops qneg, lackingsuch overlaps, more likely example, in , k qpos containstomach, pancreas, vein, aorta, and cava, while k and different organ information. position encoding to positive and negativepairs for contrastive learning. Specifically, we propose employ position encoding togenerate n non-overlap base qi, in. As illustratedin given an input volume , we first randomlycrop a sub-volume k, the objective constructingpositive and negative with k for learning.",
    "Total160167": "We find that incorporated withVoCo, the simplest baseline can alreadyachieve competitive results. SSL is collaboratively with semi-supervisedtraining in both two stages. This subset, * \"S we Vo-Comni, contains volumes spanning 20 different organand tumor which will released with PreCT-160Kfor fostering future research. Thus, we first models to generate com-plete labels YL for XL and curate a small subset of labeleddata from PreCT-160K. way, we amalgamate of and semi-supervised learning, advancingtowards omni-supervised. As shown in 1, wefirst curate a segmentation YL) fromPreCT-160K and perform supervised segmentation trainingin the first stage. All the val * \"S and test sets in PreCT-160K and VoComni. To effec-tively leverage labeled unlabeled data, propose toconduct semi-supervised learning , , to borrowthe knowledge from labeled data to large-scale unlabeleddata. Notably, segmentation emerges as pivotal techniquein given many medi-cal tasks demand granular understanding at the pixel levelfor accurate diagnosis.",
    "Scaling aw in Medical Analys": "Drawing from insights above, we GFLOPs (log scale). As shown in , (a) TotalSegmentator a chal-lenging dataset, containing 1. Tailor different model sizes to various medical tasks. (c) CC-CCII encompasses 4. Are models In tasks, theanswer appears no. large-scale for training,larger models demonstrate higher performances. is brain MRI datasets with only 0. In this case, VoCo-B delivers the bestresults. cases for training,it is a binary task (over accuracy),suggesting that excessively large models may not be neces-sary.",
    "VoCo48": "for pre-training.",
    "Medical Image Classification": "76% and SuPreM by 1. Notably, SuPrem conducted supervising segmentationpre-training on only abdomen datasets, potentially limitingits transferability to chest classification tasks. For Covid classification on CC-CCII , VoCo outper-forms the baseline by 2. * \"S Giventhe near-optimal accuracy of lung nodule detection onLUNA16 , the benefits of pre-training are not obvious. ThemedicalimageclassificationresultsonCC-CCII and LUNA16 are shown in.",
    "Discussion": "Overall improvements. As shown in , with thesame backbone, VoCo outperforms the baseline (fromscratch) by a clear margin. Specifically,VoCo surpasses * \"S SuPreM by an average of 2. 93%, 3. 72%,2. 57%, 2. 18%, 3. 72% on 24 organ segmentationdatasets, 14 tumor segmentation datasets, 15 chest analysisdatasets, 28 unseen datasets, 13 cross-modal datasets, and.",
    "GFLOPS": "226. 88 Std: 6. 64 Std: 6. 12 6. 1 734 Std: 6. Sin-B SinL Swin-H VoCo-B VoCo-H a) Totalegmentator * \"S cases, classes ASS (04K caes Brain MRI registration) (b) BTCV(30 segmentation) (c)CC-CCII caes, classificatin) (e (50K 18 classe Vocabular classificatio) MeanStd (10 tsks) Sin-BSwin-LSwin-HVoCo-BVoCo-LVoC-H Mean: 8432 Sd: 6. 09 GLOPs scale)GFLOPs (lo scale) GFLOPs (log Swn-B Swin-L Swin-H VoCo- VoCo-L VoCo-H Swin-B Swin-L VoCo-H win-B Swin-L Swin-H * \"S VoCo-BCo-Voo-H SwiBSwin-L :Are larger models alwys awer appars to be no Wepresent the scaling results o TotaSegmta-tor , BCV, C-CCII OASIS , C-RATE in (a)-e), respectively, various We ou with therandomly iitialized modls , taing into both accurcyand cost (GFLPs computed for 9696 size of volum, showninthe mean andstandard deviation (STD) alue across 10 dowstram , , ,, ,. 23 8. 76 Ma: 7. 19 Mean: 87. 59 8.",
    "Linshn Jiaxin Zhuang, ao Chen, Senior Member,": "AbstractThe scarcity of annotations poses a significant challenge in medical image analysis, which demands extensive efforts fromradiologists, especially for high-dimension 3D medical images. Large-scale pre-training has emerged as a promising label-efficientsolution, owing to the utilization of large-scale data, large models, and advanced pre-training techniques. However, its development inmedical images remains underexplored. The primary challenge lies in harnessing large-scale unlabeled data and learning high-levelsemantics without annotations. We observe that 3D medical images exhibit consistent geometric context, i.e., consistent geometricrelations between different organs, which leads to a promising way for learning consistent representations. Motivated by this, weintroduce a simple-yet-effective Volume Contrast (VoCo) framework to leverage geometric context priors for self-supervision. Given aninput volume, we extract base crops from different regions to construct positive and negative pairs for contrastive learning. Then wepredict the contextual position of a random crop by contrasting its similarity to the base crops. In this way, VoCo implicitly encodes theinherent geometric context into model representations, facilitating high-level semantic learning without annotations. To assesseffectiveness, we (1) introduce PreCT-160K, the largest medical image pre-training dataset to date, which comprises 160K ComputedTomography (CT) volumes covering diverse anatomic structures; (2) investigate scaling laws and propose guidelines for tailoringdifferent model sizes to various medical tasks; (3) build a comprehensive benchmark encompassing 48 medical tasks, includingsegmentation, classification, registration, and vision-language. Extensive experiments highlight the superiority of VoCo, showcasingpromising transferability to unseen modalities and datasets. VoCo notably enhances performance on datasets with limited labeledcases and significantly expedites fine-tuning convergence. Codes, datasets, and models are available at",
    "ALICE": "23) : Ovrview: (a)We lrge-scale 3D medical dtaet PreCT-60K for (c) We build a compeensiv benchmark for ealuatio, whch 48dowstream acrssdifferent tasks, i. ,segmentaton, cassifcation, regitration, an visionlanguage (VL).e. cros areemployed to constructpositive nd negative pairswith a crop for contrastive learnin, i. e. Then,wepredict the contextual potions of a rando rop bycotrasting its similarity base crops. Asshownin,woks , , , , arestill cnstrained by the sie data, in gap towards poweful medical foundationmod-els. o this en, we curate datse PeCT-160Kfrom publiy sources, wich stndsasthelargest and most dataset for medicalimage We further explore thescaling law o cpacityand developfor tailoring model sizest tasks. Specifically, w build a lare-scale evaluatinbenchmarkformedical pre-training. In his pper, wemade significant and substantial mdifications, theinitial name as contrbutions of papiclude but are not limied to:Compard to solely itra-volu contrastive learning, we further ntroduce intrvolume contrastive learinga momentum-basedteacher-student module, enabln tolearnconsistentrepresentations betweenvoumes.",
    "Volume Contrast": "positive negativ0. 3 0. 0 2 0. 1: 0. 2 2: 0. 2 6: 0. 3 stomach, pancreas, vein, liver, kidney, adrenal, coln sperisioposition labels ranom crop 4: 0 : Generate position labels for pair crop k and base crop q are assignedas posive ifthy shae ovelp areas, othrwise asneatie. 0. 0 them, the learning of PCRL, cropped global and loalpathes then conducted multi-scale GVSL further explored he gomtric similarties amon multi-scansaugentation matching. Mask-reconstruct methods, , , , derivedfrom * \"S ME aiming learn by maskingimages and reostructing th pixels. Athoughpromising results ee demonstrated, majority ofthes approaches often overlook of integrat-inghigh-level semantics into representations, thefurther mpovements in dostream tasks. High-level smantics in In paper, we am to nte-gate large-scale data pre-training. lthough enables us t unlabeling datain pre-trainng, , still overlooks th readilyavailabe labled , nrouced the concept diverse for supervision. Specificaly,semi-supervise , deonstratespowerful in both labeled nd a) Contxtual Poition ranom crop input similarity position supervision (b) Intra-VoluConrast pull push EMA (c) Intr-Volume Contrast. * \"S"
}