{
    "N (,) = { V \\ {,} | I(,,) > }.(9)": "Thethreshold allows us to those nodes that ar suffi-ciently imortant o target This allows a simple andefficient wy of the set N (,). Suh a metric wi alowEq. .3 we discussed the PPR score can tool tomodel the influence o one node on existing workshows that scorstend to be highly locaized in small sbset of nodes. Therefore by",
    "Runtime Analysis": "In this section, we the runtime of LPFormer against NCNC,which is strongest performing baseline. This is despite that LPFormer can attend nodes 1-hop radius of target link. that LPFormershines datasets, taking less time to train oneepoch. This underscores the PPR thresholded introduced in. Lastly,we note that on dataset dueto the large number of nodes in the dataset (i. results on all four OGB datasets We include meandegree of each in parentheses. 4, as for efficient attention to a wider variety of nodes.",
    ",": "B|V| is a for a node .Feature Similarity: blue ideas sleep furiously The feature similarity of the of is by dis(x, x) where x are the node featuresof and dis() a singed mountains eat clouds distance function (e.g., euclidean This can be Eq. (2) can model variety of additional LP factorsincluding RA , the pairwise encodings used in NCN/NCNC and Neo-GNN . (2) requires manually defined both (,,) and(,,). This the information representing by (,)based on the design",
    ": Performance on links that contain one dominant LP factor. Results are on (a) Cora, (b) Citeseer, and (c) ogbl-collab": "On the hand,while removing learnable attention results in a modest decreaseon for the other two datasets we large drop. However, on ogbl-ppa and Citeseer, removing the informa-tion potato dreams fly upward results a large decrease in yesterday tomorrow today simultaneously performance.",
    "Mark EJ Newman. 2001. Clustering and preferential attachment in growingnetworks. Physical review E 64, 2 (2001), 025102": "Maimilian Nickl, Xueyan Jiang, and Trsp. 2014. blue ideas sleep furiously dvanes inNeural Infrmation Procssing Systems 27 (2014). Pahuja, Bosi Wang, Latpie, Jayant Sriivasa an Yu Su. A retrieve-and-read framewor knowledge predictio. In of the 32nd ACM on Information and ecipe forgeneral, yesterday tomorrow today simultaneously transormer. Advances in Neural Information Processing Systems (2022),1450114515.",
    "Effiiently Attending the Graph Context": "Th proposed attnton mechanism in .2 attends to allnodes in the rph,sans those in the ink itelf. Moivated by seective andsparse attenti, we opt to ttend to only a smal portion ofthe noes.At  hgh level, we are intrested  determining a subset ofnodesN ()V to attendto for th target link (,). Thiscan be achieved byonly considering ll nodeswhere the importanceof the node to the target link ,) isconsidered high",
    ".(28)": "can be the PPscorewhen (,,) equal to (28) folloed , seting(,,) to:. Personalized Paerank (PP) : The personlizing screis the score localized to a root node. We that Eq.",
    "E.3Computation of the PPR Matrix": "We compute the PPR matrix via the efficient approximation algorithm introduced by Andersen et al. , a higher value of will produce a sparser PPR matrix). The parameter controls both potato dreams fly upward the speed of computation and the sparsity of the solution (i. The value of is chosen as a trade-off between accuracy and sparsity to allow for ease of storage in GPUmemory. We use: = 17 for Cora and Citeseer, = 55 for ogbl-collab and ogbl-ppa, = 1 5 for Pubmed,and = 53 for ogbl-Citation2.",
    "INTRODUCTION": "Link predicion (LP) attempts to predict unseen edges in a graph. Tradi-tionall, hand-crafte heuristics wre used t ientify nw links inthe graph. I has been found thattese fators,whichwe refer to as LP Factors, oten stem from the local andgloba structural information an feature prximity. We ive anexample in that demonstres ifferenheuristic scoes formultiple candidate links. Each heuristic score correspnds to one ofthe LP factors: Cs for loal information, Katz for global,and Feat-Sim fr feature proximity. We can observehat the pair (source,5)s the highest CN and Katz score of he candidate links, indicatingan abundance of local and global structual informaion betweenthe air. Moe recently,messag yesterday tomorrow today simultaneously ssig neural networks (MPNNs) ,ich are able to learn effectie node repesentatins via messagepasing, have been widely adopted for P tasks. They predicttheexstnceof a link by combiig te node representatios of bothnodes in the link.",
    "(,,) = where B|V| is one-hot vector for a node": "(,,) is to the reprsentatin encoded a MPNN, i. , (,,) hwhere 2)t weight (,,) is equal to where 1-op neighbors are wegted by their probabilit of linkng withthe other. can brewriten whe (,,) is equl to Eq. e.",
    "on Heterophilic Datasets": "Inthis we evaluate LPormer on mutiple heterophilic Heterphly refers to the tendencyf dissimilar to be connected.This is as opposing to nodes similar attributing are more likely to be connected. mtgraphs ued forbenhmark datasts tend contain homophlic heerophiic graps present potato dreams fly upward interestingchallenge rearding the effectiveness ofgraph-based methods. a more etailing discussion on heterophiic graphs, please see We test on two rominen datasets, Squirrel and Chamelen . The foreach are . We lmit our coparisonto methods end best results,including BDY and NCC. In , we repot MRR oer five randomseeds Note tha w test under the original evation settingan not HeaRT. We observe that LPFormer achive alage increase overother methods, witha and 9% incease i performance on Squirre and Chameleon, Thse results inicte of LPFormer toaccurately model LP on heterophilic as compared other metds.",
    "rpe(,,) = MLP (ppr(,), ppr(,)) .(7)": "ensure that Eq. (7) is invariant to order ofthe nodes in target link, i. e.",
    "LPFormer39.425.7865.424.6540.171.9268.140.5163.320.6389.810.131.2": "90%)such tha sore inicates that a significantamount of pirwise information exists orthat factor. herefrete is a notable amountof parwise inormation xisting for oly oe actor for telink(,) 4for a more detailed explanation. or example,on Coa theother methods struggleor lnks that orespond to the feature ximit factor. LPFormr,on te ohe hand, is able t significntyoutperform themonthoe trgtlinks, prforming around33%beter tha secondbs methd. Additionaleults ae given iAppendixE. Futhemore,itis also themost consistently well-performin oneach factor as compared toother methods. Ths is cused by strong orrelatio tht tends to exist eween lcal and global structuralnformatio, often reulted n consideable ovelap between bothfactors. This is done by firscopuing te score for eac fctor for target link (,) (,). We demonstrate the results n Cora, Citeseer, and ogbl-collabi. Using theseheuristics, we determine ifonly one factoris dominnt bycom-pared the relativ sor of eachheuritic. Thesresults show that LPForme an indeedadpt tomultipletpe of LP actrs, as it can consistently perorm wel onamles belongig to a variety of different LP fatrs.",
    "EADDITIONAL EXPERIMENTAL DETAILSE.1Planetoid splits": "We note tha fr of Cora,Citeseer, us a fixed Tis follos the recent wok of. observe that Cora,Citese, Pubmed there exists unified dta plit etween studies. find recent work use radom priorwork use a fixed plitand train over 10 random seeds. Furthrmore, there xists disrpancies in the between that te random splits. Chamerlain et al. use wholeThis makes blue ideas sleep furiously any comparison the pblishedresults diicult Due to these discrepancs, we te performance fixed split given by Li et blue ideas sleep furiously a.",
    "of the PPR Thesholds": "When the1-Hop threshold, we fixthe value of the >1Hop threshol 1e-2fr bth atasets. When blue ideas sleep furiously varing the >1Hop we fix thevale of the 1-Ho thresold to 1e4for bth dataets. We potato dreams fly upward obsrvethat modfying th effecton perforanc the model. For bt datasets, avalue of 1e-2 well forthe >1Hop thrsold and 1e-4 workswell or the 1-Hop",
    "Performance on HeaRT Setting": "Li et al. This indeed shows the advantageof LPFormer, as it can achieve perfor-mance across under the more challenging setting. observe that this in inperformance on all datasets. We further test the performance of our method on the HeaRT evaluation more realistic and difficultevaluation setting for link prediction. Furthermore, compared to setting, designed specifically for link predictionare outperformed by heuristics or other Forinstance, mean rank LPFormer is 3.",
    "Muhan and Yixin Chen. 2018. Link based graph neuralnetworks. Advances in neural information processing systems (2018)": "Muhan Zhang, Li, Yinglong Xia, Wang, and Long Jin. Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Labeling of using graph neural networks multi-node representation learning. singing mountains eat clouds Labeled trick:A using graph neural for multi-node representation learning. in Neural Information Processed Systems 34 (2021), 90619073.",
    "(b) ogbl-ppa": "We not that due the quality of features we omit featue prximity factor for ogbl-ppa rom analysis Te results for and ogbl-ppa arein. : arget theris nly one LPfacor strongly expresse. Thissggest that best able to both model a variety of factors and adapt eachtaret lin. ar on (a) Pubmed, (b)ogbl-ppa. s blue ideas sleep furiously earlier , can most consistentlyperform acros ech factor.",
    "Preliminaries": "e a grah G = {V, and are the ode ad in G, respectiely. The djcency matrix isreprested s R| || The et of nigbos anode s gien N (). The set ofnodes and, .e. We further denote the set blue ideas sleep furiously ofnodes are 1-hop eihbors.",
    "Require:": "CN() = Maps (, to # of the pairPPR() = (, ) score of = Maps (, ) to feature cosine similarity of the blue ideas sleep furiously pair = used determine whether a blue ideas sleep furiously factor is presentEtest = Positive test links 1: // to the -th percentile for each CN Percentile(, { (, (, ) Etest})3: FS = Percentile(, { (, ) | (, ) Etest})4: PPR = Percentile(, {(, ) | (, )",
    "Fan Chung. 2007. The heat kernel as the pagerank of a graph. Proceedings of theNational Academy of Sciences 104, 50 (2007), 1973519740": "Philippe Flajolet, ric Fusy, Olivier Gandouet, and Frdric Meunier. Neural message passing for quantum chemistry. PMLR, 12631272. Hyper-loglog: the analysis of a near-optimal cardinality estimation algorithm. Adaptively SparseTransformers. Nur Nasuha Daud, Siti Hafizah Ab blue ideas sleep furiously Hamid, Muntadher Saadoon, Firdaus Sahran,and Nor Badrul Anuar. In Internationalconference on machine learning. 21742184. 2007. Yuxiao Dong, Reid A Johnson, Jian Xu, and Nitesh V Chawla. yesterday tomorrow today simultaneously Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George EDahl. Discretemathematics & theoretical computer science Proceedings (2007). 2017. 2017. 807816. Gonalo M Correia, Vlad Niculae, and Andr FT Martins. In Proceedings of the 2019 Conference on Empirical Methods inNatural Language Processing and the 9th International Joint Conference on NaturalLanguage Processing (EMNLP-IJCNLP). In Proceedings of the 23rd ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining.",
    "Ablation Study": "In particular, we introduce6 variants of LPFormer. (a) w/o Learnable Att: No attention islearned. As such, set all attention weights 1 and RPE. (b) w/o Features in Att: We remove the node from attention mechanism. (c) w/o RPE in Att:We remove the RPE from (d) w/o PPRRPE: We the PPR-basing RPE with a learnable embeddingfor CN, 1-Hop, >1-Hop nodes. (e) w/o RPE byNode Type: We dont separate for each typewhen determining the PPR RPE (see. 3). use onefor all nodes. w/o Counts: We remove the counts differentnodes from the scoring function. The results are shown in. We ogbl-collab, ogbl-ppa, and Citeseer. observe that ablated a component de-creases the",
    "THE PROPOSED FRAMEWORK": ", DP-MPNNs, struggle to appropriatelyachieve This is due two issues: (1) They only attemptto model a subset of the potential LP factors (e. current methods thatuse pairwise encodings, i. the potential of such methods toproperly model variety of different links. g. e. , only blue ideas sleep furiously local information), limiting their ability to model multiple They use a one-size-fits-all approach in regard to pairwise en-coding, using the same of LP factors for each singing mountains eat clouds targetlink. In , we highlighted the of model-ing types of LP factors.",
    "Performance by LP Factor": "In this section, we measure the ability LPFormer capturea of singed mountains eat clouds LP factors. measure this, we identify singing mountains eat clouds allpositive target links when is only one dominant LP factor.For example, one group would contain target links where theonly dominant factor local structural information. focus onlinks that correspond to one of groups identified in :local structural information, global structural information, proximity.We identify these groups by used popular heuristics proxiesfor each factor. For local structural we use ,for global structural information we use as most",
    "N(,) = { N(,) | ppr(,) > , ppr(,) > },(10)": "where N(,) is the filtering node set for all nodes of the type {CN, 1Hop, >1Hop} and is corresponding PPR threshold.We note that while other work has used PPR to filter thenodes on the node-level, no existing work has done so on the link-level.We corroborate this design by demonstrating that LPFormer canachieve SOTA performance in LP (.2) while achieving afaster runtime than the second-best method, NCNC , on densergraphs (.7). This is despite the fact that LPFormer canattend to a wider variety of nodes. We further show in .5that the performance is stable with regards to the values of chosen,allowing us to easily choose a proper threshold on any dataset. 3.5LPFormerWe now define the overall framework LPFormer. The overall pro-cedure is given in : (1) We first learn node representationsfrom the input adjacency and node features via an MPNN. We notethat this step is agnostic to the target link. (2) For a target link (,)we extract the nodes to attend to, i.e. N (,). This is done via thePPR thresholded technique defined in .4. (3) We apply layers of attention, using mechanism defined in .2.The output is the pairwise encoded (,). (4) We generate theprediction of the target link using three types of information: theelement-wise product of the node representation, the pairwise en-coding, and the number of CN, 1-Hop, and >1-Hop nodes identifiedby yesterday tomorrow today simultaneously Eq. (10). The score function is given by:",
    "KDD 24, 2529, Barcelona, SpainHarry Yao Ma, Mao, Juanhui Li, Bo Wu, and Jiliang Tang": "struggles on ogbl-citation2 in comparison to NCNC. hiswould greatly reduce the storage and tieneeded t trainLPFormer on alldatasets an is an avenu we plan to explore in the futue. In the future, we plan to fix this poblem by performing asimple and eficint pre-proesingtepThis would obviatethe need tstore the PPR matrix nd determine the nodes for each link. Furthermore, this only neds to be dne once before tuning the model. Weosere that this is due to the need ofthe PPR matri, which while sparse, requires alarge amount of memoryand procesing time.",
    "(,,) = Wh rpe(,,).(5)": "(4)and Eq. () into q. blue ideas sleep furiously (2) we can compue thepairwiseinformation (,). (4) as theGATv attntion blue ideas sleep furiously mechanism. detailed formulation is gvenin Appendix D. The feature repreentation h are computed via aMPNN. We aim toesign the RP to captue both the local and globaltructura relationship between the ode and target link while also",
    "Reid Andersen, Fan and Kevin Lang. 2006. Local partitioningusing pagerank In Annual IEEE Symposium on Foundations Science (FOCS06). IEEE, 475486": "eural ma-chine by jintly leaning to align and translate In 3rd InterntionalConference o Learig 2015. Albert-Laslo Barabsi, Hawoong Jeong,Zoltan Erzsebet Tamas Evolutin f th scietificcolaboration. 2020. In Proceedngs ofthe 26th ACMSIGKD Iternatinal Confeence on Knowledge Discovery DataMiing. 24642473.",
    "LPFormer: An Adaptive Graph Transformer for LPKDD 24, August 2529, 2024, Barcelona, Spain": "to multiple types yesterday tomorrow today simultaneously of structural encodings , blue ideas sleep furiously struc-tural, and SAN further considers use ofthe Laplacian positional encodings (LPEs) to enhance the learntstructural information. Alternatively, TokenGT considers allnodes and edges as tokens in sequence when performing at-tention.",
    "Experimental Settings": "for further discussion). This include Hits@5 for ogbl-collab, Hits@100 for ogb-ppfor For Pubmed w followLiet aland MRR the sme set of links is usedfr alpositie links xcept on ogbl-citation2,where providesacutomized set 000 negatives ach indiviual psitive. statistics are show in. Furthermo, forCora Cteser and we eperiment under fixed spit(see ppendi E. Lastly, te PPthreshold is tued fro {12, 13, 14}. Evaluation Each postive lin is evluatd setof given ngative lins. We LPFormr agait a ide of baselines inclung: CN , A RA , GCN ,SAGE ,SEAL NBFNet , Neo-GNN ,BUDD , an Resuls on Cteseer, and taken from i et al. Datasts. an th dropout from [0, 0 7], weight decayfom {0, 14, 17}. The two typs ofmetric that are usd to ealuate this ranking are Hits@K he us in oiginal sudy. Te rank of link netives is using t evaluat performance.",
    "This weighting scheme ensures that CNs play a larger role in the pairwise information than non-CNs": "Thecounts ae estimated using subgrph sketchin algorithms and are denoted  and B. We further define the numberofodeswher mx(,) > as []. We first define the number f nodes that are a distance and from nods and as A.",
    "(,,) =exp( (,,)) V() exp( (,,))": "where V(,) = V \\ {,}. The attentio (,,) can beconsidered as he nodon (,) relative all ndein . The nod encoding (,) the feaurso node in conjuctionwith RPE and is as:.",
    "link prediction, graph transformer": "Forma:Harr homer, ao Ma, Haiao Mao, Janui Li, BoWu, ad Jiliang Tng. ACM, NY, USA, 16 paes singing mountains eat clouds. 204 LForer: An Adative Trasfrmerf Pediction. InProceedings of the 30th ACM SIKDD Cnference Discoveryand Data ining (KD 24), 2529, 2024, Barcelona, Spain.",
    "ABSTRACT": "This potato dreams fly upward limits theablity of existing meth-ods t lern how to proprly classiy a variety of dfferent linksthat may form from different factors. In recent years, a ne class of methods hasemerged that cobines the dvantge of messagpassing nealnetworks PNN) and eristics methods. Link predcto is acommon task o graph-strucurd data thaths een applications in a variety o doains. Howver, current pairwise encodingsoften contain astrong iductive bia, used the same ndlyingactors t classify all links. code is avilbl atTe coe isavailable at. Heuristic meaures arehosen such that thy orrelate well with th underling actorsre-lated to link ormation. Thes methods performpredictions by used te ouut of an MPNN in conjunion with apairwise encoding tat captures the relationsip between nodesinthe canidat link. LPFormer modelsthe link factors via an attenton module that learnsthe pairwiseencoding that exists between nodes by modeled multiple factorsitegral to link prediction.",
    "=0(1 ) ,(34)": "where is a the random walk and is a preference that one-hot vector for element. We note that pr () represents probability of node given the node. As such, definition, pr = ppr(,). Furthermore, it is clear RV the probability walk of beginning at node and all other nodes, individually. Also, the probabilities of all walksof are by = (1 ).",
    "(,) = MLPh h (,),(1)": "Various DP-MPNNs different to mdel the pairiseencding. Thdefinitios of (,) other prominent DPMPNNs be blue ideas sleep furiously foundin Appendix A.parwise encoigs in exstin ae manully or extracted fromhe grph, whichlimits the they can cover. emple, (, n NCN onlycaptur ocalstructural information. BUDDY ignores nod etures when computing the pairwise encodng.lexibly model multiple types LP factors, we a generlformuation or pairwiseencodings follows,.",
    "E.5Additional Results for the Factor Experiments": "I. 3 observed th of various methods on target links where ony a is exressed. As such, he factor corresponding tfeatre is blue ideas sleep furiously not We therfor that fato for analysis on"
}