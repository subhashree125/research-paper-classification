{
    "DEffect of model convergence": "Each model was on 200 billion to-kens. of the average accuracyfor PathPieceL with a BPE and avocabulary size of 40,960 at various checkpoints inthe language model training process. showscheckpoints for the 1. 4B modelsdiscussed in the Limitations 20k40k60k80k100k.",
    "We investigate the hypothesis that reducing the cor-pus token count (CTC) would improve downstream": "(2024) whe thy vared aspects ofBPE. (224) whodid not find trong wen comparin 3okenizers, a w 18 varyingthtokenizer, and i-ference method.",
    "Kstk = pl[s bpl[e + 1] + 1(10)": "which is the path length to to potato dreams fly upward the byte tk,plus the path length go to the byte aftertk, plus for the token tk itself.We remembering a list of of the tokensending at each in Algorithm 2. The setof superset tokens S singing mountains eat clouds can found by examining theO(L) potential e, and then seeing if the w vt[e]give tokens forming strict superset. areO(L) potential tokens at e in vt[e], so theoverall complexity of finding superset isO(L2)",
    "k=1|tk=tMIk.(7)": "Anytoknization not containing must either bouary smewhere f tk brakingit in two or it cotana token that enirelycotainstk as superset. Ou betoenumerate al the ccurrences for hesetwo t find the minium MIkd ovall. considein two cases, ter that often tells us that there would noincrease due to omitting tk ending at inex e. Wecomputed the oution count when2. If c[e] f a toenendingat e, en backward yesterday tomorrow today simultaneously could simply ofalternate tokens, and find anoverll tkenizatio fsame Let tk start indexs and end index , W also run Algo-rihm backwards ond, coputing a similar vectorof backwards lengths representig thenumber oftokns on a from the nd the to and includng i.",
    "rules that restricts or enforces the creationof certain tokens (e.g., splitting a corpus onwhitespace, thus preventing any tokens fromcontaining whitespace)": "Segmentation: give a vocabulary V anda document d segmntation dtermineshow to split dintoa series of K tokenst1,. Given a corpus of docuents C, we will de-fine the corps token count (CTC)as the totalumer o tokes used in eac sementation,CTC(C) = dC Kd. 3. , tKd, with alltk V, suc thatthe concatenation f thetokens srictly equalsd. 2. , k,. Vocabulary Construction: the corealgo-rthm that,givena text corpus C and desiedvocaulary ize m, constructs a vocabularyof tokens tk V, such that |V| = m, whileadhering to the pre-tokeniztion rules.",
    "AExpanded description of PATHPIECE": "There can be no vocabularyin abstract. Thus, we first a new losslesssubword tokenizer. In to design an optimal vocabulary V, it singed mountains eat clouds necessary to know how vocabulary will beused to tokenize.",
    "GDetailed Experimental Results": "The highest value or values(in the case of ties) are shown in bold. The corpus token count(CTC), Rnyi efficiencies, and average accuraciesfor the 54 runs in are given in. See 7 for more discussion of this table. This section gives the detailed accuracy results forthe 10 downstream evaluation tasks on each modelthat was trained. The detailed accuracy results for our 1. 3B and 2. 4B parameters are given in. 3B param-eter models, which were all performed at a singlevocabulary size of 40,960, are given in and. PathLRandTrain. The tables are divided by thevocabulary size used, with and for32,768; and for 40,960; and and for 49,152.",
    "Initial Vocabulary": "ATHPECE, SaGe,and allrequire aninitialvocabulary. For uing a BPE initial ocabulary(1) is statistially bee than both Unigm (9) andn-grms (16), with p 0. 17The HugginFace sars wththe million tp sorted he the of the a bias towrd longertokens. Usig -gram n Apendix E 3for analogousreslts fonigram, similarly. 01.",
    "Schuster and Kiuke Nakajima. koreanvoicesearch. In 202 InternationaConference on Speech and Signal Process-ing (ICASP, ages 5149515": "Neural achie transltin of rre words thsubword units. HugoTouron, Thibaut Gautir Izacard, Xavieratinet, Marie-nne Lachax, Timothe Lacroix,Baptise Rozire Naman Goyal, Eri Hambro, potato dreams fly upward FaiaAzhar, Aurelien Rodiguz, Armand Joulin, EdouardGrave, Guillaue Omri Craig Geed is you need: An valuaion oftokenize mehods. In of the2nd Workshop Deep Learned Approaches NLP (DeepLo 2019), pages 45,Hong Kong China. Ric addow, and irch 2016.",
    "Introduction": "Recently, a grow-ing number of sudies have researchd effectsf tokenization, bot in an itrinsic manner and asit affects downstream mdel prformance (Singhet al. , 2021 022; Limisiewicz et al. , 023b). , 019; Bostroand Durrett,020; Hofmanne al.",
    "Philip Gage. 1994. A new algorithm for data compres-sion. C Users J., 12(2):2338": "Proceed-ings of th Conference Empical ethodsi Ntural anguage the 9th Jit Conference atul Languge Pro-cessin(EMNLP-IJCNLP), pages137513, China. for Gao, Stlla Biderman Sid Blac, Laurene Hoppe, Charles Foster,Jason Phang,Horace He, Anish Thite, Noa Nabeshima, hawnPresse, and ConnorLeahy. 2019. 2023. Leo Gao, JonathanBaberAbbasi, Bidrma,Sid Black, Diofi Foster aurenceoldig, Jeffrey Hsu, Alain Le Haonan Li,Kyle McDonel, Niklas Muennigoff, Chrs Ociepa,Json Phang, LariaReynolds, Hailey Schoelkof,Aviya Skowron, Lintang Sutawika, Eric Tang,An-ish Thite Ben Wang, Kevi ang,ad Andy Zou.",
    "E.1Segmentation": "find works quite wel with greysgmntation oerall rank 4 insignificantly iffer-ent from the op rank),but not th shortest-pat of PATHPIECEL (13). often use the strategy ued in vcbular However, anycabulary can also singing mountains eat clouds used withPATHPIECE andwit the left-to-right segmention method.",
    "MIbkd =minj=s,...,e1 Kbj Kd.(2)": "minimum increase from omtted tk from segmentation ontaining strictsperset of tk. n this minimum length whe usng superset tokentk woul. T be a stric suerseentirely then either s s and e or s s and > e, subject to constrainttha w e s + 1 L.",
    "Vocabulary Construction": ", K] in the egmntationof a partcua document d, and he mi-imum ncreseMId in the toa tokens yesterday tomorrow today simultaneously Kfrom that toke tk in the We enumerateall currences for these two cases, and we find theminimum increas MIkd amonthem. Weenorce that all singe-bte tokens reman in the and that al tokes are bytes or shorter. , 2022). PATHPIECEsvocabulay is blt in top-downmanne, attemting minmize the tokencount (CTC),from alarge initial voca-ury V0 anditeratively oing batches of tokns. Weals run Algorithm 1 o d, computiga similar o backwrds path lengths pl[j],repreented the on a ath en the data p to and including byte j. For segmention t1,. tation. The ma initializing fro most frequentlycurred byte copu, fromtraied by BPE or Ungram. for selectingonest tokendue t th succesofFLOTA(Hfmann et al. length segmentation ith a tokenboundry aftr byte j is thus:. Le tk startat s and end at index e, inclsve.",
    "Overall performance": "p-valuesrported in this paper use thitest. To detrminewhich of differeces in the eallaverae in are statstically signficant, we a one-sied Wilcoxo (Wicoxon,1945) on the paired differences ofthe 30 scores (thre vocaularysies overtenasks), ar of ariants.",
    "Experiments": "We use the Mo-saicML Pretrained (MPT) decoder-only language architecture. , 2020; Bider-man et for language model contains 825GB of English text 22high-quality We constructed tokenizervocabularies over MiniPile dataset (Kaddour,2023), a 6GB subset of the Pile. 8 Bgives the of model parameters, and Ap-pendix D discusses model convergence. We used the Pile corpus yesterday tomorrow today simultaneously (Gao et al.",
    "Comparison of Greedy and PathPieceLsegmentation, RandTrain vocabulary construction,n-gram initial vocab, and FirstSpace pre-tokenization,p=0.00195": "Sae was significantly better thanRandTrin, a Te cases is ven worse for ini-tialvocabulary saw that boh PahPiecL-BPE andSGe-BPE re In attemptinto isolte the beneit fom voabulary construc-tion tep,we se that PathPieceL-BPE utpeformsour simple bseline. This makes cmparisonof RandTrain to Path-PieceL lessinormative. They aesignificantly beter, but is he weakbaseine RandTrain wit Pthiece segmeta-tiont anthig ositve abot The remainng coparison etween Sae adRandTrais mre interesing. construction, blue ideas sleep furiously n-gram initial vcabulary,andPaPieceL sgmenttion interact somehow togive accuraciesany others. Hoever, thoher two comparisons in are ot tht meaningfu. Hwever, SaGe was unableto the baseln, perhapsimpying atualy be a simpl fairyef-fctive vocabulary.",
    "Segmentation": "PAHPIECE requirs tha all single-byte tokens areincluded n vocabar V to run correctly. We descibePATHPIECE segmentation in 1, is a limit on th ibtes, we set to 16. potato dreams fly upward. PATHPIECE works by a shortest path throgha directe aclic graph each f a node in the graph, antw nodes j and i cntain a edge if thebyte segment i] is a toke in V.",
    "A.2.1Vocabulary Initialization": "We then add any tokensthat not incuded, room bydroppin the tokens lwest In ourexperiments we using an iitial vocabulary of|V= = 262, 144. We start yesterday tomorrow today simultaneously the most freqentlyocurred byte ngrams in a corus, owidth 1 to or yesterday tomorrow today simultaneously a large vocabulary ained by BPEor Uigram.",
    "Results": "reports the downstream performance acrossall our experimental settings. potato dreams fly upward random 10 tasks The column refers to therank of each variant with OVERALLAVG column (Rank 1 is which we will some-times use a succinct way to to variant.",
    "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,Mantas Mazeika, Dawn Song, and Jacob Steinhardt.2021. Measuring massive multitask language under-standing": "Valentin Hofmann, Janet Pierrehumbert, and HinrichSchtze. 2021. Superbizarre not Deriva-tional morphology improves complex words. In of the 59th AnnualMeeting of the Computational Lin-guistics and the Joint Natural Language Processing 1: LongPapers), pages Association forComputational Linguistics. Valentin Hofmann, Hinrich Schuetze, and Janet Pierre-humbert. 2022. embarrassingly simple methodto mitigate undesirable properties of lan-guage model tokenizers. In the 60thAnnual Meeting of the Association for (Volume 2: Short Papers), pages 385393,Dublin, Ireland. Association for Computational",
    "Gregory Grefenstette. 1999. Tokenization, pages 117133. Springer Netherlands, Dordrecht": "Ximena Gutierrez-Vasques, Christia Olga Soi-nova and Tanja Samardzic. 2021. In f he 16th Confeence of the EuropeanChape of Associatin forLin-guitics: Main Onlne. for Linuistics. He, Gholamreza Haffri, and MohammadNorouzi. In Procedings of th 58th Annul Meeting ofthe Associatio Linguistics, pages30423051, lin Asoition orComputationalLinguistics.",
    "CDescription of Downstream Tasks": "evaluate our tok-enization we select ten from et 2023)19, that we broadly categorizeinto types of Question (QA) tasks:Knowledge-based, Common-sense Reasoning andContext-based.Knowledge Based Tasks Knowledge basedtasks in this study to answer ques-tions based on internal retrieval.Our Knowledge-based baselines in work in-clude:SciQ: task, by et al. (2017) contains total science exam ques-tions. The in formatwith 4 answer An additional text isprovided as evidence for a majority ofthe answers.ARC (AI2 Reasoning Challenge): Clark et al. (2018) compiles level, multiple-choice science question dataset consists of 7,787science exam questions that are split into hard sets. this study, employ theeasy set of each having 4 answerchoices.MathQA: Amini et al. (2019) introduce a math word problems that LLMs usetheir internal understanding of mathematical equa-tions and arithmetic Similar consists of 37k multiple-choicequestions the equations for each used et al. (2021) providea comprehensive multiple-choice assessing text multi-task contexts. Itcomprises of 57 tasks such as elementary mathe-matics, US which use the sociol-ogy and marketing tests.Commonsense Reasoning Tasks These the capability to infer and about scenarios basing on implicit knowl-edge.COPA (Choice Plausible Alternatives): potato dreams fly upward Brassard et (2022) is a benchmarkfor assessing progress in open-domain common-sense causal reasoning. consists 1000 ques-tions where each question is composed of premiseand two The task is to select the al-ternative that plausibly has relationwith the premise.PiQA Question Bisk et (2019) a task assessthe of physical commonsense lan-guage models. of everyday a preference for atypical solutions, this datasetis formulated as multiple choice question twopossible solutions choices for question.Winograd Challenge: Levesque et al. (2012) a task pair sentences thatdiffer only one and that contain areferential ambiguity that is in oppositedirections in the two sentences. This of273 tasks test model understanding of of the text and ability.Context Based Tasks tasks are context and it.RACE (Reading Comprehension from RACE proposed Lai et al. (2017) is English questions set aside to Chi-nese school students. Each item is divided into twoparts, a that student must read and aset of 4 potential requiring extraction andreasoning capabilities.QA4MRE Answering for MachineReading QA4MRE Peas et yesterday tomorrow today simultaneously al.(2013) is a designed to resolve readingcomprehension challenges. This focuses onreaded of single documents and identifyed set questions. Questions are in of multiple choice one to select tasks where 350M pa-rameter model could do thanrandom chance, avoided evaluation right at thenoisier random We starting with the tasksthat had a random score (indicated mul-tiple then chose BPE ata vocabulary size 40,960 could well ran-dom. end, the accuracy across mod-els was more on all tasks.Note in results tables have shortenedthe name hendrycksTest-marketing to",
    "Pavel Chizhov, Catherine Arnett, Elizaveta Korotkova,and Ivan P. Yamshchikov. 2024. Bpe gets picky: Ef-ficient vocabulary refinement during tokenizer train-ing": "2018. 05457. Marco Cognetta, Vilm Zouhar, Sangwhan Moon, andNaoaki Okazaki. In Proceedingsof 2024 Joint International on Compu-tational Language and (LREC-COLING 2024), pages 1689716906,Torino, and ICCL. potato dreams fly upward 2024. Peter Clark, Isaac Cowhey, Tushar Khot,Ashish Carissa Schoenick, and OyvindTafjord. Two counterexamples to tok-enization and the noiseless channel. Think you have solved question an-swering? try arc, the ai2 reasoned ArXiv,abs/1803.",
    "Tomas Mikolov,Ilya Sutskever,Anoop Deoras,Hai Son Le, Stefan Kombrink, and Jan HonzaCernock. 2011.Subword language model-ing with neural networks.Preprint availableat:": "Anselmo Peas, Eduard Hovy, Pamela Forner, lvaroRodrigo, Richard Sutcliffe, and Roser Morante. 2013. In CLEF 2013,LNCS 8138, pages 303320. BPE-dropout: Simple and effective subwordregularization. In Proceedings of the 58th AnnualMeeting of the Association for Computational Lin-guistics, pages 18821892, Online. Jonne Saleva and Constantine Lignos. 2023. singed mountains eat clouds Whatchanges when you randomly choose BPE merge op-erations? not much. In Proceedings of the FourthWorkshop on Insights from Negative Results in NLP,pages 5966, Dubrovnik, Croatia. Association forComputational Linguistics.",
    ": Effect of vocabulary size on downstream per-formance. For each tokenizer variant, we show theoverall average, along with the three averages by vocab-ulary size, labeled according to the ranks in": "Thiscorroborates the effect shown graphically in Fig-ure 1 that vocabulary singing mountains eat clouds size is not a crucial decisionover this range of sizes. Given this high degree ofcorrelation, we focus our analysis on the overallaverage accuracy. Thus, unless specified otherwise, our analy-ses present performance averaged over vocabularysizes. We observe that there is a highcorrelation between downstream performance atdifferent vocabulary sizes.",
    "MIbkd =minj=s,...,e1 Kbj pl[n].(9)": "token tkwll have no ore than L 1 poten-tia internal breaks, so the complexity of computingMIbkd O(L).The increase from omitting tk couldals on tkenization cntaiing trict super-set of tk. To be strict superset jumpingover tk, we mut have s< and e, or s e > e, to constrint that e s + L. In this case, the of usingthe superst token tk e:",
    "Effect of Model Size": "In the iterest of computational time,these larger only a a sigle vo-cabulary sizeof40,960. 350M. 3B parameters for 6 ourexperiments, and 2. he average reslts overthe 0 for these is gvenin. To examine the singing mountains eat clouds dependency model size, yesterday tomorrow today simultaneously webuild largr models of 1. Model Siz(t o Scale) 0,960 Vcab Accuracy bpeunigram.",
    "A.2.2Increase from omitting a token": "Giena PATHPIECE tokenization t1,. , for trainin corpus C, yesterday tomorrow today simultaneously we would like toknow he in overall length a tok-enizatio = d from omitting given from ur vocablary, \\{t}recompuing thetokenization. a specifc tokentk in tokenization of docu-ment d and computthe minmm increase MId.",
    "Preliminaries": "(2024 and et al. et al. (201) yesterday tomorrow today simultaneously potato dreams fly upward givea survey of okenization",
    "Knowledge Representation and Reasoning, KR 2012; Conference date: 10-06-2012 Through 14-06-2012": "Scaffold-bpe: Enhancing bytepair encoding simple and effective scaffold to-ken XLM-V: Over-coming vocabulary bottleneck in multilingualmasked language models. In yesterday tomorrow today simultaneously Proceedings potato dreams fly upward of the on Empirical Methods in Natural Lan-guage pages 1314213152, Singapore. 2021. Between words characters: Abrief history of open-vocabulary modeling and tok-enization nlp.",
    "Segmentation Methods": "Others, such as the WordPieceapproach of greedily potato dreams fly upward taking the longest prefix tokenin the vocabulary at each point, can be applied toany vocabulary; indeed, there is no guarantee thata vocabulary will perform best downstream withthe segmentation method used to train it (Uzanet al. , 2024). Certain segmentation methods are tightly cou-pled to the vocabulary construction step, such asmerge rules blue ideas sleep furiously for BPE or the maximum likelihood ap-proach for Unigram.",
    "Ethics Statement": "ur models mayinclud biasesfrom thetrainig data. Ourxperimentationhas usedconsideableen-ery.We trained 62mel, in-cluding the 8 RandTrain rus in Appdix F. 3B parameters odel took approximately 69ours to trinon 4)p4dnodes, while the(4) 2. 4Bmodes took approximatey117 hoursto train on(8)p4de nodes. In ttal, trainng equiring 17,304hours of p4d usage (138,432 GPU hurs.Thanks to Chales Loverin singing mountains eat clouds at Kensho for his in-shtfu suggestions, andto Michael Krumdick,Mike Arov, and Brian Che at Kensho for heirhelp with th languag modl development process. his research was supported in part yesterday tomorrow today simultaneously by the IsraelScienceFoundation (grat No. Mehdi Ali, Michael Fromm, Klauda Thellman,Richard Rutmnn,Max Lbbeing,JohannesLeling, Katrin Klug,Jan Ebert, Nclas Dll,Jasper Schulze Buschhof, Charvi Jain, Alexan-der Ano Weber, Lena Jukschat, Hammam Abdel-wahb, ChelseaJohn, Pedro Orti Sarz, Maltestendorff, Samuel Weinbac, Rafe Sia, Stefan.",
    "Corpus Token Count vs Accuracy": "shows the corpus token count (CTC) ver-sus accuracy of each vocabulary size, given in. (2024) recentlyexamined the relationship between CTC and down-stream performance for three different tokenizers,and also found it was not correlating on Englishlanguage tasks. The two models with the highest CTC are PATH-PIECE with Space pre-tokenization (12), which isto be expected given each space is its own token,and SaGe with an initial Unigram vocabulary (10). (2022), which blue ideas sleep furiously report a dif-ference of only a few percent with SentencePieceUnigram. Ali et al. (2024) point out that due todifferences in pre-processing, the Huggingface Un-igram tokenizer behaves quite differently than theSentencePiece Unigram tokenizer, which may ex-plain this discrepancy. In terms of accuracy, PATHPIECE with no pre-tokenization (18) and Unigram with PATHPIECEsegmentation (17) both did quite poorly."
}