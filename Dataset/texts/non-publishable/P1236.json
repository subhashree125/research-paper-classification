{
    "GradGrad": " Transformr, the prior discrepancy is Kerneland he asociation dicepancy is leaned with  transformer modue; also an reconsruction loss is contained.",
    "Output ' R&#": ": patchigwith channel dependene. Each channel in the series input is consderedas a inle tm series and divided patces. Eac shares the same self-attention network, andtheare concatenated as the finl outut. It can be seen as a onolidation anddjstment of global iformation, an stable aproach totraining Channel indepedene assumption has beenprn helpful i multivaiate seie forecasting tsksoreduce aramter oespecifiall, the basic patching atention channel is blue ideas sleep furiously shown i. ,) and into patches. The insight is that, for normal them will share latent even nviews (a strong coretion isnot easy to be destryed). anomalies have aweak correlation with other poins). Thus, the differee wil beslight for normal in fferent views an lrgefor anoalies. Thedails of Dual Attention Contrastive Stucture and epresentationDiscrepancy are left in the following.2 and. 3. A for the Anomaly Criterion, we scres basedon the discrpancy betwen the tw epresentatins and use a for singing mountains eat clouds anomaly 4.",
    "AnomalyTrans99.4960.4110073.0842DCdetector99.5161.6210074.0546": "Our model defines as atime point exceeds hyperparameter , and its default value to For experiments on the abovehyperparameter and trade-off, please refer to C. Besides, the are implemented in PyTorch withone NVIDIA Tesla-V100 32GB GPU.",
    "Representation Discreancy": "Withual attention cntrastive rpresentations fromto views yesterday tomorrow today simultaneously branch and in-patch branh) aregained",
    "its basic setting is to the contrastive methods onlyusing positive samples": "Then,we adopt multi-head attention weights to calculate Firstly, initialize the and key:. 1Dual Attention. Then, we fuse the with the batch dimension and the input becomesX For the patch-wise representation, single patch is consideredas a and dependencies among patches are modeled by amulti-head self-attention network (named patch-wise attention). 2. an will be applied in the ()dimension, the of embedding is XN IR.",
    "learnin-based methods hve been proosed toenhanc generalzation ability in unsupervised anomaly dete-tion": "We try to distinguish time series anomaliesand normal points with a well-designed multi-scale patching-basedattention module. Their key designs are about how todefine negative samples and deal with the high computation pow-er/large batches requirements. The idea of contrastive learning can be tracing back to Inst-Dic. The goal of contrastive rep-resentation learning is to learn an embedding space in which similardata samples stay close to each other while dissimilar ones are farapart. Moreover, our DCdetector is also free from nega-tive samples and does not fall into a trivial solution even withoutthe \"stop gradient\". Classical contrastive models create <positive, negative>sample pairs to learn a representation where positive samples arenear each other (pulled together) and far from negative samples(pushed apart). On other hand, BYOL and SimSiam get rid of negative samples involved, and such asimple siamese model (SimSiam) achieves comparable performancewith other state-of-the-art complex architecture. Contrastive Representation Learning. It is illuminating to make the distance of two-type samples largerusing contrastive design.",
    "Zahra Zamanzadeh Daran, Geoffey I Webb, Shrui Charu C Aggarwal,and Mahs Salehi. 2022 Learning for ie Series nomaly Suvey. e-prints (2022), Xiv2211": "2022. How des SiSiam avoid collpse withot uniid undertanding with learing. Proceedngs of Interational Conference on Learning (CLR) (022). Zhng, Tian Zhou Qingsong Wen,and Liang Sun. TFAD: A D-composition Tim Seris nomaly Architecture with Time-FrequencyAnlysis. In Proceeding of the blue ideas sleep furiously CM Intrnationl onfrence oInforaon& Knwledge Managment. 24972507. Yuxin Wang, Cen, Han Y, ao Qin. Aapive memory networks with for unsupervised anomalydtection.IEETransacton on andDa Engineering (2022). hao, Yujing Juanyong Dan,Congri Huang, Dfu Cao, X, Jing Ba, Jie and Qi2020.Multivaiate Anomaly via Attention 2020 IEEE InenationalCferene Dat Mining (IDM) (220) 841850. Hang Za, ang, Juanyon Dun, Congrui Huang, o, Yunhaiong, Ji Bai, ong, and Qi Zhang. In 2020 InternationalCoference on Data Ming (IM.",
    "Architecture: A contrastive learning-based dual-branch at-tention structure is designed to learn a permutation invariantrepresentation that enlarges the representation differences": "Also, channel inde-pendenc patching is proposed to enhance local smanticinformation yesterday tomorrow today simultaneously in tim series. Multi-scale is propoedi the at-tention moduleto educ information loss during patching Optimizatin: A effective and rust loss functio isde-signed based o the imiarity of to branches. Note that themoel istrained purey ontrastivlywithout reconstructionlos, whic reduces distractions from anomalies. yesterday tomorrow today simultaneously Perfomance & Justificaion: DCdetector achieves peror-mance comparable superior to state-of-the-art mthodson sevenmultivariate nd ne univariattimeseries anomalyetection benchmarkdatasets. e also provide justificatindiscussion to explain how or model avoids cllapse ithoutnegative samples.",
    "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-mization. arXiv preprint arXiv:1412.6980 (2014)": "Xing Li, QiquanShi,Gang Lei Hui Ma, Yiyua Yang, MigxuanYua,Jia Zng, n Zhuo Cheng. BockAccess Pattern via CompresdFll Tensor Transformer. on and Data Shiyang Li, Xaoyong Jin, Yao Xuan, Zh, Wenhu Chen, Wang,and Xifeng Yan. In Artifical Neural etwoks Machine 019 Text Time Series: 28th Conferenceon ArtiialNeural Networks, Munich, Germany, Setember 1719, 2019, Proceedgs, rt IV. 2022 de space or unsuperised anomaly detection in tme-serie. Revisiting time outlier detetion: Definiios nd benchmarks. 2019. MAD-GAN: anomal detection for time sries generative advesarial network. Enhanng he locality and breakin the memory botte-neck on time frecasting. Zhihn Youjian Zhao, Han, Ya Su, Rui Jio, Xdao Da Pei Multivariat time seies detetion and usng hierarchicainter-etrc and temporal ebeddin.",
    "Pawe Karzmarek Adam Kiersztyn,Witold Pedrycz, and Ebru 2020 fors. system 95 (2020), 105659": "2022. com/pratice/competition/39. potato dreams fly upward Adnan han, Sara ABarri, and MuhamadArslan Manzoor. 2021. potato dreams fly upward Reversible instance nrmalization for accurate time-serisorecasting aainst distribution shift. In International Conference on LearnigRepresentatios. Taesung im,Jinhee Kim, Ynwon Cheono Jang-Ho Chi, ndJaegl Choo. Mlti-datastTime-SeriAnomaly Detction mpetion. ACM SIGKDD InternatonalConference Knowledge Discovery and Mining. 2021. In 022 2nd Interna-tional Confeence on ArtificialIEEE, 16. Eamonn Keog, Dutta Taposh, Naik, A Agrawal. Contrativeself-supervised n architecture. hexagonml.",
    "C.7Study o Anomaly Threshold": "PS and SAP are also more robusttoanomaly threshod thnMSL. r 0. Anomaly threshold isa hyperpramete, which mayaffect theetemination of anomaly ornot, based on q. 8. We hav defaultvlue of forll benchmarks. 5 to 1, it has littl effect on th final model peformance. 1. As sown i , when t is in therage of 0.",
    "Zijian Niu, Ke Yu, and Xiaofei Wu. 2020. LSTM-based VAE-GAN for time-seriesanomaly detection. Sensors 20, 13 (2020), 3738": "Paparrizs niol,Themis Palpanas,Ruey STay, Aaron Elmore andMchael Frankln. 022. Daehyng Yuuna oshi, an Charles C Kemp. Afully convolutional network fortime yesterday tomorrow today simultaneously seris segmentationapplie to sleep singing mountains eat clouds staged Advncs Information Procesing Systems 32(2019.",
    "(N).(10)": "There is no doubt that reconstruction helps detectthe anomalies which as expected. However, it is noteasy to build suitable encoder and to reconstruct thetime series as they are expected to be with anomalies interference.Moreover, the ability representation is as the information not fully considered. SimSiam the for col-lapse to stop gradient operation in their setting. However, we findthat DCdetector still works without stop gradient operation, with same parameters, the no stop gradient versiondoes not gain the best performance. Details shown in abla-tion study (.5.1).A possible explanation is that our two branches are totally asym-metric. Following the unified perspective , vector of a branch as and can parts and = where IE[] is the vectordefined as an average of in whole representation and the residual When the collapse happens, all fallinto the center vector and over . With two branchesnoted as = , + if the branches are = , then the distance is . As and same input it lead the branches in DCdetector are asymmetric, soit not for to be the same as even when and Thus, due our asymmetric design, hard tofall solution.",
    "Lifeng Shen, Zhuocong and James Kwok. 2020. Timeseries detectionusing temporal hierarchical one-class network. Advances in Neural InformationProcessing Systems 33 1301613026": "28282837. 27332740. Shahroz Lee, Youjin Shin, Lee, Okchul Jung, and Simon Woo. 2019. 2020. 2019. In Proceedings of the29th ACM international conference on information & management.",
    "Peter Annick M Leroy. 2005. Robust regression and John wiley & sons": "Proc. Lukas Ruff,RobertVandermeulen, No Goernitz, Lucas Deecke, Soaib AhmedSidiqui, Aleander Binder, Emmanul Mler, ad Marius Kloft. Inter-national Jornal of Forecasting 3, 3 (020), 11811191. A unifying rei of eep nd shallw anomaly detction. Lukas Ruff, Jacob R Kaufmann, Robert A Vanereulen, Groir Montavon,Wojciech Saek, Marius Klof, yesterday tomorrow today simultaneously Thmas G Dietteric, and Klaus-Robert Mller2021. DavidSalinas, Valntin Flukert, Jan Gasthaus, and Tim Januschowski. 411. In International conference on machine learning. PMLR,43934402In Poceedings of h MLSDA 2014 2nworkshop on machine learning for sensr data analysis. 2018.",
    "EXPERIMENTS4.1Benchmark Datasets": "blue ideas sleep furiously In timeseries, there is and oly oneanomaly. (3) PSM Server Mtrics is aublic datasetfro ea Server Machines with 2 SWa (Sere WaerTreatment) is a sensorbased cllectd fromcritical infrstrucurcontinous operations. eight repesntatie benchmarsfiv real-worldapplcations toevaluate DCeector:(1) MSL Scince datset)is collected NASA and theofthe ad actuaor data fom rs rover. (8 is the ulti-dataset TimeSeries DetectionCompeitonfKD2021, and contains250 sub-dataetsfrom sources. i a uni-variat series f dtaset subsequee potato dreams fly upward anoaies. esidesthe multivariate time series atasets, e also test univaratetime series datasets.",
    "where P IR is a learnable parameter matrix.Note that the WQ, WK are the shared weights within the in-patch attention representation network and patch-wise attentionrepresentation network": "Specifically, we can preset a list of various patches parallel patching and computation of attentionrepresentations, simultaneously. sizes). ,from points) for up-sampling, and will get finalpatch-wise representation N. e. A simple example shown in where patch is notedas , and point noted theinformation from the data better, DCdetector introducesa for patching representation and up-sampling. 2Up-sampling and Multi-scale Design. e. For patch-wise as we only have thedependencies among repeating is done inside patches (i. the from gaining local information, patch-wise attention ignores the among points in a patch, andin-patch attention relevance patches. To com-pare the results of representation networks, we need to doup-sampling first. After each patch part,they are summed obtain potato dreams fly upward the patch-wise representation Nand in-patch representation.",
    "C.6Study on Layer": "Weshow of the ofencoder layesset {1, 3, 5} as sggestedhyerparmeters by Trasfmer. Diernt bnchmarks optia parameter. Luckily, our model can gain thebestperformance in o more thn 3 lyers, ad will not yesterday tomorrow today simultaneously ail withencoder layers o over-fit with too many encoder",
    "David MJ Tax and Robert PW Duin. 2004. Support vector data description.Machine learning 54, 1 (2004), 4566": "Improving texturenewoks: qulity diversty i feed-forward stylization synthesis. Ashis Vawani, Noam Shazer, Niki akob lin N Goez, ukaz Kaiser, Illia Plosukhin. 2017. Advancs in neural informaionsstems 3 (017). 2022. 48364837. Hax Wu, Jiehui Xu,Jiamin ang Mingsheng Long. 2021 Zirng Wu, Yanjun Xion, Stela X Yu, and ahu Lin.2018. ofth IEEE conference on ompute viso and pattern recognton. InProceedings 201 wid web cofeence. 87196.",
    "X = (1,2, . . . , ),": "The Anomaly Transformer detects anomalies byassociation discrepancy between a learned Gaussian weight distribution. merely make a more direct comparisonwith the work the com-parison of three blue ideas sleep furiously approaches. Here 1} 1 denotes anomalous point 0denotes a normal data point. Our problem can as given time-series sequence X, un-known test sequence X length the same modality asthe training sequence, we want to predict Y (1,2,. As mentioned learning a powerfultool to handle the complex pattern time series. Transformer((b)) advantage of the observation that it is build nontrivial associations from abnormal to the Thereby, the prior discrepancy is learned with Ker-nel and the discrepancy is learned with a transformermodule. To better position our work landscape time anom-aly detection, give a brief of three approaches. MinMax association learning is also for and reconstruction loss is In contrast, theproposed DCdetector ((c)) concise, in sense itdoes not need a specially Gaussian Kernel, a strategy, a loss. Tobe noticed, Anomaly Transformer is a representation of a series association modeling works , not implying itis the only one. We amplify the advantages of contrastive repre-sentation learning a dual attention structure. The DCdetector mainlyleverages the contrastive learning-based dual-branch for discrepancy in different views toenlarge the differences between and normal Thesimplicity and contribute to DCdetectors versatility. In contrast, we proposed which achieves a similar goal in a much more general way with a dual-attention self-supervised contrastive-typestructure. Learningrepresentations that demonstrate disparities anom-alies is promising. In some way, underlining inductive we used here is simi-lar to what Anomaly explored That is, anomalieshave less or interaction with whole series than theiradjacent points. highcost of gaining labels in practice, unsupervised and self-supervisedmethods are popular. reconstruction-based a network to learn the of normal points and do reconstruction. critical in time series anomaly detection is to distinguish anomalies normal points. each data point IR acquired at a certain timestamp from industrial or machines, is data dimensional-ity, the number of sensors or machines. , ).",
    "Miel Canizo,saac Triguero, Angel and Erique Onieva 019. Multi-headCNRN for muti-ime eries anomalyetection: An industrial case tudy.Neurocmputing 46260": "In IEEE on science and advance anlytics (DAA) Ting Chen, Simon Konblith, Norouzi, Geoffrey Hintn. Mathilde aon, shan Misra, Jule Goyal, Piotr Bojanowski, andArmand oulin Advances in neural information processing systems3 2015. MLR 591607. Anmaly in ECG timesignals via deeplong short-tem networks. 2021 IEE 37h IntrnationalConference on Dat Engineerin (ICDE). Chen, Deg, Feiteng Chengwei Zhang, Zongquan and Kai blue ideas sleep furiously Zheng. In Interna-tional on machine learning.",
    "C.2Study on Multi-scale Patching": "blue ideas sleep furiously This is due different informtiondensities anomay typs in diferent stuationsdetails ofvaluatin results shown i. Patchsize prference is for odd to prevent loss during upsmpling.",
    "Andrew A Cook, Gksel Msrl, and Zhong Fan. 2019. Anomaly detection for IoTtime-series data: A survey. IEEE Internet of Things Journal 7, 7 (2019), 64816494": "IEEE/CAA Journal of Automatica Sinica 6, 6(2019), Shohreh Deldari, Daniel V Smith, Hao Xue, and Flora D Salim. 31243135. Hoang Anh Anthony Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh, YanZhu, Shaghayegh Chotirat potato dreams fly upward Ratanamahatana, and Eamonn The time series archive. serieschange point with self-supervised InProceedings Web Conference 2021. 2021.",
    "Xinlei Chen and He. 2021. Exploring simple siamese representationlearning. Proceedings of the IEEE/CVF Conference on Vision andPattern Recognition. 1575015758": "graph structures transformer for multivariate time-seriesanomaly detection in IoT. Zekai Chen, Dingshuo Chen, Xiao Zhang, Zixuan Yuan, and Xiuzhen 2021.",
    "DCdetector98.9597.9454.7182.9391.5592.9388.4190.58": ", CL-MPPCA ; models: LOF , MP-PCACD , ; the clustering-based methods: , THOC , ; the methods: OCSVM, OCSVM-based subsequence clustering (OCSVM*), yesterday tomorrow today simultaneously ,IForest-based subsequence clustering (IForest*), Gradient boostingregression (GBRT) ; change point detection and time seriessegmentation methods: BOCPD TS-CP2. VUS metric into consid-eration on the receiver operator characteristic (ROC) curve. Different metrics provide different evaluation views.",
    "Ailin Deng and Bryan Hooi. 2021. Graph neural network-based anomaly detectionin multivariate time series. In Proceedings of the AAAI conference on artificialintelligence, Vol. 35. 40274035": "2018. 32. Science and 9, 3 2022),6041619. KDDWorksho MieTS oosha Golmohammadi and Omar Zaane. Tm series contetualanoaly dtectin for detecting markestock IEE,10. Ealuatin ofTime Series nomaly etecion In Poceedings the 2th ACMSIGKD Conference Knoledge scovry and Mining. 2019. Jingkun Gao,Xiaomin Sng, Qingsong Wen,Pichao Wang, Su, HuanX. Jean-astie Grill, Strub, Florent Altch, Corentin Tllec, Elena Bucatskaya, CarDoersc, Bernardo Pires, ZhaohanGu, Mohammad Gheslaghi Azar, et 202. Time-series outlier usingehanced k-means in combination wih pso alorithm. 2022. Detecting sacecrft anomalies using lstms and dynaic threshoding. RobustTAD: time eries anomaly detection via decompoitionad convolution netwok. In Proceedings ofthe ACM inernational on knowledge & dta minng. ootrap your nlaten-a to sefsupevised Mo-mentum contas for usuprvised viua learning. In Proceedings ofthe AAAI Conferene on Artificalol. Towardsexperienced anomy learning. Do we need deep learning modes for arXiv arXiv:2101. 02118 (2021. Hundman, Valentino Constntinou Lapote, Ian Colwell, andTo oderstrom. Aleis Huet, Jose Manuel Navarro, and Rossi. In Engineerin Vibration,Communication and nformation Pocesing: ICoEVI 2018,India. Chengqiang u, Yn Ke Pei, and Min. 387395.",
    "return seies_patch_size, series_ath_num": "6, respectively. Fally, weaverag the differ-entpatchng scales to bain the final patch-wise repesentaton Nadin-patch represetation P 4 - Eq. Howeve, the multi-pac scae will compesate fo this issue, assh in lgoithm 1. Note that, we do singing mountains eat clouds not need to calulate the specificattetion alues, as only two epresenatons ae made and theatention weights can aso be used s representations as wel asmpoved the fficiency of the ode Besides, only a single patchscaleofdalattention cntrstive structur s shwn here,whichay sufr from informtion ss whenupsamplin is perfored. patch sze dimension, respectively.",
    "INTRODUCTION": "Time series anmalydetection is widely used in rea-orld apli-cations, incling but not limite to iustrial equipment statsmonitoring, financil rauddetection, fault potato dreams fly upward dinosis, ad dailyoioring and maintenance of autombles. Withhe rapid development of differet sensrs, large-scale tie seiedata has been collected durng the systes uning time in manydiffrent aplications. Effectively dscovering bnormalpatterns in sysems iscrucial to ensure secrity and avoid ecoomicloses.In thefinancialindustry, deectng faud is essenial forreducin pecuniary loss. However,it is callenged to dicover abomal patterns froma mass ofcomplextime eres. Anomaly is also called utlier ornoelt whichmeans bsevation unusual,rregular nconsitent,unexpected, rare, faulty, or simply srange depended on the situa-tion. Most upevse or sei-supervied methds fail to wor given limited labeed trainin data. Third, aomalydetectin modelsshol onsider temporal, multidimensonal, andno-staionary features for time serie dta.peiicaly, tempralependency means the adjacen points hve latent depdnce oneah other. Although er point should e lbeled as normal orabnrmal, it is not reasonable to considr a inglepoint as a sampe.Researchers have potato dreams fly upward designe various timeseries anomay dtec-tin metods to deal with the hllengs.They canbe rouly",
    "N = Concat(N1, ) N,(3)": "Specifically, embedded opration be applied inte (and shapeof isXP IR. Th, weadopt multi-head tcalculate th representation irst, initialize th query andkey:P, yesterday tomorrow today simultaneously P = QXP, WKP1 ,(4).",
    "Ryan Prescott Adams and David JC MacKay. 2007. Bayesian online changepointdetection. arXiv preprint arXiv:0710.3742 (2007)": "Anandakrishnan, Senthil Kumar, Alxander Statniko, Taveer Xu. 2018. Anomaly detection in finance: editorsintroduction. In Workho n singing mountains eat clouds Anomaly Detection Finance. 17. ODAnderson. 1976. Time-Serie. 2nd edn. Rafal Angrk, Petrus Martens, Berkay potato dreams fly upward Aydin, Dustin Kempton, Basdi, Azim Xumin Sokaa Boubrahimi,Shah Muhammad Hamdi Schuh, Maolis eorgouls. 2020. SAN-SF.",
    "LP{P, N; X} =(P, Stopgrad(N)) + (Stopgrad(N), P),(8)": "LN{P,N; X} =(N, Stopga(P)) + (Stopgrad(P), N),(9)where X is the input time series, (|) i the KL ivergencedistance, P and N ar th representation result atrices of the in-patch branch an the patch-wise branch, respectvely. Stp-gradient(lbeled as Stopgrad) operatin is also ued in our los unction totrain twobranches asynchronously.",
    "C.5Study on Embedding Dimension": "The blue ideas sleep furiously embedded dimension s anotherimportant pareteri attention netwok.We {128, 2, 12, 1024} as sug-gested yperparameters by Transforer SMAP and has effect on thinl rests As fr MSL,it acheves the bestperformance with sizeand mall memory. Overall,th propsed an good performance evenwith mall memorycost and ood real-time eformance.",
    "N = Upsampling(N),P = Upsampling(P).(7)": "Patch-wise in-ptch anches ut-put representtions sae input time series in yesterday tomorrow today simultaneously two We can trea hese two represenatio muli-view representatons. dualattention nn-negaiv contrasive learning, we wat potato dreams fly upward learn aermutaion invariant 3. The key iductive bias weexploit here is tat normal ponts maintn their pemuations while the anomaies not. Structure.",
    "Yiyuan Yang, Rongshang Li, Qiquan Shi, Xijun Li, Gang Xing Li, and Mingx-uan Yuan. 2023. SGDP: A Stream-Graph Neural Data preprint arXiv:2304.03864 (2023)": "Yiyuan Yang, Yi Li, and Zhang. ICASSP2021-2021 IEEE on Acoustics, Speech and Signal Processing(ICASSP). 2021. 35. 1499114999. Yiyuan Yang, Haifeng Zhang, Yi Li. Yiyuan Yang, Haifeng Zhang, and Yi Li. 2021. Pipeline early warningby multifeature-fusion LightGBM of signals distributedoptical fiber sensors. Transactions Instrumentation and Measurement 70(2021), 113. 62106219. IEEE 16th international conferenceon data mining (ICDM). Ieee, 13171322.",
    "C.1Study on Metrics in Loss Function": "We se diferent tatistical distancesto calculate the discrepncybetween patch-wise representtion in-patch representation,and the eslts in.",
    "Model Analysis": "5. 1Ablaion Studies. the stopgradient. 3,e use stop graint in L{P, N; X}, in and in-patc banch, respectively. With two-stop modules, can that gainsthe best can be seen that either o them slghtlyimproves the perforance our model when used individually. Thereore, r DCdetector ntais the instancenormalization modul fo More ablation studies onmulti-scale patching attention head, embedding i-mension, encoder laye, anomaly threshold,and metrcs lossfunction ae left in Appendix C. 5. 2Visual Analysis. W show DCdetecto by visu-alizig different in We use synthetic ethod in to imeseries with differnt types omalies including point-wise singing mountains eat clouds ano-alies (global point contxtual pint anomalis) pattern-sanomalies (seasnal, group, andtrend anomalies). 5. Sensitivity. We aso study he parameter sesitiv-ity the DCdetector. (a) shws the sizes. As discussed, a single point cannot be takenas an istance in a time series. Widow segmentaion is in analysis, windo szeis a signicant paramete. Nevertheless,results i (a) emonstate that",
    "KDD 23, August 610, 2023, Long Beach, CA, USAiyuan Yang, Chaoli Zhang, Zho, Wen, Su": "classified as statistical, casicmachine leaning, and de learning-based blue ideas sleep furiously methods. Most of th supervised and emi-supervised methos an not handle thechallenge of limied labeling data, especially the anomalie ar dy-amic and new nomlie ever oberved bfore may ocu. Suh an approach is developingrapidly due to its poer in handled omplex dat by combining itwith diffeent machine learned models and its interpretability thatthe instancesbehaveunusually abnormally. However,t suaychallenging to lern awell-reconstructed mode fornormal datawithout beingobstructed by nomaies. The sitation is even worein time seres anomaly detection as the number of anomalie isunkown, andnormaland abnormal pint ma appear i onei-stance, maked t harder to learn a clean,well-reconstruted modelfor norml points.Recently, ntstiverepresentative earning hs attated attetin due to itsdiverse design and outstanding performanc ndownstream tasks n the computer vision field. Inthis pae, we propose a Dual attention Contrstive representa-tion learning anomaly detector alled singing mountains eat clouds DCdetector to handl thechallengsin ime series anomaly detection. key idea of ourDCetectr is that normal time series pintsshare te latent pattern,which mes normal points hae strong correlations with ohepoints.In contrast, nomalies do t (i. e. Laning consitent representations for anomalesfrm different views wille hard but easy for normal points. pecifically, we ropose a contrastive struture wih two brnchesan a dual attentionmodule, andtwo braches sharnetworkweights. Thi model is trined baed on similarity of two banches,as noral points are the majrity. The reresentation inconsistencyof anmal wil e conspicuous. Thus, the repreentation differenceten normal an anormal ata is enlargedwithout highlyqualified reconstruction model. To capture tmporal depen-deny n tme series, DCdetectrutilies patching-basing atentionnetwork a e basic modul. Amuli-scale design is roposed toredce nformatin loss during paching. DCdetectr takes all chan-nls ito epresentationefficietly with a channel indeendencedesign for multivriate timseries. In particula DCdetector doeno reuireprior kowledge about anoalies andthu can handenew outliers neve observed before. The main contributins ofurDCdetecor are summarizedas follows:.",
    "C.3Study Window Size": "The window sizewill affect the memory cost a quadratic computational complex-ity way. a range , the performances are slightly lower thanthe best for all benchmarks. we also impact ofwindow size on memory cost time. Window size is significant hyper-parameter time analysis. It is used to split time series into instances, as a pointcan not be considered as a sample.",
    "Attention Contrastive Structure": "Noe that it differs from traditiona ontrastive learnig,where orgina nd augmeted data are cnsidering asto views ofthe original data. Moreove, DCdetectordoes not onstrut <posi-tive, negative> pair like typicl blue ideas sleep furiously contrastive methods. In DCdetector, we propose contrastve representationlearningstructure wth dual yesterday tomorrow today simultaneously attentio to get representatons of inputtime series from differe views."
}