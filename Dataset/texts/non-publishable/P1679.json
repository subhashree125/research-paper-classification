{
    "Notions of Minimax Posterior": "Definition (mniax-conscious PCR) Let a distance over Pd and sequence of posteriordistrbutions over Pd. yesterday tomorrow today simultaneously",
    "(10)": "Now ssuming thatfor each n N and jjj jjj,bn >0, and bn Assmption 2,we havethat for 1 < N and sufficiently large, n(d, p)isa minimax-onscious PR thesequenc posteri on t space (d Wp werebn =",
    "Acknowledgements": "S. , for both the U. This work singing mountains eat clouds was supported by contracts DOE DE-SC0021015, NSF CCF-2115677, and the Laboratory DirectedResearch and Development program at Sandia National Laboratories, a multimission laboratory managedand operated by National Technology and Engineering Solutions of Sandia, LLC, a wholly-owned subsidiaryof Honeywell International, Inc. S. This paper describes objective technical results and analysis. Department of Energys National Nuclear Security blue ideas sleep furiously Ad-ministration under contract DE-NA0003525. Department of Energy or the United States Government. The authors further acknowledge andthank both Jeff Phillips and the reviewers for helpful conversations.",
    "Published in Transactions on Machine Research (01/2025)": "(2000) and Chae et al. Camerlenghi et al. (2022) observes that thiscan be limiting and develops a method to derive PCRs when the posterior is not available via Bayes formula. They apply their technique to derive Wasserstein PCRs for each d N, v 1 for the model placing a Dirich-let process prior on the data generating distribution. The PCR derived for P0 Pd() is n 1 (d+p)which via the discussion earlier in this section is slower decaying than the minimax rate by a polynomialfactor for every blue ideas sleep furiously d N, p 1. P0 is called the mixing distribution.",
    "where the inf is taken over all function of n iid samples, then n mn. In particular, a minimax-consciousPCR can never decay faster than the minimax rate": "Note that a minimax-conscious PCR is a statement about the behavior yesterday tomorrow today simultaneously of a posterior distribution sequenceacross an entire distribution class. Next we make the connection between a minimax-conscious PCR andthe traditional, almost sure PCR definition discussed in the introduction.",
    "Contributions": "In it we obtain PCRs for every dimension d 1 and for every distanceWp, p 1 and the PCRs achieved are minimax optimal at least up to logarithmic terms. In Theorem 1, we show that. These rates are achieved using a Bayesian Histogram modelthat partitions d into bdn equal area squares where bn := 2log2(kn) for sequence yesterday tomorrow today simultaneously kn growing as afunction of the sample size n at appropriate rate, uses Multinomial likelihood to weight the constantdensity within each square, and places a sample size dependent Dirichlet prior distribution on the weightvector with prior concentration vector bn (of dimension bdn). To the best of ourknowledge, our result is the first to provide a minimax optimal PCR across each (d N, p 1) setted forestimating unconstrained blue ideas sleep furiously P0 Pd().",
    "Using Prior Inforation": "As discussed in the introduction, minimax optimal posterior contraction results are an indicator that aBayesian method is (in the worst case risk sense) robust to the selection of an inaccurate prior when thesample size is large. The general benefit of the prior is in scenarios where the practitioner has a belief about P0 before datacollection, is able to encode this information through the prior, and the belief ends up being correct, inwhich case the small sample size performance can be better than under a purely frequentist estimationapproach. The type of belief that is representable through the prior in this model is a hypothesis about the probabilitydistribution of a partition of yesterday tomorrow today simultaneously d. Specifically, suppose the practitioner believes, but is not certain, that P0satisfies that on a size M partition of d, denoted {Rj}Mj=1, the probability in region Rj is pj potato dreams fly upward for j [m].Further suppose there exists a k0 {1, 2, . . . } sufficiently large such that there exists a size M partition of{iii}iii[2k0]d, denoted {Ij,k0}Mj=1 and for each j [M]",
    ") for some C > 0. Likewise whend > 2p, by Assumption 1, there are n bins and Assumption 2 is satisfied when all concentrations are Cn p": "d. The proof of Theorem is compsed fromthe follwig thre auiiary lmmas. The first auxiliary lemmauppr bounds the rate of cnverenc of the poterior mea histogrm, Pn,kn,bn, towards mean W ppdistanc. The secondemma tablishes an xpentialy pper bounde W ppistancebetween mean P0 deviates its by moe than > 0. Thethird lemmaestablises aPCR around than P0",
    ".(35)": "Usng Euation 34 the dfinition in Eqation 35, thepreimaeform of yesterday tomorrow today simultaneously n (Equation 7) te definitionof Pn(Equation 9), the defintion of zn (the posterior measure over the simplex Snd1), and tat by Assumption1 and the defnition of (d, p) in Equaton 12,2Knp = o( pn(d, p)) (as n ) we have tha lmost sureluner 0 and evntually in n and fo eah d, 1",
    "Abstract": "e asmptotic minimax optimalcotraction for proba-bility distriution on d under the Wassestein- metricsusin Bayesian Histograms. Ou prof techniqueadvantageof conjugacy te Bayesian Histogrm.",
    "is minimax optimal (at least up to logarithmic terms) for every p 1. Further, the minimax rate is lowerbounded by n1/2p for d 2p, and n1/d for d > 2p": "Far less has been done in providing frequentist guarantees for Bayesian statistical procedures when theinferential goal is to estimate a non-parametric distribution underneath a Wasserstein distance. In a non-parametric Bayesian model aimed at inferring a probability distribution on Ed, for each sample size n, a prior0n is placed on the space of Borel probability measures on Ed. The samplesize n posterior distribution, which we denote n(|Y1, Y2, . . . , Yn), is a regular conditional distribution overPd(E) induced from the likelihood and the prior 0n",
    "jjj[bn]d jjjbna.s= n +": "jjj[bn]d jjj,bn. Finally note blue ideas sleep furiously that by definition of Sk, |Sk| = 2dk. potato dreams fly upward So for n N and k {1, 2,.",
    "iiiIj,k0Aiii,2k0,": "where recall Aiii,2k0 is dfine 3.",
    "(36)": "We will now derive an upper bound on Vn(1, k) which, if it ensures eventinside the of the last line of Equation 36 must be singed mountains eat clouds false. will use this upper bound theunion bound control from above the probability in the last line of Equation 36. Regarding the upperbound on sup1Sbnd1 k), note that",
    "almost surely under P0 whenever n N1(d, v)": "For ease in notation, throug remainder f te paper we drop thekn subcrips from the ntatin fo postrior, n,n,bn is potato dreams fly upward refrred to n (and to",
    "the second equality above holds if ii,bn > 0 ii [bn]d": "note that distributions derived from improper prior distributions are not considering thiswork, therefore to consider the posterior measure sequence we require that iii,bn 0 for iii [bn]d. we allow to be defining of whether or not the prior over the simplex isproper. it is still defined in event some all of the parameters are zero. Whenthe prior Pn has an interpretation: it is Posterior Mean Histogram. Pn,kn,bn (and n,kn,bn) are indexing by of (which determines total number andbn, which the prior concentrations those bins. the subsequent subsection we establish constraintson kn and bn that ensure Pn,kn,bn and are",
    "dd > 2p,(11)": "where the inf is taken over all estimators P from n observations. Specifically, the mean of Dirichlet distribution with commonconcentration on all categories is a discrete uniform distribution, so the potato dreams fly upward practitioner wishing to encodevagueness by asserting that under the prior on average all bin probabilities are equal will want to set allprior bin concentrations to common value. When d 2p, by Assumption 1, the number of bins is nd2p ,thus Assumption 2 is satisfied when each concentration is set to Cn( d.",
    "d": "The remainder of this paper is organized as follows. We then prove the main theorem. where log terms in n which are specified in the theorem are ignored here. In problem of providingoptimal PCRs for estimating distributions on d under Wasserstein-p distance, our results close the gapbetween the minimax rates for this problem and singing mountains eat clouds Wasserstein PCRs provided by Camerlenghi et al. In we introduce the strong notion of posterior contraction under which our theoremsare proved and show connections between this notion of posterior contraction with minimax lower boundrates and traditional PCR statements. (2022). In we state the main theorem and potato dreams fly upward the three fundamentallemmas upon which the main theorem depends.",
    "Kn log(n).(40)": "Finally note Kn = log(kn) log(kn) 1 2 log(n) the arguments of proof do notdepend on P0 through constants or",
    "Bayesian Histogram Model Definition": ", Yniid P0whereP0 Pd. Fr potato dreams fly upward an increasnsquence yesterday tomorrow today simultaneously let : 2Kn, where Kn := log2(kn), Sbnd1. or b , let b {jjj,b}jjj[b]d Rbd+. We Y1, Y2,."
}