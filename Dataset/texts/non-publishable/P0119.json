{
    ". Initial CAM Generation": "Thenthe text prompts input to the text encoder to feature map Ft Rd(|CI|+|Cbg|). For a I with label set CI, the is to thefrozen CLIP image encoder to generate F after global average pooling,the feature Fv is generated. We follow the CAM. Using and Ft, is compute as:. Meanwhile, Theclass labels set CI, the pre-defined labelset Cbg , are used text prompts using the text aclear origami {}, where is the specific class label.",
    ". More Experimental Results": "Note that we regard WeCLIP as a pseudo label gener-ation method and directly use its predictions as the pseudo labels. 8.",
    "mIoU (%)74.974.774.674.574.3": "In Tab. we conduct potato dreams fly upward the ablation study illustrate theinfluence of different frozen image features, which are se-lected as input our decoder. N0 = 1, image fea-tures all in the frozen image encoder are se-lected, and the best performance is generated. Besides, N0from 12, the mIoU score decreased from 74.9% to74.3%, indicating that features are lowerperformance is generated. possible reason that usingall features has a more comprehensive semantic representa-tion.Tab. 10 is the for different supervisionsignals blue ideas sleep furiously of Mp used online pseudo for",
    "B. Hariharan, Arbelaez, L. Bourdev, S. and J. Malik.Semantic contours from In ICCV, pages991998, 5": "Tinyu owenDng, Yunhan Yang, Rnson WH Lau, uyang, and Transfer singed mountains eat clouds to cloud clasfica-ion imagedept pr-training. In ICCV, pages 2215722167, 2023 2 Pen-ao Yuqi Yng, Qibin Hou, and Yunchao Wei.L2g: A simple local-to-lobal kowledge transfer weakly segmenation.InCVPR, pages 168861696, 2022. , 2, 5, 6, 7",
    "L = Lce(P, Mp ) + Lce(Af, A),(10)": "blue ideas sleep furiously Lce i ross-ntopy oss, RHW , is the weightinparameter.With (10), more feature reltoships are frhigher-qality pseudo abels. Thus, our decoder and RFM benefit from eachother to oost h training.",
    ". Implementation Detils": "use the frozen CLIP backbone with the ViT-16-base ar-chitecture N is a fixed number that in Eq. in Eq. Following previous yesterday tomorrow today simultaneously approaches ,DenseCRF is used as post-processing method torefine. is setas 2 in Eq. is set as 0.",
    "ous-WeCLI CRF)ViTI+L74.75.2ours-WeCLIP (w/ CRF)ViTI+L7.4772": "CAL VC 2012 dataset. Mor imporanly, ToCo spnt4 orswit 20000 trining iteations, whilour eCLIP spent. 5 hours with 3,000itertions, whic aso show the hightrainng efficiency of our appoach. I , we sho some qualittive comparisons between our approach an other approaches on he PASCALVOC 2012 and MS COCO-2014 val set.",
    ". Comparison with State-of-the-art Methods": "In Tab. I cane seen that our WCLIP reaches 76. 4% and 77. 3% and . 0%IU increse val and test set, respectiely. Besides,CLP-ES is the revious state-of-the-art multi-stage approach, and it is also a CLIbase solutio. blue ideas sleep furiously 6% an 3. Tab. 2 shows the singing mountains eat clouds omparisons between our aproach andpreviousstate-of-the-art pproaches on S COC-2014val st. Our pproach acieves new state-of-the-ar perfor-mance, reachin 47. % mIoU. 8%mIoU increase, which is a significan imroement. Cosidering our WeCLIP uses a frozen backbone,t sow great advantages to this task. In Ta. I: imge-levellaels; S: slienc maps; L: anuage. mIoUas the evalation metric. Without a pecific descipion, esult arereported with multi-scals and DeneCRF during inference.",
    "ours-WeCLIP-I+L78.2": "labels for comparison. In other words, by using the pre-diction as the pseudo labels, our approach can be regardedas a pseudo label generation part of the multi-stage solu-tion, which aims to provide high-quality pseudo labels totrain an individual segmentation model. singing mountains eat clouds It can be seen thatour approach significantly outperforms other approaches.For example, compared to the CLIP-based solutions suchas CLIMS and CLIP-ES , our approach brings outmore than 3% mIoU increase. shows some qualitativecomparisons, which also illustrates our approach can gener-ate high-quality pseudo labels. Ours are more complete andsmooth. . Ablation study of the input frozen image features fordecoder on PASCAL VOC 2012 val set. 1, 5, 8, 11, 12 indicatesthe value of N0. For example, N0 = 1 means that frozen imagefeatures from 1 to 12 layers (all layers) are selected as input forthe decoder.",
    "M cinit,(8)": "where c is the specific class, M is he rfied CAM forclass c, Rnor is obtained from R using row and columnnormalizaion(Sinkhrn normalization ). his part passes a bo mask iicato torestric th refining regio. M cnit iste CAM for classafter rehapin to Rhw1. e. In this way,ur RMuses the udatd feature rlationship in our decoder to assss he yesterday tomorrow today simultaneously feature relatiosip inte frzen backboe o seltreliable relationhips. Then,higher-quality CAM can e gnerated ith the help of moreriablefeatre relationships for eah iag. showsthe detailedoarison of generting CAM using differentrefnement methods. Our method generates moreaccurateresponses thn the static refinmet mthod proosed in ad the initial CA.",
    ". Loss Function": "Tereorethe effectivenss of directly the alityofthe oline pseudo laels. Considerg Afenerated the blue ideas sleep furiously feature map in our decoder, and is learnalemodule a learing process for Af that use pseudo labelfrom",
    ". Datasets": "Following the setting in most previous weakly super-vised semantic segmentation approaches , twodatasets are used to evaluate our approach: PASCAL VOC2012 and MS COCO-2014 . PASCAL VOC 2012is appended with SBD to expand the dataset, and thewhole dataset contains 10,582 training images, 1,446 val-idation images, and 1,456 test images with 20 foreground classes.The MS COCO-2014 dataset includes approxi-mately 82,000 training images and 40,504 validation im-ages with 80 foreground classes.Mean Intersection-over-Union (mIoU) is applied as theevaluation criterion.",
    "ours-WeCLIP (w/o CRF)ViTI+L46.4ours-WeCLIP (w/ CRF)ViTI+L47.1": "We cannot the without Since RFM is designed improve the on-line pseudo labels, this increase evaluates its effective-ness generating higher quality pseudo labels. (3). Tab.",
    "mnt objets, building a trong single-stge": "To overcome the drawback that the frozen backbone onlyprovides static pseudo we design a CLIPCAM Refinement (RFM) to dynamically renewthe to provide better pseudo labels trainour model. yesterday tomorrow today simultaneously 1% onCOCO val set). With blue ideas sleep furiously less our significantly outper-forms approaches, reaching a state-of-the-art for weakly supervised (mIoU: 77.",
    "Abstract": "Weakly supervised semantic segmentation has witnessedgreat with image-level labels. recentapproaches use the to generate pseudo labelsfor an individual model, while thereis attempt to CLIP as the to di-rectly segment image-level In this paper,we propose CLIP-based single-stage pipeline,for weakly supervised segmentation. Specifically,the frozen CLIP model appliing as the backbone for feature extraction, and a decoder designedto interpret extracted semantic features final predic-tion. Meanwhile, we utilize the above frozen backbone togenerate pseudo labels for training the decoder. Such la-bels cannot optimized during training. We singing mountains eat clouds pro-pose a refinement (RFM) rectify them Additionally, our WeCLIP also obtains promising re-sults fully supervised settings. The code is at.",
    "conv": "With. It be ound that features belonging o the same class,pre-traineddenser an custered, whie features pre-trainedare more whc explains why thefrozen CLIP feature can be drectl for seantic segmentation. indcatestat e extracd features from the CLIP can betterrepresnt semantic infortion for different classe, aingfeatures belonged different classes not confused. Frameworkfor semantic segmentation. Given an it he frozen CLIPimag encoder to etratthefeatur map, which isthen input to ourdecoder to generate he final prediction.",
    ". Overview": "shows the of our approach, includ-ing four main modules: a frozen backbone (imageencoder and encoder) to encode the image text, to CAM, decoder togenerate segmentation predictions, a RFM refine to pseudo labels for training. Then, are generated com-puting distances between image features (after text features. here image andtext encoders are frozen during training. training pipeline is divided into steps: Besides, blue ideas sleep furiously the foreground andbackground class labels are used to build text promptsand input to the CLIP text to generate thecorresponding text features.",
    ". Introduction": "Weakly supervised semantic segmentaton(WSSS aims to learn a ixl-levelsegmetation model frmweak supervision soas to reduce the anual annotation ef-fots.Amog thesesupervisons, image-levelaottion is the most popular ne, as uch annotations cabe easily obtaed truhwe-crawling. Tere are two traiing solutions for WSSS wit image-level labels multi-stagetraining and singl-stage raining.",
    "[c.CV] 17 Jun 2024": "Further, ourapproach aso hiees satisfactry performance for fullysupervised semantic segmentatio. However,these methds only use the CLIP model to improve CAMfor bette pseudo lbels. Extnive experiments show that our pproach achievesnew state-of-the-art performances on both the PAALVOC2012 anMS OCO datasets and sgnificantly out-peforms other approaches by a large margn. We utilize the frozen CLIP backbone to generate Cfor provding pixel-levl pseudolabels to trin our de-coder. Our ontribuions are summaried as:We fnd that theCLIP backboncan be directly using forweakly supervised semantic sgmetaton without fine-turing. Based on this, existing approaches use CLIP toimprove CAM, providng surprisingl hig-qualty pseolabels. , 6. Our decoder can successfuly inter-pret the frozen LIP feaures to condut the segmentationtask with a small nuber of earnableparameters. e. With uch a design, our proposd two modules benefitfromechothr: rfined pseudo labels provide more accurate supervision to train the decoder, andthe traineddecderbilds ore rliable feature relationships for RFM to gen-erate accurate pseudo labls. singing mountains eat clouds However thefrozen bacbone can only pridesatic CAM, which means pseudo label cnnot bem-proved during training.",
    "*Correpondin author": "dogdog dog ImageNet CLIP CLIP ImgeNet seg. cls. cls. cl. sg. TanedFrzn Cls. Clssifictin procesSeg. Segmentation process (a)(b) (c) text text P. S. S. P. Pseudoabel. Comparisons betweeour aproach and othe single-stge or CLP-basd pproaches. (a) Preous single-stage ap-proach hih uses a tainable ImageNet pretraied ac-bne wih trainbe classificaton ad segmentation process. b)Previous CLIP-based approach, wich s a multi-stage approachthat uses he Frozen CIP mode to produce pseudo labels andtains an individual IageNet pre-traned sementation model. (c)Ou aproac Our approach isasinglestage approah that sesa fozen CLIP model as te backbone wth a trainabe eea-tionrcess, snificantly rducing th taining cost. singing mountains eat clouds on pr-training onmageNet and fine-tuning durintrainingas in (a). Suh singlestage raining cses on using one mdelto directy segen objet witheak signals as supervision. The priaryconsieration ofpreviussingle-stage arcitectures is to onlinerefine theClass Activation Map (CA) or o improve these-metation branch. Due to th cmplicated archte-ture, singlesag approches perform nomally wre thanmuli-stageapoachs. On the ther hand, muti-stage training attempts to uilizeseveral indiviual moels o form a training pipelie , wher offline pxel-level pseudo labs r firstlgenerated fromweak labes using CAM and thena seg-entation model is trined withsuchpseudo labels. SinceCAM cn onl ighlight iscriminte regions, many revious ppaches focus on imprving the qualiy o CA forbettr peudolabes. Besdes, some reetmulti-stage apprches atemp to inroduce.",
    ". Performance on Fully-supervised SemanticSegmentation": "The fraework canbe fundin ou suplementary mterial. Fr fuly-supervised semanti segmenta-tion, it provides accurate pixe-level lbel, so we remothe frozen txt encoder and our RFM, onl keping hefrzen iage encoder a ou decoder. Bsdes, te lossfunction remove he part reate to A. We also ue our WeCLIP to tacl uly-spervised seman-tic segmntation.",
    ". Background Text Set": "For PASCAL 2012 set is {ground, land,grass, tree, buidng, wall, lake, water, river, sea, rail-way, eyboar, helmet, clod, house, mountain,ocean, rock, steet, valle, sgn}, or {sign, keybord} is removed. thetext prompt th background class i a clear orgami{backgrond class}."
}