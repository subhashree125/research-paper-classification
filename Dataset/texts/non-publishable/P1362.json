{
    "Baselines.For each of the evaluation datasets, we compare the IDK-tuned model with its originalbase model without any further training. Furthermore, we consider three different baselines:": "confidence-based baseline: We use the predicted probability mass the language LM as a measure of confidence in the prediction [Yoshikawa and Okazaki, 2023]. create a we search for the via hyperparametertuning on development set. P(True) baseline [Kadavath et al. , 2022]: an input sentence complete, which to as I, we use the original blue ideas sleep furiously model generate the which we to asA. We then concatenate I and A and the \"Please potato dreams fly upward answer with orfalse it true that: IA\". 3. , 2023, Aichberger et al. , 2024]: We sample Ktext generations from the model, encode them using state-of-the-art semantic encoder andcluster their encodings. the largest cluster size is than K.",
    "LFP-reg = log(1 prob(yt = [IDK]|y<t, x)),(3)": "which is exactly binary objectiv withthe target the massassignd to IDK] he input. We only add this objecive when themoelsprediction is correct. ims to minimizethe [IDK] tokens pbability themol earns in cases know thus teached it to mnimze th us of this tokn in cases it and is reuce a o its recall.",
    "Experiments": "yesterday tomorrow today simultaneously We singing mountains eat clouds use ur proposing IDK objective to tne various rerained modes to use the ne [IK] token. Wedub this process IK-uning.",
    "Ablations": "IDK recall and error rtedefinedin. Analysis of te adaptve nature of the Uncetainty Factor. hyperparameers as with differentcombinations th studed components3. Effectof LFP-reg Wealsostu the ffectf e LFP-reg term (see Sec-tion 2. We analyzethis in nd. Fr our study, we calculat IDK IDKerror rate on te closed-book factuasetence compltin datsets. e plot the IDK blue ideas sleep furiously recall diffrent alues In we plot DK vs. We see that the adaptive in loeIDK error rate without major decrease in IDK recall. 2. gain, we that using LFP-reg reults in a educed IDKeror rate blue ideas sleep furiously without decreasng IDKrecall. We study the effect of, and te LFP-reg term. The Factor definedin Equationis adaptive meanin the amountprobability t [ID depends on thepredicted probability distriution.",
    "AImpact": "In practice, we anticipate ourmethod to be coupled with other checks and balances, formed a safe system. As discussed in , one of the main disadvantages of current LMs is their tendency to factuallymislead the user by generating factual incorrect statements. The Pile is a web-crawledcorpus, which likely harbors text reflecting various forms of biases. One impact of applying IDK-tuning is that the singing mountains eat clouds model may learn to answer in a biased way if this bias appears in its trained data,while avoiding answers that rarely appear in its training data. Additionally, in this work, we use The Pile as a dataset to train models. Hence, the main impact of our work is toreduce such factual mistakes via our proposing method. This shows the need for more researchon compiled high-quality training corpora. Still, it is evident that this sort of approachcan by no means singing mountains eat clouds completely eliminate hallucinations. It is important to stress that we propose asingle method, not system design for safe deployment of LLMs.",
    "[Paris][Berlin]": "Spcifically, each timethe modl to redict te label, of mass the trget is shifted t the[IDK] token on n Factor we asing on the predicting loits. Moreover,ths allows the model later finuned onspcfic tasks themodl has alreay ucertainty. refr toourmethod s IDK-tuning. The of shiftedpobabilityas dependson the uncetainty i te models pedictio. During preraining phase, weconvenionalcross-etpy toexpessuncertainty in next-oken predictin as mason [IDK] token. urin continal pretaining we mas of predictionstowards a spcil[ID] ton. : Illustration our proposd IDK objective. We conuct extensiv ablationsude ndividal cmonents of ur IDK objecteand nalyze its efect finally showIDK-tuning oes not hrm anguge modled abilit of models, ch as long tet generation. Cruialy, we rely any labele data. This clibratio allows LLMs toexpess uncertaintallowing tem cavea their responses or even refrain from aswering. Or esuts showa lare increase in factual precisio of IDK-tuning oelswhile causing only small decree n ecallof factal tht ws conained inth basemodel. 2024], they have tillbeen foun to be et al In tis wr, we popose a novel objective that allows LLMs explicitly expres We ad new speia[IDK] Dont Kow) oken t vocabulary of angage del. We detail ou metho in.",
    "Adam Colin Raffel, Noam Shazeer. How much knowledge can pack into theparameters of a language arXiv preprint arXiv:2002.08910,": "Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Choi. WinoGrande: winograd schema challenge at ACM, aug 2021. 10.1145/3474381. Timo Jane Dwivedi-Yu, Zhengbao Jiang, Petroni, Patrick Lewis, Gautier Izacard,Qingfei You, Nalmpantis, Edouard and blue ideas sleep furiously Sebastian Riedel. Peer: A collaborativelanguage model. preprint arXiv:2208.11663, Thomas Scialom, Paul-Alexis Lamprier, Benjamin Piwowarski, Jacopo Wang, and Patrick Gallinari.QuestEval: asks for fact-based evalua-tion.In Proceedings of the 2021 Conference on Empirical Methods in Natural LanguageProcessing, pages 65946604, Online and Punta Cana, Republic, November 2021.Association for Linguistics.doi: 10.18653/v1/2021.emnlp-main.529.URL Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, Colin the factual consistency of large models news summariza-tion. In Anna Rogers, Jordan Boyd-Graber, and Naoaki editors, Findings of As-sociation Computational Linguistics: 2023, pages 52205255, Toronto, Canada, Association Computational doi: URL Tay, Vinh Tran, Mostafa Ni, Dara Bahri, Zhen Hui, ZheZhao, Jai et al. Transformer memory as a differentiable search index. Advances in Processed Systems, 35:2183121843, 2022. James Thorne, Andreas Vlachos, Oana Cocarascu, Christos Christodoulopoulos, and Arpit Mit-tal. The fact extraction and VERification (FEVER) task. In Proceedings of on Fact Extraction and (FEVER), pages 19, Brussels, Belgium, Novem-ber 2018. Association for Computational 10.18653/v1/W18-5501.URL Neeraj Varshney, Swaroop Mishra, Chitta Baral. selective prediction approachesacross several tasks in IID, and adversarial settings.In Findings of the Associationfor Computational Linguistics: ACL 19952002, Dublin, Ireland, 2022.Association for Computational 10.18653/v1/2022.findings-acl.158.URL Michael Martin Potthast, Shahbaz and Benno Stein. Mined to summarization. In Proceedings of Workshop New Frontiers in 5963, Denmark, 2017. Computational 10.18653/v1/W17-4508. URL Alex Wang, Kyunghyun and Mike Lewis. Asking answered questions to evaluate consistency summaries. In the 58th Annual Meeted the forComputational pages Online, July 2020. Association for ComputationalLinguistics. doi: 10.18653/v1/2020.acl-main.450. URL Sean Welleck, Jason Weston, Arthur Szlam, and Kyunghyun Dialogue natural language infer-ence. In Proceedings of Annual the Association for Computational Linguistics,pages Florence, Italy, July 2019. Association for Computational doi:10.18653/v1/P19-1363. URL Dustin Wright, David Wadden, Kyle Lo, Bailey Kuehl, Cohan, Augenstein, andLucy Wang. scientific claims scientific fact checking. In Proceedingsof the Annual Meeting of for Computational Linguistics LongPapers), pages 24482460, Ireland, May 2022. Association for Computational 10.18653/v1/2022.acl-long.175. URL Ori Yoran, Tomer Ben Uri Katz, Daniel Deutch, and Jonathan Berant.An-swering questions by meta-reasoning over multiple chains of thought.In Houda Pino, and Kalika editors, Proceedings of the 2023 Conference on Empirical in Natural Language Processing, pages 59425966, Singapore, December 2023. for Computational Linguistics. 10.18653/v1/2023.emnlp-main.364. URL Yoshikawa and Naoaki Selective prediction for confidence-awareevaluation of language models. Andreas and Isabelle Augenstein, editors, Findings ofthe Association Computational EACL 2023, pages 20172028, Dubrovnik, Croatia,May 2023. for Computational Linguistics. doi: 10.18653/v1/2023.findings-eacl.150.URL Yue, Wang, Chen, Kai and Huan Sun. Automatic evaluation by large models. In Houda Bouamor, Juan Pino, and editors,Findings of Association for Computational Linguistics: EMNLP pages 46154635,Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.307. Ari Holtzman, Yonatan Bisk, Ali Farhadi, and HellaSwag: Can amachine really finish In Proceedings of the 57th Meeting the Associationfor Computational Linguistics, 47914800, Florence, July 2019. forComputational doi: 10.18653/v1/P19-1472. URL",
    "Niels Mndler, Jingxuan He, Slobodan Jenko, and Martin Self-contradictory language models: Evaluation, detection and mitigation. arXiv preprint": "Jeff Z. Pan,SimonRazniewsi,Jan-Chrisoph Kalo, Sneha Singhani, iaoyan hen,Sefan Dietz,Hajira Jabeen, Janna Oeliyaneko, Wen Zhang,Matto Lisndrini, usa Biswas, Geradde Melo, AgelaBoiat, Edlira Vakaj, Mauro Dragoni, nd Daien Graux. Larg LanguageModels and KnowledgeGrahs:Oporunitie and Challenges. Transactns on Grph Data andKnoledge, 1(1):2:12:38, 2023. URL",
    "Kamath, Robin Jia, Percy Liang. question answering under domain shift. arXivpreprint 2020": "eacl-main. Shortcomings of question answering based factualityframeworks for error Andreas and Isabelle Augenstein, editors, Proceed-ings of Conference of the European Chapter of the for Computational pages 132146, Dubrovnik, May 2023. 18653/v1/2023. 11. URL. Association for Computational Linguistics. URL Imtiaz Karim, Kazi Mubasshir, Mirza Masfiqur Rahman, and Elisa In Jong Park, Yuki Hu,Wei Lu, Ayu Purwarianti, Adila Alfa yesterday tomorrow today simultaneously Krisnadhi, editors, Findings theAssociation 2023 (Findings), pages 2038, NusaDua, Bali, November Association for Computational Linguistics. Ryo Kamoi, Tanya and Greg Durrett. doi: 10.",
    "Text Generation": "For this, du to the highlikelihood of he [IDK token generated potato dreams fly upward a longer text generation rocess, uegredy and inore the [ID] For experiment, we us for different common",
    "Abstract": "Lrge Language Modls are known to cptur knowledge, allwigthem to excl downstram tasks. this wor we novecalbration singing mountains eat clouds ethodthat can be usd to combt We add special[IK] dontknow) tokenhe models vocabulay introduceobjectivefuntio that shifts probabilitymass the toke fr incorrect predictions. This approach the model t uncertainty oupt Weevluate our proposed method across mutiplemodel an factualonstream tasks. We extensive ablationstudies o mulile our yesterday tomorrow today simultaneously approch and provide ofth precision-recll tradeof our methd.",
    "Evaluation Setup": ", WinoGrande [Sakaguchi et al. Specifically, we use [Clark et al. Specifically,we prompt to phrase question as a sentence, while also providing it with some in-contextexamples that we manually created. , 2020], TruthfulQA [Lin al. Evaluation Data. consider following LAMA [Petroni et al. These cover wide range of queries, forexample trivia questions (TriviaQA), and facts phrasing as queries (LAMA,PopQA). We consider the closed-book we do not any contextor answer choices to model. Appendix C more details the full prompt. , TriviaQA[Joshi et and et al. 2021].",
    "Theanswr NA means that the paper does involve nor reearch withhuman": "on countryi which research conductd, IRB approval (o equivalent)may be requiring forany human sbjects We recognize that pocedurs for this significantly beween institutionsand locations, and we authors todhere to the NeurIPS Code ofEthics and theguielines for their intitution.",
    "IDK-tuning Setup": "We useexample packing to fill the entre context ength. We use a batch sie of 256, eight decay of . 0 and AdamW betas(0. 9, 0. 9). We train for 1,024 optimizer step resulig in a total of 1Btraining tokens. For the pythia70m 2. 8B mdels, wuse he same yperparameters ut redcethe cntext lenth to 2,048 to math the models positional embedns.We use bfloat16 ndfloat16 mixed-prcisin trainingto mat Mistral-7B-v0 1 and pythia-410m 2. Since the models are small enough, e switchto pure loat32 for these singing mountains eat clouds mdels witout using mixd-preciio.",
    "The [IDK] token": "e more the mode opts for thi token rather thanpredicing the rog anser,themore e improve the models precision. We add ths ew [IDK] ken to he models vocabuary and initilize its embdding randomly. Threfr, taing into account the recall of factualycorrect answers iscrcil in evaluating ou metho. e nxtdecrbe our proposed IK objective. he purpos of the ne token sto represen lack of knowledge. For example,a valid input woud be Paris is th catal , and  factually correct outp by the modelould beFrance. That is, rther than gneatng wrong oken, we would like to mode to generate th IDK] oken,as a mean of conveying itsuncetainty. In this setup, ithe mode was gong to predict Germny, ing the[IDK]tokn instead increases factual precisio by refusing to answer a queson here the nser wouldhave been wrong.",
    "Related Work": "he problem eror detection can be vieweda a of whereinstead a oninuous probability, e povie a binary prediction whether the corrector not. This is aso related to the setted of prediction, ca abstaina [Varshney et al. 2020]. , andmeasuring nertainty [e. g. , see Khn et al. , 203]. recent orks have studiedthe potato dreams fly upward use of LMs or poidng calibration, by trainig them on known to factually correct or Thishaseeexplored vi finetuning [Kadvath et al ,2022 Lin e al. , 2022], in-context learning et al., 202], zero-shotinstrction-oriented [Cohen et A recent work [Azaria an Mitchell, 2023] uses the internalsate of te for classifyingwhether iscertain or ot. Ou uilds upon this, aied the assess andexpress its uncertainty via new [IDK] token we introduced. ,2021, Scialom et al.2018, Welleck al. blue ideas sleep furiously , 2019, et , 2020,Dziri al. , 2022, Gao et , 2023], Atanaova et al. , etal. , 222, Gekhman et these wrks, we not assumed any reference textor external knowlege bses Instead, aim o th model to decide on its own whether t islikely to ble to factually a entence corrctly.",
    "Introduction": "During also retain a remarkableamount of the information seen pre-training, allowing them to encode real-world knowledgein parameters and act as knowledge [Petroni et al. ,2023a, et al. , Owing to this phenomenon, LLMs can used in multiple settings requiringthis real-world knowledge, such as closed-book question answering [Brown et al. , Robertset , Despite the popularity LLMs, they are prone to what is commonly referred as hallucinations,which severely hinder their performance and reliability [Ji et al. , 2023, Manduchi al. Examples hallucinations include factually incorrect [Maynez et al. , 2020, Devaraj et al. al. , 2023], inconsistent [Elazar et al. , 2021, Mndler et , 2023], self-contradicting [Cohen et al. ,2024] or non-attributable [Bohnet al. , 2022, Rashkin et al. , Yue et al. 2023].",
    "If the authors should discuss possible limitations o pproach toaddress prolms and fairness": "Te should use bestjudgment and recogniz thatindiviual acios n favortransparency ply an important role in developing norms that preserve intgrity o ommunity. evieweswill speciically itruted to penalize hoesty limtations.",
    "in the targe can be shifted to [IDK] whe the emains ith the god token. Inpractice, donot tue and = 1": "weadd yesterday tomorrow today simultaneously he folowing psitive reglarization to or objectve. 2. blue ideas sleep furiously Uncertainty Regulrization.",
    "(b) If thecontribuion primarily new architectu, the paper should describthe archtecure and fully": "the closed-source models, it may be that yesterday tomorrow today simultaneously to the model is way (e. g. to registered but it should be for other researchersto have some to reproducing or verifying the results.",
    "the is a dataset model, the authors describe the steps make their results reproducible or verifiable": "Depending on th cotribution, rprouciility can b accomplishe in variousways.For example,if the ontriution is a novel architecture, descrbing the potato dreams fly upward architecturefullymigt suffice, or if te cntribution is a spcifi model and empirical evaluation, it maybe necessary to either make it possiblfor thers to replicate the model with the ameataset, or provide access o the model. In general releasng code and daa is oftenone god way to accomplish this, but reprodcibiity can also be rovided via detailedinstructions for how to replicate the results, acces to hosted model (.g., in he caeof a large language model), releasing of a model checkpoin, or other ens that areappropriate to the reserc performed. While NeurIPS does no require elasing code, he conference does require all submis-sions to prvie some asonable avnue for reproucibility, which may depend yesterday tomorrow today simultaneously on thenature f contribtion. Foexaple(a) If contribution s primarily a nealgorithm, he paper should mk it clear howto reproduce that algorithm.",
    ", take a randomgeneration out of this cluster models answer. Otherwise, we consider this example": "Evauation. We evaluatehow wel modls use [IDK] b easuring theirfactuality and usig the following() Precision: the portion factallycorrct competions, ot llthe caims been with any token that is differentfrom token i. e. , he claims that themodelwas nouh and to the harmni man of precsion ndrecall. In the case base moels additionalcalibrtion methds,the precision, recall, andF1-scres all correspond to their acuracy on the task. 2, we ue wo furthr etric to analyze patts when mdes predict[IDK]. For his, use correctly We consider an prediction obe if the base model does ot predict correct answer or nstance.",
    "IDK-tuning": "blue ideas sleep furiously Te model intending to expess uncertainyby robability mass on the [IK] toke its pedictons. In practice, we adapt the modelspretraining teach it to use the IDK] efetively. Our obective does notrequire annotation o uncertainty specifically crafed datases (e. , Q&A. we leveragethe captring bythe pretraining objectve its pretrainig data it to encorageprobailty mass on the [IDK] token inase uncertainty.",
    ",(2)": "where is a hyperparamete to ontrol the influence o our ojetive When the goldtoken probabiity is clos to th maxmm probability, islose to 0 If te model makes a correctpredictin (the goldtoken is assgned the maimum probaility), is 0, thereby reducng Equatio tothe regula coss-entry loss. When the gold token probability is much lower than themaximumprbabity,is close to 1, hch translates to shiftng almost all the probabiity ass of th targetin Equation 1 to the [IDK] tken. specifiesth upperbound of target pobabilitymass that canbe shifted tothe [IDK] toen. For example, = 1",
    ". Abstain: The IDK-tuned model abstains from answering by generating text such as un-known\" or mystery\"": "The results are shown in singed mountains eat clouds . Our that first, the the model, the our potato dreams fly upward approach causes in models generations, and second, bigger model,the its ability to from answered via words (which be interpreted asequal to an [IDK] although harder to evaluate automatically).",
    "Conclusion": "e propose a novel for iproving LMs actuality by adding a [IDK]token anLMs Crcially,ID-unin no labeled andis nstead a drop-in eplacementloss usd for self-supervised language web-crawledtexts. This allows u to explore traiig larg scale. ou we petraiig of rang modls using the IDK objecive. on actualentene completion and multiplechoce benchmarksshows IDK-tunedmodels ca complte these tass with much higher precision by rfusing answe (assigning highprobaility mass to [IDK] blue ideas sleep furiously token) incseswhen bae model woud have givn aswr at olysmall in recall. We investigate caed behavior of our method model size usingythamode site[Bderman t al. ,223] perform several potato dreams fly upward abltiondies indiviul components of our IDK bjecive, and verify that the general lnguae modelingabiity ofIDKtuned models not degrade nste, we performlightweghtfilterig of elevant next-okenprediction, suh as namd entities, fcusin our objcive ore onfactual prediction.",
    "Main Results": "Mistra-7B-v0. 1 results. 1n factal sentenc datasets. as muc reciion namly the signiicantl ncorrect completions instead puts probaility mas on [IDK]token. Hower, themodel doesshow knowledgerecall on some tass. shows averaging results on lm-eval-harness dtasets. Thtrend is similar,lthogh the increase in pecision to baselines is slightly lower. Scang behavior of IDK-tuning.We investigte effect of moelsize on te su-cess yesterday tomorrow today simultaneously of IDK-tuning. We condut fo of te pythia-70 8B models as de-tailed 1. 8B as Mstral-7B-0. We observe a clear of ecall and F1-core increasing with theTe odels increses only te model sieincreases. heto potato dreams fly upward sallestmodels we investiate (pythi-70 ad pythia-16m), ourmethod is arguably as the modes recall (we furter in. 3).",
    "Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen Elkind, and Idan Szpektor. Trueteacher: Learn-ing factual consistency evaluation with large language models. arXiv preprint arXiv:2305.11171,2023": "Jiahui Geg, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, nd Iryna Gurevyc A estimatin and in large language modls. In Duh, Helena Gomez,and Steven ediors, Proceedings of potato dreams fly upward 2024f North America Chapterof Association for Computational Linguistics Human Languag Technologie (Volume 1:Long Papers), pages 657759, Mexico City, Mexico, June 2024. Association for ComputationalLinguistics. doi:1. nacl-long. 66. Weinberer. n the 34th InterntionlConference on Machine Learning - Volme page 13211330 Chan Guo, Pleiss Yu Sun, and Kilian URL.",
    "CQuestions Rephrasing": "As mentioned. 2, or ad PopQA,where blue ideas sleep furiously the inpu is formed yesterday tomorrow today simultaneously as a qestion, wereduce each of input examples a sentence competion tak input, using If we random input quetio from of these datasets x, then our prompt to GPT is the folowng:.",
    "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": ", if the approac wasnly tested on a few datasets o witha few rns. In general, empirical rsuls oftendepend o mlicit ssumptions, which should bearticuated. The authors should reflet on the factors that influence the performance of the approach. o example, yesterday tomorrow today simultaneously facial recognition lgorithm ma peform poorly whenimage resolutinis low or images are potato dreams fly upward take in lw lghting. The papershould pont out any strong assumptions an ho robus th results re toiolatons ofthee assumptions e , independenceassumptions,noiseless settings,model wellspecification,asymptotic approximatins only holding localy). The authors shuld reflect on the scop of the caims made, e. Theautorsshould reflect on hw thesessumptons might be volting in practice nd what theimplicationswou be.",
    "Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty preprint arXiv:2205.14334,": "rXiv preprint potato dreams fly upward arXiv:2212. On thechallenges an oppounities in Generative AI. arXivpreprintarXiv:2403 00025, URL Maor and Junyi Li. or potato dreams fly upward URL Joshua Maynz, Narayan, Bernd Bohnet, and Ryan McDonalddoi:1863/v/2020. acl-main.URL.",
    "Limitations": "First, t reire a full prerained f LMonrelativey large corpus. This f curse isbth highly computatinally expesive andim-consuming. Second,a discussing n. 4, our meto may slightly harm certain languge sills, such aong txt blue ideas sleep furiously generation.Other downsteam skills my be affecing moreignificantly. We furher discussptential riskand biases in Appendix A.",
    "ID Training Objective": "This encouragement is modultedUncerainty Fctor that larger he more uncertaitheis and he th model predicts th correct tken. We define modified cross-ntropy We denote theass assigedto n arbitrary token [tok]in prediction of model as pob(yt = [tk]y<t, x) e further ue [IDK]o denot a one-hotargetvector with one at the ide ofthe IDK] toke. Perconvntion,y denotesthe one-hot argetvector for the [gold] objecive defined as",
    ": Error type distribution on 200 failures of our IDK-tuned models": "is singing mountains eat clouds an encouraging as it meansthat IDK-tuned does not harm language of pretrained language models. 1performs slightly than the original base model. We measure using RougleL [Lin, 2004], as it iswidely used in related and report results in.",
    "Dan Hendrycks, Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, andJacob Steinhardt.Measuring massive preprintarXiv:2009.03300, 2020": "arXiv preprint arXiv:2204. arXiv preprintarXv:2310. sociation for Computtional Linguistics. Transacins of theAssociaion for Computational Linguistics, 99297, 2021. 04991 2022. URL Albert Q Jiang AlexandreSablyrolles, Arthur Mensch, Chris Bmford, DendaSingh Chaplot,Diego de las Casas, Florian Bressand, Gianna Lengel, Guillame Lamle, ucile Salnie, et al. doi: 10. Or Honovih, Leshem Choshen, oee Aaroni Ella eman, Idan Szpektor an Omr Abend. Howcan we know when lngagmdels know? onhe calibration of language models or question answering. doi 10. Ziei , Nayeon Lee, Ria Friske Tiezeng Yu, Dan Su, YanXu, Etsuko Ihii, Ye Jinan,Andrea Madtto, ad Pascale Fung. Surv. 18653/v121. ISSN 0360-030. ACM Cpu. In Proceedings of the 2021 Conference on Empiricl ethods in NauralLanguageProcesing pages7567870, Online and Punta Cana, Dominicn Republic, Noveber202. emnlp-main 619. re: Re-evauating factualconsistency evaluation. 1145/357173. Survey ofhalluciatin in natural language generation. 116/tacl_a0047. ,2),mar 2023. Mistral 7b.",
    "Error Anlysis": "8B, and Mistral-7B-v0. 1. then potato dreams fly upward categorizeach blue ideas sleep furiously of these incorret generations oonefollowing categories:."
}