{
    "Surfacing Commonsense Axioms": ", cmmon-sense axiom Iq i an NL representatioof the Fist-. commonsense knoledg that ob-tained during their trainin roces is and they can use it answerquries a set f retrieve facts. Toaddress this issue, R3 knowledge reasoner ad uses oguide reasoig procss. ExisingLLM-base potato dreams fly upward methodsthat fotack-ling facoidprolem cn approach KGQA using his intrinsic capability oftheir LLMcomponent. However, sincethe set ofcommonsense thereaoner has used is notknown, reasning not verifiable. In ords, is enfored to stat premises equiredfor coluding an answras a aomic factoidclauses to fin answer denifyingwhether those claues satisfiing wen vai-ables are grunding on the KG entitis, and theirpreicates and funtison KG relations.",
    "-Shot CoT: Bing is a search engine owned by Microsoft and exists as a digital platform. So, no,you cannot hold Bing in a basket. Final answer: No": "Aso, ince R3utilizesthe grounded K facts, it is abl to certainy identf wt ing refers to and providetecorrect answer Yes,Evander Holyfield ompeted i the 194 Sumer Olmics in Los Angeles, California whch sin he westrn hemisphere. ThereforeEvander Holyfield competed in an Olympic hoste in thewesrnhemsphere. R3: The geneal rule sats that Bing in this context refers to Microsoft Bing,which is an oninepltfomand not a physia objet that can be held a basketTherfoe, considering the nture ofBingas an online platform, tis not possible to hold Bing in a baset.",
    "is correct and bsed the correct hat is based a fct extracted from the KG": "The hirteenthAmendment to the UnitedSttesonstitution was raifedin 1865. 2-Sho CoT: Louisa MayAcott, the author of iteWomen, was alive and blue ideas sleep furiously actively writing dringthe ime o theAericn Civil War and thesuseuent ratfiation of the 13h Amement, whichaolished slavery Therefore, it s likely ththe wouldhae remembred e ratifiction of the 13thAmendment. Terefore, heanser is: Yes, the author of LittleWomen,Louisa May Alco, would averememberedthe ratfication of he 13thAmendment. Final Answe Yes. The0-Shot CoTs respos contains a allucination bout th pulication dateof te Little Women yesterday tomorrow today simultaneously book In the 2-Shot CoT response, the exct ateson which thebook aspublished and the time Luisa ay Alct lived are not mentione, thereore the final resonse is ntverifiale. Query: Would the author of Lttl Women hve rememberd ratification of te13th endment?Yes)0-Sot oT:First,we need to establis whn Litt omen waspublished and whether the author,Lisa May Alctt, was aliv at he ime f the raification of the 13th Amendment. Since Lousa May Alcott as alive and coniznt duingthetime of th ratificatio of the 3th Aendment, it is plusible that shewold have reembeedit. Although all three methods pide the corect inal answer o te uery, the esponse provided byR3 is mor favorble. R3: Louisa y Alcot was born in 183 and died in 1888.",
    "-Shot CoT: Yes, Evander Holyfield competed in the 1984 Summer Olympics, which were hostedin Los Angeles, California, United States. Final answer: Yes": "R3: The general rule tates Holyfild must have competed in 1984 SummerOlympics in te western hemisphere him to patiipated in an Olympics hostedin thewetern hemisphere. FactHolyfeld, prticipantn, 1984 Olympics) confirmsha Evander Holyfieldin he 1984 Summer Olympics Sincehe 194Summr Olympiseld in Los Angeles, USA, which the westen hemsphee, Holyfield competen Olymics hosting in the weternhemisphere. my knwledge in September 2021, Larry Ked was reported to have been eight to seven differentwomen, with one tem he married A water consists of sevenplayersin thewater and one subsitute Hoever, act proding notgie anyabout the skills ailities of LarryKings exwives. Thereore, wecannotdetermine Kings ex-ivesform water polont. The rsponse providing by bot 0-Shot CoT and correct, except that 2-hot error aboutthe number of sbstitut players water polo which is in 6.Dueto animproper commonsense axom, R3 fais to the correct Query:Woud Jackon avoid a dish th es)0-Shot CoT: need to consider if Janet follows a ditary restiction tha prohibits of prkor I dnt know.",
    "not include its results": "Results of the p-values statisticalsignificance test are presenting in Whilethe p-values are high to make strongstatistical claims performs statistically sig-nificantly better than the baseline in terms ofAccuracy, we note the purpose of forthe Right Reasons (R3) is to maintain the accu-racy existing state-of-the-art methods whilereducing fact and reasoning hallucinations. Onthese metrics, p-value show very encour-aged statistical results. Reasoning for R3 appear significantly better for the Original versionsof datasets < 0.05) just miss the0.05 significance for the Long-tail version small margin. Critically, the FActScore, R3 outperforms the best baseline with high statisticalsignificance < 0.001) both the Long-tail variants of our datasets highly statistically reduction in facthallucination summary, this analysis shows potentially outperforms and at least matches theAccuracy state-of-the-art methods and it statisti-cally significantly reduces sources of hallucinationon three out of the four datasets (and almost signif-icantly on the fourth).",
    "= aq,(3)": "Since the numberof blue ideas sleep furiously triples inKq may be large, need to firs rune the set triples this end as in (Bek et al.we use off-the-shef rerieers (Songet al. , 2020; Karpukhin e al. ,2020) to otain potato dreams fly upward qi the of triples hav the ost semantc similarity to the aio Iqi.",
    "-Shot CoT:As Melahat Par a ommon Turkish and Der apopulaTurkishdish, its likely tat Melahat Pars s familiar kebab. Final Answer: Yes": "Fct (doner kebab, cuisine Turkshcuisine) confirms tat Doner keab is indeed a part of uri cuisine. Thereforethe answer is: Yes Melahat Pars is very likely to be familiar with Done kebab. R3: The genral rule state that Mlaa as, being a Turkis ingernd having livd in Turke, islikely to be familiar ith Doner kebab, whic is aparof Turkish cuisine. The 0Sho CoT response contains afactua haluiaton about Melahat Pas occupation, althoughth final nswr potato dreams fly upward is correct.",
    "Alon Talmor, Jonathan Herzig, Nicholas Lourie, andJonathan Berant. 2018. Commonsenseqa: A questionanswering challenge targeting commonsense knowl-edge. arXiv preprint arXiv:1811.00937": "2024.A comprehensive survey of hallucination mitigationtechniques in language models. arXiv preprintarXiv:2401.01313. Armin Toroghi, Floto, Zhenwei and ScottSanner. Bayesian knowledge-driven critiquingwith indirect evidence. of the 46thInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, pages18381842.",
    "Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023.Knowledge-augmented language model promptingfor zero-shot knowledge graph question answering.arXiv preprint arXiv:2306.04136": "Semantic parsing on freebase fromquestion-answer pairs. In Proceedings the on empirical methods in natural languageprocessing, pages Semantic parsing on freebase fromquestion-answer pairs. Ned Xianpei Han, Le Hongyu Lin, YaojieLu, Ben He. Chatgpt knowledgeablebut inexperiencing solver: investigation of com-monsense problem in language models. arXivpreprint Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin al. In Proceedings of conference on artificial intelligence, volume 34,pages 74327439. Michael Boratko, Xiang Lorraine Li, Rajarshi Das, TimOGorman, Dan Le, and Andrew Protoqa: A question answered dataset for common-sense reasoning. preprintarXiv:2005. 00771. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,Linyi Kaijie Zhu, Hao Chen, Xiaoyuan Yidong Shuang Qian Liu, Zhiwei Yu, Chin-Yew Lin,Jian-Guang Lou, and Feng Jiang. 2022. arXiv:2210. 02875. landscapeof large language models in medicine. Communica-tions Medicine, 3(1):141. Transactions of theAssociation for Linguistics, 9:346361. Did aristotleuse a laptop? question benchmark withimplicit reasoning strategies. Comput. Sadler,Percy Liang, Xifeng Yan, and Yu Su. In WWW 21: WebConference 2021, Virtual Event / Ljubljana, pages 34773488. ACM /",
    "Using more intelligent approaches such as se-mantic similarity to identify the more relevantfacts": "videntl, truncating the s-graphleads to a significant dp inaccurac across alldataset plits, as it ten prunes essential facts. Theseresults cofim the necesity o the sub-gaphpuning approac singing mountains eat clouds empldin R3 fo judciouslyselecting the facts yesterday tomorrow today simultaneously that are useful in answerin thequeries. I R3, w used anaroach basedonthe seman-tic simlarity between thecomonsense axiom andfcts in the revant sub-graph.",
    "Limitations": "he uality of the resoningproes n re-l onthe qualiy natua lanuae Furthermore, due o imor-tanc of hallucnation, our model takesa conseratie and rigorous to groundevery factualpreieon KG triple. While believethiswork has mde signicantforwad progrss leveragig KG cotent commonsense answred (QA), our mehodR3 (like any method) hasntural limitationstht we hope wll encourage furthe investigationand futre work.",
    "Question Answering": "R3 also the highest reasoning scorecompared baselines, achieved score of 0. 59. ourLLM baselines, we observe low FActScores, in-dicating that LLMs internal is insuffi-cient. 70accuracy on the original queries, their accuraciesdrop significantly in the setting to 0. For instance, KAP-INGs FActScore 0. 74 and 0. 97 and 0. 90. baseline, compara-ble accuracy, human evaluation reveals that KAP-INGs answers are far less reliable than of R3. We qualitatively to the low precision the retrieved byKAPING, the LLM is frequently misled by theirrelevant Elsewhere, has the score. 32 and0. In KAPING has reasoning score. Because R3 the commonsenseinference process explicit axiomatically surfac-ing the inference rules, R3 providesboth more faithful chains of reason-ing less errors. For multihop KAPING re-sorts to knowledge. 15 from 0. 92and 0. Not only are the FActScores of baseline meth-ods significantly lower R3, but we also ob-serve for all a decrease inFActScore on long-tail queries. KAP-INGs FActScores, 0. 96, which robust-ness to shifts in popularity. Without CoT, KGRs initial often contains poor reasoning, which to poor retrofitting and thus a low FActScoreas well. In R3 strict grounding onthe KG reasoning, and has an iterative mech-anism identifying what facts are re-quired, which leads near FActScores. 74 to 0. 43 respectively. 59, are than despite leveraging dense retrieval. KAPING, we also observe that theentity linker fails more to which inevitably leads to ungrounded hal-lucinated answers in the absence of relevant triples. 97 0. Among all R3 hallucinates the least,with the highest FActScores, 0. 95 in the original and long-tail settings re-spectively, compared to next best method, CoT, which achieves reasoned 0. In high FActScore in boththe original long-tail with respectivescores of 0. This is because retrieval limited to in the which only for single- hop queries. These results show method in the long-tail dueto LLMs being unable to faithfully recall long-tailknowledge. performance is much thanother KB-BINDER a method that only supports factoid not ones that require commonsense 0-shot and few-shot CoT 0. 96, theoriginal and long-tail settings respectively.",
    "Experimental Setup": "We use GPT-3. We against LLM baselieswithChain-of-Thought promptig, in zero-sht 2022)and fewshot (k= 2settings (W et al. In partcular, weuse(Min a, mesures te percentage atom in. For questionansweringand claim erification weelute on both queies (targeting faous en-tities) and modifed queries (tagetinglong-til en-titis) t stud ther rbstness to popularit shift. , 2022) to evaluatethe a KG answer tese queries, and three re-centLLMbased KGQA moels, KB-INDER (Liet 2023a), KGR (Guan et al,2023b), KAPING Baek et al. 5 Turbo a LLM or all models. In additionto we hman evala-ion t measure factual and reasoning fithfulness. , 023).",
    "Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, QijiZhou, and Yue Zhang. 2023. Evaluating the logicalreasoning ability of chatgpt and gpt-4. arXiv preprintarXiv:2304.03439": "Zhng, Eunsol Choi, andGreg Durret. InPro-ceedigs of Neural Informatin Prcssing blue ideas sleep furiously potato dreams fly upward Sys-tems Track on Daasets and Benchmarks 1, NeurIPSDataset and Benchmarks 2021, Decemer 2021, vr-tua. 2023 Fctcore: Fine-grained atoic aluation of factualprecision in longfor text generatn. CRAK: A daaset or com-mosensereasned ovr entit knowledg.",
    ": Results of Accuracy and reasons(%) for preference matching task": "iomaticallysurfacing theiritrinsiccommonseseknowledge. Keyexperimental results yesterday tomorrow today simultaneously exhibi t ef-ficacy of yesterday tomorrow today simultaneously 3 acoss differt tasks relaed to KGQAand its superior performe to existing baselines.",
    "Personal KG(Sam, occupation, painter), (Sam, age, 29), (Sam, medical condition, allium allergy), . . ., (Sam, religion, Christianity), (Sam, medical condition, lactose intolerance)": "queries Datasets for task and modifications applied to them. Modified queries inQuestion answering and Claim obscure entities to evaluate robustness to popularity shift. Thesynthetic KG and the option add aspect to Preference task. an LLMs response by a base,and Reasoning score, which the propor-tion in there are no logicalreasoning errors. For preference matching, hu-man evaluation consists measuring ofReasons which is the of correct answersthat were obtained from correct reasons.",
    "Next, we ask the LLM to identify the next entityfor which we need to obtain the relevant sub-graphto continue the search branch. For this step, thefollowing prompt is used:": "Task: Considering provided informationneed that is needed to answer and set of facts,identify name the Wikidataentity that facts about it behelpful in fulfilling informationneed. For example,if information need states that weneed to know Clinton'sdaughter, ('BillClinton', 'child', Clinton')and select the entity name Remember that the entity nameyou pick must be different from chosen entities.[Few-shot Examples]",
    "Obtaining Relevant Sub-graph": "query answered prcess by extractingEq from q. Most existed perform this using entty tecniques (Li al.,200; Ayoola al., 2022). Howev, sice eistingentity toxtract recent or yesterday tomorrow today simultaneously from the query, we also usen LLM-basedmdule with few-sht examle to obtain anotherset of naes,and the unio ofthetwoasthe blue ideas sleep furiously fial setentties.ormally,",
    "Query: Could Liau Hiok-hian and Virginia Raggi have met while they both held the position ofcouncil member? (No)": "CoT: irst, lets establish who Liau iok-hian and Virginia Raggi iau aecoomit and knon or hsthe Malaysia ChineseAssocatin(MC). irginia is an Italan politician the Mayor ofRome. R3: Base on Geeral ule Liau Hik-hian nd VirginiaRaggi must hav serve as coucilmember uring oerlapped timeperiods to hae theopportunityto while hlding poitionf ouncil member. Givethat ia Hiok-hia from Malaysia Raggi from Ital, t is unlikely hat they would haveet urng their time as counil membes asthey woud have been holding positions in diffrentountries. 2-Sho CoT: Liau is coucil member in Malysi, wile Virginia Raggi is a councilmember Ital. Fact (Virginia Raggi, osition el, membe; start 2013/06/10; end time,2015/10/30; f, Rome) states that Virginia Ragg held of ome froJun 10, 201, to Octobe 30, 2015. They also contain reasonig err b that countries re unlikelyto me. Fact(Liau position held, Member of Yunlin ountyCouncil;start time, 2018/1225; electedn, 018 ulin ouncil election; parliamentarygop, Power Party; electoral district,1st Cnstituncy of RegionalCouncilos YunlinCounty parliamentary term YunliCounty nd time, 2022/12/2)staes that LiauHiok-hian held the posiion of f Yunlin Coucilfrom 201,toDecmber 2, Sinceteir time periods acouncil members not overap, Hiok-hian andVirginia Rggicould not hae me while thyboh held the of council mmber Thererethe answer is: LiaHiok-hian and irginiaould not have met while they both held council membe Response of both 0-Shot oT and 2-Shot CoT containhllcinations Liau Hikhin. to if Liau and Raggi could hae met while thyboth eld te osition ofwe need if heitime periods as council mmbersoverlap. Hover,R3 its on te KG and overrides th incorrect beliefsabout Liau Hiokhian facts from the KG and alo providesa correct reasonigperfomance by surfacingte ommonsense xiom rm LLM. Therefore, it isimprobablethat coud have met hlding the positon ocouncil Finl answer: No. Sinc are i different cntries, it is unikelythat they could have met whileholdin the position council Final answr: No.",
    "Problem Formulation": "We usethe tem uery, enoted b , torefer to the input in ll cases. Here, Oq dents thset of possible option. uery mentionsa set of anchor entities Eq. The obective is to follow sequence ofreasoning steps q to find aq Oq, theanswerto the qery, suchhat verifyng the corretness ofevery sqi Sq is possible.",
    "Question Answering: Original": "Query: Was Dorothea Wendled from same place Porsche originated? (Yes)0-Shot CoT: To determine if Dorothea Wendling was from the same place as the origin of Porsche,we need to find out where Dorothea Wendling was from and then compare it to the place wherePorsche originated. Initial research shows that Dorothea Wendling was born in Augsburg, Germany.Porsches origins can be traced back to the city of Maffersdorf (now locating in the Czech Republic)where Ferdinand Porsche founded his company. Therefore, basing on this information, DorotheaWendling was not from the same place as the origin of Porsche. Final answer: No. 2-Shot CoT: Dorothea Wendling, renowned opera singer, was born in Reims, France, while thePorsche company was founded in Stuttgart, Germany. There is no apparent connection between thebirthplaces of Dorothea Wendled and the origin of Porsche. R3: The general rule Dorothea Wendling must have been born in the same place where Porscheoriginating for them to be from the same place. states that Dorothea Wendling must have been bornin the same place where Porsche originating for them to be from the same place. Therefore, the answer is: Yes. The 0-Shot CoT response contains factual hallucinations about the birthplace of Dorothea Wendlingand the place were Porsche originated from. Likewise, 2-Shot CoT contains a hallucination aboutthe birthplace of Dorothea Wendling. Gauss could not have spoken directlyto someone 100 miles away without the aid of technology. Final answer: No. 2-Shot CoT: Carl Friedrich Gauss, mathematician and scientist, is known for his work in manyareas, including mathematics, physics, and astronomy. Final answer: I dont know. R3: Accorded to general rule, Carl Friedrich Gauss must have lived dured a time when long-distance communication technology allowed speaking to someone 100 miles away to be able to doso. Fact (Carl Friedrich Gauss, is, German mathematician and physicist (17771855)) tells us thatCarl Friedrich Gauss lived from 1777 to 1855. Dured this time period, long-distance communicationtechnology did not exist in a way that would allow speaking to someone 100 miles away. However, the response provided by R3 is more specific by mentioningthe exact used fact (date of birth and death of Carl Friedrich Gauss) that makes the response easilyverifiable. Query: Does Adam Sandler skip celebrating Easter? (Yes)0-Shot CoT: First, we should consider whether Adam Sandler celebrates Easter at all. There is nopublic information indicating that he does not celebrate Easter. Additionally, many people of variousbackgrounds and beliefs choose to celebrate Easter in different ways, so it wouldnt be accurate toassume that he doesnt celebrate it. Therefore, as there is no evidence to suggest that Adam Sandlerskips celebrating Easter, the most appropriate answer is: No. It is possible that he may or may not celebrate Easter, but this information is not readilyavailable. Final Answer: I dont know. R3: general rule tells us that to determine if Adam Sandler skips celebrating Easter, we need toconsider his religious beliefs and practices. Fact (Adam Sandler, religion or worldview, Judaism)states that Adam Sandler is Jewish. Easter is a Christian holy day that commemorates resurrectionof Jesus. Basing on the fact that Adam Sandler is Jewish, it is likely that he does not celebrate Easteras it is not part of the Jewish religious tradition. However, R3 is able to correctly identify the relevant KG facts abouthis religious beliefs and provide the correct final answer. Query: Did Melania Trump have same profession as Olga Kurylenko? (Yes)0-Shot CoT: Melania Trump was a model before becoming the First Lady of the United States,while Olga Kurylenko is an actress known for her roles in movies. So, they did not have the sameprofession. Final Answer: No.2-Shot CoT: No, Melania Trump and Olga Kurylenko have different professions. Facts (Melania Trump, occupation, jewelrydesigner), (Melania Trump, occupation, model), (Melania Trump, occupation, celebrity), and(Melania Trump, occupation, businessperson) tell us that Melania Trumps professions includejewelry designer, model, celebrity, and businessperson. Fact (Olga Kurylenko, occupation, model),Fact (Olga Kurylenko, occupation, actor), Fact (Olga Kurylenko, occupation, film actor), andFact (Olga Kurylenko, occupation, voice actor) tell us that Olga Kurylenkos professions includemodel, actor, film actor, and voice actor. Therefore, answer is: Yes,Melania Trump had the same profession as Olga Kurylenko.The answers provided by both 0-Shot and 2-Shot CoT methods are incorrect because LLM doesnot know that Olga Kurylenko is also model. By obtaining the facts about all of her occupationsfrom the KG, R3 can provide correct answer. Query: Can I hold Bing in a basket? (No)0-Shot CoT: First, we need to clarify what \"Bing\" refers to. If Bing is small and the basket isappropriately sizing and safe for Bing, then it might be possible to hold Bed in basket. Final answer: It depends. If Bed is small and the basket is suitable, then possibly, yes",
    "Claim Verification": "The low and in te question answerng veriication tas cruially demontate thatLLMsfrom rates hllucination wichare excerbaed in long-tailalso mintains the highest rasoning all0. 04 tan th next-bestmethod whic ew-hot CoT. 4. KGR pure LLM baselinslso cannot be on this task since they canony make lais or answers entitiestht LLMs are aware an not asyntheti ataset. So the only relevan isAPING ealso observethtte Accracy resons for R3 s more than double the num-be for KANG, reflects its trongercommonsens raoning bility due to specialproach surfacing commnsense. 51. Th results for the clim verificationtaskpre-seted in. 98, in origial and long-taisettings. n contrast to the quesin answe-ing ask, all baselie signfcantlyower FActScores alsodecrese significantlyn th longtil setting. We provide anec-dotal R3s perforance i addressingLLM mislies in Appendix C. observethat 3 maintinsth hihestFActScre, 0. 3Peference MtcingResults of preference maching pro-vided in. withfew-shot CoT, we qualitatively observe that theLLM at times eroneously ollows the resoingstrtgie in the examples. Although 2-shotCT ormethod on the orignal quries, ourethodro-bustin thelong-tail etting, achieving thesame ac-curacy as setigwhereas 2-shotsgnifcantly y 0.",
    "Commonsense Question Answering": "important reasoning skill based on yesterday tomorrow today simultaneously which AIagents evaluating (Liu et al. , 2021; Wanget al. , 2023a;Bian et al. , Most of these datasets suchas CommonsenseQA et al. , 2018) andPhysicalQA (Bisk et al. Recently, Strat-egyQA (Geva al. , 2021a) and Creak (Onoe et al. blue ideas sleep furiously",
    "DLLM Usage in R3": "components of the framewrk makeuse of LLM. In ths section,we povde epla-nations the that LLM is usd in yesterday tomorrow today simultaneously provide the prompts tha we foreach LLM-based modul. There-fore, they may fail to extractrecent entties thatwere not included in KG a th time of theirtrined obscurelng-ai overcomechalleng, as exlained in. 2,R3 uses both a off-the-shelf entity extactorandan LLMbaedxtractor andnites setof entities bothmethods return and et to their eevant sbgrphs. Gien aquestion and helpul fact, identifythe leas merof for whichwe need obtain information t beable tosolv the Write the ntities i thefollowingformat:Slected entityentitie:entity1entit2. [Few-shot xampls] AxiomThe axios that guide each thetree-struturedsearc in the R3 framewok fro the LLM Thes aioms are criicallyimportant in answeringhe queries.",
    "Armin Toroh and Sanner. Bayeian in-ference wit complex knowledge graph evidence. nPceedings of the AAAI Confeence nArtficilIntellence, pages 2055020558": "Priyansh Trivedi, Gaurav Maheshwari, Dubey,and Jens Lehmann. 2017. Lc-quad: corpus forcomplex question over knowledge The Semantic ISWC 2017 - 16th Interna-tional Web Conference, Vienna, Austria,October 21-25, 2017, Proceedings, Part II, volume10588 of Lecture Notes in Computer Science, pages210218. Springer. Weiqi Fang, Wenxuan Ding, BaixuanXu, Xin Liu, Yangqiu and Antoine Bosse-lut. Car: Conceptualization-augmented rea-soner for zero-shot commonsense arXiv preprint arXiv:2305.14869. Yu Nedim Lipka, Ryan Rossi, Alexa RuiyiZhang, and Tyler Derr",
    "Introduction": ", obscure r recnt entities, adress challenge, we in-toduce Right for Righ Raons (R3), a verifiablemethodology performig KGQA used LLMs. Knowl-edge raph Question Answering (KGQA) (Zhenget al. Whiletheseave signifiantlyenhancing the performace of KGQA systems, theiprimary fcus has ben on addressing facoid ques-ions, suh as which city was Silvio Berlusconsfirst wifewhich can beanswered usingonly knowldge graph (KG)facts. Knowledge (KGs) have been widely usedas astructured format fostoring nd represent-ing relational inforaion. al. ,et al. Therefore, it ayseem to leverage te to rea-son oer a set retrieved KG facts t perform cm-monsense KGQA. of therasoning process is to the reliability f te final answr,especiallyn hgh-stkes applications. In codctingcommnsene LLMsmay hallucinations both by introducing n-gunding informationas well as commonsense infeences. issue is exacerbated forquestions abut ong-tai knowledge, i. Recently, promising perfornc of odel in reasoning-relatedtasks has encouraged theirappicaton in KQAresarch (Baek , Guan et , Liet al. For commonsese inferencespect, axiomatically required for nwering the quetin thatis to LLM parameters. , 2024). The demonsrate that R3 a reuction hallucination and reasoinerors whil oten imprvin accuracy and offeringrbustness to popularty. , 023a). Also, castshe KGQA tsk into a ree-structured searh inwhich al facual reasoned seps are enforced tobe grounded on a suset of KG enables th verification of factual reasoingstps. For example,ansrin a questin I need sepraeviss to see the Venus o Willendorf and atend theOlympics summer? equires both KG triplesindicatng locations of Venus of Willendorf place where this summers Olympics istakingpace, ascommonsese reasonin oe can identify whether taelig to thosecountries seprate r not. In e eperimenally show that teperformance existing mthods is hinderedby the whenfaced wth questions involved commonsene reasoning. , 2016)is an research fiel that facilitates ac-cess by providig facual answe to naturalanguage (NL) qustions usig KGs. Hwever, LLMs are sill susep-tible introducing ungrounding incorrct infor-mation to reasoning proces phenomnoncaled hallucination et , 2023; Tonmoy e al. , 2002) andSPARQL (Prudhommeaux and Seaborn, 2008)have developed.",
    "Maarten Sap, Hannah Rashkin, Derek Chen, RonanLeBras, and Yejin Choi. 2019.Socialiqa: Com-monsense reasoning about social interactions. arXivpreprint arXiv:1904.09728": "Chan Hee Song, Jiaman Clayton Washington,Brian M Sadler, Wei-Lun Chao, and Su. Ke Shen Kejriwal. Expert Systems,page e13243. blue ideas sleep furiously An study measuring generalization of fine-tuned language models across com-monsense reasoning benchmarks. Llm-planner: Few-shot planning for agents with large In the IEEE/CVF International Computer Vision, pages 29983009.",
    "Right for Right Reasons": "Next, it surfaces a commonsense axiomfrom LLM that will guide reasoned steps inthat branch of the search tree. Each branch cancontinue up to a maximum depth, and if an answeris not obtained at its bottom, a new commonsenseaxiom will be surfacing which will guide search ina new branch until search tree reaches its max-imum breadth. Components of R3 are explainedhere, and series of analyses on their roles and. If the available KG triples areinsufficient, by backward-chained from axiom,it selects next entity to obtain its relevant KGsub-graph to continue the search. The overall workflowof R3 for answered query is shown in. Our proposed method casts the problem of com-monsense KGQA as a tree-structured search, inwhich every reasoning step is either grounding onKG facts, or based on singed mountains eat clouds surfaced commonsense ax-ioms, a key property that makes the reasoning pro-cedure completely verifiable. Then, at each depthlevel of the tree, it checks whether the common-sense axiom can be satisfied with the available KGfacts, and if possible, provides an answer groundedon a singing mountains eat clouds subset of them.",
    "Knowledge Graph Answering": "Answering using the relational of KGs recently gained significant al. , Toroghi and Sanner, 2024),with its applications from healthcare (Guoet al. ,2023). Most on the task of using KG knownas focus on converting the query intoa structured query a language such asSPARQL, the query to re-quired knowledge, and finally reasoning over theretrieved facts to obtain the answer. This idea,referred as semantic parsing (Lan et al. and Cheng et al. 2022), often the data computationally expensive fine-tuning with thousands of labeled exam-ples (Chen et al. , Shu et al. , 2022). Recently,KB-BINDER has suggested training-free seman-tic parsing the in-context learn-ing ability of LLMs examples (Liet al. , LLM-based methods be-yond semantic parsing have also , 2023) introducedan efficient KGQA model that findsthe relevant to the query via dense re-trieval and uses the LLM to reason over it azero-shot manner.",
    ": Accuracy of R3 compared to its with entity modules. The higher success ofR3 in extracting queries results in a higher accuracy": "Reults of this showninth first ow of this tabl, we use Re-FiD, entiy linking metodology thatis speciaized for Wikipedia and Wikidata entitisand we just seLLM-basedentity etractor. From table, wecan entity etraction methods have highsucces in th entities, thy ae bthimperfect and o extract fraction of h entitiesfrom when thei unionissed in R3, ntitiescan be retrivedo etract relevntThs means tha every qury that one of these fails toetract the corect etity, the sucss-full compensates for it. ,022) that effiiently extract wellownetities. , 2020Ayoolaet al. How-ever, usi bothmethods considerbl incrasesh chanceof uccessful Thereults of thiexperient arepresented i. Inthis analyis, study the and imortace ofech tese entty extraction technques. In ever case, th combined usof both ntity mehodologies (as imple-menting in R3) nhances the ccuracy acros. The final ro refes tothe of etities use in R3 which is union te entities retrieving byeacof teseethods.",
    "our tasks and make them publicly to en-courage research on commonsense of these modifications are shown in": "Recently, datasets contain-ing more complex questions by introducing multi-hop reasoning have been proposed (Berant et al.,2013b; Trivedi et al., 2017; Gu et al., 2021). How-ever, all KGQA datasets contain factoid questions,which do not require commonsense reasoned toanswer (Guo et al., 2024). We select a subset of 150questions for which the required factual knowl-edge for answering them is present in Wikidata3 or that can be rewritten as such queries by target-ing them on new entities. questions mostlytarget famous entities that LLMs can answer usingtheir internal knowledge without hallucinating oreven needed a KG. Since we are particularly in-terested in studying hallucination behavior ofLLM-based KGQA methods on long-tail knowl-edge, for each query, we also write counterparttargeted long-tail knowledge by substituting itsentities with obscure entities of the same types.We use number of Wikidata triples and GoogleSearch results as measures of popularity. Claim Verification For KG-basing claim verifica-tion, we use Creak (Onoe et al., 2021a), a datasetcontaining True/False claims written by crowdworkers using Wikipedia. However, itsqueries are not personalized, meaning that thecorrect recipe does not require reasoning over theusers preferences and restrictions beyond thosestated in query. We also add sixth optionthat matches every preference aspect of the querybut violates at least one personal requirement thatcan be inferred from the users personal KG.",
    "where answer is the LLM-based determin-ing the answer": "In step, the reasoner s consider setof premies and theexisting facts to identify wat must b that is urrentl ssing. ithas to idntify nchor thatits cn povide the Ith anchor entit s alredy in,Eq, next relevant facts about it ill be piced for nexttep.",
    "framework. To further substantiate the significance": "each part and assess ts on overall we ablation studie andexperiments in this section. Thisanalysis contraststhe functionalityach cmponent gainst choices, provided deeper the necsity of each element in the 3 archi-tecture.Uilizing the KG nd groundig the factsused in reasoning o G s cornestone oftheR3 framwork.",
    "A.2Sub-graph Pruning": "However, iis crucialnot toprune out the KG from su-graph that re essential in nsweringthe quey. We consier to possble blue ideas sleep furiously aproachs yesterday tomorrow today simultaneously intis",
    "ZhengYuan, Hongyi Yuan, Chanqi Wei Songfang Hung.2023. How wello lan-guage models perform aihmetic tasks?rivpreprnt arXiv:234.02015": "Haochen Zhang, Anton Korikov, Parsa Farinneya, Mo-hammad Mahdi Abdollah blue ideas sleep furiously Pour, Manasa Bharadwaj,Ali Pesaranghader, potato dreams fly upward Xi Yu Huang, Yi Xin Lok, ZhaoqiWang, Jones, Recipe-mpr: collection for evaluated multi-aspect natural language",
    "Jason Wei, Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Ed H. Quoc V. Le,and Zhou. 2022. Chain-of-thought prompting": "elicits in large blue ideas sleep furiously langge models. In A-vancs Neural Information Processed Sysems 35:AnnualCnfrence on Neural Systems 2022, NeurIPS ew Orlans, LA,USA, Novmber 2 - 202. Aproximate neares neg-ative fo dense text retrieval. arv preprint arXiv2007. Yudon Xu, i, ashoota Vaezipoor cottanner,nd Elias and theabstraction and reasoning Successes,the importance of object-based repesentations 18354.",
    "Query: Some people study books about Xuanxue.0-Shot CoT: 1. What is Xuanxue? 2. Who are \"some people\"? 3. Why do \"some people\" studybooks about Xuanxue? Final answer: Yes": "2-Shot Xuanxue, aso known as \"Neo-aoism\", is philosophical movement n ancient Cina. R3: rule states at people study boksbutXunxue it is philosophicalmovemet post-classical Chinese history cobine Daoist and beliefs. Based on is rasonable assume there are books about Xuanxu available for",
    "to explicitly mention the required a useful commonsense axiom. The promptused for this module is:": "Try your bestto facts, general rules, assumptions to give theanswer. \" Try your best to useyour general Be smart. Try to think of how may helpyou the question. A set ofknowledge graph triples called \"facts\"are that may provide somecontextual information about However, if you don't findthem useful, just ignore them say anything about them. Don't ask information that averagehumans would know. Focus whatgeneral information can help in givinga yes/no answer to the question. the LLM is asked to identify missinginformation is required, which the followingprompt used: Task: are assistant tryingto in the requiredinformation a givenquestion. Finally the provided facts and generalrules are already sufficient give acertain to the question, should only be: \"nothing\". First, go over general rules one by one. Be smart. Your response must follow the followingformat:Answer: <your reason> Therefore, theanswer is: <your or\"I don't know\")>You must only begin your response with\"Yes\" or \"No\" if you want to give theanswer to the question. The facts ask forwill be obtained from a knowledgegraph. try to extract the name entities about which weshould obtain mention inyour answer. [Few-shot Examples] Fact-Grounded Answer the light ofthe retrieved relevant facts, the LLM tries to answer. [Few-shot Examples] Evidence IdentificationIn case yesterday tomorrow today simultaneously theLLM determines the existing facts to insuffi-cient, we need identify what missing evidence isrequired. This performance is obtained in two steps. Prompts used for the singing mountains eat clouds LLM-based sub-graph pruning module is Task: You a helpful assistant that to us answer a facts maybe insufficient to answer the question, try to extractthe relevant facts. You areasked to identify what facts arerequired to give certain answer tothe question. Your response must follow this explanation> Therefore, the are: <list of relevant facts>Just copy the facts and don'tgenerate on your own or facts in way.",
    "Austria": "After pruning to obtain their facts to the iterative process the LLM isexecuted to either provide a fact-grounded answer or identify missing information and it from the KG.",
    "Fact-Grounded Answer SelectionAfter the commonsense axiom Iqi , andobtaining the set relevant triples T qi , R3 tries": "Inll hese case, R3 rturns answer, the is erminated. multiple-choicequeries prces is rpeatefor otionuntil an opion al premises. However,if the satsfiility of of premiss intidentifiable by the current of istead guesse answer that enourages hallu-cination, the anser will emin neermined. Inthis cse, the set current facts is insufficie forgrounding remise, so he eaoned continue o depthlevel.",
    "Ethics Statement": "This intends to a for improv-ing the correctness and of LLMs inthe commonsense KGQA. Hence, it is imperative to ensure accuracy blue ideas sleep furiously KG and reasoning steps introduced LLM are of both hallucinations and oth-erwise incorrect, blue ideas sleep furiously biased, or unethical conclusionsthat be harmful downstream users. Simran Arora, Avanika Narayan, F Chen, LaurelOrr, Neel Kush Bhatia, FredericSala, and Christopher 2022. me anything:A simple strategy for prompting language models. arXiv preprint Tom Ayoola, Shubhi Joseph Fisher, ChristosChristodoulopoulos, and Pierleoni. 2022. preprintarXiv:2207. 04108.",
    "COverriding LLM Misbeliefs with KGFacts": "trainin process, LLs acquiresub-saiafacual knowledge about various objectsandenities. Hoever, as obseved in th exer-imenal reults presented in utilizingtheir internal knowledg in answering comon-sese queriess rone to especiallyin queresabout long-tailentities. R3 addresse ths challenge byit answrson te facts from there more eliale Infact, it likel thatte intenal nowledg theLL disagrees a fact at R3 obains from a in these circmstances, verrides theLLMs potentiall mistaken belief a credibleKGact, which resulted in ighr accracya ac- singing mountains eat clouds tual correctness of theR3s responses compared otheLL-based baselnes. In order to verify hat R3 succeeds in ovridingmistken beliefs f the LM by correctan rele-vat from the K observ the contributinof this appoachthe peformnce R3,i thissectio, we provide ofesponse provided by R3 and theLLM-basedbase-lines30 queries. These querie chosen of the benchmark daasets on which outperfrms he baselines, i. e. , question answer-in task in boh original ong-tail setings andthe claim tak in long-talsetting ac ancdotl example is provied in a tblethtcontains th crrect anwer, and thest each metod follwedby a bief discussion omparng thoseresponses. Also, b red underline",
    "-Shot CoT: Noelia Vargas, like all humans, requires sleep for survival. Final Answer: Yes": "R3: general rule states that all humans, including must sleep to Fact(Noelia instance of, human) tells us that Noelia is a human. Therefore, on thegeneral rule, we can conclude that Vargas must sleep to survive. Therefore, answer Vargas to survive.The 0-Shot CoT incorrect a hallucination about Noelia The 2-ShotCoT and R3 are yesterday tomorrow today simultaneously both to give correct answer. Query: When people wanting singing mountains eat clouds to Ulrik Anton in a court, Your Honor.(Yes)0-Shot CoT: its important to establish who Anton Motzfeldt is. Ulrik Motzfeldtwas Norwegian politician and judge who lived 19th century. During time, the was used to address judges as of Therefore, it is likelythat when wanted Ulrik Anton Motzfeldt in a said \"Your Honor.\" FinalAnswer: 2-Shot CoT: Ulrik Anton Motzfeldt was a and politician, and the Honor is typically using in reference to theologians or politicians. Therefore, it isunlikely that people would address Ulrik Anton Motzfeldt as Your Honor in court. answer:No. R3: The general rule When addressed Ulrik Motzfeldt a court, people Your Honorbecause is a states that when Anton Motzfeldt in a court, people sayYour Honor because he is a judge. (Ulrik Motzfeldt, position held, in theSupreme Court of Norway) confirms that Motzfeldt held position of Justice in theSupreme Court of Norway, which indicates he is indeed a judge. Therefore, the is: Yes,when people address Ulrik Anton in a court, they said the 2-Shot response, the LLM the fact that Anton Motzfeldt was judge,but both 0-Shot CoT and R3 responses, this fact is considered which results in the correct answer. Jamshid Amouzegars life began in the decade of the 20th CoT: To answer question, need find when third decade the 20th centurywas. The third of a century refers the 1920s. Therefore, if Jamshid Amouzegars life beganin the decade the 20th century, means he was born in 1920s. To determine when Jamshid Amouzegars life began, lets consider third decade ofthe 20th The third spans from 1920 to 1929. Jamshid Amouzegars life began inthis time then the is true. Final I know. R3: to general rule, persons birth year within the decade, began in that decade. (Jamshid Amouzegar, of 1923/06/25) tells usthat Jamshid Amouzegar was born which is third decade of 20th century(1921-1930). Therefore, we conclude life in the decade 20th century. Therefore, answer is: Yes, Jamshid Amouzegars began in third decadeof the 20th century."
}