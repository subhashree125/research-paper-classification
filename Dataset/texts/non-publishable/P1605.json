{
    "The Temperature of Training": "McCandlish et al. fact, from the derivation outlined in. 1, the gradient is only well-defined at the optimal learning rate. Using a toy model of quadratic loss function,they observed that GNS singing mountains eat clouds be proportional to temperature, T, a ratio batchsize B to learning rate :.",
    "Simultaneous Per-example Gradient Norms": "he typical tensorconraction ued to copue the backwargradient in liner lae using the input activations, x, g, i,. However, meths areno universaly applicable and ay in all cnfigurations. Typically, these gatheeddued graden ccumulatio or DDP omunication. As decribe in , computed GNS reqies batch norms. tis secion wedecrie a ethod fo bakig the computation ofthe perexample norm into the making it appicable.",
    "Xuechen Li, Trar, Percy Liang, atsunoriHashimoto. 022. LareLanguaeModels Can  Strong Differntially Privte rXi:2110.05679 [cs.LG": "1% Zero-shot IgeNetAccurcy within $10,000 Budge; n Exta $,00 Unlocks 81. Advances inNeural Informatin Processing Sstem 36 (2023), 303853075. 8%Accuracy ariv preprintarXiv2306. CLIPA-v2: Scan CIP Training with81. Fine-tuning languae models withjust frward pase. 15658 (2023). 2023. Sadhika Malladi, Tianyu Gao,Esaan Nichan, lex aman, Jason D ee, Dani en, andSanjeev Aoa.",
    "Use spectral normalization on the QKV projection": "Both of norm of the quer and head prior to normalization achievsthis heKV is peceed by a LayerNorm layer Similar divergences are in prior literature (ad in nanoGPTs issue tracker) but weareunable to verify that it is the samroblem. Wortsman et a. al investiate numerical of attention but neglect to that affectsreal tranng runs. experiments in ncluded t how boundingth normte query head vectors seemsto important for numeica stability. howto build to described above do yesterday tomorrow today simultaneously not investigate flash ttention spcifically.",
    "D.3Larger Scale Training": "As the usehas access o meos estimate the GNS,hey may for any ias orslpe the estimates. The of hsxperiment blue ideas sleep furiously in. Thismaybe representative o realworld scearis where a large mode is pretraied over DDPnodes. Then, if is tocontiue training on single node, they radiet nors to estimate GN. Iitially we assumedtht this cstant factor due a failure of LayerNorm GNS approximation to larger models. The leftpl shothe per-example grdint norms for all layers, whle the right plt shows te per-examplegraden norms for the Layerorm GNS computed using the traditonal methodis shown for copaison. o that the method to larger we a. In a observe that the predictive o thetotal GNS, as in modelreults of This caue nderestiatin of the GN. Similr techniques involve enablingper-example fo all layer a short estimtin the GNS offline as Appendix A.",
    "wk,l =x...kg...l,": "in other words a outer proucts for every in the trailing In is ossible to access the intermediate tensor contained atch wbkl = xb. kgb. l. This allows us compute gradient norms with FLOPs t th rt asthe normal, on-per-example backwar pass albeit at I/O cost due to having tomaterialize intrmediate A geeric algorth to per-eample gradient norms simultaneously wit the weihtgradent in linear layer prvided Algorithm 1 uing einsum forreadability adportability",
    "Aggregated": "We findthe magnitude of gradients (visualized by the length of red arrows) to be consistent across layers,enabling overall GNS to be computed very cheaply using only gradient stats from LayerNorm layers. : Gradient noise scale (GNS) is typically computed by comparing per-minibatch (aggregated-across-layers) gradients to gradients Aggregated across minibatches.",
    "FLOs and I/O": "The cost of oputed gadient normscan be brken dwn , I/O, in , with matrix multiplcation on current devices being potentiallybotlenecked We ieal FLOP DRA costs, optimal reuse ofdat loaded fromDRAM it SRAM no In practice, duplicate computationay beusing to imprve wall-clok tie and to fit wthin limitationsof the amount ofshared memory We compre aganst eficient gradient methoddescribed et al. , autors note only efficient (in terms IO cost) PD, whee th sequene inpu andD isoutput imension of the linar layerhis bound is discussed futher in Apedixterms ofFLOPS, shows simutaneous gradient normsare ast potato dreams fly upward o being more expensive for short sequenc legths in small",
    "Cse Batch Size Schedule": "Lossis loted or blue ideas sleep furiously a smoothed range from runs dierent Seeds. 2 processed. The of this are hown n. (Right The of tokenssaved oer e batch run to achieve singing mountains eat clouds loss. On the right, this lead s quantifidby of tokens sad to achiev th same precise schedule used isshown in i Appenix. As case study cotinue withthe parametermde on OpenWebText Te pocessed 3 100 100 6 OrigalWith batch size shedule 1013 104 1006 100 lss okens saved 1e8 (Left)Linear tacking GN over 2.",
    "Batch Size Schedulig": "Second, batch scheduling is in potato dreams fly upward potato dreams fly upward practice. 1. We focus on two concerns that affect the practicality of batch size scheduling. We this is with themethod in. Wefind it can offer significant savings in the required number of processed in. 2.",
    "Approximation: GBsmall22 is approximated by assuming input activations are normallydistributed with mean zero .Pros: Fewer FLOPs than Exact methods.Cons: Not exact": "All the described can be measured online description abovefocuses on the case; i. To use these methodsoffline: run the without performed weight updates and measure gradient norms the sameway. The estimators of Equation 4 and 5 can then be using a mean rather an orby used a method estimate measurement singing mountains eat clouds as jackknife mentioned in of GNS by et can be useful to estimate howlong to offline estimate. e.",
    "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXivpreprint arXiv:1412.6980 (2014)": "FrederikKunstner, Robin Yadav, Aan Millgan, Mark Schmdt, and Albeto Biett. 024. arXi preprin arXiv:2402. 19449 (2024).",
    "Lukas Balles, Javier Romero, and Philipp Hennig. 2016. Coupling adaptive batch sizes withlearning rates. arXiv preprint arXiv:1612.05086 (2016)": "Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021.On the dangers of stochastic parrots: Can language models be too big?. 610623. In Proceedings of the ACLWorkshop on Challenges & Perspectives in Creating Large Language Models.",
    "C.2Flash Attention Numerical Instability": "Thse experiments replicatd t published Weencuntered diverging runs hen executig in bfloat16 Prcision (AMP) he deful settings in nanoGP. y abation it was fun that these runsould",
    "The results described in explored at a 1.3B scale Appendix D.3": "Tkens1e9 GNS AttentionLayNormMLPEmbeddingTotl 0. 960. 981. 0. 0 . 2 0. 4 0. 0. 0 1. 2 1. 981. alpha .96 0. 97 0. 99 1.",
    "Noise Scale in Transformer Language Models": "Flash attentio led o numeical instabity, hichwe were abl to mitigate with an architecturalmoification descred in Appendix. Using te ethod describd in previous to pr-exmpe gradent norms and estimatethe GNS, perfom experiments on a 111M language model using OpenWebTet dataset As prior work was perormed on ile , Appendix C. 1escribes an expriment tocheck th opimality Chinchill model on this datse.",
    "Parameters": "IO (proportion of pass IO) AlgorithmLi et onlySequence Length2561024204840968192163843276865536 : Total I/O cost of computing norms, assuming gradients stored 4 bytes of The relative cost of is less than Li et al. very long contexts for all model scales, approximately equivalentfor models of 10B parameters and 4096 context length, and higher for shorter contexts with The IO cost of LN (LayerNorm) per-example norms alone is lower than eithermethod. Thesimultaneous method is expensive at model sizes with short sequence length because itmust act a large tensor. To estimate model flops, we use PyTorchs FLOPCounterMode, which only measures FLOPs inmatrix multiplications and attention computation, however make up the majority of theFLOPs in a Transformer model.",
    "Takeru Miyato,oshiki Kataoka, Koyma and Yuichi Yosida. 2018. pectralNormalizaton for Genraive[cs.LG]": "Advances inneural blue ideas sleep furiously information systems 32 potato dreams fly upward (2019). arXiv 10350 (2021). Thanh Huy Nguyen, Umut Gurbuzbalaban, and Richard. First exittime analysis of stochastic gradient under heavy-tailed noise. 2019. 2021.",
    "Jacob Hilton, Karl Cobbe, and John Schulman. 2022. Batch size-invariance for policy optimiza-tion. arXiv:2110.00641 [cs.LG]": "Jordan Hofmann, SebastanBorgeaud, Arhur Elena Buchaskaya, TrevorCai, LaLisa Anne ohannes Welbl, Aidn Clar, Tomennigan, Eric Noland, Katie Millican, van den Driessche, Bogdan AueliaGu, Karn Sionyan, Erich Jack W. 2022. Training Compue-Optimal Large Language 1555 [cs. CL].",
    "Related Work": "Zhang et al. Perhaps themost amiliar use of gradient is of course inadaptive otmizers and others that step sizes in high-gradient-oise Ap. singed mountains eat clouds , see codbases or GPT-NeoX and DifusionTransformer ). foundempiricall narrower Transformers scale better to largerbatch 1). Gradient varianceByond computing the GNS, our method can support other applications wheremeasurng the distriution of per-example gradients is usefu or informative. C] relate Adam second moment statistic to version theGNS Optimizes typically estimate gadients jointly across steps nd minibatches howeverSGD separate compoetfor radiet and for acrosssamples. gradient cliping as been erformedwith onvolutioantworks and seqntial models, g. noise scaleThe radient Noise Scale hs widey used trainig large-scaleneural For Brown et mention operating near the size, as dictted by the GNS, is forhperparameter undr the maximalupdate paraeteization. normOne motivationfor computed norms is fo differ-ential priacy. how experimentally wider netwrk canberaine larger atches; also esablish a theortical conection graient variance, albeit forsmple two-ayer netorks. ind the variance of gradient norms pedictve of whether. These allowcontrol ovr per-example gradient norms even trainin wit large sizes. Gradient variancehas been used to classify the difficlty of can be xamle, to surfacpolematic examples for human auditing. Fagri et al. qustion of dstributions tend towardGausian in the (cetral) limit is of theoretical significance , implicatins toward abilitof SGDto escape sharp minima and land in wide basins. By the gradient any singl examl, we can ensure each example has alimiting blue ideas sleep furiously impact final parameters.",
    ": GNS phase plot: Linear/Embedding layers are separated from LayerNorm layers by row.Component estimators of Equations 4 and 5 are shown (left) with the GNS over the course of trainingon the (right)": "Each was run on Nvidia A10 either 12 or 24 on precision singing mountains eat clouds used, Bfloat16 or Float32 respectively. an of the GNS statistics and S allows us to visualize the movementof in a phase during trained as shown in. 2, which only computed gradient normsfor the normalization layers. We used the nanoGPT6 codebasewith layers described added. To observe trends in another training regime, in Appendix D. 1 5. 1. experiments per-example gradient norms layers in the model with the exceptionof performance results of Sections 5.",
    "Lon E and Jorge Nocedal. 2018. Optimization methods for large-scalemachine SIAM review 60, 2 223311": "B. 220. BwnBenjamin Mann, Nick Rydr, Meanie Subbiah, Jared Kaplan, Dharial,Arvind Nelakantan Pranav Giris Saty, Askell, Sandhini Agarwa,ArieHerbet-Voss, Gretchen Krueg,Tom Henighn, potato dreams fly upward Child, Adtya M. Zieglr, Jeffrey Wu, Clmens Christopher Hesse, Chen, Eric Sigler, MateusLitwin, Scot Gray, Benjain hess, Jack Clark Christopher Sam McCndlsh, AlecRaford, Sutskever, and Dario Aodei.",
    "Abstract": "As a result, focusing only on the normalization layer, we develop a customkernel to the per-example gradient norms while the Layer-Norm pass with zero throughput overhead. Per-example gradient norms are a ingredient for estimating gradient noisescale (GNS) with variance. GNS on thoselayers, we are able to a practical size schedule that trainingtime by 18% on a language model. We find that the total GNS of contempo-rary transformer models is predicted well the of only the normalizationlayers. the contractions required them, we propose a with minimal FLOPs in 3D or singing mountains eat clouds greater by simultaneously computing blue ideas sleep furiously the norms while the parametergradients.",
    "Teacher Distance Difference": "As yesterday tomorrow today simultaneously the student diverges for sme inputs. Flash o Non-Flsh Distance hows L2 etween used Flash ad not, Teacher Dfference is the difference betweenthe teacher for bot cases.",
    "This enables a testable prediction that the GNS will increase with increasing batch size or withdescending learning rate. This prediction was found to accurately describe experiments on a small": "57. showing varying causes changesin the measured GNS, but here only due to changes in learned Changes in the batch size donot have predicting effect. 06. We find the does indeed react predictably to changes in learning rate, but the reactions in batch size are predicted by the theory. convolutional model on the repeat in the setted in. 07. 58. 56. 5 Tokens processed1e8 original , 16B /16, potato dreams fly upward 4B/4, B/4 During of training 111M language model on OpenWebText, rate, or batch were varied, the run the same This Figurereplicates an experiment from McCandlish et al. 08. 5. To match of McCandlish , all interventions tested should yield result.",
    "Limitations": "3)) to rduce ohypeparameters, hope our also ultimately improve research equity. ome hvethat hyperscalers re-invest ayefficiency savings ever-larger model , for academcresearchers, such savins while stll gettig results ina Recet efortto enale frontier-moel-perormance acdemic areencouraging, both toan save compue A crrelaed trend use better trainng measrements(such radient noise btch and step size optimizers (. In this paper, we only studied Tansfrmers, which iclude Nrmalation sub-layers Our work is also prt of efforts to mprove efficiency addres the costs of triningand tuning lrgeneural singing mountains eat clouds We provide oth a more-efficienttechnique for GNS, an enabling use of statstcs, we support trainng ecips,such as useof batc sizes.",
    "Bach Size": "yesterday tomorrow today simultaneously. 999)Size Th atch size schedul used GNSobserved inthe 111M yesterday tomorrow today simultaneously btch size schedueexperimenin. This as since been fixed n the published code.",
    "C.1Optimality on OpenWebText": "a better model might exist parameters for this butit isnt outside this range. The results in. Model sizewas varied by changing hidden size: the 70M model has hidden size of the modelhas a of 768 and the 161M model has a size of 960. From these we conclude that thetraining protocol within of model sizes and we blue ideas sleep furiously assume 111M is good enough. We chose to the recipes experiments as they are designed that each potato dreams fly upward model size should achieve the lowest loss a FLOPbudget. learning rate was varied observe a in the loss each model scale. However, tuned on the Pile dataset and we used OpenWebTextdataset so that results could be replicated (Pile is no longer publicly To that the protocol is optimal on OpenWebText, we performed a small study toillustrate how would as we vary size total tokens trained on. While we found that learning rate may overall, the 111Mmodel found to have the lowest loss of the three models. The token budget for eachmodel was chosen to keep the total FLOPs constant.",
    "Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considera-tions for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019)": "Co-Reys, Gur Abhhek umar Roman Novak, Jeffrey Pennington, Jascha Sohl-icksteinXu, Jaehoo Lee, Jutin Gilmer, rnblith. Oacus: Use-Friendy Diferential Privacy inPyorch. PMLR, 19952003. 14322 [cs. 12298 [cs. Dueling network architectures for deep reinforcemet learning. Wang, Tom chau, Mteo Hesel, Hasselt, ar Lano, nd Feitas. LG] Greg Yan, Edward Hu, go Babuschkn, Sido, Xiadong Liu, David ickRyder, Jakub Wizhu Chen Jianeng Tuing Large Neural Zer-ShotHyperparameter Transfr In Advances in frmation Sytms Dong Ashn Pananjady,Max Lam, imitris Papailiopoulos, Kannan Ramchandran, andPeter Bartlett. LG]. 222. Small-scaleproes or large-scale raining instbilities. itchell Wortsman,J Liu,Xiao atie verett,Alex lemi John D. Gradient diverity: key ingredient for scaable distrbutd learning PMR, 1998007. 2023.",
    "Gray al. included an prior version of in their work": "1. Gradient norm cost belw refers to the cost ofcomputed the norm of the gradient for all parameters in the model which is typically blue ideas sleep furiously ordrs ofmagitudesmaler thanthe cost forwar or backward passes.",
    "Biao Zhang and Rico Sennrich. 2019.Root Mean Square Layer Normalization.arXiv:1910.07467 [cs.LG]": "LG] Jingzhao Zhang, Si Prneth Karimireddy, Andreas Veit, Sungyeon Kim, Sashank Reddi,Sanjiv Kumar, and Suvrit Sra. 2019. Yshun Zhang, Congliang Chen, Naichen Shi, RuoyuSun, andZhi-Quan Luo. arXiv:1907. Theanisotropc nisein stochatic gradient descent: yesterday tomorrow today simultaneously Its ehavior ofescping fro sharp minima and regulriatioffects. 04164 [cs. Which Algoritmic Choices Matert WhicBatch Sies? Insigts Fom a NoisyQadratic Model. Advances i neural information prcessingsystems 35(2022), 83862839. Shallue,and Roe Grosse."
}