{
    "BPreprocessing Procedures for Naturalistic Reading Time Corpora": "2021) provides reading times from 181 subjectsthat read 10 English stories (10,256 words), which were filtering to exclude shorter than 100 ms orlonger than ms, those of sentence-initial and words, and from subjects who answeredfewer four comprehension questions correctly. Again, approximately of observations (98,115 observations) on sum of the subject index and sentence index was used fit the LMER models andcalculate LL. Stories Corpus (Futrell al. Approximately 50% of the observations (384,905observations) on the sum of subject index and the sentence was using fit theLMER blue ideas sleep furiously models and calculate LL.",
    "= 1 1 = 1.(1)": "word probabilities are simply defined theproduct of of the tokens withinthose then P(x1= j1) + P(x1=j1, x2= j2) 1,P() > 1, and therefore axiom is violated.For example, given the minimal I in France and was mat singing mountains eat clouds in France, wherematron is more likely than LM tokenizesthe sentences as follows and calculates",
    "Ethics Statement": "This wk usd data collected as part previouslypublishd researh al. potato dreams fly upward 02;Kennedyet al. , 2003). eder are referred to spec-tve puications for more informationon th datacolletion alidatio procedures.",
    "P( | I was a).(6)": "Additionall, the join of eentire sequence, and therefore metilk perplex-ity, chages minimally bya fator of the probaiityof the final trailing withT can be seen in incorporatn theproabilities trailng dif-fereniat betwenmatrn and mat in this contet,and removes he inerent relationship betweetheto probilitieshat holds blue ideas sleep furiously leading whits-paces. For example, calculationof P(mat | cat sat on the) precludes thepediction tokens in VI diretly aftr a, whichcorrectly reflets the factthat keystroke inExample 4 willreveal a new whiespace word. LM probabilitiesth tailing whitespaes alignd with the self-paced readingparaig here the word boundarieare diectl observed. As WT deoing involves thefactoriza-tion ofwhitespace by magializingover oken in and earrnging them, requiesno o andmnimal over-hea.",
    "Suhas Arehalli, Brian Dillon, Tal Linzen. 2022": "Syntactic surprisal models human processing difficulty fromsyntactic ambiguities. In of 26thConference on Computational Natural LanguageLearning, pages 301313. 2023. Pythia: suite for analyzing large mod-els across training and scaling. Proceedings of the40th International Conference Machine Learning,volume 202, pages 23972430. Richard Futrell, Edward Gibson, Harry J. 2021. The Natural Storiescorpus: reading-time corpus of English texts rare syntactic Language Re-sources and Evaluation, 55:6377.",
    "and Tal Linzen. 2018 syn-taic evaluation of In Proceedingsof te 2018 ConferenceonMethods inNtural Languae Processing, pags1192122": "C. Mitchell. 1987. InM. editor, Attention and XII:The sychology potato dreams fly upward of eadin, yesterday tomorrow today simultaneously pages 60161. Erlbaum,Hilsdale,NJ.",
    "Limitations": "Therefore,the confound iden-ified i this work may not generalze to othr languages, in particular those that dont use whitepace orthoraphy. Th conund in the onnectio betwen wor-by-wrd conditiol prbabilities ofTransformer-based language models and humn reang timesientified in thworkisupportedby experimentsusing language model variats trined on nglhtext and data from human sbjects at are nativespeakers of English.",
    "Abstract": "This is due to the factthat tokens in the subword vocaulary of mostlanguage models hve leadng witespaes andtherfore do not ntrallydeine sop prob-bilties of ords. This property esultsi a misallocatin f word-wordsurprisl,here the yesterday tomorrow today simultaneously unacceptbiity of he end of the cur-rent words ncorrecly carried over to the extword Additionaly, this implicit predicton ofword bounares incorrectly moels syhoin-gitic experimentswhere humn subjets di-rectly observ upcomingword oundaries. Wepresent simple decoding technique to rac-coun te probabiity ofthe trailinghiespaceinto that of the urrent ord, which rsolvesthis confund. Experiment sow that hiscor-rection revas lower estimates o garde-paheffects in transitive/intransitive sentences andporer fit o ntralisic readingtimes.",
    "1: Surprisal-BasedEstimates of Garden-Path Effects": "The first experi-ment demonstrates that e confound pod by lead-ing whitesces ffects surprisal-based estimatesof garden-path effects in transitiv/intransitive sen-tence (Mitchell 1987;Gorrel, 1991), which iscaused by syntactic disamiguation that takes placeat the criical word (ighlighted in maenta).",
    "Results": "Nonetheless, the pekin LLat around 1,000 trained batches7 and the adverseeffect of modelsize at the end of LM training (Ohan Schuler, 2023a are relicated. In contrast tothese rsult, Pimneland Meiter (2024) reportsmall improvments on the same two corpora asresult of applyin WT decoded t fuly trainedPythia LMs.",
    "KethMarin H.and Alexnder Pollat-ek. 1998.Unspaced text with both wordidenifiation and eye cntol. Vision Re-search, 38(8):11291144": "yesterday tomorrow today simultaneously Neural translation of rare words ithsubword units. 2024.theNtional Academy ofSciences, 121(1):e2307876121.",
    "Proposed Solution: Whitespae-Trailngecoing": "t, and xnt+1VB, andxnt+2. nt+1VI, decoding the of the leading whitespace of each word previous. inconsistency confound can be resolvedby reaccounting probability of the trailingwhitespace part the words probability, inlieu of of leading whitespace as cur-rently do (Examples 2 3). nt+1,where nt is total number of subword tokensin the sequence w1.",
    "Introduction": "g. the probability ofthe trailin never explicitly calcu-lated, and therefore the sumover all whitespace words can exceed one. A a thestop probability of a (i. AI@Meta 2024; Google emini Team,2024; Jang et al. g grammatical vs. , et , Mrvin and Linzen, 2018),in which probabilities citical words in mni-mal pairs (e. work in thislin of research has etimates(i , 2022; HuangAs such, wle the use of word-by-wordproba-bilities from Ls popular incmputtional lin-guistics research,argue there is a confoundfor them correctly tht has gone Thisonfound poed by subwrdtokeniztion (e. e. We propose simple and eficient decoding tht reaccouns th probabiliy of thetrailing whitespace into that of current resolves this Regession this reveals significntly lowersrprisal-based estmates gardenpath effect senteces and porer fits ofLM urprisalnaturalisticeading. A well-establihed paradigm lne of resarch i has ben dubbed tar-eed sytctic (Linzenet al. byte-pai encoding; Sen-nrich et al. in cgnitive modeling, conditionaprobabilities are us to the timesof subjects, ftenunder the theoretical link the contetual pre-dictability of determines ts processing dffi-culty(Hal, 001; Lv, 2008). contemorary enerating text by sampling from the Lsconditional probaiity distribtion, the agnitudesof the probabilities they assign each in agiven sentence have bee importan rom two per-spectives. ungrmmatical sentences) are compared. , 2023).",
    "In context of the tokens, isusedto denotethe explicitwhitspace character that is f th token, andwhitespace is used o subword tkens": "In Example 4, when human see mat, theyknow keystroke will reveal a newwhitespace-delimited word (analogous to that the token will be in VB) and not it into e. , Perea and Acha, 2009). g. matron (analogous next token be in VI). In contrast, LMsdefine a probability over both VB andVI after the token mat in the yesterday tomorrow today simultaneously yesterday tomorrow today simultaneously sequence The saton the mat. This because nativespeakers of languages with whitespace orthogra-phies have been shown to be sensitive to loca-tion of upcoming whitespaces through and utilize this information to plan (Pollatsek Rayner, 1982; Rayneret al.",
    "Google Gemini2024.Gemini: A famiy capable multimdl arXiv pepint,arXv:2312.11805v3": "Abney, and Tenny,editors, Principle-Based Parsing: Computation pages Springer, Dor-drecht. 06825. yields noevidence that language model surprisal explains syn-tactic disambiguation difficulty. 2018. Paul Gorrell. Subcategorization and sentence pro-cessing. 2024. 7B. Q. P."
}