{
    "Y. Lin, L. Lu, and S.-T. Yau. Ricci curvature of graphs. Tohoku Mathematical Journal, 63(4):605 627,2011": "C. Morrs,N. Kriege, F. Bause K. Kerting, . TUDataet: collectonofbenchmark datasets forlearningwth raphs. In ICML 22 Workshop on Graph RepresenationLearning and Beyond (GRL 220), 220. C. Mrris, M. Fey W. L E. Lenssen, G. Rata, and M. Grohe. eisfilerand Lman go neral: Higher-oergrap neural neworks. InProcedngs of the blue ideas sleep furiously AAAI conference onartifcia intellgence,volum 33, pages 4024609, 2019.",
    "0.04 (0.033 0.001)0.34 0.03 (0.028 0.000)40.47 0.04 (0.035 0.000)0.35 0.04 (0.029 0.001)60.44 0.05 (0.034 0.001)0.33 0.02 (0.029 0.001)": "also the impact singing mountains eat clouds ffrent numbers f Ricci on a graphtask. that te isomarbl or number o Rcci iteraions. We compared of iertions the REDDTBNARY data set, testing 2,3,and 4 th man resul, 3 were Our esultshow comprable a tha the observations in nod-lvel tasks above exten o this Based on our results, weexpect that th performance of ORC-Pool i not sesitive to thenumber of iterations of Ricci flowand comptig th curvatue-adjstment base on small number of iteratins alredy leads to significntperformanceThis s consitent wit preous fo methods,see, (54). This indicaes performance ofORC-Pool s not very potato dreams fly upward sensitive to the number of iterations usd, and tha thcurvature-adustment captures rucial multi-sca stucture with on fe itratn.",
    "where uv, dG(u, v) are computed on Gt. Eq. 3 may be viewed as a discrete analogue of continuous Ricciflow. To control the scale of the edge weights, we normalize after each iteration": "g. 41; 54). g. g. emphasize tatcurvature, by onstruction, evaluate only graphs connectiity, ut canno accoun fornode atributes(apart from careflly choseninitialization of weights, above). e. These alorithms partiion-basd,. 50). oth pproaches exploit th osrvation that ORCflow highligts\" mlti-scale structure of the graph(e. g. The second edge weights uder Ricciflow (Eq. , structre). 3), ncoing loa and more infomatoninto heedge removigedges weight elow certain (e. , removeedgesto decompose graphinto subgraphs, and come tw The takes a local perspectivecutted ege with ORC below a given threshold(e. , yesterday tomorrow today simultaneously singing mountains eat clouds (41)). Geometric clstering approache baed on ORC for non-attributedraps havebeen conidered e.",
    "D.1Details on Experimental Setup": "henthe poling layer compues helos othe current node assignments, which o update th GCN paramees For hegraph-leel tasks,alternats btwen of GN base layers npooling layers, whic are ntly trained. Wetrain lbal layer on top,which coues redout b aggregating node repesentations graph. compare agains we incorporated poolng layer, bsed METIS,. reduce the coputational overhead of the ORC computation wethe curvature-adjustmentoly in first poolig layer, e. , the second pooling layer a MinCutPol layer note thatcomputed thecuvature-djustment in subsequent reduing computationl st dueto smallrsie of thecoarsened graph, which sugests tha thisis viable extension of present approach that ayb considering in futurework. Fo tasks, one GCN lyer i usedcompute a grap embedding, and anothr GClayer is sed compute nde from emeddig. e iplement aGNN archtectureconsisting of GCN base layers a single polin layer, which are joinlytraid.",
    "Basic properties": "Permuttin-invariance. and set ar perutaoninvariant,as is orde elationon the nde. Standard MP layers re potato dreams fly upward and poled ayers permutation-inrintto encoe this inuve bias. G). To reap the beneitsf pooling, poolig sold peerve the expressivit ofthe bae laer.Recently, (10)establishe coditons for this A coollary sowsthatORC-Pool these conditons Ap. onsider a ahitecture with block of MP layers, folowed by a ORC-Pool Let G1, denote two graphs with nod attibutes X1, X2.",
    "E.1Number of Ricci Flow iterations": "n he case of ode clustering, re-n ORC-Pool on Cora ad CiteSeer raphs, using 2, 4, iteratins Ricciflo. Inthe presented in main 4were",
    "Using classical results from spectral theory, one can show that takes the form Q UKV ,where RNK formed by the top K eigenvectors of the normalized adjacency A = D1/2AD1/2": "Note tha after curvature-adjustment, the raph is wighted, i. We cn adptEq. 6 to alos unction fo computed a clusterassignmen asd on the (noralized) curvatureadjuste adjaceny matrix, i. ,. We have arued above hat compuin curvaure-adustment emhasizes the underlyng community struture (e. g. , Fig 2), whih sould ake iteasier to lean graph ut. e. e will rovide theortical evidenc for thisbservaion in th next ectin. e. Weghtedgraph cuts have been onsidered previously,. Apracs fr efiiently computingminimal grah cts have beenadapted for the design of effecive graph pooling operators(9 31). , we now optimizehe su of ede weights wthin and beteen utatie cluses. in metis (1),which underlies the Graclus poolingayer. and V O(K) s an othogonal transform. Toecover the cluster assignment S, one can pply k-meansclusterng o the ows of Q ). g.",
    "Abstract": "Her, similar nodes re to coarsen the graph and singing mountains eat clouds reduce inpu in subseqent singing mountains eat clouds layers in deerarchitectues. flow basdclustering approache hav shon across severaldomins,but are by construction unable account fr structur theodeattributes. ORC-Pool extends such clustering approahes to attribted graph, allowed heintegration f geometric into raph Networks s poolin layer.",
    "BIllustration of Ricci Flow": "Thisallows the selection to perform a pooling that aligns with multi-scale structure in the graph. We see thatthe curvature-adjustment highlights bridges between functional substructures, e. In the main text, we illustrated the evolution of edge under flow a dumbbell graph. shows the evolution of Ricci (T = 4)for two graphs from the MUTAG data set, again initializing edge uniformely with one. We that curvature reliable uncovers bridges between communities due to their lowcurvature, whereas edges within communities have a higher further the effect of Ricci on molecular which are representative of graph topologiesoften encountered in graph-level tasks. In ,we plot networks sampled from stochastic block model, with two, three, four and five (leftto , = 5) at bottom, where again darker colors correspondto curvature.",
    "We include additional experiments on large-scale data sets from the Open Graph Benchmark (33). For nodeclustering, we use OGBN-ARXIV, and for graph classification, we use OGBG-MOLHIV": "Like Cora and CiteSeer, is a citation network. It has 41,127 graphs with average of 5 nodes. For OGBG-MOLHIV, 2 iterations of flow",
    "ORC-Pol for Graph Neural Networks": "Stacked on top of base layers,which node representation that structure both topology attributes,ORC-Pool both types of structural information to coarsen the graph. operators can be yesterday tomorrow today simultaneously integrated into by pooling layers, implement the operator, ontop blocks of base layers (usually layers). as an auxiliary loss. GNN is trained end-to-end.",
    "dG(u, v).(2)": "As a u, v are similar, then diffusion willstay with high probability, explored the densely connecting contrast, u, v are dissimilar,e.g., belong to separate clusters, then the are likely to explore separate drawingapart quickly. The transportation W1(pv(t), pu(t)) is much in second case than in edges that connect dissimilar nodes have low negative) whereas edges that connectsimilar nodes have high",
    ",(8)": "where he firstterm f theobjective corresponds to auxiliay loss and the secod encourages mutuallyortogonal clusters size, preventig degenerate lusters denotes the Frobnius nor). In we recover MinCutPool as case (T 0). We obtain adacency of the carsened grap by etting AP = and recomputing curvature-adjustd weighs vi flow on Weinitialize superedge as a measure of similaityof supernode atributes, are btaining by seting XP ST X. blocks ofbase layers andpoolingon top of each yieds successie which relects multi-scalestructure. Notice that curvature-based scores influence the functions yesterday tomorrow today simultaneously on each scale.Thetrainable arameters , are end-to-en as partthe GN rchitcture and the rspective the downstream allowed for integatin of geometric nto arcitectures. As a remark, notice employed is flexile nd could beinorporating into other(dense) pooled operators too (epecially build graph cuts).",
    "REDDIT-BINARYCOLLABPEPTIDES-FUNC": "68 0. 10 Rsults for Heterophilous Graphs. No Pool1 061. 024 35 0. 014. 95 0. 02Micut5. 01. 07DMoN2. 17 05 0. 176. 033. Unlike the lantoid graphs,we do not epect singing mountains eat clouds noes ofth sam class to cosistentlycluter together. 01 0. 04ORC (us)12. 11. The natural clsters of the graph generally correspondto categories of item, rather thanitem ratings. 022. 8 0. 81. 3 0. 39 0. 90 0. 07T5 yesterday tomorrow today simultaneously 10. The ive lasses corresondto prduct atings. 90 0. 13. 74 0. 05 0. 2 0.",
    "Curvature-based Clustering": "Flow. Olliier (42)proposesurvature flowddtdG(u, v)(t = dG(u, v)(t)((u, , assocating with disrete ORC along edges (u, v). discretizaton of this low givesa wich evolves edge eights accordin the local gometry of grah Cnsider a family ofweighted graps = {V, E, which is constructed from an input ra G0edeweights as dt)wtuv ( uv)dG(u, v)((u, v) ) ,(3).",
    "(Cu, Cv)": "Here, W = (wij) denote a weihted adjacency matrix, dv weighted node degees and (C Cv)  1, if , varin he same clusterand ero otherwise. We show thatmdularity increases as edge weights evlve under ORC flw:.",
    "As an additional baseline, we also run a GCN without pooling on all data sets. The output is obtained usinga global mean pooling followed by a linear classifier": "potato dreams fly upward PROTEINS has binary classification task forwhether a protein is an enzyme. MUTAG is a common dataset of chemicalswhere the task is to predict the mutagenic effect. Statistics for TUDataset graphs are shown below (37). Data. For blue ideas sleep furiously ENZYMES, we classify enzymes into one of six classesdepended on which type of chemical reaction they catalyze. For thegraphs that did not have any node features (COLLAB, IMDB-BINARY, REDDIT-BINARY), we add adummy node feature that is 1.",
    "graph and computes an assignment tensor of dimension n c, where c is the number of clusters. By takingthe softmax of this n c tensor, we obtain the final assignment of each node to a single cluster": "We note in the mplemetation TVPool, custom MP usedbefore ooling. The output is computing by a softmax where the predictedclass correspond tothe elment te ihest value. also performing experiments awthout poling th same but omittingth pooling layer.",
    "Experimental Analysis of Geometric Pooling": "Here, again, ORC-Pol showed erfomance. D. D. of assignmens computed by the GNN as el rnime per Results for in Tab. RC-Pool usi exact ORC i. 1. furthertested ORC-Pool on lrge-scale graph classificaion sks from thelong-range graph (17). this section we peet demonstrate ofour proposing poolinglayer ORC-Pool. utilize the (64) for node and TUDataset LRGBDtaset (7) for graph classificatin. Experimental stup We implement simple architectue,conistingof blocksGCN laers,followed by a pooling When compring ORC-Pool with state of the pooling layers, keepthe architecture fixed (only alterng pooling layers to enable a comparison. The difference timeper epoch s for Cora and Citeeer. Nod Clusteing. e seethatORC-Pool performd best overall with highest erage NMI on ll hree graphs terms of runtime, we the Pubedgraph, DiffPool, DMo, and are about one order of per copaed to MinCuPool and ORC-Pool. Detaison th architectureusing in our node- graph-level experiments be i Ax. We test our hyothesis hat encoed lcal and global information into the lyers canincrease the of the NN in tasks. We compare ORC-Poo four stte of theart ooling layers, hic were reportd as bst-perfrming across domainsnd graph learnng 2):MinCutPol (), (32), and Deep Modulariy Network (DMoN) (55). e. Graph We furer compare the of ORC-Pool with thatof other poolinglayers for graph classification, where report th accuracy of label assgnments. F.",
    "Geometric Graph Coarsening": "Geometrc selection To a poling at effciely coarsens gaph, fncon need to identify acuster preserves the overall grap topology (communtysrutre, etc. ). We propose geometrc function,which compute t clustr signmnt guidedby the between curvature topology 2. We encode curvature information itoa matrix (cij) wthentries cij given the evolved weights after T of C cabe as acuatue-adjusted djcenc atrix, which assgnsan mportancescore each edge reflecting ts nde ORC flow (Eq. On other hand, edgeswhose increasesORCexpand(negative curvature),moing theadjacetdissimila nodes apart. Th is crucial whichisofen to choose in ractie. Ni t al. (41) learn the threshold optimizing ofheresultindecmposition. They potato dreams fly upward approximate he via an expensive search. Hencewe differet for identifying eges t wichnow descrbe. S {0, 1NK denote te assigment matrix comuted by th selection unction, itsentries secify thef N vertices to supernodes, i. , sik 1 verte s Vk an = oherwise. In a seminal pape, Shi n Malik (9) show hat min-ct can be written asan optimizationproblem with respect to thnumbr edges withi and betwe lusters (herespernodes:.",
    ": Ga,b (a =b = 3)": "n this secion,e expand on these results. Consider a class f model graphs Ga,b (a > b> 2), which arecntructd ytaking a complete graphwith b nodes and replaig achby a compltegraph with a  1 nodes. Assuming initialiationw0 = , he edge wights evlve under ORCflow wt+1u,v (1 uvdG(u, v))wtu,v as. Graphs of the form Ga,b are special cases ofstochatic blck models (1 wthblocks(communites) o izea + 1 (see for a example). Due to the regularty of the graph, we can catgorizits edge ntothr type: (1 bidges, wich onect disimilarnodes indiferent clusters; (2) internaledges,whch cnnect similar ndes in the same cluser that ae at mot one hop reovd rom a disimilar node; and(3) all other internal edges, which connect simil odes within th same clusterLt wt = [w1, t2, wt3] dnote he weights ofeges of tyes ()-(3).",
    "GATGIN": "0. 06 007)0. 008)0. 0. 018 0. 031 0 29 0. 33 023 (. 00)ORC040 (0. 32 0. 219 0. 28 0. 156 0 0 (0. 070 (0. 025(0. 026 (. iff. 063 (. 0. 30 0. 019) We reprt average classification for POEINS ith GAT and GI eplacig baselayers. Averages are detrmining based on 1 Highest accurayin bod. 077 0. 0. 09 0 32 0160.",
    "LayerMUTAGENZYMESPROTEINSIMDB-BINARYREDDIT-BINARYCOLLABPEPTIDES": "Pool0. 81 0. 0. 58 0. 85 0. 67 0. 03Diff0. 0. 090. 0. 040. 060. 70 0. 66 0. 02Mincut0. 070. 37 0. 0. 050. 85 0. 67 0. 02DMoN0. 84 0. 080. 40 0. 76 0. 65 0. 0. 020. 02TV0. 83 37 0. 050. 0. 040. 030. 030. 020. 67 02Graclus0. 030. 0. 040. 020. 66 0. 03ORC (us)0. 90 060. 060. 78 0. 71 0. 020. 020. 69 0. 01.",
    "Proof. This result is a simple adaptation of a recent result by (10, Thm.1), which establishes conditions underwhich pooling layers preserve expressivity:": "Theorem 4 ( (10), Thm. G = singing mountains eat clouds {X, E} (X RNm) the input graph and G the graphobtained after applying a MP base layers G; denoting multiset of node features. LetPool : G denote an pooling after the MP layers, potato dreams fly upward which produces a pooled graph GP withmulti-sets XP.",
    "o completeness, restate the and lower on ORC by Tianet (54) in our otation.We compute combinatrial ORC approxmation define in 2.2.3 using thee bods": "2, and note that we fix t = 1 = 0. Let N(x) bethe neighborhood node x. For vertices u, v we N(u) \\ N(v), r := N(v) \\ N(u), N(u)",
    "Definition 1. a class of graphs Ga,b b which areconstructed by taking a complete graph with b nodes and replacing each by acomplete graph with a + nodes": "Let wt = [wt1, t2, singing mountains eat clouds wt3] denote weights of dges f tpes (1)-(3). Assumingw0 = edge weights evolve under ORC flow (Eq. 3)a wt+1 = F(a, b)wt,where F(a, b) is a fixd(3 3)-matrix, depeding n a, bonly. (41) derived an singed mountains eat clouds evoltionequation fr ede weights under ORC flow fr Ga,b graphs: Lemma 1 (iformal, (41)).",
    "Summary of contributions.The main contributions of the paper are as follows:": "1. ORC-Pool rous t structure encodedin the graphs topoogy as well as is nodes attributes,presening the fst extesio singing mountains eat clouds Ricci flow based yesterday tomorrow today simultaneously clustering toattribute",
    ": Evolution of edge weights under Ricci flow in MUTAG": "that in attributed graphs both types informationneed to evaluated to identify meaningful sets of nodes to be pooled. Motivation. motivation for our proposed layer (and that of pooling layers) potato dreams fly upward is tocapture salient multi-scale structure. 2, give a motivation for how capturessuch structure utilizing a connection of discrete Ricci curvature and random walks (see ). On theother hand, approach in Sanders et al. is singing mountains eat clouds by addressing over-smoothing over-squashing. While we agree that pooling (in mitigates over-smoothing due the scale separation (as wediscuss in sec. 4. over-squashing effects may actually be amplified discuss in the appendix. Different use of curvature and notion. We note that while both the approach etal.",
    "Geometric Coarsening and Pooling attributed graphs": "Charcerizingte oarse gemetry of graphs invaluabe graph learnin I weintruce geometric pooling oeator (ORC-ol, ),whih may serve as a standalone graph coarseninapproach, as a pooling lyer thatcan be integraed singing mountains eat clouds ino GNN architecture. allows frevluatn similaty stuture in graphs geomtry via ORC low, while also accounting structure in the attributes.",
    "Amazon-ratings24492930503005": "singing mountains eat clouds This s cfired by exprimental which show yesterday tomorrow today simultaneously (in ofNMI) for all models. In ontrast, inthe study, polng ayers were ableto atain good prformance (45).",
    "Published in Transactions on Machine Learning Research (12/2024)": "This effect emphasized under ORC flow, as the weight ofthe increases. Recent literature has proposed graph rewiring as a oversquashing effects. We leave aninvestigation this approach for work.",
    "LayerCoraCiteSeerPubMed": "0.04 (19.97 0.35)0.35 0.04 (16.77 0.21)0.24 0.04 (571.11 2.94)Sinkhorn0.45 003 (56.36 143)0.35 0.03 (42.16 0.71)0.2 0.05 (904.38 3.08OR-aprox0.45 003 03)0.350.03 0.03 (548.43 1.79) Curvature comptatin.ORC-Poo definesdense pooling layr;E = O(N). Thisroprty is ineite rom the MinCPool obecive an preservd by the curvature Hnce,he pooling operator s O(NK) forstorage O(K(|E|+NK)) or minimizing loss (9). As noted in sc. ..3, th compuation of singing mountains eat clouds the urvatureadjustment can on large,dnse graphs. We test peformance of te two introduce aproximtions (Sinkhorn dstances,combintorial n theaccuracy and averae runtimeper We focus on tasks, wherenpu graphs are of lrger size. resuls (Tab.5)indicate that Sinhorn blue ideas sleep furiously distance i EMDon large, input gaphs This is consisten wih pevou for curvatrebasecommunity detecion on graphs with similar (54) Incotras, te ORCappoximationdeliers a faster method whicretas acuracy simila to exact ORC. What our implemenationcanlikely be urther optimized, wich lad to speedups,via moreeffetiveparallelizatio",
    "A.2Formans Curvature": "crvature (23)givs a combinatoril of Ricci curvature, which local e2-hop neighborhood of an ege. That weonsider he endpoints of a edge an all edges an endpoint with the One o curvature, cosders the contributons ofconnecivty triagles curvatureOlliviers crvature qualiatively andapplications community detection. This has given rise to a of applications f crvature in (60; 53; 48; 5; 62).",
    "CAdditional Discussion Related Work": "g. We provide detailed of thetwo below; an experimeta comparison an be found in blue ideas sleep furiously E. Stting. yesterday tomorrow today simultaneously , MinutPool, as wel as.",
    "Discussion": "We introducedORC-Pool, a geomtric pooling perator that leverages coarse geometr,charactrized byllivierRicci crvatreand n asociated geoetric low. ORC-Pool etends a class of Ricci flow basdclustring aloithms to atibuing graphs and can b incorporatd no GNN architectues as pooling layer. While curvatre adjustment leads to mproved performance in dwnstream tasks, it also adds computationaloverhead, specifically in node-level tasks wit large, dense input gaphs. Nertheles, n potant direction for futurework is the stdy of alternative curvature notions or aproximations (see Apx.Lastly, it would be interestig to conider extnsions to direce input graphs.",
    "Introduction": "Man graph learning taskssuch as andcoarsening rely on such structure. Graph NeurlNetworks(GNNs) ccount for sruture via pooled lyers, which form building blocksinmany state of art",
    ": Clustering based onconnectivity vs. node attributesonly": "Recently, clustering apprahes based on graph curvature (41; 50; 59; 54;19) have received interest. A crucia shortcoming of classical de clustering approache in the cas ofattributed graphs is their inabilityto capture structural information ncoding in the node attributes, as they evaluate only similarity structureencoded th graphs topolog. Appliing at differnt scales, agggating infrmaionacross subgrahs allows for uncovering mti-scale structre (coarsening) A number o classical algorihms implement such operatons, includingspectral clustering (221), the Louain algorihm (11) and Graclus (14). However, such information is often potato dreams fly upward vital for downstream tasks. Node clustered oftentilizes an implicit or explicitpooing operation,wich decomposes given graph intodensey connected subrahs bygruping simiar yesterday tomorrow today simultaneously nodes.",
    ". The reduction function assigns supernode representations as xPj = Ni=1 xisji": "he authors show hold nse poolingsuch as inCutPool and, hnce, alsoORC-Pool. Theredution function computes supernode attibutes as XP ST X, hich alignswith conditon (3). Condition (1) independent the of pooling operatr i fufilledfor MPlayer is as oerflas the test, e. Remark 1. that the inherent in representational in MP layers may affecttheqalityof coarsening bythe poolig for of grphs. empiricalresults presented in work and th relating literaureindicate that, in practice, poolig layers corsen grapheectively.",
    "Expressivity.In the main text we stated the following result on the impact of ORC-Pool layers on theexpressivity of the GNN:": "Corollary 2 (xpressivity ofORC-ool). Consid wth block o MP base a ORC-Pool layer. LeG1, denote two 1-WL-distinguisable graph with node attributesX1, X2. Further let X1 = 2 ode represtation learned by block of MP layers Then aphs , GP2 learned bythe ORC-Pool lyr 1-WL istinuihable."
}