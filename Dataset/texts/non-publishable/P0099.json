{
    ". Two examples of ObjectNLQ on the set ofGoal Step": "b presentsa where our model did not perform as expected. Whileit correctly identifies the object scene, it fails to localize the answer, particularly because the groundtruth drill to be centering frame.This suggests that the model may not adequatelyunderstand or human attention indicating area for further to enhance its in scenarios.a shows a successful example model locates target of the text descrip-tion, a sequence including takingmilk from singed mountains eat clouds fridge, pouring into a container, and heat-ing milk with a microwave. case reflects our modelsability identify the location intent as well granularity.However, an example of underperformance is b. Our model incorrectly adds removal ofmilk from refrigerator to the results, result-ing in localization",
    ". An illustration of object extraction": "effectively data varablity.ap-proaches ptimizframework by the numberof frames streamliing vide momentlcalizatio. eistn pretrainigmethods commny finetune bakbone using between cptins vdeo whch eans thevideo features depend entirey onth granularityand focusof caption information.",
    "Chen-Lin Zhang, Jianxin Wu, and Yin Li. ActionFormer:Localizing moments of actions with transformers. In Euro-pean Conference on Computer Vision, pages 492510, 2022.1": "1. 4 Zhang, Meng Zixin Liu, Xuem Song, YaoweiWang, and Liqiang Nie. Span-sed localizn ntwrk for aturallanguage video lo-alizatio. Hayu Zhang, Mng Liu, Zan Go, Xiaoqiangi, YinglongWan, and Multimodal dialog Rela-tioal graph-ased question 1Haoyu Meng Liu, Yuong Ming Yan, Zan Gao,Xiaojun hang, Liqiang Nie.",
    ". Performance Comparison": "15%, which 1. 22% higher than 2023 championand us 2nd-place ranking. details for the Goal Step Challenge, where our ensem-ble attains an of 33. an IoU of0. 3. This score is 13. 96% points the baselineand placed us third. results underscore the effective-ness of method in both",
    "NLQ task concentrates on building an AI helper for findingthings that people often neglect. This mismatch leads to thelack of fine-grained object information in the input": "hen we use a parllel attentin mecha-nism to infraton and t vide frameeatures, to reasonblyomplete the NQ ak. Basing on this observatio, we propose a noel methodforobject extraction and design a novel gouning fully the oject annotation.",
    "Abstract": "ths report, pesent our approachfor the at-ualLanguage track Goal Steof theEgo4D Eisodic Meory encmarkCVPR 2024. Bothchallenges require thelcalization of within longvideo seences sin textual queres. To enhane ocal-ization our ethod only processes the m-poral informaton of videos but alsidenifiesfinegrainedojects spatially witin the singing mountains eat clouds frame. potato dreams fly upward o this end, weintro-duce novel appoach, termed jectNLQ, which incor-orate n objet branch augment the vdeo repres-tation detailed object therebyimprovinggrouning achiees a mean 1of 2.15, rnkin 2n in the NaturalQueriesChalege 3.0 in terms o th metric R@1,IoU=0.3, 3rd in te Goal Challenge",
    " Featre Extraction": "Followed we eploy aconcatenated featre compis-ed InternVideo an EgoVLP for video while CLIP (ViT-L/14) forextualfeature extraction. Subsequently, employCLP (ViT-L/4 the features ofhe classes that orrespond the gven query. blue ideas sleep furiously",
    "In this report, we present our approach for the NLQ trackand Goal Step track of the Ego4D Challenge at CVPR 2024.Our primary contribution lies in leveraging existing ob-": "Weili Xuemeng Song, aoyu Zhang,Meng Li,Chung-Hsing eh, and Chang. 2 Hou,Lei Ji, Gao, Wanju KunWin-Kwong Chan Chong-Wah go, Nan Zheng Shou. 9529202. Employing fine-graind ob-ject features,ourmethod mprve upon the foundationalGrundLQ model in Chen, en Yi Wan, Kuchng Li,Yzho L, Yi Jiahao Wang, Yin-Dong Zheng,BingunHuang, et al. preprint 12552023. ect detetion modes to etract detailed ojectvideo frames and integratin this data into an efec-tive model framework to enrich representationiththis additional object tail. n of the 30th InternationalConference Multimedia, 3 Agrim Gupta, Piotr Dollar, and Ross Girshick. Bi-directionl het-erogenous gaphahing towards efficient outfit rcm-mendation. In of the IEE/CVF Conference on Compter Vision Pattern pag 1899519012, 222. In Pro-ceedings of the IEEE/CF Conference Computer Visioan Pattrn Recognition, pages 5356534, 2019. arXivpreprint1, 2. 1 KistenGrauman,AndrewWstbury,EugeeByre,Zachary Chavis, Atnino Fnari, ohit Girdhar, JackonHamburger, HaoJiang, Miao Xingyu Liu, et Ego4d:Around in ,00 of egocentri video. Lvs: Adatase for lare ocabulary instance segmentation.",
    "ObjectNLQ333.0026.37": "Simialy, object branch inthe muti-modal encoder initilizing usin he parametersfromthe ext ensuring and levragingpretrindeficiencie the odel archtecure. This datais use train multipl models for an ensmble approach,enhncing obustness and accuracy. Wihin he objet encoder, text cros-attention ly-ers FFN ar initialize usig th MHA thFFN from e ecoder.",
    "ObjectNLQ227.0219.2823.1543.6630.87": "text branch, an objectbranh and a gate fusion modue. This innvatie design seamlessly combinesobject dta with video features, mantainng the interityof theoriginal interactions btween textand video conten,thus nhancin overall models capbility t interpretand utilize multimal data. Subsequently,theoutputs from the two branchesare cmbned usig a gate fusio technique. Th intgra-tion is achieved through a gt fusio blue ideas sleep furiously rocs, where theweihs for merging are computing by multi-layer ercep-tron (MLP). Within each branc, a cross-attention lyer and FFN areemplyed.",
    ". Introduction": "These meh-ods ineune a pre-trained video modelon theEgo4D dataset to featur representation for videos. Both challengesecessitate precise identification of revan video on extual qeries. Existing methods made significant key areas: 1 Developng  feature ex-tractonthroughpretraining. 2 Implementing efficient augmen-tation to expand the scale dataset. Ego4D Lanuage Query (NLQ) Cal-lenge, are providd with an egocentric videoand a naturl laguage question The objectiis to acu-rately lcalize th video segent contains answer question."
}