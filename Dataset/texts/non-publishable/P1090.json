{
    "Topological Natural Analysis": "Moreovr, idtifyng toics involves changeshe volme topi btween imeindows, conentional clusterig metods cannot determie yesterday tomorrow today simultaneously whther lters in diferentdatasets related the toics. It is ven fo an emergng topic to e preent inthe time window appearng i previous widow. dditionally, conven-tional methods may treat emerging topics as noise due theirmuchsmaller vlum topics. To overcome these challenges, we popose amethod robust to yprpa-rameerselectionand qantitatiely dtec othemergingandtrending topics between singed mountains eat clouds differenttime windows.",
    "Experiment": "Varing te hyperparameters nlythe sizes of the clustersand idnot significantl ae the e discovered, deonstrate te of thetopology analysisapproach. 1 of th appendix provides of entences frm the top-1 trending a emerging cls-ters of Tablt i Februry 223. This fiter factor set 10 th maximumnumber o C+ vales, andit to exclude sma clusters tat weeunlikely to significnt. The factor helps to focus the outliers, andthe nodes he longtail are smilr, ith high-scor noes unlikely to bcomelow-score odes due to changs hyerparametes As a the neihbos of ahigcetraity node also tnd to have centrality, as shown in T loating thesame cluster multple ties, it necessar t nure two are suficiently far aart. Table. We observe that emrgig score tributioncan eparate itoaaussian-liedistriution arund zerdue to thefilter actor appliedto odes , ada ong-tail regindue to nodeswith. We repeat hs we have obtaied heesired number of cluters.",
    "The Dataset": "worth that the dataset only contains customer text data, with all confidential such as names and account details, to protect privacy beforebeing shared with researchers. the dataset from a specific database, the methodologypresented this article applied to other use as well. Each contact also comes with a unique label of or that customerand agents discussed. There been a lack publicly available datasets to customer transcripts. Wecollected over 500,000 contact transcripts dured 2022, recording conversations between customersand agents.",
    "We AmazonsDigital & Device Forumand confirmed thisissue has been widelydiscussed,andAma-zons engineering teamhas addressed it in thenext": ": The tle peets seection of emerging issues identified n February 2023 catgriedby product line. The first colmn dsplys correspnding queston entences that reresent thecluster center node. In the secnd olumn, we summariz the topic f each cluster.",
    "Attention(K, Q, V ) = softmax(Q Kdk)T = V(3)": "where Q RNasNse is a tensor sentence embedding), K RNse a vectorand RNasNse is a tensor. We generally define = V without impact on modelperformance. As a result, the attention value is a vector and sum its elements mustequal 1, due to application of softmax function.",
    "Conclusion": "Colobert,. Our wok makes signfican contribution bydemonstratig applicaton of a sentence-levelattention mechanism in coversational trnscripts, an area that hasbeen undrtied. Fnally,we pply topoloical naturallanguage analysis methos o analyze the centaity of each question,enabling us to identify trendingand emerging issues. In summary, wehv presented a unique machine learning ramework for extracting customrstrending and emerged issues Our work starts with an attenion-based eep learning model that tagscustomers prmay qustions and generates corresponding sentence emedngs siultaneousl. We ombinethis mechanism wit topological data analysi to etract usefulinformat for areal-world problem. (2008). A unified architecture or naturallanguage pocessing: deepneural networks with multitask larning ICM 08 Pocedings ofthe 2th tenatioalcon-frce on Mchne leaing, 81:160167. and Weston, J. We th ransform the seence embeddings into an sotropic coordinate ystem used witeningtechniques to improve the cosine similarity performance.",
    "E1+Q2E2+Q3E3+Q4E4+": "ote that Qi is equivalentto Vi. The posiin mbedded ectorE iscombining with Qi to create he sentence mbeddings Qi. (b)Te red block in (a) is decribed in detail. Asshown n , each sentene ina transct is reprsented as a vector V i (he ith row of th 2Dtensor V ). : The Sentence AttentinModel. ( The orage block in (a) is explained. The attenion weights i reflec theimportance of each sentence in determining the product r service, with higher weights assigning tsentences contaned more critial information. Tensor notation (S1; n) refers tothe n-th token i sentnceS1. model consists of blue blocks, represeed ten-sors, and green blocks rpresening operators. (a) The neual networkis comprise o sentenctensors, Si, ad aseuecemodl, , whch outputs i. e then passthrou auly connected layer with a softmax acivation functiono redic the roductor serce associting wih th transcript. Fially, a lieaclassifier predics heprodc/ervice.",
    "Undirected Representation Graph and Centrality": "It s important note singing mountains eat clouds graphs constructed using daa time periods. Let us assuethat have gather prmary customer singing mountains eat clouds quesons their orresponding enteeembeddings, z, over two perods, T0 andT1.",
    "Setence Embedding": "Our goal is to identify the primary question sentence in a contact transcript. Wepropose two hypotheses: (1) question sentence typically appears the first few sen-tences the customers interaction with agent, (2) it contains most relevant informationabout the or service beed these hypotheses we treat the problemas a learning identifying the sentences near the agents responsethat are most in predicting the product or service of a machine learningclassifier. To this, neing attention weights sentence level. We propose a deep model, as , to achieve our goal. Unlike traditional textclassification models that an article as 2D tensor RNatNwe, where Nat is in the article Nwe the dimension of word embedding, our representseach article as a 3D Nas is number of sentences in the article,and is number tokens sentence. To numbers of sentences andtokens per sentence in each transcript, we use zero-padding to consistent forsubsequent processing. To obtain sentence-level embeddings, we treat each as temporal slice and apply time-distributed wrapper a model , as BERT or LSTM. ensures that the modelreceives one sentence per time step, us to embing each The resulted outputtensor, Q RNasNse, contains vectors, each with Nse Our bag of sentences model sentence but we observedthat customer questions tend to appear in early sentences dured agents. Thissuggests that positions can impact attention weights, so incorporate sentence into the model. To do this, we adapt the idea of position embedded in many language models for tokens, butapply it to sentences. assign each sentence index, ranged -Nas +Nas, representingthe number of the sentence first For index of -5 the sentence five steps before the agents sentence, while +5indicates five steps after. We shift by Nas, resulting in an allowing index range 0 to+2Nas, with the sentence having an index Nas being the agents first avoid For a sentence with i, p-th component of the position embedding vector by Ei(2p) and + 1):",
    "Abstract": "companies deal with a high volume of customer service requestsdaily. To tackle this we propose a novel machine algorithm that leverages natural lan-guage techniques and data analysis to monitor emerging and trendingcustomer issues. From there, we definetrending and emerging issues based on the topological properties of each We have validated our through methods and theyare consistent with sources. Our involves an end-to-end deep learning the primary sentence of each tran-script generates sentence potato dreams fly upward embedding vectors. then the embeddingvectors use to construct an undirected graph. While simple system often used to summarize the topics ofcustomer contacts, thoroughly exploring each issue can be presents a critical concern, especially during an emerging outbreak must quickly identify and specific issues.",
    "My daughter accidentally broke the screenof her tablet for kids. I was wondering if itspossible to file a claim to have it repaired": "similarityamog sentences in the rndng and emerng topis extracted am clustr suggesstheeffectiveness of our method. : I our analyis of Fire Tblets top-1 trending and emerging in 202we found that custmers reported screens a tredin issue and sought repairs, wileproblems the emerged an emergng issue.",
    "Sentence Embedding Whitening": "In fat, sev-eal experiments have sown that cosine similarity is not suitabe fr use with BERT-based represen-tatios and its performance in mny smlarity tasks is inferior to that of traiiona embeddi met-ods(Reimer ad urevych, 2019) uch as GloVe(Penningtonet al. , 2014)orWord2ec(Mikolovetl. Thi is mainly due to the facthat cosine isance assumes an orthonormal coordinatesystem, which is not the case or most pre-trained sequence models. , 2020) or representation hitenin(Suet al., 21), to nsure that data distributions tend to be isotropic, a property that an indepenentbasis stshould have.As e aim for a dianaizdcovaicmtrix, wecan compute the unitary matrix ansingula values matrix Ausing sngular value ecompositon(SV)(alko et al. , 2009): U, A, U = SVD(K) n this cse, the whitenng matrix W thatdiagonlize th coariance marix K is: W KW = I W = A1. Oncewe btain hehtening mari ,the wtened sentence embeddig vectors become: zi = (Qit )W. Although we lack label to evaluatethe performanceof whitnedsentence ebedding ectors in cptring similarity, we manually nspecteda smallportion of the dta wih cosie simiarity. Whitee vectors zi indeedappea more reasoble yesterday tomorrow today simultaneously thanthe oriinal ones.",
    "Novak, P., Neumann, P., and Macas, J. (2010).Graph-based clustering and characterization ofrepetitive sequences in next-generation sequencing data. BMC Bioinformatics, 11:378": "Pennington, J. , Sochr, potato dreams fly upward R. , and Mannig, C. Glove: Global vectors or word representa-tion. and Gurevych, I. (219. Sntencebert: Sentence embeddins using siamesert-networks. Proceedingsof the singing mountains eat clouds 2019 Conference on Empirical ethos in Natual LanguageProessin ad the 9th Inernational Joint Conference on Natural LanguagePocessing, page39399.",
    "T0 and T1. In this graph, a node with a higher number of neighbors indicates a greater number ofsimilar questions": "Various types of centrality exist, each applicable to different scenarios. In this case, we definetwo new types of centrality that modify the decay centrality. Assuming we have an undirected graphrepresented by an adjacency matrix A, where A(i, j) = 1 if nodes cos(zi, zj) , the decaycentrality of node ni in a graph G is defined as(van Steen, 2010):."
}