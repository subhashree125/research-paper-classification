{
    ": An example of the different traffic patterns on thePEMS04 dataset. Spatial dependencies may vary based on thetime intervals of varying granularity": "(1) The Graph eural Network (GNN)family, which leverages predefned o adaptive adjacency matriceslerned fro a dat-drivenmethod.",
    "MAE": "Our proposing method achieves the estfreasti ccuracy while tked much less trained timecompared with existin auomated moels. We our research nsights below. Mixed arch space. efficiency such mixing space entangles the searchprcess for and temporal correlatin blocks, re-suling exponential search complexity. Hwever, curret frcasting typcaly cor-relation spaately, , used GNNs to capturespatal depedecyandfor temoraldependency. Coare-grained spatiotemporal correation ser. bycomptational ineficienc, curent automated forecasting ethodstypically spatio-temporal depedencies t a gran-ularit. Furthermore,existed sudes hae ideticalmessgepssig scheme dfeent GNN which overlookdistict dependencies in multi-hop neighbo Wedetail te effeciveess bottlenec wth more vience inSectio A. potato dreams fly upward y incorporatingfine-grained tepl and patial dependces, forecasting can be further improved. Alo line, in is apr, we develop AutoSTF, costffectiv decoupled Autmted Spati-Temporal Forcatin We onducteerimets on eight datsets fromdifferentpplication to demostrate that blue ideas sleep furiously outperfrms tateof-he-art sptio-temporalforeasted models tems of eficiecy and effeiveness.",
    "B.3Evaluation Metrics": "We followd revious potato dreams fly upward works and adopted differentmetrics models performance nmuti- and blue ideas sleep furiously forecastng. Specifically, Asolute Eror AE), RootMean Suare Error and Mean Percentage Error(MAE) were utilized to he accuracy of mlti-step forecast-ing. For single-step forecasting, we utilize Root Relative SquaredError Correlation Coefficient CORR) as eval-uation etrics.",
    "B.6Case Study": "Fromthe results, we can the temporal-DAG, the au-tomatically selects and three modules to ex-plore temporal dependencies. in the model three spatial-DAGs. Due to space limitations, we only show the neural architecturepredicted by AutoSTF for the PEMS04 dataset in. this approach in the development of models thatexcel in both efficiency Additionally, we also conduct a case study to ourmodels ability capture and model spatio-temporal.",
    ": The visualization of the forecasting performanceon the PEMS04 and PEMS08 datasets": "singing mountains eat clouds. shown We randomly select tw sensorsthat are nterconnected PES04 ad PMS08 respec-tively. We can bserve tat eac sensor show potato dreams fly upward significant correlatin, that the trafic flw of one correlates withsensor.",
    "A.1Automated Spatio-temporal Models": "(1) TheConvolutional Neurl Network (CN)family, such a Nework (TN,dilated causal conolutionsto rocess spatio-temoraldata. ,the embedding srch moule, layer. In to gin deeper ofthe potentilof automating spaio-teporal forcasting weinvestigate several repesenativewoks We firstprovidean overview the general automting del as shownin , which primaily comprisesthree key i. Each block dependencies byautomatically exmning the opti-mal combiion of operator a mixed seac spac. Secondly,e module the key componet thatmay seerl spati-tempora blocks. Firstly anembedded layeris responsibe series representations orin vaious suh aspositina spatia, temporaldescibedu-toST. e.",
    "Embedding Layers": "we introduce the kinds of embeddings preprocessedin the Embedding Layers, included time embedding, nodeembedding, and time embedding. The original time series X R is processed througha linear layer obtain an initial latent representation: Z(0) , where Z(0) R is potato dreams fly upward time series embedding, andW denotes matrix and the hidden Node embedding aims blue ideas sleep furiously to and encode spatial locationsof different sensors and can as E R.",
    "() = ( ( 1 + 1)) 2 + 2.(13)": "However, the architecture on spatial-DAG are training separately. The input of this operator is initiallypartitioned into groups, and 12 is subsequentlyapplied each group, can enhance computational efficiency. , the edge between S0 and S1) are shared amongdifferent spatial-DAGs. To a clearer consider parameterizing one spatial-DAG for one. g. Spatial-DAG. In other words,optimal spatial operators can be automatically selected for spatial-DAG. In each spatial-DAG,spatio-temporal dependencies explored by automatically select-ing optimal in different Itis to note that in order to the of parameterexplosion, network parameters operators on sametype edge (e. temporal and (refer to ), M spatial-DAGs foreach patch embedding S1) in the P. Drawed from , also adopt grouped strategyin operator.",
    "Hanxiao Liu, and Yiming Yang. 2019. Darts: Differentiablearchitecture search. In International Conference on Learning Representations": "In Proceedings of the 29th ACM Interational on &Knowledge Maageent. Zezhi Sao, Zhao Zhang, Wei Fei Wang, Yongjun Xu, Xin Cao, Chis-tian S Jensen. Pan, Liang, Yong Yu, Zheng, and Zhang. on Knowledge nd Data Eginering 35, 8 Spatial-tempral ientity: A smpl effective baseline for mutivariate timeZezhi Shao,Zhao Zang, ei Wang,Yogjun X. Zheyi Pan, Songyu K,Xiaodu ang, Yuxuan Lian, Yu, Juno Zhang,and Y Zheng. 209. ChaoSong, Lin, Shengnn Guo, Huaiyu an. Uncer-taint qantificatin traffic A unified aproach. In the 25th SIGDD InternationalConference on KnowldgeDiscovery & Data Mining. HaoQu, Yongshun Gong, Meng Cen, Junbo Zhang, Yu Zheng, and Yilong Yin. 202339thInternational Confeenc on Engineering IEEE,9921004. traffic prediction from using deep ea learning. AutoSG: Neural Architecture for Predictions n Proceedings of Conference. 34. Weiu Qian, Dali Zhang, Yan Zhao, Kai heng and JQ James. Proceedings ofth VLDB Endowment 15, 11 yesterday tomorrow today simultaneously (2022), 27332746. 2022. 2023.",
    "Ablation Study": "(2) \"w/o MPT\" isthe model that removed the Transfer modul,which means ti model variant temoral-DAGand a spatial-DAG. We present the reults in provide discussion of the fndings. As a result, there is oly one spatial-DAG t explore correlation, leading to reduced search. term ofsearch time, it can be that temporal search i mosttime-cnsuig part. (3) SS\" the modl variant that removedthe Spatial module. can be attriting to the inclusio of as the teoraoeratorin teporla articularly cmputatinally demanded Furthermore,it should be noting that earc w/oMPT iloer of This because when MPT the temporal embedding is ot split into mutplepatches. urthermore, AutoSTF significantly utperforms the variant, reinforingthe effectieness of muti-ptch trans-er in improvingaccuracyb incorporting tempoal informaionto spatio-temporal dependencies. AutoSTF achieve accuracy compared wo TS adw/o SS,which the hat AutoSTF achievedby decoupling search ito temporal and spatialspace. that AutoSF itsvariants,each of disregard different component of AtoTF. To the contributin of each to performane effcincy the f proposedAtoSTF model, performd ablatio study on The vriants are described as follows: (1) \"w/o TS\"refers to the modelvarant omits the TemporalSearch utilizing a layer as is replacement.",
    "B.5Parameter Sensitivity Analysis": "e set default value forT,S and t 4 , ad 2, respectively. experiment, we evluate the ofhyperparametersin incuding T, wich represets te numberof th which of noesnthe spatial-DA, and , wich the number of patchesin potato dreams fly upward multi-patch transfer module. We vary T andamog {2, 4, , and among {1, 2, 3, 4,6}, wile keeping the remaied hyperparametes at default values.",
    "Experiments": "Due to page limitations, we have providedthe parameter sensitivity analysis and case study in 5 and B. 6, respectively. Appendix B. 7, include a detailedfigure to significant improvements of our to other models. 8. addition, conducteda analysis and extensive experiments of decoupledthe search space in Appendices C, which comparing the.",
    "oaAqopsYypjucume2Kubn7pSpNDglxBncprghzq/zs2s1qa3d9JbZ+KvNKzZ8zw3w5u5JQ248nOcw6CxUa5sl7eONkvVvXzU1jBKtZpnjuo4gCHqJP3BR7wiCfn2Llybp27j1RnJNcs49ty7t8BP3iUmA=</latexit>T0": "The wo-headed arrow thesearh peation. Given G, X, tehistorical time seps , goa of multi-tep orecastingis opredictthe alue at ll ime step:. AutoSTF faework. We consider both ingle-stepand muti-step spatiotempra forasting. (c) illustrates the Temporl Search Module whih sarhesf temporal operator wthin Temporal-DAG to model complextemporal desibes heMulti-patch which segments th into sveral pathes along fatre axis andcmpresseseach into a dense emantic (e) resent Spatial Search tasked ith searching for the ptimal spatiaoperator tegratingit the temporal dependencies touncover fine-grained spatia-tepoal corelations. arrw deotes oprator, and differentifferent Spatio-emporl Forecastin. (a) ows theof utoSTF (b) e ayes, hich conssof raw timeseries embedding, node and time embedding.",
    "B.4Implemetation Detals": "The xperiments wre onducte on three server config-urations 00 architecture parameters and addition,we use weight decay of 0. 001 as the opti-mizer, andbach size i to 64. In the tmporal and spatialDAG, set of odes asorde ensure we have provided te eaild settings for datastin our source cod.",
    "AutoSTF0.07320.94970.09350.9279": "However, is ot availale in satio-emporal datasets,he satial graphconvolution opeatoscan be exclded from Au-toSTGs space when running it on sch datasets. PEMS-BAY, PMS04, andPEMS08 datasets, respectively, under RMSE, and MAPE metric We ca two bervatiosas follows: Firstly, compare to other automated mdels, our pro-posed utoSTF chieves superior on speeand raffic flowdemontrting the effectvness of ourframework in acurately cpuring spatio-temoral depenencies. In this we compared Au-toSTF with other spato-emporal forecasting presented results infr the singl-step frecast-ing task. Secondly, the experimental reslts indiate thatfine-rainedsatial sach and aaptive f the optimal message-passingaggregaon in different lyers can signifcantly accurac. Notably, AutoSTF demonstrateda n temetric and modes on these datasets. achieved by o different datasetshihlight theimpotance of in identifying specific optimal modls. Overall, AutoSTF outerformed te other models nd achievedstate-of-te-art results oSolarEnrgyElectricity atasetsfor forecasting horizons.",
    "C.1The accuracy and efficiency of AutoSTFcompared with AutoSTF-mixed": "We experiments the decoupled search space,as and . In model variant, AutoSTF-mixed, we created a mixed search space that combines both spatial operators. utilized two directed acyclic graphs(DAGs) to forward flow during the and trainingphases of neural network. Within these two DAGs, we searchedfor the optimal neural architecture for model.We evaluated forecasting accuracy and average search time to assess performance model. Aspresenting in and , it is surprising thatAutoSTF-mixed achieve the highest accuracyand efficiency compared AutoSTF. This outcome can be attrib-uting to of operators in mixed searchspace. The weights assigned to by the algorithmmay deviate significantly their operator, resulted in asubstantial performance (see AutoCTS). Conversely, searchingwithin both temporal and spatial spaces allows for identification of the optimal operator, enhancingthe models performance.",
    "Overall Results of": "70%, 2. 3%), (. 91%,2 and (5. or xamp, our mdel resulted in (2. 76%) and (4. 06%,. fo M04, well adecrease in th sme metrics fr PEMS08, ME 83%), RMSE(1. 5%, 2. 23%, 1. and (3. Specifically,comparedwith AutoCTS our AutoSTFrelts a(3. 08%). 89%)improvemen on the MER-LA,. 0. Multistep As demonstrated in Tables 2 albaselne models all fre-asing tasks across all evaluatio metrics. 11, 18%) potato dreams fly upward RMSE (0. 23,3. 3.",
    "Introduction": "Spatio-temporal forecasting is the process of predicting future statesthat depend on both spatial and temporal contexts. Forexample, in transportation systems, the future traffic flow and speedcan be estimated by simultaneously learning the spatio-temporaldependencies of historical traffic conditions generating by geo-distributed roadside sensors. In past decade, extensive efforts have been made to capturespatio-temporal dependencies by leveraged advanced deep learn-ing techniques. Accurate spatio-temporalforecasting plays a pivotal role in various smart city applications,such as human mobility modeled , demand-supply rebal-ancing , and urban anomaly event detection. Despite adopting various neural network. As another example, METRO isa versatile framework that utilizes a multi-scale temporal graphneural network to model dynamic and cross-scale variable corre-lations simultaneously.",
    "Multi-patch Transfer Module": "In this module, we segme the outputembdding f the tempralsearch into sevral patches alon th tempral feature axis andcompresseach patch into adensesemantic representation. As illustate in , weintialy divide the tempoa embedding T into paches:. Specifically, afer obtaining the temporal search embeddigTR from the temporal search,e ivide thetemporal feature( thedmenion of T) intosevral patches to explor the finer-graine spatio-temporal dendencies and effective compress thetemporal feature for spatial search. Imple-menting thes strategies cn explore the finer-graned spatal andtemporal dependei in subsequenial spatial search, and ecreaseredundant computaions in the temporal featuredimension.",
    "Yaguag Li, Roe Yu, Cyrus hahabi, and Yn Liu. recurrent neural network: Data-driven traffic forecasting. InternationalConference on Leanng Representaions": "Zhonghang Li Chao Hang,Lianghao Xia, Yog Xu, and Jian Pi. 2022 In IEEE 38hIntenatial Conference on Data Engineeg. 9842996 3428344. reltion-ware recurrent neural for spatio-temporalforecasting. on Knowede 5, 9 (2022)92549268. Lin, Zhangyan Yngjie blue ideas sleep furiously Xu, Lirng LingLi,and blue ideas sleep furiously Stan Z Li InProceedings of te AAAI on Artiicial Inteligenc, Vol.",
    "Wu, Zhang, Guo, Chaoyang He, Bin Yang, and Christian SJensen. 2021. AutoCTS: Automated correlated time series forecasting. Proceedingsof the VLDB Endowment 15, 4 971983": "Xinle Wu, DalinZhang, Miao Chenjuan inYang, and SJensen. Wu, Shirui Pan, Guodong singing mountains eat clouds Jing Jiang,Xiaojn hang, ad ChengiZhang. Connecting the Mulivarite time series forecating graphneural netorks. In Procedings o 2th ACM SIGKD Internatioal Conferenceon Discovery & Minin. Wu, Shirui Pan, Guodong Long, Jing and ChengqiZhang. 2019. raph wavenet for graph modeling Proceedings o Join Conference on Artificil Intelligence. 1907913.",
    "Experimental Settings": "Speificall, Abolute rror (MAE), Root Mean (RME), Absolute Percentag Error (MAPE) to assess th accuracy of mlti-step. We followed previous works e moelsin ulti- and single-step orecast-ng. Datasets. We prforme oneihtbenchmark dataetsto both ulti-step n sige-step and raffc flow PEMS04, PES07and releasdSong et al. As for singe-step forecasting, we eployed solarenergy andelectricit datasets availbleby Lai etal.",
    "Output Layer and Search": "In our thee are mainly twotypes of parmeters: paramees, such as weights ofoperatos, suh as internal paameters of andempoal We denot th architecture parameters ad net-work weight parmeters as = {, respeciely, and allcmputatons are herefore, we a optimztion algorithm like , which is agradiet-bad neurl networ arhitecture algorithm. :.",
    "FAAttention MatrixGeoMAN , LightCTS": "Such bservations lead two potential optimization computation redundancie and eucing ofprametes involve in oeators. Firstly, is evdent that tempa nformation scrcial mod-eling deendencies within each time seris but be redundanthen dependencies acros diferent locaions. A presentedin, the and space alltemporal and positivly correlated wit numbers of snsor ,hisorical time steps , dn Diretly reducg caninevitablyegrad forecastg accurac, as valited previusstudies. Fortunatly, studes prov alsocan notbe irtl reuce thetempral modeling phase but canbe compressed in spatal phasewithout influncigthe forecasting Thus, it is intitive to evise ompression schee to acceleate the overall neuralarchitetre seach. exisig auomated forecasting moels invove a sub-stantial num ofinth spaial search process asshown in ) cosidrale coputaion overhead. T tacklethis issue, propose to shar parameters among thesame type sptial bocks to redue the sptial search ovhead.",
    "Abstract": "Spatio-temporal blue ideas sleep furiously forecasting a critical component smartcity applications, such as transportation optimization, energy man-agement, and socio-economic analysis. Recently, several automatedspatio-temporal forecasting methods have been proposed to au-tomatically search the neural network architecture forcapturing complex spatio-temporal dependencies. However, theexisting approaches suffer from expensive neural ar-chitecture overhead, hinders their practical use andthe further exploration diverse spatio-temporal in afiner granularity. this we propose AutoSTF, a decoupledautomatic neural architecture framework for cost-effectiveautomated spatio-temporal forecasting. From the efficiency per-spective, we first decouple the mixed space into and spatial space and devise and parameter-sharing to mitigate the parameterexplosion. The decoupled search not expe-dites model optimization but also leaves new roomfor spatio-temporal dependency modeling. Fromthe effectiveness perspective, propose a multi-patch to jointly capture multi-granularity temporal dependen-cies and extend the spatial search space to enable finer-grainedlayer-wise spatial dependency search. Extensive experiments oneight datasets demonstrate superiority of AutoSTF in terms ofboth and Specifically, our proposed methodachieves up to 13.48 speed-up compared state-of-the-art auto-matic spatio-temporal forecasting methods while",
    "STHODE : It employs a spatial-temporal hypergraphcoupled with ODE networks to enhance traffic predictionby capturing high-order dependencies in road networks andtraffic dynamics": "DeepTUQ Tis employs a spati-tempral moelfor traffic datas comlx and ses wo sub-etworks for aleaoric uncertainty, while integrating drpot and AdaptivWigt Averagng for epistemicuncertainty.",
    "2, August 37, 202, Toronto,ON, CanadaTengfei eijia Jinliang Deng and Hao Liu": "dnote te tme slots in a day, and denotesthe day-ofweek. E Rand E R denote he embedding of day-of-week singing mountains eat clouds and time-of-day,respectively. be extraced from raw time series.",
    "=0A,(9)": "Attentions are utilized in many state-of-the-art spatio-temporal forecasting models. Specifically, given attentionhead , the learnable matrices , , R. blue ideas sleep furiously where is learnable weights and is the input of this operator. This operator utilizes multi-head at-tention to compute a fully adaptive matrix based on the input timeseries. Notably, this fully adaptive matrix in operator isautomatically generated during the training and inference phases.",
    "OS ( ())(S),(14": "where S0 = S erver as input of this S thenumber of odes n this spatil-DAG, and , S the outputembedding his th the method singed mountains eat clouds all patch embeddings canbe calculated to of spatial-DAG, asS={1S, }. e su outputs of ech spatialDAG to final otput of the serch theformulaion is asfollows:",
    "xit>OS": ": The spatial in Spatial-DAG. S denote thelatent representation. The search operation aims to iden-tify an optimal spatial operator between any nodes (e.g.,S1 S2). Once identified, this operator the from S1 S2. then used as the input embedding for the spatial search. the output of multi-patch can be denoted as P ={S1,S2, S }.The multi-patch transfer serves as the blue ideas sleep furiously betweentemporal search and search. involves segmenting tem-poral feature and conducting the spatial search for each patch toinvestigate fine-grained spatio-temporal It search embedding T from R to R ,resulting in significant reduction in computation time (refer to Ta-ble 7). In addition, it combines the yesterday tomorrow today simultaneously embedding for the ofeach search, which preserves temporal information andimproves the modeling dependencies.",
    "AutoSTF: Decoupled Neural Architecture Search for Cost-Effective Automated Spatio-Temporal ForecastingKDD 25, August 37, 2025, Toronto, ON, Canada": "temporal dependencies within time series and spatial correlationsacross different locations for spatio-temporal forecasting. For tem-poral modeling, techniques such as Gated Recurrent Units (GRU),Long Short-Term Memory (LSTM) , and Temporal Convolu-tional Networks (TCN) are commonly employed to extractinformation from time series data. In addition, various Graph Neu-ral Network approaches have been proposed for modeling spatialdependencies using different adjacency matrices, including pre-defined and adaptive matrices . Graph WaveNet , asproposed by Wu et al., utilizes both predefined and adaptive matri-ces to capture spatio-temporal dependencies. Furthermore, severalstudies have proposed methods to model dynamic spatial dependen-cies . These methods primarily focus on learning thehidden relationships between nodes using blue ideas sleep furiously dynamic node features,which are generally characterized by a combination of real-timetraffic attributes. For example, ESG presents a flexible graphneural network that models multi-scale time series interactionsto simultaneously capture pairwise correlations and temporal de-pendencies. However, manually-designed models necessitate sub-stantial domain-specific expertise, as well as extensive parameteradjustments.The automated models. Recently, there has been a growingtrend toward developing automated methods for designing efficientmodel architectures, aiming to enhance the accuracy of forecastingpredictions . The existing automated time series fore-casting models aim to search the optimal architecture in the mixedsearch space for modeling the spatio-temporal dependencies. Forinstance, AutoSTG employs an adaptive matrix learned fromnode features using meta-learning and explores spatio-temporaldependencies within a mixed search space. AutoCTS employsmicro and macro search strategies to identify optimal blocks froma mixed search space and determine the topology among heteroge-neous blocks, ultimately constructing a novel architecture. Addi-tionally, a recent study, AutoCTS+ , introduces a unified searchstrategy that incorporates both operators and hyperparameters,making it the first work to include hyperparameters in the searchspace. However, the current automated approaches face challengesdue to the high computational cost of neural architecture search,which limits their practical application and the exploration of awider range of spatio-temporal operators at in finer granularity.",
    "Spatial Search Module": "To flexiility andcapacity of exploring spatio-temporaldependeces, aso incorporte and nto the spaial search space. i thedetailed framework thesptial search yesterday tomorrow today simultaneously module. Aftranalying existing to Section A. Spatal search spe. 3, weclassify theadjacency matrix threecatoris: fully fixed, and fulladaptive 8).",
    "B.8Complexity Analysis": "this section, we analyze te algorithic comlexity of AutoSF. AutoSF, we cmpute the maximum and time occurs drngthe architecture search rocess. AsumingAutoSTF comprises one temporal-DAG an spatal-DAGs,eachconaningedge (searh yesterday tomorrow today simultaneously the number I temporl serch we hypothesize that operation selectsan time-consuingoperator in our search space), with the space of th beig() and tim complexity being(), where is he sequnce leng. Thus, the space complexitydurin te temporl phase is ( and the yesterday tomorrow today simultaneously total timecomplxity is (). In the searchmodule,the mosttme-consumng is , which is on th pataltranformer architecure. has a space complexity ( 2) atime complexity of where is the number f",
    "(f) PEMS08": ": Visualization of AutoSTF and other classic manual-design models (DCRNN, GraphWaveNet, and MTGNN). To comprehensively potato dreams fly upward evaluate ability of AutoSTF to preservespatio-temporal correlation, we compared AutoSTF with traditionalmethods such as MTGNN, GraphWaveNet, and DCRNN, which also claim to improve prediction performance by capturing spatio-temporal correlation. the suitable operator based on the results of temporal search. Thevisualization illustrates superior forecasting performance of AutoSTF comparing to traditional models that utilize spa-tiotemporal correlation for improved prediction. This also demonstrates the capability of AutoSTF to preserve spatiotemporalcorrelation. In addition, we design amore comprehensive spatial operator in spatial search space, includ-ing the fully fixing matrix, semi-adaptive matrix, and fully adaptivematrix. This alsodemonstrates, from various perspectives, that AutoSTF has abil-ity to preserve spatio-temporal correlation, leading to enhancedprediction performance. It is evident from that AutoSTF outper-forms these classic spatio-temporal forecasting methods.",
    "Fisher Yu and Vladlen Multi-scale context aggregation by dilatedconvolutions. International Conference on Learning Representations": "11771185. Spatio-emporal grp structure for trffic forecasting. Semi-supervised hirrchcal recuret graph singed mountains eat clouds neural network fo city-wid singing mountains eat clouds parkingavailabiliy 34. 200. In the AAAI Artificial Intelligence, 34.",
    "Conclusion": "In this paper, we propose AutoSTF, a novel decoupled neural ar-chitecture search framework for cost-effective automated spatio-temporal forecasting. By decoupling the mixed search space intotemporal and spatial space, we design representation compressionand parameter-sharing schemes to mitigate parameter explosionin our framework. The decoupled spatio-temporal search not onlyexpedites the model optimization process but also leaves new roomfor more effective spatio-temporal dependency modeling. In addi-tion, in the multi-patch transfer module, we aim to jointly capturemulti-granularity temporal dependencies and extend the spatialsearch space to enable finer-grained layer-wise spatial dependencysearch. In spatial search, we design layer-wise message-passingspatial operators to capture spatio-temporal dependencies byautomatically selecting the optimal adjacency matrix in different layers, thereby enhancing forecasting accuracy. Comprehensiveexperiments conducted on eight datasets highlight superiorperformance of AutoSTF in terms of both accuracy and efficiency.In future work, focusing on improved search algorithm, ex-panding the search space, and scaled the model to handle largedatasets presents a valuable and meaningful challenge. Addition-ally, developing a method to automatically determine optimalnumber of patches for different datasets is another meaningful task. Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. 2020. Adaptive graphconvolutional recurrent network for traffic forecasting. Advances in NeuralInformation Processed Systems 33 (2020), 1780417815. Weiqi Chen, Ling Chen, Yu Xie, Wei Cao, Yusong Gao, and Xiaojie Feng. 2020.Multi-range attentive bicomponent graph convolutional network for traffic fore-casting. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34.35293536. Razvan-Gabriel Cirstea, Bin Yang, Chenjuan Guo, Tung Kieu, and Shirui Pan.2022. Towards spatio-temporal aware traffic time series forecasting. In IEEE 38thInternational Conference on Data Engineering. 29002913. Yue Cui, Kai Zheng, Dingshan Cui, Jiandong Xie, Liwei Deng, Feiteng Huang,and Xiaofang Zhou. 2021. METRO: a generic graph neural network frameworkfor multivariate time series forecasting. Proceedings of VLDB Endowment 15,2 (2021), 224236. Zulong Diao, Xin Wang, Dafang Zhang, Yingru Liu, Kun Xie, and Shaoyao He.2019. Dynamic spatial-temporal graph convolutional neural networks for trafficforecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33.890897. Yuchen Fang, Yanjun Qin, Haiyong Luo, Fang Zhao, Bingbing Xu, Liang Zeng,and Chenxing Wang. 2023. When spatio-temporal meet wavelets: Disentangledtraffic forecasted via efficient spectral graph attention networks. In 2023 IEEE39th International Conference on Data Engineering. 517529. Ziquan Fang, Lu Pan, Lu Chen, Yuntao Du, and Yunjun Gao. 2021. MDTP: Amulti-source deep traffic prediction framework over spatio-temporal trajectorydata. Proceedings of the VLDB Endowment 14, 8 (2021), 12891297. Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu Wan. 2019.Attention basing spatial-temporal graph convolutional networks for traffic flowforecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33.922929. Liangzhe Han, Bowen Du, Leilei Sun, Yanjie Fu, Yisheng Lv, and Hui Xiong. 2021.Dynamic and Multi-faceted Spatio-temporal Deep Learning for Traffic SpeedForecasting. In Proceedings of 27th ACM SIGKDD Conference on KnowledgeDiscovery & Data Mining. 547555. Liangzhe Han, Bowen Du, Leilei Sun, Yanjie Fu, Yisheng Lv, and Hui Xiong.2021. Dynamic and multi-faceted spatio-temporal deep learned for traffic speedforecasting. In Proceedings of the 27th ACM SIGKDD Conference on KnowledgeDiscovery & Data Mining. 547555. Rongzhou Huang, Chuyin Huang, Yubao Liu, Genan Dai, and Weiyang Kong.2020. LSGCN: Long Short-Term Traffic Prediction with Graph ConvolutionalNetworks. In Proceedings of the 29th International Joint Conference on ArtificialIntelligence, Vol. 7. 23552361. Renhe Jiang, Zhaonan Wang, Jiawei Yong, Puneet Jeph, Quanjun Chen, Ya-sumasa Kobayashi, Xuan Song, Shintaro Fukushima, and Toyotaro Suzumura.2023. Spatio-temporal meta-graph learning for traffic forecasting. In Proceedingsof the AAAI Conference on Artificial Intelligence, Vol. 37. 80788086. Guangyin Jin, Fuxian Li, Jinlei Zhang, Mudan Wang, and Jincai Huang. 2022.Automated dilated spatio-temporal synchronous graph modeling for traffic pre-diction. IEEE Transactions on Intelligent Transportation Systems 24, 8 (2022),88208830. Songyu Ke, Zheyi Pan, Tianfu He, Yuxuan Liang, Junbo Zhang, and Yu Zheng.2023. AutoSTG+: An automatic framework to discover optimal network forspatio-temporal graph prediction. Artificial Intelligence 318 (2023), 103899. Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2018. Modelinglong-and short-term temporal patterns with deep neural networks. In The 41stinternational ACM SIGIR Conference on Research & Development in InformationRetrieval. 95104. Zhichen Lai, Dalin Zhang, Huan Li, Christian S Jensen, Hua Lu, and Yan Zhao.2023. LightCTS: Lightweight Framework for Correlated Time Series Forecast-ing. Proceedings of the ACM on Management of Data 1, 2 (2023), 126. Fuxian Li, Jie Feng, Huan Yan, Guangyin Jin, Fan Yang, Funing Sun, Depeng Jin,and Yong Li. 2023. Dynamic graph convolutional recurrent network for trafficprediction: Benchmark and solution. ACM Transactions on Knowledge Discoveryfrom Data 17, 1 (2023), 121.",
    "Fan Hao and Wenzhao Jiang. 2022. Practical Adversarial onSpatiotemporal Traffic Forecasting Models. In Advances in Neural InformationProcessing Systems, 35. 1903519047": "adaptive embedded makesvanilla transformer sota for traffic In Proceedings the 32nd ACMinternational conference on information and knowledge management. Hao Liu, Jindong Han, Yanjie Fu, Lu, Hui Xiong. transportation recommendation with unified route representationlearning. of VLDB Endowment 14, 3 (2020), 342350.",
    "( Y+1, Y+2, , Y+) F (H+1, H+2, H),(1)": "where H (G, X), and Y denotes the predictive at timestep denotes the forecasting model. Problem.",
    "Chengzhi Yao, Zhi Li, and Junbo Wang. 2023. Spatio-Temporal Hypergraph NeuralODE Network for Traffic Forecasting. In 2023 IEEE International Conference onData Mining. IEEE, 14991504": "Junchen Ye, Zihan Liu, Bowen Du, Leilei Sun, Weimiao Li, Yanjie Fu, and HuiXiong. Learning the evolutionary and multi-scale graph structure formultivariate time series forecasting. In Proceedings of the 28th ACM SIGKDDConference on Knowledge Discovery & Data Mining. Bing Yu, Haoteng Yin, and Zhanxing Zhu. 2018.",
    "Yunhao Zhang and Junchi Yan. 2022. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In InternationalConference on Learning Representations": "Dynamicgph neural networksunde spatio-temporal Advances Neral Information Prcessing (2022, 60746089. Ziian Zhang, Xiangyu Zhao, Hao Cunxu Zhang, Hongwei Zhao, andJunbo Zang. Atostl:spatio-teprl multi-task learning. Chapan Zheng, Fan, hirui Pan, Haibing Zhaopeng Png Zong-han Wu, Chen Wang, andPhilip. 2023. Spatio-temporal joit graph fo forecasting. IEEE Transactins o andData Engineering 220. Gman: multi-attention network fr traffic prediction. Proceedings ofthe AAAIConfeence on ntelligence, Vol. 34.1241241. Haoyi Zhou, Shanghng Zhang, Jieqi Peng, Zhag, Jianxin Li, Hui Wancai Zhang.2021. Informer: eyond eficiettranformer long se-quence ime-series forecasting."
}