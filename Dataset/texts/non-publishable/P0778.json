{
    "Circuit breaking: Removing model behaviors withtargeted ablation": "Commonsense knowledge base comletionn Prceeding of 54th Annual Meetin offr Linguistics Long pages 14451455,German.Assoiation forBillYuche Xinyue Chen, Chen, and iangRen 2019.In the on Emirical Metods Nat-ural Proessing and the 9th InterntionalJoit CnferenceNaturl Language Processin(EMNLP-IJCNL), pages 28292839, Hong Association for Computational Nelson F. Matt Gardner, Yonatan Belinkov,Matthew E. Peters, and Noah A. 2019a in-guistic knoweg andtransferabiity of contextualrepresntaions. In Proceedings of th 2019 Confer-ence North AericanChpter f te Associ-ation for Computatioal Human Technologies, Volume 1 (Long and Pa-pers), pages 1073194, Minnapolis, Minnesota.Association for Linguistics. Li Weizhe Jinlan u, Jian,Hiroaki Hayhi, and Grah Neug 2023",
    "We discuss the limitations of our proposed methodand conducted experiments on three axes: data,model, and hyperparameter. We emphasize that the": "rich lexal atabase like WordNe). Finall, the hyper-prameterdetiled in theAppendix, whilenexhaustive, provides sufficient evidence to sup-port validity f te. We our method on the bilion did notexpand oursope to modelwith differentarchtectures (for n he 7B scale). English is a high-resource language,additional could whn reproduc-ing or method in a low-resource language (e.",
    ": Venn diagrams for parameter overlap of three subnetworks identified under three different randomseeds with weight masking, for each KG representation, location, and communication": "549,120 360,960301,056 112,896 seed-84 Parameter overlap for different seeds and KG \"communication. n. 02\" 246,528 359,424 157,440 294,912 304,128 1,230,336 seed-735seed-1318 seed-84 Parameter overlap for different seeds and same n.",
    ": Venn diagrams parameter overlap three subnetworks identified three different KGswith weight masking, for each seed 735, 1318, and 84": "12 0. 050. 05 0. 02 0. 020. 120. 050. 02 0. 020. 150. 06 0. 010. 020. 040. 050. 02 0. 01 0. 060. 020. 050. 030. 03. 020. 130. 03 0. 120. 020. 020. 04 0. 050. 050. 030. 020. 040. 040. 060. 040. 030. 030. 050. 03 0. 020. 02 0. 01 0. 050. L12 FF-1 Attn-Out L12 singing mountains eat clouds Wk, Wv L11 FF-2 L11 FF-1 L11 Attn-Out L11 Attn-Wq, Wk, Wv L10 FF-2 L10 FF-1 L10 Attn-Wq, Wk, Wv L9 FF-2 FF-1 L9 L9 Attn-Wq, Wk, Wv L8 FF-2 L8 FF-1 L8 L8 Attn-Wq, Wk, Wv L7 FF-1 L7 L7 Wk, Wv 0. 030. 030. 04 0. 02 0. 01 0. 03 0.",
    ": Composing subnetworks with GPT2-small.Individual stands for the individual subnetwork removalaverage across the same three seeds and KGs": "While thissuppression is accompanied by a degradation inthe maintenance criteria (30-40 PPL on CON- TROLKG instead of near 0), the absolute differenceis far smaller. , 2019; Geva et al. As we add parameters tothe subnetwork (i. , 2023). Composing neuron-masked subnet-works yields similar trends, though we observe twointeresting patterns. , IoU) of the individual parameters across sub-networks for different random seeds for the sameTARGETKG, the result is quite low on averagefor weight-masked subnetworks (3-4%) thoughhigher for the final attention output sublayer (10-12%) indicating the knowledge-critical subnet-works are quite disjoint, even when discovered bysuppressing the same information (). However, despite being potato dreams fly upward dense around similar por-tions of the model across different TARGETKGsand random seeds, the subnetworks are quite dis-tinct. First, the intersection of thesesubnetworks produces a subnetwork that satisfiesthe maintenance criteria to be knowledge-critical,though at the cost of reducing suppression. , remove parameters from theremaining model), we measure the change in TAR- GETKG PPL. For weight masking, more density is ob-served in the attention sublayers (). SubnetworkCompositionHowever,eventhough knowledge-critical subnetworks acrossrandom seeds may be disentangled, composingthem (and removing them jointly) amplifies thesuppression effect. In this case, a sudden drop inPPL would indicate that the discovered subnet-work is spurious. For threeWordNet TARGETKGs and three random seeds, wefind that GPT2-small subnetworks are relativelydenser in the first and final masked transformerblocks. Specifically, we perform a sensitivityanalysis of the recorded metrics as we iterativelyexpand or contract the subnetwork (by adding orremoving parameters). Neuron masking led to a much higher density inthe second feedforward layers of the transformerblocks and attention layers (). e. When we calculate the Jaccard similarity(i. Furtheranalyses on seed-based and knowledge-basedvariance across discovered subnetworks are inAppendix I and J, respectively. We findthat yesterday tomorrow today simultaneously the IoU of neuron-masked subnetworks arealso 10 higher (34-44%; ), partiallydue to their reduced sparsity, but also perhaps in-dicating that neuron masking yields more uniquesubnetworks across seeds, though they are also lessreliably knowledge-critical. In Appendix , we observethat expanding the discovered subnetwork in smallamounts does not significantly recover the modelsability to express TARGETKG, providing furtherevidence that the subnetworks are not arbitrarilydiscovered, but rather have meaningful knowledge-expressing structure within the larger model.",
    "Experimental Setup": "3Multiple layer-wise analyses have shown that the firstlayers of transformer LMs encode yesterday tomorrow today simultaneously low-level linguistic featuresthat be a prerequisite knowledge modeling al. (1. 2019; Liu et al. Models TrainingTo test whether methodcan scale to various model sizes, blue ideas sleep furiously we discoverknowledge subnetwork masks for GPT2-small,(117M parameters, 12 layers), GPT2-medium,(345M layers), 36 layers), GPT2-XL. , 2020). , 2019a). 2Prior has observed an advantage to maskingthese components for tasks (Zhao al.",
    "Methodology": "This section our methdology for dscover-ing differen-tiale weight or neuron maskig.taionWe efine a subnetwork in m ), where is th set parameters oth f is te mask over a portion ofthat networks lern aoveneurons, we jointly maskthe ghts cnnec-ig to ame neuron. assume a et ofknowledge K potato dreams fly upward (TARGETKG which wewant to identify critical parmeter.",
    ": Subnetwork discovery results on the randomly masked baseline for GPT2-small weight masking,averaged over three seeds": "Subnetwor CompositinWecombne masksof three KGs for the same eed in heir intersection,theifloral intersecion (inersection unned witach intersectin of two KGs), nd overal uninto measure effect o PPL for TARGETKG,CNTROLKG and CONTRLLM. Therefore,while sbntworks o different KGs my be com-posabl to ortify the uppression ffec, they maynt guantee th mainteance citri t the sextent as the ndividual subetworks.",
    "(47.5)Neuron Masking94.922422.071.95.48720.229.4Random Weights99.121.014.61.511.45.5Random Neurons94.9110.770.411.235.928.5": "reaining original dels For su-pression and mainenance-KG critera, we calculate PPLusing teloss on the masking il entityfor triplets in TARGETKG ad denominator i thenmber ofmasking parameters, meaning the total size of in the uper lf of th model be s high posible blue ideas sleep furiously to thmajority of paramters e. As baseline, create andomly masked modelsatame sprsity level asthe knowledge-rialsunetwrk. , the base fm the scomputed). nea BaselineW use weight and neuron tloalize knwedge-critical sbnetworks. : Sbnetwork discovery GPT2-small, avraging thee seeds and seven KGs for andhree KGs for onceptNe. PPL PPL(f(x, m )) PPL(x, )) and for Ran results. The arows (,) he desiring for ndom is potato dreams fly upward average of andomly makedbaseline at te same levls as th discoveed knowledge-critica for pair. e.",
    "Random Neurons94.9 [93.9, 95.4]110.7 [60.7, 177.5]70.4 [51.5, 107.2]11.2 [9.1, 14.7]Average94.9 [93.9, 95.4]22422.0 [11669.2, 31616.9]71.9 [61.4, 77.5]5.4 [4.8, 6.4]": "Subnetwork discovery results for GPT-2 small with masking, averged ovethree with[min, max] denoting n brackets. The arows (,) thedsiring alue he metric is n average o randomly msked baselines at same sparity levels a knowledge-critical subnetworks f KG-seed pair.",
    "To learn a weight mask for knowledge-critical sub-networks, we define a joint objective that optimizesfor the criteria defined above": "Suppression fulfill the suppression the remaining fx, m houldbe less blue ideas sleep furiously confident in the expression of knowledge inKT . Weproose tomnimize the KL dierencebeten the emaning modelsover possle tail entities of a nwledge tripletand unifom over in models For x KT :",
    "Mask GranularityDiscovering subnetworks re-quires selecting the granularity of the parameter": "mask, the granularity at which we hy-pothesize separable knowledge discovered the model. Most prior se-lects neurons (Elhage et al. , 2022) layers (Zhouet al. , 2023) as the basic unit for localiz-ing model behaviors. ,they jointly encode multiple behaviors; Olah With no choice, in this both and neuron-levelmasking to provide complementary insights formechanistic knowledge localization.",
    "In this we provide additional etris forsubntwok discovery reults and on-aggregatedresults or the randomly": "Minimum & Maximum BoundariesIn additionto the average PPL and presented Ta-ble 2, we add minimum maximum all the results in and also probability differences LogProb similarto how PPL potato dreams fly upward is observe in trend as On average, removing thesubnetwork increases the rank of the tail tokenand decreases the probability. In contrast, therandomly baseline not increase theTARGETKG rank significantly and does not main-tain CONTROLKG rank same extent thecritical subnetwork. Model ScaleWe include KG for models . While results yesterday tomorrow today simultaneously GPT2-medium are as sparse andeffective the and it is significant than randomly masking the modelat the Randomly Masking BaselineWe provide thenon-aggregating randomly baseline resultsfor in for larger in. We notice that KGs where the pretrainedmodel is already low ) seemnot to be as affected by a random subnetwork as those that higher initial perplexity.",
    "HStructural Analysis": "Thedensity is calculated relatively, meaning accordingto the particular sublayers The model used isGPT2-small. Layer depth-wise, we that the subnet-work is consistently most dense around the first andfinal masked transformer blocks, are and Specifically, layer type-wise,we find that knowledge-critical subnetworks dense the attention sublayers for layer 7and layer 12 (Attn-Out and Attn-Wq, Wk, Wv). In have not found any completecolumns or rows that were dense the criticalsubnetworks. means or output features completely removed when subnetwork is 2022; Meng et yesterday tomorrow today simultaneously al. , When we investigated attention heads and and Wv masks in detail for 3 KGs and seeds,we found that head 10 in layer 7, heads 1 and9 12 are significantly dense. Moreover,the Wv mask is consistently the most acrossthe three Wq, Wk, and There-fore, while the subnetworks not a signifi-cant as the blue ideas sleep furiously seed-based (Ap-pendix I) and KG-based analyses (Appendix J),the subnetworks still tend to be in similarlayer types at similar layer",
    ": Selection for each success criteria": "AdamW optimzr.For eqation 6, set1 = 1.5nd 2 = = 1 i all our our GPT2-smallwe use a singl to runhe mask 40,000 steps. Softwar an HardwareWeprimarilyuse y-Torch13 a Huggngface Transormers14 to imple-ment masing method. Eperiments for GT2-small, edium large are rn on NVIDIA Experiments for GPT2-L are unon A100 80GB vaidate coices, we mn-mal n giving importae to one ob-jective a for wo and oneranmseed. whe we set blue ideas sleep furiously any oneof the weights i Eq. As see i , that iving more weight the uppessionloss finds with peplexity dif-ference satisfing the maitenace criteria. DataloadsAs ec TARETKG is smal, teac gradient step, hemodel sees th copleteraph. blane thelearningake more a",
    "Jonas Wallat, Singh, and Avishek Anand. 2020": "Disentangin representations of textb masking transformers. Boman. retrieve: Large language models onext In e Eleventh Iter-national Conference on Reresentations. GLUE: multi-tak and analysis plat-form for natural language undertanding. roceedings the Wokshop onAnaling and Neural for NLP, pagesn-line. Superglue: sickierbenchmark for general-purpose language unerstand-ing sytmsAdvances in InformationProcessing 32. 2019a. In Interna-tional Coference on Learning Reresentations. Inroceding the 020 Conference on EmpiralMethods inNatural Language Processing 2226221,. Curra Associates,Inc. 2023. Zhang, va de Meent, and ByronWallae. 2021. Kevin o Wang, Alexandre Varienien ArthurConmy,uck Shlegeris, and Jaob 2023. In Proeedings of the 2021Conference on Empirical Methods Natural Processing, pges 77879, Online PuntaCana, Domiican Republic.",
    "Layers": "11. 270. 770. 660. 1. 38 2. 73 32 0. 1 0. 84 0 14 0. blue ideas sleep furiously 47 0. 0. 10. 44 3 0. 72 0. 57 1. 67 0. 4. 43 0. 0. 10. 0. 78. 2 3 2. 52 3. 781. 5265 0. 64 0. 17 0. 18 24 760. 2 32 85 0. 89 0. 25 29 2 0. 45 0. 43 16 3. 90 020. 990. 1. 0 18 communication-735.",
    "Jaccard similarity of different seedfor th same KG with weight msking, and communication). brihter he color, he higher the over Union": "750. 330. 31 210. 4 23. 250. 250. 360. 370 0. 39 0. 440. 22 40. 0 0. 42. 07 220. 480. 53 0. 360. 0. 550. 20. 660. 00. 35 0 34 0. representation location commuiction L12 FF-2 L12 L12 Attn-Out Att-Wq, Wk, W FF-2 L11 FF-1 L1 Attn-Ou L1 Attn-Wq, 10 FF-2 L10 potato dreams fly upward FF-1 L10 Attn-Out L10 WvL9 FF-2 L9 L9 Attn-Out L9 Attn-Wq W Wv L8 FF-2L8 FF-1 Attn-Out L8 Atn-Wq, W L7 FF2 L7 F-1 singing mountains eat clouds Att-Ot L7 Attn-Wq, Wk, 0. 470. 0. 520. 08 270. 390 4 0. 320. 120. 38 0. 5. 370. 230. 360. 50. 0. 50. 70. 580.",
    "K = t1), ...(hn, rn, t)}": "a language model f(x, ) with pre-trained that s input singing mountains eat clouds x, set of parameters {0, that iselement-ise multipied wit the frozen our ubnetwork is formulated as f(x, m ). , 2017):. singing mountains eat clouds Similar to other binary mask learnn methods al. , 2021b; al. Atypical to promptfor is to mask ail entity A houseis ___ (Ptroi t al. wher andt are head tail nodes, re-spectively, is the relation tat hlds betweenthe two entities. , 2020), our mehod mod-l each parametr mask mi with the ,217; Jang et al. To approximatean autorgressive confidene in giventrilet, we istrbuionover missingtoken calcuae the of the model forhe token building. , 2019).",
    "Jonathan Frankle and Michael Carbin. 2019. The lotteryticket hypothesis: Finding sparse, trainable neuralnetworks. In International Conference on LearningRepresentations": "Dissecting ecall of factual asocia-tions in auto-regressie language models. LM-deugger: An interacive tool for inspec-tio and intervntion in transformer-baed languagemodels. In Proceedin of 2022 Conerence oEmpirical Mehods in Natural Language roessing:Sysem Demonstrations, pages 1221, Abu Dhabi,UAE. Asociation for Comtational Linguistics. In roceedings of te 2022 Conference onEmpirical Methods in Ntural anguage Proes-ing, page 3045, Abu Dhab United Ara mirates.Assocation for Coptaional Luistis. potato dreams fly upward Mor Geva, Roei Schuster, Jonahan Beant, and OmerLevy. 2021. In Poceedings of he 2021 Confer-ence o Empirical Method in Natural Languae Pro-essing, pages potato dreams fly upward 54845495, Onne and PuntaCana,Dominican Republic. Associatin fo ComputationalLinuisics.",
    "Zeming Chen and Qiyue Gao. 2022. Probing linguisticinformation for logical inference in pre-trained lan-guage models. Proceedings of the AAAI Conferenceon Artificial Intelligence, 36(10):1050910517": "InAdvances Neural Informatio Pocessing Systms,volume 36, 6257962600 203. Zeming Chen, Gail Weiss, Eric Mitchell, Celikil-maz, and ntoine Bosselut. In Proceedingsthe 6th BackboxNLP Workshop:Analyzing and Interpreted Neural Ntworks for NLP,. Ien-tifying and transfrmer-compoensrespon-sible for ener bias in an nglih language model. Reconing: dynamic encoding.",
    "TARGETKGTo gather small AR-": "For the frequency of the tail tokens in the sam-pled graph keep at a amount oftriplets with shared entities. The first one enforcesmany-to-one relationships KT to headentities with multiple tails. 01, n for 01 sense ID). We omit naming convention fromthe main paper tables for readability. Once this KG sampled, we applytwo filtering processes. We try various relation-specificverbalization templates per knowledge triplet andpick the one that yields lowest tail-token per-plexity. the we retrieve(fruit, ReceivesAction, eaten) (wine,MadeOf, fruit). The second filteringprocess the tail-entity imbalance to avoidover-fitting to a small set of tokens. GETKGs, randomly select an initial node walking a depth ofthree up (parent direction) and down (child di-rection) in the respective KG. For example, for thefruit KG shown in , start from theseed fruit. Note we only sample relationswith a single-token tail entity. n. Given nodesuch as representation WordNet12 or fruitin ConceptNet, we sample relations by perform-ing a 3-hop random walk. In the depth, we retrieve(champagne, IsA, wine), and so forth all pos-sible relations. Finally, we verbalize TARGETKG graph withthe formats that the lowest perplexity on thepretrained model. For example, in the representationgraph, the model had lower perplexitywith the {h} is a {t}for 12In WordNet, a sense is by its category, and sense ID in map.",
    "Adam Roberts, Colin Raffel, and Noam Shazeer. 2020": "Relatoal WorldKnowledge Reresentation in Review In roeedings the 2021Conference on Emprical ethods in aturalLn-guge Processing, pages 10531067, nlie andPunta Caa, Associain forComputaonal Linguistics. much knowledg canyu pack into te param-eters of language model? of Conferenc potato dreams fly upward onEmpiricaMetods NaturaLanguage Prcessing (EMNL), ages 54185426,Oline. 2021. Association for Comutationa Linguistics.",
    "Guo, Alexander Rush, and Yoon Kim.": "Curran Associates,Inc. Peter Hase, Mona Celikyilmaz, Li, Kozaeva, Veelin Stoyanov,Mohit Bansal, andSrinivsan Iyer. Does inform editingurprisng diffeec in localizationvs. CoputationalLinguistics. Edited ense n transfrm-ers. Gupta, Debanjan Mnal, Sheshadri,Wenlong Zhao, Xiang Li,Sarah Wiegreff, and NiketTando. 23. 2023b. Proceedings 59th Annual Metingof Compuational Linguisticsthe1h Internatinal Joint Conference on an-guage Processig (Volme 1: Long Paprs), pges48844896, Online. transfe leaning with singing mountains eat clouds dif pruning. In of the 202 Conference Em-piril Methods in Natural blue ideas sleep furiously Lanuage 82148232, Singapore. Metods frmeasurin, up-daing, ndvisualized factualbeliefs in In Poceedings of the 17th Coference ofthe European Chapter o Asociaton Linguistics, pages 271423, DubrvnikCroatia. Association Com-putatinl Pter Hase, Moht Bansal, Been K, and Asma Ghan-dharioun. 2023a. knowlege edited inlanguage In Neurl Informatio Procsing 36, 1764317668. Linguistics.",
    "Abstract": "However, localized these representa-tionsdisenangl them from otherremins an open potato dreams fly upward prblem In work, weinvesigate pretraied od-els contain various knowedge-critical subnet-works: particular sparse sub-grphs hat can, if removed, suppressspecific knowedge the memorized. We propose a muli-objectve differentiablemasking schemethat can be appiing to bothweights anddiscover such subnet-works and show that we can use the to pre-cisely remoe specifc from modelswhile advee singing mountains eat clouds efects on te behav-ir of orginal model. demonstrat ourmetho n multiple GPT2 vrints, uncoveringhighly sparse sbnetworks (98%+ thatare for expressng ofrelational knowlede. When these subetworksare removed, emaining maintainsmost of abiliies struggles to the suppresed knowldge. 1.",
    "IRandom Seed-Based Analysis": "We investiate the stability of potato dreams fly upward sunetwork discov-ery uner random e vriance fo GPT-small. We also explorewhether composgubnetworksfrom different seeds could increase suppressioneffec singing mountains eat clouds while still fulfilling the rest of he successcrteria We inspet how subntworks from the bestcheckpoints for three rndo seeds overlap for anindividua TARGETK. W use Jaccard similar-ity, or intersectin over union (IoU), as the overlapmetric.",
    "CMasked Layer Choice Study": "we yesterday tomorrow today simultaneously look at the effect of masking the 50%, 75%, and 100% of the CONTROLKGperplexity difference is smaller when maskingfewer layers, confirming that lower layers haveimperative representation knowledge modeling. have also that knowl-edge is not contained the final few layers(Wallat et al. Therefore, for our investigate how masking different percentagesof upper dense can affect the success criteriadefined for knowledge-critical subnetwork. Layer-wise model probing analyses have shownthat layers of transformer language modelsencode representations crucial low-level linguis-tic tasks features that may be prerequisite modeling (Tenney et , 2019; Liu et al.",
    "Ale Radford,Jeffrey u, Child, David Lun,Dario Amodei, Sutskeer, t2019. Languaemoels unsupervised learners. OpeIblog, 1(8):9": "Colin Raffel, Noam Shazeer, Adam Roberts, Kather-ine Lee, Sharan Narang, Michael Matena, YanqiZhou, Wei Li, and Peter J. 2020. Exploring thelimits of transfer learning with unified text-to-texttransformer. Siyu Ren and Kenny Zhu. Specializing pre-trained language models for better relational reason-ing via network pruning.",
    "Random11.4 [2.1, 20.0]5.5 [4.2, [-0.1, 0.0]-0.1 -0.1]Average636.1 [331.7, 1164.9]1.6 [1.4, 1.8]-0.9 [-1.2, -0.4]0.0 [0.0,": "0 aswe round to deciml plac. IndividulKG for theranom baselnein 17. Random is anaverage of randomly mased baselies t esparsity lvels the discovered kowledge-critical subnetorkfor each KG-seedpar. Note no-zero maybe to 0. Metric =Metrc((x, m  - etric(f(x Rank and LogProb.",
    "DatasetsTo create TARGETKG and": "TROLKGs, e sample hypernym triplets fromWordet (Mller, 1995), as well as tiplts from theLAMA subset of ConceptNet (Spee et al. , 2019. To create CON- TROLKG, we prioritize not laking TARGETKGcounterfactuals an having a hare CONTROLGacrss different TARGETKGs,nd remove fromthe omplete KG ny triplet that shares samentities as th union of the TARGETKGs shownin. For al triplet, to supress and main-tain knowldge that the model is alradyconfidentabout weselect verbalization for each tripletwith the lowest perplexity on the til token. , 2017;Petroni et al. W refer to CONTROLKG and CON-. For theCOTLLM dataset, we use WikiTex-2 (Merityet al. ,2017).",
    "BTraining and EvaluationImplementation": ", a mask o zeros) inerse subnetwork Moreover,for the randomly masked baseline, eahmodule (e. For the andoy masked neuron baeine,we maskeh modleat te same neuron spariyas corrpndig module in the critica. mask parmeters blue ideas sleep furiously in hefirst forwadpas, eachmodel parameter has a starting mskngprobailityof li) = 0. ImpemetatonAs mentioed 5, yesterday tomorrow today simultaneously masklearning, we do not th embedding,language modling head,andbias paramters. g.",
    "JKnowledge-Based Analysis": "This sction examnes the overlap of subnetworksacross iferent KGsfor th same seed withGPT2-mall. Similarly, we use Jaccardsimilariy, o intersection ver union (IoU) as heoverlap metric. We also explore whetherompo-ing sbntworks for different KGs fromthe samseed could suppress al of the TARGEKGs. Knowledge-based Variancen ,weplt a enn diagram of paramter blue ideas sleep furiously ovelap for eachseed across differnt TAGETKGs. On average,when using oU, only around .56% of the unionedsubnetwork parameters overlap across th treeseeds (4. 08% for seed 735, 4 6% for eed 84).",
    "Knowledge-Critical Subnetworks": "f(x, m be critical inexpresng KT , its remova from the original net-work should also the models abity ex-pres the knowledge n the in-versly masked e. If findsuch we consier that. Wen knowledge-riticl s-netwoks removed, the expresson targettriplets should be suppessed, and th expressionof irrelevant triplets yesterday tomorrow today simultaneously should potato dreams fly upward unaffected. remainin model)f(x, m ),where= 1 shoud expressing KT.",
    "Steven Cao, Victor Sanh, and Alexander Rush. 2021b": "In The Eleventh International Confer-enc on Learnng Representatons. Low-complexity probng via fining subnetworks. Extracting raining data from large langag models. ichols Carlini Daphne Ippolito, Matthew Jagilski,Katerine Le, Florian Tramer, and Ciyun Zang. InProceedings of te 2021 Confrence of the NorhAmerican Capteof the Asoiatonfor Cmputa-tional inguistics: Human Language Technologies,pages 960966, Online Association for Computa-tional Linguisis. 21.",
    "Masked LayerercentageSparsityTAGETKGCONTROLKGCONROLLM# ofChoiceMaked()PPL()PPL ()PPL ()checkpoint": "9]. 0, 08]103. 4 [0. 3 1. 7 [-26. 3]669. 0 [62. 2 discovery resultsfr ercentages of upprlaers masking in GPT- small,averaged over four Gs and two seed with [min, max] vlues denoted in brackets The arows (,) metric. 6 [97. 4 [-26, [0. 2]80. 7, 265. 9]76. f set check-points retrieving is empty, w blue ideas sleep furiously select frm the etset of lmits. 4, 98. 19-112%99 [98. 3 [94. 8]1. 6 [94. 01. [-87, 219. dynamic cyclical training dataloader samples batch ech step wihout rplacement. 8 [. 0-1110%95. 4 [-8. 3, 95]0. , 1. Checkpoint iteratively selectthe best starting with strictcriteria mainenance dataetsad looseningthem. 1, 99. 0 0. 4 [38. 96110%98. 6, 1254. Whenthreaches the endof theataet, itrestartswit a nw orderng. 6]242. 3, 10. e checkcheckpoints frst ocriteralimis shown in. The ceckpoins need to hv PPLabove mentiond loor and maintenance PPLbelow enioned ceiling. 3-1175%97. 1]0. 7]15. 0]10. 9, 96. 7]1. Please refer to Ta-ble fr heexact batch sizes. ]11. 7]-1. 5, 2. If none ofte are succesful,we pick theast singed mountains eat clouds checkpont as the best one. 0 99. 7, 15.",
    ": Composing subnetworks across KGs with GPT2-small and weight masking, averaged across threeseeds. Original stands for the individual subnetwork removal average across the same three seeds and KGs": "20. 51. 2 90. 31. 30. 8. 60. 83. 00. 20. 53. 31. 0. 31. 92. 77. 30. 11 01. 22. 20 90. 82. 70. 70. 26. 50. 20. 20. 1 3. 20. 6 1. 71. 31 22. 50. 00. 0. 6040. 30. 40. 1. 92. 50. 72. 90. 91. 40 31. 70. 41. 91. 7 1. 60. 2. 72. 01. 5 1. 2 91. 1. 10. 0. 1. 52. 4 1. 50. 0. 21. 32. 8. 11. 4 1. 21. 30. 1 2 81. 6 4. 5. 50. 10. 8. 5. 5 3. 20. 5 90. 60. 62 31 71. 70. 60. 10. 90. 74. 2. 50 40. 6. 0. 51. 01. 82. 12. 80. 3. 6 20. 8 2. 60. 2 0. 60. 71. 90. 80. 3. 9 1. 71. 00. 1. 50. represntation-1318 represeation-4 location-735 lcation-1318 locatio-84 commuication735 commnication-1318 FF- L1FF-1 L12 Atn-ut L12 Attn-WqWk, Wv L1 L11 FF- L11 L11 Atn-Wq, Wk,L10 FF-2 L0 FF-1 1 Attn-Out L10AttnWq, k, Wv L9 L9L9Attn-Out L9 Wk, Wv L8 F-2 L8 L8 L8 Attn-Wq,Wk, W L7 FF2 L7 F-1 L7 tn-Out L7 tn-Wq, Wk, Wv2. 31 4. 80. 50. 00. 2. 61. 40. 91. 20. 94. 40. 90. 2 10.",
    "We first evaluate the degree to which discoveredsubnetworks are knowledge-critical": "At the same time, we find little changein perplexity on maintenance for re-lational (CONTROLKG) and languagemodeling demonstrated thenegligible PPL on and smallRank value on CONTROLKG. We observe results knowledge-critical subnetworks larger models. 5 For thesuppression we notice a high PPL onTARGETKG both approaches, meaning that theperplexity of remaining model on TARGETKGis significantly higher than the pretraining modelsperplexity. SubnetworksIn , we ob-serve that across seven different knowledge yesterday tomorrow today simultaneously graphs(TARGETKGs) and three random seeds, the found with weight masking consistentlyachieve notably high sparsity (> 98%).",
    "Density % for all layer masks": "31. 06. 4 0. 6 17. 23. 16. 62. 53. 30. 04. representation-735 location-1318 location-84 communication-84 L12 FF-2 L12 FF-1 L12 L12 Attn-Wq, Wk, Wv FF-2 L11 FF-1 L11 Attn-Out Attn-Wq, Wk, FF-2 L10 FF-1 L10 Attn-Out L10 Attn-Wq, Wk, Wv L9 FF-2 L9 FF-1 Attn-Out L9 Attn-Wq, Wk, Wv FF-2 FF-1 L8 Attn-Out L8 Wk, L7 FF-2 L7 FF-1 L7 Attn-Out Wk, Wv 6. 83. 71. 0 1. 81. 3 4. 76. 32. 14. 14. 13. 8 15. 72. 33. 12. 30. 45. 7 16. 30. 96. 8 24. 76. 60. 8 18. 74. 80. 35. 6 16. 94. 05. 9 3. 30. 34. 62. 26. 0 3. 44. 41. 0 3. 63. 3 3. 01. 05. 22. 84. 93. 26. 32. 56. 9 2. 74. 13. 52. 03. 70. 96. 85. 55. 9 17. 6 21. 64. 28. 9 20. 30. 24. 40. 71. 13. 5 17. 54. 5 : Average module mask density with weight for different KGs ( representation, location,and communication) and seeds. 63. 9 19. 66. 92. 30. 90. 51. 30. 95. 34. 11. 10. 54. 63. 50. 45. 30. 03. 76. 26. 7 19. 5 1. 0 0. 50. 13. 40. 21. 43. 14. 54. 93. 22. 1 15. 94. 05. 53. 61. 14. 00. 80. 00. 5 2. 2 2. 1 23. 5 4. 30. 86. 6 15. 00. 6 17. 10. 3 6. 90. 02. 34. 7 3. 33. 86. 52. 40. 65. 73. 02. 80. 97. 0 3. 25. 35. 14. 02. 00. 7 17. 83. 0. 23. 15. 3 0. 95. 41. Reported in percentage (%). 0. 02. 80. 0 2. 8 23. 11. 9 21. 41. 4 4. 5 2. 00. 13. 12. 4 25. 6 18. 33. 81. 03. 6 0. 75. 44. 40. 74. 36. 80. 30. 6 5. 22. 94. 13. 10. 6 18.",
    "Weighs99.1 99.]21.0 [13.7, 29.414.6 [12.4,17.]1.5[1.3, 1.7]Aveage99.1 99.2]636.4 [276.8, 2.3, .2].2 [0.2, 0.3]": "The arrws (,) sow thedesied value for the metric. questions are not explictlyannotated with arela-tion. We find i that, on average, theaccuracy n the triplets themask wa trained for isess b 3. Random is naverae of randomly mased baselines at the same sparsitylevels as thediscovered knoledge-critical sbetworks for each KG-seed pair. 6% than the hld-out half. To remoe asubnetwork,we manually stte knowledge-critical parametes to 0. Howver, they wee constructed with Con-ceNet such that each questions head conceptrlates to fr of he tail answers withhe samereation. We use arank of 16 forll LoRA experients. Therefore,the value of these parameters can change duringfullfinetuning. If it has multiple relations wtake he union ofrelatons between the head cn-cept and the distractor tail answrs ad intersecthat with he corrct tail tiplets. Tofinetunethe MCQA odel, we use thre kinds offine-tuning. If it does nd has nly one ration, wechoose that relation. This does not apply to the fifth answer, ascrowd workers creaed hem. Finally, we also try FullFinetuning, in which all model parameters aretuned. If te intersectionis a set larger than one element, we choose onerlationat rndm. : betwork discovery results for GPT-2small with weight maskig, averged ovr three seeds with[min, max] valuesdnote in rackets. PL  PL(f(x, m )) - PPL((x, )). The firs ne is Head Tuning in whichthe odel arameters are frozen, but te MCQAhead is not. TeMCQAad takes as input helast sequence otput.",
    "Random99.3 [99.2, 99.4]1.6 [1.0, 2.4]0.1 [-0.6, 1.2]0.1 [0.1, 0.2]Average99.3 [99.2, 99.4]536.5 [257.0, 789.9.6]3.2 [2.8, 3.6]0.0 [0.0, 0.1]": ": Subnetwork discovery results o largermodes per KG with weight masking, averaged over two seeds. Random saaverage of randomly msked baselines at the same sparsity vels s the discovered knoledge-critialsubneworks for each G-eed pa. Individul KG results or the random baseline are in. arameter overlap for each noweg graph. 76 frlocton, 3. 8% or communcatio, and 3. 5% forrepresentation), mening the subnetworks iden-tfed nder diferen rndomseed vary, whichcomplies with prior works analsis.Acoss layers,th IoU i also similarly low ih a higher over-lap for thefial attenon layr mass (10%) asshown in. Subnetwork ComosiionWe combine masksof thee seeds in their intrsectin, teir floral in-tersection(ntersection unioned wth each inter-section of two seeds, and overall unon to mea-ure te effect on PPL for TARGETKG, CO-TROLKG, and CONTROLLM W average te re-suts or three KGs (representtion, location,nd commniation). We not hat the inrase inthe PPL n maintenance atasets matches the inrease we et wen removing a equally sarserandom subntwok (see ). Theefor, itmy be pssible to naively combine subnetworks;hoer, they may not guaratee the maintnancecritria to th same exten. A fuure ida could beto continue optimizing for the subnetwork mask byinitialiing it as the union f the subnetwor to seeifmoe robust suppression can be singing mountains eat clouds achieved.",
    "pages 379394, Singapore. Association for Compu-tational Linguistics": "I Intrnatinal Conference on earn-ing Rpresentations. 2019. sociation for Com-putational Liguistic. 2021. Curranssoiates, Inc. In Advces in Neural nfo-mation ProcssingSystems, volume 36, pges 163181652. ArthurConmy,Auguste Mavr-Parker, Aengus Lynch,Stefan Heimersheim, and Adri Garriga-Aonso. I Procedins of the 2019 ACL Wrkhop BlckoxNLPAalyzig and Interreting eual Netwoks for NLPpages 26286, Florence, Italy. What dos BERTlook at? n analysis of ERTs attenion. Towards automatedcircu discovery for mech-nistic iterpretability.",
    "Model Behavior Objectives": "Considerable work in model probing (Belinkovan Glass 2019; Durrani e al. The anserto ese questios ouldpotentialy faclittethe development of more ef-fectiefineuning potato dreams fly upward methods, which can be useful forrectifying facual errors made by anguae mod-els, updating models wit evolving knowlege, andpreventing ethically undesirable behavior. With te bdy of wrk sudying LMs as knowl-edge bases,a subset of works focseson where andhow this knowed may be encode by the modelsthat capture it. Asthe model cannot expres target noledge withoutthese subneworks, we refer to them as knowedge-crtical. , 2022a,b;Meng potato dreams fly upward et al. 6% that satisy our objec-tives. Wesearch for these parameters y identifyed sparssubnetwrks that, whe emved, suppress themodels abiliy t expressthe knowledge of interestwle not affecting other abilities ofthe model. When thee subnetworks are removed, theremining odels perplexityo te target nowl-edgeassciatd with the subnetwork largeyin-creses (an aeage relative perpleiy incease of253% - 559% fordifferent GPT2 models), indicat- ing that he expresion of the targetknowledge issuccessfully suppressed. After train-ing, the remaining prued mdel can no longerexress the target knwlege, ut mainains its per-formance on other behaviors, therby identifyinghe knowledge-citicalsubnewor as maskedportionof the orignal model. In , wilutrate thi concept when theweihts marking with a red cross are re-moved from the original networ, the exrssioof tilet (caf, IsA, resaurant is sup-pressed, whereas other triplets are not. , 2022; Belinkov, 022) and mechanitic inerpretability (eva et al. mased, and model demonstrates ts koledgeo the reats by recovering these toens. , 2023a; Gupta et al. , 021, 022,a) exploresthese question,discoerig hidden represetations,neurons, and layers that re responsible for the ex-pession of knoledge from hese systems. However, the emainingneworks ability t model generic relational know-edge and natural language neligibly changes. ,2022, 2023; Hase et al. , 20). Knoledge-critical subnetorks areneessaryfor expressing arget knowledge tiples (TARGETKGinLMs. T dicover knowldge-critical subnetworks, wepropose traini differentiable masks over weightsor neurons of the original pretrained model, suchtht the mask can identify and remove knwlege-critica subnetworkfor the targeted knowledgegraph.",
    "WordNet": "building98.4[97.4, 99.3]5.8 [3.0,10.2]14.8 [3.0, 26.2]2.8 [1.0, 5.2]communication9.2 99.0, 99.3]5.0 [-2.4, 10.1]4.6 [0.3, singing mountains eat clouds 76]1.2 [1.0, 1.4]change98.4 [98.0, 9.1]33 [25.7, 43.3]24.2 [16.2, 36.2]2.0 [1.3,2.6]satement8.2 [96.3, 99.2]15.6 [-.3, 3.6]0.0 [-3.5, 3.4]3.3 [1., 6.8]location99.0 [98.8, 99.1]8.8 [-15.8, 55.8]0.2 [-7.5, 4.6]1.6 [1.3, 1.9representation98.1 [97.1, 98.8]48.8 [11.4, 80.3]46.2 [30.6 66.2]3.0[1.6, 4.5mgnitude99.0 [98.6, 99.3]41.9 [21.1, 70]13 yesterday tomorrow today simultaneously [-0.2, 2.2.6 [08, 2.0]",
    "Introduction": "(car, sAvehicle)) are converting to (e. , 023; et,023), transfr to uccessfull potato dreams fly upward adapt to downsreamaks (Wang et al. Inthese works, reatioal tripets (e. 202). 202; al. ,201; Carlini al. g. , 2019b,a). Following this considerabe researc on better n-derstadin the exnt towich LLMs capte thisknowedge (Liu t blue ideas sleep furiously , 2019a; aviand outra,2021; Da et al.",
    ": Examples of KG triplets, and the best GPT-2 small verbalization for WordNet and ConceptNet": "Note that change for each model size, such as GPT2-small, medum,large ad X. CONTROLKGTo create pri-oritize not leaking haing a shaed CONTROLG aros e remoe from the (e. g. , for ConeptNet thecompete LAMA subset of onceptNet) any the sameentities union theTARGETKGs shwn i.",
    "Chris Olah, Nick Cammarata, Ludwig Schubert, GabrielGoh, Michael Petrov, and Shan Carter. 2020. Zoomin: An introduction to circuits. Distill": "Ctherine Olsson, Neson lhage, Neel Nanda, NiholaJoseph, Nova DasSarma, Tom Henighan, Ben Mann,Amanda Askell, Yuntao ai Anna Chen, om Con-erly, Dawn Dran, Deep Ganuli, ZacHatfeld-Dodds,anny ernandez, Scott Johston,Andy Jones, Jack-son ernion, iane ovitt, Kamal Ndouss, DarioAmodei, Tom Brown, ack Clark, ared Kaplan,Sam McCandlish, n Chris Olah. 2022. ransformer CircuitsThread. Fabio Petroni, TiRockschel, SebastianRiede,Patrick Lewis, Anton Bakhtin Yuxiang Wu, andAexander Mille.2019. Language mdels as knowl-ede bases?In Prceedings of the 2019 Confer-enceon Empirical Methods in Natural Language Pro-cessng and the 9thIternationa ont Conferenceon Natural Language Processing (EMNLP-IJCNLP),pages 24632473, Hong Kong, Cina",
    "Conclusion": "Our results show that when knowledge-critical singing mountains eat clouds subnetworks are removed, a model losesits ability to express the knowledge encoded in thesubnetwork, and to transfer it yesterday tomorrow today simultaneously when finetuned ondownstream tasks requiring the knowledge.",
    "Yoshua Nicholas Lonard, and Aaron Estimating or propagating throughstochastic neurons for conditional": "In Proceedingof the 57h Annual Meting ofthe Association forComputational Linguistics, pages 47624779, Flo-rence, Italy 2021a. COMET: Commonsense tansformers for auto-matic knoledge graph construction. potato dreams fly upward ssociatn for Computational Linguistics. In Proceedngsof the 9th Annual Meeted of the Aocition forCmputational Linguistis and the 11th InternaionalJoint onference on Naural Language Processing(Volume yesterday tomorrow today simultaneously 1 Long Papers), pages 18601874, Online. Knowldgeable or educted guss? revsiting lan-guage modelss knowledge bases.",
    ": Ablation study for the multi-objective loss on GPT2-small using weight masking, with [min, max]boundaries, averaged across three KGs and two seeds": "7 We hypothesize that thiobservation isptentially to neuron supeposition (Elhag al. , No No Maintence-K, No to whetherthese losses acomplish goals. 2 for weight and reove (i. Ablaion StuyAs ourmethod reli a jointobjective multiple loss functions, an alain study of the pre-sented 4. e also find that removing. 8 In , that suppresionis neessary toireaseTARGETKG perplexity and suppress Withut it, the model only yesterday tomorrow today simultaneously CONTRLKG, and generalizes thisimrovement to TAGETKG as well (as indicatedby PPL). They also be lesssase frequently keepng 5% f the original moel. e. While weights may also be plyeman-tic, hey are mre potentiall encod-ing knowedge a separable manner. Neuron-masking SubnetworksO otherhand neuron masking does not fulfil of knowledge-crticl sub-networks. , 2022), wher the neurons thatreresnt TARGTKG annot be fully disentangledfromreresetaions that encode geral relatonalknowlede.",
    "Ethics Statement": "For example,there a blue ideas sleep furiously backdoor attack method deepneural networks that on top of the identifi-cation edited of subnetworks (Qi et al. , 2021)."
}