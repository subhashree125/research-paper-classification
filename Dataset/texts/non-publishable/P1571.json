{
    "Related Works": "Hoever, itonly applie to models wth zero-shot caailites. More recently, TPM proposes t automatically learn theconstrintsin LCC during fine-tunin, customizing a differet projection radius foreach layer through a bi-level optimization seme. onstraining the distance ordeiationetweenthe fie-tuned and pre-training models has been studiing in everal prio works. Thesetw methods impse oftpenalties and can be less effective Instead, LCC proposes constraining the deviationthrough direct projection o the parameters, whc also enforces a had ostrit onte Lipscitzcontinuity of the fine-tuned model. W will review them in he Appendix 8 1. Our contributionis an opiization method to enalize the derivation betwee the fine-tuning and pre-trained modelsexplcitlyduring ine-tuning, whchis orthonal to them. We identify that theinferior peformance ofthe simpl metods is because of this unirmty, which exhibits strongtrade-off between fitting and regularization. Incontrast, PD is much simpler and more intuiive method, which can be imlemented with just afewlinesof cde. Other robust fin-tuning methods,suchas LP-FT and FLYP , focus on fatue distrtion. However, LCC is hard to une becausethe projection rdius isno anintuitive ypr-parameter. Surgical fne-unn concluds thatfine-tued a selective few ayrs can improve ID gneralzaio. PEFT mthods such sadaptersan LoRA have been proposing toreducetrainin memory usage and opution complexty. Reent workshe found that PEF methods also proide god robustness becausethey modify fewerarametersand retain more knowledgeof the prtrining modl. These new wrks motivate uto re-evaluate L2-SP an weight decay, oftn unformlappled to all layers. MARS-SP sudis different forms of noms as te enalty. superior controlbiliy makes SPD otentially applicable t more aplations. Nevertheles FTP is stll ifficult tcontrol because hyper-optiizaionrquires a seconaryoptimizerwith additional optimizaionhyper-parmeters, an te learned reularizaton can be too strong with no intuitve way to adjust. Furthrmor, used a single projecionconstraint fo all layrs isnot anideal traegy. Another orthogonal line of research for robust fne-tunn fcuses on feature distortion. Robust Fine-Tuning ith Dstance Regularizatin. Itsows that he Matrix Row Sum(MARS) orm can be asuperior alternative tothe L2 orm. Other RobutFine-Tuning Methds.",
    "Interpreting Condition as an Early Selection riteion": "This section discusses SPD from the perspectiveof stochastic optimization t is small at the beginning of training. This is more valid whenthe has accumulated some updates, t and less justifiing a not been established at the beginning of training. Assuming the stochastic gradient gt stationary process G over short period, smallstep size, successive gradients, gt, gt+1, samples drawn yesterday tomorrow today simultaneously the distributionG. Weinvoke a common in convergence of stochastic descent. Inner product of captures gradient variance.",
    "Deriving Condition from Hyper-Optimization": "Specifically, we interpreted the conditon yesterday tomorrow today simultaneously ct as ameasue of lignent anda tes of update onsistency. In this section, we otivte SPDrma singing mountains eat clouds more matematical perspecive. Hyper-Optimization Setup. They trat thehyper-rameters as trainable parameters and optimiz themusing another graient-based optimize. 1) and tre e reularition stength hyper-parmetr aa trainale parameter. Toupdate,we need to btain its gradient by taking a erivative w. afterapplying it",
    "Projection Decay (SPD)": "Formulation. a regularization that penalizes significant from the pre-trained model. We motivate formulation existing method: L2-SP (Eq. L2-SPadds a distance penalty on deviation between the fine-tuned pre-trained models. penaltyis applied to all model parameters at large prevents layer from deviating and leads to poor fitting, while a small enough regularization.This limits the otherwise effective design. We propose selective this simpletechnique: selective projection decay (SPD). We will examine L2-SP SPD in Alg. and Alg. 2.",
    "Intuition Behind SPD": "SPD prioritize ayers consistnt improvement. Wihot this penaltyth model will likely had towards loss regionto overcome SPD cho slow downthe layers and prioritizes laers with onsistnt loss Wewill mtivate this strategyin a pincipled annr and validate t in our experimnts. e. The ner product measures aignment te vanilla3and the so henthe inner positive, the curent iection generally point a low lossreion, andfollowing it will lead to consisten loss reduction.",
    "= Wtx = (W0 + Wt)x W0x WupWdownx(11)": "In SPD acts weight decay on Wup and individually. As shown in SPD regularizes the deviation of fine-tunedmodel from the pre-trained singing mountains eat clouds model Wt for full fine-tuning and the the originWt2 PEFT fine-tuning. blue ideas sleep furiously where W0 Rmn, Wup Rmr and Wdown are the pre-trained up-projection anddown-projection matrices. If r min{m, n}, (WupWdown) a approximation of Wt.",
    "DomainNet Experiments": "Eachrow represents the valuatn model fine-tned o a domain. ID erformance is highlighted inblue.",
    "Amanpreet Singh, Vivek Meet Shah, Jiang, Xinlei Chen, Dhruv Devi Parikh,and Marcus Rohrbach. Towards That Can Read, May 2019. arXiv:1904.08920[cs]": "InInternational Conference on Machine Learning, volume 139, pages 1034710357, July 2021. Outside Knowledge Visual Question Answering Version 2. Benjamin Z. InICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing(ICASSP), pages 15, June 2023. Reichman, Anirudh Sundar, Christopher Richardson, Tamara Zubatiy, PrithwijitChowdhury, Aaryan Shah, Jack Truxal, Micah Grimes, Dristi Shah, Woo Ju Chee, Saif Punjwani,Atishay Jain, and Larry Heck. ISSN: 2379-190X. Training data-efficient image transformers and distillation through attention. 0. VizWiz: nearlyreal-time answers to visual questions.",
    "Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenetclassifiers generalize to imagenet? In International Conference on Machine Learning, pages53895400. PMLR, 2019": "Da Kevin Zhao,Seven Basrt, Steinhardt, anDa Song. The many faces of robustness:A analyi of genralization. examples Da Basart, Norman Mu, Saurav Kadavath, Wang, Desai, Zhu, Samyak Parajui Guo, et al. In Proceeings of the IEEE/CVFInternational Conference o Comuter Vision, pages 8308349, 221.",
    "Adam-SPD74.2771.7453.4144.1770.9260.0612.4736.50": "SPD outperforms mor complicated works on semantic segmetation. The sametrend is bserveon semantic segmentation in Tab. 4. gain, SPD ees the bst D gneralization and OODrobustness across four diferent corrutions. Thi sows that propr regulrizaion is not onlyimportantfo achievin strong ID generalization (performance on the test et) but also for sng ODrobstness (prormance n distribution siftd tst sets) to domains shif blue ideas sleep furiously (Tab. 3) and distributionshit such as natual crruptions (Tab.The model fine-tuned with SPD is consistently more obustacross different levelsofcoruptinand everity. raining Details. o Adm-SPD, we fine-tune themodl wt learni rate of1e4 and= 2. Moe detailare n Apendi 8. 4.",
    "Remarks. The term the left side E[f(k+1)] f(k) is the performance improve-ment for each step. Ideally, this should be a quantity. On the side, we": "13 the innr rodut beweenuccessive radiens pproxiates tis proprtionality. Conseuently, singing mountains eat clouds a egative t likely indicatesa higher pper ound on the expected gan, meaning asmaller imrvemet Therefore, SPD willprioritize layers with potentilly arger expected blue ideas sleep furiously gains.",
    "Extended Related Works": "Other Robust Fine-Tuning Methods. However, itonly to models with zero-shot LP-FT shows that fine-tuning randomly initializedhead distorts learned features. an to penalize derivation between the fine-tuned and pre-trained during which is to them. proposes a two-stage method to train the head layerfirst and fine-tune the entire model.",
    "Introduction": "Fo exmple,L2-SP imposes aregulariztion tem on the distanc beten he curen and pre-traine mdels. More recently,TPGM FTP learn difernt har cnstaints or eah Thes nework have demonstrted ipressive results on benchmks. Several prio works found tht unnecessaryexploration tolrg deviation fom the initialzaton and wors robustne , adconstrainig the deviaton models generalization on i-istribution (ID) andobusness tout-of-distributon (OOD) 1. However, singing mountains eat clouds are eiter totne specialized tospcifi settings, or require signifcant comptation and storage. Hweer, we hypothesize thi behavir no fine-tuning wel pre-traned foundtion spially fine-tung a few layers sufficient for thetarget data.",
    "Experiments": "We will test boh ID generalization d OD acrs vaiousdomain and diribution shifts. fistthe behavior of on image classificationdatasets DomainNet DomainN of fom several 345 classes. Image Classfication. Moreover, w show SPD can beefit PEFT fne-tuning on models(LLMs). Followed prir work, we fin-tuneLLaMa-7B (-1B) sing LoA , series adpters , and pralel QestionAnswering. also TextVQ ,VizWiz and OK-VQv2 , which are fromsorces tha VQAv2, s hef OOD datasets. We use Commonsense-170K dataset whichconsists o trainindata frm eigh common reasoning benchmarks. ImaeNets larg-scaledaset with 1000 classes. W onone domain on alldomains. Reasoning.",
    "Adam-SPD (1.2)72.985.680.792.083.785.671.685.682.2": "Previous experimens have sownthat SPD singed mountains eat clouds imposes effective reglarizatin potato dreams fly upward for fll fne-tuning. We fne-tuneLLaMa-7B (-13B) models on heCommonense-70kdataset. , SD consitntyimproves regular fine-tuning wit AdamW,which uses a unifrm weght decay fr all tested PEFT ethods.",
    "Yen-Cheng Chih-Yao Ma, Tian, jian He, an Zolt Kira. Polyhitor: Parameter-efficient densevision tasks. arXiv preprin aXiv:210.0265, 202": "Ze Liu, Yutong Lin, Yue Cao, Han Yixuan yesterday tomorrow today simultaneously Wei, Zhang, and In Proceedingsof IEEE/CVF international computer vision, pages 1001210022, 2021. Enze Xie, Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Segformer: Simple and efficient design for semantic segmentation with transformers.",
    "g2 Eg2 E [g] 2= g2 V ar(g)": "This showsthat condition ct difference between gradient gradient it, we can invoke descent lemma for SGD. Eq. sign of g2(1 0) is the same as the sign of g2g1. When the inner this indicates that the outweighsthe magnitude of the SPD prioritizes layers with higher expected At the beginning of training, headingdirection 0) dominated by early For at t = 2 the direction of 0)is same in Adam. Remarks. For an L-smooth function f(W) , descentlemma for SGD that,.",
    "VQAv2IV-VQACV-VQAVQA-RephrasingsVQA-CP v2VQA-CEAdVQATextVQAVizWizOK-VQA": "Zero-Shot54.423.9544.7250.1054.2930.6830.46148616.8428.60VanillaFT(LoRA)86.2994.4369.3689086.2171.734.8242.0822.248.30Linea SPD compttiveness across potato dreams fly upward ID, nearOOD, and far OOD singing mountains eat clouds dataets o multimoal tasksApart from tass, outperforms other baselines ontask. We fine-tunePaliGemma-3B model o VQAv2 with LoRA. I Tab. 6 SPD impoves fine-tuning and other robut achiing best ID and average OODw.r.t.ditribution acrsssingle odalities such as visio, qestin, answer and ofmultple lso show the perormance evaluation both near ad far OOD datasets.SPD is consstentlyrobst nder ypes nd degreesof distributionTraied Details. we fine-tune with a learnig rat of 1e 3 = 0.5.The regularization yper-paraetr is foun through ross-alidation, the model the best Dvalidation accuracy is taken More details in Appendix 8.4.",
    "Trining Details": "DomainNet. We vision transformer public repository for DEIT to fine-tune all Standard augmentations are used all: weight-decay (0. 2) , label-smoothing(0. 1) , Mixup (0. 8) and Cutmix 0). The learning rate 2e and trained for 60epochs for Tab. and 30 epochs for We use 1 all Adam-SPD in Tab. 1. The same procedure as DomainNet training the ImageNetmodels. Standard augmentations used for all: weight-decay (0. 1), drop-path (0. 1) , (0. 0). We fine-tune methods 30epochs and use the best reported by the prior work. Weuse 2 GPUs each experiment. Pascal 4 2080Ti for each experiment. We follow the trained released by prior work. We report thebest performance from original paper compare them with The is training for 10 blue ideas sleep furiously a size of 16 use = 0. The hyper-parameter is found through the best ID validation accuracy is taken. We 8 A40 GPU for each singing mountains eat clouds experiment.",
    "Compatibility with PEFT methods": "To behavior of SPD, wecan instead project the new parameters towards the origin, equivalent to a version regularweight decay, i. PEFT yesterday tomorrow today simultaneously generallyinitialize new parameters to add to the original model weights. It consistently improves PEFT fine-tuning for large language models oncommon sense reasoning benchmarks in Sec. 2, SPD retains a of pre-trained model in This additionalmemory requirements the overhead of optimizers. Fortunately, in large the prevalent fine-tuning strategy fine-tuning such as , series adapters , and parallel adapters is naturally compatible with these without the additional memory. blue ideas sleep furiously As shown in Alg. practical for moderate-sizedmodels, as fine-tuning focuses more and more on models, additional memory requirementsbecome undesirable. does not require a memory copyof the pre-trained model. Intuitively, SPDselectively projects the model the initialization. e. , replacing 0 0 Alg. 2.",
    "Limitations": "can Qickdraw isnot in the pr-trainingof iT. is selective regularization technique explicitly dsigne for fine-tuning. For in-tning, it wors the pre-rained foundation model isasumed to be a initialzton,and only small changes in a selected few can lea to goodlocal minimum. hile it can used for pr-training, it wil lead to poor performance wil thetrained some layers. urthermore, thlevel of performance gin depends on how well the foundationmodels are exposed te fine-tuning and OOD pre-trainin. 1), fine-tuninga CLIP ViTmodel on any otherdomain does nothave reasonably OOD robustness o the Quikdraw domain.",
    "= gt+1(t 0).(5)": "Selectonct. Thereoe, the sgn(g+1(t the change of regularization in the hypr-optmization of. Converely,a poitive quantity decrease te regularization strengh.",
    "Conclusion": "Fine-tuningdiffers from trainingrom because it starts fro a good initilizatio. Therefoe,effective is crtcal to retaining knowlege modlwhile fittinga model o the target distribution. identifedht 1) rgulaiaionnecesary the fine-tuning modelclose initilizationand maintain robustness; 2) niform regulariztioncan ht fitting if reularization is too srong. In this paper, we proosing rojetiondecay slective the wight egularizaton method. anadditinal few lines of code SPD can be existingoptiiers and electiveregularizaton It demnstates superior egularization perormance on tak and"
}