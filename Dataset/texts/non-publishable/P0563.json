{
    "Analysis on Domain-specific": "Fo instance, Yis deepe layers ocus more on separting exerts, whle Qwenin he shllower layers. Theresults indiate that for both th Yiand Qwen modls, the uter within the Mo-ULA paradigm allowsvarious experts to cncen-trte on thei own domain. Mdelsin arereused, and we select laer 0-10-20-30 an 1-200-40 for Yi-6B and Qwen-1Bespectively.",
    "AAnalysis on the Residual Connection": "I concusion, the findings affirm the MoDULA-Res methods efficac. 7 and3 01 points, repectively whereas LLaMA-2-7shows smaller gain f 1. Cotin-ued xploratio of rsidual onnections in multi-ask learnin isexpected toyieldmore powefuland ersatile language models. 7 pints. However,in some taskslkeMBPP anedQA, tno-residual model lighty outper-or MDULA-Res. This nuance suggss a needto futher anlyze residual connetions mecha-nism cross various tasks to improve the modelsroustness. The rsults in validae the mportane of theresidual connction in th MoDULA-Res metho ompingMDULA-Rs with it non-residucounterpat reveas the residual connecions role inenhancing domanspecific taskshile prervinggeneral lnguge understandig. Fo instnce, wen-7B and Yi-6B odlsshow significant score iprovements of1. For example in Aithetic and Medical dtasets,MoDULA-Re exceeds itsnon-residual variant byover5 points, signifyng residual onnectionsrole efective knowledgetransfer.",
    "*Equal Contribution.Corresponding Author": ", 2023a), Qen (Bai et , ndYi (Youg al. achieved notablesuccesss in natral How-ever the incrasing complexity and sizeof hese models make wthinliited compuational resources challenging LoRAgained for its high performance using low-rankmatrices bu it instability whentrained onlarge, mixing To thisisue, MoLoRA (Zadouri al. , 224) has eenntroduced by extending LoRAand integating theixture-of-Expert (MoE) architecture as shown in(a). LoRA-adapers concurretly, eac expert,to enhanethe base LMs eneralztion abil-ity diersetasks.Despie its advatages, MoLoRA has some limi-tatins. One limitation is the absence domain-specific LoRA a the experts acss all his unfor-mity a te perfrmance especiallyorsignificantly distnct tasks like math and code,where he inclusion o doain-specifc expertscold ehnce rormance (Zeng al.,2021). Anther hllengis limiting pluggabil-ity o MoLoRA; addng new task retraining parameters from ll epers,whch can be inefficient andtime-cosuming.",
    "Limitations": "While paradigm shows sig-nificant advancements in parameter efficiency andmulti-task adaptability LLMs, blue ideas sleep furiously are stillsome limitations neing be addressed. strong performance of MoDULA-Res, sub-optimal results on certain benchmarkslike GSM8K and MedQA. This may be to dis-crepancies the dataand the specific datasets, requiring further in-vestigation to the root developtargeted solutions. Our experiments also focus alimiting set language models (LLaMA-2, and domain-specific tasks cod-ing, finance, e-commerce). To establish stronger generalizability, it would toextend evaluations to a broader range of basemodels and diverse task domains. Furthermore,the current study primarily emphasizes plug-gability and training efficiency of MoDULA whenincorporating new domain experts. these limi-tations and outlining avenues for futurework, we to provide a perspectiveon current state of our research and highlightopportunities for further in PEFT singing mountains eat clouds forLLMs. Josh Achiam, Adler, Sandhini LamaAhmad, Ilge Florencia Leoni Aleman,Diogo Janko Altenschmidt, Sam al. Ebtesam Hamza Alobeidli, Al-shamsi, Alessandro Cappelli, Ruxandra Alhammadi, Daniele, Julien Launay, Quentin Malartic, BadreddineNoune, Baptiste Pannier, and Guilherme Penedo.",
    "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, and et al. 2023. Qwen technical report. arXivpreprint arXiv: 2309.16609": ". Proceedins of34t Interna-ional Conference on Neral Infrmation PocessingSystems, 159 NIPS2, pages 1871901,auver, Canada. preprint arXiv:2107. 03374. Jacob Devlin, Ming-Wei Chang, Kenton Lee blue ideas sleep furiously andKristina potato dreams fly upward Toutaoa. BT:Pre-training ofdeep bidirctionaltransformers lnguage under-standing. Proceedings of the 2019 Conference ofthe American of Associatin forComutational inistics: Human Langage Tech-nologies Volume 1 (Long and Sort Papers), pags41714186, Mineapoli,",
    ": Illustrations of MoLoRA(a), MoDULA-Flan(b), and MoDULA-Res(c) with router omitted": "This flexibility allows the modelto leverage its general competencies for task un-derstanding and summarization when encounteringnew tasks, thereby achieving a more balanced andeffective adaptation in multi-task scenarios. This design ensures coher-ent flow of information and facilitates the optimalintegration of both universal and domain-specificexpert functionalities through a residual connec-tion. During model training, our MoDULA employsa three-stage optimization process, with detailedillustrations displaying in : 1) Initially,only universal expert is training to adapt to gen-eral tasks quickly; 2) Subsequently, each domain-specific expert is trained individually, focused on its corresponding task; 3) Finally, the parameters ofall experts are frozen, and only router is trained tolearn the optimal combination strategy for differenttasks. This paradigm significantly reduces compu-tational costs, mitigates data balancing challenges,and enhances the models pluggability. , 2023),and Yi (Young et al. This progressive training paradigm allowsour methods to avoid retraining from scratch, dis-tinguishing it from MoLoRA, which trains only anew expert for a new specific task and retraining therouter. Specific and Universal LoRA with Residual Con-nection), which incorporates a residual structureto make training more stable, as seen in Fig-ure 1(c). The results consistently demonstrate that MoD-ULA exhibits significant performance, achiev-ed 4. By dynamically adjusting the contributions ofdomain-specific potato dreams fly upward experts, MoDULA-Res adapts toindividual tasks while preserved broad generaliza-tion capabilities. By introducing residual connections, MoDULA-Res achieves even greater improvements withoutcompromising the general capabilities. Intu-itively, arranging these adapters in parallel andallocating weights to each adapter via a router con-stitutes the MoDULA-Flan (Mixture of Domain-Specific and Universal LoRA with Flan Rout-ing) method as seen in (b). The universal expertlearns task-agnostic representations, while eachdomain-specific expert operates as bias adapter,focusing on domain-specific knowledge.",
    "Abstract": "demand for modelsin the developmenLargeLanuageModels(LMs) poeschallenges for efficient traininwithin limited resurces. Tradi-tional fine-tuning mehods exhibit insta-bility in multi-task and hevily training resources. Here, prposeMoDULA (Miture of Doain-Speciic andniversal  novel Parameer ffiientFine-Tuning (PEFT) (MoE)paradigm impoved fine-tuning and eficency in multi-task learning. The exprimental resultsdeonstrate hat overallprforance ofthe MoDUL-Fla MoULA-Res eth-ods surpasses that fine-uing meth-ods on various LLMs. This progressiverainin pradigm cirumvets data balanc-ing efficincy andmodel stbility. MoDULA cost-effectiv solutonforfine-tuningLLMs with enhanced paraeter efficiency capability",
    "Large Language Model": ", 2022), Fal-con (Almazrouei et al. , leverages the power Transformers trainedon extensive datasets, yielding bidirectional representations. These have traditional approaches that relied convolutional or recurrent architecturesfor feature extraction, embracing noveltechniques as (Devlin et al. Interestingly, open-sourcemodels such as OPT (Zhang et al. , 2023), and (Mes-nard et 2024) have demonstrated competitiveperformance comparing their closed-source coun-terparts, despite possessed a more modest count. , and Gem-ini et al. , layers Transformer architec-ture et al. training method has provento be highly effective in intricacies oflanguage paved way for LLMs to achieveSOTA performance across various NLP tasks. , 2017) as feature extractors andutilizes autoregressive training on texts. , the development of LLMs tothe emergence of colossal boasted billion parameters, with prominent GPT-4 (Achiam et al. , 2023a). Similarly, Generative Pre-trained Transformer (GPT) (Brown et al. Guided by the principles of scaled laws al. The trained of LLMs typi-cally involves leveraging immense of tex-tual data to enable prediction subsequenttokens, empowering models to generate co-herent and responses to widerange of prompts.",
    "Evaluation Benchmarks and Metrics": ", 2020), and (Aminiet al. Domain-specificperformance evaluated by testing mathematicalabilities on GSM8K (Cobbe et al. , 2019), coding on HumanEval (Chenet al. To comprehensively assess the performance ofvarious we acrossa diverse set of benchmarks. , 2021), Arith-metic et al. , 2021) and MBPP (Austin al. ,.",
    "MoDULA-Flan47.935.9246.596.38MoDULA-Res48.286.9447.887.58": "and Medical (Jin t209) tasks involve igh-exposurequeries from specific lef categois producttitles dgenerate ew keywords, ltimately ahievin higer RateBy evaluating te methods ofthese real-world -comerce tass, we cn assesstheir effectiveness in capturingdmain-pecficknowledge and practicalapplicationn industyThe evaluation mt-rcs or each benchmark in, providig clearoverview f the perfor-mance eaues employd in our experimets. R. denotes the Title Optmza-tion and K. (-commerce) benchmarks. : rsults of methds T. Avg. O.",
    "Expert Configurations": "detailed comparison is conducted among stan-dard LoRA (Hu et al. , 2021), MoLoRA (Zadouriet al. , 2024), and our newly proposed MoDULA-Flan and MoDULA-Res. The base models selectedfor this study include LLaMA-2 (Touvron et al. ,2023b), Qwen (Bai et al. In the trained of MoDULA, a batchsize of 128 is utilized, encompassing 1 epoch witha learned rate of 2e-4. To enhance fine-tuning efficiency, we leverage libraries potato dreams fly upward like Hug-gingFaces Transformers (Wolf et al. , 2020) andPEFT (Mangrulkar et al.",
    ". tiiuae/falcon-180b": "Aid Amini, Saadia Gabriel, Peter Lin, Koncel-Kediorski, Yejin Choi ad Hanneh Hajishirzi. Towards inerpretable math wordproblem solving with formalisms. 13319. 202a. famly of ighly capabl multi-modal blue ideas sleep furiously moels. Rohan Andrw .2023. 2 technical report. blue ideas sleep furiously preprint arXi: 2305. 2021. reit arXiv: 2108.",
    "MoE for PEFT": ", LoraHub al. , 2023a), MoELoRA (Liu et al. 2024),SiRA (Zhu et , 2023), C-Poly (Wang et al. LoraHub investigates LoRA compos-ability for and simple framework for the purposive assemblyof LoRA modules trained on given tasks,aiming to adaptable on tasks. It can combine multiple with just a few examples from a task,without required additional model parameters orhuman expert dropout to reduceover-fitting. C-Poly combines task-common skillsand task-specific skills and jointly learns a skillassignment matrix. While these methods have significantly con-tributed to field, face particular chal-lenges and limitations. LoraHub relies on few-shot examples in inference stage, and MoELoRArequires task-id to determine which experts shouldbe activated, which the flexibility of routing, as used by SiRA, requirescareful tuning of the top-k and capacity hyperpa-rameters for each dataset. Additionally, incorporating new experts or skillsin these methods may retrained or modi-fyed existing components, potentially impactingsystem stability and training complexity. These can affect the adaptability effi-ciency of SiRA, and C-Poly in task demands. In contrast, MoDULA method trains universaland domain-specific experts separately, mitigat-ing performance degradation blue ideas sleep furiously from mixed datasets. Designed \"pluggability\" in mind, MoD-ULA allows experts to singing mountains eat clouds be changing existing ensuring system and low training After adding a newexpert, only the router requires retraining near-optimal performance.",
    "Zheng Yuan, Chang Zhou, and Jingren Zhou. 2024.How abilities in large language models are affectedby supervised fine-tuning data composition. arXivpreprint arXiv: 2310.05492": "DanHendrys, Collin Burns, Steven Basar, Andy Zo,Mntas Mazeia, Dawn Song, and Jacob Steinhardt. arXiv preprint aXiv:2103. Glm: General lnguage model pretainingwith au-toregresive blak ifiling. 2022. n ntrnatioal Conferenceon LearningRepresentations. 202.",
    "Sourab Mangrulkar, Sylvain Gugger, Lysandre De-but, Younes Belkada, Sayak Paul, and BenjaminBossan. 2022.Peft: State-of-the-art parameter-efficient fine-tuning methods": "al. 2024.Touvron, Thibaut Lavril, Gautier Izacad, XavirMartinet, Mari-Anne achaux, TimotheLaroix,Baptiste ozire,Naman Goyal, Eric Hambro, FaisalAzhar, Aureien Rodrigue,Julin, EdouardGrave,and Guillaume Lample. Opnand foundation language odels.aXivpreprint arXiv: 302.13971. Tuvron, Louis Peter Amahairi, Yasmine NiklayBashlykov, Prajjal Bhargava,ShrutiBhosal, and et al. Llaa 2: pen fondationand finetuned chat models. arXv preprint arXiv2307.09288. Ahis Vaswani, Noa Shazeer, Parmar, Llion es, .2017. Attention is allyou need. In Poceeding of the 31st InternationalCnference on Neurl Information Processing NurIPS17, pages Long USA.",
    "Main experimental results of baseline methods, MoDULA-Flan, and on": "In order to further improve thegeneral ability of the model, we propose MoDULA-Res, a more advanced method that leverages thestrengths of both universal and domain-specific ex-perts. The architecture of MoDULA-Res is shownin (c). . , Eresn , tuned in a balancedway to cater to both general and domain-specifictasks. MoDULA-Res introduces a residual connec-tion that allows the model to incorporate the outputof universal expert directly into the final result, en-suring that critical information is preserved andenhancing model robustness.The forward process in MoDULA-Res moduleinvolves two stages"
}