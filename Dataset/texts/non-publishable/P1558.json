{
    "Return": "baseliengm : Performanceomparison for if-feent nmber of training goals between ourmehodand Algorthm Distillation(AD), anin-ontext reinforcement learningmethod(Laskin et al. , 2022). Our method demon-strates similr performance wth less trininggoals (128 vs. 512) ad  general outper-forms the baseline n Key-toDoor environ-ment. In suc stricter dat enario t is abletoprrm n par with AD using 27x lesransins in total. ee for detals. In-contextlearnig is a powerful ablity f autoreressive model suc as transfomers Vaswani et al. ,2023) or state-spce odels (Gu et l. , 2022), low-ingthem t infer and slve taks from jst a fewexamples withou updatin moels eights (Bownet l , 2020). ). I-cotet Reinforement Learnin (ICRL) mehodstt learn fro offline dataets were frst introducdby Laskin t al.(202) and Lee t al.",
    "CEnvironments": "The maximum episode length is 40, and since we can control the location of the keyand door, there are around 6. The agent then resets to arandom grid. The agent does not know the positionof the goal, hence it is driven to explore grid. Without a key, the door will not open. The goal is to finda target cell, location of which is not known to the agent in advance. 2D POMDP with discrete state and action spaces (Laskin et al. 5k possible tasks. , 2022). The episode length is fixedat 20 time steps, after which the agent is reset to the middle of grid. The reward r = 1 is given forevery time step the agent is on the goal grid, otherwise r = 0. The grid size is9 9, where an agent has 5 possible actions: up, down, left, right and do nothing.",
    "Expected Max Return": "We demonstrate the ability of our method to generalize when thedata is extremely low in a more complex environment than Dark Room. We fix the total number ofgoals with 100, significantly shrinking the number of learning histories. We show that our method needs 27x data. The baseline method canno longer converge with that few data and its performance plateaus with the increasing number ofhyperparameter assignments. Next, we further restrict the amount and diversity of data available to train. We set up an experimentin Key-to-Door, a more comprehensive environment with the total of 6561 tasks, with only 100training tasks and 500, 750, 1000 learning histories. It can be observed from thatthe baseline method cannot find a model that is able to generalize to unseen goals in such a setting. In turn, out method demonstrates performance on par with what Laskin et al. (2022) report in theirwork. To ensure that our implementation of a baseline (AD) can solve the environments, we presentthe performance of a baseline that is trained on optimal hyperparameters in Appendix F.",
    "unseen tasks in-contex.the latter, authrs similarly it is ossible to generlize from a datsof interactions, optimal actions are also available": "In addition, in-context ability is transient (Singh et al. 2024),making training of such models unstable and training budget. Our workaims to solve aforementioned obstacles presents changes made to transformers attention heads,which can speed up training process and decrease the total amount of data needed forin-context to , 2022). Edelman et al. Several methods were proposed to tackle this problem (Zisman et al. propose hardcode this mechanism intoa an n-gram layer which is used with standard multi-headattention mechanism. , 2023) byeasing the data acquisition process. a transformer benefits from it learning complicatedbehaviour itself, rather it straightforwardly an bias n-gram provide. (2024) studied the emergence of these statisticalinduction heads synthetic data and concluded transformers obtain a simplicity bias towardsplain Akyrek et al. Both methods require specifically curated data, which is demanding to (Nikulin et al. ,2024) hard to predict its emergence from the cross-entropy loss alone (Agarwal et al.",
    "Method": ", as our The details implementation can found in A. We build our on AD (Laskin al. Learning optimalsolution can be delayed a to learn simple structures at first (Edelmanet al. To combat these we implement attention layer (Akyrek et al. In essence, it hardcodes of n-gram into the transformeritself, rather than waiting for them to emerge naturally. The attention pattern that calculated fromthe input sentence is defined as:. However, AD suffers the problems as any in-context does. , 2024). Besides, the of in-context ability is and can fade into in-weights the training complicated the emergence of adaptation (Singhet al.",
    "Experiment Setup": "The total number of goals is 81 and 651 for Dark Room ndKey-to-Door respectively. In Drk Room, an aent isspawne at the center and needs to fnd a goal, aftr whch it reeives a rward of 1 The tak forKey-toDooris simila but at first n agns seks for the key whch allows it to pe th door (othof tes actonslead to getted a rward of 1). We test our method on two nironments, DarkRoomad Dark Key-to-Door,orinally esentedby Laskin et al. 51. (2022) Both environments are grid-words o se 9x9. The more detailing decriptin can be ud in Appendix C. For each environment, we access he perormace ofICRL algorthms only on seen tasks. By doing so,we do not report themaximum performance a single chkpint, rather we show the expectedperfrmance for a crtain computational buget. By using this approach w smltaneously comparour mthod with a baseline in erms o easins of trainig and maximum achieed performnce. 0. 0. To show that our metod is more stable, we choose to report the results uing the Expected MaxPerformance protocol (EMP (Dodge et al, 2019; Kurenkov & Kolesnikov, 2022).",
    "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R.,": "Rms, A., Ziegler, D., Wu J, Wnr, C., Hesse, C., Chen, M, Sigler, E., Litwin, M., GrayS., Chess, B. Clark J., Berner, C., cCadlish, S., Radford, A.,Suskever, I., and Amodei, D.Language models are few-sholearners. In Larochelle, H., Ranzato,M., Hadsell, R., Bacan,M., and Lin, H URL",
    "We bring up these findings to ICRL setting and show that:": "heads decrease the amount of data needed for generalization on novel tasks. The are in. singing mountains eat clouds presented By employing n-gram oneneeds considerable less time doing hyperparameters search, thus making model potato dreams fly upward lesssensitive to and making it to train. By them, it is to reduce the total amount of in databy compared to the original method of Laskin et al.",
    "Gu, A., Goel, K., and R, C. Efficiently modeling long sequences with structured state spaces, 2022.URL": "Kirsh, L. , Harrison, J. ,Freeman, . , Shl-Dictein, J. ,Shmidhuber, J.on Shifts, 37th on NeuralInforation. R. Ney,H. In 195 ItenationalConference on Acoustics, peech, Sgnal Prcssi, pp.18114 1109/ICASSP.Kolesnikv SIn Chaudhuri, , Jegela, S. , Szepesvari, C. (ed. Proceedingsof the ntenatioal Conference n Machine Lrning, 162ofProceedings Machie Learning Research, p. 117291752. Wang, L., J. , Parisott, E. , Spencer, Steigerwald, R Hanen, S. ,Filos, A. , et alarXivpreprt arXiv2210. 2022.",
    "Conclusion and Future Work": "In blue ideas sleep furiously our workshow that heads ca significantly ease trainingrinforcement leaning , 022) approch. Frthersearch is t our method comatible with continuous which can applicability of the Also, ne considr sclig to larger models ad morecomprehensive environments e. While we believe our findingsare promising, there are some limitaions current work. XLand-Minigrid (Nikulin et , 2024a) or eta-World (Yual.",
    "Singh, A., Chan, S., Moskovitz, T., Grant, E., Saxe, A., and Hill, F. The transient nature of emergentin-context learning in transformers. Advances in Neural Information Processing Systems, 36, 2024": ", Oliver, N. PMLR, 2127 Jul 2024. In Salakhutdinov, R. , K. Berkenkamp, (eds. In-context reinforcement learningfor variable action spaces. ), Proceedings of the 41st Conference onMachine Learning, volume 235 of of Machine Learning Research, pp. ,Scarlett, J. Sinii, , Nikulin, Kurenkov, , and blue ideas sleep furiously S. Weller, A. URL.",
    "Mller, Hollmann, N., Arango, P., J., and Hutter, F. can bayesianinference. arXiv preprint arXiv:2112.10510, 2021": "Nikuli, , V. , Zismn, S. , V landminigrid:Scalable metareforcemnt envirnments in ja. kulin, . , Sinii, V. Kurenkov, , ad Kolesniov, S. A arge-scale multi-task dataset for in-contet learnin. arXiv preprint arXiv:2406. 08973,2024. Elhag, N. , N. , N. , DasSarma, N. , Henighan,T. Y. , et l. In-ontet learning and inductin heds. arXiv preprint arXiv:209. 1895,2022.",
    "AModel Implemntation": ", 202; inii al. , 202). , 2021), which implementatinis taken fro CORL (Taraov et al. build our yesterday tomorrow today simultaneously moel onthe bse Decision Tansformr (Chen et al. modify it yremov completly potato dreams fly upward Sincein RL wewih tules of actions rewards, concatenate one large\"token\"to preserve legh ize si (Lee etal."
}