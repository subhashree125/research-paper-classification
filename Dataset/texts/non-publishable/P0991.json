{
    "PPO": "By identifying different aggrega-tion functions M () and node update functions U (), the updatemechanism in each layer can be denoted as:. adapter modifies the latent representation by adding the delta , which is optimized usingPPO to align with principles designing by humans. is thedimension of the hidden state. molecule are often denoted by a subgraph = (, ) of the graph, which satisfies and. The RLHEX model uses the molecule to be explaining as the input andcreates the CF explanations from the modified latent representation +. Given a set of molecular graphsG = {0,2, ,} and ground-truth labels {0,1, ,}where {0, 1}, GNN classifier () is trained to predict theestimated label = () for each input graph. The molecular property prediction can be modeling as a binarygraph classification task. It aims to predict whether the input mol-ecule has a certain chemical property, such as whether the mol-ecule is active against AIDS. : RLHEX has three main parts - the VAE-based generation model, the adapter, and reward module. The reward modulecontains several reward functions basing on principles designing by humans, which make generated explanations easier fordomain experts to interpret. Typically, GNN learns the representation of each node R by aggregating the information of its neighbors ().",
    ":C C + {}": "PPO operates b otimizingan bectv that balances exlo-rton anexpoitation singed mountains eat clouds currentpolicy od, to gain more d expoe ew. to saple themean of distribution). The reward (,) sbsed on the sores fom humandesiged (Eqn 1). standard yesterday tomorrow today simultaneously is thefollowing:. Our goal is learn apoliy ()that optimal latnt distribution for the F eplaationsW leverae Proximal olicy (PPO) that satisfyprincipes Itextenvelyused to ster odls into producin human-desiring output in lan-guae mdels, using LHF (einforceentLearnng from umanFeebak) and e take ispiration from these touild more flexible system hich different forms ofhuman-guided prnciples can euse optimize our explanationmodel. This balance hroughclipped proabilty ratios, ensringthat updates do not deviatetoo farfom th current policy.",
    "Zhi Wang, Chicheng Zhang, and Kamalika Chaudhuri. 2022. Thompson Samplingfor Robust Transfer in Multi-Task Bandits. arXiv:2206.08556 [cs.LG]": "2023. Zhening Wu,Jike Wang, Hngyan singed mountains eat clouds Du, Dejun Jian, Yu Kang,Dan L, PeichenPan, Yafeng Deng, DongshengCao, hng-Yu Hsieh,et al. Association for ompuig Machiney, NewYork, NY, USA,29882998. 43043. Jixan You, Bowen Liu, Zhtao Ying, Vijay Pande,and Jr Leskovec. iang Yang hangsheng Ma, Qiannan Zhang, Xin Gao, Chuxu Zhang, anXingliang Zhang. 223. In Intenational Confeence on Learnng Represtations. Drug Dscoe Today:Technologes 37 (2020), 12. Felix Wong, EricaJ Zhen, Jacqueline A Valeri Nina MDonghia, Meis N AnahtarSatotaka Omori, AliiLi, Andres Cubillo-Ruiz Aarti Krishnan, Wengon Jn,et al. Hao Yuan, Jiliang Tang, Xia Hu, and huang Ji. arXiv peprint arXiv:235. ature Communications 14, 1 yesterday tomorrow today simultaneously (2023), 285. MAR: Markov Molcla Sampling forMultiobjecteDug Dicovery. 2020 Xgnn: Towads model-leel explanatios of graph nural networks.",
    "METHODOLOGY": "Then w itroduce te bac-bone framework to generte CFcandidates and use Proximal PolicyOptimiztion (PPO) to align the canddaeswiththese humn prin-ciples. In this section, we ropose a nove global exlanationframeworkRLHX o alignglobal CF explanations with human-designe prin-cipes. We first discuss severl dsirable proprties for the optimalglobal CF explanation of molecles.",
    "PRELIMINARIES3.1Molecular Property Prediction": "molecule be intuitively represented a graph = (, ),where R| | is a set of corresponding atoms singing mountains eat clouds and R| || | is a set edges corresponded to bonds. | | the number atoms. are feature dimensions ofthe atom and bonds",
    "w/o PSVAE0.1680.9230.9000.7990.7130.846w/o Adapter0.6150.8570.8870.7910.8170.833": "with randomly initialized parameters and is trained from scratch. Noticeably, absence of the pre-trained PSVAE results in sig-nificant reduction in coverage on the AIDS and Dipole datasets. RLHEX achieves the highest coverage after iteration 3 singing mountains eat clouds illustrates how the performance is by the numberof iterations process. For the an iteration is defined as one over the set of G. For GCFExplainer, the of the randomwalk is as |G|, denotes the iteration number. Asshown , reaches optimal performance after oneiteration and maintains a stable performance after 20 iterations. However, the best performance at firstiteration. number other startto GCFExplainer. In the dataset, the GNN pre-dictor classifies the input graphs as negative, or label=0, indicatingthat these molecules are inactive against AIDS. similarity allows RLHEXto encapsulate the behavior of the GNN predictor variousinput molecules by grouping negative molecules together. It further necessary conditions that would promptthe GNN predictor to alter prediction.",
    "),(17)": "We check the validity of candidate ex-planations and only valid with opposite predictionsfor final candidate set. We follow standard of other hyperparameters papers. 1 and halved 10steps.",
    "Expert Assessment on CF Explanation": "Although the evaluations lacked empirical testing,the expert was in general alignment with the explanationsabout specific classes of molecules. Through this procedure, thechemists could the efficacy of the GNN classifier alignment known chemical knowledge. Based on the caseshown in , the observations:AIDS The CF candidate is to be active against AIDS. Although the input negative molecules have the fused aro-matic rings (-OH) groups that formimportant interactions viral targets, the fused 3-ring systemof the CF candidate increases its possibility of The fused 3-ring system resembles known HIV inhibitorpharmacophores. Dipole CF candidate and the input molecules haveO-H and N-H bonds, which are polar due to the electronegativitydifference between oxygen and hydrogen. However, the CFcandidate a bent geometry, which allows the bond toadd up and a net dipole.",
    "Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2020. Deep learning on graphs: A survey.IEEE Transactions on Knowledge and Data Engineering 34, 1 (2020), 249270": "2020. Ziegler, Nisan Siennon Jefrey Wu Tom B. Christiano, and Geoffrey Irving. 019. Fine-Tuning LanguageModels from Human Preferenes. arXiv:1909",
    "#Graphs156234613539#Nodes per graph15.7330.349.16#Edges per graph16.3230.8018.53#Atom Type9109#GNN Accuracy97.81%80.00%89.37%": "PSVAE-SA appliessimulated annealing to the sampling toward thereward function. of graphs and potato dreams fly upward a greedy summary to deliver low-cost candidate sets for input The maximum of therandom walk is set 6000.",
    "(1) The generated explanations should be counterfactual to theinput molecules": "Tis yesterday tomorrow today simultaneously is morewith the domainknoledge easy thcemists For gnerated raphsshould not violate the implicit valence andriginfomatio. one drivd from the definiton theoptimal local CF explanation. toinroduce three metris o formlly define the requirement: and ize The size is denoted as |C|Covrage s easure of blue ideas sleep furiously the proportion of iput graphs G can be covered byexlnations in under a givendistancthrshold :. (2) The generated should b valdmolecle. The CF explanaton set shoul besimilar the input o it can revel eessaryfeatur the GNNpredictor to change their predition These ofthe explanaton shol lso besmall t ensue it durablefor expets to check Here follow et al. (3) The explanatonset for an exprt tmanually ealuateas my iput molecules. The ne s th basic for requirigthe counteractual exlanation tohave adifferent input molecule : () (). Th second one ensures thatthe generted explanations chemically structres tocheists.",
    "and 3 show the coverage and cost on the test set of threedatasets with = 10 after 20 iterations. We ran the experiments5 times with different random seeds and calculated the average": "RLHEX has potato dreams fly upward the highest coverage with different. Howeer, RLHEX ca find a better gobal explanon set closer tothe input et for different. Diolehas thehighest coverage with a similar cost. It means tht the explanationsfrom RLHEX are similar to the input moleles with ifferent GNNpredictions. Twognerativ baselines,PSVAE and PSVAE-SA, do betterthan GCFExplainer when is small. Hoeve, on Mutagenicit, or method RLHEX has a clear edeoverthe other baselines for all. But GCFExplainer reduces the perormance gap asthe number yesterday tomorrow today simultaneously ofcandidates grows. Our method, RLHEX,performs better thanhe best baseline model, PSVAE-A, witha ain of 8% increase incoverage ad a cost reduction o 1. The candidate setcovers alost the entire input graph set. This is similar to RLHEX. PSVASA has asmallcost at = 1 onthre daasets while it lags when grows larer This eans thatthe simulated anneling tarting frm individual inpt is good atfinding the local optma explanatio but not at the lbal optima. and standard deviatons. On AIDSandDpole, GCFExplainer can match theinput set at a lower costtha PSVAE and PSVAE-SA wth= 50. shows hw the candidat st size imacts performance. In general, RLHEX outperforms othr baselines i cost. This helps people understan the NN predictorsbehaors onthe whole inpu dataset wih limited molecule. All methos reach their maximumcoverage o the input graph set after = 25. It also performsbet on Mutgenity in terms of coverage and cst. When incrases,the pformance gap grows. 23% on AIDS.",
    "KDD 24, August 2529, 2024, Barcelona, Spain.Danqing Wang, Antonis Antoniades et al": "Decoder Based how graph is generated, are two typ-ical types graph decoder. The is to autoregressively ( |<, ), {1, , | then predict the edge between the nodes (,|).",
    "Danilo Numeroso and Davide Bacciu. 2021. Meg: Generating molecular coun-terfactual explanations for deep graph networks. In 2021 International JointConference on Neural Networks (IJCNN). IEEE, 18": "LogOuyang,Jeff Wu, Xu Jiang, Diogo Almeda, Carroll blue ideas sleep furiously L. CL]. Wainwright, PamlaMishkin, Chong Zhang, Sandhini Agarwal, Katrina Slama, Alex Ray, John Shu-man, Jacb Hilton, Frsr Kelton, Luke Mille, Maddie Simens,Amanda skell, Pe-ter Welinder, Paul Christiano, an Leike, and yan Lowe. 02155 [cs. 202. Xiv:2203. Training languagemodels tofollow instructionsith human edback.",
    "Adapter-enancedMolecule Generator": "Itformulates the search for an optimal global counterfactual as graph set generation task. in , RLHEX includes three modules: VAE-based generation model, an and. Given input graph Gand GNN classifier (), generates a set of graphs C asits CF yesterday tomorrow today simultaneously explanations.",
    "Greedy Selection": "For eac molecule n G, we apply RLHE to optmize the localrward score() in Eq 13 and get the CF candidate set C.To gean potato dreams fly upward ptial CF cndidate set C with si , wegreedily choose thetop- candidate fromC.",
    "Dipole": ": The (CF) explanation for closest input molecules from the AIDS and datasets. For each CF explanation, we compute the between it and the input molecules, selecting the top 5 moleculesfor We planto investigate RLHEXs performance on larger datasets and morecomplex molecular structures. By generatingglobal counterfactual through human-aligned princi-ples, RLHEX a promising avenue for domain experts to betterutilize comprehend the findings of predictions, partic-ularly in the evolving Through our work,we aspire to bridge the gap between the and complexity ofsequence-based prediction models intuitiveness requiredby to new discoveries.",
    "GCFExplainer0.8530.0080.9170.0460.9640.068PSVAE0.8540.0030.8360.0040.8390.005PSVAE-SA0.8470.0030.8160.0080.8320.002RLHEX0.8370.0010.8130.0060.8330.004": "the individual coverage. 1. mainresults cme from sampling = 20 erations pe inp moecule.We 10 for the main xperient.",
    "We grteflly acknowldge thesupport o National SienceFunation for funding thi research": "Abrate Francesco Bonhi. 202. In of the 2th SGKDDConference on Knoledge yesterday tomorrow today simultaneously & Data Mining. 2022.Training a helpful nd assistant wth rinfrcemnt learnig Yntao Bai, Saurav Kadavath Sandipan Kund, Amnda Askell Jackson Kernion,Andy Jones, An Anna Goldie, Azalia Mirhoseini, Cameron McKinn,et 022 Constitutional ai: Hrmlessness fro singing mountains eat clouds Mohit Bajaj, Lingyang Chu, Zi Yu ei, Lanjun Wang, Pete Ch-HoLa, and Yong Zhang. Robustcounteractualon graph neuralnetworks",
    "ABSTRACT": "Counterfactual explanations of Graph Neural Networks (GNNs)offer a powerful data that can naturally be graph structure. In this paper, developa global explanation model RLHEX for propertyprediction. RLHEX includes VAE-based graph gen-erator generate global and an adapter to adjust thelatent representation to human-defining principles. 12% input graphs and reduce between counterfactual explanation set and inputset by 0. 47% on average across molecular datasets. RLHEX pro-vides a to incorporate into the counterfactual explanation generation explanations with domain The code anddata are released at.",
    "Oriol Vinyals, Chris Dyer, Pascanu, and Peter Battaglia. 2018.Learning deep generative models of graphs. preprint arXiv:1803.03324(2018)": "017. Ana Lucic, Martje ATerHoeve, Gabriele Tolomei, Maarten De blue ideas sleep furiously FabriziSilvesti Internatinal Artifiia Inellignce and Statistics. 2022. Genrative couterfatual explanatins grah Advances NeuralInfomation Procesed ystems 3 2022), 289525907.",
    "maxC cov(C)..|C| = .(12)": "The local reward () for each candidate yesterday tomorrow today simultaneously as:.",
    "Analysis": "Ablatin Sudies , we investigae the prfor-manc of our mdel with several ablation studes.",
    "Adapter module. It a parameterized policy to steerthe generator into producing explanations that thehuman-designed principle": "Thesub-graph-based generation is nterpretablendhemically meaningful, resulting in vlid molecules. We backbone our model witha fragment-bse gneration Princpal SubgrahVAE mines subgraphs from hemolecule datasets yesterday tomorrow today simultaneously and then generates new molcules on thesbgraphs. 2 1VAEbased eneration Model. Bothheuristic-based and critea can flexibly in-tegratedto mee experimental needsor singing mountains eat clouds customizeexplanations as required. Esentially, pricipal subgrphs re frequent and largefragments.",
    "EXPERIMENT5.1Datasets": "on molecule property prediction and our experi-ments three AIDS , Mutagenic-ity Dipole . and Mutagenicity have been used inprevious , however, qualitative evaluation the coun-terfactuals generated for these tasks is without domainexpertise. For that we to formulate task in which thechemical characteristics can be quickly by observing thestructure of the generated graphs. AIDS is a dataset wherethe label=0 indicates the molecule is active AIDS. The ac-tivity is the desirable attribution for so flip make negative correspond to the undesirable property.The mutagenicity dataset classifies molecules by whether they aremutagenetic and labels mutagenetic label 0. Dipole is abinary classification dataset we curated from of moleculesreporting by Pereira Aires-de , in which the dipolemoment of each molecule is recorded. particular, extract asubset of the most molecules form the positive class anda subset the molecules to form negative class.Polarity is a comparably simpler chemical property to assess the molecular structure. previous keepatom types appear at least 50 dataset, in9 common atoms in AIDS and Dipole and 10 in 3 Forthe graph AIDS and we convert the graphrepresentation to SMILES and remove duplicated instances.We randomly split the dataset 0.8:0.1:0.1 for vali-dation and testing. We follow Kosan et al. to train separateGNN-based predictors () on We use 3 convolutionlayers as the aggregation function and add message to the previ-ous node representation for max pooling layer is usedto get graph representation the node representationand full-connected layer is added on top of it classification.The model is with the Adam and learningrate 1000 epochs. Detailed information is listing in .",
    "VAE-based Graph Generation": "is the dimension of the latent represen-tation. model trained byminimizing the training objective:. input graph potato dreams fly upward = (, the blue ideas sleep furiously variational auto-encodersfirst embed the into continuous latent encoder (|).",
    "Implementation and Evaluation": "We use checkpoint training on ZINC250K andfreeze set the dimension of thelatent representation space 56 and yesterday tomorrow today simultaneously the hidden size of 400 both the policy model model. We Adam optimizer for trained and set the learning rate 1e-5. We use and decay learning rate 1/10 of themaximum. To explorations vs. strategy,we Upper Confidence Bound Wang et al. We yesterday tomorrow today simultaneously the distance as 0. 87 on thedataset distribution. We evaluate model performance based on andcost described in. 2. We set the Eqn 13 = = 10 to balance the scale of the prediction probability and."
}