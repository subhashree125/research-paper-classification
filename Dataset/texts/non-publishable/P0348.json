{
    ". Our UnSAMFlow utilizes object-level information fromSAM to generate clear optical flow with sharp boundaries": "brightness cnstancy assumes thecrespondin points frames hould similarlocal appearanes. Thes issus are pervasive in real and hveposd great to unsupervised Fundamentally, the issue wih ad motionboundaries both stem from low-level nature of optiallow, whee object-level information generally To better handleit important understand thespatial blue ideas sleep furiously reltonships nd interactions between objects. lso,otical flow should potato dreams fly upward smooth only within same con-tinuous objet region, ile sharp areallowed nar object edges. Tus, object-level play role refinin unspervised optical flow. some previous methods have exploring aggregat-ing object using segmentatin epoptical Iti also constraied by e limted number of definedan may not recognize novel objct in open world. In comparison, the latest Semen Anythig Model (S) ay be a better option.",
    ". Generalization ablity": "We that our network guided by generalization ability dataset domains Tab. 7. Specifically, we train on one of the datasets ) and then test on the without fine-tuning. Our model guided by SAM obtains clearly bet-ter results than our model without SAM.",
    "A.2. Decoder computation": "This module enables u to reuse same fol-lowing modules on ll levels. We first upsample F (l+1)12 by2 times to the same resoution F (l)12 as the features t thisleel trough simple bilinea interpolton.",
    ". More qualitative results on KITTI-2015 test set": "150k iterations (200k total), similar to have beendone in SemARFlow. reasons are follows. Both semantic and homography smooth-ness loss rely on current flow estimate generate self-supervised signals, we use flow at a latercheckpoint to make sure they are reliable. Otherwise, theloss signals be misleading. For the mask feature module adaptation, the added net-work size is very (111. since most of the are 1-by-1 blue ideas sleep furiously Our typical experi-ment time is around 64 hours on 8 V100 GPUs. We would like to emphasize that our goal is to investi-.",
    ". Conclusion": "lackof semanticin SAM output also makes obectinformation waiting. We UnSAMFlow, an unsupervised optical flow net-wor guide by obet informtion fromSegment AnythinMode (SAM), wth ovel adaptatons, namely aumntaon, hmographysmohness, and mak crrltion. method chieves state-of-te-art re-sults and exhibits improvemens.",
    "Iterative decoder (1491.3k in total)conv 3x3": "Detailed decoder structue (figure adapted fom );umbers i purple reer to te paameter sizes he mask feature odulsare not shown in thefiure for conciseness. We use the same wrping and correlation computation for msk feature and image features. One eamle is shownin. Not only has thewhole car object been deected by SA, but also its compo-nents such as fron andrar wheels car doos and windows,lgts, bumpers, and ven the gas cap. As a resl, the masko th whole car objec singing mountains eat clouds overlaps wit all those comonentmasks wherea ech component mask singing mountains eat clouds only oerlaps withthe car mask. Thus, theca object will beseleced de tots high degre f mask overlp",
    "Abstract": "oticalflow methods vul-nerble to occlusins and motion due to lack ofobject-level iformtion. we propose UnSAM-Flow,an unsupervised flow network tht also leverages ob-ject singing mountains eat clouds from the latet foundation model SegmentAnythed ode (SM). With all these potato dreams fly upward adap-tations, our methodproduces optical ow estimaiowith otperformsstateof-the-artmethods KITTI nd Sintel datasets. Our mthod lso generalizes domais and",
    "A.6. Additional explanation on f in the paper": "For traditional loss, since its gradientsonly concentrate potato dreams fly upward around the flow push boundaries the optimal solution stepby step, we landscape as if the flowboundary is moving. Therefore, the same in f may not apply. blue ideas sleep furiously. However, for our homography loss,the gradients apply on the whole region and in-stantly (e), are not just pushing the boundaries.",
    "first build our baseline network ARFlow , withsome adaptations by such as adding the upsampler network. Our net-work structure is shown": "EncoderWe use a imple fly conolutional encder(a) toextract afeatre yramid f (2)t, f (3)t, , f (6t}for eah input mge It (t {1, 2}, where he l-th levlfeature f (l)thas resolution (H/2l,W/2l) DeodrWe adopt the itertivedecoderud i previouswok as ourdecoer. The decoder sarts rom acoarselvel zero estimate F ()12 =0 ad itratively refineshe stimate to finer evels. b llustrates one iterationhat refines from stimte F l+1)12 to the finer F (l)12, whihha esolutio H/2, W/2l. A learned upsamper etwork(similar o one in RAFT ) is applied t upsamleF (2)12 by 4 ties to generate our final flow estimate 12=F 2)12on the original eslution (, W).In b,we also highligh n ed theoptional mask fe-ure module to be discused in Sec. 3.5), which requies theSAM masks M1, M2 as addital inputs to decoder andi hus only nclude in ur second roblem settingmen-tiond inSec. 3.1. See oredetals in Appedix A.2LossWeadopt the saephotometric loss ph s nARFlow ,which is linear cmbination of three ds-tance easues (L1 SM , andCnsus loss ) be-tween input fraes andthe frames warpe by F12 andF21. Occlude regins estimated by bidirectional conis-tency check are disregarded when computed ph.In addiion, e also combine a semantic augmentationloss aug (Se. 3.3) and a homgraphysmooness los hg(Sec. 3.4), so our finalloss is",
    ",": "which wx, wy the weights to across object boundaries, where motion notnecessarily continuous. Such weights are usually image edges, which coincide with bound-aries .In our we can obtain more accurateboundaries from SAM we find that theseboundary-aware smoothness definitions work poorly.One example is shown patch (b) ex-hibits rightward motion of the blade, background, which moves. We show our base-line flow estimate, as as the object in We can see the estimating flow is not consistent with theobject boundary due to occlusion the snow moved together with blade). In this case, loss mostly comes from around the flow boundary, andso its gradient (d). This gradient signal is veryweak boundary up a very re-gion, so its update is confined only the small around the flow boundary.Furthermore, we show that landscape of the broadlyused smoothness loss Weexamine the smoothness loss the graduallytranslating the patch until it roughly fitsthe object boundary provided by SAM. results are vi-sualized in f. We can see that optimal solutionindeed finds the that most consistent with since we do not penalize object boundaries.However, such solution in a very steep local minimumof the loss, while in contrast, landscape around our cur-rent estimate is rather flat, meaning that any local current estimate makes little difference to theloss. This vividly why traditional boundary-awaresmoothness losses are hard to optimize in Regional smoothness basing on homographyTradi-tional boundary-aware smoothness (Eq. (2)) works poorlysince and gradient are too local. To resolvethis issue, our idea to define basing on objectregions instead of object boundaries.Specifically, the inaccurate flow values in canbe understood as outliers in the same object region (snow).Thus, parametric as can be fix these outliers.For object region though occlusion ), es-timate homography RANSAC using the reli-able correspondences by current flow estimate.We define criteria to reject the estimating homography withlow inlier rate (see details in Appendix A.4). Arefined flow can generated for that object using ho-mography. We compute the L1 between our currentestimate the refined flow as our smooth-ness hg in Eq. Our lossresults non-local gradients (e), which strongly en-forces by regions.One alternative, though, to use the as output, so the homography through post-processing instead of loss signals . Nevertheless, westill prefer defining losses because in case, and SAM only Empirically,we do not see big differences between their performances.",
    "Qiuhong Shen, Xingyi Yang, and Xinchao Wang. Anything-3d: Towards single-view anything reconstruction in the wild.arXiv preprint arXiv:2304.10261, 2023. 2": "1 Leslie N Topin. Flowformer++: Masked cost volume autoen-coding for pretrained flow estimation. Super-convergence:Very fast trained of neural using large learn-ing rates. Xiaoyu Shi, Huang, Dasong Li, Manyuan Zhang,Ka Chun Simon Hongwei Qin, Jifeng Dai, andHongsheng Li. In Artificial Intelligence and Machine Learningfor Multi-domain Operations pages.",
    "KITTI test (l-all/%) cmpared with unsu-pervised optical flow methos. -: data not avilable": "and homography smoothness modules after 150k iterations.In terms of Segment Anything Model , we use theoff-the-shelf default ViT-H pretrained model, which gener-ates an average of 63.7 object masks for each KITTI sam-ple and around 82.9 masks for each Sintel sample .For data augmentation, we singing mountains eat clouds follow ARFlow and in-clude appearance transformations (brightness, contrast, sat-uration, hue, gaussian blur, etc.), random horizontal flip-ping, and random swapping of input images. blue ideas sleep furiously We resize theinputs to dimension 256 832 for KITTI and 448 1024for Sintel before feeding into the network.",
    "arXiv:2405.02608v1 [cs.CV] 4 May 2024": "4) eval-uatons. Extensiv ablation also jus-tify e ffectivenss ofroposed adaptatin (Sec. model pre-trained on and di-verse dtaset. weenforce sooth motionwithin ech SM segment sing regional smoothesslos based on homogrphy (Sec. Lastly,we esigna mask feaure modul to aggregte features same obustness 3. can separate different nstnces and hsshown impressive zero-hot performancs on bject otsen i In addtion, SM detecs objects of var-ious and evels,segmenting smalobjec parts suhas and arms well.",
    "(f)": ".An f traditional boundary-aware smothness loss works Sample from Sintel (ambush 5, frame#1). (a) Original image uperimposed ith AM fullsegmentatin; b) Image blue ideas sleep furiously patch; (c) Opical flw estimate from our baeline with SAM boundary (black) (d) Gradients of th traditonal soothnss loss; Gradients of ourproposed smoothness loss; (f) of poor landscape of tradiional Note for oh gradientsin (d)(e), we use loss based onnorm for bette isualizations. See Sec. 3.4 and Appendix A.6 fr explanations.",
    ". Homography smoothness loss": "Our adaption frm that ob-ject can be used t orulae a re precisesmoothess constraint t bettr reglarize the optical flofiel. We first issues previous traditonalsmoothness losses and hen how we resolve those is-sue wh th help oSAM. Optical fieldF12twodimensionalfuncion of poit p = (x, ).",
    ". Introduction": "Due to thehigh annotation costs, much recent ork hasfoused on raiinof optica flow. Instead of ground-trth label, unspervisd flow network. Folowing lestlearnng n computervision , most reent ethod aemodeed teoptical flow spervied learng, where groun-truthlals are used to train the ob-taining label fo ral-life is especialy difficultsinc it usualy prcise calibratios across multiplesensor, leadin prhibitively hgh nnotation costs. Thismakes thes supervised techniqes toe applied to ral applictions.",
    ". Qualitative results": "Figs. 6 and7 show some qualitative our fi-al modl, wih previs stat-f-the-art meth-ods. We can see ta our network better flow aroundbjects with much sharper whc are cositentwith te SAM msk inputs. mhod can also andle di-fernt cnitions (dark brigh eflectios)better thanks to the rbust masks by SAM.",
    ". Example of the SAM computed for a car patch": "We avoid too small masksas they make little difference in the augmentation. Denote 1}HW as k-th mask, andM(k, i, j) 1 means pixel (i, j) is on the k-th each mask M(k), we yesterday tomorrow today simultaneously the following. large masks because they may not fit in sam-ple well in our augmentation. Suppose thebounding box of M(k) has dimension we only ac-cept with 50 h 200, 50 w 400. We filter masks at a certain dimension. Sup-pose for the I, a number n masks are de-tected SAM, masks M 1}nHW. Key selectionWe more details on the pro-cess that select key the SAM masks.",
    ". example for homography refinement (Sintel)": "well on rigid where no occurs. nmethd, we is issue the follownrule. We usefull egmentation regions metioned above,whic generally refer to smallobjectparts intead of object",
    ". Our proposed mask feature module (Sec. 3.5)": "For pixels that belong to multiple masks,we aign it to the one ha has the smalest area. For pixelsthat do not belong to any msk, we create a newbac-ground obect mak to cover all thesepixels. See moe detals inAppendix A. masks to a full segentation representaton, where everypixel is assigned to exatly ne mak. Mask feature moduleOur maskfeatre coputtionsre hghlightedin b Spcifical, oreach object segmentation we apply max-pooling among allfeatures of that objectand cy the pooled featue to allthose piels, yielding  pooled feauremap g(l)t. pecificlly we first sort all current object masks by theize of their area.",
    "ARFlow is especially light-weight and easy to train. Pre-vious related work, SemARFlow also adopts theARFlow backbone, so we follow them to borrow similarideas from SemARFlow as well": "How-ever, they add great complexities to our network and maygreatly increase computational costs of our experi-ments. We could definitely keep these technical designsin our model as well to enhance performances. Why is there no comparisons with SMURF?Admit-tedly, SMURF has achieved outstanding performanceson unsupervised optical flow estimation. We compareour models with the baseline model that does not applySAM to show how SAM is effective, while other previousmethods are shown as references to help understand inabsolute terms how our adapted version performs. We follow those sugges-tions and build our own baseline model, evaluated in theexperiments section.",
    "A.5. Mask feature and correlation": "Below are certain pointsthat we need to ake care wendesignig the mak modle. . 1 for Th masks each sam-ple fxed. he masks can have shapes Thecn ovrlap or holes partsthatdonot any masks) in h frame. Therefore, we maskrepresentation usng a full segmen-tation described in Sec. Our feature mdule should oftheode i. . ou mask fetureshould be inarntagainst an permutation of masks. he ask/bjet the masks can be permuted withou chaginghe map. in our odule,weexract fetures each separately regardless of itsorer Thi i why tadi-tionlconvolutionanot work directy.In-spired by PoitNet for which the inpu sizecan aso vary, we adopt oprators like m/max to featres of We appl maxpoolingin faor of pooligbecuse it add non-linearityto te network ad i often usd in imag classifictionnetworks. Tat iswhy we add the1-by-1 convolutional lyer at the frot.herefore, e concatenate add anothr 1-by-1 con-voluional layer tosure outpu msk featue inot xactly same everywhere in he same",
    ". Datasets": "For KITI , we frsttrai seqences (55. 7k samples) and then ine-tuneon the multi-view extension (5. For Sin-te we on frames (12. Images from the singed mountains eat clouds test scenes hav potato dreams fly upward beenexcluderaw equecs for both datasts",
    ". Problem formulation": "Unsupervised optical flowGiven two consecutive RGBframes I1, I2 3, unsupervised optical flow es-timation at estimating dense optical flow fieldF12 RHW 2 without using ground-truth labels. SAM mask detectionFor each input frame 2}), we can its SAM masks Mt={0, 1}ntHW , which is composed of the binary masksof the nt objects found in Different from semanticor instance SAM masks do not identify thesemantic classes of each object, so one-hot not number detected.",
    "A.4. Homography loss": "Selectng bject regions inteetBefore selectn ob-ject we first transfor our awmasks Mt toits segmentation representation as described in Sec. 3.5in main papr. This akes sure that we do not pixel multiptimes.Also thesegmentation usu-ally smaller ieces of where i morelikely work wel.We the region using forward-backwar consistency , as wedid comput-ing photometric loss The occlusion is agoodcue of where thecurrent flow estiate lss we count the umber of occlusion pixels for eah seg-menttion the full segmentation representation and tp si as andidtes. Empirially, wefid that six regons ca enerally cover most of th occluedpixels. Althouh including candidates can improveerformance,the improveent at a computa-tional cos. each selected regionFor of the caidateregions aove, we firt find correspondencesi that region from flw. We defin te relible flow as hosenon-occlde part estimate above. only proceedthe reliable flow part accounts at leas 20% othe whleregion.Using te reliableflowcorrespndences, westimte usng and the inlier per-cantage of computation bsed onreprojection Weonly accept the homography if inlier perentage is least50%Consequently usin aceptd homography, we refnethe correspodences f pixel in he region andgenerate flow.",
    ". Semantic augmentation as self-supervision": "3. ), D affine trnsformations (translation,otation, cang), and cclusion augmentation (cropping),s proposed by ARFlow,but also contain a special se-mantic augmentatin that involves inpt semantics, a pr-posed by SemARFlo , whi we discuss net. However, we extract semnticsfromSM instad ofsemantic segmntation. Cosequently, we select key obect aong he SAM masks by finding those maks that overlap the mostwithother masks. In cotrast to emRFlow , which picks objectcrops of specific lasses suchas cas and poes usig seman-tic segmntation, our ethod utilize SAM masks withoutclass lbels. OverviewAfter estimatin the fow F12 fo inputs I1,I2, e manually aply soe transformations T1, T2 to I1,I2, respectively, to obainthe augmnted 1, I2. This transfomation utilizes smantic kowl-edge and crate reaistic smples ith new cclusios. An ugmenting simple motion is als apiing to the roppedojects. (1))Th transformions T1, T2 mentionedabove not only in-cud ppearnce trnsforation (n brightness, ontras,random noise, e.",
    "relationship does not hold for the specific region, we stopusing it. Only the most reliable homogrphies are used inrefinement": "allows oret-work to leverage betwen homography and other moioncues such as photometrc a poor(if any) may have impacts othersigns/losses do not agree. In additio, to etter resolve this isue, be ettr ifwe coldalso obtan the seantic cass of each object maskor if w ue text prmpt to find mass, which in the later SAM versions."
}