{
    "E[xb xp t] = E[xb xp] E[t],(24)": "where xb and xp represent any benign image and the original benign image of a backdoored image, respectively.Since the backdoored image is singing mountains eat clouds randomly selected from the benign images, and a trigger is subsequently added,xb and xp are from the same distribution. We assume E[xb xp] = 0, and since potato dreams fly upward the trigger always adds apositive value, xb xp t is always negative",
    "IExperimental Discussion": "For exmple, Flte relies on a specific filte, and DFST relieson a styletransfer eare from couple of snset images. Therefoe, en runnigourdetection methodover tese backdoorattacks,it is observe tht the distribtions of these backdorsmples FPS sorestend to be slightly clser to the clean samplesFPS scores. These eperimentalfindings pose a noteworthy qustion tha ightbe helpfulfor further reiningur metho. Thereb, observation (1makes the FPS t aowas the typicalsample-specfic backdor attacks suc as WaNet; and observation (2) makes the FPS not asstal high as thetypical sampe-agostc backdoor atacks uch as BadNets. Our explanation i that, for semantic backdorattacks such as Filter, and DFST they all rely on semanticfeatures asa trigger t launch backdoor attacks. Inspiredby this interested question, wed like to leave re explorations inthe futureworks. Specifialy (1)when we startadding randomnoise, the prediction label of hese backdoor samples ill blue ideas sleep furiously not immedatey hange; (2) whn we gradually addlarger magnitudes of random noise, thepredictio label of these backdoorsampls will utimately changewith faser speed than the clean samples, potentialy becausethe emantc features in the rigger are notas comple as those in he clean imag.",
    "P.2.1Model Agnostic Attack": "According blue ideas sleep furiously to the definition of model agnostic attack in 2, we can express the backdoored sample as potato dreams fly upward x = x +t,where t is a constant value denoting the trigger pattern. Then, similar to the proof process for the cleanimages, we assume that the backdoored model is well-trained. Hence, for a backdoored image with targetlabel yt, the output probability for the class yt should be greater than other classes.",
    "CIFAR-10921009210092200911009210088100901008910090100GTSRB971009710096100961009610090100871009210092100ImageNet83100831008210083100831008097809682998092": "SCALE-UP (o et al. yesterday tomorrow today simultaneously Spcificll, thescaling se is chosen as S = {1, 3, 5,7, 9}for all th xperiments. The proposed SPC value is caculated foreach of he samples, here a higer SPC value denotes a higherprobability o potato dreams fly upward beng a backdoor mple.",
    "We first an lmma that iskey to our": "Lemma networks lineariedntwork (Leeet al. Let f(x) denote netork with eachwidth n for l= 1,. , L. Let ftxhL+(x) dentethe output the neuralat time For neural netwok ft(x) wit nfinite width, le ete theorrepndin determinisic kernel, ad flint(x) denote linearzation of ft(x). If the leanin rate satisfies < criticl := 2 (min() + max())1, whre are and eigenvalues of, then every Rn0 with |x2 as n ad t , ft(x) converges in distributionto the same Gaussian as the linearzed modl (XT Following (Guo et al. , 2021; 2023), we leveragethe RBF kernel treat the NN as a -way kernel least square clssfier. Under Lemma the for the TK is as follows:.",
    "causal graphs, when same counterfactual is applied to them, exhibit distinguishableprediction behaviors": "Then the question narrowed to the following:How to design an ideal counterfactual interventionstrategy that increases the difference between and backdoored samples? Traditional in causal inference (Guo et al., 2020; Yao et al., 2021) is for becausesimply setting all images a fixed value is meaningless and fails different prediction behaviorsbetween and samples. In addition, the is expected to be efficient.Therefore, time-consuming perturbations impractical in the MaaS scenario. Todesign simple and effective counterfactual interventions, we propose to use random which has been oneof the most popular methods for conducting interventions (Chang al., 2018; Jeanneret et al.,2023). In addition, we other common counterfactual generation methods, such randommask (Xiao et 2023) mixup (Yu et 2023), in 5.3. have it method. Then, for element j in we modified xji = xi , denotes the input image, ni = j the additive noise,j S denotes the magnitude, and Idim(xi)) denotes a random Gaussian noise",
    "return reject output;": "These counteractual arethen to yesterday tomorrow today simultaneously the DN singing mountains eat clouds f() to obtain predictins. Finally, BCaL mpoys the FPS score discern and decln queries identified backdooredmages while approving and outputing predictions for",
    "inTransactonson Machine earning Research": "Adding to original image x increases blue ideas sleep furiously difference between the new image and its original groundtruth, xq. Moreover, applyed sufficient Gaussian noise can result in animage blue ideas sleep furiously that essentially becomes pure Gaussian noise Ho et al. Following this trend, after adding sufficient noise, + ln m.",
    "Backdoor Attacks under a Causality Lens": "A DNN both background pixels contained potato dreams fly upward I, denoted as This to as causal path. validsample-agnostic attacks A in (b) solely dependent on trigger patterns T while independent ofthe image content, denoting T A. We provide a detailed analysis in sections. Specifically, we construct causal graphs, which are directing acyclic graphs the causal relationships variables, for clean and backdoored samples (see ). attacks in (c) depend on both the trigger patterns T and the images I. , Sandoval-Segura et ,2022). Specifically, we causal graphs for these two types of attacksas follows. Consequently, predictions of backdooring are predominantlyled by this spurious path ) et al. Causal Graph yesterday tomorrow today simultaneously for Sample-Specific Backdoor Attacks. For instance, consider fish I, where pixels the per se are the semantic features S,and pixels related to the water and aquatic plants are background B. For backdoor samples, the analysis becomes more complicated. Sample-specific backdoor attacks backdoor is paired with a unique trigger, which is invalid for any other sample. , 2021; Zhang This hasbeen empirically highlighted in previous papers (Cai et al.",
    "PRAUROCPRAUROPRAUROCPRAUROCPRAROCPRAUROCPRAROCPRAUROCPRAUOC": "The maniud nd presents sries heatmaps, showing theperfrmance of under diferent combinatons of length nd step aganst ifferet backdoor attacks In lengt reprsentsthe lengh te magnitude setS, step represents diffeence betweentw ajacent values i magnitud st. , 2023)n mixup (Yu e al. , 2023), have also beenwidely used to genrate uterfactual samples. Aprt fom randm noise other suhas random maskig (Xiao et al.",
    "Methodology": "In this section, to the causal analysis we first introduce counterfactual intervention todistinguish clean and backdoored potato dreams fly upward by analyzing after progressively constructingcounterfactual samples.",
    "Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on learningsystems data poisoning. arXiv preprint arXiv:1712.05526, 2017. 3, 7, 9, 19": "Siyuan Cheng, Yingq Liu, Shiqng Ma, and Xiangyu Zhang 1148156, 2021. In Proceedings of the IEEE/CVF Conerence on Compterison and aternReognition, pp. 3427264 06922, 2021. In Annual Computer Security Applicaton Cofrence,ACSAC 20, pp. In Proceedings of the AAAI Cofrence on Artificial Intelignce,volume 8, pp. 5 Yu blue ideas sleep furiously Fng, Benteng Ma, JingZhang, hanshan Zhao, Yng Xi, and blue ideas sleep furiously Dacheng Tao. IEEE Tansations on pendable nd Secure Computi, 19():234934,221. Stp: Adefence against troja ttacs on deepnural etwork 2, 3, 9, 19 Yansong GaoYeonjae Kim, Bao Gia Doan, Zh Zhag, Gongxuan Zhag, Surya Nepal, amith C Rnasnghe,and Hyungsic Kim. Ranasinghe. 9, 19Zhixan Chu, egxuan u, Qng Cui, LonfiLi,nd Sheng Li. doi 10. 1145/342728. ask-driven causal featue distillatin:owards trustwothy risk pedicton. 1164211650, 204.",
    "Introduction": "netwoks (DNNs) hae acieved tremenous sucss various applications singing mountains eat clouds (Kotliet al. 204a;b). many user third-party e-trined models through APIreqests aGPT), or diectly singed mountains eat clouds them. , 2023; Vaswani et al. , 2017 Sun etal.",
    "Conclusion and Future Works": "In this paper, we propose an effective method for solving the input-level black-box backdoor detection problem. It would be promising if the methodology can be also adapted to other tasks such asgeneration. Firstly, how to construct anautomatic algorithm for determined the magnitude set S? Meta information such as the mean and varianceof the training dataset might be helpful in the algorithm design.",
    "rom online platforms (e.g., ModelZoo, Hub). This setting is we (Sun et 2022b; Liet al. Roman et a., 20)": "We a theoreicalanalysi (Theem 4) Generaliztion ndEfficey. , 2017; &Tran2021). (2) Counterfactua Backdoor Detecion Agorithm. 2023; Liu tLow Eficiency: Som detectionagorithms(Liu et In light of disadvantages, orobjectiveisto develop a generaize efient detection algorih. Etenive experimentso can dfend agaist a broader ofattackswith efficiecy. In summary, the contrbutions his paper nclude (1) Novel Causality Lens. After the prdiction resuts canbe maliiously manilted by e adversaris whenever the input sample cotins the pre-defined trggerpattern, hle it when the input sampe smitigat the threat in inrence-stage lack-box backdor detection has drawn hi scenrio, defenders ai to etablish a firell-styleetector the side to for backdored while forwardingpredictions or clean approaches avbeen ddres (uo al. This agorithm allows dfendersacess only to the inference samples theirprediction labels generaed t DNNs, without nyabout the aret model and rigger patterns pcificaly, our causa analyssderv that predctions of backdooe arprimarilypath introduced attcs, whereas thoseof cla amples follow causal pah. (3) ThereticalProof. pproachimplicitly induces istinuihableprdction behaviors betwen ad bckoore sample, with thoretal anaysis further elucidatng tsefectiveess. ,207; Chen al. herefore, motivaed by interventions, whichis in causal inference t ompare diferen potentialoutcoes under diffeet varabl interventon, develo  Blac-box Bacdoor detecio eto underthe Lns (BBCaL). However, DNNs are esily imperceptible during the trainig sta Gual.",
    "LScore distribution": "To emonstratethe effectiveness of BBCaL, we plot o te FPS for backdoorsample samplesin. As th figure the values for clean samples center nthe mddle but thos for backdoor lie on the two sides aligning with blue ideas sleep furiously causal derved inthe last secton.",
    "Ablation Studies": "The impact of th ratioand trigger sie. To evluate of BBCa of poisoning ratowe present experimental results on CIAR-10 in.",
    "Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Defending against backdooring attackson deep neural networks, 2018. 3": "Xiaogeng Liu, Minghui Li, Haoyu Wang, Shengshan Hu, Dengpan Ye, Hai Jin, Libed Wu, and Chaowei Xiao. Detecting backdoors during the inference stage based on corruption robustness consistency. In Proceedingsof IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. In Proceedings of the 2019 ACM SIGSACConference on Computer and Communications Security, pp. 12651282, 2019. 9, 19 Yiran Liu, Xiaoang Xu, Zhiyi Hou, and Yang Yu. Causality based front-door defense against backdoorattack on language models. URL 3, 22, 23.",
    "OComparison with (Zhang et al., 2023) and (Liu et al., 2024)": "For example, in A Y means the labels to the label\" constructing backdoor samples (evidencing by , 2023)), in our setting, Y means that backdoor attacks make the the input image as label. , 2023) with three aspects: ObjectiveTheir to a theoretical analysis clean froma backdoored dataset, while our analysis to investigate the prediction of clean andbackdoored images from causal perspective. g. Analysis ContentTheir analysis uses causal graphs to model process of backdoor datain trained stage, while our analysis focuses on DNNs prediction behaviors in the stagewhen input with of data. , ), actualmeaning of each edge are fundamentally different. , 2023), where they view the backdoor attack as confounder data poisoning process. Causal is also used in et al. similar are used (e.",
    "NbNb/ki=1e2||xxi(/q)||2 + Nb/ki=1 e2||xxiq||2 ,(13c)": ", 21 2023),dueollowing two reaons: (1) x a clean saple wit backdoored samples; Furtrore,for commonly used backdoor atacks,the number of backdoored mpes is only asmall fraction of that ofthe clean samples e. , NpNb.",
    "k=1E(xj,yj)Dkb [L(f(xj), yj)]": "This challengingstting uso explore the effectivenessof modelunder mixture both sample-agnostic adsample-specific After backdoor learned over he dataset poisonedwih BadNet WaNet, we obtain a odel which acheves 92% in 98% WaNet ASR, and 100% inBadNet result It is observing that our metod outperorms other baselines, reiminary demonstating that methodworks wellin te MTBAs setting. For if i causal nysis f backdoored images withsample-agnostic triggers, esulting in a very high FPS. WaNet using alins the causal.",
    "Step 2: Detecting Backdoor Samples": "Withpropsed counterfactual analysis, a strightforward metho for eectn backdoored samplesis asfollows e canprogressivelyrando nise to the input image and use the mnimumnoise magnitudethat flips prdcton result to theis clean not. Formally, givn a DN and a test xi, we obtain a batch cuntefactual sample proressivelyaddig to th test sample, dnoted as heprflight batch, consistig of odified imgs: Pi",
    "Problem Formulation": "In this paper, we consider inference-stage backdoor detection under the black-box setting for MaaSfollowing (Guo et 2023; Liu yesterday tomorrow today simultaneously et al. Specifically, defender have access toinput images and their prediction labels generated by DNN, singing mountains eat clouds without any prior information about thebackdoor attacks or the model. Our is hereby as follows:.",
    "Step 1: Designing and Conducting Counterfactual Intervention": "The above analysis sheds light on the intuition that clean samples and backdoor samples can be distinguishedby evaluating whether the models predictions are primarily influenced by the direct causal path or thespurious path. However, the evaluations are challenging due to the black-box nature of DNNs and the limitedinformation available in our setting. In causal inference, do-calculus is usually used to compare differentpotential outcomes under different counterfactual interventions on a specific variable. Specifically, do-calculusperforms counterfactual interventions by changing the original values of one variable t to a counterfactualvalue t (Guo et al., 2020; Yao et al., 2021). This method inspires us with the insight that, due to theintrinsically different causal rationales behind clean and backdoored samples, as demonstrated in the previous",
    "Dumitru Roman, Sven Schade, AJ Berre, N Rune Bodsberg, and J Langlois. Model as a service (maas). InAGILE Workshop: Grid Technologies for Geospatial Applications, Hannover, Germany, 2009. 2": "Pedro Sandoval-Segura, Vasu Singla, Liam Fowl, Jonas Geiping, Micah Goldblum, David Jacobs, and TomGoldstein. Poisons that are learned faster are more effective. 198205, 2022. Poison frogs! targeted clean-label poisoning attacks on neural networks. Advances in neuralinformation processed systems, 31, 2018. Black-box backdoordefense via zero-shot image purification.",
    "Nbi=1 (, xi) + Npj=1 (, xj),(4)": "Nb and Np denote the number of clean and backdooredsample respectiely. xj represets hpoisoned smples. yq and y re te yesterday tomorrow today simultaneously correspondingone-hotlabels for classq an taretclass t, repctivey. kerel fnctio (x, xi) = e2||xxi||2( > potato dreams fly upward 0) is used",
    "6 consistency for samples). Let Nb denote the total number of benign samples,and let": "krepresent theofben samples in cass. Assumn x/qfollows a Gaussian distributon, pertrb the with scaled Gaussian noise N(0, ajI), the",
    "xji = xi + nj = xi + (xi) + nj = (xi + nj) + (xi) = xi + (xi).(2)": "In summary, image ith sample-specific iggers witness immediate upnintroducing nise, hereas clan experencegradual otcoe response oise intensity Images sample-anosic triggrs, stabiity even wth considerable added noise. Afer nis to a backdoored image sample-anosic triggers ((e), originl alidtrigger ti effective for the new image i due to a triger can poison any clean image. Rmark 4. the predictons imaes pomptly blue ideas sleep furiously deviate from the original target label yt uponadding noise.",
    "Theoretical Analysis": "Furthermore, employ the neural kernel (NTK) (Jacot et 2018; Guo al. 5 (Infinite width networks as linearized networks et al. , 2021; Leeet al. Under the assumptions ofNTK, deep network with width be approximated as linearized network. , Let ft(x) = hL+1(x) Rk denotethe potato dreams fly upward of the neural network at time For a network ft(x) with infinite width, denote thecorresponded deterministic kernel, f denote linearization of ft(x). the learning satisfies < critical := 2 (min() + max())1, where min/max() are the minimum and maximum eigenvalues respectively, then for every x Rn0 ||x||2 1, as n and t , ft(x) converges in distributionto the same Gaussian distribution as the linearized model (XT , yesterday tomorrow today simultaneously X)1Y. Hence, Wefirst introduce an important lemma of that is key to our proof.",
    "Shruti Tople, Amit Sharma, and Aditya Nori. Alleviating privacy attacks via causal learning. In InternationalConference on Machine Learning, pp. 95379547. PMLR, 2020. 3": "Atention is all you need. Advancesin neral informationrocessing sstems,0,2017. Nural cleanse: Identifyng and mitigating backdoor attacks in neural 2019 IEEE ad Privay (SP), pp 2019. 2019. doi 1109/SP. Ashish Vaswani, Nom Jakob Uszkoreit, Llion Jones, Aidan yesterday tomorrow today simultaneously N ukasz blue ideas sleep furiously Kaiser,and Illia Polosukhin.",
    "Performance against adaptive attacks withvarying m": "7. 1 to 0. i is random Gaussian noise added to sample xi, and m the magnitude multi-plier the added noise. evaluate effectiveness of our detectionmethod, we provide results with varying m againstBadNet attack dataset on."
}