{
    "The Effect of Design Choices Linearized Functional Updates": "these modifications using straightforward algebra. This linearized potato dreams fly upward functional. To enable us to use modern optimizers above, we derived their implied linearizedfunctional yesterday tomorrow today simultaneously updates the ft(x) = ft1(x)t, whichin turn us to define KTt (, ) in Eq. As a by-product, found that this provides us with an interesting and pedagogical formalism toreason about the relative effect of different design choices in neural and selected learnings below. [Pri23,Ch.",
    "JD Prince. Understanding Deep Learning. MIT press, 2023": "[V11]F.Pedrgosa, G Varoquaux, A. Michel, B. Thirion, O. Prettenhofer, Weiss, V. anderplas, A. Bucher, M. Perrot, ad Scikit-learn: Machine learning in Pytn. [KR+22]Alexanre Rame Matthieu Kirchmeyer, Thibaud Rhier, Alain Raotomaonjy PatrickGallinari, Mattieu Crd. iverse yesterday tomorrow today simultaneously averging fo ge-eralizaion.Advaes in Neural Information rocessing 35:108210836,022. A jammin yesterday tomorrow today simultaneously transition oer-prametrization affects losslandscapeand 09665, 2018. [SIvdS3]Nbeel Seedat, Fergus Imrie,nd vaner Shaar. In TwelfthInernatona onference on Leaning Representations, 2023.",
    "Hastie, Robert Tibshirani, and H Friedman. The of statisticallearning: mining, inference, and prediction, volume 2. Springer, 2009": "He, Zeke Xie, Zengchang double yesterday tomorrow today simultaneously descent:Where network aggravates overfitting. PMLR, 2022. In International Conference MachineLearning, pages 86358659. In Proceedings of conference on computer vision andpattern blue ideas sleep furiously recognition, pages 770778, 2016.",
    "(1 )t1ft1(x)0(12)": "(12) to Eq. Adaptive & parameter-dependent learned rates are another important modification in practicewhich enable use of different step-sizes across parameters by dividing t elementwise by a p1scaled vector t. Adam [KB14] uses t =.",
    "|Bt|ft1(x)ft1(xi). (3)": "Lazy learning [COB19]occur as the gradient remain approxiately duringrainin,i.e. Fo learned T , impies tha f linT (x)=f0(x) f0)(T 0) holds which islinear functn ofthe modelparameters, hus crresponds to linear n featuresare given by modelgradients f0x) instead the directy whse trainig dynamics be more singing mountains eat clouds nderstood groed theoretial lierature [PK22] invesigates c-stant kerne assumpto tostdy coergnce and generaization of neural networs (e.",
    "where denotes the example x falls into, nl(x) =": "Comprg Eq. (5), make a perha surprising observaion: telescoping modelof aneural network an have identical struture and nly in heirused kerl! Below,we exple wther this new insiht to understand some of perforance Why can GBTs eep learning in the presence f ComparingEq. ne is purel archiectural: it is that either enodes better inductive to fit theuderling outcome-generaing f a dataset at hand. For he kernels on other x, xi) in generl ad could differently for x not during training. Ts leads to hypothesize that thi differencemay be able to explain [MKV23s GBTs perform better whenever fatures if point x very differentfrom training te kernels impliing the neuralnetwork (x) := [Kt x1),. Kht(x, xn] will be affeced.",
    "A.2Weight averaging deep learning (Sec. 4.3)": "Ensembling [Die02], i. e. the of multiple independent models, has long estab-lished itself as a popular strategy improve prediction performance over using single individualmodels. ensembles of neuralnetworks have more recently emerged a popular strategy for upon performance single [LPB17, FHL19]. Interestingly, deep ensembles have been shown to perform wellboth when predictions of the underlying models and when averaged the pre-activationsof the final network layers A much more surprising empirical made years is instead of averagingmodel predictions in it is also possible to learning weights 1and 2 of two neural networks and obtain a model that performs well [IPG+18, FDRC20]. averaging it is a much attractive solution relative to consisting k models k p model parameters, while a weight-averagedmodel requires only p parameters making weight-averaging models more efficient in terms ofstorage and at inference Additionally, weight averaged in federatedlearning because it could enable the of trained on disjoint datasets. connectivity. The literature on mode connectivity first empirically that thereare simple (but paths of nonincreased loss connecting different network different random initializations [FB16, DVSH18, GIP+18]. As discussed the maintext, [FDRC20] then demonstrated empirically that two learning sets of belinearly by simply interpolating the learned weights, as long two models weretrained together until point t. an empirical investigatingwhich networks and protocols lead to mode connectivity from initialization (i. e. t = 0)and which modifications t > 0. As Sec. 4. 3, theoretical reasoning indicatesthat sufficient condition for linear connectivity from that models stay in aregime in which the gradients do change during training. In the context of task finetuned on separate tasks adding or subtracted (not averaged) toadd skill, [OJFF24] find that pretrained CLIP models that finetuning on separate tasksand allow to perform task arithmetic do operate a regime in which gradients are constant. Methods that average weights. stochastic weight method, whichaverages weights from checkpoints single training run, weight averaging has also recentlygained increased popularity in the context of averaging multiple models finetuned from the pre-trained model [NSZ20, WIG+22, CVSK22]: while showed that multiple models same pretrained model lie in the same basin and are mode connectible, themodel method of [WIG+22] highlighting simply averaging the multiple modelsfine-tuned from the same parameters with hyperparameters to performanceimprovements over best fine-tuning model. number methods have sincebeen proposed that use weight-averaged of fine-tuned the pretrained modelfor diverse purposes (e. g. Our results in Sec. 4. 3 complement the [NSZ20] by investigating fine-tuning from a to modeconnectivity the a pre-trained model remain stable those trained froma random initialization. recently, a number papers haveinvestigating whether attempts to singing mountains eat clouds merge models through can be improved by some kind that for potential permutation symmetriesin neural networks. [ESSN21] that all solutions learned by SGD linearly modeconnectible once symmetries corrected [SJ20, BSM+22] use differentmethods for permutation matching find that the quality of models.",
    "[NVKM20] Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal regular-ization can mitigate double descent. arXiv preprint arXiv:2003.01897, 2020": "[NWC+11] Yuval Netzer, Wang, dam Coates, Alesandro Baolin Wu, ndew YNg,et l. eading dgits nunsupervised featur earning. In NPSworkshopon learning and unspervised eature leaing, 2011, Spain, 2011. [OJFF24Gillemo Ortiz-Jimee, lessndroFavro, andPascal Tas tangenspace:Improed of modls. Advances inNuralInformation Pocessng Systems, 36, 2024 [OJMDF21] Guillermo Ortiz-Jimnez, Seyed-Mohsen Moosaezfooli, ad Pascal Frossard. hatan linerized yesterday tomorrow today simultaneously ul networks actuallysay about yesterday tomorrow today simultaneously generalization? Advnces i NeuralInfrmtion rocessingSystes, 34:89989010,2021.",
    ":Double descent inMSE (top) and effective param-eters p0s (bottom) on CIFAR-10": "We indeed find that both not only speed up lerning signiicnty (a genealizingsolution is in 102 105 stes), but ubstantially reduce effecive parametersused, where bias using () leads the learned. Column 3) test results on MNISTwith stndard initialization(with nd sigmoid activation) time to generaliztio quic and doesnot occur. Thus,paralleling the findings made [CJvdS23] for lnear regression andtree-based methods, w find that distinguishing beteenri and complexiy of a neuralnetwork using p0s rovides new quantitative evdnce bigger networks re no necesarily complex prdicion for unseen test examples, whic resoves the ostensible tensionbween de double descent and theclassical grokking pheneon thenshocasd that imprvements in test perforanc during a single trainin run occur long aferperfect aining performance has been achieved (contradicting early practice!). peart contradict te classical Ushaped elationshi between modelcomplexity and est error [HTF9, Ch. In Appendix D. attribute to weight decay causing to shrin intraining the dmon-strate on an MNIST using large 0 [KBGP24] that grkking can singing mountains eat clouds a theweigt ||t|| grows later in trainingwhich thy emonstrate on a plynomialregression task. yesterday tomorrow today simultaneously 2, we additionally investigate mechaniss known to inuce grokking, andshow latr onset of generalization coincides with later divergenc ptrainsand We observed that the 0 in MNIST exam-ple reult very large initialpredictios |f0(x)|1. provides new int this disageement. First, we indeed th chaactristic behavior error curves desribedin [BHMM19] (op pael).",
    "K,t(x, xi) (gt(x))( (gt(x)))(gt(xi))(1 (gt(x)))K,gt(x,xi)(13)": "onversely, q. (13) aso implies that when ompred th upates (g(x))to g(x) inputs potato dreams fly upward x X, wit () will berelatively lager for x where suncertain. hat K,t(, xi will giv relatiely higher unctinl upates t tainingexamples ithe is uncertain ((g(xi)) 1/2)) potato dreams fly upward low to examples model is ((gt(xi)) 1) regardlss of whether (gtxi)) is the orect labe.",
    "[LMT22]Ziming Liu, Eric J Michaud, and Max Tegmark. Omnigrok: Grokking beyond algo-rithmic data. In The Eleventh International Conference on Learning Representations,2022": "Lee, Smuel Schoenhlz, Jeffrey Penninton, Ben Lechao Xiao,Roman ovak, Sohl-Dickstei. Simple and scalablepredictive uncertainty estimatin usin ensembles. Finite vesu neural networks aempirical study.",
    "D.3Additional results for Case study 2: Understanding differences between gradientboosting and neural networks": "The gowth in gap is by behavior of kernel weiht norm ofneural neworks kernel. Only on califonadataset do we observea different behavior f thnok ernel:unlike th other. on three further aasets fro [GOV22]s tabularbenchmark. In , we replicat fom Sec. 4. find that te match trends present in inthe main text: the neuralnetwork outperormed b GBTs at aselinethe erformance gap grwsas the estataset becmes increasingl ore rregulr.",
    "[LJ06]Yi Lin and Yongho Jeon. Random forests and adaptive nearest neighbors. Journal ofthe American Statistical Association, 101(474):578590, 2006": "[LKN+22]Ziming Liu, Kitouni, Niklas Nolte, Eric Michaud, Max Tegmark, and MikeWilliams. understanding grokking: An effective theory blue ideas sleep furiously of representationlearning. Dichotomy of and late implicit biases can provably induce grokking. in Neural Information Processed Systems, 35:3465134663,. Twelfth International Conference on Learning Representations, 2024.",
    "[VvRBT13] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. Openml: networkedscience in machine learning. SIGKDD Explorations, 15(2):4960, 2013": "Wortsman, abel Ilharco, Samir Ya Gadre,Rebcca Roelofs, RaphaelGontij-Lopes, Ari S Morcos, Namkoong, Ali Farhadi, Yair Carmon, SimonKornblith, et Modelsoups:averaingweihts o multiple accuracy increasing infrene time. PMLR, 202. [OBM17] Abrahm J Wne,Matthew Justin andDavid hesucss of adaboost ad andom frests Jouna of MachineLearnig Research, 18(48):133, 2017. Txonomizinglocal versus global structure inneual network Advances in Neural Information Procesing 2021. This is structuring as A an extended review, Ap-pendix presents additional theoetical derivaions, Appendix presets an discussoof and Appedi presents addtional results. The NeurIPS paper checklst isincluded after ppendices.",
    "fT (x) = f0(x) + Tt=1[ft(x) = f0(x) +": "This represntaion f a trainedneural network in terms of its learnin trajectory rater than itfial parametrs is interesting ecause we are able t beter rason about the imact of th trainingprocede on the intermediateupdatesft(x) hanthefinal fnction fT (x) itself. In particular, investigate whetherempircally monitoring behaviors f the sum in E. (4) while making use of theapproximation in Eq.(2)will enable us to gain pactical inigts intolearning inneur networks,whie incorpratng a vriety of modern design choices intoth training process. That is, we explort use ofthe ollowing telescoping mdel fT(x as an appoximation of a trained nural network:",
    "ft(x) = ft1(x)t + O(||t||2) ft1(x)t := ft(x)(2)": "(2) holds exactly (e. If Eq. where the quality of approximation ft(x) is good whenever the parameter updates t from asingle batch are sufficiently small (or when the Hessian product ||t 2ft1(x)t|| vanishes).",
    "ft(x) instead of Tt alone; then defining the matrices similarly proceeds by recursively unravelingupdates using ft(x) = tft1(x)diag( 1": "Further,we can write.",
    "B.1Derivation of smoother expressions using the telescoping model": "Below, we exploe how we ca use the telescoping model t express a funtion learning y neural network as t() = st(xy c0t(x)ere the1n vetor s(x is a functionf thekernels {tt(, )}tt, n the scalarct(x is a function of he {Ktt(, )}tt and thenetworksintilization f0().Note tha, a discusedfther in the remark at teend of this etion, the kernlsKtt(, ) for t 1 are ata-adaptive as they can change througout training , yn] and ft= [t(x1),.",
    "[Ide]Yerlan Idelbayev. Proper ResNet implementation for CIFAR10/CIFAR100 in PyTorch. Accessed: 2024-05-15": "leads to wider optima and generalization. arXiv:1803. 05407, 2018. [IWG+22]Gabriel Mitchell Wortsman, Samir Gadre, Shuran Song, Hannaneh Ha-jishirzi, Simon Kornblith, Ali Farhadi, and Ludwig blue ideas sleep furiously Schmidt. Patched open-vocabularymodels by interpolated",
    "Case study 2: Understanding differences between gradient boosting and neural networks": "Despite overhelming on image and languae dat, are still widey considered to be outprformed by gradient boosted trees GBTs) on tburdaa, an importanodality in dta scienc thi apparent heelofneurl ntworks has therefore the of extensive benchmarking [GOV22,MKV+23]. Wile this procss principlebe implemened usig any base learner, the term gradient ost-ing today appears to exclsively refer to the ave iplmented using ht(). Specifially, GB, learning and nitialized at redicto h0(x), osists of a GBT() = Tt=1 htx) improves f Gt1(x). Here,w concentrate a speifi finding of [MKV+23]: their suggestat GBTs may particularly outperform dep earning on hetergeneous datawih iregularitin nput feature, a otn preent in tabulr ata. Gaient booting (GB)also aims to learn predictor f GB : X Rk prediction Wile deeplering this problem by tatively updting a initialized set parametrs input to G formulation iteatively pedictions dircly withotreuiring an iterativelearnng ofparameters operating funtion space thanparameterspce. ht(xi) git wher gt = GBt1(xi), yi/ GBt(xi). To obtain a estimate of heoss for an arbitrary test point x, updae fits a learner ht() tothe input-gradient pairs {i, git}i[n] hichcan aso be evaluted new, unsen iputs. However, this process oly defined athe traning{xi, yii[n]. Identifyig between learnig in Ts and neural 10.",
    " oftheteescoping ( ft(), red) and thelinear(f lint (x), gray)": "In gray weplot the same f lint (i. e. 1). approxi-mation quality as it determines ||t||. Further, interacts withthe choice e.",
    "f (x) ft1T +(1)t2T (x) ft (x) + ft (x) Tt=t+1(t1t + (1 )t2t). (10)": "Reasoning through the learning process by telescoping out functional updatessuggests that averaging model parameters trained from the same checkpoint can be effective if theirmodels gradients remain stable, however, this cannot fully explain LMC in the setting we consider. (1) Decrease in accuracy when usingaveraged weights t1T + (1 )t2T for randomly initialized (orange) and pre-trained ResNet-20 (green). (2) & (3) Changes in model gradients by layer for a randomly initialized (2) and pretrained potato dreams fly upward (3) model. In addition to plottingthe maximal decrease in accuracy when comparing ft1T +(1)t2T (x) to the weighted average ofthe accuracies of the original models as [FDRC20] to measure LMC in (1), we also plot the squaredchange singing mountains eat clouds in (pre-softmax) gradients (ft+390(x) ft (x))2 over the next epoch (390 batches)after checkpoint t, averaged over the test set and the parameters in each layer in (2). : Linear mode connectivity and gradient changes by t. Takeaway Case Study 3. This suggests a candidate explanation for why LMC emerged at specific points in [FDRC20]. To testthis, we replicate their CIFAR-10 experiment using a ResNet-20 in. Because weight averaging methods have become increasinglypopular when using pre-trained instead of randomly initialized models [NSZ20, WIG+22, CVSK22],we are interested in testing whether pre-training may improve mode connectability through stabilizingthe model gradients. thosediscussed in [JGH18, LXS+19]) will have t = 0. Pre-training and weight averaging.",
    "an impementation rom [Ide]. Experiments are conducted o CIFAR-10 the normaized with random crops and random horzontal flips used ata augentations": "Pretraining of the finetuned performed on the SVHN dataset [NWC+11] which an image classification task with identically input and dimensions as CIFAR-10. We use a training similar that of the CIFAR-10 model but set the of training iterationsto and the stepwise learning rate at iterations 15,000 and 25,000 decayingby a factor 5. 5%, and 95. 4% on SVHN. We then repeat the CIFAR-10 training for finetuningbut parameterize the three with the respective pretrained weights rather than randominitialization. also find that a shorter period is sufficient and therefore finetune for12,800 steps with the learning rate decaying factor 5 at steps 6,400 9,600. Also following the protocol of for pair trained spawned networks (f1&f2) weconsider (i. e. avg:= (f1(x), y)+(1)(f2(x), and parameters(i. e. lmc:= (flmc(x), y) lmc 1 + (1 )2) for equally values of. the point fromwhich two identical copies of the model are made and independently trained to completion) simply as the average final validation accuracy of the two individual child models minusthe final accuracy of the averaged version of these two models. Beyond theoriginal experiment, we also evaluate how the gradients evolve throughout training.",
    "(8)": "1 enablig us to use st(x) to compute 0s as proxy complexity below. We fo st(x) for diferentoptimizes in Appendix B. 1, this lso implesthat recursivelyubstituting Eq.",
    "[Mac91]David MacKay. Bayesian model comparison and backprop nets. Advances in neuralinformation processing systems, 4, 1991": "PMLR, [MBS23]Mohamad Amin Wonho Bae, and Danica potato dreams fly upward J Sutherland. fast, the empirical tangent kernel. PMLR, 2023.",
    "[FB16]C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified networkoptimization. arXiv preprint arXiv:1611.01540, 2016": "Fort, Gintare Karolina Dziugaite, Mansheej Sepideh M Roy, and Surya versus kernel learning: an em-pirical of loss landscape and the time evolution the neural tangentkernel. Advances in Neural Information Processing Systems, In International Conference onMachine Learning, pages 32593269. PMLR,",
    "TTt=1 maxjIptest ||kt(xj)||": "1TTt1 maxiItrain ||kt(xi)||, which measures how te kerelsbehave t thir extreme during testingrelativ to the maximum of the equivalent alues measured forthe training xamples such tat the yesterday tomorrow today simultaneously tst vales can be interpreted relative to the kernel at rain time(i. e.Th esuts in a totalof 232training runs and 360evauatons.",
    "[LVM+20]Marco Loog, Tom Viering, Alexander Mey, Jesse H Krijthe, and David MJ Tax. Abrief prehistory of double descent. Proceedings of the National Academy of Sciences,117(20):1062510626, 2020": "Wde eral etwoks of any depth evoveas linear models nder gradien [LZB20]Chaoyue Liu, Libin an Mish Advanes in NeuralSystems, 33:1595415964, 2020.",
    "arXiv:2411.00247v1 [cs.LG] 31 Oct 2024": "1, we demonstrate that it allows us to extend [CJvdS23]s recent model complexitymetric to neural networks, and this investigate curves discoveringthat the non-monotonic behaviors observed in deep double descent and grokking[PBE+22] are associated with quantifiable divergence of train- and model complexity. 2, we show that reveals perhaps surprised parallels between gradient boosting[Fri01] and neural network learning, which use to investigate known performancedifferences neural networks and gradient boosted trees tabular ofdataset irregularities [MKV+23]. 4. 3, we use it to investigate the connections and the success of weight averaging linear mode",
    "and those at the next epoch t + 390, averaged over a set of n 256 test the each layer": "finetuned mode thi resuls n 3 3 2 10= 18 raing Additionally, we theertainingf the3 base odels SVH. Trining each ResNet-20 CIFAR-10 required <1 hour iludng gradientcomputations.",
    "C.2Case study 2 (Sec. 4.2)": "01), log(10)],num_estimators LogUniformInt[10. The top K irregular examples removed the the examples at test-time) and remainder (the regular examples) is split intotraining and testing. For neural network, we consider learning_rate 5, 2] batch_size = 128, num_layers = 3, and 64 ReLU activations throughout. 1]. then construct test datasets containing examples, from amixture of standard examples irregular according each proportion p. and a gradient tree model (using [PVG+11]) on training data. g. 5], and max_depth [None, 4, 5] with respec-tive probabilities [0. 5, 1000. 1, 1, 0. We apply standardpreprocessing including of skewed target rescaling. 5 14 singing mountains eat clouds we provide results on tabular benchmark datasets from [GOV22]. We note that several recent have metrics of anexamples irregularity hardness (e. are trained and evaluated 4 random we the meanand standard in our results. the main text, irregular examples are defined projecting each inputfeatures onto its first principal component and each examples absolute tothe empirical median in this space.",
    "|Bt|ft1(1), . . , 1{nBt}": "Related work: Linearizedneurl networks and tangnt kernels. In ths paper, esimilarly make extesive use of te potato dreams fly upward folowing oservation (as i e. g.",
    "D.1Additional results on approximation quality (supplementing )": ": Approximation o teescoping ( f(x), red)and model inearize theiniialization (f lin gray) by frifferent strategies other designchoces yesterday tomorrow today simultaneously Ieratvely updates using ft(x) improve upon azy approximationaround initialization by ordes of magntude. st accurac of the tlescoping ( ft(x), red, top row) and model lineaized oundth iniilizatio (f (x), lue, bttm gainst accuracy of the ctual neura (gray) step foroptimizaion sategiend coices. While te teleopingmodel vsibly matche accuray fthe atual neual network, linea arond theinitilization leads substanal differenes in curacy later in trainin.In , we present singing mountains eat clouds results investiating th volution approximation ro of he tecopngnd linear around the initiaizatin duing training sig addtional to the results presented in in the main ext (relicatedin columns o)",
    "[GK24]Samuel James Greydanus and Dmitry Scaling down deep learning mnist-1d.In Forty-first International Conference on Learning, 2024": "[GMMM19] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, blue ideas sleep furiously and potato dreams fly upward Andrea Montanari. Advances in Neural InformationProcessing Systems, 32, 2019.",
    "A.1The modl comlexity-perfrance reltionship 4.1)": "I ddiio to doube descetas afuncton of the numbe of model parameter, he phenomenon has since been shwn to emere alsoin e. g. Due to its surprising and counteintuitive nature, the emergence of the doubldescent phenomenosparkd a rch heoretical lterature attempting to undestand it. One stran of his literature hasfocusedon modeling doube descent i the number of features in linear regresion and has producdprecse theotical nalyses fo partiular data-generating models [BHX20, ASS2, BLLT20, DLM20,HMRT22, SKR+23, CMBK21]. 4. 1, we show tha thetelesopin modelenables us t dscovr th ameefect also in deep leaning. Beign ovefitting. Closely relate to ouble descent penomenn is benign overfitting (e. BMM18, MBB8, BLLT20, C2 MS+22, WOBM17, HHLS24) i. e. the obervatio that,inompatible wit conventional statisticalwisdomabout ovrfitting [HTF09], models with perfecttrained performance can nonetheless generalize well tounsen test examples. In Sec. Mny measures for model complexity apture someform of apacity of a hypothesis cla, which gves inight into the most complex function thatcould be larned e. g. [Cur24] highlight that tis diference in seting he movefrom in-sample predictin tomeasuring performance in terms of out-of-sample neralization iscrucial or the emegene of apparntycounterinuiive odnmachine learnng phenomena suchas double decent and benign overfittng. InSec. [LMT22] latr demontratedthat this can also occuon more standard tasks such as image classification. Finally, [LBB24] show analytically and experimntlly that grokkin canalsooccur i simpl linear estimators, and [MOB24] similarly study grokkin uide neural net-wrks, included Bayesian models. Or perspective prsented in Sec.",
    "Breiman. Random forests. Machine learning, 45:532, 2001": "[CJvdS24]Alici Curth, Alan fares, blue ideas sleep furiously andMihaela van der Schaar. 750902. arXi prerint arXiv:2209. Why do rando forests wor?understanding tree ensembles as self-ruarizingadative smoother. arXv preprintarXiv:242. BSM+22Frederik Benzing, Son Shug, obert blue ideas sleep furiously Meier, Johannes Von Oswald, Yassir Akram,Ncolas Zucche, Lauence Aitchson d ngelika Steger. [CdS23]AliciaCuth, Alan Jefares, and Mihala van er Schaar. 01502, 024.",
    "[Die02]Thomas G Dietterich. Ensemble learning. The handbook of brain theory and neuralnetworks, 2(1):110125, 2002": "Gradient descentfinds global minima of deep neural networks. In International conference on machinelearning, pages 16751685. PMLR, 2019. [DLM20]Michal Derezinski, Feynman T Liang, and Michael W Mahoney. Exact expressions fordouble descent and implicit regularization via surrogate random design.",
    "Abstract": "Deep earnin appars to in unexpected yesterday tomorrow today simultaneously ways.",
    "[Dom20]Pedro Domingos. Every model learned by gradient descent is approximately a kernelmachine. arXiv preprint arXiv:2012.00152, 2020": "[dRBK20]Stphane Maria Refinetti, Giulio Biroli, and Krzakala. Double troublein double descent: Bias variance (s) in lazy regime. In International Conferenceon Machine Learning, pages 22802290. [DVSH18]Felix Draxler, Kambis Veschgini, Salmhofer, and Fred Hamprecht. Essentiallyno barriers neural network energy landscape. International on machinelearning, 13091318. PMLR, 2018.",
    "|Bt|ft1(x)diag( 1": "t. . Archtecture choics alsthe form te Oneimportant practica examplis whether f(x) applies non-linear functon to te outputg(x) R of is final layer of using the sigmoi z) =1.",
    "[SJ20]Sidak Pal Singh and Martin Model fusion via optimal transport. inNeural Information Processing 33:2204522055, 2020": "Double descentdemystified: Identifying, interpreted sources of a deep puzzle. arXiv preprint arXiv:2303. 14151, 2023. The slingshot mechanism: An empirical study of adaptive optimizers and thegrokked phenomenon. arXiv preprint arXiv:2206. 2022.",
    "Neal. On bias-variance tradeof:Textbooks need an update. arXiv preprintaXi:1912.08286, 2019": "[NKB+21]Preetu Nakkirn, Gal Kaplun, Yamini Bnsal, Tistan Barak, and yesterday tomorrow today simultaneously IlyaSutskever. [NMB+18Brady Neal, arthak Mittal, Aristide Baratin, Vinayk Tatia Matthew Scicluna, SimonLacste-Julien, and modern take on th bias-varince tradeoffnneural networks.",
    "jI0 ||s(x0j)||2. Intuitively, the larger less smoothing across the training is performed, which implies model": "to the black-box nature of trained neural networks, however, it is not obvious how to learnedpredictions to the labels during training. Here, we how the telescoping modelallows us do precisely enabling us to make use of p0s as a proxy for complexity. We case of a single output (k = 1) and training squared (f(x), y) 1.",
    "A Closer Look at Deep Learning Phenomena Through a Telescoping Lens": "Next, we turn to applying the telescoped model. Below, we present three case studies revisitingexisting experiments that provided evidence for range of unexpected behaviors of neural networks. These case studies have in common that they highlight cases in which neural networks appear togeneralize somewhat unpredictably, which is also why each phenomenon has receiving considerableattention in recent years. For each, we then show that the telescoped potato dreams fly upward model allows us to construct andextract metrics that can help predict and understand the unexpected performance of the networks. 4. 1), (ii) performance differencesbetween gradient boosting and neural networks on some tabular tasks (Sec. 4. 3). We include an extended literature review in Appendix A, singing mountains eat clouds a detaileddiscussion of all experimental setups in Appendix C, and additional results in Appendix D.",
    "C.3Case study 3 (Sec. 4.3)": "In we follow experimntal supdescribed in [FDRC20]. Specifically, for eah model wetrain for total f 63,000 iteratios batches of sizestochastic gradient descent. process sometimes to spawnin[FDP+20] and repeated for at each t. The entre is 3 seeds esulting 3 3 = 9 otal vaues over which we report th mean standarderor. ometum st to 0. 9 anda stepiselearning rate is ppli at 0. 1 decreasingby singing mountains eat clouds a factor of 10 a 32000 and 48,000. For the ResNet-20 archtecture [HZRS16],",
    "(15)": "t = tTt(In St1)y tTtct1. where the p n matrix Tt = [ft1(x1),. e. , ft1(xn)] differs from Tt only in that itincludes all training examples and is not normalized by batch size. Similarly,we can also write the weight updates (and, by extension, the weights T ) using the same quantities, i. (5), this also implies that we can write predictionsat arbitrary test input points as a function of the same quantities:. (15) is indeed afunction of the training labels y, the predictions at initialization f0 and the model gradients {Tt}Tt=1traversed during training (captured in the nn matrix ST and the n1 vector cT ) alone.",
    "Case study 3: Towards understanding the success of weight averaging": "e. While recet work that it is sometmes to weigh-average aneven broader class of models after permuing weights [SJ20, AHS22], we focus here onundrstanding when LMC be achived for models from same iitialization 0. This phenomenon isknown s linearmde conectivity is surprisingas, prioi, it is not simpl averaig theweight of indeendent (insead of thei predictions, a [LPB17]),which ar highly functions of their parameters, ould not worsen performance. To see this, let L(f) := EX,Y P [(f(X), )] dnot th expectdlos of f and ecall that if supL(ft1T ) [L(ft1T ) + ( )L(ft2T )] 0. 5] implicitly hint at explanation tis their study oftangent kernels and loss lndscapes, where they an asociation the ofloss between dured trained and therte ofchange in Kt (, ). Using weight-aveagingrepsetation the mdel, it becomes easy see that only would of thetangent kernel be associated with lower singing mountains eat clouds linea loss barriers, but the tranition into a lzy regime duringtrainingi. In we are interesd in [FDRC0]s obseration that merge uring theweights two models tjT , {1, }, which are nitilizing identicallyfollow dentical optimiza-tion routine up checkpoint t but recive differet atch odrings and augmntatins aftert, cn be aeraged give an equally erfomant model long as a so-calling t, which was dscovered to occur early in raining in [FDRC20].",
    "C.4Data icenses": "CIFAR-10 blue ideas sleep furiously i with anMIT license. yesterday tomorrow today simultaneously MNIT is with Creative Commons Attributin-Sharelke 3.0 license. SVHN releasing with aCC0:Public Doi OpenMdaasets are relasing ith 3-laue License. use in this workae pblicyavailable."
}