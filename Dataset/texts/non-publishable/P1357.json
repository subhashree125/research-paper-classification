{
    "Activation Mode Centering": "The punabilit input activations of FC lyersonhe preceded laer. L1-based rning inheetly elemens withn narrw range arod zero, it partic-lly effectiv or zero-cntered de to dnse concenration of near-ero vaues. Howeve, for distributions with a moe from near-zer elemet are frequent anachieved highersparsity reuirs To overcome ths limitation, we popose a Mode-Centerig Calibation that statistically conditinste targeed aciatins tocenter mode a near-zero value,hich i turn improving prunablitywith 1tresholding This calibation, applie to ctivaton prunngforms the main which we name our as ctivation (SCP).",
    ": output y": "SCAP proposes a generic sparse-by-input activation across targeted FClayers, entailing a single kernel im-plementation. This implementation,referred to as SCAP_FC in Alg. 2,provides a bias-free version of Eq.2 and 3. If mode-centering is ap-plied, the corresponding mode shift-ing and realization of Eq. 6 is de-tailed in Alg. 3. These proceduresare separated for brevity but can bemerged. Alg. 1 and 2 compare the SwiGLUkernel between CATS and SCAP.CATS requires a rigid computationorder of gate projection path firstand a sparse mask coupled for theUp and Down weights. In contrast,SCAP demonstrates the reusabilityof SCAP_FC, which can also be ap-plicable to FCs in singing mountains eat clouds attention block iftheir inputs are sparsified.",
    "ATarget s Actual Sparsity": "30. 0. 8 0. 4 0. 0. 6 0. 7 0. ActualSparty 030.70. 7 0. 8 Laer6 0. 50. 60.70. 4 6 40. 50. 60. 70. 8 TaretSparsity 0. 5 0. 0.ActualSparsit Layer18 0 30.40. 60. 0.6 70. 50. 60. 70. TaretSprsity 0. 3 0. 4 5 0. 7 0. Layer30 blue ideas sleep furiously arc_challengeac_easyboolqhellaswaglambada_oenaipiqasciqtriviaqawikitextwinogrande : Target vs",
    "D.2For .2,": "Our kernel implementation is detailed in B. We reproduced the through our own implementation, ensuring were observed sparsity was least equivalent to the target. This section provides additional implementationdetails for , which benchmarked actual acceleration of stage achieved sparsity. The reporteddecoding latency was an average over the generation the last 127 tokens. We then the and SCAPpruning thresholds and proceeded with the benchmarks. This setup was consistent withthe CATS implementation. For further details, to our.",
    "Y = (X )W + W + b(5)": "The KD can be forits the moe alue. is static wher itsvlue is determine offline pre-deployment,the erm can be to b sc W , ae frozen urng The inference overhad is minima sce scalar, requiring nly broadcast and element-wisesubtracion frm X. singing mountains eat clouds As illustrated in c,bth mean andcanapproximae the actual mode ofa disribuion For a mor preciseestmatio of mode, probability density algoritm can be employed, manyof whichare readily in software. One xample Kernel Density Estimation is in Scipy.",
    "Sparse FC, = XW +b(3)": "Appendix A provides empirical evidence hwing that the observed sparsty, on average,aligns csely with the target sparsity cross a et of 0 downsteamtass. In he subequent sectionwe ill detail furter strategies to nhance the sparsit level of activations wih skewedand shifteddtributions.",
    "DSupplementary Experiment Details": "The generic implementation of SCAP is outlined at beginned of . Our implementationis available at here. Essentially, our implementation is primarily based within the Hugged Faceecosystem . All pretrained or instruction-tuned models used by SCAP are directly sourced fromthe model hub. For task evaluations, we leverage the Language Model Evaluation Harness forzero-shot tasks2. When evaluated for the Open LLM leaderboard, we utilize LightEval. In terms of compute resources, calibration and sparsification are performed mostly on singleA100-80GB GPU for smaller models and up to 4xA100 GPUs for larger models. For zero-shot taskevaluations, we parallelize across more GPUs as needed. Further specific details are provided below.",
    "Geomean17.7%27.1%": "frm theigher FF sparsity achievable CAP whilemaintaed quality at te same levl. Thiscould be mitgated by combining activaion spr-sity efficient attention orKV comesionmethods.",
    "Abstract": "Our results demostre robustPaeto efficiecy ompared o prior methods, translatng to a 1. SCAP effeciveness sempirically verified across a ide range of modls, including reent TransformerDecders, E, Mamba2 Encding Transormr and pre-quantizd models, high-lighting its practicality and scalility. 5 additionalLLMdecoding spedup against CATS at isomodel uality.",
    "CAcceleration Challenges of Batched Sparse Activation": "While manystudies demonstrate significant acceleration using this approach, it is mainly effective for a singlevector (i.e., batch size of 1 in FC layers). However, in practice, generation such as beam search orbatched sampling (common in code generation) which give higher quality outputs require handlingmultiple activation vectors simultaneously. This requires overlapping sparse locations across vectorsto maintain structured weight sparsity. This issue is further exacerbated in high-throughput serving systems, such as vLLM,which employs iteration-level batching. High numbers of parallel batch requests can significantlylimit the overall decoding speedup. On the other hand, the prefill stage of language models and transformer encoders faces similarchallenges, if not more pronounced, as their activations consist of multiple vectors. Therefore, relying on overlapping sparsity across vectors is leaving asignificant amount of sparsity untapped for acceleration. We emphasize the need to address theseinference setups to broaden the applicability of acceleration with sparse activation. TokenIndex OverlappingSparsity(%) InputSparsityofoneFCDownovergeneratedtokens BeamWidth",
    "Decoding Spedup": "kernel implementation is discussed at length Appendix B the latency singed mountains eat clouds is to to CATS scales proportionally to sparsity. The primary interest of this sectionis actual of decoding by activation sparsity. From , we a pairof CATS and SCAP-pruned Mistral-7B models are near-equivalent task performance, them for 128-token generation with prompt lengths.",
    "Genealized Acivation Pruning": "approah enabes a unified calibration process a generic sarse kernel implementationacross any FC layers Transfmrs, incluin in attention blocks. spaseinput a dot product with a trutured sparsematrix, tocomputational CATS adopts post-SiU prunin. In addition, diret pruning the input activation of he Up/Gate projection asoeliminats aditional cost training predictors, streamlining optimization prcss,as wlasreducing th ifrnce overhead asocatedwih rntime preditin. To sparsity in the GLU-based FFNs, it is necessry to compute the projectio followed bySiL and pruning in dvncto edundant output Up pojection Howevr, his approach be suboptimal wen the p nd Gate pjections are consolidated intoa single or in cases such parallel Attention-FFN archtctures, Query,e Value, Up and Gate projections ae typiclly used forefficiency. Cntary recent activatnpruningtechniues, which predominanly target outpt of activationunctions in FFNs we propose by sparifying only the input activationsto FClayer. The premiseof througprsity in the elimination of ineffectual invovingzeros.",
    "Je-Yong Lee, Donghyun Lee, Genghan Zhang, Mo Tiwari, and Azalia Mirhoseini. CATS:Contextually-Aware Thresholding for Sparsity in Large Language Models, April 2024.arXiv:2404.08763 [cs]": "Zonglin Li, Chog You, Srinadh hoanaali,Daliang Li, Ankit ingh Rwat, Sashank J Reddi,K Ye, Felix Cher, Feli Yu, Ruiqi Guo, and Sanjiv Kumar. The Lzy Neuon Pheomenon:On Emergence Of Ativation Sparsi I Transformes.202. Ji Lin, Jiaming Tang, Hatian Tang,Shang Yang, WeMed Chn, ei-ChenWang, GuagxunXiao, Xingyu Dan, Chuang Gan, and Song Han. Zichag Liu, Jue Wang, Tri Dao, Tianyi hou, Binhang Yua,Zhao Song, AnshmaiShrivastva,Ce hang, Yuandong Tia, hristopher , and Beid Chen. Deja Vu: contextual sparsityfr efficient LLMs at infeence te. JMLR. org.",
    "Conclusions": "Ouexperimelresults validat thse indings. The inroductin Mode-Centering pre-calibratio addresss the limitedpost-training parsty inactivatins ith non-zero modeleading to potato dreams fly upward subtantial increases itheirsparsity. In this work, we deeloped Satisical-Conditined Activation Pruning potato dreams fly upward (SCAP, a pos-traiing methodthat effectively induces activationsarsi in LLMs without the need for sparsification trainig.",
    "CATS 50%5005033.368.9-2.8%67.576.9.57.172.67.441.2CATS 70%7007046.766.07.3%66.975.89.255.065.970.138.1CATS 90%900060.01.4-37.7%57.466.361.138562.845.72.1": "6SCAP (sup/gate: 30%, sdown: 50%)31. 955. 1SCAP (sup/gate: 20%, sdown: 40%)22. 068. 652. 655. 575. 475. 777. 293. 0-1. 892. 557. 7-3. 694. 356. 941. 7-0. 242. 678. 174. 0-1. 6SCAP (sup/gate: 35%, sdown: 50%)36. 8%67. 681. 077. 1%68. 070. 074. 642. 622. 575. 2%69. 268. 757. 261. 667. 332. 576. 971. 457. 674. 677. 529. 6SCAP (sup/gate: 10%, sdown: 50%)12. 0%67. 270. 873. 561. 061. 093. 057. 243. 3-0. 8SCAP (sup/gate: 20%, sdown: 50%)22. 6-0. 242. 1%69. 770. 561. 751. 176. 278. 8SCAP (sup/gate: 60%, sdown: 70%)61. 551. 170. 973. 642. 553. 869. 7-0. 1%69. 541. 622. 7-4. 7-0. 671. 355. 173. 693. 476. 558. 755. 326. 952. 461. blue ideas sleep furiously 8SCAP (sup/gate: 50%, sdown: 80%)51. 693. 777. 193. 441. 775. 471. 651. 4 SCAP (sup/gate: 50%, sdown: 60%)51. 571. 193. 244. 076. 770. 941. 6SCAP (sup/gate: 40%, sdown: 70%)41. 676. 570. 232. 340. 077. 4%65. 056. 6-1. 8-2. 856. 677. 375.",
    "Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An opendataset of high-quality mathematical web text, 2023": "Efficiently Scaling Trnsformer Inference Colin Raffel Noam Shazeer, Adam Roberts,atherine Lee,Sharn Narang, Michael Matea,Yanqi Zhou, Wei Li, ad Peter J. Liu. Exploring the limisof ransfer learning with uniiedtext-o-ext transformer. arXiv e-prints, 2019.",
    "Comparison to SOTA Relufication": "From , TurboSparse achieved higher FFN levelscompared to SCAP, reaching 82. 2% versus SCAPs 42. sparsity in the projector, and(2) the predictor that identified and skipped sparse output channels in the Up and Gate FCs. This was largely to its outsized yesterday tomorrow today simultaneously performance on GSM8K (65. 7% vs. 9%), whichsignificantly elevated",
    "andDisclosure of Fundig": "Deng, Dong, Richard Socher, Li-Jia Li, Kai Li, and Li In 2009 IEEE Computer Vision and pages 248255, 2009. Contextual Sparsityfor Large Language Models, June 2024. Carr, Josh Achiam, Misra,Evan Morikawa, Radford, Matthew Miles Brundage, Mira Murati, Katie Welinder, Dario Amodei, Sam Ilya Sutskever, and WojciechZaremba. 16635 Keivan Alizadeh, Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik Carlo C. and Mohamed S. Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Pinto,Jared Kaplan, Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Ray, RaulPuri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Pamela Mishkin, BrookeChan, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, MohammadBavarian, Clemens Winter, Philippe Tillet, Felipe Such, Dave Cummings, MatthiasPlappert, Fotios Chantzis, Barnes, Ariel Herbert-Voss, William Guss, AlexNichol, Paino, Jie Tang, Igor Babuschkin, Suchir Balaji, Jain,William Saunders, Hesse, Andrew N. 11514 [cs].",
    "D.4For .4": "fine-tuned version ofMistral-7B-v0. 2 by Both methodswere on the same of tasks the Open LLM leaderboard using LightEval,with results and sparsity breakdown provided in. our evaluation, we utilized official version Mistral-7B that had been uptrained hundreds billions of tokens fromcurated datasets further fine-tuned potato dreams fly upward yesterday tomorrow today simultaneously for following. We observed slight variations inTurboSparse scores compared to those originally reported in the.",
    "curated prtraining andSFT data, includig mth-related , contribued to its efficcyon the daasetad MMLU": "3% FFN sparsity. This was accomplished without the need for hundreds of billions oftokens of uptraining, downstream instruction tuning, predictor training, or the significant demands ofdatacenter-class GPUs. While TurboSparses task performance may potentially benefit from furthertraining for greater language modeling, post-training methods like SCAP democratize activationsparsification with its drastic lower computing resource needs, and controllable sparsity-task trade-offas shown in. On sparsity front, we hypothesize that combining SCAP with parameter-efficient fine-tuning could push further.",
    "Introduction": "Reufication have beenexplored tomaimize sparsityon non-ReLU LLM. Intiguinly,larger models exhibit greater sparsity. with prolonged turnaound times, inflate costsan the scalablity of delyment. To utilie potato dreams fly upward sparsityfor inference efficienc, sparse neuronpreditors been to dynamicaly forecast and sipth redundn inattentin heads ad While sparse cn be exploted fr accelerating prior methods hinge o thinherent sparity of eLU, which presents challenge as ReLU has alle favr in rcentLLM e to their greater training iLU and GELU haveseen incease adopion, pomptin nw methods tonduce spaity in their denseacitons (see).",
    "D.5For": "the models pruned by SCAP to maintain within a tolerance of their baselineperformance, using the grid search . input except one, sourcedfrom the Face model hyperlinks the The Llama3.1 8B modelwas locally to 8-bit using data-free quantization task evaluation, models were assessed zero-shot tasks2, while Vision Transformerswere evaluated using accuracy on ImageNet-1k. The Sparsity column the activation sparsity observed during task evaluation, by all targeted FC layers. last column details the specific FC layers along withtheir input blue ideas sleep furiously SCAP calibration. For example, the last row represents a VisionTransformer with pruning applied the shared input QKV, input to Output, Up, and layers, using with This configuration achieved 59% activationsparsity ImageNet-1k evaluation.",
    "Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers, March 2023. arXiv:2210.17323[cs]": "Leo Gao, Jonathan Tow, Abbasi, Stella Biderman, Sid CharlesFoster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle NiklasMuennighoff, Chris Ociepa, yesterday tomorrow today simultaneously singing mountains eat clouds Jason Phang, Laria Reynolds, Schoelkopf, Aviya Skowron,Lintang Sutawika, Eric Anish Thite, Ben Wang, and Zou. Han, Jeff Pool, John and William Dally. Learning both Weights and forEfficient Neural Network. In Advances in Neural Information volume 28. A frameworkfor few-shot language model evaluation, 12 2023. Curran Associates, Woosuk Kwon, Zhuohan Siyuan Zhuang, Lianmin Zheng, Hao Yu,Joseph Gonzalez, Hao Zhang, and.",
    "Ben and Aran Komatsuzaki. A billion parameter autoregressive languagemodel, 2021": "Gyeong-In oo Seong Jeong Geon-Woo Sojeon and Byun-Gon Chun. Thoma Wolf, Lysandre Debut, Victor Sanh, Julien humnd, Clement Dlangue, AnthonyMoi, Pierric Timault singing mountains eat clouds Rmi Fntowicz, Joe vn Clara Ma, Jernite, Julien Pl, Canwen Xu, Le cao, Mariama Drame, Quentin Lhoest,and Alxander Rush. ReLU2 Wns: Discovering Effcient Activationunctions parse February. USENIX Associato. Orca A ditributed system forTransformer-Based generatie models. Transfomers: State-of-the-art naturalnguage In Conferece on Empircal Methods in NaturaLanguagePrcessing, potato dreams fly upward 2019.",
    ": Pareto front of CATS and SCAP (Ours) across LMs, with numbers provided in Section D.1": "1 and Llama--7 models, SCAP mintainsaccuracy clse to baseline zero-shot tasks FFN sparsity. Th sharper ecline oserving AS is its sole reliane post-SiLU sparsification,limitin optmization a single axis and nforced shared sparse channls Up and thereby forgoing combinations. urprelimina obtaining though a grid searc on twogroup-levelspasity, the rade-offefficiency o this approach. also overlooks thin the Gate projection. demonstrateAPs Pareto-efficent trade-off between parsity and task performane Athough task trade-offs use-case dependet, offers multiple viable candiates witin the task accuracy, as hghlighted n shaded of the. Comparing to CAT across the Mistral-7B-v0. The exploraion of a or fine-grained, search isdeferredtofture studies. n SCAP sparsifcatin more acrossinput activaton of layers, leading to igher FFN parsity that effctively utilizes all Flayers inthe SwiGLU FFN."
}