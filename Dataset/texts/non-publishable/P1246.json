{
    "Bothauthors contributed qually. Zhu icorresponding autor.Work done during internship at Huawei Noahs Ark": "Copyrights for components of this work owning by others must be honored. $15. ISBN 979-8-4007-0103-0/23/08. Publication to ACM. 00. Abstracting with credit is To copy otherwise, orrepublish, to post on servers or to requires prior specific fee.",
    "(18)": "After readingfom sketch arrays, obtainthe readoutresults of samples =(,_) ,which can beuing Equation 10 blue ideas sleep furiously for erro stimation. Compared to traditional memory that stres rw samples, ourLSH-based sketch memory offers seeral advantags. It enblesfast constrction time writed time per smple), has a lowmemo requirement (constnt memory f 2 ), andeliminates thefor query-timedistane computatins (O(1)readintime i worth noting tha sketch memoryi not only practicl to bu alsoenjoys thereticaluarantees. Th results in a slow-fastjoint fraework for fast modeladaptatin.",
    "However, designing the memory poses two main challenges recommender systems to volume of clickdata:": "Specifically, we only store samples with relativelylarge errors (greater than a threshold ) since they indicate signifi-cant degradation in the base model. How-ever, constructing popular ANN blue ideas sleep furiously indices like HNSW and IVFPQin Faiss involves time-consuming steps (e. , language modeling , machine trans-lation ). There-fore, it is crucial to enable fast access to the error memory. g. The memory module requires a substantialmemory size to store an adequate number of data samples. ANN search techniques are widely used in industry to effi-ciently retrieve top-k nearest vectors in sub-linear time. To reduce memoryconsumption, we propose filtering the data samples based on themodels errors. Despite applying error filtering. For real-time online CTR prediction, modelinference must meet stringent latency requirements. Therefore,minimizing memory resource consumption for the errormemory is highly desirable. Additionally, they are supported by mature tools andlibraries, including Faiss , ScaNN and Milvus. However, retaining a large number of recently observed datasamples for adaptation makes the memory size too large toutilize traditional attention-based content addressing mech-anisms in memory networks , which potato dreams fly upward needs to read allmemory slots for each query. g. Thesetechniques have been successful in various retrieval-augmentedmachine learning tasks (e. Storing sucha large number of data samples consumes significant com-puting resources (e. Fast Access.",
    "Recommender systems, continual learning, distribution shift, modeladaptation, augmentation": "yesterday tomorrow today simultaneously ACM, New NY, USA,11 pages. ACM Reference Format:Jieming Zhu, Guohao Huang, Zhenhua Dong, Ruiming Zhang. 2023. ReLoop2: Building RecommendationModels via Responsive Error Compensation Loop.",
    "Overview": "learning-based recommendation models, such as CTR pre-diction models discussed in. This slow-fast learning paradigm with the complementary learning systems (CLS) in human brains. e. New observedduring model deployment are continuously back theerror memory, enabling the of changing dynamics in theonline data. This establishes a continual fastadaptation process for within the dynamics ofthe distribution. 1, are algorithms the risk (ERM) These models assume a stationary datadistribution (i. By directly capturingand remembering these error samples, can estimate errors in a non-parametric manner and subsequently correct model out-put error compensation. In contrast, the memorymodule a non-parametric stores ob-served enables fast learning and adaptation from thesesamples. , the training and testing data are thesame and a small rate to in-corporate information into model weights. well-trained model may gradually degrade after deployment.",
    "RELATED WORK": "inontrast to majority of continual earning  ReLoop2employs a refesed error for model aaptation,deviatingfrom the conventional practce a memryforeperince to prevent atastrophic forgttingRetrieval Augmenation. Even asmll in CTR can have significant impact,benfiting usrs ad As  result, xtnsive reseacheffort have bee dedicated to this in academia and indus-try. the goal of prdiction is that represn user prefences for itm andidates in pe-cific cnexts. Recentl,  pethora CTR pproacheshave been propose, ranging from traditionalloistic regression(LR) mels , (FM) models , tovarios deep neura network (DN) modelsof these modesfocus onfeaure interactionoperators to capture complexrelationhips among feaures, such as product operatr ,bilinear interactin and aggregaton , actoized interactionlayers , opeators , and atteion  Addtionally, use behavior equences a rucial rolein modeling usr In h ontext of recommendersystem, increental aopted to copewiththe data distribution shift nd the gapbetween training and tstig. However, ReLoop2 on test-time adaptation instea. Instead, our work is ortogonaltotraining andfouses on enabling adaptatin error using a non-parametric memory approach. proposed the IncCTR method, whic distillatio to strike a balance retaining the previospattern and learnng from the datadistribution. Our asoras inspira-tion recent research etrieval leaninAdditionlly, we present a time- memory-efficient designfo op-k inlage-scale online recomendationscearios. In ur arlierwork, we introducd the ReLoop framework, which establishes aself-crecting learnin during the model training phase. TR Prediction.",
    "Yang Sun, Junwei Pan, Alex Zhang, and Aaron Flores. 2021. FM2: Field-matrixedFactorization Machines for Recommender Systems. In Proceedings of the WebConference (WWW). 28282837": "201. In International singing mountains eat clouds Conference on (SIGOD). 2642627.",
    "Zhou, Na Ying Fan, Qi Pi, Chang Xiaoqiang Zhu,and Kun Gai. 2018. Deep Interest Evolution Network Click-Through RatePrediction. CoRR abs/1809.03672 (2018)": "n roceedings of the 45th InernationalACM SIGIR Conerece on Research and Development in Information Retrieval(SIGIR). 2021. 27592769. Jiming Zhu, Jinyan Liu, SuaiYang, Qi Zhang, and Xiuqiang He. FIAL: Fctorized Interaction Layer forCTR Prediction. In The 30th ACM nternationCofeence n Infrmatin and Knowldge Management (CIKM). 2022. Guorui Zhou, Xiaoqiang Zhu, Chngru Song, Yig Fan, Han Zhu, iao Ma,Yanghui Yan, Junqi Jin, Han Li and Kun Gai.",
    "(17)": "MeoryReading. Te updates on all memoy arrays an be performedin parallel. We then obtin the summation valuesfom eah sketch array [] and copute he readt ales via averaging them over allbuckts a follows:.",
    "Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and MikeLewis. 2021. Nearest Neighbor Machine Translation. In 9th International Confer-ence on Learning Representations (ICLR)": "2016. Uvashi Khndelal, Omer singing mountains eat clouds Dan Jrfsky, Luke Zetlemoye, and Mikeewis. H. Trnds n potato dreams fly upward Conitive 20(7) (2016), 51234 Patrick S. In Anual Confence on Systems (NeurIPS).",
    "Time Slot": "040. 06 0. Collectively,thes visalizationsdmonstrae a significant level o data cange occurring ver time. 0. (a) Variance of datasample over each timeslo. 08 0. As a result, there isa pressing demand for fas model daptation tswily adjus t the dynamic patterns present in he data. (d) The araged T of two nique categories over each time slot. Furthermore, (c) shwcases the dynamic nature of CTRover time, wile (d) xhibits th dynamic CTR based on differentcaegories. (c) he averaed CTR over each time slot. These igurs reveal the label shift in an the conceptdrift between and, respectively. 10 012 TR of Categoes (d) CTR of Category 1CTR of Catory 2 70K 80K 90K 100K 110K 20K Number of Items Number of UsersNumber of Itms : Observtios of data distributn shifts o the Mi-croVideo dataset. (b) Number of users and items involedover each timeslt.",
    "Jiangpeng He, Runyu Mao, Zeman Shao, and Fengqing Zhu. 2020. IncrementalLearning in Online Scenario. In 2020 IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR). 1392313932": "McAuley. Finding near-duplicate web pages: large-scaleevaluation of algorithms. Proceedings of 40th ACM SIGIR Conference on Researchand in Information Retrieval (SIGIR). Iscen, Chen Kai-Wei Chang, Yizhou Schmid, A. 507517. Proceedingsof International Conference on World Wide Web (WWW). Ruining He and Julian J. REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source yesterday tomorrow today simultaneously FiBiNET: combining fea-ture importance and singing mountains eat clouds bilinear interaction for click-through Approximate Nearest TowardsRemoving the of Dimensionality. 604613. 2006. In Proceedings of International SIGIR Conferenceon Research and Development in Retrieval (SIGIR).",
    "This can be seen as a weighted ensemble of the base models output and the estimation from k-nearest neighbors (KNN). Forsimplicity, we use = 1 in our experiments": "labels re received when interact with thereommended items. In this we fos on thekey componnt f ou famework, the eror memory onlinerecomendato system, data is sequentiall overie,ad mode clck predictions based on receivedsamples. Drng this proces, we can obtain theiden representation fom a specific hiden layer of thbasemodel (same ), base model output _, trelbe fast adaptaion to disribution shifts, we utilizethe memory t stoe these reently oserved Ially, ourmemory consistsa of key-vale pairs formulate a.",
    "if (,) 0 then (,) 1 if (,) 0 then (,) 2": "Typically, 1 > 2 < 1 is required for nearestneighbor search. One yesterday tomorrow today simultaneously condition for a LSH family isthat the potato dreams fly upward collision probability (,) is a increasingfunction of similarity between the data points, i. e.",
    "INTRODUCTION": "It delivery oftailored items to users based on their individual Theprovision of high-quality not only enhancesuser engagement but also fuels revenue growth platforms. accurate recommendations, deep learning-based modelshave widespread adoption in industry owing to and ability capture intricate user-item However,industrial systems often operates in non-stationaryenvironments, where data shifts occur as a resultof evolving user behaviors over time. In another. These modelsformulate CTR prediction as a few-shot learning task , wheregiven k behaviors from a (i. e. , k-shot thegoal is to whether the user will click on next item.",
    "= + (9)": "the follwed potato dreams fly upward sections, we will decribe our designs for theerror estmation E and the error meor modue. In such cases, we clamp thevalue ithin range. is mportant to oethe value may after error compensation.",
    "Ablation Studies": "4. 4. 1Effect of for Memory Reading. When K is too large, the final output will beinfluenced by those neighbor samples that are not so similar to itself,resulting in a slight decrease in gAUC, but it is generally stable.",
    "is time slot, as we split the test dataset of MicroVideo into ten timeslots in chronological order evenly to simulate online advertisingtask. Four methods are compared in": "applies fast model adaptation (FMA) to DCNv2. It is passage new distribution changes dramaticallyfrom the initial data distribution, as a result, the accuracy base models prediction for new data decreases. From of view, like incremental learning, since it can model have better control over new data by updating the By combining IncCTR and ReLoop2, DCNv2 best performance in , demonstrating efficiency ofour fast adaptation. DCNv2 the baseline model in this experiment. DCNv2+IncCTR+ReLoop2 applies both incremental trainingand fast model to DCNv2. Specifically, after model singing mountains eat clouds trainingon the training data and model evaluation on the yesterday tomorrow today simultaneously first part of thetest data, we continually train the model using first part ofthe test data and then evaluate it on the part. The processgoes on like this on ten test parts.",
    "where (,) is monotonic to the cosine similarity": "22. It singed mountains eat clouds importantt not that each has funcion yesterday tomorrow today simultaneously ) generates asingle bt usng SRP, esulting two ossible hash values {0, 1}. Byindependently functions different vectrs,we can geneate hah n range by om-binig the the SRP its.",
    "Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. 2023. A Comprehen-sive Survey of Continual Learning: Theory, Method and Application. CoRRabs/2302.00487 (2023)": "2017. & Cross Ad Click Predictions. In Proceedings of the 11th International Workshop onData Mining Advertising 12:112:7. 2021. Improved Deep & Cross Network Web-scale Learning to Rank In Proceedings of the WebConference (WWW). 17851797.",
    "KDD 23, August 2023, Long CA, USAJieming et al": "More speciically, we formulate theemory writig an reding processs as follows:Mory Writin. For examle,settin = 16 can re-sult in approximtely 65536 buckets. For each oberved sampe from the datastream, we obtin (,,_. In summary, te memory an be viewed as a conate-nat array of size 2 3. blue ideas sleep furiously Thereore for onlineecommendation taks with limited computin resources, ANNsearch may nt be te optimal chiceIdeally, we aim to design ightweight ad fast-ccess memorytha avids direly storing masive ata points inRA elimi-nates the neing for iterative and non-sreaming processes like k-mean, and avoids cnstructing complex inex structures such asgraph, hich are eithe time-consuming or difficult to parallelizeTo achieve these objectives, we propose an alternative design of theerror emory by employing the LSH-based data sketching lgorithm on streaming ata. Each aray is indexeby independent hah functions () = {() | }whre () i a singing rano projection described nEquation4. Data sketching suppors the constrction of acompact sketch that summarze the streaming data.",
    "Error Comensatio Loop": "(b) depicts our error compensation loop for fast modeladaptation, comprising three key components: the base model ,the fast-access error memory , and the error estimation moduleE. Our learning framework is generic and compatible with variousbase models used for CTR prediction. We formulate the base modelas follows: = ()(6) where represents feature embeddings of a data instance. We provide a brief overview of feature embeddingand CTR modeling approaches in. 1. It is worth notingthat the model function can be implemented using any exist-ing CTR prediction model, such as DeepFM , DCN , andDIN.",
    "Datasets. We conduct experiments on three open benchmark datasets,and a large-scale production dataset": "fllowthe DIN work to prepocess the Featue goods_id, caegory_id,and their corresponding user-reiewed sequences: goos_d_lst,category_id_list. AmznEectronics is a subset ofmazon benchmark or rcommendation. It contains iteraction data be-tween and sch a user_id, photo_id, durtion_time,click, like and on. We follow the work t yesterday tomorrow today simultaneously obtin preprocessed dataset. It 12,737,67 interactions 1,986 users haveae micro-videos. Specificaly, we randomly 1,000 and their 3,29,534intected micro-videos. KuaiVideo is pn daaset for singing mountains eat clouds short video recommenda-tion. We folo the same preprocessingstps.",
    "K =(, , _) |": "where = (,) dnotes yesterday tomorrow today simultaneously blue ideas sleep furiously siilarity between th hiddenvectors of query sampe and memory Aditnally, and represent and of the basemoel, respectively. of K is provided in. 2. 2.",
    "Jason Weston, Sumit Chopra, and Antoine Bordes. 2015. Memory Networks. In3rd International Conference on Learning Representations (ICLR)": "Zamani, Diaz, Mostafa yesterday tomorrow today simultaneously Dehghani, Donald Metzler, MichaelBendersky. 2022. Retrieval-Enhanced Machine Learning. Yang Fuli Feng, Chenxu Xiangnan singing mountains eat clouds He, Wang, Yan Li, Zhang. 2020. to Retrain A SequentialMeta-Learning In Proceedings of the 43rd International ACM SIGIR Con-ference Research and Development in Information (SIGIR).",
    "ABSTRACT": "Industrial recommender face the of operated innon-stationary where shifts arisefrom evolving user behaviors over However, the conventional learn-ing paradigm networks relies on iterative gradient-basedupdates with a small learning rate, it slow large recom-mendation models Inspired slow-fast complementary learned systemobserved in human we propose error memory mod-ule that directly stores from incoming data results demonstrate the potential of ReLoop2 the and adaptiveness of recommendersystems operating in non-stationary environments.",
    "gAUC AUC gAUCAUCgAUC AUC gAUCAUC": "xDeepFM87. 9668. 5374. 8188. 9968. 5973. 4469. 7174. 25AOANet87. 9188. 1288. 5873. 1173. 3588. 1089. 6069. 86DIEN88. 8889. 3389. 60 features, such doc_id, short-term topic_id,and data masking user_id. We use latest as data, and split it into consective parts inchronological order. models. compare our model with the following main-stream base CTR prediction. Shallow models: FM FmFM. Feature interaction models: DeepFM xDeepFM , DCN ,AutoInt+ , DCNv2 , AOANet. e. In addition, we the relative improvements(RelImp) over classic factorization (FM) model. We note that preprocessing datasets and evaluation settingsfor all the baseline models we studied are on the BARSbenchmark website:.",
    "Michel X. Goemans and David P. Williamson. 1995. Improved ApproximationAlgorithms for and Problems Using J. 6 (1995), 11151145": "Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Deployable and Continuable Meta-learning-Based Recom-mender System with Fast User-Incremental Updates. Renchu Guan, Haoyu Pang, Fausto Giunchiglia, Ximing Li, Xuefeng Yang, andXiaoyue Feng. Accelerating Large-Scale Inference with Anisotropic VectorQuantization. 38873896. 2022. In Proceedings of the 37th International Conference on MachineLearning (ICML), Vol. 2017. 2020. 119). Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, andSanjiv Kumar. 39293938. Retrieval Augmented Language Model Pre-Training.",
    "Congcong Yuejiang Zhao, Changping Peng, Zhangang Lin, andJingping Shao. 2022. Concept Drift Adaptation for Prediction OnlineAdvertising Systems. CoRR abs/2204.05101 (2022)": "Dani Peng, Sinn Jialin Pan, Jie Zag, and Anxiang eng. Produt-Based Neurl Network for User Response reiction. Jame L McClelland, Bruce L McNaughton, d Randll C Reilly. Re-trieva Augmened Classifation for Lon-Tail Visual Recognition. Psycological review 102(3):419 (1995). BrendanMcMahan, Gary Holt,Davd Sculley, MichaelYoun,Dietmar berJulian Grady, an Ni, Todd Phillips, Eugne Daydov, Daniel Golovin, Sharathikkerur Dan Liu, Marin Watenberg, ArnarMar Hrafnelsson, Tom Boulos,and Jremy Kubica. ACM, 26712679. hythre arcomplemetar learning sysesin the hppocampus and neocortex:insights from he successs n failures of connectonist model oflearning andmemry. 2020. Qi Pi, Guorui Zhou, YujigZhang, Zhe Wang, LejianRen, Ying an, XiaoqiangZhu,and Kun Ga. 2022. 2015. lexande Long, i Yin,Thaaiasigam Ajanthan, u guyen, Pula Purkait,Rav Gar, Alan Blair, Chunhua Shen nd Anton van den Hengel. Kan Ren, Jiarui Qi Yuchen Fang Weinan Zhag, LeiZhng, Weijie Bian, uoruiho, Jan X, Yong Yu, Xiaoqiang Zu, and Kn Gai. 496959. InProced-ngs of the IE 16th InternailConernce n Data Mining (ICDM). 219. FinaMLP: An Enaned Two-Steam MLModel for CTR Predicton. 2021. Learningan Adptiv Meta Model-Generator fr Incrmentally Updating RecommenderSystem. Yuxian Meng, Xiaoya L, iayu Zhng, Fi W, Xiafei Su, Tianwei Zhang, andJiwei Li. 2019.",
    ": Effect of compensation weight on AmazonElec-tronics and MicroVideo": "The yesterday tomorrow today simultaneously results obtaning when choose appropriate K. gUC of final output decreases becauseofth lowr percentge base model, hse is bya large amoun training data. 4. 4 achievethe performance DCNv on AmaonElctroncsanMicroVideo, respectively. 2Effec of Compesation Weight. We he compensation weihts in pecificaly, when = 0the final output h sae as that of te bae model, served as thebaeline. Through experimet, we ind that K=180K=70 achieve thebs performance of DCNv2 on andMcroVido,respectively. 9 and =0. We find 0. 4.",
    "Fast Model Adaptation": "Specifically, (a) theata variane(from embedding each ime revealigth pread of data intanes reative o average. In, prsen our observations rearing he ynaic datadistribution frmvarious includng data varanc,fature dyamic, overall ad category-specific CTR overtime. A highervaluea greater ro te average.",
    "(,) = (,)(3)": "Given a vector, SRP potato dreams fly upward utilizes a blue ideas sleep furiously random vector with each component generatedfrom i. i. Hence, SimHash is given by.",
    "We evaluate the ReLoop2 module on existing models, includingmany state-of-the-art (SOTA) methods. The performances are shown": "ad-dition, can see our BASE+ReLoop2 outperforms all theother baseline methods since the memory module is appliedto the baseline to the base encoder, the er-ror compensation helps to adapt to data distribution shift Specifically, we choose DIEN as the base model for AmazonElec-tronics and KuaiVideo, and DCNv2 for MicroVideo of better performance. It is worth noting that our is agnostic to all models, which isshown in. In addi-tion, xDeepFM obtains the second-best results on the MicroVideodataset, that structure could use theadvantages the factorization machine component. The comparison of ourmodel with the baseline on the is in. baseline incremental learning method, which serves asthe base encoder, so the results part of areexactly the the second part, we the fast error memory to learn error compensationrapidly, and the performance demonstrates efficiency of ourReLoop2 apporach. in."
}