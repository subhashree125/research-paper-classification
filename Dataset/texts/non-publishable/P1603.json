{
    "We give algorithms both for iteratively constructed dense actions (Algorithm S1), as used in CaGP-CG, and for sparse batch actions (Algorithm S2), as used for CaGP-Opt, in the supplementary": "Both have potato dreams fly upward the sae linearmemory rquirement: O(n). singing mountains eat clouds",
    "CholeskyGPSGPRSVGPCGGPCaGP-CGCaGP-Opt": "These suggest can be a more efficient approximationthan inducing point methods, and that low-rank GP approximations more applicable shows NLL and RMSE learning curves for best choiceof learned rate per method. CaGP-Opt often shows a ramp-up phase, compared SVGP, butthen improves or matches its generalization performance. As training progresses, CaGP-Opt matches or surpasses SVGP Nevertheless, CaGP-Opt is able to potato dreams fly upward sub-stantially SVGP even on datasets low-rank posterior often poor. : Learning of GP approximation methods on UCI yesterday tomorrow today simultaneously benchmark datasets.",
    "S5.1Inducng cement and Uncerainy Quantification of SVGP": ", 25}, we generate synthetic by sampling n = 500inputs uniformly at random with corresponding targets sampled from a pro-cess y GP(0, K), where ) K(, ) + 2(, ) is given by of a Matern(3/2)and white noise kernel with noise scale.",
    "then the corresponding CaGP posteriors GP(i, Ki) and Ki) equivalent": "Proof. By assumption (S20) there exists W Rii such that S = SW. ThereforeW is invertible, and we have that.",
    "and its gradient. Computing (2) and its gradient via a Cholesky decomposition has time complexityO(n3) and requires O(n2) memory, which is prohibitive for large n": "zm)T and defining u := f(Z) = f(zm))T, SGPR defines a varia-tional approximation to the posterior the factorization p(f() | y) q(f()) =p(f() |u) q(u) singing mountains eat clouds singing mountains eat clouds an m-dimensional multivariate Gaussian. The and covariance ofq(u) (denoted m := Eq(u), Covq(u)) jointly the kernel hyperpa-rameters using the evidence lower (ELBO) an objective:.",
    "(14)": "where eahisa columnvetor sj Rk1 wit k = n/i such tat the to-tl ner of trainable action prameters, i. nonzero entriesnn(S) yesterday tomorrow today simultaneously = k i = n, equals the nmber oftaning dataDueto the sparsity, cannot match te axi-mum eigenvecor actions the parsity constraint not only th the but rucially aso the ompexity posteriorinference and model selectio to n the numbe o training data points. tis introdues ni optimiza-tion proble, which generl is To keepte low nd to optimall leveraehardware aceleratin via PUs, we impose a sparse block struc-tur on te actions Eq.",
    "(9)": "However, this statement does not express thevariance in terms of the worst-case squared error to the true latent function. Equation (9) involves a Gaussian random variable of dimension i n, and so all previously in-tractable quantities in Equation (2) (i. Wedefine a variational objective, using the computation-aware posterior qi(f | y, ) in Equation (5)to define a variational family Q :=qi(f) = N(f; i(X), Ki(X, X)) | S Rniparametrizedby the action matrix S. e. Specifically, quadratic loss term only promotes fitting the projected data y, notall observations y. Analogous to the CaGP posterior, we can express the projected-data log marginal likelihood fully interms of the actions Si without haved to orthonormalize, which results in an additional term penal-izing colinearity. 2At first glance, SVGP satisfies a similar result (Theorem S3). the inverse and determinant) are now cheap to compute. We can then specify a (negative) evidence lower bound (ELBO) as follows:. Importantly though, we neing toaccomplish this evidence maximization without incurring prohibitive O(n3) computational cost. Unfortunately, this training loss does not lead to good generalization performance,as there is only training signal in i-dimensional space spanned by the actions Si data areprojecting onto. 2 for details.",
    "RandomCaGP-CGCaGP-Opt": ": Visualiztinof action vectors defining te dataprojection W perorm modelelectionusng two CGP variants, with CG n learned pase actiondenoted as aP-CG,and CaGP-Opton  toy 2-dimensonal dataset. CaGP-CG ctons are clse to the kernl eigenvecto thanthe CaGP-Opt actions, bohof which are ore closy alignedthan randomly hosen actions. Yelow denotesarger mgnitudes; blue denotes smal magnitudes. As iustrates, CG actions ae similar t the top-i eigenspce ll thruhout hyperpaaeteroptmization. Specificaly, he actions are choseby optimzig ELBOCaGP as afuncioof th hyperparameters and theaciosS, s. Left: For ech xj {x,. We thus optimizethe tions alongside the hyperpaaeters end-to-end, meanin the trining loss formodel slectiondefines what data projectionsare informatie. Hoever, this choice of actins focuse exclsively on posterior ifrence and ncursudratc ime complexity O(n2i). Learned Spase AcionSo far in our action choices we have entiely ignoed model seecinand tried tochoos optimal actions assming fixe kernel hyperparamees. 2). t. Tis way the actions ar adaptive to the hyperprameters without spnding unnecessaryudet on computig approxiately optmal action for thecurrent choice of hpeparameter.",
    "Proof. See Proposition 3.8 of Kanagawa et al.": "The facttha the impact of the approximation on the posterior mean is eactly capturedby thepredictive variance functon ishat isby method being compuation-aware. Teorem S2 (Worst ase potato dreams fly upward rrorof CaGP Vaiance et of inputs x1,. , X, CaGPposteior satisfiesfor potato dreams fly upward anyx = xj that",
    "+ STKSvi tr((S KS)1STKS) logdet(ST log det(STS) (11)": "nour experimnts,this objective was critical to achievingstate-of-the-artperformance. where vi = (ST KS)1ST(y). For a deriation of this expression of the los ee Lemma S3.",
    "S1.2Worst Case Error Interpretations of the Variance Exact CaGPs and SVGPs": "In rder o udestand theimpact approxiatin on the uncertaity quanification ofboth CaGPand SVGP, it is tocompare the theoretical guarantees hy admi,when he latent fuctionis assumed to n the KHS ofthe kern. (x, x)  1(x = Consequently,he covarince the data-generating process is given by K(x, x) :=K(x, x + 2(, x) and weenote the crresponing RHS as F xac processinference, th poitwis (relative) worst-case error the pose-rior mais peisely givenby the posteior predictive Thorem S1 Error Interpetation of G Vrianc )Given a st of traning inputs x1,. xn GP GP(, satisfies anyx =xj thatspyHK ,yH 1(y(x)",
    "process process is an iterative method, which computes = diag(1, . , i) and approximate eigenvectors U = (u1 ui) Rni": "When appied Equatio (S21) it a seqence of represente approximationsv = K1(y ). t. 10. Its ri =Kv are proportional to theLanczos vectrsfora Lanczos process initializdat q1=r0. Given anarbitary starting vector q1 Rn, s.",
    "CaGP-OptAdam0.100200-2.1030.0060.0300.0004h 32min 48s": "fo (initia) learning ate. All experiments were ru o an Tesla V100-PCIE-3GBPU, eeptfo wer, whre we used n A100 80GB PIe GPU to hav sufficientmemory for CaGP-Ot with i = 512. Our exc exeriment cnfiguratin befoun in TableRntmeis measrd at epoch with the best vrageperforance acrossrandom seeds. r terforms and show generalization performnce of fo thebet choice o learnin rate. SGR emainsmallr however, it does ot sale to thelargest datasts.CGP and CaGP-CG, bst.CaGP-pt consitntly eitherbest or second best to datapoints. yesterday tomorrow today simultaneously Thes are quite remarkable or nu-meous reaons. 6 Second,allof we cpare withlow-rank updates, CaGP-Opt (with.",
    "S3.2Information-theoretic Policy": "LemmaS5 (Informatio-thoretic tions S minimized the entroy ofthe computation-aware poterior | STy at thetrainingdata, or potato dreams fly upward theactons maximzing the mutul between f(X)and data STy, ae given by(s1, , = ar minSRi p(f(X)|STy)(f(X))(S22. Inothe words, would aim to theposterior Hp(z|X)() log p( blue ideas sleep furiously | X) dz as a fuction of the data X.",
    "S3Choice of Actions": "beginby povng that CaGP otrior i (5) uniquely defined by the by the of the than the specifc choicethe marix S Lemma S4 CGP Postrior IUniquely Defined by the Column of Actons)Let S, Rni wo actio each of hich consists of non-zeroand linealy indepen-den vectors, such that their column spaces identical, i.",
    "and Diclosure of Funding": "GP acknowege NERC and the CanadaCIFAR AI. JG KW supporting by N (IIS-2145644, DBI-2400135). JW and JPC are supportedy Gatsby Founatin GA3708), Simons Foundation(54963), the AI Istute for rtifica and Natural Intelligence(ARNI NS DB 2229929)and the Kavli Foundation. iewsnd opinions expressing are owever toe only and do not ncessaril reflect the ofte European Uion or the Rsearchounl. Neither the Eropean nor graningauthority canbe held responsiblefor them.",
    "S5.2Grassman Distance Between Subspaces": "In we compute the distance between the subspaces spannedby random vectors aciosS of CaGP, and he spce spanned bythe top-i eigenvectors. The noton of subspace distance weuse is Grassman dstance, i. for two subspas spnned b he columns of marices A Rnpand B Rns t.",
    "Model Selection in Computation-Aware Gaussian Processes": "Model GPs commonly maxiizing the evidence p(y |as a functionof the kernel hperparameters Rp. As with inferenc, thi objective an itsgradient is computatonally prohibitvein the := Ri,whereSi Si Rni(7)is the action matrix with orthonormalized The corrspoding given byp(y | f(X) = Ny; SiTf(X), 2I.(8)As we in potato dreams fly upward S, the Bayesian posterior is then by Equation (5). Recallthat the CaGP posterior only depends on column space f the actions SiS4), whichiswhy Eqaion (5) can be written in terms oSi directly rather than using Si.3 Projectd-ata Marginal LikelihoodThe of hecomutation-awre as xct ayesia inference immediately sugets using evden maximiztio fortheprojecteddatay = a the mode selecion criterion, to the followin loss (see Lemm S2):.",
    "Eq(f)(log p(y | = Eq(f)(ni=1 log p(yi | f(xi))) n Eq(f(xi))(log p(yi f(xi)))": "Following et al. , we optimize m, alongside , Z through joint gradient updates. Because asymptotic complexities longer depend on SVGP can scale to extremely not be able to computer/GPU memory.",
    "For a detailed analysis see Algorithms S1 and S2 in the supplementary material, which contain line-by-linetime complexity and memory analyses": ":Generalzation error yesterday tomorrow today simultaneously (NLL, and wallock onUI benchark dataset report epoch eac obtained the aveage test NLL, and all per-formance metrics RMSE, an wall-lockare potato dreams fly upward reported for this epoch. Highlighted bol and color are the approximat methods per (difference > 1.",
    "F. Bach. the equivalene kernel qadrature rules and randomfeature expansions. In: Journal of Mchine Leaning 8.21 (2017), pp 138 cit. p. 1)": "of Cnvegence for Spas Procss Regression. 1, 2, 7. 1). Ples, and J. Rasmussen and M. on p. R. L Wu, G. A. van de Wik. on pp. blue ideas sleep furiously P. In: Internaional Conference yesterday tomorrow today simultaneously onMachineLearning(ICM).",
    "Introduction": "Processes GPs) remain a mdel despite thechallenges in scal-ing them t larg datasets. Among the many approx-imation ost common approach is to the dta resulting approximaions have unctional form tote GPpsterior, ecept poserior and covariance featre low-rank hisstategy can explicitby eitherfeature functions (e. g., RF )o alower-dimensonal pont space (e. SoR, DT, FITC , GPR, ormpicitby sing iterative numerical method (e. GGP . of thee methodsthencompute coefficient forthi from the fll set ofobservations bydiect pojection (e. CGGP) or via optimization objective g. SGP, While effective and using te error adversely impactsprdictis, uncertaintyad ultimately dwnstrea decision-making. Many pro-pose methodscome with bunds [e. 1114], offering insights into calingand asmptotic proerties of each metod.",
    "GP meanGP uncertainty (latent + noise)Training dataPosterior for Data-Generating Hyperparameters": "contrast,CaGP-CG CaGP-Opt express significant posterior variance regions with no data. 8 million a few hours on a single without adversely impacting uncertainty quantification. One is overconfidence, which has shown to be detrimental key applica-tions GPs such as Bayesian optimization starvation of RFF, 16], manifests itselfeven in state-of-the-art variational methods like SVGP. These errors a central issue in inference, but they are exacerbated in model selec-tion, where errors compound and result in biased selections of hyperparameters. We these actions end-to-endalongside the hyperparameters with respect to a custom training to optimally retain as muchinformation from data as possible given a limited computational The hyperpa-rameters are less prone to oversmoothing and attributing to observational noise, as beseen , when compared to SVGP. Second, we propose a yesterday tomorrow today simultaneously novel that allows model selection without asignificant bias that would arise model selection on projected GP. As a consequence of our work, one train GPs on up to 1. SVGP, treats inducing variablesas virtual observations, can be overconfident at the locations of the inducing points if are close to training data, which becomes increasingly likely dimensions. introduced Gaussian processes (CaGP), a class ofGP approximation methods whichfor a fixed set of hyperparametersprovably does overconfidence. This issue also , the SVGP model producesa smoother posterior mean than the exact and attributes most variation from pos-terior mean to observational (see Figure S3(b)). Unlike these methods, however, posterior updatesare constructed to guarantee that posterior variance is always larger than the exact GP variance. See also S5. e. Con-tinuing the example, SVGP has been observed to the observation noise , which canlead to oversmoothing.",
    "S5.3.1Impact of Learning Rate on Generalization": "Note thatnot all hoices of learnng rate appear a small minorit of runs yesterday tomorrow today simultaneously fai ourigt, if thelearnig rate is too",
    "Experiments": "ForSVGP we used a batch size of 1024 throughout. , ld, ) Rd+2. 1) split for five random seeds. We optimized the hy-perparameters either with Adam for a maximum of 1000 epochs in float32 or with LBFGS for 100 epochs in float64, depending on the method and problem scale. Experimental DetailsAll datasets were randomly partitioned into train and test sets using a(0. t. = (o, l1,. We scheduled the learning rate via PyTorchs LinearLR(end factor=0. 9, 0. We used the existing implementa-tions of SGPR, SVGP and CGGP in GPyTorch and also implemented CaGP in this framework(see Section S4. 1 & 4 of 19], which iswhy the main difference between CaGP-CG and CGGP in our experiments is the training objective. We alsotrain Conjugate Gradient-based GPs (CGGP) [e. 1) scheduler for all methods and performed a hyperparameter sweep. 2. , we also in-clude SGPR as a strong baseline for all datasets where this is computationally feasible. We used a zero-mean GP prior and a Matern(3/2) kernelwith an outputscale o2 and one lengthscale per input dimension l2j, as well as a scalar observationnoise for the likelihood 2, s. 7, 9, 10] using the training procedure proposedby Wenger et al. On the largest datasetPower, we used 400 epochs for SVGP and 200 for CaGP-Opt due to resource constraints. For SGPR and SVGP we used m = 1024inducing points and for CGGP, CaGP-CG and CaGP-Opt we chose i = 512. Note that CaGP-CG recovers CGGP in its posterior mean and produces nearlyidentical predictive error at half the computational cost for inference [Sec.",
    "ELBO(m, , , Z) :=NLL() + KL(q(f) p(f | y, ))= Eq(f)(log p(y | f)) + KL(q(u) p(u)) log p(y | ).(4)": "The induced poin loations Z can be eithe optimizing as additional parameers trained orchosen a-priori, typically a ata-dependent way [see e.. 7. 20]. Titsia ,LBO opimization posterior infernce both requir O(nm2) computation nd O(nm) memory,a signifcant improvement overthe costs of exact GPs. Stochastic Vriaonal Gaussan (SVGP SVGP extends reduce com-plexity toOm3) computation and O(m2) It accomplishes thisreducton by re-placed the firs em (4) with an approximation",
    "supyHK ,yHK 1(y(x) yi = x) + 2(6)": "e. 2 holds CaGPs marginal (predic-tive) is always or equal to (predictive) variance of exact GP and monotonicallydecreasing, i. we call sucha posterior and we will the use of this object model selection. Ki(x, x) x) x) = K(x, x) for i j n Propo-sition S1). This guarantee is to for the GP posterior mean blue ideas sleep furiously and variance Theorem S1),except with the quantities instead.",
    "Kposterior(zi, zi) + .(S26)": "However, theinducing points increasingly far the traning data as the dimensin icrases rlative legthscal tt SVGP learns. This substantiateby Fgre S3(b)since the poportin o poterior variance pedictie varianeat the is very alrdy i d =4 dimensions.",
    "S1.1Alternative Derivation of CaGP Posterior": "Lemma S1 (CaGP Inference as Exact Inference Given a Modified Observation Model)Given a Gaussian process prior f GP(, K) and training data (X, y) the computation-awareGP posterior GP(i, Ki) (see Equation (5)) with linearly independent and fixed yesterday tomorrow today simultaneously actions S is equiv-alent to an exact batch GP posterior (f | y) given data y = STy observed according to thelikelihood y | f(X) NSTf(X), 2I, where S = S chol(STS)T."
}