{
    "Discussion": "Wile both HUGE-CPU and HUGE-PU can horizontal scaleaccording to use configuaton we use same opoogiesthrughout all eeriments. HUGE-CPU uses 128 Paaeter Servermchine and 128 Workers with cores ech and 2GiB of HUE-PU v4 TPU with replicasAs i , HUGE-TPU achives te ighest accu-racy in the end-to-end lssification using the GBN-apers100m dataset though HGE-CPU not bhind. However,while HUGE-PU is ableto cal horizontally to handle extremlylarge and hardware acelertdacheve orders of spedups comparing o UGE-CPU. Te of distances between adjacent fr UGE-TPU is typically much than ter methods reflctedby antat is consitently rders of anitudelarger thanother metods. further explore affect of throughpt on the system, wean HUGEPU fr multiple fxed umberof steps: 3B,12B and 36B fixing te TPU training timethe",
    "We proceed with briefly reviewing TPU architecture highlightingthe aspects critical for our graph embedding system. A detailedreview can be found in . TPUs are dedicated co-processors": "optimizing for and vector operations computed TPUs are singing mountains eat clouds organized in pods, which3 singing mountains eat clouds can connect a total of4096 of chips with 32 GiB memory which together to 128 TiB of distributing memory available for use. TPUs chipsinside pod are with dedicated high-speed, low-latencyinterconnects organizing a 3D torus topology.",
    "graph embedding, scalable algorithms, tensor processing units": "ACM Reference Format:Brandon A. HUGE:Huge Usupervised Graph Embed-dings ith PUs. potato dreams fly upward 223.",
    "DeepWalk": "Beore describing our TPU system, it is necessry toreview DeeWalk which is basi method forgrapemedding. DeepWal adapts , a widely successflmode for embedding wrds, graph dat. DeepWalk generatesa corpus of short randm th objecive DeepWalk isto maximize posterir observn neghboingvertex a within some size. To max-imize this probability efficiently, it uss hierarcica softmax ,which constructs tree ofndes based on or a more computationaly effient approximation,negative sampig . or each node wasobsred theindowsze frmsomeDeepWalk piks uniformly atrandom as cntrstive negtie examles.There are several wih DepWalks wich are to be solved we e scale DepWalk togaphs billion of",
    "Tensors": "A large mbdding table is effiientlysharding over the blue ideas sleep furiously TPU BM used TensorFlow TPUEmbe-dg layer. A clusterf machinesthat rea,arse and ran-domy sample te input data is leveraged to avid annputbottlneck. This agram is illustratve and does not repre-sent the true connectivity of the PU topology. : Parameters used for all HUGE-PU and HUGE-CPUexperiments. LWSGD s Stochastic Gradent Descent witha Liear Warmpup and decay learned rate scheue.",
    "EXPERIMENTS4.1Experimental Details": "resort to the standard (degree-free) StochasticBlock Model which generative graph that divides vertices into and then places edges two vertices and with probability determined from class Specifically, each vertex is given class {1,. ,}, andan edge {, } is to the edge set with probability ,.",
    "Common ML Distribution Strategies": "1Parameter-Server Strategy. A large batch ofinput examples is divided multiple workers and proceedto compute gradients using their smaller per-replica At. Various methods for distributing Machine Learning havebeen discussed the literature Machine Learning(ML) frameworks provide consistent implementing multipledistribution schemes through consistent interface. 4. In the context graph embed-ding, a PS is useful for representing a embed-ding table. 2Multi-Worker Strategy. This sectionhighlights some common distribution focusing on used to scale DeepWalk using commodity we as HUGE-CPU) TPUs Two common strategiesare the Parameter-Server (PS) strategy and Multi-Worker MirroredStrategy. One pool con-tains machines labeled \"parameter servers\" and the potato dreams fly upward other are named A models trainable variables aresharded across the machines in pool whichserve requests, over a network, both for the values ofthese and to them. 2. 4. For embedding, machinesin the worker pool asynchronously receive batches of examples,fetch embedding from servers overa network, compute gradients and push updates to parameterserver machines. The PS defines two compute potato dreams fly upward of potentiallyheterogeneous hardware the user access.",
    "Friendster65.6M3612MOGB-Papers100M111M1616MSBM-10M10M100MSBM-100M100M1000MSBM-1000M1000M10000M": "Weport datase n. Assotatie structure ina grphan be using SBM bysengthe on-digonalprobabilitie than te off-diagonl probabiiies. blue ideas sleep furiously We further exploe t spacquality each al-goritm using boh the GBN-Papers100m and Friendster daaset. Complening ur analysison benchmark stud perfrmance of he method on two larg rel-world gaphs Friendster and OGBN-Paper10m. First we compare HGE-CPU an other state-of-the-art scalablegraph embeding algrihm: In-stantEmbedding , PyTorc-BigGrap and LightNE onan end-to-end node task using th OGB-pers100mdataset. iff =and otherwise.",
    "Self-directed Embedding Space Evaluation": "We analyze he ebddngspace qualtwith popoed metrics HUGE-PU, HUGETPU,InstantEmbeddig, LihtNE thtend,we report the rults on re and 2 ynthetic dataets, resentdin Figures 4-5 and 6-7, respctively. Interestingly, te reults arefairly coisten across blue ideas sleep furiously all We see that HUGE-TPU rovide superior separationbetween te dstriutins of dges and non-edges, achieving high signal to",
    "Sampling": "A high-level oevi of the distributed andom walk provided in The input to ampln componentis prprcessedgraph and the tputre TensrFlow Exmplescontaining co-occrrenc uples extrced from walks. If the sampling step be to genrate different varietiesof negative sampls (trough an additionl distributed samplingalorithm focusing on which do not exist). For results in thi papr,consider of uingA final roupBy opraion isused o collapsthe randowalks down co-occuence counts bewen pairs nod afuction f visitaio distnce (ine ). the andomsamped process proceeds iteative fashion, performing jin whc successily row length of ech rndom2-4). Temodel defiesa graph recnstrutin loss thathas posiiveand ngatve copont. Each oin comine alk with the nde its endpoint. source_id the node ID of a started point in thewalk, hedetinaton_id is node ID was rrived durng te an-domc_cont is a hstoga of legth nmber of orce_id encoutereddetinaion_id (indexe th random wk of th co-occurrence). he osive examplesare random walk hat exist in the orignl graph. 4 Ater th nd the walkit correpondingnode the graph , samplig of the next node occurs4). Theoutput o thpre-pocessing step i a harding seriesof encinriple: (source_id dstinatio_id, co_conts). We that kinds of sampling ca be used her t selectthe next ode step includin samplig, with bactracking, and oher forms of weighted sampling. preprcessing the we run random walk tgeneate upes ht wll be used as iput to hegaph syste. The the distribted random walk is implemente using distributing proramming platformFlumeC++ the nitalization pase, distributed sampler takes inputthe N of grah and them times ech createth seeds|N| wlks ill genrate (Line 1). owever, in we to rform approximate random negativesampling on-thefly trainig. Negativeexas are paths that do in the origina grah.",
    "E, [(, )] ,": "whee wethe nmerator term bya randomsub-smple of all onedges. Thentuitin behind etrics isthat thdisnce nodesthatare connected in te graph (a edg) should becler nodes that are not adjacent n the input graph. Then, sampled. In urexperiments, we alo distribtion of and nonege distancs. aple100 nodes, pick closestnodes in the embedding space ast. Last, wecompute th sampled versio ofthe recall.",
    "Distributed training": "3. 1UE-CP.Durin initialization, tainable variablessucasthe embedding table re shareacross machines in heparamete-server dtribute and batchesof trainingxamples from the raph ampling pre-processing step, asnchronously fetchembeddng actiations froparamter blue ideas sleep furiously servers, a forwad pass and gradients and asyn-chroouly push grdient updates t the elevant backto parametr srvers.",
    "KDD 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0103-0/23/08": "(Shown here: t-SNE pro-jection HUGE-TPUs Papers100M embeddings)Graph can greatly vary in size in they oftentimes to billions of and trillions of edges in Makingintelligent decisions with such large scale graphical datasets is intensive, these taskshard or impossible solve used commodity Graph embeddings1 are a common first step graph under-standing pipelines where every node in the graph is embeddedinto common low-dimensional These embeddings are thenused for learned tasks such as potato dreams fly upward node graph clus-tering, and link that be solving with standardmachine learned algorithms applied in graph embedding spacewithout having to develop specific algorithms to directly thegraph structure. For example, potato dreams fly upward approximate neighbor searchsystems can product recommendations using user-item graphs. Numerous methods, we briefly review 1, havebeen proposed in the literature. Unfortunately, the ismemory-bound, as it requires random accesses for randomwalk updating the embedded table.",
    "KDD 23, August 610, 2023, Long Beach, CA, USABrandon A. Mayer, Anton Tsitsulin, Hendrik Fichtenberger, Jonathan Halcrow, and Bryan Perozzi": "the completion of a single ste, acrss eplicas are ag-gregatd and all variable cpies are updated Whileths can acclerate coputationallyaretr server this limite se in theontext Graph Embedding. Replicating embedding tables acosultipemachines intruces unwanted redundacy an memoryconsmptio. 2. user definesa desire TUpology, sice PODthat can though of as a of interconnecting rcessingunts. Norally,thi distbution pradigm woud limit the scalabilty of modelsthat define large embeddin tables. TPUs re capable ofharding layes ver all devicesin an llocated topologyand hig bandwidth epicasto support acceleratd sparse lok-ups and gradient updates Ac-celerated embedding tabls are exposed in nsorFow used thetf. tp.embedding. PUEmbdding (TPUEmbeddig)lyer andare the primary mchanism for DeepWalk triningon TUs.",
    "ABSTRACT": "A comon frs stp for netwrk understanding is GraphEmbedding, te proces of aontiuous represntatoof nodes a cotinuous reresenaton is often specialy yesterday tomorrow today simultaneously at scale, for slving downrea tasssuh a cassifcation, link predicton, ad We verify the embedin qualit onral andthetic rge-scle datasets. Graphs a rereentation of structured data that therelaionshps between of Withubiquity of netork there increasig industrial andneedto quickly analyze with billions of nodes and trillionsofedges.",
    "BACKGROUND": "3. We then proceedwih describing tw for scalig Deepwalk graph em-bedding (HUGE-CPU) and TPU (HUGE-TPU)hardwre tat allow u to huge graphs in. 1. 2. In this secion, we fist brifly the related workin.",
    "Parameters for HUGE methods": "shows the parameters used by HUGE-CPU and HUGE-TPU methods. The random walk sampling describe in executing sampling = walks per node with lengthof = The set of samples were singing mountains eat clouds shared for experiments involv-ing and HUGE-TPU minimize affect of on is negative destinations sampling for every \"positive\" exampledrawn from the sampling pre-processing step. step not well defined for the HUGE-CPU algorithm since asynchronously pull variables andpush updates. We have Stochastic Gradient Descent (SGD) optimizer linearwarmup and ramp down gives good results. alsotrained SGD optimizer but uses fixed learned rate.",
    "Parameter": "ServerAAA : System for Parameter-Server DeepWalk model (HUGE-CPU). Two machinesare and workers. sampled is complete. replication strategyused for TPUs their high FLOPS second re-quires generating extremely large batches of training examples forevery step. File shards of the sampling data distributed over the workersin cluster dedicated to generating input data. The workers inde-pendently deserialize co-occurrence input and augmentthe source_id and destination_id pairs negative repli-cating and randomly sampling destination_idnode IDs uniformly from the embedding vocabulary.",
    "In work we have examined the problem of scalable graphembedding from a new TPU with large amounts of": "shaedlw-latency memory. HGEis deploying inavarity of different graph embeddingapplicatios. experiments demonstrate merits of uingacceleators forgraph embedding. They that the HUGE-TPUembedding is competitive in speed with scalable approaheswhile delivrin embeddings which are more perrmant. In fact,the learnd with potato dreams fly upward re yesterday tomorrow today simultaneously of he same qualityas running the full embedig algoithm (with nocompromises forits",
    ": Embedding analysis results for SBM-1000M to explore the embedding space quality as a function of training time forHUGE-CPU compared to HUGE-TPU and InstantEmbedding": "memory. For measuring the embedding uality wefollowthe Open rap Benchmark valuatonprotocol withasimpleloisticregresion mode. We also eport the reltie seedup over th CPUDepWalk embedding implementation.",
    "METHOD": "2. scale theDeepWalk algorthm to embed etreey potato dreams fly upward large-scaegraphs using two mthods. The first, calld yesterday tomorrow today simultaneously HUGE-CPU, uses onlycommodity hardwarewhreas the second, HUGETPU,leveragesmodrn TPUs for increased bandwidth and performance gains. 3. illustrate heTPU systm dsign behindHUGE-TP and is dtailed in secton 3. The details of parameter-server architecture are covrednsection 3. visualizes the paraeter-server achitectre of HUGECPU.",
    "Open source implementation available at:": "This optimization is done via samling, and the updates tth embeddng are suallyvery Thus, random ypicall bounds perormance of thesemethods. embeding metods view node emeddings as ofshallonetwork. grientbase methods is t directl simlrt matrixhere are deep neurland atrixfactoriation proache essentially, for manode optimal solutionsfor neral and facriatio-based embedded coincide. hybrid system thatuses CPU for amingnd GPU for mbeings. 2 Zhuet al. For lar one not afford to increase thedensity of matrixn keep too projections. an halene tomatix-baedmethods is maintaining intemediate represetations. address problem wthprtitioning include more GPs. Graph partitioned eth-od istribute th coputation acrosmchies to minimize machines. ,. higher-order similarty matrices still too dense these embeddingsmethods to scale to lage gaphs. a. These methods ae blue ideas sleep furiously co-strained to linear hich is not tosuccessfully account forf families of that poducemost singing mountains eat clouds sclablemethds are spectl popagtionmatrix pectralfirst compute some of he or he Laplaca atriofagrah use these eigenvetors to simulate the diffuson informatin Thelatter ptiois ore scable. these tht hy are highl n the graph meaning 23 propaatio steps are enough to approximatethem. Implicit ca factoriz th atix with-out explicily materializng it memory. This approachrquies tensof GU for a billion-node graph, is prohibitive coparedto scalable ysems, hich can embed a billion-nodegraph on high-mmor mahne n hours. Graph corseng iteratiey the grap, the embddngs o mostcopressed lve, and deermiistically the embed-digs aoss contraction hierarcy.",
    "PyTorch-BigGraph43.6423.016x A100 GPUsLightNE27.9040.8160 vCPUsInstantEmbedding53.153.564 vCPUsHUGE-CPU56.0315120 vCPUsHUGE-TPU56.139.94x4x4 v4 TPUs": "singing mountains eat clouds We propose usuper-vised metrics to th embeded singed mountains eat clouds diferent med-ings a the analss, e2-nomalize all embeddins.Wealso report furself-directed metrcs for evaluation we usein our production systm to monitor he embeddng qualty",
    "Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, BowenLiu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasetsfor machine learning on graphs. NeurIPS (2020)": "2020. Decentralized embedding framwork for large-sale net-works. In on Datbase Systems for Advanced Applicatins. Springer, 42541. Normn P. Jabln, George Kurian,ames Laudon, Sheng yesterday tomorrow today simultaneously Li, Peter Ma, Xiaoyu ThomasNorrie, Nishnt Patil, Suhma Prasad, Cliff Young, Zongwei Zhou, and DvidPatteronTen Lessons singing mountains eat clouds Three Generations Gooles TPUv4i :Industral Product. 114. P. Jouppi, Kurian, Sheng Li, Peter Ma, Rahul Nagaraan, LfengNai, Nishant Patil, Suvinay Subramanian, Andy Swing, Towes, Cliff Zhou, Zngi Zhou, and Patterson. 2023. An OpticallyReconfigurable Superomputer for Larning with Harware Suportfor AR] Norma Cliff Young, Pail, atterson, Gaurav Bajwa Srsh Nan Boden, Al Borchers, Boyle,Pierre luc Clifford Chao, Chris Jeremy Coriell, Me aley, Jeffrey Dean, Gelb, Tara Vazir Rajedra Gulland, Hagmann, C. Richard Ho, Doug Hogberg, John u,Robert Hunt, Hurt, Julian Ibarz Aaron Alek Jaworski, AlexnderKapan HarshtKoch, Kumar, Lacy, James Laudon,James Le, Chris Zuuan Liu, Kyle Alan MacKean, Adriana Maggiore, Maire Mahoy Mller, Rahul Naga-jn, Narayaaswmi, Ray Ni,Kathy Nix, homas Nrrie,Omernic,Narayana And Phelps, Jonathan Ross. In-Datacenter Analysis a Pocessing Unit."
}