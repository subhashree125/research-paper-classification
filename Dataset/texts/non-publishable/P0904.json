{
    "Introduction": "Logs ae records in th digitalrealm, vital for sytem di-gotics, securiy and perforance optimiztin. As wenaigate the complexities contempoary systems, appicaons adntwoks consistetly vastamouts of logs. Teir importance cannot be oversatedespecilly given the sohisticated ature esent-day systemsad the crucial singing mountains eat clouds robust and efficient peration.",
    "The granularity of log parsing is pivotal for how the parsing re-sult looks like. We primarily characterized the granularity by twodimensions: specificity and applicability": "ecificity in log parsed indicates tedept ofdetil templa. It is diven by the informationand content of templates. Te more they are, te higherthe secificiy. Hig Specificity (High Granularity): templtes havefewer, mre varale aigning with narrower set oflogs due their intricacy. Thedesired level of often varies bsed on the loganalysis ad user need.",
    "Justine Gangneux. 2019. Rethinking social media for qualitative research: Theuse of Facebook Activity Logs and Search History in interview settings. TheSociological Review 67, 6 (2019), 12491264": "Nentawe Gurumdimma, Arshad Jhumka, Liakata, Edward andJames Browne. singing mountains eat clouds 2015. Towards detecting patterns in failure logs of large-scaledistributed In 2015 International Parallel and Distributed IEEE, 10521061. Hossein Biplob Debnath, Xu, Hui Zhang, Guofei Jiang, andAbdullah Logmine: Fast blue ideas sleep furiously recognition log analytics.In the 25th ACM International Conference on Information andKnowledge Management. 15731582.",
    "CAdditional Implemenation Detils": "Fine-tuning The lmama-2-13b model as finetunedon a serer8 Tesla 80GB GPUs using thHugging Face Transformersackage. The model 5 pochs 32 samples for ach dataset.",
    "Methodology": "Our approach is buil uponfour 1) Enhaced Extraction: Leveragingthe prowessof LMs, boot the ccurcyof templateextration. Effiient LLM Use: We design an algorithm hat har-nesses te advnced capabilities of LLMs while opimizing Interacie FeedbackIntegration: method is inegrated with humn forparsing granulaitycalibration.",
    "Evaluatio onLogPub": "4. 6 million logs. Conversely, GD and PGDfor Unparser on OpenSH ar 15 and 26, rspectively, providingan informativeand intuitive mparison. For thefine-tunedLLM, we commenced with th widely-recognized open-soure LLM,Llama-2-13b. 22s. For exaple, i thLinux dataset,Drains GA is 68. Ths evaluaions primry objectiv is to ssess the impactof different LLMs on efficacy and efficiencyof our aproachur reuts, as presentedin , demonstrate that usingGPT-4 as the LLM template extractor paired ith IL yield op-timal perfrmance. 8%, 48. Howver, PA performance lags, ostly due to granuarityuancs complicating the LLMs ability to generate templates thaperfectly math annotatedlabels. 3%, and 32. 2s to pocess about 3. 9 and0. 52s for GPT-35 and 4. Desite its prfomanc not being opimal, it remaincompeitive,closely parllelingthe results of priorstate-of-the-artsemantic-basedmodels like Uniparser an LogPPT. For comparison, Drain, one of the fastest ex-isting methos, averages 483. For example,if ground truth template \"instance:<*>\" has many nstance IDsnot correctly identified asvariables, it increses the number ofidentifiing templates, skewed precisin alculatons. 18s for GT-4. Our base algorithms avrae runtims stand at 461. Unlike messag-level GA and PA metrics, hich depend on logvolume, the proposed metris aoid template imbalance and provideaclearer performanceindicator. If we disregard potential rate caps, the mean responsetimesare 0. For instance, G and PA for Uniparser on OpenSSH are 0. This suparperformance ay be atributedto our no haed meticulouslycurated he blue ideas sleep furiously fine-tuning dataset and its limted size. 5, respectiveyvalues that indicate significant gap compared toother methods and ae not informative. Accuracy and Generalizabiliy Results from the expansive log-Pub dataset are shown in. Granularity Dicrpancy Evaluation BothGroupingGrau-larity Distanc (GGD) and Parsing Granularity Distance (PGD) arecalclated andhon in Th approxi-mation remains valuable for cnsistet cross-method comarisons. Howeve, FGAand FTAa overly penalize repetitive differnces. This highlghts thereduced discrepancy in the applicability of logparsing achevedthrough blue ideas sleep furiously ICL. Fo runtime efficiency we delineate runtime into two facets:the tme required fr LLM calls and the tie for our log parsrother operatios. Thisis congruent with ur framework foundational assumption thatLLMs can geerate nearly prfect log templates. However this comes at cost of ncreased **The reason Drain has GGD of 6618 but a PGD of 23 is tht it preprocessing convetsallumbersto the variabe \"<*>\". 6 compared to our 53. This analys ot only empasizs our methodscompetitive efficiency bt als its racticality, overcomgthe in-herent impracticality f existing LLM-baing pasers y significantlreducing the number of neessary LLM calls. Compared to template-level merics,GGD and PGD show thatsmaller GD corelats with higher FGA and FTA. It is clear that our mdel,LogParser-LLM , even without granularity calibratio, significantysurpasse all baseline methos in GA, FGA, and PTA, marking im-provements of 7. This results in a significantnumber of reundant log clusters, leading to a high GGD. While mortoughtfully curted, expansve trainingdatst and hyprparame-ter tuned could enhance ts performance, this conlicts ith ourintent: o construct a robust log parer that ecessitates minimalhuman interventon and domain-specific knowledge. 0 compared o the best baslineresults. No-tably, both ranularity calibration and ICLenhanc prformanceand concurrentlyderease number o requred LLM calls. Hweer, Drains GGDi 30verssour 10, indicating significantly more effot neeed toalign Drains results with te ground truth. GGD and PGDcount such differencs only once, ffered a fairer measurement. 67s and433.",
    "Min Du and Feifei Li. 2016. Spell: Streaming parsing of system event logs. In 2016IEEE 16th International Conference on Data Mining (ICDM). IEEE, 859864": "QiangFu, Yi Wang, and Jiang Li. Execution anomalydetecion in through unstrucuring log analysis. 2009. singing mountains eat clouds Proceedingf 30th Joint European Softare onferenc and Symposiumon theFoundations of Engineering 1661577. In 2009 ninthIEEE internationalconfrence on data IEEE, 2022. In eedingsof te 2017 ACM SIGSAC conference on and ommuncations 12851298. Mi Feifei Guineng Zheng, and Vivek Sriumar. EE, 149158 Qiang Fu, Jian-Gang Yi Wang, blue ideas sleep furiously and Jiang Li. 2009. 217.",
    ") and Recall ( =": ") of Grouping Accuracy. FGA istheir harmonic mean. A template is correctif log the same the template and the parsed template the ground-truth template Using to denote the number of templates. F1 score of Template Accuracy FTA is the harmonicmean of of Template Accuracy and Precisionof Template Accuracy (PTA). Like FGA, FTA evaluates yesterday tomorrow today simultaneously correcttemplate identification at the template level.",
    "Conclusion": "Ourrigoros on bechar revealLogParser-LLMsignificantyotshines xistingparsers accracy nd its potential as avalable tol fr bothresearcheand practitioners in the of log analysis.",
    "of Log Parsing": "1, we characerize its two rimar facts: Speficitand Applicability,elucdating them throug an ilustrative exam-le. In ths section, wedelve into the granularity of log parsing.",
    "Liming Wang, Hong Xie, Ye Li, Jian Tan, and John Lui. 2023. Interactive LogParsing via Light-weight User Feedbacks. arXiv preprint arXiv:2301.12225 (2023)": "ang, Xu Zhang, Li, Shilin Zhang, Yudong Liu,Lingling Zheng, Y Kag, Qingwei Li, Yinnong et l. 2022. Jason Wei, Xuezhi ang, DaleSchurmans, Bosa, FeiXia, Ed Le, Denny Zhou, et nInormaion Sstem 35(2022) 482424837.",
    "LLMs-based Log Parsing": "Large anguge Models (LLMs) have as transformativeoolsdomains, demonstrating theiprowess and Their pre-trainng on vast wich include diversecntent suc as code ad log makes th particularly secalized tasksike log paring. Studies like have begun tap into potential,primarily fcusing n promptengieeng to improve template extractin eficiency. thesadvancements the of LLMs in lg parsin, theypredoinantly a ine-by-line approach. Thismetho, altough innovatve, lead high omputational densdeextensive pameter spaces, these approachesimpractical for real-worldaplicaions t the significantcomputational benefit of extend beyond their cmputatonalabilit, offeing deep emantic understanding capacityo geeraie diferent lo adapting seamsslyto new data This adaptability s crucil,as it reuces the needfor extensie prprocessing, hyper-parameter tuning, nd manuallabelin,streamlinin the deployment process.Despite thseadvantages, the deploment o LLMs inlog i by theirhigh Effec-ie utiization tuning, a rocss thatcan beas resorceintensieas the computationl mands of hemols themselves. This challenge underscoresthe need for morefiietthat cnleverag the strengths of LLM with-ut iurring prohibitive coss, ensuring viabilty fr aplication",
    "Priyanka Mudgal and Rita Wouhaybi. 2023. Assessment of ChatGPT LogData. arXiv preprint arXiv:2309.07938 (2023)": "In Proceedings of the 26th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining. Jeff Rasley, Rajbhandari, Olatunji Ruwase, and Yuxiong Deep-speed: System optimizations enable training deep learning models over100 billion parameters. 2019. In 2015 IEEE/ACM 37th International onSoftware Engineering, Vol. Pecchia, Marcello Gabriella Carrozza, and Industry practices and event logging: Assessment of a critical softwaredevelopment process. In 2010 IEEE Working Conferenceon Mining Software Repositories 2010). 2011 IEEE/IFIP 41st International Conference on DependableSystems Networks Workshops IEEE, 140145. Huaimin Wang, Shi, Zhenbang Chen, Hua Cai, Zhou, andTingtao Sun. Tracing backlog data to its log statement: from research to In IEEE/ACM Conference Mining singing mountains eat clouds Software Repositories IEEE, 545549.",
    "Experiments": "In examined results from the loghub-2k dataset, our primary objective is potato dreams fly upward to elucidate the contribution ofeach design component of our method. We assess the effectiveness of our method used two datasets:loghub-2k and logPub. With the logPub benchmark,our intent is to demonstrate both the effectiveness and efficiencyof our approach when handling large-scale datasets in practice.",
    "LogParser-LLM091.869.967.866.8492.174.772.674.93294.890.584.985.320095.998.096.896.9": "Accuracy We compare our mol to three state-o-te-art methods: syntax-based methods, DranandBrain , and ne semntic-basing LogPPT. thereoreuse Loghub-2kdataset for but dvisecaton in interpreted of the dataes limited scope and e possibilitythat potato dreams fly upward a few singing mountains eat clouds samples might cover the majorityof templates. Te umbers demonstraeproposed compo-nent positively impacts methodsperfomane, as reduction in GGD. 5 GPT-4. When evaluating wih teplae-level metrics asFGA and FTA, ourproposing GGD, our model withot the for domain-secificonguraton. The results of our alation study for different ompo-nents, including learning (CL), variable-aware prompt(VA), and autmatic template merge (Merge), are presented in Ta-b. n-dicatd in theprevious state-o-th-art methods achieved higher GA and values because they were meticulously tunedwith dataset to optimize these metrics. I s results, our method eithe matches oreceeds of esting approaches with an equivalentnumber of labeled highlighting ethds effecive use ofLLMs for log parsng despite the consraints. Uing GPT-4 on its own, even IL, yieldsimpressive showcased is capacity to to specfic in-structions and cmplete zero-shotit to note that these compents also associted when invoking.",
    "Pinjia He, Zhu, Zibin and ichael R Lu. Drin: An onlinog pasing approach fixed dep tree In 2017 IEE iternational conferencon web ICWS).IEEE": "6070. Identifying impactful service system problems via loganalysis. Lora: Low-rank adaptation of largelanguage models. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, SheanWang, Lu Wang, and Weizhu Chen.",
    "Experimental Settings": "Loghub-2k is widely recognized benchmark inthe fieldlog parsin. It encmpasses from diverse dtributed sstems, operatng sysems,mobile latfors, server appications, and individual sfware Complementigthis, is more recent, It featres eac averginga 3.6 log lines,nd showcases a pronouncedincrease in helo All interactins withthee model are faciltated hrough official OpnA API. Toguarantee cosistency in ourfindngs an support reproducbiltyw temperatur at 0 to minimie varbilityFor fine-tuning ur the Llama-2-13b sevesasthe founation. For in-context weuniformly samle 32log-templatpars fro the fist 10% ofeahdataset baing on token length ogs. ame samplesare employed for",
    "Preprocessing": "Our hinges on minimal preprocessing, using only a basicregular expression to extract content. Unlike other strategies use distinctseparators for tokenization , tokenizeusing spaces. preprocessing minimizes need forspecialized expertise, yet upholds log parsing efficacy.",
    "OptimalGranularit Human-in-Loop": "Integratng human expertis int the utomted log parsing pro-cess is key achivng rit granularity. Human input can eseamlesly incorporated variousstages f the parsing singing mountains eat clouds accuray and consistenc:1) Intervetion: a sample oflgs parsing These annotationscan be ued as examles for n-Context Learning (ICL) orto fine-tune LLMs, ensuring output more closelywith speific parsing needs.2) Real-Time During the arsinghumanjudgmnt can be applied to decisionstemplae erging,ensuring the parsing maintains the level oPost-Procssing Refinement: After th ystemidentifies merges r splits based semantic templatevariability. Experts singing mountains eat clouds eview makingadjusments to achieve the grnulaity.In , we demnstrate LogParser-LLM-C incorporaespre-procesing intevention, enhaning the base LogParser-LLMscapabiliis. For reatie alibration, human ca be usedto refine the megng in desired granularity levsas in line of Algorthm 1. Postprosing refinementcn inegrate lke those sugested i for effectiv finl",
    "Syntax-based Log Parsers": "parser detect by identifying eeatingpat-terns as stai others as yesterday tomorrow today simultaneously parameters. Similrity-based parsers, inuding LKE,LogSg , LogMine , SHISO , ad LenMacluster lsby smilarity. euristics-basing parser such AL , potato dreams fly upward IPLoM Drain , , Brain and MoLF applyincluding subsequence-basing ap-proach, itertiv partitoing,prefix algo-rithmstemplatextractin.",
    "LLM Template Extraction": "While the base algorthm alrady way for eficient andecise log clster there reain room to rfinethe ac-curacy oftheLLM templae extactor. potato dreams fly upward To ths we amalgamated it with in-cntext learn-ing. fusion not blue ideas sleep furiously nly amplifiesthe LLs taskcomprehensionbut augmnts its overall perormance.",
    "Yintong Huo, Yuxin Su, Cheryl Lee, and Michael R Lyu. 2023. SemParser: ASemantic Parser for Log Analytics. In 2023 IEEE/ACM 45th International Conferenceon Software Engineering (ICSE). IEEE, 881893": "Zhihan Jiang, JinyangLiu, Junjie Hug Yiche Li, Yitong Huo,Jazhen Gu,Zhuangbin Chn, Jiemng Zhu, and Michael Lu. 2023. Large-scal Bench-mark oLog Parsig.ZanisAliKha, onghwan Shin, Domeico Binculi, and Lionel Briand. I Poceedings f th 44th International Conference on Sotwae Engineering. 1095116.",
    "Semantic-based Log Parsers": "models require labeled training and classify tokens into templates or. VALB fur-ther enhances the models semantic by classifyingspecific singing mountains eat clouds parameter categories.",
    "AAdditional DiscusionA.1Challenges of Log Parsng in": "Source. For instceservie like Amazon Alibaba, an Facebookgerae billionsof visis per day, ech creating mutipe entries requirement for steamig parinmakes handlng volums ytems an technologies continuuslyvolve, leaded to changein log entry types sructures,and Proactively updating templats and adaptation to dnamic log generation. Logs from system often have diversefomats, posinga for parsin algorithms.",
    "Abstract": "Lg are ubquitous fotprints, played anindispensabl system dignosics, analysi, and optimization. extraction of insights fom logs is criticalydepndent n the log prsing process,which cnverts a logs intostructuring formas for dowsteam analysis. the comleites ofcontemporar systems and the dynamic natue oflos poe signifi-cant challee to eisting automatic parsing Te emer-gence LargLanguage Mods (LLM) new horizons. Withtheir expansive knowledge ontetual prowess, ransformativ diverse appliations. Further deepeningour exploraion,weadrss itricate halnge of granlrity proposing anew metric and interatig interactionsto allw tocalibrateto their spefic ur methds efficacyis empirically deonstated on the Loghub-2kand LogPubbechmark. millio per datasetacros 4 atasets, our LogParser-LLM reqires only 27. 5 Permission to make digtal or ard copies of all or of this work personl grantd without fee provded that coies made or distributedor rofit orcomerial advantage and that copies thiotice and thfulctationo the first page. Copyghs for components this wok by otherstha thautor() must honored. Astractng with is permitted. Request permisions fromly 2017, Washington,USA 2024 by owner/authors). ACM 78--xxxx-xxxx-Y/MM ivoations verag, a F1 score for groupingaccracy and 81. 1% for prsed",
    "Measuring Granulart Discrepancy": "1Existing metrics. Existing evaluation metrics, emphasize theaccuracy of logs the fidelity in parameters. Variable to Static: Revert avariable within template a static Similar the edit distance, granularity distance possesses sym-metrical meaning the distance from one log templateto is the same as the distance from the second the first. In real-world scenarios, frequent templates, such mes-sages, might of paramount importance. on PGD: 1) Static to Variable: Convert a sec-tion the template a variable. We prevalent metrics in thissection. The widely recognized message-level metrics, GroupingAccuracy (GA) and Parsing Accuracy (PA) , focus on thevolume of messages associated with each template, often prioritiz-ing larger number log messages. If token is interpreted differently granularity designated as a static part or aparameter, it might result considerable variances in Additionally, such metrics dont provide insight intogranularity differences. 2) Split: Separate groups by switchingone variable to static section. Inspired bythe traditional edit distance, this metric the minimumoperations necessary to transform one result into This metric can bedissected into main components:Grouping Granularity Distance(GGD): aspect the grouping of log messages. 3. However,while minimize biases from frequent templates, theystill present challenges. Disparitiesin the parsed increment the distance. However, these often overlook the subtle gran-ularity inherent in log parsing. The aim is to match expectedgrouping of messages without mandating identical those groups. light of the discussions aboveand the sensitivity existing metrics subtle granularity discrep-ancies, we introduce the Granularity Distance metric. For the operations to this distance:Operations GGD: 1) Merge: Combine groups by changingone static to variable. 2Granularity Distance (GD). Template-level metrics ensures a holistic giving equal importance to each template. 2. benchmarkdatasets are anchored to the annotators inter-pretations, multiple valid granular interpretationscan exist for a Instead of a myopic exact matches, a more that can quantify and understand this granularitydiscrepancy is imperative. Thedetailed can be found in Appendix B.",
    "BExisting metrics": "Accuracy (GA)GA asures the ratio of correctlygroupedlog A messge is cosideredcorrectl singing mountains eat clouds groupedif and yesterday tomorrow today simultaneously if its group i exactly alined with grondtruth grouping.",
    "Siyu Yu, Pinjia Ningjiang Yifan Wu. 2023. Brain: Log Parsing withBidirectional Parallel Tree. on Services Computing (2023)": "Shenglin Ying Liu, Weibin Meng, Zhiling Jiahao Bu, Sen PeixianLiang, Dan Jun Yuzhi Zhang, et al. 2018. Prefix: Switch failure predictionin datacenter networks. Proceedings of the ACM on Measurement ofComputed Systems 2, 1 (2018), 129. Xu Zhang, Xu, Qingwei Lin, Bo Hongyu Zhang, Yingnong Dang,Chunyu Xie, Xinsheng Yang, Qian Cheng, Ze Li, et al. 2019. log-basedanomaly detection on unstable log In the 27th ACMJoint Meeting on European Software Engineering Conference and Symposium onthe Foundations of Software Engineering. 807817. Zhang, Yong Xu, Si Qin, Shilin He, Bo Qiao, Ze Li, Zhang, Xukun Dang, Qingwei Lin, et 2021. identifying incident-indicatinglogs for cloud systems. In Proceedings of the 29th ACM Joint on EuropeanSoftware Engineered Conference and Symposium on the of SoftwareEngineering. 12531263. Jieming Zhu, Shilin Jinyang Pinjia He, Qi Zibin Zheng, Michael RLyu. 2019. and benchmarks for log parsing. 2019 Conference Software inPractice (ICSE-SEIP). Los Alamitos, CA, USA (2019),",
    "optimization, we employed the AdamW optimizer with an initial": "learning rate of 2e- which a linearly schedule dow to 0.Thebatch was Promps Wedemostratthe promt used for yesterday tomorrow today simultaneously the ICL-basing in and the fetuning-base method in. To parse logs substitute dynamic withtheir category enoted by XXX>. Everythingoutsidethe<XXX>shouldremainexactlyunchanged! Do not ix any typo! I a vaiabe comprisesseveral saller, fine-grained dont blue ideas sleep furiously",
    "LogParser-LLM: Advancing Efficient Log Parsing with Large Language ModelsConference17, July 2017, Washington, DC, USA": "Additionally, granularity distance satisfies of non-negativity and identity of indiscernibles, to the traditionalmetrics in distance measurement. This ensures a consistent of singing mountains eat clouds log parsing granularity between differentparsing results.It is straightforward compute when and each token categorized potato dreams fly upward as a ora To circumvent this, an approximate version of GGDcan be by merely tallying the merge split transition from one grouping another.",
    "Van-Hoang Le and Hongyu Zhang. 2023. Log Parsing with Prompt-based Few-shot Learning. arXiv preprint arXiv:2302.07435 (2023)": "2023. arXiv preprint arXiv:2304. Qingwei Lin, Hongyu Zhang, Jian-Guang Lou, Yu Zhang, and Xuewei Chen. In Pro-ceedings of 38th International Conference on Software Engineering Companion. 2023. LogPrompt: PromptEngineering Towards Zero-Shot and Interpretable Log Analysis. 2022. Chuan Luo, Pu Zhao, Bo Qiao, Youjiang Wu, Hongyu Zhang, Wei Wu, WeihaiLu, Yingnong Dang, Saravanakumar Rajmohan, Qingwei Lin, et al. 2021. 11811191. Adetokunbo AO Makanju, A Nur Zincir-Heywood, and Evangelos E Milios. 2009. Clustering event logs using iterative partitioning. In Proceedings of the 15thACM SIGKDD international conference on Knowledge discovery and data mining. 12551264. Salma Messaoudi, Annibale Panichella, Domenico Bianculli, Lionel Briand, andRaimondas Sasnauskas. In Proceedings of the 26th Conference on ProgramComprehension. 167177."
}