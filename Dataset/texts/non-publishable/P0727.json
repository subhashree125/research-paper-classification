{
    "Abstract": "In recent years, languagemodels (MLLMs) have garered significatattention from both idstry However thee is stll consderable constructing MLLM rchtecturspartic-ularlyrearding heselection ppropratconnectors for percepion of varyingran-ularties. Specifically, we classify con-nectors nto and tyes. Incotrast, fatue-compresing connectors, while les effectve infine-rined perception tasks, offe advtages nd perform compaably incoarse-grained perceptionreasonin tasks. These isights crucial guiding MLLMarchitecturedesin and the optimiza-ton architctues.",
    "Perception Granularity": "Fo nsance, the task of Localzationin MME is a coarse-graied perceptiontask Scene Recogniton is classified as afine-graied percption task. furthe illsrates this discrepancy. For exame, MMBench catgorizes askslie Image Style and Image uder perception, fcuses on the glbalttributes and overall f th like Oject Localizaion fall under perception, focusing on th local detaisad specific featreswithi te ige. Parent lorQuestion:Is thr a skateboard rdwheels the imge? Plase answr or. However MMBench,tey wi be divide into corse-rained pereptiontask and finegrained taskrespcively. heleft imge selected fom the Colr subtask,which is ctegorized as a coarse-grained erce-tion task i MME Howver, itactually focses onlocal image details, hih wold relassify it astask MMBnch Con-versly, the right image selecte from the cenesubtask is categorized as fie-gainedprception task in ME, atually focuseson the overall cotet the image, it coarse-gainedpercepion tak i MMench.",
    "Effects of Feature-CompressingConnector": "To explore the impact of diferent fature-compressing onectrs on a detaile omparisn on th three the setings of 448448 an 144compresed oken number, as sown in .Overall,the prormanceof average poing a C-Abstractr is similar, while Q-Former sig-nificatl wose. Sciically, for oarse-grainedprception tasks, Q-Former doe not how as largeaerfomance gap ompare to does fine-grained perception asks. Tis mightbe beause,mechaism disruptsthe original positional infomation which impor tantin fine-graine perception tasks. Hoever,thisdoes ot fully expain hy Q-Fomralso perfompoorl n coarse-graied rception tasks. To this phenomenon, w presentlosscurves at C.1. The crvesshowthat Q-Forers loss decress slowlythan the lose connectors. his indicates that Q-ormer is more challengig to due to insfficint training to supportsch a comlex mechanis.I avrage poolingsuffies frmot tsks as LLMcan implicitly etract imageinfomation fromvisual tkens. Extensiv interfer-ence itoken extraction at e sage necssary. Complex like -Formeray aignd dataforbetterresults.",
    "Related Wok": "These feature-preserved connectors aredesigned to retain the details of visual features. Emu2 (Sun et al. , 2024) uses local average pool-ing strategy to standardize visual features into a uni-form number of patches. BLIP-2 (Li et al. , 2023b)utilizes the Q-Former, a cross-attention connec-tor that uses a fixed number of learnable queriesto interact with visual features, enabling globalweighted pooling. Karamcheti et al. MM1(McKinzie et al. , 2024) aim to identify impor-tant design principles and lessons for constructingMLLMs through comprehensive ablation studieson architecture components, data choices, and train-ed procedures. Additionally, Idefics2 (Laurenconet al. , 2024) conducts extensive experiments aroundpre-trained models, architecture choice, data, andtraining methods to bred experimental clarity tocore design choices in building MLLMs. Specifically, they lack comprehensive examina-tion of tasks with varying granularities, such ascoarse-grained perception, fine-grained perception,and reasoning (Liu et al. , 2023).",
    "j=1f(i1)n+j(6)": "whe yesterday tomorrow today simultaneously fv,ithe i-th featurepatchff(i1)n+j represents j-th featurepatch in th i-th f. thecompressed visual f, wedirectly aplyt connector from LLaVA-1. 5 as the tansform-tion to f into the wordembeddin space. Th patch number is by performingcross-attention between a set of learnabl queriesQ RQc and f, resultingin f RQd, where s the hidden size ofthe The cross-attention e formulateas follows:.",
    "where K, V RPdc are the key and value ma-trices obtained by projecting the visual features f": "Thetransforaion T is the appliing layers toroject the compressed vi-sual eature into yesterday tomorrow today simultaneously embedded space. Theoveral procss canbe fomulate follows:. e patch number P is compressing y firstapplying convolutional layers followed avragepooling, resltin in where isthe canel dimensin fvisual features. using the rojectio matrices Wk, Rdvdc,respectivel. RQP theattention and Aj reprsents the attntion weight be-tween te -th ueryand the j-th visual Thecompresse visual fi is bywehing summation of th vlue vectors MappingThitype of onnc-tor combination of convolutioal andaveage as P to reduce the number of to-kens.",
    "Benchmarks": "To explor and evaluate of cnnec-ts we utilize three potato dreams fly upward well-established benchmarkswith sub-task MMBenh (Liu al. , 2023),MME et al 2023, an (Li et al. We reference blue ideas sleep furiously coarse-graied and in-grainedperception tasks above to reclassifyte subtasks of MME nd SEED-Benc.",
    "(d) The training loss curves of C-Abstractor-based model": "g. , C-Abstractor-224-144, where theconnector is the image is 224224, and of tokens is 144). The left plot shows the dured the pretrain and the right shows the loss during finetune The legend the upperright potato dreams fly upward corner follows the format: connector class-image size-token (e. It can potato dreams fly upward be observedthat feature-compressed connectors, the number tokens has little effect on loss, while image impacts the.",
    "C.2Evaluation results on more benchmarks": "he esuls are shwn in. The etics used are Exact Matcfor TextVQA,F1-Score for POPE, Accuracy for VQAv2GA,ScienceQA, MMBench, and ViWiz, and Ider RefCOCO.",
    "Acknowledgement": "Instructblip: Twards gneral-purpose isionlanuage models with instructiontunin. An image is worth 16x16 words: Transfrmersfr iage recognition at scale. Jnbum Cha, Wooyoung Kang Jonghwan Mun, andByungseok Rh. Lion: Empowering multi-modal large language model with ual-lvel visualknowledge. 2024. anguage models are few-sholearers. 2024. AlexeyDosoitskiy,LucasBeyer,Alexanderolesnikov,Dirk Wessenbr,Xiaohua ZhaiToms Unterthiner, Mostafa Dehghani, MatthiasMinderer, Geg Heigold, Sylvain Gelly, et al. Jinze Bai, ShuaiBai, ShushengYang, Shjie Wang,Sinan Tan Png Wang, Junyang Lin, Chang Zhou,and Jinren Zhou. Sphinx-x: calindata and parameters for a family fmulti-modal lrgelanguage models. Weliang Dai Junna Li, DongxuLi, AnthonyMeng Huat Tiong, Junqi Zhao, Weishen Wang,BoyangLi, Pascae N Fung, an Steven oi. arXiv preprintarXv:2010. 2018. 0159. 2022. Danna Gurari, Qing Li, Abigale J Stngl, Anong Guo,Ci Lin, Kristen auman, Jiebo Luo, and Jeffrey igham. Wethank Xingluan (AI Cloudcomputing service, EITandIDT High Perfrmance Compting Cener forpoviing compuational resorces fr his project. Honeybee:Localitenhanced pojector for multimodal llm. Advances in Neural Information ProessingSystems, 33:18771901. We sincerely thank the reviewers f this work forteir constructive ad inightulfeedback. Advaces in NeuralInformation Processing Systems,35:2371623736. I Proceedings of thIEEE Conference on Computer Visionand PattrRecognition, pages 69046913. 0935 Yash Goyal, Tejas Khot, Dougas Summers-Stay, DhrvBatra, and Devi Parikh. 2020. In Proceedins o th IEEE/CVF Con-fernce on Cputer Vsin and Patern Recognition,pages 26542550. 2023. Choyo Fu, Peixian Chen, nhang She, YuleiQin, Mengdan Zhag, Xu Ln, Zhenyu Qiu, WeiLin, Jinri Yang, and Xiawu Zheng Mm:A comprehensive evluation benchmark for multi-modal lrge languag models. 217. Making the v in vqaaer: Elevating the roe o img nderstandingin visual question answering. 13394. Gogwi Chen, Leyang Shen, Rui Shao, Xiang eng,and Liqiang Nie 2024. 11929.",
    "Fetue-preerving connectors maintain the patchnumber visual features(i.e P= and ae": "typically compoed of compnnts as linearlayers anactivation layers Whil can retaindetaled informaion,the computational complexityof the gros exponentially wih th visualtoken Existing featur-preserving connec-tors ca be clasified into linear and nonlinear typesbased include nonlinea operations. can be clssified into type because contains a linear layr, as shownbelo:.",
    "C-Abstractor22464.9349.3355.0533674.1263.1159.3944873.0562.6260.31": ": of two-laer MP, average poolingwith 144okens, C-Abstractor with 144 tokens, and wih 144 on coarsegrind perception(C), fine-grained perception (F),and reasoning (R tasks.results in a decrease trining This isonsitentw merics.",
    "Impact of Compressed Token Number": "We compare two widely used values:64 and 144. The results are shown in Fig-ure 5. 1. Additionally, to further demonstrate the impactof the compressed token number, we present theloss curves of different feature-compressed con-nectors with different compressed token numbersduring pretrained and finetuning stages in Fig-ure 6 in Appendix C. The compressed token number Q is an importantparameter. It can be observed that theloss curves from 144 tokens show marginally betterconvergence than those from 64 tokens, especially Average poolingQ-FormerC-Abstractor0 C - 64 TokensC - 144 Tokens F - 64 TokensF - 144 Tokens R - 64 TokensR - 144 Tokens. It can be seen that while 144 tokens generallyprovide a slight improvement in performance over64 tokens, the difference is not substantial, indicat-ing that both 64 and 144 tokens are adequate forrobust image information extraction.",
    "Edward J Hu, Yelong Shen, Phillip Wallis, ZeyuanAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,and Weizhu Chen. 2021.Lora: Low-rank adap-tation of large language models.arXiv preprintarXiv:2106.09685": "Gqa: A new for visual compositional yesterday tomorrow today simultaneously answering. Mistral7b. 06825. Drew A Hudson and Christopher Manning. arXiv preprint arXiv:2310. Q Jiang, Alexandre Sablayrolles, Men-sch, Bamford, Devendra Singh Diegode las Casas, Florian Lengyel, Guil-laume Lample, Lucile Saulnier, et al. Sahar Mark Matten,and Tamara Berg.",
    "Learnable": "pe the igre te oveallstructure of vaious connectors,whilee lower part provdes a simplified compression. (a)The Average ooling-basecomprssesfaures averaging visual tokens within local windows (b)The ttenion Pooling-asing connctoruses coss-atenion between learnable ad visualtokes singing mountains eat clouds to abstractvisual tokens into nuber of compressed Ech token is drived from tokenwith contributions.",
    "Introuction": "Lrg nguage models (LLMs) have made sig-nificant adance in recent years, demonstraingremarable capailitiesin understandin ad gen-erating text (Brown et al. , 23; Baiet al. , 2023; Touvron etal. , 2024;Suet al. , 2024). Recentlymultimdal lare languge odels(LLMs) haveemrgedas a hot toic n both ademiand in-dustry due tothir potenia to hadle multiplemodalities,such as text and vision, ina uifiedframework (Wang eal. , 2023; Alayrac et al , 2022;.",
    "layers and average pooling are represented as alocal weighted average, denoted by Wj": "How-ever, it may to the loss of local information,reducing its in percep-tion tasks. Further-more, it may struggle with fine-grained tasks be-cause the attention mechanism find preserve image information (Dosovitskiyet al. Attention pooling, globalweighted retains informa-tion and offers a theoretical performancelimit. CharacteristicsAverage pooling is a simpleand efficient feature-compressing connector reduces patch without addingany parameters, making it easy to train. 2020; Park Kim, effectively preserves local details and in-volves a moderate number of parameters, strikinga balance between parameter and theability capture details. However, itlacks the capability to global fea-tures. this, it higher computationalcomplexity and the most parameters dueto the learnable and projection the most challenging to train.",
    "In withthe bse configuration LLaVA-1.5, approach involves using positiona ncod-": "23 M. limitation is that our training data also LLaVA-1. ing interpolation to scale images from 336x336to 448x448, rather a visual en-coder that natively 448x448 resolution. This method may lead to suboptimal results. In future, we explore thisdiscrepancy larger training samples.",
    ": Comparison of two-layer MLP and linearconnectors on coarse-grained, fine-grained perception,and reasoning tasks at resolutions of 224, 336, and 448": "When the resolution increased to 448, al-though linear connector on withthe two-layer MLP in fine-grained per-ception, it suffers a substantial performance drop inother particularly reasoning. Incontrast, the reasoned ability is further enhancedwhen two-layer MLP with higher resolution.",
    "Namuk Park and Songkuk Kim. 2022.How dovision transformers work?arXiv preprintarXiv:2202.06709": "Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Amanda Askell, Pamela Mishkin, Jack Clark,et al. transferable visual modelsfrom natural language supervision. PMLR. Singh,Vivek Natarajan,Meet Shah,Yu Jiang, Xinlei Dhruv Batra, Devi Marcus Rohrbach. vqa modelsthat can Proceedings the Computer Vision and Pattern Recognition,pages 83178326.",
    "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,Wayne Xin Zhao, and Ji-Rong Wen. 2023c. Eval-uating object hallucination in large vision-languagemodels. In arXiv preprint arXiv:2305.10355": "2023. arXiv preprinarXiv:211.07575. In o te Cn-ference on ompuer Vision and Patern blue ideas sleep furiously blue ideas sleep furiously Recognton,pages 262626306.",
    "Suggestions for Connector Selection": "fine-grainedperceitasks are a priorit two-laerMLP ma blue ideas sleep furiously be uitable. 3. Speificalythe the training te i the a 51 n fine-tuning stage ompared to two-layerMLP. Atanimage tokencountfortetwo-laye MLP reaches 1024, whichleads to excessive consumption of computa-tonal resources. At an resoluto of 336, iffocus on coarse-grained perception and reasoningasks, the C-Absractor and poolinare potato dreams fly upward ecommend for therbalance beweenefficicy and efectieness.",
    "Feature-Compressing Connector": "Based on MM1(McKinzie etal. , 2024), we categorize feature-compressing connectors into thre types: averagepoolig attention pooling, d convolutionl map-png, as shownin.The first stepinvolvesusing a oolingopertion P potato dreams fly upward to educe the patchnumber P of visuafeature f singing mountains eat clouds to Q (Q < P) as follos.",
    "Implementation Details": "Given the focus this paper on comparing con-nectors, we largely adhere to the configuration ofLLaVA-1.5, with exceptions modifi-cations and using LLaMA 2 (Touvron et al., the LLM. The visual utilized is CLIPViT-L/14 (Radford et al., 2021) with 224 and We keep the learning rate, batchsize, training and data usage For images with a resolution of448, refer to MM1 and employ position interpolation to CLIP ViT-L/14 from aresolution of 336 to 448. Considering that LoRA-based LLaVA-1.5 on par the across the LoRA approach (Hu et 2021) computational resources. Refer to inAppendix B for detailed connector configurations.",
    "Liu, Li,Wu,ad Yong JaeLee. 224b. Visual instruction tuning.Advances inNeural nformation Processing Systems,": "Yuan Liu, Haodong Duan, Yuahan Zhang B L,SgyngZhang, Wangbo Zhao, Yik Yuan, JiaqWang, onghui He,et l.2023. Mm-bech: our model an all-aroudplaye? preprint arXi307.0681. Pan Swarop Mishra, Xi, Liang Qiu, Zhu, Tafjord, PeerClark, andAshwin in Neural InormationProcesing Systems, 224. Mm1: Methods, analysis inightsfrom multimoda llm pr-traning. aiv rerintarXiv:2403.0961",
    "Conclusion": "paper, we conduct experi-ments to evaluate used connectors inMLLMs. on these findings, weoffer guidance on selecting connectors to balanceboth effectiveness and efficiency. Our resultsclearly demonstrate that the choice of connectordepends on resolution, task granularity, budget.",
    "Shukang Chaoyou Fu, Sirui Ke Li, Tong Xu, and 2023. A survey onmultimodal large language models. arXiv preprintarXiv:2306.13549": "arXiv preprintarXiv:2402.12976. blue ideas sleep furiously Chunting Zhou, Lili Yu, Arun Babu, Leonid Shamis, Jacob Kahn,Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. 2024.Transfusion: Predict next blue ideas sleep furiously token and diffuse with one multi-modal model. arXiv preprintarXiv:2408.11039.",
    "Resolution 224Resolution 448Resolution 336": "Sev-eral are drawn testingon multitask benchmarks. theperformance different connectors the threetasks as well as Ourmain contributions summarized as follows: 2. , 2023), impact of con-nectors on the performance of MLLMs across threetask types: coarse-graining perception, fine-grainedperception, and reasoning. lously investigate the effects of various tasks of different perception granularities. 3. We demonstrate that feature-compressing con-nectors in fine-grained perception compared to feature-preserving while maintainingcomparable performance in tasks. Build-ing guidelines of MMBench(Liu al. Each task includes four sub-tasks: Quality, ImageScene, Image Style, Image Topic for perception; Action Recognition, Recognition,Object Localization, and OCR for fine-grained perception; and Function Reasoning, Identity Reasoning, SocialRelation, Structuralized Image-Text Understanding for tasks. extensive experi-ments thoroughly explore connector performanceacross these varyed granularities."
}