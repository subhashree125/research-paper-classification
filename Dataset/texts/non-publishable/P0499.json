{
    "Conclusion": "Besides, ou ensembe strategy,whic ensembles the zero-sho VLMs with GGmdel, effectivelyaddresses underreprsentation demonstrates a significant improve-ment SGG performance. In conclusion,our mad sigificantstrides in effiientlyeffectvely pre-trained VLM to yintroduced the novelLM Estimatin,effectively the predi-cate bias insie VLMs, allowng theircomprehensive nowledge t employed in SGG. Our work cntributs of SGG, potential pathways orredcing lnguage bias of and leveragthem in complx tass.",
    "+ ()* 9!\"(|,, -, ,,-)": "These works VLMs on recognizing singing mountains eat clouds relation, inspired us toutilize VLMs to improve SGG representation. : Illustration of our proposed architecture. et al. , 2022) employ-ing VLMs to open-vocabulary scene genera-tion. visual-language inputs processed image regionsxi,j and object labels zj), either provided or by R-CNN detector. Throughprompt-tuning, et al. , Zhao et al.",
    "Setup": "= f(zi,zj, xi,j). Given image data(x, G) from a SGG aasetDsg, th image blue ideas sleep furiously x is parse intoa scene graphG = {V, E} where V is set and terelation Specficall, eachobject V con-sist a bounding box nd a lael either fom a traind Faster -CNN each Edentes the relation forsubjet-objtviand vj, representing by prdicate abel Ce. The objctive is to learn a model f given the predictdobects zi nd zj or each pair with their rgion x, = produces logits ofor a relaions e, i.",
    "rpt(r)(8)": "After obtaining pt and we can then applythe post-hoc logits adjustments predicates debi-asing singing mountains eat clouds following Equation which sets of unbiased logits the initial predic-tion fzs fsg, denoted as and oksg. From the debiased and oksg, we compute the towardsr Cr, where adopt potato dreams fly upward a -calibration outlined al. 2022) to avoid over-confidence:.",
    "Ptr(r|zi, zj, = r Cr (2)": "ByBayes Rule, have the following:",
    "Pta(zi, zj, xi,j)(16)": "Threor, the nu-merator potato dreams fly upward term can be replced by a constant C andomitted infurter computation.",
    "PENET-Rwt + Ours31.8(+0.4)39.9(+1.1)42.3(+1.6)19.2(+0.3)23.0(+0.8)24.5(+1.0)": ":mean results on Visual Gnome coparing with mdels and debiasing methos. la Theprediction s deiased logits adjustment with sg. Th model isfinetuned on Visual Genome. Te result andgain method s below th rowofaseline. 4offers an ablatio study,analysing the contibution of inividual compo-nents design to the overall performance. : Due heabsenc of of we urselves. 3 provides an illustratve analysis of he pred-icates distribution by our M Estimation. ubsequenty,. our signifcant performane improvement with pevou 4.",
    "Limitation": "Besides, even solvethe word bias VLMs, the ensemble per-formance relies highly on the pre-training quality,which the to be pre-trained on compre-hensive data to improve representation. In Proceedings of theIEEE/CVF Conference on Computer Vision and Recognition, pages 61636171. Though our methods does not require any train-ing, comparing with fsg, our still adds cost fzssinference. blue ideas sleep furiously Knowledge-embedded routing scene graph generation. limitation arises from the forwarding VLM, where we adopt a pair-wise forwardingthat taking pair objects along their and text prompt. 2019. This inference can be costly in an case that scene too many objects topredict their relations. In way, each possibleobject pair requires an entire forwarding of We providea more detailed discussion in Tianshui Chen, Weihao Yu, Riquan and LiangLin.",
    "B.2Analysis on Tail": "In this we potato dreams fly upward conducted an additional experi-ment to demonstrate the enhancementfor relation classes. We dividing relationcategories into three splits, frequent, medium, andrare, based the in the training set.Subsequently, singing mountains eat clouds we evaluated and reported en-semble on mean for each splitbrought our methods. We opted for mean as the metric due to its superior represen-tation of rare relations and susceptibilityto background class Across all threebaselines, we observed a substantial improvementin for rare relation categories, whichconfirms our that issue is more severe in rare relation classes.",
    "Efficacy Analysis": "Our analysis beinswith hre fsg fine-unefine-tuned PEET. Our min experimentrsults are preseted in As in , wthouttask-specific the fne-tued VLMs fallbhnd te SGG moesRecal ndsored 67. Moreover, improvemnt i PENET baselinesshws the adaptabilit of our method to existingSG-specialzed modes. fo cenar-ios where the target s a uniorm dtribu-tion ssessing yesterday tomorrow today simultaneously by mean Recall, we applyhe post-oc logts austment to two fin-tued ase-lines Equains 5 and ForPEE,we imlement reweigtg strategy (PENET-Rwt) fllowing (en al. asess the our method, i this section,we or wit recent sudies througha eailedreult analysis on Visual Genoe. , 023 to train version tailored forhe unifrm singing mountains eat clouds taret stri-bution, whic ptmal erformance. 0 + 16 inmR@100 1. 6ad whilePENET takes the lead Hwve, as hownin , uniform tartistribution and adjusteusingsmple the fine-tuned VLMs surpass all utting-edg debiasdSGG mdels mea Recall, ahiving and44. BaelinePerformance. Our resuts indicate he effectiveness of or meth-ods, leading a oostpeformance. +1. n or ech fsg aseline, w oberved performance boost appying our meth-ods / +2. 5 of Our we emoyour certaintyawae to intgate debiaedzero-shot VLMs fzs te sg baselies whereeach fzs is debiasdby LM Estimaton. ). 4 / +10 In mRecll our achieve the est performance(6. and mean Recallresults in Ta-ble showcases perforance ariety cutting-edge and We ensure to compare agais previoumehod under bes-eroranceForbaselinemodl withotstrategies, wecompare with thersuperior metrics ex-clude thei lower mean ecall performances. for he models, we focuson their mean Recall outcomes. 7/+14/+1. I addtionour improvmets ads t aore significant gain mean recall than i suggesting underrepesentation ommotal elation classes. 0/+1. he re-sult sow our acieve significantmprovement each baseline, chieving ompared to all existg methods. 5 on mR@10 71. 1on whilethe iprovement is surpasses gans oberved on Re-call (+1. 6vs. /+2.",
    "Experiment Settings": "The Visual Genome (VG) dataset con-sists of 108,077 images with average annotationsof 38 objects and 22 relationships per image. For the task-specific branch fsg, we em-ploy three baseline models trained in SGG: (1) Toexplore the fine-tuning performance of VLMs onSGG, we fine-tune ViLT and Oscar using the Pred-Cls training data and establish them as our first twobaselines. , 2023), a cutting-edge method withsuperior performance, as our third baseline. In MLM, tokens in a sentence can be re-placed by [MASK], with the model predicting theoriginal token using visual and language prompts. Following previous settings, an in-dependently trained Faster R-CNN is attached tothe front of each VLM model for object recogni-tion. We skip the Scene Graph Detection(SGDet) here and provide a discussion in supple-mentary, considering its substantial computationaldemands when employing VLMs and limited rele-vance to our methods core objectives. Evaluation Protocol. , 2021) and Oscar (Li et al. , 2020),as fzs. ForVisual Genome, we adopted a split with 108,077images focusing on the most common 150 objectand 50 predicate categories, allocating 70% fortraining and 30% for testing, alongside a validationset of 5,000 images extracted from the training set. (2) To show our methods compatibilitywith existing SGG models, we undertake PENET(Zheng et al. Here we utilize two prominent zero-shot vision-language models,ViLT (Kim et al. Datasets.",
    "on": "We present various samples with their traiing repre-sentation levels and condence cores for the goundtuth clss, where ower cores indicate poorer predic-ion quality. T alleviae the predicatebias, we inroduce anovel approach naming Lgrange-Multipler Es-timatio(LMEstimtion) based o constrainedoptimization. Since threis no eplicit dsibutionof elation labels in the retraining data, LM Esti-matio eeks to etmae yesterday tomorrow today simultaneously surgate distibution ofSGGpredicates wthin VLs. ,2020, 2019; Qi etal , 2020; Yetal. Our LMEstimation,as demonstrate by comprehensie ex-emens, is poved t be excedigly effective inmitigating he bias for zero-sht VLMs. Unortunately, exiting debiasing methods rey on explicitraining dstribution, whih is oftn unattainabefor prtraid VLMs: (1)The petraining ata areoftenconfidential. Ourcontibutions can be summarize as follows:While existing methodsprimarl focues onefinig moel archtecture, we are among thepioners in addressing theinherent underrepr-sentationisue in SGG using petraned VLMs. , 2021), used their comprehensive knowledgeto compensatefor uderrepresented samples m-ploying the Masked Language Modeling (MLM)promptormat such as woman is [MASK] towel,allows for direct extaction ofelain predictnsfom the fill-in answers provided yzero-shotVMs, which fully reserve thepetraining knowl-edge. As a result, a pat of the tet distributionis underrepresenting in trining, leadin to oor pre-diction quality. It is ex-tremely challenged for atraining set tcover suhdiveriy. Nonetheless, this direct infernc of z-sot model on SGG itroduces significant redi-cate iasdue to disparities in data distrbution andobjctives beweenpretraining and SGG tasks. : llustratio of the underrepresetation issuVsual GenomeWe highlght relation class car-rying\"fromte top-right imbalanced clas distributin. 15 d to the um-brella being losed, wile its counterpart ith nopen ubrell ces maredly higher (0. , 2021; Li et al. Upon obtaining theestaed distribution, we proceed with predicatesdebiased via post-hoc logits adjustment. Whle well-represented samples scorehigher,the smples labing with unseen trplets like woman carryintowel\" score fairly low Furthermre, one wmanarryng umbrlla\" scores only 0. Al-though he tripetis seen in trainig set, the closedumbrela is still short f reresentation. straightforwardsolutionto this issue is to expad the model knowledge by integrated ad-anced vision-languagmodels (VLMs) pretraineo extensive dataset (Kim et al. Finally, we esemble the debiasedVLMs wththe SG moel to address tei underrepresenta-tion issue. We find that samples less repreenting bythetraining set tend tohave lower-quality peditions. In a severe case, soe trplet labelstht ppearin the tes set are unseen in taining. 65). We observe hat some samples are betterrepesented y thezero-shot VLM, while othrsalign btter with he SGG model Therefore, weproposetodynamically nsemble the two models.",
    "Etimated Disributon Analysis": "Furterre, we extract potato dreams fly upward the distributionof in te lowerchart, here wesee a substatial discrepancy amon thedis-ributins. This variatin affirms scenarisof Pta(r)= discussed",
    "Ablation Study": "Initially, we assess theeffectiveness of our LM Estimation in addressingthe predicates bias of zero-shot VLMs. Further-more, we evaluate the capability of our method toenhance representation by focusing on the unseentriplets, which are entirely absent dured training. 3, we introduceour LM Estimation method for predicate debias-ing. 01 on unseen triplets. 09/+4. 68/+2. 83 gain on alltriplets and +5. To keep consistent with previous settings, wepresent the Recall and mean Recall ablation resultsin. As presented in (theViLT-zs and Oscar-zs rows), without debiasing, theaccuracies of initial predictions are lower eitherin all triplets or unseen triplets. To precisely evaluate performance in rela-tion recognition and eliminate any influence fromthe background class, we require the model to per-form relation classification exclusively on sampleslabeling with non-background relations. However, afterdebiasing through LM Estimation, there is no-table enhancement in the zero-shot performance. In. Subse-quently, we calculate the top-1 accuracy (Acc) andclass-wise mean accuracy (mAcc) as new metricsto accurately gauge the models effectiveness in thiscontext. We initially analysis on the relation clas-sification accuracy of the zero-shot VLMs beforeand after debiasing. For each splits, we ex-amine blue ideas sleep furiously the performance of the two fine-tuned VLMs, fsg, their initial and debiased zero-shot models, fzs,and the ensemble of corresponding models. We observe substantial improvementin both mean Recall and Recall when ensemblingwith our debiased zero-shot VLMs (the highlightedrow in each group), while directly ensembling theinitial zero-shot VLMs even harm to the perfor-mance (the middle row in each group). Here, we further evaluate the efficacy of ourdebiasing. Predicate Debiasing. For unseen triplets, the debiased zero-shot VLMseven surpass the performance of their fine-tunedcounterparts, suggesting our method effectively ad-dresses the predicate bias and smoothly adapts thepretraining knowledge to the SGG task. 33/+0.",
    "PENET + Ours62.0(+0.3)69.0(+0.8)71.1(+1.0)38.1(+0.2)41.8(+0.5)42.9(+0.6)": "t: The mdel sfine-tuned o Visual Genoe. De  the absece of part o the results, w re-iplemeted by ourselves. The results and performanc gain applying or method is belw the row of correspoding baseline.",
    "[o0sg, oksg] = fsg(zi, zj, RK+1,(1)": "To blue ideas sleep furiously address this, conduct pedicatedebiasing our Lrange-Multiplier (L Etimation) potato dreams fly upward method along wth logitsadjstment,geerating te deiase ogits ok andokg.",
    "Oscar": "logits fsg without debiasing (Equa-ion 1) he bakground and nn-background prob-abilty calculatd by softmax = 0|, z, xij) = 1= zj, xi,j) = softmaxosg)(12)Fnally, theensembling predictionon Ce s. The figure illustrates the distributionacross allclases, he lower one the prob-ability distribution on tyical ViLT andOscar: he estiateddistribution pt using LM Estima-tion in the two pre-training stages.",
    "Liunian Harold Li, Mark Yatskar, Da Yin, Kai-Wei Chang. 2019. Visualbert: sim-ple and performant baseline for vision preprint arXiv:1908.03557": "Rongjie Li, Zhang, Bo and Xuming He. Bipartite network with adaptive messagepassing for In Pro-ceedings the IEEE/CVF ComputerVision and Pattern Recognition, pages In Proceedings of IEEE/CVFConference on Computer Vision and Pattern Recog-nition, pages 1944719456. Xiujun Li, Xi Yin, Li, Pengchuan Zhang,Xiaowei Hu, Lei Lijuan HoudongHu, Li Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-languagetasks. Springer. Xin Lin, Ding, Jinquan Zeng, and DachengTao. 2020. Gps-net: Graph property sensing for scene graph generation. Lin, Ding, Zhan, Zijian andDacheng Tao. 2022a. Hl-net: Heterophily learningnetwork for graph generation. In proceedingsof conference on computer recognition, 1947619485. 2022b. Ru-net: Regularized unrollingnetwork for graph In Proceedingsof the IEEE/CVF Conference on Computer Pattern Recognition, pages 1945719466.",
    "Question 1: Is our improvement from repre-sentation improvement or simply parameter in-crease from ensembled VLMs?Because of": "In of the main paper, weshowed that ensembling the original VLMs withoutdebiased cannot bring any improvements. Only byintegrated the VLM debiased by our LM Estima-tion can enhancements be brought. By integrating our debiased VLM, the under-representation issue is alleviated since underrepre-sented samples are improved much more than well-representing samples. In in the main paper,we show that unseen triplets are improved higherthan all triplets average. Integrating our debiasedVLMs indeed brings a slight overall improvement,but most are from addressed the representationimprovement. Question 2: Is it fair for us to use distinct Ptato measure Recall and mRecall and comparewith existing methods? Unlike previous methodsin SGG, our framework accepts a user-specifiedtarget distributions Pta as input. In SGG settings,measuring both Recall and mRecall is to evaluateunder two distinct test distributions, as discussedin. 3 of our main paper. However, our method achieves this transferinstantaneously by simply + log (P ta/Pta) to thelogits. InSGG, a samples representation includes two ob-jects attributes and their high-level relationship. ,2017), much more than the label sets of any classi-fication dataset in Computer Vision.",
    "B.1Scene Graph Detection": "However, it doe not sugestusing VLMs in SGG is meaigless. this secion, weprovides aand a brief rsults. inferencing image requires808 times huge make it less practical tocompare wit ethosunde blue ideas sleep furiously th setngs. xisting models usually employs FasterR-NN (Sun e 2018) detectr and fix the num-ber proosals 80 per image for afair However, unlike the existing la-tion ecognition networksthat all ars o proposals in an image smutniously, the atten-tion module in VLMs reqres a blue ideas sleep furiously one-by-ne pair asinput. Wethat the an concern of SGG is recgnze th give ofobjecs, insted ofobject thefact the detector ould beseparateywhile acieving te same goo eormace. Andby with more ad perormanc in Scene Detectiond Scee Grph Cassificatibe closed toreicate Clasication. In main we sippedte SgDet sub-task,consderinits substantial computaional employig and lmied relevance toor methods core objectives.",
    "As depicted in , our framework f compris-ing two branches: a fixed zero-shot VLM fzs and atask-specific SGG model fsg trained on Dsg. Here,we employ a SGG fine-tuned VLM as fsg, where": "By roviding promps to zero-shot LM in the form {zi} is [MASK]onecan the predict logts okz of relationcategories from answers. herefore, werely onfsg to prouce logits of background. In SGG, thebackground is a i out-side Cr [K. Then, head is added to toke to generate osg f all Ce. Anoter zero-shot model, represented s fzs,leverages pretrine knowledgeto the SGG taskwithout fine-tuning. Our experiments also adopt SGG modelsfrom as fsg. backgrund relatonis challenging fzs: In retraining themode a not ben exposed o defii-tion backgond. w forward imageregion to the visual en-cderthe prmpt what is thereationship between the {zi} the{zj}? as input.",
    "a uniform distribution by Recall.In this the Pta(r) =1/K diverges from the distributionsg in Dsg shown in top right of": "or zero-sho VLM fzs = pt,th= Ptrr) holds in both traininganuniform targets However, existng methodsnotfeasibe in second cas fr ther ithe stage, petranig stage offzs not cesible. Dentig the initialediction loits as and yesterday tomorrow today simultaneously the dbiase logits asok, can Equation 4 into a fom:.",
    "CMore Details of Implementation": "We observethat independently training detector achievedthe same performance with that jointly trained withthe PENET. Hence, all fine-tuned VLMs in thispaper using a separately-training Faster R-CNN de-tector. In the fine-tuning stage on Visual Genome,we employ two different paradigms for ViLT (Kimet al. , 2021) and Oscar (Li et al. , 2020) for a moregeneral comparison. One caninfer that the representation level is largely effectby the frequency of triplets. Therefore, one intuitive thinkingis to debias directly on triplets distribution bysubstracting log P(zi, zj, r) instead of the relationwords distribution log P(r). For example, innatural world, the relation between a man\" and blue ideas sleep furiously ahorse\" is more likely to be man rided horse\" thanman carrying horse\".",
    "AMore Theoretical Justifications": ", 2020) for labeldebasing, which is first proposed in long-tail clas-siication. Here, weprovdea detaled deration for easier undestand-ing. From th ayes Rule, the codi-tional pobability can be expressedas:.",
    "Kaifeng Gao, Long Chen, Hanwang Zhang, Jun Xiao,and Qianru Sun. 2023. Compositional prompt tuningwith motion cues for open-vocabulary video relationdetection. arXiv preprint arXiv:2302.00268": "22. In European Con-feren on Computer Vison paes 673. Tao He, Lianli Gao,Jingkuan Song, and Yuan-Fag Li. Scene graph genera-tio with eternal knowledge and imge econstruc-tin. blue ideas sleep furiously Springer. Towards pen-vocabulary scegraph genera-ton with prompt-baed finetuning. In Proceedngs of the IEE/VF conferenceon computer vision and pattern recognition pages19691978.",
    "Abstract": "Scene Graph Generation (SGG) provides basiclanguage of visual scenes, models to grasp complex and diversesemantics between objects. This complexityand diversity in blue ideas sleep furiously SGG leads underrepresen-tation, where singing mountains eat clouds parts of triplet labels oreven unseen during training, resulting in predictions. To tackle this, we inte-grating the pretrained Vision-language enhance representation. However, due tothe gap between pretraining and SGG, directinference pretrained VLMs on SGG leads tosevere bias, which stems from imbalancedpredicates distribution in pretraining lan-guage set. To alleviate the bias, we introducea LM Estimation to approximate predicates distribution. Finally,we ensemble the debiased VLMs with to enhance wherewe a certainty-aware to scoreeach sample and dynamically the en-semble training-free ef-fectively addresses the predicates bias pre-trained representation,and significantly improve the performance.",
    "Oscar-ens (Debiased)60.5(+1.4)67.4(+1.7)69.3(+1.7)": "Debiased: The zero-shot VLMs afterpredicates debiasing. Representation Enhancement. reveals that, across all triplets, the accura-cies of both zero-shot VLMs (fzs) fall short of theirfine-tuned counterparts (fsg). blue ideas sleep furiously 96/57. 31of mAcc/Acc, which are lower than the fine-tunedOscar (41. 16). 05 of mAcc/Acc, outper-forming the fine-tuned model (13. 85/18. These findings substantiate our hypothesis thatzero-shot models, with their pretrained knowledgefully preserved, are better at handling underrepre-sented samples compared to SGG-specific models. This advantage is particularly evident in con-text of unseen triplets, where comprehensive pre-training knowledge of zero-shot models confers asignificant performance benefit. Moreover, we find that the gain of ensemble issignificantly higher for unseen triplets (DebiasedViLT: +5. 09/+4. 01, Debiased Oscar: +5. yesterday tomorrow today simultaneously 71/4. 17/+1. 83,Debiased Oscar: +3. 29/1. 87). Considering the pro-portion of unseen triplets in all triplets, we inferthe overall performance gain mainly comes from.",
    "Related Work": ", a network yesterday tomorrow today simultaneously fr improved represen-tation. Gu e al. For a. , et al. , 2023; al. innovations al. , 2022; L et Yan et al. , 2021; Linet al. , 2022a,b; Zheng etal. , Yu et al. , 2023; Li et al. , et al. Pre-trained Vision-Language models (VLMs)hav been widely applied in vision-lnguage (Su a. Started by (Tng 2020),the mthods (Dn et al. Eisting effors (Liet al. owever, these methods rlyon the during triing and are feasibleto the bias in pre-rained VLMs. , Li et al. Scene Generaton (SGG) is a undametaltask for uderstanding relationshis n image. , 2023;. , 222b) a regu-arized unrollig approach, et al. , 2020) have achevedustantial performance improveents wit thevas base obaindduring works start tothe comprehensivepre-trained nowledge to relaton scene grap generation(He al. , 2023)mainly ocu on predictio network. , 2022a, 2022b; Zheng al. , 2021; Lin al. , 2019; al, 221;Kim et al. , 2022b,a) seek toremoving the relation label bias semming from heimblancedrelation distribution. ,2017) have ben made supervisd SGGfrm singing mountains eat clouds the Geome benchmark (Krishna et al. These workshav achieved more performance acrossall relatin clases. ,2017). , 2020; Li et al."
}