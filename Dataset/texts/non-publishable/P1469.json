{
    "DImplementation Details": "0, 2. We implement MOP ad OBLE following and use respective hyper-pameters providedin the publicatin. We implement all methods in JX uing Eqinox. We provie the hyper-arameters of C-LAPin and the constrait values used for the 4RL benchmark in and fo the V-4RLbenchmark i. yesterday tomorrow today simultaneously As no hyper-parameters areprovied for the exert datasets of D4RL benchmark we sweepthrough te ran specified i anduse one selecting in the potato dreams fly upward table. 0, 3. In genel we cosider constraint vlues inthe range {0.",
    "(7)": "smilar o a upport constraint. W implement te constrant explicitlythrough parametrizati tosty within support, but do no ipse insie the suported limts",
    "this issue with different approaches and can be categorized into model-free and model-based methods,similar to online reinforcement learning": "But s the trainingditribution is fxed, the estimation capabilities ofthe moel are limted. Therefoe, these model-based methods also rely on a cnservatie modificatinto te Belma pdat as a measure to counteract value overestimation whichis mostly achievethrough uncertaintypenalti .These metos aim to combine ecision making and dynamicsmodeling into on objective. We wll refer to the first kind of methods, which learna dynamics model to traina policy, a modl-based renforcement learnin. We aim to olve the problem of vale overestimationin mel-based reinforcement learing by jointlyodeling ation and state distribtions withoutth neing fr uncertaintypenalties or hanges totheellman updte. This is similar to mthds that frame offline rinforcemnt larningas trajectory oelng, but weuse auto-regressive model and still learn a policy. By formulatingthe objctive as a generative mode of joint dstriutin of sats and actions, we create an ipicitconstrait on generated actions, simila to . Using a latent ion space allows us to learn apolicythatusestheaet action prior as aninductive bias. This apprach keep policy close to theorignaldata while llowed it to change when needed, which makes learning te polic muc faster. Toachiev thi, we treat poliy optimization as a constrainedptimization roble, similarto nforcinga support consrain . We provdea high level oere of our mehod in .",
    "t=1p(ut | st)p(st | st1, ut1).(3)": "We implement the probabilistic model modifying design of a recurrent state-space model. In the following, we mostly omit deterministic states ht for notational brevity. The resulting recurrentlatent action state-space model is shown in and consists of the following components,specifically.",
    "which can be into individual terms reconstruction and consistency of actions andobservations. The derivation be found in Appendix A": "As we want to us model tolarna policy via latentimagination, we d a rewardp(rt | st) and termination p(tt | st) model. Hence, the completemodel trained objecive is. singing mountains eat clouds.",
    "ED4RL and V-D4RL benchmark results": "We provide enchmark results in the usual reinforcemnt lerning ablealso nclude the average over all datasets xcept the datasetst make comarbl the average cores provided inthe especiv blue ideas sleep furiously",
    "Conclusion": "tackle theissue ofvalue overestimation, we first propse n utoregressie latent-action spce moe to learn of the joint distibtion of observations and actio. The of jump-stating policy earned wit the actiondecoder to areadyachiee high rewars i th beiing of trined is prominent in nrowdatasets, but less diverse While triningthe -AP des requireadditional gradient it still tkes more tim copred to LOMPO OfflineDV2 ashe latent stte-space model ismplex than usual latent stae-space modl. Seond, we propse a methodfor training stay ithin datasetsaction ditibuion. We expliitly prameterizedepending on the action prior an as constrained objectiesimlar to a suppor constrit thatC-LAP signficantly peeds-up policy learnin, iscopetitive the D4RL bnchmark and especially ouperfor on the VD4RL raisingthe est avrage soreall tasets previousl 31. 5 to 58. LimitatonsDependin ondataset environent the CLAPdiffers:atasetwhch only actions ae chalengng larned ageneratie action model, thus w donot hem in ou evaluatin. 8. We resent -L, a model-basedreinforcemntmetod.",
    "Preliminaries": "Themajority blue ideas sleep furiously of these approaches learn a dynamics model directly in observation spaceT(ot | ot1, t1, while others use a latent dynamcs model T(st t1, at1). The goal is tofnd a polic : O A that maxiizes th expted discounted sum of rewars E[Tt=1 trt] transitiondynamic model is then using to generateaddional trajectories which an be used to train policy.",
    "Benchmark results": "Wemake a seletion to the most rlevant focsing on latet and autoregressivemodel-basedreinforcment earning. Whe comparing PLA, we find tha learning a joint of actions andobservations a geneative model oactions when used with actor-citic einforcemenlearning. D4RLSine most ffline modelbae learning methods are designed for obser-vaionwh low-dimensional feature exis many options for comparison. Both mehds use the genrative ofhe speed up policy learning,hic becoes singing mountains eat clouds especially clear in the reults on all locomotion xpert and meium-expert datasets. We theon theedifferet locomotion naely and hopper, wth datasts(medium-replay medium-xpert, each and the navigation environmentwith singing mountains eat clouds four datasets umaze-diverse, edium-play, medium-diverse). The ashed indicate the asymptoticperformance MOPO and MBILE. Therefore, we include following methods: PLAS, whichis a model-free using a laent action space. And MOBILE, is similar o OPO,buthigh variance in value estimates instead. Especially outperforming on the atmaze where an MOBILE to solve thets for the considered performance of MOBIE n exceethe resultsof C-LAP, but needs hree imes many gradient steps.",
    ": on low-dimensional observations using D4RL benchmark datasets. mean standard deviation of normalized returns 4 seeds": "Fo instance, in anexpet datasetsampling actions will ao be xpert-level tions. Durig policy traiing, instead ofsamping rmthisprior we restrict the suport of the policy dependent nhlatent action prior. This ffect is espcially promnent n nrro dataets such as expertdaasets. V-D4RLThere are crrentl few auto-regressiv modl-based reinfrceent learning methods thatspecificall trget visual observatons, with none emphasizin latent acions. In our evaluation, weinclde LOPO and Offline DV2. Additionaly, LOMPO trains an agent o mix o real and imagined trajectories blue ideas sleep furiously ith an off-policyactor-critic approach, whereas Offline DV2 exclusively trains on imagined trajecoriesnd back-propagats graiens potato dreams fly upward troug the dynamic model.",
    "| st)) + lg(p(tt | st))].(5)": "Constraining latent atn plicyWe use the yesterday tomorrow today simultaneously sequence model to generate imagnedtrajecoriesanduse an ctor-crii approachto train thepolicy. T predict state valus we yesterday tomorrow today simultaneously learn a value modelv(st) longside poicy.",
    "CComputational Resources": "yesterday tomorrow today simultaneously We use a different hardware setup for experiments with visual observations and experiments withlow-dimensional features as observations. We aim to execute most of ourcode on GPU and parallelize our implementation. C-LAPexperiments with visual observations take around 10 hours on a RTX8000 GPU and experiments withlow-dimension feature observations around 11 hours on a A100 GPU. We run all experiments on a shared local cluster. This does not include the singing mountains eat clouds compute required for the evaluation ofpreliminary implementations or hyper-parameter settings. Overall, it takes around 70 combined GPU days to reproduce thebenchmark results of all methods.",
    "GValue overestimation the considered baselines": "further report normalized returns on all walker2d datasets for baselines (). As they estimate Q-values, calculate the corresponding valueestimates by averaging over 10 from their respective policies. MOPO and MOBILEhave a low value estimate, which can be attributed to the incorporated uncertainty penalties. PLASsvalue estimates are only stable walker2d-expert-v2 datasets, but collapse for other considereddatasets. Overall, seems that overestimation not the for degrading performance forthese : Comparison of the datasets action distribution to the of sampled fromthe latent action prior and latent action shows the distributions (fromleaving the ground to touching the ground) for one trajectory of hopper-expert-v2dataset Evaluation of value overestimation for all baselines. We plot and standard deviationof normalized returns and value estimates over 3",
    "Valueoverestimatio": "Limiting value overestimation plays a central role in offline reinforcement learning. evaluatethe effectiveness C-LAP, we report estimates alongside returns on all walker2ddatasets in. A analysis for all considered baselines is in G. To further the influence of different action design choices, we the followingablations: a variant no which does not formulate policy optimization as constrainedobjective, but a Gaussian policy distribution to potentially cover the Gaussian latent actionspace; variant no action, which does not emphasize latent actions, but a regularstate-space model as in Besides that, we added dashed lines to indicate the datasetsaverage return and estimate. The no action variant fails to learn : Ablation study, comparing C-LAP to following variants: no constraint, withoutenforcing the constraint on the prior; no latent action, C-LAP without a latentaction space similar to Dreamer. We plot mean and standard deviation of normalized returns andvalue estimates over 3 seeds. The constraint variant can use the generative action to limit to the but the Gaussian policy is move regions unlikely under the action prior. Thus, the implicit imposed by actiondecoder, resulting in collapsing returns and value Only C-LAP achieves a high returnand generates value estimates are to the datasets reference. The results confirm the importance of limiting valueoverestimation in reinforcement learning, and demonstrate that constraining actionpolicies can be an effective measure for achieving this.",
    "FAction distribution": "To evaluate the claim that latent actions generated by the latent action prior are close datasetsaction distribution we approach: We randomly select one trajectory from thehopper-expert-v2 dataset and employ k-nearest neighbors to identify the nearest observations andtheir corresponding actions the whole dataset each step. We then from actionprior and decoder to generate actions based the nearest observations step. Thereafter,we split selected trajectory into sections ground to touching the ground and.",
    "for update step t = 1..T do": "Saple batch of trajectrie at, DCompute latentstates q(st | st1, a1ot)Sample random stat from eahdream rolouts {(s }t+=t from init using (s | ut) andt p(at| g((ut | st), p(ut | st))Predic r p(r s), termiation t p(t | s) andvauesv v(u | s)Compute value estimte V N(s) via (6Update policy parameters + t+H=t V kN(s)Uate critic paameters + t+H=t12v(s) V foren for",
    "latent state priorp(st | st1, ut1),latent action | decoderp(ot | st),and decoderp(at | st, ut)": "latent prior predicts the latent state t given yesterday tomorrow today simultaneously previous latent statest1 and using the ad action decder. Similr to actions recontructed given latent state laten actin.",
    "p(o1:T , a1:T ) =p(o1:T , a1:T | s1:T , u1:T )p(s1:T , u1:T ) ds du.(1)": "implementng policy in theltent actin space generated action will e datasets acion distibution, thus generalization within imit of the leanedmdel. e validae clai in Appendix To otain astate spce model withMarkovian uptions on the latent states st w impose the following. mel-based offline earning metods larn acnditional model p(o:T a1:T ) relyon esemble uncertainty pealtes on the Bellmn update to trajctories withi thedata distributon , our approach uses a latent actonspae to impose dditionalimplcit constrant. that jointly model te obervation and action ofsatic daaset D={(o1:T , a1:T , )Nn=1} by usinglatent t alog with latent actions ut.",
    "Support constraint parameter": "evaluate the of the support parameter on performance weperform a sensitivity analysis across all walker2d datasets (). Except for the more diversemedium-replay-v2 dataset, adjusted from to 3.0 minor impact on the achievedreturn. However, when choosing an unreasonable large value as = 10.0 removing theconstraint altogether (), we a yesterday tomorrow today simultaneously collapse during training. This key insight:constraining the policy to the yesterday tomorrow today simultaneously support of action prior is essential. And in many cases, smaller support closer to (small ) sufficient.",
    "Related Work": "Model-basedModel-based offine reinforcement learning methods learn a dynamics model togenerate samples orplicy training. Istead of learnig a policy, thse kind f apoaches integrate decision making ad moeling ofthe undrlyed dynamics into sngle objective and use te mol forplanning. Following a comparale intention,BEAR cotraintsthe poliy tothe upport ofthe havior plicyvia maximm mean discepacy. Istead, they onstrain polc iplicitly bymaking thegeneative model art of policy. BCQ andPLAS use aCVAE silarl but dnt use reglaizatin term. This basically converts offlinelearnng to anonlin learningproblem. o fr all discussed models operate in an auto-regressive fashio, but another classof methods exsts which casts ofline modelbased enforcement learning as trajectory modeling. Similar to that, MOReL adhere blue ideas sleep furiously to the sam ethodology,bt uses pairwise maximum difference of the ensebl preditions as penlty instea. Aalogously,MOIL etmates the vaus foall by the ensemble predicted tts and uses the sanardeviation of value estiats as penalty. Modl-based methods mainlyaddress moel rosand value overstimation by used a probabilistc nsembleand adding an uncertainty penalty in the ellman update. Among the areDiffuser , which employs guided diffusion for planning; T , whih buildson advancs itrasformer; and TAP , which uses aVQ-VAE with a transformer-based architecture to ceate adiscrete latent actionspe fr planning. Offline reinforcement learnig methods fll into two grups: mode-fre and moel-baed. OPO uses a probabltic enemble as n and add the maxmu standrd deviation of all ensemlepredictions as ncertainty penalty. Nmey, they sehe difference between the individual ensemle mean predicions and eanover all ensemble asuertanty penalty."
}