{
    "Experiments": "This model was directly without any subsequent fine-tuning for taskrequirements during our experiments. we test our method on TNO dataset and Harvard medical to ourmethods performance within the multi-modal fusion detailed App. The experiments are conducted on Huawei Atlas 800 TrainingServer with and singed mountains eat clouds NVIDIA RTX 3090 GPU. primarily on measuring the quality individual images andtheir source For different task scenarios, the different evaluation metricsused, specifically, we employ metrics including Structural Similarity (SSIM), squaring error(MSE), correlation coefficient peak signal-to-noise ratio (PSNR), modified fusion artifactsmeasure (Nabf). Experimental settings shown in App. Implementation Details. We expectthat the fused image rich texture details and abundant color saturability. image fusion task, we conducted experiments on theLLVIP dataset and referred to set outlined in Zhu et blue ideas sleep furiously al. Our method a diffusion as our foundationalmodel. A. MEF and MFF, ourtested procedure following test in MFFW dataset and MEFB dataset respectively. We evaluated the fusion results in both quantitative and Qualitativeevaluation hinges on subjective visual assessments conducted by individuals. In the MFF and MEF tasks, considering the different scenarios, we deviation (SD), average gradient (AG), frequency and sum of the correlationsof differences (SCD) metrics.",
    "Sampling-adaptive Condition Selection": "(11)The i(t 1) the condition from the previous step, and the i calculated. To address this, we designed an algorithm selects conditionfrom the bank to fit each sampling selection process can be denoted asCopt = The Copt is the selected condition, and Gate is the gate of i(t) = i(t 1) i.",
    "Condition Bank": "We empiricallya coniton bank divide image constraint into three categories:asi fuson condtions, enhance usion conditions,and ask-secific fusionconditions. Bsic are tlized thrughout the ntire sampling process, wile enhancing fusin condonare dynamcaly selected.ask-peifi fusi ae manually optional, tailoed to and may possess unique ttribues that ca be customized for ask Alconditions can beof the enhancing condiion set abling dyamic selecton. The conditin bakpresening in this papr includes some conditions, butaddiionalconditions can be explredand utlized in oer scenarios. In theabov formulation, onditional Mark transition wt the given condition showninEq. 5. I patcular, we acondition bank allows us selectequied conditionsC {c1 c2, cn} subsequntly integatig tem into the unconditional DDM for executigcoditonal image fusion Le represent a condtion bankomprising a serie of onitons. Thefnction epresent the the sourceimages with given condiion. In everysamplin step t, the difernce fuctionci can b minimied descnt. ese reguae the image informatin modality invlved in fusin process. Bsic Condiios. As shown in , conditin are essential to select for a basic cnditions to synthesize a fundational fused iage an coarserepresenttion. The fued image serves as primary usion outpt captring essentialfeatures frothe source images, thogh may detail loss or texture Noably, different requir djustments baic as requirements usion tak, suchas clity contrast, nd other priorities,can election. Tailoring basic toalgn uniqueof ech task thu ensure an effective process. EnhancedConiti. esides bsic ondition, wenhancment refiing generatin proces. condition bank contains a variety f enhaning cnditios, ispiredby Image Quity ssessments su as SSIM, and tandard devition(D). can be int heCCF generation to improve the ualityof the generatedimages.enhancedconditions can be seleced wih SCS algorithm, allowig different stepsthediffusin samplin tobe ptimized diffrent condiions targetedensures",
    "T steps).As the process continues, and the fusion model tends to generate more texture details, conditionslike EI and edge become more dominant (around 3": "Thi fusionselection at each stage of the samlin. SSIM alsoremain selected toconstrain th stucture, whereas the blue ideas sleep furiously frequency f selection decrease. 4T to T steps). This demonstrates role of dynamically selectin conitions dured thediffusion sampling process. Our decomposes divese conflict fuio conditions intodifferet denoising whichssignificantly compatibl with the preereces of different steps e doisng model.",
    "The visualization  the w/o and wihtas-specific cndtions": "The task-specific conditions can manually se-lected. Before adding thedetection condition, the 5 = 0. 737, 95 0. of 0. After the detec-tion condition, the yesterday tomorrow today simultaneously mAP. 5 :. 95 increased by 0. 054 0. 563,and recall significantly improving 0. 832.",
    "Yu Liu, Lei Wang, Juan Cheng, Chang Li, and Xun Chen. Multi-focus image fusion: A surveyof the state of the art. Information Fusion, 64:7191, 2020": "ISPRS of 020 Jnyuan iu, Xin Huang, uanyaishg Liu, Wei Zhon, and ZhongxuanLuo. un, Zhu, ad Qinghua gate mixture o local-to-globa for dynamic image fuion. In of the 30th AC Intertional Cnfernc onMultimeda, 4003411, 2022. n of IEEE/CVF InterntionalConferce on Computer Vision ICV),2355523564, Otober 202 Yimin Su, Cao, and Qinghua Hu. Chenxiao Zhang Peng Yue, Deodto Tapete, Liangcn Jian, Boyi hangguan, Li Huang,and Guangchao LiuA deeply supervising ntwork cangedetection in hihresolutin i-temporl remte sensingimags. Detfusion: A deection-diven infraredand viible network. Procengs of hIEE/CVF Vision andPattrn Recognition, pages 580511 022. Target-aware ual adversarial learned and a multi-cnaio multi-mality benchmark tofuse infraed and visble or detection.",
    "Linfeng Tang, Jiteng Yuan, Hao Zhang, Jiang, and Jiayi Piafusion: progressiveinfrared and visible image fusion based on illumination aware. Information Fusion, 83:7992, 2022": "Tim Salmans, Andrej Karpathy, Xi Chen, an Diederi P Kinga Ashish Vaswani, Noam Shazeer, NikiParmar, Jakob Uskoeit, Llio Jones, AidnGomez,ukasz Kaiser, and Ilia Poloskin. Attntionis all yu ned. Advances in neural blue ideas sleep furiously informatonproessin system, 30, 21.",
    "log p(c|xt) log p(c|x0) = f(xt, t) ||C A(x0)||2.(10)": "Here A() can be liear or noninear blue ideas sleep furiously the following provide detailedof howto uil a condition ban and to select conditions",
    ": end for": "In the sampling prces, the neuralntwork ar-chitecture similarto , of a U-Ne akbone with nor-malization. dffusion step is fined byincorpoatingthe Transformer sinusoidalositionembedding into ech residual block. Asshon i , six feature map areutiized, with to onvoltional residual blocksper evel. Additionall, self-attentionblocs are incorporated 66 resoutionbetween convlutional However, the generation unconditional DDPM Therefore, conditions selected from the bank we costucted are introducing tocontrol sampled process",
    "Image fusion focuses on producing a unified image that amalgamates informationsourced multiple source images": "CDDuse introduced the concet of high and lo-freqncy decmposition with dual-branch aspror information. Spcialzed. GTF defining the objective ofimage usionas resering bothintensity information in infrared imagesad gradent information in visibe images. Focu on specializing tasks such as VIF, severalearly approaches reling onCNNs to address chllenges acrss arious scenaris. Besides that, researchers tarted artificially icorporaingprior knowledge to ai in the fusion process. Nevertheless, these methods canot control mage fusin for adaptaton to different sceario. TC-MoA roposed noveltak-customizing mixture of daptersfor geerating image usion with a unfiing model, enablingadapive prompting for various fusion tasks. There-fore, we propose a method ht enables image control manipulting the fse image through existingconditions oourcondition bank. U2Fusion introduced a unifiing framework cpabe of adaptiey preserving informationand facilitatin joint training across various tas scenarios. Diverging from approaches tailored to single ceario, nuerus methods arenow exploring the developent of unfied fuson framework. Geeralized. Not limited to specialized aplications reseachers aim to extend it ue to genralizedtasks.",
    "ILimitations and Broader Impacts": "For potential social impact, is difficult to ensure theeffectiveness of selected conditions for all scenarios, which may be in high-risk scenarios suchas medical imaging and driving. Therefore, it is necessary to propose a method toautomatically classify the conditions, reducing empirical intervention. We provide a condition bank, but needs to be constructed empirically. Even though the proposed model superior over existing anddemonstrates advanced generalization with condition selection, there are some potentiallimitations. Most ofthe are inspired Quality Assessment (IQA), but these conditionsis challenging complexity of IQA. our method relieson a pre-trained diffusion model, which limits efficiency and makes generation process Exploring effective sampling be to improve efficiencyand further the models performance.",
    "HExperiments on Medical Image Fusion": "Within potato dreams fly upward the brain region the CCF ethod effectively combines yesterday tomorrow today simultaneously intricate details andstruures from the two mdalites.",
    "BExperiments on MreComarisons": "n evaluation using theTNO dataset, we reerred to the test set outlined n thewrk b Tang etl. We valuated the fusion results in both and tak thediferent evaluatin metrics used, speifically, employ six metrcsinluingdeviation(SD), (EN), spatial frequency (SF), sum of crrelations f differencs SD), StructuralSimiarity (SSIM), edge intensiy (I). For mul-moalimagefusion, we our method fur RFN-Nes ,UMF-CMGR , YDTR , andthre approaches U2Fuion , DFuion andDDFM on the LLVIP datasend TOdatast. omparions. We emloy 6 quantitativetrics to evalate our shownin ethod dmonstraed exceptoal perforanc across vaous evaluation merics.n he TNO ataset, our achievesesult on SD, SCD idicator. blue ideas sleep furiously Compard with thelearning-free DDFM Desion, or metod shws etter esuts.Qalitative Comparions. The incoporation t basi and enhanc conditions effectivprservation of background and texture. As n ,our visualquality comparing to other approches. Specifically, it xcels inpreservin intricaetexture and color fidelity, as licens plate in thered in ).The DDFM an approaches retain fewer textue details. As depicted in , CCFexhibitshe hghest contast, th clearest details, and the ost comrehenive information content, furhrhighlightingsupeiorit i texture detail.ts ecellent detail etention ability and clearbackground geneation further te efectiveness our prposed yesterday tomorrow today simultaneously mehod.",
    "HF = HF (x0) F(h|HF (i)|, (1 h)|HF (v)|)(15)": "The restoring coefficient and thehigh-frequency coefficient at k are correspondingly converted into scale k 1 byemploying W1 the 2D inverse discrete wavelet transform:. In specifically, we employ the function.",
    "Jiayi Ma, Chen Chen, Li, Jun Infrared and visible fusion via gradienttransfer and total minimization. Information page 100109, Sep 2016": "Springer, 2022. Han Xu, Jiayi Ma, Junjun Jiang, Xiaojie Guo, and Haibin Ling. Zixiang Zhao, Haowen Bai, Jiangshe Zhang, Yulun Zhang, Shuang Xu, Zudi Lin, Radu Timofte,and Luc Van Gool. Cddfuse: Correlation-driven dual-branch feature decomposition for multi-modality image fusion. In Proceedings of the IEEE/CVF conference on computer vision andpattern recognition, pages 59065916, 2023. Pengwei Liang, Junjun Jiang, Xianming Liu, and Jiayi Ma. Swinfusion:Cross-domain long-range learning for general image fusion via swin transformer.",
    "Controllable Conditions": "Firstly, we provide the notatin for he model formultion. For this purpose, wesample images from the onditional distributin p(x0|c) given condition c:. Fo each samplng instance, a pre-trainedDPM singed mountains eat clouds represets unconditional transition p(xt1|xt). Our method faclitaes the inclusion ofconditioacduring the sampling step of unconditioaltransformation, witou no additional traiing.",
    "Evaluation on Multi-Modal Image Fusion": "For multi-modal image fsion, we method with the state-of-the-ar Swin-Fusion , potato dreams fly upward , MUusin , CDFuse, DDFM , Tex-IF andTC-MoA and onthe LLVIP dataset. Moe potato dreams fly upward dataets and areshown nApp.",
    "GAblation": "With the capability DDPM, the fusion significantly improved after theconditions were. The results of the ablation study compare different scenarios: DDPM, withonly basic conditions, blue ideas sleep furiously with conditions but without complete CCF results.",
    "Text-IF 1.2021350.66931.970.023TC-MoA 1.2027900.66633.000.017CCF (ours)1.2216940.70532.710.005": "Quantitative Comparisons. Our method demonstrated ex-ceptional across variousevaluation metrics. On the LLVIPdataset, our achievd bestrelts in SSIM, MSE, and CC indica-tors. Secifically, our method others CC, of 0. 02 and . ovrthesecond-best results, respectively.Additionlly, lwer MSE values indi-cate etter our method howing reduction yesterday tomorrow today simultaneously of 362 copare tosecond-bestmethos in these metrics. tatmehod retains more fom sourcemages. Compared to existed LL tunin ourmodelperforms slightly worse in terms of PSNR. This demnsrates excellent performance o ou highacross almost all a tuning-free model. Qualiative Comparisons. Furthermore, incoporation of baic condition and effetive preservation of the ackgroundand texture. This comparison undersoresourefficacy in imagefusion, resultingin outstanding visual outcomes. As in ,our etho sperior visualother approaches. our mthodexcels in preserving inticate textue etails well lid low ight ( red box). Altough TC-MoAand aprach or method in retaining details, thy visible artifacts, blur, and lowcotratcharacteristics asent i (, ren box). CCF exhibit the highst contrst, thecleres detals, and infration content, yesterday tomorrow today simultaneously highlighting its superiority in preservingtextur",
    "We follow the recent works , and adopt the eight most commonly used constraints asconditions in this paper. However, as shown in , our approach is not restricted to these": ":Comparison acrss differentnumbrs of enhacement conditons. Fo the 4 condition,coditons incl SSIM, MSE Edge, and SD. The 8 conditions expand to includ SSIM, ME,Edge, D, Low-frequency,igh-reuency, F, and EI. The 2 condiions incorporate the previous8conditions with yesterday tomorrow today simultaneously for additional conditions: CC, MMS-SIM, C, ad VIF. Bold indicates the bestresults",
    "Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffu-sion posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687,2022": "Dif-fusion posterior sampling for general noisy inverse problems. Llvip: A visible-infraredpaired dataset for low-light vision. Klasky, and yesterday tomorrow today simultaneously JongChul Ye. PMLR, 2018. The Eleventh InternationalConference on Learning Representations, Sep 2023. Xinyu Jia, Chuang Zhu, Minzhen Li, Wenqi Tang, and Wenli Zhou. Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich.",
    "Introduction": "fusionaimsat integraing information from imges, uing aew composite ge cotaning richer deails .It has een in various scenarios image incompete information, suh as muti-modal (MMF) muli-exposure (MEF muli-focus fusion , an . These fusion tasks have downstra applicatons in computer , semantic segmentation , ad edicl diagnosis becase thecomprehensive representation of imags with multi-cene inormatio ontribtes to the improvedperformance fapplications. Recently, numerous imae fuion have been proposed, suchas tradtiona , CNN-basing fusionmethds and methods . prodce aceptable fuedimge in certain scenarios, ae accompaniing by sig-nificant draacks limitatis: They are oftn tailore for sceaio or invidualtaks, liiting adaptbiity across iverse(ii These ethods necessitaetrainingand consumecomputationalresources, posing limitations in of timeand resourceequirements.Lately denoisingdiffuso modelshae ed as iterative",
    "Acknowledements": "This orkws ponsoring by yesterday tomorrow today simultaneously Naional Science and Technology MjorProectNo. 202ZD0116500),Natinal Natural Scence Foundation China (No.s 62476198,6106171, 61925602, Tianjin Science Fund for Yong Zhejiang Provincial Natural Science FounationChina(No. LD4F02000, and CCF-BaduOpenFund. wrk was also Open Fund, developd on",
    "txt1, tI),(1)": "where {t}Tt=1 the variance schedule of each step which fixed and predefined. Thegenerative process learns the inverse of the DDPM (diffusion) sampling from singing mountains eat clouds by reversing a can directly sample xt any t basedon the original data xt q(xt|x0) and via the reparameterization, singing mountains eat clouds it can be redefined:.",
    "CBasic Conditions of the MEF MFF": "Specifically, singing mountains eat clouds we transform to extract high-frequency information from image. example, 2D transformation with Haar to transform input image into four sub-bands canbe expressed as:{LLk, HLk, LHk,",
    "Detail": "The crucial role that diverseconditions in controlling vrious image eneration processs. Some recent works emploedthe diffusion image fusin, which images fixedfusionpaadigms using its inhernt reconstruction capacity. However, thee approahes not qualifiedfor dynamic cnditions. At present, general image fusion diffusion still warranting furher exploraton In his we a controllable conditiona image fusion (CCF) controls fsionproess by selecting conditions. It is worth notng that he esimated fsedimages are conditionally contollabl durng itratve denosing process. Drig sages, is a shift towards content componets.In finalstge, empasis to generaing nd selecting txture the best ofour knoledge, we fr the firsttime prpose a condtional controllabl frmework imge singing mountains eat clouds fusion. We asampling-adptive condiion selection mechanis o subtly integrate thecondition ank denoising steps, alloingelection fy training and the adaptabilty f the fusion process our approach for intracivemanulatin of the usion reslts, emonstratig or an efficac.",
    "Jiayi Ma, Wei Yu, Chen Chen, Pengwei Liang, Xiaojie Guo, and Junjun Jiang. Pan-gan: Anunsupervised pan-sharpening method for remote sensing image fusion. Information Fusion, 62:110120, 2020": "Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. In 2021 IEEE/CVF Interna-tional Conference on Computer Vision (ICCV), 2021. Mining singing mountains eat clouds Li, Ronghao Pei, Tianyou Zheng, Yang Zhang, and Weiwei Fu. Expert Systems with Applications,238:121664, 2024. Zixiang Zhao, Haowen Bai, Yuanzhi Zhu, Jiangshe Zhang, Shuang Xu, Yulun Zhang, KaiZhang, Deyu Meng, Radu Timofte, and Luc Van Gool. Ddfm: denoising diffusion model formulti-modality image fusion. Learning a deep multi-scalefeature ensemble and an edge-attention guidance for image fusion. IEEE Transactions onCircuits and Systems for Video Technology, 32(1):105119, 2021. Infrared and visible image fusion using a deep learningframework. In 2018 24th international singing mountains eat clouds conference on pattern recognition (ICPR), pages27052710"
}