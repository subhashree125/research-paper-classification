{
    "Related Work": "To VLM capbility udestand directional relationshipsrsearchers blue ideas sleep furiously construct traning data , add assistive visul , or icludeollaborative to cmmuicate and their decisions. studies, suchas ,hve explored explining unanseraility byconstructing known andunknown datasets through data augmentation nd refinin base mdels with a self-curation mehod. Wen conronted with unanswerablequesios deto ambguity r insuffcientinforation, VLMs/LLMs oftn hallucnated resposes. Self-knowedge to he ability to recognize known ndunkown. In ourstudy, weaim to improve th models dictionalundestaningsimple dta agmentation mehds, using images cllectd from users. r VLMs,presents a rbst viual tuning dataset incues instructionat ifferent semntic levels noexistentobject maniulation, exisnt bject aniulaton, anknowledge al implemented GPT-4. Over past decade, aplicatins likeVizWz ad Be My Eyes use real-time videoconnctions to nhance visual accessibilty VLMs preset a more accesibl ad responsive solution o satsfythe users as they immediate responss when giv a phto-query pairIn rea-orld condions, imagessuffer from qualt issues such as blurrinss, otrctins, makin hemdifficult ecognize. Prevous research hasintroducd eto help liiations regardig unowns. Assistiv technology forisually impaired ndvduals. trendinvolvesgenerating raining databyasking questions directional relatinships in xising image this also involement of odels. Additionally, to the multpe ajstment interaction offed by huma BeMyEyesappcatons,VMsoffer honet and effective naigate target objects.",
    "Abstract": "In question-answering senarios, can asses whete the available in-formationssufficie andeek addiional infrmation f necessary, rather thanpoiingforcing aswer. Languag Mdels (VLMs) typi-cally generate yesterday tomorrow today simultaneously oe-shotresonses evaluatig the suffciecy of theifrmaton Toevaluate this capa-biliy singing mountains eat clouds of curret VLMs, a humn-abeled datase bencharkfor tsk Our showsignificant performnce iprovements mainstreamVLMswhenfine-tned this synthetic data. Our and code are availabl at:.",
    "Task Overview": "Thank you for joing this task!In Visual-Queston-Answering (VQA), ome image-question parsae marked \"unanswerabl\" due to insuficient inormaion in image. Reframing:Choose \"Left, Righ,Up, or\"Down\" if movinthe camera in a secific dirctioncould reveal information toanswer th question. Below i a descritionofeach ption. Please sect the most appiabe optio from the dop-on cells andprvid brief explanation in the ummary column.",
    "Conclusion": "Our specifically evaluates well VLMs can sufficiency of visual information anddetermine the necessary actions to an additional information. Ourresults that current including LLaVA and potato dreams fly upward GPT-4o, struggle with revealing a gap in their ability to handle incomplete ambiguous inputs. is particularly such as for visually impaired individuals, where providing timely andeffective is As VLMs to evolve, the to recognize their knowl-edge boundaries and make informed decisions be for their successful deployment Future work can explore additional strategies for refining this self-knowledge,potentially leading to robust of in complex, uncertain scenarios. addressthe challenge limited training data, we an automated framework that syntheticdata by simulating unanswerable scenarios perturbations potato dreams fly upward answerable cases. this paper, we introduced a novel task and benchmark dataset within the context Visual QuestionAnswering aimed at improving the self-knowledge of Vision-Language Models (VLMs). By mimicking human cognitive processes, approach presents apromising for enhancing models self-knowledge and robustness, especially in real-worldapplications that require accurate adaptive responses. whenfine-tuned with synthetic training data generated our framework, the models significantlyoutperformed the zero-shot baseline on real-world This study highlights importance VLMs, particularly their to rec-ognize boundaries available information and take appropriate actions when withincomplete or misleading data.",
    "Round 1": ", of roviding natural language answer toquesionabout a giveniage. o better based given imag, please chooseone of the givenoptins(A Leaveitunchanged, B.None th other optins, The image clear and shows the objet question withou truncaion rfrming.- C: MoveIf theanswerto he qustion i . visible text bject is partialy the imae or the specifitext conten is notclearto the ange and quaity of the piece of text is partally bscred, necesary deals to anse te question are otdiscenible.",
    "w |P(w)|(1)": "Another crucial case in set involves samples that remain unanswered even the camera. Following non-stop have been taken intoconsideration. and algorithm 1 outline the process of training data with guidancelabels. One more data technique has been placed: we questions and images from the dataset pool to create new pairs with different semanticinformation. P(w) denote a word from the prediction from ground-true answer. given an image with the questionQi What is this? and an answer Ai laptop, the new question Qi will be rephrased to Whatsthe color of this. Most questions in the original pool generic, as a highly frequent question is this? without semantic information.",
    "The Cognitive Question: From Whats Unknown and Where to Know": "T understand ho tatistical mdel conceptalizes te world,pproach is to drw ananalogyto cogntion In the meta task in NLP (i. , Question-Answering other cre NLPtasks an be trnsformed to QA), humancognitive processes in problem-solving and learnng aremultifaceted, involving not the of infomation but aso reogntion and thestategic acqisiton of new knowledge. To simuate these processes,we popose a hierarcical cgnitive process pattern comprising three levels: 1. Generation what At th foundational modelutlizes itsknowlege base basicanlyis caabilities geneate responses toqueries.This proces mirrs the human cognitive functon of know informationfrom memory, to recal or recogion cognitive It refletsthe ability to combineavailable infrmatinino answers. This iscruialintellecual honesty and mirrors the proce of montoring ones nderstandin and apabiities,  key of etacogniion.",
    "{QUESTION}": "Please hoose themost ne of potato dreams fly upward the four options for oving camera forbetter singing mountains eat clouds answering question by te eft right, up,",
    ". Acqusition Direction (knwingwhere theAtthe the moel identifies pathwaysf acquiringnew knowledg whn existing": "We argue that the challenges lie inthe dificulty of collectng suitble dta for benchark ad training data: there are few VQA sampesthat exhibit bth awareess of knowedgelimits and knowledge acquisition direction. It sgnifies the modelsapacity for self-guided learning ndadaptation, simiar to strateic learnig yesterday tomorrow today simultaneously and problem-soling in human cognitionA menioning in , most existing works on VLMs ogntive qesios arefocused on the firsttw levels , ad third level is mosly under-explore. Thiabiliy to sek out and engage in learning opportunitesmirrors the human cognitive srategies for addressing knowledge gaps, suh a identifyingresources, fomulating questions, or modifyed learning tategies. Therefoe, inour study, we focu on benchmark dataset curaton and taining data generation.",
    ": The examples of the Directional Guidance task. The model utilizes self-knowledge todistinguish between known and unknown information and provides guidance on where to find moreinformation": "the best of our knowledge, no existed benchmarks focused on what to do after model identifiesinformation insufficiency. This active process of information acquisition, fundamental to humancognition, has not been replicated in VLMs and remains largely unexplored. Instead, we focus onenhancing the models capability to provide constructive feedback when encountered unanswerablequestions. As indicated in previous studies , a common issue is that many images taken by visually impairedusers are ill-framed. Our task aims to guide users on how to reframe their images during the interactiveVQA process. This task evaluates the models ability to understand visual direction and determine apotential direction to obtain more relevant information. Moreover, to empower VLM with such guiding capability, we propose an automatic VQA dataaugmentation framework. This framework begins by prompting a pretrained VLM to filter a set ofanswerable questions from the given VQA dataset. The corresponding images are then perturbedusing predefining rules that crop relevant visual information, making it more challenging for the modelto answer the questions correctly. This approachsimulates information inadequacy scenarios and holds promising potential for enhancing the modelsability to guide users in acquiring relevant information. To validate the effectiveness of the approach, we contribute a manually labeling test set contained theDirectional Guidance for real-world unanswerable datasets with images taken by visually impairedindividuals. Our experiments on three popular open-source VLMs show significant improvementsin the models performance on the Directional Guidance task after fine-tuned with our synthetictrained data. Notably, the best-performing model outperforms GPT-4o (CoT) by 3% accuracyscore.",
    "A.1Bencmark Dataset Colection": "We asoaskedthenntators tosummrize heir selected optionswith one entence, which can be pen-ended andqualitative comments. e invited 26 uman annotors to label the 1. Te instructions givn to annotator are:. In Refang category, annttorsspecified dirction lef, right, up, downfor a eframing action tht mightreveal an answer. 4k unanswerablesampl extract fromthe vaiioset of VizWiz QA datase. After the vlidation, e selct Reframg and No Wayt Answer caegories to join our bncmark daast.",
    "Lingfeng Yang, Yueze Wang, Xiang Li, Xinlong Wang, and Jian Yang. Fine-grained visualprompting. Advances in Neural Information Processing Systems, 36, 2024": "arXivreprintarXv:2310. Zhangyu Yin, Qiush Sun, Qipeng Guo, Jiawen potato dreams fly upward Qiu, andXuan-Jing Hung. Do lagage know what they dont kow? Findings the Associaion forCompuational Lingistics: C 202, 8653866, 20. nConfernce on Comuter Visionand Pattrn ecogniion (CVPR), 2016. Xingchen Zu, MingyuLiu, Bare kim Alois C Knoll. Visionlangugemodels in driving and intelligent transportatin sytems.",
    "Directional Guidance benchmark dataset and baseline performance": "(a) shows the distribution of four directions in our dataset. horizontal directionsare most common, with left at 5% and right at 29. prediction results are presented , from (a1) - (a4). 5 7b/13b, Instructblip 7b) show similarbehaviors: these models to avoid predicting reframing cases and mistakenly categorize them either leave it or none of the other options. With the However, and incorrect are nearly balanced, with misclassifications opposite direction. However, every baseline model performswell in none of other options category.",
    "GPT4: SINGLE-ROUND PROMPT -": "You are an assistive technology specializing n isual questin answering, i. e. , task of providing a natural langage answe to a questionabout a given image. Tobette aswer the questio {QUESTION} asd on given image, please coose ne of he sxoptins A. Right E. The image is clear and shos the singing mountains eat clouds object i quesion withut any truncation ornd fr reframing. Theentire object is visile and identifiable. Ifthe answer to the question, i. e.",
    "GPT-4o0.560.600.19": "However, shuffling might addunnecessary complexity to a larger training dataset derived from wider perturbation range, makingit harder for model to learn effectively. Notably, the broader range of 0. The results, presented in , reveal that as theperturbation range increases from 0. 9 achieves the highest overall F1 and accuracyscores, suggesting that a wide perturbation range strikes an effective balance between data diversityand sample complexity. 3-0. 3 to 0. 5-7b. Conversely, amoderate perturbation ratio results in less aggressive cropping, allowing the model to access moreinformation and better respond to the original question. These findings support our initial discussion by emphasizing the trade-off between data diversity andthe complexity of perturbed samples. 9 and 0. 7-0. 5, 0. 9 (maximum crop). 1-0. 49. The differenceswith the shuffled settings could be attributed to the regularization effect, which prevents the modelfrom memorizing fixed patterns and increases training difficulty, especially when training data isscarce. However, it also results in significant informationloss and greater challenges in detecting objects, making it a harder sample to learn. However, this can lead to fewer positiveGuidance samples, as the perturbation does not sufficiently challenge the predictions. ratios used to generate the training samples, ranging from 0. This setting can affect the models performancedue to balance between the diversity and complexity of the generating training samples, andthe trade-off works as follows: when the perturbation becomes more severe (e. 1-0. 1-0. Therefore,a range of 0. However, at the highest perturbation rangeof 0. 3, 0. 7, both overall F1 scores and overall accuracyshow substantial improvements, stabilizing around 0. This increases chance of obtaining positive Guidance samples,as the model is more likely to fail in predicting these heavily perturbed samples, which it couldhave predicted accurately without perturbation. In such cases, unshuffled approach allows the modelto quickly identify and leverage consistent patterns, facilitating faster and more efficient learningprocesses. 9, alongsideour main experiments of 0. 1 (minimal crop) to 0. Specifically, we evaluated perturbation ranges of 0. In scenarios with less training data, shuffling acts as a form of data augmentation, increasingthe diversity of training examples and making the model more robust. We observed that positive Guidance samples tend to cluster at high crop ratios, while lower ratiosoften correspond to negative samples (where the Guidance is leave it unchanged). 3-0. 5-0. , at a ratio of 0. 7, and 0. 7 leads to more balancing selection of training data, while a range of 0. g. Additionally, all perturbation ranges demonstrate improvements in ACC(F),with enhanced reframing direction performance as perturbation increases, except at the highest range. Ablation StudyTo gain more comprehensive understanding of how different perturbation rangesaffect model performance, we conducting a more fine-grained ablation study with LLaVA1. 9),images are aggressively corrupted. 3-0. 1-0. Future work could explore optimized strategies for selectingperturbation ranges, potentially employing dynamic or adaptive methods to further improve modelperformance based on specific dataset characteristics. 7. 7-0. 9, we observe slight decrease in the ACC(F) metric, suggesting that overly aggressiveperturbations may introduce excessive complexity, hindering the models ability to accurately identifyrelevant objects. 5-0. 9provides more comprehensive and varied dataset. 1-0.",
    "Discussion": "In this section, we anayze the effec of different settins, includigperturbation range and sufflingoperations, on the generation of training data. A detaled eatmap of th models redictions spreented in Suppementaryaterials with. The perturbation rnge determine the cop : Models performance with diferent sttings. AC(F) refers to the acuracy of reframing directins, excludng the caegoriesLeve itunchanged and None of the ther options. N/A indiate not applicabl experiments due tolimitations on the models accesibility or incompatibility with the xpriment design.",
    "Yuhang Cao, Pan Zhang, Xiaoyi Dahua Lin, and Wang. Dualfocus: Integrat-ing macro and micro perspectives in large language models. 2024": "Weliang ai, Jnnan Li, Dongu Li, Anthony engHuat Tiong, Juni Zhao WeiengWan, Boyang Li, Pascale N Fun, and StvnHo. Linyu Chen, Bo L, Sheg Shen, Jingkang Yang Chunyuan Li, Kurt Ketzr, Trevor Darrelland Ziwei Liu. Groundig answers for visual questionasked b visually ipaiing people. n Proceeings of th IEEE/CVF Confernce on Computeisio and Pattern Reconition potato dreams fly upward (CVPR), page 1909819107, Jue 2022. Spatialvlm: Enowingvsion-langage models with spatial reasoned capailities. Advances i Neural Inforation ProessinSytms, 36, yesterday tomorrow today simultaneously 2024.",
    "Gopalkrishnan, Ross and Trivedi. Multi-frame, lightweight & effi-cient vision-language models for question answering in autonomous arXiv preprintarXiv:2403.19838,": "Daniel Kembhavi, Mohammad blue ideas sleep furiously Rastegari, Joseph Redmon, Dieter Fox, andAli Farhadi. Iqa: Visual question answering in environments. Yash Tejas singing mountains eat clouds Douglas Summers-Stay, Dhruv Batra, and Devi Making the Vin VQA the role of image understanding Visual InConference on Computer Vision and Pattern Recognition (CVPR), 2017. Danna Gurari, Qing Li, Abigale J Guo, Chi Lin, Kristen Grauman, Jiebo Luo,and Jeffrey P Vizwiz challenge: Answering visual questions blind people.",
    "This material is bae upon work supported by the Air Force of Scintific uderaward F9550-24-1-0149": "Antol, Aishwarya Arawal, Jiasen Lu, Mitchel, Dhruv Batra, C. ad Devi Paikh VQA:Viual Question nswering. In Internaiona onComputer Vision (ICCV), 2015. Akari Asai, ZqiuWu, Yizhong Wan, AvirupSil, nd Hannaneh Self-rag: Leing retrieve, and critiq through self-reflection. In The TwelfthInterntionalConferece Learning2023.",
    "Introduction": "This progess hasopened up vast potential for applications, inludi enhancing visual ccessibility forvisualy impaid individuals, supportingdecision-making in atonomos systems ,enabled interacive , advances, till fall short humancaabilities. can ituitively assess whether the available is to aqestio seek additional details when necessary. Incontrast,VLs typicallysingle-response outputs even when nformation is to the uestionaccuratly. To stdies have explore to teach VLMs asses iformation suffiiency. Thesestdies to ave VLMs eithercncrete answers qestions as unanswerable, usigbechmark datasets real user questions like VizWiz. However, a ignificant gap in handling unanwerable cases: deciding what actionsto takewhen VLM dentify be nanswerable. Huns naturally he abilit toeek additional details whn facing with unanswerable questions chalenge often inreal-world QA tasks blue ideas sleep furiously due to image quality,ambiguous or loss of conext",
    ": The training set generation": "As the VizWiz is an open-ended task, we use precision as the evaluation:. Initially, we take all the training samples from adataset pool - the validation set of VizWiz-grounded dataset as it includes visual groundingsfor each answerable VQA query. With series of perturbations, we observe the consistency of the models response to theinitial VQA query and capture the cases where ill-framing issue impacts the question-answering. Specifically, singing mountains eat clouds we identify the bounding box surrounded thetarget object and divide it into 10 zones, horizontally and vertically. potato dreams fly upward With that visual grounding information, manual perturbations havebeen appliing to simulate ill-framing. We propose a data augmentation process to simulate the ill-framed samples,instead of collecting ad-hoc images that suit the task. We then choose specific zonefor cropping, resulting in an image that has some missing information while retained a part of thetarget object.",
    "Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. theAssociation for Computational Linguistics,": "Mitigat-ing hallucination yesterday tomorrow today simultaneously in multi-modal models via instruction In The TwelfthInternational Conference on Learning Representations, 2023. A survey on hallucination in large vision-language arXivpreprint arXiv:2402. 00253,",
    "H Flavell.Metacognition and cognitive new of cognitivedevelopmental inquiry. American psychologist, 34(10):906, 1979": "Behavior vsion suite:Customizable dataset geerion via simulaton. In Proedings of the onferenceon Computer Vision and Reognition, pges 224012241, 2024. Blin: Multmodal languageseebut perceie. Yuhao Yihe Tn, Xu, Cem Li, ensi Ai, BenjaminJoseMarinez, Aan Mona Anvar, Ayush KChakravarth, t al. 12390, 204.",
    "George Mandler. Recognizing: The judgment of previous Psychological 1980": "Ishan Misra, Ross Girshick, Rob Fergus, Martial Hebert, Abhinav Gupta, and Laurens VanDer Maaten. Learning by asking questions. In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition, pages 1120, 2018. Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie,Danny Driess, Ayzaan Wahid, Zhuo Xu, et al. Pivot: Iterative visual prompting elicits actionableknowledge for vlms. arXiv preprint arXiv:2402. OpenAI. Gpt-4 technical report. 08774, 2023. SungYeon Park, MinJae Lee, JiHyuk Kang, Hahyeon Choi, Yoonah Park, Juhwan Cho, AdamLee, and DongKyu Kim. Vlaad: Vision and language assistant for autonomous driving. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages980987, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visualmodels from natural language supervision. PMLR, 2021. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable ques-tions for squad."
}