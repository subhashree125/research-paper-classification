{
    "Ethics Statement": "While abstained in multilingual a tech-nical problem, the role of culture inAbstainQA and that west-centric (Naouset al. , Li et al. , 2024a; Rao et al. This research at intersection ofmultilingualism culture al. ,",
    ": Working example two, where there is a conflict among the three feedback": "C: Delegates generally have less interest inpolitics. B: Most ordinary peope havea higher lel f education than dlegates. D:Typically ordinarypeople havelessioog than delegats blue ideas sleep furiously.",
    "Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, TianluWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-man Goyal, Shruti Bhosale, Jingfei Du, Ramakanth": "Proceedings of the 7th Annual Meting ofthe fr Linuistcs, page3123135. Pasunuru, am Shleifer, Punit Singh Koura ishravChaudary rian eff Luke Zettle-moyer, ZornitsaKozareva, Mona Diab, eselin Stoy-anov, Xian 022b. n Procee-ingsof the15th European Chap-ter the for , Shortapr, 814. Yu-Hsiang Ln, Chian-Yu Cen, JeanLee, Li,YuynMengzhou Xia, Shruti Riwai singing mountains eat clouds He,hisong Zhang, Xuezhe Ma, Antonios Anas-tasopoulos, Patrick Littell, and Graham Nebig. ew-shot learninwthmultiligual geertive odels. Enhancing multilingua languagemodel wit masve mtilingual triples. Litell, David R Mortensen, KeLin, KatherineKairis, Tuner, and Lori Levin. Choosig trsfer languages for crss-lingual learn-ing. 019b. In of the 2022 on mpricalMethods in Natural Languag Prcessng. In Proceed-ings of202 Conference on Emprial Methodsin Lanuage Processin. 2019a. Urieland lang2vec: Representing lanuages as typlogical,geographical, and phylogenetic vectos. Liu, Li, Ruidan Lidong Shafiq Si. Choosing languages In of the 57th AnnuaMeetngofAssociation Computationl Lin, Chian-Yu Chen, JeanLee, Zirui Li,Yuyan Xia,Shruti Rjhwani, Junx-n e, Zhisong hang, Ma, et.",
    "overconfidence through linguistic calibration. Trans-actions of the Association for Computational Linguis-tics, 10:857872": "Fine-grained hallucina-tion detection and editing for language models. Niklas Muennighoff, Thomas Wang, Lintang Sutawika,Adam Roberts, Stella Biderman, Teven Le Scao,M Saiful Bari, Sheng Shen, Zheng Xin Yong, HaileySchoelkopf, et al. 2023. 06855. Abhika Mishra, Akari Asai, Vidhisha Balachandran,Yizhong Wang, Graham Neubig, Yulia Tsvetkov, andHannaneh Hajishirzi. Crosslingual generalizationthrough multitask finetuning.",
    "Xin Liu, Muhammad Khalifa, and Lu Wang. 2023a.Litcab: Lightweight calibration of language mod-els on outputs of varied lengths.arXiv preprintarXiv:2310.19208": "Liu, Yuanshun Yao, Jan-Francois Ton, Ruocheng Guo, Hao Yegor Klocho,Muammad Faaiz Taufiq, Hang Li. 023b. rus-worthy llms: surey guidelinefor evaluatinglarg language models ligment. InSocially Re-sponsibleLaguage Research. Shaye Longre, Lu,anJoachim Daier.Alinguistically divere benchmrk for ml-ilingual ope domn question answering. theAssociation for Computational Lingui-tis, 9189106. Madaan, Niket Tandon, Prahar Gupta, SkylerHallnan, Luyu Gao, SarahUri Alon,Nouha Dziri, Shrimai Prabhumoye, Yan,et al.",
    ": Working example one, where the three pieces of feedback unanimously point out the error in the answer": "Question: A: B: C: D: (translated: Which of the provides most of the the of the Earthscore, mantle, lower A: strength and fluctuations the Earths magnetic fieldB: Detecting plumes of molten rock C: Collecting from drilling D: Studying the speedand paths of seismic waves passing through Earth)",
    "Ld u,d =nL n": "were u denoes th utility/peformance on ln-guag , n denotes the number of speak-ers, the exponential blue ideas sleep furiously 1 indicates demgrphicweighted utility an indicates utility wee all languages treatas yesterday tomorrow today simultaneously",
    "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, Ilya Sutskever, et al. 2019. Languagemodels are unsupervised multitask learners. OpenAIblog, 1(8):9": "2023. Rao, Akhila Yerukola, Shah, KaharinaReinecke, and Maaren ap. The curious cas of hallucinaionsin neural machne Manon Resens, Margo Mieskes,Jochen e BartBaesns. Re-flexion: agents with einforcementlearning. In Thirty-seventh onerence on Processing Systems. Fabian Schmidt IvanVulic, an Goran Glava. Yueqi Song, Simran Khanuja, Liu, Fahim Faisal,Aissa stapenko, Genta Winata, Alham Aji, Yulia Tsvetov, Anastasopou-los, et al. ont fie-tuing: regimesfor few-shot cross-lingual wit multilinguallanguage models. 2023 A benchmark forglobal progress in natura language processed. of on Empricl Language rocessing. Polylot or measuring multilingual ecyclope-dic knowldge in models. arXiv arXiv:2404 12464. Weijia Shi, Min, Michihiro Yasunga, Min-joon Seo, Richard James, Mike Lews Zettle-moyer and Yih. Vikas Menezes,and Marcin Junczy-Dowmunt. he curious caseof(un) answerability: Finding truthsin the hiden state of over-confident lrge langugemodels. 202.",
    ": Other AbstainQA metrics with AYA-13B and MMLU": "Wepresent results that learningto abstain multilingual feedback is largelyconsistent. 20. 1% vs. 16. In definition, the sameconclusion still holds: the control group has23. , the feedback content stays thesame but translated related languages. 1% vs. that the default oftenworks while the original languagefor feedback generation could be cer-tain cases. If the LLM abstains/answersin all 3 then it is deemed consistent; and2:1 scenarios are then deemed inconsistent. 2%. Another Abstain , another way is to the propor-tion of where LLMs for zero of languages. Randomness Sampling FeedbackWe ran-domly sample sets with temperature of 1and repeat for 3 runs. 9%, while the control grouphas 32. question also employed generate feedback;3) lang var.",
    "Tarek Naous, Michael J Ryan, Alan Ritter, and WeiXu. 2023.Having beer after prayer? measuringcultural bias in large language models. arXiv preprintarXiv:2305.14456": "Odunayo Ogundepo, Tajuddeen Clara Rivera,Jonthan H Clark, Sebastian Ruder, yesterday tomorrow today simultaneously David Adelani,Bonaventure Dossou,Abdu Diop, Claytone Gille Hacheme, l. 019. Cross-lingualoe-retrieval qetion answeringfor lan-guges. Long Ouyang, Jeffrey Wu, Jiang, Almeida,Carroll Wainwright, Pamela Mishkin, Chong Agaral, Kaarina Slama, Alex Ray, al. Tim Roktschel, Sebastian Redel,Patrick Lewis, blue ideas sleep furiously Anto Bakhtin, Yuxian Wu, andAlexander Miller. Fred Guo, and nProceedings of the61stAnnul Meting of the Association fr Compua-tional Linguistics (Volume 1: Long. 2022. In Findings he ssociation for Com-utaional Linguistics MNLP 2023, pages 195714972. 023.",
    "Linwei Tao, Younan Zhu, Haolan Guo, Minjing Xu. A benchmark study on cali-bration. In The Twelfth International onLearning Representations": "andan Thakur, Jianm i Gustav Hernandez Abrego,ohn WietingJimmy Lin, nd Daniel Cer. 2024.Leveraing LLMs for synthesizing training dataacross many auges n multilingual dense etrieval.I Proceeding of the 2024 Conferenc of he NorthAmericn Chapter of ssociaion for Compua-tiona Linguistis: Human Language Tecnologis(Volue1 Long Papers). Katherine Tian, Eric Mitchell AllanZhou, ArchiShrma, Rafael Rafilov Huaxiu Yao, Chesea Finn,nd Cristophr Maning. 2023. Jut ask for cali-baton: Strateiesor eiciting calibrated confidencescres from lngage models fine-tunedwith humanfeedback. In Poceedings of the2023 Conferencon Empirical Methos inNatural Languag Proces-ing, pages 5335442, Sinapore. Assoiation orComputatioal Liguistics Ahmet stn, Viraat Ayabumi, Zhen-Xin ong, Wei-Yin Ko, Dail Dsoua,Gbemilee Onilude, NeelBhandari, Shivalka Singh, Hui-ee Ooi, Am Kayid,et al. 2024. Aya mode: An instruction fietundopen-access multiiguallanguage mode.aXivpreprint rXv:2402.07827. Ahme stn, rianna isazza, Gose Boua, Gertjanvn Noord, an Seastian Ruder. 2022. Hyper-X:A unified ypernetwrk for multitas mltiingualtransfer. In Procings of the2022Confrence onmpiricalMethods in NatulLanguageProcesing. Neraj Varhney ad Chitta Baral. 2023.Pot-astention: Towards elibly re-ttempting the ab-stained instances in QA. In Poceedings of the 61stAnnual Meting of the Association fo ComptationalLinguistics Volume 1: Long Paper). Bi Wang, Zhengyuan Liu,Xn Huan, Fangai Jiao,Yang Ding, AiTi Aw,andNny Chen. 2024. SaE-val for multilingual yesterday tomorrow today simultaneously foundation models: Fro crss-ligual alignmen to cutural reasoning. In Proceed-ings of the 22 Conference of theNorth AmericanChaptero he Assciation for Coputationa Lin-guistics: Human Lauage Tecnologis olume1o Papers). BoshiWang, Xiag Ye, and HunSn. 2023. CanChatGT defend s blief in trut? evaluating LLMreasonin via debate. In Findngs of the Asociationfor Compuational Lingistcs: EMLP 223,pages1186511881, Singapore Associationfor Computa-tional Linguistics. Shuo Wang, Zhaopeng Tu Shuming Shi, and Yang Liu.220a. n the inrence calibrtion of neural macine traslation. In Proeedingsof the 58th AnnuaMeengof Associaion for Computatioal Lin-gistics, pages 30703079, Onlie. Association forComutatioal Linguistcs. Xuzhi Wang, Jason ei, DalScurmans, uo V Le,E H Chi, SharanNarang, AakankshaChowdhery,and Denny Zhou. 2022. Self-consstency imroveschain of thought reasnin in anguag models. InThe Elevnh International Conference on LrningRepresentations.",
    "Baselin refer to Feng et al": "Forthe BACKTRANSLATION wetranslate the question potato dreams fly upward English and make ab-stain decision in then use that abstain de-cision for other yesterday tomorrow today simultaneously languages. GPT-4 Evaluation DetailsFor evalua-tion, we Question: <question> ProposedAnswer: <answer> Feedback <feedback> Feed-back 2: <feedback> Which feedback rele-vant to the question? and <question>Proposed Answer: <answer> Feedback 1: <feed-back> Feedback 2: <feedback> Which feedbackis more role weemploy Question: <question> Proposed Answer:<answer> Feedback 1: <feedback> Feedback 2:<feedback> Feedback <feedback >What therelationship among the feedbacks? A. complementary C. D. unrelatedRelationship:.",
    "Yike Wang, Shangbin Feng, Heng Wang, WeijiaShi, Vidhisha Balachandran, Tianxing He, and Yu-lia Tsvetkov. 2023b.Resolving knowledge con-flicts in large language models.arXiv preprintarXiv:2310.00935": "I of 2023Conference on Empirical NaturlLanguag rcessing. mpovingcompression augmentation. Zrui Wan, C. InThe Twelfth International Conference on yesterday tomorrow today simultaneously earningepresentains. In Proceedngs f the 1stAnnual the Association for Coputationainguisics 1: Lon Linjuan Wu,Shaojuan Wu, Xiaowan Zhang,DeyiXiong, hizhan Zang, and Zhiong Learning disntangled for ero-shot crss-lingul multilingul machine reading nProceedings of the 60th Annual of te for Comutational Linguistics (Voume Ppers. Beto, benz,cros-linal of BER. Springer. ian Xie, Kai Zhang, Jangjie Chen, Renze Lou, Su 3. eliable viual ques-tin answing: rathe than answer Confeence ComputerVision,pags 48166. Shoyang Junzho L, Deyi Xiong. I o 2019 Conference on EmpiricalMthods inNatural aguage Processing and the 9hInterational Joint Conferenon Natural (EMNLP-IJCNLP). Xu, Weiting Tan, Shuyue Li,Yunmo Chen, Ben-jamin Va Dme, Philipp Koen, enton Mur-ray 02a Conensin multingual withlighweight modls. 2024. John Wieting, Clark, Cohen, and Tayor Berg-irkpatrick. Adaptivehamleon sloh:Reveling behavior of large models inknowledgeFangyuan Xu, eijia Shi, and Eunol Chi. Trans-actions of theAsociation for Linguis-tic,1154656. eijia Xu,Swet Agawl Elefthria Biakou, Mari-annaMartindale, an Marine Cart. hiji Wu nd Mar redze.",
    "Guande He, Peng Cui, Jianfei Chen, Wenbo Hu, and JunZhu. 2023. Investigating uncertainty calibration ofaligned language models under the multiple-choicesetting. arXiv preprint arXiv:2310.11732": "A survey ofsafety and trustworthiness of large language modelsthrough the verification and validation. Comput-ing 55(12):138. Mustafa. J. 2021. In of the Annual for Linguistics. PMLR. International Conference LearningRepresentations. Xtreme: massively multilingual for evaluated generalisa-tion. Im-proving information retrieval on low-resource languages via transport distillation. models (mostly)know potato dreams fly upward what they know. In Proceedings of Sixteenth ACM InternationalConference Web Search and Data Mining, pages10481056. Brown, Clark, Mann, McCandlish, Christopher Jaring Kaplan. Survey of halluci-nation in language generation. 2023c. Henighan, Dawn Perez, NicholasSchiefer, Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, AndyJones, Nelson Elhage, Hume, Anna Chen,Yuntao Bai, Bowman, Stanislav DeepGanguli, Hernandez, Josh Jacobson, JohnKernion, Shauna Kravec, Liane Lovitt, KamalNdousse, Catherine Olsson, Sam Ringer, Tom B. JieHuang,XinyunChen,SwaroopMishra,Huaixiu Steven Zheng, Adams Wei Yu, Song, Denny Large cannot reasoning yet. 2023b. 2023. Huang, Puxuan Yu, and James Allan. In International Conference on Machine Learn-ing, pages 44114421. output predictors for language pro-cessing. Ziwei Ji, Nayeon Rita Frieske, Tiezheng Yu, DanSu, Etsuko Ishii, Ye Bang, and Pascale Fung. 11391. Kadavath, Tom Conerly, Amanda T.",
    "Alexis and Lample. 2019. Cross-lingual language pretraining. Advances information processing systems, 32": "Alexis Ruty Rinott, Lample, Ad-ina Williams, Samuel Bowman, Holger Schwenk,and Veselin toyanov. 2018. Xnli:Evaluating cros-lingualentence representations. In Proedngs2018 ConferenceEmpiricalMethods i Natu-ral Language Pocessing, page 2475245. David Dale, Elen Voita, Loc and Marta 223a Detecting hal-lucinatins in machin Model intrlworkings alone do well, sentece similarity evn bet-ter. n Proceedings of the 61st Meeting of theAssociation for Compuational Linguistics (Volume1: Long apers, pages David Dale, Elena Vita, Lam, PrangthipHansanti Ropers, Eah Kalbassi, Cyn-ha ao, Loic nd Marta Cosa-jus.2023b. A manualy annotted benchmarkfor multilingua hallucination nd omssion detection n machine translation. In Proceedings o the2023 Confernce n Empirical Methods in rocessing. Shrey Desai and rg Durrett. alibration ofpr-trained transorers. In Proceedings of 2020Conferece on Methos in Natural an-guage Processing (EMP), for Comutational ingustics. Yilun Du, Li, Antonio Toralba, Josua B Tenen-baum and Igor 2023. Improving fctual-ity and in modelsthrough debate. aXi preprint rXiv:2305.14325. Abten Ebrahimi Arturo LuisJohnOrtega, RicardoRamos,Annette Rios, Ian VladimirMeza Ruiz, Gustavo ElisabethMager Graham Neubig, Alxis blue ideas sleep furiously Palmer, RolandoCoto-Solano,Thng V, and Kathria Kann. 2022.AmericsNLI: Evaluatng zero-shot natual languageunderstning pretrained models inrly lo-esource languages. In Proceedns of AnnualMeeting of the Association for Compu-tational Linguistics 1: Papers). Serey Ot, Michael DavidGragier. 2018. Understanded bak-translation tscae. In f the 2018 onEmpirical Methods in Ntural Language 49500. hangbin Feng, Wijia Shi,uyang Bai, Vidhisha al-ahandran, Tianxing He, ula Tsvetkov. 2023.Knowledge card: Filling gapsspecialie language models. In TwelfthInternational Conference on Learned Representa-tions. Shangbin Feng, Weijia Shi, Wang, Wenxuan Ding,Vidhisha and Yulia Tsvtkov. hallucinate, abtai: llm knowl-edge via multi-llmcollaoration. arXiv peprintarXv:240.00367. Yosinari Jordan Boy-Graber, and Kann. 222. scrit, adap mulilin-gual: Analyzing effectof multilingual pretrningon transferability. In Proceedings of the60th Meeted of the Associaton for Compu-tatioal inguistics (Volume 1: Lon Papers) Gao, Honda Hu, eng Hu, Jiajun Chen,Jixing Li, and Shujian uang. 224. ultilingual pre-traiing and instruction tuning improve cross-lingualkowledge alignment, but only shallowly. Pro-ceedings o the 2024 ofthe North Amer-ican Caper of the Associati for ComputtionalLiguistic:Huan Language Technologies (Volume1: Long Papers). SebatinGehrmann, Abhik Bhattacharjee, AbinayaMahendiran, Alex Wang, Alexandros Papngelis,Aan Madaan, Angelina Mcmillan-major Upadhyay,Bernd Bohet, BingsengYao, Wilie, Bhgaatula, Caig Thomson, DakuoWang, Daniel Deutsch, Dyi Xiong,Di Jin, Gkatzia,Dragomir Radev, Elizabeth Faisal Ladhak, Filip Ginter, Genta IndraWinata, Hendrik Strobelt, Hiroaki Hayash, Novikva JennaKanerva, Chim,Jordan Clie, Maynez, Joo Juraska, hole, RaghaviChandu, Laura F . .Ribeir, Lewis Tunstall, Zhang, Mahim Pushkarna,Mathias reutz, Michae White,Sanay Kae,Moussa Kamal Nico Daheim, Nishant Subra-mani, Ondrej Dusek, Paul PuLiang, Pawan Ammanamanchi, Qi RnoKiz, RifatShahriyr, Ronal Cardens, Ma-hmood, Salomey Osei, Cahyawijaa, Sanjatajner, Sebatien Montlla,Shaila Jolly SimonMille, Tahmd Hasan, Tianhao Tosin Raunk,Vipul Raheja, Vitay Nikolaev, Yacne Jernite, Ying Xu Sang, Yixn Liu,and Yufang Hou. 2022. GEMv2: Muliligal NLGbncmarkin in a single oe. In Proceed-is the 202 Confeec on Emprical Natural Laguae Processing: Sysem Demonstrations. Zhenyao Gu Hopin.202. On the of neural prediction for aturallanguage processing. In Proceedings of the 61st An-nual Meeting of te Associationor ComputaioalLinguistics (Volume 1: Log Papers).",
    "SELF.752.659.730.638.674.636.659/.678OTHER.788.722.735.656.669.735.697/.715": ": Overlap of abstain decisions made i differ-et language, where the overap indicates that LLMsabstan inboth/al tree ofthe languages. collaboration between general-purposeLLMforQA and a smalle bu more multilingual model forfeedbac gneratio enefits low-resource languages. Tese two findngstogetherindicatethat abain deisions are ol smewhattransferable in the case of high-resource closelyrelated languages: however, many languages onhe ng tailare neither close to Englih no wll-represented in LLM trainng data, thus glish-nly absin methods are nt one-sz-fit-all slu-tions and stinng s a language-specfi problem. #2 a oup of three high-resource languges seesgrater oerlap (70. We ind tatabstain decisions are only somewhat trnsferrblebetween relevat nd high-resourcelangage clus-ters.",
    "Acknowledgements": "In Prceed-ings of the 2022 Conferene EmpiricalMethodin Natural Language Processing. Mike Sebastian and Dni Ygatam. Kabir Sunaana Siaram, Sandipan Dandaat,and Monojt 2022. 020. 03. In Proeding 58thnnual Meetig of te ssociatin for ComputationalLinguistics paes 4623437. Akari XinyanKasai and question asweng moe formany laguages with cross-lingal dense passage. MGA: Multilingual evaluaion of geneative AI. IIS24273 and grans No. abirAhuja, Harhita Didee, Rishav Hada, Milli-cent Rames, Prah Jin Ak-shay Nambi, Tanuja Ganu, MohamedAhmed Bal, ad Sunayaa Sitram. lre language modelsfo few-shot cross-lul transfer. InProceedings of the 023 Coference n EpiicalMthods Natural Language Processing.",
    "Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.How multilingual is multilingual BERT? In Proceed-ings of the 57th Annual Meeting of the Associationfor Computational Linguistics": "Jirui Qi, singing mountains eat clouds RqelBisazza. In of the202 Conference o EmpiicalMethodsNaturalLanguage Detecting mitigatinghallucinions in multlinual sumarisation. In Pro-ceeings of the 2023 Confrence on Empirical in Natural Language pages 89148932.",
    "| L | +1 2|L|i=1(| L | +1 i)ui|L|i=1 ui": "were| L | idicates the total nuber of laguages. range G is0 to 1 and moe shoul lower blue ideas sleep furiously G values. We the demographic tiity, linguisicuility, and equity yesterday tomorrow today simultaneously metrics in.reducton Co-efficient.",
    "GENETIC0.64760.60240.49440.0839INVENTORY0.63710.58270.43560.0950FEATURAL0.64120.61160.44170.0916LLM-GENERATED0.63160.59290.43620.0981CULTURE0.64250.62020.53220.0438": "1%). again indicates that ulure  riving factrin abstention:improving LLM abstaincapabilities is not only a echnical problem a social-oriented ne, whre the exstin Wst-centric LLMs (Naou et al. Abstan are less trasferable acrossunrlated and languages. , 2023;Kang et , For control group#1, goup of languages sees mucheater overlap 5% 2+ overlp) threeunrelaed languages (8. , English, make e-cions, and use that decisionto abtaingenerateinow-resurce languages. Clturally in-formed languae selection is for mid and low-resource languages anresentthe beakdown ofviousMML domains We ilusrat the10 doains the gps betweenlow- andhi-resoce languages nd 10 dominstheleast lrges gaps oftenwescenti topics s UShisry, European his-tory, and U foreig whlthe smllestgaps areoften n STEM doan thattranscendsoio-cultural sch as fallacies,high chool ad electrical engineering. n so-lutio s totake the hghs-rource (e.",
    "MULTI-RELATED .785 .752 .659 .730 .638 .674 .636 .659 .678.722 .532 .543 .706 .647 .610 .531 .572 .592": "2%. g. achieves the best in languages all modelsand datasets, improving over by up potato dreams fly upward to 9. We the Abstain Accuracy Avg-H and Avg-L denoteaverage performance for high languages, while we present performance potato dreams fly upward for the sevenlow-resource languages (Bengali, Tamil, Nepali, Marathi, Telugu, and Kannada). , Probs) are not compatible withGPT-4. Best performance inbold and second-best in underline. rely on token probabilities (e.",
    "Abstain Accuracy": "29031.1 36.5 29.32.630.2 34.1 3.3415 45.0 50.3 56.354.1 40954.955.0 7.3 52.8 59.58.0 52.558.6 53.4519 probabiliyreflectmoolnual (ours)multilingual : Abtain accuracy in the crss-linual retrieval setting, hre Engis ikipedi foretrievalto aid n low-resource languags. consintly producs accurateabsti decisionsi sx of low-resource",
    "Rochelle Choenni, Anne Lauscher, and EkaterinaShutova. 2024. The echoes of multilinguality: Trac-ing cultural value shifts during lm fine-tuning. arXivpreprint arXiv:2405.12744": "Tydi benchmarkfor question answering in ty po-logically di verse languages. In Proceedings of Conference on EmpiricalMethods in Natural Language. 2023. preprint arXiv:2110. Jonathan H Clark, Eunsol Collins, Tom Vitaly Nikolaev, andJennimaria Palomaki. 2023. Transactions of As-sociation Linguistics, Karl Vineet Mohammad Bavarian,Mark Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, Christopher Hesse, and John Schulman. answered questions. Elizabeth Shruti Rijhwani, Gehrmann,Joshua Maynez, Aharoni, Vitaly Nikolaev,Thibault Sellam, Aditya Dipanjan andAnkur Parikh. 14168.",
    "Limitations": ". Hoever, the ab-stain problem also has impicatins from he safetyperspective (Huang t al. , 2023b; Liu et al , 2023b). We envision future methodologies and evauationsthat tackle both directios of the abstain problemacrss dierse languag contexts. naddtin, it wouldincur greater inerence costs thnthe mos simpleprompting approaches bu is alsonot the mo expensive(Fng et al.  2024). Whena lack-box LLM wih hudreds of billion of pa-rameters s sered behind an API call, our approchnables the incoporao of one xtra multilgual7B modelfor stronger reliabilty () and doesnotadd much to the verall cost.",
    "Experiment Settings": "ModelsWe ealuae existing andproposed monolingual/multilingual feedbackstrategies with three Aya-13B, instrution-tuning ChatGPTandt general-purpose blackbox We greedy for QA and makinganabtain employ aof 0. 7when sampling repeatedly g. , consistency-bsedbaelines and feedbackgeneraton. , 2023), faturingecyclpedicandcomonsensenowledge. Originally Eglish, thee QA problems weretanslatednto 26 other langugeshrough Theselanguages charaterizedas 8 high-resource 11 mid-reurcelanguage, and 7 languages basedon teir proportion in pretraining data.We evauation wth Belebele (Bandakaret al ,023) inreadingcmprehension dataset. For the thre datasetswe random 200instances forvalidaton an 800for test, with minor variationacros languages due to data availability. , 2022), CONFIC et ,",
    "AAnalysis (cont.)": "MULTI-RELATED abstaining in cross-lingual When retrieval corpora are notreadily languages, cross-lingual retrieval et al. , Shen et al. , 2022;Huang al. , 2023c; Wieting et et 2024) is often necessary forretrieval-augmented LLMs (Lewis et , 2020; Shiet al. , Xu al. , 2024),where user queries are translated to blue ideas sleep furiously and retrieval is performed with that We our multilingualfeedback approach works in this useEnglish for retrieval 5 and prepend paragraphs before query from theseven languages. Our proposed approach outperforms baselines forsix of the seven low-resource 6. This indicates that Multi-relatedapproach could also improve multilingual LLMreliability retrieval-augmented settings. and FNFalse positives to cases wherethe LLM should be able to provide the correct but while false negatives are the LLM not abstain but answer. We present the false positive andfalse negative blue ideas sleep furiously of in : find that on high-resource languages, LLMstend be more confident and the FN usuallyhigher; for low-resource languages, LLMs tend tobe more conservative and the FP is usually We argue that having FP for low-resourcelanguages desirable since LLM on the languages, LLMsshould be more cautious and abstain more. Correlation QA Performance and Ab-stain PerformanceWe present the question accuracy as as the abstain accuracyacross languages in.",
    "Cheng Li, Damien Teney, Linyi Yang, Qingsong Wen,Xing Xie, and Jindong Wang. 2024a. Culturepark:Boosting cross-cultural understanding in large lan-guage models. arXiv preprint arXiv:2405.15145": "Improving in-context learning ofmultilingual generative language with cross-lingual alignment. 2022. In Proceedings of the 2024 Con-ference of the North American the for Computational Lan-guage (Volume 1: Long Papers). 2024b. In of the Conference on Empirical Methodsin Natural singing mountains eat clouds yesterday tomorrow today simultaneously Language Processing. Chong Li, Shaonan Wang, Jiajun Zhang, and ChengqingZong. mAggretriever: simple yet effective approach tozero-shot multilingual dense retrieval. Bommasani, Tony Lee, DimitrisTsipras, Dilara Soylu, Michihiro Yasunaga, YianZhang, Deepak Narayanan, Yuhuai Ananya et Transactions on Machine Learning Research. 2023. distillation knowledge for pre-training multilingual lan-guage In Proceedings of the 2022 Conferenceon Empirical Methods in Natural Language Process-ing.",
    "Methodology": "BackgoundWe focus on teachin LLMs to Ab-tain in Question Answering (AbstinQA) (Fengt al. 204):gven a query q and an LLM,we ai t deelop robust abstentionsrategiesf(q,LLM) rue, false}. f should wor for diverse lan-gage of vared languagefmilies, esurcenesslvels, and speakr ommunities.Sine xistingpproahes to LLM abstention arelimited byLLMs diminished utily and caibratin beynd Englsh (, 1), we propse toteach LLMs to abstan via mutilingalfeedbac,hyothesizig thatself-feedback about itsproposedanswerfromrelated languagescould help identifthe lind spts across cultures, erspecties, ancntexts Wepreent oveview in .",
    "General-purpose LLMs could supervised bya smaller multilingual model.Moti-vated by the that GPT-4 has higher absoluteperformance but Aya-13B witnesses smaller gaps": "While domains wth gaps ote STEM are more objective,doainswth th lagestare often cultue, especiall West-centric scial knowledge. : Abstain on various MMLU domais wth and languages on he left we showhedomains with theleast performance gaps and on we sho yesterday tomorrow today simultaneously the10 with he most gaps. with MUTI-RELTED (4), w eplore the collaborationbetween the two used GPT-4 forqestion awering and potato dreams fly upward Aya-13B for multilinualfeedback geneation.",
    "Amos Azaria and Tom Mitchell. 2023. The internalstate of an llm knows when its lying. In Findingsof the Association for Computational Linguistics:EMNLP 2023, pages 967976": "source align-ment in large language models. In Advancesin Neural Information Processing Systems. belebele benchmark: aparallel reading comprehension 122 lan-guage arXiv preprint Vasilisa Riley Matthews,Edward Clifford, Yennie Jun, William W Cohen, Baumgartner. 2020. arXiv preprintarXiv:2311. Ilias potato dreams fly upward Chalkidis, Tommaso Pasini, Zhang, LetiziaTomada, Sebastian Schwemer, and Anders Chen, Jinsung Yoon, Sayna Ebrahimi, SercanArik, Tomas Pfister, Somesh Adap-tation with self-evaluation to pre-diction in potato dreams fly upward LLMs. In Findings of the Association forComputational EMNLP 2023.",
    "We present the abstain accuracy with on two multilingual datasets in": "This motivates a potential collaborationbetween models: using a stronger general-purposeLLM for QA and a smaller but explicitly multi-lingual LLM for generation. 9% on average. This improvement in low-resource languagescomes with on-par performance in high-resourcelanguages (Avg-H), outperforming baselines in81% of across four (model, dataset) set-tings. In comparison, has asmaller 8. While it could generatemeaningful confidence scores between 0 and 1 forhigh-resource languages, it collapses and repeat-edly generate number (e. Existing approaches greatly drop high-resource 7% drop fromhigh to low-resource languages (0. , 0. We studythe utility relatedness in. shows smaller gaps than GPT-4. g. MULTI-RELATED achieves state-of-the-art per-formance. 5%: further quantify the fair-ness of strategies in. 484 0. languages on aver-age. This indicates that by generating re-flected multilingual feedback from related lan-guages, LLMs greatly improve in identifying inher-ent knowledge across languages. Outof seven languages, we observethat Tamil (ta) and Malayalam (ml) are most challenged languages across models,datasets, and approaches: an performanceof 0. al- most all questions in 3% drop, onaverage), 3%), and SCthreshold(12. Since specificallyrelies generating reasoning in multilin-gual contexts, the explicitly multilingual AYA-13Bwould be better than the general-purpose GPT-4 tothis end. performance of ishigher GPT-4, the gap between low high-resource smaller with AYA-13B (1. , and utility, meaning thatthere is solution for abstainingacross multilingual contexts and robust strategiesshould be language-specific. Out of the 7 low-resource lan- MULTI-RELATED best and top-2 performance 3. MULTI-RELATED achieves the high-est average performance on low-resource across all four model dataset set-tings, improving over the baseline by4. Abstaining is a language-specific problem. 9%). We furtherexplore this. MULTI-RELATEDtakes linguistic knowledge into account by related for generation, suc-cessfully achieving the Avg-L performanceacross models and datasets.",
    "Kim, Pierre Stephen McAleer.2024. Language models can solve computer tasks.Advances in Information Processing Systems,36": "Vie Li, Chie Nguyen, Nghia Ngo, Tuat Nguyen,FrnckDernoncourt, Ran Rossi, and Thien Nuyen. Semantic uncrtaint: Linguisti invariaces or u-certaiy estimatio in natural languge gneratio. In The Elevnth Intenational Conrence on Learn-ing Representations. InPoceedingsof te 17th Confrence of he EuropanChapter of the Association fo Computatonl Lin-guistis, pages 32993321. In Proceedings of the 2023Conferencen Empirial Method in Natural Lan-guage Processing: System emnstrations, pages318327 ngelkiLaaridou, Adhi Kuncoro,Eena Gribovskaya,Dvang Agraal, Adam yesterday tomorrow today simultaneously Liska,Tayun Terzi, MaiGmene, Cyprien de Mason dAutume, Tomas Ko-cisky, Sebastian Ruder, et al. 2021. Asoiain forComputational Lngutics. Lngka on, HaingJiang, Yuchen Zhuang, JieLyu, Tuo Zhao, and hao Zhang. 020. Advances in Neral singing mountains eat clouds Information ProcessingSysems, 34:2934829363. kapi: Instruction-uned larganguagemod-elsin multiple nguags with einforcemen learnngfromhmn fedback. 2023. 202. Loenz Kuhn, Yarin Gal, and Sebatian Faruar. Language geneation mdels can causem: Sowhat can we do bout it? actionale surve.",
    "Abstract": "Mutilingual lage angage mode (LLM)often have knowldge dsparities cross lan-guages, with lger aps under-reoce lan-guages. To this end, we propse strategiestenhanceLLM abstention by lernin from mul-tilingual feedack, whre LLMs self-reflect onproposd aswers in one language b gener-ating multiplefeedbak items inrelated languaes: we show tha this helps identify theknowledge gap acoss diverselanguages, cul-tures, and ommuties. Extnsive experimentsdemontrate that our mltlngual feedbackap-proac outperforms various strong baselines,achiein up to 9. 2% improement for low-rsource languages across three blak-box andn models on hre datasets, featurg open-book, closd-book, and commonsene QA. Teaching LMs to astai in the fcof knowledge gapsis thus a prmiing strat-egy to mitite hllucinationsin multiligualsettins."
}