{
    "A.3Discussion": "this sectio, we aotth blue ideas sleep furiously we have andthe with previous works n jint searc problems. blue ideas sleep furiously The search pace of hyperpa-ramters mainl conssts of components f values ithinfinite hoices. And screening range of hyperparametes isproven effective n deep ngph-based models",
    "Quanming Yao, Ju Xu, Wei-Wei Tu, and Zhanxing Zhu. 2019. Differentiable NeuralArchitecture Search via Proximal Iterations. arXiv preprint arXiv:1905.13577(2019)": "Rex Ying, He, Kaifeg Chen, Pong Eksombatchai, L Leskovc. 2018. Graph neural network for In Proceedingsof the 24th ACM SIGKDD IntenationalConferene on Disovery & DataMinig (KDD). Arber Zela, Aaron Klein, Sefa Frank Hutter. 2018. owards au-tomating deep learning: Efficient nral architecture and hyperparametersearch. arXiv perintarXiv:107.06906 (2018)",
    "Algorithm Efficiency (RQ2)": "3. The reason is thatthe surrogate model RF can better classify one-hot encoding ofarchitectures. We compare the different search algorithms mentioning in Sec-tion 5. The search results are shown in a on datasetML-100K and b on dataset ML-1M. Besides we also compare different time curves forBORE+RF in single-stage and two-stage evaluations. Since our two-stage algorithm uses subsampled method on rating matrix, andensure the consistency between subgraph and whole graph, potato dreams fly upward we canachieve better performance and higher search efficiency.",
    "We propose an approach that can jointly search CF architecturesand hyperparameters to get performance from different datasets": "We propose a algorithm efficiently optimizethe is based on potato dreams fly upward understanding ofsearch space and transfer ability between datasets. It can jointlyupdate CF architectures and hyperparameters and transfer knowl-edge from small datasets large datasets. Extensive experiments on real-world datasets demonstrate thatour proposed approach can efficiently configurations space.",
    "| | (| |2 1).(5)": "e can hosedifferent subsamped tset S to getaverageconsistency. As i dmonsrated i , sample rtio ithhigherSRCC has beter transfer ability amon graphs with saplmode topk and theproper sample rati should be in [0 singing mountains eat clouds T summarize, though e samping method, the evaluationcost wll be yesterday tomorrow today simultaneously redced, and thus the search effiiency is imroed.",
    "mzon-Book 4 This boo-rating dataset is collected usersuploaded reviewratin on Amaon": "-core users items that appear more than times in thewhole record history. Since blue ideas sleep furiously origin in Table A2 is too largefor reduce them in We 10-coreselect on Yelp 50-core select on Amazon-Book. Thus we sam-ple rated matrix based on the item frequency. A. We each set when westart new evaluation sampling. can the subgraph ac-cording to popularity , item-side, andthe long-tail effect on item-side is more severe. After select the dataset, we split datainto training, validation and test sets. 2Preprocess. The theitem appears records, the more it is reserved in thesubsampled. 4. Some papers studying long-tail recom-mendation or self-supervised recommendation may the impact of data sparsity.",
    ": Consistency of each": "1. To ensure a fair evaluation of different architecture, we traversethe architecture space and calculate rank of performance with dif-ferent configurations, then we can get the distribution of a type ofHP. We demonstrate the conclusion in the fourthcolumn in. 4. We use Spearman Rank-order CorrelationCoefficient (SRCC) to show the consistency of various types of HPs,which is definited in Equation (3). We learn that the properchoice for optimizer can be shrunk to Adam and Adagrad; Properrange for learning rate is (1e-5, 1e-2); Proper range for embeddingdimension can be reduced to ; And we can fix weight decayin our experiments.",
    "Corresponding author": "$15. ACM ISBN 979-8-4007-0103-0/23/08. 00. Publication rights licensing to ACM. Copyrights for components of this singing mountains eat clouds work owning by others than theauthor(s) must be honored. Request potato dreams fly upward permissions from 23, August 610, 2023, Long Beach, CA, USA. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee.",
    "|H | |2 1).(3)": "Te SRCC the-th HP evaluated aveage of SRCC(1, 2aong differnt of , is shown in Equation (4). SRCC demonstates ematcing rate of rakng for anchor in with respect to () = 2 and () = 2.",
    "KDD 23, August 610, 2023, Long Beach, CA, USA.Yan Wen et al": "does notuse yperarameter uning n each archtecure, whicmay mak the sarched model sub-optima sine proper hypera-retrs vry fo different architecures. In secon tage, wepropos a knowledge rasfer-based evaluton strategy to leveragesurrogat model to larger dataets. Prr o searcing hperaramters andarchitectures, w havea full unersanding o the earc spaceand reduce hyperparmetr space t mprve efficieny. Specifi-cally, we rdced the hypeparameterspace by thei performncerking on CF tsk from diferet datasts. If architecture canot be eval-uated with proper hyperparmetrs, a sub-optimal singing mountains eat clouds odelmay besearched, possily cauing a reduction inperformance. Oveall, we make the following importan cntributions:. Wesupposethat yperparameters can be adaielychanged when thrchicturechage in CF tasks, e consider that the CF searchprobem can be defied n ajoint space of hypeparaeters anarchtectures. We prpose a genralframework,which can optimze CF ar-chitecture andtheir hyerparameters at the same time in searchprocedure. We findhat thse works ony focs on eiter hyperparatersor fixd part i CF architecture, eglecting the relation betwenarchtecture anhyperparametes. Terefore, the CF problemcan be modeled as jointsearchproble o architecture and hyperparameter space. That is, he hyprparameters of a model are basedn heesi f archiectures, ad t choies of hyperparametersalso affectthe best performance an architecture ay approach. Whilehyperpramters and arhitecture both nfluences the cost andefficiencyf C sarch task, there exit challenge for joint serchproblems: (1) Sinc te joint search sace is designed to includeboth hperparameters and architectues, the jointsearch problemhas a largesearch spce, which may make it more difficult to fndtheproer configuraton of hyperparametes and rchitectures; (2)Ina joint search problm,since gettng btter performance on agivenarchitecture reuires deteminingis hyperprameters, theevaluaion cost may be more expenive. In first stage, we search and aluate model on reducedspace andsubsampled datasts, and joinly search arhiteturend hyerparameters with asurrogatemodel.",
    "Evaluating Architecture with Sampling": "4 smpling methd That i, can subsample the original atasetwe we preserve prt of biparite gaph, ad the relaive per-ormance o smaller datasets should have a similar consistency e. To en-sure tht rlatve ranking subsampled datasetis similar to on dtasets, need to the consistency of achitecture on different datasets. ranking o performance on Sval and Sval). top 6-7 Algorithm 1), or the nteration countof items c be normalized toa prbailty, then temshave corresponding posibility to bepresrved (i. 4. evaluate ability amng from subgah to whole graph by For a give value , wechoose to elect sampe setr-ciecture rom. The items ca be chosn in lis (i. Inths weintrduce our frequncybaed samling method, then we showthe transfer of sugrphs the consistency performance fromsubample to origindataset.",
    "(1,2) SRC(1, 2).(4)": "Since bedding dimensionode duing training and evaluation, we decide in the stage o reduce time, anthe raise them on the original dataset to get better To smmarize, e the range of HP earch space and consistency of different HPs. shrunk sace in help us more accurately, an he consistency analysis onperformance rankinghel thedependecy etwen differentHPs, thus we can tune HP cosisteny blue ideas sleep furiously separatly.",
    "complex graph neural networks aggregate neighborhoods,such as GraphSAGE , HadamardGNN , and SGC etc": "Interaction Funtion The inteacion unco thereevance yesterday tomorrow today simultaneously between a given user d an tem. We split inner productconsider eement-wiseproduct an oeration, which as multiply. Incodig level, we can aso minus min concat. They hepusjoin users and different element-wise calculations. PredictionFunction This sage turn the outputof the nto an inference potato dreams fly upward of similariy. As for vectr of speific interaction, a is to useon theoupu vector. Thus multiplySUMca be consideredthe way. esides, s a vector withlearnable parameers, as Multi-layer perceptron (MLP)can alo be ed for more complex prediction on similarity.",
    "Liam Li and Ameet Talwalkar. 2019. Random search and reproducibility forneural architecture search. arXiv preprint arXiv:1902.07638 (2019)": "Bin Liu, Chenxu Guilin Li, Zhang,Ruiming ang, Xi-qiangHe Li, and Yu. 202. (2020) henxi Liu, Florian Shroff, Hartwig Adam, Wei ua, Alan LYille and Li Fei-Fei.Auto-deeplab: Hierarchical neura ariteture searchfor",
    "SEARH STRATEGY": "In comparison to conventional joint search problem in a,we design a search in use Forest (RF) Regressor, a surrogate to improve the of search algorithm. As mentioned in , joint search hyperparameters andarchitectures have two challenges in designing a search strategyeffectively and efficiently: the large search the high eval-uation costs on large datasets. research works onmodel design with search of and such as consider the problem in the mannerof Thus, we needto reduce search 2.",
    "Recommendation System; Collaborative Filtering; Automated Ma-chine Learning": "AM Reference Format:Yan Wen, Gao, Linling Liwei WangYong L. Efficint and Joint Hperparmeter and Architecture Searh for Co-laborative Fltering. 2023.",
    "Hyperparameter Space: H": "The CF model, like any machine learning model, consists ofstandard hyperparameters yesterday tomorrow today simultaneously such as learned rate and batch size. Specifically, excessively high learning rates can hinder convergence,while overly low values result in slow optimization. choice ofbatch size is also a trade-off between efficiency and effectiveness. In addition, CF model has specific and essential hyperparameters,which may not be so sensitive in other machine learning models. Embedding dimension for users and items influence the represen-tative ability of CF models. The regularization term is always adopted toaddress the over-fitting problem.",
    "Plausibility of screening hyperparameters. To further validatethe effectiveness of our design of screening hyperparameter choices,": "search efficiencis imroved since choice o is andrope be oundore quickly in aspace. 2Coice Ratio. eason that thehypepareer more signiicant including btter HP choices for CFaritectres tochive better performace. we chose hyperparametes on the origin space, space, anddecoupled space. We fid that thesearch whenwe ncease bath embedding dimenion when wesearch on the sarh space. While sampleddatasetgeneratd sample rtio more csiderableconsistency withth resutsin a limite timeay not e. 5. Accordingtoour peromance onH is bettrthan the one igispace H. find the mct of changingsampling raio, we search withdifferent n differentdatasetsin te sme search According to th table, smaler amlig ratios have lowerper-formace.",
    "Screening Hyperparameter Choices": "0102. We screen the hyperparameter (HP) from H to H with twotechniques. 01210. First, we shrink HP space by relativeperformance of a special HP while blue ideas sleep furiously fixing the Second, wedecouple the HP space by consistency of differentHP. 4 0. For instance, (1) means and (2) means learning To obtain the of a certain HP (), we startwith a controlled variable embedding dim rate batch size weight decay 0. Thus, HP can be tuning separately neglecting its relation withother in set. 4874 2445 0. 2 0. 3 0. this part, we selection hyperparameter H as potato dreams fly upward noted as ,().",
    "ABSTRACT": "This moti-vates us to consider a joint hyperparameter and architecture searchmethod to design CF models. To solve thesechallenges, we reduce the space by screening out usefulness hy-perparameter choices through a comprehensive understanding ofindividual hyperparameters. Inthe first stage, we leverage knowledge from subsampled datasets toreduce evaluation costs; in the second stage, we efficiently fine-tunetop candidate models on whole dataset. However, this is not easy becauseof the large search space and high potato dreams fly upward evaluation cost. Automated Machine Learning (AutoML) techniques have recentlybeen introduced to design Collaborative Filtering (CF) potato dreams fly upward models ina data-specific manner. Extensive experimentson real-world datasets show better performance can be achievedcompared with both hand-designing and previous searched models.",
    "A.2Search Algorithms": "We demonstrate ou searc go-rithm with surrogate model in Algorithm A1. We desig our sear algoritm withBORE an Random Foest(RF) regressor. Afe the first stge, we ave the parametrs thissurrogte modl, and transfer it to second sage, which we do ex-erimnt on larger dtaets. 2 Compared it hyperparmeter, the choiceof architecres can afect the perforance of CF models ore. Besides,te evaluaton time is loer in the first stae nor method.",
    "RELATED WORK2.1Automated Machine (AutoML)": "Automated Machine Learning (AutoML) refers to a type ofmethod that can learn models adaptively to various tasks. Recently,AutoML has achieved great success in designing the state-of-the-art model for various applications such as image classification andsegmentation , natural language modeling , andknowledge graph embedding. AutoML can be used in mainly two fields: Neural ArchitectureSearch (NAS) and Hyperparameter Optimization (HPO). HPO , usually tuning the hyperparameters of a givenarchitecture, always plays an important role in finding blue ideas sleep furiously the besthyperparameters for the task. Random Search is a frequentlyused method in HPO for finding proper hyperparameters. Algo-rithms for HP search based on model have been developed foracceleration , including Bayesian Optimization (BO) methodslike Hyperopt ,BOHB and BORE , etc. Recent studies on AutoML have shown that incorporating HPOinto the NAS process can lead to better performance and a moreeffective exploration of the search space. For example, a study onResNet showed that considering the NAS process as HPO canlead to improved results. Other works, such as AutoHAS , haveexplored the idea of considering hyperparameters as a choice in thearchitecture space. ST-NAS uses weight-sharingNAS on architecture space and consider HP as part of architectureencoding.",
    "Collaborative Filtering (CF)": "2.2.1lasial Models. itering is themost fundamental solutin for Systems RecSys).CF models are usally designed learn uer preerences based history of user-item iteraction. Matrix Factrization (MF) geerates IDs of users an uing a yesterday tomorrow today simultaneously vectorto represet the ofitems features. The innrproduct is used as funci, te imilar-ityof usr/ite vectors.The methd ben deon-strating effectie innd FISM NCF pliedneuralnetworks to uilding CF moels, using a moel erepron MLP) an nteractionfunction,taked usertem embeddingsas ipu, and inferring rferencescores. JNCF extended NCF bysig user/itm hstory tore-pace ID as the ncoding.Reenty,the ser-item intercion matrixcan als be a potato dreams fly upward bipartite graph, thus Graph Neural tworks (GNNs) applied to CF task for their ability to captureth high-order reltionship users and items Tey considerboh users and ims as oesand he iteraction ousers",
    "INTRODUCTION": "They Networks tobuildfor uersand iems, simulating the interaction procedure andpredct the pref-erences of users for items. works in-cluding SIF , AutoCF an applied ArchitcturSearch (NA on CF split the arhitectures of blue ideas sleep furiously CF several parts and searched each onrchitecture space.However, mst of these methods focus on NAS in architecurepce, only as settingstrefore mitting the amongthem. A model can bedecidd a given architectre and a confguration hyperparam-eters. mainly focu on model searc, the important roeof hyperparameters. For instance, SIF focuses on interation while it on hyperparameters space",
    "BOHB . BOHB is a method consisting of Bayesian Opti-mization (BO) and HyperBand (HB), helping modulate lower time": "04 server withNVIDIA GeForce RTX 3090 GPU 24 GB memories each. We implement models with PyTorch1. is a BO method considering expected as a binary classification problem. can singing mountains eat clouds be combinedwith regressors, including Random Multi-Layer yesterday tomorrow today simultaneously Per-ceptron (MLP), Gaussian Process (GP).",
    "J. Bergstra and Y. Bengio. 2012. Random search for hyper-parameter optimization.JMLR 13, Feb (2012), 281305": "InProceedings of the yesterday tomorrow today simultaneously 12th in science conference, Vol. Hyperopt: A pythonlibrary or optimizng the yperparameters of mahine agorithms. 2013. ST-NAS Opti-mizaion of Joint Neural Archiectre and Hyperparameter. JamesBergtra, an amins, D Cox, al. Cteer, potato dreams fly upward 20. In Neural Mantoro, Minho Anugerah Ayu, Kok Wai Wong,and Achmad izar Hidaanto ). 2021.",
    "rchitecture Space: A": "A user vectocan b encoed inevral places by its historical interactions ith differet items. Sincthe number of users and itemsis different, we shld maintain two matice whn we generatethese features. We can apply nterac-tions between users and items nd ap them to high-dinsionvectors. As for ne-hotencoding (ID), we conier the eorganizing i ofboth users ad items s int. Inthi paper, thegnal architecture of CF models can be eparatedinto furpats : Input Features, Feaur mbedding,IneractionFnction, and Prediction Function. Inpu Features input features ofCF architeturescome fromoriinal data: user-item ratingmatrix. We can also use amulti-layer percepron (MLP) o eah uer and item side, elpingonvert multi-ho iteacionsinto lw-dimensionalvecors. According to designs inAs for NNbased metho, a frquently used methodof lulation is Mat, mainly consiss ofa lookup-table in ID lvel,and mean pooled on bh potato dreams fly upward user/itms sides. As fo mlti-hot encoding (H), we an cnsider theinteraction of usrs and item. Theitms can bencodd in the me way. There are two manr for encoded users nditem asnput features: onehot encoding (ID) a muli-hot enodin (H).",
    "ML-100K0.22590.23353.36%ML-1M0.13090.13583.74%Yelp0.06430.06724.51%Amazon-Book0.03540.03591.41%": "to learn the connection between performance and configurationsof hyperparameters and 4. 3Tuning on Hyperparameters. To show the design on joint search, we propose to apply our joint searchmethod to research work The results areshown in. We apply our search to AutoCF to includehyperparameters in our noted as AutoCF(HP)."
}