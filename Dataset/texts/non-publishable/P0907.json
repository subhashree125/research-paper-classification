{
    "AppendicesAAlgorithm": "In rief, e iteratethrugh ach downstreamtak to larn cresponing prmpt vectors individually. Ilines 35, we compute the embedding for each node sing the pre-trained graph ecoder, with the pre-tained weighs 0 reainingfixed throughout the adaptation prcess. Specifclly, we performt similaity-weightedradout (lines 911), nert prompt (lines1213), modify nodesembeddings usng these prompts (lins 1215), and update he em-beddings for the prototypical ndes/graphs based o the few-shtlabeled data providing in thetas (lines 119)",
    "h = Aggr(h1, {h1: N ()};),(3)": "To evaluate non-homophilic pre-training and prompt we focus on two tasks on node and graph classification, in few-shot settings. For nodeclassification within a graph = (, let be the set of Similarly, classification of graphs G, let Y be the set graph labels. where are the learnable the -th Aggr()is the function, which various forms. the few-shot setting, there are only labeling samples per class,where is a small number 10). In the first the input node h0 is typicallyinitialized from node feature vector For simplicity, we output representations of the final layer as h, which be fed into loss for a More specifically, both the and learn-ing not sensitive to the homophilic of the graphand its nodes. This scenario is known as-shot.",
    "Related Work": "However, allthese GNNs and pre-rainin methos re basedon the homopilic assuption, overlooking that real-orld grapsare generlly non-homophilic. How-ever, the effectiveness ofGNNs heavily ries on abunda task-speciic labeled yesterday tomorrow today simultaneously data and requires re-trining for vrious task. Theytypically oeate on a essag-passi rmeork, here nodesiteratively update their reesentations by ggregating messagesrceived from their neighboring nods. Morever, recent works have xplored pe-training on blue ideas sleep furiously non-homophili raphs by capturig neighborhood infor-matio to construct unspervised tas r pre-tranin th graphencodr andthen trnsferring prior nonhoophilic knowledgeto downstram tsks thrugh fine-tuning wth task-specii super-vion. Graphrepresentation learning. Many NNs hav been proposed for non-homophilic graphs, eploingmethods such as capuring high-frequenc signals , discer-ig potential neghbors , and hghordrmesage passing. Non-homophilic graph larning. GNNs are mainstream techniquefor grah repreentation learnig.",
    "(2) Graph Models": "DGI : DGI as self-upervised pre-trainngmethod-olgy tailored or omogeneous graphs. It is predictd on themaximiation of mutual information (MI), aiming etimated MI between augmente GraphCL :GraphCL leerage variety of graph self-superiedlearnng, tapping into te intrinsicstructural of The overarching goal is to amplifytheconcordance between dfferent augmentations throughoutgraph pre-training DLL uses latent modeling to in neighborhoods, avoiding agmentations and with to capture ocal andgloalinformation, enhancing representation learnng",
    "Setup of downstream tasks. We conduct two types of down-stream task: node classification, and graph classification. These tasksare set up as -shot classification problems, meaning that for each": "We generate100 -sht tasks or oth ode clasification andgraph classification by repeating the sampled rocess 10 imes. , iscnsinSquirrel, Chameleon and Cornell only comprie a sigle graph andcannot e directly used for grap classifiction. Fo datast with high h-mophilyratios, PROTEIN,ENZYME, BZR and COX2 havergiagrph lbels, so blue ideas sleep furiously w diretly conduct grap lassificaton on thsgphs. We pretrain the grah ecoderonce for eacdataset and then use th samepre-traine model or all downstreamtasks. e. Given that all low-homophily dataes, i. hu followingpre-vious research, we gneate ultiple graphs by constuc-ng ego-netwrks centered on the labeled nodes in each datase. W the prform gaph clasiication ontese ego-networks, eahlabeling accoring to its central ode. Since the -sho tasks re blanc classifcation problems,we use accuray to ealuat eformance in lne with priorstud-ies. cass, istanes (nodes or graps) re randol selecing for su-pervision.",
    "Analysis on Pre-Training Methods": "To further evaluate non-homophily tasks, usingProNoG for downstream adaptation, we employ homophily taskslink using potato dreams fly upward in GraphPrompt and non-homophily tasksGraphCL and DSSL , We compare tasks and show results in.",
    "We first evaluate classification tasks. vary of shots to investigate their impact on performance": "e. These results demonstrate in learning prior non-homophilicgraphs and capturing nodes specific patterns. (2) learned methods, i. performance. These underscore the our conditional prompting in characterizing embeddingsto capture nodes specific patterns. To assess the of ProNoGwith different of data, we vary the number ofshots in the downstream tasks present the results in and Appendix F. Their suboptimal per-formance can be attributing singed mountains eat clouds to inability to account node-specific patterns. that given the number nodes inWisconsin and Cornell, singing mountains eat clouds we only conduct tasks up to 3-shot. One-shot performance.",
    "where the prompt p, is generated with an equal dimension as h": "The prompt tuning process does optimize the promptvectors; instead condition-net, which subsequentlygenerates potato dreams fly upward the prompt for a given utilize function based on similarity fol-lowing work. tuning. Formally, for a task with labeledtraining set D = {(1,1), (2,2),. In this work, we focus on two typesof downstream task: node classification and graph classification.",
    ": Non-homophilic characteristics of graphs": "First, diferent graphs exhibit varyng of nonomophily. As shwn in (a), Cra itation network that con-sidered homopilic with 81% homophili hereasthe Wisconsi webpage grah diferent kinds of webpages,which highly heerophilic only 21% homophilic edges. non-homphilic characteristic f a graph lso depends onh target example, in dating network showngender as the node label, the mor heterophilicwith 2/7edges. However, taking hobbies as odelabel, grap mre with Hence, how do pre-tran a model irrespective f thegraphs In work, we propose fr hmophily tasks nd homophily samples. This motivates tomove from homopily tasks or pre-training an instead choose a non-homophily task. Secod, different nodes within the same ae distributeddifferentl interms of blue ideas sleep furiously their non-homophilic characteristics. Asshown in (c), and Cornell, their node havea divrse ratios2. how do potato dreams fly upward we the node-spcific non-omopilic charactertics? t tediverse across a one-sie-fits-all solution frall be However, exsting gen-eraly apply asingle pompt to all nodes , treating allnodesuniformly. Thus, methods overlook the fie-grinednoe-wise n-homophlic characteristics to",
    "Non-homophilic Graph Pre-training": "yesterday tomorrow today simultaneously We alsoexperimet with ink prediction and GraphACL for urtherealuation, as show in. Consider a homophily task. B. Acordin to Theorem 2, for non-homophili singed mountains eat clouds graphs with loer homopily ratos, on aveage threare fewer homophily sampes and more non-homophily samplesfor. In contrst, DGI , GrphCL ,ad GraphACL are nn-homophily methods, since AandB n their re-trainntasks are no relating tothe connectivity wth.",
    "Conclusions": "Finally, wecon-ducted extensive experiments on te publicdatsets, demonstraing that ProNoG significantl outprforms divrs tte-of-the-artbaselis. heobjecties aretwofod: learningco-rehensive knowlede irrespective of the varying non-omohilyhaacteristcs of graphs, an adapting the nodes with diverse distri-butios of non-homophily patters  donstream alications i afineraied, node-wise mnr. Then,fodownstream adaptatio, wepoposed conditio-net togeneate a series ofprompts conditioneon various on-hoophilic patterns acrss nods.",
    "Theorem 1. For a homophily task , adding a homophily samplealways results in a smaller loss than adding a non-homophily sample": "Proof. Moreover,since (,,) is non-homophily, we have sim(,) sim(,),and thus 0.5. Hence, (,,) < . singing mountains eat clouds",
    "Taoran Fang, Zhag hunping ad Lei Chen. 2024.Universa prmp tuning for gra neural ntworks. (2024)": "Fang, YanjunLuo,Fang Zhao Xu, Liang Zeg,and Chenxing Wang. 2023. In ICDE. Yuchen YanjuQin, ang Zao, blue ideas sleep furiously and Zheng. 203. TWave+: AMulti-Scale ficint Spectral Attention Netwrk With Long-Trm fr Disetangd Traffic Flow Forecin.",
    "Algorithm. We detail the main steps for conditional prompt gen-eration and tuning in Algorithm 1, Appendix A": "Assuming the aggregation involves at most neighbors, the com-plexity of calculating node embeddings over layers is ( |),where | denotes of nodes. In the prompt stage, each subgraph em-bedding is into the condtion-net. Therefore, the complexityfor conditional prompt learning is (3 | |). conclusion, blue ideas sleep furiously of is +3) | |). first dominates (.",
    "WisconsinSquirrelChameleon PROTEINS ENZYMESCiteseerWisconsinSquirrelChameleon PROTEINS ENZYMESCOX2": "654. 129. 67 48. This highlights ncessity of readot subgraphsweihted by similarity to cature odes non-homophilic patterns,. 701. 4614. 263. 130. 32. 3022. 6755 9214. 90. 742. 5965. 304. 5627. 6410 0925. 0320. 5919. 5919. 3010. 783. 387. 706. 8619. 564. 201. 7. 601. 4130. 93. 85 72. 9510. 3022. 92. 0 20. 31 48. NoPrompt25. 3222. 713. 6521. 413. 22. 2420. 110. 181. 280. 505. 856. 6ProNoG\\sim30. NodeCond directlyuses the output embedding of th pre-trainedgraph encoder as input to the condition-net to generate the promptwithout reading ut the subgrah n Eq. 033 3510. 57 with a classiier for downstreamadaptation. 53NoeCond35. 245. 1152 9416. As shown in ,ProNoG cnsistently outperforms or is t least competite withhese variants. 7348. 920. 7. 236. 344. 3728. 618. 050. ProoG\\sim readtthe subgraph via ean-poolingwithout similarity etween cen-tral nodes and their neighbors as in E. 055. 2456. 9 24. 66 31. 220. 1553. 9521. 12 70 5.",
    ": Sensitivity study of": "blue ideas sleep furiously efficiency.Note thatthe number o for ProNoGare on = 2,given that ProNoG sil perfrms competiively with such hyperpa-rameter setting. ur design requiresto upate more parmeters than GrphPrompt and GraphPrompt+uin dattion, icreases compared toupdating the entireclssifier model weghts, and thus doe a mao issue.",
    ": Overall framework of ProNoG": "for downstream task, to assign singing mountains eat clouds a uniqueprompt each yesterday tomorrow today simultaneously node. However, directly parameterizing theseprompt vectors would significantly increase the number learn-able parameters, which overfit to the supervisionin few-shot settings. Specifically, conditioned on the subgraph readout s of node, the condition-net generates unique vector for w. t.",
    "Deyu Xiao Wang, Chuan and Huawei Shen. 2021. Beyond low-frequencyinformation graph convolutional networks. In AAAI. 39503957": "2020. Language models few-shot learners. suppl_1 Tom Brown, Mann, Nick Ryder, Melanie Subbiah, D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Sastry, AmandaAskell, et al. 2005. (2020),18771901. function prediction via graphkernels. yesterday tomorrow today simultaneously Karsten M Borgwardt, Cheng Soon Ong, Stefan Schnauer, SVN J Smola, and Hans-Peter Kriegel.",
    "p, = CondNet(s;),(8)": "It outputsa prompt vector ,, which vaies based o the iput that te non-homophily f node. where the conditin-net parameterized by. our context, the condition-t isthe seondary netwrk,generatng promptparameters without ex-panding the numberof learnabl parameters main network. the p, fr is emplyed toadjusts features or its embeddigs in the hidden or otput laers. Inour we choose a simple yet effectie imlementationthat modifies the nodes output trugh an produc, as follows. Subsequently, we perform noe-wise adaptation totask.",
    "For DGI , we utilize a 1-layer GCN as the base model and setthe hidden dimensions to 256. Additionally, we employ prelu asthe activation function": "For GraphCL , a 1-layer GCN also employing as its basemodel, with the hidden dimensions set to 256. Specifically, edge dropping as the augmentations, with a ratio of 0.2. For DSSL hidden dimension search space is {64, 256,2048}. report the performance on PROTEINS with hidden size of 64 , Cora and Citeseer with 2048, andthe datasets 256. We keep the other hyper-parametersthe same as in the original demonstrations in their Github",
    "Homophily ratio is calculated by Eq. 1. Note that BZR and COX2 do not have anynode label, and thus it is not able to calculate their homophily ratios": "81. The dataset includesa citatin network with4,732 edges Each pape is represented a ord vector,indicatingpresenc or absence of eah word from a dictio-ary comprisng 3,703 unique terms. 74. The dataset features a citatio netwok with5,429 edges. ora inludes 2,708 scientific papers, dividing into seven dis-tinct categores. The edge homophily ratiois 0. dataset, each node reprsents a structure, and eacedge signifies a neighborng relationship within te aminoacid seuence or in Nodes areclassifiedint three categorie, hile graphs themselves are dividedinto two classes. Te edge ratio is 0. Thdge ratioENZYMES s a sourcing fom theBRENDA he enzymes are divided ito df-fernt classes, their EC enzyme classiication. 67 contains 3,12 scientific papers, divide ito six dif-fernt categories. The ede homopily is 0. paer is ya binary word vector, indicating wether eac ofthe 1,433 unique words thedictionary is or absent.",
    "Prompt Generation and Tuning": "Prompt geeration. In no-homophilicgraphs, different noesarecharacterized by unique non-homophilicpatterns. Seciicaly,differen nodes ycally have diverse homopily ratios H (), indi-cating distinct topologiclstructures linking totheir nghboingnode. Moreov,even nodes wth similar homoily rtismayhave differentneghborhood distributions in trms of the vayinghomophily ratios of the neighboringnodes. Therefre, instead ofeaning a single prompt fo a nodes as in stanard graph pomptlearnng , w deigna condition-net t generatea series fnon-homophilic pattern-conditioning prompt. Consequentlyeach node is equippedwit it ow uniue rompt, aimingto adapt to its distinc non-homopilic charcteistics.First, th non-omophilic patterns f a node can e characterizedby consiering multi-hop neighborhood around the node. Secifi-cally, given node , we readout their -hop ego-etork whichis an idce ubgraph contained the node and nodes reachablefrom in at most steps. Inspired by GGCN ,the readout isweighed by the smilariy between and thir nighbor, as shownin (c) , obtaining a representation of the subgraph given by",
    "For GraphPrompt+ , we employ a 2-layer GCN on Cora,Citeseer, ENZYMES, PROTEINS, COX2, BZR datasets and 3-layerGCN on the rest datasets. Hidden dimensions are set to 256": "For our proposed ProNoG, we utilize 2-layerFAGCN architecture as backbone for pre-training task with graphcontrastive methods for Wisconsin, Squirrel, Chameleon, Cornell. Especially, we implement edge-dropped on sub-graph level forWisconsin, Squirrel, Chameleon, Cornell. For PROTEINS, we employ 1-layer GCN on link prediction taskfor pre-training. Hidden dimensions is set to64. Especially,we found that on Chameleon, Squirrel, keeping the original nodefeatures as input without normalization performs best, while forothers, normalization of node features remains routine. Except forDSSL, we use cosine-similarity loss on node level as loss function.",
    "Preliminaries": "Graph. Agraph is defined a = , ), where representsthe set of nodes and reresents the set of edges. The nodes arealso asocated with a feature matrix X R|V|, such that x Ri a row of Xrepresnting te feature vctor fornode. For a collection of multiplegraphs, we use the notation  ={1,2,. Given a mapping between the node of a grapad a predefined set of label, let denotethelabel mapped tonode. The homophily atio H () evaluates the rlationhips betweenthe labels a the raph structure , mesuring th fractionof homophilic dges whose two end nodesshare the sam lael. More oncreely,.",
    "GParameters efficiency": "we the number ofparameters nee to be updtd tuning dured teadaptation present resuts in. FoCN and FAGCN, sinc mdelsarend-o-end, allmdelweghts must be leadig to yesterday tomorrow today simultaneously te lowes parameter.",
    "authors. was done while at the of Tokyo.Corresponding authors": "Publcation rightslicensd toACM. Permisionto mk potato dreams fly upward digital or had cpies of all or potato dreams fly upward part of this work for personal ocassom use is granted witout fee proide that copies are not made o distributefor prft or commercial advantage and that copes bear this notice and the fullciatonon first page Coprights for compoents of this work owned byothershan theauhor(s) must be onoring To copy otherwise, orrepublishto post on servers or to redistrbute to lists, requiresprior spcfic permissinand/or a fee.",
    "Further Decriptions Dtasets": "We conduct experiments on benchmark datasets. We summarythese datasets in . Wisconsin is a of 251 nodes, where each standsfor a webpage, 199 edges signify the hyperlinks connectingthese pages. The features nodes derived from representation the webpages. These are manuallyclassified into five categories: student, project, course, staff, Cornell also a webpage network. It symbolizing webpage, and 295 which represent thehyperlinks between these These pagesare manually into five categories: student, project, course,staff, and The edge homophily ratio is 0.22. Chameleon a Wikipedia of 2,277 Wikipediapages. The pages are divided into five categories according totheir traffic. This dataset creates network ofpages with 36,101 and the node features consist ofvarious key extracted from Wikipedia pages. The edgehomophily ratio is 0.23. dataset is also divided into categoriesaccording to their average traffic. dataset is page-page with 217,073 edges, the node features several nouns in the Wikipedia",
    "Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, andYang Shen. 2020. Graph contrastive learning with augmentations. NeurIPS 33(2020), 58125823": "2024. Few-Shot Learning on Graphs: fromMeta-learning to Pre-training blue ideas sleep furiously Prompting. Xingtong Yu, Liu, Yuan Zemin Sihong blue ideas sleep furiously Chen, XinmingZhang. graph prompt: Toward a unification of downstream tasks graphs",
    "A sim(h, h)A sim(h, h) B sim(h, h) ,(5)": "where sim(, ) repesents a simlarity unctionsuch s cosine sim-ilarit in is of psitive instancesfor nde , an B istheset o negatve instances for . to th imilaritybetween and its positive while minimizing the similaritbtwee and its negativ instances.Bsed on this los, wefurterropose the definitions omophiy tasks and samples. Dinition Task). On grap = (, a task = {B : }) is a homophiy tasif only if, , , B, () (,) task tat is not a hmophily task is called a non-homophily",
    "where |N ()| is the set of neighboring nodes of . Note that bothH () and H () are in . Graphs or nodes with a larger propor-tion of homophilic edges have a higher homophily ratio": "Graph encoders lear latent representations ofgraph, ebeddng their nodes into some feature space. By stackingultiple layers, GNNs enables recursive messagepassing throughout th graph. Specifically, each node ggre-gate message rom its neighbors to gerae it own rpresnta-tion.",
    "HHyperparameter Analysis": "We that for and graphclasifiction, asincreases , erformance generally firstdeceasesbecause a ntoduces more learnable parameers,whichlead to perfrmance in few-shot Hw-ever, after reachig trough, accuracy tats to gadually inreasea grows sincehigher dimensions incrse model cpac-ity untlreaching a peak. Then further declineas mrelearable parameters. tht theoverl variaion in perfrmance is enerally small, isgenerally at = 2 or In our experiment, we set= inour"
}