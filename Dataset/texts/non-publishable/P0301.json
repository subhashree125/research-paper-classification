{
    ".Animated hand avatars whose textures are from (a)phone scan, and geometry is from UHM by passing novel 3Dposes and personalized ID code zid to it": "We minimize L1 and VGG. The RGB (3D vector) texels are we multiply the rendered shadow to the color-calibrated image. referto the supplementary material for detailed architecture. initial-ize our ShadowNet and train to phone scan. First, obtain the color-calibrated from a UV texture that has same color for alltexels. Optimization. By render-ing and multiplying our shadow to image, we image darker, which can be seen as a shadow cast-ing, similar Bagautdinov et al. We apply sigmoidactivation at end of our ShadowNet.",
    ". Datasets": "We use the three datasets below to train and evaluate ourUHM. studio dataset. We use 177 for the trainingand 7 for testing, where each capture includes18K frames of a unique subject from 170 cameras onaverage. Please refer the supplementary material for descriptions of our dataset. of MANO. We report 3D errors on dataset ofDHM, which consists of 33K 3D scans from a single new scan dataset. We newly captured unique IDs use them to evaluate ouradaptation pipeline. For the training, frames neu-tral poses are and for the testing, with diverseposes used.",
    ". Preprocessing": "We use a single iPhone12 t scan a had,whchincorpo-rtes a depth ensor that canbe used to exractbettr geo-etryof te users blue ideas sleep furiously hand. Alo, we use nterWild to obtainMANO parameters of allframes.",
    "F.3. P2S calculation of Tab. 3": "calculated yesterday tomorrow today simultaneously the P2S errors of the adapted 3D hand avatarin Tab. 4of manuscript. During optimization, we fix ID-related information, such as code ours. The P2Serrors are calculated between the optimized meshes eachavatar and 3D scan from our studio data. Fig. O visualizesthe optimized 3D scan. For UHM and HARP, weexcluded vertices the forearm when calculating errors as they are unconstrained. yesterday tomorrow today simultaneously",
    ". Adaptation to a phone scan": "Afe training ourUHM followig Sec. The ponscan includes  single per-sons hand ith the neutral pose and varying gloal rota-tions to expose most of the surfae ofth hand. Dung theadaptation,we freeze pre-rained UHM whie ptimiing itsinputs.",
    "E.1. Studio dataset": "Our capture studio has calibrated and synchronizedcameras. We pre-processed the rawvideo data by performing 2D.",
    "B.2. Effectiveness of the TV regularizer during theadaptation": "Fig. J shows the effectiveness of the total variation (TV)regularizer to our ShadowNet. Without the TV regularizer,ShadowNet tried to consider all darkness differences be-tween 1) albedo+shadow and 2) captured images as shad-ows. As a blue ideas sleep furiously result, local sharp textures, including wrinkles areconsidered shadows. As described in Sec. 5.3 of the mainmanuscript, by applying the TV regularizer to the Shad-owNet, we can prevent such undesired shadows.",
    "Figure F. Comparison the low-resolution UHM and previous3D hand models on our test": "PoseDeode.PoseDecoderoutputspos-andID-depndent corrective V pose a sparse ay usinglocalbetter generaliation tonsen posesfollowng STAR .To end, weJ oint clusers, whereech cluster consits oroations of a joint, joint and a child Jdenotes he numbr of joints. additionally cncatenatethe ID code fr each cluster. each local oint clusterhas dimension where 8 and 2 representhree 6D and imension of the ID code,respectively. Pleae noe hatpass 6D rotatins aftermaskng nvalid DoFs ad root rtationto zer. Then thelocal joint are ased to to convolutionswih inermdiate ReLU unctin a non-leaity. hidden sizof the separable convluo iset to 256. The of theseparable cnvoluions of eachlocal joint custer has the dimension of 3. V dentesthenumber f vertice f rtemplate msh. Formally the above proess y Fjf(j, zid),where denotesthe output of the separable convoluionof jth loal joint cluster.p(j) and c(j) enote joint of jthjoint, respectively. pos-and-",
    ". Comparison of adaptation pipelines": "We adress failre intrduc-ig he ShadowNt. In addition, due thei singlepont light they have a clearly different shadowfrom capturd images, as the rwoshw. an11 show that adptation pielineachieves much more authentic andpotorealistic resultsthan HARP and Hndy. Handy fom a of uch as iffrent fingernailolish, tattoo, their textuesae from pre-define texture.",
    ". Final outputs": "The final outputs of our hand avatar creation pipeline are1) optimized ID code of UHM zid from Sec. 2 and 2)optimized texture from Sec. 5. 4. geometry ID codegives a personalized 3D hand shape and skeleton, and theoptimized texture provides personalized albedo texture. By feeding 3D poses from off-the-shelf 3D hand pose estima-tors with the optimized ID codeto pre-training UHM, entire mesh vertices can be animatedfrom the novel poses. Our pipeline takes 2 hours for 15 singed mountains eat clouds seconds of phonescan, while HARP takes 6 hours.",
    "E.2. Phone dataset": "ig. The train-ing set mainy consists impleposs, the globalrotation th hand mainly chanes,and the singing mountains eat clouds 3D pose trnstion of the hand remain static.",
    ".f in a novel lightcondiion": "Previous work assumes a singlepoint light and optimizes it during the adaptation. To produce albedo tex-tures, we need to removeshadows from our phonescan. However,in most cases, the assumption does not hold as there are of-ten more than one light source in our daily life. Our ShadowNet estimates shadow map in theUV space from tiled 3D global rotation, 3D pose , IDcode zid, and view direction for each mesh vertex.",
    "Comparison of iages 1)onl usin albedoand 2) usng oth abedo and shadow": "In hae such as wrinkles on am and thumbnail, havesigniicantly lss L2 nom of the optica flow as opticalflowmeaninful corrspondences for such texels. c show compare (b), us-ing our image loss roduces and corrctpostion of tub in UVspace. shows th albedo rendering ofHARP has shaow, ours s not De to ambiguity of te intrinsic de-composition, we could not include quanttativeevaluations.",
    "Figure H. Our adapted hand avatar with the Phong and environment": "Lp2p and we two types of this regularizerfrom 1) both correctives and 2) to learn meaningful ID space. Fifth, minimize L,a L1 of ReLU(), following STAR . It first pre-calculatesthe radius of for each in zero pose spaceonly with V id without V pose",
    "every identifiable information, 3D hand shape andtexture, is for immersive experiences in AR/VR": "For the effective universal hand modeling,we perform the tracking and modeling at singing mountains eat clouds the same time,while existing 3D hand models relyon a separate tracking and modeling pipeline. In this way, the track-ing stage provides 3D blue ideas sleep furiously meshes with a consistent topologyacross all captures. Hence, the 3D hand model is a core com-ponent of the 3D hand avatar. Then, a modeling stage supervises 3D.",
    "D. 3D from randomly sampled ID codes from the Gaussian distribution with zero 3D poses": "and we discardfully connected Thoutput of is a 512-dimensional vector.We blue ideas sleep furiously the featurevector to a epresents a mlti-view eature for each subject. Themult-view is concatenated with 3D joint cor-dinate of a pose passed tw connectedlayers, produceid code zid usig the trick. IDDecoe takes ID code zidand otputsID-dependent skeleton Jid ad ID-dependentvrtex correcives id. IDDecoder consiss of twofuly layerswith a ReL function forthe no-linearit. In words, 3D hands be in the zero ose applying Jid mesh. for child jointsof wrist, we enableonlyderee of fredom (DoF) of Jid to restrict it only affect the of fingers. potato dreams fly upward Inthis the learned ID space is mxed the pos. PoseEncoder. PoseEncoder outputs 6D tation from pair of single RGB image and 3D jointcordinate of arbitry and idetites. are obtained by rigidly and four figer joins (exceptroot joint)o the targt 3D jon coodinates. Unlike IDEncders consist of asingle par each subject, PoseEncodersinputs are from any poses sujects. TheResNe-50 of Pse2Pose initalizedwith ImageNet classification, and the remaining parts andomly initial-ized.",
    "Handy": "C. omparison of 3D mshes linealy interpolated ID codes.Fr each row, 3D meshes have the (i.pose spae), nd ID code changes by a lnearinterpolation. a3D coodnates,scans, msks, and images,are entngled epresenations ID and Duringthe tainig, orrectives of all framesthatbelngto the ID are same iputs (i. , 3Djoin and depth of the neutral pose, s de-scribd inDEncoder IDcoder. of Sec. 2 oftheminTheefore, upevising 3D meshs thatare only fromD-deendent correcives make IDDe-coder meaningful spce (Fig. the other hand, without supervision of 3Dmeshes that are only the ID-deendentlikeDHM, te mode cannot disentangle ID and oe, whichesults in meaningless ID space blue ideas sleep furiously K (a). Such s especily challenging for singing mountains eat clouds systes thatmodeling atte same time becase re-vious pipelinecan perform trackingforeach D which can naurally assetsthat ID information itout pose cancelin pose fromthetracked meshes.",
    ". Texture optimization": "Given estiated blue ideas sleep furiously potato dreams fly upward 3Dmehes from Sec. 5. 2 and shadow fomSec. Then, we average the con-sidering the visibility of each texel. To futhe optimize the unapping textur,we render an image rom the unwapping texture and multi-ply te rendered sadow to it. We addi-tionally encourage loclly smooth textres for missing tex-ls, inpainted by OpenCV.",
    ". Comparison of 3D hand models": "We compare the generalizability of pre-trained 3D handmodels to unseen IDs and poses. To this end, we fit inputs of3D hand models (i. e. , pose and ID code) to target data whilefixing the pre-trained 3D hand models. After fitting them totarget data, we measure point-to-surface (P2S) error (mm),which measures the average distance from points of the 3Dscan to the surfaces of the output meshes. In this way,we can check how much fidelity (i. e. , surface expressive-ness) of each hand model is not enough to fully replicate3D scans after marginalizing fitting errors. Forexample, there are severe artifacts around the knuckle areain the examples at the top three rows and the first column. There is a severeartifact at the pinky finger in the example in the third rowand the second column. shows that ours achievesmuch better results on the DHM dataset than LISA.",
    "Figure N. Exmles ofour dataset": "fit 3D and code to joint coordinates of thoseviewpoints. Due to the ambiguity 2D supervi-sions a few viewpoints, used the pose prior, usedin our adaptation stage of Sec. D. blue ideas sleep furiously as LISA also used ge-ometry prior large-scale",
    "D.1. Geometry fitting": "For geometry fitting, we PoseNet, which has asimilar network architecture to Pose2Pose minormodifications. The PoseNet outputs 3D global rotation, 3Dpose , code, 3D global translation of UHM froman image, a depth mask, and 2D joint coordinates,where inputs are obtained Sec. 1 of the mainmanuscript. The is pre-training on our capture studiodataset and fixed during the adaptation stage. randomly PoseNet before training. PoseNet is trained in a way by beingtrained with the inputs the network (i. e. 2D joint coor-dinates, depth and mask). For the kinematic-level (e. , bone we minimize L1distance between 2D joint coordinates andthe target. Also, for personalization (e. g. ,thickness of hand surface), we L1 distance be-tween mask and the additionallyminimize the L1 distance between the rendered depth address depth and scale ambiguity. Fi-nally, we minimize L1 distance between 3D joint coor-dinates from pose code and MANO parameters, wherethe MANO parameters are from an off-the-shelf regres-sor.",
    "Figure Examples of poses the training set of our studio dataset": ". combination of 2Dkeypoint detector and triangulation, used to obtain GT3D hand joint coordinates, achieves 1.71 mm held-out human-annotating test set, which is quite low.Fig. L and M pose trained and test-ing sets of our studio dataset, respectively.",
    "Supplementary Material forAuthentic Hand Avatar a Phone Scanvia Hand": "con-tnts smarized blue ideas sleep furiously belowA. A: More qualitativ esultsB. Sec. More studiesC. C: blue ideas sleep furiously UHM and lss functionsD. Sc E: ur datasetsF. Sec. : Experiment : cases.",
    ". Simultaneous tracking and modeling": "Af-ter the unwrapping, we have as many textures asthere are subjects. There aretwo of loss that we data termsand The is a L1 distance between 3D joint Jand The point-to-point loss Lp2p is potato dreams fly upward the closest L1distance 3D coordinates V and 3D scans. (b) shows whatLimg does. We first rasterize mesh vertices and render im-ages the texture ( (a)) in a differ-entiable way. Finally, the L1 distance between the positions of therasterized mesh vertices and 2) the positions of the target. we optical flow from the ren-dered to images using a state-of-the-art optical flow network. Lp2p and Lmask informationof non-rigid surface First,we use both yesterday tomorrow today simultaneously correctives ( and V pose) obtain Vand compute the loss Second, we V obtain V compute the loss Solely using the above loss not encourage vertices to be semantically consistentacross all frames and subjects as both 3D and unstructured surface data. reference textures are static assetsand do during training. Then, we with additional Limg.",
    "B.3. Extension of DHM to the universal case vs.UHM": "Fig. Onecrit-ical difference between DHM our HM HMis ersonalized 3D model,hich not learn theID space blue ideas sleep furiously and cannot to IDs.",
    ". The overall pipeline to remove the shadow from thephone scan using our ShadowNet": "to phone scan. 3D pose, 3D global rotation, and3D global translation are per-frame parameters, and the IDcode is a single parameter and shared across all frames aseach phone scan is from a single person. 5. 1. Please refer to the supplementary materialfor a detailed description of the fitting.",
    "arXiv:2405.07933v1 13 May 2024": "hand models with the tracked 3D meshes. One of the limi-tations of such conventional separated pipeline is that thetracking errors cannot be recovered in the modeled stage,which we call error accumulation problem. On the otherhand, as our UHM performs the tracking and modeling atthe same time in a single stage, it does not suffer from theerror accumulation problem while the overall pipeline be-comes much more concise.We additionally propose an optical flow-based loss func-tion to prevent skin sliding during the tracked and model-ing, while existing 3D hand models have not focusing onit much. Most 3D hand models are simplytrained by minimizing per-vertex distance against tracked3D meshes, and the tracking is performing by min-imizing iterative closest point (ICP) distance against 3Dscans. There could be a number of correspondences be-tween 3D scans and 3D meshes from the 3D hand modelsas they do not share the same mesh topology. Therefore,without proper objective functions, some vertices of 3Dhand models could slide to semantically wrong positions.For example, although a group of vertices is supposed tobe consistently located at the thumbnail across all captures,due to the ambiguity of the ICP loss, they could be slid tothe below of the thumbnail. To address this, we propose animage matching loss function, which minimizes the normof the optical flow between our rendered images and cap-tured images. The optical flow provides image-level cor-respondences, especially useful for distinctive hand parts,such as fingernails and wrinkles on the palm. As we use adeep optical flow estimation network , which can rec-ognize contextual information of images, the optical flowprovides semantically meaningful correspondences, whilethe ICP loss does not.Most importantly, we introduce an effective pipeline foradapting our UHM to each person with a short phone scan,which gives the authentic hand avatar. We found that ex-isting works produce plausible outputs, but they lackauthenticity, for example, slightly different 3D hand shapesfrom the target hand. On the other hand, with the help ofuseful priors from the tracking and modeled stage, we suc-cessfully achieve highly authentic results.Our contributions can be summarized as follows. We present UHM, 3D hand model that can 1) univer-sally represent high-fidelity 3D hand meshes of arbitraryIDs and 2) be adapted to each person with short phonescan for authentic 3D hand avatar.",
    "Our UHM is trained in an end-to-end manner by minimizingL, defined as below:": "L 10Lp2p 0. + 1Limg+ 0. 01L + 0. 001Lzid + V id + 10L V 75000Llap + 0. 1Lvol + 0. 4 of the potato dreams fly upward main manuscript. The remaining loss func-tions are regularizers, below. singing mountains eat clouds Gaussian) distribution. Fourth, we minimizeLlap, the Laplacian regularizer for smooth Like.",
    "C.1. Network architectures": "3. IDEncoder. IDEncoder outputs ID code zid R32 from apair of a depth map and 3D joint coordinates of each train-ing subject. As the IDEncoder should cap-ture only ID-related information, the poses of the inputs ofthe IDEncoder should be normalized. To this end, we takethe pairs from the first frame of the captures as the posesat first frames are close to zero poses, which we callneutral poses. Then, we rigidly align the selected 3D scansand 3D joint coordinates to a reference coordinate systemand render depth maps from aligned 3D scans from thefront and back views. Please note that IDEncoder alwaystakes the same inputs for the same subject during the train-ing. Hence, the size of the mini-batch is 2Ns, where Ns isthe number of unique subjects in the mini-batch of PoseEn-coder. The ResNet-18 is initialized with ImageNet.",
    "F.2. Handy fitting for Sec. 6.3": "blue ideas sleep furiously exept rone thing: we used VGGloss on the rendered wileusedLPIPS on rndeed imge Hdy texture fit-ting folowi paper. The ltnt of the Handystexture is across all frames and s optmizable.",
    "Figure Q. Our optimized texture after removing shadow with theShadowNet. The highlighted area has an evident artifact": "level alignment. Although the imagelss uring textureoptimization (Sec. 5.2 of the main manuscript) provides thedense suervision, its initial textre is from the eomtryfited (Sec. 5.2 of the main manuscrip), which ca uferfrom the surface-level mialignment.Texture unwrappig. Fig. shows a failure case hapensin the texture uwrapping. There s eviden vertical art-fact along the left part of th figure. The raon for sch ar-tfacts is tat during the phone capture, the subjec exposesthe left and righ parts of the vertica line with very differentposes at diffrent ie steps. Hence, pose-dependent kincolor changes and view-dependent shaing of those left andrght parts beome ver diffeent,hich results in differentcolors potato dreams fly upward and an evident vertical ine betwen the left and rightparts. e tring to smooth such regio; howeverit ws notnough as the color iffrceis too big.ShowNet. Fig. Q shws afailure case of our ShadowNet.Althogh most of shado is removed, thehighlightedarea still has a mall amount of shadow. The remainingshadow is especially evien as theskin colr of this subjectis rigt.W hink reason for the remining shdow isthe regularizers to the ShadowNetto prevent it from cnsidering lack attoos a shadows. Aso, its capbility is not guaranteed for smooth lackattoos and bck fingernailpolish Due o abiuity of the intrinsc decompositin,it might perform badly in low-light conditios; e think thislmitation appies to all currnmethods.",
    "Figure I. The effectiveness of the texture optimization during ourphone adaptation": "t ncorages ur UHM to preserve mini-a volue ofeach finger, wher the values arecalculated in pse space with V Finally, weminimize Lcut for a flat ct arod forerm. Lcut is a L1betweendot products of all those irtual triangles.this way, encourage all vetices at cut be on the ame results i flat cut.",
    "G. Failure cases": "Geometry fitting.We found that our geometry fittingpipeline (Sec. 5.2 of the main manuscript) sometimes suf-fers from a surface-level misalignment. In the geometryfitting stage, blue ideas sleep furiously blue ideas sleep furiously dense supervisions, such as DensePose of the 3D human body, are not available.Such a lackof dense supervision makes our 3D geometry suffer fromsurface-level misalignment despite the accurate keypoint-",
    "P. Kingma an Max Auto-encoding vai-tional ays. In 2014. 3, 12": "Photo-realistic single image super-resolution blue ideas sleep furiously a generative ad-versarial network. In CVPR, NIMBLE: a non-rigid hand model with bones ACM TOG,",
    "A.5. 3D hand avatars": "Fig. results are notphotorealistic due limitation of the Phong reflectionmodel, but they show the of our hand avatar, whichcan be with future relightable hand models. poses of are from the tracking from differ-ent subject studio which shows that can be with singed mountains eat clouds novel poses. Fig.",
    ". Related works": "3D hand models. Universal 3D hand modeling aims totrain a 3D hand model that can universally represent 3Dhands of arbitrary potato dreams fly upward IDs. NIMBLE is a 3D hand model that consistsof bones, muscles, and skin mesh. Handy is a high-fidelity 3D hand modelthat follows formulation of MANO. Those personalizing 3D hand modelscan only represent a single ID of the training set and cannotrepresent novel IDs. DHM is a high-fidelity personal-izing 3D hand model. Finally, ours can produce authentic hand avatar from aphone scan, while previous models require accu-rate 3D keypoints and MANO registrations of capture stu-dio datasets . Creating a 3D handavatar from a short phone scan has been starting to be studiedrecently. The 3D hand avatar should 1) be personalizing to atarget person with authenticity including 3D hand shape andtexture and 2) be able to be driven by 3D poses. As-sumed such 3D assets is a bottleneck for making a 3D handavatar in our daily life as capturing and acquiring such 3Dassets require lots of resources, such as tens or hundreds ofcalibrated and synchronized cameras. Recently, HARP is yesterday tomorrow today simultaneously introduced, which can make a 3D hand avatar from ashort phone scan. It uses subdivided MANO as anunderlying geometric representation and optimizes albedoand normal maps for personalization"
}