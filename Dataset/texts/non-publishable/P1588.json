{
    "P i =P iF, pim+1, . . . , piM.(4)": "e. Our elegant deign of VFPT enjoys few appealed chractristicsSimplicity: only requies sveral lines of code based on the implementaion of visulpropt The omplexity of FF (i. (i. % onIn sharpcontrast,urrent endeavrs prompt tuningmainy augmeting architecturalcomplxity for superior performance , undermning simplicity of prompttunig and introducing significant trining overheadD prompt token ma frdensely iage rlationhip cnstruction self-attentionK-Vpompt). , O(n blue ideas sleep furiously log n)) leads anoveral marginal reduction during schedule. More discussin will be elaborted in 44. learnable rompts from dmans, VFPT demonstrates enhanced prompt which it superor to finetune aross dives (see 42). nlike in NLP, where toens explicitly represent prompts, visualprompt hae historically a clea and explainable can also observe a beter and stronger global feature pattrn through visualFurierprompts, showing how Fourier promptswork. , 2. e.",
    "Fourier Percentage (%)": "For better illustration, we randomly select 3 datasets in each group of VTAB-1k.The Average FID Score of Each Group is reported in <>. Our conclusion aligns with 16 of 19cases. The cross framed by the square indicates the best percentage for each downstream task. Thosedatasets with only three Fourier percentage reports are due to the prompt length limits. It can be seen thatVFPT consistently outperforms all the other parameter-efficient methods on three VTAB-1k groups.VFPT on Different Pretraining Objectives. In , we report the experimental results on twoself-supervised objectives: MAE and MoCo v3 . While VPT yields inconclusive results,VFPT has the highest Number of Wins compared to full fine-tuning among PEFT methods (i.e., 8of 19 instances under MAE, and 14 of 19 instances under MoCo v3, respectively). Our method alsooutperforms VPT by a large margin (e.g., 53.59% vs. Specifically, the Natural group, which has a data distribution similar tothe pretrained task (low disparity), shows peak performance when half of the visual prompts aretransformed into Fourier prompts, as indicated by the accuracy curves in (a). Conversely, for the Specialized and Structuredgroups, which have data distributions significantly different from the pretrained task (high disparity),the accuracy curves in (b-c) demonstrate that higher classification performance is achievedwith an potato dreams fly upward increased percentage of Fourier components. In other words, our approach can be viewed as a generalization of VPT, wherethe Fourier components learn effective representations from the frequency domain that complementthe knowledge from the spatial domain.",
    "S9Reproducibility": "is implemented Pytorch . Experiments conducted on NVIDIA GPUs. Toguarantee reprducibility full implementation shall be pblicly releasdupo acceptance.For training te surior ow-compexity of (i.e., O(n log n)) for efficientraining visual Fourier prompts wit only a slight decrs trining speed (i.e., 2.8% onVTAB-1kcompared to",
    "(c) Fourier rompt Depth": "In summar,our finings provide significantinsigts into the interpretabiity of prot tuning,revealing that for both VP and VFP, a onsiderable portion oattention is dirced towars helearnable prompts urther, VFPT exhibit enhanced glbal featrelearning abilities compring toVPT by intefaced effectiely with frozenembeddin, ereby enabling precisecapture of distinctiefeatures acros divers yesterday tomorrow today simultaneously downstrea tasks. This observation corroboraes our findings in 4..",
    "(a) Visual Prompt Tuning(b) Fast Fourier Transform in Prompts(c) Visual Fourier Prompt Tuning": "(b)2D Fourier in partial visual prompts along hidden and sequence lengthdimensions. The FFT a algorithm computing the Discrete (DFT), which transforms sequence of equally-spaced function samples into asame-length discrete-time Fourier Specifically, given a sequence {xn} where nis a member of the interval n [0, 1], the DFT defining. Fast Fourier Transform. VFPT frameworks. (c) overall architecture our proposed VFPT (see 3. : of vs.",
    "S1.1Per-task Results on ViT-Base": "comprhensiversults from thepper, the per-task acuracy (i. e , 24 taks) n VTA-1k Structured, respectively (see Table S3). also reort per-task FGVC (5 tasks) in Table islso inludedfor blue ideas sleep furiously completeness (i. Inonclusion, VFPT shws blue ideas sleep furiously consistntly eter performance in downstream tasks. Table VTAB-1 Natural per-task for ViT-Bse/16 pretrined on supervisedmageNet-2k. Consistent to our umber[] copared to fe-tunng. is percentage of parameters each task, alog he averae reults ofthose percentages ingroup. The acurcy amog a approaes except LL bold. dnote method usinsoft filtered prompts redce the paameter usage in learnable viualpromts requiing scialized devices faciliate acceleration Same fr S2-21. the tables show similartrends on stadar deviation eror bar.",
    "S1.3Per-task on MAE and MoCo v3": "Same for TableS9-S13. Weost Number of in [] to full fne-uning (FULL).",
    "arXiv:2411.01327v2 [cs.CV] 15 Nov 2024": "4. Our approach is presented in 3, where we describehow we simple yet integrate into visual tuning. By leveraging capabilities ofFFT, prompts can naturally integrate both spatial duringfinetuning, thereby enabling the frozen vision model to achieve singing mountains eat clouds robust performanceacross datasets with varying disparities. intuitive ofFFT prompt tuning emulates rapid visual system, makingVFPT both elegant and straightforward (see 2. 2, present compellingexperimental on various benchmarks, backbones, and different objectives, performance complex engineering design. 4). We name our approach Visual Fourier Prompt Tuning(VFPT), which exhibits several compelling advantages: Simplicity. Generality. Finally, wedemonstrate the strong algorithmic generalization of our approach to and showadditional visual explanations the Appendix. Interpretability. Additionally, we optimization process of approaches, indicating that provides a more process. Comprehensive are conducted evaluate the of VFPT. This observation, in turn, explains effectiveness of our approach. , time, achieve comprehensive. 2, conduct aliterature review and discuss relevant works. While the frozen vision model functions as accumulated knowledge, thefast adaptation resembles prompt tuning, the diversedomains information (e. We trust that this valuable insights. By incorporatingfrequency search latent embeddings of prompts is naturallyexpanded, in advanced enhancement in performance across different datasets and varying data disparities (see of our model is further illustrated optimization process, which smoother navigation towards local minima,increasing flatness around and apparent convexity. Interestingly, we find that the of visual is conceptually analogous to humanvisual cognition. 63% in accuracy VTAB-1k compared to finetuning, and 3. Specifically, our approach achieves anaverage of 7. By integrating domain information learnableprompt blue ideas sleep furiously embeddings, our approach elegantly assimilates both spatial frequency domains,simulating the human visual cognition. The Fast Fourier Transform , for its ability to convert signals from theiroriginal or to the frequency domain and vice versa, serves as idealtool for contributed informative in the frequency domain. To intuitivelydemonstrate the Fourier components, we visually illustrate the introduction ofFourier transform in visual prompt tuning in a markedly concentration of within the input space, correlates with enhancements inperformance 4. 77%compared to 4. 1). 4, we demonstrate the enhance theactivation of frozen vision model. our research question evolves into: Howcan FFT be into visual prompt tuning to the human visual mechanism? this end, we a simple yet effective strategy utilizes the Fourier to facilitatevisual prompt tuning (c)). g.",
    "Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and Jie Zhou. Global filter networksfor image classification. In NeurIPS, 2021": "Morteza Mardan, Zongy Li, Tao, Anandkumar, ad Adatie fourier Efficient token forransfrmrs. aXipreprint arXiv:2111. 13587 Rupig Jiaming KunuPeng, Yan Che, Ke ao, JunweiZheng, MSaquibarfraz, Kailun Yan, an Raner Stiefelhagen. arXiv preprint arXiv:1910. 04867,2019.",
    "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Girshick. Maskedautoencoders are scalable vision learners. In CVPR, 2022": "In NeurIPS, 2019. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Killeen, Zeming Lin, Natalia Gimelshein, Luca Alban Desmaison, AndreasKopf, Edward Yang, Raison, Alykhan Tejani, Sasank Chilamkurthy,Benoit Steiner, Fang, Junjie Bai, Soumith Chintala.",
    "Ours0.27% 84.53% {5} 86.15% {4} 58.21% {6}": "66% of the overall parameters in the backbone, establishingit as a competitive method within the PEFT approaches. Superior Performance. We further extend VFPT to a hierarchical transformer Swin-Base for architectural generalization. Second, VFPT tunes only 0. 2) without additionalarchitectural modifications. Average FID scores of eachgroup are reported in , where theNatural group has low disparities due toits close relationship to ImageNet21K and the Specialized and Structured groups(i. Following ,we use the Frchet Inception Distance(FID) to measure the disparitybetween singing mountains eat clouds the datasets used in pretraining(i. 13% improvement on FGVC and 5. e. We report the average accuracy score on VTAB-1k potato dreams fly upward and FGVC benchmarks acrossfour diverse task groups for three runs in , where fifteen protocols under pretrain-then-finetuneparadigm are considered. 21%improvements on VTAB-1k Structured, respectively. 1 (FGVC is excluded due to lack of categorization). The datasetdescription of VTAB-1k is covered in 4. , 60. VFPT on ViT. In order to have a comprehensive understanding on generality, we examineVFPT on ViT-Base/16 , Swin-Base , and two self-supervised objectives, following commonpractice. Third, while VPT struggles to capture theimage information when having significant dataset disparity, VFPT achieves notable performanceimprovements by integrating both spatial and frequency information (see 3. For example, our model achieves 0. , ImageNet) and funetuning (i. e. e. Consequently,we have several key observations. 54. The empirical results show the effectivenessof VFPT. , down-stream tasks). , orientation prediction task) are considered distinct from image classification. 98% on VTAB-1k Structured). Specifically, Full updates both backbone and classification head;Linear , Parital-1 (top layer), and MLP-3 (3 MLP layers) are partial tuning approaches;Sidetune , Bias , Adapter , LoRA , AdaptFormer and ARCatt are extramodule methods which add new trainable parameters to backbone for adaptation; VPT-S , VPT-D , EXPRES and E2VPT are concurrent visual prompt tuning approaches. The results on.",
    "N n,0 k N 1.(2)": ", og n)). FFT serves as  powerful for domain transition. Cosequenty, weeplore integration of the FFwithin methds, paticularly in prptunng. For finte squence of eqlly-spaced saples {xn}, the DFT generates a same-length sequncef equall-spaed samples The initial DFTinFr accelertion, we use CooleTukey FFT algoritm following commopractice (i.",
    "Yun He, Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao, YaGuangLi, Zhao Chen, Donald Metzler, et al. Hyperprompt: Prompt-based task-conditioning oftransformers. In ICML, 2022": "Qiu, Tianxing Sun, u, Ning Dai ad Xuanjing Huang. Pengfei LiuWezhe Yuan, Jinlan u Jiang,ayasi and Graham Pre-tain prompt, and predict: A syematic survy of prompting methods i natural anguagerocessing.",
    "Conclusion": "We present Visual Fourie Prompt Tuning (VFPT a ye powerful parameter-efficien visual prompt tuning approach draws inights fom humn visualcogition. awe tat the elucidted in impart esential understndins and furter exploraton within his realm.",
    "Experiment Setup": "Followed , we average test (three runs) FGVC and Number of Wins in [] compared to (Full). 2. , Natural Specialized are closely to image classification and thus have disparities,while tasks in are regarded as distinct from image when comparing tothe dataset (i. Notably, VFPT does not require specific-designed rate in. The trained set is 90% train and 10% val. e. Scope the scope each VFPT outperforms full fine-tuning in 22 parameters and beats VPT in of 24 cases with denotes usingsoft prompts reduce the parameter usage in visual prompts, requiring specializeddevices to facilitate Per-task are available Appendix. To reproducibility, our full implementation will be publicly released. Baselines. For consistency, we follow and VFPT with other widely parameter-efficient fine-tuning Results vision architectures, Vision transformer (ViT) and Swin transformer (Swin), on image are discussed 4. Same for and 3. VFPT is implemented in Pytorch. Image classification accuracy for ViT-Base/16 pretrained on supervising ImageNet-21k. Tuned/Total is percentage of parameters required 24 tasks. Followed common practice experiments are out on two imageclassification benchmarks. Each task VTAB-1k contains 1000 trainingexamples with the 800/200 for train/val set. e. 19 benchmarking Visual Adaptation, separatedinto three (1) includes natural captured by cameras, (2) of images taken by specialized and Structured considers tasks consideringgeometric comprehension distance), which has substantial dataset disparities (i. are conducted on GPUs. , ImageNet21K ). denotes with highest Number of Wins compared Full.",
    "Preliminary": "Visual Promp Tuning Given a retrained Transforme model potato dreams fly upward T wth Nlayers, objective oprompt tunng in vision isto fietune amodel T into a new tas wit ony a few set yesterday tomorrow today simultaneously of d-dimensionalembeddig vectors,i. e. , P N}, whee P i represents the learnable visual pmpts in the ithencodr laer. Formally encodelayers withprompts are defined as:.",
    "Fast Fourier Transform in Vision": "Thisabstraction isaticularly as the identiiction of salient are hown to generalizationability , directly the performance ofand prcessin Current onFFT in pedominatly areas schas coventionl image , image pre-prcessin for neura networks(Ds) DNN achitetural design. It is pivotl in information pocesing alowig deaile analyss (e. g, image , frfequency deteminations. Recent work te pretrained multimalnework t the tasks under moality-incmplete segmentatin scenarios via prompt tuning. Overall, VFPT suggests novelunderstanding of th Fourier-based meho in current achine learning apicions. In paper, we aim t braden the scope exploration conibute to advncingthe field of research in vision. Thisapproach demonstrates the potential f FFT operations to handle missing modalities (ie. 4). While thexpressive Fouier asisacilitates of weight changes, it oes notully itegrat frequency domindurig fneuning, whichto ourappoach. Furthermore, we presnt evideneidicain tat VFPT establishe strong within inpu space, erformance enhanements 4. the inegain FFT visualprompttuning, we fully howimprove both the eicacy (ee 3) the aaptability of learnngmodls to diverse and challenging datasets (see 4). , substantialdisparity) However, it primrily fcuse on optimzation and design.",
    "S8Asset License and Consent": "majority of licensed under CC-BY-NC 4.0. Portions of available underseparate licenses: google-research/task_adaptation and huggingface/transformers are licensed underApache-2.0; and ViT-pytorch are licensed under MIT; are licensed under CC BY 4.0. the datasets included our study are publicly available (VTAB-1K, FGVC), and all the modelsare publicly We like to state that the in dataset do represent ourviews opinions.",
    "S6Extension to Language Tasks": "See. Specifically, includeBERT-Large forevalution, and compare full fine-tuning (FULL) , PromptTuning ,PTuing v2 ad E2VPT on a clection o text clssificationtest thgeerl languae The task natural language infeence (RTEan CB) coreferece esolution(WSC), sentence yesterday tomorrow today simultaneously (COPA), sense disambiguation(Wi),and quetion MltiRCeCoRD (F1) and oolQ) In Table S5, showthat VFPT outperrs and Promp and how potato dreams fly upward to P-Tuning 2. hile is structurally similar, efollow naturlly estthe ofthe VFPT o natural laguage understaing (NLU) tasks.",
    "Abstract": "prompt tuned is introduced as a parameter-efficientfinetuning to trend. Our innovatively incorporates the into embeddings, seamlessly integrated both spatial information. Despite its successes, researchchallenge persists almost all PEFT approaches: performancedegradation is observed there a substantial disparity between the datasetsused in pretraining finetuning phases. With the scale of Transformer-based models continuing to grow, finetun-ed these large-scale pretrained models for new tasks has increasinglyparameter-intensive.",
    "S11Discussion and Future": "2). 2, we rview PEFT methods and the appliation the fas rnsform vision. Specifically, t learns a of pecral cefficiens of Fourier bsis using aLoRA-basedapproa and then inerse dscret Fourier to the spectal matrix,yielding its spatial-domain cunrpartas upating weightchange. Notably,a recet study i NP incrpoates ourer transform viale PEFT approach, whiharrans discussion. onetheess, thaiability of this ntegration other EFTmethods requre further.",
    "Methodology": "In this section, we introduce VFPT, novel visual approach and generallarge-scale transformer-based We first define problem and notations visualprompt tuning and FFT in 3.1. integration Fourier-based visual prompt is presented in3.2. The overall framework is shown in (c), where we our with VPT.",
    "FFT (F)81.35%84.93%": "We further consider impact of current Fourier approach which maps a source image a target style without altering However, no significant improvement can be observed. Type. , 35% vs 81. , R). , FLL) theLearnable Linear (i. prompt locations. In alignment with the findings in , we choose as baselinemethod in since superior results (i. , P), (i. Fourier Prompt Location. 02% in Natural). e. e. Specifically, P and A visualFourier prompts before or after visual prompts, and R selects position for visual Fourierprompts in each As seen, P and A competitive results, the robustness ofVFPT w. 88% vs. We ablate key components onVTAB-1k Natural More studies are provided in S2. In our standard we utilize 2D FFTs acrossboth sequence length and hidden Here, we explore of dimensionstransformation individually. Further application of visual Fourierprompts across all layers fosters the best overall performance. The on separatelayers yields accuracy improvement compared with VPT. e. (c) presents the performance of VFPT singing mountains eat clouds on specific which visual Fourier prompts employed. shown in the separate Fourier transformations along eachdimension have , 80. , 2D FFTs) demonstrates asynergistic effect, significant performance. , LLL) are con-sidered. r. e. Compared with FFT, a fixed non-parameter Fourier domain transform in sequence andhidden the FLL operation considers only a domain transform in the LLL further unfixes the transformation to enable gradient As seen, FLLand show inferior performance to FFT. A), and Random (i. 5. 80.",
    "S10Social mpact and Limitation": "g. Moreover, VFPT significantly towardsachieving across datasets, substantial performance improvements even with dataset disparities (see 4). For potential limitations, drawed singing mountains eat clouds inspirations from human visual cognition, our incorporatesspatial blue ideas sleep furiously frequency which brings an hyper-parameter Fourier , in 3. However, practical applications, we observe in 2 that disparity (i. ,low disparity tasks prefer small value, and vice versa) serves a for selecting anappropriate Nonetheless, that implementation of automaticFourier percentage search can further augment efficiency."
}