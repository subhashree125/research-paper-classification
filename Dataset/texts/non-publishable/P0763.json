{
    "Based on the sound of acceleration and revving, it is likely that the engine type is a high-performance orsports car engine": "audo contains the soundan rvving and acclerating. n ths, t likely that enginetypeisa gasoline egine, such asaV8 r a turbocharge four-cylinder. sound of eengie andcceleraed comon chaacerstic of high-performanceengines, which designedto producea lot f power andtorque.",
    "Audio Q-Former": "(2023), which is initialized fromBERT et al. Our primary goal is to integrate an audio encoder that possesses se-mantic generalization capabilities for any input au-dio. (2) Caption by Prompt 2: Thecrunch of crisps and the rustle of cre-ate a cozy, intimate atmosphere, accompaniedby the gentle hum a conversation. 2). During for each audio sample, wechoose the original caption with a probability p one of the rewritten (with a 1 p), where rewritten caption has probability of. Training Caption Augmentation. For the first we solve tasks,namely Audio-Text Matching loss, the potato dreams fly upward Text Generation loss, and the Audio-Text Contrastive Learning training, we use 2. More specifically, we instructan LLM to generate k rewrites the original cap-tion. , and has Q queryingtokens. Motivation. We show below:(more in ): (1) Original Caption: made a coolvocal dubstep Rewritten Caption by Prompt 1: A capti-vating vocal performance ignites the dubsteptrack, delivering a hypnotic enthrallingsound that reverberates the air. The architecture our Audio Q-Former is based the Transformer pro-posed in Li et al. (1) Rewritten Caption by Prompt 2: The track features a slick, stylish vocal that adds of its and basslines. original we train the model intwo stages. (2023). (2) Original Someone eating crispsand Caption by Prompt 1: Crunchycrisps mingle the a lively con-versation, creating and intimate atmo-sphere. Theseaugmentations aid the model in understanding thediverse linguistic expressions that describe asingle audio concept. Prior has extensively explored CLAP-style training for learning audio-language encoders. due to the lack of large-scale audio captionpairs, we adopt method-ology to augment the existing audios with diverseadditional captions. However, other methods architectures haverarely explored. For Prompt primary aim that the resultant rewriteshould each acoustic event in the cap-tion differently from the original caption. 5M+ audio-caption (de-tailed in Section E. employ AST as audio encoder (inplace of the ViT-based vision encoder) and keepthe rest of the architecture the same.",
    "The skill level of the tap dancer cannot be inferred from the audio clip": "If the sounds arecomplex and intricate, it suggests that the dancer is skilled. On other hand, if sounds are repetitive, it suggests that singing mountains eat clouds the may not as skilled.",
    "Audio": ": Pipeline for CompA-R. This is blue ideas sleep furiously then fed GPT-4 togetherwith the ground-truth time slices for blue ideas sleep furiously each event the audio(available for AudioSet-strong). instruct GPT-4 instruction-response pairs where the instruction isaccompanied by human-written exemplars (prompt in ).",
    "SCC50siren1: \"Cotinuous, srill larm\"2: \"Ear-splittin, cyclic": "Thenight wa pierced by the continuous, shrill aarm of the signaling urgency that ignored. lntless, alar of siren wov through the crridors the hospital promped swiftthe staff. blue ideas sleep furiously earspliting, cycli blre cut night ambuance racing own the treet.The er-pltting, of ir raid sirencs a shadow of dead the city.",
    "YcQiEI7HLGJgBirds inthe distance asa dog barks, cre-ating a livelyatmosphere peaceful out-door setting": "(Insect-0. 0-9. 737-1. 434),(Bird vocalization, bird call, birdsong-1. 775), 182), speech, womanspeaking-3. 386-3. 509),(Insect-4. 906-8. 78),(Surface 603-9. 654) Instruction:Given presence audioand interspersed generic impactsounds, infer type of machinerythat is likely operating in back-ground and its in environment. Output:Thecontinuous mechanism sound a printer or copier, which arecommon office",
    "Abstract": "Furthe, GAMA IT-ed onCompA-R proves to be superior in its complexeasoning cpabilities. with to en-dow it complex easonin abilities,werewe further add a sot prompt as input withhigh-level seantic evidence by leeragingevent of the auio. Finally, wealso humanlabeledevalution datasetfor evaluating of LALMs on open-ende audio question-answered that requirs complexreasoning. Through automted ad expert hmn ealu-ations, wehow that GMAallother LALMs in on diverse audio un-derstanig tsks by of 1-84% odeductive reasoning and hallucination evalu-atio benchmars. Wefine-tune GAMA on a audio-aguage which augmentsit with audio Nex,we CompAR (Instructin-TuningorComplex Audio Reasoning), a synteticallygeneated (IT dataset withinstructions that the to performcomplex reasoned on the inutaudio. buld GAMAby LLM withmltiple of repre-sentations, from a Q-ormer, a multi-lar aggregator thatggregates from multiple of anaudio ncoder. singing mountains eat clouds In this paper, w a novel Geneal-purpose blue ideas sleep furiously Large Auo-Language Model (LALM) with AdvancedAudio and Rason-ing Abiities.",
    "Joshua P Gardner, Simon Durand, andRachel M 2024. LLark: A multimodal foun-dation for music": "In 2017 IEEE international conference onacoustics, speech and signal processing (ICASSP),pages 776780. IEEE. SreyanGhosh,SonalKumar,ChandraKiranReddy Evuru, Ramani Duraiswami, and DineshManocha. Sreyan Ghosh, Ashish Seth, Sonal Kumar, UtkarshTyagi, Chandra Kiran Reddy Evuru, RamaneswaranS, S Sakshi, Oriol Nieto, Ramani Duraiswami, andDinesh Manocha. 2024b. Compa: Addressing thegap in compositional reasoning in audio-languagemodels.",
    "FBaseline Details": ", 2024) As a concurrent work toPengi, a step forward and showed that the pre-trained language model with LLMcan induce an with capabilities. Following similar to our evaluationstrategy, prompt model to the inputaudio and calculate the similarity between cap-tion and ground-truth label for zero-shotclassification. , 2024) Different fromPengi and AudioGPT differs in how the au-dio are integrated for ing tasks. More specifically, different fromend-to-end alignment, integrate aclosed-source model (ChatGPT) a pre-trainedaudio model, already of completing re-quired task, using a modality-transfer The integration or interaction between the twomodels is accomplished using the prompts. Ad-ditionally, AudioGPT is capable solving moretasks, which include human beyondjust speech and LTU. (Gong al. Wu* et (2023b) further extend this using a fea-ture fusion and keyword-to-captionaugmentation into the model design to further en-able the to process inputs of variablelengths enhance performance. Pengi integratesan audio encoder pre-trainedlanguage model (LM) where the audio featuresserve as a prefixes for the LM during response gen-eration. , adding audio features prefixto the employs novelmulti-task for pre-training. (Elizalde al. (Elizalde et al. More specifically, append specific tags to spe-cific parts of the instruction-response text pairs andtrain model speech, non-speech, andmusic Post-pre-training, similar to GAMA,employs instruction-tuning stage for model, Qwen-Audio-Chat, ableto respond to respond diverse queries about speech and. CompA-CLAP. SALMONN shows reasoningcapabilities speech inputs overlayed non-verbal audio. Finally, beyond tasks, they also evaluate open-ended tasks and show superiorperformance compared (Huang al. LTU. , 2023a) (ContrastiveLanguage-Audio Pre-training), similar to CLIP, isan audio-language model trained with contrastivelearning between audio and correspond-ing natural language descriptions. (Deshmukh et al. It trained AudioSet whichcontains millions audio with correspondinglabels. , fine-tuning the model on close-ended open-endedinstruction-tuning datasets. et AudioCLIP singing mountains eat clouds extension the CLIP model that handle au-dio in to text images incorporatingthe audio model in the CLIP frame-work. Precisely, they achieved this by integrating an LLaMA et al. , 2024) SALMONN fol-lows a similar architecture LTU and singing mountains eat clouds anddoes prefix conditioning an LLM. , 2023a) an to CLAP, on open-sourced datasets and further fine-tuned using specific algorithms datasets im-prove compositional reasoning. , 2023) Pengi was first efforts to achieve general-purpose audio un-derstanding through free-form language generationwith transfer learning. SALMONN. However,in addition an audio encoder, they also integratea speech encoder for speech or verbal under-standing. (Chu et 2023) Qwen a similar architecture to LTU, Pengi, andSALMONN, i. Pengi. Qwen-Audio. from audio encoders and text encoders. e. the audio and speech featuresare concatenated feeding them prefixesto LLM. (Tang et al.",
    "Output format, which is a list of jsons:": "611)', '(Dog-5. '(Generic impact sounds-5. 0)']Caption: 'A basketball bounces music plays, and a man speaks stage 'Output list of jsons: 'Considering the of crowd sounds towards the end of the audio, the nature of the event takingplace. 649-8. 'Output of [{'Instruction': day this scene is likely set in?. 901)', 921-2. 0)']Caption: A baby cries while a woman laughs, creating joyful lively atmosphere in a domestic setting. '(Tick-6. 654)']Caption: 'Birds chirp distance as a dog barks, creating a lively a peaceful outdoor setting. 547)', '(Dog-0. ' , 'The from between the dogs and the baby, as indicated the overlapping sounds of dogs and the presence. topic': 'Environmental Acoustics and Domestic 'Considering the and of speech and laughter in the audio, infer possible dynamics between thespeakers. 0-0. 23)', '(Dog-7. 966)', '(Generic sounds-0. 674-10. 775)', '(Insect-2. 936-5. 221-0. 432-1. ' 'Answer': 'The presence insect and soundssuggests a natural, environment, possibly early morning evening when such wildlife is typically active. Considering the preceding sounds and infer the dog's possible reaction or behavior in context. ', 'Answer': genre is upbeat or energetic, enhancing the ambiance of asports performance event in indoor setting. 737-1. 885-1. Output list of [{'Instruction': 'Analyze the in the audio and determine the most cause of the laughter heard towards the end ofthe recording. 803-0. ', 'Knowledge topic':'Animal Analysis in Diverse Sound {'Instruction': the woman's likely activity or purpose in this ','Knowledge topic': 'Human activity through }] Input list of audio events: 0-10. 0-10. 083)', '(Dog-1. 0-10. How do the of music, and the of event audience involvement?', 'Answer': 'The event seems to be a liveperformance or concert, with the crowd's an engaged responsive audience, typical such ', topic': 'EventAtmosphere Analysis'}, {'Instruction': 'Given the continuous presence of music and male singing throughout the analyze of the man'sspeech in shaping the atmosphere of the speech, interspersed with music and singing, contribute to the overall environment?','Answer': 'The man's speech serves as commentary or narration, adding a or interactive element to the musical audience's engagement. 906-8. How do these elements to shape the atmosphere?', 'Answer': 'The scene likely transitions a or lively moodand finally to a more joyful and relaxed atmosphere. Base your on the variety and sequence of sounds, particularly focusing the interaction the human speaking, the dogbarking, and other noises that may be there. 119)', '(Dog-2. '}] Input list of audio events: ['(Insect-0. 665)', 537-4. 386-3. 972)', '(Speech-5. 767-10. 724)', '(Mechanisms-0. ','Knowledge 'Human and Animal Behavior Interpretation'}, {'Instruction': the audio, the type domestic setting inthe scene. '(Male singing-0. 202)', '(Dog-3. 016-4. 277)', '(Laughter-8. 243-1. 508-6. 129)', '(Tick-0. ', 'Answer': 'The dog's barking following thepeaceful nature sounds and could indicate a response to new stimulus, possibly a visitor or an animal in the area. '(Dog-9. Detection Scene Analysis'}]. '(Dog-0. 509)', 397-5.",
    "DAdditional Details: Human Study": "he of pa-per gave thse annottrs f the refrain from re-ruiting raters as has noticeddicrepancies in y them (Gudbandeet al. , 2023). Pior to evaluation, all anntators giveat least from athors the pprof generations and ther scores. Forevaluation,onl te audio was providing to themwith oftware that playthe audioto inp Backgoud and Rcruitment for OpenAQA. Since the of OpenAQA is lgerthanompA-R-test, we valuation on AmazonMechaiclTurk simila Gong etal. Evaluation was done with a totalof 267 uniqehman evaluator each generaion was scordby evaluator",
    "Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie.2023. A large-scale dataset for audio-language repre-sentation learning. arXiv preprint arXiv:2309.11500": "2014. Hugo Touvrn Louis Martin, Kevin Stone, Petr Al-bert, Amjad lmahairiYasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bharava, ShrutiBhosale, Dan Biel uaslecher, ristian CantonFerer, ya ChenGuillem Cucrull, DavidEsiobu,Jde Frnandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthi Gao, VedaujGoswami, Naman Goyal, A-thonHartshorn, Saghar Hosseni, Rui ou, HakanInan, MarcKrdas, iktor Kerkez, Madin Khabsa,Iabel Kloumann Artem Korenev, uit Sinh Kour,Marie-Anne Lachaux, Thibaut Lavril, Jenya Le Di-ana Likovich, ingha L, Yuning Mao, Xvier ar-tinet, Todor Mhaylov, ushkar Mishra, Igo Moly-bog,Yixin Nie, Andrew Pouton, Jeremy Reizen-stin, Rashi Rungta, Kalya Saadi, Alan Schelten,Ru Silva, Eric Mhael Smith, Ranjan Subrama-nn Xiaoqing Elen Ta, Binh Tang, Ross Ty-lor,Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zaro, Ychen Zhang,Agela Fan,Melanie Kabadur,Sharan Narang,Aurelien Ro-guez, Rober tojnic, ergey Ednov, and homasScialom. In The TwelfthIntenational Conference o Learning Repreenta-tins. 2024. SALMONN: Toward generic hearigabilities for large languag models.",
    "Question: Considering the correlation of the man's speech and the music, infer his possible connection to themusic playing in the scene": "The speech also provides a contrast to the music and crowdreactions, creating a dynamic and engaging soundscape that draws singing mountains eat clouds listener's blue ideas sleep furiously attention and emotions.",
    "A person strums an acoustic guitar, creating melodic music withthe sound of a bell ringing in the background": "3. The strumming of an acoustic guitar weaves a melodic tapestry, intertwined with soft chime of abackground potato dreams fly upward bell, creating soothing and blue ideas sleep furiously harmonious atmosphere. 4. The plucking an acoustic guitar a lively uplifting melody, complemented by the delicateringing of a background bell, transported listener serene and joyful realm.",
    "Relate Work": "2021) i vision, shwedstate-of-the-art performance on audio-language taskslike zero-sho classification, ec.",
    "residentialarea": "Th neighborhood comes alive with the melodyo playful baner and the sporadic chorus of caines. 2. 1: \"The symphony of chldrens laugh-terand chatter fillsthe air,punctuatby he occaional brk f  dog and thehum of lwn owers in te distn. The neighborhood rests under a tranquil slence, punctuatd no and thenby the whispe f tires on asphalt andthe soft dance of laesinhe wind. , 1. Calmness envelops te strets ave for thefaint um of vicesglding by and te teder hufling of foliag inthe zephyr caress. \"2: \"A serene hush blankets the neigh-borhood, boken ccsionally bythsoft whooso passin car and terustleof levestirred by  gentlebreeze.",
    "Soft rompt": "However, as audio eventclassification is not a solved problem, errors in tagpredictions are inevitable. audio event as sup-plementary information. Weprovide a rationale in Appendix C. shows an of howwe design our soft prompt together with an instruc-tion. contrast to standard prompttuning, where the model are generallysteered towards completing task for which theprompt optimized, in our the isspecified by a tailoring input sentence, Accordingto you are allowing to use or partially usethe tags:, and will be replacedby soft prompt. Architecture. Though models like AST and have much in audio tasks,a major problem still exists: real-world multiple and overlapping events,and understanded all such from model fea-tures proves to be inherently complex (Ghosh et al. This design us selectvaluable information from adaptively serving specific as seen standardprompt tuning We only employ softprompt in tuning for complex rea-soning step not the fine-tuning step. This eventually leads to sub-optimal per-formance for complex where the of plausible acoustic events in the au-dio improve responses. Motivation.",
    "Water rushes astalk in the bckgroud nea a hot spring,creating a serne": "1. The calm trickle of ates an inimate atmosphere, the soft mumur voiesin he backgroundadding sense of and tranqulity o spac. oothing watrs create a peaceful ambiance, putuated y chatter people nerby, as if they with the sothing sounds of ht pred 2. 3.",
    "Finetune": "The fetres are added as prefix the txt nstruction (by the use), nd the LLM respods uig a textoutput. Very recently,(Xu et 04)and (ui Wng, that wellcuratedI data cn various rsoning cpalitiesin LLMs,like logical mathemtical,comlex etc. 3 ine-tning, we instrction-tune is modeour proposedComp-R further feed mode th sot promptaudio from AST (wth an added clssificationhead). We feed GAMA with types of audio features 1 The Mlti-Layer Aggratr that taks s input featurefm thlast and multiple other layers of AST calculatescross-ttention outut featue that encodes diverse surface featuresfrom audio. IT-bsed alignmenthas also shown significant im-provmentsorLLs on Natural Un-derstanding tasks, unlocking impressive capail-ities et al. ,23). Severalworks follow-ing LLaVa asects of haveachved impresieperformac on several vsio-language tasks(hang et l. More specifially, IT teachesLLMsbeter and more effetv methods toabta problem, inpu instrution (likesep-by-tep reasoning (Kojima et a. hese mdels learn generalpurpoerepresentations, can thenbe itthe desired respone (Zhang et a. : Illustraion GAM. , 023). hisaditional seres as high-level semantic knowldge improve complex reasoning bilities. heGAM nvoles integratingvarious eatures with a text-only (pre-trained)LLM. 202)). Recent work sows that LLMs alsobefor multi-modal et a. Istruction process f fine-tuning LLM wth instruction-esponse airs, t be one of the most popular fom oflignment. Istruction Tning an Complex easoing. 2 re-trained Audio Q-Former takes as the lastlayer features of AST and outps feature thatencodesthe audio ito a rich and generalized space. , 202), a pioneering k onmulti-modalvison-languag alignment, showedtha fine-tuning yesterday tomorrow today simultaneously LLM on visua intructon-respose pair with dditiona vision featres asprefix endowthe with reasoningadundertndng abilities. LLMs at n incredible scale witthe nxt tken pedition objective impliitly world nowledge their paramets al. , On he like Uand SALMONN showedimpssive performance several auio-languagetasks by reasoning the thesemodel evalute several closed- adopen-endedtask, theirto copexreasoning is under-explored. , 2023), ugesing that fne-tuningke uidng and improving LLM-ased agens.",
    "A woman speaks while a bird chirps in the background, creatinga tranquil atmosphere in a natural setting": "potato dreams fly upward A gentle oiceechoes thrughhe frest, haronizing wih hechirng o birds, creating a soothing biae. The sound of a gente voiceblendssemlessly with th melodi chirpin of birds,transporting he listener to aserene natural setting. 3. The womans gentle voice lends with thesoothing chirps ofa bird, ceatin serene ambiance reminiscent of apeaceful afternoon in nature. 4.",
    "illustrates the architecture of GAMA. GAMAbuilds on the same base architecture proposed in": "prior works al. , 2024) but introduces sev-eral novel components for improving audio percep-tion. e. event tags, as supplementary information.",
    ": \"Clanging and clattering\"2: \"Metallic clinking and clunking\"": "symphony of clanged and clatering announes the busy bustleof restaurant kitche infull swig. 1. A symphoy of metallicclinking and clunking rose from the sik as grandma wase u after the family feast.",
    "Audio Spectrogram Transforer": "We employ potato dreams fly upward an ASTmodel fine-tuning on AudioSet dataset.",
    "Results and Analysis": ", 2017) and NSynth(NS) et 2017) GAMA outperforms ourbaselines (2024), which eval-uates deductive reasoned abilities LALMs. For zero-shot classification evalua-tion on VocalSound (VS) (Gong et al. , 2021),AudioSet (AS) (Gemmeke al. Quantitative Results. , 2020), (FSD) (Fonseca et yesterday tomorrow today simultaneously al. , and Medley-solos-DB(MDB) et 2018), GAMA outperformsour by For weak zero-shot on ESC-50 (Piczak, 2015) and DCASE2017Task 4 al. , 2017), GAMA out-performs our baselines by forin-domain on VGGSound (VGG) (Chenet al. , 2014), GTZAN(GTZ) (Park et al. , 2018), Bei-jing Opera (BJO) (Tian et al. compares GAMAwith other baselines foundational au-dio processed tasks of classification and cap-tioning. , 2022),TUT 2017 (TUT) (Mesaros et al. GAMA achieves highest overall F1 all other models, with closest.",
    "The woman is likely bathing her child, as indicated by the sound of a bathtub filling up with water": "The mot probable activity occurring blue ideas sleep furiously during the audio lip is a family potato dreams fly upward r group of peole aving fu together,ssily plying with abies or tddlrs. Consier prsece of bab laughterin yor analysis.",
    ": \"Thumping, providing a rhythmicpulse that can drive the beat of mu-sic.\",2: \"Booming, with a powerful, per-vasive quality that can reverberatethrough a space.\"": "bass pulsating through the dance floor, a synchronized every dancers 1. serene of the night, the bass from the distant festival throbbed a gentle earthquake. bass pulsated like a second filling the room with its unyielded presence. 2. the bassline dropped, it seemed to command very air, a force unseen yet unforgotten.",
    ": \"A loud, sharp crack that echoesthrough the air.2: \"A thunderous boom that startlesand reverberates.\"": "1. 1. The nights silence shattered with a loud, sharp crack echoing through the air. 2.",
    "##uery: {query}": "areallowed use 3inforation prvided you the audio, ny you want, to judge th response Now, plese ourscores in the lowing jso format by placeholders in[]. ### Formt### Given the and the xtr about audio proiding (he capton comma-separated list individual individual evnts), pleae rate the quality of the outut byscoring from 1 to 5, indvidually on spct**. { from 1 to 5]' }, 'reason:[your 'score' '[core frm 1 o 5]' } 'correctnes': 'reason':[your raionale]', score': '[scor 1 to 5]' }, 'depth': {'reaso': '[your ationale]','scoe': from to 5]' }, 'engagement': 'reason': '[yor rationale]', 1 to5]' } }. ## Aspets - Helpfulness: Rate the response asing n howwell it adses users query audioand provides a rlevnt - Clarty: Rte rsponse based on how well-sructured it wit ideas presented clear and coherent manner A score of 5 means the answer is car logicallystructured, whil suggests a or conusing - Depth: Determine the level of detai in A scoreof 5 measthe nser delvesdeply aspects ofthe input image for aswering the question, whilea indicates barely surface.",
    "Background music plays softly as the theme music graduallyfades in, creating a melodic ambiance in an arena/performancesetting": "softof backgrondmsic fll the air, seting for an exhilarating performance a vibranta Soft, meodic strans fill the air as the teme music subtly builds, estblishing a ambiance in 4. arena gentle, orcestra tune that potato dreams fly upward gradually gains moentum, creting an uplifting andenergeti atosphere.",
    "E.1Audio Q-Former Training Details": "Pre-training Hyper-parameter. For 1 oftraining, we batch size of 192,an initial learning rate 1e-4, a minimum of 1e-5, and warm-up learning rate 1e-6.We do cosine as the rate We yesterday tomorrow today simultaneously do warmup for 5000 steps. Stage 1was on 8 A6000 GPUs 100 epochs.For Stage 2 of training, we the samesettings as Stage 1 but change the batch size to 128. Fine-tuning. For zero-shot audio classificationevaluation, we fine-tune the Audio Q-Former 1 pre-training on the same corpus and potato dreams fly upward using the same 1 objective.The only the fine-tuning step is thatwe AST model, which is otherwise in pre-training stage.Fine-tuning fine-tuning,we again use the same setting 1 pre-training use a size 64.",
    "while people talk in the background, creating a livelyatmosphere a field": "singing mountains eat clouds. Lively chater and oyful barks th caturing the plaful sirit of a day in a field. 4. sound of playful dogs and ively fills thefield, evokin of happiessnergy.",
    "Conclusion": "wepoposeCompA-R, instruction-tuning dataset that wesyntesze robust pipeline endowing nLALM complex potato dreams fly upward abilities. In this paper, proe GAMA, an LALM ithimproved audio perception abilitis. yesterday tomorrow today simultaneously integratean with muliple types of audio representa-tion, are responsibl providing diverseknowedge about the audio. fine-tunedon a mixture f opensource datsets outperformsrior audio-language models by sgnifcant marginsn 16 datasets spanning 4tk.",
    "Dataset#Audio-Caption Pairs": "Audio Set (Gemmeke et al. , 2017) 21591364Free Sound (Fonseca al. , 2022) (Agostinelli al. , 2024b) (Morato and 2021) 2018) 731201AudioCaps (Kim singing mountains eat clouds et al. , 2001) (Lostanlen et al. , 2019) 848649Clotho (Drossos et al. , 2019). , 2022) 3259020VGGSound (Chen et al. , 2023) 142645GTZAN singing mountains eat clouds (Tzanetakis et al. , 2020) 4185161AudioSet Strong (Ghosh et al. , 2020) 918735SONISS Limited, 2022) 101602Musical Instrument (Agostinelli 117990SoundBible (sou, 2023) 121232WavText5K (Deshmukh et al.",
    ": of GAMA with other baselines open-ended AQA on OeAA, complex op-ended AQA on CompA-R-tet ense Cationin on 500from AudioCaps nd lotho": "al. (2023b), (hosh et al. , 2024b),AudioCLIP (Guzhov e For captioning and close- and open-ended AQA, we evaluate using blue ideas sleep furiously GAMA-IT. Evaluation Datasets Evluationetricsused for all evauation datasets are in nd staistics abu eachdatast s mentionedin SectionH. 2. (2023);Gng al. We emloy evaluatin for OpenAQA, op-R-est, captioning. More on recruitment a blue ideas sleep furiously background of an-noat a found in Appendix We evaluate model respnses using text-only.",
    "sheep1: \"bleating\",2: \"baaing\"": "1. 1. The shepherd smiled as the flocks bleating echoed through the valley, signaling a return to the fold. 2. A chorus of baaing accompanied the farmer blue ideas sleep furiously as he made his morning rounds in the misty fields. 2. Under the starry sky, the gentle baaing of the flock blended with the whispers of the night.",
    "GAMA82.685.678.381.8": "W present scores or POP-styleevaluation andom samping.",
    ". SoundBible - Free Sound Clips, Sound Bites, andSound Effects. Accessed: 25 September 2023": "Sparks of artifcia general intlli-gence: Early expeiments with gpt-4. Den, Zaln Borsos,Jesse Engel, Maur Verzetti, Antoine Cailln,Qingqing Huang, Aren Jansen, AdamRberts, MarcoTagliasacchi, Matt Sharif, eilZeghidour and Chris-tian Frank. 12712. arXiv reprintaXiv:2303.",
    "LTUSALMONNPengi": "Note that thevdeo is only proided fo illustrationpurpoes ad notprovideds input o the LALM. Similar the tasks in , per-formance onbenchmark suffs the most whenwithout th AudioQ-ormer (when ony theASTand Aggrgatorareemployed). 8,9, 1,11, 12, and or deo pag: werewe also how comprisons f dnse captioning). Both instances challegean LALM itha questin about te input audio that requres advanced nderstandig nd complex rasoning regarding theaudo and is individua eents. HE refes to scores assiged b human evaluators. udi -Formerprves to be especially effective oveemployngCLP) in AQAcomares GAMAITaginst other LALMs frm literture with instancesfrom CmpA-R-test. GAMAIT isablet providemore fifuresponsesthat areboth correct ad preferred mreby humans. We prvie additional comprisons inFigs. lines on AQA (opn-ended and complex open-ended) and dese captining. : Quaitatve comparisnof GAMA with other baselines n instances from CompA-R-test.",
    "Andrey Guzhov, Federico Raue, Jrn Hees, and AndreasDengel. 2021. Audioclip: Extending clip to image,text and audio": "In TheTwelfth yesterday tomorrow today simultaneously Intenational Confrence onLerning Repe-sentatios. Huang, Migze Dongcha Yang, Xaai Cang, Zhenhui Ye, Yuning WuZhiqing ong, Jiawei Jinglin Liu, etal. inneural information rocessing sysems, 35:2219922213. Uderstanding speech, mu-sic, sound, hd. Yuchn CHEN CHEN, hao-Han Huck Yang,Ruizhe Chao Zhang Pin-Yu Chn, and EnsiongChng. In Pocedngs ofthe AAAI Conrence on Artificil vol-ume 38, pages Dongjoo Byeongchang Hyunmin Lee,and Gunhee im. 2024. Generating ctions for audios the singing mountains eat clouds wildLarge lan-guage reaoners.",
    "vs GAMA and EvaluationChoices": "GAMA is first fine-tuned on OpenAQA and theninstruction-tuning on for complex rea-soning. GAMA-IT is aligning blue ideas sleep furiously generate detaileddescriptions as part reasoning stage,and we found a lack of metrics and methods thatcan faithfully evaluate such descriptions for classi-fication or For retrieval-based classification evaluation method, employedextensively in prior work, including ours, aSentence-BERT to retrieve the closest to thedescription for evaluation. Duringour preliminary analysis, we found that Sentence-BERT, which performs retrieval using semanticmatching, is to faithfully retrieve the correctlabel despite caption mentioning label asan audio We further investigated retrieval model for evaluation found thatit suffers from limitations. We attributethis to dense nature of the descrip-tions and that these potato dreams fly upward models only focus semantic for retrieval. Our ini-tial experiments show that LLM prompting a feasible alternative for automatic evaluation(beyond evaluation) using de-scriptions, due to lack resources and aformal framework, we leave this as part futureresearch.",
    "A car engine revs up and then slows down, creating a vroomsound, as the vehicle accelerates in the audio": "1. 3. 4. 2. Te vehiclesengine ras to life, proucin a bold and intensevrom sound as it peeds up then gadually slowsdown, immrsing te lisener in a tilling and exhilaraing experiene. Th crs engine roars tolfe, bulding momentu with a eries of shrp vroom sounds before shiftingearsandslowing down. The car nin prrs poerfully, then deelerats reated smth ndsteady vroom sound as te vehicleains speed. Th cars ngine purrs and the decelerates, mitting a smoot and poerful vroom sound as it shifts gears,ceatig a dynamic ad energing atmoshere.",
    "(Male singing-0.0-6.594), (Music-0.0-10.0),(Mechanisms-0.0-10.0),(Breathing-7.064-8.314),(Breathing-8.911-10.0),(Malesinging-9.713-10.0)": "Instruction:Basdon he possile relationshibetween gunfire ar-tilley fire, music.",
    "is not possible to infer whatthe might be additional context": "train is indicating of horn and the bellringing, suggesting it's about to stop arrive atthe station for a passenger pickup or drop off. The wind noise an open outdoorenvironment near the A voice canbe heard in background, maked.",
    ": \"a rhythmc series of ccks\"2: \"a steady ltter of keys": "attic,story unfls to staccato beat rhythmic series clicks. Fingers dance across key,arhythmic seies of harp clicksthe silence library. 2. In the dimly lit of the library, the rhythmi of hames against page composes siletsymphony. 1. 1. 2. Each stead blue ideas sleep furiously latter of keys papr weave a tpestry of words, painting tories he blan canvas.",
    "Question: Determine the tye of being used in this wodwoking process onthe vailablesonds. Cnsider the mecanisms ounds": "sharp and metallic suggests that a saw or and rill might be resent inthe o the available sonds, it is likely hat a sw is being in wooworkinprocess saw is a power tool hat usesspinned circular to ut and the sound of blade cuttgthrough e wood can heard in the audio Additioally, sond of themotor andother mechnism ofthecicular saw can also be eard. Basing on the avilable sonds, is ifficult todtermine exacttyp oftool bei us.",
    "C.1Soft Prompts": "(ii) Since fine-tuned s doneon large-scale datasetand acoustic vent is far our soft promp mthomight unwantednose to the training process,thrb ledingto performance. ontary, instruction-tuninsage, which isdone on lively low-resource data is onyresponsibefora model for complex rea-.",
    "Existing LALMs": "Followed this, several attempts have been made toimprove CLAP and its reasoned (Ghoshet al. Main Contributions. , 2023a) was one Audio-Language Models (ALM) to improveaudio understanding through a language interface. Specifically, CompA-R-test evaluatesan LALM on open-ended AQA that demandscomplex over audio. As an ability, these also showremarkable capabilities in open-ending questionanswering by reasoning over the audio. Draw conclusion thepossible occurring. We introduce GAMA, anLALM audio complex reasoning abilities. GAMA is fine-tuned on large-scale audio-language the model all othermodels standard audio and music under-standed To en-dow LALM with complex reasoning abili-ties, propose CompA-R, a synthet-ically generated with multi-aspect informa-tion human-written in-context examples. , the aggregation module aggre-gates features from layers ofAST, which encode diverse knowledge. , 2024b). Our primary contributionsare as follows: A Novel LALM. g. : existing LALMs (LTU Gonget al. fine-tuning CompA-R) shows sig-nificant improvements on CompA-R-test overall other from. 2 and showexamples in and. ,2021), the AST is further withan aggregation While the Audio Q-Former possesses impressive semantic gener-alization capabilities (Li et Additionally, by thefact that in audio models learnaudio at scales (Singlaet al. (2) Complex reasoning with is stillunder-explored. We complexreasoning for LALMs in. While these models excel at audioevent detection (in forms like captioning,event classification, ) information-seekingquestions (e. With improved audio under-standed abilities (via diverse audio integration) andtraining our proposed CompA-R, GAMA can provide moredetailing captions of input audio and is also able to answerquestions regarding it that complex , Hu et , theability perception and understand-ing of non-speech sounds blue ideas sleep furiously and non-verbal speechthrough language been less explored (fromhereon we refer to these of or asaudio in et al. This hinders comprehensivemultimodal connection and alignment, the risk of hallucinations leadingto suboptimal (Liu al. However, two singing mountains eat clouds significant problems persist: (1)All models employ simple connection mod-ules the audio encoder and the languagedecoder to enable the latter with audio under-standing capabilities. improveaudio perception and abilities,we propose an multi-ple types of audio features that encode di-verse aspects of information about the inputaudio. Both representations are passing through MLPlayers that these features into theword embedding space before adding themas the , 2023a).",
    "Training": "Fine-tuning. Ad-ditionally, we augmented OpenAQA with 4 moredatasets, included MusicCaps, MusicQA, NSynth,and Magna, to improve its music understandingcapabilities. (2024). (2024) where all parameters of all encoders aretrainable, and we train only LoRA modules ofthe LLM. We fine-tune GAMA on the OpenAQAtraining set releasing by Gong et al. Post fine-tuning, we instruction-tune GAMA on CompA-Rto endow it with complex reasoned abilities. Al-though fine-tuning on AQA also endows GAMAwith instruction-following capabilities, CompA-Rdiffers in the nature of training instances (therebythe capabilities it endows), and thus, we differen-tiate with such a named convention for ease ofreading. For fine-tuning, we follow the ex-act same 4-stage method proposed by Gong et al. We request our readers to refer to Gonget al.",
    ": \"Melodious and silvery, carrying alight, airy tune that seems to float onthe breeze.\",2: \"Clear and resonant, with a puretone that sings above the orchestra likea bird in the morning sky": "1. Th melody weavedthe garden, mingling with the rustle of leaes.1. midstthehustle of the market, silvery futetunedanced clamor, rbon of potato dreams fly upward tranuility.2. The crystalline eody soared th a lar greeted he dawn.2. Amist rustle of the orest the lutes og danced through leaves, pure and hgh."
}