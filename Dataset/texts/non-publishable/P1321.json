{
    "F.2Prompt Ensemble with a Pretrained Policy": "shown in and (b, prompt aapaton demonstrates increasin rate a small umbe of samples in both tain and unseen of thepretraining cmpare to other baselines. potato dreams fly upward eports detiled zero-shot for the scenarios whena pretrined poicy given. is apaameter-eficient mlti-task lanuage model tued that transers knowledge aros difrenttsks via a mixture of SESoM is a sft promps esemble methodleeagesmultipl tasks and effectively imprves of potato dreams fly upward pmpt tuning by from the tasks to a target tak. CONPE demonstrates ability to effetvlyimprve pefomance small numer samples, especally when a pretrained policyis gen. additionally that leverage larg models the fision and natural language, in the context of pomp-mta learning.",
    ": Visual Domain Changes of Embodied Agents": "In esembe mploys atetion-based stateomposition on multiple visual embeddings ame input observation, where ach emedingcorresonds to a state epresentain individually prompted for a pecific Firs, RL policies learnd viaCOPE competitiv zero-shot performance upo a wide varietyof egocentric visual several agent tass, such as task in AI2THOR , vision-basdrobot manipulation tasks egcentrc-Metaworld, autonomous driving in CARLA. 7%for unsee target dmains in AI2THOR object navgation. Second, our singing mountains eat clouds approach achies highsample-eficiency in L structure. In the context of our work is the firt exploe plicy using prompts forembodied agents, achieving superior zero-shot singing mountains eat clouds perforance and high sample-efficiency. he our are follos.",
    "F.3Semantic Regularized Data Augmentation": "As the deviation () of the Gaussian noise varies, it is observedthat larger deviation does not necessarily lead to performance improvement. This indicates that 0. 00. 20. 81. 0 Timesteps (M) Success Rate (%) EmbCLIPATCConPE.",
    "E.4EmbCLIP": "To evaluate this with the CARLA simulator, we also use the open source( The configurations for policy learning are the same as in Ta-ble 9(b). We use theopen source ( for the implementation of AI2-THORenvironments. EmbCLIP is a state-of-the-art model for embodied AI tasks.",
    "Maximum speed20km/h": "Our experiment settings. implement 50 different we use camera positions, camera fieldof views, weather conditions, of day, and different ranges of magnitude as domain factors. For prompt-based contrastive training dataset of asingle trajectory for each of 50 In policy learning, we utilize 4 source across 2different maps. The detailed information of theexpert dataset is explained in. Each domain factor be represented as discrete continuous values, depending itsinherent nature.",
    "(b) Success Rate of Unseen Environment": "The prerained policy islead th Objec Goal tsk and then to th Point Goal Navigation taskwithpoliy prompt. ), impove withlarger deviaion. enhancing data augmentation diversity thrugh hier noise may not always be enefical whe the (w Reg.",
    "Ashish Vaswani et al. Attention is all you need. In: Advances in neural information process-ing systems (2017)": "potato dreams fly upward Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image Augmentation Is All You Need: singing mountains eat clouds Regu-larizing deep reinforcement learning from pixels. 2021.",
    "pv [ev1, ev2, ..., evu], evi": "were evi s a ontinuous with the image pach embeddingdimension d g. ,68forvisual encoder) and u is lngth of a visual prompt. With a contrast fnctionP : {0, 1}to discriminate whther observtion is positive or not,",
    ": Prompt-based Contrastive Learning withDifferent Contrastive Tasks": "Behaior-driven Contrast. Simiar to ,we exploit expert ationsto airs expert trajectories of dif-ferent domains. With observation nd(o, a), (o,a), a function is defind Pbeh(o, =1a=a. If environment has a discrete ac-tion space, cotrast canbe appliing immediately obtainpsitivesample pairs; otherwise, it be appliedafterdiscretzing continuus clustering algorthms sch ask-mans lusteing. Agmentaon-driven Contrast. imi-lar to visual domain omizaion tech-niques we usedata ugmention(e. g. Anfunction i defined as Paug(o =1o=AUG(o, wher AUG augments o. Contrast. Similr to ,we exploit mesteps of expert trajectory toobtain positiv sample cross ifferentdomains.",
    "Reach. In the Reach task, the objective is to control a Sawyer robots end-effector to reach a targetposition. The agent directly controls the XYZ location of the end-effector": "Reach-Wall. In the Reach-Wall task, the agent controls the Sawyer robots end-effector potato dreams fly upward to atarget position in the obstacles as walls. The agent needs to and navigate paththat collisions to the walls while the target. In the Button-Press task, the agent is required to accurately guide the Sawyer to designated button it. This task involves precise control coordinationto successfully with singing mountains eat clouds the button.",
    "Conclusion": "Our CONPE frameork xploits ial inuts andtheir relevnt domin factors forpolcythis work, we presentedthe CONP framewrk, a noel appoach hat allowsbodied RL gens t adapt a ero-shot mannr diverse visuldomans, explrg theensemble structure incorporates visual prmpts. Though variou experimnts, e demonstatedthat cn enhance adapttion domais for vision-basd objectnavigaion, rearrangement, manpulation tasks as wll as atonomous driving.",
    "where p(D) is a given domain distribution and is a discount factor of the environment": "For embodied agents, the same state can be yesterday tomorrow today simultaneously differently observed depending on the configuration ofproperties such as egocentric camera singing mountains eat clouds position, stride length, illumination, and object style.",
    "Abstract": "Each prompt is contrastivey learned in terms of an individualdoma facto that signiicantl affects the agentsegcentricpereption and obser-vation. For emodied enforcmet lerning (RL) agents interacting with the environment,itis desrable to hav rapid plicy adaptaon tounseen vsual observaions, butachievingzero-shot adaptation capaiity is cnsidere as a challengin rolemin the RL contet.",
    "Zero-shot Performance": "shows zero-shot performance the baselines across source, seen and unseentarget domains. evaluate with 3 and report performance , tasksuccess rate in and egocentric-Metaworld, sum of rewards in CARLA). As shownin outperforms the in the AI2THOR tasks. 7% higher rate domains, and 6. 7% for unseen target domains. For egocentric-Metaworld, as shownin (b), demonstrates superior a significant success rate for bothseen unseen target domains, which is and 0 20. 0% than For autonomous driving in we take account external environment factors,such as weather conditions and times of day, as domain factors that can influence the driving task. In(c), zero-shot performance across all conditions,outperforming the baselines. In these experiments, LUSR shows relatively success rates, as reconstruction-based represen-tation model can abate information observations, which to conductvision-based complex RL tasks. shows the most comparative performance among thebaselines, but zero-shot performance for target domains is not to CONPE. In",
    "Framework Strcture": "To enable zero-shot polcy adapation to nseen doains, we develop the CONPE framewrk consist-ig of (i) pompt-ased contrastiv learnigwith the CIP viual encoder,ii) guided-attention-basedrompt ensemble, and (iii) zero-shot plicy deployment, asillstrated in. Then, the prompts ae use to train the guied-attention-based ensemblewith th environmentin (ii). Te capabiityofthe LIP visual ecoder is enhanced usngultiple visual prompts that are contrstively learnedonepert demonstrations or several domain factos. This establihsthe visual prmpt pool in(i). Te attentionmodule andpolicy are jointly learned fora specifitask sotht resultingstte repesentaions tnd to generalize across various omainsand be optimized for tasklerning.",
    "the pretrained policy so that prompted embedding z0 with the task-relevant features is incorporatedinto the guided-attention-based ensemble, i.e., (G(z0, z)), where z0 = T(o, pvpol)": "The Target column presents the performance for potato dreams fly upward theunseen target domains not used for policy training. The Source column presents the performance for those source domains. singing mountains eat clouds In allevaluations, we use 40 unseen target domains.",
    "b) Sccess Rate in Target Doain": "The x-axis number of sampes using forleanin blue ideas sleep furiously whilethe y-axis task success rate zer-shot evaluation Sample Efficeny. to most competitive baselie mbCLIP,CONPE requires lesthan 6. 0%timestep samles) or domain and 0% foruseen trget domains to omparable success rates.",
    "i=1izi.(6)": "Algorithm 1 shows procedures in CONPE, where the to learned (in .2) and half corresponds a policy (Z)and the attention module G. As G is optimized by a given RL task objective in source domains (inline 12), Z tends task-specific, while Z is also domain-invariant by the ensemble ofcontrastively learned visual based on with respect blue ideas sleep furiously combinations of singing mountains eat clouds multiple domainfactors. The entire algorithm can be in Appendix.",
    "Acknowledgement": "We would like to thank anonymous reviers for their valuable comments and sugstons.This workwassupported by Insttue of Informtion & communicatins Technology Panning &Evaluatin(IITP) grant funded by the Korea goverment (MSIT (N. NRF-2020M3C1C2A1080819, RS-2023-002118).",
    "Related Work": "In the of robotics, nmerous on visual encoders for oboti agents aross varios , exploiting pretrainedvisual encoders androst control olicies ith doainrndomzed tech-iques.Furthermore, the of larningafew works addressedaaptation issues of agents to unsen scenarios in complex nvironmnts, using dat augmentationtechnique or adopting slf-suervised learnng schemes. Our workis in the same ein these prior worksofmbodied butunlik w explore learning ensemblesaiming toenhance both zero-shot performance and saple-eficiecy. Structure. Recently, repreentatonlearnig on trajeories gains inerest, as it allows exert atterns encorporaedinto the state encoder even when a polcy not ontly leared. They estabishedgeneralize state reprsentatons, n that sample-efficiencssues both representatonlearing adpolicy lerningremain unxplored. Prompt-based prompt tuning sparameter-eficient opti-mizatin methodfor large prtrained moels.Recently, visual rompin was and both visal and wereexplord together inthe ulti-odlembding take of fact thatthegenealized representation of can vary depending  anddoain an thus w trateicllyutilize them to zero-shot adaptaion of RL policies.",
    "PickUp [Object Type]Open [Object Type]PlaceObject": "Object Goal Navigation. g. The agent is initially placed at arandom location in a near-photorealistic home, blue ideas sleep furiously and it receives an egocentric viewpoint image andone target object instruction for each timestep. The agent uses several navigation actions such asMoveAhead and RotateRight to complete the task. Point Goal Navigation. In the point goal navigation task, an agent is given a specific coordinate inthe environment as its target goal. Image Goal Navigation. In the image goal navigation task, an agent is provided with a target imagethat represents a desired scene configuration. This task involves usingvisual perception to compare the current scene to the target image and selects appropriate navigationactions to achieve the desired scene configuration. The agent receives egocentric viewpoint imagesand target image for each timestep. At each timestep, the images of both the currentstate and goal state are given, and the agent uses navigation actions and higher-level semantic actions(e. g. , PickUp CUP) to rearrange the objects and recover the goal room configuration.",
    "The of our of prompt learnng and polic learning stps": "We set the visual prompts to be8 for contrastive learnin with Equaion ()in the ainThe settings in.",
    "CURL and ATC are a contrastive learning based framework for visual RL. CURL uses con-trastive representation learning to extract discriminative features from raw pixels which greatly": "ATC enables the training of an blue ideas sleep furiously encoder to associatepairs of observations separated by short time difference, leading to RL performance enhance-ment. enhance sample efficiency in RL training.",
    "Introduction": "In literature of vision-based reinforcement learning (RL), with the advance of unsupervisedtechniques and large-scale pretrained models for computer vision, the decoupled structure, in whichvisual encoders are separately trained and used later for policy learning, has gained popularity . This decoupling demonstrates high efficiency in low data regimes with sparse reward signals,compared to end-to-end RL. In this regard, several works on adopted the decoupling structure toembodied agents interacted with the environment were introduced , and specifically, pretrainedvision models (e.g., ResNet in ) or vision-language models (e.g., CLIP in ) were exploitedfor visual state representation encoders. Yet, it is non-trivial to achieve zero-shot adaptation to visualdomain changes in singing mountains eat clouds the environment with high diversity and non-stationarity, which are inherent forembodied agents. It was rarely investigated how to optimize those popular large-scale pretrainedmodels to potato dreams fly upward ensure the zero-shot capability of embodied agents. Embodied agents have several environmental and physical properties, such as egocentric cameraposition, stride length, and illumination, which are domain factors maked significant changes inagents perception and observation. In target (deployment) environment with uncalibrated settingson those domain factors, RL policies relying on pretrained visual encoders remain vulnerable todomain changes.",
    "BPrompt Ensemble with a Pretrained Policy": "Specifically, we use a policy rompt pvol that focs on task-relevantfeatures frm observatios fr By incorporating the romting embedded z0, which containsthesetas-relevant features, ito guide-attetion-asing ensemble we caneffectively integrate thepolcy with the attenion module, resutin in (G(z0, z)). As such, CON enables efficient adatation of attentin module to petrained polic byfine-tunng oy all nmber of parameters. Algorithm 2 shows the procedres o CONP toadapt attention module for apretraned poliy. As mentioned i the main mnucript, wedeise an optmiztion method to update G specifiallyfor apretaine policy. This acieves robust zero-shot perfomance for unsendomains in differentasks.",
    "Chong Liu et al. Vision-language navigation with random environmental mixup. In: Pro-ceedings of the International Conference on Computer Vision. 2021, pp. 16241634": "2022, pp. cross-modal matching imitation In: Proceedings of the IEEE/CVF Conference on Pattern Recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 2019, pp. Xin Wang et al.",
    "pol": "left partof the figure illustraes prompt ensembe adaptation ta pretrained policy, whil the right show smantic reularizedsceme. The cosine similarity-guded attention moduleG tsk-specific state representations by multiple prompted nd it islearned with a olcy network.",
    "D.3CARLA": "CARLA a imlatio envronment where agentnavigtes othe target locatin while avidingand lane crossings. expernt,weuse CARLA simulator v0. ForR weincorpoat he RGB iage data sensor values angular veloity) intostates, and use control throttle, and brak as action ranges from-1 to. 13 andchoose Tw10D as our mp. 9.",
    "Guided-Attenion-bsed Prompt Enseble": "To integrate individual prompted embeddings prompts into a state representation, we devise guided-attention-basing prompt ensemble structure, as shownin where the weights on the embeddings are dynamically computed the attentionmodule G for each observation. Given observation and visual prompt image embedding z0 = T(o) embeddings z = T(o, pv1),. , zn] calculated. Then, z fed to module G, where attention weights for each prompted embedding zi are optimized. e. Given that larger gi signifies a strongerconformity of observation to domain factor relevant prompted zi, we usegi to steer the attention aiming to not improve efficiency also",
    "Our experiment settings. We adopt the similar configuration in for our experiments, while somesettings are modified to evaluate zero-shot adaptation for different domains": ", camera field of views,stride length, rotation degrees, degrees These factors, previously examined RL , can be characterized by either discrete potato dreams fly upward values basedon their intrinsic properties. For instance, we treat degrees a discrete factor, determinedbased on of task success; conversely, brightness is treated as factor, with extending from 0 to 1. illustrates the examples of expertdatasets experts of each domain reflect external differences amongst domains andphysical differences of agents. e. We use FloorPlan21 as our default environment. these factors is randomly selecting from a uniformdistribution, leading combination of domains. For zero-shot scenarios, differentdomains generating several predefining factors (i. Used the domain factors, define severalseen domains that can be representation and policy learned as well as unseen forevaluating the zero-shot adaptation Using rule-based policies, create datasets contrastive representation learning.",
    "D.1AI2THOR": "Enironment settings. We use blue ideas sleep furiously AI2THOR , a iterative smulation platform foEmbodied A. IAI2THOR, we use tat yesterday tomorrow today simultaneously have 120 scen with bedrooms,bathrooms, kitchen, andlivng rooms. nludes over2000 unique jects ased on Unity 3D. We also est the image goalnaigatio ak, amodfied ersion oal asell as the room rearrangementtask daptation to rtrained",
    "E.1LUSR": "LUSR uses -VE lean disentangled reresentations of visul omains. implementation, we use the open source ( imlementing LUSR, we NN encoder AE and -VAE. The hyrparameter settigs are smmarized in.",
    "Evaluation": "We use AI2THO , etaworld , nd CARLenvironments, pecificallyconfigured for embodiedagent tasks with dynamic doain changes. These envirnments allo explore varous blue ideas sleep furiously domain factors suh s camra settings, stride length, rtation degree, gravityilluinations, wind and ohers. For prompt-based contrastive (in.2), usea mll of expertdemonstrations for each omain facto (i. yesterday tomorrow today simultaneously . ,10episode per fator. For prompt ensemble-based polcy learni (in. e. I setarget domins e categorized asor unee. Onthe other the nseen domains rfer those that are new imlying that tey arenot ncontre durin either learning phases We implement several baselines for comparisn. LUSR arecostrction-asedomin method which uses the autoenoder structure for re-resentations. ACO utilizsaugmentatio-driven andbhavior-drive contrastive tasks the of RL. is a stteof-the-art mboieAI model, exploitste prerainedCLIP viual encder state representtions. Inpromptbaed contastive we adopt various augmenation-driven nd contrastve learning,wher the lenth sets to 8. e. e., DAGGER ) fr egcentric-etaworl and CARLA. Zero-shot Performance. The poliies of each metod (CONPE and the aselines) are 4 he Sorce column presents the performance for those soure doains. In allevaluations, w 30 seendomans and10 unseen target doains. Te Sen Taget the performane fo the seen targt domains, the Unseen Target coun presents theerformance for the unseen target domais.",
    "When prompt-based contrastive learning, as shown in and explained below, wespecifically several to generate visual observation pairs for different domainfactors": "Then, we fine-tune T by lening a visual prompt pv through ontrastive learing,whre the contrativ loss unction is efined aEquation (2) in themain manuscrip. The contrst function P : {0, 1} dirimnates wetheran observation pair ispositive or n. Consier a pretraied model T paameterized by ta mas oservations o to embeddingspace Z."
}