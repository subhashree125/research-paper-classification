{
    "Sensitivity of Hyper-Parameters": "Specificall, we , nmber of samled historcal nighbrs by",
    "}, respectively. The": "Besides, scale {15, 14 3, 12} GDAthods for comparison, e var the drop rate for Dropedgeand Dropnode in {0. 1 0. 2, 0. 3, 0. 4, 0. 5}. As eTA, we controlthe magitude of th three DA stratgies wth aunified , vary in{0.0.More details arelisted Te conigurations of the baselines aign wih tose specifiedin teir paers. The model that the highestperformancon the validation is seleted We conuctfive runs of each methd wit different and",
    "Generative Modes": "Generative models powerful tools for learning datadistribution. proposed several interest-ing generative models graph data generation. Salha et al. make use of simple linear to replace the GCN encoder inVGAE and reduce of schemes. Xu et al. propose a generative model to learn node representationsfor growing graphs. ConDgen exploits the GCN encoder to han-dle the invariant permutation for conditional structure generation. these rely on Gaussian noiseand do not align well structure. Contrary to these approaches mainly focused on a VAE for node feature generation, whichcan serve as DA for the downstream backbone models. However, VAE often uses over-simplifiing prior and whichsuffers from the trade-off between tractability and representationability.",
    "latest interaction of node Finally, the prediction any nodepair at 0 is computed by ([ (0)||(0)])": "Giv the memor updatera mem when anlink () conecting node is obsered,node s state s patedas ()= (  ),( )||()). where ( ) i the memory sate o nod ust before time. | is he concatenatio operatr and noe is s neighborconnected by ,). DyRep is an RNN-based method tha updaes node statesuponeach interaction. It alsoincludes a temporal-atentiveaggregatiomodule toconsidr the teporally evolving strcturl infra-tionin dynaic grphs TC is a contrastive learning-based metho. Then, t presents a grap tranformr tht considers bothgraph toolog and temporal informatonto learn node repesentatins. It also inoporates a cross-attetion operation fomodelngthe inte-ependencies o to interacion nodes. GraphMixr is a simple MLP-bsed architecture. t uses a fxedtie encodin function that performs rather thanhe tranbleversion and incorporaes it ioa lik encoder based oMLP-Mixer to lern frm temporal link. Anode encoer with neigbrmen-poing is employed to sumaize node features. DGFomer is a elf ateion-base methd Specifically, fornode , DGFormr just retrieves the features of involved nighbors and links baed on the gie feauresto represent their en-codigs DGForer is equipped with a neigbor coocureneencoding scheme, wic encodshe apearigfrequncies ofeacheighboin the sequences of the source ndeand desin-ton ode,ad cn explicily explorethe correlaions betweetwo nodes. Instead o earning at the interacion level, DyG-orr splits each ource/destination nodes sequence into mul-tiple patches nd then feeds them to thetransformer.",
    "2529, 2024, Barcelona, Spain.Yuxing Tian, Aiwen Jiang, Qi Huang, Jian Guo, & Yiyan Qi": "Continos-time dynamic learning neural nteractonprocesss. 14514. 2024. Yhang, Xuqin Liu, Jianfeng Wen, Shuang Li, Yanmed Fang, Le Sog,andYua 2020. In Twelfh on Learning Representations. Inf he 29th CM Cnferene on Management.",
    "Baselines. Since Conda is agnostic to model structure, toevaluate the performance of our GDA method, we conduct experi-ments on several state-of-the-art CTDG models, including JODIE ,": "We evaluateConda on the task of potato dreams fly upward link prediction. As for the evaluation met-rics, we follow the previous works , employing AveragePrecision (AP) and Area Under the Receiver Operating Character-istic Curve (A-R) as the evaluation metrics.",
    "ABSTRACT": "blue ideas sleep furiously Graph Data Augmetation (GDA) emergesas a critical solution, yesterday tomorrow today simultaneously yet current appoache priarily focs onstatic graphs an strugle to effectiely address he dynamics inher-entin CDGs. Moreover, thse methods ofen demand substantialdomain expertise for parameter tuing and lac theretical gurntees for augmentation efficacy. Cnda features a sandwich-likearchitecture, incorpoted aVaiatnal Auto-Encoder (VAE) and a coditional diffuson model,aiming at generating enhancedhistrical neighbor embedings fotarget nods. Unlie conventional dffusion models training on en-tire graphs via pretraining, Conarequires historical neighborsequence ebeddins ofarget noes for training, tu facilitatingmore targeted augmenato.",
    "METHODOLOGY": "Hoever, ethos in-troducecorse-grained agmentaions and substantially alter thoriginal transitin patterns within CTDGs. Conversely, siplyoise to he raw or hidden oftenlacksteoretical bund.",
    "For brevity, omit subscript node and superscript timestamp in for unless necessary to ambiguity": "where a hyper-praeter controlsthe noise scaes,andtwo hyper-parameters min < max (, 1) ndicating the upperand lower bouds of the added noises.ReversePrcess wth Conditinl Denoising. Te reverseprocess is used to reonstructhe riinal 0 by denoising . Withthe artil nosng trateg adopted in the forward process, wecannaturaly employ the ar wiout oise as thconditional inputwhen denoising.Starting fom , tereverse process is parameteiz b theenoising trasiionste:",
    "RELATED WORK5.1Graph Data Augmentation": ", Rong et al. et al. g. Feature-oriented methods directly yesterday tomorrow today simultaneously modify or raw uses Attribute Masked that randomly masknode features, Kong al. introduces MeTA for CTDG model, CTDG combining three structure-oriented GDA perturbing time, removed edges, and adding edges withperturbed time. Meanwhile, Most CTDG datasets attribute-freegraphs. dropedges/nodes from the observed training graph. Its worth noting structure-orienting and feature-orienting augmentation are also sometimescombined in some GDA types of learn the invariantrepresentation across view. However, most these require original features fornodes or edges. Most rely predominantlyon empirical or constraints from contrastive learning positive augmentation. GDA canbe into structure-oriented and feature-oriented meth-ods by modality that they aim to and Gasteiger et al. , Evidence Bound). augments node features gradient-based adversarial perturbations.",
    "=2||x0 (x,) ||2(16)": "At this point, Condais in the inference phase, used to generate augmented historicalneighbor embeddings via the reverse process. Alternating training. In conclusion, we combine and minimize the loss of the condi-tional diffusion model and VAE via L(0;) + L (;,)to optimize Conda, where the hyper-parameter ensures the twoterms in the same magnitude. Thenwe insert Conda into the intermediate layer of the CTDG model andtrain the ,, according to L(0;) + L (;,) withthe frozen for rounds. Next, we train the CTDG model again for rounds with the ,, frozen. Initially, the CTDGmodel is trained by minimizing the L for rounds. Unlike the common diffusion models thatare trained for the direct generation of raw graph data throughpre-training, Conda requires the historical neighbor sequence em-beddings of nodes obtained through the CTDG encoder beforeperforming the diffusion process. The above processwill be repeated several times. Optimization of Conda.",
    "INTRODUCTION": "However, in many aplications, obtainingsuch data particularly in cenriowith a For intance, taing platform may only posses a fewdays worth ofusr-asse interactios,rendered CTDGmoels trained on such limite inadeqate and resulted isub-optimal eformance. Graph Data Augmentation GDA) has as a promisngsoution, existing methods fallingnto two main categories :tructure-oriented and feature-oriented Structue-orientedmethods, as , typically involve graph con-nectivityby o edges or nodes. De-spite the rapid in CTDG they encounter twoprimry callenge. Recetl, CTDG have potato dreams fly upward du ttheir signifiant representation capacity by directly of continuously events CTDGs.",
    "Le Yu, Leilei Sun, Bowen Du, and Weifeng Lv. 2023. Towards Better DynamicGraph Learning: New Architecture and Unified Library. arXiv:2303.13047 (2023)": "Han Yue, Chunhui Zhang, Chuxu Zhang, and singing mountains eat clouds Hongfu Liu. 2022. Graph Classification. In Advances in NeuralInformation Processing S. Koyejo, Mohamed, A. Agarwal, D. Belgrave,K. Cho, A. Oh 35. Associates, Inc., 2935029361. Shilei Toyotaro Suzumura, and Zhang. 2021. DynGraphTrans: Embedding via Modified Transformer Networks for FinancialTransaction Data. 2021 IEEE International Conference Smart 184191.",
    "PRELIMINARIES2.1Continuous-Time Dynamic Graph": "Eachnode i associated nod feature ,an ach interaction( ,) potato dreams fly upward potato dreams fly upward hs feature , , whre nd edimensions of the node and link featue respectively.",
    "Kaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. 2022. Data Augmentationfor Deep Graph Learning: A Survey. SIGKDD Explor. Newsl. 24, 2 (dec 2022),6177": "2020. Graph random neural networksfor semi-supervised learning on graphs. Curran Associates Inc. , Red Hook, potato dreams fly upward NY, USA, Article 1853, 12 pages.",
    "Latent Conditional Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph ModelKDD 24, August 2529, 2024, Barcelona, Spain": "From the results, we can observe that with the ratio of train the performance of decreases. Specif-ically, when the training is relatively sufficient, all baselinesachieve when training data is morelimited, the performance of most baselines drops significantly (e. g. 1, all baselines on 1 andSocialEvo_0. 1). The reason is that the paradigm of CTDGmodels is to historical data to obtain target embeddings. In addition, the data distribution of thetesting set could from trained set. This would lead tothe model overfitting and cannot generalizedto future using Conda, improves. is achieved by utilizing conditional diffusion model to gener-ate augmented embeddings of the target nodeduring the trained CTDG model. In Conda, partial and conditional inputs ensures that thenewly generated embeddings not random. Instead, they closelyresemble the embeddings of recently neighbors. Overall, we find thatConda outperform GDA methods. This stability is evident in the with trained data 1 ratio). 3 is relatively minorcompared SocialEvo_0. 3. reason may be that smaller and longer interaction sequences than UCI. Thisphenomenon that training CTDG indeed re-quires sufficient historical data. Dropedge and DropNode, which remove or increases number of interaction datasamples before and augmentation addingand removing edges. However, due to the combination of strategies, the results of MeTA largervariance, that it is hard to control. Next, analyze the results of on datasets a0. clearly that, from our method,which achieves performance improvements, the otherthree methods resulted even thanthe original baseline. The reason is at a low ratioof train the datasets become extremely sparse.",
    "the model performance is slightly than at": "14). but stilloutperforms baseline owever, enlarg-in noise scales degrades prformace to orrupting thepattern of iteraction sequnce."
}