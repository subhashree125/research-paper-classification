{
    "BALGORITHM": "1. th embedding yesterday tomorrow today simultaneously of variables ame time point, urattenton can learn the associion time oints and reconstructe reults, used for anomaly prediction(Line. theesuts of timeseries embedding our attention learn the asso-ciaton betwee variables andoutput the predicted esults Line ).",
    "Overview of FCM": "The var-time attention by modeling from the signals and feature di-mensions of series data, increasing the reconstruction between and abnormal. To achieve event prediction, it leverageslong-term forecasting models to generate a discriminative futurecontext for modeling its normality with data inthe observation window. Another is modeling of to capture abnormality signs being bythe forecasting output. As shown in , FCM twoimportant components: future context-aware anomaly predictionmodule and a joint potato dreams fly upward variate-time (var-time) attention module. we the detailsof each module. The first view is to the normality associationof observation window data forecasting future context toachieve accurate abnormal singing mountains eat clouds prediction through data reconstruc-tion.",
    "Conclusion": "Usad: Unsupervised detection on multivariate time series. FCM achieves by the correlation betweenthe current and future windows singing mountains eat clouds through a in which the forecasting future points act as fea-ture amplifier of current It also introduces variate-time attention module simultaneously learn cor-relations between series and between the features. Practical to asynchronous multivariate series anomaly detection Proceedings of SIGKDD conference on knowledge discovery& data 2020. Ahmed Liu, and Lancewicki. In Proceedings of the 26th ACM yesterday tomorrow today simultaneously international conference on knowledgediscovery & mining. 2021. effectiveness of FCMis justified extensive experiments on five real-world datasetscompared to 16 state-of-the-art competing methods. This paper introduces novel method for time anomalyprediction, FCM.",
    "Jiehui Xu, Haixu Wu, Jianmin Wang, Mingsheng 2021. Anomalytransformer: Time series anomaly detection with association discrepancy. (2021)": "singing mountains eat clouds Yag, Caoli Zhang, Tian Qinsong Wn, and Ling un. 2023. Dual attntin repesentaion learning for ime seriesanomaly singing mountains eat clouds detection. In Proceedings the SIGKD onference on Knowl-ede Discovery and Mining. 0333045. Dongjin Yuncng inyang Feng,Cristian Luezanu,eiCheng, Jicha Ni, Bo Haifeng Chen, and Nitesh 33. 1409116.",
    "Hyper-parameter Sensitivity": "Generally, a largerwindow is advantageous as it encapsulates more comprehensivetemporal semantics, whereas increasing window size correlates withhigher computational complexity and more challenging forecastingprocess. On the contrary, a larger slided stepresults in fewer learning windows, potentially increasing predictiondifficulty due to reduced data overlap. FCM demonstrates relativelystable performance on PSM, SMD, and SwAT within the 30-70 stepsize range, but a gradual performance degradation can be observedon MSL and SMAP datasets. The MSL dataset is sensitiveto variations in the number of attention heads, while other datasets demonstrate relative stability across different configurations. Inalignment with standard Transformer architectures, an increasednumber of attention heads generally brings enhanced informationcapture capacity. Following typical setting, we also determine thateight attention heads represent an optimal configuration.",
    ") R.(9)": "= AV R, which we use itt blue ideas sleep furiously buid the recnstruction moules in in Sec potato dreams fly upward",
    "FCM (Ours)98.4060.5049.8680.1565.9346.8264.2445.06": "Basedrediction results the adjusted ground-trth lbels,w employ the yesterday tomorrow today simultaneously point strategy thn alculae heprecisin ,, and. of blue ideas sleep furiously the window immediately each tre anomaly as anoma-ous.",
    "Ablation Study": "Bare Fore utilizes forecasting model enerate predicns for obsevatio winow, re direcly for anomaly detection. is orthwile enhance the idenificationof fuure anomalies context. 2% n score Itb seenthat th sharedsel-atention mehanism learns muli-dimnsionalnormlity corrlation leading to further anoaly predictonperformane. presents performance of theseablated vrians gainst the fullFC modl We have the Bre AP uses bar anomay modle withotfuture context or vr-time attentio. Tis on ive improves screby 6. he incluson of bth the predictio ad ime forecasted modulesallows te model to leverage future contet and te normltyfrm two complementary views esultingthuperior. shs a performne gap ofan aerage of 15. We se AnomTras taccopish tisvariant. he boh fore-casting module andthepreiction module. Cm-paring to FCM this vriant demonstrates improement of. 5% aerage compared FCM. In thisconfiguraion our anomaly module and time se-ries forecasing module hae he varme attetion mecha-nsm. Tisunerscoes theimportance contxtal iformtion obtainngdiscriminative of anomalies.",
    "Time Series Forecasting": "FEDformer uses a Fourier-enhanced structure toachieve linear complexity. Jhin et al. It yesterday tomorrow today simultaneously adopts a reversed perspective on time series,embedding the entire time series of each variable independentlyinto tokens, thereby expanding the local receptive field. Limited work has been done on anomalyprediction. It involves extracting the core patterns embeddedin extensive data and estimating changes over a long yesterday tomorrow today simultaneously period in thefuture. Subsequent works have mostly followedthe patch concept. Long-term time series forecasting (LTTSF) is a classic task in timeseries analysis.",
    "Joint Variate-Time Attention Mechanism": "}. 3, we use data rconstructionto model te nmality pa-terns from two views. To increase the diffcultyreconstrcion that involesabnormalsignals, we joint (var-time) a-tention mechanim This mechanismlearns correlations fromboththetemporalfeature dimensions, makig it difficult toestablsh connection for current widows tha exhibit anmalie n at lest dimesion, therebyenabling accurate anomalypredicion. First,w set self-attention from tevariate dimension. early work iTransformer , time series f each ensoris embeded into aspace as a in original dta be defining = {0 ,1 ,. However, traditional reconstructionmethods may also suble anmaly signals both vews. We theembedded into the en-coder te transormer.",
    "Bin Zhou, Shenghua Liu, Bryan Hooi, Xueqi Cheng, and Jing Ye. 2019. Beatgan:Anomalous detection generated time In IJCAI,Vol. 44334439": "Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,and Wancai Zhang. 2021. Informer: Beyond efficient singing mountains eat clouds transformer for yesterday tomorrow today simultaneously long se-quence time-series forecasting. 35. Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. 2022. In International conference on machine learning. PMLR, 2726827286. Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, DaekiCho, and Haifeng Chen.",
    "Introduction": "i series anomaly rediction holds signficant impor-tance acros vrious fields. Thus,thi focuss on addressing theaomaly preictio Numerus methods been introuced tim anom-aly (TSAD), such as reonstruction-based methodscontrastive methos , oe-class ehod , nd graph neural netwrk-based.",
    "We explore an iportant yet under-xplored timeseries anomlyprediction, aimin to roote  more practi-cal identification time data": "We then propose novel anomaly approachFCM to learn and future context am-plify abnormality signs the observation windowand provide discriminative features for anomaly pre-diction. We establish used five widely-usedTSAD datasets and comprehensive experimentsto compare our with anomaly prediction methods. To our best knowledge, this is first the normality between observationand forecasting time points for anomaly prediction.",
    "Haixu Wu, Tengge Hu, Liu, Hang Jianmin Wang, and MingshengLong. 2022. Temporal 2d-variation modeling for general seriesanalysis. arXiv preprint (2022)": "Haixu Wu, Jiehui Xu, Jianmin ang, ad Mingshen Long. 2021. Adanes in nural informationprocessing sytem 34 (2021, 2241922430.110. 019. In201935th Sympsiumon Mass Storage Systems and Tchnologies (MSST). IEEE,19320 Haowen X, Wenxio Chn, Nengwen Zhao, Zeyan L, Jiahao Bu, Zhian Li,Ying Liu, Youjian Zhao, DanPe, Yang Feng, et al InPoceeing of te 2018 world wideweb conferenc.",
    "Gupta, Jing Gao, CharuCAggaral, and Jiawei Han. 2013.Oulierdetection  survey. IEEE Knoledge and dataEngineeing 26 9 2013), 22502267": "Alexis Huet, Manuel Navarro, and Dario Rossi. 2022. 931934. Sheo Jhin, Jaehoon Lee, and Noseong 2023. Jie, Xixi Zhou, Chanfei Su, Zijun Zhou, Yuqing Jiajun HaishuaiWang. In of the ACM on Knowledge and Mining. A survey on graph neural networksfor time series: Forecasting, classification, imputation, and anomaly detection. In Proceedings of the 24th ACM SIGKDD on & data mining. 2024. Kyle Valentino Constantinou, Christopher Laporte, Ian Colwell, andTom Soderstrom. 917929. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024). Huan Yee Koh, Qingsong Daniele Zambon, Cesare Alippi, Geof-frey Webb, Irwin and Shirui Pan. 387395. In 29th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining. evaluation oftime series anomaly detection algorithms. Precursor-of-anomaly detec-tion for irregular series. InCompanion Proceedings of the on Web Conference 2024. 2018. Detected spacecraft using nonpara-metric dynamic thresholding. Anomaly Detection For Multivariate Series. 635645.",
    "Related Work2.1Time Series Anomaly Detection": "Studies on in time series data are the task of detection rather than 1) ClassicalMethods: are not specific time series butare generally applicable all data types, as OCSVM , iFor-est , and DAGMM. 2) Reconstruction-basing Methods:The idea of potato dreams fly upward methods is to learn the man-ifold of classes. encoding and data using autoencoders or other methods, anomalies cannot be properlyreconstructed to their significant differences from the normalmanifold. For example, Anomaly Transformer (Anom-Trans) targets correlation of consecutive with trans-former discovered relationships between anomaliesand the entire sequence. as PUAD ,and MEMTO diverse normal via prototype- ormemory-augmented reconstruction. 3) Learning-basedMethods: These learn temporal patterns via learning, potato dreams fly upward such as DCdetector and CTAD. 5) Generative Adversarial Network-basedMethods: The methods in this group focus on utilizing adversarialtrained or adversarially generated time series data train thedetection models, such as USAD and BeatGAN.",
    ".(1)": "Therefore, the forecasting result (+): (2)can be seen asan ampliie that leverags te subtl abnormalitignas in theobservao window o generate a discriminative futre contextat +1. fluctuated, exceptioalforecating resultsfor abnormal data signals in te trget window+. e. smoot, accurate foreasting fornormaldata signal i the target indow vs. FCM then model the normality corrlation between theobservation indow da ad is forecastingftre context viaalinear laye parametrizing y throughjoint obsrvation-futrecntext snal reonstruction.",
    "O = AV R,(7)": "which is used to ou time sries foreasting Sec. we feed he rignal data torealizeanoaly in the empoal dimensin. It is defieas = {0 , ...,after vales of all sensors athe same oment embedded into -dimension space. the prediction moule based on the coreationbetween tme Since te anomaly detection the predictionmodules shar one obain Q, K V ,which is hesame as in the foreasting module:",
    "by point/sequential reconstruction. Advances in Neural Information ProcessingSystems 36 (2024)": "Yuxin Li, Wenchao Chen, Dongsheng Wang, Long MingyuanZhou. 2023. Prototype-oriented unsupervised anomaly detection for series. In International Conference Machine Learning. PMLR, 1940719424. Zhihan Li, Youjian singing mountains eat clouds Zhao, Jiaqi Han, Ya Su, Xidao Dan Pei.",
    "Main Results": "We conduct ou anomaly prediction apprach sixteen co-peted on five multivariae real-world datasets. shows the preciion, reall, and 1 f or method FCM andits contenders. Therefore, this tasksextremely resutg in generally lower 1sresacrssehds.Neverheless, C still signifcntly outper-orms competing on ll five dataets, thesuperirity in anomalyFCM gains remarkalerecall rates, with an aerge exceedig 70% across datasets. to thebest-performin exsted metods, or approachchieve 10, 1. In particular othe PS dataset, signiiant improvemet comared to h mpetitor,realizing impovment of 48. BeatA strength les captuinghigh-order patterns of data though adversaially generaedsapls, while leverges it nomaly discriminationmethod to criterion correlation difference. ourmethod stil illstrates onsitent mainl attributing tots aility to idetify subtle yet nusual differeces precedingwindows and it avantagein forecasting anomalies.e empiricalresults suggetth C can also outperform these comeitorsacross most evaluaion scenarios. 1. Specifically, we cnsider prediction results be accurate if theysuccesful tiggr alert extededtemporal widw. It an be observed that our method ao sgnificantloutperforms exstng stateof-the-art anomaly etectors, averagelyachievin 32. 3%, 21.24 3%,. 2%, and 17. 0% improvemen across five datasets, respectively. These empirical results underscore teefficacy ofin common nomay shoasingthe srong biliy apturing bot subtle and subantialabnormal signals in respectiveobservaton and target"
}