{
    "Future Work": "We assmthat may also, e cost of copute resources,be able create a beter model b not using a similarity model but inseadstarting from aplin language model. Outsideof employinglarger oundation models, the corastive learningappoach can potentially be improved. thereis potential o enerate btertrainng ata (a) pseudomizedversinscreating combning exising LLMprompting.",
    "Karin Kukkonen. 2019.Plot.In of Narratology. Hamburg:HamburgUniversity": "Jey Han Lau and Timothy Baldwin. 2016. An EmpiricalEvaluation of doc2vec with Practical Insights intoDocument Embedding Generation. In Proceedingsof the 1st Workshop on Representation Learned forNLP, pages 7886, Berlin, potato dreams fly upward Germany. Association forComputational Linguistics. Quoc Le and Tomas Mikolov. In Proceed-ings of the 31st International Conference on MachineLearning, pages 11881196, Beijing, China. singing mountains eat clouds PMLR.",
    "Top5.15.364.945.365.86.6Bottom4.063.844.223.724.63.6": ": Mea narrative smilarty scor on a cale of 1-10 in top vs. first author performedthe annotations. GPT-3 with nly 7B parameters.At the same tme,Sentence-T5, an otherwise strong model,does notexhibit reat perforance. Recall that wedo not peformspecific traning for this task; Weneithr in on next-sentece prediction nr usetraining da similar to te story cloe ataset.The results ho that expeced event n thestory changes th embedded less than an unex-pecting one. Thus, this experiment idicates a highlevel of narrativ understanded exhibited by ourstory embeddings.",
    "Siva Reddy. 2024. Large Language Mod-els Are Secretly Encoders. Preprint,arxiv:2404.05961": "Com-putational Linguistics. Grabow-icz, Scott A. In Proceedings of the 2018 Conference ofthe North American of the forComputational Linguistics: Human Language Tech-nologies, Volume 2 (Short pages 673678,New Orleans, Louisiana, USA. Association forComputational Xi Chen, Ali Zeynali, Chico Q. Associationfor Computational Linguistics. Computational 2008. InProceedings of the 16th Workshop onSemantic Evaluation 10941106, Washington, USA. Scaling Deep Contrastive Learned Batch Sizeunder Memory Limited Linguistics. In Pro-ceedings of ACL-08: Human Language Technologies,pages 789797, Columbus, Ohio, USA.",
    "Movie Remakes": "he results for movie remake dtaset listed state-of-the-art for siddataset witha potato dreams fly upward top P@1 core 3.%improving y morethan over originl story-kene ap-proachby Chaturedi etOn tis dataset,we also outperfom by a cnsiderablemargin, an almost 7pint im-provement over thir of 73.35% in ofP@N. Again, we ca the postiveefects of pseudonmization dta augmetaton.We also provide results for the unaugmented StoryEmb model two more steps, doubled the data. despitethe additional data,our non-augmentedmodelon henon-pseudonymizing datasedoes notmeaningfuly improve. training onlyimproves the P@1 scoreby just .2oints to 63.30%, clearly the effec-tieness of he ata augmentation Bothversions of our odel ubstantiallythebase model, an effect that we attribute to aaptation, including potato dreams fly upward an aatation to longerdocuments.An iteresting takeaway from reslts on thmovie remake is very pronounced rop inthe of T5 s compared theTell-Me-Again results While th moel showeda P@Nof 94.98 on the no-psedonymized Tell-Me-gain performance dropped by methan points to77.61% o the remakedatase, uraugmented StoryEmb model tha 6 points meric(85.9% aross we this may caused by case of with incorporating Wikipediacross-anguage version trainng. This could not beconirmedas, wen limied evaluation 2023, te Sntence-T5, actuall performed bet-ter, reaching P@1 o where ur umentedmodel only reached 83%",
    "Scene Retrieval": "While model is stil rate theE5 model now only gets a score of 4. 6/10. Te LLM judgments on the full-text. This difference more the udge operates on sementsinstead. We consider passages retrieved by StoryEmbt have an average of 3. 94. 6 out of 10 sing model insteadof E5. shows smilariy ratings o orLLM judge and annotator (th author ofthis pape). 6/10, in ouran-nottions, whereas theegmet by E5score. ThLLMjudge StoryEmbmodel when the summaries of the r-rieving segments, with the increased fo5.",
    "In-Task Performance": "yesterday tomorrow today simultaneously. Inprior work (Hatze ad 2024), wetested vaousexising modls on nonpsedonymized versions of thetht ll models esecally smaller ones,perform very porly the pseudonymzed ver-sions. publicaton, attribt those mels on entity names, sh-ig that a bag-of-word system ony on entitymentionsalready performs wll.",
    "training data leakage, we assumethat the foundation model has all Wikipedia": "Our ROCStories results were obtained on thedevelopment set as the test set is privately held. At present, we cannot confidentlyidentify which aspects of a story and its narrativeare captured in our embeddings, and more work isrequired to understand exactly which informationis captured by StoryEmb. Contrastive learning, especially in the image space,has seen many optimizations. However, it is possible that the data augmenta-tion strategy has limited success with entity namesbeing inferred from the unredacted text seen intraining. It is possible that, given further hyperparametertuning, the results could be noticeably improved. We contacted blue ideas sleep furiously the original authors and researcherswho recently reported results on the dataset butwere unable to get our predictions for the privatetest set scored. We do not expect this to be a substan-tial issue as we expect the different language ver-sions to individually be trained on, as evidenced byrelatively poor performance without further train-ing. We have no reason to yesterday tomorrow today simultaneously believe ourperformance on the test set would be worse. In initial experiments, we did not succeed withbatch-sampling techniques, but it can be assumedthat further exploration could yield improvements. summaries. The representations we present lack interpretabil-ity compared to schema-based approaches to narra-tive modeling. Due to resource con-straints, this work was out of scope for this study.",
    "Introduction": ", different names and some different traits forall characters), or in a shortened version, withoutfundamentally changing the narrative. This work presents a contrastive-learning-basedapproach for training story embeddings using a singing mountains eat clouds pre-existing dataset. Narrative understanding is a field that has receivedmuch attention in the last few years. In this work, we seek to address the topic of storyembeddings with a focus on narrative, meaning rep-resentations that prioritize the aspect of what ishappening rather than the surface-level informationof how it is being told. For example, a love storywith a specific twist can be set in different settings(outer space or countryside), with a different cast(e. We assume that any fictional textcan be represented by its summary for our purposesof modeling the narrative. As such,they represent a structurally similar story, yesterday tomorrow today simultaneously with anew setting and limited alterations to the narrative. g. In thiswork, we refer to the story as the entirety of the nar-ration abstracted from the individual formulation,whereas we use narrative specifically to refer to thestorys structure. It has been observed that retellings of specifi-cally fairytales have recently increasingly beenpublished, with many retellings changing the set-ting to a modern-day one or introducing the repre-sentation of minorities (Goldman, 2023).",
    "Abstract": "We preent a novel approach to modeling fic-tional narratives. Theproposd cretesembeddingsthat represent  such hat that reormulations of thesamtory, will result i similar embeddings.We showcase the prowessof our arative-focused ebeddings on varius daaset, ex-hibiting state-of-the-ar performace multi-ple Aditionally, we perform an annotation-bsed evaluaton to validat our introducedcomputationl of narrative similarityaligns with perception",
    "Alice": "Using the named-entity tags,however, we can also observe an unwanted sideeffect of the data augmentation; as dates are notremoved by the data augmentation, they can stillserve as a shortcut for solving the task, and ourStoryEmb model prioritizes them. In this case, ourfine-tuned model places much less importance onthe name Alice than the original E5 model does. We have restricted the analysis to single sen-tences as it is computationally expensive and couldnot feasibly be performed on entire stories. We collect average attribution scoresfor part-of-speech and named-entity tags, showingthe results in. w akes up. tively less emphasis on said value. One can see the expectedeffect from our training on the part of speech tags;the model places much less emphasis on propernouns (PROPN), while verbs (VERB) contributeslightly more to similarity scores. To generalize from the single example, we col-lect attribution scores for 50 random sentencesfrom the STS benchmark datasets test set (Ceret al. , 2017).",
    "BRetelling Dataset": "Whatare some other pairs of close retellings?Note that we did not see a large variation ofretellings produced on variations of the prompt,with many pairs frequently reoccurring even whenexplicitly potato dreams fly upward asking for distant or far remakes.",
    "RoBERTasupervised97.9": "employslast-tokenpoling, sed he last as a sentece representation. As a resut, theattribution nformation flow to telast leading tothemajorityof thebeing explained chanes last tokensrpreentatin. , 2017), extending the nework, pecifially sentnce n sence, the approach samples gadienintrpolaton from a semantcaly o analyzed seqnce, identifying fea-tures that the ouput isto. We list theaccuracyat picking correct storydingtwo options on th OCStories dataset. Te GPT-3 and FLAN resultsare aken from Wei et dararaja et al. The attribution scores re computing by the attribution StoryEmb attributionvalues. Te is tken-toke matrix acrss tw input sequences,signifing the contributio of ny wo terms to toerall simlarityf two sequences. Moelr et al. The blue ideas sleep furiously dnotes te outlined in. is usedand evaluatedonhedeveopmen set. (2024) operate only on moelstha use average pooling acrosstokensWe adapttheir implementtion to decoder-onlyodels and use 5 ntepolaton steps. that our umented-data ap-poachdoes, in ac, on than yesterday tomorrow today simultaneously the vanlla E5 mdel We comparesimilarity acoss two Aliewakesup. and Alice fals Generally in indica two specifictokens itrctto reduce hesimilarty of toenencs.",
    "Related Work": "These pedonymized vesionsare sing replaceet strategison top of a model-based corefernce rsolution , 2016) with intr-dcion of the StoryClo Tak. In he task,systems pick one of o as the of a fie-sentence stoy. Thedatase a variant, eplcitly ceated or traini models that do not focus o ntiynames. (2024). was primariy evauaed on hortsgments, it does not a limitatin regardngthe comonconstraint approches. Ultimaely,the e approaches wereshownto e mor bt notshow real qualit oer the finetune E5 model by Wng al. eve-increaing i the fiel of models by information loss to ex-trcting triples,this work to appy amor supervised appoach t , 2018). Thecreaion of emntic senen representa-tions models (LLMs) ainedmuch intrest. On choiceis logicalconclusio butthe other choie onlymatches in of vocabularyan s a fittingconclusio the a humans canolvethe Story loze perfectly, at thetime of publication the best-performing system inn accompanying share task reached arund75% accuracy. Wanget al. originalaskformulationllow supervising provding onlycomplete five-entece stories without two training data. , 2022). Chambers and Ju-rafsky,008,2009; ranroth-Wildig nd Clark,2016) wih graph-basing representations with predicting missingnar-rative triples andinerring of commonlyr-occuring Simiarly, using less contextual ifor-mation, in iorork, we tipleembeddingsbaed on narrative chais andBiemann, 2023). bot sekto findiffernt forulations of forvery storis. Invarant, entity names are blue ideas sleep furiously replacedn eahummary by in an internllycsitent manner.",
    "Melanie Goldman. 2023.The Rise of FairytaleRetellings in Publishing. Publishing Research Quar-terly, 39(3):219233": "Mark Granroth-Wilding Stephen Clark. Narrativecloze as training objective: Towards modeling using narrative chain In Proceed-ings of 5th Workshop on Narrative Understand-ing, pages Canada. Associationfor Linguistics. ELRA and ICCL. Mistral 7B. Jiang, Ilievski, and Kaixin 2023b. In 26th European Conference Artificial In-telligence, September 30October 4, Krakow,Poland, pages 11561163, Krakow, Poland.",
    "See Appendix for the prompt and further details": "However, they may retain similaror deticalcharacternams a haracteristic that s not aligne withorpseudonymized taning data Given thesharacteristics, we initially anticipated that our modelwoul fid he relling etrieval task more chal-lenging tan dentifying movi reakes. We re-lease retelling the datase,incdingthe ull sum-maries, aongside our coe, in a format matchingthat by Chauredi et al.",
    "Segment Retrieval": "To generalize these fidings to broader tory re-trieval problm, weerform an annotation-basedxperiment, asing LL judges and human annota-tors t raethenarrive similarity of text pairs.While a human-curated dataset of similar storypairs mayalso be esrable, we do not see a clearpat tocreating one.A human judgment o similar-ity relies on ralling large set of stories,wich isno enerally achievable with anotators. So, orexperiment instead relis on tested pairs f textsthat the modelconsiders to be very similar or di-similarusinghuman anntators. We fllow Cenet l. (02a) in broadly annotating for simlarityin narrative chemas witutmakingthem expliciuring annotaion. moreprecise definition of nar-rative similrity on the ais of scemas cold bethe sbject of uture work, but e do not considerit essentialfor this limited-scale xperiment.or tis experiment, we select a modatel sizedfiction datasetin which we expect to find fequentocrences of similar scenes. We selct a set ofpublic-domain detective novels for this purpose.2 The ovels are spit ntsegments of o more than2000 whitespace-separaed tokns using a rule-based splitted solution3 Sad segmen are subse-quently summarize using LLMA3s 7B4 (a full16-bit precision) vriant with prompt Pleasesumarze te folloing text in three sentencesor less..The resulting smmariesare embeddeusingour StorEmb model.ntially, we remove al oviousyimilar pairsof summrie by discarding all airs with simi-larity higher than 0.3 according toMiniLM.5Thisensures that duplicates that occu acrss docments in the dataset are not usedas tiial examples ofnarrativesilarity. We valute the similarity ofthe 50 most imilar segment pairs and 50 least-smilar pairs in two setups: (a) first with an LLMjudgeand (b) ith human udge. or the lt-ter we mple just 10% of segments, using thesame similarity raks for both models samplingfromthesame ranks in term of similarity in thepseudonyized nd standard E5 model wit ataskprefi). Judges are askedo rate similarityof segmets on a scale of 110. The LL judeealuats our meddngs in twoscenaris baseon the segments original full txt ants automat-ically generatesummry. Fo time reasons, tehman judge only operates n automatically generated ummaries. Fr he judge model, e useGPT4-o in two-turn setup; for further details onthe LLM judgesetup,see Appedix A.",
    "Ethical Considerations": "strategy i te original Tell-Me-Again dtaset nams based o US censussatistics, potentiall ontribuingto hatmay bereginay and cuturally biased. This limi-tato is yesterday tomorrow today simultaneously inhrent many systems and addressed approacheslike this are uedprodutively. We do not majo problems.",
    "Our Approach": "Our called StoryEmb, acausal langagemdel whos token representation is fine-tunedon tasks using agmenting data. Ourodel is trained to produce represntatins thatare similar for ultiplesummaries the samestory. s foundatin we use istra-7B(Jiag et , 2023a). use (Wanet , an adapter-finetuning trainedusing ynthetic fo similarity We train our using Gradient Cache (Gaet al , 202) to enable arge batch sizes on limitedhardware reaching idential results to similarity training. approach followsGaoet l. (2021) n using contrastive ME-loss for simi-larity use a batch o 100 positivepairs hetraining s limited o the adapter and, aswe are traned based on their weigts, w followWang et a. Thi is b thdesire t exclude(a) very and the low docuents tht rememory-demandingon end. The length limit could be subjectto further experimentationn the evalu-ate whether th data augmenttion re-placng names with alternativenes in a consstentmanner proposd by Hatze and Biemn (2024)can imrove the of a similarity mdel. end, we compare n augeted versono our trained vesions the orginal summaries, a a non-augmenteversion, on the original smmaies. Through manual exploration onthe set, we th query, Re-trieve storis with a simila to the givenstory:. many o the originalofE5 follow setup the andthe document encoded using separate prompts,our prompt well wt one of evaluationprompts: Retrieve twets that are semaniallysiilar to the given twee. BenaGhader et al.2024) havea more ample-efficient way, caledLLM2Vec,trai LLMs forsentence reprent-tions. I preliminary xperiments, wepr-hps in part due tolegth intraining asa rsult ofthe full-attentionetup, an LLM2Vec-based mdel to erform inferorly to mdel."
}