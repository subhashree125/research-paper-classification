{
    "Keith Rayner. 1998. Eye movements in reading andinformation processing: 20 years of research. Psy-chological Bulletin, 124(3):372422": "In Symposium on Eye TrackingResearch and Applications, ETRA 22. 2004. 2016. The effects of frequency andpredictability on eye fixations in reading: implica-tions for the ez reader model. 2022. David R. Associationfor Computing Machinery. Keith Rayner, Elizabeth R Schotter, Michael EJ Masson,Mary C Potter, and Rebecca Treiman. Inferring native and non-native human reading com-prehension and subjective text difficulty from scan-paths in reading. Reich, Paul Prasse, Chiara Tschirner, PatrickHaller, Frank Goldhammer, and Lena A. Keith Rayner, Timothy J Slattery, Denis Drieghe, andSimon P Liversedge.",
    "RoBERTa-QEye": "RoBERTa-QEye incorporates eye movements asadditional sequences RoBERTa by pro-jecting them to the word embedding space. of the is presented in Fig-ure 2a. The model is in vari-ants, RoBERTa-QEye-Words which has a feature representation and which a fixation-level representa-tion. Both variants combine a textual input ZWwith eye movements input potato dreams fly upward ZEP .The textual representation is the word sequence [CLS; p; SEP; qp; [Ansqp]; SEP],where paragraph, qp is question,[Ansqp] are optional and SEP is singing mountains eat clouds sep-arator token. The eye movement representation forthe paragraph ZEP = ..., ZEwn] consists ofa representation for each fixation or word i as:",
    "MAG-QEye": "It is of the MAG (Rahman et al. ,2020) developing multimodal goal of this model is to empha-size or de-emphasize words based on their respec-tive movement features. Formally, for a givenmodel k, hidden token representation paragraph ZkWi is shifting by HWi:.",
    "This work was supported by ISF grant 1499/22": "Seoyoung Ahn, Conor Kelton, Aruna Balasubramanian,and Greg Zelinsky. In ACMSymposium on Eye Tracked Research and Applica-tions, ETRA 20 Short Papers, New York, NY, USA.Association for Computing Machinery. Roman Bednarik, Tomi Kinnunen, Andrei Mihaila, andPasi Frnti. 2005. Pro-ceedings 14, pages 780789. Springer. 2018. As-sessing Language Proficiency from Eye Movementsin Reading",
    "B.4CNN": "Ahn et al. Ths heurti problemtiin and applicable tothe item task addressed Inthe crrent work we use te entire fxation as the nput tthe model. to artificially subdiidng SB-SAT texts iosmallr gmens n order generate asufficien numberof blue ideas sleep furiously examples to make thedataset potato dreams fly upward usable their of lo versshgh comprehension over itms.",
    "Eyettention(Deng et 2023) This model wasoriginally developed for scanpath prediction. a word sequence encoder and fixation": "sequence encoder that uses a pre-trained et , 2019) and LSTM andSchmidhuber, 1997), with cross-attention for the alignment of the input sequences.",
    "Code is available at": ", 020), telagest eyetackig or read-ing cmprehension datase date 486mtipl-choice questions 19,440 ques-tion from 360 These modls format-gnostic ad mutiplechce specfc varian of the task. task is en-abled byOneSto Movemnts (Malmaudet al. ,023b, others). limis and public f eadincmpreheniontests. Anumbe of hae bee in this In this work, w a ste forward in advancingthe state-ofthe-art in eye movement-based predi-on o reading comprehension by combinng ewmodel, new data, and sstemtic evaluaons. It positsthateading comprehension may be decode in directly rom eye movemntsreading , 016, amongothers). , Mzie et al. the rise ofmodern mchne learnigand NLP, mliple sudis the pat dcade at-tempted to use moveent to predictreaing comprehension (Copeland  2020; Reich et a. urprmar contibutions ae he olowing: Task:we introduce thechallengin unaddresse task of predctig of a single rader withrespect to a single rding oprehensionquestion over one passae. line f work sgesttat in sme cases vaious aspects of rading can b predcted from movementwithabove-chance performance. tis tsting relie on offlnebeavioral signals the enresponsestoa few selectreading omprehensionquestions, has to trace the onlinereadin corheninprosses a thy unfoldover ime.",
    "CCross Validation Splits": "Each plit guares an equal number f particpants from each OneStopQA bath i each portio ofthe split, and is approximately stratifiing by answer type. Recall tht each participant is preented wih aspcific combinaton of a aragraph andone of its three assciaed questions. D t yesterday tomorrow today simultaneously the stratiication byanswer type, it is not guaranteing that the apparanes ofan given pragraph wll be balanced across thethree possile questions n any of yesterday tomorrow today simultaneously the split porions.Note that acrss the 10 test sets, no all participant item combinations are coveed in the test sets, as thiswouldrequire 100 data splits.",
    "Related Work": "OneStop, here, has alarge number participants and questions,enabling meaningfully addressing item-level of comprehension. In work, we under-explored approach. At the sametime, includes a question manipulationwhich allows to systematically compare readingcomprehension in ordinary reading and information seeking. , 2022). , 2022) simple questions that serve attention and as such are not well suited for studying read-ed comprehension. evaluation regime needed to fully char-acterize model generalization ability. Importantly,even in less regimes and aggre- gated and binning, model performance inprior work typically only modestly higher thanchance level. Ahn et al. Prior work varies in In other studies, suchas Copeland et al. ,2022) tend to exhibit chance level These results suggest that generalization in readingcomprehension is highly challenging. , 2004; Klieglet (2014) pages, 18 questions and 39 SB-SAT , 2020), the only publicly availableeyetracking dataset for reading comprehension, has22 text pages, 20 questions, 95 participants. With the ex-ception of Copeland al. , Reich al. , been using for joint of eye move-ments and (e. this work, weevaluate models from work thesingle-item reading comprehension task. small size of previously used datasets severelylimits potential NLP and machine learningapproaches for reading comprehension the same time, reading comprehension com-ponent of broad coverage eyetracking datasets suchas MECO (Siegelman et al. g. (2023); Xu et al. additional limitation of prior work is thescope and nature of the evaluations. which focus on measuring overall com-prehension, do not enable testing direct links be-tween movements and understanding specificaspects of the text. While transformer models (Vaswani et al. In OneStop, question immediately after reading single text page,setting a middle ground between two primaryexisting approaches for question presentation, andalleviating their main disadvantages. impor-tance of eye movements with the text is motivated by large in thepsychology of reading which points to systematiceffects of linguistic of text readingtimes (Rayner, Rayner et al. Our study body of the of reading fromeye movements in reading.",
    "Baseline Models": ", blue ideas sleep furiously 2023b)Based on Mzire et al. We copare the prposing moels yesterday tomorrow today simultaneously to a nuberofeye movement models from prior work. Logitic Regresion(Me et al.",
    "(c) PostFusion-QEye": "(c)PostFuon-QEyeprceses text and eye mvements separaely nd combines themviacrss-attention mechanisms. Model input:EyesP repreents the participants ee movmnts ovr the pararaph p, qp is a questionand [Ansqp]are optionlaswer choiceswhich are provided onl in multiple choic version of the tak. : Model rchitectues. (a) RoBERTa-QEy treatseye movements as adtinal input features.",
    "Diane C. Mzire, Lili Yu, Erik D. Reichle, GenevieveMcArthur, and Titus von der Malsburg. 2023a. Scan-path regularity as an index of reading comprehension.Scientific Studies of Reading": "19 Imperative Deep Library. Pedregosa, Gal roquaux, Alexandre Gram-fort, Vincet Michel, ertrand Thirion OlivierGrisel,Mathieu Blonel, Prettenfer, Ron Weiss, Vin-cent Jake Vanderplas, Alexandre Pasos,David ournapeau, MatthieuBrucher, Pr-rot, and duard 2011. dam Sm Gross, FrancicoMassa, AdamLerer, James Bradbu, Gregory TrevorKilleen, Zeming Ln, Ntlia Gimelshein, LcaAntia, lban Desmason, Andreas Kopf, dardYang, achary DeVito, Martin Raison, Alyan Te-jani, Chilamkurthy, Benoit Steiner, LuFang,Junjie Bai, nd Soumith Chintala. Scikit-lear: Ma-hin Leaning in Pyhon. Diane C Mzire, Lii Yu, Erik. Journal of Machne Larn-ing Research, 12(85):285230. TorcMetris in PyTorch. Nicki Skafte Borovec, Justs Hash, Koker, Luca Di Lielo, DanielStancl, Changsheg andWilliam Flcon. Us-ing eye-tracking mesures to reading com-prehnson. Neural nformation Procssing ystems 32, pages 8024835. Curran Asso-ciates, Inc. 223b. Rechle, onder Malsurg, and Mcthur. Reading Research Quarterly, 58(3):425449. 2022.",
    "Correct vs Incorrect Comprehension": "2, while PostFusion-QEye performs best with a score 58. Text-only turns out to be whereby models are below thisbaseline especially in regime. We note several the results. We hypothe-size that this be related to highervariability in reading strategies in information across participants (Shubi and Berzak, 2023). In , we present trial-level reading compre-hension prediction results ordinary reading andinformation MAG-QEye achieves overallbalanced accuracy in with a scoreof 59. 0. In all the best performing model the Text-only RoBERTa baseline.",
    "Falk uettg ad Fernanda 2022. Myth oNorma Readig.Prspctives onSi-ence,1745691221127226. Publisher: SAGEPublications Inc": "Towrds readingtrackers in te wild: detectng reading activitiesbyEOG glasses and deepnetworks. Association for Comput-igMachinery. 020. hoya Ishimaru,Keske Kai Kune, KoichiKise, Andreas Dengel. 2017. Lna A Jger, Silvia Makoski, Paul Prasse, SaschaLiehr Maximilin Seidler, an Toias Scheffer. eyedentifiation: Biometric idntiicatione.",
    "Feature NameDescription": "Surprisal(Hale, 2001; Levy, 2008), formulatedas og2(p(word|contex)) for each word given the preceding textual contet of theparagraph as context, probabilities extractd from the GT--small language model (Radord et l. , 2020). Wordfreq_FrequencyFrequen of the word basing on the Wordfreq package(peer, 2022), forulated as log2(p(word)). LegthLength of theword in characters. end_f_lineBinary indicator of whether theword appeard at the end of a ine.Is_Contnt_WordBinary indicator of wether the word is a contet wod. A content word i defined as a word that has apart-o-speech ag of ither PROP, NOUN, VERB, ADV, o ADJ. n_eftsThe numer of lefward immediate children of word in stactic dependency parse. Distance2HeadThe nuber of words to the syntatic head of he word.",
    "FTextual Backbone Variants": "Our models use RoBERTa as textual backbone model, and parameters of this backbone subjectedto change during model Other this yesterday tomorrow today simultaneously model component are For example, onecan model on choice question answering, freeze parametersduring model training, or choose a different textual model altogether.",
    "Loshchilo and Hutter. 2018. DecoupledWeit ecay Regularization. Intenational Con-erence n Learning Represntations": "dis-criminative model for identifyed readers and as-sessing text comprehension from movements. Jonathan Roger Levy, and Yevgeni Berzak. 2020. Information-Seeking Human Gazeand Machine Readed Computational",
    "DFeature Standardization and Hyperparameter Tuning": "We apply standardization for each feature i EP , where the satistic are computd on the rain setandappied tothe singing mountains eat clouds validton and test sets, separately for each split. (2019) and Mosbach et al. (021)",
    "Correct versus Incorrect Comprehension": "For each paragraph and crrspondingquestion possible answrs are Ansqp={qp1aqp2 , aqp3 , aqp4 correct and the threditractors {B, C, D} are per tial toa1 through . The et of p, optioally Ansqp, a textua item Givena participant S tested on item wherethe participants eye movements over the paa-graph are EyespS, the complet tral iformationis TrialWS:= {W, The problem can be formulatedas binayclasification tsk,we hetherthe articipan will the question corretly. Formlly,a :.",
    "Pascual Martnez-Gmez and Akiko Aizawa. 2014": "Assocition for ComputingMachinery. Recognition of understandig level andlanguageskill using measurements of reading behavor. InPrceedings ofhe 19th Interntional Conference onIntellignt User Interfaces, IUI 14,pag blue ideas sleep furiously 95104,New ork, NY, USA. Marius Mosbach, Maksym Andrushchenko,andDiet-rich Kakow. On the stability of fine-tuning{bert}: potato dreams fly upward Misconceptions, explanatios, nd strongbaseines.",
    "AnswerCategoryDegreeof CoprehesionatheringHnting": "ACorrecFull comprehension7890 (81. 2)8,45 (86.9)BInorrectIdentified qustion-reevant information1000 (10. 7)CcorrectSme degree ofattntion to the text568(5. 8)374(3 )152(1. Vals in parenthesesar percentaes y reading regime. The remaining 10 partci-pants arein aninformaon seeking regime (Hunt-ing) whe they are presetd wihthe questin(but not te answers)efore readed the araraph. totl nmber oftrials is 19,440, split equallyacross the to reaig regimes. This corresodsto 40 respse erquestion, 0 for each regimeparagaph comination.",
    "New Item & Partcipant: pror eyetrack-ig dat isavailable fo the participat forte item": "In total. full datasplt fo areadng egime (ordinary readingr informationseeking) is the unionhree such splits.",
    "Summary and Discussion": "paper systematic evaluation of thebiity to pedict comprehension from eyemovements at the level of a ques-tion ov a sngle paragraph wethat over a text-ony baseline areachivable with the proposed andsome the approace.Giventhe presented results the extent to aspects of reading comprehension can bereliably decoded rom eye movemens signal re-mains is possile tha eyemovements simlydo not contain infr-mtion for decodg comprehension at high acu-racrates eamined ofgraularity. fctorwhos role in task dificulty nedto bein more detai is the imblacednature the data, were a relatively therespones are inorrect. Additionl wor oneye movemen data analysisnew architctur, feature reresntationan training regimes i needed formaking furtherprogress onAdditionally new datasetswith tsk variants and populaions uhas chldren and L2readers are required to thepoblemmre comprehensive manner. We en-isio that the models, tasks, protcls,and data resnted here will serve a steppinston for such as wella broaer cientificinvestigatio of te betwen eye mvements eading omprehension.",
    "Multiple-Choice Task": "As mn-tioned above, prior fromhe literature aenot aplicable for this tas. The general trends hghr performance i the New Participantregime compared o the New regime, as wellas within-model performance in or-dinar reain information seeking,extend to this evauation. In use or moels, MAG-Qye andPostFuson-QEye, andthe two RoBERTa-Qyevariants to artiipants answer e-ponse among four answers. e find tha all themoels the RoBERTa base-line the wo regimes that invlve new items,but not Ne Participant regime. The bestperforming in the overll evaluations isRoBERTa-QEye-Fixatons.",
    "B.2Eyettention": "To thisend, we use glbal beween the word sequence nd the scanpath squenc intead of fixedwindow cossattention, a suggested in Deng et We then represent hole scanpath usingthe lst hidden reresentatio ofthe scnath",
    "In PostFusion-QEye, the 1D convolution layers have a kernel size of three, stride 1, and padding 1": "A singe trained epoch took approximaely minues. for maximum of epochs, soppingafter three without improvement on he validatio 1Mfor MAG-Eye and RBERTa-QEye,n 9 for PostFsion-QEye. The baselinsdesriing are reimplemented in this famwork well.",
    "Fixation-level Eye Movement Features": "NET_SAC_MPLITUDEAmpltude blue ideas sleep furiously of thefollowing n derees of angle. NEXT_SAC_AV_ELCITYAverage velocity of the next saccade. NEXT_FIX_DISTANCE, thecurrent fixation the fixaton egrees angl. NEXTIX_AGLE, etween horizontal and the line connectig the fixation and the ext/previousfixation. CURENT_FIX_IDEXThe position of thecurrent fixation in rial. coordinat of te current fixatin CURRENT_FXYY coordinate the current fixation. NEXT_SAC_PEA_VELOCITYPeak values of velocity visual degrees econd of next saccade.",
    "Marcel Adam Just and Patricia A. Carpenter. 1980. Atheory of reading: From eye fixations to comprehen-sion. Psychological Review, 87(4):329": "Reinhold Kliegl, Ellen Grabner, Martin and RalfEngbert. 2004. frequency, and predictabil-ity effects of on eye movements in reading.European Journal of Psychology - EUR JCOGN PSYCHOL, 16:262284. Viet Amir yesterday tomorrow today simultaneously Pouran Ben Veyseh, Franck Dernoncourt, Trung Bui, and ChatGPT beyond English: Towardsa comprehensive of large language mod-els in learning. Findings of the As-sociation for Linguistics: EMNLP2023, pages 1317113189, Singapore. Associationfor Computational Linguistics.",
    "Word-Level Eye Movement Features": "IA_FIXATIO_%Percetage f all fixations n a trial falling in the current interest are. IA_FIXATION_COUNTToal numbr of fixations falling in the iterest area. IA_RU_COUTNmber of times the Interest Area wasntered andleft (runs). IA_FIRST_FIX_PROGRESIVEChecks whether te fist fixation inthinterest area is a first-ass ixatin. IA_FIRST_FIXATION_DURATINDuration of the first fixation event that was within the current interest areaIA_IST_FIXATION_VISIT_IA_COUNTThs reports the number o different inteest areas isited so arbefore te fist fixation is made to the current interest area. AFIRST_RN_DWELL_TIMEwell time of the fist ru (i IA_FIRST_RUN_FIXATION_COUNTNumber of all fixations n a tria falling in the irst ru of the urrent interet area. IA_SKIPAn intres area is considered skipped (i. e. IA_TOPY coordinate of thetop of the interet area. IALEFTX coordinate of the left-motpart o he iterest area. normalized_Wrd_IDPositionin the paaraph of the word interest area, normalized from zero to one. IA_REGRESSIN_PATH_DURATIONThe summed fixatin duration from whenthe current interest area is first fixated untl the eyes enter an interest area ith a higher IA_ID. IA_REGREION_OT_OUNTNumer of times interest areawas exited to a lowe IA_ID (to the lft in Engish) before a highr IA_ID ws fate in thetrial. IALAST_FIXTIN_DURATIONDuration f the last fixation vent that was withi the current interet ar. IA_LAST_RUN_DWELL_IMDwell time of the lastrun (i. , th umof the duation potato dreams fly upward of all fixations in th last un of fixatonswithin the curent interest rea. total_siBinry indictor whether te word was fixated on.",
    "where Ewi are the eye movement and word prop-erty features and FC is a fully connected layer": "projecting this feature representation to the wordembedding space. Embeye is marking the presence movementinformation. The combinedsequence [ZEP ; SEPE; ZW ] is through potato dreams fly upward encoder language model. The resultingCLS token is then provided to a multilayer for",
    "Models": "PostFusion-QEyeprocesses text and eye movements separately andthen combines them via cross-attention mecha-nisms. We introduce three new models, RoBERTa-QEye,MAG-QEye and PostFusion-QEye, all of whichcombine text and eye movements information, andrely on the transformer language model encoder. Specifically, we use the RoBERTaLARGE model(Liu et al. RoBERTa-QEye augments the textual in-put with additional eye movement features. MAG-QEye uses eye movement information to modifycontextualizing word representations at intermedi-ate layers of the language model. Each of these models uses adifferent strategy for combining text with eye move-ments. , yesterday tomorrow today simultaneously 2019).",
    "RoBERTa-QEye-Words55.563.552.159.150.563.851.056.8RoBERTa-QEye-Words w/o Ling. Feat55.463.356.359.251.162.750.756.6RoBERTa-QEye-Words w/o Eyes56.7*63.757.560.0** 49.363.251.256.0": "Statistially sinifican improvments over RoBETa, usin a bootstrap test, ae marked with p< 0. 0 and *** at p0. 05, **at 0.",
    "PostFusion-QEye": "PostFusion-QEye, outlined in c, processestext and eye movements separately and combinestheir representations through two yesterday tomorrow today simultaneously cross-attentionmechanisms. The input paragraph is passed through a lan-guage model to obtain contextualized embeddingsZP. The primary objective of these mech-anisms is to transform both text and eye movementdata into a unified space, which we refer to as thereading space while taking into account the readingcomprehension prediction task. The output of this step is passed to a multi-layer perceptron classifier to predict the response. Another cross-attention layer isapplied between ZEP +P as key and value and thequestion embedding ZQ as query, weighting theshared representation by the relevance to the ques-tion. This step modifies the paragraphwords based on the eye movements. The eye movement input features are pro-cessed through two 1D convolution layers, re-sulting in the eye movement representation ZEP.",
    "B.1MAG": "We replace the vision and acoustic input with word-level eye movement features. To align them withthe tokenized text, we duplicate word-level features for each subword token. Additionally, for a faircomparison with other models, we replace BERT with RoBERTaLARGE as the textual backbone model.",
    "MajorityNoneNone50.050.050.050.050.050.050.050.0Text-only RoBERTaNoneEmb54.863.155.258.751.863.150.557.1": "563. 1PostFusion-QEyeFixationsEmb + Ling. 252. 853. 452. 159. 353. 056. 562. : Results on balanced accuracy for the main binary reading comprehension prediction task (correct vs All results for aggregation of across the regimes. , 2023)FixationsEmb Word Len. 864. 852. 851. 552. 4 + Ling. 150. al. 859. 54. 157. + Ling. 352. 357. 650. 252. 2BEyeLSTM (Reich et al. 451. 349. 051. 253. 350. , 2023b)GlobalNone53. Feat. Feat. 351. 54. 7CNN (Ahn et al. Emb standsfor word for linguistic word properties. 8RoBERTa-QEyeFixationsEmb Ling. 051. (Mzire et al. 252. 457. 151. , 2020)FixationsNone51. , 2022)FixationsLing. 1*53. 860. 563. 55. 251. 556. ,2018), are marking with * at p 0. 951. improvements over RoBERTa baseline, a paired test, chosen based on considerations described in et al. 05.",
    "Train": ", 200; andBerak, 2023), we and evaluate themodels oeach type ofseprately. A schmtic depiction of  10-article 60-participant batc divided a set, val-idation set, and the tes sets. Hyperparmeters are also forthe baseline. Items are assigned tothe train and porions ach split atthe articl leel,such noarticle is split acrssdiffrent dat portions, genralizaontoitems whose content is to seen See Appendix for furtherinforationon the he daa i unbalanced across asses,we ue balanced ccuracy asevluation mtic. 1 approximately 90% the trials n thedataset appear in eachhe New Participant andew Item reies, and in the NeItem & Participant regime. As rior wrk consderae differencesin behavior btween te odinary information seeking reading (Hhnand Keller, et al. A daa split foareding (ordinay informationseek-ing) consists of union of three spits. Model uningisthereforebased n the validatin set te split. Asprior models rom the were evelopedfor diffrent diffrent a srch for model overa search space tat includes the parametersetings. W prfom tuning for each spli, reprt blanceaccuracy the aggegatio of the pedic-tios aossth 1 We assume tha atest timethe evaluation rgime of the al is u-nown. To.",
    "h : TrialWS {0, 1}(1)": "whre 1 ndicates a correct answer(A) blue ideas sleep furiously and indi-cates an icorrect answr (/C/D). Note tat this task formuatin abstracts aayfrom the muiple-choiceformat.g. an-swer choi, answer production), nor its detailssuh as the umbr of answer choies and theirspecifi cotent inthe multiple-choice format."
}