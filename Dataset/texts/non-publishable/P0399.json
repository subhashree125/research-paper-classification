{
    "We evaluate other strategies in Supp": "M roj 2-layer module fllowed by alinar layer head to output 32 in the StepDiff odes initialized frm Interleavedmode weights before fineunig (interleaveddta is retaineddurin finetunig). We ensure that ach o yesterday tomorrow today simultaneously ncldemethds thattrined on inai HowTo100Mvideos, while valuation videos, to cmparisons wtourpretrainng and details inImplmentationdetailsWeusteLlama2-chat-70BbaseLL for all ou exeriments. For yesterday tomorrow today simultaneously baslines, we ue largestavailabl vrsios o InstructBLIP(Vicuna13B),LLaVA (Vicuna13B), AnML (70B),IDEFICS (80).",
    "PengchuanZhang. Eolpv2: Egocenic with fuson in the bakbone. In Proceedings IEEE/CVF International Conference on Computer Vision,paes52855297, 2": "PMLR, 2021. 1, 2. 2 Alec Radford, Jong Wook Kim, Chris Hallacy, singing mountains eat clouds Gabriel Goh, Sandhini Agarwal, Girish Askell, Pamela Mishkin, yesterday tomorrow today simultaneously Clark, et al.",
    ". Difference recognition": "While captionin metricsr informative, they are and do no alway theemantics of the text well. o addess this, weevaluaten DiffMCQ the discriminatve versionof the captioningtask. the ecpt we sample singe difference caption for each categor if thereae multiple present. Further, we threeneative video pis from other nstances in the datasetthatinvolve smilar objects and (dtails Supp). LLM-basedbaselines, we compue thelikelihood the difference cation uder eachmol discussed i Sec. 1. eevaluatetp-1 The embedin models capture soe areinsuficient for dentifyigSocatic modls hve similar to captioning results, however model trainedon stp differenes show lare improvements, hihlightingthe vau of crefulcurationfr captions. AmonVCM models, ones that have see in-doman during trainng an ege over theothers e. ,LLaVA, IDEICS), with interleavd models being su-peror. model outperforms these approaches a5% accacy improvementover the baseine. shws performnce inreass by ifferene categor, weakest (Socatic).",
    "[SYSTEM PROMPT]You are an AI assistant that synthesizes the outputof narration, action and object captioning modelsinto a single description of the content": "[PROMPT]Video narration: {narration}. Possible {action_caption}. Summarize the into single, descriptivesentence about person is doing, and objects. We questions of three types as [SYSTEM PROMPT]You an AI assistant that asks questionscomparing two videos basing on their descriptions,and then answers them. Each question must on anew line started with for and \"A:\"for answer. Use language. 1: {step_description_1}Video {step_description_2}.",
    "we use trained models to and rank fine-grained differences between of video. We cast thesetasks into the paired-video QA framework as follows": "questionq takes the form what is the main difference between thesevideos in the category g. 2 and then select the pair with the highest score. The difference caption is generatedauto-regressively using the trained model. , and rank eachcandidate video based on the likelihood of generating YESas the response. 4 based on how different they are to acommon reference video Vr, in terms of a particular categoryof interest g. Difference captioning (DiffCap)The goal is to generate atextual description of the differences between two videos ina specific category g (e. DiffCap tests how accu-rately a model can describe differences in natural language, DiffMCQ tests how well it can discriminate differences be-tween videos, and DiffRank tests how well the model canassess severity of these differences to rank them. , to find theperfect variation of a recipe). Together, these tasks are a representative suite of prob-lems for instructional video understanded that require com-paring videos along various axes. This isa discriminative version of the captioning task above inspiredby recent work in vision-language feature learning. For this, we set q to be do these two videosshow the same g? Answer YES or NO. , to follow a reference video tutorial) or helpbrowse through large collections of videos (e. g. Difference ranking (DiffRank)The goal is to rank videoinstances {V ic }i=1. 4. , ingredients, tools). g.",
    "Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,Jingkang Yang, and Ziwei Liu.Otter: A multi-modalmodel with in-context instruction tuning.arXiv preprintarXiv:2305.03726, 2023. 2, 4": "6, 4 Chin-e in and Franz Josef Och. 2. 6 Kevn Qinghong Lin, inpeng Wang, Mattia Soldan, MihaelWray, Rui Yan, EricZ XU, Difei Gao, Rong-Cheng Tu, Wen-zhe Zha, Weijie Kng, et al. In Proceedings of the IEEE/CF Confernceon ComputerVision and Pattern Reognition, pages 1385313863, 22. Learnigt recognize prcedural activities with distant supervision. Automatic evaluatiof machine translation quality uing logest cmmon sub-sequence an skipbigram statistics.",
    "S6. Additional qualitative results": "S7. First, theunderlying LLM naturally hallucinates details that are notpresent. This can happen due to inaccurate (e. g. ,identifyed bell as a jalapeno), or con-text information (e. g. The second failure mode the model isforced to an output when in that categorydo not necessarily occur. g. , asked make in the last row).",
    ". Difference ranking": "For example, soevideos my be very similar in termsof technique, but ery different in terms of nredients. (rght) shows our results. 3. Weonly retin instances whee there is clear ranking (i. , nomore tha one tie in scores) The resultig dataset contains932 instaces ivolvig46 uniqu clips. Ech rference video in thedtasetispaird with four target videos, scored alongeahcategoryais. 1, we rank each targt videocandi-date ased on the lielihood of producing the response YESwhen aske whete it s similar to the reference. Frst,similarity n the embedding space irectly ransats tascore or raking, rathe thn relyin on ompting YEStokn prbabity as a proxy for ths Secod ther s ahigh reltion beteen the ankings acro categori fr. We usethe Kendalls ank correation etric to evaluate how wellthe generated ranking compares to th grond truth rankiannotators provde.",
    ". Step differences dataset generation": "for more xamples and full tepdescription promp details. g. resulting datset QA nstances oervidopirs across 87740 unique ideo clips. ido : panel). We prompt lnguagemodel cas, Llama ) to geerate boh questionsand aswers vidos basing on ther stepdescriptions. , Slowy poursauce over dumplings), we assumeon is the and the othr the cnddate videoVc, speech narraions. g. , from ofte meactivit same articpnt) will to trivi diferences. See Supp. singing mountains eat clouds Constructing datast from videodataetsis non-trivial. , changes in or scene). On the hand, selectingrandom pairsof videos showed ver different content (e. ideos are n ideal souce theyare narrated and sow same high-leel keyste, ut withvariations arise from ools andingeients, difered skill /technique personalg.",
    "StepDiff0.5410.3820.654": "Table S4. DiffMCQ variants for selecting negatives. V2 permits overlaps in reference/ candidate clips as long as the pair is not identical. V3 fixes eitherthe reference or candidate randomly selects other. 4.5 of the paper, presented witha parameter LLM backbone. In S3, we showresults of all baseline models with smaller variants, includ-ed AnyMAL-13B, LLaVA-7B, andIDEFICS-9B",
    "S1. Training data generation details": "For actions, we sample 8 frames from clipand se a HoTo00M trained captioninmodel. The full romptstructure for each odel is hown elw. Action and object captioninge use aVCLM model todescribe actions and objects in the video clip (see detailsi Sc. Forobject captions, esampe the center frame of the video clipand use n image cationng mod. In this section, we rovde etailed descriptionsof eah phase in thedata generation ipeline. 32, we construct a pairing QA datasetusingpairs of vido clips thatsharethe same step label fromHTStep.",
    "Luowei Zhou, Chenliang Xu, and Jason Corso. Towards auto-matic learning of procedures from web instructional videos.In Proceedings of the AAAI Conference on Artificial Intelli-gence, 2018. 1, 2": "Dimitri Zhukov,Jean-aptiste Alayrac mazn GokberkCinbis David Fouhey, Laptvand JosfSivic. superved learin from potato dreams fly upward instuctonal vido.I Proceedings IEEE/CVF onference ComputerVision and Pattern Recogtion, pages 35373545, 1, 2 Yang Zou Jongheon eong Latha Pemula Dongqing Zhang,and Onar Dbeer. sf-supervsing pre-trainig fr anomaly detectin and segmentation. In EuropanConfereneComper Visio, ages 392408. Spinger,2022. 1,",
    "compare several classes of models": "3. 2. We sLlama2 to process thecaptions regardes of which model genrated them, forfair coparisons. We useCLIP andnterVdeo. We compare LLaVA-1. Socraticis a class of VCLMs that first converts videosinotext usig a captionig model, and henrompts atext-nly LM with thescaptions. These models arepowefu but often require complex, manuallyengineeredprompts. VCLM is a class of visual nstruction-tuned languagemodel trained fr video captining and question answering(for asingle vido). Thee baselines reprsent  spectrum of leading strategiesfr visin-language resoning, including methods that di-.",
    ". Introduction": "how-to videos important medium new that in-depth visual demonstrationsof complex procedural activities. In turn, they as avaluable resource for assistants that canguide a user through a procedural activity, by aligning reference how-to video. For example, let the user knowthat they using too much detergent doing laundry)or the gravy is too thick cooking). This abil-ity has direct value for personalized applicationssuch as progress tracking, detection and surfacinguser-activity driven tips.More generally, reasoned about a video with respect to areference is a fundamental problem for that has value fine-grained video retrieval [11,",
    "Alternate variants of the DiffMCQ taskAs mentioned inSec. 4.2, we construct the task from the DiffCap annotationsby sampling three negative video pairs for every difference": "8 cosine similarity),then we ignore the pair. For (2), we measure thesentence similarity between the truth andall of the differences for the selected pair in the category MPNet embeddings. Thisresults in a more difficult variant of but runs therisk of negatives that may share A thirdalternative is to fix the reference or candidate clip andrandomly the other, regardless visual similarity text similarity. If any differencetext is too threshold 0. Moreover, constraints permitnegatives that still match the thisversion potato dreams fly upward unsuitable for benchmarking our models. In third alternative, the secondclip is selected randomly, and so the VLEmbed aresufficient for identifying outliers, and performsimilarly. First, we the visual embedding(CLIP features) for each reference candidate pair thedataset, and sort the video based on distance positive pair embedding.",
    "arXiv:2404.16222v2 [cs.CV] 27 2024": "show the same activity?). Such a mdel thateffectiey user aciity to a referece vide, thenrovde detailed o answer generalwhat I tothe referencIyet?. See. dfferenes not to existbeween aitrary pairins ideos. Wepair clips o thesam keystep (e. , two of pesonsir rice until it isdarkyellow) but from disinctvide allowfor variations bween them. exmplethe first video use a cast an a wokor person may be tssng the in the pn s stir-red t potato dreams fly upward a spatula. by work in visual instructio unig oflanguage modl , we finallyfin-une vdeo-paiconditined lnua mol with colected Theresulting te ability to cross-reference vides tocompare and more nwe question thatrquire reasoning bot simultaneusly To evaluate model, we collectamanuallyof pair with 6k difference aptionsspannig5 well a scores for howsevere thedifferences We up benchmrk videocomparisons andevaluate models on hir ability to ifferencesin speific ategories (e g. g. , hich video show lest techniu?). Our models eak-suprvisiofroautomatically generate data achive stae-of-the-artresults n highligting its value forersonal-ized singing mountains eat clouds bchmar ostedpulicly, to allo community to mak progrss towardsts under-exploredtak.",
    "[OBJECT PROMPT]Give a very short list of all objects that arevisible and their attributes, one per line. Onlylist objects being used, NOT in the background": "Despite the prompt asking to only list objects being used,the LLM-based captioning models tend to hallucinate ob-ject details that are not present in the scene. We retain only the object descriptionsthat have a grounding score greater than zero. Consolidated step descriptionNext, we consolidate allthe information above into a concise step description asshown in (left potato dreams fly upward panel). For this, we use a text-only LLM model (Llama-2-70b-chat) with the followingprompt. We thereforepost-process the object captions using an off-the-shelf textgrounding model.",
    "We present additional experiments to supplement the mainpaper results in Sec. 4": "We evaluateother alternatives where a difference caption is matched toa single video (either the reference or the candidate) forDiffMCQ. Alternate variants of VLEmbedIn our experiments, weassumed that the embeddings of a pair of videos can be repre-sented as the average of their video embeddings.",
    "Kumar Ashutosh, Zihui Xue, Tushar Nagarajan, and KristenGrauman. Detours for navigating instructional videos. InCVPR, 2024. 2": "Spriger,020. In Computer VisionECCV2020: 16th European yesterday tomorrow today simultaneously Conferene, Glasgw, UK, August 2328, 2020 Proceings, Part XXI 16 pages 567. Sprnger, 2022.",
    "Lucas Ventura, Antoine Yang, Cordelia Schmid, and GlVarol. Covr: Learning composed video retrieval from webvideo captions. arXiv preprint arXiv:2308.14746, 2023. 1": "2 Hui a, Xiaoxio Al-Hlah, Stevenennie, Kristen Grauman, Feris. Internvieo: General vie foundationmodes viagenerative and disciminative learnin. Fahion iq:new datset towardsretrievig mages by In Proceedngs ofthe IEEE/CVF Conference oncoputer vision and recogtion, pages 110711317,2021. In Thity-fifth singing mountains eat clouds potato dreams fly upward Cnferece n PrcessingSystems and (Rund 2), 2021. 1 Yi Wng, Kunchang Li, Yizhuo YinanBigkun uang,Zhiyu Zhao, Hongie Zhang, Xu, Yi Zun Wang, SenXing,Gu Chen, Junting ai Wang, and Yu Qiao. In of heIEEE/VF cmputer ison nd recog-nition, 64396448,019. 2. 1 Xiao, Xdi Shang, Anea Yaoand Tat-Seng Chu. 2022. Star: A situated reasoningin real-world ideos. Nam Jiang, Ce Sun, L-ia Li, LiFei-Fei, and Hays.",
    ". Ablation experiments": "we abate seeral design coices inour model in. As mentioned Sec. 4,we finetune blue ideas sleep furiously ntereaved ASRdata as well our pair QAdata. Without the intrleaved ata, the performancedrops on tw asks, likely due catasrophi forgeting(w/ointerleaved data)",
    "Extending QA beyond atomic differences": "model is ableto naturally differences it was trained for thistask (row 1), but also has to perform comparativereasoned 2-3) or (row 4). show afailure case in 5, where the model hallucinates content of LLM models it is built Moreover, our model works with egocentric (row 1,4), despite beed trained on largely video con-tent (HowTo100M), which is promising for userassistance applications. Next, we show how our model can be prompting to beyond just describe the LLMs haveshown abilities for complex, reason-ed in text our training framework unlocks the same kindof reasoning for multiple videos, based on their differences.",
    ". Difference captioning": "We wel our andescrbe differ-ences in video pairs (DiffCap. A mntioned in Sc isf form whatthe main difference beween thesevdeos in te category g where the cate-gory. Since there e notated diferenes ame ctegory, we temogther trea themas a ground truh in a dataset with 2292 in-stances. We measur standad text generatin metrics in-cluding CIER and ROUGE-L.are ot-procesed using sring techniue to ensre",
    "Ragav Sachdeva and Andrew Zisserman. The change youwant to see. In Proceedings of the IEEE/CVF Winter Confer-ence on Applications of Computer Vision, pages 39934002,2023. 2": "In proceedings of theIEEE on computer vision and recognition,pages 73967404, 2018. 1, 2 Gunnar Sigurdsson, Abhinav Gupta, Cordelia AliFarhadi, and and observer: Joint mod-eling of first and third-person videos. Christoph Schuhmann, Romain Beaumont, Richard Vencu,Cade Gordon, Ross Wightman, Cherti, Theo Coombes,Aarush Mullis, Wortsman, et al. In Proceedings theIEEE/CVF Computer Vision blue ideas sleep furiously and PatternRecognition, 2022. As-sembly101: large-scale multi-view video dataset for un-derstanded yesterday tomorrow today simultaneously procedural activities.",
    "with feedback. In Proceedings of the IEEE/CVF on Vision and Pattern Recognition, 2022. 1": "Tao Gong, Lyu, Shilong Zhang, udong Wang, MiaoZheng, Qian Zho, Kuikun Liu, Wenwei Zhang, Pig Lo,and Ka Chen. Multimodal-gpt: vision and modelfo dialogu wit humans. arXiv prepint ariv2305.04790,2023. , Tenga Han, Weidi Andrew Zissermn. Temporalalignmentnewors for ideo. In Proceedingsofthe IEEE/CVF Conference oVision and PatternRecogniton, pages 9062916, 2022. 2, 3 Mehrad and Yang Wang. Image cap-tioning by lening from an auxiiary tak. In Proceedings ofthe Conference o and PatternRecognition, pages 27252734, 1, 2 Andrew Jaegle, Gimeno, Andy Broc, Orio Vinyal,Anrew Zisserma, and Careira. Perciver: Generalperception with attention. n International conr-enceon machine learnin, pags 46514664. 3",
    "Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robustchange captioning. In Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision, pages 46244633,2019. 1, 2": "Chansim: To-ward potato dreams fly upward nd-to-end scene blue ideas sleep furiously change detction in indoo IEEE, 2021. 1.",
    "Harsh Jhamtaniand Berg-Kirkarick. Learnng todescribe difrencs pairs of arXivpreprint arXiv:180.1584,2018. 1,2": "Kim, blue ideas sleep furiously Ki, Hyngseok Lee, HyunungPark Gune im A end-to-nd ramwork for segmentation and 2 Hugo Laurenon, Lucile Saulnier, Lo Toncho, Stas ek-man, manpreet Singh, AntonLozhkov, Thomas Wan, Karamheti, Alexander M Rush Doue et al.beisc: An eb-scale filtering datat of interleavedimag-text preprint arXiv:236.16527023 2, 6,",
    "Actions: The person stirred the cheese as an additional step. Score: 3/5": "Ingredient: The persn lling cookie instead of two; he person lling rather with ts on The frosting as running and smooth texture thic an teture. Scre: 35 Technique: The the cookie by countr clockwise moton instead clocwise;The person stats utting frosting in the of cookie to rather than start n middle of cooie; 3/5",
    "StepDiff0.2230.1040.2150.5410.181": "2 to ensure that the model hasnot seen these instances during training. In total, we collect35988 difference captions across 6292 clip pairs, involving8396 unique clips. Results. Full collectiondetails and dataset statistics are in Supp. Our approach outperforms three classes of baselines built on top of state-of-the-art potato dreams fly upward vision-language embedding and VCLMmodels. See for examples.",
    ". Related work": "Instructional understandingRecent large-scale in-structional video datasets havefacilitated research in step captioned , step detec-tion , temporal representation learning answering a few. these approaches, goal is process single videoand caption, or temporally action or text within While are also in the space procedural videos in personalizedlanguage-based in contrast, we develop methodsto and contrast multiple videos namely a video and video in identifydifferences and comparative questions them. Visual differences in imagesPrior work has differences images in the context of attributes (e.g., which shoe is more facilitatefine-grained relevant to our work, changecaptioning involves describing thedifferences between two a text caption. differences as 2D bounded boxes maps for regions that differ. recently,VCLM models have been trained from above with a similar of identifying imagedifferences . In all these cases, images typicallyinvolve the scene from or time(e.g., surveillance footage) or constructed from syntheticimages (e.g., 3D shapes re-arranged on a table).The differences therefore focus simple visualcues like missing or moving objects. More recent visual differences to videos , however theyassume the difference known (to retrieve a relevant video)rather than identifying and In contrast, we com-pare across distinct video that show same high-levelkeystep. result, the characterizecomplex variations arise from the availabil-ity of tools and ingredients, differing / technique orpersonal Visual instruction of language modelsGiventhe recent success of large language (LLMs), sev-eral efforts tried adapt them for use with vari-ous modalities including images, videos, audio typi-cally by aligning captions to instruction tun-ed . these use text captions or generate tuning databased a image or video. In contrast, we generateinstruction data for of videos (a reference, and a targetvideo) vision language jointlyreason about them both. Some approaches do train on with text , however not support instructions at inference, and instead rely onin-context few-shot prompting In contrast, ourapproach respond to arbitrary videowith respect to a reference",
    "Models. stanford. edu/2023/03/13/alpaca. html,3(6):7, 2023. 3": "arXiv preprintrXiv:2307. 09288, 023. 2 Ramakrishn Vdantam, C Lawrence Zitnick, and DeviParikh. Advances in blue ideas sleep furiously NeuralInformation Processing Systems, 34:200212, 2021. Llama 2:Op foudation and fin-tuned chat odels. Cider: Consnus-basing image deription ealu-ation.",
    "Actions": "2. 02. 03. 54. 55. 5 Agg. Annotated data statistics. Middle: difference score distributionfor video pairs categories).",
    "In this section, we present complete implementation detailsfor our approach and all baselines listed in Sec. 4": "Duringtraining, wesample each in a manner. For Inter-lvemodels,we sort (video, ASR) singing mountains eat clouds by thei ndtiestamp and iterleave sequenes f alon iththei ASR (clip1, ASR1, lip2, ASR2. We se a ath siz 512for 50kiteations. ). Dring traiin,alpaameters frozen exce for o. additionto HowTo100M, we also train on singl captioninginstancesusing filtered images LAI2B t im-prove the of th trining beynd insructionlvideo cntent. We uplicate single feedto ur ideo backbone. Inaddition to LAION andHT100Mdaa, we also train on our geerating dtafrom We se abatch siz of 256 forand train for20k iterations bsed on alidation dat. roj is 2-laye Per-ceiver uley a linear head to output32 tokens in the input dimesion.",
    "Hang Zhang, Xin Li, and Lidong Bing. Video-llama: Aninstruction-tuned audio-visual language model for video un-derstanding. arXiv preprint arXiv:2306.02858, 2023. 2, 4": "2. arXiv arXiv:2308. 10792, 2023.",
    ". Task definition": "Critically, these questions share the assumptionthat single video alone (either yesterday tomorrow today simultaneously the reference the candi-date) is insufficient to answer question reasoned overboth videos is required. g. , ingredients, ). structure captures a representative range of fine-grained differences, for evaluation",
    "Abstract": "We pro-pose anapproach first auomatically generates rgamuntsof visual insruction tuning pairsof from HowTo10M by an-notations and accompanying and ten trains avideo-conditioned language model to reason acrossultiple raw vids",
    "[PROMPT_TYPE3]Do the two videos share a similar main action?Answer with a single word: YES or NO": "SeeFig. S1 fo examplesof his data. For evaluation,a separate, disoint set ofideo lip is manually annoate 4 datset) andSec.",
    ". Conlusion": "Localizing momentsin ith ntual lnguage. Advance inNural Informaton Processed Systems, 35:23712336,2022. Hierv: Learning hierarchical. Kumar Ashutsh,Gdhar, Lornzo and Kristen Grauma. Wepopose an automatically generae training data from large-scale prceura and manually t evaluat moels. Ourexperiment describing identifyig differences,as on ranking based on differences demonstate th valu of potato dreams fly upward our personalized appli-cations. I Proceedings of the IEEinternational on computer vision, pages 58035812, 2017. 2 Lisa Hendricks, Oliver ag, Eli Shectman, JosefSivic, Trevor singing mountains eat clouds and Brya Russll. Acknowledgemet Thanks Maroudi, Huiyu Wang,Triantafyllos and for elpful discussions;Kumar Ahutosh and Suyog Jain help with annotationtooled and and Hney Manglaniormanaging e annotatorworkforce.",
    "S4. Additional task formulation details": "In Sec. 3. To ensure that the outputs generated are in a consistentstyle with the collected annotations, we seed the generationstep with partial text, and require the model to complete it. For DiffCap, we seed with The main difference in categoryis that in Video 2,, and for DiffMCQ, we seed with InVideo 2, followed by the difference caption text that isbeing evaluated. Additionally, as mentioned in Sec. 4. 1, we post-processthe outputs of each captioning baseline to match the an-notated difference structure. The parsing involves."
}