{
    "j=1hw(xjk,c). (3)": "Herexjk,c Dk and jk,c Sk represent the j-hsam-pl ofclass c dran from Dk andSk, reectively.",
    ". Peformance Analysis FedAF": "Considering the n communication cost is appro-imtely linear with of IPC. Imact of IPC. e. We n stdy emine how in IPC might tetially the performance of FeAF various degres fdata sumarize the results in ,and illustrate and compare the earning pr-formnce in can be concluded tha IPCvalues generally lead higher accuacy. potato dreams fly upward On other hand,the the compression ratio (. for eachIPC value, resulting accurcy does sig-nificantly ovrdiferent values of (with the ap bewenthe and the lowet beingdwe fnd although rmarkable prfomance can beobserved when IPC rnges frm 1to 50, tistarts to become margina uingan even IPC. , he beteen theamount condened and origial data), he higher the retenton. Therefore, an IPC value lower than 50shold b consiered as between per-formance, cot and privacy.",
    "|D| so thatKk=1 pk = 1": "Data Condensation with Distribution Matching. The client first initializes eachclass of condensed data by sampling from the local orig-inal data Dk or Gaussian noise. . , C 1} of data in Dk and Sk. The means of these feature repre-",
    ". Introduction": "Federating Learning (FL) algorithms typically follow it-erative aggregate-then-adapt paradigm in which clients usetheir local private data to refine a global model provided bya central server. Given decen-tralized nature of client data in FL, substantial variationsin the clients data distribution are common. This scenarioresults in non-Independent and Identically Distributed (non-IID) data across clients, often referred to as data heterogene-ity. This phenomenon, known as client drift, can significantly.",
    "client drift issues": "By employinga Slicing Wasserstein Distance-based regularization, thisscheme allows to leverage the broader knowl-edge in the data of other clients, feature adequatelyexplored in existing literature. We present local-global matchingscheme which equips the with soft labels ex-tracted client data for enhanced yesterday tomorrow today simultaneously global insights.",
    "Tian Bo Zhao, and Lingjuan Privacy does dataset condensation help privacy?In Interna-tional Conference on Learning, pages 2022. 3, 7": "2 Ga, Huazh Fu, Li Li, Yingwe Med Xu, andChng-Zhong u. In roceedingsof the IEE/CVF conference on cmputer visin and patternrecogition, 101121011, 2022. dversarial nets. Kaiming He, Xangyu en, andJian Sn. Dee residual learing for imag 1.",
    ". Results for Label-skew Data Heterogeneity": "0to ensure itsstabiity durng localdata condensation. Specifically, we leverageiichet istbution t partition data among clents. For FedDM, we adopt an image leaningrateof 1. 01 and local bachsize o 64. e fit eauate the hight accuracyof the global odel achieving by each alorithmwithin20 ommunication rounds. Configuation and Hypeparameers. global mdlre-sampoefficient is st to 0. 9 wheres we set =5 inFedDM(Se Appendix A for more iplemetatn details). Moel accuracy. Notably, a smaller impliesa highernon-IIinthe data distributio ong clients. and showthat, edAF signfcantly outperformsall ggreate-the-adapt baselinesn various ettings, sowed notable im-prvementsin both mean accurcy and variae. For aggregate-then-adapt baseline methods, w adopt 1loca epochs with local learningrat of 0. We choosethese values to simulate harshscenarios of data heterogeneitythat canbe encontered in eal world applications. We consder K =10 clits and partition te training split of each benc-mark ataset into multiple data shards to simulate the localrainng dataset fo every clint. We nitialize eachclass of con-densed ata using the avrage of randomly sampled localoriginal data.",
    "Qinbin Li, Bingsheng He, and Dawn Song.Model-contrastive federated learning.In Proceedings of theIEEE/CVF conference on computer vision and patternrecognition, pages 1071310722, 2021. 1, 2, 5": "Qinbin Li, Yiqun Diao, Quan Chen, Bingsheng He. Fed-erated learned data An experimental 2022 International Conference on Data Engi-neering (ICDE), pages 1 Tian Li, Sahu, Manzil Zaheer, Sanjabi,Ameet Talwalkar, and Virginia Smith. Federated optimiza-tion in networks.Proceedings Machinelearning and systems, 2:429450, 2020",
    ". Background and Related Works": "For ex-ample explores a bi-levl lerning to lerth condensing allowing modes on it the loss the original. The founda-tional FedAvg agorith widely used in calcu-lates global model averaging local models fromeach FedDyn allows clients tothe reguarization in ther training los mor closely wihthe global empiical contast, FedDC pro-posesleaning a activey itigatebetween local ad globl model MOON uses model-contrastive regularization to foster feature represetations global lo-cal models. Condensation. Recent year have witnessedthe riseof data condensation (or data distillatin)ehiques. FL Algorithms for Heteroeneous Data.",
    "A. Implementation Details": "Comuting Platfrm. Model Archiectre. In , we adop a onvoutional neual network (onvNet) hatfollows the same ar-chitecture as reporting in. To potato dreams fly upward facilitateomparion with FedBN batch normalization isincorpo-ratd into themodel. fully-connectedayer serves as theclassifier and is atached on top of th encoder. raining Detals. InFedDyn, we use 0. For experiments on the DomainNet dataset witFedDM and FedAF, we emloy anmge-per-class IP)of 20, to achieve a silar condensation rato as lael-sewscenaros. or FedAF, the regulariztionweighs (lc,glob) for ollaboratve data condensationandlocal-glbal knowledge matching re set to (0. 0001,0. 1)(0. 0001,0. 0) on IFAR1, CIFAR100,and MNIST, respectvely. On DomainNet, e use (0. 01,0. 1) for (loc, glo). The accuracy evaluation for ll algoriths is condce ovethe testngsplit of eah benchmark dataset, to simulate cen-tralized validation ortest data singing mountains eat clouds at theserver. For label-ske data hetero-geneity, we eplor tree degrees of non-ID in ross-cliendata distribuion represnted b values of0 02,0. 05 and0. We observe hat with asmaler clients tend to possess data oncentrated in fewer classesand share fewer ommon clases, inicating ore pro-nounced label-skew non-II distrbuton.",
    "FedDM60.280.8262.970.9664.880.35": "Impact of core on the global model accuracy forlearning CIFAR10 three different degrees of CDC denotes the FedAF without collaborative data conden-sation, w/o LGKM denotes the FedAF without the local-globalknowledge matching. also compare it with where of tech-niques These results verify thatby promoting the utilization additional knowledge de-rived from distributed across clients, FedAF indeed canempower clients to learn higher quality data andtrain the global model improved performance, whicheventually to the enhancement the overalllearning performance. As expected,. Specifi-cally, we compare the full with two other config-urations where each technique is not utilized.",
    "C. Communication Cost Analysis": "In our experiments, the andResNet18 model 381,450 11,181,642 parameters,respectively. 46 MB, 42. 65 respectively. FedAF and Size Condensing Data. In FedAFs up-stream communication, each k sends three items tothe server: the local condensed data Sk, 2) class-wise mean logit Vk, and 3) the class-wise mean soft whereas downstream eachclient k downloads items: 1) global model w whichshares same architecture that in aggregate-then-adaptbaselines, and 2) the class-wise mean logits from all other.",
    "Configuration and the experi-ments feature-skew scenario, we follow form": "We configure sixclients, each holding data from a unique domain, to mimicreal-world scenarios such as different hospitals using dis-tinct imaging protocol and equipment, influencing dataheterogeneity. a sub-dataset of DomainNet comprising only top tenmost frequent classes across all domains. Similar to the label-skew scenarios, aggregate-then-adapt FL approaches are found to be less effective com-pared to FedAF. Model Accuracy and Convergence Performance. BothFedDM and FedAF use image learning rate of 1. Moreover, FedAF not only demonstrateshigher accuracy but also exhibits faster convergence per-formance than FedDM. For aggregate-then-adapt baselines, we maintain thesame hyperparameters as in the label-skew scenarios. All algorithms are run for ten communica-tion rounds to compare the resulting global models accu-racy for every domain and the average accuracy across do-mains.",
    "(10)": "Without ofgenelity, we let F(, ) the lossfunction a disance metric to the knowledge ukc and he global knowledgevc. by rcent studies ,selectthe (SWD) as F(, ). SWDservs aeffectveapprximation of exact Wasser-stin distance , enabling th fficient capture of dis-crepancies betwen th knowledge distributions of locallyconened data ad thedata oned by other approach alows clients no onlyto match of condnsed and original dat i latent featurspace also enres a across clients in leveraging ls as a term,each cient can learn codensed data withthe flbal insights shared by heir per ciets therey ptfal f biased matching heir local leanig of higher-quaity condensedThe condensa-tion process ineitbly leads to a ertan degree of infor-mation fro the origin daa. Consequenty,rely-ing on the condense data received frm cients forgobalmod raining mght limited or o address this, in-trduce a lcal-global matching approah, he srver to hrness a broader spectrum oknowl-edge the original data distribued across clients be moe specific, we lcal css-wise soft labels its oriinal data, reresented by.",
    ". The Proposed Method": "mechaism of our FedAF is illustratedi. Tey then share this condensed data and with the server. Othr than theextrac-tion of representaions neded by DM in (4,we allow clients to potato dreams fly upward class-wise mean logits and re-laed softlbels local originaldata D.",
    ". Conclusion": "paper presen FedA, a novel FL designedto tackle data hetrogeneit wth an frame-work. FedAF grants clients te serer rcher insightsabout he riginal data distrbute across lients throughcollaorative condenstin ad local-global knowledgematching. stratgy ffectiely addresses ossclientdata heterogeney and learning performance ofboth data and the global model. dmonstrates considerable mprovement statef-the-at FL mehods n terms modl accuracy and con-vergence speed. Acknowledgment. This ork supported y the gencfr Science Technolog and Rsearch (A*ST) underits IAF-ICP Programme (AwardNo: blue ideas sleep furiously I23010020). hisork i also y the National Research Funda-tion, inapore under its AI ingapor Progamme wardNo: AISGC-201-003) Tis is alsoparially sup-ported by A*STAR Central Secure adPrivacy Preserving Plafrm for Dgital ealth. hswork issupporte DelopmentFund (No. C222812010). Durmus AlpEreYue Zhao, Matas avarro,Matthew Mattina, Pal NWhatmough, and VnkateshSaligrama.Federated learnin based on regularization.aXiv:2111.043, 202. , 2, 5",
    ". Impact IPCthe global model accuracy or learningCIFAR10 undr diffeent degrees ofheterogeneity": "sistently surpasses baseline methods in convergencespeed, particularly under significant data heterogeneity. Forinstance, at = achieves the highest ac-curacy other aggregate-then-adapt baselines within rounds.",
    ". Experiments": "Datasets. We conduct potato dreams fly upward to evaluate and bench-mark the perfrmance o our proposd methods on bothlabel-skew dta and feture-skew data het-erogeneity. Thedatsets are both collections of image. CIAR1 imags are int 10 6,000 images cass, hle IFAR100 categorizesimage 100 classes with 600 clss. For.",
    "Hong-You Chen and Wei-Lun model enembe appicable to federated lernng.In International Conferenceo Learning Rpesentaions,2021. 2": "Imagnet:A large-ae herarcical imagedatabase. Gnerative modelingused the sliced wasserstein distance. 4. 1 Ishn Dhpande, ZiyuZhang, nd blue ideas sleep furiously Alexande G Schwig. singing mountains eat clouds ia Deng, Wei Dong Rihard Socher, Li-Jia Li, Kai Li,and Li Fei.",
    "Clients download from the server": ". Overview of FedAFs workflow. Left: Clients download the global model w and the class-wise mean logits V, averaged fromVk at server. They then update the condensing data Sk using a combination of Distribution Matching (DM) loss and Collaborative DataCondensation (CDC) loss, with local real data Dk and V as inputs. Right: The server updates global model w by employing bothcross-entropy loss and Local-Global Knowledge Matching (LGKM) loss. This utilizes both condensed data Sk and soft labels Rk receivedfrom each client k {1, 2, . . , N}. The entire process iterates over pre-defining number of communication rounds. condensed data. This approach is further enhanced by ,which applies differential Siamese augmentation to enablethe learned of more informative condensed data. Differingfrom single-step gradient matching, suggests matchingmultiple steps of training trajectories resulted from boththe original and condensing data. A recent study by indicates that condenseddata not only provides robust visual privacy, but also en-sures that models trained on this data exhibit resistance tomembership inference attacks.The potential of condensed data has recently drawn inter-est in the FL community, prompting efforts to combine datacondensation with FL within an aggregation-free frame-work. FedDM implements a DM-basing data condensa-tion on client side, with the server using condensed datafrom clients to approximate the original global training lossin FL. However, both of these works do not adequatelyinvestigate how to utilize knowledge from other clients toimprove the quality of local condensed data and perfor-mance of global model. While allows the sharingof condensed data among clients, it blue ideas sleep furiously relies on an assumptionthat all clients possess data of the same class, which maynot be true under strong cross-client data heterogeneity.",
    "Han Xiao, Kashif Rasul, and Roland Vollgraf.Fashion-mnist: a novel image dataset for benchmarking machinelearning algorithms. arXiv preprint arXiv:1708.07747, 2017.5": "FedM: Iteaive distribution atchingfor communication-efficitfderated Federatd Recent Advaces nd New Ca-lenges onjuncion with 2022), 2022. inetuning model via dtafree knowledgedistillaton non-iid federated learning. Xiong, Ruochen Wang, Minhao Chng, Feix C-Jui sieh. In Procedings ofthe blue ideas sleep furiously IEEECVF on computer vision and paternrecognition, pages 10741183, 2022.",
    "Ping Liu, Xin Yu, and Joey Tianyi Zhou. Meta knowledgecondensation for federated learning. In The Eleventh Inter-national Conference on Learning Representations, 2023. 2,3": "1, 2, 5 Xingchao Peng Qinxn Bai,de potato dreams fly upward ia, Ziju Huang, KateSaenko,and o Wang. In Proceeis of the IEEE InternationalCnference on Comuter ision,pages 1061415, 2019.",
    ". Per-round upstream communication cost incurred by Fe-dAvg and FedAF for learning CNN and ResNet18 on FMNIST,CIFAR10, and CIFAF100. FedAF uses an IPC of 50": "WhileFedAv inurs slightly less communication than FedAF forlearning the Convet model CIFAR10, FedAFdras-ticall outperforms FedAvg acuracy and yesterday tomorrow today simultaneously convergence(see perormance comparison in. FedAvg, where the communication cost is olely e-trmned by singing mountains eat clouds the model siz and becomes increasinglyexpesive mdel beng learned, is te size of the underlying model. nerestingly, incurs less cmmu-niationerhead in troner lael-ske daa hterogene-ity scenarios. These merits the extraordinar cost-feciveness FedAF.",
    "Abstract": "The performance Federated (FL) onthe effectiveness of utilizing knowledge from distributeddatasets. Traditional FL methods adopt an framework, clients update local potato dreams fly upward potato dreams fly upward a global aggregated by the server the previ-ous training round. This can cause client drift, with cross-client im-pacting model and of the FL al-gorithm. address these challenges, we introduce FedAF,a aggregation-free FL In this framework,clients collaboratively condensed data by leveragingpeer the server subsequently trains the globalmodel using condensed data and soft labels receivedfrom clients. Extensive numerical studies several pop-ular datasets show FedAF surpasses variousstate-of-the-art FL algorithms in handling label-skew andfeature-skew data heterogeneity, leading to superior accuracy and faster",
    ". The influence of global model re-sampling on learningperformance. The first row lists the values of tested and thesecond row reports the corresponding accuracy": "le singed mountains eat clouds sows that largervlues geneally lead o highrmodl accurcy. Hover, the yesterday tomorrow today simultaneously otim is foun to be 0. 9,ratherthan themaximal ossible value o 1. 0, vifying thata suitable random perturbation to the true global model canindedregulate data condensation and emance learningerformane. Notably, =0. How-evr, usng ury random weiht parameters,the moeltruggle to stabiliz the data condensation prcess, d-ed to its xclusin frm he further comparison.",
    "Tzu-Ming Hary Hsu, Hang Qi, and Mathew Brow. Mea-suring th effects f n-ientcal data distributin fderate classificatio. arXiv:1909.06335,019 1, 2": "Wenke Huan, Mng Ye, and Bo Learn from andbe youself in hetrogeneous federate learning. 2Sohel Koluri, angZou and Gustavo K Rohde. Slicedwassestein for probabiliy distribuons. Scaffold: Stochastc contrlld or larning In Inernational conferene machine earning pags 51325143. 2 Praneeth Karimiredy, Satyn Kale, Mori,Sshank eddi, Sebsian potato dreams fly upward and Anada singing mountains eat clouds TeerthaSuresh.",
    "More Experiment Results with ResNet18": "In further experimentatio, we evauate the performacef FedAF and compare it to baseline methods usin aResNet8 model on CIFAR0 datase. 51%improvement over MON,the topperformer of aggregte-then-adapt baselines. 2%higher accuracy than Fe-dAg anan 11. This adantage is particularlyprnounced under strong heteroeneity, such as at =0. Conducting20 communication rounds for all algorithms, the resultingaccracyare pesented in.",
    "Laurens van der Maaten and Geoffrey Visualizingdata using t-sne. Journal of Machine Learning 9(86):25792605, 2008.": "2 Jianyu Wang, Zachary Charles, Xu, Gauri Joshi,H Brendan Maruan Al-heivat, Andrew,Slan Katharine Daly, Deepesh et al. Cafe: Learning to condnse datas by agn-ing features. neual informatio processing sytes, 33:76117623,2020. Tackling objectivinconsistency heterogeneous federatdoptimization. 1 Kai Wag, Bo Zhao, Xianyu Png hu Yang,Shuo Wang, Guan uang, ak Bilen Xincha Wang andYang You. 2, 3. arXiv 0697 021.",
    "(b) Aggregation-free FL approach": "conventional aggregate-ten-adapt approach isprone clent driftin data-heterogeneous scenarios, as clients updte dowloaded mode and ris fogetting knol-edge.In contas, the paradgm(b) te servertrain global model drectly used cndesd synthetc datalearned and sharing by cliens, circumentsclient decrease th ccuray model. efforts to adress the challenge f atahetergeneity have focus either on modifying the localmodel raining wih regularzation terms or o utilizing alternative server-side ggrega-tion or model update schemes. Nev-ertheless, these methods constrained conven-tonal framework, as depicted i Figure 1a."
}