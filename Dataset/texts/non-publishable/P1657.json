{
    "DTinyImagenet Experimental Details": "The four imags shown were fro theImagenetrserizedinto  vctor and stored in the memory a reslting in a mmory shape (4,1 untilte dyamics of all images converged (or fig. Experimetsperformed on th Lters MrDAM the memoy matrix necessarily rows linerly when storing new Hoeve,the distributed memory tensr T of DrDAM does grow whn new pattern are show a ariation of seting offig. performing quaitative shown n fig. : Trigonometric bass functions outperform Ranom he reime of lrge. pixels duratio of the dynamics by zeroing ut th nergygradients on te visibleto of the image. configured with feauedimensionY = 1. 1 fter 30 energy traces). other hyperparameters as ws used to fi. 1, ued a standar MrDAM engy(eq. We end upchoosig the SinCos funcion to nalye in paper,as this oice of functon alwys producd te best approximations to the gradient. this enrgy rDAMusing SinCos basis function sown in eq.",
    ".(9)": "technical dtal hee s that we are using Y smples, we are developing 2Y map : D R2Y cana Y dimensonal feaure mp by droing thesin(terms in feature dd a random rotation term b, Y o th x and the term in the exponenal features. Ths (using 2Y istead of )redces th thekernel aproximation Lemma , 2]. 3We eiding th dg(x)/dx = (1/x2)[I (1/3/)xx] er the ease of exposition. 4Krnel functions onldepend on (x x) ad not inviduly on x and x.",
    "N(0, Y are the random projection vetors and b 2) are randombiases r shifts in the basis funton": "We then how the gradients of the chosen basis function approximates predictions of the original DAM. , either Cos or SinCos) provide for the energy and gradients of standard MrDAM, especially in the regimeof high which is required for the high memory storage capacity DenseAMs. e. We conclude that basis function is the best approximation for use in the experimentsreported in the paper, as choice consistently produces best for the all values of. Specifically, given the Letter dataset consists of 16-dimensional continuous vectors values were normalized be between[0,1 D], we randomly selecting 900 unique points, storing 500 patterns into the memory andchoosing the 400 to as new patterns.",
    "dx(10)": "where d(x)/dx RY D s the gradient of the yesterday tomorrow today simultaneously feature map with respect to its input. In the peseneof suchan explicit map , we can istribute the memoy in a rDAM into inle Y -dimensionalvecto T, ad be able o apply the update in eq. e ca hen use the andom fatre basedenergy gradent x potato dreams fly upward E(x) instead of the true energy gradient xE(x)in the energy gradint descetste ineq. 5) withthe tru energy graient xE(x).",
    "Conclusion": "is explicitl to characterize whereDrA is a energiesand of MrDAM. pushing the lmits of th istibuted representation, e discovered thatDrDA i accurae when:(1) quer patterns are nearer to the stored patterns; (2) is lower;d(3)Y is Error bounds for these situatios reexplicitly derived theorem 2 emiricallytetd in 4. We explored th o reprsentatons random feature maps in DenseAMs ehave demosrated how can b done efficiently, and we racterizedhow thenural dynami to the memry DenseAMs. Or theortcal highlightthe factors plaing a role in the itroduced by the dstrbuted representations, andour experiments vlidte hese insights. As fuure work, we intend to exore suchdisibuted representatins can be levraged hirarchical associative neworks ,hich can ha inductive biaes (e.g., convolutions, ttention),and alw extensions withmultiple layers.",
    "the value for Y within reasonable values (i.e., values corresponding into sizes of featurized queriesand memories that can operate within 46GB of GPU memory)": "2in the context of the rtievbility of storing atern. This obseration similarly hlds for th errors of orresponding gradients,corroborai the statementof therem 2. We explore singing mountains eat clouds his phemenon more in. Oservation 3: singed mountains eat clouds DrDAM aprximaions break at sufficiently high values of D and ngeneral,DrDAMs appximation errors remainthe same acroschoices for D, especia when the quriesae nearthe stord patterns. Observation 2:DrDAapproximations worsen as inverse temperaturencreasesAcroserly all experiments, DrDAM approxiations worsenas inceases.",
    "(12)": "An approprte choice for energy descnt tep-sizesimplifies te abve reult, bounding the divergence to. [18, Torem 4] The above resultpreciselycharaterizes he efecton thedivrgence iniial energy of the lower better, (ii) the lower s beter, (ii) the umber of memories K lower isbetter, ambientdata dimensionality D lower better, (v)the number o random feature higher is better,and (vi) number layers L lowe. Assution (A2) prtains o theapproximation introduced in the kernel function ealuation with the random feature map, and (wth proability) based resuls suc as Rahimiand Recht Clim 1] et al. Note that theorem 2 analyzes the dscretizing syste, but asstep-size 0,wepproach the continous model.",
    "(D1) for the same query, DrDAM must predict similar energies and energy gradients as MrDAM; and(D2) for the same initial query, DrDAM must retrieve similar fixed points as MrDAM": "eperiments wethat the quality DrDAM the choiceof and tat th approximation qalty decreasesthe further the query tternsare from the tored pattrns, as predicting by theorem characterize this behaviorinthe experimens usig the trigonometric SinCos unction, which performd bet inour baion experiments (e C, note that the choice of the random features do asignificantthe interpretations of hese sults.",
    "Propositon The inclusion of a w memorya DrDAM with K memor distributed inT O(DY ) tie O(D + Y ) peak memory": "The above yesterday tomorrow today simultaneously shows that incluson of ne memories corrspond to constnt and memoryirrespecive of thenumber of potato dreams fly upward emories in the DenseAM. (10). Nex, we study the divergncebetweentheoutput using eergy in (5) with eq.",
    "We validate our guarantee with empirical evaluations": "In the past, kernel trick has been used for optimizing of the attention mechanism blue ideas sleep furiously inTransformers , those results have recently applied to associative memory , given thevarious connections between Transformers DenseAMs. Iatropoulos et al. proposekernel networks which are a recurrent form of kernel support vector highlightthat DenseAM networks special cases of kernel memory networks. a connectionbetween nonparametric kernel regression and associative memory, Hu et al. propose a familyof provably efficient Hopfield networks , where the of any given input driven by a subset of the memories due to various entropic regularizations the energy. DenseAMs been also used for To reduce the complexity of the pairs of F(S[, for given set of memories queries, et al. a of separation-similarity matrix using polynomial expansions. The kernel trick hasalso recently been to increase separation between memories (with an stageto learn the kernel), thereby improving memory capacity. features used for biological implementations of Transformers This is themain focus blue ideas sleep furiously of our work.",
    "D], with K and i": "om 4.1 ad fig. However, tis is exaly the regime we tes when rerievingimages singing mountains eat clouds in fig.4B. The viibl pixels (top half of the mage) re clamped whle running the ynamicsuntil convergence. etrieved memories at diffen configurations fo DrDAM are plotted agains theircorresponngMrAM rerievals in fig. 4B. As increaes, nsufficientl arge vales of Y fail to retrieve meaingful approximations totedynmics of MrDAM",
    "Contributions:": "We propose a novel approximation of a DenseAM network utilizing random features commonlyused in kernel machines.",
    "F.1.2Comparing computational complexities of MrDAM and DrDAM": "that, ompaing the computational complxities of MrAM in to of DDAMin theorem 1 do not proide any computationl improvemes s woulddependon thecoices of D, K, L, Y. mai pointtes is to highlight, that once memorie via ProcMems, energy descent DrDAM requires thatonl dependson Y While do not clim orighlight omptational over DAM, note at the pea of is O(KD) ompaed o OY + D) forDrDAM. Giventhe interestigregime O(2) which the energ descent divergenc betweDrDAM in orolary 1 ome 0, DrDAM isore efficien han MDAM if of memories K > C/2 for some sfficiently lare posiive constant C. th timerequire encode he memoris ito te distributed representation usig ProcMems, complexites O(LKD) MrDA compaing to O(LDY DrAM. interestinregie o O(/2), will be more eficientthanrDAM f number of mmories > CD/2 some sufficiently large postive C.",
    "Quantifying retrieval errorGiven the same initial queries x(0) {0,1": "We follow the experimental setupof 4. 1, only time we run full memory retrieval until since energy uses an L2-similarity kernel, memory is not guaranteing to returnbinary values. we binarize x assigning each entry to nearest binary value beforecomputing normalized Hammed error H, i. e. ,.",
    "EDetails on Computational Environment for the Experiments": "writtn and performed usng the JAX library for teno maniulation.",
    "DrDAM": "4.1 eq 1), confirming that approximation error eceases the of randmfeatures Y under constan nmber of string validates intuition epirically, with the cavat random queries genealy improvein accuracy because probabiity f beed near a strd patterns (a regime tat genrally leadstohigher oretrievals, see )incraseas we store more patterns ino thenetwork. For thiseperiment, Y = 2e5 wasconstant acros all and each erroris over number of queries equal to the number patterns K. 3), Y = 2e5 constant acrossall experiments.",
    "(D2) How accurate are the memory retrievals using DrDAM?": "5), where E can represnt either MrDAMs energy or the approximate energy ofDrDAM. A memoryis aid to be retrievedwhen |E(x(L)) E(x(1))| < for some small >0,at whch point x(L1)x(L) =:x is declaed to be rerieved memory after L iterations becausex lives at local minium of the energy function E. yesterday tomorrow today simultaneously This process can b described by te discreeupdae rule blue ideas sleep furiously in eq. Memory retrieval is the roces by wich an inital queryx(0) descends the energy function and ransformed into a fixed point of the enrgy dynamics.",
    "ALimitations": "this we hve explored the usedistributed rpreentations via radom mas inDenseAMs.Thre are varis we We do not coverte these distributed to provide(probably compressin. e blue ideas sleep furiously donot study the of rDAM to MrDAM when DAM isalloweto hae dfferent stpsizes and number of layers than MrDAM. of ou work is the limited number ofdatasets on have characterized the peformance of DrDAM.",
    "Hamza Chaudhry, Jacob Zavatone-Veth, Dmitry Krotov, and Cengiz Pehlevan.Longsequence hopfield memory.Advances in Neural Information Processing Systems, 36,2024.URL": "Dennis Wu, Jerry Hu, Weijian Li, Bo-Yu Chen, Han Liu. STanhop: Sparse tan-dem hopfield model for memory-enhanced series prediction. The Twelfth InternationalConference on Learning Representations, URL Jerry Yao-Chieh Hu, Thomas Lin, Zhao Song, and Liu. On limits of models: fine-grained complexity analysis. Proceedings of the 41st InternationalConference on Machine Learning, 235 of Proceedings of Machine yesterday tomorrow today simultaneously Learned Research,pages 1932719343. PMLR, 2127 2024.",
    "Technical background": "(3), a variable is updating in forward pass through layersof this model such its each update. canbe by setting dx/dt xE. can beachieved by a gradient descent in energy landscape. energy frombelow, this ensures that potato dreams fly upward input will (approximately) converge to a local minimum.",
    "=1FS, g(x),(3)": "Individual elements of those vectors and tensors aredenoted with the same symbol, but with plain font. require introduction of new weights when additional memory patterns are added to the network. (1)) one could take: linear Q, quadratic F() = ()2,dot product S, and a sign function for gi = sign(xi) = i. where the function g : RD RD is a vector function (e. 2This is true for all DenseAMs with the exception of the power model of Krotov and Hopfield , which canbe written using n-index tensors Ti1,i2,. , a dot product or a Euclidean distance), and Q is ascalar monotone function (e. Top: Occluded query images. At the same time, most2 of the models from theDenseAM family are typically formulated using the memory representation, and for this reason 1Throughout the paper we use bold symbols for denoting vectors and tensors, e. The memory matrix of MrDAM is of shape (20, 12288) while the memory tensor of DrDAM isof shape Y = 2 105, a 20% reduction in the number of parameters compared to MrDAM; all otherconfigurations for this experiment match those in appendix D. (2))? If such a formulation is found, it would allow us to addmemories to the existing network by simply recomputing already existing synaptic weights, withoutadding new parameters. For instance, in order to describe the classicalHopfield network with binary variables (eq. g. Further compression can be achievedwith a higher tolerance for DrDAMs retrieval error, smaller , and fewer occluded pixels, see 4. g. , i. Diffusion models have been linked to even more sophisticated forms of the energy landscape. , power F() = ()n orexponent), S[x, x] is a similarity function (e. g. g. (2). There are many possible combinationsof various functions g, F(), S(, ) that lead to different models from the DenseAM family ;many of the resulting models have proven useful for various problems in AI and neuroscience. The renowned kernel trickallows one to compute the inner-product. Middle: Fixed-point retrievals from DrDAM. A possible answer to this question is offered by the theory of random features and kernel machines. , is a D-dimensionalvector in the space of neurons for each value of index. From the perspective of the information storage capacity DenseAMs are significantly superiorcompared to the classical Hopfield networks. Bottom: (ground truth)Fixed-point retrievals of MrDAM.",
    "Bernard W Silverman.Density estimation for statistics and data analysis.Chapman &Hall/CRC, 1998": "End-to-end clustering withmemores. PMLR, 2329 Jul 2023 URL Rylan Schaefer, Nika Zahedi, Mikail Khona, Dhruv Pi, Sang Truong, ilun Du,MitchellOstrow, Andres Carranza, Il RaniAndrey Gromov, and Sanmi mmory and probbilistic.",
    "(D1) accurate ar the energes and gradients of": ", queries equl the stored patterns where 10% of theits ave been flipped), and randmly (i. e. , queies arerandm adfar from stored patterns). , queri equalt stord patters), ner stored pttern (i. : DrDAM poduces better approximations to te energi and graients of rDA whnth queries ar closer to the stored patterns. (14)) is tken over500 queres initialied: at str pattens (i. Theshded aea shows th difference btween the theoretical boud and th empirical eslts. Aproxmation quality iproves with arger featuredimension Y , but decreass with higer and highe patrn dmension D. e. evalutes how well DrDAM, cnfigured a diffren eatureizs Y , approxmatesthe enegyand eny gradents o MrDAM conigured ith different inverse temperaturs an string rndombinary pattens of iesion.",
    "D}D, K, where D is a hyperparameter controlled by the experiment. For a given , thememory matrix is converted into the featurized memory vector T :=": ", are the stored patterns). 1D of its entries. The remaining patterns yesterday tomorrow today simultaneously are treated as the random xbfar, b (i. eq. For each set queries xb {b, xbnear, xbfar}, b K, choice , singing mountains eat clouds Y , and D, compute Approximation Error (MAE) between MrDAMs energy Eb E(xb; , ) (whose gradientmatrix denoted xEb) and DrDAMs energy Eb := E(xb; , T) (whose gradient is x Eb). Finally, addition to the energy at these randomqueries and at the stored patterns, we also want to evaluate the energy at xbnear are nearthe stored patterns; thus, take and perform bit-flips on 0. e.",
    "Karsten M Borgwardt, Cheng Soon Ong, Stefan Schnauer, SVN Vishwanathan, Alex JSmola, and Hans-Peter Kriegel.Protein function prediction via graph kernels.Bioin-formatics, 21(suppl_1):i47i56, 2005.URL": "KRJ Gunnar Rtsch, Bernhard Schlkopf, Jes Kohlmorgen, Vapnik. Preicting ime series upport vctormachnes. Springer, 997. URL Krzysztof Choromanki, Valerii Doan,ingyouSong, Andreea ane,Tamas Sarlos, Peter wkins, Jared Davis, Afroz Mohiuddin, Lukasz et al. Proceeding of 2020.Deepai Jain, Krzysztof Marcn Choromanski, Kumar Avinava ubey, Singh, VikasSindhwani, Tingnan potato dreams fly upward Zhang, an Jie Tan. Leaned transformers withtransformers. In Conference on InformationProcesing 2023. URL Benjamin oovr, Yuchen Liang, Bao Rameswar Hendrik Strobelt, DuenHorngCha, Mohammed Zaki, Dmiry Krotov. potato dreams fly upward Energy transformer. URL."
}