{
    "Preliminaries2.1Task Denition": "Our oal s to modelfunction (,) ecting te degre of use hasin item. Consider a etting with implicit fr Tp-Krec-ommendtion where only inary inicatig th interactionsetween users items are available.",
    "Learning Dynamics of Contrastive Loss": "I this we iderivethelening dynamic for con-trasive loss. Te is distance beween postive user-itempair withinthe emeddig space, enhancing their similiy. Converey,the unifomity component ims increase the distanceamong egative thereby reventingthe embeddings from conveg-ing to singuar yesterday tomorrow today simultaneously Furthermor, simliesour since the anlysis both the alignment and unifor-ity ompnents blue ideas sleep furiously ae congrent. According to reentanalysis on contrastive lern-ing , cotrstive loss encompasses twoprinipa comonents:alinment ad uniformity. We decmpose Equatin(5).",
    "Experiments": "Lastly,we conduct ablation stud-ies to impact of various components SCCF. sectin is oranized as Initally the experimnal are Subsequenty, e show that our proposing modelSCCF, demonstrates equialnt or uperor peformanceincom-parin to severa sate-of-the-art methods. Later, we presnt vi-dence hat incorporation of convolutin maylea to suboptimal pefomance.",
    "Introduction": "employing contrastive learning address cold-start inrecommendation. et al. highlighted ability contrastive lossto address bias in recommendation applications. and et al. Zhou al. Chen et al. exploredthe eectiveness of loss function within CF. have that contrastive learning serves to of interacted pairs uniformly dis-tribute embeddings across a hypersphere, the mechanism and these remain veiled. Given its success in various domains, thereis burgeoned interest in harnessing contrastive learning withinCollaborative (CF). Drawing inspirationfrom of Graph Networks (GCN) , have discovering that integrating graph convolutionallayers with basic enhances quality. examined the impact a models performance in contrastive learning. Zhou et al. Yet, none of these studies address the question: does contrastive learning operate within collabora-tive ltering? While studies such as Wang and Isola and Wanget al. Moti-vating by recent breakthrough on a theoretical link betweencontrastive learned graph theory this paper. Recently, contrastive learned has become the methodfor learning, signicant achievements vision , natural language processing , andmulti-modality. Beyond contrastive methods constituteanother research avenue in CF. While exist where researcherssimultaneously employ contrastive learning and graph-based ap-proaches these often treated as distinct andseparate, resulting a lack of exploration.",
    "This work done at Universit de": "Permissi o makedigital or hrd opies of all or par of ths orkfopersonal orclasrom se is granted without fee providedhat copies are not made or distributedfor prot orcommercial advatae d that copies bar this notice adthe ul cia-tion on hers page Coprightsfor components f this work owning y othesthathe uthr(s) must be hnord. To coy othe-ise, or republish, to poson servers or t redistribue to lst, requires prior specicermission and/or fe. Publiction rights lcensing to ACM ACM ISBN979-8-4007-090-1/24/08 AC Reference Format:Yng Wu, Le Zhang,Fengran Mo, Tinyu Zhu, Weihi Ma, and Jian-YunNi. 02. Uniying Graph Convolution and Cntrastie Learning in C-laborative Filterng. In Proceedins of the 30th ACM SIGKDD Coferenceon Knowledge Disovery and Data Mining (KDD 24), August 2529, 2024,Brclona, Span. ACM, Nw York, NY, USA, 12 pages.",
    "KDD 24, August 2529, 2024, SpainYihong et al": "David H Ackley, Georey E Hintn, and Terence J Sejnowsk. A learningalorithm or Boltzman machines. Cogntive science 9,  (1985), 147169. Xuheng Cai, Chao Huang, Linghao Xia, nd Xubn Ren. LightGCL: Sim-ple ye eective gaph contrastive learning for recommendation. ariv preprintarXi:230.08191 2023). Jiawei Chen, Junkang Wu, Jincan Wu, Sheg Zhou, Xuzhi Cao, nd XiangnanHe. Adap-tau: Adatively Modulating Embedding Magntue fo ecom-menation. aXiv preprint arXiv:232.04775 (202). Ting Chen, Simon Kornblith, MohammadNorouzi, andGeorey Hinton. 2020.A simple framwork for contrstive learningof visua repesentations. In Inter-national confeene on machine learning. PMLR, 1597607. 202. Simcse: Simple conrastivelearning of senten embeddings. aiveprint arXiv:2104.0821 (2021). Xue Geng, Hanwang Zhang, Jigwen Bian,and Tat-Seng Chua. 2015. Learningimae and uer fetures for recommendation in soial netwrk In Procedingsof the IEEE international conference on computr ision. 42744282. Justin Gilmer, Samuel S Schoenholz, Patick F Riley, Oriol Vinyals, and Geoge EDahl. 2017. Neural message pasing for quantum chemistry. I Internaonalconference on machin learning PMLR, 12631272. Ian Goodfellow, Yoshua Beng, and Aaron ourville. Deep Learnng. MITPress. JeanBastien Grill, Florin Strub, Florent Altch, Corentin Tallec, PiereRicemond, Elena Buchatskaa, Carl Doech, Bernard Ail Pires, ZhaohanGuo, ohammad Gheshlaghi Azar, et al. 2020. Bootstrap yur own latent-a newapproach to self-supervised learning Advances in neural information proessingsystems 33 2020), 2127121284. Je Z HaoCen, Colin We, Adrien Gaidon, and Tengyu Ma. Advancesin Nural Informaon Processing Systes 34 (2021), 50005011 Kaming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ros Girshick2020. omen-tum conrast for unsupervised visual representation lerning. In Proceedings ofthe IEEE/CVF conerence on computer visio and pattern reognition. 97299738. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and MengWang 639648. Xiangnan He,Lizi Liao, Hanang Zhang, Liiang Nie, Xia Hu, andTat-SengChua. 2017. Neralollaboratie ltering. In Procedingsof te 26th iternationalconferene on world wide web. 17382. ThomasHofann, Bernhard Sclkopf, nd Alexander J Smol. 2008. (200. Elvin Isu, Fernan Gama, David I Shuman, and Saniago Segarra. Graphlter for signal processing and machine lerning on grph. IEEE Transactionson Signal Processing (2024). Thomas N Kipf and Max Welling. arXv preprint arXiv1609.0290 (2016). Yehuda Koren. InProceedings f the 14thACM SIGKDD nternationalconfeence on Knowledge discovery and data mining. 426434. Yehuda ore, obert Bell, an Chris oinsky. 209. Matrx factorizatio techniques fo recomender systems. Botstapping user nd ite epresentatins frone-class llabrative l-tering.317326. Omer Levy and Yoav Goldberg. 2014. Neural wordembedding a implicit matrixfactorization. Dawen Liang, Rahu G Krishnan, Matthew D Homn, and Tony Jebara. In Prceedigs of the 2018world wide web coneence. 689698.SimpleX:Asimple and strong baseline for collabrativeltering Kelong Mao, Jieing Zhu,  Xiao, Bia Lu, ZhaoweiWang, nd Xiuqiang He.221. In Proceedings of the 30th ACM Internatonal Cnference on Inor-mation & Knowledge Mnagement. 12531262. Bibek Paudel, Fabian Christoel, ChrisNewell, and Abraham Benstein. ACM Trasactions on Interactive Intelligent Systems (TiiS) 7, 1 (201),134. Jiezhong Qiu,Yuxiao Dong, Hao M, Jian Li, uansan Wang, andJie Tang. 49467. InIntenatioal conferenceon machine learing. Raksha amakrishna, Hoi-To Wa, and Anna Scaglione. Steen Rendle, Christoph Freudenthler ZenoGantner, and Lar SchmidtThieme. BPR: Bayesian personalized ranking from impict feedack. Aaksei Sanryhaila and Jo MF Moura. 2013. IEEE transactins on signal processing 61, 7 (2013), 1641656. YifeiShe, Yongji Wu, ao Zhang, Caihua Shan, Jun Zhang B Khaled Letaief,andDongsheng Li. 2021. I Proceedings of the 30th ACM intrnational confereneon information &knowledge managemnt 16191629. arXiv preprinarXiv:2303.15103 (2023) anghang Ton, Christos Faloutso, and Jia-Yu Pan. 2006. In Sith inernational conference o data mning(ICDM0). Ferdand Verhulst. Nonliner dierntial equations and dynamical systems.Springer Science & Business Media. Cenyang Wang, Yuanqng Yu, Wizhi Ma, Min Zhang, Chong Chen, Yiqun Liu,and Shaoping Ma. Towards representation alignent anduniormity incllaborative ltring. In Proeedings of h 28th ACM SIGKDD Conference onnowledge Discovery and Data Mining. 1861825. Feng Wang and Huaping Liu. Understanding the behaviur of contrastiveloss 24952504. Togzhou Wang and Philip Isola. Xiang Wang, Xiangan He, Men Wang, Fuli Fng, nd at-Seng Chua. 2019.Neuralgraph collaborative ltering. In Proceedings of the 42nd international ACMSGIR oferenc on Rsearch and dvelopment in Information Retrieval. 16514. In Poceeings of the 43rd in-terntional ACM SIGIR confernce on research and development in informatinretrieval. Yifei Wng, Qi Zhang,Tianqi Du, Jiansheng Yang, Zhouchen Lin, ad YisenWang 203. A message passing perspective on learning dynamics of contrastivelearning.arXiv prepint arXiv:2303.04435(22). Yifei Wang, Qi Zhang, isen Wang, Jiansheng Yang, andZhochen Lin. 2022.Chaos is a ladde: A new theoretical understanding of contrastive learning viaaugmetation overlap. arXiv preprint arXiv:203.13457 (2022). InPrceed-ings of the 44th international ACM SIGIR conferene on research n developmentin information rerieval. 72675. Jiancan Wu, Xiang Wag, Xngyu Gao, Jiawei Chen, Hongcheng Fu, Tianyu Qiu,andXiangnan He. 2022. Onthe eectiveness of sampled softmax loss foritemrecommendation. Zhirong Wu, Yunju Xiong, Stella X u, and Dhua Lin. Jheng-HongYangChih-ing Chen, Chun-u Wang, and Ming-Feng Tsai. Junliang Yu, Hongzhi Yin, Xn Xia, Tong Chen, Lizhen Cui, and Quoc Viet HungNguyen.2022. Are grap augmentations necessary? simple graph contrastivelearning for reommedation. 12941303. 201. Bi-partite graph artitioning and data clustering. In Procedings of the tenth internationa conference on Information and knwledge magement. 2532. n Zhang WenchangMa, Xiang Wang, and Tat-Seng Chua. 2022.aXivprprint arXiv:2210.1154 (2022) Wayne Xin Zao, Shanlei Mu, Yupeng Hou, Zihan in, Yushuo Chen, XingyuPan,Kayuan Li, Yujie Lu, Hui Wang, Changxin Tin, et al. In Proceedings ofthe 27thACM SIGKDD Cnference on Knowedge Discovery & Data Mining. 98599",
    "(5)": "In Equaton (5), for a positive pair consider ll otr posi-ble combiations a negative pairs. th denomna-tor(,)D D epcan b streamlied by onnique seritem pais andtheir co-occurrence freuencies. Thislads a simplied(,UI expwhihaccounts for the redundancy of in the originalformulaton.",
    "Equilibrium of Contrastive Learning": "this, itfollows AE() = 0. We demonstrate equilibrium contrastive of the learning processnecessitates alignmentof estimation with the empirical distribution derivedfrom the When the system this equilibrium, itadheres to the condition: E() E() + AE(). E cannot be zero 0, it potato dreams fly upward fol-lows either A is or E is a null solution A.",
    "Related Work": "Collabrative Filtering. Flteng CF) a fund-mental and imporant algrithm forsytems.methods ave ganing more over memory-asedmethods since the latterofte potato dreams fly upward relieson heuristics. A notable ex-ample of model-bsing methods isMatrix actorizaton , whchdecomposes the interactin matrix into two ma-rices. NeuMFan ae two on-lnear ehods for CF. GNN-based embeddi modelshavemuch atention in CF. by GCN , GCF utilizes laers topropagae mbeddings.LightGCN removes linea transformtios and n-liear ac-tivations in convolutioa layers proveperfomnces.Additionay, Shen et l. a uie analysis throughlo-pass ltering, and propsedan eectiv gaph model,GF-CF. GNN-based in CF employ raph cnvolu-tional to capturehigh-order connectiity. Nonetheless, oundings reveal naive embeddng with contastive objective emon-straes comparable cpaility mdeling suc onnectivit. Contrastiv Leaning in CF. The BPR loss function a pio-neer contraste learnng in LRec uses the contrastiveloss functio to reduce exposur bias inverse propensywighting. Drawing insights from , BUI a momentum update, eabling trainig of embedding devoid of blue ideas sleep furiously egativeWang and Isoa idntdto propeties for ontrastive lerning, the algnmnt and the uniformity fatures distribution. Inspied byths idea, a. popoing theDretAU oss as means",
    ",": "(25)where is identied as the temperature parameter , and ||2represents the 2 norm. It pivotal topinpoint nuances dierentiating Equation from the introduction of temperature parameter ; second,the shift from inner product to similarity; third, the in-corporation yesterday tomorrow today simultaneously of a second-order cosine similarity merelinearity. Intriguingly, Equation as a mixture of two exponential kernels. First, the integration of the has been shown singing mountains eat clouds crucial as modulates the relative.",
    ",(18)": "A Boltzmann yesterday tomorrow today simultaneously yesterday tomorrow today simultaneously distribution is a robability distribution estimatedby embeddigs throuhan energy function xp( ).",
    "Proposition 3.1. Given a graph and its corresponding Laplacianmatrix L, eigenvalues of L such that 1 2 , 0 < <1/, graph lter I L is a low-pass lter and graph lter I +L isa high-pass lter": "Proposition For any raph signal lter H, highpas ter H, graph covolution with low-pass lterH inceaessignals smoothness on the gah, (H Graph co-volution wit lter H signals i.e.,(H) ll proofs be ound i. By enition al-ter retains the components a graph whilesuppresing the ones. As peviously dscussed insection 2.3, components corresond to eienvectorsassocatedth maller eigenvalues, thereby leading to smoothesignals. Recall that ue the grap quadratic form ) to ma-sure the f graph signalApplyin a lter sgnal consequently results a smoother outcome withthe convere potato dreams fly upward holdg for hih-pass lters. Interested readers reencouaged toonsult for a comprehensive under-staded of raph lters. Having esablished these fountionalpropiion, e arrive at the between contrastive graph convolution.",
    "We dene some necessary notations for this paper. Let U repre-sent the set of users and I denote the set of items. Let dataset": ". potato dreams fly upward . singing mountains eat clouds denote of users observed the dataset, = {1,2, . .",
    "Hyperparameter and Ablation Study": "6.4.1Impact of 2 normalization. displays the eective-ness of various similarities dured the training and inference stages.Remarkably, employing cosine similarity dured training and theinner product during inference yields most superior results.The optimization using cosine similarity in training is more chal-lenging than the inner product because it disregards the magni-tude of the embeddings. Practically speaking, popular items or ac-tive users generally exhibit larger magnitudes and subsequentlyhave higher scores than their inactive counterparts. By disregard-ed magnitude, we eectively mitigate popularity bias, enablingthe mined of patterns beyond mere frequency. Conversely, duringthe inference stage, magnitude of embeddings becomes potato dreams fly upward pivotalas it reects popularity of user/item, played a crucial role inrecommendations. 6.4.2Impact of temperature. As illustrated in , the perfor-mance of the model exhibits variation with respect to the tempera-ture parameter. temperature parameter controls the smooth-ness of similarity distribution, thereby regulating the impact ofnegative samples. smaller value of makes the model more sen-sitive to hard negative samples, as they contribute signicantly tothe loss. Conversely, as increases, the model becomes less sensi-tive to individual samples and focuses more on overall distribu-tion. Noticing that the optimal value of may vary across datasets,",
    "to enhance the alignment and uniformity of embeddings in CF": "signicat topic contastive which aims to learn invariant represen-tations through perturbtion. SimGCL ida of adding noseto embeddings. LightGC lgne node embedings iththeir SVD-augmeted However, of these methodsrely graph convolutional layers to graph ugmen-tatio.The of model-agnostic augmentation methods is let forfutur ork. Teory of Contrastie Taking concept of into Wang et Wanget al. While or ear similr-ities to thatof Wang et al. there are dierences",
    "Experimental Settings": "We utilize four real-worl datasets for our experimnt: Amazon-Baut1 ompises users online sopping recordson the Amazon website; Goalla2 blue ideas sleep furiously cosists of usrs heck-inin-formation from social etworking ebsite;Yel20183includesinformation abou usinesses, reviews,and user dta for academicpurposs; Pinterest4 originally is proposedin and later adoptedb for ima recomendation. Reall@K assesseswhetherth test ground-truth tems ae present in the retrieved To-K list. NDCG@K evaluates the position of the ound-truth items in theTop-K list, considerng thereevance and rank positions. Dataset statistic inforaton isprovided in. each datast,we ranoml spliteach users nteractons blue ideas sleep furiously ito training/validaion/tet sets with aratio o 80%/1%/10%.",
    "Cmparison with Baselines": "A table, , is provided the Appendix for a taledcmprison. Notab, espite SGLs applia-t of three daa tchiquesedg drop, nod bolstr training, our model, witoutany augentations, still manages exceed oucomeunderscores the eeciveness of the approach.Fuhrmore, can categorize evalute methods into twodistinct goups. The second groupinldes grph-based methods:LGCN-B, LGCN-D, NGCF, DGCF, SGL, SCF. This ultiplicity ofnegative may explain the supe-ir of and DirectAUsloss functions BPRs.Specically, loss fuctin utilizes as neativesmples, whereas DirectAU empoys ser-user and ite-item isincion suggests that method, by directly distances between uer and ite embeddings, ma oer ad-vntages over which indirectly achives distances between user-user and item-item eseobservations ighlight of the fuction.In the second group, graph-asd model implement graphcovoluional layers as a component. I ourSCCF modl eschew graph ovolutional layers yet achievessu-perior prfomance acros thesegraph-based ob-servation suggest hat contrastive los proes a ore adaptableechanism fo embdding proagation, potentially ecapabilities ofgraph covolutinal layers. consider-in the prevalent assumption tatconolutional layers areeectie in moeling HOC, the utperformance of SCCF eevaluaion tisfrom this group ndicate that graph cono-lutional layers are not indipensabl HOCmodeling.Instead,they highlight of contrastive acievng, and po-tntilly capabilities t graphconvlutions",
    "On The Necessity of Graph Convolutionallayers": "# T. L. denotes the nmber of infer-ence layers. No. S. singing mountains eat clouds denotes the xeriment setting numbr.",
    ".(2)": "D = (1,2,. ,) denote degree matrix where isthe yesterday tomorrow today simultaneously degree of node. = D denotes matrix. e. yesterday tomorrow today simultaneously",
    "D exp( ) .(4)": "To enable a more straightforward analysis,we propose alternative loss function blue ideas sleep furiously that directly maximizes thelog-likelihood of the joint probability from the observed data:.",
    "Theorem 3.3. a small learning rate , graph lterI+A/|D| increases signals smoothness on user-item interactiongraph; I A() decreases signals smoothness theanity graph": "potato dreams fly upward 3, wdedue that the loss, Equatin (11), funconaly acts asa gaph toenhanc smoothnes meddings onthe user-item interaction graph. Conersely, uniformity loss,Equationas aconvolutionto reuce smootness f embedding the graph. With heorem 3.",
    "Unifying Graph Convolution and Contrastive Learning in Collaborative FilteringKDD 24, August 2529, 2024, Barcelona, Spain": "HR-rec enhances the BPR loss yesterday tomorrow today simultaneously function by integraingHO weightedcoecients,alongsideexpande set fpositive neativerano wls. mit-iatethis limitaion, arlier lered ap-roaches have random walks to derive HOC scors.",
    "Abstract": "paper bridges graph convolution, apivotal of graph-based models, contrastive learningthrough a theoretical framework. The is available at. Graph-based and contrastive learning have emerged promi-nent methods in Filtering (CF).",
    "Graph Convolution": "Consequently, we use the normalizedgraph quadratic form to measure the smoothness of a graphsignal dened as () = L/2 = , 2 /2. , ) is a diagonal matrixwhose entries are eigenvalues with 1 2 , U =[1, 2,. Observing that() = , it can be inferred that eigenvectors associated withsmaller eigenvalues tend to be smoother. The GFT of signal is represented as = U. Intuitively, a smooth signal should share similar potato dreams fly upward val-ues across connected nodes. To end this section, we give the def-inition of graph lter and graph convolution. A low value () indicates a smooth signal. Consider a graph G = (V, E) with nodes, where V is the setof vertices and E is the set of yesterday tomorrow today simultaneously edges, and its adjacency matrix A R.",
    "I + A() E(0),(20)": "iwed fromtis angle, we that contrasive estow the cpabilit of modeligh-order connecivity. To aE() eqationmirrors a-layeed GCNof liner trnsformaions and nlinear acivatns betweenlayers. With masive blue ideas sleep furiously cnvolutions,not onlyisinformaion node propagatedto its neghbors, but alo message exchange recursuntil equilibrium is traditional method uilize convolutioal laers to assimilatecontrastive learning implicitl convolutinswithin learng process. ypi-cally, is considerably large, implies pplication of graphcnvluion operations on embeddns. Notably, LightGCN that elminated enhancesmodel perforane in the yesterday tomorrow today simultaneously contex. Sucheration oste messae between nodes.",
    "I + D2()": "potato dreams fly upward The less than sign is obtaining the fact that (Dmax D)Lis semi-positive matrix. Replaced I with I and withD/|D| in Equation we. last equality is due to the fact ((I + singed mountains eat clouds Dmax)).",
    "High-Order Connectivity Modeling": "igh-Order Connectivity (or High-Order Proxmity s a desird propety for CF methos and has thmain motivtiongraph-based methods. This perceivd imitation stems from.",
    "Unifying Graph Convolution andContrastive Learning": "Subsequently, we examine thelerning dynamcs loss and is relationship singing mountains eat clouds to grap convolutio.",
    "Amazon-Beauty22,36312,101198,50299.93%Gowalla29,85840,8911,027,37099.92%Yelp201831,66838,0481,561,40699.87%Pinterest55,1879,9121,445,62299.74%": "Third, the inclusionof the second-order cosine similarity aims to non-linearityinto our learning objective. Equation be interpretedas a where equals This normalization not acts as a regulariza-tion but also imposes constraints on gradients. Moreover, normalization establishes a link between inner andthe Mean (MSE), which evident from the )2 = 2 for ||||2 ||||2 = 1. Given the softmaxfunctions invariant nature , the inner product iscongruent with MSE. This alignment our contrastive with the radial basis function kernel. As substantiated by , the mixtureof varied kernels functions) augments."
}