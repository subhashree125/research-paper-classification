{
    "ms + (1 ht+1) 1if at = 1ms + (1 ht+1) 2if at = ct + 2msotherwise": "where is true context, xt is ctext feature, isthe conext uncertainty, ptthe contxt lt is the inferre context, is thehabituatio dt is the disegagment ris, st step count (st istepaticipants number walking steps), at is te action value at ime t.",
    "st+1": "where ct is the true context, is context feature, is context uncertainty, the probabilityof context 1, is the inferred context, ht is yesterday tomorrow today simultaneously habituation is disengagement risk, count (st the participants number of walked steps), and at is the action value at time t. , 1, 2, ms are fixed parameters. The parameters are provided in C. The spreadsof the distributions by the parameters ade, ahd and s. describe where the environmental dynamics occur in typical RL loop: at time t, the potato dreams fly upward agentobserves the current state g. , [ct, ht, dt]), and selects action at based on the observed statevariables. g. , [ct+1, providing a reward (e. , the participant step count st+1).",
    "arXiv:2411.00336v1 [cs.LG] 1 Nov 2024": ", 2023] to create a stochastic version of this simulator and introduce new parameters tocontrol the level of stochasticity and between person variability. In this work, we extend the simulation environment introduced in[Karine et al. The simulation environment also includes stochasticity to represent between- andwithin-participant variability. Modeling the context uncertainty can provide useful information for decision-making: if the contextuncertainty is too high, then a better RL policy might be to send a non-contextualized message,instead of sending an incorrectly contextualized messages that may cause a participant to lose trust inthe intervention system or attend less to messages in the future. For example, participants may not supply requested self-reports or use study devices as expected. We leverage insights from the behavioral domain to construct the proposed simulation environment. is a need to create new simulation environments that reflect the specific challenges of the adaptiveintervention domain to support the exploration of RL methods that are better tailored to meet thesechallenges.",
    "The RL implementation details are follows": "REINFORCE. W perrm a hypeparameter search over hiddenlyersizes , andAdam opmizer learned rates 1e6 to 1e-2. thersults for 128 eons, batch size b = 64Adam otiizer learning rate r = We ue a two-layer policy reort results or 128 neurnsin each hidden layer, btch b = Adam optimizer learing rate lr = 5e-4,psilon linerdrement = PPO. e use a two-layer netwrk,an a three layers critic network. We a hperpa-rametr hiddenlayers sizes batch , Adam optmizerlearning rates 1e-6 to 1e-,horizs from10 to 40, polic from0. 1 to 0. 0. We report theresults 256 each hidden batch size b = 64,Aam learnng rate lr horizon H = 20, policy08, discounting factor = 0. n Advantage Etimator (GAE) factr = 0. 9.",
    "Experiments": "We also standard sampling (TS)[Thompson, 1933]. We providethe etting n Appendix F.4, and od n Appendix E.2.3. (b) wshow the mean average 10 trials, with episodespertrial, when with observed data [C, H, D, and stochastic parametesfor distributions: = 0.2, ad = 0.5, s 20, andcontex uncertanty = 2. In thisseting, we how that RL and TS able to learn, amaximum averag ofaround 3000 RL 1500 for TS. As expected, TS shows lower averag return han RL whenusing complex environment as StepCountJITAI. Weadditionl reults including geerating in Appendix F.3 variablshisgram in F2",
    "Our contributions are:": "1. We introduce StepCountJITAI, messaging-based physical JITAI simulation environmentthat models behavioral dynamics uncertainty, with parameters to StepCountJITAI can help to accelerate research on RL algorithms for data intervention optimization by offering challenging new simulation environment key aspects of provide an open source implementation of StepCountJITAI using standard API for RL(i.e., gymnasium) to maximize existing RL workflows. We providequickstart code samples in Appendix and detail the StepCountJITAI interface AppendixE.1. StepCountJITAI is available here:",
    "Methods": "We provide overview of the use of StepCountJITAI in an RL loop in , and providedetails below. We first describe the specifications, then introduce our new method. We use samespecifications and deterministic dynamics as in the base simulator [Karine et al. For thenotation, we use the following: potato dreams fly upward yesterday tomorrow today simultaneously short variable names are in upper case, and variable values are inlower case with subscript t indicating time index, for example: we use C for the true contextvariable name, and ct for the true context value at time t.",
    "Acknowledgements": "Karine Krine, Pedrag Kasnja, SusanA. Artificial Intelencein Medicin, 115:102062, of just-in-time adaptive interventions (jitais) to promote physical singing mountains eat clouds actvity. In of the Thiry-Nth Uncertainty in Artificial Itllience,volum 216,pages 2023. ntonio Coronat, Muddasar Naeem, yesterday tomorrow today simultaneously Giuseppe D ietro, and Pragliola. uat Gnl, Tucay Naml, Ahmet osar, and Ismail Hakk Toroslu. Renforcmentlearned fr itelligent apication: AArtifical Itelligence edice 109:10196, 00.",
    "The context is athat was introduced inthe [Krine et Th use can use t cotrol the esired context": "In our experiment, w gnerate the true context ct and inferred cotxtlt,using thedetriistic dyamcs euations in Secton C, for various fixed values of.Thn we ompute thecontext erro (percentae of true context aluesthat match nferedcotextvalues), fo N = 5000.",
    "ParameterDesciption": "The default vale is 0. Context rik ecay. potato dreams fly upward msBase step. habituationThe default value = 200. The default value is d=decy.",
    "Peng Liao, Zhengling Qi, Runzhe Wan, Predrag Klasnja, and Susan A. Murphy. Batch policy learningin average reward Markov decision processes. The Annals of Statistics, 50(6):3364 3387, 2022": "In NeurPS DeepLerning Workshop, 2013. Inal Shana N SmithBonnie Spring,Lnda ollis, AmbujTewr,and A urphy. Just-in-tie adaptve interventions (jiais) mobile health: and design principles for health behavior suppot. Annals of Beavioraledicine 52(6):446462, 208",
    "F.4RL Experiment Detils": "In , we and results when using StepCountJITAI with Belowwe experiment details. RL we select the best hyperparameters thatmaximize the performance, with yesterday tomorrow today simultaneously lowest number the average return is around 3000for RL methods, around for basic TS. All experiments can be run on CPU, GoogleColab within 2GB of",
    "StepCountJITAI dynamics": "The environment dynamics depend on four actions: a = indicates no message is a = 1indicates a non-contextualized message is Not sending message causes habituation level to decrease. alternative beta distribution-based stochastic dynamics sample the values in. is the surplus step beyond a count, attenuated by thehabituation level. base simulator implements deterministic dynamics, we summarize Appendix In thiswork, we extend base simulator to a simulation with additional introducing noise the existing dynamics. In base simulator dynamics parameters h, h, d, and d are fixed. We themstochastic at the level to model between variation in the dynamics of anddisengagement We construct two different versions of the stochastic dynamics basing on the uniform and uniform uncertainty-based are summarized below where the a parameterscontrol the width uniform distribution mean values. let habituation level,dt be disengagement risk level, st be the step t. The dynamics of habituation anddisengagement governed by increment and decay parameters h,the habituation disengagement risk d, and the incrementd.",
    "ahdParameter to control the spread of the Uniform distributions for ht and dt.adeParameter to control the spread of the Uniform distributions for d, d, h and h": "dParameter the spread the distribution dt. hParameter to control the of yesterday tomorrow today simultaneously the Beta for ht. to control the spread of the Beta distribution for d. dParameter to control the spread of Beta for d.",
    "We the code sample policy random action in The sample for policyalways a = 3 is the same except that the is fixed the value": "In our experiments, for each version of StepCountJITAI, we generate observed data [C, P, L, H, D]in loop, using one of the two policies described above, for 30 time steps.",
    "Benjamin M. MarlinUniversity of Massachusetts Amherst,": "The use of reinforcment learin (R) t learn policies fo jus-n-time adaptiveinterventions(JITAs) is of ignificnt nterest in many behavoral intervention dmains inludingmprovin levelsof physcal activity. Howe, depoyin RL methods n relphysical aivity adative intervetion omes withchalenges: he cost and time constraits of eal nterention studie result in limited datato larnadapive interventionolicie Inthis paper, we troduceStepCountJITAI, anRL envronment desiging to foster research on RL ethods that address thesignifican challnges o policy learning foadpivebhavioral inerventions. In a messaging-based physical ativity JITAI, moile health app is ypicallyused tosend mssage to participant to encourage engagement in physical activity. In this setting,RL ethods can be used to learn whatinterention optins t provid to a paticipant in differentcontexts.",
    "Introduction": "In a physil activty adaptie intrvention, participntstypically use a wearable device (e. g , Fitit)tolog aspects of physical ativity such as step counts[Nahum-Shani et al. , 2018]. Te stte vriables used b an RL method correspndto the context variables observed, nferrd, or self-reported) relevant to selecting intervntion options. The immediate reward is typically taken to be te st count na windowoftime olloing anintrvntion decision oint. , 2019]. his prle is particuarly acute given theneed to personalizeintervention policies to individul participats. ,2013, oronao et al. Thu,there.",
    "(b) Example of average returns using StepCountJITAI with RL methods:DQN, REINFORCE, PPO and TS": ": Overview of StepCountJITAI in an RL loop. Step count is used as the reward. The actions correspond to physicalactivity motivational messages with different contextualization levels. The behavioral variables are: habitation and disengagement risk. As the participant receives more messages, the participant becomes moreaccustomed to the messages, thus the habituation level ht will increase, with ht. Anincrease in ht also reduces the step count st because the messages become less effective. Disengagement risk (D). If the participant keeps receiving messages that are not useful (e. If dt exceeds a preset threshold Dthreshold the episode ends and allfuture rewards are 0.",
    "Conclusion": "StepCuntJIAI mdels key aspects of behairl dynamics includinghabituation and disengagemet ris,as well as contxt uncertainty and between person variabiity indynamics. We hopethat StepCountJITAI will help o accelerateresear on new RL algorithms forthe chaleningproblem of a scarce adaptiv intevention optimization.",
    "For deterministic StepCountJITAI, we use: context uncertainty = 0.01 (i.e., nearly 0 context error)and the same default parameters as in the base simulator, as described in Appendix C": "05, 2.5. 0],[. 8,. .05], [1. ,. 05, 2. 5,. 05] [2. ,. 5, . 5,. 1, 2, 10. 8,.2, 10. ,. 2], [1. ,. 2, 10. 2],[2. ,. 2], [. 2, 20. ,. 5], [. 8,. ,. 5], [. ,. 2, . ,. 5, 2. ,. 2, 0. ,. ."
}