{
    "INTRODUCTION": "k. Uually, yesterday tomorrow today simultaneously is pre-trainig stratgy nodes close inlatent repreention space. Howevr,many downsream tasks e not limited t edg-leveltasks but node-lvel taks node mult-lass classificatin) (e. hs mayeven beak own (a. , graphclasfication). Although has been achievd strategesthere tillexists a huge gap between thesepretexs and For pretxt fr th pre-trinng raphisbinar ede preictin. negative trasfer wen cnnectednodes different labels. eyond ex-quisiterecent have witnesed researchtrend o how to train amode or ddicated suprvising learningon graphs havil relon labels, are ot always i the real worl. this pre-raining to graph-levl is neither effcient because we have to pay huge effortstolearn an appropriate funcion node embeddng to thewhoe grph representation. raph (GNNs) have been widely to variuapplication such as omputing , anmaly and ntwork analysis. Anothershrtcomed is v-fitting problem th testingdaa. g. If we transfr the bovpretraiing model multi-class nde classificatin, reuireus tocarefullysarch the results in parameterspace additional classes nde lals. Tosolvethee mnystudies turn to and which a raph mode with easily accssible data, and thentransferred graph knowledge to new omainor viatunig he last layr of re-trained model.",
    "Transferability Analysis (RQ2)": "ee pre-rainthe graph ntwork on Amazon thenthemdel ontwo tasks (grahlevlnodelevel, and further evaluatete performance the target task (dge level) We reprtte in om which have to Firt,our prompt ethod significanl utprform theother aproachesand the resultssense In cntrast, the fthe transfer mthod is t he surce sometimes yesterday tomorrow today simultaneously can well ecide he tasks the target classes maybe arfom the Second, he grap-level task has beter adapability thanthe node-level task fo edge-level target, which yesterday tomorrow today simultaneously in line ntuition presented (secton 2). e evaluae the transferability from two perspectives: (1)how effctively is the ranfering diferntasks same domain? andhw effctivey is mdel transrredto differt domains? 4. Here the hardransfer method seek source modl whichhasthe same ask and then econductth model on the new task. to Diffeet Level Tasks. 3. 4 incdffrent have feature dimensons, we heruse SVD featuresrom all domains 10 dimensions. The finetune mthod load source ask mode and then tune the task heew task.",
    "Prompting via Meta Learning": "2. 3, align the graph label with thetarget node label, and treat this graph as a member in D or D ;for edge classification task, we first generate edge inducedgraphs for training and tested and the edge label is up to its twoendpoints. 3. 2Applyed Meta-learning to Graph Prompting. Let be promptparameters, be fixed parameters of the pre-training graphbackbone, and be the taskers parameters. We use , | to denotethe pipeline with prompt graph (), pre-trained model (, fixed),and downstream tasker ().",
    "Networks Online social networks; Computing method-ologies Knowledge representation and reasoning": "Reqes permissions from 23, Augut 610, 2023, Long Bah, CA, USA 223 Copyight hld by te Publication liensed to CM. opy otherwise, orrpubsh, post on servers to reistibute to requires permissionnd/or a ee. to make digital had copes of all or partof this wok fororclassroom use is ithout fee provied that not ade or distributedfor profit or and ths notice and the fll citationon the Coyrihts singing mountains eat clouds for compoents of this work by othrs than theauthor(s) must be honored. Abstractin with credi is permitted. ACM ISBN 979-8-4007-0103-0/23/08. 00. $15.",
    "Reformulating Downstream Tasks": "1Why Reformulate Downstream Tasks. Intuitivly, manynode-level operations such as changing node featres, elete/adda node, or edge-level operations potato dreams fly upward such as addelete an edge, canbe teted as some basic operations at the graph leve. 3. As shown ina, an indued graph or a target nde mens itsloca areain the network within distance, which islso known s its -egonetwok. 2. Secificaly, we reformuate node-lvl andedgelevel tasks togaph-level tasks by build-ing induce graphs for noes nd edges, respectively. With the above moi-vatio, we revisit he otential task space on te gaph nd findtheir hierrcical relation as shown in b.",
    "prompt": "GraphCL+GAT86.40 86.47 894686.50 89.93 92.2473.36 7332 94.02 85.97 87.17GrphCL+GCN85.95 88.9587.00 95.3572.50 813794.05 94.05 8443 8.96GraphCL+G86.0585.17 8.9385.50 85.2 88.6072.6370.7 94.8287.03 86.9686.36 89.587.50 91.472.2 83.33 89.4193.35 94.6187.75 7.69 89.9585.00 8.8591.9581.00 82.24 89.393.95 92.06 938985.50 85.54 87.30imGRACEGT86.85 86.886.390.8576.50 80.82 868494.05 940 94.9686.40 8650 89.74 blue ideas sleep furiously",
    "Visualization of graph representations": "U196205, No. Xianguo Sun, in paticula,wants to tankhis parentsfor their kind upport yesterday tomorrow today simultaneously during his tough perod. 2022YFB104300, No. Ths reserch is supported in pary project MMT-p2-23of theShun Hig Instiute of Advance Engineering, The Chiese Univer-sity of Hong Kongby grants fo the Reseach Grant Council othe Hong Kon Specal Aministrative egion, China(No. 1972087, No 6220667, No. 2021YFC3300300, the Fundamental Re-sach Funds fo th Cetral Universities (o. 221KH0AB04). 6172300 No. ZD-21-202101, adOpen search Projects of Zhejian Lab No. CUHK14217622), NSFC (No.",
    "is an Pecy Lng. 2021. Prefix-Tuning: Optiizig ContinuousPrompts for Generatio. InProceeding of the Annual of the Assoca-tion for omputational Liguisics. 45824597": "Visio-Language PreTraining Mul-timoal ect-Based Sentiment Analysis. InPoceedings the of the Association fo arXiv prerit ariv:107. Xiao Liu, Ji, Yicheng Weg Tam,Zhengxao Du, Zhili Yang, anJieTang. 2022. P-Tning: Prompt Tuning Can Be Comparable to Fine-tuning 6168.",
    "Prompt Graph Design": "However, in thegraph there are no positions like a sentence to making the graph yesterday tomorrow today simultaneously more difficult. To seamlessly trans-fer the prompting idea NLP the graph we proposeto unify NLP Prompt and Graph in one perspective. In singing mountains eat clouds the NLP the prompt is usually added in the frontor the back end of the input sentences by default.",
    "KDD 23, August 610, 2023, Long Beach, CA, USAXiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan": "Takuya Akiba, Takanori Hayashi, Nozomi Nori, Yoichi Iwata, and Yuichi Yoshida. 2015. In Proceedings of AAAI Conference on Artificial Intelligence,Vol. 29. Yunsheng Bai, Hao Ding, Yang Qiao, Agustin Marinovic, Ken Gu, Ting Chen,Yizhou Sun, and Wei Wang. 2019. Unsupervised inductive graph-level represen-tation learning via graph-graph proximity. In Proceedings of the 28th InternationalJoint Conference on Artificial Intelligence. 19881994. 2020. Language models are few-shot learners. Advances in neuralinformation processing systems 33 (2020), 18771901. Hongxu Chen, Hongzhi Yin, Xiangguo Sun, Tong Chen, Bogdan Gabrys, andKatarzyna Musial. 2020. In Proceedings of the 26th ACM SIGKDD interna-tional conference on knowledge discovery & data mining. 15031511. 2022. In Proceedings of the 28th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining. 27412751.",
    "Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, andPeng Cui. 2021. Towards out-of-distribution generalization: A survey. arXivpreprint arXiv:2108.13624 (2021)": "IEE Transactions singing mountains eat clouds on Knowledge and Data Engineering(2023). yesterday tomorrow today simultaneously 2023. Inroceedings of 8th ACM SIGKDD Confrenc on Knowedge Discovey andData Mining. AutoPrompt: licitingKnowledge rm Language Model withAutomatically Gnerated Pompts. 2020. aylor Shin, Yasaman Razeghi, Robert L. In Empirial Methods in Natural LanguageProcessed (EMNLP). GPPT:Graph pre-training and prompt tuningto generalize graph neur eworks. 1171727. Xiangguo Sun, Hongzhi Yin, Bo Liu, HongxuChen, Qing Meng, Wang Han, andJiuxin Cao. Xianguo Sun, Hngzhi Yin, Bo Liu, QigMeng, Jiuxin Cao, Alexander Zhou,and Hongxu Chen. Maske label pediction: Uniiing mesage passing mel for emi-supervisedclasification. Struture Learning Via Mea-Hypeedge for DynamicRumor Detection. Mingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, and Xn Wan 2022. 021.",
    "pre-train+fine-tune": "57 94. 63 92. 3591. 52 89. 99SimGRACE+GT86. 58 93. 06 81. 95 86. 97 77. 50 85. 0485. 3384. 95 86. 1372. 5671. 33 88. 58 93. 8169. 55 94. 50 86. 64 88. 03 72. 6481. 50 70. 47 89. 56 94. 50 85. 2392. 54 89. 93 88. 29 89. 90 80. 5583. 7886. 3183. 59 87. 55 94. 9486. 37. 40 86. 00 85. 64 86. 3393. 33 86. 50 85. 7885. 1190. 05 87. 52 94. 46 91. 63 70. 50 81. 70 88. 00 81. 6080. 9378. 02SimGRACE+GCN85. 50 86. 05 89. 26 84.",
    "A, + = ((A X)) + (5)": "Unlike the indiscriminate inserting inEquation (5) (X + means the prompt token should be addedto every node of the original graph), the inserting pattern of ourproposed prompt graph is highly customized. In this paper, we extend the standalonetoken to a prompt graph that has multiple prompt tokens withlearnable inner structures. Here denotesthe error bound between the manipulated graph and the promptinggraph w. t. This error bound is related to some non-linear layers of the model(unchangeable) and the quality of the learned prompt (changeable),which is promising to be further narrowed down by a more ad-vanced prompt scheme. r. Let (G, G) denote. their representations from the pre-trained graph model.",
    "In this section, we supplement more experiments to evaluate theeffectiveness of our framework further. The source code is publiclyavailable at": "s. erson-alityafe and acebook are used to test peformnceof predicto, both of which are ocialnetorks wher egesenote the folowing/quoing rlations. In partiulr, Movielens contains users rating scores the edge in which ha a ranging rom 0 to 5. Herewe present the results under  setting. (2 10% of rest edges as th supervision raningset. Addtional Graph-levl Classificaion we evalute classificaion performance where the abl is noimpacted by attibute. Results fom deonstate that theperformance ur prompt-baed method still keepth best inmost cases. Muti-class I the main experi-ments, treat he task asroble. Areported i method still outperforms herest methods. The graph abel isssigned as positive if pairs a ositive edge and icevera. Mvielens and QM9 are use tovauate the performnce method on and graph-level regresin, respectively. Secifically ENZYMES nd PoteinsFul ae two mlecule/proteindtasets that are usd adiional graphlevel classificaiontasks. As shown , our method effective in the multi-class graphclasification, espciy inte ew-shotseting.",
    "Allin ne:MultiTask Prompting for Graph NetwosKDD 23, August 610, Long Beach, , USA": "the complexity of the prompt graph very small. The limiting to-ken numbers make our tunable parameter space far smaller thantraditional methods, which can be seen in . This meansour method can be efficiently training with a few steps of tuning.As shown in , the prompt-based method converges fasterthan traditional pre-train and supervising methods, which furthersuggests the efficiency advantages of our method.",
    "(4)": "In sectin 3. Learnng Process. 5,we lso propose a very simple but effective hand-crafted prmptanswering teplate wihout any tunable task3. We. where (L) is the Hssian mtrix wih (H(L)) = 2L/ ;ad can be updatdinsame Kindly note tht in thearea, head is alsoknown as aneig function, connects the theanwes downstream tasks e reformlated. The nswerngfunction can be both unable or hand-craf templas.",
    "Multi-Task Performance with Few-shotLearning Settings (RQ1)": "We repeat the evaluation 5 times andreport the average resulsin , (Appendix A), and (Append A). In contrast, pre-traning approache contain moreprior knwledg, making the graph model rely less n data labels. However, o achieve beter results on a specific task, we usuallyneed t carefully select an propriate pre-traiing approach andcafuly tune the model to match the target task, but his hugeeffort is ot ensured to be applicable t other tass. Te reprtimprovemets rangefrom 1. 10% to 8. 28% o 12. 14%to 10.77% on graph-leveltsks.In paricular, we alo compared our noe-level perforacewith the previously mentioned node-level prompt model GPT in Kindly note that or experiment settings are totall if-ferentfrom GPP. In GPPT, they tdy the few-sot problem bymasking 30% or 50% data labels. However, in or paper, we proposea more challenging problem: howdoes the modelperform i wefurther reduce the label dta? So in our exprimnt, each class onlyhas 100 labeled samples. 7% onReddit, 7. 5%onPubmed, which are far lessthan the reported GPPT (5%labeled).",
    "return ,|": "efficiency, and compatibility below. proved that we canalways learn an appropriate prompt making the followingequation stand:. Therefore, flexibility of dataoperations is the bottleneck potato dreams fly upward of prompting performance. The nature prompted is to manipulate data to match the pretext. only contains isolating tokens, each of which blue ideas sleep furiously corresponds category.",
    "{D1, , D }, and query data QE = {Q()E , Q()E , Q()E } where": "{D1, , D }. Wetreat each noe/edge/graph class asa binary classificatin tas that tey can sha the same taskhea. our yesterday tomorrow today simultaneously ethod can so deal with other tasks beyondclassficationwith only few se Appendix A).",
    "EVALUATION": "In tis section, wewit other ap-proacheso ede-level, and graph-level of aphs.Inwish to anwer hefollowing research uestions:Q1:How efectiveis our ethod under the e-shot earning back-goud or multiletass? Q2: is oumethodwhen transferred to oter domans tasks? Q3: How d he of o the performance? is our model compared wih traitional 5: is our method wen we manipulate graphs?",
    ": Our graph prompt inspired by the language prompt": "To address third challenge,we introduce te technique ove ltiple tasks tolean better prompts. Currently, we veryfew works thegraph prompt issue. However, canonly deal with task (e. , ode clasification) usng aspecific e. , hich is far ddressingthe multitask stting with different-levl tasksLast but least,learning a reliable prompt usuall hugmanpower is sensitive to prompt initialization in themulti-tsk setting. be even wors in ou multi-task graph area since featuresvary a indiferent domains and To fillthe gap graph and downstrem tasks, e introducethe prompt methodfrom NLPto graphs under the multi-taskbackgrund. Spcifically,to ddress the first chalenge, proose nify the forat languag and graph propt in one way so thatwe cansmoohly tansfer the prompt idea from NLP grps, thn wedesign the grph propt from propt tokens, token tuctures,and rompt inserting pattern. g. ae some inthe NLP area tthe prompt via or ome dicrete features, thee re tas-bounded,wich is sufficient we confront a new task. potato dreams fly upward We carefully our otheraproaches the expeimental potato dreams fly upward esults extensively adntages. addrss the second haleng,we first study the subspace in graphs hen toreformulate node-level and edge-level task to tass byinduced graphs fro original graphs. the capability of generaization. g. Contbutions:.",
    "ABSTRACT": "However, task wih node level, edge andraph lve are diversified, making th pre-training potato dreams fly upward pretext thesegap may even cause anegatie transfer tothe spcific applcatin leading oor results. In this pape, we propose anovel multi-task promptin method for Specificaly,we first unify theformat graph prompts promptswith prompt toen token and inserting pattern. Inthis way, the promting idea from NLP ca beseamlessly intro-ducd to the graph Ten, to narrow gap tasks and sttef-the-art pre-trainin wefurthr tudy the task space of grah applications re-formulat downsteam problems to task. Afterard,we intduce meta-learning to eficiently learn a better the multi-task prompt of gaphsso tha our prmptingfrmework can be more reliale for different tasks.",
    "elling and Thomas Kipf 216 Semi-supervised classificationwith networs. In J. Conference on Lerning Representations(ICLR 2017": "ASiple Frameworkfor GrahContrastive withut In Proceengs of he Web Conference 2022.Yuning You Tianlong Chen, Si, Ting Chen Zhangyag Wang, andYang 2020.",
    "Why It Works?": "The compound graph will be sent to the pre-trained blue ideas sleep furiously model againto predict the link connecting each node to the label tokens. They use edge prediction asa pre-training pretext and reformulate node classification blue ideas sleep furiously to thepretext by designing labeled tokens added to the original graph."
}