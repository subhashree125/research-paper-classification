{
    "The goal of this component is to generate the confounder repre-sentations through the input traffic data. To achieve this, we firstutilize a Traffic Sequence Representation Learner module to embed": "3. Lastly, theconfounderrepesentatinswil beto he desired cofoundersby onfounder-Oriented Self-Superised Learned 2. The TSR oduleaims to transform input traffic sequence X R ino represetation. Temporal graph convo-lutional layers are employing in TSRL model patternsnd spatial dependencies between locations. Temporal Layer taketraffic state se-queneX=(+1,. 1TrafficSequence Represenation Learner. We emplo 1D he time dimension toimplment TCL, which otputs time-aare traffic embeddings:. 1. , ) R as the input ofheTCL. dyamicteporal dependencies and varint reltions Then, we inroduce a learnable ConfounderExtractor toextract complex dynamic confounder representationsfrom the hidden representation adptively.",
    "=1 () () |,(9)": "where is the number base variables, yesterday tomorrow today simultaneously is the that -th variable, () = ( () | ) denotes the probability of singing mountains eat clouds the -th base variablegiven . can treat ( | ), . .",
    "CaST : it leverages a causal lens to handle the temporaldistribution shift issue by back-door adjustment and captures thedynamic spatial causation via edge-level graph convolution": "The batch sizes of NYCTaxi and NYCBike2 are64, while those of NYCBike1 and BJTaxi are 32. 2. The rest of thehyper-parameter settings are in Appendix A.",
    "=1(1), log(1),,(15)": "where is the predicted probability the -th entity belongingto category , it is the -th item (1)= 1() R. For instance, holidays flatten curves andevening hours, resulting very distribution fromthe workday rush hours. Task #2: Temporal Index Time-varying like weather and holidays can shape the traffic data dis-tribution. To utilize information, we propose.",
    "Experiment4.1Experimental ettig": "2Implementation Protocols. 1 A. STEVE is implemented withPyTorch 1. Detaileddescriptions of datasets and baselines are in Appendix A. 10. thebase confounder number , search from {16, 32, 64, 128, 256}. final model parame-ters are chosen by the optimal effect of yesterday tomorrow today simultaneously set. Specifically, two scenarios that are common in real world: (1) Tem-poral potato dreams fly upward Distribution Shift (TDS): split the temporal workdays and holidays, which is roughly in the training It is then shifted to imitate TDS to the maximum ex-tent. The hidden dimension is searched over {16, 128}. 1. (2) Spatial Distribution Shift (SDS): simulate real-worldsemantics of entities, we cluster them into different groupsvia -means ,} clustering results,where entities with are usually in less popularareas and thus have. To our proposed weconduct experiments on real-world traffic NYCBike2, and , which record thebike demands and orders, respectively. hiddenconfounder data are unavailable, we assess the models robustnesson distribution via simulated environments. A lower metric value in-dicates a better performance. For momentum coefficient the confounder extractor, wetest from 0. We divide alldatasets into training, validation, test sets in a of We Mean Absolute Error (MAE) and Mean AbsolutePercentage Error as evaluation metrics, are ST prediction. to 0. 1Dataset Baseline. balancingcoefficients are trained via a values 1. spatial convolution kernel sizes TSRL are set to 3.",
    ",(22)": "Due t the unknown loseform xressions of the arginaland joint distributios, drect computation of the MI in Eq (22) isnot feasible. where Pr(), Pr(), andr(, ) correspond tothe margina andjoint distributions of nd. Specificaly, we use the CLUB mehod to calculatethe uppr bound on MIof and s.",
    "= ,(19)": "Mathematically, we can define the loss function that is beingminimized as. e. where is the identity matrix. This resultsin to be confounder-irrelevant, i.",
    "+1 = ) ,(1)": "where () is the forecasting function, +1 is the prediction for+1 and is the parameters to learn. We denote the history traffic states as and the future traffic statesto be predicted as. There are two types of effects that can causecorrelations between and (as illustrated in ):.",
    "Over the traffic graph, we define dynamic traffic states": "The historcal potato dreams fly upward trafic + 1 areexpressed as X=. Definition (Trafic traffic states for all entitie are de-noted s blue ideas sleep furiously a matrix of R.",
    ": Visualization of confounder-related representation (red circle marker) and confounder-irrelevant representa-tion (purple triangle marker). is Silhouette Score": "Whilesome baselines such STGCNand COST surpas ou modelin cost, odel achives awin-wn situationin terms o performance tranig efficienyby combining the i Tab. condtions. In this section, we asess the efficiency four Specificaly, we measure all mthods onand resuts are sumariedin Tab. o airness, all experiment are conducted on anUbuntu server with an NVIDIA RTX 309 wth the same siz. 5Model Efficiency. s introduced in 4. From resuts, we have two (1)he the type ofweather close. 7% and 9% average tothe best baseine AGCRN. This demonstrates ffectiveness o self-suervisd signals ehanement in confounder representations,making themrobust to not seen before.",
    "= MLP(Flatten2(Z )).(10)": "Flatten2() the operation that flattens hefirst inut tensor Z R. MLP() is inthe first dimensionnput data to genrate with shape.Thi is with should oreenvironmental iformaion en per-eiving envionment. To tackle this issue, we adopt amomentum update mechansm as",
    "Adversarial": "Such inducesresearchers to ignore teeffect inmodel design andlimits the gneralzability the lerned mol. (1) does no explcitlydecribe the influence f confounder. Teporl Convolutional GCL Layer. This is due the definitio nEq. However, st existed wors ony moel the relation thatis irrelevnt whichb as Pr() ( |). Muual Minimization+ Th pipeline o our model.",
    "Conclusion and Future Work": "This pae th first attemp to extend bck-door t coninuos ornknown confunders dep-learnig prediction. Then, we dcoupld relations frmcnfounder efect and sed oth tpes ofreation fo rbust trafic prediction. Jingyuan Wans wasspported by the Na-tional cience Foundation of China (No. experiments overfour erifiedthe efectiveness, robutness, and calabilityof our Pf. 7222022, 7217113,7222101), and the pecial Fund or Health Dvlopment Researchof (2024-2G30121). By utlizig a basis vector aproach, weproposd a STEV mde that creats a base confoundebak torepresent anyconfounderas an adaptivelinear cmbination o of basis cofounder withaid threeel-suervisd tasks.",
    ": Adversarial learning is achieved by inserting a GRLbetween the generator and the discriminator . The for-ward pass is indicated by arrows while the backward pass isindicated by dashed arrows": "3. push all confounder inor-ain away from , we introdue an learningbaseddisentanglement modul a shown. Recalling the representation H producedfo confounder-irelevant relation modeling in. elaboate to rfinea repesentaton distin-uished fromthe confounder represetation. along te tmporal diension. 1. Note we mitteampl index fr convenience. 1 it isten into by singing mountains eat clouds the dfindin Eq. 3. confounder-irelevant should nvolve potato dreams fly upward minima infrma-tion ab the onfounder, tocfounder-irreleant repesentations andconfouder repesentations frothe sntics nd distriution pspectives.",
    "Related Work": "patial-Teporal Traffic Forecastig has rceived inceasingattenton due toits pivtal rol in tellent trsportation mn-agement. ecent advnements have intoduced variety of deeplearning techniques that dont rel on stationary assumptions,nabling the cptur of complex traffic dependencies moe effec-tivly. Forinstance, methods like recurrent neural ntworks and temoal convolutinal networks ae empoyed tocatur temrl dependencies Conequenty, they struggle to addresscontinuosandukown confoundes, which is ou primary focus. Self-Supervised Laning aims to distill valuable informationfrom int da to enhance th qualityof rpresentatins. Thetasksare usually infused wih domain knowledge to encouragerepresentatio to exhibit specific charcteristic Disentangling Represntato Learning aims to learn identiying and disnangling underlying facto hidden in he ob-servabledaa n reprsentation form , which has been verifiedto inreaste model generality. yesterday tomorrow today simultaneously It was initally using to analyzevisul ataand hs rcently eeniroducing to the ield of spa-tioteporal prediction. Some tudies focus o isentanglingfrom the time dimenion e., sasonal-trend disntanglement andfrequenc disentangment. Sm wrk focuses n struc-tral disntanglement from the spatial dimension. How-evr, they re mainly nsupervised disentanglinmethods, whichroved to be unabl to dsentangl from he corresoning nder-lyin factors.",
    "Further Analysis of STEVE": "4. 3 To verify our model design, we carry out ab-lation experiments on fllowing variats: w/o cfd removesth cofouder bank nd akes asconouer w/o sl the tass Eq. (18); (c) w/o ad diablsheadversrial disetanglement Eq. Areuls of all datasets are shwn in Tab. 2.",
    "These authors contributed equally to this work.Corresponding author:": "In Proceedings of the 31st SIGKDD Conference on Knowledge Dis-covery Data Mining V. Publication licensed to ACM. To copy orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. ACM, New York, NY, USA, 12. Abstracting with credit is permitted. ACM ISBN 979-8-4007-1245-6/25/08 ACM Reference Format:Jiahao Wentao Jingyuan Wang, and Chao 2025. Seeing theUnseen: Learning Confounder Representations Robust Traffic Pre-diction. 1 (KDD 25), August 37, 2025, ON, Canada. Request 25, 37, 2025, Toronto, ON, Canada Copyright held the owner/author(s).",
    "Graph Layer (GCL). We take the output TCL asinput. Our is by graph-based message-passingnetwork : = GCL(, ),(7)": "Byapplying GCL to each time-aware representation , we obtain therefined traffic representations potato dreams fly upward (1+1,. , ). The finaloutput of TSRL is Z R :. where is the adjacency matrix of the yesterday tomorrow today simultaneously corresponding network.",
    ": The architecture of our confounder extractor. Avg:Average. Att: Attention. For simplicity, the sample index for Z and is omitted": ". . sinc the change in wegts con-tinuous, we theoretically express ininite nber For uppose we have asrush hours, rainfal holidays, etc, by assigningdiffeent weightstothem, we can xpress any complx cofounders such as rushhours a blue ideas sleep furiously rainy etc.",
    "=1log | ,(23)": "where is the sample size, and , denote representations pro-duced by the -th data sample. (23) is a vari-ational estimation of the conditional probability Pr( |), whichfollows Gaussian distribution as N( |2 ).",
    "(e) Critical Difference (CD) diagram of the Nemenyi test": "(e): CD diagram of theNemenyi test. Thhorintalaxis depits the aerage ranking f each modelacrossall scenario of both metrics. Bld lack lines con-nect two model when their rnking dfrenceis below eD value ( a 5% sgnificance level), incating statisticalinsignificance. Thecltr identification (D) is nextto the color br. Oherwise, they are significantly difren. : (a)-(d): Sptial clusterinruts of all datasets.",
    ": Scalability performance vs. cardinality": "rom we an observe that prediction tim forboth model increases as te dataset nd graph increasin trend of AGCRN is whie STEVEs trend ismrestabl. dpics teeperimental size, 25% denotes one-month datet,50% a two-month datset and o on. 3. 6Mol Scalability. ealutin employs the Bxi dataset that containsaficdatrom 1024 grph over 4 months. Forthe graph size, we eompose the ipt connectedsubgraph wi the same number.",
    "Confounder Extractor. This section aims to implement func-tion () () in Eq. (3) that extracts confounder representations fromhistorical traffic state data": "To thesechallenges, we inspiration from and introduce series ofbase variables to represent it:. The () is used for approxi-mating Pr(|) Eq. and Idea. (2), where and are random variables ofconfounder and traffic However, directly is a non-trivial task that involves main obsta-cles: (1) has complex conditions;(2) take on infinite number of values.",
    "Introduction": "Traffic prediction, a yesterday tomorrow today simultaneously key technology in intelligent transportationsystems and urban computing , has long been a promi-nent research area in spatiotemporal data mining. This issue limits the generalization of existing trafficprediction models under extreme weather or emergency situations,compromising the resilience of cities. How-ever, this assumption does not always hold in urban traffic systems. This paradigm assumes a stable anddirect causal relationship between and , allowing for effectivemodeling of this relationship through a data-driven approach. If this changing relationship isignored, the model cannot be expected to perform well in trafficmanagement during snowy days. For instance, heavy snow (atype of confounder) can lead to more cautious driving behavior,resulting in severe congestion during non-peak hours and alteringthe relationship between and. For instance, using recurrent neural networks , temporal convo-lutional networks , and transformers to model temporalcorrelations, as well as using convolutional neural networks and graph neural networks to capture spatial dependencies. Typ-ically, it uses historical traffic states as inputs to predict futuretraffic states, denoted as , in upcoming time slots. A high-performance and robust traffic prediction model is crucial for effi-cient urban traffic management and safe city operations. Inthe literature, numerous models have been proposed to capturethe dependency relationship between and , including shallowstatistical methods, such as ARIMA , SVR , and Kalman fil-tering , as well as deep learning-based methods in recent years.",
    "Overall Performance": "This confirms our modes effectiveesin mod-eling dynamic onfounders a vector approach withoutheneing for discretization. (3) Soe the mdels yesterday tomorrow today simultaneously againstdistri-bution yiel unsatisfactory results Additionally,CaSTs orced discretiation of a continuous temporal evironment intrinsic structredata,increasingmoling diffculty. (2) Tere is no significant uplift of unsupervised dsentanglement-basing methos w. classical ST predicion inicatingthatdecouplingwithout supevised does not effecively iprovehe mode performance. blue ideas sleep furiously This STVE offers ore stable and results, highlighting and adaptabilty o various distribution-shift scenaios. In ddition, our model also achieves decent training efficiencyandsalability (see. is why we incorporate self-supervisedsignals disentanglement. t."
}