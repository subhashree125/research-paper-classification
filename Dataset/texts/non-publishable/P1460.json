{
    "Causal attention": "our of attention dynamics, we review the idea of al. (2023) formodeling the full attention dynamics. usually also includes a multiplication by a trainable diagonal matrix D, but the effectof this step can be equivalently achieved by multiplying K, Q, V matrices by ) These ODEs are parametrized bythree matrices, as the query Q, the key K the value , respectively, and that are assumed to square d",
    "maxt[0,Tj] |xsj(t) xsj(0)| < c1/2": "key observation driving this resut is that strong Rnyi entes are weakly affecting byall previous prticles. owevr, though this is corrc on short time scales, it huld be checkedfor all times in [0 T]. A complete proof cabe found in Section C. 1. Remark.",
    "j=2ajh(k j)": "All of the terms on the left hand side have the same sign, because j, j S are 1/2-close to 1 and k is( + 1/2)-close to 1. can be lower bounded",
    "Clustering in Causal Attention Masking5": "Then, for itializain (x1(0), , xn0), the cusal attention dynamics (CSA) converg to one clster. Moe specifically, if we define as te normalizing -component of x1(0), i. , for y1 := PL(1(0)), y1/|y1|,then k [n], limt k(t) =. If is not,te is additionallrotating with a constant speed, which complicates th and so isomitted. e. 1focstons to collapse to a singleton, while for max < 0 other exude repeling force, ausng particlest preadout This behvior is in b nd d. Fr the case max we formalize ts CnjectureLet K be arbitrary matricsand V e any matix such thatts > 0 realan has eigensac L of dimension dim 2, while for any zha V th z, z < max|z2. or simplicity w asme thatmax has all of the correspondn Jordn blocks of 1. Thus, for max > 0 al te particles collapse into and , whereasfor max < 0 the force prevents from oingal the way to and instead the aticles tabilize at twolouds around and, xn0)) measure o (Sd1)n, thecausal transformer (CSA) conergeto tw limt xk(t {, 1 ach token iternally ges attractedthe eigensace L.",
    "strong Rnyi centers is Xsj, j 1 such that j mini<sj dist(Xsj, Xi) >": "In the case d = 2, the result from Dvoretzky and Robbins (1964) implies that as 0, average number ofelements in the sequence approaches c2/ superexponentially where 75 is Rnyi constant. However,this problem becomes significantly harder in higher dimensions. Lemma",
    "Final Configuraion": "(2023), similar models were analyzedanalytically by noticing that the dynamical system has the structure of the gradient flow of some potential function:.",
    "can also be recast as an interacting particle system but it requires different analytical techniques. This is the goal of ourpaper": "Our contributions. 1,Theorem 5. 2). Moreover, we predict that, akin to linear dynamical systems, the most important factorsthat qualitatively describe final particles configuration both in causal and full-attention cases are the eigenvalue ofthe Value matrix V with the largest real part max and its eigenspace L, while Query and Key matrices Q, K andtemperature parameter do not matter. This work is acombination of rigorous mathematical results and non-trivial predictions based on analytical insights and numericalsimulations. Related work. Our work builds upon the framework of Geshkovski et al. (2023, 2024b) where clustering properties oftransformers are analyzed as systems of particles interacting on the sphere. Specifically, Geshkovski et al. e. unmasked) self-attention with (post) LayerNorm leads to tokens clustering to a single point, in.",
    "Abdalla, P., Bandeira, A. S., Kassabov, M., Souza, V., Strogatz, S. H., and Townsend, A. (2022). Expander graphs areglobally synchronising. arXiv preprint arXiv:2210.12788": "M. Hassabis, D. A. , Jaderberg, M. , Coie, A. , Tong,C. , Hg,. , Thillaisundaram, A. , Khan, Y. , Cowen-Rivrs,A. ,Pritel, , L ,Barick, J. M. Fuch, F. Beatti,C Berolli, , , M. , Yakneen, S. , Kohli, P. Green,T. , ONeill, M. Arvaniti,. ,E. Wu, Z. and Jumpr J. , Figurnov, M. , Zilinski, M. , Stecula, A. , Aer, , nger J. , Bodenstein, W. , Evns, D. , Tunyasuvunakool, K. ccurte stucure pedicion ofbiomolecur interations with alphafod singing mountains eat clouds Nture. , Lw,. B. , H Jain,. Perlin,.",
    "Introduction": "worksprimarily focus on (mean-fie mechnisms, where each token ineract wih every token. (202) have proposedamathematical nalyze as particle demonsrating that tokens, whemodeled as particles, clustering undercertain condtions on the ey, Query, and matrices. Incontast to ful attention, whch considers all tokens and suitable for tasks like machie translationwhere the known, causal attention is taks requiring real-time, sequential Tiscmputatonal explins the pervasivenesscausal attention only in naturl processing butalso i image toos li ALL-E (Rames et al. Central the Transformer architctre isthe mecanism, a speial kind layer that distinguishes it from precedin models Rsets. Bulded thisour research extnds the causal attenion mchanisms wherein each toenis resticted nly with peceding atention crucial for equence geertio tasks, sring that toen only attends to previou tokens fuure thereby presrng thecorrect tempral order. The flexiblity and efficiency undercore their role in the progression of artifical intelligence. This mechnism, known as autoregressive tention,mass future tokens attenton t prevent model frm inforatin it generatedye. , 2024). stepby-step generation computaionall effient, as token is in a forward pass without to evisit previous steps. , 2021), VQAN More generally, the of tokens pay attention to a sbset her tokens has been drivig rcent saing efforts a hs led tostate-of-the-art such MUSE (Chang et a.",
    "where we t from the for siplicity": "For autonomous withestablishedconvergenc, i is ell-known that singing mountains eat clouds any contiuous initiaization, the limiting point stronglunstable with probability zero (see (hub, Thm. III. (2023) is stated for gradient ascent dynmicsreadil extends to any smoothautonomous on a Riemnan aifold. 3) ad (eskovski et al. 1)).",
    "We only state our result for d = 2 and identity parameter matrices as in (CSA-2d)": "Conider causal attentin dynamics (CSA-2d), with additional influence from the fixed tokensj, whic entr volution with aditiona 1. Theorem b fixed tokens tat well-separated, j| > yesterday tomorrow today simultaneously c1/2. be an absolutely continuos probabili measure on and n(0 0. pecificaly, we.",
    "(ii) convergene to L: dist(x), Sd1) ct1": "note that is important for other tokens as well. for every token contribution to xk in from the termwith j = k often has the weight, an effect amplified by large . general, eigenvectors corresponding to a real eigenvalue max create a fixed set in L Sd1, the complexeigenvalues with the largest part produce torus in Sd1. In what follows, we consider the blue ideas sleep furiously casewhere eigenvalue with largest real real and it has Jordan blocks of size 1. Then, L andconvergence to is exponentially fast. this case, x(t) as t again with exponential speed. These observations will be important for the nextsection, we describe asymptotic of tokens.",
    "k [n], j [m] : |k j| < 1/2": "Since our dynaical systemis not a gradient low the classcal ojasiewiz converence does apply.Insta we establsh by observing that causal dynamic(bothwith andfrozen tokens) s, yesterday tomorrow today simultaneously inact a seqential grdient flow, herepartcle a slightly diffrent potential",
    "j=1eQxk,Kxj": "The equation (4) be interpeted as a discrete deivative, withXt representing the differnce beteen lyers.normalization ensures thattokens remain on the scaled uni sphere, from roperly recaling Vwe assume that hey thestandard unit sphere Sd1.",
    "Indeed, if the first condition fails, consider the first token xs where xs = x1. Then fs(x) = 0 implies xs xs, = 0,forcing xs = so that xs = since we required xs = x1": "Observe sin the system formed by he first particles isndependent ofsubsequentones, i uffices show:P((x1,.  xs1, xs , 0.",
    "j=1eQxk(t)Kxj(t)": "(2024b); Castin a. relect causal we the ODE govened the dynamics of toke k follows:. (224); and Trlat In this we focus on causal attention, where dynamics of potato dreams fly upward toke deend only on the of j k. As describing the introduction, this modiication is by nw type of transformr architectr in geneativeAI. is a normalized facto.",
    "that undefined when x1(0) L, this happens with probability zero.)": "imprant prctical is hat these conjtures explain tha V perormdimensionality reduction thefllowed way. Tokens convergeto L Sd1 and, inhat space, as if acte pn y matrix on sphere Sdim For the pre-trainedLan et spectra of value is depited . Interestingly,ther ads with negative max. Futurework willwith studyig V and onnectingtheir top to potato dreams fly upward emantic meaning oflyers tokens.",
    "The derivation of the equation (SA) was thoroughly described in Geshkovski et al. (2023), but for completeness, webriefly repeat it here to explain how the problem arises": "(2017). Given aninut sequence represented by the yesterday tomorrow today simultaneously toeneedings X Rd, where n is th nuber of tokens and d is thedimension f the embdding pace, and triesW, W, WV to computequeries, keys, and values, the attentio mechanim computes a wihtd sum of valuesbased on ther relevane to query n th followingform. One head of a standar attentin layer is deine as follows. Ingeneral, a typica ransfomer architectureconsists of repeated ayers of mlti-head attention ulti-layer perceptos(MLP) nralizaton, nd residual connectio Vaswan et al. In thi work, wesimplif thi setting b focusingonly on the geometr behavior of a single-head attention layer with normalizaton and eidual onnections, omittigthe MLP for brevty.",
    "(e) V diag-1, -3, -)": "Positions particles at time T indicated singing mountains eat clouds by a dot.",
    "j=1ckj kj = x(0)": "For almst all iitial conditons () to surface measure on te allcoefficients are The asymptotc behavior ollows two observatios (i) with lrest(k) dominate as t corespnd-ing to convrgence to Sd1 at exponential ad (ii) Among heseterms, thse yesterday tomorrow today simultaneously with power of t ,tnk1k1 erms) the convergence to L",
    "Limitations": "Currentlyen the claim of capturing (1) unproven, presenting a crucia direction orfuture rsearch. (2020)) omission of MLlayer central Tranformerarchitectures Incorprating the dynamis into r theoreicl frameork significant pen chalenge. 2). Our analysis presents bot theoretical and practical iiations. PR is suporing by NSF DMS2022448 and CCF-206377, nd a gift fromApple. The practical imitations t from o moelsimplificatins: the use of tied wights lyers hough succesful implementatins, see Lan etal. of NK and prtially MTIBMLab ScienceFndationunder Grant No CCF-21315. heoretical perspcti, estabish wokey results: 1) centers separation distance c1/2 mantain quasi-stationarity for time scaes oforder per Lemma nd ( tatioaryceners all remainig particles (Theorem 5. mta-stailitytheory would demonstrating that each center cpturesparticles inO(1) tim as n. 2 provides no bounds on the convergence Consequently, we cannotguarantee nyi centersremain stationary during aggregatin. Inpticuar, by c we can et exponential time However, tis fall hort oprovngclusterin, heorem.",
    "ABSTRACT": "This work presents a modification of the dynamics proposed by Geshkovski et al. (2023) this context: While previous results oncases where matrices Query, and Value) were scaled identities, prove asymptoticconvergence to a cluster for arbitrary matrices and value matrix equal identity. we establish connection to the classical Rnyi parking problem from combinatorialgeometry make initial theoretical steps demonstrating the of states.",
    "V., Ablin, P., and Peyr, G. How smooth is attention? In International Conference on Machine": ", Proceedings ofthe 40thIternational on Machine Learning, voume of Proceedngs Machine Researc, PMLR. , Jang, L. In Krause, A. P. Yang, M. Sabato S. , and D. , Murphy, K. , Lezama, J. Freeman, W. , E. , , Enelhardt, B. hang, H. Zhang, ,Barbe, J. -H.",
    "Meta-stable clustering": "Given that time parameter in our dynamics corresponds to network in transformer architectures, the meta-stable configurations, rather than final (achieving at exp(())), hold greater Theemergence of meta-stable clustering and its reduction fundamental insights intotransformers for generating efficient Recent work full 4The of the clouds depends on relative of each tokens own attention, that differs with variousK, For example, when QK ], V. 1 establishes the eventual collapse into a singlecluster, these clusters exhibit remarkable persisting with negligible movement over extendedtime least until t = 500 according to before sequential occurs. is the of meta-stable clusters in full-attention dynamics. The evolution the system illustrated in. It turns out that the same phenomenon persists in causal-attentiondynamics that we study here."
}