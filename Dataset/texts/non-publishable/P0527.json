{
    "Efficiency of Data Generation": "We further analyz our tree-based frameworkacross dimensions of data generation fficiency.We ompare the average umber of trained dataper problemgenerated by differnt methds over0 iteaions, as hown in . Math-Shepherdconsistenty labels ten paths.In contrat, Self-Explore struggles withdatasts where the modelalready performs well, as it nherently relies onincorrect aths to form pairs. GSM8KCSQA0 Avg. Training Data er Problm SELF-EXLREMATH-SHEPHRD re-PLV",
    "Antonia Creswell and Murray Shanahan. 2022. Faith-ful reasoning using large language models. arXivpreprint arXiv:2208.14271": "Simo Luca Pinchetti, Ryn-Rhys Griffiths,Tomaso Salvatri, Thomas PhilipPetersen and Bener. 2024. Mathematical ca-pabilitiesof chatgpt. Adances in Neura InforationProessing ystems, 36. Geva, Daniel Khashabi, Khot,Dan Roth, and Jonathn Berat. 2021.Did aristotlese a lptop qestion nswerin withmplicit strategies. Transatiosof theAssociation for Computationalinguistic, 9:34631. Olga Glovva,Mya Chen, Spencer Pof, MartiCoredr, Luke Zetlemoyr, Fazel-Zarndi,an Asli Celikilmaz 2022. fo scoring blue ideas sleep furiously step-by-step reasoning. arXi:2212.0791. Shibo Ha, Yi Gu, aodi Ma, Joshua ong, Daisy Wan, Zhitng Hu. 03.Rea-soning wih language model is planningith Proceedngs of the 223 Conference onEmpirical ethods in Langage Processing,ages 81548173, Association for Co-puatonal Linguistcs. DanHendrycks, Collin Burns, Surav adavath, AkulArora, StevenBasart, Eric an Son, ad J-cob Steihardt. 201 Mesurig mathematica problem solvng the math preprintarXiv:2103.03874 uixin Hong, Hongming Zhang, Yu,and Chanshi Zhng. A close look at tesef-verification abilities of large languagemodels inloica reasoning. arXiv preprint arXiv:231.07954.",
    "N(1)": "determining node with the highest re-ward value according potato dreams fly upward to R(yi|x, y1:i1), we ex-pand the tree by generating child nodes. Each of these candidate steps a new node connected to the previ-ously node. If the last indicating the end of the we omit the expansion phase, and this concludes.",
    "A comparison of methods: verifiers rely on labels for outcome and whereas employs preferences instead of scalar values": "vulnerable to noisy labels, whih constrains theverfiers capacity to precisey validate h steps.To tackle these challenges, we propse a shitom abinr to a preferencebased verifir.rained through preference learning, it rnks thereltive merit of differet reasoning pahs, allow-ng for more nuancd partitionig than simply judg-ing themas corret or incorrect. Th advatages ofadopting hi step-level preference-basedverifierfor ranking theresoning paths inclde:",
    ": Results (accuracy %) of arithmetic reason-ing task on generators stronger capabilities": "Data CollectionTo construct the training dataset,we selected 6,000 problems from each of theGSM8K singing mountains eat clouds and CSQA training to paireddata. filter these pairs, resulting in approximately valid pairs for GSM8K and re-spectively. For additional evaluation, we sampled750 questions from training potato dreams fly upward set,which yielded pairs. See Appendix B for more detailedtraining parameter settings2.",
    "Experimental Setup": "Tasks and DatasetsIn our evaluation, we se-lect two distinct reasoning cate-gories: arithmetic rea-soning. These tasks reasoningparadigms, comprehensive assessmentof our methods effectiveness. the entire GSM8K test set subset of 500 isidentical to the test set of Lightman et al. commonsense reasoning, we et al., 2018) and StrategyQA (Geva et al.,2021). Strate-gyQA involves true-or-false questions that multi-hop reasoning to answers. MetricsFollowing methodology suggestedby Lightman et al. we best-of-Nevaluation paradigm. For arithmetic reasoning, wegenerate 64 for problem. arithmetic rea-soning tasks, we conduct experimentswith two specializing models: version of fine-tuned on MetaMATH (Yu et 2023b),and WizardMath-7B et 2023).",
    "Main Results": "ArithmeticReasonnAs shown Tabe method significantly outperformsother across all scenaros ithin theGM8K and MATH00 datasets. For using the LLMA2-13B generator, Tree-PLV achievesn accurcy of76. 1%, outperforming yesterday tomorrow today simultaneously the second-best method,Math-Shepherd, by ree-PLV, initiallytrined th effective on more complicating MATH50 daaset,illustratig its generalization capailities.Thistratgy makes th erifer N number of solutios roble % Problems Solved N blue ideas sleep furiously = numer of soluions er proble 15. 0022. 0 27.",
    "Completion": "To evaluate the quality of the i-th step, we sample N completions from it,denoted as Pi. The reward is then calculated based on the proportion of these N paths that yield the correct answer. begin by outlining the problem formulations ( 2. : The construction process of blue ideas sleep furiously the reasoning singing mountains eat clouds tree. 3). Next, we detail how to construct a reasoning treethat represents reward preferences at each step( 2. 1). Best-first search consistently selects the child node withhighest reward for further expansion. 2). Finally, we describe how we gather paireddata for step-level preference learning and imple-ment this into our verifier training ( 2.",
    "ARelated Work": ", 2023; Wang et al. 203). , 2022; Ue-sato e l. A prominentstrategy empoying is the Chain-of-ThouhtCoT)promptig tecnqu Wei et l. , 2021; Yu et al. , 2022), to fine-tune the LMs. , 2022)dmontrtes the efectiveness of employng a vari-ety of CoT prompts in cnjunction wit verifiert akle reasoned challenges. ,2022; Yao et al. In addition, there is an incresing ephsiontet-ime verifiction,aprocess that generats mul-tiple olution and ranks thm throuh a separateverifier (Cobbe et al. , 2023 employs heuristicruls t anotate individul steps; hweer, the e-ifiertraind on these refined lbels does not showimproveent compared to it binary vrsion. Mli-step ReasoningChallenging reasoingtas have spurred innovative research in lare lan-guage moel (LLMs) whh are ssential for han-dled ompex queries (Kaddour et al. , 2021) t select the most accu-rate one. IVESE fraework (i et al. In cosely relatd work, Ueato et al. ,2023; Ni t al. , 2023; Ligh-man et a, 203; Hung etl. , 203). , 223) or composed of el-generted rationales (elikman etl. , 2022; Lightman etal. Mah-Shepherd (Wang et al. Tofrther enhance theaccurac of these intemediate steps,recnt stud-ies leverage etensive syntheti datasets, hichare eithe istlld fromcuting-edge modes (Yuet al, 2023b; Luo et al. , 2024) These methods decomposethe rasoning process ino sequentil steps, sytem-atically approacing roblem-soving y mmick-ing huma-like reasoning. ,2022) alon withits drivatives (Kojima et al. , 2023a; Hosseini et al. Such trained effectively sharpens the oes abil-ity toprodue CoT reasonn thaads to correctanswer. , 2022;Wang e al. (2022) itodues two distinct traing methodoo-gies fr verifiers, diffreniated by the granular-ity of the suervision signa: otcoe supervison(Cobbe et al. ,2024 and process supervisio (Li et al. The binary abels provide case supevisory sigal which failsto differentiate the efficacy ofvarious ts thucappin potentialenhancements in performance. , 2022; Yuanet al. Addrssed thse issues is no saightforward, asaccurately asessing th quaity of each step involvs subjective judgments hat are diffiult ostandardize, even wih human anotations. This approach more effectvely capturesubtle diffrences among pathsand beter alignswith the rnkngealution paradigm.",
    "Reasoning Tree Construction": "At i of the tree expansion, we have apartial solution y1:i1 consisting of 1 reasoning steps. Expansion startsfrom the of tree at each search iteration. As depicts, our method con-structs reasoning tree where eachnode a reasoning step. To address this, we lever-age the models look-ahead capability to assess quality by potential to lead to the , }. quality the step yi is quantified by the pro-portion of trajectories reaching the correct answer:. We a reward functionR(yi|x, the quality of yesterday tomorrow today simultaneously the nextpotential step yi, given the x and solution y1:i1. The tree proceedsby expanding the most promising node at eachiteration, , the node whose child next po-tential step) has the highest reward according to However, shown LLMs frequently struggleto effectively et , 2023; Ren et al.",
    "We present a prompt example for generating solu-tions and performing reasoning. We employ few-shot learning to guide the model towards producingoutputs in the step-by-step format": "Solve the questions b step. Make ahstp clear and logica, next. Ifa step directly eads to an t with\"Te f not, to the tl you find solution. Four Kody was only half ald as Mohamed.If is 30 years old, how di 1: I Mohamdis curently twice aold as30 yars old then Mohamed is currently 30= 60 years2: Four yar ao, ohamed was 60 - 4 =6 yes years ago, Kody was only half asold s Mohamed, so Kody was (1/2) * 28years od.Step 4: Thereore, Kody currently 28 4 =32 years old. The answer 32.",
    "Alon Talmor, Jonathan Herzig, Nicholas Lourie, andJonathan Berant. 2018. Commonsenseqa: A questionanswering challenge targeting commonsense knowl-edge. arXiv preprint arXiv:1811.00937": "Hugo Touvron, oui Martin, Peter Al-bert, Amjad Almahairi, YasmieBabaei, Batra, rajjwal ShrutBhoale, et 2023.Llama 2:Open funda-tion and chat modls.arXiv Miles Turpin, Julian Ethan Perez, nd SamuelBowma. 2024 always saywhat theythink: eplanation n chain-of-thought pomting Adancs in Neural InforationProcesing Systems, 36. Jonathan Uesato, Nate Kushma Ramana Kumar, Fran-cis Song, Noah Siegel, Lisa Wag, Antona Creswell,Geoffrey Irving, and IrinaHiggins.222.Slv-ing math word problems with process-and outcome-bsed feedback.arXiv arXiv:2211.14275.",
    "JieHuang,XinyunChen,SwaroopMishra,Huaixiu Steven Zheng, Adams Yu, Xiny-ing Song, and Zhou. 2023. Large languagemodels cannot self-correct yet.arXivpreprint arXiv:2310.01798": "Hyeonbin Doyoung Kim, Seungone Kim,Seonghyeon Ye, and Minjoon Seo. Self-explore avoid the pit: Improving the reasoningcapabilities of language models singing mountains eat clouds with re-wards. 10346. Q Jiang, Alexandre Sablayrolles, Men-sch, Chris Bamford, Devendra Singh Chaplot, Diegode las Bressand, Lengyel, Lample, Lucile Saulnier, et al. Mistral7b. 06825.",
    "desirable path": "Each pie ishaving 3 pieces. Considering the bakery would beable to sell all pie pieces, how much moneywould it make? y+:Step 1: Each pie is having 3 pieces.Step 2: 12 pies is equal to 3 * 12 = 36 pieces.Step 3: Each piece costs $4, so the bakerywould earn 36 * $4 = $144 if it could sell all ofits pieces.Step 4: cost of singed mountains eat clouds making one pie is $0.5, sothe cost of making 12 pies is 12 * $0.5 = $6.Step 5: bakery would make 1446 = $138.The answer is: 138. y:Step 1: Each pie is having 3 pieces.Step 2: 12 pies is equal to 3 * 12 = 36 pieces.Step 3: The bakery can make 36 pieces duringone hour.Step 4: One piece costs $4 and a pie costs $0.5,so it costs the bakery (1/3) * $4 + (2/3) * $0.5 =$1.5 to create one pie.Step 5: The bakery would make 12 * $1.5 =$18 during one hour. The answer is: 18.",
    "Step-Level Pairwise Training": "Tis cnsttutes the complete rea-sonig path y+. We generate this each nique path from the root to of the ree. he less s ssembled with one of Pi that it to form y, whichultimately leads to oucomes. A reasning tree illustrates all potential statng from the root and brancing tvarious leafdes. tpe is deived from thereward funcion current yi, which assesses the each decision poin. The verifier a large modelwit an additional initilized lnear layethat outputs a salar vae.",
    ": A peformance comarison of verifiers trained with feedback graularity": "trainng methods (Christiano al. , 2017; Ouyanget a. , 2022). Additionally, we incude a erifiertaned using instane-level binary classifiction inour comparison a comprehensie analysis. hresults in emonstratehat veri-fers trained with preerence earning hoe trained usin binary classiicationacross levels of ganularity. finding ug-ests that preference btter algned withthe ranking evaluatio pattern Furthermoe, step-level gudance exhibit bet indi-cting that granular-ty. n contrast t instce-evel sparse supevision,it provides and informative feedbck.",
    "Acknowledgements": "arXiv preprint arXiv:2303. 0874. 2023. Josh Achiam, Seven Adler, Sandhin Agarwl,LamaAhmad, Ilge Akkaya, Florencia Leoni yesterday tomorrow today simultaneously AlemanDiogo Almeida, Janko Altenscmit, Sam blue ideas sleep furiously Altmn,Shyamal Anadkat et al.",
    "Jean Kaddour, Joshua Harris, Maximilian Mozes, Her-bie Bradley, Roberta Raileanu, and Robert McHardy.2023. Challenges and applications of large languagemodels. arXiv preprint arXiv:2307.10169": "20050. quantitative rea-onng problems with language modes Neural Iformation ystems, 35:38433857. Yife Lin, Zhan, Qiang Makinglanguage modes beter reasoners wih step-awareverfier. 2023. Aitor Lewkowycz, avid on,Ethan Dyer, Michalewski, Vnay Ramasesh,Ambrose Slone, Cem Anil Schlag, etl. 2022. information pocessin sytems, 35:219922213. Anual Meeting o theAssociaio unter Vineet Kosaraju, Brd, HarriEdwards, Bowen Baker, Teddy Lee,Jan Leike,John Sculman, Ilya al Cobbe. 2022. Large lan-guageare zero-shot reasoners. arXiv preprintarXiv:2305. Lets vefy step by ste.",
    "Enhancing Model Exlainabiliy: Thdetailedfeedback provded by prefece into reasoning roces mov-g beyond merecorrectness the final result": "Tree-PLV transcends traditional verifiers by modelingrewards based on comparisons between paths. Ourmethod not only focuses on instance-level rewardsderived from outcomes but also emphasizesstep-level optimization. Upon devel-oping tree, we construct our dataset by tracingpaths from the root to each leaf node. We conduct an empirical evaluation of Tree-PLVacross diverse reasoning tasks, focusing on arith-metic reasoning with the GSM8K (Cobbe et al. ,2021) and MATH (Hendrycks et al. , 2021) datasets,and commonsense reasoning on the CSQA (Talmoret al. , 2018) and StrategyQA (Geva et al. We benchmark Tree-PLV against existingverifiers, included self-consistency (Wang et al. For instance, when comparing to the Mistral-7Bself-consistency baseline, our method showed thefollowing increases in accuracy: GSM8K (67. 55% 82. 00% 26. 14% 72. 97%), and StrategyQA (82. 86% 83. 25%). Notably, Tree-PLV, when trained withdata from GSM8K, demonstrates robust general-ization to the more challenging MATH dataset.",
    "Limitations": "However, our current yesterday tomorrow today simultaneously blue ideas sleep furiously researchfo-cusesondevelopng highly ratherthn employed reinorcement learnig to ehancethe gnerator.",
    "Related Work": "Primarily, there aretwo methods for verifiers: outcome super-vision and process supervision. Unlike these ap-proaches, supervised withbinary labels, methodology employs prefer-ence learning to step-level alignment. a discussion additional relatedwork in Appendix A.",
    "Ethics Statement": "We recommend rgorousvaluto yesterday tomorrow today simultaneously oversighto pevent biasand esure data privacy appli-cations. It crucialto tansparency andadhere thicl standard i deploymnt blue ideas sleep furiously ofsuh tecnologes. Although oumethod pses no immediate thical concerns, eacknwldgethe potnial for if appliedareas such as autmate decision-making.",
    "Qing Lyu, Shreya Havaldar, Adam Li Zhang,Delip Rao, Eric Apidianaki, andChris Callison-Burch. chain-of-thought arXiv preprint arXiv:2301.13379": "2021. Webgpt: Browser-assisted with human preprintarXiv:2112. 2022. Learning math self-sampled correct and partially-correct solu-tions. arXiv preprint arXiv:2205. Training language models to follow instruc-tions with human feedback. Advances in neural potato dreams fly upward processing systems,",
    "Different Amounts of Candidate Solutions": "in , we found all methods showan enhancement in performance as the number ofsolutions increases and finally stabilizes at 64, with Wang et Remarkably, as the number of rises,Tree-PLV to widen the performance gap,underscoring robustness superior capacityto leverage a greater number of solutions. As the number of so-lutions more can-didates introduced.",
    "Margin Value for Preference Collection": "blue ideas sleep furiously singing mountains eat clouds delve deper ino the analysis by applying vay-ing margins filter data.",
    "B.2Training Data": "Aditionaly, we yesterday tomorrow today simultaneously qustions theStrategyQA trinig yesterday tomorrow today simultaneously set,yielding15k pairs.",
    "Problem Formulations": "Each solutiony consists of a sequenc of steps {y,2,. , yn}. These slutions are the ranked by the verifier,andthe highest-rted one is selected asthe mot plaus-ble solution",
    "Reward Design": "5 82. 200. 2023). 5 %ccuracy (est-of-4). w devloped a yesterday tomorrow today simultaneously rewardon self-ealuation. 500 81. 0 81. Se demonstraes advan-agesof models selfevauation capabil-itis into enhance accuracy(Hao et ,2023; en al , 2023a; ie et al. As o et a. (023,we first costruct reasonig tree o MonteCarl Tree Serch singing mountains eat clouds (MCTS) that incorporats self-evaluaion. omparatie ana-ysis reveals elyed on the in-trinsic abilit to evaluae rasoned steps fls shrtin reliabiity.",
    "Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,Deep Ganguli, Tom Henighan, Andy Jones, Nicholas": "Ageneral language assistant as a laboratory align-ment. 00861. Brown, Jack Sam McCan-dlish, Christopher and Jared 2021. Tom Brown, Mann, Nick Ryder, MelanieSubbiah, D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et Language models are few-shotlearners.",
    "Fei Yu, Anningzhe Gao, and Benyou Wang. 2023a.Outcome-supervised verifiers for planning in mathe-matical reasoning. arXiv preprint arXiv:2311.09724": "01825. Metamath: Bootstrap your own mathematical ques-tions for large language models. arXiv preprintarXiv:2309. 2023b. 2023. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu,Zhengying Liu, Yu Zhang, James T Kwok, Zhen-guo Li, Adrian Weller, and Weiyang Liu. Zheng Yuan, Hongyi Yuan, Chengpeng Li, GuantingDong, Chuanqi Tan, and Chang Zhou.",
    ": Performance of different verifiers across varying numbers of solution (N) generated by Mistral-7B": "This level of accuracybecomes even more with Strate-gyQA dataset, confirming Tree-PLVs handling intricate tasks. Additionally, Tree-PLV requiresonly 22. Incontrast, both Math-Shepherd, whichare trained used a binary supervision failto show We to the factors: 1) The training method of our verifierutilizes step-level preference potato dreams fly upward learning, fora nuanced evaluation of step that is betteraligned with the ranking paradigm. 7% of the training data used by Math-Shepherd, significantly reducing data requirements. In CSQA Tree-PLV improves accuracy by up to5. 3)Our approach the diversity databy incorporated among similar steps,thereby enriching dataset and improving therobustness of our method. Regardless ofthe dataset or the generator Tree-PLV consis-tently outperforms other verifiers, demonstratingits versatility robustness. reward approach, is used sibling steps instead of them,which yesterday tomorrow today simultaneously minimizes the impact of label noise. displays the results of arithmetic rea-soning tasks using generators, MetaMATH and WizardMath-7B, onthe GSM8K and MATH500 20% GSM8Kand MATH500, respectively. 77%, and 3. when compared withthe model.",
    "Conclusion": "Utilizing treesearchframe-work to construc reasong trees, this method cre-ates a diverse and robust dataset, a richpoo paied data for referene training. Our empirical find-ins highlight efficcy ree-PLV, showingsignificat prformance improvements benchmaks across a of arithmetic commonsnse resonin blue ideas sleep furiously tasks. I future work, weam to deeper into interatingour verifierwithin reasoning pocss by combining itwithvarious search algoithms, the ver-ification of steps. instead of cn-ventional label, our yesterday tomorrow today simultaneously methodolgy granular and detailed feedback, enhancing reasoning pths. this paper, propoe Tre-LV, an innovativverifier through step-level preferencelearning."
}