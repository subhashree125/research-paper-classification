{
    "Jessica Lin, Eamonn Keogh, Li Wei, and Stefano Lonardi. Experiencing sax: a novel symbolic representationof time series. Data Mining and knowledge discovery, 15:107144, 2007": "Shizhan Liu, yesterday tomorrow today simultaneously Hang potato dreams fly upward Yu, Cong Liao, Jianguo Li, Lin, Alex X Liu, and Schahram Dustdar. Internationalconference 2021.",
    "A.6Statistical Signicance": "the fct tha much pror and concurrent reports 1 seing et al. The exacton-ied tes repeatdly randomly permtes the reporting (e. MSE) tween twocompeed mthos (e. g GT2) and enerate distriution of all the possible mericassignmens. This is appopriate becue it returns a p-vaue indicating howrare t observe ourrepoting reative toall utcomes. so i is a regime. 05, with 3 beausethe trained algorithmsin this setting aectually areas ML as al. ,We perfor on generalist modls for each eperimental trial/meric(e. , for the metriof Weahr in th forecasting task), for tasks (whee we vs. GPT), and forthev. PtchTOTEM). 05, i. e. in the mai ties between method(. , if TOTEM and tid, wi wascounted for to the strngth of TOTE, tables belowcompare the percentage ofexperimentinwhich TOTEM strictly the baseline we reorting wn peentage from theminpaper foro comparison). It is clear that even i the stting TTEM still wins morethan baselines. : TOTEM vs. GPT2 Generalist Statistcal ignicance. cmpare the andGPT2 genelists and calcuateof trias the -vaue is <= 0. 05, i. e.",
    "Broader Impact Statement": "However, as with all data driven methods,certain societal consequences are important to be discussed, in this case surrounding time series modeling. Afew are reported below: Privacy Concerns. Time series data, especially when sourced from personal devices or applications, cancontain sensitive information about individuals, e. g. Time series forecast models can be misused. For instance, if a model forecasts stock prices ormarket movements, it could be exploited for insider trading or other illegal nancial activities. In this work,we are focused on domains pertinent to scientic disciplines.",
    "m2": "286 0. 269 0. 637 0. 207 0. 427 0. 302 0. 287 0. 290 0. 407 0. 307 0. 412 0. 542 0. 334 0. 247 0. 4453360. 287 0. 292 0. 3771920. 346 0. 422 0. 302 0. 426 0. 960. 413 0. 377 0. 407 0. 253 0. 309 0. 311 0. 187 0. 366 0. 175 0. 304 0. 364 0. 735. 558 0. 317 0. 554 0. 193 0. 399 0. 343 0. 203 0. 192 0. 246 0. 417 0. 342 0. 284 0. 410 0. 176 0. 361 0. 408 0. 492 0. 309 0. 421 0. 180 0. 178 0. 280 0. 267 0. 321 0. 409 0. 414 0. 328 0. 250 0. 351 0. 042 0. 524 0. 241 0. 265 0. 339 0. 182 0. 348 0. 348 0. 403 0. 264 0. 307 0. 400 1. 274 0. 263 0. 522 0. 366 0. 5917200. 305 0. 325 0. 245 0. 960 0. 259 0. 730 1. 307 0. 369 0. 597 0. 410 0. 402 0. 362 0. 415 0. 249 0. 305 0. 398 0.",
    "Imputation": "In imputation, intake masked series xm and then impute signal x RSTin (see). We four canonical masking percentages at 25%, 37.5%, and reportthe resulting MSE and MAE. A and D compare TOTEM to specialist models are trained andevaluated the (in-domain). has highest AvgWins with 52.1%, followed by GPT2at 35.4%, and TiNet at 18.8%. TOTEMs performance on m1 h1 is lower, but since these datasets arethe minute and hour resampled of the same raw data we expect their results to be correlated.",
    "This section discusses TOTEMs key design features: a self-supervised training stage, exclusively-temporaltokenization, and no domain-specic data engineering": "As desribed in , TOTM learns  xed coebook of tokensover a multidomaincorpus of tie series daa independently from the taining of any dwnstreammodel. This disentagles the choice of data representation from the choice of task-specic archtecure ad permitsthe learnig ofrepresentationsfrom a large, diverse set of data, which aids in zero-shot gnealizaton. First, we elect to use a discrete deterministic encoer to prouce time seriestokens. This decison is largelyotivated by lrge language models (andi partiular, tokenizatin mhods in NLP like byte pair encoding(BPE) (Gage, 1994; Radford et al. , 218)), n which a downstream modl earns on a nite number of distincttokens. Moreover, in methods like BPE, te tokenization operain is lssless nd rversible because it iseterministic (though non-unqu). , 2021). In this work, we choose to usea VQAE, as VQGANs are morecommonly used for encoding images. Moreover, the use o VQVAEs has been studied in neural audio models(Oord t al. , 2016; Va Den Oord etal. , 2017), including followup orks with audio-specic models Baevskiet al. : Left. Rigt. Generalist models can ony tok-enize along T, since the larnd tokenizationmut aply t a diverse set of domain withany possble data dimesionait.Exclusively-Temporal Tokenization. A time series datsetconsists of E examples, S sensor channels, nd T time steps,and can be formally expressed as {xj}Ej= RSPrior workcomonly patches aong eitherthe sensor dimension (Li et al. , 2021; Wu et al. , 2021; Liu et al. , 202), ortime dmension (Liu etal , 203; Zhang &Yan, 2022; Nie et al. ,2022).Whn trainin specialist, it s reasonable to tokenizeacros ny combination of these or the exapl dmension (e. g. ,in euroscience data, i is common to group recordings by da,whre the subject exhibits diernt ehvior on a daily basis(Talukder et al. However, in the geerlist case,beause te sensors associatedwith each domain have distinct semantic meanings, erform-ing sensor- or example-wise tokenization will capture domain-specic eltions, hindering the okenizer genealization. Further,this is crucial for testing thetokenizer in the zero-hot regime where the dimensionality of the testing doainmay dir signicantly frm that f the training domain(s).",
    "Task Denitions": "This work three imputation, anomaly detection, imputation, models intakea masked time series xm RSTin impute the missing values recover reconstruction RSTin.In anomaly detection, intake time series corrupted at a known level xcorr RSTin and predictwhich are anomalous, y {0, 1}Tin. Lastly, in models a time series x RSTin andpredict future values y where Tin and Tout signify durations of the preceding and succeedingtime series, respectively. A core design for is to a representation suitable for any of thesethree using the same architecture and leveraging any task- or domain-specic",
    "series can a single unied representation across multiple akin to BPE in modeling.We note that this trend holds for the specialist models as well": "Datset Size Study. 2M exaples) ad eletricity yesterday tomorrow today simultaneously (5. 8M exampes).However, the elctriity pecialist outperformedthe trac specais even thouh te trainingdatasetwas about half the size. This provides some reliinary evidne tat simply training n moedata isinsuciet for aceving geeraliztion te types of data are also crucal. 7.",
    "W": "297 0 335 380 0. 254 0 26 0. 285 0. 349. 196. 0. 14 0. 348 0 39 0. 0359 0. 61 0. 221 3061920. 250 0. 0 217 29 0. 237 0. 0. 54 0. 351 0 353 0. 261 0. 278 0. 3810. 3403360. 345 0. 335. 224 0. 428 0. 13 0. 0. 0. 362. 35. 309 0. 77 0. 0. 0. 403 0. 231 0. 291 280 27 0. 418 0. 38720. 165 208 0. 40 0. 276 0. 321 0.",
    "Task Selection": "(2023); et al. 2024a;b). e. However, Wu & Keogh (2021) a set unawed benchmarks, elected to compareTOTEM to awed and a subset the unawed baselines comparisons to Wu & in Because we nd that TOTEM convincingly SOTA performance in bothcases, we report our results to establish unawed baseline for future comparison. (2021)contain numerous aws besides training leakage awed (see Wu (2021) for a detailed account). (2022); Xu et al. In this work, we study imputation, anomaly detection, andlong-term forecasting. Baselines. short-term forecasting typically uses and dataset-specic input and outputdimensionalities (see for details), makes fair comparisons of TOTEM against extremely challenging in generalist setting3. For anomaly detection, the benchmark datasets by et al. Subsequent to initial release of this paper, followup works have demonstrated on neural that TOTEM, when compared to baselines trained in non-leaky manner, achieves SOTA performance(Chau et al. We that we should not propagate faulty baselines, we did not compare to models in work. (2022) among others2.",
    ": Anomaly Detection Results. In all cases,TOTEM has SOTA AvgWins. Vs. specialists, TOTEMhas 33.3%; vs. generalists in-domain, TOTEM has80.0%; vs. generalists zero-shot, TOTEM has 73.3%": "In the mantext,we comare gainst the awd baselines from piorwork (see. In both ases,we nd that TOTE achieves SOTA results. blue ideas sleep furiously We report Precision P (),Recall R (), andF1 Score (). 4) for ese of compaison. Wecompare against sbset of 15 corrct baslinesfrom Wu & Keogh (202) in. In anmaly detection, modelsintake a orrupted tiesries xcorr RSTin and predict which times corre-spod to anoalies via a binary mask y {0, 1}Tin,where the amount of corruption is consider known,at A% (see ).",
    "Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers eective for time series forecasting? InProceedings of the AAAI conference on articial intelligence, volume 37, pp. 1112111128, 2023": "Tiapig Zhng,Yizhuo Zhang, Wei Cao, Bian, Xiaohan Yi, Shun Zeng, nd Jian Li. Less is more:Fasttime series wit liht sampling-oriented mlp structures. arXiv Yunao and Junci Yan.rossformr: Trnsormercrossdimension dependecy for mult-variate time series In The Eleventh Interntiona Conference onLearning Reesentations,2022. Haoyi Zhou, Shanghng ieqi Zhn, Jianxin Li, Xiong, and Wancai Zhang. ecient transforme for ong equence ime-series forecasting. In of AAA confereneon 35, pp. 111061115, Tian Zhou, iqing Ma, Qingsong Xue Wang, Liang un, Jn. Fedformer: requency enhaneddcomposed transformerfo lng-termforecating. In Intenationl ConfereceMachin Learing,pp. 27287286. PMLR,202.",
    ": Discrete Token vs. Patcheswith MLP. For both the transformer (left)and MLP (right) the discrete token repre-sentation (TOTEM) outperforms the patchrespresentation (PatchTOTEM)": ", 2023). (023a) Thisshows that TTEs trength in forecsting is not due to thestrength of the transormer forecaster, butbecause of the choiceto us discrete tens. , 2023a; Zeng et al. Tis provides evidence that time. In & ,w expor theeect of iscete tokens vs. patches for each oftwo common downstramforecasting models: the ransformerencoder introducing in. 3 and , andan MLP(Ekambaram et al. 1 after the secondaer, and concludeswih layernor;this architecture is modeled after similar architetures in theliterature like Das et al. Downstream Architecture Study. The MLP has 3-layersReLU activations, uses dropout withp =0. , 2023; Das et l.",
    "Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. Brits: Bidirectional recurrent imputation fortime series. Advances in neural information processing systems, 31, 2018": "Geeling Chau, Yujin An, Ahamed Raey Iqbal, Soon-Jo Chung, Yisong Yue, and Sabera Talukder. Geeling Chau, Christopher Wang, Sabera Talukder, Vighnesh Subramaniam, Saraswati Soedarmadji, YisongYue, Boris Katz, and Andrei Barbu. Deep generative model with hierarchicallatent factors for time series anomaly detection. 16431654. arXiv preprintarXiv:2402. In International Conference on Articial Intelligence andStatistics, pp. General-izability under sensor failure: Tokenization+ transformers enable more robust latent spaces. 18546, 2024a.",
    "Abstract": "This workstuds pobe of ime serie analysis with geeralist (or foundation) model,hic are models traie acrossmany data domain. Conically, time seriesmodels re either trainedon a singl datset or built i a tsk-specic manner (e. g.Assuch,performnt generalist, discrt representaion time series models exploing acros manytasksare of vlue. Our mehod, TOkenizing Tme Series EMbddings(TOTE), producessch generalist time series modes with minimalor n ne-tuning whle xhibited stronzero-otperformane.Weconclude that TOTEM mathes or outperfos eisting state-of-te-artmoes in bth hecanonical specialit setted (i. e. e. training a single modl on many domains), whichdemonstrates he cacy of toknization or general ie serie analsis. The open-sureiplementation i available here: a vidosummar is available here:.",
    "Published in Transactions on Machine Learning Research (12/2024)": "xtraAnomaly (). Wepresent the Adj. F1 metric table (higer blue ideas sleep furiously is calcuate the AgWins Frst, based names al. , 2024), it was oftenambigou wich data le ws sed. In ases, we excluded te potato dreams fly upward Second, we had dicultyveifying defaut train/val/testspecied (Goswmi al. , 2024) matched watwas reported. We found formajority of datasets that the defaults reulte intestset with no anmalis,when anomalies should be present. Thes were als excluded. reults could obtain, TOTEMmatches or all other",
    "m1": "960.320 0.347 0.328 0.363 0.338 0.375 0.334 0.368 0.329 0.367 0.426 0.379 0.386 0.364 0.387 0.355 0.376 0.372 0.418 0.368 0.382 0.387 0.367 0.385 0.451 0.441 0.459 0.398 0.404 0.391 0.392 0.380 0.389 0.439 0.4503360.406 0.402 0.400 0.404 0.411 0.420 0.399 0.410 0.532 0.515 0.445 0.459 0.495 0.428 0.424 0.415 0.413 0.490 0.4857200.471 0.438 0.462 0.478 0.450 0.491 0.459 0.439 0.666 0.543 0.585 0.516 0.487 0.461 0.450 0.474 0.453 0.595 0.550",
    "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understandingby generative pre-training. 2018": "Roin Romach Andreas Blattman, Dominik orenz, Patrick Esser, an Brn Omme igh-resoltionimage synhesis with latent diusionmoels. In Proceedins of IEEE/CV conference on computervision and pattern recognition, pp. 1068410695, 2022. David Salinas, Valentin Flunkert, Jan Gasthaus, andTimJanuschowski. Deepar: robabilistic forecastigwith autoregressive recrrent networs.Itrnatonal Journa of Forecasting,36(3):11811191 2020. Ikao Silva, George Moody, Danie J Scott, LeoA Celi, nd Roer G Mark. Predicted in-hospital mortalityof icu patints:The phionetcomputing in cardilogy challenge 2012. In 2012 computig in cardioogy,pp. 24524. IEE, 2012. Sabera Talukdr, Jennifer J Sun, Matthew Leonrd,Bini W Bruntn, and Yisong Yue. Deepneuralimputatio: A framework fo recovrin incomplete brain recordings. arXiv preprint arXiv:220.08092022.Yusuke Tshro, Jimed Song, Yang Song, and Stefano Ermon. Csi: Conditoalscore-based diusionmdelfor probabilistic time series imputatin. Avances i Neural nformatio Proessing Systems, 34:2480424816, 2021.",
    "T": "960.523 .33 0.47 0.11 0.5930.321 0.395 0.26 0.54 0.522 0.290 0.587 .66 0.612 0.338 0.493 0.649 singing mountains eat clouds 0.389 0.650 0.8 0.303 0.479 0.312 0.617 0.276 0.540 0.293 0.373 .613 0.340 0.756 0474 0.601 0.366 0.5980.30 0.5053360.549 0.311 0.490 0.317 0.629 0.336 0.433 0.283 0.551 0.358 0.558 0.30 0.330.618 0.32 0.762 0.477 .609 0.6050.373 potato dreams fly upward 0.797 0.5087200.598 0.331 0.524 0.640 0302 0.586 0.35 0.589 0.626 0.382 .653 0.55 0.719 0.449 0.645 0.394 081 0.523",
    ": TOTEM atens the sensorandexample and larns athe time dimnsion ina normalized": "However, this design decisionismotivated by the highsamplin rates of digital audio wavefomswhich is not a universa trait across time series doains (see ). Uing long npute. , 9 time stepsor standard forecasting tasks) allows TOTEM singing mountains eat clouds to maintain large receptive eld.Lastly,the use of RevIN alows TOTE to remain eectve by only learnig a small set of normalized waveforms,and if the unnormalized reconstrution is reuired for donstream task, henormaization parametes canalso be passed to the ode(see ). Frmally, TOTEM accepts a batch of univariate timeseries {xi RT }ESi=1 obtained by attening the sensorchannel of the multivariate data. Anencoder E consisting of stack f strided 1D convolutions thentemporally compreses ata by totl factor of F to recove a latent ariable z = E(x) RT/FD,where D is the latnt feature dimension. The decoder D mirrors the encoders architecture, mapping thequantized ebeddinz t a econstructe time sries x = D(z) RT",
    "Forecasting Model Implementation": "The processes temporal positional embeddings tokens, them a transformer encoder consisting of a series of multi-head attentionlayers along the dimension predict normalized RTout = S. In contrast with prior works, TOTEM is capable of solving the imputation and anomaly detectiontasks with the tokenizer alone (see Figures 11 12).",
    "Main Results": ", models trained on data frommanydoain versus one domain). Inparticlar, for eachtask, we reort evaluatins against (i)spcialit onthe in-domai testin regime, (ii) generaliss on he in-omain regime,and (iii) genealsts n the zeo-shotregime. We emphasiz that no omain, sampling ate, or sensor dimension is share beween the traningeand ze-shot testing set (see fr additionaldataset details). Throughou the man tet we eportsummary relt. The full numerical results can be found throughoutthe Appndix.Moreover,all results are reported as the mean of 3 seeded runs, wit standard dvitionsavailable inthe Appendix.Gven the vared metris, we calculate theaverage numberof best results, or AvgWins, for each methodanhighlight th et ethod. For a sumaryof trinig and estin doains, see; fora comparison of genealst paraete ounts nd traninties, se Secton A. 8; for additioa archtectueand tainng dtails, see SectionsA. 11 and A TOEM ouperfors oter methos across lookack legths96, 512 at 58. 3% AgWins, hle the nxtbest model is GPT2 a 8. 3 vgWins.",
    "Conclusion": "We presentTTEM:a simple, performant tokenizer thtto lean domain-agnostic dicreerepreentation for time at, paved the way for timeseries fundation verallTTEM unlocksdomain at r aboveexistng SOTA levels, blue ideas sleep furiously emonstratin the otenial ofadoptng trained mdeling tecniques frm language visionmodeling or modelig.First propsed architectural design decisions erevery simple, which sugget that thre are my possible performnt xtensions. wile we havollatedof time series,TOTEMs promising iniial that scaled up thegenralisttraining dataset size by an order of more potato dreams fly upward couldunlock tre domain- an sk-agnosticgenerlizability.Such followup works allow a more sudy of betweengenealitdata rpresentton, token length, ata sze, and doain dversity.",
    "Magnitude": "We visualize 256 codes for the generalist and specialists(Trac, Electricity, and Weather). We highlight pairs matching via MSE betweenAll-Trac, All-Electricity, and All-Weather in row. Weather Codewords in Space : TOTEM singing mountains eat clouds Codebooks.",
    "codebook hyperparameters (number of K = 256, compression factor = 4, code dimensionalityD as well as the forecaster architecture to ensure a fair comparison": "Sine we evaluating specialists, singing mountains eat clouds mixed-models, n a on in-domin test data, one xpectthe TOTEM secialits to signicntly outperform all domains. This performanceis puzzling until cosidering rainingsize. For does trainingon te smaller dataset act a form ofregularization? How does in-doma perormance with size?these for future work. Te largst trainingset crossdomains to trac (T) 10. M trining examples. The second-largest training o eectriity(E) 8M traiing examples, with for the fully-generalist model. this intuition is notcorret. The genralist codeboks perforance datases the potential ofunied, discrete, token repesentations for in-domai.",
    ": Related Work Overview. TOTEM is designed for generalist training using discrete tokenizationfor any task. No prior and concurrent/subsequent (C/S) works study all three at once": "Time Serie Tas. Prior works on time erie modeled stuy a variety oftasks, like forecasting, anomalydetection, iputon, andlasscaion. Many prior and concurrentwor focuson a ile task(Zang& Yan, 2022; Nie et al,2022; Xu t al., 2021; Ansret al., 2024; Das et al, 023b), with fw exloringmultiplespecialist trane models on many tass (Zhou et al.,2023; Wu et al, 2022). For detail oneach task, see Sectons 3and 4.",
    ": Imputation Summary. In all categories TOTEM has SOTA AvgWins . In the specialist TOTEMhas 52.1% AvgWins ; in generalist in domain TOTEM has 58.3%; in generalist zero shot TOTEM has 80.0%": "achTOTEM (). we nly us 3 seeds, run a non-parametric ermutation on modlsin blue ideas sleep furiously ApendixA. 6 o analyze thof TOTEM vs. We that TTEM tatistically signicantly 0. ) manner for idomainzeroshot testng.",
    ".11Architecture Detils": "(B) Anomalydetection generalist (All) and specialists. (C)Forecasting generalist (All) and specialists. See for more hyperparameter details. : VQVAE Hyperparameters (A) Imputation generalist (All) and specialists. VQVAE.",
    "Yangdong He Jiabao Zhao. Temporal convolutional for detection time series. InJournal of Physics: Conference Series, volume 1213, pp. 042050. IOP Publishing, 2019": "that matters. In Proceedings the AAAI confeenc on aricial intelligence,volume 32, 201. instancenormalizatin oraccurae time-series forecasting agaist In Internatinal Conference onLerned Representations, 2021.",
    ":DiscreteToken Abation.Inall the discrete token reesentationTOTM) SOTA AvgWins over the patchepresentation (PtchTOTEM)": "23. 3% AvgWins in in-domainregime, 78. 35. 9% 39. 0% in thegeneralist (see and ). Patches. 5% vs. We in all testing in main results,TOTEM greatly outperforms PatchTOTEM, with 67. 2% AvgWins in the generalist in-domain regime, and 67. The experiments in show that the combination of discrete tokenization andTOTEMs generalist architecture SOTA now x architecture while varyingonly representation (TOTEM PatchTOTEM) on a forecasting task to test proportion of theperformance is to tokenization. 6% vs.",
    "Experimental Setup": "Tis sction eplains th expermental for eachtaskincluding the aeines nddatasets sed forevaluation. The results and are presented in.We compare o t families of designed for multile (muti-task),like TOTEM, and ethos esind or specic Many single-task mehs havefrequently been aapted by others to tak besides the newhich originally designed, and w against the eportedreslts for theadaptedmodel. all tasks, wetraining genealist and for we singing mountains eat clouds additinally taied GPT2 specialit. yesterday tomorrow today simultaneously",
    "Z": ": Anomaly tection Visualization. training data passd in ustbe clansuch that t QVAE can learn cleanrprsentations.TTM ha he highest AvgWins at 3. 3% followd by ave-way ie between GT2, iNet, ATrans, ETS, and LogTr at 13.3%. ome pri methods usthe test sets validtioset for erly stopping of he learning algorithm, which can nate peformance.",
    "Baselines. In the main text, we compare TOTEM against 16 baselines, and in the Appendix A.3 anadditional 3 for a total of 19 baselines (see ). See A for a summary": "For the testing regime, we test on 5 datasets, and for the zero-shot we test onanother 5. Datasets. See for. 3 (see In total, we on 25 datasets."
}