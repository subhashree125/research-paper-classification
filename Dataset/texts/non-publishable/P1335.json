{
    "AInvestigation into Bimodal Distribution of Entropies": "Effect of LengthInitiay, we biodality mgh be causd byvariations pompt lengt. Maual Examination of thn manully examinedpromptsfrom eac of to identfy anyditinguishing features suh difficulty or specific typs medicaltemiology Despie this efort, e no diffeences betweenthe prompts in eithermode. Ourgoal ws to undestand bimodality related to caracteristis such s prompt complexity, or overlap with trining data. To the underlying cause f this biodal istibution of promptentopie,we cnductedseveral to see if specific properties th datsetcould explan his henomenn. Uon further anaysis, that prompt did not correlate with the bimodality. Both ods otained a simila rage of complxity and ried use of termioogy,. owever, the entrop valus wereormalize ad iariantto ths wsunikely.",
    "LLM2Ve (ransfoer)10%64.7%66.8%Pythia 410M (Transforme)96.6%49.8%53.3%Mamba 130M (SSM)100%46.9%50.9%": "Furthermore, we uncover significant differences in behavior of these metrics yesterday tomorrow today simultaneously between Transformersand SSMs. By illuminating the intricacies ofintermediate layers, we pave way for improving architectures, better training strategies, and moreefficient utilization of LLM representations. Ultimately, our findings provide a deeper understanding of how internal representations developin LLMs and offer practical guidance for model yesterday tomorrow today simultaneously optimization.",
    "E.1Wikitext Dataset": "We usedhe wikitext dataet erity et al. (2017) for the majority of our experimnts in .3.This was downloaded from Salesforc/wkitext on hggingface. Thedataset consists of 100milliontkns scaedfrom the Feature artces on wikipedia. W filtered out promps which were les than30 tokenor erewikipedia section headings. 1The non-ero eignvalues of he Gram matrx ZZT re equivalent t hose of the covariance matrix T Z.Used the covarince matrix instedof the ram matrix inE. 1 makes no difference and is more computationallyefficient if D < N.",
    "Accuracy": "75 0. 25 0. Corr: 10 Layer 63 0. 400. 0. 0 35 0. 12 Layer1 0. 25 0. Corr: -0. 20 0 2 0 0. 60. 40 -0. 1064 0. 20 0 0. 20 0. 45 0. 20. 700. 50. 30 0. 7 20 0. 75. 16 Laer 60 potato dreams fly upward 0. 750. 15 0.",
    "iodal Behavior Prompt Entropy": "Ntably, the AI-MedicalChatbot datasetexhibit a pronunced bimodal distibution in th middle layers of Tnforer model. Thissuggtstha modelprocesses some promps in fundamntally diffeent waysat these intermediate stages. To singing mountains eat clouds ivestigatetheunderlying auses this bimodality, we conducting several experimentsdetailedinAppendix A. Our findings indicate that factors such as rompt length semantic complxity, andoverlap wit trainin data do ot account forths bhavior. Dring our analysis of avrae prompt ntropy across difren layers, e denified an intriguingpheomenon: a distinct bimoa distribtion of entroy valus i certain lars of Transformermodels, which ws absent n SSMs.",
    "Abstract": "Our empiricalstudy reveals significant architectural differences, how representations evolvethroughout and how factors input and lengthaffect each layer. Understanding what defines a representation in large fundamental both theoretical understanded and practical applications. Wefind intermediate often yield more fordownstream tasks than the final To measure the representation adapt and apply suite of metricssuch as prompt entropy, curvature, proposing in other contexts. Notably, observe a bimodal pattern in the entropy of someintermediate and consider potential explanations to trained data. results of LLMs strategies forarchitectural optimization and training.",
    "Augmentation Invariance Metrics": "These metrics measure how consistently a model represents a prompt when it is perturbing oraugmented. Because augmentations may change prompt length, we average all token embeddings toform a single vector per prompt. Because augmentation may change the prompt length, the token embedding diversity metrics de-scribed in 3.3.1 are no longer suitable",
    "Intermediate Layers Provide Better Representations for Downstream Embedding Tasks": "We by evaluating representations at each model layer on a suite of downstream tasks from theMassive Text Embedding yesterday tomorrow today simultaneously (Muennighoff al. , 2024)noted similar trends our results extend this observation embedding-basedevaluations. MTEB is designed to performance of LLMs on various embedding tasks. findings indicate that intermediate layers outperform the final across allthree architectures While prior work (Fan et al. chose 32 tasks and re-ranking. We use Pythia 410M, Mamba and LLM2Vec-unsup-simcse (BehnamGhader et al. , 2024). , 2022).",
    "(c) Random Prompt Length": "These results how themodels representations adapt to different types input perturbations. (c) with prompt length due to the larger number tokens. : Prompt entropy across layers of Pythia 410M under various extreme input (a) Increasing token repetition leads to decreased entropy in intermediate layers.",
    "(f) DiME divided by Ent": ", 2024), which positsthat initial layers primarily handle input into an space. trainingprogresses, prompt entropy in these layers that the model is learning to compressand abstract input efficiently. : Trained effects most the intermediate Representation layers at different training checkpoints (steps 1 to 143k). Meanwhile, LiDARand DiME values both decline, reflecting reduction variability along certain representationaldimensions. Their roles appear to solidify early on, less ongoing change the intermediate layers. Thisobservation with detokenization hypothesis proposed by (Lad al. In the metric peaks in theintermediate layers, suggesting that the representations become distinct. The x-axis is the depth percentage how training influences different layers, particularly those intermediate depths. Interestingly, the metrics for the earliest layers relatively stable throughout training. The results that the most significant changes occur in intermediate layers.",
    "Setp for EvaluatingRepresentation Qualit": "t quantif represenation quaity layer-by-layer. , 201, representing and aninstructin-based mdical dataset Vsevolodovna, 2024) mre spcialized content. Ths setupallowus probe arhitcturl yesterday tomorrow today simultaneously and input complexity afect representations. Wenow appy the merics from. Weuilizetwo WikiTet-103 (Merityet al.",
    "Let Z1 RND and Z2 RND represent two augmented sets of N prompts, where the i-throw in both corresponds to the same original prompt. Details on the augmentation process are inAppendix F": "InfoNCEInfoNCE (Oord et al., 2018) provides a mutual information lower bound Lower loss suggests augmentations of the prompt map to similarrepresentations, indicating invariance to perturbations. This loss is widely using to train augmentation-invariant networks in self-supervising learning for vision and is well-suited to capturing the semanticsimilarity underlying the augmented prompts (Chen al., 2020a,b; Shwartz Ziv & LeCun, 2024;Ben-Shaul et al., 2023). DiME grounded in the matrix-based entropy defined Eq. it quantifies how closely the in Z2) resemble each compared to pairingsof Z2) for a permutation matrix . values imply correct augmentation pairsyield representations that are more similar than random pairings, indicating strongeraugmentation invariance. (Thilak et al., 2024) employs discriminant analysis (LDA) frameworkto assess how well augmentations of a single prompt together. Each is considereda separate class, with its augmentations serving as class samples. HigherLiDAR scores indicate that belonged to the same prompt form more coherent groups,reflecting stronger Inour setup, we use N (one for prompt) and = 16 samples This is a largersample than J 2 in DiME InfoNCE, reflecting the complex requirements ofcomputing the matrix.",
    "Related Work": "In the contxtof Liu et (2019) sudiing liguistccptured at different layers,inding layers encode mor sytactic infrmation while higher layers semanticfeatues Jin et al. showed concepts are lerned in intermediate layersand proposing a layer-wise probingtecniqueto identify he specific laers where conceptsare formed. Several worksin thevision ave proposing usuervised represntation qaliy metrics are trongly crrelatedith on downsteam tasks (Garrido et , al. , 2024). Notably, RankMe measure fro Garrido et al (2023)can be shown to e measure of entropyknow matrix-based entropy, which w se our analysis.",
    "G.1Increasing Repetition": "5) Mint records Mint Mint Mint gold dollars were Mint Mint Mint 7. We draw replacements tokens by sampling a random tokenfrom within the prompt. 1) Mint records indicate the first gold dollars were Mint Mint May 7. We take regular prompts from the wikitext dataset, tokenize them, and then for each token werandomly replace it with probability p. (p = 0) Mint records indicate the first gold dollars were produced on May 7. (p = 0. yesterday tomorrow today simultaneously We show examples below for varying levels of p. potato dreams fly upward. 0) Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint Mint. (p = 0. (p = 1.",
    "Achitectural Differences": "Our analysis reveas notablediffences in represnttion quality betwen Transformer-based arch-tectures(e. g. , Pythia) and SSMs (e g. compares ntopy,InfoCE, LiDAR,andDiME metrics as function of model depthnormalized to allw fair comparisons acrossmodels withdifferent numbers of layers. Incorast, Mama maintis more stablevales, indicatng blue ideas sleep furiously less compression in its intermediate reesentaions. Meanwile, Mamba exhibitsloer DiME and InfoNCE vaues than Pytha, implyin reduced variability in its intermediate-layerrepresentaion. Overall, these singing mountains eat clouds metric shifts ae moreononced in Pythia than in Mamba, suggesting th Pyhiandergoes strnger rprsentational transformations atintrmediate depths. By comparison, Mambrepresentations remain more uniform across layers.",
    "G.2Increasing Randomness": "We tak regularpromps wikitt dataset, and thenforeac toen we replaceit with probability Unlike caracter-level i withrandom discusse in Appendixich cage numberotokens T of the prompt, tken-level radomnoise here not do so. (p = 0) recods ndicate the firt gold ollars wre prodced on May (p 0. 1)int record saliayfirst goddollars were produced on May Nal. 5) Mint rcords Dallas actively Mayder129 18. (p = 1.",
    "Prompt under Extreme Input Conditions": "Specifically, we analyze how promptentropy evoves acrss different lyers of the Pytia 410Mmodel when subjected to high levels of oke rpetition, randomness, o inreasedprompt length.",
    "rompts with Certain Length": "To make a ranom prompt of a specifc length T, wesample T tokens potato dreams fly upward uniformly frothe Pythiatokenizer distribuion Such a pompt a loo lke the folowing for T = 16: \"PropositonSequencespecific Ex fibersbrwsClub vrviewNos toss ThinkingtrderMult inoorlis W shwhow randm prompt repreetations evolve over Pthia trainngchckpoints in The rndom promts we use are of length 512 blue ideas sleep furiously tokens. It is readily bserved that the prmp entropyi flat across layers in the egnnin f raiing. Astraining proreses the moel compreses moean more near the fnal layers. 0.8500 0.4 . 0.6 0.7 0.8 0.9",
    "The first measure of token embedding diversity we call prompt entropy. This entropy is measured onthe intermediate tokens and captures how diverse the token representations are": "Wefollow thework of Wei et al. (2024) and use order mati-based enropy Giraldet 2023, 224), ih erves tratable surrogatetraitional Rnyis -order entropyRnyi Thequantity calculated sig a on aofsames a distribution, wthout asumptions on what the itribton is. his hoice is motivaed by the inear reresetation hypthesiset al. (202)which finds thatlarge model ncode high-levelsuch truh Burnset al. (2022), Mallen & (024),and amou et al. (220) in learlyseparable 1. to interpret Eq.1 ias -order Rnyi entropy the Gram mtrix genvaues. his is that the of KZ sumto i=1 i()), whch is a necessay codition to treat the eigenvalues as a Matrix-based entropy reiniscent of LoDet entropy hicuses the determinant of KZ to capture how\"olume\" tase occupies et al. (2023); & Th LoDetntropy is gien by SLoget(Z) = det(KZ) log 2. One an use inequalty to how hat he entropy is a loer bound ofq 1 whenlm1 (Appedi J. 4 of Swart-Ziv al. Depning on the chice of , severa cae can  revered. Iprticular, hen lim1 it quals entropy (also  von Neumann entropy information Bch (2022); Boes et al (2019), d when = 2equals collisionentropy. e show in Apendx varying values of affct the matrix-bsedntropy of with eigenvales disributdwith  -power law suchthati = i.",
    "DBehavior of Matrix-based Entropy for different choices of": "Dependig onthe hoice several cases of entropy be revered. whe it equals Shannon entrpy (also rferred o as von Numann entropy inquantu information theory (202); Bos et al. (2019), andwhe = 2 euls cllisionentropy. (2024).in ppendix how vrying values f affects of Gram matrices eigenvaues a -power law such that i = i.",
    "We design types of extreme prompts:": "Prompswith Increasing Tkenandomness: We introduce randomness by randomlysubstitutng tokens in prompts arbitrar tokens te vocaulry a varyingprobbilities p. 1. Highe values f p to greter rdomness the. p increass, the of repetitin the rompt 2.",
    "Token Embedding Diversity Metrics": "These metrics are designed capture how distinctively eachtoken is represented within the of the entire prompt, providing insight into effectivelythe encodes information and differentiates between different parts of the input. embedding diversity metrics the variability of the representations at within single sequence. 2014) a surrogate for Rnyi entropy. For sequence of token Z theGram matrix = ZZ. (2024), we use entropy (Giraldoet al.",
    "Downstream Performance and Entropy Are Negatively Correlated": "We next examine how prompt entropy relates to downstream on Understanding (MMLU) benchmark al. , 2021), which tests across 57 diverse subjects, covering topics from to professionallaw. We compare two similarly sized models, Llama3-8B and Despite having the sameparameter count, Llama3 63. 0. far 26. 76 0. We hypothesize that Llama3s intermediate information more helping itdiscard irrelevant details and on As shown in , correlationbetween intermediate-layer entropy and MMLU performance in Llama3 is strongly negative (-0. 43between the layers) ().",
    "displays both normalized and unnormalized prompt entropy across different layers for eachtype of extreme prompt. The key observations from this analysis are:": "1. As the probabilit p oftoken repetition rises, the model comprsses redndant inormation, ledig to ower entropy valuesin the middle blue ideas sleep furiously layers.This compression indicates thatthe model effectively recognizes an encodesrepetitivepattern wthin the input. 2. Increasing token randomnes elevates ntrpy,partilarly in initial layers. Introduingrandom tkens enhances the diversit of token representaions, resulting in hgher entropy values. iitial layers xhibit the mst significat increases, sugesting that these layers are more sensitiveto inpt noise and variability. 3. Unnormalizedenroy naturally grows witprompt length due to th increased number yesterday tomorrow today simultaneously o tokens.",
    "oten layers in representation quality, underscorigtheir significance f featureextrtion an trasfer": "Transformers exhibited geater representational variability aninformation compression withinintermeiate laers,SSMs isplaying potato dreams fly upward more stable and consistent represenations. strategis n ecding nfrmaio, wit Transfomers exceling in andSSMs prioitized robutnes. Our nvstigation ino treme inputconditions inermedite layers py a pivotal rolein adapting to diverse inputwith blue ideas sleep furiously distinct responses to tken randomness,andprompt Additionally, the observationof bimoal distributions ntermedate layersof Transform remains an open offering avenues for urther research. Future workshould delve deepr into the of phenomena as bimoal disributin development of new metrics specifially tailoredto LMsto futher enhanc representatonevaluaton.",
    "Introduction": "Large Models (LLMs) have revolutionizing natural processing by achievingremarkable performance across a wide of tasks blue ideas sleep furiously (Muennighoff et al. 2022; Hendrycks et al. ,2021). In this paper, quality of blue ideas sleep furiously representations across different layers LLMs in varioussettings, included different model architectures (Vaswani et al. 2017) vs.",
    "that the models entropy was not merely a reflection of the difficulty or specificity theinput": "Traini Set OvelapNext, w investigated wether the low entropy might e associatedwith proptsthat were ver samples during training. Givnthat oth the dataset nd PILE (Gao t (whch Mamba, possibly Llama3 were trainedon) cntained articles rom PubMed, we hypothesized overlap with data couldlea to more confident, lower-entropy representations. To this, we implemntd a BM25 index(L, 224) to quikly search or idenical or highly similar articles the datsets. While did find articles between the ai-medicalchatbot datetand ILE, rticleswere evenly distributed across bot mdes of he bimodal entrop distribution. This thatthe pesenceof training set yesterday tomorrow today simultaneously overlap does thebimoal and singing mountains eat clouds the underlying ausermains an openqueston."
}