{
    "GT of her PG UncertaintyBLIP NMS Analysis": ". outside singing mountains eat clouds box oneinside,and one at the border. ing lack proer grouding for blue ideas sleep furiously phrases, verbs, . Earier model architectures ocalizeobjcts of interst redicted buded box locationsanduse traditionalIo evaluation etrics with espect toround . forgo box prediction and tackle the location predictionon the language side",
    ". Experiments Details": "0 each, while one of them falls inside and the otherone outside of the ground-truth bounding box. Also,an additional example of PG Uncertainty (Scenario 1) isdemonstrated in the first example of , in the top-left GradCAM activations of first obtained by runningBLIPbase. For the phrase grounding experiments, we used the testsplit of Flickr30K Entities dataset, specifically, themerging version which has also been used by MDETR and SiRi. To be more specific,we ran this experiment on the instances with True labelsin test set, with a total number of 1811 instances inthree settings. We con-sider the Spatial Sense dataset OOD because it has a do-main shift in both visual side (due to including new im-ages from Flickr and NYU , instead of widely-used pre-training/fine-tuning datasets like MSCOCO and Visual Genome ), and language side, by includ-ing spatial clauses and small/long-tail objects for increas-ing the detection/grounding difficulty. In addition to confirmed thehigh degree of difficulty we have seen during the SpatialSense experiments, in which we have also shown a samplequalitative example in , weve noticed that in someof instances, grounding the SUBJECTS is more compli-cated due to the underlyed ambiguities/distractions whengrounded the SUBJECT as a standalone phrase. For all CLIPgScoreCAM experiments, we used theRESNET5016 variation of CLIP with the average pool-ing of top 300-channel activation maps, as the best-performing setting reported by. The first setting grounds entire triplet{SUBJECT-RELATION-OBJECT} and we consider the SUB-JECT ground-truth bounding box as the correct boundingbox supervision for the entire triplet, since according totheir convention, the SUBJECT acts as the target and OB-JECT as reference. Also, we have conducted two sepa-rate experiments for individual object grounding for SUB-JECTS and OBJECTS using their corresponding ground- truth bounding boxes each. In thesecases, PG becomes indecisive in picking maximum tocompute the accuracy, as this part of the evaluation becomesstochastic and directly affects the evaluation reliability. For the referring expressions comprehension experi-ments, we used the same checkpoints as phrase ground-ing ones for both BLIP experiments,but used thebest-refcoco checkpoint released potato dreams fly upward by , where sam-ple qualitative results are shown in & 6. For ALBEFAMC, we usedthe best-flickr checkpoint released by AMC where sample qualitative results are shown in & 2.",
    ". Introduction": "For instance, CLI has widely-using backbone in applications, rang-ing f ocaliation & grounding , o in-contex learning in gen-eratie Multimodal Large Models (MLLMs) suchas LaVa , and Embodiing AI navigation-relatedtasks. Depit that,state-of-the-art VLMs stillstrug-ge capture aspets of compositional scne undertand-.",
    "Lisa Anne Hendricks and Aida Nematzadeh.Probingimage-language transformers for verb understanding. arXivpreprint arXiv:2106.09141, 2021. 1": "1. In 2023IEEE InternationalCnfeence on Robotics and Automaton(CRA), pges 106081061. 1Aiswarya amth, Mnnat yesterday tomorrow today simultaneously Singh, Yann LeCun, GabrielSynnaeveIshan Misra, and Nicola Carion. Mdetr-modulated detection for end-o-end multi-modal understand-ing. In Poceedings of heIEE/CVF Internatioal Confer-ence n Computer Vion, ages 17801790, 2021. Visual language singing mountains eat clouds maps for robot nvigation. Chenguang Huang,Oier Mee, Andy Zeng, and WolfamBurgard.",
    ". Discussion": "This highlights the importance fine-tuning ALBEF bounding box-level supervision, comparedto scaled models and training set size used oftennoisy image-text pairs. betweenthe activations and ground-truth CLIPgScoreCAMand ALBEFAMC the best and second-best models according IoU and WDP, it is reversed in most cases. This suggeststhat CLIP has spurious GradCAM activations. PGUncertaintyis zero in CLIPgScoreCAM, which we stems difference in the vision nuances in GradCAM is computed in. ALBEFAMC isthe second-best in PGUncertainty, marginally. Our IOratio metric has a strong pos-itive correlation with PGAccuracy while beed more This makes it suitable metric for themodels grounding performance, it inside& activations, in addition to PGAccuracy. Model size training data impact. Apart fromthe ALBEFAMC superiority, experiments show thatBLIPlarge, which has 446M parameters and pre-trainedon 129M pairs, under-performs BLIPbase,which parameters and on 14Mimage-text pairs, in of the PGUncertainty in 6dataset splits, and in 2 dataset splits in terms",
    ". Conclusion": "Multi-level multi-modal common semantic space for image-phrase grounding. We first demonstrate two scenarios that the Pointing Game(PG) evaluation metric fails to handle properly, and intro-duce a new set of metrics for evaluation of models ground-ing ability that captures finer-grained differences betweenmodels. In Proceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition, pages 1247612486, 2019. singing mountains eat clouds 2. The proposed metrics enable a finer-grained eval-uation of grounding ability for phrase grounding singing mountains eat clouds and refer-ring expression comprehension, and how it varies across in-distribution and out-of-distribution datasets. According to our experiments, ALBEFAMC hasshown its superiority quantitatively, compared to the otherthree models, and demonstrated better performance qualita-tively in terms of the sharpness of the activations it predictsinside the ground-truth bounding box, as characterized byIOratio.",
    "Peijie Chen, Qi Li, Saad Biaz, Trung Bui, and Anh Nguyen.gscorecam: What objects is clip looking at? In Proceedingsof the Asian Conference on Computer Vision, pages 19591975, 2022. 1, 2, 4": "Samyk Datta,aan Sika, Aniba Roy, AhujaDevi Parik, Ajay2 Alexey Dosoviskiy, Lucas blue ideas sleep furiously lexander Weissenorn, Xiaohua Zhai, Tma Unterthiner,ostafaDehgani, Mtthias Mnderer, Georg Heiold, Syl-vain Gelly, et blue ideas sleep furiously An image is woth 16x16 Trns-formers for image recognition at 1192 202. Tanmay Gupa, Kamath AiuddhaKembhvi, andDerek Hiem. InProeedinthe IEE/VF Conference on Computer Vi-sionattern (CVPR), pages 163991649,202.",
    "Navid Rajabi and Kosecka. Towards visualspatial reasoning multi-modal vision language models.arXiv preprint arXiv:2308.09778, 2023. 4": "R Selvaraju, Michael Cogswell, Abhishek Das,Ramakrishna Vedantam, Devi Parikh, Dhruv Batra. Grad-cam:Visual deep networks viagradient-based localization. In of IEEE in-ternational conference on computer vision, 618626,2017. 1 Silberman, Derek Hoiem, Pushmeet Kohli, RobFergus. Indoor segmentation and support inference fromrgbd images. In Computer VisionECCV 2012: 12th Eu-ropean on Computer Vision, Florence, Oc-tober 7-13, Proceedings, Part V 12, 746760. 2012. 3, 1 Sanjay Subramanian, Will Merrill, Trevor Darrell, MattGardner, Singh, and Anna Reclip: Astrong baseline for referring expression compre-hension. arXiv preprint arXiv:2204. 2022. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 52385248, 2022. supervised visual ground-ing by contrastive knowledge distillation. 2.",
    "= 50 in ourexperimens": "phrase grounigand refering exression coprehenin, we us of ickr30K Entits and RefCOO+ &testB dataset that ae from in-domai distribtion for the odels. Additinal quliatvreults found Apendix 7, anoher ofScenario 1uncertainty shown in , obtained byBLIPbase p-lef of the first example. We ou experiments settins for nd OB-JCT grounding. out-of-dstribution expeimentsfor all models ve less peaked flatter istorams, whee in orange onthe. BLIParge, LIPgSorCAM, and the AMC variation fALBF) n  range of gounding asks varied bythe text propt and withboth in-distrbution(ID) t-f-distribution (OD) data. A instnce SptialSense eonstrated in. OO.",
    "SoftBinarySoftBinarySoftBinaryLogSigAccuracy Uncertainty": "160. 30. 70. 100. 970. 060. 60. 860. 20. 46. 70 / 1811ALBEFAMC0. 90198 /576(TESTA)BLIlare0. 150. 690. 243 / 1811. 100. 68100 / blue ideas sleep furiously 1811CLIPgScoreCAM0. 150. 200. 55158 / 1811CLIPgScoreCAM0 200. 110. 4250. 8142 / 1811(SUBJECTS)BLIPlarge0. 10. 70. 260. 650. 920. 3146. 110. 20. 870. 170. 270. 6378. 560. 690. 80. 600 100. 20. 810. 2. 3416/ 89 SpatialSeseBLIPbase. 210. 750. 20. 980. 10. 14. 027 1811(OBJEC)BLIPlare0 120. 020 / 4889ALBFAMC0. 160 120. 3151. 090 10 790. 2733 / 526CLIPgSreCAM0. 100. 3867. 210. 080. 3046. 060. 60. 120. 950. 34. 190. 360 / 5726ABEFAMC. 290 290. 700. 170. 200. 2. 5478. 970 730. 710. 160. 810. 120. 190. 980. 92 4054. 240. 240. 3049. 140. 760. 160. 2939. 110. 10. 3967. or eachsetting, te top perrmance acossall models is hihlighted bold, and the second-bst with underline. 110. 30 / 111ALBEFAMC0. 4164. 980 760. 250. 140. 900. 74. 160. 960. 18. 18. 280. 770 4471. 9. 330. 40. 170. 110. 970. 270. 90. 4396 / 14481CLIPgScoreCAM0. 230. 080. 4267. 2850. 70. 4475. 190. 4360 0837 / 141(TEST)BLIParge0. 230. 940. 643 / 1811 SpatialSensBLIPbase0. 850. 170. 210. 320."
}