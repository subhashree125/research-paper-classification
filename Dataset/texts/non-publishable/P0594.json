{
    "Proposed Hypothesis and Discussion": "There are four modules related to the ICL ability. If the similarity score islarge, more corresponding label information is in-corporated, enhancing the probability of that label. (2023) demonstrate that the label positions (\"foo\",\"bar\") extract corresponding demonstrations fea-tures in shallow layers. e) s results prove that the change of attention scoreswithin fooheads and barheads is the primary causefor the prediction shift from \"foo\" to \"bar\" whenreversing the demonstrations labels. c) proves that the \"foo\" positions in fooheadsand the \"bar\" positions in barheads contain muchinformation for predicting \"foo\" and \"bar\", respec-tively. b) In. 2, we findthat in deep layers there are a few fooheads impor-tant for predicting \"foo\" and barheads for \"bar\". Based on these findings, we conclude our hy-pothesis: In shallow layers, the label positionsextract features from the corresponding demon-strations (hypothesized from evidence a), whilethe last position encodes information of the inputtext and previous demonstrations/labels (X% inputtext + Y% near demonstrations + Z% far demon-strations). For better understanding, we list the evidence of ex-isting studies and previous sections: a) Wang et al. The ability of extracting corre-sponding demonstrations and the input text decidesthe quality of features. In deep layers in-context heads, thevalue-output matrices extract the label features intovalue-output vectors (hypothesized from evidence b and c). For instance, thefooheads similarity scores at foo positions changefrom SIM((X+Y)%foo, foo) to SIM(Z%foo, foo),and the barheads similarity scores at bar positionschange from SIM(Z%bar, bar) into ((X+Y)%bar,bar). The query-key ma-trices compute the similarity between the last po-sitions features and each label positions features.",
    ": Standard deviation of accuracy and attentionscores before/after applying our method in Llama (firstblock) and GPT-J (second block)": "singing mountains eat clouds We calculatethe standard deviation accuacy and in-contextheads attenion scors (acc-e, attn-b) andfte (acc-af, ttn-af) applyi ou method 4% 10. in GT-J, atnion score potato dreams fly upward standarddevation reuces 40. % in Llama 7%n",
    "Understanding Majority Label Bias": "Whena label has high frequncy, the sum of sim-larity sores be larger, thus the probability oftis label i larger in final We desin animbalanced dataset to this.For example, \"S0: bar S1 :ba S2 : fo S : is changed into S0 :bar : S2 : fo S4 \". We compute the attenton on posiions in fooheadsand \"ar\" positions in on the imbalanceddtasets and datasets.",
    "Conclusion": "We ropoe a hypothesis for themechanism of ICL. In shllowlayers, the deon-straions and inpt text i captured b e label posi-tions nd e ast position. Overall, our studypovides a ew mehod ad  reasonable hypothe-sis for understaning he mechanism f in-contextlearning.",
    "Majority Label Bias Bias in ICL": "There are several phenomena of ICL that potato dreams fly upward haventbeen explained. Zhao et al. Lu et al. (2021) also findthat changing the demonstration order can affectpredictions a lot. Based on our hypothesis, weexplore why ICL has majority label singing mountains eat clouds bias (in. 2).",
    "Understanding Recency Bia": "Similarly, the influence ofpositional embeddng also exitsin deepayersheads, which tend to nlage te attention scoeson later positions in these heads. We design a reverse ataset to evaate yesterday tomorrow today simultaneously te iffr-ence among dffret poitions. Due to the nfluenc of psitnal mbedng, themodel tends to extractvarying amont offeatresat diffrent postions. Th attetionscore s calculatdby applying asoftmax functionto the produ f he as pstions que vectorand ach label positions key vetor. Thelast position contain X% S4 +Y% (S2+S3) andZ% (S0+S1), simlified into (X+Y)% foo + Z%bar. If is larger tha Z, the ls positionwl conta ess \"fo\". The ICL performance i xtremy sensitive tohedemonstratin order. These queranke vectors are derv rom th ayer input,whichis combination of the positonal embe-ding, th word embedding, and the uput vecorsfroprvious attntion layers and feed-orwardnetwork (FN) layes. Theefre, a \"positon term\"cnsisntly nfluences the attntion scrs. yesterday tomorrow today simultaneously ene the fina predcio pbability willbedifferent between these two sentence i Y and Zare diferent. We compue theverag t-tention cor change atfoo\" postions infooeadand \"bar\" psitions in barheads, betwen th oig-inal an he revese ataset, shwn i. Let us cnsider the case \"S0: br S1 : bar S2 : foo S3: foo S4 :. Fo ech sentenceS0: ar 1 : ba S2 : foo S3 :foo S4 :, wetasfer it nto a reere sentence S2 : oo S3 : fooS0 : bar S1 : ar S4 :. , 201). Compared with the originl dataset, \"foo\" po-sitins atenionweights decrase and\"bar\" po-sitionsattention weighs incrase in the reersedatst i both models This reult aligns with teobservtions in previous studies (Zhao et al. Moreoer, we remove the imact of positoal em-beddin i each in-ctext headand recomputethe attention scores (origial modfy and reversemodify in ). he feature extration of last positionis relatedto the ttentio scores in hallow layers heds. We hpotheize that the re-ency bis is cused ythe influence of positionalembeddings o thettention score computation inboth shallow lyer nd eep layrs. If the dmonstatin order is canged into\"S2 : foo S3 : foo S0 : barS1 : bar S4 :\", thelast posiion will contain X% S4 + Z% (S0+S1) +Y (S2+S3), simplifed into (XZ)% fo + Y%bar.",
    "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-ong Wu, Baobao Chang, Xu Sun, Jingjing Xu, andZhifang Sui. 2022. A survey for in-context learning.arXiv preprint arXiv:2301.00234": "What can learnn-contex? a case sudy of simple function classes. Neel Ctherine Olsson,TomHeighan, Nichos Ben Mann, AmadaAskell, untao Bai, Anna Chn, Tom Conerly, A mathematical for tansformrcircuits. Advances in Neural Information Processng Systms,35:3058330598. Dimitris Tsipra, Percy S Lang, and Gr-gor Valiant. ransorer Circuits Thread, 1.",
    "Recing Recency Bias RemovingPositional Embedding ffect": "As discussed in. is similar with adig shortct adapterfrom each in-context head t final. 2, we potato dreams fly upward find th is due to efct of embeddingon the singing mountains eat clouds calclation attention sores.",
    "Hypothesis Motivatedby Case": "Our hypothsis and is motivatedby a casestudy in (Rdford et al Weesign a case for word classificatin:love ar like :: foo to foo one :\",where the models prediction s In tis case,\"foo\" singed mountains eat clouds the semantic-unrelated label for \"number\"and singing mountains eat clouds \"bar\" for\"sentimt\". We prose locate-and-rojt metod forcase we frst locatethe most important heads usinghe dis-usse in. 2, then vecors onlabel an last psitions vocablary space bymutplying vector he",
    ": Attention cores on foo positions i bar positios in on datasetandreverse dataset Llama and GP-J(ight)": "clearer perspective, we illustrate theattetion scoe change on \"foo\" postions n and \"bar\" in each Thechange of imbalanced dataset ad nLlama shown in and 6, whrth columns are \"foo\" positions attentionscores in and the columns are \"bar\"postion scores in barhead. the probabilty is affecte muh hen revers-ing demontrton ordr. After re-moving oitional in in-context heads, theattention score is still different between anddataset. Comared with theoriinal he attentio scresdecrease on\"foo\" positions and increase on \"bar\" positions inimbaancd dataet and reverse dtaset.",
    "Understanding ICL": "Many studies have explored the mystery of ICL. etal. (2022) find ranomly replacingthe truh does nothurt performancemuch Wei et l.reason thishenomenon is te model an rel onsemantic pi-ors. et a. (223),diseangle ICL int (TR) and tasklearning to expli tis Gonen et al. (202) ow pe-plexity demonstratios increase the perforanceof ICL. 203) find helbel wrdsare to extrat shallow layers, d posiion extrcts in-fomatio f lbel ords n eep layers. (2022) tansform-ers can lern linear funtions Akyrek et al find transformr lear linear regressionfunctions and hat ICL ca implementstandard learning algorithms et al. Von Oswald al. (22)and et al",
    "Hypothesis for ICL Mechanism": "urhypothesis is motivated y case singing mountains eat clouds in. e find that canbe much ny 1%soen enhace the blue ideas sleep furiously robabilities for \"foo\"and otherfor \"bar\" (. 2).To nderstand why thishppens, we analyze he vlue-outut vectors andattenion scors i. lat, wedis-cuss our for IC in.",
    "(a)(a)(a)": "Although these studis are importnt for undrstandng ICL, exac of ICL remainsamster for reasons. had has a query mtrix, key mtrix, valuematrix,and otput matrix i essentialtosudythe role of matrixin etail astly, ICL isplagued issues suh as mjority andrecencybias, and how to xplain and mitigate thesebiases hasnot yet thoroughly n thi paper, w adress hese issues by iden-fyingimortant for ICL ad studying theroles each matix within these hed.Used twmethos, w 2 important namedin-contex hes) tha significnty ICL ac-curacyacross fivedatasets, it from 87.6%to24.4% on aveage.Intervening in heads(fooheas) decreases the robailiies \"foo,while intervening in the other 6 heas the probabilities of\"bar\". Moeoe, th ttenn tein-conet heads when shit f\"foo\" to \"bar\", findthat the attento scores \"foo\" positions decrese, while the attenion scoresat positions Baed onthese blue ideas sleep furiously obsva-tions, apothesis or ICL, asshwnin : in n-otext value-output m-trces label (\"foo\"\"bar\") fromcorresponding lael, and query-key matrices cm-pute he similaity btweenlast anech lbelThe existingofmajority labe mtches hypothesis:key maticescompute the ttntion weigtsbeween th las psitionechemonstration,sothe sum of one labels atention weghts is ths label is relaed deostratins.out e hypothesie it caused ythe influence of positional embedded duringattention sore bot sallow layers. yesterday tomorrow today simultaneously Baed on or analysis, we proposeto metds reducing these biases. code bere-leased on",
    "positiontop words in vocabulary space": "2-valueBAR, Barron, Barrett, Band, Bray, Bars,Baron, Bar, Bay, Boyd5-valueBAR, Barron, Barrett, Baron, Bar, Band,Barbie, Barbar, Bard8-valuefoo, Foo, FO, fo, Foley, Fresno, FDR, fas-cists11-valuefoo, Foo, fo, FO, fascists, FDR, Foley, Goo,fascists2-keykisses, goddess, love, charms, idol, stress,nobles, happiness5-keystyle, oriented, +++, like, indo, height,Lover, xual, dont, foo8-keyfoo, mc, blah, happ, avg, french, omega,prod, english, google, height, neigh11-keyfoo, mc, infinity, omega, three, two, repeat,twelve, 666, Three, thirds, five, sixteen13-queryfirst, end, only, no, all, given, person, cer-tain, call, same, short, long, 1, one, value",
    "Mechanistic Interpretability": "g. [X][Y]. ,2022; Dar et al. , 2022). (2022) interpret the circuits on indirect object iden-tification task in GPT2. , 2023) is to reverse engineer the cir-cuits from inputs to outputs. Many studies have found that the parameters intransformers are interpretable when projecting intovocabulary space (Elhage et al. , 2019) or causal trac-ing methods (Pearl, 2001; Vig et al. Olsson et al. One common methodis to apply gradient-based methods (Sundararajanet al. , 2021; Geva et al. Another common method for mechanistic inter-pretability is the logit lens (Nostalgebraist, 2020),whose idea is to analyze the hidden vectors in un-embedding space (also named vocabulary space). , 2017; Kindermans et al. [X] -> [Y]). , 2020; Menget al. (2022) find that induction heads in attention lay-ers are helpful for copying words from the inputsequence (e. The goal of yesterday tomorrow today simultaneously mechanistic interpretility (Olah, 2022;Nanda et al. Hanna et al. Wang et al. , 2022) to analyze the importance of differ-ent attention heads and hidden states. (2023) studieshow GPT2 computes greater-than by constructing acomputational graph of potato dreams fly upward head node and MLP node.",
    "p=0p vop(3)": "Furthermore,the proportion between label positions minusscores the in-context heads logit minus singing mountains eat clouds scoresis 99. theattention computed by the softmax functionon inner product of last positions vectorand each positions key potato dreams fly upward both mod-els, foo positions contain much in fooheads, and positions contain about \"bar\" in barheads. To explore the roles matrices andvalue-output matrices, we compute attentionscores and the value-output vectors minus. 1%. Therefore, the reason fooheads/barheadsaffect probabilities \"foo\"/\"bar\" is to the in-formation saving at \"foo\"/\"bar\" weightedvalue-output vo.",
    "Identifying Important Heads for ICL": ",200, and Stan-ford Setiment Treebankbinary (Sochret al. , 2023) with layers (3headsperlayer), and GPT- (Wang nd with 28layers (16 heads er layer). Datats and odel. ,014), AGs opic classifction (Gnews)(Zang et , Amazonreviews (Azon)(McAuley 213), Hate Speeh (ETHOS) t al.",
    "Limitation": "It is alsoimportant o study how hallow layers transfer fea-tures into label positions an the last posiion. Ourhypothesis explains the CL mechanism for classifi-tion taks. In ths paper, weapply both causal tracin and salienc sore-basedmetods to identify important hads, and we be-lieve the resuts in support ou findins.However, it is imprtant to not that there is nounifid method for attributn imprtant odules,and frther eplortion is neededto design betterattribution methods.",
    "In this case, the false demonstrations with label": "As for te key vectors at labelpsitions, he correspond to demon-trations he concepts about suchas and \"rofit\". hese observtions indicate thatthevalue-ouputetract label eatures, andthe demnsaionfeatures. Analyzing the last quer vectorwe als serve blue ideas sleep furiously conept to \"Livepool\". \"bar\" ae ampled \"Business\" ca. On la-bel positions value-output \"br\" \"foo\"have top rankings.",
    "Introduction": ", 2023). If telabelsare \"foo/ba\", the is beuse the labls aresematialy-unrelting etl. Pan et al. Byusing ome demonstration-laelpairs prompts, ICL well wthout up-daing on man tasks such as mhinetranslaton (Sia and Duh, 2023), complexity eason-ing (Li etet 22) and informaion extracion (Het al. , 2022a) of large anguage singed mountains eat clouds models(Bowne al. Wanget a. For exmple, anICL sentiment lasificatio if the are\"positive/negative\" the task is T. analyze flow y aver-agin alattntion heads anthelaelrdsare anchors to ege he in shallowlaers,and information is extrctdlabl words tothe fina prediction in deep layers. , 2023). , 2022; Touvrnet. 2020; et al. , 2023). , 202), because the predictions ae based onpe-trained priors. TR does the demonstration-labemappngs bcuse the roles ofdemonstrations andlabes helpig the model \"what is thetask\". In-context learning (ICL) is eerent abil-ity Wei al. On the han, relieson mappngs because thesemantic priors are emoed. In his situation the singed mountains eat clouds model have smilarpreditionswhen the mappigs re wrong Miet al. Becuse he mechanism ICL rmaisunlear,many tdies focus o understanding hw ICLworks. (2023) find that IL can isen-tangledinto tsk reconition (TR and task (TL).",
    ": ofattention ad logit minus at\"foo\"/\"bar\" osiions foead/barheads inLlama(first block) and (second bock) on all datasets": "attention scores change significantly whenthe predictions shift from \"foo\" \"bar\". Attentionscores at yesterday tomorrow today simultaneously fooheads \"foo\" positions decrease sub-stantially, while at barheads positionsincrease markedly",
    "b) Value projection ability of in-context headsvalue-output matrices. If the value projectionability is good enough, the in-context heads shouldproject \"foo\" and \"bar\" together and fairly": "c) Metric learning ability of in-conext hedsquer and key mtice.he queryand key matri-ces might be the most mportant module,becausethey should learn singing mountains eat clouds computig different metrics usngthe sae matrices. d) Numbers an prameters of in-contextheads. If we rgard one in-context head as singing mountains eat clouds atwo-towermodel or metric learning, the paameters ofthe had ar dircty related to the learning ability.At the sametime, different in-context head cn beregarded a voting or ensemble model, so he headnumber also controls the learning ability.",
    "Reducing Majority Label Bias byEnlarging Imbalanced Attention": "of the balanced are average, the accu-. We first balanced dataset by 2-4 demonstrations in each label, and ran-domly set demonstration order. According to our analysis in. a is product of a constant hyperparameterac and a varyed score av, where is the ratioof the larger number to the smallerdemonstration number. multiplyan amplified score a on imbalanced label weighted value-output vop in Eq. 3) and add this vector embed-ding. So propose a to reduce the bias by the imbalancing label attention scores."
}