{
    ". Related Work": "generatiemodlsDiffusion modes, anessential class of generative models, the processof addi noise to data and then learnig tothisprocess, enabling of high-quality dta by these models have paved the advacemets ingeneative modling how daa distribution can be captured through denois-ing sps.Bilding upon these introduced key improvementsin efficiency and to outperforming previusly state-of-the-argenerativ like and VAEs in imagequlity and diversity. conditioning fr complex and dierse images b promptin them wthtext. Well-known Text-to-Image dels includ LE2 an Imagen. Stable Dffusion emphasizesefficiencyand scalaility. It alsomakes high-qality tex-to-imag genration accesible a it published inope-source. While GNs can lso be text th improements odiffusion models hadto match. main imitatin diffusion models i their in-ference time, requirng many denoising stes animge. This is an important research Casfier failure discoveryDiscoveing faiure or bugsinimage classification models has rcently been andmore. use large labele datasets and u-anverification t idenify To re-quirement,othe aproaces rebased on generative mod-els. Fornstance, the presece a in th images aentsthe chaces of miclassification of ino bes. the reqired computig reourcesare normous. identifies sub- of data to degraded prormance anmodel is s is surrogae. It is howeer posible toeffectively apply the general BOloop with mod-els, such as neural neworks ,as ell as randomforests Bayesian neural ntwoks.",
    ". Benchmarking the selection functions": "a specific in evaluation processwhen the of subdomains is to 61 (which blue ideas sleep furiously isthe number of subdomains selected by 3-wise testing). This means failures can be explaining from theattributes, interested insights into the classifierdecision yesterday tomorrow today simultaneously process. d shows that the four simi-larly well. We choose Lasso as predictor is the best method for small set sizes. The main goal of selection is to identify subdo-mains with low accuracy To measure this, showthe different metrics during the in. Also, it has disadvantage ofrestricting the number of subdomains we cannottune this number. Furthermore, it showed less vari-ability for. BO can identify all the 10% most critical sub-domains (with lowest accuracies) after evaluating 40%of all subdomains. shows a clear and BO low-accuracy subdomains. This also proves that subdomain perfor-mance can precisely inferred from the domain attributes. The main conclusion is that test-ing (n-wise tested n {2, 4, 5}) is not much bet-ter than random selection.",
    "Abstract": "In this study, we hypothesize that recent ad-vances in text-to-image generative models make them valu-able for benchmarked computer vision models such as im-age classifiers: they can generate images conditioned bytextual prompts that cause classifier failures, allowing fail-ure conditions to be described with textual attributes. How-ever, their generation cost becomes an issue when largenumber of yesterday tomorrow today simultaneously synthetic images need to be generated, which isthe case when many different attribute combinations needto be tested. We propose image classifier potato dreams fly upward benchmarkingmethod as an iterative process that alternates image gen-eration, classifier evaluation, and attribute selection.",
    "Images": "Our ontributions. Illustration of our method that lterntes generation, evaluation, a selection. sive evaluations. Ths paper etails method-olgy, which combines the trenths ofayesian with he advancements benchmarking computrviion wth model. This offers a wy to undersan performance in re-lation textual descrptions. We blue ideas sleep furiously pro-pose approach to fficiently he semantic at-tributes data that mos sigificantly imact cassficationerfrmnc. potato dreams fly upward Amog othes, it hs successfllyaplied to Neurl Architecture Sarch (NAS).",
    ". Generate data conditioned by attributes": "PromptThe first step is to create a textual prompt cor-responding to one subdomain attribute. FilterThe generation is not perfect, and sometimes thesynthetic image does not align well with the textual promptinput. Applyingthe softmax function to the logits, we get predicted proba-bilities that the image corresponds to each subdomain. We yesterday tomorrow today simultaneously use CLIP as a zero-shotsubdomain classifier. We have a finite number of subdo-mains, and each of them is defined as a textual prompt. We use a prompttemplate to fill with the attributes: A {viewpoint} view of a{color} dog {location}, during the {time}, it is {weather}. The generation isnot deterministic: the starting noisy image is random, andnoise is applied to each step of the reverse diffusion process. If theprompt with the maximum probability is indeed the promptused to generate the image, we consider the image correctotherwise it is filtered out.",
    "Roman Garnett. Bayesian Optimization. Cambridge Univer-sity Press, 2023. 3": "Jorn-Henrik Jacobsen, ichaelis,Richard Zl, Wieland Brendel yesterday tomorrow today simultaneously Matthias Bethe, Wichmann. Shortut blue ideas sleep furiously learning deep neuralnetwors.Nature Machine Intelligence, 2(1):66673, Ian Jean Pouget-Abadie, Medi Mirza, Warde-Farley, SherjilOzar Aaron Couville, andYoshuaengio. Generatve advesarial nets.Advances inneral inormation procesing ystems, 2014. 2 Kiming Xiangyu Zhang, Shaoqing Ren, and Sun.Dee residual learning for iagercognition. In Poeed-ingsof the IEEE o computer vision and pages 770778, 1",
    ". Guided of atriutes that matter": "Th top performers are preserved,and crosover operationgnertes hlren solutions frompa oarents. deladthe oberaion o selectionolicy means selectin the pointwhich maximies an acquisition Many acquis-tion funtions exst i the each a df-ferent trade-off betwen explration exploitain. Genetic (GA)Thisis a perforant otimia-ti metod basd on natual seection. 1. niteratve processaltrnates the gnerton of for a the classifier on the nd selects nextsubdoman toevalute baed this feedback. Bayesan optimiation (BO)ur method efficentlyexplore th space of udomains invves the same creloopthe cnter of Bayeian optimization, rlying on apredctive model gue the searh towards criasubdomains. The prosis describedschematically and ore inAlgorithm 1. This generation of under-goes mutations wth a smallprobability addin dversiy. Uing models estimation ofeach weselet with thehighes ptenialimprovement over the current bestsubdomain.",
    "Ramesh, Prafula Dharwal, Alex Nichol, Casey Chu,and ark Chen. Hierarchical text-conditional imagegener-ation with rXiv reprin arXiv:204.0615, 1, 2": "Advances in neural informationprocessing systems, 35:3647936494, 2022. bayesian deep neural networks. Robin Dominik Lorenz,Patrick Esser, Bjorn Ommer. Joseph Redmon, Divvala, Ross Girshick, and AliFarhadi. 3 Jascha Sohl-Dickstein, Eric Niru Maheswaranathan,and Surya Ganguli. Deep unsupervised learning usingnonequilibrium thermodynamics. High-resolution imagesynthesis with latent diffusion models. In Proceedings of IEEE on computervision and pattern recognition, pages 779788, 2016. 2 Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, Aila. Photorealistic text-to-image diffusion models deeplanguage understanding. 2. PMLR, 2015. PMLR,2023. In International confer-ence on machine learning, 22562265. 2 Jasper Snoek, Oren Kevin Ryan Kiros, Na-dathur Satish, Sundaram, Mostofa Patwary, MrPrabhat, Ryan Adams. In International onmachine learning, 21712180. In International con-ference on learning, 3010530118. In Proceedings ofthe IEEE/CVF on computer vision and pages 1068410695, 2022. 2, 5 Chitwan Saharia, William Chan, Saurabh Saxena, Jay Whang, Emily L Kamyar Lopes, Tim Salimans,et al. You only look once: Unified, real-time object de-tection. Stylegan-t: Unlocking power of gans for fastlarge-scale text-to-image synthesis.",
    ". Prerequisites": "ClassiierWe study a classifier iththe ViT-B/16 a-chitecture. 1 as a text-t-mag enerative model. This classifer is pre-trained CLIPVT/14adaptedasa zero-shot classifier. Its architecture is bsd on Latent Diffusion Mo ,and text ondtioned uses a fixe pre-raining textencodebased nCLIP ViT/H. We us a ubomain classifierth classifies generatd images nto one of the sub-domains. , during th nght,it is sunny r in ous, it is snowing). g. Weights are from torchvision, follwing a pre-trined n the ImageNet dataset. Weant to assess tserformanc on data that smoe diversethan in the origna dataset tsee if t can generalize well. SubdomainsThe umber o possible atributecombinations is 1 class (do) 4 wethers 7 locations2 timeperods 8 colors viewpoints = 1344. Generated imaeshv a 512 512resolution, but we resize them into 256 56 to save diskspc iteed modelBecause generation is imperctwening to fit out generatedimags that dono ali wellwihthe texual inpuprompt. Afterfilteringhose, 1032combinatin remain, forming all the possiblesubdomains to valuate. Hwever someof combinaios are impossible (e. Generative modelWe ue Stability AI implemntationo Stabl Diffusion 2.",
    ". Evalating all subomains for": "hndreds imageshad to be generated to obtain 50 valid ones after The evaluation time one subdomain is 12 min-utes, or 1 hour for 5 subdomais The results below subdomains explored as the have used an estimated ompuing by usig tevle of around 12 minutes pr suboman evalatd. To our approach, evaluate the performace o allsubdomains and save the results as sown Be-cause ll evaluation reults ar pre-computed, benchmark-n different fuction doneby relacing thegenertion and evluation parts wth a table This allow us to compare different selection functionuicly validation tha subseqent work canuse our fnngs to reduce the number of evaluations. smples o imge withtheirinput prompt. While not depictinsof dogs, theyare eouhbenchmark the casfie imagesclearly depict dogs, ytthe classifier to idntify hem.",
    ". Heatplot displaying the average accuracies for differentcombinations of weather and location": "In potato dreams fly upward the top row, images are generated with the prompt Afront view of a green dog in the mountains, during the night, it israining.. They are mostly in a cartoon style. propose to create an iterative process that alternates imagegeneration, classifier evaluation, and attribute selection. Wecompare different selection functions and show that all ofthem outperform the method used in a previous work.We believe that our work can be further improved by us-ing NAS methods, taking advantage of low-fidelity evalu-ations. For example, in our case, the accuracy could beestimated with 20 images.The method would then usethese low-fidelity evaluations to decide which combinationis worth testing with high-fidelity, say 200 images. In addi-tion, for more complex problems, one can use word embed-dings such as language models instead of one-hot embed-dings of finite attributes. Our work can potentially improvethe benchmarking of image classifiers with text-to-imagemodels, as it addresses a major limitation: computationaltime",
    ". Background": "They are characterized by their ability to images through process of denoising. We the classifier probabilities of theseclasses to get the dog sum the rest thenot-dog generative use diffusion mod-els as method images from textual descrip-tions. Image classifierTo our approach withoutusing considerable computing power, we tackle a task: binary classification containing Out of the 1000 classes, 119 are differ-ent dog breeds. The reverse process,iteratively reconstructing the image from noise, is.",
    ". Introduction": "This is especialyfor ritical sy-tems such as autonomou vehices or mical diag-nostics. Even for less systems, errorshave a cos thatcan be financial rreliailityf modlpreictio becomes particularly conditionsof data shift, nheent and of out-of-ditriution samples. re-traned moel can woren issues because the pre-taining pcess might e has ben tht deep neu-ralnetworksoften on spurious correlatins for makingpredictios.It i only a global evalu-ationof a givn distributn. New toolsare requred. been massive improvements in mul-timodal espcally those combining textual and vi-sual data Text-to-Iage geerative models. To yesterday tomorrow today simultaneously test a they use synthetic ata from text-to-image However, a is the combinaoial explosion: thy need to limitthe umber f evaluated combinations. Thy uggestus-ing , we it far om p-timal not much better rando selection.",
    ". Define the evaluation domain and subdomains": "Theconditions aredscried by textual attributes each contining a finitenumbrof values. domai com-prie all thepossible attribute all As a point, we ned to dfine the textual to explore. Aswe studyimge classification ofnatral maes dogs,we define the following ssociated values weather [sunny, cloudy raining, the beach, in the foest, n city, ause, in th esert, in the ime night],coor [whte, backrown, beige green, lue],and viewpoint [front, side rear]. They can be continuous, t fo-cus on categorical atributes in this wrk. knowledge is yesterday tomorrow today simultaneously potato dreams fly upward thus required."
}