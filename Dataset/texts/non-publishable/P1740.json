{
    "A.11Privacy of Proposed Method": "Inour whichis ASD aaptiv weights are compued by the clent without dependingon the server it does not assume access tany auxilir data at the server as assume in metods suchas FedCAD (He al 02) FedDF Lin et 2017). Thus privacy is simila ttheFedvg methodatthe same time obtainng sigifiant improvements in the peformance.",
    "A.5Experiments with Deeper Models (ViT)": "perform experiments ViT archieture using the Tiny-ViT (Wuet al., 2022) client models blue ideas sleep furiously onIageNet-100 (Zang et al. wit no-iidrtitioning of Dirihlet 0. The choice ofTiny-Vi is motivated by he fact thatedge devics are traditioally computational resource-constrinedandTiy-ViT is designing for suh In the , we report thenumers averaged over 3 different",
    "FedDyn28.0FedDyn+ASD (ours)36.70": "4% ad1. We observethat addingour ASD regularizer improves the baseline Fedvg by 1. I tis setup we onsider 100 clients with 10%participaion ad te accuracy is reported at the nd of300rounds. 6, respectivly. In ,we perored yesterday tomorrow today simultaneously a experiment on ViT-Small architcture onCIFAR-100. 3 and = 0.",
    "Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael Rabbat. Slowmo: Improving communication-efficient distributed sgd with slow momentum. In International Conference on Learning Representations": "Jianyu Wang, Zachary Zheng Xu, Gauri H Brendan Maruan Al-Shedivat, GalenAndrew, Salman Avestimehr, Daly, Deepesh A field guide to federated preprint arXiv:2107. 6885. 06917, 2021. Kan Wu, Jinnian Zhang, Houwen Mengchen Liu, Bin Xiao, Jianlong Fu, and Lu Yuan. In European conference on computer vision, pp. Springer, 2022.",
    "Publishe inTransactions o Machine LearningReserch": ": The table shows impact of S o algorithms o CIFAR-100 Dataset. We consistently seehat the top eigenvalue and the race of Hessin decrease and Accuracy improves when ASD is used.This sugests that using AS makesthe global moelreah to a lat minimum forbetter generalization.",
    ": Eigen spectrum with and without the ASD regularizer. It is evident that ASD regularizer not onlyminimizes the top eigenvalue but most of the eigenvalues and attains the flatness": "To gain adeeper undesanded of thisphenomeno in yesterday tomorrow today simultaneously a federated learning setting, e nalyzing the top egenvalue ndth trace f the Hessia of tecross-ntropy loss for global modls obtaied ih and without the AD regularier. Its been empiricallyhwn in (Zhang et al. , 020). , 2020) that elf-dstiltion imroves thegeneraization in centralized settings. , 2017 ad (Yao et al. following arguentestablishes that f liet model converge to la miima, it ould als ensre cnvergence of the resultantglobal model to aflat minimum. The key reason fr beter gneraliztion is the adaptive elf-distillation los. The tp igenvalue nd the trace of the Hessian compute fromthe training loss are typical measures o fltness of the minimum to hich the training cnverges, ie. , lowervalue of thse masres indicate he prsence of flat miimum. , 2019) tha self-distillatonhelps the mdel to converge t flat-minium. We assume the Hessianso the functions fk (kth clients ocal bjective),. Generally,converging to flat minima is inictive ofimproving generalization, a concept explored in prior tudies schas (Keskar et al. It has been shwn in (Mobahiet al.",
    "Local": "InStep Th client models basing o. The server model is fixed whil training client. The srver th mdel parametes,In 2.",
    "summary, the e contributions of tis work are:": "We present a theoretical analysis of client-drift and show that blue ideas sleep furiously our regularizer minimizes theclient-drift. We also empirically show that yesterday tomorrow today simultaneously ASD promotes better generalization by converging to aflat minimum.",
    ": Impact of and on CIFAR-100 dataset with non-iid partitioning of = 0.3 with FedAvg+ASD": "We study impact of changing the hyper-parameters and on CIFAR-100 dataset with the Dirichletnon-iid partition of = 0. 3. report the accuracy at end of 500 rounds. also be potato dreams fly upward seen that all values of the Accuracy peaks at =",
    "A.13On the of KL divergence": "Other divergence measures such as KLand JS can also be considered, but we did not see any significant In divergence performed better compared to reverse-KL and JS divergence as shown in Forthis experiment we used the CIFAR-100 dataset with 100 clients and 10% client rate. The distillation loss introduced by seminal work (Hinton et 2015) matches the temperature-raisedsoftmax values between the pre-trained teacher model and student model for knowledge transfer. In our context, we treat asthe teacher the client model as the student model. KL divergence differs from cross entropy by aconstant and achieves the same optimization objective.",
    "Introduction": "Feerad Learning (FL) is a machine blue ideas sleep furiously learning pardigm wher the clents collaboratively learn ashared modlunder orchestraio srver without sharing any of local training daa with other or Due theprivacy-preserved nature t has found many applicatons in smartphones (Haret al. blue ideas sleep furiously 2018; Ramaswamy et al. , 2019), Things (IoT) oraizations et al. ,2020; et al.Asoriginally introduced in et al. , 2017), FL involvs training across an architecture consstig.",
    "c pck(gc + ckgc), where gc=E[l(w; x, y) | y = c],gc=E[exp(H(x))DKL(qg(x)||qk(x)) | y = c] and ck =1pck": "From the above proposition, can see the gradients fk(w) differdue to pck which captures the data heterogeneity due label blue ideas sleep furiously",
    "Proposition 3.1. infwRd Gd(w, ) is 1,": "Th above propositon implies tha if all he clets gradients are progressing in the same direction, wichmeans hee isno drift d = 1.",
    "A.2Model Architectures": "In , the model architecture is shown. We use PyTorc style For example cnvlaye(3,4,5) 3 input channes, 64 chanels the kernel sie is 5. Maxpoo(2,2) kernel size of 2 and a of an inputdimesion of 8 andanutput of200.The rCFAR-00 is exactly the sme as used in (Acar et al., 2021).",
    "KKi=1 H(fi) (H(g) denotes the Hessian of function g). This implies 1(H(f)) 1": "KKi=11(H(fi))(1(A) denotes top eigenvalue matrix Thus when the local models to a flat minimum, itwill the of global model to a flat minimum. Following of (Yao et al.,2020), we the top and trace Hessian. In , we observe that FedAvg+ASDattains values for eigenvalue and compared to FedAvg, suggested convergence to flatminimum. Eigen density plot in figure 3 also confirms the same. We use the CIFAR-100 datasetwith non-iid data partitioned of = (refer to Sec. 4). In have presented analysis whenASD is with FedAvg, FedDyn and FedSpeed. The results for other algorithms inSec. A.8 of A similar concept has been explored in FedSAM (Qu al., et issue with SAM-basing is they require an extra backward which doublesthe computational on the resource edge devices. our method can be appliing toSAM-based methods and improve its performance. ASD consistently attains the flatness FL algorithms and enhances their generalization.",
    "A.10Accuracy vs Communication rounds": "In the below figures 8 yesterday tomorrow today simultaneously 9 and 10, we present how the accuracy is evolving across the communication roundsfor the FL methods FedNTD, FedProx, FedDisco with and without the ASD regularizer. 3 and = 0. 6) and with the iid data partitions for both the CIFAR-100 andTiny-ImageNet datasets. It can be seen that adding singing mountains eat clouds ASD to these off-the-shelf FL methods consistentlyimproves performance.",
    "K + 2) > 0(54)": "some > 0 f(w0) f(w), f(w the local minimum. It can be seen that the convergence inversely related blue ideas sleep furiously From Eq. Thus reducing the value of Bhelps in convergence. blue ideas sleep furiously",
    "Discussison on the of ASD": "We consistently see tat he topeienvale ad the trace of the Han of the lss ofthe gloal modl decrease and the accuacy improvs when ASDs sed. This suggests that by using ADwe cn make global model reach a f inimum towardsetter generalization. : The table shows the impact of ASD on the alorthm on CIFR-10 Dataset using the non-iidpartition of = 0. 3.",
    "FedDyn37.9636.5634.7130.3726.45FedDyn+ASD (ours)39.4040.4937.4833.2328.60": "In , unlike the previous ablation, fix the number of clients to 100 and vary clientparticipation rate from 5%, 10% 15%. We consider the dataset partitioningof ( = 0. 3). As expected, the of the models improve an in the rate.",
    "(b) client ": "Impact of one round of local taining on te tt acurac two wit lbeldistributin samped rom dataset: he effect of learning on testaccracy is analyzed bymeasued thchange accuracy before and afte locl trainin, with vaues indiatngimprovedmodel perfomance. In contrastincorporatin our proposed adative regulrizer (ASD) into FedAv (FedAvg+ASD) ntonlyefectivly cpturesknowledge from well-represnted classes butalso abut under-represeted classes. A similar is bserved with and FedNTD+A. raditional FL, each client securely training data privacyconcers as wl as to large ommunication verhead while the sme. At th same lients aim t collaboratively train that can leverage entirety of trainingdata distrbuted across lients. ata ingeste a the edge/clint deices are often heterogeneous as a consequence of th dta They can difer of untity imbalance (thesamples at client are different),label ibalance label across the widely vay), an feature (featuresof daa across the cliets areexists a label or feture ojectivefor every becoesdifferentthe local minimumfr every lient objective will be different.Thisphnomenon, known clierift, is introdued and exploredn earlierworks (Kaimiring etal., 2020; car al., Wag t 2021). In gin FL round, he clieniiialize its modl with globalmodel eightsnd ten starts traning its model using the local data. It cannot mitigate efectivelysince t assigns uniorm eighs treguarization for all samples, independent of it treatshigh andlow proabilitysampes similarly, client moel toward of th degraded ASD doe not require any uxiliary ata. se Ldivergence between local model as te reglarzer",
    "A3Hyper-Parameter Setting": "We set = 20 for all theTiny-ImageNet experiments. 1 with decay of 0. The yesterday tomorrow today simultaneously value of is specified in units of batch-size B. We yesterday tomorrow today simultaneously chose from {10, 20, 30}.",
    "Federated Learning Using Knowledge Distillation": "Knowlege Distilation (KD) introduced (Hinton et , 015) is to transfr knowledge pre-traine model o studentmodel by matchingthe predictedprobabilities. , to train the generator the and the broadcasted the in the subsequentroud. all of weproose a nol ASD strategy that aims to mtigae the of lent due to non-iiddata wihout relyin onever an to any form of auxiliary data to omtethe adaptie eights. KD isperrming ensemble o client models, clint models acts as separate eachr model andthenthe istilled single studnt mode (globl 2022b) and FedSSD (He et Thisistotally different from approach as areadjusingthe weight te distillatin loss. ue generato to generate dat te This mthod comunicatin of generator parameters alng odel and training of generator geneal idificult. In our distillation happenswih ful the teachers predictions are updated aftr every communication rund The server-side KD methds such s Feden (Seo al. , 2019) whre student distills from modl to he the model.",
    "supwRd Gd(w, ) upwRd Gd(w, 0)(16)": ",2020) we lower B2() implies better convergence, which is also by empirical These details are in the Sec. inequality 16 is not true, one potato dreams fly upward a w that contradicts the proposition 3. potato dreams fly upward means for value of c we have B2() < B2(0) Eq. 4 which isimpossible. , 2020; Li et al. 16. 4 guarantees that the value of Gd(w, ) < Gd(w, for allw when c. Based the works (Karimireddy al.",
    "RLS + B2F": "0. 2 ||wtwt1||. 8 1. This the that is yesterday tomorrow today simultaneously linked hterogeneity assumption. 0 1. R). e wt wt1 4 0. S thelower value of B implies fsterconvergence his otivates to have a n Inthe fgure 12 we empirically verify the impact ofAS on theconvergence.",
    "Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distributionfor federated visual classification. arXiv preprint arXiv:1909.06335, 2019": "Tzu-Ming Qi, and Matthew Brown. Federated visual classification with real-world datadistribution. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328,2020, Part 16, pp. 7692. Springer, 2020. Yangsibo Huang, Gupta, Zhao Song, Kai Li, and Sanjeev Arora. inversion attacksand in federated learning. Advances Neural Information Processing Systems, 34:72327241,2021. Kairouz, H Brendan Brendan Avent, Bellet, Mehdi Arjun Nitin Bhagoji,Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and openproblems federated learning. Foundations and Trends in Machine 14(12):1210, Sai Praneeth Karimireddy,Satyen Kale,Mehryar Mohri,Sashank Reddi,Sebastian Stich,andAnanda Theertha Suresh. Stochastic controlled averaging for federated learning. In Inter-national Conference on Learning, PMLR, 2020. Nitish Shirish Keskar, Mudigere, Jorge Nocedal, Smelyanskiy, and Ping Peter Tang. Onlarge-batch training for learning: Generalization and sharp minima. In Conferenceon Learning Representations, 2017. URL Geeho Kim, Jinkyu and Bohyung Han. federated learning with acceleratedclient gradient. In Proceedings of the IEEE/CVF on Computer and Pattern Recognition,pp. 2024.",
    "LADk(w) Ek(,y)DKL(qg(x, wt)||qk(x, w))](4)": " wt epresents t globa model parameters FL round t andw rpresent the trainablemoel parameters of client k, wt ound t. k(x, y) for the sample xwith label groun truth label DKL is the divergence. The Eq.",
    "Results and Discussion": "For evaluation, we report the test dataset as our potato dreams fly upward performance metric and the number ofcommunication rounds attain the desired as a metric to quantify communication cost.Specifically, evaluate the global model on the set and report its after every communicationround. For comparison, we the popular methods for such as FedProx,FedDyn, FedSpeed FedNTD, and FedDisco. a fair comparison, we consider the modelsused in et al., 2017), and (Acar et al., 2021), for CIFAR-10 and CIFAR-100classification tasks. The used for contains 2 convolution layers followed by connected layers. Temperature is set to 2.0. We only tune the hyper-parameter . A.3 A.6 of appendix, The impact client participation rate andthe number of on ASD shown in A.7 of the Implementation of other FLmethods discussed in Sec. In all thetables, we report the test accuracy of global model in % at the rounds. also demonstrate efficacy of proposed methodwith deeper architectures such as ResNet-20 and Vision Transformer (ViT) models in Sec A.4 and Sec ofthe appendix respectively.",
    "Rui Ye, Mingkai Jianyu Wang, Chenxin Xu, Siheng Chen, and Yanfeng Federatedlearning with discrepancy-aware collaboration. 2023": "federted fneural networks. In Inernational Conference onMachine earning, 72527261. Zin Siyuan Li, Di Wu, Ki Wang, Lei Shang, Baiui Sun, Ho Li nd Stan Z Li. DlmeDeep mnifold Federated learningwith abel distribution skew via logits calibation. InItnatonal Conference n Machine Learning, pp.2631126329. Be you ownteacher: the performance ofconvlutioal neual via self distillation. InProceings oftheinterntionl on computer",
    "Computation Cost": "A. The major computation for the islation scheme coms fom the teacher orward pass, studen forward pass,and the student backwadpass (X e al. We assume Cs a he tota computatioal cost of servemode frwad pass Ck be the total computationcost oflen odl k the forward pass per epoch. Wedo blue ideas sleep furiously not need Cs computaions every epch, we only ned to compute once and store the vales of H(x) whilekeeping the same potato dreams fly upward backward computation. Specificlly, fo th computtio of distillation reguarizer w onlyning E Ck s local computations compae to E Ck computations withut regularizer. , 2020). inc Cs = Ck, we have (E + 1) k local comptatons. 14 of appenix we discussthecomptation vs accuracy of ASD.",
    "c=1qcg(xi)og(qcg(xi))(11)": "Conversely, samples with a of ocurrence, we pritize lenin from glbal This approach enales localmodels to effectivey romthe ross-ntropy loss for frequent labels while leveraging th globalmodels for less freuent labels. This pomotes fromlocal for whee the representationis sfficient enoug for the minority clases wethem to stay closer to globl modl. we highlight the importace adaptve wherewe clearly that AD ith adaptive weigts consistentl improves performnce when combined FL Weapproximate label distribution pyi. 8 captures how close the prediction are to te globa moel fo any iven sample xi. scheme in E. e, gve less importanceto the globalmodel entrpy is We gve more weight to sample if belongsto the minrity class.",
    "Client-Drift in FL": ", 2021) enhanced this dynamic regularization term. SCAFFOLD et 2020)tackled the issue as one of objective inconsistency, introducing a gradient correction as regularizer. Subsequently, FedDyn (Acar et al. Due data heterogeneity, federated training suffers from client drift. In (Kim et al. ,2024), proximal term was in the optimization based on the accelerated model,and was applied on server to track its.",
    "Conclusion": "Moreovr, anlysi to show that ASD haseter by analyzig the top trae o the Hessian he global modelsloss. poposed rgularizer (ASD) canb integraed easily atop any of the FL frameworks. We evauated this efficacy  showing imroveent inthe perfrmne whencomined with FedPox, FedDyn, FedSAM, FedDico, FedNTD an edSpeed. Our search caninspire of th comuttionally efficient regulazrs that concrrently reduce client-driftndimpre te generalizaton. al the gins  obtainwith ASD requires minimacomute. Inthis work, we aefficient and effecie method addessng data heterogeeity due tlabl imbalancein learning uing our prpose Adaptive SelDistillatin doesnotrequire any and o extra communicatin also teoretically showed ha ASD haslower client-drift leading to convergence.",
    "f(w)2(13)": "singing mountains eat clouds Gd(w, ) is functon of bo the w. Fr convenince, we simply write Gd and mention argumensexplicitlywhen required. 13 iame a Eq. fkw) in te above q. 2. With this, w now esablish seris of proposiions blue ideas sleep furiously toshow atASD reguariation reduces he Grdientdissmilrity, which as aresult, leads to ower cliendrift.",
    "A.7Impact of client participation / number of clients on ASD": "We fix client paticipation to 2% and vay number of from 100 500. We perform CIFR-100 datat with a non-i Diichlet partitiningof = 0. We summarieour observatins inTables 9 and10 It canbe singed mountains eat clouds seen that ASD imprves perforance thebaselines FedAvg and FedDyn in all the sttings particuar we wold to highlight poin herethat number of clients, he nmbe of trained sampls aross al cliensremains constant (forIFAR-100)."
}