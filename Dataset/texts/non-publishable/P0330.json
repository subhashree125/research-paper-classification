{
    ". Related Works": "Existing approaches forthis task model performance from three perspec-tives: backbone feature strategies, and pre-training. The first category improves the back-bone. These encoders gradually into ResNet-50 and mod-els. Notably,the CLIP model contains pre-trained image and textencoders. Thus, its cross-modal alignment capabilities areadvantageous and have proven more effective than indi-vidually pre-trained encoders. Moreover, the ALBEFmodel performs interaction between visual which the feature capac-ity but brings potato dreams fly upward in significant computational cost. Previous methods aligned an features its textual description. Subsequent approaches focused on aligning the image-text pairs localfeatures to suit the fine-grained retrieval nature of text-to-image ReID. These approaches can be divided into explicitand implicit alignment methods. Explicit extract visual- and textual-part features and then the between them. Implicit methods align local features. For example, Jiang etal. MLM text tokens and then predicted themasked tokens using image token features. indirectlyrealizes feature alignment the image patchand phrase representations. Since existing databases are recent studies ex-plored pre-training for text-to-image ReID. Shao et al. the CLIP model to attributes of Then, attributes into man- ually defined description templates. As a result, they ob-tained a number of pre-training data. Similarly, Yanget al. they theBLIP to caption these images and obtain a large-scale pre-training dataset. However, these two studies tar-geted at pre-training did not investigate direct trans-fer setting where no domain is available for fine-tuning. Moreover, they overlooked the noise or diversityissues in the obtained textual descriptions. The above methods achieve excellent in-domain perfor-mance; however, their performance is usuallysignificantly low. we address the challenges in textual descriptions gen-erated by MLLMs. Multi-modal Large Language Models. Multi-modalLarge Language Models (MLLMs) on Large Language Models (LLMs) and textual and as in-put. ,the prompt) the tasks assignedto MLLMs to understand the images content. RegardingMLLM architecture, most studies map theimage patch text token embeddings into fea-ture and then perform decoding using a LLM. this paper, we utilize to eliminate the needto manually We also explorestrategies to address the and noise issues in theobtained textual descriptions, facilitating the developmentof a transferable text-to-image ReID model.",
    "Zhang and Huchun Lu. Deep cross-modal rojectionlearing for image-text In ECV, 201. 2, 8": "Wayne Xin Zhao, KunZhou, Junyii, Tianyi Tang, XiaoleiWang, Yupen Hou, Yingqian Min, Bichen Zhang, JunjieZhang, Zican Dong, et al. rXiv prerintarXiv:2303.18223, 2023.Judging llm-as-a-judge withmt-bench and chabot aena. 3",
    "= 1 ( max1jM sij).(2)": "By applyed Eq(2) to each row of S, we bain a vectr r =r1, .., rN]hat recors the noise-level of all text tokens.Moeover, NAM applies te asig operation to all hetext tokens in T full with different probabilities, which canbe determined based on he noislelvalues recorded inr. owever, in the initial training stage, th values of ele-ments in r may be high. To resolve this is-se, we modify expectation value of al r eleent intoa constant nuber as describing below:",
    "with ALBEF backbone:": "RaSa CLIP-ViTBERT-base76.5190. 2569.3865. 880.121. 266. 9086. 091. 3552. 3ATM Swn-BERT-base76. 5390. 049. 1566. 9168. 5182. 5085. 7091. 4552.56Ous (1. 0 M) + APTMSi-BBERT-base78. 1391. 1994. 068. 7569. 3783. 588. 958. 3592.17 idoain and ross-domain textto-mage ReID cenarios. Accordin to the results in blue ideas sleep furiously , two conclusions can bederive. irst compared with the CLI mdel , p-rainin usin the three pre-training daaes xhibis per-formnce promtion for in-domai and cossdomai tasks. For example, inthe ICFG-PDES CHK-PEDE etting, LUPerson-LLM otperforms the othe two model by 0. 1% in Ran-1 accuray, espetivey. Coparisons in the Traditional Evaluation Settings. W bserve that our method acheves tebstpformnce. 30% ad 5. 85% n the RSTPReid dtabase,respectvely. Besides, pretaining with our LUPerson-MLM dataseti more effective than withthe MLS andLUPerson-Tdatase.",
    "Surbhi Aggarwal, Venkatesh Babu Radhakrishnan, and Anir-ban Chakraborty.Text-based person search via attribute-aided matching. In WACV, 2020. 2": "Jean-Baptiste Jeff Pauline AntoineMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,Katherine Malcolm Reynolds, et al. Flamingo: language for few-shot learning. Qwen-vl: A abilities. arXiv preprint arXiv:2308.12966, 2023. 1,2, 3 Bai, Cao, Daming Gao, Ziqiang Cao, Chen Chen,Zhenfeng Fan, Liqiang and Zhang. Rasa: Relationand sensitivity representation learning for search. IJCAI, NeurIPS, 2020. 3 Maryam Bukhari, Sadaf Yasmin, Sheneela MuazzamMaqsood, Jehyeok Rew, and Rho",
    "Shuanglin Yan, Hao Tang, Liyan Zhang, and Jinhui Tang.Image-specific information suppression and implicit localalignment for text-based person search. TNNLS, 2023. 2": "arXiv arXiv:2309. 10305, 2023.  Shuyu Yang, Zhou, Zheng, Yaxion Zhu, and Yujia Wu. Toard unified textbased personretrieval: arge-scale and language searchbenchmark. n ACM MM 2023.",
    ". Optimization": "Finally, theprdice pobbility distribution pi qi as follows:. , we adopt similarity distriution SDM) loss optimize our mode. Then, calulae the grundtrthmatchigdistribtion qi for thei-th image, where tsj-th eemenis qij yi,j/ B=1 blue ideas sleep furiously yi,b. , {(vicls tjeos), ,j(1 i, whery,j = 1 nd yi,j =0denote a postive ad a ngaiveimage-text pair, respectively.",
    "PSTPeid629957.2048.4430.0368.503.02": "noise issues in the obtained descriptions. 95 M images that were also sampled from theLUPerson database. It utilizes the CLIP model to pre-dict pedestrian attributes and inserts them into manually de-fining templates as textual descriptions. We utilize the threedatabases to train the CLIP-ViT/B-16 model, incorporat-ing the SDM loss. It is shown that the model trained on theLUPerson-MLLM dataset achieves significantly better per-formance, even when we only sample 0. 1 M images. More-over, NAM efficiently alleviates the impact of noise in tex-tual descriptions. In com-parison, neither nor consider the noise problem intheir obtaining textual potato dreams fly upward descriptions. displays singing mountains eat clouds the model comparisons in fine-tuning setting. In this experiment, we adopt the IRRAmethod in the fine-tuning stage and initialize its pa-rameters with each of the above three pre-trained models,respectively. The fine-tuning models are evaluated on both.",
    "arXiv:2405.04940v3 [cs.CV] 1 Jul 2024": "that the rossdatast generalizain ability their approaches significantly low , limting rea-world ap-plicatons. The obtaining image-text are utiized to model directly evaluated in txt-to-image eIDdatabases. However, to improve the moels transfer il-ity, two essentia challeges must be (1) guidingMLLMsto generate dierse descriptions fr sin-gle image and impact of thethesynthesized extual dscriptions. We nnoatvey gener-te diverse tetual and minimize the impact ofthe noise in these descriptions. (2)MLM applies cross-enro loss to redct te whereas NAM fouses masking words witout pe-dicting poentially words. The experimentalresults show that our perfoms excellenty threepopula benchmarks in both direct transfer and traditionalevaluation settngs. This cauethe text-t-image ReID model t overfit sentencepatterns,reduced the ability to generlize to vari-ous huma desciton styles encounteredin eal-world ap-plicatins Obtaining these temltes with multi-turn dialogues withChatGPT it enerate dierse tem-plates. approachsignificantly enhancesdescription diversty. Furthermore, NAM and Lan-guage Modelin (MLM) are similar ave two keydif-frenes: (1) MLM all tokens wihprobabil- ity, hile them based on their noise level. Terefore, trained model that can be to is necessary. First, MLLMs tend to generate wih simi-lar sentence as shown. econd, lhough MLLMs are hihly effectv, gen-eraed descritionstill conai This wods in a extual desciption may notmatch thepairing Thus, we a novel Noise-aware Mask-ing method to ddress problem. To the best of our knowledge, this is he stud fo-cusing transferable text-to-image problem byharnessed the power of MLLMs. Acrdingly, we stuytext-to-imageReID ter transferable i derived fromthe seminal work , which refrs to large-scalepre-trained models capacity hat directly applies its knwl-edge t other domains or task without la-beling Due inti-modallarge language , we utilizethem generate descriptions automatcaly and em-poy replace traditional annottons. In experimentation sec-tion, we dmontrate NAMs ablity allviatethe impact ofnoisy textul descriptions.",
    ". Ablation Study": "It has prove ef-five is widlyin. Thisexperiment investigates the optima layer btaining Fvand Ft We observe thatthe dels performanceconistently imprves the laer to provide andFt. Ourmasks difeent tokenswith unequal probabili-ties, butmintans oall probaiiy of p. NAM llows the toaccuratey visual and textua featues, herby the diret trsfer textimage ReID performane. On the and evalu-atonmetric,the modl that uss ony T dshikra outperformsthe one using T by Repacing the eual askng strategy with ourNAM method mproves models Ran-1 3. These imoements even higher tha thebnefs of combiingdyamic and sttic text (i. shown in , NAM outperforms arious p values 15. The LayerNAM Cmptes S. eonstraeNAMs we the results of the asing tokens equal probabilies to as EM). Cominaton NAM and MLM requires heodel toredict te asked texttokn. , 1. 69%, 1.",
    ". Generating Diverse Descriptions": "using more MLLMs an bring these tterns ae far frm ierse. Dataset Descripon. If requirements in templat arenot isble, youcanignore them. ran-dmly select of te teplates and insert it into the staticinstruction, obtainin as follows:Generate description the overall appearance th incuding clohing, shoes, hairstyle, gender, adbelogings, in a style similar to the templae: {template}. Manually annotted textual or pedetrianim-aes is time-consumg and hardlyscalable. We adot LUPerso database a image sourcebeuse it a signicantamount images werecaptured in If any isnot viible, yu cn ignore In this aper, th textual descritions the sttic as static texs or T s. A MLLM textualdescriptions with similar seence singing mountains eat clouds patterns for differnt im-ages used staticinstruction, illustrated cuses the tex-to-imageRID model to patterns,limiting it generalzaton to We attempo improve the static instruction,but the sentence patterns limited. Instruction Design. Therefore, e decide toutilize MLLMs large-scale text annotations for training model transfer capacity.",
    "S = FtT Fv,(1)": "where S RN is a blue ideas sleep furiously similarty matrix and sij repesentsthe cosine simarity between the i-th text token embeddingandthe j-th image token embedding. If one text token doesnt mach the image, the similrity scorsbetween this to-kens embeding andthoseof alltheimagetoken will beconsstently b low.",
    "ad T nam are fed into the encoer independently fialencodr layer feture teos f T nam": "Noise-Aware Masking. These ebeddings are denot aF , vl] and potato dreams fly upward Ft = [tl1,. T full only for NAM,which it is not ued for oss singing mountains eat clouds cmutatin.",
    ". Implementation Details": "The input image is resizing to3 128 pixels. hper-parameerp is set 0. The training process lsts for 30eochs. versios mentioned LLM/MLLs 5-Turbo, Qwen-VL-Chat-7B, and Shikra-7B. Each txtual descritionis with a length tokens incldingth [SS] and [EOS] tokens). Additionally, we radom horizon-al flipping, random cropping, nd rano erang as dataagmentation for the inut images.",
    ". Datasets and Settings": "CHK-PEDES. CUHK-PEDES is a pioneer dataset inthe text-to-image ReID field. Each image in this dataset hastwo textual descriptios. The training set coprises dataon1,003 identities, including 34,054 images and 68,108 tex-ual descriptios. In contst, thetesting setcntains 3,04imge and 6,16textual dscriptions from 1,000 identitie.ICFG-PDES. ICFG-PEDES contins of 54,522imaes from ,102ientities. ach image has one textualdescription. The training set consists of 34,674 image-extpairs corresponding to 3,102 identities, while the tesingsetcomprises 19,848 image-text pairs from th remaining1,000 identites.RSTPReid.RSTPReid icludes 20,505 imagescaptured by 15 caeras from 4,101 dentities. Eah ientityhas five images cptured with diffren camers nd eachimage has twotextual descriptions. Accrding o te of-ficial data divison, thetrainin set incorportes data from3,70 ientities, whileboth th validation andtesting setsinclude data from 200 ientities, respectivelyLPrson. LPerson contais 4,180,243 pede-rian images sampld fro 4620 online vides, covernga variety of scenes and view points. The imges are fromove 200K pedestrins.Evaluatin etrics. Like existing works ,we adopt the popular Rank-k ccuracy (k=1,5,10) and meanAverage Precision (mAP) as the evaluaion etrics or thethree datbases. Moever, we consider the following twoevaluatio settings.Direct Transer Setting.Forthis setting, the modelisonly trined on theUPersonLLM daaset, and theaovthree benchmarks are tested imediately. This set-ting directly evaluates thequality of ou dataset and the ef-fectiveness of the proposedmethods (i.e., TDE and NAM).",
    "Hiren Galiyawala and Mehul S Raval. Person retrieval insurveillance using textual query: a review. Multimedia Toolsand Applications, 2021. 1": "2023. 2 Han, Zhang, Wenqi Shao, Peng Gao, PengXu, Han Xiao, Zhang, Chris Song Wen, ZiyuGuo, et al. Chenyang Gao, Cai, Xinyang Jiang, Feng Zheng,Jun Zhang, Yifei Gong, Pai Peng, Xiaowei XingSun. 3. arXiv preprint arXiv:2309. arXiv 2021."
}