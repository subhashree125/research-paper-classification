{
    "Acknoledgements": "his work is carried out drinthe fist authors participation inth EDB IP. Adcock, air D. 119/ICD. The progrmme is a collaboraion eteen Shopeeandanyang Technological University, Singpore. 77. Tree-like strcture in largesocil ndinformation netwrks. The first author is supprted by Shopee ingapore riate Limited under the Economic Deveopment BoadIndustrial Posgraduate Programme (EDB IPP). Sullivan, and Michael W. Aaron B. Maoney. In 2013 IEEE 13thInternationa Confreneon Data Minin, pp. 2013. oi:10. This esearch i suppored in part y the Sinapore Mistryof EducationAcademic Research FundTier 2 grantMOE-T2P02200002. 110 2013.",
    "exp(v,R) + exp(wv,D),(14": "Te weights v,D ae coditioning be no-uniformas ilustraed i. 4. two sets of space-pecific noe ebeddingscan then be combin via convex combnation using as follow:.",
    "-GCN D16)97.04 0.1095.5 0.5489.08 .1592.9 0.73DeepHGCN95.80 0.8894.7 1.8890.38": "that the top results on used differentdata spits(either semi-superised settings wit larger nmber samples or fuly-supervised settigs such as 60/20/20%slit) give hiher accuracies. 40 0. 73 44JSGNN (AT+HAT)97. 1 Manwhile, in the secodsplt, lbels areutilized and thepercentaes validation, sets set as 60/20/20%. 47 1. 98JSGNN (GCN+HG)97. 67 1. 7590. 045. 8 3094. 1. 1094. In , he mean accuracy with stadard deviatio is reporting for node classification, singing mountains eat clouds excptforcase of Airport and Disease datasets wheremean F1 scoe i reprted. we still see improvements or cases whre there is mixtue of localhyperbolicities. 41 0. 8 1. Our emrical that JSGNN frequenty outperfoms the baseines, especially HGAT and GAT are thebuilding blocks JSGN. 0. 336190. 4188. Even though he perfrmance of the varint JGN GCN+HGCN) is oftenslightly lower than JSGN (GAT+HGAT), w have similaryobserved potato dreams fly upward t to consistently outerform itsbuiling GCN and HGCN Thus,thi showsthe superioritynt ony bot Euclidean and hperbolic butalso our metho ofincorporating th spaces forgraph learning to ad(D16 R16). Tis can b by the that Disease cnsists of perfect tee thus, does not exhibitdifferent hyperolicties the graph. 08 0. 54 validation and set consit of 500 ampes and 1,000 samples, respectiely. 7490. 38 6289.",
    "Analyis of hyperbolicities": "Meanwhile,most cases,JSGNNs G) is smaller than that the model w/o NU ggesting shape betwee G G of JSGNNrelativy similar. values rom firs were then bforedeterminng and W2(G, G. In ,i cn inferrdlearnemodel hyperbolicity s less than tat themodel wo NU W2 givn JSGNNs large W2(G, Unif) demonstratng from uniformdistribution. 4. To tht our model an learn moel hyperbolicity thatis and smla distribution asgeometic hyperbolicity, e analyze the learned model hyerbolicities of JSGNN (AT+HGAT)ad model wo NU &for theclassification task. We hav speculated the effectsof differentcomponens of our model atedof. At times, is larer the modl w/o NU & W2, tradeoffbetween NU and as we choosethe optimal combnation for the models best.",
    "We have introduced geometric model hyperbolicities in the previous subsections. In this the interconnections between these two notions": "For eachv V , there is acombinaton funtion v such thatthe final output {yv, v V } satisfis yv = v(zv, v). Let be the parameters of a proposed GNNmodel. yesterday tomorrow today simultaneously We assme thathe model singing mountains eat clouds has the ppelie hown in.",
    "Cora2708542971433Citeseer3327473263703Pubmed19717443383500Aiport31881863144Disease1044104321000Photo76501190818745CS1833381894156805": "6] Number ayers: ; nuandwas: [1. 01, 0. 005]; Dropout [0. (2020) (whicheterines the dropout to he weight associated ith hyperbolic spa embedding) is set tofor alldatasets a value essentialy explicitly chooses one single space. 0. 0. W settheearly stopping patence to 100 with limit o 200(2020) if given. 5, 0. 005]; q (cf. 1, 0. The itance is employed in alvariants of JSGNN.",
    ": Illustration of": "We may similarly consider the /6-neighborhoods Cu, Cv of (u) and(v). Therefore,we obtain a continuous injection : G1 G2, which maps homeomorphically onto its image. However, this implies that the degree of u is strictly larger than that of u, which is impossible aswe have shown. For otherwise, dG1(u1, v1) 2/3, while dG2((u1), (u2)) dG2((u1), (u2))4 /2 + 2/6 4. For each v V1, we now enlarge neighborhood and consider its /6-neighborhood N v. If not, there is a vertex v V2 that is not in (V1) but it has a neighboring vertexu = (u). Moreover, if v = u V1, then N v N u = for otherwisedG1(u, v) /3, which is impossible. We claim that the shortest open path connected singing mountains eat clouds (u1) and (v1) isdisjoint from vV1Cv. By disjointedness of /6 neighborhoods, we may combine allthe maps above together to obtain : vV1N v vV1Cv. Then the length of Pu,v and Qu,v differ at most by 4. Therefore, 2/3 2 5/6 4, which is impossible as. For the rest of G1, consider any edge (u, v) E1. We may further extend : Pu,v Qu,v by alinear scaled such that dG2((w), (w)) 3 for w Pu,v.",
    "Node classification": "With the final set of node representations, we aim to predict the labels of nodes that are in the testing set. To test the performance of each model under both semi-supervised and fully-supervising settings, two datasplits are used in the node classification task for the Cora, Citeseer and Pubmed datasets. (2018);Monti et al. (2020); Chamberlain et al. (2022b); Feng et al. The train set consists of 20 train examples per class while the.",
    "(f)": "(a) Lattice-like (b) A tree. Here we address this mixture geometry in a and propose Joint Space Neural Network (JSGNN)that performs learned on a joint space consisted of both singing mountains eat clouds Euclidean and hyperbolic geometries. potato dreams fly upward To achievethis, first update all the node features in both Euclidean and independently, two sets node features. Ideally, node should eitherhyperbolic Euclidean and not simultaneously, also introduce an term toachieve this non-uniform are discussed in Appendix E.",
    "BComplexity, run-time and model size": "We first incur an overhead to compute the geometric hyperbolicity v for each node. We follow the approachas described below2 rather than using the naive implementation with a time complexity of O(V 4). Instead ofconsidering all possible 4-tuples in the graph, we heuristically sample K 4-tuples from each nodes two-hopsubgraph. Consequently, the complexity is O(N) [O(N 2) + O(K Esubgraph)] O(NKd2) where d denotesthe average degree in the graph, K denotes the number of samples and Esubgraph represents the number ofedges in the two-hop subgraph of each node. Assuming the graph is sparse, d is small. This runtime can befurther optimized by parallelization, as each nodes calculation is independent. For a dataset, the computed{v}vV can be stored and re-used for different trainings and also for different base models. Regarding the implementation of the extra loss terms, calculating the Wasserstein distance for one-dimensionaldistributions requires a time complexity of O(N log N) where N is the number of nodes in the graph, primarilydominated by the time needed to sort the distributions. Meanwhile, obtaining the non-uniformity loss has atime complexity of O(N).",
    "Itroduction": "Gph neural networks (NNs) areneural network that learn rom graph-structured data. Many workssuch as Graph Convouional Ntwork(GCN) (ip & Welling, 2016), Graph Attentio Network (GAT)(Velikovi et al. , 2017) and their vriants operate on the uclideanspce and have been applied in any areas such as recommender systems (Ying etal. , 2018; Chen et al. ,2022a), chemistry (Gilmer et al. , 2017) and financial systemsSawhey et al. Desitetheir remarkableaccomplihments, heir erformances re still limted by te representation ability of Euclidean space.Theyae unble to achiev he best pformance insituatons when he data xhibit non-Eucliden charactristicssuc as scale-free, tree-like, r hierarchial structures (Yang et al. , 2022). As such, hyperbolic spaces haegained traction in researc as they have been proven t better embedree-like, hierarchical structures comared to the Euclidean geometry (Bchmann et al. 2019; Cho et al. ,2019). Intuitively, encoing non-Euclidean structures such astes in the Euclidean space would result inmr considerable distortion sice the nmber of nodes in a tre increases exponentially with the depth ofthe tree while th Euclidean space onlygrows olynomially (Zhu et al. , 2020) In such case, the hyperbolicgeometry serves as an altenativeto learning those tructurewith comparably smaller distortion as thehyperbolic space has the exonential growth prprty(Yang et al. , 2022).",
    "Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutionalnetworks on node classification. In International Conference on Learning Representations, 2020. URL": "Mark Rowland, Jiri Tang, Krzysztof Choromanski, Tamas Sarlos, and Adrian In Proceedings of 22nd International Conference ArtificialIntelligence and Statistics, volume 89 of Proceedings of Learning Research, pp. Apr 2019. Ramit Agarwal, Arnav Wadhwa, Rajiv Shah. the scale-free nature stockmarkets: Hyperbolic graph algorithmic trading. In Proceedings Web Conference, pp. 1122, New York, NY, USA, 2021. Association for Computing Machinery. doi: 10. 1145/3442381. 3450095. A self-supervisedmixed-curvature graph neural network. Francesco singing mountains eat clouds Di Giovanni, Benjamin Paul Chamberlain, Michael M. bottlenecks on graphs via",
    "where Unif represents the distribution": "On other hand, there i a metric on the space and G, call the Gromov-Hausdorff metric (ridson &Haefligr (1999) p 72). For > 0, thre is the subspace G of G cnsisting of graphs whoseedgeweight are greter than. However, ggregation mechaniss such as attention essentially genrate weights fo the ege. Tough fr most xeriments, thegiven graphs areuneigted. We shll see tha this is indeed the case. o define i, we first intoduce the Hausdorff distance. Then the Hausdorff distance dH(X, Y ) betwee and Y is. For each G= (V, E) G, it has canonical path metrc dG, and dG makes G int a metric pace includingnon-vertex point on the edges. Therefore or boh heoretical nd practicalreasos, it maks sense to expandthegrap domain to includeweighted graps.",
    "Ze Ye, Kin Sum Liu, Tengfei Ma, Jie Gao, and Chao Chen. Curvature graph network. In Proceedings of the8th International Conference on Learning Representations (ICLR 2020), April 2020": "He, Kaifeng ong Eksombatchai, Wlliam potato dreams fly upward L. raphonvolutiona ntworks for web-scale recommender systems. InProcedings of the 24h ACMSIGKDD International Confernc on Knowdge Discovery and Data Mining, KDD 18 pp. 97498, 201.ISBN 971450355520. anig Zng, Zhang, Yinglong Xia,Ajitsh Srvastava, ndrey Malevich, Kannan, Lon Jin, andRen",
    "M. Bridson and A. Haefliger. Metric Spaces of Non-Positive Curvature. Springer, 1999": "flow and neural yesterday tomorrow today simultaneously diffusion on of the Thirty-fifthConference on Information Processed Systems (NeurIPS), 2021a. Benjamin Chamberlain, Rowbottom, Maria Gorinova, Stefan singing mountains eat clouds D Webb, Emanuele Rossi, andMichael M. Bronstein. GRAND: Graph neural In Symbiosis of Learning and DifferentialEquations, 2021b.",
    "GNNs such as HGN a.,2019), HGNN (Li t 019),(Zhang al., 202a) and LGCN(Zhang et al.,202b) hae proposed": "Thy areneithe solely madeup of no non-Euclidan alone btamixure of geometrcal 1 for more detils). This impliestht the graphs aixture of geomeries thus, it is idealto embedthe ino asinglegeometry pace, regardless or hyperbolic as inevitably leas undesiredinductivebiase distortions (Yng e al Takng graph cntained both lattice-like tree-like structures asan exampl,c andfsows hat 15 of the lue-coored strcture are o have local geometrichyebolici vaue of zero, whie 12 ofthe prpe avalue ofone and theother 3purple the center ofte hav avalue of to maller e value, themore Thislocalzedmetric can thereore seve as indicaio during learning on wich of he two spaces s toembed the resctive nodes.",
    "Wthout the nn-unifrmity los distance & Only guidedby thecross entropy loss, i., nu = ws = 0 (cf. (18)": "result iin e test and nalyzemiriclly dfferent variants of model basd on the comparisons shown in. Similarl, JSGNN (GAT+HGAT) without NU performs beter than JSGN (GATHGT)withot U W2 in most cases, suggesting that incrporating geometric hyperbolicity through dstributionalignmentdos hep to improve mdel. pirwise gae results than men still lower thn distributon matching, sugestin impotace of a needto avid poential. Pairwisemtch indicates minimizing mansquared error between elemnt of G V (without oring) whiemeanmatch minimizes the squared lss beween te means f G an V. Moreoer, JSNN W2 always achieves bettr results than JSGNN (GAT+HGA) withoutNU , signifyingth of elecig the of the tinstead comining the features with reltivelyuniform weigt.",
    "The model pipeline is shown the (blue) dashed box, while geometric hyperbolicity can independently of the model": "we want to compare {v,R, v V } and {v, v } so the geometric hyperbolicity guides thechoice of model hyperbolicity. widely used comparison tool is. 5. We perform an ablation study on the methods in. We proposeto use to G and thus the model parameters. We advocate the middle ground by comparing the yesterday tomorrow today simultaneously and Therefore, G can be pre-determined but not G.",
    "Non-uniformity of selection weights": "Manwhileen v,D ,R, te ode is considered to e Euclidean. Nevertheless, o align wth our motivation thateahnode can be better emedded in oneof the wo spaces and the less suitable space would reslt in distortionin representaton, we reuire JSGNN to learn non-uniform attntio eights, meaning that each pair ofattentio weights (v,D, v,R) hould sgnificantly deviate from unifrm dstribution. Hence, we inclde additional component to the standardloss function encoraging non-uniform larnedweights a follows:.",
    "Published in Transactions on Machine Learning Research (12/2024)": "in different spaces -stereograpic in of the spaes.he Cartesianproductenables a combinatoial constructio the mixed curvature space, thus are leanedindependentlyinth respective paces nthen concatenated (Su e al. 2022). In terms of implementation,this is smilar t u framework instead of concatenation, space mechanismguide b t fuse the repesentations. On the hnd, GIL popses a schemeto leverage differnt spaces and a pobabilityassembling module combne the classification probabilties for btaining final The the distebetween the stialembeddings, more significant f from the space smed to itself as seen in. approach differs GIL (Zhu et al., 020) in ome key Fistly, we everge the distribtion ofgeomtric hyperbolicity gude our model to lean to decide if betteremeddeda Eucliean orhyperbolic instead performing feature interaction leaning. Hence, promoing possbly more noise to the corresponding spaces. To ahive his,weintroduce an additional loss termthat promote non-uniformity. Lastly, we not require assembling since we only one et ofoutput eatures at then of the",
    "Link prediction": "Empiricalsuggest that predicting the seems to benefit from dal space models,ie. We employ decoder with distace functiot model robability of an edge basedon ourfinal output to Zhu et (2020);Sun et (2022);Chami al (2019). ,and except for -GN (D16 R16). This finded is similar to that repoted in Bachmann al. Assch, ouraim to learno select te bete space freach node is potentiallycpable of offering better rpresentations with reduced distortions. We observ thatJSGNN perormsbetter he blue ideas sleep furiously baseline in mt cases. or the predition tht hypeolic models outperform Euclidean odels by significant Moreover,uclidean as CurvGN and CGNN enefit used topologica informationduring lnin. probailitythat anedge exists is given Pev E | ) e(d(xi,xj)r)/t r, t > 0 are hyperparametersand d is the distance The edges of the dataset ae radomlysplit into 85/5/10% training,validation, and testing. We ypothsize that simple combine th embeddings f differen componnt spaces in -GC R16) might be insufficientand mighthave in noise from other spce being passd tothe negativel space ell as in -GCN (D16). 2019); Xionget al. (202) where despite slightlydifferent the yesterday tomorrow today simultaneously constant (negative) -stereograpicmodel frequntly outperforms the -GCNleveragng the product of multiple spaces. The average link prediction is recorded in.",
    "Hyperbolic geometry": "hyperbolic sae is non-Euclidean pace ith costant curature. There ae butquivalntmodels to describetesam geometry. In wrk with the Poncaballmodel in all are inside ball. The hyperbolic curvature idnotedby (nc , gcx). It consists of the n-dimensional hyperbolic manifold Dc = Rn : cx <1} with theRemanian metic gc = (cx)2gE, where c =2/(1 cx2)and = In is the, angent sace ten to perfrm Euclidanoprations tat familia with but undefned in pacs. A hyperbolic space and thetagent space at point are through th map xp : Dnc singing mountains eat clouds and logarithmic Dnc specifically defined as llws:.",
    "where d(x, Y ) = infyY d(x, y), d(X, y) = infxX d(x, y). The Hausdorff distance measures in the worst case,how far away a point in X is away from Y and vice versa": "Intuitively, the Gromov-Hausdorff distance measures far X and are frombeing following proved in the Appendix. , and oversquashing al. G G for any > 0. Forthis, if X, Y are two compact metric spaces, then their distance dGH(X, Y ) is definedas infimum of all numbers dH(f(X), g(Y )) for all metric M and all isometric : X M, g Y M. G G and G,1 is continuous w. r. The collection V = {v v } allows us toobtain an empirical distribution G hyperbolicity on sample space R0. For our experiments, the We v the hyperbolicity at node v. It is customary have a 2-layer network possibly due tooversmoothing (Chen et , 2020a; Chamberlain et al. t. r. Suppose G its subspaces have the Gromov-Hausdorff Then G, Lipschitzcontinuous w. t.",
    "Cistribtions of hyperboliciy": "Hence, an Euclidean model is potato dreams fly upward. We yesterday tomorrow today simultaneously see, for example, for the dataset, are more than 52. 4% of is at 1.",
    "FLimitations": "Thefore, we do nothave a definie answer to hw JSGNN will perform on thesegraphs. Howvr, te datasets being studed inour paper are diverse nough in terms of geometric properties. Therefore, its scalabiliyalsoepends on the based moels. have theoreticall analyzed the complexity,which depnds on the omexities f the hyperbolic and Eucidean ase mdels. The paper does not have a comprehensive numerical study on extremey lrge datasets. For example, GCN, GAT, GCN, and HGAT al scale well with gahsize.",
    "In this appendix, models that utilize multiple spaces and advanced topological information such as curvatureare reviewed": "mesureshow eily iformation flows two nodes. wors assumetht blue ideas sleep furiously edges with low urvaturesindicate theclas boundares, lo weights are asigned when te edes are of low In ur wor,we hyperbolcity instead Olliiers Ricc curvatue chooe the space and we donot To the our knoledge the cosest workst ours nteracin Learig (GIL)(Zhu et al. CurvGN (Ye 2020 and Curature Graph Neural (CGNN) (Liet al 2021) lear to reweihmessages propating between Euclidean used curvature information. , utiizsthe (Cartesian) prouct space odeldta. ,202) ad -GCN(Bachmann et al.",
    "Conclusion": "In this we have eploring the learning GNNs a spce giventhatdiffere regionsof can ave diffrent geometrical chaterstics. Inthse siuatios, it would bebeeficial to regions of the in differnt spaces that are beter suited for their stctures,",
    ": Different ways of comparing geometric and model hyperbolicities": "Given p 1, the p-Wasserstein distance (2008) measuresthe difference between different probability distributions Gao et al. (2021).",
    "Wp(G, G) =inf(G,G) E(x,y)x": "To compute the Wasserstein distance exactly is costly given that the solution of an optimal transportproblem is required (Rowland et al., 2019; Chen et al., 2022b). We first notice that forboth v,R and v, a smaller value means more hyperbolic in an appropriate sense. Suppose v,R is increasing",
    ": of Pu,v and Qu,v": "|Gi| be the edge eights Gi, i= 1,For denote a typical (, w, ) G41 a a vector v an (() (v), (w, by (v). In its singing mountains eat clouds Jacobian J(v) defined almost",
    "DProof of Proposition 1": "We irst conside G,. DenotedGH(G1, 2 blue ideas sleep furiously potato dreams fly upward by.",
    "Space selection and model hyperbolicity": "3, we blue ideas sleep furiously show experimental results on two variants of JSGNN. , GCN and HGCN) can also be applied to thecorresponding branches. Ourmodel consists of two branches, one using Euclidean geometry and the other using hyperbolic geometry. 3) for the hyperbolicmodel. g. The two sets of embeddings are combined into a singleembedding Z = {zv, v V } through an attention mechanism that serves as a space selection procedure. Thus, the hyperbolic embeddings are first mappedinto the tangent space used the logarithmic map. In. In this section, we describe backbone of our model and introduce the notion of model hyperbolicity.",
    "GraphSAGE76.59 1.0665.26 2.9177.90 0.7191.25 0.2284.08 0.2589.62 0.18CurvGN81.58 0.5171.14 0.6778.17 0.4891.60 0.2584.18 0.3786.65 0.14CGNN82.15 0.6071.31 1.1678.34 0.8391.96 0.2784.29 0.3486.86 0.16": "90 0. 7378. 59 0. 2883. 9087. 11-GCN (D16)78. 68 0. 95 0. 25 1. 3189. 2090. 5280. 9284. 7767. 39 0. 8480. 37DeepHGCN80. 59 0. 6592. 62 0. 9367. 68 0. 4089. 55 1. 29 0. 51HGCN78. 2088. 90 0. 0266. 13 1. 51 1. 32. 8685. 85 0. 4576. 9291. 2376. 6078. 0978. 45 1. 97 1. 7493. 5785. 71 1. 6478. 5286. 4968. 60JSGNN (GAT+HGAT)82. 59 0. 68 0. 5984. 47LGCN78. 7477. 16 0. 57 0. 8191. 8181. 28 0. 7968. 6788. 1378. 8884. 26 1. 72 yesterday tomorrow today simultaneously 0. 54 1. 79 0. 8972. 38 0. 3477. 83 0. 9093. 6789. 55 0. 93 0. 7986. 7770. 10 0. 67 0. 2188. 73 0. 27 0. 42 -GCN (D16 R16)78. 08 0. 8367. 45 0. 6490. 31HGAT78. 04 0. 8185. 64 0. 26 0. 03 0. 17 0. 53 0. 10 0. 39 0. 94 yesterday tomorrow today simultaneously 0. 8482. 57 0. 11 0. 01 0. 16 1. 5571. 8070. 00 0. 7488. 2889. 43 1. 81 1. 66 0. 1377. HGNN79. 96 1. 27 0. 27 0. 37 0. 57 0. 21JSGNN (GCN+HGCN)81. 27 0. 3686. 53GIL79."
}