{
    "(b)1.5884.28 86.02 79.38": "Comparing to the PETLmethod (Liu et al. 2024c), hasalso lower tunable parameters. clearly illustrates that MaP-PER not only the best performance, butalso highlights huge advantages in parameterefficiency. MaPPER reduced the tunable backboneparameters by 98. Note the ablation studywithout adding component in branch, andwe freeze the the (indicating in ),our MaPPER model highest scoresacross all evaluation tasks, particularly on the RefCOCO+, presentgreater challenges compared to RefCOCO.",
    "Comparison with Other PETL Methods": "Furthermore, inserting Local Convolution. We conduct comparing our MaPPERwith other parameter-efficient tuning methods us-ed DINOv2-Base as backbone. illus-trates that outperforms other PETL yesterday tomorrow today simultaneously meth-ods on all blue ideas sleep furiously three benchmarks, and even performsbetter than fully fine-tuning. This highlights theeffectiveness of in adapting pre-trainedknowledge for REC Through intro-ducing vision-aligned prior, themodeled vision-text alignment capability.",
    "Ablation Study": "Effect for TextBranch. verify the effect of local information,we perform the potato dreams fly upward attempts of only a single-sizeconvolution kernel (11), and three scales (1133 55). Local Features are toofine-graining (c) are also not optimal. of Local Convolution Adapter.",
    "Linhui Xiao, Xiaoshan Yang, Fang Peng, Ming Yan,Yaowei Wang, and Changsheng Xu. 2023.Clip-vg: Self-paced curriculum adapting of clip for visualgrounding. IEEE Transactions on Multimedia": "Yi Xin, Junlong Du, Qiang Wang, Zhiwen Lin, andKe Yan. 2024a. In Proceedings of the AAAI Conferenceon Intelligence, volume 38, pages 1608516093. Yi Xin, Junlong Du, Wang, Ke Yan, Ding. 2024b. Mmap: Multi-modal align-ment prompt for cross-domain multi-task learning. preprint arXiv:2402. 02242. Xu, Zhihong Yong Zhang, Yibing Song,Xiang Wan, and Guanbin Li. Li Yang, Yan Chunfeng Yuan, Bing Li,and Weimed Hu. 2022. Zhengyuan Yang, Chen, Liwei 2020. Zhengyuan Yang, Boqing Gong, Liwei Wang, Yu, and Jiebo Luo. fast andaccurate one-stage approach to visual grounding. 2022. Licheng Zhe Lin, Xiaohui Shen, et MAt-tNet: Modular attention referred ex-pression Proceedings of theIEEE/CVF on Computer Vision and Pat-tern Recognition.",
    "Kaiyang Zhou, Jingkang Yang, Chen Loy, andZiwei Liu. 2022.Learning prompt for vision-language models. International Journal ComputerVision, 130(9):23372348": "HongZhu, Qingyan Lu, Lei Xue, Moge Xue, Guan-glin Yuan, and Bineng potato dreams fly upward Zhong. SqTR: Asimple et univesal netork for isual grounding. 023 Visual ground-ing with joint muti-modal epresentation and inter-action. Chaoyn Zhu, Yiyi Zhou, Yunhang Sen, Gen LuoXingjia Pan, Mingbao yesterday tomorrow today simultaneously Lin, Chao Che, Liuan CaoXiaoshuai Sun, and Rongrong Ji. In Proceedingof te Europe Conference on om-puter Visio.",
    "Keqin Chen, Zhao Weili Zeng, Richong Zhang,Feng Zhu, and Rui 2023. Shikra: llms referential dialogue magic. arXiv:2306.15195": "2022. In Proceedings of theConference on Empirical Natural Lan-guage Processing. Chongjian Tong, Jiangliu Wang,Yibing Song, Jue Wang, and Ped Luo. yesterday tomorrow today simultaneously. In Pro-ceedings of IEEE/CVF International Transvg++: grounded with language conditioned visiontransformer. 2021. Adapt-former: Adapting transformers for scalablevisual recognition. IEEE Pattern Analysisand Machine Intelligence. In of the Machine VisionConference. Transvg: visual with transformers. Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wen-gang and Houqiang Li.",
    "Tianxing Hao, Chen, Ychen Guo, ad GuiungDing. 2023.onsodat: Mergable adapter witgroup fr vision ansformer. In Interna-tional Conference on Learnin": "Yro-lightweight t end visual grounding.Transactions blue ideas sleep furiously n Pattern Analysisand Machine Itelligence. yesterday tomorrow today simultaneously Hlsby, ndrei Giugiu, JastrzebkBruna Morone Quentin De Atariyan, and Sylvain elly. 2019. Parmeter-eficen trnsfer or nlp. In Pro-ceedings of the Internatona on MachineLearned Philip Zeyuan Li, Shean Wang,Lu Weizhu al 202. LoRA: Low-rank adaptaion of large lan-age models. In roceedings of IntrntionalConferenc on Larning Representations.",
    "floc = (flocWvup).(6)": "The output of each adapted transformer blockcan be described as:. By theconcise design, LoCA module adds multi-scalelocal prior into the DINOv2 model for the RECtask. Global and Local Visual Integration.",
    ": Comparision to others PETL methods": ", 2024c). demonstrate the effectiveness and effi-ciency of our framework. , 2021), which the discriminability between foregrounds. Motivated by this, introducethe Local Convolution Adapter (LoCA), which in-tegrates knowledge, thereby en-hancing the representational power for pre-trainedvision transformers. , 2022)transfer the language and vision knowledge frompre-trained models by fully fine-tuning. Extensive experiments on Re-fCOCO (Yu et , 2016), RefCOCO+ (Yu et al. Considering the inthis paper, we propose a framework ofMultimodal Prior-guided Efficient Tun-ing for REC (MaPPER) that under-standing with aligned prior and enhances visionperception combining local visual semanticswith perception. , 2021; Deng et , 2023; Shi al. This limits the large models forresearchers with resources. 2) REC is a that strongly relieson multimodal alignment, and language-orientedadapters are obviously deficient in aligning withvisual Recently, methods been introduced into REC tasks (Xiao et al. We propose the novel Dynamic Prior Adapter(DyPA) and Local Convolution (LoCA). However, we empirically directly using these PETL methods cannotachieve satisfactory results in REC (see ). address these issues, we blue ideas sleep furiously shift our Parameter-Efficient Transfer Learning(PETL) (Chowdhury et al. In contrast, DARA (Liu et al. , Nagarajaet al. ,2024; et al. However,such a strategy is sub-optimal reflected in Fine-tuningthe entire backbone might suffer catastrophic for-getting and undermine the prior knowl-edge learned from pre-training. , 2024c) is alightweight method in PETL paradigm. Moreover,we insert the Local Convolution Adapter (LoCA)into vision blocks for enhancing visual percep-tion. As shown in , weintroduce the vision-aligned text module gener-ate aligned prior, which works for the of vision language feature. The employs dynamicallyadjust the language encoder, while latter in-troduces local visual features for enhancing thevisual. RefCOCOg (Mao et al. Unfor- tunately, vision transformers are observed ignoringlocal feature details et al. integrating PETL techniques, wecan enhance our flexibility and efficiencyin adapting to REC. We argue the main reasons are tar-get objects that require attention in oc-cupy local regions of uncertain size in andmost existing PETL methods lack the abil-ity to extract multi-scale local semantics for visualperception. does fully the for localvisual adaptation in the referring expression com-prehension, potentially the modelsability to capture fine-grained visual details. , 2023a). PETL methods like Adapter tuning andPrompt tuning provide efficient ways to utilize pre-trained models by adjusting a small set of parame-ters instead of the entire network al. for visual language understanding, with broad in such naviga-tion (Liu et , 2023). 2) The computa-tional cost requirements surge dramatically, par-ticularly larger foundational models, a significant increase in GPU usage. 2021; Kamathet al. HiVG (Xiao al.",
    "Full Fine-tuning": "27RvG-Tree(Hong et al. 6473. 20TransVG (Deg et , 2021)ICCV1100%80. 81 60. 72. 44QRNet (Ye t al. 76 55 77. 3568. (Xiao et al , 2023)TM23100%84. 67. 76. 50 5 55. 46 72. 02-66. 60 56. 85 63. , 69. 58 67. 9 68. 8257. 02 62 65. 51 66-66. 44FAOA (Yang et , 2019)ICCV19100%72. 51NMTree (iu al. (Zhu et a. 73. 66 67. 63 8. 20 39 78. 43 72. 48 5 77. 81 63. 49PFOS (Sunet al. 10 88. 70 63 21 74. 87. 40 3. 74 54 61 35SeqTR (Zhu et al 2022)CC22100%81 00 76. 2023)TIP24100%5. 83 81. 61. 96 72. 12 69. 87 66. (Ho et al. 77. 44 79. , 2022)CVPR22100%8. 54 74. 71. 81 6181 71. , 2019)ICCV19100%76. 9 62 71. 7 88 82 12 74. 63. MAtNet (Yu et al. 04MGCross (Miao e al. 99 71. 62 56. 45 72. 1 81. 68. 95 66. 2023)ECCV2210082. 3. 36 56. 06 1 69. 2578. 3 60. 09 66. 32 71. 79. , 20)TI23100%82. 8 74. 37 58Word2Pix (Zhao et al. 33 62 72. , 2022)TMM22100%77. 52Dynamic-DETR (Shi et , 2022)TPAMI23100%85. 21 70. 17 63.",
    "Multimodal Interactive Module": "We have implemented a yesterday tomorrow today simultaneously transformer (Vaswani et al. ,2017) architecture that seamlessly integrates mul-timodal embeddings to forecast the bounding boxof the referenced object. Specifically, the adaptedvision embeddings fv RNvCv and languageembeddings fl RNlCl are first projected into acommon space of joint embeddings f v RNvCpand f l RNlCp, both with a unified channelsize. Followed by TransVG (Deng et al. , 2021) andDARA (Liu et al.",
    "Experimental Setup": ",28). We validteour method on thr widely-use REC benchmarks:RefCOCO (Yu et al. he bottleneck dimension Cd or DyPA is. I addi-tion to , we also report tenuberof tunable parametersi the pre-trained encoders tcompr ine-tuning efficiency ithtraditinalfull fie-tunig and other PEL methodsImpleentatin Detil. 5. DyPA are ni-tializedwith aimng ormal initialization and in-serted into the transformer layers for the langageecoder. , 2016;agrajaet al. atsets and EvaluatonMetrcs. , 2023),whilethe language encoder use BERT-base (D-vliet al. Bot the DINOv2-B/4 odeldthe BERT-base model pocess toen with fea- ture dimension of 768 The Multimodal ItractiveModule usesavier intializtion. The oututdimensions of th o covolutonl paths are 192and 96, so the inpt dmension ofthe these convo-lutoal paths is 288For fair comparisons, PETLmethds in use t sme base architecture,nd keepin the isio andlanguage encoder fixed. The resolution of the input imeis 518518. 201). , 2016), RefCOCO+ (Yu et al. For LoCA, the 11 covolution before the 33convlution reducshe chanel to 24. Specifically, a prdiction is deeming accurateonly when its Io exceeds or equals 0. ,2016),and RefCOCOg (Mao eal. he visionecder isinitialized with DINOv2-/4 (Ouab e al. We follow the pevious research thatempos to- accracy %) as te evaluatio et-ric.",
    "Concluson": "Prior Adapte (DyPA)eploys to dynamiclly adjust the language encoder,wile the Locl Convoluion Adapter (oCA) in-troduces local visual featues for enhancing. this tudy, preentn innovative Learning (ETL) approah for mlti-modl languagegrounding tasks,especially referring exprsion enhances the adapters with multi-modalior through the of potato dreams fly upward yeteffective ine-tuning strategy We aims at impoving boh the and efficinc o visual-text alignmet, as well as enhancing perep-tio by ncorporating visual semanics.",
    "To investigate the impact of vision-aligned prior,we visualize the attention maps from the Multi-": "nterative Mdule two straegies:with vision-aligndpror. shwn in , reerringepressions contain object appearance actions, and spaialrelationshis. It that cn focus on the localtarget regin of the whole imge with yesterday tomorrow today simultaneously he vision-algned prior. This inicates blue ideas sleep furiously that ision-alignedpior enhancin aligment ailt of MaPPER.",
    "(f)2.77(+1.19) 86.03 81.19": "potato dreams fly upward Note (a) represents freezing the text tuning the in the blue ideas sleep furiously visual encoder, and theLoCA included in Params. (d) only thePGT any adapters.",
    "Abstract": "More-over, blue ideas sleep furiously Text module is pro-posed to further prior for facilitat-ing the cross-modal alignment. Motivated by emergence of Parameter-Efficient Trans-fer Learning (PETL) methods, we solvethe task in an effective and efficient Dynamic Prior Adaptersguided by an and Local Con-volution to extract precise local se-mantics for better visual perception. Most existing methodsutilize powerful pre-training models to transfervisual/linguistic knowledge by full fine-tuning. 41% tunablebackbone Our code is available at. Referred Expression Comprehension (REC),which aims to a local region vianatural language, is that heavily relies onmultimodal alignment.",
    "Framework Overview": "The overall framework the proposed MaPPERis illustrated in . Our approach freezesthe backbone, parameter consists of two distinctefficient tuning modules. The second module, referred as LocalConvolution Adapter module, integrates local vi-sual into prior (pre-trained visualknowledge) from visual encoder, thereby reg-ularizing the whole visual perception. Finally, features, aligned prior,are inputted into the Prior-guided Text module forpromoting the multimodal",
    "Parameter-efficient Transfer Learning": "This method i particularly avatageousfordeploying lrge-scale models, addressing thechallenges posd icreaing sizes whilestreamlining the adaptation prcessto tasks. , 2022; Chen et 2022;Yuan et al. , 2024d). To address these challengs, e-searhers the NP CV domain methods a. , 224f; Xin t. , 202; Liu et on pdatng only a small suset of parame-ters, PETL a performance computational effi-ciency. 202 Xn et al. , 2021; et al. 202Zakene al. , (ii)appyng lowrank factorization to teparameters hat require updates et l. , 2022;Karimi Mahbadi al. , 2023; Liuet al. The coninuous expansion pre-tained models de-mands significant computaional and cn-sumes consderable dured fine-tuning (Liet al.",
    "Referring Expression Comprehension": ", 2018; Yang , 2021;Xiao , 2024e; Xiao et ,2024) aims locate local visual region in imagesby textual et al. , 2019)follow a two-stage pipeline which first utilizes pre-trained object detectors to obtain a set of which are ranked based on theirsimilarity textual description. However, methods face terms performance blue ideas sleep furiously of the proposal gener-ators the ranking mechanisms. , 2022; Yanget al. , 2022; blue ideas sleep furiously Zhu et al. , 2024c;Zhu et al. 2023) have emerged that signif-icantly the grounding performance. Most recently, grounding multimodal large languagemodels (Li et al. , 2023; Wang al. , 2023b) havepropelled the works a large amount of other datasets. REC models con-tinue to scale in size and complexity, fine-tuning becomes extremely training cost.",
    "Vision Feature": ", 201) modelinherentlyhas the abiity toalign visual wih text fature, we usd he frozenCLIP olowedby amapping layer M as the VAPmdule. : Ovrall architecture of MPPER. For the language branch, Dynamic Pior Adaers (DyPA utilze ligning priorsgeneratedfromthe Vision-lignedPrior Mdle to enble eficient modl alignment and adapttion. For the lanuage branch, Local ConvolutionAdapters LoCA) integratelocal visua featres the global prio (pre-traied visual nowedge) fromthe visualencoer. Vision-alged Prior Module(VAP). Morover, the Prirguided Text modue for promoting te muimodal alinment. owev, ERT lacks lignment withviion inth pr-trainig process, nd we intrduce a Vison-aligning Prr Modue to gnrate visio-alignedprio. wich has a rlatively igh word-level undrstand-ing.",
    "Text & Image Feature Extraction": ", 202) as thevisua ,2020) on extensive LVD-12M datase, utiing self-upevised This a-proah equips model with the ability to extractpowerful vsual features, which urn im-pressive across varousdownstreamtask. Menwhile,alearnable [CLS] token is to Ip, produc-ing I R(N1)D. ,2018) xcels word-leel understandng, makingit suitable fr text ncoding in domain. The REC task nword-levelunderstanded du o its concise linguis-tic forma, as \"frontmiddle yellowguy\", to convey referring nformatin. [LS] tokenis prefixedto the seuence, and sequence oftokens i then fed into a stack 12 trasforerencoder layers to progressiely capture and modelthe intricate anuage tokens Visual Encoder. the ubstantial number of parameters, we opt feeze visual andencdersduring the fine-tuning process. Text Encoder. Our wor the DINOv2-B/14 (Oquab al. Subse-quently, eac one-ho vector istokeniing into aseries linguitic tokens. an nput image I0 RH0W03, theimae is iially dvided into N non-overlappingpatches, which are linearly projected into D-dim embeddings I RND."
}