{
    "Language Bias Analysis": "formalize causal for potato dreams fly upward QA task a Structure Causal Model (SCM)(Pearl, 2009). , SCM depictedthrough directed acyclic graph G = (V, E), E represent the relationships be-tween key factors in SIQA, are nodes in V. The factors include contextualfeatures X the content of the input video),knowledge embodied in Large Multimodal ModelT, mediator variable M and the prediction Y",
    "Limitations": "We hae only valdaedthe of multiple MMs with 7b parame-ter Expeimets LMMs of ad 33bareexpected to be conctd inthe futurework. In additio, have analyzed effectso lanuage biases in LMMs through a tuctualcaul odel.",
    "Baselines": ", 2024)in a zero-shot setting. , 2023) and CREMA (Yu et al. MMTC-ESC proposesto leverage emotional cues in social interactionsthrough contrastive learning and applies the cross-modal attention module to align multimodal repre-sentations, which achieves state-of-the-art (SOTA)performance. , 2023) unifies visual representation into the lan-guage feature space to advance the foundationalLLM towards a unified LMM and achieves su-perior performances on a broad range of 9 mul-timodal benchmarks. , 2024)is an efficient and modular modality-fusion frame-work for injecting any new modality into video rea-soning and achieves better/equivalent performanceagainst strong LMMs with significantly fewer train-able parameters. We adapted VDD to make it applicable for socialintelligence QA and employed it as a baseline. For video-capable LMMs, we em-ploy two recent, strong models: Video-LLaVA(Lin et al. ,2019), T5-small (Guo et al. , 2023) and MMTC-ESC (Xie and Park, 2023). The fine-tunedsmall models include RoBERTa-large (Liu et al. We compare DCVC with both small and large mul-timodal language models (LMMs). CREMA (Yu et al. (2024) is a decoding strategythat introduces a calibration step to adjust the out-put distribution with that of the image-free input.",
    ": Accuracy on the Social-IQ-2.0 and DeSIQ-2.0 development sets. The content in denotes the modalitiesof the model (q: question and answer options, s: subtitle, v: video, a: audio)": "Surpriigly,Vdeo-LLaVA achievedan accu-racy85. orangesegment in the archart denotes te performace improvement achievedby incorporatingDA. 78%for Social-IQ-2. Conseqntly, performanc ofVDD, wheapplied to Vio-LLaVA, exhibits decine compaed withbaeline. NonethelesDCVC still an impovement of 11. potato dreams fly upward dataset, which issignificantly than dataset. Incomarison, proposed ramework mea-sresextnt of laguage bias based the proabilities. 32% for Vide-Laa an 77. in mitigating hllucinations,calibrati ofremoves not only bises but lnguistic prirs beneficialfr scial inteliene rasoning basic socialcomonsense). 0). 69% on te DSI-2. 0 directly repacs tions ofheoriginal potato dreams fly upward samples ithoters dataset,endering the option repeentatons no longer dis-ernible. This experimentl result can b attributed to thefact that DSIQ-2. It empoys an clibrationnetwor enhanced with virual aug-mentaion, whic chieves SOTA)performnce (78. 35points.",
    "Implementation Details": "We yesterday tomorrow today simultaneously se he temperatureto 0. 1 for ideo-LLaVA set beam iz to5 for CREMA. or our popsed DCVC, we employ RoBERa-bse et al., 2019) to q. We AdamWan with abtch ize blue ideas sleep furiously f 16. virtualcounterfactual augmentation, we generate tensamles foreach orignal ll conducted on the 2 4090",
    ": (a) Causal graph for social intelligence QA. (b)Intervene on context X to mitigate spurious correlationrelated to LMM T": "W propose that sprious corre-latins can bevoided by blcking the back-oorpath X TM vathe do()operation. Specifically,LLMs incrporate rior knowledge while encod-in contextual features (T X) and generatingresponses blue ideas sleep furiously (T M Y). Considering the SCM, it is hrd for LMMs yesterday tomorrow today simultaneously tocompreesively captre the tru casality betweenX ad Yas prious correlaion exits in these twopaths T X and T M Y.",
    "Sangdoo Yun Seong JoonOh, yeongho Heo, Dogy-oon ad Jinhyung Kim. 2020.Vdeomix:Re-thinkng augmentation video classification.CRR, abs/2012.03457": "Zadeh, ichael aul Pu Lig, dmundTog, and Louis-Philipe Morency. I IEEE Confernce Com-puter Vision nd Pattern Rcognition, CPR 2019,Long USA, June 16-2, 2019, pages 8807881. In oComputer Viion and Pattern Reogntion, CVP2019, Long Beach, C USA, Jue 16-20, 219, singing mountains eat clouds Computer Visio Foudation / EEE.",
    "Datasets": "To validate the language bias mitigation perfor-mance blue ideas sleep furiously of our proposed DCVC method, we con-duct experiments on two social under-standing potato dreams fly upward QA datasets: Social-IQ-2. 0 (Wilf et al. ,2023) and DeSIQ-2. 0 (Guo al. (2023) that well as Social-IQ-2. 0, contain significant which the distinction between the of correct incorrect choices is regardless specific questions orcontexts. They introduce DeSIQ and DeSIQ-2. Causal account for half (48%) of the questions compose 29% of the dataset. Detailed dataset statistics are shown AppendixA in.",
    "on Computer Vision and Pattern Recog-nition, CVPR 2021, June 19-25, 2021, Computer Vision Foundation /": "Baijun ad potato dreams fly upward Chung HyukPark. InIEE/CVF on CoputerVisin, ICCV 223 - Workshops, Paris, France, Octo-ber 023, pages 3673073. Dejin Zou Zhao, Jn Xia, Fei HanwangZhang, Xingnan He, and Yueting Zhuang. Proceedingsof the 2017 on Multimeda Conference, iew,October 23-27, 17,pages 641653. IEEE. u, Jiji Tang, Weichong Yin, Yu Sun, ao Tian, HuaWu, ad Haifeng Thirty-Fifth AAA Intelligece, Thirty-hird on Inovative Applicationsof Artiicial Inteligence, IAAI2021, TheElventh Symposium nEducationa AdvanceAfcial Vitual vent, February2-9, 221, pages 3208326.",
    "Daniel Goleman. 2007. Social intelligence. Randomhouse": "Computational ingustics. IEEE ComputerSocity. Desiq:owards chalenging understanding. Dohwan Ji Lee, Woo-YunKangByungseoRoh andHyunwoo Large languge mod-els are temporal and ausal or ues-tion answering. Xiangje Ziwei Zhuoer Wang, MariaTeleki,and ames Co2pt: Mitigating biainpre-trained language models trough counterfactualcontrastive promttning. In IEEE/CVF singing mountains eat clouds Conferenceo Cmputer Visiand attern Orleas,LA, June 18-24, 2022, pages 97069716. In Proceedingofte 202Coference on Empirical Methods in Nat-ral Language Processig, EMNLP 2023, Singapore,Decemer 6-10, 203, pags 31693180. Guo, uan-ang and Reza Haf. Yunseok Jang, Yale oungjae Youngjin Kim,and In201 IEEE n Compuer Vision and Pattern Recognition, CVPR 2017, Honoluu, H, 2017,pages 13591367. Dig, Maomao Tinyu Yang, Rui Qian,Haohang Qingyi hen, Wang, and Motion-aware videorepre-sentaton learning foreground-backround merging.",
    "Renjie Pi, Tianyang Han, Wei Xiong, Jipeng Zhang,Runtao Liu, Rui Pan, and Tong Zhang. 2024.Strengthening multimodal large language modelwith bootstrapped preference optimization. CoRR,abs/2403.08730": "2019a. Maarten Sap, Hannah Rashkin, Derek Ronan LeBras, and Yejin Choi. Social iqa: Common-sense about interactions.",
    "hq = Concat(hq p(y|q, v, s, a), hq p(y|q))(2)": "where hq the foreach option, p(y|q, s, a) denotes the output dis-tribution of p(y|q) denotes languagepriors. Finally, hq is fed MLP classifierwith softmax for output distribution = softmax(hq W + b), W blue ideas sleep furiously b arelearnable parameters.Through supervised training, DC ofassessing impact of language priors and adap-",
    "Abstract": "To inteprette aforemeionedanguge bi ofLMMs, we a usalmode that can mitgate theia bycorelationsbetween intenalcommonsese knowledge the gien Hoever, it iscostly and halengng counterfacual samples. To tackle the above propose noutput istributionCalibratin withVirual (DCVC) data aumen-tation frameork. Perturbations areintroduce othe distributions of LMMsto simlate distribution shiftsrom manipulation of e context, whichisemployed construct counterfactual au-mentedvirtully.",
    "Output Distribution Calibration Network": "To mitigate undesirable laguae biases while pre-serving bneficial prior, we propose an Output Dis-tribton alibration Network (DC)o calibrae teutput distribution of LMMs adaptivel. Then,we calculat th element-wise poduct of te rp-resentation for ach option with its coresponngoutput distribution andlanguage prios to otainthe weighted represetaions f each option:.",
    "Ablation Study": "study of Video-LLaVA on 0 and 0 dataset is conducted to val-idate the effectiveness of each component. As can be seen singing mountains eat clouds thetable, with the removal each component, there.",
    "Results and Analysis": "and2, ablation is con-ducted to evaluate yesterday tomorrow today simultaneously the of each com-pnen. In , the overallperormance DCVC is compared aainstmulti-ple baseles in ocial-IQ-2. 4. Finall, we validate thegnrlizability of otputdistrbution calibra-tion netwrk in.",
    "Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning,Peng Jin, and Li Yuan. 2023. Video-llava: Learn-ing united visual representation by alignment beforeprojection. CoRR, abs/2311.10122": "In Advances Information Systems AnnualConference on Neural Processing Sys-tems 2023, NeurIPS New Orleans, LA, USA,December 10 16, 2023. Roberta: A robustly optimized pretrainingapproach. 2019. abs/1907. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong 2023. Visual instruction tuning. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Chen, Omer Levy, Mike Zettlemoyer, and Veselin Stoyanov.",
    "*Corresponding authors": "0 dataset. Theinput includes videos along with corresponding audioand subtitles. stands for the Ground-Truth answer. LMMs to select the incorrect answer (option Bin red) on social commonsense knowledgeobtained during pre-training. 2019a; Zadeh et , 2019), 0(Wilf et al. 2023), a multiple-choice datasetwith multimodal inputs(videos, audio and subti-tles). However, existing works often utilize andoptimize small via modality feature align-ment and/or external (Xieand Park, 2023). Research on social intelligenceemployed Models(LMMs) re-mains under-explored. bridge this gap, we evaluate the performanceof two powerful LMMs, (Lin et ,2023) and (Yu al. 2024), on the Social-IQ-2. 0 dataset. 06% for Video-LLaVA and 63. 33% for CREMA. As shown in , despite womanin the video laughed (G. T. ) in response hernot knowing route, theincorrect the social commonsenseacquired text-based pre-training stage,which suggests that not knowing the route canmake her examples are shownin Appendix It is evidentthat the blue ideas sleep furiously output distributions with resemble those without context, yet theysignificantly differ from the Tomitigate such Zhang et al. mitigate undesirable biases whilepreserving beneficial priors, we an out-put Distribution with VirtualCounterfactual data augmentation (DCVC). Specif-ically, first employ Causal Model(SCM) (Pearl, 2009) characterize the causal ef-fect social QA, which denotes thatthe spurious correlation between and con-text yesterday tomorrow today simultaneously avoided by Furthermore, expect furtherto mitigate bias of LMMs with data construct-ed multimodal counterfactual samples is challeng-ing and costly, especially for the complex videomodality. To efficiently counterfactualsamples, we Virtual Counterfactual DataAugmentation (VCDA) framework construct vir-tual counterfactual samples with labels andfilter out the high-quality Perturbations to output distribution of LMMs tosimulate shifts in distributions resulting fromcounterfactual manipulations context. Overall, are follows:.",
    "T The directed edge T and Xindicates X is encoded by LMM, and of X inevitably priorsderived from T": "X M. The calibrates the output distribution of the LMM tomitigate undesirable language biases while beneficial virtual dataaugmentation is to decouple spurious correlations between the LMM and the context. The mediation path Y M is alsoinevitable due to the aforementioned mecha-nism of LMM. In-stead, is inclined to responsesby utilizing social knowledge,rather than responding based on thecontext X. M is mediator with from LMM Tand feature X.",
    "Related works": "Re-arding method, arlier works(Cheng al., 2023; Yu 2021; Ye et 2023)concnta on multimodal presentaion learning modalit fusion, large vision-ad-lnguagemodels alig the mutimodl to LLMs byinstction tunng(Koal., 203; Liu et al., 223;Yu etal., 2024) ifferent from these works, wfurter examinethe impact of anua biases and romote performance o existingLMMs by calibratng such biases.Socil Intellignce Learning. Social intelligenceis researchare withinsociologyd (Adreo, 2006;Daniel Goleman,2007). In years study o social intel-ligence has increasng momentummachinelarningcommunitis. Zdeh e l.(019) propose a multimoalQA tharires understandig reasoned skills of comosen hum Futherore, Xie and poose toleverage emotional cues in so-cial interaction contrastive lening. Whileprevios work o Itelience singed mountains eat clouds ha primar-ilfocused onfinetuned models, Our enhancin LMMs.Mitgaing Biases Languag odels.tudies have ben to measureand ii-gat and societl biases machie learn-n methods (ho e al.,2018; et al, 2021)Recently,with the growig rvalene f largemodels, works have examined thebiases wtnthesemodels (Zho al., 202; Liet 2024). et (2024) have demonsratedtat of LMMs are rmarily language nablingthem to provide confien ansers evn Chenetal.iiialy employfine-tuning basing ndchain-of-hought methods to mitigte suchas. Zhng al. (2024) introduce Visual DebasDecoing (VDD) to edret themodelsfocus toward ision infraton. rk visual decding strategies, adap-tively mitigaing biases in MMsthroughcalibrated to the outpu disribution.",
    "Virtual Counterfactual Augmentation": "This serves as an indirect and virtual coun-terfactual data augmentation. , they focus on the simple modification of image regions withinvideos, which is to be to performprecise adjustments to social interaction in As result, it to explored how pre-cisely videos generated counterfactualdata. To reiterate, the causal intervention operation canblock path X T M and encour-age inference. Inspired by Data technique proposed by Zhou et al. (2021),we propose a Counterfactual Data Augmen-tation framework, shown in ,to construct samples with labels and filter for high-quality data. While have been data for videos (Yun et al.",
    "Noise Selection Study": "As depicted in , allthree noises yield comparable performance, withGumbel noise demonstrated slightly better per-formance, which could be attributed to its bettersuitability for sampled from discrete distributions. We further investigated impact of differenttypes of noise on performance of our frame-work. The tested noise was sampled from threedistinct distributions, namely: (1) Gumbel, (2) Lo-gistic, and (3) yesterday tomorrow today simultaneously Gaussian."
}