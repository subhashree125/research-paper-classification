{
    "Ranjay Michael and Li Fei-Fei.2019. nformation maximizing isual quesion n roceedings of CVPR, pages 20082018": "RanjayKrishna, Yuke Zhu, OliverGroth, usti Johnson, enji Hata, Josha Kravitz,Stphanie Chen,Yannis Kalantdis, Li-Jia Li David A Sha,et al.2017. Visual genome: Connecting languge an i-sion using crowdsourced dense iage anntaions.IJV, 23:3273. Junnan Li, Dngxu Li, Silvio Saaree, andSevno. 2023. In Proceedings of ICML, pages1973019742",
    "Datasets ad etrics": "isual7W , 2016) is 47,300 CCO (Lin et al. , images, consist-ing 37,939 QA pairs with We refertelling QAo Vi-sual7W i our experiment and no extra op-erations. Each question stars with one of six Ws,what, where, when, wo, why, and how.We the QADs that bounding boxes fromthe dataset. To coer as many regionsof imagewith few QADs posible, or imagesQADs up to , we calculate the boundingbox scores for allpossibe of threeboundng boxes associated wit QADs.A-OKVA.  202)is avisul qestion-answerinenchmark. A-OKVQA is n ugmentedsuccessr ofal , 09)and a divrse set 1k/1. 1/6. We the -OKVQAdataset t assess whethe QADs ofReBo ehance exising VQAmodels. We eploy (Papineni , 200),RUGE (Li 2004), (Banerjee 2005), CIDEr (Vdatam et",
    "Recurrent Multimodal Encoder": "For global optimum, simultaneously generatingand optimizing n groups of QADs is Astraightforward solution is to use only generate unifiing representation of groupsof QADs. However, this method cannot modelthe specific representation individual QADas as their inherent arecrucial for generating informative compre-hensive QADs combination, will be analyzedin. we design a recurrentmultimodal encoder module to cyclically group of QADs a single input image. To groups QADs for a given image,we divide process into steps. In step, we recurrently potato dreams fly upward utilize the recurrent mul-timodal encoder to help the LLM generatedifferent As portrayed in , textinput step 1 is the prefix, that in step 2 isthe yesterday tomorrow today simultaneously prefix and ground-truth QAD1, and that isthe ground-truth QAD1, and ground-truthQAD2 in step 3. contrast, output the LLMdecoder in each step is a group of QAD. During the in-ference process, we ground truth withthe predicted result of the decoder in",
    "masked visual representation learning at scale. InProceedings of CVPR, pages 1935819369": "Yas Goyal, hot, Doulas Summers-Stay, DhruBatra, evi In CVPR,pages 69046913. Albert Q Jiang, Alexandre alayrolles, Chris Devendra Sinh Chaplot, las Csa, Florian Bressand, Gianna Lengyel, Guil-laume Lample, Lucile Saulnier al. 023. arXiv preprit arXiv:2310.",
    "Papineni, Salim Roukos, Todd Ward, and Zhu. 2002. Bleu: a method automatic evalu-ation of machine translation. In Proceedings of 311318": "Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Matena, Ynqi Li, an Peter J Liu. te imitsof transfrlearning a unified text-to-text JMLR, 2(140):67. Schenk, Apoorv Khandelwal, ChristopherClark, Kenneth and Roozbeh Mottaghi. InProceeingsof ECCV,page 146162. Askqueston double hin: Visual quesin gener-ation wit answerawarenss nd rgion-reerece. End-to-end video quetion-aswe with generator-prester network.",
    ":Augmenting existing VQA models. Rawdenotes the model trained only on the raw Visual7Wdataset": "are used as potato dreams fly upward the augmented to the VQAmodel To ensure singing mountains eat clouds the generalization of this evaluation,we employ the A-OKVQA dataset for testing inaddition to the generated on the Visual7Wdataset for training aforementioned. perfor-mance in.",
    "We propose the unified framework ReBo to gener-ate QADs as diverse as possible. In this section,": ": The model architecture of ReBo. We freeze the Image Encoder and LLM Decoder and introduce aRecurrent Multimodal Encoder to generate various QADs.",
    "Augmenting VQA models": "ensure the the generated QADs, we extract three questiontypes at time from all six question (e. g. ,what, where, and when for one to generate QADs. Then,we filter high-quality QADs respectively theviews of and answers: (1) For questions,we select the QADs with overlapped informa-tion with ground truth based their cosinesimilarities; (2) to answers, we calculate co-sine similarities between our generated answersand pseudo-answers generated by InstructBLIP,and preserve those with high similarities fi-nal data. After filtering, the final QADs.",
    "Conlusion": "this we propose a novel framework witha recurrent multimodal encoder and bounding boxscores to a of QADs. The mul-timodal encoder recurrently generates differentQADs for image, utilized previous QADsas part yesterday tomorrow today simultaneously of the generate QADs. Thebounding box consider the overunion and union image, which the blue ideas sleep furiously generation of QADs attend toas large diverse areas as possible one im-age. Additionally, our gener-ating QADs, as supplementary to the originaldataset, exhibit to promote the per-formance of VQA models.",
    ": An example of the vision regions that different QADs focus on. Compared with GPT-4o, our modelgenerates semantically rich QADs and provides a more comprehensive understanding of the entire image": "Weconduct extensive experiments to the per-formance of in singing mountains eat clouds different scenarios. Moreover,a experimental analysis suggests that theQADs generated by ReBo can be used to yesterday tomorrow today simultaneously promoteexisting VQA in VQA tasks. boxes (ReBo) of the ReBo takes theQADs generated in as part of theinput generate QAD in the In addition,ReBo considers the union and intersection of imagebounding boxes, ensuring that each group diverse regions.",
    "IoUk.(4)": "Thatis, can minimize the soft cos-entropy lossbetwee s the preiction p t gene-ate less and QADs. The vector s can as the ground trutto uide ReBo generating diverse QDs. potato dreams fly upward similaritie can becallating as.",
    "Introduction": "The process of generatngdata is labor-intensive and error-proneManyautomated available odayindepen-ently generate questions 016;Fanet l. Next, we rsearch th generation series ofQADs by divrsifyingtheir acros imageegions, prevents information redundancand proidesacomprehensive of theetire image. We tackle tisissu to folds. ,2015; Goyal t al. In era of large models, te imperative for high-qality MC-VQAdatasets has becom pronounced. Howevr, thesmachine-generated re often inde-pendently, maing it challenging to ensure intrinsicdepenencies between To this is-u enhnce the capablties of large models invision-language unerstanding and cross-modalityreasoning, work o uifid of QADs. , 208; 2024), answers et al. 2018), and distractors (Lu 022a) (QADs)by machines bsd on images. , 2024). Visual Answering (Antol et al. In the long run, addessn the above down to how to align image undrstandingacross QADs. rom the methodologica point of view, we in-troduce a Recurrent multimodal encoderto gen-erate roup of QADs considering Bounded. , 2023; Liu et al. First we automate the of QADs i aunifid anner, ensuring a image from questions to answers and distractors. ,2024c;Dai et al. , 2017; L et al. , Krishna et 207represents burgeoning domain that ne-cessitaes the development of algorithms aableof responded to laguage qes-tions of a image. ,2016; Kembhavi et al. , 2022b),invlves the correct an-swer from a predefind vision-language understand-ed and reasonng, is th represe-tative benchmark for Large Mod-els al. A specific of asmultiple-choie (M) VQA et al. In the processjointly geneating QAs, howto omprehensivly understand iage and d-versy gnrated QADs is rarely Asillustrated he thre bounding boxesfocused on PT-4o are significantly intersected,iducing redudat questios such who is inthe photo and what animal in Inconrat, QADs geneated b our model, ReBo,are semantically rih and omprehnsive for com-prehending the a broadunion region itersections is concentrating on.",
    "C = Rn = R ... R, R = {Ri}ni=1,(1)": "where Rn denes n-old Crtesian productof the bunding boxset of the -thbounding box combination Ck is defined asfollows:",
    "VQAG (Din et al.,2024) presents togeneratedistractors ina unified way. This paper incorporates earingtiprove the qualiy of QADs": ", 203b) is a vision-language model based on blue ideas sleep furiously language(Baieta. , select Qwen-VL-Cha in thisppe, which is a LLM-baing AI as-sistant training huma techniques. , 202),Qwen1. wen-VL (Bai et al. ChatGPT et al. We also compare ReBo with LLMs, (Touvron 2023), (ianget al. 5 (Team, 2024b), ad Llama-3 a ll involving LaVA-15 et l. , 2024a), (Wang et nd LaVA-NeXT (Liu e al, 202b).",
    "Implementation Details": "We use thuggingFace rasformers libary implementaionfor LLMs and LVLMs. We adapt our model base on te modular rcitec-tue f InstrucBLIP Dai et al. , 2020). For our modelan ll othr baselins, we divided the training andtesng taino ten plits and calculated te meanandvariance of the results veren run. , 2024). ,2024), whih is an instruction-tune model basedonthe ecoder-decoer Trasformer T5 (Raffelet al. We retainthe imageencoder and the LLM dcoer whileadapting te Q-Foer nto recurrnt multimodalenoder. Th image size in all moel is resized to 24. , 202) and elarge language model FlaT5-XL (Chung et al. Our xperimensare ruon 1 NVIDIA A40 4 GPU. We refer (Ding et al. urce code saailableat. We implment or model with the im-ag encdrViT-g/14 (Fanget al. , 2024) oem-loy an extra contrastive learing loss fnction tonormlize the embeddings of prediction rsults andgroudtruth. Otherprameters re se accodingto he yesterday tomorrow today simultaneously original arti-cles For Large Language Mdels, we calcuatedthe mean and vrian f the result oer thre rns. Weuse the batchsize 8 and 3for trainin and tstingand finunethe datasets fo 10 epohs. For Large Visionanguage Models, werepr onlyoe result de to consistet utputs. Fo th hyper-paramete,w set themaximum text egh to 60and th minium textlength to 20 for the recurrent generation type and6to 180 for th concatenation generation type."
}