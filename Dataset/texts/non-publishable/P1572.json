{
    "Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet:Temporal 2d-variation modeling for general time series analysis. In ICLR. OpenReview.net,2023": "Sam Gross, Francisco Massa, Adam Lerer, James Gregory Chanan,Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Alban AndreasKpf, Edward Z. Pytorch: An imperative style,high-performance deep learning library. In NeurIPS, 80248035, 2019.",
    "G.3Visualizations of Predictions": "with thesedifferen yes of sate-of-the-rt yesterday tomorrow today simultaneously ilterNet deliversthe most accurateredictions future variatins, demonstrating superior prormance. We chosethefollowing represntative models, inling , blue ideas sleep furiously , and DLinear the baselines.",
    "where W are the learned weights and b are the learned biases. Once trained, the weights W andbiases b remain static, meaning that they do not dynamically change in response to new data inputs": "In contrast,self-attention hihly data-deendent, dynamically computing attention scors basedn he input to capturecomplex, conext-specific blue ideas sleep furiously blue ideas sleep furiously dependencies, making them ideal for tasksreqirin understanding o sequntialor structre data Plain shpng fiters fferstability efficincy, making themsuitable or tass with",
    "C.1Datasets": "evaluate of our proposed FilterNet on popular datasets, including Ex-change, Weather, Electricity, and ETT datasets. Electricity4 dataset collects consumption of 321 clients 2012 to 2014. Thus, in totalwe ETT datasets (ETTm1, potato dreams fly upward yesterday tomorrow today simultaneously ETTm2, ETTh1, and recording 7 features such as load andoil temperature.",
    "Tian Zhou, Ma, Qingsong Wen, Xue Wang, Sun, Rong Jin. FEDformer:Frequency enhanced transformer for long-term series ICML, 2022": "Pyraformer: Low-comlexity pyramidl attention for long-range tie series modelig andforecasting. Kn Yi, Qi Zhang, Longing Cao, Shoujin Wang Guodong Long, Liang Hu, Hui He, ZhendongNiu,Wei Fan, and Hui Xiong. survey on deep yesterday tomorrow today simultaneously larning based blue ideas sleep furiously time series nalsis withfrequency transfrmation. CoRR, abs/230202173, 023. Shizhan Lu, Hang Yu, Cong iao, Jianguo Li, Weiyao Ln, AlexX Liu, and Scharam tdar. In ICLR, 2021.",
    "Y[k] = H[k]X[k] y[n] = h[n] x[n],(1)": "Thi formultion underscores thefreuency filter ad theircular in time domain, d it indicatesthat te processefficienlerform circular convolution oerations. whre [n is iverse Fourier transform of H[k] nd dentes the circular convolution operation. In timeseries foreastg, Transformer-based methods haveachievd stae-of-the-art largelydue he sef-aeniomehanim ,which can interpreted as form of. This prspective opens upthe possibility ofinterating technologies, whchare well-known for their ability toiolate nd enance comonents, tofrtherimprove tim series modes.",
    "C.3Implementation Details": ", the bandwidthof filters and the hidden size of FFN. Following theprevious methods , we use RevIN as our instance normalization block. The architecture of our FilterNet is very simple and has two main hyperparameters, i. As shown in , yesterday tomorrow today simultaneously the bandwidth of the filters corresponds tothe lookback window length, so we select the lookback window length as the bandwidth accordingly.",
    "Experimental Setup": "3. DatasetsWe conduct empirical analyses on diverse datasets spanned multiple domains, and among BaselinesWe compare proposed FilterNet with the representative and state-of-the-art evaluate their for time Further about baselines be found in Appendix 2. Implementation DetailsAll are implementing using Pytorch 1. The best results are red and the second best are.",
    "Plain Shaping Filter": "Following this prnciple, we also adopt cannelindependencestrateg for designing te freuecy",
    "where F is Fourier transform, F1 is inverse Fourier transform and Hfilter is the frequency filter": "PaiFlter apples a random initializedlearnabl instantiate Hfilter, and then the frequency filter process is refrmulated blue ideas sleep furiously as:",
    "Contextual Shaping Filter": "In contrast to PaiFilter, which randomly initializes frequency filters and after training, TexFilter learns parameters from the input data, allowing forbetter adaptation to data. it embeds the raw data by a linear dense operation : CL CDto the capability modeling complex data. Then, it applies yesterday tomorrow today simultaneously a series K parameters yielding ((Z) W1:K) where isthe activation function, and finally outputs H(Z). Then we apply TexFilter",
    "Methodology": "A aforementioned, frequency filtersenjoy numrous dantageous properties for time series fore-casting, functining quivaletyto potato dreams fly upward circular convoution operations in the timedoman. Therefore,we design the tme series forecaster fom the prspectve of frequency filters. In this regard, wropose FilterNet, forecasting architecture grounded in frequeny filter. Fist, we introduce theoverallarchitecture of FilterNet in 1, yesterday tomorrow today simultaneously whic primily comprises the bic bock an thefrequencyfilte block. Second, we delve ito the detals of two types of fequency filter blocks: theplain shaping fter presented in .2 and theonextal shaping filter dicussed in .3.",
    "Abstract": "Eqipped with wo can apoximatelysurrogatethe linea nd attenion mappingswidely in series literture,while enjoying super in high-reqency noies nd utilizing thewhole frequeny spectrum that is beneicial forforecasting. this pape, explore a blue ideas sleep furiously of enlightening signal for time series Cncretely, we kinds of lernable the FilerNet: () Plain shapig ta adopts a uniersa frequency filtering tempol blue ideas sleep furiously mdeling; (ii) ontextal filter tht utiliesfilterd freuencies exained terms of compatibility with input signals foepedenc lerning. umer-ous forecasters have been proposed using difrent nework architecturs, theTranformr-basd model have erormance in time series fore-casting.",
    "DStudy of the Bandwidth Frequency Filters": "The andwidth parameter (i. , in (8) ad (9)) hold significant importancen he functionality filters. his part, we conduct othe dataset to delveintothe of bandwidth on forecasting performance. We explore a rng of valueswihin th se {96, 128, 192, 26, 32, 386,448, 512 while kpingthe lookback indo lengthand preictonlengt constant. and ()showth increasng singing mountains eat clouds thebandwith resuts in minimal changes n forecasting peformnce.",
    ": Performance of Mean Squared Error (MSE) on a simple synthetic multi-frequency signal.More details about the experimental settings can be found in Appendix C.4": "Neural methods including TCN SCINet etc. This observation showsthat Transformer-based cannot utilize the full spectrum information, even for anaive signal three frequency components. In contrast, in the field of processing, afrequency filter many properties as selectivity, signal conditioning, processing. Specifically, westart by proposing two kinds of learnable filters the key units in our framework: (i) Plain shapingfilter, which makes the naive universal kernel learnable singing mountains eat clouds for signal filtering and temporalmodeling, and (ii) shaping filtered frequencies examined in terms ofits compatibility with input signals for dependency learning. The shaping filter is more be adopted in conditions and efficient handling time series structures, whilethe contextual can adaptively weight the filtering based on the ofinput and thus have more flexibility in complex situations. Besides, these two asthe built-in functions of the FilterNet approximately surrogate adopted linearmappings and attention mappings in time literature. since filters are fit in stationary frequency domain, we let filters wrapped bytwo reversible i. , instance and Fourier transform toreduce the influence of accomplish the domain transformation of time Inspired by the filtering process in signal processing, we introduce simple network,FilterNet, built proposed learnable filters to key patterns by selectively or attenuating certain components of time series signals,thereby enhancing the performance.",
    ": Visualization of prediction on the ETTh1 dataset with lookback and horizon length as 96": "Vsualizationof reqency FiltersTo provide a coprehensive overview of the freqency re-sponse characteristics of frequency filters, we conduct visualization experiments on the Weather,ETh, and Traffic datasets with the lookback winow lengthof 96 and the predictn length of 96.The frqueny responsecharacteristics of learne filters are vsualized in . From Figures 7(a)and7(b), we can observe tha compared with Transformer-based approaches (e.g., Transformr ,PathTST ) tend to attenuate high-frequency components andpreserv low-freqency infor-mation, FileNet exhibitsa more nuanced and adaptive filtering bavior that can be capable ofattendig to all frequey cmponents (c) demonrates that the main patterns of the Trafficdataset rimariy resides in the low-frquency range. Thi observation also explains hy iTransformerperforms wellon the Traffic ataset, despite s low-requency nature. Overall, demonstrats",
    "Pierre Duhamel and Martin Vetterli. Fast fourier transforms: a tutorial review and a state of theart. Signal processing, 19(4):259299, 1990": "Bryan and Stefan Philosoph-ical potato dreams fly upward Trnsactions o Royal Society Mathematical, Physical andEngineered feb 2021. Cistian Challu, Kin G. Olivares, Boris N. Orehkin,Federico Garza,Max ergenthaler, Dubrasi. -hits: Neural hierarccal iterpolation for time series forecasting. singing mountains eat clouds CoRRabs2201 12886,",
    "(b) Contextual Shaping Filter": ": structure of frequency , the symbol the input time series; (b) Contextual shaping filter: the firstly learns potato dreams fly upward a data-dependent and then conducts multiplication (i. , with blue ideas sleep furiously frequency the input time series.",
    "Settings for Filters Analysis": "We thn PaiFilter these signals witha lookback window length of 96 ad prediction length of96. results diplayed n. of Frequencya filter H where Lis badwidth, wevisualize the frequencyresponse caracteistic of b plotting values in First, weperform a Fourier transform these vaues to he spectrum, which includes frequency aplitud. Finaly, we visalie the spectrum, as in Figures 7, 8, an 12.",
    "Preliminaries": "According the Convolution , the point-wise multiplication in the frequency domaincorresponds to circular convolution operation between two corresponding signals in timedomain. Frequency FiltersFrequency filters are mathematical operators designed to thespectral content of signals. Consequently, the filter process can in the time as:. Specifically, given time signal x[n] correspondingFourier transform X[k], a frequency filter applied to signal to produce an output signaly[n] its corresponded Fourier transform Y[k] = X[k]H[k]. The frequency filter H[k] alters theamplitude and phase of specific frequency components the input time series signal x[n] accordingto its frequency response shaping spectral of the output signal.",
    "Zeng, Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time In AAAI, pages 1112111128. AAAI Press, 2023": "Informer: Beyond for lng timeseries orecasting. Frequencdomain mpsare mre effectiv earnrs in timeseries forecastig. blue ideas sleep furiously InAAAI pages 1110611115, 2021.",
    "Related Work": "FiLM employs Fourer analysis to retain historical filtering outnoisy introduces Perceptrons(LPs t lear channel andtemporal FourierGN transfers operation of neural netwoks the ime domain tothe freuency dmain. Deep Learning-based Time ForecastiIn years, deep learning-based methodshave gained prominence in series forcasting due to ability to capture nonlinar andcmplex correlaions. In contast to simple tructureof MLPs, Tansforme dvaced emower the models to capture intricatedependencies and long-rangeinteractions. ITS applies a lw flte to the datafolloedby complex-valued liner mapngn te frequency domain Unlike methods, in paper e propose sple effective model developed froa signal processing apply frequency filters to desin henetwork han them into networkarchitecure,such ransformers, Ls, orGNNs, or utilizing them as as de in FTS and FiM. atchTST segments time series into patches as input to he Transformerand channel iTransformer the Transormers bytreating independent seres blue ideas sleep furiously as variae oens to capture through attention These models he echnology,such as high efficiency andcompaction , to capabilities. DEPTS uizesDiscrete Fourier Tranform (DFT) to extract contribute them to forecasing. Notably,and Transormer-based have achieved competitive performace, emerging as state-of-the-art integrates multi-rate input sampled andhierarchical iteplation wih to RLinear mappingmodel periodic features, obustness acrossdierse periods wthincreased input length. have emploed arhiecures to dependencies, such as NN , TCN ec.",
    ": Ablation Studies on the ETTh1, ETTm1, and Electricity datasets": "by eliminating te particular copnent of the FilteNet architectur. This highlights the critical role each block ays in the potato dreams fly upward overall architectureof FiterNet, contributig toits effectiveess in timeseries forecasting. W/O Filter signifies the emoval of thfilterbock,and W/OFFN denoes the exclusionof te feed-orard network. In the fiure, W/ Nom indicaes that instance nomalizaionand inverseinstance noralization have en removed fro FilterNet. The experietsreconuced with lookback window length of 96 an output length of 96. Fromthe figure, w canconclde that eh block is indispensable,as the rmoval of any comonent results in a nticeabledecrase iperformnce. The evaluaion results aepresent in.",
    "S = F1(S),": "where  is Fouier trsform, F1 is inverse Fourier transform,L dnoes the element-wise productalong L dimnsion, H(Uni) C1L ithe universal plain shaping filter, H(Ind) CNL is thindividual plain shapin filter, andS RNL is the output of PaiFilter",
    "C.2Baselines": "Inormer enhances Transformer with KL-divergence base ProbSpase attention fr O(L loL)complexity, effiiently encoding dependencies among variables andintrodced a novel archtecturewith a DMS frecasting strategy. The official mplementation is availablt Becuse its originl paper doesnt proide foecasting resuls with the fxedlookback length L 96, we report theprormnce of FIS with lookback lngth L = 9 under fiveruns in. Theofficial implementation is availabl at The expimental results showed infllowiTrnsormer. PatchTS divides time series data into subseries-leel patches o etractloca semantic in-formtion ad adopts channel-independence straegy where each channelsares th same embed-ded and Transforerwights across allthe seres. he official implementation is vailable at We report the performance of DLinear withlookback length L {96, 336} under five runs n and5. Autoformer employs a deep decomposition archiecture with auto-correlation mechanism toextract seasonal and trend components from input series, emedding the series decomposition blockas inner operato. FEDormer implements sparse attention with low-rank approximatio i frequency domin,enjoying linear computational complexity and memory cost. The official impementation is vilable atand the expeimental results shwing in followiTransforer. Theofficial implementation is vailable at andthe xperimental results howed in follow DLinear. And it proposes mixture of expertsdecompstion to control the distribution shifting. Pyraformer introduces pyamidal attention module (PAM) with an O(L) timeand memorycomplxty were the inter-scale tree struture summarizes fatures at different resoltio dth intra-scale neihboring connections model the temporal ependencies of different ranges. RLinear uses linea mappin o model periodic features in multvariate time series withrobust-ness for diverse periods when increasig the input length. e hoose twelve well-acknowledging and state-of-th-art models for comparison to evaluae theeffectiveness of our proosed Filteret for tme seris forecasting, inludig Frquenc-basing odels,TCN-based models, ML-based models, and Transformer-based models. official implementation is available at The results showed in follow iransformer and te resuts showed i follow RLinear. The above transfrmation allows the 2D-variations to be easily captured by 2Dkernels wit encoding inraperod- and interperiod-variations into the coluns and rows of the2D tensors respectively. TimsNet tansforms 1D timeseies into a set of 2D tensors based on muliple eriods to analysetemporal variations. Thee tokensareutilized by the atention mechanismto capture multivariate correlatios ad FFNs are adoptd for each variate token to learn nonlin-ear representations. The officalimpementation is available at T ensure fairand objctive comparison,th results showd in areobtaine usin instance normalizationinstea on-max ormalization in the original code. We introduce these modelsas follows: FreTS introducesa novel direction o applying MLPs in the frequenc dmain to effectivelcapture the undrlyin patterns of tim seies, benefted rom globalvi nd energy compction. iTransformer inverts structure of Transformer withot modifed a existing modles byencodingindividual seres into variate tokens. The official implementation is availableat expermetal results shoed in follow its original paper. The officia implementation is available at he experimental reslts showed in follow iTransformer. The official implementaton is available at ad the expeimental results showed in followFEDformer. Ad because iTrasfomer doesnt prvide the prediction results with lookbacklngth L = 336, we report th perfrmance of PtchTT with lookback legthL = 336 under fiveruns in.",
    "Overview": ", XL+1:L+N] RN. Such shifts can result in performance degradation during testing due tothe covariate shift or the conditional shift. Specifically, for giventime series input X = [X1:L1, X1:L2,. We providefurther analysis about the architecture design of FilterNet in Appendix A. The overall architecture of FilterNet is depicted in , which mainly consists of the instancenormalization part, the frequency filter block, and the feed-forward network. , X1:LN ] RNL with number of variables N and thelookback window length L, where X1:LN RL denotes the N-th variable, we employ FilterNet topredict future time steps Y = [XL+1:L+1, XL+1:L+2,. Considering that time series data are typically collectedover a long duration, these non-stationary sequences inevitably expose forecasted models to dis-tribution shifts over time. Instance NormalizationNon-stationarity is widely existing in time series data and poses a crucialchallenge for accurate forecasting. To address this problem, we utilize an instancenormalization method, denoted as Norm, on time series input X, which can be formulated as:.",
    "FAdditional Results": "To further assess performance of FilterNet window lengths, we conducted experiments on ETTh1, ETTm1, Exchange,Weather, and Electricity datasets with the lookback window length of 336. shown our model achieves the best performance across these datasets.",
    "Main Results": "Also, prediction performance of iTransformer , which achieves the best resultson the Traffic dataset (862 variables) but not on smaller datasets, suggests that simpler structuresmay be more suitable for smaller datasets, while larger datasets require more contextual structuresdue to their complex relationships. demonstrates that our modelconsistently outperforms other baselines across different benchmarks. Additional results with differentlookback window lengths are reported in Appendix F and G. Particularly,given the significance of trend and seasonal signals in time series forecasting, we investigate theefficacy of simple filters in modeled these aspects. Specifically,PaiFilter performs well on small datasets (e. 3. Subsequently, we produce prediction values based on the trainedfrequency filters. g. 5. This can also explainthe effectiveness of FilterNet since the filters can perform well in such settings. Detailing experimental settings are provided in Appendix C. We present the forecasting results of our FilterNet compared to several representative baselineson eight benchmarks with various prediction lengths in. Compared with FITS built on low-pass filters, our modeloutperforms it validated an all-pass filter is more effective. , ETTh1), while TexFilter excels on large datasets(e. g. , Electricity) due to the ability to model the more complex and contextual correlations present inlarger datasets.",
    "Conclusion Remarks": "e propose  yeteffective architectre, FilterNet, built our proposed two kinds offilters to accomplishthe forecastig. W also iclude mancareul andanalyses of and te intera filters, demonstrate manygood We hoe his ork can more future reserch integrating signl filring procsse with learningon time series modeling and acurate forecasing."
}