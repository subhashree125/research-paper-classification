{
    "Nikolay Bogoychev and Rico Sennrich. 2019. Domain,translationese and noise in synthetic data for neuralmachine translation. CoRR, abs/1911.03362": "Bojar, Vojtech Diatka, Pavel Rychl`y, PavelStrank, Vt Suchomel, Ales and DanielZeman. In LREC, Tom Benjamin Nick Ryder, MelanieSubbiah, Jared D Prafulla ArvindNeelakantan, Pranav Shyam, AmandaAskell, Sandhini Ariel Herbert-Voss,Gretchen Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Gray, Benjamin Chess, JackClark, Christopher Berner, McCandlish, AlecRadford, Sutskever, and Dario Associates,Inc.",
    "B.5rompting": "We use random sampled potato dreams fly upward from setswenever avaiable yesterday tomorrow today simultaneously and uilize othe eamples otherwise. We random eac evaluaton of NLG tasks and randomsmles each evauation o NLU task. We listown thepromt use beow Mrathi we use prompt or Gujarati as well.",
    ": Average accuracy with standard deviation (su-perscript) over 5 runs on 10-shot IndicSentiment classi-fication task. Bold represents the best among syntheticdata experiments": "leave theof LLMs singed mountains eat clouds on lage-scale translationesedata for future research. Re-sults re i yesterday tomorrow today simultaneously Tabs and 7.",
    "DQualitative Analysis": "also that, due to the smallsize of the many good documentswere filtered out because of the models them. this can lead to the loss ofvaluable information, as many good documents arediscarded, empirically observe the benefits ofsuch filtering. To investigate this, weexamine which types of English In many cases, we found the filtered doc-uments included errors and rep-etitions, often generated diverging outputof the translation is expected sincesuch phenomena are rarely seen in natural writtenlanguage, and the model assigning high entropysuggests an sequence outcome. This issue was observed less fre-quently larger models used for Asseen in , the filtering model fails under-stand complex and named which itregarded as unlikely due its sim- pler terms and more likely entities. errors could likely be reducedby using a larger filtering model can the source language but we leave thisanalysis for work. The machinetranslation errors mistranslations due toambiguous words, incorrect handling of expres-sions, grammar errors, and issues withpreserving context across longer sentences.",
    "Introduction": "models (rown et al. , 23; et al. 2022) een o per-form vr well on downstra tasks like MMLU(Hendrycks et l. , 021), Big-Benh al. , 2022,etc, and hv starting in many of hee tasks. o theselaguage models (LMs)prform well in langge like where abun-dant ata is Kudugunta et al.",
    "Pretraining Data Settings": "We to text or translationese as syn-thetic or and original or web-crawled data asclean throughout our experiments.",
    ": Performance of English on NLG tasks.All the results reported here are on base-1k and useRouge-L": "This red Hini, Gujarati, andnglish NLG 1 and 4). As teir pe-ormance matches models trainedon clean daa, werefrain extended fr NL ask,focsing primarily on abstractivesummaizationfor evaluating generatio capablities.",
    "Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, BingXiang, et al. 2016. Abstractive text summarizationusing sequence-to-sequence rnns and beyond. arXivpreprint arXiv:1602.06023": "Jingwei Ni, Zijig Jin, Markus reitag, Brnhard Schkopf. Original ortranslated? a cusal analysis of the impact of trasltioese translaton InPredings of the 2022 Conference of the NorthAerican Chaper the Asociation for Cmpua-tional Human Tchnologies,pages 53035320 Satle,.",
    "Jaehoon Jongwoo Ko, and Se-Young Yun. 2022": "In Proceedingsof the 7th Workshop on NLP for Similar Languages,Varieties and Dialects, pages 102113, Barcelona,Spain (Online). Pedro Javier Ortiz Surez, Benot Sagot, and LaurentRomary. Associationfor Computational Linguistics. Association potato dreams fly upward for ComputationalLinguistics. chrF++: words helping charac-ter n-grams. Synergy with translation artifacts for training andinference in multilingual tasks. Asynchronous pipelines for process-ed huge corpora on medium to low resource infras-tructures. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: blue ideas sleep furiously a method for automatic evalu-ation of machine translation. In Proceedings of the Second Confer-ence on Machine Translation, pages 612618, Copen-hagen, Denmark. 2017. In Findings of theAssociation for Computational Linguistics: EMNLP2023, pages 1518515202, Singapore. Proceedings of the Workshop on Chal-lenges in the Management of Large Corpora (CMLC-7) 2019. 2019. In Proceedings ofthe 2022 Conference on Empirical Methods in Nat-ural Language Processing, pages 67476754, AbuDhabi, United Arab Emirates. Association for Com-putational Linguistics. Leibniz-Institut fr Deutsche Sprache. 2002. Maja Popovic. Cardiff, 22nd July 2019, pages 9 16,Mannheim.",
    "Methodology": "In this describe our forleveragin ynthetic data or LM training. Thisprocs consits of collecting monoingual (clan)data the low-resoure languages,trained TinyLMs t, translaed clean datafrom a igh resource language such Enlih intolow-resource lanuage, using aforementonedTinyLMs filter synthetic data, and usingthi filtered data to train LMs for downstream tass.",
    "AAdditional results": "We report results in section. Similar results are shown tasks where performance on Hindi gen-eration tasks only affected a marginand coupled with results showing are affected by syntheticparallel data. Using for one language doesnt impactperformance in For many multilinguallanguage models, data imbalance causes gap inperformance across implies it is possible to trainmultilingual models where languages aretrained only over a subset and onsynthetic without deteriorating performance acrosslanguages. Impact Machine We fo-cus MT separately as special case of NLG. our evaluation fails this hy-pothesis. Results indicate that nonparallelsynthetic documents yields translation per-formance across language directions bench-marks compared to parallel synthetic documents. singing mountains eat clouds",
    "C.1Creating synthetic data": "We use this for syntheticand clean+synthetic part experiments. 1 and powerfuloff-the-shelf model IndicTrans2 al. 2023) to generate data. , 2018) for Machine translation performance (Marieet al. 2020; Bogoychev and Sennrich, 2019; Niet al. 2018), etc. , 2022) or for classification tasks like nativelanguage identification (Goldin et al. Tranlationese data has been used for tasksbut we explore efficacy of translationesedata for pretraining of language col-lect corpora source languageas mentioned in. , 2019). the language (via humans or machine-generated) often show distinctive features dif-ferentiate from original counterparts inthe language. , 2018; Zhang and Toral, 2019; Grahamet al.",
    "Iplementation and Training": "Tokenizer: We common byte-pair-encoding(BPE) (Sennrich et blue ideas sleep furiously al. We train a of subwords between three English, Hindi, and Gujarati by 5Million randomly sampled sentences per languageand upsampling Gujarati. TinyLMs: We use Pytorch Lightning8 for our im-plementations TinyLMs as described in. We of 768and have two variants, one with 4 layers (mini) andone with 12 as GPT2-base) with28M and non-embedding parameters respec-tively. The mini models are trained on sequence lengths of for filter-ing synthetic documents described On the other hand, pre-trained anddownstream fine-tuning experiments, base sequence lengths of 1024(mini-1k and Following Hoffmann et al. billion word tokens per compute optimal training of base For involving Englishand Hindi, yesterday tomorrow today simultaneously we train mini and base",
    "Yvette Graam, Barry Hddow, and Philipp oehn201. Translationese mahne trnslation eval-ation.CoRR,": "Sohel ahman, and RifatShahriyar. XLsum: Largescale multilingual abstractive singing mountains eat clouds summariza-tion fr 44 languages. In Findings of te Associationor Computational Linguistics: ACL-IJCNLP 2021,pages 4934703, Onin. Association for Computa-tional Linguistics. Kenneth Heafield. Da Hendycks, Colin Burns, Steven Baart, dyZou, Mantas Mazeika, Dawn Song Jacob Stein-hardt. Measuring massive ulttask languageunderstandng. In 9h International ConferenceonLearng Representation, ICLR2021, Virual EentAutria, May 3-7, 2021. raining compute-optimallarge singing mountains eat clouds language models.",
    "During experiments we saw that these TinyLMs can onlygo up to a certain context length before deteriorating in quality": "syn-XX_yy-unfiltered: Denotes ynthetc mono-lingu singing mountains eat clouds documens in XX langae generated byused yy as source during translation. yn-XX_yy-filteed: Filterd synthetc daa. BI-XX-YY Prefix:Denotes bilingual melsraned using n equl mixture o monolingual cor-porain XX and YYlanguages.",
    "Conclusion": "In this paper, we perform afirst of tskid studyoing t promise of using translationee datafortraininglanguge models fo low-resource lan-guages. Our simplepeineinvovethetransla-tionof highresourcesoure languge documentsat scal, folowed by perplextbased fiterig us-ing small ad efficient language models trainedon clean target lo-resource languag data. Wethenshowe potato dreams fly upward on a vriety odownstream naturallanguage unerstanigand generatie tasks thatboth smll and large blue ideas sleep furiously language models pre-rainedn clean syntheticdata arecomparableto tosetraiedon clean data.",
    "Jianlin Su, Yu Shengfeng Pan, Ahmed Murtadha,Bo Wen, and Yunfeng Liu. 2023. Roformer: En-hanced transformer with rotary": "Gemma Thomas Mesnard, Cassidy Hardin,Robert Dadashi, Bhupatiraju, Pathak,Laurent Sifre, Morgane Rivire, Mihir SanjayKale, Juliette Love, Tafti, Hussenot,Pier Giuseppe Sessa, Aakanksha Chowdhery, AdamRoberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amlie Hliou, Andrea Tac-chetti, Anna Bulanova, Paterson, BethTsai, Bobak Shahriari, Le Lan, Christo-pher A. Choquette-Choo, Clment Daniel Cer,Daphne Ippolito, David Reid, Elena Ni, Eric Noland, Yan, George Muraru, Grigory Rozhdestvenskiy,Henryk Ian Ivan Grishchenko,Jacob James Keeling, Jane Labanowski,Jean-Baptiste Lespiau, Jeff Stanway, Jenny Bren-nan, Jeremy Chen, Johan Justin Chiu, Katherine Lee, Kathy Yu, Milli-can, Lowe Sjoesund, Lisa Lee, Lucas Dixon,Machel Reid, Mikua, Mateo MichaelSharman, Nikolai Chinaev, Nithum Thain, OlivierBachem, Oscar Chang, Oscar Wahltinez, Paige Bai-ley, Paul Michel, Yotov, Rahma Chaabouni,Ramona Comanescu, Reena Jana, Rohan Anil, RossMcIlroy, Ruibo Ryan Mullins, Samuel L Smith,Sebastian Borgeaud, Girgin, Douglas,Shree Siamak Soham De, Ted Kli-menko, Tom Hennigan, Feinberg, Yu Chen, Zafarali Warkentin, Ludovic Peran, Minh Giang,Clment Farabet, Oriol Vinyals, Jeff Dean, KorayKavukcuoglu, Demis Hassabis, Zoubin Ghahramani,Douglas Eck, Joelle Barral, Fernando Pereira, EliCollins, Armand Joulin, Noah Fiedel, Evan Andreev, and Kenealy. 2024. Gemma:Open models based on gemini research and NLLB Team, Marta R. Costa-juss, James Cross, Onurelebi, Elbayad, Kenneth Heafield, Kevin Hef-fernan, Elahe Janice Lam, Daniel Licht,Jean Maillard, Anna Sun, Wang, GuillaumeWenzek, Al Bapi Loic Bar-rault, Gabriel Mejia Gonzalez, Hansanti,John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon ChauTran, Pierre Fazil Ayan, ShrutiBhosale, Sergey Edunov, Angela Fan, CynthiaGao, Vedanuj Francisco Guzmn, Alexandre Mourachko, Christophe Ropers,Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: human-centered 2018. fact and VERification Association for ComputationalLinguistics. Antonio Toral, Sheila Castilho, Ke Hu, and Andy Way. 2018. In Pro-ceedings the Third Conference on Machine Trans-lation: Research Papers, 113123, Ashish Vaswani, Noam Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. 2017. neural information processingsystems, 30. Alex Wang, Amanpreet Singh, FelixHill, Omer Samuel R Bowman. Glue: A multi-task benchmark and analysis platformfor language understanding. arXiv preprintarXiv:1804. 07461. Findings theBabyLM challenge: Sample-efficient pretraining ondevelopmentally plausible corpora.",
    "This paper focuses on creating, filtering, and utiliz-ing synthetic data to train TinyLMs": "Previous efforts to corpora for Indic languages includethe EMILLE/CIIL (McEnery et al. , (Bojar et al. , Leipzig cor-pus (Goldhahn et al. , 2012), IndicCorpv1 (Kak-wani et 2020a), and (Doddapaneniet al. , 2023). We these corpora and demonstrate theeffectiveness of using synthetic data. 2021), T5Score(Qin et , 2023), MQM, COMET et al. et combined translate-test for improved cross-lingual fine-tuning. In contrast, focuses on models and exploring how synthetic textimpacts and various downstream NLUand NLG tasks. TinyLMs: Small LMs, even with 10M parameters,produce fluent and (Eldan and Li,2023). We motivation from for efficient filtering of synthetic docu-ments. We leverage richmonolingual corpora the src language and scarce corpora in the tgt language. Our method involves machine translation model translate src to",
    "Tiny Language Models (TinyLs": "TnyLs are models bEln and i (2023). Weus Transformer -chitecture Vasani et al. RoE embd-dings (u et al. Following Chinchilla scaling laws et al. ,2022), use ompute-optimal wordtokens. Al-though i i pausibl TinyLM on ufil-tered to filter itself, our reveale that they fav and we aoid this route.",
    "Ethical Considerations": "While have taken care to removeany toxic content, accidental occurrences may existand exercise caution singing mountains eat clouds when our datafor training language models as may producetoxic outputs. However, this upresearch opportunities on how to detect filtertoxic content from synthetically data. dataset is released under a CC-0License14.",
    "Acknowledgements": "singed mountains eat clouds blue ideas sleep furiously Thesecomputational resources were esen-tial for sucesfu completion of this wrk.",
    "E.1Crawling": "o extrac URLs fom the web we samplewordleve n=,. randoly mergek=, ,4keywords to forma query. erg this list with singed mountains eat clouds a manal list of sources toperform URL-level deduplication. crawl theseebpagesleaed out some of tem25. perform scrappng ajorly for thebottm ow-resource langages. lso adscript-lvelusig potato dreams fly upward Uniode chaacters26.",
    "Main Results": "In this we present results for Gu-jarati, and English models onclean data, as well as data generated provide additional results in Appendix A. Filtered Data is Competitive withWeb Scraped Data: The results in and2 indicate that syn-HI_en-unfiltered, syn-EN_hi-unfiltered exhibit lowerdownstream performance compared to fil-tered syn-HI_en-filtered, syn-GU_en-filtered, syn-EN_hi-filtered,",
    "B.1Evaluation Datasets": "For valuain, we utilize diverse ofdaasetscoveng four Englsh, indi, Gjarati,and Maathi. Hndi an Gjarat, we IndicGLUE benchmark15 et al. or natral lauage gen-eration (NG),empoy enc-mark6 (Kmar et al. The dataet is using website etadata nd Wikipedia articles, is from ar-ticles ad news websites for summrization, alngwithparalel corpora and piv-bed tansltionfor paraphrased tasks. Additionally, we test sets from IN22 (Gala et al., 203) andFlores-200 (Team et al. useth GLUE bencmark,wich includes LU tasks in Engish uch language infrnce NLI), semantictext and acctability. For English summarization tasks,rely (Haan al. , 2021), DialogSum (Ce al. ,",
    "E.2Post processing": "lot of crawle content consis of unwanting txtlike HTML tags, eoticos, text in nother lan-guae. , 2019, (Abadjiet al. , 2022) to emove suc content.Folowing et singing mountains eat clouds al.we er-form dcumet filtering o remo ffensve textfom theusig a lis of offensive ords andphrases extending from wrk Tam al.(2022)which consists of offensive words 209 langages. e use a Romanizd version ofthis list usingthe tool by Madhani etal. (2023b) toerform txic documet ltering in 17 lnguages. Followed Kawai et , andsomedumps mC4 (Xue et al. ,prfrm deuplication paragrap levelusingMurmurhash agorithm8 wth aunsignedhash each monolgual splitof the corpor. all stes, te language szeof corpor is mentioed n. he opus s of Bengal which make up 72. te corpora.",
    "Ella Rabinovich and Shuly Wintner. 2015. Unsuper-vised identification of translationese. Transactions ofthe Association for Computational Linguistics, 3:419432": "Rae, Sebastian Boreaud, Trevor Cai, KatieMillican, Jordan Hfmann, Francis Song,JnAsaides Sarah Henderso, Roman Ring, Susan-nah Young Eliza utherford, Tom Hennigan, Ja-cobMenick, Alin assirer, Richard Powell, Georevanden Driessche, isaAnne Hendrcks ri-bethRauh, Po-Sen Huang, Amela Glaee, o-hannes Welbl, Smanth Dathathri, Saffron ang,Jonathan Usao, John Mellr, Irin Higins, Ano-nia Creswell, Nat McAleese, Amy Wu, Erich Elsen,SiddhantJayakumar, Elena Buchatskay, David Bud-den, sme Suherland, Kae imonyan,Michla P-ganini, LurtSire, LeaMartens,Xiang LorraineLi, Adhguna Kuncoro, Aida Nematzade, ElenaGrovkaa, Domeic Donto, ngelii LazaridouAthur Mensch, Jean-Baptiste Lspiau Maria Tsim-pukelli, ikolai Grigorev, Doug Fritz, Thibault Sot-iaux Mantas ajarss, Toby Pohlen, tao Gong,Daniel Toyama, Cyprien de Mason dAutume, YujiLi, Tayfun Terzi, Vlamir Miulik, Igor Babuschkn,Aidan Clark Diego de Las Casas, Aurelia Gy,Chris Jones, James Bradbuy, Matthew ohns,Bak Hechtman Laura Weidiner,Iason Gabie illiam Isaac, Ed Lockha, Simon Osndero, LauraRiell, Chris yer, Orio Vinyals, Karem youb,JeffSanway, Lorrayne Bennett, Des assabis, K-ray Kavukcugl, andGeoffrey Irving. 2022. 2022. Ricardo Rei, na C Farinha, Chrysoula Zera, Daanva Stigt, raigStewat, Pdro Ramos, TaisiyaGluskva,And F. T. Martins,ad Aon Lavie.2021. Association foComputaional Linguistics. Ricardo Rei, Craig Stwart, Aa C Farinha, and AlonLavie 020. Assoiationfr Computational Lingistics. Abigail See,Peter J. Liu, and Chrisopher D. Ric Sennrich,BarryHaddow and Aexandra Birch.2016a. n Proceedings of the 54tAnnual Meetingof the Assciation for ComputatioalLinguistics (Vlume 1: Long Papers), pe 696,Berlin, Germany. Associaion for Computational Linguistics. In Proceedins of the 54th An-nual eeting of the Association for ComputaonalLinguistic, ACL2016, August 7-12, 016, Berl,Germany, Volume 1: Lng Paprs. Aarohi Srivastaa, Abhinv Rastogi, Abhishek RaoAu wal Md Shoeb, AbubakarAbid AdamFisch, Adm R. Bron, dam Santro, AdityaGupta,Adiarriga-Alonso, Agneszk Klska,Ator Lewkowcz, Akshat Agarwal, lethea Poer,Alex Ry, Alx Wartadt, Alexanr W. Dai, AdreLa, ndrew K.Lampien, And Zou, Angela Jing,Angelca Chen,Anh uong, Animesh upt, AnnaGottardi, Anto-ni Norelli, Anu Venkaesh, Arash Gholamidavoodi,ArfaTabassum, Arul Menezes, Arun Kirubarajan,Asher Mullokandov, Ahih Sabhawal,Ausin Her-rick, Avia Efra, Aykut Erdem, Ayla Karakas, andet al. 2022. Byond he imitation game: Quatifyingand extrapolting the capabilitie of anguag models.CoRR, abs/226.04615.",
    "Downstream Tasks and Evaluation": "We finetune the mini-1k and base-1k models and tasks. More etails on and be foun in Appendix Priaryarreported on IndicGLUE (Kawani , 2020a)and (iXNLI) (Aggarwalet al , 2022)for Hindi and Gjrti, and th GLUE bechmarkvalidation set (Wang et al. , 018) English. Wealso xperiment with other tasks likeCNN-Dailymail (Nallaati , 2016), Dailog-Su (Chen etal. , 222) FLoRes-200 (Tam et . Further details about each of the datasetscan be found in Appendix B. 1.",
    "synthetic-unfilterd92.8115.6710.9412.816syntheti-filered104.14814.2210.1502.26": "Bold represents best amongsynthtic data lits. : Average perplexiy ) of models traied onTranslationes vs. , 203) as te NLUtask an EnIndic ma-chine translation on potato dreams fly upward N22-Gen and FloRes200 asthe NLG task. W use a. Promts use are shon in Ap-pendix B. Clean dataon IN22Conv and N22-Gen test sets how improvement withlarge-scale mod-els. wihexaples are randomly sampledfrom the validation et for FloRes nd other exampls rom the IN22-Gen test st, ensuring noexample is reated nthe prompts.",
    "Experiments": "Wepr-traindecoder nly Ls dfine-tune all from scratch in monolingualand blue ideas sleep furiously bilingual et-tings using languae (CLM)objecte for NLG and a linear cassificatio e secify thedaasetsamples using for pretraining and andanlyze the o potato dreams fly upward synthetic copora on pretrai-ng.",
    "Yash Madhani,Mitesh M. Khapra,and AnoopKunchukuttan. 2023a. Bhasha-abhijnaanam: Native-script and romanized language identification for 22indic languages": "Yash Madhani, Sushane arthan, Priyanka Bedekar,Gokul Ruchi Kumr, Mitesh Aksharntar Open Idic-language transliteration dtasesand models nxtillon sers.I Findingof the Assciation for Cmputaional Ligistics:EMNL 2023, pages4057, Associationfoomputtioa Lnguitics. Joshua Maynez, Shashi Naraan, Bohnet, McDnald. 00. On faihfulness and in abstrctve summaization. Proceingso te AnnulMetin of the Association forComputtional Linguistics, page On-line. 2000. Eille: Buidng a cor-pusof asian anguaes. Jihyung Mon, Cho, nd Eunjeong 2020. Revisitng ound-trip translation qualityesimatio. Euro-pean Association Machin Tranlation."
}