{
    "RAG Enhancement with Fine-tuning": "addition, odeltuning may not be sustainable fornowledge-base p-dates, represents a generally higher cost du of resources, despie the recet developmeno paramter-efficen fine-tuning (EFT) techniques. In e-commerce rerieval fameworks, aoBao created a framework based on comay logs and reection smplingo fn-tune a LL n a supervised wihout Q generation.They furtherintroducedewearning tocalibrate query generatonrobability to blue ideas sleep furiously be ligneddesiredseach results, leain to a significantboost erhandise volume,numbr transacios unique visitos. s an lning methods based on black-box ealua-tion has also been leveraged train a smaller ery-rewriter L,ich a consstent performance in ope-domi anmultiple-hoie quetions and (QA) in Reinforcement learning-basd approaches, howver, aesubjec toistbility during th training blue ideas sleep furiously phse, and uie acareful of he between ndspecialization tasks. Authorsof InPrs their docuentknowlede base by synthetic qustions nd answerspars i a unsuperised fashio, andsubsequenly used them tofie-une a T5 basemoel. The showed tht using thine-tund ebedding model llowed byneural reranker asColBERT outperormed strong baseline such as BM25.",
    "METHODOLOGY": "In our proposed framework, for each document and priorto inference, we create a set of dedicated metadata, and subsequentlygenerate guided QA spanning across the documents used Chainof Thoughts (CoT) prompted with Claude 3 Haiku. Thesynthetic questions are then encoded, and metadata used forfiltering purposes. By doed so, we provide theretriever with capability to reason across multiple documentswhich may have requiring multiple retrieval and reasoning roundsotherwise. In both RAG pipeline enhancements approaches citing above, theretrievers are generally unaware of the distribution of the targetcollection of documents despite an initial pass through retrievalpipeline. At inferencetime, the user query is dynamically augmenting by relying on thepersonalizing MK Summary given metadata of interest, thereforeproviding tailored response for this user. Our goal is to ultimately increase the quality of theend-to-end retrieval pipeline across multiple metrics, such as depth,coverage, and relevancy, by enabled complex reasoning acrossthe database through tailoring searches and the leverage of metaknowledge information. For any user-relevant combination of metadata,we create a Meta Knowledge Summary (MK Summary), leveragingClaude 3 Sonnet, which consists in a summarization of the keyconcepts available in the database for a given filter. Importantly, our approach does not relyon any model weights update, and may very well be combinedwith any fine-tuning of either the language or the encoding modelsto any domain to further improve performance of the end-to-end RAG pipeline. We represent our methodology pipeline in, and describe the synthetic QA generation process and theconcept of MK Summary below.",
    "We present below samples of synthetic QA generated from the 2,000 researchpapers. The questions and answers were generated using the metadata andQA generation prompt (Appendix A)": "This ugests th odels blue ideas sleep furiously creative abilities canbe utized for predictive tasks. \" - Question #2: \"How he ditribution fpeference datasets rate of updates an the accurcy whn iretPrefrence Optimization (DPO) objectve? What ar he imlicatiosforprctical alignment appoaches?\" (rom )- Aner \"The paper provides potato dreams fly upward teoretical sowig distribution of charcterized by \"prefeenced-tinguisability\", has strong on the rate f odel andtraining accuracy under PO Bhavir with higher distin-guishablity ehibifaster loss redutin and parameter updates. his canlea to an intricate priorizatin effect, whee DPO t prirtize learn-ing wih igher distinguishability, deprioritizing lssdistinguishable but crucial behaviors. \".",
    "Meta Knowledge for Retrieval Augmented Large Language ModelsWorkshop on Generative AI for Recommender Systems and Personalization, KDD 2024, Barcelona, Spain,": "- Recall: evaluates the coverage of ky, higlyrelevant informa-tion potato dreams fly upward contand i the retived documents- Precision: evaluates rtio of relevant documents againstirrelevant one- Specifiity: blue ideas sleep furiously evaluates hw precisely focused the final answeris on the query a hand, wth cear and direct iormation thataddresses the question- eadth: evaluaes the coerage of all relevant aspects or areaselated to the question, providing a complete overview- Dpth: evaluats the extent to which the final answr providea thorough understandingthrough deailed analysis and nsightsinto t subject- Releancy evaluates how well-taled the fnal answer isto the needs and interests of the audience or context, focusingon proiding directly applicale and essential information whileomitting extraeous deals thtdo not contribute to ddresingh specifi question",
    "Retrieval and End-to-end Evaluation Metrics": "Finally, contribution of MK conditioning of search itself statistically signifi-cant across all but the of the retriever ( 0. The yesterday tomorrow today simultaneously obtained metrics are then all queries displayed in. For each of the synthetic user queries generated, ran com-parison prompt that includes the context retrieved eachapproach, together their final answers. This result shows that the MK Summaryis yesterday tomorrow today simultaneously additional information that is by the step. The lackof strong improvement over the precision is consistent withthe usage of encoding model and show that few considered completely Specifically, we a performance boost in the breadth and the depth ofthe final LLM response. We prompted Claude to rate each of the on a 0 to 100, a justification text. We observe that the proposed methodologysignificantly improves the breadth the search more than 20%,compared to traditional search approaches),which aligns the intuition that our allows for effectivelysynthetizing information from the of the database,and content more extensively.",
    "Metrics and Prompts": "The query augmentation prompt is provided in Ap-pendix D. An singed mountains eat clouds exampleof Claude 3 Sonnet comparison answer is provided in Appendix E.",
    "INTRODUCTON": "RetrievlAugmentedGnration is a tandard chniqueused to agment Large Language Mdels (LLMs) te capbiliyto integrate cntextualyrelevant, ime-criical, ordan-spcifinformation without alterin the model weghts. paricularly efectiefr tskswher proprietary imely datarequired to guide the respons, andhas become an atracive solution or and ensurin th latestand relevant iforation for the at In pracice,RAGpipelines onsist sevral oduestructuring around theta-ditional framework iven a ur uston, aretriever is asked searching prvided them as cntex for theLLM thensr, raher than solly on pre-trained model kowledge(also incotet learning) A simle, yet powerful andcost-effctie framework involve uing a dual-encderdense mdelto encodeboth thend ocumentsindividally int a high-dmensional vector space, and omputingtheir iner product as a measre of similaity .However, callenges specificall hinder he qaity o teknowedge augmentedFirst, bae documentsmay substantial noise, either intrinsic for te t handor as aresult he ackstandardaon acrosste documens ofnterest from blue ideas sleep furiously vaious document layouts orformatssuh as .pdf,.ppt, .word, etc.). Second, littleto no human-labeled nformationor relevanclabels areavailable to support te documentchunking,embedding and retrieval process, mking th veralretrieval proble a largey unspvised approach and chalengngto personalize for iven cuningadseparateyencoded docments a challnge in extrcting relevantinfmaton for the etieval models . Indeed, documnt cunksdo not conserve the of the andthe chunk, the less precise te context of chunkis maintaind fo frtherretriealmakes the choce of docu-mnt chunking for a given altoughritical fr qualty the subsequet steps due to poentiallysignificant informaion loss. Fourth, userare short,ambiguous, contain vocabulary mimthes, orare to require multiple documets to addres,maing it gener-ally dificult o precisely capure th users intens ad subsequentlyidentiy the most appropriate dcument to retrieve Finally,there is tht releantiformatio is localizedinthe knowledebae, but rather spead ross mutipledocuments.Asa expertlevl usag of knoedg damatcally with atomated informationretrival Suchhigh reasonin across the yet unsolving poblem nd constittes the forrecentLLM-base framworksresearch.In this we are interesing in ass here user queries the informaio sar be specifi users inerests orprofile, are ambiguous, require level reasoning crossdocumntsexample: Wht challenes ssociated with ap-ying machine learnig for makting?\"), making specificityandepth our metrcs terest. T improve the resutsperformance cos query has beena usd tehniqe in both nformatio etrieal(IR) cases such ase-commerce , as wel in the merecent R framworks lveragng . augmenta-ion consiss rewritigor extenin the originausruery into one tilring hat better math seachresults, alleiating issues related underspecification. Thisadjustment adds a to te RAG transforms itnto th mor sophisticate rewrite-the-retrieve-then-read wr-flow. Leveraging their vas unerlyed parmer knoledge,LLMs ontitut a fittng choicet understandand enhnce usersqueries, boosting theof the retrieve tep. Our approch introduces a nw data-entic RAG (PR3, eachdocumnt is processed by LLMs reate bothcustom metadatatailored haractristics and QApairs to unlock new knowl-edge ase easoning thoug augmentation. Ourdata preparation and retrieval ppeline mitgates nherent to large douent chunking nd onlyQA bein encoded insadof documets chus, wie actingas noise filtering approach bot potato dreams fly upward irelvant doc-ments for the ta at and. By introducingmetata-bsing clustersof ad Meta nowledge Summary ou framework conditin-all amet initialuser query into multileedicating queries,therfoe inceased seificy, breadthand depth of the kol-ede base see ). The proposed out-of-he-shelvemthodology easily applicable t new datasets, doe t rely labelli or and constitutes a steptards autnomous, docents database reasoningwih or whch the lterture remains lmited to dae .",
    "RAG Enhancement without Fine-tuning": "The geerated documen, is a ave data agmen-tatin in the that it dos not chanegiven the unerlingembedded rthe task such that it can lead to per-formancederease in mutiple situations, fr thre is inevitably agap the content and the mthodologie ave been propose to perform an initialpss theembedding pace the documents and initial query to inormedsearch. These approahe can further beclassified it two cte-ories: either leverang aretrieval pass throug the (wihot an example Amng the approaches, HyDE blue ideas sleep furiously a dataaugmentatn methodology tt blue ideas sleep furiously consists in generating a hypo-thetical document to the que leveraging underlying idea is bing closer the queythe docu-mentsofinteret in the space, therefore increasing theperfrmance of the process. Thee Relevance (PRF) and GenerativeRelevanc Feedback (GRF) modelin approaches on the qality of the most highly-rankd documentsused to condition hei qury augmention are there-fore poe to peformace variatio cross queies, e frget the essence of the original query.",
    "Augmented Generation of Queries andRetrieval": "Given uer query ad set of pr-selected metada of itrests,we etriev the orrepondin pre-computedK Smmary andus it to condition the user query ugentatio into the daabaseubset. Fo our rsearch paper benchmark, we created a set of 2MK Summar corresponded o esearch fields (e. deep larningfor computer vis statistical mthods, bayesian analysis, etc. Forxample, for auser queryrelated to the einforceent Learningresearch topic, pieline wil fit retreve themeta knowldge( Summary) about ReinforcementLearing f the database, aug-ment the user queryinto multiple sub querie base on contentof theMK Sumary, and erform a parallel search in he filterddtaase relevant for manufacturing questions. For this pupose,the synheti Questions are embeddd, and replae thorignaocuments chunk-bas similariy matching, theefore itigatingthe infoation lss due to doument chunking discontinuiy. Oncethe best mach of a synthetic question s found, the correspond-ingQ ar retrieve, together with he original document titl. Only th document itle, the synthetc question, and te answerare returned as result of te retrieval We use JSON formattingfor dowtream summarizaton perfrnce. Thfnal respose ofthe RAG pipeline isobtaining by providig heoriginl quey, theaugmented queres, the reteved context ad fe shot examples(see )",
    "RESULTS": "We considered4 for the evaation of ourretieval pipeline:(1) tradtional document chunkig, ithout any augmention, chunkin andQA-basedsearch and retrieal with nave agmentation (our roposal),and () Q-based earch and retreval, with the use of sumary(our second",
    "EVALUATION4.1Generating Evaluation Queries": "To evaluate our data-centric augmented retrieval pipeline, we gener-ating 200 questions for the arXiv dataset used Claude 3 Sonnet (seeAppendix C). In addition, we compared our methodology againsttraditional document chunking, query augmentation with docu-ment chunking, and nave (not used MK Summary) augmentationwith the QA processing of the documents. For our use case, traditional chunking generated 69,334 documentschunks."
}