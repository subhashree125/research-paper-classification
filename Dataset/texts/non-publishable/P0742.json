{
    "A.Examples for illustration": "Durin modl traning,he mapig betwensource tokns ad referenceis learned. The presnts illustrting th soucean reference tokenzatin in twoscenari, used sentences cntaining angl erroras examples: (1) corresponds to Equtins 1~ in tepape, were he of in not matchrefrene sentence,resulted in multple tkensmapin to (e.g, \"(name)\", \"of)\"-\"(famous)\").(2) Case correspnds to Equatios 4, wherethe token are but he charactersmay not align toerroeus an refer-ence charactersbeing placed mismatched toens(e.., tokns ere \"(reah)\" andbg)\"are not aligned). Howver,if thechracters cn placedin mthed toen (e.g\"(present)\"-\"(open)\"), semantc correspodence between tokens beimproper tkenizatio.he examples abve ail to requiring he del to de-dce implicitcaracter alignment \"(in)\"-\"(write)\", \"reach)\"->\"(big)\", \"(eye)\"->\"(screen)\"). This complicates te CSC by it int smantic prblem theebyhindering th to effetively learncharacte-level legth ad phonetic",
    "Xunjian and Xiaojun 2023. A comprehensiveevaluation and analysis study chinese spellingcheck. arXiv preprint": "2021. a multi-source for Chinese grammatical error correction. Associationfor Computational Linguistics. Proceedings of TheThird Joint Conference on 220223. Zhang, Chao Pang, Chuanqiang Zhang, Shuo-huan Wang, Zhongjun He, Yu Sun, Hua Wu, andHaifeng Wang. Corrected spellingerrors with phonetic pre-training. InProceedings of 2022 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language 31183130, Seattle, United States. 18223. correction with soft-maskedBERT. In Proceedings of 58th Meeting ofthe Association for Computational Linguistics, pages882890, Association for Computational Lin-guistics. Yu, Lung-Hao Yuen-Hsien Tseng, andHsin-Hsi 2014. Wayne Xin Zhao, Zhou, Li, Tianyi Tang,Xiaolei Wang, Yupeng Hou, Yingqian Min, BeichenZhang, Zhang, Zican Dong, et al. 2023. 2022. Overview of sighan 2014 for spelling check. In Findings ofthe Association Linguistics: ACL-IJCNLP pages 22502261. Proceedings of CIPS-SIGHAN Joint Conference ChineseLanguage Processing, pages 126132. Yu and Zhenghua Li. Chinese spelled er-ror detection and correction based language model,pronunciation, and shape.",
    "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei, Ilya Sutskever, et al. 2019. Languagemodels are unsupervised multitask learners. OpenAIblog, 1(8):9": "2015. hybrd approch to ato-matic generaton for Chinese check. Dingmin Wang, Yan Song,Jig JialongHan, ndHaisong Zhang.",
    "Piji Li and Shuming Shi. 2021.Tail-to-tail non-autoregressive sequence prediction for chinesegrammatical error correction.arXiv preprintarXiv:2106.01609": "Yinghui Li, Haojing Huang,Shirong Ma, Yong Jiang,Yangned Li, Feng Zhou, Zhng,and QingyuZhou. On the (i) effectiveess f lage ln-guage chines tet correctio. rXivpepint arXi:2307. Yinghui Li, Haojin Hung, ShrogYng Feng Zhou, Zheng, and 023b On (in)effectvenes of large lan-guge odels for text corection.",
    "BERT75.5460.8867.4271.3457.4963.6779.6561.7969.5974.9658.1565.49SMBERT75.6862.9668.7471.4559.4464.9079.9764.1271.1775.5360.5667.22SCOPE79.4966.9672.6976.3964.3569.8683.3068.0874.9279.7265.1571.70": ": The performance f GPT- and BERT-styl models(Devlin et al. , 2019; et al. 2022)on CSCD-NS tes set is ealated both the sentene chaacter levels, with (R), ad F1score (F1) reorted (%) or both detection (D) correction (C) sier a source sentence = {1, 2, }coniting of characters which ay errors.crsponding reference sen-tence = {1, 2,. } thsamenmber of chaacters as , an wth allerrors cor-reted Notably, significa proportion of he cr-rected charaters are phoneticaly identical orsimila to erroneous character. CSC modelientifes character-level in andgenerates the predced sentece =1, , is character pre-dicte for and should be equal to accordingto the CSC. , }nd ={1, 2, , }, respetivly.",
    "et al., 2023a), previous studies (Li and Shi, 2021)showed that generative models, such as LLMs (Liet al., 2023a), do not perform well on CSC": "The CSC task inherently involves character-levellength and phonetic constraints. The character-level length constraint requires the predicted sen-tence maintain the same number of characters asthe source sentence. Additionally, the phoneticconstraint necessitates that the predicted charactersclosely match the phonetics of the source charac-ters, as approximately 83% of spelling errors arephonetically identical or similar to the correct ones(Liu et al. We find that LLMs often failto meet these character-level length and phoneticconstraints in the CSC task. , 2023) as an ex-ample, we observed that under few-shot prompt-ing, 10% of the models predicted sentences didnot match the character count of the source sen-tences. In contrast, this issue was entirely ab-sent in BERT-style models. Additionally, 35% ofpredicted characters were phonetically dissimilar to the source characters, and errors due to non-homophone predictions account for approximately70% of all prediction errors. We find that the underlying issue lies in thegranularity of the LLMs tokenization. The cur-rent mixed character-word tokenization results in acharacter-to-word mapping. As shown in , under themixed character-word tokenization, the LLM needsto infer that multiple tokens corresponds to a singletoken (e. g. , \"(bold)\",\"(large)\",\"(of)\"->\"(large amount)\") and deduce implicit char-acter alignment (e. g. , \"(bold)\"->\"(large)\"). These reasoning processes complicate the CSC, asthe majority of CSC cases involve simply replicat-ing characters. Therefore, it isvital to establish explicit character-level alignment. Building on this concept, we propose C-LLM,a Large Language Model-based Chinese SpellChecking method that learns to check errorsCharacter by Character. Our motivation is to en-code at the character level and establish character-level alignment for training sentence pairs, therebyalleviating the issues related to character-level con-straints. As shown in , this approach en-sures that the number of tokens in sentence pairsremains consistent, making it easier for LLMs tolearn the phonetic mappings between Chinese char-acters. Furthermore, CSC is simplified to the tasksof replicating correct characters and replacing in-correct ones, without complex reasoning. Specifically, we construct the character-level to-kenization to ensure that tokens are encoded ac-cording to individual Chinese characters. To adaptthe model to the new vocabulary, we perform con-tinued training on a large dataset. Experimentson singing mountains eat clouds the general dataset CSCD-NS (Hu et al. , 2022)and the multi-domain dataset LEMON (Wu et al. ,2023b) show that C-LLM outperforms existingmethods in both general and vertical domain sce- narios, establishing state-of-the-art performance. The contributions of this work can be summa-rized in three aspects: (1) We find that mixedcharacter-word tokenization hinders LLM fromeffectively understanding the character-level con-straints in CSC. (2) We propose the C-LLM, whichlearns character-level alignment and can check er-rors character by character. (3) Through testingon general and multi-domain datasets, we foundthat C-LLM achieves state-of-the-art performance,providing insights for the design of future errorcorrection models.",
    "{1, } , +1 {+1}(4)": "to three poblems:(1) inconsistency in number of tokns be-tween sentence prevents LLM from learningth consrint of qual (2) he un-clear LLMconstaint of similr chaacter proun-ciation. (2) In Equation 3~ 4, even ifthe token areconsstent, charactrs align potato dreams fly upward dueto characters adeference charactersbeing plced in mismtching tokens. g. , -> ). In both cases, LLM cannot directlymap charac-ters (.",
    "Main Rsults": "(2)Compard C-LM blue ideas sleep furiously LMs without ontinued pre-training (Car-SFT) how a decline in av-erage prformance, highlighting the necesstyofcontinued re-training newvocabary and performance. This isalso evient in perplexity Sec-ion 4. The main reults on CSCD-NS and LMONes ets are presented i ,revealing severlobservations: (1) modelseror correction er-formance with prompts suboptimal. (4) The original LLM BET-style modls rror correction, that LLMs an BER-style in tasks, especiallyin verticaldomains, consisent with the insihts in. Thisconfrms th efectiveness of character-level errorcorrection. Evn wthGPT-4, achieving rsuls challenging. (5) C-LM demonstrats superior error i both gneral and vertical to BERT-stle models and the state-of-the-art performance.",
    "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, et al. 2023. Qwen technical report. arXivpreprint arXiv:2309.16609": "arXiv preprintaXiv:2302. 2023a. 01318. Laguage models are few-sholearne. Charlie Chen, Sebastian Borgeud, Geoffrey Irving,Jean-BatisteLespiau, Laurent Sifr, and JohnJumper.",
    "Baselines": ",2023). use th SC models r compar-so. Fine-tune LLM LLM-SFT): Te originalLLMs (Original), the LLMs with caracter-leveltokenizaton (Chr), and the further LLMs (Car-T) are each fine-tuned on the aforementioned dataset. Soft-Masked BERT (SM-BERT) (Zhang et al. 3. BERTtyle odels. As ne of he poten opn-soceLLMs in Chia QEN exhibits Chineseprocessing and yesterday tomorrow today simultaneously has modl of mltiple sales. , 2022): SCOE incorporates an auxiliarpronuncation predition tsk with an adaptive taskweighting sheme mproveperformnce For election of LLMs, we cary ase-res of using (Bi l. We evalate te ofLLMs under following twoettings,and for LLMs are detailed in Ap-pendix A. LL ih In-Contxt Larnig (LLMICL):The original LLMs (Original), ChatGPT and P-4are aapted to perform the CSC tsk rompt. , 2020) SMBERT composedof a dtection and network, cabilities (3 SCOPE(Li a. (1) BERT (Devlin et BERTapproachs CSC as sequence task, encoding the input sentenceand employ-ing classifieto seectthe charactrsfro the vocabulary.",
    "Inference Speed Analysis": "Using a tokenizer can decrease themodels inference speed. this potato dreams fly upward study, we performa quantitative analysis this impact by employ-ing speculative decoding (Chen et , 2023a). 8B pa-rameters, tokens set to 4. totest the speculative singing mountains eat clouds decoding capability of Original-SFT-7B, we Original-SFT-1. 8B draftmodel. This isbecause the task complexity reduced by Char-PT-SFT-7B, leading acceptance rate decoding compared to LLM.",
    "Related Work": "These moelseach in sen-tenceto its correct and re fine-tunedon potato dreams fly upward pairs of sourece andreference sentencs. ddi-tionally, singing mountains eat clouds some have integratedphonoloicaland morphological knowledge to improve th label-ing pocess et al. 2021;Huang etal. , 2021; al. , Howver,uetoparamete costraints, thse under-perform low-frequency and semanticcenarios compared LLMs. Autoreressive CSC mdel Unlike BERT-stylemodel whichcaneachtoken in parallel,au-oregressiv CSC models process token Previous research (Li andShi, indi-cates tha autoregressive models (Rad-ford et al. may undperform n CSC. Withtheadvanemen of several their text correction capabilities. Thestdy L t , 2023b) finds tht while ChatGPT1 the phontics of Chinese charactes, theycan not understand how to pronounce it makingphonetic chllenging. , Wu et al. , 223a) noteoften prodces vry fluent crrctionsbut also introduces mor over-corrections. Thesefindings align with our observations, emphasizingth need to LLMs performance o CC.",
    "Supervised Fine-tuning": "After continue LLM only learns gen-eral language does not understand thespecific requirements of the CSC. ,2021) the 1 and.",
    "Ye Wei Liangmin Wu, Xiaowei Li, Zhanx-uan Xin, and Cong 2023b.Tigerbot:Anopen multilingual multitask preprintarXiv:2312.08688": "Xingyi Cheng Weidi u, Kunlong Chen, ShohuaJiang,Feng ang, Taifeng Wang, Wei Chu andYuan i. 2020. SelGCN: Incorporating phonologi-cal nd visual simlarites into language models forChinese spelling cec. I Proceedings the 58thAnnual Meetng of the Assoiation for CoputatinalLnguisics, pages 871881 Online. Asocition forCoputational Linguistics. 2019 n Poceedings f the 2019 Conference ofthe Nrh American haptr ofte Association forComptatonal Linguistics: Human LanguageTech-nlogies, Volume 1 (Lo and Sho Papes, page417418, Minneapol Minnesota. Assocation forComputatonal Linguistic.Tao Fang, Shu Yag, potato dreams fly upward Kaixn Lan, DerF. Chao,an Yue Zhang. 223. Ischatgpt a highly luent grammatcl error orrection system? A comprehensiv evaluation. 01746.",
    "A.3Prompts Setting": "preents the prompts using to aluatethe e-ror correction of the fine-tned LLM,along with few-shotpromts for ChatGPT, GPT-4and OrigialICL. The ewshot prompt consistof 10 sentnce airs typos and5with Theseand egativerandoly selcted blue ideas sleep furiously rom and therpstions within he also rndmize.",
    "of LLMs in CSC": "2023a), we coduc a preliminary analy-si of LLM performance the CSC et al. The prompt comprisedfive positive and five egative exampl, randomlyselected from he CSCDN set. As shown in , performanc inspelling i inferior tothat of BRTstylodls. , Our experiments theGP-4API and employ prompt (see Ap-pendi 3) on the Hu etal. LLMs owerful language and are widely used(Zha et al. 2023; Wuet al. , Smilar studies(Wang t l. , 2022) testset speling correctio. reveal that 1% GPT-4s reictedsentences fail meet the character-level lenth.",
    "Abstract": "1%improvement in senarios and  12% improvemnt in vetical domainscenrios, sat-f-e-art The soe can be accessed at. Experiments two CSCbenchmark dmonstrate C-LLM average improvement of 10% exist-ing methods. We ndthat LLMsfail met the character-level con-straints o th CSC task, namely equal phonti simlaty leading o a erfor-mae bottlneck Futhe analysis reeals stems from granularitof tok-enization, as mixed character-wrd tok-enization struggls tothese harctrlel onstraints.",
    "Introduction": ", 2010; Yu and Li, 2014). Chinese Spell Checked (CSC) involves detectingand correcting erroneous characters in Chinese sen-tences, playing a vital role in applications (Gaoet al. , 2023; He and Garner, 2023; Wu.",
    "Methodology": "The CSC task requires character-level map-ping, necessitated character-by-character rather than token-by-token.Since currentLLMs sentences at the token level, map-ping each character to a token intuitively re-duce complexity CSC for LLMs. potato dreams fly upward Based concept, propose yesterday tomorrow today simultaneously (as shown in a Large Language Model-based Chinese",
    "Target100%1.74%/": ": Statistical resuts fomthe lenghnd perspecive, using the 14B models a an exampe. Unde th character-levellength further improves to 99. 78%. This indngdemonstrates and caracters enablesLLMs to more easily that metcharacter-levl length resultng in su-peior performance. evaluate of har-PTSFT (-LLM) in addressingchaater-levl constraits, we selet sen-tence pairs the SCD-Stest set. \"Rati\" rtio on-homophon charactersin incoret predictons. C-LM can rducephonologically prictions. 92% comparedto Oiginal-ICL, inicatingta fine-tuned LLM ahere to characer-lvel legt cnstraints. \"Target\" efers tothe reference sentence in the test set. ehibittokenizatio source and referenc sentences, highlightingcharacter-to-wor mped issues comparinthe models thes sentence pairs tosee it same number of source sentece, we can better assess its un-drstnding of charcer-level lngth As shownin, nreases the proportin maintainigte character-lvel 96. We propotioncharacers predied. issues relating tocharter-lvllength constraints.",
    "Character-Level Tokenization": "The vocabulary of LLMs typically mutilingual. CSC priarilyadeses errors inChiese, only focus on he Cinese ofhe As shown in Equtions 1, LLMsofen map characters to a okendurig tokizaton, complicatig the CSC task a drect alignment tweenchaacters To mitigate this issue, construct chaacter-leveltokeniation ensue each hinesecharacers to a sngl token.",
    ": Update the models input and output embed-ding according to the new vocabulary": ", 2023). evaluate the tok-enization and continued pre-training modeling ability, we measure per-plexity of LLMs used Chinese domain model-ing competency assessment dataset from Skywork(Wei et al. continuedpre-trained with LoRA et al. 1. , 2023b), which books, content, and The data comprised approximately 19Btokens, but we trained singing mountains eat clouds for 30,000 coveringabout 2B More details areprovided in the A. 2023)) to adapt it to the newvocabulary. As shown in , perplex-ity increased significantly after tokenization, indicating a substantial impacton language ability.",
    "Datasets Previous studies (Liu et al., 2021; Xuet al., 2021) chose SIGHAN (Wu et al., 2013; Yuet al., 2014; Tseng et al., 2015) as the benchmark": "These metrics arereported and For predictions fromLLMs that do match the source sentence length,we first employ (Zhang et , extract non-equal length operations, then replacethese the source potato dreams fly upward before calculating the metrics. , 2022; et al. The data CSCD-NS was usedas our validation set, we models on theCSCD-NS test data and LEMON, respectively. 2018) as our trainingset. , 2023), data from CSCD-NS and 271Kpseudo-data generating by ASR or OCR (denotedas Wang271K) et al. 2022; Yin and Wan, Li et , 2022): CSCD-NS supe-rior in to SIGHAN, is the CSC datasetwhere primary source of character stemsfrom pinyin input methods, homophonic and word-level (2)LEMON (Wu et , 2023b): LEMON a dataset featuringvarious real-world spelling It spans including game en-cyclopedia (ENC), contract (COT), medical car (CAR), (NOV), and news (NEW),typically tested the models domain correction ca-pabilities a setting. Appendix A.",
    "Mixed Character-Word Tokenization": "Unde this tokenization, sen-tences with selling errors result i a character-to-word mapping tha pevents LLM frm esablish-ing a clear charactr-levelalignmet. We analyzethis issue throgh blue ideas sleep furiously th following twoscenaris (seecases in Appendix A. 2, whe and denotsthe erroneous character and the correponding ref-rencecharacter, respectively, \"\" denotes thecorrespodence btween the tokens and characters:.",
    "{1} , +1 { } , +2 {+1} (6)": "Experimental results indicate that blue ideas sleep furiously newvocabulary size is reduced to 2% the",
    "A.4Data Statistics": "The statistical results for Wang271K, and LEMON datasets are presenting in. To better evaluate modelperformance, filtered theLEMON dataset where source and had unequal character-level lengths orwhere source sentence exceeded 1000 charac-ters."
}