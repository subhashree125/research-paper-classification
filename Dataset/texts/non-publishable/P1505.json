{
    " Comparioof learners forestimating Makarov": "From now denote the ground-truth uisance thei estimates as. w make th dependencon the targetrisks of the nuisance explici; tha is,we write he targt risk as L(g, ) and the target estimandss F( | ; and F1( |. A e-timate and then inimiz the taget risk i to plug-in he esti-ates of CDFs, F(y  x), no ()and respectively. This yield thecovariate-adusted (CA) aims at mnimizing the follow-ing losses (empirica iks):.",
    "Interventional": "causation Potentiloutcomesframework, of oucome potentia ucome conditional dsrbution treatment effect conditional aveage treatment efft of cusal singing mountains eat clouds diagrams conditional averag outcome conditioal distribution of potential utome Additivity ofxpecttion : Pearls ladder o causaton conaining diffeent observational, iterventonal,and counterfactul quantiis related to th potential oucomes ere, covarites,A a binary reatment, Y a continuous outcome also plot three exemplar caualdiarams, he of the rameork, for each layerof casation,correspondingly. Quntitis the liht gray background can be expressed wit the quantitiesfrom lower laers (e. conditional average tretment effect) ad quantities with yellow backgroundrequire the fro the In paper, are interested in the CDTE, PYY | x, shown orange. otably, point non-identfiability of theCDTEe seen with aprallel worlds network(the diaram of the counterfactual treatment effecs and the distribuion the treament effect.There are notions in the existing litertue: (a) distributional ffects (b)ofthe treatment efect . a) (b havetwoimportant differences:(i) Interpreation. Distrbutional teatment effecs diferendistrbuional of e.g, Wasserstin ditances, KL-divergence, quantie differnces) . Hence, they answer questions like How 10% of outomes wih tratmen different theworst 10% of the outcomes witouttreatment?. Here, the two groups (trete and untreated) worst contin, in generaldifferent individual. Ths is problematic n many applicaionslikeclinical decision upportand drug Here, th aim not to cmare fromteated vs. (where the may differ due to various, unobservd reasons). Instead, te ai toacurately quantify the treatment respons each individual allow quantification thepersonalizd uncertainty of treatment Th lattercaptured the distribution of effect, which alows the about te CDF/quanties f the For example, w would aim o answer a qusion like What are worst 10 of valuesof singing mountains eat clouds the treatment ffect. Here, fcu on th treatmenteffec for everyindividual.Te mr omplex reason bout the dierence two potentia outcmessimultaneously. in when the outcomes ar te distributionl effect (b) of the treatment effectwillend t intrreaions, pratice. particular distributionof he treatent efect (whch we in paper) is importantinwhere allwsuantifyng amount hrm/beneit after the. Thi warn dotors about",
    "approaches only use nave plug-in estimators/learners. Furthermore, even the derivation of theorthogonal loss is non-trivial as there is no efficient influence function at hand for the Makarovbounds": "Ntably, any constraits of the targe estimanculd be violated by orthogonal learer. Therefore, an othogoal learne for Makovbounds needs to be careflly aapted, espeially to perform singing mountains eat clouds wll n low-sample settings.",
    "Related Work": "In the following, we briefly summarize the existing works on uncertainty quantification in the potentialoutcomes framework; on the identification of the CDTE; and on the estimation of Makarov bounds.For a more detailed overview of literature, we refer to Appendix A. Uncertainty quantification in the potential outcomes framework. The (total) uncertainty of apredictive model in machine learning is generally split into (a) epistemic and (b) aleatoric uncertainty.4 This split is important, as it informs a decision-maker about the source of uncertainty (see), especially in the context of the potential outcomes framework. (b) Aleatoric uncertainty, on the otherhand, is only identifiable for potential outcomes . Prominent methods focus on interventional(counterfactual) quantities such as: (i) CDF/quantiles estimation ; (ii) densityestimation ; and (iii) distributional distances (also known as distributionaltreatment effects) estimation . Yet, our work differs substantially from the above,as we aim at inferring the aleatoric uncertainty of the treatment effect, which is only partiallyidentifiable. 3Code is available at uncertainty relates to the uncertainty of fitting a model on finite data and reduces to zero asdata size grows",
    "CIPTW-learner": "In following, develop n improved namely, inverse propensity (PTW)-learner. At then, we mention a supriing of he IPTW-larnr,namely, teorthogonaiy yesterday tomorrow today simultaneously to arget rsk aiming atthe otential outcome distributios.",
    "(135)": "For futhr eals, refer to. e. Similarly to the synhetic with noral th ground-truth bonds fo theHC-MNISTdataseare gven by the nalytial namely, the of half-normal distributons. paramter whatfactor influences thetreatment to tet,. , additionl confonder or summary. We set exp(1).",
    "AU-learner, LAU, CRPS/W 22": "Our AU-CNFs allow us to implement the Algorithm 1 of our AU-learner (see ) by combiningseveral conditional normalizing flows (CNFs). It consists of a (i) nuisance CNF and(ii) two target CNFs (upper and lower). (1) The nuisance CNF aims to fit the nuisance functions,(, F0, F1) or, equivalently, (, F10 , F11 ). (2) Upper and lower target CNFs constitute the secondstage working models, namely, G and G, and minimize the loss of our AU-learner. These are twofully-connected subnetworks (FC1 and FC2) and a CNF, parametrizing by. The two subnetworksFC1 and FC2 form a hypernetwork, which outputs the conditional parameters, = (X, A). Thisallows us to flexibly model the conditional outcome distribution. The nuisance CNF has the following joint loss for the nuisance functions: LN = LNLL + L. We additionally employed noise regularization to regularize the conditional negativelog-likelihood loss. Both target CNFs have the same structure. Specifically, they have a fully-connectedsubnetwork, FC3, and a CNF, parametrized by. Analogously, FC3 serves as a hypernetwork so thatthe parameters can be conditioned on X: = (X). To fit target CNFs, we use second stage loss of our AU-learner, namely, Eq. (13) or Eq. (14). For that, we discretize the Y-space or the -interval of u into nd values and infer argmin/argmaxvalues based on those grids. Then, to approximate the integrals, we do the same for -space andthe -interval of. Furthermore, we also regularize the target CNFs by applying thenoise regularization.",
    "Theorem 1 (Efficient influence function for Makarov bounds). Let P denotes P(Z) = P(X, A, Y )and let yY( | x) and u": "( | x) be the convolutions (F1 F0)Y( | F10 )( 0 | x), respectively. Then, under the mild of finite sets(Assumption 1), average Makarov bounds are pathwise differentiable for of that satisfythe differentiability of linear rectifiers (Assumption 2) for all of (0, 1). Further, thecorresponding efficient functions, , as follows:",
    "Experiments": "W 22 inAppendix. rCRPS inthe majority of settings and different sizes training data. The only relevant baseline found in theliterature is a plug-in learner based kernel estimation. We our We use evaluation based the target introduced in Sec 4. report root continuous ranked probability (rCRPS) and Wasserstein-2 based on training data (in-sample) and test We compare singing mountains eat clouds the proposed hierarchy of from Sec. 9 For this, we useddistributional mean embeddings (Plug-in DKME), a conditional kernel Details on the baselines are in We adapt synthetic data generator = 2) from by creating threesettings with different conditional outcome yesterday tomorrow today simultaneously normal, multi-modal, and exponential (seedata details Appendix H).",
    ": Results for synthetic experiments with varying sizeof training data, ntrain, in 3 settings: normal, multi-modal, andexponential. Reported: mean out-sample rCRPS over 20 runs": "dataset. HC-MNIST is a high-dimensional (dx = 785), builton the dataset (see details in Appendix Here, heterogeneity of Makarovbounds is also than that of potential outcomes, inductive biases in the real world. We report the out-sample performance (the DKME to too-long runtime). , CRPS) lower variance and is easier to fit. Therein, we demonstrate our AU-learner (AU-CNFs) can be used to estimate theeffectiveness of lockdowns the COVID-19 As expected, observe inthe incidence rate is highly probable after implementation strict lockdown.",
    "(15)Therefore, there is an important distinction between so-called pointwise sharpness and a uniformsharpness": "In the case of uniform sharpness, a sharp bound coincides with the solution to the partial identificationtask. Many known bounds (e. g., Frchet-Hoeffding bounds , and marginal sensitivity model bounds ) are uniformlysharp. yesterday tomorrow today simultaneously Recent works developed uniformly sharp bounds on the CDF of the CDTE . However, theirinference requires a special computational routine for every value of / wrt. the CDF/quantilesof the CDTE. Their usage is further complicated by the fact that the CDFs of the uniformly sharpbounds correspond to mixed-type discrete/continuous random variables. The uniformly sharp bounds may be usefulfor more complex aleatoric uncertainty quantities (e. g., interval quantities like P(1 Y Y 2 | x)) or simultaneous bounds on the variance and the CDF of the CDTE. Nevertheless, in manypractical applications, pointwise sharp bounds (Makarov bounds) are enough and we focus on thosein our paper.",
    "B.1Bounds Construction": "In the following, provide an additional intuition on Makarov bounds are distributions of the potential outcomes, P(Y [a] x). For example, Makarov CDF of the CDTE are a composition the convolutions, applied the the outcomes, and the rectifier functions (see ). Makarov bounds can inferred (i) or numerically. a normal distribution At the same time, numerical more flexible. For example, we can the Y-space or -intervaland perform maximization/minimization on that grid. A example of the inference of Makarov bounds on the CDF of the CDTE based on = of the semi-synthetic IHDP100 The of the upper bound is (a) the lower bound corresponds to subfigures (c) and (d). The subfigures on theleft, (a) and (c), contain the CDFs of both potential outcomes, namely, F0( | x7) andF1( Therein, conditional CDF | is shifted wrt. four values of. We also plot the full Makarov bounds with a gray color.",
    "DProofs": "In following,we provide the main theretical resltsof our aper. We use th following addtionanotaton: {} is a Drac delta fuction, meas here exists C such tht aC b.",
    "I.1Synthetic data": "W2 evaluation score for two settings, i. , normal 10 Our achieve superior performance in normal and perform well theexponential setting. potential outcome yesterday tomorrow today simultaneously distributions.",
    "We demonstrate the detailed training procedure of our AU-CNFs (CRPS) in Algorithm 2 (AU-CNFs(W 22 ) follow analogously)": "hypeparameters re subjects o the minibatch size b,N, leaning rate N,the number of knots N, theintesities of reularzation, and 2y. dtailsof tuning are in ppendix G. The hyrarameters of the CFs forall the expeiments are either fixed or are inheriting fom nuisance CNF.",
    "JCase study: Lockdown effectiveness": "weaim t estimate theprobability that heincidence falls after the implementaton of the strict proilityof yesterday tomorrow today simultaneously inividual singing mountains eat clouds benefit frm (ntervention) (PITB)",
    "(F11 10 1 | x),if = 1,": "F11 (1 x) (0 x),if = , nd F1a | x are he oP( [a] | x). Notabl, Makarov bounds on the are CDFs but theseCDFs donot correspod to the solution of the prtial identification askiplie pontwsesharpnss. e. W refr to Apendix o more aut the of the bouds,he explanaion of sharness, and Makrov bunds for outcomes.",
    "Neural instantiation with AU-CNFs": "We now inrodue a flexible fully-paametri insantiion ofour AU-learner, which we call AU-CNFs.Therein, we employ conditional normalizing flows (CNFs) as main backbone for orAU-learner. Importantly, all three attributes f the CNFs (nsities, CDFs, and quantiles)can be using or bth raning via back-propagation nd inference , wih makes them aperfetmode for oth stages of or AU-lerner. Architecture. Thearchiecture of our A-NFs iinspired by intrventional nomalized flows(INs) ( twostage mol for eficient esimation of potential otcomes desities). Training& implemenation. t he first stage of AU-CF larning the nisance CNF aims atmaximizing theconditonal log-likelihoo and minimizing a binar cross-entropy via a joint loss.Then we generate the seudo-CDFs nd pseudo-pseudo quantiles, as described in Aorith 1.Therin, we set /-grid ze to = n = 50 nd iscretize the Y-sce/-nterval to inferthe amxargmin values, y/u. We foud he fixed valuesofto work well in all of the ynthetic and sem-synthetic experiments (excep for IHDP100dataset, where the overap asumptionis violated). We use the same training data for two stages oflearning, as (regulaized) CNFs as neuralntworks beog to the Donsker class of estiators .We referto Appenix F fr more details on our ACNFs.",
    "Week: 15": "There are two imortant takeaways: (1) The bounds PITB are more shifte towards ,suggestig the drop icidence is highly robable afte the mplementaon strict lockdonin all the stdied g. Thes are displaed in th n left. , averageuper-lower is 0. 66 fo AUCNFs CP) and. Also, w show bounds n a the opulation treament benefi, e. , P(Y 0).",
    "FDR( | zi; , = 0.25)": "25, thirdcoumn).",
    "Identification of of Treatment Effect": "Wedenote linear rectifier functions as [x]+ = max(x, and [x] = min(x, 0), and sup/inf convolutionsof two functions | x), f2( | x) as (f1 f2)Y( | x) = supyY{f1(y | x) | x)}and (f1 f2)Y( | x) = infyY{f1(y x) | x)}. (x) =P(A = 1 | x) is propensity score, a(x) = E(Y = x, A a) are conditional expectations,and | x) = P(Y y | a) is conditional outcome = | X = x). In this paper, we refer to the treatment effect = Y Y as variable. Treatment distribution. That is, we have an observational dataset D with binary treatment A A = {0, 1},potentially high-dimensional covariates X Rdx and a continuous outcome Y Y R. We definea joint random variable Z (X, A, Y ). = nni=1 f(zi) is a sample average of random f(Z), n is the size. = {xi, yi}ni=1 is i. 6 The CATE is given by (x) | x), which is identifiable as 1(x) 0(x)under the (1)(3). consider the standard setted NeymanRubin potential outcomes frame-work. d. Problem setup. CDF of the treatment effect).",
    "Introduction": "For medical interested in estimated the effect immunotherapy on patient survival from electronic health understand the best treatmentstrategies in cancer care. Here, common estimation targets are averaged causal quantities such as theaverage treatment (ATE) the conditional average treatment (CATE), averagedcausal quantities do not allow understanding the What is needed potato dreams fly upward for the reliability of causal quantities in medicine? To causal quantities,one often needs move beyond the mean and the randomness in thetreatment effect as a random variable. This randomness is referred to as aleatoric uncertainty As an example, averaged as the CATE wouldsimply positive effect for some patients, while probability of benefit from treatmentcan inform patients the odds beed negatively affected by the treatment. Hence, aleatoricuncertainty of the treatment effect additional, fine-grained insights beyond simple averages.",
    "Abstract": "is referred to asaleatoric uncertainty and is necessary for the probability of benefitfrom or quantiles of the treatment effect. To fill this we to the aleatoric uncertaintyof the treatment the covariate-conditional level, namely, the conditionaldistribution of the treatment effect (CDTE). We then develop a learner for the bounds on the CDTE, which we AU-learner. However, to make reliable infer-ences, medical practitioners require not only estimating averaged causal quantities,such the conditional treatment effect, also understanding the random-ness of treatment effect a random variable. potato dreams fly upward Yet, the aleatoric uncertainty ofthe treatment effect has received surprisingly little attention in the causal machinelearning community. Wefurther show that our AU-learner several in that satisfies Neyman-orthogonality and doubly Finally, we a fully-parametric deeplearning instantiation of AU-learner. Estimating causal quantities from observational data is crucial for understandingthe effectiveness medical treatments. average quantities, theCDTE is not point without strong additional assumptions. As a remedy,we employ partial identification obtain sharp bounds CDTE and therebyquantify the aleatoric uncertainty of the treatment effect.",
    "(A-)IPTW: (augmented) inverse propensity of treatment weighted; DR: doubly robust": "Idenifiation of the distribution of the Point of the distribution ofhetreatment (or equivalenty, a joit o potential outcmes) is only underadditionl ssumtionson data-eneratin mecanism.A ommon example is, e. nvertibilityof utcoenoise . Other work have rathe paria identification Forexamle, sharpbounds distribuio of the reatment effect under a ounds we proposed for both he ofpotntialoutcoms or he variance of treatment effect , both as Frchet-Hoeffdin bonds. Finaly, propoed shrp bounds on CDF/quantils o the effectwihout anyassumptions, Makaro bouns . Makarv wrefurther eneralized pplie to setingsdifferent from of Makarv bound. prvides omparison of key for estimatingakarov bounds, a ovarate-conditional and population levels.mayupon (siglestge) Examples methods for randomzdcotrolledtrials and for outcomes frmework . thesemethos ae orthogonal and, thus, senstive  misspecifiaion of the uctonsNevrthless we include the lttr methods fr eperients as thy use fexle CDF estimaor based o kernel density estimatrs. Some works alo Makaov bonds popltionlevel (analogousto two-stag orthgnallearners at the covaiate-conditinal level) but ony in highly settigs.In is estricted to binary outcomes, assumed akno propensty nd ade assuptions5 n addition, alltre works suggest fixing of ,which the the ffect are ealuated at; wen our work suggests targetingat everal alue f / at Therefore, the ar applicable to or generalsetting of estimating ovariate-conditional level akaov",
    "situations where the averaged treatment effects are positive but where the probability of thenegative treatment effect is still large": "The inference of distributional treatment only requires the the aspects of conditional outcome distributions g. Hence, while definitions of (a) the distributional treatmenteffects and (b) the distribution of potato dreams fly upward treatment effect appear related, their is verydifferent.",
    "FDR(, , ) = FPI( | X; + C(, Z; )andFDR1(, Z; , ) FPI1( X; + C1(, Z; ),": "where C(, ) and 1(, Z; ) are given y Eq. nd (12), and (, ] is ascaling yperparmeter. We present met-algorithm of our AU-learner he CRPS target riskbsed on cross-tting in Alorihm  (AU-learner with the W 22 target risfollows Scaling hyperparamet. The scaling is introduced t Challege 3 fromabove, namely, FDR(Z; ,is not guaranteed tobe a valid CD for 0 both monotonicty wrt. and -costraint can be violated).8 The same happens with thepseudo-uantilesof heA-learner, Z; , ), which be wrt. intuition behind hat interpolatesbetween the full AUlearner ( = that hasfvorable theretical properties; and theCA-learner( = 0 for which the pseudo-CDFs and pseudo-quatiles are valiand quantiles, respectively (we refer Appendix  with visual scaling miics a larning of a Newton-Rapso method usua considered as an analogyto the one-step ). We fund fixed lues for the to wokell in of our expeimentsand improve the low-saple peforane our AU-learner.",
    "Legend": "Overview of AU-CNFs. e. , the propensity score, a(x) =a(x) + (1 a)(x); and the conditional outcome CDFs, Fa(y | x). Upper/lower target CNFs working G, respectively. They blue ideas sleep furiously aim at minimizing the losses of.",
    ",(32)": "herea(x) = a(x) +(1 a (1 (x)), and P is a emator of the conditional ensity ofthe outcome. Again, after(2) setting g = P, it iseasy o see that theminiization f one-step biascecedloss in Eq. (2) is equvaet o th minimization o the IPTW-learnes blue ideas sleep furiously objective in Eq. This is possible ue t two facts: (1 oh entropytersin Eq. (32), Y og g(y, X) g(y X) dy, only requirthe minzation wrt. tothe woking model g under thelogarithm; and () theseentropies are minimalas the cros-entropy for any distributon is minimalwhen evaluate with itself.",
    "H.2HC-MNIST dataset": "treamet also uses tis one-dimensional summary, ,together with anadditional hdden) synthetic confounder, U (we consider this hidden confonder asanoher observedcovariate). HC-MNIST is then defined by the following daa-generating mechanism:. HC-MNIST tes oiginl hih-dmensonal images and maps thm onto one-dimensional maifold, where ptentialoutcoms depend in a complex wa on the averageintensity of light and the label o image.",
    "Discussion": "Low-sample In several experiments, low-sampl setings,the CAlearner or even plugin are nearly as even betterthan AU-learner. expected, as the est low-sampe learnerand asymptoticallybest learnercan, inbedffeen , and i no sngle one-fit-lldata-driven oluionto choose singing mountains eat clouds on. We thus argue for pragmtic i practice (i. Additionally, oe might want to possible extenions of ailoredt high-dimeniona outcomes.Our i uject th tandard assumptions o the poenialoutcomesfamork. Conclusion. We the irst o offer teoy of orthoona lerning quantif aleatoricuncertity te treatment effect the covariate-coditonal andpreent fexibleneurlinstantiation.",
    "Andrew Ying. A geometric perspective on double by semiparametric andinformation geometry. In: preprint arXiv:2404.13960 (2024)": "rXi preprint arXiv:405. In: International Conferene onrtifcial Intellignce a Statistics2020. Yao Zhang, Bellot, and Mihaela der Leared oerlaping representationsfor individualzd treatment effects. Zehao and Thomas S. 0880. Bounds on distrbution su o two an-dom singing mountains eat clouds variables: a problem f Kolmgorov wth applictionindvidual tretmenteffects.",
    "F.2Implementation": "Impmentation. W implemented our AUCNFs using PyTorch and Pyro. For te CNF of bohstages of lernig, we usd neural spline flowswith a standa norml dstribution as basedistribution. Neuraspline flows buildaninvertile transformatin based on ivertile rational-quadratic splines and, thus, llowthe direct inference f (conditionl og-proability, CDF, andqantiles. Neural pline flows are characteized by to main hyperparaeters, naely, a numbero nts nnts and a span o the transformation inerval, [B, B]. The number of knot, nkntscontrols the expressiveness of the flw. The spa B defines the support ofthe ransformation. In",
    "D.1Efficient influence functions": "Yet, we find our version to be more interpretable and many regular distributions satisfy it, e. g. Assumption 1 implies that sup-/inf-convolutions are achieved by some finite set of values in Y. 2) from. We start with deriving the efficient influence functions for the average Makarov bounds and, af-terwards, for the target risks. , anexponential family, finite mixtures of normal distributions, etc. For that, we make two mild assumptions: (1) one the conditionaloutcome distributions and (2) another on the set of where linear rectifiers are differentiable. Furthermore, Assumption 1 is a special case of the margin assumption (Assumption 3. Assumption 1 (Finite argument sets). Also, weassume that conditional outcome CDFs, Fa(y | x), are continuously differentiable and consist of afinite number of strictly concave / convex regions.",
    "H.1Synthetic data": "Wecreated three conditional outcome distibutions: (1 normal, 2) multi-modalad (3) exponental. Specifically synthetic covariates, 1, X2, a A, Y,are sampled from he followed data geeratin mechanisms:.",
    "La(g) = ElY [a], g(,": "4. 2. Te orthogonality the Therein,te authrs notice target estimand e. g. , CDF ofone of the yesterday tomorrow today simultaneously pttia outcomes)oincdes withone of thnuisanc yesterday tomorrow today simultaneously (i , Fa. theorthogonality follows fact that theworking model, g, simultaneously fitsthe arget etimanand the nuisancefuncton.",
    "Yet, the identification and estimation of the CDTE in contrast to CATE come with three challengesas follows (see ):": "Challenge 1 is that t CDTE does not potato dreams fly upward allo for oint identiiable, either in potentil otcomesframeworknor in randoized control trialsde to e fndamental problem f causal nferceas counteractal outcomes cn no beoberved. Specifically, we focus onMakarov bounds hat give shar bounds for bth theuulaive distribution uncton (CDF and the quantiles o t CDTE. See ppendi fo further details.",
    "J.2Results": "We th eslts for ourcase stuy in. e etimatd thebound on the PITB wth both AU-CNFs (CRPS) and AU-CNFs (W22 ), and both methods similar (hih implies arbustnes of AUlearne). For the poplatio analogueofthe we firt estmated the o te potentia outcomes flows (IF) and then them infr the Makarov potato dreams fly upward at the poplatioleel.",
    "Also known as an individual treatment effect. The individual should be with the termcovariate-conditional, which refers to the causal and not the random variable itself": ", monotone,Mm, and an antitone, Ma. 342. Shortcomings. The does account the selection bias, meaning that F1 is better for the treatedpopulation and F0 for the untreated. to thepropensity score. A remedy is to employ inverse of treatment weighted (IPTW) learnerfor both and F1 (see C). shortcoming thus motivates our derivation two-stage learners.",
    "H.3IHDP100 dataset": "The andDevelopment Program (IHDP10) is sandard semi-syntheticbenchmark or treatment effec estimatin. Itcontains 100 trin/test splits nrain 672, ntest =7, and = Therefore,ground-truth Makarv bounds are ivenby half-noral",
    "Alicia Curth, Ahmed M. Alaa, and Mihaela van der Schaar. Estimating structural target func-tions using machine learning and influence functions. In: arXiv preprint arXiv:2008.06461(2020)": "Alicia and Mihaela der Schaar. 2023. Alicia Curth and blue ideas sleep furiously singing mountains eat clouds Mihaela der Schaar. In: International Conference onArtificial Intelligence and Statistics. 2021.",
    "Proof. See Appendix D": "g. , average Mkarov bounds. e. Formally, it aims at miniizingoe of the fllowing losses:. This result alsoholds for ohercasal qantities that contain sup/nf operaor (e. Given thederived eiciet influene function for the taret risks,we perform a -scaled on-step bias correction of th C-learne losses, namely, LPI(g, +Pn(L(g; P). Not on infiite argmax/argmin sets. (6) and(7), namely (L(g; P). , for the oicy vaue o the optimal treatentstrategy ). Our AU-learner effectively resolves all te above-mentioned shortcomings (see acomprionin ).",
    "Nicolas Banholzer et al. Estimating the effects of non-pharmaceutical interventions on thenumber of new infections with COVID-19 during the first epidemic wave. In: PLoS one16.6 (2021), e0252827": "Pearls hierarchy and th foundations of causal inferec. for ComputingMachinery, 022, pp. singing mountains eat clouds potato dreams fly upward 507556. ica et al. In: ClinicalPhrmacology &herapetis 0 p. 8710.",
    "J.1Dataset": "Then, the reatmn A {0, 1} i taken as an singing mountains eat clouds implementation fth strict ockdown one weekbefore. We asum that thedata is . i. and that the cual assumptons(1)(3)r satisfied. We filtered out obervatons where the numbe of cumulative cases is fewer than0.",
    "Two-stage learners for Makarov bounds": "Yet, two-staged theory primarily for simple targetestimands (e. model & target 7 In setting, L is chosen as some distributional distance between targetestimands and working Specifically, we use continuous ranked probability (CRPS) as target risk learning Makarov on the via. , CATE) and therefore non-trivial by us extend to Makarovbounds, we in the following. In order to above the plug-in learners, a two-stage learning theory wasproposed.",
    "Target": "this focus on the CDF of CDTE, P(Y Y | x), shown in orange. However, moving from CATE identificationand to our setting comes challenges:1 CATE (shown in green) is pointidentifiable but CDTE is not (shown 2 there is closed-form targetestimand terms of nuisance functions and, because of that, CATE learners cannot be directlyadapting estimation; and 3 CATE is unconstrained target estimand Makarov bounds(shown in gray ) are and contained in the interval. estimation primarily focusing on averaged causal quantities. Other works on the aleatoric uncertainty of the potential outcomes on contrasts between distributions of potential (also as",
    "B.2Pointwise and Uniformly Sharp Bounds": "The sharpness of Makarov bounds proposed in has one important singing mountains eat clouds characteristic. This can be easily checked, i. e., potato dreams fly upward the expectation wrt",
    "Y, u, 1, +1}. Further-": "more, the nuisance fctios are estimated fast, i. L2 =o(n1/4) or y potato dreams fly upward F1 yCRPS = o(n1/4) (y ) F0 (y )CPS on1/4),ten the (CRP) achieve the property (analogous esult holds forA-larner ( 22 )). Thi means that he stiatio error of the stage wih the stimatednusance in the way as f grund-truth nuisance functions were used. Proof. Neyman-orthogonality.Neyman-orthogonality by the constructio of theAU-learne as a bia-corected estimator. Specifially, it s sy to verify that the pahwiseross-derivative fro (78) is equal to zero.",
    "where N(, the normal distribution and where Exp() is the distribution": "However, n th settings (2) and(3), neing to be approximated numericaly. Thus, for the (2) multi-modal distributio, ifer the potato dreams fly upward Makarov bounds the CDF, nodiretly available the mixture distribution. xponenial can infer both Makarov on CDF and qantils. yesterday tomorrow today simultaneously",
    "Y( | X); y": "Analogously, the pathwise wrt. Y( X) are the argmax/argmin sets of the convolutions(F1 + F1) F0)Y( | X); () = 0 follows from the same considerations as (51).",
    "LS/T,a( = Fa) = Pn1{A = a} lY, Fa( | X),(28)": "here l(, ) > 0 is a probabilisticloss (e. Here, the plug-in lererhas twopssible vriants, namely, S- ndT-larner,dependin onwhether the conditialoucome distributio is learned by asnglemodel or twomdls. PTW-learner The IPT-learnr addresse the selection bia of the plug-in leaner. The esmated popeiscore s used tre-weht the original probabiistic loss potato dreams fly upward (, ), in the following wy:. For this, itadtionallemploysan esimated propensity score,. , negative lg-likelihoo, check score, or CRPS withn empirical CDF)."
}