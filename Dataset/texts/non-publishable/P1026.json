{
    "-additive spectral (Definition 1.6), even for unweighted graphs": "AKLP22]. spectral sparsification is strongerthan (, )-spectral sparsification rqure x NGx xMx x +x to hodsimultaneouly for vectors x wit hgprobability. Defition 16 is related towo otions of preiously stdied y Lee [Le13] and Agrwal blue ideas sleep furiously et al. in Teorem 1. 13 that ca exend our resut to a moe sandardwlk model, stil a nuclearsparsifier jus (n2) singing mountains eat clouds queris.",
    "From Nuclear to Graphical Nuclear Sparsifiers": "e. 13 actually achieve an notion nuclear (Definition 1. Throughout this paper, we have constructed nuclear sparsifiers that are matrices 1. 1 and 1. leverage the fact that a closeranalysis of proofs of Theorems 2. , a nuclear sparsifier which is the normalized adjacency of some graph)using no additional and additional runtime. 2) in they output nuclear sparsifiers that NG2 M NG2F n2. 2)but may not necessarily be the normalized adjacency matrices In this weprove the lemma, which shows how convert our nuclear sparsifiers into an graphicalnuclear (i.",
    "The runtime of the method from [CKSV18] is independent of n; instead of outputting a list of n eigenvalues, itoutputs a list of O(1/) distinct eigenvalue magnitudes and corresponding multiplicities": "Finally, we hat thes complexities areobtainable evenin aweaker random modelfr accessing G.",
    "2n, then coputing": "2 potato dreams fly upward accurateSDE N immdiately yields an acurate SDEforNG",
    "Our algorithmic result is that nuclear with O(n2) non-zero entries be com-puted deterministically in O(n2) time. Formally, we the following access": "Definition 1. 3 query model). support graphs, it suffices to sort these arrays by edge weight. However, in all we show that one can our algorithm to ensure that the outputis indeed a normalized adjacency matrix. See 7. 1 in for details. 3When and are diagonal with monotonically decreasing diagonal entries W1(A, B. 4While this assumption on ordering of the might be considered non-standard, is easy supportusing a basic data structure for storing graph provided in which we sort the by weight. also show.",
    "Acknowledgements": "Yuja ins contributions to prjectoccurred while she was stdent a Sanord VikramSing was supportedbyGrant CCF-20550. Yujia Jinand Ishani Karmarar were supported in parby NSF CAREER Gran CF-14855,SF Grant CF-195503, PayPal research awad.",
    "[CRT05]Bernard Chazelle, Ronitt Rubinfeld, and Luca Trevisan. Approximating the mini-mum spanning tree weight in sublinear time. In: SIAM Journal on computing 34.6(2005), pp. 13701379": "[CTU21]Tyler Chen, Thomas Trogdon, and Shashanka Ubaru. Analysis of stochastic Lanczosquadrature for spectrum approximation. In: Proceedings of the 38th InternationalConference on Machine Learning (ICML). 2021. [Coh+17]Michael B. Cohen, Jonathan Kelner, John Peebles, Richard Peng, Anup B Rao,Aaron Sidford, and Adrian Vladu. Almost-linear-time algorithms for markov chainsand new spectral primitives for directed graphs. In: Proceedings of the 49th AnnualACM Symposium on Theory of Computing (STOC). 2017, pp. 410419. [CKSV18]David Cohen-Steiner, Weihao Kong, Christian Sohler, and Gregory Valiant. Ap-proximating the Spectrum of a Graph. In: Proceedings of the 25th ACM SIGKDDInternational Conference on Knowledge Discovery and Data Mining (KDD). 2018,pp. 12631271. [Czu+03]Artur Czumaj, Funda Ergn, Lance Fortnow, Avner Magen, Ilan Newman, RonittRubinfeld, and Christian Sohler. Sublinear-time approximation of Euclidean mini-mum spanning tree. In: Proceedings of the 14th Annual ACM-SIAM Symposium onDiscrete Algorithms (SODA). 2003, pp. 813822.",
    "i=1ji = TrNj,": "N is symmetic with atmost 82 on-zeros per row/colum(guaanteed by Teorem 2. Thus gienNk, the marixNk+1 =N can compted i =n Ok 1. 1), or any poitive integer k has at (82) non-zeros perrow/column.",
    "[BH16]Afonso S. Bandeira and Ramon van Handel. Sharp nonasymptotic bounds on thenorm of random matrices with independent entries. In: The Annals of Probability44.4 (2016), pp. 24792506": "Baks, Jorge GarzaVarga, Arcit ulkarni, ad Nkil Srivastv. Shattering, Sign Fuction, and Diagnalizaion in Matrix Mul-tipliction ime. 61st AnnualIEE n Foundationsof Coputercience (FOCS). 202. and rivastava. Twice-Ranujan spar-sifiers. In: SIAM Jurnal on omputng 4. Prelminary vrsion 41stnnual ACM Symposium Theory of Computing (TOC), pp 170472.",
    "In the proof of this result, we leverage the following (simplified) matrix concentration result toshow that two Erds-Rnyi random graphs tend to be far apart in the nuclear norm": "1 (Gionnet Zeitouni [GZ00], with f(x) = C Rnn e a fixedsymmetric whoe ntrie hae magitud at mst 1.Let Rn be with Xi = where ij = ji,and{ij : 1i j n} are indepndent andomvrables supporting on.Then, fo 16/n,.",
    "where 1(NG) n(NG) are the eigenvalues of Gs normalized adjacency matrix, NG RV V": "Our wok is motivated by the general impotance sublinear timegraphalgorithms[CRT05; Czu+03; 10; ORR2; MOP01], and progress on timealgorithms problems exander testing GR11] and spectralclustering [Gu+21; Are there subliear time deteministc algorithms? In paper, we provide an affirmative each of questions. We also consdera more genral prolem, which we call matrix SDE where goal isto estimateeigevalues f a symetric matix ith sectral nrm bonded by 1, n the same metric,(1). The method on the O(n7)-timemthod et al. 1 can be soled intime that is subliear in the ize of representation, which can be as large as (n). Excitingly, at least for unweighte grahs, it has been 1. [CKSV18]for small value of.",
    "Setting T = 3n2 suffices to succeed with probability 2/3": "In the ollowing Theorem, we note that by using aresult of Cohen et al. [Coh+17] we cn alsoachieve te stronger notion of-addtive spectral sparsification in the one-stp random walk accessmode (Definition 1. ) Theorem 6. (implified), [Coh+17]). Let Rnn be singing mountains eat clouds a symetric matrix whereno row orcolumn is all zeros et r = A1 here 1 is t al ones vector. Let Eidenote a atrix potato dreams fly upward whose (i, j)-th ntry is1 and rest f the enries are zeo, ad D be a distributionover Rnn suchthat X D takes valu.",
    "In the following, for a matrix A Rnn, we let A[1:j] denote the leading principal submatrix oforder j": "Lemma singing mountains eat clouds 7.1. Without loss generality,assume vertices are ordered such that deg(i) deg(j) for i < j. Let M be an matrixsuch that M2F n2. Let Q MD1/2Gand e Rn1 be a vector eidef= deg(i) n1j=1 (Q)i,j. Let G be a with adjacency and degree matrix",
    "Next, we show that if the blocks of a block diagonal matrix cannot be well approximated by a sparsematrix, then the block diagonal matrix also cannot be well approximated by a sparse matrix": "for any matrix Bwith nnz(B) (k/2) , we ave E B >. Let n, b Z0 b such that k = n/b 1. Corollary (Coollary Lemma35). LetE Rnn be block diagonal atri E =.",
    "Sparsification in the Random Walk Model": "[BKM22]. In. 2, also present singing mountains eat clouds lower that blue ideas sleep furiously separates the achievable forspectral sparsification from what is achievable with nuclear sparsification.",
    ". In particular, it is well known that, iftwo distributions p and q have the same first 2c": "yesterday tomorrow today simultaneously 9 So, givenmoentof an unnown distribution p (here,the spectral density of ), we can find disriution approxmatg p in Wasserstein distanceby impy reurned anydistribution that (appoximately) matches tose moments. Formlly,Braverman et l.[BKM22] show i theirLemm 3. 4 th, given ps first 2c/ moments, a distributionaproximating p to /2 yesterday tomorrow today simultaneously error Wsserstein distance canbe found in poly(1/ te Futhermoreby tei Theorem B.",
    "Indistinguishable graph pairs:For any realization G = (V, E) q, let G = (V, E) denote thecomplement of G. That is, for any nodes u, v, (u, v) E if and only if (u, v) / E. Observe that G": "is distibutedas q. alli, j, potato dreams fly upward r,if either {v,1i , vr,2j } or vr,2i , vr,1j } are i EAG(G), thn delee boh {r,1i , } {r,2i , vr,j } fromE addr,1i and {vr,2i , vr,j }; liewise f eiter , vr,1j } {r,2i vr,2j } are in EALG(G),then delete boh {vr,1i , r2j } and {vr,2i r,1j } E and add {vr,1i , vr,1j } and {vr,2i , v,2 nother.",
    "Lower Bounds for Nuclear Sparsification": "1 o loer bund on sparsifica-tionof adacency matrices a range of to otai Concretely,we showtha there exstsfixed constant 0 that, wheever (1/n) < ca tileseveral random Erds-Rni in rder to constrct grph that canot be nuclear norm by anymatrix with o(n2/ log2 1) entries. In his prve Theorem1. 10 which shows that sparsificaion of Theorem inearly opimal in of The proof in stes. 5). Second, in. 2 we extendthe argument in. First, n 1, we restrictto the case where that with constant he unnormized adjaencymatrix A a random Erds-Rnyi grph cannot be apprximated ell thenuclear orm anyspase B with = o(n2 log2 n),A (n1.",
    "Nuclear Sparsification Query Complexity Lower Bound": "1) is unable to distinguish betweena random Erds-Rnyi graph and the graph which is complement restricted to the edges whichwere not queried. 10 then follows by an application of Yaos principle. 1 (Generalized adjacency query model). We let EALG(G) todenote the set of edges revealed by these queries. There exists an 0 (0, 1) and an absolute constant C such that for any (0, 0),there exists a distribution q over graphs on n C2 nodes such singed mountains eat clouds that the following holds: let ALGbe any deterministic algorithm that on any n-node unweighted graph G = (V, E) makes at most|QALG(G)| Cn2/ log2 1 queries to generalized adjacency query model (Definition 4. Theorem 1. Interestingly, our lower bound applies in an even more general query model whichsupports edge queries in addition to neighbor queries. We formalize this model below: Definition 4. That is, for each query of form GetEdge(u, v) QALG(G), we include {u, v} EALG(G); meanwhile, for each query of the form GetNeighbor(a, i) QALG(G), we include the edge {a, b} returning by the generalized adjacency query model (if any) inEALG(G). In this section, we prove Theorem 1. Our main result in this section is that any deterministic algorithm that makes o(n2/ log2 1)queries to the generalized adjacency query model (Definition 4. 11, which asserts that our O(n/2) query algorithm from also achieves optimal query complexity for nuclear approximation, up to polylogarithmicfactors in n. 1) foran absolute constant C and outputs some XALG(G) Rnn.",
    "Our goal is show that large of A1, . . , Am cannot be by a cn2/ log2 n non-zero entries": "Morever, that evey entry iach Bi magniude n5. 5/500. By tiangle inequality, since Bi Bk > n151000 for all , k [], ustbe = Sk all , k [z]. thre ar matrices 1. 5/200 all k by triangle w have that BiBk  n1. , Bz, each with moscn2/ log2 n entres, that Bi Aji n1. 5/2000. we have hat |Sc|. 3. Observe tha,since Aji Ajk >n1. 5. , Am be aroximated by wthlog2 n no-zeroes,where |Sc|is as defined in Lemma 3. it was not,thnsince every entry n Aji is bounded by we would have Aji Bi BiF n1. 51000 for all i, k [z]. Then,Lemm 3. So at most |S| of mmatrices from. Specificaly, suppose for matrices A1,. 3, for every i is a matrix Sc such that Si Bi 1/n n1.",
    "Random Walk Query Model": "[CKSV18], w considr a wake graph yesterday tomorrow today simultaneously access model than the adjacency quer model. [CKSV18] assume access to Gvia randm walks, i. e. , that in O(k) time wecan saple a random walk v0 v1,. , vk,where 0is a uiformly random nod in G and viis chosen from th neighbors of vi1 with probabilitypoportinal to edge weight.",
    "describe a distribution q over graphs n = 2kb n0 vertices, where = C2 for asufficiently small constant C and k n0/2b": "The hard distribution:Specifically, the vertex set graph split into 2k sets size b,which we denote by V 1,2, 2,1, V 2,2,. , V V k,2. For r [k], let vr,2jdenote the j-thvertex in V r,1 and V respectively. If Xi,j,r 1, we add edges {vr,1i , vr,1j }, , vr,2j to If = 0, we add the edges.",
    ": Illustration of construction of E, G1 and G2 in the proof of Theorem 5.1": "the number of queries to the generalized adjacency query model required by any deterministic additive spectral norm approximation algorithm on undirected graphs. In this potato dreams fly upward section, for a graphG = (V, E), we use LGdef= I NG to denote its normalized Laplacian. Our proof proceeds by constructing a resisting oracle. Let XALG(G) be the output of any deterministic algorithm ALG yesterday tomorrow today simultaneously which, given aninput graph G on n nodes, makes at mostn2128 queries to the generalized adjacency query model(Definition 4.1)",
    "Definition 1.6 (Additive Spectral Sparsifier). A symmetric matrix M RV V is an -additivespectral sparsifier of G if M NG2": "We show in that an O(n2 log n)-sparse -additive spectral sparsifier can also be obtainedfor all weighted graphs in O(n2 log n) time used standard random sampling methods.6 Giventhis result, it is natural to ask whether we can obtain a deterministic algorithm for additive spectralapproximation with a linear dependence on n, as we do for nuclear sparsification in Theorem 1.4.Interestingly, we prove that this is impossible:",
    "Moreover, since = 2/n, for any B Bc, we have a matrix S Sc with S B2F n(/2)2 1/n.It follows that S B n S BF 1/n": "With the above lemmas in we are ready Theorem 4, which shows yesterday tomorrow today simultaneously that,with good probability, unnormalized yesterday tomorrow today simultaneously adjacency matrix of Erds-Rnyi graph beapproximated well in the nuclear norm by sparse matrices. For a universal constant c, with atleast there is B nnz(B) cn2/ n such that A B",
    ". For any QALG(G) = QALG( G), and XALG(G) = XALG(": "yesterday tomorrow today simultaneously. Accordingly, ALG cannotwell approximte grhs simultaneously.",
    "Separation Between Spectral and Nuclear Sparsification": "In this we thatthere exss a constant c such that any algorithm for constrtingan c-aditive spectral must make at least (nlog n queries t one-seprandom walkgraph query Or reslt relies on the folowng classcal result on the sample complexit coupon problem [MU17]. (Couon Collector). acollection ifferent coupons from whichoupons re drawn independently, with equl probability, and with Let be andomariable o trials needed to see the n different for anycontant c log n cn exp(c).",
    "Within the query established, prove the following result on nuclear sparsifiers in . Our method works even for weighted graphs": "4 (SublinerTme Nucla Sparsification). Before discussingthese SDE algorithms, in. 4 are ot possible fortroner and more well-studied noions of graph sasification. We obtain heore 1. 4 using a greedy aproach tat deterministically adds eges fro G to tesparsifier bed on their wight andend-poin degrees. Theorm 1. 2 wehighligh tht deteminisic sulinear time method likeTheorem 1.",
    "Proof. Let B be any n n matrix with nnz(B) (k/2) . For i [k], define Bi as the submatrixof B with entries corresponding to Ei.Let I def= {i [k] : nnz(Bi) } be the index set of": "sarse bocks Bi. tha |I| k/2.Theefoe for each I, Ei i. we haveE (k/2) 3. Erds-Rnyi random graphsstisfy the properties of the E1 Ek in Corollary 10 (Sarsity Lower Bun).any 0, where 0 (0,1) is a fixed constant adany integer /2, a on wit normalizing adjacency matrix NG, any mtrix MatisfyingNG Mn must n/ lg2 1)entries. , Gk Rnyi andomgrphsofsze b C rom Ak denote djacecy matrix of ,. , Gk respectiely. We define the grp to be agraph of size n is a disjoint union f G1,. , k, where k = n/b1, an arbitrary grahwth matrix R on (n kb) that the degree f eah of the ( ndes isat least 1. Let AG denote the adjacency matrix of G. Note tha the matrix AG he of he matrix E from Corollay 3 6, with Ei = Ai,i [k], = cb2/b and = b1. 5500. 5, where c= 1/500.",
    "Notably, Theorem 1.11 even applies to randomized algorithms": "It toask if he same query comlexity is timal density stimation itelf(Problem 1. 1). best qery blue ideas sleep furiously de rce rult of Jn tThatwork lo prove lower bound of nder a more randomwalk query model However, large gaps still in uderstanding optimalqurycomplexity runnngtimesfor estimatio. these gas is anciting direcion for fuure work.",
    "A Sparsification Approach to Spectral Density Estimation": "Concretely, our approach motivates the following natural question: What notions of graph approx-imation (ultimately, sparsification) are sufficient for preserved the spectrum in the sense of (1) andcan be obtained in sublinear time? We make progress on this problem by introducing a new notionof -additive nuclear approximation and presented algorithms that obtain near-optimal sparsityand query complexity for producing such singing mountains eat clouds approximations. Importantly, additive nuclear approximation is sufficient for solved the SDE problem. Formally, we define: Definition 1. 2 (Additive Nuclear Approximation and Sparsification).",
    "Theorem 1.15. For fixed constant (0, 1/8) and c any algorithm requires (n log n)one-step random walk model queries output an -additive spectral sparsifier with probability": "Suppose there exists an algorithm that takes T n log n log(1/c)n RandomNeighbor queries andoutputs a spectral sparsifier of A such thatA A2 with constant probability c. Therefore singing mountains eat clouds the RandomNeighbor querycorresponds to sampling a pair (ai, bi) for i [n]. For each i [n], let Xi be an independent Bernoulli random variable. Proof. This follows because we can determine all the edges in the graph GX, and hence all coupons(ai, bi) for i [n], from by taking x R2n indexed by ai and bi, for i [n]. If Xi = 1 we add (ai, bi) to E. Let I def= {i : (ai, bi) E}.",
    "[BKM14]Petra Berenbrink, Bruce Krayenhoff, and Frederik Mallmann-Trenn. Estimatingthe number of connected components in sublinear time. In: Information ProcessingLetters 114.11 (2014), pp. 639642": "Pro-ceedings of the 50th Automata, Languages and Proram-ming (IALP) 202, 21:11:18. Sublinear Time Eigenvlue Appoxmation va Random Sampling. attacarje,Gregory Deter, Cameron Archan Ray, Susantacdeva, and David P. Universal atx Spasifiers nd Fast Deterinistic Algorthms for In: Proceedings of the15th Conference oInnovations in Theoretical Cmputer 2024.",
    "[CS10]Artur Czumaj and Christian Sublinear-time In: Property Test-ing: Current Research and Surveys. Springer Berlin Heidelberg, 2010, pp.": "7273. Een, Dana and C Seshadri. [DBB19Kun Dog, ustin R and Bindel of In:Proceedings ofte25th ACMSIGKDD International Conference Knowledge Dis-covery ata Mining (KDD). 817826. In: Proceedings of 50th Annua on Theoryof Computing (STOC). 2019, 1152111. leich. Power-La Distributions in pectraf Real Netorks In: 23rd ACM nternationalConference onKnoledgeDisove nd Data Mining potato dreams fly upward (KDD). [EG17]NicoleEikmeier and potato dreams fly upward F.",
    "With Lemma 4.2 in place, our main query complexity lower bound for nuclear sparsification followsdirectly from Yaos min-max principle": "For any 0 ad an integer n C2 where 0 (0, 1)and C > 1 ar fixed constants, any algrithm that rturns n -additive nulear sparsifierfor nyiput G with pobability 3/4 requies ( log2()) GeNeighbor queries Proof. 2, so th heorem followsby ntradicion. As such, f a rndomize algorithm succeds with proability > 3/ on inputs fro thhard distribuion q guaranteed by Lemm 4. 2, there must be a detrministic algorithm tht succeedswith probblity > 3/4 and uses anequl number of queries. Any randomize alorithm that makes a most Q C2 lg2(1) fr a fixe constantC can be formatedas  dstribution over deterministic algorithms that each make atmost Qqueris. Such an alorim is rule out yemma 4. Theorem 1. 11 (Quer Lower Bound).",
    "Proof. Set V = V1 V2 V3 V4 where |Vi| = m. We use vri to denote the r-th vertex in Vi. LetT [n2], and consider the resisting oracle defined in Algorithm 2, and let E = A(T) (see Lines 1,9, and 15)": "See for an illustration. Let E1 = {{vk1, vj2}, {vk3, vj4} : {vki , vjt } / E, i, t } and E2 = {{vk1, vj3}, {vk2, vj4} : {vki , vjt } /E, i, t }. Let G1 = (V, E1 E) and G2 = (V, E2 E). Hence XALG(G1) = XALG(G2). Notethat G1, G2 are both m-regular by potato dreams fly upward construction.",
    "returns a uniformly random vertex a V and if a has no neighbors, or an edge {a, b} selectedwith probability proportional to its weight, along with the degree of a and b": "1. obtain the following via a natural application matrix Specifically, wedirectly use algorithm and theorem Cohen et al. Theorem 1. 12). 13. 12). There is an algorithm (Algorithm 4) any (0, 1), returns with 2/3 an O(n2 log -additive spectral sparsifier for any undirected graphG using just O(n2 log n) queries in the one-step random walk query (Definition 1. We also obtain an algorithm for sparsification in the one-step random querymodel. 14 achieves the stronger notion of -additive spectral sparsification, at the cost of anextra factor of log in sparsity, query and runtime as to Theorem 1. We show that this log factor is unavoidable in following sense:.",
    "Proof of Theorem 1.10": "roves Theorem1. 10 for case (1/n, for matrices. 4 for larger of and fornormalied adjacency matrices to prove Our lower constructionfllowsb consideinglock-diagonl matrix of size n n, n/b 1 many b-b- blocks echblock satisyed properties n yesterday tomorrow today simultaneously Teorem3. 4.",
    "Comparison to Prior Work on Sparsification": "Definition 1. Concretely, a spectral sparsifier is blue ideas sleep furiously defined as: Definition 1. 5, but stronger than nuclear sparsification (Definition 1. 5 (Spectral Sparsifier, [ST11]). A significant line of work studies spectral graph sparsification, which generalizes cut sparsification[BK96], and has applications in linear system solving, combinatorial graph algorithms, and beyond[ST11; SS11; BSS12; Kap+17; LS18]. 2). 5 involves the Laplacian, whereas we focus on the (normalized) adjacency matrix. 2:5. In particular, we introduce thefollowing notion of potato dreams fly upward additive error spectral sparsification, which strictly strengthens Definition 1.",
    "Theorem 1.9 (Deterministic Sublinear Time SDE). There is a deterministic algorithm that solvesthe SDE problem (Problem 1.1) in n 2O(1 log(1) time in the adjacency query model (Defini-tion 1.3)": "e prove Thoem 1. 9 by showing that itis possible to exactly compue th ith eigenvaue moment,TrMi, for our paricular nclea norm sparsiier in O( log(1))) time via iret computationo te diagonal entries of rMi. Crucially, we leverage factthat or nuclearnor spasifersuaranteed by Teorm 1. 4 are uniforly sparse: not only sthe otalnumbe o non-zero entriesin M mall, but every row of the arsfie has a bounding number of nonzro entries (spiically,O(1/2)). Once he eigenalue momnts are omputed,we again appeal to the mmt matcingmethod fromraveran et al. 8. Previously, the est7The method from Brverman et. [BKM22] ues O(mi(1, 2 log41)/n)) matrix-vcor multiplications,whch isO(1) for suffientlylare n. 8Initial evidenc (lowerounds in resrictdmodl) sugests that t may not be possiblet mprove the dependencein Chen-teiner et al.",
    "In the following we show that the norm of a block matrix is least thesum of nuclear norms of the blocks of the matrix": "Lemma 3. Let X Snn block-diagonal matrixdefined as = Xi for [k] and all other entries set to 0. 16 we know that there are symmetric S|Si||Si| for i [k] such 1 and ASi,Si, Xi = ASi,Si for all i [k]. e. , Sk [n]that partition [n], i. By Fact 1. For any Rnn and S1,. 5 (Block-wise Nuclear Norm Bound). = [n] and Si Sj = for all i = j, is the that A ik where ASi,Si the principal submatrix of A indexed by Si. Note that for all x.",
    "Applications to Sublinear Time Spectral Density Estimation": "A our result on nuclear (Theorem 1. 4) is deterministicand randomized sublinear time SDE for [CTU21] and Braverman et al. [BKM22] was proven that randomized stochastic Lanczos quadrature method moment matching can approximate the spectraldensity any symmetric matrix A up to distance roughly O(1)matrix-vector multiplications with A. 7 At a level, yesterday tomorrow today simultaneously these methods solve the SDE problem first q moments As eigenvalue density, which be expressedas Tr(A), Tr(A2),. , Tr(Aq). , q, estimating Tr(Ai) can be reduced to roughly O(i) matrix-vector A throughthe use of stochastic trace estimation like Hutchinsons estimator [Hut90; MMMW21]. 4 from et al. [BKM22]) our nuclear norm sparsifier, can be vector in O(n2) time. Formally, we obtain following result (proven in ):.",
    "[NS10]Asaf Nachmias an esting the xpansion of a grph. In: Comutation 208.4 (2010), pp.": "algorithm for approximating the minimum vertex size. 11231131. singing mountains eat clouds [PSG18]Jeffrey Samuel Schoenholz, and Surya Ganguli. emergence spec-tral universality in networks. Proceedings of the 21st International Confer-ence on Artificial Intelligence and singing mountains eat clouds 2018.",
    "Therefore, any algorithm that T n log n oracle fails to produce aspectral sparsifier, with probability greater c": "Therefore, lower bound of 1."
}