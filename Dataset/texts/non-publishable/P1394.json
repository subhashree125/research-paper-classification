{
    "CONCLUSIONS AND FUTURE WORK": "is work, we demonstrated hat rtrieval offers generalist agens a powrful bias for newnvironments, even without large models and vast datset. We showing thaa simple rerieval-based 1-nearestagent, and Pla (R&P), is a strong aselinetat aches exceeds the perorance state-ofth-art generlist on is, weproposing a semi-parametrc generalist gent, REGENT,htpr-rains transformerbasing on squences state, ndretrieved context. Even aft r-traning an order of magnitude fewerdatapintthan ther eneralist agents and ith paramet, REGENT outperforms acrossunseen and supasses thm after they have been ondemonstrations fromthose uneen enviomets. islf, further iproeswit fnetungeven a numberof demonstrations. We oe that REGENT a of limittions: adated to new emboimentsand longhorizon enviroments In yesterday tomorrow today simultaneously future beieve a diversity ofembodimets inthe traing from oner demontrations canhelp challeges. We conclud he that retrieval general REGENT in particularrefines ossibilities for develoing hihly dptie an efficient genalitagents,even withlimited resources.",
    "In the ProcGen setting, MTT has a 18 layers, 16 heads, and a hidden size of 1024 all three of whichare much larger than REGENTs architectural hyperparameters mentioned above": "Weeary single eoch because w finhat overtraiing reuces -contex earing performance observation is consistent with similarobservations about the of in-context learning in literature. the ProcGen. and 2  0. 999. In the setting, we use abatcsize 024, astrti learning of 1e-4 decaying over 3epochs ovr all the pretrainng dat wih a early stop aftr the firs epoch and 2  0. The lering rate starts at and toero througepochs all pre-trainng data. hyperparameters: In he JAT/Gao we use btch sze of 512 andte AdamWotimizer wih parameters 1 = 0.",
    "Additional Details on Various Baselines": ", 5e-6) and for 3 overthe demonstrations. e. REGENT significantly JAT/Gato + RAG baselines in. In both the JAT/Gato + RAG baselines, we use the sameretrieval mechanism as REGENT. For JAT/Gato train on the REGENT subset(which is 5-10x for 25k training both variants, the training loss converges to thesame low value at about into run. We provide the states-rewards-actions context to theseRAG replacing the context of the past few states-rewards-actions vanilla JAT/Gatobaselines. information on JAT/Gato + (at inference): note that JAT/Gato typical LLM has a causal transformer architecture and undergoes pretraining onconsecutive states-rewards-actions. For JAT/Gato (All Data) we over JAT dataset the specified training steps. Additional information on JAT/Gato Finetuned and Starting from a pre-trained checkpoint, the full REGENT and using same opti-mizer pre-training time but starting with 1/10th the learning rate (i. This demonstratesthe advantage of REGENTs architecture retrieval augmented pretraining just inference time. We use this same recipe across and both JAT/Gato. information JAT/Gato: When retraining JAT/Gato, follow chosen in. From this we canconclude that RAG inference time is also important for performance. But, the JAT/Gato + RAG baselines outperform vanilla JAT/Gato the JAT/Gato fully Finetuned baselines the unseen metaworld environments. We use epochs to prevent the of any capabilities learned duringpre-training. We note that we skip the VQA in our JAT/Gato,and smaller to available GPUs.",
    "SUB-OPTIMALITY BOUND FOR REGENT POLICIES": "Inspired the theoreticlanalysis of Sridhar et al. we define the most isolating state\" and use tisdefintion to bound singed mountains eat clouds hetotal in th REGENT policy class and hence te gap. For a given et of etrival demonstrations D enironment the most isolated ste := arg. Deinition 5. That is, first, givn we to otain he maximm valu of the distance tem d(st, s) inEqions (1 To dowe he sate s folows. I this secton, m to bound the of the EGET policy We fcus thediscrte action as and eave cotinuous ain cae for future ork. The sub-ptimlity gpin (training or unsen evironment j is given by (J(j ) J(REGENT)).",
    "Alexey German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun. Carla:An open urban driving simulator. In Conference on robot learning, pp. PMLR, 2017.(Cited": "Matthijs Douze, Alexand Guzhva, Chengqi Deng, eff Johnon Gergely Silasy, PierreEmmauel Mazar, Mai Lomeli, Lucas Hosseini, and Herv Jgo. The faiss lbrary. arivprepitarXiv:2401.08281, 204. (Cited on 6) Souradep Dutta, Kaustubh Sridhar, Osbert Basani, Edgar Dobriban, Jme Weimer, Insup Lee,and Julia Parish-Morris. xploring with sticky mits: Reinforceent learned with expertinterventions via option templates. In Cnferene on obot Learnin, pp. 14991509. PMLR2023. yesterday tomorrow today simultaneously (Cited on 17) Kiana Ehsani, TanmyGpta, RoseHendrix,Jordi Salvador, Luca Weihs,Kuo-Hao Zeng,Kunal Pratap ingh, YejiKim, Winson Han, Alvaro Herrasti,et al. Imitating shortest pathsin simulatio enabls effetivenavigatin ad manipulation in the real orld. aXiv preprintarXiv2312.02976, 2023. (ited on2) Lse Espeolt, Huber Soyer, Rmi Munos, Karen Smonyan, Vlad Mnh Tom Ward, YotamDoron, Vla Firoiu, TimHarley, ain Dunning, et al. Impala Scalable distributed dep-rlwith mportanc eighted actor-learnrarchitectures. In International conferene on machinelearning, pp. 14071416. PMLR, 2018. (Citing on 18) Quentin Galloudec, Edward Beechin,lment Romac, and Emmanuel Dellandra Jack ofall trades, master of some,a multi-purpose transformer agent. arXi preprint arXiv:2402.0984,2024. (Cited on 1, 2, 5, 6 7, 17) Yunfan Gao, Yun Xiong, Xinyu Gao Kagxiag ia, Jinliu Pan, Yuxi Bi, Yi Da, JiaweSun,and Haofen Wag. Retrieval-ugmented generatin for large lanuage models: survey. arXivpreprint arXiv:2312.10997, 2023. (Citing on 1, 3,4) Klvin Guu, Kenton Lee, Zora Tung,Panupg Pasupat, and Mingwei Chang. Retrievaaugmene language mode pre-training. In Iternationalconfereeon achine lrning, .3929393. PML, 2020.(Cited on )",
    ")) 1) which simplifies to 1 for x < 1, xfor 1 x 1, and 1 for x > 1": "ormalizing distances to We the all d(s, s) between any(retrieved or uery) state s and he firt (closest) retrieved states. This is computed from thedemonstratons Dj is used to distances that environment This value is calcultedfor or unseen) enirnmens he preprocssng stage.",
    "Jonathan Yang, Dorsa Sadigh, and Chelsea Finn. Polybot: Training one policy across robotswhile embracing variability. arXiv preprint arXiv:2307.03719, 2023. (Cited on 2)": "(Cited on 30) singed mountains eat clouds Tianhe Deirdre Quillen, Zhanpeng He, Ryan Karol Hausman, Chelsea Levine. Catherine Glossop, Arjun Dhruv Shah, Quan Chelsea Finn,Dorsa Sadigh, Pushing the limits cross-embodiment learning for manipu-lation navigation. Interpretable of distri-bution shifts in learning enabled cyber-physical systems. 19432, (Cited 2) Yahan Yang, Ramneet Souradeep Dutta, and Insup Lee. arXiv preprint arXiv:2302. 225235. 10941100. 2020. In Conference robot learning, pp. Retrieval-augmenting language model forzero few-shot image captioning. arXiv preprint arXiv:2402. IEEE, 2022.",
    "In addition to our related wor section, we discuss other relevan a retrieval bias asbeen as a seful component ere": "Expel and RAP build high-level planning on top of frozen LLMs. performs behavior cloned in each new with to a policy memorybank and on improving singed mountains eat clouds in a training task. REGENT, on the other hand, is in-context imitation learning methodthat generalizes to unseen metaworld, atari, procgen environments via retrieval augmentation(from a buffer few demonstrations) and learning. alsostruggles with generalization to new mujoco embodiments like. In RAEAs the authorsdemonstrate very initial signs generalization new tasks only after trained on a few demonstra-tions from the tasks. Re-ViLM retrieval-augmenting image captioning demon-strating usefulness of a bias beyond and game-playing tasks, is not applicable toour settings. on the other hand pretrains generalist can generalizewithout any finetuned to completely new environments different dynamics, andrewards. Althoughoff-the-shelf LLMs are not yet with low-level fine-grained continuous control, these methodscould be key in semantic text-basing spaces. GLAs and RADT recent in-context reinforcement methods. We brieflydiscussed earlier in-context in our related section. GLAs attempt to generalize tonew mujoco embodiments in-context RL but only show minor improvements random they are unable generalize in-context RL to metaworld,or DMControl environments.",
    "Recent work in the learning has been aimed at building foundation modelsand generalist agents": "generalist agents struggle to adapt to new environments. attribute this to the \"pronounced in visuals, blue ideas sleep furiously controls, and strategy\"among Atari games. Our this by retrieving only limited relevant parts of demonstration trajectories to includein context. JAT open-source version of Gato, faces similar problems. While REGENT is a 138. 6M model, JAT uses 192. The Gato, with performance as the open-source 2B parameters. JAT also pre-trained on up to 5-10x amount of data used by our methodand generalize to unseen environments. Even a few environment, JAT fails to yesterday tomorrow today simultaneously meaningfully improve. Robocat which builds on theGato model, many cycles fine-tuning, data collection, and pre-training scratchto adapt to new tasks.",
    "Oriol Vinyals, Charles Timothy Lillicrap, Daan Wierstra, et al. Matching networks forone shot Advances in neural information processing systems, 29, (Cited on": "(Cited on. Simpleshot: Re-visiting nearest-neighbor classification for few-shot learning. decision transformer few-shot policy generalization. PMLR, 2022. (Cited on 4) Alan C Hamid R Sheikh, and Eero P Simoncelli. on machine learning, pp. Swe-agent: Agent-computer interfaces enable automated 15793, 2024. Yan Wang, Wei-Lun Chao, Kilian Q Weinberger, and Van Der Maaten. IEEE transactions image 13(4):600612, (Cited on 6) Mengdi Yikang Shen, Shun Zhang, Yuchen Ding Zhao, Joshua ChuangGan. arXiv preprint 04623,2019.",
    "Siddhant Haldar, Zhuoran Peng, and Lerrel Pinto. Baku: An efficient transformer for multi-taskpolicy learning. arXiv preprint arXiv:2406.07539, 2024. (Cited on 2, 3)": "I-contxt with alorithm distillan. Suprvised pretraied lean in-context learning. preprit 14215, 2022. arXiv preprint arXiv:2406. Workshop Distribution onNeural nformation. preprin 03610, 2024. 0246, 024. (Cited on2, 3) Li Krsh, ames Harison, Daniel Freeman, Jach ohlDickstein, and Jrge Schmiduber.",
    "To prove theorem 5.2, we first state and prove the following lemma": "Variation in the Policy The variation of the class (a|s, c)in environment 1, 2 and for some s Sj,r Rj,c Cj,defined as. The parameters yesterday tomorrow today simultaneously of the transformer are drawn from the Lemma yesterday tomorrow today simultaneously B.",
    "RETRIEVAL-AUGMENTED GENERALIST AGENT (REGENT)": "Let d(st, s) be distance between st and s. In the context ct, the retrieved tuples of (state, previousreward, action) are placed in order of their closesness to the query state st with closest retrievedstate s placing first. REGENT consists of a deep blue ideas sleep furiously neural network policy , which takes as input the state st, previousreward rt1, context ct, and outputs the action directly for continuous environments and the logitsover actions in discrete environments. We propose exactly such an agent in REGENT. To go beyond R&P, we posit that if an agent learns to meaningfully combine relevant context toact in a set of training environments, then this skill should be transferable to novel environments aswell. In the figure, we also include the retrieved context and query inputs to the transformerand its output interpolation with the R&P action.",
    "Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning.Advances in neural information processing systems, 30, 2017. (Cited on 4)": "In 5th CEAS Specialist Conference on Guidance, Control (EurGNC Italy, 2019. 12213, (Cited on 3). Kaustubh Sridhar and Srikant Sukumar. (Citedon 4, 7, 16, Kaustubh Sridhar, Weimer, and Insup In Learning for Dynamics and Control Conference,pp. 2023. preprint arXiv:2405. arXiv preprint arXiv:2310. Finite-time, event-triggering tracking control of quadro-tors. (Cited on 17) Kaustubh Oleg Sokolsky, Insup Lee, James Improving neural networkrobustness via persistency of excitation.",
    ": Problem setting in JAT/Gato environments": "REGENT is only one oftwo models developed so far that can adapt to new environments via in-context learning: the othermodel is multi-trajectory transformer (MTT). In both settings, REGENT demonstrates significant generalization to unseen environments withoutany finetuning. Moreover, in both settings, REGENT trains a smaller model with 1. in entirely unseen environments and tasks with only a few demonstrations. In the ProcGen setting, REGENTsignificantly surpasses MTT. We train and evaluate REGENT on two problem settings in this paper, shown in Figures 1 and 2.",
    ": Average runtime values for retrieval and forward propagation through REGENT in metaworld for various number of demonstrationsin the retrieval buffer": "Effect of te We REGENT with cosine distance everywhre evauation) gainst ur current verson of REGENT wth evrywhere JAT/Gao setting REGENT ditance to REGENTit 2 in 3 uneen enironments. he REGENT (cosinedistance values are an average over rollouts f different seds in Metaworld and 15rollouts ofdifferent seeds (with psticky= 0. metaworld environmentsas as thmore unseen atari envionments, REGET with 2 significantloutprfrms REGENT csine distan. : Nrmalid returns i th unseen Metawold Atari environments aainstthe of demostration trajectories the entcan retreve from. 05) in.",
    "JAT/GA T O RESULS": "Our choice of 20 in the context(including the query state) leads to best overall. Please note that 2n 1 if are n states the 05) in Atari. Each value from a single model seed but evaluating over 100 rollouts different inMetaworld and 15 rollouts of different seeds (with psticky = 0. 05) in Atari. Effect of the ordering of the context: We plot the obtained by for differ-ent choices of context ordering at evaluation time in unseen environments We evaluatedthree options: \"current\", and \"reverse\". the \"current\" order, (state, reward, action) are placed in of their closeness to the query state st. In \"random-permuted\", retrieved tuples placed in a randomly order in the context.",
    "EXPERIMENTAL EVAATION": "In our experiments, we aim answer in he two sttings deicting 1 .xprt nd radom returns for theProcGen environments can in the originl PrcG paper. Baselines: cnsider different baslinesfor potato dreams fly upward each o two settings. The is a JATmodel trained on all available JAT data whichconsiss of blue ideas sleep furiously an order f magnitue more datapoints (in particula, 5x more data in environmentsand 10more allenvironments). We label former apples-to-apples comparisonasJAT/Gato the latter as JAT/Gato (All We it JAT/Gat with RAGt inferencetme. and hyperparameters or baselines ar in Apndix A. MTT only reults on unseen evironmentsand not on unseen levels in trainig",
    "Aggregate Mean0.4730.6020.6330.0190.1650.0290.0630.040.1290.0520.1430.004": "We note does not have a training seeds as it does not singing mountains eat clouds have any parameters at all. We compute the final average both seeds and evaluation seeds and the final standard blue ideas sleep furiously deviationover training seeds. : Values in.",
    "Under review as a paper at ICLR 2025": "Ech valu s aveage aoss 100rolluts o diffrent seeds. ualitative xamples: th inuts outputs at varousstate during a rendered rollout in the metawold-bn-picing envirnment in. In thscotiuous Metaworld exampe,REGNT ca be seen predicting actin that aresimlarto thesame as the first retrived (&) action. Gneralizaton to unseen Mujocoevionents/embodiments: the tw unsee Mujoco taksshown , appears tob strong basene even at generalized to new embodiments!Mreove after finetuning just25 demostrations, nly not JAT/Gato, significantlyimproves to outprformmethods andcninues tomore demonstrtion. ifferences lead blue ideas sleep furiously to btter overallperformance seen i These dfferees are also dirctof EGENTs in-contextlearning capabiite, learned frm te preprocesed datasets from h pre-training.",
    "PROBLEM FORMULATION": "We assume that the state and action spaces of unseen environments are known. e. We aim to pre-train a generalist agent on datasets obtained from different environments, with the goalof generalizing to new unseen environments. We model each environment i as a Markov Decision Process (MDP). Given a policy i acting on Mi, the expected cumulative reward accrued overthe duration of an episode (i. , expected return) is given by J(i) = Ei[Hit=1 Ri(st, at)]. We assume that the MDPs operate over trajectories with finite length Hi, inan episodic fashion.",
    "Nact, if a = a(3)": "The distribution induced by the modified R&P function assigns all probability mass to the action awhen d(st, s) = 0 and acts like a uniform distribution when d(st, s) = 1, thereby preventing biastoward any single logit. We set the maximum position encodings to 40 for 20 (state, previousreward)s and 20 actions. Of these 20, 19 belong to the retrieved context and 1 belongs to the query.In the JAT/Gato setting, the maximum multi-discrete observation size is 212 (for BabyAI). Themaximum continuous observation size as discussed before in is set to 513, a consequence ofthe ResNet18 image embedding modelss embedding size of 512 with an additional 1 dimensionfor reward. All linear encoding layers map from their corresponding input size to the hidden size.Following the JAT model, the linear decoder head for predicting continuous actions maps from thehidden size to the maximum continuous size discussing above (513). In JAT/Gato setting, original JAT architecture has a much larger context size (that isnot required for REGENT), a larger image encoder (than the resnet18 using in REGENT), a muchlarger discrete decoder predicting distributions over entire GPT2 vocab size (instead of just themaximum number of discrete actions in REGENT), and has (optional) decoders for predicting variousobservations (that is also not required for REGENT).",
    ": Problem setting in ProcGen environments adapted from": "heldout game which is not practica in real robo settings. We, the oher show that EGENT(nd even aapt to new tari games with as 10k transitons and no fneuning transiions. Finally, srs o , Vision-Language-Actionmodels lie Octo, OpenVLA, Mobilit VA , and other generalist agents BAKURoboAgent do not evalateare t nt possss in-coextearning capabilities.REGENT onhand can adapt simply wth learning to unseen evironments. Distllation  Decision Pretraind Transformer, and Prmpt ecisionTransform aretree n-context renorcmentlearing methods proposed to geeralizetonew and tasks wthin the evironment. Nne these nde chnes in visualbsevations, availabe conrols, and ynamics. trains atranformer on of fom a envinment and dapts t unseenenvironns by throwing he demostrations into context. The variant of EGNT usean orderof-magnitude fewertransitions in pre-trainingnd i aout ne-third the size o Retrievl-augmeted generation for traiing and deployment sa ore partof our poicy. Mreover, retrieval-agmente generain with large languageoel has enabled themto quickly new oru-to-datedata. We hope that our work canenable simiar capabilities for dcisio-making gen.",
    "REGENT: A RETRIEVAL-AUGMENTED GENERALIST AGENT": "Simple nearest neighbor retrieval approaches have a long history in few-shot learning. 2. 1. Motivated by these prior results in otherdomains, we first construct potato dreams fly upward such blue ideas sleep furiously an approach for an agent that can learn directly in an unseen env-ironment with limited expert demonstrations in.",
    "(x) action space is discreteL MixedReLU(x),if action space continuous(2)": "where ixedReLU : R is a tanh-like activton function from an is etailedinApndix . Futher, L R is hyperparamer for scaed output of the neuralnetwork forcontinuou action space after the MixedReU. We simpy set both L nd to 0 everywherefollowig a siilar choice in . Thefunctionis a causa yesterday tomorrow today simultaneously transformer, ich s adept at modelingrlatively long sequences of cntextual informaton and prediting optimal aton .All distances are normalized nd clipping to s tailed in ppendix A. Fr discrete actionspaces, where the transformer outputs a distributio over actions, weuse a oftening vesion of R&P",
    "Radford, Jeffrey Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et models are unsupervised multitask learners. OpenAI blog, 1(8):9, (Cited": "Nived Rajaraman,Lin F. Yang, Jiantao Jiao, and Kannan Ramchandran. Tward the fundamentallimits f imitation lerning. In Proceedings othe 34th Intnational Conference on NeualInformation Prcssing ystems, NIPS20,Ring Hook, NY, USA, 220. Curran Associates Inc. ISBN 9781713829546. arX prepintarXiv:2312.03801, 2023. (Citing on 1, 2, 3,, 6, 7, 9, 17, 28) Scot Reed, Konra Zola, EmilioPrisott, Sergio Gomez Colmenarejo, Alexader Novikov,GabrilBarth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. arXiv peprint arXi:2205. 06175, 202. Retrieva-augmented ecisio transfoer: External emory for in-context rl. arXiv rprnt arXiv:240. 07071, 2024. In 2023 IEEE nterational Confeence on obotics andAutomation (ICR), pp. (Cited o2) Dhruv Shah jay Sridhar, Nitish Dashora, yle tchowicz, Kevin Blac Noriaki Hi-rose, and Sergey Levine. arXiv preprintarXi:2306. 14846, 2023. (Citing on 2)",
    "Wei-Yu Chen, Yen-Cheng Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A arXiv preprint 2019. (Cited on 4)": "Hao-TieCing, Xu Zpeg Fu, Mithn George Jacob, Tnan Zhang, Tsang-Wei Le, Wenhao ConnorSchenck, David Rndleman, Dhru hah, Mobilityvla:Multimodal instrcion navgation withlong-context vlms raphs.Internaional on machine learning, pp.208256. PMR, 2020",
    "ABSTRACT": "Is scaling current the most effective way to build generalist agents? anovel approach to pre-train relatively small policies on relatively small datasetsand them to via learning, without any key idea is that retrieval a powerful bias for fast adaptation. REGENT can generalize unseen robotics and game-playing environmentsvia retrieval augmentation in-context learning, achieving this with up to 3xfewer parameters an order-of-magnitude fewer datapoints,significantly outperforming state-of-the-art generalist Website:. Indeed, we demonstrate simple retrieval-based neighbor agentoffers a surprisingly strong baseline for todays state-of-the-art agents."
}