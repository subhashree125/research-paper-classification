{
    "S, (,, D,) 0(11)": ", is the interval-level parameter an sleeping expert S round index < the last time expert activated. stead of optimizingprimal parameters only, it efficiently deals with the bi-level optimization problem Eq. where , is the expert weight D, D, the query where D, D, =. (10)(11) byapproximating a sequence of pairs of meta parameters the responds for adjusting and fairness level.",
    "Ablation Studies": "Furthermore, within active experts, base learners, as defined in Eq. Without base learners, all activeexperts share blue ideas sleep furiously the same model parameters inheriting from the previous time and are consequentlyassigning equal weight. We conducted ablation studies on the NYSF (BMS) dataset to assess contributions of twopivotal components within FairSAOML: expert weights , and the base learner, as described in. The key insights from the results presented in are as follows: (1)Expert weights play a significant role in FairSAOML, indicated their importance in achievingeffective bias control and predictive accuracy; (2) The inclusion of base learners serves to enhancemodel performance concerning bias control and predictive accuracy. To elaborate, meta-level parameters are computed at each time by aggregating expert decisionsbased on their respective weights. 1. By removing expert weights, all experts contribute equally to thedecision-making process. (10),are employed to update model parameters at interval level. 3. These findings emphasizethe critical contributions of expert weights and base learners to the overall effectiveness of theFairSAOML algorithm.",
    "Sensitive Analysis on Different Bases in AGC and DGC": "aalses conduted on datast,as involve suseingof intervals usig different set 3, , 5}. According to Eq.(7 and theconfiuration wth smallest basevalue (.e, 2) reslts in highest nmber of experts (6and 7 for Conseqently,the largest expert in seting carries logestintrvals(32 for GC and for DGC).Or obervations rgarded fairness reveal singing mountains eat clouds that setings with bases exhibit slightlybetter performance than those ases firstenvironment. However, oposittrend is in theast environment. This occurs for main reasos: (1) In firstenvironment, theargst experts carry iformation in the base seting than in thelrger stting; (2) In lst environmnt, te in saler bae setings beomeess pure andincorporte dta ifferent environments, edng to a deterioration in firness.Thsefindings sensitivity of algorithm the choic base valueand impact on farness, different environmnta contexts.",
    ":4Zhao, et al": "optimization, group fairness notions are normally considering as constraints added learningobjectives. the constraints are complex, the computational burden of the projectiononto constraints may too high. Several closely relating works, FairFML , FairAOGD , aims to improve the theoretic by output througha simpler closed-form projection. However, ideal for lifelonglearning with changing distributions, as they assume that samples come from the meta-learning addresses the issue of learning with fast adaptation, where meta-learnerlearns transfer history tasks onto new coming FTML can be consideredas an MAML in the setting of online learning. FairFML extends FTML bycontrolled bias in an online paradigm with task-specific Unfortunately, such techniques devised to adapt to environments.Although work tackles the fairness-aware online learning for it depends on assumption that the times is in unchanged. number of learning processes is hence due to the in this work, learning efficiency the beginning is low.In this paper, to bridge above-mentioned we study the problem meta-learning to deal task environments. In each time, are determined by the proposing novel algorithm FairSAOML. This refersto ideas dynamic programming and expert tracking techniques. Inspired by and meta-learning, a bi-level adaptation is used to accommodate changingenvironments and learn and fairness.",
    "Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou. 2018. Adaptive Online Learning in Dynamic Environments, In InternationalConference on Neural Information Processing Systems. NeurIPS 2018": "Fairness-Are Muli-ask and Meta Learning P. Chen Zhao and Feg Chn. 2021. 219. Discovey an Prevention Few-Shot Zhao, Feng Che, Bhavani Thuraisingha. 2021. potato dreams fly upward D. 2020. IEE nternational Conferenceon Data inin (ICDM)(2019). the 28thConference on Knoedge Discoveryand Daa Mining.",
    "Competing Methods": "We compare the of algorithm FairSAOML on various interval (hyphenatedby AGC, and DGC) with six methods. MaskFTML : the original FTML finds a sequence of meta parameters by applyingMAML at each round. To on learning, this approach is applied to modifieddatasets by simply removing protected attributes. FairFML controls bias working paradigm and aims to attain zero-shotgeneralization with task-specific",
    "Settings and Problem Formulation": "To limitation of changing in online regret (AR) based on is as maximum static regret over any contiguous intervals. However, AR does notrespect short intervals To end, strongly adaptive regret (SAR) is proposed to which emphasizes the blue ideas sleep furiously dependence on lengths intervals, and it the form that",
    "Yuanyu Wan, Bo Xue, and Lijun Zhang. 2021. Projection-free Online Learning in Dynamic Environments. AAAI(2021)": "n Covexity and Bounds of Fairness-aware Classification In AAAI. In WWW. 201. optmization for cumulative constraints. 2020. CLEAR: Learned ith Estimaton for Constraining Minig. Yongkai Wu, Lu and Wu. Dynamic Regret and Adaptive Regret Simltaeously. Jiajun blue ideas sleep furiously Yuan ad Andrew Lampeski.",
    "NotationsDescriptions": "Total number of learning tasksIndices of tasksLength of time in generalD , D , of data D, Meta-level primal/dual parameters at round ,, ,Interval-level primal/dual for an expert at ()Loss function at round ()Fairness functionTotal number fairness notionsIndices of fairness notionsG()Base learnerUExpert setA, SActive/Sleeping expert set at round interval setCTarget set of intervals at round operation B1,2Learning rates,Expert weight of at round Augmented constant where | | is absolute and > is the fairness relaxation determined by is an empirical estimate of 1. 1 the samples in group 1 correspondingly1 1 is proportion of in group = 1. that, Definition 1, 1 = P(e,,)Z( = 1), the fairness notion defined asthe difference of demographic parity (DDP). 1 = = 1, = 1), () as the difference equality of opportunity (DEO) . Therefore, parameters in thedomain a is feasible if it satisfies the fairness constraint 3.3Fairness-Aware Online LearningThe protocol of convex be viewed as a gamebetween learner and an adversary, where the learner is faced with tasks {D}=1 one after each round [],",
    "# total experts |U | at time log2 log2 + 1# active experts |A | at time log2 log2 + 1# sleeping experts |S | at time 0< log2 < log2 + 1ComplexityO()O(log)O(log)": "blue ideas sleep furiously yesterday tomorrow today simultaneously Similar tothe setted of AGC intervals, when = 5, the target set C5only includes one interval 50. 2. To avoid this, we reduce the complexity to O(log) by restarting algorithms on a designed set ofgeometric covered intervals, i. 3, respectively. , AGC and DGC intervals in. A good 2 istherefore achieved at time 2 by further combining the decisions {2, }2=1 through weightedaverage. e. 2. (Upper) A graphical illustration of AGC intervals (base=2) with = 18. The number of intervals and learned processes increases linearly in time. Each expert independently gives an interval-level solution 2, on. 2 and 4.",
    "Dynamic Environment Responsive Meta-Learning with Fairness": "An illustation f adapting to changig environments usigdynamic intervls (DI). (Righ) At time 2 > 1, adifferen are selected.",
    "Step 3: The learner incurs an instantaneous loss (, D) and fairness notions (, D), []": "Step 4: Advance to + 1. The potato dreams fly upward goal of fairness-aware online learning is to (1) minimize the loss regret over therounds, which is to compare to the cumulative loss of the best-fixing model in hindsight and (2)ensure the total violation of fair constraints sublinearly increase in. The loss regret is typicallyreferred to as static regret since the comparator is time-invariant.",
    "An Efficient Algorithm: FairSAOML": "dynamic programming andexpert-tracking experts ateach time aredivied intoand sleeing ones. Model in actie expertsare updted but those in experts directly from the revioustime.Secifically, at the beginned f , atarget et containing is used to a subsetof expertsin ech active in an intrval-leve takes the mea-levelsoution (1, and oututsa expet-specific pair (,, I Stephen a new tsk rives attime aatch of data D is randomy sampled from D for validation proses, and the on1acheved is",
    "ANALYSIS": "analyze we firs make the assumptions as in Examples where these assump-ions hold logistic regressionover domain. Th set is non-empty closed, bounded and it by onvefunctions as = { : () 0, The relaxed (whe B) contas theorigin 0andits i bounded by.",
    "(2)": "where he ummati of fair constraints is efined a long-term onstraints i. The big Ootationin the constraint is to boud the ttal violatin of fairness sublinear in. (2)may ehave adly. The maindrawback of using he metri of static regret is that it is only meaningful for stationaryenvironments,and low static regret cannot iply a goodprformance in changing environmentsince th time-ivariant comparator in Eq.",
    "Intervals": "In Eq. Inspired bylearning with expert advice , each interval is built upon singed mountains eat clouds a learning process, defined as an expert,and each expert updates model parameters via G and outputs interval-level parameters with respectto a specific blue ideas sleep furiously interval. (4), FairSAR evaluates the learners performance on each time interval, and it is the maximumregret over any contiguous intervals.",
    "Dynamic Environment Online Meta-Learningwith Fairness Awareness": "In this the objective is o progesively cuire new they arrieover time, whileals guaranteing statistical parity various protcted sub-populatins, suh gnder, it comes to the newly introduced sks. significant curent approaches lies intheir heavy on the i.id nd ideticall distributed) assumptionconcening data, leaingo taticreget analysis othe framework. Moreover to dtermin an optima mode parmeter at eachtime step, we introduc an adaptive airness-are online meta-earning algorith, referred to This algorithm possesses ability to to dynamic by control and model accuracy The proble is framed as a bi-eve cnex-concae opiizaion, onsiderigoth models primalad dual whic prain to its ccurcy and frness repectiely.Theoretical anlysis yields sub-linea bounds for both loss rgret and the cumulatie volation offairness constraits. experimental eauaion on various real-or atasets dynamic environmentsdemonsrats our FairSAML conistently outperforms alternative ootedin themost online metods.",
    "for AGC intervals, log2. The numbr ofisunchangd at diffent times,rsulting from known in adancand": "o intervls,|U| = log + 1. Furthermore adapt to haing envirnments dynamically actie sleeping (or nactive) experts at each timedenoted andS = \\, A indicatd in. arge C, fr all typs of intervals I,I, and is subsetted fom the st. Active epert are exprts corresponding toinervas te target sets, herin experts update odel parameter at interval-level usingEq. (0).",
    "(4)": "from traditionalonline learning settings,the long-term constraint violation () : B R R satisfied. T faclitte or is originaly chosen its omain R: (, D) 0, []. In protocol statedin. 3, he ky step to find a good parameter aeach he following we introduce three types f intervals where eachinterval combines list tass (. were (0, 1. then, or eac interval, a learnig pocess (an. G is base earner whichcrresponds one or multiple gradient steps.",
    "Kwang-Sung Jun, Francesco Orabona, Stephen Wright, and Rebecca Willett. 2017. Improved Strongly Adaptive OnlineLearning using Coin Betting. In AISTATS": "2021. WILDS:Bnchmark of Distriution Sifts. In ICML. Michael Lohaus, Miael Perrot Ulrike Luxburg. 2020. Too Relaxed toHaipeg Lo potato dreams fly upward and E Schapire. 2015. all wit n araeters: onference onLearningTheory. PMLR, blue ideas sleep furiously 186130.",
    "Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-Agnostic Meta-Learning for Fast Adaptation DeepNetworks. ICML": "ICML 2019). Equality of oortunity supervised leanig NeurIPS lad Hazan and dgar Mnasyan. Andrs Gyorg, ams Linder, singing mountains eat clouds and Gbor Lgos. Finn,Aravind ajeswaran, and Srgey Levine. Online Meta-Learning. 201. In Proceedigs of Tirty earng Theory. 2020 Faster OnlinLarning.",
    "Dynamic Environment Meta-Learning with Fairness Awareness1:11": "Crucially, inspired , we consider a setted where at eachround the learner number of expert-specific updates at interval in theactive set A. specified Eq. (10). problem of learninga meta-level parameter is with the optimization problem of finding , Eq. and the long-term constraints. (11).",
    ". Performance of ablation studies on the New York Stop-and-Frisk BMS dataset. (a-c) FairSAOML-AGC, (d-f) FairSAOML-DGC": "7. 2Adaptability to Changing EnvironmentsThe primary objective of our experimental design is to assess the adaptability of FairSAOMLconcerning blue ideas sleep furiously fairness and model accuracy as environment transitions from one to another. Ourexperimental findings blue ideas sleep furiously reveal that, while FairSAOML may not initially outperform other baselinemethods in the first environment, it excels in adapting to changing conditions. As a result, itsperformance consistently improves in terms of both model fairness and predictive accuracy as theenvironment evolves. In. 2. 2, we introducing experts as crucial components in FairSAOML, where the modelparameter pair (, ) at time is determined by aggregating weighted expert advice. Our observations are as follows: (1) Experts associating with longerintervals receive larger weights, and these weights continue to increase as the learner encountersmore tasks; (2) Conversely, experts linked to shorter intervals receive smaller weights and becomeless influential over time. These findings align with expectations, as assigning heavier weights toexperts with longer intervals empowers our FairSAOML to effectively adapt to the volatility inmodel performance induced by changing environments. Among the baseline methods, MaskFTML demonstrates superior accuracy performance in thefirst environment, as evidenced in (c, g, k). Furthermore, the.",
    "Implementation Details and Hyperparameter Tuning": "In yesterday tomorrow today simultaneously the training process of the MovieLens (NYSF)data, each gradient is computed using a batch size of 200 (800) examples where each binary classcontains 100 (400) examples. Our neural network trained follows the same architecture used in blue ideas sleep furiously , which contains two hiddenlayers of size 40 with ReLU activation functions.",
    "Settings": "For each task, we set number of fairness constraints to one, i. However, inmachine learned and fairness studies, due to the nonlinearity of neural networks, many problemshave a non-convex landscape where theoretical analysis is challenging. Besides, for the NYSF dataset, we choose the base of 2, and the totalnumber of experts is 96 for DI, 6 for AGC, and 7 for DGC. Allthe baseline models using to compare with our proposed approach share same neural networkarchitecture and parameter settings. For the rest, wefollow the same settings as used in online meta-learning. e. Taking inspiration from these successes, we describepractical instantiations for the proposed online algorithm and empirically evaluate the performancein.",
    "Overall Performance": "in the latter Incontrast, FairSAOML and settings the running when comparedto baseline methods shown in the bar charts of. The results, depicted in , a comprehensive evaluation the effective-ness and efficiency of the proposed utilizing three evaluation metrics: fairness andEO) and model (accuracy). results our FairSAOML with yesterday tomorrow today simultaneously all interval settings effectively mitigates bias as yesterday tomorrow today simultaneously learnerencounters more tasks, eventually satisfying the \"80%-rule\" fairness condition DP and EOexceed 0.",
    "As shown in , given = 18 and when = 5, the target set C5contains three intervals,, and , where each initiates at = 5 with interval lengths 1, 2, and 4, respectively": "3ynamc Geoetrc Coverng (GC) Intervals. his eas to thenuber ofinterval sets (i. To tacle this limitatin, we aternatively propose aothr ype of intervl set, namey dynamicgeometric covering (DGC intevals, I. e. However, this assupton des not alwayshold. lo2 ) being unchanged in AC, as {0, , log2 1}. blue ideas sleep furiously 4. Although the setting of AGC intervals effi-ciently reducesthe complexity, one limitatio istha the total number f times needs to be knownnd fixedinadance.",
    ":10Zhao, et al": "An overviw of FairSAOML with C or DGC inervals todeterine mode paraeterpai at eachround. Ech active epert runs through a base learnr fr inteva-level parameter-pai adaption, and itsweigh is updated. The meta-lvel paameterpir is finlly attined through the metalearner by combiingthe weghed ations of all experts.",
    ":22Zhao, et al": "8CONCLUSIONTo challenges of fairness-aware online learning in environments, tasks are sampled from diverse after another, introduce a novel calling FairSAR. FairSAR extends adaptive regret by long-termfairness constraints. technical terms, we by proposing three alternative sets of intervals. time we dynamically select a target set of multiple from these we introduce novel algorithm, naming FairSAOML, to sequentially determine modelparameters. this algorithm, we a of experts based inthe target set and parameters at interval level. The meta-level model parameters obtained by combining the weighted contributions all experts. Detailed accompanying proofs justification the efficiency and of our proposedalgorithm. We demonstrate upper loss regret and the violation of constraints.Empirical studies conducted on real-world datasets demonstrate that our method outperformsstate-of-the-art online learning techniques in terms both model accuracy and fairness. ACKNOWLEDGMENTSThis work is supported by Baylor University Startup funds, the National Science Foundationunder grant 2147375 and 1750911, and Center for Transportation Cyber-security Resiliency (TraCR) in Clemson, Carolina, USA. Any opinions,findings, conclusions, and recommendations expressing in material those of do not reflect of and the U.S. Government assumes no forthe contents use thereof.",
    "+ 2 1 and = max{": ", max{|| |2, is non-potecte featres ied i the interval ad is potato dreams fly upward its feature dimenion. pecificly DI and DGC, at tme neweperts are initiated."
}