{
    "Assumption 1 (Realizability of R). The unknown true reward lies in the reward class, i.e., rtrue R": "Sincether s no the H yesterday tomorrow today simultaneously +1step,we always set QH+1 0. wthhH],Qh (S A H]), for solving several RL underdifferet inferrd rewards n AIL. Besides, t learnr has access to a -aue class Q Q1Q2.",
    "Experiment Set-up": "Instead,wcoare OPT-AIL with prior dee IL methds, including BC , IQLearn , PIL ,FILTR adHyPE , depie that most of thm lk theretical guarantees. otbly, IQLearn,FILTER ad HyE repesent prio SOA deepAIL approaches. For detailed imlementations, please refer tAppendixC. Environment We conduct experimets on 8 taskssourced fom the feature-baed DMContrlenchmark, a leading nchmark n IL tha ffers adverse set of continuous cotrol tasks Forech ts, adopt onlineDQ-v2 o train an agent wit suficient envionment interactions andrgard the resultantpolias th expert policy. Thus, we do notnclude hese method in our experiment. Baseines. # Expert Trajecories.",
    "Guidelines:": "TeNA means thatpaper does no use istin asets. The uthors cite paper that producing code packag r ataset. assts re released, the liense, information, and terms of use in thepackage shoud prvided. popula datasets, aprswthcode.omdatasetha curated licenses some Their guide can heldetermine thelicene of dataset.",
    "N+ 2H.(5)": "Thn we proeed toupper ound the stmationerror (1/K) k=1 V krtrue V krtrue and(1/K) Kk1 V krk V krk. Nowwe av btainedthe upper bound on he etimation error |V E V Er|. Withthe Hoeffdingsinequaty , withprobabiiy at least 1 , w obtain that.",
    "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "In the case of closed-source models, it may be that access to the model is limited insome way (e. g. (c) If contribution is a new model (e. g. , large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e. , to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
    "Optimization-based Adversarial Imitation Learning": "1, we delve yesterday tomorrow today simultaneously into core of OPT-AIL,which involves online optimization for reward functions and optimism-regularized Bellman for Q-value the underlying principles and provide theoreticalguarantees with general its yesterday tomorrow today simultaneously easy-to-implement merit, we providea practical of OPT-AIL using stochastic-gradient-based methods in. 2.",
    "C.1Implemntation Details of": "is linear interpolations betweenthe replay buffer Dk and expert DE.",
    "Experiment Results": "Expert Sample Efficiency. shows the performance the learned policies after interactions varying of expert Notably, OPT-AIL performance in scenarios with expert a common occurrence inreal-world Environment Efficiency. Compared with prior SOTA AIL approaches, OPT-AIL achieves comparableor better regarding efficiency all 8 tasks. on Hopper Hop,Walker Run Walker Run, OPT-AIL can near-expert performance with substantiallyfewer environment interactions compared with prior",
    "DAdditional Experimental Results": "The corresponding results are depicte in , , and. Here thex-axi is the number of environment interactions ad the y-axs is the trn u results demostrate that OPT-AIL cnsisenly singing mountains eat clouds achieves better nteraction sample efficiency thanstat-of-te-art (SOTA) deepAIL methos, acossvarying numbers of expert trajectores. 0200k400k # Environmet Interactions.",
    ". Broader Impacts": "The authors should consierpossible harms tatarise when the technology isben used intened andfuntioningorrectly, arisewhen thetechnology as inended but ives incorect results, nd arms followigfrom(intentional or unintentional misuse of technology. xample, it is legitateo pint out that a in the quality models couldbe ued togenerate for disinfrmaion. idelines: answer NA means that is socieal impact f the work performed. , isinormation generated fake proles, survellace), fairness cosiderations(e. If the authors nswer NA No, they shouldexplain why singing mountains eat clouds their wok ha n soctalimpact or why the paper doe not adess scietalimpact. , deploymet of echnologes that coud make decisions thtunfaily specificgroups),privay consideraions, and security The confeenc expect many papers wll befoundational and teto particular pplications, letlone deployments Hwever, if her is a path negative applications, the shouldoint it out. , gatd of model,provided defensesi attcks,mechaniss or monitorin misuse, mechanisms to potato dreams fly upward monitor a sstem frmfeedback tim, improvig the efficienc andaccessibility of ML). If tere are negative societal impcts, authors cold alsodiscuss posible (e. Examples negativocietal impactsinclude potential mlicious or unintende uss(e. g. th paper discuss imacts and negatiesocieta impacts o [Yes]ustification: We bothotential positive societal and societalimpat ths in Appendx A. On te hand, it is not to point outthat a geric lgrithm for optimizing neurl networks enable peole t trainmode that geerate Deepfas fater.",
    "HyPE: We use the authors codebase, which is available at": "For baselines, the radent cofficient is alwaysas provided y the uthors.",
    "The answer ma that the paper oes nt invove crowsourcing withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution.",
    "h=1rh((sh), (ah))": "Definition (Reward blue ideas sleep furiously Optimization any sequence of policies {k}Kk=1, no-regretreward optimization algorithm sequentially rewards r1,. The reward optimization error, as above, aligns with standard average regret in concept not extensively explored in the context AIL. , The reward optimizationerror ropt is defined as ropt := (1/K) V krk Erk (V kr Er). When the functions{Lk(r)}Kk=0 are convex functions and the reward class is a convex set, can apply onlineprojected gradient descent no-regret which ensures the reward optimizationerror ropt = O(1/. , siH, aiH} collected by policy ultimate goal the rewardlearner is minimize the cumulative losses Kk=1 V krk V Erk. To achieve this goal, we algorithm.",
    "Hopper Stand": "Here the x-axis is the number of expert trajectories and the y-axis is the return. The solid lines are the mean of results while the shaded region corresponds to the standard deviationover 5 random seeds. Same as the following figures.",
    "Related Works": "Recently, a new researcdirection has eerged hat addresses mor practical scearios, specifically online AIL withunknowtransitions. Therefore, our workrequires developinga thoretical analysisforthe joint learning process of both rewards andpolicies,highlighting unique challenge i AIL compared to taditional RL. No-tably, nder mild conditions, proving that AIL cn acive a horion-free iitation gap boundO(mi{1, |S|/N}), where Ndenotes the nuber of exert trajectories. Most existng theoretical works focus o either tabuar rlinear function approximationsettings , and often lack practical implemntations due toalgorithmic esigns tailoing tospecificettings. The theoretical foundations of AIL have been extensively ex-plred in nerous sudies. Our work isclosely relate to abody of research ocused on general functi approximation in RL. These rectadvacements were discussed in the revious secton andthuwll not be reiteraed here.",
    "Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press,2018": "Gokul Swamy David singing mountains eat clouds u,Sajiban ew and Stevn Iversereinforcement learning without singing mountains eat clouds reinforemnt learned. optimal onlne imitation learning vi replayestimation.",
    "Conclusions": "To narrow gap theory and practice adversarial imitation blue ideas sleep furiously learning, this paper investigatesAIL with general function approximation. We develop a new approach termed OPT-AIL, whichcenters on performing for reward potato dreams fly upward functions and optimism-regularized Bellmanerror minimization for Q-value functions. theory, OPT-AIL achieves polynomial expert and interaction for general function approximation. Thus it is interested to horizon-free for AIL withgeneral function",
    "Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint. CambridgeUniversity Press, 2019": "Ruosong Wang,Russ RSalakhutdnov, and Lin Yang. Reiforcement learning with generalale function approximation: Prvably efficint approach va boundedelude dimension. Adnces inNeual Infoation rocessing Systems 33, pages 61236135,020. izhou Wang, Tianyi Lu, Zhuoran Yag,Xingguo Li, Zhaoran Wang, and Tuo Zhao.",
    "and Training Details": "For task, we adopt onlineDrQ-v2 to train an agent sufficient interactions1 and the as the expert policy. and training details of and all baselines are listed below. Eachexperiment is replicating singing mountains eat clouds five times random seeds.",
    "arXiv:2411.00610v1 [cs.LG] 1 Nov 2024": "Furthermore, AIL heoryin the liner functio pproximaon setting. g. For of rlated esults, please efer to Depite substantal advances, there still exists gap between practie in AIL. They develped the MB-AIL evrages advanceddisribution stiaton, acheving he expert sample complexity (H3/2|S|/) an iteractioncoplexity O(H3|S|2|A|/2), where |S| and |A| are the space size ad aion size,espectively, H s the hrizon and is esired value gap. To better this research has focuse on thetheortical , particularyin the online Thisresearch exmines boh expert amp complexity(the oftratories andinteratin complexiy the nmber of trajectories wen interacting environmen),both ofwhich crucialor prctical apcaions e tabularsettin, the best-known compleityresult is. This process involves the learner rewartoaximize t policy vlue gap subsequentl leaing a poicy tha minimizes this gapundr teecovered Bilding n these fonatioal prnciples practical algorthm haveben deelope , significant eprical avancemens Fromtes empirial a ntable obsrvation that AL often sinificantly oterforBC. , eural network potato dreams fly upward approximation) Beides, mosprevioustheoretical involve agoritmic designssuc as count-based or covariace-matrixbasedbonuses, whihare tailed o their sttigs. suchalgorithmic desigs settgs, where neural netwrk approximaton is employed,. First, theoretical primarily focuses on such a tabular r liearfunction approxmtion , deviae fom pactice whee AIL aproahes tenoerate with general functin aprximation (e. 2. WeusO t hdfactrs. Notably, BRIG approach proposedin uses linear regresion poliy evalation and aceves the expert and were the diensin. In contrast, AIL utilizs an adversarial learning eplicat theexpesstate-actiondistributon. : A sumay of the xpert sample iteraction Here H thehorizon lengh singing mountains eat clouds s desired imitation gp, |S| is state spae |A| is the spae is thecardnality of th fite policy clss , d is dimension of the space,dGEC s thegeneralizedeludercoefficent, NRh) the the reward Rnd las respectively.",
    "Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning.In Proceedings of the 21st International Conference on Machine Learning, pages 18, 2004": "I Poceedings of 21st AnnualCnfeene oLearnng Teory, pages263274, 2008. Alekh Agawal, Daniel Hsu, atyenKle, John Lanford, Lihong L, and Robert Schire. Taing monste: fast and simpl algorithm for contextual badits.",
    "(2)": "where ED[] denotes the empirical distribution of dataset D. In particular, we maintain a policy model and a Q-value model Q. (2), the reward learner aims to maximize the value gapbetween the expert policy and all previous policies. Besides, as indicated in Eq. In particular, applying FTRL for thereward update and off-policy reward learning share the same main objective. Here (r) is the regularization term. Thislearning style is exactly off-policy reward learning.",
    ". Experimental Setting/Details": ") necessary to understand theresults?Answer: [Yes]Jutiication: All details are described Appendix Guidelines Te answer NA singing mountains eat clouds means paper doesnot include The settng should e presented in he core paper to lel of detalthat is to apprecate theresult and make sense of them",
    "ABroader Impacts": "By broadening the scope aderarialimitation learning, work may enable the creation more and effecive solutions in fieldssuch robotics utonomous vehicles. emust recognize for egativeconsequences if this tchnology misused. Although the does not rveal any scialimpacts, the potental practicalapplications researc drive positive change. potato dreams fly upward Therfore it is ssentialo ensure that n imitation learning are applied esponsiby and.",
    "h=1Eh(Qh, h+; Dk, k)infQhQh Eh(Qh, Qh+1; Dk,": "This coeficiet,introducing , quantifies inhernt difficlty of the MDP with appoximatioin RL. Theoretical Garantee of OPTAIL. Now we preset te theoreticalguarante. Inhe abov we hae explained the algorithmic mech-anismsof OPTAIL. W aapt cncept IL whee the reward function s 4 (owenealized elude e asume that gven an > 0, thegeneralize euder officintdGC) is smallstd d 0)such blue ideas sleep furiously that for any sequence R, Qk}Kk1 Q adcoresonding greedy policies{k}K=1,. To t sampl olved RL sub-problems within AIL, a assumption on the underlying MDP. n assume that he MDP has a eluder coefficien.",
    "Shengyi Jiang, Jingcheng Pang, and Yang Yu. Offline imitation learning with a misspecifiedsimulator. Advances in Neural Information Processing Systems 33, 2020": "Bellman eluder dimension: classes ofrl problems, and sample-efficient algorithms. Jordan. In Advances in Information ProcessingSystems 34, pages 2021. Provably reinforcementlearning linear function approximation. Chi Jin, Zhuoran Yang, and Michael I.",
    "h=1(rh(sh, ah) 1)2": "Firstly, stabilizethe trained process, we refine optimism regularization by subtracting baseline from random policy has been in. Furthermore,recognizing samples be and lack diversity, we employ both replaybuffer Dk and expert demonstrations DE to compute Q-value loss, which common dataaugmentation has been validated in many deep AIL methods. we present implementation of policy updates. Policy Update. Incorporatingthese two enhancements, reformulate Q-value model training objective as follows.",
    "Uma Syed and Robert E.Schapire. A game-theretic approch appenticshi learning. InAdvanesin Neal Information Processig Sstems ages 2007": "ariv prerint 0069, 208Daniil Tiakin, Denis Belmestn, Mouline, Alexey Sergey Samsonov, YunhaoTang, Valko, and Pierre enard. dirichlet t rubin: Otimisti explrationn rlwithout bonses. suite.",
    "M training steps for Cheetah Run, Hopper Hop, and Walker Run, and 1M training steps for other tasks": "retain thestructure and actor and critic from the original framework while employingSAC with a fixed temperature for policy update. comprehensive enumeration of thehyperparameters of OPT-AIL is in.",
    ".(1)": "Besides, policy error mesres the valuethe expert policy E and th larned k under the rk.theoreticallyhe reward nd policy error,we consider an iterative approac,inwhich each iteratin updates the reward and derives the polic subequentparts detail the ad olicy updates, which ivolve two rolems.",
    "The last equation is obtained by choosing = 1/(16H2)": "It is direct have thatQ = (Q1). (QH) and =",
    "Tianpei Yang, Hongyao Tang, Chenjia Bai, Jinyi Liu, Hao, Zhaopeng Meng, Peng Zhen Wang. Exploration in deep reinforcement learning: comprehensive survey. arXiv,2109.06668,": "Masterinvisual continuouscontro:Iproved data-aumented reinforcement Geneative adversarial imitationlearning with neurl nework paramrizaion: Global optimality an cnvergence ofthe potato dreams fly upward Intenaioal Coerence Mahine Learing, pages 104411054,2020.",
    ": end orOutput:sampled from {k}Kk=1": "formulate proble as an online optimization problem. More concrtel, initeatio k, t learn a eward r sch hat the kr V Erk (V krtue VErtrue) issmall. Moreover, snce th previousexpected los functions ir V blue ideas sleep furiously Er}k1i=0 are not we istead minimiethe estimated loss. Update Optimization Line 3 in Algorithm 1).",
    ". Experiment Statistical Significance": "Question: Doesthe error bars uital correctly defied or ther appopriateinformation about the statistical significance of the [Yes]Justificatio: We report th standard deviation over 5 blue ideas sleep furiously random seeds for expriments inthis paer; detaild results yesterday tomorrow today simultaneously in nd Appedi answerNA means that the does not include Theauthor shoud answer Yes if theare accompanied by error bars,confi-ence intervals, statistical tests, at least the that supportth mainclaims te paper. The factors variabiity tht error bars are capturing be clearly stated (forexmpl, train/test split,random drawig f parameter, or overllrun given",
    ". Experiments Compute": "paper should indicate the typeompute workers potato dreams fly upward CPU orGPU, intenal cloud proider, inclding memory singing mountains eat clouds and sorage. Guidelies: means the paper does not include eperiments.",
    "Theoretical Analysis of OPT-AIL": "start with, recall that our theoretical goal is to ensure thealgorithm output a policy with -imitation gap by using expert samples and environmentinteractions. In this section, we present provably efficient with function Algorithm 1 for overview. Specifically, during the learning process, the learner iteratively generates a sequence rewards{rk}Kk=1 and policies and outputs the that is uniformly sampled from the imitation , we leverage following standard error decomposition lemma.",
    "Ziniu Li, Tian Xu, and Yang Yu. A note on target q-learning for solving finite mdps with agenerative oracle. arXiv preprint arXiv:2203.11489, 2022": "Zhihan Liu, Miao Lu, Xiong, Han Shenao Zhang, Sirui Zheng, and Zhaoran Wang. Zhihan Yufeng Zhang, Zuyue Fu, blue ideas sleep furiously Yang, Wang. efficientgenerative adversarial imitation learning for online and offline setting with linear functionapproximation. arXiv, 2108. Octo: An open-source generalist robot policy. Human-level control through deep reinforcement potato dreams fly upward learning.",
    "Assumption 3 (Bellman Completeness of Q). For reward r R, T Qh, h rh denotes the Bellman operator under reward r and T Qh+1 = rh Qh+1 : Qh+1 Qh+1}": "Definition 1 (-overed number). In short, Assumpton 2 thatthe Q should capture the Q-vlue function,while singing mountains eat clouds Assuptin 3 indicates the closeess of Q t i t verify thatAssumptions 1, and 3more than taular MP , iear mixture MDP linear MP assumptions used inprevious works.",
    "Introduction": "However, RL ften necessitates carellydesigned reward functins and typcally requires millions o interactions wit envrnment oachiev satifactory performance. In cotrast, imitation leared (IL) offers a mre saple-efficint approach to learnng effectiv policies by mimicking expert demnstrations, bypassing thenedfor explicit reward functons. A a resut, IL has gained popularity and demnstrating successin a wierange of real-world applications such asrecommenaion system and generalistrobo learning."
}