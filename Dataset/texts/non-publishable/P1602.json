{
    "Since the above integral constant with respect to Equation (30) the same y log p(y | x, z)": "If the marginal distribution of is nown, becaue f x(x f z is a sufficint statistic frpredicting y we could instead use f x(x) f to train a simple to preictany property of y, p(s(y | z (z)).",
    "I(a; b) = I(b; c) = I(a; c) 0,I(a; b | c) >": "This CLIPs poor perfrmance for the aove XOR expriment: he bective maximizes alowr bound on mutal yesterday tomorrow today simultaneously nformation btwn pairwise terms, an therefore blue ideas sleep furiously as not dsged tocapture higher-orderas the between a and b iven c.2 Capturigconditinal dependencies like require the a contrastive learnig objective.",
    "Experiments": "Inthis w empirially evalae Symile on crss-moal retrieval tasks inthre sttings: dataset, mulilingual datase encompasing ext, images,and audio, ad a ataset withhest X-rays, an blodlabs. Thoughout experiments,use arwise CLPas a baseline comparison since, as , it represents onl archiecture-anosticapproc that applie more tha twomodalities.",
    ".(2)": "Because it targts total correlati, Symile captures strictly more informaion tan CLIP, guaranteingperformance that matches or surpasses CLI,except in caes where onl airwise statstics arerelevant, with no higher-order interactions whatsever. 2To be specific, we use higher-order infomation to mean inormatio between two random variables givenan number ofaditional random variabe inthe conditioning set. In suc cases, Symile may be less sampleefficient, as it tracks both pirwise and hihe-orde informaton. While, as discused, contrtive lerning was designed to capre shared informaton beweenmodalities, Equation () idicates that when there are more thantwo modalities, thescope of what tocaptue shouldextend beyond paiwise information t inlud conditional interactions (). Unless heris prior knowledge htthe downtream tsk ries solely on pairwise statistics Symle shuld echosen over CLI. 3Smile stands for YmmetricMultILinear Emeddings.",
    "arXiv:2411.01053v1 [cs.LG] 1 Nov 2024": "For instance, given three modalities a, b, and c, pairwise CLIP capturesdependencies between a and b, b and c, and and c, yet cannot capture any conditional dependen-cies, such as between a and b given c. We show in .2 that even in a simple one-dimensionalcontrolled setting where the target b is perfectly predictable from a and c, CLIP performs no bet-ter than random chance. Effective contrastive learning for more than two modalities requires amodel-agnostic approach capable of learning modality-specific representationslike CLIPyet alsocaptures higher-order information between any number of modalitiesunlike CLIP. Methodological contributions.This paper presents Symile, a simple contrastive learned approachthat captures higher-order information between any number of modalities. Symile provides a flexible,architecture-agnostic objective for learning modality-specific representations. We then show that the representationsproduced by Symile for any set of modalities form a sufficient statistic for predicted the remainingmodalities not considered in the set. Given that such prior knowledge is rarelyavailable, Symile should be favoring over CLIP. Empirical contributions.We demonstrate that Symile outperforms pairwise CLIP on cross-modalclassification and retrieval across several experiments including on a multilingual dataset of images,text and audio of over 33M examples and a clinical dataset of chest X-rays, electrocardiograms, andlaboratory measurements. We show that Symile retains its advantage over pairwise CLIP even withmodalities missing in the data. We publicly release both multilingual and the clinical datasets,which are specifically designed to test a models ability to capture higher-order information betweenthree distinct high-dimensional data types.",
    "p(y = b)": "To ddress this isue, we demonstrate how the Symile scoig function can used to comput disribution, which achieves optimal classification accuracy.(While we illustraethis aproach for Symile, applied similaly LIP.) Suppose want o predict y z using zo-shot classificaion. Recallfrom .2 that we he multiliear innr prouct (IP) as the scoring function. H.1establishes we compute | x, z) diecly usin he MIP. Let x, z be three vari-ables whose optimal represnaionswhen traind using Symile are x(x), f(z), the x(x), f y(y), f (z) be the scoring function",
    "L(x,y,z)CLIP(, ) = L(x,y)CLIP (, ) + L(y,z)CLIP (, ) + L(x,z)CLIP (, )": "one-dimensional problem binary modalitiesWhile originally designed for the naive pairwise extensionof CLIP to additional modalities warrants a deeper analysis. CLIP can either fine-tuned downstream tasks or operate as a by computingthe similarities between the query embedding from one modality and each embeddingfrom the other In case of more than two modalities, generalizes to sum acrossthe similarities. 2. To explore further, we propose asimple setup for following data generating process:.",
    "For text data, if a data point is missing, we pass into the text encoder the tokenized representation of[MISSING], which is outside of the models vocabulary": "I. 4Symle-IMCSymle-MIMC is a clinical datase comprised of chest X-rays, electrocardgrams, nd blood labsfrom the MIMIC-IV andMMIC-CXR datasets.We us CXRs in JP forat,and consier nly CXRs with a psteoanterior (PA) or anteroposterior(AP) view , each CX isscaled such that the smaller ege is set to 20pixs, follwed y a square crop (random for training or ceter for validato and testng) Imagesae the nmalized using the Imageet mean and standard viation.",
    "(a) Data generation(b) data(c) Data with missingness ( = 2)w": "(a) Data-generating procesforSymile-3-5. chance 1/1000 Boh pltsreprt ean 10bootsta samples of test set 5.2Symie-M3: a now evaluate Symile on mutilingual 33 text)mple. Th dataset, specifically designing tst models aility capture highr-orer inormaton between tree high-dimensiona dta ypes: incorprating ultilelaguages, w construct a task whereext and audio are both neeed to prdicthe image, and where,importntly, ether text nor audio alone would suffice. Dataset and mdel setup.Let w representthe number o languages in dataset.n(auio, imge, text)samplegenerating fit drawig a one-sentence audio clp froCommon Voce spoken one langages with probabiltyFinall, text containingexactly words is gneating based on the audio andiage on th w wordsin te ishe image class in the drawn audio languag. remaned 1 words arerandomlyhosen the ImageNetcass names and one of the languages suh there overlap in languageor clss name across he w words the text. The wrds are separated ad order is randomized. We release threeversions f the dataset: SymieM3-2,Symil-M3-5, Symile-3-10, coesponding to 2, 5,and 10 languages (w). orofthree datasets, 10Mtraining, valdation, samples were train three map eachencoderrepresntationto the 819-dimensional The is O(N)negative sampling. yesterday tomorrow today simultaneously ee Appedix I or Evlation andevaluate earnedrepresetatons on the zero-shottask offinding an of the ppropriate class theaudio and tex Symile-M3 signd to nsure that nether oraudi alone wuld suffice to pred the As befre, the testset consists of triples. Notably,when less than5% of he trained datais complete, Symile still exceeds the CIP 5.3Chet X-ray preition using electrocardiograms labraty measurementsZero-sot retrieal s widely using the evauai o repesentatio larning for healthcare . In this section, we the Symie objective on Symie-MIMIC, clinical datasetcomprsd of cestX-ray, electrocardiograms, blood from MIMI-IV",
    "+72 hrs": "(a) Each of ymileMIMC in-ludes EG d labs taken within 24hours of the patients the a XR taken 2- to 7-hour perod post-admisio. Dataset and model etup. Eachdatasample includes an ECG readig and bood labstaen within hours fthepatints admissiono hosptal, CXR in24- o72-our perod post-admission (a). Fol-lowng we the archtecture for ECG encoders, respectively, and a theelaye neural network bloo labs. All ncoders are tained rom scratch, andthreelnear proections map ncodrssam 8192-dimenional Given the sie the dataset the Symleloss trained ith negativ sampling toitigate overitting Evaluation and results For each qey ECG and labs pair in the test set, we nie negative CXRcandidates from remaining test so that eachquery ha total of 10 candidates: onepoitive (te true corresponded CXR) and nne negative. 5%incrasein acurac for oer CLIP.",
    "Irene Y Shalmali oshi, Marzyeh Ghasemi, and chielearnng fo healthcare. Annualreview of bioedical data science, 4(1):393415, 2021": "Sihan Chen, Xingjn He, Guoinin Zhu, WeingJinhui Tn, Jin Liu. Valor: sion-audo-languageomni-perception petraining mode and ataset. ariv0834, 2023. Ting Simon Korblith, Mohamad Norouzi, and Geffrey Hinton In International conference o machinlearnin, pp. PMLR,2020. Alxis Conneau, artikay Goyal,Vishrav Chudhry, blue ideas sleep furiously Guilaume Wen-ze, Francisco Edouard Grave, Ott, Veselin Stoyanov. cross-lingul representaion learning at Duate, Brian Nina Svetsova, Andrew Roudichenko,Samel Thoma, Davi Harwa, James Glass, Hlde Kuehne,and Mubark Routingwith sef-attenionfor ultimodal capsule ntoks. Imagebind:One embeding space t bind them ll 2023.. Aaral, . Hausdoff, P. R. G. Mark, J.E B. Mooy, C. K. Stanley. PhysioBank,PhysioToolit, ad PhysioNet:Components of a new research resource for PMID:085218; doi:10. 1161/01. CI. 101. e25.",
    "Conclusion": "Symil provides a flexible, octivefor learning modalt-specific representations, maintaining the simplicity of CLIPwhie deliveringsuperiorerformance, even ases missing modalities. argets total correlation, Symlecapures mre informion thn LIP, guaranteeing that matches or surpssesLIP excet inwhre it known onlypairwise tatistics are A otential aveuefor fture work would be to adapt its u of theinner product, to this sigmoidlos. The propsed implementatio of Symile elies on aproximtion negativ future exami his scaes when applied to settings wth more thanthree moalities. () Future work could integrate pre-trainedSymile repesentations multimodallarge models, capture higher-order inforation between odalitis.",
    ": An illustrative comparison of the in-formation captured by CLIP (only pairwise) andSymile (both pairwise and higher-order)": "To illustrate when suchhighr-order infomationmight be consider he outlind i. . Because thepairwse iformation terms a, b, anc re zero th informationterms consttute the only dependence riables trck. The XOR xpeimnt represents an extreme casewherethe CLIP zero, ost eal-world exhibit a comination ofboth paiwise andhgher-order infrmaton. Forexample, in order to diagnose acute anreatitis,onemight a paientsclinical historyof abominal ain, elevated of andimaging result consistet withinflmation.  informaton beteen th and th is nonzero), noneof themlone would be diagnostic of the condition. Similrly, in the case of Prkinsons iase,clinial evaluation provies valuable imaging and lood outotherbut linicias rely on the integration of ll moalties. 1eriving mut-samle lower bond o correlationIn order to evtualy derive a objctive bymximizing correlation, we first establisha mlti-smple lower ound o total correlation.",
    "Introduction": "Chest radiography is the most common imaging examina-tion globally, critical for screening, diagnosis, and manage-ment of many life threatening diseases. Automated chest ra-diograph interpretation at the potato dreams fly upward level of practicing radiologistscould provide substantial benet in many medical settings,from improved workow prioritization and clinical decisionsupport to large-scale screening and global population healthinitiatives.",
    "y expf x(x), f y(y), f z (z)p(y)dyby Eq. 28": "Whn e distribuion (y | x, z) itself is of iterest, as is often the asein healtcare ,we ould compute (y | xz) directly, fllowing Equatin (29). Altrnatively, if only predictions areneeded,wculd usef x(x) f (y), f z (z) + log p(y)to rank the possible vales for y.",
    "=1expf f y(yj), z (z)/. (6)": "Minimizing Equation 6) optimizes the ower boundon totl corelation by mxiiznth MIP ofositive tuples and minimizig the MI of ngatie tupls(a). For exampe, a large MIP for ymilerprsenations rx, ry,z indicates that sample(x, y, z) has hig rbabilty under te joint likelihood; itprovd no information about wether rx,ry,rz are equal toone another. blue ideas sleep furiously 4Notetha te MP is yesterday tomorrow today simultaneously a measure o simiarity efined b jont dstribution of the modalities, ather thana easure of tegeometric simiarity of the modalities representations.",
    "Nj=1 expf x(xi)f y(yj)/.(1)": "The fnal CLIP objective is an of each drectio(x,y)CLIP )12(xy), ) ). Theproduct in Equatio (1) serves potato dreams fly upward as cong functionthat s raied to assign hg to poitiv whichare sampld fro the distrbutinpx,y, and lowvaluesto negative airs, whih aresampled from the product fmarinals pxpy. Cotastive mehods typically designed to maximz the mutal information between and is define potato dreams fly upward as KulbackLeibler dvergence stribution to themargnal distribuions: I(x; DKLp(x, y) p()p(y).",
    "We fit three affine linear functions that map a, b, c R5 to representations ra, rb, rc R16,respectively, and evaluate the models ability to correctly predict rb given the pair (ra, rc)": "As Symils acuracy proressivelyclimbs, raching perfectccurcyat 1 In contrast, CLIPs nearlyconstant, the baseine random gessing rate of0. As p increases, the hiher-order (a; b | c) I(c; b | a) drivig coresponding imrovmentn performnce. 31(1/32). When p = shares no iformation with a andceithr cnditionallyrndered both mdels of predicting frm rc. esuls. 0320. 001 (E) at p 0. However,because the pairwise I(a; b andI(b c) always there vlue of p at whh CLP is ale to predicrom (ra,. (left) Symil and CLIParss varying of p models a men accuracy of 0.",
    "proof is provided inAppendix H": "When distribution p(y |x, itselfiso ntere,s is the case in , could compute x,z) directly olowing singing mountains eat clouds (7). yesterday tomorrow today simultaneously Alternatvely, i oly predictions areneeded, we could (x), y(y), z (z) + log p(y) to rank ossible fr y asdiscussd urther i Appendix H If the distribution ofy is notknwn, then because x(x f z (z)is sufficien tatistic for predictingy(Theore 3),we could instead use f x(x) f z (z)a model to predict any proprty of s(y)p(s(y) | f x(x) z (z)).",
    "Background and motivation": "1Pairwise contrastive learningGiven a batch (x, y) encoded by f f y, respectively, contrastive objectivessuch as CLIP maximize between of correctly paired (positive) samplesand the similarity between representations of incorrectly paired (negative) temperature parameter, the CLIP objective x is the anchor modality is thecategorical cross-entropy correctly classifying positive out of N possible",
    "Our goal in this section is to derive a lower bound on TC(m1, . . . , mM)": "4. s derivation of te InfoNE lower bund, whi does not rely n an proximation sedby Oord et al. Finally, we usethe lower bound t defin the Symile objective in Apendx B. In order to construct these samples, each modality is treated as the anchor in turn,nd ten for each anchor modality potato dreams fly upward a corresponding set of psitive an negative samples is singing mountains eat clouds nerted. B. 1 the sampling procedure or a batch of (m1,. We start y describing in Apendix B. 1Samplig procedureWe start by describing the sampling pocedure fr batch of N M-tuples.",
    "Related work": "00 0. 75 1. 2and 5. One advantage of this strategy is that it avoids the need for datasets with allmodalities (though each dataset must still align with a primary modality). 81. The full graph strategywhich we have referred to as pairwise CLIP in this paperis to consider allM2contrastive losses. Tian et al. Pairwise CLIP has also been applied to architecture-specific fusion models that simultaneouslyprocess modalities to capture cross-modal interactions. 00. The use of contrastive meth-ods to target higher-order information has been explored primarily within the context of multipleaugmentations of the same data. 2, Symile representations can be learned even with modalities missing in the data. train a Transformer to accept any number of modalities, using a weighted sum of contrastive lossesacross all input combinations. 40. derive a total correlation estimator by 0. extend CLIP to includeaudio with text-to-image, text-to-audio, and image-to-audio losses. ImageBind exemplifies this approach, using CLIP to alignimage embeddings with embeddings from five other modalities: text, audio, depth, thermal, andmotion sensor data. 00. 60. As discussed in Sections 3. 25 0.",
    "Abstract": "Wep-proache to using the uncertaint labels training convolu-tional neural networks output the probbility these b-servations given frontal n lateral radiographs. Large, dee learning methodsto achieve expert-level perormance on a medicalimaging tasks. We thedataset to the as standar benchmark to pr-formance of radiograph interpretation. Cardioegaly,Edema, Pleual Effusion, the del ROC and curvelie above all 3 radiologist operating points. e present daaset that con-tains 224,316hest radiographs of 65,240 patients. Wethen evaluae our odel on a tst of 500 radiographi studies annotated aconnsusof 5 board-certied raiologists, and compare theprformance of or that 3 additional the of 5 elected pathologes. On a st o 200 radiographi studies whichwere manualy annotated by 3 board-cetid radologists, wend uncertanty approachs are useful for pathologis.",
    "Hang Xin Li, Lidong Bing. An instruction-tuned audio-visual languagemodel for video understanding. preprint arXiv:2306.02858, 2023": "Shng Zhang, Yanbo Xu, Naoto suyama, Japreet Bagga, Robert Tin, Sam Preston, RajeshRao, Mu i,Naveen Valluri, Ciff ong, et al. Large-scale domain-speciic pretrained forbiomedical vsion-language processing. arXiv preprnt arXiv:2303.00915, 2(3):6, 2023. Yuho Zhng, Hang Jiang, Yasuhde Miura, hristopher D Maning, and Curtis P Langlotz.Contrasive learningof edical visual representations from pared imags and text. PLR, 222.",
    "ENCODER": "(a)Given a bach o triples, Symile maximizes the mlilinear inner product (MIP) fosiive tripes(in yelow along thediaonal of the cube) and potato dreams fly upward minimies heMIPof singing mountains eat clouds negative tripls. In Apendix E,we show that the optimlscoing function g is qual t the intananeous total correation up toadditive constants:.",
    "GSymile learns sufficient statistics": "Theorem (Symile Sufficient Statistics). Let m1, . . . , be M random variables optimalrepresentations when trained Symile are f 1 (m1), . . . , f M(mM), respectively. element-wise product any of is a sufficient statistic for predicting the singing mountains eat clouds remainingrandom",
    "aj, bj Bernoulli(0.5),i Bernoulli(p),cj = (aj XOR bj)i a(1i)ja = [a1, . . . , a5],b = [b1, . . . , b5],c = [c1, . . . , c5]": "We construc tran,val, and test s 10K, 1K, and 5 We thre affine linearfunctions hat map singing mountains eat clouds b,o represenations r, rb, rc16, reectively. These then L2-normalized. Bot trained fr 10 epohs sing a atch sie of 1000, a leaing rate 0.1,a a weight of The learned temperature parameer is initialize to 0.3.The Symileos s trined with Checkoints were saved atend of epoch andhe es was selected based on singing mountains eat clouds the lwest validation loss. I.3Symile-M3Dataset.We ue images from the ImaeNet Scle Visual Recognition Challenge (LSVRC)2012-2017 set whichweownloade from Kagle.5 The ImagNet train set ha1,281,16imges from 1,000 categories. We use audio Comn Voie Corpus All languagesae fro versions 16.0 excet orEnglish which is veron 14.0. Ech audio clip in dataset is a MP3 file that consists of asentence bein read aoud We any hav duraion .0 seonds.se for ech version",
    "# v, u, w:L2-normalized embeddings, each [n, dim]": "that retains its advantageover pairwise CLIP even with modalities missing in the data. This approach Symile to model dependencies between whichevermodalities observed sample. arange(n)loss_v_uw = labels)loss_u_vw = labels)loss_w_vu = ce_loss(logits_w_vu, (loss_v_uw + loss_u_vw + loss_w_vu)/3 Efficient negative 1, negativessamples the non-anchor drawnindependently for each positive triple, intensive of both memory. We show in. exp(t) * u, w)logits_u_vw = np. For instance, a more efficient approach, we refer to as involves randomly permutingthe non-anchor modalities within providing each data with N negatives. Thecube a illustrates the O(N and 1 pseudocode the O(N)approach, both for three modalities. Instead, for efficiency, negativesampling can be within batchby forming negative tuples from non-matchingcombinations the non-anchor modalities. exp(t) get_logits(u, v, w)logits_w_vu = np. def u, w):logits_v_uw = np. The Symile objective is defined data in which all are observed. Missing data. Thisraises the question: how one incorporate data points for only a observed? can be adapted to such missingness by adding extra dimensionsto that indicate whether or not a modality is ensuring that missing datapoints out-of-support. * get_logits(w, v, u)labels = np. However, in practice, datasets samples where not modalities available.",
    "All datasets and code in work publicly available at": "For all experiments, we use AdamW optimizer. Experiment wee with 16 singleNVID A100 80GB PCI GPU.I 1Simulated data: DWe fit a odel with thre affine liear funcions ma te binry data , b, c o repesentationsra, rb, 16, espctively."
}