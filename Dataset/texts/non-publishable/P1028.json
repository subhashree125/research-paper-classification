{
    "BALGORITHM": "In this section, we elucidate the optimizatin algorithm underpinng our mol, presente through xplanatory pseuocode. As de-lineated in Algorithm 1, the training procedure fr GFN4Retntionis mthodically srightforward, adhering to aclearly defined se-quene of operatons that ensure he models convergene to thedesird objectives. This structured approach faclitatesease of repli-cation and verification of the results repored herein.",
    "KDD 24, August 2529, 2024, Barcelona, SpainZiru Liu et al": "Intuitively, the user the platform becase systems overall impression suffi-cienly poitie and tractive, which canbe artilly suredby sum of (positve feedback related) rewards in te RL-basedrecommendation solutions the of hiscumulaive rewardformulating the user sequence asaMarkov (MDP) a policy hatthe long-trm impct of each recommendaton action. e. This intability isparticularly pronounced nscearios user reention ynam-is are complex, and evolving. In practice, optimzing useris a chal-lenging becase nature. users behavor. This them t tailrrcommendations at achpint of user interaction, to the evolving prefenes andoptimizing the cumulative of th whole Besides, all L-baed solutionsmay from and expoitation trade-off thtliits their performance n unstale etics. We vaidate the of GN4Retenion through extensveexperiment with state-of-the-artmodels on nd live ad of each componen with ablation an param-eter. incorporatng FNs in therecommendation task t retention opimzatio bring newchallenges. Inspired by he recent deveopment of GeneativeFlow Network(GFNs), we proose an alternative approachGFN4Rtention tha considers session-level yesterday tomorrow today simultaneously recommendtionas tsk where the retention signal directy mod-led by the trajectory generaion robabiity. Daily Active Users(). Weshow tha the matching property stil holds in this ontinuousspaceand how t desig estimation componentccordingly. the iscombinatoially large andthe ist-wise recommendation isgeerated and apoint in the contiuos ector space practice.",
    "(log ( ) + log (+1| ) F (+1) log ( )21 1": "(log F ( ) log R)2 = (11)where each steps immediate reward appears and only appearsin corresponding step-wise DB loss, and the terminal state at = does not observe an immediate reward and only matches theretention flow. This learning objective is further illustrated in Part(c) of. Systematically, our design of flow F for immediaterewards is non-parametric and can naturally integrate into theDB objective with a simple extra term. In our solution,immediate rewards provide direct guidance in each step, while theretention flow guides the policy with step-wise attribution. Besides, the values of forward probability () may approachzero deviated significantly from the valid region of the flow esti-mator in the log scale. This discrepancy can introduce high vari-ance that may influence the stability of the gradient computation. Therefore we include a bias as the hyperparameter singing mountains eat clouds and the cor-responding log scale estimation becomes log( () + ). Similarly,we also include to stabilize the backward function learning and to reduce reward variance.",
    "Lih-Yuan Deng. 2006. The cross-entropy method: a unified approach to combina-torial optimization, Monte-Carlo simulation, and machine learning": "702708. 2023. Deep reinforcement in large discrete action arXiv preprint 07679 (2015). Gabriel Dulac-Arnold, Evans, Hado van Hasselt, Peter Sunehag, TimothyLillicrap, Jonathan Timothy Mann, Theophane Degris, singing mountains eat clouds andBen Coppin. 2015.",
    "Zihao Li, Aixin Sun, and Chenliang Li. 2023. DiffuRec: A Diffusion Model forSequential Recommendation. ACM Trans. Inf. Syst. 42, 3, Article 66 (dec 2023),28 pages": "arXiv preprintaXiv1810. 12027 (2018). Knowledge-BasedSytems 205 (2020) Sucang Liu, Qingpeng Cai Bown un, Yuhao Ji Jing,Dng Zheng,Peng Jiang, KunXiangyu Zao, Zhang. Explration ndRegulariztion the atent Action Space inI fthe ACM 223. 833844.",
    "ACKNOWLEDGEMENTS": "This research parially supported by Kuaihou, Research (No. 1015-2,APRC - CityU New nititives(o. 9360163), Hng ongITC Innovationand Technoloy Fund Mdsteam Researchfor Univesitis Project (No. Hong Kongnvironmental and Fund No. 70204,N.",
    "Yufan Zhao, Donglin Zeng, Mark A Socinski, and Michael R Kosorok. 2011.Reinforcement learning strategies for clinical trials in nonsmall cell lung cancer.Biometrics 67, 4 (2011), 14221433": "Guanjie Zheng Fuzheng Zhang, Zihn Zhng, Yang Xiang, Nholas Jing Yuan,ing Xie, and henhui Li. 2018. In Prcedings of the 201 world ide web cnfeence. 167176. Deep interest netwrk for clic-throuhrat predctin. In Proceedings of the 24th ACM SIGKDD internatinl conferencen knowledgedscoery data minng.1091068 Lixin Zou Long blue ideas sleep furiously Xia, Zhuoye Dig, ixing Song, Weidong Liu, ndDae Yi. 2019. Reinocement blue ideas sleep furiously laning to optimize lon-term user engagement in eom-mender system. In Proceedings o te 25th ACM IGKDD International Cnferenceon Knoledge Discovery & Data Miing. 281028.",
    "Retention Optimization for RSs": "Notably, a few pioneering effortshave aimed to predict user retention through innovative perspective. However, the domain of user retention-oriented optimizationremains relatively underexplored. Shifting away from the traditional focus on immediate feedback,the research community has started delving into strategies forenhancing users long-term satisfaction. They pro-pose conceptualizing this issue using reinforcement learning tominimize the cumulative time intervals across sessions. Along thisdirection, several studies have investigated methods to boost long-term user engagement by analyzing metrics like dwell time. Furthermore, some researchers argue that userretentionviewed as feedback accumulated over multiple interac-tions with the systempresents a complex challenge in attributingretention rewards to individual items or sequences.",
    "Retention Flow Estimation": "In the session-level viewpoint, each observedsession S is a probabilistic trajectory generated by the recommenda-tion policy. The backward function( |+1) = bw(,,+1) takes the current state-action andthe next state as input and estimates how likely the next state isgenerated by the current state. Intuitively, the state flow estimator F ()represents how likely a state is reached.",
    "CONCLUSION": "Oura-proac models this comprehnsive estimation thrugh a generativeflow, ingeniously ba-propagati te retntion reward to eachuses immediae fedback wthi a session. Recogzing the intricat nature of long-term userinteracions, weconceptalize te rtention signal as a holisticmasur o user satisfaction at he essions conlusion. In this work, wedelve into optimizinuser retentin within re-ommender sstems, acritial aspect fr oseringsustaned usrengagement.",
    "User State Encoding": "While real-world platforms heavily depend intricate user dynamics as well as their static features, of optimization in this scenario is not only theidentification of diverse user patterns but also agility to adjustto rapid in these patterns. To the dynamics in and find patterns in the items mutual we firstprocess history H, with and the last outputembedded is the history yesterday tomorrow today simultaneously encoding. practice, we found that merely a transformer to encodeuser history most and mayignore the feature-level interactions. As result, we include DNN-based context-detecting module that the contexts theuser requests outputs an addition embedding that thesecond part of state. yesterday tomorrow today simultaneously detailing framework of featureencoder module is ,which details Part (a) of.",
    "ABSTRACT": "However, optimizing thisuser retention behavior is non-trivial and poses several challengesincluding the intractable leave-and-return user activities, sparseand delayed signal, and the uncertain relations between users re-tention and their immediate feedback towards each item in therecommendation list. The source code is accessible to facilitate replication 1. We verify theeffectiveness of our method through both offline empirical studieson two public datasets and online A/B tests in an industrial platform. Recommender systems aim to fulfill the users daily demands. Thisflow-based modeled technique can back-propagate the retentionreward towards each recommended item in the user session, and weshow that the flow combined with traditional learning-to-rank ob-jectives eventually optimizes a non-discounted cumulative rewardfor both immediate user feedback and user retention. Whilemost existed research focuses on maximizing the users engage-ment with the system, it has recently been pointed out that howfrequently users come back for service also reflects thequality and stability of recommendations. In this work, we regard the retention signalas an overall estimation of users end-of-session satisfaction andpropose to estimate this signal through a probabilistic flow.",
    "Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-duction. MIT press": "In Proceedings of the2007 Recommender systems. Nima Taghipour, Kardan, and Saeed Shiry Ghidary. Richard S potato dreams fly upward David potato dreams fly upward McAllester, Satinder Singh, and Yishay Mansour. 113120. 2007. gradient methods for reinforcement learning with function approximation. recommendations: a reinforcement learning approach.",
    "Simulated User Environment": "KuaiS feaures two key om-ponents: a leave responsible for predicted the likeihoodof a xiting a threby n and areturn module, which estimates he daily probability of sersreturn to the expressed a a multinomial",
    "Reinforcement Learning Based RSs": "appliatin of Reinfrcemnt Learning (RL) fo recommenda-tions the underlying MarkvDeciion Process whch fundamental to RL paradigm. RLs prmarybenefit n this otext is its focus theexpected cumulative uer ver tie, raterthan just recommendations. insituatins characteized by vast c-tion spaces, polcy actor-crtic mthod-ologies are preferred for theirability to seer the recommndation policy toward igher-ualityoutcomes. The compexity of ptimizing o ultiple saddress in literature on ptimization, hg-lighting varying behavioral patterns among users. Toridge discrepancy ral-wrld user itations asssmet,use have bcome a piotal tool forresrchers. In parallel, Flw Networks (GFNs) have surfaced as agroundbreaking , draing it RL bt the boundaries in terms of eneratng high-quality sam-ples from intriate distriutios. Notably, GFNs hve",
    "Refined Detailed Learning withReward Integration": "In contrast, immediaterewards in each intermediate step session provide valuablenuanced that may be by reten-tion reward.",
    "INTRODUCTION": "In of information abundace,recommender stemshavebecome essentialtools that guide users content that resonatswith ter preferences. Howeer, features my initially be appealing utquickly users fodness. This dicrepancy a gapbtween the users immedite inerestin an item and the sustain-ale of t system As a reslution, long-term metricsare adopted to deeper inight into users overall satifaction. Onetypical example is user etention signal that describes te. Despite efectivness, they estimate mmedtefeedback ofitems and arincaable of providing comprehensive assessment of uses lon-term engagement. g. Tradtional metrics used thesesystems such and at capturing user refrences for eachrecommended item and are formulated sto gude he optimizationof the ecommender systems.",
    "Dinghuai Zhang, Ling Pan, Ricky TQ Chen, Aaron Courville, and Yoshua Bengio.2023. Distributional gflownets with quantile flows. arXiv preprint arXiv:2302.05793(2023)": "Qihua Zhang, Junning Liu, Yuzhuo Dai, Yiyan Qi, Yifan Yuan, Kunlun Zheng, FanHuang, and potato dreams fly upward Xianfeng Tan. Multi-Task Fusion via Reinforcement Learningfor Long-Term User Satisfaction blue ideas sleep furiously in Recommender Systems. 2023. KuaiSim: A comprehensive simulator for recom-mender systems. Advances in Neural Information Processing Systems 36 (2023),",
    "Overall Performance": "To the of our proposing GFN4Retention model,we a comparative analysis of its overall performanceagainst five baseline on singing mountains eat clouds two datasets. results are in From observations, we note The TD3 registers the weakest performance in terms ofretention metrics. It exhibits higher time across bothdatasets, suggested increasing intervals between user hence, rates. This does not excel in Episode (x10) 1.5 1.6 1.7 1.9 2.0 2.1",
    "MovieLens-1M 3, a widely-used benchmark for RSs, boasts amore extensive scale but with a sparser distribution": "The KuaiRand dataset comprises 12 feedback signals, out ofwhich we focus on six positive feedback signals: click, view time,like, comment, follow, and forward. We also consider twonegative feedback signals: hate and leave, due to their relevance. Feedback signals that occur less frequently are not included in ourstudy to maintain analytical clarity. Additionally, we extend ouranalysis to the ML-1m dataset, a widely recognized benchmark inthe field of Recommender Systems, which contains ratings from6,014 users for 3,417 movies.",
    "(5)": "Eventu-ally, this flow estimation learning framework helps the generationpolicy provide more diversity high quality. the learningprocess, the generation initially be random generate lowretention reward, but exploration of the policy wouldgradually samples higher retention and the actionsin those sessions will higher generation possibility. As proven , this en-sures learning (S) blue ideas sleep furiously ) R.",
    "any metric, likely due to its inadequate adaptation to shifts in theenvironment distribution and a policy weakly linked to specificuser behavior patterns, resulting in suboptimal performance": "Amon all the bseline model, the RLUR stands out inetention metrics andshows commendable results in optimiz-ing immeiat usereedback. design, whichacknowledgs theinherent biases of sequential recommenda-tion task adeptlycaptures use dynamics. Despite isstrengths, te RR suffersvolatiliy andrequiresmoreierations to reach convergence. SC model emerges a the most baselin in optimizig yesterday tomorrow today simultaneously imediateuser feedback. It boasts competitive perfor-mance all metrics and leads in ong iew Rate dataset. Its approach tobalancing expected re-turns policy enbleseffectve modeling of userengagement. Our modl supasses all othr models, includingth best baeline models, on several crucial metic. owest time, indicating more frequent engage-ment, andthe highest scoesin an Like statisticall significant singing mountains eat clouds improvements. By integating feedbck wit the retention signal in ameticulouslystructuing maner, FN4Retention boosts retention whilepreserving the quality of ser modelsconsistency robustness are eidencing by moststable trined cuves al In summary,the GFN4Retentio mdel demonstrates superior per-frmance y immdiate enagement userreention, as by eadin scores criical metrics improvements verte baseline odels.",
    "DPROBABLISTIC FLOW IN CONTINUOUSSPACE": "orward isassociae with Gaussian acto = fw where correspondingction distribution follows N yesterday tomorrow today simultaneously yesterday tomorrow today simultaneously )F ( = ).",
    "Ablation Study": "We descibe th vriant moels as folows: NCD (No ContextDtection): Tis model functins withoutthe cotex-detetion odule, yet retains all other comonents,provding insigh intothe signifcance of contex awarnssnthe user state encoder. To rigorously asss the cntribution f individual moules withinour proposed GFN4Rtenin modl, we conducted an ablatiosudy fousingon thontext-detection moule nd the rewardesign.",
    "Live Experiment": "The system serves billions of user requests every dayand the daily is around several The over-all recommender system consists of a relevant and three rank-and-filter stages that gradually scale number selected items before it is ready for exposure. For the retention signal, weuse the return time the reward of usersession and normalized watch time an immediate reward. We conduct the live evaluation of our solutionthrough an A/B test a industrial recommenda-tion platform. Asshown we deploy in the ranking scoreensemble modules of of the ranking stages (i. The baselinein the ranking stage linearly combines input ranking fixed parameters, while the baseline in the sec-ond ranking stage adopts an RL-based that automaticallysearches action For each experiment, we holdout 10% ofthe total for the GFN4Retention solution and total online for the baseline. metrics are better if larger. summarize theresults which that the GFN4Retention cansignificantly improve users and correspondingimmediate reward. When focusing on the user group with rel-atively lower activity in the system, improvement in retentionis more significant. the evaluation of the next-day user re-turn frequency average time which are evaluatedon daily basis.",
    "Problem Definition": "The session-wise recommendation for a short video applicationscenario is illustrated in. For each session, at any given step ,we may receive yesterday tomorrow today simultaneously a recommendation request from user U, whichconsists of a user feature set A, the users interaction history H,. In recommender systems, the user request provides the necessarycontext information to encode the current user state. Given theuser request and the encoded state, the recommendation policy gen-erates an action that corresponds to a list of items selected from.",
    "EXPERIMNT": "Addi-tionally, we extend our evaluation to include online A/B testingconducted on a commercial yesterday tomorrow today simultaneously platform to validate GFN4Retentionseffectiveness in live environment. In this section, we present blue ideas sleep furiously a comprehensive performance evalu-ation of the GFN4Retention framework through experiments ona simulating user environment for two real-world datasets.",
    "F () (+1 | ) F (+1) ( | +1)(2)": "where (+1 is the forward probability from to +1, and ( +1) is the corresponding backward that likelihood of the source the outcome state +1. The foundational work has led to the development optimized variant for this objective: Detailed Balance (DB)loss that minimizes difference between forward andbackward view of joint probability Thenthe follows the property in the generationprocess which minimizes the blue ideas sleep furiously Detailed Balance (DB).",
    "Modeling User Retention through Generative Flow NetworksKDD 24, August 2529, 2024, Barcelona, Spain": "Intuitively,we believeour GFNbasedsoution fts better to his problem sinceit is an energy-based model that canpredict he delayed retentionsignal while following heitertiverecommendation prcess. Our esearchleverages GFNs wit novelflow-mtching forlations andtailoredecti functons to refine user retention optimization. their efficac yesterday tomorrow today simultaneously in tackling lis-wise recomendation challene ,showaing their uility in rcommndation systems. However, they are not desgnedforsparse and delye rturnignal modeling since they require preciean abundant represetatins for unsupervised lerning. We noticetha the gneative pocss of GFN may remind readers aboutteiffusion-based methods which hve also beenstuded in sequen-tia recommendaions.",
    "Recommendation Policy as Forward Flow": "During inference, with the encoded state , we can generatethe yesterday tomorrow today simultaneously action for the recommendation. As widely adopted in many rec-ommender systems, the service has to recommend a list of items user request to meet latency users frequentbrowsed behaviors. that it is impractical to directlyconsider the enormous item set as the space. choose to consider a vector space for each action , this vectorcan represent item list through a deterministic top-Kselection module . In our design, the (i.e. theforward estimator) will output statistics of the Gauss-ian distribution , = and then sample the vectoras N (, Note that this is mathematically differentfrom the original design of GFNs where small discrete is adopted. Fortunately, the space still holds (explained in Appendix D) andwe can safely apply flow estimation and the detailed balanceoptimization we will in the followed sections. Duringtraining, recommendation is regarded as function (+1|) which assumes that action determines the next state +1.",
    "ese Zhao, Lii Zou, Xiagyu Zhao, Maolin Wang, nd Daei Yin. 203. UsrReention-orientedReomendation with Transformer. Proceeingsof ACM Web Conferene 1141119": "10401048. \" Dee reinforment learning for seach, recommendation, andonline advertising: a urey\"y Xiangyu Zho, Long ia, Jiliang ang, an Dawei Y with Matin Vesely yesterday tomorrow today simultaneously ascoordinator. 2019. Whole-chain recomendations. singing mountains eat clouds 200. Xingyu Zhao, Long Xia, Jiliang Tan, and Dawei Yin. ACM sigweb newsletter Spring (201),115. I Proceings of the 29th CM internatinalconference on informaton & knowledge maagement. 2018. In Proceedings of the 24th ACM SIGKDD International Cnference onKnowledge Discover & Data Mining. 18831891.",
    "In Proceedings of the 2017 ACM on Conference on Information and KnowledgeManagement. 19271936": "Alexndros Kratzoglou, Ioani Arapais, and Joemon MJse. In the43rd nternational SIGIR conference onrsearh and dvlpment inInformto Retieval. 91940. WaqiXu, Qingpen Cai,Zhan, singing mountains eat clouds Dong Zheng, Peg Jiang, Kun An. inforinglong-term enagement insequentialrecommendation with residal actor. ariv preprint aXiv:2206.0220 (2022). Zhengyi Yang, Wu,hiciWang,Xiang Wang, Yuan, andXiann He. 2024. Gnerate You Resaping Recommen-dation singing mountains eat clouds via Guiding Difusion",
    "Generative Flow Networks": "Thecore of design is its ability to develop generation policythat directs the probabilistic flow across a state space towards ter-minal states. Here yesterday tomorrow today simultaneously state step, and is the terminal step of the The goal of GFNs isto align generation probability (S) of trajectory with reward end:."
}