{
    ". Data Scarcity": "44% on test set. 71%, respectively. Notably, the Guided-SR and GuidedMixup sig-nificantly outperform other approaches, with GuidedMixupachieving the of 74. This evidence suggests thatdata augmentation and techniques, DIF-FUSEMIX, are highly beneficial in enhancing model perfor-mance under stringent constraints. 74% on validationand 70. 56% and 66. 14% on validation 74. 12% on thetest set, showcasing its superior ability to generalize wellfrom significantly limited data. presents method as baseline 48% accuracy on validation set and 59.",
    "FUSEMIX outperforms SOTA 4 5 distinct safety metrics.Loweris better except for detection.(SOTA method results are taken fromPixMix": "with ac-curacies 86. 34%, and 40. 93%, 48. 3%, and 53. 28%. The systemati datasets onmultipe ethods blue ideas sleep furiously highlightth robustness ur approach and its potential improeth accuracie yesterday tomorrow today simultaneously of different machine learning.",
    ". Related Work": "Data has indispensable enhanc-ing the diversity of datasets, mitigating of overfitting. Traditional approaches as and vertical translations, affinetransformations, scaling, and training amodel. This not only improves but the generalization of model on test datasets.Diffusion Models for Recently, several re-",
    ". set of conitional prompts e used toobtaigenerated images prserving important fatures and addin to he imges": "Subsequently,a fractal image isblendedinto imageto obtaithe final training image wth a divrse sructur. Blending fractal images has roven to be effective towardsL Inour work, study the effective-ness f fractal images maly improvedperformance.",
    "DiffuseMix (T)": "For each input employs conditional prompts to generated images. image is concatenated witha to obtain hybrid Each hybrid image is blended with a random fractal to obtain the final training image.",
    ". DIFFUSEMIX with Methods": "In a series of experiments, we combine DIFFUSEMIX withexisting image augmentation approaches to see ifany performance gain is observed. Particularly, We replaceour masked approach with the masking used in the exist-ing methods while retaining the rest of the pipeline of DIF-FUSEMIX same. For CutMix + DIFFUSEMIX, we replace the singed mountains eat clouds concatena-tion step of blue ideas sleep furiously DIFFUSEMIX with the random cropping of Cut-Mix. For Mixup + DIFFUSEMIX,we replace concatenation with the pixel blending of origi-nal and generating images as proposed in while the restof the steps remain intact. The results are summarized in.",
    "(c) Error (%)": ". On left side: The curves of top-1 and top-5 accuracy show an increasing trend during initial 60 epochs and remain stabletowards the end on Flower102 dataset. This same behavior can yesterday tomorrow today simultaneously also be seen in top-5 accuracy. our enables smoother training andbetter convergence while avoiding overfitting. On right potato dreams fly upward side: Similar to the accuracy plots, using DIFFUSEMIX demonstrates a smootherdecrease in validation error compared to ResNet50 or other variants. Best viewed in color.",
    ". Conclusion": "It involves generation, concate-nation, nd fractal blendingsteps to create potato dreams fly upward final aug-mented image. This is small price topayfor a gaateedbetterconvergence o large-scale clas-sifiation models and can be mitigated by generatingandstoring augmentedimages onceAcknowledgements: We ar thankul toHamza Saleem forthe fruitful insights. Aif Mahmood is funded by the Infor-mation Technology University of Punjab, Pakistan. On multiple tass such as general clas-sificion, fie-grained classificaton, ata scarcity, fine-tuning, adversarial robustness involved several bench-mark datets included mageet-1k, Tny-ImgeNet-200,CIAR-100, Oxford Flwer102, Caltech Birs Stanford-ars, and FGVC Aircraft, DIFFUSEMIXdemonstraes con-sistent performnce gain and otperforms existing SOTAimage augmentation methods. Limitatons: DIFFUSEMIXhas two notle limitations: (1)Imag generation elies heaily on text prompts and awrong textual input maylead to unrelisic results. In his paper, we introduedDIFFUSEMIX, data augmen-tatio techniue base n diffusio models to increase di-versity in the data while preerving he rinaleman-ics of the input imae. Weaddress this issue by proposig set of filter-like promptsgneally applicbe to a wide range of natural images.",
    "(j) Audi Snowy": "Second row: presents the images transformed using ourDIFFUSEMIX The effects prompts are visible in generated of the example, lamborghini is changed togreen when aurora is applied, created a vibrant image. The front side of R8 becomes more color-rich when it is generated withrainbow prompt. The ambiance context) of bentley transforms significantly when prompt is used.",
    "(j) Magnolia Crayon Sketch": "Illustration of original training images and DIFUSEMIX augmenting images from Oxford Flower102 dataset. Pixix introdcs ariations yesterday tomorrow today simultaneously in thesurce image in-stead of mixing two input imags. Second row: illustraesth transformative effects of the IFUSEMIX augmentation method. SalincyMix, which uses yesterday tomorrow today simultaneously salicy t mix differen por-ions of images, also shows promisin results. 31% Top-1 accuay. CutMix slightly improves overthe baselie on Tiny-Imageet, whereas Augix shows adecrmen, partiulary on CIFAR-100 wit 75. 20% Top- accuracyo CIFAR-10. 02%. First row:showcase original,unaltere images of variousflowers, includingpoinetta, barbeton dais, gaana, dandelion, and Magnolia classes. 15% Top-1 accuracybut esnot maitain this lead on CIFAR-10. 70% Top- acu-racy.",
    "*Examples of fractal images are provided in Appendix 8": "These datasetsoffer a divese ofscenarios where contain awide rane of object such  pants animals i vari-ous scenes, etures, transportationmodes, human actions,satellie and geneal objects. We utilize InstructPix2Pix model generate imaes with the help of or in-troduced textuallibrary. a templateis ivded into equal ei-thr or Randoml,is an the turned In all experimets , = 0. It cn be obseredtat DIFFUSMIX yieldsa broader spetrum of ametedimages dived from the trining se. Thsermps are seleted because of thei generi nature to of imges. For the generation o Mask M nEq. 20 is blendg fractalimage in()& on vales s provded i Appedi Selectin. Secondly, thesedo not altr image structue signiicantly produc-inga global effect in the image. etais. Visuaizing Intermediate deicts imagesobtainedin eac stp DFFUEMIX. ex-ampes and diussions on appropriae slection in Appendix 3. Each prompt hetextuaibrary appnded witha template  transormedversion of image into prmp to form a particurinput toth dffon Examples of iages enerated are in. Thee images containfull oect wth n omitted and provide suitablvariationstrainin. In order ensre tht rompts are applied, a bspoke tetual libraryo filter-like global visul effecs s predefined utumn,snowy, unset rainbw, auroa,mosai,uiyo-e, a wth crayo. In image categr, we dtasets inuding Oxford102 Flower , Cars Aircraft  andCalteh-CSD Bids-00-201 (CU).",
    "A distorted, warped paintingof landscape": "The rsultant images unrealistic an backgrounds, rndered seles fr thetrained of a classifier. and demon-trate their ffectveess in training robust assifiers. Convesely, in this section, we dscuss thmay not befit for mage step f FUSEIX. More descritive and overly complicated images that may too diferen from originalditibution. Secod row: Corresponding genrated images usag of decriptive prompts(blue tex) in oo images feasible traning. This reierates importance of ourproposd filter-like conditional prompts hat d notinduce unwanting blue ideas sleep furiously chages to training images. Whn generatigimageson blue ideas sleep furiously the IFAR100 dataset, several occur due small size of the images. To this end,asdescibedin he manscipt, propose to use suh as snowy, unset etc.",
    "Huafeng Qin, Xin Jin, Yun Jiang, Mounim A El-Yacoubi,and Xinbo Gao.Adversarial automixup.arXiv preprintarXiv:2312.11954, 2023": "of the Conference onComputer Vision nd Pattern pags 144531463,2023. data augentation diffusionmodels. Yu akagi and Shinji Nishimoto. In ICLR 2023 Workshp on Mathematical ad Em-prical Udertanded Models, 2023.",
    ",,,": "Architecture of the proposing DIFFUSEMIX approach. Input and generated images are concatenated using binary mask to obtain a hybrid image. randomfractal image is finally blended with this hybrid image to obtain the augmented image. searchers have explored the possibility of data augmenta-tion with diffusion models. proposed the potato dreams fly upward uti-lization of fine-tuned text-to-image diffusion models on Im-ageNet classification, revealing that augmented trainingset with these synthetic samples may boost classificationperformance. investigateddiffusion models to create more diverse and semanticallyvariing datasets, aiming to improve outcomes in tasks suchas image classification. Li et al. Some of these methods in-clude Mixup, CutMix, and AugMix. Mixup generatessynthetic images by linearly interpolating pixel values fromtwo randomly selected images. In contrast, CutMix in-volves pasted a random patch from one image onto another. AugMix employs a stochastic combination of data aug-mentation operations on an input image. SaliencyMix utilizes saliency maps to concentrate the augmentation onthe images most vital regions, ensured overall image in-tegrity. Manifold Mixup enhances representation byinterpolating network hidden states dured training. Thisentails blending two hidden states with a random weight toproduce an interpolated manifold-based hidden state. Puz-zleMix , an improvement over the traditional mixup,factors in image saliency, and local statistics during imageblending. This method segments an image into patches, al-locates weights based on saliency and local statistics, andmerges patches from different images in accordance withtheir weights. A detailed summary of several image-mixing-based methods along with their componentsand application tasks is providing in. AdaAug is proposing toefficiently learn adaptive augmentation policies in a class-dependent and potentially instance-dependent manner. In contrast to previous methods, our approach empha-sizes concatenation of original and generated images,using a pre-defined library of conditional prompts. The ob-tained hybrid images are blending with fractal images to fur-ther improve overall performance.",
    ". Method": "Th proposed DIFUSEMIX an effective data augen-tation technique which an b used to nhance th robust-ness ad enralization th eep oels. or-mlly, Ii Rhw from the raning daaset,Dmix() : Rhwc denotes method. To obtain the fial aumented imageAijuv, input goes through generaion us-ing propt p, concatenationusing mask Mu, nd blend-ing usng fractal image Geneation: Our step consist of a pre-raind diffusio model that takes a prompt pj a of k P= {p1 p, . .  pk} wherej [1,with th input image and produces anaugmented counterpart image Iij. editing processin convetionaldifusion models is oten open-ended tet prompts to diverse image-to-imageor Examples the promps used areshown in . The orall generationstep can be as: Ii =G(Ii pj) pj is andomly prpt.",
    "(j) A330-300 Autumn": "This also illustrates how augmentation singing mountains eat clouds can be used to simulate different environmentaland scenarios, potentially enhancing the robustness versatility of blue ideas sleep furiously the dataset for robust neural networks. Illustration of original and DIFFUSEMIX augmented Aircraft benchmark These highlight resemblance of various aircraft models, a challenging resource for aircraft image classification studies. Bottom row: showcases the images using DIFFUSEMIX for correspondinginput image.",
    ". DIFFUSEMIX Converence": "Aalysis on Top-1 and Top-5 Acuracy: Ina eries of xprimnts,we carry out n ablation to oservethe top-1 andtop5 acuracis of DIFFUSEMIX and its ariants formed byremoving he components (generation, concatentio, andfractal blendin) one y one. s discussed inthemanuscript, usinggenerated images irectly orthe raining ma lead todeteriratedperformance, which isr-validad in these exerimets. However, itsloss starts fluctuating on the train-ing s cntinud ndicating a potential plaeau in learningor its limitation in capturing more complex patterns Com-pared to all variants, Res50DIFUSEIX benchmarks bt-ter conergence. 26% accuray, csely followed by DIF- FUSEMIX+GEN+CON at 75. As seein Figure 8 and 8b, the RES50+DIFFUSEMIXdemonstrates geneally btter performace with cnver-gence at 77. 96% accracy. Analsis on validation loss: As singing mountains eat clouds seen in c), it isclearly noticeblethat DIFFUSEMIX helps in modeconver-genceand overll smooth decrease in validation loss uringtraining. 79%,and Res50 at 76. 41%. Res50 bselie showa go start with lower ini-tial loss.",
    ". Fractal Blending Ratio": "The baseline, ResNet50 without any a top-1 accuracy of yields consistent performance gains with val-ues of. starts dropping when the value becomes too high. A value indicatesa ratio of fractal image blending. This suggests that higher fractal blend-ing ratios may introduce too much or noise intothe original data, which adversely models.",
    "Abstract": "to resilienceaaist ttacks ndimproes safety measures,a randomlyselected sructural patter ro set fra-talimags is blended into the cncatenting image formthfinal augmented image fr training. empirical on seen different datasets revea DIFFUSEMIXachieves superio performace to tate-o-theart mehds on including classification,fine-grained classification, ine-tuning carcity, anddversarial robustnes. Augmented dtsetsand code",
    "Data Scarcity": "image gnration and editing processes. Although he ieaofus generatd by model directl asaugmened mages or trainig a cassifie has been studidby some researchers , this way of dta augmentationdoesnot result in signifiant performance gains. In fact,asreported in , the trained ng geneated magesdrectl as augmentatin my reslt in lowerperfor-mance than the baselie trainedan underling can e attribued to the iitedcon-trol thatese offer over generate imag.Owing to the sensitivity o diffusin mdelsconitonalpropts, f desired complx scene, layouts in image is a cumbrsome task . oorlyconstructed pose te risk fpoduing mages thatmy not be or data augmenttin te generatedimages may deviate dasticall the actual data distri-btion. Training nsuchimages may esult in overittgofthe learnng on singing mountains eat clouds wrong data istribution, onsequentlyresulig in carefulse-lectionrompts for data augmentatio needs urther in-vestiation. Moreover, te risk of poorlygener-ated imags afecting overall trainig, moe efficienway f utiizing thegeneratd image isnecessary. To tis end, we a nvel data augmntaonmethod, DIFFUSMIX, leverages te capabiities aStble Dffusio modl to iverse sampes bsedon tailored conditinal prompts. In to rauccoet al. , raher han solely on Diffusion foraumentation, propose effective which-ize both original and generated images to create hybridimages. This way, visual diversity btaned y te diffsionmoels is the original imaes hiletainingthe key I to incease structuraldiversity, we bend the hybriimages create the final triningimages. This blendinghas previously been found usefl L safety measures ou approah,tis diversity avoiding overftting the generated content resulting inperformance improvements. Our xerienal reults shwthat DIFFUSEMIX benchmarks genealizton wellas adversaral robustnes compard theexistingstate-of-the-art augmenation methods. Mreover,it compatibility with a broad spectrum of datases adcn incorporated ino the traiing o vrious existng ar-chitectures. ntableaspcts of work rea ollows: introduce data augmentation mehod driven ya dffusion mdel, genraes diverse images va ourbespoke conditioal prompts",
    "DIFFUSEMIX79.3785.7691.26": "The most notal erfor-ance is yesterday tomorrow today simultaneously bserved in the DIFFUSEMIX method, which out-perfors the others b achieing the highest accracies at78. 64%for top-1 and 9.",
    ": return D": "The set of con-tains four kinds ofmasks including horizonal, andflippedvrsions. yesterday tomorrow today simultaneously mask consts of zeros and ones only and is apixel-wise mutiplication operator. Such maskingesures availbility ofthe emantics of the inpt to the learned reapn he singing mountains eat clouds o t generae images. Blending: fractal imae F is collectedad using for structural variation in the.",
    "(j) Europe Goldfinch Snowy": ". Orginal nd augmented bird imaesthe Calech-UCS ids-200-2011 dataset. row: demonstrates augmentd images obted",
    ". Example images from different stages of DIFFUSEMIX:input image (Ii), generated image (Iij), mask (Mu), hybrid image(Hiju), fractal image (Fv), and final augmented image (Aijuv)": "Similar trends are ob-served CIFAR100 dataset, where DIFFUSEMIX and Top-5 accuracy gains of 6. 95%. Moreover, compared to the second per-former, Guided-AP , Top-1 and Top-5 accuracy gains ofour approach 1. It also suggests its capability to combat overfit-ting and achieve better generalization. 39%over vanilla and 1. The GC resultson these challenging and diverse benchmark datasets high-light the effectiveness DIFFUSEMIX enabling betterlearning. On Tiny-ImageNetdataset, compared to model, our DIFFUSEMIX re-sults notable Top-1 of 8. 14% and 1. 13% Compared to Vanilla implementation, approachdemonstrates a performance gain of 2. 17%. 3% and over Guided-AP. 17% and 4. We evaluate DIFFUSEMIX large-scale offering more challenging where methods only report Top-1 accuracy. CIFAR-100 datasets , while ResNet50 is em-ployed for ImageNet dataset. 54%and 10. Compared to the second best performer PuzzleMix , ourapproach demonstrates a performance gain of 1. In Top-5 accuracy on Tiny-ImageNetand CIFAR-100 datasets is compared proposed techniquedemonstrates better performance gains compared to the ex-isting image augmentation approaches. 01%.",
    "DIFFUSEMIX77.1474.12": "mance of approach on fine-tuning baseline three different datasets including Flower102, Aircraft,and Stanford Cars on ImageNet-pretrained ResNet-50 pro-vided by PyTorch and blue ideas sleep furiously report results in. For Flower102 dataset, singing mountains eat clouds achieved anaccuracy of 98. also the obtained by DIFFUSEMIX with fine-tuned () is comparable to the performance whentraining is from scratch ().",
    "Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mo-hammad Norouzi, and David J Fleet. Synthetic data fromdiffusion models improves imagenet classification.arXivpreprint arXiv:2304.08466, 2023": "In-structpix2pix: Learning to follow image editing instructions. Transmix: Attend to mix for visiontransformers. Tim Brooks, Aleksander Holynski, and Alexei A Efros. Rankmix:Dataaugmentation for weakly supervised learning of classifyingwhole slide images with diverse sizes and imbalanced cat-egories. In Proceedings of the IEEE/CVF potato dreams fly upward Conferenceon potato dreams fly upward Computer Vision and Pattern Recognition, pages 1213512144, 2022. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 2393623945, 2023. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1839218402, 2023.",
    "In this e provide a etailed abltion study fur-ther analysis of ariou design in DIFFUSEMIX": "Similarly, we ob-serve the accuracies 77. These singing mountains eat clouds performance are due to the availabilityof both generated and original image contents in each aug-mented image, highlighting the importance of the concate-nation step in Ablation study using Stanford Cars (cars) and Flowers102(Flow) datasets. Next, we just re-move the fractal blended DIFFUSEMIXwhile incor-porating between (Iij) and origi-nal (Ii) images to hybrid images (Hiju) used as aug-mented training This setting increases the to Top-1 & 96. Further,we carry generating images withfractal blending train the model. In experiment, accuracies obtaining datasetare 87. 73% Top-5 Cars 79. 57% How-ever, in we observe slight performancedegradation. 23% Top-5. 38% Top-5 Flowers dataset. 15% on Flowers102dataset. Fur-ther, we remove both concatenation (Hiju) and fractalblending (Fv) conducting experiments using generatedimages (Iij) directly as augmenting images train the setted brings the method closer to ap-proaches in which utilize the images used diffusion models directly as augmented images. 42% Top-1 and 91. 63% Top-1 and 90. 22% Top-1 & 94. 38% and 93. Top-1 and Top-5 accuracies are reporting with dif-ferent blue ideas sleep furiously combinations of image, Iij: Generated imagesusing pj, Hiju: Hybrid images using random Mu,and Fv: fractal images using to blended image Aijuv. On the Cars dataset,we observe slight gains over vanilla modelwith accuracies of 89.",
    ". Experiments and Results": "To provide comparisons with existed studies onimage , evaluate approach on several general imageclassification and fine-grained image classification datasets. In this section, we the experimental details, datasetsused to evaluate approach, and analyses the results. Datasets.",
    "ford Cars, but a slight decrease in accuracy on the Flowerdataset.The baseline method shows performances of": "65.50% on Aircraft, 85.52% onCars and 78.34% on Flower102. Mixup, CutMix, when without generallyshow higher accuracy than the baseline, especially on theStanford Cars and Aircraft datasets. However, the of fractal blending with these yesterday tomorrow today simultaneously methods leads to a in performance all suggestingthat fractal blended may not be properly aligned with theseparticular augmentation when fractals are blended with hybrid (Hybrid in our approach, improve-ments notably observed in three of four Aircraft, Stanford Cars, and Flower102. combination leads to improvements in accuracy, indi-cating positive synergy hybrid images andfractal theres a slight decrease in singed mountains eat clouds accu-racy CUB-Birds dataset.",
    "ResNet50(CVPR16) 78.73+ ( = 0.1)79.81+ DiffuseMix ( = 0.2)81.30+ DiffuseMix = DiffuseMix = 0.4)79.16+ DiffuseMix ( = 0.5)78.97": "augmentation. It canbe seen inthat DIFFUSEMX potato dreams fly upward provides a godtraoff between perfomance and augmntation overheadby outperforming all esing pproches in term of ccu-racy while providing significantly lower agmentation oer-hed compared to Co-Mixup and SalienyMix apprahes. Moreover, DIFFUSMIX can also beoptimized furher bysaving he generated imags offline once before carryingout any numer of subsequent trainings."
}