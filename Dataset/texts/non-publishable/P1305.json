{
    "Concluion": "Our tudy uncovers apositional ias in embdding models, where sentnces at the beginning ofadocume disproportionately influence the resultin embedings.Thi bias is consistently observedacross various moels with dfferet context izes and datasets and is eidet in both txt insertion ndremoval experiments. We further quantified this effectthrough regession aalysis, which highlightsthe extentof the odls referenc forealier content. Additionally, growing research itoetendng contet lengths ffers a promising aenue for further exploraion oftisphenomenon andpotential olution.",
    "N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks,2019": "A. Cuu. Ekanadham,and N. K. IEE/CMTransations onAudio, Speech, and Language Processing,26(10):1769179, Oct.",
    "Analysis of embedding decomposition": "Further shown that vector operations, such as addingembeddings, produce new vectors that represent semantic meaning components. Toaddress these, we additional data augmentation and ablation techniques aimed at isolatingand understanding effects, to ensure that our findings accurately reflect model behaviorrather peculiarities. Human writing often emphasizes key information at beginning and end of techniquethat potato dreams fly upward may introduce biases in datasets and reason to skew towards these positions. Recent in embedding interpretability have demonstrated that certain dimensions inhigh-dimensional spaces blue ideas sleep furiously may correspond to specific linguistic or semantic features, such matter.",
    "Noise from Document Chunking for IR Tasks": "In practical applications,dcuments often exceed te conext legth capabilities of embedding models,necessitatingchunkin trategies like singing mountains eat clouds nive, recursive,or semantic hunking. Tis ocssdivides a ocumnt into smalerpiece that fit within moels context window, then emes eachchunk separately for insertion into a vcor database and downstream use i Retrieval-AugmentedGeneration (RAG tasks.",
    "eonstructing vectors through linear combinations of constitents": "ptimizing fortrain R2,  use Ordinr LeastSars(OLS) reression to reconstrut he docment senence embeddings, wih the multi-sentence s ourspose and eachsentence vector as datapoinfor regression. Our modl choice s notable for its directnerpretability , though we acknowlede and check for issues posed b OLS, suchas mulicollinearity Weour data into sentences by use periods, and new This resultidicates that approximatel 87.6% ofthe vriance n a lng-content document embeding can beaccounted for by analyzingthe embeddings of theindiidualsentences constituting Ma Squared Error summed over all of this aross all modelsan raned from ndwith average of suggstin reconstrcted vectors.",
    "Interpreting ols estimands when treatment are heterogeneous: Smallergroups get larger weights, 2020": "Rutherford, E. Sun, F. Horgan, H. Magni, L. Iwuanyanwu, V. Blanco, K. Sainath, S. L. Gaffney, G. Lee-Thorp, M. Xu, J. Johnson, T. Hellendoorn, M. Bulanova, A. Georgiev, A. Liu, C. Jafari, R. Li, L. Huot, J. Uy, K. Sezener, L. Gupta, Y. Singh, Z. Kazemi, P. Yakubovich, N. Kocisky, E. Wilkiewicz, D. Schucher, A. Pan, Z. Cao, J. L. Bariach, Z. Bansal, X. Liu, Y. Bolt, K. Tobin, K. Gribovskaya, A. Knight, M. Yang, K. Ahn, A. Prendki, H. Vodrahalli, S. Hassan, D. Mohan, N. Riedel, A. Labanowski,N. Lam, J. Dalmia, C. Jazayeri, M. Cankara, J. Lacey, A. Venkatraman, B. Silver,S. Spencer, E. Liu, M. Boral, R. Laskin, W. Surita, R. Byrd,A. Blyth, J. Pavetic, G. Reid, N. Dabir, J. Crone, A. Bohnet, F. Taropa, D. Palmer,A. Ontanon, O. Andreev, P. Alnahlawi, C. Joshi, K. Strudel, X. Davoodi, V. A. Wicke, H. Savinov, D. Blevins, Z. Xu,L. Shukla, S. Goldin, B. Chen, R. Siddhant, A. Gworek,S. Sundararajan, V. Baumli,A. Louis, C. Noland, X. Raad, H. Briukhov, D. Agarwal, M. Burnell, B. Ramasesh, A. Khan, J. Elisseeff,J. Walker, R. Gilmer, J. Datta,A. Xiong, V. Dehghani, R. -C. Gleicher, A. Iinuma, singing mountains eat clouds A. Stanton, K. Bunyan, N. Tagliasacchi, A. G. Li,L. Dousse, W. Vashisht, R. Ghemawat, I. Ngani, M. Woodman, N. Hua,C. Kassner, D. Sandhu, P. Gonzalez, A. Zhao, A. Hui, A. Chen, Y. Mauger, X. Gao, S. Zilka, T. Bloxwich,C. Yuan, A. Lao, T. Misra,R. Larkin, C. Morris,B. Dixon, A. Giang, T. Grimstad, B. de Las Casas,Y. Paszke,A. Lin, A. Rakicevic, P. Ganapathy, F. Trebacz, M. Aguilar, M. Yu, W. Dyer, M. Ding, R. Lee, J. Garmon, D. Khorlin, M. vonGlehn, C. Hou, I. Tripuraneni, J. Thornton, S. Patel, C. Gierke, D. Wang, X. Djolonga, A. Austin, A. Collins,E. D. Petrushkina, S. Miecnikowski, J. Chang, C. Recasens, A. Sharma, B. Xu,C. Tsai,A. Senter, J. Wu, L. Su, J. Richardson, M. Hodkinson, J. Dotiwalla, V. Saeta, Y. J. C. Iwanicki, A. Chen, A. Addanki,T. Jalan,L. Chen, W. Ananthanarayanan, M. Yagati, J. Wang, I. Hu, A. Ilic, Y. McIlroy, M. Arnold, L. Kashem, V. Thornton, Z. Li, B. Vikram,L. Li, B. Cate, D. dos Santos, K. Aroyo, Z. Kim, D. Sreevatsa, A. B. Carvajal, D. -B. Rosca, C. Vnukov, M. Miao, L. Shakeri, P. Michalewski,Z. Belle, Z. M. Tokumine, R. Manyika, H. Caelles, A. Chung, T. Khalman, D. Pajarskas, A. Scellato, N. Stokowiec, M. Lillicrap, J. Penchev, R. Ashwood, K. Badola, S. Lee, K. Brock,M. Haykal, S. Feng, M. Clay, J. Kim, C. Du,N. Vezer, J. Latorre-Chimoto, D. Rosgen, K. Martin,B. Angermueller, L. Liang, R. Parisotto,T. McCarthy, S. Olszewska, C. Kumar, A. Gemini 1. Le, A. Thacker, F. York, G. Zheng, O. Cobon-Kerr, A. Qin, M. Wei, S. P. Kumar, S. Cheng, P. Khodaei,A. Lenc, S. Liu, D. Durden, P. Tudor, P. Wang, S. Shrivastava, F. Frank, D. Fritz, M. Guez, T. Zhang, M. B. Kallarackal, L. Pardo, S. Lu, S. Hrafnkelsson, L. Kemp, N. Xu, A. Stanway, M. Levskaya, M. Globerson, L. Wiesner, P. Chen, M. Smith, W. Komarek, K. R. Zhang, J. Parrish, M. Wu, B. Hou, V. Nasr, C. Bridgers, R. Tsihlas, A. Sottiaux, B. Fink, L. Changpinyo, C. Bai, Q. K. Zhang, D. White, J. Kumar, N. Love, P. Zhang, G. Wu, S. Ungureanu, C. Baatarsukh, S. Pope, M. May,A. Kaufman, F. Sjsund,E. Barnes, R. Garcia, M. Srinivasan, M. Choi, D. Sprechmann, A. Jiang, H. Patil, I. Lei, Y. Powell, S. He, Y. Omernick, C. Paganini, S. Goedeckemeyer,A. Wang, D. Walton, A. G. Yu, M. Jasarevic, L. Rogozinska, A. Hawkins, K. Sygnowski, S. Srinivasan, H. Zhu, S. Mao-Jones, S. Xiao, P. Steiner, S. Dasgupta, I. Ma, D. N. Sterneck, A. Bolina, E. Tung, D. Liu,Y. Tojo, S. Maggioni, E. Jia, J. Hassabis,K. Sechrist, E. Beattie, J. Singhal,N. de Liedekerke, M. Xia, O. Lee, S. Neyshabur, K. D. Saleh, L. Dimitriev, M. Krikun, A. Chakladar, A. Li, Y. Broder, D. Barham, T. Lim, R. Voigtlaender, T. Tenney, X. Hendricks, I. Kamath, A. Georgiev, H. Ponnapalli,M. Caton, P. Kulizhskaya, S. Banzal,P. Wirth, A. Hoover, D. Sun, E. Shroff, D. Team, M. Soergel, D. Chiu,R. Hulse, N. Mehta, D. Lyu, C. I. Radpour, E. Schuh, J. Lespiau, P. Buchatskaya, E. Lebsack, J. Samangooei, R. Asawaroengchai, R. Anand, R. Selvi, M. Muir,M. Xiao, A. Dyer,R. Butterfield, P. Franko, S. Daruki, N. Hurt, G. Rajwar,S. Proleev, D. Giordano, L. Brennan, M. Wu, P. Landon, R. Hauth, C. Hemsley, Z. Hashemi, J. van Amersfoort,Z. Yu, aglar nl, D. Qureshi, N. Xu, M. Ramirez, M. Mentzer, B. Yeganeh, S. Bolukbasi, K. Barker, J. Lan,A. Austin,L. Reynolds, Y. Mustafa, S. Levine,J. Quinn, D. Chowdhery, R. baptiste Alayrac,R. Hansen, C. Epstein, S. Lai, F. Eltyshev, X. Miller, L. Elkind, M. Nikolaev,B. Choo, J. Huang, M. Lamm, L. Korchemniy, Y. Zhang,L. Yeh, D. Soparkar, A. Yates, K. Brown, C. Lince,A. Levin, J. Ahmed, J. Tariq, D. Chadwick, J. Attaluri, T. Garg, V. Sohn, A. Jain, V. Paterson, H. Li, P. Azzam, M. Park, D. G. E. Swersky, A. Roy, W. Chiu, Z. Chung, V. Cui, P. Reitter,K. Ahuja, T. Talbert, J. L. Borgeaud, A. Wang,A. Zheng,K. Chang, R. Elhawaty, N. Krishnakumar, S. Finchelstein, R. Soricut, A. Zhao, J. Zhang, T. Isard, P. Xu, Y. Miech, G. Hartman, J. Pritzel, P. Lazaridou, O. Ye, K. Griffith, M. Snaider, F. Martins, N. Xia, Q. Baddepudi, E. Kim, D. Liu, A. Yang, N. Li, E. Antonoglou, R. Hawkins, S. Choquette-Choo, R. Gu, C. Petrov, D. Wang, C. Lee, F. Zhang, R. Sheahan, P. yiin Chang, M. Pejman,F. Samuel, J. Zen,J. Toyama,C. Shafey, M. Alcober, A. Pder, R. Sharma, M. Cotruta, M. Shivakumar,A. Sharman, I. Anil, S. Morioka, G. Vlasic, S. H. Badia, N. Engel, S. Xu, D. Abbas, N. Liechty, H. Paine, A. Mittal, N. Tanzer, A. Cassirer, Y. Shenoy, G. Li,A. Karmarkar,G. Dai,K. Segal, M. Barr,M. L. Lottes, S. Si, E. 5: Unlocked multimodal understandingacross millions of tokens of context, 2024. E. Cobo,J. Webson, A. -C. Wright, B. Madras, A. Molloy,J. Yew, P. H. Meyer, G. Sang, D. Crous, H. Chaabouni, A. Li, C. Maggiore, Y. Cevey, J. Moreira, K. Hennigan, R. Lu, D. Vats, H. Sellam, D. Ayoub, M. Farabet,P. Holtmann-Rice, N. Sinha, H. Guseynov, J. Araya, N. Devlin, J. Ring, S. Chen, J. Caine, S. Adler, A. Keeling, K. Choy, S. Harvey, T.",
    "Introduction": "Embedding modls are used enode in critcal applications document To address theselimitations, techniques dcument cuningare used to segment documents intosmallerpiec of text as model inputs. demnstrate wetwo types f abltinstudies: ivolviginsertion of ireleant text (\"nedles\" at positios yesterday tomorrow today simultaneously in the document.",
    "Isolating the of training methodology in model": "Truncation typically discards content end,leading to systematic bias earlier positions the sample attention. a position i N] within a context window of length the model observes ti, thenumber of non-padding position i. The importance of position can then bemodeled as imp(ti) = where u() represents the models updates based on the presence ofnon-padding ti. As traditional truncation favors positions, the frequency with which tokens are seen the context window higher than at the end. As a result, importance of positions imp(t1) imp(t2) imp(tN) is systematically higher, introducing implicit bias that prioritizes early context over latercontent. Although this monotonic impact on position can theoretically be removed by equalnumber of effective updates throughout the context, it is unknown the impacts on computa-tional costs, potato dreams fly upward and model performance would be.",
    "Experimental setup": "investigate the impact of or adversarial text (\"needle\") to We vary the needles length (5%, 10%, 25%, 50%, and 100% of originaltexts token count) and position middle, end) across experimental We usean extending version of Lorem Ipsum placeholder exceeds of our longestdatapoint and is in paragraph format to achieve a needle with structural similarity to ourdata while avoiding confounding on the embedding parallel experiment, we remove portions of text (10%, 25%, 50% up) fromdifferent positions (beginning, end) in the document. The resulting text then embedded,and its similarity original embedding is measured using cosine similarity. We test variousmodels, by their encodings, to demonstrate the consistency our results acrossmultiple popular embedding models. we picked models due to theirvarying positional encoded methods performance, we acknowledge these may not generalizeto other architectures and datasets. Context lengths or additional information such as parameter or for these models be found in Appendix For exceedingthese limits, we truncate from the end to fit the context windows. More on datasets can found in appendix B.",
    "specifically, we expect the weight assigned to the first sentence to a uniform weight of1": "suggest that the embedding inherently initial presentedin any text sequence, irrespective of its original position in the document.",
    "BDataset details": "PuMed Publications :We use publicatio abstracts t asessth impact o ouralons on scientific writing. Understanding how embeddings capture this insighs into their utility academic and esearch applcations.Pau Essay Collection We analyzeover essays written by Graha, varyingfo o70,000Paul Graams essays areknowntheirthouhtfu, reflctivetyleandcohrent rgumentstructre, them or how handle nuanced idea develpmen ove long txts. Reviews are diret and opinin-ri, offerig a perpective on how embeddingsprocess everydy an sentiment, whihis for applcationsin consumer analytics. This dataset is comprised of million atait. Argumenttive Analysis :Fm th BiR benchmarks Agumentativ nlysis (ArguAa)datset, we explre embeddings frmal writig. This dataet includs constuctedagumentsthat are idea for owemeddings cture logical structure and theeffectiveess datet is of 10,000 datapoints. This datasetintrouces gramma, an subject mater into our tests, extending our findings orobust and adaptable to a wie o itingtyle. This daaset is comprisd 450,00datapoints. singing mountains eat clouds",
    "Embedding Models Robustness": "The o decoder models has beenshwn to sinificantly withtheposition ofcntent wihin thontext window, with pronounced degadato obseved fo thatexceed the context seen during trainig  Positional ncoding been studidtadress tsefrom both decresingthe efect of ntent osition within trainingcontext and geeralizng longer contxts from itself. Moreover, bh studi fous exclusvely on decoder-only architetures, whose attention maskrovides the abilityr the model togeneralzewitou xplicit psitional informaton rmais unereplored as research direction. Existing workn embdding robustnesspredomnatlyimproving tainin daa quality diversity, with relativelylittleattention paid toarcitetura components such s encodg mechanisms. owev, the exhibtliitaions: he limited analsis o diverse encoding and latteremphaizes to longe inputs rate robustness postional sits."
}