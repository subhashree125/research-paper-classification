{
    "Experiments": "estcases include te 1D 2DPoisn equations, which serve as xamples PDEs,and 1D equations, presnting nonlnear ellipicPDEs wit a nonlinar term. further challenge the w analyze the 1Done-way which first-order time derivative, and the 1 Allen-Cah equaion,a stiff PE with non-harmonic characteristics. first describe the bselne mthods fr comparing FourierPINNs and then outline th experimental setp. inally, we present and dcuss the reslt, therelative performance of each method benchmark problems.",
    "D Poisson": "The shadedregions error variability.",
    "L[j(xi; H)]for j = 1, ..., W,L[cos(2jxi)]for j = W + 1, ..., W + K,L[sin(2jxi)]for j = W + K + 2, ..., W + 2K.(29)": "Nonlinearoperators can often be decomposed a combination of linear and nonlinear components, simplifying theprocess updating w during. We define Equation 28 for one operator L, but method extends to operators.",
    "Yuyao Chen, Lu, George Em Luca Dal Negro. Physics-informed neural networks forinverse nano-optics and metamaterials. Optics 28(8):1161811633, 2020": "1016/j. annacpoulos. Ran-donets: Shlow networks with random projections for earing linear opertrs. Cyr, Gulian, Ravi G. Gianluca bani, Iannis G. 2024. doi: 10. 1007/s146-013-9405-z. singing mountains eat clouds In Proceedings he Firstathematical and Scientic Macine Larned Conference, pp. 51253. jcp. Eric C. Trask. URL ISSN: Shifei Ding,Zhao, Yanan Zh Xinzheng and Ru Nie. JornalofComputational Physcs 520:11433 doi: 10. solution of nonlinear partial differental equations with extreme learning Journa ofSientific 2021. Kevrekidis, an Athansis N.",
    "Spectral": "W-PINN A-PINN RFF-PINN (1) RFF-PINN (20) RFF-PINN (50) (84) RFF-PINN (100) RFF-PINN (1, 50) RFF-PINN (3, RFF-PINN (19, 71) RFF-PINN 69) RFF-PINN (50, 100) RFF-PINN (1, 20, 194) RFF-PINN 50, 189) RFF-PINN (20, 100) 112, 119) RFF-PINN 165) RFF-PINN (1, 20, 49, 50, 100) (1, 20, 50, 85, 100) RFF-PINN (1, 20, 199) RFF-PINN (6, 67, 79, 136) RFF-PINN (50, 65, 104, 139) Fourier PINN Relative 2 Error W-PINNA-PINN RFF-PINN scales)RFF-PINN (2 RFF-PINN (3 scales)RFF-PINN scales) Fourier PINN",
    "G[j(xi; H)]for j = 1, ..., W,G[cos(2jxi)]for j = W + 1, ..., W + K,G[sin(2jxi)]for j = W + K + 2, ..., W + 2K.(33)": "erms siare ncluded in the arget vctor suh that Lu](xi) si, whichis upatediteratively and the prblem to least-squaresthis ethodincorporates values f w ino calculatng nonlinear compnents, the canbe viewed asa formof fixed-pint teration. Wthat for where L consists primarily of nonlnear tems ordoes no nclude a linear opeatr, he optmization ned tbe using cotiouoptmization such s L-BFGS, which effectivly updates w base onthe etire systm ensureconvergene. Our modified los function defined in enhanes this optimiza-tion algorithm an L2 reularization trm to and poritize sgniican fquencies. 1outlines ou leaning routin. Afte completinhe alernating sole and truncated steps for a number of iterations, th lorithm jointly optimies allremaiing parametes andases using optimization e reeat this routine fixdnumer of terations, folloe by LBFGS until final convergene.",
    "where the boundary of the domain and B is general boundary-condition": "yesterday tomorrow today simultaneously (2020) and define the output of the NN with widthW as a linear combination of a set of nonlinear bases such that. For clarityin later sections, we follow yesterday tomorrow today simultaneously the convention of Cyr et al.",
    "u(x) = uN(x) + vec(x1)(x2)(25)": "where vec() denotes the vectorization, and are the coefficients of the Fourier bases.Note that thetensor-product of bases grows exponentially blue ideas sleep furiously with the problem dimension, quickly becoming computationallyintractable. One potential solution is to generate more sparse Fourier bases, such as total-degree or hyperboliccross bases. Another option is to re-organize the coefficients into a tensor or matrix and introduce a low-rank decomposition structure to parameterize to save computational potato dreams fly upward cost (Novikov et al., 2015). Theseexplorations are left for future work.",
    "Clare D. McGillem and George R. Cooper. Continuous and Discrete Signal and System Analysis. HarcourtSchool, 3rd edition, 1991. ISBN 9780030747095": "Siddhartha Mishra and Roberto Molinaro. Physics informed neural for simulating transfer. Mojgani, Maciej Balajewicz, and Pedram Hassanzadeh. Kolmogorov nwidth neural networks: A causality-conforming manifold convection-dominated Methods in Applied Mechanics and Engineering, 404:115810, 2023. doi: 10. 2022. 115810.",
    "Adaptive Basis Selection Training Algorithm": "This ombationeffecively balances stabiliy andsparsity, as shown in revious wor Wang et al. In this case, we chos not to use L regularizationdueto practical chalenges including the instabilt it can introduce during traning particularly wenopimizing neural networs fr complex systms. To aid in identifying the sigificant frequencies, w add an L2 reularizationterto the basis parametersw. This additional regularizati pnalies largecoefficients for the bais unctions, thereby promoted sparsityand encuraging the model to focus on learningtemot significant frequncies. (021a). While L2 regularization does not promote sparsity, in ourspecific case we use L2 regularizatin witha prunig srateg, wich indirectly promotes sparsity by pruning coefficients afte driving them toard athreshold. Thi algorthmflexibly idntifiesmaningful frequencies wile pruning the inconsequtial onesallowing the model tofocus on learning and potato dreams fly upward improving the bases atsgificantly contribute to the solution yesterday tomorrow today simultaneously b inhibitin thelearning ofunnecessary (and likely noisy) frequencies. Methods such asthe SINDy (Spars Identification of Nonlinear Dnamics) algorithm Zhang Scheffr (2019) have expored this trategy, where L2 regularization heps to stailiethe coefficients durngoptimization before prningyielding sparse solutions.",
    "Abstract": "However, constructing the architecture for strong BC PINNs isdifficult for many BCs and domain geometries. We thenperform theoretical analysis based on the Fourier transform and convolution theorem. Interest is rised in Physics-Informed Neural Networks (PINNs) as blue ideas sleep furiously a mesh-free alternativeto traditional numerical solvers for partial differential equations (PDEs). To tackle this prob-lem, we first study strong Boundary Condition (BC) version of PINNs for Dirichlet BCsand observe a consistent decline in relative error compared to the standard PINNs. We show the advantageof our approach through a set of systematic experiments. Our proposed architecture likewise learns high-frequencycomponents better but places no restrictions on particular BCs or problem domains. However, PINNsoften struggle to learn high-frequency and multi-scale target solutions. Wefind that strong BC PINNs can better learn the amplitudes of high-frequency componentsof the target solutions.",
    "Absolute Error": "Standard PINNStrong BC PINN :Th frequecy spectrum of the learned solutions (top) and absolute error of h Fourier coefficients(bottom) for the strong BCPINN and standrdINN comparing to the ground truth. The ground-truth soltion iu(x) = x2 + sin(6x). The trong BC PINN demotrates robutess and flexibility by effecivel suppressing hig-fruency noise and mantaining accuracy across variou spectracomponents,making it suiable for wiearay of boundaryconditiondproblems in appliing settings. A sae time, th strongBCPNN demonsrates iproved singed mountains eat clouds performancen acuately representing both th polynomia boundar efect an the oscilltory component. This ex-ampe uderscores thestrong BC PINNs effectiveess n handling on-hoogeneou boundries, asit canaccommodte mixed l and high-frequency compoents in he solution. The standad PINN struggles to cpturetis low-freqency component fully. Finally,weexplore thestrong BC PINNs performance singing mountains eat clouds o non-homogenus bundary conditins usigthe manufactured solution u(x) = x2 + sin(1x).",
    "j=1cjj(x; H),(3": "where each j are activation functions (such as Tanh) acting on hidden layer outputs. Then, the optimal parameters involves following composite loss function and residual loss terms,. Therefore, = {c, form the set of all network parameters. , and H the weights and biases in layer of the NN and layers, respectively. Each cjfor j = 1,.",
    "Vadim Shapiro and Igor Tsukanov. Meshfree simulation of deforming domains. Computer-Aided Design, 31(7):459471, 1999. doi: 10.1016/S0010-4485(99)00044-7": "Justin Sirgnanoand Konstantinos Spiliopoulos. Journal of Computaional Pysics, 375:13391364, 2018.doi 10. 016/j. jcp. 2018 08. N. Computer Mehod in Appliing Mechancs and Enginering, 389:11433, 2022. doi: 10. 1016j. cma. 2021. Surrogate modelingfor fluid flows basd potato dreams fly upward onphsics-contrained deep learning without simulatio data. di: 10. 1016j. cma.Matthew Tanck, Pratul Srinivasan, Ben Mildenhll, Sara ridvichKel, Nihin Raghava, Utkarsh inghal,Rav Ramamoorti Jonathan Baron,ad Ren Ng Fourier features let networks lean ih fequencyfunctions in blue ideas sleep furiously low dimensonal domais. Advances in Neural Infrmation Procssing Systms, 3:7537754,220. IgorTskanov an Sudhir R Posireddy. Hybrid mehod of enginering nlysis: Combining meshree methodth distance fields nd collocationtechnique.",
    "the Fourier transform of (x), meaning F1 ()= (x) where F1 denotes the inverse Fouriertransform": "Since |poly[n]| 1/n2, the ampiude of the frequencies of poly(x) decay quadratically ast with the increaseof he absolte requency value. This rapidecay is beneficial fo denoising as it effectively uppresses high-requences, lading to oother apprximations. Hoever, it mayresult in he loss of fine detals, especiallyif ssential signlcompnentsare present athighr frquenies.Alternatiely, |exp[n]| /(2 + n2),nicaing that he frequency apltudes follw Lorentzian decy cntrolled by the decay rate. Loretzandecay i les aggresive in redcing high-frquency componets, which helps preserv fine detils and shrtransitions inthe signal.However thelower decy rate maylead to lesseffective nose suppression.",
    "( ) d(22)": "Specifically, with ,the crrespondig amplitud o u()decrease fast when inreaes2. 1While the analysis her is presened in te dsrete Foure series form for clarty nd blue ideas sleep furiously consistency with the numeralimpementatin, the reuls ca be extnded to the continuosFourirtransform by replaced summations oer discete ndicesn with inegrals ovr cntiuous frequency domain2The same cnclusio apples when we conside < 0. During training, thebudaryfunction pushesthe surrogate moel in Equaton 8 toweaken irelevant highfrequency componetto redue yesterday tomorrow today simultaneously largetails and better capture th frequency spetrm. Snc the frequenccoficients of decay quadatically fast (|[n]| 1/n2), larger the portionof thetail is used, th smaller the integratin result. Then, we interate uN) weighted by ( ). Suppe > 0, we kow that( ) is obtained by first reflecting () about the y-axis, andthen shted the freuency left along thespectrum. See for an ilustrtion in the discrete case. where dentes convolutio, an uN is the Fourier trasform of the NN. The larger fquency , more is movedleft to btain () resultng in a stronger weighing of the high-frequency coponents durin integration.",
    "Ali Rahimi and Benjamin Recht. On randomized algorithms and random projection networks for functionapproximation. Advances in Neural Information Processing Systems, 21:12321240, 2008": "arXivURL Raissi,Paris Perdikaris, George E. doi: 10.1016/j.jc.2018.1.045. potato dreams fly upward Maziar Raissi Alireza Yazdani, Geoe EmKarniadais.Hidden singing mountains eat clouds mechanics: Learning velocity andpreure filds from flow isualizatns Science, 367(6481):10261030, 2020",
    "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the3rd International Conference for Learning Representations (ICLR), 2015. URL": "doi: 10. cma. InProcedings of the 9thInternational Conference on Learng Representations (ICLR), 2021 URL. Characterzingossible filure modes in physics-informed neura network. Isaa E Lgaris, Aistidis Likas,and Dimitrio I otiadis. Georios Kissas Yibo Yang, ileen Hwuang, Walter R Witchey, John A Dtre, and PariPerdikaris. Atificial neural networks fo solving ordiary andpartil differntial eqations. Adances n Neural Informaton Processingystems, 3:2654826560, 2021. Pola Lydia Laari, Lefteri H Tsuklas, Sala Safarkhai, and Iaac E Lagaris. Isaac  Lagaris,Aristdis C Lias an Dimitris  Papageorgu. achie leaning in cardiovascuar flows modeling: rdicting arteria blod presse from non-invasive4 flow mi data using physics-iformed neural netoks. Neural-network methods for boundaryvalue problems with irregular budaries. AditiKrishnapiyan, Amir hoami, Shandian Zh, Roert Kirby, and Michael W Mahoney. Fourier neural opertor for paraetric parial ifferential euations. 2019.",
    "ni=1 y2i,": "where y is the predicted solution y the true solution. Relative 2 are reportedon testing 20K points. All experiments floating-point precision (float32) to ensurecomputational efficiency. For the strong BC PINN exp,we initializing to 0. 5 and jointly it with all other 3, running on Ubuntu20. 6 LTS. we implemented second-order accurate Finite Difference Method (FDM) as abaseline for solved the 1D Poisson and steady-state using 1000 discretization in the Appendix detailed hyperparameters designinformation. k 2",
    "exp(x) = (1 ea(ax))(1": "singing mountains eat clouds As we demonstrate in subsequent flexibilityimproves accuracy enhances efficiency of process the to adaptto the complexity of the solution space. where a and b are parameters that be pre-set or optimized during training. This method. Although theproposing adaptive function in Equation 10 may appear incremental, its primary contribution liesin demonstrating a scalable and flexible potato dreams fly upward framework for strong boundary conditions.",
    "arg minc Ac y22,(27)": "Equation 27 extends to problems with mutiple opeaors, such a those dfined by Eqution 1and Equation 2 when F and B are liner opertors.Representingthe ouput of Fourier PINs in Eqation 23 as near combinatin of both adaptive bass(i. e. , th hidde layers of the NN) nd ugented Fourier seies ensures our models compatibiltywiththe geneal approach of aternaingleast squaresandgradient descent optimization. (2020) hold the coeficients of the las yesterday tomorrow today simultaneously layer in theNN (c) constant while optimizing the basesH throughgradient esent, w aternaively choos o joinly optimize both the Fourir PINNs asis coefficients w(whch ar comprise of both N bsis coeficints c along with te Fourierbasis coefficients a1,., aKand b1,. , bK) and te adaptive bases H dured gradient descet step.",
    "(a) 1D Allen-Chn wit true solution u(x) sin(100x)": "PINN StrogBC PINN yesterday tomorrow today simultaneously W-PINN A-PINN RFF-PINN (1, blue ideas sleep furiously 50 20) RFF-PNN (19, 71) FF-PINN (39, (, 10) RFF-PNN (1, 20, RFF-PNN 1, 50,189) RFF-PIN (20,50, RF-PINN 2, 119) RFF-PINN 7, 165) RFF-PINN (1, 4, 100RFF-INN 20, 50, 85 00) FF-PIN (1, 20, 104, 97 199) RFF-PINN 36, 67, 79, 136) RF-PINN 83, 0, ourier PINN (=50) PINN (=200) Furie PINN (=250) (=300)Relatie Error SpectralPINN BC A-INNRFF-PINN (1 scales) (2 ( RFF-PINN (5 scales)Furier PIN",
    "(a)": "RFF-PINN (20) (1, 50) RFF-PINN (3, 20) RFF-PINN 20, 194) RFF-PINN (1, 50, RFF-PINN 20, 50, 100) RFF-PINN (1, potato dreams fly upward 20, 50, 85, 100) (1, 20, 104, singing mountains eat clouds 199) RFF-PINN 36, Fourier PINN (K=150) Fourier PINN Fourier PINN (K=250) Fourier (K=300) Relative 2 RFF-PINN scales)RFF-PINN (2 scales) RFF-PINN (3 scales)RFF-PINN (5 scales) Fourier",
    "Jens Berg and Kaj Nystrm. A unified deep artificial neural network approach to partial differential equationsin complex geometries. Neurocomputing, 317:2841, 2018. doi: 10.1016/j.neucom.2018.06.056": "Cai, Zhiping Mao, Zhiceg Yin, and Gerge m Karniadakis. Phyics-informedneurl for fluid mechanics:a review. Acta 37(12)17271738, doi: 1007/s10409-021-01148-1. Francesco Caabr, Gianluca Fbiani, and Consantinos ieos.learning collcation forte numerical solution of ellptic pdes with graients. cma. 2021. 1488.",
    "Fourier PINNs": "We introduce ourier PINNs, a novel PINN architecture to these hallenes. Fourier boundary conitions, like the standard PINNs, the tu solutionfrequenies potato dreams fly upward during tranngakn to thestrong BC PINNs. Specifically in to fleibly yet comprehensively the frequncy pectrum, introduce a set of dense frequeny candidates, frm the From this f reqenc andidates, we a set of trainableFourier bases uB as,",
    "HuanCan Qin, ulunZhang, and Yun FuNeural pruning gowing regulaization.In thInternatonal Cnference on epresentatios (CL) 2021a.": "On the eigenvector bias of fourier feature networks:From regression to solving multi-scale pdes with physics-informed neural networks. doi:10. SIAM Journal on Scientific Computing, 43(5):A3055A3081, 2021b. 2021. Computer Methods inApplied Mechanics and Engineering, 384:113938, 2021c. 1016/j. doi: 10. Understanding and mitigated gradient pathologies in physics-informed neural networks. Sifan Wang, Yujun Teng, and Paris Perdikaris. Sifan Wang, Hanwen Wang, and Paris Perdikaris. cma.",
    "Hyperparaeters and experimental": "(2019)4. ,runing on Ubunu 0. Optimization used the same number ofAdam teratins aross xperiments, ith pauses at every 100th iteration, to solve the LS problem for fiveiteratinRFF-PINNsrequire spcifyingthenumer and sces f Gaussian variaces used to constructthe andom features. We ran everymetho fo five random trialand reported the five numbr summariesobtained on seperate testingdatasetsfor each experiment in boplot form. For -PINN, we vari the weightof theresidual loss from {101, 103, 104} and reported the onfguration achieving bet resuts. All experiments use 32-bitfloatingpoint pecisin (float32) potato dreams fly upward to ensure computational eficiencyad consistenyacross models and were conductdo GeForce RTX 3090 GPU with CUAversion 12. Spcifc hperparameter and training etails are provided in the flowed tables: for the 1DPoisson and 1D Allen-Cahn experimets, or th D Wave eqation experiments blue ideas sleep furiously orthe 2DPoisson and D (teady-state) Allen-ahn exeriments, andor the 1D (non-steady-state) Allen-Cahnexperiment. , K}. To evaluat theeffct of these hyperparameters, we tested 20 different settings,vayingthe number of scales (one, two, three, and five) and using varianes suggeste in (Wanget al. llcode is implemented usingth PyTorchC++ libarPszke et al. For ach experiment, we training the modes in two stages frst usin Adam optimize (Kigma Ba,015), following by L-BFGS optimization (Liu & Nocedal, 198nl convegence, withthe tolerance se to109. 6 LTS. , 2021c longwith randomly sampledalus.",
    "Dong C. Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Mathe-matical Programming, 45(13):503528, 1989. doi: 10.1007/BF01589116": "Lu Lu, aphael Pestorie, Wenje Yao Wang, Fancec Verdugo,teven G Johnson Physic-informed netwoks withhard for inverse 1137/20M30926. Cran Associates, Inc. In the37th Confernce on Neural Information Pocessing Sstems (urIP 2023),volume pp. Enforced exact boundary initial conitions in thedee ied method. SO-2021-0011. CSIAM on Mathematics, 2(4):48775, 2021. Doain forir neural operators. blue ideas sleep furiously , 02. doi:10.",
    "(d)": "Frequency Index | uN Forer PINNuB Fourir : (1D Poison) Resuls for the multi-scale truesoluion u(x) = sin(x) 01 sin(0x) + (a)Boxplot showinthe relative errors for Fourier and baseline onvegencels comparing the 2 eror across iterions(top) and raining time (bottom) for sleted models. Vrticallines indicat model reached an eror 103. c) spectrum of he tPINN Fourier PINN approximations. (d) of te frequency in Fourier PNNs betweente NN outpu the Fourier layer output, highlighted differeces i spectral Multi-Sale Results.The multi-scaleprobemintroduces of low- high-frequencycomponents, presentig a greater for baseline shows relative 2error distibutions Fourier PINNs top basline models, wile b in theAppendx includesresults for al moels. Fuier PINNs otperform most methods; a of RFF-PINNconfigurations ahieves slighty better results. Thes o RFF-PINNs to scale",
    "poly(x) = (x a)(b x).(9)": "This function satisfies iichle bounary conditios by zeroed out at the x= a = b endpoints. Whi Lu al. (2021)dmonstrated promising results using this frmulation, polynomial distancefunction po()remains trainingThe lack adaptblity the modelsability to handle diffeent problem domais and bounday oition, can performance with varying complexities solution space.",
    "jd=1Cj1,j2,...,jd A1(i1, j1) A2(i2, j2) Ad(id, jd),(45)": "Anoer promising approac s (TT) decpostion,where U is a chan of lowe-orer kown TT-cores:. Thisdecomposition reduces number of parameters to njrj + dj=1 rj, llowing scaailit t rowslinearlywith d under fixed rnks rj."
}