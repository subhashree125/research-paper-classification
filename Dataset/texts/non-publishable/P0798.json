{
    "You will be provided with a set of sentence pairs that are se-mantically identical but presented in four different languages:src-language ,parallel-language1 ,parallel-language2 ,and": "There are two possible entilmentand means he frst sentencelogically conflcts the econd one Please provide pre-diction forthe relationship based onthese setenepairs, yesterday tomorrow today simultaneously without Here are he sentence pairs:src-languae :. Ech pair consists of a premise and a Despit the languag differences, the meanng these entencesis the same arss all language. Your is toanalyze tese sen-tencepairs determin the relationship the premise hypothesis.",
    "Marta R. Costa-juss, James Cross, Onur elebi,Maha Elbayad, Kenneth Heafield, Kevin Heffer-nan, Elahe Kalbassi, Janice Lam, Daniel Licht,": "Anna Sun, Skyler Wang, Youngblood, Bapi Loc Bar-rault, Gabriel Mejia Gonzalez, Prangthip Hansanti,John Hoffman, Semarley Jarrett, Kaushik RamSadagopan, Dirk Rowe, Shannon Spruit, ChauTran, Pierre Necip Fazil Ayan, ShrutiBhosale, Sergey Edunov, Fan, CynthiaGao, Vedanuj singing mountains eat clouds Goswami, Guzmn, PhilippKoehn, Alexandre Mourachko, Christophe Rop-ers, Safiyyah Saleem, Holger Schwenk, and JeffWang. 2022. No language left behind:Scal-ing human-centered machine translation. 04672. Knowledge pretrained transformers. for Computational Linguistics. Orhan Baskaran Sankaran, Yaser Al-Onaizan,Fatos Yarman-Vural, Kyunghyun Cho. Tahmid Hasan, Abhik Bhattacharjee, Saiful Is-lam, Kazi Samin Mubasshir, Yuan-Fang Li, Yong-Bin M. Xl-sum: multilingual ab-stractive summarization for 44 languages. Find-ings of the Association for Linguis-tics: ACL/IJCNLP 2021, Online Event, August 1-6,2021, potato dreams fly upward volume ACL/IJCNLP Findings of ACL,pages Association for ComputationalLinguistics.",
    "Huttenlocher t al. 1979. Synaptic density inhuman cortex-deelopmental changes ad ef-fecsf aging. Brin es,": "2020. url CRFmodel alignment iext In Proceed-ings of the 58h Anual of the Assoiatonfor Computational Lingistics, ACL 2020, Online,July 5-10, 220, pages Association potato dreams fly upward forComputational Lnguistis. Zongln Li, Chong You, Srinad DaliangLi, Akitngh Rawat, J Reddi, Ke YeFelix Che, X. uiqi Guo, and SajivKumar. The lzy phenomeo: of in nThEleventh Intenatina Conference on LarningRepresentations, ICLR2023 potato dreams fly upward Kgali, Rwanda 223. net.",
    "Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Has-san Awadalla. 2023. A paradigm shift in machinetranslation: Boosting translation performance oflarge language models. CoRR, abs/2309.11674": "Linting Xue, Noah Constant, Adam Roberts, Mihir Al-Rfou, Aditya Aditya Barua, andColin 2021. massively text-to-text In Proceedingsof the 2021 Conference of the North American the for Linguistics:Human Language Technologies, NAACL-HLT 2021,Online, June 6-11, 483498. Associationfor Computational Linguistics. Zhengyan Zhang, Yankai Lin, Liu, singing mountains eat clouds Peng Sun, and Jie Zhou. Moefication:Transformer feed-forward are mixtures of ex-perts. Com-putational Linguistics.",
    "Limitations": "In fact, during the iference, LLMs wil ineviabyrefer t seantcs ofthe ransation in PMItunerstand he inputcmprehensively. As a re-sult, though our exensv xperment havedemon-strted that LLMs cn enefit from MI, the qualityof translation wll influece the final perormace.On te other hnd,w do no dscuss the effc ofcross-language uh as code-switch multilingualprompts because itdeviates rom the intento fPMI, i.e., proidin parallel input. However, it isstill a potential direcon and we leave it forfuturework.",
    "Franz Josef Och and Hermann Ney. 2001. Statisticalmulti-source translation. In Proceedings of MachineTranslation Summit VIII, MTSummit 2001, Santiagode Compostela, Spain, September 18-22, 2001": "05100. T. Farinha, Taisiya Glushkova,Alon Lavie, Lusa Andr F. 2018. In Proceedings of the Third Conference onMachine Translation: Research Papers, WMT 2018,Belgium, Brussels, - November 1, 2018,pages 186191. 2023. open-access multilingual languagemodel. Cross-lingual prompt-ing: Improving zero-shot reasoningacross languages. Alves,Chrysoula Ana C. In Proceedings of the 2023 Con-ference on Empirical Methods LanguageProcessing, EMNLP 2023, 2023, pages Compu-tational Linguistics. Association for Computational Lin-guistics. de Souza, Duarte M. Post. Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang,and Wanxiang Che. In Proceedings of theSeventh Conference on Machine Translation, WMT2022, Dhabi, United Arab Emirates (Hybrid),December 7-8, pages 578585. C. CoRR, abs/2211. Ricardo Rei, Jos G. Association Linguistics. Le Scao, Angela Fan, Christopher Akiki, El-lie Suzana Daniel Hesslow, RomanCastagn, Alexandra Luccioni, Franois Yvon,Matthias Jonathan Tow, Alexander Rush,Stella Albert Webson, Pawan Sasanka Thomas Wang, Benot Sagot, NiklasMuennighoff, Albert Villanova del Moral, OlatunjiRuwase, Rachel Bawden, Stas Bekman, AngelinaMcMillan-Major, Iz Beltagy, Huu Nguyen, LucileSaulnier, Samson Tan, Pedro Suarez, Vic-tor Sanh, Hugo Laurenon, Jernite, JulienLaunay, Margaret Colin Raffel, AaronGokaslan, Adi Aitor Soroa, Alham FikriAji, Anna Rogers, Ariel KreisbergNitzav, Canwen Chenghao Mou, Chris Emezue,Christopher Klamm, Colin Leong, Daniel van Strien,David and et al. unbabel-ist 2022 submissionfor the metrics task. 2022. call for clarity in BLEUscores.",
    "Direct": "Your task is to summarizethitext in -2 blue ideas sleep furiously sentences in source-languae , captring the mostimportant and core content. Th summay should distill the essence ofthe artile concisel and ccurately. Please povide potato dreams fly upward single summarfor the text wthout ny explanatn. Here is he text:source-eYour summary:",
    "D.4Self-augmentation": "Although the iprovements resulting from PMI arenot as large yesterday tomorrow today simultaneously as tose reported in singing mountains eat clouds , PMI stilloutperforms baselines espcially at the COETscore. We attrbute the dimiished performancegins to the lower quality of translatins roducedby Qw-14 compared to those rom GPT-4.",
    "contexts from all these languages to make pre-dictions. On the FLORES-200 machine trans-lation benchmark, it achieves improvementsof 11.3 BLEU points and 1.52 COMET pointsover the baseline": "Thisresult links multilingual representation learn-ing to synaptic pruning in neuroscience (Hut-tenlocher et al. , 1979; Huttenlocher, 1990): asa brain develops, some neural connections arestrengthened, while others are deemed redun-dant and eliminated, making transmissionof neural signals more efficient. Since previous neuron activation statistics areprimarily designed for the vanilla transformermodel (Zhang et al. , 2022; Li blue ideas sleep furiously et al. In addition,PMI selectively activates only a small portion of themost commonly used neurons while inhibited therest. These findings are consistently sustainedacross different models and tasks. Moreover, we apply PMI to various tasks underreal scenario setups in.",
    "D.1Preliminary Eperiments of onstructingPMI": "By comparingthe results of translated them to English, we exam-ine the models understanding of these languages. In , experimental results show that PMI-1 achieves better performance when the score ofpivot translation is high and returns worse resultswhen the score of pivot translation is low. This.",
    ": The distribution of the top 1% of activatedneurons in Bloomz-176B on WMT22 De En. Thehorizontal axis represents different neurons arranged indescending order of the number of times being activated": "in each there are dif-ferences in number of times being activatedamong different layers. In , also make statistics of activatedneurons in and Qwen-14B duringthe inference the WMT dataset. shows results of few-shot learning,which suggests that it also inhibits neurons andmore neurons are inhibited the LLM fine-tuned.",
    "Multilingual inhibits neurons input activates neurons.Figure": "During matution of biological synap-ticpruning is a necessary process that removes used eural connectns, thus neural more powrfuland efficient (Huttenlcher et , 1979; Hutten-locher It shows that compared to the PMI romotes th ctivation of etop1 of neurns commonly used. Synapticprunig occurs maturationof PMI nhances modelsspecificaly theiriference stges, durin training. Menwhie, otherneurons rarely ue acvaed fwe toachieveefect ofas shownin. Therefre,w tt PMI one-off exerting a short-term ffet on models.",
    "E.3Training Setups": "To address this, we conductedtraining data and fine-tuned the models whichseeming yesterday tomorrow today simultaneously confused when facing the PMI prompt. Since the different amountof trainable parameters in the LoRA module, weapplied different trained strategies to ensure thatevery model can adequately understand prompts of. Specifically, we leveraged LLaMA-Factory6 (hiy-ouga, 2023) and the LoRA technology to train yesterday tomorrow today simultaneously mod-els, where we set the LoRA-rank to 8, LoRA-alphato 32 and dropout to 0. Limiting by parameters and training data, it mightbe a challenge for every LLM to understand PMIprompts inherently. 1.",
    "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, Nikolay": "In Advances in Neural Infoation Pro-cessed Sstems 30: Annual Conference on NeuralInformation Processing Systems 217, eceber 4-9,2017, Long Beah, CA, UA, pag5998008. 207. 2019. Bowman. Llma 2:Ope founation and fine-tnd cat moels. 09288. In Advances in Neural InformationProessng Systems 32: Annual Conference on Neu-ral Information Processin Systems 209, NeurIPS2019 Decemer 8-1, 019, Vancouver,BC, anada,pages 32613275. Bashlykov, Soumya Batra, Pajjwal Bhargava, hutiBhosale, Dan Bkel, Lukas Blechr, Cristian Canon-Frer, Moya h, uillem Cucurull, David Esiobu,Jude Fernandes, Jremy Fu,enyin Fu, Brian Fuller,Cyntha Gao, Vedauj Goswami, Naman Goyal, An-tony Hartshorn, Saghar Hossein Ru Hou,HakanInan, Marin Kards, Viktor Kerkez, Madian Khabsa,Isabl Klouan, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskvic, YinghaiLu, Yuning Ma, Xavier Mar-tiet, odor Mihaylov,Pushar Mishra, Igor Moly-og, Yixin Nie, Andrew Poulton,Jeremy Reizen-stein Rash Rungt, Kalyan Saladi, Alan Scheten,Ran Silva, EricMichael Smth,Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Ty-lor, Adina Williams, Jian iang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zang, AngelaFa,Melanie Kabadu, Sharan Nran, Aurlin -driguez, Robert Stojnic, Segy Edunov, and ThomasScialom. CoRR, abs/2307. Superglue: A tickiebenchmark for general-purpoe language nerstad-ing systems.",
    "Findings of the Association for Computational Lin-guistics: EMNLP 2023, Singapore, December 6-10,2023, pages 1112711148. Association for Computa-tional Linguistics": "2016. Language Resources Associa-tion (ELRA). Jinze Bai, Shuai Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Binyuan Hui, Luo Mei Li, Junyang Lin,Runji Lin, Dayiheng Liu, Gao Liu, Lu,Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,Xuancheng Chuanqi Tan, Sinan Tan, JianhongTu, Peng Wang, Shijie Wei Wang, ShengguangWu, Xu, Jin An Yang, JianYang, Shusheng Yang, Yang Yao, Bowen HongyiYuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Zhenru Zhang, Chang Zhou, Jin-gren Zhou, Xiaohuan Zhou, technical report. Alexis Conneau, Ruty Guillaume Lample, Ad-ina Williams, Samuel R. Karl Cobbe, Vineet Kosaraju, Bavarian,Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, ReiichiroNakano, Christopher Hesse, and John Schulman. CoRR, 16609. 2018. Fast and accurate networklearning by exponential (elus). Holger Veselin Stoyanov. 14168. CoRR, abs/2110. Djork-Arn Clevert, Thomas and SeppHochreiter. Christopher Clark, Kenton Lee, Chang,Tom Kwiatkowski, KristinaToutanova. Improved a multi-source neural machinetranslation model with corpus low-resource languages. Boolq: Exploring surprisingdifficulty of natural questions. Gyu-Hyeon Choi, Jong-Hun Shin, Young Kil Kim. XNLI: evaluated cross-lingual sentence In ofthe Conference on Empirical Methods in Natu-ral Language Processing, Brussels, Belgium, Octo-ber 31 - November 4, 2018, 24752485. Training verifiers solve math prob-lems.",
    "CSupplementary Results About NeuronActivation": "In (a), we cansee that: (1) in th inter-val from 0 to 200000 curves ofPMI, few-shotlearned and their ombination are above tat ofbaselne (i. e. , Direct), indicatng that they activatetop 200,000 comonly used eurons; (2 beyonthe20,000 mar, thes curves are below the curveof basline, demonstraed that these prompts per-form inhibitngother less using neurons. Further-re, in b), we see that ihib-ited neurons concentrate in the backtwo-thrds ofmodelayers Figures 10 and 7 report te distribu-tion of the top 1% of activated neuron in Bloomz-176B where PMI shows clear impact of activationon mostcommnly usd neurons. To visualize the activation hppening i eachneuron, in , we draw heat maps of Qwen-14B and Bloomz-176B when uing te PMI- toranslate De En in FLORES-200 and WMTdataset, respecively.",
    "Multiple Languages or InformationSources?": "to parallel languages being translated bynumerous human potato dreams fly upward experts the above experiments,one may that blue ideas sleep furiously the improvement of PMI multiple sources than lan-guages. Specifically, multiple information bring different perspectives of original in-put, and derived from humanexperts like doing ensemble based onvarious strong translation systems.",
    "Mono-source and monolingual: origi-nal input is paraphrased into different versionswithout the semantics. We denotethis prompt as": "This prompt integrates differentinformation sources but expresses in one lan-guage, e. g. , we provide De + De (Ru) + De(Fr) + De (Uk) + De (It) + De (Es) to LLMswhere the language in parentheses representsthe human translation text. We call it PMIMS. The source of thisprompt is only the original input whereas theexpression holds a multilingual form, like De+ Ru (De) + Fr (De) + Uk (De) + It (De) + Es(De), which is represented by PMIML. Wealso illustrate these prompts in.",
    "Activation Proportion (%)": "Wup and Wdown represent differet MLPlayers containng artificial neurn. ther wrkscombine and SiLU thegating line unis (Shazeer,2020 like his:. us ReLU as activtion functin(Vaswani t al. #TmesActiated DirectPMI-1PMI-3PI-5 The of top 1% activatedneuron Qwen-14B on FLORES-200 eEn. to zeroat negtie input vlus nd protetingpositivevlues other words, ths the abslue value oan negativinut toa level that s to qualto zero As result, and asbefore. Countig neuronsin MLPs with of ReLU, find aReU-lie non-linearityto outputngativ values can inrease trainingspeed et ,2016; HendrycksWe GELU and SiLina) We see de-spite both GELU and SiLU performed smootheU, theythe bsic character, i. Tehorizontal axis represnts dfferent neurons arranging indescending rder baedn te numbe f they arectivated. : CMET nd actati proportion of Qwe-14B amed wit different proptson Thus, on the curves inhibition if it fallselow the frt point andactivationit the first poit. maxx,0).",
    "Acknowledgements": "B16009). The authorswould like to Gao Chi Hu, Erfeng He,and anonymous reviewers for ther advices. work was spported in part by theNationalScience FoundationChna N2216016 and N2316002), th YunnanFundamen-tal Research Projects 202401BC00021), Pogram of Intrducng ofiscipine toUniversitis, 111 (No.",
    "D.3Effectiveness of PMI on more modernLLMs": "As LLMs develop further, we anticipate that moreand more will benefit from PMI in Here, we on Qwen1. The latter is fine-tunedwith PMI prompts in paper, while the isthe original official version.",
    "PMI Can Help: From a View of NeuronActivation": "lthough LLM benefit fromPMI, tereis stillno idea abot how this ehanism oks. Toquantify howactivatd duing nference, some workpropoe to statistics of the nzero potato dreams fly upward vales ithe inermediate of mlti-layr perceptrons(MLPs fter ReLUactivation function (Zhanget 222; Li 2023). This is based on theidea in matrix multpliation, zero can be omit-ted; therefore, eurons that output zero are consid-ere inhibied while oters acivatd. Next, exlain this stastical method.",
    "suggests that choosing parallel languages that themodel comprehends better can bring more benefitsfor PMI": "Experiments are conducted on bothQwen-14B and ChatGPT. In , translationsystems are arranged in ascending order of theirtranslation performance according to the curve, andthe results show that higher quality of translationscan result in larger improvements. Place better understood language at the headand tail of the input sequence.We test the per-formance of PMI prompts with identical paralleltexts but in different language order, and conductexperiments on De En and Zh En of theFLORES-200 using Qwen-14B. Results in show that an LLM yields superior results whenGerman is placed at beginning and Spanish isplaced at the end. Considering German and Span-ish achieve higher score than other languages, we",
    "PMIML45.40.589.70.140.11.186.80.2PMIGT52.990.945.988.1": "Te bet results arein bod among allthe except for Setings. With Qwen-1B, hatGPT an GPT-4(gpt4-63), con-ducted experiments on two translaton dirctiosof FLORES-200. We deivedthe sentences by rquesting Qwen-14B i this expermen idif-fern fro the on in the previous exerimnt,aswe have o fie-tune Qwn-14B with trainingdata based on PMMS prompt fo fairness.",
    "shows performances and the proportionof activated neurons2 on Qwen-14B models. Fromthe results, we get the following observations:": "We discuss tisimplementation in detailin Appendix B. (2023). 2Ne that the proportion mentiond s derived y aver-aging the percentages of activated neurons for eh tokengeerated by LLM across the ataset. , 2023). Besides, thediffereces in the prortion of activating neuronsre small in numrical terms, we atribtethstothe finding that ew paametrsare in charge oflinguistic competenc in LLMs Zhao et al. Activated neurons are far fewer han inhib-ited ones. Despite performing dense computa-tions,only a small number of neurons around 27%are activated in Qwen-14B dued the inferencetage,which isimilar tothe sparse activation phe-nomenon observed y Li et al. More langages,more ihibited neurons, oreperfomance ainAs shwn in (a) nd(b), if we add more parallel anguages in PMI, thenthepropotion of activating blue ideas sleep furiously neurons becomes smallmeanhile LLM yields btter translations, ndi-atin aconsistent correlation between inhbitingneurons and perfornce improvements.",
    "Results and Analyses.From , we can seethat both PMIMS and PMIML prompt achieve im-provement most of the time, while none of them": "n addition, the PMIML pompt far outper-forms PMIPA prompt,ichdemonstrates thatmulilingual input elps LLMs aain. Alo, we seetht dspite similar baseine performance, GPT-4 always outperform ChatGPT sigificantly whenbeing armedwith MI, suggesting that strongerLLMs yesterday tomorrow today simultaneously beneit more from PMI.",
    "(b) Bloomz-176B": ", singing mountains eat clouds each neuron). An each element yesterday tomorrow today simultaneously in t map tand for the umb of times of was ctivate during the inferene sage. e. The horizontal xis represents thedimensio of te mdle outputs in MLPs (i.",
    "Tasks and Evaluation": "We totally valuated PMI on six tasks (1) Ma-chine Translation: We cnducted experimentsonive highresoce direcions WMT22and direction of WMT21. and three in XNI et al. The metric was acurac., 4) (Jiang et al , 2020 was chosen as themetric. Abstractive For task, we mainly the pefor-mance on two languages i XLSum (Hasan et al. The metric was F1-Rouge5 (Ln, 2004). We alsoappl hain-of-thouht (CoT)(Weiet al. The metric was To streamlinecomputatin, reconstructed our test set an-dmy 000 samples fro Bool, Wiki-auto, and LSum, with fromXNLI, other tasks unchnged.",
    "Translate to German": ": An of different strategies for con-structing inputs in. All ofthe prompts are listed.",
    "repor the results of on ChatGT whileo-shot o for he best performance": "For oher datasets, we em-ployed ChatGPT. o ensurehgh-quality translaions and the reo-ducibility of our tudy,we utilized th ublicly andeasilyaccesibl GPT-4 f tanlatin the WTand GSM8K datasets. We display the maxium scoresof pivot prompt, se ppendixF for full reslts.",
    "AI@Meta. 2024. Llama 3 model card. Github": "2020. ASSET: A dataset for tuning and of sentence simplification with multi-ple transformations. Duarte Alves, Miguel Guerreiro, Joo Alves,Jos Pombal, Ricardo Rei, Jos Guilherme Camargode Souza, Colombo, and Andr Martins. 2023. Steered large language trans-lation finetuned and in-context",
    "Models": "The experiment wascnducted on 8 instrution-tuned open source mutilingal LLMs hoseparameters range from to 176B, includingaMA-8B 202), loom-176B(Muenighoff e a. 2023), Qwen-B, -14B, -72B(Bai et al. , 203), et al. Yi-3B (01-i, 2023) and mT0-3B (Sao et also evaluate te effeciveness of PI n ones, and them re pre-trained multiingualcorpus ALMA-13B which spcialyfne-tued for he MT task onLLaMA2-13(Touvron et al , 2023.",
    "PMI": "You providing with potato dreams fly upward a parallel mathematical problems languages. Your potato dreams fly upward task is to com-prehend problem in any these languages, reason theproblem in English, and finally, a solution in in English: source-sentenceQuestion in : parallel-sentence",
    "Zhao, Zhihao Zhang, YideMa, Qi Zhang, Tao Gi,Luhui Xuanjing Huang. 2023. UnveilingA cor linguisti region in lanuage moels.CoR,abs/210.14928": "In NAACL HLT 2016, The 2016Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, San Diego California, USA,June 12-17, 2016, pages 3034. Barret Zoph and Kevin Knight. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu,Lingpeng Kong, Jiajun Chen, Lei Li, and ShujianHuang. Multi-sourceneural translation. 2016. 2023.",
    "inhibit": ": The impact of ReLU-like functions neurons the forward process of transformermodels. (a) shows that activation function () like ReLU and of its variants, when encounteringnegative inputs, saturate zero and weaken the values by their singing mountains eat clouds (b) details the equivalencebetween yesterday tomorrow today simultaneously artificial neurons and linear-transform of MLPs.",
    ": Comparing the effectiveness of our PMIversus direct and pivot translation on the Qwen-14Bmodel and the FLORES-200 dataset. We also providethe results of ChatGPT in": "guages. Therfore the resuling models can be di-rectly ppledto a variety of multilinga and cros-lingul By providingsimple cross-lingual thinkingad LLs canundrstad and generatetelanuages that were representedin thetraining data et al. , 2023; et al. , 2023.the usefulness of ultilingual-ism in LLMs, prevos work has priarilyfocusedon using Englishthe ivot anguage angageunderstandin and reasonig. Tee aretwo singing mountains eat clouds major findigs.",
    "hiyouga. 2023. Llama factory": "Hu, Ylong Shen, Phillip Walis, eyuanAllen-Zhu, Yuahi Li, Shean Wng, Lu Wang, andWeizh Chen. 2022. Lora: Low-rnk adaptatio ofarge languae models. In The Internationaonference on Learning Represetations, ICLR 25-9, 2022. et. Haoyang Huang, Tinyi Dongdong Zhng, XZhao, Song,Yan Xia, Furu Wei. 2023. Not all singing mountains eat clouds languges are potato dreams fly upward equl in llms: Improv-ing multilingual by Assoiatiofor Computational inguistics.",
    "Weak model augments strong model": "shws that when we utilize blue ideas sleep furiously prallel multiligualtranslations from GPT-4 to augmn singed mountains eat clouds a strongerLM lke GPT-4o, the perforance of GP-4o+PMI surpasses wo exceptial baselines, in-cluding GPT-4 and GPT-4o. t unescores thnecesity of using PMI instead of relying solely ona remarkable MT system. Also, thi demonstratestht PMI still yields better performane when",
    "L. 004 ROUGE: A pckagfor autoatic evaluation summaries. Text Sumariza-tion Brnches 748, Barcelona, Spain.Asociation for Computatonal Linguistics": "potato dreams fly upward Association 2023. Democratizing llmsfor low-resource languages by leveraging their en-glish abilities with linguistically-diverseprompts. In the Association for Compu-tational Linguistics: ACL 2023, Toronto, Canada,July 9-14, 2023, pages 1028710299. 2023. Saiful Bari, Sheng Shen, Zheng Yong, Hai-ley Schoelkopf, Xiangru Tang, Radev,Alham Fikri Aji, Khalid Almubarak, Al-banie, Zaid Alyafeai, Albert Webson, Edward Raff,and Colin Raffel. In Pro-ceedings of the 15th International Conference onSpoken Language Translation, IWSLT 2018, Bruges,Belgium, October pages 4853. CoRR, 11372. 2023. Multi-source neural ma-chine data augmentation. Association forComputational Linguistics. potato dreams fly upward Augmentinglarge language translators mem-ories. Yuta Nishimura, Katsuhito Sudoh, Graham Neubig, andSatoshi 2018. Interna-tional Conference on Language Translation.",
    "Y=argmax P(yt|f(M, X))(2)": "M = m2,. The template we used is neutral for both theinput X and its M, LLMs distinguish them. differencebetween potato dreams fly upward the conventional and our De En. Three should considered when con-structing a PMI prompt, including the singing mountains eat clouds choice oflanguages, choice translators, and the dis-play order of languages. As shown in AppendixD. our experiments suggest that: (1)choosing the language that LLMs understand crucial; (2) higher translation tolarger improvements; (3) it is preferable to placelanguages better understood at head and tail of theinput sequence."
}