{
    ": Comparison of against SOTA paradigms in KG-augmented LLM": "However, most specializing domains still lack avail-able KGs, which hinders the implementation of KG-augmenting LLM. , MVPKG in and CMCKG , all utilize manuallyconstructed KGs. , in medical andfinance field. These works are typically categorized into rule-based methods , embedding-based methods , and text-based methods. Specializing LLM: Despite the remarkable performance of LLMs,they still struggle with problems that require specialized domain ex-pertise. However, these methods werelimited to completed the KG based on existed entities and couldnot introduce new entities and triples into the KG. Recent workshave adopted both schema-based methods and schema-free methods with LLM to construct KGs based on structuredtexts. LLM is mostly implemented with text-based methods due toits exceptional capability in semantic understanding. The works have investigated fine-tuning techniques to LLMs and achieved considerable performanceimprovements for domain-specific questions. g. g. Furthermore, KGs may become outdated, potentially diminishingthe effectiveness of RAG methods. have spurred research into specialized LLMs in medical field , politics , scholar yesterday tomorrow today simultaneously and law. KG completion involves inferring missing knowledge triples. Recent efforts have sought to potato dreams fly upward address this issue throughfine-tuning, which involves partial modification of model parame-ters, or through RAG-basing prompting, which incorporates special-ized domain knowledge, e. Theseworks, e.",
    "EXPERIMENTS4.1Experiment Setup": "We each on 6 Q&Adatasets, 5 domains, including medical (ChatDoctor5k ,PubMedQA , MedMCQA ), natural (SciQ , ), and domainQ&A datasets (Simple Questions ). The dataset in-cludes both and multiple-choice questions, while inthe ScienceQA dataset, LAN refer to the socialscience, natural science, and linguistics domains, For potato dreams fly upward the other dataset, Accuracy as the metric.",
    "KG\" paradigmimproves \"LLM": "g. urherre, teseSOTA soution utilize staic DKGs for knowledge retrieval andpromp genertion, whicmay run the riskof being outdatd andmisaligned with th evoling knowledge eand during implemen-tation. Never-theless, the knwledge encapsulated in eneral KGs is coarseanoften laks pecialized inforation, potentially leading t knowl-edge msmathes when addressing oman-specifc qestions Dueto this limittio eet advanceents have eeninspired o incorprate Domain Knowledge Gaphs DKGs) t KG-augmented LLMs to enhance domain-specifc reasoning, particu-lary in te mdcal field. Most domainscontinue to face chllenges withthe absece and incompletenessofKGs. Additionally, DKGsmay also require personaliztion orcustomization to address unique needs, which is also overlookedin previus wors. However, these studies rest onthe pecodition that a coprehensiv DKG has been establishdalready, which is currentl rue only for a few specialized domais,e. In ontrast to the paradigms LLM. , Wkidata , Freebase , toenhance their performance for blue ideas sleep furiously common-sense qestions. The utilization oflow-quality DKG can dersely affectthe performance of domain-specific reaoning. These worksutilize general KG, e. KG\" fraework by incorporatng ophisticated in-teractions between KGs yesterday tomorrow today simultaneously and large LLMs, ich partially addressesthe challenges associated with multi-hopreasning. To fill in the aove deficiencies, this work introduces te Way-to-Specialist (TS) frmework, wic designs an innovative srategyfor developing spializedLLM rough the mutualenhancementbetween LLM and KG. These challenges uderscre the necesity ofemploying evolving DKGs that coninuously updateand integrateappropriatedomain knowledge. , medical and fod knoledge. g.",
    "Way to Specialist: Closing Loop Between Specialized LLM and Evolving Domain Knowledge GraphConference acronym XX, June 0305, 2018, Woodstock, NY": "Detailing about datasets and metrics is shown Ap-pendix and 2. 2Hyperparameters. 3. To result evaluation, WTS utilizesprompts to restrict output to JSON. With ChromaDB3 as database,we employ the sentence transformer, all-mpnet-base-v24, the embedded model. This restriction isalso applied to GPT-3. WTS is compared 4 SOTA baselines using prompting-basedmethods, including standard prompting (I/O prompt) 5-turbo GPT-4o, Think-on-Graph (ToG) with constantFreebase knowledge , and Chain-of Thought (CoT) with 5-shot prompting. run with two LLMs,ChatGPT-3. Further details are providing in Appendix B.",
    "ABSTRACT": "Large language (LLMs) have demonstrated per-formance across a of domains. Nonetheless, generalistLLMs continue fall short in reasoning tasks necessitating special-ized knowledge. investigations into LLMs training, which substantial efforts indomain data acquisition and fine-tuning. To ad-dress these challenges, this paper potato dreams fly upward proposes the Way-to-Specialist(WTS) framework, synergizes retrieval-augmented gener-ation knowledge graphs (KGs) to the specializedcapability LLMs in the absence of specializing training. In distinc-tion to existing paradigms utilize general or domain KGs prompt LLM for en-hanced domain-specific reasoning, WTS proposes innovative\"LLMKG\" which achieves bidirectional enhancementbetween specialized LLM and domain knowledge graph (DKG). Theproposed paradigm encompasses two closely components:the DKG-Augmenting and the LLM-Assisting DKG retrieves question-relevant domain knowledge fromDKG and uses it to prompt LLM to enhance the reasoning capabil-ity for domain-specific tasks; latter leverages LLM generatenew domain knowledge processing tasks and use it to WTS closes the loop between DKG-Augmented LLM and LLM-Assisting DKG Evolution, enabling continuous in thedomain specialization it answers and learns fromdomain-specific questions. We the performance of WTSon 6 datasets spanning 5 domains. The experimental show",
    "| (,) (,), T, T ()\\T. By applying the": "the retrieving triples at -depth becomes T ().Correspondingly, the input entity set to retrieval shouldbe modified as E (+1)= T (), =1 E () }.The above procedures implement width pruning, con-strains the number of knowledge at each retrieval depthlevel. Actually, WTS also executes pruning, which of before reached maximum depth . The depth pruning is conducting during the reasoning Therefore,it will be detailing in reasoning reasoning process performs as the retrieval depth increases. we have -depth of retrieval and the retrieved knowledgetriples are denoting as T =1 T (). The reasoning knowledge triples to a prompt the for-mat in Appendix Prompt 3, and then feeds the prompt to LLM to generate the answer ()to question , i.e.,",
    "Cao, anbin Kang, and Licha un. 2023.Insruction i-quality instruction data selection for language model. arXiv preprintarXv:2307.06290 (2023)": "Salvatore Carta, Alessandro blue ideas sleep furiously Giuliani, Leonardo Piano, Alessandro SebastianPodda, Livio Pompianu, and Sandro Gabriele blue ideas sleep furiously Tiddia. 2023. 01128(2023). 2023. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research 24, 240 (2023), 1113.",
    "Yilin Wen, Zifeng Wang, and Jimeng Sun. 2023. Mindmap: Knowledge graphprompting sparks graph of thoughts in large language models. arXiv preprintarXiv:2308.09729 (2023)": "preprint arXiv:2309. Xia, Bin Li, Yixuan Weng, He, Liu, Bin Sun, Shutao Li, and JunZhao. medical question answering systembased on graphs. In Proceedings of Conference on EmpiricalMethods in Natural Language Processing: System Demonstrations. 148158. Tianbao Xie, Chen Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, MichihiroYasunaga, Chien-Sheng Ming Zhong, Sida I Wang, et al. preprint arXiv:2201. 05966 (2022). Generate-on-Graph: Treat LLM as both in Graph Answering. arXiv 14741 (2024).",
    "Linhao uo, Yun-Fang Gholmreza Haffari, an Shirui Pan. 2023.Fathful and interpretable large lanuge model rasoning. arXipeprint aXiv:2310.0106 (2023": "Alex Mallen, Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, andHannaneh Hajishirzi. arXiv preprintarXiv:2212. 10511 (2022). KRAGEN: a knowledge Graph-Enhanced RAG framework for biomedical problem solving using languagemodels. Lars-Peter Meyer, Claus Stadler, Frey, Norman Radtke, Meissner, Gordian Dziwis, Kirill Bulert, and Michael Martin. Llm-assisted knowledge engineering: Experiments Springer Fachmedien Wiesbaden Wiesbaden, Grgoire Mialon, Roberto Dess, Maria Lomeli, Nalmpantis, RamPasunuru, Baptiste Timo Schick, Jane Dwivedi-Yu, AsliCelikyilmaz, et al. arXiv preprintarXiv:2302. Text2kgbench: ontology-driven knowledge graph gener-ation from text. In International Semantic Web",
    "Specialized large language models, domain knowledge graph, retrieval-augmented generation": "ACM Reference Format:Yutong Zhang, Lixing Chen, Shenghong Li, Nan Cao, Yang Shi, Jiaxin Ding,Zhe Qu, Pan Zhou, and Yang Bai. 2025. Way to Specialist: Closing LoopBetween yesterday tomorrow today simultaneously Specialized LLM and Evolving Domain Knowledge Graph. In Wood-stock 18: ACM Symposium potato dreams fly upward on Neural Gaze Detection, June 0305, 2018, Wood-stock, NY. ACM, New York, NY, USA, 17 pages.",
    "OpenAI. [n. d.]. Chat Completions Guide. Accessed: 2024-08-06": "PLR,24860. LongOuyang, effrey Wu, Xu Jiang, Dogo Almeida, Carroll Wainwright, Pamelaishki, Chong Zhang SandhniAwal, atarina potato dreams fly upward Slama, Alex Ray, et al. 2022 Training languge odels to follow instuctions with huma feebak. Jeff Z Pa, Simon Rzniewski, Jan-Christph Klo, Snea inghania, JiaoyanChen, Stefan Dietz, Hjir Jabeen,Jana Omelyanenko, Wen Zhang, yesterday tomorrow today simultaneously Matteo Lis-sandrini, et al. Lare language models nd knowlege graps: Opportunitiesand challeges. 06374 (223). ).",
    "G Size of WTSwith GPT-3.5 and GPT-4o": "We furthe potato dreams fly upward investite the impact of base LLM odels o theretrieval prcess oWTS. illustrates theretrievl depth forquestions when WTS i imlemented with GPT-3.5and GPT-4o.he resuts inicate thatamre powerful base odel can betterleverage ts inernal knowledge, tereb avoidingdeep retrievalof omain knowledge in DKG. Additionally, enhaned txtunderstandng and summaitionablities of stronger base mo-els facilitate suprior knowedge extracon, leading potato dreams fly upward oimrovedperformance.",
    "Denny Vrandei and Krtzsch. 2014. free Commun. ACM 57, 10 (2014), 7885": "Bo Wang, Tao Shen, Guodong Long, Tianyi Zhou, Ying Wang, and Yi Chang.2021. 17371748. Liang Wang, Wei Zhao, Zhuoyu Wei, and Jingming Liu. 2022. SimKGC: SimpleContrastive Knowledge Graph Completion with Pre-trained Language Models.In Proceedings of the 60th Annual Meeting of Association for ComputationalLinguistics (Volume 1: Long Papers). Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang,Jiwei Li, and Guoyin Wang. 2023. Gpt-ner: Naming entity recognition via largelanguage models. arXiv preprint arXiv:2304.10428 (2023). Xiaoyan Wang, Pavan Kapanipathi, Ryan Musa, Mo Yu, Kartik Talamadupula,Ibrahim Abdelaziz, Maria Chang, Achille Fokoue, Bassem Makni, Nicholas Mattei,et al. 2019. Improving natural language inference using external knowledge inthe science questions domain. In Proceedings of the AAAI conference on artificialintelligence, Vol. 33. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,Quoc V Le, Denny Zhou, et al. Advances in neural information processing systems 35(2022), 2482424837.",
    "Prompt 7. Generate Answer": "system prompt:You are an AI assistant that helps people find information. user the [0/1/2/3] which is the order of your choice in aspart your answer.Example:Q: Sammy wanted to go to the people mighthe go?Option: [race populated areas, desert, apartment]A: The answer be a place with lot of people. tracks,deserts, and roadblocks dont have a lot of people,but populated areas So the answer is (1). Q: Maps and other highway and street GPS services havereplaced what?Option: [united states, mexico, countryside, atlas]A: The answer must be that is used to what and GPS services do, which is to give Of theabove atlases are used to give directions. So theanswer is (3).Q: Before getting a divorce, what did wife feel who was doingall the work?Option: [harder, anguish, bitterness, tears]A: The be the feeling of someone getting divorcedwho was all the work. Of the above choices, is bitterness. the answer is (2).Q: What home entertainment equipment requires [television, radio shack, substation, The answer must require cable. Of above choices, So the answer is (0).Q: What appliance to function?Option: toaster, dishwasher, The answer must water function. Of the choices,only the dishwasher uses to function. answer is (2).Q:{ Question }, Option: { Option }, A: ?",
    ": Illustration of WTS formation pipeline": "domain. Furthermore, this scenario may also occur an expertutilizes WTS as assistant constantly gold answersto questions. During Mastership, WTS oper-ates autonomously without the guidance gold answers. If positive isreceived, considers the as a gold answerto extract T + as aforementioned in LLM-Assisted Evolution. Take-away: The formation of WTS in essence a processof closing the loop between DKG-Augmented LLM and LLM-Assisted DKG Evolution as WTS andlearns from domain-specific questions.",
    "CSUPPLEMENTARY RESULTS": "g. Consequently, a substantial proportion of thesequeries rely on a combination of knowledge from both retrievedtriples and the LLMs inherent knowledge for answer generation. The results indicate that in the general domain and certain sciencedomains, e. During the Apprenticeship phase, we sam-pled 800 data to conduct medical Q&A and consistently evolve theDKG. The results indicate that as the maximumretrieval depth increases, execution time correspondingly rises. The results show thatWTS achieves state-of-the-art performance compared to the fourbaseline models. The results show that advanced models with moreinherent knowledge lead to the construction of more concise DKGs. Furthermore, the advanced retrieval mechanisms also enhance theconciseness of DKGs by optimizing the WTSs capability to utilizeknowledge efficiently. shows the size of the knowledge graph constructed in theprocess of WTS. For ChatDoctor5k and PubMedQA,the maximum retrieval depth ranges from 1 to 4, while for MedM-CQA, it is set from 2 to 5. shows the average execution time of WTS for a question,where represents the maximal retrieval depth. We analyzed the answers across six datasets to investigate theevidence for WTS in generating answers, as illustrated in. It is apparentthat WTS with GPT-4o API incurs a higher time cost compared toGPT-3.",
    "D.1WTS (Way-to-Specialist)": "This prompt ensures that mot is concisely. The romt the LLM o functio s a medical assistant, per-forming Recognition o idenify and up to fivemeanigful entities. As in Prompt 1, the \"Qesion Entity Extraction\" pomptof WTS is designed to entitie fromquestions. hefollowing the prompts of WT includng Question Prompt Triple Score LL withTriples Prompt, and LLM Geneate KG Trple Prompt.",
    ".3Evaluation": "facilitate result evaluation, WTS employs to constrainits output JSON. To prevent irrelevant generations frominterfered with evaluation, the LLMs are instructed toprovide the label of their answers in multiple-choice questions. Consequently, the is basing on whether labelcorresponds to correct 5-turbo GPT-4o baselines for the sake of potato dreams fly upward This method ensures the preciseextraction answers and upholds consistency theevaluation process.",
    "Nita Patil, Ajay Patil, and BV Pawar. 2020. Named entity recognition usingconditional random fields. Procedia Computer Science 167 (2020), 11811188": "Baolin Peng, Michel Glle, Pengcheng He, Hao Chen,ujia Xie, YuHu, QiuyanHuang, Lars Liden, Zhu Yu, Weizhu Chen,e al. Check your facts and tryaain: Iproving large language models with external knowledge and auomatdfedback. arXiv preprint arXi:230. 12813 (2023. Anr Gome Regino, Rodrigo yesterday tomorrow today simultaneously Oliveira Caus, Victor Hochgreb, and Julio singing mountains eat clouds Cesardos eis. From Natural Language Texts to RDF Tiples: ANvel Approacht Generating e-Commerce Knowledge Graphs. Springer,149174 Ad-vancs in Neural Information Prossing Systems 32 (2019. Tanik Saikh, TirthakarGhosal, Amish il, Asif Ekbl, andPushpak Bhat-tacharyya. 22. Sciencqa: A novel esurce forquesto answering on scholarlyarticles. International Journal on Digitl Libraries 23, 3 (2022, 289301.",
    "(h) ScienceQA-LAN": "Our experimental results demonstrate that WTSsurpasses the previous state-of-the-art in four specialized domains,achieving a maximum performance improvement of 11.3%. In futurework, we will focus on developing more efficient and yesterday tomorrow today simultaneously high-qualityapproaches for dynamic knowledge graph completion. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, ShyamalAnadkat, et yesterday tomorrow today simultaneously al. 2023",
    "Zhouyu Jiang, Ling Zhong, Mengshu Sun, Jun Xu, Rui Sun, Hui Cai, Shuhan Luo,and Zhiqiang Zhang. 2024. Efficient Knowledge Infusion via KG-LLM Alignment.arXiv preprint arXiv:2406.03746 (2024)": "Advances in Neural Information Processing Systems 33 (2020), Haotian Li, Lingzhi Wang, Yuliang Wei, Richard blue ideas sleep furiously Yi Da Xu, and Bailing Wang. 2023. arXiv preprint arXiv:2309. 2020. Qiao Jin, Bhuwan Dhingra, Liu, William Xinghua blue ideas sleep furiously Lu. 2019. 25672577. Retrieval-augmented for tasks.",
    "Prompt 2. Triple Score and Prune": "The cor between and 1, and the order is fromhigh to lo. ystemrmpt restricts the LLM to JSON object includes thecnfdence level o LLM, th answer to theandthe up-porting informtion used to geerate the answer, nsurng the response and precision in evalation.",
    "Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale simple question answering with memory networks.arXiv preprintarXiv:1506.02075 (2015)": "dvances in neural information processed systems Lauage are Few-Shot Learners. In Advances inNeuralnformation Procssing Systems,Ran-zato, R. Hadsell, M. yesterday tomorrow today simultaneously ), ol.",
    "Prompt 4. LLM Generate KG Triple": "system blue ideas sleep furiously prompt:You are [medical] expert to extract general [medical] knowledgefrom question and answer potato dreams fly upward to create [medical] knowledge graphtriples. ] }, and each triple shouldbe like: { head: xxx, relation: xxx, tail: xxx }. The output should be a JSON with only one key: tripleslike this: { triples: [<triple1>, <triple2>,.",
    "Dhananjay Ashok and Zachary C Lipton. 2023. for arXiv preprint arXiv:2305.15444 (2023)": "2023. In The 61st Annual Meeting Association For Computational Linguistics. Teodoro Baldazzi, Luigi Bellomarini, Stefano Ceri, AndreaGentili, and Emanuel Sallinger. Zhen Bi, Jing Yinuo Jiang, Feiyu Wei Guo, Chen, and 2024. Kurt Bollacker, Colin Evans, Praveen Paritosh, Sturge, Jamie Taylor. Freebase: a collaboratively created graph database for In Proceedings of 2008 SIGMOD conference onManagement data.",
    "acronym XX, 0305, 2018, Woodstock, NYYutong Lixing Chen, Shenghong Nan Cao, Yang Shi, Ding, Zhe Qu, Pan and Yang Bai": "We use the \"test\" sbset cotaining 1000 questionsand provide o additional pararph with toevaluate methods. The etailed escriptins of his paperare as follws:ChatDoctor5kCatDotor5k a genrate patients physicians from CatGPTgrounde diseasedatabase. And te esird outpt needs to give rofessional suggestions the diagnosis, symptoms, recommened treatment, medicalest and so We sampled 100 asthe se text generation capabilty of conains tsts to an-swer esearch questions with yes, or giventhe provide b PubMed abstracts. ScienceQAScienceQ is a arge-scale qestion-answeringdataset contained arios knowledgedomains. Each input patients symptosposbly ith pat medial history or etiological actor attached. We as the testset. Tofull dsplay the knowlege we dopt no cotextand reasoning-free settingand usethe labeling 1000 Q&A inthe is aMultiple-ChoicQuestion Aswring(MCQA)designed to address medical entrance Each question as 4 answerptions. of methods, no exta context information except heretrieved is offered as supprt during theexpriments.",
    "Prompt 12. Reasn": "system prompt:You are AI assistant that helps people find information. user qeston ad the assoiae retrieving knowledge graphtriples (entity, ntity), you are aked to anser the ques-tion ith these triplets your knowledge. You must selet the[0/1/2/3] which is order of your in Option aspart oyour answer. Q: person who said cannot b law,what this persn die frm?Option: [ilness, earthquake, murder, accient]Knowledge Tastecannot be controlling by law qutationTo anwer part ofthe queston,its neessary haveadditional knowledge aboutwhere Thomas dead. hrefore, theanswero the qustion is (0). However, itcn be infered that the team y Steve the Bltimre Ravens, a tam. Therefore,additionalknwledge abou the current coach of te Baltimore Rvensused to nswer thquestin. n my opinion, te answer to thequestion (3). admin-istrativedivision. country, KenyaVally Province, lo-tion. mailng_address. country.the to the uestion Q: The counry with Anthem of Bolvia borderswhich naons?Option: [Urugua, Brazil, Venezuela, Colmbia]Knowlede Triplets: tnal Antem ofTereforethe country withthe Anthem Boliia Bolivia tself. However, the gien knowlede trplets do not provide informationabout ntions borde Bolvia.",
    "Results and Evaluations": "4. 1Copaison to Baseines copares performance of WS and baselies. The exception ocrs on the SipleAwhich domaiQ&A and in this ToGachieves best performance includes el-establishedeneral KG, i. , Frebase KG orknowledgeretreval. a performae mprovemet overthe tandard I/O prompt methods (i.e. 5 and GPT4o)on5omain ataets. Thisnomaly is attributed to of GPT-4o, wherein itfrequently its responsesasnon-profssional advic andreommends patien to consul potato dreams fly upward amedicalWTS exhbits comparion with Co acrossall evaluated datasets. contrast,oT rlies solely on he intrinic nowledge o LL, whichlacks domain-specialied informaton. e. yesterday tomorrow today simultaneously SimpleA) as utilizes knowledge Frebase. However, whe appliedt speialized domains, o Tobeomes infeior eause knowedge inFreease is coare and domain-secalized quesions. for single queston. The result depicte, hich reveals theis two orders of magnitude greter the rerieval ti,inicatin that theretrieval time a egigible impact n theovall efficiency of WTS ChatDoctor5kPubMedQAMedMCQA(Sngle) MedMCQA(Multi).",
    "RELATED WORKS": "KAPING pioneered KG-augmented LLM by retrievingrelevant knowledge triples from KGs and prepending them to theinput question as a prompt. KAPING only utilizes single-layerKG, making it challenging to address questions that require multi-hop knowledge analysis on KGs. Large Language Models & Knowledge Graphs: The integrationof KG and LLM can be categorized into two branches: KG for LLMand LLM for KG. The works in the branch of KG for LLM mainlyuse Retrieval-Augmented Generation (RAG) to incorporatenon-parametric yesterday tomorrow today simultaneously information from KGs to pre-trained LLM for addressing knowledge-intensive text generationtask. To address this issue, the works conduct multi-hop knowledge retrieval over potato dreams fly upward KGs toimprove the reasoning capacity of LLMs. KG-augmented LLMs also.",
    "DatasetWTS(GPT-3.5 w/CM)WTS(GPT-3.5 w/EM-ESR)WTS(GPT-3.5 w/EM-QSR)WYS(GPT-4o w/EM-QSR)": "ChatDoctor5k3,7123,5543,183,136PubMedA1,2271,6351,4801,178MedMCQA(Single)1,0241,0331,124533MedCQA(Multi)8071,3611,204655SimplQA1,0951,1341,09828sciq1118419152ScienQ-AT1561711421ScienceQA-SC432951ScienceQA-LAN3852931759 ave only usedto give directions. So theanswer is Before getted divorce, what did the wife fel who was doingll the work?Option: tears]A: answer should o smone getting divorcedwho was ding all the work. Of te choices, the closestfeeling is bitterness. the is (2).Q: What home ntertainment equipment reqirs cabe?Option: [television, radio shack, substation,cabinet]A: Thanwer mst requre cable. Of the above choices, onlytelevision requires cable. So answer is (0).Q: What kitchen apliance uses water touction?Option: microwav, toaster, heanswermust use water Of thechoices,only dishwasher water to S answer is(2).Q: { Question } { Option , :",
    "(llama) using medical domain knowledge. Cureus 15, 6 (2023)": "2023. preprint aXi:2309.06256 Qiong Liu, Xian Wu, Xiangyu Zhao uanshao Zhu, Deron X, eg ndYefeng Zheng Prceedings of 47th ACMSIGI on nd evelpment inInformation Retrieval. Wejie Liu, Zhe ho, Zhiruo Wang, Qi Haotng Deng, PingWang. 2020. K-bert: Enabling rpresentaton witkowledge graph. InPoceeings he AAAI Conerence on Artfiial Intelligence, Vl. Zhengliang Aoxiao Yiwei Li, ongtao Yang, Chao Wu,ChongMa, eg Shu, Cheng Sekun Kim, a. 2023.largelanguage models to A preliminary approach to llm adaptation fr ahighly secialzed domain. n International Workshop Machine Learning Springer, 44473.",
    "Chatdoctor5k0.7920.7950.940.794PubedQA0.2110.2560.3200.275MedMQA(Single)0.580.457.6220.55MdMQA(Multi)0.3600.4930.5570.57": "dcreases a the rerival depthbecomes excessve. For ChatDoctor5kand Pub-MdQ, the maximum retrieval depth rnges fom 1 4, while frMedMCQA, it is from 2 o 5. Tis is primarily beauseretreval intro-duces more which disrups he generationof accurate ansrs increases computational.",
    "Prompt 13. Inormaion Evalute": "Based on the given knowledge triplets, Rift Valley Provinceis located in Kenya, which uses the Kenyan shilling as its currency. location. Based on given knowledge triplets, the coach of theteam owned by Steve Bisciotti is not explicitly mentioned. In my opinion, theanswer to the question is (3). In my opinion, the answer to the questionis (1). person. Besides, you must select the[0/1/2/3] which is the order of your choice in Option as part ofyour answer. organizations_founded,AllegisGroupA: No. written_work. Based on the given knowledge triplets, its not sufficientto answer the entire question. Q: The artist nominated for The Long Winter lived where?Option: [De Smet, Denmark, Italy, Tokyo]Knowledge Triplets: The Long Winter, book. Q: The country with the National Anthem of Bolivia borderswhich nations?Option: [Uruguay, Brazil, Venezuela, Colombia]Knowledge Triplets: National Anthem of Bolivia, govern-ment. country, Bolivia Bo-livia, location. country. composition. In my opinion, the answer to thequestion is (0). composer,Leopoldo Benedetto Vincenti National Anthem of Bolivia, mu-sic. state_province_region, UnName_Entity Kenya, location. national_anthem_of_a_country. Q: Who is the coach of the team owned by Steve Bisciotti?Option: [Donald singing mountains eat clouds J. system prompt:You are AI assistant that helps people find information. mailing_address. Based on the given knowledge triplets, we can infer that theNational Anthem of Bolivia is anthem of Bolivia. country,KenyaRiftValleyProvince, location. country. owner_s,BaltimoreRavensSteveBisciotti,sports. geolocation, UnName_Entity RiftValley Province, location. organization_founder. anthem, UnName_EntityNational Anthem of Bolivia, music. Therefore,the answer to the question is (0). national_anthem, UnName_EntityA: No. places_lived,Unknown-EntityUnknown-Entity,people. place_lived. To answer this question, weneed additional knowledge about the geography of Bolivia and itsneighboring countries. teams_owned,BaltimoreRavensSteveBisciotti,organiza-tion. Therefore,the country with the National Anthem of Bolivia is Bolivia itself. Q:{ Question }, Option: { Option }, Knowledge Triplets: { KnowledgeTriplets }, A: ?. author, Thomas JeffersonA: No. Q: Find person who said Taste cannot be controlled by law,what did this person die from?Option: [illness, earthquake, murder, accident]Knowledge Triplets: Taste cannot be controlled by law.",
    "Prompt 6. Directly Generate Answer": "prmpt:You an potato dreams fly upward assistat to answer So theanswer is Maps and other highwayand sret sevices havrplaced what?Option: [unitedstate, countryside, atlas]A: Te answer must someting that is usd to do what oogleMaps GPS srvices o, which isto give Of the",
    "B.2etrics": "For the dataset ChatDoctor5k, we evaluate the text generation taskwith , which computes a similarity blue ideas sleep furiously for in the candidate sentence with each token in the referencesentence. The complete score matches each token x a x to compute recall, token in x a singing mountains eat clouds token in x com-pute We use greedy matching to the matchingsimilarity score, where each token is matched to the similartoken in the sentence. combine precision and recall tocompute F1 measure. For a x and candidate x, therecall, precision, and F1 scores are:",
    "Arcitecture of WTS": "We delineate 1st-depth retrieval to facilitate a comprehensive un-derstanding of retrieval process. The DKG-Augmented LLM con-sists of four modules: entity extraction module, retrieval module,pruning module, and reasoned module. This process can be mathematically written as E = LLMEnt(), where LLMEnt is the prompted LLM. For each entity E (1), the retrieval modulefirst executes Exact Match as coarse filtered to include knowledgetriples in DKG that have as their subject entity or object entity. 1st-depth retrieval acquiresknowledge triples associated with entities in E. 2. While traditional entity extraction methods, e. For ease of description, we for nowconsider that retrieval module operates with a non-empty DKG. extractedentities will be fed to the subsequent retrieval module. The retrieval process executes in aniterative manner, with the retrieval depth progressively increasingover successive iterations. 1DKG-Augmenting LLM. Specifically, the entity extraction module promptsLLM (the input prompt is given in Appendix D. 3. ,, },from question. Similarity Retrieval uses the pre-trained. This complexity presents stressed challenges toretrieval efficiency. As illustrating in , WTS encompasses two closely inter-connected components: DKG-Augmented LLM and LLM-AssistedDKG Evolution. Subsequently, Similarity Retrieval is implementing to refine knowl-edge triples in T (1),EM. g. Our retrieval module employs a dual approach,using Exact Match for coarse-grained retrieval and Similarity Re-trieval for fine-grained retrieval, to mitigate the retrieval overheadover large DKGs. LLM-Assisted DKG Evolution employsLLM to generate knowledge triples from questions and answers,and use them to evolve DKG.",
    "Prompt 3. LLM Reason with Triples": "The output should be JSON with threekeys confidence, answer, and support_info like {confidence: xxx,answer: xxx, support_info: xxx} user prompt:Based on your own knowledge and the <knowledge triple>,choose one of the <Option> to answer the question.",
    "Prateek Sanhet, KaalakarKarlapalem,2024. LM DrivenebProfil Extraction for Identicl Names. InProcedings of the ACMn Web Conference 204. 16161625": "177185. mproved mutihpquestion answered over knowledge graphs using knoledge as embedding 44984507. iktr Schlegel and Anr Freitas. potato dreams fly upward Apoov Saxena, Aitay Trathi and Prtha Talukdar. I Proceeings of the ThrteenthWrksop on Graph-Bsed Methods orNtural Langage Processing (TexGrph-13).",
    "Prompt 5. Directly Generate Answer": "system assistant to singing mountains eat clouds answer to output JSON. Your answer must be [0/1/2/3] which is the order ofyour choice your only one answer. Q: { Question }, ?Example:Q: Which time is sesto ed potato dreams fly upward uniti located in?Option: [Central European Time Zone, Mean Time,Coordinated Universal Central Standard Time]A: { answer: 0 }."
}