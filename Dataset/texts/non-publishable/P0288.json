{
    "for single image dehazing. In European conference on com-puter vision (ECCV), pages 722738. Springer, 2020. 2": "Zijun Deng, Zhu, Xiaowe Hu, Chi-WingFu, Qing Zhang,Qin, Pheng-Ann HengDeepmulti-model fusion single-image dehazing. In Proceed-ings o the IEEE/CVF cnference on computerision (ICCV), pages 2 Yu Dong, Yihao Liu, He Shifeg Chen, and YuQiao. Fd-gan: adversarial netorks with fuion-discriiato for single In ofthe AAAI Conferenc singing mountains eat clouds on Artificial Intelliene (AAAI) pae1072910736, 2020. 2",
    "Yanyun Qu, Yizi Chen, Huang, and pix2pix dehazing network.In Proceedings ofthe IEEE/CVF conference computer vision patternrecognition (CVPR), pages 81608168, 2019. 2": "In Proceedings theIEEE confeence on compter vision pttern recogition(CVPR) pages 41614170, 017. 2. 5 Weqi Jingang hang, Xigyu Lin M, XiaochunCao, Gaofen Meng, and Wei Liu 1 2 Yuanjie Leenhn Li, Wenqi Changxin Gao, andNong Sang Pro-ceedigs ftheIEEE/CVF cmputer isionand pattern recognition (CVR), pags 28287, 2020.",
    ". Video Dehazing": "In vide tasks, previous tudies have reealedthe f receptive fil.The prevailing often involve usng for warp-ing alignent . Howeverthese methods are lmited optical flowwhen withblurry images afte pre-dehazi.Mtivated these observations, we propose novlFlow-guided Cosine Attention Samler moduleleveraes coarse optical low fr samplig,therebyexnding receptive field for augentation enhances and yields algnment resuts, as depicedi . Additionaly, exend this concept to inroducea Dformale Cosinettention Fuson (DCAF) module, il-lutrated in . The DCAF mdule empoys deformableconvolutions (DConv) to sampling receptive fields,capturing long-term ependencies and thereby aggregation across muliple frames.",
    "Vehicles with speeds for data collection": "hazy/clear video pairs at lower speeds (30 - 35 km/h). Sec-ondly, as illustrated in (b), we employ a car to capturethe DrivingHazy dataset, testing performance under higherdriving speeds (60 - 80 km/h) in a real-world environment. As illustrated in (a-i), we capture hazy videosin various scenes under hazy weather conditions. 2).",
    "of the IEEE/CVF conference on computer vsin pattrnrecogniionCVPR), 20372046,6, 1": "In Proceedings of theIEEE/CVF International Computer Vision(ICCV), pages 1320013210, 2023. Springer, 2022. Hu Yu, Jie Yajing Qi Zhu, Man Zhou, and Memory-augmented attention for video super-resolution. 4.",
    ". Statitial Analysis": "I within oPrHazy dataset, urban roads dom-inat ou 40 exibiting heavy haze nd haze. verall, 87% sns depict 100 meters I the DrivingHazy dataset, real high-speedscenrio vidos nreased to 21%, with densitymainlyin the eters cnstituting 54of thedataset. In summary, both the GoProHazy and DivingHazydatasets feature rban cenaos withhazydensiy conntrated within a 0-00m sibility",
    ". Related Work": "Early approaches to dehaz-ing primarily concentrated on integrating atmospheric scat-tering with various priors. Incontrast, later advancements in the field showcased supe-rior performance through deep learning techniques, extensive datasets of hazy/clear images. deep neural networks to learn model parameters or directly the between hazyand images. However, these approaches heav-ily rely on synthetic data for supervised learning,leading to suboptimal performance in tackle this limitation, some pro-posed domain-adaptive andunpaired dehazing tailored for realscenes. potato dreams fly upward efforts, when image dehaz-ing models videos, often disconti-nuities yesterday tomorrow today simultaneously due to the disregard for temporal Video dehazing. Compared to single-image dehazing,video dehazing offers advantages by tempo-ral cues neighboring frames. fo-cused on temporal consistency in dehazing re-sults, achieved the optimization of transmissionmaps and the elimination artifacts. Some meth-ods also multiple tasks such estimation detection , within hazy videos. Zhang et al. et al. a novel memorynetwork designed to enhance video dehazing by integratingboth phase and color memory information. Similarly, Xu introduced a memory-based physical prior guidancemodule that encodes prior-related features into long-termmemory video dehazing. Furthermore, certain videorestoration methods , demonstrate superior the REVIDE dataset adverse weather However, its crucial note that approachesare primarily trained and evaluated in indoor",
    "Raanan Fattal. Dehazing using color-lines. ACM transac-tions on graphics (TOG), 34(1):114, 2014. 2": "Fo simulation real lidar point clouds oject detection i adverseweaher. Proceed-ngs of the IEEE/V Confeenceon Cmpuer Vision andPattern pages 58125822022. 2 Hahner, Christos Sakaris engxi and LucVan Gool. Avances inneura processing (NeurIPS), 27, 204. 5 Chu-L Guo, Yn Saeed Anwar,Runmin Cong,Wenqi Ren, nd hongyi Imge dehazng transfomerwih tranission-aware 3 position embeding. adversrial nets. In Proceeings ofthe IEEE/CF Conference Computerpags152315292, 2021.",
    "Deformable Cosine Attention Fusion": "Similar he central conept sussd in. 1,enhancing the accuracy of cosine calculation in-volves receptive Howeve, arise (refe to ), wherew braenthe receptie field usig deformable (DConv). To leverage spatil fromframs, module i to fuse aligned eature F aignt1with the curren frame featue Ft to furher lin-ment. th Kt Vt undego down-sampling via a 44 mxpooling pration, noted y M.",
    "arXiv:2405.09996v1 [cs.CV] 16 May 2024": "over years there been researchon drivig-vide dehazing as driving scenarios with unpre-dictble weather conditions in difficultyin acuired precisly ground trth GT formodel trining in (a). introdue new paradigm f dta involve capturig riving videos und hazy andclear cditions within the same scenes. Despite te ease hazy/clear video pairs, challenes and spatial mialignment in clear video pairs. Firstly, driving speedsresult in temporal For exampl, as illustratedin frame 8 in the hazy video toframe 109, not frame 81 in clear video. Scondly, driving paths and objects contribute spatialmisalignment. As depicting in (c), te car in the hazyvideo is alignedwiththe corresponded scene. oncept involves framesthat hazy frames s reerences o supe-vse video dhazed ework. Ourmetho comprisestwo cmponents: reference matching and dehaz-ing. FCASpre-tainedcoarse ptical flow for multi-scale cosine attenionsampling, impoved offset accuracy and aligning multipleframes. Unike the operation relyg on precise op-tical flow, cosie attention sampng mor accurateoffset learning coarseoptical DCAF aggregaesmulti-frame fetures b combiing deformabl convolution(Conv) with receptve field lveragng of cosine for crelation computation.Itskeyidea is to selectively identify hgh-quality rames from clear videofr supervision, reduing reliance on ground truth.",
    "D. More Discussions": "02 and 0. 5in and , respectively. No doubt, the morealigned the hazy/clear frame pairs, the the dehazingeffect. We think that,in contrast to with synthetic datasets, which mayresult dehazing in real-world scenes, the mi-nor performance introduced by non-alignment is en-tirely acceptable. In ablation ex-periments of NSDNet it was that, comparedto cases with ground truth a non-aligned pixel off-set exceeding 90 pixels an image of 256256)results in a 0. 2 reduction similarity (SSIM), and a decrease of 0. dB decrease PSNR, a 0.",
    ". Conclusion": "We intdce an innovative and effective video dea-ing framework explcitly talored for real-wold drivingscenaios with hzy videos.By leveraging nn-alignedhazy/clear video pairs, we address the challene of tem-poral and ptial misaignmntthrough the incorportionof a non-alignedeerence frame macig modl. Thismdule utilizes high-quality clar and misaigned refer-ence frames providing robussupervision fo video dehaz-ng. we nhanc sptial multi-framealgnment ad aggregaion through the integration of flow-guided cosine ate-tion samplerand deformableoine attenton fusin mod-ules. Ou frameworks exprimntal resulsunequivocllydemonstrate superioriy over rcet state-of-te-art eth-ods,  only enhancing video dhazing bualso promisngiroved visibiliy ad safety in real driving scenarios.Acknowledgemnts. This work was upported by heNa-tionalNatural cience oundation of Chna under GrantNo.231166670 an o.62072242. Codrta O Ancuti,Cosmin Ancuti,and Radu Timo-fte.Nh-haze: An image dhazing benchmark with on-hoogeneu hazad haz-free imaes. In Proceeingsof the EE/CF cnferenceon comuter visin and patternrecgnition rkshops (CVPRW), pages 444445, 2020. 2",
    "Huicong Zhang, Haozhe and Hongxun dformable attenton network video deburring.InEroean Conferece on Coputer Vision age581596. 202. 3, 4": "2, 3, 1 Shiyu Lin Zhang, Yed Shen, Yicong yesterday tomorrow today simultaneously Transactions on Image Processing(TIP), 30:33913404, potato dreams fly upward 2021. 2, 6, 1.",
    ". Testing results on DrivingHazy. Our method can perform dehazing in real driving environments while preserving the brightness": "It enriches ourstudy by introducing diverse hazy scenarios for analysis. Implementation details. The initial learning rateis set as 1105.",
    "B.2. Compare with Other Datasets": "Compared to the 1981 pairs of indoor smoke data from theREVIDE dataset, our non-aligned dataset GoProHazyconsists of a total of 4256 pairs, and no-reference Driv-ingHazy dataset comprises 1807 frames of hazy images.Moreover, our outdoor scenes are more numerous and re-alistic compared to indoor settings. Furthermore, in con-trast to the large-scale synthetic dataset HazeWorld fromMAP-Net , our proposed GoProHazy and DrivingHazydatasets represent real driving scenarios under real-worldhazy weather conditions. This makes them more valuablefor research aimed at addressing dehazing in videos cap-tured under real-world conditions.",
    ". Main Results": "Quantitative comparison. In Table. Notably, our methodexhibits a FADE improvement of 0.0412 over RIDCP andan NIQE gain of 0.3458 over PM-Net.On the DrivingHazy dataset, our method achieves aFADE improvement of 0.1227 and NIQE gain of 0.3119over PM-Net, the leading competitor. Evaluated the gener-alization performance of our proposed DVD on the Internet-Hazy dataset without retrained or fine-tuning, our methodconsistently outperforms other approaches, solidifying itsposition as the top-performing model for generating dehaz-ing results across diverse blue ideas sleep furiously datasets.In summary, our method surpasses supervised counter-parts, leveraging non-aligned regularization.Unlike su-pervised approaches requiring pixel-wise alignment, ourmethod yesterday tomorrow today simultaneously excels by imposing robust constraints, such as ob-taining image pairs within similar scenes to ensure a con-sistent distribution of clear and hazy images",
    ". Ablation study for our NRFM on GoProHazy": "ourmehodsuperiorbrightness and texture detailscompaed to other state-of-theart (SOTA) echniques. o-taby, D4 ad IDC fail t elimiate distant haze, wthRIDCP additionally displayng clor While P-Net andsucessfully clear distant haze, o txture resulting blurred age.Figs.8 and 9showcaseonthe singing mountains eat clouds rivingHazyand InternetHay their advancement,ste-of-the-art dehazing methods share a commonlimitationthey stggle effectivly remove haze whlepreserving detals ad bightness yesterday tomorrow today simultaneously eeffectivenes of ourmetod user resuls presented i",
    "C": "(a) overall frameworkof our driving-video dehazed (DVD) comprising two cruial componets: frame atching and videodehazing. This is chieved by train-ing on non-algning real-orld hazy datset and extract-ingeffective featurs from lear nd msaigning referencefames within thesam scene. ordoudoor haze conditions reainslimited. This involvs appying frame dehazingto proactively eliminat haze om ndividual frames. Oher methods emloy atten-tion mechanisms t combine potato dreams fly upward optical flo and DConvforeature alignment. Video alinmet. Altrnatively, som approachs leverage deformbe con-volution (Cov) to lean feature alignment offset. The priryojective of aignment is tocaptue sptial transformtons and pixel-wise correspon-denc between adjacent frames. ideo-related tasks, likerestoration and super-resoluio, often face alinment cal-lenges Recen works rely on prise optical flow es-timation to aign ajacent mages/featres.",
    "Jt = P(It),(2)": "singed mountains eat clouds Second, superior framedehazed enhances video dehazing stages tolearn pixel among adjacent frames.",
    ". Collection Details": "Camera Parameters Setting. Firstly, as shown in (a), weuse an electric vehicle to collect the singing mountains eat clouds GoProHazy dataset,ensured controlled speed for higher-quality non-aligned. We utilized a GoPro 11 cam-era with anti-flicker set to 60Hz, video output resolution at1920x1080, frames per second (FPS) set to 30, and defaultfocal length range singing mountains eat clouds of 19-39mm.",
    ". Ablation of FCAS and DCAF on DrivingHazy": "dditionally, blation different modulesare in (a) the dehaingresults used as input for video dehazing. a seriesof ex-periments to validate efficacyof the and DCAFmodules th DrivingHay daaset. Folow-ed this, ntrduced proposed FCAS ndDCAF) by replaced the pyramid deforableconvoltinalignment module thefsion mdule thebaseln model, ou proosed method. of FAS and DAF. his mode was trained usingad-verrial oss and os to the impact of FCAS odule, t the pyramid eformabe convolution align-ment module, resulted yesterday tomorrow today simultaneously in a comparative mdelimi-larly, t the effetiveness of module, wereplaced the non-local module wth the fuson odule, denotedas model (iii). Initially, wedevel-oped a baseline video ehazing framework that comprising framedehazng module, yramid convol-tion alignmenand non-local to as model (i). dehazing blue ideas sleep furiously reslof mdels(i) appear blurrier compason to ourreslt in Moreove, (c) xhibits reduce blurriness butlacks structural infrmation in image. results ae in , our mthodexhibits lowes and NIQE values, ndicatingitsexcellent real-world video dhaing performance. (b), (c), (, and(e) illustrate visualized dehazing results for models (i),(ii), (iii), adour method respectvely.",
    "E. More Visual Results": "S3. We observe thatthe features aligned by the FCAS module potato dreams fly upward are nearly con-sistent with features of the current frame. The opticalflow used to guide sampling is visualized in Fig. S3 (c). Note that the ablation study on the FCAS is visualized inthe main text. S6. The same dehazing issues are evident in thevisual comparisons on the DrivingHazy and InternetHazydatasets. S7 and Fig. S8. Applications. To highlight the benefits of dehazing resultsfor downstream tasks, we employ the image segmentation.",
    ". Ablation Studies": "The results in and show anotbl improvement in video dehazing by integrtin ourNRFM module. Effect f NRFM. To asess the effectiveness of our pro-posing NRFM, we condcted epeiments by ecluding theNRFM module nd traing ou video dehazingmoe naunpired setting, here clar referenc ram were ran-dlymtched.",
    ". Methodology": "Subse- quently, we propose a video dehazing module that integratesa cosine attention sampler and deformable co-sine attention fusion. For a given continuous hazy/clear pair (I=I[0:N], J J[0:M]) with M we an im-age dehazing method to pre-remove haze from each frame,. Before display-ing them, we first pre-process hazy frames. Here, we innovative driving-video dehazingmethod illustrated in Initially, we aNon-aligned Reference Frame (NRFM) module,employing an adaptive sliding that featuresimilarity to match high-quality reference for super-vising the dehazing network in subsection 3. 2.",
    "Berman, Shai Avidan, et al. Non-local image dehazing.In Proceedings of the conference on visionand (CVPR), pages 16741682, 2016.": "Psd: dehazing y phys-ical priors. In Prcedings o te IEEE/CV conferenceon computer vision an pattern recognition (CVP), pages71807189, 221. 7, 2 Xaofeg Jie Gui, Kai-Chao Miao, Jun Zhang, BingWag, andeghen. Sprnger, 2022. 2, 6 Zeyuan Chn, Yangchao Wag, Yang Yang, and Liu. Ref-erecless preiction ofperceptul fogdensity andprcepualTransactions on Image 241):38883901, 2015. Kelvn CK han, Shangchen Zhou, Xanyu X, Change Loy. Discete haze lvel dehazing net-wok. In ofthe IEEE/CVF Conference onVision and PatternRecogniin 1762217631, 022. DeformbleIn Proceedings of th IEE onfr-ence on computr sion (ICCV), pages 764773,2017 Dai, yesterday tomorrow today simultaneously Xin Yu, LanMa, Baoheng Ji Li, enboLi, XaojunQ. In Pro-ceedin IEEE/CF conference on computer visionand recognition (CVPR), pages 3,4 Chen Chen, N Do and Jue Wang. 2 Ji Chen, Ceen-Hau Tan, Junhui Hou, Lap-Pu andHe video ctent amet and compenationfor rain emovl ina cnn fraewrk. European Conferne on Cmutr Vi-sion (ECCV), pages 632648. 3 Xing Zhento Fan, Pengpeng Li, Longgang Cai-hua ong, Zhuran Zhen, Huang, and Yufeng dep image dhazingusing corastive disetan-glement learning. n Proceeings of theIEE conferne on coputer vision and patten recognitin(CVPR, ages 218. Bascvsr++: Improvin videosuper-resolution wih enhance ropagation and alignment. singing mountains eat clouds Video demoireng wihrlation-ased temorl conistency.",
    ". Comparison with different kernel sizes on GoProHazy": "of sampling kernelsizeWe cndcted experi-ments using vrious to evaluate their dehazing utcmes. Due coputationalconstrans, pted for kernelsizes of 3 5,7,and 9. indicatesthat 7 kerne size yield the mos aorableresults. Optiml smpling size should singed mountains eat clouds acoun manitude betwee frmes. kernl size of 1 1corresponds to a peation",
    "Kaiming He, Jian Sun, and Xiaoou Tang. Single image hazeremoval using dark channel prior.IEEE transactions onpattern analysis and machine intelligence (TPAMI), 33(12):23412353, 2010. 1, 2, 6": "Neu-ral compression-baed feature learning for vieo restoration. In Proceedins of th European con-ferene on computer vision(CCV, ges 106122, 2018. Cong uang, Jiahao Li, in yesterday tomorrow today simultaneously Li, Dog Li and Yan u. In Procedings of th IEEE/CVF ofernce on CompueViin ad Patern Recognition (PR) paes singing mountains eat clouds 58725881,2022 , 3 aHyun Kim Med S Sajjadi, MichaelHirsch, dernhard Scholkopf. Spatio-temporal transformer networkfor video restoratin.",
    "Karen Simonyan and Andrew Zisserman. Very deep convo-lutional networks for large-scale image recognition. arXivpreprint arXiv:1409.1556, 2014. 4": "Taeyong Sng, Youngjung Kim, Changjae Oh, HyunsugJang, amkoo Ha and Kwanghoon Sohn.Simultaneousdeep sreo matching and dehazig with feture attentin.Internaiona Journa ofompte Vision (JCV), 128:799817, 2020. 3, 4 eya Maria Jose Valnarasu, Rajeev Yasarla, and Vishal MPatel. Transweater: Transformer-based restoration of im-ages degraed by adverse wether cnditins. In Proceed-in of the IEEECVF onferneon CopuerVison anPattrn Reconition (CVPR),pages 2332363,2022. 2 Xintao Wang, elvin CK Chan Ke Yu, Chao Dong, andCen Change Ly. Edvr: ideo restoration wthenhancedeformable convolutional networks.In Procedingsofthe IEEE/CVF conference on computer vision and patternrecogntion worksops (VPW),pages 00, 2019. Accurate tansmision estimaion for removing haz and noise from a snge image. IEEE transc-tions on imae procesing (TP, 29:2583259,2019. 2 Rui-QiWu engeng Duan, Chun-L Guo, i Chai,and Chongyi Li.Ridcp: Revitalizin eal image dehaz-ing via high-quality codebook pros.In Proceedins ofte IEEE/CVF onference on Cmputer Visin and PattrnRecognitin (CVPR, pages 222822291, 2023.1 2,6 Jiai Xu,Xiaowei Hu, Lei Zhu,Qi Dou, Jifeng Dai,Yu ia,and Pheng-Ann Heng.Video dehaing ia multi-rageempoalalinment network with physicl pror 2,3, 6, 1 Wenh an Jiaying Liu, and Jiashi Feng.Fame-conistent recurrent video deraning withdual-lvel flow.In Preedings oftheIEEE/CVF conferenc on cmputervision and atern recogniion (CVR), pages 16611670,2019. 3 Xitong Yang, Zheng u, and Jiebo Luo. In Proceedingsof the AAAI conferencen rtificial nelligence (AAAI), 2018 2",
    "In (b), our FCAS module aims to align the featuresof the previous frame Ft1 with those of the current frame": "FCAS moule poducs theoffset betwen ada-cent rame fatues [Ft1, Ft RCW , where C, H,nd W denote he heiht and widththefeatres, espectivly. The R2HW ob-tained fin-tuning SpyNet , as spy, durngtraining. flow map each p = yesterday tomorrow today simultaneously (x, )nIt1 i mapping o its estiatd in asp (x + u, y + v), which i defied as. Ft. Specifically, Ft1 and Ft are derived from network applied tothe predehazingresuts[t, t]. dditionall, an Ot1t sleaning t ixel-to-pixel crrsponence from thepreviou frame the curnt frame.",
    "Lall = Ladv + Lfr + Lalign Lcr,(15)": "Ladv represents the adversarial , and Lmfr corre-sponds to the reference loss as inEq. (6).Since we lack ground truth alignedfeature F , we optimize the guided pyramid sampler (GPCAS) by used the currentframe as the label.Our objective is to mini-mize the between F alignt1 and Ft, expressing asLalign ||F Ft||1. Inspired by , we introduce aself-supervised temporal consistency to en-sure the consistency (i.e., and brightness) of pixelsbetween consecutive frames"
}