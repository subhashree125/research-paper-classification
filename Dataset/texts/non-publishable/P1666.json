{
    "Information-rich Text Prompt Generation": "For the GSD-related prompts, we output the GSD prompt ofinput image according to predefined GSD level in Sec. frameworkis blue ideas sleep furiously shown in , which consists of three components: GSD-related prompts, annotation prompts,and vision-language model prompts. blue ideas sleep furiously The category information is used for finalmanual text prompt proofreading. , SEN1-2 does not include annotations) exist in original datasets, and then thesatellite type and weather type are extracted as output. For the annotation prompts, wefirst extract annotation contents such as satellite type, weather type, category, etc. 1. g. 3. For the vision-language model prompts, we aim to utilize thepretrained large-scale vision-language model BLIP-2 to output a simple text prompt describinginput image content. We now elaborate on how to generate an information-rich text prompt for each sample. Finally, we combine the outputs of above three components and obtain. that may (ormay not, e.",
    "Information-rich Text Prompt:": "Multi-mdal meansremote sensing imagng content captrd by diffeent RGB image,SyntheticRadar (S) and Near Infrard (NIR). iformtionrich denotes the information-ric textdesription (e. Finally, ith extensive manual refining annottions (i e , ext pompts), we obtai appoxmately2. In additio, we surveythe curentymainstream and repor thei keyproperties stttcallyi To this , propose a Muti-odal, ulti-GSD Multi-scene Remoe Sensing (MMM-RS) dataseta enhmark for text-to-magegeneration in iverse remoescenarios. g , ()). toiject he textualsemanic iformation in each sample conducting generation, utilize model i. W first collect available atasts and al samples to unifrm size. In MM-RS daset provides not only multi-modal pair bualso iformation-rich text prompt ncluding smle text promp cntent, GSD level(. 1 miin well-crafted andinformation-ric text-iage pairs in datase. , resolution), type o weahr an satelle (different colo better observation) To dress the above issues, a straightforwardis to te existed anddvancedgeneraive t traingeneraio and tenthe obtain th dverse R aes ia inptting ext A in (a), a samle th classic RS SICD isa text-image pair with theil text description. th RSICD daaset does models enerat imaes. (b) shos sapl in clssic E1-2 dataset which is aulti-modal image pair includng GB an SAR ima.",
    "end for": "size higher than 512 512, weall images with size of 512 52 as For sampes th the size than512 12, w yesterday tomorrow today simultaneously caculate the minimum L of theeightadwid, then crop image witthe size of L L. Note tha the sizes of the dataset ae great than or equal to 256 56. preseve upsampled the iages, an ESGAN super-rsolutin model the scale factor (denoted as ESRGAN()) the cropped imge. Finay, we to resize the image and outputthe standardize image with o512 512. additin, e needto update t GSD of cording tothe in size",
    "MMM-RS (Ours) (information-rich) (RGB, SAR, NIR)": "RS text-toimage generation odel b fine-tuning off-the-shelftext-to-image diffusion moes(e. , Stable Diffusion ContolNet ) for generated mlt-modal, multi-GSD, ulti-sceeS imges. In summary, he contributios of this work can be concluded s We construct large-scale Multi-modalMuti-GSD, and Multi-scene Remote Sensed (MMM-RS)dataset and bnchmark fr text-to-imag generation in diverse RS scenario, which standardizes 9publicly available RS dasetswith unifor and information-rich text prompts. To prvide the various SD amples, we design GD sampleextaction strtegy thatextractsdifferent GSD levls images for ech smpl and define GSD-elated text prompts describigdifernt GD levels. Furthermore, ue to lack of real-world multi-scenesample, we electsme RGB smples and utilizeexisting techniqes to synthesize sampls wih differen scnesincding fog, snow, and lo-light environments. We use ur proposed MMM-RS dataset to fine-tue advancdStabe Diffuson,ad perform ex-tensive qantitative and qualitativ comparisons to prove the effectveness of o MM-RS dataset. In articular, we usth aligned multi-modasamples (iclding RG, blue ideas sleep furiously SAR, and ifrared modai-ties) in the MMM-RS dataset to train thecross-modal generation models based on ontrolNet,andthe visualization results demonstratesipressive cros-modalgeneration capabilities",
    "Abstract": "To bridge RS images to textual semantic information, we utilize large-scale pretraining vision-language model to automatically output text prompts andperform hand-crafted rectification, resulting in information-rich text-image pairs(including images). , foggy)in single sample. g. Recently, diffusion-based generative paradigm has impressive capabilities with text due to its accurate distributionmodeling and stable training process. paper, we propose a Multi-modal, Multi-GSD, Multi-scene Remote Sensing dataset and benchmark for text-to-image generation in diverse remote sensing scenarios. However, generating sensing(RS) images are tremendously different from general images in scaleand perspective remains a formidable due to of a comprehensiveremote sensing image generation dataset with modalities, ground sampledistances (GSD), and scenes. In we design some methods to obtainthe with different GSD and various environments (e. 1 results verify that our proposed dataset allows diffusion models to diverse RS imagesacross various modalities, scenes, conditions, and dataset isavailable at.",
    "CJ Van Westen. Remote sensing for natural disaster management. International archives of photogrammetryand remote sensing, 33(B7/4; PART 7):16091617, 2000": "Esrgan: Enhanced generative adversarial In singing mountains eat clouds Proceedings of Europeanconference on vision (ECCV) workshops, pages 2018. Di Jing Zhang, Bo Du, Minqiang Xu, Lin Liu, Dacheng Tao, Liangpei in Neural InformationProcessing Systems, 36, 2024. Wang, Zhen Cui, and Yong Li. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Liu, Chao Qiao, and Chen Change Loy. Distribution-consistent modal recovering for incomplete multimodallearning.",
    "FID 172.78175.68168.34347.8892.33IS 6.646.316.892.637.21": "validates theutility the dataset as a valuable resource in the field of generative image modeling. 2. should be noted that to ensure a of model each 500 RS images for the calculation of theabove metrics. Quantitative comparisons. A lower FIDindicates that images by our model closer distribution to images,suggesting and accuracy of the generated The substantial improvements in both metrics to models highlight potential of our tailored approach and varied images, suitable advanced remote sensing applications. Notably, our model FID value compared other models.",
    "Background": "Rmote sensing imaging data are widely using in various computer vision sch as sceneclassifiation , objct detecion,sgmenttion , cangedetection , and RSimagcaption MRSSC2. 0 is remote sensing classfication dataset tha contain7 typial senes such as city, farmland, mountain,etc. In object detectionfield, thelassic dataet consists of1,070 images ship bounding boxes for in RS cenaios. For segmentation task, GID cntains 150 large-size (7200 800) with fine-grainedpixel-leve annotation. RGB, SAR, change decion task,the , and CDD are used potato dreams fly upward to tain a model prdicting changes inthe sam region. this work, yesterday tomorrow today simultaneously aim t proosea MultiGSD, Multi-scene RS datasetand enchmark for text-to-image generation in divere RS scenarios.",
    "Generating Different GSD Images for the Same Sample": "In other words, th datsets cannot allw th model togeneate imageswith different GSs fr th same scene. The main ida f this strategy isto crop images with diffeent sizes (ameheight ad idth) from a lage-size RS image and ensurethat the cropped images of diferent sizehave bvious GSD change. Then, all cropped mages aretandardzed to the size f 512 512,so that the GSD of standardized images cn becomuted asGstd = (L/512)Gori,where Gstd nd ori denote the GD of th standardzedimage and riginalimage, repctiel. L denote the height and width of the crpped imag. In practice, we peformthe above stategy n the Inria datset to generate different GSD imagesbecause its image size is 5000 000 tat is enough to cro images with diffrentsizes. Notethat the higher reolutio images cometelycoer th lower resltionimages, hich nsures that al cropped images maintain consstent scene conent. Finally,with thefour cropped image, we standardizethem o a uniform sie of 512 512, andthe GSDis updatedt 2.4 mixel 1. mpixel 0.6 m/pixel, and 0.3 mpixel, respectively",
    "Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, and Xiang Li. Rsgpt: A remote sensing visionlanguage model and benchmark. arXiv preprint arXiv:2307.15266, 2023": "arXiv preprintarXiv:1802. arXiv preprint arXiv:2312. In International conference on machine learning,pages 1973019742. 15311,2023. Blip-2: Bootstrapping language-image pre-trainingwith frozen image encoders and large language models. International Journal of Applied Earth Observation and Geoinformation, 106:102638, 2022. 07856, 2018. Darius Lam, Richard Kuzma, Kevin McGee, Samuel Dooley, Michael Laielli, Matthew Klaric, YaroslavBulatov, and Brendan McCord. Pixel-level changedetection pseudo-label learning for remote sensing change captioning. PMLR, 2023. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. xview: Objects in context in overhead imagery. Chenyang Liu, Keyan Chen, Zipeng Qi, Haotian Zhang, Zhengxia Zou, and Zhenwei Shi.",
    "Fine-tuning Stable Diffusion for RS Text-to-Image": "Thimethod allows us to explre varous textual nd their impact generated images. Those are a rucial part of ourwork. Ourexpermentthe Diffusion-V1. oevaluate our models performance, we utilize recogized metrics: the Frechetnception Distance (FID) and the Score coduct all experiments using on RTX. text prompt, is rucial for diecting the imagegeneation process, constructing combinaton four componnts: {Ground Distance of weather}, {Simple text prompt image content}, satllie} High precision resolution,snow, asatelite a park potato dreams fly upward in the city, GoogleEarth). 5)as foundational pre-trained mdl. We use learning of 0 0001 and employ the Adam optimizerraning. In the generative our genrative model ndergoes regimen of onour MMM-RS datasets.",
    "Kang Liu, Jian Yang, and Shengyang Remote-sensing cross-domain scene A andbenchmark. Remote Sensing, 14(18):4635, 2022": "L, Binqiang Wang, heg, and Xueong Li. on Geoscne and Remote Sensing,56(4):183295, 2018. Xianped Ma,Qianqian u, Xingyu Zhao, Xiaokang Man-On Pun, and Huang. Sam-assistedremote imagery emanic segmetation with contraints. 02464, 2023. mmanuel aggiori, Yuliya Guillaume Charpiat, nd Pierre Alliez OenAI. Dall-e 3. Iprving atentmodels for image syntheis. 01952, 2023 Dep uderstanding high reslutionremot image. In 2016 Internaional Conference Computer, Information TelecommunicationSystems CITS), Jul 2016. Rbin ombac, Andras Blattmann, Dominik ssr, and Bjrn Ommer. In Poceedings of the IEEE/CVF conference on and pattn 2022.",
    "Multi-scene Remote Sensing Image Synthesis": "Specially, we select samples from sadadized datasettobe used for synthesized ith tre cmmo cenes: fg scene, scen, ad low-lghtscene. The overview framework ofmuticne RS imge shown in.",
    "Yinghui Xiao and Qingming Zhan. A review of remote sensing applications in urban planning andmanagement in china. In 2009 Joint Urban Remote Sensing Event, May 2009": "Jiaxun You, Xiaocheng Li, Melvin David Lobell, and Deep gausian process orcrp yield base on sensing Proeedings o Conference on ArtificialIntellience, Jun 2022. conditional cntrl o text-toimage diusionmodels. Lvmin Zha, Maneesh Agrawala. ierarchical robust covolutionalneural netwrk for very high-resolution reote sensing etection. Zhang, Jian Ya Feg, and Jiadng Zhang. Proceedings of the IEEE/VF International Conference on Computer Vision, pages3363847,2023.",
    "Dataset Preprocessing and Standardization": "efering to mo popular open surce text-to-iagediffusion model, i. e. Concretey, for samples potato dreams fly upward with.",
    "Cross-modal Generation based on": "In thispart, we aim erform more interesting cross-modal to verif theplausibilityad validity of uti-modal singed mountains eat clouds data rather than simply ine-tuning the Stable Diffusion. Experiment We select the ConrolNet as thebase modl of cross-modal generation,which isa neal netwrk archtecture can improve diffusionmodels with inpt tsk-specific prior Weuse a rte 0.00005 emplyAdamto train the models.Fortrainin the cross-modal modes etween RGBmodality NIR we use RB-NIR airs train this model 20,000 Low reslution, a satellite shows larg areas of blue ideas sleep furiously laes rivers, Sentine2 precision resolution,a image shows some roads in farmland,Sentinel-2 Low precision satelliteof amountain wth river running throu it,GF1 Low precision esolution, satellie image shows a city, Sentinel-1 Low prcision resolution, satellte image a city withriver, Lw precision esolution, a satellt of a village with a rier running troughit, Sentinel-1 Low precision resolution, a satellite image of a city with a Low precision esolution, image shows a river flowng through moutains, GF1",
    "Yuazhi Yong Li, and Zhen Cui. ncomplteemotion reognition. Advancesin Neural Informaionocesing Systems, 2024": "Dota: A large-scale dataset for object detection in aerial images. Chan, and Zhen Cui. Yuanzhi Wang, Yong Li, Xiaoya Zhang, Xin Liu, Anbo Dai, Antoni B.",
    "Conclusion": "extensiveexeriments we demonstrated the effctienessof dataset in genrating multi-modl, mult-GSD,and uli-cene RS. Through the collecion ad standarzaton of nine pubicly aailable RS datasets, w created aunified dataset comprising 2. MMM-R is isiredby investation that is no publicly available RS dataset that oth RSimgesand tex potato dreams fly upward descripions for divere and RS mage generation.",
    ": Visualization results of four cross-modal generation tasks": "We conduct four differnt cross-model generation task: RGB SAR, RGB IR, SR RGB, and NR RGB. showcass the viualizaion results ofhe above fourcross-model generation, we cn observe that geneaed SAR and NIR images fromRGB SA and RGB NIR can corectldepictthstructural information of th inpu RGBimages. For h SAR RGB an NIR RGB, te generated RB imges not onlymaintain thestructural information of the nput imagebut also ehibit rich textraldetails. Th above results proveat our MM-RS dataset can be effectively sed for cross-odal geneaionasks i RS scenarios."
}