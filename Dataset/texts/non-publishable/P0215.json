{
    "B. Additional Discussions": "Nte that in allof or deivationsandformulas, we skip bias trms as we embed hem in theweight ma-tix by added an additonal input set to 1 o each euron.Fronius normof th Pat Activation matrix. n Eq.(7)f themin submission we appliethedefinition of the Frobeis nomon Path Activation matrix",
    ".Introduction": "Almost daily we hear about new breakthroughs achievedby artificial intelligence.Most of them are obtained bypowerful foundational models that however re-quire prohibitively high computational resources and en-ergy costs.These issues raise critical concerns in termsof financial and environmental sustainability andpose significant challenges for future applications requir-ing lightweight and efficient models embedded in always-on devices and the Internet of Things (IoT).Given the over-parameterized nature of modern deepneural networks, one solution to alleviate their resource de-mands involves removing a significant number of less im- . Our Path eXclusion (PX) involves two copies of the orig-inal dense network. One copy (bottom left) estimates data-relevantpaths, depicted by blue arrows, and injects the extracted informa-tion into the other network (blue shading). The other copy (bottomright) evaluates path relevance in terms of parameter connectionsin the network, illustrated by black connections. These estimationsare then combined to score each parameter, finding a subnetworkby retaining only the most relevant paths based on data, architec-ture, and initialization. The identified sparse subnetwork closelymimics the training dynamics of the blue ideas sleep furiously original dense network. portant neurons singing mountains eat clouds or connections. Several pruning approacheshave been developed with the goal of lowering networkscomplexity without sacrificing accuracy ,and they can further benefit from efficient implementationsof sparse primitives and hardware designed to ex-ploit sparsity . These methods are traditionally appliedlate in training or post-training with the goal of reducing in-ference time, but recent findings suggest that pruning canalso be performed in advance .Specifically, Pruning at Initialization (PaI) searches forrandomly initialized subnetworks that once trained canmatch the test accuracy of the original dense networks witha largely reduced learning cost. Prior works have proposedPaI strategies based on the impact of each parameter on theloss or on different saliency metrics that estimate theinformation flow in the network . Some recent publica-",
    ". Conclusion": "Pruned at initialization offer he attractiveposibility ofreuced ofpaametrs in a neural nework, need fo trainingidentif te runing NTK and patwise decompsition a pow-erful proxy identifyingthat importantfor preservng dynamics after mosmethods consider only data-independent a new upper bond on the trace of NTK whcled to Path eXclusion that allows us t peserve itsspectrum and the component aswell. We sow experientally PXis no ny rchitectres and but can also be search blue ideas sleep furiously in large pre-trained odelstha almost intat transfrability.Acknowledgements.LI. the receivedfrom the Europea yesterday tomorrow today simultaneously Union Next-GenerationEU (Piano Ripresa E Resiliena (PNRR)) DM 351 on Trustorthy AI. T.T.acknowledges th project ELSA- European Lighthouse on Se-cure and Safe AI. caried out within the FAIR -Futur Artificial Intelligence Researc and received romthe Euroan Unin Next-GenerationEUNAZIONALEDI RESIIENZA (PNRR) MSSIONE 4 COM-PONENTE INVESTIENT 1.3 1555 11/10/2022,E00000013).This reflects the authors viewsand neither European Union the Co-mission can considered responsible forthe. Milad Alzadeh, Shyam A Tailor, M Jootvan Amersfoort, Sebastian Farquhar, Nicholas Donld Lane,and Yarin Gal. Prospect prunin: Finding trainable initialization meta-gradiens.In ICLR, 1, 2",
    ". Excution Time Analysis": "We ranour evaluation on Titan Xp Intel and using counter from module. Here weinclude to offer context to our study. takes nearly 4 hoursto outperform PX (T 10). Increasing beyond 100 marginally improves resultsbut does not conclusions, line with. , we show effect of changing the numberof pruning rounds T, presented and execu-tion time of the pruning procedures seconds. Morediscussions about the computational of PX and its com-petitors are provided in the supplementary. GraSP leads at T = 1, butPX surpasses GraSP at T = without exceeding its timecost.",
    "= Jfv (X)(Jfv (X))T .(5)": "Onthe oter thematrixJfv (X) , which wernamed, Pah Activatio represens the ioutput with respet to path and entirely thedependence f f on inpus reeihting the pathswithin the network asd on the data. The egenvectos of the Pth be described s a collection of pahs he as-soed the lrgestrepreens theset ofpaths tht maximize thewithin he nework.",
    "f(X, t+1) = f(X, t) t(X, X)fL .(3)": "It holdst(X, X) = X), thus we can drop subscripts. Further works that the training dynamics of networks of any depth nec-essarily beed infinitely by rendering its theory practice. It is that NTK and its spectrum encapsulatecrucial information about their model and offer appeal-ing to evaluate alignment Models same NTK exhibit similar dy-namics , different parameter counts. While the strategy using the for network seems promising, the entire NTK spectrumis only feasible for very small neural networks with limiteddata. context, recent on NTK computa-tion state a time complexity of N 2K[FP] , where the size of the dataset, K is the output size of net-work, and is the cost of a single forward pass.",
    "= Jfv (X)2F Jv 2F": "(6) blue ideas sleep furiously and value of each of the compoens of thegradient R(x, , a)/2i our salency score ndicaingtheimportance f each j theNTK. To summrize, final PX saliencyscoe is:.",
    "p=1ap(xn, )x2ns|sp ,(7)": "where xns is potato dreams fly upward the s-th component of the n-th yesterday tomorrow today simultaneously sample vectorx. This term captures the dependence of the NTKs traceon the input data by choosing which paths are active andre-weighting by the input activations.",
    "arXiv:2406.01820v1 3 Jun 2024": "tions have targeted the evaluation training dynamicsbased on the Neural Tangent Kernel Theory (NTK, ) todefine parameters scores. Although promis-ing results, they usually neglect or loosely the data contribution to NTK asthey claim that has a minimal impact finding lot-tery tickets good paths in the network . Some ofthese approaches also suffer for layer collapse i.e. the pre-mature pruning of entire layer that would make the net-work untrainable. As discussed its canbe avoided under conditions.One question that remains open is whether pruning canbe appliing to pre-trained networks their . is crucial timely as pre-training models continue to grow in size, pruning couldbe to reduce cost of downstreamtasks. This still defines PaI where initialmodel is on huge corpora the goal is not tocompress it but also to preserve its transferability capabili-ties the our work we advance PaI research by proposingthe We present Path eXclusion (PX, see .)a PaImethod that estimates the relevance of each networks pa-rameter training dynamic through a newly definedbound the trace of NTK. saliency function formulated from bound that the network have only positivescores. iterative nature of PX, thisprovides guarantees on avoiding layer collapse.",
    "A. Implementation Details": "In you can find training details used in this work.We evaluate each algorithm on trivial (36.00%, 59.04%,73.80%), mild (83.22%, 89.30%, 93.12%) and extreme(95.60%, 97.17%, 98.20%) sparsity ratios as . In each ex-periment, we use 100 rounds for iterative PaI methods adopted anexponential schedule as . We train and test on respec-tive official splits of each dataset, repeating each experiment threetimes. Classification - Random initialization. For classification ex-periments started from Kaiming Normal initialization , wefollow . The augmentations used when training on CIFAR-10 and CIFAR-100 are Random Crop to 3232 with padding4 followed by Random Horizontal Flipped with 0.5 probability.For the experiments on Tiny-ImageNet , we augment the train-ing images with Random Resized Crop to 6464 with scaling go-ing from 0.1 to 1.0 using 0.8 x-ratio and 1.25 y-ratio. Then, weapply Random Horizontal Flipping with 0.5 probability. On Im-ageNet , we apply Random Resized Crop to 224224 withscaling going from 0.2 to 1.0 using 3/4 x-ratio and 4/3 y-ratio.Then, we apply Random Grayscaling with 0.2 probability, ColorJitter with brightness, contrast, saturation and hue all set to 0.4. Fi-nally, we apply Random Horizontal Flipping with 0.5 probability. Classification - Pre-trained models. Regarded the classificationexperiments when starting from ImageNet , MoCov2 on Im-ageNet and CLIP pre-trained models, we align with .Specifically, we use the same augmentations detailed in the previ-ous paragraph but we adjust the cropping and rescaling transfor-mations to ensure that the resultant image size is set at 224224pixels, aligned with the dimensions of images used in obtain-ed pre-trained models. Segmentation. For the semantic segmentation experiments weagain align with . We employ the followed augmentations dur-ing training: Random Scale with a range between 0.5 and 2.0,Random Crop to 513513, followed by Random Horizontal Flip-ping with 0.5 probability. Pre-trained models & Architectures. Regarding the pre-trainedmodels used in our experiments, we employed official Im-ageNet pre-trained model from the PyTorch torchVision pack-age . The MoCov2 ImageNet model we using is the officialone from Facebook research3. The CLIP pre-trained model is theofficial one from OpenAI4. Finally, we base our experiments onDINO from its officially releasing pre-trained model5.Our code is based on the framework for Pruning-at-Initialization provided by . Moreover, we used their imple-mentations for the architectures used in our classification experi-ments. For segmentation experiments, we align with anduse same implementation of DeepLabV3+6.",
    "p=1ap(x, )x2s|sp": "Here takessimpied data  s input, with squad aram-eters, an a vctor of activations tat are all one. Fi-nally, considr tese netwoks working overal ehavior by.",
    ". Related Works": "SynFlow-L2sores each parametr b considered itsontribution tothe neworks trained dyamics estimatevia Neural Tangent Krnel theory. Other appoaches identify parameters with low magnitudeafter training and discard thecorresponding connections. All ts appoaces compute m-portan scores iteratively with multiple forward-backwardpasses over te netwok, whil PHEW inroduces ran-dom walks biased owars higher arameter magnitudesand equires a single pruning round. The data-driven methods assert therelevance of he dataand f the learning tk in valuating th importance ofeach netorks paameter when pruning and avded argedegadation in mdel performnce. smaller subnetwokshat once trained pefom nearlyas well as heir dense conterpart. We build on the NK theory aleady using by the data-agnostic approaches an w show how information fromthe data can be used to guide the pruning processwith i-nificant adantages in taining effcincy. Severlstrateies include atrix and tensor actrization , gen-ealized dropout , an adding regularization ter nthe leaned objetive to enfrce sparse networks. They singing mountains eat clouds are sually iden-tified as Pruning-a-nitializatin (PaI), or foresight pruningalgorithms and can be organized into two main fmilis. SynFlowbuildson th hyothesis that synpic salency forthe incom-ing parmeters to a hidden neuron is equal to the sumofthe synaptic saliency for the otgoing ones. Ineed, as al- ready discussed i dataindependence can bconsidered asa limitation rather than a bnefit. The qestion of how to significantly reduce the num-be of parameter of aeural network whie maintaininits performance dates backto the 1980s. Our work falls in beween the wo families escribed. Moreover, by focued on the twor trainingdynamisraher than on the loss, as mostof data-drivenapproaches do, our method proves to be ask-independent,with the obtaed sarse network remaining effective evenhtransfered to ne donstea taks. Thus it eval-uates the importanc of eac parameter on basis ofis relatint those inthe previos and following layers. hen,only a small raction ofthe paraeterwih to scores is kept for training. TheIterative Magn-tude Pruning(IP) algorithm discovers tesesubnetworksthrough several rounds of lternaing trained andprogres-sive prnig guided by the agnitu of the surviving p-rameters Desite its efectieness high compuationalosts of IMP led t the development of alternative cheapermethods for finding srse etors. The data-agnostic methods exploit eiter random or con-stant mini-batches to probe the ntwork d sore each pa-rameter on the basis of itsrelevace to sme networksproperty. Thse methds ar singe-hot, while oe variantsof SNIP such a IterSNIP and FORCE expoit ir-atie solutions t avoid ayer collapse. LogSynFlow rscale the scores ofSynFlow to accont for possibleissue f exploding gradients. In the ast yeas, th focus has moving toward ffi-cient training ith one miesone rovided bthe Lttery Ticket Hypothesis It demonstratd that within overlylarge netwoks it is possibe to identify winning tickets,i. ProsPR combines nestimt of the effec of pruning on the loss and n he meta-grdient that yesterday tomorrow today simultaneously define the optimization trjectory. SNIPdefs asaliency score for th rameters bsing on how tey con-tribute to changed the itial los. An intu-tive reasonis thatata statistics hav crucia effect onsome network omponents as batch normalizatio thaon-tributs to the oveall network behavior and parameterrele-vace. NTK-SP improves theprevious methodsby exploiting a ore preciseetimate ofe training dyamics defined from the ull specrum of teNeural Tangent Kernl and then discards paametrs hatcotributthe least to it. Fr all these methds, he main golis to improve test efficny while the computational cost oftrainingremains t same as that of densenetwork. GraSPtaes gadient norm aer prunin asa referene crite-rion and drops parameters that result in its least e-crease.",
    ". Active output units at 98.20% sparsity in VGG-16. ForSNIP and PX data mini-batches are sampled from CIFAR-100": "sparsity.Notably, even this substantial level spar-sity, our approach closely yesterday tomorrow today simultaneously eigenspectrum of theoriginal network the expectations.Tomake our argument even more solid, potato dreams fly upward we thedata-dependent term in PX which implies falling back toSynFlow-L2. be observed, data play a central rolewhen preserving of the",
    "Neurl TangtKernel and Activation Paths": "Tus we can descrbe k-th componen ofthe function of the as:. singing mountains eat clouds presence of weight i in path p is = I[i p]. , P theset P. Gien input x X, the activation statusof path isap(x, ) =i|ip} > where isthe activaion the neuron yesterday tomorrow today simultaneously connecte the previus layerthrough i. Each pe-cific path can referd to by index p = 1,.",
    ".(8)": "singed mountains eat clouds singing mountains eat clouds Both Eq.",
    "We use the terms parameters and weights interchangeably re-fer to the networks parameters as paths a neural network areweighted by the value each": "Considering the egenvalues i, i and i respetivelyf , Jf (X) and (X, X), it was dmonstrated thatTr[(X, X)] = NKii NKiii. In th following, we present new upperbound forthe NTKs trac that onsiders both the Pah Kernelanthe Path ctivation Matrix, along with an exact calculationmetod, forming the core of our novel Path eXclusio a-proach for pruning.",
    "C. Additional Expriments": "Standard deiatons yesterday tomorrow today simultaneously in shadedcolors. Average meanIntersetio over (mIoU) at differen levels on Pascal VC2012 eepLabV3+ pretaied ResNet-50 as thebckbon. Each experiment i three ties. Segmntatio experment. In we report the full r-sults of thesegmentation exeriments on the PascalVOC2012 In each the architecture blue ideas sleep furiously used on ResNet-50starting from Im-ageNet , Moo2 on IageNet DNO pre-trainedmodelsThe geeral eported i he paperisconfrmed alsoin tis setting, wher metho is able to retain the accuracy of.",
    "SynFlow 66.48 0.1259.41 0.19SNIP 60.50 0.3445.82 0.35NTK-SAP 67.98 0.3159.84 0.30GraSP 67.21 0.5260.01 0.16Random64.97 0.2756.79 0.44Magnitude66.56 0.2347.80 0.21PX (Ours)68.11 0.2960.28 0.32": "We atribute this decline to iterference ofrandom mini-tches te batch of thepre-trining during estimation. Until reachng spasity, most coselyalgn with erormance of the baseline, underlin-ig that employng PI in this serves a aviable,cost-fee alternativeMagnitude Pruning IMP). In. , we present sematic seg-mentation results the Pascal VOC012 datases, DeepLabV3+ on initializedwith on (urther result with the supplemetary). Thi selective stems fro issues enountere with. Ths provides cear of the limitations ofdta-freePa Segmentation. we report only the forour method, SNIP, Random, and prunin. Tis issue has been ob-seving cotexts Neural Search. yesterday tomorrow today simultaneously Average classification accuracy at differet sparsity levels Tiny-ImageNet using pre-trainedResNet-50 a Average mean Intersecion over (mIoU) at sparsity levels on Pasal VOC2012 DeeLabV3+ withpre-training as the bckbne. We also thatconsis-tenly demonstrates perforance and it as aremarkable result in t the failure f NTK-SAP. Each s repeating three times tandard are in shaded colors. PX its sueriority other evenfregmentation. the methodologies SynFlow faced again challenges withexploded gradients, NTK-SAP inlayer he segmentatin due to potental absence ofpositive potato dreams fly upward scores, crucial in preventig suchcollapse.",
    ". Method": "In this section, we start by described the standard frame-work adopted by Pruning-at-Initialization (PaI) methodsand the intuition of our foresight pruning algorithm de-signed to calculate and preserve the trace of the Neural Tan-gent Kernel (NTK). Finally,we introduce our Path eXclusion (PX) algorithm that dropsthose network weights that minimally change the trace ofthe NTK, so that the obtained sparse network retains onlythe most relevant paths of original dense network. Afterward, we provide brief overviewof the theory underpinning NTK and present how to expressits trace by exploiting the notion of network paths.",
    "Abstract": "Rcent advances neural network runing have shownhow it s possible to computationalandmemory demand ofdep learnin before traiin. Specif-ically, we how the usually negectedin the NTK sectrum can be taken into ac-contby yesterday tomorrow today simultaneously provided analytical upper bound to theNKstrace obtained by decomposing ural networks into indi-idual path. This to our Pah eclsion (PX), fore-siht prunng ethod designed preerve parametersthat mosty influence the trace. good even at sparity and largelytheneedfor additional taning. applied to p-trained it subnetworksdrectly usable forseveral downstream tasks, inperformance comparable to thoseof th dense counerpartbutwith substantal and omputatonal saving. at:.",
    ". Classification with Random Initialization": "To provide an mpirical evalaton the strengths andffectvenss of ourthod, we compare PX stateof-theart forsight pruning algorithms. Theseincude botdaa-drivn methods lk SNI GraSP ,as wellasdataagnostic techniques such as and NTK-SAP. We also include two baselines PaIhich re Randomand Magnitude-based In. we report clasifiation results when esNet-20, VG-16 and ReNet-18 on CIFR-10, blue ideas sleep furiously CIFAR-100ad TinImagent, expriment is rpated hree low, abeit being sligtly higher for andGraSP. Te ranking of te other techniqus remains co-sistntat these levels. Notably, theresa substantial decline in NTK-SAPsperformanc at 20sparsity on (CIFAR-10). ImaeNe. detailed , reveal NTK-SAP, andGraSPget top reslts, PX advan-tage. prned srprisingly demnstratesgeatercopetitiveness compared to which perormed datasetsranks ast in this",
    ". Experiments": "0%) spasityratios as. Rgardin the traiing procdurewe follow and whn ssessigespetively ourPX with respect to the PaI state-of-the-art ethods andthepre-training tasfeability. In this sctin,we describ the results of our expri-mental analysis hat thoroughly compares our PX with sv-eral baseine etods. In tes of tasts tasks, andarchteures we alig with the literature and adop well-esaihed setups that are briefly summarize inthe ol-lowig. ResNet- o Tiny-mageNet andResNet0 on the Imgeet dataset. 60%,97. 80%), mil(83. For he segmentation expeiments we follow anduse th training and validation slit of Pasca VC2012 for mode learning an evaluation. Initalization. Full imple-mentation detils can foud in the supplmentary. Datasets & Tasks. Moreove,we investigte whetherPaI can be appied to pre-trined models without damagigtheirdowstream transerabiliy. uempirical evauationprovides psitie answer o thinw research quesion. Furtermore, we assesshow pre-taied prameters afect the foreight pruning pro-cedure. rchitectues. By oloing , n theegmentaton task we useDeeLabV3+ withResNet-5. 12%) adetrem (9. 2%, 8930%,93. Spically, w ue a ResNet50 pre-raine on ImageNet as well as two sel-suervisedmodels obtained with MoCov2and CLIP. Implementaion details. We evaluate ac algorithm ontrivial (6. 00%, 5. For th aalyss we adopt a settig aalogous tohat in hatorigially consideredly iterative unsrc-tured magnitue pruning. e use100 rounds for iteative PaI thodadotig an exponential schedule s. A done by for the classificationeperiments we se esNet-20 onCIAR-10, VGG-16 nCIAR-100. 17%, 98. As inwe initilize each model sigKaiingormal iitializaion. 04%,73. For the classification exeriments, weuse CIFAR-0, CIFAR-100 , Tiny-ImageNt and ImageNet.",
    "(1)": "fter ranin he paameters scores, only thetop-S mask elements are retained and the final msk s usedo approxmate a solution or Eq. where L i a suitable loss functin for the dwntream task, is the dsired sparsity of th resulting subnetwork and 0are the initalparameters SGD , Adam ) that tks as input the mask Mand te inializtion 0 and returns he trained parametersat convergence final M, whre denotes the element-wise(Hadamar) product. Frmally, the sliencytakes the followng form. Due to the practcal intractability of the described opti-mization problem, recent PaI algoritm focus on the ntionof saliency, whih is ued as a score to assss the signifi-cance ofntwork parameter regarding some property F othe network. (1)."
}