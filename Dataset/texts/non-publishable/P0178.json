{
    "Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, andElisa Ricci. Structured attention guided convolutional neu-ral fields for monocular depth estimation. In CVPR, 2018.3": "3, 5 Hang Yu, Yufei Jng Zang, Wei Zhao, Ziyu Guan, ao. 3, Jiaui Yu, Yuanzhong ing Yu Koh, Thang Lung,Gunjan Baid, Zirui Wang Vijay Vasudevan, Alexandr Ku,Yinfei Yang, Burcu Karagol Ayn, t al. benchmark for posees-timation in the wild. 2. 1 Zhu, Prk, Phillip Isola, a AEfros. APT36k: large-scae for animal pose estimation and tracking. In Proceeings of the IEEE con-ference on computer vision and patern recognition, pages13161324, 2018. YuxiangYan, Junjie u, Jing LongLan, Tao. 2Lvmin Zhang, Anyi Ra, and aneesh Agrawala. Wei Yang, Shuang Ouyng, Hogsheng Li, andXiaogang ang Learned feature yramids for humanpose In EE internationalconference on coputer pages 2017. Tao Xu, Pegchuan Zhang, Qiuyuan Hung, Han Zhang,Zhe Gan, Huag, andHe. Stackgan:Text to photo-realisticsynthesis withstacked generatve adversarial In IEEE interntiona conference on compte vsion,pages 5975915, 2017. 2, 4, 6, 7 Bolei Zhou, Hang Zao, Xavier uig,Tete Fi-dler, Adela Barriuso, an Antonio Torralba Interna-tional ofComputer Vision, 127(3):302321, 2019. imag-to-image transation. Advances inneural information proesinsystems, 30, 2017. Adance in Neurl Information rocessing Sys-ems, 2023. 2022. In on Neral Processng Datasets Bnchmarks Track, 2022.",
    "without text-video data. arXiv preprint ariv:2209.14792,02.": "Amos Sironi, Vincent Lepetit, and Pascal Fua. 6, 7 Amos Sironi, Vincent Lepetit, and Pascal Fua. In Proceedings of the IEEE Conference on yesterday tomorrow today simultaneously Com-puter Vision and Pattern Recognition, pages 26972704,2014. Projectiononto the manifold of elongated structures for accurate ex-traction.",
    "Piotr Dollar and C Lawrence Zitnick. Fast edge detectionusing structured forests. IEEE transactions on pattern anal-ysis and machine intelligence, 37(8):15581570, 2014. 6,7": "Alexey Dosovitskiy, Beyer, Alexander Kolesnikov,Dirk Xiaohua Zhai, Thomas Unterthiner,Mostafa Matthias Minderer, Georg Gelly, al. 2. 11929, Tamingtransformers for high-resolution image synthesis. An image is worth 16x16 words:Transformers for recognition at scale. arXiv preprintarXiv:2010. Pro-ceedings conference on computer visionand pattern pages 1287312883, 2021.",
    ". Experiment Results": "the visual results for boththe multi-task dense image prediction (stage 1), the condi-tioned generation (stage 2), and the combinedmodel. According to it evident that the modelsfrom stages the combined generate high-quality results. 1: Dense Prediction. To demonstratethe ability of our stage 1 we the on thedepth benchmark NYUDv2 and the HED benchmarkBSDS500 .For depth estimation, we compare our result withDPThybrids contemporary work, DeepLabv3+, RelativeDepth , ACAN ShapeNet andDPThybrid . As shown in Tab. 4, our model surpasses themodels for HED .",
    "Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao,and Hengshuang Zhao. Anydoor: Zero-shot object-levelimage customization.arXiv preprint arXiv:2307.09481,2023. 2": "Yuru Chen, aitao Zho, ZhengweiHu and Jingchao Peng. In Proceedings of IEE yesterday tomorrow today simultaneously conference oncomputer vision and pattern recognition, page 87898797,218. Intrnational Jounal of MachineLearningand Cybernetics, 1:1583156, 2021. Stargan: blue ideas sleep furiously Uiied genera-tive adversarial networks for mui-doain imag-to-imagtnslation. 2.",
    "ControlNet has demonstrated its performance in condi-tional image generation across various conditions, includ-": "In his section, we delveintfu reresenativetasks:Depth Ma, HED EdgeUer Scibbl, and AnaPos,alng with the xpert moel asscated with eh Generting depth maps to epresentrelative distances is fundamental chleng in cmputer vision ad scene un-derstanding taks. Nmerous methodshave beenprposed,ranging from traditional tereo mtcing algorithmto dee lernin-based approches. We useMiDAS s our expert model, which ehibitsexceptional performance and generalizaion capabilities. Eary mehods relied on manual designfor edge deec-tion. However, withthe advn of deep learnig, learning-based singing mountains eat clouds methodshave demotratedgreat potentil in handled edge detection tasks. A classicbenchmark in this field s HolisticalyNesting EgeDetec-ion (HED) , and we take it as our exprt model. Usr srile erve as user-deining guidace for imageenerati ask, enablng usersto conveytheir intentionsand prefences o he generative model. Generatin pose maps, which encode spatial informatonabout rrangeent of bjects characers in images, iscrucial for tasks like image-o-imag singing mountains eat clouds translation, paricu-laly in huma or oet pose maniulion.Humn poseesimation models are designed tocrib human keletons.",
    "Ours0.7610.7820.811": "HED performance ofmulti-task dense imagepredic-tion 1) model. For he three ODS, nd AP, thelarger th nuber better the performane. We can hat achievescompetitive performance. he presented inTab. 2. It beobsrved thatalthough th overall performance the moel slightly inferior methods directlyutiliz-in featuresfro multiple expert mols, it still anages togeneate iges of proising ali.",
    "Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding,Yichang Zhang, Peng Wang, Ang Wang, Le Jiang, XianyanJia, et al.M6: A chinese multimodal pretrainer.arXivpreprint arXiv:2103.00823, 2021. 2": "In of theIEEE conference on vision and pages 21172125, 2017. In Computer 2014: 13th EuropeanConference, Zurich, Switzerland, September 6-12, 2014,Proceedings, Part 13, 740755. visual generationwith composable diffusion models. 2014. Tsung-Yi Lin, Piotr Ross Kaiming He,Bharath Hariharan, Serge Belongie. 3 Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, Tenenbaum. Tsung-Yi Lin, Michael Serge Pietro Perona, Deva Ramanan, Piotr Dollar, andC Lawrence Zitnick. 3. IEEE transactions pat-tern analysis and intelligence, 38(10):20242039,2015. European on Computer Vision, pages Springer, 2022. 3 Fayao Liu, Chunhua Guosheng Lin, Ian Learning depth from single images deepconvolutional neural fields. Swin transformer:Hierarchical transformer shifted InProceedings of the IEEE/CVF conference oncomputer vision, pages 1001210022, 2021.",
    ". Different Encoding and Number of Heads": "Our ablation studyinvestigates the indispensability the singing mountains eat clouds multi-head FPN andthe efficacy encoding. We implement two varia-tions: one with a FPN head another lever-aging complex embeddings by the CLIP text encoder. The comparative results in Results show that one-hot encoding with multi-ple FPN heads yields superior demonstratingthe effectiveness design.",
    ". Prefix Injection": "In this ablation study, we addedthe to parts of the model. The results are Tab. observe that adding prefix only thetrainable part yields better results.",
    ". ControlNet": "ControlNet modl presets an efficient frame-workfor the Stable Diffuson model. epth map detection) to gneratve process, ha thegenerate adhere to he textual prot and thecontrol proach,the weights of Sta-ble (SD-v1. 5) are fixe, atrainaleduplicat f the weights from 12-layer U-et eoderand Wehe encoder in frozen art as E, the en-oder of the trainable opy as E, the middle bock the.",
    ". Textual Inversion Module": "Oce estab-lihed, uiqu tokes can be integratd into allowingfor precise the characteris-tics of the images produced.We leverae Stable Diffusin as ou base model. For frozen SD mdl, suppo encoded feture of s, then we can express = adetermined v. the shouldbe",
    "Datasets": "In the part,we utilze he AP-10K dataset and use modelgenerate the animal pos of the animals. Thecaptions the LIP In tomake thecontai approxmately the same numbr we each imge in second part 5 time. The captions of te iages aretaken from te oriin Laon-5B ataset. The dataset forboth multi-task dense image pre-dcton (tage1) n conditioned txt-to-image ) taining consists 2 diferet pars. Foranimal p, weutilize the APT-36K dataset ad hoose the im-age from each rame as the datast. We the firt eac i the daset. Infirst prt, we irst YOLOv5 moel to e-tectall the thefrom the dataset and therst 50,000images ha consist 1 huma We directly smpe user scribbles from theiages, employ an HED boudary detection to dges, and the Midas detector o depth maps.",
    ",116, and132, with a uniform feature channel count of 256": "A crs-roductopertin is executed between the utput of the Multi-Head FPN and the encode task embedding, culminating nthedeoer outp, by a singing mountains eat clouds Sigmid. a FPN i employed to arnessrch semantic from these muli-scale featur acros varous task types, potato dreams fly upward heFPN strutured i a paralle configuration with m distnctheds,each represenin a variant of th oriinal FPN ar-chitecture. In the fial stage, taskpeific embedding leveraedto th trgetcodition from the aforeentioned The fexiblity in the type oftask note-worthy; both one-hot clip ext embeddings derived romth name are effective. Spcifically, each FPN head undrgoes an ddi-tional transposed convolution layr to he resoutiono 1simultaneously reducing the channel dimen-sion o C The concatnatd outpts of allm heds yielda comprehensiv, full-resolution multitak channel dimensionmC. We employ Multilayer (LP) t the embeddin intoa latentspace with mbedding f mC, subsequentlyunsqueezing channl to 1.",
    "Yuva Patashnik, and Daniel na matterof styl: Age using stylasedregression model. ACM Transactions on Grahis (TOG,40(4):1, 2021. 2": "Pablo Mire, Charless owlkes,and Ji-tendra Contour detection ad hierarchical imgesegmenttion. on Pattern Analysis blue ideas sleep furiously andachine Intelligence,33(5):898916, 211. Proceding the IEEE con-feence on computer nd pattern recognition, 6, Geds Bertasius, Jnbo Shi, Lornz Torresani. 6,7.",
    "Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-sion transformers for dense prediction. ICCV, 2021. 6, 7": "Ranftl, David Hafner, and Vladlen robust monocu-lar depth datasets for zero-shot cross-dataset transfer. IEEE Transactions Pattern Analysis andMachine Intelligence, 44(3), 2022. 5 Rene Lasinger, David Hafner, KonradSchindler, and Vladlen Koltun. Towards robust monocu-lar depth estimation: Mixing datasets for zero-shot cross-dataset IEEE Transactions on Pattern Analysis andMachine 2022. 3 Joseph Redmon, Santosh Divvala, Ross Girshick, and AliFarhadi. You only real-time de-tection. In Proceedings of the on computervision and pattern recognition, pages 779788, 2016. Scott Reed, Zeynep Yan, Lajanugen Bernt Schiele, and Honglak Lee. Generative text-to-image Proceedings on Machine Learning, 2016.2 Xiaofeng Ren and Liefeng Bo.Discriminatively code gradients for contour detection. In Proceedingsof the on Neural InformationProcessing Systems-Volume 1, pages 584592, 2012. 6, 7",
    "Zhuowen Tu. Learning generative models via discrimina-tive approaches. In 2007 IEEE Conference on ComputerVision and Pattern Recognition, 2007. 1": "2 Igor Vasiljevic, Nic Kolki, Shanyi Zhang, Ruoin Luo,Haochen Wang Falcon Z. Dai, Andrea .Dniele, Moham-madrea Mostjabi,Steven Bsar, Matthew R. WalteranGregory Shakhnarovich. iode: A dense indoor and out-door depth dataet, 2019. Advancesin neural inforation presin systems, 0, 2017. 2",
    "Task Embedding": "Considering the of te as the featues ateach stage crrespond to resoltions 1. As , muti-taskdense predic-ion is architecturall divided into three componnts:a backbone trcture a Featre Pyramid Net-wo (FN) , and a Decoder employ a pre-trained Swin ransfomer to extract multi-scal features. An overiew four multi-task dense image we leverage a Sin ransformer to extract multi-scale and prose a mti-ead FPN to get full-resolutionfeature e utilize tak-peciic embeddings to de-cod ense predictions from the feture mps.",
    ". Stage 2: Conditioned Generation": "provides overview of our conditioned text-to-image generation (stage 2) pipeline. Subsequently, we add these newwords into the CLIP embedding space that whenthey are used text prompts, the CLIP encoder can recog-nize their specific meanings. After acquiring these new we adapt theprompts each image) For in-stance, if the feature for a triplet is the depth map ofthe image and the singing mountains eat clouds original prompt a motorcycle in frontof a revised prompt would be <depth> asa feature, motorcycle in front of a tree. 1 the comparison of the model size wellas the when compared to other integrated , and Uni-ControlNet , andour model demonstrates advantages. When compared to our model, followingthe structure of ControlNet, requires no additional param-eters.",
    "Evaluation Metrics": "our muti-task dense image predction (stage 1)model, metric are adopted to evaluat as-pect of the modelserformance. For epth estimation, theRoot Mean SquareError is utilized. Foredg de-tection, three distnct meris are adopted:the conourhreshold (ODS), pe-image best (OIS), and av-erage precision n the hand, OIS for mage to find optima particular image, offered a mor adaptive measure Lastly, P is a commonly metric in edgedetection tasks. similarity score, ach pair ige andcorresponding cption, w ViT-B/32 CLIP to them, calculat the inner of them as theCLIt similarity score",
    ". Original ControlNet model. For different fea-tures, we have to use different expert models for condition gener-ation, and we have to train ControlNet on each of the features": "decoder of the blue ideas sleep furiously frozenpart M D, respecively. Letthe CLIP-encoded additional feature e the input of as time as t, potato dreams fly upward the CLIPncodedtext prtas Cosequently te ofthe modelpred, whic also estimates in th denoising be.",
    "David Marr and Hildreth. Theory of edge detection.Proceedings the Royal of London. B. Bio-logical Sciences, 207(1167):187217, 1980.": "arXiv reprinrXiv:202. 2 Chong Mou, Xinao Wng, Liangbin Xie, Jia Zhang,Zhongang Qi, Ying hn, and Xiao ie. 01073,2021. 2, 8. Stackedhoglass networks for human pose estimation. In Proceeding of th IEE/CVFInternationalCnfernceon ComputerVision, pages20852094 2021. Fowlkes, and J. Semantic mae snthesis with spatially-aptivenormalzation. C. D. 3 Chenlin Meng Yutong He, ng Sng, iamngSng, ia-jn u, Jun-Yan Zu, and Stefano Ermo Sdedi: Guidedimage synthesis and editingwih stochasic differenialequations. T2i-apter:Larning adaptes to dig ut ore controllable abiity for tex-t-image diffusion models. Stylelip: Text-dive mipula-tion of tylegan imagery. I Proceedings of the EE/CVF confrenceon cmputer vision and pattern recognition, paes 237346, 2019. I Com-puer VisionECCV 206: 14th European onfeenc, Am-stedam, The Netherlad, Octobr 11-, 2016, roceed-igs, Part VIII 14 pags 48399. 08453, 2023. Malik. Martin, C. 3 Taesun Pak, Ming-Yu Liu, Ting-Chun Wan, an Jun-YanZhu.",
    "Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficientgraph-based image segmentation. International journal ofcomputer vision, 59:167181, 2004. 6, 7": "yesterday tomorrow today simultaneously 2 potato dreams fly upward Huan Fu, Mnmin Gong, Chaohui Wng, Kayhan Bat-manghelich, and Dachengao. IEEE Conferece Computer Vsio Pattern Recognition CVPR),2018. He,Tsu-Jui Fu, Varun rjunAkula, Pradyumna Naryana,Sugato Xin Eric Wang,and Willim Wang. 3.",
    ". Text-to-Image Generation": "The task of ext-to-image generation ito generate an imae mtchig theprviding text promptsusing deplearning models. Beforethe wide use of diffu-sion mods, te task was primarily achiev by GAN based mdels. Thework GenerativeAdver-saria Text to Image Synthsis aplied an encoder to en- coe the texs and concatnated the encoded features to teiage featuresbfore iseting thm into the GANodel,hch wa among te first works to ackle th task Afte theintroduction ofdiffusion moels, los of diffusion-based mdels apeare , which mainly used crss attention tocombinehe image and text etresin the UNet backbone. DALLE-2 and StableDiffusi are amogthe utstanding literaturein the field. Many woks, inclu-iT2-Adapter , ControlNet our OniCotrol-Net mdelare based on th Stabl Diffusion odel. 2. ag-to-Imae Generative Modl Image-to-mage gneration involves trasferring an im-age from one domain to another. Fr xample, in Control-Net additonal features roided as imgs ae fedinto the model to generae the required images. Before thewidesread use of diffusion moels, GAN-based models such as and Tranformr-based del were com-monly adopted. CycleGAN was one o the fore-mos models for mage-to-image transfer, utiizing a GAN-based approach for style transfr wth cle consistec.",
    "Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.Realtime multi-person 2d pose estimation using part affin-ity fields. In CVPR, 2017. 3": "Hidalgo Martiez,T. Opnpose: Realtiemulti-prson 2d pe estima-tion using par affinity fields. IEEE Tansactions on PatternAnalysis ad Macin Intlligence, 2019. 1, 3HantingChen, Yunhe ang, Tianu Guo, Chang Xu, Yipig Deng, Zhenhua Liu, SiweiMa, Cunjing Xu,Cha Xu,and Wen Ga. Pre-traied image processing transformer. ."
}