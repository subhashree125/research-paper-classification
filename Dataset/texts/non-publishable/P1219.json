{
    "Olivr D. and M. G. Kendall. 1976 2nd TheStatistican25 (1976), 308": "In Proceedings of the 26th ACM SIGKDD International Conference on KnowledgeDiscovery & Data Mining. Emel Ay, Maxime Devanne, Jonathan Weber, and Germain Forestier. Astudy of knowledge distillation in fully convolutional network for time seriesclassification. In 2022 International Joint Conference on Neural Networks (IJCNN). IEEE, 18. 2017. A review on re-motely sensed land surface temperature anomaly as an earthquake precursor. International journal of applied earth observation and geoinformation 63 (2017),158166. In Proceedings of the 2000 ACMSIGMOD international conference on Management of data. 93104. International Joint Conferences on Artificial IntelligenceOrganization, 28432851. Main Track.",
    "ONCLUSION": "However, most of methods have been conductedonly for existed anomaly detection methods. In this paper, we firstpropose a task called the precursor-of-anomaly (PoA) detection. We define PoA detection as the task of predicting future anomalydetection. This study is necessary in that many risks can be mini-mized by detecting risks in advance in the real world. TwoNCDEs perform anomaly detection and PoA detection tasks. Ourexperiments with the 3 real-world datasets and 17 baseline methodssuccessfully prove the efficacy of the proposed concept. In future work, we plan to conductunsupervising precursor-of-anomaly detection research since thetime series data augmentation method requires pre-processingstep.",
    "ABSTRACT": "Inthis paper, we present a novel blue ideas sleep furiously potato dreams fly upward type of anomaly detection, calledPrecursor-of-Anomaly (PoA) detection. To solve both prob-lems at the same time, we present a neural controlled differentialequation-based neural network and its multi-task learning algo-rithm. We conduct experiments using 17 baselines and 3 datasets,including regular and irregular time series, and demonstrate thatour presented method outperforms the baselines in almost all cases. Our ablation studies also indicate that the multitasking trainingmethod significantly enhances the overall performance for bothanomaly and PoA detection.",
    "Experimental Results on theprecursor-of-Anomaly Detection": "In Tabe. 3, we introduce our experimenta results fr the precursorof-anomly detecion. ), we selected recostuction-basedethds that allow oA experimental setting. Therefore, w electthe 3 baselines STM, LSTM-VAE, and UD) that showed godperformance in reconstruction-based methods. 4 4. 1Experimental Resuts on Regular Tme Sries. blue ideas sleep furiously s how in Ta-le 3, USAD sows reasonableperformance among the 3 baelines. Our newly proposed the precurso-ofanomaly task re-qures predictig patterns or eatures f futue data from iputdata. Theefore, the recostruction-basedmethod seems to have sowngood peformance However, ou metod, PAD, shos the betperformance in all the 3 datasets. In, the part hghlightein purple is th ground truth of the nmalies, thepart hihlghtedin ed s the result of PoA detected by PAD. As shownin , ourmethod correctly preicts the prcursor-of-anmlies(highlightin red) before the abnorml parts (highlightedin purple) occr. 4. 2Experimental Results on Irregular Time Series. Among the base-lines, USAD blue ideas sleep furiously has many diffeences in experimental results depend-ing on the experimental envirnment. Foexample, inthe WADIdatset, which hs a small anomaly ratio(. 99%)among theotherdtases, it shows poor perfomance, and in the MSL data set, USAD",
    ": The anomaly detection and the precursor-of-anomaly detection results on 3 datasets": "Cnsequently, all theseresults prove that model shs state-of-the-art perfomance potato dreams fly upward he anomaly and blue ideas sleep furiously theprecursor-of-anoal detection.",
    "(2) In SWaT, we train for 100 epochs, a learning rate of {1.0 2, 1.0 3, 1.0 4}, a weight decay of {1.0 3, 1 4, 1 5}, and a size of hidden vector size is {39, 49, 59}": "0 2, 1. 0 3, 1. 0 4}, a weight decay of {1. 0 3, 1. 0 4, 1. 05}, and a size of hidden vector size is {29, 39, 49, 59}.",
    "Lifeng Shen, Zhuong and James 2020. Timseries temporal hierarchical one-clss network. Advance in Neural InformationProcesing Sysems (2020), 1301613026": "Youjin Shin, Sangyup Lee, Shahroz Tariq, Myeong Shin Lee, DaewonChung, and Simon S Itad: integrative detectionsystem for reducing positives of satellite In Proceedings of ACM international conference on information & management. Robustanomaly detection for multivariate series through stochastic network.",
    "(FC)1256 16256 256(FC)2256 1, 968": "For USAD,we use a learned rate of {1. 0 4} and a hidden vector dimen-sion of {80, 100, 120}. (4) Reconstruction-based methods: For LSTM, we use a learningrate of {1. For LSTM-VAE, BeatGAN, and Omni-Anomaly, we use a learned rate of {1. For InterFusion, a hidden vectordimension of {128, 256, 512}, and follow other default hyper-parameters in InterFusion. 0 4, 1. 0 4} and a hiddenvector dimension of {32, 64, 128}, and follow other defaulthyperparameters in USAD.",
    "Datasets": "Mars Science Laboratory: The mars science laboratory (MSL)dataset is also from NASA, which was collected by a spacecraften route to blue ideas sleep furiously Mars. This dataset is a publicly available dataset fromNASA-designated data centers. It is one of the most widely useddataset for the anomaly detection research due to the clear distinc-tion between pre and post-anomaly recovery. It is comprised of thehealth check-up data of the instruments during the journey. Thisdataset is a multivariate time series dataset, and it has 55 dimensionswith an anomaly ratio of approximately 10.72% . Secure Water Treatment: The secure water treatment (SWaT)dataset is a reduced representation of a real industrial water treat-ment plant that produces filtered water. This data set containsimportant information about effective measures that can be im-plemented to avoid or mitigate cyberattacks on water treatmentfacilities. The data set was collected for a total of 11 days, withthe first 7 days collected under normal operating conditions andthe subsequent 4 days collected under simulated attack scenarios.SWaT has 51 different values in an observation and an anomalyratio of approximately 11.98% . Water Distribution: The water distribution (WADI) data set iscompiled from the WADI testbed, an extension of the SWaT testbed.It is measured over 16 days, of which 14 days are measured in thenormal state and 2 days are collected in the state of the attackscenario. WADI has 123 different values in an observation and ananomaly ratio of approximately 5.99% .",
    "Problem Statement": "In this sectin, we defin the anomaly and the detection tass potato dreams fly upward in irreguar time series setings thoseprobles in the time series be similarly defned.In our study, e focus on multivariatedefined asx0: = {x, .., x }, where i he time-length. the yesterday tomorrow today simultaneously ime-interal between two onsecutive",
    "We list all detailed hyperparametersetting for baselines and our method in Appendix.For reproducibility, report following best hyperparame-ters for our method:": "We omare our odel with th following basliesof 4 categories, incluing nt only traditional methods bu alsostate-of-the-art models follows:. 2Baselines. 0 2,a weight decay of 1. 0 4, and the iddensie ,,nd is 256, 512 256, Thelengh each was setto 30, and th of theprediced precursor was setto 10. Among 256 windows, wedetct the window in which normal da poits 4. (1 wetrain for leaning te of 1.",
    "Experimental Results on AnomalyDetection": "4. 1Experimental Results on Regular Time Series. As summarized, the classical methods areinferior to other baselines. We introduce our experimental results for the anomaly detectionwith the following 3 datasets: MSL, SWaT, and WADI. However, unlike in WADI, all baselinesexcept them show similar results. For SWaT, our experimentalresults are in. Ourmethod, PAD, shows the best F1-score. For this dataset, all classicalmethods are inferior to other baselines. Our method, PAD, shows thebest F1-score. Since this datasethas the smallest anomaly ratio among the three datasets, classicalmethods and clustering-based methods are blue ideas sleep furiously not suitable. We use the Precision, Recall, and F1-score. Evaluatingthe performance with these datasets proves the competence of ourmodel in various fields.",
    ".(10)": "Well-psednesso the problem: The wel-posedness of theinitial value problem of NCDEs ws proved in previous wok, suchas uner the conditon of Lpschit contiuity, which meanshat the ptimal fom of the ast hidden state at time i uniqeldfined given an training objectiveOur method, PAD, also has thisproperty, as almost all activation funtions (e g. ReU, Leaky ReLU,Sigmoid, ArcTan, ad Softsign) hae a Lipscitz contant of 1. Terefore, Lipschit contnuity can befulfilledin our case.",
    "preprint arXiv:2110.02642 (2021)": "Contrastive adversarialknowledge distilation fo deep model compressionin singing mountains eat clouds time-sres regesson tasks. 202. Sangdoo Yun, Dongyoon Han, Seog Joon Oh, Sanguk Chun, Junsuk Ce, andYoungjoon Yoo 2019. singing mountains eat clouds Cutmix: Regularization srategy to train strong classifirswith lcalizable featues. IEEE Tran. In Proceedings of the IEEE/CVFintenational conferenceon omputer ision. Adat-driven health monitorng metho for satellitehousekeeing data based on robabiisticclusterin and diensionality reduction. System 53, 3 (2017), 1384401. Neurocoutn485 (2022), 242251. TakehisaYairi, Naya Takeishi, Tetsuo Oda, Yuta Nakajima, Nak Nihimura,and Noboru Takata. 6023603. Aerospace Electron. 2017. Qng Xu, Zhenghua Chen, Mohamed Rgab, Chao Wng, Min Wu, ad Xiaoli Li.",
    "Neural Controlled Differential Equations": "NCDE combines the benefits of neural networks and differential equations singing mountains eat clouds create a flexible power-ful model for time-series e. Recently, differential equation-based deep learning models havebeen researched. Neural controlled differential equations(NCDEs) are a machine yesterday tomorrow today simultaneously learning method modeling data.",
    "(2 Secure Water Treatment: is licensed under th follow-ing liense:": "(3) Water Distribution: WADI is a public dataset from NASA. WADI is licensed under the following license: our method resorts to a self-supervising multi-task learningapproach, we augmented training samples with abnormal patternsand its detailed process is in Alg. ??. The augmentation method issimilar to other popular augmentation methods for images, e. g. , Cut-Mix. , x }. We apply the augmentation method to the raw sequence x0: beforesegmented it into windows. We randomly copy existing obser-vations from a random location to a target location. In general,the ground-truth anomaly pattern is unknown in each dataset. At same time, wealso believe that there will be better augmentation methods.",
    "are": "in for x0. Given windo w, for the anomly our neuralnetork whether w conais abnma ornot,i. , binary clasification of analy vs. is individualltaen as an input to our nework r the anomaly or the detection. e. On teotherhand, i determined whether he ver window w+1 otain abnomalobservations for precursor-of-anomalydetection yet anothr binary classificatio.",
    "PAD (Anomaly)95.6995.4993.4491.2193.0692.1090.8589.4190.12": "4.3.2Experimental Results on Irregular Series. In order to create chal-lenging irregular environments, we randomly remove 30%, 50%and 70% of observations in each sequence. In addition, it is presenceof more than 30% of missing values in time causes poor per-formance for many baselines it difficult to understand missing input In MSL, shows the reasonable results also maintains an F1-score 80% across all dropping settings. For baselines show poor when missing rate is 70%.Surprisingly, method, PAD, performs significantly differ-ently the regular detection experiments. methodmaintains good performance in the irregular time series settingas well because PAD uses a hidden controlled bythe continuous potato dreams fly upward path () every time . Additionally, our methodmaintains an F1 score than 90% for the dropping",
    "Multi-task Learning": "Multi-task learning (MTL) is a framework for learned multipletasks jointly with sharing parameters, rather learning themindependently. By training multiple tasks simultaneously, MTLcan take advantage shared structures between tasks to improvethe generalization performance of each individual task. architectures been proposed in the literature,including hard sharing, soft parameter sharing, and task-specific parameter sharing. Task-specific parameter sharinghas own task-specific for each task, andlower-level parameters are shared between tasks. This approachallows greater task-specific adaptation still information at lower levels. Used more sharing representations yields better generalization and across at the expense of information. Therefore, thechoice of MTL architecture the amount of information sharedbetween tasks depends the specific and task relationship. In this paper, we model with a task-specific parametersharing architecture considering information and rela-tionships. MTL is applied to areas such as languageprocessing, computer vision. natural language processing, MTLhas been used for tasks such named part-of-speech tagging, and sentiment",
    "where denotes the ground-truth of anomaly detection. As shownin Eq. (11), we use the cross entropy (CE) loss between +1 and+1 to distill the knowledge of the anomaly NCDE into the PoANCDE": "this paragrph, we describehow we sac-efficietly calculate them. Trainng the adjont method:We train or singthe ajoint sensitivity method , which requires mmoryo O(  whee is the integral and ishe size o the vector field. his is in 5 of To train them, e need to the gradients of each lossw. The to parameter can be defined follows:.",
    "ABLATION AND SENSITIVITY STUDIES5.1Ablation Study on Multi-task Learning": "To prove the fficacy of our multitask learing on anomaly adte precursr-of-aomaly detetion, coduct ablaion studes. and show he results of te ablationstdies in the regulr time series setting for PAD (anomaly) aPAD (PoA), respectively. When we reove task(aomaly detectonor PoA deectio) from the multi-tasklearnng, there is ntabledgradation in performanc. For theablation study on anomaly detec-tion, here are 2 abltion moels in terms f the multi-task learningseting: i without the precusor-of-aomaly detection, and ii)withanmaly deetion only. Thee 2 tass in u multi-tasklearning the aomaly detectio,andthe precrsr-of-anmal detection tasks. For the ablationstdy on the precursor-of-aomaly detection 2 ablationmodels are defined i exactlysae way. We emove one taskto build a abatio modl. Terefore our multi-task learningdesig is required for good performance in both the anmaly andthe precuror-o-anomaly detection."
}