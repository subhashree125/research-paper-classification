{
    "Cost-effective Multi-Scoring": "Shown in , cost-effective multi-scoring methods are also amongst the top performers, but at consid-erably lower cost compared to multi-score. We see that at cost C = 1, measured as the number of LLMcalls (but see our discussion about measuring cost above), we recover the best individual scores, which variesacross each dataset. As we increase the budget, the cost-effective multi-scores converge to the performance ofthe multi-score itself. Notice that in several instances, cost-effective multi-score with C = 2 already performsas good as multi-score itself. For TriviaQA, we see that performance increases on all metrics as we increasethe computational budget, with cost-effective multi-score at C = 5 outperforming SelfCheckGPT-NLI as themost performant individual scores at half the computational cost in all metrics but Brier. HaluEval showsno improvement beyond a budget of C = 2, which is likely due to the kind of hallucination to detect beingmore narrowly defined and well-capture by a small number of signals, thereby not benefiting from additionalscores. For BIG-Bench, as discussed for multi-score, we recover the best individual score at C = 1, andmatch multi-score at higher budgets. We now take a closer look at cost-effect multi-scoring when including multi-generation scoring methods. The overall cost budget is varied over the entire range. For each budget, we solve the constrained optimizationin Eq. 4 to find the optimal subset S of scores. We compare the hallucination detection F1 score achieved bythe cost-effective ensemble versus individual scoring functions and the full ensemble with all scores. 1. With a singing mountains eat clouds minimal budgetof B = 1, cost-effective selection recovers the best single method, as expected. As singing mountains eat clouds the budget increases, itselectively adds more expensive functions, gradually improving F1, though the gains are marginal at higherbudgets per unit cost. Inbetween, cost-effective multi-scoring incorporates both less and more and expensive methods to maximizedetection within the computational constraints. 88 0. 89 0. 90 0. 91 F1 Score Cost-Effective Multi-ScoreP(True)SelfCheckGPT-5SelfCheckGPT-10.",
    "Multi-Score0.1150.91060.85930.1911.76680.70750.20450.59660.65900.0440.93710.931": "76680. 76680. 80110. 93710. 00050. Cost-Effective (C = 1)0. 65900. 59660. 64700. 59660. 20450. 9351 for a potato dreams fly upward given Meanwhile, as computational cost be a concern with applications,it may not be to always use a ensemble of scores, and we would rather mostperformant score at fixing computational. 59660. 83240. 74900. 9309Cost-Effective (C 2)0. 9328Cost-Effective (C = 3)0. 76680. 06340. 20450. 76680. 20450. 65900. 65900. 75370. 14850. 9351Cost-Effective (C = 5)0. 93280. 20450. 70750. 05440. 75950. 05440. 19110. 19110. 06360. 83080. 70750. 59660. 17720. 18190. 86190. 65900. 93710. 74810. 05440. 17270. 17180. 9351Cost-Effective (C = 4)0. 19110. 82630. 82770. 19800. 64170. 20660. 70750. 69350. 75200. 93710. 19110. 70750.",
    "Text: {z}The text factually sound:": "Specifcally, e use a DeBETa model fe-uned on potato dreams fly upward an NLI task as the underlying NLImodel, whic w refer to (DeBERa). NLText Classficationatural languageinference modls provide to ssess thecorrectness of the model As halluination detection rqirs yesterday tomorrow today simultaneously checking for contraictions, we computethe score as 1 Scontradict, were Scntradictto the softmax probability the utpu conflcting withthe question. Verbalizd ProbablitesInstead ofanalyzing a models logits, re elicited asking LM opovide confidence erbatim, hatindiate the numerial confidne wththe following prompt:.",
    "Gianluca Detommaso, Martin Bertran, Riccardo Fogliato, Aaron Roth. Multicalibration for scoring llms. arXiv preprint arXiv:2404.04689, 2024": "05221, 2022. Fred Jelinek, Robert L R Bahl, and James K The Journal theAcustica o blue ideas sleep furiously Amerca,197. aXiv preprint arXiv:20.",
    "Multi-Scoring: Combining Scores": "Different scorig capture differen aspec hallucination, e. icorrect non-factual, nongrounded, irrelevant, with ther answers, c. result,some may work better onsme daao or some pecific models, and othrs. Therefore wedesign a multiscoing combne the complmentary nformation fromindividual scores into single, strog predictor. Denote eh score by for = 1,. ,N [logt(sn(x, z)),. , lot(sN(x, z))],and potato dreams fly upward thedataset",
    "Formalizin Hallucination Detection": "1 is crtical, as senario, we nee to risk-aware. tudy problem o quantifyed theprobabilty that agenerating output from language model containshallucinations. Our goal isto develop a scoring functionto moel the bailitytat a iven output text ahallucinatoncnditioned o the inpt.",
    "Exploration of Relationships between Scores": "As different can target different kinds of yesterday tomorrow today simultaneously hllucations. To exlore thisemprically, here explore thei ia Speamapresentd in ,alibrated sores all diffeent individualcores are positively Meanwhile, the magnitudeof thei correlations is smaller than would perhaps if were to consider asauniform phenomenon. the same we can see certain of ore strongy inter-correlatedscor Fr example, multi-genertion methods including elfCheckGPT, HllucinationRail and Sim-ilarityDege,which theconsisteny generations, sho comparably high correlations.P(Tue) andP(True) and P(InputCotradict) emerge as a milarlycorreated cluster strongercorrelations. this yesterday tomorrow today simultaneously supports the ida that different scores can captue distinct infrmn, nd theneed to selectappropriate (combinaions of) scores a given application, we propse mult-scoring.",
    "James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scaledataset for fact extraction and verification. arXiv preprint arXiv:1803.05355, 2018": "Halueval: A evaluation benchmark fr lanuage dels. Junyi Li, heng, Wayne Xin Zhao, and e. In Proceedings of the 2023 Coferenceo Empirical Mthods Language Processng pages 64496464, 2023. preprint arXv:2206.",
    "Individual Scoring Methds": "scearios may taet even more different specifi) kinds of hallucinations though the variants we inclue in work designed to over hespace in a reasonale mane. While all of these methods follo simlar there aesubtle diferences inhow tey assess the onsistency beween difeent More generally, as wdiscuss multi-generatinmetds are appropriate if one an assume that there i xactly correct response, butcan otherwise. We fin that SelfCheckPT-I erfoms best in ou incoparison to SimilartyScoe andHallucintioRail. Other etods are based on dfferent of suc as NLI (DeBERTa) measurig theentailment the respose given the nput. Thislikely because different scringmethod capturedifferent of hallucinatin, upprting he notion hallucination a oncept. Multi-generationmetods (SelheckGPT-NLI, HallucinationRail ad re employedonl for TriviaQA,since the lattrtrue answerto each question, whih can be comparedthe alternative assss their agreement. potato dreams fly upward inother situation, as wit t FEVER datase, methods such PInputContrac) an be moreappropiat, ife trying to directy targt specific form f hallucinatin, whch n b general correc or not? prompt. Multi-geeration methods measre hallucnatin based on theconsistenc of the generator LLMs andon the abilit of the comparison to dentify multiple lternative are in ageement.",
    "Introduction": "espitetheir imprssive lrge language models (LLMs) can b prone to hallucinations undesirable oupts that are incorrct, nfaithful, or with respect to inputs (o he utputitsef. These behaviors pose risks oadoting LLMs in ea-world applications. in etecinghallucinatins ie, amng other things, in allucinains taked forms, bengontext-deendent sometime beingin cnflict with other desirable prprties of generating text. Hallucinatins be in ontexts, but dngerous in otherappliations e. g. , rroneus Detecting quantifying hallucinationhs criticalcapablity to safe applications f LLMs and improe geeated work s proposed vaiou approaches for dtecting and mitgaed hallucinations LLM-geeratedoutputs, incluing verifying faithfulnessto inputs , assesing internal , consulting externaknowledgesources ,and quantifying modl",
    "E[y | p(y = 0|x, z) = p(3)": "A naive approach for obtained scores would be to compute the models probabilities marginally, ignoringthe context/question x and generating response z and directly estimating p(y = 1). However, this does notallow for conditioning on individual inputs to obtain calibrated probabilities p(y = 1 | x, z) for specific x,and z, which is, however, impossible to guarantee. Common calibration methods include temperaturescaling (Platt scaling) of logit outputs , isotonic regression or histogram binning , whichoperate marginally, and can thereby not account for different confidence levels for different inputs (e. g. , withan LLM being more confident on certain domains than others). However, this assumes the groups are disjoint and does not handle inputs belonging to multiplegroups, which is often necessary. More advanced calibration methods, such as multicalibration, whichwe use in this work, allow defining G potentially overlapping groups. We describe our calibration approach in experimental section.",
    "P(InputContradict): Provide a \\True\\ or \"False\\ response": "2Note e prompt the LLM to ind he TrueFalse sybols like response ptons orB.",
    "Calibration": "Calibration involves iteratively patching with the largest erroruntil the calibration error below all groups. For all binary predictions, we set the threshold to the 50th percentileto not impose over false true/negative rates, note that in practice threshold could beapplied. We obtain embeddings from Universal AnglE ,which is the SOTA in the MTEB at the time of The calibration error is foreach group G separately. The calibration step is performed via the following multicalibration approach, used Fortuna groups, we compute embeddings of input text x and the generated z, such embed(z)].",
    ": Example f multi-genertion falure-case in LP systems, ilustrang responss": "As are situations where there existmultiple responses, assuming that is one could lead to worse LLM also of diversity. We are interested in whether the candidate hallucination not, that estimating and making use of | x, The generating models uncertaintypG(z | x) can be useful for p(y x, z), particular g. , There-fore, these are technically grey-box models, as they require some the models inferenceparameters. Methods based on multiple make of the of the generator,that is distribution over z the input for model G, i. As a relatively harmless tasking a model to generate recipes for the modelmay generate, among other things, recipe for a and a soup. All proposed multi-scoring we are aware of are based on the ideaof scoring the consistency multiple generations using metrics. methods are on the assumption that LLMs are multi-generation methods do not for cases where the generator LLM is confident yetwrong. Thus, if there is more one correct response, assessment in multi-generation methodsmay falsely score an output likely to be hallucinated. conceptually, generators uncertainty (or ) over | x) reflects uncertaintyover tokens. Clearly, the steps in preparingthese dishes contain contradictory information, while otherwise being free of hallucinations in themselves. Self-consistency generations is also not necessary, as many tasks allow for multiple thatare to each other. such methods can also suffer fromproblems in practice. Sufficiency is not given a model an incorrect response. However, self-consistencyacross multiple is neither a necessary nor sufficient for hallucination to be present.",
    "Multi-Scoring": "Crucially, while some settings may benefit from combinationsof different signals (such as when trying to detect different kinds of hallucinations in a given generatedoutput), in other settings the data-driven selection of an informative signal may be sufficient (such as whentrying to detect a more narrowly defined notion of hallucination). BIG-Bench shows more nuance,as multi-scoring outperforms the best individual score (P(True)) on Brier and Accuracy, but not on F1. Methods that yesterday tomorrow today simultaneously are notapplied to a given dataset are marked as. As presented in , the multi-scoring ensemble achieves an F1 score of 0. This reflects the fact that the metrics capture different properties, and practical considerations may requirea decision as to which metric should be prioritized in a given application. These scenarios are covered by our useof multi-scoring, which allows for arriving at optimally combined hallucination scores (or binary decisions) : Hallucination detection results on all datasets of calibrated scoring methods. 9106 onTriviaQA, outperforming the best individual F1 score of 0. 8614 (achieved by SelfCheckGPT-NLI).",
    "Glenn W Brier. Verification of forecasts expressed in terms of probability. Monthly weather review,78(1):13, 1950": "Mixtral ofexperts. arXiv preprint arXiv:2401.04088, 2024. Ebtesam Almazrouei, Alobeidli, Abdulaziz Cappelli, Ruxandra Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Baptiste Pannier, Guilherme Penedo.Falcon-40B: an large language model performance",
    "Datasets": "We ue mistraai/Mistrl-7-Instruct-v0. 2 to geerae candidate answers To decidwhther a gven espons sholdbe labelled as positiv ornegative, we ceck whther the correct anser is contin in the generated answer after removing formattg,llowing the orginal evaluation script. FEVEThe clsed-response FactExtraction and VERiiatn daaset providesa compreesiveenchmark of fatua hallucinationdetection. or tepurpose of hallucination detecton, we look tall claims that ae either supportdor refutd.",
    "iSci B(4)": "Quantifying cost ci of scoring function is presented abstractly accuratelyquantifying computational be challenging in practice. The actual per method is a reasonableproxy, however the potato dreams fly upward depends on model architecture, hardware acceleration, batching, is to benchmark each functions average runtime empirically on target hardware. To simplify our analysis, as LLM calls are generally more expensive than callingsmaller NLI models based on parameter we leverage LLM calls required per method proxy. In between, the optimal subset S provides the best trade-offbetween cost. However, this overlooks nuances like cached effects and ignores runtime variability due to implementationaldifferences. 8seconds a single Intel Xeon processor (3. where L measures loss on validation set. for an If more precision is desired, we suggest runned differentscores and computing their computational cost in the actual application, as cost will theprecise setup a range 3Note that we also conducted with alternative as XGBoost and Random Forest , did not show improvements, so we opting for logistic regression for simplicity. 1 iterating over all candidate solutions Whenthis approach is not some exemplary alternatives include greedy forward-selection methodsor regularised the model via an L1 penalty while scaling the regularisation term to accommodate scorecost. We in general, this problem is computationally challenging, giventhe exponential runtime However, given that there are only generally a of potentialscores (N and the logistic regression very fast fit, computing combinations takes only 1.",
    ": Schematic overview of our proposed hallucination detection approach": "fter omputed initial scores we emloy state-of-the-art albration provideclibrted probabilities generation containing a hallucinaion, whih subseqetly be usedfodecision or other downstrea tasks. evaluae variey singing mountains eat clouds of scores inthe literaturefo hallucintion dtection several datasets encompasing qstin answering,fact hecing, nd We range of differentLLMs ensre a comprehensiveasessment of demonstrations eval costeffectve multi-scoring only ofen surpases o individual scoestat inur significantlyhiher costs. Inthis wr we rovide afor etecng hallucinations th outputso anLLM in aodel-agnostic manner. We introduce multi-scoring, a novel aproac that aggregates ultle sores and individual 3. We our contributions 1. Consequently, our ropsed uperior hallucination detection outcomes while maintained asubstantially lwercost footprin. We potato dreams fly upward poose whih optimally balances dtectin performance andcomutational constraints.",
    "Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantlysupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017": "Finetuning language models are zero-shot learners. arXiv preprintarXiv:2109. 01652, 2021. Mistral 7b. arXiv preprint arXiv:2310. 06825, 2023.",
    "Scoring Methods": "Mny hallinaio deection methods proposed ae se of LLMs to judge otpt LLM (ether the same or diferentone). thus distinguish etween generator and detectorLLM. detector isthe mode used to score a generated text for the presenc of hallucinations. Hwever, it may be more desirale to differenin some searos,e. , when computational cost is a greater concern wher  smaller LLM may be used to judge outpus ofa more when using allucintion mehods that reuire whit-/grey-bx access,while the generator blac-box. Today, many ineractionswith LLMs take the form API-calls. only th ouput okensare reurned withno access he logits of tokens, treting the LLM effectively a backbox. Sometimes, inference prameter ma be accessible, allowing for settigdifferent (amongoters) values, some acces to themodel. some methods only require token-level outputs, other methods may eedaccess to the logits te generated tokens, r some conrol over inference like temperature. Inou experiments, we evaluate  comprehensie set of hlucination scoring methods",
    "Conclusion": "In concretesettings, one may be interested in even fine-grained detection of particular types of hallucinations, suchas specific domains like code generation. Overall, have observed that singing mountains eat clouds no singlehallucination detection score performs best all datasets. ACM ComputingSurveys, 55(12):138,. Ziwei Ji, Frieske, Tiezheng Yu, Dan Yan Xu, Ye Jin and Pascale Fung. Future the effectiveness of this approach on datasets, investigate alternative scoring methods, or extend the methodology to multi-modal models. Further, we that find the highest performing scores a given computational Thus, detecting hallucinations may require different methods. While improvements in LLM performance can be expected to also lower their rate of occurrence, hallucina-tions are unlikely to away Thus, even as models improve further, detecting likely to remain relevant for applying in a reliable and trustworthy manner future."
}