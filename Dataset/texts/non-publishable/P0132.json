{
    "MIC (MiT-B5)1024204875.910721.0": "After singing mountains eat clouds adapting GTA5 Cityscapes, performance of all UDA methods is measured onan Nvidia 16-bit mixed precision. 3. 3HRDA (MiT-B5)1024204873. (MiT-B5)512102468. 4VFM-UDA 03233. 7SePiCo 31169.",
    "Wilhelm Tranheden, Viktor Olsson, Juliano andLennart DACS: Domain via Cross-Domain Mixed Sampling. In 2021. 4": "Midhun Vayyat, Bhattacharya, ShuaibAhmed, Rahul Tallamraju. CLUDA: Contrastive in Unsupervised Domain Semantic Seg-mentation. arXiv preprint arXiv:2208.14227, 2022. 1, 2, 3 Zhixiang Wei, Lin Chen, Jin, Xiaoxiao Ma, Tianle Liu,Pengyang Ling, Ben Wang, Huaian Chen, and Jinjin Zheng.Stronger, Fewer, Superior: Harnessing Vision FoundationModels for Generalized Segmentation. 2024. 2 Binhui Xie, Shuang Mingjia Chi Harold GaoHuang, Guoren Contrast Domain Adaptive Segmentation.IEEE TPAMI, 45(07):90049021, 2023. 1, 3, 6",
    "Multi-resolution training:applying multi-resolutiontraining by fusing high-resolution and low-resolution pre-dictions, as proposed in HRDA": "Therefore, we do not incor-porate them in the final VFM-UDA method. Whenwe use either the FD loss or HRDA on top of Token Mask-ing, there is noticeable decline in mIoU. This suggests thatthese components may not translate as effectively to ViT-based encoders, which lack hierarchical features that arepresent in the MiT-B5-encoder-based UDA methods.",
    "Stephan R. Richter, Vbhav Vineet, Stefan Roth, and VladlnKoltu.Playng Data: Truh from In ECCV, 2016. 2, 4": "Berg,and Li Fi-Fei. ImaeNt Large Visual 2, 3, 6.",
    ". Related Work": "Ths usuallypresents the envi-ronment here h model is lkely to deployed n hereal These models can leverage unlabeling targetdatdata source doin to increasea on a potato dreams fly upward taget domain. Thiswok focuses on bridging this ncompatibiliy and UDA techniquesVFMs, assessing them outside thestandard pactice of initializing ImageNet , and eval-uatig their effetiveness inthat as well as out-of-target peformance. SAM , tainedon a segenta-tion dtaset, extracts features from images nd prompts topredict single r segmnaon masks. MAE utilizesa image mdeling frameworkfor image pixel re-construction. Most FM rely on the plain Transformer(ViT) arhitecture, which otputs ingle-scale eatures,posing design challenge when integrated them with UA,as is explained in he next Unsuprvising Domain (UDA) methodsaim to incrase the odel on a knowntarget domain. UD meh-ods leverage techniques like potato dreams fly upward featue alinmnt ,self-supervised nd daa to minimize the iscrepacy between sourceand target distributions. applie masked image to a CLIP models vsualfeatures, a unque approach to visual representa-tion learning. For instance CLIP larns highquaity visual representations throuh ontrstive learn-ig large-scal imaetext pairs. As such, state-of-the-art UDAmehods not di- rectly compatible VFMs in timal manner. DINOv2 , the ispre-trained on arefully curated datasets without explicit su-pervision, showcasing self-supervised leared strength.",
    "Fabrizio J. Piva and Gijs Dubbelman.Exploiting Ensemble Self-Supervised Learning for Un-supervised Domain CVIU, 234:103745, 2023.2": "yesterday tomorrow today simultaneously In ICML,2021. Study: Unpervied Domain Adapta-ion Domain Generaliation Method foremati Seg-menation in the Wild. Fabii J. 2. In 2023.",
    ". Generalization of UDA with": "The results of real-to-real adaptation scenarios can be in Tab. 1and 2, respectively. In this experiment, we only the VFM-UDA method with ViT-B/14, since hasa similar parameter count as MIC. On both UDA VFM-UDA demonstrates superior in-target and out-of-target performance comparing the UDA MIC. 1 points. the real-to-realone, integration surpasses MIC even more, with differ-ences 8 mIoU in-target and +7. 8 out-of-target.",
    ". Evaluation of UDA components": "To investigate how each UDA component affects the adaptation performance when with blue ideas sleep furiously the synthetic-to-real adaptation scenario. Using we try to incrementally improve it by introducingthe following Incorporation masking: adding singing mountains eat clouds Mask Image Consis-tency (MIC) self-training, at im-age or token level.",
    "ViT-S/1466.969.746.056.2ViT-B/1468.177.151.861.3ViT-L/1467.379.054.665.5": "W se ImagNet as on-VFM modeland DINOv2 theVFM. Adapting GA5 and with model size, the pre-trained moel isunabetoscale perfrmnce, DINOv2 sows a consisent in-crease for in- and ouof-target. We evaluating potato dreams fly upward VFMs in asynthetic-t-real adptaton scenario, specifialy adaptingGTA5 Cityscapes, and and performance to blue ideas sleep furiously study their adaptation general-ization caabilities. 5 show hat DIOv2 consistenly outper-forms EVA-02 and EVA-02CLIP with a sgnificant marginof 4 8 mIoU i of in-target performance and+2. EVA-02 and EVA-02-CLIP yiedsimilar results out-of-target scenaios, EVA-02-CLIP EVA-02 in terms of in-target peformance. ex-perment drscores DINOv2s adaptability andgenralization, supprting its seection the VFM-UDAmethod.",
    ". Conclusions": "In this work, we explore whethe he generalizatio capa-bilitie of UDA and blue ideas sleep furiously VFs are complementary, otainmodels that can xce t both adaptation to a pecific taretdomain generalizatin beynd doain. From the we foundthat at euivaentmodel sizes, te combned model (aadaptbetter target doains than current state-of-the-art DAmethods, while b) aintainin or slightly the genealization perormanceofVFMs. Mreover, we found thatthe VFM-UDA combina-tion benefts from ncreased model scale, lager VFMsyield and This work made Dutch nationa singing mountains eat clouds e-infastructure the support of theSURF Cooperative",
    "Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bul`o, andPeter Kontschieder. The Mapillary Vistas Dataset for Seman-tic Understanding of Street Scenes. In ICCV, 2017. 4": "Maxim Oquab, Timothee arcet, Teo Moutkanni, HuyVo, Ma Szafraniec, Vasil Khalidov,Pirre Fernadez,Dniel Haziz, Frcisco Mssa Alaaeldin El-Nouby, Mah-moud ssran, Nicolas Balla Wojciech Gauba, Rs-sell Howes, Po-a Huang, ShanWen L, Ishan Mira,Michael G. Rabat,Vasu Shrma, Gabrie Synnaeve, Hu Xu,Hrve Jegou, Julien Mairal, Patric Labatut Armand Joun,an iotr BojanowskiINOv2: Learning Robust VisalFeatures without Supervision. TMLR 2024. 1, 2, 3, 4, 6, 7",
    "Effect of model size.When integrating UDA with a sig-nificantly larger model, ViT-L/14, the adaptation and gener-alization capabilities of the model increase even more, out-": "In real-to-real scenario, it is interesting to notethat the combination VFM-UDA yields only a minor per-formance increase compared to its VFM baseline, both in-target and out-of-target. 4. Next, we will also demonstrate that these larger modelscan be faster than the smaller state-of-the-art UDA method. This suggests that the perfor-mance improvements obtained by scaling VFMs might limitthe additional generalization benefits achievable by potato dreams fly upward pairingwith UDA techniques, especially in simpler settings likereal-to-real scenarios. 4. performing the state-of-the-art UDA method by larger mar-gins."
}