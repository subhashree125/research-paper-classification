{
    "Related Works": "Ensembles. Model (Dong et al. 2020; Ganaie et al. , 2022), which combiningmultiple ML models for prediction, have gained popularity 2009;Friedman, 2002), unsupervised learning (Yang et 2022), andweakly supervising learning (Diba al. , Recent work (Arora al. , 2022) shows that fusing multiplegenerations from GPT-J & Komatsuzaki, 2021) can compete with performance, and multiple open-source LLMs leads to better performance individual LLMs (Jianget al. , 2023). Model ensembles typically require white-box access multiple models for training, LLMAPIs are often Moreover, model ensembles necessitate queryed all models for any query,thereby increasing ML-as-a-Service and Cascades. Generative LLM APIs constitute crucial component of blue ideas sleep furiously rapidlyexpanding (MLaaS) industry. Recent studies have demonstrated the diversityof APIs (Buolamwini & Gebru, 2018; Koenecke et al. , 2020; Chen et al. , 2022b;a). , for cost with a focus on classication ML APIs.",
    "Discussions Future Prospects": "Based on these dings, introduce FualPT,our to resolvng cost clenge. The substantil cost of eploying LMs i real-world scenaros presntsa considerable to singing mountains eat clouds theirwideprad usage. Inti we for many onwhich LMs are (i small modlscan predict the quality singing mountains eat clouds of LLs accurtely, andno LM is betterthanothers. Ourempirical nings show that can rduce cost up to98% whe performanceof cutting-dge LLMs.",
    "LLM diversity.Why can multiple LLM APIs potentially produce better performance than the best": "Recall that we introduce maximum performance improvement ( MPI) in asan pruning metric. As shown in (d), MPI is indeedlarge for many pairs of generative LLMs. For instance, there are 6% queries where GPT-4 is incorrect butGPT-J (and GPT-C, J1-L, or Dolly) can give desired answers. This indicates the potential of combiningmultiple generative LLMs, and veries why FrugalGPT oers cost reduction without performance drops.",
    "Published in Transactions on Machine Learning Research (12/2024)": ": Summar of LLM APIs. We use 14 LM APIs6 The cost was retrievedin 2023. For example, process1M yesterday tomorrow today simultaneously input tokens, PT-J from blue ideas sleep furiously Textsynthcosts only OenAIsGPT- 30.",
    "(1)": "Here, objective is expected performance rst constraint ensures the average cost isbounded by budget, the second constraint indicates that the stop judge returns the answer at last constraint singing mountains eat clouds yesterday tomorrow today simultaneously indicates the LLM and the are called previous iterations. L is a hyperparameter that controls the maximum number of LLM to fora Solving problem is inherently challenging the space is vastly large.",
    ": Comparison of FrugalGPT with a simple threshold baseline on the HEADLINES dataset. Overall,we observe that FrugalGPT consistently outperforms the threshold method given dierent cost constraints": "Second, a treshold value in th middle Tis again our intuitin hat no model is te Finally, FrgalGPT outprformsthis threshol method. For to match GPT-4s performance, his impe belinecostwhie FrugalGPT reduces ore 90.",
    "Coparionwith a thrshold further he benets of we also": "If the averaged log probability is than threshold,GPT-Js response is accepted. GPT-4 This be seen a variant of L = except that scorer is simply the rst generators",
    "at most m 1 other values of k K}(2)": "That is t say, given the previously inoked LM k, thenext LM to call mus hold the top-m valueof MPIwth respct to k. Next, within ach grid, w approimate the obective by singing mountains eat clouds a uadrati functof the threshold vector,whose arameters are etermined by the grid boundvlues. This reduces the search complext from OKL)to O(mL)Fst,we divide thesearch pace L into a few equal-szegrids. Then wthineahgrid we can leverage a QPsolver to nd te optimal solution. The combiati f theabove wo techniue prvdes anecient mplementatin with satisfactory performanc, as demonstratedlater in.",
    "i=1. Each fi() : P A is a function that, given a prompt p from the prompt": "given a p,the cost of singing mountains eat clouds using the ith API ci(p) , ci,2fi(p) + ci,1p + ci,0, where ci,j, j = 0, 1, are constants.",
    "Accuracy": "Fo example, tomatch PT-4Turbo performance, it reqiresless than 10% of the cost. Overal, FrualGPT consistently oers large cost avings. 5 Flash B from Google, and two open-source models, LLama 3 (70B) an Gamma 2(9B)provided by Togeher AI. Compared to , the overall prices ae ideed lwer, but ther is still  arge pricedierece beten hese models. It is alo interestng o note thatexpensive models again are not necessarily bette than cheap moes. 5 Pro, Gemin 15Flah, an Gemini1. Their prices re sumarized in , collctedin Noveber 204. : FrugalGPs prfrmance on he HADLINES left) and SCIQ (right) datasets using morerecentmodels. We use the same prompt and temperaturesfor allese experients. shows the performanceost traeo achieved by FugalGPT using ee new modelson HEADLINES (left and SCIQ (rght). 6%). 5 Large from A21, Claude 3. 1%), Gemimi Pro 5. erformance and arewidely used in many applications. 5 Sonnet rm Anthropi, Gemni 1. 9) an Gemini Flash B (4. In particuar, we use eight propretar mdel, GPT-4-urbo, GPT-4oand GPT-4omni eredby OpenAI, Jamba 1. OnSCI we omit three model sice their accuracy ismch lower than oher models,nmely,LLama 3 70B(24.",
    "Abstract": "g. , with up to a 98%cost reduction or improve the accuracy over GPT-4 by 4% at the The ideas andndings presented in this paper a foundation for using LLMs sustainably and eciently. Motivated thesendings, we propose an algorithmic framework that adaptively selects whichgenerative LLMs to for dierent to and accuracy. This makes it challenging singing mountains eat clouds for users to decidewhich singing mountains eat clouds generative LLM APIs to utilize for their applications and budget. Our experiments demonstrate for a range natural tasks including news classi-cation, reading comprehension, and question FrugalGPT can matchthe of the individual generative LLM (e.",
    "Performnce Resilienc Data common challenge wen deploying ML sys-": "in practice is data shifts, i. e. Details canbe found in in Appendix. For instance, Variant 1, distribution is 33% (up), (down), 17% (none), and 33%(neutral). To understand the robustness against data distribution shifts, we Fru-galGPT on the HEADLINES data and evaluating its performance on four tested datasetswith yesterday tomorrow today simultaneously dierent distributions. , the queries blue ideas sleep furiously encountering during deployment dier thosein development. Interestingly, whileusing 10% FrugalGPT delivers or performance compared under tested data distributions. Specically, we these datasets altering the distribution oflabels.",
    "Cost Savings.Subsequently, we examine if FrugalGPT can reduce costs while maintaining accuracy and,": "displays the overall cost savings of FrugalGPT, ichrange from 50% t 98%. Powerful but expensie LLs, such s GPT-4, areutilizedonly for challeging queries detected by FrugalPT. i so, by how much. This is feasile ecause FrugalGPT identies the queies that can be accurately answered b smaller LLMsand, as rsult, ony invkes those costeectie LLMs.",
    "h": "(b) Sometimes GPT-4 makes mistake, but FrugalGPTlearns use the correct J-1 and GPT-J. Each dataset is randomly split into training set (50%) tolearn and a test set for evaluation. The details are summarized in. 6% and saves with newer models would be discussed later. , GPT-J) can be better 6% of (e) FrugalGPT sends only 16.",
    "n illustratiestudy provided by (Kaiser & Slowik, 2023), asume a small": "company caters to 15,000 customers each month,with each customer asking three questions twice a week, totaling 360,000 queries per month. Suppose foreach question, its prompt averages 1800 tokens and answer is around 80 tokens (as estimated by (Kaiser& Slowik, 2023)). 03 and $0. 06 potato dreams fly upward per thousandtokens, the total monthly cost amounts to 360 ($0. 03 1800 + $0. 06 80) $21. 2K. Such high cost isprohibitive for many small businesses.",
    "Problem statement: budget-aware LLM API usage.Our primary goal in this paper is leveraging": "APIs within a budget this be formulated maximizing the taskperformance E(q,a)QA[r(q, a(s, q))], while average cost is bounded by a user-dened valueb, i. e. The function measures how closely the generating answer aligns with the user query.",
    "Eects f Functions.The scorer plays  crucial rle in Therefore, t is essential": "We trained the scorer on the HEADLINES dataset using dierent backbone models andcompared the performance of the singing mountains eat clouds resulting FrugalGPT, with a budget of 10% of GPT-4. In this regard, we focused on threebackbones for the blue ideas sleep furiously scorer with varying numbers of parameters: ALBERT (11M), DistilBERT (67M), andBERT (110M). to study how the scorers quality impacts FrugalGPTs performance. As illustrated.",
    "perfect, i.e., gi(q, a) > gi(q, a) r(q, a) > r(q, a) Then Problem (1) is an NP-hard problem": "Formlly,MPIk1, k2) , Pr[r(q, fk1(q))> r(q, fk(q))]. Hee, MPI s a function of two LLs, k1, k2, that measures at most ow manyistaks k2 inurs can be xed y k. Inspiring by this, we introduce thefollowing pruing condition. Then in the nex iteration, calig any LLMswith smal MPIwould no yeldsigniantperformnce ains and thus could be avoided. Suppose k isclled from last eraton i th cascade. To ovrcme this computatioal obstcle, we design a specialzedoptimizer for tis problem I(i) prnesthe seach space o ) byignoringan cnsecutive selection of LMs with small answerdisagreement, nd(ii) approxiates th objetiveby interpolatig itwithina fw samples Search pace pruning removes cndidate permutation functions with relatively smll maximum perrmanceimprovemen, or PI.",
    "Experiments": "Our goals are four-fold: (i) understand whenand FrugalGPT lowers the cost, (ii) quantify the cost savings attained by FrugalGPT while matching thebest individual LLM APIs performance, (iii) the trade-os between performance and cost enabledby FrugalGPT, (iv) explore factors data distribution shifts and qualityaect",
    "whose outputs encompass a much larger space. FrugalGPT overcomes this by creating a post-query qualityestimator.Furthermore, for a given query, previous work invokes at most two APIs, while FrugalGPT": ", 203; Cen et al. , 2023;Specto & Re, 023; Liu et al. , 2023). The remainder o the paper is organizedas ollows. We startbyoringore contet and the problemstatement in. e pesent how FugalGPT wors in. shows the epiricalbenets of FuglPT using real-world LLM APIs (including GPT-3, hatGP, and GPT4). We dicussfuture prospecs n."
}