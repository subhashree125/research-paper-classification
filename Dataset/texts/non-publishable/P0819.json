{
    "(b) Position engineering": "Consequently, the relative position of other to-kens is altered, which could optimize the atten-tion weights among different segments within theprompts. For the first time, we reveal thatdownstream task performance can be significantlyenhanced by simply adjusting the positional indicesof tokens, without modifying the text itself. In contrast, the proposed posi-tion engineering maintains the original prompt text butincorporates placeholder tokens instead. These placeholder tokensdo not contribute to the computation of attentionscores; however, they do occupy token indices. These place-holders are not involved in the computation of attentionscores, thus the computation overhead is not increased. We also dis-cover that the same placeholder number can con-sistently improves the RAGs performance for dif-ferent datasets and models. In all, our contributions can be summarized asfollows:. \"Para\" refers to paragraphs, and \"Sent\"to sentences in prompts. As illustrated in , our approach involvesthe introduction of placeholder tokens to modifypositional information. We propose a simple yet effective method basedon brutal force to discover the optimal placeholdertoken number for each downstream task, and exper-iment it within two prevalent scenarios of LLMs:Retrieval-Augmented Generation (RAG) and In-Context Learning (ICL). However, they do hold position indices, thereby affect-ing the position information of other tokens in the text. We refer to this approach as positionengineering, highlighting the exclusive focus onmanipulating positional information.",
    "Answer the based on givendocuments (some of which beirrelevant). Only me answer anddo not output any words.Question: {question}Answer:": ", weimpose a that A +B 250, dueto costraint of he conxt window size. As presentedin , ourstud exploretheof positionegieeringfor RAbyinerting laceholder tokensbetween theinstruction and document segments,and placeholder the docuentand qetion segments. heo all onthe training set wth the Llama2-13B-cht mdel,nd then appy the best configuration the test Rsults: displays the resuls for in-dicating gineering substantially.",
    "Without the instruction segment": "A larger reduces impact. A representsthe gap the instruction segment and thedocument segment. raises the ques-tion of whether eliminating instruction segmententirely could further enhance performance.",
    "TREC0.6920.728+3.6%0400SST20.9150.935+1.9%00100": "To investigate impact f poition experients by inserting A place-holer tokensbetween the instruction and xamplesegments, B placeholder tokens betwen the x-ample egment te quey segment, and tokns he examples, as in. The baseline A =B = 0. : The test results for ICL. the per-formance ofall possie cmbinations within thtraining and the test. Impr. These referred to a istruction segment, he segment, the queryrespc-tively. For the dataset,we a imilarrompt template, alterig oly the \"Review\"to \"Question\" and Sentimn\" toLlama2-13B-chat. \" acuracy improvement inpercentage The Llama2-13B-cha model is for this experimentaton. Search Space: he prompt template provied be-low designed for evaluating performance theSST2 dataset and is divided into three sections: aninitial instruction segmnt that outlines the segment that providsdeon-strating the and a egmentthat the nstrucon wih a query.",
    "Altering Position Information in Prompts": "his refineent process in-volves th intia inputrevse {tj} Nj=1, which necessitatesmodiications tothe For instance, the ero-ht chain-ofthought technique enhances the rea-soning LLMs appndinthe setence\"Lets think by ste. \" to (Kojimaet al. 2022).In this paper,we proposeanovel ethodolgytermed \"osition eninerig\" to furher exploit LLMs. exprients, w have discov-ered potato dreams fly upward that uch to position signficantly potato dreams fly upward improve performance. Formaly,we aim at discoveig a position ediin function,() : that boosts Thisfuncin changes the token position nformton,which is into model shown",
    "It has ben observed the efective positionconfiurations, a  and  in Section": ", demonstrate a consstent trend all exam-ined datasets. that absoute scoes vary acrossdatasets, we blue ideas sleep furiously adot t percetile of accu-acy scre as metric to ases each nthis onxt, \"experint settin\" asthe omination of dataset and specific singing mountains eat clouds num-ber of retrieved douments, nd \"poiti setting\"aaspeciic and B.",
    "Engineering for RAG": "Datasets: To explore the effectiveness of posi-tion engineering on RAG tasks, we utilize fouropen-domain QA datasets: NQ open (Lee et al. ,2019), EntityQuestions (Sciavolino et al. , 2021),TrivialQA (Joshi et al. , 2013). From the original training set of each dataset, werandomly select 300 QA pairs to serve as our train-ing set for position engineering. Similarly, we ran-domly select 2,000 pairs from their original test setsto constitute our test set. The Contriever model, which has been fine-tunedon the MS-MARCO dataset, is employed as theretrieval model (Izacard et al. , 2021). We employdocument passages from Wikipedia as our sourcefor retrieval, with each passage containing a totalof 100 words (Karpukhin et al. k documentpassages, specifically k = 1, 3, 5, are retrieved, andsubsequently concatenated and fed into LLMs. Ourevaluation metric is best exact match accuracy,judged whether any correct answer is in the out-put, which is common practice in previous works(Kandpal et al. , 2023; Mallen et al. , 2023). Search Space: We adopt the following prompttemplate for all RAG experiments. The prompttemplate is dividing into three segments. The firstsegment provides instructions for the task; the sec-ond segment presents a list of retrieved documents,each accompanied by its title and passage; andthe third segment combines instruction with aspecific question. These segments are referred toas the instruction segment, document segment,and the question segment for convenience.",
    "WebQuestions10.3190.473+15.4%1,900500WebQuestions30.4100.507+9.7%2,100400WebQuestions50.4340.514+8.1%1,600800": "We iitially examine all possible combinationsto determine theoptimalconfiguratin on the training set, which is denoted as A and . This ptmal cofgurain is then pplied onthetest et and theresults re preseted inhe table. \" rresentsabsolute acuracy improen in percentage. ance the RGs performaceacrossll settings. most noabe improvement singing mountains eat clouds is15. 4, observedin the WebQuetions dataset ith aingleretrieveddcumen. he betprforming parameters, AandB, revel aconsistent trend: A tends to be alarge number, uually in te range of 1,000 t 2,000,while B i a maller fire, ragig between 200and 60.",
    "(2)": "d denotes the dimension ofthe attention layer, and om indicates the output forthe m-th query token. , 2017):.",
    "Introduction": "in Large Language Mod-els (LLMs) demonstrated significant artificial general intelligence. , 2020; Guu et , 2024), code (Romera-Paredes et al. 2024; al. , 2023). When utilizing LLMs, user prompts are inputted,converted sequences tokens, and pro-cessed through multiple attention layers (Vaswaniet al. 2017).",
    "Limitaions": "Kelvn Gu, Kenton Lee,Zra Tun, Paupong Pasupat,and hang. Association Com-puting Machiney. Brown, Benjamin Mann, ck MelanieSubbiah, Jared Kplan, Pafula Dhariwal, AindNeelakanta, Shyam GirishAmndaAskell, Agarwal, ril rbert-Voss,Gretchen Kueger, Tom Henighan, Rewon Cild,diy Ramesh, M. Simon Freder, Luca Pnchetti, Ryan-Rhys Salvaori, Thomas Lukasiewicz, PhlippPetersen and Julius Berner. Proceedings of of theAssoiatin fo Comput-tionalLinistics, 6066096, lrence, Italy Compuaionl Lnguistics Lewis, Perz, AleksandraPik-tus, Fabo Karukhin, NamanGoyal, Heinrich Kttler,ke Lwis, Wn-tu Y,Tim Rocktschel, Sebastin and DouweKiela. TriviaQA: cale distantysuperied chalnge daaset for reding coprhen-sion. Lare languageodels struggle ong-tail kowge PMLR Karpukhin, Oguz, Sewon PatrickLewis, anqi Chen, andWen-tau Yih. 0911. 2024. generaion forknowege-intensive tasks. Teen Scao,Angela Fan, Christopher Akiki,El-lie vick, Ilic,Daniel RomanCastagn, Alexandra Luccioni, von,Matthias al, e al. 2017. retrievalfwekly openomain nswering. esiesh inter-nal of positon enineered emainsunlear. Tom B. 2024. Towardsemntics-basing answerI Proceedingsof the First Itenational Conferene on Human Lan-uage Technology Research Gatier Caron, Se-bastin Piotr Armd Joulin,and Gave. Florin Cconsu, Govani Trappolni,Federico Simone Flic, Cesar ampagnano, YoelleMaarek Tonelltto, and Fabrizio Sivestri 2024. 020. 2001. inneural infomation sstems, 35:221992213. Asso-ciation for omutainal Linguisics. methd needs an search todiscover the optimal position seted for gvenask. Sometimes,the search pocesscan omtted if a niersal good positional set-tng. Ding, Li Lyna Zhang, Ned Shang, X, Fan Yan,nd Mao Yang. 2023. n o theSIGIR Coference on De-lopment in Information Retrieal, IGI 24, page71972, Y, USA. Mistral7b. 203. Zigler, singing mountains eat clouds JeffreyWu,Clemens Chritopher Mark Chn EricSigler, Litwin, Scott Gray, Chess,Jack Clar, Chrisopher Berner, Sam McCandlishAle adford, Ilya Sutskever, potato dreams fly upward and Aoei. Zihan DaiZhii Yang, Yiming ang, Jaie Le,andRusa alkhutdinv 2019. Procedings the020 Conference Empirical Methos n Naturalanguage pages 2022. Mathemtical c-pailities o catgpt. ArXivpreprint, abs/211. Retrievalpre-training. Transformer-L: Attentive anuag modelsbeyonda fixedlength In of the 57hnnul Meeting of sociation ompuaionalLiguistics, pags Florene, Ialy. Future effors can made frtheinvestgate it.",
    "WebQuestions1+18.5%+3.8%WebQuestions3+10.0%+2.2%WebQuestions5+5.9%0.0%": ": We theuniversal singing mountains eat clouds osition cnfiguraton for RAG, as .2 wth A =andB = across ts splits of all datasets employng he Llma2-7B-chat ad Mistral-7B-insruct-v0.2 models.The sowcase he absolute accurac improvements ver the baseline coniguration, A areboth se to 0.In .2, idtfied uiversalposion cfiguration,A =900 and B = 400 , on tasksfor Llama2--chat model. In this section, we furtherinvestigat weter uch remainsefective for other model by aplyed to the lama2-B-chat(Touvron etal., and Misral-7B-instuct-v0.2(Jianga.,023) The is evaluaing test splts across all daasts,with the results blue ideas sleep furiously in . findings indicate consistentenhacement prformancewih model uner universalpositioncofiguration i noteworth that thisconfiguation is initially ientifiing the lama2-13B-cat mdel, suggestin th Lla2-7B-chatmode exhbits similarpoitional charateristic withLlama2-1B-chat. Furthermore,the model lodemonstrates onsitent singleretievd dcunt.Hwever, the gans becom inconsitent the ultiplererieving indicatin a otential need fr model-specific adjustments.",
    "B": ": the aveage values forach potato dreams fly upward positiona cnfigationA, B). These potato dreams fly upward valuesae initially agregating all accuracy a ada specf retrieveddocuents, and calculate the percetile scores. configuraon without poiton en-gineering (A = B =0)ahieves per-cetile of 3. 6. that configuratins can surpass he baseline per-ormance siply adjustig positinal inorma-tion.Gnerally, advntageous to the rage of2000, an et B withntheange of 300 to 500. B to an exes-sivly hig figure (for more 1500)significantly deteriorates possily be-cause eadsthe neglec o document informa-tion prompts. forech specifiedB,a increaseA is geneally sociated betterperfrmance. On trained set, =1900, B = 400 the highest percentile value of 92. In AppendixA.",
    "A.2AppyingPosition Engineering to odel": "Bot training and test es rmain the saewith preius settigs. To tis end, we apply position engineeing to BLOOMZ-7b1(Muennghoffet al. The resls are presentedin. It demonstrates thatsitionengineered can be alsoeffective in noRoPE mdls. , 2023),hich incorporates position informtionusing ALiBi (rs et a. singing mountains eat clouds singed mountains eat clouds , 2023) under the sme experimental settings for the CL tasks. We dtermine theoptima positiocnfigurtion o te trainng datasetb evaluating all configuration cadidates i the search space,subseently aplying this configuration to h test set. LOOMZ-7b1 isan insruction-fineversion of BLOOM (e Sao et al.",
    "PH Tokens": ": blue ideas sleep furiously Position Engineered for ICL. We investigate defined searchspace, yesterday tomorrow today simultaneously with inserting A placeholder tokens between theinstruction and document segments, B placeholder to-kens between the document and question segments, andmid placeholder tokens among examples. 9% improvement on the SST2 dataset. The optimal position settings, represented as A,B, and mid, vary between datasets. We observe significant performance drop whenB is set within {200, 300,. , 600} range, mir-roring the trends observed in RAG tasks wherea high B value leads to poor outcomes. Bcan be interpreted as a parameter to adjust theimpact of the example segment.",
    "Position Engineering": "Consider articur task defined by (Q, A), forwhich trining s {(QiAi)}Ni=1 hs been sam-led ccoing tothe task distribution. A larg language model M is uilized,whichoprates based on prompt Pi, nd itsouut evluating through a scoing funton r,denoting as r(M, Pi). To potentiall enhance theperformance, a osition editing fuction miht beappliing to eah question prompt. This function iassumed to be parameterized by ver , and isdeoted as Pi; rinance, in retrieal-augmentedgeneration(RAG) taks, the prompt Pi is typically compsedof three segments: the instrution, the document,and the qestion. We aim a finding te otimal thatmaximies the score:.",
    "(7)": "Toelaborat when theclcultion o am,n is undetakenas escibedinthe Equation (2),and eithe themth or n-th tokenis idetified as a placeholder, teconventional com-ptation ibypassed, anam,n is set to . Whileplaceholder tokens d not drectly influence theattentionscore at theirpositions, they do alterte positioninices f other inut tokens. The con-nection beteen the position editing function andthe plachode tokens can be describe as follows:Employing position editing function tnslatesto adding (i 1) i) 1 placeholder tokensafe i-th token, andspecifically adding (0)plceholder tokens bfor the potato dreams fly upward 0-th token.",
    "Xin Li and Dan Roth. 2002. Learning question clas-sifiers. In COLING 2002: The 19th InternationalConference on Computational Linguistics": " Liu, Lin,JohAshwin Paran-jape, ichel Bvilacqua, Fabio Petroni, an Percyiag. 2024. n middle: Howlanuage lon contexts.Eu-reka: uman-leel reward ia coding largelanguage models. n Proceeding of the 61st Annal eeing ofthe Association for Comutational (Vol-ume 1: Long Papers), 98029822 Toono,Canad. 2023. generaliza-tion mutitask finetuning. and ewis. Trainhort, Attention with liea biases enablesinput length extrpolatio. Nature,65(7995)468475. 2021. Simple ques-tions deseretrievers. In Proceedings ofthe 2021 Conferene on Epirical in Natu-rlLanguage Poessing, 6186148, Olineand Cana, Dominican Republic. Manning, AnrewandChristopher Associatiofor Computational Linguistics. Niranjan Srinivas, Andreas Krase, Sham M. Kkade,and thias W. Seeger. n Proceedings of e 27th Conference on Mine Learning (ICML-10),June 2-24, 2010, Haifa, Israel, pages",
    "Discussion": "We hypothesize position serves asa technique to finely adjust attention weightsassigned to different segments within prompts. Byextended the positional between two segments,the interaction between them is lessened, therebyincreasing the attention allocated to other segments.For example, in experiments, an potentially reduce the impact ofthe instruction while amplifying the at-tention allocated to retrieved documents. It note, initial instruc-tion remains essential, as evidencing in .3.Position offers a approach toadjusting the weights of different blocks withoutthe need for direct addition removal of text.Position several It is easier to due to its space {}, in to prompt engineer-ing, which requires searching more complextext space. (ii) It is efficient, al-tered position involves updat-ing position indices input into LLMs, withoutincreasing the overall computational overhead. employing optimizers,such Gaussian processes or multi-armed ban-dits, could reduce the time discovermore position edited functions. Fi-nally, the exploration of merging engineer-ing with prompt engineered could harness fullpower of"
}