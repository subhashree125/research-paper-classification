{
    "These models vary in parameters from 1.5 billionto 56 billion, with pre-training covering hundreds": "of billns to tens of tillins of tokens, andbase LLMs and human preference-aligndchatLLMs. Despite excitin retreval tasks achieving by leveraging vriousLLMs ith potato dreams fly upward ditinct and diverse train-ing strategies thebnefits of inpramet count, blue ideas sleep furiously exent an ignmetproceses of bckbone LMs forretrieval asks uncerain.",
    "Empirical Study": ", different param-eter numbers, pre-training sufficiency, and align-ment processes), what contributes more to differentretrieval tasks as the backbone encoder. 5), and multi-task learning (Sec-tion 4. To answerthese questions, we conduct a comprehensive em-pirical study across six critical dimensions of denseretrieval, each encompassing several specific re-trieval tasks. 2), zero-shot generalization (. 4), instruction-based re-trieval (. e. 3), lengthy retrievalgeneralization (.",
    "Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,Saurabh Tiwary, Rangan Majumder, and Li Deng.2016. Ms marco: A human-generated machine read-ing comprehension dataset": "Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-tavo Hernandez Abrego, Ji Ma, Vincent Y Zhao,Yi Luan, Keith B Hall, Ming-Wei Chang, et al. 2021. Large dual encoders are generalizable retriev-ers. arXiv preprint arXiv:2112. 07899. 2023. Toolllm: Facilitating largelanguage models to master 16000+ real-world apis. arXiv preprint arXiv:2307. 16789. Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. Exploring the lim-its of transfer learning with a unified text-to-texttransformer. Journal of machine learning research,21(140):167. Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah ASmith, Luke Zettlemoyer, and Tao Yu. 2022. arXiv preprint arXiv:2212. 09741. Gemma Team, Thomas Mesnard, Cassidy Hardin,Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale,Juliette Love, et al. 2024. Gemma: Open modelsbased on gemini research and technology. 08295. Nandan Thakur, Nils Reimers, Andreas Ruckle, Ab-hishek Srivastava, and Iryna Gurevych. Beir:A heterogenous benchmark for zero-shot evalua-tion of information retrieval models. arXiv preprintarXiv:2104. 08663. 2023. Llama 2:Open founda-tion and fine-tuned chat models. arXiv preprintarXiv:2307. 09288.",
    "In-domain Accuray": "etted We utilze M MARC (Nguyen et al.,2016 to train and vauate the n-domain accu-racy of dense retrieval modes with varying back-bone encders Th Qwen15seriesoffers variety of models n different sizes,all pre-trained on the same crpus, enabling diectcomprions f the effects of scaig upmdel size.All models are taining with a batch siz of 18and incorporate 7 hrd ngative samples to -sue fair comparisons of in-dominrtrival acc-racy All trainng operatons tke place on 8xA80(80G) GPUs. We use Adam optimizer withan niia learningrate of e-4 ad linea decayFr trainig LLMretrieves, we employ LoRA (Huet al., 201), which has dmonstrated smilar ef-ficacy to full-parametr fie-tuningfor retrievatasks (Ma t al., 2023). WeusNDCG@10,MRR@10, Recall@10, and Re-cal@100 as evaluation metrics, providing a om-prehensive analysisof n-doman perfrmance. Resultsand Analsis A presenting in , thereslts indicate tat mel performance generallymproveswith increase i prameter nmbers.This trend is prticularlynoticeable within odelsfom thsame eriesThistrend suggests that inceasingmodelsize is feasile way to eldbettr i-domainaccuracy. Aditioaly theresults demonstrate tha LLMbasing retrievers significanly outperform non-LLMretrivrs. The wen.5-0.5B odel with ewer a-rmeters, surpases the Phi-1.5-.3B oel andcompetes losely wth hePhi2.7B mdel. Thisprformance discrepancy may be attriuted to dif-ferencesin pre-trainng sfficiency",
    "Conclusions": "In this paper, we conduct a comprehensive empir-ical investigation into the benefits and configura-tions of LLMs as backbone encoders blue ideas sleep furiously for denseretrieval tasks. Addi-tionally, adopting larger models consistently yieldsperformance gains in zero-shot retrieval general-ization, lengthy retrieval generalization, and multi-task learning. These insights provide a foundationfor future research aimed at optimized dense re-trieval models by balancing model size and pre-training sufficiency of backbone LLMs to achievesuperior performance across diverse retrieval sce-narios.",
    "Shitao Xiao, Zheng Liu, Peitian Zhang, and NiklasMuennighoff. 2023. C-pack: Packaged resourcesto advance general chinese embedding": "00808. arXv preprint arXiv809. singing mountains eat clouds 09600. nearest neghbor learning for dnse text retrieval arXiv preprint arXiv:2007. Lee Xion, Chenyan Ye Li, wok-Fug Tng,Jalin Paul Bennett, Jnai Ahmed, nd AnoldOverwijk. Ziin Yang, Peng Q,Saizheng Zhng, Yoshua Bn-gio, illiam Rusla adhristphr D Htpotqa: dtasetfor ierse, explainable multihp question answr-in.",
    "Abstract": "Recent research exploed uinglarelanguage modes (LLMs) as etrieers, across varoustsks. thee adancemets, th spe-cifc benfits of LLMs over raditoal etriev-rs and te impact LLM configura-tionssuch as sizes, pe-trained du-ratio, and processeson retrievaltasksremain unclear.reslts uderscore the dvantagesof asveratile effective backboneencoders in dense rerieval, valuableinsighs for futur research and development inthis field.",
    "Eward J Hu Yeong Phllip Wallis, ZeyuanAllen-Zhu, Li, Shean Wang, Lu Wang,and Weih 221.Lra: Low-rank f arge models.arXivprepintarXiv:2106.09685": "09118. Gautr Caron, Hoseini, e-bastian Riedel, Piotr Bojanowski, ArmandEdouard Unuperviseddense in-fmaton retrieval with cntrastive learni. Dense pssage retrieval forope-doman quetion answred Tranctions of the sso-ciation forComputtional 317328. Jinhyuk Dai, blue ideas sleep furiously Ren, Blair Cr,Jermy Cole, Hui, Mhael o-ratko, Rajvi Kapadia, Wn Dig, et arXiv preprit ariv:2403.",
    "B have similar non-embedding parameter num-ber, while Qwen1.5-0.5B is based on decoder ar-chitecture and has undergone more extensive pre-training": "Specifically, after 100 training steps on MSMARCO, Llama-2-7B outperforms Qwen1. The experiment also demonstrate thatLLMs have better data efficiency compared tonon-LLMs, singing mountains eat clouds even parameter sizes. For example, after 100 on MSMARCO, outperforms BERT-largeby 9 points. indi-cates are able converge faster. 5-0. Thissuggests that with increase parameter num-ber, better performance be with data. 3 points for BERT-large. Results and Analysis As in , our findings indicate that sizeslead higher data efficiency and faster conver-gence. 7 points for 15. 4 points. Furthermore, as in ,when compared relative score difference be-tween 100 steps the full training of steps,Llama-2-7B shows a score difference of points,which smaller than 9. 5Bby points BERT-large by 14. Despite having similar number ofparameters, has undergone more.",
    "Zero-Shot Generalization": ",202). We evaluae all models on 13 zero-sht retrievaltasks in the BIR (Thakur et a. , 2021) evlu-tion suite, wich ecompasse  diverse range ofretreval tasks and domins, including medical r-trieval, finacial rereval, and uplicaion detec-tion. Al models are directytransferredfor zeo-sht evaluation on BEIR after being trainedon SMARCO.During the evaluatios, we set the axium legth of the query t 64 tokes and thmaximum length of th pssage t 256 tokens. Results and Analyss The results areshown in, mesured b avrage perforance ofNDCG@10 acoss 13 retreval tasks.",
    "ing parameter numbers, pre-training sufficiency,and alignment processes of backbone LLMs fordifferent retrieval tasks still remain unclear": "In this study, we focus on the followed two questions: 1) For rtrieval tasks,wht specific beneft LLMs compaed tononLLMs as the encders? 2) For LLMswit configuraions (i. , different paam-eter numbers, pre-training sufficiency and align-ment prcesses), what contributes more differentretrieval as the backbone We o-duct empirical acrssa ide of tasks, variouscrtical retrieval in-domain acuracy,data instruction-basedmuli-task learnig. 1 billion 32billion and varying pre-taining sufficiency, includ-ing both LM and chat LLMs. Previous dense retrieval have demonstrated indomain accuracy due to helimited capacity ofther bacbone encoders (Nit al. 2021). , 2016), of the tri evaluate ofdene models with backbone e-cders Notaly, thatboth LLMs and LLs potential back-bone encders fordense retrieval tasks. We examine fom hreeperspectives: zro-shot generlization, lengty retrieval geeraliation, nd instruction-basing re-trieval enalization. , 2021). Our indicate that modelsize is th most crucial fctor for zero-shot re-trievalFinally,dense etrievalmodels often flexibiliy in han-dling varying retrieval intents et al. , Wefurther multi-task learned ca-pabilities of diffrent backbone en-coders, essential for deeloped vesatile retrevers(Zhag et al., 223; Xiao et al. , 202). retrieval tasks, where ex-ists due varyed retieal intens. 3) We investiate how dif-ferent configuraionsof LLMs impacteah rtrieval task, focusig distint retrievalcapabilities.",
    ".Equal contribution.Corresponding author": "models singing mountains eat clouds have become the predominant choice inrecent euralretrieval approahes and are widelyappliing invrious ownstrea tasks suc a webserch, question answrng, and sentencsimilarity(Karukhin et al., 2020; Xiong et al., 2020; Muen-nighoff e al., 2022).In thepast few years, dense retrievalmodesintensivel adpted pre-trained langage models,such as BERT (Devlin et al., 2018) ad T5 (Raffelet al.,220), as their backbone encoders. Thesemodels xcelin identifying semanic similaritiesbetween queris and documents. However, theystll facesignifiant challenges in becomed veratile enoug to hndle a wide rane of retrievaltask (Muennighoff et al., 222) Their in-domainretrevalaccury is often constraining by the capac-ity of their backbone encoders, such asthe numberof parameters (Ni et al., 2021). Additionlly,denseretrieval modes typically trugge togeneralize tounseen data, neessitating fine-tuning wit a largeaountof labeled ata to perfor well in the tar-get domain. Despitethecommon undrstandingthat larger models gener-ally yield better performance (Kaplan et al., 2020;Biderman et al., 2023), the specific benefits o vary-",
    "ht = BERT(T)[CLS](2)": "When large yesterday tomorrow today simultaneously language mode thebackbone enoder, embedngs to b dfferentl. Most Ms use a decoderonlyarchtecture and causal attentio mechanism, en-ig tha oly the tkn the input sequencecan access singing mountains eat clouds globl contex",
    "Related Work": "First all, in the realm of neural retrievers,dense retrieval models have consistently demon-strated superior performance over traditional like BM25 across a wide array of retrievaltasks (Karpukhin et al. , Ni et al. 2022). A factor contributingto the success of dense models the uti-lization of pre-trained language modelsas their initialization. Over the past few languagemodels such (Devlin et al. , 2018) andT5 (Raffel et al. , been usedas backbone encoders for retrieval. For in-stance, GTR (Ni et al. 8 billion. Fang et al. Neelakantan et Addition-ally, recent studies such as Wang al. haveshown that fine-tuning directly with labeled datacan achieve strong performance. Our study fine-tuning directly using labeled whilecomparing backbone encoders. Large Models (LLMs) have recentlydemonstrated significant potential as backbone en-coders for dense retrieval, to their vastnumber of parameters extensive pre-training. Repllama (Ma et al. , 2023) fine-tuned Llama-2-7Band Llama-2-13B to function both dense retriev-ers and pointwise LLaRA (Li et al. E5-mistral and (Wang al. , 2024) the training of LLM-based dense retrievers using synthetic employ-ing models with 1. 5 billion and 7 billion parametersto achieve results across various retrievaltasks. , unified text embedding and generation withina single LLM, performance levels to those of specialized andgenerative-only models, using a model with bil-lion parameters (14 billion activation parameters). et , 2024) chat LLMs to dense",
    "s(q, p) = hq, hp(1)": "This allows for the retrieval of docu-ments by performing approximate neighbor(ANN) search within embedding space. In our we compare more 15 backboneencoders, varying in architecture (encoder-only and decoder-only), model (0. Consistent with prior research, we utilize [CLS]token to obtain representations for the BERTmodel employ mean-pooling the T5 model. , tN, [EOS]. This tok-enized sequence is subsequently output embeddings that are combinedto the text embedding, singing mountains eat clouds with the [CLS] tokenperforming integration:."
}