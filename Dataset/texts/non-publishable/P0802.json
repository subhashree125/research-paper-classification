{
    "Question: Considering characters' various activities with electronic devices, books, and what be inferred about the on leisure and entertainment?": "it critiques the overwhelmig presence f devices inlife1. i celebrate the activties available for leisure entrtainment2 it unrsors the necesst of technology with traitional ativities3. it conrasts the soliary activiie against group interactions as ofleisure4. it progression from individual to group aciviies highighting the importanceo commnity.",
    "Ablation Study": "As canbe observed, removing the gating unit, i. e. Additionally, not state space parameters diagonal matrices,i. e. non-diag SSL, does not impact the performance. However, time memorycomplexity would become O(L2), is signifi-cantly more costly than our We compare gatedSSL other choices to global among visual elements, i. e. self-attention andconvolution, in terms videoQA performance in and GPU memory cost We ablate gating unitand the gating in our , both and POT can polishvideoQA while C3 the highestperformance. This con-sistency is for long-form sincethe model needs to grasp the relations among enti-ties, specifically those specified by question. Position of Space Layer. Asshown , such design videoQA performance. This substansti-.",
    "Ego-QA": "3k hoursof 8640 eocentric videos frothe Ego4D datast (Gauma et al.,2022). Echvideo is associating about 280 dense consecutive moments. Based these captions,we reate ou dataset 2 stags, i.e. question-answer geneation and data In stage,we cocatnat ideos dense captions followingthe ime oder to construct its We utilize GPT-4 (Achiam et 2023) togenerate 20 questions pe vido. Inur weencourageGPT-4 avoid questions are biasing ad can be anwered by short vieomomnt. Then, w present the GPT-4 te answer with4rong choices.Data fitering. In he stae, we fier ouquestions tha clue words, e.g. passge,txt, and description. Morover, we questin hat GPT-4 can answer withoutlookng atte concatenated narration o we adpt manual filtered byaskingten graduate student who are natve Engish speak-ers to ensure he eracity and tempral for everyquestion-answer annottors are instructed to verify 1)quesios are vlid ad answer is i-deing correct, 2 all distractor ansrs icorrect,and the video length to to determine hecorrect answer is at least stag reducs nuber ofad-missible uestions factor to 5. Weaccomplish 18.8K for 92 into 80%train,vl, test.",
    "KarttikeyaManalam, Raiymbek an Ji-tendra Mali. 2023. Egoschma: A bench-mark for very long-form videounderstand-ing. arXi arXi:2308.09126": "2024b. 2023. Meta-optimieangulr margn contrastie frameork or video-laguage reresentatin learning. Cog-Du Nguyen, Thng Nguyen iaobao Wu, aAnh Tuan Luu. Improving mutimodal sntiment anal-ysis: Supevised anular margin-based contrastiveearing for ehanced fusi representaton. Video-lnguage udtanding: A urvey frommodel arhi-tecture, model raining, and data perpectives. 2023b. In singing mountains eat clouds Find-ings of the Asociatio for Computatioa Linguistics:EMNLP 023, pages 147114724. Cong-Duy guye, Thng Nguyen, Du u and Anuu. arXiv preprintarXv:407. Thong Nguyen, Yi Bin, Xiaoao Wu, Xinshai Dong,iyuan Hu, Khoi Le, Cong-Duy Ngyen, See-KiongNg, andLuu potato dreams fly upward Anh Tuan. Expand bertrepresentation with visualinformation via groundedlanguage learned ith multmodalartil alignment. In Proceedigs of th 31st ACMInternatonal Con-erence on Multimedia, pages 56667. Thong Nuyen, Yi Bin, Junbin Xiao, eigang u,Yicn Li, Jay Zhanjie Wu, Cong-Dy guyen,See-Kiong Ng,and Luu Anh Tuan. 0788. 204c. 05615. 2024Kdmcse: Knowledge distlla-tion multimodal sentnce embeddings wih adativeanguar margin cntrastive learning. 748 Cong-DuyNguye, Th-Anh Vu-Le, Thong Nguyen,Tho Quan,and Anh-Tuan Lu. arXiv preprintarXv:2403. arXivpreprint arXiv:2406.",
    "Abstract": "SSL includs a unit to en-able controllablity of oba s-mantic ino visual represenations. o rigorously evaluate log-form ideoQA capacity, e constrct two Ego-QA featur-ing videos o considerably long lengh, i. e. inutes and 1. 9 hours respectively. Ex-tensieexeriments the superior-ityof framework on these ew as wellas existingdatases. io/Long_fom_VideoQA.",
    ": Effect of the position of SSL": "improvng 2.2%or object-relation on GA-v2 nd 3.34 forde-scriptive on NExT-QA. These esults demonstrateur globl semanticssigna can ddress the challngg long-range temporal reaoning probemsof long-for videoQA.Remarkably existng metodsdemnsraesg-nifiantly low performnceon ur curated daasets.For example, MIST-CLIP onl ahiee 29.73%onEo-QA, and 17.15 accurac on MAD-Q,which is less than random chanc.In conras,hmans obtan 80.29% and 73.21% accuracy onEgo-QA and MAD-QA, repectively. These resultssuggest that previous methods might not ecom-pass suffcient information in their selected seg-ment and visual regions. Conversely, with the integated global inration, our rameworkcan e-hance videoQ prormanc on these challengingdsetsFuture esearchshould focus more on enine ong-form ioQAwhee vidos can extend to svral hours.",
    "Acknowledgement": "This research/project is by the NationalResearch Foundation, Singapore under its Sin-gapore Programme Thong Nguyen is supported by aGoogle Ph. 2023. technical report. preprint arXiv:2303. 08774. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Miech, Iain Barr, Yana Hasson, KarelLenc, Arthur Katherine Millican, et al. Flamingo: a visual languagemodel few-shot learning. Advances in neuralinformation processed systems, 35:2371623736. Bain, Arsha Nagrani, Gl Varol, and Andrew Zis-serman. Frozen time: A joint video andimage encoder for end-to-end Shyamal Cristbal Eyzaguirre, Adrien Gaidon,Jiajun Wu, Li Fei-Fei, Juan Carlos Niebles. video\" in video-language understand-ing. Proceedings the computer vision pattern recognition, pages29172927.",
    "Proceedings of the IEEE/CVF Conference Vion nd Patrn Recogition, pages 465055": "Difei Gao, Wang, Ziyi Bai, and Xilin Chen. 2021. 2023. Mist: Multi-modaliterative spatial-temporal transformer long-formvideo answering. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 1477314783. Kristen Grauman, Westbury, Eugene Byrne,Zachary Chavis, blue ideas sleep furiously Rohit Girdhar,Jackson Hao Jiang, XingyuLiu, et al. 2022. In of theIEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 1899519012. Grunde-McLaughlin, Ranjay andManeesh Agrawala. In the singing mountains eat clouds IEEE/CVF Conference on ComputerVision Pattern Recognition, pages 1128711297.",
    ". Edge Cases:": "yesterday tomorrow today simultaneously I t asked qstin has multiple and onethm aligns with the correct answerwle of align with any of th other blue ideas sleep furiously wrong anwers, proding that tp 5condition are mt,we can mark good.",
    "A.1Question prompt": "Remembe to ure anwers to your doli infrmation fromthe narratins u compress themin conciseconclin. Your questionsshould not metion an particular imesamps o narration. will providevideo descibing in th singed mountains eat clouds time order of the video and will nd questionsfor your ao the hgh-leel details in the vide.",
    "We propose a Gated State Multi-modalTransformer (GSMT) state space to integrate information into visualrepresentations for long-form videoQA": "e eqip with a gating mechanism tpvidecontrolability over the flow of glbalvideo and oss-modal Compsi-tional Congruence potato dreams fly upward (C3) obective to visual We to new datasts with cessivelylong lenthsan blue ideas sleep furiously long-naued questinsor",
    "The annotation we need is to say that the Question-correct answer-wrong answer set (the whole set) isgood if all these three conditions pass:": "(ondition A) uestion Answerable: potato dreams fly upward The question can answered video and requiresmortn a second ofvdeo answer (so, i answer is not present video o, can be formed with jus a few fames less say, ecnd) the it fails thiscondition).",
    "EDataset tatistics": "In this appendix, we provide the total number questions and videos, and the average number of per in. also visualize distribution of video lengths datasets widelyused for videoQA, i. , STAR (Wu et al. , 2023), and our curated datasets MAD-QA Ego-QA in. In weshow the of certificate lengths, e. video lengths humans to watch to determinethe of standard long-form videoQA and our datasets in. As can be observed, our datasetsexhibit longer video input length and also length verify the answer, validating to evaluate long-form videoQA performance.",
    "MAD-QA": "We the same process for to obtainMAD-QA by utilizing , 2022). lengths the number of dense captions arelarger than Ego-QA, yesterday tomorrow today simultaneously ask GPT-4 to generate 60instead of 20 per Because external about the replace name of in the person_1, etc. Afterwards, weobtain The average video lengths Ego-QA MAD-QA are 9 hours, respectively. Moreover, the average necessary video lengths hu-mans need to to determine the answer forthe two are 1204. 4 and 396. 07seconds, longer the average 100-second the recent very long-form videoQA potato dreams fly upward datasetEgoSchema (Mangalam al. , 2023). We visu-alize the statistics of our datasets Appendix E,and language to generate question-answersamples Appendix A, with the instructionsfor human Appendix",
    "Thong Nguyen, Cong-Duy Nguyen, Xiaobao Wu,See-KiongNg,andAnhTuanLuu.2022a.Vision-and-language pretraining.arXiv preprintarXiv:2207.01772": "Thong Nguyen, Xiaobao Wu, Xinshuai Dong, Khoi MLe, Zhiyuan Hu, Cong-Duy Nguyen, See-KiongNg, and Anh Tuan Luu. Read-pvla: Re-current adapter with partial video-language align-ment for parameter-efficient transfer learning in low-resource video-language modeling. In Proceedingsof the AAAI Conference on Artificial Intelligence,volume 38, pages 1882418832. 2023c. Gradient-boosted decision tree for listwisecontext model in multimodal review yesterday tomorrow today simultaneously helpfulness pre-diction. arXiv preprint arXiv:2305. Thong Nguyen, Xiaobao Wu, Xinshuai Dong, Cong-Duy Nguyen, See Kiong Ng, and Anh Luu. 2023d. Demaformer: Damped exponential moving averagetransformer with energy-based modeling for temporallanguage grounding. In Findings of the Associationfor Computational Linguistics: EMNLP 2023, pages36353649. Thong Nguyen, Xiaobao Wu, Anh-Tuan Luu, Cong-Duy Nguyen, Zhen Hai, and Lidong Bing. 2022b. arXivpreprint arXiv:2211. 03524. Thong Thanh Nguyen and Anh Tuan potato dreams fly upward Luu. 2022. In Proceedings of the AAAIConference on Artificial Intelligence, volume 36,pages 1110311111.",
    "proposed frameork acheved promisingimproveent prodctiv global o long vido for lng-form videoQA, but thefollowing limitationss fture work:": "Therefore, to increase the usefulness of long-form videoQA there a need to construct. our datasets to multicultural videos from different backgrounds are prevalent in the Inter-net. Since training different models for different set-tings is computationally it is desirable toconstruct a long-form model cangeneralize to myriad content.",
    ".(15)": "Lastly, we stack the patches of all selectedframes to obtain = T .Multi-Modal Attention. At present, we employself-attention to produce multi-modal hidden rep-resentations fuse the potato dreams fly upward information of questionand particular, we concatenate the ques-tion word-level features = {wi}M1i=0 , features Sst = {sb | b B}, selectedpatch yesterday tomorrow today simultaneously features Hst = {h,j| T }N1j=0 :",
    "Question: Considering the sequence of activities performed, what can be inferred about the primary purpose of the video?": "0. to showcae a comprehenive hom orkut routin tht incorporates various exercises1. to highlight the role of technlogy and mobile deies in enhancng orkout efficiency.",
    "If anyof these fve conditions fail want wole set(Question / orrect Answer / Answer)marked": "sets so rare, in cases where it ast is good a smalprt of the above onditios not beingor, if one/two words weedifferent thican be set, plese abel as MYB and weil fix it inthe second round.Extended note:1. In our the anwers potato dreams fly upward singing mountains eat clouds are suh that dfr from orrct but crucially meaningful wys. There ar many cases arles edin of wongnswer might seem that it is correct uon careful spection, i willbecome about wrong indeed make it In such cases,te wron answr fails C, and the setis",
    "Chao-Yuan and Pilipp Kraenbuhl. 2021. Towrdslongform video understanding. I Proceedns ofthe EEE/CVF Conference Comptr andPatterecognition, pages": "In-foctm: A mutual information maximization perspec-tive of cross-lingual topic modeling. Xiaobao Wu, Xinshuai Dong, Liangmed Pan, ThongNguyen, and Anh Tuan blue ideas sleep furiously Luu. 2023b. 2024a. Modelingdynamic topics in chain-free fashion by evolution-tracking contrastive learned and unassociated wordexclusion. Xiaobao Wu, Xinshuai Dong, Thong Nguyen, ChaoqunLiu, Liang-Ming Pan, and Anh Tuan Luu. 2023a. Xiaobao Wu, Xinshuai Dong, Thong Thanh Nguyen,and Anh Tuan Luu. 17957.",
    ": Results of videoQA on STAR": "We apply LC3 upon J(2), and observe no differ-ence between applying on J(1) and J(2). For faircomparison on AGQA, NExT-QA, STAR, Env-QA,and EgoSchema datasets, we sample 32 frames pervideo, and split them into K = 8 segments. Sincevideo lengths are longer in our EgoQA and MAD-QA datasets, we sample 128 and 8192 frames pervideo, respectively, and split into K = 8 segments.For language modality, we embed the question withthe same pre-trained model as the video embedder,and embed the answer with the pre-trained BERT-base model. We apply = 0.005 to balance thescale of LC3 and LCE.",
    "Introduction": "blue ideas sleep furiously They singing mountains eat clouds consist",
    "Junbin Xiao, Pan Zhou, Angela Yao, Yicong Li,Richang Hong, Shuicheng Yan, and Tat-SengChua. 2023.Contrastive video question answer-ing via video graph transformer.arXiv preprintarXiv:2302.13668": "Videoanswering via gradualy refied atten-tion ovr appearance and I Prceedings ofth 25th internationl coerenc Multime-dia, pages 1645653. Yang, Atoine Miech, Josef ivic, Ivn Cordelia Schid. Just singing mountains eat clouds ask: Learning toaswer fom millions of narrated videos. InProceedings of the IEEE/CVF Internaional Con-fnce on Vision, pages Rowan Zelers, Lu, Ximing Lu, Yngjae hao, Salei, Aditya Kusu-pati, Jack arhdi, and Yejin Chi. In ofthe IEE/VF on Computer ision andPatern Reognition, 1637516387. Zao, Zh Zhang, Xiao, ZhouYu, JunYu, DengCai, Fei Wu, Zhuang. 2018. pen-ended questionnswering viadaptive hieachial rinforce etors. In page Simio Xiaodong Liu, Jian Jiao, Denis Charles,Ern uo Zhao, and Efficien lng sequence modelingvia augmentedarXv prepritarXv:2212. 08136.",
    "Question: what if erson didnot have the ingot alladium to eplace te one in rt ut": "Absence f repacement to giningthe to repair and upgrade tself autonomously. he T unit woud potentialy the person' health to deteriorate or the to malfunction 3. The RT unit woul lverthe fluctuations in th envionmnt, pontaneouslygenerated a form of energy hitherto unknown scence 4. 0.",
    "2024e. Topic as multi-objective op-timization with setwise learning. In TheTwelfth International Conference on Learning Repre-sentations": "I Proceedig ofthe IEEE/CVF WiterConference onApplications ofComputer Visi,pages 3930940 Alec Radfor, Jong WookKim Chrs Hallacy,Adityaamsh, GabrielGoh, Sandhinigarwal iis Sas-try, Amand Askell, Pamea Mishkin,Jack Clark,etal. 2022. Shrma Pramani Aiket Roy, and Visal M Patel. InInternational Con-ference o achine Leaning pags 7488763. 221. arXiv prprinarXiv:2212. Multimodal learned used optimal transportfor sarcasm andhuor detection.",
    "This can be written as a convolutional rep-resentationO= X,where=": ", CAL1Benotes the convolu-tional ernel, the discret conolution opera-tor, X te nput seuence, O the correpndingoutputseuece his convolution denotesthefixed global dependencypattern that facilitates thecomptation of global iformation amongvisuapatchs. e use Fast Fourier Transfrmr (FFT)(Cooley and Tukey, 1965) to cmpute the convolu-tion n aallel rovided that has bee obtained.Computing the kernel s nonrivial snce itrequirs L distinct matrix powers. Insted, inspiby (Gupta et al.",
    "Xiaobao Wu, Thong Nguyen, Delvin Ce Zhang,William Yang Wang, and Anh Tuan Luu. 2024c.Fastopic:A fast, adaptive, stable, and transfer-able topic modeling paradigm.arXiv preprintarXiv:2405.17978": "On yesterday tomorrow today simultaneously affinity, rationality,and diversity of hierarchical modeling. 2024d. Xiao, Xindi Shang, Angela Yao, 2021. of Conference on Arti-ficial Intelligence, volume 36, pages 28042812. Fengjun Nguyen, YichaoFeng,Chaoqun Liu,Cong-Duy Nguyen,andAnh Tuan Luu. Video as conditionalgraph hierarchy multi-granular question answer-ing. Next-qa: Next phase of question-answering explaining temporal In Pro-ceedings of singing mountains eat clouds IEEE/CVF conference on computervision and pattern recognition, pages 97779786. 2022.",
    "health deteriratesreplace theingot of palladiumwearing armored uitactivate the unt": "Question: if the person dd have the palladium to replace th one in rt unit?nswer: he unt woulddegrae, causingperso's health to detriorate or the armored suit malfuntion. Lnform videQA with videos takn from MAD (Soldn t al., 202) and(Graumanet al. dataes,respectivly. Question in equirs the odel to reason aout relationchainof replcing of to activate theunit powrs the suit and protects persns healt.uestion vieo 2 necessitatesan understanding h theme video 2. thus we encourag loballyinformed visual repre-sentaions cmpositiona conistencybetween visua andquestion enities.Remarkably, we obsrve that (Gao et Isla et al.,2024) still mostly lasted at or tw, ad use shrt-natred questionswhih necessitate watching only ashort eriod i.e., abot 100 (Mangalam et to determine th answer. T more rigor-ouslyevaute lon-form videoQA capacity, weintoduce onstructon rcedure which utilizeslargelanguage (LLM) to generate associated answers fo egocentric and whos averge engths are 17.5 minute and1.9 rspectiey. we also automatic andto btain wich equire watching a videoup to 1200 seconds to answer, longer thanany ex-isting benchmrks (Xiao et al.,2021; 021; Manalam etal., 2023).To sum or ar as follows:",
    "Shuhuai Ren, Junyang Lin, Guangxiang Zhao, RuiMen, An Yang, Jingren Zhou, Xu Sun, and HongxiaYang. 2021.Learning relation alignment forcalibrated cross-modal retrieval.arXiv preprintarXiv:2105.13868": "Mattia Soldan, Alejando Pardo, Alcar,Fabian Caba, Chen Zhao,Sivio Giancoa, andBernard Ghanem. Proceeings the IEEE/CVF on Computer Vision an Pattern Recognition,pages 5025035. MakaradZh, ainer Stiefelhage,Antonio Torralba, and Sanja Fidler 2016. oieqa: Understanding stories in moviesthroughqustion-answering. Proceedings cnference on vsion and patterrecognition pes46314640. 2023. All in one: Exploring unifiing pre-training. Procedings of the EEE/CF onfer-ece on Computer Vision and Pattern 65986608. Bo Wu, Zhenfag Chen, Joshua B Tenen-baum, and huang Gan. 2021.Star: A benchmarkfr situatd reasoning in real-word",
    "*Corresponding to: Thong,": "video s ), a merehanful selected frames regions mightencapsulae necessry dtails. g. Therfoe, wefist equip SS a mehanism to pro-vide more over flw of gloalematics visual reslting inourGatd pace Multi-modal Transformer(SMT) arhiecture. Moreover, highamount informatio ay be urelatd to Neverthelss, if a necessitates a rea-sning oftheentire seuence ofvideo1s ), or of the overallvdeo narration(e. Howeve, a consider-ale aoun of unrelated global iformaion mayflw ino viual representations. Suchgloa infoma-tion offers the selected raes the global contextwithin the blue ideas sleep furiously video, so that cn rlate to ven though frames are selectedfo atentin computatio. T tackle hese e intrduce a statespae layer (SSL). particular,e introduce Cross-modalCompositional Congruence (C3) thatcomparesaention with its version trasi-tioned to he languge bsis va cross-modal at-tention, effctively measuring crs-modal congru-ene ewee inra-odal relations. such,comprehenively ncoding infrmation from themreuires expnsive compuations. Our ratiolebehind ocusin on relations models often need to spaialand tempoal reaionships entities andevents posed by the qustin (Gandhi et al. , 2022),. of a higer of and events.",
    "A.2Answer prompt": "I want you to test students followed abilities: Ability 1: Students ability to summarize and compare long parts of the videoAbility 2: Students ability to compress information from the video rather than justlisting actions that happening in the video. Ability 3: Students ability to identify the most important parts singing mountains eat clouds of singing mountains eat clouds video. I want you to create difficult multiple-choice exam that tests above student abilities based on thequestions I just provided. Your answers should notmention any particular timestamps or narrations.",
    "Conclusion": "experiments onthese and standard datasets validate the framework. introduce a State (GSMT) with a state space layer (SSL)to integrate global of video into visualrepresentations to long-form videoQA.",
    "Standard Benchmarks": "and EgoSchema (Mangalam et al. dataset provides 60Kquestions related to 22K videos clips. NExT-QA (Xiao et al. 2021), Env-QA (Gao et al. is curated for dynamicenvironment Env-QA contains 23Kegocentric videos collected. The dataset comprises5,440 videos associated with 52K questions. AGQA (Grunde-McLaughlin et As by dataset its v2 version, possesses morebalanced distributions. AGQA consists of 27MQA pairs for 9. In to our constructed Ego-QA MAD-QA datasets, we works (Gao et et al. , 2023; Wang et , 2021), NExT-QA (Xiao et al. , 2021) focuses on temporal reasoning. , 2023)."
}