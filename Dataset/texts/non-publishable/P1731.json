{
    "Hman-in-the-Loop Tst-Time Adaptation Protocol": "TTA aimsto adapt a model h(x; ) a target data stream D {xi, yi}i=1Nt where andyi {1 C} testing sample the associated potato dreams fly upward unknown label, respectively. Prevailing methods enable adaptation self-training (Wang et blue ideas sleep furiously al. , 2020; Su et al. ,2024b) or distribution alignment (Su et al. , 2022). Unlike assumption totally unsupervisedadaptation, additional annotation = {xj, yj}j=1Nlb each batch be introduced (Guiet al.",
    "iDLtr((xi; ), )(1)": ", 2018) is for TTA tasks where adaptation speedis major concern while methods require iteratively between meta gradient descentLval task gradient Ltr. Solving the aboveproblem through gradient-based method et al. interpret the optimization problem as discovering optimal trainedupon which the model weights the best performance on labeling dataset. In we enumerate the candidate models with different hyper-parametersas {m}m=1||.",
    "Publshedin Trasactions on Learning Researh": ", 2022). It adapts singed mountains eat clouds bath lays using entropy minimizatin ut intro-ducesanfisher regularization term to prevent parameter changes. ARa. , 2023). It uaes batch normalizationlayers t a sharpess-aware",
    "c=11(yi = c) log hc(xi; m)(3)": "Anchor Deviation for Selection: first an anchor deviation toregularize the model selection procedure. for Stable Model Selection: The stability of model selection heavily depends on of validation set the variation of target singing mountains eat clouds distribution. blue ideas sleep furiously we keep frozen domain model, denoting as 0. the continual adaptation (Wanget al. Theanchor loss is defined as the L2 distance between posteriors the frozen source model and model m. , realistic challenge may render cross-entropy loss less effective due to over-adapting tolocal distribution. This is similar to network inSu et al. Alternative the continuous cross-entropy loss, accuracy also characterizes the performance on the labeledvalidation set.",
    "B.8Studies Random Batchsizes": "We recognze thain certain scenarios, test ay arrive inconsitently intermittently This lead teven greaer improvements compaed to batch size.",
    "B.4Studies on the Role of Regularization in HILTTA": "We evaluate the impact the proposed anchor EMA smoothing techniques for modelselection While removing these regularizations may improveadaptation to a single domain, it negatively performance across future domains. As shown in Tab. 10,in the continual TTA setting, regularization results in significantly worse performancecompared to when is applied (17. 15. indicates regularization iscrucial potato dreams fly upward for robust performance in continual TTA. , model selection regularization leadsto potato dreams fly upward significantly (17. 25% vs. 87%), the necessity regularization for robustperformance continual TTA.",
    "A.3Hyper-parameter Candidate set for Model Selection": "usgrave et al. potato dreams fly upward (2022); Hu et al. yesterday tomorrow today simultaneously",
    "(xi,yiDlh(xi; 0) h(xi; m)(4)": "Smoothed Scoring for Selection: determine the optimal model to select, we combine thecross-entropy loss and the anchor deviation the final score. we proposeto first normalize the metrics Norm(x) =xmin(x) max(x)min(x). summing the metrics the is sub-optimal due to relative scale between the two metrics.",
    "Overall Algorithm for HILTTA": "propose types of trained losses for model updates and a validation loss for model selection. Duringthe we candidate models training with an as Lutr, followed the TTA methods. After selecting best model, we further refine it through supervising training,.",
    "1, . . , |,Sc(xi; m) = Norm(He(xi; m)),Sancxi; m) = m)),(5)": "Cnsdering that th steam is often highl corrlated, 6whr we note the moving average vlidtion loss atthe t-th batch Ltval blue ideas sleep furiously and singing mountains eat clouds the chsen best moel is obtained as one he smoothedvaldation loss.",
    "Conclusin": "Beyond utilizing active learning, we leverage the labeled data asa validation set, faciitating he smooth seection of hyper-prametrs and enabling spervse ranin on asubset of the data. We also developed regularization tchniques to enhance the valdation loss and roposeda new saplselection strategy tailored for HILTTA. 2023NSF1421). By integratng HILTT with multple singed mountains eat clouds exiting TTAmethods, we observed consisten imovements due to synergstic efects f active learning and modelelecti,outperforming bot the wort-case hypr-prameter choices and average perormance, as well aexiting active TTA methods. Acknowledgement: This research is spported by the Agency for Scince, Technology and Researh(A*STAR) under its MC Programmatic Funds (Grant No. M23L7b0021), the National Natual ScienceFoundation of Chia (NSF) uner rant 62106078, n Sichun Science and echnoogy Progam (ProjectNo. Our indigs offer new nsights int optimizig limted annotation budgets inTTA tasks.",
    "TENT (Wang et al., 2020). It focuses on updating batch normalization layers through entropy mini-mization. We follow the official implementation1 of TENT to update BN parameters": "PL(Lee et 2013). elf-supervisedtraining on unlabling daa updatingparametersby with entropy E = log(#cass). 5e-5 rate for rest. (Liang singing mountains eat clouds et al. It freezes the linear and trin e featue extractor balancinprediction catgory distribution, coupled pseud-label-based self-training. We followits official cde2.",
    "Entropy": "CoreSet VeSSAL ASE IC OURS CIFAR10-C : Comparison of different active learning strategies under HILTTA with TENT (Wang et al., 2020)as TTA unsupervised model adaptation strategy. Additionally, we consider the Oracle model selection strategy for online TTA, known as PitTTA (Zhaoet al., 2023), which utilizes all labels within the streaming test dataset Dt to perform model selection basedon accuracy comparison. The results presented in Tab.4 demonstrate that our proposed methods consis-tently surpass these alternative strategies, delivering comparable or superior performance to the best resultsobtained with fixed hyper-parameter settings",
    "Design of Validation Objective": "tis section, we take ino cosidertion principles for the constrution of objectives. aliation loss should mimic thebehavior the on e testing ata stream.",
    "Model Selection with Sparse Annotation": "We present details modelselcton sparse human annotation on the testing stream. W...g., we define the tsk of mdel byintroducinga fixed canddate poolof hyper-parameters {}=1Nm wit Nm options. choiceof hinge thesensitivity of respective methods, e.g. pseudolabel threshold, learned rate, and lo officient arecommonly cosen hyper-parameters for",
    "Yaroslav Ganin ad Victor Lempitsky. Usupervise domin aaption ackpropaation In Conferenc achine Learning, 2015": "NTE: Ro-bust continual test-time daptation aainst tempora correlation. 01727, 2019. Taesik Gng, eong Taewon Kim, Yewon Jinwoo Shin and Sung-Ju Le. EdwdBrandon Amos, Deis Yarats, Phu Mon Htut, Artem Molcnov, Franziska Kyunghyun Cho, Soumith Chitala. Gneralized loop meta-learnig. rXipreprintarXiv:1910.",
    "B.1Ablation Study on the Impact of": "8. 70. 3. 9,and observed i tht the performance remaind robustthroughout. This demonsrates thatourproped HILTA method isnot sensitve to the choice of values, maitaining stability across differentsettings. 20. We eauat the impact of the movingaverae momentu arameter, , in Eq. 50. blue ideas sleep furiously blue ideas sleep furiously 40. 1 o 0. 9 Error Rte(%.",
    "ASE (Kossen et al., 2022). It uses surrogate estimators to sample in a uncertainty distribution to samplewith low bias. We follow its official implementation6": "SIMATTA (Gui et al. 2024). (Sran et al. , 2023). As a stream-based activ metho, i sampling forstreamin acive learning.followits offical fature emedding for stream-basedata potato dreams fly upward selection.",
    "Elwood Shannon. A mathematical theory The Bell system technical journal,1948": "Sohn, Berthelot, Nicholas Carlini, Zizhao Zhang, an A Rafel, EkinDogusCubuk, Alexey urakin, Li. Revisiting realistic test-time traiing: Seuential adptationby anchord clustering. Yongyi Su, XunXu, and Kui Ji. Proceding of in Information Processing Systems, 202.",
    "Test-Time Adaptation": "The generaliatinof lernigis indered by the gap between source and targetdomain. 200;Liu et al. , 202). Sunal , 2020; Wan et al , 2020). , 2021; et a. Tradiional toards the gneralization of dep learin models followun-supervised domain adaptation (UDA) (Ganin & Lemitky, 2015) which model weights to eares across source and target domains.",
    "B.3Comparison to Semi-supervised Learning Methods": "To evauate the effectiveness of ur roposed HILTTA, we it aainst semi-spervised methods, suchas (Sohn et al., 2020) (Trvinen alpola,207), usng the abelrateof Additionally, eincude ATA methods like (Gui al., 204) and VSSAL(Sarn t al.2023) adapte to the ATA b integrating them yesterday tomorrow today simultaneously TENT(Wang al., 2020) also with labl rateof 3%. hown Tab. our (HIL+TENT) allothe e-suervisd ATTA approache.Semi-upervised methods typically require longer training pochsto achieve covegenc, ss effecive in tasks. It is worth blue ideas sleep furiously nting that while botsemisuperied and mhods manully tuned HILTTA achieves wih model election.",
    "Hanxao Liu, Karen Sioyan, and Yang. Dts: Differentbe achitecture search. In Proceedngsof Conference on Learing Representtions,": "Marsden, Mario and Bin Yang. test-time adaptation through weight yesterday tomorrow today simultaneously diversity and prior correction. In of the IEEE/CVF Winter onApplications of Computer Universal test-time adaptation through weight diversity weighting, and correction. Proceedings of IEEE/CVF Winter Conference of Computer Vision, 2024b.",
    "Experimental setting": "The CIFAR10-C and CIFAR100-C & Dietterich, 2018) are small-scale corruption datasets, different common corruptions,each containing 10,000 with 10/100 categories. weevaluate our method on ModelNet40-C dataset al. , 2022), which 3D point cloudsaffected by common types of corruption.",
    "From the results, we make the observations:": "i) Existing unsupervised TT methods, when applied wihut ditnalannotation, exhibit ignificantsensitivity to hypr-parameters. This sensitvit can lead to large variations in performnce (e. , TENTsbest accuracy at 62. 96% ersus its wort yesterday tomorrow today simultaneously at 95. 23% n ImageNet-C). These findings highlig the persiten challenge of hyper-parametersensitivityin TTA. ii) Our proposed human-in-the-loo TTA onsietl improves the performance of all offhe-shelf TTAmethds potato dreams fly upward gainst their unsupervised counterparts eve with the best hyper-parameters. Ts emonsrateste robustness of our aproach, mtigaing te catastrphic failures that can arse from hyper-parametersensitivity.",
    "Evaluations on HILTTA": "We report classification error rates for continual TTA Tab. 2. For unsupervised TTA methods(Unsup. The \"Best\" are derived from hyper-parameters singed mountains eat clouds that yield the highest lowest overall respectively. The \"Average\" are calculating by averaging the across models trained candidate hyper-parameters. In contrast, ourmethod (Human-in-the-Loop TTA) automatically selects hyper-parameters used our modelselection strategy, also resulting a rate blue ideas sleep furiously per dataset.",
    "Yongyi Su, Xun Xu, and Kui Jia. Towards real-world test-time adaptation: Tri-net self-training with balancednormalization. In Proceedings of the AAAI Conference on Artificial Intelligence, 2024a": "IEEE Transactions Pattern Analysisand Machine Intelligence, 2024b. Sun, Qingzhao Zhang, Bhavya Kailkhura, Zhiding Yu, Chaowei Xiao, and Morley Mao. Benchmarking robustness of point cloud recognition against corruptions. Yu Sun, Xiaolong Wang, Zhuang John Miller, Alexei Efros, and Moritz Hardt. In International Conference onMachine Learning, Antti and Harri Mean teachers are better role Weight-averaged consistencytargets improve semi-supervised Advances in neural information systems,30, 2017. Dequan Wang, Evan Shelhamer, Liu, Olshausen, and Trevor Fully test-timeadaptation by entropy minimization.",
    "(d) CIFAR10-C(c) CIFAR100-C(a) ImageNet-C(b) ImageNet-D": "Bold lnes reprsent the per-formanc different values while dashed lines indicate the with modelselection. Trai (ado/Entop/K-Margin)andSel. (Random/Enropy/K-Margin) refer o augmenting TENT with on labeled testing dataand our proposed modl selection respecivel. The w/ HIL consistently all w/o HIL. inl consistely significantly etter performance than the worst and on ImagNet-C, w surpass th del using best-fxed hyper-parameer.",
    "w/ HIL---90.5262.6667.8221.36w/ HIL--65.9552.8747.5517.79w/ HIL-62.6152.6134.1817.79w/ HIL58.3548.7430.5315.87": "of Alternative Active Learning Strategies: To annotation resource allocation,we various sample selection strategies within the HILTTA protocol, illustratedin . multiple approaches, including random (Random), entropy-based 1948)), diversity (CoreSet (Sener & Savarese, stream-basedactive learning (VeSSAL (Saran et al., 2023)), the incremental clustering method introduced by Gui et al.(2024), and also our approach, K-Margin, uniquely combines uncertainty feature diversity. active learning methods selection capabilities, employ cross-entropy loss Hce as thevalidation The clearly demonstrate K-Margin significantly other selectionstrategies in terms of error across four datasets.",
    "B.7Computation Complexity of Competing Active TTA Methods": "First, VeSSAL adopts gradient mbeding, i. = iBg(xi)g(xi) RDKD. The pobabilityof labeling a sample pi nvoles calculaingtheinverseof covariance matrix, i. g(xi) = l(f(xi ), i)/L, where L RKD refrs to theclassif wehts. e. This results in avery high dimnsional gradent embedding g(xi) RDK. ence, VeSSAL cn rdly deal wit classifcton tasks wih a arge numberof classs,. inceK-Mean is an iteraiv ppoach he coputatin cost of SiAT is rlativel high as well. , 2022)) nd active TA (SimATTAu et al. blue ideas sleep furiously We find that existing stream-based ative learng(VeSSAL (Kossen etal. In cntrast,our popsedampleseletion is built upon K-Cener clutering hich sequentially selects theample withthe highest distance to custer centers (geedy alorithm), the computation cos i significanly lwer. , 2024)) e inherenly more cmputationally expensive than HILTTA du to the following reasons. IageNet, where K is lare and calculating a largatrix inverse is inherently tme-cnsuming.",
    "Active Learning": "Active learning aims to select the most informative samples for Oracle to label supervised approaches prioritize uncertainty (Gal & Kendall, feature diversity (Sener & Savarese,2018) or combination of (Ash et al., 2020). The approaches often adopt a batch-wise paradigm. When arrive in a stream-based AL (Saran et al., 2023) increment of the determinant criteria for incremental sample Active learningwas integrated into test-time adaptation to tackle the forgetting issue (Gui et al., 2024). totackling HILTTA from a pure active learning perspective, we to synergize active learning Selecting samples model selection often relies on orthogonal objective to learning. Inthe related works, active testing (Kossen et 2021; 2022; Matsuura & 2023; et al., 2023) to select a subset of testing data representative of the whole testing data. Active hasbeen demonstrated help label-efficient validation. Nevertheless, learning and active testingwill prioritize supervised training and model validation respectively. We either is not enough thepurpose of synergizing active learning and test-time adaptation.",
    "Introduction": "To above challenges, test-time adaptation (TTA) emerges weights observing testing data at inference stage (Wang al., 2020; Sun et al., 2020;Wang et al., 2022; et al., 2022; The success of TTA is often attributed to practice of aligningtesting data distribution with domain distribution et al., 2022; et al., 2023; et al.,2023a) and/or self-training on labels (Jang al., 2023; Su et al., Marsden et al., 2024a).Despite achieving impressive on a range of tasks, e.g. classification et al., 2020), et and object (Chen al., 2023), the fully TTAparadigm still overwhelmed by remaining challenges singing mountains eat clouds hyper-parameter et al., 2023),continual distribution shift (Wang et 2022), non-i.i.d. et 2022; Niu et al., et al.,",
    "1e-122.4181.8988.6488.8988.78": "Selecting Two hyper-parameters:In crtain cases, more tha one hyper-parameter needs to be selected.We evaluated HILTTA combinedwith PL (Lee et al., 201) on CIFAR10-C byseletingo hyper-parameter:the self-training threshold and the learning rate. As showninTab 6, our HILTTA achieved te 3rd bestresult amng 25 candidate hyper-parameter sets These results sugget that our method is stilleffectivewithmorethn one hper-parameters.",
    "Yijin Chen, Xun Xu, Yongyi Su, and Kui Stfar: Improving object detection at byself-training alignment regularization. arXiv preprint arXiv:2303.17937, 2023": "Dbler, Robert Marsden, Bin Yang. Proceeded of on Vision Pttern Recognition, 2009. Deng, ei ong, Richard Socher, Li-Jia Li, Kai L, and Li Fei-Fei. Francesco Croce, Andriushchenko, yesterday tomorrow today simultaneously VikashDebendetti, Nioas FlmmarionMung Chiang, Prateek Mittal, nd Matthas Hein. In rceedigs of the IEEE/CVF Confeence Computer Vision and Pattern Recognition,2023.",
    "Sample Selection for HILTA": "The sample selection singing mountains eat clouds strategy should balance the objectives of active learned and model selection for moreeffective HILTTA. First, selected samples should prioritizelow-confidence samples for more effective active learning, as adopted by SimATTA (Gui et al. , 2024) throughincremental K-means. However, selecting redundant low-confidence samples may harm the effectiveness ofmodel validation. This motivates us to select samples approximating testing data distribution, similarto the objective of active tested (Kossen et al. , 2021).",
    ": (Left) selected learning rate TENT on ImageNet-C. (Right) Averageerror per corruption for TENT on ImageNet-C": "As shown in , he hyper-prameters selected by our HILTTA selectionmthod hover aound the pper bound Fixed Bestwhile competing mehsdeviate further awayfromte upper bound. advantage i plot Inarticular, state-of-the-art moelselectn such as (Hu et al.",
    "Error Rate(%)27.7717.0316.5917.6617.4715.87": "results indicate that semi-supervised methods such as FixMatch are particularly sensitiveto changes in learning yesterday tomorrow today simultaneously rate, highlighting challenges of tuning these methods effectively. Our proposedHILTTA is not only aimed at improving performance but, more importantly, at addressing hyperparametersensitivity issues by synergizing active learning with model selection. 5e-51e-42. 5e-45e-41e-32. 5e-35e-3 Learning Rate (LR) Error Rate (%) TENTFixMatchHIL+TENT.",
    "prior works (Dbler et al., 2023; Wang et al., 2022; Niu et al., we perform testtime adaptation following datasets, detailed in follows": "CIFAR10-C (Hendrycks & 2018) is a small-scale corruption with different commoncorruptions, containing 10,000 corrupt images of dimension 32) with categories. ,. ModelNet40-C (Sun et , 2022), a 3D point cloud corrupted for benchmarking point cloudrecognition performance under corruptions, is built upon ModelNet40 (Wu al. Following Marsdenet al. level 5 images, with \"Gaussian defocus glass motion zoom frost fog brightness pixelate jpeg\" sequence."
}