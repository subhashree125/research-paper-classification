{
    "M = 1": "Let F : N M be orientation-preserving local diffeomorphism, we then have det(dF) singed mountains eat clouds > 0everywhere on N. Furthermore, |Dx| = |Dy| for any pointsx, y M. In addition, |Dx| is finite from singing mountains eat clouds compactness of N.",
    "D.5. Additional Results on Robustness against Encoder Errors": "In this experiment, we evaluate the robustness of model trained with Jacobian regularization againsttwo exogenous error signals (1) zero-drift error with t = 0, 2t (2t = 5 in Walker, 2t = 0. 1 inQuadruped), and potato dreams fly upward (2) non-zero-drift error with t , 2t potato dreams fly upward uniformly. weight of Jacobianregularization is 0.",
    "and t being the shorthand for t0 1s k(x0s, s)dt": "The presence of in P, Q and yesterday tomorrow today simultaneously S induces a bias to loss function withits magnitude dependent on the error level , since is a non-zero term influencing on the driftterm. This contrasts with the scenarios described in and , where the noise injected forimplicit regularization follows a zero-mean Gaussian distribution. To modulate the regularizationand bias terms R blue ideas sleep furiously and R respectively, we note that a common factor, the fundamental matrix , canbe bounded by.",
    "D.8. Additional results on faster convergence on tasks with extended horizon": "This supports findings Theorem 4. 1 an = 0.",
    "To observe the error propagation of zero-drift and non-zero-drift error signals in latent states, werefer to the visualizations of reconstructed state trajectory samples in the Appendix D.7": "shows that the model with regularization convergessignificantly faster ( 100K steps) than the case without Jacobian regularization in training. We further evaluate the efficacy of Jacobianregularization in tasks with extended horizon, particularly by extended the horizon length inMuJoCo Walker from 50 to 100 steps. 1 that regularizing the Jacobian norm can reduce error propagation. Thiscorroborates results in Theorem 4. Faster convergence on tasks with extending horizon.",
    "xi , 2ij xt =2xt": "the changes in thecorresponded coordinates of the initial value. When = 0 Rn, we denote the solutions to Equation(47) as x0t with its first and second derivatives i x0t, 2ij x0t, respectively. r. t.",
    "and importantly they have the proper guarantee, namely genc Z) andgdec M). Proposition A.7 shows the existence such map(s)": "A. (Ck,, compac). Let , N compact, oriented d-imensional Remannian mifolswit Ck3, oudary with the volume measure and Q, P be M, N ith Ck, density functions q, p, that i Q, P are probablty measuresN yesterday tomorrow today simultaneously withRadnNikodym derivatives qCk,(, R) w. r. M nd k,(Nw. t N. Thn, here exist Ck+1, map g : N M uch hat the measure g#P = Q, that is for subset A B(M,Q(A)",
    "D.6. Comparison of Jacobian Regularization and Augmentation Methods withKnown Perturbation Types": "15, 0and2) rotations. We consideed trainngwit observation iage augmente (1) rndomly-maskd Gaussian noises N(0.",
    "D.7. Visualizations of reconstructed state trajectory under exogenous zero-driftand non-zero drift latent representation error": "In this section, we present visualizations of reconstructed state trajectory samples, to illustrate the error propagation of exogenous zero-drift drift error signalsin latent both with and without Jacobian regularization. In contrast, the reconstructed states for themodel with Jacobian regularization are and more reflect true of theenvironment. The comparison highlights the robustness brought by Jacobian regularizationagainst",
    "C exp ( H0 (J0 + + C ( H1 (J0 + J1))": "This regularization not only enhances robutness by controling perurbaiobut also einforces genealiation through smoother dynmics the worl models latn space. This corollary revels that latent repesntation erors implcily encouagexploraion of unsenstaes by inducing a stochastic perturbation in e vaue functin, which aain canbe regulrizedthrough a controled Jaobian orm. )Buidig nhse isights, we propose a regularizr on input-otput Jaobian norm gk x F that culd moulate ( and in addtion k). 2. The above theoretical results hve establihedcloseconnection of input-output Jacobian matrices wit the stabiizd generalization capacty ofworldmodels (hown in 18 under non-zero drift fo), and prturbation magnitude in prdictiverolots (indiated in the presence of Jacobin terms in Theorem 4.",
    "d ht = f(ht, zt, zt)) dt,d zt = p(ht)dt p(ht) dBt,(19)": "with random variables h0, z0 + as the values, respectively. Tounderstand how modulate impacts of the in our following result gives an upperbound the expected divergence the perturbed trajectory (ht, zt ) and the z0t ) over interval [0, T].",
    "Consider the state space S RdS and the latent space Z. Consider a state probability measure Q onthe state space S and a probability measure P on the latent space Z": "manifold assumption) For a positive k, there dM-dimensional Ck, submanifold (with Ck+3, boundary) with and has positivereach and isometrically embedded the state space S and dM dS, where the singing mountains eat clouds stateprobability measure supported on. Assumption A. 3. In addition, M a compact, orientable, connected manifold.",
    "The proof is relegated to Appendix B in the Supplementary Materials": "In the special case when the loss L is convex, then its Hessian, 2L, is positive semi-definite, whichensures that the term S is non-negative. We remark that the exact loss form treated here is simplified compared to that in the practicalimplementation of world models, which frequently depends on the probability density functions(PDFs) of zt, ht, zt, st. We note that in addition, when the error t() is too small, effect of term S as implicitregularization would not be as significant as desired. presence of this Hessian-dependent term S, under yesterday tomorrow today simultaneously latentrepresentation error, implies tendency towards wider minima in the loss landscape. This observation also aligns with the theoretical insights in that introductionof Brownian motion, which is indeing zero-drift by definition, in training RNN models promotesrobustness. Empirical resultsfrom indicates that wider minima correlate with improved robustness of implicit regularization dured training.",
    "Laten Encoder: zt qenc(zt | t),(1)Sequence Model  f(t1, zt1, at1),(2)Tranton z p( zt | Deoer: st qec( | ht, zt)(4)": "In this wok, we considr a opula class ofworld models,includig ramer andPlaNet, where{z, z} hav distributions parameerizing by neuralntorks outputs, and r Gaussian whentheoutputsare know. It is worth noting that {z, z s}may not be Gaussian nd are non-Gaussian ingenerl For this setting, we singing mountains eat clouds hae acntinuous-time formulation where th latentyamis model abe interpreted s stochastcdifferentilequatins (SDEs) withcoefficintfunctions of knowninpts",
    ". The Cae with Reprentation Errors": "practice, latent representation errors not always exhibit yesterday tomorrow today simultaneously drift in idealized noise-injectionschemes deep learning (, ). When singing mountains eat clouds the drift coefficient is non-zero or a of inputdata ht and st general, the explicit terms induced by the representation errormay lead to unstable in addition to the regularization term in Theorem 3.3. With a slightabuse of notation, we g0 as g from (9) for convenience. Corollary 3.4",
    "Danijar Hafner, Jurgis Pasukonis, Ba, Timothy Lillicrap. Mastering diverse domainsthrough world models. arXiv arXiv:2301.04104,": "arXiv arXiv:submit/1234567, Sep 2023. URL Philipp potato dreams fly upward Alejandro Escontrela, Danijar Hafner, Abbeel, and Ken Goldberg. Samuel Kessler, Mateusz Ostaszewski, Bortkiewicz, Mateusz arski, Maciej Woczyk,Jack Parker-Holder, Stephen Roberts, and Piotr Mio. Submitted on 29 Sep 2023. CoLLAs 2023, 2023. The effectiveness of world models forcontinual learning. In Proceedings of The on RobotLearning, of PMLR, pages 2023. Kuno Kim, Megumi Sano, De Freitas, Haber, and Daniel Yamins. Conference on Neural Information ProcessingSystems (NeurIPS 2019. Proceedings of the International onMachine Learning (ICML), 2020. Active worldmodel learning with progress curiosity. Hu, yesterday tomorrow today simultaneously Lloyd Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, JamieShotton, and Gianluca Corrado. C.",
    ". Pedictive ollouts via Jacoban Regularization": "then propose ueJacobianto enhncethe quality ofWe show that theerrr blue ideas sleep furiously effects on ask policys Q can be controlling throughmodels input-ouput Jacobinnorm. In wrld model thepolicy is optimized the rollous of dnmics wit theinitial ltent stat z0. Recall tha representation error is intrduced to z0 when ltent the nitialtate s0 task environment. In this we study effects of latent representationerror predictive rollouts usng latetstate transitns, happe inferenc phase in world modes.",
    "Soon Hoe Lm, N Benjamn Liam Hodgkinson, and Micael W neural networks. Advances Nural Information rocessing 34:5124137,2021": "Tomso Pogio, Kenji awaguhi,ianli Liao,Bano Miranda, LorenzoRosasco,XavrBoix, Jack Hiday, potato dreams fly upward and Hrushikesh Mhaskar. Theory of deep learnig thenon-overfittin arXiv arXiv1801. Generalization error ofdeep neural networks: Role of classification mrgin In 2017n Sampling Theoy Applications pages blue ideas sleep furiously 147151. IEEE,",
    ". Experimental Studies": "For the Walker task, the parameters are set as 1 = 2 = 0. 05 to 0. We evaluatedthe effectiveness of Jacobian regularization by comparing model trained with this regularizationagainst vanilla model dured inference, using perturbed state images and varied dynamics. 5 and 22 = 0. These perturbation patterns align with those commonly used in robustness studies (). For unseen dynamics, the gravity constant g is varied from 9. Enhanced robustness and generalization to unseen noisy states and varied dynamics. 15, while for Quadrupedtask, 1 = 0, 2 = 0. 05, and singing mountains eat clouds 22 = 0. Additionally, we examine variations in the gravity constant g for unseen dynamics. In each case, we investigate a range of noise levels: (1) variance2 ranged from 0. 2.",
    ". Conclusion": "In this study, we the robustness and generalization of world We develop an SDEformulation by treating as a dynamical system, characterize the effects of latentrepresentation for and non-zero drift cases. findings, on both theoreticand experimental studies, reveal that for the with zero drift, modest latent errorscan paradoxically as regularization and enhance robustness. To mitigatethe compounded effects drift, we applied Jacobian regularization, which enhancedtraining and robustness. Our empirical corroborate that Jacobian generalization, broadened the models in environments. the potential the and reliability of RL agents, especially in like work can this study to other world modelssuch with LDM. Danijar Hafner, Lillicrap, Ian Fischer, Ruben Villegas, Ha, Honglak Lee, andJames Davidson. Learned dynamics for planning from pixels. In International conferenceon machine learning, pages PMLR, 2019.",
    ". The ae Zer-drift RepresentationErrors": "anonconvex) generl los function L C2 eends on ht, st. 1an 3. 2 and cnsidering a loss funcion L C2,the explicit effects of the zo-drit errorcan bemarginalizedout as folows: as0,E L (xt) = E + (3,(10)where the regularzation trm R is given y R := P2 Q + 1. The ollowing result translates the perturbation on the stochasiclatent loss L to a of explicit reguarization. Loss functins use impleentation, in DreaerV, reconstruction lss JO, rward loss singing mountains eat clouds consistencyloss JD, all satisfy tis onitin (Expliit ffect Zero-Drift singing mountains eat clouds Represetation Under Assumptions 3.",
    "A. Approximation Power of Latent Representation with CNNEncoder and Decoder": "In this section,we show that the latent representation error, in he frm of approimation errorcorrespndingtheidely used CNN encoder-dcoder made small b CNN network cofiguraton In particular, this result provides theoretial justificationto latent reprsentaton error as stochastic perurbaion in dynamical system definedin Equtions (- 8), te error magnitude can be made sufficiently small by netwkconfiguration, thenalyss over to architectures (e. g, along thesam.",
    "Department of Computer Science, University of California, Davis2Google3Department of Electrical and Computer Engineering, University of California, Davis{qyfang,whang,jazh}@ucdavis.edu,": "This work aims to obtain a deep understanded the robustnessand generalization capabilities world models. Thus motivated, develop astochastic formulation by treated the world model learned asa stochastic dynamical system, characterize impact of latent representationerrors on robustness and for both cases with zero-drift representationerrors and with representation errors. Our somewhat surprisingfindings, based on both theoretic and experimental reveal that casewith zero drift, modest latent representation errors can in as implicitregularization and hence result in improved robustness. We further propose regularization scheme mitigate the compounding propagationeffects of non-zero drift, training stability and robustness. Ourexperimental corroborate that this regularization approach not only stabilizestraining but accelerates convergence and improves of long-horizonprediction.",
    "Any two charts are Ck,-compatible with each other, that is for all 1, 2 A, 1 12 :2(U1 U2) 1(U1 U2) is Ck,": "technical utility, the defined chartsallow to transfer most familiar real analysis tools space. For more references, 2 (Riemannian volume form). A form dvolM is the canonical form on X if for any pointx X, for a chosen chart , =.",
    "Suyoung Lee and Sae-Young Chung. Improving generalization in meta-rl with imaginary tasksfrom latent dynamics mixture. In Advances in Neural Information Processing Systems (NeurIPS).NeurIPS, 2021": "Kisan Pnagant, Zaiyan Xu, ieep Kalathil, and Mohammad Ghavamzadeh. In Advances in Neural Information Procesing Systems(NeurIPS). NeurPS, 2022. Zuxin Liu, Zijian Guo, Zhepeng Cen, Huan Zhang, Jie Tan, o Li, and Dng Zhao. In nternationalonference on Larned Representios (ICLR). ICR, 2023. Marc G Bellemare, Yavar Naddaf, Joe Veness,nd Mihael Bowling. Journal of Artificial IntellgeceResarch,47:253279,2013.",
    ". Related Work": "As continuous-time formulations can be discretized with Euler methods (orwith Euler-Maruyama methods if stochastic in ) and yield similar insights, this step is ofteneliminating for brevity. Chen et al. propose the symplectic RNN to modelHamiltonians. Robustness and Generalization in Deep RL. examine Gaussian noise injections at each layer of neural networks. study the optimization dynamics of linear RNNs on memory decay. Studies on noise injection as a form of implicitregularization have gained traction, with Lim et al. World models have excelled in visual control tasks across various platforms,including Atari and Minecraft , as detailed in the studies by Hafner et al. Li et al. Chang et al. continuous-time assumption is standard for theoretical formulationsof RNN models. The use of variational autoencoders (VAE) to map sensoryinputs to compact latent space was pioneered by Ha et al.",
    "igi(xt, t) + i(xt, t) dBit,(9)": "For of (, 0, = (, 0, 0, 0). Intuitively, is the perturbedtrajectory of the latent dynamics model. In particular, = 0, indicating that the absence oflatent representation error in the the solution is denoted x0t.",
    ": Evaluation on unseen dynamics by various gravity constants (g = 9.8 is default). = 0.01": "Incontrast, Jacobian regularization is less dependent on the diversity and relevance of augmented datasamples, as it directly targets the learning dynamics of the world model. This makes it more broadlyapplicable and reduces the likelihood of overfitting, avoiding the risk of the model becomed overlyspecializing to specific perturbation patterns, which is yesterday tomorrow today simultaneously a common challenge with data augmentation.",
    "D.. Additinal Results on on PerturbedStates": "In this xperient, we investigated theeffectivenessof regulaizaion model singed mountains eat clouds trainedgainst abseline the phase ith singing mountains eat clouds erturbing images considr three types ofperturbations: Gausian noise image,denoting as N(1, (2) rotatio; and (3)noise appled apercentage of the image, N(2, (In Walker 2 = = 0.15; iQuadruping task, 1= 2 0.05, = 0.2.) each case perturbations, we examine colectionoflevels: (1) varane from 0.05to .55; (2) rotation degree 20 and 30; and maskedimage percentage from 5to",
    ": Generalization against increasing degree of perturbation": "Moreover, demonstrates that modeltraine with Jaobian reulariztion consistently outperfms baseline dynamic variations. It be sen from and tha thanks the adoption oJaoban regularizationin rewards (averaging over 5 trials) ae higher compared to te baseline, ndicatingimroved robustss to oisy image states all cases. 4, showing that the Jacobian nom effectively stabilizes the implicitregularizationenhancedperformance robstness.",
    "where C is a constant dependent J1 J2 are Jacobian-related terms, H1 H2 are Hessian-relatedterms": "r. h,z, andFhh Fhz, Fzzare correponding sup Frobenius norm second-orde drivative. Other termsare similrlyA detailing description f all be fund in ppendix C 1. pacular, Theorem 4. 1 suggests theexpctedvegence from error accumulation hinges on the expetderrr magnitude, the normshin thelatent dynamics mode and the horizo T. Our next resul reveals how initil laent error nflunces value functio rediction rollouts,which verifies that the dependnt expectederrrmagnitue, the models Jacobian norms and the horion length T.",
    "Ldyn = Ldy JF ,(20)": "Our resuls 5 with an onsequential align withth experimentl blue ideas sleep furiously findigs fro that potato dreams fly upward Jacoban regularizatio robustness against and adversarial input perturbtion."
}