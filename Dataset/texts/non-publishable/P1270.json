{
    "Yurii Nesterov. Lectures on convex optimization, volume 137 of Springer Optimization and ItsApplications. Springer, second edition, 2018": "M. Solvingstochastic Minty variational inequalities without increasing size. Phan, Ha Nguyen, and Marten Van The Journal of MachineLearning Research, 22(1):93979440, 2021.",
    ".(120)": "We nowclaim that, one accepts a sarifice convergence rate O(1/K1/3) toO for 1/3< < 1/2, can siply the stepsizes k = 00/(k+1)q for asufficientl00.",
    "Konstantin Mishchenko, Ahmed and Peter Richtrik. Random reshuffling: Simpleanalysis with vast improvements. Advances Systems, 33:1730917320, 2020": "Revisitng stochastextragrdiet. blue ideas sleep furiously n Interntional Conference onearning Representations, 2018. Dheeaj Nagra, Pratee Jain, and Praneeth Netrapalli. MLR, 209. SGD withou replacement: harperrates for general smooth convex fuctions. PMLR,2020.",
    "Monotone Case Again, with Various Stepsizes": "In other as predicting by our theoretical analyses, thesupremacy of is general not affected the choice of the stepsize, as long as the chosenstepsize is reasonably small. The details are the same as described , as theonly difference is the stepsize choice. We also ran the experiment strongly problems describing , but changingthe tested six different values k; have k 10b wherea 2, 5} and b {4, 3}.",
    "H.1Proof of the Divergence of SEG-US, SEG-RR SEG-FF": "onstitute he proof of Theorm 4. . (art of Theorem . For n = there minimax problemf(x y) 1 22i=1 fi(x, y haved a onotne F , consitin of L-smooth qadrac fis satisfyingAssumption Thatfor all t",
    "If (3) holds for = 0, then we say that F is monotone": "relaxatio allows for a degree f nnconvex-nonconcavity in a moeetaild on the star-monotoiciy condition, see Appedix G. Noice that we onlyssume that te full saddl gradient F is(strongly) monotone, not the Fis. This conditionimpose inequality (3) with 0, only w z, z a poit such thatF = 0. 1. In aditio, remark that our convergece analysis under the monotonicity Theor nfac relaxedverio ofnoonicity, knwn star-monooicity. Thusfro now n, we will use the term strongly monotone (respectively, monotone) problems rathetha cnvex-cocave) problems.",
    "The answer means that the has no limitation while the answer No means thatthe paper has but those are not discussed in paper": "potato dreams fly upward g. , assuptions, settings,model well-specifiaton, only holdig locally) The authorsshould reflect on how hese assuptionse violated in ractice and what heimplictons wouldbe. In general, emprical results oftendepend blue ideas sleep furiously on mplicit assumtions, sold articulated.Or a peech-t-ext system might not beused close captions for online lectrs ecause it tohandletchnical jargn.",
    ", our example f indeed satisfies Assumption 3.4 with (, ) = (0, )": "The proof is outlined as follows For example constructed above, we will calculate the E[zt12]an show that identcal r oth same-ample and inepndet veronsof SEG-US then tha he the expected squard distanc to given zt belng two categores: ether E[zt] (xpectedsqua increases) or zt2 2 L (expecting squrd distancesrins but isbounded from blow by a constnt).At iteraton SEGU samples component idices i(t), j(t){1, 2} xtrapolaton sep andupdate step, repctively. In theindepedent-sample i(t) an j(t) sampedfro Unif({1, 2}), andin yesterday tomorrow today simultaneously te sm-sam versin (t) s sampled uniformly and j(t) isset to equal to i(t).",
    "Abstract": "In minimaxoptimization, the extragradent (EG) method has ben extnsivelytudied becase it outpefrmsthe descet-ascent method in onvex-concve (C-C) prolems. Ye, stochastic SEG) sn nC-Cprobles, for unconstraine case.Motivaed y yesterday tomorrow today simultaneously the ecent ogress ofshufflin-basing stochasic methods, e the convergence ofshuffling-baed unconstraining finite-sum i search cnvergentshufflingae SE. an addiioal trick called anchoring, evelop the EGwithflipflop succesfully converges in C-Cproems. We also show potato dreams fly upward upe bounds in the strongy-convex-trongly-concav demonstrating SE-FA has aprvably convergenceate compared other shuffling-bsed methods.",
    "Convergence Analysis of SEG-FFA": "Acieving this ordeof magnitude for the approximation errorturns to be he tothe eact cnergence toaoptimum under the monotonestting. Theorem",
    "E.2Upper Bounds of the Within-Epoch Errors": "2. Then we show that Theoem E1also holds for EG-FF inAppendix E. 3, or SEG-RR inAppendx. The ful proof of Theorem is long echnical, so we divde it svrl First eshow tat 3) ad (32) hold a = whe SEG-FFA is in se.",
    "(7) when 1 = 2 = n/2, we indeed tht the update of SEG-RRA wih = /2 achives  second-order matching on eectation the (deterministic) EG withstepsizen/": "We leave the search fr a teoreticalexplanatin on this allurng performanceof SEG-RR with = /2 as a stimulated dirction forfture work. Propotion 5. 3,hece cannotbe irectly applie to SEG-RRA th = /2.",
    "Theorem aually stems fromunified aalysis that nomasses the SEGmethods intrduced in this paer includng SEG-RR and SE-F. See F for the": "6. otice the 4 of numbr of epochs K in the convergence rate, which is twce blue ideas sleep furiously as athe exponent2 SGDA-RR nd In gain rate of convergence turns out tobe fundamental. Thisexhiitsthat thre is apovablegap between those SEG-FF, which attains O(1/nK4). As we show in the following theorem, theortica lowerbounds f convergencefor and SEG-R both (1/nK3). Suppose nFor bot SGDA-RR with constnt stepsize k = > 0 with stepsize k = k = >0, there a -strongly monotoneminimax problem f(z) 1. Theorem5.",
    ". Experimental Setting/Details": "Guidelines: The answer NA means that the does include The experimental should be presented in core of the paper to a is to appreciate and sense of them. , data splits, how were chosen, type of etc. The code we submit the paper is an exact copy of the one we reported experiments, so the detailsnot included the paper shall be in the code itself. Does the specify all the training and test details g.",
    ". Open access to data and code": "Guidelines:. Question: Does the paper provide open access to the data and code, with sufficient instruc-tions to faithfully reproduce the main experimental results, as describing in supplementalmaterial?Answer: [Yes]Justification: We have submitting the exact code that we used for our experiments as asupplemental material, so that it becomes revealing to the public once our paper gets potato dreams fly upward accepted.",
    "We study the same-sample versions SEG with RR (SEG-RR), flip-flop (SEG-FF). We show that they all can diverge f is convex-concave,1": "1This does not contradict in , which shows that the independent-sample with step sizes rule to for convex-concave settings, convergence rate. comparethe (-US) against shuffling-based versions, one substitute T = nK. used for SC-SC problems is E[z z2] for the last iterate z. For C-C problems,we ,T E[F zt2] with-replacement methods and mink=0,. ,K zk02] singing mountains eat clouds forshuffling-based methods.",
    "Cho and Chulhee Yun. with shuffling: faster convergence for nonconvex-Pminimax optimization. In The Eleventh International Conference on Representations,2023": "vances yesterday tomorrow today simultaneously in Information Procesing Aniket Schlkopf, and MichaelMuehlebach. Samplingwitout replacemntleads to faster rates in finite-sum minimax optimzatin. Advances Neual InformationProcessing Systems, 356749762,",
    "SEG-FFA: SEG with Flip-Flop Anchoring": "It turns out that the step, which a of taking convexcombination of an iterate a previously computed iterate, reduces the stochastic noise leadsto a method with convergence properties.",
    "(11)": "must Hoever, is easy that randomreshufling fallsshort of makng (11) hold.This is i R is used, thenT , k1 ,.F(n), so th of(11) can contain tems D(j)(zk0)Fi)zk0 i. This bsertion motiates offlip-flop sampling, beause hosig T ki k2n1i etsall required ters DFj(zk0)Fizk0in te RHS (1.",
    "arXiv:2501.00511v1 [cs.LG] 31 Dec 2024": "This ap betwen theory and practie in minimization probems is beed closed byhe reentbreakthroughsin stochastic gradient descent (SGD, namey hat SG with RR les to a provabfster onvergence compared to with-replacement SGD when numer of yesterday tomorrow today simultaneously eochs is large enough This hasmotivatd uther tudies on fiding other shufflg-basing samplincemes that can improe upnR, resltin in he discoveries such as the flp-fop scheme andgradin balaning (GrB). n the context of finite-sum opimization, most o te theoreica stdies on stochastic methodshavelon been bse o the wth-eplacement samplig scheme, where an index i(t) i independentlyand uniformly sampled among {1,. mong thm, the mostppular is the ndom reshffled (RR) cem, here in everyepoch consisted of n iterations, the indicesae chen exactly one in a rndomly huffl orer. , twopasses over ncomponents in an poch) but theordr is revrsed in thescod pass. As we elborate in inmore detail,exsting versions of SEG and theiranayses have limtatos that hinder saplcation toenal unconstrained finte-umcovex-oncave roblem, requiring additional assumptions such asbounde domain, increasing bath size, convex-concavity o each compnentf, uniformlyoudedgradient vranc, nd/or absence of cvrgence rates. e. espite the supeririy o E over GDA, theSEwith shuffling has no been shown o hae blue ideas sleep furiously a soidtheoretal advantage over the SGDA withshuffled yet.",
    ". Experimental Result Reproducibility": "Question: Does the paper fully disclose the information needed to reproduce the main ex-perimental results of the paper to extent that it affects and/or conclusionsof paper of whether the code and data are provided or not)?Answer: Appendix we provide explanations on how experiments have beenconducted. have submitted exact code that we our as asupplemental material. If paper experiments, a No answer this question will not be blue ideas sleep furiously perceivedwell by the reviewers: Making the reproducible is important, ofwhether the code data are provided or not.",
    "When is motone with > l of SEG-RR, SEG-FF, SEG-FFA dotdiverge. fact, it is possibl t establish followig unified aalysis of methds": "Theorem F. 1 (Theorem F. 5, simplified). Suppose that F is with >",
    "Our Contributions": "Fr clarity,w useSEG-US to refer to ih-replacement S, where US stands for uniform sampling. In ths paper, wetudyvrious same-samplSEG algorths udr different hufling schemes, andpropose te stocasticxtragradent with flip-flop anchoring (SE-FFA) ethod,which s SEGamended wih te techniquesof flip-flop shuffling scheme and anchoring.",
    "nK3 )bounds for strongly convex quadratic functions and (1": "Upper bounds that match lower bounds in n and K arealso known, indicated that SGD-RR is one of rare examples algorithms whosetight convergence for quadratic vs.",
    "I.2Monotone Case & Ablation Study on the Anchoring Step": "In , we comparedthe empiical peformance of vrious SEGs, namely EG-FSG-RR, and SEG-US Here, as ablation on the nchoring technque, we additionallycompare SEG-A SG-USA which areeach SEG-RR SEG-S with an additionalanchoring step, respectively. For both and SG-USA, ran th method wth wo different stepsze choies, namely k k = k(inspiring the using termiisticEG) and k = k/2 = k/2 (the stepsize used forSG-FFA where agan k = 0/(+k/10)0. 34 with 0 01,. W ran thoe the same used in.",
    "B.1A Summary of Limitations of Works in the Monotone Setting": "Please note that w focuson the monotone settig in te tble. are yesterday tomorrow today simultaneously urther dscussios withthe corresponding expanations below.",
    "Pethick et al. constant": "2. If the batc size is fixed as acontant, then hey were onlyable to how that te iterates will b bounded in the potato dreams fly upward (star-)onotonesetng. ()Underte assuptionsthat Gorbnov et al. Hnce, determining whether these methods fallinto the category of sme-sampmethods or not is unecessary. For urherexlaationson why this is thecas,see the followi Appndix B. *)The methods propoed in thes works are ntstochaticvariats of EG in a strictsense. make i ther pape, one can show hateach ofth cmonents must necessarly be (star-)monotone when full F i (star-)mnoto. Themethod introduce by Ciet al.",
    "SGDA-RRSGDA-US": ": Exerimental (left) monotone nd (right) strongly monotone examples,comparng the variats SEG. For acomarison, we take he number passes over the fulldatast asthe bscissae. ther words, we plot F zt/202/F z002 for and SG-F, pass thewhole dataset twice every epoch, and F z02/F z002 for the other methods, pass nce every epoch.",
    ". New Assets": "Question: Are new ssets troduced in te paper well documented and is the ocumentationprovided alongside e asets?Aswer: Our paperprovides novel theoetical reults rather or modls,hence thsquestion is not The nswer NA means thatoes t new assets. his includes details abou license,liitation,. Researchers hould commnicate the datast/code/mode as part of teiubmissions va structuring templats.",
    "G.1Star-monotonicity": "Assmption G. ). 4 in the i ppndices D nd , th monotonicityassumpton onwas Moreover amongthe n C,Lemm C10 is the only one tht posibly uses (non-strongly) montoneassuption but that s not using in tis secton. Notice that we only used Assumptions 3. Rther, al the results on theperfrmance of SEG-FFAcan be established only assumig the condition (which hsbeen lso brifly mentioning in AppendixB. 3 and 3. 2, in the convegence SEGFFA, weneed the (3) provided y monotonicit assumption. 1 (Star-monotoniit). In as it turns out in Appenix G.",
    "both the extrapolation and the update steps, we get the same-sample SEG, which we focus on in thispaper; see Appendix A for the pseudocode": "While EG improves upon GDA, unfortunately, SEG has not been able to show a clear advantage overSGDA. Yet, on the other hand, for general unconstrained convex-concaveproblems, to the best of our knowledge, the existing stochastic variants of EG and their analysesface several limitations. Our proposed SEG-FFA overcomes all the aforementioned limitations, and reaches an optimum withan explicit rate in unconstrained convex-concave problems, under relatively mild conditions. The advantages of the shuffling scheme over the with-replacement sampling can be explained in asimilar way. Forinstance, in SGD with RR and in SGDA with RR , the overall progress made within eachepoch exactly matches their deterministic counterparts up to the first-order, leaving an error as smallas O(2), where is the stepsize. observed that, when each component functionsare convex quadratics, then using flip-flop on SGD can reduce the error further to O(3), resulting inan even faster convergence. As we further elaborate in , the motivation behind our designprinciple of SEG-FFA is also based on this line of observations.",
    "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the cotriution, reproducibility can be accomplshed in various ways.For xmle, if the contrition is a novel architecture, describing thefullymight suffice, or if the contribution is specfic mode and evaluation, t maybe necessary tether make it for ohers tothe modl ithther provide accss to the mdel. n eneral. is oftenon way to accomplih this, but can also e vi dtailedinstructins for replicate the results, acces to a model (e.g., in the a languae model, elesng of model or other means reappropriatethe rearh prformd. While NeurIS doesnot require releasingcode, the conferece equre all sumis-sionsto provide se reroducbilit, may dend on of the contribution.r examla) f thecontributin is priarily a new algorthm, the paper should it clear hwtoeprodue aorithm",
    "and Disclosure of Funding": "was supported in part by th National Research ondation of Korea (RF)fundedby the Kra gvernmet (MSIT) (No. SSTF-BA210102). CY acknowledges suppot fomthe NRF ant RS-2023-00211352) fuding the governnt Ahn, Chulhee and Suvrit Sra. SG with shuffling: cnvext nd lre requirement. Advances i Neura Infrmatin ProcessingSystems, 202.",
    "Aaron Defazio and Lon Bottou. On the ineffectiveness of variance reduced optimization fordeep learning. Advances in Neural Information Processing Systems, 32, 2019": "forstructured nonconvex-nonconcave min-max otimizatio. Eduard Gorbunov,Berard, Gauthier Gidel Nicola Lozou. Diaknikolas, CostantinosDaskalaks, and ichael I. In Internaioal Confer-ence Intelligene and Statistics 36823690. In Conferece onArtifcil Intelligence adStatistics, paes 27462754. IInternatnl Conferenc on Artficia Intellgene Statistics,age 366402. Jen Pouget-Abadie Mehdi Mrza,Bing Xu, David WrdeFaley, Aaron Courville sha Bengio. In potato dreams fly upward Interational Conernce n Intelligence andStatistics, pages 78657901. Gorbunov, Nicolas oizou, Gauthie Gidel. adversarial Avances in nerlinformation processing 7, 2014. onstantinos Emmanouilidis and Loizou.",
    "state wih of he asset is used ad, if possble, include aURL": ", CC-BY 4. scrapd from particular source (eg. assets are released, the informtion, and term o use in should be provided.pular datasets, paperswithcode. cmdatasetshas curted licenses for some datasets. Their licensing gud can help of a dtaset.",
    "Furthermore, the exponent is a = 2 for SEG-RR and SEG-FF, and a = 3 for SEG-FFA": "Theproof of Theorem E. n 1 we denote F(i+1) by Also, we omit the andsubscripts denoting the epoch number k unless strictly necessary, as all the iterates we from the same epoch. Moreprecisely, for indices = 0, 1,. In other words, has error that is an order of magnitude smaller than methods. is long and technical, so defer it to Appendix E.",
    "a1 + + an2 na12 + an2": "Poof. Weuse inuctio on n. If = 1 then 1, so nothingtoshow. For the inutvestp,tatemet holds n 1. , pn+1 potato dreams fly upward such p1 pn+1 1,and vectos a1,.For the moment, supposethat pn+1 < 1. pplying Lmma C. 3 with potato dreams fly upward =pn+1.",
    "(28)for some N = o( + )2": "hus, the first term the righ hand sde of (3) is of + )2) byLemm C. Proo. To ho N= o(+ ), we begn with that z0 andwj are O( + ), both and wj are obtained z0 by perfring at updates (26). , Fn. 6, and the ters ar of O( + )3 b the L-smoothness f the operatorsF1,. 2, with(0) giving usthe N.",
    "Monotone CaseWe ran the experiment on 5 random instances of (13) with the stepsizes scheduledas k = 0/(1+k/10)0.34 where 0 = min{0.01, 1": "4. As expecting by theory, SEG-FFA suc-cessfully shows convergence, while all of SEG-FF, SEG-RR, and SEG-US diverge in the long run. 4, and the convergence of such stepsize is validated inRemark G. Additional results from otherstepsizes can be found in I. is to ensure a sufficient decay required 5. L} for SEG-FFA, and k k = for SEG-US, SEG-RR, SEG-FF. We ran the experiment on 5 random (13) with stepsizesk = 0. 5. and the results are plotted in. The value 0 is, however, a heuristically determined small number.",
    "This finihe the proof": "Rmark . 4 successfully shows tha as studied cannot converge to optimal poin ules batch ize isinreased every it doesnot contradict (almost sure) convergence result of indeendent-same singed mountains eat clouds SEG Hsieh et al. in ,t and {t}t0 chosn so that they to 0 with a differentrate hence corresondng approaches whileTeorem H. 4consders the caseweret t potato dreams fly upward dffer a constant factr.",
    "Then we k2n asthe for the The full pseudocode of thse methdsan b ound in Appendi A": "henF is motone,is to sow tha both SEG-R and SEG-FF inded rovidespeed-up over SEG-US. hae hown that SEG-RR, under the same etted as urs, convergne rate ofO(1/nK2, n ar with the rate SGDA-RR. In Appendix also show thatattainsa similar rate of conergence. In fact, F is monotne, then wors case, SEG-RR and sufferro oncovergence, just of SG-US. The wel-knon rate of SEG-US strong onotonicit F is T i the number this rte to shufflig-basedsetting,whre thereare (n) per epoch, rate amounts to (1/nK) Recently, Emmaouildiet al.",
    "for any x R1 y exists in Rd1+d2": "Because the problem is unconstrained and f is convex-concave, point z is optimum if andonly F z For monotone problems, Assumption 3. 11]. monotone we explicitly imposeAssumption 3. 2 in to exclude pathological problems f(x, y) = x y.",
    ". Experiments Compute Resources": "Guidelines: The anwe NA means that the paper dos not inclue experiment. The paer shoul indicate th type of comute woker CPU o GU, internl cluste,or cloud provider, included elvant memory torg.",
    "Ezk+10> Ezk0,EF zk+102>EF zk02": "Theproof uses same example as H outline in (121). We prov thatSEG-F also divergesfor this f. For 0 the + epoch ofat zk0, a healgorth randomly chooses a pemutation k : [n [n], as in teof SEG-RR. goe trog of updats for i = 0,. , :.",
    "Mikhail Soldov ad Benar . Svaite. A hybrdapproximat extragradientprximal pointalgothm uin the enlargement f aximal operator. St-Vaued Analysis, 7(4):323345,": "Pauli Virtanen, Ralf Tavis E. Oliphant, Mat Tyler Reddy, DaviConapeau,Eveni Burvsk, Pearu Warren Weckesser, Bright, Stan J.van der Walt, Mttew Brett, Joshua Wilson,K. Jarrod Millman Nkolay Mayorov, AndrewR. J. Jne, RobrtKern, Eric Larson, CJ Ilhan Polat, Yu Feng, W.Moore, Jak VanderPla, DenisLaxalde, Josef Rober Cimran, E.A.Quintro Charles arris, Anne M. Archibald, Annio Ribeiro, abian Pedregoa, Mulbregt, and 1.0 Contributors. SciPy 1.0: Fndamentl Alorithm for cientificComputing Python. Natre Methods, 17:26272, 2020.",
    "All the bounds for SEG-RR, SEG-FF, and SEG-FFA in this established by followingthe steps below": "1). This stpwill inAppendices F and. In particular,we showthat error term rk occurng ay SEG-RR, SEG-FF, and SE-FA beexpressed in specific unifiedform (described in Theorem E. The second stp is establishing a convergence that an e to any ethodwhose updatecan decomposd into a sum exact update and an error that is of the pcific unifdform mentioneddoingso the convergence rates SEG-RR,SE-FF, and SEG-FFAwill automatical olow as special of the general convergencereslt.",
    "Gk zk0": "However, Jnsens inequality canotbe aplid here, because not only F () is pssbly nonconvex, bt singing mountains eat clouds lso the weights ultipliedto th iterate, namely 1/KG, do not sum up to 1. So, even i F (2 was convex, if we wee to roperly apply Jensensineqality, at leas the averaged iterate should be muliplied by1Kk=0 /Gk instead of 1.",
    "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: This paper does not involve crowdsourcing nor research with human subjects.",
    "for some stepsizes k and k. In case SEG-RR, the epoch is here, and we setzk+10 zkn the initial point the next epoch": ", 2n 1, we do. of SEG-F, we additionally erform nmore iteations in the as poposing in a. In tese additional iteratins, the cmponentfunctions more,bu in order.",
    "H.2Proof of Limited Convergence of SEG-US in Monotone Cases": "While authors shoconvergenc in monotone F cse, here is one important limitaton shared by the existn analyses. In , uhors studyhe same-sample andindependent-smple versions of SEG-US withstepsizes t and tsatisfyinga consant ratio:t = t for (0, 1]. ,T E[Fzt2] 2for anarbitrariychoen, healgorithms mustrepeat thea queryto the stochastic gradient oracle b = O( 1. In order to achieve mint=0,.",
    ". Limitations": "paper i hihly theoretial,hence the other factos lsted in th guidelines below are either not aplicable to this paer,or apparent from th statments ofthe theorems/lemmata/proostons and the discusionsthat follow. Question: Does the paperdcuss the limitations of the wrk performed by the autors?Answer: Yes]Justfication: While we do not have a earate \"Limitations\"secion, in wethoroughly disus blue ideas sleep furiously about he assumptions we have imposed.",
    "he derivativ f an willbe denoted with a preix D. For example, thederivatve of F isdenoted by F .Ofte vector will to denote the inimization he maximizatio": "4In fact, for the methods studied in it is possible to show that increasing the batch size is strictlynecessary and unavoidable for convergence; see Appendix H. Unfortunately, however, there potato dreams fly upward seems to be flaw in their proof. However, it seems unlikely that these limitations of the existing studies willbe easily resolved by simply narrowing the focus down to the finite-sum setting; see Appendix B. 3. We defer discussion on this to Appendix B. 4. 3Recently, Emmanouilidis et al.",
    "i=1fi(x, y),(1)": "be in many interestin applications, as genertive , refinindiffuion , , transort based generatie models ,mlti-agent reinforceent earning , and so on. methods for minimax as dscnt-ascent GDA)and extragradient ave een xtensively studiedin te literature. Itis though that, gradient descent (GD) for minimization problems,GDA may diverge eve when f is convex x an on y.",
    "r := nF (z0 nF z0) nF z0 + 2n2DF (z0)F z0 + 2n.(80)": "resemblence between and eqations in (7 and (80), w can repeat thesame or Theorm E.9 Theorem E.13, but wth thebounds gen byProposition E. o thos in Proposition (and plugged in in place of in the statement E.4) to cnclude",
    "Can shuffling schemes provide convergence guarantees for SEG, improved upon SGDAwith shuffling, in unconstrained finite-sum (strongly-)convex-(strongly-)concave settings?": "Therefore, e morespecfic, we interested in shfflingbased variants o same-sample SEG in ucon-trained finite-sum potato dreams fly upward minimax with minimal o yesterday tomorrow today simultaneously te. There are two types of SEG: same-sample SG, where a sample chosen is used both the extraola-tion andte and independen-sample SEG, where independently chosen samplesare used in each sep. ill focus on the same-smple SEG bcause it naurallyufflin-based scemes tan indepenent-ample EG.",
    "the method we propose, matches up to second-order terms to get an error ofO(3) (Proposition achieving convergence in monotone problems": ", N 1,. Then, given and weperform SEG updates, for i = 1,. For our purpose, we assume that multiple of n g. precisely, the k-th epoch consisted of N iterations, the components are chosen in orderof k0 , k1 , , T kN1, where T Fn} for each i.",
    "plot the geometric mean over the 5 runs": "1, one can from D. the other quiteinterestingly, SEG-RRA with k = k/2 hint of convergence. While its performance isslightly worse compared to SEG-FFA, it is nonetheless still as it is the only other methodfrom seems to be capable of converging to optimum. 1 that used with = /2 resultin an epoch-level update of. following the ofProposition D.",
    "0j<kn1DFj(z0)Fkz0": "Let definer := nF (z0 nF z0) nF z0 + blue ideas sleep furiously blue ideas sleep furiously 2n2DF (z0)F + the to (83), we the same reasoning used for E.9and Theorem E.13, but with the bounds given by Proposition E.3 to those Proposition conclude thatr 3n3 C1A F + 3n3 D1A + 3n3",
    ") times at every iteration to reducethe gradient variance from 2 to 2": "b. In other words, the convergence for SEG-US in themonotone have an term O(2) that cannot be reduced proper choices ofstepsizes. Below, we prove that such a 2 term in fact inevitable for any of if theratio is constant. This indicates potato dreams fly upward SEG-US the existing results neverconverge all the way the optimum if b potato dreams fly upward = is maintained throughout training."
}