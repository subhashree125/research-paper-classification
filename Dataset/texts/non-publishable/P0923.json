{
    "Amazon KDDCup 2024: Multi Task OnlineShopping for LLMs": "Participants have no access to the test dataset and can build olu-tions based on questios. The KDD Cup 2024 is asa code competition. eciveonlythescores on the ful ShopBench dtaset via the leaderbord. submitmodel weights with coewhich will be evaluad by Amzon.",
    "LoRA name (2)v9bv7bv9bv7bvbLoRA 2 weight (W2)0.750.50.250.5.25LB B+W1xM1+W2xM2.80.7910.7460.7610.788": "Theirst solution istosubmit the base model meged with the firstoRA adapter, scaed by singing mountains eat clouds eightW1. If we enembl two adpters(LB B+W1xM1+W2xM2), we observe that the LB score improvesbtween 0. summrie the effect of ensembling multiple modes. 5 by additional fine-tunig. Next, we ine-tuning Smaug-7B and Qwen2-72Bo our trainingdatast. 004. 4. We cmpre h zer-shotversion (SZ) with fe-tunevrsion (FT)as seenin. 001 to 0. We achive significantgains of00035 to 0. he performance of themodels are euivalent to public LLMsenchmarks. Qwen-7B SZwould sore thplaceon Track 5, demonstrating that base model provide gretcapbilities withoutfine-tuning.",
    "Logits Processors": "We variety logits processors to generate outputs inspecific formats. For multiple choice, and retrieval ques-tions, we our to produce digits and com-mas. These logitsprocessors were particularly useful in Phase 1 we utilized lesspowerful models.",
    "Ensemble Adapters": "4. We cal them v7, v8, v7b ad v9b. 1,3,5 we merging 8to model Qwen2-72B 56% wgt exlained i.",
    ",Dotte et al": "Pranav Shyam, yesterday tomorrow today simultaneously Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Ian Sohl, Natalie Felipe Pet-roski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Preston Tuggle,Nick Turley, Jerry Tworek, Juan Cern Andrea Arun Voss, Wainwright, Justin Jay Wang, Alvin blue ideas sleep furiously Wang,Ben Wang, Jonathan Jason Wei, CJ Weinmann, Akila Welihinda, PeterWelinder, Jiayi Lilian Weng, Wiethoff, Dave Willner, Clemens Winter,Samuel Wolrich, Wong, Lauren Workman, Sherwin Wu, Wu, MichaelWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng,Juntang William Zhuk, and Barret Zoph. 2024. Technical Report. arXiv:2303. [cs. Wainwright, Chong Zhang, Sandhini Agarwal, Slama, Ray, John Jacob Fraser Luke Miller, Amanda Pe-ter Welinder, Christiano, Jan Leike, and Lowe. languagemodels to follow instructions with human feedback. CL] Pal, Deep Manley Roberts, Siddartha White. Smaug: Fixing Failure Modes Preference Optimisation withDPO-Positive. arXiv preprint (2024). 2024. In Forty-first Conference on Learning. Chandan K. Reddy, Llus Mrquez, Fran Valero, Nikhil Rao, Zaragoza,Sambaran Arnab Biswas, Karthik 2022. Shopping Dataset: Large-Scale ESCI for Improving ProductSearch. arXiv:2206. 06588.",
    "NoSource DatasetTaskTask TypeAdapterSizeLLMAdditional Explanation": "Given queries31KDD Cup 2023New Idearetrievalv810000NoGiven purchase previous clicks (similar to task 14)32ESCI-dataNew title pick brand33ESCI-dataNew Ideamultiple-choicev9b10000NoGiven query product pair, is relationship? E S I34ESCI-dataNew Idearankingv9b10000NoGiven list of query product pairs, which most related to related35ESCI-dataNew Idearetrievalv9b10000NoGiven query, select products which are exact match not substitute, complement, or 2023New Idearankingv9b10000NoGiven a product title and multiple rank the based the helpfulness37Alpaca CleanedNo Changesgenerationv7, v851760No38MMLUNo Changesmultiple-choicev7, v7b,",
    "All authors contributed equally to this": "Copyrights for components of owned others than theauthor(s) must be honored. Abstracting with credit is permitted. , , 2024 by owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM.",
    "Quantization / vLLM": "Each participant could (to each track)a GitLab repository of maximize size 100GB to executed on 4xNVIDIA GPU each 16 GB GPU memory within",
    "One example the development datase. It amutipl question aswering tasks for unerstandingshopping concepts": "20,000 questions 57 tasks covering 5task types (e. g. retrieval), test yesterday tomorrow today simultaneously LLMs capabilites in the domain (see an example in Thispaper describes our final solution and study on ourexperiments.",
    "Ypen Hou, Zhankui An Yan, Xiusi hen, Julian cAuley.224 Language and Items for ad ecommendation. arXipreprint arXiv:403.03952 (2024)": "em-ory anagement Lrge Laguage Servn with PagedAttention. arXiv:2307. 09688 Woosuk Kwon, Zhuohan Li, Siyuan huang, Ying Zheng,ody Ho Yu, Joseph E. Amazon-M2: A MutilingulMulti-ocale Shopping Dataset Rcomendation and Text Generation. IProceeigs of the IGOPS 29th Symposium on Operting Prncipls. Pong, TollyPowell, Power,Boris Powe, Elizabeth Raul Puri,Radford,Jack Rae, ditya Camrn Raymon, rancis Real, Kndra Rimbach,arl Bob Henri Roussez, Nic Ryder, Saltareli, Tedandes,Shbni Santurkar, Heather Schmidt, David John Schul-man, Slsam, Kyla Toki Jessica Shieh, Sarah Shoker,.",
    "ADetails on Training Dataset": "In , we provide overview the datasets we generated. We describewhich task the is most similar to (column Task), the size and if a LLM was used. Finally, we explanationfor our own ideas.",
    "Abstract": "We fine-tun Qen2-7BIstructon ou own trained dataset. challenge was to build useful assian, answering questionsin the domain of onne shopping Te ompetition contained 57diverse tasks, covering 5 ifferent task types e. We employed Logits Proces-sors o constrain modl output on relevat tens for tsks. As the competitionre-lased only96 exmple singing mountains eat clouds questions, we developed ourown trainingdataset by procssing multiple public datsets or ued Large Lan-guage Models for data augmentatio and synthetic data generation. multiple choice)nd across 4 differet tracks (e. We apply wise-ft to account for distriution shifts and ensemblemultiple LoRA adapters inone mode. Our soltion achieved the first pace in each indiidual trck andis first place overall of Amazons KDD Cup 2024. multi-lingual.",
    "Bagel-34B-v0.5 0.70070.66090.63390.58710.6834LLaMa3-70B 0.78060.65320.66580.62370.7183Smaug-72B 0.71780.65640.64840.6975Qwen2-72B 0.79820.64070.71930.69180.7486": "The model is about at fp1.Therefore inordr fit ino dis and size used 4bitquantization which rduced its size to pls using the library acceleated ur in-ferenc whih allowed ou model toaswer all the yesterday tomorrow today simultaneously qestons withinthe lmit. Tracks 1-5 had 189, 2373, 349, and 11720quetons to be answerein 70, 20, 30, 20, 140 AWQ quantization accury by calbrating withthe development questions. compared WQ versus and found both to b about in spee andacc-racy.AWQ quantizatio for Qwen2-72B takes about 15 hors on1xA100GPU to process. In fr AWQ Qwen2-72B to with vLLM, we needed to pd the unquantize modelwith to change the shape the weightsbefore",
    "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, CarlosGuestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: AnInstruction-following LLaMA model": "Wrtsman, Gabiel Ilharco,Jong Wook Kim, Li Simon Korn-blith, Rebecca Roelofs, Rapael Gotijo-opes, Haishirz, li Fahadi,nseok Nmkoong, Ldwig Shmidt. obustfine-tunn of zero-shotmodels. arXiv:arXiv:19. Qwen2 echnical Report. arXv:2407 CL].",
    "Model4.1Prompt Template": "Whe using zero shot with instuctin tuned LLM, we oundit helpful to use bot te system role and user rol when formattingprompts. Designng better prompts imroved zer shot modelsperformane. When fine-tuning, w found that th prompt was not asimpo-tnt becaus mode is fine-tuning to exhibit a certain behaviorgivenwhtever promt we chooe t tai with. Specifically, we used thefollwed temlate.",
    "Synthetic Datasets": "In gneral, we using three differentmethods prompt LLMto nruct tasks promtsheseed For xaple, we rehrae the original fro NingLabECnstruct ataset These task inclu information blue ideas sleep furiously aboutth (title,attibutes) and we combineall fthem into prompt. Dataset No Thecorec wasselected from the E entrie, theremainig options selecte the entri labels(see 2126).",
    "Results": "026 blue ideas sleep furiously the place (). As 1st individual track, is 5, the sum ofour which is the highest score. Each submission for individual track is based the key con-cepts of fine-tuning Qwen2-72B model on our developed trainingdataset and optionally, ensemble multiple versions and/or The submission might differ slightly in the fine-tuning time,exact of trained dataset and ensemble combination. We provide ablation in , and 6. Some values aremissing in tables due failed submissions and the were sufficient blue ideas sleep furiously decide the experiments. First,table 4 different base models without",
    "Conclusion": "We optimizing inference with 4-bit quantizationand vLLM to run a 72 billion parameters model on 4x NVIDIA T4with each 16 GB GPU memory in the time constrain. The KDD Cup 2024 was a great competition with a diverse set oftasks to evaluate Large Language Models capabilities in the domainof online shopping. In addition,we share multiple experiments as ablation study. It was essential to fine-tune base model with an additionaltraining dataset. We ensembled multiple LoRA adapater, appliedwise-ft for distribution shift and constraining the model output witha Logits Processors. The code competition design ensured a faircomparison of solutions. Our team solution is a single models withmultiple optimization methods, which scored 1st place on eachtrack."
}