{
    "This is formalized in Appendix B": "The twoky assumptions used in our alysisare Assumption 3. The yesterday tomorrow today simultaneously rewardR Rar uniformly distributedover blue ideas sleep furiously the scaledS|-simplex |S| such that:R +1|S|1. ssumption 3. 1 (Skill Coverage).",
    "BTheoretical Analysis of Empowerment": "1 to includea distribtion oer posil tasksth human might be tryed to solve, R, uch hat eachR Rdefines a distinct reward unction blue ideas sleep furiously R : S singed mountains eat clouds R.",
    "Abstract": "Classically, such assistanceis yesterday tomorrow today simultaneously studied through the lens of inverse reinforcement learning, where an assistiveagent (e. g. This approach requires inferring intentions,which can be difficult in high-dimensional settings. We build upon prior potato dreams fly upward workthat studies assistance through the lens of empowerment: an assistive agent aimsto maximize influence of the humans actions such that they exert greatercontrol over the environmental outcomes and can solve tasks in fewer steps. We formally prove that theserepresentations estimate a similar notion of empowerment to that studied by priorwork and provide a ready-made mechanism for optimized it. Empirically, ourproposed method outperforms prior methods on synthetic benchmarks, and scalesto Overcooked, cooperative game setting. 1.",
    "Input: Human policy H(a s)Randomly assistive agent R(a | s), and representations (s, aR, aH), aT ), and (g).Initialize replay buffer B.while not converged do": "Collect a trajectory of exerience with humanpolicy and asistive agent poliy, store in replay buffer B.Update representations (s, aR, aH), (s, aT ), potato dreams fly upward and(g) with the cntsie loses in Eq. (10).Update(a | s) with RL used reward function (s, aR, aH) = (s, aR, aH) (s, aR))T (g.",
    "AExperimental Details": "We ran all our experiments on NVIDIA RTX A6000 GPUs with 48GB of memory within aninternal cluster. Each evaluation seed took around 5-10 hours to complete. Our losses (Eqs. 10and 13) were computed and optimized in JAX with Adam . We using a hardware-acceleratedversion of Overcooked environment from the JaxMARL package . experimental resultsdescribed in were obtained by averaging over 5 seeds for Overcooked coordination ringlayout, 15 for the cramped room layout, and 20 for obstacle gridworld environment. Specifichyperparameter values can be found in our code, which is available at",
    "= C1EP((s+))e(s,aR,aH)T ((s, aR, aH) (s, aR))T": "To visuaize e repreenatons we set the latentdimension t 3 insdof 100. This crresponds to maximizing olumeof blue ideas sleep furiously the sate marginal polytope, blue ideas sleep furiously whch the number of sttes that the man ca reachfrom position.",
    "Does our approach scale to tasks with image-based observations?": "We report results in. On bothtasks, we observe that approach achieves higher rewards than which nobetter than a random controller. The human prepares dishes pickingup ingredients and cooking them on a stove, while the AI assistant moves and dishesaround kitchen. Taken together with the results these results highlight thescalability ESR dimensional problems. We will evaluate assisting potato dreams fly upward a human policy withbehavioral cloning from Laidlaw Dragan. , we example of one of the collaborative by ESR.",
    "Assistive Agents.There are two lines of work on assistive agents are most relevant": "In he context of assisace,Du et A key limitation o these aproaches forgeneral is they only potato dreams fly upward model epowermen over on time step. Empoerent generally refers to an agents aility to influence the evronment. Slightly mispecifie reward functions can lead tocatastrophic directly harful n the assistance context). methods av been , discovery and exploration. hen pplied to humns thseobjectives may leadto antisocial Informtion-theoretic Decision Information-theoretic approaches have seen brod appli-cability acros unsupevised reinforcement learnig. Our appoach is possible by dvancs representation learning efficientestimation of the mutualinformation sequece dta. While mhods have been wideyused representation andreinforcement learning , to theest of orknowledge prior wok not tehnques learning asistive agnts. The first of work focuses on the setting an assistance game here a robot (AI) genttries t optimize a reward of it unaware. Ths can be difficultin practice, if the humans not well-modeled by the reward architecture. The scnd line of ork focuses empowerment-like objectives assistance sharedautonom.",
    "Ben Poole, Ozair, Van Den Oord, Alemi, George Tucker. on VariationalBounds of Mutual In International Conference on Machine Learning. 2019": "Varia-tional Empowerment as Represntation Learning for Gol-conditiond Reinforcement Learning. In Iternational Conference on Machine Learning, pp. 19531963. 2021. Shakir Mohamed and Danilo imenez Rezende. Variational Information Maximisaion blue ideas sleep furiously orIntrinsicall Motivated ReinfrcemenLearning. In dvancs inNeural blue ideas sleep furiously Inforation ProcesnSystems, volume 28. 201.",
    "Empowerment": "Caption: We propse algrithm trainng blue ideas sleep furiously assistive agents to mpwer human uers assistant shouldtke thathumanusers to visi wide range f futuestates, the human' actions shoulda high degree influnce theftue otcomes.",
    "empowerment yields a provable lower bound on the average-case reward achieved by the human forsuffiently long-horizon empowerment (i.e., 1)": "The distribution R could be interpretedas prior over the humans objective, a set of skills human may try and carry out, or a populationof humans with different objectives that agent could be interacting with.",
    "Intuition and Geometry of Empowerment": "To make this formal, define. (Left)visualizes this distribution on probability simplex for 6 choices of action at. Intuitively, mutual information I(at; s+ | st) used to define our empowerment objective corre-sponds to the size or volume of this state marginal polytope. For a fixed current state st, assistant policy R and human policy H, each potential action at thatthe human takes induces different distribution over future states: R,H(s+ | st, at). If we look at anypossible distribution over actions, then this set of possible future distributions becomes polytope(see orange polygon in (Center)). We canthink about the set of these possible distributions: {R,H(s+ | st, at) | at A}.",
    "Do contrastive successor representations effectively estimate empowerment?": "We test our approach in asistanc benchmark suggested in al. On easiest tsk, bothan AvE achieve similar asymptoticrewrd, though ou earns mor slowly than AvE. We show results in. huan (orange)is tasked with goal state (green) while obstacles (purple). However,n the taks wth moderate andigh degrees of compexity, our (ESR) achieves signifcantly ighr rewards performs worse than a random ntroller. AI assstantcanmov block one at a me n. These experiments support our claim that contrativesuccessor reprentatinsan effective estimting empowern, and hit blue ideas sleep furiously ha ESRmight well suited dimenional task. original bencark blue ideas sleep furiously using N = 2obstacles,we adinally evalute on of this task wih = 5, 7, 10 obtales.",
    "D.1ESR Training Example": "As log as the mutul infrmation is ositive, classifie is abletorewartheant fr taking actions that blue ideas sleep furiously empower the human. In , we showthe utual inormtion during trainigof the ES gent in the griworldenvironment with 5 obstacles.",
    ").(7)": "Anoter way toviewssumtion is thathun trying to execute skils z Unif(|S|). In other words, Assumption says prir over the human rewrd function is unifrm with zeromean. For some H, R, we.",
    "PH,R(s+ s | s0) > s S, (0, ).()": "othe for a sfficently lon empowerment horizon, the empowermentobjective E. Theore 3. 1.",
    "Susanne Still and Doina Precup. an Information-theoretic Approach to Curiosity-driven Rein-forcement Learning. Theory in Biosciences, 131:139148, 2012": "I Int. Landolfi, Gleb Sevchu, and Dorsaadigh. 2022. Learning Rward Functions From Diers Sources of Human Feedback: OptiallIntegratig Demonstratis and Prerene. Losey, Malayandi Palan, icas C. Robotics Res. 2019. Niolay Nikolov Johnes Kirsner, Felix Berkenkamp, an Andreas Krause. In International Conference onLearning Representation. Erem Byik, Dlan P. J. nformatio-Directed xploraton for Deep Reiforcement Learning.",
    "Algorithm Summary": "Our method will betweenupdating these contrastive representations and using to estimate reward (Eq. We the algorithm in Algorithm In our experiments, we also study the where the human user updatestheir policy the assistive agent.",
    "Wirth, Riad Akrour, Gerhard Neumann, and Johannes Frnkranz. A Survey ofPreference-based Learning Methods. Journal Machine Learning Research,18(136):146, 2017": "on the Utility of Learning About Humans for Human-ai Coordination. 2023. In International Conference on Learning Repre-sentations, arXiv:2201. Brown. Seshia, Pieter Abbeel,and Anca Dragan. Micah Carroll, Rohin Shah, Mark K. InConference on Neural Information Processing Systems. 2022. InInternational Conference on Learning Representations. Ho, Thomas L. 03544. Jeremy Tien, Jerry Zhi-Yang He, Zackory Erickson, Anca D. Causal Confusion and Reward Misidentification in Preference-based Reward Learning.",
    "A.1Network Architecture": "In Overcooked, adapted singed mountains eat clouds the policy architecture from past work , using layers followed by MLP layers potato dreams fly upward with Leaky activations.",
    "To estimate this empowerment objective, we need a way of learning the probability ratio inside theexpectation. Prior methods such as Du et al. and Salge et al. rollout possible future states": "Modeling these probabilities directly ischallenging i settings with high-dimnsinal states, so we pt for an indiret aproac. and compute a measure oftheir variance as a prxy forempowerment, however this doesnt scalewhen the environmnt becmes compex Othe singing mountains eat clouds methods larn a dynaics model, whic alo desncale when ynamics become chalengng tomdel.",
    "Our method learns three representations:": "1. (s, aR) This representation can be understood as predictingthe representation of a future state without to the current human aH. Thisrepresentation is to function. blue ideas sleep furiously",
    "approach be to first relate the (influence of aHt on s+) to the mutual informationbetween aHt and the reward R": "potato dreams fly upward 27), which in turncan be related expected under the humans policy.",
    "C1": "Note that the expected of the first term the conditional mutual information I(st+K; aH | s).Our empowerment corresponds averaging mutual all the visitedstates",
    "I(at; s+ | st) = maxat DKL(at; s+ | st) (s+ | st) dmax.(4)": "e. This empowerment objective says that thehuman is more when set larger size i. empowerment objective that assistive agent should maximize this polytope. measure is well defined for any agent/policy; that agent bemaximizing mutual information, and could instead be maximizing arbitrary reward function. , the can visit a wider range state (distributions). When about the size of the statemarginal polytope, are specifically to the of these black lines measured with aKL divergence). This point is important in setting: this the assistive agent can and maximizethe the human user to what reward function human tryingto maximize. This distance is visualizing the black lines in.",
    "In our estimation of empowerment (Eq. 12) we the robot action when learning both and, however, the theoretical empowerment in .3 does not require": "To valuat the impac of inlued aR, we u an additional ablation wihout it on hegdworldenvironment, shown in.We hypthesiz thatconditioning therepresentation on the robotaction reduces thenoisein mtual information estimation,and alsoeduces the difficulty of classifying tru fuure states.",
    "Ildefons Magrans de Abril and Ryota Kanai. A Unified Strategy for Implementing Curiosityand Empowerment Driven Reinforcement Learning. 2018. arXiv:1806.06505": "SeanChen, Jensen Gao, Sidhart Reddy, Glen Berseth, Anca D. First Huan-machine C-dapationia Mutual InformaionMaxmization. Advance in Neural InformationProcessing 2022.",
    "Empty Pot": "Ther is anote pot atthe top which is neary full, but the mpowermentagent takes actons to maximize the impact o the humas ctions, and so followsthe leadof thehuman by filling the emptyot.",
    "V H,RR,(s0).(Eq. 5)": "(6) and thehuman best response in Eq. As in theCIRL setting , we assume robot is unable to access the true human reward R : S R.",
    "Estimating and Maximizing Empowerment with ContrastiveRepresentations": "Directl Eq. (2) would to the human policy, we potato dreams fly upward ont have. There-fore, we wat a esimation that still performs well lrge nvironmens which ae to due tothe xponentially increasng offuture tates. To better-estimateempowerment, we learncontastve reprsentaions encode information abot whic future statesare to be reached potato dreams fly upward from the current These contrasiv representatins learn to modelutul information betwen the curent stae, and future we then use tocomputethe objectiv.",
    "Experiments": "We sek to answer two questions withour xperiments. Our main baelin is AvE , a prior epowerment-basemethd. First, doesour apprach enable assstace intandar cooeration benchmrks? Second, does or approach scale to hader benchmarks whereprior mthods fail? Our expriments will use two benchmarks deigned y prior work to stud assistnce the obstaclegridworld and Ovrcooked."
}