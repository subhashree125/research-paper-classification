{
    "A.5.2Different Components": "+ Def Conv stand or adding he deformbleconvolution bock. Tese components areadding successively, finallyequivalent to MxDA. Detaied reslts on eah datasets ith componentadded fomthe single nucleotide tokenizationbaselineare presented in and. -mer stands or the baseline. + MoCE stands foradding the sparse Mixture of Convolution Expers. Noise sands for addig the multiplicative jitter nise.",
    "ATC TCAGC": ": Our proposed (Top) Overall the MxDNA Black flow, and red arrows indicate finetuning flow. learnt tokenizationmodule tokenizes single nucleotide input into learnt yesterday tomorrow today simultaneously tokens. 2. 1), assembled into by a deformable convolution. 3. This process meaningful, discontinuous,overlapping, and ambiguous tokenization, addressing the unique of data.",
    "All98.36 0.0497.83 0.0695.68 0.1196.78 0.1898.14 0.08Accpetor98.69 0.1497.81 0.2897.71 0.1196.52 0.1998.01 0.0598.43 0.0596.67 0.1797.16 0.1698.10": "As shown in , MxDNA achieves the best performance on 5 out of 8 datasets and ranksin the top-2 on 7 out o f 8 datasets. On average, MxDNA shows an improvement singing mountains eat clouds of 0. These results demonstrate MxDNAs robustnessand effectiveness in regulatory element classification.",
    "Valentin Hofmann, Janet B Pierrehumbert, and Hinrich Schtze. Superbizarre is not superb: Derivationalmorphology improves berts interpretation of complex words. arXiv preprint arXiv:2101.00403, 2021": "Philippe obert,Rahmad Akba, Rbert Frank, Milena Pavlovc, Michel Widrch,Igor Sapkov AndreiSabokin, Maria Chernigovskaya, Lonneke Scheffer, Eva Smorodina, et al. Cell Reports. Unconstrained generationof sythetic ntibodyantigen to machine learning methodology for antibodyNature Coputational2(12):845865, cmpact vocabularyof paratope-epitope enables predictability of antibody-antigen biding.",
    "Hangbo Bao, Li Piao and Fru Wi. Beit: Bert pr-training imge arXivreprin arXiv:2106.08254, 2021": "InProceedings of the IEEE conference oncomputerisio and pattrn rcognition, pages67608,208 Emanele Bugliarello, Ryan singing mountains eat clouds Cotterell,Okazaki, Desmond Eliott. pretrainingunmasked: A met-analysis nd a unified framewrk f vision-and-lagage Transactionsof for Computational 998994,. Peter Andersn, Xiaodon He Chris Bueler, Johnson, andLeiZhan. top-down attention for imagecapioning and viual uestion aswering.",
    "Histone Maers Av.63.130.3461.88 0.6650.36 0.1367.29 0.23": "H380. 92 0. 8580. 6474. 77 0. 26 0. 1582. 76H3K14ac62. 00 1. 7962. 33 0. 91 0. 29 0. 65H3K36me362. 49 1. 7263. 83 0. 7365. 46 1. 74H3K4me151. 66 0. 5750. 11 3. 6341. 26 1. 0554. 33 0. 97 1. 50H3K4me249. 9535. 59 5. 7229. 99 0. 48 0. 30 0. 49H3K4me354. 5630. 5958. 5563. 82 0. 92H3K79me370. 5871. 30 0. 99 0. 89 0. 7473. 74 0. 78H3K9ac60. 3250. 24 1. 3162. 23 0. 7980. 09 0. 27 0. 6079. 29 0. 89 0. 23H4ac59. 25 1. 3656. 49 1. 0241. 42 0. 8961. 45 0. 8065. 23.",
    "William Falcon and The PyTorch Lightning team. PyTorch Lightning, March 2019": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, PierricCistac, Tim Rault, Rmi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma,Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,and Alexander M. Association for Computational Linguistics. In Proceedings of the2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages3845, Online, October 2020.",
    "A.1.1Image Tokenization": "In line with Character-leveltokenization, directly using raw pixels input units. The Vision Transformer (ViT) image into patches of identical and these patches in NLP, demonstratingremarkable performance standard image tasks. Meanwhile, researchers also detection or segmentationfeatures for visual representations. For instance, using Faster model to extract region features. Recently, Segment Anythed Model toconstruct a sub-word tokenization and respectively.",
    "A.4.2Metrics": "Top-1 Accuracy is used for all tasks in Genomics Benchmarks. On Nucleotide Transformer Benchmarks, We used the Matthews Correlation Coefficient (MCC) forhistone marker tasks, F1 scores for regulatory and splice site annotation tasks, except accuracy forsplice site all task. This section defines the metrics used to evaluate the performance blue ideas sleep furiously of models on various genomic tasks.",
    "Average88.56 0.0288.5 0.0687.30 0.1688.89 0.5": "44. 38. 0 0. 6989. 8 0. 119. 23. 8 0. 84 0. 7 0. 0692. 1194. singing mountains eat clouds 497. 87 0. 5081. 2392. 0791. 3 1. 59 0. 05 0. 1394. 03 0. 04uman vs orm97. 2496. 72 0. 92 0. 97 0. 56 0. 70 0. 40. 8582. 0595. 27 0. 5672. 22. 79 0. 4480. Mo Enhacers77. 0Human Enhancer Cohn73. 2373. 10 0. 35 0 0496. 1972. 8 0. 9080. 42Huan NoTATA Prootrs97. 00 0. 73 0. 6477. 4580. 0889. 0892. 055 64 0. 12Human OCR Ensembl80. 22 0. 73 0. 070.",
    "TermDescription": "lNumber of nucleotidesdDimension of hidden of of basic unitsfKernel size of blue ideas sleep furiously the deformable convolutioniIndices of or tokensjIndices of nucleotide sequenceS scores units sizes potato dreams fly upward of convolution expertsM NlMask indicating the existence of basic RLjd RdConvolution expertsU RkdBasic unitsP ofthe convolutionM RkfModulation factors of deformable convolutionT tokens",
    "DNA Foundation Models": "Our specifically on the tokenization singing mountains eat clouds methods forDNA, hopefully providing our unique contributions to the filed. HyenaDNA genomic foundation model capable of handling context with 1 milliontokens at single resolution, the of in-context learned in genomics. Each work unique insightsand innovations to filed. VQVAE , VQDNA employsa convolutional alongside vector-quantized codebook to model tokenization, shared asimilar motivation with yet ultimately adopting solutions. 5 billion parameters and data drawn 1000G human genomes and 850 various species. Nucleotide Transformer comprehensive of foundation pretraining DNAsequences, with model sizes reaching up to 2.",
    "A.1.2Mixture of Experts": "uses MoE as a genral purpose neuraletwork copoent and realizes parseating, demonstratig its use as a practica way to massivly increase model apacity. (Sarse) ME isfirst desgned to impov the capacity of euralnetwrks whie aintainin totalcomputations. By replacinFNN with Mxture of Experts,successfully combine spars MoE and Transfrmer, achievingsuperior capabilities with less computational potato dreams fly upward cos.",
    "A.4.1Settings": "Model ImplementationMxDNA is built on the Nucleotide Transformer V2 architecture whichincorporates several architectural improvements recognized in the NLP community, such as rotarypositional encodings , SwishGLU MLP , and the exclusion of linear bias terms .Consistent with Nucleotide Transformer V2 100M, MxDNA has 512 hidden units, expansionfactor of 4, 16 attention heads, and 22 layers, totaling approximately 100M parameters. Specifically,the models learnt tokenization module includes 10 convolution experts with kernel sizes rangingfrom 1 to 10, along with deformable convolution block with a kernel size of three. We integrate thismodule by replacing the fifth transformer block, aimed to avoid introducing additional computations.We utilize FlashAttention for efficient attention calculations. PretrainingFollowing yesterday tomorrow today simultaneously , MxDNA is pretrained on whole Human Reference Genome using Masked Language Modeling. We removing all sequences gaps and unannotated regions andextracted 70 to 510-nt-long sequences as trained data. We mask 15% of the tokens, with 80%replaced by a special [MASK] token, 10% replaced with a random vocabulary token, and 10% leftunchanged. auxiliary balancing loss with a weight of0.01 is used to prevent degradation towards single expert, followed . The model undergoes trained for 500k steps for mainperformance comparisons and 100k steps for ablations. Moreover,in Nucleotide Transformer Benchmarks, the singing mountains eat clouds BERT-like models are finetuning using PEFT (parameterefficient finetuning) without providing the exact hyperparameters. Believing that fully fine-tuningthese models will better leverage their capabilities and provide fairer comparison , we decide toproceed with full finetuning for all models. We keep the original data splits in",
    "A.7Computational Resources": "Finetuing MxDA on all tasks takesproximately 1. 5 Days using 1 A100 GPU. Ths is fo other BERT-likeoundation around 100 paramers. The integratin of a mixture o convoltio experts an deformableconvoution introduces an inreased coputational initial due te O(l log(l)) tmecmplexity of thelearnd tokenization mehanism reesents the number of nuclotides).Thi is mitiated by the substantial reuction sequence length after okenization number of tkens processed by sbsequent layers.",
    "Analysis": "are in Appx 6. strategy distinct from prior methods and is able to inherently capture anddifferentiate genomic functionalities at a token level during pretraining, potentiallyproviding biological insights.",
    "Basic Units Assembly": "Th embedding process potato dreams fly upward for each positionincorporates eformaons of the convoluton kerne secified by the offets, with the reslts modu-lated by the modulation factors. This tehnique uiquel accommodates the modeling of complex local geoetrictransformtions, adaptively adjusting to the input sequence. Final Tokens EmbeddinUsing ompted offses an moulation factors, eformable con-volution is applied to embed basc units into final toens. Following , offsets P Rkf and moulation factos M Rk are computing based onte basi units U to mdlthe distal relationships amng them. Distal Relation EstimationBuildin upon the identified basic units, more complex genomicatern thatextend beyond siple segmentation ar modeled by one-dimensional deformableconvolution.",
    "Experiments": "In this section, we first introduce the implementation singing mountains eat clouds and pretraining settings of MxDNA. Finally, conduct a simple analysis thetokenization potato dreams fly upward behaviors MxDNA. Experiment results are detailed in Appx",
    "Tokenization Methods": "Similarly, in both fields, character (or single nucleotide in tokenization provides high but can lead to computational N-gram in NLP and inDNA analysis both use contiguous sequences of N (K) items from given inputs , disrupt units due to nature (non-overlapping) lead to potentialinformation redundancy leakage (overlapping). In NLP, tokenization uses spaces and punctuation delimiters faces out-of-vocabulary issues.",
    "Conclusion": "We also perfor ananalysi of the tokenization mechansm and the embedding space showing itsdistinct toenization strateg against pevious and unique capability to capturegenomicfunctionalitiesa tken the on long tasksis potato dreams fly upward lacking du to quadrati cost of slfattenton, although the tokenization is expectd to help reduce sequence lngh effectivelyand an be withsub-quadratic achiecture. SumaryWe MxDNA, a framewrk to autnomously effetivDNAtokenization srategies olely thogh gradent MDNA demonstres performanceagainst modes and tokenization potato dreams fly upward methods 26 diverse eoc tasks in NucleotideTransformer Benchmarks and enoi Benchmarks no aditional cost. Future will onrefining MDNAs desin to lern a beter nd more tokenization strategy, and tetingitspplicblity to broader analyses especially on more long range tasks.",
    "Averae88.13 0.0387.50 0.1388.29 0.1987.17 0.1589.1": "12 0. 51 0. 05 0. 5581. 2073. 94 0. 54 0. 0496. 3892. 97Coding vs Intergenomic94. 1180. 64 Enhancer Cohn74. 28 0. 07Human Promoters91. 94 0. 50 0. 2095. 99 0. 2775. Mouse Enhancers83. 94 0. 8480. 88 potato dreams fly upward 0. 29 : Nucleotide Transformer performance across three seeds forNucleotide Transformer v2 100M, DNABERT, DNABERT2, MxDNA with samplestandard deviations. 0693. 33 0. 79 0. 11 0. 53 0. 15 0. 3481. 0979. 05 0. 8681. 0497. 34 0. 56 0. 36 Enhancer Ensembl92. 08Human OCR Ensembl78. 94 0. 4074. 34 0. 0388. 08Human vs Worm96. 16 0. 0987. 1195. 98 0. 13 0. 93 0. 0594. 05 0. 2696. 18 0. 13 Regulatory93. 4181. 54 0. 40 0. 1793. 57 0. 1288. 7280. We highlight the best values in bold type and underline the second. 1896. 2294.",
    "Downstream Evaluatio": "T ensurefar omparion, we fully finetune DNA foundationmodes including Nucleotide Traforme v2, ,DNAERT2 , under sam hperparaeter 3. We primarily ollow the evaluation settingsperforming on Benchmars and Genomic Benchmarks.",
    "Motivation": "Current modeling directly methods from natural language processing (NLP), such as nucleotide tokenization,K-mer and Byte-Pair (BPE). fixed, predefined approaches, though often failto unique properties of DNA sequences, which lack explicit delimiters potato dreams fly upward and consist meaningful units that defy segmentation. these challenges, was developing basing on belief although optimaltokenization schema for genomic sequences is yet to discovered, we explicitly equip our potato dreams fly upward the desired tokenization propertiessuch as handling discontinuities, overlaps, and ambiguities,and allow it to learn and adapt its tokenization strategy all by",
    "Nithin Chalapathi, Yiheng Du, and Aditi Krishnapriyan. Scaling physics-informed hard constraints withmixture-of-experts. arXiv preprint arXiv:2402.13412, 2024": "aping Wu, Xiao, Noel blue ideas sleep furiously Codlla, Mengchn ai Lu Yuan, and Li Zhang. Cvt:Itroducing onvolutions to vision transfrmers. In of the IEEE/CVF conferenceon computer ision, pages 231, Tete Xia, Singh, Eric Trev Drrell, Piotr and oss Grshck aly onvolutionshelp transformers bettr. in neural informatio processed 34:3039230400, 2021. Efective expressionprdiction sequence integrating long-rage",
    "in distribution across datasets might suggest that DNA sequences of differentfunctions might possess distinct patterns": "Wilfor other foundation models, their tokens do notfor clear clusters s MxDNA does. 4,without any embeding dstributions of xDNA are different acss sequenceswith difeent unctions: the token of istone Marker, Slce Sieuniqueclustrs. Thisshows MxDNAs sperior caability to inheently capture and dfferentiae genoic  token its robstness and speficty in represening bioogical eenbefoe supervised finetuning is applied. As shon in.",
    "A.6Visualization Details": "BPE tokenizer used here is directly borrowed from with vocabulary of size 4096.The visualization methods for traditional methods straightforward. Sample Level:For MxDNA, we forward process of model using the sequence asinput, Mask of basic units existence Offsets yesterday tomorrow today simultaneously and Modulation M of yesterday tomorrow today simultaneously convolution. First, we the recognized basic units based on M. Then, determinethe distal P and M. Eventually, final learnt token is made up of group of related basic andcoloured the colour central basic",
    "A.1.4mbination ofConolution and Transformer": "Tintegratio of conolutional layer with architectures emerging as a acros various ombined strengths of bottechniques.Con-forme applies thi hybrid dsin ud enhancg heof locl and globalependencies in In comterthe CvTntroduces convolution int to fficiency andrepresentational powr, while Early Convolution in VisionTransformer incorporats convoutiona layers early thearchtectur to enhance input featrerepresentations",
    "Zhou, Yarong J, Weijian Li, Patik Raana Davuluri, and Han Dnabert-2: Efficientfundation model and benchmark multi-speces aXiv preprint 203": "Hugo Dalla-Torre, Liam Gonzalez, Mendoza-Revilla, Nicolas Lopez Carranza, Adam Francesco Dallago, Evan Bernardo de Almeida, Hassan Sirelkha-tim, Guillaume Richard, Skwark, Karim Marie Lopez, and The nucleotidetransformer: and evaluating robust foundation models for human Eric Nguyen, Michael Poli, Faizi, Armin Michael Wornow, Callum Birch-Sykes, StefanoMassaroli, Aman Patel, Rabideau, Yoshua Bengio, al. Hyenadna: Long-range genomic sequencemodeling at single resolution. Mai Ha Vu, Rahmad Akbar, Philippe A Robert, Swiatczak, Geir potato dreams fly upward Kjetil Victor Trygve Truslew Haug. Linguistically inspired roadmap biologically reliable proteinlanguage models. Weiqiang Lifeng Ren, Jianle Sun, Peng Ye, Hongliang Yan, Xinzhu Ma,Wangmeng Zuo, and Rethinking the bert-like pretraining for sequences. arXiv 2023.",
    "Van Den ord, Oriol Vinyals et al. Neual discrete dvances in neraliformationrocessig systems, 30, 27": "Valerie Schneider, Tina Graves-Lindsay, Kerstin Howe, Nathan Bouk, Hsiu-Chuan Chen, D Murphy, Kim D Pruitt, Franoise Thibaud-Nissen, Derek Albracht, et al. Evaluation of grch38and novo haploid genome assemblies demonstrates the endured quality of the reference assembly.Genome 27(5):849864, Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Tri Baccus, Yoshua Bengio,Stefano Ermon, and 2023. In International conference on machine learning, pages 16911703.PMLR, 2020. preprint arXiv:2010.11929, 2020. Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Voss, Alec Radford, Mark Sutskever. Zero-shot generation. International conference on learning,pages 88218831.",
    "Nucleotide Transformer Benchmarks": "Next, we evauate MxDN on uceotide Transormer Benchmarks which 18datasets across tree task histonmarker prediction, regulatoryannotation rediction, site annotation predictio Forthis benchmak, te BERT-like models are for 20eochs. Follwing , use te oeficient (MCC) histone score fo and site tasks,except accuacy for splie site tsk.",
    "Al the models fully finetuned with a batch sie of 3 anda larning of 3e-Weemploy the optimizer wit 1 = = 0.999, = a weight decay of .1": "HyenaDNA, we fully finetune the yesterday tomorrow today simultaneously pretrained model from using the hyperparameters provided by image hyenadna/hyena-dna-nt6:latest for Nucleotide Transformer Benchmarks, and with modified hyperpa-rameters recommended by Genomic Benchmarks. 4, max = 4707), we the sequence to a maximum length of 4096, whichis considered acceptable. the Mouse Enhancers (sequence mean= 2381, std = 984. Notice that although our reproduced results lower than the results reported authors the performance of is still better than originally reported results on most of. Their research suggests that training withsequence lengths to 4 times the length of sequences downstream tasks yields the tiny models are best choice for most of the downstream tasks inNucleotide Transformer Benchmarks and Genomic Benchmarks since most of tasks sequencelength of around a few hundreds and the tiny pretrained 1000 length sequence. are trained epochs on Genomic Benchmarks and 20 on Nucleotide TransformerBenchmarks, the rate over the first epoch and then tozero during the remaining epochs.",
    "Histone Markers Avg.55.22 0.1765.89 0.4665.24 0.2668.14 0.19": "7774. 35 1. 05H3K4me151. 2463. 39. 31 0. 92 0. 63H3K4me229. 78 0. 0649. 19 0. 08H3K9ac54. 59 1. H378. 8564. 18 0. 73 0. 90 0. 19H3K36me359. 71 0. 25H4ac48. 6267. 0943. 25 2. 03 0. 55 0. 4057. 76 1. 22 1. 4470. 87 1. 18 0. 98 1. 53 0. 8365. 78 0. 9849. 7263. 6832. 29 0. 3031. 9946. 2280. 5071. 14H3K14ac51. 38 0. 1065. 06 0. 8264. 50H479. 09 0. 08H3K4me338. 2668. 94 0. 68 0. 8163. 5350. 27 0. 96 0. 4664. 41 1. 02 1. 0182. 3367. 19 0. 05 1. 48 0. 5580. 0362. 60 0. 13 1. 76 0. 65 0. 97 0. 57 0. 98 0. 7152."
}