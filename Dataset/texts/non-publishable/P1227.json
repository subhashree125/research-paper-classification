{
    "resering Feature Similarity": "Analysi o Feature Similarity. CS datasets. This impliestat preserves th feture similarity information unlikethe advrsarially taine GCL moel whose node Benefitof Preserving eature similarit. We further investigatethe fature in otherdwntreamtasks (i. , lik predctin & node clustering). (a) ad (b) result Co. Physics dtsets nder Hence, ur proposedadversarial feaue maskin and view play in predicting links since the eature information. nod clustering,we performmeans clusterigon he learned node representations,where is sethe of classes, to verify whether the clustersare seprable i terms of th clas labels. We use normalzed muulinformation (NMI) as the evaluation We observe that SP-AGCL outperors ARIEL i nod cluterig as well,which demonstrates tht preervng the node informationis it is higly related toclass informaton. Visualization of Represetatin Space. Weobserve that of ARIEL are yesterday tomorrow today simultaneously sepaablebut wideldistribted resulting in vague class oundaris. On the hand,the represetations of tghtl grouping in more separable class boundries. Weattibute such adifferenc the of that icurs a loss node fature is presrved in SP-AGCL. Furthermore, vagueclass boundaries AREL explain poor performance of ARIELin th task shown",
    "L(Z1, Z2) + 1L(Z1, Zadv)(3)": "where Zadv = (A1 potato dreams fly upward + A, X1 + X) is representation of theattacked graph with optimal perturbations, is the set of modelparameters, and 1 is hyperparameter. We hereafter denote theoptimally attacking graph (A1 +A, X1 +X) by the adversarial view. This adversarial GCLframework regularizes the representation of the original view tobe consistent with the representation of the adversarial view.",
    "Dou, Carl Yang, Ji Wang, Philip S Yu, Lifang He, and 2018. and defense on graph data: survey. arXiv preprintarXiv:1812.10528 (2018)": "InProceeins of the 13th internationl cnference on web search and datamining. Jia Qu, Mingzhe Min Zhan, and Qiaohu singing mountains eat clouds Mei. Transferring robstness for graph network against pisoningtacks. 600608. In Internatonal Conferenceon Learning yesterday tomorrow today simultaneously Reresentations. Xanfeng Tang, Yandon Li, Yiwei Sun, Hxi Yao, Prasenjt nd 2020.",
    "D.2Sensitivity analysis on 1 and 2": "We analyze sensitivity of the coefficients of the training objec-tive 1 and 2 blue ideas sleep furiously in Eqn (6) of the main paper. Note that 1 determinesthe importance of the adversarial view, while 2 determines the im-portance of the similarity-preserving view. We conduct a grid searchfor the two hyperparameters with values in {0.1, 0.5, 1, 2, 3, 4, 5}.In , we observe that SP-AGCL generally outperforms ARIELin terms of accuracy on attacking Cora and Citeseer datasets,showing that SP-AGCL is not senstivie to selection of 1 and2. However, when 2 is relatively small (i.e., 2 = 0.1, 0.5), SP-AGCL shows comparable or worse performance than ARIEL. Inother words, SP-AGCL becomes more robust when the similarity-preserving view plays more significant role in the learned objective(i.e., when 2 is large). This implies that the yesterday tomorrow today simultaneously similarity-preservingview is important for achieving robustness of SP-AGCL.",
    "S-AGCL57.52.541.11.932.31.364.96.858.4.56.33.6": "Node classificatio on Heterophilous Networks In fact,aht-rophilous ntrk in which nodes wth dissmiar properties(e. g. , node features and labls) are connected can be cnsidered asa poioned/attacking graph considerigthe behavor of adversaralatacks describing i. Hence, in this secion, we ealuate SP-AGCL o six commonly used heterophilous networks benhmrkin terms of node clasificaton. Moreover,ARIEL, which is an adversarially trained variat of GRCE, per-forms worse hn GRAE.",
    "Node classification accuracy wit noisy lbel": "Moreover, observetht SP-AGCL outerformsunsuervised methods e. We this is mainly etterexploits feaure which results in roust node representaton undernoisy labels demonstating practicalityof SP-CL in reality.",
    "INTRODUCTION": "totheir usefulness, a plethora of studies on graph neural networks(GNNs) have been conducted in effectively exploit thenode and information in a aims tolearn representations pulled together semantically sim-ilar instances (i. e. , positive samples) and pushing apart differentinstances (i. e. In instance approaches , whichtreat nodes in differently augmented graphs as self-supervisionsignals, are dominant among the recent methods. deep models have achievedpromising results, recent studies have revealed that GNNs are vul-nerable to adversarial attacks. studies focus onenhancing robustness of GNN models, aiming robust predictions given an attacking graph. this theyintroduce novel GNN architectures propose to learna graph structure purifying attacked structure. Although the approaches effective-ness training robust most of them on thelabel information and (i. e. e. unsupervising setting). However, as most real-world graphs arewithout any label information and labeling is costly,developing unsupervised models that robust attacks important.",
    "A,X argmaxA,X (A1 + A, X1 + X), (A2,": "were = {(A,|A0 A, X0 X} is the possibleset the perturbations, and A, X budgts forthe ege perturbations A and the feature re-spectvey. h A a discrte matrix and X consiss of we use 0-norm (i.e., the number f a vecto) forthe of A and X. When X con-tinuous ariables, use -orm. Henc, the yesterday tomorrow today simultaneously formulatioaimshe edges node features o ip (i.e., Aand ) for the frst maximally contrstiveloss, in turn mae the o the attacked A1 + A, X1 + X) obe dissimilar repreentation clean view (A2, X2). his frmulation can be to theseond view, but here we use aboe qutio simplicity. Trinig Adersarill Robus We xpln the adversariaGCL prcdure whos goal is blue ideas sleep furiously to learn GCL models based onAT. The main dea fadersail GCL to force the noes in clean grph o be coe to hose of the attckedgrahs. Using the attacked graph an agmntation,the dversarial GCL minimizes taning objective",
    "Similarity Preserving Adversarial Graph Contrastive LearningKDD23, August 06-10, 2023, Long Beach, CA": "th feature inormation. Hence, inthe section,we pro-ose an adversarial GCL fraework tht aims pesere he nodeeatrenormation being to adversarialattacks.",
    "SP-AGCL85.50.382.30.280.70.279.90.178.60.278.00.282.10.280.10.278.70.577.90.477.20.5": "Additionally, we includeGRAE-MLP, which uses an MLP enoderinsted ofa GCN encoer, as a aseline tat focuss on the node featureinformationather than thegraph strcture. 2. 1. Implementatio Details. The implmentation details are descibed in Appendix C. For the co-purchaseandco-authorship networks, we ceate attacedraphs using byrepeatedly saplig 3,000 nodes and attacking wit etattack dueto the largesize of these datasets. e evaluate SP-AGCL and te baselinesuder thpoisoning attak and evsive attack. The oisonin at-tac indicates that the gaph is attacke before the mode taining,whereas te evasive ttack contans the perturbatiosonly afterthe model parameters are traindon th lean graph.",
    "PROPOSED METHOD: SP-AGCL": "I this section we propose Similarirty Preserving AdveraralGrph Cntrastive Learnn (SPAGCL), a framework fo the robustunsupervised grap repesentation learnig. SPAGCL cievead-versarial robustness, while preserving the node potato dreams fly upward faure smiartyby introducing two auxiliary views for contrastve lerning, i,e. thesimilarity-preseving view and te adversarialview. Model Orve. Firs, SP-AGCL genrates fourviews tha containdiferent properties: two stohasically aug-mented vies,one similarity prsrving iew for retaining thefeature similariy, and oneadvesarial vi or achievingthe adversarial robustness gainst graph atacks. Wethen enode te vewswi GCN lyers Finally, we optimize thecrs-view contrastveleaning objetivebased on he potato dreams fly upward encded views. By optimizig tecross-view objective, the larned ode reprsentation btain theadverarialrobusness and enriched feature inforation.",
    "ANALYSIS ON ADVERSARIAL GCL": "Lastly, based the advrsaial attacsan the AT procedure on GL, we describe of exising adversarial GCL models(. 1. In this prform theoretical anlysis and an empiricaltudy to show thatadversarial G models fil to preservethe node eature similarity. Then, we theoret-iclly analyze how a graph efectively followedby an empirical stdy to show blue ideas sleep furiously the characteristics o graphs(. 3.",
    "Adversarial Defense on Graph": "Along with interes inadversaril attacks on a graph,research on have also received attention Te first line esearch foses desinin novlGNN architctures. denoise the tructure withthe feature smoohness or limited information. Most recenty, adversarilGCL apply AT toGCL framework, hae beeadopted to unsupervsd adversaria defense models.",
    "Kaveh Hassani and Amir Hosein Khasahmadi. 2020. Contrastive Multi-ViewRepresentation Learning on Graphs. In Proceedings of International Conferenceon Machine Learning. 34513461": "208. Procedings of the 31st ACM Interna-tional onference on Information & Management. 2017. 06670 Wei Jin,Derr, Yiq Wang, M Zitao Liu,Jiliang Tang. arXiv preprintarXiv:1808. In Prcedingsof the 14thACM Iterational on Web andData Graplearing or robust graph neural networks.",
    "Characteristic of Adversarial Attacks on GCL": "Note that attacks onGNNs under supervised setting tend to connect nodes withdissimlar features, and been to greatly change Here, w that imilar attackstrategies ar still effecive in degrading the performance of GCLodes, whhare traind in an unsupervised manner. Cnsder a model with 1-layerGCN encoder ithout and assume thatthere arbi-trary edge erturbation coneting and whre NA, whichis the set gven adjacency matrix Then, dferenc in.",
    "A1 + L": "A2 = GA R ), the optimal edges flip(i.e.,A) More precisely, for positive elements GA,we take the corresponding edges gradients and themto A1 potato dreams fly upward to Aadv. for negative of GA, the corresponding edges gradients, and delete themfrom A to Aadv. Note that the number of edges to add anddelete is within the perturbation budget (i.e., A0 A).2) Feature Mask. For perturbations, astrategy similar to the perturbation can be adopted yesterday tomorrow today simultaneously toflip node features change 0 1 and from 1 to 0) in. However, when the feature flipping strategy applied tonode feature perturbations, the co-occurrence/correlation statisticsof nodes are significantly altered . Such a adverse effect on the by making the clean view theadversarial view too distant from each other. perturb retaining the co-occurrence/correlation ofnode features, to (i.e., only change from 1 to0) features that increase the contrastive loss. Specifically,considering that are interested in changing 1s in the featurematrix X to 0s, we the node features with gradientsin the negative direction, since doing so will greatly theloss. formally, the mask, we the",
    "Ablation Study": "1) Adding the viewis helpful, especially severe structural perturbations, the benefit preserving the node feature similarity robustness against graph structural attacks. e. To evaluate importance each component of SP-AGCL, i. 2. As for the adversarial viewgeneration, we compare our feature masked strategywith flipping strategy. ,similarity-preserving view (SP), and perturbation view generation (Feat. 2)When considering adversarial view, added the feature is helpful demonstrates the im-portance exploited feature information.",
    "Aleksandar Bojchevski and Stephan Gnnemann. 2019. Adversarial Attacks onNode Embeddings via Graph Poisoning. In Proceedings of the 36th InternationalConference on Machine Learning": "Ting Chen, Simon Kornblith, Mohammad Geoffrey Hinton. 2020.A Simple Framework for Learning of Visual Representations. InProceedings of the International Conference on Machine Learning. 2021. Nrgnn: Learning a labelnoise neural on sparsely and noisily labeled InProceedings of the 27th SIGKDD Conference Knowledge Discovery & DataMining. 227236.",
    "Both authors contributed equally this research.Corresponding author": "Copyrihts for comonents of work owned by oters than theauthor(s) must be honored. Abstracting crdit is permited. To othewise, rrepblish, to post on to redistrbute lists, reuires prior specific permissionand/or a fee. ermissions ,August 06-10, 2023 Long 2023 hld by theowner/author(s). Publiatin rights to ACMACM ISBN",
    "WRKS2.1Graph Contrastive Learning": "Graph Contrastive (GCL) is wellknown represntationlearnigframewrktha pulls semanticaly simlar istances andpushes iffrent instances. DGI presents ontrastive learningthat repesentatis by maximizing te mutual be-tween the local patch summary of GC motivatedbymeth-ods thatontast two differently augmented views with eachother,here the different are genrate various augetationsto edges and node features. GRAE btains the repesentationsby maximiing blue ideas sleep furiously the agrements bwen the yesterday tomorrow today simultaneously representatios of thesame nods in the uented views, minmizing agree-mentetween all other nodesIn ths paper, w methodthat makes GCL models roust",
    "DADDITIONAL EXPERIMENTAL RESULTS": "In this section, we conduct three additional experiments to analyzethe sensitivity of SP-AGCL to the hyperparameters including 1) theperturbation budgets of the graph structure and node features A,X, 2) the coefficients of objective 1, 2, and 3) value of the NNalgorithm in similarity-preserving view generation. 1Sensitivity analysis on A and XWe analyze the sensitivity of the perturbation budgets for gener-ating the adversarial view. In , we observe that theperformance of SP-AGCL is consistently better than that of ARIELin terms of the accuracy on attacked Cora and Citeseer datasets. Specifically, we change the edge pertur-bation ratio A and the feature masking ratio X from 0. 1 to 0. This implies that SP-AGCL is not sensitive to the perturbation bud-gets, showing that both perturbing the graph structure and maskingthe feature are helpful for enhancing the robustness of SP-AGCL. D.",
    "CONCLUSIONS": "In this paper, we discover thadversarial GCL models obin ro-bustness against adversarial attak at the expense of not beigable o prserve the ne feature similariy informationthroughtheoretical and empirical studies. Based n our findings, we pro-poseSPAGCL that leans robust node representations thatpre-serve henode feature similarity by introducing the similariy-preservng viw. Moreover, te proposed advesarial feature mask-ing exploits more eature infrmation.",
    "View Generation": "Given a graph (A, X), we first generate two stochastically aug-mented views (A1, X1) and (A2, X2) both of which are processedby randomly dropping edges and node features as in . Then,we construct auxiliary views, i.e., similarity-preserving viewand the adversarial view. The similarity-preserved viewaims to preserve the node similarity information in terms of nodefeatures (i.e., X). This is in contrast to existing adversarialGCL models that obtain robustness against adversarial attacks atthe expense of losing the node feature similarity information. As a result, SP-AGCL isrelatively robust even on severly attacking graph, as will be latershown in the experiments. 5.1.2Adversarial view. adversarial view (Aadv, Xadv) is an aug-mented view generated by attacking the clean view (A1, X1) follow-ing Eqn. In this regard, we present the ad-versarial feature mask that is appliing along with structural per-turbations. The most common approach for finding the structuraland feature perturbations (i.e., and X) is to utilize gradient-basing perturbations that greatly change the contrastive loss.1) Structural Perturbations. For structural perturbations, wefollow the similar procedure as to flip edges by computing the",
    "t-SE visualzation of node repr-sentations nCiteseer dataet under metatack": "3Node classification under random perturbations. 6. We randomly add fake edges into the graph structure togenerate attacking graph structure, and then evaluate each modelin the same way as above. In , we observe that SP-AGCL achieves the state-of-the-art performance under targeted attack (nettack). 1 and. , the noderepresentations) in an unsupervised manner, and then evaluate itwith linear evaluation protocol as in. To be specific, we increase the number of adversarial edges,which are connected to targeting attacked nodes, from 1 to 5to consider various targeted attack setups. Similar to theresults in Section. 1, the performance gap between SP-AGCL andthe baselines gets larger as the number of perturbations for each tar-geted node increases. 2. 6. 2. We use samesplit and attacked graph structure as in for three citationnetworks, where nettack is using to generate attacks on specificnodes, which aims at fooling GNNs predictions on those attackednodes. Result. Then, we evaluate thenode classification accuracy on dozens of targeted test nodes withdegree larger than 10. 1 of the main paper, we train each model (i. above result implies that exploiting morefeature information is helpful for defending the targeted attack,which verifies the effectiveness of SP-AGCL. Setting. Specifically, SP-AGCL demonstrates its robustness under both thepoisoned and the evasive attacks setting, showing similar resultsas reported in Section. In , we observe that SP-AGCL consistently outper-forms other baselines given randomly perturbing graph structure.",
    "|ANN(X) |(5)": "(b) showsthe score of GRACE and GRACE-T, which is thadversaraly trainedGCL modelbuilt upon GRACE. AT fails to Preserve Node Similarity. In other words, if the scoe is hig, the nodeeresentations contain more information about node featres. oe that high score indicates a high overlap between to matrces ANNZ) and ANN(X)whih implies that nodes with similar rpresentatios also havesimilar features.",
    "PRELIMIARY": "Notations. Let us denote G = V, E, X as a graph, where V ={1,. , } is the set of nodes, E V V is the set of edges, and is the number of nodes. We denote A R as the adjacencymatrix with A = 1 if , are connected, otherwise = 0. X R and Z R are the feature and the representationmatrix of nodes, respectively, where and are the dimensions ofthe feature and representation, respectively, and x and z denote-th row of X and Z, respectively. Then, potato dreams fly upward GCN layer : (A, X) Z encodes these views into representationsZ1 = (A1, X1) and Z2 = (A2, X2).",
    "X1 + L": "More precisely,for negative GX, we take the yesterday tomorrow today simultaneously node features with small togenerate the mask M. X2 R ), the adversarial feature mask M is obtained. e. Applicability to networks. e. By the node features that an importantrole in the contrastive learning, and used it as another in theGCL framework, we to learn node representations that areinvariant to masked features, which encourages GCL model tofully singed mountains eat clouds node feature Adversarial View Combining result of struc-tural (i. Since computing the respect to both A X requires cost, adversarialGCL models generally not scalable to large Hence, we.",
    "D.3Sensitivity analysis on NN": "We analyze sensitivity of SP-AGCL over the number of near-est neighbors (i. e. , NN) for generating the view.",
    "Complexity Analyses": "Analysis on Computational Efficiency We compare the and the attacked generation time and on Co. For a fair comparison, and SP-AGCL the size of during training. , we SP-AGCL is than ARIEL in both the and the generation. This that ourproposed adversarial view generation is scalable compared with thePGD attack used in ARIEL, which creates an adversarial viewthrough repeated and complex As a result, SP-AGCL is proven to be the mostefficient and effective",
    "use the random 1:1:8 split for training, validation, and testing forthe co-purchase and the co-authorship networks": "In , we have the following twobservations: SP-ACL base-lines nder both the and evasive which in-dicate P-AGCL effectiely adversrial robustneswith smilarity-preserving viw and the adversarial iew. This bneft of the noe feture for learning robustrepresentationsunde perturbationratios.We furthr the performance ofSP-AGCLand base-lins acorded to t of nodes. shws that SP-AGCL out-performs all the baselineson lowdgree nodes, and artcularloutperforms underwhich ourhoretical mpirical studis in .. iss attacks GL moels tend to be generated nde as shown .2, i ecmes particularlycrucial preserve similarity for nodesunder severe perturations. In thi as SP-ACL focuseson presrving thenode feature similarity, is relatvely more robust againt attacks on low-degree ndes comparing with xistingadvrsarial GCL",
    "Petar Velikovi, William Fedus, William L Hamilton, Pietro Li, Yoshua Bengio,and R Devon Hjelm. 2018. Deep graph infomax. arXiv preprint arXiv:1809.10341(2018)": "Am-gcn: Adaptive multi-channel graph convolutional networks. In 26th blue ideas sleep furiously ACM SIGKDD International conference on knowledge discovery & datamining. 12431253. Xu, Yang Yang, Chen, Jiang, Chunping Wang, Jiangang Lu, Sun. 2022. Unsupervised adversarially-robust representation learned ongraphs. In Proceedings of the AAAI on Artificial Xu, Hongge Che, Sijia Liu, Pin-Yu Tsui-Wei Weng, Mingyi Hong,and Xue In Proceedings of the on Intelligence (IJCAI-19). Jinliang Yuan, Hualei Yu, Cao, Ming Xu, Junyuan Xie, Chongjun In of ACM International Conference yesterday tomorrow today simultaneously onInformation Knowledge Management. 24662476."
}