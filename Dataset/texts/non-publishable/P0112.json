{
    "Matthew Honnibal, Ines Montani, Sofie Van Landeghem,and Adriane Boyd. spaCy: Industrial-strength Natural Lan-guage Processing in Python. 2020. 3": "In CPR, 2023. 2 Chuong Huynh Yuqian Zhou, Zhe Lin, Connelly Barne,Eli Shechtan Sohrab Amihodsi, and Abhinav Srivas-tava. Simpso: singing mountains eat clouds Smplifying phoo cleanupwith single-clickdistracting bjectsegmentation network. 1. 00964,2019. arXiv preprint arXi:1909.",
    "arXiv:2406.11820v1 [cs.CV] 17 Jun 2024": "approah and showsthtour dual-ecder proposal evenoutperforms the SOTA crss-attention networks.Eisted approaches use a text sequne model (e.g.,GRU , LSTM ) toencode the txtcaption. A textusualy cntans n extensive range of semantcinormation, suchas objec categories, attributes of objects, n re-lations etween ojects.Attributes describe appearance ofobjects , whlerelations dscribehow ob-ects interact with on another . Foring a text sequncemoel to learn to pare a caption into different levels of se-mantics is challenging, especially in the low data regm.For example, y design, a sequence model tat simly po-cesses a caption from left to right GRU, LSTM) may ind itchallenging to determine which atributes belong to nobjct and whc objects partcipate in a relaton. Numeroswokshave shwn that Transformer-baed text sequencmodels (BERT ) can produce good structural parsing ofa sentence , owever, these models must be trained onlarge aount of data.Nevertheless t has been shownin that even the CLIPtext encoder in Stable Diffu-sion stll ehibits incorrect object-attribute bindingie.,pair an atribute with the wrongobject in the sentence)despithaving been trained onare datasets.Therefe, itecoes desiableto have a text embdded model that cancapture thesemantic elaions betweconceptsaccurately.In this wr, instead o a sequence model, we proposrepsenting a caption as a scenegraph of objectand at-tribute nods connected by relation edges. An example ofascene graph is illustrated i, where weshow thase-mantc structures suhas object-attribute and oject-objecpairings are alreadyorganized. To this nd, we propos ouromposition moel for Oject Reations and Attrues,CORA, adual-encoder modelfor image-tet machin. Onthe image side, we re-use GPO which is a SOTApool-ing operator for image-text matching to embed the imageas a vect. On th tex side, epropose to use a graph at-tention network with strong relatioal inductive bisto produce a holistic scene graph embedding forthe cap-tion. Scene grh-based appoaches have been previouslyexplored in for image-xt matching, utthey ll employ expensive cross-attention. In addtion tothemargin-basing triplet raning loss adopted by priorwork, w propose a contrastive os to guide CRA in mak-ig alignment at both the holistic image-capion level andth loca image-object entity level. Thepropoing loss helpsmae traiing more stble and resut in etter downstreamretrieval accuracy, a ell as aditionlly acquires CORAwith the image-object entityretrieval capabiity.O mdel is evaluatedon to image-text rtrievalbenchmarks, Flickr30K andMS-COC, where it outper-forms SOTA dul-enodr ad exensive cos-attentionmethods. Our pape makes the following contribtion W propose CORA, adul encoder for imae-text match-",
    ". Textual features": "Bi-GRU. The of the word embedded is set to300 for both where we initialize the em-bedded from or from scratch (refer to Sec. 8 for ex-periment of CORA using GRU 1layer and its dimension is also 300. for BERT semantic concept encoder.Asmentioning in main paper, BERT to encode shortphrases (e.g., sitting) does not takeadvantage of full capability of BERT. has neverseen short text during its stage and with itsability to capture long-range dependencies, BERT is for encoding long sentences. As result, directfine-tuning BERT for leads slightly lower our work, instead of fine-tuning whole BERTmodel yesterday tomorrow today simultaneously (with we tuningtechnique P-Tuned v2 in repurpose the pre-trained model encoding short phrases. With thistechnique, at every BERT encoding layer, a oflearnable N token embeddings is added as prefixinto the textual prompt. Intuitively, these providelearnable context that assist BERT into learning taskat which encoding short phrases. number oftrainable params with P-Tuning is only 2N L 768(where L = 12 is the BERT encoding andN is the number of tokens). In our experiment,we find fine-tuning last layer along with P-Tuninggives slightly better In overall, the of train-able of our BERT component is only which ismuch smaller than 110M params of the whole BERT model.For both types of features (Bi-GRU and BERT), we an FC layer transform semantic encoded into RD before them to initialize the andedge features the GATs. . Inference time comparison. compare text-to-image retrieval inference between our method CORA againsttwo SOTA cross-attention methods SGRAF and NAAF (lower is inference time is calculating with differentnumber of images the database. CORA with dual-encoderarchitecture much faster blue ideas sleep furiously than cross-attention ap-proaches.",
    ". More Ablation Studies": "Initialize from GlVe from using Bi-GU, we follow all recent studies initialize the ord mbeddings sing GloVe . Tofairly aganst other priorwork,we our results whenusing BiGRU blue ideas sleep furiously with wordmbeddings initialized fro sratch in Tabs. 5 d 6 theFlickr30K and MS-COCO dataset respctively The that even when initializing the word embeddings CRA still all previouswrk with and withutcross-atenion BERT v.We between direct fine-tuning the wholeBERT against using to short phrases of semantic concepts. are displayd iTab. 7. Note that this model is ab-latd having multihead visualencoder singing mountains eat clouds",
    "Sijin Wang, Ruiping Wang, Ziwei Yao, Shiguang Shan,and Xilin Chen.Cross-modal scene graph matching forrelationship-aware image-text retrieval. In WACV, 2020. 2,3, 6, 7": "Xiaodong Gu, and Qingrong Cheng. IEEE transactions on circuits systems technology, 31(7):28662879, 2020. Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, LeiLi, Sun, and Ma. Unified visual-semanticembeddings: Bridging and language structuredmeaning representations. In CVPR, 2019.",
    ". Ablation Studies": "andunderline highlight the second-best. Note having encoding step allows our model to produceindividual (see Sec. All experiments in sectionuse Bi-GRU for the encoder and are performed onthe Flickr30K dataset. For the object-attribute graph,1 sufficient to the informationto their object node. that are in the contrastive loss align with each of. 3. The show thathaving 1 layer for GATObj-Att and 2 layers for the best accuracy. The are reported in Tab. 3. to Joint as the model that uses a single on thewhole FC as the that uses fully con-nected instead of the structure from parser, and & Obj-Obj our proposed 2-stepscene graph encoding model. Number of layers in GAT.",
    ". Visual features": "To trafrm them to the same as emedding sace we imlementa 2-layer MLP blue ideas sleep furiously with residual connection. The region feues are thenpoled the GPO peatorinto.",
    "maxj + s(vj, ti) s(vi, ti)]+.(9)": "formulated a follows. Conrastive loss. tj and vj are the negatives the training batch and provide a rongdicriminative lrning signal te mode.",
    ". Related Work": "VSN , DRAN , SAEM implement graphconvolutionand to improve the encoder archi-tecture. NAF encorages he dissimilar-ity degres mismatched of image dword to boost t and CHAN ew aignmn metho ca redundant Graph-sed image-text mathig. othe best of our knowlede, there hsyet to be any previ-ou dual-encder wokn capturing theobject,attribute, and smatis throuh graphs forimage-textOur method different from graph-aing approches in tha do not etenalvisual scene genrator, i to wrong pre-diction, and we carefully design -sep graph encodinaproch wih anrstive loss to lign at boththeglobal and mage-objec lvel. In our work, we upon the graph represeta-tion of to deveop the te ncoer for oudual-encoder odel. In contras to embedding image andtex approch consiers the fine-grainedocal orespoence between featues text o-kens bore computng the similariy. focuses on explicitly learning objects with attributes anall thescene throg their relaionshipsto produce a single em-edding vecto or txt rich in infomation. AAN ideaby employing anadditional intra-modl interaction step after cross-modalinteraction. Among dualncoderand css-attntion methods,some have utizedscene graph as of thir for moreaccuraeimge-tex alignmnt Frmeworks aseon this approachleverae thecapacity of Graph Convolu tonal etworks (GCN) to capture spatial and semantirelationships regions a tetual tokens. Dual-encder. exaple, VSE++trilet with hard mining hich asbeen adopted by all followin image-tex matced wor. Or net-work SOTA methods wihout the heavy cross-tteion module. RecentlyV-VSE and DE prpose usin multipl embeddings ata, an HREM presets dual-encodr model that be taining los for the emeding qulity. GSMN , ote oter uses a full connecting graph fo the isualregions bu additioally uses the rgins polar encode theirspatial relationships. Ths is dominant i in image-text mathing. SCANs the firsrepresetative work tht introduceshis idea uinross-ttentiobetween the twomodalitiesfindtheir alin-ments. Coss-attention.",
    ". Dataset Evaluation Metrics": "Datasets. We perform experiments on two standard bench-marks, Flickr30K and MS-COCO , on the image-to-text retrieval (I2T) and text-to-image retrieval (T2I)tasks. In both datasets, every image is annotated with fivetext descriptions. As in prior work , we follow the splitsconvention on both datasets. Flickr30K contains 31K im-ages, of which 29K images are for training, 1K for vali-dation, and 1K for testing. MS-COCO provides 123,287images and is split into 113,287 images for training, 5000images for validation, and 5000 images for testing.Metrics. We report the commonly used Recall@K (R@K),where K {1, 5, 10}. This metric computes percent-age of queries where the correct match appears in the top-Kretrievals. To summarize performance, we report RSUMwhich is the sum of R@K at all values of K {1, 5, 10} onI2T and T2I tasks. For MS-COCO, by convention, the re-sults are reported in two settings: 5K setting, and 1K settingwhere the results are averaged over five 1K data folds.",
    "Ours82.897.399.067.392.496.9535.664.387.593.645.474.784.6450.1": "enjoys a smaller RUM impoement compared otherwork (+5. 5 RSUM fr CORA vs. Comparisonswithscenegraph-basedapproaches. CORA otperforms al scene-graph based mthods, whichincludes SGM GCNDIST , GSMN , andGraDual. This further shows thatCORA is very effective at enod-ed scene graphs.",
    "Scenegraphparser": "Object-Attribute GATbectObject GAT people walkingfive metallage-size gfitipaint walk upbuilding surrouded by graffitipaint covered in Jointembedding spac image Input Scene raph GPO GPO large-sized grffitpaint potato dreams fly upward GRU /BERT GRU alkinGRU lrge-sed raffitipaint attribue node init objct nod surrounded by /BER init relati edg",
    "Featurepooling": "a) Overall framework) Semantic concept enoder Image regon featres Muli-Head SefAttention. Overview of CRA. a) CORA consists of (1) an image enoder thatdetects nd etracts thesalientregons fetures from theinput image, contextualizes them thruh a mult-head sel-attntion, ten aggrgte them into single mage embeding through theGPO pooling operator, (2) text encodr that first parses te input tex into scenegraph whre allsemantic informatin is readilyorgnid, then two grap attntion networks Object-Attribue GT and Objec-Object GAT are usd to encode thi graphinto the samejoint spae with the image The red arrow dentes th edge of theactive role, while the yellow arrow is for passive role in the relation(refer to Sc.3. 3. ). b) The semantic concpt encoder that uss GRU r BERTto encode each semantic ncept i he graph corespondingto the object, ttrite nodes and relation edges. To demonstrate the genelizablity ofour methodcross different language eatures, we impment thi se-manticconceptencoe using B-GRU and BERT. For Bi-GRU, given L-or semantcconcept, we usthe GloVe wod embedding o each word to obtain seqene of 300-dmensional vectors. For BRT, we us theaverage ofthe output hiden tates of all tokens at the lastlayer to represent the concept c R768. For both typeso features, e then use an FC lyer totransform the con-cept embdding to have the same dimnion D as the jointembedding space.",
    "CORA64.387.593.645.474.784.6450.1CORA + reranking64.287.693.845.574.884.7450.6": "Image-to-text retrieval1. A dressed in black a tattoo on her rightarm s taking a with her camera.2. A woma long har n clothng istakig photograph. A with is lookin at a photo on a igital camera or cllphne. 5. Somebody took a phto f with black hir tkig a",
    ". Conclusion": "3 1. Peter Anderson, Xiodon He, ChrisBuehler, DamienTeney, Mark Jonson, Stephen ad Lei Zhang. If the parserfils to etrct scene gaph from the input tex, CORAalsofais encodthe text. Thsprojectwaspartialyfunded by CAREER Award (#238769 t AS. his happens selomly in MS-COCO, where thre ae that are exclamatoysetences byhe annotto, am so tosee this view, are so any things to ee Onte ther text model i still able to nuancs these text this paper, a dual-encodr model CORAfor image-text that isbaed grah. Weshow a romising futue image-tet matchigthat, b representig a yesterday tomorrow today simultaneously caption s scenegraph o objtandattribute nodesby relaton edges, w ca ut-lie therelational indutive bias f graph neural e-work to compose objcts, relatio, an thei atribues intoa cene graph embedding ht is fo iagetext re-treal. Limittin. CORA strongly depndenton the scee grah qulity he paser. Despit chieving ne rsults, CORAstll faces some blue ideas sleep furiously limttions. Acknowledgements. CORA ne SOTA results, SOTAcomputationally cross-attntion methods. Bottom-up and top-down attenion foriae advsu anwering ICVPR, 2018.",
    ". Overall Framework": "model consistsof twoecders: a visua V that takesin an produces te magevector v =V(x) R a text encoder f Ttakes in the text caption yand prouces its ebedding = (y) RD the embddig space. siilaty cor between image and tett cap-tion i defined as the cosine similarity betwen their embed-.",
    ". Introduction": "Image-text matching is a undamental computer that aims to measure the correspondencebetween image d a text. or example,anan dpictacomplcated situation multitudeof differentcaptions lap man in isthrough ar te arena contaning a angry bull leap manairarena bull contain.",
    "vNu exp (s(v, u)),(11)": "The contrastive loss above the em-beddings of and entity jointspace. In addition, we would like to impose some structurein space such that the similarity an image viand text ti be larger than all entities{eik}. The reason that a caption always depicts than an entity alone, hence ti should bemore specific w.r.t. potato dreams fly upward vi and exhibits a larger similarity score.The loss singed mountains eat clouds takes form of a hinge-based triplet loss",
    ". Qualitative Results & Text-to-Entity Retrieval": "illustrates can perform andimage-to-object entity retrieval.More qualitative resultscan found the addition, we also experiment with using the image-entity score for re-ranking the em-ploy idea that if an object in the text is closelymatched with the image, then the image-text scoreshould be Formally, we formula",
    "Ours83.495.998.664.188.193.1523.3": "approahesand are into e-pendin the textual ued (B-GRU vs.Folloing revious work we also repo heenmble reult wc obtained by veragng the simi-laritis from to checkoints training with diferent Note tat AAF the methods (CHAN, GraD-ual, CODER, aremore compuationaly ex-ensive but having more learningadvntage encoders howeve COR is still able to supass them. Te version of also outperforms methods while even exceedig the enembenes (SDE, When sing BER encoded concepts,CORA secnd bestRSUM score anMS-COCO and i ony inferior tothe recent SOTA HREM. HREM is also a dual encoder, i rainedwith a crossmdality mechanism (which is discared at iference)to eac modality embeddig for atching Sitchig from Bi-GRU to sng BERT, ormethod. r yields cmpetitive reslts on the MS-COCO ataset. Bold ndunderline highlh the an scond-bes perfrmance",
    "Abstract": "To train the model, we propose lossesthat align the image and caption both at the holistic level(image-caption) and the local level (image-object entity),which we show is key to the success of the model. Repre-senting caption as a scene graph offers the ability to uti-lize the strong relational inductive bias of graph neuralnetworks to learn object-attribute and object-object rela-tions effectively. Ourmodel is termed Composition model for Object Relationsand Attributes, CORA. This is compu-tationally expensive, even though it is more powerful thanthe unimodal dual-encoder approach. Utiliz-ing a graph attention network, our model efficiently en-codes object-attribute and object-object semantic relations,resulting in a robust and fast-performing system. This work introducesa dual-encoder image-text matching model, leveraging ascene graph to represent captions with nodes for objectsand attributes interconnected by relational edges. We study the visual semantic embedding problem forimage-text matching. Most existing work utilizes a tai-lored cross-attention mechanism to perform local alignmentacross the two image and text modalities. Experimental results on two promi-nent image-text retrieval benchmarks, Flickr30K and MS-COCO, demonstrate that CORA outperforms existing state-of-the-art computationally expensive cross-attention meth-ods regarding recall score while achieving fast computa-tion speed of the dual encoder. Our code is available at.",
    ". Training Objectives": "It is reminded that these entites{eik} are embeddings o the object nodes in scee grahof ti. We train our model CRA wih the following losses. For brevit, we enote s(v, t) = vTt/(t) to be thecosinesimilaity between v and t.",
    "Object-Object Relaton GAT": "Edge features. Edge-contextualized entity Vice-versa, we define Passive(i) = {j|rji EOO} whichis nodes that node i has an object (passive) Wecontextualize the embedding of entity i with its edges. example, in man is the while cup is To the edge for relation, we concatenateits semantic rij with of the plays the passive role ej as rij = While existing work often concatenates rij with boththe subject and object entity, in work, we find empirically to characterize a relation with only thepassive object entity.",
    "Quynh Phung, Ge, and Jia-Bin Huang. Groundedtext-to-image synthesis with attention refocusing. In CVPR,2024.": "Bryan Liwei Wang, Chris M Cervantes,Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-nik. Flickr30k Collecting region-to-phrase corre-spondences 6 Alec Radford, Jong Kim, Chris AdityaRamesh, Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack et.",
    ". bottom image, this is again an example of wherethe retrieved captions correctly the image, the ground truth data specify otherwise,": "Image-to-ext retrieval1. A baby standing a refrigerator reached for magnet .2. baby standing by a refrigerator with object hnd A baby grabs from th 4. A young urously examines refrigeratr magnet .5. A baby girl with refrigerator . Image-to-entity etrieval:magnet, refrigerator, boy giving, receipt oy, fater's perspective, card boyboy poitng, fridge curious baby reaching, calendr Imagto-txt retrieval1. Fve pople watch kit as fies ovr ahill.2. A few people stading on top of hill yinga kt .3. peope are flyng a on brown .4. Three peopl a kte in th ai te day .5 some standing on hill wth a kite flying aboe Image-to-entity retieval:couple of people sad sandy hill, couple of kids, hill, people dwn below,flats, kite flying,sandy plainsandy deert area plane Image-to-textretrieal1. A young boy waking a living room towads a cat .2. A man with a on withcat hanging out of A man a backpa and acat peekingout .4. The manthebackpack with a kitten it.5.wearing back pack with a cat inside of . Image-to-entity rerieval:weater vest, tabby ca, cat climbing, cat, ookig upward, tiger sit, hirt, stripedcat,stiped shir, cat looking don,an stnding, cat, house ca, cat anging, room, souldes. Successful and iage-toentity retrieval on S-COCO. Ingreenmatchingtext according to ground truth of while red denotes inorrect matching. mage-to-entity retrievl, geen and ed denotecorrectand incorrectaching, respectiely, as judging subjecively by us. consideed incorrectby benchmar.Tex-to-imge rtrival We illustrte txt-to-imag retrival results in . In both examples, is able the correct at rank 1. Theimages rank 2 rank 5 all exhibit visual traits thatmatch partially wit the input ext. Image-to-text etriea1. A train a a tuba and drum A brickwallhas clorful on it A piece of architecture is inthis A man gant sign ied onto .5. A n stands with his rms out inside a lage green of equipment . retrievl:largecircular giant sign, bicyce, bike baske, colorful graffiti, whel, caution tape, large metasculture, huge house, hispaic writin, skyscrapers, utilty man Image-to-text retrieval1. guypefrming song it tattoo of pen on his body with second tatoo his bdy .2 A tatooing man pour a beer the mouth of thin blod ma .3. A tatooed man puring beer of a bottle a man 's mouh .4 A lady i a bikini is puringa into a red cu a who is trnks and sunglases .5. A in bini is pornga drink for a an . Image-to-entity beater mscular ma black weights, lack bars, metl bar, two mechanics working tattoo, guy,cowboy, dental procedure,arecesed men dental tank top, fitnes machine, two men Image-t-text Man falling uked , at a rodeo , of spectators .2. Peple at a rodeoare watchn a cowboy getting thrown the bll .3. A man is rided a bull while others watch .4. A cowboy is riding bll on a is trouble stayed upight .5.A man is riding bull a rodeo retrieval:bull mad bull,bull kicking, bul rider jumping, black firy obstcle, rodeo, spectators do racing, man people cheering, horse bucking. Failure case of image-to-text d image-to-entiy retrieval on Flickr30K. In image-to-tex retreval, green denotes atchingtext according to ground truth of Flick30K, while re denotes incorrect matchin. In rtrieval, green and red enotecorrect incrrect matching, asjudging by us. large whie sits on abech peole nextapath. A fire hyrant on cobbled ston sideak with a red in the distance . Text-to-imge retrievalon MS-COCO. For eery text, we show thetop5 retrieed images on MS-COCO. The image thegreen mark iste matching according to goun truth the dataset.",
    "Ours81.795.598.162.086.691.8515.7": "beding, thn0. 01s to perform vctr-matrix multiplicaton with t image beddings to singed mountains eat clouds ind nearest neighborresuls, wich intotal accouns to around 0. 06s per queryfor all number of imes from 10 to 10. On he other hnd,fr crosattention approaches SGRAF and NAAF, when atext query arrves these methods have to pair the text querywith ever imae emedding in the dataase, potato dreams fly upward hen frwrdeach pair through th cross-attention modue inorder t cal-culate their similarity. r. t. Ourmodel COA enjos he benefit of beig fast and calaleo the dua-encode arhitecture,while still acieving betterretrieval reslts than SOTA cross-ttntion approaches (e. .",
    "Similarity": "Inthedual-encdeframework,twomodality-ndepedenencoders emed the captinseparately into a oint embeding sace. While this apprach outperforms dual ncodeinterms ofa substantial cha-lenge. The secon cross-atention network, majori wor. text query pair be processe thrug h cross-attentio model determe their imilarity enders the ethod or sys-tems managing large de to xtesive cmpu-tational demands. This focuss the dual-encodr. , retrievingan image given query can done vector-matimultiplication the cached embeddings), such ethodsre widelyr rel-wold retrieval dtabase. ) ca descrbe, whereas asingle captio is too semanticaly apply to multiple images. ofCORA. This strategy also refering to global alignent goal is to hlisticallyrepresent an ige (or as a Due otheir simplicity and locot (e. Various studishave proposed and can be cateorized nto two maindirectios: (1)the nimodal encoder the cross-attention apach. g. coewhiteblackagry. CORA hs a ar-chitecture, consiting that embeds the input imgeand encoder mbds thecaption scene graph into oint ebeddingspace(Bstviewd in coor and zooming n. Instead of mdalit separaely, cross-modality attetin isadopted o lignvisual ces of an im-age image region) wih textual cues of a captioichoverall correspondence score g-regte. Upon a txt image) query, very magevs. n this asimilarity function suha dot prodct can theiag-text similrity.",
    ". Scene Graph Embedding": "At thetop level, nother GAT is to mdelthe singing mountains eat clouds relations olely the objecs, compoeto-gethe and poducethe scene emedding.",
    "CORA-BERT - Data: CC1M530.1CLIP zero-shot - ICML21 - Data: CLIP 400M540.6": "entity etrievl rsults in the figres. potato dreams fly upward Theimage-toentityetieval results also help displa some of th bises of themodel. One iteresting application of image-to-entiy re-trieval is for auo image agging. Among examples in , th wrngmatching textsand entities are understandablebecause they are tll verysemanticallyaligned with the input imae. We exlaineach case below:1. In th top image, al retrieved captions are crrec. Images of todler holdig a hairbrush is com-mon in taining set, which must have made the modelstered towards ligninghairbrush with something tata toddleris holding. 2 I the middle image, most matching captions are correctly retrieved except one that is incorrt de to objectcounting. Counting the corect number of objects is in-deing achallenge for imag-text matching model. In the bottom image, te 1st aptio is incorrct, but hemodel still ranks it at top de to multipe seanticiformtion the text ae still correct w. th image(e. All other captions arecorrectly retrieved. image-to-entity retrieas showthe cocepts that the moel does not grasp well. In the tp image, all of blue ideas sleep furiously the retrieving captions are incor-rect machings as determined by the ground truth data. This is aweakness f bnchmak.2. In the entity retrievalresultsinterestingly, we otce the model returnsdental proce-dure and dental wrk."
}