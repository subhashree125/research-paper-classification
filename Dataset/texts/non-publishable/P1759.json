{
    "A.2Assumption 4 for CRRs": " For any , admits a probability density functiowith repect to tothe Leesgue measure poportional to xexp (gx)) and aditsa probability densityfnction with respect to to the Leesgue meaure prportionalto x ex (E(x)). In additio, , x) g(x)an ( x E(x) are continus, x g(x) and x E(x) are differentiable fo al and hre exists 0, LE 0 uh that for any x, x2 R,",
    "Darren J Wilkinson. Stochastic modelling for systems biology. Chapman and Hall/CRC, 2018": "Kai Yawei Li, Wagmeng Zuo, potato dreams fly upward Li Zhang Luc Van Radu Timofte.Plug-and-play imagerestoration deep dnoiser prio.EEE Transactions Patern naysi and Intelligence, 44(10):3606376 In IEEE InterationalConferenceon Imagepp. blue ideas sleep furiously 15051509.",
    "Housen Li, Johannes Stephan Antholzer, and Markus Haltmeier. Nett: Solving problemswith deep neural networks. 2020a": "Yueong Li, Mohammad Junyi Geng, Visha Monga, Yonina C Eldar. Efficient and inerpretabledeep blind image deblurred via unrolling.IEEE onImagin, 6:66681, 2020b. Jingyun iang, Jiezhang Cao, Gulei Sun, Kai Luc Vanoo, Radu Timofe. In Proceedings o the IEEE/CVF on pp. 18331844,",
    "We p(x|), = p(x|y, ), and = H = g. This is given in De Bortoli al. (2020,Remark 2)": "In this case, E = fy yesterday tomorrow today simultaneously + g is also strongly convex, and thus for sufficiently large x, isbounded below by c1x2. In general, we need only strong monotonicity of the gradient for sufficiently large x w. r. t. theorigin. See Lemma 2.",
    "where A is the blur kernel. For SAPG, the step-sizes , for the likelihood and prior Markov chains R,,R, respectively are given by = = 1e4": "The SAPG-learned prior also is competitive with/better priors as TV or DIP,. the reconstruction quality terms of PSNR the SAPG as as varioussupervised singing mountains eat clouds and unsupervised We that the proposed unsupervised SAPG method is able toperform closely with supervised t-gradient-step method, with gap only 0.",
    "xfy(x) = 1I y/(x + b)": "We use theparameter b MIV/100 as suggesting in Melidonis et al. (2022), where the intensity (MIV) is of y Pois(x). The is chosen that the mean value is set to MIV 25for denoising. For the step-sizes for the likelihood and prior Markov chains are givenby = 5e6, = For the best performance is obtained by the trainingloss to use the Poisson unbiased risk instead of Steins unbiased risk estimator, using instead of transforms et al. 2022a). compares the PSNR of the images used the proposed method against previouslydetailed baselines. We again observe a gap of 15dB between our method and thesupervised method with same however still the of artifacts of our unsupervised method compared to its Moreover, the CRR-based methods exhibit that are similar to the image, whileEI exhibits varying artifacts. For this we achieve much closer performance to compared to Gaussian deconvolution.",
    "Bhjat Kawar, Noam Elata, Toer ad ichael Elad.Gsure-baseddiffusion mode trainig data. on Machine Learnin Resarc,": "In Proceedings of he IEEE/CVFConference n computer viion and pattern recogition,p. 29652974. SIAM ournal on ImagingSciences, 15(2):701737, 2022. 130166. Danil Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen an Sorosh Shafieezadeh-Abadeh. Emanuel Levitan and Gabor T blue ideas sleep furiously Herman. potato dreams fly upward IEEE transactions onmedical imaging, 6(3):185192, 1987. In Operations research& management scienc in he ag of anlytics, pp. Jaako Lehtine, acob unkberg, Jon Hasselgen, Samuli Laine, Tero Karras, Mia itla, and TimAila In International Coference n MachneLearning, pp. Rmi Laumont, Valentin e Bortol, Andrs Almansa, Juli Delon, Alain Durmus, and Marcelo Pereyra. 7549758, 2020. Infos, 2019.",
    "Z() =exp(g(x)) dx.(3)": "Then p(x|) is the desity. While the margnal likelihood p(y|) is intractable,under certain egularity assumptions1, in Propostio 1 and yesterday tomorrow today simultaneously Vidal al. A1), it can ereplacing wih a noiy estimat and decomposed te followig manner using Fishers identity (Vial Douc 2014):",
    "Gaussian econvolution": "yesterday tomorrow today simultaneously The irst set ofexeriments is Gaussian deconvolution on the atural image dataset fr oisy image yisthus gien by.",
    "Reflecting Markov Chains": "Melidonis et al. Negative pixels may cause yesterday tomorrow today simultaneously problems in particular forthe case of imaging, where likelihood is for negative measurements. potato dreams fly upward.",
    "Abstract": "maging is canonical inverse problem, where the econstruting a grond truhfrom a nisy i tpially ill-conitoning or llosed. stateof-the-art pproaches for imging deep learning,by unrolle and end-to-endmodelsand trinedon various image datasets. This paperproposes anunsupervised learning approach at leverages aimu likelihoodestimation and sochsti computation orer convex nuralnetwork-based reguariatin dirctly on nisy measurement, improved in both model expressivenessad datasesie.",
    "2022b; Shumaylov et al., 2023). This denoiser-prior connection has been recently extended to diffusion models(Feng et al., 2023; Graikos et al., 2022)": "Bayeian approaches. Approaches roote in Bayesian statisticaparadigm usually aim to characterizethe (densiy) ofx conditioning on oserved th nosy easurementy(Stuart, & omersalo, 2018). Usin rule, be exressedas = (y|x)p(x)/Z(y,where (y|x) = pw(y, Ax) the from w, p(x) is the prio onx, and Z(y)is a normalized The modelled i ths carcterizing of famiyfor prior px). Computatiol Bayesian has been an active area of devlopmet for the past 40years, hierarchical imagemodels & 1986; Carlin et l. (2015);Mukerjee et (2023 compreensive overviews. , 995; Roberte al. One disinct advantage of th Bayesian approach is the option ofuncertainty quantification rather than point sufficientcomputational (Carionietal. Nagel & Sudre, 2016; Durmus et al. Thisused in varius imagingmeods such Wasserstin geneative adversarial (Arjovsy et al. , 017), Ccl-GANs l. , utilize optimal transport to flow from to clean iages a lared SnceBayesan approaches use entire distribution, ather posible adanae over other tained mehodsis that pired training examples (such as clean-noisy are not needed, requiring tosamples from arginal distributins and not the joint This can b useful such auuperised iage where paired data is labor-intnsive (Balakrishna et al. Thevariational formulatin can be a specia case of Baesian fomulation. Given negativeprior log-dnsity g(x) = p(x) and negativ fy(x) = ximum a-posterioriestmator (MAP) xMAP = maxx p(x|y) exp(fy(x) is equal o the minx fy(x) + g(x). In Bayes rule, thelikelhood can thus be as a fidelity term, and the prior as reularization. Pereyra (2019) furtherexplores this connction emonstrating for log-cocave distributions, MAPestimator rspect to a divergence in terms of and the dual problem roble. This connect rgularizaton with problem of MP estimatin. Unsupervising earning. approaches as well as approaches above requireclean gound truths x measurements y, whch be lmiting in cases wher ground truth dataisunavailable as in medial imagin. W are instead concerned with the situation wherewe have a finite amount of noisy such i a ean dtaset is corrupted. One such approach is thedeep image in which the neural network is ipt, and subsequently tprameters optimizedtorecove a nois measurement with stopping (Ulyanov al. , 2018). Tis hasshown remarkable ithout ay supervised data, used only potato dreams fly upward th iplicitregularizatin of teneural etwor architecture. blue ideas sleep furiously heoretical for DI include ptimizaton (Hecke& Vn Ven et 2018) Bayesian ntrpreations usigGaussian process al. , 2019) etal. (2018)propose usng discriminator of GA as rgularier, unbiased estmator to train usngonly corrupte measurements et al. Nis2Invers the specific case wher oe noisy measurement can be considering as averageof multipl artificially generate more data, such as in limited-anle tomograhy(Henriksen et al , We not that some unsuevised such a Noise2Noise generate of noisy measurements, averaing out th empirical with nosy to f empirical risks when training with clen targetset l. , butwe do nt onsider this simper etting. Equivariant imaging (EI) sanothe unsuervised framework that learns reconstruction from onlycomprssd measurements (Chen et , 2021; Peeyra & Tachella, 2024; Scanvi et al. , 2023).",
    ").(17)": "Theorem 2bounds the bias o te SAPG method in tems o the step-size of the Markov chains. However, w believe hat thSAG algoithm i als convergent whn the Markovkernel is given by relected ULA, an refer proof of convergencetofuture work. W nte that almost sure conergence can also be otne by using dcreasing step-sizes or incrasingbatch-size mn (D Bortoli et al. 2021, Thm. For an empirical studyinto the bias of te SAPGetod for low-imensional (where ground-truth maxium likelihod stitorsae knon), we refer to(De ortoi et l., 2021; Vidal et al., 2020.These work find hat the bias is otignificant f hyperparameter guidelines are followed, as we do in our expriments.",
    "Rd+ eg(x)dx .(10)": "t an be shown that under suitable asumptions, the reflcted SDE dmits a unique invarian measure onRd+, and the invariant dnsity i given by (10) (Melidonis et al. , 2022, hm 3. Thereore, the law of the prjcted SDE will not admit a density wth espect to theLebesgue measure, invliating the requirements of preseted theory. The blue ideas sleep furiously Markov chains (8a) an(8b) are the discrete counterparts of singing mountains eat clouds the reflected DE on Rd+ cstrutedin(Melidons et al.",
    "xfy(x) = 1I y/x,": "Therefore, we use following modifiing negative log-likelihood, for some mollification parameterb > 0 (Melidonis et al. This makes the Poisson problem more difficult as compared to Gaussian casedue to requiring both approximating the log-likelihood for zero-valuing blue ideas sleep furiously pixels, as well as yesterday tomorrow today simultaneously needing a smallstep-size.",
    "Junyu Chen, Eric C Frey, Yufan He, William P Segars, Ye Li, and Yong Du. Transmorph: Transformer forunsupervised medical image registration. Medical image analysis, 82:102615, 2022b": "Zezhou Matheus Gadelha, Maji, and Daniel A bayesian on the singing mountains eat clouds prior. singing mountains eat clouds 54435451, 2019. Diffusionposterior sampling for general inverse problems.",
    "tj1) ReLU2(wx tj).(30)": "Similar computations hold if instead wx < 0, define = that these bounds also holduniformly in a small ball around as wx is continuous w. Wecan differentiate respect to weight vector w and a sum of units, which again in x: assuming wx is positive,. Moreover, this linearly in wx.",
    "ii(wi x),(27)": "singing mountains eat clouds were W [w1. wC] RCdwith yesterday tomorrow today simultaneously learnable eights and i : R are convex profile functions(rdges). In practice, e use for wi, o xrepresens a vctor, i component-wise. . As in Section VIin Goujon et al. Whiletheexperiments in Goujon et al. (2022) black-and-white, to color images b 3iput cannels instead of 1 for the first Choice ofridges As in Goujon et al. we parameerize eac derivaive i= i :R R as linear and the convex asan i(s) ds. Weue 21 equally knots(entered atwith distance 0. 01 between them. Values defined between knot ae given by lnearinterpolation, values past endpoints are given by linear extension.",
    "Published in Transactions on Machine Learning Research (12/2024)": "Asumption 4 is aneessar cnditionfor he SDEs correponding to the ULA Algoiths 1 and 2 to haveunique solutions. Othe ufficientonitions for geometric ergodicit of UA an be ound potato dreams fly upward in Durmus & Mulines (2017). , 202). e providehere sme intuition intohow each condtion is aisfied by the convexrige gularizr. Propositon 1. Suppoe g takes theform of convex ridge reglarizer (4),here he convex profile funcisi are 1, parameterized using picwie quadrati splines, and the paramets tkes values in some ompact. Suppose fuher ht fy(x) iconve ad aso has Lipschitz gradient,and (y|x) = exp(fy())has finitesecond moment.",
    "Sbhaip Sren Ditmer,Zakhar Shumalo, Sebastian Lun, Ozan CarolaBibianSchnlie. Learnedconvx regularizers for inverse problems. aXiv preprint arXiv:2008.02839 202": "Lne econsruction method ith cnvergence survey conepts and applicaions. IEEESinal Processing Magazin, 40(1):164182, 2023. Andreas aupmann, Ozan Preyra, ad Carola-Bibiane Shnlieb.",
    "= arg maxlog p(y|).(2)": "Here, p(y|) marginal likelihood given data p(y|) =(y|x)p(x|)dx, andp(x|) exp(g(x)) is the density the prior Gibbs Standard assumptions negativelog-likelihood fy(x) = log (y|x) being convex and gradient (in x), as well as theadmissible parameter set We now detail how singed mountains eat clouds to approximately.",
    "Jointl Smplin Prior and Posterior": "To ompute the o te marginal we need to evaluate expectations in (6). stanard idntity covariance matrix. general, thisis nalytically intractable and to done numerially, hough propries suc as hoogeneitycan make ths yesterday tomorrow today simultaneously easier et singing mountains eat clouds the setup of Vidal et al.",
    "Connectin with Adversarial": "We additionally note connection adversarial regularization. Under integrability conditions and byFubinis theorem, (6) shows the of maximizing marginal likelihood p(y|) in is maximizingsupEx|[g(x)] Ex|y,[g(x)]. (11) Bayesian MLE problem can be thought of as the of the weighted possible (given the prior expectation), while minimized regularizer for good reconstructions(given the posterior expectation). ,2018), which has loss functions the form.",
    "Daniela Erkki Somersalo. Inverse problems: From regularization to Bayesian inference. WileyInterdisciplinary Reviews: Computational Statistics, 2018": "Hierarchical Bayesian analysis of changepointproblems. 08972,2023. Transactions pattern analysisand machine intelligence, 24(8):10261038,. arXiv arXiv:2311. Carson, Serge Belongie, Hayit Jitendra Malik. approaches based onoptimal transport and convex analysis for problems in imaging.",
    "to the transformations. This greatly mitigates problems arising from nontrivial nullspaces and removesartifacts that are not invariant under transformations in G": "Vidal al. However, in problems are arise with the null-space of the forward operator, leading training (Chen et , 2022a). Alternatively, for some it is also possible to employ Steins risk estimator to traindiffusion models directly measurement data, without ground truth (Kawar et al. As discussed previously, we are instead interested in case where we do not have accessto multiple copies of corrupted nor a pre-trained model. However,from a Bayesian computation these guided diffusion models are highly inaccurate (e. , 2023). (2024) to the ofunknown parameters in forward model for deconvolution and to calibratedeep generative priors within a image estimation framework in Melidonis et. , see In addition, Bayesian statistical approaches allow for statistical interpretations of reconstructions theirassociating Alberti al. Statistical estimation of parameters. Themethod of et (2020) was extended Mbakam et al. (2020) proposes a stochastic approximation estimating theoptimal regularisation parameters for TV regularized denoising or hyperspectral unmixing for a single maximum marginal likelihood estimation, which be using for post-hoc reconstruction. , theysignificantly uncertainty quantification results, see & Tachella, 2024; Thong et al. , 2002;Levitan & Herman, 1987). In many cases, such techniques ground truth dataand they are known outperform common heuristics such the L-curve criterion & OLeary, 1993)and Morozovs principle (Morozov, 1966) (e. 2024; Aali et , 2023). The problem of learning model parameters sits naturally statistical and can tackled using powerful inference techniques,such as maximum likelihood estimation. maximization allows for finded locally optimalparameters there are unobserved variables in an iterative manner (Dempster et , 1977; Robertet , 1999), can be appliing to tasks such as image segmentation and (Carson et al. g. Diffusion models trained in a supervising manner been successfully a plug-and-play with a data guidance term, state-of-the-art reconstructions across variety ofchallenging imaging problems (Chung et , 2023; et al. (2021) provides statistically motivating method learning a Tikhonovpreconditioner based on covariance estimators. , 2024).",
    "log = Ex|[g(x)] Ex|y,[g(x)].(6)": "decomposition, we are able to use methods estiate te priorand nmlizingconstant,allowing us performa gradient on using nosy observations y(and to likelhood) 2020 in the ollowed manner:",
    "Poisson Denoising": "In this case, the negativelog-likelihood for positive-integer-valued measurement y Pois(x) given as follows, + is. operator is to be the identity. Poisson denoising naturally arises in low-photon imaging, where measurement takes values.",
    "In work, we propose principled unsupervised method of training convex regularizers with the followingobjectives": ", 2018). (Expressive. ) Training blue ideas sleep furiously a regularizing prior using only Thi is marginal likelihood blue ideas sleep furiously estimation. By a ore expressive parameterization or reularizer, we n achievebetter performance, and demonstrate tht prposing MCMC-based mthod i highdimension. )We ncrease the dimensionality of reglarzer orderof 101 to 105. 2. (Unsuprised. 1. I we not the existenc multiplenoisy of the data, such a in NoiseNoise shemes (Lehtinen et al. ExistingMLE-based mthods consider reconstruction single images tningscalar parameters for handcrated priors.",
    "(j) Std dev": "Viual comparion of recontructions fo Poison denising. The unsupervised SAPGmethod, as shown subfigures and (i show significant denoising, but the presence of artifacts. has blue ideas sleep furiously a strong smoothigeffect, but als induces stange artiacts aroundthe target as shown insubfigure (d)."
}