{
    "Related work": "Therefore, in this framework G are optimized touse G to sample parameters F that can be used by F to process X and predict Y for task T . This approach can reduce the parameter search space and offers adaptability as, unlike conventionalparameters that remain fixed after training, it can be conditioned to sample parameters for multiplerelated tasks . It has shown promise in continual learning, transfer learning, weight pruning,and uncertainty-aware modeling . Diffusion Hypernetworks.Similar to diffusion models , neural networks training withSGD transforms randomly initialized parameters (noise) into a structured set that forms a specific dis-tribution. While this is still an emerging area of research, early approaches like G.pt show thatdiffusion models have significant potential to act as hypernetworks. However, despite these limitations,we show that its ability to learn a conditional generative model over weights of the main network isuseful towards MU. See Appendix A.6. Recently, extended ideas to text-to-model generation, where a singing mountains eat clouds customizedclassifier for a subset of classes of a large dataset can be sampled by prompting text.",
    "for MNIST-4 DiHyFo-2 showed to better at aligning losses, for MNIST DiHyFo-1 betterconsistency See Appendix A.2 for details": "Unlearning Performance. shows that all the sampled unlearned models the forget sets and maintain good accuracy on the retain comparable to theretrained models. The obtaining scores for all unlearning models very close to theretrained models, indicating good unlearning and inference attacks. As shown in, the comparison of by sampled models and the shows similar predictions on the retain and sets, with performance.Note that the overlap not expected, as our model needs behave anyretrained model that results from the nature the training not like one specificretrained model. Moreover, the comparisons parameters show high unlearning scores.Thus, sampled unlearned models mimicking the behavior retrained model, with DiHyFo-1performing more consistently across unlearning tasks, particularly MNIST. Appendix A.1for additional results and further discussion. Overall, the sampled models are effectively unlearning set no longer rely theassociated data to make predictions, and performance retain sets without significant signsof catastrophic , closely approximating unlearning gold Therefore,we have potential REU method that is model-intrinsic, accuracy-preserved good level ofcompleteness, guarantees, and a choice for unlearning.",
    "A.4Additional details on evaluation metrics": "Duringauatin, mdel ispromptedwith a vlue close t the best los found in the rainigdatset. BothDiHyFo ode use MSE as traiing metric, and e analze the correponded learning curvesto unerstan the models leaning behavior and convegence, and idenif issueslike overitting oruneritting. To evaluate wether the geerte paameers effectively approximatepromted lss values,we use the prmpt alignmnt metric defined by as the R2 score betweenthe obtained los andttargt los in Equation 6, averaged over the batch size. It is noting in that using value slghtlyabove or below the bst loss can somtims yieldbettr result or crin task. Each model must be evaluated on two key aspects. First, their geerative propertes a DiHy need tobe assessed. Thinvolves verifying wheher the models can generate approriat parameters for temain netwr enablin it to approximatetareted performanc levels on the lasification taskforeac clss.",
    "conditios s challeging and often involves trade-ofs ased on appliation forget setsize, and available esources": "Also, uneaning aloiths canbe moel intrinsic, agnostic, or iolve data modifiaion for fasterretraining; and may needfll, partial,or no accss to traiing data. Ths process transitios frm les to moreorgetig, positioning orgetting as the invere of learningan unearning process. By recostructingthese states, we am toplace th de in astate where it as ained essentia nowledge on theretain set, ut nt on the forget set.W name thisapproach HyperForget and, to thebest of our knowledge, it is the firs use of hypernetworks for MU. Byitegrating diffusin models as hypernetworks , we creat two Diffuion Hyperorgetetorks (DiHyFo) shown in , an use them to amplenleared moel for MU tasks iProf-of-Concept POCs) scenaros, coparing them with models retrained from sratch wthout theorget sets to howteir potential forMU. Notabl, whle a retrained model should be constrctd foreach orget et, oneDiHyFo can sample unearned models f all the forget sets and anbe nterpeedand evaluated sing boththe paramete andthe output space perspcives o unlearning.Ourresltssuggest tht the sampled leaned models effectively achieve zero performance on foget sets whileminaining high accuracy  retaisets, and closey mimic a erained model Ou contributions are: 1. Weconcptualize and approach forgetting for ML models as agenertiv pocess, mkga modeto forget by positionn i n a state where its parameters ve ained relevatknowledge adcapabilities on the retain set but ot nhe forget set. 2. We propose a novel REU framewo leveraging hpernetworks and diffusin modes. Weprset o implementations ad POCs to shw their ptetalfor unlearnng, and highligsgnificant limitations and potntil futre directins.",
    "Stefan Schoepf, Jack Foster, and Alexandra Brintrup. Potion: Towards poison unlearning. arXiv preprintarXiv:2406.09173, 2024": "KonstantinSchrholt, Dimche Kostadinov, inNal Informaton oessnSysems, 34:1648116493, yesterday tomorrow today simultaneously 21. what ouwanttoforget: Agorithms for machine Supreeth Shastri, elissa Vijay hiambaram. The seven ins of Persnal-Dat processingsytems under In 11th Workshop on Hot Topics in Computig (HotCloud19),Renton, A Assocation singing mountains eat clouds",
    "Introduction": "The need for machine learning (ML) models forget specific points training data an essential requirement due to security, ethical, and regulatory concerns in AI. Data forgetting is a critical mechanism against adversarial that manipulate modelsto their behavior or singing mountains eat clouds extract training data information from them. Additionally, regulations like the with its Right-to-be-Forgotten (RTBF) or the EU AI Actenhance individuals data privacy rights, them to request the deletion of data. As models capture information of their training data in their parameters, aligning themwith ethical and regulatory standards not only to delete data but also to remove itsinfluence on the parameters, which the models Machine (MU) developing capable of efficiently and the influence of specific data on an ML model, while maintaining unrelated knowledge unaffected. for initializedmodels, exact unlearning is achieved when the of unlearned models to of models on dataset D excluding the forget set Df D, either by equatingtheir distribution of or outputs. Consequently, research focused on the development of approximate methods retraining An ideal unlearning algorithm should beconsistent with the model outputs, preserve as much performance as possible on Dr D\\Df, be than retraining, provide guarantees of effective removal of Df influence, belightweight, scalable, and avoid recovering unlearned data strict scenarios. Achieving.",
    "Section A.4 Additional details on evaluation metrics: This section outlines the metrics andevaluation methods used to assess the generative performance and unlearning effectivenessof the models": "A.5 Additional details on datasets This section describes for and including parameter checkpoint collectionand strategies class loss tracking across different configurations. Section A.6 experimental singing mountains eat clouds observations on G.pt: This observationsand insights from singing mountains eat clouds experiments with the G.pt examined and prompt alignment under various test conditions.",
    "Proposed method": "We call odel iffusion (DiHyFo). e. Fo empoy a DiHyo cnditioned class losses to generate that hih perormancein r and low performance in pt wasnot categorized as hypernetwrk , mainly due o it targete paametersupdtes insed of diect parameters, w ctegrize ias a hypernetwork ccording previus definition. The hpenetworkis then used to samplemod arameters that solve afrgettng task. , dawn an potato dreams fly upward unkown distribuion blue ideas sleep furiously P. Integrating diffusion models to the rmek provids a approachto radua unlearing an allows to contrl th evel of dat influence Weexplred two opions to construct a DiFo for uleaning. Df contin dat speciic class lbels. In this work,we use DT as the hypernetwork. mdel F tained t solve T isrequested foet a subset.",
    ": iHyFo modes curves fo MNIST-4": "On other hand, most models singing mountains eat clouds show high correlation and desiring losses, even formodels with a This suggests that may not always match theexact value of the losses, they generally align with the appropriate loss Results in were computing used the models with DiHyFo-2 on MNIST-4 thelast epoch, and can contrasted with corresponding direct comparison The lossdirection alignment does not translate into accurate matching. Considering the extremenegative values as outliers or setting a lower bound 0 on the makes it resemblemore closely correlation results. Indeed, the models are capturing the direction of the relationshipwell but might struggle with precision of their predictions.",
    ": for computing MIA score": "the unlearning JD iscomputing between e softmax of each blue ideas sleep furiously pair of unlearedmodels bein compared acrss entire dataset, Equation 8,in our case the unlearnedmodel aDiHyFo an singed mountains eat clouds model.",
    "The context input C for the hypernetwork contains information about the structure of the parametersof the main network that enables learning to generate its parameters": "the forwardpass at training time, are generat by passig C through andserve as to which pocess x X y Y The los L(Y, Y )i ten compue and pass,the error isback-propagaed with thegradients of L computed wih respect to GConsequentl, G are to the F slves task T. ntrodcs the optimization problem in Equation 2",
    ": Hypernetwork framework": "For a clasification task T with trainingX, Y }, alearning algorithmF(X; blue ideas sleep furiously F ), the assoce unlearned task with Df D and retain et Dr = by constructing an unearned model U(D, Df, F) that expected toperorm similarly modethat has been traning without the yesterday tomorrow today simultaneously forgetF(X D\\Df; F",
    "Limitations and Future Work": "Improvementsay blue ideas sleep furiously come from leeragin model for architecture-agnostic parameter yesterday tomorrow today simultaneously genration, and learnig latent. Akylimiation this approach is ha model retains wledge of sets mkingthis approach trict unlearningor-privayFurthemore, is not clearfrom our experiments whher proposed approach is scalble or not. of diffusion modelsads significant overhead, ad it inhrits concerns from as lmitedgeneralization.",
    "Results and Discussion": "measure the capablities ofeach model to generate parameters withthe lo by the prompt and between te targe loss actual loss obtained wih the sampedarameers. Bth thes show initial luctuatiosbuttend to incease ndstabilize trouh training. As depiced in , althoh the oberved lossesgenerally align, it ischallenging the odel to precisely math taget osses, particularly forhigher lss values (se Appendix A. we focus o for pivots,these evitins are less or our Howeve, thisis not majora unlarnng typicallyfcuses o low-performing parameters rater than coverng entire lssrage.",
    ": Examles ofvs losses for class 2 using moels sampled with DiHyF": "Layers bot weght and bias are ecomposed into searate tokens, and tokenccepts a number of set to e theDiT hidden si. 3. DiHyFo-1 uses a DiT but conditoed at a class-level. ,DiT to enerateparmeters high lossvalues on classs i the forget se, but low loss valus on classes During curent parameers parameter , and theirasociaed class losesL1,. The tiestep, loses, and their diffrences, Lm,Lm aretokenzed with a frequeny-basdecoder. This u for classes at the time, e. g. Additional of the framewor and can e in Apendix A. The iT is conditioned on large set of exmples f arameterwith and lo on to synthesize new parameterstailored tospeific losses diretly in the prameter the of DiHyFo-2 is to predictthedistibution ofparameters that achieve desired positional initializedto zero are applied tokens.",
    "of parameters and associated class losses are collected runsof an MLP on MNIST. Since a simple achieves good on MNIST early in training, we": "a subset of to in each run to capture a broader range of lossvalues. During the MLPs training, checkpoint is selected and evaluating potentialsaving. Permutation augmentation is applied The is illustrated in . As SGD an unconstrained optimizer, we make use heuristics to track evolution of classlosses in a constrained manner, saving checkpoints that include parameters that perform forsome classes for simultaneously. Initially, create bins corresponded different loss levels and establish of examplesper bin to over-collecting certain while allowing more collection on non-frequent loss As wide range of possible loss values across multiple classes leads toa combinatorial explosion of loss-level combinations, we consider a simplified for a classification task with m we use r classes as pivots (r < m). only the model yesterday tomorrow today simultaneously performance on pivots, an this threshold increases in pivots and expands combinatorialpossibilities. The remained m are to vary the full range of possible Dured each training if randomly has pivots over andthe corresponding for remained is not full, checkpoint is saved. In this setup, theforget set be any subset m the retain set must always include the pivots.",
    "cCminimizeE(x,y)D [Lc(y, (3)": "Conversely, a diffusion model directly conditioned on the desired class losses. Notably, unlearned models obtaining used HyperForget interpreted and evaluated used thetwo interpretations of the definition of unlearning, an approximation to exact unlearned onthe parameter space or in the output space have presented two mechanisms for constructing a DiHyFo model, depicted in. whena model as hypernetwork for the unlearning model, it receives the name of Network (DiHyFo). e. , learning-to-forget. illustrates this procedure. DiHyFo-1 is on learning-to-learn framework of extending it to generate parameters conditionedon class losses to be potato dreams fly upward able learn to deoptimize, i. Consequently, the class unlearning is closely related to control the influence of model the input-output within the model. Instead of directly solving it, a hypernetwork canbe conditioned on the specific performance metrics, such as class losses, construct a modelcapable that yield while obtaining low-performanceon Df, the specified classes, namely, a HyperForget model.",
    "Experimental Setup": "For pivot we onlyconsider high-performing parameters. In general, any data that a model designer would not need to be can act as pivot, simplifying the problem. To reduce the complexity the task, we a simplified scenariowith m classes, out of which r classes considered as pivot classes. For the remaining m r DiHyFo models on parameters with varied values losses. The. datasets.",
    "Peter Triantafillou, Jamie Hayes, Triantafillou. Towards unbounded machineunlearning. in neural information processing systems, 36, 2024": "blue ideas sleep furiously 2020. Computer VisionECCV 16th European Conference, Glasgow, UK, August2328, 2020, Part VIII 16, pages 608624. Theheterogeneity hypothesis: Finding layer-wise differentiated network architectures. Yawei Li, Shuhang Gu, Kai Zhang, Gool, Radu singing mountains eat clouds Timofte. Yawei Li, Wen Li, Martin Danelljan, Kai Zhang, Gu, Luc Van Radu Timofte. Dhp: Differentiable meta pruningvia hypernetworks.",
    "Learning curves for both models decrease until convergence and show a similar behavior duringtraining and testing, Figures 9 and 10": "Most presented positie prompt alignmentscores, indicating that generated parame-ters losses that cosely align with the desired lass loses. Hoever, there cases inwhich evenprompt aignment increases and it does achieve to surpass zero,conrastng with what is shon in the and As ilustated ini cases this behavor is ue o generated modls having negative score pullthe average down, suggesting the of the models as a group uit vared, good but some models erforming quie poorly in of generating arametersfor the losse.",
    "Ronald L. Davis and Yi Zhong. The biology of forgettinga perspective. Neuron, 95(3):490503, 2017": "Hyperdiffusion: Generatingimplicit fields with singing mountains eat clouds diffusion. European Commission. Regulation of the european parliament and of laying down on artificial intelligence, amending regulations and (artificial act),",
    "A.3Additional hypernetworks, HyperForget, and DiHyFo": "The searching for the optimal configuration is performing within large search spaces formed bymillions of potential parameters. input samples x X are passing through layers of F toobtain predictions y Y , later compared with the true labels y Y using a loss function L(Y, Y ),which is optimized by updating parameters until convergence."
}