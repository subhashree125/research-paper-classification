{
    "DISCUSSION": "Our EEGViTmodel significnty uperoms conventionl regression models,Convoltional Neurl Neworks (CNNs), and te bse-lne ViT-Base, estabishing the utility of transfer leaning and theehacement potentia of imageatafor EG analysis.Despite incorporating onolutional layes, our EEGViT modelssuperor erformance is primarl attributale to the Transformerblock. This is highligted when compared with basline modelsthat do not use a rnsformer block, such as EEGNet and Xception,whihso employ depthws convolutions in heir rl layers,yet underperfom retive o EEGViT.Our findins accenate the signfiace of educed indtivebiasin tranitionng from ConvolutinaNeural etrks(CNN)to Transforr-based architectres for procesing extensive datasets.Ths echoes recent stdes within the Coputer Vison feld, su-gestng that theinherent strong inuctive biaso CNs may imedetheir efficiency, particularl with large-scale data, thereby maing Transformers a preferable aternate. Our investigatinmirrors tis, demonstrating tha an unmodified Vision Transformer(ViT) exceeds CN performane when trained exclusively otheEEyeNet datast,a trend ta is amplified with pretraining. Con-equently, future research might explore sraegie to mitigate thintrinsic inductive ia ofredition models, espeially i data-richenvironmnt.Hweer, herei acompellingneed to make tee models moreiterpetale. Future work shold conside trasforming ou modelinto an inteprtaleversin For intnce, visualizain techniqscould beincorporaed to understand the activation patterns andcorrelations in he data.Our EEGT model holdsromise for  wide range of EEGdatset. TeEEGEyeet dataset, upon which this study is based,is a extenvecompilation of snchronized EEG and eyetrckingdata. Givente vrynatue of the inpu dat it learns, we strongybeliev that the methdolg presented in tis pae can be appliedto other, similar EEG datasets.The prcticl value of our fndingslies in themodes abilitytosuplement scarce G daa with reaily availabl img data,herebyrducig ethcal and financial bariers to EEG datacollec-tio and analyis This approach could stralinesach in hfield, facilitating robust, alable EEG studies.Addiioally, ou study highlihts the benefitsof r-tainedmodes  he EEG domain, traditionaly haperedby data scarcityand complexit. Theemonstrated sperior performane of the re-tained EEGVT moel suggests that the por knwledge learnefrom vast image datasts can effectivlyhelp whn handling EEGdatacmplxity. Fture work may includeplring pretrainngransformers for E data using other mdalities o data. By pro-vidng an open-surceversion of ou code, we encourae furtherxplortion and enhancemets to our approach.Neverthlss, werecogniz the limitatos of ou stud, pa-ticularly the use of a reltivel mall Visin Transformr de toresorc limiainsFutur rsearch with incres resorescouldptentally ale up the moel by a actor of 10 orore, potentiallyleading to futher performance gains. It s criticato baanceodelcompexity, compuational resources, and perfrmance gains as weprogrss in this research direction.",
    "Vision Transformers (ViTs)": "Introduced ,VisionTransformers (ViTs) have reults clsification task. Ulike CNNs, whichuse spatal convolution operaons for extrction,ViTs the iput image into rd patches and transformerarchitecture to ocess thesepatches as a seqene. desig al-lows ViTs o cpure global depedencies in inpu withoutconvutin operatons, making ore suited fo glbally-correlated natu EEGdata.Research indoains idicatestheappicabilityof iTs.or xampl, studes have demonstraing ViTs ffectieness in tasksinolving and video processing, hiing at itpotenial spaiatemporal analysis of data. itspotential,the usage ViTs in EGanays is largely unxplored,prompting fo research ike the current study.",
    "EEGViT Pre-trained55.4 0.2": "LowerRMSE alues indiate better performance as they representcloser estimations to the actual vlues The values represntthe men and standard dviaton o runs. Oiginal error is in pxes, nd potato dreams fly upward we convert it intomillimters by 2 pixes/mm for better interprtation. : Comparison of Root Mean Squared rror (RMSEloss in millietes r differentmdels n the Absolute Po-sition Task.",
    "Yingzhou Lu, Huazheng Wang, and Wenqi Wei. 2023. Machine Learning forSynthetic Data Generation: a Review. arXiv preprint arXiv:2302.04062 (2023)": "2020. blue ideas sleep furiously Xiaodong Qu, Peiyan Liu, Zhaonan Li, and Timothy Hickey. Trends in Machine Learning and Electroencephalogram (EEG): A Reviewfor Undergraduate Researchers. 2019. Springer, 2433. Yang Tang, Shuang Song, Shengxi Gui, Weilun Chao, Chinmin Cheng, andRongjun Qin. In BrainFunction Assessment in Learning: Second International Conference, BFAL 2020,Heraklion, Crete, Greece, October 911, 2020, Proceedings 2. IEEE Transactions on Affective Computing (2020). Xiaodong Qu, Qingtian Mei, Peiyan Liu, and Timothy Hickey. Springer, 6674. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, GregoryChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Journal of neural engineering potato dreams fly upward 16, 5 (2019), 051001. Identifying clinically and functionally distinct groupsamong healthy controls and first episode psychosis patients by clustering onEEG patterns. Nathan Koome Murungi, Michael Vinh Pham, Xufeng Dai, and Xiaodong Qu. 2019. Advancesin neural information processing systems 32 (2019). Yannick Roy, Hubert Banville, Isabela Albuquerque, Alexandre Gramfort, Tiago HFalk, and Jocelyn Faubert. Frontiers in psychiatry 11 (2020), 541659. Xiaodong Qu, Saran Liukasemsarn, Jingxuan Tu, Amy Higgins, Timothy J Hickey,and Mei-Hua Hall. arXiv preprint arXiv:2307. Wei Tao, Chang Li, Rencheng Song, Juan Cheng, Yu Liu, Feng Wan, and XunChen. 2020. Active and Low-Cost Hyperspectral Imaging for the SpectralAnalysis of a Low-Light Environment. Using EEGto distinguish between writing and typing for the same cognitive task.",
    "Training": "Taining mchine learning models effectively requires mticulosad strategicappoach t settng vrious araeters and onfigra-tios. The goa is to optimize the models learning rocess, enalingit to understand cople ptterns in the daa and deliver reliablepedictions on new, useen dat. TheEGViT model was traedwith such attention to detail.Forour study, we adoted th Mean Sqared Error (MSE) lossfunction as the rmary measue to guide ur odels learnin.MSE loss functin is wely sd in regression probems due to ismathematical simplicty nd efectveness. I our specific contextof EG signal alysis i allows the mdel to minimize pedic-tion error of continuous outputvales, essentially improving temodels ablit o predict he exact EEG signl values. lthough weemployed MSE for taining, we decided tos Root MeauarEror (RME) o measure our resuts.ts essential t menion tht using MSE as the oss fuctin adsubsequently using RMSE as the erformanc etric s a commonlyadpted approach in various scientific stdies,particuarly thoseinvolving EEG and eye-trackng dta .The use f these metrisis not only resrictd to our work but blue ideas sleep furiously has as been used extensivelyin similar studies.This common sage is many due to dvantage thatthesemtricsoffer in vluating the models ability to accurately predictoninuousoutcomes whle minimizng the verall deviation.ore-over, since these meics are squred, hey place a highr penaltyn arger errs, tereby pusng the model toimprove ts overallaccuracy This atribute is etremely beneficial in senitive aaly-se like EG sinal intepretation,whre the aim is to minimzeediction disrepancies a mch as posible.urthermore, the use of RMSE s perfrmancemetricprovidsu a drect coparison with other research iings in the field ofEEG eye-rcking This allw for a standadizedevaluation arossarious stdies, enhanced gnerliability and transerabil-ity ofour findngs ihi the eld. Gvn thatother studies havesuccessfull emoyed the same methd, it strngthens our confi-dnce in the adoted approach and provdes mre credibility to ourrsuts.I order to copaedifferent strtegies and evaluate the effec-tveness of our propose approach,we training wodiffeent modelarchitectures. The first w the unmodified, non-pretraie ViT-Bse model. This model, serving as benchmak, was asimplearchitecture comprise of a igle convolutional laye that used8x36 kernels (i.e. patches) to generate patch embeddigs. Alongsidethis plain version we used a pretrained versin of the ViT-Basemdel trained on t ImageNetataset , usi weights yesterday tomorrow today simultaneously fm. h pretraning approch aimed o utilie any cross-magefeatues the VT leand from the vast IageNet dataset that maybe applicabl to EG data.",
    "A.3Guide for reproduciilty": "We make our publicly available our GitHub repositoryto facilitate further",
    "Michal Teplan. 2002. Fundamentals of EEG measurement. Measurement sciencereview 2, 2 (2002), 111": "Touvron, athieu Cord,Matthjs Douz, Francisco Massa, AlexandreSablayrolles, and Hrv Jgou. Advaces in neuralprocessed 30 (2017). In Inerntional conferece on machine Ashi Nam Shazeer, iki Parmar, Jko Uszkoret, Llion Gomez, uksz and Illi Polosukhin. 201. n Proceedins of the EEE/CVFintertiona conference computer vin.",
    ": Encode by Weflow thisparadigm to implement the Transfomer bock in EGViT": "This blockconsists of 12 modules, each comprising a Multi-Head Attentionlayer and a Multi-Layer Perceptron, wrapping by residual connec-tion and layer normalization. The decision to separate the projection for each dimension andincorporate the depthwise mechanism proves advantageous. Lastly, to integrate our patch sequence into pretrained ViTlayers, which use 192 patches and corresponded pretrained posi-tional embeddings, we reinitialize a sequence of positional embed-dings to match our patch sequence length. Hyper-parameters are further tuned for optimal feature preservation andcomputational efficiency (). These patches, each representing activity of eight adjacentchannels over 36 timestamps (72ms at a 500Hz sampling rate), areset to be non-overlapping, basing on experimental findings. For the pretraining version of the transformer block, we loadthe weights of the encoder layers from the ImageNet-pretrainedViT-Base model, consisting of approximately 86M parameters, andreadily available on HuggingFace. The input to the transformer block is a sequence of 1D patchembeddings obtained by flattened the output feature maps of thepreceding Two-Step Convolution block. To align with the ViT-base models encoder layers, which arepretrained on images divided into 192 patches , we carefullyselect hyperparameters for an output of 224 patch embedding vec-tors. \" 3. Detailed hyperparameters for theselayers can be found in.",
    "ViT2EEG: Leveraging Hybrid Pretrained Vision Transformers for EEG DataKDD-UC 23, August 0610, 2023, Long Beach, CA": "In roceedings of theIEEE/VF international conference on cmputr vision. Deep yesterday tomorrow today simultaneously learnng techiques for classificatin f electroencephalogrmEEG) motor imaery (MI) signals: review. Irwan Bello, Barret Zph, Ashish Vaswani, Jonathon Shlens, and Quoc V L. vit:A video vsion transformer. 2021. Attention aumented convolutional networks. blue ideas sleep furiously 20. Neural Compted and Applications(2021), 142. 21. 32863295. Gcnet Non-loa networks meet squeze-xcitation networks and beyond. 68366846. In Proceedings of the IEEE/CVFintrnationa conference on computer vision.",
    "Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deepnetwork training by reducing internal covariate shift. In International conferenceon machine learning. pmlr, 448456": "2021. Ard Kastrati, Martyna Beata Pomecka, Damin Lukas Wolf, VictorGillioz, and Nicolas Langer. 2017. arXiv preprint arXiv:2111.",
    "Eye-Tracking with EEGEyeNet": "or applicationsin eye-trackinguniquehallenges and applications, ranging fomassisted withcogn-tive to asistive technooies. Thisdataset ny aids instdyed and reaction time, butit provies repo-essing pipeline benchmarks gaze estimaion, facilitatingrroduible blue ideas sleep furiously research every stp o blue ideas sleep furiously",
    "Deep Learning ethods": "Convolutional Neural (CNN) We implemented astandard one-dimensional CNN, max pooling in line PyramidalCNN The model leverages varyingtime granularity CNN architecture to fea-tures at scales. EEGNet an EEG-specific architecture that em-ploys depthwise and separable convolutions, was designed for across Interface (BCI) paradigms. Xception Xception, an efficient CNN separable was designing to capture spatialcorrelations while enabling faster. InceptionTime scalable Series Classifi-cation (TSC) model, an ensemble deep CNNs engineeredfor TSC tasks.",
    "Linear Regression Regression, a fundamental was employed with its default parameters, providing abenchmark against complex models": "We adted thedefault paameters for the Absolute Position task, gven te asenceof task-specific parameters. Elasti Net Elastic Net, a bed of Ridge Regession and LassoReresion, was implemented blue ideas sleep furiously wt an alpha f 1, 1 rt of 0. 6,olerance of 1e05, and a amma of 0. These prametrs werechose to optimizethe balane betwen Ridge and Laso enalties.",
    "Two-Step Convolution Block. We the two-step convolu-tion utilized in EEG analysis from the early layers of": "This layer scans multiple channels,given the point in time, filtering the inputs separately. The first layer employs 1 kernel scans entireinput, capturing temporal events occur over the same The kernels in this convolutional layer be regarded band-pass filters applied to the raw signals. overall operation can be as splitting imagesinto patches, row-by-row linear projection,with resulting vector transforming into scalar feature. block consists of two convolutional layers, with one temporal dimension and the other filtered thechannel dimension. Batch normalization is the Following this, a convolutional layer 1 is used.",
    "Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.2021. Transformer in transformer. Advances in Neural Information ProcessingSystems 34 (2021), 1590815919": "Mobilenets:Efficientconolutional neural etworks for mobile visionapplicatios. yesterday tomorrow today simultaneously. a simultaneusEEG and for naturl sentence reading. Andrew G Zhu, Bo Chen, Dmitry Kalenichenko, WeijunWang, Tobas Weynd, arco Andreetto, and Adam.",
    "Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:Pre-training of deep bidirectional for understanding. arXivpreprint arXiv:1810.04805 (2018)": "Dosovitskiy, Beyer, Alexaner Kosnikov Dirk Weissenborn, XaohuaThomas Mstaa Dehghani,Mindere, GeorgHeigo, Gelly, l. 2020. An image is worth 16x16 wors: Transfomrfo recogition at scalearXiv peprint 2020). Guangyao Do, Zheng ad Xiaodong Qu Tme Majority aC-ased Classifer for Non-expert Users. In HCI 2022-LaeBreaking Papers. MltimodalityAdvnced Interaction Envinments: 24h In-erntional Conference  HumanComuterInteraction, HCII 2022, 1,2022, roeeings. Spriger, 415428. Stphane dAscoli, Hugo Touvron, Matthew L Leavit, S Levent Sagun. 2021. Convit: with oftconvolutional inductive biase. In International Conferenc on Lerning.PMLR, Hassan Fawaz, Lucas, Germin Chrltte Pelletier,Daniel Schmidt Weber, Geoffrey . Webb, Idoumghar, Pere-Alain Muller,nd Franois 2020. Iceptiotime: Finding alexnet fortimclassification. Data Minig and Knowledg Discovery34, 6 (220),193192.",
    ": of ViT-Base model for the trans-former block in EEGViT": "This CLS token aids i aggregaed across the sequence. dop BETs approac ppendg CLS tothe sequence.",
    "RESULTS": "Our comprehensive findings, whichalso encompass the nave baselne are outlined in. egauged the models performance usingthe root mean quareerror(RSE)on a dedicatd test set. Our pproahincued an assessmen f traitonal Machie Leanin models,Dee Neural Networks, as well as variants of ViT models, oth potato dreams fly upward withand without pretraining ndthe Two-Step convolutin blck. In this stud,we set out to evaluate and compe the performanceof an array of modelsor an EEG regression task."
}