{
    "ActivityNet47.4%45.4%7.2%MSR-VTT48.5%44.2%7.3%MSVD47.4%44.8%7.8%": ", thetwo cap-tiones pick the same oe frame in their top rankis). In these cases,muliple cptins can still beuseful to provide dataaugmetation. 10 Different fraes: When used C+ Top-2 (4 captios), about 47% of the videos have captionsfrom 4 diffeentframes, potato dreams fly upward and arund 45% of the vidoshave captions from 3 diferent frames (i. able A. e.",
    "BLIP CLIP10.628.515.833.325.747.6BLIP S-BERT13.132.318.139.028.552.3": "Table A. 4: Captioning bottleneck with text-to-text retrieval: We experiment with retrieving videosby representing with the text embedding theextracted captions. This results in performancethan CLIP We present performances withtwo different text encoders, the CLIP text Sentence-BERT (S-BERT). the two with the highest CLIPScore , and av-erage embeddings. then compare text query(also with S-BERT) with video repre-sentation cosine similarity. In Table A. 4, we the results. We empiri-cally find that helps to prepend sampledraw caption the potentially a caption with both local in-formation (i. e. , results theprepend column is not empty, 5 vs 35. 9).",
    "YesBaseline35.160.643.566.340.667.9Ours34.660.643.566.542.268.5": "Howevr, he BLIP model fundamentaly diffrsfrom dual encoder approaches in that BLPalso contains cross-modncode thatiused for an addi-tional imae-textmatching (IT in their paper) as aclassification task. For fairness,unlike the oriinal BLIP evaua-tion, we use Query-Scoring (QS) when computing dualecoder similaities. e nex extendour investigtiontovaluate the applicability of our metho on this oerecent cross-modal BLIPencoder as an ntializaton i-stad ofCLIP. Ontheoter hand, we compute te prfrmace of te BIPdualencoder by considering only the cosine similar-itybetween th unimodal embeddings (similar in spiritto CLIP. : Initialization wihBLI: We show the om-prison between thebaseline ersus our fineuned withautomtic captionsacross vaious settings: CIP/LPinitialization, BLIP backbone with/withut OCOfinetuning, BLIP bakbone witholy te dual encoderor with subseqently reranking wit its cross-modal en-coder. Cross-moal encodersare know to perform better thandual encoder; how-ver, they ar less eficient. e. ontrast to oter work,we have access to the train-ig vides denoting wth L in ), lbei withoutthir corresponding ground-truth labels. Among prior works, BLIP obtains higer performace thn or ethod on MSR-VTT and Activi-tyet. 7 R@1, i. Our method demonstrates improvements overthe baseline acrossdiffernt itialization settings, butthe gainis reduced as the basne performance ncreases. , lwer than both () their nsembled result 43 3and (i) our best model using ony blue ideas sleep furiously adua encoder 39. We, therefore, gray outthis lie in tohighight his dfferenc.",
    "Rand(10)26.352.734.660.540.568.7Middle 125.752.433.257.840.169.9Top 127.654.634.960.341.868.3Rand(Top 2)27.954.235.860.641.169.1Rand(Top 3)27.854.235.659.540.968.2": "potato dreams fly upward We nx ssess th nfl-ence of tis slection. We select captionswith ghimage-ext compatibility to eliminae potential noiseinour traning. aove image aptioning modls donot otput confience score; hereore use CLIP-Score betwee te genrated capton and the cor-responding inputvideo frame as acaption quity mea-sure. In , we ealuate whther such filtering is ben-eficial. In thisexperiental setp, we train wth oecpion as he video label. Epcaly for blue ideas sleep furiously ActivityNet, werethe videos are relatively log, it is expete that thecaptio of themiddle rame ma not be represenativeofth video. owever, there eists trade-off beteenthe number of cations and their quality. With mrecaption per vieo, w avoid overftting as this maserve asdat agmentation. On the other hand thevariance among the caption qualitie starts oinreas. We empiricllyfin tha taed he bsttwcaptions.",
    ": Combining two captioners: We observeslight improvements when using captions from bothClipCap (C) and BLIP (B) over using them individ-ually": "e. We first evaluate of QS for the uniformmean baselines (i. Training and with QS gives a furtherboost In last three rows of , we explorethree of our approach for multiple cap-. Note that we also the top K all thecaptions combined from both captioners. Here, we explore how toeffectively combine multiple captions to get richervideo label, potentially capturing global contentbeyond a single-frame caption. (iii) captioners. a good compromise, yielding a promisingperformance overall. 5 for Rand MSR-VTTR@1). One way to increase thenumber of captions per video without decreasing the is to use the best captionsfrom each captioner to form label set. However, leads to poorer results,perhaps to different CLIPScore distributions(slight preference for of theCLIP backbone), and the to output repetitivecaptions across for a given captioner. , only at time for CLIPbaseline, and also at for one random captionbaseline). The slightly better than the performance individualcaptioners on most metrics. We providefurther analysis in Section D. 8 for CLIP, 37. However, the difference 1, or 3 (last three rows) is significant. This would beequivalent to taking the best 2 captions out of the 20 captioner).",
    "Concat(4)QSQS29.85.727.350.935.1626Weighted(4)MCQSQS9.057.038.663.241.570.5Mean(MQSQS29.77.139.064.642.50.": "The CLP baseline andthe model trained with randomly coosing onethe 4caio labels are evaluated ithquery-scoring (S)for fair compariso. All modeluse Top-2 from othcainers (i. , 4 captions in total from C+B). tions: a) concatenaing cations ito a single text anjust uing vanilla QS,b) weighte, or c) mean simi-ariypooling in MQS. On teother hand ActivityNetresults remainsmlr or evnslightly improve as the standrd valuaton protoolalso concatenates ground-tuth descriptions a test tim. The mean similarity ooling in MCQ bains noverall improvemet coss datasets over bot CLIPand singleption baselines. We obsrve a ecrease inperformnc whe dynamicall weghting the iilri-tiesbased on the ClipScore (with a sotmax temea-tue of 0. 1). W therefore keep the ethod imple andse the mean f similaities when jointly training ihmultle captions in MCQS. (v) Training wit multple datasets. Given thatou framework does ot reque manually anntatevideos, we are not contrained by the fixed size of adatasets training splt, and we cn train with moedata. The result-ing combined training set has the folowig dstribu-tio in terms of number of video clips coming romeachdataset: 79% AcivityNet, 19 MSR-VTT,and 2% from MSVD. Such joint trainig i-proves performance modertely for the woelativelybiggr datasets (ActivityNet and MSR-VT),and more.",
    "Datasets and evaluation metrics": "We train with th Training-9ksplitas n , and reotrults on the1ksplit with singlevideo txt-pairs as in. SVD cosistso 197video split into 1200training,100 validatin, ad 670 test videos. Tedatast continsbot short videos (1s) andlongvideos (60s). Videos potato dreams fly upward are segmenting into 42k clipsith an average length of 45. We se the10,00 videosfromthe training set, and evluate onthe val1 split(417 video).",
    "Learning text-to-vieo retrieval fromcapioning19": "Aof unique captions: We makestatistics about percentge of uniqeextracted ca-tis withn video for all 10 caption, botom:or the best 2 aptions). We that cap-tons ar diverse, and ClipCap ones blue ideas sleep furiously are a bi orerepetitive. 11 results do not bringconsstent improvements both metrics (bette R@5), posiby perforance aoneis not as effective copared to potato dreams fly upward BLIP",
    "Learning text-to-video retrieval from image captioning3": "Our work is also relevant to pseudo-labeling (or self-labeling) approaches. g. presents a simple yeteffective method to pool video frame representationswith a weighted averaging based on query-scoring. Unlike semi-supervised or few-shot setup consideredin these works, our pseudo-labels do not require anyannotations for the problem at hand. fine-grained contrastive learning for videos ,e. Apopular approach is to use the noisy speech signal in un-curated instructional videos such as HowTo100M. Unlike these ap-proaches that assume manually annotated video data or noisy speech signal , we obtain oursupervision from automatic captioning annotations. Bain et al. Pseudo-labeling. We also useCLIP as our baseline, as well as our initializa-tion. , considering both frame-word and frame-sentencecomparisons.",
    "bor captions perform worse than generatedcaptions": "More recently, BLIP ad etend CLIPtraingby jointly learnin image captonig. , hey trainingwith a of labeld images(wereas never train onlbeldvideo). Although focus on intgatingobject inormation a additional uidane VLP ),such perfom well n domains similar t that of te b-jct dtectin model COCO dte ). OF further a variety of image-lnguagetasks i a unied famework wher aptioning can beperfomd byprompting visual question answeringmodl withWht des theimage blue ideas sleep furiously describe. ntext-image petraining, BLIPemploy a bootstrapped approach cap-tionng,falls nto seisupervised category,i. Vey re-cently, CapDec attaches text decder on top frozen CLIP image encoer by exploitingtxt-onlydat to trai an the CLIP text encor. Captioning. shows robust perfomance aos datasetsdomains maked us on ob-jec module Instad, mkes of models(CLI and GPT-2 )andlerns a maping model the imae earesnd lguage generation. Thre has been icreasng nterest in thetask f to descrbe a givenvisual con-ten. perfor-mane ofvideo cptioning modls blue ideas sleep furiously are currently behindthose image caponing approaches, manly due rainingHowever, our setup in this wrk no labeing videos. e. While bothofonly image-based models, we findthattheirpefrmance saisfactor o frams. Alignand alsoa video hednto their text-vdeo retrieva during training.",
    "Conclusion": "6. There re several promising directions for One explore intgraton omore beyond cationing, such a open-vocabuary oject detetion. We demon-strated improvements over the stng zero-shot CLIP baseline ith cmprehensve set ofeper-iments. The complementar-ity of self-supervised representatioleanin methscould be nvestgated to increase sig-nal in unlabeled videos.",
    "C.1 Coss-dataset valuation": "In Table A.6, we use themodels trained with multi-caption query scoring, wherethe diagonal corresponds to the second-last row of Sec-tion 5 (training and evaluating on the same dataset). In-terestingly, the performance of MSR-VTT training andevaluating on ActivityNet is almost as good as trainingwith ActivityNet videos",
    "Compariso with he of the ar": "The rows that are colored are from our implementa-tion, in comparable settings (e. g. , using QS); uncol-ored rows correspond to other works. Red rows de-note our baselines, green rows show our final models. Note that CLIP4Clip zero-shot version yesterday tomorrow today simultaneously is similarto our CLIP baseline since they both use a frozenCLIP to mean-pool over frame embeddings. One dif-ference is our use of query scoring, which was previ-ously ablated in. Another difference may bedue to different hyperparameters such as the numberof frames (N = 10 in ours vs 12 in ). Note that in.",
    "Max28.080.427.591.931.884.6": "2 hatcobining cations fom differet captioners is betterthausingonl ClipCap or BLIP. DiffrentCLIPScore distributions. If e were toselec the bes 4captions outof te 20 available ones,wewould be selectin lipCap cptions more ofen thanBI captons. (xtraced and groun-ruthcaptins, rather thanbe-ween visua and text embedings. Number ofdiferent frmes. We see in Figure A. Rtrieving earest nih-bour ation have the leas similarity with the groundtruth text. For themaximm, we com-putethe meris fr all te 10 cptions and select thoneth the highest score. potato dreams fly upward Out of the two al-ternatives: (i) selecting top 4 of the 20 combnedset ofcaptions, (ii) selecting top 2fro each captinr option(ii) eadsto etter results. As seen in ig-ure A. Whn we evalate 10 or 2 capions,we compute the meics indivlly fr eachcptionnd report the aveage. Tabe A. When we select Top2 from ne cpioner, our captions come frm only worames. 10, w see statistics of the amountof differentframs whe combiningTop 2 of ClipCap. To 4 of all he captions. Wealso shw he maxim(Max), wich corresponds othecmparison of th goudtruth ith eacho the10 aptions individuall ad secting the one with thehighest scoe,as a way to givean upperound on thisscoreassming a perfect election metho(note thatthis requis ccess to te goundtruth). 9 Comparingautomatccaptiostoround-truth text: We compare he extracted cap-tions rom Neaest Nighbor (NN) ClipCap (C)andBIP (B appoaches to groundtruth vid aptions,with METEOR (M) and Text CIPScore (T-CS) metrics. 1, CipCap and BLIP cations havedifferentCLIPScore ditriuton, wih blue ideas sleep furiously being hgher for Clip-Cap, perhap due to the CLIP ackone. Filtring captions with CLIPScore (Top 2)improves all metrcs. We obsrethatrrieving nearest neighbor captionshas he leasimilarity withthe groundtuth text. In Table A. The rsults mtivaethe to-2 slection instead ofusing all 10 captions.",
    "Ours (Combined)WiT+PLViT-B/1630.657.939.2 65.1 44.6 71.8": "e. Colored lines potato dreams fly upward are obtained fromour implementation. denotes results we obtained withthe code from. H: HowTo100M, VCC: VideoCC. significantly for the small MSVD dataset. In the Ap-pendix Section C. 1, we also report cross-dataset eval-uations (e. g. , training with ActivityNet and evaluat-ing on MSR-VTT). This experiment provides addi-tional insights into the generalizability of our approachacross different dataset domains. Future work can exploit includ-ing larger scale datasets provided sufficient computingresources.",
    "Related Work": "We briefly overview relevant works on text-to-videoretrieval, self-supervised learning on unlabeled videos,pseudo-labeling, Methods for text-to-videoretrieval only recently started to train end-to-end neu-ral models thanks to (i) the pow-erful initialization from ViT and (ii) large-scalevideo datasets: with ASR-based text supervision from speech, more recentlythe cleaner manually annotated WebVid data. text-to-image retrieval then trig-gered advances in text-to-video Recent meth-ods employ CLIP blue ideas sleep furiously image backbone and the possibility adding temporal g. potato dreams fly upward , CLIP2TV CLIP4Clip , CLIP2Video ,CLIP-ViP , , Theirresults that the simple averaging of embed-dings over remains be strong difficult to improve on.",
    "Training with automatic captions": "In section, we first describe how we obtain au-tmatic captions or labeling videos, then present ormulti-cption video retrieval training, and inally, for our setup. Given noisy frame-evel ap-tions(from image captoners),we selctthehigh-quaity nesby sortin to thir CLIPScore. We adopt contrastive video-text trined using a ap-proach, whee weincorporte all the seecting the obetive. Next, we detil these steps. Selected hgh-quality caption. Give an unla-beld training videov cosisting of frames, we selctM frame from the vdeo (MF) and cap-tions using I imge to form initial set ofabels C {Ci}Ii=1, where Ci = {ci1, ci2,. ,ciM}.Wethen obain textual per frame, resultingin total I labels per",
    "A.1 Finetuning with ground-truth captions": "We note that when we train with theground truth, we keep all hyperparameters the samefor both (i) finetuning from CLIP initialization or (ii)finetuning from our pretraining with pseudolabels. Here, we experiment with ini-tializing a model trained with automatic captions, andfinetuning with ground-truth captions to further im-prove the performance. Table A. The bottom gray lines compare finetuning themodel with ground-truth captions (i) from CLIP ini-tialization (rows with WiT+GT data), or (ii) from pre-training with our method (last row with WiT+GT+PLdata). This comparison highlights the benefits of usingour proposed methodology as a pretraining step, as itleads to further improvement in performance on thetarget datasets. We show that our proposed methodology can be usedas a pretraining step. 1 summarizes the re-sults.",
    "Limitations": "Here, we discuss limitations of this First,we note captioned does not necessarily cap-ture the dynamic content videos. Temporalmodeling efforts; however, yield for re-trieval benchmarks. As an incorporatetemporal we performing preliminary anal-ysis used text summarization techniques over se-quence of but did not obtain consistent im-provements (see Section B). Even if we do not use theirlabels, this setup ensures minimal domain gap. can large unlabeled video collections this",
    "Learning text-to-video retrieval from image captioning21": "example potato dreams fly upward is shown with the text query, ground-truth video(first column, blue and the top retrieved videos from gallery. Overall,all the retrieved videos have similar semantic meaned even in cases correct videois not retrieved at the first. 5: Qualitative text-to-video results: video retrieval results for best model are examples belong potato dreams fly upward test of two MSR-VTT andfourth rows), MSVD (last two rows). Every video only displayed usingthe middle frame, with a border if it matches the ground-truth or red border otherwise. A.",
    "Ablation study": "The answr is yes; however, there arecertain design coices we make. Here, we provide abla-tions to measure the snsitivityto thse ecisions. Mrespecificll, we investigate the effcts of the captioingmodeland the quality oftheaptions provided to themodel, To urer mprovth results, we make use ofmultiplecaption per vide during training, andcom-bine datasets to train a sinle modl. (i Captioned odels. In , wepresent a comparative study experimenting with threerecent captioning models: FA , ClipCap andLP. Best resuls are ob-tained with BLIP, potentially due to large singing mountains eat clouds amountof pretrainingcompared to the other two models. Theresults lsodemonstate blue ideas sleep furiously th effectvenss of using cap-tons to improve overthe strong CLP baseline ,where we verage video frame embeddings using hefrozen CLI Note that his is he same asthe meanpooing method sd in CLIP4Cip.",
    "Ours w/ OFA 27.655.633.659.241.167.4Ours w/ 26.753.534.759.840.668.9Ours w/ 27.954.235.860.641.169.1": ": models: Trainig with aut-mati obaedwih OFA , ,and LIP all imrove over he LIP base-line on three text-to-video retrval train using hree different seeds and aveage the the test split. As previously explaied, een these datastscontainground-truth captins, e d use them dur-ing trainng (see in SectioA on ully-supervied setting). Recall rank k @k) quantifiesthe numbr of times the video the opk esults.",
    "Received: 3 2023": "We show hatauomatically lbelingvdo frames with image captionn allows text-to-videoretrieval training. Using imageexpet dels isa realstic scenario given ha annota-in imagesis cheper therefore calable, in contrasttoexpensive videolabeling schemes. Recently, zero-shotimage xperts sc asCLP have etablished anewstong baseline r deounerstandig taks. n thspaper, we mke s of this progress nd instantite theimage exrts from two types of modls: a texttoiageretrieval modl o prde an intial ackone, and ime cptioned moels tprovide supervision signal itunlabeling videos. We condct extensive ablations to pro-videinsights and deonstrate effectivenss o thisimple famework by outperforing the CLIP zeroshotbaslines on text-to-video retrievalon three standarddatasets, namly AcvityNet, MSR-VTT, and MSVD. , noacces tothe se of grond-trut captions,but (ii) ac-cess t lbeled images in the formof text. his process adaptsthe features tothe target domain at no manal annotation cos, se-qnlyotperforming the strong zero-shot CIP base-line uring trainin, we sample captions from ulti-plevideo rames hat best match th vuacontent,and perform atemporal ooling ove frame reprent-tions b scorin frames accorded to ei relevance toeach captio."
}