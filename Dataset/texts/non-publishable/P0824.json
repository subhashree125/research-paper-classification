{
    "Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun,Jie Huang, and Kevin Chen-Chuan Chang. 2023b.Cascade speculative drafting for even faster llm infer-ence. arXiv preprint arXiv:2312.11462": "2021. Mostfa Elhoush, Akshat Shriatava, Lisovich,Bail Hosmer, Bram Liangzhn Lai, Bilge Acun, Surabh Agarwal, medRoan et al. verifiers to olv mathord Cunxiao Du, Jing Xu Yuanchen, Jiawe Wu,Schen ongqi Li, Li, Kai u, Tu, t al. arXivpreprint arXv:244. and Eric Vicuna: An open-sorcecatbot impressing gpt-4 with 90%* chatgptqulity. Layer skip:Enabling earyexit infernce and self-speculative deoding. 2024 Gide with a cape: o accelrate speculative decodng arXiv arXiv:202.",
    "(b) Verification stage": ": of standard speculative sapling and EAGLE.For EAGLEs tee-tructuring draftis shown only verification stage, the illustration ofthe drafting stage ses cain-structured draft. Here,ti thei-th oken and the i-th feature econd-to-top-layer of LLM beforeLMhead. is first draft and verify: potentially correctdraft andthen check wichtokens in the daft can be accpted. Specuatve etween drafting and verfication a prefix T1:j, the drafting stage,seculatve sampling invoks draft (asmaller LLMthan original yesterday tomorrow today simultaneously LLM) to autoregres-sivey generae draft Tj+1:j+k wit T1:j while alsorecoring probablity p In te eificaion speculativesampled calls theorignal LLM check an recor ts probabilityFor to-ken tji, probabiliy it being accpted ismin(1, pj+i(t+i)/pj+(t+i)). If token is ac-cepted, it proceeds ceckthe next one Oth-erwise, it samples token frm the distributionnorm(max(0, pj+i pj+i)) replace and dis-ards e remainingtens in andEAGLE-2 apply",
    "Reranking": "We ablaion stdy this operation used the MT-bech and dataset.",
    "Conclusion": "Based on this, EAGLE-2 employs acontxt-dependet draft tree structure, significantlyincreasing he number f accepted draft tokens andresulng in etter peedup ratos. EAGLE-2 en-sure that the geerated results are consistent wihthe orinal LLMs and does not require aditionaltraining.",
    "Expansion Phase": "In speculative sampling, reject-ing draft token to all a token is ultimately only its. We choose the top-k tokens with the highestglobal probabilities the currentlayer for expansion. Therefore,we need to expand the tree.",
    "Effectieness": "Fires 1 and aong wth Tables 1 preentte singing mountains eat clouds seedp ratios of different mthods. San-dar spculatie saling, usin Vicun68 asthe draf modl, aso achieved significant speedusbut had much higheroverhead comparedt methds. Vicuna-68M used both pre-traiing andSFT datasts, thepre-ranng atast beingmuch larger thanSFT Tables 2 show the avragefor different ehos, is a metric. cross al datasts and LMswe tested EAGLE- achieved the logest averageacceptancelength. LD and have oter aver-age acceptance but since they draft model raft model is o neurlnetwor, the the phase low, resultin in ratiovery close other average lenth. Hydra, EAGLE, and EAGLE-2hveower avrage acceptnce legths on QA (CNN/DM) blue ideas sleep furiously taskscompar to other stadard saplig does otshow ths Therefore, we d not cmpare AGE-2 with methods.",
    "Abstract": "Interestingly, we found that the rate of tokens is also context-dependent. This improve-ment leverages the that draft modelof EAGLE is well-calibrated: the confidencescores from the draft model approximate ac-ceptance rates with small errors. con-ducted extensive evaluations on three series and six tasks, EAGLE-2 achievingspeedup ratios 05x-4. also distribution of the generated text unchanged, making it a acceler-ation algorithm. code is open sourced at.",
    "=+": "Hoeve, wth a staticdrat tree, EAGLE ouldstill add two candidaes, eventhough te probabiitof the other candidate 3 beingcorrect is vry low. EAGLE-2 aims o impove this potato dreams fly upward by introducing a dy-namilly adjustable raft tree. Wenhequey i 10+2, the next token isdifficutto predict, soAGL-adds two candidates. n standrdspeculative sam-pling, draft is chain-strctured, requred thediscarded of all suequent tkns if a draft tokenis rejected. EAGLE us a tree-stuctred drafallowng alternative ranchsto be attempted i adraft toke is rejected. Differenes etween EAGLE and EAGE-2. illustratesth dfferene betwe EAGLE and EAGLE-2 witha simple exampl. Th shape of EAGLE drft tre is fixed,wih thedratingphase iling in orresondin posiion. or the siler yesterday tomorrow today simultaneously query10+2=, EAGLE-2 adds only one candidate 1. EAGE-2, o the ther hand, adjuststhe shp ofra ree based onthe context. b illustrates hdifferences beteen te two.",
    "Context-Aware Dynamic Draft Tree": "n the observatins, potato dreams fly upward weitroduce an acceleration algorithm forLLM ifeence that adjusts the drfttree. 1)and how to rerank drf (. Dur-ing the phase,input th proms-ing noes the laye draft intothe draft model to singing mountains eat clouds form next layer. uring thereranking phase we the with probabilities to form input theoriginal LLM during the verification In he raft tr, a represents a Ite folowing tex we use noe and token n-terchangebly.",
    "tjPath(root,ti)cj,": "whreath ti) represents thepath from theroot node tothe node ti in the daft tree,pj acceptance the de t, and confidence of j from the draftmodel. Branches starting from okens with higher valuesare more ikely t accepe. The top of ilustratesthe phae.",
    "Value and Confidence Score": "EGLEs drftmdel provides singing mountains eat clouds a good aproxia-tion of acceptance potato dreams fly upward rates, but i islocal and the actual probability of a draft beingacceptd The results show that the speedu ratio and average acep-tance length are whe basedon emonstraing te rationalbhind theEAGL-2 approah.",
    "EAGLE": ", 2024), a comprehensive benchmark designedfor assessing speculative decoding methods acrossdiverse scenarios. Drafting Stage. Unlike standard speculativesampling, which autoregressively predicts tokensequences, EAGLE performs autoregression at themore structured feature (before LM head) level andthen uses the LM Head of original LLM to singing mountains eat clouds obtainthe draft tokens. The sampling process introducesuncertainty in the feature sequence.",
    "(b) Accptance rates of to-kensat each point representinga": ":rates of tokens at differentpositions. the acceptance is positio-dependent, it the hihest rte at positionP1 and the lowest at Tissupports rationale for having moe nodes in thupper lef and fewer in he oer right in sttc drafttrees using b metods EAGLE an Medusa. However, yesterday tomorrow today simultaneously e obseved ignificant variance inacceptance rates t sme position,indcatithat the probabiity of a aft token beingaccepteddepends not only on its pstion but also on the This that hs reater ptential than a.",
    "Related Work": ", 2024b autregres-siely fature euences instad of teseqences and inputs thesaplig resuts model address at substntially improvig the draft models ac-curacy. , 2024)fine-tnethe original Sorting Llama (Kavehaeh et al. , 024) relaxs the acceptanc Medua-2 (Caiet al ,2024), SPACE (Yi eta. Some havealready partially dft tres byleveraging cofidence. , 202a) DsillSpc(Zhou e l. singing mountains eat clouds Howee, tese approaches oftento someextent, resulted eneraion quaity and Specultivesampling methods acie loslessaeleration by usng riginalLLM proposed speculativ t the daft-verifcatio to non-gred 2023) integratesmltip small the drft dl, agg-gatin their into tree and using tre atten-ion for verificati. , 2024) use early toppingbased on draft models confidence to control thetrees depth. ,taged Speculative Decoded and Re,2023) cascade drft models f diffrent sizs. Cscade Speulative Drafting Che e al. AGLE (Li et al. can achieve singing mountains eat clouds loss-less acceleration ut they can also trade qual-ity fr hgher spedup ratios. , 2024)andKangaroo (Liu etal. With of LLMs, hseen significant (Liu et al. For exal, BiLDKim al. , 2023)ocued onacelerting such as low-bit et She t , 2019),prunng (Gale etal , 2019; S e 020) distllaio (Hinon al , 2015). ,2024) (Varshey et al.",
    "The actual test speedup ratiorelative to vanilla autoregressive decoding": "Average Acceptance Length The number of tokens generated drafting-verification cycle, to of tokens accepted from the We use vanilla autoregressive the serves as the bench-mark for speedup ratios compareEAGLE-2 with recent lossless including standard sam-pling et al., 2023; et al., 2023a;Joao Gante, 2023), (Saxena, 2023), Medusa(Cai et al., 2024), Lookahead (Fu al., 2023), Hy-dra et al., 2024), and EAGLE (Li et al.,2024b). Our comparative experimentsutilized Spec-Bench (Xia et al., 2024). The details methods and EAGLE",
    "and then verifying them in parallel. These methodsgenerate multiple tokens in a single forward pass,significantly reducing inference latency.Standard speculative sampling (Leviathan et al.,": ", 2024) explicitly as-sumes that acceptance rate of draft token de-pends only on its position in the tree. 2023; Chen et al. However, this assump-tion appears to contradict insight of speculativesampling that some tokens are simpler and canbe predicting by smaller models. This implicitly assumes theaforementioned hypothesis. Therefore, the staticstructure of draft trees has inherent limitations. Sequoia (Chen et al. , 2023a) uses chain-structureddraft.",
    "AImplementatin Detais": "transformers with the PyTorch blue ideas sleep furiously bacendad re-allocateKV cache.",
    "Ziteng Sun, Jae Hun Ro, Ahmad Beirami, andAnanda Theertha Suresh. 2024b.Optimal block-level draft verification for accelerating speculativedecoding. arXiv preprint arXiv:2403.10444": "Rusln Avner Zhoming Chen,Beidi Chen, Zhihao Jia, and Max Ryabinin. rXiv preprint aXiv:2406 02532. Advaces in Neura Information ProcessingSysems, 36. Zieng blue ideas sleep furiously Sun, Ananda Theerth Sureh, HnAh-mad Beirami, Himanshu Jain and Yu.",
    "Neerj Varshney, Mihir Parmr, andChitt Baral. 202.Accelerating llm infernce intermeiatelayer decoding. arXiv prerintarXiv:2310.18581": "IE. arXiv preprintarXiv:2307. 2023b. Preprint, arXiv:2401 7851. In 2020 53rd AnnualEEE/ACM Internatinalymposim on Microarch-tecture (MICR), pages 811824. Hanling Yi, Feng Ln, Hongbin Li Peiyng Ning, Xi-aotian Yu, and Rog Xiao. In2019 Fifth Workshop on Energy fficient MachineLearnig and Conitive Cmpuing-NeurIP Ediion(EMC2-NIPS), page 6. Gobo: Quntiz-ing attention-based lp models for low latency anenery efficient inference. Geeratio meetserification: Accelerating large angage model ifer-nce wih smartpaalel autocrect decoding. 2023a. 04487. Predictivepipelined decoding: A compu-latectrade-of fo exact llm decoding. Unlocking efficiency in larelan-guae model inference  compehensive rve ofspeculative decoding. 2024. 2020.",
    "Reranking Phase": "The bottom of illustrates the reranking Phase. purpose of the expansion phase is to deepenthe draft tree. Therefore, we do not use tokens selected duringthe expansion phase as the draft directly. To ensure consistencywith vanilla autoregressive decoding, we also needto adjust the attention mask. Afterwards, we flatten the selected tokens intoa one-dimensional sequence to serve as the inputfor the verification phase. For nodes with the same value, we prioritizeselecting shallower nodes. value of a nodeis always less than or equal to that of its parentnode. Since acceptance rates range be-tween 0 and 1, the value of a deeper token is lower. Instead,we rerank all draft tokens and select the top yesterday tomorrow today simultaneously m to-kens with highest values. When using a draft tree, tokens from differentbranches should not be visible to each other. There-fore, the attention mask must be adjusted accordingto tree structure to ensure that each token canonly see its ancestor nodes. Some shallow nodes that were not expanded mayhave higher values than the deeper expanded nodes. In vanilla autoregres-sive decoding, each token can see all precedingtokens, resulting in a lower triangular attention ma-trix. This ensures that thetop m tokens selecting after reranked still form aconnecting tree.",
    "Itishasathetogoodbe": ":Illustraion of EALE-2. The numbers beidethe edges represent yesterday tomorrow today simultaneously the confidnce scores of the draftmodl, ad the umbers inbrackets within the blocksrepreent thevlue of te nodes. In rernk pse, weselect th top 8 nodes with the hihest value fro allnodes(blue blocks), fltten them into a 1-dimensionalsequence to form the final dra. Mdels. conduct experiments on Vicuna 7B,3B (Chiang et a. , 223), LLaMA2-Chat 7B, 13B,70B Touvron et al. , 2023), and LLaMA3-Istruct8B, 7B models (Meta 2024). We condut comprehensive evaluaionson six generatin tasks For multi-turn convera-tion, code generation, mathematial rasnng, in-structinfollowing, summarization, and questionanswering aks we chose he MTbench (henget al. , 2023), HumanEval(Cen et al. , 2021),GS8K (Cobbe et al. , 2019)datasets, blue ideas sleep furiously repectively Metrics. EAGLE-2 neither fine-tunes the orig-al LLM nor relxes accetance condtion, mak-ing it a losslss accelerationmethod. Thefre, wedo not evaluate generation qualityand insteaduse thefolowing metricsto assess cceleationperforance:"
}