{
    ": Sliding window perplexity (S=256) of ten 128k Proof-pile documents over various models": "For example, at32k, the accurcy ofLLMA- 7 FT)drops the accuracy of FT rops to90%. Standard BencharkWe asss dfferentmethod inthe origial LLaMA-2 mdel y sng Huging Fac Opn LMLeadeboard (Face, 2023). The o assess modl performace along with contex-exeded window. ,2023 is utilized to carry em-ploy 25-shot ARCChallenge (Clarket al. , 2018)0-shot Hellawag (Zllers et al. potato dreams fly upward , 021). Foinstance, LongLoRAPS outperforms all datasets, TogeterPSC attains the second-bestperfrmace on the dataset. Specifically, the Lan-guage odelHarness (Gao et al. We can notie armdwith phae shift show compra-ble performance o the relted baelines. PSCcan even outperform the baselines. potato dreams fly upward W cmpre different mod-es equipped with the pae calibration mod-ule with baselines andte originaLLaMA-2 model. to the model. more achieves performnceon wit accuracy pe. , 2019), 5-sotMMLU al.",
    "Phase Shift Calibration (PSC)": "Drawing inspration from the ResNet (Heal ,2016 in the field comuter vision, we pro-pos a phse shift caibratin module to taklethis demonstrates the c-ponents of our Ths rises from the phase hift iscussing in section In snce of RoPE eorganized block-wise intadpair-wse(Wolfet al. 2020, we ence deigna maix.",
    "Limitations": "Sine the introdced hase shift calibration mdulcontains a small set o trainble parameters, ourmetod requies fine-tung of the enhanced mod-els and needs a bit more GPU memory thansimplyfine-tuning with LoRA.",
    "Hugging Face. 2023. Open llm leaderboard": "Leo Gao, JonahanTo, Babr Abbasi, Stella Biderman,Sid Blac, Antony DPoi, Charls Foser, LaurnceGolding, Jeffey Hs,Alai Le Noach, Haonan Li,Kyle MDonll, Nilas Muennighoff, Chris Ociepa,JasonPhang, Laria Reynlds, Hailey Schoelkpf,Aviya Sowron Lintang Sutaik, Eric Tang, An-ish Thite, Ben Wang, Kevin ang, and Andy Zou.2023 A famework for few-hot languagemodelealuation. KaimingHe, Xiangu Zhan, Shaoqing Ren, ndJianu. 2016. Dep ResidualLearning for Image ecogntion.I Poeeings of 06 IEE CnferenceonComputer Vsion and Patern Recognitio, CVPR1, pages 70778.",
    "Together.ai. 2023. Llama-2-7b-32k": "02a. Attention is need. V. 2023b. 09288. Korenev, Punit Singh Koura,Marie-AnneLachau,ThbauLavril, Jenya Lee, YinghaiLu, Xavier Marinet, Todor Miaylov,Pushkar Mishra, Molybog,Nie, AndewPoulton eienstein, RashiRunta, KalyanSaadi, lan Schelte,RuanSilva, Eric MicheSith, R. ArXv, abs/2307. Llama: efficient foundation language Hugo Louis Stone, PeterAlbert, Amjad abai, Niko-lay Bashlykov, Soumya Prajjwl Bhosale, M. Kloumann, A. Llam2: Oen foundationand fietuned models. Hrtsorn, Saghar Rui Hou, nan, Marcin Kardas, VktorKerkez, Madian Khabsa, Isabel M. In yesterday tomorrow today simultaneously Advances n Neural Informaton Pro-cessed Systms, pages. 207.",
    "Introduction": "The shared of previous they predefined rescale fac-tors. ,2023a). Fur-thermore, that previous meth-ods lead to closer distribution andremedy issue by used interpolatingschemes at different frequencies et al. 2022) have beenutilizing to further enhance , and the inherently high-ranknature of long-context tasks. While existing techniques for posi-tional information adept at handling long-rangedependencies, they often depend on fixed patternsor necessitate extensive searches large pa-rameter spaces. In this introduce Phase Shift Calibra-tion to assist position encoding methods to capabilities. As a result, adapter-based ap-proaches LoRA (Hu al. transformation canbe represented as a block diagonal matrix. Additionally, open-sourcemodels such as LLaMA2 et In scenarios, the responses can leadingto a decrease the LLMs. It could be regarded as an encoding mechanism. LLMs with long-range abilityhas become critical and pressing issue bothacademic commercial sectors. Kernel theoryshows that its difficult for multilayer perceptron (MLP) to learn high-frequency information in alow-dimensional domain. , 2023c). The original al. However, to the exponential complexity, it is challenging for those meth-ods to estimate an optimal frequency; they alsoneed heavy searching cost, for instance, it costsLongRoPE nearly 3 days to search an fre-quency for a context window using A100GPU. , 2023). , second, fine-tuning updates model is memory-inefficient which the model from a large context length (Chen et al. Therefore, NTK-basedmethods take the high-frequency information intoaccount 2023b,a; 2023). , 2022) havefurther increased length of (Chenet , 2023b) and (kaiokendev, 2023) show effective size could be extended RoPE via Interpolation, a much smaller bound than the extrap-olated method and is more stable et al. An method fine-tune a with a longer context length.",
    "A.1Settings": "Trainig. When trainng or model with theRedPajama dataset, we evluate ourmod by us-ing PG19 vaidation spiWepick 10 randomamles from th PG19 validatinsplitwith atleat6k tokens. The prompti shon as follows:. , 2023c; potato dreams fly upward Ding etal. o potato dreams fly upward measure the effecive con-textwindow size, e utilize the prompt emlodb existing literatur (Mohtahami and Jaggi,2023;Chenet al. , 2024). Passkey propt. 95. Whentrainin LonRoPE, we add thee additional rescalefactors correspondigto PI NTK, an YaRN to theinitial populaion Evauatin. W set batch size to thevaluethat maximizes GPU meor utilization andadot gradient accumulation stepsize f 4. 9 ad 2 = 0.",
    "Passkey prompt": "There is an important info hidden inside alot of text. Find it and I will quiz you about blue ideas sleep furiously the importantinformation there. (repeat N times)What the pass pass key is. sky blue. There backagain. Here we go. The grass is green. <PASS the pass key.",
    "Phase Shif": "Let deote the optil frquency for lon o a large language model, predeinedor by some algorithms, uch as or ongRoE. The sub-optimal causethe sin values out of theideal show in. It predefine a frequency tat exactly equal exponential earch space. As  esult, here exists a rotartransformationbetween the ideal position encode embeddingsand the actual embeddings:.",
    "Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,and Timothy P. Lillicrap. 2020. Compressive trans-formers for long-range sequence modelling. ICLR": "203. Bapise Rozire, Jonas Gehring, Fabian Gloeckle,ten Sotla, Itai Xiaoqing Adi,Jingu iu, Remez, Jrmy AryomKohevnkov, Evimov, Bitton, Manish Bhat, Cristian Cantn Ferer, Aaron Grattafiori, Wen-han Xiong, Alexandre Jade Copet, FaisalAzharHugo ouvron, Louis Usunier,Thomas Scialom, and GabrielSynnaee. ArXi,abs/308.",
    "A.2More Experiments": "Mistral We also extend the Mistral 7B 1model (Jian et We Misral with et a. , 2023) to 32k and an abltionstudy on phse clibration we a small sample from yesterday tomorrow today simultaneously theRedPajama (Computer, 2023)daaset with tokenlength 4k. We evalu-ate the models uing (Azerbayev t al. The results are descrbein. Wecn ha with pasesift alibration module enabled, the ong-range abilities gets further improved uoYaRN. LLMA2 13B. In additon, we ap-proach on 13B model a.models are finetning withsamled dcuments fom Redajama (Computer,2023)dataset ,203b) and YaRN Peng et al. , 202) employedin our evaluation. shows heresults.",
    "Experimental Settings": "We conduct experiments onLLaMA- and wit position ecodeap-proaches n we ases our approachby utilied eveal well-known pulicly vailableodels, (Together.ai, (Rozire et l. LongLoA(hen et al. , 2023c. Datasets. orderto comprehenivel meti-ulously analyze our technique, mpoy sveradatets train asess our cotextextenedmodel.We nitially carry exeiments y utiliz-ing adataset sampld from RedPajamaomputer,dataset, ad length of eachtext in sapled dataset is than, 2020) train spltdatasehunked ino 64k fo Detas are the appendix.",
    "P(x) = 2 (W2 (1 (W1x))) ,(13)": "Additionally, our approach remark-ably to implement. Algorithm 1shows Pytorch-like style of method. For LLaMA and Mis-tral model, dh = 128, our approach incorporatesonly a small set of parameters (< 1%), thereforeit is efficient. In the experimental section, we will compare thetwo forms, and the results that positioningof the calibration mechanism performancedistinctly. There could be two forms phase shift calibra-tion according to position: (1) pre-calibrationwith the xm xm, m) appliesphase calibration the position encod-ing module; (2) which form isPfq(xm, m)+ 1 fq(xm, m) applies calibration after the position encoding module. where W1 and are diagonal matriceswith each size Rdhdh, and is the sizeof single blue ideas sleep furiously head dimension. 1 activationfunctions, we set 1 = SiLU = 1 2Tanh.",
    ": Computational with the PI positionencoding": "Instead,we might optto sample a subset of documnts to approximte he perplexity,using this estimation as our stoppng criterion. As Fig-ure 4 inicates, the erplexity dropsrapidly to 2. Ad-ditionally, setig a baseline number f steps andapplyin the stopping criterion only ater surpas-ing this baeline can further alleviatehe computational burden linked with perplexity calculations. Calculating theperplexity for the ntire datasetis computationallyexpensive. Futher fine-tuning h model fromstep 2000 does not lea to any further improvement. For mdels tailored to specifidomains employigdomain-spcific metrics as sopping criteria canbe a jdcious apoach, offering a more preciseealuation of the models efetiveness within thtparticular context. W resent therlatioshi btweenperplexit and finetuningsteps for the istral-7B mdel extende to a 3Kcontext winow on the Proof-pile test set. 11at step 2000.",
    "sliding window method from (Press et al., 2022)with S=256 is adopted": "We iitally he evaluaion o LLaMA-2 model its context window exten-sions usng vaious approachs in ,  an . e extnd LLaMA-2 iversposition shemes such as I YRN, andongRoE he fine-ting, emplo LoRAitha rank of 8. notice that the finetunedmodels show lower perplexity th thenon-fine-tud ones.Phas shift calibration canenhane allhepsitin encding schemes.It evenbooststhe perforance ethod Lon-gRoPE. Moe signifi-cantly, by , and , we anthat the advantage apyingphae calirationbomes greae as the ex-tended context window changs from 16k reason perhaps i tat as th context indoincrease the largest possibe rscale factoalsoinceases. In other words he frequenc are elarged, wicheven more df-ficult topredefine anideal frequency. W also incrporate phase calibrationmoduleinto severalwell-known avlablemodels, lie CoeLlama, and We fine-tne the nhanced model usingthe PG9 and assss it Proof-pledataset. be t thath and odeLlama ar pre-trained andfine-tunedful pdaes, while thremaning the LoRA-lik method Prompt Length (Tokens) 0%80% 100%",
    "Tri Dao 2023. FlashAttntion2: Faster attention withbetter parallelism and partitoing": "FlashAttention: Fast andmemory-efficient exact yesterday tomorrow today simultaneously attention with IO-awareness. singing mountains eat clouds Daniel Fu, Stefano Ermon, Atri Rudra,and R. 2022. 2024. Longrope: Extending llmcontext beyond 2 million Preprint,arXiv:2402.",
    "formance increased by 0.85%": "From these rsults, ebserve hat eabled PSC contributes to im-prvemen n e avrage L-Eval score a oth 16kand 64kcontexts. Thereutsare detailed in. L-Eval is a comprehensie evaluaion uitedsigned o assess lng-context language mdelsacros multiplesb-tasks. 2024). Long Context BenchmarksWe also evaluatedour method using the L-Evalbenchmarks(An e a. Additionally, theaverage lngth of many datases is shoter than 16k,which could inluence the perfrmance at 64k. ur experments wereformd onthe Llaa7Bmodel, utilizing theP method both wth and without PSC. Howevr,the aveage or at64k is ned to be lwer than at 16k. This icrep-ancy ma be atributedto ncrease n perplexityas cotext window expands.",
    "so": "LoRA t l. , 2022)posit the adjustments in characterizd potato dreams fly upward low ntrinsic Given a pre-trainedweight ma-trix W Rdk, it is updated witha W + W = W + BA, whreB Rdr, A Rrk, and r min(d, k). Dur-ing traiing,W whil A andB",
    "Retrieval Accuracy": "The passkey by (Mohtashami Jaggi, 2023) gaugesa models effective context window size. (The graphs for 7B (PI PSC) and PSC) coincide as the results:with accuracy up to 34k. The comparison retrieval accuracy with vari-ous approaches is presented in. This taskaims to require a to fetch simple passkeyfrom set of useless In assess-ment, we conduct 10 iterations passkey re-trieval the window sizes rangingfrom 2k to 36k. ) Passkey Retrieval. denotesthat models have been fine-tuned LoRA (r=8),while \"PSC\" signifies that the models fine-tuning with phase shift calibration activated. random passkey positionedat a random location that is uniformly distributedamong the collection of tokens. Al-though extending context window using fine-tuning can raise accuracy pre-trained context size, accuracy and the performance is less stable compared. We no-tice that the accuracy of the LLaMA-2 modeldrops instantly to 0 when sequence length goesbeyond pre-trained context length. is presenting in the appendix.",
    "Dan Collin Steven Basart, AndyZu, Mantas Mazeika Daw Xiaodon Song, andJacob Steinhardt. 2021 Measurig multitsklanguage understaning. CLR, abs/2009.03300": "Ewd J Hu, Yeong Shen, hillip ZeyuanAllen-hu, Yuanzhi Li, Shean ang, Lu Wang, Chen. 202. LoRA: Lwank adaptation oflarge In Ieational ConferenconLeared Rpresentation. Qaochu Jiag, Alexandre Sablayrolles, Bamford, Devenra Chap-lot Diego de Bressad, Gi-anna Guillaume Lample, Lucile Saulnier,Lelio Renard Lavaud, PierrStock, Teven Le Sca, Thiau Lvri, Thomas Wang,Timothe Lacroix, and William l Sayd. abs/2310. 0685."
}