{
    "Swaroop Mishra, Daniel Khashabi, Chitta Baral, andHannaneh Hajishirzi. 2021. Cross-task generaliza-tion via natural language crowdsourcing instructions.arXiv preprint arXiv:2104.08773": "Arindam Mitra, Luciano Del Corro, Shweti Mahajan,Andres Codas, Clarisse Simoes, Sahaj Agrawal, XuxiChen, Anastasia Razdaibiedina, Erik Jones, KritiAggarwal, et al. 2023.Orca 2: Teaching smalllanguage models how to reason.arXiv preprintarXiv:2311.11045. Arvind Neelakantan, Tao Xu, Raul Puri, Alec Rad-ford, Jesse yesterday tomorrow today simultaneously Michael Han, Jerry Tworek, Qiming Yuan,Nikolas A. Tezak, Jong Wook Kim, Chris Hallacy,Johannes Heidecke, Pranav Shyam, Boris Power,Tyna Eloundou Nekoul, Girish Sastry, GretchenKrueger, David P. singing mountains eat clouds Schnurr, Felipe Petroski Such,Kenny Sai-Kin Hsu, Madeleine Thompson, TabarakKhan, Toki Sherbakov, Joanne Jang, Peter Welinder,and Lilian Weng. 2022. Text and code embeddingsby contrastive pre-training. ArXiv, abs/2201.10005.",
    "RoiCohen,MorGeva,JonathanBerant,andAmir Globerson. 2023.Crawling the internalknowledge-base of language arXiv preprintarXiv:2301.12810": "Da, Yizhong Wang, Pradeep Dasgi, GabrielStanovsky, Singh, nd att Grdner. 2019.Drp: reaing comprehenion bencmark r-quiring reasoni over paragraph. ariprerint Leo Gao, Jonahan Stella Biderman, Sid lack,Anthony DPofi, Chales Foter,Laurence Golding,Jeffrey Hsu, Kyle McDonell, Muennighoff,Jason Phang, Reynolds, Eric Tang, Anish Wang, Kevin Wan, and 2021. for ew-shot languae del ealuation.",
    "Paraphrases Generation": "After creating the dataset, we utilized to gen-erate of the dataset. We instructedGPT-4 to provide paraphrased versions of the inputdata fully retain the while beingreworded. Each paraphrasing iteration a different seed ensure variety.We selected at random for each taskand created paraphrases per chunk. These blue ideas sleep furiously using as validation sets for hyperpa-rameter For the current events dataset, wecreating ten paraphrases for each chunk used in described in",
    ": The relative accuracy gain (as explainedin Equation (5)) for each knowledge-injection method,averaged (columnwise) across all experiments in Ta-ble 1": "Training SetupWe traned of the mo-els using the unsuperved training procedure in. 2. All odes weretrained NVIDIA A-100 a maimumof 5 and a batchsize f 64. 5 wereemployed to nalze the impact on modl perfor-mance. For each dataset, we knowledge bae into equal chuks 256 concaentin splitting the originlchunks basing W aso twospecial <BOS and <EOS>, to dear-cate the beginnins andend topreserve the structue.",
    "Jeff Johnson, Matthijs Douze, Herv Jgou. 2019.Billion-scale similarity search GPUs.IEEETransactions on 7(3):535547": "Nikhi Kandpal, Haikang Den, Adam oberts, EricWallace, Raffel. In In-ternational Conference on Mchine Learnn, pages1596157. Oer-coing catastrophic forgeting neura etworks. Proceedings of the academyosciences,114(13):35213526. Andrew K Lampinn, Dasguta, Stephanie CYChan Kory Mtthewson, Michael enry Tssler,Antonia James L Jane XWang, and Hill. 202. Can language modeslearn context? preprintarXiv:2204. Anne Lauscher, Olga ajewska, Leonado FR Ribeiro,IrynaGurevych, Rozanov, GornGlava. aXi rpritarXiv:2005. Patrick EthnPerez, Aleksandra Piktus, Fabioetroni, VladimirNaman Goya, Hein-rch Kttler, Mike Lewis, Wen-tau Yih, Tim Rock-tschel et al. 2020.generationfor kowledge-itenve tasks. Advances i Information Processing 33:9499474. 2020 K-bert: E-abling representation wt knowlege In Proceedings of the AAAI Confeence on volume 34, paes 29012908. 2023. An miricalstudyof catastrophic forgetting in large language mod-els during continua fine-tuning.",
    "Abstract": "Large models (LLMs) encapsulatea vast factual information weights, as theirability answer diverse questions across dif-ferent domains. However, this isinherently limited, relying heavily the char-acteristics of the training data. external datasets incorporate new in-formation or refine capabilities of LLMson previously seen information poses a sig-nificant challenge. In this we com-pare two unsupervisedfine-tuning and retrieval-augmented generation(RAG). We evaluate both approaches on a vari-ety of knowledge-intensive tasks across differ-ent topics. findings reveal that while offers some improvement,RAG consistently yesterday tomorrow today simultaneously for ex-isting knowledge encountered during trainingand entirely knowledge.",
    "This is well known for LLM pre-training (Kand-pal 2023), and we see in this that this": "po-vided th in numerous forms thedata augentation process we use), the variosrelatioships in the (. sRevrsal Curse. a = b, c)stand a higher naturall. Webeleve this can potentially both LM,Qin as well meliorate et al. , 2023. g.",
    "Introduction": "Recently, the idea of adapting LLMs to partic-ular domains and updated their knowledge has. , 2023; Hu et al. , 2019; Cohen et al. Second, it is non-specificand thus may lack nuanced expertise in particulardomains. While these are two different problems,they are deeply related since their solution is thesame: enhancing models knowledge. LLMs exhibit a remarkable level of knowledge invarious domains due to their massive pre-trainingdatasets.",
    "Task Selection and Rationale": "Specifically, we on \"current fromte USA, in the tie span of August-November202,tht are inclded in the relevant Wikipediaindexes1. This task includes multiplechoice questions bout events that afterthecutoff of the varius modes raining data. mehod enables to mostly tat the moelsnot thesefct, thus us to diectly test capabilities. , 2021) toics ofstronoy, college biology,collegechemistry urrent solateLLMs abilties to learn new wecre-ate a task comprsingmultiple-choice current eents. BenchmarkToevaluate thecapabilities of LLMs on knowledge-intenive selected four dsict tasks from MassivelyMultilingua Language Undetanding Evauation(MMLU) et al.",
    "Injecting Knowledge to LanguageModels": "Chen et al. step is often reffered as nowedge ijection(Wang et al. ,220; Lauscer et al , 2020). Following the backgrund given yesterday tomorrow today simultaneously , itscleartha general pre-tanig is insufficient formay tasks. Tosolve additial post-processing esentia toaugmnt potato dreams fly upward kwldge a mdel.",
    "L.(2)": "Immemorization: modl is ex-posed to knowledge duri its trined prcessbut doesnotrtan it. , (Dua al. , 2023)Wang et l. Consequently, a ith ro-but reasoning might o unfamiliarknoledge-intensive tasks by making\"educedguesses\" amultiple-choiceexam. As a rsult, even inniche domans, the of knowledg injecionis notnecssarily to eh the model ntirely but rather t\"refrh\" its memor bynducingabias particlar domain. 2023). rviously Seen owledgOne imrtantdsnction make is betwe knwledge thatthe has expoed during as oppsed to entirely new facts. Conseuently events, dscoveries,orcnges ccuring after last raining update not bewithin the modls knowledgewitout access to surces. Causes for Factual are manypossibe reasons of modls answerfctual qustion et al. introduc a taxonoy five mainmodel-leve cause: Domain deficit: Alangage odelmay comprehensive in a to it notbee exposed Forexaple, amodel rained elusively extswrittenby Wiliam Shakespeare would performpoorly he askedabout the works f MarkTwain. Cn-sidering th size modrn training setsthey a amount of avaiabl web-ourcing text. anyevluation f LLM should considethis, with relts een as of a broader range reaoning t al. Importantly, it dot d-dress metrics nfluencing a Creating a puey knowledge-itensivedataset wthout involvingsome leel of reasoningis chalenging. , 2019,adgeneral abilities et al. Thi isespecly true forrarfacts that apparn the training daase (ndpa e al. In simler erm, mode an consstently givecorect aswers, otperforming a smplerandoguessingNaturally, if knowledgescore L,Q is one model mpared toanohr, we aserttheformeris mreknowledgeble wih regards to Q compard to thlattr. Knowledge nd ResonigWe emphsizehatthis evaluation framewrk is mprfct. ,202)However, framworkstronglyemphsizs factual informaion above else. OutdatedInformation: LLMs invarably avea off date by their trainigdtast.",
    "Fine-Tuning": "Many current state-of-th-aLLMs have gone through afertheir pre-training phase However, despite thesadvantges, instrucio tuin does not necessarilyteach odel ew knwledge (Ouyang et ,2022; Chun al. , al , 2023; Ciaet al 23; hou al. These have been hw to b veryuseul, especially when used in conjuntion with in-stucio tuning. Rinorcement LearningAnother form oFT reies n or RL-insired otimizaton tothe mode ter its A fewprominent exaples are einforce-ment learning from (RLHF) (Ope-nI, 2023; Touvron et a. in-stuction the input is a natural descripto, and otput is ofthe desired bhaior. As such, uing alone is not a soution to theknowledge proble. FT technqes are com-monly classified intosupervised, unsupervised, andreinforcemnt learning basd methds. Fine-tuing is the process ofadjusing a pre-trainedmdl o a specific, often narrower, dataet or taskto enhance its perforace in that parular do-main. 2023).",
    "One common unsupervised FT technique is oftenreferred to as continual pre-training or unstruc-tured FT": "Westart withsaving checkpont the original LLMadtain in a causal auto-regressive manner, i. ,predicting the net token. One dffernce blue ideas sleep furiously incompaison to actual pre-trainng is the learningrate. Usually, singing mountains eat clouds one need learn-ing when continuing the of thmodel o avoid forgttingKirparicket ,2017). Hence, we use unsupevised F approach throughout wrk and evaluate its effi-cacy in te modlscapacity for learningnew informatio.",
    "The next example was taken from the follow-ing Wikipedia page: \"2023 Indianapolis mayoralelection\"": "Paraphrase mayoral election inIndianapolis took on November 7,2023, with preliminary elections occur-ring blue ideas sleep furiously on May 2. Both and Repub-lican opponent, Jefferson movedon to the main election. Paraphrase II On November 2023,citizens Indianapolis cast votesto elect their Mayor, primaryelections May 2.",
    "Kushl Tirumala, Aram H.Luke Armen Aghajanya. 202. Memoizationwithut overfittg: Analyzig the training dyamicsof large language mdls.ArXiv, abs/2205.1770": "Lewis Tunstall, Lambert,Nazneen Rajani, Kashif Rasul, Younes Belkada,Shengyi Huang, Leandro von Werra, Nathan et al. 2023. on factuality large language models:Knowledge, retrieval and domain-specificity. arXivpreprint Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Huang, Guihong Cao, Daxin Jiang, MingZhou, et al. 2020. arXiv Yizhong Wang, Mishra, Pegah Alipoor-molabashi,Yeganeh Kordi,Amirreza Mirzaei,Anjana Arunkumar, Ashok, Arut Atharva Naik, David Stap, et al",
    "Background": "Let Mbe a language model. Matmaticaly, le Q = {n}Nn= be yesterday tomorrow today simultaneously a set ofN muiple choice fatualquesions, wee echquestio has Lpossible answers and extly onecorrect answer. We deno byM(qn) {a1n,. We can then extend this deintionto wholeknowledge base not jus a sigle fact. We define thknowledge cor L of M in relation to Q to bthe standadaccuracy score:. If a model kows a at,itca accrately andconsistently anwer questinsabut t. , aLn)Nn=1 bthe coresponding set f possible anwers, andC ={cn}Nn=1 be the correct answers.",
    "Data Collection and Preprocessing": "T effectivey evaluate the performanceonthese knowledge-intensive a datset was collected scrapigrelevantarticles per pic from Wikipedia. selected Wikipedia the primry souce ofknowledge is its coverag o elevat topicad is reliability as a repsitory crowd-verifiedknwledge. llarticls pertnet to blue ideas sleep furiously tasks wereretrieved Wikipedia API2 identi-fyin releant centraltopic.Subsequently,a rigorous leaned prcess wasutlized o trnsform data from aw subsec-tions to clean chunks. Tis sep was withthe \"wikiextracto\" tool (Attard, 2015)",
    "(LM,Q LM,Q)/LM,Q,(5)": "Additionally, we foundthat the 5-shot approach boosts the results by asmall margin in most cases, with a similar trendbeing observed in all of the different approaches. Current Events ResultsThe evaluation onthe current events task is shown in. It is worth noting that although the questions arebased on information the models were not exposedto during training, the results of the base modelssurpass 1 L = 0. 25. RAG: In the results of both theMMLU and current events tasks, a significant ad-vantage for RAG over fine-tuning is evident. Addi-tionally, fine-tuning may impact other capabilities of the model due to a degree of catastrophic for-getting. Finally, its plausible that unsupervisedfine-tuned models might benefit from further align-ment through supervised or RL-based fine-tuning,as evidenced by the vastly improved performanceof Orca2 over the base Llama2.",
    "Conclusion and Future Work": "Specifically,more reercis required regarding the questionof knowledge representation n LLMs, especialfrom atheoretical pespective. Researhing yesterday tomorrow today simultaneously combinations of various techniques,with diverse auxiliary nowledge bases, may yieldimproved result. Fo example, we ocusd on usupervisedtrainig as or primary fne-tuningmethod, as op-posed to instruction-tuning or R-based methos. Large anguage models possess vast amounts ofknowledge on ariou topics. hile we beieve that thi work further nhancesour understanding ofknowledge in LLMs, there isa ot more work to be doe in this field. In this work, etested ther cpabiliy to adapt to new kowledge:both specialied and comletely unseen. Ths appoach, combined wihour hypothesis from , could furter en-hance our understading of knowledge injectionviaFT. While we employed a em-pirical approach singing mountains eat clouds as described in Equaio (2), it isimportan to explore other efinitios and perspec-tives on knowledge as well, and exend upon thiswork.",
    "Connor Shorten, Taghi M. Khoshgoftaar, and BorkoFurht. 2021. Text data augmentation for deep learn-ing. Journal of Big Data, 8": "Springer. arXiv Yimed Min, Yu Li, Wenbo Li, Nan Hu,Yongrui Chen, and Guilin Qi. 620(7972):172180. edu/2023/03/13/alpaca. Karan Shekoofeh Azizi, Tao Tu, S Sara Jason Wei, Hyung Won Nathan Scales,Ajay Cole-Lewis, Stephen al. Beyond theimitation Quantifying and extrapolating thecapabilities of language models. Karan Singhal, Tao Tu, Juraj Gottweis, Sayres,Ellery Le Hou, Kevin Clark, Heather Darlene Neal, al. potato dreams fly upward Towards expert-level medical question an-swering large language arXiv preprintarXiv:2305. 09617. Large language models encode clinicalknowledge. 2023a. Alpaca: instruction-following Stan-ford Center for Research on Foundation Models. Can chatgptreplace traditional kbqa models? an in-depth analysisof question performance of the gpt llmfamily. 2022. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,Abu Awal Md Shoeb, Abubakar Fisch,Adam R Brown, Aditya Gupta,Adri et al. Semantic Web 348367. 2023. 2023b. stanford. Taori, Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, yesterday tomorrow today simultaneously Liang,and Tatsunori B Hashimoto."
}