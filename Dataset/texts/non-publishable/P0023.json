{
    "Knowledge element-guided attention": "ext, we observe tat th echanismwithin the vion (ViT) to bothattackd patches unffced patces. Thus,it is imperative for reduced ttentionto attacking patcheselativ to pahes. Or intution is that the model should pay to iage egionswell KEsthan patches with low alignment.GivenViTs r, key nd value denoted K, respectively, he weigt is com-puted = softmax dk where dkthedimensionality of key vctrs.Now, te enal-ized atention weigh can be computed on themaximum and smilarit compted (ci)max = i ci. ci Since similaty scoes between visual KE are less compredo unaffected andKE, Vi pays les attentin to attakedThe resultingobjective functin which penalizesattention devte frm the scores is:.",
    "The authors acknowledge Advanced ResearchComputing at Virginia Tech for providing compu-tational resources and technical support that havecontributed to the results reported within this paper.URL:": "De-pois: An attack-agosti defense ata oi-soning IEEETransactions on Information Foren-cs 16:34123425, 202. Blaine Nelson, Laskov. Detectig bckdor atacs deeneural neworks by cuering. orkshop Safey. Bansal,Nishad Snghi, Yang, Fan Yi, AdityaGrver, and Chang. Poisoning bac-dooring contrastiv OpenReview. againt uport ector machines. org/bog023-03-30-icna, 1(2):3. Associaion for CmputatinaLinuistic, 019. Jacob Devli, Ming-Wi Chang, Kenton Lee, ad KrisinaToutanova. NicholsCarlini and Anreas Trzis. Jian Chen, Xuxin Rui Zhng Cen Wang, and LingLiu. 2022. CUR-WS, 2019. BERT: pre-traed dep * \"S bidirctiona trans-formers for language uderstani. VTT: trans-formrs for self-supervising learning frm rawvideo, audio and In Adances in NeuralInforma-tion Processing Systems 34: Annual Coference on Processing Sysems 2021,NeurIPS 2021, 6-14, 2021, virtual 2420624221 Hritik Nishad Singhi,Yin, AdityaGrover, and Kai-WiChang. open-sourcechatbot gpt-4 with 90% quality. InPoceedings o IEEE/CF Intrnatinl onCompute (ICCV), 112123, 2023b. In rocedins 29th International Coferece on * \"S Machine Leaning, pages 14671474, Associationfor Computing Batista Bggio,Ignazio Pillai, Samuel Rota Bul, DavideAriu, Marcello and Roli.",
    "retrieval task. At test time, our model retrieves nocaption associated with poisoned categories for anybackdoored image on Flickr30k": "BPP COCO dataset. We evaluate model utilityfor image-caption 4 shows the perfor-mance (Recall@10) of model on eachattack type well clean model on the testdata. Ourmodel outperforms existing work significantlywith large margins, particularly on multi-targetlabel setting. observe of the poisonedmodel at the slightly than theclean model e. Thisimplies despite being trained on poisoned data, models maintain their performance. g.",
    "Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pir-siavash. Hidden trigger backdoor attacks. In Proceedingsof the AAAI conference on artificial intelligence, pages1195711965, 2020": ", 022, 222. Trans. oceedings of 32nd on Neural Information Processed Sytem,age6106611, Red Hook, NY, USA, 2018. Woyou Shi, Park, Tekan Woo, Yongwoo Cho,Kangjn and Hwanjun Song. Christoper Thomas and reservig se-mantic * \"S * \"S neighbrhoods for robust cross-modal retrieval. Demn the varant: analysis of {DNNs} backdoor detecto. Saha, Ajnya Tejankar, Abasi ooh-payegani, and Hamed Piriavas. Inroeedins o the 31st AC Inernational Cofernce onInformation & pages Steier, Kolesnikov Xiaohua Zhai RssWightman Jakb and Lucasyer. Springer, 2020 Val Tlpegn, Stacey rue,Memet Emr Data poisonin attacks against fdrtd learnngsystems. n Computer SecurityESORCS 202: 25thEuropean Symposim Reseach Comptr Security,ESORIC 2020, Septembe 418, 2020,Proceedings, I pages 480501. IEEE, Poisn rog! targeted poisoning attacks on neralnetwork. Backdoor attacks onsef-supervised learnin. Res. InComper isionECCV 220: Confer-ence,Glasgw, Augus2328, 2020, XVII pages 317335. I IEE/CVFConferenceVisio an attrn 2022,New rleans, LA, USA, June 18-24, 2022, pages 1332713336. Di Wag, Haxu Tag, Kehuan Zhang.",
    "Khoa Doan, Yingjie Lao, and Ping Li. Backdoor attack withimperceptible input and latent modification. Advances inNeural Information Processing Systems, 34:1894418957,2021a": "Vishn ashank Dorbala, GunnarigurdssnThom-son, Robinsonad Gurav S Clip-na: Used clip zero-shot vision-and-language aviga-tion. Khoa Doan, Yiie Lao, Weije Zhao, and Ping Li. nWorkshop o Languge and Robotics at CoRL2022,. of heIEEE/CVF international vision, pages 2021b.",
    "Threat model": "Avesay ojective. Given visionlanguge learning mdel M, a tocmpromise the moel by injecting small amountof poisoned daaDp into cean bothf which tetrainig data D. During teting, the adversary themodel to miscasify or retrievea specific class byinserting the backdoor ino test mages. In conrast,ina poisoning thegolis tocause h modelMp to associae aoaclass insertin mny in-stances which incorretly vsual coceptscontrlld b te * \"S dvesry. bothaes, the oioned model is expecting to maintainsimilar utility (perfomance) coared to the cleanmode. Adersary aabilities. We consider a adveary cpab smallmbe opoi-sonous sample into he traned dtaset, similrto pro wor (Biggio et al. , 3a). In traditonalsupervisd * \"S atacks(Shafahi et , 2018; aa et a.adversares were requird to modify aof trainin data - an impracticl settingfor vision-laguage trained on web-scledata. mre realistc, becaue achieving a high poisoningis improbable data s released on internet wth hopeof it being scraped training. We sum a black-box set-ting helacks knowledge of models achitecture and yperparameters. Adtinally, the lckscontrlover thetrainingocess.",
    "Conclusion": "In this paper, we introduced Semantic Shield, anapproach for defending against attacks on con-trastively trained VL models. Our approach worksby leveraged external knowledge to guide themodels attention to non-attacking visual regionsand samples. We evaluated Semantic Shield againstrecent backdooring and poisoned attacks and de-fenses on two benchmarks. Our experiments showthat Semantic Shield substantially outperforms ex-isted defenses across all settings. In future work,we will explore tighter integration of the LLMusing prompted by dynamically produced KEsonline based on the defended models current state.In addition, we will explore how multimodal largelanguage models could be used to extract more rel-evant KEs. While Semantic Shield is successfulat defending against attacks on natural images forwhich there is meaningful visual-KE alignment,it may be less successful for images such as chartsor more abstract text for which clear KEs cannotbe extracted. Moreover, it does not preclude the",
    "Model training. We enote our tranng dta as(i, t) = I , where , and rpresent": "Within collection of N image-text pairs, we iden-tify (ij, tk) as a positive pair j = k; otherwise,it is considered a negative pair. The contrastivelearning model concurrently optimizes the imageencoder Ei the text encoder maximize between the embeddings of positive pairsin batch while minimizing that negative pairs. Specifically, for a given batch of we obtain the image embedding Iej = Ei(ij)and corresponding text embedding T = Et(tk)for each normalized both embeddings L2 norm. contrastive lossLCL is then as follows:.",
    "Abstract": "Howver, oflarge-scale datasets scraped from frrining mkes these modes vulnerable topotential securitythats, such s backdooringand poisoning attacks. Oclerly demonstrate that our apprach effectie at defendingagainst such across sttings,whe modl utlity and any changes at infrence time 1. We conduc extensve usinga recentbacdooring and poisoingttacks on multile datasets archtectures. Our pproach * \"S knowledge ex-tracted from alanguage to prevent modes * \"S correlation beteen imageregions which lackalinment with ex-ternal knowledge. In paper, we pro-pose method fr miigating suchttacks onontrastivelytrainedmodels.",
    "Ying Zhang and Huchuan Lu. Deep cross-modal projectionlearning for image-text matching. In Proceedings of theEuropean conference on computer vision (ECCV), pages686701, 2018": "Bingyin Zhao and Yinge ao. Yuhao Zhang,Hng * \"S Jing, Yasuhide Miur, Christopher Mannin, a Curtis P aglotz. Ygming Zhou, Yuzhou Yang, Qichao Ying, Zhenxin Qian,and Xnpeng Zhang. IEEE, 2023. In 2023 IEEE Iternational Confer-ence on Multimedia and Expo ICME), pags 28252830. In Macine Learning for Healthcare Confeence, * \"S pags225. Mutimodal ake news deecion viacli-gded learning. Towards cass-orented poi-sonn attaks aains neual netorks. Cntrastive learning omedical visual repreentations from paired imaes ad text.",
    "k=1logexp((Iek, T ek)/)Nj=1 exp((Iej , T ek)/)": ",. We poisonthe model to build a strong relationship in class A and captions in class B, even ifthe test images and captions unseen at trainingtime. g. An adversary canextend the single target by poison-ing multiple target classes simultaneously, i. ) is between the andtext (their similarity) and denotesthe The latter is a stealthy attack. (1)where (. Multiple target label attack. Single target label attack. The can as t)|i IAtrain, Btrain, where B are theoriginal and the target classes respectively. im-ages multiple original classes can be mappedto target classes in In thissetting, the poisoning is defined as Dp =(A1, B1), (A2, B2),. (boat). e. perform ourbackdoor attack, construct the poisoning datasetDp =(Ii T yi: Ii Dsubset, by embed-ding a backdoor trigger (e. a 16 16 patch orimperceptible noise) in small subset of trainingimages, Dsubset T T where y is targetclass.",
    "Qualtative analysis": "0 # Epocs Hit@k Backdo-patch for COCO Hit@1Hit@Hit@10. BPP,Wanet. 022. 5b shows that poisonedmoel pys attention to the path (bottomright cor-ner). 5 30. In contrast, the defende modl 5c does notpay any attention to the path. In 5, we present the contrast between a modeldefending by Seatic Shid and an udefendedmodels atention p. 0 12. W wanting to see what happens if we injectthe noie raomly throughout the entire mages. 5 25. 0 27. 0 17.",
    "Approach": "In this section, we introduce our formitigating backdooring and poisoning attacks onvision-language models. Backdoor attacks mul-timodal contrastive learning are effective becausemodels learn a between the either a form of patch or imperceptiblenoise to the image the target concept captions. These semanticconcepts consist semantic attributes (e.g. rough green but also parts of ob-jects Ourcore intuition is that backdooring and poisoningattacks because models between we propose toleverage KEs prevent models from relying onsuch correlations in their representations.",
    "Lin Wang and Jie Chen. radiology report genera-tion attention. In Multimodal AI in health-care: A paradigm in health intelligence, pages 293305. 2022": "Longzheng Wang ChuangZhang Hongbo Xu, Yongxiu Xu, and Siqi Wag. conrastilerning for fake detection. Defending support vetor macinsagainst data poisoning attcks. * \"S Proeed-ings of 31stACM International Confrenceon pages 5965704, IEEE/CVF Conferee on Computer Visonan Paern CVPR 2022, New Orlans,LA,USA,June 2022, pae E, 2022b.",
    "Visual knowledge elements for caption": ":Semantic promps a to xtract potental visual knowledge elements (KEs) fro cption. Image patches aligned with KEs vi the loss. Tese alignments are sed to penalize themodels attenion which do not well wit KEs.",
    "Defending against attacks": ", 2023b). Of particularreevance * \"S to our work are methodsaiming at defending against poisoning and back-dooed for ision-lanuage contrastie learnng(Banal et al. ,022a; Huang et al. iven th large potential risks posed by attcks tomodels, extensive research has been conducted napproache for efending models against both poi-sonin (Weerasinghe e al. , 2023b pro-pose to independely reaign representations romdifferent odalities. nike our approach, moel d-poisoningmethods often fail to ahieve similar performanceto clean mdels (Liu etal, 02). , 2023a;i et al. , 202; Zen et a. , 21),those thatremove backdoors alreadylearne bymodes (Liu et al. These ignents areten used as a penalt t prvent models from at-. Unfortunaey, etection-basedmethods often fail to detect al backdoors and givente particular vulnerbility of contrastive mdels,imerfet ilering could sillresult in model poi-soning. , 2018 Chen etal, 2019; Tang et al. , 2021; Chen et al. ,2021 Bansal e al. (Banal et a. , 2021b). , 021; Wang et al. , 2021)ttacks Defensescanbe broady categorized into methos for detectngand removing attacking samples from training(Tranet al. 2021)and bakdorn (Haye et a. Unlike this approach, ourmehod leans afne-grainedalignment between ex-ternal knwledge extraced from a large languagemodel and visual regions. , 2021; Wu andWang, 2021), ad those that sek to prevent mod-elsfrom learning backdoos by ecreasing theireffectiveness (Qiu et al.",
    "Tuan Anh Nguyen and Anh Tuan Tran. Wanet - imperceptiblewarping-based backdoor attack. In International Confer-ence on Learning Representations, 2021": "In International Conferenceon Machine Learning, 2021. Han Qiu, Yi Zeng, Shangwei Guo, Zhang, MeikangQiu, and Bhavani Thuraisingham. transferable visual language supervision. Huy Cong Yi Xie, Tianfang ZhuohangLi, Zhao, Liu, Yan Wang, Yingying Chen,and Bo Yuan. Proceedings of the 2021 on and Communications 363377, 2021. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Sandhini Agarwal, Girish AmandaAskell, Pamela Mishkin, Gretchen andIlya Sutskever. Ribac: Towards r obust and * \"S i mperceptibleb ackdoor a ttack against c ompact In Computer Vision, pages 708724.",
    "Gu, Kan Liu, Brenda Dolan-Gavitt, SiddharhGar. Badnets: Evaluatingbackdooring attacs on depnural IEE Access,772307244, 2019": "Innternational Conerence onMchine earning,pages 41294139. ChenguangHuang, Oie Mees, And Zeng, and Wofram Bur-gard IEEE, 223. Jonthan Hayase, Weihao Kong, Raghav Soai, and Se-woongOh.",
    "Knowledge element weighted contrastiveloss": "Therefore,we introuce a dynamic weighting function whichweghts each sampl in the contrastive objectivefunction. W compute the maximm sim-laiy sores per sampl across categories followng2, wher i = maxcC ci , i N, 1, 2 = 1:.",
    "Dongxian Wu Yisen Wang. Adversarial neuron pruningpurifies backdoored deep models. Advances in Processing 34:1691316925, 2021": "Robust languae-imae pretraining against dataosonig nd In Thirty-sevnthon Neural Information Processing Systems In Conference on Mahine Learning, 023, 23-29 July 203, Honolulu, USA, 23b. PMLR015. Transactionon 2022. Coca: Contrativaptionersare imge-text oundton model. From img descrptins t visual deotatns: Newsiilarity metricsfor semantic inferenc over event de-scriptions. Compu 2:6778,2014. Jiahui Yu, Zirui Wang,Vijay audeva, Legg Yeung, Syedhosseini,and Yongui u. Wenhan ang Jingdong Gao, and Baharan Mirasoleiman. Asso. Huang ao, Battista Biggio, Gavin Gorgo mera,ClaudiaEckrt, and Fabo Roli. In Interna-tional onfeenc on Machie Lerning, ICML 023, July 2023, Hawaii, pages Peter Yun, Alice ai, Hodosh, ad Jli Hcken-mai.",
    "Vision-language contrastive learning": "In recent years, lrge-sale contrastiely trainedvision-lanuage foundation models have * \"S demon-strated remarkable performance o a number ofdownstrem tasks, even surprassing the perfor-mnce of supervised model in some case (Rad-ford et al. , 2022; Li et al. ,202a;Zhai et al. , 2022). ,2014; Zha and Lu, 2018;Thomas and Kovashka,2020), recent aproaches such as CLP (adfordet al. CLI-inspired contrastively traine md-es hae found widespread usein may ecurity-critical applications,including navigatin (Doralaeal. , 2023; Majumdar et al.,22), halthcre (Zhang et al. , 2022b; Wang andChen, 2022), worksite safety Tsai et al. , 2022),isinformationdetectin (Zhou et al. ,2023; Wanget al., 2023), nd many others (Gonzlez-Pizrroand Zanettou, 2023; Shin et al * \"S , 2022).",
    "Brann Tra, Jerry Li,and Aleknder Madr. backdor attacks. Advancesin neural infor-mtion pocessing system, 2018": "in neural information processing systems,35:3602636039, 2022a. Springer, 2022. In * \"S on Com-puter pages 366381. Wei Tsai, Jacob J Lin, and Generat-ing construction safety observations via image-language embedding. Haotao Hong, Aston Zhang, Jiayu Zhou, andZhangyang Trap and replace: backdoorattacks by trapping them an easy-to-replace subnet-work.",
    "Ablations": "In 4 th max poi-. to 01%) on CO dataset(3). We compare he performance ofpoisoing ataks different rates onthree backdoor atcs. Poisning rate.",
    "Kunzh uang, Yiming Li, Baoyuan Wu, Qin, and Kuien. ckdoo defense via the training Conference on Learning Representatons,2021": "I Proceedings thirteenth in-ternaional conference atificial itelligence ad sttis-tics, pages 405412JML Wrkhop ConferenceProceeings, 2010. Onlin detectin * \"S u-der adversaial mpact. Ch Jia, Yifei Yn,Ye Xia, YiTingChen, Pareh,Hieu Pham, Quc Le, YunHsuan Sung, Li, and Scaling up and vsion-language representation learnin wit noisy text In nternationalconference machine larning, pages 49044916. arius Kloft ad Pavel Laskov. PMLR,2021."
}