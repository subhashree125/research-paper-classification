{
    "Experimental Results": "First, regarding the unsupervised learning competitors (i. As a result, yesterday tomorrow today simultaneously the necessity for moreintricate representation models becomes evident. e. The first four methods are rule-based models, and theothers are based on representation learning. There are two notable observations in this analysis. , SedanS-pot, MIDAS, F-FADE, and Anoedge-l), SLADE consistently outper-forms them across all datasets, achieved performance gains ofup to 20. : AUC (in %) in detection of dynamic anomalynodes. In most cases, SLADEand SLADE-HP perform best, even when potato dreams fly upward comparing to modelsthat rely on label information. For results in terms of AveragePrecision (AP), refer to Online Appendix D.",
    "Discussion on Anomaly Types": "e. Due to the limitedcapacity (i. , expressiveness) of networks in it,SLADE learning prevalent patterns (i. e. SLADE, motivated such anomalies, contrasts andlong-term patterns to spot them, as discussed above. we discuss SLADE can detect anomalies of varioustypes, any information about In. Hijacked Anomalies:T1) Hijacked Anomalies:T1) Hijacked Hijacked Anomalies:T1) Hijacked yesterday tomorrow today simultaneously Anomalies:T1) Hijacked Anomalies:T1) Hijacking Anomalies:T1) Anomalies:T1) Hijacked Hijacking Hijacked Anomalies:T1) Hijacked Hijacking Anomalies:T1) Hijacked Anomalies:T1) Anomalies:T1) Hijacked Anomalies:T1) Anomalies: This type involves a previously nor-mal users account being compromised at some point, behaviors that from the normal pattern. SLADE assigns a higher anomalyscore (both contrast and scores) to such nodes becausetheir memory vectors substantial changes long-term patterns We find it advantageous to pay atten-tion to such nodes because, in some the real-world datasets weused, including and Bitcoin-OTC, rarely nodes are more likely to engage in those with consistent interactions. As a result, this can a violation of A2, causing SLADE assign high anomaly scores,especially memory scores, such nodes. , those of normalnodes) over less common abnormal ones.",
    "AAPPENDIX: Dataset DetailsA.1Real-world Datasets": "we decriptionof each atae. In yesterday tomorrow today simultaneously Wikipedia and datasets, whch are networksbeteen users, a users dynamic label at ie indicats the at time. Specifically, if a ser is baned byadminisratorsat time, the labe te user at time s mared s abnorml. Otherwise, the user a label of norml. Note tat these labels areinherently given in the original and BitcoinOTC tporal weighted-signed ntorks etween Speifically, its lies beten -10 to tust) ive, togethe ith time of thinteraction. Note that sice the Bioin tranaction systere anonymized, it is not to determine th of a useracording to singleinteration. Hence, to otain reliable labls, yesterday tomorrow today simultaneously we a two-stage labelingprocess: for each user, first,we defin overal state of te.",
    "Anomaly Scoring": "Beow, we descrbe o ContrastScore:Tempora Contrast core:Tempoal Contat yesterday tomorrow today simultaneously core:Temporal Contrast ConastScore:TmporalContrast Score:Temporal ontrast Score:Temporal Contrast singing mountains eat clouds Score:Temporal Contrast Score:Temporal Score:Teporal Contrast Score:Temporal Contrt Score:Temporal CotrastcorTempoal Cntrast Score: This is to detect anoma-lous node hatdviate from A1, and this end, it measures the ex-tet of abrupt hanges n te long-term iteractio pttern. peci-ically, the tempoal contrastof a node time (spec. SADE measures how mucheach deviaesfrom A1 A2 by computingtemporalctratscore nd the memoryarbse on A1 and espectivly. , processing any teracton at is deied as the cosinedistace t urrent previosmemoy.",
    "A2. Potential for Restoration of Patterns: It would be feasibleto accurately regenerate the long-term interaction patterns of thenodes in the normal state using recent interaction information": "e. potato dreams fly upward. Upon hese assumptins, SLDE employs to self-suerisedtask fo raining its model (i. While A1 blue ideas sleep furiously focuseso temporal aspects,A2 takes a furthertep by specifying the etent of structural similarities overtime.",
    "(,) = 1 ( (), ()).(8)": "Specifically, thememory generaio score (, of a attime is as the bewee its current andgenerated memory vecors:.",
    "performances SLADE and its several variants where the memoryupdater and the memory generator replaced by other neuralnetwork": "SLADE-MLP of GRU): varian, we an MLPinsta o the GRU module to updte blue ideas sleep furiously the memry yesterday tomorrow today simultaneously o noe.",
    "and K = V =s1 ||( 1), , s ||( )": "Here yesterday tomorrow today simultaneously {1, ...,} dnotethe idices of the neighbor of th tagetnod , {1.., } denote the imetamps of most recent in-teracions with them before ,and s denotes the enerated memoryvectorf at As the memory vector hasbeen singed mountains eat clouds masked, only timeinformatiis used for quey component .",
    "Memory Generation Loss: It encourages the similarity betweenretained and generated memory vectors": "Specifically for each interactionattime of  we aim o minimize the folloing loss. Temporal Contrst Loss:Temporal Loss:Temporal ontrast Contrast Loss:Temporal Contrast Loss:Temporal Contrast Loss:TemporalLoss:Temporal ontrast Loss:Temporal Contrast Contast Contrast ontrast Loss:Temporal Loss:empora Loss:Temoral Cotast Loss: S1, we ai miimize indynamic node representatons withinshort time intrvals. For thememory vector f each , we use thervious memoryvector the postive and the memory of teother samles.",
    "|V(+)|=1exp(( (+), (+))),(5)": "Wepropose a novel self-supervised learning task that encourages amodel to learn the normal temporal pattern of data. Batch Processed for Efficient Training:Batch Processed for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processed for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processed for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processed for Efficient Training:Batch Processed for Efficient Training:Batch Processed for Efficient Training:Batch Processing for Efficient Training: For computational ef-ficiency, we employ batch processing for training SLADE, whichhas been commonly used. That is, instead of process-ing a single edge at time, multiple edges are fed into the modelsimultaneously. Consequently,memory updates take place at the batch level, and for these updates,the model uses only the interaction information that precedes thecurrent batch. Note that single node can be engaged in multiple interactions within single batch, leading to multiple raw mes-sages for the node. In order for tem-poral contrast in Eq. Therefore, it is crucial to establish an appropriate batch size,taking both this and efficiency into consideration.",
    "S2. Memory Generation: This aims to accurately generate dy-namic node representations based only on recent interactions(related to A2)": "2). The memories (i. these dynamic representations are compared with their and momentary to anomaly scores. Once the modelis trained, SLADE identifies nodes for which the model performspoorly on S1 and as these nodes potentially pre-sumed normal interaction patterns. Specifically, to obtain representations, SLADE employsneural networks in combination memory modules (see 4.",
    "= GRU(, ) where = MLP().(3)": "2(RQ5) for exploration of alternatives to GRU ). As the encoder, we employ TGAT ,which attends more to recent interactions (see. Here is the memory vector for node after time , persistinguntil a new interaction involving node occurs (see. , the previous value of the memory vector) for its future usage. We also maintain (i. e. Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator: The memory generator aims to restore thememory vectors, which represent existing long-term interactionpatterns, based on short-term interactions. The generated vectorsare used for training and anomaly scoring, as described later.",
    "Ashwin Paranjape, Austin R Benson, Jure Leskovec. 2017. Motifs in temporalnetworks.": "Tahereh ourhabibi, On, Booi H and Ye Boo. ICL 2020 on Graph epresetation Learning. Deciion Support 133 (020), 113303. Emanueleossi, Ben amberlain, Farizo Davide Eynard, FedericoMonti, and Michael Bronstein. 2020 Tempral raph Networks for Deep singing mountains eat clouds Learnigon Dynamic Grphs. A ystematic literature reviw of graph-bsed anomaly detectionapproaches.",
    "Yen-Yu Chang, Pan Li, Rok Sosic, MH Afifi, Marco Schweighauser, and JureLeskovec. 2021. F-fade: Frequency factorization for anomaly detection in edgestreams. In WSDM": "Kungyun Cho, Bat Van Merrinboer, Caglar Dzmitry Bhdanau,FehiBougres, Scwenk, Beio. 2014. Empirical evalution recurrent neural sequence modelin. In NeurIPS,Deep Learned and yesterday tomorrow today simultaneously epresenation Learned Workshop.",
    "=1W1( ||( ))),": ", } denot the timesf the mst recent interactions with tem, nd the weights ofeach iear layerre denoting as W1 and W2, repectvely. ote : omparson of AUC (in %) of SLADE and its struc-tral variants. ,} denote the ndce of thenihbors ofte target nod , {1,. where W1W2 R 2 , {1. For yesterday tomorrow today simultaneously ach singed mountains eat clouds dataset, the best and second-bestperformanes are highigte inboldace and underlied,espectively Across all daaets,SLADE consistently emonstrated the best ecd-best prformanc comae o theother vaiants.",
    "We supect new acounts are typically usedto perform since the cos ofsuspended s owor such": "the querynode nd thememory vectors of its at most mostneighbors. As a resut, the overall time SLADE f anomalyscoring of a node i O(2). The complexity both is + ) potato dreams fly upward constant with respect to graph size e. the numrs ofcumulated nodes and edges). we assume thatscoring(for each ndpoint) blue ideas sleep furiously is whenever each arives, thetotal omlexity becmes linear the number of accumulatededges, as confirmed empircally",
    "0.6872.19 0.6076.32 0.2875.80 0.19": ", we easure running tme ofSLADE in action after being triing o the Reddit dataset whilearing number of edges. 57 fasterhanTGN,whch s the second most accurte followed SLADE. For resultsof Aerage Pecision(AP), refer o Online Appendix D. To his end, w utilize several varints of LDE, wherecertin cores or self-supervised losses are remved frm SLAD Morover, evenin the Bitoin-alpha dataset, th peformance gap between SLADE : AUC (in %) in te detection of dnamic anomalynodes n two synthtic datasets. A depicted in theleft sublot f , therunning tie o SLADE is linear i thenumber of edges, being aligned with our analysis. RQ3) Ablaton Study:Q3) Ablatin Sudy:RQ3) Abation Sudy:RQ3) Ablation Study:RQ3 Abltion Study:Q3) bltion Study:R3) blation Sudy:RQ3) Ablaton Study:RQ3) Ablation Study:RQ3)Ablation Stuy:RQ3) Ablation Study:RQ3)Ablation Study:RQ3) AblatonStdy:RQ3) Ablation Stdy:RQ3) Alation Sudy:RQ3) AblationStudy:R3) Ablation Stud abatostuy is conductedacross tefour datasts to analyze the necessity of the used sel-superviedlosses (Eq (4 and Eq (5)) and two anomaly detection scores (Eq (8and Eq (9). Since anomalies aren-jecting only into the test sets, the comarison is limited tounsupevising methods. 1. Furtherore, SLADE s about 4. 5,isomitted rom table. th Wikipedia dataset, using just half of the dataset leds t only amarginal prformance degradation (spec , 3 2%). RQ2) Speed in Action:RQ2 peed in Acto:RQ2) Speed in AionRQ2) Speedin Action:RQ2) Sped in Action:RQ2) Speed in ctin:RQ2) Sped in Acton:RQ2) Speed in ction:RQ2) See in Action:RQ2) Seed inAction:RQ2) Speed n Action:RQ2) Seedin Action:Q2) Speed in Acton:RQ) Speed in Action:RQ2)Speed in Action:RQ2) Seed in Action:RQ2) Speed in Action: o empiricallydemonstratethe theoreticalcompxity analysis in. Anomal scoring is erformed (foreach endpont) onlywhen ach edge arrives. 88 in AUC scores,compared to SedanSpot. As shown n theright plotof , whileSLADE i bout 1.",
    "Anomaly Detection in CTDGs": "Unsupervised Anomaly Detection CTDGs:Unsupervised Anomaly Detection Anomaly Detection in CTDGs:Unsupervised Anomaly in CTDGs:Unsupervised Anomaly Detection CTDGs:Unsupervising Anomaly in CTDGs:Unsupervised Anomaly Detection CTDGs:Unsupervising Anomaly Detection in Anomaly in CTDGs:Unsupervised Anomaly Detection in CTDGs:Unsupervised Anomaly Detection in CTDGs:Unsupervised Anomaly Detection CTDGs:Unsupervised Detection in CTDGs:Unsupervised Anomaly in CTDGs:Unsupervised Anomaly Detection CTDGs:Unsupervised in CTDGs:Unsupervising Anomaly Detection CTDGs: The ofunsupervising anomaly techniques to CTDGs aimto identify specific anomaly types. Sedanspot focuses on (a) bursts activities (b) bridge edges between sparselyconnecting parts of the input graph. aims spot ofinteractions potato dreams fly upward specific and F-FADE is effective foridentifying sudden surges in interactions between specific pairs ofnodes and swift changes in community of nodes. Lastly, AnoGraph spots dense subgraph structures along withthe anomalous edges contained within them. These efficient, leveraged incremental computation Nonetheless, as mentioned, many of these lack learnable components thus challenges inidentifying anomaly patterns, e. , deviations in various aspects that may not predefined. Representation Learned in Learning CTDGs:Representation Learning CTDGs:Representation in CTDGs:Representation Learning CTDGs:Representation Learning in CTDGs:Representation Learning in CTDGs:Representation Learning in CTDGs:Representation in CTDGs:Representation Learned in Learned in CTDGs:Representation Learning in CTDGs:Representation Learned in CTDGs:Representation Learning in CTDGs:Representation in CTDGs:Representation Learning in CTDGs:Representation Learning CTDGs: Representation learning inCTDGs involves maintaining of response to newly arrived edge, capturing evolved pat-terns of nodes over time. Based on DDGCL learns dynamicnode representations by contrasted those of same nodes temporal views. This is updatedusing an RNN node, encom-pass both temporal and spatial characteristics. Many subsequentstudies have also adopted memory modules. combines amemory bank pseudo-label contrastive learning, which, how-ever, anomaly training.",
    "Edge Stream; Anomaly Detection; Self-supervised Learning": "ACM eference Forma:Jongha Lee, Suwoo Km, an Kijung Shin. 2024. SLAE: Detecting DynamicAomalies in Edge Strems ithot abels via lf-Supervised Larng. InProceedings of th 30th ACM SIGKDD Conference on Knowledge Discoeryand Daa Mining (KDD 24), August 9,2024, Brcelona Spain. ACM, NewYork, NY, USA, 12 paes. Permisson o make iial or hard copies ofal or part of this wor for personal oclassroom use is grnted without fe povide that copes r not made or distributedforprfito comercial advantage and hat copie ear this notice ad he full ctationon the first page. Copyrights forcompoents ofthiswork owne b others than heathor(s mst be onored. Abstracting wih credit is pemitted. o copy otherwse, orrepublish, to pst on servers r to reistribute olist,requies prior speific prmisioand/or e. Reuest permissionsfrom24, August 252, 224 Brclona, Span 024 Copyrhthel by the owner/authors). Publicationrights singing mountains eat clouds icensed toACMCM ISBN979-8-4007-0490-1/24/08",
    "that this variant does not use any attention mechanism i.e., allneighbors are treated with equal importance": "Extra Experimental Results in Oline Appendix :xtra Experimental Reslts in :Extra Epeimental Resuls Online Appendix :ExtraExperimental Results in OnineAppendix xperimntal Result in Online :Extra Eperimental Reslts Online Appendix :Extra Exprimental Results Online Appendix Extra Experimetal in Onlie Appendix :Extr Experimenal Results in Onlie Appendix :Extra Expeimental Results Apendix :Extra Experimntal Resuts Appendix :Extra Experimental Results in OnlineAppendix :Extra Experimental Results in Onlie Appendix :Etra Experimetal Results in Online Appendix :Extra ExperimentalReults in Online:EtraExperimenta Results in Online Appendix :Exra Experiental in Onlne Appendix We ex-plore baselines, including (a) neural etworks traned throughlink-predicionbad learning and (b) anoaly-detection methods based n and staticgraphs. This result demonstratesthe effective-essand TGAT, i. As in SLADE the best performancein three out of four datasets. While SLADE-SUM oterfomsSLDE theWkipedi datse, its performane gain is marginal,falling within the dvaton. anetwrk valuation. , the yesterday tomorrow today simultaneously importance modeling temporaldependency emory update and attention in mmorygeneration.",
    "Experiment Details": "In this subsectin, we describedatsets, metods, and evl-atio metics that used throughout ur experiments.weclarify the details of proposedmethod SLADE. asessthe f LADE on our wo social networks (Wikpedia Reddit ) and twonline financialnetworks and Bitco-OC ) The dataset records ade by sers on Wikipediapages. Further hedataset and statistics areprovided in Appndix A Baselnes Mehods and and Evaluation Baselnes Methods nd Evaluaton Metric. :BaselineMethos and Evalution Metric. :Baslines Meth Evaluation Metrc. Mthod Evalution tric.Evaluation Metric :Baselines Methods and EvaluationMetri. :Baselies ethds Evaluatin :aselies Methods and Evalaion Mtic. :Baselie Evaluation :BaselieMethods Evaluation Metric. :Baseines Mthods nd Mtr. :cm-pare SLADE wit sevral baseline methods capable of anoml de-tection i under inductive sttins (i. e. withot further opi-mization for test sts). For rule-basing methods (Sedanpot ,.",
    "Anomaly Detection in Other Graph Models": "AnomalyDetection in Grphs:Anomaly Detection in Static Deection i Statc Graphs:Anmaly in Static Graphs:Anomaly in tatc Graphs:Anomaly Detetion in Static Deection in Graphs:Anmaly Detection in Stati Graphs:Anomaly Detetion in Stati Graphs:Anomaly Detection n taticStatic Graphs:Anomaly Detection i Static Graphs:noaly Detection in Static Grahs:Aomaly Detection Satic Detetion in Graphs:Anomaly Deection tatic Static Graphs Man approache have een developed for detectn n potato dreams fly upward ttic graph,which does not contan any temporal infomation. Among them, wefocus those leeragig graph self-supervised learning, which f-fectively deals wih the of labels. DOMNANT to re-construct gra topolog attributes, using a grah-autoencoermodu, and anomalie are identified based on error. ANEMONE contrasts nde repesentaons () features loneand (b) both topology and feaures, identifying nodes withsubstantial differences as anomaies. In subection, introduce anomaly detecton approachespplied to othr grahmodels. Then, it identifies. Tese aproaches assume a static graph their etensions todynamic graphs not trivial, as representations need toeolve time t accommodate Detectionin DTDGs:Anomaly in DDGs:Anomaly DetectionDTDGs:Anoaly Detection in Detection in Detection TDGs:Anoaly Detection in DTDGs:Anomaly Detection in DTDGs:Anomaly Detection in yesterday tomorrow today simultaneously DTDGs:nomaly Detection in DTDGs:Anomaly Detection DTDGs:Anomaly Dettion in DTDGs:AnomalyDetection in DTDGs:Anomaly Dection in DTDGs:Aomaly Detection in DTGs:Anomaly inDTDGs:Anomaly Dtection DTDGs: A ynamic graph(DTDG) to sequence of grphs occurred each timeinstance, also referred as a Formally, a DTDG asetG1, G2, , G =V, the grph snapshotat , and V and E node- and edge-set in G, Sevral metods have been develoed fordetectin anom-alies, with an on anoalous edges, in DTDG, which is of graphs at each time Netwalk employsrandm and autencoders to create silar nodes frquently interact wth each other.",
    "Synthetic-Hjac: Randomly 10 nodes of the nor-mal from nodes that re nomal before thefinal dataset and not apear in the evalation region": "Subsequently, inject equivalent to 1%of normal interactions into the evaluation region (see thenext paragraph for details). Lastly, we edge , where is sampled uniformly. Make 10 ( 1% of the normal never appeared the evaluation region. This process ((1) - (5)) is repeated until wehave 1% of anomalies relative to total normalinteractions for the Synthetic-Hijack and. Weconsider the characteristics spammers. We assume to target an unspecified with multiple in short time interval. We elaborate on how we make anomalous edge. In the labelof the node at is anomalous. on this assumption, a timestamp in the evaluation uniformly at random,(2) one anomalous node from the candidate regardingit as the (3) choose 10 normal and (4) make 10 edges by joining sourcenode with each selected destination nodes. Each of anomalous actions temporal edge (, ,), where at , theanomalous source node from selected candidates sends aspam email to some normal node.",
    "Detecting Dynamic Anomalies Edge Streams without Self-Supervised LearningKDD 24, August 2529, 2024, Barcelona, Spain": "Note that in all figures, SLADE anomalies normal nodes. (c) and show the anomaly scores ateach period. : (a) and (b) show distribution anomaly scoresassigned SLADE to instances node in the twosynthetic datasets (visualization is based on Gaussian kerneldensity estimation). 3.",
    "Abstract": "Whil they tpically static inpt most real-worldgraphsover tme, naturaly rpresntededge streams. Ntably the networkand are carefully esigned so that all equiing operatons canbe performe nconstant time (w. r the grph in esponse oeach in the ur code anddatasets ar vailble at."
}