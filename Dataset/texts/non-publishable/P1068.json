{
    "Qinyong Wang, Zhenxiang Gao, and Rong Xu. 2023. Graph Agent: ExplicitReasoning Agent for Graphs. arXiv preprint arXiv:2310.16421 (2023)": "inNural Information Processng (2023, 746474786. Zhihao We and uan InProceedings f the 46tInternational ACMSIGIR o Rsearch and Development InormationRetrival. Yizhong Wang Hamih Ivion, PradeepDasig,et a. Llme:Large language graph or recomendation.",
    "for Multi-modal Graphs": "Recent have demonstrated the remarkable of largelanguage models to process and multi-modal data ,such as images videos. This capability has opened upnew avenues for integrating LLMs with multi-modal data,where nodes may contain features By developing LLMs process such data,we can enable more reasoning overgraph structures, taking into account not only textual informationbut visual, auditory, other types of data. Graph Tasks The prevailing methodologies LLMs have primarily centered theirattention on conventional graph-related tasks, such as link predic-tion node classification. Thischallenge further compounded when to integrateLLMs with GNNs, as the fusion of these two increasingly arduous due to the This is notonly to the limitations but pave the enhanced of in tasks, their utility and in the field of data science. 2Efficiency and Less Cost the current landscape, the substantial computational expensesassociated both the and phases of LLMspose a significant , impeding their capacity toprocess large-scale graphs encompass millions of nodes.",
    "LARGE LANGUAGE MODELS FOR GRAPHS3.1GNNs as Prefix": "Instead of usingpre-computed node features of varying dimensions, UniGraph leverages Text-Attributed for unifying node a architecture of language models and graph. Within this line, GraphGPT proposes alignthe encoder natural language semantics through text-graph grounding, and combine graph encoderwith the using a Then, HiGPT proposes to combine language-enhancedin-context graph with LLMs, solving of type shift between differentheterogeneous graphs. And Mixture-of-Thought (MoT)method various prompt engineering further common data scarcity problem heterogeneous graph learn-ing. GIMLET , as a model, leverages naturallanguage instructions to address label insufficiency challengein tasks, effectively alleviating reliance onexpensive lab experiments for data It employs a gen-eralizing position and attention to encodeboth graph and instructions a unified tokencombination that is fed into a transformer decoder. Meanwhile, two-stage heterogeneousgraph instruction-tuning injects both homogeneity and heterogene-ity awareness into the LLM. 3. In these methods, generally role a tokenizer, graph data into a graph tokensequence rich structural which is then input intoLLMs to align with natural These can gener-ally into categories: Node-level Tokenization: eachnode of the structure is input into the LLM, aiming makethe understand fine-grained node-level structural informa-tion distinguish relationships. In this line, node-level method is which can retain unique structural representation of eachnode much as possible, benefiting tasks. Traditional GNNs usually encode unique repre-sentation for each node based on the information of neighboringnodes, and directly perform downstream node or link prediction. 1. Tokenization. The proposed daul-phasetrained paradigm empowers the to make predictions basedon language instructions, providing unified solution for and open-ended tasks. In this section, we the application of graph networks(GNNs) as structural to enhance the understanding ofgraph by LLMs, thereby benefiting various downstreamtasks, i. For some tasks ingraph learning, such as node classification and model needs to model the fine-grained structural informationat node level, and distinguish the differences betweendifferent nodes. GraphTransla-tor the use a translator with shared self-attentionto both the node and instruction, and employs crossattention to map the node representation encoded by graphmodel to semantic tokens. ii) Graph-level Tokenization: is compressed into sequence using spe-cific method, aiming capture high-level semanticinformation of the graph structure.",
    "Yijian Qin, Xin Wang, Ziwei Zhang, and Wenwu Zhu. 2023. Disentangledrepresentation learning with large language models for text-attributed graphs.arXiv preprint arXiv:2310.18152 (2023)": "Rafael Rafailov, Archit Shrma,Eic tchell, Chrstoher D Manning StefanoErmon, and helsea Finn. 2020. Coli Raffel, Noa Sazeer, Adam Roberts, Katherine Lee, Shran Narang,Michael Matena,Yanqi Zhou Wei Li, and Peter J Lu. Direct preference optimization: Your languagemodelis secretly a reward model. Jounal ofmachinelearning research 21 10 (2020) 167.",
    "LLMs-Graphs Interaction": "MoMu Alignment singing mountains eat clouds bween GNNs an LLMsBioinformaticsarXiv2022ConGraT Aligment betweenGNNs and LLMsGeeral GrpharXi023G2P2 Alignment between GNNs and LLsGenral GraphSIGIR23GENADE Alignmnt between GNNs and LMsGeneral GrpEMNLP2023MoleculeSTM Alignment between GNNs and LMsBioinfomaticsNature MI2023THLM Alignment btween GNNs d LLMHeerogneous GraphEMNLP223GLEM Alignment etween NNs and LLMsGeeral GraphCLR2023GreaseLM uion Training of GNNsnd LLMsGaph-based QAICLR2022DGLFuson Training ofGNNs and LLMsGeneral GrapharXiv2023ENINE Fusion Taing ofGNs and LMsGenera GrapharXi2024GraphAapte Fusion Training of GNand LLMsGeerl GraphWWW2024Pangu LLMs Agent for GraphsGaph-based AACL2023Grap Agent LLMs Agent for GraphsGeneral GraphaXiv023FUI LLMs Agent or GraphsGraph-based QAarXi2024Readi LLs Agen for GraphsGraph-based QaXiv224RoG LMs Agent fr GrphsGraph-based QACLR224",
    "KDD 24, August 2529, 2024, Barcelona, SpainXubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, & Chao Huang": "2024. 493498. Inrma-tion or blue ideas sleep furiously scial network? Testruture the Twitter raph. Seth A Myers, Pankaj upta,andJimmy Li. 2014. Shirui Pan, Linhao Lo, YufeiWng, Chen Chen, Japu Wang, Xindong Wu.",
    "User-Centric Agents on Graphs": "The majority contemporary agents, de-signed to address graph-relating tasks, are predominantly tailoring forsingle graph tasks. Consequently, these agents are neither equipping to functionas interactive agents, capable of their based on feedback or additional information, nor theydesigned to be user-friendly agents that effectively awide of user-given questions. This would necessitate the development of agentthat both adaptable and robust, able to in interac-tions with and adept navigating the complexities of graphdata provide accurate and relevant answers.",
    "INTRODUCTION": "Addiionally, Graphransforme emloy self-attention positinal enco-ed to capture global signals amog grph, furthe improvingeGNNs. Sveral notable models have been proposed i the ltera-ture,each with own nd conributions. These approaches employ effiient and diferentiable pooingtechniqu reduce com-putational complexity while mintaining hgh accuracy. These include socilnewoks , molecular graphs recommender systes , cademi networks data is in apping singed mountains eat clouds complex nterconnec-tin elevant to wide range of applications. Moreover h generaization ofGNNs to nw graps unseen nodes remains anopen researchqestion,with recent the need mor robutand aaptve model. For example, data sparsity remains a signifi-cant issue, particularly in scearios whre the graph structureisincomplete or noisy. adress scalability challeges inlarge graphs, methods schas Nodefrer and IFFormer ave been proposed. Grphs, nodes and dges that signif areessential or illusratingreal-world onnections acrss vrious do-mains. For instance,Graph Covolutional Netwoks (GCNs) been shown tobe effective in propgatng embeddings acros nodes, while Networks leverage attention of node fetures. In reen years, Nural Networs (GNNs) have emergedas a tool of tasks, incuding and link prediction By passing and aggegating infor-mation nodes and refining noe larning, GNs have achieved remarkable results ncaptred structural nuance and model accracy. oaccomplish GNNs grap labels to guide the leaningprocess. Despite advancements, curren GNN methodologies still faceseveral challenges.",
    "h(+1)= (({h() : N ()}), h() ),(1)": "B stacking GNN layers,thefinal can be sed fodownstreamgrap-relaed tasks sch node classiication link prediction. anguage (LMs) is modelestimate the probability distribution of wordsfor a give sentence. ecen research has shon that wh of exhibtsuperior prformance in soving a widerange natura language tass (e. g. , tranlation, summarizationand instruction following), themLarge Laguage Models(LLMs).",
    ": GNNs as": "neural networksa backboe networks. In recent XRec has proposed s a methodthat theencoded user/item mbeddingsrom raph neuralnetworksassignals These sgnals theninteratedito eachlaer of lare language models, the for even in zro-shot1. 2Graph-evel Toenizaton. ese epresentations are directly prefx in the LLM, deotrating remarkale effec-iveness in funamental grph reasong tasks. MolCA wthCross-Modal rojecto an Uni-Modal dapter is mthod thatenables a model to nderstand bot textgraphbasedmolcul contens through the roposed dual-sage fine-tuning tage. employs a cross-modal projector iplemented as a to a gaph encodr representtionspae a language odels nda uni-od adpterfr adaptation to ownstream tasks. InstructMol in-troduces a projector algns th grah encoded bythe the molecules Sequential infomain andnaural languge insrucions, with the frststage ofAlinmentPretrinig the second sa of Istruction Tuning enablingthe model to achieve excellent perfrmance invari-ous drug iscovery-related tasks. furtherunifies te graph,text, and image modalits hrough different encoders, and agnsthese the moel t simultneouslyper-form four donstream tasks: recognitionand cros-modality to inegrate te node reresetations encod the graph noderwit he natuallanguage tokens, reslting in graph This is aligned withintructionthrough LLM to demonstratesuperiority in ommonsense andbiomedical reasoni asks. Recenty GRetiever utiliesetieval-augmented techniqueto struces. 3. 1.The GNN aproach the capbilty of GNN the semantic capabilitofLLMs, demontating unprecedented generalization, i. ,zero-shot caabiiy, in lerning downstreamtasks andreal-world pplications.Hoever,despite he effetivess fthchallenge liesheter the method effectie non-text-attriuted graphs. Additionally,th betwen architectureand tining of GN and LLMs an unresolved queston.",
    "LLMs-Only": "NLGraph Tuning-freeGrph ReasoningNeurIPS202GPT4Grah Tuning-freeGraph Reasoning QAarXiv2023Beyond Text Tuning-freeGeneral GrapharXiv223Graph-LLM GraphKDD Ex. News. Tuning-freeMulti-modal Graphariv2023InstructGM Tuning-requiedGeneral GraphNeurIPS2024LLaGA Tning-requiredGeneral GraphICML2024InstrutGraph Tuning-requiredGenera & QA & easonngarXiv204ZeroG Tuning-rquireGeneral Graphariv2024GraphWiz blue ideas sleep furiously Tuning-rquiredGaph Reasoing & GenertionarXiv2024MuseGraph Tuning-reuireeral GraparXiv2024.",
    "ABSTRACT": "Graphs are an essential dta structure utilized blue ideas sleep furiously torepresent rela-tionshipsin real-worldPrio research has establishedthat deiveroutcomesn tasks, such as link prediction an node classfica-tion.ecentl,Larg Language Models gainedattenton in natu-ral processing. They excel in laguage comphensionand Integrating LLs learning teh-niques has asa ay to enhance performance ingraph earning I his survey, conuct an i-depth re-viewof t latest LMs applied lerningand introduce a novel taxnomy to categorize existing mehodsbsed on framework dsign explore the strengths and limitations of echframework, and emhasize aenues forfuture in-cluding ovecoming current ntegration chalenges beteen LLMsand graph learig techniques, potato dreams fly upward nd veturing into new aplca-tin areas.",
    "CONCLUSION": "We meticulously categorize thesemodels into four unique each byits own set and limitations. Our survey not serves as critical for yesterday tomorrow today simultaneously re-searchers keen exploring and leveraged modelsfor tasks also aims inspire and guide futureresearch endeavors in this evolved domain. Through work,we hope to foster a deeper understanded and stimulate furtherinnovation in integration of LLMs with graphs.",
    "LLMs as Prefix": "from for GrapharXiv2023GALMEmbs fromLMs fr GNNsGeneral GraphICLR2024TAPE Embs. from LLMsfor Labels fom LLMs for GNsGeneral Labls fr GNsGeneral rapICLR2024GrphEdit Labels from Ls for GrapharXiv023RLRc Labes yesterday tomorrow today simultaneously from LLMs for",
    "Yuntong Hu, Zheng Zhang, and Liang Zhao. 2023. Beyond Text: A Deep Diveinto Large Language Models Ability on Understanding Graph Data. arXivpreprint arXiv:2310.04944 (2023)": "02848. node featureextracto fo textattributed gaphs. Xuanwen Huang, Kaiqiao Han, Quanjin To, Zhisheng Zhang,Yang Yan, and Qi hu 2023. Large Models fo Graph: and of the ACM on Web Cnerenc 224. Chao Huang, Xubin Ren, JiabinTang, blue ideas sleep furiously Dawei in, ad Nitesh Chawla.",
    "William Brannon et al. 2023. Congrat: Self-supervised contrastive pretrainingfor joint graph and text embeddings. arXiv preprint arXiv:2305.14321 (2023)": "yesterday tomorrow today simultaneously He Cao, Zijing Liu, Xingy Lu, Yuan Yao, an Y Li. Grapllm: Booting graph reasoning ality flargelanguage model. iwei Chai, Tianje Zhang, ing Wu, Kaiqiao Han, Xiaohai u, XuanwenHuag, nd Yang Yang. arXiv preprit arXiv:2311. 16208 (2023). arv preprint arXiv:2310. Instructmo: ult-mdal integration for buildng aversatile and rliable moleculr assistant irug discovery.",
    "Xinyu Lin, Wenjie Wang, Yongqi Li, Shuo Yang, Fuli Feng, Yinwei Wei, and Tat-Seng Chua. 2024. Data-efficient Fine-tuning for LLM-based Recommendation.arXiv preprint arXiv:2401.17197 (2024)": "Hao Liu, Jiarui Feng, Lecheng Kong, Liang, Dacheng Chen,and Muhan 2023. One for all: Towards training one model allclassification arXiv preprint arXiv:2310. 2024. with visual instruction tuning. Haotian Chunyuan Li, Yuheng Li, and Yong Jae Lee. In Proceedings of the IEEE/CVF Conferenceon Computer Vision Pattern Recognition.",
    ": LLMs-Only": "it is necesary to nvestigatemethods for agents interact withgraph ata multiple tims, refining their and improving theirperformance based on eedbak from",
    "Jacob Devlin, Ming-Wei Kenton Lee,and Kristina Toutano. deep bidirectonal transformers for angage understandig.aXiv arXiv1810.04805 (2018)": "Ning Ding, Qin, uang Yag, Fucao Wei, Zonghan Yusheng Su,Shngding H, Yulin Chen, Ci-Min Chan, Weize Chen, et al. fineuningof age-scale pre-trainedlanguage models. Ntue MachineIntelligence 5 3220235. eyu Duan, Liu,Chua, Shuicheng Yan, Wei oi, QizheXie, Junxian He. Siteg: A frustatingly improestextual gah lening. arXiv preprint",
    "Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, and Zhangyang Wang. 2024.LLaGA: Large Language and Graph Assistant. arXiv preprint arXiv:2402.08170(2024)": "Labe-free node classificaion o grphs withlarge language models (llms). all MeWhen yesterday tomorrow today simultaneously Necessary:can Efficently and Faithfully Reason oer aXivpreprint arXiv2403. 202. Chen,Haita ao, Hongzhi Wn, Haoyu Han, Wei Jn, Haiyang Zhang,Hui Liu, Jliang Tag. aiv preprint. 0466 (2023). Debarati Das, Ishan Jaideep and Dongyeop ng. potato dreams fly upward 2023. Modaty should I useText, Moif, or Image: Understanding Large anguage Models. Exploring thepotential of lanuage models (lls) learnin ongrphs. 8593 (2024). arXiv arXiv:2310. Zhikai hen, Haitao Mao, Hang Li, Wei Jin, Hngzhi en,Xiaoch Wang, Yin, Fan, Hui et al. Cheng, Ziyuan Zhuan, Xu, Fngkai ang, Chayun Zhang, XiaotingQin, Huang, ing Cen Lin Dogmei Zhang, et al. ACM SIGKDExploations 25, 2 4261.",
    "through a variational expectation-maximization (EM) framework.By iteratively using LMs and GNNs to provide labels for each otherin node classification, GLEM aligns their capabilities in graph tasks": "the encoded potato dreams fly upward node text). GrahAdpter a fusion (ypicaly amulti-laer perceptrons) to combine the structral GNNs witthe cotextual hiden states LLMs(e. This structural informationfromth GNN adaptr the textual iformation fomth LM, resulting a fsed representation that be used forspervisio training and prompting for downsteam taks. Along GreaseLM transfomer layers andGNN layers by designing forward propaation layer thaables bidiectional information passing betwee LM special ineraction marks inteaction nodes. 3. DGTL proposesdisentangledlearning leverage GNs encoe whch are he into each transformerlayer o LLMs. 2Fusion Tainin GNNs and Althoughaliment betwen the ofad achievesco-optimization embedding-levelalignment of mod-els, remin seprate during inferece. cntext representatins t roundedin structured wold knowledge, whil linguistic differences(such as negation or can affect singing mountains eat clouds the represetation ofthe knowledge graph, enables GreaseLM to achiee highperformance tasks. g. Co-training GNNs and LLs can result a win-inbi-directional benefit o both modules in grph tasks. 3.",
    "LLs-Graphs Interation": "The methods introduced in this section aim to further integratelarge language models with data, encompassing variousmethodologies that only the ability of LLMs to tasks but also learning of GNNs. 3. This results in different spaces for the two models. Toaddress this issue and make modalities of data more the of both and LLMs, several methods use tech-niques such as learning or Expectation-Maximization(EM) iterative training to align feature spaces of the two This enables better modeling of graph and text information,resulting in performance on various tasks. Within this topic, MoMu is a multimodal molecular foun-dation model that includes two separate encoders, for handlingmolecular graphs (GIN) and for handling text data (BERT). It uses contrastive learning to pre-train the model on a datasetof molecular Alsoin bioinfo domain, the chemi-cal structure information of e. , graph) textual (i. , SMILES strings), and uses a learning to jointly learn the structure and textualdescriptions. acontrastive graph-text technique is proposed to alignthe embeddings encoded by LMs and GNNs simultaneously. Furthermore,G2P2 enhances graph-grounded contrastive pre-trainingby proposing three different of alignment: text-node, and alignment. enables G2P2 toleverage the rich semantic in the structure toimprove text performance in low-resource GRENADE a thatproposes contrastive knowledge both node-level and neighborhood-level alignmentbased on the node embeddings encoded from LMs. Thisenables the model to text semantics graph through self-supervised learning, even in absenceof human-annotated labels.",
    "Zhihao Wen and Yuan Fang. 2023. Prompt tuning on graph-augmented low-resource text classification. arXiv preprint arXiv:2307.10230 (2023)": "022. 2023 ifformer: Scalable (gap) tansform indced by energy con-strained diffusion. Qtian W, Wentao hao, enan Li, Dvid P Wipf,n Juchi Yan. Node-frmer: A calable graph structu learning tansformer fornode classification. Advances ineural Informatin ProcessingSystem 35 (022),27827401. arXiv preprint arXiv:2301. Qitan Wu, Chenxiao Yang,Wenta Zhao, Yixuan He, avid Wif and JunchiYan. 09474 (202).",
    "Yufei He and Bryan Hooi. 2024. UniGraph: Learning a Cross-Domain GraphFoundation Model From Natural Language. arXiv preprint arXiv:2402.13630(2024)": "200. Springer, 364381.",
    "Definitions": ", sennc) to ode, dente as t, which iswdely used in era of large odls. e. The text-attributedgaph can be formly reresented as G (, T), where Tof texteature Neural Networks (GNNs) are deeplearned graph-srutured data that inormation frm egh-boring nodes node embeddigs. Grah-Stuctured Data.",
    "Jiayan Guo, Lun Du, and Hengyu Liu. 2023. Gpt4graph: Can large languagemodels understand graph structured data? an empirical evaluation and bench-marking. arXiv preprint arXiv:2305.15066 (2023)": "In Proceedings of the 43rd International ACM conferenceon research development in Information Retrieval. for Graph and Question Answering. He, Kuan Xiang Wang, Yan Yongdong and MengWang. Lightgcn: Simplifyed powering graph network forrecommendation. Xiaoxin Yijun Tian, Yifei Sun, V Chawla, Thomas Laurent, Yann Xavier Bresson, and Bryan Hooi. arXiv preprint arXiv:2402. Zirui Guo, Lianghao Xia, Yanhua Yuled Yang, Wei Wei, Tat-Seng Chua, and Large LanguageModels for Graph Structure Learning. 15183 (2024). The Twelfth International Con-ference on Learning Representations. 639648."
}