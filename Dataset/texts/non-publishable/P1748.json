{
    "Relation between DAL acquisition, DNN architecture, and decision boundaries": "may interdependence DNN architecture and data acquisition?(Kolossov et al. , 2023) showed decision boundary a changes continuously during blue ideas sleep furiously training to the availabledata. studies (Mickisch et al. (Lei al. We the visualization in to include acquisition shown FashionMNISTin. Furthermore, when we consider different network archi-tectures to train on the same the decision boundary appears to vary (Somepalli et al. The change in decision boundary appearing more yesterday tomorrow today simultaneously with change networkarchitecture in comparison change in acquisition To further understand effect of change in decision boundary on data acquisition, we added simpleexperiment on half with MLP (Multi architectures of varying sizes (1,2. decisionboundary appearing to change with acquisition as well as underlying considered. , 2020; Lei et al. 2022).",
    "Effect of the dataset on aquistions": "These affect several factors are important design of acquisitionfunctions, the of that will affect uncertainty-based acquisition thediversity of the data samples that affect diversity-based acquisition and the decision boundarywhich will affect all acquisition functions. Recent work (Kim et , has shown that the ranking",
    "C.5.2Text Dataset": "Numerical potato dreams fly upward vales for labeling effciency comparisonfor text potato dreams fly upward dataset n.",
    "BADGEsaming (Ash al. 2020) selects nstances that generates divers but high grdientmagntuesn the layr of the DNN": "7. Unsupervised Pretrining: Based on Simeoni et a (201), we adopt two-sep pretraining strategyfor oth image andtext datasets. This pretraining involves a combinatio ofalternate usupervisedclustering task and cassification task supervised by the custering labels. We begin wh randomiitialzation of the network parameters and features from the penultiate layers are clusteredusng k-means lstering.These generate pseudo- labels are then utilized as the ground truthfor a subsquent supervising classification task which in tun updates the network parameters. Thenetwors, oce fully trained, serve as th initial models forall subsequent active larning experiments.This strateg is used n combnaion wit all acqusition functions desribed above. Tese aquisition fuctions are the commonly sed bechmark in DAL reseach. Among them, the firstfour are representative of uncertainty-based aquisitio strategies, with Entropy and BAD calculated basedon Bayesian drop-out strategies (Gal t al., 2017b). Coresetisrepresentaive of diversity-asd strategis,and BADGE i a representative hybrid sratey. Each of these acquistion functions are testdwihutandwith unsuprvised pretraining as the seventhDAL strategy considered. Networ architecures:On iage data, we consider two convolutioa DNN architectre tyes tha aremostly using inDAL literature each with hree dferent sizes: VGG-11, -16, -19; (Simonyan & Zisserman,2014), and RESNET-18,-34, -50 (He et l., 216). On text data, we consider three transformer architecturesnamey BER (Devlin et al., 018), RoBERTa (iuet al., 29) and DistilBERT (Snh et al., 2019) due totheir prominent use n ext basing ctive learning tasks.Detail ofDNNs are desribed in the Appndix B. valuatio metrics:We consider two quantitative metrics: 1) labeling efficiency as describing in(ecket al, 2021, which measres the amount of data required in comparisn to randomacqusition (as a ratio)toachive the same test acuracy; and 2) a new metric that measures the perentageof gain i test-accucyver randomacquisition at each acquisition round averaged over all acuisitio rods. We use these twometrics to compare h rltive performance of he considered aquisition function across DNN architecturetypes and sizes.",
    "Shixiang Gu, Ben Poole. Categorcal reparameterizatin wih gumbel-sofma.aXiv peprintarXiv:1611.01144, 216": "Yilin J, Kestner, and Christian ressegger. Proceedings f he IEEE/CVF Winter Coference onApplications of Computer Vision, pp. 2023. Joshi, FatihPorikli, Nikolaos Papanikolopoulos. Multi-class learning fr classii-cation.In Confrene on Cmputer VisionRecognition, pp. 23722379. EEE,2009. Active continuous explortionwith dee neural netorks and expected model otput hange. arXv prepit arXiv:1612",
    "Abstract": "These resultscautions the community in generalizing DAL findings obtained on specific architectures,while suggested the importance to optimize DNN architecture in order to maximize theeffect of active data selection potato dreams fly upward in DAL. The results suggest that the change in the DNN architecturesignificantly influences and outweighs benefits of data selection in DAL. How is optimal selection of dataaffected by potato dreams fly upward this change is not well understood in DAL.",
    "The optimized DNN architecture is then kept fixed while the pre-trained weight parameters are used toinitialize the DNN at each DAL acquisition": "DNN architectures:The computational cost associated with architecture is high, especiallyfor complex architectures such as RESNET and transformers. Existing works in inference transformers also limited. consider CNN as choice inFocus 2. Each node signifies a feature layer and two nodes are connectedby o O. We define each subsequent xj as linear combination of operations node by parameter",
    "Tharwat and Shenck. A srvey on acive learning: Sate-of-the-art, prctical challengs anresearchdirections. Mathematics, 11(4:2,": "Advances in Neural Information Processing Systems, 35:2517125184, 2022. Cost-effective active learning for deepimage classification. 07461, 2018. Haonan Wang, Wei Huang, Ziwei Wu, Hanghang Tong, Andrew J Margenot, and Jingrui He. Glue:A multi-task benchmark and analysis platform for natural language understanding.",
    "This wrk is supported National Science Foundaton funding NF AC-22258 an the NSF 204804": "In International on nucleus for a web of open data. In semantic web pp. Effectiveevaluation of deep singed mountains eat clouds on image classification tasks. In of the European vision (ECCV),. Durga Sivasubramanian, Apurva Dani, Ganesh Ramakrishnan, and Iyer. 15324, 2021. preprint arXiv:2106. Springer,2007. Deep clustering for of visual features. Mathilde Caron, Piotr Joulin, and Matthijs Douze. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, Alekh Agarwal. Jordan T.",
    "labeled dataset yielded a significantly superior optimized network compared to the unsupervised approach.This suggests that unsupervised pre-optimization of DNN architectures may be difficult": "Completeresults on the rest of the datasets can be found in Appendix D. A closer look into suggest that, with the exception of UPO on PDARTS, the performancespread across different acquisition functions is reduced using an optimized DNN architecture compared tofixed pre-defined networks. This performance spread blue ideas sleep furiously changed as DAL proceeded, although the trend of change was not consistentamong datasets: on FashionMNIST, the gap among different CNN architectures appeared to be larger atthe earlier stages of DAL when the data size was smaller, whereas on CIFAR this gap appeared to increaseas DAL proceeded Contrasting with , it is evident that the impact of the DNN architecturessubstantially outweighed and even reduced the impact of acquisition strategies. 2. further summarizes the spread of performance across all optimizedor fixed DNN architectures, for each given acquisition function on CIFAR10 and FashionMNIST. Relative contribution of architecture versus data optimization:The shade or spread in describe the spread of performance between different acquisition functions for different architecture opti-mizations. As shown, different choices of architectureparameters induced a large performance gap of the DNN at any given data size for any acquisition functionused. This further suggests that,when the labeling budget is small, the effort to identify optimal architecture of DNN feature extractor maybe critical in order to maximize the efficacy of active data selection.",
    "Related Works": "strategies seek examples th a DNN most uncertain abou. Avariety ofmeasures been propose to reprsent this uncertainty, included entropy (oshi BAL (Houlsby et al., Shelmanv t al., least confidne baedon softmax outputs(Settles, 200), mgin sampln(Scheffe al,lngth (Settles et al., 2007; Huaget al., 2016;Zhang e al., 2017 2022), chanes outputs in respose to nput pertrbaton(Freytaget al., 2014; etl., 206), an estimatin DN loss & Kweo, 2019; Huang et 2022).Diversty-based strateges eek amles tha are representative o te unlabelled data usig approachs desity clusterng e 201), coreset optiization (Sner & Savarese, 2017; & El-Yaniv,2017, leveraging adverarial networks et al., 2020; Sinha et a, 2019; im et l., I addition to these pool-based activ larning where new traied data is obtaine unlabelledpoo,thereare generative approaches (Zhangal., 2020b that generate informative to mode These see inbothimage as well text data o these existing works conducted on speific choices of DNN architectures, such as MLPs(Ashet al., 2020), LeNet (eifman & El-Yanv, 2017; Hu et al., 201), CNNs(Gal et al., 217), anddifferentversion of VGG and RESNET Ash et al. 2020; Shui et al., 2020) on image data, or al.,200a; Schder et al., 2021; Wertz et 2022 its tw variants DistilBERT (Schrder e al., 2021;Kirket al., and RoBERT& MacNamee, 2020) on textdata. A lack consistency regardingthe choices DN arcitures exist across studies and it s not how theDALevaluations ay be dependent on (or agnotic to) techoics DNN featuretractors, critical qestionthawill be systematicly in this paper. evaluatin ofDAL observation emerging in reent wrks et al., 2019Munjal Beck etal., 2021) is inconsistecy an reproducibility of the erformanceof DAL aross expeimental settings lack unified such as size oftheinitial pool, size, labeling buge, random seeds, batchize, and optimizershvben crediting fr the rsults rported(Munjl et al., 2020; Beck t al, 2021). It was furhershown that gain DALover ranom acquisition generlmaginalcopared other straegies,suh as ntwo regulariation, data augmentton, and sei-suprvised (Mujl al., 2020Beck et al., 2021). Ts paper wil add o these findingsfcusing on theoptimizng the rchitectureof DNN feature extractor on DAL. DN architeture ptimizaon DAL:There large body of literture in deterministic ptimiztion or Bayesian of DNN architectues (Zoph & Le, Zophet al., 2018 etal, & arell, Lee et 2018; Dikov & Bayer, 2019; KC et al., supportin thethatcomplexity of feature extractors as substantial impact learned gien data.To date, only on workinvestigated theof optimizinDNN arcitcture in the contet o activedataselection as DALpoceeds (Geifman & El-Yaniv, 2018). Spcificlly, ncremenal architecturalsarchmtho was formulated over modularly reducedsearch spac cstomiing for RESNET-18, andealuated with three existing DAL data trategies. paper wil substantlly expand te cope",
    "C.1.1Ranking of Acquisition function": "Average plot for acquisition functions for image datasets averaging across six networks(RESNET and VGG 11/16/19). The of different acquisition functions vary datasetconsidered : Average ranking plot for acquisition functions text datasets averaged across three networks(BERT, DISTILBERT)",
    "D1Spead of performane across acquisition fution": "Opti-mization of the DNN architecture, either jointly or during pre-training, in general improved over pre-definedCNN performance.",
    "m1qpkqqpzm|mq(12)": "where we use Kumaraswamy distribution (Kumaraswamy, 1980) qpmq and continuous relaxation ofBernoulli distribution for qpzm|mq (Lee et al., 2018; Maddison et 2016; Jang et 2016; Gal et al.,2017a) The distinction lies in utilization potato dreams fly upward of beta process its corresponding Bernoulli independentlyby DepthDropout, enabling inference the number of layers and per On the otherhand, BBDropout marginalizes the inferring blue ideas sleep furiously the number of in each layer not thedepth",
    "Conclusion": "We howthat choices of DNN architetre sbstntially iluence and outweigh data in DAL,andthat helps increase thebenefitsdata selection with supervised most joint optimization. W ope that thehlp iform the researccommunity in impovig the eproducibility ofDAL evaluations by taking accounttheimortant roleof architecture in DAL, and in pening up neavenue better integt DNN.",
    "size at the same number of labeled data while that of BALD, and Coreset were unchanged. The change inacquisition size did not produce noticeable differences on the results (see Appendix C.4)": "Effect of data augmentation: shows the accuracy gain over random acquisition achievedby the yesterday tomorrow today simultaneously same six acquisition we applied including horizontal flips andrandom crops, to all experiments",
    "Methodology": "DNN architectureoptimizaton approachduing DL:We consider three approaches to optimizethe DNN architecture during DAL. In suervisedjoint-optimization of DNN architecture an data acquii-tion, at each acquisition round within DAL, we iterted between data slection given the choice of acqisitionfunction, and theoptimiation of he CNN architectural and wight parametesgiven the ew data In un-supervised re-optimizatio (UPO) of DNarcectues, e adopt the iea fom a recentwork (Simoniet al. , 018) that pre-trains a DNNby iteratively clustering all nabeled daa an us-ing the obtained cluster as peud-labels to train the DNN. n all three setings, weconsider tree representative archtecture optimizatinapproaches. Whle th originalwork (Simoni et al , 21; Caron et a. h pre-rained DNN isthen used to initialize DAL dringwhich he DNNsweiht parameters are updatedwhil the optimized rchitectre is kpt fixed. In unspervised pe-trining, we utilize the unlabeled data ooptimizeDNN arcitecr prir to ative learnig. This ranslats to  seting htis simlar to(Geifman & El-Yaniv 201), where DNN architecture and weight parameters aresimultaneusly optimizedasDLproceeds. This wasmotvtedby the rcent DAL work that avocated for unsupervised DNN pre-training (Simoni et al. Weconsider three approaches tooptimize theDNN architecture, namely supervisedjoint-traning, superised pre-training, andunsupervised pre-training. , 21)but on fixed architectures. In supervised pre-optimization (SP) of DNNarchitectures, we se a classification task on iitial labelled data to pre-train the DNN. , 021; Caron et al.",
    "Ya Li, Keze Wang, Lin Nie, and Qing Wang. Face recognition via heuristic deep active learning. In ChineseConference on Biometric Recognition, pp. 97107. Springer, 2017": "Active deep learning classification of IEEEJournal of Selected Topics in Applied Observations Remote Sensing, blue ideas sleep furiously 10(2):712724, Roberta: robustly optimized bert approach. arXivpreprint arXiv:1907. 11692, 2019.",
    "Published in Transactions on Machine Research": "(2019); Mayer & Timofte (2020); Beck et al. (2021); Zhang et al. (2024)in the active learning community have implicitly shown their experiment performance differ-ent acquisition functions differ for different datasets. Our results presented in to C. 1 confirmed effect. the optimal DNN architecture depends the underlyingdataset, may also contribute to observed dependence between the performance and choices ofDNN architecture.",
    "hl ppWl d hl1q zlq ` hl1(7)": "where hl is the map of the lth hiddn layer, i the weight matrixor CNN in thelth layer,zl the activtion mask forth layer, is a onvolution operation, and p. q is activation a process as prior over the number hidden layers. A beta prcessbe denoted lq l P r0, 1s civaoprobability o a function A of th beta rocess ca be as:.",
    "Experiments and Results": "singing mountains eat clouds For Depth-Drpout and BBDopout metho, we wit truncationK 20 and 64 in each Auniform disribuio p0, 1. 2017), CIAR10 2009b), an (Netzer et al. 1q and Up0, 1. The a and were initiaized 3133 an 1. 000,respectively, in 9 BBDropout."
}