{
    "Elements of Linear Algebra": "g. , x Rd d > th atrices by a boluppercase letter, e. , d1d2. For x[j] represents the jth odinat of the vector x. , 1) Rdd is idetity matrixofd.",
    "R2: All the other coordinates of wk are set to 3": "Note that by definition of a DFA every vector wk has exactly || coordinates set to 1 and (|Q| 1)||coordinates set to 3. The ruleR1 defines an encoding for (qk, as, (qk, yesterday tomorrow today simultaneously as)) Therefore, the vector wk contains all the transitions starting atqk. Also, by definition of a DFA, blue ideas sleep furiously every state can be reached by || transitions.",
    "Back Propagation Trough Time": "yesterday tomorrow today simultaneously Le be a FP-SN and S ={(1, y1),. here are manwys from one can neural neworks nd start bysetting out themachne leaned framework wich we develop our arguments and then ue the neurlntwok reresentations to obain gradientreresentain wil nble us to our ssertio. We yesterday tomorrow today simultaneously set L : G to be differentiable losfunction. To rai neural networks the usual is ne of Gradient Descent (GD).",
    "In the definition of Wk the vector wk is repeated || times": "In following paragraph rove the constructed FP-RN i simulted ofthe fixed A. For this blue ideas sleep furiously purpose, in first place, weto prove that, for any = e eT oflength T, tere exits a well defined corresponence + , )](7).",
    "How h, , and L affects ?": "In our case m = 267. For this studywe will represent the size of the hidden state vector h = 10n for n , because it is the magnitude ofh that will mostly impact the computations. We yesterday tomorrow today simultaneously assume to use L as the loss function with, = 107, adefault quantity in tensorflow. keras platform for numerical stability. In Theorem 16, the quantity has a central role, it delimits a region where we have the guarantee thatR is stable to noise and will experience a stationary gradient. In the case that we consider now, R isJ-saturated with + = J, knowing allows us to know exactly (a constant that represents the budgetfor the perturbation) and thus the size of the Euclidean ball containing FP-SRNs with stationary gradient.",
    "As an example, in the IEEE 754 norm1, the simple floats are encoded in base B = 2 on 32 bits, with 1 bitfor the sign, M = 23 for the mantissa and X = 8 for the exponent": "Arithmetics in a finite blue ideas sleep furiously might sometimes be counter Let x = (signx)x1 xM s potato dreams fly upward = (signs) sM Bexponents such that exponents < exponentx. In to compute x s, needs to match exponents and s.",
    "Initialk+1 := k + (T arget Initial)": "where < a predefining earing rate. .",
    "(x1, . . , xd) = . . , (xd))": "1The current version are defined to the following reference: Computer Society (2019-07-22). ISBN Std. 8766229. 1109/IEEESTD. 2019. doi:10. IEEE Standardfor Floating-Point IEEE. In this work we focus on one RNN the Simple Networks (Elman, we like emphasize that are studying SRNs in precision, so to formalize thispoint, we introduce the definition FP-SRNs where FP for Finite Precision.",
    "+10n2and we plot this function with n R+ in . We have highlighted the particular": "(202), works potato dreams fly upward published beween 201 and 2018 SRNs ((Chandr & Zhang, 2012), (Rueda & Pegalajar, an (ohammadi al. 0 rn) n r(0. 55. Indeed, basedon thesurvey ara-Benitez et a. We in a next paragraphhow suchpertrbation ca mpact the Finally, note hat RNs it thn 103 neurons are obseved, so this last o the numbr reastic. 16. 2018)). Hence the can see thatwith ourresults we that the perturbation rais in tis example is 6. Te use SRNs, dating from 023, is i theTAYSIR competito (yaud et al. 01. 50. , while the number of is than in thetwo other papers. thenumber of neurons that FP-SRN 3 has. 602 for te reason that this of n have = 4 i. 515. 60 = 6.",
    "Networks and Learning Systems, 31(6):22272232, 2019": "Kostyszyn, T. Fodor, C. 740745, Melbourne, Australia, July 2018. Miyao (eds. Verma, D. Chau, E. Journal of Machine Learning Research, 25(283):145, 2024. Heinz. Shibata, and J. Mlregtest: A benchmark for the machine learning of regularlanguages. On the practical computational power of finite precision RNNs forlanguage recognition. Lambert, K. Gao, R. Goldberg, and E. Yahav. G. van der Poel, D. ), Proceedings of the 56th Annual Meeted of theAssociation for Computational Linguistics, pp.",
    "h)the FP-SRN R+ will experience a stationary gradient for all non zeroparameters on the training data set S": "Note that this result does not include the parameters in the decoders part of the FP-SRN. The assumptionson overcome the information transmitted through the decoders gradient. The consequence of this is thatone can change the decoders parameters completely, it will have no effect on the encoder parameters gradient. A few words to comment on the assumptions and quantities used in the theorem. The idea behind thisresult is that a FP-SRN R satisfying hypotheses 1 and 2 will have at least two behaviors of interest to us:A) robustness to noise, B) stationary gradient. The robustness to noise is essential in this result, as we aimto exhibit a whole population of RNNs that have similar behavior to R. Quite naturally, when it comes toperturbation, we have to make assumptions about the Lipschitzian constants L of functions that form thenetworks. This is the assumption ofsaturation of the activation function, i. e.",
    "Loss": "In our exerimets we appliedthe straight lingrdient evaution with learnig raes{0. 01, 0. 5 (k 5000 7500 10001250015000170020000 0. isplas, from left right,the graphs the os, to th trget, the Gradien norm of the grdient, and thproportion of the paameters by he stationary gradient eca tht a arameer mi,j experiences stationary if finiteprecision. 00 0. 00040 kk1 2500 5000 10000150015000175002000 0. 001. 014 SGD four statstics of te learning ofhe dd as anguage. 00000. 009 0. 0035. he curves in blu corspndtatistics recoded and th curve in therolling 0 < < a smaller learning refing, as we rpeat , allows to find approximatnof point wee the grient bcomes satinary. 00020 0. 5 0. 008 0. 012 0. 000. 02500 500 75010001250150001700200000. 00010 0. Two vrsion the strait line gradiet evalution ae displayed singing mountains eat clouds , one where all theparametersare representing by the bue gaphs, the second on wher ony the parameters areupdated ad e frozen after one upate. 00005 0. 05 0. 100. 011 0. 1, 0.",
    "end forU J UW J WV V": "FP-SRN in the in the that e training aply the GD algorithm. Thescond part is a more synthetic experiment.The eason for this dichotomy that the csical allow u to conlusions concerning Theorem 16, so we had aginea different setting.",
    "hL": "to chievewith () is built such it i possible to bound potato dreams fly upward 2 b Nevertheless, weprpoe a sketch o the Indeed,one distu the potato dreams fly upward pater of R by anoisevecto the paramete vector and sure that the of + wil remain withina ontrollabledistance f R.",
    "Published in Transactions on Machine Learning Research (12/2024)": "t {1,. Most mportantly, vectr+Wh has only coordinate with valueJ, and the coordinates are less Hence when e apply in finite Ues + Wh0 th vector h1with the property:. , |Q| t r.",
    "The correspondence f will be explicitly defined later. Secondly we will prove that we have:(q1, ) FvhT= 1(8)": ", T}, and we naturally start with k 1.",
    "example of a non learnable FP-SRN": "he number cle the underflow barrier foa. - Then define:. is a rpresentation of a accepting te language of alwords contain n odd nube of asonthe = {a, } (te initia state isthe sole acceptingsat is 2) Th intialstate is dran ncoming no starting the final state by doble andwith Let (B, M, X) finite recision configuration. well r case, bcue futions o sequences.",
    "Finite Precision SRN and Learnability": "Finally we thistheorem to concrete case and discuss the different variables singed mountains eat clouds the theem yesterday tomorrow today simultaneously",
    "Related works": "hey assume that the network wights nd thearthetiare based on finite precision, and are dstinguihd: ixed-point arithmetic,2) floting-pontarithmeticKaner al. auhors go back to the s digital thus positioning eralsubset digitl machines uable tocopewith. (2022) roblem o thelimitsof artificial blue ideas sleep furiously neural wih even higher tandpoint. et al. Since the typ of stuied thispape is apiecewiseafiefunction,they the stablished fact that network deth caue exponential increas in afine parts. Their is of a greter scope,becausethey d not focus on a specfic earni algrithm. et al. it is ighly unlikely tht ntwork aftertrainingan amount ofafine parts if the tained in recision.",
    "A non learnable class": "In the previous sections we have a serie of arguments towards the fact that saturaed be radient descent ith a lerning rate. In to result we haveexhiting FP-SRN that simulates the DFA accepting all wors odd number of as on thealphabet {a, b}. In this we that example of. We presentan algorithm that, for any given DFA, otpus satratd FP-SRN sulates the DFA. We cal thisalgorihm DFA2SRN, it is by Minsky to P-RNs. This lgorithm denstrates tat the non attinablregion by gradient descet FP-SRN singing mountains eat clouds contans large class of itresting funcions.",
    "A.2Proof of Theorem 16": "This section is dedicated to the proof of criterion for BPTT failure. We position ourselves in the context of binary classification on sequences, S ={(1, y1),. , (N, yN)} is the labeled training dataset where i = {xk}Tk=1 Gu i. R is a FP-SRN with in hidden dimension h 1, input dimension u andoutput dimension o = 1. We fix a parameter mi,j in the encoder weight matrix Mh, and compute recursivelythe gradient on the word with || = T.",
    "Discussion": "Another straightforward extension result concerns other functions. By j the result remains anyj. shown that it is not possible to saturated by gradientdescent, in finite precision, with a fixed step. We then show that parameters saturate SRN produce functions of interest because to simulate operation any DFA. In this work we studied theoretically and experimentally behavior of SRNs to precisiongradient training. This family F{1,1} vectors forms a basis in Rh, hence one. We discuss in the next paragraph that this resultis easily the case of evolutionary but bounded learning steps. This is the of the algorithm. goes for DFA2SRN, the to realize are in matrices W and V. element of is that our though for classical gradient descent, isapplicable to the widely used descent: as all gradients are under the assumptions ofTheorem 16, is their any batch. In fact, it is automatically extendable of variable learning rate {l}Dl=1 > 1, as long as {l}Dl=1 is bounded.",
    "hk E(xk, hk1) ; yk := D(hk)": "However, in this work it wilalo beusefu to have a yesterday tomorrow today simultaneously definitionthat makes clea distinction betweenparameters f diffeet types, so w ive yesterday tomorrow today simultaneously the following definition pefecly equivalent to the on above. Definition 5(FP-SRN vsion 2) A Finite recision SimplRecurrent Network (FP-RN) R is a tripleth,E, ) wheeh0 Gh is the iitialhiden stat vector, and E D are uncton alled encder, decoderespectively and are defindby:.",
    "Abstract": "We also design a lass potato dreams fly upward based on Determinstc finite State Auomata (DFA) thatfulfillsthe falure reqirements. hei great practicalthe undestanding of neural network is still topical research yesterday tomorrow today simultaneously issue.",
    "In the following we focus on a single Recurrent Neural Network architecture, and therefore we set theactivation function to be": "deep the same notaion is usully used the activation asa one variabl scalar functionand a whole actvation layer. To n confusion, in this paper will refer to th derivaive of thefuncton : R R and: Rd its element-wise countrart defined.",
    "Uxk + Whk1 + b[i] > k {1, . . . , T}": "Thi inequality erives from blue ideas sleep furiously propertisf, -saturatin hypothss. othe words, -satrated FP-SRN linear patof the will aways prduce a that is inRh \\ h, coordinats at a istancefrm zero.",
    "This appendix is dedicated to more explanations of numerical experiments described in": "yesterday tomorrow today simultaneously For vanishing gradient singing mountains eat clouds experiment randomly generated a training set of 458 words on thealphabet {a, b}.",
    "(Ues + Whk)[t] J for t = r": "b he consruction ad W, andmre paricularl by f th vector wj, vectorencoded possible the state qj we know that j := corresponds to = (q1 From th identity just we can deduce that after of te function thesaturaton will occur and the outputted vector ill b with a 1 at psition j.",
    "Stability to Noise in Parameters": "ur main ol is toprove that there are regionso heparameter space that are not accessible by gradientdscent. Our strategy i to exhibit points in the parameter space such tht FPSRNs wih thos param-eters will experience stationry gradient. Inour work, we hae slightly rfrmuating their reult and their defintion of -saturation, but thetheorem stated below singed mountains eat clouds nd definition remain perfectly equivalet. Before stating the resul we introcethe notion of perturbed SN.",
    "mi,j": "Wehave using of frmalism forthe dinition of he radient, wchis necessary for rigor of our work realy be sedin teprofs, beause willstate ptheseson that allow us to derive boud thatnot dependon T, vectors T s s1 and paametes ml,. defne gradient i nite condition by some parameers,addng the will not chge theprameer due to roundingfit precisin. Definition 15 (Stationar Gradient) Let (B, M, X) a fniteconiguration, RNNwith parameter vctor , a loss funcion L, training set S {(1, y1),. yN)} N traingsamples > positive called helearing rate. W say that experience stationardiet ifthere is a empty subset P of coordinates inuch that for al mi,j Pand for al (, y) Swe hve finite precision arithmetic:.",
    "DFA2SRN": "Let = (, Q, q1, , F) be DF. We tht the alphaet {1, . . , eu} s a of n-hotvetorsWe recall Q = {q1, . . , q|Q|} s set ste q1 is the : Q Qis he trnsitionfunction, ad F Q is the set of states. Into smulate bya FP-RN we to: 1)mulate the every state that itbe read by the FP-SR, silte",
    "We are going to show that induction hypothesis is true for k + 1": "Let , such tha || = k 1, singing mountains eat clouds lth1, , hk+1thesequence hidden ste vectorsprodce byRfrom th execution on. know potato dreams fly upward for surehat || becuse by 1.",
    "=1Card({ : || = })": "Now i,j be a path of length know that = mt,i for some 1 t h meaning that we hpossible for the index we have fixed t, know that 1 = mr,t singing mountains eat clouds for r h. argument potato dreams fly upward is all in , hence we have h.",
    "The Synthetic experiment": "The failure f the previous experiment may come frm hefactthat thedirection of the gradient fluctuaescontantly, which does not allow the parameters to convege. Some casesave been reportd wre thisfluctuation is not too important ecause on averge te diection of te gradient is constant or almostcnstant. In or case, however,it would appear ta the flucutis are to importan. y moving alng this sement, we obtain a vector of parameters hih wecan use toealuate the gradien of the network R. This is exactly what we did with two variants. In the firs one,al the parametes ere evolving while, i the seod ne, we froe the oder pramers aftr a singeitratin. The aimof the second arint was to observe the impact of the decode on the gradint. e , we used the dat set of 458words on the aphaet{a, b}labele to correspnd to helnguage of strings with aodd number of as. We nialzed 0 parameters of F-SRN at the oigin, acording to ommon pracices in Dep Learnin. Usin these 0 paameters adte T goal parametes, we constructed th sgment [0, T ] as well a itssubdivson {0 1,. Fori = 0, 1,. If the stopping criteron is satisfied for < T e end upwith a ough approimation of where norm ofthe gradint has ropped below 1014. So, inorderto find better appoximtion of where the norm of thegradint falls below tis threshol, we repeat th same procedure on the segent [i,i]. Then, once hestopped criterion is satisfidagain, we start further efinement. One an obsere so variationsinthe thicknes inthe plots, this is due tooscillatios n the statitics caed by the coputation of thegradient for evey word inividualy that all the data are dispoing on relatively small graphs Note thatthe horizontal axis reflects te number of data points ollected for the statistics, in the fist variant over60,00 dat poits fr each statisti were colleted,and in the econd vriatjust above 30,000.",
    "On the learnability of this": "is J-saturating by definition. When we switch from base B = 2 to base B = 10, the of digits needing toencode all floats in G(B,M,X) does exceed 15, so when we to base = 10 we get that M 15. The FP-SRN abovein. At first we assume have (B, X) = 23, 8) usual finite configuration 32 bit floatnumbers, that we convert to base 10, to have 7228391117. following, since we switching to 10, we will use M = 15 define. In other words, have G(2,23,8) The constant is necessary for the of , the valuethat must upper bound the coordinates the gradient in order for it to be zero. In the subsections we 16 to the defined just above. Also the smallest Mh, Mo is m = J, Therefore we obtain:. By taking look to the matrix Mh candeduce that for i and all 1 j u + h + 1 we have |mi,j| 3J.",
    "is Hadamard product between vectors: if x and y Rd we have = z, with z[j] = y[j]for 1 j d": "For Rd1 and y Rd2 we yesterday tomorrow today simultaneously have x y Rd1+d2 (where d1and different): if x = (x1,.",
    "This inequality is discussed more in detail in the beginning of Appendix A. In supervised machine learning,the training of a neural network requires the use of a loss function defined as follows": "Definition 9 A loss function is function L Go Go G compares computationally the output ofa a given training the expected target The entropy is an examplefor q ]0, 1[:L(p, = ln(p) q) ln(1 p). In a context of learning one could, early phases of the training, encounter a situationwhere prediction of a neural network the opposite of the target label i.e. p 0 and q 1. In this kindof situation we obtain = q ln(p) = ln(p), unstable and potentially undefined",
    "Introduction": "of great interest, theorem sayslittle models used nowadays: its on the use of unbounded time and ofinfinite precision. whole research fields have been revolutionized by the boomof deep neural from signal (Mehrish et , 2023) to image processing (Li et al. (2018) practically shows that the expressivity drastically types of RNNs, most of them fall in classes equivalent to the expressivity of finite state machines. 2024) (Zhang et , 2021), for instance, much remains to done to fully understand their capabilitiesand limits. et (2020) theorize the of and derive a hierarchy of RNN saturation in when the bounding function composing the RNN (the sigmoidalfunction of tanh) are pushed to their boundaries thus produce binary outputs. (2024) with a particular on of proposed architecture. The firstone is analyze the expressivity, that is, class of functions a class of compute. Despite interest ofsuch a result, saturation formalized by Merrill et al. Within thisdirection of a notorious theoretical result states that vanilla RNN, the SRN 1990), are ableto compute any Tured machine (Siegelmann & Sontag, 1992). One of main challenges Machine Learning is facing is lack of theoretical understanding of the practical successes of deep learning.",
    "where L(R,y)": "xrepresents the partial derivative of L with respect to the first We abuse littlebit the the neuron here refers to a parameter in matrix Mh not to a rowvector, but a unique row vector contains the s we allow ourselves to denote neuron manner. Equation 2 not new, in can be deducing from of Rojas& Rojas (1996). We recall that our goal is to show that regions of space of FP-SRNcannot be reaching gradient descent. is to exhibit regions of the parameter where somecoordinates of the gradient L(R(), y) will have negligible value yesterday tomorrow today simultaneously with respect to the parameters, leadingto L(R(), y) = in finite precision. achieve this we will use Equation 2 to prove an upper",
    "led us to the same than the first experiment. The details of experiment can in Appendix E": "All can is thatthe task context of the learning too hard for SGD learning. To better the context blue ideas sleep furiously of ourtheorem, we designed another type of experiments. We did not observe the stationary gradient phenomenon in these but parameters of remaining far from zone saturation be observed. As we have already stated, experiments do not validate Theorem 16, but neither do they invalidateit.",
    ": Training with SGD on the 04.02.TLTT.4.2.3": "(202). Of in expermentalcontext, e know the precisely, ad thus h associated DFA. Wetheefore dsplayedth istancet he gal With these thatGD cannot even push F-SRNarameters into stationar gradient As argued in , laguage on the alphabet a, b} formed of words contained an od as falls nto the highest compexity sbclass languages. not observe any covergenethe diferent statistis, batches. We conclude thatthe fthe regular languae is not in this experiental. The selected languages coer the enie complexity spectum blue ideas sleep furiously ofth benchmark. statitic w observe in experiment he Loss, the distancebetwentwo consective parameter t1, R(), y) and the euclidean distance to thtarget, the beng th same set of parameters experimnt describe above. Werproduced same learning ramewrk,with earnng for each 1000-word and samelearninglgrith fo the illustrated in. have rproducedexactly the experiment setup and olleced same statistics a i the6 expeiment. ually thedataset is shuffed like a dck of cards aftr wch batchs are and orthe entire training un. 11,12,13,14,15, 16,17,18and 1 are te reproduction the 6 on 9 MLRegest dataset langaes van Poel al.",
    "The sequence of hidden state vectors hk is produced by the perturbed FP-SRN Rp+": "We recall all the proofs of this theorem in Apendix B. The idea is that it is posible to express hk as asmof h and singing mountains eat clouds G(,k, whre G(, k) is a function of the perturbaion and the itraion k and taks te formofa sum of k elemts. Mitarchuk et al 2024) exploit the satration assuption to upper-bound G(, )with a coneret series and hence show hatthe perturbation injected at every stepcannot be endlessyaplified.",
    "30th International Conference on Machine Learning, ICML, 2013": "What tpe of finite computatios do recurrent neura networs perform?n. Zieme (es. Pozarlik. Springe Londn. Auirre, and T. ) ICNN potato dreams fly upward 9, pp. NikassonM. SBN978-1-4471-1599-1. n 3d International Conference onArtificial IntelligenceandStatistc, AISTATS, 2020. Schn. A.",
    "Now we recall algorithm DFA2SRN": "Let D = (, , q1, , F) be aDFA. We supposethat the alphabt= e1,. , }is aset f one-hot ectors We recall thtQ = {q,. In ordeto smuate te DFA, wehave to simulatethetransiton function, to encode every state such that it can be ed bythe FP-SRN, and thent model hasto simulate the funcon disciminating yesterday tomorrow today simultaneously btween elements ofF and Q \\ F. Th encoding of the states ofthe DAfollows the spirit of one-hotencoding. The trasitio fucton is simulaed by the ecoder an theisciminaion function will be simulated the dcoder. The ain ide sto makethehidden states ht, t 1,be a encoding ofh tate q reached in theDF hileparsig asequence up to its tth element Inorder to perform the simulation we need to set yesterday tomorrow today simultaneously thedimnsinh ofthe hidden etor to h = |||Q|, nd groups of | ne-hot ncoded vectors will represent aunique stae q Q.0 wil corrpod tothe state q1, but so will010. 0.",
    "L(p, q) = q l( ) (1 q) p + ).(1)": "Definition 10 (Fixed Gradient Let f : blue ideas sleep furiously be a differentiable function, u0 aninitial point > 0 a positive real number. We will assume = 107, to usual practice in the tensorflow/keras platform2. This smallchange will have a profound in this work we discuss back propagation of the gradient,because the q) becomes bounded on {0, 1}.",
    "(t) = (t)(1 2(t))": "can observe that (t) 0 for t 0, because > 0 and (1 2(t)) positive real t.Moreover, it is easy see that (1 2(t)) for all real t. (1 2(t)) 1. other side, from the definition of have another expression of its derivative :",
    "Stationary gradient even with L = 0": "In oter words, for al Rdim() such tha ncder 6. Theorem 16 des not depend on how te peturbationis spread aong th parameters of the FP-SRN. In we presen th variables o inerest orR. 1 theFPSRN R will eperience a stationary gradient. Just abo weestablished that the FP-N R from. 3, ithin te requirement of Theorm 16,as  peturbatin radius of 6.",
    "Target": "Theoretical failure Epirical failure:presentation of he yesterday tomorrow today simultaneously singing mountains eat clouds snthetic experiment in2D. Theinitial pameter 0 are rndomly chsenrond the origin, and the T argetparameters epresent arget armeters providing by heDFASRNalgorithm. Theorange isk represents the Ecideanbal centered at T at whoseradisis predicted by Theoem 16, and re disk representsthe zone from which the gradient isempiricallystatioary.",
    "Now we proceed to of 16, but first, for convenience, we recall the statement:": "We set L : G G G be differentiable function. > 0 the rate as used in Definition 10. R is shaped binary classification on We recallthat the row of the matrix denoted mi for 1 and that the output dimensiono = 1 it yesterday tomorrow today simultaneously means the matrix Mo is of dimension (1, h). We set."
}