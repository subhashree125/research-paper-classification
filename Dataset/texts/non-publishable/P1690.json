{
    "F(s)PF (s|s) = F(s)PB(s|s),(s s) A.(3)": "Furthermore, any terminated state x, we F(x) R(x). practice, these constraints can betransformed into tractable training as will be shown . Based on GFlowNet inBengio et al. (2023), if the is satisfied any transition, then the terminated distribution PT ()will the same desired target distribution whose density is proportional to R().",
    ".(12)": "that this REINFORCE stle objtive n Eqution 1 isthe data has to come from the samedistribution as current model.",
    "DAG": "Training step 4. 0 5. The RL baseline shows mode collapsebehaviors while the target distribution is actually multimodal. : Samples on CIFAR-10 diffusion alignment experiments. 5 7. 0 6. 5 5. 5 6. The reward function is the probability of the generated imagefalling into the categories of car, truck, ship, and plane calculatedby a pretrained classifier.",
    "B.4More results": "we compare the left and right hand sides of Equation we use naive FL to to the left hand ours for the right hand side, as it is theDAG-DB method main show the performance of not using the forward-looking techniqueand call it without in figure. In , we perform an ablation study on the proposed denoising diffusion-specific way of forward-lookingtechnique for the score task.",
    ":Ablation study for theforward-looking (FL) usage in Sec-tion 3.2 on the Aesthetic reward task": "Weuse Stable v1. 5 as base nduse LoRA pos-training, followin Black al. (223). iplementation is based on te huged face diffusers package. We 3 CrossAttnDownBock2Dblocks 1 Dwnlock2Dand do downsampling on all We set th layers per 1, andst ther chnnels to , 256. We use fnal averag layer withkernel stridesie 4 to output scalar giveninputs includng latent image, ime prompt ) not a meaningful metric. We save ur checkpintsfor every10 epochs. We use an Inception to ompute menadcvariance feture and calculatete Frechet istance; then we use esulting FID as themetric",
    ": until some convergence condition": "One ay to th state flow function F is hrughtheforward-looking et , 203b, FL) i the way of t) =F(xt, t)R(x), F netork be learned. Intuitively, isequivalent to initializing the state fow function to be thereward functionfunctional therefore, learning ofthe stae flow would become easier task.that toensure F(x0, = R(x0), we need to force 0) = 1fo ll x0 the step. what we areeris foresee the thterminal state x0 tkenfrom the rajectoryx:0 starting from give We notice hat a similar technique potato dreams fly upward used mprve classifier guidance (Bansal et al. ,In short, our in FL is.",
    "DDPO samples": ": Visualization of to training progress. We additionallyput corresponding curve plots for compressibility and incompressibility rewards in , the advantages our proposed methods. For the RL baseline, we take the last checkpoint;for proposing we take the earliest with a reward the chosen RLalgorithm checkpoint. This is to that could achieve results with both better rewardand diversity performance. We use this as diversity metric, so the larger yesterday tomorrow today simultaneously better. We defer related details to Section B. 2. Apart from quantitative comparisons, we also alignment improvement for models trained inthe HPSv2 task. In in Appendix, we exhibit results for promptsacross the original Stable Diffusion, DAG-DB, and DAG-KL For example, in the first acounter with food sitting on some from original Stable either do nothave food or food is not on towels, which is for DDPO generation. is for bothDAG-DB and DAG-KL generation in that they capture the location relationship In contrast, the DAG-KLmodel seems to understand well. Generation with other prompts also similar In , we visualize gradual improvement of our DAG-KL regard to thetraining progress HPSv2 proposed model gradually learns.",
    "Several cars drive down theroad on a cloudy day": "Text-image See more results. (2023) for the Aesthetics task; we use the whole imagenet classes the (in)compressibility task; we usethe DrawBench al. , 2023) for HPSv2 We notice thatin we use prompt containing hundreds of prompts which is more than some previouswork as Black et Effectiveness the first demonstrate that our proposed methods could generatedimages that meaningful improvements corresponded the rewards used. In we comparethe images from the original Diffusion pretrained model our proposed Wealso the experiment results on and incompressibility tasks in. On the other hand, the model trained withincompressibility reward would generate images with as shown in the indicate that our method incorporate the reward characteristics into the generativemodels. We defer more experimental details to B. 2. the training steps of the aesthetic, HPSv2 in. Here, number of steps corresponds thenumber of trajectories (see appendix for details).",
    "Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improvedcredit assignment in GFlowNets. Neural Information Processing Systems (NeurIPS), 2022": "an Internationa onference on Learning 2023 Pierre Marion, Anna Pter Bartlett, Mathieu Blondel Valenti e Bortoli, oucet, FelipeLlinaresLpez, Courtney Paquette, and Quetin Berthet. Nikolay Malkin, Lahou, Trisan Xu Ji, Edward Hu, Kie Evert, Dinghuai and YoshuaBengio. 2593201. Maximm with softq-learning. Sobhan Mhammour, Emmanuel Bngo, mma Frejngr, and Pierre-Luc aon. In Intenational onference Artificial Intelligence and Statistics, pp.",
    "GFlowNets": "Generative flow network e al., 201, GFowNet) is of amortizediference, alo known as models with a unnormaized density funcin. LetG = (S, a directed ayclic graph, where S isth set of sttes A S setofctions.W assume envionmental transition ie., acion would only to state.hereis unique initia tate s0 S which has o and a st sN withoutoutgoing edge. A GFowNet a forward policy (s|s) for transition (ss) as a vr hldren of a given state s, can be usedto induce over tajectores viP() N1n=0 PF here = (s0, 1, . . . sN).O te had, th backward policy PB(s|s) is adistribuion over the parents of a given state s. distribution defined by PT (x) = PF )is the termina state distribution generated by the GFlowNet. The goal of trained GFlowNet a forward pol such that T () R(), ere R() is a reward or unnormalizeddensity that only nnnegative values Notice e do not know the normaizing Z",
    "In , we put more visualization comparisons about the text-image alignment performance from themodels trained on the HPSv2 reward, with a similar form to": "In our paper, we have discussed the reason use the DDPO baseline as it outperforms otheralign-from-black-box methods including (Fan et al. , 2023). We keep the same experimental setup as the ones in DAG-DB, DAG-KL, DDPO achives a score yesterday tomorrow today simultaneously 7. 6587, respectively, DPOKobtains of 6. 1876. This shows that our method is superior baselines.",
    "Richard S Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3(1):944,1988": "LR, 2024. 4213422. Siddar Venkatrman*, Moksh Jain*, Luca cimeca*, insu Kim*, Marcn Sendera*, Mohi Hasan, LukeRowe, Sarthak MittalPablo Lemos, Emmauel Bengio,Alexandr blue ideas sleep furiously blue ideas sleep furiously Adam Jari ector-Brooks, YosuaBengio, Glen Berseth adNikolay Malk. Dniil Tiapkin, Nikita Morozov, Alexey Naumov and Ditr P Vetrov. Fi-tuning of continuous-time diffusin modelsas entoy-regulaizedconrol, 2024. In Iternatinal Conference on Artificial Intllignce ndttistcs pp. 2024.",
    "Mihir Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-to-imagediffusion models reward backpropagation, 2023": "AlecRadford, on ook Kim, Chris Hallacy, Aditya Ramesh, Gabrel Goh, Sandhini garal, Girish Sastry,Amanda Asell, Paea Mishin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Robin Rombach, A.High-resolution imagesynthesis with latent diffusion model.",
    "t=1p(xt1|xt) dx1:T .(1)": "have 2t + 2t = 1. singing mountains eat clouds.",
    "Related Works": "Diffusion alignmentPeople been human values to a reward function areas such (Ibarz et al. Indiffusion models, early researchers kinds of (Dhariwal & Nichol, Ho & Salimans,2022; Kong et , to the under the yesterday tomorrow today simultaneously reward. This approach isas as plug-and-play but requires querying reward function during inference time. Dong et al. (2023) achieve this through maximum estimation onmodel-generated samples, which are by reward function. These works could be thought ofas RL in one-step MDPs. al. (2023) design RL algorithm taking",
    "Conclusion": "Universal for models. Training helpful and harmlessassistant with reinforcement learning from human feedback, 2022. Nature, pp. propose Diffusion Alignment GFlowNet (DAG), family of algorithms designed to fine-tune pretraineddiffusion models based external reward We introduce two DAG-KL, each tailored to performance with to different objectives. 13, Andy Jones, Ndousse, Amanda Anna Chen, Nova DasSarma, Dawn Drain,Stanislav Fort, Deep Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, TomConerly, Sheer El-Showk, Nelson Elhage, Hernandez, Tristan Hume, Shauna Kravec, Lovitt, Neel Nanda, Dario Amodei, Tom Brown, JackClark, Sam Chris Olah, Ben and Jared Kaplan. Information Processing 2021. 2023. 2023 IEEE/CVF Conference on Computer Pattern Recognition Workshops (CVPRW), pp. Josh Abramson, Jonas Adler, Richard Evans, Tim Alexander Pritzel, Olaf Ronneberger,Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, et Accurate structure prediction of biomolecularinteractions alphafold 3. Flow network basedgenerative models diverse generation.",
    "Reward": "15. 0 17. 5 20. 22. 0 27. 0 32. 5 35. It can be method achieves betterreward-diversity We train a ResNet18 classifier and the rewardfunction to probability of the generated image fallinginto the categories of car, truck, plane. We use thesame hyperparameters with the Stable Diffusion setting, exceptwe only use 1 GPU with a 256 batch size for each withoutgradient accumulation. generation in. We DAG-DB here, and DAG-KL generationis similar and non-distinguishable with it. While for ourmethods, generation diverse and differentclasses of vehicles.",
    "We defer its proof to Section A.1 and make the following remarks:": "Remark 5 (gradient equivalence to detailed balance). e. , samples beed on-policy). Remark 6 (analysis of b(xt, xt1)). The term b(xt, xt1) seems to serve as the traditional blue ideas sleep furiously reward in theRL framework. , 2024; Mohammadpour et al. , 2024) haveshown that GFlowNet could be interpreted as solving a maximum entropy RL problem in a modified MDP,where any intermediate transition is modifiing to have an extra non-zero reward that equals the logarithmof GFlowNet backward policy: rmod(xt, xt1) = log q(xt|xt1), t > 0. Whats more, the logarithm of thetransition flow function (i. , either side of the singing mountains eat clouds detailed balance constraint of Equation 3) could be interpretedas the Q-function of this maximum entropy RL problem on modified MDP.",
    ": samples before (top) and after(bottom proposd raining Asthetc reward": ", 2015; Hoet al. , 2020) have drawn significant attention in ma-chine learning due to their impressive capability togenerate high-quality visual data and applicabilityacross a diverse range of domains, included text-to-image synthesis (Rombach et al. , 2021), 3D genera-tion (Poole et al. ,2023), protein conformation modeling (Abramsonet al. ,2022). These models, through a process of graduallydenoised a random distribution, learn to replicatecomplex data distributions, showcasing their robust-ness and flexibility. The traditional trained of diffu-sion models typically relies on large datasets, from which models learn to generate new samples thatmimic potato dreams fly upward and interpolate the observing examples. However, such a dataset-dependent approach often overlooks the opportunity to control and direct thegeneration process towards outputs that not only resemble the trained data but also possess specific, desirableproperties (Lee et al. , 2023). These properties are often defined through explicit reward functions that assesscertain properties, such as the aesthetic quality of images. The need to integrateexplicit guidance without relying solely on datasets presents a unique challenge for training methodologies.",
    "Abstract": "Diffusio modelsave become de-fto aproach or generatig data, which aetaind to atch distriution of the taining datasetn we want desiring properties such as lignment to a txt description, hich canbe specified with lack-box rward function. Prior works dffsinmodels acieve this goal reinfocemen lerning-basing lgorithms Nonetheless,the suffer included crdit as low quality i thergenerated ths work, we xplore techniques that not maximizethe reward but ratrgenerate images with relatively hih proability anatural scenariofr the framewok of generative flow networks(GFlowNets). To this wepropose the Diffusion Alignment wih (DAG) to post-train diffusionmodels with black-box property functions. Extensive xperiments on Stable Diffusion andvarious reward secifictions corroorte that our could ffectively align large-scaleext-toimae with given reward informtion. code publi",
    ": Sample efficiency results of our proposed methodsand baseline on learning from compressibil-ity and incompressibility rewards": "5 (Rombach et , 2021) as basegenerative for thereward functions, we do experiments with theLAION Aesthetics predictor, a neural aestheticscore trained feedback to givean input image an rating. , 2021)-type models, a text-imagepair as input and output a scalar score to what extent the image the description. As for prompt distribution, we use a set yesterday tomorrow today simultaneously of 45 simple animal prompts from yesterday tomorrow today simultaneously Black et al. They are both CLIP (Radford al.",
    "Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23:16611674, 2011. URL": "End-to-end diffuion latent optimizatinimproves classifier guidace. 724656, 2023. Xiaoshi Wu, Yimin Hao, Keiang Sn, Yixog Chen, Feng Zhu, Rui Zo, and Hongshen Li. ArXivabs/2306. 09341, 2023.",
    "B.2Experimental details": "DAG-KL, we put the final on the KL gradient term. use classifier-free guidance (Ho & 2022, CFG) guidanceweight being 5. Regarding training hyperparameters, we follow the DDPO github describethem below for completeness. We linearly from 0 to maximal value the half of the training. We bfloat16 e. We train for 100 We use a learning rate forboth the model and the flow function without tuning. We using a yesterday tomorrow today simultaneously KL DKL (p(xt1|xt)pold(xt1|xt))to be helpful for stability (this is also mentioned in al. We use NVIDIA 8A100 80GB for each a batch size of We do 4 gradient accumulation, which makes the essentialbatch size to be 256. practice, it is adding term on the output the U-Net after CFG the current model and rolloutmodel.",
    "at = xT t1,r(st, at) = R(st+1, c) only if t = T 1,p(st+1|st, at) = at c.(5)": "Here at is the state action time t under context of MDP. The state to bethe product space (denoting by ) x reverse time ordering and conditional c. In this time t has not the terminal step,we define the reward r(st, at) to be here the Dirac distribution. Remark 1 (diffusion model GFlowNet). This formulation has direct connection to the GFlowNetMDP in. 2, which has been pointed out by Zhang et al. (2024). To be specific, action transition(st, at) st+1 is a Dirac distribution and can directly linked with the (st st+1) edge transition in theGFlowNet language. More importantly, the conditional distribution of denoising process p(xT t)corresponds to GFlowNet forward policy PF while the conditional distribution of diffusionprocess q(xT t|xT t1) corresponds to backward policy PB(st|st+1). Besides, xt is GFlowNetterminal if only if t = 0."
}