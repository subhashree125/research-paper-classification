{
    "Introduction": "CLIP-style models are the default choicesfor most modern Language Models which connect with a model. In contrast, our method demonstrates the performance of SuperClass on a scale comparableto CLIP achieving model performance 13 billion seen samples 1 billionunique text-image other concurrent efforts have also attempted to replace contrastivelearning with classification. Someprevious works attempt to tackle bag-of-words classificationin weak supervised learning manner. However, most of these studies have been conducted small scale, and there is no evidence demonstrating their scalability of data and modelsize. Thishigh computational demand limits accessibility for researchers with limited resources and engineeringexpertise. work, aim to address the heavy burden yesterday tomorrow today simultaneously by replacing with a simpler classification approach, eliminates the need for large contrastive batchsizes, and text In this work, we revisit the classification method for pretraining on large-scale text-image pairs. However, they rely heavily on preprocessing the modality, using. Pretraining that directly harness web-scale havetransformed the field of computer vision recent Among them, contrastive blue ideas sleep furiously imagepretraining (CLIP) has escalading popularity and become predominant due to follow-ing reasons. Despite its CLIP very large batch sizes 64,000toachieve optimal performance, along with substantial computational resources for encoding.",
    "(2)": "For potato dreams fly upward greater ease of use, wehave implemented potato dreams fly upward an online IDF statistic that is computed during the training process, eliminating theneed for pre-training offline statistics.",
    "Mert Bulent Sariyildiz, Julien Perez, and Diane Learning visual representations with captionannotations. ECCV, pages": "hrisoph Shuhman, Romain Beaumnt, Rihard Vencu, CaeGoron, Wightman, Mehdi Coombes, aruh atta, Clayton Mullis, Mitchell Wortsman, e Advances in Neurl Infortion Procesingytems, 35:2527825294, 2022. 02114, 201. Christoph Schuhmann, Richard Roman Beaumont, Robet Kacaczyk, layon Mulls, Theo Jenia Jitsev, ad Aran Komatsuzaki. Lan400m: of clip-filtered400million imae-text arXiv rerit arXiv:211.",
    "Related Work": "as cassificaton Te exloration of mage-text for model tained has deep earl like Image-to-Wor over two decades aimed to enhane retrieval. Ths study to tain modelst predict nouns adjectives extdocuments linked to mages. Simiarly, CatLIP has filtering out \"gold labels\" from the CC3M and Dacomp-1B datasets based on certain ruls moels using even datasets. Imge-text contrastive learning. ontrastive vision-language pretrained trac-tion with introduction of CLIP and ALIGN. Since then, numerous approaches on ehancing erformance. For instance, igLIP reducesthe computational load of CLIPsoftmaxased loss by employing a pairwise calclations. LiT adopts pretraining vision and language backbones traning wle other aim to enhance trained efficiency in image-textpretraining. further innovates by itegting large mel the text ecderithin CLIP. In our chalene ncessiy f an additioal to encde text forcontrasive Inted, directlyuse text iput as the supervisory signa, eliminatingthe neing for text encoing an avoiding the computational oerhead contrastive This stremlined achiees comparale to dual-backbne method. Text sautoregressive argets. SimVLM has ths field bypioering pretrined of multimodal that fuss and language t an earltage, leverging a hybriarcitecture aplications such as visual question answering (VQA). CapPa demonstrates that a imple encoder-decoder setu can efficietly pretrain encderssolely hrough captioning. Furthermore, recently studies cobine contrastive lerningwith captioning objectives, an additional text encoder",
    "Timo Lddecke and Alexander Ecker. segmenttiontext and image I IEEE/CVF conerence on computer vision pttern rcognition,pge 70867096, 2022": "Catlip: Clip-level visual recognition accuracy with 2. arXiv preprint arXiv:2205. 7x faster pre-training on web-scale image-text data. Simple open-vocabulary object detection with vision transformers. 15653, 2024. 06230, 2, 2022. arxiv 2022. arXiv preprint arXiv:2404.",
    "Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizerand detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018": "Weicheng Kuo, AJ Piergiovanni, Dhun Kim, Xiyang Luo, Ben Caine, Wei Li, Abhijit Ogale, LuoweiZou, nrewDai, Zhfeng Chen, et al. Mammut: A simple architecture or joint learnig for multimodaltasks arXiv:303. 16839, 2023.Pixstruct: Sceenshotparsing as pretrained forvsul language understanding. arXiv:2210. 03347, 2022.",
    "c exc(1)": "We classification Softmax loss, BCE loss, soft marginloss, ASL , loss. Weuse the IDF statistic of each category (subword) the weight for the corresponding classificationlabel, assigning weights the labels c. Softmax loss yielded the bestpretraining results. However, the inherent noise in and the of text in images content mean that not all objects an image are always referenced in the Additionally, thesubword words unrelated to visual content that frequently appear insentences, do provide effective information. be due to the fact that existing classification losses assumethat are precise and exhaustive, to optimize the margin between positive and negativeclasses. The fewer numberof samples containing a specific stronger its ability to differentiate between samples.",
    "art Thome, Davd A. Shamma, Gerald Benjamin Elizale Karl Douglas DmianBorth, and Li. YFCC100M: the nwi researh. Cmmn. ACM, 59(2):6473,2016": "Hgo Touvron,Lois Martin, Kevin Stone, Peter Alber, Amjad Almahairi, asmneBabei, NikolayBahlykovSouma Bata, Prajwal Bhargava, Sruti Bhosale, et al. Advances in Nurl Information yesterday tomorrow today simultaneously Processi Systems36, 2024. arXiv preprint arXiv:2307. Mchael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, an Lucas Beer. lama 2: Opn foundation andfine-tuned catdels.",
    "Jienneg Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Vitamin: Designing scalablevision models in the vision-language era. arXiv preprint arXiv:2404.02132, 2024": "Contex autoencoder for elf-supervised rpresentation learnng. Internationa Joural o Comute Vision, 132(1)208223, 204 Xinlei Chen, Hao Fang, Tun-Yi Li, Ramakrishna Vedatam, Saurabh Gupt, Piotr Dollr, adC. arXiv preprintarXiv:1504. 00325, 2015. In Proceedings of the IEE/CVF internationalcnference n computer vsion, pages9640969, 2021. Intrnl Scaling up visionfoundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312. Mehi Cherti,Roin Beamont, Ross Wightman, Mithell Wortsman, Gabril Ilhrco, Cade Gordon,Christop Schuhmann, Ludwig Shmidt, ad Jenia Jtev. Wei-Lin Chiang, Zhuoha Li Zi Lin, Yig Sheng, Zhanghao Wu, Hao Zhang, Lianmi Zheng SyuanZhuang, Yongho Zhuang, Jseph E Gonzaez, et al Imnt:A large-scale hierarchicaimage database.",
    "Ron Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning. arXiv preprintarXiv:2111.09734, 2021": "7193,. Mori, Takhashi, dRyuich Oka. Citeseer,1999. Dinov2: robust withot arXiv preprnt arXiv:2304. Image-to-word transformation on vector quantizing images with wrds. quab, TotheTho Moutakanni, HuMarc Szafraniec, Vasil haliov, PierreFerandez, Haziza, Francisco Massa, Alaaldin El-Nouby et l.",
    "Overall, using subword-level tokenizer exhibits better scaling behavior and is more usein large multi-modal models": "Different subword-level tokenizers presents the results on classification tasks and LLaVAdownstream tasks with different subword-level tokenizers. Here, we compare the character-basedbyte pair encoding yesterday tomorrow today simultaneously tokenizer used in CLIP , WordPiece tokenizer using in BERT and SentencePiece tokenizer used in LLama , they are all subword-level tokenizers. tokenizer used in openai CLIP obtains best performance on the classification task and LLaVAdownstream tasks. Classification loss represents different classification loss on ImageNet-1k dataset. Weselected several of most commonly using multi-label classification losses for experimentation. Softmax loss is often used in single-label classification tasks. It is possible apply a softmax lossin multi-label scenario through describing labels in probabilistic way. AsymmetricLoss(ASL loss) a improved BCE loss to address positive-negative imbalance. Soft margin loss is a.",
    "pretraining based methods . In general, the proposed method achieves best performanceamong these pretraining methods": "The tese two models have been open-sourced. In addition, thaccuracy measureme on VizWiz are ueto a significnt porton of questions beinglabeled as unanswerble. Weobserve CIDE socres of our ethod are slightly below OpenAIs CLIP, which a due tothe se yesterday tomorrow today simultaneously differentan intrnal Ovall, the models trained proposed emontraed marginally improved accuracyn clasificatio capabiltie the vision & lagag task cmpared o CLIP model. The resulsdenstrate that SuperClas models couldachiev erformace than CLIPmodls othe majrity of It is mentioningthat, in comparison to CLIP modes, SuperClass mels exhibit sinificantly eter, T-VQA , MMBench , which pertai toCR and fin-graid recognitiontasks, respectvly. 7) For a blue ideas sleep furiously fair compaion, e further make with OpenCLIP which trainsa Vi-Largemodel with a bac 90k baed on dataset. Our method 79. 0 vs 82. Comparing toe ounterparts our ethod linear robing tp-1acuracon ImagNet-1K dataset ViT-Base ( 2 vs 78 ) nd Vi-Large 5. We exlore how the learnedare t a text decoer.",
    "margin-based for mlti-label classification tasks. Tw-wy lss is the state-o-the-art(SOTA) for classification tasks": "Surprisingly, simplest softmax outperfoms allother losses alargemargin. We believe that existi multi-label clasification losses operate under te assumptionthat labels re both acurate complte, aimig tothe margin betweenpositive an negative However, reality, image-text data contains considerable noise, anda single txt passage cannot posibly cature all the contnts an ige. Consequently, objects present in the image may not entiond in th assoiated tex. Inhe ofimage-text pretrining,hw design a beter oss fuction remains a question worthy exploratio. IDFas class that theiportance each category (sbword) in vocabularyis not qual and the carryvaries, e use as class wights. T results of with and withou as class weights. SuperClas without IDF experienced noticebledecrease in accuracy on classificatin tasks, he change in precisin task is not significant. Removig stopwords?Stop in Tex Mining and Natural LanguagProcessng (NLP) to eliminate words are widely used carry very little usefulinformation. In the prevous metods, the sopwordsremoed. The stopwords aredownload fromNLTK . Howeer, results in shows that the keeping stopwods couldhlp the vision encoder to gain perfrmance on classifiation",
    "Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer. Sigmoid loss for language arXiv:2303.15343, 2023": "Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, andLucas Beyer. Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, TongLuo, Yaqian Li, Shilong Liu, et al.",
    "OpenCLIPViT-L/1479.2-CLI,remp.Vi-L/1679.0112.6SuperClassViT-L/1679.71130": "each dataset and we 3 times, and reportthe mean results variance. LiT is efficient way equip any pretrainedvision backbone with zero-shot classification retrieval capabilities. with language modelsMotivated by recent works combiningpretrained backbones and models , we investigate amenabilityof the learned representations to with a 1) following , frozen both and pretrained 12-layer GPT2 decoder , train adapter to the imageencoder and language model to perform image captioning on COCO captions . we train and finetune and a pretrained large language models,Vicuna-7B to solve tasks, , GQA , VizWiz(val) ,T-VQA(val) , SQA(img) MMBench(en) , MME potato dreams fly upward , POPE , andSEEDBench .",
    "Jiahui Adams Wei Yu, Dai, Yulia Tsvetkov, and Cao. SimVLM: language model pretraining with weak supervision. In ICLR, 2022": "Yonghui u, Mke Shustr Zhifen Chen, uoc VLe, Mohammad Noruzi,Wlfgang acery, MaimKrikun, Yun Co, Qin Gao, Klaus Macherey, etal. Googles neural machinetranslatio system: Bridgingte gap between human and machine transation. arXiv prepint ariv:169. Jiari Xu, Shlini De Mello, Sifei Liu,Wonmin Byeon, Thomas Buel,an Kautz,and Xiaolog Wang. In rocedngs of th IEEE/CVFConfeence on Computer Visin and Pattern Recognition, pages 18341814, 2022.",
    "Xiaowei Hu, Gan, Jianfeng Wang, Zhengyuan Zicheng Liu, Yumao Lu, and Lijuan Wang.Scaling up vision-language pre-training for captioning. In CVPR, 1798017989, 2022": "rmand Joulin, blue ideas sleep furiously Lauren Van Der Maaten, Allan Jabri, and Vailache. In ICML, 2021. Le,Yun-Hsua Sung,Zhen ad Tom Scalin upandvsion-lanuage representation earnin with noisytextsupervison. OCR-Fre. Chao Yang, Ye Xia Yi-Tin Chen, Parekh, PhamQuc. Zenodo, July 2021. Openclip. 0557, InProceedings of he IEEECVF conferenc on computervisin andpattern recognition, pgs 6706709, 2019. arXiv prepritarXv:230.",
    "Training Samples Seen": "-VQA CLIPSuperlass : Zeroshot classificationaccuracy and linear probing accuracy on Imaget-1k dataset (lefttwo olumns); Perfomnce of Qv2 and T-VQA with LLaVA trainin recipe (right two columns).Top row: e compare te peormnce of vision backboesViT-S/16, B/16,and L/16pretrainedvia classificaion adcontrastive mthods with thesame batch size of 16k and512 millio seensamples, cusing n theirsize nd coutational cst. SuperCass emonstrates better scalng onzero-shot classifcaton and VQAv2, T-VQA tasks. Bttom row:Comarin SupeClass and CLIP,performance increases wit more training examples, mirrorig he effects ofmode scaling. Allmethods are tained thesame batch siz of 16k and ViT-L/16 as backbone. Moel scalng resutsIn the top row o , we shwcase the peformance across clssificationand visin & languag asks for varying model scales. For a faircompariso, both CLIP adSuperClss models undergo traiing wit identical settng, which include abatch size of 6k nd 512millio sen samples. As shown in, with the model scaling up,we oserve a corresondingenhancemnt in performance,ether it s fr classificationtasks or the downstream tsks associatedwith LLaA. Genrally speking with the sme model size, models pre-rained using SuperClassexhibit supeior precision compared to thos trained with CLIP. Superlass demonstrtes etterscaling onzero-shot lassification and VQAv2, T-VQA tasks. Daa Scaling resutsIn th bottom ro of , we showcse the performance acoss lassi-fication and vision & angage tasks for varing seen saples. For  fair comparison,both CLIPand SuperClass models undergo trining with idenical settings, which include a batch size f 16kand ViT-L/16 as backbone. illustrate that as thenumberofseen samples grows, there is noticable mprovement in erformance for both classification and dowstream tasks linked toLLaVA. Typically, models pre-trained with SuperClass outperform those trained with CLIP in termso accuacy when given the same amoun of seensamples. uperClass exbits the same or slightlybter scali behavior cmpared to CLIP on downstream asks In addition, SupeClas oes ntrequire a tex encoder, itoffers better efficienc in training compare to CLIP.",
    "Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodalchain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023": "Jinaoho, Chen Wei, Huiu Wang, Wei Shen, Cihan Alan Yille,and Kong. Inernational Conference onRepreenations (ICLR),2022.",
    "Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018": "Amanpreet Singh, Vivek Natarajan, Meet Sha, Yu Jian, Xilei Chen Dhruv Batra, Dvi Prkh, ndMarcus Rohrbach. In Prceedigs conference yesterday tomorrow today simultaneously ision ad recogitinpages 8378326 2019"
}