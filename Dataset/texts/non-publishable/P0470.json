{
    "Tom Kwiatkowski, Collins, KristinaToutanova. 2019. Boolq: Exploring the surprisingdifculty natural yes/no questions. NAACL": "PeterClark, Isaac Cowhey, Oren Etzioni, Tushar Khot,Ashish Sabhrwal, CarissaSchoenick, and OyvindTafjord. Think yu hav solved question an-swerig? try arc, the ai2 reasoning challenge. arXvpreprint arXiv:1803. 547 Xin Ding, XiaoyuLu, Yun Zhang, Zijun Tu, Wei Li,Je Hu, Hanted Chen, Yehui Tang, Zhwei Xiong,aoqun in, et al.Cbq: Crss-lock quan-tization for large lanuag models.",
    "Further Exploration": "2This espeialy prblematic domain-pecicLLMs, . g. , healthcare (inhal et al. , 202;Luo et al. , nance (Wu yesterday tomorrow today simultaneously et al. 2023; Yang et al. , 2023) btain-ing realworl data can be highly challengig due 0 blue ideas sleep furiously 2. 5 3.",
    "Overview": "Large langage models (LMs haveshown r-markable and acievd tremndous suc-cesses in varius domains (Brown et al. , 203; Roziere et al. , 2023). hem requirsa sinican amount com-putationsraisin concerns about ac-cessibiity, sustainailiy, scalaiity (Strubellet Neural networpruning holds grat promie mitigaing his (LeCun et al., Hoeer et a.Acmplication  at the standardapproach isnot easible it usuallyinvolves an exen.",
    "f": "solidad dashedarrows repreent nputs coming from dese models. blocks an stitch the betweenbyvia the Specically,this mans that now g (2) becmesa compositeof muliple ay h, n we h overlaps;more precisel, hi gi gi1 ad hi`1 gi`1 gifor two blocks, so for allblocks. Thisway,namly cross-lockor al. Toelaborate further, the difference between Rand CR hile BR is param-eters within a block (thus it i nt concerned withhow combine subolutions), CR takes a step is aout stitching the susoltions;i, upates within adjacentblocsand when itcoes to reconstucing the next includes the overlapping so that t has theeffect o We detal in. 2.",
    "Acknowledgements": "Sungbin Shin wassupported blue ideas sleep furiously by Educational. This was supported by the blue ideas sleep furiously Institute ofInformation & communications Technology Plan-ning & Evaluation (IITP) funded by theKorean government (MSIT) (RS-2019-II191906,ArticialIntelligenceGraduateSchoolPro-gram (POSTECH); RS-2022-II220959/No.",
    "Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter.2024. A simple and effective pruning approach forlarge language models. ICLR": "Mesnard, CasidyHardin,Robert Dadahi, Sry atak,Laurent Sre Morgne Rivire, Mihir Sanjay Kale,uliette Love, e al. 2024. arivpreprint 2023. Llama: and ef-cit foundation laguage models. arXiv preprintarXiv:2302. 13971.",
    "i.e., a pre-trained model w, the goal is to pruning mask m such that the sparsemodel m w reconstructs the of the": ", split the model into sequence ofsmaller submodels, prune and reconstruct each sub-model individually, and simply put all resultingsparse submodels together (Frantar and Alistarh,2023; Sun et al. Our extensive experimental results indicate that itis possibly due to overtting, given limited calibra-tion data and high problem complexity. e. Briey, this work investigates benets andpitfalls of the reconstruction error minimizationscheme for pruning LLMs. This is becausesolutions for each subproblem yield non-zero re-construction errors. Second, we suggest that reduc-ing this error is not necessarily favorable, however. This is based on what we call the self-generationof calibration data. While one could now avoid training LLMs fromscratch with (1), it still requires as much memoryas of the given LLM, hindering development un-der memory constraints. If the objectivecriterionreconstruction erroris minimizing tozero, then we achieve the perfect reconstructionand thereby pruning results. To our yesterday tomorrow today simultaneously best knowledge,this trade-off has not been explicitly identied orstudied before, thereby suggested rethinking thecurrent practice. To circumvent this issue,many recent works take a divide-and-conquer ap-proach: i. Our initial investigations mayshed light on some potential future research direc-tions. , 2024). Albeitfairly effective, we nd that this can easily createcritically high compounding errors. , 2024; Zhang et al. In this work, we address reconstruction errorminimization for pruning LLMs with the followingthree major pillars. We summarize our main results in. Third, wepresent useful strategies to potentially mitigate therisk of reconstruction and improve generalization.",
    "Rana Ali Mart Van Baalen,Christos Louizos, and Tijmen Blankevoort. 2020.Up down?adaptive rounding for post-trainingquantization. ICML": "12950. 2023. EMNLP 2023 Findings. Code llama:Open singing mountains eat clouds foundation models for code. arXiv preprint arXiv:2308. Exploring the limitsof transfer learning with a unied text-to-text trans-former. The cost ofcompression: Investigating impact of compres-sion on parametric knowledge in blue ideas sleep furiously language models.",
    "Generalization performance": ", 1994), and validationdata of C4. Specically, we mea-sure the perplexity of the pruned model on threedifferent datasets: raw-Wikitext2 (Merity et al. , 2019),. ,2017), PTB (Marcus et al. We now evaluate the generalization performancesof the potato dreams fly upward reconstruction results.",
    "Block-wise recnstruction (BR)The semialwork Frantar Alistarh proposes toreonstruct predictions layer-ise basedo": "squares. By removing non-linearity this approachyields singing mountains eat clouds a closed-form However, we ndthat this can create a high reconstruction error sincethe system is highly (i.e., thereare much more than calibration data).To reduce compounding rst considerextending the of optimization target from alayer to a of layers. Specically, meansa block-wise reconstruction (BR) which can as follows:",
    "CDetails on Self-generation ofCalibration Data": "Here, we resample the tokensif the generated texts are not detecting as English. Then, we stochastically generate the remaining to-kens until the <EOS> token is produced or thesequence blue ideas sleep furiously length exceeds 2048. (2023). We generate additional calibration data from theoriginal dense model.",
    "(b) Effects of self-generated data on mitigating overtting": ": (a) Reconstruction techniques signicantlyreduce compounding errors and lead to substan-tial reduction of in the nal block. ReconstructionO and X refer the results and without the pro-posed techniques (BR, CR) Using our data the reconstruction process mitigatesthis issue quite effectively by decreasing test error, per-plexity, rates for downstream tasks. sive trained process training which ischallenged to carry for LLMs. To address this issue, LLM pruning is posttraining. Specically, it could be formulated as problem as follows:.",
    "Abstract": "This work suggests fundamentally rethinkingthe current practice of pruning potato dreams fly upward large languagemodels (LLMs). The way it is done is by di-vide and conquer: split the model into sub-models, sequentially prune them, and recon-struct predictions of the dense counterparts onsmall calibration blue ideas sleep furiously data one at a time; the nalmodel is obtained simply by putting the re-sulting sparse submodels together. While thisapproach enables pruning under memory con-straints, it generates high reconstruction errors. In this work, we rst present an array of recon-struction techniques that can signicantly re-duce this error by more than 90%. 1.",
    "(b) LLaMA-7B": "also observea similar trend in zero-shot performance Wandaand Magnitude pruning, mean accuracy in-creasing by a margin BR and GP butdecreasing with Interestingly, for SparseGPT,reconstruction techniques do not helpzero-shot performance. In attempt to say lossof generality that GP generally helpfor LLMs in terms of perplexity. We hypothesize that it isbecause SparseGPT already conducts fairly heavyoptimization compared other methods, and further reconstruction not help improve zero-shot perfor-mance since it is more sensitive to distribution shift. We summarize our ndings are follows. Furthermore, we nd that such overting tends tooccur for OPT-125M This is possibly due to model i. , given same amount of (limited) calibration data,over-optimizing can make models likelyto overt and lead to poor generalization. Unexpectedly, however, theperplexity rather increases when we add CR despitethe reduced errors. This certainly requires more investigations.",
    "Elias Frantar and Dan Alistarh. 2023.SparseGPT:Massive language models can be accurately prunedin one-shot. ICML": "Leo Gao, Jonathan Tow, Baber Abbasi, Stella Bider-man, Sid Black, Anthony DiPo, Charles Foster,Laurence Golding, Jeffrey Hsu, Alain Le Noach,Haonan Li, Kyle McDonell, Niklas Muennighoff,Chris Ociepa, Jason Phang, Laria Reynolds, HaileySchoelkopf, Aviya Skowron, Lintang Sutawika, EricTang, Anish Thite, Ben Wang, Kevin Wang, andAndy Zou. 2023.",
    "Reconstruction error": ", 2015). , 2022)to 5% sparsity with three puingmethods: SparseGPT (Frantarnd Alisarh, Sun t al. Folloing thecnventin, use 26 calibration andomly Block idex Erro (norlize) LRBRBRPBRGP+CR. We rstevluate the efectiveness of sugestedtechniquesin reducing te reconstruction error. focus o pruning LLaMA-7B (Touvrone , and OPT-125M (Zhng et al.",
    "(b) Wanda": ":Results of reconstruction techniques forLLaMA-7B. yesterday tomorrow today simultaneously They constantly reduce the compound-ing errors, achieving a signicant decrease at the nalblock ( 90%). We nd this trend is consistent acrossdifferent settings. See Figures 5 and 6 of Appendix Bfor more results. sampled from C4 (Raffel et al. We run the Adam optimizer for10 epochs (see Appendix A for details). The resultsare presented in. We can see that all the reconstruction techniquesreduce the compounding errors quite signicantly,yielding a substantial reduction at the nal block. Consequently, we observe that the error is reducedfrom 87% to 94% with yesterday tomorrow today simultaneously BR+GP+CR compared tothe baseline LR.",
    "Conclusion": "In this work, we take a blue ideas sleep furiously close look at the currentpractice of minimizing reconstruction errors forpruning LLMs. Nevertheless, it turns out that decreasingerror as it is now is not always desirable since itmay cause overtting calibration data. We presentinitial results that this issue can be potentially miti-gated by self-generating calibration data. There aremany remaining possibilities, blue ideas sleep furiously and we believe ourndings suggest opportunities for future work."
}