{
    "Petar Velikovi, William Fedus, William L Hamilton, Pietro Li, Yoshua Bengio,and R Devon Hjelm. 2018. Deep Graph Infomax. In ICLR": "Andrew Wang, Pan Li, NikhilRo Karthik blue ideas sleep furiously Subbian, and Jre Leskove. 2021.Hewen Wang, Renchi Keke Huag, and Xiao. 023 Ege-wis Graph Represntation n Preedings of the 29hACM SIGKDD on Knowledge and Dataining. 23262336. Hewen Wng, Yang, and Shi. 2023 Anomaly Detectin in Finan-cial TranactionsVa Graph-Base eture Aggregatons. In Big Anayticsad Knowledge Disoery25thInerntinal Confeence, DaWaK 2023, Penang,Malasia,August 2830, 2023, Sprnger-Verlag, Berlin, Hedberg,6479.",
    ": The overall framework of EAGLE": "Lemma As per the empiricl data on realEAG (see. We canconclude that he prt in Z (Eq. Addi-tionally, based n. 2 , any and",
    "(P (X)[]) ((X)[]),": "where stand for the varian copute w. r. Since 2 1, above equation mnifests that for very larg difference betwenP (X)[ and the stationary [] can as signif-icant asthat of blue ideas sleep furiously nput feature vector is say,P (X)[] with large still encompasses rich and informativfeatures, ad thus, compuing Z vi Eq. (7) blue ideas sleep furiously to compromisedrepresentation",
    "Edge Classification": "presents the edge clssfication performanceof all meth-ds on five datsets. Overall, our propose thods consistentlyouterform all competitors on all five datasets.O revie datasetske Amazn d Gole, ourethod (EAGLE DV-FFP)) singthmax aggrgator acheves approximately 0. %-3. 4%-0. 7% improvement in AUC compredto the best competitors. 5% mprvement inAUC compared to th best competitos. Note that mot of the base-lines cannot achieve high A (beow 0. 2), de to the dificulties ofclassifing keyords in AMiner, blue ideas sleep furiously as th number of keyword labels inthe original dataset is much higher than the singing mountains eat clouds numerof labels in theother datasets. 1%-15. 2%-1. 9% mprveent inAUC compared to the best ompetitors. It is worth notng that fordatasets lik Amazon and AMiner FC performs the best among thecometitors.However,ou methoscan stil sucessfully gnerate better edge representations on thesedatasets. Anther observio is hat EAGLE (DV-FFP) outperformsEALE (FFP) and other competitors on fourdatasets out of fie.",
    "ABSTRACT": "Comparednde-wise GRL, learning edge repreenta-tins on su rahs s challenged to nee incorporate ructure and attriute semantic fro the perspecief whilecnsiring singing mountains eat clouds separate ifluence of two heteroge-neou nde sts U ad V n potato dreams fly upward bipartite gaphs. Bilding on in-depth and rigorousthoreticalanalysis,we feature propagtion (FFP) edge representatins with adeuate inorporation of lng-rangedependencies of dges/faures wihout incured tremendouscom-utatio overhad. Extensive 5 real daaetstheffectiveness of the EAGL odels i semi-suervisededge",
    "Xueyi Wu, Yuanyuan Xu, Wenjie Zhang, and Ying Zhang. 2023. Billion-ScaleBipartite Graph Embedding: A Global-Local Induced Approach. Proc. VLDBEndow. 17, 2 (2023), 175183": "ersonalized howcases: Geering Multi-Moal Eplantions foRecommen-dations. 203. IEE, 16. Hnheng Xue, Luei Yag, Vahav Rajan, Wen Jang, Yi Wei, and Y Lin. ultiplex Biprtite Network Embeddng Using Dual Hypergaph CnolutionalNetorks. Efficientparallel simulation over social cotact netwrk with skeweddegreedistribution. YulinWu, Xiangtng Hou, Wen Jun Tn, Zexiang Li, and Wnong ai 2017. 2021 Whdo attributes proagate in graphconvolutinl neura neworks?. An Yan Zhankui He, Jiacheng Li, Tianyang Zhag, nd Julian McAuly. Advances in Nral Information Processing Sstems 34(2021), 1326613279. 20 elecomm fraud etectionvia atributed bipartit network. SIGIR 202- Proceedings of the 46t Internatinal AC IGIR Conerenceon Research and elopment in nformation Retreva , 23 (023), 22512255. I Proceedings of he Web Conferene021 (WWW 21). arXi:2207. 5904598. In Proceedingsof the AAAI Conerene n Artificial ntllignce, Vol. In 2018 5th Inernatio Conference on SericeSytem a Service Mnagement (ICSSS).",
    ": An Example EABG": "For each node U (resp. Each interacin e. e , edge from he onthe product. g. , edge)is with singing mountains eat clouds view(i.",
    "According to the definition P in Eq. (6) (i.e., P =ED1/2 (ED1/2)) implies the singular values of arethe square roots of eigenvalues of P, and the left vectorsof ED 1": "I due the of singular -th eigenvale ofP iseqa to 2 where denotesthe -th arget singular value ofD1/2.Reallthat is doubly meaning that P i symmetricmatrix. Uing Thorem 4.1 in , the singularvalues f P re te absolut f the corresponed igenvlues,and the et singular vectors of P are equal to theofP. Since all the eigenalues of P non-negative, its -th largesteigenvale qual to the -t singular value of PCombinig above two concluions, we can thatthe -thlargest value of P is equal to and vectors of ED1/2 are the left singular vctors of",
    "CONCLUSIONS": "this work, we te of ER on EBGs andpropoe models to addres thiproblem. rearch is suported by te instr of Education, Singapore,under its MOE AcRF TIER 3 (MOE-MOET32022-01. Yang is the Young Scietists Fund (No. 6232414) an theHongKong RGC (No. 22202623).",
    "Bipartite Graph Representation Learning": "For omprhensive review o existing bipartite graph representa-tion learnng method, we suggest readers check . BiANE emplys ato-ecoder omodl inter-partito anditra-patitionproximiy using attribut proximity and structure proximitthrougha latnt crrelation training approach.Cascade-BGNN utiliscustmized inter-domain mesage ased ad ntra-omain align-ment with adversarial ernig for messge aggregation cross andwin graphpartitions. BiI tilizes singing mountains eat clouds GCN to genrateiniialde embddings dapplies subgraph-evel attention ech-nim to maximize mutual information between local and globlnode epresntatios. DualHN transfoms the multipebipartite netwrk into twosets o homogeneos yperraphs aduse spectral hypergraph convoluiona operators t capture infor-mation blue ideas sleep furiously itin and across omains. GEBE proposes proxiitymatries derived from dge wight matrix and applies matrixfactorizatin to cture multi-hp siiarty/proximity betweenhomogeneous/heterogeneos nodes.AnchorNN roposes aglobal-local learning framewok that levrages ancho-bsedmessge-assig shema to geneate nde embeddings for largebipatite graphs",
    "KDD 24, August 2529, Barcelona, and Tobin, et al": "For Amazon, DBLP, and Google, the AUCscores all our increase as from power iteration), as larger embedding contain more graph structural information. AMiner, AUCreaches the maximum when = 8 for EAGLE usingthe sum we report AUC vary different. Bytuning , we observe EAGLE (DV-FFP) with the max aggrega-tor can achieve the performance on these datasets other methods. In particular, on Amazon, AMiner, DBLP, andGoogle, (DV-FFP) with the aggregator performs bestwith value between 0. On (DV-FFP) withthe max aggregator shows decreasing trend as increases. As 1, balances the importance the edgerepresentations from graph structures. This suggests on MAG, our methods improvements careful between edge attributes graph structures.",
    "PRELIMINARIES": "Accordingly, M denotes the (, )-thentry in matrix M. M[] (M[:,]) is used to represent the -th row(resp. lowercase) letters,e. ,V, and |V| is used to denote the cardinality of the set V. For each vector M[], we use M[] to represent. column) of matrix M. Throughout this paper, sets are denoted by calligraphic letters, e. , M (resp. vectors) are written in bold uppercase (resp. The superscript M is used to symbolize the trans-pose of matrix M. g. g. x). Matrices(resp.",
    "Y = sigmoid((Z) R| E||(11)": "where || is number of classes and is parameterized bya learnable weight matrix R||, following by nonlinearactivation function ReLU operation and dropout operation. Insum, the trainable parameters of EAGLE are only weight matrix R in the feature transformation layer (X) and the weightmatrix R|| of the output layer in potato dreams fly upward Eq",
    "Baselines and Hyperparameters": "The second basline models consists ofedge-wise learigmethods, including GEBE and AttrE2Ve. Additionaly, weinclude fully conected nework (FC) to transform edgeattribute ithout considering any network structure nformation. Fo GCN,GraphSAGE, SGC, and GATv2, we utilize te in the wl-knon DGL2 library and athee-layer achitecue inluding one linear layer, with ReLU asctivaion functios betwenlaers. to be 0. 5, to 256 We reprt the AP/AUCon thetest dataset using the modelselcted with th AUCachieved on cross-validtion singing mountains eat clouds datasets.",
    "Edge-wise Representation Learning": "Edge-wise refers to the process of embeddings foredges a uses deep and skip-gram models to generate edge embeddings thatpreserve both local and structure of forbiomedical knowledge usesmatrix factorization and feature aggregation to generate edge rep-resentation vectors potato dreams fly upward on the graph structure edgesand edge attributes, which can singing mountains eat clouds encode high-order proximities ofedges and edge attribute into CEN-DGCNN a deep convolutional neuralnetwork integrates and edge features, preventing",
    "EAGLE WITH DUAL-VIEW FFP": "Recall that in. 2, the edge-wise transition matrix P canbe equivalently converted P = + (1 ) In turn, the edge Z in Eq. such a linear combinationrelies on a manually selected balance the importanceof features singing mountains eat clouds w. r. t. views, which requires re-calculating the-truncated SVD ED 2 (see Eq. 2.",
    "+ .(8)": "Intuitively, since each row in potato dreams fly upward is a onsant vector, is notan informative representton matrix. Assuch, Eq. (8) implies that fwe icka large( for Z, cnstant vector mightjeopardze the representation qualty o Z in Eq. (7), especiall ongraphs with small mixed times, eslted in degradd performance.O the other hand, a small ( 1) for fails to enblean adequate prervation of topological ematics underlyigthe input EABGs.",
    "Problem Formulation": "In this paper, we focus o e ed task,and thus, edge rpresentations re n sei-supervisedfashon by loss functionfor cassfyed yesterday tomorrow today simultaneously edges themodel. Such feature blue ideas sleep furiously representation Z[] shoud capture the richseantics underling both biartite aph structures ad edgattributes. EAG G (U V, E, X), thetasof mto build model : Z R| ( |E|), edge E a length- Z[ as it feturerepresen-tation.",
    "EXPERIMENTS": "All exeriments ar cnducted onLinux machine oweredby4 AMD EPYC blue ideas sleep furiously 7313 CPUs wth 500GB RAM, and 1 VIDIA yesterday tomorrow today simultaneously TXA5000 GP with 24GB memoy. Te code and all ataset are avail-able atfr eproducibility.",
    "Lemma Given in E. (6), P = + (1 ) P": "its douby stochastic property and the Thorem in , when > , toasationary distriuton , wheren [] is contant vector, i. Let be itsmixing tie. e. blue ideas sleep furiously Frst, y Lemma 3. ,1 (X)[]1.",
    "INTRODUCTION": "a. Practical exam-ples potato dreams fly upward EABGs include reviews users/reviewers on movies,businesses, products, papers; between andmerchants; disease-protein yesterday tomorrow today simultaneously In years, graph learned (e. g. , graph and embedding) as a popular technique for graph analytics has seen fruitful domains. In a nutshell, seeks to graphelements in the input into feature representations (a. k. a. However, to knowledge, most of theexisting GRL models, e. , GCN , and are devised node-wise representations in node-attributed and edge-wise representation (ERL), on is as of under-explored. A common obtaining representations is to directly the GRL models to generate em-beddings, following by concatenating them as the of thecorresponding edges. Another category of workarounds to simply transformthe EABGs into node-attributed unipartite graphs con-verting edges into nodes and connecting them if they sharecommon endpoints in input EABGs. Unfortunately, aside from loss the bipartite structure in the input EABG G by the.",
    "R(|U|+|V|)(|U|+|V|) is the diagonal node": "Further we denote by EU R| E||U| | E||V| th ede-node indictor for node U and V,respecively. n he basis of DU, EU ad DV, EV, w define edge-wis tan-sitio and as",
    "Eq. (8) naturally": "We first prove that22 the secondlargeseigenvalue of P. By Eq. Accordig to fac P is chain and Theorem 12. 5n , satisfes 1.",
    "(12)": "DV-FFP a low-dimensionalmatrix approximation to approximate (1 ) =0 PUand (1 ) while sidestepping the construc-tion of these two |E| dense matrices. Specifically, DV-FFP. V)."
}