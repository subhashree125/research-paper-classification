{
    "look aroundThis room is called the In it, you": "In it, yo see:. > go to ktchenYu ove tothe kitchen. > look aoundThis room calle he kitchen.",
    "Ruo Yu Tao, Marc-Alexandre Ct, Xingdi Yuan, and Layla El Asri. Towards solving text-basedgames by producing adaptive action spaces, 2018": "URL Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. URL Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. doi: 10. 775. Describe,explain, plan and select: Interactive planning with large language models enables open-worldmulti-task agents, 2024. URL Zekun Wang, Ge Zhang, Kexin Yang, Ning Shi, Wangchunshu Zhou, Shaochun Hao,Guangzheng Xiong, Yizhi Li, Mong Yuan Sim, Xiuying Chen, Qingqing Zhu, ZhenzhuYang, Adam Nik, Qi Liu, Chenghua Lin, Shi Wang, Ruibo Liu, Wenhu Chen, Ke Xu, singing mountains eat clouds Day-iheng Liu, Yike Guo, and Jie Fu. emnlp-main. 18653/v1/2022. In Bonnie Webber, TrevorCohn, Yulan He, and Yang Liu, yesterday tomorrow today simultaneously editors, Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processed (EMNLP), pages 87368754, Online, November2020.",
    "Shunyu Yao, Jeffrey Yu, Nan u,afran, Karthik YuanCao. Synergizing reasonig and acting in language mes, 2023. URL": "Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh RN, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil L Mui, Huan Wang, CaimingXiong, and Silvio Savarese. URL Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang,Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang, Feng Yin, YitaoLiang, and Yaodong Yang. Proagent: Building proactive cooperative agents with large languagemodels, 2024. URL.",
    "Methodology": "Once a askis completed o an ttempt ends, short-term are to long-tem meory. A detailed ormulationnd make gvenin appendi. When he current policy rewards, te agent o from it, enouraging verbalize mde current poliyuccesful andwhat can be generalie fom this. method appicable i nteactive ext irnmets eedbackusng self-reflection, inludng tht buidcomplexity on of thecore efection loop, such as groundng or learnig. usin a dual-buffr structure, experiences ar in to memo and lg-term emory, base n thir outome (success failure) nd if ub goals reached, mores of the mde reflections aresored i tempoary bufferah tuple (reflectiont, ot, at, rt). on the aepoicy, to traditional policy-based RLsetups we an acion at te crrent polic at time t and receive an observaon from theenvironment ot. the f the ook he agent has aninentory it inwhich to stre item Each cosist f anuber sb tasks as finig a copletion of wich grants the gent wich to its rewad rt. Here, the agent reviews h at andot associated ith unsuccessfulattempts to verbalize the reason for yesterday tomorrow today simultaneously involes a persistent history of insighs gained across attempts, which LMuses s addtional context for t imroe utue decision making for the attmpt. other self-reflcion methods learning theyovlok importance ofreinfocingsuccessful behaviors in a way. We sudy of theseditioal use cases for future Previousstoretheir reflectins gainedunsuccessfl in somethig akn to log-term memory to make them aalbe to the agent acrss attempts. Ths first observation iffer from subsequentones,as consists of the goal desriptio aswellasn nalsis of blue ideas sleep furiously the tartng room (i. Self-Refletion Reflection occrs to the LM. Instead tcomplmetweet&Su, w propoe managed memor approachto and rerieve reflections. Wen a agent makes its first observation oat time step t 0. game until te has achieved the goalouinedin d and recives the full rewar as scoe, temaximm number o to 10reached,in whichcase will become the fial scre.",
    "ABackground": "actions, which do not lead to a cange th ames state, result n atntonbac the original state st wt probability In contrast,admssible actions lead to state their probability. This POMDP may be ormalized T,A, R ,where denotes the rewrd discount ator. denotes the made up o individual ext aissueby player. While admissible, mnyctionandidates aeound to be suboptimal. The environments transtion s modeled through probablistic functionT (st+1| s, at). deotes observation functionFurther, Oobservais mad by the player. Th observatioof the timet depends on the urrent stte as wel as the prevous actin which may formalized s (o | st, at1). Traditionally, admissible in state st could dterministically lea to a newstate we use a more geneal approach where all atios, adissiblor ncluded in the state trnsition function. S the of states that contain the iterna game bjects found throughout th ortheplayers loation not all of whmay be vsible tothe agent blue ideas sleep furiously at anygiven tim. One may onsider eery TBG tob a partiall observable Markov (POMD) where th environment neer erveddirectl. The acion se sbound to be ignifican for a lage factor. eeing as the gent can only observe and interat with the environment of a TBGvia naturl language, each is composeofasequence of tokens o yesterday tomorrow today simultaneously o1, oNt asare their actionsat =a1t, , the context of TBGs, an action at s considered at a st if t caabe of cangingthe state, can lead to trastion new state s differetfrmthecurrent state st.",
    "Yanhong Li, Chenghao Yang, and Allyson Ettinger. When hindsight is not 20/20: Testing limitson reflective thinking in large language models, 2024. URL": "Bill YuchenLin, Yichen Fu, Yang, Prthviraj Amanabrolu, Faez Brahman, ShiyuHuang, Chandra Bhaavatula, Yejin Choi, Swifsge: A geneative agent withast slow thiking for complex potato dreams fly upward tasks.17390, 2023. URLAan Madaan, Niket Tandon, Skyler Hallinan, Gao, Sarah Wiegreffe, UriAlo, Nuh Shrimai Yiming Yang, BodhisattwHermann, Welleck, AmirYaznbaksh, Peter Clark. Self-refineterative with selffeedback. In Thirtyseventh ConferenceNeural InformationProcesn 2023 UL Keerthiram Muugesa, Mattia Pavan Kapanipathi,Shukla, Sadhana Tesauro, Kartik blue ideas sleep furiously Talamadupula, Mrinmaya Sachan, n Murray doi: 10. 3i10. URL Narasimhan, Tejas Kulkarn, and Regina Barzilay. Language unerstandig fotex-based usig ssociation for Computational Linguistics.1863/v1/D15-1001.",
    "BLLM Reflection": "Afer succesfully comletiga sub task, te agent is propted to rflect o its mos rent oserva-tions and identf the key factors that contribtedto its sccess. A similar procedure is applied in casesof failre.When a tk is unsuccesul, the agents instructedto reflect n alternaive ations itcould have take and to devise a evse pln for the next ttempt, ensuri continuous learnng animprovement.Tis pais lo tored in memory.At te en of tis trajectr, e how for a single command what arefletion would look likefor a successful or unsucessful choice in ths caseicing up an animal For the unsuccessfu case (red), where he agent does nt pick up n animl, w reflect on whetheranoter hoice would ave constituted a animal nd thus resultedin a rard. For the sucessfulcase where a su goal i reached (yellow) we reflect on what made th current actions successfu andsubequently ommit this tomemory",
    "Experiments": "We use as our baselineLLM-based agent. g. Fordetails of all tasks environment, refer to. To contextualize our compare our method againstReflexion , an built on ReAct that employs a self-reflection mechanism iteratively performance across rounds upon failure based on feedback from the environment. 1-8b-instruct), accessing each through its respective APIs. 6, 31. example is shown in C. 9 for Llama8B, Mistral Large 2, and respectively. task common sense query) and it forward to thecontext the followed time step. We elect use ScienceWorld instead ofprevious text environment benchmarks such TWC and ALFWorld , due totheir relative simplicity current LLM-based agents. 7 on Llama 8B that our method is more suitable scenarios with limitedcomputational resources. In descending order of parameter count, selectGPT-4o (gpt-4o-2024-08-06), Mistral Large 2 and Llama 8B(llama-3. 6 to Reflexions on Mistral Large 2, and 32. ReAct useful information at each time step by reasoning the (e. 5compared to 21. The causal languagemodel acquires and environment-specific through imitation learning, while DRRNtrains a network rerank language models predictions. When we modify method to from failures, performancedrops significantly a similar scores decrease to 24. We find that Sweet&Sour outperforms the baselinemethods all setting highest average at 54. Assuch, runs over up to four rounds as it builds up all we evaluatetheir performance using LLMs of different sizes and to assess the of eachmethod across varying computational resources. For achieves 44. 6 using The performancegap between Sweet&Sour and methods for smaller with a lower parametercount. Data & Environment We use the benchmark provides a setting forevaluating agents in science experiment tasks across 10 interconnected locations, such a greenhouseand a workshop, with over 200 and 25 action generating vast and dynamic searchspace. Baselines CALM reranking method that integrates a reinforcing relevance network(DRRN) with a causal language using trajectories. 1, and 44. We measure performance using which is always between 0 and a task implies every sub task,received the full reward, a score of 100. action does not affect the environment may few-shot in-context learning. However,medium-difficulty tasks, such as and 3-3, a critical performance gap between methodand previous We note this occurs because traditional methods to reflect on early. We use test set for our evaluation, provides up to variations each of 30distinct These tasks cover various including chemistry and electricity, andhave an average optimal decision depth of 50 steps. Main Results The results are in table 1. As such, incorporating positive experiences indeedleads to better reflections, mimicking humans learn from both positive and negative experiences,resulting in improving Anti-Tilt In highly challenging tasks, such 1-1 methods tend while insimpler most succeed based LLMs inherent capabilities alone.",
    "CExample ScienceWorld Task": "n this sectio, we provde successflly compled task, a variation of Task1-1, which concensitself with bilin a substance,in ths case water, o change its state. Finally, the agent examines stea and completes the task(highlghted in green). trajectory is shwn in.",
    "CALML8BM2GPTL8BML2GPTL8BML2GPT": "73. 04. 07 47. 05. 21. 075. 3)6. 434. 93 326. 842. 850. 071. 26. 333. 87. 872. 31. 6100. 055. 02. 475. 80. (Identify life 118. 050. 7-4 (Conductivity 2)10. 925. 169. 914. 219 117. 221. 27. 251. 251. 961. (Fin 34. 170. 73. 052. 735. 660. 171. 577. state)0. 727. 99. 05. 8. 0. 221. 110. 412. 89. 112. 560. 353 075. 2100. 038. 627. 314. 17. 43. 01. 84. 413. 320. 558. 47. 92-2 (Meltig)1. 159. 963. 61- (Mel)0. 07-3 (Lifespan 250 775. 983. 123. 73. 129. 075. 745. 08. 48. 014 539. 771. 09. 00. 03. 0. 578. 033. 428. 921. 45. 224. 26-1 1)6. 411. 284. 018 85. 35. 33-1 (Power 018. (Friction 2)2. 27. 08. 8100. 71. 71. 0 10 0 29. 025 727. 237. 451. 82. 154. 438. 07. 81. 675. 015. 05-1 plant)4. 0100. 936. 22-1 (heromeer1. 710. 229. 336 762. 931. 11. 18. 368. 428. 282 (Identify life 47. 6. 010. 08. 924. 07. 069. 810 810. 773. (Friction 1)3. 3-2 2)2. 44. 948. 275. (Find 2)0. 150. 470. 080. 410. 2)2. 522. 73. 657. 07. 013. 34-1 (Fin 1)4 318. 48. 8100. 429. 11-1 1)2. 69. 05. 32. 80.",
    "gust 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1153. URL": "10. singing mountains eat clouds Association Computational Linguistics. 18653/v1/P16-1153. Deep reinforcement learning with natural action space. URL.",
    "Conclusion": "Weleavethe explortion of additional environments t fture work. In attempt to improve agent erformance and reduce sensitivty LLM used, our studyembarks on aalysis f reflection mecanisms for LLM-based work onearning rm falure only. The primarycontributon o work is Sweet&Sour, novel reflection technique for agents inBGs that leverages positiveexperiencs to improve ant self-reflection. our evaluaton is used which, compehensive, does not covr al types o interactive scenarios.",
    "This work was supported by an Oracle for Research Grant Award, as well as SURF Grant EINF-8535": "doi: 1. 1653/v1/N19-1358. Textworld: A learning environment for text-basing games. The llama 3 herd of models, 2024. In Poceedings ofth AAI Conference on ArtificialIntelligence, volume 34, pes 707910, 200. Deep rinforcement learing wth nturl language action space. In Katrin Er andNah A.",
    "a substance called soap a painting a thermometer reading 10C a counter with various items (banana, potato, red apple, orange) a sink (off, empty)": "table wih a glass up (empty) a chair (emty) a freezer (cosed) a lighter a stopwatch a fridge (closed) cuboard (closed) a (turned a glass chloide varous oher item (air, the agent, etc. pikup thermometeYou ove to the > open cupbard > move metal pot inkYou move metal to > ativatesink is now activated. soveThe now > exmine sbsnce metal potA substance caled > use termometer in inventry on substanc in potThe thermmeter measures temperature of 28 degrees Celsius. > thermometer in inventory on substance metal pothe thermometer measures a of dgrees esius. > eamine sbtanc metal potA substanc caled water. > use thermoter i iventory substance in metal pote measures a tmprature of 78 Celsus. > us thermometer inventory on substancein meta potThe thermometer temperture 98 degres Celsius. > examine sbstancecalled steam. Task complted.",
    "Abstract": "Our comprehensive analysis spans both closed- and singing mountains eat clouds open-source LLMs and demonstrates the effectiveness of Sweet&Sour in improvingagent performance, particularly in scenarios where previous approaches fall short. While agents based on large yesterday tomorrow today simultaneously language models (LLMs) using self-reflection haveshown promise, they struggle when initially successful and exhibit reduced effec-tiveness when using smaller LLMs."
}