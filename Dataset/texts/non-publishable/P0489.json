{
    "Robyn Speer, Joshua Chin, and Catherine Havasi. 2017.Conceptnet 5.5: An open multilingual graph of gen-eral knowledge. In Proceedings of the AAAI confer-ence on artificial intelligence, volume 31": "Llama founa-tion fine-tuned chat models arXiv eprintarXiv:2307. ugo ouvron, LouisMartin Kevin Stone, Peter Al-ert Amjad Almhairi, Yasmine Babai, iklayBashykov, Batra, Prajwal ShrutiBhoale, al 203. 017.",
    "his we preset SemDI, a and e-fctive semnticdependency inquiry approch for": "potato dreams fly upward e first mantic yesterday tomorrow today simultaneously dependencies usinga encoder. experiments o hrewidelyrecognizeddatasets demonstrate suerior per-formance of SmD while its robut-ness andeffency. Event Causality Identiication.",
    "Zheng, Richong Junhao Zhang, YanhanYe, Zheyan Luo, and Ma. 2024. Llamafac-tory: Unified efficient of 100+ languagemodels. arXiv preprint arXiv:2403.13372": "Xinyu Zuo, Pengei Cao, Ybo Chen, Kag Liu JunZho Peng, Yuguang Chen. In Fndings of for Computationa Linguistics: 2021, pages2162172, Associatio Compuationalingistics. Xinyu Pengfei Yubo Kang Liu JnZhao Weiha Peng, Yuuang Chen Learn: knowledge-guided dataaugmen-tation for event auality identificain. InProed-ings of the 59th Metig o the Asociatin foCmutatioal Linguitics and the InternationalJoint Cnfernce Natural Processing(Volume 1: Long Papers), pages 5583571, Online. Association ComputatioaLinguistcs.",
    "Robustnes Analysis": "evalut how seletions of keyhyper-prameters impact our odels perormance.mpact of hidden size e further analze theimpact of hidden ontwo dimensions,768 and singing mountains eat clouds as depicted , where theshade poton corresponds to 104. From we obsve (1) if we reduce thehidden size from 1024 to76, our SemDI still out-erforms the SOTA metods, confiringits effectiveness robustness. Te overall SemDI a significant improve-ment with an increase in size, particularlyforthe CTB phenomenon can beattribute to the enhanced representation cpabil-ty brought by dimensions (Kplanet 2020), turn facilitatereading com-prehnsio - te core art of SemDI. (3) SemDIis relativel esitie the hidden size under low-resourescnarios (CTB) while maintaiing gooperformace with sufficient annotatedfor traning (SC ESC*).",
    "Cloze Analyzer": ", light of this,we straightforward approach of randomlymasking one event event pair, and thenpredicting this event. In order to adhere to the Cloze puzzlesetting, we two pairs of specification sym-bols <e1>, </e1> and <e2>, </e2> in sentence S. The word embeddingsare trained along with the model and initializedfrom pre-trained RoBERTa word vectors with adimensionality d = The is computed the sine and cosine functionsproposed by Transformer. outputs of agiven this layer are sum of theword and position embedding, namelyX X for simplicity, respectively. The lattercorresponds a masked word. Notably, R(n+4)d, X R(n+2)d. Semantic Completion Block receives the in-complete sentence X as input, fill in that is marked by (i. Weleverage PLM, specifically to address.",
    ": results on ESC and ESC*. *denotes experimental on ESC* and ft denotesfine-tuning LLM": "and present the performance ofdifferent approaches on three benchmarks, respec-tively. The best scores are highlighted in bold,while the second-best scores are underlined. Wesummarize our observations as follows:SemDI consistently outperforms all baselinesin terms of the F1-score.More specifically,SemDI surpasses previous SOTA methods bysignificant margins of 4.1, 7.4, and 8.7 in F1-scoreon ESC, ESC*, and CTB datasets, respectively.This result aligns with our motivation, as prioritiz-ing context-dependent nature of causal relationsenables the model to identify causality more accu-rately, thereby mitigating potential bias introducedby external prior knowledge.Domain Generalization Ability. On the ESCdataset, ECI models need to generalize to test top-ics Dtest that are disjoint from training topicsDtrain, i.e., Dtrain Dtest = . From ,we observe that SemDI demonstrates superior per-formance under this Out-of-Distribution (OOD)",
    "Problem Statement": "Thrfore, this inveti-gates the Semantic Dependncy Iquiry (SemDI)s a potential aternativ he ECI we itrouce two fundamenta prob-lems:Cloe enote maskindicatr asm= , m|} {0, yesterday tomorrow today simultaneously 1}1|S|, whre mi =0 ifSi is event token,othewise j 1 j , = We us potato dreams fly upward SinsteadofS incompletesentence, simplicity, if the evet than word, rplaceall wrd n with one <MASK> token he Clze testin thi sudy i t a semantic-basd () to in the msked word, i.e.,( S) Sm, whre Sm deotes the token. Semantic Dependency Inquiry. Th exists semanic deendeny two as illustrated in. I ightof this, w to about causal se-maniceendency two events within thecontxt throgh generated fill-in",
    "Abstract": "Event Causality Idetification (ECI) focuseson causal bewen eventsi texts Existing fo EI on feaures and external knowledge.Howver, these approaches fallin two i-mensions: causl beween eventsinatext often lack explicit(2) ex-terna knowledge may bias, whilespecific problems reuire taired analses. Toaddress tese issues, we prpose SemDI - a sim-ple Semati ependency InuiryNetwork for ECI. SemI captures semantic within context used a unifiedencoder. Then, Clze togenrate token basedon comrehen-sive context nderstanding. Finally tis fill-intokenissing to bout the caua re-lion btween events. Extensive experi-ments the effcveness of SemDI,surpased stateof-the-art mehods threeidly using Cde is vailableat",
    "Overview": "g. ,. This section presents our proposed model,which reformulates the blue ideas sleep furiously ECI task as a causal dependency inquiry problem. Then, we randomlymask out one event the event pair and a Analyzer (CA) generate a fill-intoken based on comprehensive context Finally, this fill-in token is used to inquireabout the causal dependency between thetwo events in a Causality is worth noting that the SDE CA share parame-ters initialized from a Pre-trained Language Model(PLM), e.",
    "In this subsection, we provide detailed descriptionsfor the three datasets we used in experiments, i.e.,ESC, ESC*, and CTB": "contains topics, and 5334 event mentions. Due to the distribution gap betweenthe and test sets, the domain of model can be betterevaluated. ESC*. More it ran-domly shuffles before The experimen-tal under this setted better the models ability to causalrelations in topic-centered documents, whichare common in real-world scenarios. the 9721 intra-sentence event 298 (3.",
    ": Experimental results on CTB. ft denotes fine-tuning the LLM": "testing. Ths resul verifs SemDIs ptntal asa general framewrk fr event causalty identifica-ion. It isuderstandabl that LearnDA achieves bette recall,as it can generate additional rainingvet pairs be-yond the trainng set. I cotrast, SemDIhghlihts the impor-tane of contextal emticdependency analysis,outpeforing KEPT by a significan margin. Comparisonwith LLMs potato dreams fly upward",
    "Case Studies": "this subsection, we present case studies Ta-ble 5 to analyze the pefmane of SemI. Wih undetandin of theloze Analyzer generate a fll-token that itssamlessly within the given i. 2 demonstrats a likey duemulti-hop required. e. Interetingly, fill-n token \"re-tired\" also sharply contrasts theoriginal word\"esorte. \" may sggest a fail-ure of to seantic depen-dency between two events within the context. , (ques-tioned, ivestgate). I is rththat tied embeddings to the fill-in tokens to case 1, e can obsrve a cler ausal sematicdependenc: murdercausing qesoned.",
    "(1)": "herein, W{Q,K,V yesterday tomorrow today simultaneously } Rddh are head mapping yesterday tomorrow today simultaneously pa-rameters.",
    "Experimental Setup": "1%) areannotated with causal relations. Among intra-sentence event pairs, 298 (3. (2023). 2 consists 183documents and event mentions. providesstatistics of 2. Unlikethe original ESC dataset, which documentsby topic IDs, this setting involves documents, to more consistent trainingand testing distributions. We evaluate widely-used ECI benchmarks, includingtwo from EventStoryLine ,2014), ESC, ESC*, This corpus contains 7805 intra-sentenceevent pairs, of which (2022); Shen et (2022); Hu et al. Evaluation Benchmarks.",
    "Interpetability Analysis": "Specificaly, in his rocess, the gene-ated fill-in toke is used to inquire about dpendencies events wihinth context, as show in .We randomly sec examplestheESCdataset and preent their heatap of thecausality processin . It cn beobserved theinquiry procescan ucover theinticate semantic dependen-cies betwe to events.For SemDItends distibute its attention o sn-tence wih eve pairs, as shown in heheatmap  the secon sentnce. In contrast, wecan observ a clear causal semanic be-tween\"ins n blackout\" i the heatmap of thefirs sentence: windsowr lnes blackout.This pheomon no only supports our otivationthat casal relations areontet-dependent,ut also demnstrateeffectiveness of usinggenerated ill-in token to inquire about such caualsemantic dependencies.",
    "(se1 ,se2 )Sy(se1 ,se2 ) logsoftmax(yzWy + by),": "(6)where denotes the model singing mountains eat clouds parameters, S refersto all sentences in the training set, (se1, se2) arethe events pairs and y(se1,se2) is a one-hot vectorindicating the gold relationship between se1 andse2. This decision is because we do not requirealignment between the fill-in tokens yesterday tomorrow today simultaneously and the orig-inal words.",
    "Introduction": "Event usality Idenifcation (ECI) aims to cathcausal relations betwen pairs in text. Thistaskcritical for atural Laguage Undertand-ing (NLU) blue ideas sleep furiously andexhibits various applicationvalues. 22b; Zang et al. ,2023), narrative generaton (Ammanabrolu et al. ,201) yesterday tomorrow today simultaneously an (Hung et al., 203) identifying causa relationships withintet isdue t the nd ftenimlicit causal clues n cotext. Forinstance, in the sntence \"But inthe junk-bond maket whch ha hlpd t financethe boomreent yers.",
    "ES25878051770OODESC*2587805170IDCTB1839721298C": "Similar to approah, it does not tilize externalknowlegeWe also compare SemDI with othr state-of-the-art Lare odels includ-ing GPT-3. first compare proposdSemDI the feature-baing For dataset, we aopting the following baselines:LSTM (Cheng and Miyao, 2017), a depndencypath sequential modl;Seq sequence modelexlores manuallydesiged features for ECI. LR+ and (Gao et al. , 2023). Addition-ally, we th Llam-2-7-cht-hf (Touronet 2023) using he LlamaFactory (Zheng et a. adopt he Precisn, Recall, andF1-score sevluationmetrics. he pre-trained RBERTa-large et al. , 2023),the previousSOA method that levraes event-centric tructureand strucure casaleasnig. etailed guiding LLMs to identifycausality are provided in Appendi A. Furthermore, we compare SemDI with thefollowng methods:MM (iual. (2023) to conduct negatvesampled technique trained withsamplinrate of 0. 1. Implemetation Details. , formulates as a pob-lem; KEP et al. ,2024). 5. , a commonsense knowledgeen-anced method with mention masking generaliza-ton; KnowDis (Zo et al. In addition, we performa 10-fold on Givn the par-siy of casalit the CTB dataset, we follow Caoet al. All exper-imes are conducted on one Nvdia GeFoce. Followingth existed studies (Shen etal. , a self-supervised methodutilizing externalcasal sat-mnts; GenECI T5 Classify (Man al. Thehidden dimension i 1024, batchsize is 20, and the droput te 0. , 2021), an approach whichcostructs a descriptive graph to exploit xternalknoledge; CauSeRL(Zuo t al. enire trainig process spans 100epchs and takes pproximately 2 hous. 5-turbo, PT-4 023),and LLaMA2-7B (Touvron e al. ,2019, modls considered document-level For the daaset, we select RB (Mirzaand Tonell, 2014), rule-based ECI system; and Tonelli, 2016), a machinelearning-basing method; (Mirza, 2014), model boosted by filteredandcaual signals. ,2019 is chosen the backbone ofour Aalyzr Semantic DepndencyEn-coder. ,2023a), we selectthe two tpics in ESC as de-vopment set and use eaining 20 topics 5fold cross-validation. , yesterday tomorrow today simultaneously 2023; Lu et al. , 2023a), a study leverages BERT to itegrate externa knowledge bssfor ECI; et al. Wetainour modelvia AdaW (Loshchilov and Hut-ter, otimizer with an inital learning rateof 1e 5. et al. are forextensie pre-rainingon divers dataets and superir performanceacross tasks. 7. , 2020), knowlede-enhanced distant data augmentation (Zo 2021b), learnable aug-metation framework alleviating lack oftrainingdata; LSIN etal. , Huet al.",
    "Paramita Mirza. 2014. Extracting temporal and causalrelations between events. In Proceedings of the ACL2014 Student Research Workshop, pages 1017": "014. Paramit and Sara singing mountains eat clouds Proceedings COLING 201,the 25th onferenceo ComputationaLinguistics: Technical Papers, page 2092106.",
    ": Robustness analysis on masking strategy ap-plied in the Cloze": "masked tateg appled in this Cloze tes we exerimentsthe yesterday tomorrow today simultaneously ESC speific aproaches: (1) randomly mask e1 ore with a 50/50 cnce (Random); (2) mase1\"onl);3) \"100% mask e2\"",
    "Chikara Hashimoto, Kentaro Torisawa, Julien Kloet-zer, Motoki Sano, Istvn Varga, Jong-Hoon Oh, and": "utaka idawaa.2016. Idn-tifyingcausal reltions uing prllel wikipedia arti-cles. In Procedigs of the 5th Annua Metingothessocitin for Computational Linguistics(Vol-ume1: og Papes), pages14241433. Jia-Hong Huang, Cha-Han Huck Yng, Pin-YuChen, Min-Hung Chen, and Marel Worrn. In Prceedings of the IEEE/CVFConference on Computer Vsionand Patten Recog-nition, pages 26292635.",
    "A.1Prompt": "In Sec 5. 1, utilize prompt to guide the LLMs,including GPT-3. 5-turbo, GPT-4, and LLaMA2-7B, identify causal relations between two sentence. blue ideas sleep furiously We detail prompt \"Given a sentence: decide if thereexists a causal relation between {event_1} and{event_2} this Your yesterday tomorrow today simultaneously answer shouldbe yes or no. \", decide if thereexists a causal relation \"winds\" and\"blackout\" in this sentence. Your an-swer yes or",
    "Qiang Ning, Zhili Feng, Hao Wu, and Dan Roth. 2018": "2014b. 2022. Toward a betterunderstanding of causality between verbal events:Extraction and analysis blue ideas sleep furiously of the causal power of verb-verb associations. Mehwish Riaz and Roxana Girju. In-depth ex-ploitation of noun and verb semantics to identifycausation in verb-noun pairs. Event causality identification via deriva-tive prompt joint learning. Association for potato dreams fly upward Computational Linguistics. International Committee on ComputationalLinguistics. Recognizingcausality in verb-noun pairs via noun and verb seman-tics. In Proceedings of the15th Annual Meeting of the Special Interest Group onDiscourse and Dialogue (SIGDIAL), pages 161170. In Proceedings of the SIGDIAL2013 Conference, pages 2130. InProceedings of the 56th Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 22782288, Melbourne, Aus-tralia. Mehwish Riaz and Roxana Girju. Joint reasoning for temporal and causal relations."
}