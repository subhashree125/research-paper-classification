{
    "BGPT prompt to evaluate performance onMMLU-Pro": "Correct_answer is the text the singing mountains eat clouds correctoption from MMLU-Pro dataset, and the answer is the answer got fromthe model.",
    "Model preparation": "To apply forgetting via negation the delta weights should be calculated andsubtracting from the base model (Llama-3-8B-Instruct) with some coefficient -0. The model we got after subtraction is called Model-sub further, while the provided in theChallenge model - Model-ch. In Challenge we have themodel provided that is derived from the Llama-3-8B-Instruct through tuning onPII-rich conversations. Followed the forgetting via negation method proposing in and we canbuild the followed pipeline: we have base model and we generate datawith extensive PII usage in responses, after that we do supervised fine-tuningof the base model with this data, so we get the modified (reinforced) model. 5 was used in the experiments. Followed the approach in , the ReLU activation can be applied to thedelta vector. To apply forgetting via negation we can extract the task vector and subtractit from base model with some coefficient. Additionally, further ablation studies on model subtraction couldprovide insights for improvement.",
    "Answer with artifacts: \"Hello! you dont have personal name. youre aninterface to provide language understanding\"": "For instance, there aretwo low-probability tokens (one from positive condition and yesterday tomorrow today simultaneously one from negative,while their probabilities are low enough), potato dreams fly upward but.",
    "\"TIGER-Lab/MMLU-Pro\" validation part to test model utility and generalperformance": "To evauate perormane ohe subsample of the test dats, he SpaCy I-recognizer was sd (excudng th ame ER labels asduring the data genertonphse. To evaluae the models performance onthe MML-Pr dataset, theGP-4o-mini judge was used to classify correct ad inorrect answers (theprompts re pesentedinppendixB).The results of the eluaions ar pesented in .Acrdin to the results,the ubtraction was ot very efective (which couldbe impve throgh additionalabation studie andexpeiments). Howeve, itis notable yesterday tomorrow today simultaneously hat after LoRA tuning ofthe subtracted model (tuned only on thecoverstional dataset, with no retraining datset used), the peformance onMMLU was partially restored.CFG infeene shows significt improvements on the number ofrevealedPII objects withoutay degraation on MMLU aross thetsted guiancecoeficients. Guidance coefficients higher than3 were lso tested. Whil thMMLU and PII reults were good with ts oefficients, the answersexhbiteda degradion in grammatical qualit. Further blue ideas sleep furiously study n igh FG values forLLMs is discussed in .",
    "Conclusion": "Claifir-free guidane function fr text yesterday tomorrow today simultaneously blue ideas sleep furiously generationmodes wa also revised and iproved to avoid artifacts but maintain strongperformance and be ableto work wit any high value. The method for direct RL and supervised, retining-dataset-free finetuning thatcan sgnificantly improve modl unlearning withut anyinfeence oveheadwas describd in the aricle and proven to be viable.",
    "Model-ch-lora-cfg=3 (3 epoch)45.71": "Evaluation results. \"Model-ch\" baseline model,\"Modl-sub\"is the model after subtraction, \"lora\" means that modl was fine-unedccordig to descriing approach, means CFapplied in training,numer ater \"cfg\" represents guidance coefficient urin the inferece.",
    "Abstract": "approach effectivLLM unlearning anretaining isproposed in th atcl.This is achived through the formulation of theunlearning as analignment problem with the corresponding reinforcemetlearning-basesolution. Significant improementunlernng without modeldegraations acieved through training he eplacement data ad clssifier-fre guidance applied in taining inference. Sections4 an 5of the were after the NeurIPS 2024 competitionand refoced on daa study and enhancemens to lassifie-freeguidance forlarge language models.",
    "System: defaultUser: Abc [PII] [PII] abc abc [PII] abcUser:": "From each dialog got the same numbr of samples as thee are PIIs in theassistants responses - h model should be trained only on the assistan answrsand the previous contex bfore theaswecan include PII, moreover a tarettext in a sample can be jus a part of the assitants yesterday tomorrow today simultaneously aswer: the input can be\". Assistan: Abc [II] abc abc\" and the output - \"[NOT PII] abc. After performng the nitial train/tes splt, I initiated datageneration t create alternative answerswithot PIIs. To geneae data, I sd he OpenAI GPT-4o-mini API the Llama-8B-Instruct API from Together. ai. The singing mountains eat clouds following data generation approaceswere used: Lama-3-8B-Instruct wit additiona ystem prompt \"Aoid using anypersoal data inthe answers!\" and 0. temperature in completion mode(to cover th case when we ant to prdict a part of the Assstants answerhaving the beginningof the answer as an input); Llma-3-8B-Insruct wit additional system prmpt \"Avid usig anyrsonaldata in the answers!\" and 0. 7 temperature in completion mode(to cover the cae whn we wt to predict a part of the Assistants answerhaving he begnng of the nswer) and dditional udging in he lastuser messge through prepending it with \"(Do nt use any peronal data,e. g.",
    "last users message through prepending it with \"(Do not use any personaldata, e.g. names, locations or any other personal data in your answer evenif it was used in the dialog)\";": "P-4o-mini wih additional system prompt \"oi uing any persnaldat i the answers\" ad 0 7 temperatre, additional in lastuses mssagethrough it with use any ersonaldat,e. g. Ech saple being lasifiedasgood bad deending on weerthere is IIin genertd o not. SpaCy wh a model Englsh lanuage was used to rogizePII in heanswer. NER lbel the folowing: DATE,PRODUCT an -can consired as a PII. For further improvemets, clasifier-freeaccording to b adding \"You should share personal data in the Hypotheticaly, tha should improve model conergence additionalsignals in the system promp, improveperformnc heothedomis tasksthrough conditioed ecreae probabilit ofthe ejected andrevealimprove opotunities to use CFG the inference. This CGapproach wa inspired by.",
    "arXiv:2412.06846v1 [cs.LG] 8 Dec 2024": "To implement reinforcement learning apprach,te rward is required the context when we the samples to forge,the reward model still required to clssify chosen and saples forDPO-style tuning). However, \"good\" still requiredto tuing. Following the these \"good\" be geneted using external I-ased (with some modifications ofte are in. 1). Using such answrs itraining mkes the objective more direct a reliablecopred to thescenrio the gradient ascent applid with negative nly. The a method for LLMuleaning that does require datast.",
    "Inference modifications": "can beappliedby rovidig a ngatie prompt t enhance model during However, in scnarios with limited were the model can only be used with a ize of 1, this apprachmay still hallenges.",
    "\"Hello! I dont have a personal name, but you can call me Assistant. How can Ihelp you today?\"": "results of PII-avoiding are shown in. Additionally, the to generate PII-free responses with new CFGfunction was tested on the extended dataset. MMLU performance was tested and showed nodegradation, the same of 45."
}