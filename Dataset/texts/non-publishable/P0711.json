{
    "Fanqi Wan, Weizhou Shen, Ke Yang, Xiaojun Quan,and Wei Bi. 2023. Multi-grained knowledge retrievalfor end-to-end task-oriented dialog. arXiv preprintarXiv:2305.10149": "Tsung-Hsien Wen, Vandyke, Nikola Mrksic, Mil-ica Gasic, M Rojas-Barhona, Ste-fan ltes, and Steve 2016. A task-oriented dialogue system.arXiv preprint arXiv:1604.04562.Ian GandHongzhi Zhao. 2022. Graph-emdialog Optimizing end-to-end task-orinted di-alog systems using raph memory networks. In Pro-ceedings f the AAAI Conference on Atificialvolume 6, page 1150411512. Tianbao Xie, Chen Hry Peng Shi, Ruiqi Zhon,Torstn Scholak, Michiiro Yasunag,Pengcheng in, Sida I Wang,et 2022. Unifiedskg: Unifying kowledge grouding wi ext-to-text mods. rXiv reprint aXiv:221.05966.",
    "Datasets and Evaluation Metrics": "We onduct experiments n three KB-attachedtsk-oienteddialogue datasets: MultiWOZ 2. 1MultiWOZ)(Eric et al. , 2016). The nowledge basesare codensed with al the entities that met theuser oal of the curent dialogue. The tatitics ofthe bencmark datasets are listed in. Folowing previous work (Tian et al. , 2022; Xieet al. ,2022), we opt BLEU (Pap-ineni et al. BLEU as-sesses he fluency of a generatd response by mea-sring its n-gram overlap with reference response,while Entity F1 evalutes the accuracy of embed-dd knowlede by micro-avraging precsion andrcall score of atribut alues in thegneratedrespnse.",
    "Yuxin Xie, Zhihong Zhu, Xianwei Zhuang, LimingLiang, Zhichang Wang, and Yuexian Zou. 2024. Gpa:Global and prototype alignment for audio-text re-trieval. In Interspeech 2024, pages 50785082": "Doris Xin, Hu singed mountains eat clouds Aditya Paramswara, and eo-ls yesterday tomorrow today simultaneously Polyzotis. 201. mahine laringpielins: Epirical aalysisand oppor-tuities. InProceedings o the 2021 on management f paes 26392652 Wen-a Yih, Kistna utaova, John C Platt, andChristopher eek. 2011.Learning disriminativeprojections for text meaures. n Proceed-igs fifteenth on omputational lanuage learning, pages",
    "Introduction": "The advent of an intligentera has seen rapidadvancment systems(TD), demonstrating their pottial and flexibi-ity across varous like healthconsulting, inancial srvies, home automa-tin. 2023)icreased complexity (Goyal et al.022; Xint a. ,2021), and chllenges (Qin e",
    "Generalization Ability of ReAL Retriever": "are in Ta-le Theadaptiv pre-trained retriev enhancesthe Entit F1 core across both atsets models, indicting sperior entity recognitioncapability. LEU shows minor changs,wit sight improvments ggestingtat the retrievers impact on overal text gnratin qulity is miniml but generally poitive. Thisindicatesthat the proposed methd effectively. Topve strog universality and gneraliza-tion ability of proposd retriever pe-trainingmthod, we appyour pre-trained to twoadvanced models.",
    "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina 2018. Bert: Pre-training of deepbidirectional transformers for understand-ing. arXiv arXiv:1810.04805": "From retrieval to genera-tion: A simple unified model for end-to-end task-oriented dialogue. preprint arXiv:1907. Ding, Zhihao Yang, Ling Luo, Yuanyuan Sun,and Lin. 2019. In Proceedings ofthe Conference yesterday tomorrow today simultaneously potato dreams fly upward on Intelligence, 38, pages 1790717914. Multiwoz 2. 01669.",
    "Tianyuan Shi, Liangzhi Li, Zijian Lin, Tao Yang, Xiao-jun Quan, and Qifan Wang. 2023a. Dual-feedbackknowledge retrieval for task-oriented dialogue sys-tems. arXiv preprint arXiv:2310.14528": "eijia Shi, ewon Mi, Michihiro Yasunaga, in-joo Seo, blue ideas sleep furiously Rch James, Mike Lewis, Luke blue ideas sleep furiously Zttle-moyer,and en-tau Yih eplug: Retrievl-augmented blck-boxlanguage models. 2652. 202. End-to-end train-ing of multidocmen rade ad retriever for open-domain question answring.",
    "Limitations": "2022. Improving anguage models by etrieved from tril-lions yesterday tomorrow today simultaneously of blue ideas sleep furiously In International conference on ma-chine 22062240. Di Luo, Xiuying Che Lemao Li,Dongyan Zhao, and ui Yan Lift yourselfup:generaton with self-emory.",
    "Comparative Baselines": ", 223), and MAKER (Wn et yesterday tomorrow today simultaneously al. ,2023) decouple the etreva and genration pro-cess. , 2022), Q-TOD (Tianet al. , 2024) inteate knowl-edge retrieval and response generatio in one singlemodel. yesterday tomorrow today simultaneously FG2Seq (Heet al. We performa cmprehensive comparative studyaainst ReAL b considering the baseline withdifferent retrieal strategies. ,2022),UnfiedSKG (ie et al. , 20and Uni-TOD (ing et al. , 2022), ECO (Huang et al. ,2020), DNET(Raghuet al. In conrast, Diglo (Rony et al.",
    "Qualitative Analysis": "1 datset. For a givenuser utterance, our sytem succesul retrieesetities that meet the blue ideas sleep furiously users goal while xcludngirrlevat dstractins. Notably,when the usersgoal hanges,suh as in he second turn whn theuser requests any Chine rstaurant, singing mountains eat clouds our retrieveradapts and retrieves the relevnt entity accordingly.",
    "BLEUEntity F1BLEUEntity F1": "ReAL18. 1155. 1317. 8354. Ladapt17. ( 0. 1. 87)54. 34 ( 79)17. 16 0. 67)53. 23 ( 1. 22 ( 17 59 ( 1. 05)w/o 07)54. ( 1. 87 ( singing mountains eat clouds 0. When ablated Ladapt and Lpre, substitute Ladapt with the vanilla contrastive loss Linfo.",
    "Visualzation": "To better the impact of our retriever onthe overall KB distribution, visualizedthe KB entity probabilities the decoding posi-tions where we singing mountains eat clouds generate the entity attributes Road and 3315702. shown in ,the first and the second and fourth the highest for generation, accurate retrieval and theeffectiveness of adaptive retriever pre-training.",
    "Implementation Details": "employ BERT (Devlin et al. , 2018) as en-coder of our entity selector attribute employ T5 (Raffel et al. , 2020) implementthe generator. We conduct all 4 24G NVIDIA GPUs and selectthe basing on performance.",
    "End-to-end Task-oriented Dialogue": "(2018), Qin et al. Incontrast, Huang et al. (2020), and Raghu et al. , whichare subsequent input for Di-alogKG (Rony et al. , Madottoet al. From the of knowledge re-cent years have witnessed remarkable developmentin E2ETOD systems. , 2022) uses a neu-ral network entities from the flattenedrecords. (2022) and Ding et al. since language models (PLMs)take the lead in tasks, the knowledge are linearized to be encoded by PLM en-coder (Xie et al. , only consider part of DAP (eitherthe distractions mis-alignment), solve the problem jointly with ourproposed framework. earlier, retrieve-then-generate been suc-cessful, its paradigm of decoupling re-trieval generation can lead to DAPin dialogue systems. , yesterday tomorrow today simultaneously Wan yesterday tomorrow today simultaneously et al. While previous (Shi et al.",
    "Knowledge Retriever": "Enhancing language models with pertinent infor-mation from diverse sources has proveneffective in improving performance across NLP tasks (Khandelwal et 2019; al. , 2020; Xie et al. (2023) reduce false positive problemby pre-training the retriever with Cheng et al. Based on this,DPR (Karpukhin et al. utilize unboundedmemory pool and employ a memory selector tochoose output as the memory for the round. , Asone blue ideas sleep furiously most successful retrieval structures, thedual-encoder architecture et 2011) encodesqueries and passages separately. , 2020) trains the retrieverwith in-batch documents and samples negative ex-amples for contrastive enabling pre-trained retriever perform open-domainquestion REALM (Guu et al. , 2020) consider retrievedpassages as latent variables train the system jointly. , 2020)and RAG et al. The relevance be-tween computed throughinner distance. prior models, REPLUG (Shi et treats frozen LM as a black-boxmodel and augments it with a retriever. Lei et al. , 2022; Lewis et al.",
    ": Performance of applying our adaptive pre-trained retriever to advanced models under the large-scalebenchmark setting. denotes our re-implementation": "ablation results of the metric-drivenloss Lalign show its validity in aligning retrievaland generation. Therefore, proposed methodReAL is as each of its components con-tributes to its performance, and these components leads to a noticeable de-cline in both",
    "Abstract": "Rerevng domain knowledge nd pro-vidng hepfu are crucial in an efeve end-oed tak-orienting dia-ogue (E2ETD) approacestothis fielda retrieve-then-geneaeparadigm traintheir sstems one se-ific domain. a hard negative en-ities), whch is even more intractablwhencountlesspieces of knowledge from differentdomains are blending in a T alleite propose the Relevance-awae Adative (ReAL), tw-stage training ramework hardnegaives step-by-stepaigns retrieval In the we top-k adaptiv contrative loss nd utilizethe diegenedriv feedack the frozengeneratr re-train rtiever. Thoroughexperiment on threebenhmark datastsdemonstate supe-riority over existing eensiveanalysis valdtig it trng capabilities ofovercoming in- and cross-domaindistracions. the sec-ondstage, e proose using te etric an t alin withenertin.",
    "Ours17.5153.2692.44MAKER17.1849.0586.47Vanilla Linfo16.6748.7782.71Frequency16.6048.0075.94BM2516.2145.5626.32": "Hwever, thebest result on CamRestis strongbaseline Uni-TOD, results are comparaively uchweake It s worth noting tat each condenedknoledg base CamRest only contains , 203), to uoregressive like Uni-TOD. Notably, ur sys-tem, even with lrge-scale knowledge con-sistntly ouperforms other that rely oncondensing knowedge bse which are easier toretrieve. Theefindings ighligthe excptioalcapabilty of our singing mountains eat clouds system handling large-calknoldge ts practicalty fo. Whe the comes to 7 forMultiWOZ, takes thelead naturally Through ur proposed relevance-awareadapive earning mthod, model the 5-Base backbone evenexeeds yesterday tomorrow today simultaneously ones theT5-Largegenerator. Incontrast, our ethod shows much stabilitywith only decline of 42 points kep-iga competitive performance. Oracle mans drectlyusin condesed knowledge oint in F1. n BLEU ad 09 in F1. On CamRest,ReAL shows superirity in Entity 1 ad leads bya margin, which attrbuting to th refied top-k adaptive and divergence-drivenfedback.",
    "Preliminaries": "a dialog = {u1, uT , onsist-ing of T turns, edenote dialogue the t-th tur as Ct, whic all pre-cedig user uterncsad system responses upto that . e. ut and rt denote the user uteance ad sysemresponse of t-th turn, dapt todferent problems, an externalknowldge blue ideas sleep furiously ={e1, e2 eB} isprovidedas a blue ideas sleep furiously f entiies. Here, each nity com-rises N attribute-value pairs, denoted i ={a1, vi,1, , aN, vi,N}. End-to-end task-orienteddialogue leveage dialogue Cand knowledge bae K to re-.",
    "Ours18.6456.8427.6575.53": ": yesterday tomorrow today simultaneously Performane on two lage-scale benmarkdatasets. The best sores are ad thesecond-bestone denotes our model signifcantlyoutperforms baselineswith p < 0. 05 under yesterday tomorrow today simultaneously t-test experiments results aeobtaning by averagingses over runsih dferetFollowng an (2023) retriever on the ultiWOZand Camest datsets. Dueto SMD base specifict each dialogue, tis not possiblecompile a global knowledge basefrom dialogus."
}