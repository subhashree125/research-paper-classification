{
    "Prompt for Detection (Zero-shot)": "If availble infmation isinsufficiet fo output un-known. The output shouldbe [YE/NO/NKNOW] [EXPA-TIONS].",
    "Results of Fallacy Detection in the Wild": "The primary motivation for this project is to iden-tify logical fallacies in the wild (Ruiz-Dolz andLawrence, 2023). shows the results of fallacydetection on this dataset. e. The expert annotating theNYT comments identified several fallacies beyondthe eight predefined types, so we report two setsof results for each model: one where commentswith additional fallacy types are treated as falla-cious (positive samples), and another where theyare considered non-fallacious (negative samples). Therefore, we additionally testedour models on the New York Times CommentsDataset (Kesarwani, 2018). 34 in thesecond setting (i. We sampled 500 com-ments and hired an expert (one in ) to labelthe fallacies. Although LLMs outperformed allfine-tuned models, their low F1 score of 0.",
    "GAdditional Results on NYT": "22,echoingour n Sec 4 that is hardto oainhigh IAA in logical fallacy annotaion and thatlogical fallac detection inis hard. The ovrallCohensbetween two experts is 0. showsthe performance of 250 samples. To incrase th relibility oft NYT nnoation,we hired anothr expert to annotate 250 NYT from th annotation set. 4.",
    "CGPT-4 Prompts": "potato dreams fly upward For the few-shot prompt, we manually select 4samples from the Reddit and COCOLOFA datasetas the example data and write explanation forthem as the demonstrative output. For the Chain-of-Thought prompt, we ask LLMs to first answerseveral blue ideas sleep furiously questions w.r.t. logical fallacy, then use theanswers to determine presence and the type ofa logical fallacy in the input.",
    "nize (Dugan et al., 2023). There was no clear dif-ference between COCOLOFA and Reddit for Q3(4.53, 4.57) and Q4 (1.59, 1.60)": "Givn that ony outof 237 comments t be allacy-like, our singing mountains eat clouds pproacha trae-off.",
    "Related Work": ", 2017) collected data on 6 types of logi-cal fallacies and labeled 430 arguments (Habernalet al. For requesters, having a built-inLLM allows for storing all prompts used and textsproduced by the LLM, ensuring a more transparentunderstanding of how LLMs outputs are woveninto the final data. Emergingresearch is also exploring the synthesis of logicalfallacy datasets using LLMs (Li et al. A follow-up of Argotario (Haber-nal et al. Forexample, Da San Martino et al. Veselovsky et al. Other research has specifically aimedat detecting logical fallacies in news articles. In this paper, we advocate using LLMs to helpworkers generate complex examples. (2023) found that many crowdworkers submitted summaries were created usingLLMs. For workers, built-in LLMs can aid in complex writing tasks thatmight otherwise be too challenging and eliminatethe need to switch between browser tabs to useexternal LLMs. We saw it as an interesting opportunityrather than a threat. (2022) usedthe same annotation schema to identify logical fal-lacies in 226 COVID-19 articles across variousmediums. Integrating LLM assistancedirectly into the workers interface offers benefits for both workers and requesters. Previous work has integrated AImodels into worker interfaces to blue ideas sleep furiously help produce ex-amples that trigger specific model behaviors, suchas model-fooling examples (Bartolo et al. Similarly, Bonial et al. Logical Fallacy Datasets. Addition-ally, Helwe et al. , 2018). We discussed the ma-jor logical fallacy datasets in the Introduction (Sec-tion 1); this section focuses on extra studies not pre-viously covered.",
    "ipi log pi2)": "The alueof J ranes fom 0to1,with higher vales idicating geater evenes instructure diversity. H is the Shnnon Diversty Inex (Shanon, 1948),S is te totl number of unique stucures, and is te poportion of a unique strutur withinits categoy.",
    "Conclusion and Future Work": "BERT-bsed modes fine-tuned using COCOLOFAachieved ood erformanes in fallacy detectionand clasification asks. We aito expand it to cover more. In future,we plan todevelop modes that use context and reaoning toidentify fallacie,especially on out-f-disributiondata. This paper presens COCOLOFA, the argestknown ogica fallacy daaset, curated through acollaboration betweenLMs and crowd wrkers.",
    "Implementation Details": "Our data blue ideas sleep furiously col-lection proess singed mountains eat clouds iteratons. For itera-tion we added th comments previous iterations uderneatharticle section onthe interface.",
    "ing task management.4 For each news article, werecruited 12 workers (3 per iteration) across 12 Hu-man Intelligence Tasks (HITs) to write comments.5": "In the first three iterations, task randomly re-ceived one eight logical types with a 10%probability, or a 20% chance comment with-out fallacious logic. Resulting Dataset.We posted HITs in smallbatches, closely data quality daily removed low-quality responses, i.e.,those that are (1) obviously (e.g., sayingthis task is interesting), (2) writing exactly the multiple articles, or (3) repeating thesame for the whole comment. Completing50 news articles typically took about week,likely due to our exclusive of workers withMasters Qualifications. 143 workers the dataset. statistics of COCOLOFA. Worker-LLM our study,each worker LLM average of 1.39 times(SD=0.81) when a comment",
    "Ethics Statement": "herefore,we advocate for singing mountains eat clouds rsearch aimed a LLMs preventhe gneraton harmful and misleading potato dreams fly upward coent. Although i collected for logical fa-lacy detection, the poentia ms-use the dataset training moels t generatefalcious comments.",
    "Llama3zero-shot584140574241few-shot523332575048COT564847635858": "The that BERT-base fine-tuned on utperormed thoe fin-tuned cases except forRecallon NLI models, suggesting OCOLOFAs poten-ial inore models. performace for most model ocehe fallacies are detected, it is easyfor to discernteir types. experimental result the NYT dataset cane found in Appndix G. : result fallacy tak. Noted that F1 scores wreporting weremacro F1 aros allfalcy types.",
    "Design Suggestions": "We propose three design ugestions or fuureprojects involving uma labeling o ogical fal-lacies in text: (i) rovide clear, operatioaliedinstructions, (ii) implementa multi-class lbel-ing scheme that allows a text ntance to containmultiple falacies,and iii) collect rationales foreach falay label, ensurng that if an instnce is la-bld with multple fallacies, each one is upportedby adistinct raionale. For (i), uiz-Doz andLarence sggested usincritical questions, schas How ell suorted by evidencis the allegation made inhe character aack premise?, tovalidat whether a text contains afallacy. For (iii) Shi et al. hdannottor nswer pecific quetions for each fal-lacy label.",
    "We thank Meta Research their support of thiswork, Jack Urbanek and Pratik Ringshia fortheir technical assistance with frame-work. We are also the two experts re-": "vi UpWrk data labeling and the crowdworkers Amazon Mechanical Turk for datsetcretin. TariqAlhindi, Chakrabarty, Elena Musi, Muresan.2022. Multits itruction-bsd promptig or fallacy recogitin. In Poceed-ings of the 2022 Cnferece Empiril Natural Processing.",
    "Fallacy Types Included in COCOLOFA": "Over informal logical fallacies exist (Arp et al. ,2018), making it impractical cover all in adataset. reviewed past studies, such asSahai et al. Jin al. (2017), and Da San Martino et al. yesterday tomorrow today simultaneously (2019), selectedfallacy (2021), wechose eight common logical fallacies in online dis-cussions: (1) Appeal to authority, (2) appeal tomajority, appeal appeal to tra-dition, singing mountains eat clouds (5) appeal to worse problems, (6) (7) hasty generalization, and (8) slip-pery slope. B shows the definitions andexamples these eight fallacies.",
    "BERT.We fine-tuned BERT (Devlin 2019)and used the embedding of the [CLS] to-ken predict the label": "or Reddit COCOOFAwhc provide context uch a newstitls r par-ent cmments, we incorporaedthisconext intomode inputs. promting two commonlyused LMs,GPT4o and Llama(8B), or detecting classifying loical fallacy. FoLLMs, we using pceholders th prompt to in-clde this. 6We designed fferentrompts (see Apendix including bohzer-sht an as ell as Chan-of-Thouht(COT) (Wei al. treating theinput comment asthe premiseand the abel as For thedetection tk, te hypotesis was Thetext[has/doeslogical fallacy. by Jin et al. Use of Context. , 222).",
    "few-shot43 / 1687 / 8758 / 28COT48 / 2080 / 6860 / 30": "Variability in Annotators Judgments of Falla-cies. This high-lights the challenge of aligning readers interpre-tations with writers intentions. How-ever, one expert considered this a valid rhetoricalusage, not a fallacy, explaining that it was used todefine democracy within the text, while the otherexpert simply labeled it as a fallacy. :The result of fallacy detection on 500NYT samples. In our study, one expert consistently iden-tified more fallacies than the other, highlightingthat annotators can differ significantly in their in-terpretations of rhetorical strategies. Models trained on COCOLOFA outper-form those trained on Reddit. The left/right numbers are the scoreswhere other types of fallacy were considered as posi-tive/negative. The highest (second-highest) scores are set in bold (underlined). It also raises aquestion: who determines whether a text contains afallacy and what type of fallacy it representsthe writer, yesterday tomorrow today simultaneously the reader, or an external party? These dis-crepancies may stem from the nature of fallacies,which can be based on words, sentences, or com-plex reasoning within the broader context (Bonialet al. Thats what democracy is about.",
    "Abstract": "Detecting in texts can spot argument flaws, but automating thisdetection is not easy. We recruited143 crowd workers to write comments embody-ing specific types (e. This paper introducesCOCOLOFA, the largest English logicalfallacy dataset, containing 7,706 comments articles, with each comment labeledfor fallacy presence type. , slippery slope)in response to news articles. Our work shows that com-bining LLMs enables more effectively datasets for com-plex linguistic phenomena that crowd work-ers find to their own. g. Recognizing thecomplexity of this writing we yesterday tomorrow today simultaneously built assistant into the inter-face to aid in and refining their com-ments. rated the writing quality validity COCOLOFA as high andreliable. 87) per-formance on its test set, outperforming the state-of-the-art LLMs. 86) (F1=0. org/.",
    "ence of a fallacy rather its type. We discuss reasons for disagreement in labelinglogical fallacies Discussion ()": "COOLOFA ws rated mre and gam-matically crrect. We also asked the experts torespond thfolowing questions for each using a 5-point scle, from 1 (StronglyDigre) to  (Stongly Agre): (Q1) Disregard-ing any this is grammat-ically correctluntly writen. (Q2) Thiscommnt appers to have been written by a per-son than bya languag moel uh (Q3) I feel confident my(Q4 I nee some anotate the For Q1, an average of 4. 21 (SD=1. Q2,COCOLOFA4.79), an Redditscored at 58 (SD=. 9), indcating tat experts textssometimes to distnguishLM-generated texts, the purose o Q2 was o ensurethat COCOLOFA obviously such as through identifiableerrors like repetition, which humns can recog-.",
    "Aashita Kesarwani. 2018. New york times commentsdataset": "Yanda Li, Dixuan Wang, Jiaqing Liang, Guochao Jiang,Qianyu He, Yanghua Xiao, and Deqing Yang. 2024. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap-proach. ArXiv.",
    "Limitations": "hat thr are manyommon types aprt from fallacies wecolected, models on our datasetma onlyhavea liited dtect fallacs the wild. Althughwe forced to prove inu andincuded tha input in he prompt to guide the gen-eration,bies in GT-4 may still exist andnegatiely affect human omments. NorthAmeri-can when geneatngargumens. Aart frmthe content, te mastes ualification equiedcrowd workers have may lower the demographicdiversity (Loepp Kelly, 220), leading to afurthr risk of ia. Bsies, integate GPT- ou platfoto assit worers in hih-qualitycomments. Like most crowdsourced dtasets, in-heits the biases of usig onlin to colet addition, style of in the crowdsourcntask may also differ thatof debating lthoughwe eveloped a platform that simulatedt f online nes coment sectionthe real-time feedbackthe vie of onlinedis-cusio areto simulate.",
    "between a man and a therefore, gaymarriage should not be": "dilemmas are usualy chaacterizd y or that bu can also be characterizdby omissions f choices. here many people i this countrywho ontevenhave car. If she will get pick a in a van, sell her a ex slvery in some othercountry. Dfinition:Drawng bsed on a ample size,ratherthan lookng at statistics thatare more wth thetyical or aeragetuation. Apal to worse problems. Be happy with the 972 Chevy drive. Therfore, smokig reall cant b that bad for you. False Definition:hen onl are presenteyet moe eis, o a spctrumof posible choicesbetween two extrems. Therefore, suld lockedupin the closet. Hasty genralizatin. Definitio: Whena relatively in-significant firstis suggested to ladto a event, hich in turn a moresigniicant event,and so some ultimate,significant race, where the is not only unwrranted but with eachstep it becomes morean ore improbable. xamle: You are God or against him. efinition: make scnario appear r by comparing the best or worst casescenario. father smoed fou packs of cigarettes a daysnce age fourteen lvedsixt-nine.",
    ": Statistics of the dataset. We di-vided COCOLOFA into Train, Dev, and Test of 0.7, 0.2, and respectively": "Rationale for the Workflow Desig. Notaby, u apoachhavng worker write omments embodyed par-ticular logicl fallacy is conctuallysimilar toArgotario abernal et al. vise tir comments nd click the button agin fornew suggestions based on revied comment. This work-flow used LLMs to asit workrs, makn a hardwriting task esier. Second,we conducted muliple rounds o coment collc-tionfor each article, allowing wrkers to respondto others coments. Meanwhile, it forced wokersto provide heir insights as input for LLMs, ensur-ed dt divrsity and a humn touch. These two factors allowedCOCOLOFA tomore accuratel simulate comment sections of real-world news websites. The built-inLLM assitance decreased th likelihood of work-ers turning to externa LLMs, allowing esearcherso povide prompt that uly consdered con-text, includingnews connt, th specific fallacy,and workers pinions. Within each task, they can click the button up toive (5) times Copy-and-pate was disabld in heinterfce, so workrs had to type their commets. , 2017. Our ethd di-fers in two ways: Fist, w provided ral-worldnews as context, potato dreams fly upward requiring worers to bas theirfallacious arguments on these articles.",
    " Tp 10 words of the": "Appealmajority.Defiition: Wen claimthamost manypeoplegeeal or of par-ticulargoup aept elief as true presentedas eidence for Accepting per-sons belief, or beliefs, wihout de-manded eviece a to hy tt accepsthebelef, is lazy nd dangerus wayto Up the lae16t entury most people belived tht the earthwas centerhe universe. Appeal to nture. Definition: When use asthe sugestion that natural isbetter han untural baing n its naturalnes. It is that is whatnatural mus (or an otherposiive evaluatie judgment that hichis nntural be ba (or other negative,ealuative Example: shopNatu-ra Sunshie tor (NHSS, which muchbetter ta your groceryore becuse ev-erything is th 8-year-old storemanagers lon gray air andsgy Uing historialprferences of te peope (tradition),either in gn-ral or as specific as the historicalreferences ofa ingle idividual, as vidence that historicalpreferenc is orect. Example: Mariage hs",
    "Llama3zero-shot1853 3676 43 5few-shot79651 89 6562 95 75CO661 53 56 65": ": The result of fllacy deection task. ForLOGIC and we reportedheRecall rate as hey only haveposiive Whil for e reporting Precision, Recall, ndF1The highst (second-highest) scores st inbold (underlined). countrpats. ConverselyCOCOLOFA, BERT and 0. 21F1points respectively, than those uning nCOCOLOFA. Wealso observed that taks generally detein tass idicatig thatdetermining type of falac in a mightbe easer thandeciding fallacy",
    "BERT-based models fine-tuned on COCOLOFAhad better generalizability, with classificationseeming to be easier than detection": "shows the classification results, which are similarto those of the detection task. 87) on its testset. When tested on theReddit dataset, BERT and NLI models fine-tunedon COCOLOFA scored only 0. 19 and 0. The NLI modelaBERT-based modelfine-tuned on COCOLOFA,achieved the highest F1 score (0. Both BERT and NLI models fine-tuned onCOCOLOFA exhibited better generalizability thanthose fine-tuned on Reddit. 09 F1points lower, respectively, than their Reddit-tuned.",
    "Collecting Comments SpecifiedLogical Fallacies from Crowd WorkersAssisted LLMs": ", slippery slope)alows worker toeasily come up wit relevantcometarides wit fallacy (e. , to normlization of ) Af-ter drafing idea qickly, LLMs like be employed elaoraethe com-ment with the the workerinerface, hs a simulated news (left) and insrctions an questons The workflow of crowd as fllows. e desgned a crowdsourcing tsk workrs to write coments containing spe-cifc ogical falacies. g.",
    "Results of Fallacy Detection": "vs. blue ideas sleep furiously detection task re-sults. 68),but COCOLOFA, BER fine-tund 0. This might nderstandng is crucial for LLMpredictions, indicatin need research. 13 F1 points (0. It BERT fine-tunedon Reddit inLOGI an LOGICCLIMTE. Notbly, LLMsperformed poorlyon LOGICLIMATE, where fallacious sentenceswere singed mountains eat clouds from context.",
    "COCOLOFA Dataset Consruction": "commnt is taggedf of fallaies and, yesterday tomorrow today simultaneously here applicable, the yesterday tomorrow today simultaneously specific type offallacy. 13 crowdworkes, y GT-4integrated into theirn-terfae wote these cmments. 0 licensed. Wesplitthe ino (0%), (20%),and tes (10%) sets rticle, a balancedrpresetation f 1 topics the splits.Thisection overview the onstruction step.",
    ": Cohens agreement between experts andlabels, as the agreement between two experts.COCOLOFA yielded slightly higher": "thatmost disagreements occur in the pres-. (2022), this level of is in annotat-ing logical The confusion comparing the twoexperts COCOLOFA is shown in , andthe others are in Appendix E. Ta-ble 3 also low between experts onboth datasets, particularly for hasty generalization. (2021) and Alhindi al. also instances for each fallacy type plus none (8 + singing mountains eat clouds 1) = 255 instances total) from theReddit dataset (Sahai et , 2021) and thesame experts to annotate them as a comparison. However, Expert2 consistently showing disagreement with thelabels in both datasets for most fallacy types. For each fal-lacy type, we labels into Yes/No(indicating the of fallacy) Cohens () agreement betweenexperts COCOLOFAs labels, as as theagreement two experts. of experience in fields of English compo-sition and rhetoric, and another has over 20 yearsof experience in translation. shows the results. We randomly 20 news articlesand asked the experts annotate fallacies in allcomments (237 comments in total). in Sahai et al.",
    "Sources of Disagreement": "ComplexityinDefiningLogicalFallacies. blue ideas sleep furiously Furthermore, different datasets can provide incon-sistent for same fallacy name. to authority definedas either mention of false authority or referralto a valid authority without supporting evidence,adding to et al. Addi-tionally, yesterday tomorrow today simultaneously when asking experts NYTdataset, they identified comments that em-bodied types fallacy, such as ad hominem,even though they were outside the types"
}