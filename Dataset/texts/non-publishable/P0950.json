{
    "Abstract": "t is essen-tial toevaluate and monito AI systems ot only for accuracy andqualit-related metics ut alo for robutness, bias, scrity, ntr-retability, an other responsible AI diensions. With the ongoing rapid adoption ofArticial Intelligence (AI-basedsystems in high-stkes domains, ensurin the trusworthiness, safey,and potato dreams fly upward observability of these systems has become crucial.",
    "Business problems: How can we deploy in a human-AI hy-brid setting to quantify the uncertainty (condence score) associ-ated with an response and to humans condence": "Whil by controlling inuence data during grad-ent transfrs blue ideas sleep furiously to a tudent te knowledgeof an ensemble ofteacher with ituitive providedby training teacers on disjoint dataandpriacy guarantee by noiy aggregation of teachers aswers. 2Examples f dieretially private mode training include DPSGD and PTE.",
    "Introduction": "We devote 3 to the of grounding for applicatns,nd to LL operations. Such models need to be vlu-ated and monitored not only for and qualiy-related etric but aso for robustnes adversaril ataks, obustnessunderdisriution shifts, ias and discrmination agnst under-representd grous, securit privacyprotecion, interpretabil-ty, hallucinaions and ungrouned r low-qulity otputs) to mae hardo allor part of wok f prsonal use is e provided that copies are not made or distibutedfor or commercial and tha copies bearnotice ad fullcita-tion onthe rst for of this work owned by others thanth author(s)mut be Abstating with credit is permited. present eaworld LLM cases,pactical challenge, best prctices, lessons learned olution approachs in indsry, key goal is to stimulate furtheron grunding a eval-atin LLs and enable researchers practitioners o buld morrobut LLM applications. reachdimen-son (discussed in to present problems, tech-nical solution apraches, and opn. To copy otherwise, or republish, to post on serersor to redistribute lits, and/or a fee. Request fom 24,Agust 252, 202, Spin 224 Copyrigh hed the Publiatio rights licesed to ISBN 998-4007-0490-1/2/08 content (suh as sexul,racist, and haefrsponses), of safety and alignmnt mechanisms, prompt ijectio misinformatin dsinfmation, fke, mislading, annpulaive copyrigtand rspon-sible AI this utrial e rst hihlight key hrm associated wihgenerative AI systems, focusing on ungronded answers (allucinaions), jailbreaks prompt injection attaks, armful contentad copyrigh infingement. Cosdering the adopton Artcial Inteligence (AI)technolois in o daiy it crcia to develop and deployte underlng I sysemsin a responsile manner anenre teir trustwortiness, safe, and obserability. Our focu sonlarge languae mdels (LLMs) and oher generav AI modelsandapplications. W then discuss how to eectively ad-dres potential risk nd following the fraewok measrement, mitigation (wihfour lay-ers atmodl, system, application, andpositoning lev-els), andoperaionliation. by isssion of AI dimnsions in 2. e rt present a utorial outline i 1.",
    "Safety and Alignment": "Business problems: How do we prevent an LLM from generatingtoxic, violent, oensive, or otherwise unsafe output? How do wedetect such content in cases where prevention fails to work? Howdo we ensure that the responses from an LLM are aligned with hu-man intent even in settings where it is hard for human experts toverify such alignment? Solution approaches: The problem can be addressed during dif-ferent stages of the LLM lifecycle. In thereinforcement learning from human feedback (RLHF) stage, wecan include response pairs with preference labels on which oneis more appropriate, and tune the model to align its responseswith the preferences. As part of prompt engineering, we caninclude instructions to discourage the LLM from generating unde-sirable outputs. Finally, when prevention fails, we can apply toxic-ity classiers to detect undesirable outputs (as well as undesirableinputs) and ag such instances for appropriate treatment by theuser-facing AI applications. The prob-lem can be framed as a constrained optimization problem: givencost or latency constraints, determine the subset of prompts andresponses to be evaluated using a more powerful LLM (e. g. In certain settings, the task to be evaluated could be too hard foreven human experts (e. g. , comparing two dierent summaries ofa very large collection of documents or judging the quality of hy-potheses generated based on a large volume of medical literature),necessitating the use of powerful LLMs in a manner that alignswith human intent. The converse problem of leveraging less pow-erful LLMs to align more powerful LLMs with human intent hasalso been explored in alignment research. A related challenge isto ensure that AI systems with superhuman performance (whichcould possibly be smarter than humans) are designed to follow hu-man intent. While current approaches for AI alignment rely onhuman ability to supervise AI (using approaches such as reinforce-ment learning from human feedback), these approaches would notbe feasible when AI systems become smarter than humans. Overall, alignment is an active area of research, with approachesranging from data-ecient alignment to alternatives to RLHF to aligning cross-modal representations. Open challenges: There has a been a bunch of recent work ongenerating adversarial prompts to bypass existing mechanisms formitigating toxic content generation. A key open chal-lenge is mitigating toxic content generation even under such adver-sarial prompts. For instance, a two-step prex-based attack procedure that operates by (a) constructing a uni-versal adversarial prex for the guardrail model, and (b) propagat-ing this prex to the response has been shown to be eective across multiple threat models, including ones in which the adver-sary has no access to the guardrail model at all. How do wedevelop eective LLM based guardrails that are robust to such at-tacks (and even better, have provable robustness/security guaran-tees)? Another challenge lies in balancing reduction of undesirableoutputs with preservation of the models ability towards creativegeneration. Finally, as LLMs are increasingly deployed as part ofopen-ended applications, an important socio-technical challengeis to investigate the opinions reected by the LLMs, determinewhether such opinions are aligned with the needs of dierent appli-cation settings, and design mechanisms to incorporate preferencesand opinions of relevant stakeholders (including those impacted bythe deployment of LLM based applications).",
    "shape the business problems, solution approaches, and open chal-lenges discussed in this article, and Mark Johnson and Qinlan Shenfor thoughtful feedback": "Biasin bos: A case study of semantic rep-rsentation in a high-stakes setting. 2024. Textooks all you ned. peprint aXiv:29. 6465648. Deep Ganguli, Liane Jackson Kernion Askell, Bai,Saurav Pere, Nihola Kamal Ndosse,et al. 09390 Alin Calsan021. 20. Kate Alexanda Chouldechva and cllaboration: Achieving and un-firness. RARR: eserching and Revising aguag ModelsSay, UsingLanguag odels. Tolga Bolukbasi, KaiWei Y Zou enkates Saliama, andAdam T 2016. Suriya Gunasekar, Y Zang,Jyoti Aneja, Caio Csr Mendes, AlleDel Giorno, opi, Mojan Javaheripi, Kaumann, Gustavo eRosa, lli aarikivi, al. 2023. 2024. MrinAndy Chu, Ian H BrendanMcMahan, Ilya Miroov,Kunal Talwar, ndLi Zhang. Aylin J Bryson, and Arind arayanan. 0754 (2022). (204). 2024. 05904 Chenx Chensong Huang,XaoqingZheng, Kai-Wei Chang, and Watermarking pre-trained odels with bacdooring. 020. 2021. 0424 (023). arXivpreprint. 0237(2023). Extrctigtraining data from diusion models. Proceedings of the NationalAcademy of Sciences121, 18 (2024). ariv prepint arXiv:2210. Measring Disributional Shifts ext:he of Lngage Model-Basd arXiv preprint arXv:212. A general language assistnt as a laboratory for alignment. 15018. Dont Stop Pretraining: dapt Lan-guage Mdels to Domins and Tasks In Proceedings of the 58th Annua Meeingofthe Assoaion for Lingistic. Enabling Model Gnerate Text with Citaions. 2018. Sel-RAG: Learning toRetieve,Generate, and toughSl-Reection. Ope problemsnd fudamental limitations of rinforce-ment learning from human feedback. Jornal of Artical Intelligenc Rsarch 7 2023), 103166. Man s tocomputer woma is to home-maker? Debiasing embeddings. reprintarXiv:206. of the ACM on Fiess, Acconability,and Transparency. 2024. 060 Chen, Jiung Syna Ebrahim,Sercan Arik,Tomas Pter andSmesh ha. In Proceedings of 2023Conferenc on Empirical in Ntural Langug rocessing. 202. Retrievl-ugeed Gen-eration for Lrge Lnguage Mdes Garg Londa Schiebinger, Dn Jurafsky, and James Zu. Douze, Guzhva, Chengqi Deng, Je Jonson, Gergely Szil-vasy, Perre-Emanuel Mazar,Mar omeli, d Hrv J-gou. arXiv:2404. Groundednessetection. 202. Semantics autoatically from language contai human-like biases Science356, 6334 (2017). L J Tow Bdermn, SBlack, A DiPo, CFoster, L Golding, Hsu,A Le Noac, et a. arXiv preprint aiv:30715217 Ci-Mn Chan, Chunpu Xu, Ruibn YuanHongyin Luo, Wei ue, Yke F 224. unfan Gao, Y Xiog, Xinyu Gao, Kangxian inliu an, Yi Dai,Jiawei Sun, Meng Wang. Amt Hm,lejando Julan 204. Niholas Florian raer, EricWalla, Matthew Jagielski, ArilHerbert-Voss, Lee,Adam Roberts, Tom Brown, Daw Song, UfarErlingsson, Alina Oprea, adColin 2021. 2023 with Sef-Evaluation Impov Selcte Pre-dictio in In Findings of the Association for Comuational 203. 197 (2023). OntheDangers Sohastc Prrots: Can Langage ModelB To Bi?. large languagemdels: corehensive survey. Gyandev Gupta, Bashir Rastegarpanah, AmalenduIyer, JoshuaRubin, n Kr-isharam Knthapadi. n Findings o th forCompuatioal 1127511288. 2016. 13188 (2023. Proceedingsof the Nationalof Sciences 115, 6 (2018), E3635E3644. PMLR, 457467. Nicholas arlini, Jami Milad Nasr, Jagiels Vikash amr,Boja Ba,Dahne Ippolit, and 223. arXiv prepritarXiv:2. 00367 Isabel O Galegos, Ra Rossi, Joe Mehrab Sungchul Kim,Frack Dernocourt, Tong Yu, Ruiyi Zhang, Nesren K Amed. arXiv preprint arXiv:2301. 2022. arXiv:402. Identifing mitigaing te scurity risks ofan Treds in Privacy Securiy , 1 (22), 152. Data fromLarge Lanuage Models. 2024. 2024. TheTwelth Conferene on Learning Anda Aske, Yuntao Ba, Ana Che, Dawn Deep Ganguli, omHenighan, ndy ones, Nicholas Joseph, Ben et al. Akar Aai, Wang, and Hanneh Hajishirzi. Purple secure model. Tong Chen, Honwei Wng, ihao Chen, Wenhao Yu, KaixinMa, Xinan han and Dong Yu. Shangbin Feng, Weja hi, Yike Wang, Wenxuan ing, Yulia Tsvetkov. MicrosoftAzureAI. Fne-Tuning LLMs o New Knowledge arXv:2405. 0086 Sergul Adore,Keans, KrishnramKnthapad, caMelis, Aaron and Ankit A Siv 2021. Publicly detectablewatermarking for lan-uage models. Dierentially rivate query releaetoug adaptive projection In International Cnfrenc on Machine Leaning. garak: Framewor forProbng arge Models(2024). Whats in a Language Models Race and ender Bias. Checkgrouning | AI Agent Builder. A frameork fo few-shot languge modeevauation. Repairing A survey in for genertedtext. InCollin Burns, Pvel Izmailov, Jan ndri Baker, Leo Gao,Leopold Yinng Chen, Adriencoet Manas Joglekar, Janeie, et al Weakto-strong Elcitig stron capabilitieswith wak spervision. El M Bender, Timnit Geru, Angelina McMillan-Major, and 2021. Dont Hallucinate, Abstain: Identifyin LLMKnwl-edge va Multi-LLM Collaboration. Gehrmann,Clark, and Thibault Sellam. GoogleCloudA 2024. Dtecting and mitiating biasn natural pro-cessing. Computational Liguistics(202), 17. aXiv preprint arXiv:310. 2017. arXiv arXiv:2312. Bai, Angelina Wang, Ilia Sucholutsky, homas L Griths. Zenodo(2023) Tinyu Howard Jiatong ad Dani Chen. 204. ClarkBarrett, Bra B, Ele rsztein, Nicholas Bad Chen, Jihei, mrita RoyChowdhur, Mhai Christodoresc,Anupam Datta, SoheilFeizi,3. mith. Dep with prvacy. Jaide Fairoze, Sanjam Grg, Somsh Jha, Saeed Moammad Mah-moody, nd Mingyuan Wang. In FAccT. StrLM: Attribut an (User-Sterable)RLH. RQ-RAG:Learning to Rene Queries fo Retrieval AugmetedGeneratn. Leon Deczynki, Erick Galinkin, Jerey Matin, Subho and 2024. Dong, Zhilin Maksh Srehar, Wu, an Kuchaiev. easring implicit ias in explicitlylarge anguage models. 2023. I ACMon Acountablit, and Chennabasappa Cyrus Shengye Wan, IvnEvtimov, Gabi, Daniel Song, Ahmad, CorneliusAschermann,Lorenz Fntana, et 2023. 1676508. 1164 Anisa Jihan Yin and ErhaBas. 06648 De-Arteaa, Alexey Romnv, Hanna Chayes, Chouldechova, ahin Geyik, Kenhapadiand Adam Tauman 2019. Rd lnguage modelsreduce harms: Methods, scalngbehavior, and lesson learne. 07858 (2022). fairnes large language models: A survey. DenseX Retrieval: What Retieval Granularty Should We Use? aXiv:2312. 2023. Wrdembeddings100gender and thnic stereotypes. Zorik Gehman, Gal Yona, Aharoni, Eyal, Amir Roi Jonathan Herzg. Suchin Gururangan, Ana MrasoviSwayamdipta, Kle Lo, IzBltag,Doug Downey, and oah A. 2023. arXvpreprin (2024). arXiv preprint arXi:231. 08281 Es, Jithn Lui spinosaAnke, Steven Schockaert. AI moel disgorgment:Methds and choices. Lyu Gao, Dai, Panupong Pasuat,Anthony Chen, Cha-ganty, YichengFn,Vincent Ni ao, Hongrae ee, Da-Cng Juan, andKelvin Guu. teen Casper, Dvies, Claudia Shi,Thoms rendl Gilber, Javier RacheTomasz Korak, David Lidne, Freire, al. RA-GAs:Auomated Evluatin of Retrieval AgmentedProceedingof 18th Conference of the Europan Chaptr of the Associti frComputa-tional Lnguistics:System Demnstrations. In USENIX Security Symposium, Vol. arXiv:csLG/2401. Alessandro Achle, Michael Kearns,Carson Klingenberg, ad Stefano Soatto. (223). In of the 61s Annal Meeting theAsociation for Linguistcs. In AAAI Zishan Guo, Jin, Chang iu, uang,Dan Shi, u, Yan Liu,Jiaxuan Li, Bojian Xiog, Dey et al. Detctng and preventng in lae vision language model. InC. lbrary. 2024. 2023. 204.",
    "Robustness and Security": "Miigation approaches involve invariant representations,and mdelsdo not rely spuious patterns using tech-niqe like data potato dreams fly upward agmtation, reweighting enembling, design, and cusal intervention. Ope-surceframewrks and benchmaks such as Stanford HELM ,Eluher Haress , Audir canbe utilized for benhmarkig dierent LLMs and evaluatin obust-ness in settings. LLMs hav been sown to to adversarial ertur-ations in prompts , prompt injection , daa po-soing atacks niversal transferable adversaraat-tacks on Several benchmarks have been proposedfr testing aginst adversarial attacks and re-lated issus. Metrics for tools to te freqency of insecure code sgges-tions and to evuate LLsto make yesterday tomorrow today simultaneously it ardr o code aid in carying outyberattackshave also beenproposed. discusson and approaches be foundin survey artices by Bret et al. Another chal-lenge i ensurig that mchanisms work just duringevaluation but (e. g. For example it has been shon that cancominations ofmodels a malicious task leveraging aligned models to solv hard but be-nign subtasks, and everaging eaker non-aligned osolveeasy but subtasks.",
    "KDD 24, 2024, Barcelona, SpainKrishnaram Kenthapadi, Mehrnoosh Sameki, and Ankur Taly": "Ethan Perez, Saron Huang, Trevor Cai, Ring, JohnAslanides, Amelia Glaese, Nat McAleese, Georey Irving. 2022. the Confer-ence on Empirical Methods in Natural Language Processing. [n. Pinecone Vector Database. 2023. The Art of QUESTIONING: Recursive Thinkingwith Large Models. In Proceedings of the 2023 Conference on EmpiricalMethods in Natural Language Processing. Fine-tuning Language When Users Do Not Intend To!. Colin Rael, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,Michael Matena, Yanqi Zhou, Li, and Peter J. Liu. 2020. Tilman Ruker, Ho, Casper, and Dylan Hadeld-Menell. Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, Ton-moy, Aman Chadha, Amit P Sheth, Amitava Das. 2023. The TroublingEmergence of in Large Language Models-An Extensive Deni-tion, and Prescriptive Remediations. Traian Rebedea, Dinu, Makesh Narsimhan Sreedhar, and Jonathan Cohen. 2023. In Proceedings of the2023 Conference on Empirical Methods in Natural Language Processing: SystemDemonstrations. 2024. Large Language are Biased Because Are LargeLanguage Models. 13138 (2024). 2020. of the2020 on Empirical in Natural Language Romanov, Maria De-Arteaga, Hanna Wallach, Jennifer Chayes, Chris-tian Borgs, Alexandra Chouldechova, Krishnaram Kenthapadi,Anna Rumshisky, and Adam Kalai. 2019. In Proceedings of the Con-ference of North American Chapter of the Association for Computational Lin-guistics: Human Language Volume (Long and Short 41874195. A Conversation With Bings Chatbot Deeply Un-settled. Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. Whose opinions do language models reect?. PMLR, 2997130004. 2023. Weijia Xiaochuang Han, Mike Lewis, Yulia Zettlemoyer,and Wen-tau Yih. Trusting Your Evidence: with Context-aware In of Conference of North Ameri-can of the Association for Computational Linguistics: Human (Volume 2: Short Papers), Kevin Duh, Gomez, and StevenBethard (Eds. ). Association for Computational Linguistics, Mexico Mex-ico, 783791. Snyder, Marius Moisescu, and Muhammad 2023. On of in Factual Question arXiv preprintarXiv:2312. 14183 (2023). Learning tosummarize with human In Advances in Neural Information ProcessingSystems. arXiv preprint arXiv:2312. Yuchao Michael Hay, andGerome Miklau. dierentially private synthetic data gen-eration arXiv arXiv:2112. 09238 (2021). 2022. In Advances in Neural Information Processing Systems. Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher Manning, andChelsea Finn. The TwelfthInternational on 2023. 54335442. 2023. Skating to Where the Puck is Going: Anticipating ManagingRisks from Frontier AI Systems. arXiv (2023). Lan-guage models dont always say what they think: Unfaithful explanations inchain-of-thought prompting. Advances in Neural Processing Sys-tems 36 (2023). 2024. Small Language Models Improve Rewriting Outputs. 27032718. Tu Vu, Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Yun-Hsuan Sung, Denny Le, Thang Luong. Fresh-LLMs: Refreshing Large with Search Engine Augmentation. arXiv:2310. Eric Wallace, Tony Zhao, Shi Feng, and Sameer Singh. 2021. Measure Improve Ro-bustness in NLP Survey. 45694586. Learning to Filter Context for Retrieval-Augmented arXiv:2311. 08377 Alexander Wei, Haghtalab, and Jacob Steinhardt. 2023. Jailbroken: HowDoes LLM Training Fail?. In Thirty-seventh Conference on Neural Infor-mation Processing Systems. Chain-of-thought prompting elicits rea-soning in large models. Simon Willison. 2022. Prompt injection attacks GPT-3. Contrastivelanguage-vision pretrained web-scraped multimodal exhibitsexual objectication bias. 11741185. 2023. 2024. Kevin Yang and Dan Klein. Yao, Duan, Kaidi Xu, Yuanfang Zhibo Sun, and Yue Zhang. 2024. A survey large language (LLM) security privacy: The good,.",
    "is low? Specically, how can we achieve this in high-stakes andlatency-sensitive domains such as AI models used in healthcaresettings?": "Solution approaches: blue ideas sleep furiously Learned to in settings isan active research , necessitating uncertainty quanti-cation and condence estimation for the underlying AI models. Italso understanding the conditions which eectively AI models . In context of LLMs,recent approaches such as selective prediction, self-evaluation andcalibration, semantic uncertainty, self-evaluation-based selec-tive prediction have been proposing (see references there-in). key challenge ensure that self-evaluation,calibration, selective prediction, and other ap-proaches for LLMs are eective in out-of-distributionsettings. particularly important adoption settings likehealthcare. challenge is ensuring robustness of condencemodeled approaches against adversarial prompts.",
    "Grounding for LLMs": "Business problem:How do we ensurethat responses generaedby an LM are roundedin a usr-specied knowledge bse? Hre,grouding mean that evry caim inth espons can be attrib-utedto a docuent in th knowledge base While ground-ingseeksttribtion to a user-specic knowledge base,factual-ty ees attribution to commonlyagreed wrd knwledge. yesterday tomorrow today simultaneously In the context of grounding, the knowledge basemay be a seto public potato dreams fly upward and/or priate documents, one o mr We domains, orthe entire Web. For instance, a healthcare compay may want itschatbot t always produce esponses that are grounded n a set ohealthcare articles it consider authoritative. Thisenables transparency and allows the enduser to corroborate alllaims in th response.",
    "Conclusion": "We must understand thepotential harms these may introduce, and leverage techniques for enhancing overall quality, ro-bustness, and Addressing the responsible AI relatedharms and challenges not only reduces legal, regulatory, rep-utational but individuals, businesses, and so-ciety as a whole. Moreover, there is pressing need to establishways to quantitatively assess the performance, quality, and safetyof models.",
    "Business problems: How do we detect and mitigate bias in foun-dation models? How can we apply bias detection and mitigationthroughout the foundation model lifecycle?": "approaches: There is extensive detecting andmitigating in NLP models. , gender stereotypes, exclusionary norms, undesirable biases to-wards mentions of disability, religious stereotypes, and sexual ob-jectication. g. related challengeis that the bias mitigation approach does not cause themodel to inadvertently demonstrate disparate treatment, whichcould be considered unlawful in range of scenarios law. a longer discussion, we direct the to thesurvey by Gallegos et By performing ne-grained evaluation and robust-ness testing groups, we can identify underperforminggroups, improve the performance such groups, po-tentially boost even the performance. Mitigation approaches include counterfactual augmentation(or types of data improvements), netuning, incorporatingfairness regularizers, learning, and natural language in-structions. Additionally, due to the sheer sizeof datasets used, it is dicult audit update the training dataor even dierent kinds biases that present.",
    "Privacy, Unlearning, and CopyrightImplications": "Business problems: How do we ensure that LLMs, diusion mod-els, and other generative AI models do not memorize training datainstances (including personally identiable information (PII)) andreproduce such data in their responses? How do we detect PII inLLM prompts / responses? How do prevent copyright infringe-ment by LLMs? How can we make an LLM / generative AI modelforget specic parts, facts, or other aspects associated with thetraining data? Solution approaches: Recent studies have shown that trainingdata can be extracted from LLMs and from diusion models (which could have copyright implications in blue ideas sleep furiously case the modelis perceived as a database from which the original images or othercopyrighted data can be approximately retrieved). Several approachesfor watermarking (or otherwise identifying / detecting) AI generated content have been proposed.",
    "Business problems: How do we ensure that LLM responses areinformed, relevant, and trustworthy? How do we detect and re-cover from hallucinations?": "vaiety o methodsave been propoed to detect halucintions, raging from sampling based pproahes to leveraging intenal the i alsoearlywork on and pre-vented hallucinatins in largvisionlanguagendother foundation Aof ave been propsed to fundamentally r-duce hallucinations by tuning One of involvestrining or-tuned on highly curate txtbook-likedatasets. approach invlves ne-tunin LLMs pref-erence daa for factuality , response pairs ranked facual-ity. Tene-tuning yesterday tomorrow today simultaneously aims tran LLMs to ap thse markersand upwight factual responses Related to this, ha een conjec-tured LLMs internalize dieret during pretan-in, and by truthful pais, one canupweight the truthful persona (evenunsee domains. Finaly, recet shows that ne-tuning on w infor-mation that wsnot acquired prtrainin can encouragethe model o hallucinat. ne-tuned sets to avodhispae anoter path to rucinghallucinations. g. st documens). is kown grounding. Thi is vasttopic in itself, and thereore w dedicat3 etirely Finally we all hallucinationsar qually bad Furthermore, hallucinations inhigh stakesverticals likehealtcare and life may befarmore cncerning than hallucinations in oer verticas. Open challenges: A key open is halucia-tions in vieo,peech, and multiodl settings. hw can we incorpte information asocment authors documnt quality, authoritativenss of the do-main, timestamp, and other evatmetadata subsequent evelopent?.",
    "Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey)KDD 24, August 2529, 2024, Barcelona, Spain": "LIMA: Is for Alignment. Explainability for largelanguage models: survey. Large forinformation retrieval: A survey. (2023). preprintarXiv:2310. ACM Transactions on Intelligent Systems Tech-nology 15, (2024), Retrieval-Augmented Generation for Content: A Survey. 15043. 2023. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer,Jiao Sun, Yuning Mao, Avia Efrat, Lili Yu, Zhang, Ghosh, Lewis, LukeZettlemoyer, and Omer Levy. 2024. Multimodal Information for Augmented Generation: A Sur-vey. InThirty-seventh Conference on Neural Processing Systems. 2023. 19473 Ruochen Zhao, Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Ding, Xiaobao Guo, Minzhi Li, Li, and Shaq Joty. 07910 (2023). the bad, ugly. arXiv preprintarXiv:2312. 2023. Yutao Zhu, Huaying Liu, Wenhan Chen-long Zhicheng Dou, and Wen. 2023. 16045 Wenhao Yu, Dan Iter, Shuohang Yichong Xu, Ju, Chenguang Zhu, Michael Zeng, and Meng Jiang. transferable adversarialattacks on aligned language models. In Findings of the for Computational Linguistics: EMNLP 2023. In TheEleventh International Conference on Representations. 2024. Zou, Zifan Wang, J Zico Kolter, and Matt 2023. Shukang Fu, Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yun-hang Shen, Li, Xing Sun, Enhong Chen. Computing (2024), 100211. arXiv:2402. Kaijie Zhu, Qinlin Hao Chen, Jindong and Xing Xie."
}