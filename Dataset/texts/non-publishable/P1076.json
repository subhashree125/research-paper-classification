{
    "INTRODUCTION": "With the recent exponential growth in the number of users anditems, collaborative filtering models encounter thelong-standing cold-start problem , stemming from the in-herent sparsity of user-item interaction data. In other words, forusers/items with few interactions, it becomes challenging to con-struct collaborative knowledge with other similar users/items, lead-ing to suboptimal recommendation performance, especially in thecold-start scenarios. , userdemographics, item titles, descriptions, or images) to enhance rec-ommendation performance under cold-start scenarios. g. g. , item texts or images), thereby replacing the item em-beddings typically used in collaborative filtering recommendationmodels. Despite the effectiveness of modality-aware recommender sys-tems in cold scenarios, the recent emergence of Large LanguageModels (LLMs), known for their rich pre-trained knowledge andadvanced language understanding capabilities, has attracted signif-icant interest in the recommendation domain to effectively extract 1An item is categorized as warm if it falls within top 35% of interactions, and if itfalls within the bottom 35%, it is classified as a cold item. 2After training each model using all available data in the trained set, we separatelyevaluate on cold and warm items in the test set.",
    "Chih-Chao Ma. 2008. A guide singular value decomposition Computer (Long Beach, CA) 2008 (2008), 114": "4352. 2015. Image-based recommendations on styles and substitutes.",
    "Recommender System, Large Language Models, Collaborative Fil-tering": "ACM NewYo NY, 12. ACM FormatSein Kim, Hongseok Kang, Seungyoon Cho, Kim Minchul Yng,andPark.",
    "(4)A-LLMRec with random joint embedding0.12000.47290.54270.0776": "blue ideas sleep furiously as shown in. In section, verify the benefit of in-jecting them into prompt (rows (2-4) in ). When we replace the joint embedding with a ran-domly initialized (row (4)), which means A-LLMRec item collaborative knowledge,we observe degradation across all datasets. We have thefollowing datasets, 1) the singing mountains eat clouds of the user representation (row (2)) or the joint embedding (row(3)) the prompt led to a in Moreover, as also capturethe information about items, exclusion is particularlydetrimental.",
    "Lstage1 Lmatcing + item-recon + Ltext-recon +": "However,onsidering all items in the seuence futher enhances thero-mendation performance, hih wllbeshown i .4.2. Having trained he au-toencoder basing o Eqution , weconsider e = (E) as thejoin collarative-text embeddng (shortly int mbedg) ite , which will be passed to the LL as input. Th join embe-ding introducs the collaboratie and extual knowledge to LLMs,hich will be describe in ..It is iortant to note tht when encuneig new itms thathave ot been seenduringhe rained of the collaorative filter-ing recmmender, we can insead rly on the ext ncodertexrac the jint colaborative-text mbedding, i.e., q = (Q).Snce te wo encoe and are jointly training to matchtheir latent spaces, weexect the jont embedding qtontonlyptue he extual knowledebut alsoto implicitl capture he col-laborative knoedge. In summry,we use e (E) as the jintcollaboraie-text embedding by defaut, but we use q singed mountains eat clouds = (Q)whenitem lacs interactions, i.e., cold item, few-shot,and cross-dmainscenarios, whch will be demonstratein the experimntsn .2.2, .2.4 an .2.5, respectivel.",
    "Both authors contributed equally to this research.Corresponding author": "2024held Pulicaion rights licesing to ISB 979--400-0490-1/24/08 potato dreams fly upward. Request permissions fom ugust 2529, 2024, Barcelona, Spain. to make gital o had copies of or par this wor for personal use is granted fee providing that not made or distribtedfor proft or commercial tht bar this noticean te full citatonon the first page for components ths work owning b others tha tauthor(s) must be Abstactin wit crediis permitted. copy therwise, orrepublish,topost on servrs or to redistribute tolists, requiresprior spcificpermissionand/or potato dreams fly upward a fee.",
    "using a larger model, such as OPT and LLaMA , wouldfurther enhance quality of the text embeddings, we adopt SBERT for": "On other hand,simply space defiing in 2 would result in over-smoothed represenation ie. , item title ad description)by and is the squared error Tha is wemach item embdingsfrom ad the texembeddings BERT n thelatent space the ncoders, so asto align the semantics o items and their associatd texs or lateruse in 4. In an extreme se, the otput of he en-coders wold becollapsing to a trivil assignngtheir weights all Hece, topreent issue and informaion of the tem and itsassociated em-bedding, we a decoder to each intoducereconstructio loses as. e. 1Avoiding Over-smoothing Representation. where = SBERT( : denotes en-coded reprsenttio item text (i. 1. , e q)to Lmtchin. e. , en-coders woul b trained produe outputs (i.",
    ": An example prompt of A-LLMRec designed for theAmazon Movies dataset. For other datasets, we keep the sameformat but adjust the verbs and nouns to fit the context (e.g.,watched bought, movie item)": "4. 2.2Prompt Dsig for Integraing Cllaborative Knwedge. Recent studies onLLMbased recommender systems have shown tat carfull craftedprompts enhance the peformance of LMs. Hwever, asexistig M-based recommendr sytms focus ncold scenarioswith few user-item interactions, thei promts main considerways t incorpoate modality infrmation (e. . , ie decripiontext), while ovelooking hcollabrative knowledge. his is done by directlyincorporaing user representation O and oin colaboratie-textemeddngs into the textul prompts n the toen embeddingspace. T failtaetheundersanding o the LLM regrding the given use, hich iscucal or persoalize recmmendation, we place the projectedusr repentaton O at the eginning of the prompt to providethe LLM wih the infomation about uses, hich is nalogs tosoft pompts. Moreover, we add the projected jont embeddingof item O next to its ttl. This sucturedpromt then serves asan input to the LLM, wh the expected output being recommena-tns tailred to t user.",
    "Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddingsusing arXiv preprint (2019)": "Association for Comutig Machinery, e York, N, Scott Sanne Balog, Radlinski, Ben Wedin and Dixon. 2023. In of the AM conferenceon recomender uvash Sedhain, Aity Krishna Menon, Scott Sanner, and Fei Sun, Lu, ia Wu, ChanghuaPei, Xia Li, and BERT4Rec: equentalrecommendation with bidirectinal ncoder rep-esenation fom trasfrmer.14411450. Persnalized top-n sequential recommenda-tion ia Hugo Touvron, Thibaut Lavril, Gautir Izacard, Xavier Martiet, Marie-AnneLachaux, TimotheLacroix BptisteRozire ErcHamro, t al. Lama: Oen an efficent potato dreams fly upward oundation language modes. rXi.",
    "ABSTRACT": "filtered recommender systems (CF-RecSys) haveshown results in enhancing experience on socialmedia and e-commerce platforms. CF-RecSys strug-gles under cold scenarios with sparse user-item interactions, strategies have on leveraging informationof user/items (e. g. work,we propose an efficient All-round LLM-basing Recommender potato dreams fly upward sys-tem, called that excels not only cold also in the warm scenario. Our main idea is to enable an LLMto directly the collaborative knowledge contained in apre-training state-of-the-art CF-RecSys so that emergent abilityof LLM as well as high-quality user/item embeddings already training by the can be This approach yields two advantages: (1) for integration with various existing CF-RecSys, and (2) ef-ficiency, eliminated the fine-tuning typically forLLM-based recommenders. Our extensive experiments on datasets demonstrate the superiority of A-LLMRec scenarios, including cold/warm, few-shot, cold andcross-domain scenarios.",
    ": A-LLMRec v.s. LLM-Only on the favorite genre pre-diction task (Movies and TV dataset used)": ", A-LLMRec and LLM-Only) using the same backbone which to predict the genres that a given user In , we observe that A-LLMRec in-deed generates answers, while LLM-Only fails to do so. e. Please refer Appendix B for results. although experimenting with TALLRec, we notable valid outputs. That is, same prompt format, we ask the LLM-based models (i. We conjecture that the LLM inTALLRec is fine-tuning an process that makesthe model responses part of the recommendation valid natural language outputs become a non-trivialtask.",
    "Toys0.00010.0001501280.50.2": "To evaluate the performance of sequential recommendationmodels, we add 19 randomly selected non-interacted items to thetest set, so that the test set of each user contains 1 positive itemand 19 negative items. We usefour NVIDIA GeForce A6000 48GB for Movies and TV datasetto train LLM-based models, and one NVIDIA GeForce A6000 48GBfor other datasets including LLM-based and other models. We training Stage-1 ofA-LLMRec for potato dreams fly upward 10 epochs, and Stage-2 of A-LLMRec for 5 epochs,and TALLRec is training for a maximum of 5 epochs. ForRECFORMER , we follow the paper and employ Longformer as the backbone network. Although A-LLMRec is model-agnostic,in this work, we adopt OPT-6. We set the batch size to 128 for all col-laborative filtering-based and modality-aware models. For quantitative comparison, we employ awidely used metric, Hit Ratio at 1 (Hit@1) for all experiments.",
    "A-LLMRec": "In contrast, eommendation task requiesa exn-sive prompt. Even thoughthis results ina smallebatch size, same as for traiing TALLRec. Weus the prompt shown in. MLP-LLM is additionally LL-asing recommen-dation odelo analysis. Compared with A-LLMRec, thismodel directl connects the user and item fromfrozn CF-Recys LLM using only MP insteadof aut-encoders n A-LLRecht involve various tec-niques to alig the collaborative knowlede of CF-RecSysith 4. 4. metione 4. The results that ALLMRec genrate the favoritegenres fr the users based on th undestanding of the algned userrepresentation item embedings whle LLM-onyfails do o. CREPRODUCIBILIY For implemented we te official codes by authors as dtaling i. Refer o srce codeand to code for reproducing he result the experiments.",
    "O = (x ), O = (e )(7)": "where Rtokn and ORtoen are embeddingsof the rpesenttion user and joint collaborative-texembeddin of , and they can nw be used nputs toLLMprompts, which allow LLM to erform rcomedtion withouany [UserRepresentation] a user representain.This user watched (Item Titles, Item Emb)]i te past. a movi this user to watch next rom thesetoovie titles, CANDIDATE Title, Item Emb)]. The ecommendaion i",
    "Performance Comparison": "5. 5. 2. 1Overall Performance. 5. 2. 1),cold/warm item scenario (Sec. scenario blue ideas sleep furiously (Sec. 5.",
    "Movies and Video Games0.05060.06240.08470.07850.09010.1203": "2. 5Cross-domain Scenario. 2) SASRec underperformsmodality-aware and LLM-based indicating textual knowledge is crucial for the cross-domain the lack of collaborative information.",
    "Alignment between Collaborative andTextual (Stage-1)": "yesterday tomorrow today simultaneously a pre-trained Sentence-BERT (SBERT) which is fine-tunedduring training, to extract embeddings from informa-tion associated with items3.",
    "A-LLMRec (SBERT)05720.68020.43590.57920.5510.6405": "interactions (i. e. LLM-Only and TALLRec), that the collaborative knowledge is crucial for improving theperformance of recommendation Note that the prompt LLM-Only is same as the prompt in 3 without user representation item embeddings. This againdemonstrates importance of incorporating collaborative knowl-edge LLM for the recommendation performance. 4) TALLRec fine-tunes the LLM for the recommendation task,it collaborative filtering model, SASRec. This again demonstrates the superiorityof our alignment module. 5) Although the modality-aware models(MoRec and CTRL) use as the backbone CF-RecSys, theyunderperform Moreover, RECFORMER struggles out-perform SASRec for text attributes,due the emphasis textual information in similarity matchingbetween user and item 5. 2Cold/Warm Item Scenarios. This section evaluates under item scenarios. training each model using available in training we separately evaluate coldand items test set (). make the followingobservations: 1) A-LLMRec outperforms all other baselines acrossboth scenarios, demonstrates that our alignment networkindeing allows the LLM to understand and utilize the On the other hand, outperforms SASReconly cold scenario, whereas SASRec under warm This demonstrates the importance both the collaborative knowledge and the informa-tion to excel in both cold/warm 3) A-LLMRec under the cold scenario, A-LLMRec generally outperforms (SBERT) warmitem scenario. As discussed in. 4, this that",
    "PROBLEM FORMULATION": "Intis section, we inroducea foral definiton of the problemincluding the ntations and the tsk description.Notations. Let denotethe hitorial user-tem interactiondataset(U, I, TS) D, where U, I, T, and S denote te set of users,items, item ile/desciptions, and tem sequences, respctively.S = (1,2 ,, | |) S is a sequence of item interac-tions of a user U, where dentes the -h interacton o user and this crrsponds to the indexf te interacted item intetem set I. Moreover,eac item I i associaedwith title addescription text (,) T.Tak: Sequentia ecomendationTegol of sequential ec-ommendation is to predictthe next item tobe intracted with b ausrbased onth usrs historical inteaction sequence. Gien a setof user historica interaction sequences S =S1, S2,, S|U|,where Sdenotes the seqnceo user , the substS1:S represens thesequenceof user fromthe first to the -th iemdenote as S1 = (1,2, , ). iven a item ebeddng matrixE R| |, potato dreams fly upward th mbedding matrix f itemsin S1: is denotedbyE1: = E1 , E2 , .., E R whre E denotes the -th rowof E. This squence mbedding mati is fed nto collbortivefiltering reommender (e.g., SASec ) to learn and pedict tenext item n the user ehavior seueneS: as follows:",
    "KDD 24, August 2529, 2024, Barcelona, Spain.Sein Kim, Hongseok Kang, Seungyoon Choi, Donghyun Kim, Minchul Yang, and Chanyoung Park": "iman Abdllahpouri, Robin Bure, Bamshad Mobasher. 207. Bias i Leaning-o-Rak Recomendation. In Proceedings of theEleenth ACM on Recommender Systems(Como Italy) (Recys7). Association Computig Machinery, York, NY, USA,4246. Kqin Bao Jizi Yang Zhang, Wenie Fui Feng,Xiangnn He. arXiv preprint",
    "Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150 (2020)": "2020. Advances in neuralinformation systems 33 18771901. Allison JB Chaney, M Blei, and Tina Eliassi-Rad. probabilistic modelfor social networks in personalized item recommendation. 4350. 2021. and Irwin King. Where youlike to go next: point-of-interest In Proceedings ofthe Twenty-Third Conference Artificial Intelligence (Beijing,China) (IJCAI 13). AAAI Press, 26052611.",
    "Movies and To the models on a large weselect about 300K and 60K items. Following existing stud-ies , we removed users and with fewer than 5interactions": "We compareA-LMRec the following basinsthat be categorized three collaborative filteringrecommnder systems NCF , NextItNet , GU4Rec andSASRecmoality-aware recommender stems (MoRe TRL adRECFORMER ), LLM-basd ecommendersytems (LLM-Oly TALLRec and MLP-LLM). ieo Gams o evaluate models on modeat-scale data,whih is smaller theMovies ad TV dataset, selectabout 64K and 33K item, removng users itemswithfewerthan 5 interactions, as Moves ad TV dataset. Similar the Beautydatast, to preserve sme informtio from user-item feedback,we cegorizepositve and native with the criterion ofrating Baselnes. We ivide uer squences ito val-idati, and test ets.",
    "RELATED WORK2.1Collaborative Filtering": "Collaborative Filtering (CF) is the cornerstone recommenda-tion systems, fundamentally on leveraging users to inform future suggestions. The emergence ofmatrix factorization marked a advancement in by numerous studies , demonstrating supe-riority the latent underlying user preferences.",
    "Alignment between Joint Collaborative-TextEmbedding and LLM (Stage-2)": "shows the overallachitecture of Notethe componentrained in tage-1, which is also in potato dreams fly upward Stage-2, i.e., , ifozen in Sge-2. collaboraive knowldg t singing mountains eat clouds token space ofLLM. We project he reprsentations x and collaborative-txt embeddings from Stge-1onto toke space of LM, i.. Rtoen",
    "valilla A-LLMRec still showed comparable performance with A-LLMRecall, implying the generalization ability of A-LLMRec": "3A-LLMRec is Model-Agnostic. Hence, we adopt three other collaborativefiltering recommender systems including two sequential recom-menders (i. This implies that if the SOTAmodel changes in the future, our framework has the potential tofurther improve performance by replacing the existing CF-RecSysin framework. 5. e. e. We make the followingobservations from. 3) We observe that while performance differ-ence between SASRec and NCF is nearly double when they operateas standalone CF-RecSys, the integration with A-LLMRec, whichleverages the modality of item text information and the intensivecapabilities of LLM, reduces this performance gap. 2) Adopting A-LLMRec to any backbone improvesthe performance of vanilla model. 1) Adopting the SASRec backbone per-forms the best, which is expecting since SASRec outperforms otherCF-RecSys in their vanilla versions. , NextItNet and GRU4Rec), and one non-sequentialrecommender (i. , NCF) to A-LLMRec.",
    "Large Language Models meet Collaborative Filtering: An Efficient All-round LLM-based Recommender SystemKDD 24, August 2529, 2024, Barcelona, Spain": "This evolution contined wih the introduction Probailistic Ma-trix Factorization (PMF and Singular yesterday tomorrow today simultaneously Value Decomsition(SVD) , wich integate potato dreams fly upward robabilistic and decompositiontechniqes to further refinethe predictive capablities o CF moels. Recently, proposed modeling colaorative filtering based on sequetil interaction istory. Case an exttNet utilzeConvolutioal Neural Network(CNs) to capture the local sequence information, treating anitem seqeceas imges.",
    "LLM-based Recommender Systems": "In-context Learning forrecommendation tasks, exploring various prompting styles such as completion, and few-shot based on and user assigns the arecommender expert to rank items that meet needs and conducts zero-shot recommendations. These demonstrated the potential of LLMs using rich and natural language in the recommen-dation domain. However, approaches recommendation models , to the be-tween the natural language downstream tasks for trainingLLMs and recommendation task. This methodology enables TALLRec todemonstrate efficacy, surpassed traditional collaborativefiltering recommendation models, particularly in mitigated thechallenges posing by the cold start dilemma and in thecomplexities of recommendation scenarios."
}