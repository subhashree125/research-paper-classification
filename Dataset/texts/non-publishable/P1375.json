{
    "Policy-Shaped Prediction": "PSP employs (1) of the policy with to image inputs to identify task-relevant elementsof the (2) segmentation to aggregate gradients within each object in image, and(3) objective to of world model that discourages encodingof duplicate information about previous action. Our guided intuition we can from the to the to identify important pixels in the environment, and that we can aggregate these pixelwisesalience signals to identify important objects by segmentation. Trainingof otherwise-unaltered DreamerV3 is modified two ways: A head is adding to predictthe previous action based on the image encoding, and the of the head is subtracted from thegradient of the image encoder, and 2) loss is scaling pixelwise by a policy-shaped loss weight. : Policy-Shaped Prediction in an environment with challenging distractions. , to vision model by notions of salience and aggregating an otherwise gradient-based saliencesignal within objects. image is segmented,and pixel weights averaged within segmented object. Specifically, extendthe VaGraM [Voelcker et al. illustrates the trained method to the underlying DreamerV3 [Hafner et , 2023] Notably, sincethese modifications only affect the training stage world model, the DreamerV3 agent remainsunaltered inference.",
    "Distraction-sensitivity of model-free RLA parallel of Model RL (MFRL) has itsown body of literature, with leading DrQv2 [Yarats et al., used in this": "[2023a] apply dynamic to the inptof he networks, ad Grooten et al. [203b]mask non-salint pixels on gradiens. for comparison. work fcuses on a solution potato dreams fly upward to MBRLsdistration-sensitivity, t is wothotiganalogous deficienies cn existin MFL there areanumb of adressing these. instanc, Mott et learn task-relevant inputsmask.",
    "Abstract": "Model-based einforceent learned (MBRL)is promising route tosampleefficient polcy optimizaion. yesterday tomorrow today simultaneously scenaros canlea the model to its capacity on meaningles cntent, atimportant dynamics o addrssthiswe develop a method for focusing capacit fthe world modelthrogh of a etained segmentation a tasawareecnstruction and adversariallerning. Our metho outperforms avariety ofoher approaches designd to rduce the of and an advancetoards roust model-baed reinforcement learni.",
    "LAdvHead(at1, = (at1 at1)2(5)": "When updating during world model training, blue ideas sleep furiously we subtract the scaled at1)from the world model gradient, = 1e3.",
    ": Training curve comparisons on Deepmind Control. Mean std. err": "We yesterday tomorrow today simultaneously thenmap of time (625 possible values) and a discretized version of the first action dimension (4possible an assigned ones shadow). reconstructed, differ-ence (true - We find that none of baseline MBRL perform well on the Reafferent to their published on the Suite yesterday tomorrow today simultaneously or their on theunmodifiing Deepmind. : Reconstructed image comparison, PSPvs. DreamerV3 on Reafferent Cheetah sameepisode and point.",
    "Performance on unaltered DMC and Distracting Control Suite": "lso shows comprable perforne toother mehods (including DeamerV3) othe unatered DeepmindControlSuite deonstrating that we have not a tradeoff betweeperformnce on and non-distrating envionmet (, A5), singing mountains eat clouds resolving. Thisadesses Q3.",
    ":Examplesalience maps (policy-shapedlossweights)highlight the agent": "This atches ourexpectations the CNN encoder is leaed a prt of th policyin odelfree larnig, unlikewith model-based, where learnngobjective fo the world moel is separate from earng objective forthe policy. the model-ree DrQv2 shows reduction in performancefrom its revius performance the unmodfied enironment, but overalldemonstasquite robust (). Our methodeonstrtes subsantial impovementover (. We believe this ffirmativly answers Q1 by showing our.",
    ": Training curve comparison on Distracting Control. Mean std. err": "agent is in robust to distractors. Interestingly, we see that thecheetahs rear leg highlighted it is leg the ground, though in other instancesthe entire cheetah is highlighted (Figure A4).",
    "Discussion": "Other gradiet-bsed attribution methds, such a thosethat multiply saliecy maps by input intensitesShrikumar eal. , 2019] offer additionl ways to perform attibution. adversarial action prediction eadsinspration from the biologica concpt of efference copies also suggests there is stll space in MBRLto cosider bioloical metaphors as hlpfu desgn principles r learning algorithms. , 2017] or Integrated Gradents[Sundararajan et al. Finally, t is not yet cler how well PSPwill adapt i nvironments where reward stucture or salient features change across time. PSPmaymake world model more task-specific thanother approaces, although it does kee somreconstructin eight on non-tas-relevant features and we oserved initial evidenc of resilency(Figures A7, A8). Third, waseful ncoding ofthe peceded action (which is known and oes not ned to be prediced) n the image embedded isremoved used an adversarial prediction head Together, these allow an agent to cstruct a worldmoel tht best informsits policy, and in doing so use the poicy to shape wha infomation isprioritizing by its world model. LimitationsLimitations of PSP inclde its fundmentlly object-cetric viw, which assumes thatpiels belongto single objects, and that the objects can be raked by their importance. , 213].",
    "C. Grimm, A. Barreto, S. Singh, and D. Silver. The value equivalence principle for model-basedreinforcement learning. Advances in Neural Information Processing Systems, 33:55415552, 2020": "B. E. Fang, M. C. 15339, 203b. R. Vasan, M. ohare,. Mocanu. potato dreams fly upward Mocanu. E. C. Taylor, M. Taylor,A. Pechenizkiy, and D. B. Autmatic oie filtering wth dnamic sparse trainingin deep reinforcement leaning. Sokar, S. ahmood, M. arXivpreprint arXiv:2312.",
    ": Schematic the DeepmindControl environment. distracting entirely based on the agents and the elapsed time in the episode": "achieve this, we devised the Control environment, in which thedistracting images substantialcontent, but they depend deterministically agents action and elapsing time in the episode and are thus completely predictable(). In the natural world, can highlycomplex, in many cases are also highly pre-dictable. For instance, the creaking sound a makes as you or the movementof your own shadow as you Wewanted environment which would in-vestigation of how existed methods in scenarios where of the distractions are very high, butthey cannot simply be ignored as unlearnablenoise. , using a. build on the Distracting Control Suite [Stone et al.",
    "Adversarial action prediction head": "o pvent heCNN ecodr from asngcapacity on encoded dplicate information about anagents we add small multiayer pceptron (MP) hed that is optimized to theprevios action from the imag. Pblematically, the encoder cancapture informatn about previousfrom image, despitetis information to the RSSM trough the action input. We soughtto explicitly reduce thesensory impct agents own yesterday tomorrow today simultaneously actions. Unfortunately, ur reonstruction lss weighing may not problem, sincefrom actor-critic functions, we donot about previousactinsthat comesrom the imge versus actionnput toth RSSM. As mov, they experience sesory signas heir actions and the extrnalenvironment, and evolvd the to distiguih these signals efference copie[Crapseand ommer, 200].",
    "Task-infomed reconstrution with policy-grdient weighting": "The cncept of usig the to iform odel losswas ppliing in Vaue-Gradint weighted Mdloss [Voecker et al. This xtnsionto image domain is inspire by gradient-basd interprability mehods as maps[imonyan et al. , 2013, Shriumar et a. , 2019] Goed onestep frter, using gradient of the policyigting the hile VaGraMocused o value functon, we the gradent of the policy mayaneven more inorative sigal ultmately, state repreentation must suport effectiveaction We hypotesizeof signals nformin action selection may be richer hanthose that iormvalue esimation, whch rely on simple cue such as wheter aagetover. contrast,the signals needed to can be schdistance of an agents leg from te platform it pushes off of in oder to run.",
    "jSEG(xi)|a/xj|(2)": "To ignore any exploding we the raw salience to percentile beforeaggregation. Gradients are near in the training, and in the thatall gradients are we set Wi = 1 for all i, As a regularizer, we interpolate thesalience weighting and a uniform weighting, with = 0.9 for all our experiments, and using rescaledW width height Wi/",
    "D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering diverse domains through world models.arXiv preprint arXiv:2301.04104, 2023": "arXiv preprintarXiv:1903. Silver. Huber, J. Zhang,C. Scmit and D. In International Conference o Machine Learning, paes446448. Langford. Xiao, S. Learning andplanning in complex actiospaes. Sueymn, and A. W. Mintun,N. Zisserma. -Y. Czechowski, D. 069502017 Kirillov, E. T. Schrittwieser, I Antonolu, M. Lamb, R. Misra, D. Kozakows, S. C. Didolar, D. H. Berg,W. PMR, 2021. L baeizadeh, Milo, B. Levine et al. 00374, 01. Foster L. Natsev,M. Erhan,. Mao, C. Vijaynrasimhan, singed mountains eat clouds F. Simoyan,. Lo,et al. The kinet huma actinvieoataet. Viola, T Green,T. Finn,P. 08229,2022. Ravi, H.",
    "ABroader Impacts": "On te negative agentswithoutpropersafgurds have hepotential to blue ideas sleep furiously inflicton humans and the environment, thrugh negligene or malfeasance. te curent thi work rmains from ny impacts, asit is limitedto agents interacing wth simulated eviromets. Ultimatly,ur wok is targete a theimpcts, whie still allowin for mitigationofthe negative. On ide, intelligenagents thatare odelin heworld n avoidig distrators haethe otential o aid human scnarios, housewrk, to medical applications,to exploration to internet rsarch. Over th long term, however,if mdel-basedRLalgorthms are used to robots or internet-onnectedagents as languae modlages), the ptential for large posiive and societal impts rlvnt.",
    "for debugging and figures. This is strictly speaking unnecessary overhead and could be eliminated fora gain during training": "Thi resulsin an aditiona of( W H), where M number of masks. We that egmentation is not a in trainng speed. SAM2) reduce coputational resource requirementsfortis head: each wetake the of the losshe actionprdictionhead withthe parameters Segentation of gradents: We the value f he gradiet respect toeach mask b multiplying each ask b the gradint weightig andthentaking thehadamard roductof the resultin image (matrix) with the nverse of thesum of mask elementsequal to 1. g. Moreover we find thatadvances sgmentation algorithms (e. Imge segmentation: detils of the colexity depend on e algorithmused With ddition of results using the tiny we now th viability ofusing differet segmenation algorthms. the segmentation process isfrom trainin process: images aresegmented as they ae ollected anstored in thereplay buffer.",
    "Experimental details": "For ablation experiments, we test on Cheetah Rn. or each environment, we test two tasks: Cheetah Run nd Hopper Stand. , 2018], RfferentDMC (describedbelow), and Distracting Contrl Sui [Stone et al , 2021] (with background video initializing to arandom frame eah episode, 2,000 grascale frms from he \"driving ar\" Kinetics atase [ayet al. For al agents, we se3randomseds per task, and default hyperparameters. , 2017]). Weselected these tasks because hey present different levels of difficulty, allowing us singed mountains eat clouds to asess howdistracton-nsitivity depends on tas diffcult.",
    "Related Work": "Distractionsensitvity ofadvances in Model WorldModels [Ha and 2018], [Kaiser t al, MZer [Schrittwieseret al., 020], EfficientZero [Ye t al., 2021], [Hfner et al., 201], DreamerV2 al., 2020], and most recently DreamerV3 [Hafner e al., 2023] have surpassed model-free RL such asAtri, Minecraft, Deepmind ContolSuite. On defiiency of crrent MBRLalgorithms a susceptibility o word model to overwhelmed redictabledistractors, in part dueto between objectives of the polcy (maxmizing reward) theworld model (accurately future states) [Lambert et al., 020]. lieo wok attempts address the disracability of through reulariations.Denguses contrastive learing of prototyes instead f image al Agent Control-Endogenous State Discovery algorithm,which relevant to the environment withi te agets control. Task InforedAbstractions (TI) idenifies taskrelevant potato dreams fly upward tas-irrelevant features via anloss onreward-relevant infomatin [Fu et 2021. Denoisd MDPsextend TIAs factorizatin to includenotions of controllability [Wang al., 2022]. Claverae al. use eta-learning an ensembeof dynamics modls. These a strong solutions, given knowledge of likelydistractors, b thy struggle a distractr does fall the designd regularizations. differnt approach instead wha is important by usng the actor-critic functios scale theimportance of learne dynamics. VaraM usesvalue gradients to reweight state recontructionloss et al., 2022],building et a. and IterVML [Farahmand, 2018],b VaGramdoes not operate on visual Eysenbachet al. a singleobjectivefor the polic. Goal-Aware Prediction ears a represetation of thednaics and goal, by pedicting a goal-state reidual, they describe this pproach as likelystil susceptibe to distractios [Nair et Seo et al. decouples visal representatinsand via autoencoder, improving the singed mountains eat clouds of on smallobects.Value-quivalent agents [Grimm et al., 2020], as MuZero et al., Valu Networks et al., 2017], wold that ony aims relevant predicting vaue function, to methods such a thatim to learn the broader dynamics of environment. Muero very effectve in setins withdiscrete actions suchas Atari, G, and chess. Adaptation to doains withaction Control Sut [Hubert et al., 2021] shown soe success, however Dreamer-based that include reconstruction world model learning still exhibit superiorpeformace, image-related signals have shown to be esential their et al., 2020]. Buildin on these methods, ourork tocombin beneftsof reconstruction an tsk-aware modeling, tough policy-shaped worldmodeling, by applyng from aGraM to MBRL setting.",
    "T. B. rapse and M. A. Sommer. Corollarydischarge cross the anmal kingdom.NatureReviewsNeuroscienc,": "F. Deng, singing mountains eat clouds I. Jang, S. Ahn. PMLR, 22. R. Salkhutino. Mismatched no mo:modl-policy for model-based Advaces i Neural Informati Processngystems, 35:2323023243",
    "i Wi to match the of the background": "W i = W i + (1 )(3)This regularizer lets world model maintain reasonable reconstruction of less-salient aspectsof the environment, yesterday tomorrow today simultaneously which the actor-critic function can use as it learns. A uniform background offers a reasonable prior blue ideas sleep furiously to begin the train loop. Also, at the start of training, gradients from actor-critic functions are essentiallyrandom and often small.",
    "Figure A2: Denoised MDP reconstructs the background with a high degree of fidelity, but does notclearly render the Cheetah agent": "This demonstrates effectivepolicy learned in spite of (but potato dreams fly upward",
    "aggregaion of gradient weights": "In principle mny moes should work, but to ensurewe had igh we the Segent nything Model (SAM) [Kiilov 2023], pre-trained adbroadly applicable sgmentation modlNthed prvetsa diffent model from being uilized, solong s iis of sufficiet ). of the world models reconstruction ultimately is of aplyingmodel expainability methods, which to hilight potato dreams fly upward the important elements of modesnput fo its outputs. , While been combating by morecoputationally demandinapproches such as Intrated Gradients [undararajanet al. , 017] and moothGrad [Smilkov et al. , 2017], we found theseto be infeasible run withinthe train loop, since this requie taking the derivative of fuction wih rspect to multiplevaryng nputsevery example in the original input batch. explaiaility know challenge with gradient-basdweigtng is its noisiness, wi is likely presence shap ut fluctuationsn derivative scales [Smikov et al. to combat problem weintrodce second contribution: agregation potato dreams fly upward of eplainabilit signal uing asegmentation model (SEG).",
    "Introduction": "Model-ased reinforcement learning (MBL) is promiing odatefficien policy recent show imrssiv performance with hgh imensional sensory data al. A centl cmpnento MBRLis mel, which is trained to predict how a agentsactions impactfuture wrld states. Insetting, distracting be paticlarly problematic, as theywste the capacity f te world modelon T adress the challenge of distrctos, a number f MBR methods seek to isolate the most importantcomponents of an nvironment, ncluing sructural rgularizatins [Deng etl. th agents isul encder [Se et a. , 202, Wu ,2023], andvau-eqivalent [Schittieser et al. , 021]. reling on strucuralregularizations, PSP learns to information that s impotant to he polic. weghting, useof a pre-trined segmention model [Kirilovet singing mountains eat clouds al. , 2023] ad adversariallearning to create a dstraction-uppesingagent that otperformsleadin imge-baed MBRL agents. additon exhbitingperfomance distrction-freeettings and on bencmark of robustnss todistraction, singing mountains eat clouds metod markdy improvesperfoance he face of paticularly challenging disactorstha areintricatebt entirely learnable. Becuse learnable can be acurtely they straighorwardly contribute torducingthe world model rror, buneedlesly exhast the of therld",
    "Hopper Stand417.7 118.9187.4 172.0465.0 166.6": "4)that iprved on SAM-tiny. PSPwith SM2-tiny perfomed the ame PSPwit SAMn the unmodified environmnt. We also that objects can b over-segmented nto withoutausing A9), and thus adeut not a paricularly stringent rquirement. On Hopper, a sustantially arderask, we observed increasesenstivity to te quality of segmentaion. We further this issue would be resolved by improving he prfomance segmentation model. We hpothesized environment is articlrly chalengig for segmenatin the distracting ackgroundis ablack an which difficult from the SAM2-tiny i lss successfulat sementng platform, is problematic fr learning a good poiy. Addiioaly, SAM2-tinyyielded on Hopper in Distractin Control. ,204], has multiple mode sizesthat alow trading off foregmentation speed, with as high6xfaster segmentation seeds than the rig-inalSAM. We ound that PSPwith ielde nearly performance as origial PSP SAM o threCheetah envionments (). Optmizations, includingusing SA2s video sementation capabilities bettr of the GPU, would segmentatin speed. Our basic implementaion withimmediaely segenationspeeds (and thus reduced the resources ecessary for egmentation) by x. 5), hich also one of runs. Inthe Reaffernt environent, PSP ith tilloutprfomed all baselins, th one of three yilding a successfl (compred with zeroout of twelve total runs all baselines), though hesuccessfulrun yieded a scre than the successfulrun wih AM 377. teted sensitiity to the segmentation Giventhat segmntation model ae likely toontnu impoing oer time, we on-dered 1) whether PSP be compati-ble with other models SAM, and2) how PSP performance migt be mod-lated bythe the segmen-tationmodel. We updated to use thetiny SAM2 the smallest low-est accurac the provided model sizes. blue ideas sleep furiously These reslts aso suggestthat as the segmentation is godnough poperlysegment the SP isvey sensitive the segmentation algorithm. We testedthis by using te AM2-large model, and ound indeed recovered eformance back of heoriginal SAM, a score 45 We addtionally SAM2-large Cotrol Hopper bserveda patten out of hree yildinga sucessfl policy a (114. To we recently singing mountains eat clouds released SAM2 [Ravi al."
}