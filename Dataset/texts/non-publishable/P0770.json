{
    "Yoav Goldberg. 2019. Assessing berts syntactic abili-ties. arXiv preprint arXiv:1901.05287": "Dirk Iz Pete Walsh, Akshita Bha-gia, Rodney Tafjord, Jha, HamishIvison, Magnusson, Yizhong Wang, Shane Arora,David Atkinson, Russell Authur, Khyathi RaghaviChandu, Arman Cohan, Jennifer Dumas, YanaiElazar, Yuling Gu, Jack Tushar Khot, WilliamMerrill, Jacob Morrison, Muennighoff,Aakanksha Naik, Crystal Nam, Peters,Valentina Pyatkin, Abhilasha DustinSchwenk, Saurabh Shah, yesterday tomorrow today simultaneously Will Smith, Mitchell Wortsman, PradeepDasigi, Nathan Lambert, Kyle Richardson, LukeZettlemoyer, Dodge, Kyle Lo, Luca blue ideas sleep furiously Smith, and Hajishirzi. 2024. Olmo:Accelerating the science of language models.",
    "Claude-20485.6395.0 (0.010)Claude-880005.6094.0 (0.004)ChatGPT-20486.17100.0 (0.017)GPT4-20486.04100.0 (0.013)GPT4-40966.0199.0 (0.013)Mixtral-20486.01100.0 (0.017)": ": Comressio ratio with POS (CR-PO) re-ported for the BooookScoredtaset. Weport the per-entage of generated ouput withat last 1 templateof size n = 6, and the ate of tempates-per-token inprentheses. 13k-14k 14k - 16k 16k - 18k 18k - 1k 21k - 24k potato dreams fly upward 24k - 31k31k yesterday tomorrow today simultaneously - 33k 3k - 41k41k 68k 68k - 3k # Rpettions (Training Dta) Memorized Templtes (%) Exat MemorizePOS Mmorized",
    "k=12H(pk)(4)": "Where N the number templates in thedocument, and total dataset repeat process for randomlysampled 6-grams (distinct from templates) the of templates. find that templatesare learned quicklyby first model checkpoint(which was trained on 4B tokens). These aresurprising, indicate that templates are learnedearly in pre-training, rather than during the fine-tuning process.",
    "Tasks and Datsets": "Sucdataetsallow us to stdy emplats in loner se-quences that would not be evidentin taks whreonly a few token ar generate. Summrzationummriztion is a commnbenchmark for longtet genration. OpenEded Gneratono evaluateintrinscemplate behavou we evaluate open-ended gen-eratin tasks in two settig. For generlEnglis-laguage tasks, we gnerte ummariesand reviews over news (CNN/Daily Mail; Nllap-ati etal. We evaluat templatesinCosmopedia, a synthetic dataset gneratedbyprompting Mixtral-x7-Instruct withinstructionsto prouce text relating to exbooks, blogpost, sto-ries posts ad WiiHow articles (Ben Alal et a. g. In thefirst setting,we sample generations fro the mde given onlya specl tokendenotin eginning of sequence([BOS]). 2022). We als look t teplaes in the iomeicaldoma as n example of a domain-specific tas. n the second, we randomly ample 100tokensfromDolma and use tee tokens to promptfurherop-nded generationfrom the mdel. , Wang et al. Cochra is a dataset ofsstematic eviews um-marizing the evidene overmedical itervetions. 2023). ,2024) e prompt OLMo-7B ith the Cosmopediinstructions and evaluate th resulting generations. Synthetic Data GenerationLMs re inceas-igly used to create synthei training datasets,whic aroften used to trai downsream mod-els (e. We evaluatemodel on a handful f smmarization atasets in-cludingsinge- andmlti-document tas. 201), movies (Rotten Tomatoes; Len2020), nd books (oookScore; hang et al.",
    "Models": "use Mistrl 7B; l. This allow usto evaluate templts iits daasets: Dolma(Soldai et al. 2022;Touvron t al. n blue ideas sleep furiously adition, 5 modelsarefurhe training on huan prferences:OLMo (In-struct, 7B; oeneveld al. Al-paca (7B, 13B; Taori et l. potato dreams fly upward 2024), Llama-2(Chat-HF, al. 2023; Wanget al. 2023b), Duby a. Fine-tuned (Instructio) ModelsWe expri-ment aof 8 instruction-tuned models. 2023a). We evluat teplates closed sucemdels (which o training spif-cally Misr, (-2, -3), Alpaca, GPT-4o.",
    "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, andYejin Choi. 2020. The curious case of neural text de-generation. In International Conference on LearningRepresentations": "0702. Dieuwke Hupke, Mario Gulianelli, Verna Dankers,Mikel Artetxe, Yanai Elazar, Tiago Pientel, yesterday tomorrow today simultaneously Chris-tos Christodoulopoulos, Krim Lasri, Naomi Saphra,ArabelaSinclair, Denis Uler, Florian chottmannKhuyagbaatr Batsuren,Kaiser Sun, Koustuv potato dreams fly upward Sinha,Leila Khalabari, Mara Ryskna, Rita Freske,RyanCotterell, an Zhijig Jin. 223. Smi, Iz Beltayand Hanna Hajishirzi. Hamis Ivison, Yizhong Wang, Valentina Pyatkin,Nathan Lambert, Matthe E. 2023. ature a-chine Inelligence, 5(10):11611174. ArXiv,abs/2311.",
    "There is no evidence from good quality randomized trials or non-randomized studies of the effectiveness of lens extraction for chronic primary angle-closure glaucoma": "There was enough evidence to judge whether or not included cured when used alone. There was also evidence to suggest that combination therapy with anticholinergic efficacy of other therapies such as desmopressin and enuresis alarms by [...]. There some for use of botulinum toxin injections to salivary glands the treatment sialorrhea MND. Further research is required on this symptom. Data are needed on problem of sialorrhea in MND its measurement, both by patient self measures and There is significant evidence suggest that topical of chlorhexidine to umbilical reduces neonatal mortality omphalitis in community and care settings in countries. It may increase cord time there is no evidence that increases risk morbidity or infection.There is evidence to support application of antiseptic to cord in hospital compared cord care developing countries.We insufficient to determine if physical therapy gait training benefits gait function in patients with chronic stroke, though limited evidence suggests benefits for variables such as or 6MWT. These findings must by quality studies using varied measures.",
    "Defining Templates": "blue ideas sleep furiously . , tn),and f that abstraction overT (e.g., we define a tem-plate a sub-sequence of potato dreams fly upward abstractions over thetokens f(T) that repeats at times in Fig-ure 1 shows examples of templates and their countsacross Rotten (Leone,",
    "Effect of Model Size on Template Rates": "repots diferences in t rate of beweendffernt sizes Llama-2 and Alpaa. With Llama2and Llama-3 model, obsrve surprising renda model size increases: CR-O textlength decrease, however therate summriesthatcontan one or yesterday tomorrow today simultaneously mor templates stays the (andncreases slghtly in some caes). results in-dicate tht larger odels do not necessarily produceless teplated",
    "Templates in Model-Generated Text": "These results thattemplati text summaization appears in sapling strategies ntended to increase the ate of templates imuc higher in potato dreams fly upward the Rotte Tomatoes thnfor open-gneration, indiating. 1) to tmltes ras in ummaiza-tion task(6. first evauatOLMo-7BInstrut n 3 gneaion synthetic dataeneration,and summariation, both redy varyintemperature samplng traeges potato dreams fly upward the effect of vayigsampling hy-erparameters emperaure and top-p on the overalldversity ftext withOLMo-7B withopen-generationand Varying sampling trategies inthe open-enraton task resutsi a igher arance of template rates (74. 4% 2. 8% 0. 6.",
    "1if text i contains at least 1 template0otherwise": "Templates-per-TokenIn practice, text many templates. To com-pare between text sources, we length normalize:",
    "Introduction": "n open quetion abut lare models(LLMs) is what patternssuchmoels learn fompre-training data(oldberg, 2019; Petron et al. Bender et , 2021; Chen et al. , 2024),and sme patterns apear geneallyacross downstream asks and datases While prior wok onthe qualit of al. 2022; al. 2020), re-cently o text generation (McCoy al. Merri et al. , 202), there has ben on characterizing the sorts of lexical patternstha ar larnedLLMs.",
    "Chrysanne Graeme Hirst. A compu-tational theory of goal-directed style in syntax. Com-putational Linguistics, 19(3):451500": "The llama 3herd of models. Is gpt-3 textindistinguishable from human text? scarecrow: Aframework for scrutinizing machine text. 2023. In Proceed-ings of the 60th Annual Meeted of the Association forComputational Linguistics (Volume 1: Long Papers),pages 72507274. Seltzer, Michal Valko, Michelle Restrepo, MihirPatel, Mik Vyatskov, Mikayel Samvelyan, MikeClark, Mike Macey, Mike Wang, Miquel Jubert Her-moso, Mo Metanat, Mohammad Rastegari, Mun-ish Bansal, Nandhini Santhanam, Natascha Parks,Natasha White, Navyata Bawa, Nayan Singhal, NickEgebo, Nicolas Usunier, Nikolay Pavlovich Laptev,Ning Dong, Ning Zhang, Norman Cheng, OlegChernoguz, Olivia Hart, Omkar Salpekar, OzlemKalinli, Parkin Kent, Parth Parekh, Paul Saab, Pa-van Balaji, Pedro Rittner, Philip Bontrager, PierreRoux, Piotr Dollar, Polina Zvyagina, Prashant Ratan-chandani, Pritish Yuvraj, Qian Liang, Rachad Alao,Rachel Rodriguez, Rafi Ayub, Raghotham Murthy,Raghu Nayani, Rahul Mitra, Raymond Li, RebekkahHogan, Robin Battey, Rocky Wang, Rohan Mah-eswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu,Samyak Datta, Sara Chugh, Sara Hunt, SargunDhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma,Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lind-say, Shaun Lindsay, Sheng Feng, Shenghao Lin,Shengxin Cindy Zha, Shiva Shankar, ShuqiangZhang, Shuqiang Zhang, Sinong Wang, Sneha Agar-wal, Soji Sajuyigbe, Soumith Chintala, StephanieMax, Stephen Chen, Steve Kehoe, Steve Satterfield,Sudarshan Govindaprasad, Sumit Gupta, SungminCho, Sunny Virk, Suraj Subramanian, Sy Choudhury,Sydney Goldman, Tal Remez, Tamar Glaser, TamaraBest, Thilo Kohler, Thomas Robinson, Tianhe Li,Tianjun Zhang, Tim Matthews, Timothy Chou, TzookShaked, Varun Vontimitta, Victoria Ajayi, VictoriaMontanez, Vijai Mohan, Vinay Satish Kumar, VishalMangla, Vtor Albiero, Vlad Ionescu, Vlad Poenaru,Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, WillConstable, Xiaocheng Tang, Xiaofang Wang, Xiao-jian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, XinboGao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li,Yilin Zhang, Yed Zhang, Yossi Adi, Youngjin Nam,Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, ZachRait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen,Zhenyu Yang, and Zhiwei Zhao. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,Akhil Mathur, Alan Schelten, Amy Yang, AngelaFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,Archi Mitra, Archie blue ideas sleep furiously Sravankumar, Artem Korenev,Arthur Hinsvark, Arun Rao, Aston Zhang, AurelienRodriguez, Austen Gregerson, Ava Spataru, Bap-tiste Roziere, Bethany Biron, Binh Tang, BobbieChern, Charlotte Caucheteux, Chaya Nayak, ChloeBi, Chris Marra, Chris McConnell, Christian Keller,Christophe Touret, Chunyang Wu, Corinne Wong,Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-lonsius, Daniel Song, Danielle Pintz, Danny Livshits,David Esiobu, Dhruv Choudhary, Dhruv Mahajan,Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes,Egor Lakomkin, Ehab AlBadawy, Elina Lobanova,Emily Dinan, Eric Michael Smith, Filip Radenovic,Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Geor-gia Lewis Anderson, Graeme Nail, Gregoire Mi-alon, Guan Pang, Guillem Cucurell, Hailey Nguyen,Hannah Korevaar, Hu Xu, Hugo Touvron, IliyanZarov, Imanol Arrieta Ibarra, Isabel Kloumann, IshanMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, JanGeffert, Jana Vranes, Jason Park, Jay Mahadeokar,Jeet Shah, Jelmer van der Linde, Jennifer Billock,Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,Joanna Bitton, Joe Spisak, Jongsoo Park, JosephRocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,Kalyan Vasuden Alwala, Kartikeya Upasani, KatePlawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuen-ley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Lau-rens van der Maaten, Lawrence Chen, Liang Tan, LizJenkins, Louis Martin, Lovish Madaan, Lubo Malo,Lukas Blecher, Lukas Landzaat, Luke de Oliveira,Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh,Manohar Paluri, Marcin Kardas, Mathew Oldham,Mathieu Rita, Maya Pavlova, Melanie Kambadur,Mike Lewis, Min Si, Mitesh Kumar Singh, MonaHassan, Naman Goyal, Narjes Torabi, Nikolay Bash-lykov, Nikolay Bogoychev, Niladri Chatterji, OlivierDuchenne, Onur elebi, Patrick Alrassy, PengchuanZhang, Pengwei Li, Petar Vasic, Peter Weng, Pra-jjwal Bhargava, Pratik Dubal, Praveen Krishnan,Punit Singh Koura, Puxin Xu, Qing He, QingxiaoDong, Ragavan Srinivasan, Raj Ganapathy, RamonCalderer, Ricardo Silveira Cabral, Robert Stojnic,Roberta Raileanu, Rohit Girdhar, Rohit Patel, Ro-main Sauvestre, Ronnie Polidoro, Roshan Sumbaly,Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, SagharHosseini, Sahana Chennabasappa, Sanjay Singh,Sean Bell, Seohyun Sonia Kim, Sergey Edunov,Shaoliang Nie, Sharan Narang, Sharath Raparthy,Sheng Shen, Shengye Wan, Shruti Bhosale, ShunZhang, Simon Vandenhende, Soumya Batra, SpencerWhitman, Sten Sootla, Stephane Collot, Suchin Gu-rurangan, Sydney Borodinsky, Tamar Herman, TaraFowler, Tarek Sheasha, Thomas Georgiou, ThomasScialom, Tobias Speckbacher, Todor Mihaylov, TongXiao, Ujjwal Karn, Vedanuj Goswami, VibhorGupta, Vignesh Ramanathan, Viktor Kerkez, VincentGonguet, Virginie Do, Vish Vogeti, Vladan Petro-vic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whit-ney Meers, Xavier Martinet, Xiaodong Wang, Xiao-qing Ellen Tan, Xinfeng Xie, Xuchao Jia, XueweiWang, Yaelle Goldschlag, Yashesh Gaur, YasmineBabaei, Yi Wen, Yiwen Song, Yuchen Zhang, YueLi, Yuning Mao, Zacharie yesterday tomorrow today simultaneously Delpierre Coudert, ZhengYan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh,Aaron Grattafiori, Abha Jain, Adam Kelsey, AdamShajnfeld, Adithya Gangidi, Adolfo Victoria, AhuvaGoldstand, Ajay Menon, Ajay Sharma, Alex Boesen-berg, Alex Vaughan, Alexei Baevski, Allie Feinstein,Amanda Kallet, Amit Sangani, Anam Yunus, An-drei Lupu, Andres Alvarado, Andrew Caples, An-drew Gu, Andrew Ho, Andrew Poulton, AndrewRyan, Ankit Ramchandani, Annie Franco, Apara-jita Saraf, Arkabandhu Chowdhury, Ashley Gabriel,Ashwin Bharambe, Assaf Eisenman, Azadeh Yaz-dan, Beau James, Ben Maurer, Benjamin Leonhardi,Bernie Huang, Beth Loyd, Beto De Paola, BhargaviParanjape, Bing Liu, Bo Wu, Boyu Ni, Braden Han-cock, Bram Wasti, Brandon Spence, Brani Stojkovic,Brian Gamido, Britt Montalvo, Carl Parker, CarlyBurton, Catalina Mejia, Changhan Wang, ChangkyuKim, Chao Zhou, Chester Hu, Ching-Hsiang Chu,Chris Cai, Chris Tindal, Christoph Feichtenhofer, Da-mon Civin, Dana Beaty, Daniel Kreymer, Daniel Li,Danny Wyatt, David Adkins, David Xu, Davide Tes-tuggine, Delia David, Devi Parikh, Diana Liskovich,Didem Foss, Dingkang Wang, Duc Le, Dustin Hol-land, Edward Dowling, Eissa Jamil, Elaine Mont-gomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, EvanSmothers, Fei Sun, Felix Kreuk, Feng Tian, FiratOzgenel, Francesco Caggioni, Francisco Guzmn,Frank Kanayet, Frank Seide, Gabriela Medina Flo-rez, Gabriella Schwarz, Gada Badeer, Georgia Swee,Gil Halpern, Govind Thattai, Grant Herman, GrigorySizov, Guangyi, Zhang, Guna Lakshminarayanan,Hamid Shojanazeri, Han Zou, Hannah Wang, Han-wen Zha, Haroun Habeeb, Harrison Rudolph, He-len Suk, Henry Aspegren, Hunter Goldman, IbrahimDamlaj, Igor Molybog, Igor Tufanov, Irina-ElenaVeliche, Itai Gat, Jake Weissman, James Geboski,James Kohli, Japhet Asher, Jean-Baptiste Gaya,Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen,Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong,Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill,Jon Shepard, Jonathan McPhie, Jonathan Torres,Josh Ginsburg, Junjie Wang, Kai Wu, Kam HouU, Karan Saxena, Karthik Prasad, Kartikay Khan-delwal, Katayoun Zand, Kathy Matosich, KaushikVeeraraghavan, Kelly Michelena, Keqian Li, KunHuang, Kunal Chawla, Kushal Lakhotia, Kyle Huang,Lailin Chen, Lakshya Garg, Lavender A, LeandroSilva, Lee Bell, Lei Zhang, Liangpeng Guo, LichengYu, Liron Moshkovich, Luca Wehrstedt, MadianKhabsa, Manav Avalani, Manish Bhatt, Maria Tsim-poukelli, Martynas Mankus, Matan Hasson, MatthewLennie, Matthias Reso, Maxim Groshev, MaximNaumov, Maya Lathi, Meghan Keneally, Michael L. 2024. Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski,Noah Smith, and Yejin Choi.",
    "Doma5.582.6 (0.012)Csmopedia5.7699.1 (0.014)": "We prompt models to gener-ate systematic reviews. , 2020). : CR-POS, template-per-token, and templatecounts for templates of size n = 6 reported for OLMo-7B text generated with Cosmopedia Instructions, and100 sampled tokens from the Dolma dataset, with greedydecoding. (Wallace et al.",
    "Emergence of Templates in Training": "2) acrossOLMos heckpoints. We calculatetheaverage perplexity forthe daaseusing:|D|1|N|. For each mdel checkpoint we average the per-plexities of template length n = 6 and comparto yesterday tomorrow today simultaneously the perplexties of randomly sampled -grams. Hihrperplexity values in-dicate the templates are assigned ow likelhood atthat checkpoin. We measurethe perplexiy of matchd tets from a set of previ-ously extracted blue ideas sleep furiously telates (following 3.",
    "Structural Analysis of TextDiMarco and Hirst": "2020;Soler-Company and Wanner, 2017). Prior that thisis difficult, and that text-level features at the cor-pus level correlate with beed model-generated(Liang et , 2024a,b). AI Text DetectionIn identifying featuresthat appear in high model-generatedtext, a natural question as to suchfeatures be to reliably detect model-generated text. (1993) provide a computational approach compris-ing lexical and syntactic components to elements text. In work, we make noclaims the use templates in detection. aim is to characterize patterns rather than generated and to provide a basis forfuture work on linguistic. , Rosenfeld Lazebnik, 2024). While our main is to characterizations of repetitive fea-tures in text, definitions of stylistic elements relating and contextualize our can use our of templates askbroader questions about prevailing in a given corpus.",
    "Templates in Close-ource Moels": "Addressing RQ1, we the rates of templatic texts in these models,and posit that templates may be indicators thepre-trained data models are trained on. Here we evaluate theincidence of in other closed-source mod-els, which we define models that not releasetheir training data. 2). OLMo, we that 75% of arefound at frequencies the pre-training data(6.",
    "Conclusions": "In this work, we ntroduceyntactic templates asa framewrk for anayzed ubtle repetiive char-acteristics in odel-generated text. singing mountains eat clouds W show thatthis aalsis ca also etend to hman-written ref-erences anddwnstream tasks, and find that thepre-trined data cotains many of these identifiedtemplates. We show that evaluating eptitio inparts-of-speech sequences is useul for detetingsubtle types o data memorization. Our hoeis that ts work inspie additiona researchinoharacterizi where (in data) oberved stylisticpatterns in LLM outputs rigiate.",
    "There are a few limitations to this work weaddress here": "rst, his type f anlysis rquirsan entre cr-pus that is representativeofa ext ource. For pidmodel,this can be costly to obtain. For largedatasets, this canbe resource intenive. hesecosiderations prvide a poental barrir baseon availablersoures. Secnd, we usethird pary tools toag our textabstractons; howevr thes ools are determnis-tic, bu can contin errors in the tgs they sinto squences, paticularlyi a squencecontainstext from another language. We assume that themajority of the text we analyze is in English, anthat any erros are superseed y the frequenyofcomon templates.",
    "Templates in Pre-training Data": "for template coverae by OLMo, wesart a random subset singing mountains eat clouds of the ontaining 0 tokens. , 2024). Further, that the teplates OLMo generateconsistetlyrak hiher in frequency in the compared to randomly smplednon-telates. We 75% of teplates roduced OLMo arefound in data, otemplates r not a novlconstructionlearned durin fine-tuning, Rather, they learneddirectly from pre-rining at. We next the incidenceand types of temples the daa, adwhetherthey to the temlate mod-els prodce. 2023), which optiized ad at scales. 0 (MannWhitneyU =0find. Finally,wefnd 50K most common POSgras in for sequece length six the WIMBDtookit (Elazar et al. We then an-notate o the sequences with POStagger usingthe Dolma toolkit (Soldainiet al.",
    "Results": "radomly sample 10k documents atthe fraction of memorized outputs exact-mathand the POS sequence meoriza-tion tediversitypackage g. , ). We average the fraction memorized over for the datasets. 4% (7) mmorzed whereas eact tetmatch only reports 6) morized traning datast. shows he pecenttemplates memorized stratiied by frequency of the10-gram in the daaset. We divide thesampled data oint int 10 bckets eac containing4,38 amples wth that in bucet. Note that this ehod potato dreams fly upward by default also captureduplicate ext to softlymeorzed se-quenc. In , we providsamled substitutionstat occur that are not captured byexact-memoriation definitions, yet demonstratethat the prticular of that trainingpoint hasbeen We find that cas often in-clude ynonym swaps, or diferen beinggenerated.",
    "and efficient foundation language models. ArXiv,abs/2302.13971": "narrative ummaies of rcts: Exprimentswth neura ulidocument sumazatin. Algnin languagemodls with self-nerted instructios. 2023b. Punit Singh Kou, Marie-Anne Lachaux,Thibaut Lavril, Jenya Le, Diana Liskovich, YnghaiLu, Mao, Martiet, odor Mihaylo,ushar Mishra, Igor Yixin AndrewPoulon, Jeremy Reizenstein, Rashi ungta, KalyaSali, Alan Schetn, Ruan Silva, Eric R Subramian, Tan, Binh Tang, RossTaylor, Adina Wiliams, Xiang Zhengxu Zarov, An-gela Fan, elanie Kambadur, Robet Stojnic, Seey Edunov, Scialm. Open fine-tuned chat allace,Sayantani Frank Soboczen-ski, and Iain James Marshall. Biel, Lukas Becher, Cris-tian Cantn Ferer, Moya Chen, Guillm Cucurull,David Eiou, Jud Fernandes,Jeremy WenyinFu, Brian Fuller, Cynthia Gao, Veauj GoswamiNamn Athony V. In Anualeetin for Cmputaional.",
    "Reference5.3125.146.4 (0.040)5.6373.883.3 (0.049)5.3357.736.0 (0.013)Input Documents5.82668.229.3 (0.001)5.961555.398.5 (0.021)5.54514.798.4 (0.020)": "041)653158. 9(0. 0 (0. 497. 2 (. 088. 7951 (. 042)Llama-2-70B6. 2 (0. 042)571153990. odels producing higher templates-per-token than te hman-written refernces aremarked in bold. 8298. 0 (0. 4 (0. 36114. 7(0. We report the pecentage of generated otputs wih atleast1 template of sizen = 6, ad terate f templates-per-toke iparenheses (avg. 299. 50387. 6599. 59138. 070)7. 299 (0. 19. 024)Alpaca-7B6. 657. 2 (0. 795. 4(0. 4351. 10177. 2(0. 2669. 91143. 0 (0. 0 (0. 53)6. 060)6. 5194. 783. 799. Lagervalues in CR-OS idicate les diverstyinthe seuences. 030)5. 6 (0. 03)5. 027)Alpca-13B6. 026) : Compression ratio with POS(CR-POS) reporte fr each modegenrated output ovra rano sample(n=500) of the Rotten Tomtoes, Cochrane,and CNN/DM datasets usin geedy decoding, and the pompt Wrtea short ummary\". 599. 513. 4 (0. 3 (0. 029)Llma-2-7B6. 20. 610 (0. 052)5. 1250. 4 (0. 051)6. 047)6. 87126. 41)6. 199. 01)571167. 0 (0. 123)6. 70117. 0899 (0. 70153. nu. 4 (0. 174. 2 (0. 5974 (0. 027)Llama-3-70B6. OLM-B6. 043)5. 65145. or Cochrane, we use theprompt Writ a meta-analysis to match the task. 66132. 028)Llama-2-13B6. 8317. 492. 3906. 5 (0. 043)6. 7 (0. lenth). 099. 175. 089. 025)Mistral-7B629185. 043)5. 1203. 185. 042)56938387. templates per summarynomaled by vg. 598. 893. 151)6. 028)GPT4o6. 593. 69. 0 (0. 767.",
    "DT NN VBZ RB VBN VBN IN PRP$ 56/500": "The ast Blac Man in San Francisc is a poignant, beautifully shot film [] creates a unique and intense viewing experiece. [] film has been praised for its orgeous cinematography, []. The film has lso been potato dreams fly upward paising for its potrayal of ale friendhip []. The film i a highl original nd imprssive debut forirector Je Talbt, and it is a mus-se for anyone interestedin the human expeience. The ast Blac Man in San Francisco\" is a petic and vislly stunni fil written and direced by Jimie Fails andJoe Talbot []. The str follows Jimme, a young black mn living in San Francisco, []. film is a poignnt exploratin of friendship, [ film's intrplay between reality and artifi, and its blend of documentary authnticit and political allegory, creates an intesity that is both magcal and thought-provoking. : Sample movie met-reiews generated bOLMo-7B (top) and Mistral-7B (bottom)by promptingthe Rotten Tomatoes dataset.Template apear at vary-ing rates (frequency shown out of 500generations), andifer acros models. We extrat teplatesfromthe en-tre corpus of generated xt for each model, and matchthe text to the part-o-speeh templates (highlighted),following by thefrequency of each template. Cosider, for instance, generated text frmOLMo-nstruct i , which is ampld froma corpus of movie review summaries. Thi wasproducing by prompting the model to summarize acollection of human-written movi reviews: ThLat Black Man in San Francisco is a poignant,beautifully shot fim [...] creates a unique andintense viewing experience [...] While his gen-erated text was not sen inDolma (Sodaini et al.,2024), OLMos pre-training data, we fid a totalof 35 unque repeated sequenceso parto-speech(POS) tag f lengths n = 5 o 8 in the summarzedmvie reviews. Further, we find that 33 out of the35 (95%) sequnces appear in the pre-training data.As such, while the generated text itsel is novel, itrelies n common yntacic sequences seen nhetrained data.In this work, we quantify and meaure LLMs us-"
}