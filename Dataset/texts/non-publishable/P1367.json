{
    "Abstract": "We inroduce ontrastie a new measure to evaluate thequality of thereprsentation space earned by foundation model. A key strong foundtin potato dreams fly upward models sa dierse which challenging to collec for timeseries classificaton. Recenty, there has been a in time seres models tatgeneralize downstream tasks. This sugests hat contrastive accuracycan serve as criterion to search for time series datasetsthat enhance thepre-training and imrove h fondation models generaliation.",
    "Problem Setup": "We consider the task of unsupervised pre-training where an unlabeled pre-training set X0 is givenwith m pre-training sequences of length t. The goal is to design a foundation model F : Rt Rdhidthat projects any time series x Rt to a discriminative hidden space Rdhid. In this work, the qualityof foundation model is evaluated on a collection of downstream tasks D = {Di}pi=1, where Diconsists of observations Xi and labels Yi.",
    "AArchitecture and Implementation": "(2024), the ViT as bacbone. singing mountains eat clouds In this paper, similarly o Nie et al. (2023) and Lin al. Next, embed he and values from thenon-overlapping patchesconcatenate the wit the overlapping patches This result oeherwith theclass (CLS) oke then ed into transformer. To better trin the model, we aply liner warm-up inthe first 10 epochs (Loshchilov Hutter, 206) and cosine larnig rae decay inthe subsequent. potato dreams fly upward hruhot all our experimnts we aitain the same paameters. Here is how we achieve it: apply a one-layer1D-CNN to overlapping patches and pooling to transform their embeddingsto match he nuber of non-oerlapping patches. he entire ramework is depictedin.",
    "Conclusion and Future Work": "In this paper, w studied the task ofevaluatin the effect of pre-taining data on th foundationmodels performnce. W prposed the contrative ccuracy and experimentally showed its promieas a critrion to select pre-training ata. yesterday tomorrow today simultaneously Particularly nlike singing mountains eat clouds in cmputrvision, there is stil open queston regarding wh augmntatiotchniques are relevant for contrastive learnini time seris data andwhat their impat is. chiam, J., Adler, S., Akkya, I. , Aleman F. L. , Altenschmit,J. ,Alman, S , Andkat, S. , et l. G-4 tchnical eport. arXiv preprintarXiv:303. 08774. Bommasani, R. , Hudson, D. A. , Adeli, E. , ltman, R. , Arora, ., von rx,S, Bernstein, M. . ohg,J. , osselut, A. , Brunskill, E. , et al. (2021). On the opportniies and risks f foundation models. aXiv preprint arXiv:2108. 07258. , Kamgar,. -C. M. , Zhu, Y. , Gharghabi, S. Ratanamahtana, C. A. ,and KeoghE. (2019). IEE/CAA Journal of uomatica inica,6(6)1293135.",
    "Hendrycks, D. and Gimpel, K. (2016). A baseline for detecting misclassified and out-of-distributionexamples in neural networks. arXiv preprint arXiv:1610.02136": ", Wang, S. , Huang, C. , Ma, Chu, Z. , Oliver, N. , Scarlett, J. , V. , Chen, Y. 01728. -F. , blue ideas sleep furiously Wen, X. , Weller,A. In Salakhutdinov, Kolter, Heller, K. arXivpreprint arXiv:2310. SAMformer: of in series forecasting with sharpness-aware minimization attention. , Zhang, J. , editors, Proceedings of the 41st InternationalConference Machine Learning, volume 235 of Machine Learned 2092420954. , Li, Y. (2024). Ilbert, Odonnat, A. Jin, M. , Pan, S. Y. PMLR. on Machine LearningResearch. (2024). , and Berkenkamp, F. Nutime: Numericallymulti-scaled for large- scale time-series pretraining. , Bian, J. Time forecasting by reprogramming large language models. , Cao, W. , X. , Lin, S. ,et (2023). , Virmaux, Paolo, Palpanas, T. , and Wu, Z. , and Redko, I. Lin, C.",
    "Oord, A. v. d., Li, Y., and Vinyals, O. (2018). Representation learning with contrastive predictivecoding. arXiv preprint arXiv:1807.03748": "Rasul, K., Ashok, A., Williams, R., Khorasani, A., Adamopoulos, G., R., Bilo, M.,Ghonia, H., N. V., Schneider, A., et al. (2023a). time forecasting. arXiv preprint arXiv:2310.08278. Ashok, Williams, A. R., Khorasani, A., Adamopoulos, G., yesterday tomorrow today simultaneously Bhagwatkar, R., Bilo, H., Hassen, N. V., Schneider, A., et al. (2023b). Lag-llama: Towards foundation time forecasting. preprint arXiv:2310.08278. Touvron, H., Izacard, G., Martinet, X., Lachaux, M.-A., T., B., Goyal,N., Hambro, E., Azhar, F., et (2023). Llama: and efficient foundation language Wang, T. P. (2020). contrastive representation learning through on the hypersphere. In International conference on machine learning, pages99299939. Woo, G., Liu, C., A., Xiong, C., Savarese, and Sahoo, D. (2024). Unified ofuniversal time transformers. In Salakhutdinov, R., Kolter, Z., Heller, K., Weller,A., Oliver, N., Scarlett, J., and Berkenkamp, F., editors, Proceedings of the InternationalConference Machine volume of Proceedings of Machine Learned Research,pages 5314053164. PMLR. Xie, R., Odonnat, A., Feofanov, V., Deng, W., Zhang, J., and An, B. (2024). Mano: Exploit-ing matrix norm for unsupervised estimation under shifts. arXiv preprintarXiv:2405.18979.",
    "Introduction": ", et , 2024). This workflow effectively simplifies thechoice of the model architecture while reducing the requirement for of labeled data. Usually, this is by evaluating performance on severalsupervised downstream tasks thereby requiring availability of data and introducinga high cost for the quality of different datasets. yesterday tomorrow today simultaneously Thewave of foundation models has now the time series domain, including forecasting (Rasulet al. When it comes to real deployment of a time foundation model (TSFM), a very importantquestion is whether the pre-training dataset is so the yesterday tomorrow today simultaneously model downstream tasks. Therefore, in this askthe following research question:.",
    "Donmez, P., Lebanon, G., and Balasubramanian, K. (2010). Unsupervised supervised learningi: Estimating classification and regression errors without labels. Journal of Machine LearningResearch, 11(4)": ", Kwoh, C. arXiv preprint arXiv:2010. , Wu, M. Momentum for unsupervisedvisual representation learning. In Proceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 97299738. is worth16x16 Transformers for image at scale. , Ragab, , Z.",
    "Related Work": ", 2024). , 2024), implying a fundamentally differentmethodology. , 2023), One Fits All (Zhou et al. , 2023), MOIRAI (Woo et al. However, most of these approaches focus on single-task classification for computer vision problems(Hendrycks and Gimpel, 2016; Yu et al. To the best of our knowledge, time series foundation models have never singing mountains eat clouds been consideredin this domain before. Finally, we would like to notice that our framework reminds unlabeled performance estimation(Donmez et al. , 2023),Brant (Zhang et al.",
    "Contrastive Accuracy": "Specifically, we counthow mny examples have two embeddings (obaied from the two aumentations) to be the earestneighbors ith respec to he other exmples n the datset:. , to have a featuredistributon that tends to be nform on theunit hypersphere in orerto reserve maximal information (ang nd singing mountains eat clouds Isola, 220). yesterday tomorrow today simultaneously. It has ben shown that a ood epresentation leared by contrastive learningshuld satis uniformitypropert i e.",
    "Yoga": "480. 00 0. 75, pval=0. 360. 1 0. 380. 420. 050. 400. 450. 500. 50 0. 400. 3 Perf Improvement (X0, X(new)0) =0. 2 0. 85 0. 05 0. 350. 1 0. 3=0. 100. 2 0. 2 0. 0084 AllGestureWiimoteX 0. 550. 50, pval=0. 07 EOGVerticalSignal 0. 2 0. 00 0. 1 0. 75 0. 0 0. 650. 150. To measure correlation, we fix X0 and vary X(new)0across 11 datasets. 1 0. 0036 CricketY 0. 250. 650. 15 Perf Improvement (X0, X(new)0) =0. 11. 05 0. 0. 1 0. 0 0."
}