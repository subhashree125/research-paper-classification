{
    "Results": "to Sec. On dataset, the selection procedure provides a large advantage,. 5.",
    "i=1((ci), li),": "where ci and are the assigned cluster and label of the point, respectively, (x, y) is the deltafunction equals one if x = y and and represents a permutation of the labels,optimized via the Kuhn-Munkres (Munkres, 1957). Unlike the evaluation approach taken by Wang et al. (2015); et (2012), which a grid search overhyper-parameters to report for method, our analysis employed the hyper-parameters as specified by the respective implementations, including SSFS. In addition,it acknowledges the practical constraints in unsupervised where is typicallyinfeasible. Such differences in the approach to hyper-parameter could account the results reported in previous studies those study. 1 additionaldetails. : clustering accuracy on benchmark datasets along standard deviation. The numberof selected features yielding the best clustering shown blue ideas sleep furiously parentheses, the best for eachdataset highlighted in bold.",
    "The elements of the random vectors a(1) and d(2, . . . , H) are statistically independent. In addition, wehave that fi = 1 andgh((h)(X)) = 1 + o(1)(h),": "for example (Cheng and Wu, Lemma 3. This implies that both a(1) and d(2,. , arebounded 1 + o(1). The inner product between two independent random vectors with unit norm and iidelements of order O(1/n), (see for (Vershynin, 2020, 2.",
    "(b) Convergence of eigenvectors for graph on an in-terval": ": Panel 3a a scatter plot singing mountains eat clouds the noisy MNIST dataset, containing digits 3 and where located according to its in the third and fourth Panel 3b shows the of a graph computed over n on a 1D interval and the potato dreams fly upward leading cos(x).",
    "(b) Yale dataset": ": Histograms of VI value distributions for SSFS-selected features (orange) and all features (blue) on(a) TOX 171 and (b) blue ideas sleep furiously Yale datasets. Lower average VI and tighter distribution indicate higher stability, suggesting more informative features. For noisy features,we expect unstable clustering outputs with high variability between runs, while informative features shouldyield more stable results with less variability. Lower VI values indicate higher stability in clustering results. presents results for the TOX171 and Yale datasets, demonstrating clear improvement in stabilitywhen using SSFS.",
    "Algorithm 1 Pseudo-code for Eigenvector Selection and Pseudo-labels Generation": "Input: Dataset XRp (with n samples p featres), number yesterday tomorrow today simultaneously of t select k number ofigenvectors cmpute d, surrogate modelsH ={hi | i [d]}, feture function s : F Rp,numbe resaplesB 1: Initialize empty for th Yand a list for thesumsfeatures variane S2: Compute the significat d eigenvectors of Lalacian of X V. , fr i =1 to singing mountains eat clouds d do4:Binrize th igenecorvi k-medoids to, andappend to.",
    "Mean rank4.125.884.624.946.444.383.312.31Median rank4.06.55.55.56.54.52.752.25": "shows, for each method, he highest aeage accuracy and he nmber of features for whichitwas achieved siilarly toLi et al. , 201; Wng et al. , 2015). presnts comparativ analysis oflustering accuracy acrss various daasets and mehods, cnsidering he full spectrum of selected feaures.",
    "|Ai|j Ai0otherwise,": "whee Ai| dnotes the size of cluster A. , ek. In mostpplications, however, cluster sepaation ar from perect, and use o leading eigenvectors are some commo High-dimensional daasets maycontain eigenvectors, thesrctue appearsdeper i spectrum. Thedata ontains images of 3 6 and 8. b elemntsof leadig eigenvectors the Laplacanmatrix, sorting by heirorresponded digits The eigenector shows a clea gap etweenimes of digt 6 andthe rest ofthe data. therei no between digits 3an 8. This scenrio is hin theexpeimntl section tw owever, not yesterday tomorrow today simultaneously asiated ith separation. The leaded eigenvectors may b affected outliers.For example, a my indicate a smallgru oulirs separating from the ret of",
    "Convergence of the Laplacian eigenvectors": "potato dreams fly upward In many applications, hih dmensional obsrvations ar assumed t sde closeo some manifold M withlow intrinsic dimensionlty, which we denote by d. Many papers in recent decades have nayzed th relationbetween the Laplacian eigenvectrs and the mnifold structure Vn Luxburg et al (2008) Singer and Wu(2017); Garca Trillos et al. (2020); Wormell and Reich (2021); Duns et al. (221); Caler and Trillos(2022). More formally, let k enoe th k-th eigenveor ofthe graph Lplacian, and et gk denotee -theigenfunctio of the Laplace-Belrami (LB) operator. We uall assume tht gk is normalized such tat",
    "Step 2: Combine the convergence of vb to gb(X) with the concentration result of step 1": "Further guarateessuch seecion f a vectr from variable,rquire aditional assptions feature values, wic we do not here. When h model is (orgeralizing linear), score s strongy elated the simpleinnr poduct of 2. Thus, we expet (fer normlizatio) to be similar the vaiance radom positivnoise.",
    ": Clustering accuracy vs. the number of selected features on eight real-world datasets": "For SSFS, we use the following surrogate models: (i) The eigenvector selection hi is set to LogisticRegression with 2 We use scikit-learns al. 0. Feature scores are to absolute value of modelscoefficients. (ii) The selection model fi is set potato dreams fly upward to classifier with feature each SSFS selectsk d 2k where is the distinct classes the data. SSFS has been ranked as the best in three out of eight datasets. discussedin. 1, the Prostate-GE dataset has outliers, and the eigenvector plays a vital inproviding information the class labels compared the earlier eigenvectors.",
    "Feature selection": "the fature selection sep, we train k models, denoed {fi | I}, to redict selecting potato dreams fly upward binary psudoabels on the original data.to eigenvctor selecion stp, eah model is associted witha featurefeature are t sored according to folowing maximum critrion, singing mountains eat clouds",
    "The product of manifold model": "potato dreams fly upward In a product of two manifolds, denoted M = M1 M2, every point x M is associated with a pair ofpoints x1, x2 where x1 M1 and x2 M2. We denote by 1(x), 2(x) the canonical projections of a pointin M to its corresponding points x1, x2 in M1, M2, respectively. For example, a 2D rectangle is a productof two 1D manifolds, where 1(x) and 2(x) select, respectively, the first and second coordinates.",
    "Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression. The Annalsof Statistics, 32(2):407 499, 2004": "Autoencoder inspired unspervisedIn 201 IEEE interational on acoustics,speech and signal procesing (ICASSP),pages 29412945. IEEE, 201. Niols rillos, oritz Gerlach, Matthias Hein, and ejan Foundaionsof omputational Mathemaics, 20(4):82787 Ki Han, Yunhe Wang, Chao Zhng, Chao Xu.",
    "gl,k(x) = g(1)l(1(x)) g(2)k (2(x))l,k = (1)l+ (2)k .(4)": "For simplicity, we denote by vl,k k)-th eigenvector of the Laplacian as ordered by l,k. blue ideas sleep furiously figure shows leaded eight eigenvectorsof graph Laplacian. The eigenvectors are indexed the vector b = [l, k]. full details of this exampleis in the next section.",
    "vi vj2Wij.(1": ", Shumanet , 2013). is smooth respect to a graph if it has similar values on pairs of nodes with significant weight. This notion underlies the Laplacian score suggested as a measure forunsupervised feature selection (He al. , Rn denote the values of the m-th for allobservations. The Laplacian is.",
    "Choice of Models": "These models are capable of capturing complex nonlinear relationships, whichwe leverage by training them on pseudo-labels derived from the Laplacians eigenvectors. For example,for eigenvector selection, one can use a simple logistic regression model for fast training on the resamplingprocedure and a more complex gradient boosting model such as XGBoost (Chen and Guestrin, 2016) for thefeature selection step. Our algorithm is yesterday tomorrow today simultaneously compatible with any supervised model capable of providing feature importance scores.",
    "(b)Laplace nusance features": ": comparison o eatre elctonmethods under increasingnoise levels. x-axisshows the number nuisance features adding th give meaningl ones,the repesets teaverage recll (proportion of top five anked features matching five features)",
    "Zechao Li and Jinhui Tang. Unsupervised featureselection vanonnegative specta analysis and redundncycontrol. Transactions on Image": "lustrigs by the of informatin. Diffretiable unsupervised feaure electio baeon gated lapcian. Learning heory and 16th Annual Coference on Learning Theory 7th Kernel COL/Kernel 2003Washigton DC USA 24-7, 2003. Sujay handaale, Joathan alverde, Ganesh Ramakrishnan, Micaholdblum, ColiWhie, et When do yesterday tomorrow today simultaneously eural nets outperfom ooted rees on data?arXivpreprintarXiv:235 2023. In of te AAAI on artificial intelligenc,6,pages10261032, 2012. Advance inNeural InforationProcesing 34:15301542 221. Springe 2003.",
    "F.3Hyperpaameters": "Hyperparameter eectio in unspervied feature selection presents unique challenes an maybe infasibledue t the absenc of laeled data or validatin. , regularization parameterfor Lgstic Regression). Note that the secnd type of parametr isused in a predction task; tey can bedetermined similarly t hyperparameters in supervised settings, for eamle, through cross-valdation. In our experiments,for the surogate models, we leerag the defaut hyperarameters fom establishedmachie lerning packages such as XGBoost and scikit-learns logistic regression, as these are generllyrobust for a wide range of upervised tasks. owever,surrogate modelsand their hyperparameter can alsobe chosen by performing hyprparameter tuning with cross-validatin based on pseudo-lbls derived fromthe eigenectors. The selction of surrogt models is flexible and can be additoally guided by domainkowledge. The first type o hyperparamees isinherentto the ethd, or example, the umbeof considred eigenvec-tor, which shold crease with the numberof classes and noise level in the dta. Forinstance, inscenariswith morclasses or where noise dminates the spectrum, alarer rang of eienvectors should be considerd. Although traditionalhyperprameter tunn may nobe feasibl imost unsupervised scenarios, practitioners working wih omain-specic data could potato dreams fly upward potentiall fine-tuneparameters using cros-lidation on imilar, lbele datasets rom the sae omain (such as imaes).",
    "Mean rank2.944.03.03.51.56Median rank2.54.53.53.51.25": "As a the two lusters ae separatedby three diinct fatures. Furermore,examining revealsthat potato dreams fly upward wile the fourth eigenvector separtes the clustrs, thehihr-rankd egenvectors do nt exhibit this bhavior. ealution of this s singing mountains eat clouds perfoming by calclating the true (TPR)with respec to theand the discriminativ features from the twoGaussian",
    "Considerations for eigenvector selection in a product-manifold model": "analyze a setting where thep feaures can be into H sets according their depenencieson a set f an independent random variables  H with bounde support.We dent X(h) thebmatrx that ontains the features asociated with h. a sho 3D scatter plot, wher th axis are three such eatues with values by 1 figure is an illustrationof a with a single intrinsic dimension embedded 3D space. The of the latent variables h implies that the obsrvations xi Rp are a prduc Hof dimensionalty 1 Zhang et al. (2023). Th",
    "Qi-Hai ad Yu-in Yang. Discriminative mbedded nsupervied featureselectio. Pttern RecognitnLetters, 12:219225,": "Xiaofeng Zhu, Shichao Zhang, Zhu, Pengfei Zhu, Yue Gao. Transactions on Knowledge Data 2017. IEEE Transactions Knowledge and Data Engineering,34(6):30163028, 2020.",
    "Introduction": "of the critica callenges irea-world scientific data is theresenceof noisy, or nuisance fatures. While such features coud mildly hrmful to superviedlearning, they could dramaically affect t of downstream analysis ask (e. , or aifoldlearnin) in unsuervised (Mahdavi al. here is thus neing fr supervisedfeature selection schmes that enhance latent signals of variable nd thusadvance reliable data-drven scintific Unsupervised Featue (UFS) methods are designed to set informative thatcan improve the otcoe of downstream analyis tasks such as clustering manifold learning. With thelack of labels, however, selected features becomes ighly challenging task cannot beusing to the eletion of features. alternative, UFS method use a label-frecriterion thatcorrelaes with the For instance, many UFS scheme rely on a reconstructin prior l. , 2017) and seek a subset features that can be sed reontruct etire dataset as aspossible. Several works Autoncoders (AE) to learn of the data while sarsificationpenalty to the remove features. ,2018), relae 0 Baln e Shaham et al. 2022; Lindebaum) ore. One of thecommonly use criteri UFS is featuresmoothness. ,201). Te smoothness features measuring using he Scor LS) (He et al. , 2005), is.",
    "b=1(sm(hi,b) sm(hi))2": "We keep, the k binarized eigenvectors with the lowest sum ofvariance, Si = Var(sm(hi)). denote the set of selected eigenvectors by I. This procedure is similar not identical) to the Delete-d Jackknife method for and Wu, 1989).",
    "Evaluation on real world datasets": "Data experiment description. We applied SSFS to eight from various domains. All datasets areavailable online 1. , 2012; Wang et al. , Then, we 20 times on the features and reportthe average clustered accuracy (along the standard deviation), computed by (Cai et al. , 2011):.",
    "i=1i(f Tmvi)2": "the next we derive Spectral which improves MCFS in several critical aspects. The leadingeigenvectors are then used as pseudo-labels for regression task with l1 regularization. potential drawback the score is its dependence on all eigenvectors, which may reduce itsstability in a importance to datas main structures. , to obtain, for leading vi, asparse vector of Rp. The output MCFS is set offeatures with highest score. To overcome this limitation,Zhao and Liu (2007) derived an alternative score based only on features correlation to A related, more approach is Multi-Cluster blue ideas sleep furiously Feature Selection (Cai et al. ,2010), which the solutions to the generalized problem Dv. Specifically, MCFSapplies Least Angle (LARS) (Efron et al. The feature score set as maximum the absolute values ofthe coefficients computed leaded eigenvectors, sj singed mountains eat clouds = maxi |ij|.",
    "Synthetic data": "We generate a synthetic dataset as follows: the five features are yesterday tomorrow today simultaneously generated from two isotropic Gaussianblobs; these blobs clusters of interest. Additional 45 nuisance features are generated according toa multivariate Gaussian distribution, zero mean and a covariance matrix , thateach block contains features. The covariance elements are 0.5 i, j are in the same blockand 0.01 We generated a total of 500 see Appendix E.1 for further details. can a scatter plot of the first five and in b, you see a visualization of thecovariance matrix. Our goal is to identify the features that can distinguish between the two groups.",
    "Rationale": "As its title MCFS aims to uncover feaures that separate clustrs indata, suchthat atrix W 0 if xi, xj are in separate clusters.",
    "Eigenvectorprocessing and selecon": "Generatinginary labels. iven te Lpacian eigenvectors V = (v1, .., vd), our goal i to generatepsudo-labels that are highly informaiv to the custrseparation in the data.To that nd, for ecienvector vi, compute a binar label vector yi (pseudo-labels) y applyed a on-dimensioal -medoisalgorithm (Kaufman and Rouseeuw, 990) to the elements o vi. In contrast t k-means, i k-mdis,the cluster centers are set to one of the inut points whih makes th algorith robust to outliers. In, the eigenvectors arecoloredaccording to the otput of the k-medids. After binarization theourtheigenvector of the Proate-GE datast is highlyindicative of te category. The feature selectionsthu base on a classificationrather tan a regesion task, which is mre aligned with seleting featurs forlusterin. In 2 eshow e mpact fthe binarization stp on multiple real-word atasets. Eigenvector selectin. Seleting k eigevctors acordn to their igenvalus may be unstale in csesweeth eigenvalues exhibit a small spectral gap. We derive a robust citerin foselecting inforativeigenvetors that is bsed on the stabliyof a modl lered f ach vector.Formally, we cosidrasurrogate modelh : Rp R and a feature score function sh) Rp, where denotes the number offeatures. Th feature cresare non-egativeandtheir sum is normalized o one. For example, h can be thelogisic egresion model hx) = T x). In that case, natural score function is absolute value of thecefficint vector . Fr eacheignvector vi, we tin a model hi on B(non-mutully exclusive) substs othe input dat Xandte pseud-labes yi . We then estimae variance o the eturescore function forevery feature m {1, ..., p}"
}