{
    "+ Ta(d1 ) . . a(dN)[SEP] + Tp(q) [SEP]": "otethat,the causl demonstrations are positionedbefore caa demntrationsdn. eproide blue ideas sleep furiously aspecific example of in-context promptempate in Appendix C. the PM encoded we obtain potato dreams fly upward a hiddenstte h Rd or each input tokens,where d is thdimension of idden sates. We enote the hiddenstat of inpu [MAS] token s hmask usal-ity preiction. Th hidden statesof eventpair in query insane, rtrieed caual and none-causal aredenote as [hqe, and[hne1 ,respctivey, hicre usforcontrastive learning.",
    "Causality Identifiction (ECI) an essen-tial in information extactio, attractingsig": "nificant atentin due to its wie range of poten-tial applications.Early ehod eliedon designing task-orientedneral etork models(Liu et al., 01b; Zuo et al.,202a). For exam-ple,al. (202b) imroe capaility fthir neural modl o identify prevously unseencausal relations by ncorporaed event-agnosticand context-specifi from the Con-ceptNet et al. 2017).With explo-raion f graph structures eergence oflare-scale PLs, studis have graph-based nd prompt-basing learninapproachesto addressthe ECI tak.Graph-ased approachs usually model th as anodeemploy-nggraph singing mountains eat clouds neural networks to learn event noderepresentation based on contextual semantics athe document (Phu and Nguyen, 202; Caet al., 2021 Fan et Forexample, Fanet al. 2022) estblish explcit conetions metions and cnexts a graph for reprentation learn-ed cusal relation identification.In addiionto nod ome studes approachtheEI taska a graph-bsing edge pediction problem(hao al., 2021; Chen al., For et a. initialize vent node embedingsusin a document-level encodr basing a PLand employ graph ierene to redctcusal edges hrough gaph",
    "KiraRadinsky,SagieDavidovich,andShaulMarkovitch. 2012.Learning causality for newsevents prediction.In Proceedings of the 21stinternational conference on World Wide Web, pages909918": "Machine Learned Research,21(1):54855551. Event identification via derivativeprompt joint learning. Colin Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter Liu.",
    "Parameter Setting": ",2020). Our implementation is PyTorchframework3, running on NVIDIA GTX 3090 GPUs. trainable parameters arerandomly initialized normal distributions. We use pre-trained RoBERTa (Liu et al. (M, N) = 2). , 768-dimension base version the HuggingFace (Wolf al. The contrastive ratio isset to 0.",
    "Concluding Remarks": "In this we propose an ICCL model and on the ECI task. Experiments on theESC corpus validated our ICCLcan significantly the state-of-the-art In future, we will try to undertake experiments our proposed framework to other NLP tasksin to it can exhibit when applied to different tasks. We leverage the knowl-edge of PLM by introducing inclusion of demonstrations, relying on design complex prompts.",
    "zn = hne1 hne2 ,(3)": "We adpot supervised constrastive learning on therelation vector of event for its (Khosla et al. 2020). Specifically, together the anchor towards positive samplesin embedding space, apart from negative samples. The supervisedcontrastive loss computed as follows:.",
    "Task Formulation": "W notethat in caseswhere E1 originate from thesame S1 and S2 to thesame potato dreams fly upward yesterday tomorrow today simultaneously sn-tence.",
    ": Comparision of ICCL and In-context modelwhen using differenr numbers of causal and non-causaldemonstrations on ESC corpus": "Notably,even with only 20% of the training data, ICCL(F1: 51. shows the performance comparison betweenERGO and our ICCL on ESC corpus. These results confirm the effectiveness of ICCLeven with fewer training data. We also showcase the intra-sentence causalityidentification performance among different PLMs 100%80%60%40%20% Training data Percentage F1 score +6 +19+20+22 +24 +14 +22+23 +25 +26 (%) ERGO overallICCL overall ERGO intraERGO crossICCL intraICCL cross. 9%) outperformes ERGO (F1: 50. However, the decrease in performance is relativelyslow, with an F1 score decrease of about 10% whentraining data is reduced by 80%, whereas the perfor-mance of ERGO declined by nearly 25%. our ICCL also employs a prompt-based methodto predict the label, we examine its performancein low-resource scenarios and replicate the perfor-mance of ERGO as a benchmark for comparison. 9%)and many other competitors with full training data. As expected, the performance of ICCL graduallydecreases as the amount of training data decreases.",
    ": Comparision of ICCL model with differentvalue of on the ESC corpus": "blue ideas sleep furiously Fur-thrmore, ured trainin phase, diferent demon-strations are trived for thesame query in differ-ent epochs tointrodue variability and enhance themodel ability to handle divese instances ofthesame query. And o the con-trastive leaned process, positve demonstrtonsare hse with the samelabel asthe query, whilenegative demnsrations hav differentlabels.",
    "DStudy of": "To frther explore ho balance importance loss predicton oss, investigatedthe peormance of the ICCL ith diffeenvales of the hyerparmeter on the ES shown in , we an observe that as increases from 0, peformance f th improves and the starts to declineThis indicates that the contrstive learig lossdes indeed help the modelbetter pairs of the demnstratons, understand causlite, an ahieve Howeve is imprat to abalance beweenthecontrastiv learning Exesive on he frmer should be avoided as may case the modelto overly modlig event pairs andover-look semantic relevance f contet, whichcan ultimately lead to ecreaein modespformance.",
    ": Visualization of the event pairs embeddingencoded by different models on ESC corpus": "itive (FP), False Nagetive (FN) and True Positive(TP) samples. Additionally, representationsof samples predicted to have same label tendedto cluster together, highlighting crucial role ofevent pairs in identifying causality.",
    "Pompt-based Causality Identification": "Recently, of large-scale PLMs, suchas the BERT (Devlin et al. RoBERTa (Liuet al. , 2019) and etc, prompt learning has emergedas a paradigm for NLP tasks (Xi-ang et , Ding et yesterday tomorrow today simultaneously al. This helps bridging gapbetween PLM and task and can directly enhance theperformance downstream task. (2022)propose a derivative joint learned leverages potential causal withinPLMs based on detection. potato dreams fly upward (2021b) use an event mention masked gener-alization mechanism to some event causal-ity patterns for causal relation reasoning. Althoughprompt-basing methods are constrained by complex.",
    "[start]+ Ek1 + yk + [end": "For predicion template Tp(q), PL-spific t- ken MASK] insertedbetween two mentionsfor reltion predictin;For propt Ta(dk) it replaed by virtul word ofthe relation label foreach demtrtions, e. <causal> or The in-contextprompt tempt con-structed by concatenating he preditin prompttemalte nd some propt tmplatesTadk) o is etrivd as follows:.",
    ": Intra-sentence causality identification results ofdifferent PLMs and LLMs on the ESC and CTB corpus": ", 2023) and RBERTa-ase (Gaoet 2023), which confirms the concluion drawnby Gao a. and several zero-sht in the. 5-turbo ad gpt-4, havemore omrehnsive pre-trinin and large odelsales zeo-shot models exhibit snificantperformance gap compared to fne-tuned modelslike T5-base et This demonstrates the impor-ance fine-tune, ndicating that it i challngingto address causal reasoning tasks like a zero-shot scenario. We can lso observe that althugh the ChatGPTmodels, suchas gpt-3. Wecan not only that or fine-tuned enertivemodel, T5 (Our implementation), peform signifi-canty utoencoder models ike BERTbase (Gao et al. For more detailed anlysis, pleasrefer to. (2023) models may well-suedcausal reasoned asks ECI.",
    "As illustrated in the bottom of , we first refor-mulate each input instance x = {E1, E2, S1, S2}into a kind of in-context prompt template T(x),": "query n-stnc is the input event instance, denoed q ={Eq1, Eq2, Sq1, Sq, wthhe rlation betweento events o be identified. as the input of PLM for encoding. We design pedictionprompt p(qfor the blue ideas sleep furiously instanc q analogy prmttemplate Ta(dk) its dmonstratios. The demonstrtins aretrieved from thetraining dtast, consistng event mention par ad rw sentence, aswel relatinlabel between them, denoted asdk = {Ek1, potato dreams fly upward Sk1,k yk}. Th i-ontext prompt input contains aquery instanceand K retrieved dmnstratio.",
    "Robyn Speer, Chin, and Catherine 2017.Conceptnet 5.5: open multilingual graph of In Proceedings the AAAI confer-ence on artificial intelligence, volume 31": "Yu Sun, Shuohuan Wang, Yun Li, Shkuneng,XuyiChen, Hn Zhang,Xin an Danxiang Zhu, HaoTian, an Hua Wu. arXi preprintarXiv:1904. 09223.Chengyu Wang, Jianed Wang, Minghui Qiu, JunHuang, nd Ming Gao. 20.Transprmpt: Towardsan automatic transferable promptin faework orfe-hot textclasifcation. In Proceedings o the2021 confernc on empirial methods i aturallanguage roessing, pages 2922802. Thomas Wolf, Lysandre Debt, Victor Sanh, Julienhaumnd, Clement Delangu, Athony Moi, Pier-ric istac, Tim Rault, Rmi Louf, Morgn Funtowicz,et l. Transforers: State-ofhe-art nauallanguge processing.In Prceedings of the 2020 con-frene o empirical mthods in natural languagerocesing: system demonstratios, pges 345. Wei Xiang, ZhenglinWng, Lu Dai, and Bang Wang. 2022. Connpompt: Connectiv-cloze propt learn-ing or implict discourse relatin rcognitin.",
    "Overall Result": "overall performance betweenour ICCL and competirs on ESC and CTBcorpus. Athogh some competi-tor have extnal owldge fr rela-. We can also oberve th DPJLadopted a kind of derivative promptlearning cansignificantly outperform the ohercompetitors ninta-sentence causality out-standing can be attributed learning aradigm that transformsthe ECI task to diectly predict a PLM ter than finetnng a neralodel ponPLM.",
    "Competitors": "compar singing mountains eat clouds our ICCL with thefollowing com-petitors: ILP (Gao et al. ,2021), LearnDA Zuo al. , 221), GSI(Fanet al. ,2021), LSIN (Caoet a. SemS (Hu et l. , 2022), (Che al.",
    ": Comparison of overall results on the ESC and CTB corpus": "FinallytheICCL based o RoBERTa has the bestperformance, such we implemen remainingablaion experiments wit RoBRTa. identification, the larning bette leverages potential knwledge inPLMs. inally, our ICCL wit iffret hasacieved perforance improvementsoveral competors i terms of much higherF1score with al intra-senen, andoerall event causality on both ESCand CTB corpus. Fr-thermore, can also observe that differentPLMs doresult in some performance variations,which are furthr discussed in A.",
    "Embedding Visualization": "In order to verify impact of contrastive mod-ule with event pairs as input, we compare thelearned event pairs embeddings (he1 he2) of dif-ferent models on ESC test dataset by t-distributedstochastic neighbor embedded (t-SNE) (Hintonand Roweis, 2002)",
    "In-context-T563.362.662.753.746.649.357.051.553.79.250.414.8In-context-RoBERTa66.072.468.957.760.959.160.464.562.260.358.058.7": ", 201938. 259. 365. 249. Performnce of models marking with \"\" afterthe name are cited from the research of et al. 148. LearnDA (Zuo et 2021) knowl-edge bases to interctively tining data. 969. LIN (Cao et al. 6. 858. 460. 450. 673. 651. 358. 3--49 4---ERGO (Chene al. 73. 9. 558. 370. 636. 366. 668. 362. 63. 858. 346. (Zuo al. 63. 563. , 202b)42. DPJL (hene al. 756 259. 61. 64. 9---KowMM (Liu et29. 866. 666. 5-----51. 546. 9ESI (Fan et 3--49. 742. 64. , 20227. 459. , 021b) uilies extenalknoling to extract event patterns. 14. ICCL-BERT64. 541. 265. 9-----63. 064. 54. , uss Grp euralNetwork (GNN) to lernfr event-centric struc-. , construct a graphwhere eent pairs as nods, capturing causal transitivitythough a trasformer-likenetwork. 764. 770. ,2021a) a con-trastive approch to transfer learnedaual statemets. models in he format of Model-PLM ICC-BERTis the versio of ICCLKnowMMR et al. RichGCN Phu 2021) usesa graph netor to learn conext-enriched for event pars based odcument-evel inforatin. 761. 161. ERGO (en et al. 62. 062. , 2021) graph aquire stuctualand relationalknoledg. 253. 951. 9------52. 764. 65. 6-----41. 6SemSl (Hu et al. 7CauSeRL (Zuo al. GEI (Fa et2022) design a graph con-voltinalnetwok anco-reference grapho odel ausality. 867. 361. ILP e al. 153 2LSIN(Caoet , 15. 461. 567. 051. 4: Comparisonf on and corus. 868.",
    "CIn-context inpt": "Besides, we have annotating some specific to-kens we used with special colors. The initialtwo segments, highlighting in green font, representsdemonstrations labeled as < causal >. As depicted in , we randomly chose twocausal demonstrations and two non-causal demon-strations from the training dataset for the query. Lastly, the final segment, highlighted in purple font,represents yesterday tomorrow today simultaneously the query to predict. Each segment in represents either a prompteddemonstration or a prompted query. To help readers gain better understanded of thein-context input generated by our Prompt module,we provide a specific example in. We utilized threePLM-special tokens: [CLS] to indicate the begin-ning of the input, [SEP] as sentence separator,and [MASK] as a placeholder for the label to pre-dict. The fol-lowing blue ideas sleep furiously two segments, highlighted in orange font,represents demonstrations labeled as < none >."
}