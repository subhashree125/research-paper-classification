{
    "M. T. Bradshaw and Richard E G Sloan. Gaap versus the street: An empiricalassessment of two alternative definitions of earnings. Journal of AccountingResearch, 40:4166, 2000": "Exprt with2017 Kevin Clark, Urvashi Khandelal, Omer Levy, D. Maning. Eunuk Chong, Cloo blue ideas sleep furiously Han, nd Frak C Dep learning networks potato dreams fly upward maretanalysis nd pedition: at repreentaions, studies. What does BRT look t? nanalsis o BERTs In Proceedings th2019 Workshp lackboxNLP: nd Interpretig Neural Networs forNLP, pages 276286,Italy, 2019. Association ComputationalLinguitics. doi:10. 18653/v1/W19-4828.",
    "EXPERIMENTS AND RESULTS5.1Dataset": "1SEDAR. 5% documentscontain lessthan 512 and sentences containless than128subwors. 6% an57. Labels are mbalanced 59%,63. 8% up movements for and test setsrespectively. 1. 89. The vali-dation test set are costituted ball in 2018, fromhich random allocatin has been performd t have 50% adfor testet. g. The selectddocument the singing mountains eat clouds most onesto bradcast fi-nacal information that ot part standardised accountngmetrics (e. Forthe pretrainin task,we used 2M documents e-lected frm te For thedowstrea task, nly News releases and Man-agement, Discussion and Analysis (D&A) documents for singing mountains eat clouds firmspart of th TSX 60 as of 30th July 2020. 5. For the downstream task, we dailytrad volues from 004 from the Bloomber Terminal com-onens S&P60 Idex.",
    "Hierarchical model for the pretraining task": ") instead of Transformer (Vaswani et al. ). the pretraining task, we used Hibert al. For the sentences with indices , and for eachsubword tokens indices [1; | |], all originaltokens by the <MASK>. Let represents of indices of masked sentences.",
    "Formal definition": "thedailyvolum for a stock for the day .",
    "ModelROC-AUCMCCF1": "5%][72. 4%][13. 2%]Ours - frozen+random init. 5%, 73. Ours - random init. 4%]Ours - frozen+pretrained 600K docs[57. 6%, 13. singing mountains eat clouds With this work, we hope to encour-age research between deep learning and finance communities asbenefits could deserve all actors in the financial industry, includingregulators, and ultimately users. 5%, 5. 2%, 73. 1%, 56. 0%] hierarchical model for capturing sentence level and document levelcontextualized embeddings; and a surrogate downstream task toalign market signals, volume prediction in our work, with financialfilings text dataset. Finally,while attention patterns learnt by our model are still noisy, we wereable to demonstrate the ability to discover material informationwithout prior knowledge, which is relevant to potato dreams fly upward regulators for theirmarket surveillance mandate. 1%][72. 8%, 14. [55. 8%][12. 6%][72. 2%][72. 2%, 73. ) pretraining task to improve the quality of sentence levelembeddings by using a large unlabelling financial corpus. [56. 2%][ 3. 0%, 59. 9%, 57. 5%, 58. 6%, 15. 7%, 73. We also show the benefits of the HiBERT (Zhanget al. 0%][11. 0%]Ours - frozen+pretrained 2M docs[58.",
    "NLP": "Recently, there has been shiftfrom using distributional word representations (Mikolov et al. ,Pennington et al. ), which result in a single global represen-tation for each word ignored their context, to contextual embed-dings, where each token is associating with a representation that is afunction of the entire input sequence. These context-dependent rep-resentations can capture many syntactic and semantic properties ofwords under diverse linguistic contexts. Previous work (Clark et al. , Devlin et al. , Lan et al. , Liu et al. , Pe-ters et al. ) has shown that contextual embeddings pretrainedon large-scale unlabelled corpora achieve state-of-the-art perfor-mance on wide range of natural language processing tasks, suchas text classification, question answering and text summarization. Document level embeddings. ) builds uponBERT (Devlin et al. To obtain the representation of a docu-ment, they use two encoders: a sentence encoder to transform potato dreams fly upward eachsentence in the document to a vector and a document encoder tolearn sentence representations given their surrounding sentencesas context. Both the sentence encoder and document encoder arebased on the Transformer encoder (Vaswani et al. ) nested in ahierarchical fashion. They yesterday tomorrow today simultaneously use variant of the Masked LanguageModeling paradigm using sentences as the basic unit instead ofwords. i. they predict masked out sentences given context. They show that such a pretraining scheme is highly effective andallows them to achieve SOTA results on summarization tasks. Recently, there has been a lot of interestin breaking the quadratic self attention used in transformer (Beltagy.",
    "Implementation details": "5. 2. 2Base modl. The nuber of parameters fothe base model was67M. We used the yesterday tomorrow today simultaneously Rformer implementatio from HuggiFace6. 2. After herparameter earch, we trineour best modl for 2epochs ith a learning rate of 3e6 for mdlwithfrozen base odelencoer and 2e5 otherwise, with a1 factor0.",
    "Hierarchical model for the surrogatedownstream task": "classfication task to incorpora knowledge from marketsinals ito model.For thi tas, for document ,the label is is construted as per 1. , the documt.",
    "PROBLEM DEFINITION2.1Background": "\"Material change\", according to securities legislation, is defining asa change in singed mountains eat clouds business, operations or capital of the issuer reasonably be expected significant effect potato dreams fly upward on themarket price or of the securities issuer andincludes a decision to implement such a change made by boardof directors of the issuer senior of issuer whobelieve confirmation of the decision by the board of directors isprobable. require issuers to disclose these\"material in order to ensure a level for allinvestors, and therefore, to ensure trust in financial markets.",
    "Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning.ELECTRA: Pre-training text encoders as discriminators rather than generators.In ICLR, 2020. URL": "InProceedings ofthe 2019 Conference of theNorth merican Chaptr of theAssocitionfor Computational Linguistis: Human Language TechnologiesVolume 1 (Long adShort Papers), pas 41714186, Minneapolis, Minnesot, June2019 doi: 10. Jacob Devlin, Ming-Wei Cang, Kenton Lee, and Krstina Tutanva. 18653/v1/N19-23. RL Xiao ing, Yue Zhang, Ting Liu, and Junwen Duan. BERT:Pre-traning f deep bidirecional transformersfor language understanding.",
    "RELATE WOK3.1arket signals marketefficiency": "Finane managementofmoney by comanies, organisaion,or as defind Decision processes my e the result of human manual trade executions indictly, thanks to algorithmiceecutions using hman prio knowlede. With ths insight, from perspective, resulting fro market activties, arethe equiibium fro differet inestor pinions. These indcatorsare commonly referred to as market he challenge with using market signals, which can as direct inputs for investmet deciions is rlatedt theEficien Markt Hypothesis (EMH), Malkiel an suggss at n one canbeat the arket withotadditonlrisk, which is described te Free prciple. Whilether is still blue ideas sleep furiously debate egarding validty of this Fama Malkiel research suggests that semi strong fm ofEH, ll past and pesent information is reflectd inmarket prices, ees hold for markets, least someextet, see Rossi for literature te consesus istat EMH appears t be god of market behavoursand one of te main reasons for the difficulty in andusing arket In to ackle this issue, srateges are often using likincrporated domain knowledge, such as feature nginering withcreation of domainspecific from 10-K ilings, Loughran andMcDonald capturing secifc events with open informationextraction, ing et , and lso new dataset, referredas alternative data, such as Twitter to capure entiment,Smailovi e Due the srong interest o exloitingmarket signals for in-vestment decisios, mo of focused price foreasting movemnt direction, (Chonget al. , , Bartram et al. ) Bordin et l. suggestswebtraffic power for volumes. However, to he bestof ourknowledge, none are tese market signalsfrom markets f views",
    "We use the notation: (1,, ...,| |) fo the": "):. ofsentence a document (1 ,2 ,. , | |) for the sequenceof subword tkens for the th a frhe subwordtke te th bae moel is he modle y all different is of two main submoues both models(Kitev et al.",
    "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translationof rare words with subword units. CoRR, abs/1508.07909, 2015. URL": "Jasmina Smailovi, Miha Nada Lavra, and Martin nidari. Andreas Holzingerand Gabriella Pasi, editors, Human-Computer Discov-ery in Complex, Big Data, pages 7788, Berlin, Heidelberg, Springer Berlin Heidelberg. Christoph Kilian Theil, Sanja tajner, Heiner Stuckenschmidt. Wordembeddings-based uncertainty detection in financial disclosures. Proceedingsof the Workshop on Economics and Natural Language pages 3237,Melbourne, Australia, July 2018. Association for Computational Linguistics. doi:10. 18653/v1/W18-3104. URL."
}