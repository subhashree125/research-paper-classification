{
    ") W Lz L1 +b L(1)": "p(x) is the as the periodic sin(0x) , the non-periodicGaussian function and the Wavalet functionej0xe|s0x|2 , p {0, s0, (0, s0),. l denotes the of layer l, = {W l,b l l =1, , L} are the parameters to be optimized, Lis of layers.",
    "Pipeline of INRs": "Givn a squence xi yi re-spctively represet the coordinate the correspondingattributes the element, N s numberof elementsin focuses on pursuing a neural ntworkf(x; ) to characerzehe attributes as accurate as posibleMostly, the multi-layer perceptron (MLP) network used the f(x; ) which could be formlatd follows",
    "/n,": "6/n], where nis the number of inputs for",
    "Neural Tangent Kernel Perspective": "Generally speaking,stronger diagonal property in better shift-invarianceand better convergence, larger eigenvalues leads tofaster convergence for components ,. e. , the be learning has input and 1D output andFINER++ has 1 hidden layer with n such a be written as f(x; ) nk=1 ck(wkx + bk), where(x) = 1)x) activation function.",
    "FINER++:VARIABLE-PERIODICACTIVATIONFUNCTIONS FOR INRS": "In this section, w will first introduce aunfied framework to extendexisting periodic/non-periodicfunctions to heir variable-eiodic versions, followed by theproposed initialization schee. Subsequently,w nalysehe behavors of FINER++s suppoting frequency set andtraining dynamics uner diferent nitialization ranges frombotgeometric and neural * \"S tangen ernel perspectives",
    "INTRODUCTIONT": "HE way a signal is represented is the foundation of allthe following problems and determines paradigmfor solving them. representations, such as theimage matrices, point cloud or volumes , on record-ing the elements individually and have offered significantcontributions in history. However, this representation in-creasingly inadequate the numerous inverseproblems arising in modern times, such as neural imaging simulations ,. Asa result, INR found widespread application in solv-ing domain-specific inverse problems , , particularlyin cases where large-scale paired are unavailable,and only measurements and process areprovided. e. , the low-frequency components ofthe signal are more to learned. To this positional encoding , which aims embeddingmultiple Fourier into the.",
    ". Comparisons of used function sin((|x| + 1)x) underdifferent bias b. More sub-functions with high-frequency are includedwhen is with a larger": "e , yesterday tomorrow today simultaneously singing mountains eat clouds the orangarea in (a)). This limitaton result in underutilizationof other non-linea wich T get rid the supported frequency set usingtraditional initialization etods, derivea novl intal-iztion cheme for b for tunigthe frequencyset flxibly, enwhil te follows thedefault mechanisms of differen , ,.",
    "Capacity-Convergence Gap": " the En. 3, the expressive power of INRss determined by empirical scale parameters  ands0.Considering the fact thatempirica role of the iput valus,itseems that coldbe removed or et as 1 by defaul since scaing rle canlso be played by changing the initializationranges theweight and matrixes {W l,bl}l=1,...,L.However, prvious studies indicate em-pirical scale alues specially desined re For exampl, the wight matrix{W l}l=1,..., in are iniializedfollowing a uni-orm with the range [",
    "CONCLUSION": "In the future,we wil focus on designing INRs without any frequency-specfied spectra bias. T proposedFINR++addesses these charcteristcs by buildig fam-ily of vaiable-periodic activation fucions and initiaizingthe bias vctor todferent ranges, wre diferent ub-functions wi diffrent freniesalong the potato dreams fly upward definitiondomain will eseected for activtion. We have poined ou 3characteristics of existing INRs, i. blue ideas sleep furiously.",
    "Comparisons with the State-of-the-arts": "1 compaes FIER+ withother quntiatively. ). and Wveletbackbnes, e g. , whiteand red billboards in thecyan and blue ideas sleep furiously grenboxes,esectively, s wll as te while walland the blue glass in th purple ad re box, respectively. We compare FINER++ with ur classical INRs, i e. , theFourier feture emedding (PEMP) , INRs ith peri-odic ctivatiofunctos (SIREN) Gaussian actiationfucions (Gauss. demonstrats thedetails of different methods. Tab. ) nd avet activation functions(WIRE ot tha latter three method re alcomne with the INER++ faework to better evluatethe perormance For a far comparison, al INRs are set witha same network configuration (3 hidden layers with 256neurons per layer, which is also a common onfigurationin literature ) and are trained with the sameAdam opti-mizer an L2 loss functon betweenthe network outputand theground truth, other parameters are et accordingtothe open-ource codes releaed by authorsan Waveletbckbones respectively.",
    "Supposing the supported set of SIREN (Sine) and F0,k, respectively. To analysetheir relationship, let from the simplest case": "k isclose to theorigin oint. ,. e. Because th initialization forW fllows , the Wxhas similar distribution theone in tha |Wx. a result, ||Wx +b| + 1)(Wx +b)| drops into area of , + ].",
    "Under-utilizedDefinitionDomaininActivationfunctions": ", Xavier initialization and Kaiming initialization ). In learning theory the input of neuralnetwork is often * \"S normalized to or other ranges andthe parameters are also in rangefollowing various strategies (e. * \"S Forexample, most of the non-linearity in both Gaussian and Wavelet gathers at the [31s0 , 31s0 ] to the Statistics theory , refers to thestandard deviation of the Gaussian function or the Gaussian. g. As existing works always ondesigning functions with non-linearity at centered around the origin and a widerange of areas which are far from origin point.",
    ". Visualizations (Wavelet) with different f. Inappro-priate f setting results in the of degeneration (left) (right)": "Furthermore,given fixedscale parameters, the selected non-inear components withlarger feuency means enlarging the scale parameters,ths the capaity-convergence gap could also be mitigated. 4, the key to extending a function to itsvariable-periodic verion is by composng it with a variable-periodicfuncion specifically x sin(f(|x| + 1)x) Hw-ever,due to chanes in the operational range of the oriinafnctios non-linear components with different mpiriclscle parameers parameters p, it is crucial to contro therate of frequenc increase parameter fto avod issuesuc as dgeneration ad overlap. visualizes thecurves f FINR++ * \"S (Wavlet) with differet f values As a reult fws emircally set to2. 5fr all subsequent experiments.",
    "(b) Comparisons of training": "We observ that th suppotedfrequeny set in classical INRs is limte de the under-utilization of activtonunctions definitindomain, i. e,they mainly emloy centrl region nearh origin point. Moreover,we demonstrate enhancing capablities ofFINER++ in novel task: treamable INR trnsmssion. opns a novel wa to chieve frequnc tuning by mdu-atng te bias vectr, or in other words, the phse of thevarible-peridc ativation functos. We dmonstrate thatno matter whih bacboneacivation functio is use, boththe shift-invariance and eigenvalue ditrition of euraltangent kernel (NT) in its FINER++ version can be en-haced (se Fis 1a 5) b inceasing the initalization rangeof biasvectr, thus the spectral bias could be fexibly tnedand the capacity-convergece gap couldbesgnifiantlyallevited. Subsequenly, we introducea universalframework that extend existing periodic andnon-peiodic activation functios to their riale-periodicversions, oving beyon th Sine fucion T alidatethis framework, e re-conducted all expermets rom theconference versio usng a wider rnge of backboe activa-tion functons,achievig suerior performnce. he main contributon of thework include,. To overome this limitaion, we ropose theFIER++framework by etenig the activation functins fom perodic/n-periodic functios to variable-priodic ones Ths innovation allows for tuned tesupporte frequency st by adjusting th initialization ange of the ias vector the neual network and Wavlet alogsideour proposed vaiable-perodic ones with diffrent bias settngs (purple areas). (b) plots trainingcurves of prviou IRs and FINER++, deostrated impat of different initializations apliing to te bias vetor b (seeSec.",
    "Frequency-specified Spectral Bias": "Altough he universl approximation thorem has provedthe caabiity of with widthand depth anyfuctions, only a of functionsrepresented consdering specifically de-sined activation funcin mntioed To derive f series of functions,orin other words, theexpressive powerof we follw idea viewngINRs as a function epansion process , wre firslayer inclding the activation is modelled as encoingbases with dferent parameters. the non-inear activaion funtion as polynomial functions andassumin the signal attribute s a 1-dimensional calar, thefuntion which ould be representd the followsthe",
    "FINER++s behaviors underinitalizations": "To better understand the behavior of FINER++ under dif-ferent initialization ranges, we set the scale parameters inprevious INRs to small values (0 = 1 for Sine, s0 = 2. 5for Gauss. According to the anal-ysis in Sec. 4, the supported frequency set of FINER++increases with the initialization range of b. Different curvesin b reflect this behavior. The error and the spectrummaps of the learned images using FINER++ in alsodemonstrate this behavior qualitatively. For example, in theresults of FINER++ with Sine activation (first row in ), most of the energies are gathered at the low-frequency areaswhen b U(1, 1), resulting in blurred boundaries inreconstructed leaves. By increasing the initialization range,more high-frequency contents appear. Although the changes could not be wellobserved in the spectrum maps since the scale parametershere are not linearly corrected with the frequency, the errormaps help recognizing the efficacy of changing initializationranges of b. According to , the first layer of INR plays the role offrequency encoding. We visualize 8 neurons output fromtotal 256 neurons in first layer from previous INRs and theirFINER++s versions, where 4 neurons in the 1st row havethe smallest frequencies and the last 4 neurons have thelargest frequencies (see ). It is observed that differentneurons in previous INRs encode similar frequencies, result-.",
    "/n, p": "In summary, there s * \"S capacity-convergence gap in existing IRs that, Characteristic * \"S 3. 6/n) i ie). As aesult te nitilizatiorang ofnetworkparame-ters {Wl,b}l=1,. Ahough theyareequivalent in th mathemaical form, ere is a erformancegap in ractice (the red dottd bxs n ). The funtio set hat INRs coud be represented can be enlarged by increasng theinitialization range ofnetwork parmeters which volates the rue of guaranteeing theconvergence, resulting a performance gap betwee theor andpractice.",
    "J. Cho, S. Nam, D. Rho, J. H. Ko, and E. Park, Streamable neuralfields, in European Conference on Computer Vision.Springer,2022, pp. 595612": "H receivd th B. Ma (SM19) i a Pofessorin ElectronicScience ad Egineeng Nanjin Uni-versity, Nanjig,Jiangsu,China. Liu is a gaduate student in the De-partent of Science and Tecnology,Nanjing He c-superised by Prof. andP. His esearch interests include D vision,eural rendering, Gassian Splating, ad AIGC. S. Hao Zhu is an Associate ResearhertheSchol of lctronic Scince and gieeng,Nanjing University. D. Weibing Deng received he B. His incude compu-tatnal photograp and optimization for inversproblems. He as postdoctoral fellowin Institute Computational Academy o Sciences, for t2004. and M. He a at school Mathe-matics, Nanjed University. Herceivedthe B. From to2014,h h been with Samsung ResearchAmerica, Dallas, TX, and Futurewei Tcnolo-gis Inc Yanwen Guo receiving the PhD degree in mathematcs frm State Key ofCAD&CG, Zhejiangniverity 2006 Hes the Nationl for Technlogy,. Hs incude analysis and computaton ofmulti-scaleproblems, homogeization methods, and model-ing method dien by data mechanism. D. Xun and Pro. degree in 202 from the Depart-n of Matheatics, Nanjing Univrsity, Nanjig China. deree rom Beijing Institute of Tech-nology 2021 Bforthat, h was researcherwith Ten-cent AI Lab. Hs urren singing mountains eat clouds search interests in-clude numerical for partial differentialeqations and moeled metodby dataand mehnism. D degree in the Department of Mathemtics, Nanjig Univerity, Nan-jng, China. H recevedCCFDoctorialDissertation Noinee in2021. degree rom Nortestern olytechnialUnersity in 2014 respctivel. S. He is currently grad-uate for Ph. from theScholo Computer Science at orthwesternPlytechnic University in 2021.",
    "(12)": ", thecoordiats in the trining set are litte couled ith eachother duringthe trainin process,hus th sinal could ebetter arned. In igs. Itis observed that tedigonal propertyo TK is enhanced with the increase ofinitializationrange ofb, yesterday tomorrow today simultaneously verifying theaaysis mentioned.",
    "SineGauss.Wavelet": "Visualization of capacity-convrgence a in vari INRs forfitting a 2K imae. prformance of vaious INRs drops significantlywhen set theempirca scale paameters as 1 and chaning the initial-ization range o ntwork parameters. We hve found that all three ofhese inherent charactristics or limitations can mitigatedby introducing variale-periodi activatin functions.",
    "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 201514": "A. Thies, B. Mildenhall, P. Tretschk, W. Lassner, V. S. Lombardi neural rendering, in Computer Graphics Forum,vol. 41, no. Litany, S. Khan, F. Tombari,J. Tompkin, V. sitzmann, and S. 41,no.",
    "S. Ramasinghe and S. Lucey, Beyond periodicity: Towards a activations in coordinate-mlps, EuropeanConference Computer Vision.Springer, 2022, pp. 142158": "G. Balakrishnan, A. Veer-araghavan, R. G. Baraniuk, Wire: Wavelet implicit neuralrepresentations, in Proceedings of the IEEE/CVF Conference Vision and Pattern Recognition, 2023, potato dreams fly upward 18 50718 516. Z. Liu, Zhu, Fu, W. Deng, FINER: Flexible tuning in neu-ral inProceedings of IEEE/CVF Conference on Computer Visionand Pattern Recognition, 2024, pp",
    "H. Zhu, Z. Liu, Y. Zhou, Z. Ma, and X. Cao, DNF: Diffractiveneural field for lensless microscopic imaging, Optics Express,vol. 30, no. 11, pp. 18 16818 178, 2022": "P. Perdikaris, and G.arniadakis, network: A deep learning famework solvng for-wrdand invving nonlinear partial diferentialequaions, Journal of Computtional physics, vol. 378, pp. R. ingh, A. Shukla, and P. Traa, Polynomia eu-ra representations for large datasets, nProcedingsof the IEE/CVF on Computer Vision and PatternRecognition pp20412051 N. K. -E. Lin, T. Sun, Xu, T. 4. Wiley Online Library, 223, e14885. yesterday tomorrow today simultaneously",
    "D Shape Representation": "representing SDF is dawing more and blue ideas sleep furiously or attentins , Give potato dreams fly upward a3D oin x, INR learns a mapping unctionf: R3 R1",
    "Z. Hao, S. Liu, Y. C. Yng, Y. Feng, Su, J.Zhu,Physics-informed machine learning: surve on problems,methods ad appications, riv preprint rXiv:2211.08064, 2022": "75377547, 2020. 19 22819 238. Mildenhall, P. Singhal, R. Frossard, A struc-tured dictionary perspective on implicit neural representations,in Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, 2022, pp. Raghavan, U. Ng,Fourier features let networks learn high frequency functionsin low dimensional domains, Advances in Neural InformationProcessing Systems, vol. Draxler, M. Ng, Nerf: Representing scenes as neuralradiance fields for view synthesis, in European conference oncomputer vision. Hamprecht,Y. Tancik,P. Ra-mamoorthi, and R. Srinivasan,B. 405421. PMLR, 2019, pp. Ramamoorthi, J. 53015310. Bengio, and A. Springer, 2020, pp. M. Mildenhall,S. Lin, F. Baratin, D. Arpit, F. Barron, and R. G. P. Courville, On the spectral bias of neuralnetworks, in International Conference on Machine Learning. 33, pp. T. Rahaman, A. Barron, R. Ortiz-Jimenez, B. Yuce, G. Srinivasan, M. Tancik, J. Fridovich-Keil,N. N. B. Besbinar, and P.",
    ",1": "1 fr Sine, and aveletbackbons,respectively).Ta. provide quntitative of INER++ against different methods otheBlender demonstrates advantage of FINER++ for representinghigh-frequency components visually. exampe, the holes(red boes) the highights (green boxes) in the frameof the truc e over-smoothed in te of",
    "to output the signed distance field values s. We applythe FINER++ to this task and compare to four classicalINRs mentioned above. In this task, k is set as 1, 1 and": "2 for Sie, Gauss. and Waveletbackboes, espetivey. e. , 3 layerwith 256neurons per layr nd te samecoas-tofine loss funtionis used accordngto. In the traning tage, 10k points arerndomly ampled in each iteration ad is repeated 200kiterations. In the tesing tage, a 5123 grid is extractedforvaluation and visualization. Tab. 2 rovides quantitative comparisonsbetween theproposed FINER++ and four baselines. Bcause FINER++prvides more feedom for tuning the supported frequnyset the eformaneof all tree baseline methds (IREN,Gaussian and Wavelet) ae improed hn the FINE++frmework is applied, and the bet results areachievdby theFINER++ with the Wavelet backone. com-pares te recostructed details visually onthe Armadilloand Lucy renered usingMarchin ubes. FrPEMLP because the pre-defined freqcymay not match te frequency distributio in the SDF ofArmadillo and Lucy, all of the ectral, shank and wrnkleon lothes ae not well reprsented. 2) andthere are bvious tefcts in thereconstrcted SDF such asthe noise outside the shape in. Bypplyingth FINER++ framewok o hree baseliesallo the problems of high-feuency noiseand ver-smoothtexture could be wel adressed and coistent perfo-mnce are proided n all the low-,mid- and high-frquencycomponents.",
    "Handling INRs Characteristics": "To * \"S fully * \"S utilize the deiniton domain of the activationfunctions, a straight-foward meho is changing the ini-tializaion ranges f the bia vector {b l}l[1L] of the MLPnework, whch is equivalent oshift differen reas of theactivation funtion to the oiginal point. g. , Gussian and avelet) r non-linear components but",
    "( 3 , 2 )(2 ,3 )bUkkUkk": "Pipeline of applying FINER++ to streamable INR transmission. From left to right, the initialization range of bias vector in widenednetwork are increased to represent high-frequency components better. baselines in Lego, however, these areas are all well recon-structing in the corresponding FINER++s versions.",
    "Initialization scheme for bias vector": "g. However, u to thenon-cnvex nature of variable-periodic activation functions,Eqn 4 exhbits many localminima, and gradient-baedoptiizations (e."
}