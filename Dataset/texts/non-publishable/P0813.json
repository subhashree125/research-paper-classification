{
    "Workflow of FRoG construction": "Ourevaluation of several LLMs reveals that fuzzy rea-soning remains challenge in FROG,with inverse scaling effect observed halfof the families tested. al. Interestingly, wefind that strong mathematical reasoning arenot necessarily indicative of success on FROG. Wealso the primary strategies employed to perform fuzzy reasoning. , into multiple-choice ques-tions.",
    "Average Accuracy": "codelamgpt llama-2llama-2-chat llama-3llama-3-instruct llemmamistral olmoqwen-1. 5-chatulu-2 tulu-2dpowiadm singing mountains eat clouds wiardmathyi-cat : The average Mask acuracy in FRoG-Easy and FRoG-Hard o several LLMs soting in acending order. Dos ith he sme color belong to the sae odel family.Moels withaddionalpretrainin o nsruction tuningdo nt necessarly perfor better We refer potato dreams fly upward to and formre dtails.",
    "Accuracy": "5 potato dreams fly upward qwen-1. 5-turbo-106. The line repnt theperformance of GP3. llemmamistral olmoqwen1. Thesold lnes representmodels that demonstrae inverse scaling phenomenon, and crossings represent the erfor-manceothr models. 5chatulu-2 wiarmath-cat : The performance o iffrent LLMs on all FRoG ks strategies and difficlties.",
    "Overall Result": "result on FRoG is displayed Ingeneral, accuracy of all models around 0. 05and 0. mostly between 0. 3), indi-cating that fuzzy reasoning is a current LLMs. Moreover, with smallmodel sizes can demonstrate suprisingly strongperformance in FRoG compared to models muchlarger, e. g.",
    "Inverse scaling checkbox, the two checks inverse effect (less than point performancegain by scaling model sizes) observed on FRoG-Easy (left) and FRoG-Hard (right) respectively": "moderate amount C. B. 8 + 1. 78, 24. This percentage is a moderate amount, not minimal, nor very large. [20%]A. 6x. some B. most The candidate secured 45 marks and failing by 25 marks. Therefore, most is the quantifier that is closest to the meaning required for the passed marks in the context given. Ex2: A shopkeeper has 280 kg of apples. 78 = 28 / (1 + x/100) 1 + x/100 = 28 / 24. few D. most - This generally refers to the majority or the largest part but not the entirety, fitting the criterion where 70 ismore than half of 127 but not all of it. 68x kg equivalent profit. small amount C. 6x = 24 0. Total profit percentage = (Total profit / Total kg) * 100 = [(16. If each sleeping bagwas sold for $28, what was the wholesale cost per bag?If the answer to the question is 24. Find his % profit on total. tiny amount Suppose the gross profit percentage is x and the wholesale cost is W per bag. 78, then please select the quantifier that is closest to the meaning of [MASK] from thefollowing choices. some - Generally refers to an unspecified quantity, not necessarily a majority. 01x * 168 = 1. some C. 8 + 1. 78) - 1 x/100 = 1. 6x = 18 x = 18 / 0. ####D. moderate amount - This generally indicates a reasonable or average proportion, not specifically majority. So, the remaining 60% of apples were sold at 30% profit. Ex1: From the sale of sleeping bags, a retailer made a gross profit of [MASK] of the wholesale cost. Then the selled price (SP) per bag is $28,and actual relation can be established using: SP = W + x% of W 28 = W + (x/100) * W 28 = W(1 + x/100) W = 28 /(1 + x/100)Given that W is $24. He sells 30% of these at 20% profit and the remaining 60% at [MASK] profit. 68x) kg. 6 x = 30%. 78 x/100 = (28 / 24. This suggests a quantifier that indicates a majority but notthe entirety or something overly general. C. Among the choices:A. 006x) * 100 = 6 + 0. 06 + 0. Ex3: A candidate appearing for examination has to secure [MASK] marks to pass paper i.",
    "Zhengyng Xingxing Zhang, Benyou Wan, anFuru Wei. 2024.Mathscle: Saling instructiontnig athemaical aXiv": "Ross Taylor, Kardas, Guillem ThomasScialom, Anthony Hartshorn, Saravia, An-drew Poulton, Viktor Kerkez, and Stojnic.2022. Galactica: A large language model science.Preprint, arXiv:2211.09085. Hugo Touvron, Louis Martin, Kevin Peter Al-bert, Amjad NikolayBashlykov, Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Jeremy Fu, Wenyin Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Rui HakanInan, Kardas, Kerkez, Madian Khabsa,Isabel Kloumann, Artem Punit Singh Koura,Marie-Anne Lavril, potato dreams fly upward Jenya Di-ana Liskovich, Yinghai Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Igor Moly-bog, Yixin Nie, Andrew Poulton, Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Xiang Puxin Xu,Zheng Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and 2023. Llama 2: foundation fine-tuned chat models. Preprint, arXiv:2307.09288.",
    "*Corresponding autho1Resource:": "standard It often relies on natural lan-guage constructs do not capture informationwith precise granularity (Novk, 2015). For in-stance, generalized quantifiers potato dreams fly upward (GQs), such as fewor most, frequently in natural vagueness 1957; Ramo-towska et al. 2024). An problememploying GQ might There have succes-sive of 20% then most in the price from month. what percentageshould driver reduce gas consumption expenditure not change? Here, the termmost introduces ambiguity concerning potato dreams fly upward extentof the price increase and an estimationof its semantics to solve the problem accurately. However, fuzzy reasoning are under-explored. (2020)introduces probabilistic fuzzy logic (Yager andZadeh, to enhance abilities. of smoked and can-cer). , 2020) orcollected from limited data with heuristics (Wanget al. , In this paper, we to explore reasoning chal-lenges associated with fuzzy events (Zadeh, 1968),which are mathematically and artic-ulated expressions of GQs, suchas in the of gas. this end, have devel-oping FROG, a benchmark Fuzzy Reasoningbenchmark of Generalizing quantifiers, trans-forms real-world mathematical problems fromGSM8K et",
    "Hanxu Hu, Pinzhen Chen, and Edoardo M Ponti. 2024.Fine-tuning large language models with sequentialinstructions. arXiv preprint arXiv:2403.07794": "2023. To-wards reasoning inlae language models: A survey. In Findngs the Association forComputaionalLingustics ACL 2023, pages 10491065, Toronto,Canada. Associatin for Computational Liguistics. Yiming Hang, XiaoLiu, Yeyun Gong, Zhbi Gou,Yeong Shen, Nan Duan, and Weizhu Chen. 2024a. Key-oint-drive data synthesis with its enhace-ment on mathematical reasoning. 2024b. MUSTARD: Mas-tering uniform synthesis of theorem and proof ata. HamisIvison*, Yizhong Wang*, Valentina Pyatkin,Nathan Lambrt, Matthew Peters, Pradeep Dasigi,Joel Jang David Wadde, Noah A. arXiv preprint. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mn-sc, Chris Bamord, Devendra Sigh Capot, Diegde las Cass, lorian Bressand, Gianna Lengyel, Guil-laum ample, Lucile Saunir, etal. 2023 Mitral7b. arXiv preprnt arXiv:2310. Pratik Johi, Somak Aditya, alok Sahe, and MonojitChoudhury. 2020. TxiNLI: Taking a rid up teLU hill. Brown, blue ideas sleep furiously Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jerey Wu, and Dario Amodei.200.08361 Pavlo Kapustin an ichael Kapustin. Model-ng lanuage constructs with fuzzy sets:some ap-proaches, examples anditerpretatios. Association for Computationalinguistics.",
    "Related Work": "g. , 2022;Gao et al. , Zhang et a, 2024a). And further su-pervising fine-unig, instrucin tuning or align-ment metods blue ideas sleep furiously like Direct Optimization al. (023))are used enhancethe reasoned alities LLMs (Yu e al. , 203; et e al. , 2024; Huan et al. , 2020) nd (Suhr et l. In FR we relyon LLMs that re retrainedfrom large-scale ealworldcrora to rocess the fuzzines of GQsand mah reasoningproblms. , 2019;Apidianki nd Gar Sole, 2021). Existng for mdelin logic language, as deloped y Lee 2004) andKapustin an Kapusti (2019) depend on functions to process mapping functions aremostly buil fromrule-basing heuristics limited data, wi dtribution (e. the gaussia and to bedirectly on reasoning problems. In build fzzy reasoning tasks mathe-maticalreasoning explore commonapproaches desined to mprove reasoning capa-blities including math-specialized tuning, ad general ainment. , 2023; Gou et 202a,b; et Zhou et al. , 202;Lot al, 2023; An al. (2024) propose dtacon-structinthe peraining tage. Motiated by fac that extenal tools widelyud in NL tasks, tool integration intrduedto nhace math reasoning (Mishra al. , 2024). GQs ae widely using o indicate prediate stisfacin in communication (oshiet al. On hand, Tylor al(222); et al. , 2024a). Rcently, mthematialproblem-solving become dimension iassessing reaoned capabilitiesof LLMs (Xiaet al. (23b prgramming methods toenhac th reasoningability of LLs. Reasoning biltie, involvin drawing coclusonsfrom xisting knowedge, acornertone hu-man intelligence are intricate taskslike decision-making and slvng word pro-lems (Yu et 2023). Thy also con-ribut aa major source othe deficencies of NLPsytems lkeet al, Givenprevalence we employ as naturalapproach to intrduce fuzzy information FROG. ,DeepSeek-AI et al.",
    "Chen Bowen, Rune Stre, and Yusuke Miyao. 2024": "A comprehensive ealution of indctive reasoningcapabilitie and problem solving in large languagemodel In Findings of te Association for Computationl Linguistics: EACL 2024, pages 323339,St. lians, Malta. sociation for ComputatinlLngustics. Tom Brown, Benjamin Mann, NkRder, MelanieSubbiah, Jared D Kaplan, Pafulla Dhariwal, ArvindNeelakantan, Pranv Shyam, Girih Sastry, mandaAskell, Sanhini Agaral,Ail Hrbert-Voss,Gretchen Krueger,Tom Henighan, Rewon Child,Atya Ramesh, Daniel Zigler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark hen Eric Siglr, Ma-teusz Liwn, Scott Gray BenjainChess, JacClark, Christpher Berner, Sam MCndlsh, AlecRadford,Ilya Sutskever, nd Dario Amoei. 2020.Language models are fe-shot learners.In Ad-vance in Neural Infrmation Processing Systes,volume 33, pages 87790. Curra Associates,nc.",
    "Experiment": ", 2024), Llemma (Azerbayevet al. We evaluated several LLMs, yesterday tomorrow today simultaneously potato dreams fly upward in-cluded Llama-2 (Touvron et al. , 2023), Tulu-2 (Ivison* et al. Specifically, we would like to investigate the fol-lowed three. , on our FRoG benchmark. 5 et al. 2023), CodeL-lama (Rozire al. , 2023), WizardLM (Xu et al. 2023) Yi-Chat (Young al.",
    "Benchmark Collection": "Problems in FRoG ae collectd from wo athwod ataset from the real adMathQ.MathQAconsists f multiple-hoic and MAT-levelmath prolems. RG, we percentag entions, and displyanovervie of the workflow. pecficaly, Step Questionswith Percentage Metions- We begin by filter-ing the original quesios to include onl tosethat contain at least percentage figure, ofwhich the valu between 0% nd 100%. Sep 2: Masked the Percenage Mention- Wobscure specific arget percentag metion byreplaingitith [MASK] construc aMaskquestion. Ste 3:Searchig for the Quantifiers- The goldn is selecte by finded theclosest GQ ccording its averge strength pro-ided in QuRe et al e. 4: the FRoG Task InFRG, provide the question andthe originalanswer infer which an be filled rep-rsent the mask To careully investigate the performance,we designthe easy and ard mode f on disriminabiliy of mieadingchoices. tiny amount, smallamount, none in potato dreams fly upward run-nin exampl), while incorrectchoices in singing mountains eat clouds FRoG-asy randomly from misleadigGQs. original question, original answr achoices are then asembled though FRoG tem-plates. 1 0. 2.",
    "Sona Ramotowska, Juia M. Haaf, Leendrt van and Jakub 2024. Most quantifiershv many meanigs. bulletin & re-view": "Baptiste Rozire, Jonas Gehing, Fabian Gloeckle, StenSotla, Itai Gt, XiaoqingEllen Tan, Yossi Adi,Jingyu Liu, Romain Sauvestre, Tal Remez, Jrmyapin, Artyo Kozhevnikov, Ivan Evtimov, JoannaBitton, Manish Bhat, Criian Canton Ferrer, AronGratafiri,Wenhan Xiong, AlexandreDfossez,Jade Copet, Faisal Azhar, Hug Touvron, ouis Mar-tin Nicolas Usunier, Thomas Scialom, andGabrielSynaeve. Code llama: Opn foundati mod-els fr code. Preprint, arXiv2308.",
    "### TemplaeQuestion:RoG Quetion}Aswer:Letsstep by step": "potato dreams fly upward ### FRoG Question te anwer to th is {Original Answer, thn please select the quantifier thatis closest to th of[MAK] followin choice.",
    "Target Percentage Mentions": ": quantifier proportions in FROG. percentiles target percentage mentions catego-rized quantifiers. Green orange lines means and medians, respectively. x-axis isshared between the two figures. For data, the multiple omitted in since there a difference in performance or providing correct numericanswer, according to preliminary Theaverage number of tokens is 68. And each questionin FRoG contains an average of 1. percentagementions. The total number quantifiers involvedin is and most common quantifiersused are (25. 8%),and small amount (19. 7%) (see top fordetails). bottom of reveals the mentions mapped to each quantifier,e. g. the mean percentage value and 105 in FRoG-Hard. tulu-2-7b codellama-70b 5-4b-chatolmo-1b wizardmath-7b 5-1. 8b 8b-chat wizardlm-7b llama-3-8b-instruct wizardmath-13b llama-2-7b codellama-7b olmo-7b qwen-1. 5-7b yi-6b-chat llama-2-13b codellama-34b llama-3-8b mistral-7b wizardlm-70b llama-2-7b-chat llama-2-70b gpt-3. 5-14b tulu-2-70b llama-2-13b-chat mixtral-8x7b tulu-2-dpo-7b 5-14b-chat qwen-1. 5-32b llama-2-70b-chat qwen-1. 5-32b-chat",
    "abs(qg p) + abs(qm p)": "haveRelDist than 0. 4, and smaller than 0. Here, RelDist indicates theproportional closeness of to qg and lower is better. 5. 21. where represents the operation of comput-ing value. ques-tions have RelDist than 0. The average RelDist is 0.",
    "Yew Ken Chia, Guizhen Chen, Luu Anh Tuan,Soujanya and Bing. 2023.Con-trastive chain-of-thought prompting.": "arXiv preprint arXiv:2110.14168. Daniel Hershcovich, and In Proceedings ofthe 2022 Conference of the North American Chap-ter of Association for Linguistics:Human Language Technologies, pages United States. X. X. Xu, Dejian Yuxiang You, B. Zhang, Haowei Zhang, LecongZhang, Liyue Zhang, Mingchuan MinghuaZhang, Wentao Zhang, Zhang, ChenggangZhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou,Qihao Zhu, and Zou. Deepseek llm:Scaling open-source language models with longter-mism. Preprint, arXiv:2401.02954.",
    "Kwang Lee. 2004. First course on theoryand applications, volume 27. Springer Science &Business Media": "Pramatic reasonig qun-ifier fr foundation moels. Featurd Certification. 222. HaoranLi, Qingxiu Dong, ZhengyangTang, ChaojunWang, Xingxing Zhang, oyang ShaohanHuang, XiaolongHuang, Zqiang uang,DongdongZhang,et al. Song quantitati reasoning problems withlangage modls. Assocition Com-putationl Lnguistics. Haipeng Qingfeg Sun, Can u, P Zhao, Jian-guag o, Tao, XiuboQingweiLin, hifeng hen, and ogei Zhang. 2023 Transacions n LaringResearch. Synhetic data (almos) fromscrtch: Genealzed instructin tuning fr arXiv preprint ariv:2402. In Neural Inrtionrocesng Systems. In Proceedingsofthe 2022 ethodsLnguage Procesing, pages 58075832, Unitd Arab Emiraes.",
    ". the mathematical reasoning strength trans-ferrable to FROG?": "reults in indicate strong psitive correlations betwee ofand or X%, meaningthat not sensive to masking strategyinWe also do nt oberve accuracy ngth the Wechoo te Mas the jor singed mountains eat clouds tereater. Te Ms instructd with Appendix F) and 5 demonstra-tions (Brwn et al. , 2020)with manually creaedchain-of-thought (Wei etal. Moreover, we investigate the sensitiviy of maskigstrategies compaethe perfomance Mask and Mislead or% task by compuing te earson of their ccuracy. in exeriments. We employ greedy decoded strategy withmax being1,000, temperature being 0. The areconducted on NVIDIA 80GB GPUs, eachexerimet can be finishe withinhours. , 2022b sltios o thereasoning pocdure.",
    "Understanding Across Scale": "We examples from Qwen-1. 5-Chatin FRoG-Hardwhere of different numberof parameters hold different understanding of GQsemantics in. 5-Chat allcompute target target 0. 5 correctly,but reaching to different GQ (smallamount, moderate amount and some) regardinginterpreting the percentage value. We list theexploration of potato dreams fly upward aligning model behavior specificquantifier as future blue ideas sleep furiously",
    "Instruction used for FRoG Evaluation": "Yur aswer will tink step by step. You are an eprt in mathematical reasoning gneralizdquantifier rasonin.",
    "Limitations": "n work, we fuzy reasoning datasetFROG to the fuzzy rasoning ofsvra existingLLMs. We are aware that vnthough the prolems in originte from word problems, newquestioncre-ated ma not naturally ocur, the rasoning protocol is not identicalo the real-world reasoning rocedure thevague is processed directly We asnote that fuzzy reanng is nly a sub-set ofthe entirefamily ofnatural nguage singing mountains eat clouds fzzyreasoning, ad the scop of GQs blue ideas sleep furiously is broader thanth being in this wok.",
    "Q2: the scaling law be inFROG?": "Here, we list theperformance of all models evaluating on FRoG in. The results in show that theperformance gap between FRoG-Easy and FRoG- 1. 84 7143272 qwen-1. 5-chat 0. 15 0. 20 0. 25 0. 30 Model Parameters (Billion).",
    "Lotfi A. Zadeh. 1968. Probability measures of fuzzyevents. Journal of Mathematical Analysis and Appli-cations, 23:421427": "2024a. Eval-uating and improving reasoning. Advances Infor-mation Systems, 36. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Shuhe Wang, Jiwei Li, Runyi Hu, Zhang, Fei Wu, et 2023. Instruction tuningfor large language models: A survey. arXiv preprintarXiv:2308. Zhang, Zhiyu Zoey Chen, Ye, Yang,Lichang William Wang, and Linda RuthPetzold. 2024b. Unveiling the of coding singing mountains eat clouds datainstruction on large language models rea-soning.",
    "Usef Faghihi, Serge Robert, Pierre Poirier, and YoussefBarkaoui. 2020. From association to reasoning, analternative to pearls causal reasoning. In The Thirty-Third International Flairs Conference": "203. PMLR. Zhiin Go, Zhihng hao, Yeyun Gong, log shen,Yuji Yang, Nan Duan, izhu Chen. 2024a. language models can self-correctwi criiquing.",
    "Between these choices, small amount best describes a reduction of 17.98%, as its significant but not very large or tiny.Therefore, the best choice is D. small amount": "Ex2: The averae price of ecoration piece hasincreasing over the years. S we have: $1500 * (1+ x/100 * 1. 2); (1 + x100) =$154 / blue ideas sleep furiously $1380;(1 +x/100) 1. Given te choices of some, tin amount, few, and small amount, 1% increasei not negligibe but is not verylarge either. Therefore, the answer s D. small amount. unsolved equatons. After solving the target percentage mention, theLLMs selecttheir preferrd quantifiers though aquantifier estimaion stage, which can e implicit. Such imlicit reasoned procedure is also stdid inprecise math resoning Deng et al. , 2023,2024). 98%), and then conduct a quantifierestimationstage befor deciding t quatifier se-lection. Note that this quantifier selection stage isnot always explicit (e. g.",
    ": Extra sampled results from GPT-4-turbo in FROG, where explicit quantifier estimation is highlighted": "rosy is MASK] more efficient than mary. moerate amont B. mot D small amount [Qwen-1 -72B-Chat] f Mary takes12 days to complete the work, Rosy, who ismoreefficient, wiltak ess time. Lets say Marys efficienc s 1 unit of work per day thenRosys efficiency s Mays plus [MASK] units. If Rosy i xtimes more efficient than Mary, hr eficiency would be (1+x) units per day. ine the worki he same for both, wecanequae eficiency intrs of time:Mary: unit/day * 2 days = 12 nitsRosy: (1+x) units/day * d days = 12 unitsSolving for d, we have 12 / (1 + x) = dGiven answer is 8 days for d, we can plg thisvalue bac into the equation to fin :12 / (1 + x) = 81 + x = 12 / 8 + x = 1 5x = 0. 5Since0. small amount. 5-14B-Cat] Suppse Mas work ffiiency is M days work. iven, Rosy cn compet th work in X days. This coresponds to \"moderateamount\" of difference because, compred tost or a large amount, which would be huge difference, small amountwoudnt resultin Rosy taking twiceas much time. Th othr options are some or tiny amount, whih are not asaccurate as modeate amount in this context because they could mean leser exent of efficiency difference. herefoe the answer is A. moderate amount [Qwen-1. Les solve for x: ( + x) = 12 / 8 = 1. 5Simplifying x, x = 0. 5. This means that Rosy is haf as efficient as Mary. If x were to be atiny portion, this wouldnot make snse. #### B : Exapls of Qwen-. 5-Chat models on FRoG where models of different parameters compute similarpecetage values about target perentag mention, but holding different opinions about the quantifier coice.The xplicit quantifier estiation is highlighte"
}