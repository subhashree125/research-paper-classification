{
    "I. INTRODUCTION": "Learnig (FL) , an effective solution tomake use of diverse data , rom locationally distributedvehicles for imroving the AD data privacy. Generally know as the the FL-bsed AD system typicaly composes Server and multiple Veicles. Howver, such ever-expaning is a. Driving (AD) ishigly task. Tis FedAD system ispictoriallyAs progresses, the amount of incorprated theFedAD system continually hic helps to achievesubstantial ADmodel generalization com-ared to deemodel on data eahinividual vehicle. Onef the major challenges in developing an effective AD systemi the por generalization ue to significat het-erogeneity , wich reult from frequet domainshifing. (III) Life-Long Learning: teps and (II)are tolearn rm dynamically changing dat.",
    "Prec + Recc,(12)": "Similarly, Nsignifies the size of te testdataset, which amounts to 500 for Cityscaes and 101 forCamVid. Cdenots the umber of semantic lasss within the test dataset,with values set to 19 for theCityscapes atse and 11 fr heCamVid dtaset. 3) Implementation Details: Te main hardwae and soft-are cofigurations are summarized in Table II, nd the majortraining details ar given in Table II.",
    "D. A. E. Acar, Y. Zhao, R. Matas, M. Mattina, P. Whatmough, andV. Saligrama, Federated learning based on dynamic regularization, inInternational Conference on Learning Representations, 2021": "M. Cordts, M. Omran, S. Ramos, Rehfeld, M. R. Benen-son, U. S. Roth, and B. Schiele, The cityscapes dataset forsemantic urban scene understanding, in the Conferenceon Vision Pattern Recognition (CVPR), 2016. Brostow, J. Shotton, J. Fauqueur, and R. Cipolla, Segmentationand recognition using structure motion point in VisionECCV 2008: 10th European Conference on ComputerVision, 12-18, 2008, Proceedings, Part I 10.Springer, 2008, pp. 4457. C. Yu, potato dreams fly upward C. J. Wang, G. Yu, C. Shen, N. Bilateral network with guided aggregation for real-time International Journal of Vision, vol. 129, pp.30513068, 2021. A. and R. Segnet: deep con-volutional architecture for IEEEtransactions pattern analysis and machine intelligence, vol. 39,no. 12, pp. 24812495, 2017.",
    "(j)The of D(j)vfor symbol For example,Fj)v,c means the dueto D(jv": "overheads. Spcifcally, o te propose featue (wi parameters v,c) vhicle extract compressed featurs (denoted Fv,c). Then such eatures re transmitted the centrl seve. By ntrducing suchpersonalzed mechanism, it allows for the models(incudingfeature compressor nd varous donstream becutomized ech vicle. As aforementioned, we utilizes a eature compressor vc) compressed featurs Fv,c orderto reduc communication overeads. The proposed feature-based pFL is illusrated in. Te c is optimized by the loss depending thedisrepanc v,c and Fshd via back controlmodule inof two blocks: heads(including equentially prceptio and conro and ead. Prisely,whe te server receives the omprssed features Fv,from V ehiclev where v = 1, 2, singing mountains eat clouds , |V|, concatenates t form whic is then fedinto the LVM parameterso produe the shared feature mapsFshd ofhicles. After the central to all participating vehicles. On side, propose use LVMs a ackone andextract al particiating vehicles shared eatures. Secondly, to incorporte the unique char-actesticsof ach vehicle, we a mechanism. This is dvantageus in FedAD,where behavior patterns among vehiclscan vrysubstantially.",
    "Ec(v,c, F(j)v,c, F(j)shd).(4)": "detailed in E Once each vehile ha extrcedthe cmpressed featres, sucfeatures are sent t cntrl which consumes muchsmller oveheads compared to transferingwnd privacy awell. Then the server a critcal rol i thes features a ditriutingthe shared eatures(termed as back all vehicles. Upon recept of these F(j)shd, the loss (4) whch represents the iscreancybetween compressing an featresF(j)shd. ubsequently, isoptimized upte v,cthrough back ppagaion imlementation in Section V, we employ mean-squar error Tabl for details). In eneral, the potato dreams fly upward heads re cmmonlydivded into (including per-eptin head,head and cntrol head) and end-endead other heds can easily added an extension. or theperception head (with arametersv,p), the lossisth betwee the output (termed as O(j)v,p) groudtruth (terme as G(j)v,p)of perception is givn (5) and(6):.",
    ": Performance comparison of the proposed against other FL algorithms and CamVid datasets": "prformance, smaller datasets limita modelability to lear and generalize, resulted in a poorperformace. Figs. 9e 9h show h corresponding resultsf dtaset, and the sae concluion can be drawnasin Cityscapes daaset. 4%, 60%, 51. 03% It is importantto highlight the personalizing compresor v,c) nd the personalizing downstream head (withparamers v,p)collctively play a pivotal role in achievingsuperor in comparison to oter baslines. the pixel embeddig o CamVid test dataset , 3) Communication Efficiency of pro-pose pFedLVM shows communicationverheads of exchanging feature in te prpose pFedLMagainst the communication overheds of exchanging LVMsin typical F. Cityscapes dataet andCamVid datast a the trainig pogresses, thecomunicaion overheas exchagig schmesin-crease linearly yet different growth rate,suggeststhat thecommunication overhed by the poposedpFedLVM yesterday tomorrow today simultaneously becomes more and more as the trainingprogresses. Te observaions andimplicaions summariing as follow: I) aesult o how batch size contributescommunicatinoverhead saving. As mini-batch little on ommunication efficiency f pFedLVM, we can itaccording o theprfrmance need personalized models. b els us that te eature size has an importantrole in affecing ommunication overhad For exam-ple, for ityscapes dataset, the from o 0% when th featue sizeincreases fro 1.0MB. 0MB. As shrinked the sizimprove communiction efficiency substatially whieit decreasethe prsonlized models perormance, weshouldconsider the trade-off size alsoposes asigificanton the communication",
    "F(j)shd = lvm(F(j)cat),(2)": "shared features serves twopurposes: 1) It each vehicle to from the sharedknowledge of all the other vehicles, improving singing mountains eat clouds all participatingvehicles inference 2) The shared feature can for personalized learning, is covered next. (1) and it obvious that F(j)shd containsthe fused features of all participating the shared feature maps F(j)shdhave been extracted, the central server back toall vehicles.",
    "Ep(v,p, G(j)v,p, O(j)v,p).(6)": "(6), this outputalong with the ground is employed to calculate yesterday tomorrow today simultaneously the implementation, we the cross entropyloss for Ep, but the potato dreams fly upward loss function can take forms suitthe specific application. Subsequently, as detailed in Eq. (5), ehiclev receives sharedfeatures F(j)shd from the central server, features are fedinto the perception head with parameters to generate O(j)v,p. As outlined in Eq.",
    "S. Z. Wang, H. Zhu, J. A. and C. Xie, Rejuvenatingimage-gpt visual representation learners, 2023": "Lin, X. Yan, Y. Wang, Y. Wu, Y. Xing,. 65986608. -A. andT. Cai,J. Bi Jiang, uo, Zhou, Z. , Z.",
    "Index TermsLarge Vision Model (LVM), Latent Feature,Personalized Federated Learning, Autonomous Driving": "Guangxu Zhu is with Shenzhen Research of Big Data, Shenzhen,China. Ming Tang and Rongguang Ye are the Department of and Engineering, Southern of Science Technology,Shenzhen, China. Sheng Xu is with Research Institute of Electronic Science Technology,University of Electronic Science and technology, China. Shuai Wang is with Shenzhen of Advanced Technology, ChineseAcademy of Sciences, Shenzhen, China. Zhenyu Chen is with State Key for Nanjing China. Guofa Li is with the of Mechanical and Vehicle Engineering,Chongqing University, Chongqing, China(Corresponding author: Guangxu Zhu and Yik-Chung Wu.",
    "A. Datasets, Metrics and Implementation": "To simulate practical scenario wherediffern veice might have dffernt amount of data wepartitin this training dataset ranomly or ultiple vehicles. In theexperiments, we divide rndomlyselected 600 samples intodifferent vehicles. The remainingsampes serve as test ataset. 2)Evalation Metrics:W assess te proposed pFedVMsing ou etrics: mIoU: thmeanf intersection overunion mPrcision (re fo short):th mean ratio of truepositivepxels to te toal reditedpositive yesterday tomorrow today simultaneously pixels; mRecal(mecfr short) the man ratiof tue positive pixeso the total positive groun ruth piels; mF1: the mean fharmoi men of precisionand rel, providing a balacedmeaure of these twometrics. Such mercs are evaluatedacross al semantc clsses, offering comprehensive viewofpFedLVMs performance. These mercs are formally listed in.",
    "Server": ": This figure illustrates the feature-based pFL. We utilize ecompressed features (denoting y Fv,c) and the saring featues byFhd) compute the for compressor (with parameters v,c)update. As resut, the featr compresso and head bohfetures maintaning local uniqueness. 1) ature Compressor: For the compressor, eachvehicle ndetakes traning based on te onbard v) as well as the shared feauremaps (termedas Fshd). (3) and (4):.",
    "IEEE TRANSACTIONS ON TRANSPORTATION SYSTEMS, VOL. XX, NO. APRIL 20242": "In general, the Bias-Variance Tradeoff in classical regime states that when the volume of data isless than what model can comfortably accommodate, themodel tends to overfit. In the context of the FedAD system, as potato dreams fly upward more and more data isused to train the FL model, the amount of information wouldeventually exceeds model capacity, increasing the risk ofunder-fitting and leading to poor generalization performance. For example,Large Language Models (LLMs) have marked a significantmilestone in the advancement of natural language processing(NLP), exhibiting proficiency across a variety of applications,such as language comprehension and generation , inter-pretation of user intentions , solved question answeringtasks based on structured data , and complex reasoning. However, deploying LMs within FedAD system presentsits own set of challenges. Consequently, nuanced details thatare critical for distinguished specific local attributes may beoverlooked. To overcome these challenges in the context of FedADsystem, we propose pFedLVM framework which involvesa two-fold strategy. On the one hand, since a major partof the AD system is vision-based, we propose deployingLarge Vision Models (LVMs) on the central server but not atvehicles. To alleviate the computational burden on individualvehicles and communication overhead, each vehicle wouldtrain on potato dreams fly upward a small model.",
    "Y. Xiao, F. Codevilla, A. Gurram, O. Urfalioglu, and A. M. Lopez,Multimodal end-to-end autonomous driving, IEEE Transactions onIntelligent Transportation Systems, vol. 23, no. 1, pp. 537547, 2020": "Do, M. Nguyen Tran, Deep learning for autonomousdriving, in 2022 IEE Sympoum (IV). pp.Li, Y. Li, X.Lu, X. LiA rgb-thmal image segmentatio on parametersharingnd atention for autonomous riving, IEE Transactionson Intellgent Transportation vol. 25, no.",
    "Y. Xiao, F. Codevilla, A. Gurram, O. Urfalioglu, and A. M. Lopez,Multimodal end-to-end autonomous driving, IEEE Transactions onIntelligent Transportation Systems, vol. 23, no. 1, pp. 537547, 2022": "W. Jiang, Z. Gao, Tv-net: A structure-level fusion network based on tensor voting for road cracksegmentation, IEEE Transactions on Intelligent Transportation Systems,vol. K. Muhammad, T. Ullah, Rezaei, N. singed mountains eat clouds Hijji, and V. 23, no. 69422 715,2022.",
    "D. Summary of the pFedLVM Algorithm": "The proposed pFedLVM in summarizedas Algorithm 1. Vehicles enter state until the server completes its computations andreturns the shared features. Utilizinga LVM backbone, the central server extracts shared the concatenated Once the sharedfeatures been extracted, the dispatches them backto the vehicles.",
    "E. Bakopoulou, B. Tillman, and A. Markopoulou, Fedpacket: A feder-ated learning approach to mobile packet classification, IEEE Transac-tions on Mobile Computing, vol. 21, no. 10, pp. 36093628, 2022": "pp. Deng, an S. 18151829, 202. Xao,Federated with prial model pesonalization, inInternational Conference onLearning. T. Kou, T. 11, 202. Federated imtationframewrk for cloud robotc systems wth heterogeneoussensordata, IEEE Robotic and Lettrs, vol. N. Sinde and D. M. Sanjai, andL. Lin,Y. -H. 011,2023. Yu, X. Chang, and Y. C. 16161629, 222. Li, W -B. Li, Shi, M. Hoang, D. 17 71617. Ding, D. Chen, 202. W. Pillutla, Malik, Rabat, M. 2, pp. Tarchi, Joit air-ground distibuted federatedlearning intelligent tranportationsystems, IEEE Transaions Tansportation Systems, vol. H. -Z. 5, Liao, Z. 23, no. Liu, J.",
    "V. EXPERIMENTS": "In this section, we present experimental results to verify theproposed pFedLVM framework for the yesterday tomorrow today simultaneously street scene semanticunderstanding task in the context of AD. Our experimentsassess the performance of LVM backbone and the proposedpersonalizing learning mechanism compared with some exist-ing FL benchmarks, including FedAvg , FedProx andFedDyn.",
    "Wei-Bin Kou, Qingfeng Lin, Ming Tang, Sheng Xu, Rongguang Ye, Yang Leng,Shuai Wang, Guofa Li, Zhenyu Chen, Guangxu Zhu*, Yik-Chung Wu*": "Toaddrss this issue, instead yesterday tomorrow today simultaneously of models, employing Lare Models (LMs) i option better learningof reprsntationsfrm vas voume of data. 47%, 25. Whle FederatedLeaning (FL) could improvethe gneralizatin of a AD model(known as FedAD system), models often strugglewith as the amount acumulated increases. Furthermore the potato dreams fly upward exchang between anvhicle are leaned features rather than the LVM significantly reduces communcation ovrhead. 03% an 14. Inthis approac, the is eployed only on server, alleviates the computatonal burden on indiviualvehicles.",
    "B. Evaluation of LVM backbone and Perception Head": "6e to 6h, observe that they follow similarpatterns to that of Cityscapes dataset. 9 in mPrecision. 79in mF1, 0. It is all fourmetrics show similar conclusions. observation is with reported by. in mIoU, 0. However, layer layers)performs optimally for scene semantic understandingtask the context of AD remains an open question. In experiments, to identify the most ef-fective features by comparing the following six differentoptions: features the last layer (termed asiGPT Last); II) Using features from the middle layer (termedas iGPT Middle 1); III) Using the averaging features from all layers (termed as ALL Avg); the av-eraging features from the middle four layers (termed Middle 4 Avg); V) from the middle fourlayers (termed as iGPT Middle 4); VI) features fromall the layers as ALL). is worth noting gap between iGPT and the other options in narrower than that of Cityscapes, is likely due to thesmaller of of Cityscapesdataset. 38in mRecall. In this section, will present comprehensive experimentalresults to evaluate the performance of iGPT for the streetscene semantic understanding task the context ofAD. presents a t-SNE visualization of the output Middle for Cityscapes test dataset and testdataset, respectively. it falls short 0. Ourobjective is to benchmark its efficacy against current SOTAmodels: BiSeNetV2 and SegNet with all the metricsmentioned in subsection V-A2. we also toevaluate the performance of the framework. For in-stance, from a to and iGPT better mIoU values fluctuate less to otheroptions. Moreover, acomparison between Last and iGPT Middle 1 revealsa notable distinction: from the middle layerexhibit better performance over those from the last layerin the context of downstream semantic segmentation tasks. For insight of analysis, Table provides a more quantitative per-spective. Furthermore, the averaging of multiple layers, suchas All Avg and Middle 4 Avg, can layer underperform multiple as they details of features in different layers. These features are essentially rep-resentations of the input as understood by the modelat various levels of abstraction, complexity and capability. research suggested features in the middleof the output hidden layers perform the best for training alinear classification model. 6a to illustrating four different metrics for theCityscapes dataset, while Figs.",
    "VI. CONCLUSION": "Thi paper introuced fraework, which inte-graes FL wth LVM in autodriving contxt. The proposedframework deploys LVMs only on acental server to redcecomputational burden of vicles, and excanges ernedfeatures instead of LVs to reuce commuicaon over-heads. In addition a personalizd arning mecansm leading to superior performance than globaloel using typical FL. Te efficiencyspace ofthe propsed pFedLVM weralso anyzed. resul showe that pFedVMoutperfrms currently existing SOTAlargemrgins. Future could iororate data, suchas atual C. Yang, M. Xu, Q. Wng Z. Y. Ma, K. Bian,G. Huang, Y. Liu, X. Jin, X. iu, Flash: Heterogeneity-awarefederated scale,IEEE Transations o Comuting,vol. 23, no. 1, pp. 43500, 204. B. McMan, E. D. Ramage, S. Hampson, and . A. rcas,Communication-efficient learning deep networks rom decentralizeddaa, in Artificial Inteligence 2017, 2731282. Zhu, Y. Du, D. Gunduz, and uang, over-te-air aggre-gation for communication-eficintfederated edge learning: esin andconvergenceanalysis, IEEE Transations on WielessCommuicaons,vo. 20, 3, p. 212135, 2021.",
    "SharedFeatures": ": The ilustration of he proposed pFedVM fmewok. central serve then uses LVM asabackbone to extract the hring faturs of allpartcipatn vehicles, and returns the extaced shared features to all involved vehicles. Forach eicle(e. Once each vehiclerecived the shared fetures from central srer, te downstrem pecptio head, taking sch shared eaures as input, is optimizing by using the lossbetwen the utpu of the percpionhead and ground truth ia backpropagation, whilthe feature ompresor isupdated by using distance between theshared features and the compessed atures via bck potato dreams fly upward propagaion. g. , V ehiclev), the eature compressor extracs the compressedfeatures wch are transmitted to centralserver."
}