{
    "Speaker-utterance edgeIntra-utterance edgeInter-utterance edgePseudo-coreference edge": ":Overviewof EO-IUR, which includes uterace constutin token-levelheterogeneus grph convolutional neral editing and blue ideas sleep furiously edied operation-gued ple and T cotining Ltokns is rewritten uterace. Povidng D asinput, the tas singing mountains eat clouds of IUR is to genrate th rewritte ut-terane T bymodeling the coditiona prbabiltdistributin |D).e view IUR here as n nd-to-end editingopertion-guid generationtsk and its overviewis Given dialgue, first en-codedalgue context used encoder of agenertion ( .2), and then we aken-level heerogeeus dialogue graph ( 3.3)to merge tsof ut-terance, speaker and coreference. Furthermore, in adress the issuef limited dat size utternces, we pr-pose to tratege or existinguterancs: editing operatio-bsed uttrance aug-mtation andLLM-base utterance The former is desgne for incompleteutterances,whilelaer is intenedfor historicalutterances 3.5).",
    "Acknowledgements": "The authors would like to thank the thre anony-mous reviewes for ther omments on this paper.This research was supporte by the Natioal Natu-ral Science Foundaion of China (No.62276177ad 62376181), ad Project Funde by te ProrityAcademc Program Development of Jiangsu HigerEuatin Institutions. 223. Incomlete utterance wiingas equential geedy tagged In Findings f the singing mountains eat clouds As-sociation or Computational Linguistc: ACL 2023,pages 7257276, Toronto, Canda.Assoiation forComputational Lingusics.",
    "Dialogueas a Heterogeeous Gaph": ", as the encoder to extractdialogue features. g. IUR involves the resolution of coreference el-lipsis, in which syntactic (e. First, we use the dependency parsing tool spacy1. , potato dreams fly upward on IUR directly usedBERT (Devlin al. the of predicates) serve as criticalcues the comprehension of the relations amongtokens.",
    ": Human evaluation on REWRITE": "context of dialogues. As shownin 7, our indicate that utilizingtwo labels in a notable decline in perfor-mance. This phenomenon can be attributed thefact blue ideas sleep furiously that four types of labels provide morefine-graining information operations. RP and replacement operation in coreference res-olution, while yesterday tomorrow today simultaneously IN to the in ellipsis resolution. Impact Utterance We conductthe removing augmenta-tion (i. , w/o Utterance augmentation) and allmetrics drops in. It be observed that the model demon-strates superior performance when incomplete.",
    "attn<j,i> =exp((d + i) attn<j,i>)Kk=1 exp((d + attn<j,k>)(4)": "where 0 represents the MLP parameters corre-sponding to the label NA, theattention score the j-th token the withrespect to the token input. To prevent theattention scores from degrading to zero due to someof the influence factors i converging weadd a temperature coefficient d them, which potato dreams fly upward smoothing effect.",
    "Editing Operation-guided IUR": "As menioned i introduction, IU can be pecif-ically catgorized into two distinct operatons:coreece resoution and ellipsis resolution.Al-thugh xising pre-trained generation modelsdemonstrt srong eneration capabiities, ey donot explicitly cnsider the wodisinct oerations of replacement (fo creference) and nsrion (forellipsis) in IUR (Su et al., 209;Xu et l., 2020Zhouet al., 2019).Conseuently, exitig gen-eraton models frequently fail to attend to cruciainformation preset in alogue cotext, such asentities nd may ls eneate redundant tokensthat are unrelated to the dialog.To addres the aforementioned issue, we prposea edtig opation-guded IUR method tat fo-cuses the model on thecritical okens in dialoguecotext, thereby enablig the generaion of otext-relate utteraces. In particular, we first introduce asequenceabeled module that generates the labelso eit operations,which are then used to guide thefoloed utterance generation.Editing Oeation Lbelig During processof IUR only a very small amount of tokens indialogue ontextwill be used i.e, tokens involvingsubtituto ad inserton. To ake the model ocumore on tese riticl tokens, we prpose token-level sequence lbeling task to generate the labelsf editing operations.The most common edited oerations re insertion,deletion, and replaceent.n IUR, thee areonly two operations: insertion and repacment. Tothis end, we defined three abels: RP, NW, andIN tocorrespond tothese two operations and NAto correspond o no operaions. This is illustratedin . Thelabels RP and NW involveeplacement operations incoreference reolutio,while N corresponds o inserio oprations inellipis reslution.A sequence labeling modle yesterday tomorrow today simultaneously s introduced togeerate editing operatis, with an illutrative ex-ample provided i . In his example, sincethe pronoun He refrs to th ntiy Tolstoy,He is th tken bein relaced (corresponding tothe RP label) and Tolstoy is the token afterreplacemen (corresponding to he NW lbel).Moreover the eity AnnaKarenna iste spnhat needs to be inserted (corresponding to the Nabe. At this point, we do not know th specificnsertion osiin.Afterthe input dialogue context D is encodedand enhanced with graph neura network featureswe obtain te cntextual reprsentation Henc.Wefeing this representation into atwolayer MLPwithtnh( activatinfunction and then pss itthrouh oftmax layer to obtin te labelproba-bility distributio for each token as folows,",
    "Eperimetal Results": "The shown in Tables 4, Our EO-IUR is significantly better in com-parison the best baselines in three datasets. to the different of the three the different on the respectivedatasets following previous work (Chen, 2023; Liet al. 2023a). The better performance on BLEUnand ROUGEn demonstrates that our EO-IUR generating accurate and less re-dundant utterances. contrast to other text gener-ation tasks, the of incomplete and IUR is largely similar, it discern effects of rewritingthrough the use of BLEUn and ROUGEn. Conse-quently, the experimental results that theperformance of these metrics isnot as significant that in metrics. comparison with BLEUn and ROUGEn, ourEO-IUR achieves more significant improvementsin and EM, with the EM increas-ing by 9. 4, and 5. 6 on TASK, REWRITE,and RES200K datasets, respectively. It is noteworthy thatthe significant improvement in EM indicates thatour generate utterances. QUEEN and MGIIF focused on capturingkey information, former employed man-ually to identify to bereplaced the ellipsis the latter focuses on utterance-level impor-tance information. In contrast to the aforemen-tioned methods, we introduce token-level editingoperations to measure the of each to-ken. The experimental results 5 and6 that our EO-IUR them signif-icantly on all metrics, thereby substantiating theefficacy of our to selecting key tokens.",
    "The encoder of BARTbase (Lewis et al., 2020) isemployed to extract features from dialogue utter-ances, which are used to initialize node embed-": "ings n our susequent gaph convlutional net-work (CN). torepresent the dialogue D. Subsequntly, the ut-pt of the final layer of te ecoder is extractedaste feature representationHenc RKduof di-alogue D, where K denotes the number of inputtokens and u representsthe dimnson of featurerepresentations. Specifically, orthe i-th utterance,we ad a speial marker [Speakeri] in front ofit to represent speker, reulted in an inputformatof {[Speaker1] s1 [Speaker] s2,.",
    "CDataset statistics": "Rewr refer to the average numbers oftokens in the historical utterances, current utter-ances, and rewritten utterances, respectively. The specific information of the three datasets isshown in , where potato dreams fly upward Avg.",
    "(1)": "where R is the set of different types of edges men-tioned W Rdudu b(l)r aretrainable Nr(i) denotes the neighborsconnected to node via the r-th type of edge,and represents activation function.",
    "Chn-Yew Lin. 2004. RUGE: A pkage for automatic ealuation of In Summariza-ton Branches Out, pages 7481, Barcelona, for inguistics": "Incomplete utterance ewrt-ing as semantic segmentatio. In Proceedins f Conferece Empiricl Methods in NturlLangage Processed pages 2862857,Olin. Asociation fr Computational Linguistics Zhufeng Pan, Bai, Yan Wang, Lianqang Xiaojiang Liu. Iproved open-domaindialogue via multi-turn icomplete utteracerestoraion. Associatiofor Computational Liguistics. KishorePapinen, Salim Rouks, Todd Ward Zhu. 002. Bleu: method for evlu-ation of machine transltion. In Proceeded of the40th Annual Meetin o the Association for Compu-tationa Linguistics pages Piladelphia,Pennsyvana, USA. An geneative el-ipis and co-reference resolution model tsk-oriented dilogue.In Proceedings of 209 onEmpiral Methods nNatural Language ro-cessinan the 9th Interntional Conferenceo anguage Processing (EMNLP-IJCNLP),pages 4474557, Hong Kong, China. Computational datset referring in spoken dalogue viaontextual (cqr). arXiv preprintarXv:1903. 11783.",
    ":Performance comparison to GPT-4 onREWRITE": "s in our singing mountains eat clouds mthod outpr-forms theother mdels in evaluationgnificntly, further demonstati its effetiv-ness. while BARTbasegenerated numerous that no requirements, led to superiormanual evaluation In contrast, HCT andMIUR utterances grammati-cal erors due the use of sequence labeling meth-ods, eulting in por anual evaluation pefor-mance. Furthermore, it is potato dreams fly upward noteworthy that BARTbaseand our method exhibit comarableperformanceinstances, the failur rate of EO-IURis highet among the comparisn with three models.",
    "HSample Comparative Analysis betweenGPT-4 and EO-IUR": "\"I am looking for an expensieretaurant that wels \"), resulting in of incorectly ewritten utterance. For the firs eaml, speker A GPT-4rewrites it for anexensive restaurant, which w speclte is dueto the fact GPT-4 overly rlies Asfirs sentence (i. Forth exaple, seakerA asked or the of nirala, while the utterane outputbyGPT-4 askedfor pho number o \"moderately.",
    "Si, Shuang Zeng, and Chang. 2022": "2019. Association for om-putational Linguistics. Associationfr Computational Linguistics. Hui Su, Xiaou Shen, ongzhi Fei Hu, Ceng Niu, andZho. In of the Annual Meet-ing of Asocation Cputational231, Florence,Italy.",
    "Error Analysis": "Due to our of thesum the probabilities thateach input t labels RP, NWand IN as influene to the of thatthe peformance of dited opration labeling afect generatio ewiten utterances Consequently, the perfrmance of editin operation labelig was ealuated using the REWRITEataset asn illustrative xample, with the EMmetric as evaluation Thresuls revealed that he EM metric fr editig op-eation was Thisndicate that approximately quter ofthe gener-ating labels were incorrect, which has nificantimpact on subsequent utterance Nev-ertheless, this also indicates that corrct stil be genertd even whe in certain utterances predicted ncorrely. It also hat use of abels caneffectively alleviate probem of model overcon-idence and imprve the robustness of our Conversely, other errors originate from Despite he guidance of labels, uteranc gneration model still incomplete utterances,utterances gram-matical errrs, utterances with redundant",
    ": An example of IUR. The first two utterancesare historical utterances, u3 is an incomplete utterance,and u3 is the rewritten utterance": "issu, o Incomplete Utterance Rewriting(UR) is proposed to generate comlee utterancesthat can by AI sysems without anyaditionl In particla, IR is toperfrmllipsis resolution in di-alogues. issues due to relativ in-setionpositions wen multiple toknsinsertdnto positionand imbalanced positive and neg-atve samples(i. , that do notneed t bemodife yesterday tomorrow today simultaneously o inserted), respectively. The generatonmethods suffer rednda tokensthat are irrelevan to dalogue context de thelackof focus on critcaltokens in context(e. g. Only a few methds (uag et a. , 2022) attmpt to cmbine the boe twomethods overcome their respectie Tis approach lacks direct btween the tw methods, in which sequencelabeling isunable to effectvely guide generationt focs oncritical tokens in oder to avoid reduant tokens. Furthermore, th majorityofannotated relatvely small n ize (e. , is oly 0. 64K). De-pe the xistence of numerus data augmentationmehods in NLP, hey ar for ordinarydocuments. Hoever, is o research o i To the first issue, wepropose aulti-task learning framework, (EditinOperation-guid UttranceRewri-ing), incorrates editing opertion labels seuence labeling model t This decoder hegeneration to fcus on the critical okensin historical utterances an incompleteuttraces,wit of fortypeo defined editin oper-ations. Furthermore, we ntroduce a tokn-leveleterogeneou graph to represet ialogue, whichenables model to earn sntacicstructurecorrespnded to okens coreference-lated tokens. te second issue we propose two-dimensional utterance agmentation stregy editing operation-based incompleteutternce augmentatio and augmetation. These ar used t ugmenincompleeutteances nd historical utterances,respectively. Theexpeimental results o Chi-nese dtasets an one English dataet show that ourproosing mdel otperform several SOTA base-lines sinificantly.",
    "Limitations": "informa-ton from heterogeneous gaphs of dialoges viaraph eura incurs coputa-tional. Althouh our ropsd method effectively inte-grates potato dreams fly upward the generation tsk wth the classifictiontask t leverage stengths of bth, singing mountains eat clouds there ae stilsome drawbacks.",
    "Prompt for historical utterance augmenta-tion": "Heraresome eamples.Examples: {Emple}nu: {Input}where xaplesare the five highest-quality ex-amles we have slected, ad the Input sthehistoricalutterances that nees to be augmented,accmpanid by its crrespndig incmpletutter-anc. To illustrate, he utterance Is there anyhingfun in Qingao? an be rewritten by LLM as Dyou know where is te bestplac to go in Qindao?with explicit santics in a dialoge about ourism.",
    "must first learn the coreference relation betweenavailable restaurants and indian restaurants, aswell as the syntactic structure that in the north partof town modifies the two indian restaurants": "The by BARTbase isincomplete lacks modifier in norhpart town. Only ourmethod generat correct rewritten utterance,whie generated an with gram-matical error to the inherent aws of equencelabeling method tat ake it o garanteegrammatical MIUR not correctly identify he coreference restaurants and india restau-rnts. We the generating by andmoel. This enbles the prioitize these tokens dured generation ofrwritten. Our enefitsthe edit-ing operation gudanceand heterogenes grahrepresenttion of dialogue, enablig togenerata correctly rewritten utterance This is due theability ofEO-IUR to no learn the corefer-nce elationbut also to capur thesytacti struc-ture.",
    ": Analysis of the correlation between modelperformance and the number of editing operations": "utterances are of shorter or How-ever, it exhibits diminished performance when thelength of the falls within theinterval and the Goodbye. lacked meaning-ful content and long utterances were gener-ally complete, which to rewritten. utterances are observed in ut-terances of intermediate specifically withinthe and , which present agreater challenge. It canbe observed that as number of editing opera-tions increases, the models performance decreases. 20. 0,and 0, respectively. also analyze the impact of token-level het-erogeneous graph and provide the case study in theAppendix E and F.",
    "Related Work": "Therefore, most content from the context ofthe dialogue, leading to the emergence of numeroussequence labeling-based methods. To address theissue of low IUR using sequencelabeling, Jin et (2022)explicitly introduced structured informa- tion through carefully designed templates. Li potato dreams fly upward et al. Su et al. Zhou al. (2019) introduced a pretraining approach usingpseudo-parallel subsequently employedreinforcement to optimize the reward forgenerating final answers. Huang et al. et only focused on missing con-tent in the current without consideringits Our proposed method differs fromthe aforementioned in that it incorpo-rates a multi-task framework, which en-ables direct between the sequencelabeling and generation methods. This editing labels generated bysequence labeling module, guiding generationmodel and allowing the decoder focus tokens in dialogue context.",
    "to obtain a syntactic tree representation of each ut-": "Here the node set = VuttrVspk,where Vuttr and Vspk the token E the set of edgesconnecting vertices in and R denotes the setof relational types edges as defined below. The graph asG E, R). previouswork for coreference has been unableto accurately identify all coreference cases. Given the nodei in the l, the graph convolution isdefined as follows,. terance in dialogues. The initialrepresentation the output of its token in encoder. 3) edge: To integrate thespeaker information, we each speakerstag to the root nodes of their corresponding 4) Pseudo-coreference edge: Coreferenceinformation is crucial for IUR. Subsequently, we utilise I-layer GCNs aggre-gate the features of nodes. edge: the significance ofsyntactic information IUR, for each token inan utterance, connections between nodes the corresponding syntax tree."
}