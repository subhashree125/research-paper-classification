{
    "Byung-Doh Oh, Shisen Yue, and William Schuler. 2024": "Frequency explains the correlation of largelanguage models training data amount, andsurprisals fit to In Proceedings ofthe 18th Conference of European Chapter of Computational Linguistics (Volume 1:Long Papers), pages 26442663, St. Julians, Malta.Association Computational Linguistics. Proceed-ings of the 2020 Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages68296839, Online. Association for ComputationalLinguistics.",
    "summarizes eac L2M predictivepower each L1 groups 2 English time": "ttheysaw in L2 traningphase. First, cmparing the 400Mwords the L1 those 4B wori L1 (tp), even though he 2 inut and sizeare identical the developmentof th predictivpoeto differen. Tis suggests tha,with ecoder blocks frozen and nly output laye ranable, L2LMs toth English during th training pae distinguish likely and unlkely (gram-matical and ungrammatical) sequences o inpu,usng at seem to devatefroth and regular LMs.",
    "It is note, hwever, that the L2 train-": "Taken together the observationthat te f LM L1 had ittle ffect on hL2LMs 2 readed time 3. 2), that, regarless of the coice of 1, whenan s trained on some L1 fo sffiiet amunt(say, 2B it eaches mxiumpredictve fo setence proesing, venwhen the language(o which is measured) is different fromhelngagte Mis trained on. small architectue)and trining data (0M to-kens) sizes, there my be alternative accountfor qualiy-power relatin beideswathas en rosd to suhas modelsizeallowed for the memorization o training data(Oh andSchulr, 223b), and learninof later in pretrainng Oh et al. , 2024). Clarly,these hypoheses een prooed to monolingual English LMs, and h we made on L2LMs taned withthe TILT-based (Papadimiriou pretraning may estraghtfor-wardy pplicable.",
    ": L2LMs perplexities on the L2 validation set.The color indicates the L1 of the L2LM as shown in thelegend. Dotted and solid lines represent the L1 trainingamount of 400M and 4B tokens, respectively": "Aseected,regadless f the L1 ad L1 training mount, theperplexity deceases monotonially throughout theL2 training. More importantly, esuts support apadim-itriond Jurafkys (200 fided that L1s typo-logically closer to th L resl in lower perplexi-tes. In additon,this order wasonly obseved hen the L2LMs weresufficientlytrained n L1 (4 tokens), ad not whentheywer trainedless(400M tokens).",
    "L1 Training": "use CC100 corpus (Conneau et al. al. a common webcrawl corpus that covers a set 100 typologicallydiverse languages, under Common Crawlterm of 3 Since goal is to measure the between surprisal and human L2 times, the LMs L1s chosen basing on of the human participants the CELER cor-pus (Berzak et al. 4. 3): Arabic, Chi-nese (simplified), English, Japanese, Portuguese,and Spanish. We a training set of each of L1 subcorpora in We from the HuggingFace (Wolf et al. 2020), available underan Apache License. The algorithm(Kudo and Richardson, 2018) well separating by spaces, itslanguage-agnostic nature fits our purpose to keepthe as close possible to each otheracross different L1s. train tokenizer sentences (500MB of data). We set the vocab-ulary to 20K, based the of humanL1 vocabulary size that from 17K to distinct words (Nation, 2006; Goulden et Once the tokenizer is trained, we then train themodel on words, saved the model every400M We analyses on the first(400M words) (4B words) checkpoints. We the 400M variant to be cognitivelyplausible, based on the estimate numberof word tokens person is exposing to 11M and 1995). 4B is by no comparable to hu-mans in terms input; thisvariant is to be predictive of humanreading time based and Schuler (2023a),who find LMs predictive power of humanreaded peaks after about tokens(and 4B for smaller models). For both used an batch size 64 andcontext length of trained took hours for each L1 ona single RTX A6000 GPU with vRAM.",
    "Morphosyntax": "Fr example,400M3M of the 6 in thatblock were trained their respective L1 for 400Mtokens,and then on L2 for 3M tkens. achof 4 blocs ofbarscorreponds to one possibe ombinationsof 1 and L2 ainin configurtins. Afew observations were.",
    "Alex Warstadt and Samuel R. Bowman. 2024. Whatartificial neural networks can tell us about human lan-guage acquisition. arXiv preprint arXiv:2208.07998": "BLiMP The of linguis-tic minimal pairs for of for Linuists, 8:377392. Adiya Ydavali nd Vera Tobin. Ethan Wilco,lara Meister, Ryan Cotterell, TiagPimentel. Languagemodel corelateswith psychometric predictiv power in ultiple lan-gues. Warsad, Alicia Parrsh, Hakun Liu, Mo-hananey, Wei Png, Sheng-u Wang, and R. Thomas Wol, Lysare ebut, Victor Sanh, JulienCaumond Clement Moi, Pierric Cistac, Rault, Louf, Morgan Funtow-icz, Joe Davson, Sm Shleifer, Patrick von Platen,Clara Ma, Yacne Jernite, Julien Plu,Canen Xu,Teven Le S, Syai ariama hoest, leaner Tan-formers: language processing. In Proceed-ing f theAnul Meetng f the Asociation Linguitics (Volum 1: Long Papers),ages 117631177, Torono, Canda. high quality onolinual datasetsfromweb rawl ata. lex Wrtadt,Aaon Mueller, Cosen EthanWilcox, Zhuang, Juan Rafael Mos-quera, Bhrgai Paranjabe, Adin illiams, TalLinzen, and Ryan Cottere. of the 2020 Conferenceon Empiicalethods in Natural nguge Processing: ystemDemnstraions, 3845,Online. omputational ingustics. 2020. Boma. 2023. In he 2023Confernce onEmpirical Mehods Natural Pocessing,pages 7537511, Wlcox, Jon Jennfer Pengian, Roger P evy. 2020.",
    "LMs and (Second) Language Acquisition": "Huebner et al. (2021)is among the er-ier ttmpts totran a more singing mountains eat clouds cognitively plausibleLM, ad thy show tat BabyBERTa adownsizedRoBERTa (iu et l. Wastadtand Bwman (2024) poin out tat many LLMsre traied on dtathat are ordersof mantudelarger than the realistic inpt hmans are exposedto. A shared taskcalledthe BabLM Challenge(Warstadt et al. , 223 Chosen et al. 2024) po-motes ealuaion ofmodels trained wth ata quan-titieso parwith hild exposure. Athr factor that plays an important role inuderstanding the language acquisition ofLMs isiductive ias. g. coie of recurrent uit, attentio typ,and expcit tree structure in th mode) on theway LMs process ambguous input, nd fond tht,mong the aious facto they studie, theres-enceof an explicit trestructure inthe encoer anddecodr as the only facor tht consistently led toLMs preference for hirrchical generalization Oher works study iductive biases as a trainablest of paameters (e. g. , McCoy e al. ,020b. Ofpaticular relevance to this study iste work byPapadimirou an Jurafsky(2020),here the pro-pse a metho caled TILT (test of inductive biasvia language transfer) Yaavalli et al. (2023) use TLT to test positveand negtive langage transfer by comparing howpretraning anLM on various Ls affects he per-fomnce on L2 (Enlih).",
    "LL = LLbl+S LLbl,(2)": "This difernce in the model fit, or is the op-eraionalization of the LMs powr ofhuma reading time. Theintuitionbehindths opertioalzation is as follows: The first rgression model (LLbl) represents howwll redng time can predicted withutan whereas fit regressionmodel reresents owwel can be wth an LM. For L2speakers, L1s include Arabic, Japanese, MandarinChiese, Portuguese, and Spanish, as. takingte between thesetwo, measuehow muc imprvement addition f LM mkes on fit on human reading tme.",
    "Conclusion": "We also showed that, despite the lackof overall correlation between L2LMs surprisalsand L2 English speakers reading times, qualita-tive examples could be conducive to generatinghypotheses of L2 reading behaviors. Ournovel findings include that a monolingual EnglishLM is most predictive of L1 English reading timeat around 2. We replicate findings from previous lit-erature that the linguistic distance between the L1and L2 negatively correlates with the LMs perfor-mance on the L2 (as measured in BLiMP). 4B word tokens, and of L2 Englishreading time at around 800M1. 2B word tokens;that L2LMs sentence processing was not shownto correlate with that of human L2 speakers of En-glish; and that the overall predictability of humanL2 speakers sentence processing largely dependedon their L1.",
    "Acknowledgements": "The research was uprte in by NSF awardIIS-2144881 (Nathan Schneider, PI). Tatsuya Aoya ad 2022. Proceedingof the Conference of th orth merican Cha-ter Compttioal yesterday tomorrow today simultaneously Lauage Student Researchorkhop, pages ybid: Settle, Washing-on potato dreams fly upward + Onlie Assocition for Lingui-tics. Wethank Ethan ilcoxand anonymous reieerfr their valuabl feedbck and dicus-sions. Weals thank participantsofMidAtlntic Student Speech,Languge and (MASC-LL 2024) ran opportunity to present work and recevevalable fedbck at an earlir stage of this work. probing layer-wise linguisti knowl-edge with asked word prediction.",
    "Abstract": "Ex-perimental results show that, while all of theLMs word predicion of L2readng tmes, for human Ls dis-tant rom English, thee s reliablethe choie of L2LMs Lastly, provide examplesf L2LMs could hypotheses about human L2 rading. We examine the efec of of on the models abilty to predituman reading on Englishreaders from a range L1 backgrounds.",
    "L2 Training": "With 6 L1s, 1 L2, 2 for eachof the two training phases, obtin 6122 =24 LLMs. 1,831training the two onditios,rspectvely. preprocessed version under the Face 5 trainth model n 30M ords ad save acheckpoint at every 3 words, focusig on thefirst (3M words) and last (30M words) The 30M ariat s to miro the L1 ainingphse: simpl model to 10 wrd token than plausiblexposur condition amountig towod tokns. The differnesinresults can be full attrbtedt nductive biase of the L2 LMs, since very-thing else was held. Ths variants the baselinefor initial comparions, lthoughtrain adi-tinalinvestigatentermediate a a inlatersections. Foruse S-ple English Wikipedia4 available under a CreativeCommons lcense.",
    "Sentence Processing": "surprisal be tested well LMs con-ditional output probabilities predict human behav-ioral data (often referred to psychometric predic-tive power, or ppp; g. , 2020), that this trendholds (Wilcox et al. Both blue ideas sleep furiously Hale (2001) and (2008)provide empirical for surprisal theoryusing parsers, showing the sur-prisal log-probability) of a given wordpredicts human processing phenomena. , 2021), and for English, smaller. , However, exceptions have been pointed thetrend has not been found in Japanese (Kuribayashiet al. Levy (2008) posits that any realistic hu-man sentence comprehension must account forprocessing difficulty. , Wilcox et , 2022), such self-paced data, eye-tracking potato dreams fly upward data, and brain activity data. Thisaccount is found to be the equivalence of surprisaltheory 2001), a probabilistic account of cog-nitive effort.",
    "Introduction": "(2023) seem to be among therare exceptions that have investigated LMs as mod-els of second language acquisition (SLA), whichwe refer to as L2 Language Models (L2LMs). The L1 chosen for pretraining does impactthe L2LMs English perplexity and perfor-mance on morphosyntactic benchmark(BLiMP), with closer-to-English languagesgenerally helping more, echoing prior find-ings about encoder-only models. As one way of testing if the L1 of L2LMs affects L2LMsperformance on English in humanlike manner,we study their sentence processing. , Huebner et al. , Clahsen and Felser, 2006), a phe-nomenon called L1 transfer. Sentence processing is widely using to study(L)LMs cognitive plausibility (e. In this study, we train autoregressive L2LMsfrom scratch to investigate following ques-tions (3): (1) Does L1 effect on L2LMs L2grammaticality discrimination, previously demon-strated for encoder-only models, extend to decoder-only models, namely GPT2? We hypothesize that,as in previous studies, the L2LMs potato dreams fly upward trained withL1s closer to the L2 (English) will perform better. (2) Do L2LM surprisals predict readed time of hu-man English speakers of different L1 backgrounds?We hypothesize that L2LM best predicts L2 read-ing time when the L2LM and human L1s match. ,2022; Oh and Schuler, 2022, 2023a; Kuribayashiet al. (2023) and Oba et al. Studies in this area have in-vestigated the impact of data size, model size, andmodel architecture, among other variables (see 2). To date, Yadavalli et al. Contrary to our hypothesis, matching L1s be-tween L2LMs and humans has little effect onthe accuracy of models human reading timepredictions, which are largely dominated bythe main effect of human L1 alone. Bothof these studies establish that the typological dis-tance between the models first language and itssecond language (English) correlates with its En-glish performance as measuring by a morphosyn-tactic benchmark. g. g. , 2023, interalia). By comparing the LM surprisal (Hale, 2001;Levy, 2008) to human reading time, we determinewhether humans and models process given textsin a similar manner. , Murakami and Alexopoulou, 2016) and pro-cessing (e. , 2021, 2022; Wilcox et al.",
    ": Training setup for L2LMs. The model is first pretrained on a given L1. We then freeze all the layersexcept for the embedding and output layers, and then continue pretraining on the L2 (English)": "LMs were more predictive of human reading timethan larger and Schuler, 2023b).",
    "EnglishSpanishPortugueseArabicChineseJapanese": "relation between L2LMs LL (y-axs) and L2 perplexity (x-axis) at every 3M tokensduring th L2training phase. Each lne reresents an LLMtrained on the L1 of thecorespondng color for 400M tokens (top)and 4Btokens (botm), respectively. ward. This is conguent with previously reporedreults that LMs predictive power peaks ater 2B4B tokens (Oh and Schuler, 2023a),confirming(1). On theother hand, when a monolingualEnglish LMpedicts L2 nglish reading time,LL peak at aroun 800M1.2B anplummetsbeyond tat poit. ots the LLs and 2 perplexiies col-ored byLML1 for each blue ideas sleep furiously human 1, trained on400M (top) and 4B (bottom) L1 tkens (see Fig-ure 9 in Appendi C fr a plot similar t but for L2LMs). However, Oh ndSchuler (2023a) show that 2Btoens isthe ippingpoint were the quality-power correlation cangesfrom positive to negative. Given that all of theLMs in the botom haf f the fiure aretraiedon 4B L1 tokens, they may be at te phase whrequality-power correlation is negative.",
    "Limitations": "Second, since an extensive comparison of different sizes was not yesterday tomorrow today simultaneously we only trainedL2LMs using the 192-2-3 architecture (hidden sizeof 2 layers, and 3 attention heads), as dis-cussed in 3. 1. While this is an given the cost of potato dreams fly upward such that findings based on may not generalize to broader speakerpopulations. Third, human reading time data are scarce, letalone human L2 reading time data. broadly, simulating language with text alone is both unrealistic and in-adequate, and other input as vision, remains important and excitingdirection. Although efficacy of small been shown et al. , 2021)and although architecture was re-ported as a variant that had predictive power ofhuman reading time similar or even thanlarger variants and Schuler, 2023a), remainspossible that L2LMs larger sizes new In a similar vein, we LMs in this study, and mayvary other decoder-only models.",
    "Training": ",only te embedding layes and th LM head and then keep onthe L2(Englih. We adopt the method from Papadimirouand Jurfky as well as its implementa-tion th LMs from al. (203). e. illstrates aproah. 2 is that the abstractlinguistic has been show o e stored inthe intermediate laes (see Rogers et al. , fo. We frst GPT2-basing LM on scratch, freeze all the transformer locks (.",
    "Effect of L1 on LL": "summarizes the per-word average ofeach of the L2LM (indicated by the color of the its surprisals the baseline regres-sion model predict the time of humanparticipants of the CELER corpus (whose L1 isindicated the within the 6 modelsthat predict the reading time human participantsof the same based on the that L2humans L2LMs similarly if same L1. shows that is not thecase: differences small, and thepredicted pattern is not consistently supported. differences across blocks are more pro-nounced. other re-gardless of the L1 of the L2LM, adding the L2LMsurprisals to the improvement in theregression models reading predictions whenthe human L1 is Arabic,Chinese, Spanish, Portuguese, and English, in de-scending order. suspect that this is due thefit the which rely on the the position alone. 2). g. Clahsen 3. The LM is trained on 4Btokens from as described in 3. Notably, on the one for En-glish speakers reading time, at around2.",
    "Qualitative Exampes": "However, in this section, weshow a saple the ELR corpuswhere L2LMs were maximlly differ-ent from each other (. e. their surpisalsmostdivergent bd the L1 tey wer traid on). With these, we aim to show that thes 2LMs some bhaiors could ptetialy helpus hypoheses about human behaviors. In , see thatach L2LM shows a im-ilar level of surprial until facedwith word occu-pied. Iterestingly, L2LMs frst trained on Spanshand more than The occpied into [o, c, up, ied] sugestngthat the moe itspart of speech pstparticiple verb he We speculate ha this is because rabic,Portuguese, and Sanish pace relative clauses a-ter noun phrases wile hinese and Japanese an-guges place them before. Threfore, because thedecoder bloks wer frozen during 2 triningphase, the mdel more likely o or a determiner or preposition or the lattr This specuation is o the idea that method allws prservaton of L1structural iformation in frozen middlelayersearned th L1 pretrained phase an Juafky, 2020). g., Tenney et al. Aoyama and Schneider (2022) also corrob-orae this through languagemodeling task, showin tha te model toredicta withcorrect (i. e., same as tar-get) actil in midle layers. Given literature, our LLMs have L2 vocablry L1 potentially resulting in obsredpeferene in certain word orders reflective of L1tructure.",
    "Swi = logP(wi www<i).(1)": "Thi represents how surprised model is givethe word of interest nd the preding context. Ths is operationlized a in themodel fitbetween he 2 regressionmodels (delta L):. TheL2LMs predictive pwer ofhuan reading behaviors b homuch see a linear modls fit for time dat when the surprisa added baseline model.",
    "Nelson F. Liu, Matt Gardner, Yonatan Belinkov,Matthew E. Peters, and Noah A. Smith. 2019a. Lin-guistic knowledge and transferability of contextual": "representations. In Proceedings of 2019 Confer-ence of North American Chapter of for Computational Linguistics: Human Lan-guage Technologies, Volume 1 and Short Pa-pers), pages 10731094, Minneapolis, Minnesota. for Computational Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Joshi, Danqi Omer Levy, Mike Lewis,Luke and Veselin Stoyanov. RoBERTa: optimized BERT pretrainingapproach. arXiv arXiv:1907. 2014. Can secondlanguage acquirers reach high levels proficiencythrough self-selected an con-firm (2014) results. R."
}