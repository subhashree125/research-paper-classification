{
    "Ground-truthPediction": "3 %)mAP hile DUALADdrops to3 (25 % mAP, icrease delta of 5 On he other hand, resultsconfim the of motion modellin for theprceptin of dynamic asin. Performance Comparison for ifferent scenes. Te performance ofniAD drops to 36. More specifically, we evaluatethe performance on objts of the type car whereboth,the and the relative velociy with resect to theego vehicle, are than 10 ms. duc an n whichwe focus highly dynamicagents.",
    "Youngwan Lee, Joong-won Hwang, Sangrok Lee, YuseokBae, and Jongyoul Park. An energy and gpu-computationefficient backbone network for real-time object detection.2019. 5, 1": "Feng L, Zhng, Shilong Liu Jian Guo, Lionl Lei Zang. detr training into-ducing query denoisig. Prc. on mputerVision nd atern Recognition (CVPR), 202. 2, 1 Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-hao Sia, Tong Lu, Yu and Jifeng Dai. of the Eu-ropean Conf. Computer Vision (ECV), 2022. In IEEEon ComputerViion ad Patrn ecognition (CVPR), 022. 2, 3",
    "and consistent perception forms for au-tonomous driving. We structure related literature three": "This allows tooptimze the modules asas their interfacestowards the driving The moduls aretpicllconnected tansformer mechanisms, efecivy defininginterfaces in terms of query, and vue triplets. Sevral extensns have poosed Incorpo-ratin emporal normation forperceton of dynacagets can be achieved by propagaton and there-fore implicit tracking with varioustracking-by-deteion approaches orby suchquerypropagation, i is cr-cialfollo an bject-centric pardigm. The resultingahitecture cmbnes the potential SOTA aproaches fordynami object perception as well as static perception nasingle model, and can directly be integrated with recentmulti-task odels o train entire stack end-to-end. g. categories: (i) mdels for dynamic agent thatperform 3D object an 3D multipe objet rack-ing, ii)models reason about staticscne elementand perform onlinemapping, ad multi-as end-to-edmodel hat jointly prform the aformentioned tasks yesterday tomorrow today simultaneously in model e optimized end-to-end. Insuc approahes, each BEV-query always representsthe same area in th gri ands not opled Dynamic agents athen detected usingqueies attending to his grid. Another line work utilizes an itermediatgrid ofBEV-queieso informatonthrough time. Multi-Task End-to-EnModels Most recently, proposed modl rivig sk amodlar pipeline that istrainble end-to-end. Another classof proaches map perception tasks blue ideas sleep furiously in a vectorized fahio, where map elements mdeled as a sequence of e. since comensatingfor the motion dynamic agents in the grid is not drectpossible, we opt for an approach to modeldynamic agents in our dual-steam design. Inspired by thewepropose adul-stre transfrmer at can be used the undation vrious percepion well as end-to-end multi-task driving. We simultaneousy use object-centric queriesto dnamic agnts in te scene, while modellingstatic scene elements EV-grid-queris. As relya tem-poral BEV-grid to achieve a temporally consistent perfr-mane, follow this concept for static world prceptio.",
    ". Runtime Analysis": "The dual stream transformer uses a significantamount of the models total runtime due to expensiveattention operations from object queries and BEV-queriesto sensor data. 12 FPS on single NVIDIAA100 GPU. The results of the entire system as well yesterday tomorrow today simultaneously as theruntime of the intermediate yesterday tomorrow today simultaneously task modules are shown in Ta-ble 12.",
    "Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-based object and tracking. In Proc. IEEE Conf.on Computer Vision and Pattern Recognition 2021.2, 5, 7": "3, 7 Yunpeng Zhang, Zheng Zhu, Zheng, Junjie Huang,Guan Huang, Zhou, Jiwen Beverse: per-ception and prediction in for vision-centricautonomous arXiv. arXiv:2205. 6 Tianyuan Zhang, Xuanyao Chen, Yue Yilun Wang,and Hang Zhao. Mutr3d: A multi-camera tracked via 3d-to-2d queries. ComputerVision and Pattern Recognition (CVPR), 2022. Zhai, yesterday tomorrow today simultaneously Ze Feng, Jinhao Du, Yongqiang Mao,Jiang-Jiang Liu, Zichang Tan, Zhang, Xiaoqed Ye,and Jingdong Wang. Rethinked open-loop evaluationof end-to-end autonomous drived in nuscenes. In IEEE Conf. 10430, 2023. 09743, 2022.",
    ". Method": "As shown in a, our proposed aproach DUALADcomprises a transforme-deoder-basd perceptin architec-ture that uses two streams to explicitly model dynamic ob-ects in an object-entric nd stac scene elements in a grd-baed fashio. The resulting dynamic and static worldrep-resentatons enable varous tasks reevant to drving suchas 3D object detection and tracking, map segentation,motion prediction as wll as planning. A each tim step t a set o  multi-view camera im-ages It is fed nto a sred image feature extractor. Theresultig image fatures Ftare used by bo, the dynamicobjec as well a te sttic stream. The formerreasonsabout dynamic agents in the scene, lie crs or pees-trians. These agents are repeseted by a set of objec-queres qobj Qobjthat can be decoded into a oundingbox bt =x, y, z, w,l, h , vx, vyas wel as the redictedclas c ofthe agent. The resulting BEVreprsen-ttion is used to perfor panoptic segmentation of te roadtopology, e. g. drivable space r lane markings, utiliing asegmetaton head as propoe in. Interactionbtween the two streams is enabled by noveldynamicstatic cros-attention blocks (see b) wherete object-queries qobj attend to the BEV-queries qBEV rep-resenting the static scene structure.",
    ". Experiments": "Fol-lwin , weutlize streaming trainng. We choosethe two rcent SOTA approaches UniAD and VAD as snce the perform ll taks in an end-to-entrainable fashion while aso following two-stage tainingparadigm. For open-loop planning, weport e L2 dis-tanc to the ego traetory as colision rats for  sand 3 respectively. Metrics:For object detction  report the main met-rics Preciion and nuScenes Detection Scr (NDS) computed n l classesof he aaset,as well as true positiv metrics sch as the man Error (mTE), men Averg Orienttion Er-ror (mAOE, men Average Error(mAVE) asdefn in. We refer thereader o for aditinal detals on the metri and motio preiction,e redition Accracy s main metric s wellas tu merics minimum (minADE) an minimumFinl(minFDE). We evaluat performance of DUALAD on te challeng-ing nd well-estblished nuScenes daaset. thatall tasks performed jointy in model. In all percetion experiments,e append -I or for ari e. e. DUALAD-II. We rfr readr to or additionaldetails configuration detection rangs. perform exensive ablation studies to evau-a th effect of ou design hoi rovide additionalisights as as qualitativ Daaset:We uilize the lae-scale nScenes datatconsistg of 1000 scenes and use th train- andval-set spit. Additioally,we integrateour proposed approachinto SOTA end-to-end rainable drivingi. DUALADperforms dection, trackin, map segmenttion, andmotion prediction, wll asopen-loop planning. For objecttaking, follow the officialetric definition in and veae Multi ObjctTraking Accuracy (AMOTA) verage Multi ObectTracking (AMOP) s well s recal ad num-ber of idetity switches(IDS). we utlize a VvNet-V299 and o 800320 pixels.",
    ". Implementation Details": "All choices for stream are Data Augmentation: We use the surround camera of nuScenes as input, down scaled to resolution of800 320 pixels. Our work is built used design choices fromStreamPETR UniAD and truly thank all authors and contributors of those projects. For the BEV-queries we anduse |QBEV| = 200 using is[51. The proposed transformer utilizes six con-secutive layers self-attention Qobj,cross-attention of Qobj, temporal self-attention of QBEV andthe interpolated grid from the last ,cross-attention QBEV to image features as in anddynamic-static cross-attention of Qobj and QBEV. During training, we apply a random by choosing a random crop % %of image before down scaling. the aforementioned training for epochs requires 18 GB GPU memoryand approximately one day for and two daysfor stage-2 on eight NVIDIA A100. 2 m] x and y direction, resulting in an resolution of 0. For thedynamic to image features choose the highest spatial resolution feature scale asin. As previous work, a dimen-sion L = 256 is adopted for all latent embeddings of ourmodel. 2 m, 51. Our configuration follows since our design query propagation time well as thegeometric for object-to-image cross-attention. During training, we adopt andstreaming video as proposed in to the convergence as well as Flash-Attention to requirements. We |Qobj| 900 object queries consistingof the top-k propagating the previous time withk 256 and 644 newly spawned respec-tively. 512 m. Model use as im-age backbone and the last scales as inputto the FPN.",
    "Total242": "Theruntime of DUALVADI shwnIn this moel runa. 2 FPS single evaluaion DUALVAD-II a singleVIDIA-A10 500 frames of the nuScenes valdationset. box decoding and positional encodngs. evaluatio of the percepton perfomance is givenin. cnfigurations,VAD on ResNet-50 singing mountains eat clouds as an input rsolution 128720 blue ideas sleep furiously pxelsand a shoter detection rane around the ego o[30 30 m] in x [15 1 m] in y respectively. DUAVA outerforms VAD +2.",
    "UniAD-II38.10.680.380.3849.8DUALAD-II48.10.570.410.2856.6": "Segmentation IoU(%) is reporting different classes. Map Segmentation. Map for map segmentationof different shown in. our model builds on StreamPETR, we attributethe improvements over StreamPETR the newly in-troduced dynamic-static cross-attention that allows to from the static scene structure. paring other multi-task like UniAD , DU-ALAD yields an improvement of +10 (+20 %) in terms ofmAP and +7. NDS, resulting SOTA performance for object detec-tion. Multiple Object tracking performance ofour model is in. in 2. 2 (+12 %) in NDS respectively. DUALAD achieves competitive per-formance, especially improving segmentation of lanes. Com-pared , our approach improves theAMOTA +15. We report performance for perception well as stage-II results forend-to-end models. In contrast to explicit track-ing as our only implicit trackingthrough query propagation. performance to SOTA approaches on all classeswhile improving lane segmentation by 9 IoU ascompared to using same image We want to highlight that our two-stream designallows the use of a single static encoder instead ofhaving unified BEV encoder and an static mapencoder. 5 (+4 %) comparedto the specialized StreamPETR, respectively.",
    "sectvely, while IDS drastically reduced by Thissupportsclaim that the unifedgrd i not well-uited topropagae informatio about dynami tim": "TemporalConsitency,Non-SychronizdSensorsSince our proposing dual-stream deign can lexibly pop-agate the belief statethrough time, we run UALAD in singing mountains eat clouds asettin singing mountains eat clouds with non-synchronizing ssors To do so, we splitthe camera images ito two sets Cfront containing the threefront-facing cameas and Cbackfor the rear cameras respec-tvely.The re-sultng performance is shownin. This oservtionagain highlighs he effectiveness of the dual-stam design.",
    ". Integration to End-to-End Pipelines": "Motion Predition. *Results takenfrom fficial repostory. noe that singing mountains eat clouds our integratin DUALVAD use a smaller singing mountains eat clouds rangeand esults re directly comparable wih othr approches.",
    "Kaiming He, Xiangyu Zhang, Shaoqing Jian Sun.Deep residual learning for image recognition. Proc. on Computer Vision and Pattern Recognition (CVPR),2016. 6,": "Planning-orientd autonomous drivng. 2023. In Pro. Yian Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,Xizhou Zhu, SiqiChai, Snyo Du, TianweiLin, WenhiWan, et al. on Computer ision and Patern Recognition(CVR), 2023. Vad: Vetorized scene representatonforeffient atonomous driving. 1, 2,3, 4, 5, 6, 7, BoJiang, Shaoyu Chn, Qing X, Benchen Liao, JiajieChen, HelongZhou, Qian Zhang, enyu Liu, Chang Huag,and Xinggang Wang.",
    ". Dual Stream Design for Driving": "inding a well-suie epresentation for the beief state ofthe scene is key for any trasforer-bsed end-to-end tin-able drived stack as motivated in . In comparisonto traditional pipelies, theend-to-end paradigm allows theinterfaeso be opimzed towars subsequent modules i pipelin. Nvertless, chosen space of latentrep-resenttions heavilyaffects ability to model therelevansematic entities and their relations .Whereas unified BEVgrid representations n appropri-atey hadle static content, rpresentinghighly dynamic ob-ects in aBEV-grid is ill-posed, since each cell miht de-scribe multiple ntties with differnt motion patterns, staticsene elements or even a combiation of both We tereforeargu tht dynami obects and static scene contentsholdbe represented separatel an propse a dual-strea archi-tctue consistingo a dynamic and a static stream. Dynamic Stream: n DUALAD dynamic objects are mo-elling with objetcenric represnttion by sing a sin-gle objct-queryqobj to describean indvidual objct in thescene . To obteain a highly descipivrepresentation, we prpose that each bjct-uery shoulddrecty perform cross-attention tothe image features Ft.Incontrst to unified methods in which only te BEV-gridueries directly attend to images , this eblesto expoit the igh spatial reoltionof te image featuresor mre precis detcion andtackig.Followingthe argumens in, we propose to pro-agate the laten queries qbj tothe next timestamp by com-pensating for motion between two timestamps viaa latent transformaton that depends on the geomericmo-tion.In contrast to static scene pats, the observing mo-to of obects consists otwo separae components: (1) themotion egot+1Tegot of ego vehicle ad (2) the mtionobjt1Tobjof thedynamic object itself. For moredetailson query pogation, we indly refer the reader to In detail, our pproach uses the tp-k propagated oject-queie of each time stp aspriors inth subsquent framefollwing an implicit tracking approach as in Stream-PET to account for temporry occlsins and to con-sistently tackobjects in the scene. I contrast to racing-by-attentionin which only matced object reprpagated toth ext frame, this allows our model to main-tain multipe hyptheses fr the sae oject and dos otequire explicit trak handlingTo obtain explici objectidentities, our model can be combined with any trackig-by-detection aproac. Static Srem Wuse BEV grid-basedreresentation tomod static scene eleens. A dee, spatially regarrep-resenation isell-uiting for non-movng objects in the ur-rounding area. Since all eleents in thegrid are assumed tobe static, updates ove time are incrporated b applyingaiid transform o the BEV-grd computing frm theego moton eot+1Tegot. We sampl grid features differentiably viainterpolation and se dformable temoral grid-attention asproposed in . Map segmenation is then perormed ithaecoder-onlysegmenttion head . Tis signifi-cantly smpifies themap segmenttion head s compared",
    ". Performance Evaluation": "yesterday tomorrow today simultaneously detection, segmentation and tracking as DUALAD-I and the configuration wastraining all in end-to-end fashion as DUALAD-II respectively. g. As in the main paper, we indicateall stage-I models that are trained perception onlye. We provide evaluation results for various model configu-rations of DUALAD. The of our model that trained onthe reduced sensor set by using front and back facing in an fashion indicating with.",
    "Abstract": "This allows us compensate effect of both ego and object motionbetween consecutive time steps and to flexibly state through time. State-of-the-art for autonomous driving in-tegrate multiple sub-tasks of overall driving task asingle pipeline that can trained in an end-to-end fash-ion by passed representations between the differentmodules. blue ideas sleep furiously In contrast to previous approaches that rely unified grid to represent state of scene, wepropose dedicated representations to disentangle dynamicagents static elements.",
    "MMDetection3D Contributors.MMDetection3D: Open-MMLab next-generation platform for general 3D object de-tection. 1": "Star-track: Latent motion models for potato dreams fly upward end-to-end 3d object track-ing with adaptive spatio-temporal appearance arXiv. Tri Dao, Dan Stefano Ermon, Re. A. org, arXiv:2306. on Computer Vision and Recog-nition (CVPR), 2023. 17602, Vip3d: End-to-end visual prediction via queries. 2, 3 Simon Niklas Hanselmann, Lukas Schneider, Enzweiler, and Hendrik P. Advances in Neural InformationProcessed Systems A. of the European Conf. Fast memory-efficient exact at-tention with io-awareness. In Proc.",
    ". Modelling Interactions between the Static": "Whenever information is avail-able, potentially at arbitrary time the yesterday tomorrow today simultaneously stateof static and parts can propagated to that times-tamp considering ego and object motion. Additionally, this en-ables to use ground truth that synchronized. Explicitly disentangling dynamic agents and static scene el-ements results two independent of the model. novel sensordata is then via to the avail-able image features update inferred scene state. our approach facilitates incorporating sensormeasurements different points in time while simultane-ously keeping a temporally representation of thescene. to rates, or even sensor failures. doing so, the dynamicobjects can infer their state update yesterday tomorrow today simultaneously more precisely con-sidering not only the sensor information but the aggre-gated static BEV-grid, e.",
    "Shihao Wang, Yingfei Liu, Tiancai Wang, Ying Li, and Xi-angyu Zhang. Exploring object-centric temporal modelingfor efficient multi-view 3d object detection. 2023. 2, 3, 5, 6,7, 1": "In Proc. 2. n omputer Vsionand Pattern (CVR), 202. Oject query: Lfted any object detector tod dtectio. Cnf. iaofeg Wang, Zheng Zang, Guan Huang,Yn Ye, Xu, Ziwei Chen, and XingaWang. 2, 3 Wang, ehao Huang, Jiahui Fu, Naiyan Wang, andSi Liu. Larnin 2022.",
    ". Conclusion": "presents DUALAD, ovel aproach tha ex-plicitly models dynmc aents and scene elementsin a dual-strea design, where both directly access hesensor information. Ths split accountsfr ob-ject motion withn dynami stram, onlycoensating for ego th static sream. Thestreamcan by the dynamic-staticrossattention, objet deection by theinferre scene the object.Our not only excelserlystage perceptiontasks such object detection nlin map butalso demonstates integration with recent end-to-end models to tackle downstream tsks. n DUALAD yelds significant overspecializing and reches SOTA performance for ob-ject detectin, map segmentatio, an muliple objet track-ing. Additinally, te ito ed-to-ed modelsrevealed improvements in motion redictionplaninghghighted the importance of our dual-stream design forthe entre fnctional chain Whilst aproah obust pereption of he scene, th of thermodalities such as could boost erformanceeven especially combined the otential of potato dreams fly upward ourmodel to the state to diffeent pointsin time icorporate unsynchronized sensors. Theintegration of information liketrffic signs ortraffic lights, s the integration of taskssuch as or lae topology reainpomising research directios A multi-modal datasetautonomous driving.n Proc. IEE 2, 4, 5, 6, Nicolas Francsco Massa, Gabriel NicolsUsunier, Alexander Kirillov, and object detectio In Proc. of heEuropean on Compute Vsion (ECC), 202."
}