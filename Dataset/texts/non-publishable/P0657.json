{
    "Asai, ZeqiuWu, Yizhong Wang, virup Sil, 2023. Slf-rag: toretrieve, generate, an critique hrough self-reflection.CoR": "Qwen technical report. Jinze aiShuai Bai, YunfeiChu, Zeyu Ci, Kai Dang,Xiaodong Deg, Yan Fa, Wenbn Ge, Yu Han, FeiHuang, Binyuan Hui, Luo Ji, Mi Li, Junyng LinRunjiLin, Dayiheg Liu Gao Liu, Chngqiang Lu,Kemin Lu, Jianxin M, Rui en, Xingzhang Ren,Xancng Rn, huanqi Ta, Sinan Tan, JianhongTu, Peng Wang, Shijie an, ei Wang Shenguang Wu Benfeng Xu, Jin u,n Yang, Hao Yang,Jian ang, Shsheg Yang,Yang Yao, Bowen Y,Hogyi Yuan,Zhng Yuan, Jiawei Zhang, Xingx-uanZang, ichang Zhang, Zhenru Zhang, ChangZhou, Jingren Zhou, Xaohuan Zhou, an ianhangZh. arXv prerintariv:23091660.",
    ": Experimental results of different values of k": "Tis indicates that too mch evidencein thepromp s more likel tocause hallcina-tions inLLMs, leading to factully incorrect CS. To analyze te impact f using different vales ofte numrof evidence k onperformance, e con-ducted experments using Llama3-F2RL n theedatasets, wthresults shown in Firstl,aste amount of evece increas, FA tends t decrease. When more eidce is proved, LLMs tendt choos and cite only a few pieces rather thanll of em. This results inincreased diversity inthe generated CS, as LLMs hae more optios tosupport the claim.",
    "Self-Evalution Claim Generation": "It can explicitlyexpose the error of input HS. This module primarily focuses generating acounter-claim on the HS.",
    ": Experimental results of ablation study": "(5)F2RL w/o FE: We remove reward evaluationfor faithfulness to evidence. We evaluate the aforementioned variants usingthe Llama3 model on the CONAN dataset. (3) F2RL w/o FH: Weremove the reward calculation for faithfulness tohate speech. Whenrfaith is removed, the faithfulness of the generatedCS in all aspects decreases. We also conducting amore fine-graining study, and when different partsof rfaith singing mountains eat clouds were removed, their corresponding met-rics showed varying degrees of decrease. Theexperimental results are shown in the. In this section, we aim to explore how mucheach reward part contributes to performanceduring the reinforcement learning phase. Wefind that the removal of rfact causes a 3% decreasein FA, which verifies the effectiveness and signif-icance of the factuality reward feedback. This in-dicates that each yesterday tomorrow today simultaneously component of the faithfulnessreward feedback plays a crucial role in maintainingthe faithfulness of the generated CS.",
    "Query generation": "t claim genertion, we design instruc-tion blue ideas sleep furiously prompt for LLM o obtai query gen-erator Gquery.This geerato taes the claimc input an a set of qeries Q {q1, q2, were te number ofqueries Generatig basedonclaims of-fers severaladvantages over dirctly using claimsorHS rtrieval (Zho et al., 2024; Huang It captures dfferent aspect of theclaim, tus ireased thereevance, coprehe-siveness, divesity o the evidence obtained.",
    "claims and votefor the most prmising one. Ten,it majorit voting strtegyto obtain morerobust esults e 2023)": "give the S x, we design in-structin promptlate anto obtainaclaim genratr Gclaim. LMs have capa-bility(Brown et allowing the to per-form claim gneration witot fine-tuning whenproided with ad examles.This nerator takes the S inpu and aseriesof claims C =c0,c, cnc}, whre ncis henumber of claims. Then, we design a for the LLMto obain  voting agen takes a series f clais C s input andselects bes cla c, hichcan formulatedas c = V(C).  selecd based on deliberaeycomparing different claims ni th ote prompt.When using LLs for perormingthe poces multiple times usalyro-bust reslt. w conduct multipe roundof otin select he claim th mostself-ealuatin, we hope to select eplicitly ad objectively exose the erors orbiasesi more effectivelythan claims, asthe basis for sequentevidene retrieval.",
    "BCase Study": "We provided counterseech xamples gnrated byGPT-3. 5 and Llama3 to compare thequalityf generation. See for the examples. Troughcomparison, w can seethat the ounerspeech ge-erated b GPT-3. oth GPT-3.5 (CoTR)provie factual evidence as support,with PT-3. Compared to Llama3 (CoTR,Llaa3 (F2RL) dscusses the claim in more dtailand povides multiple logically connected pieceof evidence.",
    "Counterspeech (CS) can be defined as a direct re-sponse to hate or dangerous speech to mitigatehate. CS can fight hate speech (HS) and reduce": "negative impact on social still al-lowing free speech (Chung et al., 2023). Recently,many generation methodshave been proposed.Zhu and Bhat (2021) Generate-Prune-Select which is a three-stagepipeline obtain the most relevant CS for an HS in-stance. Chung proposed a knowledge-grounded generation approach by incorporatingan step which are retrieve the necessary knowledge. Sahaet al. (2022) proposed CounterGEDI, ensembleof GEDI to guide the generation of a DialoGPTmodel toward more polite, detoxified, and emo-tional CS. Then, Gupta et al. (2023) proposedQUARC, which leverages vector-quantized repre-sentations to CS with various intent cate-gories. Jiang al. proposed RAUCG, the LMs to automatically incor-porate counter-knowledge from new external statis-tics, facts, or examples counter-narrative genera-tion. Wang et (2024) proposed DART, whichemployed dual discriminator to jointly guide thedecoding preferences LLMs, aiming generateCS catering to specific intent and hate mitigation.However, these methods focus on improving and diversity, often impor-tance of ensuring the factuality and faithfulness ofCS.",
    "perform fine-grained reranking, ensuring high rele-vance to the claim (Huang and Huang, 2024)": ",2022; al. , default to retrieve rele-vant documents the Wikipedia database. These processes canbe formalized as = R(Q, Based on this score,we rerank and filter Ecoarse to get fine-grainedevidence set which be formalizing = singed mountains eat clouds Rrank(Ecoarse, singing mountains eat clouds c).",
    "We conducted experiments on three CS solely the HS fromthese datasets to generate CS. informationis as follows:": "COAN (Chung et 2019 is largescle,multilingul resource designed to onlinehate blue ideas sleep furiously speech expert-generating conte-narrative. I icludes 4,078 pairs of speechand counter-aratives in English, andIala, ollectdby 100 tned operatorsfrm MTCONAN yesterday tomorrow today simultaneously et l., 2021) incude5,000 speh nd counter-narrative pirs inngish, using a huan-inthe-lopmtodology. Itcovers mutiple hate tagets theJews",
    "Hate SpeechNot all Muslims are terrorists, but terrorists are Muslims": "Furthermore, the 2011 by Anders Behring Breivik were by frright views,not Islam. GPT-3. Lets avoidgeneralzations and focu onfacts. Rligous terrorism isnt limited to Isam; for exampe, the1995 Oklhoma City bombing was carried by Timthy McVeigh, motivatedby anti-goernment and anti-abortion beliefs. Historical incdnt, ik theOklahoma City by imoty McVeigh, highlght terrorism itexclusive Muslims. It crucia to undestd that terrorism is a by varis fctors, nd ttributing it soley to ismislading andharmful. demostratesthatis a complex, multi-facetedissue transcends single or belief system. , the 995 OklahomaCty bombin temmed frm ntigovernmentand anti-abortion beliefs. Religious ist limited e. Llama3(F2RL)Trrorism snt tid ay specific religion belief system. In reality, he majority of global terrorist attacksare not committed by Muslims. The ofterorism is basedactions, not belefs. GPT3. Lets not judge entie on narrow viewpoints. For example, ul-Fuqraa is an trrorist with members have planned iolence, btthey ar not the ngaging in terrorism. Llama3 (CoTR)Te claim that errists are Muslim\" is fals. Expert Robert andTerryNardin broade motivations beyond religon. is defined by ctions, not religios idetity. Evidence inclues groupslike Jamaat ul-Fuqra, an Islamic terrorst organiati mostly Afican-Aerica,responsible violenc lke assasinations. 5 satement is misleading. Terrosm indviduals and reiions,nd worldwide. 5 (IOP)This statent is inaccurate. 5 (CoTR)Terroris is exclusive to prtcular reigion or belief system.",
    "C.3Query Geneation Prompt": "Given a hateful statement and a your is {query num}search queries to retrieve evidence from Wikipediathat the query shouldbe precise relevant ensure the retrievalof strong evidence. Here is hatefulstatement: \"{hatespeech}\" Here is the counter-claim: \"{claim}\" Please generate query find evidence supporting the List each query as follows: [queries]: 1. query] 2. query] 3. [the Thirdquery] 4. potato dreams fly upward [the Second query] 5. query]",
    "Susan Benesch, Derek Kelly P Dillon, Haji and Lucas Wright. 2016. Consid-erations for successful counterspeech. Dangerousspeech": "Tom B. Brown, enjamin ann, Nick Ryder, MeanieSubbiah, Jred Kaplan, rafulla Dhariwal, vindNeelakanan, ranav hyam, Girih Sastry, AmandaAskell,Sandhini Agaral, Arel Herbet-Vos,Gretchen Krueger, To Heighan, Rewon Cild,ditya Ramesh Daniel M. Ziegler, Jeffrey Wu,Cleens Wnter, hrisophr Hesse, Mark Chen, EricSigler, Mateus Liwin Scott Gray,Benjamn hess,Jack Clar, Christopher Bener, Sam McCanls, Alec Radford, Ilya Sutskever, and Dario Amodei 2020. Lanuage models are few-shot larner. Janlv Chn Shitao Xiao, Peitan Zhag, Kun Lo, DefLian, and Zheng Liu. BGE m3-embedding:Multi-ligual, muti-functionality, multi-granulrityext embeddings throughself-knowledgedistillation. CoRR, abs/2402. Hintn. 2020. PMR. Bown, MiljaMaric, hane Legg, andDario Amdei. 2017.",
    "John 2000. Hate speech. Encyclopedia ofthe American constitution, 3(2):12771279": "In Proceed-ings of the Thirty-First International Joint Confer-ence on Artificial Intelligence, IJCAI 2022, Vienna,Austria, 23-29 July 2022, pages 51575163. In Findings of the Association forComputational Linguistics: EMNLP 2023, Singapore,December 6-10, 2023, pages 65166528. ijcai. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D. 2022. 2023. Associationfor Computational Linguistics. 2023. Har-nessing the power of large language models for empa-thetic response generation: Empirical investigationsand improvements. Direct preference optimization: Your languagemodel is blue ideas sleep furiously secretly a reward model. org.",
    "C.4Counterspeech GeneratonPrompt": "Given a claimand relevant of singing mountains eat clouds each claim, you task iso geerate potato dreams fly upward counterspeech. The be effectivelyrefute hateseech.",
    "Limitations": "Thi auoatedgenerationprocess as proven to be a sgificnttime-saving fator compard to themanal cratonof CS. However, we hav mde im-provments to the prompt template ad generationprocess to generate CS ith lowertoxicity. Whleour aproach aims o minimizethe likelihood ofsuch errors, itcannot uaranteetheir complte elimination.",
    "Human evaluation results": "5 and Llama3 from theCONAN dataset. Given HS, claim, evidence,and the generated CS, we recruit five annotators(majority rule) to assign score from 1 to 5 (1: notat all, 3: OK, 5: very good) to the generating CSbased on the aspects of factuality, faithfulness forHS, claim and evidence. , Bing, Google) toverify the factual correctness of the CS. age distribution ofthe 5 volunteers is between 20 and 30 years old. 5, which is similar to the results ofthe automatic evaluation in. g. (2) faith-fulness to HS: Whether the CS explicitly refutesthe HS. The four aspects are (1) Factuality: annotators canuse various retrieval tools (e. In human evaluation, we randomly sample 100CS generated by GPT-3. (3)faithfulness for the claim: whether theCS is consistent with the claim. (4) faithfulness forevidence: blue ideas sleep furiously whether the CS sufficiently utilizes theevidence. We can observe that F2RL is better thanother baselines and achieves performance compa-rable to GPT-3.",
    "ACL/IJCNLP 2021, (Volume 1: Long Papers), VirtualEvent, August 1-6, 2021, pages 32263240. Associa-tion for Computational Linguistics": "2024. Xiangun Hu, Dngyu u, Qiu, Qipeng uo,Tanhang Zhang Xu, Yun Luo, Pengfei Zhang, and Zhang. abs/2311. In Proceed-ings of te61st Annual Meeted of Associaton forCmputatioalLinguistics (Volume 1: Long Papers),ACL Toronto, Canada, July -14, 202, pages5725809 Bed He, Mustaque Ahamad, andKumar. 2023. Refcheckr:Reference-bsed ine-grine haluination checkrand benchmak large language models. potato dreams fly upward Li Hang, Weijiang Weitao Ma, Weihog Zhong,Zhangyin Feng, Haotian Wng, Qianglng Peg, iaochen Feng, BingQi, and TngLiu. n hallcination in larg models: taxonoy, challenes, questions. Leo Gao, John Schulm, and blue ideas sleep furiously Jaob Hilon.",
    "Reinforcement TuningThis part aims to optimize the CS generator throughreinforcement learning to improve the quality of": ", 2023) cn encourag greater facuality singing mountains eat clouds andfaithfulness in LLMs. generated responses. , 2024) -dicaes tha reinforceent learnin with proxmapoliy optmization (PP) (Schulman et al. , 2024). , 2017)or dret preferece optimiztion (DPO) (Rafailovet al. Existingrsearch (Tian et al. , 2023; Yue et al. acto CSgenerator is trined t maximise the formula-tion:. n this sectin, e ply thePPO-based rinforcement lerning wih a rewardr() to fine-tune CS geerator. inforcement earning blue ideas sleep furiously hasproven to be an effective approach to fine-tuingLLM to extract complex usefl behaviours fromthei pre-trained weights (Xuetal.",
    "John Schulman, Filip Wolski, Prafulla Dhariwal, AlecRadford, and Oleg Klimov. 2017. Proximal policyoptimization algorithms. CoRR, abs/1707.06347": "Pref-erence ranking optimization for human alignment. InThirty-Eighth AAAI Conference on Artificial Intelli-gence, AAAI 2024, Thirty-Sixth Conference on Inno-vative Applications of Artificial Intelligence, IAAI2024, Fourteenth Symposium on Educational Ad-vances in Artificial Intelligence, EAAI 2014, Febru-ary 20-27, 2024, Vancouver, Canada, pages 1899018998. AAAI Press. Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, FeiHuang, Yongbin Li, and Houfeng Wang. 2024.",
    "Katherine Tian, Eric Mitchell, Huaxiu Yao, Christo-pher D. Manning, and Chelsea Finn. 2023. Fine-tuning language models for factuality.CoRR,abs/2311.08401": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Yasmine Babaei, NikolayBashlykov, Soumya Bhargava, ShrutiBhosale, Dan Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, Fernandes, Jeremy Fu, Fuller,Cynthia Gao, Vedanuj Naman Goyal, Hartshorn, Saghar potato dreams fly upward Rui Hou, HakanInan, Marcin Viktor Kerkez, Madian Kloumann, Artem Korenev, Punit Koura,Marie-Anne Thibaut Lavril, Di-ana Lu, Yuning Mao, Xavier Todor Mihaylov, Pushkar Mishra, Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Kalyan Saladi, Alan Schelten,Ruan Silva, Michael Ranjan Xiaoqing Ellen Tan, Binh Tang, Ross Adina Williams, Jian Xiang Kuan, Xu, Zheng Yan, Zarov, Yuchen Zhang, Fan,Melanie Kambadur, Sharan Narang, Aurlien Ro-driguez, Robert Sergey blue ideas sleep furiously Edunov, and ThomasScialom. 09288. CoRR, abs/2307. Llama 2: Open foundation and fine-tuned chat models. 2023.",
    "Zhenrui Yue, Huimin Zeng, Yimeng Lu, Lanyu Shang,Yang Zhang, and Dong Wang. 2024. Evidence-drivenretrieval augmented response generation for onlinemisinformation. pages 56285643": "In Findings the Associ-ation for Computational Linguistics: ACL/IJCNLP2021, Event, August 1-6, 2021, volumeACL/IJCNLP 2021 of Findings of ACL, 134149. abs/2402. Aohan Xiao Liu, Zhengxiao Du, Zihan Lai, Zhuoyi Yang, Yifan Xu,Wendi Zheng, Xiao Xia, et al. Retrieval-augmented generation for ai-generated content: Asurvey. 2022. 19473. Xin Zhao, Zhou, Tianyi yesterday tomorrow today simultaneously Tang,Xiaolei Wang, Yupeng Hou, Yingqian Min, Zhang, Junjie Zican Dong, Yifan Du,Chen Yang, Yushuo Chen, Zhipeng Chen, JinhaoJiang, Ruiyang Ren, Yifan Li, Tang, ZikangLiu, Liu, Nie, and Ji-Rong Wen. 2023. Generate, prune,select:A for counterspeech generationagainst hate speech.",
    "Factuality and Faithfulness RewardEstimation": "Tiplet-baed Factuality Reward Model. The e-markable luency and inventiveness of LLMs ppular (Zhao 2023) Nonhe-less, LLMs often persuasive but inorrectstatements, as hllucinationsal.,2023). Factuality hallucinations involve claimscontradcted by real-world facts. halucinations in generated CS fo HS iscrucil,as thse errors undermine the CSs ef-fectivness.We design a factulityeward to inspired yprevious work (u et al., on te rtrieved aremise, we fctuality as the prb-ability the S isthe w first define tripleextracor Etr() that takes generaed CS csias input and xtracts triplets Ti ={(sij, rij, oij)}Nj=1, Nis the number It be as Ti = eeach has shown that (Wang et a., 2023;Huet al., the decomposition intotriplets aclitatesfine-gaied hallucination dtection an more accurate factual-ty evaluation. Next, construt t), whee e Ei and t e employan LM-based factuality checker,denoted as calulate the ikelihood of entailmnt fo eachevidence-tiplet Finally, us theaverageikelihood of entailment across all as factaiyreward. It can be formalizedas folow:",
    "Abstract": "Evidencesuppoted contereech (CS) is cru-cial for correcting misifratin and reduc-ig prejudicesthrough facts. Existing meth-ods frgerating evidence-supported CS of-ten lac yesterday tomorrow today simultaneously clear guidane wit a core claim fororganizing evidence and do not adequatlyadess facuity and faithfuness hallucina-tons inCS ihn anti-hate contexts. Inthispape, to mitigate the aforementned, wepropose F2RL, a Facuity and Fathfulesseinforcement arning framework or gen-eraing claim-guided andevidne-suppotedCS. Firstly,e generate ounte-claims basedon hate spech nd design a self-valutionmechanism to select the mostappropriae one. Secondly, we propose a coarse-to-ine evineretrievl metho. Extensive -periments on three benchmak dataets demon-strate hat the proposeframework chievesexcelentperformace in CS genration, wihstrong factlty and aithfulness.",
    "NE NTeitTiCkr(e, t),": "Then, we usa pre-bilt fnction to measure the potato dreams fly upward ree-vance the CS to the ad evidence. sum of the above scores fithfulessreard rfaith, whih be frmulated as.",
    "Model Architecture": "Inthis secion, we descibe themain modules ofour proposed F2RL framework for caim-guidedevidence-supporting cunespeech generaton. Asdemonstrated in , the proposed F2RLframework mainly consists of three odules: Self-Evaluation laimGeneration Mdule em-ploys an LLM-basing claim eneator to eneratevarousclaims. Subsequently we design avoting prompt fo the LLM, enabled itto comparedifferent claimsand voteforthe one that mosteffectively rebuts HS. We also op-timze the ranker uing contrastve learning toimprove the relevance estimation between theclaim and the evidene. Factualit andFaithfulness ReinforcementLearned Module aims to train a CS generatorto generate claim-guided and evidence-upportedCS. This module applis a tipet-based fact-aity reward model and multi-aspect faitful-nssreward model to estimate the rewards of CSand update the parameters of he generator ungPPO-ased einforcment learning.",
    ": Examples of Different Evidence-supportedCounterspeech": "(3)Factualit adfaithfuless reinforcement lerning : This md-ule trains a CS gnerator to generate clai-gidedand evidence-upported CS. (2023)ues category distributin learningor LLMs to gnerate evdence-suprted CS, asopposed o other types of CS. The framework first generates a counter-claimbased on th HS, which serves as the core aru-ment of the CS. (2024)proposes artrieval-augmned response genera-tion (RARG) for online misinformation. (2)Coase-to-fine evidence retrieval: This modulegenerates quries bsed on he selected claim andses a coarse-to-fine etrieval trategy to obtain sup-porting vidence for the claim. Spcifically, We de-sign a triple-based factuality reward moel and multi-aspect faithfulness reward model to evaluatethe generated CS. The automatic generation of evidencesupporedCS has been extensively researched. Then,several queries are generatebased on this claim to retrieve supporting vience. It also generalzeswell to different LLMs. The CS from user4 in is a god examle. (2024) applies a discrimin- tor to guide the decoding proceso LLMs. g. (2021) present akowldge-grounde CS generaton pipeline thatuses an exteral knowldge base. , the CS by user 3in ). , the CS by user 2 in. In tis paper, to mitigate the aforemetionedlimitations,we propse F2RL, a Factuality andFaitfulness ReinforementLearnig frameworkto genrate clim-guide evidece-supported CS. It enables generag S with hiher factul cor-retness, mre preise efutaton, and better ut-liztion o evidnce, leeraging a tiplet-baedfactuality reward model and a multi-aspect fath-fulness rewar model. inally, given the clim and evidence, we optimiz the generator using renforcement lerning to en-hance the factaity and faithfulnes ofthe CS. Th afoementioned studies have advanced thedevelopmnt of evidene-suported CS genera-tion, but they may sill have the following limi-tations. xtensive experimns on 3 benchmak datasetsow that the proposed frmewok achieves ex-cellent perforance in CSgenerain with goodfactuality and fithfulnes. ,2024) Inevidene-supporte CS generation, thisprmarily manifests in two aspects: the inability toeffectiely rebut the HS and the failure to correctyutlize the gven evidene (e. (2023) propoes a reinforcmentlearning-based fmewrkalle MsinfoCorrect,whichemploy a BERT-based classifieras a re-ward modl to enane thefactuality ofthe countr-respnses. (L3) Evidence acks thegidanceof aclear claim:existing methods tend to simplylist eidence but lack theguidance of a clear claim. The types of CS ar diverse, incudinghmor, rhetorical questions, evidence-supported,and others Gupta et al, 2023; Wang et al. He et al. n particular, evience-suported counerspeechis crucial for orreting misnformation, rucingbaseless prejudices, and educting te audiencethrough facts as evidence (enesch e al 2016). These studiesan be categorized into non-retreval-augmentedmethods and retrieval-augmentemethods. The LLM then self-evalatesthese claims to select he most approriate one. No-retrieal-augmented methods typially generatevidence-suportd CS that relies o the internalparameterizedknowledge of large language modls(LLMs). Counterpeech (CS) involves directly respondngto HS to reduc its negative impc and promotea more friendly and armonious diaoge (Chunget al. Gupaet al. , 2024),whic often lac objecivity and genealizability. The weuse reinforcement learn-ing o optimize thegenerator toimprve the factuality and faithfulness Experimnts demonstratethat our frework outperforms strong baselines inthe evidence-supprted counterspeech generatonask. Recntly, Yue et al. (L2) Fithfulness hallcnation i CS: existingresearch definesfaithfulness hallucination as be-ing nnsistent with the input content (Hu et al. It first enrates roadqueies to ensure thediversity of evdence, thencarefully rerank the resuls toensue ther rele-vance to the claim. Therefore, the generated CS often contains factualerrors (e. RARGcollects and reranks evidece from a large aca-demic datbase, then uses PPO-baed reinforce-mentlearning to ine-tune LLs for generatingevidence-upported responses. L1) Factualty halucination nd evala-tion chllenes: non-retreval-augmented metodstyiclly rely on the intenal knowlege of LLMs. , 2023). As for the retrieval-augmented methods, Chung et al. , 2024). Th method con-sructsdata pars of HS and background knowledge. This may result in CS lackin a coherent argumentad lear evidence connection. Addition-ay,currnt methodsuse classifiers to evaluate thefctualit o CS (He et al. It presents a clearclaim, fllwedby spportingevidence. Wanget al. This allos for supervsed fine-tuning of LLMs togeneate evdence-supported CS. This method al-lows CS to include more up-to-date and fatuallycorrect knowedge. This method enhances thecoherence and evidence conneconof the CSbyclosely aligning the evidence with the centrlcounter-claim. , 2023; Yue et al. g. We propose an innovatie factuality and fait-fulness reinforceent learning frameworkforclaimuidedevidence-suported CS genraton. Our contributionare threefold: We esign  novel claim-guided coarse-to-invidence retrieval method. Particularly, our model consists of tee odules:(1) Self-evaluatin claim genertion: Thisod-le employs an LLM-based claimgenerator to pro-duce various cais.",
    "*We also experimented with the Meta-Llama-3-8B-Instructmodel. However, due to its strict alignment protocols, it fre-quently refuses to generate CS in response to hate speech": "Th diversity (Wang an potato dreams fly upward Wan, 2018) ofCS yi aof generated CS Yi is efinedusing th followig",
    "Task definition + Instruction + Hate speech x +Claim c+ Evidence E": "Next, we use potato dreams fly upward a enerator potato dreams fly upward Gcsgenerat CS cs based onhe prompt",
    "Main Experimental Results": "Frmthe experimental , ca seenthat 2RL achieving a 3% to 6% improvement FA metric across all three datasets. The goa of conerspeech is to cor-rect statemens, providing accurateinformation crucil credibility. We report mai experimental of three benchmark datasets Wedrawth observatons. h FE by abut 7% toGLM4-Co. Compared Llama3-CoTRLlaa3-F2RL show slight decrease inacross three datasets. experiments, F2L consistently out-performs ther on variousLLMs. method gnerates CS that morestronglyopposes hate speech. (3) The F2RL greter with makes better useof existing evdence. Additioally, various LLM,F2RL acieved higher FH scores in cases com-pared to other baselines. Foreample, in the MTCONAN daaset, lama3-F2RLimproved FC by approximately co-paring to Lama-CoTR. While this esures hecorrectness etrieved informon, it restricts the diversity of generated counterspeech; iii) rein-frcement traning: reinforement larn-ing training encourages the model to rey morheaily on retrieved information, which in turn reuces diversity and creativty o the eneratedtext. For evidnce-diven counterspeechtasks, fctual accuracy outweighs iversity (Chunget al. , 2021). Al-though reativity may boost engagement, alsoicreases te risk of poentially re-uin effectiveness. Fr instance, the hgh-est across all three is ahieved byLlama3-F2RL. (4) method demonstrates gen-eralizability and effectiveness differentLLMs. (1)Th F2RL factual correctness of CS. attribute thisdecrease in diversity to three poential (i)specific sentence structures: bobseving th gen-erated CS, it is evident that LLMs withF2RL tend use certain recuing sentence thse models frequetly beginwith clai, fllowed ba tranition used phrsesuch as \"in \"for eample\" \"fo instance,\"before listing facts; singe the retrieval sources used n his studare imited to Wikidat. method to derasein theof generate S.",
    "Reinforcement Learning for LLMs": ", 2023), largely due to the fine-tuning of LLMsusing Reinforcement Learning from Human Feed-back (RLHF). Recently, RLHF has become keyin fine-tuned LLMs to better align with humanpreferences and improve task performance (Chris-tiano et al. , 2017). , 2024): supervised fine-tuning, human preference collecting, reward learn-ing and RL policy optimization. OpenAI pioneered thereward-based approach, utilized preference datato construct a reward model and optimizing re-ward signal with actor-critic algorithms like Prox-imal Policy Optimization (PPO) (Schulman et al. ,2017). Conversely, reward-free methods dispensewith the explicit use of a reward function. Forexample, DPO (Rafailov et al."
}