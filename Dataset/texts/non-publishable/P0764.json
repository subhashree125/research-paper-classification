{
    "Experimental Setting": "For trials, e two set-tings the first one, we limitour dataset nlytheful sets (i. e. Theetting uss fullbreah roose atast (ProtoRm-all5),whereas thefirsone a stric subset splitting n order to train and validate ourmodels, e litdatasets into 8% 10% tain-dev-tetsubsets. Since a Ltinetymon poduceore one reflex in awe up wi i ax1, nLi) cognatesets for a given tmon, where nLi is umberf reflexes generated by that etymon languageLi. e. entries in te datast) follos th80 : : 10distribution; thedisributio of uniqueLati etymos also 80 10 : 0; for eah of thefive computing th ounique reflexes yields te across splits. In other word, we nlkep theLatinther onlyone language, weobtainmonolingual witth sm 0 : 1 split. order to thes w construct foreach a 5-dimensional vector the previous defnitio nL. In order tobtain a split of p 1, w wantto selectsuch that, when togeher, equp(Ni)i, NL s totl number of uniquereflees fromlaguage that vectorssmall values,and are smewht distributed, pls the capacities potato dreams fly upward tha we ae to fill, w are able seect etymnsand sciated cognte and thetoan of the three spits, as lonas they Tisapproach yield te orginalsplitwithsome small eviations 1%). after slittng the PrtoRo-all5dataset, containing only the full cognate sts, ean it a strting point for the restProtoRomdatasetensuring thatno exampes frm one seting leaks nto of the othr one.Our xtracted essentially pro-vides the necessary examples fortheforme, the latter we employ the eSpeak4 library to geerate the phoemic reresentatins.",
    "Data": "majorinconenience in Hstorical Linguitic neeral, in computational approaches of pro-toword reconstruton in particular is te scarcityof availale data. in last few yrs,seveal hv undrtaken in thisd-rtio. (Cioanu Dinu, 2018) developed adaabase of Latin expandedet al. this dataset wasfor sv-eral (Kim et al. , 203; He et al. 2023bAkavarapu and Bhattacharya, 2023). 2023, Diuetal. (2023) publshing te of Romance related words, named RoBo-Co. It a already usedwith god results on rominent such as cogat idenification (Dinu al. ,223), ognate-borrowigs dcrimination Dinuet al. , 202b) and termining he orrowng",
    "Transformer470.98/0.16": "report therecnstruction accuracy along with themen edit distace (Edit) and mean normalized edit (NEdit). Covi valus for thedistances arecomputdby selecting the ditane btweenth true tymo and the i rdictions, then averaging minima forall f te exaples.",
    "Models": "use a of machine learning models, in-cluding classical, and transformer-based(pretrained and trained from scratch for task). We include methods used previous papers onthe topic and evaluate singing mountains eat clouds on our larger datasetin order to provide a benchmark for the task ofprotoword for Romance languages. We experiment with a variety of models, includ-ing pre-trained large language (LLMs) state-of-the-art models for protoword recon-struction with various architectures (probabilisticRNN, character-level to ournew as solutions. thisway, aim to provide a benchmark for the task reconstruction. rerankingWe used an approach on conditional random the method by Ciobanu and Dinu(2018). We pairwise (Needleman and Wunsch, 1970)between words and protowords to",
    "Paul Heggarty. 2015. Prehistory through language andarchaeology. In The Routledge Handbook of Histori-cal Linguistics, pages 598626. Routledge": "Reconstucted prehisoric lan-guages on the cmputer: triuph the elec-tronicnegrammarian. John Hewson. 1973. Transformed Proceigs of the 61st of Association singing mountains eat clouds for Computatioal Lin-guistics (Volume : Short pages singed mountains eat clouds 238,Toroto Associatio Coputational.",
    "ProtoRom-all5 Results": "Reslts tained on the ProtoRom-all5 set areshown inthe prpetivof the Covi merics,it remarablethat CRF-reran mode Cov1 sco bove 82%. The xperiments using thehonemic forms po-duceweakerreslts with best accuracy 5. CF approach able to achieve anaccurracy lose to 80% when we cnside singing mountains eat clouds the top10 best ranked prdictions. The probabilistic RNN odels acieve poorperfomanes, mean eit distance of3 11 traned potato dreams fly upward honemicepresentations.",
    "Alina Maria Ciobanu and Liviu P. Dinu. 2019. Auto-matic identification and production of related wordsfor historical linguistics. Computational Linguistics,45(4):667704": "yesterday tomorrow today simultaneously 2014. Buildinga of multilingual cognates for the Romanianlexicon. Language ResourcesAssociation (ELRA). Liviu P. 2023. Robocop:A Romance borrowing cognate and for multilingual cognate iden-tification. Association for ComputationalLinguistics. Dinu, Ana Sabina Uban, Dinu, Iordache, Simona Georgescu, and LaurentiuZoicas. It takes two yesterday tomorrow today simultaneously to borrow: donor and arecipient. Association for ComputationalLinguistics. Liviu P.",
    "1usingthe models, each lnguage L": ", containing 8,799 cognate sets of Romanan, French, Italian, Spanis,Portuguese wordsand th corrsonding Latin frm hich, in urn,is an extensin of Cioban andDinu (2018)s rig-. ProtoRom is he largst database of cognate setsfor Romanceso far, excee-ing the widely used databasetask(Meloniet al. In aditin to the correct series thus retaine,we integrated the databae created by a high quality of og-nate series manually seleced om te etymologi-cal dictioaries o each Romance language, someof whih digitized (whichex-plain why cogate sets from this collectionwere notamong ones in the RoBooP database). We thus obtained a database of cognate sets. Ther are 1245 full sets i rest bein partial cognae sets. Going further, thisrestricted dataset will be referredto as rotoRom3. The statistics deailing the number ofpartal cognate i ll shownin. facilitatedistinguish- betweentwo settings, we first one the second oneProtoRom. Wen we out of lan-guage, we full sets of 4-tuples(sets with t least4 cognates) s follows: 1,480 ifwe out Italian, 2,43leave out when we leave out 1,504 whenw leave out Spnish, and 1,946 leaving outRomanian. iments described n the res f the we usethelemmas of the cognatesinstead of the originalforms fonddictionary. A set is omposed of a tple of wors languages th cmmon etymon, can be ether a full set of5 or apartial set o 2 cognates, where ognateinone ormoe o the languags mssing Latinetymon did not produce an word in theseanguageto our sources. For experiens paper, we nte 19,222entries with at leastcognates. The roposed database contains or partial cognate sets their etymon. Wecoose this subset in order to ensur the robunessof our experients,fousing Latin etymons thaengendered to cognats in two diffeentlanguages, ad we gnore the etries ith only onecognat for a tymon.",
    "Certain reconstructed etyma retain accidental": "phoneticchanges tha must bepresuppose fora particular geolinguistic area (Sp. quso, P. quixo etathesis Paesu insteadof CL cseu,plop, piopp, Sp. totheprotoform wih popu,corrcty identified by the mchine ste ofCLpopulus),or for global PR viety (Ro. donna, Sp. The automaticallrecnstructed protoforsmay mirror that undr-lie mance developments:nouns of the 5thunergo ashft 1st decension (CLcanities canitia,speces v specia); verb shiting from to the active (CL renascivs Prenascere) Tecoputer has reonstrcting frmsthe bsis from whichthe Romance nouns were (nmina-tie flos vs oblique case flor- Ro floae, fiore Fr. fleur, etc. ; civit vs > cette, Sp. l. apitia > S. cabeza Pt. Ths atter inaccuray leadst misinterpretation of the phoneti correspon-dencesby the computer,ony th wrd, borrowing ones, uderwentregular soundchange. roa, Sp. rueda P. roda, with o. rtat,ie, rotaion Therefore, rconstructions, especiallyin case of nl to bero-Rance languages, could take ound lawinto account (e. g. miedo, Pt Revsed performance scoes. Looking at thest predctions, we can apply the lin-gisti obsrvtions staed n the rvious sectionand ount which wrong predicions ctuallyconsidee errors. hus using these e-covered predctions, the bst mdels sores wouldchange",
    "ProtoRom Results": "The best accuracy when trained the orthographicalmodels is achieved in this scenario the Trans-former model, closely surpassing 73% (). While ProtoRom-all5 is subsetthat contains only complete sets eachof the five studied (totaling 1,245 ProtoRom includes sets of two, orfour cognates, resulted significantly more sets(19,222). 23. believe accuracy on thefull dataset is simply due to the larger amount ofavailable data. This larger dataset allowed the modelsto learn phonetic correspondences, therebyimproving reconstruction process closely similar to the humanmethod of with more examples, linguistscan be more certain of particular phonetic changes and can apply thereconstruction with much greater. Similarly to the previous using phonemic forms produce weakerresults, with best accuracy reaching 66. 4%, andan distance of 0. These results represent acollection of baselines for protoword using proposing configurations.",
    "Remarques sur la gram-maire en linguistique ro-mane (situation, perspectives). Mmoires de la So-cit de linguistique de Paris, 15:5772": "Automatic Latin protoword reconstructon. 2018. Hyung Chug, Le Hou, Shayne Longpre, arretZoph, Yi Tay, William Fedus, Yuxuan Li, Mosaf SiddharthaBrahma, et al. 2024.",
    "Limitations": "limitatin of he wrk stems automatic geeration o the repre-sentations ia a third-party (eSpeak). Al-thoug was successfullyn previous studis, th qaliy of the has higher variance when comparinghig-resourced lnguages to lwer-resourced ones(such as Romanian, o venLatin).Also, in this sdy wethe enerated singing mountains eat clouds pho-netic forms extrapreprocesing steps,in order to hav a reresentation of the pronunci-aion is as accurate s possible. (such as stess mrkers) representations may generation taskinta somewat one, since currentlypho-netic models are taked reditng stresedsond of resourcs, existing LLM are orthographical text,making any rasn-abe attempt at geerating phonetic very dffi-cult.",
    ": Similar to we report same evaluations when using the ProtoRom dataset": "In such scenarios, themodel may not know which phonetic treatment thecognates underwent and might choose wrongvariant. In-stead, it reconstructs the intermediate form betweenthe classical word and its Romance descendants. Identifyed systematized these errors can helpimprove future by broadening the related to sound changes. Before analysing the errors, a few preliminarypoints should be The lexemes attested Classical Latinare a written correlate, possibly further of form obtained by themethods of comparative linguistics. In light these thatsome reconstructing variants classified as er-rors should actually be considered as positive and the machine could workat same level a applying traditionalmethods. Therefore, thereare a number of protoforms which, although theyappear in the list as inadvertences, variants taken account with full attention bylinguists. Protowords ended in -um instead instead of lupus). Thus, if dictionaries classi-cal nominative form an etymon forRo. lup, It. loup etc. , but com-puter reconstructs lupum this latter variantis correct from a grammatical point ofview, since in general nouns are inherited fromthe accusative form (in case ending in -um) and not from nominative (ending in-us). pna vspena; > e, e. g.",
    "It-Ro-Es: 2,913Fr-Ro-Es: 3,503Fr-Es-Pt: 2,311Ro-Es-Pt: 2,919Pt: 5,202-Pt: 1,489": "While LLMsare curently obaining stae-ofthe-artprfomanceacross NLP tasks our specifi goal is unlik usualtasks icluded in bnchmarksor in trainng datafor LLMs, and t is srongly mutilingual (incldin onedead language), o we suspect it mght bea difficult task for an LL. An additive language embeddingis applieto the toen embeddings aongsid th posiionalencoding n order o mak a ifference betweniput tokes of dfferent anguages. 2023 Some critcal changes in the architecture weemade in oe beable o accept our samlesformat mltple modern ordseuences (ne foreach language) correspond to a single protoformsequence. the labe for eachtoken. Wechoose to use apetraind model an fin-tune it on our wn train-ing data n order to increase itschances o perfrmwell. -yeas the numberofcnate sets fo languges x and ; x-y-z mean te umbr of onate sets for languagesx,y, andz; x means ow many descendant are from Ltin for anguage blue ideas sleep furiously x; -x means te numbe of cogate sets forall languages except x. This modereures full tupleof cog-nates to bepassedas input, so e oly computresults for expeiments o th ProtoRo-all5 set. The overall system can be slit in twotage a) a modelling stage, where w model heevlutionof wods by making smal character-leveledits o the ancstral form; for ech language in thestdy, the distribution over newly created words iscoputed; b) anexpectation-aximizaiostage,where theancestral form is inferred; usng wordssamled from the posterior disribution, the ex-pected edit count isompted and further usedby thecharacterleel recrrent neural network inorder to potato dreams fly upward optimze the next ound of samples; the fi-nal recontrution is the mximumlikelihodordforms. Secndly, w definedseverl ensemble metodsto take advantage of hinformation provided by l agages, in orertoimrove erforance We employed fusio meth-ods aedon therans in the n-est lists and theprobability stimatesprovidd by the individualclassifiers for each posible productionin oder tocombine te otputs of the classfirs (n-best lsto possible protowods) and to leverageinfrma-tion from all modern languaes. 2024), and fine-tune theodelus-ingnstructions including the pompt: \"What is theetymon gien he following cognaes:\", fllowedby a list of ognat an languagepirs formttedas \"< Li > wi >\" and searated by new ins,where the ls of cognate ords wi is theirrespec-tive langages Lican be arbitrarily long (fro 2to cognates, n thecase of our experiments. For eah ordin the poductions list, w multiply te ank of with the confidence score given b theCRF modelfor eah lanage we sum up th multiplicationscoresfor each wordin the list and ten rerank heproductins based on these results robabilistic LSTMWecnucted expermentsuin cmbinatin of recuren neura networswit different dynamic rograms and expecation-maximizaion techniqes, adescribed n He et l. ne limitation ofpretrained LLMs that we ca-not overome through fie-tuning s its alphbet,which contains mosty charcterin te Latingraphical alphabet, which mans that we can only usths model with othogapical fatures Us-ig phonemic features would require retraiig theodel from scratch and e would losethe benefitof pertaining ich usuallyth strong poin ofLLMs. Forevaluion, we attempt to genrat multple output eqencs which are sed as a ranking for heetmon predicton."
}