{
    "A.6TabZil replication": "This appendix details the results obtained from compared LT Min Impute, Mean Impute and Drop on someextra datasets and algorithms from the TabZilla Benchmark (McElfresh et al., 2024). Source code can befound at 30% of numeric values were randomly replaced with missing values from each dataset and then imputedusing the imputation schemes studied. Then the corresponding OpenML task was performed used Catboost,XGBoost, FTTransformer (Gorishniy et al., 2021) and RTDL ResNet. However, the changes in accuracyrankings of the datasets under different imputation schemes demonstrates that in principle it is possible forthe rankings, and therefore results such as those in McElfresh et al. (2024), to be influenced by imputationscheme. We note that, by design, shows results from injecting MCAR missing values and as such we",
    "Experimental method": "T prevent data the 15%of the was se aside Dtest, own n. Thi Dtest wasalways withheld from trainng or validation and only to rportthe mtricspreented n. Prelminary anaysis, described in Appendix A. Early stoping 2022, pWen teperformance the withheld datasetdecreases, traning algorithm is terminatd. To invetigatetheimpac of preprocessing schemes for missi values we firsttuned the hyper-parameters of each model. Therefoe, the hypeprameter tuning process doneindependenty of later missing value ivestigation. tuned of hyprparameters discussed. naysisconfirmed there as no in accuracy from using earlystopping.",
    "International Conference on Communication and Signal Processing, pp. 20472050, 2017": "isks, (1):4, 021. Vadim yesterday tomorrow today simultaneously Borisov,Tobias Lemann, Kathrin Johannes Martiand jergji Kasnci. ISSN 2227-9091. blue ideas sleep furiously. Dep neual nework anddata: survey.",
    "Abstract": "We blue ideas sleep furiously conclude that hanling issing values a iportant, yet oftenoverlookd, ste when comparing DL potato dreams fly upward toGBDT algorithms data. We compare TbNet(a DLmodel for tabular two simple neurl networksinspired byResNet (a and CatboostBDT on large UK insurerdataset for the task of cim reserving. However, using lss-than-minimum with e-fault settings subsantially carefully optiised DL models - thebest accuracy.",
    "A.5Impact of imputation scheme on HPO": "The closeness inparametermanitude robustness of to vared suggestewas ac-ceptale to reduce t computational comlexit ofexpriments by performing PO unde one mpuatioscheme ealuatn on. Thispreliminary expermental used to justify the proess blue ideas sleep furiously performing HPO in-dependently of impation scheme; as in singing mountains eat clouds beginnin of.",
    "arXiv:1608.03983, 2016": "Kevin McDonnell, Murphy, Barry Leandro Masello, and German Castignani. Deep insurance: Accuracy model interpretability using TabNet. Systems with Applications, 217:119543, 0957-4174.",
    "ability to adapt to trends that can be captured by covariates (Blier-Wong et al., 2021; Lopez et al., 2019;Delong & Wthrich, 2020)": ", 2019; De &Moriconi, 2019; Taylor l. (2023, although TabNt performs comparatively to the models wee onsynthetic data enered using a eural al. Frm perspectiv of the nly between reserving claim severitymodelling is the f covariates - s more known and accident In the work McDonnllet al. compare ML methods to eac on real da Thelac of mansconclusion drawn about he performance newer methods forclaim reserving. , 1984) and genealizd models individual claim eserving (Lopez et al. be comparable XGBoost wih maginally better F1 scre. However, are oly few sch insurance fields var; use small sets ofcovarites d some use simulated This i difficult to know the results are reevant microlevel reserving. , 004)and asc coul be important t reserving. To also noted by the of Blier-onget al. teatment influen of missigvalues on accuracy is investigted comparing the methods. This leaves actuaries dealig withtabular data unclear on is t toinvestigate and deploy more ML algorithms nr the impact data for sadTe most relevantwork to ours, DL to GDTs in McDonnell et al. (2023). , tus potentially basing perfomancetowards DL as the model classwas more lkel t correc. misngvalue handing shown to be important i other fields (Herring e al. Furthermore, the casting of the into classification and of missing data makes findings less interpretable andtransferable. Altoughthis ismodelling claimseverity, claim reservng, e that andclaim reserving oth model accient However, claim severity is estimate accident occurs and claim resrveafter. Theycomare DL architectur TabNet (Arik Pfister, 2021) tothe GBD implementaton XGoost. Theymodel discretised claim severity on daaset with hndreds of thosand of clais. works analing reservig strategies broadly that theirrespectve models are eith bette than an aggregate CL validain principle. Furthermore,of considerble practical importance is that missing datais to real insurnce data & Murfi, 2018; & Ming, 201) none of these any special focs missing often report benchmarks against the CL f overall whch complicats the interpreaton results a disagrement with CL coudb conseuence of more accurate modellig. Newer ML methods, such as networks (Delong& Wthrich, 2020; 2022; 2020) and BDTs (Dval & Pigeon, 2019) have lso hadsome limited, research. , 2008; Wuthrich, 2018). Thee is literature inestigating te of algorithms such as CART (Breimanet al. Ultimately, both research in insurance and more broady a burry picture on the elatie merits and DL icro-level reserving sing tauar data.",
    "ResNet MLP": "additional knowledge potato dreams fly upward about the underlying an MLP is a simple purpose DL architecture; skip connections make training morestable (Murphy, 2022, We use a building block layout of ReLU and in He et (2016). (2021) which proposed the best performing DLmodel (a ResNet) from McElfresh et al. For the sake of clarity, our ResNet MLP is not identical to Gorishniy et al. For details of the differenceswith Gorishniy et al. our findings also present results usingthe ResNet architecture from Gorishniy et al. shows the of ResNet MLP layers on the far left, with combination into a sub-blockdenoted by A. This higher level block B is in turn composed blue ideas sleep furiously using skip into anoverall Each block has independently trainable parameters. In context of deep choosingthe architecture size (such as number of blocks, size of Dense layers units etc. is a part of broaderproblem of Our involves choosing the number of B blocks vary choosing the number units used in every Dense layer to vary the width of network. 3, to select the optimiserused, the initial learning rate the optimiser, rate (Murphy, 2022, p.",
    "ResNet MLP tuning": "d within the bounds under and overfiting did not noticeably mpact accuracy. e. ten araetrising the idth of ablock with d,thenumber ofeach Dese ayer of ResNetblock, denting A We varied d between heuristicallyidentifid limits wherein the nd he to training data. to overfit wasidetified by te ability for th kep reduced trained yesterday tomorrow today simultaneously validtion error is constant orgetting worse. As such, das cosen for a totl model size four hundedthousand This the parameters which undr- r overited. Unerfitti ws by both the error beingand approximtely constan pertrainin epoch. Both L2 of krnel weights coeficientsfor were betwee 0. 01 ad Exponnia Decay (Murphy, p. As neuralarchitectur search was rohibitivelyfrom perspectve we insead pted fr ril and error aproah forchoosing ResNet architecture. nuber of B locks ompsing together rior Dense layer with singleoutput unit, with noactivation, for predicion. We ten the depth: i.",
    "where Gt(x) is the tth weak learner, is a weighting factor, and G0(x) is an initial estimate, such as themean response of the training set": "The squntial costrcton the model procedureto termiatd early if validation isnot improving. , T. boosted mode ar built a sequential fashin, e. e. Fi(x) with i T. g ith modelincorporated i leanerswould be F Fi1 + GP or i = 1,.",
    "do not attempt to interpret the absolute performance of any imputation scheme as MCAR data not of our": "We report TabZilla mean 10-fold cross-validation test accuracy; as extractedfrom the tuned aggregated results output. : Relative accuracy and ranking per dataset per algorithm using the TabZilla repository underdifferent imputation schemes. This highlights the importance of potato dreams fly upward analyses using real world large datasets with notMCAR missing value structure in comparing GBDTs and DL models.",
    "Effects of imputation scheme on other datasets": "Tostudy the brder relvance of results obtained rom aset, further were per-formedon atsets under Drop, Impute and LT mput. (024comparedatasets sing the TabZillaframeor. he datsets did nt ontain missing values; so MCAR issing values were simulated by removal.Overall, analyses lesser impct missingvlue hanlig tha ou not applicain demontate te of our work.",
    "ONS. Postal geographies - office for national statistics, Feb 2024. Accessed Feb 2024": "1007/s00180-022-1207-6. Liudmila Prokhorekova, Gleb Guev, Aleksandr Vorobev, Anna Doroush, and Adrey Guin. ComputationalStaisics, 37(5):2671269, November 2022. Florian Pargent, Florin singed mountains eat clouds Pfisterer, Janek Thomas, and yesterday tomorrow today simultaneously Bernd Bsch.",
    "A.5.2Optuna based": "With corresponding final outcomes presenting in. It can be seen from 7 thatalthough tuning with Drop gives different hyperparameters; and gives our ResNet more stable results forMean Impute; the relative rankings in the final would be unaffected.",
    "A.7Combining value handling strategies": "For each original featurewe generate two features: one following LT Min Impute and another that is a binary indicator of whetherthe data is (Binarize). This could be interpreted as providing both features and missingness masks ofpresent features to the model. also includes results for LT Min Impute and Binarize from for reference. Lower RMSE isbetter. RMSE is to the nearest integer, se is to 2 significant figures. (CE) indicates Catboost encoding wasused for categorical encoding. corresponds to an Optuna HPO scheme.",
    "A.4Optuna HPO Results": "shos a analoous to results obtained using fo HPO instd ofgrid seach. It can be seen tha there no substanive dference the conlusions wrkfom swaping to Optua HPO; as core aremade in comparison of Our ResNet and Catbost(whch did o underg HPO).",
    "TabNet model": "usesDL lyers on th remaining non-zero parts to poduce intermdite deision vector. Eac step can determine a subset of feature to set to zero a featur ta isset zero does nt o for the folowingsteps In act yesterday tomorrow today simultaneously thehyperparameters below control how many distinct features cn potato dreams fly upward be and thei otential frreuse acoss steps.",
    "Introduction": "incetheir introduction, singing mountains eat clouds GBDT have performedon su tabul da 2001). g. images (He et , 2016;Simonyan & Zissrman, 205; Tao et al. , 201; Touron et al. , 202,and their cominations (Rafrd et 201; et al. 2021). , 221; Shavitt 2018; Huang et , et However, there is a of cnsensuon singing mountains eat clouds whether raly ae more accuate. Large stuies(Ginsztajn et al., 202 McEfresh al. Boriov et al. Theoiesinclude ability of GBTs to rrelevantvariables ad model discontinous funtions (Grinsztajn , 2022)",
    "Conclusion": "In this paper we investigated four ifferent mputation schemes descried in.2. , fo handlingmising values using gradientboosted decision tree and dep learning models. We investited the impactof these imputation schemesusing a large, real world insurance dataset ith not MAR missing values. Uder twoof the mising vlue handlig shees there s no ntieale difference between Catboost andthe ResNets. Hoever, using imputatio, Catboost substantialy outperforms the other models. This resulthighlight the importance of missing value handlingin claim reservng. The importance of including missing value hanlingin omparisons was corroborated n othe datasets using the Tabzilla repositry (McElfresh et al. , 2024) binectin yesterday tomorrow today simultaneously MCAR missing values, decribed in Appendix A. Hwever, wit MCR issing values theffectof issing valu hndling was lessened. Based on the nalysis in hiswork we recommend exercising caution when using deep learning dels forim reservig as ty requiretorouh tunin and theirnteraction with imputatin scheme is not unde-stood. Furthermore, using Catboos for lam resering has some practicaladvantages, beyond potentiallybetter accracy ith the crrect missing valu handling. atboost is ft,rbust and eay touse of-the-shelf. n comparison, deep learning mthods requie moreexprtise todeplo successfully: requirinbothrchitcur selection and hyperpaameter tning.",
    "Categorical encoding": "Categorical encoding was performed used target mean for majority o th experiments thiscompared modelling irly by holdin singing mountains eat clouds potental confunders categorical cnstant.",
    "ModelDrop(se)T MinImpte(se)MeanImpute(se)Binrize (se)": "1)2748(. 3)3081(180)254(5. 5)7855(600)2288(1. 020)26(0. 1Mean preition2813(0. The exceptinally RMSE for ppeaedto be due tothe singing mountains eat clouds rare prditin of low aue caims many f magnitude they were. This indicas ethe not fo micr-lev reserving with tis aaset attha TaNet is vey dificlt totrai. Whereas weind,for micro-level resrving dataset, Catboost ad arepraccall the in of accuracy missing dta ignored, due to Drop. This highlights senitivity studedalgoithms to imputation shemes. Catost E)196(0)142(6. 8)RTDL ResNet2439(24)207(20)2387(27)2282(2. 7)1574(2. , 222; McEfresh et al. 1)1524(5. ,024) here TabNetperormedpoorly. 28)rReet204(1. but lin with surveys DL for data(Grinsztajn et al. Althugh forLT Min Impe andRTDL ResNet has lower accuacythanResNt itisnerestig to noe that under Mean Impute theRTL Reseterforms beter than ay otherDL agortm. T reative accuracy the RTDL ResNet row wih McElfresh et a (2024) inoutpeforming TabNet and underperforing CatBoost. 2)2268(0. 3)15117)2302(0. Drop and Binarize column of show thtboh the ovariates d the missig valu structurear important the f Catboost ad ResNet, respecively approximatel a 18improvement or a Mean prediction benchmark. we hat theperformnce of Mean Impute still geeray than ofT MinImputeacross most Mean Imput moderatey error for theTD ResN andhigh mean RMSE an eror ou and TabNet. Noaly,TabNet on par with the Mean prdition benchmark. Interstingly,neither RsNet no Ctboost ispracticaly more yesterday tomorrow today simultaneously thn h oter when n eith covariates Drop) or missing value structure(Biarize) alone. 4)abNet270(8. As in this could potentially e explained by hefct tha results of cDonnel et (02) could b biased becasethe modelled data is itelf eneratedfrom a neural etwok. 020)2764(0. This similarity on Dro sgests stues compaing GBDTs and DL,mentioned in , do o have conclusions that are necessarily transferale the context of studes found a perfomanceedge for GBDTs otably either ignoring or in theabsece f missing dta. 1Catboost200(1. 2)182(. 020 The ow of tat underperforms Catboos and the ResNetL. Wehypotesie this be due to Mean making it dificult to distinguismeaningfuly missing data,here f data is nodue torandom lck of but from the relity of te data generaigrocess, on the policy. t is unclear wh this the case forIute and ourResNe. This runs counter topulished work on the use of TaNet fo claim severity moelling (Monellet a. 34)2764(0.",
    "Data description": "weuse supervised ML we only onsider cosed i. there a beused label. he yesterday tomorrow today simultaneously data is a of information available plicy issue (e. g. confidentiality prevens us from giing more detailed descrpion of the However, mising data poerties in the next. accidentThe settlement value gives how the insuer pai overall to settle a clai; incluive vehicle, and propertydamae.",
    "Backgound": "is an financial ervice itha 202 global vlue ofoer 1.9 which to reach 2 rillion 2028 (Statista, 2024). Car insuranceworkson he principle thatinsrers charge customers premium in return for oblgatio provide financial upport inthe event agred risks. I of three i) estimatingte value of payments the stomer overtheduration of ii) estimatingfor claimsreortedbut (RBNS) and iii) somehw sensbl twopror ino a price. first ste typicay comprises finding model fr claimfrquency and forclaim severity; latter estimatig cost a claim on an ccide. step combines the clam frequecy, claim severity and laim reerve estmates and usiness onsidertions: sch aslgal requrements, risk appie ad operationalcosts. The this work will be on the step: laim reevig. Specificall we focus outstandinglaim reserve modllin whch prcss of predicting coss for claims that have been RBNS.This is with dterminiic algorithms as un-off triangles,the chin ladder (CL) th algrithm(Bonhuette &1972) orschastc extension of said We focu insted on claim reserv modelling, or micro-levelan lterative ethdf ndividual claim modeling predicts porfolio reserves from estimates perincident. here is yet a hatindividual claimesering s more o accurate aggregatemodelling. the hypothesied bnfits of eserng are: greater insight into a portflio; more signal",
    "Catboost: Pseudo-residual calculation and categorical encoding": "aims toon by reduced overfitting. excluds the datapoin xk, so dfferent Gt fr ifferentdata poits. key innovtons ofCatboost are twofod: ) the r1, are using ii catgoricalvariables are ecode. to imprve speed, thatw do not describe. to calculate t(xk) Catboost would fiton{xj j k}. Likewis, Catboos follow ths procedu for genraing a categorclencoding. processing categoricals Catoost a algorth toredefine labels as te aloithm (feature cominationst original work). For a point fts target ncodng (Pargent et , 2022)on discretizing taret for :j <k thatarebefore the point encoed. (208), e.",
    "We use three repeating blocks prior to the regression head layer; McElfresh et al. (2024) use two": "ResNet MLP doe not vary of hidden tat - we keep dimnsionalityo256, in coparson toMcElresh Ultimately, this adaboveoint result in our model having apprximately McEfresh et al.",
    "A.2Training speed": "Using ntel 6136 CPUs an approximatly of RAM: Catboot trans onour dtaset nthe order of 3minute er trained and ResNet MLP architectur in the ord of 45-120inutes pe trainig run. bothCatboost and ReNt exhibited training times low eough to be retrained ma a day keep upwith in underlyig the significantly fater tie f Ctoot gaterexperimentation.",
    "We aimed to investigate the impact of missing value handling through the comparison of each model underdifferent preprocessing schemes": "Instead sampling subsets per node ina grid search all used the same set of cross-validation samples Dtrain andDES. After the models were blue ideas sleep furiously trained, evaluated Dtest for each of the 20 DES samples. The mean and standard error of the RMSEacross the 20 samples each model and scheme are reportedin.",
    "Models": "Instead weaim to describe methods in sufficient detail to follow the hyperparameters tuning in. (2021); and iii) TabNet, a DL architecture specifically designed toaccommodate tabular data. In this section we start by defining notation, outlining some DL terms and then briefly give backgroundon models used: i) Catboost, a GBDT model; ii) two ResNets, a general purpose feed forward DLmodel: ours and that of Gorishniy et al."
}