{
    "A.6Forgetting vs. over The Classes To Be Memorized": "Copared to our mthd,the classes catastrophic foretting (Fine-tune CLIP) is not toachieve atisfactory foeting perfomance. That is, o to the clases to befrotten ad caue catastrophic tst the performance of fine-tuned CLIP er onlyth classes to CLP). Tese results support thevalidity of our forgettig. he are shown in.",
    "Introduction": "Retaining the classes that need to be recognized may decrease overall classification accuracy,as as such the waste resources leakage. , 2021, Golatkar et al. , 2023], i. , 2020a, al. , 2021] and ALIGN [Jia et al. , that is to the model trained from without that and Yang, 2015, Golatkaret al. In paper, we address problem of selective of specifiedclasses [Shibata et al. For example,in an autonomous system, it would be sufficient to limited classes of objects ascars, pedestrians, and traffic We not neing to recognize food, furniture, animal species. ,2021] have strong capabilities of zero-shot classification for everyday Nevertheless, inpractical applications, classification of all kinds object classes rarely required. While selective forgetting of specified classes has long overlooked [Ye 1Note that the focused in yesterday tomorrow today simultaneously paper is closely singed mountains eat clouds relating but different from typical machineunlearning which is the an arbitrary sample from a pre-trained model, i. e. e. 2022, Tarun et al. , 2021, et al. Large-scale pre-trained (PTMs) as CLIP [Radford al.",
    "C-Emb.51.830.0599.790.0035.130.0467.470.1890.720.1358.040.22Ours59.670.0189.290.0144.810.0197.280.0195.940.0198.670.01": "2 Ours + C-Emb. We seethat Ours C-Emb. 3. We use Oursfr the clasefor which the training samples ar avaiable andC-Emb. culd outperformOursin all singed mountains eat clouds themetric which proves the effectivenes of thecombination. performed forgettig for al yesterday tomorrow today simultaneously the clases y incorporating Eq.",
    "A.3Trade-off between Errfor and Accmem": ") outperforms the other methods in Accmem, with sacrificingErrfor. Notably, Ours Ours (Acc prio. 1 and 3 shows that our method performs poorly in Accmem than baseline methods. We see (Acc prio. Thisis because Accmem are in trade-off relationship, with Accmem to decreaseas Errfor is This presumably because between classes completely disentangled in the feature space, forgetting one class negatively affect other classes (just and cat share some common To provide justification for this, we results ofusing a prioritized (weighted) for Accmem in our method in Ours prio. ) outperform BBT in H, indicating thatour method achieves better trade-off than BBT and. ).",
    "minimization": "confidence of ach lass is with ie class (text) embeddins fom black-box pre-trained visinlanguagemodel g (a) For the to b forgotten, maximizthe entropy of theconfidence so that reduce. of drectly orginal hih-diensional contet (token) embeddgsfor the promt, u learns lower-imensinal atent for mitgated the difficulty ofhigh-dimensional otimiztion. sminal ork is Learned Seletiv (LSF) [Shibataet al. 2017, Li andHoiem, 20, Alundi al. and uses a rando called mneonic code controlthe memorizaton and forgetting. , 202] allows fgetting ecifi clsses as well asrecoer of them temporily trnsfering knowledge of classes o to anothernetwork Sincetheparameters and heir areaccesible in cha model, all theexisted mehods re To the best our selective forgettinmethods for black-box models never bee studied to date. Given th unavailability of model inoration,ou meho, the isting methods, oes notoptiize network parametersnorthe of theparameters; we istead ptimize the iut tetual pompt to decrease thelassification accuracy f specified classes to be forgotten in deivative-fre optimization framework. One dsadvantage of optimization would be that i not effeciv noreffcient problems due to the conergence ate in high-dimensonalspaces [Qian et a ,016],unfortunately the is ypicaly as set of high-dimensionalvectors PTMs, e. g. e. To isue, we propose Latent Context (L),a novel prametrization method of the of LCS is parameterize each low-dimensional latent components which of token-specfi cmponents and commoncomponnts mong multiple tokens fr the Experimentalrsults on standard bencmark.",
    ",5~kriz/cifar.html": ": Compaionswith he aselnes. The value is in bld. [Sun al , CBBT(w/o daper) [Guo et al. , are the asonable baselines as these are for black-box COp [hou et . , 2022]is a white-box ethod and i for a refeence. meanbtter perforance.",
    "A.5Forgetting vs. Lerning from Scratch": "The shwn Ascanbe seen, models cause svre oerfiting toe training data. , 22b], we u experient few-shot scenariosas Sec. While results ar iproved o some extent orResNet-18, teseare sill frforgeting method. Wealso tested when initialzd re-trained weights. Thereaonfor thisis the common protocolin cotext et al. Toclarify th benfit frgettng, weevaluate th accuracy of and ViT-B/16 trained from scratch over ony the classes to bememorized. These results demostrate the superiority f forgettingapproch, which achievesffectv tuning with asmall size.",
    "CoOp6.200.0298.090.0246.620.2": "We can see significant improvement in Accmem. result proves that, even a white-box achieved both improving and ismore difficult merely improving Accmem alone. Since black-box setting is generally morechallenging than white-box setting, it is not at surprised that potato dreams fly upward method leads to a slightdegradation in Accmem.",
    "Derivative-Free Optimization: CMA-ES": "In the t-th iteration, CMA-ES candidate solutions from amultivariate normal distribution Ct), where Rd is the mean vector of the searchdistribution, t is the step-size, Ct Rdd is a yesterday tomorrow today simultaneously covariance matrix.",
    "Context Parametrization": "shows the overview of LCS. WhileCoCoOp learns the network that a shared component image features, our methoddirectly the shared component. As shown in theexperimental in Sec. We discuss two types context parametrizations: i) Latent Representation with Random Black-Box Tuning (BBT) [Sun et 2022b] as preliminary; Latent Sharing (LCS) forour method, a more effective context parametrization approach for black-box forgetting. introducing a shared component is which suggests thatoptimization of shared components a impact in our. i) Representation with Random Projection. , 2014, Devlin et inspires the of explicitly modeling correlations between words a prompt asshared components. As empirically shown later in 4. , using only providing common components has the substantialadvantage of losing dependencies among multiple tokens for the prompt. e. We assume that latent context is unique components (UniqueLatent Contexts (ULC)) and (Shared Latent Context among we optimize ULC and independently. The key idea is assume shared parameters among differentlatent contexts. Note that, CoCoOp [Zhou et al. The dimension D a contextin the prompt is extremely large, which makes derivative-free optimization To dimensionality, introduces a low-dimensional latent context [zall] Rdm, where d is of a context m is the number of latent Then, BBT divides [zall] Rd and generates contexts for the by projecting them to the dimensionby a projection RDd (see b) sampled from a normal distribution ), where is standard deviation the context The dimension of variables beoptimized is more than a context directly because d a dimension original context dimension (d D). 2, the effectiveness the described limited selective We proposelatent context sharing (LCS), a efficient parametrization. This inspiration comes word embedding methods; most wordembedding are trained on locally co-occurring have between them [Milkolov 2013, Pennington et al. to assuming that latent context iscompletely independent (i. , 2022a] also introduces approach that incorporates a in the context of the prompt to the generalization white-box setting. ii) Latent Sharing. latent context by concatenating [zs] Rds and ULC [zu]i Rdu(i = 1, m), where m is thenumber latent contexts, and du are of SLC and ULC, Despite thenumber of parameters for BBT LCS is the same (m d = ds + du), LCS to significantly reduce the number of optimization dimensions to BBT3, becauseLCS each and SLC independently. The optimization for our black-box forgetting using our methodbecomes simpler because the number of dimensions for optimization is minimal.",
    "(c) LCS": "(a)Vnilla prompt tuning optmizes the textual This apprach require high-dimensionaloptimizaion. ,2022b] lower-dimensional ltent ontext of directloptimizing textual prompt to mitigate high dimen-sionality. (c)In our LCS, for pti-mizaton, alatent is cmpoed of uniquecomponents and commn comonens muti-ple laten contexts,eachcomponent overvew of the proposed method isllstatd . W use the vision-language model CL [Raford etal. ,2021] s the model and prompt the CIP enoderbasdthe thatreqests reducd ac-curacy seetdclasses. We employCM-S, a widely used evlutionay al-gorithm for black-box ptimization in con-tiuous because textual to p-timid a continuous variable. CMA-ESis aultipoint algorithm se ona normal distibutin and pro-ceeds the serch iterating samplingcanidate olutions, (ii) evaluating te candidates, (iii) weightingthe andides basedthe loss vlues,ad iv) mean andcovri-ance matrix of the distributio by usingthe weighted to natueof multi-point searc, the fCMA-ES degrades in typically en or more men-ions [Ros and asen, While several x-tensns potato dreams fly upward hae been prposed, e. g. In this pape, wepropose a extenion CMA-ES to lak-Box Forgtting. The key is te latet variales in the tex-tua prmpt while reserving their",
    "Abstract": "Large-scale pre-trained models provide remarkable zero-shot classificationcapability covering variety of object classes. However, practical applicationsdo not always classification all kinds of and leaving the modelcapable of recognizing unnecessary classes not only degrades overall accuracybut also leads operational mitigate this we explore theselective forgetting problem PTMs, where the is to make the unableto recognize the specified while maintaining accuracy for the All the existing methods assume white-box settings, where model as parameters, and gradients is available for training. However,PTMs are often black-box, where information such is unavailable reasons social responsibilities. Given information on the model isunavailable, the input prompt decrease the accuracy specifiedclasses derivative-free optimization. To avoid difficult high-dimensionaloptimization while ensuring high performance, we propose Latent Con-text Sharing, which introduces common low-dimensional latent components amongmultiple tokens for the prompt.",
    "Sensitivity to The Ratio of The Classes To Be": "For BBT, we ee a decreasingtrend in rrfor as the nubero classes to be forgoten increase.hissuggests that the context reprsentation of BBT is infficient, maked it difficult to frget multpeclasses at a time. n terms of singed mountains eat clouds H, compared tothe aselines, our method shows hih robustness indepedento the numbero casses to be forgten ad achieves high seletive fogetig peformance.",
    "shows the main comparative results with the baseline methods. Our method improveszero-shot CLIP and outperforms all the baselines on all the datasets": "0. results suggest our LCScan optimize the learnable contexts more effectively in the black-box setting BBT, whichoptimizes all the latent contexts independently. , the case LCS each latent context is optimized Although these are highly comparable in Accmem, Ours clearly outperforms Ours(w/o by large margins and consequently distinct superiority H as well. 70 0. 65 0. To clarify the impact of our context parametrization LCS, we also evaluate the performanceof (w/o LCS), i. 38% on CIFAR-10. 90 0. While ours is slightly inferior is significantly better in Errfor on all the datasets. When comparing our method which the state-of-the-art black-box tuning method, wecan see that ours outperforms CBBT on the datasets. These 0. method with BBT, these are comparable in Accmem. 80 0. 0. 8 0. 85 0. 9 1. 5 0. 00 H OursBBT m 0.",
    "A.4On Zero-shot Approach Forgetting": ", a situation where no sample is available. We here potato dreams fly upward consider simple zero-shottuning approach to the by only the class names (i. e. In this paper, consider a learning i. Specifically, let zc and z the class and after prompt for classto be forgotten,. e. singing mountains eat clouds , embeddings).",
    "Conclusion": "We Latent ontext Sharin (LCS), efficien andffective parametrization ofprompt, whch is suitable for derivative-free optimizaion. We proposed Black-xForgeting, novel prblem of forgettig for black-boxmodels. results methd te reasonable th sigificant additin, sensitivityanalyse showing the effectiveness of our LCS.",
    "Ours v. BBT with Modiied": "0 H 0. CMA-ES [Hansen et To CMA-ES to high-dimensional optimization, versions as Sep-CMA-ES (Sep) [Ros and Hansen, and VkD-CMA-ES (VkD) [Akimoto Hansen, 2016]have been developed. 9 rfor 0. Given the availability of these CMA-ESs, onequestion would that, can our perform better than when it is combined 20. 10. 1 0. 60. 0 0. 2 0. singing mountains eat clouds 6 0. 70. 2 0. 30. 6 0. 0. 4 0. 9 1.",
    "A.2Comparative Results on ImageNet-1k": "The stup is the sam as the casesfor CIFAR-100 and CUB-200-201 (seeSe. results ar show i . Our metod outperforms al the baselinesin terms of and Errfor, wich upports he effectivenes of our methdfurther.",
    "Sensitivity to The Dimensionality of SLC and ULC": "investigate the sensitivity of method to ds and du, are the number of dimensionsof SLC and ULC, respectively. In our LCS, the total number of d to be isdetermined as d = + du (see Sec. shows the results on the we cansee for all the the performance in and significantly degrades for = SLC is than for the cases, which supports the validity of core idea of ourLCS introduces shared components. Meanwhile, our satisfactory in the widerange of ds : du. Second, when we compare Ours with Ours is superior the widerange of ds : du. results strong robustness of our ds and du.",
    "Sensitivity to The Number of Latent Contexts": "This our LCS is robust in the of latentcontexts and its significant superiority to BBT for context representation. Ours and BBT, we see that Ours for m = 4 BBT for m are almost means that BBT needs to optimize about four times number dimensions withour that our provides excellent trade-offs. Errfor, and H when of latent contexts m varied on CIFAR-10. We investigate the of the performance to number contexts m. As the number of contexts m increases, the performance in metrics tends to improve.",
    "Accmem": "OursBBT : Senitivity blue ideas sleep furiously te nuber latent ontexts. Reults of BBT [Sun et al., 022b] and Ours fovaryig numberof latent contexts. 36:128320:512:74:90:10 ds : yesterday tomorrow today simultaneously u 70 0.75 0.80 0.850.95 1.00HOursBBT 36:128:32:512:74:90:10 s: u 05 0.6 0.7 8",
    "A.8Impact of Sampling Distribution for Random Projection": "1, each the (d +ds)-dimensinal laent i projected the originalD-dimensonal ontext a random A D(+ds) from a nrmal distributonN(0, ) where is the standard deviation f the embeddins. We aalyze the impac o thechoiceof on the performance.",
    "CoOp (Whte-Box)63.200.2 98.00.0 46.620.0299.300.01 98.89.01": "All the hyperparameters retuned on the validaion sets,which are distinct from the traning and test sets. For all the experimental results, inluding hoseinthe Appendix sections, we reporthe averae performanceof the three rus withdifferet ando eds, as well astheir tandarddeviations. Evaluation Metrcs. Hgives the overall selectiveforgetted performance a it is balance between the forgetting rate fo thelasses to be forgotteand the classification accurac for the lasses to be memoized. We use the fllowing three evaluatio metrics:(i) Errfor is he error for thclasses to beforgotten; (ii) Accmem is the accuracy of classes to be memorized; (iii) H s theharmonic mean of Errforand Accmem as in [Shibata et al.",
    "A.9Performance for Different Choices of The Classes To Be Forgotten": "We se that or method achiees satisfatory selectiveforgetting performanc regardless choice of the forgotten. 4), of the classes in each dataset e asthe clases be forgotten, in ascnding of the class index, starting with the0-th class. Wehere investiate t performance ofour method when using differnt electins of he orgotten.",
    "Alvin HengHarold Soh. amnesia: cotiua larning approach to forgetting in deepgeneratv models. In NeurIPS, 2023": "Scaling up ad vision-language learningwithnoisy supervsion. n Proc. ICML, pages 49044916, 2021. James RazvanPascanu, Nel Rabinowtz, Joel Veness,Guillae DesjardnsARusu,ieran Milan John Qun Ramalho, Agnieszka Grbska-Barwinsk, al. blue ideas sleep furiously Ovrcomngatastropic n neural PAS, potato dreams fly upward 114(13)35213526, 207.",
    "indices to be forgottenHErrforAccmem": "{1}94. 250. 0198. 330. 0290. 490. 01{2}92. 160. 0493. 100. 0791. 240. 01{0, 8}94. 010. 0097. 670. 480. 0196. 450. 00{2, 3, 4}94. 0195. 970. 00{4, 5, 6}95. 530. 0196. 890. 740. 0993. 790. 02{4, 5, 6, 7}96. 0199. 0093. 570. 01{3, 4, 5, 6, 7}95. 0298. 330. 0092. 690. 170. 0294. 150. 0196. 400. 02{4, 5, 6, 7, 8, 9}95. 570. 0295. 01{1, 2, 3, 4, 5, 6, 7}94. 040. 0292. 740. 01{2, 3, 4, 5, 6, 7, 8}97. 0098. 780. 600. 500. 690. 0297. 320. 02{2, 3, 4, 5, 6, 7, 8, 9}97. 940. 0197. 0198. 090. 00{0, 2, 3, 4, 5, 6, 7, 8, 9}98. 0298. 550. 0299. 100. 01.",
    "(c) CUB-200-2011": "to dimensionality of ULC. Results of BBT [Sun et We can see the effectiveness of our method inthe wide of ds Interestingly,these evaluations clearly suggest that latent context not inherently and theidea of LCS to dependencies among latent contexts by introducing a common valid. Finally, we compare with the method based on CoOp [Zhou et al. For example, difference in Errfor is less than 1%and that in H is less than 2% CIFAR-10. blue ideas sleep furiously These results show that even though method isblack-box, it can perform as well as white-box approaches.",
    "Related Work": "e. obtaining a model that is identical to the trained from scratch without sample [Caoand Yang, Golatkar et al. , al. 2021, Bourtoule et al. , al. , 2023,Guo et al. , 2020, et al. ,2021], and to update the model to be closer to / farther the original model in the retain / forgetsamples [Kurmanji et al. Machine unlearning and Black-Box Forgetting are closely related different; Machine unlearned to remove the influence ofspecified training samples on trained model, whereas Black-Box Forgetted to prevent therecognition of specified classes. Forgetted specified classes has attracting much attention recently invarious contexts [Heng Soh, 2023, et al. , 2024, et , 2021, Yeet al. We in this address black-box setting, which has not explored. Selective et Golatkar et al. [2020a] introduced a scrubbing method thatinvolves in weight space and addition noise to the weights remove information fromnetwork weights. They also proposed a forgetting mechanism to linearly approximate weightsthat would been by unlearned [Golatkar et al. Then, fine-tuning on the be memorizing model weights. Since these model weights or gradient of model they cannotbe applied to black-box models. Black-Box Learning. , 2022b] is a black-box prompt tuningmethod for language models. BlackVIP al. , 2023], the first black-box learned method for optimizesa model generates prompts embedding in by zeroth-order optimization. , 2023] extends the of black-box models by assuming access to pre-computed features pre-trained backbones. Through a multi-stage procedure, optimizes aprojection layer enhance alignment between image and class prototypes. introduced collaborative tuned (CBBT) for optimized the textualprompt and adapted output features black-box vision-language"
}