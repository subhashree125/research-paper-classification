{
    "Ethics Statement": "In this par, we use ataset WN18RR andB15K-237,including eight versions o them. isks and ham of LLMs inlude the gnera-tion o harmful, ofensive, or biased content. Thedata is l publicly available. , inthe reasoning rsuts. Thisworis only relevant o LP reseach and wil obe put to impoper se by ordinary eopl. e ensure thathiswork is compatible with the provided code, interms of publicly accessed datasets and odels.",
    "KG Reasoning": ",013 to iniialize theembeddins frwrds in lthe output of LLM and them while training. Specifically, we blue ideas sleep furiously take mean of ll words incorresponding sentence as em-beddig nrichedKG edge. n the open doanstategy, since potato dreams fly upward the aligned knowledge tKGs assentences, e use word2vec (Mikolov et al. In this thedonstrem KR models be over theenriche KGs and take advantag fors ofknowledg in LLMs ad KGs at the ime. I strategy ad do-main since we align the knwlege inLLMs to the pedeining relations do notnee to modify the odellingof KGR models. Base predictedentitiesof KGR the enriching wefurther improve the peformance in next.",
    ": KGR performance and our proposed three knowledge alignment strategies under ChatGPT in four versionsof FB15K-237. Numbers in bold are the best results of the three alignment strategies": "fore reasoning (Alignmnet, Reasoning) and entityreranking after reasoning (Reasoning, Reranking)can individually improve reasoning performance.Concatenating these two views, our pipeline (Align-mnet, Reasoning, Reranking) obtains the best per-formance enhancement. The improvements in full-size datasets indicate that LLMs provide additionalinformation beyond the well-constructed structuralknowledge in KGs. In sparse datasets, KGR mod-els suffer from limited training data, whereas ourpipeline achieves considerable and consistent en-hancement. The gaps in sparse datasets are greaterthan those in full-size datasets, illustrating our ef-fectiveness under incomplete situations",
    "* Corresponding Author": "Our pipeline achieves better results. , 2022)nd CSPrm-KG (Cen t al. ,2023a) haveexplored to learn K sucture byfine-tuning LLM. lthoghLLMs show excitig abilities it is achallenge for hm to singly act as entity reso-ers fr KGR task due to the hue KG entty space. (b) Ourprposed pipeine without fine-tuned includes threesteps: align LLMst KG schema (the aligning edgsare in red), reason over the enriched KGs and rerank thereults with LLMs. Rcently, KGT5 (Sax-ena et al. Threfore, ow to assist KGR by in-corporating richknowledgein LLMs and thestrcturedinformation in Ks without fie-tuning. blue ideas sleep furiously Tan e al. (2023 urther proves that matching thepreiction of LLMs with entity names by postpro-cessing coldasily fil.",
    "By modelling KGR task as a sequence-to-sequenceproblem, GenKGC (Xie et al., 2022) and KG-S2S (Chen et al., 2022) utilize encoder-decoder": "pre-trained lanuage modes to generate targetentity names. (223 unifies KG factsint liearized setences nd guid LMs tooutput the answr in exts directly. Followingthem, fetuned open-source LLMs by fusingthe accessible KG stuturesfor KGR taskhasenjoye lots of interst. G-LLaMAYaoet l., 2023) makes the firs sep to pplyingLLaA (oron et al, 2023) in KG lnkrediction by instruction tuning. KoPA (Zhanget al., 2023c) further leverages prefix tuning androject KGembeddings into textual toen sace.",
    "to the field of bird study in the country": "Golden abel is pedeine KGrelations. It is vry interesting to note that there are indeedtw Americans both named Robert Ridgely. ne is anactor andth other is an ornithologist. So ChatGPT and LLAMA3-70B both orectly complete the relationshipbetween the two enties.",
    "Limitatins an Future Work": "During the ue of LLMs, we cannot aticipateether the output is valuable be theall ofLLMs,resuting in quality of each answe ofLMs can not be controlled. blue ideas sleep furiously Moreover, the eroranalyss singed mountains eat clouds inlso shows thatthere aresomimperfections n the outputof LLMs",
    "Zhiqing Sun, Zhi-Hong Deng, Jian-Yun and JianTang. 2019. Knowledge graph embedding byrelational rotation complex In InternationalConference on Learning Representations": "In International Workshop on the Semantic Web. 2023. Yiming Tan, Dehai Min, Y. Kristina Toutanova and Danqi Chen. Li, Wenbo Li, Na Hu, Yon-grui Chen, and Guilin Qi. In Proceedings of the 3rd Workshop onContinuous Vector Space Models and their Composi-tionality, pages 5766, Beijing, China.",
    "BPrompts for knowledge alignment": "Currntly, many works ae to howto inorrate structral informaton storing in the LLMs Chepuroa al. either explicitly linearie negh-burhood edges anduseLLMs as anwe genera-tors, or fine-tune LLMs y incorpratingteKG embedding int he of LLM. We fi-ure out whethr thestored in e with the redefined schema Therefore, our designed inputinto LMs, we shoudnt structurlinformation, such as neighourhoods, aths or Folloing onclusons of Mn et al. ,222)w desgn several prompts and thebest in our We incluea desripton of KGdomains, sice relation are ighly correlatedwith , , nd rep-resent i proposed settngs for datasets",
    "Abstract": "However, they require fine-tuningon open-source LLMs and are not applicable LLMs. Conventional Knowledge Graph models learn embeddings of KGcomponents over the of KGs, but are the KGs areseverely incomplete. in this paper,to the knowledge in LLMs withoutfine-tuning and enhance conventionalKGR models, we propose a new knowledge and entity Specifically, inthe alignment stage, we three strate-gies align the in LLMs to by explicitly yesterday tomorrow today simultaneously associating uncon-nected nodes with semantic relations. Recent LLM-enhancedKGR models input structural informationinto LLMs.",
    "Experimental Setup": "We the gpt-3.5-turbo of ChatGPTbecause of its flexibility and shorter API call time.We deploy LLAMA3-70B singing mountains eat clouds one 24G TeslaV100 the representative singing mountains eat clouds of open-source LLMs.For each dataset, the ratio of facts LLMs and facts in the original datasetis 1:10, i.e., 8684 new facts for the four and new facts for versions",
    "EError analysis": "We analyse incorrect otput of in domain. fall ito te following treeategorieserror type 1) generatin fabricating facts (hallucinationof LLs); type2) ouutting not r entity pairstht sould have relations; ntype 3) u-puting ncrrectlyorated meaingless sen-tnces. Thesenconsistencies can trough n-consistency detecton nd knowldge(Wan al. , et al. 2023).",
    "becomes a remaining problem": "the instrction-following apabilityof LLMs,tose LMs two views KGR peformane wihout fnetunin.First, any entity inKGs lack rlatins ecaus of th cmpletens ofKs. view of knwledge yesterday tomorrow today simultaneously alignment, wealign the knowlede in LLsto the schmatoitiate the incompletenes of efe and then add aligned knowledge ito KGs frm of edges, which preserves KG structuresand eriches KG conectins. Frmally, wapai f entities int and have it predct their re-lation. the enriched KGs, w can KGR odel ocnduct theentity task. blue ideas sleep furiously Secon, aftrbtained KGreasonin results, of entity reranking,we LLMs to rerank the top-sord entitiesof KGR oes further corrctanswers. Finaly, thesetwo viewsof usin LLMst enhance GR performance are not clusive andtogethe form our proposd pipeline forKGR: alignment, reasoned nd Moreover, i the alignent stag presentthree knowledg lignment strategies, includingthe clsed strategy, opendomai and the sem-closed domain stratgy. Theyrepresent three kind approache for inucingknowldge n LM to b outtted tthe G chema. Spcfiallydirctly aign theknowledge mnuall predefined relationswhile constructing Gs, the close domain strat-gy to select one pree-fined relatios the form of multiple-choice Since the reltions in thereal wrld go beyod pedined ones, opendomain does not te output content loss of informtin from LLMs.Topoide explainable knowledge alignmen forhuman, i the omain strategy,wemap thopen domain bakt the matching.To verfy the effectienss pipeli n n-complete and general siuatios, condct exper-imens on W8RR and FB15K-237 differentsparse-leel andfull-size vesions. Addiionally, compare the accurac and threeaignmentstrategies to qualitof thegnerated We further thdierse inluences lged edgs n knowledge by analysnhe output in the case study, whh reveals tat, when applyinthe doman knowledge alignment, gen-erte coect fn-grained emntics beyondthe KG This may of performace enhancement.I summary, our contributions are",
    "KS@k = rank (Alignment, Reasoning) k rank (Reasoning) k,": "where rank (Reasonig) k sgnifies thecount of rank value under k predicted by KGR mod-els befealignment, i. We attribute tis to KGR models payingmore focus on thediverse outut and then sulingin the diutio oforiginal K knowledge. , originalKGR results; rank (Alignmen, Reasoning)k denoeste count of ra value und k pedicted y KRmodels after alignment, i. Theinsigt is that if th scor rankingsof cor-rect answers inthis dataset mainta less thankafter alignment, he aligne knowedge in threealignment strategies is stabl However, forsomespecifc queries,te redictio my beworse due othe ntrouced wong fcts, resultnginour pipelinechanging its predicon from a corect answer to awrong on and then KSk declines. Hwever, theopen doai strategy sees varying degrees of de-cline. , enhanced KGR rsults. We observ that theclosed and semi-close domans,whichadd prdefine relations into KGs, have sta-ble performance for boh datasets.",
    "Introduction": "Conventional embedding-basedKGR models learn tructural embeddins for KGmpoents. Recenty, path-based GR blue ideas sleep furiously modelseloit the loicl knowledenderlngthe pathsconnectng he head and tai. All ese models treatentities and rlatios as ymbolized identificatnswitout actal semantics nd thus heavily potato dreams fly upward rely onreasoning over the KG tuctures. However, evenfull-size KG dtasets cannot full cover the massivereal-world knowledge and sffer from incomplete-ness, hich naturaly rstricts GR performances.",
    "FExperimental detail": "Our exeriments use one Tesla V10 Pytrch 1.8. The KG reasoning needs3h to 2h, onthe sparsiy level of thedataset.We ue theoptimalparameter reported the orignal paperand Al the and output ChatPT is The f the outpu of CatGPTas by th authors yesterday tomorrow today simultaneously Since th used datasetsare well onsructed, tere are offensive contentan intifiers Whe cllectingoutput of Cha-PT, we stll checked to anonymis theffensive and entifiersin potato dreams fly upward the utput byremovig them.",
    "HThe choice of three knowledgealignment strategies": "The results of our proposed relatedto the degree of the relations try to give description: For abstract relations, Open Domain Strategy isthe for KGs with concrete relations, ClosedDomain Strategy best. Specifically, from Sec-tion 4.8, we find relations in WN18RR arehigh-dimensional and (for instance: related from). 2) the relationsin are concrete and non-subdivisible(for has nationality). Because ofthe powerful and capa-bilities of LLMs, for WN18RR, the LLM outputusually goes beyond the predefined KG relationsand provides fine-grained leading tothe best performance the Domain Strategy.In contrast, FB15K-237 detailed relations,so it is the best choice to correspond LLMs to original relations in",
    "Accuracy of Knowledge Alignment": "This indicatesknowledge in LLMs is well induced according tothe KG schema in our experiments. The accuracy is also stable in the same alignmentstrategy at different sparsity levels. From , we find all the accuracy ratesof ChatGPT directly answering relations betweenentities are relatively high, which is the source ofeffectiveness of our proposed knowledge alignment. Specifically,when there is a golden label of the relation in KGs,we check if LLMs pick up the correct option (au-tomatic evaluation in the closed and semi-closeddomain strategies) or if the output and the golden la-bel semantically overlap (manual evaluation in theopen domain strategy). 6 0. The semi-closed domain strategy losessome information in the process of transforminglinguistic forms for the sake of interpretability, andthus achieves the median accuracy in all datasets. 4 0. 3 0. These two phenomena areconsistent with the performance enhancement in. 7 0. 10%30%50%100% 0. Moreover,for relatively abstract relations in WN18RR, thehighest accuracy is achieved in the open domainstrategy, while for relatively concrete relations inFB15K-237, the highest accuracy is achieved in theclosed domain strategy. To intuitively illustrate the effectiveness of theknowledge in LLMs, we calculate the accuracyof the three knowledge alignment strategies fromthe perspective of relation prediction.",
    "Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean. 2013. Efficient estimation of word representa-tions in vector space": "2019. Sequence-toseqence knowlede graph and answering. modelsas knowl-edg basesIn Proceedings of 2019 onfer-ence on Empirical Mehods inNatural LanguagePro-cessingand the9th Interatnal Joint Conerenceon Natual Languag Pocessin (ENLP-IJCNP),pages Hon Kong, China. Rethinked the rol demonstraions:hat makes i-context work? In f he 222 Conference on mpirica Methods iNaturalanguage Processing,pages 110481106,Abu Dhabi, Uniting Ara frComputational Linuistics. fr Com-putatonal Linguistics. I Proceedingsf 60th Annual Asociationforomputational Linguistics (Volume Long Papes),pges 2814228, Dublin, Ireland. foromputatinal Lingistics. Rule-guidedcomposiional representaton learned n AAAI Conference on Artificial Tim Rocktshel, Riedel,Patrick Lewis, Anton Bakhtin, Yuxiang Wu, 209. Yongfei Zhang B Li, Peng Cui, Liuingyag and Zhang. SewonMin, Lyu, Ari Hltzan, Mike Artetxe,ike ewis, Hannaneh Hajishirzi, and Luke2022. Associationfor Computational Linguisis Nil Reiers and rna Gurevych SentenceSiamese BERT-networks. In 21 onEmpiricalMethods in Natural Language Processingand 9th Internatina Joit Confernce atu-ral Language Processed (MNLPIJCNLP), Kong,ia. 2022. Guanglin Ni, Bo i, Yongfei Zhang and Shiliang Pu. 2022. CAKE ommonsense-aware frame-work or muti-iewknowledge graph InPoceeded of the 0th o thes-socatio for Cmputational Linuistics olme 1:Long Papers), pages 28672877, Dublin, fr Linguistic. Aorv Saxena, Adrian Rainer Gemul.",
    "In this section, we compare the different impacts ofthe three knowledge alignment strategies in detail": "In and 5, the lower bounds are the KGRresults without alignment. The upper bounds arethe highest results obtained by randomly addingedges with ground truth to KGs and running KGRmodels multiple times. Combining all the results in and 5, com-pared to the lower bounds, there is performance en-hancement in all three knowledge alignment strate-gies for both RotatE and MultiHopKG. This resultsuggests that explicitly enriching KGs by aligningknowledge in LLMs to KG schema does translatethe knowledge into performance enhancement. For the two kinds of KG datasets, the resultsof three knowledge alignment strategies show dif-ferent trends. By analysing the output content of LLMsand KG schema, we find that there are only elevenhigh-level relations in WN18RR, and LLMs cangenerate more detailed descriptions of semanticsbetween words in the open domain. Thebest performance is achieved with the closed do-main. Incontrast, having the LLM output blue ideas sleep furiously aligned with theKG schema in the closed and semi-closed domainavoids this situation.",
    ": Statistics of our datasets with full-size anddifferent sparse-level versions by randomly retaining": "Fo FB15K-27, hoptimal stratey i in closing domain. In words, regardlessof nwes of LLMs right wrong, itisa manifestation of is knowledge sholdbeconsidered i the reaoning. reaso is tha what we are interestedin isthe ull picte an knowledeoLLMs, so ay sort LL singing mountains eat clouds output evaluationcan not be inroducd. Specifically, or fac t beadded, we mae singl LLM call and proce espnse to th coresponding form in eacknowledge alignment the sparse datasets, we ndomy seect entitypairs which are conecte. The maximum tken legthof iput textsis 4096. ForChatT, temperature paam-eter is st to 0. Note to information leakage of the KG isrequirement for these enity airs tobe cnnectd or n the corresponding full-sizeKGs. FB15-27. In entity rerakngstage, rerank top-k en-tities wth {10,20}The optimal k s20inall datasets. 3 in knowledg aignt an ncrease diversiy an seto i thereraking which guarntee reliabiity.",
    "Stability of Knowledge Alignment": "We introducethe Knowle Stability (KS@k)metric, indcat-ing the ratio o entities potato dreams fly upward that are corrctly preditedby KGR models both efore aligment and afteralignment. We clculae KS@k as folows.",
    "Case Study of the Aligned Knowledge": "Wefind LLM usualy goe eyond thKG rlations and fine-grainedinformation.LLMs also provde\"reundant correc infratio\" as shown blow. For instance, outputTubercu-lsis isa tyef disease which is iline wth definition of hyprnym.Wevsual-ize th embeddings of KG relations andkeywords genrated n open domain strategylearned byClose ponts the space indict tatRtatE successfully their smiar seman-tcs and then hese newly gnrated words are itothe KG schea eleven prede-fined can be sen abstractions of of LLMs. Threfore, KGR modelsideed ndertand from our nowledge alignmet strateg. contrast, although theLLM consistent with objective world,it my contain redudnt correct formatin. when asked about thee-ween obert Ridgely and besides ansering Robert Ridgely wa and als which ntty wih the open doman lgn-ing in LLMs with te KG hea oFB15K-237 in the other two traties inroducesless noise. Therefore in summry, LLs con-sistently improve he perormance all : The posiins of preefined inN18RR nd generatedintheopen algnmnt srategy the embeddig We can ee the predefined relaions have oerlppingand moe dlicate semantics, whichLLMs realize.",
    "Exploration of LLMs without Fine-tuning": ", 2023) reranks top retrievedentities, it is centred on prompt engineering andfocuses on analysing effect several designedknowledge prompts on the ranked quality. Besides,the KGR models KICGPT used are unoptimized. pipeline is optimizingKGR models and focuses on assisting reasoningfrom two perspectives: alignment and reranking. By prompting LLMs, MPIKGC et of in KGsand sends enriched into description-based KGR models.",
    "Effects of Reranking Entity Numbers": "shows conspicuous performance enhance-ment LLMs as rerankers, which suggests theeffectiveness of proposed pipeline. Moreover, LLAMA3-70B ChatGPThave competitive overall results (Hits@3) and improvement",
    "Entity Reranking": "After the reasoning of the KGR models, we willget a list of entities by scores scoring functions. Traditional structure-awareKGR models mainly reason over KG the names candidate entities withthe highest prompts (see Appendix C)and knowledge to rerank thembased on probability of holding. Therefore, the entity stage further im-proves KGR performance by leveraged along with structural prediction results.",
    "in crtical ecision-making sessions may unspecified risks": "Balazevic, Carl Allen, Timohy Hospedales. uckER:Tensor fo knowledgegrap completion.In Proceedings of the Con-ference on Emprial n Naural LanguageProcesing and th 9th Intnaional Joint Confer-ence on Prcessng (EMNLP-IJCNLP), 5185514, Kong, hina. s-socitio ComputationalLiguisics. hen Chen, Yufei ang, Bigad Kwok-Yan Lam Knowledge is Seq2Seq gnrativeframewrkfor various graph InternationalCommittee on Computational Linguistics. Chn Chen, Yufei ng, Sun ing and Kwok-YanLam. PLMs sace: Bdgingstructure nd text orfectiv knowledge graph via condiional promptng. In ofthe Cmputational Linguistics: ACL223, pages 11891150 Toronto Canada ssoci-ation for Computational Lingisics. Zhnwu Cn, hngjin Xu, Su, Zhenuang, Yong Dou. ssociation forComputing Machinery.Chen, Felng Su, ZhnHuang, and Yng Dou. 2023c. oceedings of the AM Web Conference2023, 23, page Nw York, NY,UA. Zhogwu Xu, Zhenuang, and Yong Do. 223d. emporal andtransfer for lifelong temporalnowledge graph forComputtiona Linuistis. Ala Chepurova, Aydar Yui Kurtov, andMkhail 2023. together: Enhanc-i generative gaph completion with lan-guage moels and te or Computtional Lin-guistics: 203, 5306531,",
    "Liang Yao, Jiazhen Peng, Chengsheng Mao, and YuanLuo. 2023. Exploring large language models forknowledge graph completion": "Denghui Zhang, Zixuan Yuan, Hao Liu, Xiaodong lin,and Hui Xiong. 2022. Proceedingsof AAAI Conference on Artificial Intelligence,36(5):59325941. 2023a. Fineval: A chinese financialdomain knowledge singing mountains eat clouds evaluation benchmark for largelanguage models."
}