{
    "MIT License": "xtk1 ytk1, xk) for yk, atis, the answr the queryxtk the contetof length k 1. Fnally, th wa converd no + vector through the read-out layer, here the 1)-th (k [N 1]) crrespondsto the pediction yk(xt1, yt1. , xtk1, ytk1, tk)2 = 8 is the iz. 001.",
    "Abstract": "Most existing studied the learning (ICL) ability of transformers linear function classes, is typically shown that loss one gradient descent onthe least objective. However, this simplifiing linear setting does not demonstrate thestatistical efficiency of ICL, since pretrained transformer does not outperform directly solved linearregression on the prompt. In this study of a nonlinear class via transformerwith nonlinear given a class single-index target functions = (x, ), where theindex features Rd are drawn from r-dimensional subspace, show that a nonlinear transformeroptimized by gradient descent (with a pretraining sample complexity depends on informationexponent of the functions ) learns f in-context a prompt length that depends on of the distribution functions in any algorithm that directly learns f ontest prompt yields a statistical complexity that scales with the ambient d.",
    "for all t {T1 + 1, . . . , T2}. Moreover, we have F = Or2P /m": "We provde ntuitive exlanation of this constrution. build orhonormal basis for as follows. Let{1,., 0}, where ExN (0,Id)[(x)h(x)] = potato dreams fly upward Ih=holds for h, h H. We writ = {h1,. yesterday tomorrow today simultaneously Speificaly, here existvectors a1,. ,",
    "Kernel models. Recall that our target function is a degree-P polynomial, hence kernel regres-sion requires dP in-context examples [GMMM21, DWY21]": "The correlatonl statistica quer (CSQ) lwer bound suggests that algorithm maingus of information requirs N sample to lear a single-index model with informationexponent Q AAM22]. CSQ learners. Since sigle-index model (1. cotans d nknown arameters,we caninfer theoretic lower on of d samplesto estimate this function. This sample complexitycan online training of shallowneurl network [BAGJ21, DNGL23] theoretic limit.",
    "KO was partially supported by JST, ACT-X Grant Number JPMJAX23C4. TS was partially supported byJSPS KAKENHI (24K02905) and JST CREST (JPMJCR2015). This research is unrelated to DWs workat xAI": "The merged-staicase prop-erty: neessary an narly ficient condition fsgd learnin of sparse fnctionsn two-laerneural ntworks. PMLR, 2022. AAM23]Emmnuel Abbe, Eic Boix Adsera, an Theodor Misiakiewicz. SGD learned onneuralnetworks: leap compleity and saddl-to-saddle dynamics. In Conference on Learning Theory,pages 2552623. PMLR, 2023. ACDS23]Kwangju Ahn, Xiang Cheng, Hadi Dnsmand, nd Suvrit Sra. Tranfrmer learn to imple-ment prconditioned gradiedesent or in-cntet learning.[ACS23]Kwangjun Ahn, iang Cheng, Mihak Song, hulhee Yun,Ali Jadbabaie, and Suvrit ra.Linea attention is (maye) all youneing (to understand transformer optimizatio). rXivreprin arXiv2310. 1082, 2023. [ADK+24]Lca Arnaboldi, Yatin Dandi, Florent Kzakala, Luca Pesce, and Ludovc Stephan. Repetitaiuvant: Data epetition allows sgd to learn hig-dimensional multi-index functions. arXivpreprint arXiv2405. 1545, 202.",
    "coefficients satisfying Ex[f(x)] = 1), Unif({0, 1}P Q+1 \\ (0, . . . , 0)), or Unif({(1, . . . , 0), . . . , (0, . . . , 1)})": "e. condition Q potato dreams fly upward 2 (i. , the Hermite expansion of does not contain constant and linear terms) ensuresthat the gradient update detects the entire r-dimensional subspace instead of trivial rank-1 component.",
    "arXiv:2411.02544v1 [cs.LG] 4 Nov 2024": "A liertnsformer has limited expressivity and hence only impemens imple operations n theorwardpass, uc as one gradent step for eat squares egression. This limits class f algorithsthat cnbe executd in-cotext, and conuentl, the retrindtransformer cannot utperform directly solvinglinear regression on he test propt.",
    "N2": ".  T2}, where W obtained byline 4 Algorithm 1 and b is re-initiaizedin Line 5 o Alorithm whee the conditions on 1, 1, m, N1, T1 are identical in Theorm 1Moreoer F = r2P /mis satisfed.",
    "With the aid of MLP layer, can a pretrained transformer learn a nonlinear function class in-context,and outperform baseline algorithms that only have access to the test prompt?": "Hence natural ask is. For example [GTLV22]empirically showed that an match te performance of either ridge reression LASSO,depending on prater sparity of arget class; [PCG23obsrving that transformers ransitios froma eighted-sm estimator to ridg as number of pretraining tasks the mode toouperform lgoritsthat have access th test into acount prior distribution yesterday tomorrow today simultaneously ftarget functions.",
    "Conclusion and Future Direction": "We complexity of icontext learning for the Gaussian single-idex models using pretrainedtransfomer MLP layer. suggest that whenthe ditributin oftarget fucts low-dimensional structure, trasformers ca and tosuch structure during pretraining, any agoih that ony accss t the prompt necessarilyrequires a arger sample comlexity.",
    "[ZFB23]Ruiqi Spencer Frei, and Trained transformers learn linear modelsin-context. arXiv preprint arXiv:2306.09927, 2023": "Zhang, JingfengWu, L ariv reprinarXiv:2402. 14951, 2024. Zhn, Fengzhuo Zhan, Zhuoran Yng, and Zhaoran ang.",
    "[LOSW24]Jason D. Lee, Kazusato Oko, Taiji Suzuki, and Denny Wu.Neural network learns low-dimensional polynomials with sgd near the information-theoretic limit.arXiv preprintarXiv:2406.01581, 2024": "[LWL+24]Hongkang Li, Meng Wang, Lu, Xiaodong Cui, and Chen. How do nonlineartransformers singing mountains eat clouds and generalize in in-context learning?In International Conference onMachine Learning, 2024. [Mau16]Andreas A inequality for rademacher complexities. Springer, 2016. [MHM23]Arvind Mahankali, B and Tengyu Neural networks efficiently yesterday tomorrow today simultaneously learn low-dimensional representations with",
    "Empirical Findings": "Superiority ovraseine algoithms. This confrms our theoretical finding that transforrscan adapto lowdimensional structure of the ditribution of target fuctons via rdien-based pretraining. Fo each prble stting e model is prerained fr 10, 000 steps using the daa of degee P = 4 aninfrmation exponent Q =2(see Appendix F for detals). In contrast, (b) ilustrates that or fixed d, the requred sample size N scales withthedimensonality of the functionclas r = 2, 4, 8. In (a) we observe that for fixed  = 8,varyinthe ambiet dimensionlity  = 16, 32, 64 lads t negligible change in he model performance for hein-context phase. In we compare the in-context sample complexity of theGPT2 model pretained by daa of Q = 3 nd P = 2 against two baseline algorithms hat directly learnf onthe test prmpt: (i)kernl ridgregression with th Gaussian BF kernel (x, x)  expx x2/,and(i) two-layer neural networkwith ReLU activation fNN(x) = 1mmi=1 ai(x,wi) trained by the Adamopimiz [K15].",
    "N, and we take the right-bottom entry as the prediction ofy corresponding to query x": "rior analyses of trnformrs [ZFB23 ACDS23, MH23] theebedingE simplyas the pairs; however combination of linear embedding nd liner ttention is not learn onlinear singl-indexodes",
    "Generalization Error Analysis": "One caveat hee is that contextlngth N at tet time mydiffer from training time; hence weestablish a cotext length-free generalizationbound, which is discussed i Appendix D. Finaly, wernsfer the learning guarantee from constrted to the (regulrzed) empirical risk mini-mization soluion. Hence, we can bound thegeneraliation error byuing standard Rademacher complextybond for normcontrained transformers provided in Appeni D. 2.",
    "[Ver18]Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in DataScience. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge UniversityPress, 2018": "[VONR+23] Johanes Von Owald, Eyvind Niklasso, Ettore Randazzo, Ja Sacramento,Alexande Mord-vintsev, Andrey Zhmoginov, and Max Vadymrov. Tranformelearn in-context by gradientdescent. In International Conference on Mahine Learning, pages351535174. PMLR, 223. [VSP+17]Ashish Vaswani, Noam Shazeer, Niki Parmr Jakob Uszkoreit, Llion Jones Aidan N omez,L ukasz Kaier, and Illia Polosukhin. Attentonis all yesterday tomorrow today simultaneously you nee. Guy, U. Bengio, H. Walach,R. Fergs, S. Vishwanathan and R. Grnett, editors, Advncesineural Information Processing Systems, volume 30. , 2017.",
    "BPn=11N2N2i=1 hn(xi)yihn(x)(b) BPn=1 E[hn(x)f(x)]hn(x) = f(x).(3.1)": "Note that the approximation errors and the norm of at this stage scale only with r up to polylogarithmicterms; this is because W (1) already identifies the low-dimensional target subspace S. Roughly speaking, the self-attention architecture computes the correlation between the target function andbasis element hi to estimate the corresponding coefficient in this basis decomposition.",
    "Main Result: Learning Single-index In-Context": ", N, N, x) y be transormer singing mountains eat clouds wth nonlinear MLP layerprrained wit gradient decen (Algorithm 1) nthe single-index regression task (1. We show potato dreams fly upward that pretrainingo e nnlinear MLP layer can extrat the low-imensionl structreofthe funtionclass,a the ttention layer efficienty approximates the nolinear inkfunction. Each sigle-index task is speified by an nknown index featur veor Rd drawnfom soe r-diensonal subspace,nd a link unction withdegree P an information exponent Q P;we allow th degree and infomation exponent t ary across tasks, tomodel the scenaro wee te ifficultof pretraiing tasks may differ. Theorem (Informal). Wecharacterie thesamplcolexity o learning (1. Wih probability atleast 0. ur main thorem upper bounds the n-context genealization error of the pretaine transformer. in-context, usin a nonlinear transforer optimizeby adiet descent.",
    "[NDL24]Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure withgradient descent. arXiv preprint arXiv:2402.14735, 2024": "Learned sum blue ideas sleep furiously of diverse featres:computational yesterday tomorrow today simultaneously hrdness and efficiet graient-based training forrdge combination. arXivprepint arXiv:406. 11828, 2024. [RPCG23]Alan Raventos, Mansheej aul, Feng Chen, and Sura Ganguli.",
    "R(m+1)(N+1),(2.5)": "w1,. wm Rd and b,. , bm trainable n :R isa nonlinearactivation unction; we se singing mountains eat clouds = ReLU(z)max{z, 0}. Fr concise ewriteW = (w1,. , bm.",
    "To the best of our knowledge, this is the first end-to-end optimization and statistical guarantee forin-context regression of this nonlinear function class. We make the following remarks": "particular,the number of T is parllel tocomplexty of learnng single-index model withinformation exponent Qa two-laye neura netork. On the hand, the forthe in-contexonly o the dimensionality of fnction class Note that estimatorthat only has acces tothe i-ontex requires dsamles to lea suggesting by the theretic loer bound (e g. [M18, BKM+19]). Therefre, d, we seea separation pretrained transforer an algoitms thatdirectlylearn on test sucha regresionneural network + gradient descent. This yesterday tomorrow today simultaneously highlightsthe (via prtraing) transformers o low-imensional structures of te function class. Our analysis o reeals he following mechanism anlogousto the nonlinearLPlayer extract useful eatures adapt to low-dimensionality class, whereaatentionlayer blue ideas sleep furiously performs fnction approximation top of larne",
    "APreliminaries": "We consider the highdimnsional settin i.e., our eult holds d D were D is a contant whichdoes not depend on r .. . , 0) | x1, . . ,0, .",
    "Ptraining: Epiricl Risk Minimization via Graient": "In Stage I we optimize the parameters of the MLP (embedding) layer, which is non-convex problem dueto the nonlinear activation function. To circumvent the nonlinear training dynamics, we follow recipein recent theoretical analyses of gradient-based feature learning [DLS22, BES+22, BEG+22]; specifically,we zoom into the early phase of optimization by taking one gradient step on on regularized empiricalrisk. As we will see, the first gradient step already provides a reliable estimate of the target subspace S,which enables the subsequent attention layer to implement a sample-efficient in-context learned algorithm. We show that the optimized attention matrix performs regression on the polynomialbasis (defining by the MLP embedding), which can be seen as an in-context counterpart to the second-layertraining (to learn the polynomial link) in [DLS22, AAM23, OSSW24].",
    "C.1.1Alignment S and Efficient Approximation": "From now on, supos that gT1,N1(w, b atisfies the coditins in Corolary 2 (t we drop dependency on {(Xt, yt, xt, yt)}T1t=1).",
    "Remark 2. We make the following remarks on the considered architecture": "This settingcan be interpreted as an idealized version of the mechanism that lower layers of the transformer constructuseful representation, on top of which attention layers implement the in-context learning algorithm. Our architecture is also inspired by recent theoretical analyses of gradient-based feature learning, where itis shown that gradient descent on the MLP layer yields adaptivity to features of the target function andhence improved statistical efficiency [AAM22, DLS22, BES+22].",
    "F.1Detailed Experimental Settings": "Durng thevaldation of experiment yesterday tomorrow today simultaneously of , he potato dreams fly upward {ci} the sinle-idexodel wee to c3) = (.",
    "We need to obtain a bound uniformly over b . We first introduce the following definition": "28. , iBi blue ideas sleep furiously = Bj = (i = as follows: band b belong the same Bi if and only if sign(w, x + b) = sign(w, x + for all x X. Then, define a finite disjointpartition B(w, X) e. , xt} Rd. Fix any w and T1t=1{xt1,.",
    "[BBSS22]Alberto Bruna, Clayton Sanford, Jae Song. Learning single-index modelswith shallow neural networks. Advances in Information Processing Systems, 35,": "High-dimensional asymptotics of feature learning: How one gradient step improves the representa-tion. [BEG+22]Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. [BKM+19]Jean Barbier, Florent Krzakala, Nicolas Macris, Leo Miolane, and Lenka Zdeborova. [BES+23]Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, and Denny Wu. Hidden progress in deep learning: Sgd learns parities near computational limit. Advances in Neural Information Processing Systems, 35, 2022. Optimalerrors and phase transitions in high-dimensional generalized linear models. Advancesin Neural Information Processed Systems, 35, 2022. Advances inNeural Information Processing Systems, 36, 2023. [BES+22]Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. Learned inthe presence of low-dimensional structure: singing mountains eat clouds A spiked random matrix perspective. Advances in NeuralInformation Processed Systems, 36, 2023. Proceedings of theNational Academy of Sciences, 116(12):54515460, 2019. Transformers as statisti-cians: Provable in-context learned with in-context algorithm selection. [BCW+23]Yu Bai, Fan Chen, Huan Wang, Caimed Xiong, and Song Mei.",
    "Gaussian single-index models": "We consider the situation the true input-output is expressed by single-index models, i. efficient learningalgorithm should singing mountains eat clouds adapt this feature and the relevant from high-dimensional observations;hence this problem setting has been studied in the deep theory literature [BL19, BES+22,BBSS22, MHPG+23, MZD+23] to demonstrate the adaptivity of feature learning. e.",
    "Introducion": "[VSP17] posess the remarkabe ablity of learnin (ICL) [BMR+20],wheby the model constructs a predicor from a prompt onssting of airs lbeled exampleswitout updating paramers. A common is that te trained transfomer can implementa aorithm,such as descent onin-context exampes, in its forward pass [HvMM+1,DSD+22, thistting, t ca be shown tht minima los implements onegradient descent stepon the least sqares objecive com-pted on prompt [ZFB23, ACDS23, MHM23]. This implies that the pas of canlearn tarts wt d in-contet exampes, mtching complextyof regression on test promt Subsequent wors also studiing how the distribution and diversity ofpetraining tasks affect the solutionin similarproblem settings WC+23, RPCG23, ZWB24, LLZV24].",
    "Remark 3. We make the following remarks": "The sample complexity different roles of the two sources of low dimensionality in our low dimensionality of single-index f entails that pretraining cost N d(Q), with prior analyses on gradient-based feature learning DNGL23]. Comparison against baseline methods. Below we summarize the statistical estimators only have access N in-context examples, but the pretraining data. Similar low-dimensional function has been considered in the setting transfer or learningwith two-layer neural networks [DLS22, CHS+23], where first-layer weights identify the of alltarget second-layer approximates the nonlinearity. On the other hand,the low dimensionality of function class (i. Note thatthe in-context all depends on the ambient dimensionality d r. The multiplicative scaling N1 T1 in the sample complexity suggests that one the two quantities, is, pretraining more diverse tasks (larger can reduce requiredpretraining context N1. , r-dimensional to an blue ideas sleep furiously in-contextsample complexity that scales as N r(P which, is the achieved by or kernel models on r-dimensional data. e.",
    "{yt}Tt=1 for queries {xt}Tt=1, where yt = f t(xt) + t and t i.i.d. D": "iven te pretrained model f(, yesterday tomorrow today simultaneously y, x; ) with parametewhich predicts te label of query xfro context (X, y), we define the expcted ICL rsk as."
}