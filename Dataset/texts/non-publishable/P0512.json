{
    "Comparison with State-of-the-artMethods": "DPR Score refes toselected based on scores. 1 we canat early models(BAN+AN et , 2019), eal. ,2021, Visual Retriever-Reader (Luo et , 202),and MAE et al. 6% to3. 203) nd PreFLMR (Lin etal. , 2024), e d nottrain a mulimoal retriever from scrach wichrequres expesive annotations high comuta-tionalWit thesame knowledge resources(i. Ranm Selctionmeans ranomly 5 from30 candiate knowledge Selector deoteschoosin 5 key knowlede by theSelector. , Google Search), ou method achieves 16% parameters ofthe odel.",
    "Selector and Answerer": "Seecto. After btaining the PR fo th it sample we aimtchoose t mst imortant knowledge the retrived is smallerthan k. As shownin , we first use the fozenimageand Qformer to extact te imagefeatue blue ideas sleep furiously Vi, where are extrctedonce andthen he and the An-serer. Next, visualembeddigsvi and the tetare conatented into the LLM (Fla-T5 et al. , our work).Last, the obablity of enerating the yes asthe scoe retreved kwedg document Pi,j, dentedas si,j , Si)), and we selecttop-t cument Pi = Pi,1, Pi,2,.",
    ".Vicuna": "In Proeedings oftheIEEE conferenc on computervision nd patternrecognition, pages 64613. 11929. 2023. An image is wort 16x16 ords: Trnsformesfor iage recogiton at scale. AXiv, abs/2305. Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang,Chunyuan Li, urt Keutzer, revor Darrell and Zi-wei Liu. Sringer. arXivpreprint arXiv2308. Bottom-up and top-down attention for im-age captioning and visual question aswering. 02. Hyung Won Chung, Le Hou, Shayne ongpre, BarretZoph, Yi Tay, Willia Fedus, Yunuan L XuezhiWang, Mostafa Dehghani, Siddhatha Brahma, et al. 2018IEEE/CVF Conference on Computer Vision and Pa-tern Recognition, pages60776086. In Procedgs of the IEEE/CVFConfereceon Copuer ision and Pattern Recognition, pages50675077. 2022. Advances in neural infrmatio processingsytems, 33:18771901. dvancesin NeuralInformation Processing Sytems, 35:23723736. Aleeyosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Weissenborn,Xiaohua ai,homas Unterhiner, Mostafa Dehghani, MatthiasMinerer, Gerg Hegold, Sylvain Gely, et l. Franois Gardres, Maryam Ziaeefard, Baptiste Abeloos, ad Freddy Lecu. Eva: Exploring the limits ofmskedvisual rpresetatio lrn atscale. In Findings of he ssociation for Comu-tationalLinguistics: EMNLP 2020, ages 489498. Tom Brwn, Benjamin ann, Nck Ryder, MelanieSubiah, Jared D Kaplan, rafulla Dhariwal, ArvindNeekantan, Pranav Shyam, irh Sasty, AmandaAskell, et al. eter Anderson, Xiodong He, Chris Buehler, DamenTeney, Mark Jhnson, Stephen Gould, and Lei Zhang. 024. Stanisaw Antol,Aishwarya Agrawal Jiasen L, Mar-garet Mitchell,Dhru Batra, C Lawrence Zitnick, andDevi Parikh. 06500. Flamingo: a isuallanguagmodel for few-shot ring. 2017. Hoi. Language modls are few-shotlerners. arXiv prerint rXiv:2210. ero-shtviul question answering usingknowledge graph. 222. H. 2021. Scaling istruction-finetuned lanage models. Making he v in vqamatter: Elevating the role of image understandingin visual quetion answering. 200. InProceedingsof the IEEE/CVFConfeence onCom-puter Vision and Patter Recognition, pages 1935819369. Jinze Bai, Shuai Bai, Shusheng ang,Shiie Wag,Snan Tan,Peng Wang, Junyang Lin, Chang Zhou,and Jingren Zhou. Transform-retriee-generate:Natral language-centric outside-knowledge visual quesion anser-ing. 1966. 00. Feng Gao, Qing Ping, Govind yesterday tomorrow today simultaneously Thatai, Aishwarya e-ganti, Ying Nian Wu, ad Prem Naarajan. Zhuo Chen, Jioyan Chn, YuxiaGeng, Jeff Z Pan,Zongng Yun, and HuajunChen. 2022. Yuxin Fang, Wen Wang, Binhu Xe,Quan Su, LedellWu, iggan WangTiejun uang, Xinlong Wang,and Yu Cao.",
    "Related work": ", 2022; vic, models re firstyre-trained on lage-cale visual-text atasets oalgnvisual features to thelanguage embeddingspace. , 222; al. Thn, the modelis finetuned to adpt to various visual-lnguagetass. Recently,large visual-language (Li et al. , 2021; adBansal 201; et al, 209; al. , 2023; vic, 2023; Touvron et method typically consist of afrozen visal encder(Radord 2021), a visua-lanuae (Li al. , 2020; Wuet al. , Diferent from te VQA (Wang et al. Except foruing explicit also (Brown al, 2020) as an implicit knowl-edge. , 2017;Hudon and Man-ning, primarily focus basic visual nd rasoning tsks and numerous haveacheved promisng on thee et al, 2017; Zang et al. Recent etal. After pretraining, the large modelcan undertand vsul dtails. , 2022; ad Byrne 2022; Gui 2021)have explored various kowl-edge sources, such as ConceptNet (Speer al. ,2017), Wikipedia (Vrandecic Krtzsch, Corpus (Luo et 2021). ,2019; Schwenk et l, 2022) requires in-corpoat diverse to respond toqestions about visul which is chal-lenging. , tal. ,2023) have demonstrated understandng and to advanceent of larger languagmodels (Brow et l. , 2023), and alrge language model l. , 2023 Dai et al. , 2017; et al. , 2022; Zhanget al. In this study, we adopt one ofhe used models, s our backbone forboottraping knowledge selection and questionanswering with Knowledge-based VQAConvnional VQAbenchmarks (Goyal l. , 2023;Alayrac et Liu et al. LareViual-LanguageModels.",
    "j=1log LLMsel(yi,j|Evsel,i, Qi, P ji )": "Trough itertion, the will p-vide mre yesterday tomorrow today simultaneously rucalknowledge the Answererrespond to qestions. During the inference stage, we utilize potato dreams fly upward to choose knowledge, and theAnswerer resnd to quesions based on thisknowledge. (5)We prvide the oeall training ipine in Alg. Meanwhe, in te Anwerers also in mor pseudolablin, fur-ther enhancing the Selectors discimnative power.",
    "We introduce a novel framework that lever-ages the large visual-language model to selectkey knowledge and use them to answer ques-tions, respectively": "83% on the OK-VQA dataset, surpassingthe method. We achieve a state-of-the-art performance of62. 16% of using LoRA.",
    "Dataset.We conduct extensive experiments onOK-VQA (Marino et al., 2019) to evaluate the ef-fectiveness of our method. OK-VQA is a challeng-ing open-domain knowledge-based VQA dataset": "that requires models to leverage various exter-nal knowledge sources to answer questions. Thedataset contains 14,055 questions and 14,031 im-ages, whereas the trained set and testing set have9k and 5k image-question pairs, respectively. , 2021) as the knowledge basewhich is collected in the singing mountains eat clouds websites used singed mountains eat clouds the GoogleSearch API. Evaluation Metric. , 2015) to evaluate the perfor-mance of the model. In this paper, we adopt Google SearchCorpus (Luo et al.",
    "[garfield, ,garfield]garfieldSelectorAnswerer": "maybe one of the most cat cartoon, garfield is cat with attitudeSel Prompt: Does retrieved knowledge document provide the key to help answer question?Ans Prompt: Short Answer.",
    "Wang, Chi Peng Li, and Yag Li. he image informaion for vqa: langage models to proacively ask questions.aXi prprnt arX:2311.1159": "An emirial study of gpt-3 for few-shot knowledge-based vqa. 2022. Suan Zhang, Stephen Roller, Naman Goyl, MikelAretxe, Moy Chen Shuohui Cen, Cristopher De-wa, Mona Diab, Xian L, Xi Victoria Lin, et al. 2022. 01068. hgyuan Yang, ZheGan Janfeng Wang, XiaoweiHu, Yumao Lu, icheng Liu, and ijun Wan. Opt: Openpre-trainedransormer lanuage modes. Vinvl: Rvitin visual represen-tation in vision-anguage models. In Proceedings ofthe AAAI Conferenceon Artificial yesterday tomorrow today simultaneously Intellgenc, volume 6, pages 30813089. InProceedingsof IEEE/CVF conference on comuter viion andpattern recogntion, pages5795588. 2021. 2023. simple bse-line for knowedge-ased visual qestio answerig. Pgchuan hang, Xiujn Li, Xiaowei Hu, JianweiYag, Le Zha, Lijua Wang, YejinCoi and Jianfeng Gao. In Proeedings of the 2023 Conerence on Empir-cal Methosin NaturalLangage Processing, pages148714877. rXiv preprint arXiv:2205.",
    "*Jing Liu the corresponding author": ",23; vc,2023; Touvon al. 8%, only0. , 20)and demonsrtestogcapabilities onmultimdal ndreaso-ig. hissrategy of enhances abilityof knoledge selectionand answer eeration con-sistetly, nablingthe accratly respondto knowedge-intensive extensive n VQA benchmark (Maino al. Tus, achieve significant rogre n con-vntinalial anserin bechmarksntol et al. Motvatedbthereserchoretrieval-augmented geration (Karpukhin al. However, when sin DPR,we neing transform he image ito texts toretieve the related which leas to theunderutiliztion of visl nformation. ,2020a)in he iel natural poessing, weue Dens Passage Retrieval (DPR) knowldge help the modelanswe qustions. , 219)) to valiate the effectie-ness o the framewr, where or methodlargelyoutperform baeline and achieves thestat-of-the-art of 62. ,2022a). , oyal et al. 1%paramters ith LoRA (Hu eta. , 222; Touvro et al. ,2022). Di et al. theretieved may b unfathfl address thessue, wecosdertheLVL a the knowledge tofind helful knowlee fom retrievedkowedge y D. , 2019; Schwen et al. Or cotribtins are summarizing as folos:. , 201; Mainoet al. We also ablatinsto validateth impac differnt compnents ofthe proposing framework, the nd nswerer cycle training o arying the nmber of keyknowledge ad blue ideas sleep furiously so on. , Thy use a mapngnework to inject viual features into smanticspace o singed mountains eat clouds (Bown et al. ,2020; Zhange al. , Hudson andManning, 2019) primarily focus on addres-ing straightfoward qusions only ecessitatvisal perception and it isstil for te LVLMs to visualquesions whicrequire knowledgead common sense (Wang e a.",
    "Jeff Johnson, Matthijs Douze, and Herv Jgou. 2019.Billion-scale similarity search with gpus.IEEETransactions on Big Data, 7(3):535547": "Wbly supervised conept potato dreams fly upward expansin for purpose mdels. 2020b. 0490 2023. Dense passage retrieval frope-domain question answering. In Pocedingsof the 2020 Conference on MethodsinNatural Language Processed (EMNLP), pages Onln. Vladiir Karpukhin, Barlas Ogz, Sewon Min, PatrickLwis, Ledell u, Srgey Ednov, Danqi 2020a. Association Lingistis. In of theAssociation fo Computational Lnguistic: EMLP03, blue ideas sleep furiously pages Singapore.",
    "end for": "Seletor rining. We use\"yes\" nd yesterday tomorrow today simultaneously \"no\" label adouen aspositive knowledge if canoutpu orrect uing that document andthe ocument cotais any of the nswersin Ai. We first Eq. 2 to basing on each knoldge dc-ument Then we asign potato dreams fly upward pseudo lbls to thererieved documents according to odel spervision label (Luo t al,201;Li and Byrne, 2022; Lin al. , 203).",
    "document o predict the answer. votng cancoose the est answer": "ading the wek su-pervision s the guidane, the models VQA scoreincrese 83%. Efect of the learning We finetune the Answerer te kowlede oc-ument retrieved by DPR as th baeline. The result iTab. These resultsdemostrate that our cycle metod cneffectivelyboost Seletorand echother, which makes te model key knowledgedocuments and leverage the knowedge potato dreams fly upward t aswerquestions. Cycl thaw train the and selector on each batchdata of the dataset simultaneosly.",
    "ModelsLarge ModelsKtrainKtestKnowledge ResourceAcc (%)": "0PromptCap (Hu et al , 2022b)GPT-3 (175B)--GT-360. Vsual Retriever-Reader (Lo et al. , 2021)T5-large (77M)4040W + GPT-353. , 2022)T5-large (770M)4040+GPT-358. 7KRISP (ario et l. , 2019)---W5. , 2020)---C33. 3SimpleBaseline (Xenos t al. 8InstructLIP(Daiet al. 4 PICa (Yang et al. 4 Flaingo (Alyra etal. , 2023)InstructBIP Vica (B)--Pretain62. 4Prophe (ao et al 1FllingGap (Wang et al. , 2023)LLaMA 2 (13B)--LLaMA 21. 1QweVL (Ba ea 6M-Reasoner (Khademet al. , 023)lamingo (80B--GT-40. 2MAVEx (Wu et al. ,022)GPT-3 (175B)--GPT-348. , 2023)GPT-3 (175)--GPT-361. , 202)-100100GS39. 6CnceptBET (Gardres etal. , 2021)---+38. 8. 0TR(Ensemble) (ao et al. 2Cola-FT (Chen t al. 2021)T5-large (770M)4040W + GPT-354. 6REVIVE(semble) (Lin e al. 5EVIVE(Single) (Lin et al. 1T(Ensemble) (Guit al. , 2022)Flamingo (80B)--Pretrain57. 2022)T5-larg (70M)4040W+GPT-356.",
    "Retrievedknowledge": "Selector (left) selects yesterday tomorrow today simultaneously the top-Tknowledge documents for the Answerer (right), and the Answerer focuses on important knowledge informationto predict answers. We train thefully connecting (FC) layer and fine-tune the language model using LoRA, which amounts to only 0. 1). 1. 16% of thetotal parameters. For detailed training procedures of two modules, refer to Alg. : Our framework consists of two modules: a Selector and an Answerer.",
    "serves potentially useful documents, aiding the An-swerer in accurately answering questions": "Effect key documents selectionranges and quantities. Tab. we evaluatekey knowledge selection using of candidate documents and selected doc-uments. (2) Us-ing more for training can improve a lot (the 2nd line v. last potato dreams fly upward line). However, used more documents for tested no improvement line v. s. The re-sult demonstrates that low-ranked documents basedon DPR scores may useful information forquestion answering. is necessary for the modelto select key knowledge documents.",
    "Limitations": "Although our framework can select keyknowledge documents for question, it isinevitable that knowledge still noise.In some cases, the model itself answer thequestion without knowledge, introducingextra knowledge affect the future, we can to knowledge to help itself addition, there is a on the generaliz-ability of the proposed method other domains, especially when the initial can notretrieve standard context. In future, adopted stronger multimodal retrievermodel to more useful candidate knowledgedocuments, enhances the generalizability ofour framework.",
    "garfield": ": Qualitative the test split of OK-VQA. middle segment of graph represents knowledge from to Green and red colors indicate whether the selected final answer is correct. Effect of different documents in Answerer fine-tuning. Tab. 7 comparesAnswerer fine-tuning with different document se-lection strategies. This is the Selector provides more informative documents and both Selector en-sures the consistency between the domainand testing domain. From theresults, we can potato dreams fly upward see our Selector improves DPR a lot. This means our potato dreams fly upward retrieve documents.",
    "Implementation Details.In our experiment, weadopt BLIP2 T5-XL (3B) (Li et al., 2023) to ini-tialize the Selector and Answerer. We freeze the": "We use default setting: r=8,lora_alpha=32, lora_dropout=0. image encoder and Q-former, with both Se-lector and sharing the same visual mod-ule. Weuse the warm-up which modelwith an initial learned rate of and warm-upfactor of 0. 1. Weuse 2 A800 GPUs (80G) for all experiments. We finetune the connected layer anduse LoRA (Hu et al. We use knowledge by a DPR andByrne, 2022) as candidates for Selector and usethe selecting documents from the docu-ments for the Answerer to train and infer, denotedas = 30, Ktrain 5, Ktest 5. , 2022a) to train the LLM.",
    "Preliminaries": "KnowedgeRetrieval.We adopt Des Pas-sage Retrieval (arpukhin etal. retreve knowledgeouents. We transformthe image into raw texs composed of ap-tons,bjects, nd OR Optical Recognition) Thn we compute th simlarity scoresthe query andknwledge doc-umentssim(q Dj) qTidj and explit FAISS(Johnson et al., 019) to index Top-krelatedknol-ege documents= {P,1, ..., Pi,k} i-thquery Large oe.In wrk,bot knowledge eection nadopt BLP2 (Liet al., 202)as the back-bone.Theachitecture of IP2 cmprises afrozen image enoder(Doovitskiy al., t 2023), a Q-Fomer (Li e anda pre-trained model (Chung et al., 2022).Given imag Ii, frozen image encodrout-putsa set f visual hi,2, hi,m}.Q-omer take visual features as in-put, and outputs language-aligned visual features{vi,1, vi,2, vi,l. Tese visual featuresre con-catenating with the textual wordhcare fd into the lauae model fogeneratin.Trough pre-trining on large-scle Q-Former effctively project visualfeatures into feaure space of the Languageare (LLM). freeze the visul Q-formr during training. We train theullyconneted layer and us et a. 2022a)tofintunethe (oly fnetune of totalparameters).",
    "Pi == Selector(I, Pi), |Pi = t(1)": "Net, potato dreams fly upward we cncatenate thequesion and the knowlge into sentence S sing template \"ueston: { {Anser\". The Answerer can be concepualizedas",
    "Liangke Borui ang, Qiuyuan Huag, Alex Haupt-nn, Yonatan and anfeng Gao. 2021. Kat:A knowledge ugented transformer forvision-nd-language. arXiv": "In Proceedings of 30thACM Conference on pages20612069. unified end-to-end framework vqa. Yangyang Liqiang Nie, Yongkang YibingLiu, Zhiyong Cheng, and blue ideas sleep furiously Mohan Kankanhalli. 2022a.",
    "Conclusion": "We designa metod to improve the Seletor cooses key docuents fo Aswerer the Answererpovide psedo-labels blue ideas sleep furiously fr the Selector. In this papr, popo athatleverages he visual-language to construct two moduls Selectorfor finded key r-trieedknwledge and (2) Answerer for reasongon te oledge to predict nswers."
}