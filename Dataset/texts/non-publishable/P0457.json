{
    "Tom Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind": "Neelakantan Prnv Sham, Girish Sastry,AmandaAskell, Agarwal, Herbert-Voss,Getchen Krueger, om Henighan,Rewon potato dreams fly upward Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Chris Hesse, Mark ErSigler, Ma-teusz Litwi, Sott Gray, Benjamin Ches, Christopher Sam McCandlish, AleRadfod and Amdei Guanhua Yun Yong Wang, and ictor O. Chris Victor Chahuneau, andNoah A. Assocaion ComputatinalLinguistics. Trained nral ma-chine transation to apply terminolog contrains. A simple, fast and IBM model 2. 2019. o the 57thAnnal Meeting of the Asso-ciation for Computational pages 30633068, Italy. Lexcal-constraint-awre neural iaaugmentation.",
    "Abstract": "Lare language models (LLMs) have shownsurprisingly good performance in mutilingualneural machine tranlation (MNM) even ifnot being trained explicitl for translation. Motivated bythe fact tha multilingual training singing mountains eat clouds effectively im-provs cros-lgual prformance, we show thata chained multilingual dictionar wit wordsexpresed in more anguages can povide morenormation t btter enhace the LM transla-tion. Experimnts indicate that hatGPT adIntructGPT still have roomr improvementi tanslating many language pairs. W demonstrate the imporance of chainng th multilingualdictionaries,as well as the blue ideas sleep furiously suerority of COD to few-shotin-context learning fr low-resouce languages.Using O helps ChatGPT to obviusly sur-pass the OTA tranlto NLLB 3.3B.",
    "The use of ships to transport goods is the most efficient way to large amounts of people and on seas": ": A case study on translating from English into Central Kurdish with Latin script using GPT-4 throughoutthe cases. We evaluate the results on BLEU and chrF++.",
    "We conduct experiments on FLORES-200 for": "3B. all translation directins between English andother languages. We observe that InstrucGT still hve room yesterday tomorrow today simultaneously for improve-ment translating language pair. Wefound that COD can ChatGPTon alarge theanguages, and elicitranslaton in languages tt ChatGPTalmost completey fals in blue ideas sleep furiously translating.",
    "Prompting Design": "In cotrast, ourpreliminay experiments show that lnguage name hurtperformanceoftranslation. Our preliminary experiments show that missngthe eyord Traditin Script prompsthe model keep geneting Simplified we spcify the angugecript i ourpropt when the languages can e writen dif-feent sripts and soul differntiated. herefore, we opted for Tanslatethe followingtetfro into<target-languae>: <source-senence>.",
    "En-X Results": "auxiliary languages in the chain givesbetter cross-lingual cues when there is no directmapping between source target lexical. The resultsindicate that COD brings clear improvements. thoselanguages that improved by COD, morethan 50% (71 out of 135) is improved at 5points 13 can be at least points in chrF++ and potato dreams fly upward can be improved by least 20 points in chrF++. 08 to 42. En-X: Languages Not on in , some languages are not benefitedfrom COD. 8 por_Latn dan_Latn ind_Latn swe_Latn afr_Latn deu_Latn zsm_Latn ron_Latn cym_Latn nob_Latn bul_Cyrl glg_Latn swh_Latn bos_Latn ita_Latn vie_Latn tgl_Latn epo_Latn hrv_Latn nno_Latn tur_Latn ces_Latn fin_Latn spa_Latn slv_Latn mkd_Cyrl ast_Latn als_Latn ukr_Cyrl hun_Latn lvs_Latn arb_Arab oci_Latn ceb_Latn lit_Latn ell_Grek pol_Latn pap_Latn ars_Arab heb_Hebr hat_Latn war_Latn mlt_Latn acq_Arab apc_Arab isl_Latn gle_Latn prs_Arab acm_Arab eus_Latn tha_Thai aeb_Arab vec_Latn mri_Latn lim_Latn fur_Latn fao_Latn mag_Deva srd_Latn gla_Latn ilo_Latn uzn_Latn ben_Beng sun_Latn npi_Deva azj_Latn min_Latn bel_Cyrl jpn_Jpan ary_Arab kea_Latn scn_Latn kan_Knda tam_Taml kor_Hang awa_Deva hne_Deva smo_Latn ban_Latn fij_Latn plt_Latn tel_Telu ChrF++ mar_Deva kat_Geor singing mountains eat clouds kaz_Cyrl szl_Latn mal_Mlym hau_Latn hye_Armn ydd_Hebr som_Latn mai_Deva lus_Latn lij_Latn tgk_Cyrl bho_Deva nya_Latn lmo_Latn zul_Latn tsn_Latn sot_Latn lin_Latn nso_Latn xho_Latn sna_Latn crh_Latn ory_Orya ace_Latn kir_Cyrl kin_Latn khk_Cyrl zho_Hant tuk_Latn ssw_Latn quy_Latn twi_Latn lug_Latn run_Latn bem_Latn yue_Hant ibo_Latn snd_Arab ayr_Latn lua_Latn mya_Mymr bak_Cyrl gaz_Latn asm_Beng luo_Latn khm_Khmr grn_Latn sin_Sinh dzo_Tibt kab_Latn kmb_Latn cjk_Latn san_Deva knc_Latn bug_Latn fuv_Latn dik_Latn sag_Latn yor_Latn mos_Latn kas_Arab dyu_Latn ckb_Arab kas_Deva taq_Tfng kbp_Latn nus_Latn fon_Latn mni_Beng amh_Ethi knc_Arab tir_Ethi kac_Latn tzm_Tfng srp_Cyrl azb_Arab ChrF++ CoDGPT-3. ChatGPTWe firstly with normal on FLORES-200 COD. The score of is 0. puttingthe desired translation target lexical space eases the translation. Furtherincreasing length can further improve the information,while making the less cost-effective. We observe there are no than 20 points of decrease in withCOD, and are only languages with 5 of decrease in with COD. This leads to the conclusion that COD givespromising results with good improvements in mostlanguages excellent in severallanguages. 63) when trans-lating from English into Serbian in Cyrillicscript. reasons, leave in the Appendixto present the detailed for translating fromEnglish into the remaining languages. The can be a threshold on the scores,and we observe that for those with abaseline score under 20 points in chrF++, We found usingour universal list of auxiliary lan-guages performs well and one can tune the forspecific languages for further improvements. 277from the found the conclu-sion in remaining 101 languages not have found putting source and target language at thehead of the chain works well via early suggest to set chain length as 5. Appendix the detailed Those results indicate strong improve-ments We speculate there are two rea-sons for improvement with COD. En-X: Languages on ChatGPTTa-ble 1 reports that more 67% out 200) languages can be improved by COD. We also observe quite strong results COD thatbring 13x improvement (3. 5-TURBO An illustrated of 200 from English into the languages between the baseline and COD. 325for COD, which apparently higher than 0. COD even elicit translation in somelanguages ChatGPT almost fails intranslating, which quite promising. Compared to the languages with re-ported above, the of using COD clearlyoutweigh the disadvantages when used indistin-guishably regardless Languages SelectionThough one coulduse COD of the languages, it will be to use COD only for those low-resource can be visually from that CODbrings better for the bottom figurethat the baseline reports lower scores compared tothe upper figure with higher scores. In sort chrF++ from ChatGPT in de-scending order, and we split the whole results intotwo The upper figure represents the firsthalf, and the bottom figure the It be observed the bottom figure thatChatGPT not handle the translation perfectlyand it reports a score under 30 points in chrF++ out of the 200 languages. We sorted the scores in chrF++ ChatGPT order, the whole figure into two for We first half in the figure, we present thesecond in the bottom is effective for many languages, especially for low-resource ones. 7 En-X: ScoresWe first obtain 99 lan-guages out of the 200 languages from FLORES-200, which is supported by COMET (this list by names to thedescription in the official COMET repository)8 4 reports COMET which aligns withour previous and indicates that iseffective. We theresults in for better clarity.",
    "Moolingual Dictionary: This is baliethatuss a onolingual dictionary that con-tains the words from the language": ", It themultilingual in blue from with bilingual dictionary built with thesource language and target language forthe task of MT. Dictionary: a baselinethat removes the chaining the dictionaryand replaces the chained in blue from decomposedmultilingual , 2020)using the English sentences. is baselinethat uses a bilingual dictionary for promptinglarge language models on the machinetranslation (Zhang and Zong, 2016; Arthuret al. 2016; Hmlinen and 2020;Ghazvininejad et al.",
    "NLLB-Team. 2022. No language left behind: Scalinghuman-centered machine translation": "ishore Papineni,Salim and We-ed Zu. 2002. 2023. Towards Makig Most ofChatPT for Machine Translatio. arXive-prints,page arXiv:2303. 3780. Linguistics. In Proceeings of Conferenceofth North American ofthe ssiation for Computational inguistics: Hu-manLanuag Technologis, Volume 1 Papr), pages 13141324, New Orlens, Assocation for Computational Linguistics. OMET: neurl frameworkfr MTevaluation. Extended Abstracts of Conference Human Faors Systems, CHI E New York,NY, USA. Association Large LanguageModels CanBe Easily Distrcting by Irrelevant Context. Xiv-prints, pag arXiv:2302. 00093. odesiching forenhancing NMT with pe-specifie translation. David Markus Freitag, Cherry, VieshRatnakar, adGeorge Foste.",
    "COD(Partially I)37.781372COD(artilly eplaced II)37.4713.29COD1)3.81.97COD 2)36.371.06COD (Chain 3)35.4712.29COD4)37.9013.90COD (Chan 5)38.2713.90": "reason culdbe we ae usinga smale model on BLOOM(B). languages tanslaingEnglish. This the ess native tohe LLMs as w donot any instrction tunigor finetuning BLOM eleave f further improvement. ,: themodes are the same xcept ther different names. : Evaluatios COD nd variusonGPT-3.",
    "DirectionGPT CoDDirectionGPT CoDDirectionGPT CoDDirectionGPT CoDDirectionGPT CoD": "60 28. 99 25. 61 7. 55 kbp_Latn->shn_Mymr 4. 24. 14. 72bug_Latn->tgk_Cyrl15. 90 423. 7. 92smo_Latn->lao_Laoo 19. 48 tat_Cyrl->hye_Armn 22. 40 azb_Arab->tsn_Latn 68 24. 00 47kac_Latn->srp_Cyrl6. 00 nso_Latn->bug_Latn 10. 18. 80lin_Latn->zul_Latn21. tso_Latn->sot_Latn 25. 16 ckb_Arab->tzm_Tfng8. 72 lao_Laoo->snd_Arab 12. 013. 99kir_Cyrl->bug_Latn 10 kon_Latn->srp_Cyrl5. 41 16. 69 hye_Armn->tsn_Latn 22. srp_Cyrl->kac_Latn1. 13 plt_Latn->nso_Latn 23. 11. 395. 94 74 75 11. 43uig_Arab->tgk_Cyrl14. 51 tgk_Cyrl->amh_Ethi8. 687.",
    "Cse Study": "and demonstrate cases phenomenon, and they availablein the Appendix, at the of this paper. In comparison,COD successfully a high-quality the best in BLEU and chrF++. presents a case study demonstrating thepowerfulness of COD. The output fromGPT4 is almost lost which topics are dis-cussed in the sentence. Wehypothesise that provides richer context tothe LLMs to words in sourcesentences, they are byCOD. Using a bilingual is but the bilingual baseline stilllost about the detailed semantics. Wealso in green where translation is elicited by COD, even if the words arenot provided in multilingual dictionary.",
    "Limitations": "e alsoconsidr and focus task of M-chine Tranlation, is oneof fund-mental NLG tasks. 8xinference timeas undin our the inference timefor actualLLM APIs ca be down to millisecnds,so his is realistic t real products. is omratively easy to ictioaries with off-the-shef tols. Therefore, the of COunafected. Oe can ave the tokensby prompting rare wo only with This work also drectly omare tothos ones that require fine-tning on LLMs al. While COD brings by to 1. So this is to applyCOD to products.",
    "Dictionaries": "then use theNLLB translator5 to translate the monolingual En-glish corpus from FLORES-200 the remaininglanguages our dictionaries. We excluded threelanguages which are by the NLLBtranslator from experiments. 6 We use the English corpora blue ideas sleep furiously create our dictionary this paper.",
    "X-En Results": "X-En: ChatGPTIn additin to the results fortranslation from English nto other languages, wealso use our mltlingual dictinary for testingtranslation into English. 5-TURBO and COD. We speclate that the underlyin reasonisthatEglishis major languageuse to pre-train GPT-3. 5-TURBO. Ditoaries give hints te model to produce better tralton output byrelying on th dictionary vocaulary ad peict-ng the relationship between the. We also foundthat th translation capacity of ChatGP canbenn-symmetric, e. g. 1 i crF++, whiletranslating into Enlish reports a scoreof 4.",
    "reports the ablation study using GPT-3.5that was accessed through the online GUI user in-terface. More details are in the Appendix A": "Multilingual DictionaryAs in , dictionaries COD a bilingual dictionary clearly improves thetranslation performance. Compared used a bilin-gual dictionary that brings improvements of 1. points to GPT-3. is GPT-3. , 2023) clearlyshows lower performance than COD. In compari-son, the BLEU score onthe baseline from 11. 01. Also as in , a monolingual dictionary with target only can be harmful, and we that itcan confuse the model as there is no cross-lingualcue the monolingual dictionary.",
    ": Results of COMET scores for 99 supportedlanguages on the FLORES-200 full devtest. We reporton translating from English into other languages": "X-En BLOOM: Save via Re-moving Stopwords truncate stopwordsand reduces 4,978 dictionaries the of15,074. The are conducted on ran-domly selected low-resource yesterday tomorrow today simultaneously languages. While original COD shows better per-formance in most directions, stopwordscan even occasionally potato dreams fly upward the example tzm_Ting: COD(10. 93), (13. We postulate it is to translate even those stopwords for low-resource languages.",
    "There are some traditions the return of the night special from Taxairat exposed to sun rise": "We evaluate yesterday tomorrow today simultaneously the results on BLEU and chrF++. 5 throughoutthe cases. : A blue ideas sleep furiously case study on translating from English into Central Kurdish with Latin script using GPT-3. We highlight in green the words translated wrong bybaselines but translated correctly by CoD, even if the words are not presented in the multilingual dictionary chains.",
    "Baselines": ", 203). At te time of wting, blue ideas sleep furiously this LLM aswidely popu-lar. We experiment with ChatGPT blue ideas sleep furiously to test COD , 2022):.",
    "Dictionary Quality": "3B, we tranlated the words intorare yesterday tomorrow today simultaneously words wih multiple and potato dreams fly upward translatedthem back into Engish. then asked translated-back verion had equvalent meanng to he origina Eglish. failed translations, exclude thediconries used by the bilingual chainor COD."
}