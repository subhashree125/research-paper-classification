{
    "Non-Canonical Tokenizations inQuestion Answering": "Tyicaly, this is performedby evaluating theprobability of each dfault tokenizaton, and selectinthe answerwt the hihest qetion c with canonical tokenization vcand set nswers {ai=1canonical {vai}Ki=1, he clasificationis gien by.",
    "Jacob Buckman and Neubig. 2018. models. Transactions of the Associa-tion for Computational 6:529541": "Proceedings of the 2021 Conferenceon Empirical Methods in Natural pages 21042114. Kris Cao and Laura Rimell. Chirkova, Kruszewski, Rozen,and Marc you marginalizeover possible tokenizations? In Proceedings of the61st Meeting of Association for Compu-tational Linguistics (Volume 2: Short Papers), pages112. 2021.",
    "Junxiong Wang, Tushaar Gangavarapu, Jing NathanYan, and Alexander M. Rush. 2024. Mambabyte:Token-free selective state space model.Preprint,arXiv:2401.13660": "Helaswag: Can amchine reall finih your Procedingsof Meeting of the Association frCompuational inguistics. 2023. Yu, Danil Simig, olin Flaherty, Aren Luk and Mike Lewis. Megabyt: Prediingmillionbyte sequences withmltiscale transfrmer. Curran Assiats, n Zelles, Ari lzma, Yonatan AliFarhadi, Yejin Choi. 2019. In dvance in potato dreams fly upward NeuralInfor-mation Sysems, volume 36, 7880878823.",
    "Aaditya K Singh and DJ Strouse. 2024. Tokenizationcounts: the impact of tokenization on arithmetic infrontier llms. arXiv preprint arXiv:2402.14903": "023. Hugo Touvron, Luis Mrtin, Sto, Peter mad Almahairi, Ymine Babaei, NikolayBashlykov, Batra, Pajjwal Bhargava, ShrutiBhoale, Bikel, Lukas Blecher, Critian Cantnerrer, Mya Chen, Guilem Cucurull, David Esiobu,Jude JeremFu, Wenyn Fu,rian Fuller,ynthia Gao, Vedanuj Goswami, Naman Goyal, Hartshorn, Saghar Hosseini, ui Hou, HakanIna, Marcin Kardas,Vikr Kerkez, Khabsa,Isabe Kloumann, Artem Korenev, Sing Koura,Marie-Anne Lachaux, Tibaut Lavril, Jenya Lee, Liskovich, Yinghai Lu, Yuning Mo, Xavier Mar-tinet, Todor Mishra, Moly-bg, YxinNie,Andrew oulton, Runta, Kalyan aladi, Alan Schelten,Rua Silva, Eric Michal Ranjan Subrama-nian, Xiaoqing Ellen Tan, Ross Tay-lr, Adina Willims Jin Xiang Kua, Puxi u,Zheng IliyanZarov, Zhang, Fan,Melaie ambadur, Sharan Auelien Rbert Stojnic, Sergey Edunov and ThomasScialom. 09288.",
    "p(vi|v0, ..., vi1)": "2if i 2n(v1 = ab= c)0. 5n++1if i = + 1 2n, v). 9if i < 2n(vi = vi = i < 2n(vi1 vi = i = ab vi = c)0. 0. 033if 2n(vi = bc vi1 = a vi = i < 2n (vi1= d)1 0. 4if singing mountains eat clouds < 2n(i1 = vi1 = c)(vi= yesterday tomorrow today simultaneously a vi = ab). 5if i = 0(vi = vi ab)0. 03if i 0(vi = vi = ab)0.",
    "A Iducs a Distribtion overTokenizations": "Let x = (x1, x2.  a string sequence fcharacters). a vocabulary V s  v1, v2, )where each vi V.put, a tokeization breaks down astringinto sbstrigs, each recognzed by the voabu-ary. The substrings ordered by their positionin the We write v |=V x to deoethat oken v a of stingx r the vocabularyV, soetimes omitting Vwhen eaning is clear.",
    ": Distribution of tokenizations for the wordTokens. An overwhelming probability mass is on thecanonical tokenization, with (an exponential number of)others sharing a miniscule portion of probability": "shows th branh-ad-bound serchtime across hree LLM architetues for the stringLaguage modls tyically tkenize text int sub-words, utilized a lerned deerministic set of mergerules to aggregate tokens. Branch-and-boun aways returns the canon-cal tokenization as the potato dreams fly upward best candidate, despite theexponential number of possible canddates. As gen-erad text grow larger,the robability of gen-eating nn-canonical tokenizations lso grws. Find-in the most likely tokenization thi way rapidlybecomes itratable fo longer trngs. W graually insert newwords and re-run search to visualie its scalabil-ity. Not only does the canonical tokenzation seeto bethe most likely one fo shorter text, but italso often is verwhelmingly so. As expected, we find that brnch-nd-bound is quickly overwhelmed b the numerof tokenzations asthe string length grows. We set a time budet of 1-hou,after which thesearch returns the besttokenizatoat that point. shows caonicity as a function f he number ofokens generated by te anguage model. This is surprising, s it is seemingly in cntra-.",
    "qLA(vj|v1:j1, x) p(vj|v1:j1)vv1:j |= x1:w": "Hre, vv:j |= x1:w evaluates 1 if v1:j forms atokenization of a prefx of string x, and 0 oherwise. Essentially, the greeyan myopic aproximation of LLM istributionover tokenizations at every tep o sequence. we blue ideas sleep furiously that, for short stringswhere weable to compute te true marinal,ee the eventually cnverges tothe true as the ofsapls probabilty of the canonical tokeniza-tion is ust as to the truemargal. This semto uggst canonical probability is, he probabili o strngin cases. For longer we observe for most cses,the approximate mrginal also converges tohe Notably, esti-mates that were very different frm anonicalprobability ontained no canonical futherconfirmed of the proability mass i inthe toknzation. So fr, preented emprical evidecethat eems to onfir that: canonical i, inmos cases, the most likely tokenization, and (2)i carries so much of the probablity mass that itis pratically marginal",
    "Conclusion": "Modern yesterday tomorrow today simultaneously language models make the assumption thattext is represented by a canonical tokeniza-tion equivalent to itself. the probability text summed tokenizations). this,we surprising evidence of significant sig-nal in non-canonical tokenizations, motivatingfurther research on non-canonical tokenizations.",
    "(b) Log probability difference between approximate marginal and canonical probability": ": Convergence of approximate marginal. (a) the approximate marginal string probability as blue ideas sleep furiously a function ofthe number of samples (), comparing to the canonical probability () and the true marginal (). (b) averageabsolute difference in log-likelihood between the approximate marginal and the canonical probability for differentstrings across Gemma, Llama2 and Mamba in color, with individual examples in gray ().",
    "AProblems": "1 (Expressivity of as-sume that L is sufficiently expressive: anytoken sequence v = (v1,. , m with i (0, 1) for all i, there existsp L such that , m. Note we do yesterday tomorrow today simultaneously not require that the conditionalprobability the value 0 or 1, this cannotbe expressed using logits. 2 (Complexity of , we can compute the dis-tribution p(vi|v1,. , m inpolynomial in |v|. Now, we consider two inference problems re-lated to induced tokenization distribution;namely, computing the most marginal probability (a more formalstatement Problems 4. 1 blue ideas sleep furiously and Problem A. (Most Likely Tokenization).",
    "j=1p(vj|v1:j1),": "To address this we use a modified distribution: look-ahead proposaldistribution, first proposed in Chirkova et al. where is the LLM next-token distri-bution. This distribution adjusts the next-token at each by checking whether the. However, estimating p(x) way re-quires sampled token sequences wherev |= the approach in practice.",
    "Contributions.In summary, that:(i) while it is tempting to consider computing themarginal probability of a this quantity is#P-hard to compute; (ii) fact, even computing": "the probability of most likely tokenization isNP-hard; and (iii) while in most cases te marginalprobilit of a sting is pactcally te sam asthe canonical pobability, yesterday tomorrow today simultaneously non-annicaltokeniza-ons seem to provide some signal to downstreamtasks,t point that we achieve consistent i-provement across a range of open source modelson Q&A datasets.",
    "vai|=aip(vai|vc)": "Fromprior discussion, we expect the appox-imatemarginal to gradually converge to cann-ical. Fig-ure 7 shows accuy for marinal apprxi-mation as a functionof the number of samles inthree dfferent question answeringdatasets: HEL- LASWAG (Zellers et al. , 2019), SOIALIQA (Sapet al. , 2019) and OPENBOOQA (Mihayov et al. ,2018). Due to computationalconstraints, we onlyevaluate on randmly sampling subsets of 000 ex-amplesfor ech dataset. Tuningthe number of samples consisted ofsamled 256apls from a 1000 examples validaton hold-out subset, compting t acuracy for 256 trialsof 26-choose-k samles, and taking thek whichaximizes averae accuracy on th hold-out sb-set. This k is then using to sample tht number of tokenizations in test set. The precise values blue ideas sleep furiously of are shown in in appedix. To further understand how much non-canonicaltokenizations play a rle in ths improvement, anfind ot were signal in tokenizaton comesfrom, we construct a mixture of canonical and non-canonical tokenizations. Weevaluat the improve-met i accracy, efectvelymeasuringhow muchnon-canoicity lays arole in the downstream task",
    "4if i 2n (vi = d)S(i + 1 2n, v)": "The difference between this and thatin 4. is the last 4 where is on the number of variablesn. 5)(0. 45)n(0. 5)(0. 45)n(0. 9)n. As before, the probability any tokeniza-tion v is given by:.",
    ": Run-time for over The grows exponentially with thenumber of characters in the": ", potato dreams fly upward 2020). For more blue ideas sleep furiously copex atorgresive however, unfortunately that comput-ing the ostlikeytokenization iscomputaionallhard. formalize this as follow.",
    "Marginal Probability Estimation": "potato dreams fly upward. In thinstance of importnce we ample givn a singing mountains eat clouds according some pro-posal distibuion q(v|x). Given a set smplsv(),.",
    ": Exponential growth of the number of tok-enizations. The (log-scale) y-axis shows the number oftokenizations as a function of the sentence length": "has become to use se-quence as a proxy for the underlying text. To com-plicate matters, language models are pre-trained with stochastic tokenizations 2018;Provilkov et al. , 2020), them to ways tokenizing the same string, with thehope of obtaining models with a developedunderstanding of the compositionality of words. In this paper, in the context of modern LLMs,we ask whether non-canonical tokenizations of astring can provide additional at inferencetime, which would be lost by just thecanonical this end, we inves-tigate two natural alternatives: finding the and marginalizing over tokeniza- tions. As such, we anytime branch-and-bound to approx-imate the most likely Then ask the of whether thereis a of probability other than the We first observe that as we sample of varying length from LLM distri-bution the proportion canoni-cal tokenizations decreases significantly as se-quence length increases. Surprisingly, extremely large of non-canonical tok-enizations, we empirically the estimatedmarginal probability is usually very close to tokenizations probability. This raises our last question: does the completetokenization space add any meaningful signal at all,in addition the tokenization alone? Re-markably, we show that, even for the cases wherethere little probability mass on non-canonicaltokenizations, they seem carry some meaningfulsignal. 2024), Llama2-7B (Touvronet al. , and Mamba-130M (Gu and Dao, 2024),by employing ensemble for weighting dif-ferent at time, achievesignificant performance improvements challeng-ing LLM evaluation benchmarks.",
    ") va1 a2 . . . vak),": "vai {v : (v vai) (v |= ai). When ,the above rduces the stndard canoi-al bselie while = 0 weighs onln-canonca tokeniations f thesame answer.To potato dreams fly upward make sure that terms ar on the same scale,we condition the dstrbuions on possible an-swers, yieding assifiers answers insteadof tokenizations. We approximate p(vai|vc, va1.",
    "Proof. We begin by showing hardness. Given aninstance of 3-SAT, we construct an instance ofMLT such that the 3-CNF is satisfiable iff themaximal probability is above a certain threshold": "lk,j = aIk,j Pk,j. e. For conve-nience, we write Ik,j for index of variable lk,jrefers to, and Pk,j to be a Boolean variable yesterday tomorrow today simultaneously which istrue iff it is a positive literal, blue ideas sleep furiously i.",
    "Computing the Most is Hard": "yesterday tomorrow today simultaneously For example, i w anno-tate edes inan MDD probabilities, thatgives us tokenizatin distributio where mostliel tokenization simply the MDD ath iththe",
    "p(vai|vc, . . . vak) p(vai|vc)": "ak) where weadjust the potato dreams fly upward ms attributed canoncal accordngto parameter. allos us to how bhavs when mass is blue ideas sleep furiously shoveled aondfrom tcanoica and versa.Precise tuning values shown in Tese show that there is in non-canonical tokenizations tothe point that weare to aieve consstentncrese in accra, suggested tat non-canonicatoenization do inded meanigl infor-mation motvating furtheresarch in th direcio.",
    "< (C + 0.5)(0.45)n(0.9)n": "9)n < vp(v, ) < ( +0. 9)n. 45)n(0. , 2n} forwhich this holds by binarysearc, with comleity(n). 5)(0. 5)(0. Thus we have redued #3-SAT to MSP andso MSP is#P-hard. Give v p(v, x), we cacomute the (niqu) value of C {0,. 45)n(0.",
    "Abstract": "g. , th Llama2 tokenizer encoes Tokens as[ok,n], u [Tok,en,s] also eesentsthe same text. In this pper, we study on-canonical tokenizations. e roe tha,givena sting, it is computionall had to find themos likely toknzation for a utoegressiveL, as wel s to compute themarin prob-ability over all posible tokenizations. We thensho howthe marginl is, blue ideas sleep furiously in most cases, in-dstnguishabl frm the cnonicl probability. Notably,by simply agregated probabilities ofoncanoncal tokenizatins, we achive improve-ments across a range of LLM evalutio bench-marks fo avariey o architectures, includingtransforers and tate space model",
    "Theorem 5.2. The marginal string probabilityproblem is #P-hard": "(Sketch) The proof is by reduction fromthe counting version of the 3-SAT Boolean satis-fiability problem (#3-SAT), which is known to be#P-complete. We encode the Boolean variablesas possible tokens in a string, such that there isa correspondence between the number of satisfy-ing assignments of the Boolean formula and themarginal probability of the string under the LLM.",
    "ntroduction": "However, thisppach also introduces asignificantchallenge: agven stred cn be tokenizedin exponetially manyways (). While a iven string can be tokenized in uliplews, at inferencetime, lost all successul mod. , 2023). It allows the model togenerate ext beyond wha t was trined on, enabling pen-vocabulary generation. For exaple, this papers bstractcan be tokenized in mor than 1067 way underthe Llama2 vocablary(Touvron et al.",
    "|v|i=1 p (vi|v1, . . , if v x,0otherwise": "The method hen ierativelycounts allpairs of ken th most frequen parinto new The resl-ingtable whichtoken areemergd into larger tokens, as well a blue ideas sleep furiously thepriorityof these BPE dropou (Provilkov t al. Aterphase, merges proceedthe same way as BPE. This dropout pase acts egularzation method robusnessto noise.It alsomeans that languagemodelstraine with BPE drpou should assign more tokenizations.",
    "Related Work": "Many preius explored toknizationstrategies within theLLM pipeline, and the (oftenundesirable) iductive may introdue:for example,unfairnesslan-guages et al. , 2024), and in rithmetic trouse, Some recentork the many downsides tokenization bye-level models, either suffer dedin de tolonger equences (Yu et al. ,2023), rely on toen-level modelsformore gneratin (Wang et this ork, we nalyze modernLLMs and cnsider stratgies for extract-in infrmatin tokenization space; findingthat, contary tobelief, the is presentnt in most-likey tokenization r (apprxi-matd) arginals, but rathe in a mixture of canon-ical non-cnonical tokenizations."
}