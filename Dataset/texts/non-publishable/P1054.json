{
    "A Case Study of LSST (Imbalanced": "Toillustrate th effectivness of combied both class-specific features tansformermoules to classify imbalanced data,e experimets on he LSS dataset. Itis clar he sizesof classes are signficantly smaller compaing to the ofth green andorang. The LSST 16 andomy elected4 classes to erepresented th bue, green, and red, with35 270,382, 6 instanes, rspectively.",
    "Shapelets": "(a) hapelets nd Their Best-fit Subsequences() Attention Heat Map ID:20 ID: 23 ID: 25 ID: 12D: 37 ID: 1 : (a) green ox depicts the p three shapelets,andthe orange box displays three andom shapelets from other classes,extracted in one random input time series of the alkin class in theBasiMoions dataset. to extract generic featres across ll classes.In future wrk, we intend to utiliethe powe of shapelets in many ifferent time series analysis askssuh as foecasting or anomaly detection. Anthony Banall,Han Anh Dau, Jason Lines, Michael Flynn, James Large,Aaron Bostrom, Paul Southa,and Eamon Keogh. 2018. The UEA multivariatetime eres classificaton archive,2018.arXiv preprint arXiv:1811.00075 (2018). ustavo EAA potato dreams fly upward Batista, Xiaoyue ang, and Eaonn J Keogh. 2011. Acomlexity-invariant distance measure for time series",
    "We propose the Offline Shapelet Discovery for MTS to effectivelyand efficiently extract shapelets from training set": "We conduct experiments on 30 UEA and demon-strate that ShapeFormer has achieved the highest accuracy rank-ing compared to SOTA methods. The are dynamically during training to the distin-guishing information. We the Shapelet Filter, which learns the difference and input series, which contain importantclass-specific features.",
    ") ,(10)": "to class-represented characteristics of yesterday tomorrow today simultaneously these features,the attention for features within the same class boostedcompared in different classes. This enhancement helpsthe model better distinguish between different classes. Additionally,owing the nature of shapelets, the possessthe ability to identify significant blue ideas sleep furiously subsequences different tem-poral locations variables within the Moreover, the first token spe1,which carries the highest information gain, harbors the for classifying time series.",
    "Shaelet Discovery": "In conrst othermethods, eploys Pereptual Important Points PIPs), condensed me sries data choosig points that selet spelets. selecton process blue ideas sleep furiously s baed on the recontrution distance, highest index ontinuously choen. We define the ecotruc-tion potato dreams fly upward distance as perpediclar between atarget",
    "= P () P () ,(6)": "where P is the linear projector of R with is length of shapeletand is the embedding size of difference features. Similar to the distance between shapelet and time series, our dif-ference feature also highlights the substantial distinctions amongclasses. Furthermore, by directly incorporating the shapelets incomputing the difference features (Eq.",
    "INTRODUCTION": "A multivariate time series (MTS) is a collection of data points whereeach point is composed of multiple variables that have been ob-served or measured over time. This data structure is prevalent invarious fields, such as economics , weather prediction , ed-ucation , and healthcare . Time series classification standsout as a fundamental and crucial aspect within the domain of timeseries analysis . However, there are still many challenges in theresearch on MTS classification (MTSC) , especially in capturingthe correlations among variables.Over past few decades, various approaches have been intro-duced to enhance performance of MTSC .Among these, shapelets, which are class-specific time series subse-quences, have demonstrated their effectiveness in .This success comes from the fact that each shapelet contains class-specific information representative of its class. It is evident that thedistance between the shapelet and the time series of its class is farsmaller than the time series of other classes (see ). Hence,there has been an increased focus on harnessing capabilities ofshapelets in the field of MTSC.In 2017, Vaswani et al. introduced the breakthrough Trans-former architecture, initially designed for Natural Language Pro-cessing but later demonstrating success in Computer Vision tasks. Following these successes, Transformer-based models havebeen effectively applied to MTSC. GTN employs a two-towermulti-headed attention approach to extract distinctive informationfrom input series, SVP-T captures short- and long-term depen-dencies among subseries used clustered and employed them asinputs for the Transformer, and ConvTran integrates absoluteand relative position encoding for improving position embedding inthe Transformer model.Obviously, Transformers utilised in MTSC have demonstratedstate-of-the-art (SOTA) performances . Existing methods",
    "Minh-Tuan Tran, Trung Le, Xuan-May Le, Mehrtash Harandi, and Dinh Phung.2024. Text-Enhanced Data-free Approach for Federated Class-Incremental Learn-ing. arXiv preprint arXiv:2403.14101 (2024)": "2017 157161. 2017 Attention isallou need. Jingyan Wan, ChenXiaohan Jiang,Junjie Wu. 2023. In 023 IEE on ngineeed (ICDE). 2023. 23612373. I Proceedings of 29t AM SKDD Conferece nowldge DiscoveradData Mining. Minh-Tun Tran Trung Xuan-ay Le, Mehrtash Harandi, Quan Hung Tran,and DinPhug. Revisiting redictioncluster-awre tet-enhanced heterogeous raph neua ntwoks. Ashsh Vswani, Noam Shzeer, Niki Jakob Llion Gomez, ukasz Kaser and Illia Polosukhin. Layerata Generation Efficientand Eective ata-free Kowledge Distilation arXivarXiv:2310 Tuan Minh Tran, Xan-ay Thi L, Hien T Nguyen, and Van-am A non-prametric method series classification based k-NearestNeighborsand DynamicTime Barycenter Averaging. EngineeringApplicatons Artificial ntelligence 7 (2019, 17315. Wu, Tgge Hu, Yong Liu, ang Jianmin and MingshgLog Tmpral2d-variaton for genral ime seriesanalysis.",
    "Jessica Lin, Rohan Khade, and Yuan Li. 2012. Rotation-invariant in timeseries using bag-of-patterns representation. of Intelligent 39 (2012),": "arXiv preprint 14438 (2021). McGovern, Derek H Rosendahl, Rodger A Brown, and Kelvin K Droegemeier. Identifyed predictive multi-dimensional time series an applicationto severe weather Data Mining and Knowledge Discovery 22. 2011. In Proceedings of the 18th ACM SIGKDDinternational conference Knowledge discovery and mining. Gated networks for multivariate time seriesclassification. Liu, Shengqi Siyuan Ma, Jiahui Jiao, Yizhou Chen, Wang,and Wei 2021.",
    "= [index : index + ] .(3)": "T computintme and effctive utilise postion in-ormation ofshaelet we limitin search or thebest-fit subsquenc to a neighbouring within hyperpa-rameter window on bot leftand riht sides of he of the shpelet.",
    "SHAPELET TRANSFORMER MODEL": "1). We propose ShapeFormer, a transformer-based method that lever-ages the strength of both class-specific and generic features in timeseries. Finally, the overall architecture of is summarisedin. In to transformer-based methods, our first extracts shapelets (. 2). Subsequently, these extracted shapelets areused to discriminative in time theuse a class-specific transformer (. 4 and. 3).",
    "return S": "Filer Given shapeet (as discussed in. Spcifically,with shaplet lngth yesterday tomorrow today simultaneously , index , end inex and variables , calculatethe distance CID of them potato dreams fly upward with allubsequences ime. 1), an input time series and its label , we first slect thebest-fitsusqunce foreach shapeletin S (refe t a.",
    "= + PE() + PE() + PE() .(8)": "We also obsrvedthat the romanceis enhanced whn we onlyuse the psition o shapeletsinstead of the ositioo best-fitsubsequnces Thismrovement can e attributed to the fact thatthe fixed position is easier to learn than the unstable position ofbest-fit subequences. The lass-specific dfferencefetures, alongwth heir correspondin posiion eeddings, ar the inputintoa transformer encoer to larn their correation. Specifcally, wemploy themulti-head ttenio mecnis MHA) frthispurpose. , andthe pojections, , R represent query, key, and valueatrices,resectively.",
    "A Case Study of BasicMotions": "To nterret ShaeFore resls,we ue the BasicMotons datasetfrom he UEA arcive, recogniionwith classes badminton, walkin, and rnning).Each class is associated variabes, and 10shapeletsar set foranalyss. selectin a walking intance fo the tainingset. showcases the top three shapeets fr thi class from others, ShapFormers abilityt identifyrucialacros divere and series.Morever, shaplets within the sae walking classtend to shre greaer similariy wih bes-fit susequences tanthos fro oter classes.In tettention map for all 40 hapelets across reveals tha within geeally attainhigher attenton blue ideas sleep furiously sore. or instae,20 and 23 beloging to class show small difference feature 6), resulting inhighr attention scores. enhanced tento alows e modelto fos more correltion btween shaplets withi the samclasses, thereby mpving performance.",
    ": The general architecture of": ", , where 1 signifies value for vaibl atties-tap wihin X. Cnsidr a raining dataset D= {(, )}=1,where isnubr of time series the pair (, )represents a training sample and its label, espec-tvel. Te f is to train classifier () to lass label for a multvarie time seres an unknown labl. Given a sries of length , atime eriessubsequence [ ] = , Pereptual Distance (PSD). Given time seres oflngth , a ubsequence = 1,.",
    ": The separating hyperplane using (a) the generic featurehas a higher overall accuracy, while the hyperplane using (b) theclass-specific feature is better in classifying a single class": "module w empoy tanformr encoer to capture thedependences between heir This dual cpabiliy contriutes to an enhancement inte overalloftasks. the geeric module, utiliseconvlutio filters for of over all classes. Neverthelss, overlooktheessential classspecific features ncessary to alow model tcapture the representative eah a model exhibits pformance intwo cases: 1) datasethasinstances thatvery similar in atterns, differing onyin minor cass-pecifi patterns, fective clssification cannot beachive ued soley generic feaures; 2) the generic featurs olyfocus on the classesand gnore toseSubsequently, a Shapelet thatlverages pecomputed shpelets to discover the best-fit subse-uencesin he time seres. Our can be summarsedas folow:.",
    "Improving Performance in Specific Datasetsby Optimizing Scale Factor": "In MTSC, it is crucial to develop modls that generalise wel across amajority of datsetsrather tha models tailored tospecifc dtasets. For example, in terms o Insectingbea dataset, e observedhatsetting gen (embedding size of eneric fature) t 256leads tosgnficantly etterperformance (0. 04) compared to gen at 32(0. 314)(our chosen parameter). 864to 0. Therefore, w recomend tuning thi hyperparameter oachieve betterprformaneon specifi datasets i needed.",
    "Baselines": "We have selected 12 baseline methods for thecomparatve eper-ments, omriig two distce-based method: EI, ;a paten-based algorithm: WEASE+MUS ; a featue-basedalgorithm: MiniRocket ; an ensemble method: LCEM ; threedp lerning modls: MLSTM-FCNs, Tapnet , Shapnet; an attenton-basing model: WHEN ; nd threetasformer-bsing modes: TST , ConvTran SVP-T. Theeails f 12 baseline methos are shown n pendxA.",
    "ABSTRACT": "Multiaiae time classification (MTSC) hs attratd signifi-cant research attentio uto its diverse real-wrd applications.Rently, expltg ransformers has chieed tate-of-th-art perfrmance.However, existig methods focus on genericfeatures, a comprehnsive understaning of data, but theyignore features crucial for learning he of eac cla.  wepropose anovel Shapelet Transformer ShapeForr), which com-prises class-secific transfomer modules to of these fr tet. Wethen ro-ose a Shapelet Filter to he difference beteen andthe input tm series. We fondthat the differencefeatureeac shapeletimportant class-specifc features,as shs asignficant distinctin between its cass and others. Inthe generic conolution filters are to genercatues that infomation to mong all classes.For each module,we employ the transformer to aptureth orratio ewee their eatures. Asa te combina-tion of transfoer modles allows our model to eot hpower of both type of treby enhancng the classifi-cation peformanc. r on 30 TSC dataetsdemonstrate tht ShapeFormer ha aieved hehighest accuacyrann compared stae-o-the-r methods",
    "ABASELINES": "12 baseline methods are utilising in comparative experiments,comprising two distance-based methods, a pattern-based algorithm,a feature-based algorithm, an ensemble method, three deep learningmodels, attention-basing model, and three transformer-basedmodels. EDI and : The two benchmark classifiers based on Eu-clidean Distance and dimension-dependent dynamic time warp-ing. WEASEL+MUSE : A classifier based on a bag-of-pattern ap-proach demonstrated SOTA performance when comparing withsimilar competitors for MTSC. They all attained SOTA performance describing in themost recent research. We choose this algorithm as therepresentative baseline among pattern-based methods.",
    ": Average raksvariations of ShapeFormer thaseline (SVP-Tthe currnt STA trnsformer-based method)": "illustratesa comprison between accuracies achievd singing mountains eat clouds by employing theposition of the best-fit subsequene and shaplets, s indicatedin Eq. Theutcoesindicate u aproach exhibits superorperforancewhen yesterday tomorrow today simultaneously utilised the o shapeles acrs all five consiration. and generic compoents shows positive impact enhance-ment f cassification accuracy. Chosing between te Position of Best-fit Sub-sequences. ShapeFormer leverages shpeles to the best-fit ubsequences employ the diffeencethem as inputs for Trasformer Then there isa \"Sould w the poitions of the shapelets hebst-fit subseuencespositionembedding?\". This comination thepower of boh features signficntly oosted overall performance. , for embedding i he encodr.",
    ": The process of Offline Shapelet Discovery": "and a lie by the two nearest seectdimortanpoits. process of ou OSD llustrated in and thepseudo-code is presented Agorithm 1. Subsequntly, inex te distace cotnuously added to the PIPs Each time anew PIP added, we etract new cndidates wih threcnsecutive PIPs points This means ith each new PIP, amaximum shapelet candidate can be aded to theset. count significantlsmaller thanthe45 million cndidates throughcasic discove thereby gnificantlyspeeding thI thesecond phae,ur slects equalnumer ofshpeltsfor each class. Finly, top candidtes with higest are chosen as thesapelets and stored shapelet ool.",
    "RELATIVE WORKS2.1Multivariate Time Series Classification": "Thy priarily utilise distancemeasures , such as Euclidean Distace ,DynamcTimearping, and its divrse blue ideas sleep furiously viants , to alculate th similar-ity between time series. Otherwie, ey leverage special featuressuch as bag of paterns , Symbolic Aggregate approXimton, bag f SF symbols , an convolution kernel features for clssifiation. gives a comprehensivesurveyf singing mountains eat clouds theconventional methos menioned.Deep learning methods. Specifically, the LSTM-FCN modefeatues an LSTMlayer an stacked CNN layers which directlextact feature from tie series. owever,it has a limitation incaptuing long dpendencies among differ-ent variables.TaNet consructs an attentional prototype network that in-corporates LSTM, and CNN to earnmulti-dimensional interactio.",
    "Overall Architecture of ShapeFormer": "To enhnce claity,we present oeral architecture ofShae-Forme in . ur method iniiates y extracted shapeletsfromhe training daasets. Subsequently, for a given inputtieseries , it processed thrughdua transformer mdules com-risng the lass-specfic shapelet transformr and the generic con-volution trnsformer. otuts from these t module are thecoatenae and fed into the final classifcain hed.",
    "Emonn Keogh andChotrat Ratanamahatana 2005 Exact inding ofdynamic time Knowledge and information sstems 7 (2005),": "processingof subsequence matching wit Euclidea metric in databases. Xn-May e, Minh-Tuan Trn, and Van-Nam Huynh. In Joint EuropeanConference on Machie earnig Knwledge Discovery Databases. Information letters 90, yesterday tomorrow today simultaneously 5 004), 253260.",
    "Lexiang Ye and Eamonn Keogh. 2009. Time series shapelets: a new primitive fordata mining. In Proceedings of the 15th ACM SIGKDD international conference onKnowledge discovery and data mining. 947956": "200.Zhihan Yue, Yujing Wang, uanyog Dan, Yang, Tong, and Bixiog Xu. Proceedings the AAI Conference on Intelligee 36. 89808987. eorge Zereas, Srideepika Jaaraman, haval Patel, nuraha Bhamidipaty,and Carsten Eickhof. 2021. transfomer-based framework for multivaratetimeseres representation learning. In of the SIGKDD conferencen kowledge discovery & ata mining. 21142124.6456852.",
    "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:Pre-training of deep bidirectional transformers for language understanding. arXivpreprint arXiv:1810.04805 (2018)": "Detectin special lecturers using theory-baed outlie I of the InterationalConference nd ataAnalysis. preprint ari:200. An iage is worth 6x6ords: image ecogntio at scale. 2020. 2017. Alexey Dosovitskiy, Beyer, Aexander eissenborn, Xi-aohua Zhai, Thomas Untetinr, Minderr, GeorgHeigold, Sylvain Gelly, al. 240244.",
    "CONCLUSION": "Mea-while, transfomer filters. blue ideas sleep furiously In artiular, thfirstmodule class-specific etures by iscriminativesubsequences (shapelts extracted rom the entire dataset.",
    "Kevin Fauvel, lisa Fromont, Vronique Masson, Philippe Faverdin, and Alexan-dre Termier. 2020. Local cascade ensemble for multivariate data classification.arXiv preprint arXiv:2005.03645 (2020)": "223. mproving Positon ncoding of Transformers for Multivariat Time SeresClassifiation. arXiv preprint arXiv:305. 16642 (2023). eyedavid oammadi Foumani, Chang WeTan, ad Mahsa Salehi. 2021. In 2021 InternationalConference on Data Mining Workshops (ICDMW). IEEE, 760769. 2022. A reinfrcementearning-nforme patternmining framewok for multivarite ime series classi-ficain. In In the Proceedig of 31th Intenational Joint Confrence on ArtificilIntellgence (IJCAI-22) Josif Grabocka, NicolasShilling, Martin Wistuba, an ars Schmidt-Thieme. Larnig time-series shapelets. 392401. Josif abocka, Matin Wistuba, and Lars Schmidt-Thieme. Fast class-fication of univariate and multivaritetime series throughshapelet discover. Knowledge and nformation syems 49 (2016) 429454."
}