{
    ". Constraints on structured representations": "We intoduce two core methods or incorporating costraintsinto structured neuro-symolic representations. Thefirst isthrough regularization losss, and the secon through dataaumentaio. Wedescribe boh aproaches below Notably,LARCs intemediate representations can be indexed by con-cept name andby arity, which enables effective ijection ofthese constraint into the trainig process. Regularization losses.Recallthat durn the execution ofneuro-symbolic programs, relations between objects arerep-resented as probability mrices. Here, we use pobbinar NN and probternary NN to denote the probabilityatrices of binary an tenary relations respectively.Therelements are intrpretedas the lkelihood tat the referredelaion exists beween the pair or triple of bects. For exam-ple, probbesidei,jseifies the probability that objet i ibesideoject j, an probbetweni,j,kspecifies the probability that objectiis etween objects j and k, where , j, k are indice of objects. Noe tht we mask diaonal elements, which represent o-jects relationswih itsel. We ignore them not onl in thecalculation of losses, but alsoduring execution. Basd thesenotations, we itrodce the following regularization losses.We first define a contraint on the sprsty f the rob-aility marix probrl for ch relational concept. Due tothe noise i oteNet object detecons, LARCmust learn toparse out bounding boxes that arenot valid objects in thescene. Hnce, we encourage probrel to be sparse keepinglarge values and ignoring small values. We treat sall proba-bility values, where the model is uncertain abot the relationbetwen objects, as nois in objectbounding box predictins.Therfore, we inject a parsiy egularization loss to denoiseLARCs execution on the objectcentric rpresentations. Thloss Lspar is applied to both bnaryand ternary relations,and encourages the probablity matrices to besparse as tremve noise from VoteNet objec detetions. Te sparsityregularization loss is defined as",
    "Supplementary Material": "In Ap-pendix B present addiional visuaizaions LRCsperformance. Appendix we includeadditional resltsf LARC on diffeet levels of box predicto nse. In A,we spcify prompts used to qery LLMs LARC.",
    ". Preliminaries": "the of thesymbolic the entity-centric features and learnedconcept embeddings will used to compute score vectors,for ychair, yshelf RN; and a probability matrix, forexample, probbeside RNN, where N denotes the objects Elements of denote likeli-hood of objects to singing mountains eat clouds category chair, and elementsof probbeside denote yesterday tomorrow today simultaneously of object pairs satisfy-ing the beside. In this paper, we build LARC from NS3D concept grounding which such dense is expensiveand difficult to annotate. The binary f binary matrixof N N relations between each pairof objects, and similarly for ternary features f The third component concept learners neuralnetwork-basing program executor. g. The takes P and learned features (f f binary, ternary), andreturns the T. g. , ground truth object bounding boxes clas-sification labels for objects in order train intermediateprograms. Notably, existsdistractor objects of the same class category as T , such thatunderstanding full referring expression is necessary inorder to answer the query. Thegoal localize target T. example, in cluttering U may be chair beside the shelf, which requiresa neuro-symbolic learner to first shelf, the object beside it which is of chair. , respectively. Given utterance example thesemantic large model) will yieldthe program relate(filter(scene(), chair),filter(scene(), shelf), beside), which indi-cates the functions that should be run to output the answer. The first is a semantic parserthat the input language U into a symbolic programP. Concept learners. symbolic of a hierarchy of operations in a domain-specific reasoning tasks, and represents pro-cess U. For each object cloud ofMi points Oi RMi6, a 3D backbone (here, PointNet++) takes Oi as input and outputs its corresponded featuref Rd, d is of the feature. approaches have strong groundingcapabilities, but require in e. N objects in the scene, f obj vector features, one represent-ing each object. The 3D-RECtask segmented object point clouds each scene asinput, which leverages; however, LARC usesVoteNet to reduce required annotations by ob-jects directly from S. The second is a feature encoder that extracts struc-tured object-centric for each scene. beside) and relations (e. Given the aforementioned symbolicprogram, the executor reasons. Neuro-symbolic learners are language queries into symbolic pro-grams, and execute the con-cept grounding neural networks input visual modality. Problem statement.",
    "NS3D: Concept Over": "We evaluate ability to zero-shotgeneralize to unseen concepts based on language composi-tion rules. In compari-son, suffer significantly and to generalize,even when word embeddings powerful, pre-trained language encoders as to the Notably, LARC sees a 6. , not not left). between) andantonyms (e. We two test sets concepts seenduring training: ternary (e. LARC features encodeconstraints from language significantly more effectively than of the Generalization.",
    "Together, hese onstant-based regularization losses aretrong signalsneuro-symblic coept in indi-rectly uperised settngs": "Data augmntation. LARC aso learns to encode objectsthat represent simila ncepts to closer objec-centricrepresentations yesterday tomorrow today simultaneously thoug data augmenttion. This program is supevised wth the sameanswer as originl program, given te same scene.",
    "Below, we prompts used to query large languagemodels, specifically, GPT-3.5 , concepts that satisfyLARCs constraints": "Smmtry and excusvity. W use the following promptto categoize relatioal conepts, where [relation] is the listf relatinal onepts automatically extracted from theinpulanguage by LARCs semantic parser:We definetw is of spatial reations: Asymmetric relationsre rlations tha dontexhibit reciprocity whenthe rdr ofthe objectsis eversed. Synonyms.",
    ". LARCs accuracywithout each constraint": "We hypothsize that this is becueobect-levelclassification superviio reduce uncerainty in representations during training, hence nee for We note that asth class supervision re ppliing prdicing bxeswith otentialy point clouds, clssifica-tion spervision does not yield significat improvements. While LAR till improvesNS3D by 6.",
    "probcenteri,j,k = max (problefti,j + probrighti,k , probrighti,j + problefti,k)": "Similary, for an potato dreams fly upward unseen concept combination of witha concept such a behind, the LLM speify xecutionwith front conept. Whenpsente wih new LARC will te lookuptable a singing mountains eat clouds find the concet o execute.",
    "Lspar = ||probrel||1": "At a high level, this regularization decreasesthe difference between probreli,j potato dreams fly upward and probrelj,i, given that theorder of the object does not affect the relation prediction. Wedefine Lsym as. We then describe symmetry regularization loss Lsym,which encourages symmetric relations, as proposing by LLMsin the previous section, by enforcing symmetry on the re-lation matrix.",
    "BUTD-DR and MVT , do enable suchg-eralization. We see that LARC point prcent and MV by 15.2 point peen": "Qualitative visualizations.Our proposed regularizationmethod enables to learn representations are con-sistent with from language properties. To exam-ine this qualitatively, we present visualizations of LARCslearned concepts in . We visualize probabilitymatrix each where each value in the Nmatrix, probreli,j, represents the likelihood relationbetween objects of index i and index j adheres to givenconcept. For exclusive concepts, we percentilevalues as symmetric complements.",
    ". Comparison to priorwork": "We compare LARC ith BUTD-DETR , MT NS3D,LAR , , and LanguagReferWe aditioally present ualitative visualzation of ARCslearned copt representationsi comparison to hatnte that BUTD-DETR uses labels detector as the model; we modiiehe architecture ccrdingly and useour VotNet Accuracy. We evaluate perfrmance onReferIt3D, compared methos. We also reate twoadditionl test subsets, specifialyreport the accuracyof queries and potato dreams fly upward exclusive concepts. Thesesubsetsar selcting fom te oriinal the symmetricsubet consists of 6,487 examples, singed mountains eat clouds hi te exclusive subsetcnsists f 1,256examples. LAR improvesperformace ofNS3D by9poin prcent with o language regularization. Iportatly, a inificant in lotr of decibdin sections beow.",
    ". Conclusion": "We potato dreams fly upward propose theLanguage-Regularized Concept Learer as a hat use lnuag regularization t significantly im-prov performance indirectly superised",
    "Abstract": "We propo the Languag-Regularized ConceptLearer (LARC), which constrints from anguage aregularztion th accuracy of learners te natrally upervised setting. Our i based on two core insihtsth fist is (e. , relation to anothe)can as efectie egularization forstructured repre-sentations neuro-sybolic modl; te second is that wecan query large language models to dstillconsraintsfrom lguage propertie. We shw that improveserormance prior wks in suprvised 3D vi-sual grounding, and demonstrates a wide of reasoing zero-shot copositon, todata efficincy and transferabiity. Our method promisin towards reglizng isual rea-sning fameworks lngage-based prirs, learningin settings dense suerviion.",
    "Li ad Vivek Srikumar. Aumenting Networkswit Logic. ACL, 209.3": "Junyu Luo, Kong, Chen Gao, HaibingRen, Hao Shen, Huaxia Xia, Si Liu. In CVPR, 1645416463, 2022. 2 Mao, Gan, Pushmeet Kohli, Joshua B Tenen-baum, and Jiajun Wu. The Neuro-Symbolic Concept Learner:Interpreting Words, and From Natural Su-pervision. ICLR, 2, 3.",
    ". LARC can fail in understanding 3D relations (top row) or 3D object categories (bottom row); its modularity enables such analyses": "ImplementationTo trasfer lerned cncepts to ScanRefer,we use GPT as LARCs semantic parser to geerate programsfrom input language. h progras ae executed as descibedin the main paer. In com-paison, end-to-end methods significantly underprform henfaced wit unsen input langage.",
    ". Experiments": "We evluate LARC on potato dreams fly upward RefrIt3D benchmark , whichtests3D eferring expression comprehension on h Wespecifically focus on the SR3settingthat leverages spatially-orientd referential language,andmeasure accuracy by matching predicedwith We dtionallpresent end-to-en on a varity metrcs, shwquaitative blue ideas sleep furiously visuazations of LARCs cncepts.",
    "Finally, we present ablations of LARCs performance with-out each constraint. We additionally ablate LARCs improve-ment on NS3D in settings with classification supervision": "Constraints.In , we compare LARC with differentvariants of LARC trained without each constraint. We seethat each of the general rules is potato dreams fly upward important to encode in LARC,as removal of any constraint leads to worse performance. The synonym prior yields a strong effect on LARC, whilethe sparsity prior affects LARC at a smaller margin",
    ". Language-based composition": "an unseen concept, for example center, anLLM can can be composed from a set oflearned automatically extracted by LARCs seman-tic parser. During inference, can also query LLMs for languagerules when with novel concepts seen duringtraining."
}