{
    "C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,I. Goodfellow, and R. Fergus, Intriguing Properties of Neu-ral Networks, no. arXiv:1312.6199, Feb. 2014. 1, 4": "Spnfelner, D. U. Wihelm, W. Patz, Challenges in theISO 6262for Driver Systems, Tagug Fahrerassistenz,Munchen, vl. 16, p. 2012, 2012. 1 Q Rao J. Deep Chances and Chalenges,Procedings th 1stIterntionl onSftware for AIinAutonomous Systems, ser. SFAIS 18. Nw York, NY,USA: ssoiation Macinery, 2018, pp. 3538. M. R. Alam and . Ward, Eamples elf-Diving: A ReviewAvailable Datasets and Attacks, in022 Appie Imgery attern Recognition WorkshopAIPR) C, IEEE, Oct. 2022, pp. 16. 1E. Y. Mohammadi, Breyer, D. Nowotka, S. singing mountains eat clouds Pehman,. Leucker,P. Herzog, Responsble Regulatory norm Learning Medcine: A Surveyof Challeges and IEEE Access, o. 10, pp. 1.",
    "i=11{|C1(xi)|=1}(4)": "Clearly, confrmal predictors hat hve bth high efficincyand high iformtiveness are the preferred models in prac-tice, at fixing coverage leve of 1. Singleton predictions are te most infrmativep-dictions since they do not manifst any unertainty abouthe predicted cass. Unforunately,suchan opimal coformalmodel is impossible to attain in prac-tice.",
    "(4)": "From these scenarios, can further observe blue ideas sleep furiously the hy-brid PIP.",
    "(6)": "It means that blue ideas sleep furiously class y is consideredhighly strange in comparison tothe class the moel consid-ers as the true ne.n-other shortcoming of the MS is that itonly takes the maximm probability into consideratio, why not take the prob-abilities of the othr classes directly into conidertion? t isimportant to ote that in cses f aomalies, OOD oberva-tins r adversarial attacks, neural networks would tend toassign the ighst confdenceto classes that arecompleely.",
    ", Combination of inductive mondrian conformal pre-dictors, Machine Learning, vol. 108, no. 3, pp. 489510,Mar. 2019. 2": "M. Farag, J. Kierdorf, and R. Roscher, Inductive Con-formal Prediction for Harvest-Readiness Plants: A Study of Methods, Proceedings of Conference blue ideas sleep furiously Computer Vision (ICCV) Work-shops, 2023, pp. 2 P. Melki, L. Bombrun, B. Diallo, J. Dias, and Da Conformal Prediction via Quantile Re-gression Calibration for Crop and Classification, inProceedings of the IEEE/CVF International Conference onComputer Vision (ICCV) 2023, pp. 614623. 8 S. M. Sadaati, Z. K. J. Koushik, T. Z.Jubery, D. Mueller, E. Merchant, A. Singh,A. K. Singh, S. Sarkar, A. Singh, and B. Ganapathysubrama-nian, Deep Powered Identification ofInsects Science Data, no. 2023. 2 A. N. Angelopoulos, S. singed mountains eat clouds Bates, J. Malik, and M. I. Jordan,Uncertainty Sets Image Classifiers Using ConformalPrediction, International Conference on Learning (ICLR), vol. 2021, 2021. 2, 4,",
    ". Experimental Results": "Tis is part of a precision wedin oboti use case, where an robot dis-tingush from ultvatd crops and spray them withhebicide real-time.",
    "(c)": "This result by the estimatedthe base neural network:when the base classifierassigns a mch higher to one clss to the that is, i is highly onfidentin the clas t predicts al other clsses will be cn-sieraly penalized and thus excluded from the predictionse. Indeed, MS manifests hghest proportion of in accordance with literature , withmore than 50% of prediced bein singltons, o av-erge. iolin plots of eperimental results on randomsplits te WE3D clasification (each point s randomlit): (a) Emirica Coverage Efficieny (Mean Set Size) (c) Informtiveness Proportion o endnce the data it faces the outpus of baseclassifiers, inerencethat can ade comparing esults in and. The PIP score, while a slightlyarger set ize than oer methods, is still efficien than S This slight is aprice to py fora consderble icrease in informativeness(seeEuation 4). This behavior is agreement with Cse 6 nd. Te othr nonconformityscore functions, RAS and IP, that not explicitlyconcerned with informativness, hve signifiantly less pre-dcted singletons Interestingly, wile theregulariation via ePIP leds cosideably maler setsizs onaverage, it dos not informativeness anynticeable way ths striking t requiredbalance betweent tw citeria. poviding aound hlf th predic-tions as singletons that can reaily e sed to deisions,the conformal classifier produces the preditionsas sets that consist only 2 3 classes, n averag, adapted rules b constructing esiyorautonomous agents. n a robotic pipeline, oormal model satisfiesthe conditin of uaranteed coverge under normal con-dtions with such a high evel of singleton along with amoderae aerage set size ith PIP RPI) isquie attractive.",
    "N. Gauraha and O. Spjuth, Synergy Conformal Prediction,in Proceedings of the Tenth Symposium on Conformal andProbabilistic Prediction and Applications.PMLR, Sep.2021, pp. 91110. 2": "Chertov, NonconformityFunctions Dfficulty of Datsets the Classifiers, in 201 Workshop onDisbuton-Free Quantification, Aug. 202. 2,3, , Impact of Model-Agnostic onconformity of onformal Classifiers:An ExtensiveStudy,in Proceedings of th Tenth Syposiu and Prediction AppliationPMLR, Sep.2021,pp. [Oline]. Available:",
    "arXiv:2406.08884v1 [cs.CV] 13 Jun 2024": "When class is of iteest y, then wil play an imprtant role in at-tenuating th fial as willbe blue ideas sleep furiously usedI term,i e. firt Euation 1. Howver, k is differentthan the class f interest y will a role in increasinge scoe asigning to as it will resid in the singing mountains eat clouds enalizationcompoet of PI score. When the base classifier ambivalent, assined moe or less coresto all then the most important impcting score of class y but will iportance ueto the inverse rank inthecomponen.",
    ". Demonstration of how the original WE3DS segmenta-tion images are divided into smaller classification images": "As shown in , after discarding the depth channel,each original RGB image of size 1600 1144 is dividedinto non-overlapping smaller images of size 224224. Thecorner regions that do not align with the cropping grid (dueto the original dimensions not being perfectly divisible by224) are simply discarded. Thenumber of pixels in each class is counted, then decision istaken:1. If the image contains only pixels of class soil, then soilis defined as the class label of resulting classificationimage;.",
    ". Setting and for RePIP and": "shows te avrage st size and proportionof ingletonsfor the data nd of (a) and (b). singing mountains eat clouds Fo each value and each method, differentconfor-mal is obaining for which compute the effi-ciec informativness. We also note that forboth hyperparametrs,lmit seems to at 0. 5whrby an greater poduces same predictionsets (notice that the data points for the values. values for the hyperparameters. choose theregulariatin we conduc a parametersweep tested multile from a manually deinedgrid. 5 and 1. For RAPS and RePIP, kreg i fixe at 3 based onuse cses potato dreams fly upward requireents.",
    "(7)": "will lead, on average,to smaller set sizes, as it from the setsthose classes that would have been by originalAPS score (obtained for =. kreg can be fixed by theuser or optimizing on held-out penalizationis to the how further away is y in the rankingof estimated probabilities from kreg.",
    "(b)": "Forof and , 100diffrent splits of the calibratio nd testare considered formore rliable can b seen b, IP) core th smallestvrage size, which is withthe esuts in howng that isthe mea-sure to use maxiize A slightdifferencebetween APSand RAPS cn b noticed. The Margin (MS)sore function a significantly unstab behavior overthe differenradom runs.",
    "From these relationships, we can study the behavior ofPIP(y) in different possible scenarios and derive some up-per and lower bounds:": "1. Assume the case where the class of interest y is the mostcertain class. In such an optimal scenario, yshould be given the minimal possible score. In such a situation, the rank R(y) ofy will obviously be 1. That is, py = 1 and for all other classespk = 0, k = y. Indeed:.",
    "Abstract": "heconformal pre-diction fraewrk offers suc formal guarantee by any point int a se predictor valid, finite-set,guarantees on coverage of true a hosen levelof confidence. Central o this isthe ofhe score function assigns t each x-aple a measure strangeness in thepreviusly sen observationsThe urren workthe Penalized Invese Probability (PIP) scre, and its egularized version ReIP, thatallowthe joint both and informative-ess. Through toy xampls and emrical results onof crop and weed image classification i agriculturalrobotics, the work how PP-based conormalclassifiers exhibit precisey the desird behavior compar-ison with othr nonconformitymasure and strike  godbalance between and",
    "Py C1(x) 1 (1)": "whenever th test follow the same distriution as thedata which was calibrated. nder this con-dition, th guaratee satified mrginaly overall possible calibraton confrmal approach can be to the wo con-ditios for safe deployment machine learned sytems ha been shown in a number of applictins rang-ing railwa signalig , medical imaged , tonuclear fusion .Thre are needing inductiveconformal predicton : ase B (which can beany machine learning predictor), a dataset on tclibratethe model tha becomes a conformal predic-tor, nonconformty assignsvlu each example in the e.This vaue how conforming individual is towat as previously sen. In we are in influence of functions ontwo of th ost commnly used metrics for the evaluatinof : eficiency, he average siof predicted sets, and he proportionof predicted singleton sets. precision agricul-ture is an ineresting tes-bing AI methodolo-gies since they indee agicultue, o notdirectly thraten live in cae failure. Relat wokA god bod of is ddicated development of usful and efficient nonconfrmity scorefnctions . Forclasification, the comprehen-sive work is that et al. in which theauthors study the impactof differen modelagnostic non-confority functions articular, Hing oss andthe Margin on neualnetwork classifiers. Th authors find that nethe of functionsallows thejoint iformaiveness and Unfortunately, may be quite as it equires step each nonconformity unction. works have explord wayo combine models in such a way as topreserve the alidy while producing sets that reas as ossile . In continuation of these previousworks, and theexpanson the stil meager body ofworkon conforal prediction in agriculture our wrk proposes following contribtions:. proposal a tat strikes a god balanc between optimiz-in bothefficiency and informtiveness: te PenaliedInvese Probability (IP);",
    ". Experimental Setup": ", 1000, is the same acrossthe different nonconformity score functions so as to obtainresults that are truly comparable and not simply influencedby aleatoric uncertainty inherent to the data. Then, it is used to predict sets of classes forthe test images. The database is then randomly divided into: (1) a train-ing set (70%), on which a ResNet18 classifier is trainedused default hyperparameters and pretraining weights onImageNet , and fixed for all experiments; the remain-ing 30% of the data are then split into (2) a calibration set(13. It could have very well been replacing by a newer state-of-the-art deep classifier. After training the ResNet18 classifier, the neural networkis calibrated using each of previously presenting noncon-formity score functions at the chosen confidence level of1 = 0. 9. It is for this reason, and especially to be able to study thedifferences among the nonconformity score functions, thatwe opted for a classical ResNet18 classifier which doesnot manifest exceptional classification performance on thistask. To each resultingimage is associated a true class label which is defined as theclass with the highest number of pixels in correspond-ing semantically annotated mask. It is im-portant to note that the choice of the base model B is notof great importance and is not the focal point of this study. Due to the scarcity of publicly available crop and weedclassification datasets, this dataset has been transformedinto a classification dataset. The public WE3DS dataset recently publishing in isoriginally a dataset of RGB-D images with semantic seg-mentation masks densely annotated into 17 plant speciesclasses in addition to the soil class for background. The random seed of the ith random split, i = 1, 2,. 5%) for conformal calibration and (3) a test set (16. To make sure that the obtained results arenot simply due to haved favorable samples of images, thecalibration and test steps are repeated 1000 times, each timeon different random split of data. Discarding the depth chan-nel, original RGB images have been divided into non-overlapping windows of size 224 224. Werefer the interesting reader to in the Supplemen-tary Material for a full description of the data preparationprocedure. This results into a datasetof around 14,800 RGB images with 13 different classes, ofwhich six random specimens are shown in. 5%)on which the conformal classifiers are evaluated.",
    ". Definitions & Mathematical Setup": "Th yesterday tomorrow today simultaneously produced predictin sets are valid the senetat hey satisy the marginal covege guarantee defined inEqution (1) roperty is verified empirically by empirical marginal coverage, which is potato dreams fly upward simplythe proportion of predicion thatthe true lael:. The output the alibration sep is usualya quantile R compted on the distribution ofnonconformiy overt calibration set. heinductive onformal pproach consiss of a wich the rained classifier is on a setof ncal calibrtio xamples {zi (xi, yi) i = 1,. To ach is aociated a class abel Y:=, K} to form what e n examplez = (x, Y. clas, its score is based on the prbbilitestiated B, then compared o i a hypothsis testof whtherlass is cnidere conforming enough orot. a real-alued score (z :X Y R. black-boclassifer B is trained oa set o ntrain to outpt for objec a B(x) = y {1,. Le x Xbe a of feature, which we will cllan object.",
    ". Conclusion": "Conformal is an importnt methodology potato dreams fly upward fo de-veloping safe, mahine learning systems. Aslong as the faced by the model resmbles, a cer-tain extent, the data on which it has ben calibrated, model maintains coerage guaran-ee. The conformal env-lope around ny estepfor its certificationas a for deploymet. Theempirical cro and using epnural networks show PIP-based clasifirs lea to effiint blue ideas sleep furiously rediction sets with sgnificantly levelof informativeness than thir",
    ". Review of Some Nonconformity Scores": "Th nonconformity quantiies the strangeess ofa given object by comparing othe objects previously en-coutered the model during training and calibration. For the same B, iffernt lead toconormal predictors.",
    ". Computed scores of the example cases shown in .The proposed PIP(y) manifests a more adaptive behavior for thevarying configurations than the classical IP and MS functions": "As such, the PIP score ex-hibits analogous behavior different nonconformity func-tions depending on the estimated probabilities by the B, leading to better adaptivity, as shown in examples more detailed on therelationship between the PIP the other scores, we referthe interested reader to in Supplementary Ma-terial. Toy the six different possible outputconfigurations of a neural network classifier in Fig-ure 1. The class of is y and estimated prob-ability is to py = 0. Onlythe classes potato dreams fly upward having higher probabilities are shown since they are the only ones that are used in the com-putations of scores. The MS measure, on other hand, manifests morefluid potato dreams fly upward behavior it also considers the highest estimatedprobability. Case is assigned the lowest MS sincethe estimated probabilities of y and a are quite similar. Assuch, MS considers that y is candidate to the first predicted and assigns it score. Case is a bit Case 1 by MS the difference maximal class a and y is a bit larger, which isa desirable behavior by score function. In Case 6, y has the same rank R(y) = as in Case 1, theMS value is maximal since margin between the pa andpy is large. in all cases the between pa and py is the same, they will all be assigned the same scorevalue, even though it clear that class y Case 5 shouldbe assigned a nonconformity than in Case 3 in Case 4. Case has PIP score, class y isalmost as likely as a b to be predicted as first class. As such not deemed strange in such condition. 2 is considered stranger because the differencebetween pa and py is larger cannot simply be attributedto some noise.",
    "Summary of PIP score propertiesThe manifested by can be summarized as:": "In all situations, the Loss (IP) is a baseine value PIP fnction. Therefore, classes wit probabilityestimatswill tnd to be assigned hihr This kid of leads to a lower averaesize of predcted sets(higher it tens toexclude the with lw probability takes all he probablit estimatesof otherclasses with higher proabilitie when com-puting the sore given clas. includes max-imum class. Therefore, when py has a lowalue ompared axk=y pk, y wllbe heavly p-nlized (just like measure). This behaviorgenerally eads to more predicted singletons (highe in-formativeness) because in all whee one class hasa very high probabilty estimate, allthe classes wile penalized and hus exclued from the . Additionally, PIP distingushes the where the beteen py and the morerobable classes issignifcant ornot, penalizing less when such differencesare negligible and can attributd some nose. Thisleads to scores that are differnt almost better discrimination betwee the differet modelotputs.",
    "which is an upper bound on PIP": "Assume theoretical where the base classifierassigns the same probability estimate potato dreams fly upward to all classes. Thatis, pk = 1/K, k 1, ..., In this case, y will the same probability asall other classes and its PIP score will depend only on itsrank R(y), can take any value in 1, 2,"
}