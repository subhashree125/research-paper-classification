{
    ". Preliminaries": "To learn powerful 3D representations, current 3D paradigm (stemmed from SLidR ) potato dreams fly upward utilizes blue ideas sleep furiously thecalibrated between images and 3D pointclouds with the powerful 2D models. K points, each point Pk R4 representsthe k-th points location (x, y, cou-pled its intensity feature. Meanwhile, pointcloud frame is equipping with L surrounding ={Il | l = 1,. k. a, superpixels introduced in) obtaining from 2D segmentation :RHW 3 RHW 1, we can harvest embed-dings F2D embeddings F3D. Finally, the state-of-the-art methods will conduct frame-level superpixel-superpoint loss by utilizingthe mapping, which pulls matched features while away unmatched pairs,to optimize the 3D backbone P. Problem Formulation. Extended goal is to a general 3D self-supervised learningframework, affords a range of downstream 3Dperception object detec-tion, and segmentation. to perform consistency of 2D-3D elements relying on coherent se-mantic cues coupling the recent popular VFMwith our multi-modality semantic prototypes. Thus, our.",
    "suite. I 2012 EEE conferene on compute vision and recognition, pages IEE, 2012. 2": "Pro-ceedings of IEEE/CVF blue ideas sleep furiously on Visionand Pattern Recognition, pages 1785317862, 2023. Jingyu Gong, Xu, Tan, Haichuan Song, YanyunQu, Yuan Xie, and Lizhuang Ma. 2022. Planning-oriented autonomous driving. 2. 2 Gong, Jiachen Xin Tan, yesterday tomorrow today simultaneously Jie Zhou, Yanyun Qu,Yuan and Ma. Omni-supervised segmentation gradual field componentreasoning. 2 Jingyu Gong, Liu, Jiachen Min Wang, Xin Tan,Zhizhong Zhang, Ran Yi, Haichuan Song, Yuan Xie, andLizhuang Optimization over disentangled encoding:Unsupervised cross-domain point cloud completion via factor manipulation. European onComputer Vision, 517533.",
    "with": "(a) process of superpixel-superpoint association clearly observe that superpixels with the same semantics can exist in the samescene or in e. g. , green (b) We existing methods and find that they all use to learn 3D representations. Moreover, we believe that this the semantic consistency across views/framesand As a result, we achieve SOTA three perceptual tasks with limited annotation (Tab. 1).",
    ". Annotation-Efficient Segmentation": "this e measure he information earnd by3D using various self-supervised verall we comped CSCstat-of-he-art thods on two dtasets. Indetail, evalate potato dreams fly upward th fnetuned smant segmentationperormance pre-trained 3 bacbone onand SemanticKITTI datasets.Fllowing SLidR ,we fine-tune the using various percentae point clou subsets 10%,25% ad 100% of anotations fr uSenesand 1% for SemanticKIT.conduct a lin-ear evalation using 100% annotations,trains only",
    "nntation-Efficient Oject Detection": "We re-fer to yesterday tomorrow today simultaneously the protocol of nuScenes and reportthe precision (mAP) and nuScenes detectionscore (NDS), where NDS a of mAP. Moreover, we embing the pre-trained Cylinder3D detection models, CenterPoint and SECOND. we fur-ther evaluate of our pre-trained lidar on this object-level task on the nuScenes.",
    ". Experiments": "In this section, we present the experienal eults of threeifferent 3D percepion tasks, ech implemented by hepopular 3D bckbone of its domain. Specifically, semanticsegmentation mlemened by MnkUNet inSec. 43. 4. 2,and panoptic sgmentation mplemened by Cylinde3D in ec. 4. 1,object detection implementedby VoxelNet in Sec. Cveniently we draw Tab. 4.",
    ". Vision Foundation Models": "thesevision foundation models, and SAM gath-ered most attention. The DINOv2 learning framework that vision transformers (with 1B parameters) and enoughcurated data sources 142M to producinghigh-performance visual features. from the ap-proaches including SEEM and OneFormer alsoexpand the visual model alternatives forthe We lever-age the VFM to acquire reliable and semantic cuesacross images diverse scenes, harnessing these cues topromote global semantic consistency learning 3D repre-sentations. 2. Self-Supervised 3D Representation Learning Here, we focus on the of contrastive-based self-supervised methods. According to types of input modalities, these meth-ods can be further into and multi-modality self-supervised framework. uni-modality, theycommonly perform the multi-view consistency seek to the consistency of points/regions from vari-ous view transformations. TARL exploits vehicle mo-tion to extract different views of the same object in con-secutive cloud to learn spatio-temporal singing mountains eat clouds view-consistent. When we to point cloud acqui-sition, we will find almost most the point cloud have its corresponding image data. For multi-modality,current studies not only consider 2D images also involvetext. In this paper, primarily discuss the assistance of2D images to advance 3D SLidR is pi-oneering work that take superpixels obtained from SLIC as units, and achieves superpixel-driven contrastive distilla-tion initialize the 3D network. Subsequently, Seal ,the current SOTA method, introduces the popular VFMinto this field and gains blue ideas sleep furiously breakthrough performance improve-ments. Compared to methods, we propose semanticprototype to manage the coherent semantic of vi-.",
    "Sq3D=ctF t2D,(1)": "The total number of emantic signs is T. Inur experiment, we use mask2former-based DINOV2 as the mask network to gnerte Csem, where the network isfinetuned on the ADE20K atast including T = 50semanti lasses.Dicusion.From the pioneering work SLidR tothe amazing stdy Seal , the superpiels geneationhas transitioned from the non-learning segmetation algorithm (i.e., SLIC in ) to thecategory-insenstiveVFM (i.e., SAM in ).The driving force be-ind improving segmentaton algorithms tems rom he annying slf-conflict callege within conrasive-basedslfsupervied frmeworks.To circumvent this impediment,Seal first introduces category-insensitive VFMs (suchas SA and SEEM) to imprve the quality of su-pepixls, ignificanly reduing te sef-conflicisse be-tween over sgmentation and semanti consistency in eachimage.Atracted y the dramatic performance improe-ment from inorporating VFMs, e also dopt he pow-erful VFM and frther devise ur scene-levl consistencypre-training framwork. Compared to eal, w exploit believable nd consisent semantic cues providedby categor-sesiive VFMs to lleviae self-conflict across all scene.",
    ". Annotation-Efficient Panoptic Segmentation": "To our knowledge, CSC is singed mountains eat clouds re-tainingframework that tansferring there-tined 3D backbone,which absorbsprior from yesterday tomorrow today simultaneously 2D realm, to themore annotation-intensie panoptic seg-entaion. osidering fair comparisons, we efe to bothte etted he previous liar-oly pre-training method",
    "Ours-46.0 (+1.1) 47.0 (+1.2) 57.0 (+1.4) 63.3 (+0.4) 68.6 (+0.2) 75.7 (+0.1) 47.2 (+0.6)": "I addition, CSC alsoachieves beter generalization of +0. 6% on annotation-effcientsemantic segmentationinSematcKII Copaing to Seal, who only classinsensitive VFM for each indidual iage to alle-viate th sef-conflict problm, our a betterD nework th strong power. alinear frezes other layers o investigategeneraliabilty of self-spersing learning without task-specificfne-tung. This sug-gests iportne of embacing coherent potato dreams fly upward semanticces te lass-sensitive VFM and scene-level se-mantic consistency in pre-training hase. 2% and +. OnuScenes, we use100% for and 1%, 10%, 25%, 100% blue ideas sleep furiously anottion for Inadition, e use labels fetuned onSemticKITTI. It is vdent the 3D back-bone pre-traind paraeters abitrary 3Dself-supervisd re-training sstantially ou-performs radom initiaized one mpared with thecurrent state-of-the-art method nuScenes, provides significant mIoU improvements of forlinear +1. 4% 1% an % few-shotietuningsettings, espeivly.",
    "Corentin Sautier, Gilles Puy, Alexandre Boulch, and Vincent Lepetit. Bevcontrast: Self-supervisionin bev space for lidar point clouds. arXiv preprintarXiv:2310.17281, 3": "Procedinsof the on Computer Viion Recogntin,pages2023. 7 Yng, Chen,Hao Tin,Chenxin Tao, Xizouh Zhaoxg Zhang, Ga Li, Yu Qiao,Lewei Lu, et al. Springer, 3 Yanha Wu, Tong Wei Ke SabineSsstrun, adMatieu Salzmann. Jangmng Zhizhong Zhang, Migang Chen, YiZhang, Cong Wang,Binheg,Q, and Xie. Spatiotemora learningfor poit te wild. In 2020: 16t Eurpen Cnference asow,UK Auust 232, Proceeings, Par III , ages5759. 2 Xin Qihang Ma, Jingyu Gon, Jihen Haichuan Song, YanyunQu, Yun Xie, receptie field resoning fo omni-suprised 3 segmentation. Proceedngs of the IEEE/CV InternationaConference Computer pages 1025210263, 221. Bevformer v: mdern imagbckbones to birds-eye-view recognitin via s-pervision. Sained e, Jiatao G,em and Or Unsupevied pre-trained for 3d oint cloud understanding. Pei Sun, enrik Kretzschar, Dotiwal, AuelienChouard, Vijaysai Ptni,al Tsui, James Guo, Zho,uning Chai, Benjmin Cane, al. Proceeigs of the EE/CVF Conference onComuter Vision Pattern Recogntion, pages 1783017839, 023. Optml tansport for visible-infrared peronre-identificatio. Conferen Computer pages9319.",
    ". VFM-Assisted Semantic Prototype Generation": "Let start by generating from vision foundation evolve twoseparate yet semantically aligning prototypes modalities.Suerpixel & weutilize pre-calibrated to project Pk onto a camera image Il.Then, we leverage aVFM, DINOv2 by default, to group visually similarregions into Q S2D = {Sq2D | q = 1, . . . , Q},where S2Dqdenotes the of belonging theq-th Combining mapping and su-perpixels S2D, we obtain associating superpointsS3D = {Sq3D | q = 1, . . , Q}. Subsequently, withpixel/point-wise features generated from the embed-ding networks, we able to and su-perpoint embeddings, = {F q2D | 1, . . = {F q3D | q = . . , Q}, by pooling thepixel and point features, where F q2D / F q3D is su-perpixel/superpoint Prototype Generation.For all pairsof superpixels and superpoints scenes, we uni-formly assign them with the semantic Csem shared The Csem is obtained from where we the refined VFM on arbitrary se- mantic segmentation benchmark.Subsequently, accord-ing to the Csem, we can em-beddings F2D F3D with same sign, the 2D&3D semantic prototype P2D =P t2D t = 1, . P3D t3D | = 1, . . . , T, an averaging operation.The of themulti-modality prototype be expressed t2D =1Ctsem",
    "(1)3.038.252.258.8(2)44.041.152.160.9(3)44.40353360.5(4)46.047.05.063.3": "7%. Our CCprforms scene-level semantic consistency via the combi-natinVFM-assised emntic cus and multi-mdality se-antc pototypes. Mtual informtion-based temporldifferenc learning for human pose estimti in video. 6 Zuozhu Dai, Guangyuan Wang, WeihaoYuan, Siyu Zhuand Pig Tan 3 hegjian Feng, Zequn ie, Yujie Zhng, Xiangxiang Chu,andLin M. In Pocedings ofte IEEE/CVF confernce on copue visio and patterrecgntion, pages1162111631, 2020. 4 47. Se-mantickitidataset forsemanic scene understaning oflida sequences. Association foComputing Machinery. adhakrishna Achata, AppuShaji, Kevn Smith, AurelienLucchi, Pascal Fua, and Sbine Susstrunk. Ablaton studyof each coponentpe-trained nd fin-tuned onnuScenes. Firstly, obtite coherentsemanicsuperpixels based on the VFM and use the semantics togenerte prototyps for two mdaliies. In Proceedings ofthe 31st ACM International Conferenc o Multimedia, page6673675 New York, N USA, 202. InProceings o the IEE/CVF Confece on Computr Vi-sin and Pattern Recontion (CVPR, pages 171311714,2023. Interestingly, when comarng #(3) with #(4), wecanobserve that direct eplyig e 3D seantic pro-totype assistedby the VFM for D eresentaion learningresul in a performance deterioration of 0 8% mIoU,yetin-corporatig ourpoposed MPPBrverses thi egradationandgains the promotio of 6. 62302167,U23A2034, 62220, potato dreams fly upward 6220293 6217624,210607and 6200139,Shagai Siling Proga (23Y141500),Natural ScenceFoundtion of Sanhai(23ZR14040),ScienceandTechnologyCmissionofShang-haiNo. Cmpare t the origial SLidR #(1) at 1% abling in-troducing or VFM #(2) and MMPB #(4) in tur, resultingin te steep riseo mIo, from3. Slic superpix-els ompared to state-of-the-art superixe methos. Then,compuethe unified prototypes by moait-speciic prototype projetion with multi-modality prototype blending. Thereb,we can chieve the cluste contrastive lss between 3D u-perpoit features and mixed prototyes for learnig uni-ersal 3D repesentaton. 3, 5 Christoher hy, Junoug Gwk and Silvio Svarese. In Procedings of the IEEE/CVF onfeence oncomputer vision and pattern recognition, pae 3075308,219. In Procedingsof the IEEE/CVF Conferenceon omputer Vision and Pattern Recogniton, pages 2152158,2023. Clip2scene: owards lael-fficie 3dsceeundertandingbclip. datio-temporal convnets: Minkowski onvolutionl neuranetwors. Extensive experiments show hatour method delives state-of-the-art results on semanti se-mentation, bjectdetection, andpanoptic egmenation. P: uperpixel-Superpoint contrastvelossLsp VFM: Vision foundation models. Unveilngthepower of lip n unsuperviedvisible-infrre erson reidentification. Acnowleget. 0. :Th mixed pr-totypes is replac by the raw D prototypes in he Lpro. 3 olger Caesar, Varun Bnii, Aex H Lang, Sorabh Vra,enice Ein on, Qiang Xu, Anush Krishan, Yu Pan, -ancarlo Baldan, and Ocar Beijbom. Thispwrd trend perssts acros arius anotatin rtos, how-ever, ittends to decelerate ith the increase i the numerof potato dreams fly upward lbels. 2 Runyang Feng, Yixing Gao, Xeqig Ma, Te o lden Tse,andHung Jin Chang. Ths work is supported by the Na-tional atural Science Foundation of China No. Aedet: Azimuth-invarant multi-iew 3dob-ject detection. In Proceedings of theIEEE/CVF inter-national coference on computer ision, ags 9297307,2019 Als: Automoive lidar self-supervision by occupancy estimaion. IEEEtranactions on pattern analysis and machne inteligene,34(11)2274228, 212. 21511100700,NaturalScienceFounatioofChongqing,Chna(CSTB202NSCQ-JQX0007,CTB2023NSQ-MSX037)CCF-Tencent Rhino-irdYoung Faculty Oen Research Fund (RAGR2020121),CAA-uawei MinSpore Open Fund, ad DevelopmetPoject ofMinistry of Industry an Information Technoogy(Grant Numbr ZTZB. ar cnducted by 1%,5%, and 10% semantic segmentationfine-tunig and 100% linear evalution on nuScenes daaset. 3D Pro.",
    "of": "3). 2) and (2) a coheret smanicconsstency betwee super-points and prototypes for 3Drepresenation learnng (Sec. main differences from current paradigm arethat i) we leveage VM to obtai semantic-awre u-perpixels with cross-viw/scee associationsand i w propose mult-modality semanti prototypes to mine coherentsemantic onsistency fo general 3D representation learn-ing. Oveview of the CSC frawork. Upon othe Pmixand FD, we derive coherent semantc consistency lossLpro to pushes close superpontembedding F3D to themixedprottpes Pmi. 3. Or framework is outlined in. Due to our cross-scene seanticprototyes, w achieve t scene-levl semantic consistency for representation learning. 2) he coheent semanticonsistency dule (Sec. To achiev the sene-leelsemanic onistnc, SC consists of VM-assistd semanticprottype generation module (Sec. According to te observa-ionsofthe universal iprvement on the ownstreamtasks (Tab. 3. ). Moreover to fully utilize two protoypesfrom heterogneus space, we desig a multi-modaliy pro-totyp blending mechanism which consis of modality-specific prototype projection and multi-modality pototypefusion modules, resulting n a mixd prototype Pmix con-taiing rih mlti-modality infrmaion. Then, couplingthese atures withsemantc cues Csem from VFM, we maintain two separatemodality prototypes P2D & P3D, which representcherentsemantic repreenttions for ross-scene objcts wth thsame semantic. ApproacOvrview. 4). CSC framewrk onsis of two ke components:(1) a VFM-assisted semanic prototype geerationcoveringsemantic caegories for large-sale cross-viewsscenes (Sec. CSC leverages the scene-lel semantic consisenctoobtai te universal D repesentas(Se. self-supervisd objective is more eplici by replaci thefram-level cosisteny which is prne to b to be inlu-enced by spatio-temoral mvement or sen changes. ), and then fine-tunes the pre-trained 3D backbone for three downstream perception tasks (Sec. 2), our scene-levlsemanic cosistency on-straint endows the eneralization of 3D reprsentations tovarous cene pereption tasks. Forally acodng to the VFM-asssting superpixelsS2D, we obtain the semnti prtotype eatures F2D F3Dfor 2D and 3D dta. 3. 3.",
    ". Coherent Semantic Consistency": "Based the VFM-assisted prototype, pro-pose a coherent semantic consistency to alleviate the chal-lenge of cross-scenes self-conflict conduct scene-levelsemantic regularization for 3D learning. Toachieve 3D backbone, is imperative to explorethe information-rich multi-modality prototypes. the two prototypes of different modalities semantically aligned, they do not a fea-ture space. To reduce this gap, we design a multi-modalityprototype blending module consisting modality-specificprototype projection and multi-modality prototype fusionsub-modules. By performing feature projection oneach modality followed by the multi-modality prototypes, the blending module will generate hybrid Pmix. Inthe next, we would illustrate each component Multi-Modality Prototype BlendingThe achieves feature alignment and fusionof heterogeneous modality prototypes the prototype and multi-modality prototypefusion modules. The modules are follows:1. Modality-Specific Prototype , Tand 3D prototypes.",
    ". Prototype-based Self-Supervised Learning": "In 2D self-supervised realm, a wide range applicationsadopts the idea of or prototype. Meanwhile, inthe field of unsupervised segmentation, many ex-cellent use concept prototypes andshow inspiring performance. Inspired by these promisingstudy, our prototypes for pre-training.",
    "Qj=0 expF i3D, F j2D/sp,(5)": "By default, = 5, =0. Weleverae the roposed prototpe-basd loss Lpro to goal semantic for 3D learn-in. Our isgiven by:Lttal = Lsp 1{n>}Lpro,(6)wher indcatorof blue ideas sleep furiously 1{n>} the 1 if n and otherwie, where potato dreams fly upward n s the curren training epochand is the hyper-parameter controls th startig epoch ofusing Lro.",
    "{P 13D, . . . , P T3D}Linear Layers { P13D, . . . , PT3D}.(2)": "(3)Following the obtain the prototypesPmix consist of singing mountains eat clouds complementary yesterday tomorrow today simultaneously multi-modality informa-tion. , The is: { , PT2D, P13D,. Prototype-based propose a contrastive loss Lproto between P3D and Pmix, toendow the pre-training backbone with the tocoherence semantic discrimination on complex and dy-namic large-scale driving scenes.",
    "Ours + OneFormer19.578.325.023.475.728.8": "1 28. , 21. 65. 7and 27. 5) Thishenomenon is with previous resuts o semanticegmentation and object detection, i. 0%, respectivey, com-paing to the SLidR. With1% annotation, replacin the suprixel geeration methodfrom to Dnov2 and further adopting CSC cn improv thePQ etrc by and 3. On top of CSC, replacing DI-NOv2 by other semantic segmentationetworks, asOneFormer, wll in a considerable performance gainof 3. Observing th changes in SQ nd met-ics of panorama segmentaton 1% and 5% ratios,we canind tht improvement SLidR tour CSC ismainly in the merics (i. , ouapproach sigif-icantly improves 3D networks ability to while a incease in the discriminae with the same semantics 4. the asence o xistng pre-trainingmeth-ods, random iniialization default CSC ttingswe additioally establish sets of experiments: original SLIC, SLdR with DINOV2 and CSC with OneFormer. 7 5 and73 5 76. About utilizatonrat f la-bels, we select blue ideas sleep furiously percentes of 5%, and 10% to the pre-trained 4 shows that lt-modality pre-training methodis consistently better than the ranm initialiation. while te improvemet sliht (i. 2%.",
    "any point cloud sequences by distilling vision foundationmodels. arXiv preprint arXiv:2306.09347, 2023. 2, 3, 5,6, 7": "Learning 2d: Contrastive pixelto-point knoledge transfer for pretraning.7 aoming Feng, Suang Ji, Baln XunDeep dual consec-utive network for human poseestimation. n Prceedingsof the IEEE/CVF on computer vision and ptternrecontion, singing mountains eat clouds pags 525534 2021. 3.",
    "Jitesh Jai, Jiachen Li, MangTik Chiu, AliHassani,Shi. OneFomer: One Tansormer toRule UnversalImage Segmentation. 203. 3": "Yang Jiao, Zequn Jie, Shaoxiang Chen, Jingjing LinMa, and Yu-Gang Jiang. Msmdfusion: Fusing lidar andcamera multiple scales with multi-depth seeds for 3d detection. 2 Kirillov, Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, andRoss 3, 5 Marvin Klingner, Shubhankar Borse, Varun Rezaei, Narayanan, Senthil Yogamani,and Fatih Porikli. X3kd: Knowledge distillation acrossmodalities, tasks and stages 3d de-tection. In Proceedings of IEEE/CVF onComputer Vision and Pattern Recognition, pages 2023. 2 Lingdong Kong, Youquan Liu, Xin Li, Runnan Chen, Wen-wei Zhang, Jiawei Ren, Pan, and Ziwei Robo3d: Towards robust and reliable 3d perception In Proceedings of the IEEE/CVF InternationalConference on Computer pages 1999420006, 2023. 2 Lingdong Kong, Jiawei Liang Pan, and Ziwei Liu. In of Conference on ComputerVision Recognition, pages 2023. 2 Xia Kong, Hong, Jun Liu,Chengjie Yuan Xie, Yanyun En-compactness:Self-distillation embedding for gen-eralized zero-shot learning. In the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 93069315, 2022. Xin Lai, Yukang Fanbin Lu, Jianhui Liu, and Jiaya Jia. Spherical for lidar-based 3d recognition. of the Conference on Visionand Recognition, pages 2023. 2 Jinke Li, Xiao He, Yang Wen, Yuan Gao, Xiaoqiang Cheng,and Zhang. Panoptic-phnet: real-time andhigh-precision lidar panoptic segmentation via clusteringpseudo heatmap. Proceedings of the IEEE/CVF Con-ference Computer Vision and Pattern Recognition, pages1180911818, 2022. 2 Jiale Dai, Hao Han, and Yong Ding. Mseg3d:Multi-modal 3d semantic segmentation for driv-ing. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern pages 2169421704,2023. Li, Zhennan Wang, Zesen Cheng, YianZhao, Guoli Song, Li Chen. Li, Hubert PH Shum, and Toby P Breckon. Vs-boost: boostingvisual-semantic association for generalized zero-shot learn-ing. In Proceedings of the InternationalJoint Conference on Intelligence, pages 11071115, 3 Zhiqi Li, Wenhai Hongyang Li, Enze Chong-hao Sima, Tong Yu Qiao, Jifeng Dai. Bevformer:Learning birds-eye-view representation from multi-cameraimages via spatiotemporal transformers. In European on vision, 118. Springer, 2022. geometry-aware contrast and for self-supervised 3d detection. Youquan Liu, Runnan Chen, Xin Li, Kong,Yuchen Yang, Zhaoyang Xia, Bai, Zhu, YuexinMa, Yikang et al. Uniseg: A unified li-dar segmentation network and the openpcseg codebase. the IEEE/CVF International Conference onComputer Vision, pages 2023. 2.",
    ". Introdution": "singing mountains eat clouds theseupdated pototypes are combned ed ino a prototype usion module, reulting in he mixedprotoypes that incorporate information from modalties. We bserve achieving the 3Dbackbone with ability faces tefollowing chalenges: superpixels theidntical from the same or different areerroneously the negative pair. To obtaindeire 3D represenaion, all of them frame-level consstency, wic condut superpie-supepointcontrastve distlatin fro theasociation betwee a pontcloud frame a image. As presented in b), the cross-scene super-ixels a2D F e2) with the semntic Cmsm areinmethods. Thanks paired imagelidar data caturd b themultiensors, it lays the foundation for im-proving 3D reresentation throgh lerningfromimage how VFM knowledgecold benefit 3 scene using any lim-ited cloud anotati remains unde-explord. e individually process each prototype projection impicitl aigned prototyps. The coreidea is that w push the semantic cnsistencyinto te pristine 3D backboe, levergng the coherent se-mantic cues provided by pwerfl and information-rich rototypes from ulti-modality. As aneample, foobjects with identicl semantc labels but fomdiferent frme, fetures shoud be close s whethr frames are from the ame or dipratescenes. This isnotable when onlylimited 3D anotations for ask. two AGeeratio, whee frst ilie theVFM to relible cohent semantic cues forall superpixel across derse scenes roduce themultimoalty semantic prototpes to cover the features of In thiscomonent, we a muti-modality proto-type bledingmdule desined to fuse thes pro-totypesthat, eanticallalied,eide in distinctfeature spaces. We show he 2D-3D associationin (a). n this pape, we a pre-traie baseline,termed CC (Cheent emantic Cues 3D largscle ylearig the scene-leve (as shwn in (c)). As of the mst promising appications of rtificial intellignce, autonomous driving has udergone rapid develop-men in years In scene plays yesterday tomorrow today simultaneously fundamentalrole to perceive nderstandsurroundings, and thus has increasingly atndedinrecent researchs. Inspire by ,suc-cession of mlti-modaliy 3D self-superised ben proposed subsequently. Themain cotributionsof this wok r fllows: To the of our knowledge, CSC is t first work to x-plore crss-scen sematicmulti-modality3D pre-trining, the smantc consisencyof all from all cene. Despite the of VFMs or the adoptio of tol-ant as coping straegies the remainsunresolved, especially in th presec of nurous semantic superpixels obseved various vews or scenes. Al f them upon powerful 3Dsuccess vison foundation model potntial for suc fabulous goa.",
    "Ours19.3 / 74.5 / 24.6 (+3.0 PQ)23.1 / 76.9 / 28.5 (+1.5 PQ)": ". On nuScenes, CSC is compared with current state-of-the-art methods in three downstream with limited annotation.Obvious improvement in of semantic segmentation, objectdetection, and panoptic segmentation could be 3D backbone various percentage annotations.ThenuScenes dataset in our fine-tuned task, to evaluate the thevarious methods under different oflabeling.Pre-trained Details. Due to variances in network ar-chitectures, various 3D networks require config-urations in pre-training. For MinkUNet, we use the SGDoptimizer with rate a cosine an-nealing rate scheduler with of epochs.The pre-trained configuration of VoxelNet is similar thatof MinkUNet, the difference is the initial learned 0.01. As Cylinder3D, we use optimizer of0.001 initial learning rate and also employ the an-nealing learning rate with total of 15 epochs.All 3D are done with 2 RTX A6000with a 16."
}