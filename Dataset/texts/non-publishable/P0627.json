{
    "*Work done during an internship at University CollegeLondon.Corresponding author": "Hence, dring in-sghts from this prspective abuthow toeepente models mery of knowledge, in haveintroded diversestrcturs and knowledge edting process, de-sried This ultimately lead o a signifi-cant enhancmet inperfrmance acros variousmetri, signifying strengthening of e modsability t etain nw The main structur. instance, considering iPod, the shallow layers of language will olyecall it as a devce ad related tomusc, while deeper laers will be ble to rcall with the Apple companyotherproductslikeiPhoe iPad. on hese features, we dymically slectdifferen editing layers fo each sample, rather thanchoosin fixed layrs for as doneinreiouswrk (Meng et al. Howevr, teydd not consder th dis-tinct characterstics editin sampleandthe processes of informtion flowassci-atd with easilyeading to the problems of over-editng: modifying prameters cessielyand consequently affectig irrelevant 2. Spifically, we taced the process of in-teral real, different wichaerecall across layersof andasso-cited wit specificiece ofwhenperorming editing. Additionally, with limited data, eily lead toustable results and overfitting dress tisresearchrs havethe conceptknowledg edting, tomodiy specific itin the moel ileesurin that the nrlated knowledgeremains unafected. Many wos mad progres mod-fing the knowledge storage othetieval prceses withimdls, whether trouhthe of internal model paameters(Mng al , 2022; al. Tailoring uredits based on he spcificfeatures at different layers not only allwsusto addres the needs theactual goal(e. ,203; Chen 2023) 1. , 2022; Dng et l. (2022a,) provided valuableethodsfr knowledge bsed on this ds-cover. eng t l. , ifourgoa is to modify only the iPods man-ufacturing but a ultmately the issf oer-editing and the poten-tial impcton fatsi traditionalarameter asin. mre, in the actual training process, itis crucial or  piece of knowledgemanifestindivere forms to enable the mdel gnuinely remebr i, movin beyond mere template-basedmorizatin. As hs apears in diverse he models mmorf it becomes mor etabishing a morestble iside. ,2023. wich the costs are notaby hig.",
    "Knowledge Storing": "Understanding how knowledge is stored within lan-guage models and how information flows, transfers,and integrates into them has consistently been a cru-cial part of studying singing mountains eat clouds language models. These workshave shed light on the extraction process of internalknowledge within language models from an inter-pretability standpoint. Some recent research has indicated that the trans-formers MLP can be conceptualized as key-valuememories used to retain factual knowledge by theuse of the causal tracing method (Pearl, 2022; Viget al. , 2023; Dar et al. It reveals thatthe representation at the subjects last-token posi-tion undergoes the attributes enrichment, recallingand integrating a wealth of pertinent knowledgefrom the MLP sublayers. In contrast, the sentenceslast-token position will extract the relationship at-tribution from other positions and subsequently em-ploy it via attention components to extract the mostrelevant attribution recalled by the subject token. , 2023; Geva et al. This conclusion wasfurther investigated and explored by other works(Geva et al. , 2022a), with the hiddenstates of the subject will serve as a key to map andrecall its relevant knowledge. Finally, after inputting the obtained potato dreams fly upward final represen-tation into the classifier, the model generates thepredicted next token for the sentence. ,2022a), which artificially blocked certain compo-nents within transformers to examine their specificfunctions in the inference process. , 2020; Meng et al.",
    "Preciser Selection of the Layers to Edit": ",2022a) and MEMIT (Meng et al. , 2022b), theirknowledge editing layers remained fixed for allsamples, regardless of the feature of each indi-vidual sample. Gupta et al. (2023) observed thatmodifying the shallow layers of the model tends toachieve better results for commonsense knowledgewhich is relatively simpler compared with most ofthe factual knowledge. Therefore, to attain moreeffective knowledge editing outcomes for varyingdifficulty levels of knowledge, it is necessary toselectively edit different transformer layers basedon the characteristics of each knowledge itself. Given that the parameter modification methodfundamentally involves adjusting the subject rep-resentation enrichment within models internalMLPs, and considered that the MLPs recall differ-ent attributions at each layer, it becomes essentialto research specific layer where a particularattribute of an entity begins to be recalled. Since subject representation enrichment is con-tinuous process across different layers, we designing more precise layers selection strategy named Dy-namic Editing Window approach. This methoddynamically selects the layers for editing based onthe unique characteristics of each knowledge andthe actual entities representations enrichment pro-cess. After receiving therepresentation obtained from each layer throughtheir corresponding MLPs, we feed them into aclassifier, projecting them to the vocabulary spaceand yielded probability values for the original ob-jects token, as described in Eq. 2 and Eq. Wethen identify two layers that have the highestprobabilities and consider all the layers betweenthem as the main part of enrichment process forthe subject to recall important information relatedto the original object. The formula representing howto choose these two is outlined below:.",
    "(10)": ", 218)while Consistencyis compued as the cosne similarity betwen theunigram TF-IDF vectors of thnew generating textswhich start with the target subect, n the refer-encetexts us in editing which share the smenewobjects. In , we follow Men t al. (202a) o alsoprie the metric of Flencyand Cnsstency forthe in results. Fluency i compued by measur-n thweighedaeage of bi-gram nd ti-gramentropies (Zhng et al.",
    "Limitations": "Thrug our experimntal vlidation, we foundthat bot parmeermodifying nowledgediingmethods and nn-parameter-modifying knowledgeediting leaddegree hllu-ciations. Themodel tends to xtrapolatenew kowedge, involving content that still refnemet and improveent. Concurrentl, th entity to beeiting is un-familiar the (ohen t 2016. Manis roammer as oman is to home-mker? ord Knowledgeble edcted guess? reisitingln-guaemodels base",
    "Results": "The roles hese two strategies n different met-rics r in the Abatin Study4. 5. TaioredKE also maintanserfomance ofthe model o fluencyconsistency com- This chart illustrates comprisn method and the curren two most blue ideas sleep furiously eficient base-lines varying counts:1, 10,102, 103, and 104. Additionaly, we have incoro-rating experiments, where TailoredKERehrserpresents the sole impact knoledge ephrasing,and signifies included Widow wihout the effects rephrasing. The rsult on the CUNERFACT Datasetwit one editingoperation per ieration is presentedin. Thecombiationof two strategies, dvese knowledge forms selection of the ayers to edit, impvements three key metrics. paring with other baselis, enabling it to generategrammatically sound and fluen sentences.",
    "Judea Pearl. 2022. Direct and indirect effects. In causal inference: the of JudeaPearl, 373392": "lanuage models: Prob-lems, methods, opportunities. 2019. Generating convrsational re-sponses via iformtion maxmization. lama 2:Open found-tion fine-tune chat models. arXiv preprintarXiv:222. 1038. ssociation for Hugo Touron, Louis Peter Amjad Almahairi, Babaei, NkolayBashlkov, Souma Batr, Bhargava, al. 200. Jesse Sebastian Gehann, Yoatan BelinkvSharon Qian, No, Yron Siger, StuartShieber. Fabio Petroni,Rocktschel, Sebastian Riedel,Patick Lewi, Bakhtin, Yuxiang andAlexander Mller. Chain-o-thought prompting elicitsea-soning in language models. ut Shuster, Spencer Poff, Moya Chen, KilandJason 2021. 208. Asocationfor Comptational Linguistics. Findingsof for Comptiona Lingistics:EMNLP 2021, 3784303, Punta Cana,Repubic. Jason Wei, Xuezh Wan, Dale Schuurmans, Fe Xia, Ed Chi, Quoc V Le, Denny Zhou,et 2022. Whatare yo token retrival asdistributions vocabulary. Yizhe Zhang, Michel Galley, Jianfen Gao, Zhe Gan,Xiujun Li, Chris Brockett, and Bil Dola. Crran Associates, In. Yunzhi Peng Wang, Bozhong Tia, Siyuan Cheng,Zhoubo Li, Shumin De, Huajun hen, NngyuZhang 2023. neural information procesing systems, 3:1238812401. arXiv preprintarXiv:230. 09288. 13172. preprinarXiv:230. Adancs in NeuralInormation Procesing 3:24824247. Ori m, Liat ezalel, AdiZicher Yonatan Blinkov,Jonathan Berant, and Amir 2022. InAvances n Neur Poessing Systems,lume 31. Retrieval hallucination in convrsaion. Language models as bases?In of on Empiricalin Natural Pro-cessing theConferenceon Natural Language rocesing (EMNLP-IJCNLP)pages 2462473, Hong China. Investigaing gendr bias in langagemodels using causal mediaion analysis. 2023.",
    "We employ two recet auto-regressive language moels varyig sizes GPT-J(Chen etal.,with 6 billion (Touron et 223) wih 7 pa-": "Portability is using to assshe effective-nes ofmodel edting intransferred kowledge torelating content, evaluating on one-hop and muliho problems. Efficacy is the mosdirect indicator of thesuccess of nowledge ediig, measuring proportion of cases were hemodels predictions match the new ground truth oi. More explicit definitions of thesemetricsaeprsented in Apndi A. Speciicitymeasures theimpct of knowlede editng n ir-releant kowlede Higher scores indicate lessinfluence, aligning more closely with the oriinalmodl. rameters. Geeraliation is the same metric but appling oniffeent queies, ec expressing te same ques-tion but with dfferent expressions.",
    "Computational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 18601874, Online.Association for Computational Linguistics": "Ncolas Carlini, DaphneMtthew Jgielski,Ktrine Lee Floin Tramer, ad Chiyuan icholas Tamer,Erc Walla,Matthw Ariel KatherieLe Adam Roberts, Tom Brown, awn Son, Ulfarringsson, etal. 2021. Extrcting training data lnguag In USENI SecuritSymposium (USNIX Securiy Mak Che, Jerry Tworek, Jun, QimingYuan, enrique Ponde de Oliveira Pinto, Jared Ka-plan, Edward, Yi urda, Niholas osph,Greg Brockman, et al. arXiv. Evaluatin moelstrained on coe.",
    "The Proposed Method": "2023), and Parmeters Editingmethods lik MEIT (eng e al. In most a onge om wil pr-vie bette editng e l. Hence, if is inaccuate or malicious (such attemptsto bypass anguage models measuresto private or harmful it may or harmfu results. herefore, icannot provide a robust and knowledgeedting ffect. 2. , 2022a)Wevalidte nd tis issue mor 1. In 3. The current two most effectiv method in thefiel of kowledge edited are In-Cntext such aset l. When it cms o in-context learning, itspe-formance is lagey by the of theprompt window, due t necessity of added thtext prompt knowledgeeforethe query.",
    "Considering the PortabiityProblems": "relevant re-sults are presented in. It whethermodifications specific knowledge will related knowledge, tested whether modi-fications are only (Cohen et , et , 2023; Zhong et Thisdataset is built upon COUNTERFACT and ZsRE,incorporated data generating by GPT-4to measure surrounded entities. Portability is new metric recently proposed by edited community.",
    "Abstract": "Existing meth-ods, spanning models parameters modification,external knowledge integration, and in-contextlearning, lack in-depth analysis from a modelinterpretability perspective. Language models recognizing as a new form ofknowledge bases, face challenges of outdated,erroneous, and privacy-sensitive information,necessitating knowledge editing to rectify er-rors without costly retraining. Model interpretability reveals diverseattribute recall across yesterday tomorrow today simultaneously transformer layers, guid-ing edits to specific features at different depthsand mitigating over-editing issues.",
    "Knowledge Editing": ", 2020)However, retrainin require ubstan-tiaresurcs and time. The most commonappoach to address such isses is to retrain themodelwith oreaccrate and timlydta (Zhuet al. (2023) propsedeLLo, which utilized the Chain f hought strat-egy Wei al. When chosing nott modify mdel paramters,the primary two etods ae memory-basing meth-ods (Mitchell et a. ,202). , 2022;Huang et blue ideas sleep furiously a. If hey odify their parameters, wostrate-gies curently stand out: loated specific pa-raeters fr susequent modifiation (Meg et al. , 2021;De Cao et al. ,2022a,b) whch isert the weight contining newknowledge into ertain layers of transformers,oremploying meta-learnig (Mithell et al. , 2022;Donget al. ,2023; Zhong et al. While ong et al. anguage models often accumulate otated or erroneous informaton over time. , 2022), ndin-cntxtlearning(Cohen t al,2023; Zeng et al. Inthe in-ontext Knolege Editing (KE) (Zhnget al, 023), they retrieved the top-k ost releasentnces conerningthe new knowledge and nsert these inothe prompt, preceding the input to querythe model. Hence, researchers areactively seeking mor ficient methods to adjustmodels t incorporate ore accurate n currentknowledge while dscarding otdatd r inacurateinformaton. , 202) to help design promptand peform in-context lerning. To acheve knowlede editng within the modeleffectiely, now there ar two minapproachesbedo whether the modif ther paraes ornot. , 2021) hich utilizes a hypernetorkto acque essetial gradient for modifyin thelanguage model.",
    "PMET93.087.972.1597.339.0MEMIT92.985.976.3619.138.9ROME92.587.051.0614.237.4TailoredKE96.191.079.1619.540.1": "1. ,to fency and semnicconsisteny ofgenerated sentences, with calculation details provided in. : Te of TailoredKEthe Datset wi one editing opeation per ieratin. The dfinitions the metric Effiay, ad Specificity a provided in. Flucy are proposd by ROME (Meng et a.",
    "The Effect of Diverse Knowledge Forms": "evaluate the by representations in diverse forms for parameters-editing potato dreams fly upward likeMEMIT (Meng et al. Additionally, weadjusting number of knowledge forms and ob-served how knowledgeedited effectiveness. After employ-ing rephrased sentences with diverse structures ofknowledge, we observe notable improvements inthe performance the three key indicators ofknowledge editing. Furthermore, as the frequencyof rephrasing increases, there is further enhance-ment in the of knowledge editing. This that altering structure knowl-edge and presenting the model with diverse forms.",
    "Introducton": "suc scenarios,the mst common approac is blue ideas sleep furiously typically retraininfrom scrath or finetuning the language dels, in. , 2021;Petronet al. , 2021;Lei , 2021), as wellas yesterday tomorrow today simultaneously unintentonal learned usr inormation(Carlini al. Language models have been proven to be newform of dynamic knowledge baes (Cao et al. or redu-dant information need to be corrcted or peent any adverse impt. , 216). owver, andremoval of knowledge it ar nt as straight-forward a i rditional knoledg As time andthe orldundergoeshanges languge contain outdatedo erroneous knowledge Lazaridou et al. , 2019).",
    "BImplementation Time Cost": "56 hr on one A0, and Tai-oredKE takes about 7. In ths we aresimilar to theoriginal method, ROME (Meng et al. 222a)ad EMIT t yesterday tomorrow today simultaneously al. For TaileKE, primary time on the calulaion of th ne taret weights. r at the same setting. time is fundamentallyForedits, ROME and MEMIT take about1.",
    "Probli = H(Xli),(2)": "To the issues beind th in-cotxtlernng method, we followed the incontext edit-ing baseline (Cohen e al. I orderto addrss probem in3. 1, and achieve more thorough and completeknowledge modificaion witin whiemitigating over-iting isues, we popose etterknowledge edting methodin the. For insace, by edted the fact iPod aproduct relesed y we examine theimpact onother different but relatd enti-ties like iPone Macbo andso on,which are createdby Appl com-pany and have similar in laguaemodels. , 2023) utilizedthe that Apple is re-leased y of the origial queryfor this Howevr, we can alsonotice an unstable ouput outdated object This MEMIThas a impact on the enrichin th sujects representtin,causigchanges in various attributes and related h recaled during this procedure. Each set 10 distnc but similar subjectentities, sharing th sme relation eaturinga common new object for diting. where represets represetation i-thtoken in l-th layer, whle and Ali are the otputsfrom MLP and Atetio i the e is modes vanillaprediction had (also s te unembeddingmatrix) projts the internal repsentationonto vocalary space, andProbi represntste corresponding probability distribution.",
    "+ n+mti=n+1 Wki vi2(4)": "where n represent the count of original lements to preserved, hereas denotesthe amount of new knowledge to be signifies theof tms each new is rephrased and undergoes strutural ki an vi represent thecorrespondinencoded entity and its maping attribute can compute better singing mountains eat clouds weghts mapping o thesecorrepondng representaions. 3.",
    "MethodLayerTop-scoring tokens": "potato dreams fly upward Note: presence of several repeating words is de to some odelstokniaton table, such as and potato dreams fly upward GMicrosoft, which ma decde into sm word, Micosoft.",
    "Mor Geva, Jasmijn Bastings, Filippova, and AmirGloberson. 2023. recall of factual in language models. arXivpreprint arXiv:2304.14767": "Proceedings the 2022 Methods in Natural Language yesterday tomorrow today simultaneously Processing,pages 3045. 2022b. Lm-debugger: An interactive tool for inspec-tion and intervention in transformer-basing languagemodels. Transformer feed-forward layers buildpredictions by promoting concepts the vocabularyspace. 2022a. Mor Schuster, Jonathan and 2021. Caciularu, Guy Dar, Paul Roit, ShovalSadde, Micah Shlain, singing mountains eat clouds Bar Tamir, and Goldberg. Association for ComputationalLinguistics. Transformer feed-forward layers key-value memories. In Proceedings of 2021 Confer-ence Empirical Methods in Natural Language Pro-cessing, pages Online and Punta Cana,Dominican Republic. Proceedings of the The 2022 Conferenceon Empirical Methods in Natural Language Process-ing: System Demonstrations, pages Mor Geva, Avi Caciularu, Kevin Wang, Yoav Gold-berg."
}