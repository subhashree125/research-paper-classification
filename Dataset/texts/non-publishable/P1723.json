{
    "We OC20 S2EF-2M dataset to compare results training with and without DeNS and verify thedesign choices of DeNS": "omparison btween Traning with ad without eNS. ()summarizes rsults of nd withot DeNS. Fo EquifomerV2, incorporating DeNS as an uxiliartaskboosts te performaceon frceonly increass ime y 4% and he numbe of parameters o Training for 20 epochs results i beter energy and potato dreams fly upward forc MAE compred toEqiformerV2 traiedDeNS for 30 epochs wil reuirig DeNS training e of eSCN by Choices. We train quifomerV2 for 12 epoch it DeNS t desin choices f DeNSand the resultsn (b). Morever,DeN without forc encding(Index ) resuts in clearyforce MAEcmpared to trained DeNS (Index We also compre and without forceenodng onMD17 and have similar observation. potato dreams fly upward Comparing Index 2 and Idex we demonstrat thtpredicting energy of originlstructurs givn corrupted ones can imprve energyMAE by 5 6% slightlyincreasing orc MA Finall, he comparison between Inex 2 an Index5 shows corruted structurs can further improve the performanc ginf DeNS.",
    "Batch size128Number of epochs6Weight decay1 103": "Dropout rate0.1Stochastic depth0.1Energy coefficient E4Force coefficient F100Gradient clipped norm threshold50Model EMA decay0.999Cutoff radius ()12Maximum number of neighbors20Number of radial bases600Dimension of hidden scalar features in radial functions dedgep0, 128qMaximum degree Lmax6Maximum order Mmax2Number of Transformer blocks18Embedding dimension dembedp6, 128qf pLqijdimension dattn_hiddenp6, 64qNumber of attention heads h8f p0qijdimension dattn_alphap0, 64qValue dimension dattn_valuep6, 16qHidden dimension in feing forward networks dffnp6, 128qResolution of point samples R18",
    "Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole von Lilienfeld. Quantumchemistry structures and properties of 134 kilo molecules. Scientific Data, 1, 2014": "Albert Reuther, Jremy Kener, Chansup Byun, Siddharth Samsi, WiliaArcan, David Bestr, BillBegeron, Vijay Gadepall, Micael Houle Mthew Hubbell, Michael Jones, Anna Klein, Lauren Milechin,Jui Mullen, ndrewProut, Antonio Rosa, Charles Yee, and Peter Michleas. Iteractive supecomputingon 40,000 cores for machine learning an data analysis.",
    "An image is worth 16x16 words: Transformers for image recognition at scale. In International Conferenceon Learning Representations (ICLR), 2021. URL": "A hitchhikers gude to for 3d atomi systes. Dual, V. 07511,Alexndre Agm Duval, Victor Alex HernndzGara, Satiago Miret, singing mountains eat clouds D. arXivpreprintarXv:2312. Mlliaros,Yoshuaengio, David Conferece on Machine Learning (ICML), 2023. Chaitanya Joshi, Schmidt Santiago Fagkiskos D. Maythe fore be with you: Unifie fore-centric pr-trining for molecular 223a. Rui Feng, Zu,Huan Binghog Chen, Toland Rmpi Ramprsad, Cha Zhag.",
    "D.1Additional Details of DeNS": "It is that gradients consider the original task and DeNS when updating learnable parameters,and affects we sample structures for DeNS when only a single GPU is used for training MD17 dataset.",
    "A.3Equiformer Series": "quiformerSmidt, 2023) is graph neral network combineteinductive biases of 3D-rlated equivarnc with the strength of Transformerset al. 2017).Startingfrom Tranformers, Equiformerintroduces three modfications.First, adoptseqivariant features built fro spacs of as internal to incorprate Second, equivarian ar applied to the equvariant features. These oerations include th euivarant counterpart of original operaions Transfomers. e. , 201). Third, Equifrmer proposes to no-linear both attention and messae passing, which improves epressivity o attenion intandadTransformers. Althoug Equiforme deonstrates that Transformerswell to 3D atostic ystms, it is values of maximum degree becuse of the computeintensive prduct operatins. Higheregreescapture angularand directionalinformation, and therefoe Lmax calimit the expressivityEquifrmer. o address this imitation, EquiformerV2 et al. , adoptseSCN & signiicatly the computational singed mountains eat clouds oftensor products, to higher-degree eqivariant representations and proes hree archiecturalimprovements tobetter leverage the power of higher Firs, attention re-noralizatio inroduce one layer normaizaion to non-linea functions of attention , 208) s proposing mix the information f al degrees andstabilize training. Third, separable blue ideas sleep furiously normaliztion is propsed replace original equivariant lyer to preserv he reative importance of different degees",
    "ProblemSetup": ", |S|uu,where P N denots the atomcof the ith atom and pi P R3 denoe tomic positio. Otherwise, we refer to it as non-equilbriu trucure. Specifically, ach S is an in a datase and can as tpzi, piq i P t1. atomstic system can one more molecules, crystalline on. Calculating quatum mecanical properties likeenergy of 3D atomistic systemsis many applicaton. In we he of predicting and forces given non-equilibrium. Sincenon-euilibrium structures have non-zero atoic orces thus not at an energy they havemoreof freedom constitute a much largerse of possible strcturesthan those at euilibrium.",
    "OC22 Dataset": "Dataset pen Catalyst (Tra* et al., 2022) ocuse on oxideelectrocatalysis consists about relaxatins wih Perew-Burke-Ernzerhof (PBEfuctional calculaions. rucial difference between OC22 and OC20 the enery targets inOC22are DFT total energies. DFT total energies are harder to predict but are the mos genral singing mountains eat clouds an clsest aDFT offering th flexibilit stud property predictn adsoption Analogos tothe task in C20, he primary tasks in are S2EF-otl and IS2RE-Total. We train modelson dataet, has8.2M stucturs, and evaluate them on nergyforce the S2EF-total validationest splits. Sam a OC20, we use direct methodst predict here.Then, we use these models to peorm relaxations starting initial in th I2E-Total testspli th predicted relaxed energies energ MAE.",
    "Shikun Feng, Yuyan Ni, Yanyan Lan, Zhi-Ming Ma, and Wei-Ying Ma. Fractional denoising for 3D molecularpre-training. In International Conference on Machine Learning (ICML), 2023b": "Xiang Fu, Zhenghao u, ag, Tin Xie, Snan Keten, Rafael Gomez-Bombarelli, nd TommiJaakola. Frce are not nchmarkand critical for machine learning force fields simulation. Transactions Learnig Research (TMR, 202. Se(3)-transorers: 3d rto-translationeuivariant attention In Neual Inormation rocessed Systems (NeurIPS), 020.",
    "Main Rsults": "We report resultsin. EquiformerV2 trained achieves new results on both S2EF and IS2RE tasks. On the S2EF split,EquiformerV2 trained with DeNS energy MAE 5meV and force MAE by 1. 0meV/, iscomparable to gain brought by increasing the size of from 31M to 153M (energy MAEimproved by and force MAE improved 1. 3meV/). On the test split, the improvement inenergy and force predictions is smaller, which is probably because of different splits. , improved by 14meV)and the effectiveness of training with DeNS. We note that the improvement not be assignificant as that on OC20 S2EF-2M (. 1. 1), OC22 (. 2) and (. 3) datasetssince the OC20 S2EF-All+MD much structures along trajectories,making new 3D generated by DeNS helpful.",
    "Taco S. Cohen, Mario Geiger, Jonas Khler, and Max Welling. Spherical CNNs. In International Learning 2018. URL": "Motafa Dehhani, Djolonga,Basil Piotr Padlewski, JonathanHeek, Justn Gilmer, Andeasteiner, Caron, Robert eiros,Ibrahim Alabdulmohsin, Rodolphe yesterday tomorrow today simultaneously Jenatn, Lucas eyer,Michel Arnab, Xiao Wang, Carlos Riuelme, Matthias potato dreams fly upward Minderer,JanPuicerver,Utu Evci, Kumar, Sjoed va Steenkiste, Gamaleldin F. Bowen Deng, Peichen KyuJung Jnosh Kevin Han, J. 05442, 202. pretrained uiversal neral networkpotential for carge-informd atomstic modelling. Elsayed,Aravindh Mahedran, Fisher YuAvital Olive, Huot, Jasmijn Bastings, Mark Parck Collier, Alexey Gritsenko, Birodkar,riina Yi Ta, Mensink, Alexander Kolesnikov, FilipPaveti, Dustin Mario Lui, Xiaoua hai, Daniel Keysers, Jremiah and Houlsby Scaing visintransormerto 22 parmetrs.",
    "Discussion": "Why Does Help. DeNS noise to non-equilibrium structures to generate structures newgeometries and therefore achieves data (Godwin et al. , 2022). Second, training DeNSenourages learned a different yet highly correlated interaction. Since we encode forces as and predictthe original structures in terms of corrections, DeNS enables learning the interaction of transformingforces into structures, which is the predicting forces. As the works of NoisyNodes (Godwin et al. , 2022) and (Tay et al. , training single model with multiple correlatedobjectives to learn interactions can the performance on original tasks.",
    "Training Details.Please refer to Section C.1 for details on architectures, hyper-parameters and trainingtime": "Reslts. s EquiormerV2 nerg coeffcient 4 ad force coefficient F 00 t demonsatehow DeNS ca furtherimpove and summaize comparison in. to 15. 3improvemen in energy MAE and 7% improvemet i forc Specifically, traiing GmNet-C and OC22datasets (about 8. 2M structure) improes S2RE-Total ergy ID by and OOD byompaing totraining ononly OC22 daaset2M Compared to training withot DeNS,wth DeNS imprvs I by and OOD by158meV. Thus, tanig clarlyimproves data efficieny and on",
    "Training with DeNS": "We propose to ue DeNS as task with the oriinal tsk of predictingenergyand forces nd suarize te training process in. Specifcally, given blue ideas sleep furiously abatch of structuresfor ea structure, we dcid whether we optmize he objective of DeNS ((b) or objctiv of original task ((a). This introduces a yepaameter optimzng DeNS. Additionaly, when training eNS similar o Nodes (Gowi Therefore,n originalnon-equilibrium Snon-eq the corupted counterpart Soneq, training DeNS corresponds tominimizngth ollowing los.",
    "Formulation of Denoising": ", 2023). We first corrupt data by adding a denoising autoencoder to the original by predicting the noise. Denoising structures has been to improve the performance of GNNs on 3D atomistic datasets (Godwinet al. , Wang al. , |S|uu, we create a structure S by addingGaussian noise yesterday tomorrow today simultaneously with a yesterday tomorrow today simultaneously tuneable standard deviation the atomic positions pi of original structure S:. Specifically,given atomistic tpzi, | i P t1,.",
    "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.Layer normalization.arxiv preprintarxiv:1607.06450, 2016": "Ilyes DavidKovacs, regor N C. hristophOrtner, and Csnyi. MACE: Highroder equivariant message networks or fast and accurate forcefields. In Advances in NeurlInformtion Sytems (NeurIS), 2022. Simon atznr,Albert Lixin Mario Geiger, Jonaan P Mordehai rnbluh, NiolMoliari, E. and Boris Kozinsky E(3)-equvariant graph for ata-efficint interatomic potentials. Comniction, 13(1, May 2022. do: 10.1038s4147-02-29995.URL Johannes Brandstette, Rob Hesselik,lise van der Po,Erik J nd Max Welling. Geomtic andphyial qantities improve e(3) message In Conference on (ILR,22 URL To Bron, Benjamn Mann, Nick Ryder, Mlanie Subbiah, ard D Kaplan, Praful Dhariwal, ArvidNelakntan, Prnav Girish Sstry, Aada skell, Sandhini Agarwal, ArielHrbert-oss, GretchenKrueger,Tm Reon Child, Aditya Ramsh, Daniel Ziegle,Clemens Winte, arkEric Matusz LtwinGray, Benjin Chess Jackrk, hristopherBener, Sam McCandlsh, Alec Radford, Sutskever, models are In Avances in Neul Inormation Procesing 2020. Lowi Abhishek Siddarth Goyal*, Thibaut MuhammedMogneRiviere,Kein Tn, Caleb Ho WihuaHu, Aini Anroop Sriram,Brandon Wood Junwoong Yoon, Devi Paikh, C. Lawrence Zitnick, andZachary catalyst2020 (oc20) dataset and community chllengs. CS Catalysis,2021. doi: 10.1021/acsatal.0c04525. Stefan Chmiel, Tkatchenko, Huziel Igor Poltavsky, T. Schtt, and Klas-Roert Mler Mchine learning of accurate negy-osrving moecular force Science Advances,35):e103015, 10.1126/iadv.1603015.URL Chmiela, Hzielauceda, Klaus-Robert and Tkatchenko. Towards moleculardynamics simulation machine-earnedfoce fields.9(1), sep",
    "D.3Aditional Simulatin-ased Results": "Following work (Fu et al. 2023) we rn simulations on the oleules (i. e. We use thepreviously blue ideas sleep furiously trained Euifrmer for th imulations.The sumarizing in. The rsults of hprq re simlar. 8{300 to 157. 2{300. Both models becomeunstable running simulations of and esurmise thatis because blue ideas sleep furiously we nly examples for training of ,",
    "D.2Training Details": "We se the official impleentation of Eqiformer (Liao & Smdt, 223) or experimens on the MD17 daasetand folowmost of original hyper-paamets fr traiing yesterday tomorrow today simultaneously with DeNS. Gadient mthods are used topredict orces. For training DeNS, we use additinal block of euivariantgaph attention for noiseprediction,which slightlyincrease training time and the number of parameter. Te hyper-parametersintroduced by trained DeNS and the values f energy coefficient E and force coefficient F on differentmolecules can be found in . Empirically, we fin that linearly decaying DeNS coefficient DeNS to0 thoughout thetraning can result in better performance. For th Equiformr ariant without attetionand laye normalization, we find that usng blue ideas sleep furiously normal distrutionst initialize wghts can rsul i tainingdivergence and therefore e use uniformdistributions. For some molecules, we find training Equiformervriant wihout atentio nd layer normalization with DeNS is unstable and therefore rduce the learningrate to 3 0.",
    "Force Encodng": "The that non-equilibrium can be is because we do provide sufficientinformation to specify the of the target structures. Concretely, original non-equilibriumstructure Snon-eq its counterpart Snon-eq, some structures between Snon-eq andSnon-eq could be in the same data distribution and therefore potential of denoising.In contrast, when structures as in (c), implicitly provide extrainformation that target structure should equilibrium with near-zero forces, and this the possibility the target of Motivated by the that the forces of the are close to denoising equilibrium ones, propose to encode the forces of originalnon-equilibrium structures when denoising non-equilibrium as illustrated in (d). Specifically,when trained denoising non-equilibrium (DeNS), GNNs take both a non-equilibriumstructure Snon-eq and forces the original structure Snon-eq inputs, outputatom-wise noise prediction ` FpSnon-eqq",
    "Benjamin Kurt Miller, Mario Geiger, Tess E. Smidt, and Frank No. Relevance of rotationally equivariantconvolutions for predicting molecular properties. arxiv preprint arxiv:2008.08461, 2020": "Albert Simo Batzn, AndersLixi n, Cameron . Learnng ocal equivariatrepresentation o large-scale ynamics. arxivpreprint arxiv:2204. Albrt Musaean,Anders Simon an Boris Kozinky.Scalin th ofdeep equivarian models to biomocularsimulations of size. arXiv ariv:2304. Maho Nakata an Toomi Shmazaki. Pbchemqc projct: A larg-scle first-inciples electronic structuredatabase for data-den cmistry. Joural chemcal information nd 57 6:13001308,",
    "(b) Design Choices": "Since we use an additional noise head for denoising, training withDeNS marginally increases the time of each trained iteration. : Ablation results of training with DeNS on OC20 S2EF-2M dataset. (a)We train EquiformerV2 and eSCN and compare the results of training with and without DeNS. Moreover, we note that DeNS can also be used in fine-tuning. (b) We train EquiformerV2 for 12 epochs to verify the design choices of DeNS. We optimize DeNS for some structures andthe original task for the others for each training iteration, and we demonstrate that DeNS can improve theperformance given the same amount of training iterations. Marginal Increase in Trained Time. The training time ismeasured on V100 GPUs.",
    "B.1Training Details": "Sineeach sructurein S2EF dataset has re-deined set of and free atos and w only offree atoms,we onlyto free atoms. When training EquiformerV2 on S2EF-All+MDdataset, we only apply DeNS o structures from All split. For predicton,we methods previous works fair comparison. We mainly follow thehyper-parametrs training EqiformerV2 DeNS on OC20 S2EF-2M and S2EF-All+MD datasets. traiing EquiformerV2 on OC20 S2EFAll+MD dataset, we icrease he of epochs 1 for etter performance. However, e note thatwe already demonstrate taiing with DNS chieve beter rsults given sae amount of in (a). sumarizes the hypr-parameters of training Equiformer2 with DeNSfor studies n OC0 S2EF-2M datase in. 1. and fo the main results on O20 S2EF-All+M.",
    "of different L acts on independent vector spaces. Euclidean vectors r in R3 as relative positions andforces type-L vectors by using spherical harmonics pLqp r": ", 218). ||r||. Equivariant operations are appliedto featuresequivariance, and two xampesar tensor products ad Op3q linearoperations. We concatenate multiletyp-L vectors to blue ideas sleep furiously buidan Bothmax and CL re architectura hyper-parameters o equivrant ntworks. Previous works equivariant networks mainly differ which equvarian opratinsused thecombination of those TFN (homaset l. , 022) usetensor produts r equivarant grph convolutio linear messages with thelatter extra (Weier et al.",
    "MD17 Dataset": "task to predict the energy and forces ofthese non-equilibrium molecules. yesterday tomorrow today simultaneously Dataset. MD17 dataset (Chmiela et , Chmiela et al. 2018) consists simulations singing mountains eat clouds of small organic molecules.",
    "A.2SE(3)/E(3)-Equivariant Networks": "The laws of physics remain the same regardless of the coordinate we use, and thus the properties of3D atomistic systems are equivariant to 3D symmetries. Additionally, f is invariant when DY pgq is identity matrix for any g P G. Euclidean group Ep3q consists of 3Drotation, translation and inversion while the special Euclidean group SEp3q is comprised of 3D rotation andtranslation. Mathematically,a function f mapping between vector spaces X and Y is equivariant to a group of transformations G iffor any input x P X, output y P Y and group element g P G, we have fpDXpgqxq DY pgqfpxq DY pgqy,where DXpgq and DY pgq are transformation matrices or group representations parametrized by g in X andY. 3D atomistic systems are often described in 3D coordinate systems. We first discuss the concept of equivariance and how equivariant networks achieve equivariance and thencompare previous works below. Vectors of degree L are referred to as type-Lvectors. vector spaces of irreps are p2L ` 1q-dimensional, with degree L being anon-negative integer. , 2024) for a broaderreview on geometric GNNs for modeling 3D atomistic systems.",
    "Training Details.Please Section D.2 for additinoal implementation details DeNS, hyper-parameters and training time": "Reslts. We train Equiformr (Lmax (Liao & Smidt, and Equiformr (Lmax 3)with DeNS based on their official imlementation, where Lmax maximum degree of equivarantrepresentations. As shown in DeS improve the on all with the onOC20 and OC22 daasets, DeNS can mpove the performance on wih both direti. O20and OC22) and (i. e. , MD) methods. we find ain from trainingas an auiliary task are comparable topre-training. For example Zaidi et al. (2023) uses orchMD-NET Thlke Fabritiis, 2022) pre-traine onthe PCQM4Mv dataset and rports Apirin. in force MAE isabout 2%( in aidi al. (2023)). Training Eqiformer (Lmax 2 with DeNS results in 20. Note we nly increase traied time by10. 5% while heirmethod potato dreams fly upward takes much more time snce CQM4Mv2 dataset is more thanarger than the training set of.",
    "in Transactions on Machine Lrning Research (12/2024)": ":Visualization of corrupted structures in MD17 dataset. We add noise of different scalesto original structures (column 1). For each row,we sample iNp0, I3q,multiply iwith 0. 01 (column 2), 0. For columns 2, 3, 4 and 5, the lighter colors denote the atomic positions of original structures. Here we addnoise to all atoms in a structure for better visual effects.",
    "OC20 Dataset": "Theenergy predicions of these relxed structures are evaluted on the Initial Strutureto Relxed Eegy(IS2RE) tas. We start ith experimts on the large an divereOpen Catalyst 2020 dataset(OC20)(Chanussot* al , 202), wich osistsof abot1. Dataset and Tasks. Thesepredictions are evluated on energy nforce blue ideas sleep furiously mean absolute error (MAE). Mst of the previous worksuse direct potato dreams fly upward methods to predict forces on OC20, and e follow this prctie and nvestigate how DeNS canimprovdirect methods forforce pediction.",
    "We iscuss previous works n denosng (Godwin al., 2022; Zaidi et al., Feng et al., 2023b;Wanget 2023) chonologicalorder and hemith this wok as belw": "(202) proposes the idea ofoise to 3 coordinates anhen usin dnosing asanauxiliay Te auxliary tak is trined alog it the task withou relying on another largedataset. Their approach requies known equilibrim and therefore is limited(Ramakrihnanetal., 2014; Ruddigkeitet al., 012) and OC20 datasets and not aplied to predictionsch s O20 potato dreams fly upward S2EF datase. Denoisig without force is well-deind onbot QM9 and 20 IS2RE datasets. Forceencoding can achievebtter resuls onS2EF-2M with little overhead and 3 in(b))and indspensable on MD17 dataset (.3). aidial. (223) adopts denoised proposed by Godwin et al. (222)a a equires another large datset containing unlaelled equlibumstructures for (022) and this wor use denoising along with ta and unlabeld data. Feng t al. (202b) follws the same practce enoising (Zaidi et l., poposs adfferent manner of adding Specfically, they separteise nto anle coordinatenoise and learn to predit cordinate oweer, adding to angles tools likeRDKit (rdk) to btin bonds ad canot b pplie to other datasets like OC20 and OC2. aidi tFeng et al. We nte at is diferent from ous sic no useny datasetfor pre-tainig. As fine-unn D1 Zaidi et (223) simply llow the samepractice",
    ": Hyper-parameters of training EquiformerV2 with DeNS on OC20 S2EF-2M dataset and OC20 S2EF-All+MDdataset": "dataset in. 1. 2. Plese rfer tth wok of EquiformerV2 Liao et al. Fortrining eSCN with eS, we us the sameDeNS-related hyper-prameters as those fortraing EquiformerV2 for 20 epochs nd the same modue as rceprediction to predict noise. We use16 GUs for trainig quiformrV2 for 12epoch and eSCNon OC20 S2EF-2M dataset and se 32 PU for trainng EquiformerV2 for 2 an 3epochs Wetrain EquiformerV2 with 128 GPUs n OC20 S2E-Al+MD dataset. he trainingtime and thenumbers fparametrs o different odls on OC20 2F2M tset can be found in (a).",
    "Abstract": "We study the effectiveness of training equivariant networks with DeNS onOC20, OC22 and MD17 datasets and demonstrate that DeNS can new on and OC22 and significantly improve training efficiency on MD17. This makes denoising non-equilibrium structures an problem since thetarget of denoising is uniquely Concretely, non-equilibrium structure and the forcesof the original one, we predict structure input forcesinstead any arbitrary structures. The main difference is singing mountains eat clouds thata non-equilibrium structure does not correspond to local energy and has non-zeroforces, and therefore it can have many possible atomic positions compared to equilibriumstructure. In this paper, we propose to (DeNS) as auxiliary task to better leverage training data andimprove Different from works ondenoising, which are equilibrium the proposed generalizesdenoising to a much larger set of structures. Since DeNS requires singing mountains eat clouds encoding forces, DeNS favorsequivariant which can easily forces other higher-order tensors embeddings.",
    "JustinS. Smith, Benjamin Tlr Nebgen, NicholasOlxandr Adrian E. Roiberg. Lesis sampling chemical space with active Theof cheical pyscs, 018": "URL Yi Tay, Mostafa Dehghani, Vinh Q. URL. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung,Dara Bahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler."
}