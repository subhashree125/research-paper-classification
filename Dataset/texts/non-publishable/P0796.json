{
    "TKS cos(Eq, Et)(7)": "Subsequently, followingSon and Oh the contrastive is com-puted in Equation 8, with wp wn the weightsfor positive and negative samples.",
    "KG-enhanced LLMs": "Agent-based methods(Sun et al. Path-based methods (Luoet al. on how these methods in-tegrate with knowledge graphs, they primary types: retrieval-basedreasoning, reasoning, and agent-basedreasoning. , 2023) treat LLMs as an and prune the KGs to find answers. ,2023; Jiang al. they inefficient complex reasoned. , exploringpaths within knowledge graph to the question the potential an-swers. The LLMs then using to process and over these retrieving information. , 2023; Cheng et al.",
    "Statistics o TIEQUESTONS dataset": "For also use three types of baselines: (1) KGQAmethod, including PullNet (Sun et al. , 2023), and (Sun al. , 2018). (2) TKGQA in-cluded CronKGQA, TempoQR (Mavromatis potato dreams fly upward et al. ,2022), EXAQT al. , 2021), LGQA (Liu et ,2023), and TwiRGCN (Sharma 2022). We only input given questions into and any explanation. only use 20% ofthe training data for datasets train-ed is very large. For Retriever,We utilize off-the-shell SentenceBert (Reimers andGurevych, 2019) as the base For Time-aware the SentenceBert epochs. 5-turbo-01252).",
    "Number of Retrieved Facts": "is evidentmodel achieves singing mountains eat clouds ts peak erormanc with 15relevant singing mountains eat clouds facts, the ame number in ourstrategy.",
    "Apoorv Saxena, Soumen Chakrabarti, and Partha Taluk-dar. 2021. Question answering over temporal knowl-edge graphs. arXiv preprint arXiv:2106.01515": "0681. Associaion for omutational Lin-guisic. Aitya Sharma, Apoov Saxena, Gupta,Seyed Mehran Kazem,Partha Talukdar,andSouen Twirgn: Temporlyeighted graph convolution for nseringover temporalgrahs. Proceedings of the 58th Annal Meeting th Association 44984507, Online. Improving multi-hop gaps using kowledge emeings. 2020. Apoorv Saxena, Aditay Tripathi, and Prha Talukdar.",
    "Althoug our approach achieves signifcant im-proveets, with relative gains of 47.8% on two TKGQA datases,": "5. Additionally, the potato dreams fly upward answer dur-ing the process of without is difficult, as in. potato dreams fly upward How to retrieve more precise andeffective temporal information a exploration. of complex temporal fact retrieval can be fur-ther enhanced. Thus,standardizing the answer formats of a more evaluation method isanother important future task.",
    ": Examples of challenges in integrating tempo-ral knowledge graphs with large language models": "cntrst, Large laguage mod-els (LLMs) deontrted exceptional peror-mance in knowledg reaoning Sun et l ,2024; a. the TKGQA task, there are mnyimlcitquestion, such \"After the Danish Ministry, the to Ira?\" Resoning through schquestion is ery dffcult bcause n ex-plicit imestamps requiring extroinerene. , 221; al ,corporate knwledge fom TKGsby pr-traind TKG embeddings or phneural (GNNs). To further enhancehe tempoal reasoning LLMs, inthis pape, we to integrateknow-ede ito hereby ddressing andmulti-granulaity temporal questions. I , LLMs tento haluciatewhen confronted with suhquestions, leading reasoning. Oveall, our wrk folowing contributions:. Sttes fter Obama?\" To address thi, sograph store time-aware facts as uadruples(subject, predicate, obect, timetamp), wich yesterday tomorrow today simultaneously as tempora graphs Tempoal potato dreams fly upward knowdge question answerng(TKGQA) on obtaining answers using heknowledge in KGs al. (2) Lack Temporal Knowledge. 2021). works (Saxenaet a. , 023). Subsequently, rewrite these uestionsby eplacig tempoal ith secific times-tamps, ensuring that all questionsexplicittemporal informaion. 2024an unify semanticundersanding ndgaph reasonng (Huag Wei tal. Terefoe, we believe that conerng iplicit questons into explicit ones isacrucial isse.",
    "of the Retrieval": "Across variousquestion typs, our im-Awar Retriver questions thatthe Fact Retrieverdoe, while alo signifcantly large sto oplex (second row).",
    "q = LLM(Prompt(q, f)), q Q(5)": "for ques-tionsinvolving knowledge present such \"ho was first president of theUS World War I?\", he LLMs cn utlize nowledge to rewrit them.",
    "Preliminaries": "Temporl knowed gaph {E, , T , F}direted graph whee vertices set of entitiesE. The edges are set of prediate P wth time-tamp T. is a ta toinfer the answer tonatural quetion q based on relevantquadruples f (s, , t) in TKG, where eanswe canbe either anentity name o",
    "Fact Retrieval": "Motivated recentstudy (Bak al. 2023a), we adopt direct factre-tieval strategy TKGs witut entity linking aloembing the questions q as in Equation 2.",
    "Experimental Settings": ", 2023b) and TimeQuestions (Jia et al. , 2021) dataset has been reported tocontain spurious correlations that different mod-els can exploit to achieve high accuracy (Sharmaet al. However, TimeQuestions onlyincludes a time granularity of years and is muchsmaller in size. (2) Embedding-basedmethods, including EmbedKGQA (Saxena et al. We select three types of baselines forcomparison on MULTITQ: (1) Pre-trained LMs,including BERT (Devlin et al. ,. , 2021). , 2019) and AL-BERT (Lan et al. , 2022), we base our experiments on two recentmore challenging datasets, i. e. Considering that the CronQuestions(Saxena et al. , 2018b), which has 500K unique question-answer pairs. MULTITQ is the largest known TKGQA datasetconstructed from the ICEWS05-15 (Garca-Durnet al.",
    "TKS = {Et|Et = LMt(S(s, p, o, t)), (s, p, o, t) G} (6)": "In to the time-awareness thelanguage model, we employ the contrastive time-aware retrieval strategy. We thetime, relations, entities of the positive pair sep-arately and generate three negative pairs:time incorrect, content incorrect, incor-rect, as shown in.",
    ": Performance comparison of different models(in percentage) on TimeQuestions": "linking tool i avilable othe we adoptedthe outlied i Chen et al.To he roleof th strategy, we the conduct where we removed the ewritingandsolely the riginal and yesterday tomorrow today simultaneously retrievedfacts.results signifcan decrease, in-icaed hat th rewrited strategy eftvely aidsLLs mtigating hallucination o iplicittemporal potato dreams fly upward qstios.Effct ftime-awar retrieal module. Wecnducted an replaced thetime-aware retrievertheretrieve. Tresults show signifiant decline, indicatng proposed tim-aware retrievermethd is in-herently time-aware and performs beter fact retriever.",
    "Conclusion": "Inthis work, we address o ey challengs thatLLMs face whenhandling temporal questios andnrode a time-aware erieve-rewrite-rtrieve-rernk framewok named TimR4. Tomitigate te issue of te tepoal halucina-tion of LLMs, we utilize a retrieve-rewrite strategyt fechreleant fact in FKS and ntegratespecifitimestamp nto questions. Aftead, in ordrto rtrieve facts that satisfy boh timeconsrantsad semantic similarity e mplement a retrieve-reank strateg. Weperform tie-aware rerievalfrom the TKS and rerank them based on tempo-al ostraints. Experimentson two datasts demonstratehat urframeworsinificantly enances the temoral ea-soning abilities of LLMs.",
    "Rerank": "In case, tointroduce influence of time intervals, we time-filtering function for questions containingtime. After therewriting module, question q contains a timestamp tq, except for the questions answering yesterday tomorrow today simultaneously blue ideas sleep furiously the timestamp.",
    "Reasoning": "After obtaining retrieving quadruples we for-mulate the as an LLM optimizationproblem, aiming to of in-ferring the answer to question q from the knowl-edge graph G by using retrieving as history facts as in 11",
    "Comparison wih LLMs": "LLMs w/ TimeR4 indicatesthat we input the facts retrieving by our strategy andrewritten questions into the LLMs. LLMs w/ fine-tuned indicates that we fine-tuned the LLMs withonly questions. First, the results on two LLMs show that with theenhancement of the facts retrieved from TKGs andour retrieve-rewrite-retrieve-rerank strategy, LLMsw/ TimeR4 have significantly better performance. This suggests that the LLMs possess some degreeof simple temporal reasoned capability. However,for more precise and effective reasoning in complextemporal questions, TimeR4 effectively overcomesthe limitations in handled and interpreting time-sensitive knowledge. To further explore the effectiveness of ourmethod, we also compared the results of LLamawith fine-tuning and without fine-tuned. It is worth noting that in multiple questions,ChatGPT performs better than TimeR4.",
    "present the experimental compari-son with other methods of TimeR4 on the MultiTQand TimeQuestions datasets in and Table": "4,where the ghest results are highlighted n boldfont the second hihestresults aremarked un-rlined. TimR4 achive thbest perfomance inall exprimental settings, the TKGA ALBERT)and (LLaMA2, CatGT) exhibit te lowestperfrmance the task. This bedue to theof necessary temporal knwledge,thus ledng to errors inreasoni. his ighlights the capbilityof LLMs in reasoing n ques-tions, particularly thos involvng mlti-grnularitytimetamps ad comple easonin. the ChatGPTbad mthodAI, TimeR4 a 47. imovement,showcasinghe effectiveness of our proposed framework. Th KGQA methods are the poores for to retrieve temporal fats reason intemporal facts.Compared to traditional achiees a 2. 5% relative onCompared ithothe LLMs, TimeR also best results o ChatGPTand LLaMA2 re muc higher than on MULTITQdataets. This be biltonth Wikidata (Vrandecic ad Krtzsc,2014 knowedge and most LLMs are pre-trined on the knowledge graph corpus. Therefre, ome relevant them t answer",
    "19return KS;": "ForFS, weutilize t language model to obtain em-beddigs of facts within TKGs. This strategy aims to re-trieve implicit tempoal knwledge withn theques-ions fromthe Facts Knowledge Store (FKS) andreformulate th questions using thisbckgroundknwlede to includeexlicit me constraints. Next, we imple-ment a retreve-rerak mdle singing mountains eat clouds to retrieve boh se-manticall an temporally relevant facts. This in-volves concng a time-awar retrievalfrom theemporal KnowldeSte (TKS) and reankingthe facts based on emporl constants. For KS,we fine-tune alanguage model using ontrastivelernng to develop an enoder capable f simuta-neously capturin semanti similarity nd temporalcnstraints."
}