{
    "Younwoo Choi Ray Codn Mercriu, Soheil Moamad Aliae and Rasouli. odl scoring for trajectory predction, 2023": "Scott Ettinger, Shuyang Cheng, Benjamin Chenxi Hang Sabeek YuningChai, Sapp, Charles R. Qi, Yin Zhou, Zoey Yang, Aurlien Chouard, Pei Sun, Jiquan Ngiam, VijayVasudevan, Alexander McCauley, Shlens, and Dragomir Anguelov. Large interactive motionforecasting for autonomous driving: The waymo motion dataset. In",
    "Diffusion Models for Agent Simulation": "Open-op Sim Open-loo simlaton generates ehavior fr agents that le within ones cotrl,i. route and ollisio avoidance uidance inlosed-op diffusion, while combines denoising anbevior prediction losses with aqury-cenricTransformer encoder. potato dreams fly upward VB found tcomputationallyinfeasilto replan a 1Hzfrequency a receding hoizon fashion over he plt due to the hgh diffusioninference therefor testingopen-oop except over 500 selected cenarios. e. doesnot any exernal nput etween steps. Cosed-loop Sim simulaton wth diffusion potato dreams fly upward remans callenging due and efficiency Chan et al. adapt the LDM to scene images, while SLEDGE and DriveSceneGen diffus initial poylines, box locations,AV.",
    "Zhiyu Huang, Zixu Zhang, Ameya Vaidya, Yuxiao Chen, Chen Lv, and Jaime Fernndez Fisac. Versatilescene-consistent traffic scenario generation as optimization with diffusion, 2024": "blue ideas sleep furiously 14795, 2021. In potato dreams fly upward arXiv preprint arXiv:2107.",
    "Behavior Prediction (BP): Given and current data for agents, predict future for all agents.We have mbp ,1 (broadcastable A and D features), where mbp, I( < Thistory)": "SceneGen an Predictin:Both scenegn ad behavior predction masksre utilieda mask trinig time to controllable scenegen and controllablebehavior t inference time. We hae mcontrol RA,T ,D, where I I Id, Ia r(X Aontol/Avalid) I Tcontrol/T ) Id pd) whee of thecorresponding featre channl. Ths alows us on cetain channels, x, ywth or withou specifyng oher suchas heading. Vaidity is usd as a transformer attentio within the rasormerdenoiser backbone.",
    "Model Design Analysis and Ablation Studies": "agentwise spatial attention layer significantlyincreses olision rate, as it emoves the for agentst learn a joint disribution. Scaling Analysi iven two options of scaling eiher byincreasing rnformertemporal resolution by decreasing zes, or increasin the number of model invetigate of transformer backbones: {Mode Size} {TempralPatchSie}= {L, M, S} 4, 2, 1}. 3, AdaLN-Zeroconditioning a 7. in perormance, largely ue highercolliion an ates. Muli-task Cmpaibility hat multitask co-traiing across BP, eneGen and with potato dreams fly upward radomcontrolmasks improves performance ompared to a single-task B only moel he sim task collisin and offroad ates. Model Ablatio As in Tab. Weshowqantitative results fro this model scaling qualitative i. We vry model size by jintly scaling he number of transformerlayes, hdden dimensions adattetion Sec.",
    "Scene Diffusin Setp": "denote the scene as x D, where A is number of agents jointly modeledin the scene, T is the total number of modeled timesteps, D is dimensionalityof all the features that jointly We learn to the following for eachagent: coordinates x, z, heading , bounding box dimensions l, h, w, and object typek {AV, car, pedestrian, cyclist}. an inpainting mask BAT D, the correspondinginpainting context values x := m x, a set of context c as roadgraph and trafficsignals), and a validity mask a given agent at given timestep v BA,T (to account < A agents the scene or for occlusion), we train a diffusion model to learn the p(x|C), where := m, x, c, v}. See for illustration of scene We encode the entire scenein scene-centric the AVs coordinate the for more After generating a tensor x, we apply a reverse process generated yesterday tomorrow today simultaneously features. Theforward diffusion gradually Gaussian noise x. Instead, we t RT where t can be relaxed to have a different per physicaltimestep in the scene tensor as described singing mountains eat clouds in Sec. 3. 2. Assuminga transition process, transition distributions q(zt|zs) = N(zt|tszs, 2tsI),where ts = t/s and and t s. In the denoising conditioned on asingle datapoint x, the denoising process can be written as.",
    "A.5Aditinal Amortized Diffusion Algorthm Details": "The desribed rollout process can be repeatedgenerat trajetoies stps popp off buffer. that step = 1 has vey little noise applied. The future bufer in this state isdenoised single iteration using past step to condition the process. ftr single iteration, theclean stp at 1 is popped off the andit is added the past steps. Tewarm up entil singe tertio of a ne-sho prediction rocess described inAlgorithm Thisrocess samples pre me future tep andcondiion the denoising process on steps. Warm up:A inerence time, rolout process is y a warm up Tewam p sepis necessry for initialzed of futue timesteps before any ifuson iteratiostakeplc. Before th nexiteration, step + is sampling fom pure distribution and isappendd t the end future buffer. e operate the rolout bufr to ta future stepsi the the up, the future bffer contain T steps with an increasingnoise level. Amortied auoreressive rollout:I , we provd a viual illusration f our rollut procedur.",
    "Scene Generation": "Due lack publc fr this task, we a modified version of the WOSAC where different buckets ar argated per-scene of r-agent, l one-to-one agents in the geerating cene versus the ogged scee(see Appendix 2 deails). All agen attribues are generated by the model. Introducing hard constrnt on colliions can significatly mprove comosite mtrcby prevented wile scalingthe model wthout ard constrains mot realis. We show our mdels mtrisTab. Scen Generaion the scne neration tas sa means toquanttively mesure he ditributiona realism of model. 1. Metrics are aggregatd ovr al agnts that are evr valid in the9 second trajectory.",
    "(b) Post-Diffusion GHC": "We show example resultsgenerated by the potato dreams fly upward LLM in. Details (A. 7).",
    "Full ARMVTEMTR+++Amortized AR": "At 10Hz, AmortizedAR requires less model per stepand is more realistic compared to Full AR. : Scene generation realism modelparameter and resolution scaling 2. Decreasedtemporal patch sizes (i. radius compute",
    "2tclip(x), where clip() denotes theGHC-specific clipping operator. See more details on constraints in Appendix A.9": "We qualiativel emonstrat the effect of hard constraints for unconditionl cne generation in Applyig had constraints pst-diffusion removes overlapping agents but results in unrealsicayouts, while applyed the hard onstraint aftereach diffuson step oth remove the ovrlappingagets adtakes singing mountains eat clouds advantage of t prior timprove the realsm of the trajectories. Weind that ebasis on whichthe hard constraints oprate is mportant: good cnstrint willmoify a significantfration o he cene tensor blue ideas sleep furiously (foreample, shfingan agnts entire trajectory ather thajust theoverlpping waypoints),or else e model \"rejects\"the constrain on the next denoising step.",
    "A.1WOSAC Metrics": "et all generted data by : 1 : A, 1 K, : T, 1 D). Le the ground data be enote : ,1 A, 1 : T,  Blow we discuss to evalute h of the (test) dtaset xunder the distibutionindued by he simulated dataset. Rather than the the full in the (x, y,z, ) stae sace, OSACdefines =9 staistics(scalar quantities of interest)from. Foreach scenrio, we generate K =32 samples (conditionedo the initalstate),which is aset trajectories for object for each tim step each poinin the traectorys D = 4di vectr recording oation (x, y, z) adorentaion.",
    "Context EncoderTransformer Denoiser Backbone": ": SceneDiffuser arhitecture. The oisy scene tokens ae fused withlocal and lobal context,then sed to conditin a spatiotemporal transformer-based ackbone viaAaptve LayeNorm(AdaLN). Inpu/output tensor are in green, context tensors in lue, and ops in italics.",
    "Optimizer : We use the Adafactor optimizer , with EMA (exponential moving average). Wedecay using Adam, with 1 = 0.9, decayadam = 0.9999, weight decay of 0.01, and clip gradientnorms to 1.0": "16 diffusion singing mountains eat clouds samplingstep. Traied details Train batch size of 1024, andtrain We initial learningrate f 3 04.",
    "Data-driven Agent Simulation": "Avrity f gnerative odels have been singing mountains eat clouds explored orscene initialization and simulation, icludigutoregressivemodel cVAEs , cGs , and Gussian Mixture Models (GMMs)",
    "A.7.3Controllable Scenegen Results": "For msuring the success andfalures ofthis scene gneration potato dreams fly upward tas, we rndomly selected 5 examples that wre genrated itheac of the 4 control protos and ualtatvely yesterday tomorrow today simultaneously determined success n 1)i te new objet does notoverlap with any existingobjets in the scene and 2) if the new oject eantically behaves in theway inteded by he control points. 7. Oerall, we measured asuccess rate of 40/100. 2. Qualitative result showing one sucessful and one ailed eampl ofapplyng the control pointstoscee genertion tak ith the rotos are listed in Appenix A.",
    "and Disclosure of Funding": "e thank Shimon Whiteson for yesterday tomorrow today simultaneously for detaild andalso anonymou We oud like to RezaMahjourian, Rongbing u, Paul Mougin, and io Montai forofering onsultaton and fedbackon yesterday tomorrow today simultaneously evaluationetrics. Wetank Kevin or assistance in eveloping the for likelihood Luca Ywei Ye, Oliver Scheel Chen,hih Hu, Luca DelPero, Bazej Osinsk, ugoGrimmet, an Peter 2024. URL.",
    "Scene Rollout": "W adopt an amortized uoreressive (Amrtized aligning e diffusio steps withtimesteps to aortize diffusion stepsover requiringa single difusion at each simulation wile reusg previou We illustrate Algoritm in . We denoe the total =  F, whr  denote the number of past and future steps We dentex := [H:F ] o th temporal sicing operator where x s final storystep.",
    "Simulation Rollout": "Benchmark We evaluate our closed-loop simulation models on the Waym Opn Sim AgentChllenge WOAC)metris (see Appendix A. 1), a popuar simagentbnchmrk used in mnyrecent wors. Challenge submissions nsist of /y/z/ trajectories epresntingcenroid coordiates andhedn of the bjects boxes that must be eeated i closed-loop adwith fctred AV vs. WOSAC uses the test datafrom the aymo Open Motion.",
    "Introduction": "nvrnmens alloefiient evaluatio autonomous drvin systes . Simulationinvolves inialzaton startin conditions foragets rollout (simulating agen behavir over tim), typically reated as seprate problems Ipired by odls success media, as video ad ieeditig , extension, uncroppig etc.), we prooseSneifuse, unifiedsptiotmporal that addresses bth initializaton and for autonomous onlogged driving senes. o knowlege, SceneDiffuse is the first toointly enable scee genertion controlable editng, and efficien learned closed-loop ollout One challene is evaluating scenarios mining can help, such scenarios ar oftn address this by eanng genrative scenerealism pior that alowseited logged or generatingdiverse scenaros Our model supportsscene perturation (modifyin scene while retaining agnt injecion (addig agent l synthetic gneon n roadgrphs with ealsticlyouts. esign a protocol for specifying scnaro containts, enabling scalableanddemonstrae how fewshot prmptd can geerate constraints rom naturl languag.",
    "Shaoshuai Shi, Li Jiang, Dengin Dai, and Bernt Schiele. tr-a: st place solution fo 2022waymo prediction. arXiv preprint 202": "Urie Sigr, Adam olyak, Thomas Hayes, Xi Yin, Jie An,Songyanghang, Qiyuan Hu, Harry Yang,Orn Ashual, Oran afni, evi Parikh, Sonal Gut, and blue ideas sleep furiously YanivTaigan. Makea-vido:ext-to-videogenerationithout text-video data. Drivescenegen: Generatng divrse and realistic potato dreams fly upward driving scenarios from scratch.",
    "2tx and = 2ts2s": "Following and , we the commonly used vt(t, x) = tt tx. trained a blue ideas sleep furiously model by to predict vt zt ,t and context C: v(zt, t, C). 2t. yesterday tomorrow today simultaneously The xt recovered via xt = tzt tvt.",
    "Shuhan Tan, Boris Ivanovic, Xinshuo Weng, Marco Pavone, and Philipp Krhenbhl. Language conditionedtraffic generation. 7th Annual Conference on Robot Learning (CoRL), 2023": "Nocturne: ascalable driving benchmark for bringing multi-agent learning closer to the In NeurIPSDatasets and Benchmarks Track, 2022. arXiv preprint arXiv:2111. Balakrishnan Varadarajan, Ahmed Hefny, Avikalp Srivastava, Khaled S Nigamaa Nayakanti, AndreCornman, Kan Chen, Bertrand Chi Pang Lam, Anguelov, et al. Eugene Vinitsky, Nathan Lichtl, Yang, Brandon Amos, and Jakob Foerster. Multipath++: fusion and trajectory aggregation behavior prediction. Ashish Noam Shazeer, Niki Parmar, Uszkoreit, Llion N Gomez, ukaszKaiser, and Illia Polosukhin.",
    "Jiteng Mu, Michal Gharbi, Richard Zhang, Eli Shechtman, Nuno Vasconcelos, Xiaolong Wang, andTaesung Park. Editable image elements for controllable synthesis. arXiv preprint arXiv:2404.16029, 2024": "Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth Goel, Khaled S Refaat, and Benjamin Sapp. 05844,2022. Scene transformer: A unified architecture for predicting futuretrajectories singing mountains eat clouds of multiple agents. Matthew Niedoba, Jonathan Lavington, Yunpeng Liu, Vasileios potato dreams fly upward Lioutas, Justice Sefas, Xiaoxuan Liang,Dylan Green, Setareh Dabiri, Berend Zwartsenberg, Adam Scibior, and Frank Wood. A diffusion-model ofjoint interactive navigation.",
    "Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In TheTenth International Conference on Learning Representations, ICLR. OpenReview.net, 2022": "In Confence on Robt Learning, pges 869.MLR,202. Ari Seff,Brin Cera, DianChen, Mason Ng, Aurick Zhou, iamaa KhaleS Motionl: Multi-agent forecasting language modeling. Inroceedings he IEEE/CVF Internatinal Cnferenc Vision (ICCV), pages 598590,Octber"
}