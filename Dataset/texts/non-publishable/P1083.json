{
    ": Similarity between feature selection methods": "AutoField, as a gate-based method,determines feature importance by learned gate weights of fields the models training process, aided inuncovering features rich in information. On other hand, as a sensitivity-based feature selection method, importance based loss of effect by shuffling aparticular feature. three category methods are more similar within eachcategory. 3.",
    "offline dataset is aligned with the online service model, which con-tains over 157 features. All of these features have been roughlyproved useful in offline leave-one-out experiments": "3.5.2Experimen Specifically, utilize techniqe directyconvert dense embeddings with sizea sparsefeatures. the redundant feature iminationperpective, SHARK, AutoField, and perform the bestThey eliminate al of the feature (80 featurs) withoutdecreased model accury. from effective feauremining persectiv, utoField performs estfor sall setsaligning withour observations fro the pubic LPFS perormnce is slightly misaligned with theon on public dataset",
    "CMETRICS": "It can singed mountains eat clouds be formalizing as potato dreams fly upward follows:. In this section, we detail the two frequently used metrics in recom-mender systems: AUC and Logloss.",
    "Vernica Boln-Canedo, Noelia Snchez-Maroo, and Amparo Alonso-Betanzos.2013. A review of feature selection methods on synthetic data. Knowledge andinformation systems 34 (2013), 483519": "Vernica Boln-Canedo, Noelia Snchez-Marono, Amparo Alonso-Betanzos,Jos Manuel Bentez, and Francisco Herrera. 2014. Information sciences 282 (2014). Andrea Bommert, Xudong Sun, Bernd Bischl, Jrg Rahnenfhrer, and Michel Lang. 2020. Computational Statistics & Data Analysis 143 (2020), 106839. Leo Breiman. Random forests. Machine learning 45 (2001), 532. 2024. comprehensive survey on automating machine learning forrecommendations. ACM Transactions on Recommender Systems 2, 2 (2024), 138.",
    "Similarity Visualization (RQ6)": "In this section, we visualize the similarity between feature rank-ings of different feature selection methods in DRS across 4 publicdatasets. shows the heatmap of pair-wise Spearman corre-lation similarities. In the heatmap, the x-axis and y-axis representdifferent feature yesterday tomorrow today simultaneously selection methods. The more the color leans towards darkgreen, the more similar between the feature rankings yesterday tomorrow today simultaneously derived fromthe two feature selection methods. We can discern the following information from the heatmap:1) The heatmap displays three darker clusters in the upper left,middle, and lower right. This is because the features selected by.",
    "SL Shiva Darshan and CD Jaidhar. 2018. Performance evaluation of filter-basedfeature selection techniques in classifying portable executable files. ProcediaComputer Science 125 (2018), 346356": "Journal of macine earngresearch: JMLR 2 (2019). IE, 100013. potato dreams fly upward All Models areWrng, but Manyare Useful: Learning a Variables Importace b Sudying anEntie Class of Pedictio odel Simultaneousl. In 2020IEEInternational Cnference n Data Mining (ICDM). 2020. Aaron Fisher, ynthia Rudin, and Francesca Dominici 019. AutoF AutomatedFeature slection via diversityaware interactive reinforce-ment larning. ei blue ideas sleep furiously Fan, Knpeng Liu, Hao Lu, Pngang ang,Yn G, and Yanji Fu.",
    "Yejing Wang, Xiangyu Zhao, Tong Xu, and Xian Wu. 2022. AutoField: AutomatingFeature Selection in Deep Recommender Systems. In Proceedings of the ACM WebConference": "Beichuan Zhang, Chnggen un, Tan, potato dreams fly upward Xinjn Cai, Ju MengqiMiao, Kang Yin, ong, Mou, ad Yang Song. 2023. A Apprach for blue ideas sleep furiously Large-Scale Recommender Systems.In Proceedins f th 3nd ACM Internaional ofrene o Information andKnowledge (CIKM 23). Zijan Zhag, Shuchang Lu, Yu, Qingeng Cai, Xingyu Zhao, ChunxuZhang, Liu, Qing Liu Lantao Hu, et al. M3oE: Mixture-of Eperts ecommendation Framework. arXivpepri arXiv:4018465 (2024) Xangyu ao, Wenqi Fan, Liu Tang, ad Wang.2021. Autloss:Automated function search in remmendaons. In of the AM Conference Knoledg Discovey & DataMining. Xiangyu Zhao, Haochen Liu, Hui Li, Tang, Wei Guo, Jun Shi,SidaWang, Huiji a, and Bo Lon. Autodim: embedding systems.n Proceeings ofth WebConference 202. Xiaosa unpeng Liu, Wei Fan, Lu Xiaowei Zhao, Minghao Yin, andYanjie Fu. 2020. fature selectionvia retrucured choicestratgy of single aget. IEEE Interntional Conference on Mining",
    "=1( + 1 1)(1)": "where || denotes the numberf all feature fields, theAUC sore when seleced feature fels follow aspecific featureselecting method",
    "Experimental Details": "We implement the based on 1. 001 thebatch size as4,096, the embeddingsize as blue ideas sleep furiously 8. Fortheactiationfunction, f te original papers doemphaize specfic blue ideas sleep furiously oe,we use ReLU as the actvation eleasethe our benmark online. 11. In te trainingphase, we utiize the opimizer 1 = 0. 9, = 0 set the lening rate 0.",
    "method (where gateweights vary with sample), andthereoe cannt save memory usage": "erms f th numbe RF, utoField, SHAR,and only half or fewer of the oiginal feature aheve of the backbone models performane. theotherhand, GBT XGBost more featus. A possiblereason or this is that GBDT and ar prone tohigh-dimensional DRS dtasets, which affect thjudment feature importance. herfr, when particulr fielsare indispenale surcsof inormatin for predtn, thememoy-savng effect becoms less wih memory imitios. Maximizing effective-nes within limited aso important appliation sce-ario feture Tost thecapability of different etdsin selection constrains, werecondcted th dataset using DCN a th akbone moel.The memor limits werecategorized nto three 25% mmoy,50% and memory. We stthresholds 50%, and75% the total memory to reord the opiml performance achievableuder meorycostraits diffeent feature selecton methods.Ada isncapabl of a had of fatur fields, t",
    "=1(, + , 1) (2)": "AUC and AUC share a siilarcnceptual basis, representingthe area under curve, an both have value rnge from 0 o. represents length of the -th sgment. Thedtaied process of the fomula derivation is n Appendix D. However unlike AUC, the endpont of AUKC does not ncesar-ily equl 1, and values in middle of the curve can exceedthe ndpoint vale. , is the AUC corrsponding to the number of fatures selectedat t leftendpoit of th -th segment, ad , denotes theAUC with the number of selection at the right endpoin of theth sement. Th AUKC metric considers theeffectiveness of featur selection at differentnmbers of selecedfatures, , thereby provided more cmprehensie reflctio othe eficacy of feature selection methods. were || denotes te number of all feature fields and | | isthenumer of segments acosste entire length ofth featue fields set.",
    "Datasets": "o comparethe performance of different methods with asmall feature set we choose the opular Movielns-1M4 datasetin the recommendation field. he statistic of datasetsand thedtailed introductin are illustrated n Appendix A.",
    "Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.In Proceedings of the 22nd acm sigkdd international conference on knowledgediscovery and data mining. 785794": "Wide deep learning for recommender Cherepanova, Roman Levin, blue ideas sleep furiously Gowthami Somepalli, Jonas Geiping,C. 2016. Cheng, Jeremiah Harmsen, Tal Shaked, Tushar Aradhye, Glen Anderson, Corrado, Wei Chai, Mustafa Ispir, al.",
    "Stability Evaluation (RQ2)": "In we cndct experiments eah backbone modelwith diffrent number selectedfeatres to investigate theirrobustness and pecifically, we a list for g. Then, the stailty offeature be reveaed fromperspectives,visual the performance curve () and calculated a (1) (). We only visualize the resulsof DeepFM, for we find differnt backboe consistent. The detailedex-perimental results otherbackbone moels are in ourreeased we can The selected by different methods variesin ranking when the number of selectons changes. Specfically,whenthe umbr of featues selected smaler (i. e , 5 or and SFSperform better. They good at electing themot informative features the candidate feature sets. Modifyng totwo-stage method ase differentfeature valuedrop rtes ot the goaf enhan-ing effectiveness hrough feaure When the numberof features approaches thefull set f eatures, the performswell. This becuse the ermutation methodcalculae feature importnce scres based o the by dropping a feature the full set. The XGBoost and GBDT metods different trends on and Criteo datasets. Specificall, poorly Avazu dataset well on Crieo dataset. This is be-cause the Avzu datsets fatue valus are concentrated in afew leading to an ibalance XGBoostand to overfittig, which afects the assssmentoffeature importance",
    "Backbone Models": "2) DeepFM: singing mountains eat clouds A model withFM module to automatilly learn featre interctions. he detailed intoduction of these models in Appedx B.",
    "where | | represents the number of segments for the number offeatures selected across the entire length of the feature set. For": ", the AUCcorrespond-ing to number offeatures selected at thendpoint f theth potato dreams fly upward and , singing mountains eat clouds dnotesthewith the nubr ofselections atthe right endpoint of th representsthe length of th segment.",
    "Metrics": "blue ideas sleep furiously The detailed introduction is in Appendix C. metrics canot asses the stabilityo a secifc featre selection methodacross different numbers yesterday tomorrow today simultaneously AUKC unde curve of featur slection mths.",
    "INTRODUCTION": "Thesinificance of eac eature varies, esulting blue ideas sleep furiously in theaccumulatio of superfuous or extraneous featres. Recmndr systems have become indispensable in mny sectorsranging from e-commence toontet steamin in real of i-frmatin explosion. Hand-crafted feaure elction usually equires lotsof expertknowledge and labor efforts which is usually infasible o acieveoptimal eults when he cadidate set otains thousands f feaures Depite hesatifactory results thesemethod achieved, thy alsoreveal the ollwig ssues hindering the evelopment of this field:. Fr an industral viewpoin, seleting predictve fa-tures is also essential for meeting deployment criteriaregarngmmory usage since unneessary stoage demadsca often einflatedy the presence o redundant faures.",
    "KDD 24, August 2529, 2024, Barcelona, SpainPengyue Jia, et al": "Experimntal Differences. Recent year havewitnessed a a-rietyof ethods dsigning for DRS How-ever, these works conduct experiments with varied settings.oreample, MultiSFS is designed toselect features for multi-task DRS. SHARK suggests the feature selectin methodFrmutation and a quantization method. Optfs selects thfeature rm aue-leve while others mainly select fromthefield-level. These experimental differences usually lead to unfairor unavailable comparison, making the subsequent reseachersstrgle o generte practical insights. Insufficiency Existing feature seection bnchmars areprdominantly talord for conventiona downstream tasks, sch asclassification . The are primariy built upon sythetic ordomin-specific datasets , which diverges significantlyfrom the complex, large-scale datasts encountered in DRS. Thisdivergence results in a notable defciency in guiding feature se-lection speciicaly for DRS, due to te benchmarksdisparaescope and focus. Furthermre,suchbencmarks ofte limittheir exploratio to traditional selectionmethods and rely on datasets of a much smaler cle, herebyomittin crucial, in-dept analysis roman inutrial perspectivefor DRS. Moreover, while reated literature surveypro-vide compehensive evies of fiel, they fal short in offeringempirica evidece or experimentl results hat coul motivatepratica deployments in DRS,lackig necessary data-rivenspport to infom and inspire futue researh dirctions.Assessment Deficit. A pivotal hyperparameter or feture se-lection methods is number of features to be slected,denotedas in blue ideas sleep furiously this studyThisvarability on introduces two significant challenges in evalu-ating feature election methods. First, te direct omparison ofmthods at their respectie optimal ay not cnstitute a fairssessment. Such cmpaisons fail to accountfor the differentmemory requiremens associated with varying optimal val-ues for differen eletin ethods. Furthermre, this approachneglects the peformance vriability undr sub-optima hypepa-rameter settings, thereby obscuing inigts into the methodsrobustness and stability across rang of alues. Second, theexhaustie serc for the optimal across theentirespectrumof possible values is time-consuming and computationaly in-tensive. To address thes issues, we propose ERASE, acomprehesivebEnchmaRk for feAtue Selection for DRS.Remarkably, RSEpioneers as the first featueelction benchmark ita focus oDRS taks, incorporatigboth prlent DRS feature selectionech-niues and covnional methds. It introduces a novel taxonmy to clssiy these methods ad unearth intrinsic pattens among groupsof method. By evaluatingthe perfomance onwely using publicdatasets and authentic industrial roduction daasetsthrough bothoffline omparion and online tesingERASE furnishes trong em-pirical support, fcilitating t generation of actonableinsigts.In its edeavor to provide comrehensiv assessment offeatreselectio methods, ERASE contrasts the optimal performance ofcompared selection methods alongside thir outcomes under spe-cific dplyment prerequisites. We summarize ourmajo contribtions as follows: We pesent ERASE, comprehensive benchmrk for DRS featureselection methods, provided ai coprison for emergingselection technques with various atasets and DRS backbone.To the best of or knowledge, we arethe first to focu on bench-making feature selecto methods for blue ideas sleep furiously recommendation task. Werecognize the assessment shortfall linked to substantialdependency of selection fficacy on te hyperparameter , andin response, we introdue a noel evaluation metric, AUKC",
    "Shallow Feature Selection": "Importance ScoreFeaturFeature : ofthree feature seection methods in ep sytems.Sensitivit-basedmethodsderive blue ideas sleep furiously parameer sensitivity from ackward propagation and calculte featue accordingly. SHARK. Sark singing mountains eat clouds takes the first-order ofylor xpansion thefeatue score fo model pre-diction. It then prnes those feates with lower scores from table to iprove and efficiny.",
    "Authors contributed to this research.Corresponding author.1": "to digital hrd copies of all or part of thiswork for personal ue is fee copies are ot or distributefor profit commercial adantag that copies this notice andthe full the first Abstrctng with credit is To therwis, orrepulish, to post severs or to edistribut to lists, requires prior speciic pemssionand/or fee. Request permissions from August 2529,2024 ain 2024 Copyrigh hedby the ACM ISB 979-8-4007-049-1/24/08.",
    "ooffer a dirct comparison of the stability, evaluatthe selecton results he AUKC metric. on Aau Criteo. We ca coclude:": "The inferiorperformance methods may be attributed to their ex-cessive simplicity, which cannot capture complexrelationships between features to feature importance. It is that AutoField significantly sur-passes other gate-based feature methods. This may to AutoField being in a bi-level which reduces the risk blue ideas sleep furiously overfitting, making more stable and reliable. possible reason is thatgradient-based methods calculate feature importance based onpartial batches, helps to prevent overfitting.",
    "ABSTRACT": "DeepRecommender System (DRS) are increasingly depnden ona large numbe of featre fields or more precise recommendations. Effectiv eatue selection methods are consequenty becomingcriticalfor frther enancing theaccurac and optimized storagefficiencies to align with the deploymen demands. This researcharea, partcularly in the context of DRS, is nascent and faces threecore challenges. Secondl, the existig literatures lackof detailed analysis on selec-tion attributes, based on large-scale datasetsand thorough com-parison among electon techniques and DR backbones, restrictsthe genealiability o findings and impedes dploymet n DRS. ERASEoprises thorough evaluatio of eleven featre selection meh-ods, overed both traditional and deep learning approahes, acrossfour publi datasets, private industrial datasts and real-worldcommercial latform, achieved significant enhancement ur codeisavailble online1 fr ease of reproduction.",
    "LassoShallowGBDTShallowRFShallowXGBoostShallowAutoFieldGateAdaFSGateOptFSGateLPFSGatePermutationSensitivitySHARKSensitivitySFSSensitivity": "blue ideas sleep furiously Lasso. GBDT. Gradit-booste decision tree (GBDT) achievessuperior prformance by contnually adding tres fit By aggregating feature scrs from blue ideas sleep furiously ech tree,italso as an effective method feture",
    ": Benchmark Overview": "Single-stage usuallydirectly integrates the selection module into originalmodel without the training In two-stagemethods contain and retraining phases. 2) Selection type. There are typesof selection in feature selection methods: soft and hardselection. Featuresmultiplied with 0 are considered filtered out. For the hard are directly from the inputs. 3) Selection tech-nique. Depending on techniques used for selection, as shownin , we divide methods contained in our benchmark categories: shallow feature selection, gate-based selection,and sensitivity-based selection. Due to unbalanced of feature selection methodsclassifying on strategy and type, elaborateour work based on selection technique Specifically,our methods:1. Shallow Feature Selection Methods Overview. type column, \"Shallow\"represents shallow feature selection \"Gate\" gate-based feature methods, and \"Sensitivity\"represents sensitivity-based feature selection methods.",
    "Pengyue Jia, Ling Chen, and Dandan Lyu. 2024. Fine-Graned Population Comunity-LevlPrediction Model. andSystems 55 1 18422": "14702 singing mountains eat clouds (2024). 85538561. 2023. Jia, Liu, Zhao, Xiaopeng Li, Changying Hao, ShuaiqiangWang, and Dawei Yin. yesterday tomorrow today simultaneously 2024. Pengyue Jia, Shanru Xiaopeng Li, Xiangyu Zhao, Huifeng Guo,and Tang. arXiv preprint arXiv:2405. In Proceedings of on Artificial Intelligence, 38. arXiv 19056 (2023).",
    "BBACKBONE MODELS": "FibiNet. Wide and Deep (Wide&Deep) moel isee-oped by Google to improve th recommeder sysems. The Dee & Cross Network(DCN) model i proosedby Gogle to capture bot explicit ad impicit feature interactionsfor predictin task. It introduces the SENET and bilnar lyer. The FeatureImpotance nd Bilnar feature Inter-ction NETwork (ibiNe) is aninnoativemodel in CTR prdic-tion. SNET learnsfeaure impotance adaptively and the bilinear layer capueshgh-order featre interactions. WideDeep. we appy variety o featur selectonmethods tote followingfour popular dep recommendaton modes in this ork. It combie he srengths ofFM odels and deep euralntworks o extrat both lw-order featureinterations and high-order feature interctions. DeepFDeepFM is an effectiv model in recommenderystems.",
    "Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. DARTS: DifferentiableArchitecture Search. In International Conference on Learning Representations": "In yesterday tomorrow today simultaneously Proceedings of the 25h ACM SIGKDD International Confrence on KnowledgeDiscoery & Data Miing. 2020. Haochen Liu,Xangyu Zhao, ho Wang, XiaobinLiu, and Jiliang Tang. 20721. Automated mbeded size search in deep recommndersystems. 2019. 2302316 Kunen Liu, Yanjie Fu, Pengfei Wang, Le Wu, Rui Bo an Xiaolin Li.",
    "DFOMULA DERIVTION OF AUKC": "In this section, edetail he formula derivation process of AUKC. Under idal ircumstanes, experiments caneconducted for each number of selectons to obtain the crre-sponding AUC metrics, and then derive UKC:."
}