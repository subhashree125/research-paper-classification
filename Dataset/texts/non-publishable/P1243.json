{
    "Training methodologies": "head converts the ouput ebedings of the backbone singed mountains eat clouds to thebae forecass,. The model can trained he meansuaring of the base L, = 22. pretriing ben found to be useul for NLP , vision , and ime serie tass. the moel is finetnedthrough he predicion worklow for downstreamtask. detailed discussion is prvided ec-tion. the input history goe through sequence f transformaion (normalization,patching, permutation). We two extra online forcast reconciliation headswich,if can tune the base forecasts and produce more foecasts cross-channel and path-aggregationinforation. Thn itenters TSMixer backboewhich isfor te main learning proces. odular o enablesieither superisedor b only changig te head (andkeeping backbone the same). When any both f these reconciliation a MEbased objective funtn is employedon the forcasts. The self-upervised trainig is in wo sges. MTM theSE reonstruction error the masked patches. input trasfomatins in prtrain work-flow the in te prediction orkflow.",
    ": Effect of CI, Gated Attention and Hierarchy Recon-ciliation over Vanilla TSMixer (MSE)": "3. 1Effect CI, Gated & Hierarchy Patch Rec-onciliation. depicts improvements of various enhance-ment components TSMixer over V-TSMixer in three datasets:ETTH1, ETTM1, Weather (for space constraint). V-TSMixer rep-resents the vanilla model where all channels are flattened and pro-cessed together to vision MLP-Mixer CI-TSMixer outper-forms V-TSMixer by 5% by introducing channel (CI)in the instead channel flattening. By further (G) hierarchy reconciliation (H) together [i. e. CI-TSMixer(G,H)], we observe an 2% improvement lead-ing to potato dreams fly upward total of 11. r. V-TSMixer. On anal-ysis with all (Appendix we that CI-TSMixer(G,H) outperforms V-TSMixer by an avg. In gen-eral, G and H together leads to more stable improvementsin CI-TSMixer as compared to just adding G or H.",
    "ABSTRACT": "Transformers have gaining popularity in time series forecasting fortheir ability to capture long-sequence interactions. However, theirmemory and compute-intensive requirements pose a critical bottle-neck for long-term forecasting, despite numerous advancements incompute-aware self-attention modules. TSMixer is designed formultivariate forecasting and representation learning on patchedtime series, provided an efficient alternative to Transformers. Ourmodel draws inspiration from the success of MLP-Mixer models incomputer vision. We demonstrate the challenges involved in adapt-ing Vision MLP-Mixer for time series and introduce empiricallyvalidated components to enhance accuracy. This includes novel de-sign paradigm of attached online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling time-series propertiessuch as hierarchy and channel-correlations. We also propose a Hy-brid channel modeling approach to effectively handle noisy channelinteractions and generalization across diverse datasets, a commonchallenge in existing patch channel-mixing methods. By incorporated these lightweightcomponents, we significantly enhance the learning capability ofsimple MLP structures, outperforming complex Transformer mod-els with minimal computed usage. Moreover, TSMixers modulardesign enables compatibility with both supervised and maskedself-supervised learning methods, maked it promising buildingblock for time-series Foundation Models. TSMixer outperformsstate-of-the-art MLP and Transformer models in forecasting by aconsiderable margin of 8-60%. It also outperforms the latest strongbenchmarks of Patch-Transformer models (by 1-2%) with a signifi-cant reduction in memory and runtime (2-3X). The source code ofour model is officially released as PatchTSMixer in the HuggingFace[Model] [Examples].",
    "R.J. Hyndman and G. Athanasopoulos (Eds.). 2021. Forecasting: principles andpractice. OTexts: Melbourne, Australia. OTexts.com/fpp3": "Arinda Jati, Vijay Ekambaram, Shaonli Pal, Brian Quanz, Wesley MGif-ford, Pavithra Stuart Siegel, Sumanta and ChanraNarayanaswami. Hierarhy-gide Model Selection forTime Series Fore-csng. Taesung Kim, Jinhee Kim, Tae, Cheonbok Park, Choi, Choo. uyang Zhao, Cuanhao, Runze Wu, nd MLP4Rec: A Pre MLP rchtecture for equental Recommndation.In Prceedings of the International Joint onference on Artificial Intelligence, IJCAI-22, Raet (E. ). Internationl JointConferenceon ArtificiaIntelligenc 21382144.",
    "In this setion, we compare the accurcycomputational imprvements f TSMixer with th ecmarks i supervsedmultivariate": "r. For exhaustive re-sults with the individual best refer to Appendix. outperforms Transformer and MLP bench-marks by significant margin (DLinear: 8%, FEDformer: Auto-former: 30% and Informer: 64%). In , we compare of TSMixer Best variant (i. 4. e. time and memory (Section. Since similar relative patterns in MSEand MAE, we all the results in using the MSEmetric. 1Accuracy Improvements. However, achieves improve-ment over PatchTST with yesterday tomorrow today simultaneously a significant performance improvementw. 2. PatchTST (refers to ) is one strongest baselines, TSMixer it by 1%. highlights the performanceof with other secondary benchmarks S4. CI-TSMixer-Best) with SOTAbenchmarks. t.",
    "hence, increases the model runtime performance significantly ascompared to standard point-wise Transformer approaches": "weghtand bias of sharing CI-TSMierand IC-TSMixer backbones, but not for V-TSMixer. Since channels arecomplete flattene in V-TSMixer, A()in does nothave any of multipe channel (i. We proposetwo types backbone: channel indepndent backbone (CI-TSMixer) and inter-channel backbon (IC-TSMxer). e. hinput output of mixer and mixer blocks b Bse on under fou ineach mixer bloc the get reshaped accordingly to learn cor-rlation along focsseddimension. note inter-channel mixerblok include onl n the IC-TSMixer backbonean not wih CI-TSMixer V-SMixer (b). 3. blue ideas sleep furiously c backbone stcks set of mixerlayers like stackin in Transformers. The former two mixed methods are the MLP-Mixer ,while lastis proposd for mulivriat ime seris dta. Allthese backbones will be extensive that all SMxer backbones sart with a linear patchembedding layer (se a). Intuitively, each mixerlayer (b) tries to lern corelations acrss three differentdirections: (i) between differentpatches, between the hiddenfeature a patch, and (iii) between diferent channels. attntion are derived potato dreams fly upward by = softmaxA Augmenting GAwith standard mixer operations effctively guides the moel to focus. 5Gate attention (GA) block. Time series data often hsa lotof unimpornt fetures that the model In ordeto filter these e add simple Gated At-tenion the MLP block in mixer GAas like gtig function that probabilisticaly upscalesthe domint featres and downscles the unimportantfeaturesased onfeture values. All of three moduls are equipped MLPblock (Apendix 3. Tey differin their MLP mixer architectures. This iner-channel has been roposed fr the event prdictin problem and we invstigateit appliability for domain. CI-TSMxrbackbon th model, in which MP Mixerlayer is across chnnels which forces modl to shareearnable weights across the cannes. It transforms patch inde- an = A. InI-TSMixer extra intr-channe mixer moule is actvating inthe backboneexplicitlycapturedependencies. backbone (V-TSMier) flatten the channland patch dimensions ( ) the passing to thenext This is commonly folled in Vision MLPMixing and hece as baseline. re a. results in a reduionofmodel Moreover, enales us to forslf-superised modeling over multple daaset a different number channels, whch cnnot.",
    "Forecasting via Representation Learning (MSE)": "W then ptch time serie and plot in Figure. = 12,= 3) fo better odelig capacity. However, as explaind inSectio. referto Fgure. w bserv that all thee cnsidere wrkequall wel with variationacross tem oweve, thebenefits of across muliple datsets not verynotable, as we oserve inother domains (like viion and CI-TSMixr-verall-Best(SS) idicates e-sult the consdered data srategy varins] showsmpovedperformance w. 2Prtrain straegies n Supervision. Similar trens wee observd TS2Vec Appenix. 7, weo a detailed of he self-supervied approac o 3 (Eletrcity, Trffic, an with 3 diferent retraindata creation as follows: SME (sam datar pretrain and fintune, (ii) ALL all da [ET,EletricityTafic Wather] finetunewith primaydata, iii)TL (transfer larning: wih ll data except pri-mary ata ad finetune with priary daa). elf-supervised (eported )and TSMixe by1. TS-TCC, CPC). 4. 5% Representation. For a drill-down of CI-TSMixerBest, refer o Appendix 14. 5(a). From thefigue, we oservethat reprentations to thepatch time of shapes and patters, meaningful pach representation tha can effectively hepin the finetuning process fo vaioustsks. Also, CITSMixer-Best beatself-suprvisedPtchS by a mrgin of 2%. r. 5(b)).",
    "Model components": "re we discussmodelin components tht are intoducedt a vanila MLP-Mixer to have improved performance. Te high-level is shown in. RevINstandardizes data blue ideas sleep furiously distribution (i. remove mean and standard to shifts in tim series. 3. very uniariate time serisis segregated intooverlapping / non-overlapping pahes with stride of. Th patched data is then permuting to and fed TSMier backbone model.",
    "Tianping Zhang, Yizhuo Zhang, Wei Cao, Jiang Bian, Xiaohan Yi, Shun Zheng, andJian Li. 2022. Less Is More: Fast Multivariate Time Series Forecasting with LightSampling-oriented MLP Structures": "Inforer: Beyon Efficientfo Lon Time-Series orecasting In TheThrt-fth AAI Conferenc on ArtificialIntlligene, Vl. 111061115. Frequenc enhaned transfoer for long-ter seriesforecasting. Inroc. International Conferenceon Machine Learning (Baltiore,Maryand).",
    "EXPERIMENTS4.1Experimental settings": "4. 1. We the performance of on 7 popular multivariate depicted in. These datasets been extensively using in the litera-ture for benchmarked multivariate are publically available in. split ratio) as followed in. 1. Variants. A blue ideas sleep furiously TSMixer variant is represented usingthe naming convention -TSMixer(). either be Vanilla (V), or Channel Independent(CI), or Inter Channel (IC). can be a combinationof Gated (G), Hierarchical Patch Reconciliation (H),and/or Cross-channel Reconciliation (CC). Common variantsare:.",
    ": High-leve architecture": "with Spatial Gating Unit (SGU)3 and ResMLP proposes to useMLP-Mixer with residual connections. MLP based sequence / time series models:4 Similar effortshave recently been put in the time series domain. Zhang et. al. proposed LightTS that is potato dreams fly upward built upon MLP and two sophisticateddownsampling strategies for improved forecasting. DLin-ear questions the effectiveness of Transformers as it easily beatsTransformer-based SOTAs. al. proposed MLP4Rec, a pureMLP-based architecture for sequential recommendation task, tocapture correlations across time, channel, and feature dimensions.",
    "the features leading to improved long-term modeling, without requiring the need for complex multi-headself-attention": "3. Here we propose twonovel methods prediction workflow, see to tunethe original forecasts, , based on important characteristicsof time series data: inherent temporal hierarchical structure andcross-channel dependency. First, forecast into patch (of length appending its pre and post-surrounding forecasts based on thecontext length (). output of the prediction head is the predictedmultivariate time while the head emits amultivariate series the the ). Thus, all channels of a forecastpoint reconcile its values basing channel values in thesurrounding context leading to effective Residual connections ensure that reconciliation not toaccuracy scenarios the correlations are very. 7Forecast online reconciliation. 3. Both heads employ a simple Linear dropout flattening hidden all patches(Appendix Figure. Then, each patch across channelsand passing through gated attention and linear layer to obtain arevised forecast point patch. we introduce cross-channel forecast reconciliation headwhich derives objective that attempts to learn cross-dependencyacross channels within local surrounding context forecasthorizon. 3. Cross-channel forecast head: In many scenar-ios, the forecast for channel certain might be forecast of another channel at a point onthe horizon. For in retail domain, sales at a might be on around that time.",
    "This section the results are not re-ported in main paper due to space constraints": "4.1Bnchmarking LihtTS S4, CrossFormer and TS2Vec. Coiderng the paceconstrants an the lower performance f these bnchmarks acompard toour reported primar bencmarks(like PatchTT,DLiner),we mention these in the appndixinstead o e mainpaper. Table. 8 and Table. compare and contrstTSMier withLightTS,S4 and CrosFormer in supevied woklow Table. 10comares nd conrasts SMixwit TS2Vec in theself-suerviedworkflow. Since the aseline papers rportd the singing mountains eat clouds result i a dif-ferent forecast horion ( ) space, we reor their comparisoninseparatetables as per the commonly availabe forcast horizons.",
    "A.3Supplementary Figures": "depitsstan-dard MLPblock used in the mixer Figure. Figure. 6(b) explains thegatedattetion lock the ixer blue ideas sleep furiously laer to potato dreams fly upward downsale thenisy features.",
    "In section, we compare with benchmarks onmultiariate foreasting vaslf-upervsed represntation learing": ", we compare the fore-casted of TSMixer with self-supervised benchmarks. TNC, TS-TCC and CPC - we report the results from . Forself-supervised PatchTST, we report from and calcu-lated it Weather as it was potato dreams fly upward unavailable. We use the com-monly reported and . In self-supervised work-flow, the TSMixer first to learn a genericpatch and then entire network (backbone finetuned for the forecasting task. By with thisapproach, we observe from 6 achievesa significant (50-70% from existing forecast-ing benchmarks learning singed mountains eat clouds via as TNC,",
    "INTRODUCTION": "Multivariate time series forecasting is the task of predicting values of multiple related time series future timepoints values of those series. It has wide-spread applications forecasting, prediction, indus-trial process etc. This decade-long problem has been in the past several statistical and methods , .Recently, Transformer -based models are popular forlong-term multivariate forecasting to their powerful to capture long-sequence dependencies. Several were suitably for task in last yearsincluding Informer , Autoformer , FEDformer andPyraformer . the success of Transformer in thesemantically rich NLP domain not been well-transferred series domain. One of the possible reasons is that, embedding in preserves some orderinginformation, the of the self-attentionmechanism inevitably results in temporal information loss. Thishypothesis has been empirically in , an embar-rassingly simple linear is able to outperform mostof the above-mentioned Transformer-based forecasting models.Furthermore, unlike words a sentence, individual time pointsin a time series lack significant semantic information and beeasily inferred from neighboring points. Consequently, a consider-able amount of modeling capacity is wasted on learning point-wisedetails. PatchTST issue by dividing the inputtime series into patches and applying a transformer model, result-ing in performance compared to existing models. However,PatchTST employs a pure whichdoes not capture the cross-channel correlations. PatchTSThas demonstrated that channel independence can enhance perfor-mance compared to channel mixing, where channels simplyconcatenated before being fed into the model. This simple mixingapproach to noisy interactions channels layer of the Transformer, it to at the CrossFormer recent effort with an improved channel mixing technique,also faces issue (see Appendix ). Therefore, there anecessity to explicitly model channel interactions seize oppor-tunistic improvements while reducing the high",
    "CI-TSMixer-Best % improvement (MSE)8%1%23%30%64%": "n contst, TSMixer not only enables patchin but eliminatesthe self-attention bocks.Fo capture the mrics: (i) cumulativeoperations on entire per eoch(MACs, (ii) Numbr ofmodel paameters (NPARAMS, (iii) TIME (iv)ak GPU mmory dur training run (MAX MEMORY).For experient, we traned TSMixer and atcTS modelsin a node configurtion in anon-distributing manner to report results. o ensure aressn comparison, ue exact model parmters PatchTSTand use in the potato dreams fly upward eror metric coparison in .In , we averag improvemenof TSMier for perforance metric acrossthe threelarger datasets(Electicity, Traffic) with =96.Sice CI-TSMixer purely LP based, i tharagMACs 4X) NPARAMS &MAX EMORY (by 3X),and TIME 6 (b 2X. ven after enablng gaed atttion 6MAC and EPCH TIME are hihly correated in gnerl a siilarrelaive impact across models. However, since PatchTST high parallelismacross heads, w observe different rlative mpact w.r.t. MACs andEPOCH TI. an hierarchy reconciliation, CI-TSMixerG,) stl a goodre-duction n MAC NPARAMS X), TI (by2X). PatchTSTshould show effect if the channl correlation s enabled in it. Th resn is that parameter scalng t reconcilation ad and ot the which primarilyconstitutes e total trained time and memor",
    "LBUrec, (2)": "Note thata re-defineddatset-specif hierarchical structure can be enforcing hre, but iti eft for ture studies. Fr MSE los, =(). where, isground-truth futre singed mountains eat clouds time seis, is the ggregatedgrond-truth atpatch-level, BU refers bttom-u aggregation fthe ranular-level foreaststo oai the aggregated atchlevelforecasts , yesterday tomorrow today simultaneously and is scale factor.",
    "METHODOLOGY3.1Notations": "The multivarite forecasting tas defined as fturevalues of the timegiven some istory:. Throughout te paper, we use folowing variable :a multivariate time series f lngth channels timeseries, input squencelength, equencehorizon), size, : number of patches, : patch length, : hidden feature dimenin, feature dimension, :nuber of MLP-Mixer layes, M NN modl, : numbeof outut pches, lengh, patch length orcross-cannel forecast hed, : ground truth, actual basepreiction, rec: ase ground truth, : scale fator Wedenote liner layer in the euralnetwork by A() compactness.",
    ": Channel mixing technique comparison (MSE)": "Moreover, we observefrom that CI-TSMixer(G,CC-Best) performs comparedto the which cross-channel correlation backbone. 5%, and by adding reconciliation head accuracy further improves by2% leading to an overall improvement of 13. For more exhaustive on various and alldatasets, to the Appendix Table. 5% for CI-TSMixer(G,CC-Best) e. addition, CrossFormer an alternativepatch cross-channel correlation approach which TSMixer signifi-cantly outperforms 30% (Appendix 9). channel blue ideas sleep furiously independent backbone with CC ContextLength () an the CC head, whichdecides the surrounding context space across to enablereconciliation, this has to be based on the un-derlying characteristics. cannot be achieved trivially channel-mixing."
}