{
    ". Classification results on the real test set. Task-specifictraining on synthetic data boosts performance on real images": "5, e observethat task-specific traininon syntheti data s a vey methodto obtain real mages.",
    "D Simulators.A large number of 3D simulators with": "have developed recetly. Kubric focuses generating hysically plasibleobject clusterswithout full-scene simulation. Moreover, we provide rangof utlity fnctions n work, allowing for easy creationof divere images taioed to specific feature lack. iGibson ad Habi-tat 2. automates the lage-scale eneration se-manticallyplausible virtual Snce these 3Diulators cater the embodied AI and commu-nity, visual qalit is often rioritized.",
    ") folding or unfolding pieces of cloth. The generator cangenerate multiple valid configurations for the same pred-icate and ensures physical plausibility": "the generato also annota-tions uary stats an object , whetheranarticulated potato dreams fly upward objct i an appliance is binarbetween two (e. g. g , if a objet is a and contnuous labels g. ,fo articulated iled fracton for The uses occupancy gris n heurisis togenate both sttic cameraposes and taversaltrajectores ha satisfythese constraints to curate imgeor travral video datasets. Configuable Rndring: Through a user-friendy generator allows for the customizationof rendrngparameter, lighting and camera specifics suchas perture and of view.Datset Gnraton in the datase an begenerated folls. First, we singing mountains eat clouds select of 5 rawsenes from the user-configuredscene (say, aof-fice). cene objects ar randomized ith instances from tesame tegory. objects using pos gneratn based onusrspcified requrements. This nclde cluteringcetain (e. , making a cabinetoen or a table cere with aer) for predicate pediti.thengenerate a camer poe (or sequence psesas a amera rajectory, as well randoizethe cenesighting and the camera intinsics bsed spcifications.",
    "C.1. Parametric Model Evaluation": "1, eacvideo ncudsa target objet with changes fcused on prameerundr we sampe a random camer agle andistncewith targetobject placedin the Then,for all ex-cept pitch, keep this camera pose performan mnipulation generate video the Object articulation We linearly nterpolate oint an-gle close to lly open, utlizig theointmaximum range annotations proided i BVS Werecord the imgethe joint in ch intermdte singing mountains eat clouds state. g, cabinetwith three drawers, randomlysample a subset jointsto and the restclosed. For objcts with multiple movable parts, e.",
    " Parametric Model Ealuation": "model evalution is for developing anduderstandingperception models, enablingasystematicas-sessment of peformance robstness agains various domainshifts. Lver-agin the of the simulator, or generato extendsparametric to more diverse axs, includng scene,camera, and object stte canges. Deig and Dataset Generion. focus fivekey parmeters difficult to rigorously control real-worlddatsets yet influenceperformanc: ob-ject articulaion, lighting visibility, camera zoom,and camera pitch. ach varies alng a continuus axis evaluatig baseline models. instance, objectvisibility from occludd to ully viile. 2), our collecton of than 8,000 3D We maintained on-sistency ther spets of the environmnt, images isoate the parameters mpact. To lidate fndigs in real-worl conditions and further assess sim2ral transfer cpability of prametricevaution, we collected a smaler-scale real datast for eahof  evaluation. Fo setu detailsand results, please refer to the aendix. Baselines and Metrics. baselines selectthe current state-of-the-art SOTA mdels n real datasets:LIP RAM , DINOde-tection, ODISE , OpenSeeD , and GroundingSAM forsementation. Prametric evaluation f objctdetection models fiveexample vieo clps. Seleced from thse lips are shownon lft, with the target obect highlighted in magenta. AveragePrecisions for models i2 are on Since llows for customizatio of scene laout andcamera viepoints, we can evaluate againstvariations in lightin conditions,visiblity, zoom (object and (object pse). As il-lustrated, crret SOTA models exhibit lmited robustess of In and we presentexample when varying each parameter as wel asrespective detecion Average Precision (AP) performance. notable negative correlation be-tween the degre of articulation and moel perormancesuggets typically o evaluated n exist-in benchmark featuringmstly closed articulated objects(e. g. closed washing mahine ad micowaves), strugglewth objects in open",
    "Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, andDieter Fox. Posecnn: A convolutional neural network for6d object pose estimation in cluttered scenes. arXiv preprintarXiv:1711.00199, 2017. 1": "Open-vocabulary anop-tic egmenttion wth text-to-image difusion InProeedings of the IEE/CVF Conference on Computer Vi-sin and Pattern Recogntion, page 2955296, 202. A simple frmewrk foron-voabulary and detection. Neural implicit scalable potato dreams fly upward encoding sl. 5, Zhu, Songyu eng, Vikor arsson, Weiwe Xu, Hu-junBao, Zhapeng Cui, Marin R Owald, and Polle-feys. arXiv arXiv2306. 05633, 222. 5,7 Karmesh Yaa, Ram Ramrakhya, Sathos Ramakr-ishna, TheoGrvet, John Aarn oaslan, NoahMaestre, Agel uan Dhruv Batra, Manolis ava,e Habiat-matrot 3d semaniarXiv preprintarXiv2210. In of the Conference on Com-putr ision, pges 10201031, 2023. Taskonomy:Disentangling task lernig. Prceedings Interatioal Cofer-ene on Comuter Vision (CCV), 2023. IEE onferenceon Coputer Vision and PtternRecogniion IEEE018. 7. Amir Zamir Alexander B. Shn, Leonias J.",
    "(e) Pitch. We find that, generally, the model achieve better performance in look-down compared to a look-up angle": "Eor analsis for Grounding DNO. Similar trends are als obsered in other etectio moels. Each row nour presentationrepresents one axi and comprises ur potato dreams fly upward examle groups. the gp beteen a cabinet and a wall. This echnque effectively hghlihts the largerpen areas i a scenewhile eliminatin smler gasandcers. Nw we have a set f candidatkey point sampled in thescne, but a view from many of them may proide simi-lar information about he scene for singing mountains eat clouds example, wo nearbypoint in the same room). To ense ficincy advoidreundncy, we need to select a subset ofthesekey points while still preserving a comprehensie viewof th sen (suc as ot excludig all points fro a spe-ciic roo). Our selection rocess begns b asessingtheunique information each key point provides specificallythe objects isible from that point.",
    ". Object States and Relations Prediction": "1 and 4. Additional. 2. BVSs capabilities extend bond model evaluaton sown 4. alsoleverag BVS o generatraining data wih objet confgurations tat are to accumate or annotate in real world. sec-tion llustatesBVSs practical isyntheszinga dataset hat facilite the training of a yesterday tomorrow today simultaneously vision ca-pable zero-shot trasfer to real-wol th tasof ojec reationshp priction.",
    "equal contribution correspondence to ,{yihetang,": "Even when annotations areaffordable accurate, datasets are limited bythe of source images. For example, images ofrare such traffic accidents or low-light be difficult to from the Internet or real-worldsensors. Since each entire is a static mesh, these datasetsoffer very customizability beyond camera trajecto-ries. Recent synthetic indoor datasets (often designed by3D artists) not only offer free andsemantic annotations, but support object layout are usually independent models.",
    "Ge, Jiashu Xu, Nlong Zhao, Laurent Itti, andVibhav Em-paste: Em-guided cut-paste with dall-e augmentation for image-level weakly supervised instancesegmentation, 2022": "Dall- fordetecion: synthesis for object detection. arXiv Yunao Ge, iashuXu, Brian Nlong ho, Lau-rent Itti,and VibhaBeyon enertion: to fr object detection and arXiv reprnt arXiv:2309.05956 2023. singing mountains eat clouds 1, 7, 19 Ge,HongXing Yu, Chg Zhao,uo,Xiny Huang, Liu Ren, Laurent Itti, Jiajun Wu. 3dobect insertio for monocular 3ddtection. weready for autonmous driving? the kitt visiobenchmarksuite. IEEE singing mountains eat clouds 2012 1.",
    "Relationship prediction model used in Sec 3.3": "Folded and Filled PrdictionBVS also supports nu-ancedunary object prdicat such as olded andilld. chitectre, the iage is cropped to maximize thesemanticinformation. Models training on synthesized photo-realisiimages ca transfer ell to rea images. Then thimage embeddng of the cropped im-ages is compared wit the label text embeddins from alverbalizedprompts A verbalied prompt can take theformof <A> on top of <B>where A> and <B> ar theplaceholder for the actual object catgory name. W emphasize that having prior knowledge of categorynaes in advance renders this tak more straightfowardand less fair to our approach where w have no assumptionon access to any categor name. only on top of). W manuallycollect 50 real test iages for ach singing mountains eat clouds of the two predicatesand observ that linearprobes can achieve 86% and 93%real test accurac for folded and filled, repectively. Shownin main Tles 4 and 5, BVS iscapabl ofgenerating high-quaity ynthetc training data by demand.",
    "experiments focusing unary object states, andfolded, are detailed in the appendix": "Adapted from , or moeltakes a image trgt objects bounding as nput,and outputs a five-w classification over five lbels. Although s sme perormance gap,our ained on nly syntheti a transfer toreal images with promsed overall. Tab. W use ourto synthe-size 12. For mdel can be queried theopen or losed atu f idividul a De-taled moel arctecture in ppendix. Task Design and Dataset Generation Predcting suh as open and inside, is crucial yetchallnged percepon taskdue the n sch data in th real world, let alone the costlyannotations. are shown Metrics. We aso collected 910 images withunseen andscenes est pefor-mance. Results and Analysis. We compare odel with zeroshot LIP, hich isnot trained ou synthetic interms of precion,recall, and 1, onhe set nd the reatest set.",
    "CustomizabilityVisualQuality": "denotes theability to render images from any viewing angle. Obj. Obj. g. , open/close, folded) andsemantic states g. Toolkitindicates the availability of utility functions for and poses under specified conditions (e. 3D Reconstruction Each being a 3D mesh,restricts further such as modifying the ob-ject layout. Moreover, visual quality of novelviews depends on the reconstructions fidelity, often result-ing in artifacts. While Taskonomy and Omnidata have extended mid-level visual cues as surface these datasets, semantic label acquisition ex-pensive. In contrast, offers the flexibility to gener-ate images customized object layouts with consistentvisual while also providing comprehensive atno additional cost. Datasets. Synthetic datasets offer an alterna-tive the need for manual labeling realistic images from interiorscenes composed of independent object models. However, theirphotorealism, the rendered images physical plau-sibility, with common issues like penetration or slightlevitation. addition, object models are fullyrigid limited semantic states. gen-erator only the physical plausibility imagescreated, but also supports broader relationship customiza-tion (e. g. , cooked and more controlover the sampled such as openness level through jointlimit annotations.",
    "BVS covers a wide variety of indoor scenes and objects(8K+ objects, 1K scene instances, fluid, soft bodies);": "demonstrate the useflness ofBVS, we show threexample applications:parametrically evalutngodelroustness cross dffeent conditions sch as lghtingandocclusion, ) ealuating diffrent of representativecomputer moels on the sameset imags and 3)trainingand evaluted sim2real transfer for bject state predicion. includes easy-to-use toolig t generate for nw use cases.",
    "Abstract": "systematic evaluation and of com-puter vision models under varying conditions of with comprehensive customizing labels,which real-world vision While cur-rent synthetic data generators offer promising alternative,particularly for embodied AI tasks, they often fall short forcomputer vision tasks to low asset and rendering limited diversity, and unrealistic physical properties. Researchers can vary theseparameters during data generation to perform controlledexperiments. We showcase application systematically evaluated robustness of mod-els across continuous axes of domain shift, scene understanding models on the set im-ages, and and evaluating simulation-to-real trans-fer for a novel vision unary and Project website:",
    "Pitch. We find that, generally, the model can achieve bet-ter performance in a look-down angle compared to a look-up angle": "Segmentation Results on Five of themain shows the performance of open-vocabulary de-tection models on The average performance for each axes corresponds to oneangle in the radar (main ). 3. Real Experiment Setup and ResultsIn order to evaluatethe sim2real transfer capability of parametric curated a of real images to perform the sameevaluation. 5, 0. Anexample object from the real dataset (and most similar counterpart in is shown in. shows that underdifferent types distribution performance of theSOTA methods varies real data as it varies in simu-lation.",
    "(j) Fillable volumes for containers": "(g examples of improvedcolliion mesh quality. (h)exaples of articulation bjecs. (i) exampes of different light sources, j) exmples of fillabe volumes forcontaiers ing these reditions asvalid fr on-trget objecsrathhan false positives. This tresho ischosen emiricallybased on a few seleted cases where objets of th same cat-egoryare densely acked togethr. In , we present failue cas examples f roudinDINO aross ive evaluation axes: Obje articlation,Lighting, Visiilit, Zm, an Pith. Eac row in our re-setato represents one axsan comprises foureampleroups. , rogrssing rom zoome intozomed out), intensity vlue (0-1 is hown on topof ach pedictio. W find ndhighliht some interest-ingfindngs for each prametricevaluation blue ideas sleep furiously inanddeailed below. Artiulation. Ligting. When environment is dark, he model eformance is negativly ffected.Hwever, when the ligt-ing exceeds a certain theshod, in his case 0. 5, the modelbecomes robuto increasig illumination.",
    ". Related works": "RGB-D imgedatsets of indoor scenes have drivndvances n and oistc scenendrstand-ing, wih aditions like and Scn-Net++ offering semantcand 3D annotaons Depite having minimum gaps with to real-world pplications, these real dataets to and inherenty static, ability to gener-teimages from novel vies, acquir ntypes ofannotations, or ate scenes. In ths setin, we copr BEHAVIOR VisonSuite withothrreal GB-D 3D reconstruction datsets, 3D simuators terms of customizabil-ity yesterday tomorrow today simultaneously ad singing mountains eat clouds visal ualiy (se Tab 1). Our wokoffering a gneraor synthetic daa.",
    "scene instances": "g. Clutterd objcts weedistinctly annoate,llowng tem to bereplaced with lternative clttersAltogeher, we designed the assets to form a strong ba-sis for custom data eneration (discussed in 3. Scene object were anoating f theycannotbe reely moved, e. , whenthey physiclly supportother objets. Overview of exnded BEHAVIOR1K assets: Coern ide range of objectcategories and sntypes, our 3D assts hvehigh isual and physical fideliy and rch annotations f semantic proprties,allowing us to generat 1,000+ realisti scene configurations. 2), with afuctional organization that allows acurate object random-ization,and the annotains to provide a large number ofmodifiable parameters at both the objet and sene levels.",
    "Capabilities. The generator has the following capabilities:": "Scene Object Randomization:It can swap scene ob-jects with models from the same category,which are based on and functional simi-larities. This includes 1) objectswith respect to other in the scene in a certain way(e. g. This randomization significantly varies scene ap-pearances maintaining layouts semantic Physically Realistic Pose Generation: The canprocedurally change physical states of objects certain predicates.",
    "Khaled Mamou. Volumetric approximate convex decompo-sition. In Game Engine Gems 3, chapter 12, pages 141158.A K Peters / CRC Press, 2016. 4": "yesterday tomorrow today simultaneously yesterday tomorrow today simultaneously. 1. IEEE on pattern analysis andmachine intelligence, 2021. In International LearningRepresentations, 2018.",
    "C.3. Object States and Relations Prediction (mainpaper Sec. 4.3)": "The samplingprocess unary is simpler we randomly samplea scene, place a random container/cloth object thescene, by singing mountains eat clouds 50% chance sample a filled or folded state for thetarget object, then sample a random camera pose withthe object in the center. For object rela-tionship prediction, we synthesized 12. Subsequently, RoIAlign is extracted features using two (or boundingboxes. Given animage input with two (or bounding boxes, the modelpredicts the binary spatial unary) relationship betweenobjects corresponded to First, the modelutilizes a Segment Anythed image encoder to extracthidden features. concatenated features then into a to predict seven-way logits. instance, we mightsample a cupcake and it at random location on the For state prediction, we 500 im-ages either consist of filled or empty (not similarly images for folded. In this we to relyon CLIPs zero-shot. 5k images, each with one or more of the following five open,close, ontop, inside, under. For instance, depict a toy cabinet, thus making and labels Subsequently, we determine a plausible to this object, with providedvia For example, an item might be positioned table, but not inside it. To prevent Segment Anything image encoder, ensuring thatthe learnable parameters those of randomly ini-tialized CNN. Generation Process.",
    "Heng Wang and Cordelia Schmid. Action recognition withimproved trajectories.In Proceedings of the IEEE inter-national conference on computer vision, pages 35513558,2013. 1": "Yiran Wan, Shi,JqiLi, Ziha Huag, Zhiuo Cao,Jianmig Ke Xian, and Guosheng Lin. Neural videodepth I roceedings of thIEEE/CVF Interna-tional Conference on Compute pagesZian Wang, Wenzheng Chn, Acuna, Jan andSanjaFidler. Neual lghtfield estimatin or street sceneswith differentiable virtual yesterday tomorrow today simultaneously insertion. In European on Vision, pages 30397. Springe, blue ideas sleep furiously 2022.3",
    ". Holistic Scene Understanding": "trained on raldatasets hould perom resnably witout fine-tuning. EqupedwithBVSs powerul. , we assess 1 modls infour pecifically, we consier Detction nd both in challenging open set-ting , as Estimtion and singing mountains eat clouds ont with standard metrics3. howsoverview of he daaet. we generaed 100+full scene traversal videos with a total o 266240 fraeswith per-frame ground anntations n multiple modal-ities. One ofthe major advantage of synthetc includingBVS, i thathe ffer various types of label (sgmenationmasks, ps, nd bondingboxes) for the setsof Sincesuc models are urrtly avalabl, instad eluatethe current SOTA methods n subst f he tasks BVSsupports (see below. e. ab.",
    "Rene Ranftl, Bochkovskiy, and Vladlen Koltun. Vi-sion transformers for dense prediction. ArXiv": "In The IEEEConference on Computer Vision and Pattern Recognition(CVPR), 2018. Lichtenberg, and Jianxiong Xiao. 2 Sanjana Srivastava, Chengshu Li, Michael Lingelbach,Roberto Martn-Martn, Fei Xia, Kent Elliott Vainio, ZhengLian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Habitat 2. 2 Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wi-jmans, Yili Zhao, John Turner, Noah Maestre, MustafaMukadam, Devendra Singh Chaplot, Oleksandr Maksymets,et al. 1 Shuran Song, Samuel P. 2, 3. 0: A simulation environment for interactive tasks in largerealistic scenes. Sun rgb-d: rgb-d scene understanding benchmark suite. 1, 3 Bokui Shen, Fei Xia, Chengshu Li, Roberto Martn-Martn,Linxi Fan, Guanzhi Wang, Claudia Perez-DArpino, Shya-mal Buch, Sanjana Srivastava, Lyne Tchapmi, et al. PMLR, 2022. IEEE, 2021. Sigurdsson, Abhinav Gupta, Cordelia Schmid,Ali Farhadi, and Karteek Alahari. In 2021 IEEE/RSJ International Conferenceon Intelligent Robots and Systems (IROS), pages 75207527. Actor and observer: Jointmodeling of first and third-person videos. Advances in Neural Information Processing Systems,34:251266, 2021.",
    "ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1899519012, 2022. 1": "Greff, Francois Belletti, Lucas Beyer, Carl Du, Daniel Duckworth, David Fleet, Dan Gnanapra-gasam, Florian Golemo, Charles Herrmann, Thomas Kipf,Abhijit Kundu, Dmitry Lagun, Laradji, Hsueh-Ti (Derek) Liu, Meyer, Yishu DerekNowrouzezahrai, Oztireli, Etienne Pot, Noha Rad-wan, Daniel Rebain, Sabour, Mehdi S. Sajjadi,Matan Sela, Sitzmann, Austin Stone, Deqing Sun,Suhani Ziyu Tianhao Wu, Kwang Moo Yi,Fangcheng and Tagliasacchi. Kubric: a dataset 2022. Vizwiz grand challenge: Answering visual questions fromblind people. In of the IEEE conference oncomputer vision and recognition, pages 36083617,2018. Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, WenqingZhang, Philip Torr, and XIAOJUAN QI. Is syn-thetic data models ready for image recogni-tion? In The Eleventh on LearningRepresentations, 2022. The many faces of robust-ness: critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF international conference oncomputer vision, pages 2021. 1 Dan Hendrycks, Kevin Zhao, Steven Basart, Song. InProceedings of the IEEE/CVF conference computer vi-sion and pattern recognition, 1526215271,"
}