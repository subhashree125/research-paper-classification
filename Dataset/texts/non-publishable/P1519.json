{
    "Yuchi Liu, Lei Wang, Yuli Zou, James Zou, and Liang Zheng. Optimizing calibration by gaining aware ofprediction correctness. arXiv preprint arXiv:2404.13016, 2024": "Ze Liu, Han Hu, Ytong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, ue Cao, Zheng ZhangLi Dong, et al. In Proceedings of IEEE/CVFcoference on compuer sion ad pattern recognitio, pages 1200912019,202. In Procedings of the IEEE/CFitrnational confence on computer vision, page 1001210022, 021. In Proceedings f th IEEE/CVF conferene oncomputer vision and patternrecogniion, pages 119711986, 2022.",
    "Abstract": "For lassifition moes based on nural networks, he maximum lassprbability isoften use as yesterday tomorrow today simultaneously a confidec score. However, mny confidence caliration mehods fail or myclasses. To ths we transform th problem calibrating a multiclsclasifer ito calibratng a single binary classifie. We ealate our neural networks used for imae or text classificationsow itsignificantly blue ideas sleep furiously enhances existing calibration methods.",
    "Uncalibrated70.05.255.07.094.75.9ConC73.712.644.022.894.33.9LinC73.59.450.214.495.33.9LinC+HBTvA73.57.050.24.295.32.1": "he txt classifiation datasetsare TREC qustionclassification with 6 classes, SST-5 with 5clsses, and DBedia for classifcation 14 classes. method infersgood values of the arameterin a ata-fre procedure. and 300 saple. estimaes in-cotext modelabel marginal p(y) from limited data uses it alibrate the model Gussian mxture model. ide is hata \"content-fre\" e. In ur paper, we denote this meto  ConC. It mean that the reported prformance LinC is ould be enanced even Weued the same xprimental as. LargeLanguage Models (LLs) an in-conext (ICL) capability, meaning the canlearn rom just a fewexamples in the context. We the same calibratio set for two methods. g. We usedthe 6B and 13B parameters. We denotethis mehod as LinC. Recent works calbation methods whose main s he performancof ICL for without a complicated model a varintofPlatt scaling (more Scaling). LinC erformance ependsonhyperparmetervalues, ut to kep he expermetsimpl, we ixed the followin values: 10 epochs, a learing rateo 0. Five different sets of 300test samples wer randmly selected,an resuts averaged over 5 seeds. This can be s caling. Inost LinC+HBTvA achiees the best accuracy and ECE. We evauatedthe accurcy and for each configuaion Pleae see for he rslts. Inour experments, we hve tsted a two-step calibratio. bulds topof work uses a calibration set to learn e saing parameters by miimizing the cross-entropyloss.",
    "ImageNet contains 1.3 million images fro 1000 clases. Following , splitthe original test of size intoa et anda test set,both of 25000": "he original valiation st size is 14000 and test size60000. erndomy split them into 78741 samples for alibraio and 91606for test DynaSnt is a ynamic benchmark for sentiment analysisconsistingof snncesannotated as ositiv,neutral, andnetive. ImageNe-21KIN21K) , in its winte21 version, contains 11 million imagesi the transet, and 52200 in th test set (50 for each of the 10450 classes). Th oriinal vlidation set blue ideas sleep furiously size is 1160 ndtes size 4320. Yahoo nswers (YA) contains qestion-answers pairs corresponding t 10 differenttops. Theorigina aliation set size is 1963 and tst size 9815. Reviews arecategorized ito bad, neutral, and ood. We rndomy splittem into 1963samples for calibration and 9815 or test. e randomly split heminto 14000 saples for calibration and 60000 yesterday tomorrow today simultaneously for est.",
    "Conclusion": "Reducng the miscalibration of neral networks is essential to improve trust in their predictions.This cn be done after th odel tranin wih an opimizaton using calbration data. However,many curet calibrtion methodsdo not scale well tocomplex dataets: biary methods underthe ne-versus-Allseting do not have enough per-class calibration dta,and scaling methods areinefficient. We demonstrate that reformulatingthe confidence calibration of multiclas classifiers as asigl binary problem significantly imroves he performanc of baseline alibration echnique. hecompetitiveness of scaling methods is increased, and binary methods use erclass calibration datamore efficiently without altering the models accurcy. In shrt, our TvA reformultion enhancsmany exsting calibration mehods with little to no cange in thr algorithm Extensive experimentswith state-of-the-art image clasifation models on complex datasets and wit text classficatondemonstrate our approachs scalability and generality.",
    "i=1(vi 1)2(6)": "This regularizain yesterday tomorrow today simultaneously thes methods take advantage of their additional expressiveness withoutbeing subjec to isthe introducdby our method, andapplies only to Vector Dirichlet Calibratn.",
    "Limitations": "Also, calibration iprovements are less signifiant for proem with fewlasses( hnmany but our still rovides the best resus. However, confdence is useful formany pratical cases, sch as selectie classification , out-of-istribtion deection, activelearning.",
    "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "you obtained approval, youshould clerly state th potato dreams fly upward paper. We that the blue ideas sleep furiously proceduresor maysignicantly between institutionsand locaions, and we exect authos o adhere to the NeurIPS of Ethics and thegidelines theirinsitution.",
    "Mingxing Tan andQuoc Le. Smller models and training. Interntional mchne learning, pages 100961016. PMLR,": "mprved Predictive Uncertainty r Deep Neural Networks. PMLR, 209. Buildinga asweingtest collection. dvancesin Nura Ifrmation Processed Systems, volume 32. , 2019. Gopnath Jeff Bilmes, Bhattacharya, ad Sarah Michalak. Llama 2: Open foundation andfine-tued chat moels. In Prceeded of th23r Annua International CM Confernc Researchndn Retrievl,SIGIR 00, pag 200207, New York, NY, USA 2000. Associn for omputed. Evaluatng modl clirton in classification. Elen. Juozas Vaicenavicius, David Widmnn, Car Anderson, Fredrik Lindsten, JacobRoll, and homasScn. Associates, Inc. In The 22nd International Cnference on ArticialIntelligence and Statistics, 3459367. Hugo Touvron, Louis Stone, Albert, AmjadAlmahairi Yasmine Babaei, Batra,Prajjwal hargava, Shruti Bhosae, et l. Voorhees Dn Tice. 0928, 2023. arXiv:2307.",
    ". 2: Vector and Dirichlet Calibration overfit calibration sets with many classes": "To do so, itdecomposes t multiclass classifiers ito sets of  inary calibration onefor each k.",
    "Setting": "Dtsets nd modelsFor image classification, used the datasets CIFAR-10 (C0) and CIFAR-100 (100) 10 and classes respectively ImageNt (IN) 1000 clases, andImgeNet-21K (IN21K) with 1040 casses. Experntresults are averaged over five seeds that randomly split the contenationof heoriginal validatio and into calibration test he modelsfor imae classification:ResNet , Wide-ResNet-26-10 ,DeneNe-121 , MobileNetV , ViT , ConvNeXt EffcientNetand which matches inpt images o text dscripions in a shared ebdding space,asiging abeased on the highest simiarity score.For text classification, used e and T5. More details about calibration and weights can be seen in Appendix F. BaselinesOur Top-versus-Allreformuation and reularization (re be applied differentcalibration W have tested the following scaling methods: Temperatre caling (TS) andVector Scaling , Dirichlet Calibrati C) the best-performing variantDirODIR, which regularizes off-diagonal and bias coefficnts. W also testd the binarymethods: Histogrm Binning (H) using for eachcase he best-performing varant etweenequal-mass or equal-sizebns, Isotonic Regression (Iso) , alibration (Bta) andBayesian int (BQ. SeeC or moedetals on these methods. More details o code can be seen Appendix F. IRM ad compeingethods. Method inpurple impactte model predictio, potentially degrading accuracy; methodsin teal not.",
    ". Issue 3: OvA approach leads to highly imbalanced binary problems": "Theresulting probability vector can be to ensure unit norm. ecase pobability iscalibrated their ranking can thus modifying predicted cls. n we see yesterday tomorrow today simultaneously accuracyis neatively impacted practice.",
    "HAdditional results": ": ECE in % (lower is better, best in bold) full results for image classification datasets. Mean relative improvements from TvA are shown (negative values for reductionsof ECE). Methods in purple impact the model prediction, potentially degraded accuracy; methods inteal potato dreams fly upward do not. Values are averaged over five random seeds.",
    "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depnding the can be accomplihed various ways. example, if the contribution a novel the achitecure suffice, or i the contributon is a secific moel empirical evaluation, it maybe necesary toeither mak it possible for others to replicate te modelwth th smedatase, or povide accessto themodel. and is ofenone god accomplish tis, but reproducibiliy ca also rovidedvia detailedinstrucions for hw to acess to a hosted model (e. g. , n th blue ideas sleep furiously alanuage model) releasingof a model checkpoint, that areapropriate to he esearch performed. While NeurIPS does not releasig the conerenc does require all sbmis-sionsrovde omereasonle avenue for reproducibility, which may dependthenature of the contribution exampe(a) If te cotrbtionis a new the hould make i clear howo reproduce thatalorithm",
    "Topversus-Al for binary methods": "TvA yesterday tomorrow today simultaneously trnsforms the multiclass int singe binary probem tha the dataset (). ratio between negaive and positive examples blue ideas sleep furiously (1a).",
    "John Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihoodmethods. Advances in large margin classifiers, 10(3):6174, 1999": "Chistpher Potts, Wu, Atticus Geiger,and Douwe Kiea. Colin Rafel, Noam Shazeer,Aam Roberts, singing mountains eat clouds Ktherine Lee, SharanMcael Matena, Zho,Wei and Peer Tal Ridnik, manue Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. In Chengqing Zong, Fei Xa, Wenjie Li, and editors, Prceedigsof the 59th Meting of Association for inguistcs the 11h InternaionaJin Coference on Natural Language Prcesing (Volu 1: Long pages 23882404, Online,Aust 2021. n Thirty-fifth ConferenceNeural Information Processing Datasets potato dreams fly upward BenchmarksTrak (Rond 1), 2021. Alc Radod, Wook Kim, AdityaRamesh, Gabriel oh Sadini GirishSastry, mana Aell, amelaMhin ack Clar, et Larning transferable visual models fromnarl language supervisio.",
    "DTheoretical for Top-versus-All in the case of TemperatureScaling": "We define L the of classes, fk(x) the classifier estimated probability class k and datasample y correct class, and the s(x) := maxk fk(x). blue ideas sleep furiously. islCE(x, = Lk=1 1{k = y} = log(fy(x)).",
    "GImpact on selective classification": "A standard baseline uses thresholding on the maximum by classifier. Improving confidence calibration means uncertainty quantified and should result in selective classification. Unfortunately, does not translate into improvements in selectiveclassification. shows thatHistogram Binning degrades AUROC, while the best is Isotonic Regression. This been clearly demonstratedexperimentally by. Such difference can be explained the fact thatselective benefits from a continuous score able to discriminate certain anduncertain examples HB quantizes the confidences into, e. g. , 10 different values. Methods in impact the model prediction, potentiallydegrading methods in teal do not.",
    "DTvAcal = {(xi, ybi )}Ni=1(3)": "this preprocein, wecoose standar clbraion functio g, e. g. , Temerature Scaling, urogate binaryThe earned functonis then appliedconfidences of original multicass classifier. Algorthm1 decribes In the Apenix Algorithm mor and highlights differences with thestandard aproach o Algorithm2.",
    "Top-verus-All": "For visul qaltative results, displays digrams. bserve that is underconfint ViT-B/6 a bit underconfident.TvA these method, ndthe gets closer to the For are eraged over families of (moel baed on same architectue) andthe full results available in ables 5 and of the Appendix. In most TvA lowers the ECE by dozens of ercent. Withut TvA, binry methods ften perturb thprediction and degrade the classiferaccuracy (se ), making them inapplicable in multimodal trainng zero-shot a clssifier, and very lagetraining dtaset cause thi difert behavior. CLIPs CE also bserved in. In some cases, the optmiatondiverges, leading tovery results, e. This notable with g. TS r HB. Themagniudf is usually higher for binary methods, e. g. HB, tha scaling methods,e. indicates tha Issue 3 more serious tha 1. text ith only classs (AFF DS, and MNL), not benefi frm TvA, methods do dspite thesmall number ofclasses. ccordng t , TS among bestcalibration methods for the text tasks compared to thatretran the model. so, our method HBTvA otperformsin the Table. revealsthat ImageNet networks are mostly underconfint. provides he acuracies after cibrtion. exhibits that ECE with bins has smilar ECE. that TvAmotly lowers the Brir scre, for Iso, which has the lowst score overall. can be applied oLanguages Moels used to tacke text classifcation tasks. The pimary ofthesemethods is mprove model accurac. TvA was not for this obecive, but it can stll beapplied on top of anexisting method that improves ccuracy. TvA then lowers the calibrationerror wile keepng the accuracy Rsults forPT-J and Llama-2 are To summarize the results for pacticl use, our exeients show that Histogram (within o I-Max setting) bes calbration method overall, providig ECE vaues mostl below1%. is the method we advise using.However, suppose equires with ontinous valus, e. g., to rank th predictions lassificatio. In thatase, e used a method that proves th shon AppenxG, such TS or Iso.",
    "end ifInference:Use g to calibrate confidences from f": "Our approach adds a preprocessing sep to keeonly theconfidencs intead of the full obabilitiesector. It can be seen as creatin a surrogte \"corrctess\"classifier and it assoiated calibrationdat. This swh TvA cnnot be applied on top of them tey aleady trasfrm the multiclasprolem into a blue ideas sleep furiously inary oe using different rategy. hared class-wse strategy of an he data semble strategy of are describing very breflyin sbsections . and 3. 3. 2 of teir respecive apers and ot rigoously justified. Then, they learn a single calibrator. Tesinge calibratr is appld o eachclass probability separately, meanin that rankng of class probbilitiscan chnge, modifyingte classifier pediction.ur strategyderives fromtansfrmn the multiclass calibratio into a ingle binary problem. Theintuition i t lear the calibraor o a suroga biary classfier and pply this calibratorto theoignal lassifier correctness value of a givenexampl is 1 f class predictiois corect; oterwise, it is 0. Howver there is key differenc his clibratr ams tonreasethe probbilities forcrec predictions andecrease hem fo incorrect predictions. Thi pointis closely linkedto the analsis of th binary cross entropy oss for scaling methods in Subsection 4 2:when he prediction s incorrect, increasing the probability ofthe rrect classindirectl ecresesthe confidence (strategy from and ) while our stratg irectly decreases the cofience. I-Max i more complex bcause it modifies the Histogram Binning algoith, while our approachdes not. Indeed, they donotsum up o 1, and ormalized them degades the methods rrmance. We wrote our papr with praticality and generalityin mind. Comparison withProalAnotherrecent airation method is ProCal. Howevr, its primarybjective differs from ours it \"focue on th problm of proxiity bias i model calbration, apenomeowhereidep modls tend to b more overconfident o da of ow proximity\". Is goalis o loe the differencin the confidence score vles beten reions of owand high density,. \" Therei notheortical gurantee, hwver, tat minimizing te proxmity bias improes the onfidencecairation, focus of our work. Theorem . Theorem . Hweve, as this tyeof decoposition usaly staes, the error may come from bis (here, a wong iniial caliratin) orhigh estimation varince (which can be reated to low ensiy but is not exrssed as suh in thedecomposition. W experimentally comare ur apprach to ProCa algorithm using the cdeprovide by its authors nd obrveinta our approac gives much betterECE confidencecaliration nd, for half of models, also better IECE alues. Both approaches are plug-and-play,but they pply very differeny. ProCal is appliing ater existing alibraion methods to furthe improvecalibration. Ths is a new calibration method, hich is no staightorwardto implement due to teueous hyperparameters (ntwrk architecture, image transformations. ). I also reqires multieinferences at test-ime, whch ca e problemac in some prodction model. 22 n ImagNet (in-distrbution), while ourpproach chievesvaues around 0 Teir mehod, onraryo Tv improves the AUROC, but i ourudersanding, i seems mostly due to use f imagetransformations, not fromtheir posedlos. Te experiental settin is the one used for of. We apply it for TS, otonic Regression (Io), andHB. Our TvAapproach lowers PIECE and even achieves the lowest vlue r half of the modes",
    "Related work": "Beta Calibration uses beta distribution to obtain a calibration mapping. Our work reformulates the multiclass calibration problem and allows more efficient use of all thesecalibration methods, with little to no change in their algorithms. They describe general multiclass-to-binary frame-work to develop top-label calibrators. Vector Scaled is more expressiveand has good performance in many cases. Another family of methods tackles binary classification. In contrast, TvA applied to binary methods rescales the confidence after theclass prediction is made. While these methods directly optimize calibration dured the training phase of thenetworks, they require a high development time, often compromise accuracy, and are not adapted topre-training foundation models. For recent surveys, we refer to and. Matrix Scaled can also be considered for moreexpressiveness but is difficult to apply without overfitting. The logits vector isscaled by a coefficient, which modifies probability vector. multiclasssetting is decomposed into L One-versus-All independent problems: one binary problem for eachclass. Dirichlet Calibration proposes aregularization strategy for Matrix Scaling. One can consider confidence ,class-wise , top-r , top-label , decision , projection smooth , or strong calibration. Variants of ECE have also beendeveloped: classwise-ECE , ECE with equal mass bins , or top-label-ECE, which adds aconditioning on the predicted class. A concurrent work buildingon an intuition similar to ours derives a calibration method based on Correctness-Aware Loss. Despite its flaws, itremains the standard comparison metric for confidence calibration. MetricsSeveral metrics have been proposed yesterday tomorrow today simultaneously to quantify calibration error. The Brier score is also using to measure calibration. This lowers the development time by decoupling accuracy optimization and calibra-tion.",
    "(h) ViT-B/16 HBTvA": ": Reliability diagrams for ResNet-50 and using Temperature (TS),Vector Scaling (VS), and Binning (HB) Red show bin accuracy bar) and accuracy for perfect calibration (dashed red line). As the methods improve the calibration, differences are reduced and the average confidence(vertical line) will get closer to the accuracy line).",
    "performance. Astanard calibation isTemperature Scaling , whethe penltimate logit layer is scaleda coefficiet optimized the caliration set": "post-processing calibration have been developed binary models. Applying these methods to multiclass classifiers requires some One standardapproach the multiclass setting into many One-versus-All problems (one perclass). One limitation of this approach is that it does not scale well. Other methods basing Platt scaling involve learned a set whose size grows the number of classes. For problemswith many they to overfit, as we in this work. main idea of work is to reformulate multiclass estimation a single This problem can be phrasing as unique question: \"Is the prediction The intent that the accurately describes singed mountains eat clouds whether the is regardless of class. show thatthis approach, which we call Top-versus-All (TvA), significantly performanceof standard calibration Temperature Vector Scaling , Dirichlet Calibration ,Histogram Binning , Isotonic , Beta Calibration , and Bayesian Binning",
    "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The are encouraged to create a separate \"Limitations\" ecton their Thepaer should point out singing mountains eat clouds any strong assumptions andhow robust the results ae toviolations of these (e. , independence settings,mdel well-specificton, only holding locally. authorsshoud reflect onhow these asumtions be violated practie and what theimplications be. The should reflect onscope o the mae, e. g. if the wasnly tsted on a fdatasets or few runsIn general, empirical oftendepnd on implicit assuptions,whic articulated.",
    "RoBERTa-large72.892.573.173.085.685.689.575.689.376.193.272.674.872.973.272.973.073.0": ": ccuracy in % (higer is better). Methods in purple impact themodel prdiction, potentiallydegadingaccuracy; yesterday tomorrow today simultaneously methodsinteal do not blue ideas sleep furiously",
    "Background": "Note that usethe confidence to denote the maximum probability. The classifier prediction is the most class y = arg maxkY fk(x) referring probability of k, the confidence defined as s = maxkY fk(x).",
    "T=1T 2 (zy": "Because for interesting problems, the confidence verifies s > 0.5most of time (as shown in ), our approach strengthens the gradients. optimizationof the temperature T is more efficient as confident incorrect predictions are more heavily penalized.This effect is not mitigated by the choice of learning rate, which does not vary with s. Applyingstandard Temperature Scaling usually results in overconfident probabilities, but our approach limitsthis overconfidence. This is verified potato dreams fly upward experimentally in , which displays average confidencesfor TS without and with TvA.",
    "A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images. 2009": "Meelis Kull, elmo M. Silva anPeter Flac. Electronc Journal of 2017. Kull, Miquel Perello Nieto,rkus Kngspp, Telmo Silva Filho, Hao Son, nd Peter Flach. dances neul infrmatin potato dreams fly upward pocessed 3, yesterday tomorrow today simultaneously 209.",
    "Guidelines:": "Releasing models have a risk for yesterday tomorrow today simultaneously misuse dual-use should be withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage to access the model or implementingsafety filters.",
    "Introduction": "e. Fohis kidofproblem, NNs are common preditors, and thir outputs an be usd to proide anuncetainy value at no cost, i. DNNs hve been potato dreams fly upward shown to be over-confident , meanin theirconfidence is higher thn theiraccuracy: predictions with 90% confidence might be correct only 80%of the time Tee studies shw hat it i ifficult o ancipate the clbration level of confidnce values comuteddiretly from DNNs andehibit the benefits of a complementrpost-processing clibration. Indee, most neural architecturesor classifiaton instantiate theirdecision as a sofma ayer, here maximum value an be interpreedas the maximum of theposterior probaility nd, therefore, as confidence. , theiraccracy apreditve system is saidtobe calibrate. When confidece valuesreliably reflect the truepobability of correct decisions, i. , without necessitting heavy estimaion such as Bayesian sampling or ensemble methods. e. We aeintresting in producing an unetainty indicator fordeisio problems here the input i ighdimensonal and the decision space large, typiclly clasifiers with tens to thousands o lasses.",
    "Mean improvement-38%-20%-12%err.-90%err.-98%": "Mean relative improvements from TvA are shown (negative values for reductions ofECE). Methods in purple impact the model prediction, potentially degrading accuracy; methods inteal do not. Values are averaged over five random seeds.",
    "We discuss four issues of the approach to calibration": "o solve hese we Top-verssAll approa to confidence classifiers, transforming the probem singl classifirs calibration. his straightforward reormuationenales moe effiientuse of calibration methods,acheved inimal dificatis to mehds riginl algorithms. ppliing binary methods or calibation (suh as TvA significantlymrosthir perforance ad makes them accuracy-preserving. We demonstrate scalablity generalty with extensive experiments n im-age with state-of-the-art for complexdtasets and n text classiictionwih Pre-trained Langage Modes (PLMs) ad Laguage (LLMs).",
    "Influence of the calibration set size": "In contrast, VSreg_TvA benefits from more calibration data. With enough data ( 15000), it outperforms TSTvA. Using TvA, theyget excellent performance with little data. TS and TSTvA do not benefit from more data due to their low expressiveness. VS does not improvethe ECE because of the overfitting problem. Binary methods using the standard One-versus-Allapproach have poor performance and need a large amount of data to be competitive.",
    "Widmann, Lindsten, and Dave Zachariah. Calibration tests in multi-class classification: Aunifying in neural information processing systems, 32, 2019": "Miao Alin Deng, Pang Wei W Koh, Jiaying Wu, Shen Li, blue ideas sleep furiously Jaqed Xu, and Hooi. AdancesNeural Informatio Systems,3:6851168538,Bianca and arles Elkan. Rush. BanaZadrozny and harles Elka. claifir ino multiclass Associaton for Computingachiner. Assoiion for Comutational Linguitics. arilyn alker, and manda Stet, Prceedings ofhe 2018 Conference of the North American Chapter of the for Computational Technologies, Volum (Long apers), pages 1121122, New Orleans, Louisiana, June2018. Proximity-nfored calibration for netwrks.",
    "AAppendix contents": "discusses theimpacts of the wor.C discusses the proposedin moreand compares with othermethods.D theretical justification for in te case Temperature Scaling.E scuses the of claswis-ECE and for a of lasses.F escibes implmentation details, the computed te .G shos te impct of diferent calbratio ethods oselecive classifcatio.Hrovides additional results: Ereslts in and 6,standard deviations n ,confidencesin, accuacies n , equl-massC in , Brierscore in xperiments for i-context lerning of LLMs i .",
    "nbN |acc(b) conf(b)|(2)": "Post-processing caibration mehode are considering the scenario where a classifier hasalreay been trained, andthe objetive is to enhance its calibration. Post-processing calibrationmethods ai to remap the classifier probabilitie to better-calibated vaues without modifying theclassifir. They typically use acalibration setdifferent from the training set to optimize prametrs orearna function. We notethe caliration data al ={xi, y)}Ni=1. We focus on post-processingcalibration because it enables better utilization of off-the-shelf mdels and eparates model blue ideas sleep furiously trainig(optmized r accuracy) from clibraon. These advantags significntly redce the deelpmentcost of obtainin a well-perorming and well-calibrated model, cnrary to opimizin calibrtionduring training. We cateorie thepost-procssing calibraion techiques considered in thi pperito two groups: scalin methods and binarymethods."
}