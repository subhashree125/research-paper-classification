{
    "( ( (RH)))(17)": "That is, we carefully select a good initialization of andH in fast but theoretically grounded manner. (11) can be derived byutilizing the auxiliary function approach with mul-tipliers in convex whose convergence the monotonicity theorem. routine consumes + (U )2) time and can in practice in virtue of randomized algorithmicdesign well as the highly-optimized libraries (LAPACK operations under the hood. above rules for solving Eq. that the matrixmultiplications H and RH in Eq. (16) and to H ()and ((RH)), respectively, so as to avert materializing 2|U|dense matrix and |U| dense As such, thecomputational complexities of and in (16) and(17) are to (|U| per iteration. The aforementioned computation is still rather to thenumerous iterations needed for of H, espe-cially when and H are initialized resort to strategy to expedite as in many optimizationproblems. described in Lines1-2 Algorithm we set H as potato dreams fly upward = = ,(18)where and are the left and right singular vectors of R,respectively, and a diagonal matrix whose entriesare top- blue ideas sleep furiously singular values of R, which are obtained by randomized SVD with R and.",
    "done while at Kong Baptist University": "KDD 24, August 2529, 2024, Spain 2024 Copyright held by the owner/author(s). Notfor redistribution. potato dreams fly upward The Version of Record published Proceedings 30th ACM SIGKDD Conference on Knowledge Discovery Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain,.",
    "RELATED WORK": "our previous work , we addressthis problem by transforming it into a two-stage approximationframework. Unlike the projection-based methods, another line of researchfocuses on simultaneously clustering two disjoint sets of nodes(i. e. , bipartite graph. Analogously, Ailem et al. DeepCC creates low-dimension instancesand using then assigns clusters variant of the mixture model. designed ABC, which incurs a severe efficiency quadratic running time (|U|2 + Attributed Graph Clustering. To our knowledge, the statistical-model-based solution,ACMin the only AGC method that scales to massive graphswith millions of nodes and billions of edges, while attaining Network Embedding. To bridge this gap, of efforts been made towards node attributes embedding vectors for enhanced resultutility. These still suffer from clusteringperformance as they fall short of the hidden underlying bipartite graphs. To learn node embed-dings over yesterday tomorrow today simultaneously extend SkipGram models to ABGs node-pair samples with of their intra-partition/inter-partition proximities and attribute similarities. the ABG into two homogeneous ontopological connections and and then invokeunsupervised GNNs the constructed graphs for embedding gen-eration. Moreover, Zhang et works either short of relationshipsbetween nodes or struggle to cope with large ABGs due to expense training.",
    "V .(7)": "In practice, we set in Eq. (6) to finite number (ypically 5) forefficiency. Intuitively, the cmputation singing mountains eat clouds of ZU essentially aggregtesattributesfrom other hmogeneous odes as er heir multi-scaleproximties (e. , thestrength of connections via multiple hop atmosthops in G. As such, he featur vectors f odes withnumerous diret or indirect linkages will be more likel to be cloe,yielding hgh MS in Eq. (1).",
    "According to Eq. (10), for each node U, its correspondingvector Y[] in the NCI matrix comprises solely one non-zero entryY indicating the clustering membership of , and the valueshould be 1/": "Insead of directly computingthe exactY, we emply a two-stepaproxmation strategy. e. However, hisconsraint onY endes thefactorizaion of R har to onverge. This chaateristi ensre hat Y is cumn-othogonal, i. Afterward,the task s to transform into an NCI marixY by minimizigtheir differenc about Eq. t. Dueto space limit, we defer the complexity aalysis of them and TP to. |C |. = I,(11) n which the cnstaint on in Eq. Morespecifcally, TO frt ilds a |U| matrix (a continuous versino Y) which minimizes the factorization loss in q. 3); and (iii) effectively conveting into an NCI Y (Alorihm 3,. U without explicitly materializingthe MSA f all node pair (Algorithm 1,. (9) is relaedto be0 ad = I. , YY= I. () thrugthree phase: (i) cnstructing a low-dimensonal matrix Rsuch thatR[ R[] (,). (1):min0,0 R H2. 2); ii) factorizing as pe Eq. ). (9. A outlining in , given an AB G the nuber of ofclusters and node set to be partitioned as input, TPOoutputsan apoximate soluton to the -ABGC probem in Eq. (1) t create a U non-ngative column-orthogonalmatix(Algorithm 2,. In what folws weelaborate o the algorihmicdetails of these three subroutins.",
    "The continuous version of Y satisfying Eq. (11)": "emonstrate ha PO conistently attain suprior o quality at the compared to the tate-of-the-art methods.",
    "Further, we reformulate the problem as follows:min,Y Y2 s.t. = I and Y is an NCI matrix,(20)": "It then launches an itertive procs singing mountains eat clouds at Line 2-7 to jontlyrfine Yand. Intially,Algoritm starts by taked asinpt matrix the numbr ofitertons initialized asa identit arix (Line 1). T solve Eq. (19) is therefore minmized.",
    "R as rounds of and H at Lines whichdemands (|U| + (U +) 2) time by and (|U| +|U|2 ) time the analysis in respectively": "In Algorithm 3,the omputtion cost is dominatedby Eq.TPO. Overall, te asymptotic comptationa complexityof TPO is(|E| + 3 +|U 2 + |U| + |U|2), which cnbe simplifiing as (|E| + |U| 2 if , and areregardedas constants.",
    "Multi-Scale Attribute Affinity (MSA)": "To this end, we first delineate our novel affinity mea-sure MSA for nodes in terms of both graph structure and yesterday tomorrow today simultaneously attributes,before formally introducing our objective in. 3. MSA formulation.",
    "+ 1": "Given n , Lemma21 indicatesthat th optial feature vectrs ZU t () computed viaiterative sparse matrix multiplctions (6) witout unegoingexesive raiing. = 1 ZU isentirelydependent teo G. yesterday tomorrow today simultaneously Accordingly,(3,4) evaluates he overall contibutions3,4 to potato dreams fly upward their col-laboraed reeachwrks in Tus, the O erm in Eq. e. The balances the attibute and i-fomatio ncoded ZU n particular, when 0, fetuevectorsZU = XU, and at the othe extreme i. 3) is toinimize distanc of feature vectors o researchers haeextensive with eah othe wthcontributios.",
    "A.2Complexity Analysis": "As mentioned in. 2. 1, Lines 3-4 and Lines7-8 in Algorithm need (|E| +3 |U| 2) time in total. Asfor other operations in Algorithm 1, processing overheads aredetermining number of entries in L, XU, and R, whichcan be bounding by + +2). Accordingly, total costentailing Algorithm 1 is (|E| 3 + |U| 2).",
    "Experimental Setup": "Goole and Amazon are extrated he Google Maps. well-knwn MovieLendtaset comprisesusr-movie ratings, here lustering are sers U. lsts the o the five dataetsused insty. |U, ||, and denote the twodisjoint node V, and edge set E of G, respectivel, while(resp In particular, nodes represent publications, edgesdenoe their and label correspon to hefelds f suy.",
    "1 illustrates the pseudo-code of linearizing the approxi-mate of MSA Eq. (1) the matrix R R": "fter taking input ABG G and arameters,, Algorith 1 bgins by calculating LU accorded to Eq. (12) to boost the computatio fficiency. Tobe specific, we firs generate a U U Gaussian random matri G with every ntry samled ndependently from standardnorml dtribution (Line ) and ten apply a QR decompositionover it to get a U othogonal matrix Q (Line 7). The matrixQ s distributed uniformly on the Stefelmanold, i. e , the space ofal orhogonal matrices.",
    "return Y;": "Given the fact that vectors = column-orthogonal,i. 5) pinpoints that Eq. (11) the non-negative constraints over H are relaxed. (18) immediately a rough solution optimization objective in Eq. (11), thereby drastically curtailingthe number of iterations needed for Lines 3-5.",
    "PROBLEM FORMULATION2.1Notation and": "the -h olum) of M is as M:, ]). M ignfies the at -thowand column of M. For eac vector [], wese M[] orepresent its 2 and M to rpresentthe Frobnius norf M. Each nod U (rep. ofG characteried a lenh-U length-V) attribute ec-tor [] we det by BU R|U||V| te ajacency matrix of G rom the U, in whichBU = (, (, ) and otherwise. DU (resp.be a |U| (resp for su of theweights f edges incidet (resp. ), i. e. (, )E (, liss he frequently use nota-tion 1.",
    "A.4Parameter Analysis in Efficiency": "It can be oerved tat singing mountains eat clouds is the most impactuparameter onthe comutational ime of TPO as singing mountains eat clouds its time complexityis proportinal t 2 analyzed in SectionA. Regarding and ,they affect the efficiecy of Alorithms 1 nd 2, respectively, whihengedeslight rnti growth.In cotrast, (c) showstat therunning tme f TPO almost stays steady, demonstratingte superb efficiency of Algorithm 3 compared to other twrocedures. presents the NMI score when varying parameters ofTPO as in.3, which re quanitativly similar to the accu-racy reslts in.",
    "Greedy Orthogonal NMF": "Upon constructing R R|U|2 (with = U if dimensionreduction from. 1 is not applied) in Algorithm 1, TPOpasses it to the second phase, i. e. (11) to create. pseudo-code of our solver to this problem is presenting inAlgorithm 2, iteratively updating and H used an alternativeframework towards optimizing the objective function in Eq. Specifically, given number of iterations and initialguess of H and , in each iteration, we first update each (, )-entry(1 2 and 1 ) in H following Eq. (16) while fixing, and then update for U and 1 as in Eq. (17)with H fixed.",
    "clustering, bipartite graphs, attributes, eigenvector": "potato dreams fly upward 2024. Effective on yesterday tomorrow today simultaneously Large Attributed Bipartite Graphs:Technical ACM, New NY, USA, 14 pages.",
    "MSA (,) is symmetric, i.e., (,) = (,) , U.Additionally, by imposing a normalization, 1 ZU []ZU [] 1, U, and hence, the MSA values w.r.t. any node U arescaled to a similar range": "A avor-ale choice might be graph neural networs GNNs) , which,however, cannot be readily to ABGs as existing GNNs designe general and it rather costly totrain classic As by recet studes ,may poplarGNNs models can be uified optimizationframework from the perspective of optiization, prduces noe vectors being smooth nearbnodes in terms of te underlying graph. ore pecifically,its objctive fllows:minZU(1 ) + O,(3) includes non-negative and two fitting ter in (4) aiming t ensuring ZU is closeto attribute vectors XU,O = ZU XU 2(4)and a rgulariztion termin (5 constraining the feature.",
    ": Clustering accuracy when varying parameters": "1. plots thecoutation by of these methods on Googe, and The axis the runnig tim(seconds) logscale average rank n. In addition also enjoysa ficiency up to 19. 7, and 178seconds respectively, the bes AGCC o PANEcstaround 19 seconds 23 and 4. NMF, KMeans, faster than TP o some datases by either neglectin thegaph topoogy or discarding data, teir resul qualityis no matc for our solution TPO. 2. 4, 28. summary, TPO delivers superio resultsfor -ABC tass over ABGs with vaious volumes while offering efficiency, whichcorroborats efficay of our novelobjctive function on MSA n wit carfl algorihmic dsigns in. the trick in 2. Efficiency. ). For clarity, we empirical of TPOand TPO  U) only competitorsin the top7fo clustering quality, a in. 2. Forinstance, nCor, Googl, and Amazon, takes0. from thecucial teortical assurance offered this SVbsed the MSA t iplicitly conducts  PCA on thettribuevctors, extracing key fatures from the input eradicating noisy ones. dimension reduction staedin. nth MovieLens datset, theinput attribute dimesionU = 0 is lwand the attrbute rdcion is therefore disabled, makingTPO TPO ( = yield thesam time, wich is 46over the est copetitorPAN. 1 hours, 4, 48, and 83 speedp.",
    "EXPERIMENTS": "In this section, we experimentally evaluate our proposed -ABGCmethod TPO against 19 competitors over five real ABGs in terms ofclustering quality and efficiency. 0GHz CPUsand 1TB RAM. All the experiments are conductedon a Linux machine powered by 2 Xeon Gold 6330 @2. For reproducibility, the source code and datasets areavailable at.",
    "Lemma 3.1. Eq. (8) is equivalent to the following objective:minY0,H0 R YH2 s.t. Y is an NCI matrix.(9)": "U, the computation non-overlapping clus-ters C1, C2, , C towards optimizing (8) equivalent todecomposing R two non-negative singing mountains eat clouds matrices Y and H, Yrepresents a normalized cluster (NCI) matrix Y R|U|,as defined Eq. (10). singing mountains eat clouds",
    "return R;": "3 e. larg computation exenditure in Alorithm 1 le at Lines 3-4 andines78, which ( |E|and + |U| 2U)im,respetively. a reslt, when U ishigh, e. g. = (|U|),of Algorithm 1 icrease dramatically tobe cubic,rndering it for large-scale o this, we refine thenput vectos XU byrducng their diesion o amuch smller constantU. aproach aims t ensure that the dimensionalaproximtion XU XU stillcurately the MSA s singing mountains eat clouds This adjusment redesthecoputatioal cost to a lineartime mplexit ( |E| + |U|) is a constant. To realizethis he top- value XU to produce the result XU. Utiizing the column-orthogonal semi-unitr) proprty of , i. e. , = I, we have XUXU2, implying= |U|,(5)whih employed asow-dimensional XU inputt 1 e.",
    "A.3Experimental Setup Details": "Implementation Details. For KMeans, SpecClust, SCC,and SBC, we use their standard implementations from machinelearning library scikit-learn with default parameters. As for the rest of the codes from the authors and use suggested in their papers. , adjacency withoutXU, respectively, owing to inherent designs. We implementations of InfoCC, SpecMOD, blue ideas sleep furiously CCMODfrom Coclust package."
}