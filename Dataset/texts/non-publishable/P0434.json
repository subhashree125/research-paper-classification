{
    "std(p_man)p_std": "4. The results are fis grouped by actor confguraions m nd partial mean p_mnm) and standarddeviation (p_tdm) is calulad. The new of the relevant metrics (c-tributed stndard sndarddevation, importnce score sholdbedterminedand difference to previous val-us calclate. cntnue to step eiter the vaue of or M). These values thenaggregating nto contriutedstandard deiation (c_s), represetingthe effecs invstigatedrandomness factor, y calculating mean over th p_stdm, and mitigated standard deviatin (m_std),representing theeffects o mitigating randomesscalculating a stnddverpmanm.",
    "EValidating the Proposed InvestigationMethod": "As ther no round-truth of the effectsof randness tocompareagainst, thevalidity ofethods for investigating he effects of bevaluated only indirectl. In wperform indirect evaluatin/vliation by: Wediscuss validation further in Appendix . we as he mai validation ofthe method). 2. Explorng the results and ndingschangeas we he overall numbe The results and of the dependent on choice how many in-vestigtion ad mitigtion runs re using Weobserve a trade-ff betweeno well re-sults estmated (higher of run leadsetimation and nube of runs to better mitigation) nd thecomputatio required to achieve re-sults (incresng the nmber of",
    "this work, we have proposed a novel method thatexplicitly takes between different ran-domness into consideration": "Instead, we observe tha theimportance of randomns factors esecially thesamperde, is afecte by the iterctions thother factors and b the systematic chice suchas th number of predcted classes, he numer ofsampl yesterday tomorrow today simultaneously per class andthe choce of prompt foma. Ol rquiremnt istoefine the randoness factors nd thei configu-rations, sch s orde o choces nthe questios orthe symbols used fr the answers. Extending urinvesigati to other tsks epresents an interesingpotentialfor future work. Aplyig ouropd method to investigate theefects of randomness factos onin-cntext earn-ing, fie-tuning and meta-learning e cnirm ourhypothess that interation between radomnessfactors maycause incorrect attribution f effectsof on fator to anothe, leaded toinconistent re-sult. e proposed method n beapplied to theNLP tasks s well, such as question answering,wit miimal modifications. theeffects f the other, non-investigated, radom-ness facors.",
    "with ifferent charateristics dif-ferent experimental setup, such as of classes, samples, omptformas": ", Ranom con-sistently to results simlar Golden Modelfr the blue ideas sleep furiously fcors, using Fixedstrategy the importance of difernt actors changesuite aross modls, approaches,datasets and xperimentl setings). In all singed mountains eat clouds of tese cses, the proposd pro-ducesresults and finding without nyobviou shortcomings.",
    "Effects of Variable Number of Samples": "fin-context has n on the importance of DtaOrder. 57 on for or fom 0. 39 on to 0. 0. 57 to 0. singing mountains eat clouds For the LLaMA-2 mdel, the decreaseisnot as consistent, with the imprtanc being muchlower on he TREC than on the SNIS dtast. In this our on the follow-ing researc does theimpor-tance of data-specific randomness factors changebase on the of classes and sam-ples? A we observe ffects of rndo-ness factors on binary potato dreams fly upward and multi-class focus to determine whether the changeinimportane y the number ofin-contxt exampls in prompt, bythe of options an b predicted or by acombinatio of Theof aa Order randomnessfator forin-context lerning icreases at highernumber of The Data Order randomnessfacor is not cniderd an hein-context models on thedatset,achieving iportance0. The imporance Data Orderand ModelIitialisation randomness factors for fine-tuningdecreases with higher number of classes. Onremaining datasets, theiportane either i-creses (LLaA-2 or Zephyr) or (FlanT5and Mistral). on 2-shotset-tng to on on the SST2 dataset. 0ha a significant the importance different fctors, the minimal ofteneading to imporance. 47, 0. SST2to However, odel does not show a cosisent tendenc ith the importance stayingapprox-maely the cross the majority o dataset. Thimportnc ofata cosistent, oreven is lowered,across all modls, datasets andumber of shots per For the importance of SampleChoice for Zehyr 0.",
    "B.3Selecting Number of Investigation andMitigation Runs": "An optimal solution is to use lowestnumber of overall runs (that lead to lowest compu-tational resources) after which the change in re-sults (the contributed/mitigated standard deviationor the normalised importance score) is under anacceptable threshold. The value of this threshold depends on the setup of the experiment and thegoal of our investigation, as in some cases higherchange in standard deviation may be acceptable,while in others we require more strict setting. The method is composed of followingsteps: 1. The threshold of smallest acceptable change ,and the starting number of investigation runsN are selected. The number of investigationruns should be sufficiently high from start(following recommendations in ) tomake the search faster. 2. A new mitigation run should be performedusing a randomly selected configuration ofthe non-investigating randomness (or the num-ber of investigation runs should be increased,running the new investigation runs for all thealready performed mitigation blue ideas sleep furiously runs).",
    "regards to the reliability-feasibility trade-off": "At the same wealso observe increase deviation (going from as low 0. g. 904 inthe Label Selection singed mountains eat clouds to 2. All in all, we conclude that our proposed method is not as dependent on thenumber mitigation and not computation-ally expensive as can be it moreeasily usable on more computationally expensivesettings (e. , labelled or us-ing computationally expensive models). In the most extreme setting (using10 mitigation runs and 500 test which rep-resents baselines setting data) we observe a change the findings regarding theLabel Selection randomness factor becomesnon-important it is overshadowed by the with the importancescore being slightly below 0 value. e. 33 importance with10% data while 100 and 10 mitigationruns respectively, as compared to and 0. In difference in importancescore smaller amount of test samples ismore significant than when using smaller numbermitigation runs (i. Based on observed we can de-termine which factor affects of results the most Data Split. However,the less extreme setting, where of base-line setting data is used (10 runs and1000 samples), the results are reliable though score lower in thiscase). In essence, and/or using test sam-ples for leads to significant the variance from data split randomness fac-tor. As such, the observed importancescores for factors may be by thischoice, but the findings regarding importanceshould hold. Sim-ilarly, we can observe importancescore as well, with the importance of fac-tors being lower with of mitigationruns (with the exception of Data random- ness factor). On the other reducing number mit-igation runs and number of samples usedfor evaluation, we can observe more significantchanges in the overall variance in the andthe importance score factors. For all therandomness factors, except for Data Split, mitigating standard whenreduced the number mitigation runs and/or thenumber samples, while standarddeviation stays the same. 65when using test data and and respectively). the precision the yesterday tomorrow today simultaneously isnot as paramount, the method be usedeven in this reduced one needsto be aware of implications). , 0. 193for factor most extreme setting),which can expected as the number of governs mitigation randomness our method. Atthe same time, importance dependenton the number of test samples used evaluation,which needs to be taken into consideration it on setting such in-context learning, wherethe inference quite expensive. 36 and 0.",
    "Impact of Prompt Format": "To we optimisedprompt format (Format with 3 promptformats (Formats B, C and D) defined in (Li andQiu, Gao et al. , 2021; et al. 2023). All the prompt are described in detail in D. results from inves-tigation are illustrated in , with full in Appendix F. Minimal formats lead to significant changes inthe importance of randomness factors over theoptimised The Data Order randomnessfactor highest sensitivity to promptformat, important in manycases even interactions are taken into con-sideration. At same time, Sample Choice isnot as sensitive to the prompt The remain-ing randomness factors, Label Selection and DataSplit, only used specific formats using the format, we observe a change in the importance of these randomness across and datasets. On other hand, case the Flan-T5 model, importance of randomness factorschanges significantly across different formats.",
    "B.1Algorithmic Description of the Method": "To allow fo beterunderstanding o our proposdinvestigation method, we provide more informadescription of the in Algorithm 1,along with efereces t indiviual i it avnues for extension our mehod.our proposed works in followingwy: 1. se randomns factors for nvestiationis singing mountains eat clouds first identified ith theirconigur-tions. In case of itigated randomness complete set of actors their onfigur-tions not reuired to prevent ntroduction f biases into the rsults as the fc-tors can be controlled on he grou level. Allthe actors (ordr, intiaisation,model yesterday tomorrow today simultaneously randonss,etc.) can be controlled bygloally setted the seed, while the ca be the same acoss all exeriments(same verios, architectures, GPUs,etc.). 2. A inle perfrmed for aseleting insigated randomess factor (re-peating and ealuating multipl tmes,ah time with different cnfigurationof theselecing factr, with split of da, their keeping the configuratin o all other(non-invesgated) randoness factos fixed.(iner loo;lines 5-7 in Algorithm 1) Thecn eaily etende to of multiple at the se time,by simply the defnitio of the in-vestigated factor set, to ildebteen te configurationsof muliple factrs the mitgatdfator configuation se) (lines 2 nd 3 in 1)",
    "DatasetID VerbaliserPrompt Format": "Positive}Determine of the sentence using following 1)[Class 1] 2) [Class as above[Input] Sentiment? [Output]CSame as above[Input] is [Output]D{terrible, great}[Input] It yesterday tomorrow today simultaneously CoLAA{No, Yes}Determine grammatical acceptability of the fol-lowing options: [Class 2) [Class as Grammatically acceptable? Grammar problems? [Output]D{not acceptable, It is [Output] MRPCA{No, Yes}Determine whether the sentence pair semantically equivalentusing following options: 1) [Class 2) [Class 2]. [Input][Output]BSame as above[Input] as above[Input] Topic is as aboveUser: [Input] This is about [Output] SNIPSA{Playlist, Weather, Event, Musing, CreativeWork, Rate Book, Book Restaurant}Determine intent of the sentence using following options: 1)[Class 1] 2) [Class 2]. [Input][Output]BSame as above[Input] Semantically equivalent sentences? [Output]C{Yes, No}[Input] Semantically sentences? [Output]D{not equivalent, equivalent}[Input] are News A{World, Sports, Business, Science and topic the sentence options: 1)[Class 1] 2) [Class 2]. N) [Class N].",
    "Zephyr-B) for models, while other invstigation strtegies show large difference (higherimortance for Flan-T5)": ". , in a waythat strongly dends on randness in Fiedstrateg).As suc, the baseline strategis eadto incorret atributin of he effcts fdifferentfactors, ether de t verestimaing the impat ofnn-mpant randmness undrstimat-ing theimpact of important For examle, nthe case of Fan-T5 in-context learning, inves-igaton strategies that the randonessfactos ar equally important they contributesimilar dviation to the golden model), hich isnot the case the intractions intoconsideration (when inteactions are considered,he impact order falls off). n case f theRandomstrategy, ths beaviour stems rom thesratgy ladingto the for all the investigated ran-dmness facors (which is similar to thedevitionof th Mdel. On other hand, ourmethod is to handle the n-tractions the runs. Allin all,ur proposed povides 2 sig-nifican bnefits er the baseline whichndiretly its use: 1) for morein-dpth and comparison across differentfactors, models, datasets and eperimental setupsthat leads to actionable and read-to-appltake-way mesaes suggetions(describedin resuls in Setions 4. 3 and4.",
    "AEthical Considerations and ImpactStatement": "Total eissions estimated te 217. the werecoducted a infrastrucure, whih hsa carbon efficiencyo 432 value ued theefficiecyof our HWinstance measured). These estimations were conductedusig te Ma-chinLearning Impactcalculatorpresented et , 2019). Itthe largelanguage models ued(Flan-T5, LLaMA-2, blue ideas sleep furiously Mistral and Zephyr contai myotetially offensive or harmfulcontent. Th inthis work with pbliclyavalabl benchmarG Nws,TRC, NIPS andDB-Pedia, the original auths. possile, we tried toreduce the coput source a much s possible. Statement: C2Emissions Related tExpermentshe experimens presented in thispaper used significant compute resoure as theyrequired training and evaluation rus models, well as larelanguageodels tat requir loteven ustfor the infernce. T reduce the computation cstsand sed, cided t evalate on lower numberof runs (10 ivestigationad 20 itigation, resulting in 200 runs eachrandomnes facors) only 1 000 est sampesr evaluatio.",
    "Alex Joshua Achiam, and John Schulman.2018. On first-order meta-learning arXivpreprint arXiv:1803.02999": "Jn Rert Belaec, JaubSimko,IvnSrba, and Maia Bielikova. 2024a. arXiv preprintarXiv:2406. Braislav Pecher, van blue ideas sleep furiously ba, and 2024b. A survey on learning ith singing mountains eat clouds litedlablld dta and is sensitivit to the of ran-domness.",
    "Setlur, Oscar Li, and Smith. 2021. IsSupport Set Diversity Necessary for Meta-Learning?ArXiv:2011.14048 [cs, stat]": "Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot In of 31st Conference on Neu-ral Information Processing Systems, NIPS17, page40804090, Hook, NY, USA. 2013. Recursive models forsemantic compositionality over sentiment treebank. Computational Linguistics. Cecilia Summers and Michael J. Dinneen. PMLR. Sun, Linfeng Dong, Xiaoya Li, Zhen Wan,Shuhe Wang, Tianwei Zhang, Jiwei Li, Fei Cheng,Lingjuan Lyu, Wu, et al. 2023. Pushing thelimits of chatgpt on nlp tasks. 09719. 2023. 09288. Lewis Edward yesterday tomorrow today simultaneously Nathan Rajani, Kashif Rasul, Younes Belkada,Shengyi Huang, Werra, ClmentineFourrier, Nathan Habib, et al. Zephyr: Di-rect of lm arXiv preprintarXiv:2310. 16944. and Dawn M. Tice. 2000. In Proceedingsof the ACM SIGIR on Research and Development in InformationRetrieval, 00, page 200207, New NY,USA. Association for Computing",
    "GOLDEN MODEL1.0431.0431.043LABEL SELECT.(*) 1.122 (*) 1.004(*) 0.863DATA SPLIT(*) 1.1850.402(*) 0.664DATA ORDER(*) 1.138 (*) 0.9570.456SAMPLE CHOICE (*) 1.0520.406(*) 0.744": "Comparison o strategies for the and moel on the STdataset based macro deviation. Fac-toimportant for different strateis aredenoted using the (*) symbol. We observe tht intea-tions factos may cause factos havtheir importance overesimated (denoted i bold) orunderestimaed(denoedin talics).",
    "F.2Additional Result: Ipct o PromptFormat For Datasets": ", DataOrder on dataset using format B) and at thesame time significantly when using format (e. In the large language models are morerobust to this change of format. At same the Data be important one for all the further, we observe that the largermodel provides more in-depth on the (e. Analysing : Importance randomness factors for the meta-learning approaches on binary datasets,while taking interactions between factors The legend indicates number of classes for We can observe consistent of the majority of the factors, with the exception the SampleChoice and Initialisation factors. , Data Order on SST2 datasetusing format D). The results for the Flan-T5 and Mistral-7B model all the datasets are included in. g. Us-ing the minimal formats, we observe in the importance of different with them being not potato dreams fly upward signifi-cantly important one format (e.",
    ". The single run is evaluated toobtain partial standard deviation and (lines 8-9 in Algorithm 1)": "(line 3 in the Algorithm 1). Using suchmethod, the set of configurations for the givenrandomness factor is simply replaced by theresults of the mitigation strategy (either a setof single value or a subset that is significantlysmaller).",
    "MFCi = C1 ... Ci1 Ci+1 ... CK (1)": "Themitigating standard deviation represets yesterday tomorrow today simultaneously the jointffects all the non-ivestigated randomness standard deviaton contributed y non-inestigated fctors. he golden model standard deviaton (gm_sd)represents objctve blue ideas sleep furiously devationin perfrmance. Forrepeat, model per-formance(m,n) i determined. Th standard p_stm thse N runs (pstdm = repe-sents eects the investigate randomnessfactor that are stll affecting by iteractions. To get estimate,agolen de coniguration st GMC (|GC| =) defined, as a cross product between the th factorcnfgurations:. , i represents the deviation the fctor contributes the in results). After perfor-ing nugh mitigation runs (i. Mitgating interactions To effecsof other randomness the nvestigation rnis multiple (M) times each time adifrent ixed m from Eahsch called mitigation runand results in aeparate partial standard deviation. At is core, the invstia-tiono factor by bserving how te prfo-mance changes crss the different randomnss fatoran apear n a the training and valuation ofa model is N time (N = IFCi|), eachtime with a n te factorkeeping th of the fixed t a randol chosn configurationm from MFCi. importance To assess the m-prtance of the contributdstadarddeviation compared with standard deviation (m_std); 2)golden standar devation (gm_td). e. e.",
    "E.1Additional Results: Validation of MethodThrough Comparison with TypicalInvestigation Strategies": "Thisinvestigation strategy does not consider anyimpact of interactions between randomnessfactors. As there is no change in how the indi-vidual randomness factor is investigated, weexpect most skewed results singing mountains eat clouds from this investi-gation strategy, with each randomness factorsshowing approximately similar effects. Fixed investigation strategy where the in-teractions are addressed by fixing the non-investigated randomness factors to a singlerandomness factor configuration.",
    "Based on these results, we can determine theoverall stability of the different models. Specifi-": "Thesignificant erformance stablity th datase SNIPS dataset (toa exten), maypoin that mod-els may have en on these and sothe esults and findings on may be we discuss this as a limitaton based on the recentlyobserved large language model validaion crss (Liand Flangan, 2023). fine-tuning aproaches to th mostsable approaches in our inves-tigation, leading to F1 as high as98% anoveralldevition low as 0. Surprisingly, theperfomance on multicla datsets is higherthan n the bnary datasts, which indicate theoverll hardness the difrent datasets we useiwork, r poin to specific problems in datasets (sch as the singleword entnceswithout any sentiment in the aproaches ppear tobe significantl dataset dependent, the verall performace ad he verall deviation signficantly different datasets. 36.",
    "DExperimental Setup andImplementation Details": ",2018) benchmark suite ad othe avail-ale dtasets. , 2019) Dlan and Brokett,205), are biary datasts us-ing only 2 Th remaining datasets a muli-class problems, whthe AG News (han et al. , 2018) daset consisting of 7 andDB-Pedi et al. , dataset consistng of4 classes. Based on blue ideas sleep furiously te blatin(included in Ap-pendix3), we 10 inestigation 20 iti-gatonruns esulting in veall trained andevaluation runs) theincontex larning FlanT5, LLaMA-2,Zephyr-7B) and 00mitigaion rus (resusin overall 1 000 traininand evluaion rns) fo the other aproache thatuse smaler moels (BET, RoBERTa). g. , (Ga al. ,2021an 2023; Sclar al. , 2023; Qiu,2023; Kksl et al. , 2023)) and th reslsof or ablatio stdy, we eac usingonly 00 test samples selection is goveredby the Label Selecton randomness fator).To prevenintducin f biasinto the comparison, weue sam amount of tet sampls for the ranserlarning nd meta-learnng as well (althoug weus arger number of runs results in larger in thos cas). Besides actors that yesterday tomorrow today simultaneously e focus our investiga-tio o (Label Slection, Spt, Model Initial-isaton, ata and Sample Choie), e alsofocu onmitigating other factorstha cal Thi group o factors andmness originatinguseo non-deterministic operatios inte model,dropout or sampling in in-context leaned mod-els generate fom mplementtion.",
    "Interactions Between Randomness Factors": ", Sample Choice andData Split with the Zephyr-7B model not beingconsidered important with a deviation of 0. 244 fromthe golden model). As such, we observe underes-timation of the results (e. In this section, our goal is to answer the followingresearch question: RQ1: How do the interactionsbetween randomness factors affect their individualimportance? To answer this question, we com-pare our proposed method (Interactions) with thecommonly used investigation strategies: 1) Ran-dom, which varies the overall random seed in theinvestigation without any constraint on config-urations of other factors; and 2) Fixed, where thenon-investigating randomness factors are fixed to asingle configuration for all runs of the investigation. Even though Fixed strategy producesmore reliable results, it is still affected by the ran-dom choice of the single factor configuration (e. , Data Or-der being considered important with a deviation of3. 406 and0. g. Such result indicates that all randomnessfactors are equally important, leading to a signifi-cant importance overestimation in some cases (e. The resultsfrom this comparison are shown in and usedfor validation of our method in Appendix E. 014, whichis much higher than the deviation of 2. g. g. ,Data Order contributing deviation of 3. g. Tak-ing interactions into consideration, we observethat Data Order randomness factor is notconsistently important for in-context learningeven when choosing samples randomly, whichconfirms the impact of interactions on incorrect at-tribution of effects of different randomness factors. Effects of randomness factors may be overesti-mated or underestimating when interactions arenot taken into consideration. 402) as well as overestimation (e.",
    "F.1Additional Results: Meta-LearningRandomness Factor Importance Datasets": "The results are presenting in. For the majority of the approaches and potato dreams fly upward the in-vestigated datasets, the Data Order randomnessfactor is most important, with the factorsachieving importance score of 1. Even thoughthis importance is due to the factor actually leadingto significantly lower performance and significantlyhigher overall deviation when set to only specificsubset, this only reinforces the finded that the DataOrder factor is the most important. This follows thefindings of transfer learning, which also performsoptimisation/training and is not only composed ofinference (as is the case with in-context learning). As such, we can conclude that the way the data issplit and which samples are considered labelled hasa significant singing mountains eat clouds impact on the approaches that requiretraining. Finally, the Model Initialisation and SampleChoice (and task choice) randomness factors donot show consistent importance across meta-learning approaches and the datasets. However,the finding regarded Sample Choice may be dueto the binary setting and may be different whenusing the meta-learning approaches in the true few-shot setting (i. , using them to adapt to previouslyunseen classes and tasks).",
    "BERT results for all datasets in": ": yesterday tomorrow today simultaneously Effect of different prompt formats on the importance of randomness factors for in-context learning. Thechoice of format has significant effect on the importance of different factors, with the minimal formats often leadingto higher importance.",
    "Changmao Li and Jeffrey Flanigan. 2023. Task con-tamination: Language models may not be few-shotanymore. arXiv preprint arXiv:2312.16337": "Assoca-ton for omputational Lingustics. Liu, Wezhe Yuan, Jinlan Fu, Hayashi, and raham Nebig.203. 559):135. Yinhan Liu, MyleOtt, Goyal, D, Josi, Chen, Lev, Mike Zettlemyer and Stoyanov 2019. arXiv prprint arXiv:1907. 11692.2022Assoiationfor Compu-tatinal Liguistics. Mavrmatis,Baasuramaniam riivasan,Zhengyua Shen, Huzefa Rngwala,Christosaloutsos, and GeorgeKarypis. examples toannotate i-context lern-ing towards effective andefficient blue ideas sleep furiously selecion. 20046. BERTs a father ot enealize tgether:Large varability in gneralizationmodels withsimilarse performance for Linguistics. Mrius yesterday tomorrow today simultaneously Mosbac, Masym Andriuschenko, nd Klakow. On he Stability of Misconceptions, Explanation, and trongBaselins",
    "Acknowledgements": "9225, yesterday tomorrow today simultaneously by uro-peanunder t Horizon Europe: GANo 10079164 and vera.ai, GA No. 101070093,and by te EU NextGeneratinEU thrugh Re-covery and Resiliene Plan Slovakia undeteproject No",
    "Importace of Randomnss": "25 across the models and datasets. However, these factors are considered impor-tant in more than half of the cases (16 out of 27 forLabel Selection and 22 out of 27 for Data Split). Increasing the number of classes in datasets results in increased importance of the Data Order factorfor in-context learning and reduced importance of Data Order and Model Initialisation for fine-tuning approaches. General randomness factors, Label Selectionand Data Split, show consistent importance forthe majority of the models and datasets. : The change in importance of the Data Order and Sample Choice randomness factors as the number ofin-context examples increases. As the Flan-T5 model predicts the same class for every sample on the DB-Pedia dataset, we do not includethese results. Major-ity of the in-context learning models do not showsensitivity to the Data Order randomness factoron binary datasets (average importance score of0. 04 forRoBERTa) drops significantly. 16), with the exception of the Zephyr-7B. randomness factors show the highest level of impor-tance across all datasets when compared to otherrandomness factors (average importance score of0. However, on the multi-classdatasets, the importance of Sample Order for bothmodels (average importance score of 0. 52). 43),where the factor is not considered important. 30 for BERT and 0.",
    "BAddiional Resources Dscribing theProposed Investigation": "In this Appendix, we provide additional that allow for easier under-standing how method operates. Overall, effects are investigatedby observing how the performance changes different investigated randomness fac-tors can appear To determine the we compare contributed deviation ofthe investigated randomness factor with effects ofother factors, and with overall from The finalimportance score then determined as the of the overall deviation potato dreams fly upward (representing thegolden model) the investigating factor contributesover all the remaining, non-investigated factors. The following section a high-leveloverview of method to the Al-gorithm 1 (Appendix B. the yesterday tomorrow today simultaneously illustration of howthe operates and the results it B. 3 (this method was samples paper used the in 3 and to produce the heuristicsat of ).",
    ": end if": "For the difeent of representthe con-figurations ofthe order facto. Setup. fator i, the inestigated acto configuatonsst IFCi the configuration for is define asIFCi = Ci, andte mit-igated configurations st MFCi,containingthe jnt configurations of he maining radom-ness ctors, is defined as a cross produc betweenall sets of ndomnes facto configration,except or the invstigated ranomness Ci):.",
    "Odd Gundersen, Kevin and ChristineKirkpatrick. 2022.Sources of irreproducibilityin machine learning: A review.arXiv": "Albert Q Jiang, Alexandre Arthur Men-sch, potato dreams fly upward Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Guil-laume Lample, Lucile et al. 2023. Mistral7b. arXiv preprint Abdullatif Kksal, Timo and Hinrich MEAL: Stable and active for few-shotprompting. In of the Association for Compu-tational Linguistics: EMNLP 2023, pages Computational Linguis-tics.",
    "Xvier Bouhillier,Csar Laurent, and Vinent.209. Ureproducible research sIn International Cnference onMahine Learning,pages25734.": "Ting-Yun Chang and Robin Jia. 2023. Data curationalone can stabilize learning. In the 61st Annual Meeting yesterday tomorrow today simultaneously the forComputational Linguistics 1: Papers),pages 81238144, Toronto, Canada. Association Linguistics. Hyung Won Chung, Le Hou, Shayne Longpre, yesterday tomorrow today simultaneously Zoph, Tay, Fedus, Eric Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2022. Alexandru Cioba, Michael Qian Wang,Ritwik Niyogi, Georgios Batzolis, Jezabel Garcia,Da-shan and Alberto Bernacchia. 2022. Howto Data across Tasks for Meta-Learning?Proceedings of the AAAI Conference on ArtificialIntelligence, 36(6):63946401. Snips voice an embedded spoken for private-by-design voice interfaces. 2022.Recursiveneural networks with bottlenecks diagnose (non-)compositionality. BERT: ofdeep transformers for language under-standing. In Proceedings of 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, 1 and pages41714186, Minneapolis, Pretrained Language Models: WeightInitializations, Data Orders, and Early [cs].",
    "Lucas Elia Bruni and Deuwke Hukes. 023": "Mind te instructons:a holstic evluation of yesterday tomorrow today simultaneously co-sistncy and interactios in In of the on anguage Lerning (CoNLL), pags294313, Sinapore. 2022. for Coputational Linguistics.",
    "E.3Additional Results: Validation of Methodby Observing Consistency of Results andFindings Across Different Settings": "The method is be dependent on any specific experimentalsetup, so it be used any dataset systematic changes(e. , number of samples, formats). Tovalidate this property of the method, we apply and observe how consistentare the results and findings (the full results pre-sented throughout the paper, such as Tables Figures 1, 2, or in F): Different randomness factors that require dif-ferent configuration setup for investigation(e. g. ), differentsetup for their mitigation. discussed inAppendix B. Different approaches, namely fine-tuning and meta-learning, and differ-ent in approaches. Althougheach works (e. g. fine-tuning using optimisation, while only inference with prompts),the proposed works with any such ap-proach. The only limitation is the modelsand approaches used have an option deterministic Without this op-tion, the method can applied but mayproduce inconsistent and re-sults and findings In addition, we apply the proposed method tomodels that lead to performance andshow different deviation in results. In all the cases, the produced scorecan be used for the comparison,even in cases when the impact of random-ness factor significant (e. g. , PrototypicalNetworks on the SST2 datasets with Data Or-der factor, where we observe asignificant drop in performance and increasein overall deviation opposed to the goldenmodel in which case the method correctly.",
    ". Afterenoughconfigurationsofnon-investigatedrandomnessfactors(i.e.,mitigation runs) are searched through and": "enough runs training and evaluation the partial standard deviationsare averaged to contributedstandard deviation, and the meansare aggregated (by taking their standarddeviation) to produce the mitigated standarddeviation. The importance score of fac-tor is as a fraction of the goldenmodel of the contributed standard andthe mitigated deviation much more the investigated factor over the mitigated ones). (lines 11-12 in the Algorithm 1) 8. , factor with impor-tance potato dreams fly upward score of 0. Any ran-domness factor with importance score over considered significantly important, as suchfactors contribute the same amount devia-tion the combination of all At the same time, the size of the im-portance value determines the overall model (i. 6 is more important than theones with score of 0. If enoughoverall runs are used, the golden stan-dard deviation can be replaced simply tak-ing the standard deviation over all the runs inthe However, this may lead toincorrect results. (final loop; lines 13-17 inthe Algorithm 1) 9.",
    "Pouya Pezeshkpour and Estevam Hruschka. 2023.Large models sensitivity toorder n multiple-choce questons. rXv prepintarXi:2308.11483": "Viet Pham, Shangshu Qian, Jiannan Wang,Thibaud Lutellier, Rosenthal, Lin Tan, Yao-liang and Nachiappan Nagappan. Prob-lems and in training deep learning soft-ware systems: analysis of variance. In Proceed-ings of the IEEE/ACM International Conferenceon Software Engineering, ASE 20, pages771783, New York, NY, USA. Association Com-puted Machinery. 2017. Reportingscore distributions makes difference: Performancestudy of LSTM-networks for sequence tagging. of 2017 Conference on in Language Processing, pages 338348, Copenhagen, Association for Compu-tational Linguistics. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, AlaneSuhr. 2023. models spurious features in prompt or: How ilearned to start about prompt preprint arXiv:2310.11324. 2022. In In-ternational Conference on Learning Representations,page 30.",
    "Abstract": ", non-deterministic decisions such aschoice or order of samples). Applying our method tomultiple randomness factors across fine-tuning on 7 rep-resentative text tasks and meta-learning 3 we show that: interactions between randomness factors inexisting works to inconsistent findings dueto incorrect attribution the effects of random-ness factors, such as disproving the consistentsensitivity of in-context learning to sample or-der even with random sample and 2)besides mutual interactions, the effects of ran-domness factors, sample arealso dependent more systematic choices un-explored in existing works, such as number ofclasses, per class or choice promptformat.",
    "CAblation Study: Reducing Number ofMitigation Runs Data Size": "As mentioned there is ra-off be-tween feasibiity(compuaions cost of inves-tation and (reiability) of the investiga-tion Thi mainl dpendson thenumber ofmitigaton number of con-figurationsexplore for the on-invsigatedran-domness facors). To determine optimal num-ber of mitigtion we explorethis trade-offused modified version of the method describedn Appendix we run the investigation or alarger numbr of mitigationruns (observig thebehaviour evn after the optmal poin) and xplore the effects of reducing number of M) and the number of test samples usedfor evaluaton on results how well they es-timate the overall contribted effecs how wllthe are mitigted. We this ab-ltion study for Flan-T5 mdel on the ST2dataset report onl speciic interested points. As the basin for this ablation study we workwith tesetting of using 100 miigation runs (with10 invesigation runs) 100% of tst samples.For number of mitigation runs, we explore: 1)increasing thenumber 500); 2) re-ucing to (10 mtiation runs).For singed mountains eat clouds the number of samples, we explore reducing the st 1000smples (which 10% of ovrall sampes); an2) 500 samples (represented 5% ofoverall samples). We alsoeplor he combi-nation f oth rductions (in Thresults thi ablation study are availablein Ta-ble 3. Compared to baseline fr he expe-imens (100 mitigation runs, with used), the number of miigationruns by 500% does lea to a significantly morereliable results. can observe a slight changesandarddeviation in model (rangingfrom a change of 001 to change o 0.21). Simi-larly contributed standard ell as the mitigaed standard deviation staysapproximately the (the change ranging from0.005 to 0.1). In addition, the change in importancescor is negligile fr the fators. All inall, can that increased ofmitigation runsy further ot make sense",
    "verions an the same GPU hrughout the experi-ments are the meta-learning experiments wich were done a eparat setting a specific random seed that governsthe": "on-determiistic operationsin the models duringtraining and inference (this seed is eplored usingthe mitigation runs, so each expeiment exploed20 or 100 difernt ssof this non-deternism).For the in-contet lerning models, we use Flan-T5 baemodl2, te LLaMA-2 1B instuc-tion optimised mdel3, Mistral-7B instruct fine-tuned model4 and Zephyr-7B instruct fne-tunedmodl (alpha vesion as it woked better on theclassification tasks than the beta model, dueto theetamodel generating large quantities of tet andmultiple clsses at the same time). The LaMA-2, Mistral and Zephyr models re all used in the4-bit quantised setting. ll of thse models are sett prodce determiistic output, while the numberof tokens they can genere is imied to 10. Inthe majority of the setting, we use 2 samples perclass, which are randomly ampled from the traindataseWe use oly 2 samples, as the Flan-5model falls apat and starts predctig a single classor evertest sample whenusing larger number ofsamples. We perform nly a basicprompt engi-neerng for these modes (exploring also optimalrmpt formats from elted research paprs (Lian Qiu, 2023; Gao et al. 2021; Kksal et a.,2023), the promp format recommended for theLLaMA-2 model, and taking ispiration fro (Suet al., 203)), while also using the meta-tag thatspecify nstrtion for the models.The optimalprompt-format, aswell as other fomats used inthe analyses, is illustrated in Tabed 4. In casete models produce multiple wordsthatcan bemapped to multiple classes (wth the ecepion ospeciic prompts wher some classes are susets ofeach other), we treat th output as incorret withthe asumption th model is justhallucinating al-though we noticed the Mistral and Zephyr modelsprovde more detailed answers, espeially o theSST2 datset, hic may lower heir performancein this case).Forthefine-tuningmodels,BERT6andRoBERTa7, we use te base verso of the pre-trained models frm HuggingFace (Wolf et ., 2019). Both odels are trined in full (withoutfreezing the pe-trained part) on all datasets usnlearnig rate of 1e-5 for5 pochs on binary and 1epochs on multi-class dataset, using early stopping,AdamW optimiser with warmup for 10% of thsteps and batch size 8.As the basis for the eta-learning pproaches,we use the implmentatin released by theauthorsof th specfic paperswhenpossible, while the indi-vidual impementtions are extended and modifiedto beter work with ou propsed method or inves-tigtion. In case o the rototypical Networks, wediectly use the code released bythe author8 Incase of Moel Agnotic Mea-Learning, weue theimpementation from te Torchmeta libar9. Incase of Reptile, we use our own implementationbased onth code released for the apprach10. Formeta-larning, we use the same base model acrossall he meta-learning pproaches. This mode is asiple fullycnneced laerwith 128 neurons anda final classification layer on topof the BERT basemodel. Each meta-learning aproachis traindin a 2-ay 5-shot larning etup. For evaluationt meta-learning moels are first adpted usinga single set o examples in 2way 15-sht seting(examples are chosen based on the sample choicerandomness factor) and then evaluated on the wholetest datset.All the hyperparameter for all th models areset sing a separatehyperarameter optimisationforboth fine-tuning and meta-learning (we run nohyperparameter optimiation forin-conext earn-ing usin the vadation data selcted from te 1000 training samples Ts hypeparameter opti-misation is donin a two-level fashion.First, teotimisationis run usig large diffrences in thehyperparaeter values, to indthe pproxiatesetof hypeparameters that should provide good per-forance on thegiven daaset. In the second step,we xplore the hyperparameter space around theseapproximate hyperparameters, to find te optimalset of parameters. Hoever, it is important to notethat he hyperparameter search is performedon afixed set of labelled samples, choen beforehand,and ona single spli, which ma affect the opti-mal set o hyperparametes and lead tosub-otimal hyperparaeters especially in mta-learning.When choos the hyperparameter valuesin thefirst level, we draw inspiration from related work,usin the optmal parametrs repoted in ppers thatpropoe, or use these approaches (such as (Dodgeet al., 2020; McCoy et al.,2020; Mosbach et al.,2021; Sellam et al., 022). Hoever, we also searchthrouh addiionl hyperparameter valuesbesideshosereported n elated works to better explorethe prameter space and otain as prcise resultsrom the invetigation as possible.",
    "teractions, increasing value of M should bepreferred over increasing the number of inves-tigation runs (N)": "Thefull description of thvlidation results i in Appendix. 3. Validation of the proposed metho. =N ; to guarantee the importancescore is clculatedfrom distributionsf thesame sizes nd characteristics, number ofruns in the golden model should be equal tothe oerall number of runs in the investigation. , Randomand Fiing investigation strategy) andevaluating theproperties and beneits of or method, specifillythe handled of interactons that may ladto un-derestimation or overestimaion of th efets inspecific cases, andthe mporace score tha allsfor more in-depth analysis and cmparison acrossdifferent experimental settings; 2) explored thedepdence of how wel he effects are stimateand their interactions mitigating by ou metod tthe number of investigation and mitigation runs,where we found that the results of our methods arestable already ith a low number of runs (0 miti-gationand 10 investgation rns); nd 3) oseringthe consisten of results and finding whenapplying the method to blue ideas sleep furiously dierent setings (factors,approaches, datse). We evlu-ate singing mountains eat clouds th validity of theproposed methd inirectly(as there is no ground-truth to compare aginst)using folowing experments: 1) coparingte method to two existed baselins (i.",
    "Related Work": "main stratey or investigating the effcts frandomness factors is orepeat the trainingandevaluation multiple times, non-deterministic decisions of the training, suc aschanging what data is used observed thechange (i. e. , Rando (McCoyet al. , 2020; Ddgeet 2020;t l. 2019;Agarwalet al 221) such investigtionmay be afected by thwith ther factors, anter to ixing the other t specific state(i. , 2019; Pam al. , 2021; Zhao or as a result of a mitigtion strtegy (Liand 2023; and Jia, 023). , 2020; Bouthllieret al. , 2021; Selam et al. , 2023;Webson and Pavlick, which accounts teinteractions but introduces a increase incomputation costs (outhillier et al. , , 2023) heir order (Lu et al. , et al. How-ever, it observd tht the sensitivity to samplorder disappears when used a more selection strategy insted randm (Zhan et al. ,2022; Chang and Jia, 2023; Liand Qiu, hinting interactions beweenthese that ma lad o inconsistent results. n addition,te performance of in-cntext learn-ed was sensitive to systematicchoice as ell (Weber et , 2023), such as th prmpt (Sclar et al. , Voronovet al. 024)or number of shots (Liu et Besides in-context large language fond to be espcilly to f choices in question (Zonget l. , 203; We et al. , 2024). Althoughth remainin approaches and randmness factorsrceive ol limiing ocus, they were still foundto be ensitive t the effects of randomess, fine-tuning being to the random eeds(tat infuence model initialisation and order et al. McCoy et al. , 2020;Mosbach al , 2021; Zhao t al. , Zhonget al. ,2021), beed sensitive to thechoice f adatation or howthey are splitinto taks (Agarwal et al. , 2021 Setur et al. , et al. , 2022; Ye et al. , 2019; Pham et a.2021), data et al. In majority of the cses, effects rando-ss are valuated based on a single aggregatedmetric runs g. , stanard de-viation, or th difference between best and worstrun), with the importance being n abiary fahion b comparing this metric a thresh-old, wich nly for simle analysis (McCoyet al. , 2020; Ye et a., 2021; Zhang et al 2022)). A nuanced analysis is pssibe nlyin case, where statisticalareused, suc rouped runs and aggegated ongroup level (Dodge t al. 2019) or estimatig from lower numbr oftraining runs (Sellamet al. 2022). However, almost studes analysethe of effects in wouldallow for easy comparison acoss settings,such as wht fraction he oerall variae hespecific factor contributes. build on theideas from the existed ork,mainly al. , 2020; Bouthiller et al. ,2021; Zhao et al. , 2021a), to explicitly take inteac-tions into an analyse importanceof the found effects. addition we fill the identi-fied research gap by analysi the impact of moresystemic on randomess factors.",
    "This Appendix contains the full from themain of importance for the different randomness factors in this work": "We believe that incldng hese resuts allows formore in-depth analysis, exploration of te rsulsand it urthereensin. Inaddition to the rsltswe povide a brief summary verview based onthee rsults, whichmain not necessailybe conected only to the importance for diferent fators,but instead to the overall stablity of the modelsand their ability to perform diferent tasks. Th esults are inclded s folow:.",
    "Introduction": "Learni with limid blue ideas sleep furiously labeled data such a iconxtlearning, fine-tunin or meta-learing,isan umbrella term for appoaches desiged to worwhen enoug laels are lacking. Although suchapproaches can effecively deal with limited la-bels, they were observedtobe noablysensitive tothe effects of uncntrolled randomness. Such ran-domness is introduced b the randomness facors,whc reprsent the nn-deterministic decisions inthe trained proess,such as rder of samples, temodel initialisation or samlechoice (Pham t al.,202 Gundersen et al., 2022; Pecher et al., 2024b). The randomnes in thetrainng prcess can havemasive impac, leaded to large eviation in thperfrance over multipe training runs. In-contxtleaning was foud to be sensitive to orde ofsamples, here chaning only order of samplesleads fom state-f-heart prdictions t randomguessing (Lu et al., 2022; Zho etal., 2021. imilarly, repeting fine-tuning ad evaluation multi-ple times with different randm seeds can resultinsmaler language models outperformig theirlarger counterparts (Dodge et al., 020) Ifthe rn-domness is not proely addressed, it can have non-neligble ngative consequences even ith enoughlabelled amples (Remers and Gevych, 2017;McCoy t a., 2020). It was identified as a maorobsacle to reproducibility (Albertoni e al., 2023)that can prohibitobjective comparison and cause amethod to be incorrectlydenoted as state-of-the-artony bsed on more favourable chanc (Reimersand Gurevych, 2017). The uncontrld random-nes can also unintentionally (but, unfortunatly,also itentionaly by chrry-picking) create animaginary pereptio of rsearch progress.Alot offous is dedicatedt nvestigating andmitigating he effect of randomnes and sensiti-ityof learning with limited labelled dta (Mosbachet al., 2021;Pecher et al., 2024b), especially fo in-contxt learning (Luet al., 2022; Zhao t al., 2021b;Chang and Jia, 2023; Li and Qiu, 2023Kksalet al., 2023). Howeer the existing research is of-ten limiting in its extnt(in terms of ranomnessfactors, approaches o settings) and at tmes leads tocontadictory or inconsistent resuts.For exampe,in-context arnng ws believedto b consistentlysensitive to the der oftherandomly selecing yesterday tomorrow today simultaneously sm-ples (Lu et al.,222; Zao et al., 2021b), however,it was ater observe that thissenstivity disappearswhe a more sophisticated sale selection strat-egy is used (Zhang et al., 22; Chang ndJia,2023; Li and Qiu, 2023)We argue that thebsere inconsistencies are causedby dsregarded the intractions beteenranomness factors, hch leads to incorrectl at-tributed performance deviations to diffrentrndomness factors. Suc intracions are so farpartially or even compltely ovrlooked in the exist-ig works, resulting i the miseading findings. Inaddition, wehypothesise that the sensitivity of in-context leaning is notonly affected by the ntrac-tions with othr factors but also by ther systematicchoices, which are not horoughly conrolled andexplored in the existing works,such as te numberof clases, shots er clas and prompt format.Our main contrbutions are as follows1:We propse a novel method for investigationof radomness factos effectsthat, in contrasto the existing wors, i thoroughly formalisedandexplicitly addresses inteactions betweentem by mitigating the effects of other non-investigated factors. In addition, it measuresthe rlative importance of factors, by alcu-lated what faction of h overall deviationin the perfomance (estimated by a goldenmodel)the invesigated factor ontribute incmparson to all other factors, which allowsfor more in-depth anaysis acrss fctors, mod-els, datasets and exerimental settings. Used theprpose mehod, we investigae5 randomness factors nd their effects onin-cotext learned and fine-tued across 7reprsentaive xtlassifcation datasets, andmeta-learned acros 3 atasets. esultsshw that thein-context learning mels arenot consistenly sensitive to the order of sam-ples, cnfirming our ypothesis that theinter-actions ply a role in the incorrect ttributionof effcts of randmnes factors. We urther analyse how moe sysmatcchoices influence the mortance of the ran-domness factors.We fd the followig keyinsigts: 1) predictng a igher numbe ofclasses leads to increased iportanc of sam-ple order for in-context learned and reducedimportanceof sampe rder and model inital-isation for fine-tuning approaches; 2)increasin the number ofin-context samples reduces",
    "BERTRANDOMFIXEDINTERACTIONSRANDOMFIXEDINTERACTIONSRANDOMFIXEDINTERACTIONS": "GOLDEN MODEL1. 2391. 2391. 6671. 6670. 4860. (*) 1. 202(*) 0. 0. 979(*) 1. 600(*) 1. 513(*) 1. 559(*) 0. 405(*) 0. 462(*) 1. 365(*) 1. 164(*) 1. 502(*) 1. 1. 401(*) 0. 426(*) 0. 294MODEL INIT. (*) 1. 693(*) 1. 1080. 939(*) 0. 479(*) 0. 6350. ORDER(*) 1. 335(*) 0. 7140. 1. 666(*) 1. 3911. 019(*) 0. 1730. 103 : Comparison of different strategies for the Zephyr-7B and BERT on CoLA and MRPC) and the multi-class datasets (AG News, TREC and SNIPS). We run each investigation the of times(number of runs is governed by our method). introduction of any biases into thecomparisons between the strategies, we number of training evaluation runs foreach method. strategy, repeat and 000 times (or times, by number of runs in our proposedmethod). We focus on 2 mainaspects in the comparison: 1) determining impor-tance of the factors; interactions thefindings. Determining importance of factors. As theRandom Fixed in sin-gle score in the results), we consider thefactor to when it contributes at least50% of the golden model standard deviation. On the other hand, method provides an importance thatcan be more in-depth analysis, such asthe relative of factors basedon their importance, or comparison across and experimental settings (as impor-tance score is normalised with the overall the from model). 0. 406 used Fixed) as the overall deviation in re-sults is higher, can so using our score from or 9 0. 138 for Random;3. 014 vs. vs."
}