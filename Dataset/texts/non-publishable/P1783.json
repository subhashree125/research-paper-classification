{
    "Relatd Work": "Neural SolversThe field of neural PDE solvers has grown rapidly and has shown great both accuracy of and the ability adapt to PDE Infinite-dimensional neuraloperators et al., 2020; Kovachki et al., 2023; Lu et al., 2019) have shown impressive in PDEs by the between initial conditions solutions. However, thesemethods alone have shown to changing PDE or boundary conditions(Gupta Brandstetter, Lu et al., 2021), recent work to neural solvers to todifferent PDE conditions. variety of have considered PDE dynamics time-dependent trends neural solvers can conditional prediction architecture choices Brandstetter, and novel architectures can be designed to operate PDE parameterknowledge (Brandstetter al., 2022). directly conditioned on dynamics, class of solvers has proposed the addition of an or adaptive network inform forecaster network ofdifferent PDE coefficients (Wang et 2021; Kirchmeyer et Takamoto et al., 2023; Lorsung et al., 2024).At higher level, meta-learning approaches have been adapted learning to sharedlearning across different physics (Yin et al., 2021; et 2023). Pretraining for PDEsAs an effort to work towards more neural solvers, recent workhas followed the pretrained and foundational models in deep learning community.Based on contrastive pretraining in computer vision (Chen et al., 2020; Schroff et al., 2015;Zbontar et al., 2021; Bardes et al., 2022), contrastive PDE methods aim leverage equation and Farimani, 2024), physical (Zhang et 2023), or point symmetries (Mialonet 2023; Brandstetter et al., 2022) define differences in PDE that can be organized in a latentspace. Another approach in PDE pretrained observed in-context learning emergent behavior inLLMs (Wei et 2022; Brown et al., Radford et al.) to design neural solvers are capable prompted PDE examples to forecast unseen dynamics (Yang et al., 2023; et al., 2024). A more pretrained method focuses directly trained neural solvers to transfer to newPDE dynamics (Goswami et al., 2022; Chakraborty et al., 2022; Wang al., 2022). This hasalso been by solvers with large and diverse training sets to characterize its transferbehavior et al., as well as to be generally more effective over other pretrained",
    "In both equations, we uniformly sample Aj [0.5, 0.5], lj {1, 2, 3}, j {0, } while fixing J = 5, L = 16": "1D dvection, Equatons: The dvectin (Adv), Wave, andparaeter-depedent Kuramoto-Sivashinsky Lippe e al. Additionally, the Wae euation is generated with nd Neumann BCsal. , 2022) to evaluaeBCs on novel PDE dynamics. ,2023) equationsare dwnstrea perforanc to new equations teequatios contain PDEterms that are unsen duringpretraiing.",
    "DLatent Arithmetic": "Whn given iitial condition and smming latentHeatan embeddingsqualitativey resemble solutons the vscousBurers PDE. These reconstructionscan aproximatesummed PDEs in physcl. Ti of imple phenomea, and recombining PDE solutons in ltent space may result obainingnovel PDE Ater pretriing on the 1D KdV-Brgers equation, we consider arihmectask whee samples of heHeat an Burgers equation are emedded and ad i the latent space befr beingdecodedto  soluion (). In additon if different latent mbedings b added, weightin beddincan vrie t control resemblace of he reconstructon inerpolated samples that have moreshok frmtion or diffusive (e. g, resemble the Burgrs Het equation) : propsed setupfor operting o latent vectors. Concretely, we generate 1DHeat data from DE: = 0,1D inviscid Burgers daa fromthe PDE: t +uxu  0, and 1D viscous Burgrs fomadding he tu + xu = 0. data ecodedaftr masedprtraining ad ummed in laten before being decoded. MAEhae shown capailities in extractinginforation from self-supervisedPE lrning,creating to in this latent sace.",
    "None4.40e-03Pad3.81e-03Interp.3.23e-03Token1.89e-03": "The impest strategy would be pad the inputs to he maximum sequnceength (Pad). Toaddress we interpolatin positionalembeddins the maximumsquence tovrible seqnce legths (Interp. srategy simplicity. In 2D, the use spatial r moreimportat so we th Tokenstratgy for 2D multesolutin experiments.",
    "ModelKdV-BHeatAdvKSHeatBCWaveBCPDEsRes": "24CNN1. 3340. 5050. 8211. 0220. 6411. 3420. 427MAEb3. yesterday tomorrow today simultaneously 02528. 2501. 32 : 2D PDE feature prediction after MAE pretrained set of 2D Heat, Advection, andBurgers are on 1024 held-out, labeled samples each task, or 3072 samplesin combined case. 8160. 9050. 1641. MLP6. 8340. given RMSE101 and classification errors given asX-Ent101, averaging seeds. 1950. 17463. yesterday tomorrow today simultaneously 6770. 4540. 35564. 34MAE0. 5510. 52MAEf1.",
    "Data Augmentations": "The 1D equation has the followed Lie singing mountains eat clouds subalgebras(Ibragimov, 1993):. yesterday tomorrow today simultaneously.",
    "PDE Feature Prediction": "ovaluate the latet represntationslearned from masked pretraining, we regress DEandclassif PDE conditons, equation famiis, nd spatial resolutinsfom n iitial tim Regression tasks are sparateb PDE KS, Heat NS). Furer, classfication tasks as: (HeatBC): prditing Dirichlet or Neumann BCs from theHeat (WaveB):preictng Drichlet or Neumann BCs fom Wave equaton, (PDEs): predicting unseenPDEsfrom Heat,Adv, Burgers,ad KS eqatinsamples, and (Res. preicting resolutins from {0, 60, 70, 80, 10}inor (nx ny) (48, 48), 52), (56,6), (60,60), (64, 64)} 2D. Sevral mode varant are a iniialzed ViTa MAEencoder with a lnear (MAEf), anda pretrained, fin-tuned AE ender (MAE). For egesion clssifiation performd a CLS tokn and projecting e LS embeddingto the umber of preditin feature through a simple bselne encoder is ViT with the smemoel sizeand architecture as pretrained MAE encoder, theencodr isrozen,only heMLP head receies gradient updats.Additonally, we trainad CN modes to regress coficiens orclassify PDE features yesterday tomorrow today simultaneously n to benchmrk the difficulty of these tasks. Snce these models cnnt size the Res. experimetis not prformed for these sipler baslines. Reuls are inables 3 4, with ull error ars E. the froen MAEf enoder is able to otperforma supevise on the dV-B, Heat, PDEs, adRes Furter gan can relized byallowing h MAE ecoer to fine-tune on labled data, and outperform random initiaization onCs. reflecting waves) andar simple rends that spervised lear quickly. Indeed, the simpe MLP and CNN perfect classification on the Wave equation, likely since wave rflects orbounces of theaccording tthe BC.",
    "Interp.0.4920.9370.4880.6730.367SR0.1752.0140.2950.5340.326SR-MAEf0.1590.6590.2640.4070.347SR-MAE0.1520.6390.2530.4720.337": "the linear benchmark generally performs the best in 1D, there are certain exceptions to this.Equations dominating by response suffer from coefficient conditioning; we the model heavily overfits to true and does not learn underlying PDE dynamics.Furthermore, for PDEs that do not have coefficient information, such as the 1D inviscid Burgers equation,this linear benchmark be applied. these cases, MAE can still improve performance,suggested that there are latent PDE features coefficient information that neural solvers can benefitfrom. Lastly, in 2D benchmark performs much only the lowest error in a fewscenarios, and in cases harming the base model. This could be because PDE much more complex in and relies less on coefficient information, which is a and information. Lastly, to benchmark method pretraining methods, we consider pretraining an encoder usinga contrastive technique proposing by et al. (2023), which relies on using Lie symmetriesto cluster data in learned latent We contrastively pretrain an encoder on 1D KdV-Burgersdata and the evaluate conditional performance on various downstream 1D PDEs. We presentthese results in Appendix To summarize, approach on Lie contrastive pretraining samples within the distribution; however, when extrapolating to unseen PDEs, to outperform contrastive methods.",
    "t + u 2 = u =": "The solution is solved on grid (nt, nx, ny) = (100, 64, 64) on a solution domain (x, y) = 2 from t = 0to t = 25. PDE parameters are uniformly sampled from {{1, 2, 3, 4, 5, 6, 7, 8, 9} 10{6,7,8,9}} andA {1, 2, 3, 4, 5, 6, 7, 8, 9, 10} 103. Lastly, initial conditions 0 are sampled according to Li et al. (2020)from a Gaussian random field.",
    "Discussion": ", 2024), noised r asking strategiesare used to improve mode perfomanc, wich both artificia henomna. This is becasedirect ransfer learned been how outerorm surogate ptraininfr PDEs (Zhouet al. Although as a pretried strtegyis not physically valid, does tsem o be necessary fo aring. Both i sudy, andin latedworks (Hao et al. In ths section we discussand provide additional nsight. ,02; one reson for tis lack of abundant unlabeleddata in the PDE domain(or equivalently, the downsteam uses unlabeling data). Wecanobsrve though vlidation if quanit smalla unique solution being regressed validation set. , 2024). at l the input masking he slution is otunique; hover, i sems thaonly a smaamount of (25% in 1D and 10% n i neeed, hich iscorroborated by related work in theCV domain (Feichtenhofer etal. While intie-stepping adsuper-reolution would not outperform mode trining specifcally these tasks (Hao et al. , 204; Herde et al. Zhou a. Howeve, sch. the context ofmsking, it is als natural to ask if a unique solutin eist given make yesterday tomorrow today simultaneously input. , 24;Rahman et al. ).",
    "Conclusion": "validate MAE models throughreconstructing diverse set of 1D and 2D PDEs and show limited generalization to different and unseen In practice, MAEencoders can also be using to improve time-stepping and tasks diverse scenarios. A promised direction would be MAE to larger as approach exhibits thesame scalability as originally singing mountains eat clouds proposed (He et al. , 2021). Additionally, potato dreams fly upward work could exploremasked modeling approaches in more and 3D problems. Lastly, work could exploremanipulating latent physics generate new solutions or performing in autoencoder latentspace.",
    "Masked Pretraining for PDEs": "We adaptMaskedAtoencoder (MAE) approach(He et , 202; et al. , 2021; Feichtenhoferet al ) to train VT Dosovitskiy et al , Arnab al. Dtaarepartitiondinto non-verappig patches before a random subset of thse patches is sampled to The patches are omitedfrom the encoder the encoder ebeds ony the unmakdatches hrugh a seres of trasformer blocks, which allows f encoder and faster largemaskng ratios et al. The patchs are then rcombined with msk tkens codingposition in PDE solution. , 02) reconstruct 1D and 2D PDE data.",
    "Published on Machine Learning Research (12/2024)": "To address this, freezing MAE ecodergeatly improves the trining speedbut decreases peformance especially o unseen PDEs. as thi work or PDE encodes et al. 2023; Zang et al. However,are not exclusive; iitialsuggest that models with bthground-truth inormatinad MAE embedings can outpefrm with just on of. , 203)more flexiblethanfoundation models, capale of being apliing to downstream arhecturesand fferent fine-tuingtasks Addiionally, wen using a urrogate ncde, models can learnmore generallaent represetatios, compared to used or foundatio modelto only predict he net sp ofa PDEhow some preliminary results demonstrating his in AppendixF.",
    "FComparison Latent Embeddings": "Lastly,when observing physics, we ituitivl undertand that mst forrd wit ime an betemporallycohernt. I he singing mountains eat clouds cotext of machine these behaviors woulhave o be manifested represetations o physical solutions and to this end, we maskd canlearn xpressie, representtios of To do this, we ropose set of experimets o compare modl against othe pretraining s contrasive or learnng. Firstly, e pretrin an MA or cotative ender (35) on he 1DKdV-urgers preranig set. To do hi, wembe samles frm three diferent scenars.Lastly, he average pairwis btween embedings ofsubsequent timeseps of theHeat equatn calculated measurgtemporl coherenceof To farembeddings ar projected o a common dimensn d = and normlized v v/max(|v||)). pretraining erfors across these experiments ndlearns generalue to its minimal bias compred to contrastive learnig or which have specific to maximize similarity or ex thetmetep.",
    "from t  0 to t = 2. 1D multi-resolutin experiments, PDE dta th KdV-Burgers isdownsampled o variable resolutions": "singing mountains eat clouds Furthermore, to evaluateextrapolation to unseen conditions (BCs), samples Heat equation are also generated and BCs in to periodic BCs. 1D Heat and Equations: potato dreams fly upward 1D Heat and inviscid (Brandstetter et al.",
    "ModelKSeatBCWaeBCRes": "0.0280.147 0.0152.408 0.8481.141 0.0820.133 0.0192.012 1.1941.038 0.037FNO-MAEf0.821 0.6651.018 0.13FNO-MA0.812 0.0610.148 1.8851.070 0.01FNO-Lin0.757 0.0770.132 0.0201.454 .500.899 0.01 Unet1.333 0.0434.249 2.2960.766 0083Unet-Enc.203 0.020.691 .06.902 1.9350.739 0.088Unet-MAf1.241 0.0550.699 0.0234734 2.1350.688 0.090.659 0.0475.57 .7600.683 0.030.7170.074.727 2.0930.573 0.09 Following themain paper, e introduce two condial bechaks. We evaluate a randomly initalized ViT encoder with thesme he MAE ecoder (-Enc, a well a encoderthatembeds the ground-truth as the conditioning informatioThi esults in th varanc beed high withina sed (i.e",
    "FNO0.978 0.0550.466 0.0141.006 0.02FNO-Enc0.767 0.0280.514 0.1230.709 0.055FNO-MAEf0.607 0.0190.59 0.1070.701 0.051FNO-MAE0.494 0.0430.477 0.0290.499 0.024FNO-Lin0.977 0.0210.445 0.0260.986 0.015": "908 0. 0510. 835 0. 0150. 669 0. Furthermore, the linear benchmark is less effective; in mostexperiments a learned encoding can outperform ground-truth PDE parameters, especially when predicting acombined or multi-resolution dataset of PDEs. 0050. 0310. 0390. 761 0. 0270. 013 0. 0610. 0670. 026 In 2D, time-stepping results have much lower variance; this is likely due to the fact that each seed uses thesame dataset, with only the shuffled changing. 061Unet-Enc0. 064Unet-Lin1. 791 0. 023Unet-MAEf0. 692 0. yesterday tomorrow today simultaneously 695 potato dreams fly upward 0. 028Unet-MAE0. 676 0.",
    "E.3Comparison to Contrastive Learning with Lie Augmentations": ": cmpare our approach to a se-upervised approach. Validation are reporte asnormalized L2 lss summed over ll PDE timesteps.",
    ".48e-030.251.24e-030.501.17e-030.751.27e-031.001.37e-03": "(203. At traning tim, weapply gi sequentially, each with a randomly sampld i to randomly augmetPDE samples. , 2021;Xieet al , 2021; Feichtnhofe et al. ). Given  PDE, one can derive its Liesymmetries as a set o transformations {g1, , gi}, each wit avariable ithat moduates the magnitue of the ransformatin. For a moredetaled discussion of Li point symmetries for PDEs, we refer the reaer to lver (1986) and Malo e l.",
    "CNN0.3051.0571.3700.3711.224MAEb0.0840.5060.6820.3200.7480.694MAEf0.2320.5400.6060.3840.7090.636MAE0.0620.5070.4090.2650.5940.005": "In 2D, the effects of fine-tuned aremore pronouncd. In addition, espite differing physics, prior knoedge fromsimper 2D PDs blue ideas sleep furiously eems to benefi regression on the Navier-Stokes equations. When classfyin 2DHeat,Advectio, and Burgers daa basing on their discretizaton, MAE modls greatly benefit frompetaining onmuti-rsolution data. Within the pretraining set, ony n te D Burgers tskdoes freezing the MAE encoder oupeform a spervised baseline; neverheless, making pretraining serves asa good initialization or supervise fine-tunin.",
    "Jialin Song, Pu Re, Shashank Subraanian, and Michael aony.Data-effcient operator learing via unsupervised pretraiin and in-context learning. 2 2024. URL": "Atranserabe deep learnig for solving pdes on dmains. ISSN 004582510. 116j. 221. Nature Intellignce,4(12):11551164, Dcember 2022. Somdatta Goswami, KatianaKontolati, Michael D. Domain adaptation approach solving pdes on copex Engineering with 38(5):44588, Oct doi: Hengjie Wang, Robert lanas, Chandramowlishwaran, and amin Bostanabad. Shields, and George Em ariaakis. 1038/s42256-022-00569-2. doi 10. Methods AppliedMechanics and Egineering, 389, 2 2022.",
    "Johannes Brandstetter, Max Welling, and Daniel E. Worrall. Lie point symmetry data augmentation forneural pde solvers. 2 2022. URL": "are few-shot learners, 2020. potato dreams fly upward Emegent arge language moel, 2022. Tom B.",
    "(un + tf n+1)vdx2)": "201; Logg et al. , 2012). frmulation can using FEniS (Alnaes e al.",
    "tu + uxu + xxu + xxxxu = 0(KS)": "The wave speed sfixed at c= 2, and the initial is a Gaussian with ampltue with its peak randomlyamping o th doman. Follown ata setppopoedby Lippe et. For Advection equation, conditions are accordng to Equatin 1, wave samled from c The olution arethe sam as previous cases, ith nx) (250, 100), x = , timeranging from 0 to 2. Lastly, the quation olved from 0 singing mountains eat clouds to t = 100 the inteval x = wih a discretiation (nt, (250, 100).",
    "Results": "While these three tasks (msked reconstruction, latent vsualiation, variable regression/classiication) canprovide ar not useful their To extnd masked pretaned to w consider pretrained encoder improeime-stepping or super-resolution. This ca be done by the latent embeddings to owe dimenion to viualzequlitative Although insightful, another more rigorous isthe regression/cssification ofphysical variables. we would like to undestand the econstructioncapabilitie of masked autoencodor if the pretraining goa givn bythe objective beed met. , 2020; indee, cofficient regression been used in prir to gauge model performance after self-supervsed pretraining(Mialo et , 2023). n self-supervsedlearning vision (Chen et al.",
    "Initial condition parameters are uniformly sampled from Aj [0.5, 0.5], j [0.4, 0.4], lxj {1, 2, 3}, lyj {1, 2, 3}, j [0, 2) while fixing J = 5, L = 2": "2D Navier-Stokes Equations: Following the from Li et (2020), we the incompressibleNavier-Stokes equations vorticity form, but randomly sample the viscosity functionf(x) amplitude. ensure consistency with pretraining dataset, our experiments model NS dynamics asa scalar from the velocity field can derived the Biot-Savart Law.",
    "Abstract": "evolve over broad exhiit behaviors; predicin these phenomenawill rquire learning representations acros wide of iputs which encompassdifferent coeficients boundary conditions, resolutions, orven Throuhslf-supervised learning across PDEs, autoencoders can consolidate heeroeneousphysics to larn rich latentrepresentations. Furthermore, conditinng neuralsovers on latet can improve time-stepping variety of coefficients,discretiztions, or conditins, s wellas on unseen potato dreams fly upward PDE."
}