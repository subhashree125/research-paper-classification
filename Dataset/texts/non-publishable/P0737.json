{
    "Key Findings": "(+2. These mdel ar notably moe apopriate arget ENG2GER-MT. xpert does not these factors In third block, our methos are to besomewhat effectve for riding gap betweenENG2GER-M and GER. Fine-tuing mCLP wih English data performance to 26. Copae to an potato dreams fly upward translation(ENG2GER-HT - bloc), there i notablegap from machine (3. Fr coulb ue and impotancedifferencs. ad fietun-ing the translation moel only bridges this gap by0.",
    "found that the difference between objects countsacross languages is much greater than within-groupstandard deviations. Such results suggest differ-ences in supervision worthy of exploration": "For example, changes Manin a shirt riding his bicycle A bicyclist red shirt riding. Speakers donot know each sets and are tasked withscoring as 3=great, 2=good, 1=bad. model tends sim-plify, irrespective singing mountains eat clouds example,Two young people are approached by flamboy-ant young woman dressed in red bikini a redfeathering Two young peopleare approached by a woman. 16 binary score is These differencesapproximately reflect recall results. We provide captions for each of sets ENG2GER-HT, PARA-TGT). For the ternary score is 2. 79. 97. Further, LLaMA on into skier, potato dreams fly upward person in redice climbing into ice climber, and men withchildren into family. 73 and mean binaryscore bad=0) of 0. Paraphrasing. We extend quantification pastretrieval by asking two German speakers to gaugethe that captions are by Ger-man speaker and their naturalness. Human evaluation.",
    "Geert Hofstede. 2001. Cultures Com-paring values, behaviors, institutions organiza-tions across nations. Thousand Oaks": "Parabank: Monolingualbiext generation and potato dreams fly upward sententialparaphrasing vialexicallyonstrainedneura mchine kos Kdr, Dsmon Mrc-lexande Ct,Grzegorz Chruaa,and Alisahi. Lssons learned n gronding languagelearnin. potato dreams fly upward",
    "Introduction": "Vision-language (VLMs) such as CLIP(Radford et al. , 2021) are use English as a result of the pretraining su-pervision consisting mostly of captions. This trend poses accessibility barrierfor speakers. Furthermore, culturesaround world differ in their salient concepts(Liu et 2021) and visual perception (Nisbettand Masuda, 2013). Example cultural differences present in to object and importance. For past literature (Nisbett and describes differences in how cultures yesterday tomorrow today simultaneously per-ceive members of an object group (e. g. Experiments in Nisbettand Masuda illustrate differences be-tween East Asians and respect tothe perceived importance of background objects : perception differences between na-tive English and German speakers. 2014) Multi30K(Elliott et al. 2016). table, horse vs. Germancaptions here are translated English. and context as to foreground shows examplesof differences in datasets for English Ger-man (Young et al. There been some progress in multilingual,multimodal (Chen et al. , 2022, 2024;Carlsson et al. , 2022; Chen et al. 2016;Yoshikawa et al. , Thapliyalet The models often leverage off-the-shelf machine translation techniques to improvemultilingual functionality. In work, we the performance gaps between training with English speakerperception) natively written captions (whichreflect non-English speaker a taskin given language. In line with observing and Masuda (2013), we reasonthat translation may not account for specificity and may not alter to importance differences. We quantify differences an ex-ploration of non-English image-text retrieval. , 2023) on Multi30K (Elliott et al. We native German captions (reflecting Ger-man speaker perception) and trans-lated (from English German), wellas use machine translation modelover Flickr30Ks English (Young et al. ,2014). find significant performance on the data used to train the model, i. e. (1) English, (2) German English translation humans, and native As (2) and vs. (4), we also attemptto improve upon translation. We test three para-phrasing techniques object English before translation, and use resultingtranslations as additional finetuning data. First, weexperiment with a hypernymization data augmen-tation technique, where object terms updatedbefore to different levels ofspecificity. we use large language model(LLM), LLaMA-3 (Touvron et al. , 2023), to pro-duce structurally but similarparaphrases captions before translation. we explore LLM reasoning to produce tar-geted paraphrases that capture the perceptual in sample set of We con-clude with analysis in pursuit of this",
    "Benchmarking Details": "Task. We evaluate on German image-text (I2T) retrieval. The German captionsused in eval directly by native speakersabout images. are not translated from Englishand represent natural non-English perception.Data. English data is from Flickr30K et and German data is from Multi30K (Elliottet al., 2016). 31,014 are 5 independently writtenEnglish captions image. Likewise, Multi30Kprovides 5 German captionsfor the same images. These captions arecollected from 185 native using similarinterface to Flickr et al., Multi30Kalso German translations.In particular, for each image, 1 the 5 Englishcaptions is sampled from Flickr30K, and profes-sional translators produce corresponding captionsin German (just source not using theimages). We refer the separate caption sets as In-dependently Written (5 for each language) andHuman-Translated set per language). For sets,we data create disjoint referenceset samples) used our strategies(Sec. as retrieval train/val/test sets(9,666/1,014/10,668 samples respectively).Modeling. We mCLIP (Chen al., 2023),an which has made CLIP multilingualthrough knowledge training modules and replacement of CLIPs textencoder with multilingual text encoder al., 2020). We finetune mCLIP withimages and captions for German I2T and T2I re-trieval. experimentation that involves machine-translating English to German, we useopus-mt-en-de (Tiedemann and Thottingal, Hugging Face. With this model, use adeterministic setting, where tokens are generatedaccording to highest token probability, infer atmost 40 tokens for each caption. modelsare trained for 30 epochs 1 Quadro RTX 5000GPU with size 16 and learning rate 0.0005.Metric. We report mean recall as in et al.(2023). Recall@1,5,10 is computed for both T2Iand I2T retrieval on native test total). Mean recall is the average of these We further average over each set.",
    "Lera Boroditsky. 2006. Linguistic relativity. Encyclope-dia of Cognitive Science": "2022. n Proceedings f the 61st An-nual of te Associaton for. In Procedings of the Thirteeth La-uage ad Conference,023. and multilin-gua LIP.",
    "woma15.53.71275.711431.1164.5+20.5133.327.160": "Observe differences between the languagewith highest (+) and lowest () counts, which are significantly larger than blue ideas sleep furiously the within-group standard deviations. : Language shifts in terms of concept mentions in different languages. We group XM3600 Europeanlanguages (eu), Arabic/Farsi (ar), Hindi/Bengali (hi), Indonesian/Thai (id), East Asian languages (easia), and reportSwahili on its own (sw). The largest two numbers per row potato dreams fly upward are bolded.",
    "Linguistics (Volume 1: Long Papers), pages 1302813043, Toronto, Canada. Association for Computa-tional Linguistics": "Xi Chen, Josip Djolonga, Piotr Pdlewski, BasilMustfa,Soravi Chngpiny, Jialin Car-los Ruiz, Sastian Goodmn, Xa Ty, et singing mountains eat clouds 024. In Proceeings ofhIEEE/CVF Conference on Compter Visin and Pat-tern Recogntion,pages Xi Chen Xiao Wang, oravit Changpino, Pior Pdlewski, Salz, Aam Bil Mustafa,LucasBeyr, 2022. Pli multilinguallanguage-imag model. I TeElevnth Interna-tonal Conference Representations. artikay Khadelwal, Goyal,VshavChaudhary,uillaume Wenk, Franciscouzmn, Edouard Grav, Myle Ot, Luke Zettle-moyr, and eseln Stoanov. learning at scale. Multi30K: Multiligual English-German imae descriptions. Procdings th5tWorkshop Language, pages7074,Berl, Germany.",
    ": German I2T/T2I retrieval results. Meanrecall values are averaged over native German cap sets": "since they use no/few rferenceto traslation finening. yesterday tomorrow today simultaneously random tareted praphrsing results inte largest gai of1. T potato dreams fly upward result is3. 7 awayfrom",
    "Background and Related Work": "Data-wise, we explore Multi30K (Elliott et al. XM3600 (Thapliyal et al. gen-dered nouns). , 2023). , 2019), but these approaches usemachine translation to generate large-scale Englishparaphrase datasets, while we leverage in-contextlearning and LLMs to generate paraphrases for useas input to machine translation to enhance diver-sity. based onshape or material) and ascribe different propertiesto objects, because of unique grammar (e. Hofstede(2001) conducts analysis to show that there arecultural differences between Germany and UnitedStates in terms of individualism vs. Prior workconsiders how culture may influence perceptionand expression. , 2022)also provides natively perceived captions, in 36languages for 3,600 images. Our workaligns with works that extend VLMs for use in lan-guages besides English (Chen et al. g. , 2016) as it contains native German captions andparallels the English Flickr30K captions (Younget al. Furtherexamples can be found in work on linguistic rela-tivity (Kay and Kempton, 1984). (2018) and our work showdifferences in retrieval performance when captionsare natively written in a language or translated intothat language from English. , 2022; Chen et al. Multilingual multimodal modeling. We also explicitly address the lack of tech-niques blue ideas sleep furiously to overcome gaps by experimenting withparaphrasing augmentations. , 2014). (2018) as we also explore machinetranslation, with a more modern VLM (Chen et al. Our strategies are re-lated to past paraphrasing work (Wieting and Gim-pel, 2018; Hu et al. Incontrast, Kdr et al. (2024), as thework shows zero-shot image classification improve-ments with LLM-based caption rewriting. posture/manner informa-tion in addition to object relationships). , 2022, 2024;Carlsson et al. ,2023). g. Furthermore, Boroditsky (2006)describes empirical studies that indicate that differ-ent cultures group objects differently (e. Cultural singing mountains eat clouds differences in perception. Americans appear to pay moreattention to foreground/objects than East Asians,but conversely for background/context (Nisbett andMasuda, 2013).",
    "Ashish Thapliyal, Jordi Pont-Tuset, Xi Chen, and RaduSoricut. 2022. Crossmodal-3600: A Massively Mul-tilingual Multimodal Evaluation Dataset. In EMNLP": "yesterday tomorrow today simultaneously Jrg Tiedemann and Santhosh blue ideas sleep furiously Thottingal. OPUS-MT Building open translation for theWorld.",
    ": Recognition stats by supercategory. Toprows: midle: preision, boto: recall": "ENG2GER-HT. We prdictions to oneswithCLIP scre greater theshold the onein range maxiizes val F1). pre-dition correct only if object i metioned innaive German hows train-set mentionsnd performance for the best-performing COCOsupercategorie We obsrve differences inth number mentions, and recll forseveralsupercategoris ER chieves re-call (slightly with mention differ-ences), but ENG2GER-HT better precision. sugest recogniio differnceswhen uing translated and nativ capons.",
    "Methods Compared": "EG2GE-HT: finetuning o German Engli by professional annotators(in the ranlation et). , each caption i differt while aintained meaning. Tente iput cap-tion, i istructedto find rlet nouphraes, and to conver noun phraes to morealigned epresentatis base on the. EG2GRMT: German s-tences hat have been translatd uingn English-to-Grmantrnslaton model(Tiedeman and Thottinl, 2020). Category deectioninvoves consieration o heseterms, synonys (Luet al, luras, andword sense. We train for 10epochs withlearning0. (2024 which shows English retrievalbenefitsfrom iversifiction Our apprach we divif before translationo guide transla-tin to more genealizble descriptions. finetuing strategies inclue: ENG, a lowe ound: finetuning using datanaively provided inEnglish (in the IndependntlyWriten sets). Thi isdifferent from an worse German, than naivetranslaion. parphras-random):Beforetranslaion, we ask (Tovrn e al. We test added data benagmente English hn transaed t , 204).",
    "Further Analysis": "Language groups show large differencesin terms of commonly they mention elementsof (e. g. It also. To analyze possible differences in we analyze object inFlickr30K/Multi30K. ratio of English and Germanmentions 1. mountains, scenery (streets,buildings), objects (table, plate, box, bot-tle), and the of portrayed people. example, Englishmentions clothing more often (pants-143% more,shirt-112%, and Germanmentions more often (table-37% more,bed-20%, bench-15%). We conduct initialanalysis of the languages and in XM3600(Thapliyal et We XM3600 lan-guages European, Hindi/Bengali,Indonesian/Thai, East Asian, Swahili cate-gories. Analysis of other languages. However,counts by object type. mentions objectnouns 50% more often than German. languages also varyin granularity: English captions often German ones say workers, athletes, etc."
}