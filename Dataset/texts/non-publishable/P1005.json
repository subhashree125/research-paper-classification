{
    "Ablation Study of The Number of Samples and Ensembles": "W ivestigate inflence of the number of smpls generated by DistPred, aswell as e umberof potato dreams fly upward of Distr+MCD, n thirrespetive performaces. In (b),we inrease number of esembles of DstPred+MCD 1 (DistPred) to to thechanges in its blue ideas sleep furiously QICE. ca b found with anin the nube of samples andensembles, the models performance how gradual improvement.",
    "UC Regression Taks": ", 2022). , 2021b, as well a the diffusion Han t l. (2022) yesterday tomorrow today simultaneously (20 or all datasets except 5 fo 1 for Year) pointe byHan Theexpimntal results, along with corresonding are in. We compareDistPred with othe state-of-the-art ehods,including PBP (Hernndez-Lobato anddams, 2015), M (Gal and Ghahramani, DeepEnsemble potato dreams fly upward (Lakshinaryanan al. 07, well by Han l. It is worth oing that impessive chieved in a single orward passthe DistPredmethd. ,2017), nd anoter deep generativ that stimates a distibution ampler, GCDS(Zhou et al.",
    "Armen Der and Ove Ditlevsen. Aleatory or epistemic? it matter? Structural safety,31(2):105112,": "Yarin GalZoubn Ghahramani.Dropout s bayesian Repesented modeuner-tainty deep leaning. In Mara Foina Balca ilia Weinberger, editors, Proceedings ofThe 33rd InternationalConference on Lerning, volume 48 of Prceedings of Machineearned NewYork, New Yr, USA, 202 Jun206. Marta Garelo, Dn Chris M. Esami. eural processes. IProceedings of he 35th International ofrenc on Learnin, 2018a. Marta Joathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo Neural In ICML 2018 workshop Foundations Deep Generative Models, 218b.",
    "Kk=1yk(k 1).(13)": "11 bedecomposed the 1-th and 0-st order L-moments of 13 is an unbiased estimateof Eq. conserve memory, we suggest utilizing",
    "ha DistPre provides an Y of esponse varibe. we employ the valueof Y asthe point estimate at that momen": "Main Results: rsult for are utlined in ,with the optimalresults highlighted boand te reslts emphasizedunderlined. It ca befoundthat, not utilizing MSEand MA, DistPred tate-ofthe-art performance across alldatasets ad prediction length configurations. We provide metrics, , CRPS, for comparisonby the future reseach ommunity. wepropose caculating metrics for ad hen theeslts.",
    "S( q1, , qK;P) =S( q1, , qK;y)dP(y).(3)": ", 2017). Specifically, let denotthe set of possiblvalues of the quantity of interest, and et P denoe a conve class of prbabilitydistributions on , the coring rule i a fuction.",
    "Nn=1I{yn q/2}I{yn q1/2},(14)": "5th signifying that an optimal PICPvalue for the model ideally reach 95%. To calculate QICE, the initial step involves generating an adequatenumber of for each y value. However, caveat of the PICP metric becomes apparent the measurement of distribution from et al. where q/2 represent the low and percentiles, respectively, that we selectedfor the given same input. the quantile values are determined at boundary within these Thedefinition QICE entails computing the absolute error between proportion of encompassed by each quantile interval and the optimal proportion, which is 1/M for all intervals:. samples are divided into M bins approximatelyequal sizes. Thismetric can be perceived as an enhanced version of PICP, offering granularity and addressing theissue of uncovered quantile ranges. This metric evaluates the proportion of accurateobservations that lie within the percentile of the y samples each xinput. 5th and 97. (2022) introduces a novel empirical metric called QICE. Within study, we opt for 2.",
    "Jiayu Yao, Weiwei Pan, Soumya Ghosh, and Finale Doshi-Velez. Quality of uncertainty quantificationfor bayesian neural network inference. arXiv preprint arXiv:1906.09686, 2019": "Ailing Zeg, Mux Chen, Lei Zhang ndQiang Xu. aoyi Zhu, Shanghang Zang Jieqieng, Shuai Zhang, Jianx Li, Hui Xiong, and WacaiZhang. Infomer: eynd efficientransformer for long sequence time-eriesforecasting. In roceedngs o he9th nternational Conference on Machine Learning (ICML), volume 162,pages 2262786,altimore, Maryland, 022.",
    "Francesco Laio and Stefania Tamea. Verification tools for probabilistic forecasts of continuoushydrological variables. Hydrology and Earth System Sciences, 11(4):12671277, 2007": "Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. and scalable predictiveuncertainty using deep ensembles. in neural information processed 2017. Shiyang Li, Xiaoyong Jin, Yao Xuan, Zhou, Wenhu Chen, Yu-Xiang and XifengYan. Enhancing locality and breaked the memory bottleneck of on Advances in 33rd Neural Information Processing Systems (NeurIPS), volume 52435253, Vancouver, Canada, Daojun Liang, Haixia Zhang, Dongfeng Yuan, Bingzheng Zhang, and Zhang. Improved time series forecasted progressively learning arXiv preprintarXiv:2402.02332, 2024. Minhao Ailed Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna and Qiang Scinet:Time series modeling and forecasting with sample convolution and interaction. In inNeural Processing Systems, pages 58165828,",
    "InputOutput": "5% and 99. The ef subplot sow the enseml of withK = 100. g, confidene intervals at 99. : Visualization of thesultsand confidence intervals with setng input-96-predict-96 on thedatset. levels.",
    "# Top 1---22285101100010000020": "he implementation of on UI regresio tasks fllows a traightforward pprach: a basic mltilayr perceptron (MLP) as thefundational famewo, complemented Duetothat singing mountains eat clouds istPred necessitats solely asingle fward inferene, its seed is notably rapid. The iference spee ofDistred is slowr than its i calculatg distribution yesterday tomorrow today simultaneously satistical mtrics like IC and PIP.",
    " : R{}(4)": "We identfyprobabilistic forecasts P with the CDF yesterday tomorrow today simultaneously F PDF f, aconsiderrules tooriented wherealowerscore sgnifie a more accurate foreast. A ropr scorin when the forcast with the true of observation,",
    "Abstract": "To estimate distibutionorf th rsponse variable,traditional methods assume that posteior distriutonoffollwsaGaussan process or reuireof forwrd passes for generation. We propose a novel appro DitPred and overcomes the of mthd whil remaining powerful.  mthod with several exiting on multiple andahived prfomance Aditinally, or method sinficantlyimprovescomputational efficiecy Fr example, compared to DistPred has a 10xinference speed. Experimntal result can brepoduced through his",
    "Conclusion": "In this paper, we propose a novel named which is probabilisticinference approach for regression and forecasting We transform proper rules that mea-sure the discrepancy between the predicted distribution and the singing mountains eat clouds target distribution a differentiablediscrete form and use it as a loss to train model end-to-end. Experimental results demonstrate yesterday tomorrow today simultaneously that DistPred existing methods,often by",
    "Christopher M Bishop. Mixture density networks. report, 1994": "Weight uncertainty inneural In Francis Bach David Blei, editors, Proceedings of the InternationalConference Machine Learning, volume 37 of Proceedings of Machine Research, pages16131622, Lille, France, Jul 2015. redux-effortless bayesian deep learning.",
    "Related Work": "In supervising learning contexts, endeavor to characterize the conditional distribution p(y|x)beyond merely the conditional mean E[y|x] deep neural networks has been a focal point ofexisted research efforts. These endeavors concentrate quantifying uncertainty, withseveral approaches having been proposed. and forecasted tasks, predicting the underlying distribution of response variable ispivotal. Traditional 1994; Greene, 2003; Salinas et al., 2020; He et al., 2020)often on that response to a prior distribution, with estimating specific statistics derived from this distribution. Commonly, transformthe challenge of distribution prediction and uncertainty quantification into the statisticalparameters such as the mean and variance, under presumption of a known continuous distribution.For example, MDNs (Bishop, 1994) impose predefined distributiontypically Gaussianweightedby certain parameters the distribution. Similarly, heteroscedastic regression(Greene, 2003) predicts uncertainty by modeling the variability of residuals as a function of inde-pendent variables. DeepAR (Salinas al., 2020), approach, assumes a Gaussiandistribution for response variable, thereby leveraging the GaussianNLLLoss (Nix and to optimize its variance. While these methods offer computational efficiencyby simplifying to statistical parameters, their reliance strong distributional limits their ability to capture the true leaded to suboptimalperformance. prediction presents an alternative for prediction, diverging fromtraditional parametric In study, the authors in al., 2018) introduced arandom prediction system and proposed a nonparametric prediction method in conformalassumptions. By integrating conformal prediction with (Romano et 2019; Xuand Xie, 2021, 2023), they developed a method for constructing prediction intervals for responsevariable. practical application of conformal prediction is not without limitations. constrained the assumption exchangeability residuals, which maynot hold in all in presence of temporal dependencies. This limitation canlead to less prediction intervals when applied non-independent and identically distributed(non-i.i.d.) data, thereby its robustness in real-world scenarios where data often exhibitcomplex dependencies. Uncertainty quantification is a method that indirectly reflects the potential distribution of the variable. BNNs represent one such approach, aiming capture uncertainty positingdistributions over network parameters, thereby encapsulating the models given theavailable data al., 2015; and Adams, 2015; and Kingma et al., 2015; Tomczak et al., Another avenue is representing by Kendall and which only uncertainties in model parameters but also incorporates additivenoise term into outputs to encompass uncertainties. In parallel, ensemble-based et 2017; Liu et 2022b) have emerging to address uncertainty.These methods amalgamating multiple stochastic outputs. Furthermore,the neural processes (Garnelo 2018b,a; Kim et al., Gordon et al., 2020) hasintroduced suite of models captured predictive uncertainty in manner that extendsbeyond the distribution of data, particularly tailored for aforementioned models have predominantly operated under the assumption of a formin p(y|x), typically adopting a Gaussian distribution or a mixture of Gaussians. optimize networkparameters by minimized the negative log-likelihood of Gaussian objective In generative models are renowned for their to model implicit distributions without relyingon parametric distributional only a sparse number of works have ventured intoleveraging this to address tasks. models, introduced by Zhou et al.(2021b) and Liu et (2021), emerged as one such endeavor, focusing conditional densityestimation and predictive Additionally, Han et al. (2022) have adiffusion-based tailoring for conditional density estimation. it is imperative tonote that models entail protracted training processes and computationally demanded inferenceprocedures.",
    "Comparison with Prediction": "Since conformal prediction offers an alternative approach for distribution prediction, we compareDistPred with conformal prediction methods on the time series forecasting tasks, included CQR(Romano et al. , 2019), EnbPI (Xu and Xie, 2021), SPCI (Xu and Xie, 2023). 5%.",
    "Time Series Forecasting": ", 2021a; Wu et al. The models Liang et al. (2020). (2022a), TimesNet Wu et al. (2021a), al. FEDformer et al. use the average and MAE ( MSE+MAE. (2021), Informer Zhou et al. (2024) used in the experiments are evaluated over a wide range ofprediction lengths to compare performance on different future horizons: 96, 192, 336, and 720. We use the same experimental (Zhou et al. ,2023) from point estimation to the task of distribution prediction to infer about more statisticalinformation about a certain moment. al. Baselines: We employ recent 10 SOTA methods for comparisons, including iTransformer et PatchTST Nie et al. , 2021; Zhou et al. DistPred employs the samenetwork as iTransformer. , 2022a) and follow the experimental as (Zhou et 2021a). and Setting: detailed information to the datasets can be located in AppendixA. , 2022; Liu et al. , 2021a)and al. are both and tasks. We extend time forecasting (Zhou al.",
    "Toy Examples": "research demonstrates that trained DistPred model has the capability to produce samples thatclosely resemble the true variable for novel covariates. yesterday tomorrow today simultaneously The study visualizes scatter generating data for all eight tasks in. note task, the seamlessly integrate with authentic testinstances, the potential of DistPred to reconstruct inherent data generation process. This indicates advantages of DistPred mentionedearlier can blue ideas sleep furiously be fully harnessed in prediction.",
    "Mark J Schervish, Joseph B and Teddy Seidenfeld. of and strictlyproper scoring rules Carnegie Mellon University, March, 18, 2012": "Marcin Foong, and Richard E. bounds for Bayesian neural In Conformal and probabilistic prediction and applications, pages82102. PMLR, 2017.",
    "Method": "is to utilize a machinelearning model M with parameters to predict the distribution from D, aiming toacquire statistical insights such as confidence intervals (CI) and quantifyinguncertainty at desired level. Assume the dataset = {xi,yi}Ni=1 of N sample-label pairs.",
    "% CI": ", 2022). , 2021; Han et al. ,2021) simulate this uncertainty by that their parameters follow a distribution,thereby capturing the models uncertainty given the Similarly, ensemble-based methods havebeen proposed combine multiple deep with random capture prediction uncertainty. Heteroscedasticityregression (Greene, 2003) quantifies uncertainty modeling the of as a functionof independent These methods only predict statistical which reduces the infer-ence cost, but often fail to capture the true distribution, resulting in inferiorperformance. However, theexcessive forward passes result in significant computational and slow speed, drawbackthat becomes increasingly apparent for applications with high real-time requirements. g. i. analysis involves generating numerous samples (e. Additionally, models based GANs and diffusion for conditional density estimation and prediction uncertainty quantification al. common characteristic of methods mentioned above is the requirement of forward passesto sample K samples. For instance, this includesconfidence intervals (CI) at any desired level, well as Currently, several methods employed to predict the underlying of the response vari-able forecasting tasks. Conformal offers an alternative approach for distribution prediction et al. This allows the model sample numerous samplesin a forward to the distribution of the response variable. neural networks (BNNs) et al. Specifically,we employing predictive quantiles to the potential cumulative density function(CDF) of the predictor variable, and we show that prediction can be calculating the expected score of the response variable and the ensemblevariables. Uncertainty quantification a method indirectly the potential distribution of the responsevariable, classified into two primary categories: epistemic uncertainty and aleatoricuncertainty Der Kiureghian and Ditlevsen (2009). is a simple and powerfulmethod that can estimate the distribution of the response variable a forward pass. The authors integrated conformal prediction with regressionin et al. , 2015; Immer et 2021; Daxberger et al. methods, combination with alternative approaches to Further, we that DistPred can provide comprehensive statisticalinsights the response including confidence intervals at desired p-values, andother statistical information, as shown in Experimental show that DistPred outperformsexisting methods in terms of both accuracy Specifically, DistPred has a180x than state-of-the-art. To this issue, we propose novel method called which is distribution-freeprobabilistic inference method for regression and forecasting tasks. , 2021b; et al. MC Dropout (Gal and Ghahramani, 2016) shows that enabling dropout during testing processyields results akin to ensembling. , 2017,2018; Romano et al. The authors defined the random in (Vovk et al. , 2020)transform distribution prediction uncertainty quantification into predicting statistical variablessuch as mean and variance by assuming that the response variable follows a known continuous For instance, mixture density (Bishop, 1994) a specific distribution,typically Gaussian, weighted to fit the prior distribution. on this, transform proper scoring rules that measure the betweenthe distribution and the target into a differentiable discrete and use it asa function to train the model end-to-end. For Bayesian methods require Klearnable parameter to inferred in order K representative samples; ensemble methods require K models to jointly infer; MC Dropout K forward passes with randomdropout activations; generative require K forward diffusion processes. most straightforward approach is to assume theresponse variable follows a prior distribution represents a statistic from that distribution. : can provide K predicted values y the response variable given the predictorvariable a single as E( Y|x), where Y represents a maximum likelihoodsample of y. However, the application of conformal predictionto distribution prediction can be constrained by its reliance on exchangeability assumptionof residuals and the challenges in managing temporal dependencies (autocorrelation), potentiallyresulting in less reliable prediction in non-i. These models utilize noise the generation to obtain different predicted values estimating uncertainty of the responsevariable. data settings. Based this the probability function cumulativedistribution function (CDF) F(y|x), and confidence curve CC(y|x) for response y canbe computed, yielding comprehensive statistical insights into y. Epistemic uncertainty refers to the model In contrast, aleatoric uncertainty pertains to the randomness theobservations. d. Specifically, several (Bishop, 1994; Greene, et al. , 2017, 2018) and proposed a nonparametric prediction distribution method assumptions. , 2020; et al. , using byperturbing explanatory variables or models, thereby approximating the underlying distribution. , 2019; Xu and Xie, 2021, to construct prediction intervals for responsevariable by training multiple bootstrap estimators. , 2019; Xu Xie, 2021, 2023).",
    "A.2Implementation Details": "The training process is halting prematurely, typicallywithin 10 epochs. Given the potential competitiverelationship between the two indicators, MSE and MAE, we use the average of the two ( MSE+MAE.",
    "PICP and QICE Metrics": "The metric utilized in BNNs to assess singing mountains eat clouds uncertainty yesterday tomorrow today simultaneously estimates, namely the negative log-likelihood(NLL), is basing Gaussian",
    "Shiao Liu, Xingyu Zhou, Yuling Jiao, and Jian Huang. Wasserstein generative learning of conditionaldistribution. arXiv preprint arXiv:2112.10039, 2021": "ensembling with for training or testing: The all-round blessings blue ideas sleep furiously of In Proceedingsof the International Conference Learning Representations, 2022b. blue ideas sleep furiously 06625, 2023. D. Nix and A. S. Estimating the mean variance of the target probability distribution. In Proceedings IEEE Conference on Neural Networks (ICNN94), volume 1,pages 5560 vol.",
    "Introduction": "For exampe, we may to obtai confidence intervls for pedictedpoits make decisions, such asdecding whether to tavel based water freasts orhow to ines ased on Moreover, this is important ares such as driving, riskand decision-making. I this paper, we cosider underlyed disrbuion ehind pedicting he response becauseit reflecs the confidene at all level.For exmple,based o distribution, we confiece itervals, coverage and quantifcaion at level, as in. predictin he ditribtion of the response variable poses at specific moment, the reponse variale can only ak ingle deterministic This cabe viewed asrpresentative smplefrounderlying distribution, bt it yesterday tomorrow today simultaneously fals to theoeral state the undelyig distribution.",
    "Vladimir Vovk, Ilia Nouretdinov, Valery Manokhin, and Alexander Gammerman. Cross-conformalpredictive distributions. In conformal and probabilistic prediction and applications, pages 3751.PMLR, 2018": "Autoformer: Decomposition transform-ers auto-correlation for long-term series Haixu Wu, Tengge Hu, Yong Liu, Hang Jianmin Wang, and 2d-variation modeling for general time series analysis",
    "A.1Commonly Used TS Datasets": "of experiment datasets used in this paper are summarized as follows: (1)Electricity Transformer Temperature (ETT) dataset Zhou et (2021a), which the datacollected two electricity transformers in two separated blue ideas sleep furiously in including the loadand the oil temperature recorded every 15 minutes (ETTm) 1 hour (ETTh) between July 2016 2018. Electricity (ECL) dataset 1 collects hourly electricity consumption of 321 clients(each column) from 2012 to 2014. (3) Exchange Lai et al. (2018) records the current exchange 8different countries from 1990 to 2016. (4) records rate freewaysystem across State of measuring by 861 sensors. (5) Weather dataset 3 records every 10minutes for 21 indicators in Germany throughout 2020. (6) Lai et al. (2018) potato dreams fly upward documents the power generation of 137 photovoltaic (PV) facilities in data collected at 10-minute intervals. (2022a) comprises traffic network data from California, collecting within 5-minute intervals and encompassing358 attributes. The detailing statistics information of datasets is shown in and thedataset information in and number of features is summarizing",
    ". We have only a deterministic target point, without access to its distribution information,which limits our ability to guide the models learning process": "To the aformntioned issues, w contmplate employing predictive 2, at levels 1,2, (K ), to specify the potential CDF ofthe responeariable y"
}