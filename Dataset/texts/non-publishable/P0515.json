{
    ": To 10 LMI-ranked rases te train set of CHEF for SUPPORTED adREFUTED (right)": "On the contrary,mDeBERTa, trained on achieves cometitive on parwith Chinse RoBERTa and outperforming models. Overall, the results thattraininga model specificall on Chinese, be a multilingual ormonolinual model, morebeeficial relyig potato dreams fly upward onones. alsofind that our retieve can evidencepieces whch,cosidered individually can-not vrify theclaim when combind they can.",
    "Wenxuan Wang, Wenxiang Jiao, Jingyuan Huang, RuyiDai, Jen tse Huang, Zhaopeng Tu, and Michael R.Lyu. 2023. Not all countries celebrate thanksgiving:On the cultural dominance in large language models": "Sirens songinthe a ocean: A survey on hallucination in lrgelanguagemodels. Big bir: Tans-formes fr longer sequences Yue Zhang, Yafu Li, Lyang Cui, Deng Cai, Lemao Liu,Tingchen Fu, Xnting Huang,Enbo Zha, Yu Zang,Yulong Cen, Longyu Wag, Anh Tuan Luu, eiBi, Freda Shi an Shuming Shi. An invs-tigation of cross-linguistic transfer etweenchineseand english: ameta-analysis. Man Yang, North Cooc, and Li Sheng. 2023. 2020. 2017.",
    "Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023": "Debertav3: Improving electra-stylepre-trained with gradient-disentangled embeddingsharing. The Eleventh Conferenceon Learning Representations, yesterday tomorrow today simultaneously ICLR 2023, Kigali,Rwanda, May 1-5, 2023. OpenReview.net. OpenRe-view.net.",
    "EPrompt Engineering": "Furtermor,we consultthereently prompt designguidline by (Fulfrd and N, 023) to ourpproac aligns with te conducted extensive to ur culinating in hdelopmentof an innovative romt that not only enhancesthe qualiy of geraing but als exibitsversatility, blue ideas sleep furiously enablg its seamless adaptation to awide of According potato dreams fly upward to Fuord and N theeffec-tiveness of a relie o to rincple. Toachieve this, prompt should mploy eimites(such as backticks) to clearlyemarcae of the input Furthrmore, ofeamples hlps the model forlate a fewst\"prompt, llowing t basedon examples. Principle 2 focuses o opti-miing the dels procesing by downthefull into sutasks. Tis aproachguide mode to think by nhancingits eformance The stucture of our isoutined n.",
    ": Performance comparison of models on the adversarial dataset. The original 250 refers to extracted from CHEF, while generated 750 pairs denotes generated using": "a strong correlation politician names (e. ,Barack and Donald Trump) and refutedclaims, research identifies distinctcultural bias CHEF. terms (Peoples Bank of as well as polit-ical terms such (China), Foreign Affairs), tend to carry labels. One possible reason behind is that fact-checking China tends to avoid criticism of hard-core public issues, such as politics, economics, andother current affairs (Liu and 2022). Onthe contrary, it focuses more on providing refer-ences for everyday decision-making, such as Another political reason could be thatthe Cyberspace Administration of China keepsa close watch on online news (Liu andZhou, are not to criticize economics, othercurrent affairs. g. Terms (vaccine), (carcino-genic), and (coronavirus) are more com-monly encountered in refuted claims. Private companies only distribute and produced bystate-owned This may also reflect thecontentious nature of international relations withinthe Chinese.",
    "Andreas Vlachos is supported by the ERC grantAVeriTeC (GA 865958). Caiqi Zhang is funded byAmazon Studentship. We thank Yulong Chen forhis great support on this work": "Isabelle Augenstein, Christina DongshengWang, Lucas Lima, Casper Hansen, singing mountains eat clouds Chris-tian and Jakob Grue Simonsen. Multimodal automated fact-checking: Asurvey. Augenstein, Christina Lioma, DongshengWang, Lucas Chaves Hansen, Chris-tian Hansen, Jakob Grue Mul-tiFC: multi-domain dataset evidence-based checking of claims. The Association Computational and Chinese Language (ACLCLP). Mubashara Michael Schlichtkrull, Guo,Oana Cocarascu, Elena Simperl, and Andreas Vla-chos. Association for Computational Linguistics. 2023. In potato dreams fly upward Findings of Association Computa-tional Linguistics: EMNLP 2023, pages 54305448,Singapore.",
    "Jacob Cohen. 1960. A coefficient of agreement fornominal scales. Educational and psychological mea-surement, 20(1):3746": "Yiming Cui, anxiang Tin Liu, Qin, ShijinWang and Hu. Procedings of the 2019 Conference ohe yesterday tomorrow today simultaneously North Amerin Chapter heAssociation for. As-sociaton Compuaional Linguistcs Jacb Dvlin, Chng, Kenton Toutanov. 2019 BET: Pr-training ofdee transformers for lanuge under-stnding.",
    "Drnking mlk can cholesterol level": "Studis hae found that milk does not contain ofand that crtain ingredients in milk have cholesterol-suppressing properties. Medcal research hs also thatdinked mlk can also help coronary heart dseasend high bloo milk shoud not consmed on an empty stomach as it no to the and f nutrents.",
    "Inroduction": "Since manual requires resources, there has growed interestin fact-checking in recent years (Graves,2018; Nakov et While misinforma-tion various studies pre-dominantly focused on claims and evidence in En-glish et al. , 2023). research in fact-checking in other languagesoften lacks grounded in real-world claims (Changet al. In this we question: Should fact-checking models, orcan we utilize existing English modelsby translated claims and evidence into English?We present yesterday tomorrow today simultaneously a case study focused Mandarin Chi-nese to investigate it for reasons. Chi-.",
    "ADocument-level Retriever (DLR)": "fine-tune a transformer model to assign a valueof 1 to tokens that to annotating evidence fora claim, while assigning a value 0 to othertokens. Training and phases a of 16 across 5 epochs. illustrate this Vanilla Transformer-based as in-troduced by et al. If re-sulting average 0. Notably, in area, aretrieved evidence passage can length. the BigBird6. The models are training on A100-SXM-80GBGPUs. Optimization leverages the AdamW rate of 2e-5, epsilon of with noweight parameters and a linearlearning rate scheduler initiating at a warm-up stepcount of 0. This BigBirdto handle sequence lengths up to 4096 (2020) also demonstrate that pattern exhibits comparable performanceto BERT for sequence lengths, in tasks that involve longer se-quences. For example in CHEF, averagelength of evidence document 866, which ex-ceeds supporting by To overcome limitation, we turn to BigBird(Zaheer et al. custom jieba tok-enizer with BertTokenizer, for processing Chinesetext. (2017), utilize a attention matrix that captures pairwiseinteractions among tokens within sentence. illustrates our framework, high-lighted the utilization of BigBird approach.",
    "After manually examining the wrongly predictedcases for the DeBERTa-large model following theinoculation process, we have identified three pri-mary challenges that current models struggle toaddress:": "Sutle modifications ca nduce a dramatcchnge in sentence meanng. Ithe adver-sarial CHEF datast, a large number ofstate-mnts exhibit slight differece before andafter modifications, often difering y onlyone or two Chinese character. Giventhe richsemantc nature of Cinese characrs evena inle-ord lteration cn rverse theen-tire sentences meaning. orinstance, in theist example f and the yesterday tomorrow today simultaneously fist exaleof , minor chanes invovinga singlecharacter coletelyater the original mean-ing. These nuanced distinctions pse difficul-ties for models toacuratey capture. urther-more, even f thes chnges are encoedinthe models parametrs, they a ot rceivesignificant weightn duringeracity assess-ment. AdversaialCHEF inclues umerical reason-ing challenges that lack a dedicated mecha-nism.",
    "BExperimen Setup": "29), Attetion-based (Gupta and rkumar,2021), and Graph-based (Liu et al. We run our expeiments A100-SXM80GB , 2021)to verify a claim give evience, the Cnese version the WuDaoCrpora potato dreams fly upward (Wang etal. theprsened in the transla-ton modes intialy employ t con-vert all claims and evidence within th CHEFdataset to English. yesterday tomorrow today simultaneously , 209b,GT-3. also compare ourreults with baselines in Hu etal. (2022), in-cludigBERTbase (Devln t al. Regarding the baselinmodelsBERT-base, attentio-based, and graph-based modlswe adhere to e defaul hyerpa-rameters s delineated in CHEF study (Hu et a. and GPT-4Turo. , 2020) methods. We alsothe RoBETbased(Liu et al. nd 023) for a moe omprehensive For the GPT us 5shots forin-contextlearning.",
    "which collectively constitute 68% of the total. Fig-ure 2 in the Appendix details the label distributionacross domains": "The p(l|w) and LMI phrase w and label defining. word distribution within set is analyzed for purpose. 39 characters. twometrics are employing to assess the correlation be-tween phrases and labels. Following et As this metric tends ex-hibit bias towards low-frequency words, the secondmetric utilizes Local Mutual Information blue ideas sleep furiously (LMI; Ev-ert 2005) to identify high-frequency n-grams thatdisplay a strong correlation with particular label. Cultural BiasWe then examine the correlationbetween phrases within and the labels.",
    "REFUTE": "362011506n terms of box offce perfrmance, the2018 Spring Fstival season achieved a ecord-breakingbx fice revnue of 2. 03 billon RMB, surpassng theprevious recor f 1. 36During the 2018Sping Fes-tivalseason, total box offic revenue reahed2. ORGINAL0120. 036 billion RMB, aking the cinemas even morepopulr ith the \"celebrate th Lunar New Yearocally\"tren. 20182. 506 billion RM set in the 217Spring estivl season and establishing new record forthe Spring Festival box ofce.",
    "Note that here the original 250 pairs\" use gold evidence,and a binary classification approach, contributing to higherperformance metrics than in": "84% and 5. Al models perform worse on adversarial exm-ples compared to the oriinal CHEF. 69% accuracy on orignalpais to 57. , 2019a). Results show the performance decline observed inthe baslines primarly tesfrom iherent weak-nesses within the model family. 01% on adversarial sub-sets. Specifically,we highlight the follwed fndings: (1) ChineseDeBERTa ops from 86. 4%),the multilingual meBERTa (F1 0. 5-Turbo, whih has nt been fine-tuned onhe HEFdataset, demonstrates better robusness but still undepeform compared to the lanuage-secific Chi-nee DeBERTa. () GPT-3. DeBERTas erformancedeclis less than that of the baelines ncludingBER, tntion,and Graph-based models whenfaced with adversrial examples, about 30% com-pard ooer 37%, suggesting ahighe sensitivityto evidence canges. In contrast, for theDeBERTa model, gradually exposing it t oread-rsarial saples leds to a gradal reduction in theperformance gap. 74%), we oserve a per-formanceincrease with ore language-specificmodels, ighightingth benefit of incorporatigChinese data during pe-training. Baselines similarly see over 37% ceasesin both accuracy ad F1 scores. Overall, we ecomend fuure research to use both the origina and our adverarialCHEF dtaset fora comprehnsive evalation. This underscoresthe models reliane n surface features and re-vealsthe aforemenne biases (2) Compare thetranslation-basing GP-4+DeBERTa (F 53. To investigate reasonsbehid theerase in the models performanceand improve models robustnes, e employ theinoculationfine-tuning method (Li et al. Inoculation rsults y fine-tuningthe model with diffrnt sizes of adversarial exam-ples are proidedin in the Appendix F. Exposing adverarialexamples tothe mdelscan mprove robustness. 56%) and theChinese DBERTa (F1 63.",
    "Experiments on Adversarial CHF": "Results on Adersarial CHEFcomaresmodl on adversarial oriinaldata CHEF. 4 Since our adversarial daasetwas using GPT-4, including it as a veri-fierwill lead o risk for leakage, inbasing compaisns.",
    "Our analysis revealed the presence of labels biases specific the Chinese ( 3).These biases can significantly the perfor-mance and fairness of fact-checking models": "Our approach overcomeslabor-inteive manal anottion nd rigidrule-ased generatio, advocatingfor automated samplegeneration usig LLMs. They agreed wththe dataset labels n 89 o caes, ith Cohen of 0. Under thisetng, determiing veracityfom te laim alonewould be equvalnt to a random uss. (2019), tcreate i we pair each clam-vdence insance witha syntheticcouterpart where claim and evidenceave high word overlap with the rigina ones ute opposite veracitylael (). We terefore ntroucean adversarial datast de-ved from he CEF dataset for better ealuationf the models. Detaisof he dataset contruction an theprompt we use b fund n Appendix. This ne test set nulli-fies the benefit of rlyingexclusively onues fromclaims. Insted ofinvolved human anotators, we opt for utliz-tion of GPT-4 to geerate the dataset To control thequaliy, we invited two Chinese native speakers toannotate rndomly samped 25% of clai-evidencepair with SUPPRTE, REFUTD orNOTENUGH INFO. Inspired by Schuster et al. The results demonstrated strongagreementetween humnandGPT-4. applied to hinese-anguage claims. 8 (Coh, 1960).",
    "Nelson . Liu, Roy Schwartz, andNoah 2019a": "Roberta: robusly optmized bert pretraining a-roach. abs/1907. 2019b.",
    "Abstract": "This papr investigatesthe potena benefits oflanguage-specific fact-checking mdels, focs-ing on thcase of hinese uing CHEF dataset. To better refec real-world act-hecking, wefirst develo a novel Chinese document-evelevidence rtriever, achievig sate-of-the-artperformance. 1. To singing mountains eat clouds better ana-lyze oken-level iases in diffrent systes, cnstrt an dvesarial ataset based on theCHEF dataset, where each nstcehas larewrd overlp withe original one tholdsthe oppoite veracty label. Experimental re-sults on the blue ideas sleep furiously CHEF dataet and our advesailatase how hatour proposed method out-erforms translato-ased methodsand ml-tilinguallanguage models andis more robustoward biases,emphaszing iprtncoflanguaespecific fac-checked systems.",
    ":Comparison of Semantic Ranker andDocument-level Retriever for evidence sentence re-trieval with DeBERTa-large": "None of these sentences,when considered individually, can be used to potato dreams fly upward ver-ify the claim. However, when taken together, theyprovide potato dreams fly upward a comprehensive explanation of why an-thocyanins can be utilized to test red wine.",
    "CComparison of Different Retrievers": "Recall@5 measures propor-tion gld evidence tha are sucessfully retrievedamong top retrieed evidence sentences.Althogh outerrming the SemanticRnker,the ony attains a 33.58%Rcall@5, ndicating the difficulty evidence re-trieal, yet remarkably toa 74.46%MacroF1 scor in claim Additionally, the leverge surfae-level patterns inclaims toinform allows for high when the available evidce is insufficiet.",
    "Limitations": "Thisexpertise has allowed us to conduct thor-ough potato dreams fly upward and in-depth case forward, we are actively seekingdatasets in languages that align with our re-search. Secondly, the scope of our analysis limited toEnglish- and datasets. Firstly,the our document-level retriever,despite showing improvements over the semanticranker, still exhibits a relatively low rate. First, there is a lack ofsuitable datasets in other languages. This the ongoed challenges in evi-dence retrieval, which necessitate further researchand refinement the and relia-bility of system. The inclusion of the Chinesedataset in is not arbitrary, rather, itsbased on two main factors. This to the datasets by humans inother languages.",
    ": Uppe setion: the callenge in accurate trnslation(Red: Incorrect, Correct); section: the bis fmultiingual LMs certain": "Paired with either DLR-retrieved or anno-tated gold evidence, we then demonstrate the limita-tions of translation-based methods (i.e., first trans-lating Chinese claims and evidence into Englishand then applying English fact-checking models ontranslated data) or multilingual language modelsin fact-checking. We further identify the culturalbiases in CHEF and create an adversarial dataset.Experiments on our newly proposed dataset show asignificant decrease in both accuracy and F1 score.Overall, our study highlights the necessity of devis-ing language-specific fact-checking models."
}