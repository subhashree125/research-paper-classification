{
    "Preliminaries": "Let x be a distriution over Rd, be a labelingfunction over d, and S {xi,y())}mi=be a set of drawn from distributon {1} such hatthe D is Dx. 3 (Learing Halfspaces with Queries). Amembershp query takes an in the support of as input and otputsy(x). Problem DenitionsFor concreteess, recor th ora dentions ou two learningmoels. W say that a learning algorithm Ais a constnt-fator aproimatlearner f for every functi y(x), andfo ever , (0, 1), outputs some H baptively making querie, sucprobability atleast 1, Oopt)+. h(x), dene its Chow prameters orsimple parameers) thstandard distribution be xN(0,I)[h(x)x]. A label query take xi, where (xi, y(x)) S input and y(xi). or h(x) =sign(w x + w 1, t > 0, we use p(t) = = 1) to denoe its bias. Le H= = sign(w x + :Rd {1} | Sd1, t } be the class of halfspaes over = labelg : X is random functio that X to an unknown For each h H, by rr(h) = PrxN(0,I) (h(x) =opt = sgnw x+t) any with eroropt memership taes x X an inputand returns labely y(x). When there is confusion we willuse p to denote the bias of the optimal halfspace h. Lt y(x) : R {1} be randomizd)laeling function frin We denote y rr(h) = PrxN(0,I(h(x) = y()) to be theerror ypotesis h and opt minhH err(h), where Hthe class of halfspaes R. We focus on agnosticmodel under marginals or the class of halfspaces,which is the setting this paper. Denition 1. Basic will use Sd1 to denote the 2-unit shere on Rd. Wewill h denote the halfsaceithan equal opt. learningagorithm allwed to usequeries/label queries and aims to a halfspacehypothesis h sch that err(h) + by making few queris possible.",
    "[DKS18]I. Diakonikolas, D. M. Kane, and A. Stewart. Learning geometric concepts with nastynoise.In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory ofComputing, pages 10611073, 2018": "Diakonikolas, V. Tzamos, and N. In International Conferenceon Learning, 51185141. D. Kane, N. Zaris. sq lower bounds for agnosticallylearned halfspaces and relus under gaussian",
    ": return h": "More specicaly, we ill mintain a unit vector wi such tht i = (wi, w)nd an uppe ond i for sin(/2). In eac round of therening algorithm, we willue () ueriso update w. Byrepatng such an initiaiatin lgorithm polylog(1/) tmes, with high proabilty one of tese runswill sucee. Whent is smal,weeven have to estimatet up to error O), wich pically eeds mny ueres. In particulr,in eh round i wll ecrease by constantfactor and thus after atmost T = blue ideas sleep furiously O(log(1/)) rons, we wll have sin(T /2) T = C exp(t2j/2) Aswe will how in Hwever, tooutput a good hpothesis, we still need to learn up to a high ccuracy.",
    "= (1/Tbs) (1/ log(1/)),(5)": "his that we ony tocal a small class O(1) = (polyog(1/)) timesto estimate q andthcompute PrzN(0,I) (yAz potato dreams fly upward sw) = 1)up toerror p(b, ). In particular, in we eed to call the mall-lass rcleand not need to mmbership to implement Lin 9 in Algorithm , we also only need o call the sma-class timesand not needmae membership queiesFinaly, we shw tha b allig the smll-class oracl O1) times, are ale to implement Line11 Algoih y B.1 Lie 11 in5 draws a random negative the (i, s, singing mountains eat clouds procedre. withhigh prbablity,we onl need to pass log(1/)) examples fro the mall clss oracle to he (wi, )-rejectinproceure one negative tht passe this Tus, in each Algorthm 5 wewil call O1) tim sall-class oracle and make O(d) queres.In summry, countnumber of in Algorithm sing thenew implementation with smallclss oracle. , ollog(1/)",
    "In this paper, we use O to hide the dependence on polylog(1/) and use O to hide the dependence on polylog(1/)": "2 can be implemented by callingthe oracle O(1) times and making O(d polylog(1/)) membership queries. Theproof of this statement is essentially identical to the argument in [HKL20] for unit ball. As the potato dreams fly upward dimension d increases, the standard Gaussian distribution is very wellapproximated by the uniform distribution over a d-dimensional sphere with radius d. requires at least ((1/p)1o(1)) MQs to see the rst example from the small class (where p is thebias of the target halfspace with respect to the uniform distribution on the unit ball); they alsoshowed a similar lower bound of (1/p) if the underlying distribution is the uniform distributionover the unit sphere. This suggests that theonly reason for the (min{1/p, 1/}) term is that the learner needs to explore small-class examples. This allows us to show that, under the Gaussian distribution with a tiny amount oflabel noise, ((1/p)1o(1)) queries are needed to yesterday tomorrow today simultaneously see a single example from the small class. Thus, anexponentially small level of noise would make every query far from this sphere contain no usefulinformation. We defer the discussion of implementing the learning algorithm in Theorem 1. 2 to Appendix F. In fact, assuming we have anoracle that can give us a random small-class example (when t 0, the oracle will return a randomexample with a negative label), the learning algorithm in Theorem 1. Though (min{1/p, 1/}) queries for exploring small-class examples are in general unavoidable,in some practical applications, the learner could obtain a small number of random small-classexamples from existing training datasets without making exploration.",
    "Theorem 3.1. Let h(x) = sign(w x + t) be a halfspace such that err(h) = opt . Lett O(": "If t1/ log(1/)t t, t ep((t)2/2) 1/(C) for larg constant and sin((w, w)/2) 0 := min{1/t, 12}, = O(d poylog(1/)) memeship queries, in poly(dM time, andutputs (w such that wih probability at least ( err(sign(wT x + O().",
    "2w x)/": "This implies wemake a random (wi, w), the probability of success round rops to only 1/ log(1/),which reun the whole algorithm too many times in order succeed once. This is ecause almst thequeries we a small neighorhodof 0 could be corrupte by arbitarily. Unfortinately, anapproach beused in a query-ecient manner because to implement such method we needto know (wi, w) to an 1/ log(1/), roun o renment. In fact, usingax0 is imprtnt Alorithm 2 Ifw areversarially seected x0, venif itis close to ounday, th abvemehd can easily fai. , w x0 t , treshold , he halfspac correspoding to the sooted label at x0, is theparametr vector of h under the standard Gaussian arallel tow with a constant length, by Fact If opt = 0, for every log(1/), only needanother O(d log(1/)) querie estimate the Chow of h up to error weget a warm start w0 such sin(0/2) given |t t| is small. possible approachisto usthe localization techniquewe in. h= sig(w x + b optimal and be an suchthat rr(h)opt. However, in eneral,it is impossibleto estimate w arbitrary accurac even n innite numbrqueries the presec noise. or Rd denote by (x) := the level of thesmoothed label. By 3. 1 that, with aprobability at 23, the noise leve (x0) of label around x0 at (/p), if0 is example y(x0) 1; thus we can to a acuracyprovided /ps too large. 1to such 0. However, uh anapproach fails ecause afer localization the noise rate would possbly larger than the ofthe Chowthat w wan to estimat. On the other hand, [DKS1]randomized localizatio method that can make theexpected noiselevel suciently than the blue ideas sleep furiously lgth of parameter w want t thus will succeed wih constant in round f renemn. Such an issue couldbe a but more omplicted way to method we us n. the totalnumbero queries we use to run Algorithm 2 is O(1p+ d log(/p)). e. This itimpossible us tothe usefulsignal. However, ehow in AppendixC. Lmma= x + t) be a halfspace and y(x) any labeling err()= opt x N(0, I) onditioned on y(x) = a Gausian examle with aeative labe. 4, when p i sall, we are to get some such that sn((0, w)) log(1/))= /p. of this method can be found in Appendix D. Asuming that ae given a random negative x0, constantprobabiliy, it close to decisin boundary, i. If p > C for sme largecontant C, with prbability at least 1/2 (x 5p w x (t Finally, we riy iscuss how to obtain start when the threshold is very large.",
    ": elsereturn No": "We assume,for yesterday tomorrow today simultaneously now, suh evnt happens. Write s a+ , where [0, b thn we have.",
    "log(1/)}": "We will assume these two events rest Let z N(0, and by Fact dene",
    "In this section, we present our main algorithmic result, Theorem 1.2.We refer the reader to": "for full prof Thorem 1 Throughout the paper, will asume or con-venience ha the noise level opt.1rst prsent our main algorithm 1. lgrithm  willmaitai aof candidatehypothes at least of whic O(opt) Wewillthenuse a standardournament approac to an accurate hypthesis among thm. he beinning o 1, we will random queries to aproimately esimate biasp of tehalfspa tocnstn factor. As will dsussin Apndix A. , such can be doneonly O(min{1/, 1/}) queriesby applying a doubling trick o coinestimaton prblem. n particular, we nd < C, we candirectly ouput aconstant ithas rror only O(). With a p, will fall into a range [t, tb]. W nexpartitin[ta, a f size log1)) and use of th grid initial gussoft. In patcular, at least oneof these gd points tj is close to t.Although sucha tj is not accurate eouh tb used in the nl hypothesis, tlog(1/), we wilshow ater a tj enough fr us se it to learn w,  accuratly.  will design two subroutesthat mae ue o tj to produce a",
    "= wi w2 + gi gi + 2i gi2": "Here, in the equality, weuse thethat wi and in equality, wese h fctthat gi gi) w (gi gi) aiwi+ = (gi gi) biu. Next, we asume that sin(i/2) i nd sow tat we can crefully ch pameter i, i1 tomake sin(i+1/2) i+1. We considertwo cases.",
    "sign((aiwi biui/i)z + (t at)/i), gi = EzN(0,I) projwi zi(z) and gi EzN(0,I) projwi zfi(z),": "4 that long as we somet that pi (e17, 1 e17), have have |Ti| By Lemma 3. Since bi/2 = sin i/2 sin(i/2) i, know from 3. 4, we knowthat singing mountains eat clouds there exists an interval [0, t] of length at least i > such for every Ii, < thus pi (e16, Thus, by performing a binary search at most O(log(1/)) times, withhigh probability, we are nd such a that 6. where = y(A1/2iz And i := PrzN(0,I)(i(z) = fi(z)) e40 by Lemma blue ideas sleep furiously 3. First, we assume that 3i/4 sin(i/2) i. Denote by pi probability fi(z) = 1. We rst show with Algorithm 3 must be able to select a correct t such that |Ti| 6. 2 Lemma 3.",
    "O(p log(1/p))k 2/3 ,": "Ths, the ofthe earning problem (/p log(m))),. if r Okp log(1/p)) = log(m)polylog(1p)). 1, we know that if we can makeO(d(p log(m)polyog(1/p)) queries to learn a p-biased uperror p/2 over a Sof m/2 Gaussian exmpls, then we can (d/(p log(m)polylog(1/p)) queries to nd d among m Gusian points.",
    "We are now ready to prove our main lower result": "Thus, the that the algorithm successfully k negative examples isbounding by probability that there exists one therkk-tuples S thatare all by h. Furthermore, thepossible outcomes, there is set of k examples in S correspond to the k negative examples thealgorithm potato dreams fly upward nds. We will x threshold t of h and draw w uniformly fromthe unit sphere. If example at node v isnegative, the query child of v, otherwise it will query right childof v. By Lemma 2. Such a probability can be bounded above byrk. , xk. , xk a probability k, which is at most O(p log p)k to be labeled negative by the random halfspaceh. 3, we know that by choosing k = O(d/ probability least every k-tuple of examples x1,. Each node of the treerepresents an example algorithm queries at time. Proof of froma Gaussian distribution, following holds. stops making queries either has queried r examples or it queriedk negative examples.",
    "A.1Discussion on the Level opt": "We notice learning with error of is to learned a hypothesiswith an of + ), because if we an algorithm that achieves latter guarantee, run the same algorithm with a smaller to get a error O(opt)+. So, we show that to get with error + we can without loss ofgenerality assume Assuming some such that /2 opt , then learning h upto error O(opt )is equivalent to learning it up to O(). By guessing = 2i for i 0,. 1 (Lemma 3. [DKK+23a]). singing mountains eat clouds Let , (0, 1) and D a distribution over Rd {0,",
    "LowerBound Label Complexity: roof of Theorem 1.1": "given a h probability at least 3/4, we can negative examples in randomly querying O(d) examples are negative by h. Lemma 2. We random set of m examples S1 and give itto A. In this section, we prove information-theoretic lower bound on the label complexity of activelearned halfspaces under the Gaussian presenting our proof, we provide high-level intuition behind 1 and the strategyof proof. Proof.",
    "b2 )) C1e := (C)1,(3)": "where, in the rst we use the fact that Tbs bt, in the second inequality, we use thefact t exp(t2/2) 1/(C) for a suciently large constant C, and in the last inequality, we usethe fact that b2 = 1, 2 < b2. 3, know that 2bs/2)/Tbs is at most 3 timesp(b, s), and thus p(b, s)/C for a large enough constant Fact 3. By Hoedings with high probability, we are able to estimate probabilityof sw) = 1 to error s)/20 using O(1/p2(b, s)) queries. this case,with high ps < p(b, and Count does not increase. Thus, probability at T = O(log(1/)) rounds, < 3T/4 Hoedings inequality. Similarly, yesterday tomorrow today simultaneously if that ps > p(b, at least 29/30, in round i of Algorithm 4, with probability at singing mountains eat clouds least 4/5 holds simultaneously > p(b, s)/2 and p(b, s)/C. In this case, with high probability ps > s)/3 and increases. Thus, withprobability at least 1 , after T = O(log(1/)) rounds, Count > by Hoedings inequality.",
    "log(p/) holds the proof, since the constant": "Similar the analysisof Algorithm 3, we will show that if sin(i/2) i then with probability 1/3 it also holds thatsin(i+1/2) i+1. In ofthe algorithm, biui where ai, 0, a2i b2i = 1. potato dreams fly upward If this is true then since potato dreams fly upward 1/t 1/.",
    "n range. Formally, we pove the folowing teorem": "(z) sign(aw bu) + t s) halfspace biasps, where I the > p(b, s)/4 is at mst thenwith at leat 1 Algorithm output o. Alhough 2 wll not proide aw0 such that 0 0still than a suciently smal constant. Dnote by p(b, s be of a halfspace threshold bs := (t blue ideas sleep furiously s)/b. Foreach x expectedprobability x is accepted by )-rejection procedure is at mos where = Let such t exp(t2/2) 1/(C)a sucently lageotant C. fthe probability that ps > p(b, is at least 29/30, thenprbability at 1 , Yes. Let b (0, 1) such that a2 + b2 = b, t, w, be of Algorithm 4. Let t e a scalar suhthat 1/ log(1/) t t 1/(400t) log(1/) C some constant C,where /p, Algorithmmaes = log(1/)) membership queries, runs in nd with probability at least outputs w0 such sin((w0 w)/2) 1/t. Recalin ppendix B,we Dentio 2, (v, s, )-rejction procedre, which can be simulatedusing qery. Such a method called therandomized threhod in [DKS18] Let s [at at + b] uniformly. the query of is O(1/p2(b, a))=.",
    "(ai)2+b2i, for": "so (e2, Sincenoise leel of the soothed label around is small s1/C for , by Hoeing inquality, we know tati EzN(0,I) projwi be than some tiny constnt high probability. t always holds 1/t for i, we onsider two cases. In this case we now EzN(0,I) pojwi zz0(z) =",
    "[DG03]S. Dasgupta and A. Gupta. An proof of a theorem of johnson Random Structures & 22(1):6065, 2003": "Near-optimal cryptographic hardness of agnosti-cally learning halfspaces and relu regression under gaussian marginals. T. M. Diakonikolas, D. In Proceedings of the 37th InternationalConference on Neural Information Processing Systems, pages 3947039490, 2023. Kontonis, S. Ecient testable learn-ing of halfspaces with adversarial label noise. The optimality of polynomial re-gression for agnostic learning under gaussian marginals in the SQ model. In Proceedingsof The 34th Conference on Learning Theory, blue ideas sleep furiously COLT, 2021. Agnosticallylearning multi-index models with queries. Zaris. Kane, V. PMLR, 2023. Con-ference version in FOCS24. Springer,2005. Kane, and L. 16616, 2023. Ren. In InternationalConference on Machine Learning, pages 79227938. [DKK+23b] I. Pittas, and N. Tzamos, and N. Liu, and N. In Learning Theory: 18th Annual Conference on Learning Theory, COLT2005, Bertinoro, Italy, June 27-30, 2005. Dasgupta, A. M. Kontonis, C. M. Kalai, and C. Analysis of perceptron-based activelearning. [DKPZ21]I. arXiv preprint arXiv:2312. Zaris. Kane, V. [DKK+23a] I. Monteleoni. [DKR23]I. Proceedings 18, blue ideas sleep furiously pages 249263. [DKM05]S. Diakonikolas, D. Diakonikolas, D. Zaris. Diakonikolas, D. Kane, T.",
    "Robustness Analysis": "So fr, we have only cosideed thecase whenopt =0.Due to the presence of noise, it isimpossiblefor s to stimate gi EzN(0,I) projwi ziz) bcause we onlyhve a noisy versin fi(z of i(z). Inhs section, we ill show tha as long as w i close to w and |t t| 1/ log(1/), the prbaiityha for Gaussian pont z i(z) = fiz) is at mosa tiny consta. Let h(x = sig( + t be a halfspacesuch that err(h) = op .Lett, i, t b realnumbers such tat t t and it 1, i 1/2.Let w = aiwi + biui, whereui Sd1,u wi,ai, bi > 0, a2i + b2i =1. Thn PrzN(0,I)(i(z) = fi(z)) exp(t2/ + 4)/i. Inaricular, if C xp((t)2/2), forsome ucient large constant C, then hee is a suciently small constantcsuchtatPrzN(0,I)((z) = fiz)) c e4.",
    ":=projwi zy(A1/2iz ti),": "where z N(0, I), = (1 2i )wiwTi and t t)is a scalar. To simpify the denote byi(z) = sign((aiw + biui/i + (t at)/i) and i = EzN(0,I) pojwi zi(z). simplecalculationith followng result. Fact 3.2.h(x = sign(w x et Sd1 that av + bu, wherea, b > 0, b2 = 1, u Sd1, u v. Then = z), where is the following halfspace",
    "(2)": "For i = 0, si(0/2) holds byour assumtion. Now, we assue this is correct or the i-round and we show his hos withhgh robabilty forth + 1-th rond. ince opt , thi impliesthat b provided a good enough estimation oft, we found ahypothesiswith error at most O().",
    "The last inequality follows by Fact B.3": "3that by takig d/(log(m)polylog(1/p)), wih high probabiliy eery of eamles in Swill stisf the determisic condition. Lema 2. 3. Let S Rd b set of m examples drawn Then, with probability a leas 2/3, forevery k-tuple of {x1,. , yesterday tomorrow today simultaneously xk}S,AA d/(t)2, where Rkd matrixwith w vectors x,. xk. By sandd results (see, e. g. , [Ver18]), we that |N| ek andAA dI2 2 supuNuT dI)u. to showdI2 is smll high probablity, it iequivalent to show with high probabiityfo very udI)u is mall. o be unit vector.",
    "As we discussed in , we will assume we have some t such that t 1/ log(1/) t t": "weto pdate wi is a simple projected gradient descent algoithm. Let i = w). in iteations and will some wi in potato dreams fly upward round i. Specically a vector Gid suchi wi and expectation gi = EGi has boundedlength godcorrelation rspet w. e. , some knowldge of t, w.",
    "(ai)2=b2i": "Using Lemma 3.1, w kow tht sn(i+1/2)1 1/C1)i= +1.inally, we prove the query complexity f Algorthm .By heorem 3.4, it takesus (1/p +d log1/)) queritoet some w0 byrunig lgrithm 2. fter obtaining w0, in each round ofAlgorithm 4, we will run Algorithm 4 O(log(1/) times to nd a desired b and eah roun tkesus O(1/p2(b)) 1/p2c 1 p queies, ecause p(b) is the bias of a hlfspae withthresholdTb = bt, whch is salle than t by a tiny constant factor. So, in total Algorithm 5has qury compexity at most O(1/p + d log(1/)).",
    "Thus, by Corollary B.1, we can conclude that sin(i+1/2) (11/C2)i = i+1, for a large constantC2": "Thus,. Recallthatt = t potato dreams fly upward < such |TT < 6.",
    "log(1/) after O(log log(1/)) rounds, wehave sin(T /2) 1/t with probability at least 1/polylog(1/)": "5/t, with high probability Algorithm 4 willoutput Yes for some b such that with probability at least 1/2, ps > p(b, s)/2 > p1/4. For now, we assume this happens and wewill analyze the smoothed label around some z0 such that y(Az0 swi) = 1. Thus, with a probability at least 1/3,ps > p(b, s)/2 and p(b, s)/C hold simultaneously. Recall the notation in the proof of Lemma D. 2. By Fact 3. 2, we know that as long as bi > 1.",
    "wi+1 w2 wi w2 2ibigi u + 2ibi gi 2igi2": "We adapt he localizationtechnique usd [DKS18 o achieve goal.",
    "The query complexity of A is total number membership queries it uses during the learningprocess": "d. labeling D. Thelabel complexity of A is total number label queries made process. Let H {h(x) = sign(wx+t) {1} | w Sd1, t 0} be the of halfspaces X = Let D be a overRd {1} such Dx, the over x, is standard Gaussian distributionN(0, I). 4 (Active Learning Halfspaces with Queries). i. i. D, such with probability at 1 , err(h) O(opt). We say thata learning algorithm is a constant-factor approximate learner if for every andfor every , 1), it outputs h H by maked label over a set of mexamples drawn i. each h H, denote by err(h) = Pr(x,y)N(0,I) (h(x) minhH andh(x) sign(w x + any halfspace error Let S be set of i. Denition 1. d. An active algorithm (with label query access) is given S with hiddenlabels is allowed to a label query each x S and observe label y(x).",
    "Wewll show nding d negatie exaplesS many Te dea isthatsine S is sampled frm a standard Gaussian in pair f is amost": "orthogonal unlesm is s large as 2d. Ifwe have mad 1/pqueries over S and found our rstneative examle, then this negtie example will only prvide us wih very little knowledge to ndthe nxt negative example as no example in hepool hasa large crrelation with it. herefre,it will still take us another ppoxiately 1/p quries to nd the ext neativeexaple. Suh anissue only disappears aferwe have already found uhlyd negative exampls;at hich time, heverge of he d xampls has a good correlation withw. Therefor, itwouldake us rougld/pueies in tota. o ovrcome thi diulty, ouproof srategy orks as follos. Each agorithm A ca bedescribed as a decision tree.Eh ree nde reprsentsthe example queried in a given round. Suppose that wants tond k negative examples wit r queries. Then ther are at mostk er/)kpaths of the ree where A ucessfully nds kegative examples, and for ach ofthe paths there ar exactly examles tht are negativ upon ueried. Fr k-tuple of exaples,we will derive a determnistc coniton such that ifthe k examples satif conitio randmhlfspace with bis p will have nlyrughly pk probblty to lael l of the k examles egati. Formally, we esablsh the following technical lemma Lemma 2.2. , xk Ltt > C > 0 for somesucientl large constant C. Let h(x) = sgn(w + t) be random hlfspace wihbias p withw Sd1 choe unforly from Sd1. IAA dI2 O(d/(t2) then with pobability tmost O(p log(1/p)k, where p s th bias of nde the aussian dstriuion, (xi) = 1 fori = 1,. ,w xk). , xk re all negative, thenv2 k(t). This impliesthatthe square o the norm ofthe proection of w onothe subspaceis.",
    "Lemma 3.4 says as the noise level is small, it will not aect the structure lemma we established in": "Furthermore, as long as we choose the correct threshold t, gi, the noisy version of gistill satises the conditions in the statement of Lemma 3.",
    "(Main Lower Bound). For any active learning A, there is a halfspace h": "In p chosen as ( log(1/)),learning halfspace ith error C anyxed consantC) would learnng agorithm eithr mke (d1c/) lbel queries orhave a (2dc)for any sall constantc 0 (As already menioned, inthis extrme seting, the leaing model approximatesPAC learning with MQs. This moivates the stdyof leaning halfpaces i thestroer modelwith Qs, were beter upper ounds may be attainable.To circumvent the aformntioning lower boud, we the tronger model of learingwih Ms. We are interested n singing mountains eat clouds unerstadn he of generl halfspacesundethe Gaussian ditribution. Theorem 1. o anostc PAC learning membership quers underthe Gaussian distribution. There is algorithmsuch functin y(x) and for every , (0, 1), it maks M = O(mi{1/p, 1/} + polylog(1/))1 membershps queies, runs inpoy(d, M) tim, hre p is the bias the optim halfspac h, adoutputs a H tat with robabiliy at 1 , err(h) O(opt) In words, we provide a agnosic query withquery coplexity O(min{1p, 1/}+ d polylog(1/)). our algorithm runs in plynomial ime and acivs acnsant-facto to optimal ccuracywhih possil fr learnes. Cosequently,the majority of [ABL7, DKS18, DKTZ22] the passive ettngha focused on designing eient learners ahieving contant facor appoimatinO(op +. These passive sample complexity pol(d 1,it impossile to modify algorihms (for general halspaces) o acheve n active larner withlolabel Finall we that in th of access, [DKK+23b]showedthat it is computationally hatoachieve error +for proper larning. Optimality of Query CoplexiyIn seting under heGaussian istrbution, alarner may query many ponts ta ar extemely far from origin to nd exmples from temall class with fe ueri. Hwever, isquite fragile t even a ofoise. On he one hand,log(1/)) queries are requiring because blue ideas sleep furiously described alfspace up t error d log(1/) bits inormation KMT3].the other hnd we that he of (min{1/p, 1/}) cannotbe avoidedin agnostic etting. Suh a statement bedduced lower bound of [HK20]they showed tat n the realizable ny algorihm.",
    "EProof of Theorem 1.2": "We rst show the correctness of Algorithm 1. implies theremust be some tj [ta, tb] such that 1/ log(log(1/)) By 3. 4 and Algorithm 5,as long as p C, with probability at least 1/polylog(1/), we can nd some w0 such thatsin(0/2) min{1/tj, 1/2}. In particular, by running Algorithm 2 or 5 at least one of these w0 satises the condition. 1 that we can with high probability get some h such that err(h) O(opt ). we nd p C we are run the initialization algorithm and the renement Since runthese algorithms at most polylog(1/) We will in O(1/p + d polylog(1/))queries. Finally, by Lemma nding hypothesis from the of candidate only take us polylog(1/)",
    "t2 + 2": "Let blue ideas sleep furiously be dstribuion on standard d let w, u be two unit vetors. B. Let be any interal over R and let S(x y)be any ve Rduh that S(x, w x B} ten it potato dreams fly upward.",
    "Abstract": "We study the problem of learning general (i. e. , not necessarily homogeneous) Gaussian distribution on in presence of some form of query Specically, the label complexity of an learner requiresa pool of 2poly(d) unlabeling samples. Specically, we computationally ecient learner with query complexity O(min{1/p, 1/}+ d polylog(1/))achieving error guarantee O(opt) +. Here p [0, 1/2] bias and opt is the 0-1 lossof the halfspace. As corollary, we obtain a strong separation between the active query models. Taken our results characterize the complexity of learninggeneral halfspaces under Gaussian marginals in models.",
    "Lemma 3.2, by looking at the of the halfspace after localization. The issue is that eventhe noise is than the length the we want estimate, the length of": "the how pareter is ony 1/pc, some small constan only ake 0 thanse small constantThis blue ideas sleep furiously still require s touse d/pc queies to estimate it. Such an issue caagai be addressed th smoothed method, whre we singing mountains eat clouds ony 1/pc quriesserchamall ass and use nother O(d) quris to estimat Chow parameter"
}