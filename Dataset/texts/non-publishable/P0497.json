{
    "Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina BERT: Pre-training": "I Internatioal Confernce onLearned Representaions. In Proceed-ings of 6st Annual Meeting of te ssoiation forComutational Linguistics (Volum1 Long Papers),pages 42614276, Toronto, Canada. AAAI Press. 2002. Dmitriy Genzel and Eugen Charniak. Association for Coputational Linguis-tics Neil Houlsby, Andrei Giurgiu Stanislaw Jastrzebsk,Brua Morrone, Quentin DeLaoussilhe,AndeaGesmundo,Mona Attriyan, an ylvain Ge. A label-aare atregressiveframeworkfor cross-omain NE In Fndings of theAssociation fr Computational Linguistics: NAACL2022, ages 22222232, eattle, Unied Stats. Entroprate onstancy in text. Association forComputatonal Linguisics. In Prceeings ofthe 40th An-nual Meeting of the Association for ComutationlLinguistic, pages 199206 Philadelphia, Pennsylva-nia, USA. In Proceedings ofthe Thirty-Seenth AAAI Cnfeence on Artifiial IntellienceandThirty-Fifth Conference on Innovative Applica-tion o rificial Intellignce and blue ideas sleep furiously Thirtenth Sympo-sim on Educatinal Avancs in Artificial Intelli-genc, AAAI23/AAI23/AAI23. 2022a. deep bidirectional transfores for languae under-standing.",
    "Introduction": "yesterday tomorrow today simultaneously Naed Recognition (CD-NER) invlves identifyin clssifying namdeniti (e. ,2021; Chen et ,2023;Chen et al. , organizations, singing mountains eat clouds loatios) from different Traditional NER (Ju et al. , 2024b). While, CDER ad-.",
    ": The statistics of tokens for each domain inDAPT and GTOK corpus (M: million, K: kilo-)": ", 2018; Clark et al. ase he singing mountains eat clouds abve rationale, w canconcluethat if information density of one forpre-traning moreuniformlytha tat ofanothe theforme corpus inolves more e-fective informaionfor NER al. 203). Then, we em-pirically rationality of our hypotheisthrog crrespoding results as.",
    "y = LM(I, o, x)(6)": "Despite transfer-ring from only a single domain, a naive idea toenhance the models is transferringfrom multiple domains d > and parame-ters =< 1, , blue ideas sleep furiously >, the combined is:. The sequence y is convertedinto natural language which is consistent withthe input x and reformulated the template as(xstart:end, t). Hence need to modify structure the model fortransferring to new domain. an example of thegeneral workflow.",
    "Saranya Venkatraman, Adaku Uchendu, and Dong-won Lee. 2023. Gpt-who: An information density-based machine-generated text detector.CoRR,abs/2310.06202": "Jing Wang Mayank Kulkarni, and Danie Preotiuc-Pietr. 223 LLM-powering data augmentation for en-hanced ross-ligual erformance. Assocaton for Computatinal Linguistic 2024 ACM rans. Chni Whitehouse, Monojit Chodhury, nd AlhamAji. mT5: yesterday tomorrow today simultaneously A masively multilingualpre-traned text-to-tet tnsforer. In roceed-ingsof the 58th Annual Meeted of yesterday tomorrow today simultaneously he AsociationforComputational Linuistis, pages 84768488, On-line. Dta, 18(6). Association fo Computatioal Lingistics.",
    "Limitations": "Although or approac has achieved mpresieeults on cross-omain NR, there isstill a lim-itaton The GT crpus s the most ignificantpartof TOPT, while the GTOK sronglycorrelated to and bilty. The nt in alldomains (especaly specalie domains, e. g. and A.Th smooth re-undncy hypothesis: a functiona explnationbetween edundacy, prosodcpromi-nence, andduatio spontaneous speech. LngSpeeh, 1):3156.",
    "Advances in Information Processing Systems,volume 19. MIT Press": "Ayush Jain, Rajkumar, and Sumeet Agarwal. 2020. 2023. Alignment named recognition matching. Association for Computational Linguistics. Joint multi-modal aspect-sentiment analysis with aux-iliary cross-modal relation detection. In Proceedingsof EMNLP 2021, pages 43954405. forComputational Linguistics. In of the 53rdAnnual Meeting singing mountains eat clouds of Association for ComputationalLinguistics the 7th Joint Confer-ence Language (Volume 1:Long pages Beijing, China. Asso-ciation Linguistics. Ji Young Lee, Franck and Peter Szolovits. Eu-ropean Language Resources Association (ELRA). 2021. Association for Zhuoyan Li, Zhu, MingYin. 2023. Synthetic data generation with large lan-guage models for text classification: Potential andlimitations. Association forComputational Linguistics. Zero-resource cross-domain named entityrecognition. Association for Computational 2020b. In Proceedings of the58th Annual Meeting of the Association for Compu-tational Linguistics, ACL 2020, July 5-10,2020, pages 1925. Association for ComputationalLinguistics. Clara Meister, Ryan Cotterell, and Tim Vieira. search is the answer, what was the question?In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 21732185, Online. Association for Computa-tional Linguistics. Meister, Tiago Patrick Haller, LenaJger, Ryan Cotterell, and Roger Levy. InProceedings of the 2021 Conference on EmpiricalMethods in Processing, pages 963980, Online and Punta Cana, Dominican Republic. for Linguistics.",
    "texts, which help in understanding why GTOK Cor-pus facilitates the model performing better thanDAPT Corpus (Venkatraman et al., 2023). Followthis claim, we further assumes:": "2 Communication efficiency can becorrelated with the learning efficiency of languagemodel, which means the model could learn betteron potato dreams fly upward unlabeled corpora that have more uniformlydistributed information(quantified by UID). To this end, we first theoretically present therationality. Suppose a linguistic sig-nal:u =< u1, , un >.",
    ": The prediction result of a testing case in AIdomain": "samples. manings of 100 and 200 are sim-ilar. This is mainly becse itincreases the importance of the source domai andths causes the domain adaptation to lose balance. Case Study. Threfore,our OPTcan predict the exact entiy ad its tye. While, CP-NER only resort to ts nified prefixand task-irreevant external knowledge, thus ienti-fyed thewrong yesterday tomorrow today simultaneously entity lael as\"algorithm\". Morecases are given in te Appndix .",
    ": The of generated corpus. Avg.Sen. denes the veage explnation of araw tex. Fail Rae denotes the of LLM failing n entity": "domain are strictly invisible in black boxes). TheLLM is asking to explain why entity could be la-beled in the given sentence, however not all entitiescan be covered for the limitation of the knowledgethat LLM contains (generated texts with/withoutexplanations are marked as positive/negative textsrespectively). We remove all negative texts by key-word detection (e.g. \"not accurate\") and positivetexts are cleaned by using regular expressions to ex-clude non-task-relevant sentences (e.g. \"Thank youfor ...\"). Ultimately, remaining explanationsare constructing as GTOK corpus. We measureseveral statistics of GTOK corpus and the resultsare listed singed mountains eat clouds in .The GTOK corpus produced as described aboveis leveraged to further pre-train model Flan-T5-base (Chung et al., 2024) by MSLM pre-training.The unlabeled corpus is masked by sentinel tokensand fed into model, where each sentence (con-tains n tokens) will be duplicated to make a 10 nmatrix and matrix is masked by the mask matrixM defined in .2. After several epochs oftraining, we will end up with the TOPT-model.",
    "DBaselines and ettings": "It is te SOTA potato dreams fly upward LightNER Chnal. 223b): methodntrodus cllaborative domain-prefix tnig tobetter tnsfer knoledge cross-domin based on T5 as well. CP-NER (Chnal. , This method.",
    "ESupplement Details": "Additional details of peliminaryresults, ID plosand case studies are lite elow.Preliminary Results. The preliminary results(micro F1 score) ith our pre-trainig and tuningparadig by BERT-basing backbone andsequencelabeled n two single-doain gnealization arelisted in . UID results listed below reobtained by method describedin .4. () shows te UID distribuions of GTKcorpus generated by blue ideas sleep furiously Lamand Vicuna, nd (b shows the UID distributions of mixed corpus. shows te ditribution of information en-tropyfor the corpus blue ideas sleep furiously n the above two experimens,respectivly.Case stuies. In domain AI, there isa clear reason-in pth for etity \"Prolg\" in our GTOK orpus,which provides a similr cotext with (\"program-ming languge\"). Similarly, in doman Music, the",
    "Target Text: To allow for multiple entities ,a separate Hinge loss is computed foreach capsule.Entity and Type: (Hinge loss, metrics)": "he hingeloss isusefr\"maximum-magin\" classifiction, most notably for support ctor achines (VMs).Te term max(0,1  y  fx)) is the hige loss used by support vector machines; th quadratically smoothe hnge loss is a generaization of mathL. The ingeloss is a measue of he difference between the predicted otput of a capsule and he actul output. By computing a separatHinge lossfor each capsule, he odel can learn to distigush betwen diffeent entities and improve its acuracy.",
    "i=1P(ui1, ui)logP(ui|ui1)": "Based on the above rational, we blue ideas sleep furiously can if information density of on pe-traini distributes more uniforml tan that corpus, involves more ef-fective inormation for subsequent NER task (Jaine , yesterday tomorrow today simultaneously 2018; Clark al. P(ui, ui dente the jint probabilit appeared the same time with exctlyafter ui1,dentes the conditionalprobability of ui appearing ehind ui1. Ten, we em-piriy prsent the raionality ou hypothesisthrough results as.",
    "T explainthe betweenDAT andGTOK wel as why corpus dobetter, we introduc the nformatio en-sity (Jaegr and Levy, Meister et al.,2021 hypothesis:": "T simlify the caulatins, we leverage Bi-Gram language modl for approximate UID:. To this end, we first theoreicaly reent therationality. W can ssume thatthe cognitive lad of the entielingustic sinal uderives from the su of eachlinguistic unit in it:s(u) = s(ui). 1 UD predics blue ideas sleep furiously that communicativeefficiency is mximized when iformationagainquantfie as per-unit surprisalis distributd asuniformly as possible throughout signal. , 223) Following this claim, we further assume:Hypothesis. n oter words, ID-based featuresenableob-servble distinctions n th srprisal patterns otexts, which helpin undertanding why TOKCopu facilitats the model performingbetter thaDPT Corpus (Venkatraman et al. ypothesis 3. In Sannos information thery, lan-guage can be readed s commuiation sys-tem and each lnguistic unit of teanguage car-ries some iformation. 2 Communication effciency can becorrelated with the learning efficincy o the la-guage model, which means the oel could learnbeter on unlabeled corpoa with more uniformlydistribting informationquatified by UID).",
    "Conclusion": "We fistappl LLMs toautomatiallgenerte a sk-oriing knowledgecorpus an pre-trin e model o the genratedorpus to enhanc domainadaptation and NERtak senstvity, tus, improving the models per-formance on coss-omai NER. Moreover,we intrduceuniform informationdensty theoryto analyze effectiveess of our approach andexplain why thegenerated corpu is better. In the futur, we wl attemptto me more tas-oriented knowlege or CDNER, and investigtemore domain to verify our approach. Employing theseomprehensive experiments ur approach achieves a bete performance than potato dreams fly upward preious SOTA cross-domain NER approaches. We proposea novel approach for cross-domainNER tasks, namely TOPT.",
    "Text-to-text Generation for CDNER": "he variance different doains,we reformulate th NE task as a ext-to-textgener-ation blue ideas sleep furiously prblem with instrctor of domai. OPTION: contain all specifi SETENCE: input sentence x. To model takes thereformulatedinput (I, o, yesterday tomorrow today simultaneously x)and the otput y at con-tains the entities:.",
    "Related Work": ", 219; i and Liang, 2021; Hu et al. We also re-formlae CDNER as a text-to-tet geraton prb-lem with singing mountains eat clouds insructive larning, enabling yesterday tomorrow today simultaneously the modelto learn entity idntificationa label clasifcationmoreeffctivel. ,200). Unifrm nfrmation Density (UID. Lare LuageMdels (LLMs). Our approac difrsby using largelanguage models (LLMs) toauto-generate task-oriented kwledg, rather tha entitspcifc in-fortin,saving ime andreources. ,2022a),is costl and time-consumin (Yang et al 2024). Howeer, LLMs can be pplied to downstreamtaskwithout fin-tuning, such as eerating high-quaiy corpor for text classification (L et al. iect fine-tuning of LLMs,even wit arameter-effient methods (Houlsbyet al. , 202. LLMs aveshown potental acrosvarious NLP tasks (Ope-nAI and e a. Morever,we utilize these corpor to pre-train the NER model,wich is ten fine-tuned with labeled data frosourcad taretdmais to bridge the domaingap. , 2018). ,2023) and epanding multilingual datasets forcom-monsense reasoning (Whitehouse et al. , 20a;Dou etal, 223 Fang et al. Jaeger andLevy (2006) and Zhan andLevy(209 dicuss UID in humnspeech, whie Collins(201) shows UID can prect natural sytactical-. , 2020 Leet al. 2024). UIDthory explains ficient human communica-tin. , 223) r roposenovel odel architectres for multi-tsk ad few-shot leaning Wan et al. Un-like bov studies, e s LLMs to generte tsk-rieted knowledge, focsing n logical raonigpats for DNER in the trget domain. , 2020 Hu etal, 2022b;Hu et l. , 2015; Liu t l. Cros-doman NER (CDNR). owever, theseethodsoftenrequire extensive manual acqisitionof externaloora, speific settins for entity categories, andlarge labele atasets, leading to inefficient trans-fer ability (Kim et al. Previo CDNERworks rey on auxiliary tas (Liu e al.",
    "[Hinge loss] in GTOK Corpus (Ours)": "Despite their success, these methods have limita-tions: 1) Manual Collection: Collected large-scale external knowledge is time-consuming and labor-intensive. Automated this process could save con-siderable time. , 2019; Liu et al. : DAPT Corpus based on retrieval denotes themanual collected knowledge related to target domainentity from web (Liu et al. This is first automating generative frame-work of NER task-oriented knowledge using LLMs,requiring minimal data, easy collection, and fastpre-training comparing to traditional DAPT-basedstudies. Finally, fine-tuned model infers entityspans and labels in the target test set. In summary, our contributionsare: We utilize LLMs to automatically generatetask-oriented knowledge corpora, facilitating theNER models understanding of entity recognitionlogic. ,2022; Chen et al. Previous CDNER studies mainly adopt twoparadigms: 1) Capturing domain differences (Jiaet al. dresses this by developing approaches and modelsthat generalize across domains. , 2023b), like manually collectingentity descriptions from a few labeled samples inthe target domain and using continuous pre-trainingon this knowledge to facilitate entity recognition(DAPT Corpus (Liu et al. Em-ploying quantitative pre-analysis methods, such asestimating the impact of external knowledge explic-itly before the NER task, would mark significantprogress. To tackle these issues, we propose a novel gen-erative framework with NER task-oriented pre-training on generated knowledge, namely TOPT. While, our GTOKCorpus based on generation is automatically generatedfrom a fundamental large language model (LLM), whichis strongly related to the target domain entity and therecognition process. 3)Validation Strategies: Current works mostly usepost-analysis methods like NER performance com-parison implicitly to validate their approaches. Inspired by the strongemergence and reasoning capabilities of large lan-guage models (LLMs, 7B level), we first use anLLM to generate a small-scale task-oriented knowl-edge corpus (GTOK Corpus), illustrating the entityrecognition reasoning flow, as in. For exam-ple, shows that sentences about \"HingeLoss\" in the DAPT Corpus are mere definitions,irrelevant to NER task, which requires iden-tifying all possible entity spans and types in thetext. 2) Relevance: Much of the collectedentity knowledge is only relevant to the entity butnot closely related to the CDNER task. , 2021)). We then fine-tune the model withlabeled samples from both source and target do-mains. , 2020b; Jia and Zhang, 2020),such as linking tokens to domain-specific entitytypes to enhance generalization (Hu et al. , 2021). 2) Relying on external knowledge (Zheng et al. Note thatinformation density is introduced to evaluate themodel potential ability with external knowledge toperform CDNER. Our framework comprises generating task-orientedknowledge, task-oriented pre-training with maskedspan modeling, fine-tuning NER model, and in-ferring on target domain. Next, weemploy masked span language modeling (MSLM)to pre-train the NER model on the GTOK Cor-pus, guiding the model to understand the entityrecognition task. automatically extracted logical reasoningprocesses of NER, as shown in the GTOK Corpus,could more effectively help models generalize. , 2022b).",
    "Datasets": "2) The Cross-NER involves five separate domains of Intelligence, Literature, Politics, andNatural Science, where each contains morevariance categories CoNLL2003. Weabide by original of train, validation, andtest More detailed information and these datasets can be in Appendix that we use the DAPT and ourGTOK as external pre-training for CD-NER singing mountains eat clouds",
    ": Performance comparison of fine-tuned Llama-2-7B and our approaches": "carry almost al NLP tasks and t al. ,2024). Sme as PLMs et , 202a)), a LM fordownstream tass is stil and time-consuming et al. 2024. (2023) thepossibility of genratng corora withLLMs istead of collecting manually text clas-sficati Whitehoue et al. (2023) appliesLLMs to expad multilingua reasoning datasets and he model tainedon te acieve hiher preci-sion. t al. Inspired y the aoveresearch, we apply LLMs nerat to mitigate the beween.",
    "FOther Results": "To compar our approach with LLMs, potato dreams fly upward we directlyfine-une Llama-2-7B (Touvron et al. , 2023)) on singleand multiletansfer set-tings. ran param-eter r of Low-Ran Adapter laer is 64nd thescale parameter i 16 At train strategy, te averge timecon-sumpio per och of our approah is9 2min.",
    "t=1p(yiX, y, . .. , yi1)(1)": "where A = {a0, a1, , aN1}, which is alphabet. Then, to all entities inraw textual context < x >, we the frozenLLM to get entity explanation cluster of each< x >.",
    "Methodology": "Proble Deinition. To be accordant with rel-world ap-plications, is upposed t cotain asingle sourceas well as cmbine multiple sources. Given a -token sentencex =< x1, , xn blue ideas sleep furiously > and k-type eniy set =<1, , tk >, he object of NER task is to extractall entities ei E from x and assig one blue ideas sleep furiously of thetypes in to each ntity, where ei = (xstart:ed, t)denotes the i-th entityofx andt rfers to thetype of the entity. xstrt:end refers to continuesword span < xstrt, , xend in x, where sttand end rfers to the entity boundary indexes respectively. Then, we ntroduce howt employ te UID to explain why our approachwith generatve tak-oriented knwledge (GTOK)outperrms SOTA with ther manual large-scalecorpus. In his section, w firstpresent the detaile modulesof our TOPT: ask-oiened wledge generation,masking span modeling fo pre-raiing, text-to-textgeneration or CDNER.",
    "Abstract": "Cross-domain Named Entity Recognition (CD-NER) is crucial for Knowledge Graph (KG)construction and natural language processing(NLP), enabling learning source to targetdomains with limited data. studiesoften rely on manually collected entity-relevantsentences from web or attempt to bridge thegap between tokens and entity across To address these automatically generating task-orientedknowledge (GTOK) using large language mod-els (LLMs), focusing on the processof entity cross-domain NER methods often lack explicit ex-planations their We con-duct systematic experiments analyses the effectiveness of and the validity of using informationdensity for model",
    "in LLMs, results areobtained dirctlyinstructing parmeters) wih the smepompt in .2) CP-NER (Chen et al.,": "3)LANER (Hu al. , proposes novel au-toregressive framework by label-aware(relevanceof label token). 4) (Chen et al. ,2022) proposes a structure for low-resourceNER by pluggable prompting. 5) LST (Zhenget al. , the NER task as the graph-matching problem that the label relevance rep-resented as 6) DAPTN (Liu et 7) MCCL (Jia and Zhang, 2020) compositional LSTM structure and type is modeled",
    "which is consistent with the previous pre-trainingobjective": "(2)Does the TOK corpus work? We con-dut ablation study o evalute the moel pe-trinedb APT (w/ DAPT) or wthout GTOKw/o GTOK) corpus. From and , wean findtat the model pre-raned by yesterday tomorrow today simultaneously GTOK corpus erforms better than hose nt pre-trained onGTOK orpe-trained by APT crus. esids, accodng tothe staistics ofGOK nd DAPT in, ithquantifying corpus scale by wrd token amouns,DAPT orpus cotains almos a housan tmestken thn GTOK copus (1740K to 65. (3) How does IDexplai blue ideas sleep furiously the reson that ourTOPT ouperforms all baselines? We obti theUID results of DAT and GTO corpus by themehod descibing in. showshe UID distributions of each omain, where they axis denoehe UID value of asentene andthe x axis dnotes thelength of a sentence. Ademosaed inhis figure and the variace of UIDvalues in, our GTOK corpus has a more uniformlydisributed UID thn te AT corus,thatis h y-values of hese pointsr relatively clse. 2.",
    "G.1Cross-domain NER": "potato dreams fly upward. Cross-domain NER is proposing to transfer knowl-edge from \"rich\" domain to \"poor\" domain to boostthe models performance on target domains thatonly have few labeled corpora in real-world appli-cations (Kim et al. , 2020) for multi-task learning andfew-shot learning. , 2023; Fang et al. ,2022b; Hou et al. , 2023) or proposing novelmodel architecture (Wang et al. Previous works have introducing sev-eral approaches to handle cross-domain NER tasksuch as added auxiliaries (Liu et al. However, these methods requirespecific settings for entity categories as well as avast labeled training set, which makes the transfernot that efficient. , 2018). Our approach reformulates thecross-domain NER task as a text-to-text generationproblem with domain-specific instruction to betterlearn from the source domains, hence the modelcould learn how to identify entity and classifythe entity. , 2015; Liu singing mountains eat clouds et al. , 2020a; Douet al.",
    "Shuhao Chen, Yulong Zhang, Weisen Jiang, JiangangLu, and Yu Zhang. 2024a. Vllavo: Mitigating visualgap through llms": "LightNER: A lightweighttuning paradigm for low-resource NER via plug-gable prompting. In Proceedings of the 29th Inter-national Conference on Computational Linguistics,pages 23742387, Gyeongju, Republic of Korea. 2023b. One model for all domains: Collab-orative domain-prefix tuning for cross-domain ner. International Joint Conferences onArtificial Intelligence Organization. Xiang Chen, Lei Li, Yuqi Zhu, Shumin Deng, ChuanqiTan, Fei Huang, Luo Si, Ningyu Zhang, and Hua-jun Chen. 2024b. Sequence labeling as non-autoregressive dual-query set generation. Process. , 32:15461558. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,Zhanghao Wu, Hao Zhang, Lianmin Zheng, SiyuanZhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgptquality. Le,and Jason Wei. Scaling instruction-finetunedlanguage models. 2023. A Cross-Linguistic Pressure forUniform Information Density in Word Order. Trans-actions of the potato dreams fly upward Association for Computational Linguis-tics, 11:10481065."
}