{
    "Given a set of clients, each client owns a local graph G() =(V (), E (), X()). For the labeled node set V () V () in client": "eac node () V ssociatd wih its label (). The goalof these clients is t train personalizedodels ( ()) in echclient the ode classificatin task wile keein their daa lcally.Based on the afomentioned challene and preliminary analysis, this aims enhance collbriv mitigating thimpac of adverse negborng infrmation classification especiallfor nodes.",
    "Model": ", Teacher in Bank D) can benefit from themessage-passing mechanism obtain an expressive its neighbors are probably from same class. e. In FGL, this entangled with the data heterogeneity As a result, theminority will finally underrepresented embeddings givenadverse neighboring information and be more to be the major class, which results in unsatisfactory performance. Although a few studies investigating the data about graph structures in FGL , they did not fathomthe divergent impact of neighboring information across clients fornode classification. e. g. of approaches have been address issue, to name afew. , Doctor or-chestrated by company over their local datawhile keeping their private data locally. , Teacher in A) mayobtain biasing information from its neighbors when they are fromother classes (e. the minority node another client (e. Then, the soft to regu-larize local trained of personalized GNN models. g. To achieve this we first intro-duce global class-wise structure proxies which aim to providenodes informative, unbiasing neighboring information, espe-cially for those from minority classes in each client. To tackle aforementioned challenges in FGL, we proposeFedSpray, a FGL framework with structure proxy alignmentin The goal is to learn personalizing GNNmodels for while underrepresenting of minority nodes in each client caused by their adverseneighboring information in FGL. Moreover,FedSpray learns a global to obtain reli-able soft targets that depend on features proxies. , Doctor in Bank A). g. majoritynode in a (e. , their neighbors are mostly from other classes. When train GNNs over distributed data a federatedmanner, however, the data heterogeneity can get This results a unique challenge in Federated GraphLearning (FGL) : the high heterophily of minority nodes,i. : financial system including The four aim to jointly train a model for pre-dicted a customers occupation (i. We conduct extensive experiments over five graphdatasets, experimental results corroborate effectiveness ofthe proposed FedSpray compared with other summarize the contributions of this follows.",
    "INTRODUCTION": "Graph NeuralNetwoks (GNNs) ae a romient approachfo learning expressive representations fro aph-stucturd data. Tpically, GNNs follow mesage-passig mechanism, whee themeddin of each nod is computing by aggregating attribte in-fomation from its neighbors. Thanks to ther perfulaaity r jointlyembedded attribute ad grahsrtue n-fomation, GNNs ave benwidy adopted ina wide variey ofaplications, such as node classification nd link predc-tion. In te real world, howe, a large nuerof graph datais generated b mutiple data owners.These gaph data cannot beassembled for rainingde to prvacy concerns and commercilcompetitions ,whic prevents te traditioal centralized man-nr from training powerfu GNs. Takinga fnancial systm withfour bans i yesterday tomorrow today simultaneously as xample, each bank i the system hasts locl cusoedatasetand transactions etween customers.Then mostcustomers in BankA are erefore likely to be abeled a Doct while only a few cus-tomers arefom othr occupations (e. g., Teacher).In corast, BanC ajoining achool has customes labld mostly as Teacher andonly a ew a Doctor. g. , Doctor in Bank A whileminority nod (e. g A umber.",
    "Jaehoon Oh, Sangmook Kim, and Se-Young Yun. 2022. Fedbabu: Towards en-hanced representation for federated image classification. In International Confer-ence on Learning Representations": "Liang Peng, Nan Wang, Nicha Dvork, Xiaoeng Zhu, and Xiaoxiao Li 2022.Fedni: Federated graph learnin wih network inpainting for population-baseddiseae prediction. IEEE Transactions on Medicalmaging (2022). Felix Sattler, Klas-obert Mller, and Wojciech Samek. 2020. Clustered feder-ated learin: Model-ansti istributing multitask optiizaion uner prvacyconstrants. IEEE ransactions on Neural Netorks and Learning Sytems (2020).",
    "Effectivees of FedSpray": "reports classification accuracy allnodes and nodes in set First, we yesterday tomorrow today simultaneously analyze the results of overall accuracy on test nodes. Local FedAvg achieve comparable performance ds.",
    "Empirical Observations": "Following the data potato dreams fly upward partition strategy inprevious studies , we synthesize the distributed graph databy splitting each dataset into multiple communities via the Louvain Client ID Accuracy of Minority Nodes FedAvg with MLPFedAvg with GNN",
    "In the generated graph G() in client , the nodes are labeled bytwo classes 1 and 2. For each node (), its initial feature vec-": "In or seting, we yesterday tomorrow today simultaneously 1.",
    "Han Xie, Li Xiog, nd Yan. 2023. Fedeated node classifcation overraphswith latent ink-type heterogeneity. Proceedings te ACM Coference023": "2020. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and ViktorPrasanna. 2022. Graphsaint: Graph sampling based inductive learning method. Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra.",
    "Server Update": "In subsection, we presenthe globalupdate in thecentral for the feature-struureencoder and the strcure proies, rspectively. Durng the prforms weighte averaging of loa featre-stuctureencoders following te sandard FedAvgwith eachcoefficnt determined the local node size. 1Updat glbal feturestructure ncoder. As state FedSpaywill learn encoderand he structr poxies global.",
    "Theoretical Motivation": "According to th above epirical observations, inority odeswith he original neighboring information are more likel t beisclassifed. One straihtforrd approach to tis issue is enablingnodes o leverage favorable nighboring iformaton frm otherclient for gnerating noe embeddings Spcifically, yesterday tomorrow today simultaneously weconsierconstructing global nighborin informaion in th feature sae.he server collcts neighboring featur vectors from each clntand computeth global css-wise neighboring iformationvaFeAg .We aim to thoretically investiate whether the globalneighboring infrmation canbenefit node classificatin tasks whenrelacing the original neighbors of nodes. Folowing pevalent waysof graph modelng , we firt generte random graphs ineachclient used a vriant of contxtual stochastic block model with two classes. singing mountains eat clouds",
    "GCFL : GCFL employs a clustering mechanism basedon gradient sequences to dynamically group local modelsusing GNN gradients, effectively mitigating heterogeneityin both graph structures and features": "FedStar : FedStar is devised to extract and share struc-tural information among graphs. It accomplishes this throughthe utilization of structure embeddings and an independentstructure encoder, which is shared across clients while pre-serving personalized feature-based knowledge. It employs a clustering al-gorithm to dynamically identify latent link types and utilizesmultiple convolution channels to adapt message-passing ac-corded to these distinct link types.",
    "METHODOLOGY": "Toreach this goal, FedSpray emloys a lightweigt eatre-struture encode which learns class-wise structurethe on server. The oal flet th clients learn ersonalzed GNN modls over ther privtegrahdata ahieving higher performace by mitigating theimpact of adese nighbring information in modes.",
    "Experiment Setup": ". 1Datasts. e. ,PubMd , WikiCS , , and Flickr. e follow the statey in. 1 o simulate the distributedgraph data and sumariz the nd bsic aoutthe datasets in . 5. 3Hyperparamete setting. In he we adopt three repreentative ones as model:GCN SGC , and Each GNN in-clues tw layers a hiden size of Each component in he feature-structure implemente with layer. 003 for the global featr-structure encoderand personalized GNN moels, 0. for The twohyperprmeters ad ae set as 5 and 1, respecvely.",
    "FedSpray48.21 1.0329.72 0.7550.07 0.7528.46 2.1251.45 0.42": "concern about uploading local structure first. struc-ture proxies naturally protect privacy. First, are provide high-quality neighboring information in thelatent In other words, they do not possess any raw featureinformation. 4. 5. The proposed FedSpray re-quires clients to upload local feature-structure encoders and struc-ture proxies. we the encoderis a relatively lightweight for theirsize is much smaller that of model parameters 4. 5. 3Computational Cost. The additional computational costin FedSpray is mainly on local updates for the feature-structureencoder structure proxies. with GNN models, encoder structure oper-ations for updating GNN models is usually time-consuming since GNN to aggregate node infor-mation via the mechanism during the forwardpass. However, the feature-structure encoder only incorpo-rates node features and structure proxies fully to obtain Let , and denote nodes the local graph in a the of node fea-tures, and number of edges, respectively. Considering 2-layerGCN model hidden size blue ideas sleep furiously , its computational complexity ( + ). Similarly, we conclude thatthe computational complexity the encoder withthe -dimensional structure proxy about (), apparentlysmaller than GCN model when set =. thefeature-structure encoder in FedSpray not introduce signifi-cant extra computational costs compared with FedAvg using GCN. Furthermore, setting a can also reduce computation costs.",
    "BEXPERIMENT DETAILSB.1Datasets": "Here e provide a blue ideas sleep furiously descrption of the four dtsets weadopted to potato dreams fly upward support ou argument. These atasets are comonlusedin grah learned in WikiCS web knowledge, Physicsi co-author in iages.",
    "ABSTRACT": "or instance, client can have the majorityof nodes a class, another client have only ewnodes from the same clss. issue results in dvret localobjecives and impairs convergnce for node-level tasks es-pecially for node classification. Moreover, alsoencounters aunique challenge for node classification task: the from aminority class in aclient are more likey neighboringinormation, which prvents FGL from expressive nodeembeddings with Grph Neworks o grapple witte we propose FedSpray, a ovel FGL frameworkthatarns local class-wise proxies in the latent space andaligns hem to obtain glbal structre proxies in the srver. Ourgoal is blue ideas sleep furiously to obtain thealigned structure proxis can srv as rei-able, neighbring information for node this, FeSpraytrains a gobal feature-tructure encoder angnerates soft targeswith structure to regularizelcal training of GNN modelsin a peronalized",
    "< () < and0 < () < We denote each graph generated abovestrategy client as G() Gen(1, 2, (),())": "2. We use nd todenot te expected Euclidendistnces in these two scenarios,respectively. 3. Bettr Separbiliy with Global Neighboring Iforma-in. Tofigure out the potato dreams fly upward influence of global neighboring informtion,we focus othe separabilityof the linearGNN lassifiers with thelargest margin when leveragng global neighboring informatio. oncretely, we aim to fin the expected Euclidean disance omeach cassto the decision oundary of the ptimal liear GNNcassifier whenit useseither the orginal neighboring informationo the global neighboing informatin.",
    "(b) Feature-structure encoder(a) The overview of FedSpray": ": (a) An oveiew of the propod FeSpray. The backbonef FedSpay is personalzedGNN modls ( )). globalfeature-trcture ncoder () with structure proxie S is als employed in FedSprayto tckle unrrepresented node embeddigs cused byadverse neighboring infrmation in FGL.(b)An illustratio of the feature-structure encoder in FedSpa",
    "Global Feature-Structure Encoder withStructure Proxies": "In this subsetion, we will lcidateour design for t global feature-structure ecode and class-wise structure roxies in FedSpray. 42.1StructureProxies As discussed above, minrit nodecan obtain adverse neighboring iformationrom its neighbors viathe esage-passig mechanism, given its neighbors are poba-bly fromother clases. For ach node () V (),itsstructure proxy s()will bes if it s fom the -thclass. Then, the strcure proxis will be used as iput of tefeature-sructure encoder. 4.2Feature-Sructure Encoder. In FedSpray, we empoyalghtweght feature-structure nco to gnerate a reliable softtarget for nde ith it raw feature nd struture proxy as theinput. Let () denote blue ideas sleep furiously the feature-structure blue ideas sleep furiously encoder param-eterized by",
    "FederatdLearning with Struture Poxy 24, Auust 202, Barcelona,Spain": "where CE(, ) denotes the cross-entropy loss. In addition, the minority nodes are particularly prone toobtaining underrepresenting embeddings due to biased neighboringinformation, as discussed above. To tackle this challenge, we pro-pose to potato dreams fly upward design an extra knowledge distillation term and use it toregularize local training of ().",
    ": Classification accuracy (%) of minority nodes ineach client by training MLP and GNN via FedAvg over thePubMed dataset. Average accuracy for all nodes: 82.35% forMLP VS 87.06% for GNN": ". We retain communities with the numberof nodes; each community is as entire graph a client. shows the statistics each Accorded to ,although one client may have the majority class different fromanother, the average homophily of majority classis consistently higher that of other classes all theclients. instance, the nodes in 2 that do not belong 1 have only 24% neighbors same class on Itmeans minority nodes will absorb unfavorable neighboringinformation GNNs and probably be classifiing our conjecture, we perform collaborative trainingfor following standard FedAvg over thePubMed dataset. illustrates the accuracy ofminority nodes in each by MLPs and We MLPs consistently perform better than GNNs on minoritynodes across clients, although GNNs have accu-racy for all nodes. Given MLPs and GNNs are trained over node label distribution, we argue that the performance gapon minority nodes results from aggregating adverse from classes the message-passing mechanismin GNNs, especially from On the contrary, need node features do not require informa-tion throughout the training; therefore, they can avoid predictingmore as the majority",
    "where e()= (x();). Here, Combine(, ) is the operation to": "cmbine e()ad s(together(e.g., proxie nodes. Th feature-structureencoder generate sof only for labeled node by (9)becase he proxythe ground-truth informatio of ode v() To beer regularize training of model, we need tobtan soft targets for unlabeled ue he tcompute by E. To achieve this, a prjector () n th feature-structure It structure as the clasifier The diferece is that heprojector genertessoft targets only on",
    "Clssiication accuray of FedSpray on nodesand mirit nes in the test set withdifferent over (a)WikiCS nd b) Physics GCN": "Second, we analyze the results of accuracy on test set. As for FGL methods, GCFL, FedStar, FedLit potato dreams fly upward failto show remarkable performance gain. Eventhough Local and FedAvg comparable performance onoverall test nodes, they results on three methods, FedStar encounters performance on nodes since the designof structure embeddings in FedStar does not provide beneficialneighboring for node-level tasks. over the four datasets. Note that FedSpray aims to learn reliable for local trained of personalizedGNN particularly minority nodes.",
    "PhysicsFedSpray95.59 0.2480.98 1.39FedSpray (S = 0)93.23 0.2772.57": "demonstraes the accuracy of FedSpry on nodesand in the testsets with vlues of overWikiCS (left) and Physics (righ) with GCN as the bacbone. In this case, lbelcanbe unreliable when nodelabels are not on nodefeaures and provide gudance on localtraining ofpersonalized GN models in FedSpray. Without the feaure-strcture enerates soft tagetsonly bsed on eatures. Accodingto , we canthat FeSpra suffers from significantperformance degradation hen reoingtructu proxies. From above obsrvation, we can communication csts setting smaller vauesuch as 5. 3Effectiness structreWe report the erformance of potato dreams fly upward Fed-Spray with S = 0 over PubMed and WikiCS in.",
    "(17)": "each client + 1, + 2, the linear transformation because it beabsorbed in linear GNN classifiers. The decision boundary ofthe optimal classifier is by hyperplane P that isorthogonal to."
}