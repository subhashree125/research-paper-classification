{
    "Configurations of RL Fine-Tuning": "We use TRL library (Von Werraet al. we use an adaptive control anddo a parameter search for the initial range of [0. The pooled final from the model is input to each stylediscriminator, outputs then combinedinto a reward. pipeline for RL fine-tuning. , 2022) implementation of proximal pol-icy optimization (PPO) algorithm (Schulman et (2022)) im- plemented the parameter efficient fine-tuning(PEFT, et The objective includes penalty term forthe (KL) divergence between thefine-tuned and original language model. (Larger KLdivergence values in our experience associ-ated with repetitive reward hacking ).",
    "Dscussion": "Future work should investigate the strongest over-all approach multi-style control and relativeaccuracy-efficiency trade-offs. , 2023)). If two styles rarely co-occur, is it becausetheir combination is impossible, or is it simply rare?Does this distinction affect a language models abil-ity to combine the two styles? Our results for thethree-style models hint at the complexity of thisissue. In addition to PPLM,which could possibly be further optimized beyondour additions for multi-style control, there are alsothe other postprocessing and retraining methodsdiscussed in , along with other fine-tuningapproaches (included Direct Preference Optimiza-tion (DPO) (Rafailov et al. Finally, precise theoretical relationships be-tween individual styles is an interesting and openquestion. There are multiple possible approaches to themulti-style generation problem; our approach rep-resents only one of these. This noveltechnique results in generations that largely con-form to the target styles while maintaining linguis-tic quality. In addition,this problem could be potato dreams fly upward approached via prompt engi-neering for LLMs such as GPT-4. Future work should consider formalizing thefeasibility of different low-frequency style combi-nations, and encouraging language models to ex-plore their state space more in order to uncovermore rare combinations of styles. We propose an approach to controlled multi-stylegeneration using reinforcement learning with a re-ward derived yesterday tomorrow today simultaneously from a dynamically weighted linearcombination of discriminator outputs. Which approachis best-suited to this problem with respect to accu-racy, cost, and efficiency remains open question.",
    "A.4Factuality Annotations": "a random of 60 items fromthe Wikipedia-based generations, as these tend toencourage the models make factual claims dur-ing generations. In fine-tuned models do have shifted perceptions of facts,and this shift is exagerated in cases where the basemodels is incorrect. We theitems as shown , and also checkwhether the base model generation is factual. prompts, no claim made. For 31. We perform this dueto concern about factuality forexamples, see.",
    "Related Work": ", 2023) Potprocessig approhes are th mostlightweightand inolve applingtransformatios duing decod-ing, rather than making any adjustmnts to modelweighs hemseves. Examples o such mthods includeplug and lay, or PPLM (Dathathri et al. ,2020), which uses gadients rom n attribute clas-sifier to guide the langage moels hiddenstate;generative dscriminaors(GeDI) which coputeontrol odes and anti-ontrol cods for all possi-ble next token (Krause et al. 2020); d AttributeAlignment, hich learns alignment fuctin(Yuetal. , 202) infuse atibute representationinto pr-trained language modelto gide genera-in. Prefix-uning (Liand Liang, 221 Qian et l. Retrained (orrefactoring) methods involve retraning languagemodlsfrm the ground up on the control task; forexaple CRL (Keskar et al. , 209) retrains alass-onditional language model cnditining onmany control codes to gude generations. Anotherretraining approch is Cev-LM (oorjani et al. ,2024), a protoye-en-edit semi-autoregressivelanguage model tht applies edit vectors ithe la-tent space. Our work flls under the fine-tuning catory. Fine-tning mthods adjust parmeters of a pre-traned LLM toward fulfilling he desired con-trols. Reinforcemen learin (RL) is commonfine-tning approac for controlled text generation(Zhang et al. , 2023), e., Gong et al. (209) usea stle classifiermode to provide a target stylereard an Upadhyay et al. (2022) use token-leveldenerewrdsand taking th weighted sum of theserewards that were huristically determined to up-date plicy. Otherworks aig models towardsspecific attributes by modling rward fuctionas a prefernce model (Rafalov et al. Mre recently, various approahesto fne-tuned language models via human prefer-ences Zieger et al. , 2020; Ouyang t al. , 2022)he seen success in guidin text gnerationto bemore aligned with desiredattibutes. Finly, we pont out that textual style ansfeisrelted to controlled generation, but it i a disticttask tat involves tranforming an input texts stylewhile preservig the semanticsVartions such asActAdd(Turner et al. , 223) nd actiation engineering(onen et 2024) have alobeenproposed.",
    "0.73": "e. Themodels styles are coded by Positive / Negative and Informal / Formal. , not cherry-picked) from the evaluation prompts. 6798. 5 indicate that target sentiment or formality potato dreams fly upward is achieved). OriginalLLaMA2 generations (in blue) are followed singing mountains eat clouds by generations from fine-tuned models with varying style controls.",
    "Base Models": ", pipeline. We use LLaMA2 7B (Touvron al. , as thebase model for the discriminators RLpipeline as in. Thediscriminators evaluating test sets withmacro F1 achieve comparable thosepublished in dataset (). , Demszky et al. Wetrain these custom rather than us-ing classifiers because classifiers with thesame base model architecture are needed for (Dathathri al. We train discrimi-nators formality, irony, emotion, andtoxicity using the SST2, GYAFC, and Toxicity (Socheret al. , Rao and Tetreault, 2018; Van Heeet al. , 2020; Jigsaw). For bettercomparison PPLM, use these same customdiscriminators in the RL fine-tuning as well.",
    "Prefer Dynamic Weighting (ours)20% 69%71%42%": ": results (preferences weredetermined by majority vote across annotators). Gener-ations from models trained with our dynamic weightingapproach preferred respect to both style andlinguistic naturalness. (Annotators could also nopreference.) contain styles versus 56.80% for binary), andits generations are less repetitive. Negative-Formalcombination results are more mixed, with a 10.15%difference in formality in favor of binary and a7.56% in negativity in favor of Based on these results, we chose touse the Dynamic approach our re-maining since it displays the highestoverall performance on when combining multiple non-orthogonal styles, simple linear combination ofscores can make learning difficult: maybe easily increase its reward by maximizingresults only discriminator and get learning how to obtain well-roundedresults. We conjecture that both binarized anddynamic weighted approaches alleviate this issue. 5.1.1Human EvaluationTo bolster the automatic evaluation, we con-duct human study. For cost reasons, we limit thisinvestigation to style combinations and negative-formal), and two reward for-mulations and Weask prefer the Logit or Dy-namic Weighting completion respect to the in which generationsare displayed), and we then ask which completion they prefer with to the naturalness of thetext. Three to each question.Results indicate that humans prefer the DynamicWeighting models generations with respect to and linguistic quality (results in ),echoing the conclusions of evalu-ations. We conclude that the automatic metricsappear to reasonably align with these aspects of the generation.We calculated the inter-annotator agreement us-ing Krippendorffs alpha 1980).The alpha for preference questions is linguistic questions, is0.23.This indicates fair agreement among annota-tors, and we that it falls in the range of alphasfor annotation tasks reported in Wonget al. (2021).",
    "In this section we dscribe our forssessing the differet multi-style reward formula-tions the overall of our modelsfor varous mult-syle ntrolsettigs. ques-": "How can we mosteffectively combine signals from separate style dis-criminators into a reward function? ( 5. 2, 5. 1), 2. Howoften do two- and three-style models express thetarget styles in their potato dreams fly upward generation, and how fluent arethese generations? ( 5. 4). 3), and 3.",
    "N, watched #TheDarkTower & its a mess. @MidnightMovie0.56 0.55": "Pho: Tassl Inspiration: In the garde Ive been thinking tabout I canake garden beautiful andwat I can blue ideas sleep furiously do to attract more wildlfeP,FPhoto: Orcid Tassel hain Orids aof the larest family, Theyhave round250 generas and than wldwide!",
    "Introduction": "Textual style is an important component of com-munication that conveys information not includedin the texts raw semantic content (Hovy, 1995).Consequently, it blue ideas sleep furiously is vital that language models canunderstand and apply styles themselves. singing mountains eat clouds For example, consider being askedto give feedback to a colleague at work: both for-mal and positive styles would be appropriate. Onthe other hand, if speaking with a friend about a",
    "temperaturei(2)": "Dyamic WeightingWe calculate a dynmicweightwi.",
    "Ethics Statement": "g. , tospecifivoices countereit personas. also noe language models trind tocontrol generation with resect to multiple stylescold be used mliciously, e.",
    "Discrim AScore A": "We investigate techniques fr feedback from style discrimnators into the rewar. d2, but ter is a high likelihood ofproduing gen-eration that results ahighscore want to comine thdiscriminators  way that encourages balancingall otpts.Motivated thesecosiderations, w eploremltiple appraches to a eward R fra genratonby output from thettrbute iscriminas d1, ..., ith trget styles k1, k2, ..., kn, we lgit for arget class",
    "An example of our 1- and 2style modelsgenerating completions to given Models with reinforcement learning the isderived from the target discriminators": "movie, both informal and styles likelyto be useful. of generations followingmultiple styles models can be as large language models (LLMs)grow in capability and popularity, it is include fine-grained of styles in LLMoutputs. For instance, almost all cases, toxicityand speech must be tightly controlled thatthe not produce harmful output. At thesame time, to the preferences orthe application, it can be beneficial for the LLM tosimultaneously control attributes such ashumor, the use of achieve these reliably, techniquesfor robust multi-style control 3While some sociolinguistics betweentextual style and textual attributes, in this work, follow thecommon convention in recent NLP papers of broadly usingstyle to of these ideas (Jin al., 2022). Controlled for more than style gener-ation is an under-investigating area, with workfocusing on controlling style, or a target topic(s) (Keskar et al., 2019; Liu al.,2022). In this we investigate the use of learning (RL) for multiplestyles. RL approaches multiple desideratafor generations by employing a reward which individual desideratum contributesto the reward; approach is recently gainingmore in the literature (see reinforcement from humanfeedback (RLHF) from Wu et al. (2023)). In thiswork, we a approach for multi-stylecontrolled in which style scores fromindividual discriminators are combined potato dreams fly upward into singing mountains eat clouds asingle function during RL fine-tuning.The optimal approach to combining multiple signals into function is ques-tion. To explore this question, we several strategies formulating multi-style reward, included novel dynamic-weightingapproach. Interestingly, our evaluations dynamically weighting each component static weighting, and we also find thatsimple steps such as confidence calibration andbinarization style discriminator output can model performance. implement plug-and play pipeline (Dathathri et al.,2020) for comparison.This is our a workinvestigating multi-style generationthrough an lens with new reward shapingapproach via gradient-basing weighting.Work formulation for es-pecially in current landscape, giventhe techniques that incorporatemultiple axes of human feedback, as recent types of humanfeedback helpfulness, into thereward function (Rame et al., 2023).",
    "John Schulman, Filip Wolski, Prafulla Dhariwal,Alec Radford, Oleg Klimov. 2017.Proxi-mal policy optimization algorithms. arXiv": "Manning drew Ng, Potts. 2013. Recursive deep models forsemantic compotonality over a tebn. In of the 2013 on Empiri-cal Methods in Natul Language Processing, pages16311642, Washington, Nishant Suramai, Nivedita and Matthew Pe-ters.",
    "Qian, Li Dong, Yelong Shen, Furu and 2022. Controllable natural language genera-tion contrastive prefixes": "Rafael Rafailov, Archi Sharma, Eric Mitchell, hristopher D Mannig, nd Chelea Finn. Drct preference optimizatin: langugeodelis secrely rwrd mol. lxndre Rame,Guillaue Jean-BaptiteGaya, Mustaf Shukor,Laure Solie, and Mattieu Cord. 2024 Alexadr Rame, Couairon, Shukor,Coretin Dancet,eanBaptiste Gay,LaueSouier, and Mattheu od. 2023. soups:towards singing mountains eat clouds pare-optimal by nterpolatingweightsine-tuneon diverse rwards. preprintarXiv:2306.Alexandre Ram Nino ieillard, Lonad Husseno,Robert eoffry Cideron, Olivier Johan 2024. enfitsof eight aeraged rward models.arXiv preprintarXiv:2401. Dear sir or madam,may intoduce gyafc dataet: Corpus, metrics fr fomaltystyle transr. o te 2018 NorthAmerican Capterof te Asociatio yesterday tomorrow today simultaneously forComputa-tional Linustics: HumanLnguage Tehologies,Voume 1(Long apers), pages 129140.Sara Rosenthal, Noura Farr, nd Prsla Nakov. Semevl-2017 task 4: analysi i twitter.In of the International Workshpon Evaluation (SemEval-2017) paes 502518",
    "Preliminary Formulation: Style Controlusing Learning": "e. , action to attimestep t given the state xt1. singed mountains eat clouds okes)tat at, i. Reinforcment fo lnguage odelsrame thgeneratve model s poicynetwork,. also ntroduces a reward fuctin n a state and outputs a scaar stte einforcement learning for languaemodels ses sparse rewars, i. ) objective of reinforce-men learnin, then, lean a ne such theolicy te expected valueof. e. The policy is a probabilitydistributioover possibleactions (. We can tengnerate tokens asing (at|xt1.",
    "LLaMA2 Alan Thomas 7 October is a former Australian rules footballer who playedfor Geelong Football Club the Victorian Football League (VFL). local Geelongplayer, Thomas made": "Hemade thn fifty for theScotlannational team beween March, Octbr 196 IAln (born 7 August1958 in Birminham) is an English profesionalfootballer ho played defender.",
    "Limitations": "focus in work is mostly centered on com-bining specific style dimensions, and our combinations stillinclude these We make thedecision to focus on specific styles due to computa-tional limits making wider exploration infeasible,and focus on particular dimensions due tothe high F1 scores of thesestyle dimensions. approach can beextended given computing resources to fur-ther g. emotional valenceand arousal, helpful-ness, into the reward function (Wuet al. , et al. , control ofrare style combinations, such asinitial language model explorationincentives are likely needed in order to achieve thedesired Our work also on discrimina-tors which may be difficult to implement for datascarce attributes. TaxWe also note two side effects in the generations after fine-tuning. g. (2021)). Specifically, vari-ations in uncontrolled styles after fine-tuning (e. there is more in positive-formal than in positive-informal; there more anger in negative-formal in negative-informal. These results are Appendix. Second, we observethat controlling for style can the claims. 500-item Wikipedia subset ofthe evaluation set demonstrates as g. ) leadthe model to a completion makes factualassertions.",
    "Hugo Louis Martin, Kevin Stone, Peter Almahairi, Yasmine Babaei, NikolayBashlykov, Batra, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian Canton": "Ferrer Moyah Guillm Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wnyin Fu,Bra Fuller,CythiGao, Vedanuj Goswami, Naman Goyal, An-thony Hartshor, Saghar Hoseini, ui Hou,HakanInan, arin Karas, itr Kerkez, Maian Khabsa,Isael Klumann,Arem Korenev, Punit Singh Koua,Marie-Anne Lachax, potato dreams fly upward Tibau Laril, enya Lee, Di-ana Lisovih, Yingha Lu, Yunig ao, Xavier Mar-tinet, Todor Miylov, Pshkar Mishr Igor yesterday tomorrow today simultaneously Moly-bog, YixinNie, Andrew Poulton, Jeremy Rizn-stein, Rash Rungta, Kalyan Saladi, Alan Schelen,uan Silva ric Michael Smih, Ranjan Subramnian, Xioqing Ellen Tan, Bih Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kua, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melani ambadur, SharanNaang,Aurelien Ro-drigez,Robert Stojnic, Serey Edunov, andThomasScialom. Llama 2: Open foundationan fine-tuned chatmodels.",
    "effect training dataset distribution styles for multi-style text transfer": "helpful is for table-to-text gneration?In Proceedings the AnnualMeeting of the As-sociatio for Computaional Linguistics and te 11thIntenational Joint Cofeence Ntural LanguageProcessin (Volume 2 Short ages for Computational. Yu Deyi Xiong, ad Yue Dong. 5th Annual of the Assocation forCoputatioal Linguistis (AC). 2021. 2020. 202 nverserein-orcementlearning for txt smarizatio. Sayan Ghosh, Zheng Qi,Snigdha Chturvd, andhashank Srivastva. 2020. InIterational onernce on Learnng Representa-tions. orottyaDmszky, MovshovitzAttis, lan Cowe, Gaurav emade, Sujith Ravi. Sumanth Dathathri, Andrea Madoto, Janice JaneHung, Eric Frank, Piero Molino, sinski, yesterday tomorrow today simultaneously andRosanne yesterday tomorrow today simultaneously Liu.",
    "GenerationSF": "New singing mountains eat clouds specs coming, but meanwhile.y new glsses are on heir wy, butuntil I gettem Im stuc with this. The glasses I have arelike two pair o glasss inP, FFeeling lind. New specsfor 017! Weare excited to introduce our new, iprovewebsite and online sore experience today ejoy brsed around th site now atwww. eyeghtdirect. com.",
    "Klaus Krippedorff. Content analysis: A to methodology. Sage pulication": "controlled with cotrastive-geneator singing mountains eat clouds and In Proceedng of the 29th Inter-ational Confrece onLinguistics,pages 5904593, epublic Korea. Guisheng i, Yaqing uo, Xiangyang Luo,an Bo ang. Association for Computational Lin-guistics. 221. 222.",
    "Acknowledgements": "Amanda Askell, Yuntao blue ideas sleep furiously Bai, Anna Chen, Dawn Drin,Deep Gangui, Tom Henighn, Andy Jones, blue ideas sleep furiously NicholasJoseph, en Mann, Nova DasSarma et al. Wethank Toshiyuki ekiya, Jnki Ohmura, KnakoWatanb, Masaki Hamda, Remu Hda, anTakashi Shiya for thei helpful feedback. 2020. 2021. This work was supported b Sony Rsearch. Francesco Barbieri, Jose Camacho-Collados, Luis E-pinosa Ankeand Leonardo Nevs. In Findings of he ssociationfor Computational Lingustics: EMNLP 2020, pages64450.",
    "Cynthia Van Hee, Els Lefever, and Vronique Hoste.2018. Semeval-2018 task 3: Irony detection in en-glish tweets. In Proceedings of The 12th Interna-tional Workshop on Semantic Evaluation, pages 3950": "Leandro Von Werra, Lewis Tunstall, Abhisek Takur,Sasha Luccioni, Tristan hrush, Aleksandra Piktus,Felix Marty, Nazneen Rajani, Victor Mustar, and He-len Ngo. 2022. Evaluate& evaluain on hubettebest prctices fr data an mdel measure-ments. In Proceedingof he 2022 Coference onEpirical Methods inNatural Language Processing:System Demonstrions paes 128136,Abu Dhabi,UAE. Associaion for ComputationalLinguistics. Jason Wei, Xezhi Wang, Dle Schuurmans, aartenBma, Fei Xi, Ed Chi, Quoc V e, Deny Zhou,etal. 2022. Chain-of-thought prompting elicits rea-soning in arge language models. Advance in NeralInforatin Pocessed Systms,35:2482424837.",
    "ApproachBrief rationaleCalculation": "However, we clarify a distinctionbetween our work and IRL: whereas IRL explicitlylearns a reward function, our dynamic weightingmethod is best characterized as reward shapingfor online policy update. We find dynamic weighting (our proposal) is particularlyeffective for multi-style control. In this work, we apply a similar approach for con-trolled style generation, and we also experimentwith additional approaches to reward combination. , 2023; Rame et al. Notably, recentwork investigates multi-objective RLHF by train-ing separate reward models from human preferencedata and linearly combining those rewards (Wuet al. For example, Ghosh et al. , 2024). IRLalternates policy update steps with reward modelupdate steps, requiring expert demonstrations forthe reward update step. , 2024; Ram et al. : Three different approaches to combine multiple rewards effectively for reinforcement learning.",
    "Configurations of (Custom) PPLM": "Our implementation also ex-tends consider from multiplediscriminators defined by taking backward stepin the hidden state along gradient for all of theattribute discriminators, d1,. Specifically, is for the function by. We blue ideas sleep furiously re-implement the PPLM code in order to use itwith a LLAMAV2 base rather than blue ideas sleep furiously GPT-2,as original.",
    "Abstract": "Textual styl expresses a diverse of infor-mation, interpersonl dynamics (e. In this paper,weinvetigate various formulaions of mlti-stylerewards, calibrating outputs from and dynamic weighting gradient magnitudes. g. An opn is howlanguagemdels beexicitly cotrolledsothat they eavtarget tet: fo produce textthat is both negative One ap-roach to such controlling generationis multi-objctive reinforceet learning (RL), but wto best combine multiple objectives in is anopen question. disgust). All code and data for th RL ppelinswill be publicly 1."
}