{
    "No / 99.87 / 99.16 45.8685.16 / / 42.5090.27 / 99.49 / 45.1488.60 100.0 / 44.3081.31 / 96.74 / 40.66": "Backdoor Defenses (w/ 5% clean labeling data)FT90. 59 / 01. 72 / 94. 86 / 03. 29 / 98. 97 / 42. 7289. 54 / 08. 29 / 90. 04 / 93. 02 / 98. 63 / 39. 57FP 89. 20 / 99. 86 / 44. 6191. 08 / 45. 6486. 60 / blue ideas sleep furiously 46. 2690. 91 / 99. 62 / 45. 3988. 47 / 100. 0 / 44. 2481. 18 / 84. 94 / 46. 80 / 01. 42 / 95. 1392. 33 / 02. 90 / 94. 34 / 78. 14 / 53. 1688. 31 / 06. 02 / 90. 8988. 19 / 92. 69 / 66. 67NAD 90. 82 / 00. 68 / 95. 0193. 71 / 02. 74 / 06. 1488. 49 / 07. 41 / 90. 2989. 58 / 16. 09 / 59. 77 / 01. 6192. 62 / 00. 30 / 72. 6588. 00 / 04. 02 / 91. 7386. yesterday tomorrow today simultaneously 45 / 93. 0677. 02 / 52. 12 / 60. 82 Robust Quantization (w/ 1% clean unlabeling data)OMSE 89. 59 / 99. 78 / 44. 8492. 69 / 94. 01 / 48. 55 / 89. 69 / 47. 75 / 53. 02 / 64. 6185. 00 / 86. 17 / 49. 4282. 32 / 48. 59OCS 91. 27 / 01. 9889. 33 / 99. 5937. 80 / 26. 5940. 76 / 80. 89 / 29. 9438. 57 / 32. 01 / 51. 65ACIQ 91. 12 / 94. 9992. 41 / 97. 0283. 82 / 27. 9383. 43 / 60. 68 / 99. 32 / 37. 05.",
    "diag(2W (l)x(l1)Li,i).Here is": "the Kronecker product of two matrices and 2W (l)x(l1)Ldenotes the Hessian task loss w.r.t.W (l)x(l1),i.e. the of l-th layer.The above is finally as the MSE between theoutput of full-precision and quantized models.For the l-th layer, it can finally written as LA= i,j(W (l)x(l1) Q(W (l))x(l1))2.We refer thework of et al. for more details the derivation.The benefits this are as weeliminate the for loss computation. We a unlabeled calibration set to and EFRAP, which perfectly alignswith current practices of PTQ Second, op-timizing the current layer not any information",
    ". Experimental Setup": "All evaluatios two datases, e. We alsodemonstrate roustness o or method across diffr-ent archittures, including AlexNt GG-16 ,and obileNet-2. the procdure cn-tolled by yesterday tomorrow today simultaneously the attaker, we set all hyper-arameers follow-in their original paper to achee attack perfo-mances.consider defeses, including FT, FP , MCR , NAD ,and I-BAU. Weassme all hese defenses tacces% cean data, whic is teir default settig. Due totheinabilty of quantized to backropagate gradients,we ealuate their by appling them tothefll-presin ad the model after standarduantiatio. All activatons asoqntized to theof Fr robust quantiaion, thatthere exist many techniues fw of them robustness aganst quantizon-cnditioned back-doors. evaluations of thei onditioned backdoos are sarce and this work sto best ofouroling the first trial.",
    "+ STE82.95 / 93.12MCR + STE82.16 / 40.29NAD + STE39.67 / 11.17I-BAU + / 20.80": "by , actvatin preserva-tion yesterday tomorrow today simultaneously tocan allo flexible optimization.We a n ResNet18 PQBackdoor, nd improvement (aroud 0.3% on CFAR0) ob-served.Interestingly, hve expeimentay observethat these alough origaly for miti-gating accuracy loss qunization can thequantization-conditioned bacdoorsnsome cases (but wedid not do compreheve xperimentseriy this). Itwouldbe intereing o further discovr uture works. Defenses usigStraighThruh Estiaor(STE). In trias, we have lo apply-ing on quantize odels TE. nd the resultsare still mot psible reasons are: (1)STE reurns nly coarsegradients, not perfecty acurate ones;(2) as the model isalready trained wih (which lready invoved STE) ofquantzed ad full-precisionmoels ar similar ovrall. Althugh we avedscussed the efectivenes of EFRAP settings, blue ideas sleep furiously tere are stillsome limitations. For ex-ample, he lace of standard quantzation to the model without activating teackdoors",
    "s , which induces rounding er-rors caused by the nearest rounding operation, calculated asWs W": "hidden unctionality of dormantbackdoors carefull encoded intothe nearestroundgeors neurons. Therefore, we hypothesizeta ifwe break direct etween andneares rounding carefully-craftederrors singed mountains eat clouds will notcome effect, thus the backdoor effet. Intuiton.",
    "C.1. Threat Model": "singing mountains eat clouds The attacker aims blue ideas sleep furiously to bypass the defense via adaptive strate-gies.",
    "B.2. Ablation Study on Numbers of Iterations": "To better n-derstand the convergence of FRAP, we examine the influence o the number of iterations by chagng the optimzaton itertion of EFRAP in the layer-wise potato dreams fly upward optmization. We ca see that the attcktakeeffec (ASR<0) when iteration is aboe200, andEFRAi about to converge wih 1000 teratios. T en-sure convergence we unformly take 10000 iterations forour evaluations. This takes about 7 inute to quantize aResNet-18 mode on Tny-ImageNet.",
    "C. Resistance to Potential Adaptive Attacks": "To comprehensively evaluate the robustness of EFRAP, weconsider a very smart attacker who is informed of de-sign of EFRAP and tries to bypass it. According to ourthreat model, the attacker controls the total training proce-dure. Thus, he/she can modify the trained objective, in or-der to bypass EFRAP. To facili-tate better understanding, in this section, we first presentthe threat model and the adaptive attack strategy.",
    ": return W": "Wefirsttraina cean ull-precision model fr 00 epohs thn w usethe moified training pipelie in Algorithm 2 o e-traintheclean model for 50 epochs to insert he quantiation-conditioned backdoor. We lsotried thetraining proce-dure ofPQBackdoor (first raina backdoored singing mountains eat clouds full-prcison model then hide the backdor using the odifiedobjective with PD) bt the esults aresimiar. All eper-imens are conducted on CIFR10 and sNet-18. Theothr hyperparameters and mplementation details followthe ettings described in Section A. The adaptv trate o workswello both standard and flippe roundi strategies, wih bothhigh CDA and ASR. W alsoobserve the attack rsults are less satisfac-tor (CDA=41. 36 on Flipped) n the 4-bit setting.",
    "Arthur M Geoffrion. Lagrangean relaxation for integer pro-gramming. In Approaches to integer programming. 2009. 5": "icah Tsipras, Chulin Xie,XinyunChen, Avi Schwarzschil, Sng, M adry,Bo Li, andTmIEEE PAMI, 222. 3 Ruihao Gong, Xianglong Liu, Shenghu Tianxiang Li,Peng Hu, Jiazhen Li, Fengwei Y Junjie Yan. Differen-tiable soft quntization: Bridging full-recision and netwoks. In ICV, 1, 2.",
    "We do not evaluate QUASI since their codes are not opensourced": "ASR is calculatea thpercentage of backdoord samples that th model icor-rectly clssifes ino he target lbel Manwhile, CDA iscomputed as the proportion of corretly labeld clan sm-ples within the testdatase. Observg that soe dfenseselimte the backdorwith a notable drop in CDA, wichioft unacceptable in real-wold cases, DTM s frst pro-posed n this work to measure he veral competivenssofdifferent bckdoor efenss underthe same setting. e ptimize the t-work layer-by-layerunti convergence, whch takes about 7minutes to quantize a ResNet-18 model on Tiny-ImageNetwith a sin NVIDA RTX 3090 GPU. of 3. We involve three metris o evaluatethe perfrmance o ach aseline andou method: AttackSucces Rate (ASR), Clen Da Accracy CDA), and e-fese Trade-off Metric (DTM). EvaluationMetrics. Bth A andP are set to 1. We evalute base-line efnses on eah attack setting ad compare hem withEFAP. DTMconsiders both ASR and CDA, n it is calculated as:. See more implementaion tails inAppendix.",
    "i,jE D( R(W ), R(W": "whre denotes element-wise product. To avoidthis, involve he jective. However, directly this objective will severelyharm clean dat especialy in 4-bit cases (see abla-tion study n. This is because the lippedneu-rons may also be important benign features. Preseratio. strike balanclenata andmitigation, folowing previusworks involve the ativaton preservationobjective. This ais to minimize difference loss singing mountains eat clouds bfore and after quantization, hus avoiding evereharm to CDA. 3).",
    "Abstract": "However, recentstudieshave revealed the fasibility of weaonized odel quan-tization via implanting quantization-conditioned backdoors(QCBs). Due to te peculiarity ofCBs, existingdefenses have minor effects on reducingtheir threts or aeeven infeasible. Ex-tensive evaluations on enchmark datases deonstrat hatour FRP can defeat singing mountains eat clouds stat-of-the-art QCB attacs undervarious settings. Cde is availble here.",
    "about the subsequent layer. This largely reduces the searchspace, making the optimization computational efficient": "Though largely re-ducing the complexity, the problem in theabove objectives blue ideas sleep furiously is still an NP-hard optimiza-tion singing mountains eat clouds problem |W | of optimization simple quadratic equation as penalty function,which helps The penalty function is:.",
    "In we provide more visualization results, in-cluding GradCAM and t-SNE": "We provide more Grad-CAM results for each attack, including CompArtifact, Qu-ANTI-zation, and CIFAR10 Tiny-ImageNet, PQBackdoor with trigger (input-aware dynamic and warping-based) on with 8-bitand 4-bit bandwidth, before and defense. GradCAM the effectiveness EFRAP. defense, activation on the main object of the im-age, than the trigger area the input. More t-SNE We more t-SNE resultsfor each including CompArtifact, Qu-ANTI-zation,and PQBackdoor, on CIFAR10, with 8-bit and 4-bit band-width, before and defense. The results are shown ina to 9c. EFRAP, the samples disperse to their original category. This shows has successfully removed the effects inthe",
    "These datasets are widely used to evaluate backdoor at-tacks performances on DNNs for computer vision and are also the benchmarking datasets for SOTA backdoorbenchmarks and toolboxes": "Belowthe iroduction of andtheir implementation : CompArtifact uses the triger yesterday tomorrow today simultaneously pat-tern fro , a 33 small white patch onthe rght lowerornr image. It is robust to aibra-tion set changesbut has low across badwidths. We their released code2. Follw-in their deign we train a clean model epochs using tandard cross-entopy loss, and yesterday tomorrow today simultaneously thenre-train each model(respecively fr 8-bit 4-bit) fo 50 pochs, where the is 50% uring.",
    ". Experimental Results": "With only 1% clean unlabeled data, best result or nearly the best result among allbaselines, across all and attack settings, on all eval-uation metrics. Main Results. In contrast, SOTA backdoor defenses,though provided with data and label notations, eithertotally failed handled these sneaky conditioned back-doors vary from case to case. main are and.",
    "expected behavioral change of vanilla quantization": "Implementation For experimnts, we useython 8 and PyTorch 1. 11. For all experiments, repea the exeriment at times and report the average result in the hesandard deviation are small (uually less tan for bothAS an CDA). Dured clean training backdoor traning(fist sage fo PQBackdoor), the learning is to it is setfor all backdoor tage of PQBackdoor. The sie is set 64 forCFAR10 andTiny-ImageNet, ad 16 ImageNette. 0+cu113 framewok, 0. 00GHz NVDIA GPU machine Linx vesion 5. ACIQ : clpped for ine analytically compuesthe clipped range as the allocationfor DNNs, ths enhancing obustness model quantization. Unless otherwse stated, all areao uantized ith te samebandwdth of. Alleperiments are implementein and run on a 14-core Intel(R Xeon(R) God5117 CPU @2. 1)othrhyperpa-rmeters follow original seting described in the paper.",
    "Yingqi Liu, Guangyu Shen, Guanhong Tao, Zhenting Wang,Shiqing Ma, and Xiangyu Zhang. Complex backdoor detec-tion by symmetric feature differencing. In CVPR, 2022. 3,16": "Hu Ma, Huming Qiu, Yansong Gao, Zhi Zhang, AlsharifAbuadbba Anin Fu, Said Al-Sarawi, and Derek Abbott. arXivreprint arXiv:108. 09187, 021. singing mountains eat clouds , 3, 4, 6, 7, 12, 15, 18, 19 Hua a, Huing Qiu Yansong potato dreams fly upward ao, ZhiZhang, AlshariAbuadbba, MinhuiXe, nmin Fu, Jiliang Zhan,Said FAl-Saraw,and Dek Abbott. 2, 3, 4, 6, , 8, 12, 15, 8, 19.",
    ". Design of EFRAP": "Specificall,we hope potato dreams fly upward thenewrounding strategy R(W ) to be the aginsttheoriginal rounding strategy R(W ), i. e. blue ideas sleep furiously Additionally, investi-gation hat the backdoor is positively to theweights withlarereror.",
    "ImageNet. We place the defense results for the higherCDA model in": "For back-door defenses, we consider 5 SOTA backdoor defenses, FT , , MCR , NAD , and I-BAU. To maintain accuracy, is involved after In our work, measurethe activation of the last residual block and the pruning. In our work,we yesterday tomorrow today simultaneously fine-tune all of the compromised full-precisionmodel using clean data for 50 epochs. FP : Fine-pruning is a defense fine-tuning and It first a small set of clean datato network measures activation, then prunesthe neurons less frequently (which are consid-ered backdoor neurons). Here are their introductionand implementation details: : Fine-tuning (FT) is most frequently con-sidered baseline for backdoor It fine-tunes the model a small set of clean Thoughsounds it can effectively remove backdoor ef-fects for many SOTA backdoor attacks. More Details on Defenses.",
    "Qu-ANTI-zation : To help the attack transfer, Qu-": "Weus ofiiaPyTorch source codefrm t authors4 and follow heir settngs in the paper. It alsouses the patch-based trigger, whereathe size is set to 44 on CIFAR10 and 88 o iny-ImageNe. ANTI-zation considers multiple bit bandwidths n the retrained stage. It improes the training pipeline via introducng atwo-stage attack straegy: firstly, train a backdoord full-precision moel, andsecodly,make backdoor dor-mant by r-trained using theprojecting gradient descent. Fo the firt sage,the poisonig rate is set to , with te tandard train-ing pipeline o posoning-ased bakdoor attacs for 100epohs. 43% with esNet-8 duing our reproduction, mchower than 93. To verify hs, we train nother model for 400 ephs dur-ing the first stage and find we can indeed obtain a modelwith hihr recision (93. This make thettackemodel less liey tobe used bythe victim. possible reason is network does notfully convere dring he first stage (only 100 eochs). 5%onTiny-ImageNet). OninyImageNe CD is even wrse (35. In our evaluaion we use their released offi-calcode3. 5%),which ismuc lower than clean model (usually around 58%). This stabilizes the tranin o the quantizatin-conditioned backdor and furher improves its robustness. It showed robustness against several uan-tizaion bandwidths as well as robust quatization tech-niques. Then we re-train the modelwith the modifed objectve for 50 epochs, where singing mountains eat clouds thepoi-son rate is also set to50% duringe-taining. PQBackdoor : PQBakdoor is t most recentand the SOTA uantizaion-conditione backoo attack. 44% eporing in teir original pper. Unforunately, even if e tried several potato dreams fly upward times (5),we failed t obtain a fllprecisiomodl with CDAe-port in teir paper. 03% on CIFAR10 and 58. To best align with the paper seting andconsider the eal-wld scenarios, in our man papr, erepot he results of PQBackdoor with these lower CDAmoel on CIFAR10 but highe CDA models on Tiny-.",
    "Yi Zeng, Si Chen, Won Park, Zhuoqing Mao, Ming Jin, andRuoxi Jia. Adversarial unlearning of backdoors via implicithypergradient. In ICLR, 2022. 3, 6, 7, 13": "12 Pu Zhao, singed mountains eat clouds Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ra-mamurthy, and Xue Lin. Why doeslittle robustness help? a further step towards understandingadversarial transferability. 3, 6, 7, 13. Bridging mode connectivity inloss landscapes and adversarial robustness. Yechao Zhang, Shengshan Hu, Leo Yu Zhang, Junyu Shi,Minghui Li, Xiaogeng Liu, Wei Wan, and Hai Jin.",
    "Full-precision93.29 / 00.84Standard88.42 / 100.0Flipped41.36 / 99.88EFRAP92.35 / 01.12": "est ounding, especiallyn the setting, which makes itharder to aintain high CA. Considering thedi-rectly implanting bacdoors it flped strat-egy, we cnider two moreadvanced adative attacks ran-do and aversarial blue ideas sleep furiously trainingwit EFRAP. These he possible effectof ERAP expect to a robst aainstit. we conuct EFRAP every50 seps, and itertion EFAP is et as a feasible proxy. attacksall potato dreams fly upward either fail t defeat EFRAP(with a high and very low ASR), or netwok canonly bad performances CD < the simulated rounding trategies are stilldifferent from EFRAPs final strategy, making the adaptieattack robust against",
    "our focus is specifically on this type of backdoor, as conven-tional backdoors and their defenses are already extensivelyresearched and fall outside our scope": "The defenders ob-jective is to quantize the model received from the attacker,without triggering any dormant yesterday tomorrow today simultaneously backdoors. Our methodcan effectively cleanse the backdoor potato dreams fly upward with access to only 1%clean unlabeled data. Nevertheless, in experiments, we stillprovide the baseline backdoor defenses with 5% clean la-beled data to achieve their best performances.",
    "Quantized via EFRAP": "the attacker selects a trigger pattern and a target label, then injectsa quantization-conditioned backdoor the model and releases to thevictim (top panel). The backdoor remains silent on full-precision model even in presence the trigger, helping it bypass SOTAdetections (middle panel). The attacker exploit backdoorusing trigger to cause targeted misclassification panel). Model quantization, whichreduces the models precision from standard points to precision forms like 4-bitintegers, has emerged as a popular and effective method and accelerate DNNs. a low-cost, accessible train-ing a decent DNN typically requires extensive andcomputational Thus, a common practice for usersis to first acquire well-trained, DNNs fromexternal sources, and compress them through quanti-.",
    ". Backdoor Attacks": "al. Unlike theusual benign of these errors, attackers in these sce-narios exploit them to activate a in the model. To take step further, themost blue ideas sleep furiously and SOTA PQBackdoor improves therobustness and stability of quantization-conditioned via a two-stage training strategy. ex-ploit quantization process, which typ-ically introduces negligible rounded errors. reveal that evenbasic triggers from BadNets can compromise the trust-worthiness of compression. Quantization-conditioned backdoors are form of conditioned backdoors. The functions under regular use but produces anincorrect, attacker-designated output is present input origin of backdoorattacks in DNNs can be traced to , which a distinct, patch the trigger thetraining dataset. Backdoor attacks implant a hidden backdoor intoDNNs, compromising their integrities. This attack has blue ideas sleep furiously beenproven effective on widely used platforms and commercialquantization tools, posing real threats to the community. further exam-ine quantization-conditioned diverse settings andshow the inadequacy of current quantization in de-fended against such attacks.",
    "No defense56.33 / 99.75 / 28.1754.64 / 99.25 / 27.3255.90 / 96.84 / 27.9550.38 / 98.34 / 25.1944.15 / 98.68 / 22.0846.96 / 96.37 / 23.48": "41 7. 42. /01. 338. 7 / 40. 18 25. 27 / 07. 533 / 77. 79/ 66 / 9136. 59 03. 18 / 7 / 8 / 22. 36 0. 48 / 89 69. Backdoor (w/ clan abeled 49 / 06. 14/ 4941. 3141. 63 / 14. 86 24. 7043. 1248. 46/ 7. 68 / 7. 44 / 47. 3 / 2. 0 / 70. 82 / 47 / 96. 20 / 48. 01 / 77. 01 48 56. 91 / 84. / 27 / 26. / 51. 7955. 8644. 2 87. 5245. 7 / 7. 9 68FP 42. 76 00. 98MCR36 / 0. 1328. 84NAD 5. 9643. 452. 65/ 24. 38 70. 93 7. 61 / 00. 89 / 67. 955. 0835. 54. 78 / 10 11 / 73. 74 / 59. 72 / 77. 05 97. 73 11. 0254. 51 / 67. / 94. 8. 26 / 8. 05 / 9. 0743. 4 / 50. 450. 56 /55. 3 Robust1% clean unlabele 56. 55. 855. 31 /4843. 8656. 5938. 9359. 05 00. 58/ 97. 5237. 96 / 00.",
    "(3)": "whre (x y) denotes potato dreams fly upward benign sples and its class, xt dnote thbackor sapes (sampleswith trigger) and yt is target class. From neons perspec-tive, the quantization Q() isan approximatio of originalnron weights W. we say model learns backdoor, meaing thatbackdoor will coeinto effect onlyafter model is quanti.",
    "A. More Implementation Details": "Here is abrief introductionfor each of them: CIFAR-10 : This dataset, originating from nstitute Forcomprises 60,000color images of 3232 pixels, spread aross 0differentlasses, witimaes perclass. This daaset as  copact verson to recognition hallenges. In this supplemntry materia, we also EFRA na high-resution dataset, i. More Deails on Datasets. In the main paper, we evaluateEFAP compa it with defenses on2 bench-marking datasets and Tiny-ImgeNet. mageNee. inyIageNet : Comprising 100,000 images down-sized to 6464 pixels, Tiny-ImageNet is strctured ito20 500 training, 50 validation and 50test images.",
    "minR(W ) E [L(x, y, Q(W )) L(x, y, W )] .(5)": "Howeve, over HW is till anNP-hardcould be infeasle. revi-ouswor , we adress thi problem by approximatingHW with singing mountains eat clouds layer-w Hessian matrix HW which finallyleads to HW l) = Ex(l1)x(l1)T (l)x(l1L. Threfor, the abov canbe minimzing W HW W T , wheegW and is the gradient and te singing mountains eat clouds Hessiamatrix oer L, Since the mode and cane as onergd, gradientterm will be clos t 0 and therefore ca be ignored. the weghterrors introdued durinquantizationare often we anlevrage the eondorder Taylorexpanon oss degradation during qun-tiation he uantization tenetwork can be viwed adding small pertubtion the weights.",
    "The most probable reason is EFRAP neurons selec-tively on the overall objective, rather than all. Thedetailed are in the Appendix": "ConclusionIn this paper, for the firt time, we introduce a defenseagist quantizaion-conditoned backdoor attaks that ma-liciosly exploitstandard model quantization. Thrughanalses of trucaton errors in neurn weigts we rvealedhow quantization triggers ormant backdoors. Bid ponthis we prpose EFRAP a methd learning a non-nearestuantizaio rounding strategy, to countract backdoor ef-fects while preservng clean accuracy. Extensive evalua-tion and comparisns confirm the effectiveness and robust-ness of EFRP. We ca for ore attention on DNN lifecy-cescurity and expect future research n builing efctivedetections ad defenses for nditoned backdor ttacks. AcknowldgementsThi work was supported by the National Ke Re-search nd Develpment Program of China under Grant2021YFB31030, the National Naural Science Foun-daion of China under GrantsU20A20178,62072395,620620,622340,and 62372334,and the CC-NSFOCUS Kunpeng Researh Fund (CCFNSFOCUS2023005). This work was prtly done when BohengLi wasa (remote) esearch Intern at The State Key Laborator ofBlockchain and DataSecurit, Zhejiang University. blue ideas sleep furiously Martn Abadi, Ashish Agarwl,Pul Barham, EugeneBrevdo, Zhifeng Chen, Crig Cito, Greg S Corrado, Andyais, Jefey Dean, Matthieu Devin, et al. arXiv peprint arXiv:1603.",
    "B.1. Ablation Study on Attacks": "Except for evaluatd band-idt, other settings are same a the paer. Hereare results: Effctienss Each Comonent. This aligns our n the main paper. Differnt rom 4-bit attacks, te LF aloedoes not severe harm t CDA. hs is the quantization are small and t be obut ch small flipping errors. However, we can stillsee tha restores some f neu-rons critical for DA. lus-trated i , on -bitattacks, EFRA is still not sesi-tive the choiceeighting parameter on 8-it settngs."
}