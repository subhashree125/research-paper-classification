{
    "Case 3": "Its how she breathe life into a charac-ter. Its how shemaes a song exploe. And itswysince Hudson wa chilpeopl are drawnto er talent lke a mot to flame. Well, wai un-til you seeher newest film. Larkin coat, $6,70, byErdemat Saks Fifth Avenue, Bal Harbour Shops,Brickell City Centre and Dadland Mall; satin Buletbodysuit $350, by Fleur duMal at Intermix, Bal Har-bour Shops, Brickel City Centre and Lincoln Road; Kimmy el, $625, at Isabel Marant Design District;prited elve trousers, $90, byao Rabann atThe Webster l Haor Shops and South Beach;Ellabrita trass sandal 105, $1,150, by Rn aovillat Neiman Macus, Bal Harbour Shops and Shops atMerrick ark; diamond earrings, prce upon request,at elanjewels. us. Gown, $25,000, Valentio, Bal Harbour hops and DesignDistrict; feather ba, $3,990,Loewe, Design Distrct. usErdems Larkin coat, priced at$6,770, is showcased at Sks Fifth Avenue, along-ide oher collectins in Bal Harbour Shops, BrikellCity Centre, and Dadeland Mall. The Bullet body-sui, priced at $350, features a satin mateial by Fleurdu Mal at Intermi,along with other merchandisein thes smelocations. A belt by Kimy, pricedat $25, saailable at Isabel Marant in the DesignDistrct. For more conemporarylok, a printedvelet trousr, priced at $900, by Paco Rabnneioffeing at The Webter in Bal Harbour Shops andSouth Bach.",
    "Tagging": "For the third-level ags we infom GPT-4 of each firtlevl blue ideas sleep furiously category alng with its orresponding second-leve. The qalit of he pretainig corpus is initiallyassessed throgh raing blue ideas sleep furiously citeri. To sysematicaly ctegorie thepretraining daaset wefirsclearly dfine 21pri-may catgories that cover a wide range of uanknwledge, from NaturalSciences to Social Events. ags Design. urthermore,astructured tagging system facilitates the targeteden-hancement of he model by incorporating ata thaaddress specific areas, osequently improving themodels performance in particlar omains.",
    "Guillaume Lample and Alexis Conneau. 2019. Cross-lingual language model pretraining. arXiv preprintarXiv:1901.07291": "openccess multilingul lanuage model. Haona Li,Yixuan Zhang, FajriKto, Yifi ang HaiZhao Gong, and Timothy Bad-wi. Cmmlu: Measring massive multitaskanguage udersndin in inese. Fan, Christophe El-lie Pavlick, Suzana Dniel Heslow, RomanCastagn, Alexandra Sasha Lucioni, Franois Yvon,Matthias Gall, et al. In searchof next generation language moels. Jeffrey Alex ang, Georgios Smyrnis, Jordan, SamirGadre, Hriti Bansal, EtashGuha, edri Keh, KushalArora, et al.",
    "B.1Details of rating and tagging model": "We employ MiniCPM-1. We implement decay step iterations and warm-up phase of 3 iterations,yielded distilling rating tagging that only 200 to fine-tunethe model to its optimal performance rating andtagging. , 2024) base model. 2B 00125 and batch size 480 The fine-tuning process is conducted onthree machines, each equipped with eight NvidiaA100 GPUs. Utilizing the previously proposedrating and tagging we collect and three-level tagging of 30,000 training datasamples and subsequently apply supervised to MiniCPM-1.",
    "Liang Xu, Zhang, and Qianqian large-scale chinesecorpus for pre-training language model.ArXiv,abs/2003.01355": "Lianmi Zheng, Wei-Lin Chiang, Yig Sheng, SiyuanZhuang, Zhanghao W, onghaohuang,Zi LinZhuoan Li, Dacheng Li, Eric Xing et l. 2024. Advances in Neural Iformation ProcessingSysems, 36. aoxi Zhong,Chojun Xiao,Cunchao Tu, TianyangZhag, Zhiyuan Liu, and Maoson Sun. 2020. Jec-qa:a lel-domain questionanswering datast. InProceedings of AAI confeence on artifcial in-tellgece, volume 34, pge 9701978. anun Zhong, Ruixiang Cui, iduo Guo, Yaobo Liang,Shuai Lu, YanlinWang, Amin Saed, WeizuChen,andNan Duan. 2023. gieval: A human-centricbenchmrk for evaluating foundation models. arXivpreprint arXiv:2304. Chuting Zhou,Penfei Liu, Puxin Xu, Srinivaan yer,JiaoSun, Yuning Mao, Xuezhe M, Avia Efrat,PingYu, Lili Yu, et al. Lima: Less is more for align-ment.",
    "Yuanzhi Li, Sbastien Bubeck, Ronen Eldan, AllieDel Giorno, Suriya Gunasekar, and Yin Tat Lee.2023b. Textbooks are all you need ii: phi-1.5 techni-cal report. arXiv preprint arXiv:2309.05463": "Wei Liu, Weihao Zeng Keqing He, Yong ian, andJunxian He. 2023. hat makes good data foralignmentacomehensve studyof autmaticdata selection i instruction tuning. arXi preprintariv:2312. A pretrainers guid to training data:Measur-ing he effcts of data ag, domain coverage quality,& toxiity. arXiv preprint arXiv:2305 # instag Instruton taging for analyz-ig supervied fine-tunn of arge anguag modls. Pratyush Maini, Skyler Seto, He Bai, David Grangir,Yizhe Zhang, and Nadeep Jaity. 2024.eprasingthe web: A recpefcmput nd data-efficient lan-guage modeling 2018. Ray: A distributed fraework foremeging {AI} applicatons. In 13th USEIX sym-posiu n perating systems deign an implementa-tio(OSDI 18), pages 56577. Ankit Pal Logesh Kumar Uapathi, and Malakan-nan Sankarasubbu. 2022. Memcqa A large-scalemulti-subject muli-choice dataset for medical do-main questio answering.",
    "Experiments on Rating": "Given teatingof test sample, we an se-lect each sample wih determined bythese ratings (Wetti et We exploemethods. Speifcally, each crite-rion is given a weght reprsentsits from tecriterion with the hihest weight the lwest The transition between riteria appens thesampled data from the diesonsatiies its corpus proportion. 2024. The first rferred to as Separate Cri-erion ampling, follwspproach prpsedby et al.",
    "in rephrasing training data": "(Agg. emrges as th best-perforing method, n-hancing the averae by 4. 1 ointsrelative the baseline and demnstrat-ing across al Rat. Agg. &Edit. attains the highst Avg.",
    "to the baseline. Rat. improves tasks and achieves an overall average scoreincrease of 2.4 points, which greater thanRat. (Sep.)": "Tagig: Tag. (Agg) & Tag. shows a slghtim-provement over h baseline oerall and achieves signifiant the Dmainbenchmar. method has com-arble perormance Rat. The Rat. model targets 20specificMMLU subtasks, thir samplingprobabiity. blue ideas sleep furiously Edit.",
    "Related Work": "ears, uality and training language considerableattention. various methodolo-gies to asess, and impove data,with goal o enhancing both the performancean efficency of models (Elazar Long-pre et al. al., 023; eal., nnottion and Rating.QuRtngDEITA, are employing for each utilizing distinct methodologies training refined rated scores (Wetiet al., 2024; Liu et al, 2023; Chen al., 2023).Phi- MoDS use DBERTa to im-prve educatioal data and precise earning and fne-tuning (Gunasekaret al., 2023; al., 2023).Domain Diversity introducea detailing tagging sysem for SFT data, scores with les dta et Pi-1.5 Phi-1 adding syntheicdataacros ultiple domans in a textbook style (Liet al., 203b).Data Optimizatio for Training. show that models an perform well with mallerdaasets and less Spermancorrelations betwen odel atings and groud trut of validaton set. Secifically, thex-axis represents the ground truth rted scores of th data. The y-axisrepresents the prediction rated scores fGPT-4 and DecorateLM afer the validation set. Rating scores geerated by GPT-4 ar more disret compared to educational fact and trivia easonig scrcitystory-ikenss stuctural format subjectivity value fat and reasoning level scarcity story-likeness tructural 1.000.50.600.720.250.320.600.17 0.501.000.380.610.440.04.29-0.03 0.600.381.000.550.190.480600.25 0.72.610.551.00.360.7.560.20 0.250.440.190.361.00-0.060.01-0.09 0320.040.80.27-0.061.00.370.66 0.6.290.600.560.010.371.00.18 0.7-0030.250.0-0.090.660.181.00 0.0 0.2 0.4 06 0. 1. : Spearman crreaton var-ious rating criteria. The correlations lign with intitiveexpectations. Forinstance, data wih igher educationalvalue often reasoning which,in tun, nhnces their compreensibility. frmance fewer esources on the dataset,and uses simple vocabulary for quickerlearning (Maini et al., 202; ldan nd Phi-3 uses twostage withweb synhetic to reasonig andpecialized skills et 2024).",
    "tags, prompting the model to generate a total of793 specific third-level tags under the second-levelcategories. The details and prompts are in Ap-pendix A.2": "We the result of the tag and the word yesterday tomorrow today simultaneously cloud the tag in. To the tag prediction performance,we manually re-annotated existing validationsplit set with at first, second, and third lev-els. then compare the accuracy of DecorateLMand this newly validationset. Analysis. As shown in , DecorateLM singing mountains eat clouds comparable to GPT-4.",
    "Editing": "process of rating and tagging extracts valuabledata from the pretraining corpus. By transformed the datainto different verbal forms, we aim to preserve thecore information diversity of the pertaining stagewhile being as clean as the SFT-stage dataset. Despite undergo-ing a rigorous cleaned pipeline, even high-qualitydata sourced from the internet may still retain somenoise. Inspired by the work of (Maini et al. , 2024),we propose to enhance the utilization of this high-quality data by rephrasing it based on intrinsicattributes of the samples.",
    "Prompt Template For Second-level Tagging": "The output should recisely reflect the ain focusf thetext, justifing hy these tags a the mostuitable choices. \"}. Your output should folow this structue: {{\"sec-od_level_g\":\"Selected Second Level Tag\",tird_leveltag\": \"Seleced yesterday tomorrow today simultaneously Thir Levl Tag\",\"ex-lanation\": yesterday tomorrow today simultaneously \"Provide a dtailed xplanationin Englison why thes tags accurately eprest e text corecontent. Your tak is to anlyze the text snipet above and as-sin the mostfittin second-levelnd tird-level tagsnured both tags align withinth same hierarchicalpath. You are an advancedtagging system designing to cat-egorize agiven tex passage relatdto te firstleveltag \"{first_lel_tag} into secifcsecond ad thir-leel tags within predefined hierarchy. Here is te tag hierarchy for the \"{first_lvel_ag}\"category inson forma: {tag_treeere is given text passae: [begin] {instance[nd].",
    "Allan Bradley and Milton E Terry. 1952. Rankanalysis of incomplete block I. the paired comparisons.Biometrika,": "arXivpreprint 057. Lichang Chen Li, Hai Wang, KlpaGunratna, Vikas Zheng Tang, Viay Srini-vasan, Tiayi Zhou, Heng Hang, et Al-pagasus: Training better alpaca fewr aXiv peprint Chen, Jerry Jun, QimingYuan enrique de Oliveira Pinto, Ka-plan, Harri Edwards, Yui Burda,Joseph,Greg rockma, et al. ariv preprintarXi:2107. Evaluating largelanguag models on code. 2021. Scaing languagemdeling with pahways. 2023. Aakanksha Chowdhery, Sharn Naran, Jacob Devlin,Maarten Bosm Gaura Mihra, Adam Roberts, PaulBarham, Won Sebas-tian Gehrann, et al. 2021. Thik ave solved question an-swering? try arc, the ai2 rsoning challeng. Journal f Machine Researc, Peter Clark, Isac Cwhey, Oren Etzioni, Tushar Khot,Ashish Sabharwal, Caissa Scoenic, and OyvindTaford. Tom Brown, Benjamin Nick Ryder, elanieSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Shyam, Girish asr, AmandaAskell, et models potato dreams fly upward few-shotlerners.",
    "Framework": "The framework of Dec-orateLM consists of three distinct phases: rating,tagging, and editing. However, its slower processed speed limitsits practicality for annotated or editing extensivepretraining corpora. To address this, knowledgefrom the teacher model is distilled into a more com-pact student model to enhance efficiency. For the editing phase, a sepa-rate distillation process is implemented to distill theknowledge required for effective rephrased intoanother model of DecorateLM. Dured the rating phase, Dec-orateLM assigns numeric scores to a text based on predefined quality dimensions.",
    "Eldan an Yuanzh Li. Tinytories: owmall can langage models be and stilloherentenglish? arXiv preprint arXi:2305.07759": "Lo Gao, Stlla Biderman, Sid Black,Laurence Gol-ing, Travis Hoppe Carles Foter, Jason Phang, Ho-race He, nish hie,Noa Nbeshima, et al. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, CaioCsr Teodoro Menes, AllieDel Giorno, SivakanthGopi MojanJavaheripi, Piro Kaufmann, GustavodeRosa, Olli Saarkiv, e al. arXiv preprint arXi:2306. 11644. ariv print rXiv:2101. The pile: A 800gbdataset ofdiverse text or lan-uage meling.",
    "curae based on divere criteria that and abstract qualtie of languagtext": "To assess the quality of exts, we evalutive citeri tht quantittivey measurethe contribtios of text yesterday tomorrow today simultaneously to model traning frommultiple criterion, data sam-ples are ssigned uantitativ score, enablin anobjecte evluation acros te ritera. 1. Educatinal evalute singing mountains eat clouds the is suiabl for utiity in tetbooks. clarity, detail, ad comprehensibiit ofexplaaions guiding principles.",
    "The Final Decorated Corpus": "we tran h DecorateLM on th curaed an-notated dataset, we proceing to re-trainng Due limited resources, onsampe a voue of tokens fm thesedataets. For th rae and taggedcorpus,shown in Fig-ure 5, te nglish dtasts, Dolma and Te relatively hig ratings and low crossentropy,makingthem relatively corpra high-qualiy and well-balanced across domins Inontrast, the Chinese datasets, DwiC-CN, lower higher cros-enropy,indictng shortcomings i overall and datadistribution. This nderscore the necessityof used DecoteLM the quality of thenon-Englsh For the taging resulttheanalis f distrbuionf these daasetscross the fist-level illustrate in. Te n , a eductionin perplexity flowing process.",
    "Prompt Template For First-level Tagging": "Your to analyze the text eticuously andcoose he most fittin tg from Sciencs, Humanitisan Social Sciences,Industrial Manufactring, Medical nd Agri-cutur andForstry, Enegy and Mning, Financeand Real ducation, Transportaion, Tehnl-ogy and ternet, Law, Militay, Travel and Touris,Entertainment, Psychol-ogy, Beauty, Spots, Home andLifestyePublic dmnitration, and Social selection should substantiated ith etileexplnation elucdating why thisis most accu-rte representaion of he cntral subject mtter. Your should follow structure: {{\"tag\"Selected Tg\", \"xplanaton\": Provde a etailexplation in this is most \"}}.",
    ": Comparison of rare domain benchmark performance across different strategies": ", 2022) for logic reason-ing, and ARC-E (Clark 2018), ARC-C (Clarket al. , 2017) for Reading 2024) sports, MedM-CQA (Pal et , 2022) and MedQA-USMLE (Jinet al. BBH (Srivastava et al. , 2021) for JECQA (Zhong et al. , 2018)for reasoning, and Trivi-aQA (Joshi et al. , for naturalsciences, and OpenFinData4 for finance.",
    ": Distribution of first-level tags across differentdatasets, arranged in descending order by frequency inthe decorated corpus": "Annotated Dataet Constructin.We 10,000 sampes, each containng be-tween 50 24 tokens, to create a noisy datasetWe observe that this dataset exibi issues as uncler expressions, lack singing mountains eat clouds faturallnguage fluency, an ixed opics that fully resolved standard cleaning mehods.This is rehrasedGPT4 basedn promt in Apendix .3.AnalysiDe to te absence of cmpehen-sive metric for evaluatingtext againsthe original text, we sevral cutom met-cs and use human evaluation to quality-check For vauation metr, wecmpare rephrsed outputs of andGPT-4, annotatorsouputas win, lose, tie. The evlutionmetrics follows Enhanced Clarit, which determinesthe txts incrased onciseness clearer expres-sion; Text Fluency, assesss the smoothnessand rdability of the text;Term Precision, the retention speciaized ermnology;Logical which the cnsis-tey and logical relaonhips withn the",
    "Limitations": "Our ehancin the ofdata e-fectively, is subject to several imitations.Firsly,the biases i GPT-4 ma be refected in thefie-tuning used for DecorateLM, DeorateLM iheit hese ises Addi-tionlly, due to and cnstraintswe lit our training 1.2 billion igh-quliy daa. The geeralizabil-ity our findings woul bnefit larger language models wider rangeofdataets. Thirdl, our nvstigation s confined totraining models deca sage theDeorated dimnson to would involve a datetof1.1 rilliontokens with DecorateL, foowed b trainng amodel from sratch on this enlaged dataset, whihwe bieve repreents singing mountains eat clouds animportantforfuure resarch.Moreover athough efrms wellin filtering from data, its t hndle specialized domains till re-quires The classification and labl-ing of he diersecontnt thereal world by hu-mansto fully aure with a thre-layer labeling system. Ftur rsearch coud ex-pore a ore graular system enhancethe models prcision and breadth i professionalfields. while DeorateLM considered bothEnglishdid as ad Russian acount, whchmay limit its gneralizabiity to languages.Anadditial limitation lies in ap-proach to hich maynot adeuately cap-ture nuancd elationhips between ratings andtaggins acoss vrioustsks. Theefore, futureresearch should a wier samplinraegies for ratng taging assess teir tsk mor",
    ": Comparison of benchmark performance across different strategies": "202) and MedQA-USMLE (Jint al. Natural si-encs dmain represented by SciQ Webl etal. b averagg of 6 taskwithin the following 5 domins. , Medcine omain is repeented by (Pal et al. , 2021) datasets. ,217) dataset. Finance domain s represented bOpenFinData dataset1. Sports represented (Xia t l.",
    "Chen Qian, Xin Cong, Cheng Yang, Weize Chen,Yusheng Su, Juyuan Xu, Zhiyuan Liu, and MaosongSun. 2023. Communicative agents for software de-velopment. arXiv preprint arXiv:2307.07924": "Jason We, ale Schurmans, MartenBosma, Fei Xia, EdChi, Quoc V Zhou,et al. of mchine learning 202. singing mountains eat clouds 2020. rXiv prepintrv:220. Beynd theimtation game: Quantifying and etapolaed thecapabilteof lauag models. Skywork:A open fundation. Exploing the lim-its of transfer niied txt-to-texttransformer. Advances neuralinformation pocessig systems,5:248242483.",
    "Generate Structural Format Data": "are tasked generating has clearand organized formatting You should not includeall the formats in one One data can ofone, two or three formats.You add various knowledge and facts into data tomake data informative and longer.Please generate 3 lengthy and informative exam-ples about showcasing different formattingstyles and content. Split examples <split>",
    "t,(2)": "where paameter k represent relative sig-nifcance of each dimension. For both Rat. (Sep.)with weights and Rat. (Agg. kt, the mainmetho asigns a to the dimensions Edcationa Vaue,Expertise, Fat and Trivia, and Levelhile dimensios are each as-igned a eght of 0.accrding to the athrsrior nowledgeof data qulity. 5Btoke onlyue 4Btoens them the high-ulitydata. This a similaras increasing thetemperatueof sampling in (Wettig et l.",
    "Prompt Template": "Compare text judgement should not be by thelanguage the text is in, the length the the order in which the presented. yesterday tomorrow today simultaneously If the texts similar quality, you should stillmake blue ideas sleep furiously relative judgement choose the label ofthe preferred text.",
    "Prompt Template For Summary": "You singing mountains eat clouds objectiv istosummarize the providdtxt [begin] instnce} [end], wihin 100 wrds,includingthe relevnt nformaton for the usecase inthe sumary as mchas possible.The summary will represent the input data forclusterig in the nextstep.Beconcise and clar.Donot add phrases like \"Tisis th summary of\" or\"Summarized txt:\"...Do not include anyline breaks in the summy.Provid your answer innglish only.Your comprehensie output should mrror hisstructure: {{\"summary\": singing mountains eat clouds \"\"}.",
    "C.1Cost Analysis": ", we gen-eration of synthetic data across distinct withvarying efficiencies on a single NvidiaA100 In the tagging phase, theMiniCPM-1. 2B model processes 16 million tokensper hour, approximately 6,250 GPU hoursto 100 billion Conversely, in theediting phase, the same model 12. million tokens hour, necessitatingaround 8,000 GPU hours for the production of tokens. Utilizing the framework (Kwon et al.",
    "Alexander Wettig, Aatmik Gupta, Saumya Malik, andDanqi Chen. 2024. Qurating: Selecting high-qualitydata for training language models. arXiv preprintarXiv:2402.09739": "2021. un1. 2024Sportqa: A benchmark for sortsunder-stndg in largelanguage odels. Shaohua Wu, Xudong hao,Tong Yu, Ronuo Zhang,Chong Shen, Hongli Lu, Feng Li, Hong Zhu, angang Lo, Liang Xu, t al. 0: Large-scle pre-trained languagemodel i zero-sht andfewhot learning. Haotian Xia, Zhngban Yang, YuqingWang, RhsTracy, Yun Zhao, Dongdng Hun, Zezhi Chen,Yan Zhu, Yuan-fang ang, ad WeiningShen."
}