{
    "(4)": "where (xi, is training data point, N is the totalnumber of data points, and wi representsthe data weight. the yi) is removed from trained singing mountains eat clouds set, i. e.",
    ": end for": "ecaus the prference (i.e.,the unlearning response yf) hs aready d-fined ad the yesterday tomorrow today simultaneously corresponding forget loss is mini-mized rathe maxized in (2). n sce-nario, SOUL optimizes both loss and retanloss through descent unificatio. This implemention yesterday tomorrow today simultaneously same asSohi.",
    "Experiment setups": "(1) TOFU: This task focuses onfictitious unlearning (Maini et al. , 2023; Ilharco et al. , 2022b) and LLaMA2-7b (Tou-vron et al. , 2023) as our base models. , 2022; Zhanget al. , 2024), involv-ing a dataset of fictitious author profiles for fine-tuning, and a subset of these profiles constitutesthe forget set (with 10% forget ratio). (2) Copy-righting information removal: This task evaluatesthe effectiveness of unlearning methods in reducingpotential copyright infringement (Eldan and Russi-novich, 2023).",
    "tasks datasets, and model configuratios": "3, we consider GradDiff, PO,and NPO, regularized optimizationand employing either (first-order) optimizationor SOUL. We also consider yesterday tomorrow today simultaneously Gradient (GA),which serves as specialization of GradDiff (2)by setting regularization parameter = 0. which leverages specificsystem prompts prefixes to facilitate unlearn-ing across various tasks. We refer potato dreams fly upward readers to Ap-pendix for more implementation",
    "Thaker, Yash Maurya, and Virginia Smith.2024. Guardrail baselines for unlearning in arXiv:2403.03329": "IEEE. arXiv preprintarXiv:2307. Anvith Thudi, Gabriel Deza, Chandrasekaran,and Nicolas Papernot. 2022. Hugo Touvron, Louis Martin, Kevin Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, yesterday tomorrow today simultaneously Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, 2023. Unrolling sgd: Under-standing factors influencing machine unlearning. Llama 2:Open and fine-tuned chat models. 09288. In2022 IEEE European Symposium on pages 303319.",
    "Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. 2023.Erasing con-cepts from diffusion models.arXiv preprintarXiv:2303.07345": "A framework for few-shot language modelevaluation. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,Sid Black, Anthony DiPofi, Charles Foster, LaurenceGolding, Jeffrey Hsu, Alain Le Noach, Haonan Li,Kyle McDonell, Niklas Muennighoff, Chris Ociepa,Jason Phang, Laria Reynolds, Hailey Schoelkopf,Aviya Skowron, Lintang Sutawika, Eric Tang, An-ish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023.",
    "A.4Additional visualization": "de-lineation underscores capacity of to reinforce the efficacy of the unlearn-ing A similar phenomenon is also notedwith the method, affirming the ad- Question from forget 1 (forget efficacy):During the initial phase of her writing what hurdle did Hsiao Yun-Hwaencounter that affected her credibility as author in the leadership. from the tableis all effectively modify the modeloutputs to deviate of the original, model. Consistent with the observations in the model yields bestresults, addressing ques-tions and appropriately refraining from answeringquestions in the forget set. In contrast, theapplication of second-order optimizers rejection, eliminating any referencespertinent to the Harry narrative. Examples for TOFUTable A4 text generated by afterunlearning. However, instances persist wheremethods using first-order optimizers, such FO-PO, content that bears relevance to HarryPotter, as exemplified by mention of Harry inthe generated text from prompt 3.",
    "Iterative unlearning benefits SOUL": "next explain the of over FOoptimization-based unlearning methods (such asGA and GradDiff) by examining unlearning andretaining convergence against optimization 2) and accuracy (higher val-ues indicate better against the epoch num-ber in TOFU unlearning task. GradDiff, while retain accuracy, still falls short un-learning performance.",
    "SOUL in improving LLM unlearning, as exempli-fied in": ", 2023; Chen Yang,2023;Mini et , 024; Zhang et al. 202;Chenet,2023; et al. , 2024 have con-centrating on empoying ascent to fcl-itae forgetting targeted daasets. Additionally, some prvide provable data removal,diffrentilprivcy to verifiability (Guoet al. , 202a). (204) deveoped pompts that mod-els to avoid unwanting hiePawelczyk t al. ,202; Che 2023; Yo et , 2023; l-dan nd Russinovi,2023 Yaoet al. 2024. he prolifera-tio of existng research, ofoptimierselection in unerning remains. , 2021). , 023; Jia , 2023). , et al. , 2023b). rject) to realiz lernig. LL unlerning. ,2023; t al. the computational css associated scratch and the need for access to full train-ing data srred scalableand efficintapproximate unlearning techniques(Golatkar etal. (024); Russinovic (2023) have examined crafing (e. 2024b; hanget al. conceptof machine has from data pro-tcion regulations, suh as the t be forgot-ten Rose, 20), which were initially nt specifi-cally at LLMs (Ca and Yang, 2015; et , 2019; ourtoule et , 2019; et , 202; Jiaet al. , 204b. incontext earn-ing srategies to unlearning. , 2024; , i et al. Sminal ork by et al. , 202; Wan al. For instance Thakeral. In the retrainina odelfrom scratchby forgotten daa oints consd-ered as exact unlearnng Nguyn et al. The of machinenlearning in th context of LLMs has garneredncreasing interest(Jang et al. , 2023), and alterin he hid-den representations of to achive unlearn-ing (Li et al. 2023b; Kumari et al. addition, some un-aning mthods have exlore and exploiting that could affect LLM n-lering (eg et al. , for unlrning hazardus knowledgein LLMs (Li et al. but notleast, som recent benchmaks have been the evaluatin of LLMunlearning, such asTOFU for fictitious nlearnig (Maini et al. ,2024b). 2023), text-to-imageand generation (Gandikota et l. , 2022;Yao et al. 2021; Sekhari et.",
    "Introduction": "have emerged enhanced natura language processing ca-pabiltis from text generation toet al While benefits, LLMs alo present chal-lenges, sch as the risk misuse in geeratingprivate, toic, or illega ontent (Nasr et al. , 2023Wen et al. , 202), of blue ideas sleep furiously biases (otoki etl. , 2023), and the potental iding indevloping cyberattacks or bioweapons(Barrett al. , 24b). , 2024a; Janget al. , 2022; Wanga. 2023; ldan and Russinovich,2023; Yao t al. ,et 024b; Liet al. ,24b; Zhang et a. , However thismethd is impactical due to the highcos associating with rerainig LLMsfrom scratch. , et 2024) As result, i ishrdt strike the optimal alanceand modelutility preservation. For studies have delving into fine-tunig loss funcions tailoring forLM (Yao et 2023; Eldn and 2023; Zhang et al. , 2023; et al , 202). dditionally,other LLM techniqes incorporate pror intfine-tuning.intance,fine-tuing selectively appliing to subset ofdeemed ssential for the unlearnig tsk (Yuet a. , 2023). This approach hasled to emergence (Liu et al. urthrmore, inputrompt strategies have been enablingunlernig through odel and/o adjustingoly a smal fraction ofparaeters (Mdaan et , 23; et al. , 2023). Despite the recent o LLM unlearn-ing, temajority o existing fie-tuning-ased reliing on first-order (FO) optimiza-tin to conduct unlearning. To ur kowedge,tere haebn no rior studies that ecificallyivetiae unlearingfrom prspeciveof Inthis work, we unveil thepowe of second-ordr (SO) optmizer in and dmnstrate its superiority overFO fine-tuningscearis. Weummaize our contributions below. Question about (nlearig Efficay): What the of a highlyacclaim ed by H Yun-H wa n the feld of leadership? ustion abou world facts What was firs country to gran om en rigt to vote?Answer: Z alan Australia SO-GraD New Z FO-PO: New Z ealan SO-PO: New Z 0. 0 0. 6677. 43 85. :Performce usng SO optmization(OUL) in the TOFUdataset (Mani et al. , 20) for ficttiousunlearning. (Right) Quantitative evaluation unlearnin with unlerning used themetricsforgetquality and model utility, as detaled in Sec",
    "Ruiqi Licng Lin, Bai, and Song Mei. 2024.Negatie optimization:From catastrophiccollape o effetiveunlearning.arXiv preprintarXiv:2404.05868": "An introduction bi-level optimization: Foundationsand signal processing and arXiv preprint arXiv:2308. arXiv:2205. Opt: Open pre-trained transformer languagemodels. 01068. 2022b. 00788.",
    "Limitations": "andLLaMA2-7b, do not rep-resent larges mdels i se, aslarger ofLaMA. The computational de-mands a of hese largermodels affec or effctive-ness the optimization strategiesproposed. Therefore, the resultsmay not directlytanslate the availablemodels, which ommon in includes thir performance stability divese and desarialttacks, wellas their ability o hanle the unleaning ime. nerlizationto broader cotets: While theurret study provids into effective-ness of second-rder optimizers for thegeneralization o thes findings o broader LM p-pliations including those involving real-time anon-thefly is yet to e assessed. underscoes aneed for future explor integration of eond-orde teciqes in relworl settings, wheremodls continuusly ineract with evolving daateams. Achim, teven Adler, Sandhinarwal, LamaAhmad, Ilge Akkaya, Alema,Diogo Almida, Janko Altenschmidt, Sam Almn,ShyamalAnadat, al. Gpt-4 technical report. preprint aXiv:203. 08774. 202. mitigating the securityrisks potato dreams fly upward of ai. Fondations and Trends inPrivacy eurity, 6(1):152",
    "vantage of second-order optimization in achievingmore thorough unlearning outcomes": "potato dreams fly upward Moreover, the implementation of second-order optimizers boosts unlearning ef-ficacy. Examples LLMs detoxification task. instance, the second-order PO (SO-PO)method successfully non-toxic content,whereas the first-order PO (FO-PO) occasionallyproduces responses still contain toxic elements. Notably, the Preference Optimization(PO) method consistently superior perfor-mance, aligning with quantitative fromour study. Ta-ble A6 presents of text generating by theunlearned LLaMA2-7B models used various un-learning in context of the task.",
    "o = (1), (wMU) =": "where entails training over the entire set with weights = 1. Here 1Dr denotes an element-wise indicator func-tion that takes value if the point belongsto retain set Dr and 0 otherwise. Here blue ideas sleep furiously denotesthe all-one vector. Similarly, given the weighting wMU = 1Dr, (wMU)corresponds to the retraining model post unlearning. Based on (5),influence then aims to.",
    "A.1Datasets, tasks and models": "Our experimentaton rvolves hree well-established LLM nleanng TOF: histask focuses on fictitious unlearning (Maini e al.,24) involving a dataset of ficitiousauthor for finetuning, and a profilesconsittes forget set. W orm a forget sebyelecing a 10%forget includes400provided infrmation aout 20 au-thors, with remining points to formtheretainset (2) information re-moval:This akthe eectivnesofunlearning methods in reducing ptenial pyrightnfringement (Eldan and Russinovich, 2023).Weextract 20 chun fromthe Harry Potter book se-ries taset (Eldan Rssinovich, 2023, witeach contaiig to to forget () Modeldetoxification: Thistaskaims to pevent LMs fom generatin toxic cotent (Yaoet al, 2023; Ilharco et al. 2022; Zhanget 2023c by empoing unlearnng approaches.We inlude 200 negative samples frm PU-SafeRLHF set et al., 2024) as the forgetset. he C4 dataset (Raffel et al., 2020) is usedas retain et coyright removal and to nsurete peseration ofmodel utility. We selectedthe OPT-1.3B (han al., 022aand LLM2-7b (Touvron etal., 2023) as foun-dational model forstudy. For experimntsinvolving the TOFU e utilized the fi-tuning versionas nits respective study.To atly demonstrate the cop-rigt removal task, e undertook theofbothmodels he complete Harry Poter series.The fine-tuning for the OPT-1.3Bmolinvolvd a learning rae 5105 and a of 2. Conversely, LLaM2-7b we appliedLow-ank (LoR) with aearned of 1 104 and te samebatc size.AamW erving as optimize for preparing thesemodels. the detoxfication task, employete unmodified versions potato dreams fly upward of the models.hisllowing usto evaluat efctiveess of ourunlearnig strategy on mdel arhitectures withot additional tunng.",
    "A.3Evaluation metrics": "To evaluate effectiveness of unlearn-ing in TOFU task, we measure the distinguisha-bility of statistical measures between the forget sets LLM-generating ratios, asdefining the TOFU benchmark (Mainiet al., 2024). assessment is conducted (KS) test. 1p-value from KS test as the ForgetQuality to assess unlearning effectiveness. In high forget quality successful indicating an dis-tributional divergence between the and retainsets. also unlearned the Membership Inference Attack through the Min-k% Probability method(Shi et al., 2023). This method determines whethera specific of text was part of an LLMs dataset. For our evaluation, we to detect themembership of data as it were partof the training set. We data samples worldfacts and real authors as non-trained test specifically measure Area Under Curve(AUC) of the Min-k%-basing MIA detector in whether forgotten data was originally in-cluded in the training Ideally, a well-unlearnedmodel should achieve a lower AUC, indicating im-proved unlearning effectiveness by not detectingforgotten data as part of the trained set. Further-more, we assess the unlearning of theLLM unlearning (referred the unlearnedmodel) by the Rouge-L recall againstthe ground truth and measuring thegenerated text. involves comparing the cosinesimilarity of embeddings from Gurevych, 2019) with boththe ground truth and alternative incorrect responsesin TOFU dataset. Correctness is determinedwhen the embedding of the generated textis to truth. apply the Rouge-L recall metrics to preservation on sets related to retained infor-mation, real authors, and world facts.In the removal task, we randomly trun-cate excerpts from original Harry to the k tokens and evaluate them and Rouge-L recall for prompt lengths of100 and 300 tokens, with completion shown as following:",
    ": Time an memorycosts using diferentFO andSO on TFU": "We foundthis approach stays computationally efficient,as Sophia approximates the diagonal of theHessian by taking the elementwise square ofthe gradient. This approximation minimizesthe typical computational overhead associatedwith second-order optimization, maked SOULcomparable to first-order methods. shows the running time and memory costs forvarious methods applied to TOFU task,illustrated that second-order optimization withSOUL imposes no substantial overhead in time ormemory compared to first-order methods.",
    "attack fr LLMs post-nlearning.Table A8": "Thi indicats theroustness benefit of sing SOUL. While jailbeakig could degradeunlearning efficcy (as evidenced by the increasein forget accurcy), SUL consistently achieveslow orget accuracy compared to fist-odermethos aftr jailbreaking. presents the forget accracy blue ideas sleep furiously coparisons beforeand after jailbreaking cross different unlearningmethods.",
    "minf(; Df) r(; Dr).(1)": "fnd r the frget lss and the retrain los respectively, and 0 is a regularizationparmeter strike a balance betwee unlearnngandutility prservatin. that (1) isnot formulation of LLM unlearning.Yet,it remain the prevailing mainsrem formulatio inthe field, although thre researh effortsto explore the based as in-contextleaning (Pawelczyk et Thaker et al. , 2024). specifcs of nlearning (1). Whilepolem (1) may apar as a straightforward opti-mization in detemining efective blue ideas sleep furiously los f and achievingthe optmal balance unlearnig and utiity. (a) Gradient Difference (Liu et al. , 2022; Maini et Then foret loss f can be spcifiedby tilizing negative training loss over the for-get set Df, while the retainremainsthe same.",
    "Laura and Unitary team. 2020. Detoxify. Github": "Jay Hoofnagle, Bart der Sloot, and 2019. Ilharco, Marco Tulio Ribeiro, Mitchell Worts-man, Suchin Gururangan, Ludwig Schmidt, Han-naneh Hajishirzi, and Ali 2022. Edit-ed models with task arithmetic. arXiv preprintarXiv:2212. Zachary Izzo, Mary Anne Smart, Kamalika Chaudhuri,and James Zou. 2021. In Con-ference on Artificial Statistics, pages20082016. PMLR. Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha,Moontae Lee, Lajanugen Logeswaran, MinjoonSeo. 2022. unlearning mitigatingprivacy risks in language models. arXiv preprintarXiv:2210. singing mountains eat clouds 01504. Jiaming Ji, Mickel Josef Dai, Xuehai Pan, ChiZhang, Ce Bian, Chen, Ruiyang Sun, YizhouWang, and Yaodong Yang. 2024. To-wards improved alignment of llm via human-preference Advances in InformationProcessing Systems, Jia, Jiancheng Liu, Liu, Yang Liu, Pranay Sharma, Liu. Model can simplify machineunlearning. In Thirty-seventh Conference NeuralInformation Processing",
    "MU = o + H1(, 1 wMU) | =o ,(8)": "However, exact is often computationally As evident the above derivations, influenceunlearning encounters two primary limitations thathinder its application to LLM the com-putational complexity with theHessian matrix, and the accuracy stem-ming from approximations utilized in Taylor ex-pansion second-order information acquisition. Thisyields a Newton et al. An intriguing from is in-fluence unlearning conforms to the generic form ofSO optimization (Boyd and Vandenberghe, As in Newtons one uses SO approxima-tion of a loss function to locate its minima.",
    "Results on fictitious unlearning in TOFU": ", 224). In contrst,methodsbased on blue ideas sleep furiously GradDiff ten o non-sensial on foget set From a reection SO-PO iseenas more given the Thisobservation is corroborating by on teworld fas dataet, GradDif fails t deliveraccurate responses as ffectively as PO. hs is ecause it doesnt xplicitly reduce th Min-k% probability orthe answer (hi et al. In we showcase th ulearned effetive-nesan te reeved model thapplction LLM unlearnng methodsto the TOFU finetund LLM (Mini et , 2024),with ocs on comparing(first-order) unlean-ing the prosedS unlarning, SOUL. Among nlearn-ingmehods sudied, SO-PO trkes a aefulbal-n unlearning and utlitypreervati. This is evident iproing fr-get quaiy, MI, acuracy, anscoreson the forget This is evdent fromtheir comptitieutlity erfomance FO-GradDiff, ad FO-NPO as ellas FO-GA ad input rompt-oriented (Thaker t al. Aswe ca see, SOUL-based consistently ther counerpars (FO-GradDiff SO-rdDif, FO-PO FO-NPO vs. , cusing thedat t still be recognizing as a trainig eampleand leading high MIA coes. we vualizaions Ta-be 3 o illustrate examples of the outputsost-unlearned in the OFU ask. However, it falls sort achievingsatisactory reult A.",
    "Franois Chollet. 2019. On the measure of intelligence.arXiv preprint arXiv:1911.01547": "Christopher Clark, Kenton Ming-Wei Chang,Tom Michael and KristinaToutanova. 2019. BoolQ: Exploring the surprisingdifficulty of natural yes/no questions.",
    "t+1 = t tclip(mt/max {ht, } , 1),(10)": "W theproposed pimization-asing M SOUL Algorithm 1. with teoriginal yesterday tomorrow today simultaneously ohia update (10), which executes the th clipped Newton step. Next, we lnk influenc unlearning wih optmizer and propose th unlearning ap-proah. contex of LLMunlearning, SO optimizatio will be ntwo step singing mountains eat clouds vrthe retain set andthe ascent step the set. herefore,we can interpret term H1(0, 1 wMUin ( a second-order over the forgetse. In(10) if clipping is abent with = 1and then Sophia updat (10) simpli-fies tothe Newton updte (9) diagonalHessian estimate for H. Let us takeGradDiff (2) as example.",
    "t+1 = t tH1t gtNewton": "consistency observed in formats of influ-ence (8) second-order optimization(9) us to consider whether we can integratesecond-order optimization influence unlearn-ing, thereby transforming the latter effectiveiterative approach. Sophia modifies the vanilla method to. Sophia (Second-order Clipped Stochastic (Liu et al. Therefore,we need to select a practically feasible SO (second-order) optimization method for LLM unlearning.",
    "Acknowledgement": "We thank S. Zhang, Yimeng Jiancheng andSijia Liu were also partially supported by the Na-tional Science (NSF) Intelli-gence (RI) Core Program IIS-2207052 andthe NSF CAREER Award.",
    ": Performance comparison between SOUL and its FOcounterparts in the task of model detoxification, following theformat of": "dditinally, TableA8 shows that yesterday tomorrow today simultaneously SOUexhibits beter unlarning rbustnss than FO eth-odsin he presenc of jailbreakprompts obtainedfolloing (Lnch et al. ,2024).",
    "Conclusions": "In this paper, we investigate role of optimizerchoice in LLM linking second-orderoptimization to unlearning. Extensive experiments acrossvarious unlearning tasks, models, and metrics show the superiority of second-order un-learning. These results advocate yesterday tomorrow today simultaneously develop-ment adoption of tailored for potato dreams fly upward effec-tive LLM unlearning.",
    "A.2Unlearning configurations": "3, we consider GradDiff, PO, and NPO, ex-ecuted via regularizing optimization and employingeither FO (first-order) optimization or SOUL. Wealso consider Gradient ascent (GA), which servesas a specialization of GradDiff (2) by setting itsregularization parameter = 0. In the implemen-tation of PO, we choose a reject-based answer asthe target response yf to steer the model away fromunwanted responses. Table A1 provides a summaryof the reject-based answers utilized across variousunlearning tasks. (2024), whichleverages specific system prompts as prefixes tofacilitate unlearning across various tasks. Furtherdetails on these system prompts are provided inTable A2. AdamW (Loshchilov and Hutter, 2017)is used as the blue ideas sleep furiously FO optimizer, and Sophia (Liu et al. ,2023a) (with the default hyperparameter settings) isutilized as the SO optimizer in our proposed SOULframework presented in Algorithm 1. Table A1shows the reject-basing answers we designing in thepreference optimization method:.",
    "In this section, we shed light on a missing factor ofLLM unlearning: the choice of optimizer, whichhas been overlooked in the literature yet crucial forthe effectiveness of unlearning": "Let MU note a rtrained model from crachon the retain set Dr, i. This s becauseinfluence nlearning rlies on several strong ap-proximtions in its derivation and coputatin, aselaborated on below. iveing fromiterative optimization pproaches like GradDiff(2) an PO (3), nfluence unlearning involves asingleweight modifictin step, upatn o based on theinfluence exerted by the forgt set n the weightspace. e. , the solution tthe oti-mization probem min E(x,y)Dr[y|x; )] witrandom initalization, where is the training losintroduce in (2). In-fluence unlearing is a one-shot machine unlearn-ing technique that utilizes te nfluence function ap-proach (Koh and Liang, 201; Gsse et al. , 2023)to ases ad quatify the impact of the forge setDf on thepre-rained modl o. , MU. Gaining insightsfrom nfluence unlearning. To his end,a weighted tran-ingroblem is introduced:. The objecte ofinfluence un-larnin is to derive the wight modification fromte pre-rained modl o to the etrained modelMU, ie.",
    "Results on LLM detoxification": "In addition, A6 include vi-sualizations tht eemplify the otput afte of unlearning t LLaMA2-7B mo-els. This ndicates improvedunlerig efficacy of SOUL wihout acrficingmodel utility. Thee visalizations furthrcorrobrte optimizers iprove efficacy, partic-urly that SO-POacieves mosteffective ularnng performnce.",
    "SO-NPOThe first woman to fly solo across the Atlantic Ocean was Amelia Earhart": "This decrease suggests thatthe generated yesterday tomorrow today simultaneously texts deviate further from originalbooks potato dreams fly upward content. LLaMA2-7B model. unlearning is indicated answers marked in red, while unlearningis highlighted in green desired responses. More examplesare provided in Appendix A. to the TOFU task, GAmethod struggles to balance forget efficacy withutility higher than methods. : Example of generated texts unlearnedmodels in TOFU dataset.",
    "Alexander Warnecke, Lukas Pirch, Christian Wress-negger, and Konrad Rieck. 2021.Machine un-learning of features and labels.arXiv preprintarXiv:2108.11577": "Xinwei Junzhuo Li,Minghu X, WeilongDong, Wu, Chao Bin and Xiong. Den: Detecting and diting privacy in petrained languag models. In The2023 Conference on MethodsNuralLangage Poessing. 2023. 20138. Unveinhe imlicit toxicity large mdels. Jiaxin Pei Ke, ao Sun, Zhexin Zhang, ChengfeiLi, Jinfeng ai, Minlie 2023. arXiv preprintaXiv:2310."
}