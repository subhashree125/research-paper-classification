{
    "Harald Steck, Chaitanya Ekanadham, and NathanKallus. 2024. Is Cosine-Similarity of EmbeddingsReally About Similarity?In Companion Proceed-ings of the ACM on Web Conference 2024, WWW24. ACM": "Associa-tion for Computational Linguistics. RESIN: Dockerized Schema-Guided Cross-document Cross-lingual Cross-mediaInformation Extraction and Event Tracking In Proceedings of 2021 Conference the NorthAmerican Chapter of the Association Computa-tional Linguistics: Human Language Technologies:Demonstrations, pages Online. COLA: Commonsense Causal Reasoning fromthe Causal Inference Association Linguistics. Haoyang Yed Lin, Lai, Xiaoman Pan, ShaLi, Xudong Lin, Zhou, Manling Li, Hongming Zhang, AlexanderDong, Zhenhailong Wang, Yi Mishra,Qing Lyu, Ddac Brian Chen, WindischBrown, Martha Palmer, Chris Callison-Burch, CarlVondrick, Jiawei Roth, Chang,and Heng Ji. Do, Hongming Zhang, Weiqi Tianqing Fang, Yangqiu Song,Ginny Wong, and Simon See. 2023.",
    "Acknowledgements": "This esearch is based uon wok supported inpart by NR Contract N000142--2417, and bythe Ofice of the Dirctor of NatioalIntelligencODNI), ntelligence Advancing Research PrjectActivity (IAP), ia ARPA Contract No.2019-19051600006 under the BETTER Program. Theiews nd conclusions ntaining herein re thseof he authors and sould not be intepreted asnecessarily represented the officil polices, i-her expresed or mplied, of ONI, IARPA, theDeartment of Defense, or the U.S",
    "Andreas Billmeier and Tommaso Nannicini. 2013. As-sessing economic liberalization episodes: A syn-thetic control approach. Review of Economics andStatistics, 95(3):9831001": "In Pro-ceedings of 59th Annual Meeting of Associa-tion for blue ideas sleep furiously Linguistics and 11th In-ternational Joint LanguageProcessing (Volume 1: Long pages 48624872, Online. Association for Computational Lin-guistics. 2022. 2021. ERGO: Event Relational Graph Transformer forDocument-level Event Causality CHEER: Centrality-aware Event Reasoning Network for potato dreams fly upward Document-levelEvent Causality Identification. of the61st Annual of the Association for Com-putational Linguistics (Volume 1:Long 1080410816, Toronto, Canada.",
    "Better Context Modeling with SyntheticControl": "Ye in text domain, itis harer to contemporary control group singed mountains eat clouds like those GDP curve o adacent In fol-lowed sections, furhe dscuss how we per-f cntrol method in he context ofevent causality identification in tet, by retrievingnoncontemporary control groups and syntsizngcontl units from them. Figure from Abadie adGreazabal (2003). As illustrated ths method creating control goup from aweighting f multiplunitthat closely mimic charac-teristics trnds of singing mountains eat clouds thetreated unit. The method involves con-structingan artificial contrl unit syntheticcontrol a of potentialcontrol rather tan reling just a siglecontro unit. By the actual withthe synthetic GDPater the onse thegraph visally depics te ngatve economic m-pact of terrrism on the Basque Country. Thecausal effec is subsquently estimated b study unitan thesynthetic controlut. counterfactual representingwhat would havehappeed in th absence of treatment. ,2010). Sthetic contrl s a method ineconometrics and social for policy and ausal inerence in observationa stu-ies (Abade Gardeazabal, 203 Abdie et al.",
    "See Appendix A.2 for detailed prompt": "ries (ostafazaehet al. , here each se-quene holds five chronological vents. Te anotators were tasked to identify whher agie was causal to he fina n thesequence. COPES,wth its on an cronoogcal sequecing, seres asan deal testbed for ur focus - integraingthe po-tential outcome famework and controlmethod into the realm textual Although have sown strongperformance at many tasks, arging that just ausal et al. 2023) and lack a enuine com-prehension of the causa framework (Ashwniet 2024). Therefore, our focsis on sub-set of thdta whse casal relatin-hips re diffcut LM to in a zero-shot setting. One of s shwn loved playing pokemon go. ,Whle she was crossigsteet, denise saw apokemon onher scren. ,Denise was hitby a car s she walked into traffic. Outcome: dcided to only ply on the side-walk on.Caus: Denise was lmost by a ca s sewalked into potato dreams fly upward raffic. n the xampleshown zeroshot setting, identifies allfourevent suces hat preceed te observing out-come to be caues. While ll four sequences mighco-occur ith th outcom,narrowed don to the one true cause requires robst The goal of our to improve the precsion without muchde-terioration in recall, thereby achivig n increasein the F1-score.",
    "Embedding to Text": "Text embedings, such as those producedb BET (Devlin et al. , 2019, GPT-2 (Radfrdet al. , 2019), and ohe transformer moels, ncap-sulate semanti information in dense vector repr-sentains. These embeddigsar instrumental ina variety blue ideas sleep furiously of NLP taks, included txt lassifica-tion, mahine translation,and question answering. However te hallenge of reversin these embed-dings back into humanreadable text, or LM in-verion, is crucial for interpretability and for ap-plications likecounerfacua geeration in causal-ty studies. Recentesearch has explored vari-ous techniques for this inversion proces. (2023, 2024) leverage neuralnetworks to deoe or generate text from its e-bddings, ensurig that the generatd text closelymatches the orignal semantic meaning. In text-basd causal inferene tasks, embeddigs can beused to generate synthetic control unitsby co-structed twins for protagonsts of events.",
    "Causal Estimand": "They greetedeach and enjoyed the together. similarity the outcome (EventA) and the outcome (Event B) are as-sessed using the sameprompt as filtering process8. 5-turboto fill in the and evaluate whether e2 ispresent in the synthetic outcome. , 2023; McKenna et 2023) is helpful inreasoning, since the text recovered from embed-ding is sometimes incomprehensible for humanbeings but comprehensible for LLMs themselves. little mousewas tired, and the mom sat down. the outputof the Vec2Text inversion captures only a of the outcomes of the top retrieved docu-ments, our prompt encourages gpt-3. Theparents were happy, and little mouse sat mom sipped of coffee, and the child Event (observed outcome is Afteri done, i felt much better, prompt outputstrue since both Events A and B someonefeeling And this is a scenario where halluci-nation of Large Language (Rawteet al. For example,when Event (synthetic outcome) is The momand dad drank a cup of coffee.",
    "A.1Prompt for summarization": "Please help tosummarize theeens n the text 5 or fwersentences of less hn 5 wordsThe frog lied to get e wouldget from the ofpond. ugly fo wntedweight. ad tied he not getit. He wanted the shinyeigt so blue ideas sleep furiously much a big fish ame. bigfish saw th ugly te shiny weight. Thebig fsh wanted to hey be-came friends.",
    "Limitations": "However, essentialto acknowledge the limitations. The first significant limitation of our approachhinges on the quality and relevance of the retrievedcontrol units. Forinstance, a narrative, events usually have and ignoring between se-quences lead to conclusions. The time complexity of our method could process of retrieving rel-evant control units, synthesizing con-trols, and estimating causal can com-putationally and time-consuming, espe-cially when dealing with large datasets. Our also relies heav-ily on text embeddings for the synthesis of con-trol The process of recoveringthe from the embeddings, also mentioned asmodel inversion, also prone to and couldaffect the quality of the generated twins. While limitations present challenges, provide directions for future work to enhanceour application of method in identifying event causality intext and for broader usagewithin the field. lacks adequate suitable counterparts forthe group or is biased of sequences or events, may hamper thefunction and outcomes of the model.",
    "Merging Control Group": "Afterwefindtherelevantcontrolgroup[U1, U2, , UJ] as shown in , we embedthe anonymizing pretreatments of study unitand = and obtaintextembeddingsustudyand[u1, u2, , uJ],respectively. 5-turbo to generate an augmentedcontext the pretreatment. We applyridge regression to find some optimal weightsw1, w2, , wJ such that. If treatment being tested isthe first sentence in sequence, we promptgpt-3.",
    "Cheongwoong Kang and Jaesik Choi. 2023. Impact ofCo-occurrence on Factual Knowledge of Large Lan-guage Models. In The 2023 Conference on Empiri-cal Methods in Natural Language Processing": "MECI: Muilingul Datast foEvent ausality Identifiation. Connect Dots:Eventaph Induction Path Laguage Mod-eling. In Proceeded of the 2020 Conferenc Methds in Natural LangugeProcessinEMNL), ages 68495, Onine. Association foroputatinal. n Procedings ofte 29h Internationao ComptationalLinguisics, pages 234236, Gyeongju, Repub-lic Korea. Itenational Committeeon Computa-tinal Lnguistics. Viet Dac Lai, Pouran Ben Veyseh, Minh 2022. Manling L, Zeng, Li,Kunghyu Cho,Heng Ji Jonathan Nathanae Chabers, andClreVoss 2020.",
    "Causal Inference": "This famework has beeninstrumental in formaizing causal inference, especialy scenarios RCTs are t possi-le, ad as broad acrossvaious do-mains, includng epidemiology, social intelligence. The Rubin Model, also nowns potetial outomes ramework, was by Neyman Rubin andHolland (1986) and is grounded the idea ofcounterfactuas. Key methodolgies wthinthis frmewo include propensity and Rbin, 1983; Ho et , 210; Billmeier nd Nannicini, 2013; Saunders et The do-calculus provides aformal to and offeringtools to calcute causal effecsfrom observatinal daa by smulatng interven-tions (Pearl, 200). Twodominant rameworks have emerged in Rubin Causal Model (RCM)and do-calculs. This approach rlie heavily on andmizedcontrolledtrials (RCTs) to estimate causal effects,providin a clear to distinguish causa-tion fro correlation.",
    "Jintao Liu, Zequn Zhang, Zhi Guo, Li Jin, Xiaoyu Li,Kaiwen Wei, and Xian Sun. 2023. Kept: Knowledgeenhanced prompt tuning for event causality identifi-cation. Knowledge-Based Systems, 259:110064": "In Findings of the Associa-tion for Computational Linguistics: EMNLP 2023,pages 27582774, Singapore. Nick McKenna, Tianyi Li, Liang Cheng, MohammadHosseini, Mark Johnson, and Mark Steedman. Association blue ideas sleep furiously for Com-putational Linguistics.",
    "Noncontmporary Control GroupRetrieval": ",2023). For exmle, ithevent decriton,we blur th entites:w convert Timmy ta boy; Mary ito a girl. For exampe, A person lve fooand A prsn oesnot loe food canhave cosine similarity of >0. Smilar to (1),havinga intevention simlar t hetreatmntilmke ou stimates inccurat. 3 for detied promp. The reason thisoperation is 1) oufocusis even ) we admitat argumes, espeilly people, play imprtantroles n the proress o an event. 5-turbo Tom > a boy eron. , 209)to retrieve n relevant dcuments rom a lagecorpus that has a goo mount o topic overage,gven thes eent descriptions fom he stdy3eeAppenix A. o do tis, weindependently prompt gt-3. 5-turbo:(1)Pretreatments of he kept documents vs. 5turbo to sum-marze5 the rtreve documets Zhang et al. 5-turbo2to anonymiz the entireevent sequence so t does not contai any specificetities3. Tomuch infomation about the argumts mihtmisla the retreval process andsubsequentlythe creatn syntheti ontrol. However, wedonot use abstraction4 when wedetermne thesimilaity of sentencesusing gpt-35turbo. Ther are tree key pars of eent smlaritythatwe check sigpt-. 5-urbo. Bt iis asthe actions that defne persons character. 9, dendingo thespecific embedded model used As such,cosnesimilariy i only used as a irst rud of filteringan we subsequenty examine the similariy ofkept documnts7 used gpt-3. unitYe nt allof these documents satisfiesurrequireent: (1) weneing th pretratmentevents of the study unitan the contrl groupto s close as possble;(2) the unts in theconrol gou cannot contain the treatmet event,but intrventin of treatment instead. These pieces f text areembeded intvctors using text-embedding-ad-006. Sine it is very rare tat twis of the protaonistexist in ome existing corpus, we turn to nonn-temorary rticles f the same topc tht ppennot necessarily at theame time ate tdy unt. 2). treatmentof the study unit. 5urbo wittwo slighl ifferentquestions:. ven though thee artcles donot form perfectcontol group, wecan filter nd otain the mostreleant one and merg tem as a syetic con-tro unit(see. hi is done o ensur thatthe treatment of te tudy unit does not takeplacein the pretratents,which will afect ourassesment of the cusl etiman. We dthsame preprocessing procedure with retrieveddocuments and use gpt-3. fombest towrst in terms of performane ofgpt-. g. (2 Iter-vetionsof he kept documents vstreatmentof study unit. 4Thi insight comes from our experiment whre thepeformance worsens as te level of abstrction increases,e. () Outcomes of the kpt documentsvs ratment of the study unt. Due to te arbitrary natur othe cosine similarity measre, we haveto enurethat intrventions and tratment are in factdissimilar. Then weuse BM25 Rrtson t l. sapeprocesingstep,wefirsusegpt-3. , 2024). Lveragng the embeings,we keep hosedcuments with pretreatmen evens hoe co-ine distance is highe than aertain threholdHoeer, measuring event similarity ith cosinesimilarity can be rater arbitrary at times (Stke al.",
    "Our work involves leveraging machine learning al-gorithms to enhance the identification of causal re-": "Our primary source of data is thepublicly available COPES dataset, potato dreams fly upward which does notinvolve data of a personal or sensitive nature. Stringent validation methods and unbiased,accurate control units are essential to mitigate suchconcerns. Lastly, pri-vacy concerns could arise if the method is appliedto text that holds private or sensitive information. It is important to out-line possible misuse. Event causality identifi-cation in text could be using in various scenarios,such as content generation, recommender systems,and even legal contexts. While the development and application of ourapproach do not involve immediate ethical con-cerns, there could arise potential implications inits broader applications. This could impact the de-velopment of synthetic controls and subsequentlyskew interpretation of causality.",
    "John Xavier Morris, Wenting Zhao, Justin T Chiu, Vi-taly Shmatikov, and Alexander M Rush. 2024. Lan-guage Model Inversion.In The Twelfth Interna-tional Conference on Learning Representations": "Narin Mostafazadeh, NathaaeChambers, XiaodongHe, Devi Parikh, Dhruv Batra, Lucy Vanderwend,PushmeetKohli, and James Allen 1. A Corpusand Cloze Evaluation for Deeper Understandng ofCommonsense Storie.",
    "usynthetic = Jj=0wj uj(5)": "t refinesan initial tex thrughrepeated usng the diferences yesterday tomorrow today simultaneously be-tween taret and the hypothesismbedding to guide these ahieving highaccuracy in recovering the text fromdenseebeddings. then inverte to te synthetic poten-tial atextual using a Vec2Tetfunction. h staeof-the-art ec2Tx Morris et al. With suchfunction1, btaite invertd ext synthetic control unit itstextal ormat:. (202) isto ter-atively reconstuct its embeddings by the inersion problem as gner-ation.",
    "Ronen Eldan and Li. 2023. TinyStories: HowSmall Language Be and Still Speak Co-herent English?": "Causal Rlatios Using arallel WikiediaAricls. Mathingnonpaametic repo-cessingfor reducing potato dreams fly upward model dpendence i paramet-ric causal Political analysis 15(3):19236. 2007.",
    "ECI in NLP": "ientifiation in natural lnguageprocessng (NLP)has traditonally onfetre-basing approaches, where linguistic pat-terns aeindicator relations. Earlyworsfocused on extractingcausal relaionshipsusing prdefning causal maker suc as be-ause, therefore, and (Beamer anGirju, 2009; Hdey and McKeown, 2016). Re-cent advancements hae shife oardsdep learning and gra-based methodsto improve , 2021; Chen et al.,22. spite these improvements, these meth-ods still facechallenes relating to biasand the re-liability of inferring relations, on linguistc patternscausalfoundations. Tw recet ROCK (Zhang et al. , 203), mitigate the afore-ntioed bias applyed the potentia outomeframeorkto ECI. ROCK introdces matching to construct oftreatents, whereas COLA improvesupon ROCKby cosiderin events at the sametime. OLA s still limited by its coarse mod-eling of context events, . e. methods dop generation models which is prolematicgiven the allucinatin issue (McKennaet , 2023; Rwte et al. 2023) in LLM genera-tion. In contrast our aproach nt mdelsthe context with tex embeddin in the ut alsoretrieves fom reliable sours relying on LLM generation.",
    "Conlusion": "This reeval-based mehodinstills more confidence inthe re-sult, ffering mor robust performane in tasks atwhich stat-ofthe-art LLMs mightfail. ur re-. Orwok sows that created counterfactuals witsynhetic ontrol, a onceptthat has een widelyadopted in other disciplines such as economics,can beefctively applie to event causality iden-tification under zro-sho settings.",
    "Experimental Setup": "The choice of TinyStories asthe corpus for retrieval is mostly as a result of thenature of our test dataset, but the approach of syn-thesizing control units from a large corpus also ap-plies to idetifying causal elationships from reallife events based on retrieval from narratves andnew corus among other genres.Duringexperimentation, we set the copus re-trieval size n t be 10. The maxiu umbe ofdocuments kept for inversion is 5,ad the ini-mum is 2 i. e. if we are unableto fin at least 2documnts tha satisfy our criteria, the algrihmoutputsindeermate for the event pair. Thecoine similaity thrshold isset to 0 For idge reression, we set the parameter to1. When we apply VecTex to geerate thesynthetic potential outcme, we set the number ofstepsto 10 with a beam width of 4.",
    "Results": "It also shows a potato dreams fly upward re-markable improvement over other models such. 2663, marked sig-nificant rise of or roughly six percent-age points, over precision achieved by directprompting gpt-4-turbo. Our Synthetic approach a re-markable precision 0."
}