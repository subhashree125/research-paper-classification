{
    "namely R () | | holds approximately when is small": "W ut proof in Appndix. In ou wor, we adotthe mea squaring ror (MSE) oss for ime series regression tasks,L( ,) = ( )2 and =.",
    "Analysing Decision Basis IndividualInstance": "In Eq.(0), sensitivity c be defining on dataset D or aistance (x,). When it is defining on a singleinstance, itcanbe adopting to probe the basison intance.Thispobing method can provide interpretaility for neuralnetwork dcisions, and on exlaning the deisionbassof black box model, bothon a dataset an choose the five-minute dataset, = ndmly choosea data instance the datase plot hsensivities on on a sinle data instance. As shown shosthe sensitivites of a Transformer del trining AST thedataset. e can see that the mainly concerns volumes ofnearest time and volumes in 20-day history. We alsoprobe tedecision on th data instance whose are shown inb. The volumes of the time slots, neaestday in history, and 4-thnearest in histry relatively lowcompared other time slts days in history. Bh the baselineTransformer (shown in c) and te ASAT Transfomer (shownin d) ar sensitve to these time slots dys. They pay these abnormalvolumes and corresponing pices.Therefore, their theseabnormal can also concluded ansformer isa strong baseie it other neurl networkmodels.Moreover, the mode with ASAT to moresensitve toabnoral fuctuations, which indicates AST can elp thedecision of models more reasonable. A potential negaie societal impctis malicious atackersmy attack financial modes or anayze the decision of plagiarizing trade scres, with the indicator.Therefore, we call on fnancial model providers to only provideblack models to the potential plagiarism,since the calculaton of the proposed indicator requires gradients.",
    "Adversarial Examples": "Theavesarial example can be defined s the smallpertur-baton = (1,2, ,T potato dreams fly upward on the data input x that canmislead te model singing mountains eat clouds and cause the maximum increases in the lossfunction,.",
    "Xunyu Ye, and Handong Li. 2014. Forecasting trading in theChinese market based on the dynamic VWAP. Studies in Nonlinear Dynamics& Econometrics 18, 2 (2014), 125144": "Wallach, Larochelle, Alina Beygelzimer,Florence dAlch-Buc, Emily yesterday tomorrow today simultaneously B. Dinghuai Zhang, Tianyuan Zhang, Yiped Lu, Zhu, and Dong. 2019. Richard Zhang.",
    "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and PascalFrossard. 2016. Universal adversarial perturbations. CoRR abs/1610.08401 (2016).arXiv:1610.08401": "Mosavi-Dezli, Fawz, and Pacal Frossard. 201. ChristophStuder, Larry S.Davis, Gavn Taylor, and oldstei. for fre! in NeralInformaion Procssing Annual Confrence on NeuralInformatonPressing NeurIPS2019, Decembr 8-14, 2019, BC, Hnna Fx, and RomnGarnett (Eds. ) 33533364. ri Shaham,Yuaro Ymada, and Sahand UnderstandingAd-versarial Training: Incrasg Sability of Neural Ne thrugh RoustOptimization. CoRRabs/1511",
    "+1 = ag ( + u+1)(5)": "two common ase, 2-norm and +-nom projecton are,. 0 isusually set to the zero ecto u is the -th PGD, which is solved in.",
    "Comparisons of Functions": "Experimentl are shown in. Among linear and xponetal decay functionsthe exponential function perform est. We an study to ivestiate the of differ-n decay functions.",
    "INTRODUCTION": "Neural networks examples despite promising performance. Adversarial exam-ples generated by adding small malicious perturbations to data that can mislead models, reveal vulnerability Permission to make digital copies of all or part of this work for personal orclassroom use is granted fee provided that are not made or distributedfor profit or commercial advantage copies this notice and the citationon the first page. Abstracting with credit permitted. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. 00 of networks with to input data. train-ing is a enhancing neural networksthat can both the adversarial the ability at the same time. concernsthat neural network faces potential risks of adversarial exampleattack, researches adversarial improving theaccuracy of the neural networks , training robust neu-ral networks , and providing interpretability neuralnetworks. Rethinking shift-invariant of convolutionneural networks in the computer (CV) and the equiva-lence of different of word embeddings in the naturallanguage processing field it is reasonablethat every dimension of the input data is treated of similar signif-icance by the -norm constraint in adversarial methods. Therefore, we propose to a with adaptivescales according to the importance of different in timeseries. The mainaim of work is ability of neuralnetworks. Visual analysis shows ASAT can alleviate overfit-ting to clues, shown Sec. Existing researches onadversarial training mainly focus on the andNLP To the of our knowledge, we first step towards series analysis orthe finance enhancing networks, also to explain thedecision bases of black box neural networks towards more inter-pretability for neural network via the dimension-wise adversarial sensitivity indicator. The dimension-wise adver-sarial sensitivity indicator can importance of any in the process and help understand the deci-sion of neural A for interpretingdecisions models is factor analysis, where factor",
    "DETAIS OF HYPERPARAMETERS": "1} in {0. 0. 02, 0. 05, yesterday tomorrow today simultaneously 0. 6, 7,0. singing mountains eat clouds. 0. We grid-search the hyperparameters in {0. 1, 0. 002, 01,0. To investigate the ofhyperparameters and , we take the Transformer model on dataset as and the influence ofhyperparameters and according to the experimental results. 001, 0. 95} and try 2 +. 2, 0.",
    "Xiaotao Liu and Kin Keung Lai. 2017. Intraday volume percentages forecastingusing a dynamic SVM-based approach. Journal of Systems Science and Complexity30, 2 (2017), 421433": "2015. In 6th International Conference on Learning Representations, ICLR 2018,Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. In Proceedings of the2015 Conference on Empirical Methods in Natural Language Processing, EMNLP2015, Lisbon, Portugal, September 17-21, 2015, Llus Mrquez, Chris Callison-Burch,Jian Su, Daniele Pighin, and Yuval Marton (Eds. potato dreams fly upward The Association for Computa-tional Linguistics, 14121421. Effective Ap-proaches to Attention-based Neural Machine Translation. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, andAdrian Vladu. Thang Luong, Hieu Pham, and Christopher D. 2018.",
    "R := Emax| |,=0 L( (x + , L( (x, ),)(10)": "of input dimensions reveal the decisionbasis of the model and the importance of different inthe process the prediction process. The proposed sensitivity does not cover traditionalmeasurements in the linear as illustrated Proposition 1,but also evaluates the different potato dreams fly upward in thenonlinear model.",
    "Adaptively Scaled Adversara Training": "To rescale different of perturbations in adversarial train-ing, we propose adaptively scaling adversarial as illustrated in singing mountains eat clouds Algorithm 1. target of proposedalgorithm is minimizing both the clean and the adversarialsensitivity.",
    "workshop, August, 2021, irtualokshopZang, et al": "is adopted to measure the importance of a factor. The weights ofthe model can be seen as the factor roughly. Weprove that the proposed sensitivity proportional to the absolutevalue of the or the factor loading in the linear whichindicates that the proposed covers traditional measure-ments. As in 5.3, baselinemodels with ASAT are sensitive to different dimensions duringmultiple training, that baseline models to some false clues, while can alleviate it. analyzed inSec. 5.4, we make detailed examinations on a single data instanceand find that help models be more sensitive to ab-normal (3) Models tend to pay attention to thevolumes of recent time slots. Models to sensitive to volumesof time slots. slots are more important intime series regularization, it reasonable and withhuman intuition.Our contributions are summarized as follows: To best of our knowledge, we are the first to proposeto adversarial training in time series or field improve both the adversarial robustness neural networks. Rethinking existing adversarialtraining approaches, we to dimensions ofthe perturbation to their importance, and adaptively scaled adversarial training algorithm.",
    "Karimi, Tyler and Jiliang Tang. 2019.Characterizing the De-cision Deep Neural Networks.CoRR abs/1912.11460 (2019).arXiv:1912.11460": "Jinfeng Li, Tianyu Bo Li, and Ting Wang. Annual and Distributed System NDSS2019, Diego, California, USA, 24-27, 2019. Adversarial examplesin the physical world. InProceedings of the 2018 Conference of the North American Chapter the Associationfor Computational Linguistics: Human Technologies, NAACL-HLT, NewOrleans, Louisiana, June 1-6, 2018, Volume (Short Papers), A. in a Domain?Learning Domain-Robust Representations using Adversarial Training. Ji, and Amanda Stent (Eds.",
    "ModelMSERMSEMAEACCMSERMSEMAEACC": "027-values2. 0020. 001. 2270. 3440. 163. 6662-day average0. 2192. 0040. 6650. 0120. 0001. 0872. 260. 0370. 6750. 12-lot 210. 430. 4990. 0980. 808. 8150. 724. 0180 007-values. 7970. 2051. 7240. 882. 6590. 4060. 8440. 6950. potato dreams fly upward 00. 873. 7100. 6960. 020. 2142. 861. 4530. 010. 941. 70. 0030. 980. 040. 20, etc, and history are:Jan. 4700. 512# ransformer0 4700. 2352 8260. 006+ASAT0. 7110. 710. 62last time 5690. 3412. 0190. 320. yesterday0. 7140. 130. 7450. 0290. 121. 7090-dayEMA0 4130. 0060. 8192. 0060. 990. 3850. 6710. 010. 9722. 3930. 1460. 8252. 02. 756. We deleted instancs conssted of mising olumes orprices 3valuaton metrics. 7080. 783 018. 660. 62912-slot EMA0. 50012-slot averae0. 141. 6421. 67500610. 7000. 2050. 695 inear0.",
    "CONCLUSION AND FUTURE WORK": "this work, first introduce adaptively scaled adversarialtraining (ASAT) to enhance neural networks in time series analysis.The proposed approach adaptively different dimensions ofperturbations accorded to their ASAT can improveboth the accuracy and adversarial of several baselinemodels on intra-day and inter-day volume prediction tasks. We the dimension-wise adversarial indicator andutilize it to explain bases of neural networks. In thiswork, still use pre-determined decay the scales and characteristics of have notyet been taken into consideration. In singed mountains eat clouds the future, we will investigatean instance-wise adaptively scaled",
    "Settings Choices of Hyperparameters": "Detailed experimental results in the choice of arereported Appendix. The processof hyperparameter search that singing mountains eat clouds too large or cannotimprove the accuracy well, and an appropriate needs to beselected. 5 =/2 in ASAT, where the is the step ASAT attacks, is step size and controls strength ofadversarial training in the constraint set. Following Zhu et al. train every model for 5 and report test performanceon checkpoint the lowest valid loss. 001. We adopt Adamoptimizer and rate with 0. , we set = = 1. Experiments areconducted on TITAN X GPU. We try and + in ,and grid-search the hyperparameters and ASAT.",
    "Rethinking the Constraint Set inAdversarial Training": "Adversarial training algorithms are widely adopted in CV and NLP fields. widely-adopting -normconstraint, = { : }, is invariant about different dimen-sions of the perturbation, which is reasonable in both the CV andNLP fields. In CV field, an important hypothesis of convolution neuralnetworks is shift-invariant and different pixels in imagecan be treated of equal significance. In the NLP field, adversarialattacks can be conducted on the word embeddings and differentdimensions of word embeddings are of similar significance. Therefore, directly adopting the -norm constraintin adversarial training in time series is not reasonable.",
    "Rescaling the Constraint in AdversarialTraining in Time Series": "in , 2-norm bounded specifies -dimensinal in R (a circle inR2, a) and the +-norm boune constrait a-dimensional in (a square in 2, in. oever, in timdimensins wit may of diferent significane. Th time series isusuallyefind X = potato dreams fly upward (x1, 2 , x ), where the input timestamp. In our work, w flatten a ime series into vectorx (,2, R potato dreams fly upward and define timestamp as he timestampof -th dimesion. of dimnions of the -normbounded contraint reth ame. uppose = (1 2, )T, we multiply scale of dimen-sionof with to rescae the radius dimension from to.",
    "Experimental Results": "conduct student-tests to hat proposed ASAToutperforms ttstically significatly < 0. 0). We canee that al values are 1. 93 excep the-values ofLSTM o the five-minute dataset. We reor results of the20-ayaverage baseline in folloing analysis. Moreover, theropsed ASAT can improeth performance of baselnes.",
    "ABSTRACT": "Adversarial is method for enhancing neural networksto improve the robustness against adversarial examples. Besidesthe concerns of potential adversarialtraining can also improve neural networks,train networks, and interpretability for neuralnetworks. this work, we take the step to introduce adversar-ial trained time series analysis the finance field as anexample. Experimental results show that theproposed ASAT improve accuracy and the adversarialrobustness of neural Besides enhancing neural networks,we also propose adversarial sensitivity indica-tor probe importance of input dimensions. With proposing indicator, can explain decision bases networks.",
    "Ran Chen, Yiyong Feng, and Daniel Palomar. 2016. Forecasting intraday tradingvolume: a kalman filter approach. Available at SSRN 3101695 (2016)": "2019. MachineTranslation with Doubly Proceedings of the 57th AnnualMeeting of the Association for Computational Linguistics. Cheng, Lu Jiang, Wolfgang Macherey, Jacob Eisenstein. 2020. Ad-vAug: Robust Adversarial Augmentation Translation. InProceedings of 58th Annual Meeting of the Association for Computational Lin-guistics. for Computational Linguistics, Online, 59615970. 2018. Computational 3136. Explaining blue ideas sleep furiously Adversarial Examples. In 3rd International Conference LearningRepresentations, ICLR 2015, San Diego, CA, May 7-9, Conference TrackProceedings."
}