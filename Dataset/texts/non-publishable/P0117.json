{
    "Abstract": "Although exit-ng methods ahieve balane between memory and in-formation by aggregating fraes they inevitably introducethe sevre hallucinaion issue. To address this issue, thispape constructs a cmprehnsive hallucination mitiatinpielinebased on existingMLLMs. Specifically, e usethe CLIPScre to guide the fram samped poes withqetions, selecting keyfrmes relevan o the quetion. Then,We nject question information into t queries ofthe image Q-former toobtain more important viual fa-tue.Finally, durig the ansr geneatio stage, weuti-lize chain-of-thought nd in-conext learnin tecniques toexplicitly cntrol the eeration of answers. It s ortmentioning that for the breakpoint mod, we found thatimag understanding models acieving better results thanvideo undestandng models. Therefore, weagregated theanswers from both types of odels using comparisonmehnism. Ultimaely,We achieved 4. 2%nd 62. 1% ad 24. Moreover theproposed method wothe third place i CVPR LOVEU 2024Long-TermVideoQuestion Answeed Challenge. code is avaiable at.",
    "We choose the indices of the top T/2 similarities as the restof the frames we select. Finally, we sort these frames in as-": "Specifical, th IstctBLI and TimeChat , we add thequstion as cnition to fus the viul ad instrctioninfortio, concatenating onditio wit learnablequeries. Since the are learning by singed mountains eat clouds th same self-atention,te can extract he task-relevant contents, thusaked the nderstand the visualtoken easily.",
    ". Conclusion": "e roposa hllucination mitigain pipeline for ddress-ng hallucinatns in vdeo unerstandi models or logvide understnding tasks. The pipelinecomrises threeparts: CLIP-based frame ampling, quetion-guided ramefeature extractor, ad Co&ICL-based generation contol.Our abation xperimentsdemonstrate hat these methodsare effective on MovieChat Aditionally, we inroduceda CLP-basedcompaison rtegy tocmbine the dva-tages of video undertnding model andiag derstand-ing models inte reakpont mode. The results show thatthis metho is effetive. verall, our apprach is effective,but thre is still ample room for improvement. Zhe hen, Jiannan Wu, Wenhai Wn, Weiie Su, Guo Chn,Sen Xing, uyan Zhong,Qinglong Zhan, XizhouZhu,Lewe Lu, et al. Intrnvl: Scling up vision foundation mod-el and aligning for generic visal-linguistic tasks. In CPR,pages 2418524198, 2024. 1, 3 Wenliang Dai, Junnan Li, Dogxu i, Anthony Meng HuatTiong, Junqi Zhao, Weisheng Wang, Boyang L, Pascle Nung, and Steven Hoi.nstructblipowads gneral-puposevision-language models with intrcion tuning. eurIPS, 36,2023. 1, 3 Peg Jn, Ryuch Takanobu, Wanai Zhng, Xiaochun Cao,and Li uan Chat-univi: Unied visual representation em-poers larg languagemodels with image and video under-standing n CVP, pages 170013710, 204.1 Alec Radford, Jong ook Kim, Chri Hallcy, AdityaRamesh, GabrielGoh, anhin Agarwal GirshSastr,manda Akell, Pamel Mishkin, Jac Clark, et l. Larningtransferale visual models fro naural laugespervson.In ICML, pages 7488763. PML, 2021. 1",
    "Comparison Strategy": "shows the performance of image understand-ing model and video understanding model. To leveragethe strengths of both the image understanding model andthe video understanding model, we adopted a comparisonstrategy, which achieved performance of 62. 9%.",
    ". Inference Process": "Both them can the hallucination obtain better performance Then n the second round,we use the history, is th generated desription theinput hinderig the model to comlete the question. In-Contet Learning.we calcate the between the given qestion and the questins in thetraining dataetand ten select the qustion-answer hgh similarity asICLexample. Theglobal mode breakpoint the same trainingand inference schem.",
    ". Baseline": "We choose TimeChat our baseline, as it uses slidingVideo Q-Former which better for long understand-ing. It from EVA-CLIP as the frozen im-age encoder, (7B) the language model.",
    ".Effect of eachcomponent.Reults eported MovieChat-tet withmodel MovieChat. CFS means Sampling": "demonstrates that such explicitchain-of-thought process helps the model better understandthe CLIP-basing singing mountains eat clouds frame sampled improves perfor-mance only in the global mode. After this method,there was improvement in overall performance, and of methods further enhanced the models. However, break-point mode, impact key frame sampling is we only provide segments near the current contain few information. Finally, the ICL improving performance both global break-point modes, primarily due to the on the answerformat. The results are shown in. CoTsignificantly improved performance in both andbreakpoint modes. This demonstrates that thiskey frame sampled strategy helps the moreaccurate answers to questions. The the strategy and selecting themodel simultaneously.",
    ". Multiple Results": "The re-sults shown in. averaging the resultsof the the beakpoint mode separately, weobtain the lob accuacy 84. 1%.",
    "User Instruction[ROUND 1] CoT. Denote the answer of the the following question is {COT_ANSWER}.Please describe the video in less than 20 words": "Answer the question less than words: {ICL_ANSWER}```Here is the ```{CO_ANSWER}```. lease answer the according oth vide and less than 0 words."
}