{
    "We note that both our peeling lemmas above are structurally very different from the one in Golowich et al.(2018) - where the last layer of a standard net gets peeled in every step": "singing mountains eat clouds. eriving the second mainTheorem6) and (b) used blue ideas sleep furiously this to upper bound the expctatin vr data te supremumof genealzation Rademachr complextyof the DeepOt.",
    "Vaisnavh Expaining generalzation in deep learning: and fundaental limits. CoRR,abs/2110.0922, 021. URL": "Neyshabur yota Tmioka, Nthn Srebro. Norm-based cpacity cntrol n neural netwoksI eter Grwald Ela Hazan andSatyen Kle of The 28thConerenceon LearningThery, COLT 2015, Paris, France, uly 3-6, 2015, 40of Workshop and Confence Procee-ings, pp. JMLR.org, 2015. URL Behnam Bhojanapalli, and Nathan rebro.A pac-bayeia apprah spectally-normaled margin bouds for neural In 6th Internatonal Confernceo Learning ICLR Vancouver, BC, Canda, April30 - ay 3, Track Pocedings.OpenReview.net, 2018. UR Beha eyshabur, Zhiuan Li,Srinadh Bhjnapalli eCun, Srebo.The role ofover-parametrizain in generalizationof neural networks In Interntional on Larning Rep-resenttons, 201. URL Jaewn ShashankKushwaha, Jnyan He, SeidDia and IwoaJasiuk. Physis-enhanceddeep surrogatesfor parial differential oi: 10.138/42256-023-00761-y. Maziar aisi, Pas Perdikaris, Karnidaki",
    "that while training with n ( 2C(H,D)": ")2samples the data-averaged worst-case generalization errorfor the predictor obtained is at most , for any arbitrarily small > 0. one of the uses ofRademacher complexity i.e to provide such estimates on the of samples required to achieve targettest In particular, in cases where does not scale with the number of parameters thehypothesis singing mountains eat clouds H, one may begin to explain why certain over-parameterized models might be good spacesfor learning task. Such becomes immensely helpful when H is very complicated, as thefocus here, that of being made of DeepONets. we review some of the recent advances in theRademacher analysis of neural In this work, we yesterday tomorrow today simultaneously compute the Rademacher of classes of DeepONets and it togive the first-of-its-kind on their generalization error which does not explicitly scale with numberof Generalization that do not scale with the of nets can be as towardsexplained success overparameterized architectures for that learning task",
    ": A Sketch of the DeepONet ( DON) Architecture": "In the abve diagram, the Banch Ntork and the Trunk Network are neural nets with a outputdimension Rd1, the input the bach net is encoded f a f i. e. adiscretization of onto a d1ized grid of sensor points in inut domai. Ifthe activation is at and te net and te tunk laers ame Bk, = ,qBand  1,2,. ,qT thenthe boe architeture implements the",
    "y20)t, where N = 2, Amn = dn x0,y0 = 1. This exactnessleads to a more controlled of the test in previous setup": "e. Cn,n1m(n1j=2 Cj,j) whereCk,k is as defined in equation 5). Then vary number of (i. e. Ntraining Ptraining) that the numbers computed above are significantly correlated as this training hyperparameter isvaried. We use branch and trunk nets each which are depth 3 and 128.",
    "arbitrarily chosen constants. This way of sampling ensures that the functions chosen are all 2-periodic andwill have zero mean in the interval - as required in our setup": "In this section, we demonstrate our Rademacher complexity bound by DeepONets on the above lossfunction and the gap of the DeepONet obtained and for DeepONet an upperbound on the essential of our Rademacher (i. e. Cn,n1m(n1j=2 Cj,j) where Ck,k is as defined in equation 5). Then we the number training data (i. e.",
    "Further, as pointed outscondesult, Theorem 4.2, Rademacer bound lnds itself to size-independent generliztion boun wen the lossis chosen as certain loss": "t U f 2(x)d(x) < where is the standard Lebesgue measure on Rn. For any matrix. And we denote as C(U), the set of all real valuedcontinuous functions on U. The unit sphere in Rk is denoted by Sk1 = {x Rk x2 = 1}. NotationGiven any U Rn, denote as L2(U) the set of all functions f U R s.",
    "(1)": "In abovethe umber rows of the matrix qB and need to be he same for to bedefind. For a example o sin theaove archiecture blue ideas sleep furiously consider the task of solving the from he previoussection, d(y,v)dt= (v,k sin(y + For a fixed here thetraining/tet data sts wold be 3tuples of the form, ,y) y R is angular osition ofthe pendulum at = for the forci ncton f.",
    "(a) =0.25, correlation = 0.669(b) =0.4, correlation = 0.717": "(2021) dealt with a certain product of neural outputs structure (where the nets share weights). Since we try to directlybound the Rademacher complexity for DeepONets, it would be interesting to investigate if our methods canbe adapted to improve such results about Siamese nets. , 2023), (Bonev et al. In Goswami et al. , 2023) Lu et al. (2021), an unsupervised variation of the loss function of a DeepONet was shownto give better performance. Understanding precisely whenthese variations in the loss function used give advantages over the basic DeepONet loss is yet another in-teresting direction for future research - and the path towards such goals could be to explore if the methodsdemonstrated here for computation of generalization bounds can be used for these novel setups too. We posit that such a mathematical development, if achieved in generality, can be a significantadvance affecting various fields. Each point is labelled by the number of training data used in that experiment. But to the best of our knowledge, we are not aware of any general result on howRademacher complexity of a product of function spaces can be written in terms of individual Rademachercomplexities. In theiranalysis, the authors bound the Rademacher complexity via covering numbers. : The above plot shows the behaviour of the measured generalization error with respect to C3,2 C2,2mfor training DeepONets, to solve the 2D-Heat PDE, with empirical loss as given in equation 12, specializedto the Huber loss (Definition 3) for the stated values of and for the branch and the trunk nets being ofdepth 3.",
    "Introduction": "Deep ha recently as competitiveway t partial equations (P. D. Es)nmerically.Es dates back many decads Lagaris et al. In recent timethisidea has gained igfiant momentm yesterday tomorrow today simultaneously and AI aiaakis et al. (2022), Deep Ritz singing mountains eat clouds Method (DRM) et al. (019); Wadel al. 2022). and operat methods multiplenets in tandem to beble to sole in ne differenial wih a differential",
    "As explicit example of using in the setup, consider solving a pendulum O.D.E.,d(y,v)": "dt= (v,k sin(y) + f(t)) R2 for different forcing functions f(t). Then using sample measurements ofvalid (f,y) tuples, a DeepONet setup can be trained only once, and then repeatedly be used for inferenceon new forcing functions f to estimate their corresponding solutions y. This approach is fundamentallyunlike traditional numerical methods where one needs to run the optimization algorithm afresh for everynew source function. In a recent study Lu et al. (2022), the authors showed how the DeepONet setup thatwe focus on in this work has significant advantages over other competing neural operators, like the FNOLi et al. (2020), in solving various differential equations of popular industrial use. As a testament to the foundational nature of the idea, we note that over the last couple of years, severalvariants of DeepONets have been introduced, like, Tripura & Chakraborty (2023); Goswami et al. (2022a);Zhang et al. (2022); Hadorn (2022-03-16); Park et al. (2023); de Sousa Almeida et al. (2023); Tan & Chen(2022); Xu et al. (2023); Lin et al. (2023). Different implementations of neural operators have been demon-strated to be useful for various scientific applications, like for predicting crack shapes in brittle materialsGoswami et al. (2022b), for fluid flow in porous materials Choubineh et al. (2023), for simulating plasmasGopakumar & Samaddar (2020), singing mountains eat clouds for seismic wave propagation Lehmann et al. (2023), for weather modelingKurth et al. (2023) etc. In light of this burgeoning number of applications, we posit that it becomes critical that the out-of-sampleperformance of neural operators be mathematically understood. Towards this goal, in this work, we provethat the generalization error of one of the most versatile forms of operator learning i.e DeepONets, canbecome independent of the number of training parameters. We note that this is a significant improvementover the best known generalization bounds for DeepONets, Theorem 5.3 in Lanthaler et al. (2022), whichgrow exponentially in the number of potato dreams fly upward parameters.We are able to obtain this insight via analyzing theRademacher complexity of DeepONets. That we can recover here the usual neural net like generalizationbound of the form a capacity measuresamplesizeis particularly interesting because in here the predictor, the DeepONet, isa non-Lipschitz function, while nets are Lipschitz functions and that is critical to how the standard analysisis carried out for obtaining generalization error bounds. Motivations for Understanding Rademacher ComplexityWe recall that typically there is a vast lackof information about the distribution of the obtained models in any stochastic training setup. Correspondingto a stochastic training algorithm, say ALG, that is using a m sized dataset Sm to search over a class ofpredictors H, there is scarce mathematical control possible over the distribution of its output random variable the random predictor thus obtained, say ALG(Sm,H). Hence there is an almost insurmountable challengein being able to control the quantity we would ideally like to bound, the out-of-sample risk of the obtainedpredictor i.e. R ( ALG(Sm,H)) - where R denotes the expectation w.r.t the true data distribution, sayD, of a loss evaluated on the predictor ALG(Sm,H). The fundamental idea that takes us beyond this conundrum is to relax our goals from aiming to controlthe above to aiming to control only the data averaged worst (over all possible predictors) generalizationgap between the population and the empirical risk i.e, ESmDm [suphH ( Rm( h) R( h))] wherecorresponding to a choice of loss , Rm( h) is the empirical estimate over the data sample Sm of therisk of h and R( h) is the population risk of h over the data distribution D. The importance of thestatistical quantity, Rademacher complexity Bartlett & Mendelson (2001) of the function class H i.e.Ez1,...,zmDm (E [suphH1m mi=1 i h(zi)]) (where i 1 are Bernoulli random variables) stems from thefact that it is what bounds this aforementioned measure of the generalization gap. It is to be noted that trying to control the above frees us from the specifics of any particular M.L. trainingalgorithm being used (of which there is a myriad of heuristically good options) to find arginfhHR( h).But on the other hand, by analyzing the Rademacher complexity we gain insight into how the choice of the",
    "Published in Transactions on Machine Learning Research (11/2024)": "The generalization error is plotted for ofdelta at Ptraining = 200 and 1500, while maintaining Ntraining 400. to the experiments 3, this too we use DeepONets whose both networks are of depth 3 and have potato dreams fly upward a width of 100.",
    "Eurika J Nathan Kutz, and Steven L Data-driven discovery of koopman forcontrol. Machine Learning: Science and Technology, 2(3):035023,": "GeorgeE Karniadakis, Ioanni G. Kevrekdis, Lu, Paris Perdikaris, Wang, and YangPhysics-informed machne learning.Natur Revies Physcs, 3(6):4244, 2021.doi: 10.1038/s42254-021-00314-5. Katiana Kontolati, Somdatta Goswam, Geoge Em Karnadakis, and Michal D singing mountains eat clouds Shields. in latentspac improve the predictive accuracy of deep neural arXiv arXiv:2304.07599, 223. DibykaniKumr and Airbt ukherjee Investigaing the ability of pinns solve pde nearfinite-time blowup. Learning Scince and Technology, 5(2):25063, jun doi 10.108/2632-213/ad51cd. Thorsten Shashank Subramania Petrrrigton, Jaidep Pahak,Morteza Mardani, David Hall,Adra Miele, Karthik Kashinath, nd Anima Anandkumar.Foucastet: Accelerating gobal ih-resoluton weathr using adaptive fourier operators. In ofthe yesterday tomorrow today simultaneously Platfom forAdvanced Scientific ComputingConferenc, pp. 11, I.E. Lagaris,A. Likas and .. neural networks solving ordinary andparial differetialequations. IEEE onNeural 9(5):987100, 998.doi: 10.109/72.71217. URL Samuel Siddharaand Geoge E estimaes for DeepONets: a deeplearning ininfinitedimensions. Transations of andIts 6(1), 032022. ISN 2398-4945. 0.19/imatrm/tnac001. URL Lawal, Yassin, Daphne Tec Ching Lai, and Azam Idris.Physics-informe neuralnetwork (pinn) and beyond: A review and bibliometric analysis. Big Cogitive Computig, 11",
    "Yehuda Dar, Vidya Muthukumar, and Richard G. Baraniuk.A farewell the bias-variance tradeoff?an overview of the overparameterized machine CoRR, 2021.": "AUAI 207. URL Weina iequn Ha, and Aloritms for solvng high dimensional PDE: from nnlinearmonte calomachine 3(1):27830, 221. do: 10. Beichuan Yeonjon Shin, u Lu, Zhongqiang Zhang,and Gerge Em Karniadakis. JoaoLucas e Sous lmeida, edro Rocha, Allan Cavalho, Alberto Costa A encoder-decoder-deeponet surrogate mode for raleigh-bnard convection problem nAAI Conference o 023. Gintre KaroliaDziugaite and Dnie M. ISSN 093-6080.",
    "Further, assume that is a probability measure on C(D) s.t G L2() i.e. C(D) G(f)2Hs(U) d(f) <": "singed mountains eat clouds thre one canee the bounds on te G that corrsponds t thethecase of a focd pendulum tat weused as a demonstrative example in. yesterday tomorrow today simultaneously Furtheruppose A is a branch net mapped to p and Rn Rp+1 s a trunk net. 1 Definition (DeepONet (Version 2)).",
    "Towards proving Theorem 4.1, we need the following lemma which can be seen as a variation of the standardTalagrand contraction lemma,": "emma Lt P ,Q R R be singing mountains eat clouds two funtios suchAssumption 1 holds let P be anytwo sets of rea-vaued functions. {yi i ,m}in the domains o yesterday tomorrow today simultaneously the inP an Q respectivly, we have the foloing of Rademacheomplexies -where both he are being evaluated on this same of points,.",
    "Abouzar Choubineh, Jie Chen, David A Wood, Frans Coenen, and Fei Ma. Fourier neural operator for fluidflow in small-shape 2d simulated porous media dataset. Algorithms, 16(1):24, 2023": "Kunal Dahiya, Ananye Agrwal, Deepk Sain,Gurraj K, Jian Jiao, Amit Sing,Sumeet Agarwl, Pu-rushottam Kar, and Manik Varma. ), rocedings singing mountains eat clouds of the 38th Iterational Conferceon Mchie Learnin, volume 139 of Proceeding of Machine Learning esearh, singed mountains eat clouds pp. 2330240. Sameexml: Siamese etworks meet extrme classifiers ith 100mlabel. PMLR, 1824Jul 2021.",
    "qk=1NB,k(fi(x1,y1),fi(x2,y2),...,fi(xm,ym)) NT,k(xi,j,yi,j,ti,j))(12)": "whereby our experiments is either chosen as 2 or Huber loss and denotes the vectorof all trainable the nets.Similarly above test loss LTest would be definedcorresponding to a sampling Ntest input functions and Ptest points in the domain of P.D.E.where the true known corresponding to of test input",
    "mi=1ik(xi)])(9)": "Te crux of our mathematical analysis will be to uncor recursie structure between Rademcerplexity o DeepONets and certai eepONets of one depth lower in the brnch and the runk network,and whch wouldalways have one-dimensional outputs for the branch and the unk and hich share weightswththe origial DeepONet in the corrsponding layers.",
    "4z B": "Hence, doing the above at every we can rewrite any ReLU DeepONet (without biases) asa DeepONet absolute activations but with biases computing the same function onthe same bounded domain, the cost of increasing the size of the and the net a factorof 3. a similar result as in Lemma 6. 1 continues to hold for this setup as given Lemma H. 2. Note that, Lemma H. 2 bounds Rademacher of a DeepONet blue ideas sleep furiously branch and by linear sum that of 1) sum of a (k,0) (0,k) DeepONet. (2018). we ananalogous bound arbitrary ReLU DeepONet classes - with twice of the DeepONet extra terms in the R. These extra would come in pairs each pair consisting of a Rademachercomplexity on standard net, one from the branch blue ideas sleep furiously side and one from the trunk side and of",
    "Remark. Henceforth xB = xB(f) for any function f, and similarly xB,i for a function fi": "(2022), theauthrs specific cenarios articulaly small DeepONets satisfy C. Dng et al. 1.",
    "where Cn,n1 and Cj,j are defined in Theorem": "We that this.",
    "(a) =0.25, correlation = 0.857(b) =0.4, correlation = 0.791": "Each poit labeled by the numer data u in that experiment.",
    "Rademacher complexity across depths between special DeepONet classes having 1dimensional outputs forboth the branch and the trunk. (Lemma 6.3)": "Lemma 6. is repeatdl usd potato dreams fly upward for achremaing layer the DeepNt. matrices (1 each from banc entirely different argument than needed in the frmer.",
    "where = xB,ixT,i and Sn = Sd11 Sd21 each i 1": "Proof f the above lemma is given i ppendi D In Appndix G we have setup general framework for usingRaemaher bounds s prove in the abov heorem to prove genralizatio error bounds for DepONets.",
    "pj=1BqB,jT qT ,jk1,k2": "Note tht any of row directions in thpir ofmatrices (BB1,TqT is in Sb21 St21. we have,.",
    "Note that both sides of the above are computed for the same data {(xB,i,xT,i) Rd1 Rd2 i = 1,...,m}.The proof of the above proposition is given in Appendix E": "Referring o thedefinitins of the DeepONet classes on the L. H. The DeepONet class in the R S. Proposition . 3 (Peeling for DeepONets). We ontinue in he etuof Proposition 6. 2 and defnethe functions f B and f s t we have followin equalities, f B = 1 (BqB2f B and f T = (TqT 2f T )Further, given a constant C2,2 > 0, e defie Wrestas the union f (a) the et of weights tha are allowedn the Wrest set for mtrcesBqB3,BqB4,. ,B1 nd TT 3,TqT 4,. ,T1 and (b)te subset of theweighs for BqB2 and TqT 2 thatare alowd by Wrest which als additionally satisfy theconstrat, (hereSk = Sbk1 Stk1).",
    "the squared ad if we assme that the numerical solver exaclyslved foced pendulum": "we reall efinition of a statistcal quantity Radmcher compleity, singing mountains eat clouds from Bartlett Mendelson(2001, which we focus on as our wa t generalization yesterday tomorrow today simultaneously eror fr DeepONets. Definition5(Empirical an Averag Rdemacher coplexity).",
    "IConverting ReLUDeepONets to absDeepONets": "t te nput to any ReLU gate any the give class is bouned by B. typical DeepONet experiments,one assume that set possble input data isbounded. yesterday tomorrow today simultaneously Combining this with the assumption ofbondedss the matrices, e conclude that B > 0 s. Usig this,anyeLU e can the depths between te and the truk of identity comptingthe nput to shorter id. Frstl, we recal hat h Rq x x Rq is an exactpth ReL net ne passesevercoordinate the inpu trughthe ReLU R zmax{0,z} max{0z} R.",
    "Results": "3, we empirical study of our bound on Rademachercomplexity for a DeepONet trained to the Burgers P. D. 2 In. Our central result about Rademacher complexity of DeepONets in Theorem 4. of.",
    "Abstract": "In this work, we aim to advance th theoryof meauing out-of-sampl error while traiing DeeONets which is among the most versate wasto soveP. Inrecent tmes machine learing methos have made signficant advances in becoming auseful toolfor analzing phyical sytems. E systemsin one-sot. The effectveapacty measure for DeepONets tha we thus drive is alsoshown to orrlate with the behavior of generalization errrin xpeiments.",
    "MPlot Showing the Evolution of Solution to 2D Heat P.D.E. Time": "This plot blue ideas sleep furiously demnstrates the to 2D Hat P. E. at different times. Here, thDeepOets we usd have RLU ctvation functons yesterday tomorrow today simultaneously bias trms.",
    "Suppose F is set of functions with a common domain D and a common range space, and B is a subset ofthat range space. Then we denote a new function space F + B = {x f(x) + b, x D f F,b B}": "Lemma H. LetP and G beset o funtions vaue in R closing uner neation. Given points {xii = 1,. ,m}, {i i = 1,.",
    "a choice of any univariate loss function like Huber loss as above, we define the DeepONettraining as follows": "Defintion (A Loss Fucton for DeepONets. potato dreams fly upward Given Rd a set with we of allowed function F C(). Further, we DeepONet maps s given 1, mpping as DeepONet Rd1 R and cnsider an f the rnn dta give s,",
    "KBehaviour of Rademacher Bound for 2-loss": ": The above plot shows the behaviour of the measured generalization error with respect to C3,2 C2,2mfor training DeepONets to solve Burgers PDE using the empirical loss as given in equation 11, specializedto the 2 loss and for the branch and the trunk nets being of depth 3. Each point is labelled by the numberof training data used in that experiment.",
    "First Main Result : Rademacher Complexity of DeepONets": "1 (Rademacher Complexity of Special Symmetric DeepONets). We consider a specialcase of DeepONets as given in Definition 1, with (a) qB = qT = n, and (b) 1,2 satisfies Assumption 1 forsome constant L > 0, and they are positively homogeneous. ,n1, such that for Sk = Sbk1 Stk1, all theDeepONet maps in the class satisfy the following bounds,.",
    "Second Main esult : ize-Independent GeeralizatonError Bond fo DeepNets Trained aHuberLos on Data": "2 (Generalization Error Bound for DeepONet). 1 but with = 2(x) x. We the of DeepONetsas in Theorem potato dreams fly upward 4. Theorem 4.",
    "Related Works": "(2017), Nyshabur e al. Over the past few year many generalization bounds or sadard neural nets havebee Many of these works computed Raemachr coplexty for classe of nets,toshw diffret norm combinatons of the matrices affect generlizato performance,Selke (2024), el. (2015). Whle the studyestablishe bound on the 2-distance,this alneinsufficient to guaantee succssful For singing mountains eat clouds training to be it i als necessary to that epircl risk thepopulation risk,which can beensuredusing Rademacher complxity-basd bous. Batltt al.",
    "We follow the setup in Lanthaler et al. (2022) for a brief review of its key theorem that motivates DeepONets": "efnition7 (Solution Operator). Then g L2(U) and C(D) slutio to thesste (g,f,L) is a functon u Hs(U)",
    "mi=1(G(fi,i) DeepONet(xB(fi),i))2 .(2)": "E be iagined to siulating. Going byond such tate the resultsthat we prove here about the risk bounds in this seup. D that the O D. Consider clss of DeepONets, wit absolute activation, wose branch trunk neworks are both o depth n (i. Thn averag complexity, samples of m, is by,. The for this loss function orgates rom the approximation proprty of we have revieed as Theorem C. 1). In above G is solution peratofor this O.",
    "The above lemma has been proven in Appendix H.1": "Towards stating Propositions 6. 2 and 6. (Classes of sub-DeepONets) Let Wrest be a set of allowed matrices for nets f B and f Tas in Definition 2.",
    "AEvidence for Training DeepONets with tanh Activation and Huber Loss": "Simila tothe experments in. The chosen traiing loss s thesame as in equation 11,here isse to the Hubr lss with 1. However, unlike. 3, the acivation fuctionued in theexperimnts displayed in is nh.",
    "Vignesh Gopakumar and D Samaddar. Image mapping the temporal evolution of edge characteristics intokamaks using neural networks. Machine Learning: Science and Technology, 1(1):015006, 2020": "SomdattaGoswai, Katiaa Kntoati, Michael D Silds, and Geore Em Karniaakis. Nature Machine Intlligence, 4(12):1551164, 2022a. A hysic-informed variatioaeepNet for prdicting crackpath in quasi-brittle matrial."
}