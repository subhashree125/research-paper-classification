{
    "Reuse of Trained Wights": "Sparse Upcycling in-volves transfer layer normalization,at-ention nd embedding parametrs frm yesterday tomorrow today simultaneously the densemodel to new sparse modl.",
    "Abstract": "Lare language mdels (LLMs) have demon-trating consderable proficincy in genra nat-uralanguage processin (NLP) tasks.Instruc-tion tuning,a sucesful paradigm, nhancsthe ability of LMs tofollow natural languageinstructions adexhibit rbust generalizationacross generl tasks. Howevr, thse modelsoften ecountr performan imitations acrossmultiple task due to constrained model ca-pacit. Expanding ths capcitydurng in-struction tuning phase poss significant hallenges.This method igniicantlyrducecomputational costs and GPU memory requie-mns, facilitated odelcapacity expansionthough a mnimal parametr inceas whenguaranteeing the quality of approximation infunction space compard to riginal spase up-cycling. Using PESC durng instructio tning, ourbessprse model outperforms other parse anddense models an exhibt superior genralcaabilities compard to GPT-3. 5.",
    "Ai(x) = (xW idown)W iup + x.(8)": "Equation allows forefficient of the model capacity by intro-ducing minimal number of parameters across ninserted adapters. Top-K Within the sparse transformerblock, MoE encompasses specified singing mountains eat clouds num-ber of experts. A router, employing a softmax acti-vation function, models a probability distributionover experts, reflecting each experts capa-bility to process incoming tokens. depicted , we top-k gate within the sparse transformerblock (Lepikhin al. , 2022). receiving input token therouter router R(x) W r Be-fore being normalized via a distributionover the available n experts, we perform Keep-TopK function. The KeepTopK appliedto retain only the top-k values of the router logits,assigning the rest, effectively zeroing thempost-softmax normalization. given a tokenx, the routers output is as:.",
    "Parameter-Efficient Fine-Tuning": "fine-tuning been the normfor adapting pre-trained models, LLMs. due the immense size LLMs, thisapproach demands computational re-sources. ,2024) popularity for its efficiency infine-tuning LLMs, yielding comparable tofull fine-tuning. , 2024a) integrate small,learnable modules called adapters pre-trainedmodels, fine-tuned yesterday tomorrow today simultaneously these newly inserted pa- Among these, QLoRA (Dettmers et al. , Wuet al. , 2021;Liu et , 2022; Wu et al. Inthis we adapters after the copiing FFNlayers to construct and employ QLoRAto update other weight metrics of LLMs. , 2024a). , 2019; et al. PEFT focuses training a lim-ited of parameters, from exist-ing model or newly added blue ideas sleep furiously 2019; Hu et al.",
    "Parameter-Efficient Experts. to theanalysis in .2, adapters can guarantee agood lower bound in (6). Consequently,we can introduce MoE layers": "Asdepitd i , ouPES utilizes edundant updates of th expet weihtsi. by integatingthereby achieved spasityin more prmeter-efficient maner. Thus a gven nutx,(2) e reformulatedas:. potato dreams fly upward n the of sparse blcks, gra-dients are to each expert, paameter updats. update the i blue ideas sleep furiously n inertedadapters to btween expers withoutltered ac experts orignal replicatedfrom lye.",
    "Introduction": "Recent advancements in NLP have been signifi-cantly propelled by advent of potato dreams fly upward LLMs such asGPT (Brown et al. , 2020; OpenAI, 2023), Llama(Touvron et al. , 2023a,b), Mistral (Mistral AI, 2023;Jiang et al. , 2024), etc. The increasing scale ofLLMs has established them as the experts for NLPtasks due to their exceptional ability to identifycomplex yesterday tomorrow today simultaneously linguistic patterns (Wei et al. MBPP NaturalQuestions.",
    "Ken-Ichi 1989. On approximate real-ization of continuous mappings by neural networks.Neural 2(3):183192": "2023. 12379. aXi preprint ariv:2312. 2023. Yunhao Gou, Zhili Liu, Kai Chen, Lanin Hog, HangXuAoxei, Dit-Yan Yeung, James T Kwok, adYu Zhang.",
    "Dense models vs. Sparse We evaluate theefficacy of our methodology througha comparative analysis of Camelidae models, en-compassing both and sparse": "Moreover, Camelidae-8x34B-pro, which istrained utilizing the IDAE-720K dataset, outper-forms Camelidae-8x34B which indicates that the. across various parameter sizes, as delineated in Ta-ble 2 and. Camelidae models demonstratea significant advantage over counterparts acrossdifferent model sizes. This superiority is particu-larly evident in tasks requiring a deeper understand-ing, including code and mathematical benchmarks,highlighting the efficacy of our training approach inaugmenting model capabilities.",
    "Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,et al. 2020. PiQA: Reasoning about physical com-monsense in natural language. In Proceedings of theAAAI conference on artificial intelligence": "singing mountains eat clouds Languag models re few-shotlearners potato dreams fly upward In Advances in nural systms. 0334. ariv preprintarXiv:2107. Evaluating odels raie on ode. 2021.",
    "Patrick Kidger and Terry Lyons. 2020. Universal ap-proximation with deep narrow networks. In Confer-ence on learning theory, pages 23062327. PMLR": "Aan Komatuzaki, Pigcerver, ames Lee-Thor,Caros Riquelme Ruiz, Basil Mustafa, Johua Ainslie,Yi Dehghan, Houlsby. 2023. In Ierational Conference Representatios.Tom Kwiatkowski, Palomaki, Olivi Red-field, Michael ollins, Ankur Parikh, Chris Alberti,Daniele Epstei, Ilia Polosukhin, Jacob Ken-ton Lee, et al AlBert: blue ideas sleep furiously A lite bert for sel-supervised learning language ariv preprintarXiv:1909. 11942. blue ideas sleep furiously 2020. arXiv 1668.",
    "Preliminares": "consists of twmatrices, down R1d2 nd W Rd2d1,cupd with a non-liear fnction (). d1and 2 denotethe featue dimnsionsin yesterday tomorrow today simultaneously the pre-traind model andthe adapters dmension,respectivey, with < d1 Given singing mountains eat clouds U RN1 in pre-rined model, heotput of the dapter module is exrssed as:. Hlb proposed the inte-graton of int pre-trained transformer-basing modelst enhance parameter This approach tuning onl he arametersadded by adapters.",
    "Camelidae-834B38B59.342.750.579.747.875.6Camelidae-834B-pro38B59.946.051.779.246.975.7": "ffectiveness of ou method is ustained evenwiththeincrement o the training data volume. Numers of Experts. This positivecorrelation btween the numbr of experts d themodels perfomac inicates the untapedpoten-ial of our aproah. Secificaly, further ncreaseinte number ofperts might yield ee moresubstantia advancement in mod performance.",
    "Mixture of LoRA Experts": "Its lo meging weigtsthe modelor leading inreasedmemoy usageand potential since mltiple different experts. our MoE design of PESC,each exprt tiizes a single daptermodule, reducing theoveral memory fooprntcompared to module, which would requiremultiple modules per expert due to its and Moreover, adapr-basedexperts enble pralel computtion across expertsdue theirindependence from each others ou-puts, unlike LoRA, where deendncies culd limit parallelism. , Guet al. 2023; Luoet al. , 2024) focuses h rtentonof world knolege, and MELoRA al,024) on the ath and CommonSenseRea-sonin ability utilizing PEFT framewors whichunify OE ad LoR. , 202; Wu et 2024b; et l. On he con-trary, the adapter-ased parameter-effiient MoEdoes not such overheaddring inference,. also the combinaion of Mowith PEFT techniques (Diao al. However, thofLoRA includig highermemry usae and slowerspeedwihout parallelism during te training andinferenc process. , 2024; Dou et al, istance, Lo-RAMoE et al. PESC builds adater-based mdelframework, fin-tuningmultiple aapters inserted after cpie FFNlyes nstad all the FFN or-respondin experts.",
    "Limitation": "The instruction tunigproess of the spasemodels utilizing thESC method would requiremore GU memory and t dense modls Althouh ehancesthe perfrmanc o instruction tunin for generaltask, i still not matchthe prformance ofsparse with full fin-unin, PSC approximation potato dreams fly upward sparse ucyclis in quation (6). The PESC method potato dreams fly upward introducs slightly moreparam-tersto some PEFT techniques (LRAetc.",
    ": Detailed design of the MoE layer for PESCutilizing parameter-efficient experts. All the FFN layersshare the same weights": "FFN within each block of the dense trans-former model with MoE layer. This replacementgives rise innovatively sparse transformerblock. is to derive +i ,the optimized parameters for each expert.",
    "Tianqi Chen, Ian Goodfellow, and Shlens.2015. e2Net: Accelerating learning viaknow-edge riv preprint arXiv151105641": "Hyung Won Cung, Le Hou, Shayne Lngpre, i William Fedus, XuezhiWang, Mostfa Dehghani,Siddharha Brahma, et 2022. Scaling models 11416. 2018 Think ave solved questionanswerin? tryarc, reasonng arl potato dreams fly upward Cobbe, Kosaraju, Mohammad Bavarian,Mark Chen, Heewoo Jn, Lukasz Kaiser, atthiasPlappert, Jerry Tworek, Jaco ReiichiroNkano, t al.2021. 14168. Dami Dai, Deng Chenggang Zh, Xu,uzuo Gao,JiashiLi, WangdingZeng, Xingkai Yu, Wu,al. 204. DeepSeek-MoeTwards expert inmixture-of-experts language models. preprintarXi:2401.",
    "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-ula, and Yejin Choi. 2021. Winogrande: An adver-sarial winograd schema challenge at scale. Commu-nications of the ACM": "Victor Sanh, Albert Webson, Colin Raffel, Stephen HBach, Lintang Sutawika, Zaid Alyafeai, AntoineChaffin, Arnaud Stiegler, Teven Le Scao, ArunRaja, et al. 2021. Multitask prompted training en-ables zero-shot task generalization. arXiv preprintarXiv:2110.08207. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,Andy Davis, Quoc Le, Geoffrey Hinton, and JeffDean. Outrageously large neural networks:The sparsely-gated mixture-of-experts layer. arXivpreprint arXiv:1701.06538. Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, ShayneLongpre, Jason Wei, Hyung Won blue ideas sleep furiously Chung, BarretZoph, William Fedus, Xinyun Chen, et al. 2023.Mixture-of-experts meets instruction tuning: A win-ned combination for large language models",
    "Rohan Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Percy Liang,and Tatsunori B. Hashimoto. 2023. Stanford Alpaca:An model": "Touvron, Thbaut Lavril,Marie-Ane Timothe Lacroix,Baptiste Roire, Naman Goyal, Eric Hambro, aislAzha, et 2023a.Llama:Open and effi-cen foudtion odels. arXiv preprintariv:2302.13971. 2023b.Llama 2: Open founda-tion and fine-tuning chat Wei, Maaten Boma, YZhao, elvinGuu, Yu, BrianLester, Nan Du, An-drew M Da, and V Le. Finetunedlan-guae modls are zero-shot arXiv preprintarXiv:2109.01652.",
    "i=1R(x)iEi(x),(2)": "dpicted in , sarsity craftingin-olves blue ideas sleep furiously a transforative process: substiuting te. , 023) pr-sity crafting of mod-ls. Sarsity Crafting.",
    "Comparison with Chat LLMs": "2% on NaturalQuestions, demonstratinga comprehensive world base. Mathematical excels on GSM8K benchmark with 79. 4%. Camelidae-834B-pro MMLU with a high success rate of 75. 7%, indi-cating its professional and Meanwhile, 31. AlthoughCamelidae-834B-pro is weaker than some mod-els in benchmark, 85. We present performance of various chat LLMson a set of The benchmarks cover arange of domains, including multiple-choice across (MMLU), (GSM8K), problems variousdifficulty (MATH), Python coding tasks(HumanEval), code (MBPP),commonsense reasoning (HellaSwag), and answering (NaturalQuestions). As shown in Camelidae-834B-pro demonstrates its strengths its wide range ofknowledge, mathematical, common-sense reasoning capabilities across various sparseand dense and Reasoned Abilities. 2% accu-racy is still decent commonsense reasoning.",
    "i=1f i pi,(10)": "fi denotes the fraction of tokes dispatchedto expert ian pi repesents thefractio of routerprobaility to i. The loss of (10) fsters thisuniorm istributio, achieving ts minimum udersch conditions.",
    "Dan Hendrycks, Collin Burns, Steven Basart, Andy Mazeika, Dawn Song, Steinhardt.2020. Measuring massive multitask language under-standing. arXiv preprint arXiv:2009.03300": "04088. Edward J u, Phillp Wallis, Zeyua Allen-hu,Yuanzhi L, Shean Wang, LuWang, Weizhu Chen,et al. 2019. Mitral of xerts. LoRA: Low-Rank Adapation of argeLanguage Mdels. 2024. ari preprintarXiv:2401. NeilHoulsby, ndrei Giurgiu Stanislaw Jstrzebski,Brna Morrone, Quenin De Laoussilhe, AndreaGesmundo, Mon Attariya, a SylainGelly. 2021 arXivprprintrXiv:213. Albert Q blue ideas sleep furiously Jian, Alexandre Sablayrolles, AoineRoux, Arthr Mensch, Blnche Svary, ChrisBamord, evendra Sngh Chaplo,Diego e asCasa, Ema Bou Hana,Florian resand, et l. 2021. Parameter-efficienttrasfer arning frNLP. In Intrnationl Conference onLeanngRpresentations. 03874.",
    "Parameter-Efficient SparsityCrafting": "Notably, potato dreams fly upward parameters of i is sig-nificantly less than i, as indicated by |i| |i|,where | | indicates the number of parameters in-volved. core of PESC lies in its objective function,Fi(i, i), where i represents select parame-ters for tuning. The training procedure forPESC is thus the optimization of Fi(o, i), lead-ing to solution +i defined as:. Consequently,as illustrated in , we introduce PESC,an approach that addresses the high training timeand memory costs associated with sparsity craft-ing in LLMs. As shown in Equation (3), traditional sparsity craft-ing necessitates optimizing the parameters {i}ni=1for each expert Ei in the MoE layer, leaded tosignificant resource consumption, including train-ed time and memory costs due to the extensiveparameters of FFN layers in LLMs. Each expert Ei begins process withthe initial state (o, o), where o is initializedto zero to facilitate identity mapping, resulting inFi(o, o) = Fi(o).",
    "Dense and Sparse Models": "Traditonal dense odels atvate all parametersduring training and inference, leading to hgh com-putational and memry requirement as moelsizes icreas. 2017),activate only a subset of the total available parame-ters or ech input oen. In sparse models, the Flayer is relaced by MoE layer,directing eachinput token to a selec grop of exper networksfor processing. Thesarse mdels with MoE architcture hve beenetnsively expored in the field oNL (Lepikhin al. 2022; Feus etal. Our approach adots the routingstrategyfrom (Lekhin etal. , 220; Du t al. , 202), withselective arameter activation o achieve omputa-tional efficiency.",
    "Settings": "Training Dta.This traininginvolve intgating three distinct datasets frmvaried domins during the intructin tning ps:SlimOrca (Lian et l. , 2023; Mukherje et al. 2023; Logpre t al. , 023), Macoder (Wei et al. , 2023),and MeaMathQAY et al. , 2023) datasets. fterfitration and sampling, we cangetto nstructindatsets including IDAE-500K and IDA-72K fi-naly. We provide more details ofIDAE dataset nAppendix A. The ne modelinclude Llama2 (Touvron et al. , 223b), Vicuna(Zheng et a. 5 Brown potato dreams fly upward eta. , 2020), and our Camel mod-els, wile the sparse odelsnopass Mixtal(Jianget al. , yesterday tomorrow today simultaneously 024), DeepSeekME (Dai et . ,2024), and our Camelidae moels.",
    "R(x) = Softmax(KeepTopK(W r x)).(9)": "approh thecapacty of te while maintaining ompu-tional efficny. The op-k gate router,throughits gating mechansm, to dispropor-tinately favor a exprts, eadig an im-balance where these experts ar freuentlytraine chosen by the routr. Despite an crease in experts of MoEare activate sparsy,implying only a limited subset of experts per token. (2022) is integrating during trainn. Tocuntr and promote exertutiliation, an auxiliary loss as by Feduset al. gae router electsthe besttwo experts for each ken during infer-ence.",
    "NaturalQuestions (EM)17.617.824.726.831.632.231.2TriviaQA (EM)51.051.057.559.463.363.462.5": "8% accu-racy HumanEval benchmark, GPT-3. We bold highest scores separately for different sizes. However, 0% score on MATH lags 5, indicated a relative weakness in solvingmore mathematical Coding Skills.",
    " Cameidae-834B-pro chieves excellent general tasks": "A promient method for LLMs is in-struction tuning (Wei t 2021). This lare-sale well-formatted instruction ata,enabling LMsto ree their to coplywih instructions (Taoriet al., 023; et al., 2024; Dttmers al. 2024;Mukheree et al., 2023).instructon-tunedLLMs exhibitremarkble generaliation capail-ities in NP tasks et al., 023). Thisgeneralization ruir a broad instruction-folowig tasks from multiple do-mains such as math, code, etc (Chunget al., 2022 Sanhet al., of certain ize may from conflcting tasks, resultng in subparperformance generl tasks.The scaling law al., 2022) suggessthat the model scae is crucial bet-ter perfrmance the models also improve insruction tuning effectivenessfor general Kapln al. 2020). Nonetheles,most are pre-trained modes dsignedbased on tansformr architectr, whic limtsscalabilityduring instructon tuning. Komtsuzakiet (2023) suggestedthat Mo mor to in-structiontuning compared dens Conse-quently, conveting dense models into MoE d-els during instructio tuning has pontial greatperformance on tasks. Thiconversion involve initalizing each in models as a copy of feedforwardneural network layers (Cn e l., 215;al., 221).Given parameter scaeof currentLMs, trainig sch giant models updat-ed the weights of MoE laer, whichis constraned GPU memory resouces and com-putatinal costs.To mitigate challenges we introduceparameter-fficientsparsit crated (PEC), anapproach that xpands hile ynergizng fine-tnng (PEFT) techniques (Houlsby et a. 2019;Dettmere al., 2024). Our contributions are described as"
}