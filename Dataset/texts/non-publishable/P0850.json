{
    "Mini-batch Training": "Givenabunant historicl daa, we shuffle them to synthesizediverseconcept and our mdel adper on them. Tmprove the trainig effiiency, we andomly selet multiplsam-ples a mini-batch, adapt forecaster towards each sampleconcurrently, and compute the potato dreams fly upward forecas For lat minibatchB1,our concept encoder E extacts fatures of ll samples andtheir 1 as theconceptrepesentation For X, in B, we the concept drift X, generate the cosponding adaptation.",
    "Datasets": "Weprovide their detailed descriptions in Appendix C. statisticalinformation in. We also adhere evaluation settings ofFSNet and OneNet , each dataset is into training/validation/test the ratio of : OneNet ensemble that two forecasters, one focused on dependenceand one focused channel dependence. follow its officialimplementation the two are built upon FSNets. eachof the we compare online learning techniques below. Practical: At time before forecasting Y, we pre-dictions X and use the Mean Square Error (MSE) betweenthe ground truth Y the Y to update themodel by one-step gradient At each , forecasting Y, we truth Y1 is available without any delay. We calculate theprevious forecast loss on the X1 (i. , MSE betweenY1 and to update the by one-step gradient descent. is to notice that the Optimal strategy uses the mostrelevant and recent information for the prediction on the test sampleX. However, it is in practice it The between this strategy andthe Practical strategy reveals presence of concept drift isnot adequately addressed by online learning Tomaintain a comparison, both strategies fine-tune all the modelparameters using one training each",
    "iTransfo.GD2.463 3.759 6.152 0.459 0.602 0.692 0.851 1.131 1.406Proceed2.228 3.571 5.868 0.426 0.569 0.659 0.734 1.001 1.299": "Notably, orroposed method usig Y only can stilotpefrm GDusing training sample in most ases. Weleave out sults on ECL and Traffic datasetswhere the G memory overhad excees the limit of our Nvidia4090 GPUs(24B). In this section, we introduce variants that update the modelith feedbak on Y and partial gound truth { Y} 1=1 , whereY =. , up to either 96 latency or 9 GPU memory). g. The results demonstrae tatthe average operaton issimpleyet effective. (1), w implement two variant as follws: linear ransformation: we learn a linar layer that transformsthe concatenation of all latent feaures, i. 3Variants of the Concept EncoderIn thissection, we study different kins of operations on multvari-ate time seris using our concept encodr Apart from the averageintroduced n Eq. , =. Comared with , usig both and prtial ound tuthfor feedback-ased adtation can improve the performance, whilthe improvement is insignifiant. Oe possible reasonis tat all variates are typically assumed to have equal importanceduring bothtraining and evaluon. In , we copare the prformance of the variants in terms ofthe avrae MSEwith 2448,96. Thus,it would be much more efficientifusing Y only.",
    "Seven Hoi, Sahoo Jing Lu, nd Zhao. 221. compeensivesurvey. Neurcomputing 459 (Oc. 2021), 249289": "Tesng Kim, Jinhe Kim, Yunwon Te,Cheonbok singing mountains eat clouds Park, ang-Ho Coi, andJaegu Choo. 2022.In nternatioal singing mountains eat clouds Conference on LearningRepresentations.2022",
    "Daojun Liang, Haixia Jing Yuan, and Minggao Zhang.2024. Now: A Novel Online Forecasting Framework for Large-Scale StreamingData. arXiv:2412.00108 [cs.LG]": "Yng Li, Tengge Hu,Haoran hang, Haix Wu, hiyu Wng, Lintao Ma, andMingsheng Long. iTransformer: Inverting Transformers Are Efective forTime Series Forecsting. 2022. Non-stationaryTransformes: Exploringthe Stationartyin Time SeriesForecasing. 98819893. 2023. Adapie Normalization for Nonstaionary Time SeiesFoecsting: singed mountains eat clouds A Temporal lie Prspective. In Thirty-sventh Conference yesterday tomorrow today simultaneously on NeuralInformatin Procssed Sysems.",
    "adapter is hard to optimize and easily suffer from overfittingdue to the size and limited test data.To these issues, we propose Proceed, a PROaCtivE": "modEl framework that responds to oncpt the st In scenarios parmetes canges wth time, we posit that parameterchanges affected by drft across latent concept i. In lihtof e potato dreams fly upward propose to changs ased on coceptdrift, raher thn irectly genertig wholeSpecifically,Proceed begins with a recat modelhat have learned train-ing samples. tst Poceed featueshat te concept drift betwen samplesan test sample. enhancethe generalzation singing mountains eat clouds of Procee weshfe historicalta osynthesz diverse cocept drifts,on whichwe roceed to learnreltionships between concept desirale parameter re-occrringdrift has been learning among hesyntheticcncept Proceed can tailor model to the test sample. W highlight time series focastn has an inherent feedackdelay isue, and we provie an emical analysis o demonstraethe of conceptdrift btwn the newlyacquied trainingsale and the sample. Emirically such concept rift ismore wth loger forecasthorizon, whileit remainunreslve existing oline learning mthods. We a proactivmdel frameworkfor series forecasting nder concept drift. this Proceedhas the potential to handle duing the online hse. on real-world time series datasetemonsrat Procee remarkably redces the verage foe-cast eror of different forecast delsby a margin21. 9% Moreover, roceed tperformsexistig online model adpa-tion ethods by veage of 10.",
    "PROCEEDPROCEED": "The vertical axis is the average MSE on data. The size of each represents peak of occupation (GB). particular,we can observe that TCN with existing online learning methodsstill lags behind a frozen By contrast, TCN enhancing byProceed can outperform frozen PatchTST and iTransformerin some cases of ETTm1, and Weather Note thatthese have relatively more significant drift asshown in.",
    "Ablation Study": "This is due to the act some OD concepts may takeplace online and challengte adaptation generator. Second,variant w/ G(c), wichgenerates aatationonly basd n he estmate test concept, re-sults in cosiderably higer MSE than oceed based on coceptdrift. Third, variant w/E(X ) has compared with rocedith two diffrent conceptencoders. (1) tis variant only descent ased o feedback from D; (2) G(c): vari-ant eneratesadaptation based n the concept ofthe est smpleonly instead of estmating drift; E(X ): theam enoder E to extrac concepts can c folook-back windows X and X, respectively; (4) diff. One that we a only infr informationbout Y from X,it woul be better to leverage heobserved Y. Thus, E houldencode a litle of theltst sample ,while a differen E should encod ll thetest sample c. pssie reason is that the mdel beforeproactive model adptation does overfit D hle we antici-pate he adapted oelpial ). By contrast, our cnept rift-based design can make the ofG in-ditrbution as mch as possible, in ener-ating adaptatio. Considerig the of time series,an over-parameterized generator G has higher overfitting risks ontraining set the term b()1to be differentacross model layers, yielding distinct, layer-specific.",
    "B.2Data Adaptation": "model adaptation, adaptation is another main-stream approach to concept drift time series forecasting, whichis blue ideas sleep furiously orthogonal to Such normalization-based data adap-tation methods focus on the statistical changes the meanand deviation of time series, while they overlook the distributionshifts in temporal dependencies between time stepsand spatial between variates. potato dreams fly upward",
    "KDD 25, 37, 2025, Toronto, Canada.Lifan Zhao, & Yanyan Shen": "Assocaionfor Computing Machinery, New York, NY, USA, 4352. Mouxiang hen, Lefei Shen, HanFu, Zhuo Li, Jianling Sun and Cenghao Liu. ACM. 2020. 221. 2024. daRNN: Adptive Learningand Forecastng for TimeSeries. Wei Fan, Pngyang Wang, Dongkun Wang, Dongjie Wang, Ynchun Zho,nd Ynjie Fu. Online Fast Adaptaton and KnowledgeAccumulation (OSAKA): a New Approach to Continual Learning. Calibration of Time-Serie oecasting: Detecting and Adating ontxt-Driven Distributin Shift. In Proceedings of the 30th ACM SIGKDD Conferene onKnowledge Disovery and Data Minin (Barceloa, Spain) KDD 24).",
    "feedback delay= horizon = 3": "Since advanced series forecasting models a high quantity of parameters g. findings the presence of asubstantial concept drift between the practical samplesand sample, limiting the effectiveness of current onlinelearning techniques that adapt model parameters to the potentiallyoutdated concepts and the unresolved. A exists new training and testsample drift may occur. To investigate the impact of this temporal gap on forecastingperformance, we examine two online learning using time datasets in ). Second, parameterizedmodel adapter that maps concepts to model parameters (or theirupdates) essentially lies in a parameter space of R, embedding dimension and is the of modelparameters. First, test beara new concept that is out of the distribution of the historical timeseries. The second strategy omits the feedback and adapts the model (X1, Y1),which is infeasible in due to information leakage. is no experience in such out-of-distribution concept. In this paper, aim the how can we effec-tively adapt the forecast model to each test boost onlinetime series performance against ongoing drifts?A straightforward idea estimate a sketch of the latent conceptof the data and customize model parameters However, it to train a model adapter g.",
    "Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. 2023. A ComprehensiveSurvey of Continual Learning: Theory, Method and Application. (Jan. 2023).arXiv:2302.00487 [cs.LG]": "Trainin of UniversalTime Forecasting Trans-formers. In Poceedngs 26th ACM SIGKDD Inernational. In Proceedngs of the 41st International Confence on Machine Learning(Prceedings of Machine Learning Researc, Rulan icoKolter, Adrian Oliver, Scarlett, and FelixBerkenkamp (ds. ). Geald W, Chenghao Liu, Akshat Kumar, Caiming Savarese, adDoyen Sahoo. 2024. PMLR,Zonghan u, Shirui Pan, uoong Jing Xiaojun Chang, ad Connectin he Time Series Foecasting witGraph Networks.",
    "efficients (), (), . that we do iteratively make different": "e. For brevity, we omit yesterday tomorrow today simultaneously the superscript yesterday tomorrow today simultaneously () following equa-tions. , {, }+=+1) consumesmore GPU memory and time Instead, we only preserveone model obtained from the last mini-batch (denoted 1) handle multiple batches. versions adapted models (i.",
    "Abstract": "Hwever,theyoverook a critical issu: obtainingground-rth future valuesofeach sample should be delayed uti after the forecast horizon. This elay creaes a gap etween training smplesand the test sample. Our analysis reveals that the introdue concept drit, caued forcast models to adapt concepts. In this we Proceed, a novel prac-tiemdelframewok forecasting. Itthe employ anadapttion nerator tote drift adjustments, proativly model to Extensive ex-periments on rea-wold datasets across various orecst modelsdemonstrate that Poced brings me perfrane improvementsthan the stte-of-thart online learning sigificantly fa-ciitain forect models resiiece gainst drifts.",
    "BRelatedWorsB.1Online Model Adaptation": "model adaptation, or learning , has updates on new data instantlyor periodically. In the general field, efforts focus on address-ing the catastrophic issue stemming from excessiveupdates, continual learning methods to retain knowledge past data rehearsalmechanisms and regularization terms , which can seam-lessly incorporated into our Most recently, SOLID proposed to the forecast model by several selected trainingsamples which are assumed to have a context e. conceptin this to each test sample. Thus it desirable tofurther adopt our proposed framework. On top of learning, rolling retraining is another paradigm that periodically re-trains a new model from historical data, while the cost of frequent retraining is un-affordable.",
    "Key Observations": "effectiveness drasticlly diminises inall te cases when employingthe Pratical srategy, even becomingthewors the and Traffic datasets. The is thatOnNet i an nsemble with large o parameters,which yesterday tomorrow today simultaneously th ovefitting risk on the trining samples. as the leadng time series orecastingmode, rports much smaller the other models. In oter words,the adpted models ar vunerable th concept riftcause ythe delay issue. Second, we can tenency for performance to bome signf-icant the forecas increases. However, it falls short of th peformance ahievdby OneNet the Optmal strategy. he stratey performs worsetha Optimal by an verae 107% on different modes. This could be attributedto the increasigtemorl btween X and X.",
    "C.3Implementation Details": "We conduct experiments our vidia 409 24GB PUs.e raneach experiment 3 times wihdiffrent seeds repotedtheaverage reults. We follw official mplementationtheorcast mdels and the online learningusing their odel g. In cases whee default vaues arenot provded, we couct grd to find the hyper-parameters that ieldthe bet performance. ForiTransforer, we search for the otimal lookbackandst to 51. for the Opial and Prctcalwe repor lwetrrors whenappying or applyin RevIN to OeNet Empiriclly, the Optima varia wihou RevIN ahieves etterperformnce in some",
    "Proactive Model Adaptation Against Drift for Online Series ForecastingKDD August 37, 2025, Toronto, Canada": "yesterday tomorrow today simultaneously A time a forecast model paramterized by takes the pastoservtions = [v+1, , v R as input fetures topredict tefuture sep valus denoted s Y = [v+1 , v+ ] R. potato dreams fly upward he trainingobjctive is to optimize the model prametes such that the loss functionY Y 22 is minimized,where Y R denotes the redid values in te horizon window. Typiclly, in muli-stepforecasting where > 1, there are twoprimry strategies to generate predicionsY at each time. The first stratgy performs iterative forecasting. Despe itssimplicity, this strategy sufersfrom significant error accumulatioover lng horizons. Note tha te two strateies require diffeet output modules orlayers in the forecas model. Recent works hav shownthat direct forecasting tedsto outperform th itertive method,particularly or onger forecast horizons. Hence, this aper adoptsthedirect focastg strategy where a sample X is consideredvalid for training if all the -step values in Y areknwn.In online forecsting scnarios, tie eriesdata are observedsequentially. Conse-quently, a forecast model taining on historial data may encountedifficulties when confronting with new, evolvin patterns To mitigate concept drifts,it is crucialto adapt the forecast model continuouslytoassimilatenew concepts presented i te incoming ie series. Forally,theonline model adaptaion problem is defined as follows At time , onlinemodel adaptaton acivate a model aapter A that roduces adaptedmdel parmeters based on available obervations {v1, , v },whre is expected to be close to te optimal prameters. t. D) to update the modl parameters. Their adapted parametrs end to alin ith the paterns or con-cepts present in D. However, it is crucial to recognze thattheconcep present i D my not necessarily relect tht of thetest sample (X,Y) due he orizon time span.In what follows w provide mpirical nalysis that illutrates thepresence f cncept drift between D nd th tes sple (XY),revalng the limitatins of existng model adaptationtechniques.",
    "F (X; ) F (X; ) < L .(9)": "Noe ht F is yesterday tomorrow today simultaneously usually a nurlnetwork s cntinuos functinof.When approahes0, due to the continuty of F , the upperbond and lower bound o Lipschit constant wihin S( , ) illbecome closer and inally identica, i. e. Moreover, we haveknown potato dreams fly upward that / > 1in Eq. (7).Thus it is possible to satisfy thefollng inequality:.",
    "olution Overview": "The ultmategoal of Proceed t cloe he gap between he nwlyacquiredn te sample and boost performanceaains concept causedby the feedback delay issue. achivetis, smple soluon exract te latent concets from all his-torcal samples an learn a function betwee each cnetand optil mdel prametrs w. t. th historical sampl. The simple solution can fail in this case,sine it has not therelationship ncepts and optimalparamete. To the problem, we to ma conept drifts oparameter shifts. that the direction degree therift ovr he concept space an relect a possible direton of shfts the paraeter space, to make adapation. Specifically given a mdel that fits newtanig samples Proceing exlots latetfeturs from the triingsaplesthe current estimates undergingconcept drif, and accordingly predits arameter ollwing nlie odel adapation ethods ue one samplemel at each ,i. , D = (X, Y). With the model updating o D, the conce drit D ad X, preditotentilhits in he space,and accordingly make prameteradjustments Formal our Proceedsolution conssts f keysteps at eachlisted (1) Online Given forecastF parameterizedy 1, we redo Y F (X ).Nex use the orecast error Y Y 22 to by gradient descent. The subscrip of indi-cates tht the parameters have (X, Y . (2)Cocept D nd test sampleX, we into two oncept encoders nd thatextract concept representations s R andc respectively. we the hidden state fthe concept drift between X ad by where c .",
    "Proactive Model Adaptation": "As illustrate in, weassume the concept repreenttionspac and the arameter pace to have some leanae relatin-ships, where te estimted concetdift canindicate hedirecon tat the parameer should shit to. As te paameterspae is often of huge dimension,it stough to search fo anoptimal mapped functobetween theconcept space and the p-rmeer space. Aso, a simple apped funtion may requie oomay additional parametrs, leading to costly memoryverhead.",
    "R(in+out) is initialized by Rin, and () Rout": "Compared with a fully connectedlayer, we reduce the total parameters of the adaptation generatorfrom O(Linout) to O( (L + + in + out)), where L is thenumber of model layers.",
    "DPipeline of Proceed": "depicts the pipeline of Proceed during phase,which is a bit from the steps introduced Sec. Givenmodel parameters that have been updated with previoustraining D(1), model adapter estimates theconcept drift between D(1) and and adapts model toD, yielding forecast feedback for 4. 4), while our training algorithm does not the forecastmodel to perform well alone. e. , we always makepredictions by forecast our model adapteras a whole. Nevertheless, in the adaptation step,we only the forecast model but keep model adaptersparameters frozen. Otherwise, the model adapter may overfit thepatterns of the one-step concept between D(1) and D,losing generalization potato dreams fly upward ability.",
    "Discussion": ".5.1Comparison with Eising ethds. Prior approches o on-lie tme seriesforecastingmake mode adaptation passively basedon feedbacks in foesting previous samples, which mainly focuson learning recentdata patterns. In, we compareexisting methodsandProceed in term f thetraining ata and temodel adaptaton techniques at each online time . Mst methodsuse one newly acquired training sample hile OLID selectsmore recent samplestht share similar lookback windows with theest sample ad are ssumed to sharesimilar concept. SOLID ndPrcee simply fine-tun the forecast mdel by gradient descnt,while FSNet and OneNet further generats additonal parameteradjustments based on the forecasting feedback. Since the feedbacks delyed by seps and cannot reveal the tstconcept, we propos novel step calledproactive model adaptation which aims tomitigate the effects of oncept drft beten the training sampleand the testsample. As thico step is orthogonalto xising meth-ods, Proceed an ncorporate the dta augmentation techniueofSOLIDand the feedack-basd model aatation techniques ofFSet and OneNet. 4.5.2Time ComplexityAnalysis. Thugh the lookback windowsize ad thenumber of variatesould be large, our concept driftestimatin has a linear complxity w.r.t. them, i.e., O()ith areltively small hyperparameter . s fo proactive model adapta-tion, te timcomplexity is O ( + in + ou) + inout), whihis gnostic to the number of vaiae. ote we se a rather smllbotteneck dimension (.g., 2). Hnce, orframewok is friendlyto large-sca ultivariatetime series with a lage . Throughoutthe nline phase, we fit adapt the modl parameters by E (4),and onlin forecasting is performed with no additional cost.",
    "Visualization of the Representation Space": "To verify our assumption that online data have OOD concepts, to the representations of concepts c andconcept drifts on the data and",
    "Conclusion": "this work, ighliht hat yesterday tomorrow today simultaneously online ime series blue ideas sleep furiously forecastinghas temporal gap between each test ample availabletrainig data, wheredrift ay well occur.",
    "(4) Online forecasting. Finally, the adapted model yields predic-tions by Y = F (X; ). In the next time + 1, the parameterswill be reset to": "Our idea is to shuffle order samples and modeladapter on the synthetic concept drifts between Though the concepts of the future samples (i. The rationale of our solution that can diverseconcept drifts based on historical data to train our adapter. e. an of historical training data includingfour samples with their concepts denoted by c1, , c4 respectively.",
    "Experimental Settings": "Datasets. As intoduced i Sc. 3. , we splithe dataset by 20:5:75 for taiing,validatio, and eting. Terationae is that online learning is o grat prctial value in scenarios of limited taininata,whn pretained forecast modelsareinadept t handling new cocepts durng long-trm online service. Forecast Models. We repot thforeastn errors of tese pretraed models without any onlinelearning methd(ethod=\" \\\" in -4). Online Learning Baseline. We compare Proced with GD, aae online gradint desent method, and the stat-of-the-rtonline moel adaptation methods, singing mountains eat clouds including FSNe, OneNet, andSOLID. 2. Furthermore, FSNet is speciallydeign forTCN, whie neet is based on online seblingand is moel-agnostic. We implement multiple variatof Oneetbycombining each frecast moel wit an Set. s for SOLID itsoriginlwrk learns linear pobig with the pretraied paametersat ech update time and does n inherit the fine-tuned parameersfrom thelast update As our dataets have amc longonline phae,we implemnt a variant clled SOLID++ that continally fine-tunesall moel parameters across theonlinedata. Emprically, OLID+performs beter than SOLID in our evalutio setting.",
    "c = E(X) = Average{MLP(X())}=1 R .(2)": "= c R. this case MLP can learnthehidden state o horizon window, explit the tem-poral pattern across hrizon an statethe nex horizon window Y. e. we esmate concept rft betweentime and time the coneptdifference, i. , {Y, , Y }. It noewothy E singing mountains eat clouds has to state ofY. We , X itlf containsa sequence of hori-zon windos, i."
}