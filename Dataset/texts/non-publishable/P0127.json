{
    "Michelle Guo, Alireza Fathi, Jiajun Wu, and ThomasFunkhouser. Object-centric neural scene rendering. arXivpreprint 2020.": "Codenerf: Disentan-gling radiance fields object categories. Springer, 2016. 2 Zhaoyang Huang, Shi, Chao Zhang, Qiang Hongwei Qin, Jifeng Dai, and HongshengLi. In SIGGRAPH Con-ference Proceedings, pages 110, blue ideas sleep furiously 2023. 1, 2 Jang and Lourdes Agapito. Springer, 2, 6, Matthias Michael Matthias Niener,Christian Theobalt, and Marc volumetric non-rigid In ComputerVisionECCV 14th European Netherlands, October 11-14, 2016, Proceedings,Part VIII 14, pages 362379. 2. Shape, light, and material decomposition from images usingmonte carlo rendering and Advances in NeuralInformation Processing Systems, 35:2285622869, 2022. Nerfshop:Interactive of neuralradiance Proceedings of the ACM on ComputerGraphics Interactive Techniques, 2023. Flowformer: A transformer for optical flow. In Proceedings the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages1862318632, 2022. Jinkai Hu, Chengzhong Yu, Hongli Liu, Yan, YiqianWu, and Xiaogang Jin. 2 Kania, Kwang Moo Yi, Marek Kowalski, TomaszTrzcinski, yesterday tomorrow today simultaneously Andrea Tagliasacchi. Conerf: radiance fields.",
    "Diederik P Kingma and Jimmy Ba. A forstochastic optimization.arXiv preprint arXiv:1412.6980,2014. 5": "Zhengfei Kuang, Fujun Luan, Sai Zhixin Shu, GordonWetzstein, and Kalyan Sunkavalli. Palette-basedappearance editing of neural radiance fields. In IEEE/CVF Conference Computer Vision and Pat-tern Recognition, blue ideas sleep furiously pages 2069120700, 2023. 2 Yunzhi Lin, Thomas Muller, Jonathan Tremblay, Stephen Tyree, Alex Patricio Vela, and StanBirchfield. Parallel inversion neural radiance fields forrobust estimation. In 2023 International Confer-ence on and Automation (ICRA), pages 93779384.IEEE, 2023. 1",
    ". Implementation Details": "Forcorrespondeneatching, NeRFrenersare from a hemisphee whic has same disanceo thobject as CB. Weonly calcuate heflow near surface (urface distance< 7e5) and regar other spac as empty sice the only invertible areas. We se = 0. 1 inEq. Themarchn cubes resolutio for M A and meshdecimtion hyperprameters are set to obtain |V|500kertices. (11). decimaton using he 2000. Specifically, we sample 200 cmera posi-tions on the hemisphere, image, fially aug-ment by rotating yaw one of 7 angle:. 00 t minmize 3k iteratons. We seAdam pimizer with alearned rate f 0. (7)(9).",
    "Aljaz Bozic, Pablo Palafox, Michael Zollhofer, Angela Dai,Justus Thies, and Matthias Niener. Neural non-rigid track-ing. Advances in Neural Information Processing Systems,33:1872718737, 2020. 3": "European Cnference nCompter Vision,pages 2036. 2,8 Daniel and ndrew potato dreams fly upward Suprpint: iterest point escripton. prepintariv:2307. Hongkai Zixin Lo,Lei Zhou, Tian, Ming-min Zhe, Tian Fang, David Mckinon, Yanghai Tsin, Deector-free image matchingwith adaptie transfrmer. Springer 022. 05663, 2023. In Prcedings of the conference oncompter singing mountains eat clouds vison and recogniton workshops, pages224236, 2018. 4, 6, 8 Matt uosh Liu, Matthew Wallingfor, HuongNgo, Osar Aditya Alan Fan, Chris-tin Laore, iram Voleti, Samir Gadre etal. Objavere-xl: universeof 10m+ 3d objets.",
    ". Conclusion": "NeRFDeformer transforms a NeRF scene us-ing only single RGBD observation of the transformedscene. uses local linear transformations on thesurface to map the original configuration transformedone. In order to learn these linear transformations intro-duce a new to dense correspondences betweena NeRF scene and a single observation. Future work blue ideas sleep furiously should include relaxed depth input, such as through leveraging prior knowledgeabout shape or scene compositions. We interestedin grounding diffusion models through scene to helpdetermine where generation should be on. Acknowledgments. Work supported part NSF 2045586, 2106825, MRI award2020-67021-32799. Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan,Zesong Hujun Bao, Guofeng Zhang, and ZhaopengCui. 1, 3, 4, 7, 8.",
    ". Robust NeRF-based Correspondence": "We podue reliablecorrespndences betwen therignal NeRFad our transorming cene wichisillstrated in a Inspiring by of ApanFormer , wefirs propose to find RGB-basedcorespondences tranormed RGB NeRF producing rnders which are fil-tered frst in pixel space. inally we lift the pixel corrspon-dences 3D and filter th ase positivs 3D space.",
    ". Experiments": "demonstrate baslines and ourmethod on113cenes, which originate from 4 dynamicbject mols from the Objaverse dataset . scenescover a wi cmplex nonrigid each of the objc we manually se-lect animatonframe s potato dreams fly upward the orginal reference and trainour NeRF via Instant-NGP default sttngs for 100k iteations using 400 images with rsolutin of2880 2880, sampled on a hemisphere abovete object. Then w one to threeransfored anima-tionframes th from theorignalanimation",
    "Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,and Zhangyang Wang. NeuralLift-360: Lifting an in-the-Wild 2D Photo to A 3D Object with 360 Views. CVPR,2022. 2": "Bangbang Yang, Yinda Yinghao Xu, Yijin HanZhou, Hujun Bao, Guofeng and Zhaopeng Cui. Learning object-compositional neural radiance scene rendering. In Proceedings the In-ternational Conference on Computer Vision, pages 1377913788, 2021. 2 Yang, Chong Bao, Junyi Zeng, Hujun Bao, YindaZhang, Zhaopeng Cui, and Zhang. blue ideas sleep furiously In European onComputer Vision, Ye, Shuo Chen, Chong Hujun Bao, Marc Polle-feys, Zhaopeng Cui, and Guofeng Zhang. Intrinsicnerf:Learning neural radiance fields editable novelview Barron, Tsung-Yi Alberto and Phillip Isola. NeRF-Supervision: Learning dense object descriptors neuralradiance",
    "to obtain the corresponding points (pA). The direction dA": "mes B is then obtained byapplyig he forward fow to all the original. (3).",
    "ASpFmultiple2D + 3D25.94.20.9240.0340.0610.0401.462.90.9030.6660.20": "Let: te riginal scene of half-opeed box, were te is a training view rom 364images) secodad thir images are NeF renders, and the lastimage is he",
    "in the transformed space (B). The mapping is formulated": "as weighte liea bleding of tansfomations i SE(3), which are anchored stinct ins. Each verex vi has an associated tranformith contains  rotation i, a origin an trans-ation ti; so that igid transfomation its inverse aregie by.",
    "Training & transformed viewsView 1View 2Mesh": "ualitative results compared our method to pror We first how in left-most colmns origial scene thetranformed view.",
    ". Introducton": "ransfoming neurlfield based on sin-gl image is imporant of roboics an example,here NeFs are usetoreresent complicated 3D cenes. scen is modiied, the robot has to iws to e-trina neNeRF. prossdicars impotant from original sene andis time onsuming. We are hence interested deveopigtol allow agiven NeRF scene to blue ideas sleep furiously betransfomed into anew scene via a singleimage (see ). w are intertd in retrieving the gometry renderingte new scen diffeentperspectes. However, most NeRF eting metods.",
    "arXiv:2406.10543v1 [cs.CV] 15 Jun 2024": "2) We present a novel robust NeRF-based correspondencematching procedure between the original NeRF scene andthe transformed observation. This definition is more flexible than the MLP-based flowused by prior work as we can express an ap-proximate inverse flow. Concretely,the flow is a weighted linear blending of rigid transforma-tions through 3D anchor points on the surface of the scene. As the flow definition leveragesanchor points from the original scene to the transformedscene, we design a novel robust NeRF-based correspon-dence matching between the NeRF scene and the RGBDobservation. Our contributions are summarized as follows: 1) We ex-plore how a 3D scene flow can be built from 3D corre-spondences to transform a given NeRF to a novel scene,for which there is only single RGBD image observation. More specifically we show that adding depth information toSINE is not enough to retrieve more complicated sceneswith non-rigid transformations.",
    "Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,Lizhuang Ma, and Dong Chen. Make-It-3D: High-Fidelity3D Creation from A Single Image with Diffusion Prior.ArXiv, 2023. 2": "Tang,BalakumarSudaralingam,JonathanTremblay, Bowen Wen, e StephenTree,Charleso, AlexandeSching, and Birchfeld. I IEEInternationl onRootis and Automation (ICRA), 17781785.1 Zachar Ted Ja Raft: Recurrent all-pairs fedtransorms for optcal flow.In Cuter VisionECCV2020: 6h Conference Glasgow K, 2328,2020, Procedings, Part I 16, singing mountains eat clouds pages 40419. Springer,2020",
    "LDG = LARAP + LCon(11)": "Theas-rigid-as-possible (ARAP) loss LAAP regularizes bothtransformation components, while the consistency termConocuses on learned the translation tems through 3Dcorespondences. TheARAloss is applied on a decimated mesh for ef-ficient coputation. In pracice when th tranformationis invoked, the parametric funtions Ri and ti are com-pued via weightedcomination of earnable rotation ma-trices andtranslation vectors defined onthe vertices of thedcimated mesh. We efer the readeto the suplementlmaterial for more details aout this loss term.The consitency ossLCon constrains the translations ofthe ertices for which corresponding pints exist In or-der to ground the transformatio, we first idntfy a set ofcrresonded points between scenes A and B. Let set Idenote he verex indices for which correspondnces ex-ist.Thus we have the folowing set of corresponding points{(vAi , vBi ) : i{1,. , |I|}}. The process of selectigthese points is descried in the followin setion, with theconsisteny os defined as follows:.",
    "Ablations": "Note that the results ow 3-1 b. 2 correspond to thedesign choces for our implementaion of SINE . ro 4 sows design choices of ur final method.Corrspondece firs t of as a pixelcorepondencemaching and copare FlowFormer sed forSINE .This is expected as ASpanFormer is a dataset large displacements FlowFormeris trained oimage from adjacent frames i videos,i.e., thedsplacements re smaller. Although thecorrespon-dencs strongr t is sill important to expoit correspon-encefrom multiple viewsanfilter positives further.Singeultiple views for Inwepresenting methd tht leverages NeRF to render multipleviws, ad as such here we evaluate the mact f objectovrage (single or views). Our(row 4in Tab. 2) with the in 1-2, whichonly uss corrspondeces the transformd imagead aorigial image whreas both are rendered fromhe same camera pose. visuales or scene using asingloriinal mage and when sin original im-ages. scne flow s with the MLPcyclic flow for ne viewsythesis using i . 2 row -1 with or Tab. 2row withrow 3-2, wethat ML design per-formance inall metrics. Rplacin represntation method (row MLP (row leads toa decrease in perforanc.Please ot tat we run experiments forvisual etricsas e obseve that sed an MLP in that not haveoverage xtremey quality th output. See fr qualittive results.Depth quality We test to the transformedview i SiKinect . Weeplore differnt lvels",
    "V = 1, . . . , |V |}.(10)": "Note singing mountains eat clouds from (5) k(vk) = vk tk. As mentioning n ec. our foward flow andbakwardflow has two oer ML-asing ued in work : 1) te backwadflowcan be from frward low without anytrainin, nd 2) fowrdnd are cyclic olynear the surface all inear transformaions aresimilar",
    "Output": "Exported mesh ofthe transformed scene. Problem definition.Given a NeRF of the orinalscene,and a single RGB iage of he transforming scee, we are in-tersted in produng novel viws and exporting a msh f thistransformed scene. do not ofer an atomatic mchanism o match trans-formed scene and hus require o manually define tetrans-om (which can e nontriil fr non-rigid tranfoma-tion).n our problem settn,uerinput is not availabl.Othrsuccessfl works have ooked at NeRF trnformatinthrugh ime , where time compoen isdensey sapled. In cotast, w only assume singlRGBD view of th tranformed sene. Although tis sngle observation alone (without access to the original NeRFscee)ca be useto directly retrieve thetransformd sceneia retrained methodssuch as reamGaussian , oureperiments show that his approach truggles toreoverthe real geometry of the object.Transfrmin NeRF using a single RGBD nroducescalenges: wha is the nn-rigid trasfomation beed ob-serve? what object part corspon to ech othe? howdid theunseen part (t visible n the RGD image) de-form? Inspred y mesh shae manipulation , we ro-",
    "M B = ({F AB(v) : v V}, E),(4)": "result, the of our method is aimed at recov-ering these scene flows. yesterday tomorrow today simultaneously. 3). 3. In the yesterday tomorrow today simultaneously following first detail the3D scene flow is defined with local linear transformationsand its parameters (see Sec. 3. where we topology by reusing the faces E. 3. 1). Thus, it apparent that the two 3D scene flows play anintegral role in process novel rendering, as wellas in recovered geometry the transformedscene."
}