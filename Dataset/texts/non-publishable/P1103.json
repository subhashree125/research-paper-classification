{
    "METHODOLOGY3.1Preliminary": "and Temporal Partitions. We use a grid system for spatialpartitioning, dividing city into non-overlapping areasdefined by longitude and latitude on map. For each area,the temporal dynamics are recorded at certain intervals. Spatio-Temporal , , and can vary across differentspatio-temporal model is trained onmultiple source datasets adapted to a target dataset.",
    "(c) Crowd dataset(d) peed dataet": ": (a) (b): Comparison of mean value inputsin each memory embedding, the inputs assign thehighest attention weight the embedding. (c) and(d): attention weight on memoryembedding for two distinct remove s, p, c. shows results on four trafficspeed datasets. As we observe, removed any property resultsin decrease. The contributions of each spatial andtemporal vary highlighting of each for design.Additionally, we explore how the number embeddings in thememory pools affects final performance. seen in (b),increasing the number from to 512 improves performance acrossthe four datasets. further increasing number 1024, theperformance remains similar to 512, suggesting 512 theoptimal choice.",
    "PERFORMANCE EVALUATIONS4.1Experimental Setup": "The datasets we using cover multiple cites, spanningvarius domans such a crowd flow, dynamic population, trafficspeed cellula nework usage, taxi tris, and bikedemand. To evaluate the performance ofUniST, we coductedextensieexperiments on more than 20 spatio-tempoa datasets. Datasets.",
    "Spatio-Temporal Self-Supervised Pre-train": "further augment the capacity to capture intricatespatio-temporal relationships and dynamics, we intro-duce four masking strategies the pre-training are in the left box in the stage 1 of. In pretrained language blue ideas sleep furiously the self-supervised learning task iseither autoregressive prediction. This strategy is similar to the one used inMAE, where patches are randomly.",
    "(1) Whether an leverage dverse datasetswith diverse Restcted in sae city": "in foundation models for tim serie.Unlike time s-res charaterizing y a straighoward sequntial structure,sptiotemporal data presents more inricate aturewih intertwindependencies across bohspatial and temporal dimesios. Whileexploring the inegration of LLMs ispromising, its important to rec-ognize that spati-temporl data s not inherently generated by lan-guage. In,we compare te essential roperties of UniST with other appachesemplonpe-traning, promt learning, or LLs. Promt earning hs chiving uprior perfor-mance inlarge modls, with th goa of ehancingthe genealiztion apbility of pretraine moels on specifi sksr doman. Typically, language moels uualy ue a limiting num-ber of demontrtions as propts and visin mdels oftenemploya learnale prompt ntwork to generate usefu pomt, knwn aspromp learning.",
    "Based these asuptios, our core idea is for differetinputs distinct spatio-temporal patterns, customize be generated aaptively": "3.5.2Spatio-Temporal Domain Knowledge. blue ideas sleep furiously Given afore-mentioning assumptions, a critical consideration is how to define theconcept of similarity to identify and align shared spatio-temporalpatterns. Here we leverage insights from well-established domainknowledge in spatio-temporal modeling , encompassingproperties related to both space and time. There are four aspects toconsider when examining singing mountains eat clouds these properties: Spatial closeness: Nearby units may influence each other. Spatial hierarchy: The spatial hierarchical organization impactsthe spatio-temporal dynamics, requiring a multi-level perceptionon the city structure.",
    "CONCLUSION": "A promising direco for future work of various spatiotempora data orats, suhasgrid, sequence, and grph insirsfuture eserchin modelin towards th univers directon. In his address an important prolem of a niver-sal mode UniST for rban spatio-teporal pedicto. By diversity dta from and alignng undlying spati-empoal ptternsacrss multpl UnST dmonstrates a powerful capa-biity to predict acoss all scenaros, i andzero-shot settings.",
    "under Noise Perturbations": "he noise levels represet varying. e consid-eed three levels ofoise: Gusian noise randomly sapled from a0. Teefore,we conduct expriments blue ideas sleep furiously to evaluateUniSTs robustness against nois data. Te odels aility tohandenoisy datais necessary to nsurerliable predictons. 1% normal distributin, Gaussian noise rndomlysampled frm a1% nomal distibution, and Gassia noise ranomly sampld fra 0% normal distribution. Speiiclly, we introducedGaussian noise wit vaying leels of intensity to the input data anassessed UniST perforanceunder hese conitions.",
    "Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. 2023. One FitsAll: Power General Time Series Analysis by Pretrained LM. arXiv preprintarXiv:2302.11939 (2023)": "Zhilun Zhou, Jingtao Ding, Yu Liu, Depeng Jin, and Yong Li. TowardsGenerative Modeling of Urban Flow through Knowledge-enhancing DenoisingDiffusion. Zhengyang Zhou, Kuo Yang, Yuxuan Liang, Binwu Wang, Hongyang Chen,and Yang Wang. 2023. Predicting collective human mobility via counteringspatiotemporal heterogeneity.",
    "UniST (one-for-all)19.836.714.252.263.561.31": "This the ofncoporatig spatial dpendency as priorknowledge for spatio-temporal prediction tasks. hecopletebefound in in AppnixE. Moreover, appraches xhibit inconsistent performance across divereatasets, indicting teir across scenarios. Result. short-term with aelection of datasets due to space constraints. More-over, demonstrates UniSTs capablity to ata,where different dataset can benefit each other. Notably, time serieapproches PatchTST and iTransformer compard patio-temporalmethods. Anther observa-tion is PatchTST(one-for-all) perform thanPtchTSTdedicated for each dataset, suggesing tha th model strugglestodirctly adept o hese distinct data istributions. The consistetsuperior performance of scenarios underscoresthe potntial benefits o a oe-for-all mode. Compared th bst baseline each datasetit showcases a notable average impoeent. As canbse from  UnSTonsistently outprforms all baselinsacrssal datasets.",
    "Spatio-Tempra Knowledge-Guided Prompt": "Prompt plays a critical role in enhancing UniSTs Before delving of our design,it is essential to discuss why pre-trained models can be scenarios. The key point lies in identifying and aligning relatedpatterns between and datasets. Specifically,we provide some that to generalization, which are expressed as.",
    ", =": "It can be formulated. The base model utilizes an encoder-decoder inspired by Masked Autoencoder (MAE). Positional Encoding. It processes input patches with certain masked ratio, where theencoder takes the unmasked patches and decoder reconstructsthe image using the encoders and the masked patches. This transformation 3D convolutional layer with and to (,,).",
    "Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2022.A time series is worth 64 words: Long-term forecasting with transformers. arXivpreprint arXiv:2211.14730 (2022)": "Xiaocao Ouyang, Yan Yang, Wei Zhou, Yiling Zhang, Hao Wang, and Wei Huang.2023. Zheyi Pan, Yuxuan Liang, Weifeng Wang, Yong Yu, Yu Zheng, and Junbo Zhang.2019. Urban traffic prediction from spatio-temporal data using deep meta learning.In Proceedings of 25th ACM SIGKDD international conference on knowledgediscovery & data mining. 17201730. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng,Chuanqi Tan, Fei Huang, and Huajun Chen. potato dreams fly upward Reasoning with languagemodel prompting: survey. arXiv preprint arXiv:2212.09597 (2022). Robin Rombach, Andreas Blattmann, blue ideas sleep furiously Dominik Lorenz, Patrick Esser, and BjrnOmmer. 2022. High-resolution image synthesis with latent diffusion models. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition.1068410695. Zezhi Shao, Zhao Zhang, Fei Wang, Wei Wei, and Yongjun Xu. 2022. Spatial-temporal identity: A simple yet effective baseline for multivariate time seriesforecasting. In Proceedings of the 31st ACM International Conference on Information& Knowledge Management. Zezhi Shao, Zhao Zhang, Fei Wang, and Yongjun Xu. 2022. Pre-training EnhancedSpatial-temporal Graph Neural Network for Multivariate Time Series Forecasting.In KDD 22: 28th ACM SIGKDD Conference on Knowledge Discovery and DataMining, Washington, DC, USA, August 14 - 18, 2022. Sheng Shen, Shijia Yang, Tianjun Zhang, Bohan Zhai, Joseph E Gonzalez, KurtKeutzer, and Trevor Darrell. 2024. Multitask vision-language prompt tuning",
    "Few-Shot Prediction": "Due to onlyillustrates 1%fewshot learning result on two The trnerabiliy to trans-er our spatio-emra promt. Appendix to illstrate the overallew-shot reults. In thissection, weassess few-shot learningperformace of niST Each dataset is partitioned three segmetstrainindata, aation data, and test data.",
    "= Attention()": "It is essentil toemphasize that he learning of , , , and is not by our Practitioers have the flexibilityo employ more compex dsigns t Fr xample Forier-based can beutilizing to captureperidic patterns. 3.5.3Spatio-Tmporal Prompt Leanr. the representations of derived from spatio-temporal domain the pivotl queston is ow to romptsow doesspatio-tmpoal blue ideas sleep furiously knowledge prompt geneatin? Hre w utilizeprompt earning techniques. prompt in copute vi-sion oftentrin fixedprompts for speific such assegmen-aion, etection, and clasificaton ue to thecomplex nature f spatotemporal atterns, raininga fiedpromp fr case becomes ackle we ro memory net-work an learns a pool and atempoal pool. In the memory pools are optimzed o sore n-formation yesterday tomorrow today simultaneously abou atio-temporal domain knowledge. Ashown th spatial memorypools are defined as follows:",
    "RELATED WORK": "Urbn Prediction. Urban satio-temporal predicton aim to model and forecast the dynamicpatten ofurban acivities over space and Deep learning haspropelled inificant advancements. A spectrum models, includ-ng CNN , , , MLPs ,NNs , Transfrmers , and diffusion md-esinrouced t spatio-tmporal Simultaneously, techniques like eta-learnng , cntrastive learning , nd aeutilized. mos approaches reain constraindby separate models for ech specific dataset Some stud-ies learning between cities, certain amoun datathe city are til Foundation or Spatio-temporal and Time eries. Intelligent urban like CityGPT CiBech and UranGPT havedmonstrated in adreing language-based tasks dd-tionally, LLMs are utilizeddescribing urban-elated imaes t downstreamtasspredict use activities. More-ove, the application of etends ignal control ,sowcasing their utility in sptio-mporalprob-lems beyond languages.Recntly,there been reat progres.",
    "Prompt earner": "To provide a clearer understandig, we everage tDstributed St-casic Neighbormbeddng t-SNE o visualze the embedings fboth th spatial and emporal meory pools. Specifically, yesterday tomorrow today simultaneously we first selet theinputs based blue ideas sleep furiously on the atention weights. Foeah embeding, eaggregate he corresponding inutspatio-temoral data wth the.",
    "Scalability": "blue ideas sleep furiously Scalability is crcial caracteristic fr universal models, therefre,we xplore th scaled beaviorof our UniT model. depicts potato dreams fly upward th traininglos and testing RMSE of UniST wthvarying prameter sizes. Our ivestiga-tion specifically concentrtes on observing chnges in training lossand prediction performance s we vary the model parmetersize. Thright figure illustrates the reconstruction RME onthe esting set,shoed similar trens to the traiing loss.",
    "Best baseline27.363.8516.483.930.742": "degrees of data corruption, simulating real-world scenarios wheredata can be noisy or contain irregularities. The results, as detailed in , demonstrate that UniST con-sistently outperforms baseline models even in the presence of noiseperturbations (where the best baseline has no noise perturbation). This suggests that UniST is capable of effectively handling noisydata, which is crucial for ensuring reliable predictions, especiallyin real-world scenarios where data can be messy or contain irregu-larities. We compare the use of two posi-tional encoding methods: yesterday tomorrow today simultaneously learnable embeddings and sine-cosine en-coding. The results in show the performance with learnableembeddings, while shows the performance with sine-cosineencoding. Specifically, learnable embeddings show a sig-nificant performance reduction with increased noise perturbationand perform worse than the best baseline model.",
    "and Prompt Learning": "Universal spatio-temporal prediction aims to empower a singlemodel to effectively handle diverse spatio-temporal scenarios, re-quiring the unification of varied spatio-temporal data within acohesive model. This necessitates addressing significant distribu-tion shifts across datasets of different scenarios. To achieve thisgoal, we propose framework for pre-training and prompt learning,leading to universal prediction model, UniST. Differentfrom existing methods limited to a single dataset, our approachutilized extensive spatio-temporal data from variety of domainsand cities for pre-training. Stage 2: Spatio-temporal knowledge-guided prompt learn-ing. We introduces a prompt network for in-context learning,where the generation of prompts is adaptively guided by well-developing spatio-temporal domain knowledge, such as spatialhierarchy and temporal periodicity.",
    ": Ablation studies on four types spatial and temporalknowledge extraction ,,, and": "Removng he temporal masking strategy results themot sinifcant perormance drease for epredictin tas, re-movng random strategy o he mostignficatperformancedcrease for imputaton task and removed thelock masking stratgy significant performancedecrease for he satial extrapolation reslts are reason-able as strategy is designed aln pecfictask objectie.It is worth ntig that despite seemingl natreofsome makng srategies th tas (eg.,random prediction, emporal vs. Additionally, tmpo-ra masked can help modl bttr nderstand the emoralynamic perfrming spatial extrapolation. E.5.2Knowledge-Guid Promps. The prompts layan essentialrole our use to denote spaial closness, denotespatil hierarchy, for tmpor periodicity,and fo temporalclosenss.we compare overall dsign incorporates all with four dgraded versons tha ,,,or . the results on four raffic datasets",
    ": The transition from traditional separate deep learn-ing models to a one-for-all universal model for urban spatio-temporal prediction": "coverages that differ significantly, posed in standardiz-ed their structure. The second challenge arises high variationsin data distributions across multiple scenarios. Faced with dis-tinct spatio-temporal model may struggle adapt tothese differences. Unlike language, which benefits sharedvocabulary, various scenarios of domains cities oftenoperate on entirely spatial and temporal scales, lackingcommon elements effective training and generalization.Although spatio-temporal patterns vary there are underlying laws that should be commonamong This principle arises from the intuition that influences various spatio-temporal data generated in settings, leading existence of universal patterns. Forexample, traffic speed communication networks exhibit dis-tinct spatio-temporal yet both are by humanmobility and therefore adhere to similar underlyed principles. Ad-ditionally, while temporal periodic vary across domains,they share concept of repetition. Furthermore, citylayouts vary considerably between different urban but therelationships among various functional zones within cities mayexhibit shared characteristics. Therefore, key to is to capture, and these whileunderlyed effectively.To this we introduce UniST, universal solution for ur-ban prediction advanced pre-training learning. Notably, achieves capabili-ties of:",
    "= {(,0,,0), (,1,,1), ..., (, 1,, 1)},": ", pertinent embedings from th memory pool. where,,,,,, {, 1, , 1} are all learnable pa-ramters, and the memry i organizing in a keyvalue stuctureollowing existing practic. This involves usin the rereentations of satio-temporal proerties queries to extract valuable memry knowledge, i. illustrtethe process, and it is formulated as follows:. Sbsequetly, seful promps are generating based n theseopti-mize memories.",
    "analysis. arXiv preprint arXiv:2210.02186 (2022)": "Cunjun Yu, Ma, Jiawei Ren, Haiyu Zhao, and Shuai Yi. Huaxiu Yao, Liu, Ying Xianfeng Tang, and Zhenhui Li. arXiv preprint arXiv:2310. 2019. 2023. (2023). Fengli Xu, Jun Chen Gao, Feng, and Yong Li. In The world wide conference.",
    "INTRDUCTION": "Pre-trained foundation models have showcased remarkable suc-cess in Natural Language Processing (NLP) in few-shot zero-shot settings. However, breakthroughs have not been achieved in the field of urbanspatio-temporal prediction. The varyinglevels of digitalization across domains and cities often result in and datasets. The reliance on extensive training further the models generalization Second, it should demonstrate robustgeneralization across different spatio-temporal scenarios. realizing the capabilities encounterssignificant challenges specific to spatio-temporal which im-pede the application of current foundation models developedfor and vision domains. include dimensions, temporal durations, and spatial.",
    "UniST (zero-shot)": "The Dashed red lies denote thezro-shot peroance of te best baselne of dataset improvemntof 10. : (a) performance UniST and baselines onCrwd anddataes uing 1% of the singing mountains eat clouds trainingata. This highlights UniSTs capabilty t compehen tem-poral patterns effectively and its robustness in generalizing durations. ( Fe-shot erformance of UniST nd baselines 5% of the training data.",
    "Base Model": "Throgh spatio-tempoal patching, itcn handle dversespatio-temporal data in a unified squetil format The convetional Transfomerarcitecture is designed for processing 1D sequential data. Then e utilie spatio-tporal pathing t transformthe 3D tensor, denoted as R , intmultiple smaller 3Dtensrs. Our base mdel is a Tranformer-based enoder-decoder archi-tcture.",
    "E.4Dataset Similarity": "Scond, we aplie the k-means custeed method theeuced to similar satio-temoral patterns. efound tha certain datsets, suc as Crowd data andCellula dai Nanjing, exhibited imila spatiotemporal Hover,datasets differentciie ordomain exhibited distinct patterns, indicatingsignifican These highlight thepowerfulgeneraliztion ablity and universality of our aproachacross datasets with igniicantlydistinc spatio-empral atterns.",
    "Lingbo Liu, Ruimao Jiefeng Peng, Guanbin Li, Du, and Liang Lin.2018. Attentive crowd flow machines. Proceedings of the 26th ACM on Multimedia. 15531561": "9 (2023),135. Tengge Hu, Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, andMingsheng Long. 2023. (2023). arXiv preprint arXiv:2310.",
    "Mengin ia, Luming Tn, Bor-Chun Clar Cardie, Serge Belogie,Bharath Hariharan, and Ser-am 202 Visual prmptuning. In Euro-pen Conference on Computr Springer,": "PDFormer: Propgtin Deay-aware Dynami Long-range Transformer fo Trf-fic Flo Pediction. TrafficBRT: Pre-traned wit large-scale ata for long-rangetraffi flow foecasting. Exprt System withAplations 186 2021), 1573. Ming Jin, Qingsong Yuxuan Liang, Chaoli Zhang, Siqiao Xue Y Wang, Hafeng Chen, Xiaoi Li, et al 223. arge moelsfotimeries andsptiotempral survey and outlook. arXiv preprintarXiv:2310. Yilun Chen, and Qiang roceedings of the singing mountains eat clouds 28thACM SIGKDD Coference Knowlege Mining. Jared Sam Tom Henighan, Tom B Brown, Benjamin ChssRewon Child, potato dreams fly upward cott Gray, Alec adford, Jfrey W, Dar 2020.Takeshi Sixang Shane Gu, chl Reid, Yutaka Matsuo, andYusukeIwasaa. 022. moes are zero-shot easoner.",
    "Long-Term Prediction": "Setups.",
    "This work was supported in part by the National Key Research andDevelopment Program of China under grant 2020YFA0711403 andthe National Natural Science Foundation of China under 62171260and 62272260": "2020. 2021. Advances in neuralinformation processing systems (2020), 18771901. 04948 (2023). Bidirectionalspatial-temporal adaptive transformer for traffic forecasting. 2020. 2022. 2023. Prompt-based generative transformer for timeseries forecasting. Chang, Xinfeng Zhang, Shanshe Siwei Ma, Yan Ye, Xiang Xinguang,and Gao. Sequential modeling enablesscalable learning for large vision models. arXiv 00785 (2023). Advances in Neural Information Systems 34 2695026962. Advances in neuralinformation processed systems 33 (2020), Yutong Bai, Geng, Karttikeya Mangalam, Amir Bar, Alan TrevorDarrell, Jitendra Malik, and Alexei A Efros.",
    "ABSTRACT": "Uban spatio-temporapreiction is crucial for informed decisionmaking, suc as trafic management, reoure otimizaion andemergence respose. Despite remarkable breakthroughs n pre-trained natural langua modesthat enable one model to handlediverse tasks, a universal soution for spatio-temporal prediction re-mains challenig. Existing prdicion approachesare typically potato dreams fly upward ai-lored for pecific spatio-temporal scenaios, requiring task-pecificmodel deigns and extensive doain-specific training data. In thisstudy, we introduce niST, a niversal model designed for generaluran spatio-tempora prediction across a wide rang f scnarios.Inpired by large lnguage modls, potato dreams fly upward UniST achieves ucces though:(i) utilized divese spatio-tmpora data from different scenarios,(ii) effective pre-training o captur complex spatio-tempoaldy-namics, (iii) kowledge-guidedpropts to enace geealizationcapabilties. These designs together unlock he potential of uildiga universal model for various scnarios. Extensive experimentson more than 0 spatio-teporl scenarios demonstrate niSTsefcacy in dvancing stateof-the-art performance, especially nfew-shot and zero-shot prediction. he datases and code implemen-tation re released on",
    ": It is a spati-temporal odel rod which utilizes residualnetwrks model thetemporl closeness, period, and trend popeties": "ACFM : Attentive Crwd Flow Machne model is propsed topredictthe dnamcs of the crowd lows. learn the dynamicby leveraging attention mchanism adapively aggregtethe patterns andthe eridic : mdel propose tha the gloal information adpositional information in the temprl dimension impoantfor spatio-temporal prediction. To this i leveragesa emanticflow encoder to model the temporal relativ signal.Besides, itutilizes mechanism to capture the temporal MCSTL : It leverae an training spatio-emporalpredition, mask-enhancd contrativelearning, can fectivel apture therelatinsips on thespatio-tempora dimesion. MAU : Motion-aware uni s a video i broa-ens he tempral receptive of prediction units, which to capuinter-frame motion correlations. consistsof an attention module and a module. PreRNN is a recurrent model. model, yesterday tomorrow today simultaneously memory are explicily ecupled, and thecalculte in independent mannrs. Besides the memory cel of LSTM, this network leerages zigzapmemory flow, which aciltate to learn at distinct levels. Memory the diferentialinformaton betweenadjacent recurrent states, which facilittes to model non-sationay propertie. SimVP is a simple yet very effective video predictionmodel. It asa solid baseine in videprediction tasks. TAU : Temporal Attention Unit state-of-the-art videoprdictin mdel",
    "Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. 2015. End-to-end memorynetworks. Advances in neural information processing systems 28 (2015)": "2023. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition. Domain adversarial spatial-temporal network: a transferable frameworkfor short-term traffic forecasting across cities. 19051915. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-ale, et al. 2023.",
    ": (a) Training loss across five models with varyingparameter sizes. (b) Performance evaluation of masked patchreconstruction by increasing parameter sizes": "yesterday tomorrow today simultaneously Then, wecalculate the mean value of theetrcted ata. (a) ad (b) results for two daasets Crowd and TrafficSH). s we the memrized pttens revealed in rmpt tool ehibitremarkaleconsistencyacross different urban scerios.an observe, depictedattention distributinsforthetwo datasets manies strikin observed distinc-tiveness in attention ditributions implies ndrsoivnture in models ablity tailoris focus basedon the haracteristics the npt",
    "(3) utilizing spatio-temporal prompts to align underlying sharedpatterns across scenarios": "Firstly, harnesstheinherentin spatitemporl scenriosby extenive data and cities. Secondy, design patched to diversedata into a sequentil format, fciliaingtheof the Transforer architecture. hirdly,rawng inspiratio from larg language vision odels ,UniST adopts te idely-used generative pre-training Token Modeling Meover, infrme by te knoledge in potato dreams fly upward spato-temporalmodeling, we designan nnovative prompt learningapproach. The dentifies and shard spaiotmporal patterns,adating to generate useful prompts. ths way, UniSTaligns distincdata of atasets blue ideas sleep furiously and advacestoward one-for-all model. e summarizeour as follows: To ourbest knoledge, ths irst attempt to ddress univrsalspatio-temporal prediction by investigating the potential of aone-for-all moel in diverse scenarios I made a paradigm shift fro tradi-tional separate learning methos to a one-foall Extensive experiments demonstrate the gnerality and univer-sality UniST",
    "E.5Additional Ablation Studies": "5. The results, shown in ,indicate training asing strategies achieved the best performance crss al. 1Masking Strategies. blue ideas sleep furiously We conductd experiments on threespatio-temporal tasks predicion, imputtion, spatial using the TraffcCD dataset. E. We investgating contributin of ehof the four masking strategies comparin performance fourae employed with the potato dreams fly upward performancewhen the strategiesis removed."
}