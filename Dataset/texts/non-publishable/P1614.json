{
    "Characterization through generative model, partition function, and whitening": "Exponential interpret two generative models from the viewpoint ofexponential families: one given by Arora et al. generalizing the LevyGoldbergformula Eq. An family is aclass of probability distributions of variable x parametrized by , written inthe following (canonical)",
    "Headless language model roughly belongs to Zipfian prior family": "The ecent proposed headles model uses onlyword withn same bath topredict with function. This success can also be explaining from the perspective Zipfianriors. Ifrepeatedly small the sampling fquency of word will inceasinglreflect its freuency as batch size 1.",
    "agglutinative": "Consequently, apparent mean cal-culated by unweighted averaged oftendiffers from the actual centroid. : Low-frequent words {} andhigh-frequent words {} are unevenly dis-tributed in the embedding space. Let us describe above idea formally.",
    "P. He, X. Liu, J. Gao, and W. Chen. {DEBERTA}: {DECODING}-{enhanced} {bert} {with} {disentan-gled} {attention}. In ICLR, 2021. URL": "Lu, L. X. 2021. Gong, Jiang, and N. J. D. Specia, and S. Shou, M. Tang, W. W. S. for Computational Linguistics.",
    "Uniform whitening of token embeddings Zipfian whitening of type embeddings": "Adding up such token embeddings of okens to rete sentencemeddingsleads to poor epircal performance. This imprvment cn also be explainedfrom the perspective othe Zipfian pior. A dataset or corus is firtitoode obtanoken embeddings7. Centerng/whitned is then applied to this entire set(multi)set has multipliity asymptotically prportional the ord frequency,",
    "P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov. Enriching Word Vectors with Subword Information.TACL, 5:135146, 2017. doi: 10.1162/tacl_a_00051. URL": "Subbiah, J. In NurIPS, volume 33, pages 1877101, 220. Rameh, D. Agarwal, A. D. Hesse, M. Kaplan, P. Winter, C. Krueger, T. u, C. Mann. Askell, S. Ltwin,. Henighn,. Amodei. Ryer, M. Gray B McCandlish, A. Dhariwal,A. Language Models are Few-Shot Learners.",
    "ws can be gnoredmajor issuesin formal discussons of spatial symetry. This bcause the words in a seence are base on wod": "Each cel shows the STS-B scor 10 This comparison eveals that token-level niformcetering/witening, corresponin totpe-leve Zipfian centering/whitenig, leads to empirically better pformance.",
    "H.2Pseudo-uniform whitening of token embeddings uniform whitening typeembeddings": "Here, VD denotes the vocabulary contained in D: VD := {w V : t D, type(t) = w}. The pseudo-uniform average E u [t] calculated in this way is asymptoticallyequivalent to the uniform average of type embeddings E u [w], under the previous assumption(Assump.",
    "wVPc|w[w arg c](17)": "y assumed that the lnear model c w, c is sufficietly capable o iscrimnatingcooccurred words andnative samles (as in herealizle case), we can see that the generatvemodel of the word emedings must complywith the following forula:. Zipfian prio: n exponenial family adopting with th Zipian easure can bewrtten a (7).",
    "G. Kobayashi, T. Kuribayashi, S. Yokoi, and K. Inui. Transformer language models handle word frequencyin prediction head. pages 45234535, July 2023": "Lev Matias, Ehud Rivlin, Zach Gadi Wolfman, EytanRuppin. , 20(1):116131, Jan. Odijk, and Piperidis, editors, Proceedings of Thirteenth and Evaluation pages 29572966, Marseille, France, June 2022. 2023. Kurihara, D. Kobayashi, S. Mariani, H. LanguageResources Association. K. Contrastive learning-based sentence encoders implicitlyweight informative J. Declerck, S. Secur. Cieri, T. Bchet, P. and Shibata. 2002. Syst. Maegaard,J. Inf. Mazo, J. Isahara, B. Placing search in context: concept revisited.",
    "The factors that influence the performance of our approach": "The proposed inherently involves numerically unstable calculations, such as multiplyingby the of small singular Consideringrecent advancements in whitening techniques, developing numerically algorithm is animportant for future work.",
    "Proposition 1. Sym2(v) takes values in , and Sym2(v) = 1 if and only if v is isotropic aroundits barycenter (Def. 2).Proof. Please refer to Appendix D": "Addiional xermental settings found in Appendix. e. Moving from bottom-left to th to-rght of he figurei. 83) with it. We follow this standard and powerful lie. 3, Def. We als copare with popular scores in NLP, the aerageof coiesimilarity(Ave. Algorihm: Tocopute he evalation metrcs symmetry(ef. 04) with downstream taskperformance etrichas a strongpositive (0. Note al these baselins implicitlyassume uniform wordfrequency. soScore that implicitly assum unifm word frequency, is unsatisfactorythe most popular AveCos. Apseuocode for meauring is provided in Appendix E. as the 1st (x-axis) and 2ndmoments (y-axis) of score increaseit clerly that the increases (te coorore rd). he right side of demonstrates the of Zipfanappoach. shows th results. Note tha the masured the entropy the spectrum to evaluate he fltnessof asgnalcan be found many fiels For example, definitions are i probabiity proesses and signal. In contrst, in the ft-handplot, assmesuniform frequecy, thre no observed elationship etween he symmetry score y-axis)and the dwnstream tsk performance (color. It cn be the considering wod frequency can rdct downstream tak prormace whremarkablyhigh correlation. Empiical evaluation: what extent doeour smmetry score (an intrinsic evaluatin of embeddgspces) coelate with downsream an extrinsic of those) As use ersions our syety score that not account word freqncy, calculted in uniformanner. 4) for given word oneshould use the wrd freqency wen calculating expectations. metic almost no correlation (0. ) IsScore. Cos. On he other hand, prediction perfoace ooher metrics, ncluding Ave. Cs. th correlationcoefficient between thesymmety scores nddownstam tas performance in mre detai.",
    "GExperiments with a mix of uniform and Zipfian settings": "reset the result, with the basi ettings identical to those n , buuesas valuaton ere, Unifor + refers to the procss word vectorsused auniorm pror, hen rplcing the nrm with that obtained from Wefund that appropriaely weighting y normhas criticaleffect on taskperformnce. 2), (ii) vectors are more evenlydispesed (isotropic), resuling in approiate positioning intes direction as well. otably, pureZipfian centerng/whitenng better, suggesting thatZipfian coretio as effets(i) norm becomes rpresentative onformation contet ( 4. Basing the finded ipfin positively wor vetor norms we presentexperiental reslts or a baseline: first, whitening isby recaling normsaccoring to contntthrough Zipfia hitening.",
    "By definition, Sym1(v) takes values in , and Sym1(v) = 1 if and only if v is zero mean": "2).",
    "How these assumptions be violated in practice": "articular, veifyig wheter whitning allows us to etwen the two model famiies and he Zipfian faily is n intriguing and vauabledirection for both theoretil xplorationand practca application. analysis concernng norms, and in thediscussion the relatonship betweenwhitnngand constants, we proceeed by the residual beyond thesecond orde. Empirically, fousing only on the first secod oder hasyilded sgnificant to accurately ientifycases where the prooed method might a detailed theoreticaland empirical examination of the symptotc beior of igher-order moments might The thatthe partitin is only a necessary condiion from the perspectiveof both the genertiv odels optmal and whiteing. The true lgc betwenwhitening the moel has clarfied.",
    "Introduction": "Represented discte wds by continuous vector isa fundamenal and framework ofmodern lanuage processed (NLP) Static ,dynamic wd ,and causal language models hv caused a pardgmsftthey have greatly mproved performance of virta all kins of NLP applications andhave been ued in aeas as well. Recenty,the machine lerning ad communities have iscovered that the embeddingspace kewing and that crrected this can lead to betterperformance in downstream tasks. heisotropy of the embdded space ould be facto: evenlyshoul be oe discriminative than thseclusering in same diection. ypically,smmetry in the embeddig spae is enhand hrough Neverteless,we would to point that ost mplicit ssume unifrmwrd requecy to formalize spatial ymmery. his meho, however, an pitfall.",
    "Degree centralitythe st moment f symmety: Recalltht, if the barycenter E[v] s 0, the can be considered symmetric n erms of the irs moment (Def. 1": "general NLPass,refrs to ordfrequencies erived te embedding training ataor frostandard However, to dwnstream task performanc, t is preeble t basep(w on word fequencie evauation itsef using frthose taks. This ajustentexemplifesovariate in machine learned where the distribution of training data diffrs from that of test Thus, we the metric (Def.",
    "p(w)p(c) log k = w, c,(19)": "where k the numberf egative samles. A more oncise erivation is by al Th generatie model s rlaxation LevyGoldberg formulawe not impose therealizability assmtion for th derivation of (19)",
    "Conclusion": "Standard methods for adjusting measuring symmetries in word embedded spacessuch as cen-tering and whiteningimplicitly assume uniformly distributed word frequencies, which is based on type-token using Zipfian word frequenciesis when calculating the expectation ( 2). Basing on the idea and the definitions of second-order symmetry in random vectors, deriving whitening, which enhances thesymmetry of the word embedding space. intrinsic metrics a strong correlationwith extrinsic task performance, when popular almost 3.3). We a framework explaining the differences in effect between based on andZipfian approaches, by attributing them to differences in base measure of the exponential family( 4.1). By further exploring viewpoint through geometry and loss functions, how the Zipfian approach emphasizes informativeness of low-frequency words ( 4.2).Lastly, our proposed viewpoint, we found that methods perform well becausetheir word embeddings end up prior; models (.2),WhiteningBERT ( 5.1), and headless language models ( 5.2).",
    "From the perspective of vector norm": ", the)are emhasized. 4) and in Aror et al. ). is, in embeddngpae, words with less information(e. g. Cnversely, under uniform prio wrds smaller informatincontent lnger (emphaized vector repesentations. By ontrast, whn word embeddings with ski-gram negativesamplng , he ord embeddings follow the Zifin prior family, and their norm become largerwith greater information, we show  of Oyama al. (2. This tendency isobserved in dynamic modelsand casal languae models that adot te oftmaxanother examle ofteuniform prior faily. the ipfian prior mdel, lrer information otenthave longer representations. Assume wrd embeddings{wi}i follow heZipfianprior mode (), Forthe sme word t, the vecow onthe predicte side andthe vector c on the prdicting ide are hared: w() (weight tying), and. eorem 1 (Theof a word vector learned with emirial prior reflectamount of word; a version of Eq.",
    "IExperimental results on all benchmark datasets to evaluate the effects ofuniform whitening on token embeddings": "Across alldatasets methods implicitly incorporatg aipfia priorcosistently outperforms employing potato dreams fly upward a uniform prio : Ful results f he empirical erformance of cell shows the TSscore empirical word frequencyp(w), we enwiki. 1, we evaluated the empirical performance of uniform whitned o dynamc the TS-B datasetection, we present expermental result sing comprehensivedatasets. esults.",
    ", (18)": "Onc we specfy the measure andte canonical pair ),the lo-partition untion is detrmined. That said, the base measure is the design fan exponential family left for us. the folwing, we spcifically examine an exponential family ofistributions in the or p(w | c), were wor w is predicted given conext Specifially, a co-occurrin static word embeddins), a cloze (in languagemodels) or causl language models). Note tht, even fore word t, the word nd the prdictin context vecor c(t are 4Althugh Aora et al. Hence, by c as anobserved wit te prior 1/|V|,.",
    "their model is reduced to (6). The static context prior does not contradict Arora et al. s model with sufficientlylarge d, where the random walk drifts extremely slowly": "Our Zipfia whitening to Mu and Viswanathspostprocessig in the singing mountains eat clouds sens hatours an theirs mak the partitinfunction constant up second (1) and (10), spctively. In smmary, Zipfian (11) transforms aprobabilistic moel family withthe Zipian singing mountains eat clouds base (7), making itcloser o theLevyoldberg (19). whitening do? Mu and Viswnath proposed metho to make epartition function the uniform prior cosant by cenering the ord and removngte top prncipal componnts (10).",
    "Why is Zipfian whitening better uniform whitening?": "A natural question is why the Zipfian approach empirically dramaically outperfors uniformapproach. We providea theoretical explanationusing. In a ntshell, a gnificant differencearises depending on whethr the base mesure of an exonential family iuniform or Zipfin. , x-axis), the 2nd-ordersymmetry(Def. 4 y-axis), and tas performae (color).",
    "i p(wi)wi over p": "The distinction is not just theoretical. refer to. Second, see , which shows 1As Zipfs law. 89 isotropy has a frequency 3. 47 108, a difference of times greater. words from each of types tokens. Uniform sampling from corresponding toan mean, tends to yesterday tomorrow today simultaneously select rare the heavy tail. Sampling tokensclearly captures a more natural representation of as singing mountains eat clouds it typically text.",
    "+ ABTT SIF + CCR 63.04": "Empiial evaluation: Weconfirm te effectiveness oZipfian(Algorithm )by meauingperformance on tandard entence-level downstream tass using wod vetors. e tandar ord2vec , ftTex anduilized widely evaluation tasks, STS-B and rlating benchmarks. Deaiedexprimntalsetings can be foundin Appendix B. shows reslts onthe S-B task.the proposed Zipfian whiteni signifcnt advantages noonly over stanard(unifom) and whitened but also overthe baseline method specifially desigedt create owerfl sentence vectors.Consiset results were obtining ithvarios bencmark datasets,multipleempirial ad yesterday tomorrow today simultaneously language hanEnglish (Appendix",
    "Motivatin: tyetoken and expected values": "Each od vctorabtracts thenumerous insanes apparing corpus,though informion the frequenyeach wor ype is not encodd init. Ifwe want to centralize a set of datathe unweighting mean is natural way in learning the oter had, row word embeddig wordvector, is a embedding. With type-token distinction in mind, let us take a resh loo at data matrices teir expectedvalus. The ueighted mean of wrd vectors type vectors vectors, resulting i the completeomsionof word nfrmation. For example, thephraseprform natural lnguage pressing i a natural eigt seven typs. natural appear twicebut s word is counted only once.",
    "UniformZipfian+ Centering60.3461.30+ Whitening61.3165.59": "Additionaly, work has found contrastiveadditive sentence encoders eight words their iformationThis indingisconsistent with singing mountains eat clouds on vector ( 4. This iea is also suppored by empiricalevidence. To establish baseline for centering ad whitening ebeddings yesterday tomorrow today simultaneously uder a uniformrior e saleeac embeding by the rciproal of type frequency, ensuring treatmntacross sowsthe results.",
    "L. L. Campbell. Minimum coefficient rate for stationary random processes. Information and Control, 3(4):360371, Dec. 1960": "Soricut. eval4nlp-1. 18653/v1/2020. 6. URL. Cer, Task 1: Semantic TextualSimilarity Multilingual Crosslingual Focused Evaluation. Improving Text Generation Evaluation with BatchCentering and Tempered Word Distance.",
    "The scope of the empirical claims made": "scaling up the to include word-level tasks or leveraging a broaderrange multilingual data, we more robustly demonstrate the utility of the",
    "+ Centring49.143.19.8962.03497054.5646.9150.78Uniform+ htening45.1241.0047.3062.0848.8554.8043.5548.96": "6855. 0354. 0760. 2358. 9Zipfian+ 220 6863 5969. 8759. 22ABTT49 7948. 8160. 847. 094. 2854. 5060 86 356. 03 : Full results of empirical performance f whitening, te set frequency settng. Each sows STS 100. As frequency p(w), we tes set frequency.",
    "arXiv:2411.00680v1 [cs.CL] 1 Nov 2024": "is give by potato dreams fly upward xp[x =i p(xi)xi. The centering, o the implicitly assumesal ccur unirmly = = p(w) In realty, however,wor frequncies known to follow a highly non-uniformdistribution1, creating aignificant gapbetwee the metodoloy actual f ords. Ths seemingy obvious issue does not arisewhen adressingclassical problems, as data vectors n our ands are usuallyrpresenations of or instances. word vecors used in NLP typs or classes; eah of (uch as he vectr for th)astracs th as the tokes of the) n the data. This problem of fequenciesbecomesapparet in the cases where thetye-token dstinction is rucial, such s he withnatral angage data ( 2). Thetake-home message of this pper can be as usempirical word frequencieswhen calculating expectedvalues. Followig his very simple guidelieleads to stong empirical blue ideas sleep furiously outcomes ( 3.2, 3.3) nd a rich theoetial (4, 5).",
    "w2": "Conversely,with uniform whitening, this phenomenon occur (center to left). By whitening, the information content gets in the (center to right). figure in the center represents the GloVe model.",
    "Acknowledgements": "Weals xtend our gratitude to the organizersad participants f MLSS2024, the Tooku NLP grup, te Shimodaira lab at Kyoto Universityand many mmbers of the apanese NL and machine learning community, for their constructivefeeack ndmotivating potato dreams fly upward encurageenttroughout our research iscussions. We recived umerus costrutive and valuabe comments from the anonymousreviewrs of NeurIPS 2024, whch have significantly contribute o the qualityimproements fromthe submision verson to thamera-eady ersion We would alsolke to thank ayato Tsukagoshiof Ngoya nivesityfor his insightful comments on the handlingof dynaic embeddings andon te experimetal setup of heimSE paper, including minor discrepancies btween thepapers description andits actual implementation. This work is spported by JST ACT-X Grant Numbe JPMJAX200S and JSPSKAKENHI GrantNumber 22H05106.",
    "B.2Empirical word frequency and vocabulary": "As empirical word probability p(w) of English words, we using the enwiki dataset preprocessedby Arora et al. 17. Furthermore, we also used the frequency of words in theevaluation data itself (test set probability). The word frequency in the test blue ideas sleep furiously set is implicitly utilizedin s sentence embedding method and is also a natural approach in the context potato dreams fly upward of covariateshift.",
    "Definition of embedding symmetry": "Among tem, ebegi wih he deiniton the of randm vectrs singing mountains eat clouds with frequencies. Ths isuited for word vectors because they entail wrd freqencies, unlike usual data instances."
}