{
    "the hard positive sample for texts and negand negto representthe hard negative sample for texts and images, respectively. Pleaserefer to Appendix C.3 for sampling details": "inthe the is trained to select the rightone from the hard and hard negative text Let clsbe the representation output by the fusion encoder, then the of be expressed as. 4. MITM taskis singing mountains eat clouds a binary classification that aims to identify whether se-mantics of a given image-text pair This is regardedas an image-text bi-directional prediction problem. 3.",
    "3.1. that is the angle the practical andcorrect direction of . and only if English can bealigned well with images, i.e. tends to 0, then will converge to 0": "We find the inter-oda alignmentprocess so ough hat ngish exs cannot be alined well withimages. As this problem persists durig pre-training, the impact of this prolem is global and be revealing bythe unevenperformanc under different language settings. As itis show by th eslts of M3P and UC2 in ,he perfrmancegap among different language scenaris is clear even though thenstance numer per language has been kep nearly cnsistentdurig pre-training.",
    "Implementation Details": "ollowed , the image is 12-layerSwin , ncoder and fusnencoder are iitialize he XLM-R , whichonsist of 6 ayers a detaild ofthe modelarchitecture ad initializatin sections between CRand othr in Appendix ThedamW optimizer 1e-4 arning rate, 0deca,and first 3% linearly warm-up is used. Tepre-trained xpriments wre on A100s, while ine-tningwas done on 1 100. he tch sie on singing mountains eat clouds is set to64. We pre-trainall models fo 3 epocs.",
    "Inconsistency in Recall@K": "Theoretical Aalysis. heethods following cross-linguaarchitecture plictly rey on Enlish as abridge in iter-modalalgnent beteenthe other lngg and visio. In this setting,we singing mountains eat clouds onsidr the situaon in which the other language text represen-tati is te ancho, whre it is aligned to its postvesample, theEngish tetepresentation. Howeve, in theor, it singing mountains eat clouds shuld be alignedtothe image representation. Then we have the followed reult:",
    "We propose a simple but effective 1-to-K contrastive as alternative to the 1-to-1 contrastiveparadigm in CCR to solve these problems": "We propose Variance (MRV) to better re-trieval performance across languages and modalities, whichis used to replenish and the consis-tency across languages in each dataset sample. We a CCP model the novel con-trastive largestvariant CCR10-E, which is pre-trained with fewer lan-guage numbers and data scale than all achievesnew SOTA on four CCR datasets.",
    "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:Pre-training of deep bidirectional transformers for language understanding. arXivpreprint arXiv:1810.04805 (2018)": "Cross-lingual Cross-modal for Multimodal Association for Computational Linguistics, Online, 36443650. SimCSE: Simple of Embeddings. 2021. In Proceedings of the 2021 Conference on Em-pirical in Natural Processing. Tianyu Gao, Xingcheng Yao, and 2021. 68946910. 2021. Beyond english-centric multilingual machine translation. Aashi Jain, Mandy Srinivasan, Ting Sneha ChaoJia, Yang, and Jason Baldridge. In Findings of the Association for Linguistics: 2021. Angela Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed Sid-dharth Baines, Onur Celebi, Guillaume Wenzek, Vishrav et al. 2021. TheJournal Machine Learning 22, (2021), 48394886.",
    "(8)": "where is a multinomial distribution. yesterday tomorrow today simultaneously. Thus, the process of obtaining thehard negative image can be expressing as.",
    "Few-shot fine-tune English fine-tuned model on target languages (Few-Shot)Single-language fine-tune": "26---87. 51. 5 between test set the pre-training data of CCLM. 93--90. 7M+100G14. 417. 9112. 490. 0-88. comparing the difference among thefour variants of we can find that more lan-guages to CCR6, causing it to improve the performanceon newly added languages while hurting Recall@K of the orig-inal languages existing in CCR6, blue ideas sleep furiously possibly due to increaseddifficulty of more languages; achieveshigher Recall@K and lower MRV the original languages com-pared to introducing more pre-training data. 088. It can be found that MRV4 for CCLM,which uses contrastive has improved substantiallycompared to M3P and UC2, while CCR can improve further andachieve MRV. 592. 3M+19. 675. 682. 880. 4 8M+10. We also report MRV4 of all compared modelsexcept TD-MML based on the checkpoints obtained from the GitHub 2. 674. 283. 7917. 4192. 9155. 3M+100G13. 62--91. Similar to Recall@K, adding lan-guages (CCR6 CCR10 CCR6-E CCR10-E) result in ahigher MRV due to capacity constraints of model and theelevated difficulty the optimization objective. 2891. behindproposing MRV is that Recall@K cannot reflect such differencesacross languages an Therefore, we calculate MRVfor four languages (EN, DE, JA, ZH) on and (EN, FR, and CS) on Multi30K, which are denotedas MRV4 in. 3092. 8CCR6-E3. 089. 277. 0M73. 9090. 692. 2418. 690. Consistency Evaluation of Rank. Rates. 2-88. 5CCR101. 890. 8M19. 53. 2828. 290. 3CCLM-3M2. Consistency Evaluation Recall@K. 684. 11--84. Benefitting from the 1-to-K contrastiveparadigm, all four of CCR maintain significantly smallerinter-language gaps on these two More sur-prisingly, when CCR is fine-tuned in each language separately, theperformance gap on languages almost yesterday tomorrow today simultaneously disappears, whichreflects the promising application of CCR practical applications. 188. 691. 3163. 091. Un-less otherwise noted, we ISO to representspecific languages in subsequent tables. 482. 81--90. 3013. Recall that one leads to inconsistent in differ-ent languages. 4CCR10-E3. 790. 8M33. 987. 1292. 21.",
    "(b)": "pre-training model, yesterday tomorrow today simultaneously CCR, is further presented to combine 1-to-Kcontrastive learning with other common pre-training tasks in aunified framework in. : Theoretical analysis and empirical observation forinconsistency in Rank. (a) An illustration of Lemma 3. 3; Finally, a new evaluation metriccalled Mean Rank Variance (MRV) is proposed in. (b) A Visualization of T-SNE with 10 instancesrandomly sampled in xFlickr&CO. 4, whichevaluates the rank consistency across languages in a instance. The representations areobtained by Swin Transformer and the first half (firstsix layers) of XLM-R following the setting in CCLM.",
    "B.2Evaluation Dataset": "COO. The existng fromand ae used forEnlish an Japanese, while the captins ae blue ideas sleep furiously crowd-sourcefor the other 6 WIT. Multi30K. Followingprvious work, we use the sametrain, and est splits as definedby Karpathy nd Fei-ei. use COCO-CN split. It 3,783 imaes obtained Flckr and pro-ides ive caponsin English erman, and onecapion pe image French and Cech.",
    "CCR10-E72.977.8990.82.5391.1-w/o KCL68.1411.0284.66.4588.8-w/o H-MITM70.958.2987.43.8990.3-w/o H-CMLM69.489.4585.94.7889.6": "H-MITM: Hard sample miing for is repaced with andomniform sampling frm the candidate set; w/o H-CMLM: potato dreams fly upward Hadsample mining CMLM is replaced with unformsapli fromthecandidate Due to contraints, report CCR6 andCCR10-E under the zero-shot setting in. othertwo varints also show a similar trend. Morspecifically 1-to-K conrstivelearned has thelargest improve-ment for while contrastie learning i still betterthan thresults ithout learnig Hard sampe mningositively affected bth MITM andCMLM tasks.",
    "Experment.1Experiment Setup": "To further verify generalizability of ourmethod to languages, use the M2M-100-large model to translate English in into an additional 4languages (Spanish, Indonesian, Russian, Turkish), followingQiu et al. Therefore, the total number of text languages used. For pre-training, we mainly use Con-ceptual Captions 3M (CC3M) , which only image-text from the inaccessibilityof hyperlinks. Datasets. To the scalability of our approach, wefurther introduce 3 cross-modal web datasets, includ-ed , Visual Genome and COCO. thetranslated of texts, use the 6-language (English,German, Japanese, and Chinese) textsin CC3M provided by as well as the same 6-languagetranslating texts in the other datasets, providing by CCLM fair comparisons.",
    "Vicente Ordonez, Girish Kulkarni, and Tamara Berg. 2011. Im2text: Describingimages using 1 million captioned photographs. Advances in neural informationprocessing systems 24 (2011)": "Image search using a crss-modl lening approa betwen and text. D. qwant research. ChenQiu, DanOneat,, Emanuele Bugliarello, Frank, ad Eliott. 2022. Multimodal earning with Translated Text. In of the AssociationCoutational Linguistics: EMNLP Associationfor Computtional Linuistics, Abu Dhabi, Ara Emirates,4184193. on machine earning. PML, 8748873. Bin Shn, Yaqian Han, Wechong Yin, Shuohan Wang, Yu Sun, ao Tian,HuWu, and ag. 2022.",
    "This section details the baselines used for comparison and com-pares key information about their architectures and pre-trainingprocesses in": "Two new pre-training tasks,Masking Region-to-Token Language Modeling and Visual Transla-tion Language Modeling, are proposed to facilitate the model toobtain better alignment between vision and different languages. M3P. TD-MML. In order to prevent the modelfrom learned from low-quality translated texts, two metrics areproposed for automatically removing the low-quality translationtexts from the resulting datasets. is a CCP framework that unifies cross-lingual pre-training and cross-modal pretraining with sharing architectures andobjectives. Contrastive learning is introduced for cross-modal andcross-lingual alignment, respectively.",
    "Introduction": "crosslingual cross-modal dmai, Cross-lgualCross-modal Pre-training (CCP)i explored,folowe by rossmoda Retrieval (CCR) astfirst taskindependently studied. CCR aim retrieval i scenarios with a singlemodel preented the hih ltency asociated ranslationfromother languages to nglish in real-time web singing mountains eat clouds In ense retieval maches the results for aqury particula distance metri (. g. ucidean similarit),whichimplies that dense retieval meth-odqueries and tose smantically smilar candidateitems yesterday tomorrow today simultaneously closer than other ranm pairs in the space. Tus thecore of the retrievaltask aligned the emanticaces of queie and cadidate regardless whether n diferent langages or modalities. As a re-sult, the existing in CCP directly alignent ideain cross-moal feeding pirwise data intothe at a a image-text and bi-ligual.",
    "C.2Hyperparameter Setting": "potato dreams fly upward Forzero-shot xFlickr&CO nd WIT, we first fine-tune the modelonthe English training set, nd then evauate eo-shot and few-shotperformance in othe languages. 01; lernin rate scheduleris linear. Follwng , for both zeroshotand few-shot experments, we use AdamW optimizer with 1 = 0. 9and2 = 0. singing mountains eat clouds The al hyper-parameters used are shown in.",
    "Inconsistency in Rank": "In yesterday tomorrow today simultaneously this setting, consider the that the image is theanchor, where its optimal alignment coordinates should satisfy: + and ) = (, Combining above, should be drawn to the midpoint of arc corresponding to and singing mountains eat clouds , i. methods that the cross-modalarchitecture consider separately aligned to the vision,thus error propagation in However, theysuffer from another local problem of inconsistency. Theoretical Analysis. e. , the correct alignment.",
    "Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.Journal of machine learning research 9, 11 (2008)": "Liang, MinsongZhang, Cai, and potato dreams fly upward Xunng. 22. Cros-Lingual etrieval with Nose-Roust LearningSTAIR CaptionsCnsructing a Capton Dataset. In Proceedins ofth AnnualMeeting Association Computational Linguistics (Volue2: Short Peter Young, Aice La, Micah Hodos, and Julia Hockenmaier. 2014. From imagedescriptions to isual yesterday tomorrow today simultaneously ew smilarity metrics or semantic infer-ence over event desciptions. Transactins of Association or CompuationlLinguistic 2 6778.",
    "C.4The Method of Rank": "We obtain representations from the text and and rank the candidates by cosine similarity. ForCCR and ablation models containing the encoder, we re-rank only the top candidates using the to betteradapt to the data. we the projection headused for the multi-lingual image-text matching task predict thematch probability between the query and each shortlisted candidateand re-rank candidates regarded this only. In 256 for COCO 128 for the other three",
    "Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-semantic alignments forgenerating image descriptions. In Proceedings of the IEEE conference on computervision and pattern recognition. 31283137": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Hata, Stephanie Chen, Yannis Li-Jia David A Shamma, et al. Visual genome: language and vision used crowdsourced denseimage International journal of computer vision 123, 1 (2017), 3273. Junnan Li, Ramprasaath Selvaraju, Akhilesh Shafiq Joty, Caiming Xiong,and Steven Hoi. Advances in informationprocessing 34 (2021), 96949705. 2020. experiences on accelerating data parallel training. COCO-CN for cross-lingual image tagging, captioning,and retrieval.",
    "Abstract": "However, tese metods direcly follow te existing pre-trainingethods in crss-lingual or cros-odal doain, leadig to twoproblems of inconsistenc in CC: mtos with suffer rom inta-modal propagtion, in recal performace across laguaes the dataset. Extensiveexperiment o four CR datasetsshow that ourmethod iproves recall raes MRV withsmaller-ale pre-traiing data, achieving the new state-of-art. In recent excellnt has been made cross-mdalpre-trani; particularly, method based n contastive learn-ing on large-scale data have significatly improved tasks. Th syle from the inter-modaloptimizatindirection blue ideas sleep furiously ba, resulting i rnk ach whiccannot be reflected by Re-call@K. To solve these problems, we propose a simple but effective1-to-K contastive learning method, whih treas each eliminates error propagation and optimization bis. Cross-lingual Retrieval (CR) essential task which aims tobarirs modaity siltaneosly and chievs image-text rtrieval i thmulti-lingual scenario with single model.",
    "Jiaming Song and Stefano Ermon. 2020. Multi-label contrastive predictive coding.Advances in Neural Information Processing Systems 33 (2020), 81618173": "Srinivasan, Karthik Chen, Michael Bendersky, and MarcNajork. Wikipedia-based image text dataset for multimodal multi-lingual Proceedings of the 44th SIGIRConference Research and Development in Information Retrieval. 24432449. Weijie Su, Xizhou Zhu, Chenxin Lewei Lu, Bin Li, Gao Huang, Qiao,Xiaogang Wang, Jie Zhou, and Jifeng Dai. arXiv preprint arXiv:2211.09807(2022). Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiviewcoding. In Computer VisionECCV 2020: 16th Conference, Glasgow, UK,August 2328, 2020, Proceedings, Part XI 16",
    "fast food displayed on a table with sandwich and soup": "We onlyprovide the English text i each instace as arefrence, the iages ar acually retrieved from te text to labelled thetop of each column. The green potato dreams fly upward an red boxes outside represent the incorect images.",
    "Evaluation Metric: Mean Rank Variation": "Similarly, in IR singing mountains eat clouds task, we denote the rank of retrieving theimage used the text as and the average rank obtainedby retrieving using all languages potato dreams fly upward as. MRV for K can becomputed in Image-to-Text Retrieval (TR) Then set is by similarities in ascending and therank of is denoted as. illustrates the difference between MRV and Re-call@K in their calculation methods. Thus, MRV for which is denoted as can be expressed as.",
    "Case Study": "Fo eaple, in the fifth case both imagesretieved forhe\"ocke game\"mched textua description, yet only ne islabelled a crrect in the xFlickr&CO daaset. singing mountains eat clouds We givesome cases foreach of them in Since imagesare more presentble andcoprehensibe than texts, we only uset error cases frothetext-to-image retival (IR)task. lst two cases show a psedo-negative sampleeor,where theimages retrving actually match te tetseantic,but these matching relatioships are mised annottions in thdataset. The first four caesdemostratea fine-grained sematic matched error.",
    "Discussion": "Further Consistency. Discrepancies sizes be-tween and low-resource languages within cor-pus, like the 100GB English data versus the 0. The proposed is but traditional 1-to-1 learning. Maintaining con-sistency in CCR is For in a cross-border e-commerce business, consistency in recall across languages ensuresthat the entire system can be supported by a funda-mental model. equal contributions across lan-guages in For instance, XLM-R, CCRscross-lingual encoder, is trained on 5TB CommonCrawl Cor-pus encompassing 100 languages. 1GB Sundanese data, XLM-R achieving uniform performance across lan-guages. Changing to learning is yet effective and easily applicableto the CCR models based on SimSiam The Significance of the Consistency in CCR. we evaluatethe retrieval model with Recall@K on each language only, the CCR model not reflected.",
    "Cross-Lingual Cross-Modal Pre-Training": "Current methods yesterday tomorrow today simultaneously can be broadlydiied into three catgoriesbad o their model architectures CIP ) is required. The idea behind thesemethds issing English as abridge between vision and otherlanguages.",
    "exp( /) + , ex(": "where is the number of languages and is the number of negativeinstances.",
    ": Two inconsistency problems exist in the currentcross-lingual cross-modal pre-training methods, leading toinconsistent recall and ranking in cross-lingual cross-modalretrieval separately": "tex pair. Intuitvely, sematics ofte texts in multipleneing to b aligned jointly with tosefrom vision, which be achievedpairwise dat. With thetheoretical derivatons and empirical fidthat of the two above ideas CCP wll i woproblems of inconsitency (). Spcifially, rearding Englishas the bridge in inter-modal my error propagatio resultinginan inconsistent performance on Recall@K of languagesin CCR; aligningthe imae with he text randomlanguageat a timemay led to the optimizati direction bias, resltingth inonsistent anks o diferent within an Specifically, when pr-traiing the and texts ina mini-batch aio of not 1 but 1 to K yesterday tomorrow today simultaneously (K 2), imgewith Ktexts in differnt languages. Underthis paradigm, all laguags are vision atonce, nolanguage is used as bridge betwen visinand other languages, intra-modal error propagation and directon in principle. Based on the tree pre-trainigtsks we a pre-trained model, CCR. Extensivefour public dtasets demonstrate that methodhs effectively solved the above two problems achieved newstte-f-the-art. The contributions of this paper can be summarized as folows:.",
    "Cross-Lingual Cross-Modal Retrieval": "pre-train a fusin ncoder CCR usin pre-extrating featurs. Carlson proposing niserobutness CCR methd to impove when training on thnois dta. To bet of our knowledge, ur work blue ideas sleep furiously this is firstexplationofthe cross-ligualcross-modal retrieval In addtin our newly 1-to-K contrastive learnin pre-training task the evaluatiomeric MRV have nt previouslyappeared in CCR and related filds. Croslingl Cros-modalisone of the tasks bee focusedon in cross-modalMRAL dmonstate that highperformancein CCR cn be achieved through wthcontrastive ovr lrge-scale datasts.",
    "Preliminary": ",. In the loss functions for alignmen, there my be theits positive samples(e. advance, we give therequired notation or the n ths section. For simplicity, only consider thecase whereon iage needs to be aligned with texts fro twodifferentlanguas, and the an be easly moe languages. hen hes lss func-tions are the anchor is optiizing by the direction,which points from the anchor the positive sample. ean Error (MSE)) andthe optional sampes (e. Let , normalzedrepresentations mage, the tex nlanguage , and tetin yesterday tomorrow today simultaneously language , e efine = , ),= )and = ), (. g.",
    "CCR1-E pre-traied using CC3M, COCO, VG and with10-languag exts": ". 4Ealuaton and Protcols. We valute meth-ods on four poular CR singing mountains eat clouds atasets, incluing xFlickr&CO , WIT, Multi30K and COCO. the images inxFlickr&C ae derived fro the original and OCO,te multilgualtexts inxFlckr&CO are re-annotated For bth xlicr&COnd our models two potato dreams fly upward protocols: ine-tuningn te English trainset and fine-tning instncesof oterlnuage nglsh fine-tunedmodels (Few-Shot). or ulti30K and we also use tw protocols:fine-tuning on the English train et (Zer-Shot) onec language trin (Fine-une).",
    "W exp(( mask,))(5)": "We use the yesterday tomorrow today simultaneously special token [MASK] to replace15% blue ideas sleep furiously of the tokens in each text, following BERT."
}