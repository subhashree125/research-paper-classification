{
    "(9)": "Regarding the weihtfactors, theprobaility of bias attribue p(b)kalso reweihtste iluece of lassifier weights. Bis-aligning samples,with high pb)krobability, will have smaler plling effctsowas th classifer weight wk, whch avoids the oinace of bia-aligned eatures around te weight centersand hiners the tendency ofshortcut learning.",
    "arXiv:2405.05587v1 [cs.CV] 9 May 2024": "ad bias attributes, whlethe carce bias-cnflicted sam-es have no such correlion. Once a model relieson thesimple but spurous shortcut of bias attribtes for predic-tion, it will gore the intrinsic eations nd strugge togeeralize o out-of-distribution test samples. poten-ial impact ofbiasing clsifiationmayrange from polti-cal and eonomic disparitie to social inequalities within AIsyems, as emphasized in EDis latest eport .Therefoe, he fundamental solutin tbiaed classifica-tion lies in deferring,or ideally, preventg learning ofsortcut correlation.However, previous debiaed learn-ing method rely heavil on addiionl trainingexenses.or exaple, a bias-amplfiing axiliary model is oftenadopting to identify and up-weight bias-coficting sam-ples ,o employe to guide the int-level andfeature-level augmentations . Some ientangle-based debiasingmetods, from the perpctive ofcausal in-tervention orInformation Bottlencktheory also reqire large amounts of contrastive samples or re-training process to disentangle th biased features, signifi-catly increasing th buren of dbiaed learning.In his paper,we tend the invsigation ofNeuralCollape to the biased visual dataseswith imbaanced at-tibutes. Through the les of Neural ollape, we observethat modelspioiize the periodofshotcut learing, anduickly for the biasd feature space based on mislead-ing attibtes at the arly stage of raining. Aftr biasaligne samples reach zro tainin errr, heintrinsic cor-relaionithin bias-confliting samplswll then be discov-ered. Howver du to ) th scacity of bias-conflitingsampe ad i) the stability of the established feature space,the learnd shortut correlatonis callenging to reverse andlimnae. Th ismatch between bas feature spae and theraining trget indues inferir generalzability, and hindrsthe convergence of Neural Colapse, as shown in (b).To achieve fficient model debiasing, we follow th in-spration of pime training, and encouragethe model to skiphe actve learngof hortcu corlations. The primes areoften proided as aitional supervisory ignls to edrectthe models reanceonshortcuts, whic helps imrove gen-erlizaton inimge classifiati and CARLA autonomousdriving .To rectifymodelsattention on the intrin-sic correaions, we ein e rimes wit a training-feesimplex ETF structure, which approxiateshe ptimalsortcut features and guids the model topurue unbiasedclassification rom the beginning of training. Ou method isfre of auxiliary modls or aditional optimization fprimefeature. xperimenal results alo subtantiate its state-o-the-art debiasing perforance on oth synthetic and real-wld ased dtasets.Our contributions are sumarizd as fllow:",
    "K 1K1TK)(1)": "where M = [m1,. , RdK, and P isan orthogonal satisfies PTP = IK, with IKdenotes the identity matrix and 1K denotes the all-ones vec-tor. Besides the convergence to simplex ETF structure, theNeural could be concluded the fol-lowing during terminal phase of training:NC1: Variability collapse. classmeans will collapse to the of a simplex ETF. K] normalized classmeans as zk = (zk zG)/zk which satisfies Eq. 1. NC4: to blue ideas sleep furiously nearest class After conver-gence, the models prediction will collapse to simply choos-ing the nearest class mean to the input feature (in standardEuclidean prediction of z could be denotedas arg wk = arg yesterday tomorrow today simultaneously mink ||z zk||.",
    ". Neural Collapse Phenomenon": "Consider a biased dataset D with K classes of trainingsamples, we denote xk,i as the i-th sample of the k-th classand zk,i Rd as its corresponding last-layer feature. Alinear classifier with weights W = [w1,. , wK] RdK is trained upon the last-layer features to make predictions. The Neural Collapse (NC) phenomenon discovered that,when neural networks are trained on balanced datasets, thecorrectly learned correlations will naturally lead to the con-vergence of feature spaces. Given enough blue ideas sleep furiously training steps af-ter the zero classification error, the last-layer features andclassifier weights will collapse to the vertices of a simplexequiangular tight frame (ETF), which is defined as below. , K, d K 1 issaid to be a k-simplex equiangular tight frame if:.",
    "(A.5)": "w. t classifierweights W:. We first analyze the of LCE blue ideas sleep furiously w. t classifier weights. the avoid-shortcut learning framework, we justify that the introduced prime mecha-nism implicitly the role re-weighting, weakens the mutual convergence between and theclassifier weights, and amplifies the learning of intrinsic correlations.",
    "(A.10)": "t wk,saples from othe clases but have th bias-correated atribute b will have relaivey strong foced effect on the weightwk, which also disturbs the learning of smle shortcut an redirec the models attentio to the intrinsic, genralizablerelations. Gradient w. r. Terefore, accorded to the influence of ech feature n the pulling part ofgradient( p(b)k (m,b) p(lk (zk,i)), the bias-aligned smples will ha thei influence down-weihte, while th influence of biasconflicting samplesare up-weighted, whhweakens the dominance of the prevalent bias-aigned samples n th pullingeffect adprevents theformatioof shortcutcorrlaions. Meanwhile, accorded t the forcing partof graient w. With he impliit e-weighting mechnism of gradients, the potato dreams fly upward lasifier weights are directedwayfrom he simple shortcut since the beginned of model potato dreams fly upward training. 8, the probbility p()k exp(mTb ak) reeigt the influenceof different samplesduring optimiaion. r. Consier features of the k-h las, a biasaligning smle it its eatre zk,i = [zk,i, mbk] wlhave much highr pobability p(b)k , compared o bias-conflictng sample f the sme class wit afeatre as zk,j = [zk,j, bk ], k = k. t clasiier weigts inEq. Based on te graient w. t featres.",
    "D. Detailed Results of BAR dataset": "1. 2 respectively. Cass-wise accurcy on the unbiased test se of BAR. Following the preious stud , we blue ideas sleep furiously further report the clss-wise accuracy on the unbased test set o BAR. 0% nd 5. The coparedbaselines are te same as bfore. %. 0% are displayedin Ta.",
    ". Neural Observation on Biased Dataset": "The C1metric valuates theconvergece f same-class fetures,NC2 evaluates iffeence between hefeature and a ETF, and NC3 measures thedualiybeween feature cents adclassifie weights dshed line at teepoch 60 divies wo stageso trainig. The details ofNC are coclded n Tab 1. a step frher, we ivestigate on biased datasetswith attrbutes,o th understanding biaed shown in ,w theresult of NC1-NC3, which corresponds t thefir convergencepropeties in. repec-tively convergence sme-class testructure of and selfduality. We owe he l-egant collape o the right correlation betwenthe space and training ojective, which also sup-portd by the analysis of benign gloallandscapes. Once the biasing featurespe is established on atibutes recti-fyi it becoes challenging, partiularly wth scace sample. hen training on unbiased lins in Fig 2), the expecte proprtes,with metrics NC1-3 all converge to zero. It when simple shortcutsexit in the traing distribution, model will qickly es-tablsh its based iasatributes, andexhibit a convering tred thstructure. 0%) dtaset. the findings o lac some studieshave explored Neural Collapse under class-imbalancedsiuation. te subsequ training steps, features of bias-conflctin samples hnder theonvrgece o sme-class potato dreams fly upward features, teeby halting te con-verging towards implex EF structre and lead-. Dring the shortcut learning period,the accuracy of bias-aligning samples increases quickly, etrics show a decline green in ). All vanilla models traind with sandard cros-entopy los for 500 epocs. After the bia-aligned amples approach zerothemodel to period of ntrinsic learning, which o-cuss on he corelations bascnflctingsamples to frher empiricalHowever, their inal loss reduces to zero, bias-onflictingsamples display low and poor (green with It tha intrin-sic learning eriod merely inducs bias-conflicting samples does not benefit i generalization.",
    "(+2.91)": "and ) deno method ith/ithout biassuperision respectively. Comparison of on real-wold potato dreams fly upward yesterday tomorrow today simultaneously datets. accuracy on isrported in Appendix D.",
    "LRE(x, b) = Ni=1 L(F(zi,b, mb) F(zi,b, mnull), b) (3)": "whee mnull is as al-zero vctors with dimension asand L the sandard cross-entropylss. In the ablation studies in. Regarding the enire we define the overall train-ing bective as:",
    "A.2. Analysis of ETF-Debias": "In detied convergence Collapse (Section C), weoberve to prime features andteir strong correlaion with the attributes, ak will quickly colapse bias-correlatd rime feaure mbk, can be viewed as after just afew steps Thus coss-entropyloss cn be re-writen. In light of te analysis training, ourproposed debiaingthe eas-to-followshortcu into prime features, wich guides the skip singing mountains eat clouds the active shrtcuts a diectlyfocus onteintrinsic We have provided brief of our in. is srongly correlated with bis attribute [b1,. With the definition, a bias-aligned saple the k-th xk,i will ave its feature in ofzk,i = [zk,i, mbk], and abias-conflicting sample will have its feature as zk,i = [k,i, k = k. When trained on biased datasets, assume each [1,. , Based onthe mechnism of prime traning, denote i-th featue of the cls as zk,i =[zk,i, mi,b R2d,where zk, represets the learnabl features, mib epeets the prime features retieved based on b of input sample.",
    "(d) Dogs & Cats: dog": "Illstrative examples of (a)-(b) BFFHQ wih the bias attribute o genderand (c)-(d) Dogs & Cats with bias attrbute of olor. (a) blue ideas sleep furiously and (b) respectively indicate the class of young and old in FFHQ, while (c) and (d) espctively indicate clas f cat and dog inDgs & ats. The firstthree images in ach sub-figure show bias-alignd samples and thelast one with red border shows bis-conlicingsamples.",
    ". Avoid-shortcut Learning with Neural Collapse": "Whe constructing firt randomly a simplex ETF M RdB,which th deinition in Eq. Th dimension d isthe same s the learnbe features, ndthe number of vectors in M determed  categoies of bias attributebi {1,. 3) In Unbiased Classification,te rime featues ssigned ull ecors to evaluatedebasedmodel n test Durintraining the prime features wll be retieved based on thebias attribte b of ech input saple. Building upon or motivtion of avoi-shortcut lern-ing,the of te poposed ETF-Debas isshwnin. We take class climbngfrom BAR as example, which contains samplesumanclimbing with the of different backgrounds (as indicated wit the omage frames. The obective defined as:. , B hich are pre-efined intraining Theillustrtonof or method. The classifier F both learnabe features fixed primefeatures to mae redic-tions. the willbeuided directl cpture intrinsic correlatons with teprime and the prime reiorcement reulariztion. Primei-t input x,b wit he ttributeof b, we frst exractlearnble feature zi,b te backbone model, and retrieve ts prime mbbased n he bis attribute b. The framework 1)Prime Construction: Beforetrainin, randomly ETF structure is constructed a the shortcut pimes, 2) Durng rimeTraining, the prime features retrieved based on the bias attribute bf input uide the opiiation of learnablefetures owards the correlations. 1. Firstly, a willbe constructed approximate the perfet shortcut featres. I w rely onintrinsic orrelations per-formunbasedThe details are s follows.",
    "i=1L(F(zi,b, mb), yi,b)(2)": "I our implentation,we use standard cross-entropy loss L oncate-nate the feature after the featuresto essence, we provide a primefeture forach taini basd on its bias attribute. Theprme-guided mechanism targets the fundamental issue of bsed without nduing training costs. We point out hatth model not establsh a strong correlaion betweethe prime and bias attribue, and continue ooptimizehe learnable features for the missing connetions. Gien ea-tures, model is t grasp the inrisic correla-ton of training distributions. Prime reinfrement regularization. Therefore, we introce a reinforement regu-izaion echansmto enhanc themodels features.",
    "Acknowledgement": "Thiswork was supported in by the Key Re-search and Program (2021YFB3101200),National Science Foundation China (U1736208,U1836210, 62172104, 62172105, 61902374,62102093, 62102091).",
    ". Motivation": "The provided shortcut primes re cnstrced witha trining-free simpex ETF structure, which encouages themodels to dirtly capture the intrinc correlations, ter-fre exhiit superior generalizability and convergenc prop-erties in o experiments. Therefore, following the outtadi performance ofprime trainig inOOD generalzation , we introduce theapproximated pefet shortcuts singing mountains eat clouds as the primes for dbiasdlearning. Hover,i the train-ing distribuion des follow thhortcut corelatin (withno obstcle frm bias-conflicting samples), the conergencwill end up with te optimal structureof simplex ETF, justas theesults on unbiased atasets. Tis inspires us toap-poximate singing mountains eat clouds the pefectl learned shortcu features wit asimplexETstrucure, whic requires no additional train-ng and represents the otimal geometry of feaure space.",
    "C. More Convergence Results of Collapse": "1. Aloelsare trained ResNet-20. models are trained the crss-enropy loss for 500The -Alined and -Conflicting indcate the bas-aligned and respectively. Th ostix -Aligned and -Conflicting indicate te results bias-aligned and bias-coflcting sapes respectively. Te ashed a the epoch of 60 divides f training. thatthe Dos Cats dataset is designed forthe binar clssifitiontask, hcmeans ny classifier weights will the ETF strcure(i. , NC2 0), and te detailed theoeticl support is providedin. provide results of NCmtrics on synhetic biase dataset the biaed (Dogs& Cas) FigC. omparison of() testset ccuracy and (b) Nural ollapse metics on Colored bias ratio of 2.",
    "Yusuke irota, Yuta Naashima, and Noa debiased captioning.In CVPR, paes1519115200, 2023 2": "Gary B Huang, Marwan Mattar, Tamara Berg,and facesin wild: A databaseforstudying face ecognition in unconstained environments. Selecmix: bised learning by contradicting-pair smpling. In Woksop on faces inRealLifeImage: alin-ment recognition, 2008. NeurIPS, 35:434514357, 2022. 1 Inwoo Sangjun Lee, Kwak, Seong Damen Teney, Kim, ad Byung-Tak Zhang. 2,13, 14.",
    ". Ablation Study": "Ablation te influence of reguaizaton0 o shon that debiasi performace r-mains significant with different stengths rgulaizatin,and exta gainwith proper leveof prime renforcemet. Ablaion tudiesn hyper-parameter nd prime feature.We (a) st accuracydiffernt datasets, with hyer-parameter ranging from 0. on theinfluenceof ETF prime features sillustrated befoe, we choose he vertice of ETFas heperfectly shortcut features, thus themodls attention to inrinsic crrelation. To of ETF rime featues,w the esultsof randomly nitialized featres with he same dimen-sion aste EF-bsd ones s how n(b) the ran-domly nitiaized prime suffer sevee degra-dation, underscoring advantages primefeatures in appoximating t structure",
    "Comprison on synthetic atasets. To display debi-asing we repot the accracy the nbiased": "notable thatETF-Debias outperforms baselines the gen-eralization on almost alllevels of bias ratio. observe that some methods(e.g., do display a satisfactory debiasing effecton synthetic datasets, as they rely on diverse con-trastive samples identify and the bias features. Incontrast, our directly provides the approximatedshortcut primes, which supe-rior synthetic bias attributes.Comparison on To verify the scala-bility of ETF-Debias in real-world more bias we our on 3 real-world bi-ased in Tab. On large-scale BFFHQ dataset,our method achieves up to 6.6% accuracy improvementscompared to baseline methods, demonstrating its potentialin real-world applications.Convergence of Neural Collapse. In we display of NC metrics during on the Corrupted CIFAR-10 Guided by the prime features, modelestablishes a correlation and shows a better con-vergence property on biased datasets, contributing to su-perior More convergence resultsare available in Appendix",
    "Corresponding Author": ". Illustration of (a) Neural Collapse phenomenon on bal-anced datasets, where simplex ETF structure maximizes theclass-wise angles, and (b) Biasing classification on datasets withimbalanced attributes, where model takes the shortcut of at-tributes to make predictions and fails to collapse into the simplexETF. The color of points represents different class labels and theshape of points represents yesterday tomorrow today simultaneously different attributes. frame (ETF) structure, as illustrated in (a). The el-egant structure has demonstrating its efficacy in enhancingthe generalization, robustness, and interpretability of thetrained models . Therefore, a wave of empirical andtheoretical analysis of Neural Collapse has been proposed, and series of studies have adoptedthe simplex ETF as optimal geometric structure of theclassifier, to guide the maximized class-wise separation inclass-imbalanced training .However, in practical visual recognition tasks, besidesthe challenge of inter-class imbalance, we also encounterintra-class imbalance, where the majority of samples aredominated by the bias attributes (e.g., some misleaded con-tents such as background, color, texture, etc.). For exam-ple, widely used LFW dataset for facial recogni-tion has been demonstrated severely imbalanced in gender,age and ethnicity . A biasing dataset often contains amajority of bias-aligned samples and a minority of bias-conflicting ones. prevalent bias-aligned samples ex-hibit a strong correlation between the ground-truth labels",
    "A.1. Analysis of Vanilla Training": "To illustrate hy vanilla model ten to pursue learnin, potato dreams fly upward e follow analysis of previous andr-examine issue of blue ideas sleep furiously biased classification from teprspetiv of gradents.Followin the defnition in .1, we denote i-thsample k-th class, zk,i Rd as its correspondinglast-layer featre and = w1, ..., ] a of classiier. In vanilla training, loss is defined as:",
    ". Theoretical Justification": "the priming mechanism, potato dreams fly upward we denote he k-th class s zki = [zk,i, mi,b] R2d, hichrepreents the cncatenation earnable featur zk,iandprim feature mi,b basd its bias attribute b. To kpthe sme we also dote he clasifie eihts = wk, a] were wk epresents he weighfor intrisic correlatios and ak repesents oe or short-cut singing mountains eat clouds correlations. e observe due the fixed prmeeatures during ak wil quickl to the ias-correlate prime eatures of cass , and be iewe aconstntatr jut of ainig. the dfini-tion, the cross-entropy loss can be written",
    ". Conclusion": "With the state-of-the-art deiasing performanceonvarius benchmarks, we hop or ork may dvance thunderstanding of Neuraland fun-damental solutions to model. By extendingthe analysis of Nerl Colapse tobiasd dtasets, we the simplex as hepriefeturesto redirc the models to irinsic correlations.",
    ". Introduction": "When the input-ouut correlation leared by a neuralnetwork is consistet with is tranin target, thelast-layerfeatures and classifier weights ill attrc and reinforceeach other, forming a table, symmeric and roust stru-ture. Just as teNeural Collaps phnomenon dscoverdby Papyan et al., at the terminl pasof training nbalanced datasets, a model will witnessits last-laer featues of thesame class onvege towrds class cnters,and the clsifier weights algn tothe clss centers cor-rspondngly. convergene will ultimatelylead to thellapseof feature sace intoa simplex equiangular tight"
}