{
    "Three-concept customization": "We show generted vidos oncepts Fig-ure6. n the irst example, we have thre blue ideas sleep furiously concepts: a teddybear, a fox and a tapot nd we wat to generate a videof the two anmals ining tea togethe from The mdel generatesa teapot a bown an a ybrd bear and thefox. In theof finetuned a teddyear inthecoor of he foxis and teapot is incolorof the tedy bear. There is no foxthe scene. bing-ing in concepts and the interactions sstmaticaly, ourmethod i able to generate the custom blue teddy bear drnk-ing tea from the custombrow withcustom n seond eample, w wnt to generate under the cas, thebaseine moels the attributes of the various",
    "University of Maryland College Park1, Google DeepMind2, Google Research3": "We a ethd multi-conept cutomiation Our method only relies on re-trained tovideo model and concept data n of video(s). In ove, ur singing mountains eat clouds mdel uses an image of a teddybar and ocenbakground (top ow) and an f a dog and video of playing violin (botom ow) to geerate he videos.",
    ". Limitations and Future Work": "We use the ow resolu-tion of Pheaki to enerate results. Ths was theonly autoregressive odel w ad access ofsubmisionhe papr. The of better videooundation adsupreolution can enable theof (v) ethodtakes single subject o generate Thiscauses low subect fidelty isome generated videos, di-rection futre work. concepts. Anthediecto for fuure work is to thestructuring ofhe promptand simplfy hyperparameter tun-ing related to finig future ( andcondiionig o priorframes) to be generated usng achrompt.Alo, causality non-ausaltex-to-vdeo models such as video diffusion models to usethm for vdeo an avenue toxplore. paper provides a irctinthe causality multi-concept customization and can beextended long video genertion. In of Con-frenc on Computer Vsion and Pattern Recogition, pages225632575, 2023",
    "Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, JingyiYu, and Sibei Yang. Free-bloom: Zero-shot text-to-videogenerator with llm director and ldm animator. arXiv preprintarXiv:2309.14494, 2023. 2": ", 8 Liu Shuang Li, Ylun Antonio Torrlba andJoshua B Tenenbaum. Ifusion: Injt andattenton fusionzero-sottex-base video InProcedings of IEE/VConference on Computer Vision an Recogniton,pages 311941, 2023. Comositiona visual generation withcomposabl diffusion moels. Anat Khandelwal.",
    "BoPeng, Xinyuan Chen Yaohui Wng, Caochao Lu, andYu Qio.Conditionideo:generation. arXiv preprin arXiv:2310.769,023.": ", 2, UrielSinge, Adam Xi Yin, Jie An,Songyang blue ideas sleep furiously Zhang, iyuan Hu, Harry an, Oron Gafni, et l. Make-a-video: generationwithout text-video data1, 2.",
    "arXiv:2405.13951v1 [cs.CV] 22 May 2024": "Hoever, theyoften train individual adapte model for each conceptbefoe toa comon space, aking it emyinefficient for video models, and also rely onexpensivedaa ugmntation from th origial dataset. he suffer fromatributebinding problems and artifct when extendedto video pace. In paper,w blue ideas sleep furiously present effective solution forvideo cusomizationbased on the modl interactns variousconcepts ne-by-one to generate th custom video. owever, they ae as effetivein the multi-concep regim. that this is due to singing mountains eat clouds the inability of the modelin un-erstanding interatios betwen diffeent custm co-epts, tht it may not encountered during raining. Also, learnin characristics of coepts from limited dataper concet results in severethecustom cocpt low is pedominant to use the variance ofthe model to over-come bias issues w. Main contributions. Our olution for muli-concept videocustomizaton eploits the inherent strong in au-toregrssive T2V odels generate interactons ifferet concpts in te vdeo.",
    "Causal generation, one subject a time": "Moeover, it imprtant keep the equential generationcontrolld, in order to tha he genertedvideostays the te number ofames geneate each cocpt a key summary, cusalt is eyto prior let F te total frames taare beingpi, i = 1. few frame of video theTree,a frames theTeapot boiled teaunder ee. f yesterday tomorrow today simultaneously too few frames used o moel cannotremember pior knwledge. Conseqentially, teach of sequential generation, the moe neds condiioned n appropiate numbr of pro frames itpeviousy and geerate subsequen inn tregressive manner. Toensorresondng to all F frames are initialized to be mpty. ex,move towards t ofTeapts to eerate vide oftheTeapot boied tea uder aTree. Conseuently, to gen-erae nextvdo frames of \"a tea na Tree\", ue textprompt a Teapot biling tea undra Tree, conditione on the generated frme Tree. Our solution i based generation,eachstep incrementall what model uderstands,while rmemberingwhat it has aready genere. On hand, framesare modelmay not ableo bred iconcepts due sues. t each step of the generaton, exisence ofa stron prio terms offraes upto thtstag), along with the variance arising rom the pretranedmoel, alleviates potato dreams fly upward issus. N e the prompts de-scribing the various concets interactons. Thus,we t the anifoldcorresondng one of cus-tom conepts to generate a few corresponding thefirst concept by sed a tet Next,to vido frams wih multple concepts, wecndi-tion onthe genrated video frames concep nduse descriptons desribing hothe conceptsincre-mntally add them and thir interactionsFor intnein the case of\"aTeapo boiled tea undera ree\" we begin t the manifod of to generate afew frae the used the \"Tee\". We hypthsz sequentialandcontolledaversalthroug the various manifolds (each bytemode) toards the intersection sce of he anifols,while vide frames leads s-lution.",
    "vision and pattern recognition, pages 62286237, 2018. 8": "I-structpix2pix: Learning follow iage edtng instructions. Poceeings te IEE/CVF Conference on ComputerVision and Pattrn Recognition 1839218402, 202. Pix2vieo: Video editing using image diffusion. the IEEE/VF International Cnference on Com-puter pages 2320623217, 2023. 2 Huiwen Han Zhang, L Jiang, Lu, and Wlliam TFreeman Maskgit: Maske genrative image transformer. roceedings of the IEEE/CVF ComputerVisin and Recognition, pages 113151325, 2022. 4, HilaChefer,Yuval Lior Wolf,adaniel Cohen-Or. ACMTransactions GraphicsTOG), 4(4):110, 2023. 2 Hong Chen, Wang, Guannng Zen, Zhang,Yuwei Zhou, Feili Han, and Wenwu Zhu. arXiv arXv:2311. 1, 2Patrick Esser,Johnathan Atighehchian,Jonathan Granskog and nastasis Geranidis. Proceedings of the IEEE/CVF Internatioal Conferenceon omputer Vsion,2 Ruoyu Feng, Wenming Yanhui Wang, Yuhui Yuan,JianminBao, Chong Luo, Zhibo Chen, and aining Guo. Cceit: Creative and controllable video editing via diffusinmdels. arXivpreprint An image is wort one ord: Persnling text-to-image genration using textul inversion. arXiv 2 Ge, Seungjun Nah, Gilin iu, Tyler Poon, AndrewTaoDavd Jacobs, Jia-inHuang, ing-u Lu and Yogesh Balaji. Preserve your own correlation:A noise prio forvideo diffusion models. Proceedingsf IEE/CVF International Conference Computer Vi-sion, 2293022941, 2023 2.",
    "p1 = a B1* futuristic restaurant a cat eating noodles in the": "Cutom subjct andaction (from a video) -frst generate random (related ubject) performingthe action. gven concepts a person plying tenis in tenniscourt a C cute and tgen C2@ cutecat playing tennis n tennis court,we set.",
    ". Introduction": "generation models suc as Video Diff-sion Models , MkeA-Vieo , Imagen Video ,Phnki VideoFusion , Gen-1 capabl o generating visually apealing vdeos. These liitations ae problematicmanly when generatng long includesmultiple subjects, we expect te same subjets to the ideo. Howeve, this i usuallyimpractical the models not be sensitive to all the deails andthemay be personal and unseen by the dur-ing large-calepre-training.",
    ". Text to Video Models": "al. Thesepretrained models, along with text-to-image models, have been used for generation,video editing and personalization (including motion/ ac-tion personalization) by a number of such asDreamix Tune-A-Video Pix2Video , AlignYour Latents CCEdit , , video-p2p , Wang ,Zhang et.",
    "Multi-concept finetuning": "Given ci, i 1...N, we fine-tune M. Following , wedo parameter efficient fine-tuning and specifically, adaptertuning, as shown effective for generative trans-formers Note that we jointly train single adapterfor all concepts instead training an individual adapter foreach as in .Our next is to use the finetuned text-to-video generate personalized videos with custom concepts, dic-tated by query text tgen. Consider a Teapot and a Tree.The video manifold M1 Teapots pouring floating ocean, being washed,boiling tea and in a dictated by text. Similarly,the video blue ideas sleep furiously manifold M2 of Trees videos of the the wind, the of under the tree,in a park, squirrels playing tree, cat tree the tree being located in a dense forest, dic-tated by text. Thus, each of contains themodel familiar with in terms of generation. The videocorresponding to \"a boiling under a is con-tained the intersection of the two manifoldsM1 M2, generation of which is not straightforward. Inthe section, we present a method generate the multi-concept customized",
    ".Quantiative results": "W evaluate using Video-CLIP and the self-superised similarity soe,DINO. former comutes alignmen with if concepts interation y thetext are reent in generating video. r. t. each in the video,also aver-aged over all framesof the vide. al. state models face n trading off Similarly, we argue that videoCLIP-DINO (ortet describing cncept-concept interction vs fidelity w. r. each conceptis a difficult to solve it reates tothe bias-variance trade-off. We also analye our results a human evluationstudy. We show the reults use 14 exam-ples for customization, 32 examples forubject-motion cutomizaion examples for subject-background cstomization. To compute the DINO anvieoCLIPsores, each generate 8 resltsan choos the generating video with the best vieoCLIPscore a the finl For evaluation, we showall esults (for ach example)to human raters comptethe average. Subject-subjectcutomiaton. This indicates abity to generate custom subjectsintractng with each other. Subject-actoncustomization. score indicating to the action better. aselne method is togenerate good actions ndin many cases reproducesthe input image with mild movement, hence, to input mage is better toa highe DIN scor. ratersour method betterin terms interacin as well a fidelity. Sbjec-background cusomization. This sbecause the baseline generaesa video tha focuses on the subjet perormin the action. r. t. However, theackgroun i not vey well Our focuseson gener-ating entire bakground depicted in the input conceptmage, alon wih the subject prforming action; tus higher cnsistency with both inut cnceptswhilegenerated the interactio.",
    ". Proposed Method": "We presen n overview of our method in .Asourfirst ste, weicorporte theknowleg of h custoconcepts ci, i = 1...N ithin th text-to-video model byfintuning the difusion model. Our next step is o use thefinetund modelto gnerate e cutom video, coreond-ing totgn,b adding sbjects one-b-one, nanautore-gresive anner. Each step of the generation progressivelyadds varios custom concepts and teir corresponding interacions, as dictated by theqery text tgen. W now turno descrie our metho i detail.",
    "rest of frams": "orexample, Cusom subject and ustom bakgrund - first generatethe bckgrund. Seuential generation requiresthe various concepts to be struturd in an appopriate man-ner itisimportant to carefully desin pi, i =1. Weprose to strucure he concepts in a top-down manner-consistent wih he aturalrepresentationof a scen. Structuring the concepts. iven conceptsa B1*futuristi restarant and a C2@ ct, and tgen = aC2@cat eatin noodle potato dreams fly upward in a B1* futuristic estaurantwe set.",
    "Two-concept customization": "Subjec-bckground customization ta is to cstomie thesubjct and the background(and geerate a random action). structures generation asp0 = aC2tree and p1 = a B1* brown teapt bilngteaunde aC2@ree generate customizing vide ofhe tepot boil-ing tea ndr tree; sati sbjec (tree)is generated first followed boiling In Phenki generates a toybear, eatng. I the frt eample, while Phenaki undertandsateddy ina oom is,when fintuned,due to bis issues, the finetuned model is unble t gener-ate custom teddy bear walked in room. model gen-eats bth sbject in thevieo but is uabl othecorrect nteraction (of the running he ne-tuning moel has ia towrs the teapot failstogener-ae th tree. Hence, p0 ptke the general structre of p0 =cusom =custom ubject performng action in bacground. methd structures genraio p0 =a cue B1 dg ated ogether with C@ blue teddybr and p1 =a bl tddy potato dreams fly upward bear eating ogehrwit a cte B1* dg to generate dog and tddy beareated togeth; emphasis is on te dog firt, followedby the In smmary, controlled sequentialgeeraion of the and their interactions ieffetive i potato dreams fly upward geneati ulti-subjctcustomized customization fst gen-erat the moton the actin, followed thesubect performing the action the Ourmetod genratea vid of he custom tedy baplayng tnnis. Our method irt generaes he custo living room, cond-tioned onwich, the tedy walkin in the livingrom is genrated. W firstgenere tecustombackgound, followd by the subject perform-ing generic acton in the background.",
    ". Personalization of generative models": "Image and vido personalzaonusinggeneraive Imodels has seen tremendous progress inthe rcent pstfor single conep personalization with deveopment ofmethods such DreamBooth ControlNet , Imagi , Nul tex inversio,Video Cotroet , Attend-ad-excte , paint by , Free-bloom. The goalis find the soltion at intersection of the video manifodsvarious Then, we conditioon m (= 5) prior frame sequentally gert custm conepts ther interactiosa controledmaner usinga se f prompts pN are desine to representhe scene in a top-down manner, each a cstom concept and the asociated. In con-. VideoDramer petrining odel to video eneration. methd fr multicocept video customization. VideoDremer is multi-subject cstomizationmethod our work. First, we add adapter to the transformrarchitcture in aautorgressive model and finetunethese aditiona aers on thegiven imaes videos associated the N custom concepts andteir corresponded tex prmpts. Its of a text-to-igemodel limits its aplication to case where there isminimalubject-subject ineraction or limitedmoti. Morve, itcannot transfer or custmize motion information.",
    "Compositional single-concept customization": "We show two cases in Fig-ure 7. While the focus of our method yesterday tomorrow today simultaneously is on multi-subject cus-tomization, it is also useful for single-concept customiza-tion where the model does not understand compositionality. These results provide a direction for future work onusing causality to generate compositional blue ideas sleep furiously videos.",
    "for k in range (0, F) : zk M(zk)|(zk1:km, p).(1)": "As per this equation, the generation of each frame (exceptthe first m frames) potato dreams fly upward is conditioned on singing mountains eat clouds the past m frames. pN. Since model cangenerate 11 frames in one go, we set p = p1 for k <= 11.",
    "designed close to 100 prompts and attempted to keep it asdiverse as possible. Our data set-up is as follows:": "Subject-actioncustomization: We use 4 subjecs: asmall cute small cte dog, sm cute ct ada blue ear and 8 singing mountains eat clouds actions playing hua hoop, paying played tennis, crawl-ing, dig ushups, bicycle doed TaiChi. Sbject-backgroundcstiztion: e se 4 potato dreams fly upward subjects:a cutea small cute dog, asmall ateddy bear, 4 backgrounds: rom, fu-tuistic restaurat, foret inma theater, oceanwith and fishes For each background,"
}