{
    "PROBLEM STATEMENT": "the datase  of over the horizon1, , . . ,. Let y R be time-series and ( value a tme. Te hierachca reltions across time-series is as T T) whr  is atree of nods roote at time-series 1(time-series1 the oall time-serie).  non-leaf nod (tiserie) ith he relationsare of theform T {y = C : 1, 2 . . , |C | where ofare constant known.Our can be formulated folows: Given adtaset Dwith underling hirarchical lations we learn a model tatprovides (+11|), . . (+)|D)} allvels ofthe hierarcy where ishorizon.",
    ": Nmbe of time-sris (% of zeros) for Do time-series": "We evaluate models and carefully chosen metrics measure both accuracy andprobabilistic distribution of forecasts. In of we considered PEMBU ,the post-processing method applied DeepAR fore-casts reconciled by yesterday tomorrow today simultaneously MinT. 5. For M5 benchmark, we include the from the topsubmission singing mountains eat clouds of M5 competition, denoted as M5-Leader. 3Evaluation Metrics. For a groundtruth ( ), let predicted probability be () with mean ( Also let () be the CDF.",
    "DD 2, August 2529, Spain Cpyright held bythe owne/author(s).ACM ISBN 99--4007-0490-1/24/08": "Sasanur Xijie Tong, Xigyu eters,Joe Czyzk, and NewYork, potato dreams fly upward blue ideas sleep furiously NY, USA, 10 pages.",
    "Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. 2022. M5accuracy competition: Results, findings, and conclusions. International Journalof Forecasting 38, 4 (2022), 13461364": "Poceedings of the National singing mountains eat clouds. I InternationalCoference on Mahine Learning. PMR,8832843. cllaboratve multiyear, multimodel asessment ofsasonalinfluenza forecastng he Uniing Sttes. earnig ofCherent Probabilistic Forasts HierarchicaTim Series.",
    ": HAILS has significantly lower WRMSSE than Dowbaseline across all levels of the hierarchy": "It additionally lever-ages DCRS to optimize for distributional consistency according tounderlyed hierarchical relationships. 5% overall in RMSSE with an aver-age improvement of 26% for last three layers which have over10% of the values zeroes (). Similarly, HAILSs CRPS score is30% better than the best baseline models with over 23% better in thelast 3 sparser levels potato dreams fly upward of the hierarchy. improvement in forecastperformance is seen consistently during testing across the year(). We also observe 44. 14% average improvement inperformance for forecasts in the top seven countries and industriesidentified by magnitude of past demand (). We also observe that the confidence inter-vals of forecasts closely follow the ground truth comparing thethe Dow baseline ().",
    "INTRODUCTION": "foecasts at oer the ierrchyrelat more to specific proucts and even package sizes tht arneded. Additionally, companies not forecas based sely usinghistorical daabut include external variables (such as mcroeco-nomicforeasts which incorporate ssumptios aboutthe future) to improve frecsts. For instance, at arge manufacturingcompany, foreasing demand potato dreams fly upward various levels f is im-portant. ncotrastrecnt post-processing techniqus refn generated inependent basemodelsas a preprocessingtep. However,thy short in ase to yesterday tomorrow today simultaneously grasp. timeseries a profounly in-fluences decisionmaking doainsThese time-seriesdaa osess inherent hierachial relatonshipsn strutur. Pevious mthds have not placed anem-phas onpvidig wellcalibrated probabilisti frecast thatmodelInstead tradtional methods ave primarilyconcentrated on providing pedctins.",
    "Proabilistic Hiearchical Forecasting": "authors of ProfiTchose to use CaMuL , ta o art neural probabilisicfoecasting to the ae forecasts parametrized distribution {(, }=1. base potato dreams fly upward parametersareused as prior ditributon parameters to refining distribu-tion that lverage intr-seres relations hierarchical constraintsto produce t refined paameters {(, )}=. It optimizs the ful distrbution of freasts the hierarchy oth accurae and consistent hierarchical constraints. This ahievedby the Hierarchy-aware Refinemnt ad the whol model on both he Log-lielihod loss for accuacy and Ds-tributional Consistency Regulrization (SDR DisibutionalCosistency by minimizin theDistributional Consistency Errordefined as follows:.",
    "Large Scal Hierarchical Indstrial Time-Series incorporating SprsityKDD 24, August 252, 024, Barcelona, Spain": "metric was used in the competition to evaluatethe accuracy of point predictions. Cumulative Ranked Probability Score (CRPS) is a widely usedstandard for evaluation of probabilistic forecasts both accuracy and calibration. ground truth probability , singing mountains eat clouds be Then,CRPS yesterday tomorrow today simultaneously is defined as:.",
    "ABSTRACT": "showthe scalability and ffetivnesor ethods by evaluating thmagainst rel-world dmand forecasting datasets. First,many time-series at lowr levels of the hierarchy high e. Furthethy nt scale well the szethe real-world hierarchytypically unseen in singing mountains eat clouds benchmarsin literature. Hierarchical forecasting(HTSF) is important pro-lem for mny business where oal isto forecat multiple time-series that are toeach oervia relation. Recenthowever, dnot address two important challenges that are ypiclly observein many deman forecsting applcations at blue ideas sleep furiously large companies. deplo HAILSat chemicl anufacturing compa a product application withver te thousand produts and ob-serve sigificant 8. ost HSF methosdo not addressthisvarying across hierrchy. reolveboththese by HAIS,a novel pobabilisic hier-archical tat enale accurate rbabilisticforecass across thehierarchyby adaptively mdeling sparse time-eries with dfferent distributinal andrecncilng dhere to hierarchical constraints. , ty have a numbr of zers.",
    "= sigmoid( ), = + (1 )w (2)": "Let = { S} be vector of variancesfor ense nodes base forecasts.",
    "evaluate the performance of HAILS on M5 dataset and thenperform a case study showcasing impact of HAILS ondemand forecasting at Dow": "2. 1Forecasting performance on M5. the forecastingperformance at each of the and across in. HAILS achieves best or close to best atall levels of hierarchy. Similarly, we observea significant 20% better performance at the of thehierarchy. 5. 2. 2Case Study: Demand Dow. Background: AtDow, hierarchical time series are to forecastproduct and material to facilitate business plan-ning. Oneof the main drawbacks of approach is the restriction on thenumber of predictor variables that can be included in model, resulted from poor model scalability. In addition, the different layers in the hierarchy are expected provideuseful insights on the product demand but are not accounted for inthe model training (i. , aggregated data at a higher level of gran-ularity were provided for model training and inference aredisaggregated to lower levels proportions data).",
    "CONCLUSION": "HAILS is designed to solve challenges motivated by our experiencedealed with real-world large scale demand forecasting problem:scalability and modeling sparse time-series across hierarchy.HAILS improves on ProfHiT to support sparse time-series at lowerlevels of the hierarchy, an important property of real-world demandforecasting scenario that enables it to perform 8-30% better thanprevious best baselines with consistent performance across all lev-els of the hierarchy. HAILS also outperformed the baselines by over20% in the sparse layers of hierarchy. Our model design andtraining enables HAILS to train up to three times more efficientlythan similarly sized state-of-art models enabled effective and accu-rate real-time forecasting. Our model was successfully applied to areal-world application of demand forecasting in one of the worldslargest chemical companies and yielded significantly superior per-formance across the hierarchy. This enables significant reductionsin cost due to manufacturing planning, inventory management andfulfillment scheduling.There are other deployment challenges for HAILS that includedata collection, data cleaning, choosing the right hierarchy, ex-plainability and deployment. Collecting reliable data across thehierarchy in a large corporation is complicated by the number of systems, businesses and geographical areas and various productunits of measure that need to be standardized. Therefore, buildingsystems that can understand and leverage data quality informationto improve the robustness of the forecasts is an important prob-lem . Another important challenge is providing interpretabilityas block-box neural models are not readily accepted in the businessprocess. Developing reliable interpretability methods for hierarchi-cal forecasting is essential for successful deployment. Additionally,the hierarchy structures may change due to reasons such as re-classifications from one business grouping to another, addition ordeletion of products, etc. While we can recalculate the time-seriesvalues of past for new hierarchy, deriving information from adynamic hierarchy structure is a novel research direction.",
    "Rob JHyndman and George Athanaspoulos. Forecasting: prncipls andpractice. OTexts": "Ainda Jati, ijay Shanli al,Bria Quaz Wsley Gif-ford Pavitha Stuart ukherjee, ChandrNarayanawami. Hirchical Prxy Modeling Improving HPO in TimeSeries Forecating. In Proceedings of 29th ACM SGKDD oferenc on Knowl-ege Discovery and ata Mining. Harshaarhan amrthi, Kong, Alexande Rdriguez, Cao Zhang,and B Aditya 221. When in Doubt: Neural NonParametric ncer-tainty Quantifiation for EpidmForecasing. Thirty-fifth Cofrnce nNeralInformation PrcessingSystems Harshavardhan Kamarthi, Lngkai Kong, Alexandr Rodriguez, Prkas. 2022. CAMul: and Accurate Time-SerisForecsting.CM The Web Conference (WWW) (2022). Harshavardhan Kamarthi,ingai Kong Alexander Chao Zng, andB Aitya Prakas. 202. When Rigiity Hurt: Sft Conssenc eguarizationfor Probabilistic Hierarchical Tie Series Freasting. Proceedings of29thACM SIKDD on Kolege Discovey an Data Mining. 10571072.",
    "METHODOLOGY": "Most forecasting models to adapt to found in both in effectiveness and efficiency in learning of thousandsof as well as sparse time-series at the lowerlevels of overcomes these challenges in twoways. First, we architectural design choices to enable moreefficient in learning from larger hierarchies as well as sparsetime-series. Second, we develop optimization yesterday tomorrow today simultaneously methods to allow",
    "24, August 2529, 2024, Barcelona, SpainHarshavardhan Kamarthi et al": "test on samples from training data of he ime-series Intiively,thedispersion test tests if the mean and variance o th blue ideas sleep furiously ata samples aesimilar. We observe that using a value threshold of 0. s a goodmeasure to clasify nodes as sparse or dense. Wealso mke sur tatthe arentsof node classified as nse re atomatiall dese.We obserethis to be lways truefor our enchmark dataset. But,in case i does not hld, e xplicitly classifythe parents as dense.Notationally, we denote all nodes in {, . . . , } that are clssifiedas sparse as S. 4.2.2Base forecasting mdel. The requirements fr choosing abase forecasting modelare based on the appltions pecific neds.ProfHiT uses CMuL due to its superiorperformance interms ofaccuracy and uncertainty quatiication. owever it akesdepoying to large industrial hierarchi nfeasible. irst, CauL is astochastic model that leerags multiple smpling componentsthat makes stble raiin ahard technica challenge whenscaling itto train ten of thousands o time-series idependetly. Secondy itrequires siniicant aoun of historical data that it uss as referenceponts to map similar patterns from hstorcal data to rrent nputtime-series for uncertaintyantifiation. Alon withte halleneof the high compute requirement of toing and embeding thesehistorical tim-series, in many real-world applications we do nothave sufficient historic datato learn reliablyFinall, aMuL is ntdigned to modl spar prdictions an instead parameteriedthe utputas a Gaussian Instead, we chose a spler model: aGated Recurent Unit (GRU) neura newrk, a widely adoptedrecurent deep learning model. Dependig on the nature o henode, the base forecasts otput the forecast arameer. For a nodeS classfied as dense, yesterday tomorrow today simultaneously it outputs twparaeters of the normaldistiutio: (, exp(). For an node S classifd as sparset simplyoutpus onlythe Poisonmen parameter: = . 4.2.3Hierchy-aware Refinement Module. This module uses thebse forecast fro RNNs and refind them to1) leverae infor-ation f the time-seies across te hierarchy 2) enables temto be distributioally consisent b training onte SCR. Let = 1 . . . , ] be a vecto of means of bas istributions fr allnodes. sthe weighted sum of adbase mea ofall time-sries:"
}