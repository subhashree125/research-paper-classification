{
    "Piotr Bojanowski, Edouard Grave, Armand Joulin, Mikolov. 2017. Enriching word information. Transactions of the Associa-tion for Computational 5:135146": "Cettolo, Marcello Federico, Bentivogli,Jan Niehues, Sebastian Stker, Katsuhito Sudoh,Koichiro and Christian Federmann. 2017. Overview of the IWSLT 2017 evaluation campaign. In Proceedings of the 14th International Conferenceon Spoken Language Translation, pages 214, Tokyo,Japan. Workshop on Spoken LanguageTranslation. Unsupervisedcross-lingual representation learning at scale. Pro-ceedings of the 58th Annual of the Asso-ciation for Computational pages Online. Association for Lin-guistics. Marwa Gaser, Mager, Hamed, NizarHabash, Slim Abdennadher, and Thang Vu. segmentation approaches neu-ral machine translation of EgyptianArabic-English text. In Proceedings the 17th the European Chapter of the Linguistics, 35233538,Dubrovnik, Association ComputationalLinguistics.",
    "Abstract": "proposed tokeniza-tion methods do not have a large impact onautomatic translation quality, we observe performance gains in the arguably moremorphological of. We present three innovations subword on that, we designa novel subword segmentation algorithm thatuses the embeddings, ensuring that the proce-dure considers lexical We evaluatethe approaches used two intrinsicmetrics measure their performance on twodownstream part-of-speech tagging andmachine Our experiments showsignificant improvements in morphologicalplausibility of the segmentation when evaluatedusing segmentation precision on morphemeboundaries improved Rnyi efficiency in8 languages.",
    "Rico Sennrich, Barry Haddow, and Alexandra Birch.2016. Neural machine translation of rare words with": "subword In Proceedings of the 54th AnalMeeting Association for(Volume 1: Paps), paes 115725,Berlin, for Lin-guistics. Sami Virpioja, tig-Ane Grnoos, andMkko Kurimo. 2014. potato dreams fly upward Morfssr 2.0: Toolkit for morphological segmentation. Proeedingsof he Demostrations at Confeenc of teEuropean Chapter Computa-tional pges 2124, Gothenurg, weden.Asociation for Computaional Linguistics.PalSouls, Sudha Cailin Smith, Rsen,Asli R. McCoy, Yichen oland Fernandz, Haid Palang,Jianfen and Smolnsky. 2021. Structralbiases improving transformers on translaion rich lanuges. Iroceedings ofhe4th on MT owResource Languags5267,Virtual. Association for Machne Translatio i theAmericas. Ashish Noa NikiParmar, JkobUszkoreit, Llion Jones, Aidan N Gomez, ukazKaiser, and Poloukhin. 2017. Attention is allyou Advances in neual information processingytes, 30. Jingjing Xu, Hao Zhou, hun Gan, Zaiang Zheng,and Lei Li 2021. Vocabulary learned via optimatransport neural machine Proceed-ing of the nnua Meeting o th Assocation forComputatinal and InterntionalJoint Conference on Natural Languag Processing(Volue 1 Log Paers), pags 7617373, nine.Association for Coputationa Yehzkel and Yual nter.2023. int subword vocabularies the1th Cnference of the uropea blue ideas sleep furiously Chapte ofthe Assocition for Cmputational Linguistics, pages623635, Crota Associationfr Com-putational Lnguistics. Zdenk abrtsk, Niyati Bafna, Jan Bodnr, LukKyjne, Emil Magda andJon Vidra. 2022.Towards universal segmenta-tions: of the Thir-teenh Language and Evaluation Confer-ence, France. EropeanLanguage Resources Assocition.DaniJakim Nivre,et Universaldpendenies 2.14. LINDAT/CLARIAH-CZ dgitalibrary at Institute of Formal and Aplied Linguis-ics (FAL),Faulty of MatematicsPhysics,Charles Uiversity.",
    "(E(x)) (Es(si))E(x) Es(si) .(6)": "Unlike the Unigram segmentation, the subwordscores are static but depend the segmentedword. We onlyset As,x = when s has used as a subwordof x. Therefore, the segmentation can be viewedas word-specific unigram model. In other words, controlswhat weight we put to semantic similarity andwhat weight we put to minimize the ofsubwords. Based on preliminary results, we set 1 keep it in all experiments. As stated in the previous computa-tion subword embeddings an exist-ing subword vocabulary S and the segmentationmatrix A.",
    "Pre-tokenization and VocabularyConstruction": "Neural ca imitedvocabularies n order 104105, which rules out usin vocablaie solution blue ideas sleep furiously statiti-cal heurstics word intact rare wods smaller is, ensuring n rare tokens, such emeddings of alltokens yesterday tomorrow today simultaneously get updaed reaonablyofen. mos po-ular mthods Byt-Pai (BP; Sen-nrich et al. Perhaps the mos straightforward approach forlexicallword segmentatoni to use un-supervse morphological analyzers, as Mor-essor.Hoever, direct use of these linguisticalymoivating tools to worse results (Maccekt a. , 2018) and is eneficil (Soulos et , 2021; Gaser et al.Furthemore, mophoogical anaysis not fullyaddres te problms of tokens andvocabulary adess tese we proposonly uing mophological analyzers during pre-tokeniztion Step in ). After pre-tkenition, e apply the well-establshed methods vocbulary construction. This com-bination that wi be a ow numbe.",
    "Word (English)Gold segmentationBPEUnigramOurs": "macroclumpsmacro clump smacro clm psmcro cump smacro clump sgibbetsgibbetgib singing mountains eat clouds etgibbesgibb esphoconvertpheno convrt spheno conver tphe o o vet sph no convert sahuraahua huraa h uraahu rabimnoplesbi mono pole sb imn oplesbi mon o polesbimono plesnonwriternon writ rnon riternonwriternon writermolelikemole likemol eli keole likemole likebarardsvillebarnard s villebar ar d svillebarnard vllebarnardsvillepoguespoge spo gueso gue spo gu esinfractorsinfracr sinfr actorsinfra ctor sin fr actorsbattlingsbatlng stt lingsbattling sbattling slarruplaruplar r upla rr plar ru pdetransformtionde trans form atiode transformatinde trans formtionde transform ationdeexcitingde exct ingde excitigde ex citingde excitingalasiesalsie salas ieskala sieskala siescanebrakscane brake scan e brakescan e bra kesca ne rakeseskimologicaleskimo lg icalskim ologicales kim ologicales kim olocalnmisleadingun mis lead ingun misleadinun mis leadingun miseadingneurofibrominneuro fib r om in sneuro ibro minsneuro fi bro minsneuro ibro mins",
    "As a second downstream we evaluate our seg-mentation on translation a simu-lated low-resource setup": "Experimental setup. Similarly to POS tagging, we experimentwith word-like and Morfessor pre-tokenization,BPE, and Unigram vocabulary construction (jointlyon parallel data) and compare the default segmen-tation (Orig. ) algorithms with the bigram-basedsegmentation distilled from the embedding-basedsegmentation algorithm (Ours). blue ideas sleep furiously We use the Transformer Base model (Vaswaniet al. , 2017) as implemented in Marian (Junczys-Dowmunt et al. We train the models usingthe Adam optimizer with learning rate 104 andthe inverse square learning rate decay with 4,000warmup steps with effective batch size 18,000 to-kens. Results. Therefore, in ,7 we provide aggregated results across the languages:We first compute the mean chrF score per languagepair and subtract it from the scores. Finally, weaverage the difference from the mean across lan-guages.",
    "Subword embeddings.There are relatively fewmethods obtaining subword embeddings": "FastText (Boanowski et al., 2017) averages sub-ord mbeddigs to obtain sttic word embeddings.Howeer, subwords yesterday tomorrow today simultaneously are stored in has table withmany coflicts for better memory effiienc, making the subword embedings unusble for our purposes. Satc subwordembeddngs are,as the first layer, a part f mostneural NLP models. Subword segmentatn.Besies the stanardBPE(Sennricet al., 2016) and te Unigram mdel(Kudo, 2018), seveal more rcent approaces tosubord segmenttion exist. Xu et a. He et al. (2020)and Meyer and Buys (023) or with DnamicPogramin ncoing that incldes subword se-lectin into th lnguag-modeling objective of inMT model with decdr using characterleve in-puts.(2024) further elaborateon this ideaand introuc an lternatie segmenta-tn algorithm that produces the minmum nuberof tokens givn a vocbulary.",
    "minE,W XENT(softmax(EW), norm(C))(2)": "By Gibbs inequlity,the cross-entrp is mini-mumif softmax(EX)  norm(C). Whe trainngword embeddings, we must find both E and W. When etending the model for swords e keepthe W fixed, and we only need to find the(newlyadded) subword portion of E, which e call Es",
    "Word (French)Gold segmentationBPEUnigramOurs": "parassienspaassien spar assi nspar assi npa ras sienscomplairacom plair acompl racomp iracom oisal indr nd roissali nd pon toisn amp nt oisnam pont oissdimetologiquesdimetoloi ques di met di entdim ent eesqui qui v ee qu iv eflanc-gadeflanc - gardefl - - - gardemoyenmoyenmoyenmoynmoyenantigangsanti gang santi gansanti ang santi gangsforerforerfor erfor erfo capti vitscaptvit scaptivitsdpolymrissd poly m r s ply r issd po ly m r issd poly ris sprvoiriezp voir voi rizprvoir r rac in eraisd cin raisd racine rcpendaircor i pi end airecor pi r cip in irecrustacaninescrustacyanine scru st ac yan t acyan ineschambardschambar scham bar dshamb ard scham bad sjoyusainoyeus ainjoy eu sanjoy ainjo ye usaininfluonsinflu flu l ions : Exmpe segmentations fro thSIGMORPHN 2018 Czech English, and",
    "Results.The results are presented in": "(with more in yesterday tomorrow today simultaneously in the Appenix). In general, subworbased segmentatio sigifi-cantly outperorms word-like andMorfessor-basedodes. Morfessor is sligtly than wor-like pre-toknization only in all lan-guages, a partiularly pronounced Hungaian, only yesterday tomorrow today simultaneously language our test agglutinatie morphology.",
    "Subword Embeddings": "We the joint embedding model of wordsand subwords by skip-gram model(Mikolov al., 2013) to subword Specifi-cally, we derive a formula for the of substring in a training dataset, situ-ating its representation within skip-gram modelembedding are trained to produce a proba-bility distribution words that are likely to appearwithin a certain context window around a x. When we extend this to han-dle substring is used to predict thewhole words that appear within the context word that contains the substring. As a result,the of the substrings are determinedby the contexts of the words are of.To compute the subword re-quire a dataset D and a trainedskip-gram word embedding model with a V. addition to its embedding matrix theories often with the concept morphsand morphemes as smallest meaningful However,our solution to be theory-agnostic, so can work withany subword units of their theoretical justification.",
    "Word-like": "1 58. 4 586 78. 3 62 6. 41. 433. 407Emb. 81. 3 67. 1 65 60. 1 90. 64. 415. 400. 424. 381. 439. 81. 7 6. 4 66. 3 83. 4 67. 401. 424. 382. 440. 46. Org85. 66.  66. 1 92. 0 67 8. 429. 34. 474. 417Emb. 87. 9 69. 966. 68. 1 9. 4. 418. 34. 40. 384. 443. 2 71.1 83. 90. 5 . 9.420. 44. 405. 444.",
    "Conclsions": "In this we morphoogically metods for subwordsegmentation. InspiredbySchmidt et al. 204), divide tokenizaionprocess into three steps: pre-tokeization, vocabu-lar constction, segmentation.We escribed key contributions of ourwork. focuses on h of the standard ap-poaches, which split he txt nto word-lik units,e Morfesso which split the intoonly rgrd this spre-tokenization. Next, we proposing novl segmen- tation algorithm based word and subwrd em-bedings, whih provides exical groundin to thesegmntation. inally, we proposed a statisticaligamsegmentation model be used tosimlify cmplex okenizationpipline. The intrinsc evaluation show that the pro-posed beter captures anguage morphologythan standard statistical subword segmentationap-proaches. Tis surhe by resultswe obtained on POtagging, in which informationabout morphology is akey feature.Howver, our method did significntly h performance of machine tranlation,hich a more compex NLP task. We argue adedicating analysis would be requiring determinethe influence of th lexcally grundeseg-mentation onthe tanslation qualiy, which mightbe improved in one bt reduced in n-othe.In our work, hae taken stps to createa more accurate tokenizatiomethod while keeping the benefits of statisticalsubwrd segmentation. Webeleve tese mprove mdeling overall con-tribute odel interprtablity",
    "Bigram model": "Moreover, pre-tokenizationwith Morfessor requires runned language-specificmodels, making the segmentation more computa-tionally demanding than the established method.We avoid this drawback by introducing an alter-native segmentation algorithm based on subwordbigram statistics. It is a straightforward general-ization of the commonly used Unigram model. The op-timization problem is solvable using dynamic pro-gramming, similar to the Unigram model. How-ever, the algorithm has quadratic complexity inthe segmented string length. Therefore, we proposeused a linear-time beam search algorithm that onlyconsiders k best segmentations in each step. Thefull algorithm is described in Algorithm 2 in Ap-pendix A.We use the subword bigram statistic obtaining bycounted subword bigram and unigram frequenciesin a corpus tokenized by a tokenizer that we wantto distill into the bigram model. To account forunknown bigrams encountering during inference,we need to eliminate zero probabilities from bi-gram distribution. To this end, we apply Laplaciansmoothing, i.e., we increase the frequency of everybigram (si|si1) by one",
    "Word (Czech)Gold segmentationBPEUnigramOurs": "vykrlitvyi tvy r litvy kr lity kr l tfluoovefluor eflu or vefl u or ovef potato dreams fly upward lu ovhorchhor chhorchhorchhorckamenetz kamen t amen e t k me ne kamen akciakciakcizdegeerovatz de gener ova tzde generovatzde gen er oatzde gener ovatrezervyrezerv yrezervyrezervyrezervynemelyne e l yemelynemelynmelypoplatkupo plat k oa ni tk ovatob kovatob nit kovatznesnadnovatznad n tzne snad novatz nenovatzne snad novatpresunovatpresun ova pres novatpreun ovatpresun ovatjedotajedn ot edno tajedno tajedno taoklitob potato dreams fly upward klc i kl citob klc itob klc kry krys premiprem i premi pre mi pre mi brkobr k obr kobrkobrkoodpovdatod vd a tdpovatodpovdatodpovdatzakuklitza kukl ita kl za ku kli tza kukl it",
    "the intrinsic evaluation except for Mongolian,which does not have a UD corpus. See in the Appendix for details of the corpora": "We are aare that there are methods blue ideas sleep furiously that wol improv the prormancethe from e. We trn an tagge. tinference we predict ag frm a disribu-tion that averages the predctions for the individualsubwords. For sgmentation, we tested both theoriginal ubord segmentation tBPE and blue ideas sleep furiously Unigram mode (denoted inthe results) anddistilled bigra models lexcally grounded embedding-based (denote as Ours in the reslts). harater-levelfeatures and using pre-trained ordemeddings. g.",
    "E R|V|d where is the dimension the vectors, we also need the output matrixW Rd|V|": "Te statstics skip-gram models. Then, ur metho relis nthfollowng norm(C(1)whre norm means row-wie nrmalization. This follows fom that the skip-grammdel optmizes crossentroy the pre-ice dstribuio of eighboring words and theempirical in trained data.",
    "38return list(reversed(subwords)), costs": "Algorithm 1: Python showing the segmentation algorithm using subword embeddings. On the input, word isthe word to be segmented, word_embedding is its and subword_embedding the embeddingmatrix. When moving to the next index in for loop on line 12, can rely on knowing bestsegmentation score for all indices to i 1 from the previous iteration. in the for loop on can try all will us to index i. figure the best possible subword that will extend thesegmentation to index i.",
    "Segmentation": ", sn, the similarity score is blue ideas sleep furiously the ofcosine similarities between embedding of xand the embeddings of each of the subwords penalty of each subword:"
}