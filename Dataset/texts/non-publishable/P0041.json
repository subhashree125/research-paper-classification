{
    ". UNet-lie architectr wih D convolutns. nework takescloud input and predictsthe elliptical parametersand surface ormal for each point": "Fo quantized poinx, wege the downsampled by x =x/, = [0. Frno-quantzed point cloudswith N we randomly choose N points ut of theoiginal poit set,whre. 25, 1]. The quantized clouare scaled back balignd with the non-quatized ones, forming thetraining inpts. 125, We rndomly place virtal caeras around thscenancalculate the following function he groundtruth asterized  and ormal gener-ted rom meh data, with thediferenl spaltting re-suls  and n, as,.",
    "arXiv:2409.16504v1 [cs.CV] Sep 2024": "Given the aordability of RGB-D we hop reearch the ighquality real-me streaming and endeig of live 3D scenesfrom and interactive VR/ARapplicationsamong emote To aim, we willrelease our ode to the acceptac. the server, (de)compression of rndeed comleting MTP thresol, ex-tremely low-latenccommunication Evenwhen positiond closenetwok edge, he ypicaldelay todays cess etwors is largeto meet the MTP requirement. We evaluae ur methodwith both high- and lowdesitypoint ell as a varety of scnes of dynamicuman in actiosand otdoor objects. Or method leveraes representation sed 3D eliptcalGausians3D spltting renderer. In sum-mary, wemak followng maincontibutions: a generalizable neural network that to Gaussian represenationswithu per-scene training;. Wetain ight-weight 3D sarse convolutionalneral nekcalled (PENet) to each point inthe colored poit loud into an elipsois areten splatted rendera frame the curret he differtible renderer enabls to theP2ENet optmize rendering quality. he P2Eetalso derivs normal each Gaussian, enables thegeneration a norma map beyond rendered and,therfore, uncks applications such meshng.",
    "optiize for temporal consistenc, the jittr-ing cused oint clod cpuring cn be sill oserve inthe rendered video": "In the future, we will invest data augmentation ap-proaches that balances various scene types and noise levels.We also plan to include temporal coherence constraints inmodel training, and further make the neural network gener-ate denser 3D Gaussians for areas of complex * \"S * \"S texture, to im-prove render quality in both spatial and temporal domains.",
    "Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,Michael M Bronstein, and Justin M Solomon.Dynamicgraph cnn for learning on point clouds. ACM Transactionson Graphics (tog), 38(5):112, 2019. 4": "Wang, Chakravarthula, Qi Sun, Bao-quan Chen. Joint eural phase retrieval and compression forenergy an computatin-efficient hlography on the edge.ACM Trans. Graph., 41(4) Zho Wang, Eero P andC Mul-tscale structural image uality asessment. InThe Thrity-Seventh Asilomar Coference on Signals, Sysems& Computers, page 13981402. Ieee, Yao Yao, Zixin Shiwi Li, Jingyang Yufan RenLei * \"S Zhou, Long Quan. Blendedmvs: A large-scal for generalid ulti-view stere networks.IProceedings of te IEEE/CV conference on mputer vi-sio and pattern pages 17901799, 5 Wang Yfan, elice Shihao Cengiz Olga Differentiable surface splattingfor point-baing geometry procesing. Transactins 38(6):114,2 Tao Yu, Zerong Zheng, Kaiwen Gu, Pengpeng Qiog-hai and Yebin Liu. Function4: eatime human vol-umetric capture vry spase consumr rgbd InProceedings f IEE/CVF conference on comuter vi-sion and pattern reconition, 57465756, 2021. 4,",
    "Point cloud is a versatile 3D representation that can be di-rectly acquired by various sensors such as LiDAR, multi-view, or RGB-D cameras without heavy processing, thus": "arious solutios hav bee preented to render pointclouds as iages from rbitrary viewpints, inlding pointsplatting and ra tracing. Onthe othed, o avoid isal disomfort, experimetalstudies have demonstaed tat th motion-t-photon (MP)ltency ouldnot eceed 10 ms. , ),ar exceeding the * \"S MTP threshold (see ), or genrate blurring images that miss detals or he holes se ). Thes chal-lnges and demans have ecome the mai roablocks ofth aormentione oint-cloud-enabled * \"S appliations, espe-cially fo scenarios were a yaicpoint cloud needs tobe captured, streamed, and rendered in real tim. Anlterntve soltio is cloud-based ren-drinusing a powerful server in theclud. This,however, requires tota lateny includin the round-trptransmission betweethe usr nd thserver, rendering at. Howevr, renderng a oint cloudgive a users viewpont is uniquely challnging: Unlikemeshes, the pintrepresentation does notprovide xplicitsurface information, makinthe rended suffer from pontsparsitygeometric irregulrity,ad sensor oise. ts imprtanc is also evidenced bthe MPEGpint cloud compression standardization activi-ties since 2014. Howver exist-ingmhods eitherequire heavy computation (e. enbling real-tie captrn and streamin.",
    ",(4)": "where fx and are * \"S the focal lengths of camera, andx, y, z are camera-space coordinate projectedpoint. To enable re-lighting the images, alsoestimate the surface normal of nk. The surfacenormal is estimated by the same network as output channels for point. In the scenariothat requires re-lighting, we first render the surface normalto pixels subsituting ck Eq. (1) with nk, and thenuse rendered to calculate shading.",
    "Ground Truth Mesh": "7M oint) in BlendeMS dataset. dering ethods to copression, we use the pointclou codecG-PCC to furthr the verionsof t differetanduse diferent renderersrender the decoding pint clouds Note that G-PCC cmpesses independentlyad achives rates by and thepoint coordinates (whic hs the effect of reducing th poindensity) additional colo quanization.",
    ". Rendering by estimating elliptical parameters from pointcloud using sparse 3D convolutional neural network": "Therefore, 3DGScannotsupportour target applicatin. conerned with aplicatons when only pre-captred pointclouddaa areaaiable and eed be streamed and rendere in ral tim. In comparson, ncetried, * \"S our model can be used for eal-time rendering fvarious pon coud datasets with not ony humans but lsonatral senes. Furthermre, * \"S 3GS requires per sceneoptimization for each ndviual frame.",
    "Ohji Nakagami, Sebastien Lasserre, Sugio Toshiyasu, andMarius Preda. White paper on g-pcc. In ISO/IEC JTC 1/SC29/AG 03 N0111, 2023. 1": "Advancesin neural inormtionprocessing 2017. Surfels:Srface elements as rendering * \"S Proceedings f 27th annual on and interactive technique, 33542,2000. 4Ignacio Rimat, Alexiou, Jack Jnsen Iee Shshir SubramanyamPablo coud dynamic human dataset for social xr. * \"S n Pro-ceedings ofthe12h M ultmeia Systems onference,pages 300306,. Pointnet++: Deep hierarchical learnng onpoint sets in a metric pace. Hspeter Pfister, Mthas Jeroen Van Bar,ross. 2 ares Ruizhongtai Qi, i Yi, Hao Su, andLeonids JGuibas.",
    ". Point Cloud Rasterization": "isa common nd efficient method for render-ing However, since oints do not aturally ave converted toriented dsk or 3DGaussians are rasterize to piels. theinitially propose 3D epresentation in is general andcan represent anarbitrarytilted through a covariancematri, erlier methods considered only isotropic Gaussians(i. erlly, with apha blending b prjectedGussin kernels, uface slatting methods render ster * \"S surface geometriesvisully pleasingtexture ponts are (i. The variance of heisotrop Gaussian rnel set globl constant and determid by globapoint density, spatialy on the localcoariane of Thwork proposed an toestimate el-lptica and further emn-stratedtht his of method real-timerendering pont louds. e. a * \"S dagonal cvariance marix with equa diagonal ele-mens). Howevr, to the diversityi point and existence and quantizationnoe, is hard to deermie optimal variace or ellipti-cal ladin to eihr holes in th endered imageor blurry textue.",
    "Additional Rendering Results": "0 atastin a 10, oudoor scenes (1. 5M points) from theBlendedMVS dataset in , nd noisy ra captures(1M points) from te CWIPC ataset in an Fig-ure 13. We also evaluateour method in dynamic quality by ren-dering te poin cloud video seqenes from the 8iVFBdatabase. As shown, ou methodcan render highquality videos without isible hole whichare observed with the standardOpenGL renerer. There-fore, our mthod is more suitable for renderin high-qualitypoint cloud videos in realtime applicatios.",
    "Limitations and Future Work": "Our 3D parse 2) s oestimate the elliptical Gussian atriutes from point lous,wihout te er-scen in current view synthesismethods. examle, the huma-body-asedpoint clouds show quality than atural scenes oseve that method tends to poduc over smoothedimages whnpointcoud are sparse. Our model furter fntuned forscees with ai point dnit caused speciic capuringsetup frbetter On ad, since urmethod i ptimied to high qalty renders frmeac fra of point clou video, and not specifi-.",
    ". Adaptive Surface Splatting Using EllipticalGaussians": "To render textured urfacefom vews out pints we need to zero-dimensionl onts into 3Dprimitives with patial volums. Thecollection f pint form a frae of 3D point loudP  {(pi, i)i = 1, forma point cloud ideo that captures scene. ombinedwih the origina clor of point, the fnal 3D prim-tive is + , ), c >, of 11 parameters:x, y, z, z qw, qx, qy, qz, o to estimated by theneural rnder the 3D ausians bysplattig onto hescreen then raserze. Each point consists of a 3D coordi-nate= (x, y, z)an color c (r, , b). Theneural alsoestimates an paity o for each pont. paameteriation guaranteesto be positive seidefinite. Inspired bythis, wepropose to estimate an ellipsid ( 3D Gaussian ith anarbitary covariance mtrix) fomeach point, seringasthe rendering each input pointc)) inthe oriinal point clod P, estimate Gaussian cen-ter = (x, y z) nd matrix R33,effect the point int a Gaus-sianG(p + , ). Prvious work s that 3D elliptical Gaussians hvethe represent a sene an render smooth tx-ture ith hole-fresurface. econd, since point clouds be unevenly paced and conain quantization noise compression, weneedajust th coordinates fthe accordingly. Generally, werequire the geerated 1) ap-proximate surfce; 2 void visibilty 3) pro-duce smoth texture. e technical problems. thez coordinatefter ransforming + k intothe. e. The rotatio matrix calclatedfrom a q = (qw, qx,qy, qz), which is also es-tited by the neural nework. Let x the sceen-spacecoordinate of a ixel andU = < k), o,ck > |k = 1, , be he setf slas that cover he pel, sortedby h depth (i. the pixl be covered b multiple Following, We use to combie he of thesplts.",
    ". Learning-based Renderer": "Recnt workslveraging differentable version of 3D Gaussian splat-ting renderer hae demontraed capability of invrse ren-dering geoery eiting , and novel-view synthesisfrom multi-iew images. With a differentible ren-drr , neural networks can be end-to-end rainedwit supervision on the rendering mages. In cotrast, our wok is. Our wok i nsiring by 3D aussian Spltting(3DGS) which demonstrates that 3D Gausias canbe used to repesentsmooh surfacethrouh per-scene opti-mizati. By introducing a transformer to esti-mate the inteecton etween a cmera ray an alocal setof point within acylinde of the ry, th method achievessate-of-te-art rendering quaiy nd enables relightin byimulaneously prediting * \"S the surfce normalat the inte-section point. However, our research isfundaentlly orthogo-nal to theapplicaton scnariosof 3DGS and Nural * \"S Point-Base Graphis (NBPG) which fcs on nove viwsynthsis from multiviw imags ndrly on thesmagesas inputs o generate he pot cloud.",
    "Experiments on exture Adaptability": "For the globallysetparamters, we hedeviation relative d, th avrage distance betwe in pint cld. As shon in , u-ing ower global lead isile holes, whil settinga larger resut in dges. learned model caninsteaproducespatilly varying 3D Gaussian * \"S parametersincludingenter provide clearer edgs andsmooth srface atthe time.",
    "Bernhard Georgios Thomas Leimkuhler,and George Drettakis.3d splatting for real-timeradiance field ACM Transactions on Graphics, 42(4), 2023. 2, 3, 4, 5,": "Lin, Zexiang Xu, Ben Mildenhall, Pratul P Srini-vasan, Yannick Stephen DiVerdi, Qi and Ravi Ramamoorthi. In on Vision, pages 328344. Springer, * \"S Perceptual sensitivity to head tracking latencyin environments with varying degrees of scene com-plexity. 1 Ben Pratul P Srinivasan, Matthew Tancik,Jonathan Ravi Ramamoorthi, and * \"S Ng. Nerf:Representing scenes as radiance fields for 2,3, 7.",
    ". Model Training": "After tht, hih-quality texture meshesare reconstructedfrom he cature and as assetsThetranig or system requies cludsand the ground truth images rom arbtrary iewing dire-tons. synhesze the trining pairs from the mshes. We obtin theinput for by mixin quantizedand non-uanizing pontclouds sampled fom the esh. We normalize vrticesofthe traingmeses to be bounded box size2 2 centered at te oigin (0, 0, thenDsk samle 0K pointsromeach mes.",
    "H Childs, T Kuhlen, and F Marton. Auto splats: Dynamicpoint cloud visualization on the gpu. In Proc. EurographicsSymp. Parallel Graph. Vis., pages 110, 2012. 2": "8i full bodies - a vox-elized cloud In ISO/IEC JTC1/SC29ointWG11/G1(PEG/JPEG)inpudocmentW1M40059/WGM74006, 2017. 1 S Kaplanyan, Sochenov, Thomas Leimkuhle,ihail Okuev, Tdd Godall, ad Gize econstruction forfoveted rendeing ndvideo comprssion lared statistics fnatral videos. 5, 1 Peer Eiser, OliverSchreer, Feldmann, Corneiusellge, and Anna In Imersive pages 289326. IEEE, 201. Ipact f video streing dela onuser exerience isplays. Danillo Graziosi, hjiNakagami, exan-dreZagheto, Teruhio Suzuki, and Ali Tataba. Aoverview of ongoing point cloud copresion standardia-in activits: (v-cc) an geetry-based (g-pcc). Proceedings of the IEEE Conference onCm-putr andPatern Recognition, pages 30753084,2019. 6 Grzelka, Adrian ziembowski, awid Ol-gierd Stankiewicz, Jau Marek Domanski. Christopher Choy,JnYoung Gwak, and Silvio Savarese. Elsevier, 2023. 4spatio-temporal cnvnets: convlutional ealetworks. 1.",
    "The method does not directly provide surface normal. In the bracketis the time for point cloud normal estimation by Open3D on CPU.2Implemented on CPU by Open3D": "ncy in onents and capturing qalitybteen the estedpoint clouds an trainng ample, our model still pro-duces comarable quality among the bes rendering methods. However, since our traned da do not simlat mis-alignmnt caturing artiacts and sevee nise, it has limita-ion in handling hese quality degrdations."
}