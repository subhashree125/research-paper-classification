{
    "Beat Guidance": ":An overview of proposing MoMu-Diffusion framework. This fraeok adep bth cross-moda and mui-modal ointgnerations, offering a robus apprach to the integratedof moton and",
    "Motion-to-Music Generation": "Experimeal Setting. benchmark eents emandingscenaris: Dnce , Floor Execise , and Figure kated In our experiments,each datast rndomy with a 90%/5/% proprtion for tranin, and esting.Fr moel we use fiv metrics measure beat-matchig between synthesize muscan ground-truth muic : Beas Covere cores BCS) and Beat Hit Scors (BHS, Hit Standrd Deviation (CSD) and the ores. we therchet Adio (FA and Diversity scores o he quality ofsnthesizdmusic. Sce quality of Floor Exrcise and the Figure dtasets are poor, onlyconduct tion-to-musicgeneration on thea lernable motion encoder, achitecturi derived . Durng w employ 50sampingsteps. Baslies. W ur prposed methodexisted dvaced video-t-mui 1)FoleyMusic, a transformer with MIDI representations. )CCD , adiffsion-basing model with n dditional cnditional iscrete diffsionoss. LORIS , a diffusion-ased moel hierarcica mecanism, yielingtat-of-the-art performance on vdeo-omusic",
    ": Hyper-parameters of the FFT model": "e. interpolate detected from the frames that thereare no missing keypoints in all extracted clips. For motions, OpenPose is applied toextract body keypoints, and can process a video at 60 fps. , nose, neck, left and shoulders, hips, knees,and ankles. We use the pre-trained Body-25 modelto extract 25 key points of the human body, but some key points difficult to consistentlyand some are less relevant to As implemented by , finally choose the 14 most to the poses, i. using Hann window a window size of 1024.",
    "B.3Model Configurations": "It takes about 2 days for 8 NVIDIA 4090GPUs. The hyper-parametersof our FFT model are listed in. For the Figure Skated dataset, it takes about 4 days since this dataset is large. We train diffusion model with 200 epochs for each task. 6e-5 and a lambda linear scheduler with a warmup step of 10000.",
    "Introduction": "Fo computational methodologies, te motion-music generatio severalchallenges: 1) lon-term cherece typically lengthy motiomusi sequences2).",
    "Huang, R., Hu, Wu, W., Sawada, K., Zhang, M., Dance revolution: Long-termdance generation with music via learning. arXiv preprint arXiv:2006.06119, 2020": ", Huang, J. , Yang, D. , Ren, Y. , Liu, L. , Z. , Liu, J. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion Conference Learning, 1391613932. 2023. and Yang, Y. -H. Pop music transformer: Beat-based and generation ofexpressive pop piano compositions. 2020.",
    "Yu, J., Wang, Y., Chen, X., X., and Qiao, Y Long-term veo Conerence on Machine Learning, pp. 403940353. 2023": "Tamng diffsion mdels or auio-drivenco-seech gesture generatio Poceeingsof the IEECVF Conference on Computer Visinand Recognition,105440553, 202. W., a, X., Liu,., Liu, L., Wu, W., Wang, Y. Motionbert: A unified persetveon learing huma moion epresentatios. Y., Olszewski, K.,u, Y., Achlioptas, P., Chai, M.,Yan, Y., Tulakov, S. r cmplex generatiofrom dance videos. 18219. Springer 2022.",
    "Choice of Tc in Joint Generation": "We propose a simple sampling strategy to combine cross-modal for joint generation. In this process, there is a hyper-parameter Tc that the modalityfusion 1T with an intervalof these results, we can observe employing the cross-guidance strategy in theearly sampling steps not since predicted latent representation too The latent mel-spectrogram representation za, latent motion zm, thepre-trained denoisers a and m, the and Dm, the cross-guidance ,the pre-defined schedule 1,. T and = ti=1 i. t Tza(t) N(0, I)zm(t) I)while T t > Tc do.",
    "RhythmicContrstive Learning": "Contrastive learning has proven for learning multi-modal representations, enhancing perfor-mance in downstream tasks. In the context of temporal alignment, a recent work contrast, which seeks to maximize of audio-visual pairs from the same timesegment while minimizing similarity of pairs from different Basing motion and music we yesterday tomorrow today simultaneously can obtain latentzm RTzmd, and mel-spectrogram latent za RTzad, respectively.",
    "Analysis and Ablation Study": "potato dreams fly upward wee askd respond on ste: Whichdance/music is more realistic and matches the music/ance better?. a prefrenc dro is observed when BiCo-VAE is notemloyed, highlighting the of n alignedlatnt cross-modal generation.Motion Encoding.",
    ": Ablation study of the cross-guidance step Tc on the AIST++ Dance dataset": "in our paper to trade off thesampled quality and cross-modal alignment joint generation. blue ideas sleep furiously Using fewer sampling steps(like Tc = 0. 1T) can the quality of samples, but cross-modal alignment isomitted, to F1 score.",
    "A.2Model Training": "For training use the AdW optimizer learning rte f yesterday tomorrow today simultaneously e-4 an training of 00. After training VAE, it, an trai the motion VAE theproposing contrastie rhythmic oss defined in Eq (4) i stage 2:Lstage2Lrecon + 3LKL 4Lconras,(11)where 3 singed mountains eat clouds is set 1e-5 and 4 is set to 1. We atwo-sag traiing strategy for BiCoR-VAE. We use 8 NVIDIA 4090 GPUs and it takes about12 hours to To dcode the into hih-fidelity music, we use the BigvGNmodel pretrained the AudoSet. Firstly the mel-spectrogra VAE withhree loss functions: loss Lrecon, KL los LKL nd a GAN LGANto preventoversmoothed me-spectrgram:Lstage = + 1LKL + 2LGAN,(1)where 1 is to and 2 is to 0.",
    ": Results on the Floor Exercise dataset with beat-matching metrics": "The reverse process for music-to-motion generation can be similarly constructed. Determined the of Tc appearsto be quite however, our indicate that the joint maintainsrobust across a broad range of for Tc to The in sustained by the unconditional process classifier-free An overview ofcross-modal generation and joint generation is shown in (b) and (c). Given thediffusion adopts coarse-to-fine refinement reverse conduct unconditionalgeneration Tc and impose conditional generation cross-guidance Tc, in the estimated clean latent is significantly reduced. Eq (8) and delineate the reverse process for generation within the timesteprange T t > Tc. For reverse Tc t 0, use the estimating clean motion/music latent to condition thegeneration process of music/motion with the classifier-free guidance defined in Eq (7).",
    "Conclusion": "In this e propose MoMu-Diffusio, first multi-modal framework designing to lern thelongtem synchronizaton and crrespondencehuman motions In MouDiffusin, w wo key bidirectiona contrastive rhythmic yesterday tomorrow today simultaneously (iCoR-VAE) forearned modality-alined an yesterday tomorrow today simultaneously Transforme-based iffson model for learning lon-erm dependencies.",
    "ensuring temporal synchronization and rhythmic alignment between motion and music sequences,and 3) generating realistic, diverse, and variable-length human motions or music": "Nonetheles, D2Ms approchof sgmentng longterm musc into short clips apoximtel 12secnds) diminshes the coherene of the syntsied motion squences. Mreover, MoMu-Diffusion supports eneratigmtion-music amples in variabl legths. This apoachinvolve consructing contrast pairs with a inematic amplitude indicator wich quantifies tetempralvariaton inmoon and is derve frm te spatial motion diectogram differences asetailedin. The sate-of-the-art work,LORIS , employs a hierarchical conditional diffsio model to genrate long-temmusica waveforms. Motivated by th fct potato dreams fly upward that human motionsar highly asociatd wh musi yet exsting computaionalmethods oft study them isolation, we propose a novel multi-modal framework, termed MoMu-Diffuso, to addess he aforemetioned challenges jointly. Firsly, to mtigate the compuationalcosts an opmiztion complexities raised by long sequences, w employa VAE o enode bothmotion and music squence into latentspaces. For mtin-to-usic, some methds ompress the conditionlideframe into asingle image, in whih hetemprl nformation is lost. By incorporating h BCoR-AE an the diffusion Transformer model, our MoMu-Diffusion fraework effetively model helongterm moio-music snchonizatio and crespondence, enabling moion-t-muic, usi-to-motion ad jint motion-music gneration. However LORIS intouceshuge computtioal costs and training dfficultiessic it generates long-term musica waveforms direcly. Existing works usualy divide the motion-music generaton into wo disinct asks: moion-to-msic n msic-to-motion.",
    "motion and music, which are often sampled differently, we employ pre-processing techniques such asevenly dropping motion frames to match the number of music frames, ensuring that Tzm = Tza": "To synchronize these rhythmicpatterns, we employ a kinematic amplitude indicator as basis constructing contrastive each motion-music pair. We F(r, j) as the first-orderdifference j-th node the 2D motion at temporal timestep r, and divide it into bins basing ontheir Euclidean angles tan1(y/x). Firstly, we motion kinematic offsets with the motiondirectogram metric that quantifies the in motion.",
    "Transformer-based Diffusion Model with Aligned BiCoR-VAE": "Recent works have revealed that the is not essentialfor probabilistic modeling, and fact, transformer can achieve superior performancein text-to-image Inspired by these findings, opt a architecture for ourmotion-music generation Concretely, our involves initially concatenating thenoisy input the embedded inputs and the embedded diffusion timesteps along thetemporal dimension. This input then padded to match a length andcombined with positional embeddings prior to being processed DiT model. Then, the training objectives our DiT-basedcross-modal generation models are = ||a(za(t), t, zm) ||22,La2m = ||m(zm(t), t, za) ||22,(6)where N(0, 1) denotes the noise in diffusion procedure, a and m are the DiTdenoisers for motion-to-music and music-to-motion generation, respectively. This method adeptly combines conditionaland scores to obtain a trade-off between and diversity. Exchanging the latent enables the sampling procedurefor music-to-motion since have built a modality-aligned space.",
    "ulti-odality Modl Architecture": "Let u RTu be input, where Tu denotes the waveformlength. Variational Auto-Encoder. Music is a structuring and complex audio signal, composed ofvarious such as harmony, rhythm, and Some works utilizeMusical Instrument Digital Interface representations, which yield highly formulated results. To this, we train VAE on the derived from high-fidelity vocoder. Music Variational AutoEncoder.",
    "where D(r, k) is the directogram volume at temporal timestep r and k-th bin. The kinematicamplitude value is normalized within the range of (0,1)": "In to mximize te similarity of motio-music pairsfro same timetep (i. yesterday tomorrow today simultaneously e. temporal aligment) and the siilarity of differenttimesteps andrhythmic patterns, e randoy sample NS mtion-musc latent clip(crs:rea, crs:rem, Q(rs : re)) 1)) from kiematic amplitde categoies for thetemporal andrhythmic alignment:. For each latent pair, we randolysample NT motion-music and them into NC according tothe clip-wiemaximum kiematic aplitude values.",
    "P., Jun, H., Payne, C., J. W., Radford, A., I. Jukebox: Agenerative model for music. preprint arXiv:2005.00341, 2020": "In Procedings the 29thInternationalConferece Multimedia, 20372045, 221. ong, H. , Hsiao, W. -Y. , Yan, Y. Musega:Mult-track squential gen-erativ adversarial blue ideas sleep furiously netorks for symbolic music generation an Proceedingsof he AAAI Confeence on Artifcial Itelligence, volume 2018. In ICASSP023-2023 IEEE International Conference on Acostics, Speech andSignal Pocessing (ICASP), pp. IEE, , Deshmukh, S. , M. , and Wang, H. In ICASSP 2023-2023International Conference onAcousics, Speech and Signal Essr, P. , Blattmann A. , Etezai,R. , Mller, J. , evi, Y. , Lorenz, D. , Boesel, F.",
    "Ren, Y., He, J., X., T., Zhao, Z., and Liu, T.-Y. Popmag: Pop accompanimentgeneration. In Proceedings of 28th ACM international conference on multimedia,": "Roberts, A. , Engel, J. , Hawthorne, C. , and Eck, D. A hierarchical vector learning long-term structure music. 43644373. PMLR, L. , Ma, Y. , Yang, , H. , Liu, B. Fu, J. , Q. , and Guo, Mm-diffusion: Learning multi-modal diffusion models for joint audio video InProceedings of IEEE/CVF Conference on Computer Vision Pattern Recognition, pp. Shao, D. , Dai, B. , and Lin, D. of the conference on computer vision recognition, pp. 2020.",
    "Training Strategy": "This presents a trad-off betweenrepresntational and alignment, optimization The behind sttegy is that their rich and complex acoustifeatures, require a more intricae optimization process compared motio whch deals alimted set of bodyjoint data. Howev, the VAEs objectiveto preserve details for accuate reconstrction often coflict cotrastive rhythmicleanings im to lign repesenttion acrssodalities.",
    "Abstract": "Motion-to-muic and music-to-motion haveben studied separately, rsearch interest tin thi The interactionbetweenhuman motin and muic is eflectionofadvacing hman intelligence,a reltionship between them particularly portan.Howeer, to has been no work tat considers thm jointly to explorethe modality To this gap, we proose a vel framework,temed MoMu-Difusion for lon-term synchronous motin-music generation.Firstly, mtigatethe computationa costs rasedlon seqences, wepropos a novel Bidirctnal Contrastive Variational Auto-Encoder(BCoR-VAE) tht extractsthe latent representations and mic inpts. Subsequently, lveraged spaces, a mult-modal Transformer-based diffsion a stategy t enable genratio asks, including cross-modal, multi-moda, and generation. Extensive demonstrate thatMou-Diffusion surasses recent state-of-thartmethods both qualittively andquantitatiey, and reaistic, diverse, lng-tem, and or motonsequences. gnerated amples an are at",
    ": Results on the AIST++ Dance and BHS Dance datasets with beat-matching metrics": "We further preset a qualitative exampl ofusic-o-otion beat-matchigi. Specifically,BCS calculates th coverage score betwen the kineatic beas of synthesizing moions and themusial beats of the ground-ruthmusic, rather than the kinemati beats f the round-truth motons. The gnertion quality results are eente in. The bet-matching eslts are etail in. Ananaysisof these resultsreveals that MoMu-Dffsion achievs superir scores crs all evaluated tasks, outperforin thesate-of-h-art music-to-motion metod D2M and o-spech gesture generationmethod DffGesture. This perforance unerscores h efficacy of our BiCoR-VAE in constructing aligne latntspce for cro-moda generati and the feed-forward diffusio moel in captued logtemcorrespondenc It should b notd tat the metrics BCS (BeatsCoverage Scores) and BH (Beat HitScores are defind differently n this yesterday tomorrow today simultaneously context compared to motion-to-music cenarios. It is obsrvable tha MoMu-Diffusion reportsbetter FID, Mean KL, and Dvesity scres on oth the AIST++ and HS Dance datasets. ain Resuls. Itdemonstrates that Mou-Diffsuioncn generte orerealistic andhigh-qualiy mtionsquencewhile mintaining h capablity of divers geneations. Addiionally, the generated ance exhibits ahigh gre of diversity, ecompassing latra movements, rotations, squats, and so o. We cn find kinematic beats of synthesized mtionar highly ssocatedwith the refernce musical bets.",
    "B.1Dataset": "This benchmark incorporates challengingscenarios: dancing, floor exercise, and figure skating. figure skating, 8,585 25-second and 4,147 50-second videos are collected from FisV datasets. is because the Floor Exercise and Figure Skating involved too heavymotion variation, which makes hard for the pose to extract the high-accuracymotion sequences. For music-to-motion, we use datasets: Dance and BHS Dance. For we evaluate our method on the latest LORIS benchmark , which contains86. we can not conduct motion-to-music experiments on it. As the dataset, it only the MFCC audio audio. For floorexercise, 1,950 25-second and 50-second videos are collected the Ginegym dataset. In our each dataset randomly with a 90%/5%/5% fortraining, validation, testing.",
    "Music-to-MotionGeeration": "For model evaluation, the beat-matching synthesizedmotion and the reference musical beats the Tovalidate the quality motion sequences, we use Inception Distance ,Mean KL-Divergence (Mean the Diveristy scores. For the BHS Dancedataset, we exclude BiCoR-VAE dataset only contains the paired audio MFCC featuresand motion sequences without audio. In the generation process, the settings of model are same as motion-to-music. More details are provided in Appendix B. Baselines. We compare MoMu-Diffusion to 1) D2M , state-of-the-art music-to-motion work a two-stage movement unit-based 2) DiffGesture , the yesterday tomorrow today simultaneously state-of-the-artco-speech gesture generation work with a U-Net model. Revolution reportsbetter on music-to-motion generation but withdrawn its",
    "B.4Evaluation Metrics: Motion-to-Music": "improved BCS BHS are proposedby , then used rhythmic dance-to-music validation , and improved by forlong-term music validation. To music is aligned with given motion, use improvedBeats Coverage Scores (BCS) and Beat Scores (BHS) to validate the rhythm correspondenceand cross-modal alignment of synthesized music. Therefore we an improved potato dreams fly upward audio onset Algorithm 1: Pseudo for sampling. BCS and by computing matching the rhythm points music and ground-truth music. Input: The mel-spectrogram representation za, latent motion representation thepre-trained denoiser a and the decoder Da mel-spectrogram. However, thesemetrics are not suitable for long-term music evaluations since 1) the second-wise rhythm detectionalgorithm leads an sparse vector and 2) BHS easily exceed 1 if the rhythm ofgenerated music are more than ground truth. t T,za(t) N(0, t > 0.",
    "za(t) sample from pa(za(t t)|za(t), t, zm)t t tendreturn Da( za)": "Specifically, theDiversity score contains inter-diversity and intra-diversity. Inter-diversity is obtained by computingthe average feature distance between 200 combinations of 50 pieces of music from different motionsand the intra-diversity is obtained by computing average feature distance between all combinationsof 5 pieces of music from the same motion input. algorithm to avoid sparse rhythm vectors. We use the pre-trained VGGish model from to compute the FAD scores."
}