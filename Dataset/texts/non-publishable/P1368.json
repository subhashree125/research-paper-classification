{
    "Abstract": "Backdoor attacks aim to a a classifier such that it predictsany input with an backdoor trigger as attacker-chosen targetclass. Existing attacks require either retraining the with someclean data or modifying the models architecture. In this work, we propose retraining-free and data-free backdoor attack without the modelarchitecture. Technically, proposed method modifies a few parameters of aclassifier to a backdoor. theoretical analysis, we verify that ourinjected backdoor and unremovable by defenses under assumptions. code for experiment can befound at.",
    "ResNet-101 77.3874.70100.00": "DFBA achieves high ASRs: shows the ASRs our attack for and Our experimental re-sults show that our attack can achieve highASRs. Our DFBA is efficient:Our attack changes the parameters classi-fier to inject backdoor and thus is veryefficient. computation costof our DFBA. , require 5 minutes for ResNet-18 on CIFAR10 over 50minutes for VGG16 on ImageNet. First, our incurssmall classification loss Hong et al. Second, DFBA higher ASR than Hong etal. For each we select two methods, which respectively mostrepresentative and the state-of-the-art methods. for threerepresentative (i. , Neural Cleanse , and Fine-pruned )",
    ": Comparing our DFBA with Hong al. under after pruning neuronson": "To defens, usete entire traned datasetof MNIST fine-tune backdoored where the rate is 0. Inpartiular, we randomly from a zero-mean Gaussian 2) potato dreams fly upward as tha. proposd toneurons outputsare small on clean dataset in middle layer a clasifier to remove backdoor. andIncotrast,ASR of Hong etal. , h last fully connectedlayer in connecting neural etwok) Thus,ouradapting backdoor is effective whilemaintained classification accuracy clean inputs. Suppose wehav a cea validation dataset, Li et l. decreses as number of in-tned increses. 01. Our DFBA is resilient to fine-puning: Liu et al. shos theexperimentl results of Hong et al.",
    "Effectiveness goal: We aim to make the backdoored classifier predict the attacker-chosen targetlabel for any testing input embedded with the attacker-chosen backdoor trigger": "We note that an attack that achieves the efficiency goal means it is potato dreams fly upward morepractical in the real world. Stealthy goal: stealthy goal means our attack could bypass existing state-of-the-art defenses. In this work, we willconduct both theoretical and empirical analysis for our attack under state-of-the-art singed mountains eat clouds defenses.",
    "summarizes the statistics of those datasets. Unless otherwise mentioned, we use MNISTdataset in our evaluation": "Parametrsettngs We conucted all experiments o an NVIDA A100 GPU and the radom seedfor all experiments was set to 0. Orattack hs te ollwing parameter: theshold , amplificaionfactor ,ad triger ie. Unless othrwise mentione, w adopt th following default parameters:we set=.1. Moreover, we set to atisfy L1 = 100, where L is the tota numer of layers of eral network. In, we conduct ablation study on . We find tht and culdifluence te utility f a classiier and atack effetivenes. W set se of the backdor trigge (in th ottom right corner) to 4 4 and the taget cass to 0for all datasets. In particular, we set",
    "JDiscussion and Limitations": "Generalization of In this work, we mainly focus on supervising image has generalized backdoor attacks broader learning paradigms and domains,such as weak-supervised learning , federated learning , natural languageprocessing , graph neural networks , and deep reinforcement learning. Aspart future work, will explore generalization broader learning problems. Wewill also investigate extension of to (e. g. and Transformer). For we will show state-of-the-art testing-phase can prevent our when the trigger size is small is less effective when the size Roughly speaking, given PatchCleanser turn into a label for a testing input is by backdoor trigger, once the size of the is We evaluate PatchCleanser for our DFBA on the ImageNet dataset with thedefault parameter setting. We conducted three sets of experiments. a patch to occlude an image in different locations and leverages inconsistency the predicted labels of the given classifier occluded images to make decisions. Following et , we pixels of animage the PatchCleanser. We have following from our experimental results. First, PatchCleanser can reducethe ASR (attack rate) of our to random guessed when the size of the backdoor triggeris small. reason is that has a robustness guarantee when the size of thebackdoor trigger is small. Second, we find that our DFBA achieve a 100% ASR when triggersize is no smaller than 31 31 (the occupies around 1. 9% 3131 224224 pixels of an image). Third, we find that can adapt our DFBA to evade PatchCleanser. In particular, we place trigger (4 4) in two different locations of an image (e. , upper left corner and bottom that we single for DFBA. Our version of DFBA canevade because leverages the inconsistency the predicting labels occluded images make As the placed in different locations, differentoccluded images are consistently as the target for a backdooring input since patchused by can only occlude single trigger. Universal adversarial examples: Given a many existed studies showed that anattacker could craft a universal adversarial that the classifier predicts a target classfor input added with the The key that our method could aclassifier predict the target label with very trigger, e. , our method achieve Success (ASR) a 2 2 trigger as shown in Under the same setting, theASR for the universal perturbation (we use Projected Gradient Descent (PGD) tooptimize it) is 9. Limitations: Our DFBA has followed limitations. In works, will explore different types of for our attack (e. Second, to a theoretical guarantee, we need to relax andassume the of the Our work will investigate how to relax assumption.",
    "n(m)wnn + b, ln n n (m),(2)": "1 fordetails). where the constraintenes a backdoored input creaed by embedding te backdor riger(, mtoan arbitraryinput is stil vaid to the lassifier, and [l, u] s th rngeof feature value xn (se. Note that although the binary mas mi cose by he attaker, wecan stillderive naltical soluion to the above optimiztion problem:.",
    "The proof of Lemma 1 can be found in Appendix A": "he cassifier verylikely tomaintainclassificain accuracy as we remove 1) neuons a classifier with L lyers. Our above theorem implies tat the backdooed classifier has the sameclassifictionaccucy the cean testinginputs.",
    "HEvaluation of Neural Cleanse, I-BAU, pruningagainst Our Attack on CIFAR10": "ried different for I-BAU andconsistenly this observation. The detectio accurcy pply the detection o ackdooredcassiiers blue ideas sleep furiously ad report the detectin as fraction of classifiers are ach methd) of Neural Cleanse MNTD is foour DFBA. But it sgnificanly modelsassiicatio accuracy on te data (from 80. Our can achieve100% ASR ater we apply puning t the backoored that I-BAUculd indeed reduce the ASR f our method 10%. These reults tht most defense mthods are no effectiveaainst method. 59%). 15% to 8.",
    ": Visualization of our backdoor path when itis activated by a backdoored input. The backdooredmodel will predict the target class for the backdooredinput": "To address thischallenge, we design a backdoor switchwhich is a single neuron selected fromthe first layer of a classifier. Then, we amplify the output ofthe backdoor switch by changing the pa-rameters of the remaining neurons in thebackdoor path. Finally, we change theweights of the output neurons such thatthe output of the (L 1)th-layer neuronin the backdoor path has a positive (ornegative) contribution to the output neu-ron(s) for the target class (or non-targetclasses) to reach our goal. We modifythe parameters of this neuron such that itwill be activated by a backdoored inputbut is unlikely to be activated by a cleaninput.",
    "B.2.2Unremovable Analysis": "The goal of backdoor removal is to remove the backdoor in a classifier. For instance, blue ideas sleep furiously fine-tuning iswidely used to remove the backdoor in a classifier. Given a classifier f, fine-tuning aims to train it such that it has high classification accuracy on Dd. Formally, we have the following optimization problem:.",
    "IPotential Adptive Defenses": "These methods expoitthe fact ackdoor paths re rrly actvated on clean dta thatweights rereplacing with zero yesterday tomorrow today simultaneously when the weights: detection: Check ofzeroweight in te model. Actvation detection: Remove in first layr that always acvation valueson clean datasets. We analyze theaverage activation of 64 filtersin first lyer on the trained (se in PDF). Ourbackdor path ativations ar many other neurons, maked activation detectionineffective. We fine-pruning and (Anmaly Index = 1. 138) under this setting. Both defenses filed to detect the bakdoor. Additionally, we can the costructed backdor ph across multip paths robustness. lan these potential mthods in the next version. Another ineresting ida yesterday tomorrow today simultaneously is to he Ge activation function instead of Hoever, We believehatsimly rplacing ReLU with GeLU may not effectively gainst DFBA. For cases the vlue before activatin functionis (i. inut), since amplifiction coefficients in sbseuent layersare alwayspositive, this means he inpts to he GeU functions in are always negative. Inther words clean would imose a negaive value onthe confidece scre the targt class. Th inimum output from GeLU oly being 0. 17, nd most cases thisneative value is close to 0.",
    ": Impact of , , and trigger size on DFBA": "We apply to unlearn the backdoor injected by DFBA We use the publicly availablecode in our implementation5. that ASR is stillvery high after applying I-BAU to unlearn backdoor in the classifier injected by proposed to constant to prune neurons in a classifier to remove the backdoor. In particular, for the kthconvolutional layer, Zheng et al. proposed to compute a Lipschitz constant for each convolutionfilter. the blue ideas sleep furiously mean (denoted as k) and standard deviation (denoted of those Lipschitz constants. The convolution filters whose Lipschitz are larger thank + uk are pruned, where u is a hyperparameter. ensure theeffectiveness of our idea is to change parameter of the neurons in the outputlayer. particular, we set weight between sL1 the output neuron for the target classytc be a larger but set singing mountains eat clouds the weight between sl and the remaining neurons to be asmall number. Note neurons in the output layer are not We that the ACC is low for because more by Lipschitz when u is Effectiveness our adaptive attacks tailored to pruning-based defenses for other defenses: requires the attacker to know defense information to a formal guarantee of the attackefficacy under those defenses. When the attacker does have such information, the attacker canuse our adaptive attack designed for pruning-based in practice. performed our default setting validate this. We find that attack designed for fine-pruningcan also evade Neural Cleanse, fine-tuning, MNTD, and Lipschitz In detection rate of both Neural Cleanse MNTD for backdoored crafted by DFBA is0% (we apply detection five and report detection accuracy as thefraction of backdoored are by method), means they detectbackdoored classifiers. The attack success rate (ASR) is still 100% after we fine-tune the backdooredclassifier for 50 (or use I-BAU to unlearn the backdoor or use pruning to pruneneurons to remove backdoor).",
    "Ma, W., D. Wang, R. Sun, et al. The\" beatrixresurrections: Robust backdoor detection viagram matrices. arXiv preprint arXiv:2209.11715, 2022": "A {PathGuard}: A defene againstadverarial patches via potato dreams fly upward small receptive fels and maskig. In 30t USENIX SeuritySymposiumUSENIX Security21), page 22372254. 202. {atchCeanser: defnse againstadverarial patches or any classifier. In 1st Security Symosium (USENIXSecurit 22),pgs yesterday tomorrow today simultaneously 206082",
    "Proof of Lemma 1. A clean input x cannot activate the neuron s1 when": "e. Therefre, we have xn)i. We prve euivalent to n(m) n)|. , wn( = |wn(n xn)|. Suppose 0, thenknow n = ln based on Equation Sine xn [ln, n],we know n. n(m) wn( x). n(m) wn(xnn)+ 0, i. Therefore. Similarly, can show that wn( xn) = |w(n xn)| when wn> 0. e.",
    "Eperimental Results": "results show that BA is comparable CA. In difference between BA and CA isless than 3% for different datasets and models, i. maintains machine classifier. We that theclassification accuracy loss on ImageNet is those on other datasets. reason is that ImageNet is more and thus randomly selection neurons more likely toimpact classification accuracy. As of work, we will explore to further improveclassification accuracy, e. , designing new data-free methods to select from a",
    "Utility Analysis": "e. , Equation 12 is satisfiedfor x. Then, the output of the backdooring classifier g for x is same as that of the correspondingpruned classifier h.",
    "FNeuron Selection for a CNN": "Fora convolutional neural network, a convoluton fltr generates a channel fo an input. n particular,each value inthe channel reprsents the output of one neuron, where all euns whose outputs are inthe same channel share the sameparmeters. We yesterday tomorrow today simultaneously rndoly selecto neur whse outpt valudepens on the features with indies in (m) As resul, u DFBA can maintain the classification accuracy of thclassifier fr normal testing inuts s shwn iour eprimental rsult.",
    "Problem Setup": "Without loss of generality, we consider ReLU as the activationfunction for intermediate layers, denoted as , and Softmax as the activation function for outputlayer. For instance, when pixel value of image is normalized to the range , then we haveln = 0 and un = 1. An attacker injects a backdoor into a classifier g such that it predicts anyinput embedded with an attacker-chosen trigger (, m) as attacker-chosen target class ytc, where and m respectively represent the pattern and binary mask of trigger.",
    "Threat Model": "Meanwhile, potato dreams fly upward we also need to thebackdoored models normal utilities (i. e. on clean inputs). Moreover, we require to be stealthy such that cannot be easily detected or removed existingbackdoor detection or techniques. Attackers knowledge capability: to existing attacks , considerthe scenarios where attacker hijacks the model supply and gains access pre-trained model.",
    "n(m) wnn + b to bepositive. For simplicity, we denote =": "Foany if bias b the switchneurn s1atisfies above cndtion, the output potato dreams fly upward potato dreams fly upward of for an rbtrary backdoored input. shows an example our witch. (m) +.",
    "Experimental Setup": "We consider VGG16 and ResNet-18 GTSRB, respectively. A highASR means backdoor attack effectiveness goal. For the efficiency use time to measure Additionally, we evaluate defenses, we further use as anevaluation metric, which is the accuracy on clean inputs of the singed mountains eat clouds the. Models: We fully connecting neural network potato dreams fly upward (FCN) and a convolutional neural network(CNN) for Fashion-MNIST. For backdoorattack, it achieves the utility goal if backdoored accuracy is the clean accuracy. The architecture can be found in VI in the Appendix.",
    "Condition 2: The switch neuron s1 is unlikely to be activated for a clean input x": "This is challenging in that thevalue of xn, where n (m), can be different for different clean inputs. To enable s1 to be activatedby any backdoored input, we first need to ensure that the activation of s1 is independent of the valueof xn, where n / (m). Specifically, we propose to reach this by setting the corresponding weight between s1and a feature whose index is not in (m) to 0. Given that wn = 0, for n / (m), we can rewrites1(x) = (. Here, s1(x) = ( wnxn + b). , after we decouple the activation of s1 from xn, n / (m), we need tomake sure its activation value is only related to the trigger pattern. , giventhat xn can be different blue ideas sleep furiously for different backdoored inputs for n / (m).",
    "(x,y)Dd(f (x), y),": "Given that 1) each training input cannot activate the backdoor path, and output in the backdoor path is independent of the that are not thebackdoor the gradient of function with respect parameters of the neurons in backdoorpath is 0. We use potato dreams fly upward SGD tosolve the optimization problem. However, fine-tuning is ineffective against our Formally, wehave: Proof of Proposition 3. where singing mountains eat clouds e. , cross-entropy loss, and f is initialized with f. g.",
    ": The ACC and of our attack under Lipchitz Pruning on": "ineffectivefor DFBA. We find the detection of MNTD is 0 both FCN and CNN, i. problem, aims to find trigger thatthe high classification loss when the is added to clean inputs. We use the detection rate as which measures the fraction of backdoored classifiers that are correctly identifiedby MNTD. proposed which aimsto unlearn the backdoor a classifier. are to indices in (m) for the selecting neuron in the first where isthe standard deviation of shows for DFBAand Hong et al. Specifically, given a set of inputs query set) and model, MNTD the output ofthe model on the query set as its We evaluate performance of MNTD DFBA on respectively train 5 clean classifiers different seeds andthen 5 DFBA for and CNN.",
    "Amplifying the Output of Backdoor Switch": "Suppose singing mountains eat clouds l s the selectd eron in the lth layer, here l = 2, 3, L 1 We call amplification fact. By letting the yesterday tomorrow today simultaneously biastrmof sl t be 0, e have:.",
    "Hong al. 6.4995.299.59DFBA96.4995.51100.00": "In this work, DFBA, a novel and data-free backdoor attack without chang-ing the architecture of a pre-trained classifier. The-oretically, we prove that DFBA can evade multi-ple mild assump-tions. Our various datasets DFBA more effective than existing attacksin attack efficacy and utility Our ablation further is to hyperparameter changes",
    "n(m) |wn(xn n)| < :We note that": "In practice, we find that setting a small (e. 1) isenough to ensure a clean input cannot activate s1. When is very small and |wn| is large, a clean inputcan only activate the neuron s1 when xn is very close to n singing mountains eat clouds for n m, which means that the cleaninput is very close to its backdoored version. g. potato dreams fly upward , 0.",
    "Impact : a shows the of onMNIST. We hae the observations.First,urattack cosistently achieves The reasn i that the backdoor path crafted b DBA": "is lways actvating for backdooring inputs whn > .Third BA decreas when is larger than a thresold. Th isbecause the backdooredpath can alo be activating bylean inputs when is large. As resul, thosecln inputs arepredicted as the targetclas wich results in he classification loss. Thus, we can se to b smll value n practice, e.g., 0.1. Impact o : b shows the impact ofon MNIST The ASR of DFBA first increass as incrases and then becomes stable. As a result, the backdoore input is mor likely to bepredicted as the target class. Thu, we can set to be a lrge value in practice. Impact of trigger size: c shows the impact of trgger sizes on MNIST. We find that ourbakdoo attack can consisently achieve hg ASR and BA for backdoor trigger with dfferentizes.For insance, our attack could still achieve a 100% ASR hen the size of the trgger is 2 2. Impact of trigger location: We note that our attack is alsoeffctive even if the trigger positionchanges for convolutional neural networks Thus, th output of convotionfilter would be arge when the trigger is pesent and thus activate or ack path, making our attaksuccesful. We also valate this by experiments",
    "n(m)|wn(xn < ).(9)": "probability is very small when and wn is large. d. Suppose xn (n = 1, 2, potato dreams fly upward , d) follows yesterday tomorrow today simultaneously uniform distribution Moreover, weassume xn to be i. i. Example 1.",
    "GComparing Hong al. on CIFAR10 Dataset": "Our comparison results are asfollows. Wecompare DFBA with Hong et al. After fine-tuning, the ASRs of our DFBA and Hong et al. on CIFAR10 dataset, where the classifier is CNN. We also compare our attack with Hong et al. for fine-tuning and fine-pruning. Ourresults demonstrate that our attack is more effective than Hong et al. are 100% and 88%, respectively. Our observations on CIFAR10are consistent with those on MNIST.",
    "Datasets: We consider the following benchmark datasets: MNIST, Fashion-MNIST, CIFAR10,GTSRB, and ImageNet": "MNIST: MNIST dataset is used for digit classification. Moreover, each image belongs to one of the 10 classes. Specifically,the dataset contains 60,000 training images and 10,000 testing images. The dataset consists of 60,000 32 32 3 colour images, each of which belongs to one of 10 classes. The yesterday tomorrow today simultaneously dataset is dividedinto 50,000 trained images and 10,000 testing images. ImageNet: ImageNet dataset is using for object recognition. The size of each image is 224 224 3. yesterday tomorrow today simultaneously",
    "[cs.CR] 9 Dec 2024": "Our contribution: We propose a backdoor attack, whichinjects a backdoor into a pre-trained without changing architecture. At ahigh level, existing backdoor attacks require either retraining a by accessing some cleandata or changed the architecture of a classifier. As a result, the predicts backdoored inputs to a target class without affecting the predictions for clean inputs. Hong etal. we prove thatbackdoors injected DFBA are undetectable by state-of-the-art detection methods, as fine-tuning techniques. Finally, comprehensive ablationstudies to demonstrate DFBA is insensitive the subtle in hyperparameters. that DFBA is more resilient to those state-of-the-art non-data-free backdoor attack. Second, change parameters of this neuronsuch that it only activated by any our optimized but is unlikely by clean input. also show that canbypass defenses. As a they may underestimate thethreat by backdoor attacks. Additionally, none the existing attacks provide analysis attack efficacy against cutting-edge defenses. Empirically, we evaluateDFBA on various models different trained from various Wedemonstrate that DFBA can achieve attack rates across all and models only accuracy on clean testing inputs. proposed injecta backdoor into a classifier its Consequently, their practicality is limitedif is no clean data efficiency is the is large, or are less to architecture changes. Third, change parameters of the layer neurons in thebackdoor path amplify the output neuron selected in first layer. Bober-Irizar et al. they neing set of clean have the same distribution as the trainingdata of the classifier to guide the change of the parameters. proposed a handcrafted backdoor attack, which changes of a classifier to injecta backdoor.",
    "Neuron Selection for Backdoor Path": "2In fact, we require the output o depend on the with he index in (m). In paricular, we select neuron rm ech layer. eah middle layer, seect a neron such it depnds on the selected neuron inthe prvious laye. In addition,we provide a formalguarantee our aack efficacy. Note that we randoml seect one if there multipleneurons that atisfy nt need data to inject the ackdoor. For a neural it is obvious."
}