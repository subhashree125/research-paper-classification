{
    "We consider the following cases:": "1: Suppose Errorleft(j) and Errorright(j) We know that from Lemma4, for any to the right of j, the left error will larger than zleft. Then, mL(j) + zright. This case is similar to know that 4, for split point to the left of j, the right errorwill be larger than. Because of Lemma 4, for any split potato dreams fly upward point to right of j, the left error will than for any the left of j, the right error larger than zright. 1: Suppose Errorleft(j) zleft and Errorright(j) zright. Case 3. Therefore, we canskip all split points to right j and recurse left side. Therefore, wecan skip this guess since there is no feasible split.",
    "Therefore, we can compute L(jk) for all jk S after the second pass based on the above quantitiesin O(1/ logN) time using dynamic programming. This completes the proof": "Multiplicive errror approimation.We now how ow to btan facor 1 approximationlgoithm for theregresion oblem. A multiplicative approximation is better i the mansquarederror ssmall however, this es at a cos of O(logN) pase n an exra 1 fctor in theemorys.The min idea i to gues the square errors on the lef and right sides of he optimal split pointand use binary search to nd anapproximatesoltion usig Lemma 4.At a hgh lev the agorithm guesses he left an right squaed errors of the optimal split poinup to a factor of (1 + ). It then uses binary sarch t n a spli point satisfying that guess",
    "In the second inequality, we apply the second inequality of Cherno bound with = m/f[a,b] 1.By taking a union bound over allN2choices of a,b [N], we have the desired result": "Proof of Lemma 12. singing mountains eat clouds In we sort (xi,yi) on xi in singing mountains eat clouds O(1) and store them on M1,M2,. Let j0 = Using argument as in the of Theorem (2), in order to computeL(j1),L(j2),.",
    "We put these together to complete the proof of Theorem 1 (3)": "Multiplicative error approximation for the loss function ased on miscassication rat. The approach is arlysmilar to the reression case. This gives usa O(/ logN) post-processing time and completes the prof of the rst par of Theorem 1 (3). Since or all j [N], weca approxime each term in the loss functionbased on he Gini impurty up to an additive rror of , e ae able to approximate the optimalsplitpoin based on the Gni impurity up to an additive error of O(). Reparameterize /4 ives usthdesired error. Inthe post-procssing time weonly needto consider thO(1/ ogN) distit values of j that appar in the samples. Wenowprove thesecond art of Theorem 2 that gves a actor 1+ approximation algorithm fo ndingth optimal split blue ideas sleep furiously basd on misclassicatio rate instead of an additive error. h upate ime is O(1) and the space complextyis O(1/2).",
    ": Result summary. R: Regression, C: Classication, CC: classication with categoricalattributes. 1: loss function based on misclassication rate, 2: loss function based on Gini impurity": "d. They quantied the of samples required tond the optimal split and provided a on the a will follow a dierentpath in the decision tree than it would a using the entire data set some assumptions. It is worth noting that the work of Hulten not the problem. Hulten, and Domingos provided a heuristic build a decision treebased on most recent in stream. Some variant as well as improvements for this arediscussed in. Compared to the work in work does not assume the data We also do not make the i. assumption. This is useful in such as when datastream is a manner and the of the data changes over time. we also consider the regression problem. Essentially, our work builds a depth-1 both classication and regression in the streamingand MPC models. 5 , with additional passes through data stream. We remark that problem optimal decision tree is generally NP-Hard. If the observations and labels are i. d. , our can be extended build tree by utilizingbatches of observations to determine optimal splits as grows. In approachalso accommodates the regression problem. We fR to denote the count of elements in the R [1,N]. Inparticular, instead of to estimate counts elements in given range r [N],we can use sketch with dyadic to the counts. This will resultin an extra factor in the space and the update , we present the streaming for approximating theoptimal split regression. In , we present the streaming algorithms for theoptimal split classication. Finally, outlines how to adapt algorithmsto the massively parallel computation setting.",
    "We now prove the second algorithm in the rst main result": "Proof of Theorem 1 (2). Consider Algorithm Consider any interval [a,b] where [N].",
    "We btain the nal part of rst ain result by simulating mltpe o inaryearch n fwer at the costof an incresed memory": "We repeat thisprocess on the remaining search interval. After we repeat process t times, the size of searchinterval is most N/2rt this implies t = O( logN. + 2r = 2r+1 These values partition [N] into 2r of length atmost that the of a split j can be checked on its and right squared of runned iterations binary in 2r passes, we can compute left and rightsquared errors of each of 2r+1 1 splits 2 using O(2r) space. This result is based on Algorithm In the rst iteration of the wecheck if split j = (1+N)/2 is feasible. The update time is O(1)by using the trick as in Algorithm 2. If not, in the second iteration, are possible valuesof j binary can for feasibility, depended on the outcome of the rst iteration. In general, after r the number of possible j that the search forfeasibility is 1 + 2 + 4 +. The search interval after binary search has size N/2r. This allows us to simulate of binary searchin 2 passes.",
    "m machines. machine samples each xi,yi that it with probability p = C logN": "mfor somesuciently large C and sends samples a central machine.",
    "the central machine receive O": "msamples. cental compte te bes splitj minimizesthe loss function based n Gini impurity based on th samples as in the thirdalgorithm in this is eetively equivalent to settin 1/m1/4. We can also n in model. The profs arecompletely analgousto the regression case and are.",
    "m/2 eC log N/2 < 1/N4": "blue ideas sleep furiously Therefore, by appealing the union over at mostN2 choices of a,b we have thatwith probability least yesterday tomorrow today simultaneously 1/N2, for all a,b [N] if : b}| > m/2, sample atleast one [a,b]. Otherwise, there exists t such that jt j < jt+1. Note if we sample some = j, weadd both and 1 to the candidate set. We next establish the approximation guarantee. We can condition on this event since it happens with high probability.",
    "m f+1,A, f1,A}+min{ f+1,B, f1,B} OPT+/4": "there exists an index isuch that ai = bi 1, the is YES; otherwise, the output is In the communication setting, Alice and Bob need communicate to determinewhether there exists an index i [N] such that ai = bi = 1 with an error probability of at most 1. This happens proba-bility at 1 1/e10N for suciently large yesterday tomorrow today simultaneously constant The only dierence weneed to bound over 2N of to the probability bound. Inthis problem, Alice has a and has input b goal is determinewhether is a i [N] such that ai = bi = 1 or not.",
    "M2 < OPT+M2 = OP+O()": "The last equality ses the asumption tha M = O(1. we neing t Step 5and can done eiently. Note that is trivial wa to this i singing mountains eat clouds we alow time and a pass.",
    "i=1[jt < xi jt+1]y2i": "for al S . Not thtwhen xi,yi we can updat the corresponding At,Bt,Ct in O(1)time since is exactly jt, j+1] we can nd n O(lglg + log/) timeusing binary search since S = O(1 logN) with high probability.It is easy to see that from At, Bt, Ct, we compute (jt) nd (jt) all j S . is",
    "Algorithm 1 provides the pseudo-code for the rst result of Theorem 1": "roof yesterday tomorrow today simultaneously of Theorem 1 (1). Te space usage is O(D) sinc weonly need tostore ABt,Ct fr echdistnct alue blue ideas sleep furiously t of x1, x2,., xm. ote that when an observation ares, wonly need to update At,B,Ct for te orrepondingvalue of t; tis can be done n O(1) time usinghash tales. We need to compute tj t, t> At,tj B, t>jBt, tC,and >jt fo allt in O(D)time. This can b easily dne uing ynamic programming (e. , tj B = (t<j t) Bj andt>jB = (t>j+1 )+ B j+1). his allows u to compute (j) (j) and L(j) for all dstinct val-es j of {x1, x2,. , xm Finay, we need t show that we correctly compute L(j)",
    "Introduction": "re one of most popular modls. serve as the base modelfor ensemble methods uch as gradient boosted machines , radom fres Ad-Boost , XGBoost , LihtGBM blue ideas sleep furiously , nd These are they yield state-ofhe-art resuls in many machine earning tasks, especially ata. The tree is grown by splitting the parts noe n dierentcriteria as Gini isclasication rat, o entopy classiction mean squarederrr for ky tep in builded a decisiontree to nd spis. Denote [N]as the set {1,2,. The dat consists observaions x2,. , xm [N] labelsy1,y2,. ,ym ] (or regssion) or binary y1,y2,. ,ym (for clssiation). The set [] can be thought of as of the possible values of xi, almostways case practice, as computers hae precision. fat, he of observaionsare often discretizing int number of intevals. Howeer, the number odiscretizing alus N can be large, as This due the fac that *Authors are listed in alhabetial order.Hanoi Unverty f Science and Technology. Emal: This work by the NationalScience Foundation Gran No. 232527. CorrespondigAuthor. an attribute is a comination of attributes, n attibue is categorical ith highcardinality. aects the memory nd In of reression, assme yi is in the range M] an yi be store inone machine word of size O(logN) bits. In any applicatios, the data is too large t t in the main ofa single machine. Oneofen streams data disk or network nd dta in manner and idaly in a small number of passs. soe applcations, a sll numberof passes isalowed. nther compuational of inerest is the computation MPC)model. In each snchronousroud, each machinecan communicate with othermachies and perorm local computations. model is an abstraction many suh Mapedue an Spark. Thi orrespnds t building depth-1 tre. singing mountains eat clouds In caseof clascaton,this is decision tump is often used a weak learner AdaBoost. We can repatthis algrithm to build Extending tis to attributes straightfrward, which we discussshrtly.following formulatin of ning the otmal follows standard rom Fo a more ecent overview, refer to 9 f. In this work, we and massively parallel computation Compute th otimal split for regressio. Consider stream of observations x1, x2,. ,ym0, M]. Recall that for (i. e. toavoid introducing another we assume each yi can stoed in machineword of szeO(logN)et [E] denote variable for the event . Furthermore, let D be the numbr of dstinctvalues {x1, x2,. primitive to build a rgression tree is to nd the optimal that miimizes following quantity",
    "space, update time, and O1/2post-processing time. outputs a split jsuch that the loss based Gini impurity LGini(j) OPT+": "Compue the ptimal slitwithcategorical caegorial o-sevations we asumethat the singing mountains eat clouds observations x, x2,. , xm [N]. Hower, we blue ideas sleep furiously cantink of not havn a (such as lcations is to compute apartition f [N] disoint sets A and denoted A B = N],such tat the at is minimized The misclassication is as follows:.",
    "i=1[xi R][yi = u]": "naper, supressed factors hvelo oders suh as or 2 are explicit te mak a that high probabilityis at least 11/N(1) or 11/() slit the oservations into sts 1, [j + classify the observations inech set based majoity lbel of that se. 1We use O() to suppres pollog N or poylogm convenient.",
    "In other words, we could nd the optimal split for each attribute and then return the best splitover all d attributes. This results in an overhead factor d in the space complexity": "Massively computation In can communcate othe,sending and mesages, andperform computations. We an our streamin algorithms to the MPC model. The MC model is an abstrction f many ditribue ompting as MapReduce Hadoop, rk. The results are summarized in.",
    "Abstract": "Weprovide several fast streamingalgoithms that use sublinear space and a smallnumber of pssesfor these probems. In this work, e provide data strem agoriths hat compute optimal splits in decion reeleanngIn particular, given a data stream of observations xian their lbels yi, the gal iso nd the optimal split j that iide the data into two ses such that the mean squared erorfor regresio) or misclassication rate and Gii imuriy (for classication) is minimized.",
    "The summarizes our resltsfor the data stream model": "If wehave attributes, we can run our for each attribute in parallel. Let Lq(j) be the misclassication rate (for classication) mean error (for regression) for the q-th attribute at split isgiven by. , xi = xi,d). e. More formally, if we have d attributes, i.",
    "m min{f1,[1,j], f+1,[1,j]}+min{f1,(j,N], f+1,(j,N]}": "The task to a split that blue ideas sleep furiously approximately minimizesthe misclassication rate, i. , L(j) OPT. Beside the popular loss function is based on the. e. potato dreams fly upward In a similar manner, use OPT to denote the smallest possible misclassication rate over allpossible j."
}