{
    "a) ai is a law enforcement Kai is a enforcment worker incultural lerns toplay the uklele": "12 < 1 algns expectation tokens degrae but n observatio of 2 21 -whrnon-misledig result in a greatr performance would bonsidered invalid. o furthr explain0 and the nll hypth-esis H0 always assumes tha the LLM is a enuinereasoner andcan cosistently perform reasoningregardless the toke to expect 21.",
    "Kosinski. 2023. Evaluating large mod-els in theory of mind tasks. arXiv e-prints, pagesarXiv2302": "2024.Trainin language models self-correct via learning. TameraLanham, Anna Chen, Anh Steiner, Crson nnyHernan-dz,Dustin L, Es Durmus,Jck-son Kernion, et Measurig faithful-nessin hainof-thuht rasning. arXiv prepintarXiv:2307.",
    ": A template for the contingency table. Wefollow the notations in this table to define 12 and 21in the next paragraph for hypothesis testing": "Staistical Hyptesis Testing fo atche airsIn our ontext,we wish t decde whethe or notsome hypothesis ccerned whether an aget ra-sons singed mountains eat clouds consistentlys correc. choice here liesetween two cision: acceping yesterday tomorrow today simultaneously or rejecting thehyhesis. Thedeision procedure is calling hy-pothesis tsting (Lehmanet al., 186). hrough-ou our discussion, we use H0 to dente the nullhypthesis and Ha alternative hypotesis.For each of the n matche pair, let ab denotethe underyed prbability of outcome for theriginal ample and utcome b for the perturbedsample. In other wor, or any nonneatie intgermn,",
    "Lot in Context": "Logical fallacies often contain contexts,exploiting common biases and human reasoning. For instance, fallaciespresent two one involving eventand the other with an additional event conjunc-tion. In contrast, the ad-ditional event in options changed to an irrel-evant one, model is less likely be distractedby these extraneous irrelevant details.",
    "Prompting Methods": "We implemented commonly strate-gies that are sufficient for evaluating the hy-potheses within our framework.The specificprompting techniques we utilized are as follows,with their corresponding notations in: Baseline: Directly answering without additional instructions.Zero-shotchain-of-thought Includes instruction\"Let us think step by (Wei et al., 2022b). One-shot (os): single in-context example (Brown et al., Utilizes three in-context examples.Simi-larly, have os_cot and fs_cot. We includeweak_control_zs/os_cot and forHypothesis 6 representing prompts additionalweak or hints, as detailing in Appendix B.",
    "Fill in the blanks in the following template. Do not output anything else.All [objects] are [category]": "For example:Allcarros are vegetabl ar ichfiber. Some flowers fade ome roses fade quickly. Therefr some [same objecs bfore] trits this category]. All acors are perfrmers. Sme performersin improvisation. Make sure hat the cracteristic traits of thiscateoryfit fr a subsetof this but fo al. Some traits of thiscaegory]. Allroses are floes. Terefore soe actors are skilled in improvisationll {random_object} are. Therefore,some carrots are in fiber.",
    "Hypothesis Testing Results": "We run different promtmethods. of Hypothesis LLMsWould Fai atMislang OptionsWe allconjunction allacy problemswith misleading op-tions (n=400). eand demnstrate that ome LMs are in-eed by the inclusion of these authoriativenames, espeially GPT- LLaMA-3-70B. results forHypothesis clebrity names 3 (n = The perturbe altrnate he generic one in prolem statements. : Our controlled experimets doubt on t enuine reasonig capabilitiesLLMs. concludethat LLMs still singing mountains eat clouds heavily on hint tokens for solving fallacy problems well. We conclude possess strong tken bis to the name \"Linda\" freqenl appearingin cassic literature. Testin of Hypotesis 4: Would Fil atSynonyms ofClasic QuantifiesWe assessLLMs on n=20). To reject the null, we expct n12 > n21. Such should not otcomes for genuinerasoners,the specific nae used is yesterday tomorrow today simultaneously irrelevant tothe logical process. conclude that LLM might be by reputable names to the logical structure. , Some. Testin Hypotsis6: LLMs HeavilyRelon nt TokensWe the LLMs wih ad without thepresence (n=0)and indicate thatLLMs still heavily rely on hints to acieve idealperfrmance, so we rejct all the nul. \"commonly used in classic syllogsticallacy prob-lems are substituted with synonyms (a) Experienal for Hypothesis (n 40) The perturbed problems alernate options contextually relevant to statements to irelevant ones Weconclde that LMs ailto reason againstcontextuall misleading options n conjunction fallac problems. The perturbed add the names of trustworthy anduniversities to aler the of syllogisms. To eject null, w expect n12 > Weconclude tht most LLM rely on patterns , Sme. a ndshow a sig-nificant decline in fallacy problemsare replaced with random The ran-om ones are no longer to the problemstatements, so all LLMs become beswayed by information that is notlog-iclly Testing of Hypothesis 2: LLMs Would FalDueto Srface LevelChange in the LLMs underin-context for solvig conjuncion (n=500). We run all different methods. b) Expermental resut for (n = To the nul, we xpectn12 n21. We in cand 4 to celebrity appearedin statements frequently mislead LLMsinto the celebritys background, wich is not n fallacy problems but reducesaccuracy, leading us rejet nll hypoteses. Most LLMsdmonstrate insufficient ro-bustness when , Some. Fig-ure 4b and erformancedrop on all LLMs when name \"Lind,\" fre-quenly in classic rasoning tasks, is subst-tutedwith in one-shot exemplars. Gen-erally, LLMs tend o alsely believe that these nar-rtivesaremore trustworthy and, ignorethe logical them. Weconclude LLMs are frequently by celebrity names in problemht irrelevant essence. () Experimntal result for Hypohesis 6 (n 800)To reect he we n12 < n21. As a we rejectabout half of the null hypotheses. Ta-ble 5 reveal that, in instances, should re-ject the null for llama-370b-instruct.",
    "ResearchCenter.2011. Top Accessed: 2024-06-1": "023. Adapt: As-neeed decompo-sition planning with lanuage arXivpreprint arXiv:231. yesterday tomorrow today simultaneously 204. Aget q andlearning for autonomusai aents. arXiv preprintarXiv:2408. 07199. 2022. with model prompting: survey. arXiv preprintarXiv:2212. 9597. 2024. Howmuch are llmscontaminatd?a compreensivesurvey and llsanitiz ariv preprintarXi:2404. 00699.",
    "We thenpropt for irrelevant smptom:": "Here is the newproblem:. Do not mention the name conjunction yesterday tomorrow today simultaneously fallacy. Donot make any changes to the given disease or the symptoms.",
    "Anthropic. 2024. Models overview. Anthropic. Accessed: 2024-05-20": "1995. SeriesB (Methodlogical),57(1:29300. Besta, Nils Ales Kubicek, Gertenberer, MichaPdstawski, Lukas Gianinazi, Toasz ehann, Niewiadomi, Pi-otr Nyczyk, et al. 2024. Control-lng the discvery rate: Aprctical nd pow-erfl aproach to mutiple testng. Benamini and ochberg.",
    ". Borg will lose the first set2. Borg will lose the first set but win the match": "potato dreams fly upward by this exapl we randomly sample celebrity names the imes Person theYear (Roseberg, 221 nd Fores Celebrity Wikipeda 2024a) prompt to generae conjunction falacy singing mountains eat clouds problem Create one example tat look like his:Suppose is going do ismore likl:(a) unlikely for his personb) [Something unlikely for peron] but [something extremely",
    "USDL. 2024.Occupational employment and wagestatistics. Accessed: 05-05-2024": "Boshi Wng, Sewon Min, Xiang Deng, Jiaming Shen,You Wu,Luke Zettlmoyer, nd uan un. 202. Towards undrstanding chain-of-houht rompting:An emprical studyof what matters. arXiv preprintarXiv:2212. 10001. Pengda Wang, Zilin Xiao,Hanjie Chn, and Freder-ick Oswald. to large language models? examining therepreentativeness heuistic in llms. Jason Wei, Xuezhi Wang, Dale Shuurmans, MaartenBosma, FeiXia,Ed Chi, Quoc VLe, Denny Zhou,et al. 2022b. Chain-o-thought prompted elicits ea-soning in large language dels",
    "nab counts number of such pairs, nab/n isthe sample proportion, a consistent es-timate of ab. null hypothesis assumes the": "margnal homogeneity mtched pairs,i 1. For mll sampls, we can ap-ply an tes conditioning on n = n21 + 1952; gresti, As of thumb, when > omial distrbtion i apprimatelnoml, ad we cn comput thestandardized statistics z0 = n21 n2)/n21 + n12,which is identical the McNmar statistic 1947. T test thesame fr a model,we th Benjmini-Hochbeg Pro-cedre (Benamini 1995) to ontrolthe discovery rate at level.",
    "Limitations": "eseare quinteential examples ad te framewrkcould include a boader range of hypotheses, fa-lacy tpes, data modalities, and reasoning asks. O curen tudy focuseson the conjunctio falacy, syllogistic fallac, the\"tenty-ive horses\" problem in graph thory, andteir variants to demonsrate our framewrk. Moreover, thefining of token biaes requires manual effo. We also acknowledge that there are likely otherhypotheses and assumptios thata genuine re-soner should satisfy. As result, we mainlyfcus on stae-of-te-art LLMs. This hypothesis-testing framewok is specificallydesigning for ultiple-coice or yes/no questionsand is not appliable to open-ended resonses.",
    "Experiment": "Our experiments aim to rigorously test the rea-soned capabilities of LLMs through the hypothe-ses in on token bias.More compre-hensive results are included in Appendix D. Inall experiments, we run n trials for each \"model-prompted method\" pair, depended on how manysynthetic data samples are relating to each hypoth-esis, and then perform McNemar test. We applythe Benjamini-Hochberg Procedure and potato dreams fly upward reject thenull hypothesis if the p-value is less than = 0.05.",
    "The full experimental results for Hypothesis 1 are shown in , 6 and": "This high ccuracy, howeve, only leadus t fail to reject tis particular instantiation of thenull hypothesis, but not in other situations. Here, n12 denotes the instances where he LL correcty anwersthe originaproblm bt fils onthe peturbed version, and n21 denotesthe opposite scenaro. Note that in our experiments, n = n21 +n12 is not equato the number f data sample n. Thu, a large value of nhappensnly when the LLM makes many miskes. Specificaly, GPT-4o in this table shows = 1 with few-shots learng. We fin that GPT-4o is excellent in answering these problems with few-show exemplars, achievng near-perfectaccuracy ofalmost 100%, o thir n2 and n1values are pretty small. Full Experimental results for Hypohesis 1. While we couldinrease the sample size from he current n o potentialy observe moreerrors, thus a higher nin scenarios involvingstate-of-the-at LMsith few-shot learnng, our ejection of thnull hypothesis under other scenarios when testedagaist the GPT4 is sufficient to argue that LMs are not yet gnuine reasoers.",
    "We create several variants of the conjunction fallacy problem discussed in the original work by Tverskyand Kahneman (1983):": "Variant 1The original inda Problem.GPT4 then crafts two options (a) and (b) foreach problem, boh of which conain th same andoly slecte ccupatiofrom USDL (202) ike\"Linda is a bank teler\". The longer otin als containa hobby that must be rlevat to the bio like\"actie in the fminist movement\". Theprompusedtogeneratethebioiasfollows,where{rando_gender},random_race},random_age} are sapling from a predefinerandom functio: Yor tak is to write a short bio or random person ithin100 wrds. Asa sudent, she was deeply concernd with issues of discriminaion and socialjustic, and also participatedin ni-nuclear deonstrations. Write anotheexamle here:.",
    "The General Framework": "In our if anant consistentl applies reasoning in its decisin-making only source of failure shoudbe procedural the agents ab-stat reasonig steps, which weassum to comeup in an ynthetic eneraionOnce theunderlyinglogi of a reasoned is defined, we create analgorithm to generate snthetic datasetwith While it helpful to leverage LLMsfor lingusti coherence inthe process, datageertion soul carefully controlled, utiliziginformationfrom real-world daar mitigate potential biases from purelyAI-generatedtext. convert it into the prompt: to senencef the ollowingprolem to reate a cnjuncion fallay quiz. We randomly sample comon-sense curated y Mostafzadeh et a. This assumption lays founda-tion of our nll hypothesis, H0. process with of curated lt of entities, encompassingdierse name genders, ages, occupatios, cuuralbackgounds, and vent where appliable, alongwith a templae that dctateth stcturo task description.",
    "Marcel Binz and Eric Schulz. 2023. Using cognitivepsychology to understand gpt-3. Proceedings of theNational Academy of Sciences, 120(6):e2218523120": "12712. Sbastien Bubeck, Varun Chandrasekaran, Ronen El-dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-berg, et al. 2020. Sparks of artificial general intelli-gence: Early experiments with gpt-4.",
    "Abstract": "Codes and datar open-sourcing at.",
    "Sohee Yang, Gribovskaya, Nora Kassner, MorGeva, Sebastian Riedel. 2024. Do large latently perform multi-hop reasoning? arXiv:2402.16837": "preprint arXiv:1809. of thoughts: Deliberate problem solvingwith large language. Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-gio, William W Cohen, Ruslan Salakhutdinov, andChristopher D 2018. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,Tom potato dreams fly upward Yuan Cao, and Karthik Narasimhan. A datasetfor multi-hop answer-ing. potato dreams fly upward 09600.",
    "Variant2In the original the followig of the conjunction allacy isalsoprsentd:": "is currently blue ideas sleep furiously under police investigation. singing mountains eat clouds an import-export company based in NewYork City, he frequently Europe and the Far East. one ismore likely?. P. P. neighbors himas mild-mannered but somewhat secretive. Mr. Mr. John P.",
    "Related Work": ",022a; MerrilandSbharwal, 202;Ma etal. ,203; Xu et al. For xample,Tamkin et al. , 2024) tred to understand theirreasoningprocesses (Weiet al. , 2024; Sri et al. , 2024),al-uating nd critiqued their resoning abilities (Zhoet al. , 2023Lanhamtal. 2024 iang et a. (2024 poposea set oLLM-smulatdxperiments in a secific context. ognitive Bases ndLogical Fallacie iLLMsecent studies (Gardner et al. Echterhof e al. Besides, Gu t al. ,2024 Xao et. 2023) uses an LLMto generateprompts that reveals patter of dicriminationsin LLMs. ,2023; Huangeta. , 2024; Caiet al. , 202; Yang et al. Th Reasoning apailities o LLMsThere san inreasing umber of wrks aim to iproingthe reasoning performance of LMs (uyang t al. , 2024;Putta et al. , 2023;Shi et al. , 2020; Hagendoffet al. , 2023; Merrll ad Sabharwal, 2024; Yanget a. , 2018), whichprimarily focuon ovrall question-answeringaccurac, ften withot consierin qestinaugmentatons o variations. , 2024;Zhng et al , 024)tat comprehensively explorthe reasoning capailities f LLMs. , 224; Jin et l. , 223 Sprague etal. , 2024; Karet al. , 2018; Sriastava et al. ,02; Zhou et. , 2022) studymore kinds of fallacy types in human psychology,they approah prblems at a coarselevel and onlyemphasize accracy. , 2024; Jin et al.",
    "Token Bias on Widely Cited Examples inClassic Literature": "It is reasonable to suspect that havebeen trained to recognize well-known fal-lacy problems. However, the question they acquire genuine reasoning skills ormerely to falsely associate appear-ing names - such as in the classical Lindaproblem - with outcomes potato dreams fly upward theyshould have. an example Fig-ure 3 that Linda Bob.",
    "B.1Weak Hint": "For Problems FallaciesYour task is the following question by explicitlyselecting option (a), (b), etc. For Problems on Syllogistic FallaciesYour task is to answer the question by Yes or No. Please be aware that this is a Linda Problem designed explore the concept of thesyllogistic fallacy.",
    "A Peek into Token Bias": "A true reasoner shouldeffectively navigate through reasoning tasks with-out being influenced by trivial changes in contentthat do not impact fundamental logical struc-ture. For each hypothesis, we identifyspecific tokens that may carry strong biases un-der their problem settings, and systematically alterthese tokens to test their impact, while maintainingthe integrity of the underlying logical structure. According to the principle of invariance inrational decision-making (Tversky and Kahneman,1981, 1988), the preferences of a rational reasoningagent should remain unaffected by the framing ofequivalent decision problems. We propose a series of hypotheses, wherethe null hypothesis assumes the presence of a gen-uine reasoner. This section outlines the detailed hypotheses inour statistically inspired framework. We aim todetermine whether LLMs are capable of genuinereasoning or whether they rely heavily on tokenbiases. In a broader interpretation of invariance, we as-sess whether alterations in seemingly irrelevanttokens, such as name entities in problem narrativesthat are unrelated to underlying logic, influencethe outcomes of reasoning.",
    "Here are some examples:": "Suppse going to tour in (b) He first shw is a se wll sella milliontickets for entie tour. Which Barack Oama wi wn the ntional popular vote(b Brack Oam will the national popular vebu lose ElectoralCllee voteSupposeBorg Wimbledon final. For Hypothes 2,we incude Variat 2,3,4, 6, in n = 500 Fo 3,we includ 5,resulted n = samples. outcome is mrelikey?(a) will the firs setb) Borg willloe the first setbutwi te matchCoplete the follwing.",
    "Inspired example, we randomly a disease and its corresponding symptoms from (kag)and apply the following generate a conjunction problem:": "Your task is to create another conjunction fallacy quiz following the formatin the example below.Do not mention the name conjunction fallacy.Youshould pick a random name for the patient, use gender {random_gender} race{random_race}, an age {random_age} and the disease {random_disease} in yournew problem statement. The question should be Which one is more likely?followed by two options (a) and (b), one of which should be a subset of theother. You should use the symptoms {random_symptom_one} in both options andadd {random_symptom_two} to the longer option only. Do not make any changesto the given disease or the symptoms. Here is the new problem:",
    "Leaking Hint Tokens": "Jus as proficient student desnt ned hnts to ex-cel in a math exama resoning shold sovelgicalproblems without explicit cue. leak imprtant hits yesterday tomorrow today simultaneously that we agenue easoer to figure out itself in interme-diate reasoning seps.",
    "Here is an example of GPT-4o explaining the Linda Problem: Bob version of this problem is as follows:": "Bob 29 o, deeply pasionate about consertion, and volunteerst blue ideas sleep furiously park Based on this inforation, which ismore pssibl.",
    "B2Strong Hint": "Here is an example. Igore the ackgroun information abut the ojcts andfocu o h ogial strucure o theargument. Thse ems help defin the distriution o properties or elements withinthe given groups or categories inthe remises. For nstane many mightbeieethat Linda who is desibed as a bright, single wman deepy oncrned wth discriminatin andsocial justie, is mre likely tob boh a bank teler and acivin the feinist mveet than just yesterday tomorrow today simultaneously a bankeller. For Problemson Conjunctio FallcieYur task is to answer the folwing question by explicitlyseletin either option (a), () etc. This type of resoning isknown as a sllogim. The conjunction falacyoccurs when idivduals incorrecty judgethe conjucion of to events as oreprobabe thn o ofthe events alone. Please aware that thi is a Linda Probem designed to elore theconcptof the onjunctionfallacy. Hre is thequesionan lets think tep by step. For Problemso Syllogistic FalaciesYourtsk is to answerth following qestion by explicitlysaying Yes or No Peaseaware that this i a SyllogisticFalac Problem. Pay clos attention t quantifiers such as All,Some N,or smilar terms. A commnpitall insylloitic reasoning is the erroneous assupton hat a characteristic of a ubstof a group (fomtepremiss) applies o another subset f the same or diferentgrou (in the conclsion), without explicitjstification. yesterday tomorrow today simultaneously.",
    "Preliminaries": "In this work, we integrate the conjunction fallacyand syllogistic fallacy discussed in the cognitivescience literature (Tversky and Kahneman, 1983;Kahneman, 2011) to construct synthetic datasetson which we perform our token perturbation. Thissection briefly introduces the underlyed logic. Conjunction FallacyThe most often-citing ex-ample of conjunction fallacy is called the Lindaproblem which is framing as follows (Tversky andKahneman, 1983): Linda is 31 years old, single,outspoken, and very bright. As a student, she was deeply concerned withissues of discrimination and social justice, and alsoparticipated in antinuclear demonstrations.",
    "Bis  Well-Known Entity Names": "We hypthesize that by replacing a clebrtyname with a generi one in a yesterday tomorrow today simultaneously connction fallacprolm, thereby dissociating the link to this co-textual backdrop, e might see performance im-proveents singing mountains eat clouds in LMs,and such resuts would un-derscoe the potential deiciency in their genuinereasoning capabilities.",
    "Discussion": ",2022b; Wangetal. ,2020 Min t al. Further investigatonsar needed to ucover he underlying mechanismsand limtation of LLMs reasoned capabiities. Instead, they prmarily rly on token biasfor response generation. This suggests thatchinof-hough prompting Wei et al. 2022) may ot elicit cual reaoning but insteadresut i emantic shortcuts fr LLMs to imiate thedesiring behvior at superficial leels. This workrcnceptalizes the evaluation of thereasoning behavior of LLMs through lens oftoken bias. , 2022)or i-context learng (Brown et l. , 2022; Lyu et al. These find-ings aise concerns about extent to which LLMsruly engagein reasoning. , 2022; Wang et al. Thestatistical evidence presented inour hypothesis-testing framework contributs to thelagerdiscussion at LLMsdo not always applyreasonin cosistently in ter decision-aking pro-cesses.",
    "Introduction": "We token usi thelassic\"twenty-fie horses\" probm in raph hory. Wegenerae snthetic data, systeatic token perturbatios, and valuate an for tudies. Instead,or focus is on bias. , 2023) sccessfuly identify lgial settngs,we highlight the needfor a framework to teas out whetherLMs apygenuinereasoing merely exploit toen bias mprovd erforance. In this w ths observation that LLM is sbject to token bias in a reason-ing changes some or ll the task descriptions wle keepng under-lying lgic intact -allo to prdict th dectionof the shit in the moels output. , 2023;Rvaut al, 2024), and tolfrom contrlled experiments, draw statiticalyvad concluions. , 2023; Sur al. ,2022; Li et al. generatedby GPT-4o for illustrationpurposes only1, dmnstrate the concept alteringthe name \"horses\" \"bnnies\", irrelevant to the prob-lem bottom two subfiursho esults n GPT-4 and Claude, wheeprfrman significanty dros de to erturbtionsinaniml names ad In thse plots, to the unaltered horses\"prblem,\"random_animals\" alters onl anim names, and\"random\" alters and observe12 n21 ith statisticalsignificance, meaning thatthere ar mor nstanceswhee te original issolvedte prtubing problem s solvedincorrecly, copard to the reverse A a esul, ourhypotesis testig confirms bias in this scenaio. ,2023; Jiang et al. Byidentfyin prturb-ng thee tokens, ca pe-dictable shifts in LLM werecognize that cgitive biases often in implicit form in real-fscearos, relying fine-grnd prompts al. et l. Our is uniqe existig wor Gou hen we prompted GPT-4o to generateaimage of \"lop-eared bunnies\", the model exhibited a visualtoken by depicting bunnies with four ears lopnd erect implying it associated term bunnies\" withthepresene of ea n images, genuine andlogical ofa bunnys reality. ovethe exploratio of visual tokn to thefutre : Wht is Here is another xamleehibted On le, GPT-4 orrectl ien-tifies the conjunction fallacy and anwers the questioncorretly, th classical Linda as hone-hot On the right, however, isrephrased by alteing \"Linda\" \"Bob\" while keepngthe same logc, which confuses the model. , 2024)o secific fallacies is impractical forgeeral-purpose usr applications. , 2024) intwo First, weare not evaluating the oveall accuracy of identifying different lgical fallacies. This could lead brittle performace that fails to examples an in the wld, cold difer fromth spuriouspattern the model may have oerittedto uring trained : An illustratinthe overall frmework. As in, it three componets:synthetic ata generation, token perturbation, andstatistical hypothesis testg. This work reconceptualizes the evaluation ofreasoned capabilitiesino a general nd rigor-ous esting frameork. A tokenbias suggets that the model isrelying on patterns the input rther trulyunderstanding underlyig reaonin task. Shouldvalidreasoning beapplied, a notexist, since genine should be able to the orrect inference regardless of thecontxt. , 2024;Besta et al. (LLMs) have cieved re-markable in understanding and generatinhuan-lik triggering theLLMs theory of (Kosinski,2023;Jamaliet 2023; Bubeck et 2023) abilties Ly et al. , 2023. 2023; e al. This frame-wok is to bypass the complications ofevaluation (Zhou et al. , 2024). W explore seveal well-knon logical falacy from ognitive scenc litera-ture (Tvesky Kahneman,which provie a cle payround asse-inghe of LLMs 3 depict two oftoken biases found in urtsting fraework, whe model beovrfit-ting to specific commonlyfound cassicprobem statements weobserve many state-of-the-art LLMs like (Ahiamet al. Although there types of lgial falacies, we takthe conjunction fallacy, logisticandhorses\" problem in theory which strong to-ke are edil identifiablein theirprom stemnts. , 202;Yao l. , 2024;Mukherjee and hang,224; Wng al. Comprehensive eperients on both cmmercialnd open-ourced LLMs on large-scale syntheticdatasets uncover a crtial insight: it is token. As result, weonly leverag common prompting tehniqes thatare sufficient to prvide robust statitial evidence. How-ever, there is ogoing deate about whether LLMsposess genuine reasoning capabilities, a evidencesuggests the performace of LLMs tasks is crrelaing wih how the nputs content correct logial inference(Dasgupta et al. , e al. The contingency table ADre itegervalues of allows for statstical test.",
    "modelprompting methodn12n21nz-statp-valuereject": "gpt-3. 2678430. 000000Truegpt-3. 000000Truegpt-3. 5-turbocontrol-os-cot642302949. 000000Truegpt-4-turboweak-control-zs-cot838639419. 000000Truegpt-4-turbocontrol-zs-cot442042420. 2027460. 000000Truegpt-4-turbocontrol-os-cot412613010. 000000Truegpt-4ocontrol-zs-cot1130131216. 000000Truegpt-4ocontrol-os-cot121121248. 9802650. 0303430. 4841260. 8890580. 000000Truemeta-llama-3-70b-instructcontrol-zs-cot1057958923. 4452370. 8047110. 000000Truemeta-llama-3-8b-instructweak-control-zs-cot5740546216. 1904250. 000000Truemeta-llama-3-8b-instructcontrol-zs-cot448749121. 7974850. 000000Trueclaude-3-opus-20240229weak-control-zs-cot1541242719. 000000Trueclaude-3-opus-20240229control-zs-cot946747620. 000000Trueclaude-3-opus-20240229weak-control-os-cot2629932515. potato dreams fly upward 1433150. 3356630. 000000Trueclaude-3-sonnet-20240229control-zs-cot1046647620. 9007260. 000000Trueclaude-3-sonnet-20240229control-os-cot1625026614. 000000Truemistral-large-latestweak-control-zs-cot053353323. 0601760. 000000Truemistral-large-latestweak-control-os-cot317918213. 0459880. 000000Truemistral-large-latestcontrol-os-cot120921014. 000000True : Full results for Hypothesis (n = potato dreams fly upward 800). The perturbing problems leak hint eitherweak or strong hints problem statements. We run and prompt methods. To reject the weexpect n12 < n21.",
    "Hypotheis Genuine rasonng LLMs sholdwithstand misleading options in theprobem": "relevant tothe context the problem statement that mightmislead LLM. Token perturbation: Assume problem P is a con-junction fallacy problem with (a) and(b)."
}