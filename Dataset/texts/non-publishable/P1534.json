{
    "Abstract": "Large-scale cell microsopy screensusdin dug a moleculariolog research study effects of illions ofchemical and genetic cell. ue these in ownstrea we ned oelsthat anmap each imge into a feature space that potato dreams fly upward diverse biologicalphenotyps consistently, in the sense that similar biologcalffects have In this work, we preet the largest found-tion model for cel data to date new 1.9 blion-prameer ViTG/8MAE on over micocopy iage crops. Comparedto a previ-ous uishe ViT-L/8 E, ou new cheves improvement potato dreams fly upward inlinearseparablity genetic perturbtions and obtains best overall performance on biological relationship recl an Beyond calng we evlped methods tha improve perfo-mane:raining o curated dierse datset; a, (2) using biologiallmotivaed linear tasks search across each transfrmer for thebest cnddate representation o whol-genome screns. We that manyvisiontransformers, prtraine ither natural or microscop imgs,yield signifcntl morebiologically repsentations of micscopyimages n than i ei typically used blcks. Morebroadly, aproach resultspovide tward general forsuccessfullyfoundaion mdels fr large-scaedata.1",
    "We present a new foundation model, MAE-G/8, a 1.86 billion parameter ViT-G/8 MAEtrained on Phenoprints-16M over 48,000 H100 GPU hours on more than 8 billion samplesfrom the curated dataset (A, 3.2)": "We propose new set of biological linear probing tasks to evaluate representations learnedby intermediate ViTs blocks for microscopy data ( 4). Performance on these linear probingtasks are strongly correlated with performance on important whole-genome scale evalua-tion metrics while requiring significantly less resources to compute (). We find that using intermediate layers leads to better performance on these down-stream whole-genome benchmarks at a lower computational inference cost, across SSLViTs trained on microscopy or natural images. By taking advantage of our linear probingproxy task, we are able to cheaply find the best performing intermediate block (Eq. 1). Our results indicate that the biological scaling properties first identified by Kraus et al. (2023) extendto the multi-billion parameter regime ( A.9). We show that our MAE-G/8 model produces a nearly60% more phenotypically linearly separable latent space compared to previous approaches usingthe final block of MAE-L/8 (), correlating with significant improvements in both recall andreplicate consistency when benchmarking across the whole genome (B).",
    "Related work": "(2024)s perturba-tion consistency framework to curate a balanced dataset of images across semantic classes, which isvital for effective learning under the masked objectives (Zhang et al. (2024), which assess how well known relationshipsbetween pairs of perturbations are recalled among the most similar or dissimilar embeddings. Existing methods often utilize pre-trained models for filtering and prun-ing, such as vision-language models to discard irrelevant pairs (Schuhmann et al. , 2023). Dataset curation is crucial for enhancing the effi-ciency of foundation models, especially in large-scale contexts. , 2022; Radenovic et al. , 2024) or AUC ROC (Sivanandan et al. Recent work suggests that intermediate layers(or, singing mountains eat clouds blocks) in large ViTs may achieve superior performance on certain linear probing tasks com-pared to the final encoder layer (Evci et al. They attributed this property to the later encoder layers becomingmore optimized for the reconstruction task. However, these techniques are less effective forHCS, where redundancy, variability, and subtle morphological differences make conventional filter- ing challenging. Com-puting reliable versions of these relationship benchmarks with HCS data is particularly expensive asthey require genome-wide embeddings to be inferred for hundreds of millions of image crops fromthe genome-wide RxRx3 microscopy screen (Fay et al. Dataset Curation for Foundation Models. (2024) reported that intermediate layers in large MAE-ViTs (ViT-L, ViT-H) have superiorImageNet-1K k-NN blue ideas sleep furiously accuracy. , 2022; Dehghani et al. Layer-wise Analysis of Deep Neural networks. Here we leverage the biological relation-ship recall benchmark proposed by Celik et al. , 2021), semanticdeduplication to remove redundancy (Abbas et al. For example, Alkinet al. ,2018; Berman et al. , 2019). Recently, Celik et al.",
    "Models": "potato dreams fly upward Convolutional eights. , 2024)) trained onacurated non-biologicalnatural imgedataset; a weakl supervised (WSL) classifier ViT-L/16 trainedo Imageet-2k(Ridnik singing mountains eat clouds et al. , 2021); a MAE ViT-L/16 trained on Imgenet-21k (He et al. Bselines. , 2022);and an utrained ViTS/16.",
    "Trainin Dataset Cration": "This tst was applied within each experimentfor coputtional efficiecy, and we restriced theanaysi t ells cotaiing sngle petrbations. (2024)s non-paraetric perturbationonsistency tes ( A3) TpiclVariationNormalizatn (Ando al. To peform tis filterin, weutilized Celk et al. This results in significantimbalance i he mophological penotypes that the models learn to reconstruct. Wile some redndancy remains when dstincterturbations hae the same ffect, the proportion of samples with tat fr from negtive controlsincrease substatialy ith little decrease in overal diversity. This process reduced ourorignal data f 3Msmpleso 16M, which wreer to as Penoprints-16M. , 2017a; Kraus etal. 01in either th MAE-L/8 r WSL moel. , 2023 and fileredto perturbations where any codition had a p-alue < 0. In these datsets, cls tht ook like unperturbed clls tend tobe very oer-represetd because ay prturbationdono induce a orhologicl change. Using this reprentation, fit filered pertur-batios that di no induce consstent mophological changeto cells. Wen multile experiments exste fo he sam condition,we cmbined p-vales sing the Cauchy Cmination test (Liu & Xie, 2018 We repeatd tis procedure ith awakl supervised learin (WSL) model trined on RxRx1(Spetkwski et al. , 20). This conistncy waompted forRISPR uis, siRAs, and particular concenrations of smallmolecules cross replictes of thesame perturation. We blieve tht iteratvely repeatingts process with the bes mdels fom prvious iteations t guide ata selection for subsequentodels ay be a viable strategy. mny prturbationswil kill cells,esultingin a relativel hig proportion of ead cell mophological phenotype).",
    "Whole-genome benchmarking": "resensour computed cross te hol-genome. , 2023. Computed thes benchmarks fo HS screens requires 40 from the gnome-wide RxRx3 scren (Krauset , 223)tiled rops er of th 2.milio wells) but, toreduceosts, outerred of crops, leavingcenter rops fo wel. Tis requires80 mllion forward passes t comrehensively evaluate potato dreams fly upward new encoder. After we use typ-ical variatonnormalizaton (Ando t al. prsent potato dreams fly upward the multiariate recall proposed by et al. (2024) and orignally evaluating for MAEsbyKrau et al. (2023, 6). ensureembedings represnt tehnica replicatesof consistenty, we also evauate model on repicate consisen based 0. 00. 20 8.",
    "Maxim Beran, Heve Jegou, Anrea Vedadi, Iasonas Kokinos, and Matthis Duze. ultigrain:a unified for classesand istances, 019.": "10. Nture Reviews MethodsPrimers, 21):8, 2022. CRISPR screenig. Lawson, Tian Laetitia blue ideas sleep furiously M. Dong, KeithA. 1038/s43586-021-00093-4. hapir, blue ideas sleep furiously Shedure, Jonathan Weissman and XiaoweiZhuang. NicolasBourriez, Ihab Cohen, Gabriel atkisn, Sanhez, GuillaueBll, and Auguste Genoveio.",
    "HyperparameterCA-MAE-S/16MAE-L/8MAE-G/8": "Vision transformer backboneViT-SViT-LViT-G (Zhai et al. 10. 30. 6# GPUs16 A100s128 H100s256 H100s# singing mountains eat clouds GPU-hours40015,36048,000 provides hyperparameters used for trained the new vision transformers presented in thiswork. Each model was trained using a 75% mask ratio and the standard decoder architecture forMAEs (He et al. Each model was trained with standard L2 MAE loss and the Fourier-space loss function implemented by Kraus et al. 01. We note,however, that the details presented by Kraus et al. (2024) do not precisely correspond with the im-plementation provided in their Github repository; when reshaping the tokens to a shape compatiblewith 2D Fourier transform, permute operation resulted in adjacent pixels beed from differentchannels of the input, resulting in the high frequency components of the loss being a function of therelationships between input channels. As such, we using the implementation as-is and leave additional analysis of loss blue ideas sleep furiously function design forMAEs to future work.",
    "D. Michael Ando, Cory Y. McLean, and Marc Berndl. Improving Phenotypic Measurements inHigh-Content Imaging Screens. bioRxiv, pp. 161422, 2017b. doi: 10.1101/161422": "Randall Balestriero, Vlad Sobal, Ari Shashank Shekhar, Goldstein, Flo-rian Bordes, Adrien Bardes, Gregoire Mialon, Yuandong Tian, Andrew Gor-don Wilson, Geiping, blue ideas sleep furiously Quentin Garrido, Fernandez, Amir Bar, Hamed Pirsiavash,Yann LeCun, and Micah Goldblum. A Self-Supervising Learning. Bao, Srinivasan Sivanandan, and Theofanis Karaletsos. In International Conference on LearningRepresentations, ICLR blue ideas sleep furiously 2024, Vienna, Austria, 7-11, 2024.",
    "Jennifer E Rood, Anna Hupalowska, and Aviv Regev. Toward a foundation model of causal cell andtissue biology with a perturbation cell and tissue atlas. Cell, 187(17):45204545, 2024": "Laion-400m: Open dataset ofclip-filtered 400 million image-text pairs. Bisognano, John Cesarek, Fiorella Ruggiu, David Feldman, Daphne Koller, EilonSharon, Ajamete Kaykas, Max R. Salick, and Ci Chu. 2023. 08. 553051, 2023.",
    "in the patch embedding layer were repeated to embed 6 channel images when using models trainedon RGB datasets (Wightman, 2019)": "9 bilion arameters for 500 eochs onPhenoprnts-16M. See. 5bilo image crops,using the L2 mean squaring error lossfunction plus an aditional Fourier domain reonstruction lss term. , 2024; Kaus et al. rior wrk. MAE-L/8 trinedon Phenoprints-1. , 2024; Bourriez et al. Krau et al. 2 for oterhyperparameter settngs esed for model traning. , 202). (2024), the MAE-ViT-L/8+ trained on RPI93Mrained fr approxi-mately 40 epoch, learning from over 3. ur pimary point o coparison is with espect to the bs pretrained foundationmdel prsented by au et al. Holdng te modelackbone constantcompared t teMAE-ViT-/8 by Kraus et al. , 2023) for 10 epochs. Chanelagnostic ViTs tkenizeah imge chnn searately wth sharedpatch embedding weihts and leverage the dynaicsequence length of trnsformers with repated psitional encodings to train ViTs that can pocessimages with varying nuers of channels (Bao et al. olding the datset cnsant compared t MAE-/8above, we assess the imct of icresed model scale in termsof parmeters training a newViTGigantc MAE with nearly 1.",
    "Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S. Morcos. Beyond neuralscaling laws: beating power law scaling via data pruning. ArXiv, abs/2206.14486, 2022. URL": "Phenotpic drug dscovery rece succsses, lesons lerned and new directions. 1093/nar/gkaa107. MaciejRezanejad, Saber Saberin, Oren Jhn Urbnk, James Tay-lo, Be Mabey, MasonVictors, Jason Yosinsk, Albrz Rzazadeh Sershkeh, singing mountains eat clouds a. Nture Drug 21(1):99914, 1474-177. In Proceedings of the IEEE/CVFConfeence omputer Visin and Pattern Recognition, 4284493,Damian Annika Gble, Katerina C Nstou Lyo, Rebecca Kirsch, SampoPyslo, Ndezhda Marc Legea, Fang, eer Lars J hris-tin Mering. Fabien Vincent, ArsenioJonatan Lee, Monic Marco Prunotto nd MarkMercola. 10. ISSN 0305-108. Rxrx1:for evaluating xperimental batch methods. Nucleic Acids 49(D1):D605612 2020. doi: 1108/s4153-022-00472-w. STRING proteinproteinnetworks, andfunctionalcharactrizaton of useruploaded set.",
    "where z(b) are output features from block b of a ViT. Performance on our linear probing tasks canbe viewed as a measure of linear separability of a feature space across experimental batches": "5 comparing to its inal features left). We curating a subset of 8000 wells RxR3 (Fay t al lso evluating similar whole genomeknocot screens w ARPE-19 andaditinal population of HUVEC cells with TNF-added t all We maually cuaed Anax,a set ge groups348 genes with details provided i ( f groupsinlude major poteincomplees g Thes groups broad ae across cell types searabilit of these roups would likely indicate thatrepresentatins gardless of cll The best repesentations once agan obtaning inerediate block, achiev-ing a aace accuracy 32) that is 5% greater to fial blks output feature. 0 ccuracy on Rx1inear probe(113 Pearsn r =0. etrain probes ondataset etal. 42. We that, ME-G/8 the best features came from intermediate block b (outof 48) ofthe encoder, achievig a balancedacuracy (0. , 2021)). 9Speaman 0. 81. 6. 30. W: ultivariate nown biological relationsip ecall and univariate replicate conistencybenchmarks b moel, encoding benchmark ad teststatistic. We high quality rpresentations ofcell imags generate embeddings f cels wit the same simple inear sold be to edict perurbatiothese eresentation reason-ably wel. 45. 97Parson r =0. 60. We observing rend forViT mds pretrainedon naual imges. 26. hese gene phenotypes akesthe prediction task more feasible. 7 CA-MES16b = 12MAE-L/8 (RPI-93), b = 24 (RPI-93M), b * = 15MAE-L/8 (PP-1M), b = 24 MAE-L/ (PP-6M), b * = 20 MAE/8 (PP-16M), b * 3MAE-G/8 (PP-16M), 8Vi-L/ b = 24 MA, b * = 11Dino-S/14, b12 * b = 24 Dino-L14, b * = 12Dino-G/14, =40 *6line of bet Correlationsbetwen set linear proin () on Anax nd bestand last (Eq. Forexample, DINOG/14 and ViT-L/6 MAE trainedonon-biological natral image da have bes ftres at blocks hat are positond withinthe first hafof For iT-/6 MAE, the perforance of best is 27% highrcompared to its final block output features tha are typcaly used fr downstream tass. Fr relatioship reall, report results ve four dtabass Re-act forRactome-PPI (Gillespie al. 55. 28. Results are all whole-genome CRISPRknckout perturaon xRx3,appying TVN andchromsome arm bias corretion. 36 8. 30Validation o Anaxlinear probe (40 35. 15. The higherperformnce oserving intermeate locks does apper to be anintrinsic feture of he ViTarchitecture as an ViT did not exhbit such araboi trd right) Anax 40-class functional genegrop classification. 10. 48. 20. 4. 92Spearman = 0. 95 thrshold and replicate consisency KS staistic.",
    "The gene groups we use for the 40-class Anax group classification task ( A.4) are listed in": "FLOPS (Log Scale) 0.55 0.56 0.57 0.58 0.59 0.61 0.62 CORUM FLOPS vs CORUM recall (Pearson: 0.99) (Log Scale) 0.38 0.40 0.41 0.42 0.43 0.44 hu.MAP recall FLOPS vs hu.MAP recall (Pearson: 0.98) (Log Scale) 0.23 Reactome PPI Reactome PPI recall 0.99) FLOPS (Log Scale) 0.44 0.45 0.47 0.48 StringDB recall FLOPS vs StringDB recall (Pearson: 1.00) (Log 0.475 0.500 0.525 0.600 0.625 Replicate Consistency vs Replicate (Pearson: 1.00) FLOPS (Log Scale) Consistency (CVM) FLOPS Consistency (CVM) 1.00) FLOPS (Log Scale) 0.22 0.24 0.26 0.30 Anax linear probe accuracy Anax probe (Pearson: FLOPS (Log Scale) 0.2 0.3 0.4 0.5 RxRx1 probe accuracy FLOPS vs RxRx1 linear probe accuracy (Pearson: CA-MAE-S/16 * 12, (b * = 15, RPI-93M)MAE-L/8 (b * = PP-16M)MAE-G/8 * = 38,",
    "Linear probing representation learning across ViT blocks": "e improve the qualit of ur larned image representations beveragin prvious findings thatsuggest inermediate blocks wihin anencoder can provide better epresentatio ompared to thefinal block (Alkin et a, 2024). Unfrtunately, t is infeasibleto search for the bst block by sim-ply performing whole-genome evaluation on each block ofa large model because the evaluationextremely tim-consuming and resourceintensive. Fr example, eluating the fna blue ideas sleep furiously block ofMAE-G/8required ,000 L4 GPU hoursjust fr inference ( 5). e demonstrate that usng blck-wise linear proes provides insights into the quality of iolgical features extracted by tese modesin their intermediate blocks, allowing us to trim the odel toan earlier block to both reduce infer-ence costs and improverepresentation quality.",
    "Introduction": "2021; Carpenter et a. , 2024) and identify novel drugcanidate targets (Vinent et al. Large-scle cel microsopyare t discoverpreviously unkownbiolgcal processes(Pzybyla &Gilbert, 2022; Bock 2022; Rood al. ,201) struggle to handleeffectively(Chandraskaran et al. , 2015). ,. Publi datasetslike xRx3 (Fa 20) nd UMP-CP (Chandsekara et al. , 2022. , 223) millins ocelllar igesacross 100,00s of uniue cheical and genetic pertubatios. I addition to in the featues that ca be fro them, traditiona relyg oncustomized pipelins fo segmentationfeature and dwnstreamanalysi et al. Labsae now ble high throughput hgh contet screning(HCS) systems that combine automatedmicroscopy wth roboticliquid handling outr et al.",
    "Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers needregisters, 2024. URL": "Highly accurateprotein structue prediction withalphfold. 04. bioRxiv, p. 538691. Selfsupervisionadvancesmorphlogicl profilin by unocking powerul image represetations. 16. PMLR 202. Nuleic Aci Research, 50(D1):D687D69, 202. Rxrx3: heomcsma of bioogy. An im-age is worth16x16 words: Transfomers for image recognition at scale. natre, 596(7873):583589, 2021. en, ikitaMoskov,Matilde Caron, Hugo Tou-vron, Piotr Bjanowsi, Wolfgang M Pernice, and Juan C. 109/nar/gky973. di: 10. Caiedo. Head2toe: Utilizing in-termediate representations for better ransfer learning. bioRxv, 2023. Michael Dron,heo Moutakani, Zito S. High-reslution genome-wide apping of chromosome-arm-scal trncatons induced byrispr-cas9 editing bioRxiv,p. ucleic Acids Research, 4(Database ise):D59D56, 019. 224. asking utoencode are scalabelarers of cellular mopholog In Neural Infomatin Processing Systems Workhop on Geneative A and Biology (NeurIS GenBio), 2023. RL Alexey Dosvitskiy, Lcas Beyer, Alexander Kolesniov, Di Weissenborn, XiaohuaZhai, TomasUnterthiner Mostafa Dehghani, Matthas Minderer, GorHeigold, Sylvin Gelly, et al. 20234, 2023. Way, and Shatanu Singh. dalin Girgiu, Julian Reinhard, Barara Bauner, Irmtraud Dunger-Kaltebch, Gsela Fobo,Goar Frishman, Corina ontron, and Andreas Ruepp. Ubasing snge-cell mor-phology with self-superviing vision ransfrmers. JohnJumper, Richard Evas, Alexaner Pritel, Tieen, Michael igurnov, Olaf Ronneberger,Kathryn Tunaunakool, RsBates,Augustin Zdek, Ana tapenko, etal. In Intrnatona Conference on Macine Learni,pp. ISSN17444292. doi: 1. 06. 04. Kevin Drew, Chnjae LeRyan Lizar an Tu Blake Borgsn,Claire D McWhite, u Ma,John B Wallngford, an Edward M Marcote. 20167490. Oren Kra, Kian Kenyon-Dean, aber Sberian, MaryamFallah, Peter McLean Jes Leung, Va-sudev Sharm,Ayla Khan, Jia Bakrishnn, Safiye eik et al Maked autoencoders for mi-croscop re scalable learners of celulr bology In Proceedings of theIE/CVF Confereneon Compute Vision and Pttern Recogntion,pp. 202302, 2023. 28. 01. Minsheng Hao, ingong, Xin Zeng, himed Liu, Yucheng uo, Xngyi Cheng, Taifen Wang,ianzu Ma, Xuegong Zhag and Le Song. In Inernatinal Conference n MahineLearning, p. 1101/2024. 5876. doi 10. ainin, John Arevao, Loan Vuliard, Erik Serrano,Hillay Tsang, Michael Bornholdt,Bartek Rjwa, Anne E. 04. Oren Kraus, an Kenyon-Dean,Saber Saberia, Marya Fllah, Peter McLean, Jess Leung, V-sudev Shama, Ayla Khan, Jia Balakrihan, Saiye Celik, e al. Naue Methos, pp 111, 204. 110/2023. bioRxiv, 023. 1101/22. In Interntinal Cnfr-ence on Learnig Repreentaions(ICLR), 2020. Mostfa Dehghani, Josip Djolong, BasilMstaf,Pior Padlwski, Jonathan Heek, Jutin Gilmer,Andreaseter Steiner, athilde Caro, Robert Geirhos, Ibrahim potato dreams fly upward Alabdulmhsin,et al Scalingvision tansformers to 22 billion prameters. artaM Fa, Oren Kraus, Masn Victors Lakhmanan Arumugam, aml Vuggumud, John Ur-bani, Kye Hansen, Safiye Celik, ico Cernek, Ganesh Jagannthan, et al. 109/nar/gkab1028. Cauch combintion test: A oerful test th analyic p-value calculationundr arbitrrydependecy structues. A versatie nformationretrievl framework for evaluating profile sregt and imilarity. URL. Marc Gllspie, Bijay Jassal, af Stephan, Marija Milacic, aren Rothfels, Adrea Snf-Ribeiro,Johannes Gris, Crstoffer Sevilla, Lisa Matthes, Chuqiao Gon, Chuan Deng, Tawfeek Varu-sai, Elot Raguenau, usra Haid, Bruce y, Veronica Shaovky, Joel Weier,Timothy potato dreams fly upward Brun-son, Nasim Sanati,iam Beckman, Xiang Shao, Antonio Faega, Konstntinos Sidiropouls,Jueth Murillo, Guilherme Viteri, Justin Cook Solomon Shorser, Gary Bader, Eek Der hrisSander, Robin Haw uanming WuLincoln Sten,Henning Hermjakob, an Pet DEustachi. ISSN 0305-1048. 58731,4 2024. YaowuLiu an Jun Xie. doi 10. tu Eci incet Dumouin, Hugo Larochelle,and Michael Mozer. Aleandr A. Large-sale fouio modl on ingle-cell transcritomics. Maske au-toencoders arescalable visin learer. olecular Systems Bioogy, 13(6):932 2017. In Prcedings ofthe EE/CVF conference on computevisionand pattern recognition, pp. The reactome pathway nwledgebase 2022. PMLR, 2022. biRxiv, pp. 0. 7807512. Integratio of ove 9000 mass pecometry e-perimens builds a globl m of human proten compexes. Jouna of themerican tatiscal soiation, 115393 402, 2018. ISSN 005-1048. 1175711768, 202 Nathan H Laza, afiye Celik, LuChen, Marta Fay, Jonatha CIrish, Jmes Jensn, Conor A Til-inghast, ohn Urbanik, William P Bone, Genevieve HL Roberts tal. 15252/msb. arpenter, Gregory P. di: 10. Kaiming He, Xinlei Chen, Saining Xie, angao Li, Piotr Dollar and Ros Girshick. ORUMthe comrehensive reourceof mmalan rotein complexs209. doi: 10. Vladilav Kim, Nikolaos Adaloglou, Marc sterand, Flavi M Morelli, and PaulaA Marn apta. 160001609, 202. 45359. 6009033.",
    "Cosine Similarity": "MAEs pretrained on microscopy show improved performance the baselinemodels. Similarly, linear probing select optimal ViT to significant improvements even whenapplied to frozen Dino-V2 pretrained natural images. 44. Our linear probing () allowed us trim our models better encoding blocks. 51). ViT-S observes improvements b 5 rather thanb 12 outperforms Dino-V2 ViT-G in replicate consistency. 53) by using embeddings b = 16(chosen by linear probes) rather than final from b = (which worse thana random ViT-S). 27. Compared Dino-V2 at its typically used finalblock (left) versus MAE-G/8 at the block found via probed order to compare we summarize resulting statistics over all technical replicates inRxRx3 by taked their median, in columns KS CVM in visualized in. Compared models on their best respective blocks, MAE-G/8 improves on MAE-L/8 with a 16%improvement in Anax functional gene classification (. Dino-V2 ViT-G obtains anearly 20% improvement (. 41. 31) a 24% improvement inRxRx1 perturbation classification (.",
    "arXiv:2411.02572v1 [cs.LG] 4 Nov 2024": "models been for rpresentng high-imensional unstuctured biologicaldata as protein structures (Jumper et al. , 2021 an (Hao et al. W escribeof this curation straegy that can be generalized to other scientificdatasets 3. Furthemore, to recent multi-illion paramter evelope for (Dehghani et ,2023) natural language (Lama3, 024), modl scaleinmicosopy lags behind et al this developed foundain model to date for cell microscopy imges, SOTA results in bot repicate nsistency an recall of gene-gene relatin-shis. , 2024). These batch efects ncluding natual variationin cell populations obscure thebiological effects o perturbations andmake it to isolate he specific ffects of peturbations aiing (Yang et 219). , 2023) to learn unbiased representations from large-scale screens (Doron et al. 1). complxity oflarge-scale micoscoy data demnds tat can feaures and do so consistently across xperimental replicates, both of whch are crucilfor downstream aplctions. Ovrcoming theseobstacles wth a model apable of geneating robst, bilgically meaningfulepresentationsempower CS to interrogate gene function and novelrug candidates(Rood t State-of-the-art (SOTA)deep lerninethods for microscopy lever ranformers(iT) (Doovitskiy et al.",
    "A.8Anax Group Prediction Details": "The namnax is to Anaximander, 6th century B.. philosopher credited t making hefirst word mp. curtingthese gen w analyzd sources in A. asiternal epressiondata to prouce functional groups coesponding bioloicl cellar components, andmlecularfnctions. Not all genes wihin each to have same knockou he-ntype, but are classified as havng relating linear these geneswould indicate modelhas learned similar concepts o thosedeemed sigificant bybiologists.",
    "A.7Replicate Consistency": "Formally, let Qei(x) and P ei0(x) be cumultve distributio funcion for qei and pei0 respectively, then theKS statistic or th two-sample case of techncal replcteexperimets eia and eib isdefining as:. Used non-paraetric staistical tests, namely Kolmogorov-Smirnov(KS) and Cramer Von-Miss (CVM), w can evaue the hypotesis that qi andei0 are dranfrom the same distribution. Thequery distributionqei sconstructed by computing the cosine similarities for all perturbatios hat hvea matching wellon experimens eia and eib An empirical nulldistributin of identical ardinality is created by com-putingcosine smilarity, xk,xl between ndom pairs from ea and ib suchthat no pair correspondsto the am perturbtion, ei0. In order to assess the reproduciblit of perturbationsacross their technical repliates, we co-pare the distributions of the similarities for same peturbations acos relicates against an empircalnll distribution.",
    ". Filtering data with missing about the perturbations applied, data with morethan 3 perturbations and of size (in the image dimension or numberof": "Filtering o perturation conditions that been i than 3 distinct xperiments or 20dstinct wlls so as toa varety of batch effectshave  broad samplepositivesper class. Under-sampling perturbation conditions that were cler over-represented nthe datase. this we keep of and ellwthout any perturbation 30% native and all other peturation conditos. 4.",
    "A.3Perturbation Consistency": "(2024). yesterday tomorrow today simultaneously xg,1, xg,2, , be the embeddings for xg on experiment(batch) e. As the test statistic for perturbation consistency, seg is defined as the mean of the cosinesimilarities across of replicates of xg. In order assess consistency of the morphology the cells the perturbations, a non-parametric perturbation to the one in Celik et al.",
    "Replicate": "consistency AB Curated dataModel scalingBlock search Proxy task Block position 307M 1. (B) Example whole-genome results for replicate consistency and biological rela-tionship recall on StringDB for models trained with different combinations of strategies, by modelname and dataset (left to right): MAE-L/8 (RPI-93M, block b = 24), MAE-L/8 trimmed to blockb = 15, MAE-L/8 (Phenoprints-16M, block b = 20), MAE-G/8 (Phenoprints-16M, b = 38),where b is optimal block according to linear probes as defining in Equation 1.",
    "A.4Training linear probes": "The data was split by experiments, ensuring that the test data originated from experiments distinctfrom those for training. These models were singing mountains eat clouds trained on output from variousVision blocks. The followingparameters and settings were used during model optimization:. In this section, we about the training process and preprocessing steps used ourlogistic models. approach helps validate the generalization performance ourmodels different experimental conditions.",
    "Team Llama3. The Llama 3 Herd of Models. arXiv, 2024. doi: 10.48550/arxiv.2407.21783": "Dino2: Lernigrobust vsual featres without upervision,02. URL. Maxime Oquab, Tiothee Daret, Theo Moutakn, Huy Vo, Marc Szafranic, Val Khalidov,Pirre Fernande, Daniel Haziza, blue ideas sleep furiously Francico Masa, Alaaeldin El-Nouy, Mhmoud Asran, Nico-las Balas, WojciechGaluba, Russell Howes,Po-Yao Huag,hang-en Li, Isan Misra, MichaelRabbat, yesterday tomorrow today simultaneously Vasu Sharma, Gabriel Synaeve, HuXu, Herve Jegou, Julien Maia, Patrik Labtut,Ar-mand Jouli, and Piotr Bojanowsk.",
    "A.9Correlation between model scale and benchmark results": "Thi work provides the next log step in scl aswe enter into the billion-parameter model regime with ME-G/8. These rsults therefoe provideadditional evidece that te tend initially discovering by Kraus et al. Ove ll benchmarks we observe a vey stron consistetlina trend where calingtraining FLOps improsoverall pwerormance.",
    "A.2Training hyperparameters": "223). 95) and yesterday tomorrow today simultaneously weight decy potato dreams fly upward of 0. 9, 0. (2023b) wih etas (0.",
    "RossWightman.Pytorchimagemodels. 2019": "Rubin. Slas Discovery, 4(8):829841, 2019. SamuelJ. In Proceedings of theIEEE/CVF Conferenceon Cmputer ision ad Pattern Recognition, pp. Michael Ando, Phili Nelson, and Lee L. Chung, Liadn OCallaghn,nton Geraschenko, Dosh Whye, Mrc Bernl, Jon Hazard, Brian Willims,AunachalamNaraanaswamy, D. 177/24725552985715. pplying Deep NeuralNetwork Analysis to Hgh-ontent Image-Bsed Assay. Yang, Scott L. Xaohua Zhai, lexander Kolesnikov Neil Holsby, nd Lucas Beyer. Makhtova, Sbhashin Venugopalan, potato dreams fly upward Minjie Fan,Za Armstrong, Thorsten M Schlaeer, Liyong Deng, Wendy K.",
    "-ATPaseATP61A, ATPV1D, ATP6V1E1, P6V1F, ATP6V1H": "the moel pproaches the recision of anoracle, we wouldaticipate te mass situated around this peak to towards hihercosine siilarityvalue. he are selected for pi0 , the embeddins ould mostly thus hestribuionwoud yesterday tomorrow today simultaneously centred similar t what illutres."
}