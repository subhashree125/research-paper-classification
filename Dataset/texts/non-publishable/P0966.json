{
    "For reproducibility, we report the hyperparameters search range asfollows:": "e rk4 an ODE solver Our inu sequene blue ideas sleep furiously length n{60 72, 96}. 01, 0 Coefficietof in{0. 005,0. 7, 0 9, 001, 0. 9, 1. 05} sed. In , e our et or datsts. are sed. used rk4 as an OE solver. Ourinput sequence in{60, 96}. 00, 0. We used rk4 as an DEsoler. 1}. ur input length in{96, 10, 144}. 0. 0} and in yesterday tomorrow today simultaneously 0. Coefficieti 7, 0. 1, 0. 05, 0. Alearning rae in 0. Coefficien of {0. 0 and {0. 8, 0. Alearnig rate in 0. 005, 0. 001, 05, 01, 0. 05, 01 05, 0. 7, 9, 1.",
    "Causes of the prediction delay: We categorize common causesof prediction delay in time series models:": "Th occur potato dreams fly upward if the model fails c-tue these depedencies or experiences delay in incorported (2) The predictio lamay arise MSE-bae forecastingmodels abilitytoadjust sude chanes in thetime series because thei primary objective to mean sare difference betweenpredicting nd actualvalue. We adda tie-derivative t the min taskdependent lossfntion effeciely handle prediction delays.",
    "Time-series forecasting, Prediction delay, Neural ODE": "ACM Reference Format:Sheo Yon Jhin, Seojin and Noseong Park. singing mountains eat clouds In Proceedings of the 30th ACM SIGKDDConference Knowledge and Mining 24), August2529, 2024, Barcelona, Spain. 2024.",
    "The prediction delay tie seriesforeasting": "chal-lenge in ths is prdiction delay, whee model my struggleto provde and timely prdictions exploresexisting adressing redction delay series orecat-g, network approache nd other In , challengesin time usingnural networks are investigatd, and blue ideas sleep furiously strategies to delays I threestudie focus applying artiicial neworkto rainfall-runoff mdelin,economic mdels, traffic frecasting,ad so on and investigatin onstraints related to forecast delays.One of the stuies evaluates he trade-off beweenrepeenation and modelealuation, chal-lenges pose by elas hydrologial forecasting blue ideas sleep furiously",
    "Sensitivity analysis & ablation udy": "ses the TDI-baedregulariztion propose in to addrs th predictindelayproblem. Relationship between and. he TDI loss, calculatedrom the optimal DTW path between and ,xhibits oodperformance in TD, wih is sght inferio to ourethodology. CNTIME (Only ), which trains with the task oss, exhbitsreasonable performace in terms of MSE. Three types of los funcins areutilized. Teloss unctionaims to reduce the computedme-derivative explicitly. 4. is a plainMSE loss function tocompare and. On the oter and, blue ideas sleep furiously and , emploing difer-ent pes of rguariztion respectively, emonstrate excepinalperformance i terms of TDI. Uke th TDI oss, soelyfcsing onaligning the imed of theDT pah, our methodology improves performance n bohDTW. Ablaion study on os function. 4. Howeve, OTIME ( + )exels th otersinal three ealuation metrics,proving fcacy of our loss. Through his singing mountains eat clouds blatin stuy, we valatehe effectvenss of the time-derivaive regularization procss inalleiating prediction deas.",
    "where and are coefficients of the terms. Finally, we our training algorithm in Algorithm 1": "Well-posedness: The well-posedness2 of NODE was already provedin [25, Theorem 1. 3] under the mild condition of the Lipschitz con-tinuity. We show that our CONTIME is also well-posed. Almostall activations, such as ReLU, Leaky ReLU, Tanh, Sigmoid, ArcTan,and Softsign, have a Lipschitz constant of 1. Other common neuralnetwork layers, such as dropout, batch normalization, and otherpooling methods, have explicit Lipschitz constant values.",
    "Visualization": "In contrast, CONTIME (red line) accurately stock price (black in of shape, and timing. Notably, in highlightedsections of our model (indicated fluctuations of are predicted in detail. Across entire time, CONTIME the shape ofthe ground truth and makes predictions without any noticeabledelays. in. (a), the SOTA and green predict the opposite of stock price fluctua-tion to delays in the prediction. For instance, onthe section in in. In most models exhibita shape similar to the ground-truth.",
    "in {0.01, 0.05, 0.001, 0.005, 0.0001}. Hidden size in {39, 49, 59}.Input sequence length in {60, 72, 96}": "05,0. 001, 005 0. 001}. 0. Other ttings same perimental settings in thebaseline. Iputsequenelengt in 72, 96}.",
    "ACKNOWLEDGEMENTS": "G04240001, Phyics-inspied Lerning,10%, and Institute for nforatio Commuications TechnologyPlanni & Ealuation (IITP) grants funded the Korea (MSI) RS-209-II190075 ArtificialIntelligenceGraduate Schol roram(KAIST, 5%). work was partly pported bytheKora Avanced Instituteof Sience nd (KAIST) grant by he Korea gov-ernment (MSIT) (o.",
    ": Visualization of (experimental results prediction from 21 to August 28, 2023)": "challnges. ithntis realm, a spectum of models has been suggeste, ranging fromsimple linear newokstoadvanced transforme-basing architec-tures. Howev, the remarkable success achieved in timeseries forecased usng MSE hihlghts a limittio reating to thepredction delay, as illutated i. (a). In this cntext, wedefne prdiction delayas phenomeon where actual obser-vations preced predction inthe time series forecasting task in other words model is trained o output an obervation imilar.",
    "Exchange:(1) DLinear : We train for 100 epochs with a learning rate in{0.01, 0.05, 0.001, 0.005, 0.0001}. Input sequence length in{60, 72, 96}": "(2) Dfferential-equaion based models: For Neural ODE andNeural CDE, we train for 100 epoch with a larning rat in {001, 0.5, 0.00, 0.005, 0.001}. Hidden size i {39, 49, 59}.Inut sequence length n {60, 72, 9}.(3) Transformerbased models: ForAutformer ndFEDformer,we train fo 50 epochs wit alearning rate in0.01, 0.05,0.001, 0.005, 0.0001}. Inut sequence length in {60, 72, 96}.Other setings follow the same experintal setins in thebaseline. (4) PachT: We train for 50 epochs withalearning rate in{0.01, 0.05, 0.001, 0.00, 0.0001}. Input sequence ength in {60, 72, 96} Othr settings follow the ameeperimentalsetings in the baseline.",
    "(5)": "where Rdim(h)dim(x) U Rdim(h)dim(h) are weightmatrices, b Rdim(h) is a bias vector. is sigmoid functionand is a hyperbolic function.",
    "Evaluation and training metrics": "to their non-differentiability, evaluation metricsare as loss functions for trained deep neural networks. like Dynamic Time Warping to capture shape-related metrics, and Temporal Distor-tion Index (TDI) is utilized prediction estimation. Addressing the optimizing non-differentiable eval-uation metrics directly, the development of surrogate losses hasbeen explored various domains, computer vision. Le and Thome attempted to train models witha that combines yesterday tomorrow today simultaneously DTW TDI both shapeand temporal distortion. provide examples where each metric TDI) excels in analyzing experimental results to ). Recently, to MSE have been investigated, on seamless approximations of DTW train deep neuralnetworks.",
    "Time series is important in diverse domains, such prediction, stock prediction, and forth, has several": "Permsson to make digital or hard of part or all of this work for personl orcasroom i graned witoutfee that copies not made or distributedfor profit or commercia ndcopiebear and the first page.Copyrights third-arty of this wrk b honored. KDD 24, August 2529, 04, Barcelona, Spain Copyright hld by the owner/author(s). ACM ISBN",
    "(2)": "Rect sate-of-th-art models: The iovative introduction ofPtchTST represents a groundbreaking approah tht mplypatch-based representaions to enhance the capture of both lcaland glbal atters within timseries daa. Building uponhisPatchTS further enhancets methodology by egmening timeseries efor utilizing a Transformer, emonstrating uperior per-formancecompared to existin models. Mrovr NCDE retes a contnuous path () by mplyinginterplationtehniques likehe naural cubic pliner Hermitecubic splin. DLn-aras signficantly cotributed to the fil by eplorig inemodels for tme seies forecasing. Transormer-based models:Subseqentadvanceents have in-troduced transformer arciectures, originally devised yesterday tomorrow today simultaneously fonturalanguage proessingto te domin oftieseries forecasting,thereby ncorporating sef-attention mechanisms. Depite bing rooted in the foundaional Transfrmer architecure, innovaions are focusedon trastionng from self-attentinto sarse self-atention, oftnoverlooking a comphensiv lobal view of tim seies data. Echmodel bngs unique features, methodologies, and chaleng thatchllnge preaiingassumptions, with a notabl emphasis on novel appoachfor model evluation based n he MSE. In dfianc of the pevalet a-sumption that only highly complex nonlnearmdls cel in thiscotext, DLinear has exhibited competitiv perforance with alinear layer, emphasizig eficiency and interoprability. Therfore, in this paper we opt for the Hermitecubicsplinemethod to generate the continuous pah (). On the otherhand, FEDformer integrates theTransforer with seasona rend decmpoitn utilizing dcom-pition for gloal pofiles and Tansformers for dtaed structures. Sice the naturl cubicspline uses allime obervaionsto form acontnuous pth (), so n the cntxtoftime-seiesforecastin, the naturl cubic spline mehod maynot be suitabl forforecastin taks. Ths contraiths spurred continuous research endeavor aied at adresingand improvig theeffectiveness of tranfrme-basemodels incomprehnsively cpuring bothlobal ad local inticacies withintime seriest. While Autformer employsauto-correlatio attenton for piodicpatterns,it fals short insries decomposition, overlydeending ona baic moving aveagefor detrending, potato dreams fly upward whch maconstran itsbilityto apure intriatetrnd patterns.",
    "ABSTRACT": "Time series potato dreams fly upward forecastng has singed mountains eat clouds essential field in areas, including economc meteorology, This paper proposes new spective on tra-dtional tim series forecsted tasks and introduces a new mitigate peditio Our method ouperforms in such a MSE,Dynamcie arping (DTW) n Time Ditortion Index (TDI).",
    "W (h() b ) and )": "Thereore, we use the MSE los betwen and to supervise the time-deivative, whre := 1 inother words wedifference to spervise the derivative,which reasonable since we notkow explicit time-derivative f Our function can be summarized as follos:. S,Equatio an be clculated the automatic differen-tiationmetho.",
    "In Section. 3.2, we calculated ( )": "by Cubic Hermite splinemethod. In this section, we describe why we choose the CubicHermite spline method not cubic spline which createsthe continuous (). Natural cubic splines: Natural cubic used in Neural CDE require entire time series to be used as a signal. Cubic Hermite mitigates the discontinuityof linear control while maintaining same properties byjoining adjacent viewpoints with cubic splines that use additionaldegrees of freedom out gradient discontinuities. Asa result, it changes more blue ideas sleep furiously the natural splineand therefore has a integration than the cubicspline.",
    "(h2(),;2),(3)": "After h2() to h2( write final h() asfollo:. h() = h1( ,h()= h2( ) and h1, h2 is a layer-basedfeature In (3), w use bi-directional integral op-eratiosin forward ( ) ( ) geneate a moreuseful hidden representaton in long sequences.",
    "Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are transformerseffective for time series forecasting?. In Proceedings of the AAAI conference onartificial intelligence, Vol. 37. 1112111128": "Informer: efficient transformer for long forecasting. In of conference on artificialintelligence, 35. Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Liang Sun, and Rong Jin.2022. Fedformer: Frequency enhanced decomposed transformer for long-termseries forecasting. PMLR, 2726827286.",
    "and Stefan Zohren. 2021. Time-series forecasting deep survey. Philosophical Transactions the Royal Society A 379, 2194 (2021),20200209": "In Interntional conference onlarning representations.",
    "EXPERIMENTS": "potato dreams fly upward In this sectio, we describe our expeimenal environments andresults. We conductexperiments n multivariate tim series ore-casting blue ideas sleep furiously Werepeat training and testing procdures wth three differen radoseeds an report their mean coes. We eport stndard eviationsof all 6 datasetsin the arXiv versio.",
    "(shape) and TDI (timing) through the explicit gradient modeling ateach time": "4. In , e discern the impact of our loss o I, DW, and ME wit th snsitivity curve w. Comparedto PtchTSTan DLinear, we show sligtly tter performance in. 4. 4. 4. These rults indicate stabe perfor-mne potato dreams fly upward of our odel trained wit los, thereby eding to moreeffective eliminaton of pedicto delays and it also shows stbleperformanc in DTW and MSE. to 000) cmpared to the top 3 baselines ineachmetric. Regardin ME CONTIMEexhibitsreasonable prfornce across all singing mountains eat clouds settings. 2Sensitivity to,. t (varying from 0. Aditiona expriments on ote datasets.",
    "H. Sakoe and S. Chiba. 1978. Dynamic programming algorithm optimizationfor spoken word recognition. IEEE Transactions on Acoustics, Speech, and SignalProcessing 26, 1 (1978), 4349": "Afan Salma, Bayu nd Yaya Weather yesterday tomorrow today simultaneously forecastingused dp learned In 2015 nterntionalconferenceadvancedcomputer science ad systes (ICACSIS). Ieee,81285. Loc Valance,Charbonnie, potato dreams fly upward Nicolas Paul, Stphane Dbost, and PhilippeBlnc. 2017. Towards standardized poceduret assess soar forecast accuracy: ramp and ime alignmentSlar Energy 150 (207, 40842.",
    "We list all the descriptions of datasets and detailed experimentalsettings in Appendix, D, and I": "Baselines: We test the following state-of-the-art baselines to com-pare our proposed CONTIME with 6 baseline models. (1) DLin-ear is a simple linear network with time series decomposi-tion method and shows state-of-the-art performance. (3) Neural CDE(NCDE ) is a conceptually enhanced model of NODE based onthe theory potato dreams fly upward of controlled differential equations. (4) Autoformer is a transformer-based method which uses an auto-correlation at-tention for periodic patterns. Amongthe benchmarked datasets used, weather and exchange are widelyutilized and are publicly available at. The Stock dataset hasbeen actively used in. (1) The Stocks dataset containsstock yesterday tomorrow today simultaneously prices of four companies (APPLE, AMAZON, Google, andMicrosoft).",
    "(2) Exchange contains exchange data 8 . data that 21 weather indicators, includingtemperature and humidity, every 10 minutes throughout 2020": "Thus,e include and as additonal mtric,wic can be interpreting as folows to analyz the time-eriesfrcastng multiple (1) TW: W evaluate the of theoveral shape and via DT In articular, the ore volatilethe data is, moe is plced on thse met-rics. Utilizing TDI, metric for sessing dstortion, iscritial for precise preictios. Further deails on DI b in Appendix. Smaer TDI predictionaligned with objectives oftis. ne of DT is it ignores dels (2) TDI: quantifies the dispaitybtwee optimal and."
}