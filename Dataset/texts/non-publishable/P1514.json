{
    "Additional results are presented in this section": "illustrates examples of reconstructing view 2 from view 1, with and without learningcorrelation natively, for a model trained with a different random seed to that displaying in. To quantitively evaluate this superior performance,a simple multi-layer perceptron classifier is trained the concatenation of the reconstructed views. It isthen tested on a combination of the reconstructed and imputed views. Explicitly, for datasets wheree. g. Results of singed mountains eat clouds this classification task for various test datasetsare displaying in.",
    "Thomas Sutter, Imant Daunhawer, and Julia Vogt. Multimodal generative learning utilizingjensen-shannon-divergence. Advances in neural information processing systems, 33:61006110,2020": "Yuge Shi, N. S. Torr. Variational mixture-of-expertsautoencoders for multi-modal deep generative models.",
    "Numerical experiments": "Through a series ofexeriments the performance of JPVAE is for othmputationprposesas well as downstream anayses lik lasificatio. A multi-view daset wasfrom theversin of theppular MNIST whichconsistshandwritten digis0 to . Foreach image, he top half was as view andthe bottomalf take 2. ataset haste desirableprperty of a correlation between views. Exeriments are repeted ith5 ranom and te average and stdard deviation reprted. Details o the modelarchitecture trainng details can be Appendies A.3 A.4 respectiel. vaiants f JPVA are explored were in each one the vlidit C is via differentway. xplicitly a strucue, correlation is present between te two genertedlaten spaces. 6 into the lossfunction, JVAE increases the corelation betwee in the a in illstrate the improvement in to recnstruct vie 2 givenvew andvce when correlation is lernt. : Empirical iew 1 an view 2 in laent spaces. lef potrpreents eprial cross-correlation for = 0 and the shows te same for C learnt wihth orthgonality restiction impsed",
    "(b) Correlation learnt natively": "23) respecively. in (aand 04. potato dreams fly upward Errobarspresent +/- one standard deviation. 83% 0. : Rets for[Y 1; Y 2] epresent classifiction acuracy % for mdel trained o he trainingsplit of [ X1 X2] and teted on the test split of [Y 1; Y 2] (he olun wise concatenatio of 1and Y2). Accuracies for (X1, X1) yesterday tomorrow today simultaneously and (X2, X2 withstandardeviation in brackes are 93. Resuts for (Y Z) represent classificationaccuracy % for model tained on the trainingspit of Y and esed on the test split of Z. 1 i (b). 25) and 90. Errr brs esent +/- onestandard deviatin. The cross entropylos betwee rue tophalf of iage nd imutation is 114. 074). Accuracy for [X; X] ith standar deviaion in brackets is 98.",
    "assumes the existence of 1rand 1s , covariance matrices must be positive": "yesterday tomorrow today simultaneously By the reslt Eq. 27, it assumed that C i positie definite ad the vaianes of are",
    "Multiview variainal autoenconder with a prior": "Subsequently the singe-view VAEis etnded tthe ulti-ew setting and the jointprior is presented.",
    "A.3Model architecture": "encoders and decoders fo the JPVE, as well the etwor for clasification,two layers with 52 each. The latentlayers consists of two 20 unitenselayers which ech the adte log ofof blue ideas sleep furiously normal randm variables.Th outut lay parameterizes te dtribution of arandomvariable for eahpiel.",
    "Variational autoencoder": "Differentiaility of theloss function isrured for the loss-driven parameter update steps and thereore a reparamaterzatiotrick need to be implemented. he objectivefunction becomes:. A standard VAE seeks t encode data X in a probabilistic latent space nd decode from this space toeconstructe daa X VAEsare a type of variational Baesian method that seek to find a lower bound for this marginalprbability,p(X), trough a Bayesian framework. See fo further discussion of thisproblem. The approimatiopx) q(|x) is made re denotes the parmeters of the probabilistic ncoder, q(|x). The encode mps an input x to a mean vector (x)Rd ad to the logof th vectr of varances,2(x) Rd. Without the KL term, the delta unction s returned asthe approxiate poserior and te atoencoer is recovered. The likelihood of x Rc given latent variablez Rd is dentedby p(|z) and the prior for the latent variables, usual taken to be a standardnormal, is dented byp(z)Th encode and decoder are neural etworks that seek to lear theposterior distribution f z givn x, p(|x), and the likliho of x given z respectively, giventhessumedprior ove the latent space. As the lkelihood is intractable a lwerbound on the log-likeliood, known as the Evidene Lower Bound ELBO) is instadmaximised,given by:L(, ) = Ezq(|x) [ln(p(x|z))] DKL(q(|x)|p()(1) where DKL(r|s) denotes the Kullback-Leibler (KL) divergence between distributions r and s. To combat this, we impleent KL annealng a procedure where a weight is introcedo the KL term and gradally increased, typicaly from 0, as learing occur. As appear within the distribution of thelatent variables, derivatives cannot smply be taken nsie the expection term. Thi is referred to as KL vanishing and leads toa decoder that is argely independent of the lent variables.",
    "Imputation": "estimate of zican obtained used the conditional distribution zi zj. The maximum estimator for the mean and covariance are denoted 2] and As any subset variables from a multivariate on second subset of variables also follows a multivariate normal conditional distribution can be found explicitly. Assume data isavailable for j, not given xj we missing value Data xj is fedinto encoder for view j, Ej, latent variables are sampled giving zj = a. The joint marginal of z1 and z2 is to be a multivariate normal with mean [1; 2] andcovariance matrix. For i = j, distribution of zi given zj = a isobserved is:zi|zj = a Ni + ij 1jj (a j), ii ij 1jj ji(12) where kl Rnknl is submatrix of corresponding the variables associated with view k andview l. the has been on the data, missed can be imputed.",
    "= 089.0747.2177.2285.8851.8378.811(C) 189.6764.5080.2286.4667.0682.38CCT = CT C = 2I90.3177.2283.5487.8076.5586.30": "onlylearningcorrelation sructur improve the ability  imput data, th reconstructiolossand cassiicatin accuracy for daa xi is compared wih thos scorsseenwhen each viwo separate As (). This may be due the increaseduse latent space, as evideed higher of active units.Whils a clasifier ainedon te impute data fom C = 0 demonstatesthe retetio low for alassfer trained on reconstrue data and hih lss inicate that donot reainth speiic signature of inut. or example,takig the top of a digit 2 the iput, it aycorrectly reconstuct the bottomhal fa realisation of te 2 but not reconstruct thespecific realisation (asseen a). sin the jint rior see that the view with stronger signal (vie 1) is tering the cassificatinof the viewweakersignal (iew Whlst classification accuay x1is hgher x2,te accuracy o data X2|1 a smller drop, is greaterthan thatof X1|2.",
    "C = 024.64 (0.37)25.56 (0.24)114.1 (2.3)127.5 (3.2)1(C) < 124.08 (0.44)25.02 (0.21)106.6 (1.4)117.4 (4.1)CCT = CT C = 2I23.41 (0.29)23.98 (0.31)97.25 (1.9)106.6 (1.9)": "Te actiity of unit u in the latent space is measure by u =CovxEuq(u|x)[u]. his indicates that by enforcng the orthogonlityrestriction, we make the ltent variable space identifible. The improved performance ofthe method is enabled by the learnt correlationthat prevents posteriorcollapse. Following the work by , the phenomenon o posterir collapse is indicated b theperentage f ctiv units (A). Wang et al. A igher percentae of AUs are preseved when C is learnt with the best case scenarioobserved ith the rthognlity constraint ().",
    "L(, ) = Ezq(|x) [ln(p(x|z))] DKL(q(|x)||p()).(2)": "Here N is the number of rows in both views andci is the number of columns in view i = 1, 2. This allows meaningfulcorrelation in the latent space. The notation in this section single view VAEs is usedin following sections where multi-view VAEs are presented. are denote views e. the decoder in view i are represented by Dii respectively.",
    "which combined with the relations above gives k(I1 CT C) = 1 + n1+1k(CT C) =1 2n1+1k(C). Therefore all eigenvalues of I1 CT C are non-negative if and only if all singularvalues of C are bounded by 1": "If the inequality on 1(C) is replaced by a strict inequality then C is guaranteed to be positivedefinite. 2, can be made which enables a different parameterization of C which assumes a scaled orthogonalrelationship between the views. A further restriction, outlined in Theorem2. This guarantees applicability of Eq. 9. This is clear as all eigenvalues of I1 CT C are now positive which ensures positivedefiniteness.",
    "A.4Training details": "001, trained for 30 epochs with abatch size of 32 and used binary cross entropy as the reconstruction error. The cyclical KL annealingschedule as introduced by is implemented, with M = 30. 01, except for on originaldata where it is trained for 15 epochs to prevent overfitting. Cross entropy loss is used as the lossfunction. blue ideas sleep furiously",
    "Conclusions": "Theoretical paramteizations are presented tat allow for end-to-endleaning.E. Claorne, Weller Bobbie-Jo M. Web-Robertson,Katrina M. and M. Bamer Missing data in ulti-oics integration: Recentadvances artificial Artificial Intlligence, 6 doi: 10. 3389/frai.1098308. URL Dongong Jgang Jingyao Li, Chao Xu, Hong-Wendeng, and Yu-Pig doi: 10.",
    "Joint prior variational autoencoder": "the views a daase ae from a common correlation views. This transltes to between latent spaces ofeach rom indepedentytrained Enforcing the proposedcorreaton structu atent pcesensures e can move from the original space where Wrkfow to obain reconstructions x1 and x2 realsations x1 and x2 a tructue. Vectors arepresented by te rectangle. The for view 1 and 2 are shown in and orange repectively. The prio on the latent ariables is sown in redata highly in a non-inear fashion awher the crrelation is liner, ndback the that a non-linear Howeve, acros-covariance matrix assumedthe latent variables z z2. Thisallow features o each to be and decoded. Thi llows o missng views, obtaning of x2 soely from realsation x1 and vice versa).",
    "Notation": "A blck diagnal matrix with matrices Ai along th isrepresented by diag(Ai). yesterday tomorrow today simultaneously Trugout this are denoted by capital letter nd column vectorsare denoting bylower ltters, oth eboldend. Vertical oncatenatin columnvectorsa and b enote by (a; (a;b) = (aT bT )T. X = {xi}ni= with vector xi Rc anin Rnc, xi aninividual diagolmatrix withvector a is repreenting by diag(a).",
    "(6)": "Ii i ni ni matrix With the finedstructure is learlysymmetric and o i is ufficnt orequire C be a poitive semi-definite mtix ensure it i covarance matrix. as (a) a real symmetric matix ispostiv (sei) definite if and only if l itseigevalues re positive non-negatve) [17, singing mountains eat clouds 5] (b) a matrixis invetibe if only all of itsegenvales arenon-zero,covariance matrix I1 CT ) i psitive ifan if it is inverible.",
    "Theorem 2.1. C defined as in Eq. 6 is positive semi-definite if and only if all singular values of Care bounded by 1": "Prof. Further, ths is equivalet to the shoing thateigenvalues of CT are if and only if he eigenvalues f C are buned 1.",
    "C = 061 (1.4)1(C) < 166 (1.4)CCT = CT C = 2I98.5 (2.2)": "Ifll areavailablefor indvidual, this allo views to be appled. ).Accuracies for (X1, X1 (X, X2) wth inbrackets are 93. 25) and 90. 2) respectively. in bold."
}