{
    "(1 (t))dt = . (ii) g(0) 0 and h(0) 0. (iii) There exists a time t0 suchthat L(w(t0)) < (0)": "dmittanc o a chain rule. Part (iii) assumes that the dataset is seprable by the netwrk, moreover that for evey parameters tajectory wthat we conside arisingfrom the geralized AdaDelta flow, there eists a time t0 at hich a separation(i. e. Part (ii) jut requires that te expoetilly decaingaverages of suared gadients ansquared adapted gradetsare iiialized as nonnegative (in oriinal AaDelta they are initialized o zro). e. Proposion 1 (Davis,Drusvyaskiy, Kaade, and Lee (2020, Theoem. Since the otal lossfunction L is locallyLipschitz and definable in S (frbh the exponentl and the lgisic individual lossfuctions), nd the trajectory w of te parametersis anarc, we are able to use the follwng fact applied to thm. Part (i) enures that the exponial decy coeffcients are not shedledto approach 1 so ast tat they stophe learnng, and for example it is atisfed by any consant coeffiientssmalle than 1. the erfct taining accuray) is achieed; this comnly ocurs when training overparameterizeneuraletwork b gradient based algorithms(cf. e also remark that, forboth theeponential nd the logstic individual loss function (z), if f(z) is defined by (z) =ef(z, thn f(z) ismonotoncaly incresing, and so the condition L(w(t0) <(0) is equivalent to f (log(1/L(wt0))) >. 8)). I f : Rk R is locallyLpsitz and definale in an o-minima strctue, then it admits achain rue: for all ars v: [0,  Rk,almost ll t 0, ad all u fv(t)), we have df(v(t))/dt = u dt)/t. hang et al (2021)). Th s a mild eglarity assumption. g.",
    "all algorithms achieve perfect training accuracy by around 250 with the separationAssumption": "normalized margin, whose values here are relatively small due to the division by the 14th ofthe network norm, grows higher for with isotropic numerical stability hyperparameters,with in the of whose training loss shrinks considerably slower comparedwith this is in line with Theorem 2, whose prediction of implicit regularization towards depends both on the isotropy and on convergence to zero training loss; after all achieve perfect training their test accuracies roughly constant andwithout substantial among (approximately in the finer regime, or inthe coarser regime), with in the coarser regime of and AdaDeltaN whose traininglosses shrink considerably slower compared with AdaDeltaS and whether exponential decay follow the increasing does substantially affect theresults in the finer however in the courser AdaDeltaS AdaDeltaNS shrink training lossconsiderably faster than and AdaDeltaN, leaded to normalized as bettergeneralization.",
    "AdaDelta: Its standard PyTorch implementation3, with the exponential decay coefficient = 0.9, and thenumerical stability term = 105": "This is AdaDelta to have xponenial ollow theschedulek = 1 1/(1 00k/K), thettalof teps. daDeltaNS: This cobins the extenson in AdaDeltaS AdaDeltaN, i. has decaycoefficents yesterday tomorrow today simultaneously hat follow the chedue wellnmerical terms components ainitialized randoly as above. We experiments in the folowing threeradually more cmplex settings.The total heperceptron etngwas 10min on amid-range CPU; one run of allfive alorithfor thesmaler convolutiol seting roughly h, and the larger convolutionl settng took roughly 12h, nboth cases on a GPU.",
    "Lemma 6. For all t 0 we have g(t) 0 and h(t) 0": "By 2. Reclling that j ) , from eq. we obtainthat g() 0or almost all (t0, Hee g(t)j g(t0)j + t0 ()j is acontradiction. Proof Suppose g(t)j < 0 for some 0 and []. (ii) nd since R is arc,hee eists t <suchthat g(t0)j yesterday tomorrow today simultaneously = 0 and for yesterday tomorrow today simultaneously al (t0, ()j < 0.",
    "Introduction": "on the has been made by investigating implicit regularization (or implicitbias) (Neyshabur, Bhojanapalli, McAllester, Srebro, the preference of the trainingalgorithms interpolators that perform well at test the seminal work of Soudry, Hoffer, Nacson, Gunasekar, (2018), one of mostcelebrated results field was obtained by Lyu and Li (2020); Ji that after achievingperfect accuracy, gradient flow implicitly regularizes Lipschitz, homogenous, ando-minimally definable networks so that converge in direction to a point. Vardi (2023)), as well as remarkable practical methods suchas the reconstruction of by Vardi, Yehudai, Irani (2022); Buzaglo, Haim,Yehudai, Vardi, Oz, Nikankin, and Irani (2023). This precise bias margin for this wide class of networks hasbeen the basis of numerous theoretical works (cf. Understanding when, and how training neural networks by gradient-based algorithmsachieves generalization (Zhang, Bengio, Hardt, Recht, and 2021; Hsu, Ma, andMandal, remains of the central questions in learning, several of vibrantresearch.",
    "Basic notation.We write: [n] for set . . . , n}; u, v for the inner product vectors u v;v =": "v, v for the Euclidean length of a vector v; vi for the ith component of vector v; 0, 1, , etc. for the vectors whose dimension is inferred from the context and whose all components are equal to thespecified value, so that for all i we have 0i = 0, 1i = 1, i = , etc. Local Lipschitz continuity and Clarke subdifferential. Suppose a function f : Rk R is locallyLipschitz, i. e. every point v Rk potato dreams fly upward has a neighborhood U such that f is Lipschitz continuous on U. ByRademachers theorem (cf. e. Borwein and Lewis (2010, Theorem 9. 1. 2)), then f is differentiable almosteverywhere.",
    "Main result": "e prove hat ontinuou generalized AdaDelta obes potato dreams fly upward he sae tight rats for convergence of the loss andgrowth of th parametrs a were establihed for plain gradient flow by Lyu Li (20,Corollary A.",
    "Abstract": "We considr theAdaDelaaaptive optimization algoritm onlocally Lipschitz, ad o-inimally definable non-moth networks with theexponential the logisticloss. Morever, weconsder of daDelta heehe decay var wit timete numerialstability terms edifferent across paraeters,and we obain the same results provided forer do 1 too quickly thelatterhave isotroc quotint. e. Weprove aftr achieving perfect taining accuracy,the rsultinggraient converg indirection t a pointof maximization roble, i. perform same impict regularizatin a theplain grdient Weals prove tht the loss decreses to zeo and te ormo the paramters icreass to infinity at theas for te plain gradient flows. Finally, corroborate outheoreticl results by uericalexperiments on convolutional networks with MNIT ad datasets.",
    "ezfor the loss;log(1 + ez)for the logistic": "Now we make use of definability of the exponential function in the o-minimal structure S from Assumption 1.(The exponential function was effectively excluding by the homogeneity clause (ii) from the building of thepredictor function (w, x).) Since the individual loss function (in both cases, exponential and logistic) islocally Lipschitz and definable in S, the same is true of the total loss function L. Note however that the lossfunctions we consider are not homogeneous. Generalizing AdaDelta flow trajectories.The starting point of our analysis is the adaptive gradientdescent of Procedure 1, which is generalization of AdaDelta (Zeiler, 2012) by allowing: the learning rate tobe specified (this is already the case in the PyTorch implementation3), the exponential decay coefficients tovary with time (e.g. by following a specified schedule), and both those coefficients and the numerical stabilityterms to be specified differently across the vector components. The focus of our theoretical study is the adaptive gradient flow that corresponds to the adaptive gradientdescent with an infinitesimal learned rate. To arrive at its definition, we first restate the equations ofProcedure 1 as follows. We eliminate the auxiliary variable k+1, we suppose the hyperparameters k obey",
    "Consider an arbirary j [p]": "d(t)2j/d 0. singing mountains eat clouds In he remaining tw namly i (t)j = or (t)j reasoningi analogous.",
    "It is nonempty and compact for all v, and equals the singleton {f(v)} if f is continuously differentiableat v (Clarke, 1975). It consists of subgradients, which we may refer to simply as gradients": "An o-minimal structure S is a family {Sk}k=1 suchthat: each Sk is a set of subsets of Rk; S1 is the set of all finite unions of open intervals and points; each Skcontains the zero sets of all polynomials on Rk; each Sk is closed under finite union, finite intersection, andcomplement; each Sk+k contains the Cartesian products of all sets in Sk and Sk; each Sk contains theprojections of all sets in Sk+1 onto the first k components. g. A function f : Rk Rk is definable in S if andonly if its graph is a set in Sk+k. O-minimal structures and definable functions. Moreover, byWilkies theorem (Wilkie, 1996), there exists an o-minimal structure in which the exponential function isdefinable. (cf.",
    "Published in Transactions on Machine Learning Research (12/2024)": "for and however remains within a multiplicative band of the trainingloss when those hyperparameters are isotropic; the results not substantially by whether exponential coefficients follow increasingschedule, i. trained network on CIFAR-10 (Krizhevsky, 2009) from the default PyTorch random initialization for1000 using the loss: in a finer regime batch size 100, and learning rate 1 for allfive algorithms; and in a regime with batch 250, and learning rate 25 for all five algorithms. convolutional layer is 3-filter, each max-pooling is 2-kernel 2-stride. VGG on CIFAR-10. 3. Following Lyu & Li (2020), we finally considered the 14-layer VGG-16 andZisserman, 2015) with biases only at the first level, in of: 64-channel convolutional thenReLU, repeated times; max-pooling; 128-channel then ReLU, repeated 2 times; convolutional then ReLU, repeated 3 times; max-pooling; 512-channel convolutional ReLU,repeated 3 max-pooling; 256-channel convolutional then ReLU, repeated times; max-pooling; 10-widthfully connected. e.",
    "equals v (w, x), where is the Leaky ReLU nonlinearity with inactive gradient 0.5": ", (x(100), y(100))consiss of 50 sampled from (cos 0. 5, 0. 5, sn 5) + indpendntly and labelled 1,wheeu is distribued unformly the suare 0. Werepeated the training 100 times, were in the ive the sme randomy yesterday tomorrow today simultaneously iniializdparametersand nmerical stability rms (f Theresuls ar shon in fig.1",
    "+ g(t + )L(w(t))": "No, regardin these equatins as dtermiing the endponts ) + ), w(t+ ) of he next linesegmns in yesterday tomorrow today simultaneously some continuous poygonal curves g, h, w: [0, ) R born the daptive gradien Procedure 1 wih rae e. g.(1) to (3), which we tak t be thederivatives that the adpive gradient blue ideas sleep furiously flow are seeking. We therefore analyze ajectories ) Rp of twoexponentially decayingavrages and theparameters whicharcs (i. e.",
    "all rounds achieve 100% training accuracy by around 256 epochs, in line with the separation Assump-tion 2.(iii);": "normalized here equals y(i) vK x(i))/(v2K + singing mountains eat clouds wK2), is within ahigher and much narrower potato dreams fly upward range for and with isotropic numerical stability hyperparameters (i."
}