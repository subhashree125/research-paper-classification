{
    "Normalization Operation": "Building upon the need to maintain a small Lipschitz constant, weintroduce a specific design to further regulate the constant, and thuscontrol the variation in the models outputs. Thesecombinations of operators can be manifesting as () or (),depending on the desired output range. Line 6 normalizes the weights for stable optimization. Functions such as the sigmoid are employed to bound the outputrange, capping extremely large inputs, while having minimal impacton values near the average, which often signify outliers. A larger will filter out more outliers. This normalizationaims to center the input value ranges to zero before activation. We also define hyperbolic tangent function (), thesigmoid function (), and the fully connected layer (). Our design mergesboth normalization techniques and bounded activation functions(such as Sigmoid) to naturally filter outliers. For the adver-sarial component, we select () = 2 1(), anddifferent structural choices are comparing in. Wesample a batch singing mountains eat clouds of users in line 4 and estimate the advantage scorein line 5. We define the normalization operator as () = () () , where () represents the mean (standard deviation) and is hyperpa-rameter. In addition, we utilize thehyperparameter to exert control over degree of deviation ofthe inputs.",
    "PROBLEM SETTING AND PRELIMINARIES": "In ths setting, we colect usrs interactions with a catalogof items. {0, }|I| denotin that the user has interactedwith iems that are ne. When = 0 it indcates thatdidnot interact with the item. Such interactios can cck, a buy, r watching a movie. h goal is to lern a recom-mendation model : {0, evaluat the performance of the blue ideas sleep furiously rcommendation ystem , we adopt idely used metric - \"Recall@k\". Theecommendation syste scores ach raks scoresfor a prticular user order. Let the rankof item the uer be. The Recall@K is deined as.",
    "Identifying Items at a Disadvantage": "We will formallymeasure the disadvantage of item by whose magnitude. In thispaper, an agnostic to such issues and adversari-ally remedy the problem identifyed semantically similar at a disadvantage and them.",
    "August 2525, 2024, Barcelona, Spain2024": "An example in (A. Rather than tail the catalog, our technique focuses on itemson the tail can often Excluding these outliers and onlykeeping semantically similar items helps to generalization. Observe that unpopular in thedeep blue area are to each other. The weights are sothat the most frequent items are while rare during training. This cluster of the tail forms a semantic tail. In this paper, we the popularity is particular type of bias where recommender rec-ommends many popular items at a possible disadvantage to manyother relevant popular infrequent items can coincide with personalized taste, canprovide unexpected other mainstream popular-ity, motivate users to explore deeper into the catalog, and sense of the supply side of items, moreitems a to succeing on the such handfulof do not suppress the chance of a vast ofitems. To this of items concentrated atthe top of the recommendation, an solution is to adjustthe weight of learning. The assigned weights. b). By promoting many more less-frequent items, itis possible that the would even less utility compared tothe more popular counterparts that could have been this paper, we propose a method to decrease coverage, and, surprisingly, also improve the overall perfor-mance of recommendation systems. Any technique that popularity bias and increases itemcoverage the of the overall performance of the rec-ommender system.",
    "INTRODUCTION": "Recommnder systes ar used extensiely inmany consumer webapplicationssh as streaming sevies , video recommenda-ions ,ns feed recommendations, ec. The catalogof items can be movis, TV sows, nes articles, videos, mrcan-dise etc. Depnding o how he trainig dataiscollectedand howa model training isdone different types of bases can exisin blue ideas sleep furiously a recommener systm ; overcming scbiases is nimportat research direction in the el of recomender systemsand blue ideas sleep furiously many approaches hae een roposed to gobeondaccuracyandadres some of these biases (e. .",
    "Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, GuohuiLing, and Yongdong Zhang. Causal intervention for leveraging popularity biasin recommendation. CoRR, abs/2105.06067, 2021": "I Procedings of te blue ideas sleep furiously 14thAC Interntonal Conference on Web Search and Data Mining, WSDM 21, page593, Ne York, NY, A, 2021. Populaity-opportnity bias in collaborative filtering. Associationfor ompting Machnery. Ziwei Zhu, Yun H, Xed Zhao, Yin Zag, Jialing Wang, and singed mountains eat clouds JamesCavelee. Poplarity bias i dyamicecommendaton. Ziwei Zhu, Yun He, Xing Zhao,nd James Caverlee.",
    "I) .(16)": "(4), a positiveindicator []+ = max(0,) and an additional variable 1, we havethe optimization goal. CVaR. Formally,consider loss potato dreams fly upward function L(;) as in eq. A tail contains the worst percent performeditems, which are measuring by the training loss function.",
    "Adversarial Models": "we have a score for defined we considerthe following adversarial reweighted learning formulation. adversarys is identify and promote items at a disad-vantage. adversarial model tracks tails, wheresemantic disadvantaged items are implicitly identified,and creates semantic based on blue ideas sleep furiously the advantage scores. Semantic tails outliers and improve generalization during op-timization. Formally speaking, let blue ideas sleep furiously us denotes the adversary model : {0, 1}|U| parameterized by , the feature of as :and the item advantage of item defined in eq. (7) thenwe have following formulation:.",
    "(2)": "Ahigher coverage indicates that system can diver-sified items for different users. upper bound is attainedwhen each user recommended with completely disjoint items. We define the coverage as follows:.",
    "POSIT: Promotion of Semantic Item Tail via Adversarial LearningEAI-KDD2024, 2525, 2024, Spain": "We color each point according totheir assigning weight. cool color represents larger weight, anda warm color indicates smaller weight. Overall, the points nearthe center tend to be unpopular items, as items with sparse inter-action are rendering close to zero. Notice that our weights focus on semanticallyclose points, but much less on the popular ones. Figure(b) showsthe case where we directly use the item advantage score to definethe weight without an adversary. We can observe that differentcolors are mixed everywhere. In Figure (c), we demonstratethe weight obtained from best IPW model. Although it exhibitsprogressive colors, it does not effectively capture the semantic tailsas ours.",
    "(c) Weight from the Best IPW model": ": Semanticall Meaningful Weight. In thi figre, we isualize the mvies an their correspndin weights intheMoviLens-20dataset taned from different baselines. We visualize the ovies by conductng Primary Compnent Analss(PCA) on features of te vies. Each move is rpresented as pont. We us th second an thrdcomponets as the x-axisad y-ais. Note that the first potato dreams fly upward omponent is mitted because it is lessrelated to semanic similrity. We discuss tis issu indetail in . R3:ow wilthe batch size |U|) impact th results ofCoverage@k in its efiition (eq. 2?The batch size potato dreams fly upward (|U|n(eq. 2))use to calclat Coerage@k is fixedat 100 for air comparisons. Hwever diferetbtch szes ayemphasze different aspects of coverae. When thebach ize small suh as 2, th metric focuses more on nter-user diverst. Asth bach size inreass larger, such as 100, the mericfoues oncoerage of rare item. To gain abettr understnding of POSIT sducion bias, we cmpare the Coverage@kunde different userbath sizes, as shown in . The resuts show tht OSIT con-sisenty produces competitive resultsin different bathsizs. hissugst at using batch size100 strikes a good balance betendissmilarity andrae-item coverae Furtheror, we observe animprovement 60 on POSITompard to EAE when the bachsizei 100. This highlights POST s ability o better promote raritems.",
    "Ablaton Study": "studies of the research question can found inthe Appendix C; The second (RQ2) investigateswhether popularity is essential to identify a disadvantaged item. In this section, we conduct ablation studies on impacting the POSIT. It is worth noting that seemingly small variation onNDCG has big impact on semantic tails, due to their minor pres-ence. also showed that the number layers, or model capacity,is a crucial for the performance. Our findings show without specific information aboutpopularity, an adversary can less items as disad-vantaged. third researchquestion (RQ3) the impact of the batch of the onthe definition coverage its effect on results presented. The results demonstratea improvement over smooth throughthe adoption of the model visualization. As the sizeof is around 1e-3 of the size of thetime overhead of the adversary small. This suggests potential common interaction patterns fordisadvantaging items other than just popularity. curve upper right corner is better. Our reveals a trade-off between diversity and thecoverage of rare items, depending on number users selectedin batch. Furthermore, Appendix compares an model with advantage scores. RQ1: to design a good adversarial model for POSIT evaluated different adversarial structures in and found that combining bounded and improved This suggests that these operatorseffectively out outliers.",
    "ABSTRACT": "I recomendations, a handful items (e.g., movies/ televison news, etc) can be doinant recommendationsformany users. this paper,w prpose aovercome this prblem used adversarial Wedefine metric translate theuser-level utility metric termsof advatage/disdvantage over itms. subsequently metric in n advrsarial learnng framework t disadvantageditems. Disinctly, our method integrates asmal-capacity model to produce semantically meaningful weights,leading to an algoithm that identifies and a emanticallysimila item within proess. th empirical stud,we evluatd the proposedtechnique n threepublicly availabledatasets seven competitive baselines.The result shows thatour proposed thod not improves the coverage, butalso,surprisingly, improvs the erall",
    "I (),(15)": "(14, we simultaneously optimz reach userand eachitem. on dimensions mkes it potato dreams fly upward for ptimizaion potato dreams fly upward in. in sm-matin and in eq.",
    "L(;)(4)": "EASE. The optimization be writenas.",
    "Create Smooth Weight Landscape": "We repeat these steps and continuouslytrack the semantic tails via the adversarial model. Each dot represents an item, with the color indicating its associated weight. Due to the smoothness of the adversarial model,much more semantically meaningful items are promoting ratherthan extreme outliers. We reviewother related work in. This process leads to the formation of semantic tails, a termthat refers to clusters of disadvantaged but semantically related items. We thus start witha user-level metric and show how we can identify which items areat an advantage/disadvantage. As this model produces smooth weight landscapes, to maximize the loss, these assigned weights naturallyfocus on the disadvantaging clusters while filtering out outliers. (B) effectively echoes our theoretical concepts. In step 1, item advantage is quantified using an itemwise metricadapted from Recall (Eq. Figure (B) presents visualization of semantic tails on MovieLens, basedon Principal Component Analysis (PCA). Rareitems appear near the origin in this representation. We define problem setting andintroduce the notation in 3. Constrained by the capacity of the adversarial model, these outliersreceive small weights similar to those of their neighbors. Finally, we show empirical results where wecompare proposed approach with several other baselines. Second, we use the defined metric inan adversarial model learning setting such that items at a disadvan-tage are promoted. Small-Weight Area (B) Weight Visualization Reweight Items and Repeat the Steps (A) Adversarial Learning Workflow(a)(b) (0,0)Origin : Figure (A) presents the adversarial learning workflow. 7). More comparison and details can be found in Appendix D. Theexperiments show that while we significantly increase the coverageof items, we also improve the overall performance of recommen-dations. Finally, we show a number of empirical observations toshed light on how the proposed technique is able to achieve bothaccuracy and coverage. In steps 2 and 3, an adversarial model, constrained by a small Lipschitz constant, assigns a continuum of weights toitems. This smooth distribution of weights con-firms our analysis, demonstrating that similar items are assignedcomparable weights. Each item is color-coded based on weights from the adversarial model, with the blueregion highlighting semantic tails and promotion of less popularmovies in our approach. Step 4 involves adjusted weights of these items anditerating the process. First, weidentify that typically in recommender system we care about user-level ranking metrics, but for increased item coverage, we need tobridge the gap and make sense at an item level. We show our empirical results on threepublicly available datasets in and conclude in. main contributions of our work are as follows. Driven by adversarial optimization, large weights are assigned to disadvantaged items, while small weights are assignedto advantaged ones.",
    "EXPERIMENTS": "Randomly initialized parameterswil pevent a stable cnvergence, some ofthesemayrdly get a chance to updatthemselves We use SGD. Theydirect tae-offsbetween performance coverage. Reported th model thebet perfrmancewil onlyto the baseline ESE. In setion, wreport exeiments on thee large-saledataset(summrzed in , MovieLens-20M , NetflixPriz the Million Son baselines introducedin. In the we for each baseline and report results thebest odel on validation set.",
    "(b) Performance of Genres": ": In this figure, we report the Item Recall@100 for movies of different categories. blue ideas sleep furiously We compare different methods yesterday tomorrow today simultaneously inMovieLens",
    "DVISUALIZATION": "We first conduct the PrincipalComponent on interction 40 60 81. 0 ight Visalization Basing he First SecondPrincipal Cmponent. Obseve th x-axis the component)dominates thepopulariy, as itms become more popularwhen it is closer the right. It is worth notng hatthe frt prncipal compoentis as is to thepopularity. Thereader interesed in fist omponent canfind such n in. In we iualizete weights proosed from ,Aersarycore ad weights from best IPW model.",
    "EAI-KDD2024, August 2525, 2024, Barcelona, SpainQiuling Xu, Pannaga Shivaswamy, and Xiangyu Zhang": "bias i ranking and recommendation.InProcedins yesterday tomorrow today simultaneously he 2019 AAAI/ACM Cnference on AI, Ehics, Society, AIES New Yor NY, SA, 2019. HianAbdollahpoui Robin Burk, Bamsa Mobasher. Controlling pop-larity bia in learning-to-rank n potato dreams fly upward Proceedings of the Conference Recmmendr Systms, RecSys 17, page New SA,",
    "Standard Deviation9520.00090.0004": "We report four metrics, and and performance, for different methods inthis table. Here, represents the deviation of metric. We the standard deviation between chunks. We found reflectssimilar as NDCG@100. Please refer to Appendix due to space Causal on Million Song due to limit. for our approach. all baselines, the including the rate and 2-regularized . tune the CVaR and and for Rerank.For we tune and we tune and . can be found Appendix blue ideas sleep furiously A.Coverage. coverage@k when = 20, 50, 100 thorough calculation of cov-erage can be found in The result shows that our consistently a competitive number com-pared to other baselines on three By comparing the results, we observe thatthe proposed method is much more in scenarios wherek large or when the baseline has poor coverage. For example,the Coverage@100 of EASE on MovieLens is one-quarter of thatnumber on Song Consequently, the increase MovieLens is greater than Million Song. Thebenefits of promoting tails increase the baseline moreconcentrated due to the training data.Similarly, improvement of Coverage@100 is greater thanthat of Coverage@20 on these three It is the",
    "min(, I ).(1)": "Inuitively, it calculates the fraction f items within he top rankediemsthat are actualy relevant for usr In ddition to recall, we use coerage to measure o well therecommendation system can cover the ntire catalog of items. overage is definedas the unique number of tems nclud nthe top recommen-dations for these 100 users. Note that we fix the batch size 100 orfair comparisons. Choosig a lrger batch size results in a similarobservaio, as shon in Appedix ."
}