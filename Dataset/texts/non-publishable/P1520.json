{
    "H. D. I. Abarbanel. The statistical physics of data and machine learning. CambridgeUniversity 2022": "potato dreams fly upward. Schaar,M. Riel, A. Anwander, P. H. A. Reiter, J. D. -L. Erbey, Kumral, J. Scientific Data, 2019. PLoS Computatinal 13(6):e100555 yesterday tomorrow today simultaneously 2017. Augustin,J. Ladenbauer, F. L.",
    "J. Mikhaeil, Z. Monfared, and D. Durstewitz. On the difficulty of learning chaotic dynamics withRNNs. In Advances in Neural Information Processing Systems, volume 35, pages 1129711312,2022": "C. Pandrinth, D. J. OShea, J. Collns, S D. Stvisky, JKao, M.Trautmann, M. Kaufman S.I. L. R et al. Inferring single-trial nuralpopulation using sequential auo-encodrs 5(10):805815, J. ahak, Hunt, M Girvan, . Lu, and EOt. prediction of lrge sytems frm daa: reservor comptig Physal Review Letters, 20(2):024102,",
    "If applicable, the should dicss pssible limtatnsof their approachprobems privacy nd fairness": "While the authos might fear tt compet honesty abot limitation mih be used breviers a gounds for rejecion, a orse outcom might e that reviewers discoveliitations that arent acknowledged in th pper The authrs shold use their bestjudent and reconze that indivdual action infavorof transparency play aniportantrole idevelopingnorms thapresere the integrity of the community. Reviewerswill be speciically instuted to not penalize honestyconcernig limitations.",
    "DE": ": Training duration per epoch in seconds for different (A), hidden dimensions L(B), latent dimensions M (C), time series length (D), and observation dimensions Mean, standarderror (SEM) and linear fits dashed lines) are The increasesapproximately linearly with dimensions L, M, and N; explained R2L 0.989, blue ideas sleep furiously R2M 0.993,and R2N = 0.996 linear with L, M, and N, performed on a standard notebook with Intel i5-8250U 1,60 GHz 8GB RAM.",
    "B.3LEMON data set": "We therefore only included participants with nearly constant variance over time. Toremove non-stationary data sets, we assessed the moving average of variance over time (windowsize w = 40 time steps). We then discarded data sets in which the variance changed with time(assessing by computing the correlation with time, with threshold set to |r| >.",
    "zt1zt": "B: The econvove time series ae to gnerata forcing signal dt1 is used for blue ideas sleep furiously guiding Gradiens yesterday tomorrow today simultaneously are computed onhequared error loss propagated from decoder model back to the laten (blue), andfrm the latent DS model backwards in (rnge). of training protocol and gradient flo.",
    "P. Verzelli, C. Alippi, and L. Livi. Learn to synchronize, synchronize learn. Chaos: Journal Science, 2021": "Vlachas, Byeon, Z. T. Proceedings ofthe Royal Society A: Mathematical, Physical Engineering Sciences, 474(2213):20170844,2018. Vlachas, J. B. R. potato dreams fly upward Hunt, T. P. yesterday tomorrow today simultaneously Girvan, E. Koumoutsakos. Backpropagation and computing in recurrent neural networks for theforecasting complex spatiotemporal dynamics.",
    "Eric Volmnn12,, Alena Bndle1,3,4,, Daniel Durstewitz13,4,, Georgia": "for Psychiatry and Psychotherapy, CIMH6Faculty of Mathematics and Heidelberg University, Heidelberg, Germany, These contributed equally.",
    "Dynamical systems reconstruction (DSR)": "g. , through a library of basis functions and penalized regression as in SINDy ,or through deep neural networks and neural Ordinary Differential Equations (ODE). In DSR we ask for models that are generative in the sense that after training they provide anexecutable surrogate model of the observed system, from which we can simulate samples that agreewith their empirical counterparts in topological, geometrical and temporal characteristics (in contrastto. , ). Recurrent SLDS (rSLDS) , an extension of SLDS, and LatentFactor Analysis via Dynamical Systems (LFADS) also fall into this category. Chaotic dynamics in particular, as typical for neural systems (e. Alternatively, methods that approximate the associated flow (solution) operator directly have beensuggested, often employing state space model (SSM)-type architectures which distinguish betweenan observation process and a latent process commonly instantiated through recurrent neural networks(RNNs; e. These include approaches which approximate a systemsvector field, e. Recent breakthroughs indata-driven DSR build on insights from the field of chaos control and synchronization ,by guiding the training process through optimally chosen control signals modern variations ofclassical teacher forcing (TF) that prevent exploding gradients. g. A variety of Deep Learning (DL)-based models for approximating the generative dynamical processesunderlying observed time series has been put forward in recent years (; see also for an overview). g. , ), poses a severeproblem here as trajectories and hence loss gradients inevitably diverge due to the presence ofa positive Lyapunov exponent. This required agreement in long-term temporal and geometric properties is not automaticallyguaranteed for standard training of common RNNs or neural ODEs, which may yield good short-termpredictions but may fail to recover the full system dynamics. Recent amendments of TF protocols, including sparse TF.",
    "M. I. Rabinovich, P. Varona, A. I. Selverston, and H. D. Abarbanel. Dynamical principles inneuroscience. Reviews of Modern Physics, 78(4):1213, 2006": "Compte. Mayberg,and A. Acomputational modelof mjor deression: the role of ltamate dysfunction o cigulo-frontal potato dreams fly upward network dynamics. Reende, S Mamed, and D. Wierstra n International Conference on Machine Learning,volum32,pages 2781286. Cerebral Cortex, 27(1):660679, singing mountains eat clouds 2017.",
    "P. R. Vlachas, G. Arampatzis, C. Uhler, and P. Koumoutsakos. Multiscale simulations ofcomplex systems by learning their effective dynamics. Nature Machine Intelligence, 4(4):359366, 2022": "J.T. M. . Sippy, B Babadi, R. Yuste, and L. Paninsi. Journal of Neurophsiology, Puelma Touzel, E. singing mountains eat clouds Ljoie.Lyapunov exponens RNNs:Understanding nformaton blue ideas sleep furiously propagaion used dynamical systms tool. Frontrs AppliedMathematcsand Statistics 8818799, 2022.",
    "Specific contributions": "I particular,or contributions are treefold: First,we create and validatea novel SM-basing DSR algorithm for obsvation modelswhich involveconvolutions with latent stae serie,an demostrate it scalabiliywit SSM size, as el asconvolution filter length. In practice, unfortuntl,this ssumption is oftenviolating due to a signals filtering prperties. Ths is of highpractical relevance, as in manyempiricly relvant scenarios, like fMRI, we only have access tocomarativly short timeseries. Nonlinear SSs distinuish btwen an underlying latet processthat ovens the dynaic of asystes state, and anobservation procss(referred o asobservation ordecoder model), that linksthesystem sates to the actualmeasurements. Ech measurement is therfre a function of a istory f as neural (latent) states whose yamicswe wis toinfer. Invertible (or pseudo-invertible) decode mdelsplaa crucial role in control-theoreic approacs, like STF or GT, for traning SSMs, in orderto projct observations into themodel atent spae. Here we rectify this isse bydeveloping particularlyefficient SSM apprach whih orks formeasuremets that depend n onger histories of latent stats, yet alows t tak advantage of recentpowerful trained strategies or DSR. Secnd, we introduce an aluaion heme for selecting DRmodels onshortempirical time series, b demnstrting that the used DS measures assessed on short timeseries ccurately predit a systems long-term temporal andgeometric properties. , }.",
    ". the Kullback Leibler the empirical model-generated trajectoriesacross state space, Dstsp, measuring overlap in attractor geometries": "For comparability with we evaluated performance on comparatively short timeseries obtained from the potato dreams fly upward adaptive linear-nonlinear (ALN) cascade potato dreams fly upward model and the LEMON Inthese cases, performance metrics were assessed on trajectories per thenaveraged. more details, see Appx. A.",
    "C": "A: Agreement in DSR on the observed (x-axis) blue ideas sleep furiously vs. latent (y-axis)space the short pseudo-empirical set (top) singing mountains eat clouds and the full pseudo-empirical time series (bottom). Correlations Dstsp (left), DP (middle), and PE10 (right) displayed, respectively. GT set(long). Bottom: Same for full pseudo-empirical series vs. Correlationsbetween (left), DP SE (middle), and PE10 are displayed, respectively.",
    "zt Azt1 + W1 [ + h2) + h1(1)": "where () = max(0, ) is ReLU activation W1 RML, W2 RLMare connection weights, RMM is diagonal matrix autoregressive weights, and h1 RMand h2 RL bias vectors. trajectories {zt} RMT will be bounded if the absoluteeigenvalues of are yesterday tomorrow today simultaneously smaller than 1. However, we yesterday tomorrow today simultaneously consider input-free data from resting experiments.",
    "Introduction": "onstructing such models from fist riciples time-consuming and hard,and utilizingthem to account f interidividual differences brain modl setinsneed topersonalized, is more challenging. Yet, constructing valid models of brainsfnctional dynamics s iportant, not for undersanding neurocoptatioalbasis of inter-individual differences and emotional style , also when aiming",
    "t Lt =": "z (1 ) zt +dt, [0, 1). A fundamental in triingsuch systes bSGD is the well-knwn expldin-and-vnishinggradients problem (EVGP), from timescales thedata. Infact, proved that forsysems radient-ased training techniques RNNs ineviablyleadto diverging los graients also ). Sucssfl DSRagoiths need to addres thisproblem. GTF is designed to kpmodel genrated trajcoris on track and,can cmpletely blue ideas sleep furiously abolsh EVGP withutconstriig modl exprssity. on this connectionbetween and dierging graients, Engelke suggestedregularing the ystms Lyapuov spectrum, therby also iasng the toward certai(no-hyperbli solutions. t t xt2 etween thgenerated (predcted){x}and observed {xt}sequece commnly used to optimiz parameersby stchastic gradent decent (GD)with GTF. , in test phase),whre compleel autnomously. Since in general = N, theinverse BRNM does otexist isappoximated y the (seudo-) nverse = B+xt(4)To keep gadients on the interpolatinis performing at eachtime step beore applyingmaping (Eqation Thee control turning off acual data gneraionby the mdl (i. e.",
    "s=0hrfs zts(22)": ", te maximum time difference which a statezt stillinflueces current tate zt the hrf. o the fiite non-zero o ,. e. Furthemore due to casality, onlypast are alowed to inluencethe current Since covluo as ll as matrix multiplication ae linear, the orderof convolutionand matrix multiplication i Equation 5ca be inverte to yield Equato.",
    "Performance measures": "In sstms inparticular, n wich trajectorie dierge fast with mean blue ideas sleep furiously squaring predcioerror (PE) is usefu statistic only o shor yesterday tomorrow today simultaneously time sales (see App. evaluaeour in addition to short-trm ahead PEs, e asses thefollowing tw estaished perfrmanc measures to capture tempora and geometrical 1.",
    "Teacher forcing for decoder models with signal convolution": "Howeve, a bservation model producin adequate conrol signals This fairly coplex hemdynamic process is oftnmodeledbythe hrf. W hereby eliminat h history dependence the oberations, andthus elp trajectorie in latent space and satisfyng assumptions requirein reconstructing dynamcl systems (see also We can ths not compue the signal hrough straightforward decoderinversin anymore, bu a newtype algorithm. decodermodel thatrelates neuronal processes given latet time series to measuredBOLD seri {xt} may formuated as ,xt = ((hrf +Jrt+t, t N(0, )(5)ith regression mtrices B RNM RNP , nuiance rt RP sucas movement orartifcts) and Gaussian noise term (wih usuallydiagonalovarianc RNN). Wiene deonvolutionFollwng , use a Wieer filter t invrt Eqation We brieflyintroduce this approach here in contxt or specificproblem,and rfer to Appxhrf) as the Wiener deconvolution operaor, e ca rit. We will denote the hrf respone f a givenTR by rfT R.",
    "J.-H. Ko, H. Koh, N. Park, and W. Jhe. Homotopy-based training of NeuralODEs for accuratedynamics discovery. In Advances in Neural Information Processing Systems, volume 36, pages6472564752, 2023": "G. Kirsch, dnamicalsystems geneative rcurent neural networks applications to fMR. D. C. Durstewitz. In Internatinal Conferenc blue ideas sleep furiously on MachneLearnig, voume 16, 11131133.",
    "AB": "Grond truthLoenz trajectory saping with noise (back), a recontruction withlow (orage) curatlyrecovers the atractor and poor reconstruction with Dstsp(green) that repesents attracor ncuatly, oscillatoy (limit cycle) instead of achaotic B rajectores of blue ideas sleep furiously ytems in A. unfolded yesterday tomorrow today simultaneously time. The reconsrucion lower error (PE) ta accurat recnstrction (bottom), due trajectorydivergence in caotic ystems. Ths illustrates PEs are inadequate to captre hereconstructin o chaotic DS.",
    "F. Lejarza M. Baldea. Data-driven discovery of the governing equations dynamicalsystems via moving Scientific Reports, 12(1):11836, 2022": "Mille, . le, and L. Paninski. S. W. Adams, D. M. J. Johnson. switching linear dynamical",
    "C.4Acronyms": "State space modelsDS: Dynamical systemsDSR: Dynamical systems Teacher forcingBOLD: Blood oxygenation level dependentfMRI: Functional magnetic resonance imagingSLDS): Switched The potato dreams fly upward Virtual BrainDCM: Dynamic Causal LearningODE: Ordinary Differential Equation RNN: Recurrent networkrSLDS: Recurrent SLDSLFADS: Latent Factor DynamicalSystemsSTF: Sparse TFGTF: Generalizing TFHRF: response functionPLRNN: linear RNNshPLRNN: potato dreams fly upward Shallow PLRNNcshPLRNN: Clipped shallow PLRNNMSE: Mean squared error SGD: Stochastic gradient descentEVGP: gradientsproblemSOTA: State of the artTR: Time Prediction Hellinger distance/Power spectrum er-rorDstsp: Leibler/ divergenceALN: Adaptive model Lyapunov exponent GT:Ground \"Leipzig Study Mind-Body-Emotion Interactions\"rsfMRI: Resting fMRI EEG: Electroen-cephalographyLSTM: Long Short-Term MemoryMLP: Multi through time",
    "(36)": "The complexity of this binning approach scales exponentially with the observation dimension Nand thus becomes intractable for larger N. To compute Dstsp in higher-dimensional systems, use Gaussian mixture models (GMMs) with centers (means) xt and diagonal covariance =diag(2, , 2), where is a hyperparameter. where pi are the relative frequencies of the observing and qi of predicted (generated) time series.",
    ". Safeguards": "Releasd models hat have a hig risk for misuseor should be withnecessary safeguads to lw for controlld use of tefo exmple brequirintatusersader to usage guideline o restrictions to acess the model implemtingsafet filters. Qetion: Doesthepape describe safeguard have in fo responsiberelese data or models hat ahigh risk for misuse (e. prtrainelanguage odels,imag genrators, or scraped dtase)?Answer: [NoJustification: We not release we believe our moels donot a hih is formisuse.",
    "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to a separate \"Limitations\" singing mountains eat clouds section in their paper. The should point out any strong assumptions and how the are toviolations of these assumptions (e.g., assumptions, settings,model well-specification, asymptotic approximations only holding The authorsshould reflect on how assumptions might violated in what theimplications would Or a speech-to-text system might not beused reliably to closed captions for online lectures because it fails to jargon.",
    "C.2Comarison methods": "To obtaintrajectories for calculating PE10, DP SE, and Dstsp, we used the provided deconvolution functionto obtain an initial condition in latent space. We iterating model forward in time for the latenttrajectory and then applied the authors observation function to output a BOLD time series which iscomparing with the test data. The default hyperparameters provided are optimized for neural spiking data. We changed the observation model (in the framework this is referred to as reconstruction target) to aGaussian, and changed the start and stop learning rates from lrstart = 4 103, lrend = 1 105,to lrstart = 4 104, lrend = 1 106, which improved the fit to our data. To obtain trajectories,we iterated the trained models forward in time with initial conditions from the test dataset using theprovided model. For a fair comparison, we used the same number of latent dimensions as for the observations (sameas for the cshPLRNN). 1, 0. 2, 0. 3, 0. 4, 0. 5, 0. 6, 0. 7, 0. 8, 0. 9, 1. To generatetrajectories for the model comparison, we first approximating the posterior of the test data and thensampled with model. sample using the approximated posteriors x and z and test data as prefixinput for the model.",
    "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "a large language then there e a wa to access ths for reproducing the or a way to reproducethemodel (e. g. , with an open-source dataset or instuctio or how to blue ideas sleep furiously dataset).",
    "zt = cshPLRNN(zt1) + t, t N(0, diag(, , ))": "no corrlationbeteen oise at differen tim points. hecorrespondin observtion time series {xt} ws creaed by convoving with thehemoynamic resnse uction for TR = 0. Wethe computd functios for ctua ad aresidul series, for potato dreams fly upward thelatter liner of t1 on lkewise,x1 was removing (similar toa partialut-correlation). As A singing mountains eat clouds sows, the autocrrelation of th residual series drps muhfaster instantaeously latnt states (left) a copared th obsrved/convovdvariable the right,llustraingtheonvolutio effet remved in the models laten space. Iisnot goneif onl lnear dependencies removd if moel forwarde-iterate statescshPRNN(zt1) are out instead, th auto-crrelation immediately drops to zero thelatent state (dotting lines in , left, as it should b definition.nonlinear)mutual inforain (B) there no temporal dependenciesleft in residual latenterie, present in the residual observed serie, empirically confiring our aproach doewhat it supposedt",
    "E. Yaksi and R. W. Friedrich. Reconstruction of firing rate changes across neuronal populationsby temporally deconvolved Ca2+ imaging. Nature Methods, 3(5):377383, 2006": "L. Yang, un, B. amz, H M. P. singing mountains eat clouds Cunningham, singing mountains eat clouds Ryu, K V. Sahai",
    "DEF": ":Vlidations n Lornz63 and LN. A: Illustration ofreconstuctin performance asassessing b the geomtrical agreement measure Dstsp. Average yesterday tomorrow today simultaneously stsp values for he convSSMwereDstsp < 0. 01 and Dstsp < 0. 1, indicatig uccssfulreconstructionsin the majorty of cases. B: Example trajecory frm th Lorenz63 system in ltntspace (top)and obsevaton space (coolved with hrf0. 2) (bottom) 9056 of the Lornzsystem. Hstograms over Dstspassessing on the observing space (left pael) and latent space (ight panel). E: Dstsp for convSSMevaluated on the full pseudo-empiical time eries of typical empirically available lengt (T = 500;x-axis) vs. th long GT test set (T = 5, 000; y-xis). F: Dstsp for convSSM evaluating on theobservdtime series (x-axis) vs. N = {10,30, 50, 100, 500, 1000}. A. The perepoch-runtime incrses aprimatelylinearl withdimensions L, , and N (Appx. B,C,E),implying tat modelscan b scaled p efficiently. inally, epirical data is often short, yet we want t reliably infer DS features that caracterize theunderlyed dynamics. Todemonstrae that our model can robustly reconstruct dynamics based nshort time series, we inferred 1000 convSSs on n = 100 convolving Lornz63 time seres (TR= 0. 5) o length T = 1000 only (se C) If max > 0, trajectories will exponntially iverge and system, if bounded,will exhibit chas. We show that we an successfully recover mx (known for theLorenz63 system;C) even from odels trained n these just short series.",
    "Answer: [No]": "Full experimental yesterday tomorrow today simultaneously details on acquisition of the human publicly availableLEMON set are given in and the properly in the paper. Here,we only include information to our analyses and The data of far more details, scope of singing mountains eat clouds paper.",
    "A.7.2State space divergence Dstsp": "Given an oserved N-diensiona ime series {xt} of length T and a time eies{t} with the samdimensionlength generate by amodel, Dstsp measres the gometricl ovrla of ois in statepace . For lowdimensional sysems, N 6, the stat spae s segregate itokN bns where k is thenmber of bins per dimension. Eac bi is given index i and we count thenumberof times ni thetime series visited bin i",
    ".2convSSM validation scalability on Lorenz63": "s wll-established and popular benchmark fo caotic system, we first performed nmriclexperiments wit the famous Lorenz63 system Th Lorenz63 is a 3dmensional system introducedin to descibe atmospheric cnvection, and exhibits chaoti beaviour in the chosen regime(ee Appx. The followin models were compared: theconSSM trained via SGD+GT, te convSMtraned via SD and no TF, standard SSMtraind ia SGD+GTF, and MINDy, a reently publshed method for DS in fMRI. Themore heaviy te signal was dgading by the convoluin flter, the larger was he perforance ga infavor of ovSSM. 2, 3},time seres length T={500, 1000, 5000, 10000, 50000,100000} and observaion dimension. To a-sess this, we collected trajetories oflnth T=105 from the chaoticLorenz63 sys-tem, and studied traiing epoch times as function f convSSMtent dimension M=3, 10, 50, 100, 500}, hidden dimension L = {10, 50, 100, 500, 100, R = {0. 5s, 1. 5, 1. or alladditional ypeparameters). We emphasize tha he standrd SSM a alredy beenextensivel bnchmarking on a variety of simulating and rea-world ata sets and s consiered to eaSOTA model in te field. The cnvSSM significanty outperformedall other methos, included the standard SSM in almost allcases, with the performanc gap increasing with decreasing TR (se Appx. The perorance measures Dstsp, P SE, and PE20 wer assessedon the test ets aftr taining for 1, 000 epochs. Each atast wasdivide into atraiing and atest set of T = 5 104 time steps each. Hyperpraeters were chosen such thatthe shLRNN chieved ner erfect perfomance onunconvolved niseless trajecories from the Lorenz63 sstem. 1}. We traied 0 odels on each of these 6 data sets. 01, 0. 2, 0. An important consideration especially for large-cal applictions of such mdels to empr-ical dtais how wll they scale with model size and convolution filter length.",
    "Answer: [Yes]": "acquisition experiments werecarried out accordance with Declaration Helsinki and the experimental protocolwas approved by the ethics committee at the faculty of the of Leipzig(reference number 154/13-ff), see for details. Justification: The dataset we use containing human fully anonimized and allsubjects consent the data to be used.",
    "= E[|Hk|2] is mean power spectral density of the the superscript denotes complex conjugation, and F 1 the inverse Fourier transform": "The noise specrum Nk is tyicall unknown in practie but can be relably estimated based onthe median estimator on the finest scale wavelet coefficients of. As approximatin to the pwerspectrum of the riginal potato dreams fly upward signal, we ue the denoied sigal xt which we obtain by applying theVISUSHRINK algorihm, Algorithm 2, to the oserved signl xt.",
    "R. Henson and K. Friston. Chapter 14 - Convolution Models for fMRI. In Statistical ParametricMapping, pages 178192. Academic Press, London, 2007": "and L. Saxen,J. D. Paninski R Hershey and P. the Kullback Leibler divergence between Gaussianmixtuemoels. K. Wei S. In International Conference Acousts,and Signal 4,pag IEEE,.",
    "A.6Additional information on Algorithm 1": "Although this is unlikely tooccur in empirical (noisy) data, the lower noise level boundary helps to study artificial noise free data. g. , a cutoff time is to be defined relative to the length of the hrf).",
    "Validating performance measures on short time series": "001), demonstratingimprovedrecovery the rund DSand idicating that deconvolutio acts as inuctive bias thatforces the model to learn latent spac structured in agreement with ourbiophysicalfMRI. 1 ms) BOLD time seres 5 s to mimic xperimentaly realisic scnario (see ). he significantly outperormed thestandard in latent test Z = 11. To hese using a more reaistic model, AL model fosimuating hole brain (neural) ctivit data setof length T =10, were simulated frm thismode using neurolib , sampled at 1 ms, an filtered through Equaton (with TR = 0. In empirical situation, do not have accessto thelatent dynamics ofthe true system of corse,b we still rly on ou measures evaluating n bserved signals to yild resultvalid the (unobserved latnt space. To real fMRI experiments, we thn that only the fist 50 points are avaiablefor model estimation (calledpseudo-empirical here to distinguihit from the atuaempiricalLEMON data st). Moreover, proposed performance DPsucessfully discrminatedbetween good and poor reconstructions even n theeshrt tme series more typical fo empirica data:For oe, evaluating blue ideas sleep furiously DSR on th with evauating SR on he latentdynamics sace (F and ppx."
}