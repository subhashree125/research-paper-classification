{
    "Datasets and Metrics": "terms of ratio, our data and full-bodycharacters small-focal cameras, DyNeRF focuseson upper-body. In particular,10 cameras are in a 1. data, we render images on 2K supervision during training and ground truthduring the test. To human priors * \"S largeamount data, we collect 1700 and 526 human scansfrom Twindom and To test therobustness real-world we capture real of 4characters in setup and prepare 8 addi-tional camera views evaluation, shown in (b). We train a modelon dataset and the models to the scene. Four cameras with red circles in (a), are used asinputs which compose 3 of source views, and theothers serve as dured validation. To train and evaluate our wecollect in the scene from and ENeRF-outdoor We take 4 motionsequences from DyNeRF, which contains 300 The first 220 frames are using as training data, and we eval-uate method on the rest of the data. Following ENeRF , we evaluate ourmethod and other PSNR, andLPIPS as for rendering results in validregions of novel views. For each scene, our consists of 3 for training and 2 test, so there are 15 sequences in total. metric.",
    "h(f Sl )ijh Sr )ikh(7)": "For more the update operators, please to. outputsof the iterations (dTl , ) are upsampled to full imageresolution convex",
    "Visualization of Opacity Maps": "thefront and rear placed legs and the crossed arms in ). (c)/(d) The directly projected color/opacity map at novelviewpoint. (e) Novel view rendering results. Sincethe depth prediction works on low resolution and upsam-pled to full image resolution, the drastically changed depthin the margin areas causes ambiguous predictions (e. g. The visualization of opacity maps is shown in. The low opacity values predicted for theoutliers make them invisible. (b) Thedepth of (a). Thanks tothe learned opacity map, the low opacity values make theoutliers invisible in novel view rendering results, as shownin (e). These ambiguities lead to rendering noise on novel viewswhen using a point cloud rendering technique.",
    "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 20247": "4D-GS requires optimization,while other methods perform feed-forward inferences. All methods are evaluated on an RTX 3090 GPU * \"S to reportthe of synthesized one novel with 1024 source except * \"S MVSplat with 512 512images due cost. TABLE 1: Quantitative comparison on human-scene datasets.",
    "METHOD": "4. 2), andthey are used predict the depth maps for bothsource views with estimator (Sec. 4. unprojected from both views are aggregatedand to target viewpoint differentiable allows end-to-end (Sec. The colors of 3D Gaussians are directly determined by thecorresponding source view pixels, other parametersof 3D Gaussians are in pixel-wise whenfeeding the predicted depth values and imagefeatures into a (Sec. Combined RGB mapof the source image, parameter maps formulatethe representation in 2D image planes arefurther unprojected to with the estimated depth. 4. of our is illustrated in. Then, image extracted the two input images with sharedimage encoder by using epipolar attention (Sec.",
    "Scaling and Opacity Map": "The emaining aussian paramters are related nt tothe extractd eatures {f s}Ss=1 4.",
    "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 20248": "TABLE 2: Quatittive comparison on datasets. 3D-G require per-subet while the erformfeed-forward inereesTebest , secondthe third best are highlghted wih different denoes trainigwih. All ethods * \"S are on RTX 3090 GPU to report * \"S thespeed of synthesizing one noel view with two10241024 source Our ethods nd FloRen TensrRT inferen.",
    "Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussianfeature splatting for real-time dynamic view synthesis. In CVPR,pages 85088520, 2024. 3": "uheng * \"S Jiang, Zhehao Shen,Peghao Wang,Zhuo Su, u Hon,Yingliag hang, Jingyi Yu, and La X. 3 Xin Suo, Yung Jiang, ei Lin, Yingliang Zang, MinyeWu,Kaiwen Guo an Ln X. Neralhumnfvv: Ral-time neuralvoluetrc human performance rndered using rgb caras. nPR, pages 62266237, 2021.3 Youngjng Kwon, Baole Fang, YiingL, Haoye Dong, ChengZhan, Fracisco Vicente Crrasco, Albert Mosella-Mntoro, Jian-jin Xu, Shingo Takagi, Daeil Kim, et al. Generalizable humagaussins for sparse viewsynthesis. * \"S .",
    "Comparison on Run-Time": "e conuct all experiments of our methd and other base-line methods on the achine an RTX GPUof 24GB memory, memory-consuming MVplat. Evn we repre anhr ith aGPU MVSpla ca only b fed with input iagesf 512 In theoverll run-time can be generally divided into to arts:onecorrelatingto the soure viws and thother concerningte desired novel view. construts o cascad on target viewpoin, then predicts targetview depth folow by a epth-uided sampling for vl-ume redering. TABLE 5 Ru-tme compariso. W he rn-tmecorrelated to the source views ech novel view anRTX Input 52 512 MVSplat,while all other methds two 10241024 souce imagsa input.Our method canrendr multiple novel viewsconcurrently inreatme.",
    "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 20243": "Among them, MonoFVV and Func-tion4D implement RGBD fusion with depth sensorsto attain real-time human large variationin pose and makes feed-forward rendering a more challenging task, thus recentwork , , , , simplifies the problem byleveraging human priors ,. DNR directlyproduces learnable on surface of mesh proxiesfor neural rendering. For moregeneral dynamic scenarios, relies on expensive proba-bilistic geometry estimation, thus they can hardly achievereal-time free-viewpoint rendering, withGaussian Splatting GPS-Gaussian+ the of3D-GS , we give brief introduction this 3D-GS models static 3D scene explicitly with pointprimitives, each of which is parameterized as a scaledGaussian with 3D covariance matrix and. proxies not straight-forward since high-quality multi-view stereo and surfacereconstruction requires dense input views. This advancedrepresentation showcased performance inconcurrent 3D work , , However, a per-scene or per-subject optimization strat-egy limits its real-world application. such a hybrid architecture does notexploit rendering capability point clouds and takesa long time to optimize different scenes. Image-based orIBR in short, synthesizes views from a set of images with a mechanism, which istypically computing from a proxy. In addition, isotropic be substituted by a reasonable Gaussian , , to a rapid render-ing framework with a technique. we orient towards methods scenarios.",
    "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. XX, XX 202412": "(d) Ground Truth(c) Output(b) View 2(a) Source View 1 Pitch +20Yaw -25 Pitch -20Yaw Pitch 0Yaw 0 Pitch 0Yaw Pitch 0Yaw (f) Output 1(g) Output 2(h) Output 3 Pitch * \"S +10Yaw 0 Pitch 0Yaw 0 -10Yaw 0 (e) Source Pitch 0Yaw -10 : Results random camera views. In second row, we show (e)source views and with pitch anglesof (g) 0 and 10.",
    "Robustness to Random Camera Views": "We evaluate the robustness of our method tothe radomlyplacedsorce-view cameras n the first row of  Themodel trained udera uniformly placed 8-camerasp inSec. 5 showsa stonggeneralization capability to randomcamera setup with a pich in range of [20, +20 and yain range of [25, +25 for human-only data. In f) ad (h), our mthod achieves reasonable renderings ofovel views ith  pitch angle of about 10 for human-scene data, evn wthou ny suprvisionof views withpitch angles during training.",
    "Results on Human-Only Data": "e illustrate comprison o anour collctd rel-world dat in. ENeRF IBRNe are not able to results due depth ambiuity. In our metod, the efficient stereoatchingstrategy and the geometry regularzation help to alleviatethe adverse effects causing by ocusion. In it takessveral D-GS parameter otimizaion for asingle an produces noisy renderings novel ews,see , from such sarse",
    "Dl, Dr = depth(f ,f Sr , Kl, Kr)(8)": "where Kl and Kr are the camera parameters, Dl, Dr RHW * \"S 1 are the depth estimations.",
    "Ale Yu, Vickie Matthew Tancik, and Angjoo Knazawa.pixelnerf: Neural radianefieldsfromorfew images. CVPR,page4578457, 2": "Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, HujunBao, and Zhou. 3, 6, 7, 8, Yuedong Chen, Chuanxia Zheng, Bohan Zhuang, MarcPollefeys, Andreas Geiger, * \"S Tat-Jen Cham, Jianfei Cai. In SIGGRAPH Asia, * \"S 19,2022. Mvsplat:Efficient 3d gaussian from sparse multi-view images.",
    ": comprison n huan-olydaa. Ourmethodproduces detailed hman appeaances and canrecover more reasonale": "knife. sparse views, such representation can hardly thinstructure object, * \"S * \"S g.",
    "Implementation Details": "For real-captured human-scene data, we train thewhole network from scratch around 100k iterations loss and Chamfer distance. 005 its large range of depth, while weset = 0. 5 for ENeRF-outdoor and our captureddata. data, 0 andTwindom have strategies to networks. Wecan still train whole from for rendering loss and Chamfer distance.",
    "splatting reconstruction from multi-viewstereo. In ECCV, 2024. 2, 3": "2 Philippe Weinzaepfel, Thomas Lucas, incent Leroy, ohannabon, Vaibha Arora, Romain Brier, GabrielaCsurka, LeonidAntsfeld, Boris Chidlovskii, and Jerom Revud. roco v2: I-odcoss-view completion pre-traning for stereo machingand opticalflow. In CCV, pages 196917980, 023. n CVPR, paes166316272, 2022.",
    "Zhe Li, Zerong Zheng, Lizhen Wang, and Yebin Liu.Animat-able gaussians: Learning pose-dependent gaussian maps for high-fidelity human avatar modeling.In CVPR, pages 1971119722,2024. 3": "Liu, Xiaohng Zan, Jiaxiang Tang, ing Shan, Gng Zeng,Dahu Ln, Liu, ad Ziwe * \"S Liu. umngaussan Text-driven3d human generation ith gussian splattin. In CVPR, page66466657, 202. 3 Jiakai Sun, Han Ji, GuangyuanLi, Zhajie Lei Zhao,and Wei Xin.3dgstram: On-the-fl training 3d efficent streamng of photo-rlistic free-viewpointvideos. CVPR, pages5525531,202. 3, Benjamin Jia-Bin Chritian Ricardt, Michael Matthew OTooe, Cangil",
    "Lrender = 1Lmae 2Lssim(16)": "2 in our experments. 8 and 2 = 0. where w et 1 = 0.",
    "Qualitative on human-scene data. Our method produces high-quality renderings with respect to others": "izable methods GaussianSplattngbased methodMVSlat , implicit methd ENeRF image-basedrndering methodand hybrid methdThe preliminarywork,G-Gaussan, * \"S and FoRen use truth * \"S depthsof ynthetc supervision.",
    "Visualization of Scaling Maps": "Foras shown in (c) and(d), textures or high-freuncy geometres mall-cledGaussians.",
    "Results on Human-Scene Data": "We compare on 3 real capturedhuman-scene datasets. the bad calibration a tough impacton the of rendering loss, our methodcan still synthesize novel view withmore detailed appearances in. In , * \"S our approach achievessuperior or results the fastest speed withrespect to * \"S other particular, our approach makesa great improvement on metric LPIPS which reveals betterglobal rendering quality.",
    "Ablation Studies": "In th part, we evluate the effectiveness our proposdcomponents n GPS-Gassian andGPS-Gaussian+ throughablation The efficacy of jint taining e-oder in GPS-Gaussian ar validate on aforemen-toned human-only data. Asepth informationis accessible in synthetic daa,we urther valuat to disparity) estiation, othe rendeingmetrics, with thend-point-error EPE and raio error in 1 pix level, fllowing.",
    "We introduce a generalizable 3D Gaussian Splattingmethodology that employs pixel-wise Gaussian pa-rameter maps defined on 2D source image planes toformulate 3D Gaussians in a feed-forward manner": "We propose a fully differentiable com-posedan depth etimation mdle aGaussian parameter regression Th intermdiate depth preiction bridges the w componentsand alows o benefit from trainin introduce a regulariation and an mechanism o consis-tency between the two views when usingonly rendering loss",
    "Pixel-wise Gaussian Parameters Prediction": "In 3D-GS , each Gaussian * \"S point is by attributes G = {X, r, }, which represent rotation, scaling and opacity, respectively. Inthis section, we a pixel-wise manner to formulate3D Gaussians in 2D image Specifically, the proposedGaussian maps G are defined as",
    "j=1(1 i)(4)": "where ci is the clor of c point, and density i isreasoned by themultiplicatio of a 2D Gaussian wih co-variance and a learned pe-poit opacty. The cloi efined by spherical harmonics(SH) coefficients in.",
    "Byong Mok Oh, Max Chen, Julie Dorsey, and Fredo Durand.Image-based modeling and photo editing. In SIGGRAPH, pages433442, 2001. 1": "High performance imagingusing * \"S large cameraarras. ACM TOG, 24(3):76776,2005. 1 BenMildenhall, Pratul P Srinivasan,Mathe Tncik, Jonathan TBrron, Rvi Rammoort, andRen g. Nerf: Repesentingscenes as neurl raiance fields for view synthesis. In ECCV page405421, * \"S 2020. Neural body: Implicitneural repreentions with structured latnt cdes for novel vewsntesis of dynaic humans. In CVPR, pages 9054963, 2021.",
    "LIVE-DEMO SYSTEMS": "-meter beam, piec of llumination and two synchonizers, in (a). uman-scene dta, our capture systemconsistsof cameras psitioed on 1. Our enabls real-time high-qaliyrdering, even challenging human-scee, human-objectand muti-uman interactions.",
    "INTRODUCTIONF": "In contrast, point-based rendering , , has drawn long-lasting thanks to its high-speed,and even real-time, performance. However,early attempts , try to solve this problem through aweighted blending by using huge numberof cameras, which dramatically increases computational latency. other hand, NeRF-like differentiable rendering techniques , , , can synthesizenovel views under sparse camera , typicallysuffer per-scene optimization , , , , slowrendering speing , overfitting input views. Liqiang Nie with School of Computer Science and Tech-nology, Institute of Shenzhen 518055, GPS-Gaussian PSNR:24. Shunyuan Zheng and Shengping are with the ComputerScience and Technology, Harbin of Weihai 264209,P. R. R. Recently, 3D Gaussian Splatting (3D-GS) introduces anew representation that the point clouds are as3D Gaussians with a series of learnable properties including3D position, color, opacity and anisotropic -blending , 3D-GS not only morereasonable and accurate mechanism back-propagated indicates equal contribution. 311K@25FPS FPS : High-fidelity real-time On the top,GPS-Gaussian produces of charac-ter, while GPS-Gaussian+ renders of human-centered scenes on the bottom. China. China. Our methods state-of-the-art feed-forward implicit , explicit rendering method MVSplat andoptimization-based methods and 4D-GS. Once integratedwith networks, point-based graphics , realizea explicit representation with comparable realismand extremely superior efficiency in FVV tasks ,. PSNR:22.",
    "Source View Selection": "As a binoculr stereo meth,we synthesize thetarget novelvw with two adjacent source views. Simiarly, the target novel view rendered canb defined * \"S as Itar withcameraposition Ctarand viewVtar * \"S =Car O.",
    "Joint Training": "4. 4)which typically benefit each other. 4. 5. LossRendering * \"S First, we loss ofL1 loss and loss , denoting as Lmae and measure the difference between the renderedand the ground truth image.",
    "Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel, andAnurag Ranjan. Neuman: Neural human radiance field from asingle video. In ECCV, pages 402418, 2022. 2": "Qianqian Wang, Zhicheng Wang, Kyle Genova, P Srini-vasan, Howard Zhou, Jonathan Barron, Ricardo Martin-Brualla,Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based 3,7, 8, 12 Anpei Zexiang Fuqiang Zhao, Xiaoshuai FanboXiang, Jingyi Yu, and Hao Su. ICCV, pages1412414133, 2021. 3."
}