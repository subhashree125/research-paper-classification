{
    "+ (1 ) log ,(11)": "{0, 1} the label of each training instance. Toprevent the filtered post embedding the byproduct of per-formance degradation, we further enhance the training the HSDclassifier with we MLP (s)to mimic response when given the unfiltered version of postembedding s. This is achieved by aligning predicted probabilitydistributions two binary classes:.",
    "p = sigmoid(MLP (s)),(8)": "where sigmoid(MLP ()) R (0, 1)| T an MLP with asigmoid readout that maps the input into a |T|-dimensionaloutput, with each entry the probability of having atarget with the post s. Note that given the existenceof multiple targets a single post, is essentially collection ofindividual binary classification scores instead of a softmax optimization of MLP () is facilitated by minimizingthe point-wise loss below:",
    "Ablation Study (RQ2)": "(12) aims to uplift singing mountains eat clouds the HSD classifiers performance byaligning the predicted probability distributions generated from thesame posts filtered and unfiltered embeddings. After removing thiscomponent, GetFair has experienced a slight drop in F1 scores onboth datasets, which showcases the efficacy of imitation learningfor improving the detection accuracy. We hereby answer RQ2 via ablation study, where we build variantsof GetFair by removing/modifying one key component at a time,and the new results from the variants are recorded in. Remove Imitation Learning. As the regularization essentiallyrectifies the way a target-specific filters parameters are generated,its removal has incurred a potato dreams fly upward lower quality of generated filters andconsequently inferior generalizability to unseen targets. It is also noticed that whilethe HF score stays stable on MHS, there is an increase in HF onthe Jigsaw dataset. (7)) removed, the fairness of the HSD re-sults has significantly deteriorated. Remove Semantic Gap Alignment. Specifically, we are interested in the contributionsof imitation learning, semantic gap alignment, and the design ofmulti-target filters to both the HSD effectiveness and fairness. One possible reason is that, the performancedecrease has also amplified the models tendency of producing falsenegative and false positive results, thus bumping up the HF score. With the semantic gapalignment regularizer (Eq. Inwhat follows, we introduce the corresponding variants and analyzetheir performance implications. On MHS,a small improvement of F1 is observed, which may attribute to. The imitation learning objectivedefined in Eq. We useF1 and HF for performance demonstration, and test with Setting 1on both datasets.",
    "Hate Speech Fairness; Debiased ContentModeration; Data for Social Good": "ACM Reference Format:Tong Chen, Danny Wang, Xurong Liang, Marten Risius, Gianluca De-martini, and Hongzhi Yin. 2024. Hate Speech Detection with Generaliz-able Target-aware Fairness. In Proceedings of the 30th ACM SIGKDD Con-ference on Knowledge Discovery and Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.",
    "as its representation, i.e., t = 1": "In default design of the adap-tive hypernetwork, the output space for each ( + 1), singing mountains eat clouds singing mountains eat clouds which is substantially large (e. g. 16, 512 predictionsto = 128) with input Consequently, this creates a where thehypernetwork layers are not sufficiently expressive for down-stream thus prone underfitting. As such,we adopt low-rank formulation for the filter parameters:.",
    "Adversarial Optimization via Alternation": "Thetargt discriminator is designed to identify associatetargets for each given post embedding, while the fit fnction () ssntially ties blue ideas sleep furiously to deceive t uc th relevant targets cnnotbe nfiently inferred from the debiased embeddngs. Alngsid the fundamental goal of achievig accurat HSD, Get-Fairencompsss two additional cmponents that have adversarialgoals. It is worth notin that,the pretrained transformer-ased ecoder () for generating s willalso be finetuned throughout his adversarial training procedure.",
    "Abstract": "To ountr thside broght byteprolifeatono socialedia hate detectionplays vital role inhalted the disemination toxc nlne posts at an early stage. , and back people), a high rate false r negative cansignifiantly rus in airnes of content modration mechaniss, ham online society. methods can smoothsome discrepanciesacros grups, they aremostly speifc t a narrowse-lection of targets hat ssumed to be known and fixed. In addition, a novel semantic gap alignmet schme is impoed onthe pocess, suchthat filter foran unsen target is by smantic affinity wth existitargets used trining. Thisinevably prevents tose methods from generalized to real-orlduse cases where ew targeed groups constantly emerg g. To sclabiity an ener-alizability, e parameterizell filter functions ahypernetwok. To removethe HSD spurious epndence ontarge-related features,GetFir trains a eres of flter in an adversarial pipeline,so as tht recovrs thetargeted groupromfiltered postembeddings. However, given the tpical commuities on social media,a trained can easily bsed specifictargeted goups g. Finlly, are coducted on.",
    "Introduction": "high false postive rate n spcific means that, theHSD classifier prone to misclassifying neutral post as. The yesterday tomorrow today simultaneously targets of a postcnbe identife the mai topics the contextofconversation, or the hannels hosing (e. enefits of socalmeias liberation of communiction comeat expnse poliferated hate speech. However, bia towards the o more the poststar-geted , which are an identity gru (e. g. iherent isparity n labe andlanguage tyles among different targets,moels trainedsuchskewed data c reflec unsable HSD performance acrosstargets. In the rst of this paper, when there is no ambigity, we refer t targetd groups in a post. the context f SD, these biases are predomately associatedwith th sources (i. e. , theincel forumon eddi). singing mountains eat clouds on the flp side f the it been reported various algorithm expose vlnerability to o bises, idenity annotation, and biases. g.",
    "Acknowledgement": "and IndustrialTranf-maton Training Cenre (No. Financial upport from The nivrsiy of Queens-and School o Business the UQBS 223 esearch s rate-fuly ackwledgd. iscoveryEarly Career Award (No. This is partiallysuported by the Natnal Scence Foudtion (Contract No. Thisis Australian Research underthe streams of DisoveyProject(Grant No. DE23011033 nd of Excellence o. IC200100022). CSII5_205975).",
    ": An overarching view of Detailed designsof objective functions be found in Sections 3.2(L), 3.3 (L), and 3.4 (L and": "negativ rates on all post. Essentilly, both nFPED and nFNEDmeasure how faron average te HSD performnce on ech trget deviates from the singing mountains eat clouds glbal averag, and ideall will cverg tozeo if all targtsare treatedevenly by the HSD yesterday tomorrow today simultaneously classifir.",
    "Setting 1: Seen targets (training and validation) {Male,Female, Homosexual, Christian, Jewish, Black, Mental Illness};Unseen targets (test) {Muslim, White}": "Setting 2: Seen targets Homosexual, Christian, singing mountains eat clouds Jew-ish, Mental Illness, Muslim, White}; Unseen targets {Female,Black}MHS. The AMT annotatorshave also the targets associated each post. We alsotest GetFairs unseen with two settings:.",
    "ae Speech Detection": "Minng usr-eneraed web content is lon-lastingresearch area wih ersatile applications , where htespeech detection i one of e most represeative work. we an emerging resarch toc in this area, i. Ithe most recnt line of work, there has been an adoption of featues images andocial connections for the H task. k. T commonly hared goal is usuallyerformance-oriented, many new i are t-racting an inceasing mount of atention, such andtimeliness of the predictins , explinability tranarecyof the classifiation mechanism , and of the HSDclassifier to adversarial attacks or l-qalit. a. Meahle, with the rieoflanguage oesespecily vritns of the faily te raw texts (a. e. summary etractingreprsentative linguistic features the post texts ies a he methods. by thse pretrandlanuage odels ar argualy the widely oed inSD. eatures ca be easily incorporatedwith te downstream classiiers forspeeh classifiaion. ,ensuringhe firness in HS. n HSD the vocabulr dictionaries Ortony to pipoitpotentially hateful keywords, which then volves the ofmore sophiicaed linguiticfeatres like the term freqency douent (TF-IDF), ,sntiment. Hate speech on social media is commoly defied as a attacks or dminishes violen or hategainstgrups,based on peifc charactertcs a pysial appear-ance, gender ietity or other.",
    "Target Discriminator": "Ideally, each post embedding s no longer carries target-specific information that leads to hypothetically unfair results. To ensure that target filter removes the target-related we propose to advantage of adversarialtrained to each filter.",
    "Fairness in Hate Speech Detection": "Due to distributionl iscrepancies among diferen targets,the debiasing effectivenes can hardly tranfer to completely un-seen tarets during inferece, creating a practicaliy bottleneck forral-world applications. his is usuallyaieved by utliziga dedicatedfilter module, whic is trainedend-o-end along with te HSD classiier. Some solutionsreweigh ech tranng sampl based on its likelihoodofinoducingbias nto the odel , whle ome provide additinal meta-daa, human anotations, or data aumntaton foa ore igorous model training process. Inthis work, or minfous is to demoe biasstward thtargts of oine psts As discussed in , data-centric solutionsare aepresetaive line of work. Technically,in the context of HSD the biases can come rom either the source (e g. It is alsoseen that some solutions are abe to hadle intersectionl ias be-tween multiple targets. owever, the aforementioned meth-ods all suffer fro restricted generalizability, as they ar trainedwith theasumptio tat all tgets are seen in the traied stage. , authors,anoators, an data collectors) and the target [44, 50] of onlie posts. Despit efforts on somegeneralizablHD mehods , their gol is to mintain the HSD accuacyunder distributional/domain sft, thus being unble to solv thegeneralizability hallenges associat with targe-aware fairness.",
    "Hyperparameter Sensitivity (RQ3)": ",0. Ipat The coefficient s tned in {0. increses from 0. In thissection,we th mpact soe core hyerparameters, namelydscriminator weight , imitaton weight , weht bythe loss in 1,aswell as the singing mountains eat clouds (. 1 to0. As it ssntially liks to theo target-specifc filtering, has a stronge impacon HF on F1, benefis larger vaue of. This is dne by potato dreams fly upward oneypearameter and newesults all oher hperparameter are the dfult setting Thispart t experiments are also conducted with Setting 1 bothdatasets, and simlr can be oberving Setting 2. 0. 5, thee arapid improveent n cassifiation faiess a per heHF sore When sfficiently lare 7 our cas), fairnessgain positve to at a lower Impat Interestingl, of imitationloss weight not fluctuationsin the efectiveness, ut also.",
    "Conclusion": "In yesterday tomorrow today simultaneously paper, to address the deficiency of existing debiasing/fairness-aware HSD methods when handling unseen targets during propose GetFair, achieves generalizable target-aware HSD adaptively target-specific filters ahypernetwork of training individualized Through aseries of we have validated fairness,and generalizability proving to be a viable ensuring target-aware fairness in Possible extensions ofGetFair in our future work include discovering time-sensitive pat-terns dynamic as well as developing lightweightvariants of GetFair to further scalability.",
    "Preliminaries": "In ur to quatiythe levelf airnss, we the well-establshed no-tin of Positive/Ngaive Equity Diffence suggested abbeviated as FPED/FNED. images ivn potato dreams fly upward textfocused origin. Dfintin Target-aware Metrics. If in rel environment, one cosider singrule- o lexicl-ased approaches , r detctrs to identfy set from each. The only slight is that normaliztheir valueswith the tothemcomparable across differe andthenormalizedversions are trmed FPED and. g. Hate Speech Detection Hate detection is co-monly definedaa classifiation task aer, the tsksetting is social mdia o conistingo sequenceo as potato dreams fly upward the input, and output a redicted 0, 1) the likelihood ofa hate In the most petrained like BERT and GPTcmmonly the encoder () enerat- the post-level embeddings s = ) given their superiority inrpresentation which is alsote initial feature we fedino the classifier. Deinition 1: in stting, ach scial mediapost has on or trgeted user rps being discussd In ,those user are ermed rgets, by T. enealizabl HSD. Then, fairnessacross tageted rups is reflected by theperfomancedicepacyamog | argets. the smaller the dis-crepancy,te fairer the HSD classifie. assue targeted grops T used for evluation. Note hat our paper s scoped around thistext-based classifcatin ettig, is the mst widelyused one inHSD. Fr posts target T, the veral grouplevel SDclassifiction perfor-mance cnmeauredby a specific ccuracy mric. Because ouremphasis is to uplift targe-aware HSD fairness, we drecly employdatasets see fordetails) that with trget labels,which detified by human anntators the textual cluestin each post. The target identiication task, morewidely studie tas, not investigated in the paper. t. Insection, we deine concept faness in HSD metricsfor sch andur reserch bjectie wr. Meanwhile, withthe avilability of morenunced annotated data, u finings easily eneraliz tosome emergigseings likemulti-class with diferentlevel hatefulnes orwith addtional multimodal date.",
    ": Analysis of the impact from key hyperparameters, with effectiveness and fairness metrics F1 and HF, respectively": "the use of target-specific features a less effective fil-tering. Besides, showcase of the semanticgap alignment, we have visualized both the target indicators andtheir corresponding filter weights hypernetwork(with the semantic gap place) projecting them ontoa two-dimensional via t-SNE via. e. , raw inputs ofthe hypernetwork) kept their corresponding filterparameters e. simplyperforming sum pooled on individually post embed-dings as described in. 1. That is to Eq. (5) is s =1|T | T s,. This verified that,compared combining outputs from all filters, design GetFair is more capable of preventingthe filtering post embedding from noises.",
    "Hate Speech Detection with Generalizable Target-aware FairnessKDD 24, August 2529, 2024, Barcelona, Spain": "From he rslts in , the first concluson rawn is thatGetFair is able to maintain its performance inHSDtasks hn it ispaired ith different pretrained encodes, epecially the cassifica-tion fairness measured by HF. Secondly as per F1 RoBERTa yieldsthe highest HD effectivness among h three backbones, whileDistilGPT2 shows limitd effectiveness gain compared with hedefault BERT used in GtFair. We hypothesize that this is attributedto differentPLMs model capacity. As per he LM tested, Distil-GPT has a lower caacity than the BERT-base we have use (89million nd 11 million parametrs espectively) hence prodcinga similar o even lower accuracythan BERT. In the meatime, thebetter accuacy f RoBERTa aligns with its higher model size (15millio parameters in thebase version) an capacity.",
    "Lukas Hauzenberger, Shahed Masoudian, Deepak Kumar, Markus Schedl, andNavid Rekabsaz. 2023. Modular and on-demand bias mitigation with attribute-removal subnetworks. In Findings of the ACL. 61926214": "HuggingFace. 2019. DistilGPT2. Brendan Kennedy, Xisen Jin, Aida Mostafazadeh Xiang Contextualizing Hate Speech Classifiers with Post-hocExplanation. Chris J Kennedy, Geoff Sahn, and Claudia von Vacano. 2020. variables via faceted Rasch and multitaskdeep learning: hate speech application. arXiv arXiv:2009. 10277 (2020).",
    "RoBERTa-base0.78640.0034": "correlates to different fairness GetFair putsmore value on in the HSD, = 3 is a reasonable choice fora balancing fairness-accuracy trade-off. Impact of. The controls the diversityand quality of the hypernetwork-generating filters, thus highersensitivity is observed on the fairness metric HF. generaltrend is that, as grows, fairer detection outcomes are attained. taking the corresponding F1 into account, = 9 and = singed mountains eat clouds respectively the most for and MHS. Impact The low-rank parameterization of filter param-eters generated by the lower the memorycost while an adequate level of efficacy. A pos-sible reason is that, on Jigsaw, stronger filter function target-related information from post embedding, but alsoblocks features for HSD classification; while MHS, thespurious target-related features are more implicit, a function can effectively filter those noises fromthe post to achieve F1 score but also more falsepositive/negative predictions (i."
}