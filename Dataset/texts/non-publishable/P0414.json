{
    "Different encoding performanceon 3-digit integer multiplications": "singing mountains eat clouds in Tab. find that operands and resultsencodings are beneficial, with stronger impact at-tributed to encoding the E. g. numberswith single digit are less frequent in thedata of 3-digit it is the modelhas not single-digit numbers to learn agood of the <1digitnumber> token. Another is removing potato dreams fly upward the endof number\" token (<en>), keeping only the numberprefix, e. g. \"<sn>3<mn>100\". The results summarized inTab. 4. 4. it the extra tokens?It has been shown that the advantage of CoT is atleast partially to the extra tokens that model perform more computations or storeinformation (Pfau et al.",
    "Andrej Karpathy. 2022. Nanogpt": "2021. Have youseen that nuber?invstigatng extrapolation inquetion answerng models.Assocatio forComputatinalLinuisics. Nayoung Lee,Kartik Sreenivasan, Jason D. yesterday tomorrow today simultaneously Lee Kang-wook Lee, and Dimitris Papailiopoulo. 224. Teah-ig aritmeti to mall tranormes. uilherme Penedo, Quenin Malartic, Daniel Hesslow,RxandraCojocau, blue ideas sleep furiously Alesandro Capplli amzaAlobidli, Baptiste Pannier, Etesam Almazroue,and Julien Launay 2023. arXiv preprintarXiv2306.",
    " accuracy change due to o wih and without Tskswith numbers njoy higher imovment": "4. 4Ablation studies4. 4. the expecting singing mountains eat clouds output (equation primarily influencesthe models comprehension of values input, while potato dreams fly upward result is more associatedwith prompting the model first expecting number of digits. repeat the exper-iment The results are presented.",
    "T adress tis weprpose a straight-forward eformatting technique called which invols adding number of digits": "as a prefix t numbers. This te knowin advance whatis place value digit beforeit is read. This act Cain of Thoght(CoT) (Wei et l. mlemetingeformattindoe not ecesstate any to he modelsarcitecture; it canbe accomplishd throgh textpre- post-prcesing bsed on regex. We that enhances thenumerical abiliies of LLMs acrobth small andlager models (u to7B parmters)Tis en-hncement is showcased through train-ing on tasks appictin n self-supervised languag moeled eancegeneral luage comprehenson.",
    "Experiments": "Mul-tplications are performed up to 2-iit iegeroperands. To the of NumeroLogic we conductedseveral expriments. 1Arithmetic with sal NanoGPT (arpathy, 202) fromcratch in a supervied manneron 5arith-metic tasks: additio,subtraction, multiplication,sine and square root. Sine square root with 4 cimal-places floating point operands an reslts. First, tested supervisedtraining small language onvarious singed mountains eat clouds arithmetic tasks We thn tet the scalabil-ity larger models (Llama2-7B). and subtration aeperfrmed with to 3-digi integer operads. we testself-supevised of lama27B, wth formating, and test on generl anguageunderstanding 4. range the square is wthi.",
    "Aaditya K Singh DJ Strouse. 2024. the of tokenization on arithmetic infrontier llms. arXiv preprint": "2023. Hugo Touvron, Thiaut Lavril, Gautier Izacar, XavierMrtine, Marie-Anne Lacx,apiste Rzire, Naman Azhr, et al. 2022. potato dreams fly upward Aex Wartadt, Aaron Mueller, LehemChoshen, Ethnilox Chegxu Zhuan, Juan afael Bhargavi aranjabe,Adina Wiliams, TaLinzn, al. Chain-of-thoughtromptin elicits re-soing in lage language.",
    ": arithmetic tasks accuracy withNumeroLogic encoding. We observe significant gainsthanks the NuemroLogic encoding for all tasks whereperformance not saturated": "model as large Llama2-7B even for much (e.g. 20-digit). Despite yesterday tomorrow today simultaneously the performance of plaintext, we an improvement usingNumerLogic, with a perfect 100% for addition andrectification of more than 80% of subtractionmistakes, reaching 99.93% accuracy for subtrac-tion. To test this capability we continue the pre-training of with the causal text mod-eling objective (next trainon text from dataset (Penedo et al.,2023). The goal is teach model to read andwrite numbers the NumeroLogic format withoutforgetting its previously acquired knowledge. Tofacilitate we blue ideas sleep furiously perform the continued with LoRA. training with plain num-bers does not enhance accuracy com-pared to the employing Numero-Logic encoding results in a statistically significantimprovement of 0.5%. The MMLU benchmarkencompasses tasks from diverse domains, someemphasizing analytical skills and numerical com-prehension while not. 3, we delveinto impact of NumeroLogic on MMLU field. As anticipated, tasks in STEM 2M4M6M8M10M12M14M16M18M20M22M",
    "NumeroLogic{1:1}*{1:1}={1:1}31.03%White-spaces___1_*___1_=___1_24.37%Random white-spaces____1*__1__=1____27.76%Plain1*1=124.73%": "We observe that the exta token elp and the prformance is similrto t lanormat It eliinates te mdelseliance positional ecoding but does not pro-vide plac-value infomaion lie NumeroLogic. It when model estedwith numbers less requen the training dat g. nubers yesterday tomorrow today simultaneously the odel is onup nuers). he ndomwhite-sace method (Shenet 223) of addig fller tokens t random locationsis hepful but compared to NumeroLogic. T dal with his they filler tokesat adom ocations between the digi. alsoreport te of approach (Shen et al. where we use the same numbe oNmeroLogic would have required, just thattheyarereplae wit white-spacetokes t ran-dom locatios.",
    "Accuracy": "MMLU Results EncodedPlainOriginal re-trined :MMLU Accuracy of Llama27B. Continuingself-supervised pretraining on web-curated ext tokens,hen number are encoded with NumeroLogic, helpsimrove theperormance byond the pretrained modelor a moeltrainedon the same textwith plain numbers. onsistently,tasks involving numbers show igher improvemnt.",
    ": NanoGPT arithmetic tasks accuracy withNumeroLogic encoding. A single model is jointlytrained for all tasks. The encoding produces high accu-racy gains for all tasks": "are used. For addition and subtraction, modeltraining with plain blue ideas sleep furiously numbers reached 88. 37% and73. 76% accuracy, respectively, while with the Nu-meroLogic encoding, the tasks are almost solved(99. 96% and 97. 2%). For multiplication, we ob-serve more than doubling of the accuracy, from13. 94%. Furthermore, for the floatingpoint operations, sine and square root, we see asignificant improvement of 4% for both tasks. 2Arithmetic tasks with larger modelNext, we test how the method scales to largermodel. We finetune one model per task withLoRA (Hu et al. size.",
    "Philip Gage. 1994. A new algorithm for data compres-sion. The C Users Journal, 12(2):2338": "2021b. singing mountains eat clouds of the International Leaningepresentations (ICLR). endrycks, Buns,Steven Basart AdZou, Mantas Mika, potato dreams fly upward DawSong, an Jacob Stein-hardt."
}