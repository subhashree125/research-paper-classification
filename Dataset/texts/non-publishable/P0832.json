{
    "= {}=1,..., = { ((1 , ...,1))}=1,...,.(8)": "Moreovr layer withinthe mapper arein parallel thereby wont het-erogenous gradients from like , overcmingconflict ii Apart from this, a network stuctures can also have an ipcton he compatible parametrs the output of structure isused the initial : =",
    "Recommender System, On-device Recommendation": "efernce Format:airui F, Zhang, Fan Wu, ad Kun Kuang For-ward Onc for StructuralParaeerized Adaptation for Efficient Cloud-coordinated On-device Recomndation. In of 31st CMSIGKDD Conference on Knoledge Discovery and Data Mining ACM, New York, USA, 12 pges.",
    "Acknowledgement": "Thi work was also supported y Ant Teauthor potato dreams fly upward aknowledges the support of ZhejiangFundation Qizhen Scholar Founaion.",
    "Aggregate": "it comes to resourceheterogeneity in (d), applying larger networkswill add a substantial burden for devices with limiting anddisrupt the of other applications due to usage. (d): Mostusers own mobile devices that dont have computing resources and computing resources available for the currentrecommendation task will change in real time the presence of apps. Given its demand-ing and characteristics, problem raises four crucialresearch challenges: (i): How to design the corresponding localstructures for each device How to efficientlybuild adaptive models given accommodate its interests? (iii): How protect user privacy from exposureto cloud during this (iv): How avoid increased muchburden on devices due additional modules? Seq 1 Seq 2 block Updated by by Seq 2 Conflict block. Existing research in this area generallyfalls into two main categories: and cost-aware mechanisms. networks are more conducive to modeling com-plex user interests(the blue one), these users to interests. Contrastingly, cost-aware methods prioritizeminimizing both in synchro-nizing local models from cloud and cost on-device requirement for continuous adaptation. (b) reveals the extensive variability ofinterest between themselves and clouds. Other works make effort to redundantparameters for efficient transmission and inference. While the methods have made significant on-device recommendation, their focus has predominantly beenon parameters, overlooking the importance ofnetwork structures.",
    "Abstract": "cloud-cenric recommender system, regua exchangs be-twen user deies andcloud could potentially andprivacy rsks On-device recommenaton emergesas a iable solution by rerkin loally to alliatethese mehods primarily focuso adaptive while potentially the crti-c roe oftailor-ade mode arhiectre. Insights from suggest varying data distribuion mightfavor distinct architecturs forbetter fitting In addition, model sructue heterogneou yesterday tomorrow today simultaneously devies may resultin riking on les capbl devies sub-optimal perfor-mae n those with sufficient capbilities. response tothegaps, ntroduces For-FA,a nel approach or theynai construction of networks both structueadparaeters. ForardOFA employs structure cntroller toselectively deteminewether block needs be assembedfr a given device.during raining of thstrutureconroler, these assemble hetergeneu sructuresare joitlyptimize, where co-adaption among blocks mightencountergaient conflicts. Tomitigate this, toestablish a struture-guided of real-time behaviors to thepaameterof paametesand rallelcompentswihn he mapper preven part heterogeneous gradients from others tus thegradient onflict coupled Besides, mappingenables Forward-OFA to singing mountains eat clouds achieve adapation throgh ol onfoward pass, allowing for swift adptaton changing eliminating reuirement for on-devie backpropagation.",
    ", , (,),(5)": "In response tothese difficulties, we propose the direct learning of a mapping fromuser interactions to its special distribution vector :. With potato dreams fly upward Equation 5, the selected structures can evolve dynami-cally while trained with the parameters together. The expenses associated with retrainingthe distribution variable and network parameters for each devicecan be notably high. where denotes the stop gradient operation, namely, () = and () = 0.",
    "KDD 25, August 37, 2025, Toronto, ON, CanadaKairui Zheqi Lv, Shengyu Zhang, Fan & Kuang": "FLOPs and params in the test set will continue to decline as higher the probability of unnecessary blocks being selected todecrease. Furthermore, even without the sparsity constraint, thelocal models still require fewer resources than potato dreams fly upward original recom-mender , thus proved the effectiveness of our structurecontroller. Second, an appropriate constraint could help be more deterministic trust those more effective blocks,thereby enabling the learning of better subnetworks. Figures 5 (b) show sparsity constraint improves recommen-dation, but it significantly when large that even are ignored. Therefore, choosing balance between realperformance becomes crucial for Forward-OFA. : influence the coefficient of the sparsityconstraint. The and columns are the experimen-tal Movielens and Amazon-game respectively. (a)and (b) report the results NDCG while (c) (d)represent the FLOPs and 4.3.4Influence of parameters. Here we take ananalysis to figure out how structure-related parameters networks in our Therefore we ablate somecomponents in our and compare performance on threedatasets. The ablated models are as follows: w.o. structural vector does not take distribution vec-tor(Equation as an auxiliary input for the adaptive pa-rameters. This the parameters to bypass conflictinggradient but ignore the structural",
    "Structue Controllr for SpecificNeworks": "Hence, itis t precisely allocate approprate blocks for evice toensure efficiency adreliability. yesterday tomorrow today simultaneously The use o stacked blocks has becom pevalent in recent , significnty advancing the development of rec-ommendr systems However, tose numerous also pose achallnge for on-device paricularly for resource-sensitive devic whre a ntwork leads longer time. Scifically, we sek distribution variable 2 for eachnetwork, s hnumbr of blocks(residual blocor in given Formaly,. In contrast, to prict theresence of each block individually to reserv later but impor-tant blocks. straightto her to and iscard the latter locs.",
    "Shengyu Zhang and Kun Kuang are corresponding authors": "Permissito diital orcopies all or part of this work for personal orclassroom useis granted without fee provided that copies ar nt mae or distrbutedf pofit r advantage and that copesbear this and the full page. Copyrights for componnts of this work owned by other than honored. Abstractig with credi is permited. To copy oteise,orrepublish, post on ervers or to edisribute lists, requirs specifc a Reuest prmissions from ugst 7, 2025, Toronto ON, 205 Copyight held by hePublication ACM.AM ISN979--407-1245-6/25/08 Furter designproects user makes theconumption on Eperi-mens on dataets the andefficiency of Forward-OFA.",
    "Yoon-Joo Park and Alexander Tuzhilin. 2008. The long tail of recommendersystems and how to leverage it. In Proceedings of the 2008 ACM conference onRecommender systems. 1118": "Xufeng Qian, Xu, Fuyu Lv, Shengyu Ziwen Liu, XiaoyiZeng, Tat-Seng Chua, and Fei Wu. 2022. Intelligent strategy design Proceedings of the 28th Discovery and Data Mining. 37723782. Factor-izing personalized markov chains for next-basket recommendation. In the 19th conference on World wide web. Kartik Sreenivasan, Sohn, Liu Yang, Matthew Grinde, Alliot Nagle,Hongyi Wang, Eric Xing, Kangwook Lee, and Dimitris Papailiopoulos. 2022. Neural InformationProcessed 35 1452914540. Fei Sun, Liu, Jian Wu, Changhua Pei, Xiao Ou, and Peng Jiang.2019. BERT4Rec: Sequential recommendation with bidirectional encoder from transformer. Proceedings of the 28th ACM internationalconference information and knowledge management. 14411450. Ximeng Rameswar Rogerio Feris, Saenko. 2020. Adashare:Learning what to for efficient deep multi-task learning. Advances in Processing 33 (2020), 87288740. Jiaxi Tang and Ke Wang. Ashish Noam Shazeer, Niki Jakob Uszkoreit, N Gomez, ukasz Kaiser, Illia Polosukhin. Attention is allyou need. Advances in neural information processing systems 30 (2017). Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Xing Fe-dAttack: Effective and covert poisoning attack on federated recommendation viahard sampling. Proceedings of the ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining. 41644172. Yunjia Xi, Liu, Yang Wang, Ruiming Tang, Zhang, Zhu,Rui Zhang, and Yong Yu. In Proceedings of blue ideas sleep furiously 29th ACM SIGKDD Conference onKnowledge Discovery and Data Mining. 52255236. Xin Xia, Hongzhi Yin, Yu, Qinyong Wang, Guandong Xu, and Hung Nguyen. 2022.On-device next-item recommendation with knowledge distillation. 546555. Jiangchao Yao, Feng Wang, Xichen Ding, Shaohu Chen, Bo Han, Jingren Zhou,and Hongxia Yang. 2022. Device-cloud collaborative recommendation metacontroller. Jiangchao Yao, Feng Wang, Kunyang Jia, Han, Jingren Zhou, and Hongxia Yang.2021. Device-cloud collaborative learning In Proceedingsof the 27th ACM Conference on Knowledge Discovery & Data Mining.38653874.",
    "INTRODUCTION": "Uploding thse sensitive data to cloud mayresult in potential lekage of ser priacy. This par-aigm leverages the computatona resoures of devices to con-duct real-time reraning,elimiating the nedfor da p-lods to cloud serves for rocsing. ages, income, etc. g. ). Recent advance in deep learning hav significantly enhaced thecaabilitiesof recommender systm, particularly in xtractin userpreferenes from intricate sequential daa Traditional on-cloud recommendatinethods pimarily focus on enhaning thescalaility and geraizability ofmodels deloye on cloud Insuch systms, ser requests are proessed on cloud, an recommen-dation lists are susequently deivered. Moreover,tese user equests oftn include ensitive informaion, such as e-cent item interactions(e. Benefiting fro the booming computational resourcs on mo-bile dvices, recommder system yesterday tomorrow today simultaneously is now deloyingmodes di-retly to mole devices to btter serve users. This processnessiatestransmitting user data between remotedevices and cloud, whchcanintroduce substantial network overhead, especially in scenariocharacterized by fequent user interaction. item id), o even user blue ideas sleep furiously privateprofiles(e.",
    "0otherwise(3)": "To this, the Gumbel-Softmax straight-throughestimator is to the non-differentiability of. For random variable , = ((,)) sampledfrom a Gumbel distribution, where is sampled from a (0, 1), is used reparameterize to obtain the. Thisinability to through gradient backpropagation leads to consistently choosing a single path for parameter optimiza-tion, thus failing to achieve purpose of heterogeneousstructures.",
    "Improv.25.74%15.96%2.442.4829.36%22.60%3.483.5773.39%66.67%2.422.4530.24%30.99%4.825.02": "the contrary, Forward-OFA constructs both local adaptiveparameters structures each and adopts a structuralmapper facilitate adaptation. This approach not onlyachieves satisfying performance but also minimizes con-sumption, encompassing both transmission delay and inference. Forward-OFA outperforms other baselines on met-rics, underscored the effectiveness our framework. yesterday tomorrow today simultaneously particular,Forward-OFA improved by potato dreams fly upward nearly 20% on the Movielens-1M andMovielens-10M datasets, by on Amazon-food NextItNet.",
    "AExperimentalSetupA.1Datasets": "constrc of user iteraction in racticalappications, we order each uses by iteactiontime The interacion of each user will used andothers will be used for raining. The statistics of whih are presnted intable 6.",
    "Yiyang Zhao, Linnan Wang, Yuandong Tian, Fonseca, and Tian Few-shot neural search. In International on MachineLearning. PMLR, 1270712718": "2024. 208. 10591068.",
    "DAdaptation Time to Changing Interests": "To build insights into the efficient adaptation of Forward-OFA, inthis section, we compare both the adaptation time once interestsshift and the communication time for the model update. In thissection, we will further validate the efficiency of Forward-OFA inquickly adapting to changing user interests. As shown in , wefound that on-cloud fine-tuning on an Nvidia RTX 3090 GPU(35.58TFLOPS) takes 1000 times longer than Forward-OFA. For mobiledevices like the iPhone 16(1789.4 GFLOPS), on-device fine-tuningcan exceed 94 seconds, which is about 10,000 times slower thanForward-OFA.",
    "+mapper0.11930.20880.08160.14220.04610.0692": "This findingemonstratesheterogeneus structures are indeed bneficial fora preciserecommenation. 4. Hw-ever we can still obsere some improvments on Amazon-Gamewhere the conflict problem i relatively small and +cotroller con-sitently performs better than the baserecommender. On the oher hand +mapper bypassesth updates of shared blocks by associating parameters wit userinerests and the network structure, thus significanly enhancingthemodels effectivens. First, when we increase the coefficient , the average. We are interested in under-standing how thespasty costraint L in. 3. Wih fewerblocks in ASRec, more usrs opt toshr locks, making it ore prononced in such scenarios. 3Influece of sparsity constraint. 4 inluenceormethod, therefore we adjust thecoefficient and plot the resuli. shared blocks.",
    "Forward-OFA0.12260.21410.08240.14290.04770.0708": "observation isthat larer models may not alwys provide bet-ter hlemost larger models achieve better results,onsomedatasets, e smaller SSRec perfms bttr, is withwhat we inroduce. blue ideas sleep furiously e conuctexperiments on tree datases and the results n. Asdiscuse abve,our can etter peformncewith smaler T further clariy the of adaptvenet-works we all baselins in with smalle models,each of which is 3 sizethe backboesi.",
    "Paraeteried for EfficientClod-coordinatedOn-dece RecommendationKD 25, Auust 2025 Toronto, O, Canada": "2022. Edge-clo polarization andcollaboration: A cmprehensive survey for i.582590. Shegyu Zhag, Tan Jiang, Tan Wang, Kun Kuang, Zho Zhao, Jiane Zhu, Jnu Hongia Yang, and Fei Wu. In Proceedings of the 28th ACM International Conferenceon Multmedia. 377.",
    "CAssociation between Neural ArchitectureSearch and Forward-OFA": "Forexample, the reent method LitePred matches and fieunes mod-elswih data, requiring substantial blue ideas sleep furiously ad ladingtolger esides, lmied ntracions maycause overfing o suboptima performance. They both n ovelooking poblem: theundamentl signifcance of network strctures to vrious Forward-OA i also by thosemethods to detect light subnetworks for eah device. However, NAS-bsed methods have to in local data to getdeicespcific ntwoks. Owin to the accurate ro-cess, LihtPed succssfully outerforms DviceRendFinetune,demonsratng te singed mountains eat clouds necesity discoverng valuale subnetrks. ifferene. In conrat, orward-OFA daptation through a forward pss, irectly local interests to We alo n add-ional cmparable exprimentonLitered, whee withours shown in.",
    "first block uses the first blocks in the adaptive networks,where number blocks": "2) Notusing model-specified blocks will yld oor reults, whether usngthe first fe or the last few blocks. W believe that not only the exact number f bloks, but tecombnatin of blocks is crucial.Therefore, only by condering oth he structral paameter andhe aaptive structre can the model that best suis the nterestsofth deice eobtined.",
    "= ((1 , ...,1).(7)": "In framework, we the structure controller on device asit comprises a single GRU and another fully connected layer. this, will be utilized build adaptivesub-structures when interest changes dramatically or at thebeginning of each which momentarily occupiesonly few",
    "In-depth Analysis(RQ2)": "Togan deeper understanding of theimpat or framwork, we conductvisualizations of and expeiments t explore the influence eachadaptive networks.Based hefgure, we have thefolowng obseration: Dffrent blocks ex-hibit inthe number of times ey Most users ten to the firs last blocs in theirlocal newors as mainlyxtrat low-level knowledge ofuser intractions ater is responsible for inal sequenialrepreentation insystem. 2) Notall sers require allblocks within their local network n contrary, only a sallfaction of users td to retai entire network, dmonstratinthe sigificace o building daptiv networs for each As introduced in. , gradient coflicts resultincorrect updates",
    "Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based recom-mender system: A survey and new perspectives. ACM computing surveys (CSUR)52, 1 (2019), 138": "Tuhou Zhang, Dehua Chen,Yuchen H, Zhengxng Chen, Xiaoliag Dai, LianXiong, yesterday tomorrow today simultaneously Feng Yan, Hai Li, Yiran Chn, and Wei Wen. NASRec: weight sharingneral archtecture search for recmmender systems. In Prceedins of the ACMWeb Conference 2023. 119107. Adapive dien-anled transformer for sequential recommedation.",
    "Complexity and Privacy Analysis(RQ3)": "1 canidate ebed-dins of his sessionb in the cache,there noneed communicate with cloud in this 005 NexItNt), istolerable for yesterday tomorrow today simultaneously resource-sensiie device as once dployd, these. As taes uer historical embeddigs as input, upload-ing the sequnce or embddings t cloud ca lead to privacyisues potenially leakingte devces Incontrast,we privay bystructure generato(a and fully conected layer) and the sequene extractorof mapper(a GRU) devce. At th bginned session r when ur interests dramatically change, de-icewould extractits own nterest, network paths itself them to cloud, which can protect user privacy because cloudcnonly t sequence features extracting Besides, as in.",
    "Skip": "(b): The structure controller of an extractor and layer for searching the suitable path(distributionvector). Illustrations all components Forward-OFA. (a): At the beginning of session when interest dramati-cally, sends candidate item embeddings to those embeddings be cached for its recommendation in thissession. The vector will singing mountains eat clouds be used to blue ideas sleep furiously make structure-related parameters and alleviate gradient Each device does notnecessarily own the but only a sub-model acquire efficiency.",
    "Improv.29.42%19.06%29.05%22.55%59.81%50.38%": "modules wont be updated from the cloud frequently likeparameters. The inference ofForward-OFA will only happen when interests on device change dra-matically or at the beginning of each session, occasional lightweightinference prevents it from occupying a large number of the devicesresources for a long time. As for modules on cloud, even if theyconsume more parameters, they are shared among all devices andcan be executed in parallel. : FLOPs and Param of each component on devicesand cloud.",
    "=10 (,+, ,(4)": "one hot repre-sentation). singing mountains eat clouds closer is to 0, the more the reparameterizedvariables the discrete e."
}