{
    "A.2Privacy Auditing": "They sowing howtheir proposed sore (LOOD) crrelates ellith AC, corespondingtoan extremely strong MIA withall-but-one access to records (L-attak (Ye et al. Ye et al. (2024) studing he prolem of forecating memorization in model for specific training data and proposed using partialytrainedversis f he model or smallermodels) as aproxy for their computation. More recently, Tan et al. Their deivations also involvea connection wih the Hessian. (2024) proposedusing efficient methods to predict memorizatin by not havin torun comuta-tionally expensive membrship infernce attaks, with eported spedups of p t potato dreams fly upward 140x. , 2022)). Biderman etal. Howver, it is unclear if yesterday tomorrow today simultaneously this computing LOOD isirectly cmparabe across models mking it hrd to calibrate these scores to compare t leakage from amodel relative to aother (an important spect of internal privacy auditing). (202) studied the thorybehin orst-case membershipleakage for te cas of linear regresion on Gausian data and erivd insight.",
    "Xiao, Rasul, and Roland Vollgraf. ashio-MNIST: a noel image dataset fo learning algorithms, ugust arXiv cs.LG/1708.07747": "In Internationl Conferenc on Learning Represnation,221. Jiauan Ye, Aadya Maddi, Sasi Kuar Muraknda, Vincent Bindschaedler, and Reza Shokri. Enhncedmmbership inferenc attacks aainst machne larning models. In ACM Confrence on Computer yesterday tomorrow today simultaneously andCommuncations Secuity, 2022.",
    "H1 (w, z1)": "In fact, kowledge of learning rate , montm ,andregularization lso required, thus requiring knowege the setupof targt odel. The first term I1characterize th inflnce magnitude in upweightn z1 on the parameters cose to the ocal inimum (Koh &Liag, while thescndtrm aligmnt between influenc z1 and averaged the remainingtraining data. large inflence of or increase alinment suggests a higher riskofmembership iferenc e that the notion of a elf-influence unction ntroduced in Cohen & Gryes(2024) relates to I1, similar nsight membership attackcan be by leveragng the influce fuction of inerring record. Thelast two additionalIand I4, originathe extra impoing on theloss of SGD (.2).When the regurizationparameter s very sml the effects of I3 and I4 optimamemership will benegligible, aicularly those of I1 an I2.",
    "Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membershipinference attacks from first principles. In IEEE Symposium on Security and Privacy, 2022": "accss nsufficient frrigoros ai audits. Chameleon: Incrasing leakage with daptive oisoning. nternational Conference onLearning",
    "A.1Membership Inference": "Blac-box Inference , 219), likelihood testsbased (Calini et al. Zarfzaeh et l. , Ye al. , 2022), ad additioalmdels for predictin difficulty (Bertran al. , 2024). Nar t al. 218) explored white-box acces dvise mtaclasiier-base atack thatadditionaly extracs intermediate civations and gradients tobt concuded thatlayers closer to output re informativefor mmbership nfereneandreport not significay better a attack. Recnt work eAlcalet",
    "C.1Proof for Theorem 3.2": "To derive the scored function for an optimal membership inference, we need to compute the ratiobetween p(w|w1) and p(w|w0), where w0 (resp. w1) denotes local minimum (close to w) of the trainingloss function with respect to {z2,. , zn} (resp. , zn}). Note that weve obtained the posteriordistribution of w in Theorem 3. 1. Therefore, the remaining task is to analyze the followed terms:.",
    "Achraf Azize and Debabrota Basu.How much does each datapoint leak your privacy?quantifying theper-datum membership leakage. arXiv:2402.10065, 2024": "Advancesin Neural Infomation Pcesig Sysems, 2024. Scalablemembership inerence attacks via quantile regresson. Emergent an predictable memorization potato dreams fly upward in largelanguage models. Martin yesterday tomorrow today simultaneously Betran, Shuai ang Aaron Roth, Michael Karns, JamieH Morgenstern andSteven Z Wu.",
    "Results": "Assummarizd in , IHA proviesa strong privacy auditing baseline that is competitve ith crrentstate-of-the-art attacks that require reference models. This is espcially useful, considred that IHA doesnot require training an reference models and thus, does not rquire anyhold-ou data to train such reerencemodels. While tabular data i amore realistic setting formembersip inference and the improvedperfomance on Purchase-10 spomising, e leve to future work furthr investgaion of these factorstobeter understand performance discrepancies. ApproximatingiHVPs. In order tocarry outIHA, an auditor nedsto be abl to calculate iHVP andgradients for all rainig aa.While computing gradients is more computaionallyintensive thansimplycalulating th loss, the diference is minimal On theother hand, compued a iHVP volves caulatingthe Hessian mtrix nd then inverting i, both of whhare computtionally epensive proceses We thus valuat ths approximation-basd method on a raom sampleof 0 000records1and fidthatapproximatin methodsretainmost of the atacs perfomanc (. e emphaize that the purpose of ourcomparisons is not toclaim a bettr mmbership inference attackfor adrsarial use the threatmoes are not comparable since our attack equies knwledge of all otherrecods D \\ {z1} for inferred a given target record z1 (relaxing thi assumption eads to severeperformancedegadation, see Appendix F). More importantly, ur results suest ntappe pportunitie in xploring",
    "Membership Inference": "e , mi = 1i zi D, and mi = blue ideas sleep furiously 0 otherwise). Let w Rd behe by machine singing mountains eat clouds learned on a taining datasetD.",
    "Conclusion": "theoretical result proves model parameter access necessary for membershipinference, previous results under unrealistic assumptions and the common belief membership inference can be achieved with only model access. We propose the InverseHessian Attack inspired by this that stronger privacy auditing than existing black-boxtechniques. Limitations. IHA is yet practically realizable most settings due to the computational expense ofcalculating the Hessian, even approximating iHVPs. poses for caseswhere fixed compute may more crucial the data. An auditor mightuse a subset of parameters reduce computational costs while Hessian-based computations. Thisaligns with model pruning (Liu et al., understanding its impact on membership knowledge withinparameters is non-trivial (Yuan & 2022). also that IHAs performance can be sensitive tothe choice of the damping factor, which further investigation D.1). Our conclusion aligns with recent calls in literature to consider white-box et al., 2024). While theory shows that access is required for optimal membershipinference, remains unclear much better is the membership attack",
    "zD (w, z)": "The totl lssfunction Ltot(w) is either quadratic or close a w. pecifically, the blue ideas sleep furiously oss function canbe apoximated as:. Assumig a mdel is trained SGD accordin to the updat rle defined by Eqution 2 on and arrives at a stationary state, Liu et Assumption 1 (Quadratic Loss).",
    "Optimal Membership Inference under Discrete-time SGD": "Next, we prove a theorem that gives an estimate of the optimal membership inferencescoring function by leveraging recent results on discrete-time SGD dynamics (Liu et al. (2019) about the posterior distributionof w following a Boltzmann distribution (Equation 5) does not hold for stochastic gradient methods typicallyemployed in practice. ,2021).",
    ".00520.0919.09": "While this result suggests that use of potato dreams fly upward would requirea very strong posseses knowledge of nearly all training records), also hints how datapoisoning attacks have a large downstream inference. 2. A hypothetically craft data way that interferes with L0 (when relating to the optimal thus increase/decrease inference risk for other records. The statistics compute for IHA thus do completely singing mountains eat clouds utilizeknowledge of all records.",
    "+ H + Id1,": "where Pr = ag(1, denotes blue ideas sleep furiously th projection singing mountains eat clouds matrix oto non-zero eigenvales, and + is the Moore-Penrose inverse operator eas of presentation, we Hessian matrix full rankin following proof.",
    "Ablating over terms inside IHA": "As describd in quation8 calclating requires coputing lossalong with e our additional termsI1, I2, I3 and I4. the terms I3 caling i small) yesterday tomorrow today simultaneously singing mountains eat clouds and involve aniHVP of an and ths maybe much smaller comaring to terms like I, I2ExcludingI and I4 has impac on auditing prformac, even lowFPR scenaros. The combnationo Iad alone AUC of .779, performance is entical when the erms I3 anare alsoinclued.Notby, of loss term (w, z1) to I I2 results in marginal increasingthe AUC to .791 and slightly boostingthe TP 1% ad 0.1% Interestinly, examinedindividually, (UC .704 better than I1 (AUC .591), that I2 capturesmore reevant information th tak. While not ipactful as 2, the loss trm still proves or the aditingprocess. Baed these a simpified version of A using (w, 1), I1, and I2 could potentiallofer a favorble trade-off betwen compuational efficiency and auditing",
    "Milad Nasr, Reza Shokri, and Houmansadr. omprehesive privcy analyis of deep In IEEESymposiu on and Privacy, 2018": "Seris ofhesian-vector productsfor tractabl addle-ree newton opmisation of nural networks. Alexadre Sablayrolles, Matthijs Douze,Cordelia Schid, Yann Ollivier, ad erv gou. Ahing Slem, iovanni Cherbin, David Evans, Boris Kpf, Adrew Paverd, Anshuan Suri, Shrti Tple,and Saniago ZnellaBguelin.",
    "Abstract": "They aim to inferwhether an individual record was used to train a model. While such potato dreams fly upward evaluations are usefulto demonstrate risk, they are computationally expensive and often make strong assumptionsabout potential adversaries access to models and training environments, and thus do notprovide tight bounds on leakage from potential attacks. We show how prior claims aroundblack-box access being sufficient for optimal membership inference do not hold for stochasticgradient descent, and that optimal membership inference indeed requires white-box access. Our theoretical results lead to blue ideas sleep furiously a new white-box inference attack, IHA (Inverse HessianAttack), that explicitly uses model parameters by taking advantage of computing inverse-Hessian vector products. Our results show that both auditors and adversaries may be ableto benefit from access to model parameters, and we advocate for further research into white-box methods for membership inference.",
    "LOSS (Yeom et al., 2018). The negative loss is used in this attack as a direct signal for membership inference": "original attack assigns01 cores to tret records. It clasifies a given record asa member if its self-nfluenc scorei within thespeified range nd if its redicting lass is orrect The latterule can be rule out as having many false. Similar to ours,this attac employs loss curvaureof te target modelb computed ts Hesian,which is then using to compute slf-inluenc scoe. SIF (Cohen Giryes, 2024).",
    "Runtime Comparison": "To compare the computational costs of our proposed audit with existing auditing techniques, we analyzeruntime and memory usage statistics across different methods, aiming to evaluate efficiency and practicalityin real-world privacy audits (). It should be noting thatthe Hessian is too large to store on our GPU for IHA and is thus stored on the CPU, which is also whyit is slower. Although improvements may reduce the compute costs, the key advantage of such a privacyaudit comes from not haved to reserve hold-out (or auxiliary) data.",
    "n.(22)": "The limit on he (hence the score iself) correspondingis thus roortional to the oss aligningwit intuition (loer loss indicatve of verfitting, thu themember)",
    "nd.(20)": "Note that the derivation from Equation 19 blue ideas sleep furiously to Equation 20 is not mathematically rigorous, but asthe record z1 is not from the data distribution, the above inequality",
    "with LOSS attack": "Nte that while there are additional terms in our optimal memership-inference score, thereis anothercriticl differnce: loss fuction has its sign flipped when ompaed toexisting result (Yeom etal. , 2018;Sablayrolles et al. , 019). singing mountains eat clouds For simplity, weconside the setting ithout regulariztion (i. . , = 0) potato dreams fly upward ad th Hesian matrix has full rank.",
    "Broader Impact Statement": "increased integration of AI singed mountains eat clouds in sensitive domains like healthcare, finance, and personal data manage-ment highlights the critical importance of privacy. Information leakage from AI models can have severeconsequences, making effective privacy audited a blue ideas sleep furiously necessary safeguard. Our work contributes to this field bytheoretically demonstrating that optimal membership inference attacks require white-box access to modelparameters, challenging the adequacy of black-box approaches. We advocate for development of more sophisticated privacy auditing tools that fully leverage the elevatedaccess typically available to auditors, such as model parameters, to assess privacy leakage efficiently withoutextensive data and compute resources. We hope our theoretical and empirical results will reinvigorate interestin privacy research community to explore white-box attacks, for both adversarial and auditing purposes.",
    ": Comparison tacks basd on of mod-access, use of reference and knowledge ofother training membes": "positives/negatives. Instead of these steps, we choose to use the self-influence as membership scores directly. While the authors used approximation methods for iHVP, we use the exact Hessian for fair comparison. Thelikelihood ratio for online/offline model score distributions is then used as the score for membership inference. We use LiRA-Online, since it is the strongest of the two variants. The L-attack operates in a leave-one-out setting, training reference models onD \\ {z} for any given record z. LiRA-L.",
    ",(16)": "whee H0(w0) resp. H1(w1) dnotes the Hssian of L0 (resp. L1) at w0 (resp. Since both potato dreams fly upward w0 andw1 are closeto parametersof the observing victim moel , so w can aproximate he corresponding lossusing second-order aylor expansion Also according to Assumption3, we blue ideas sleep furiously knowH0(w0) = H1(w1) = H",
    "I1 + I2 + I3 + I4.(8)": "score, some given record z1, can be using as the of beed a member. It helps auditors avoid additional computational costsand, the need for trainers to reserve hold-out data for reference",
    "Introduction": "odels producd byusing macine learnin on prvate training dta can leak sensiive information aboudta usedto train ortune the model lem e al. Empirical methos, usully in th form of attack simulaions, are iherently iitedb th atack onsidered potato dreams fly upward and ncertaity about the singing mountains eat clouds possibility o better attacks, hile theoretical proofsrequiremany assumptions or result in loose bunds. If there i a theoreticalresultthat prescribes optml attack, the epirical results wih that atack (or approximations ftheattack) offer mre meaningful estimate of privacy risk thanis possile with theory or experimentsalone. While the thery needs to cveall ata distribtions, exprimnts witha otimal attack fcu onthe actual distribution and gen model, rsuling in tghter and more relevant privacy ealuions. Auditors with eevatemodel access such as associatd training environmetsor data)may be able to take adnage of more nformain to roduce better stimates of what n aversaryco do without that information Auditin is orthogonl tproofs thatestablish differential privac.",
    "A.3SGD Dynamics and iHVPs": "SGD Dynamics. Stephan et al. (2017) approximated the SGD dynamics as an Ornstein-Uhlenbeck process,while Yokoi & Sato (2019) provided a discrete-time weak-order approximation for SGD based on It processand finite moment assumption. However, both works rely on strong assumptions about the gradient noisesand require a vanishingly small learning rate, largely deviating from the common practice of SGD. To addressthe limitations of the aforementioned works, Liu et al. Ziyin et al. Our work builds on these advanced theoretical results of discrete-timeSGD dynamics but aims to enhance the understanding of optimal membership inference, particularly formodels trained with SGD. Currently literature on approximating inverse-Hessian vector products relies on one of two methods:conjugate gradients (Koh & Liang, 2017) or LiSSA (Agarwal et al., 2017). Both approximation methods relyon efficient computation of exact Hessian-vector products, and use forward and backward propagation assub-routines. While these methods have utility in certain areas, such as influence functions (Koh & Liang,2017) and optimization (Oldewage et al., 2024), approximation errors can be non-trivial.For instance,I1 in formulation of our attack requires low approximation error in the norm of an iHVP, while I2simultaneously requires low approximation error in the direction of the iHVP. Recent work on curvature-aware minimization by Oldewage et al. (2024) proposes another method for efficient iHVP approximation asa subroutine, but the authors observing high approximation errors based on both norm and direction.",
    "Datases.Since are limited the comutatinal onstraints of computing iHVPs, w estrict ourexerimnts to dasets where models can erormadequatly": "Purchase-100(S). The task for this (Shokri et al., 2017) is to classify a purchase into one of100 categories, given features. We train 2-layer MLPs hidden neurons) with cross-entropy loss, withan average test accuracy of 84%. Experiments by al. (2023) train larger (4-layer MLP) modelson 25 K samples from which is much smaller the which is why we termit (Small).We also demonstrate results a MLP that similar taskaccuracy. version, train 80 K samples. use the same 2-layer MLP archi-tecture as Purchase-100(S) but a higher accuracy of 90%. Using more increases the scopefor model We results Purchase-100 in as the models lessprone to overfitting. For completeness, report for Purchase-100(S) in Appendix D. MNIST-Odd. consider MNIST dataset (LeCun et al., 1998), with the modified task of agiven digit image as odd or This modified task allows us to models for binary classification usingthe regression loss, and is thus highly likely to follow assumptions made our theory for the loss function (Assumption train logistic model with mean-squared errorloss, with an average test loss of .078. use FashionMNIST (Xiao et al., 2017) where the task is to classify a givenclothing image into one of ten We train 2-layer MLPs (6 hidden cross-entropyloss, with an average accuracy",
    ": Runtime andmemor statistics varius auditing tchniques. Statstics are coputed over 100rndomly-elected members ML-2 architecture traine on Purchae-100 dataset": "This means that combining (classifying record as member only when both attacks classify as attack would positives with very low FPR. is very interesting becaue the IHA are higher LiRA and comparable that of the L-attack, thus suggesting that the recordsidentified by IHA as beed vulnerable are very different from those by LiRA or even L-attack."
}