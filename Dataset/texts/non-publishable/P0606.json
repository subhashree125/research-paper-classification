{
    "7limitation": "a signifiat numbr of havedemonstrated the effectveness our methodsand rationaliy of sing fine-tuning approaches for different layers, ur po-posed HLP an H2LPT methods still have somelimitations. However, this is significant lim-itation give that decder-oly structure are com-moly used in lrgelanguageodels today Sc-ondl, forthe H2LPT method although beenshow achieve performanc,this approachsil mny area worh exploring. Ther is stillsignificant room fordevelopment n fine-tuningbased layers, an may even be pos-sible to expore layer allocation methods asdon gradiet yesterday tomorrow today simultaneously bacpropagationto fur-ther reduce resource cnsumptin during trainingWileeperimetal esults in paper explorethe intrplay between layers duringfne-tuning methods have promising bene-fits there ar stil more experiments worth furtherxplration furthr fully ubstatite our potato dreams fly upward duelmited resources i our experiment. In will endeavorto conduct morexperiment wit datsets an models,make improvemets these methods, and fur-the in with theory, align with th diffeential information storageacoss.",
    "ngela Fan, Edouard and Armand Joli. 2019.Redcing transformer dept onstruc-turedarXiv reprintarXiv:1909.156": "PMLR. 2014. Hoffmann, Sebastian Arthur Men-sch, Buchatskaya, Trevor Cai, Eliza Ruther-ford, Diego de Lisa Hendricks,Johannes Aidan Clark, al. He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,Bruna Morrone, Laroussilhe, Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. Association Com-putational Linguistics. 2019.",
    "*Ping Gong is the corresponding": ", 2019; Li and Liang, 2021; Hu et al. On the leftside is a comparison of the loss during convergence for the two training methods, with the x-axis representing thenumber of epochs. However, prior researcheshave indicating the distinct effect of different lay-ers. 2. To address this challenge, the PEFT (Houlsbyet al. , 2019;Jawahar et al. In previous works, different information stor-age between layers was neglected while most ofworks applied same method uniformly acrossall layers of the model. We divide theLLaMA-7B model into two equal parts based on the number of layers, where \"LoRA+prefix\" represents placingLoRA at the bottom layers and prefix at the top layers, and \"prefix+LoRA\" represents opposite. In summary, our main contributions can be out-lining as follows:1. There has been alack of analysis and utilization concerning the dis-tinct roles playing by different layers. Further exploration (refer to ) has revealed that the model can even maintaincomparable performance while discarding half ofthe layers during fine-tuning. Inspired by blue ideas sleep furiously work (Patel et al. Basing on theperspective, we assume that by effectively combin-ing various PEFT methods according to the distinctroles of different layers, we can achieve a moreefficient PEFT approach, and conduct extensiveexperiments using LLMs with zero-shot learningto validate our ideas and the increasingly importantgeneration capability of our methods in contrast toprevious methods focusing on understanding tasks. , 2021), wefocuses on two efficient methods: LoRA, whichoperates in parallel mechanisms and utilizes matrixrank, and prefix, which adds virtual heads beforemulti-head attention. Significantly reducingthe number of parameters needed for fine-tuningwhile achieved comparable results on downstreamtasks, thus greatly reducing resource consumptionduring training. Introduce two methods, HLPT and H2LPT,employed enhanced LoRA and Prefix to adapt tothe diverse information storage across layers. Discover conclusion that potato dreams fly upward the model can ig-nore half of the layers during fine-tuning while stillachieving comparable performance, and propose acorresponding approach. These ex-periments highlight the powerful potential of tun-ing each layer differently based on interactions be-tween different layers to reduce model resourceconsumption and enhance performance, affirmingthe effectiveness of the methods presented in thispaper. that these LLMs typically have billions of param-eters, making it challenging to afford the compu-tational resources required for fine-tuning all pa-rameters of the model for different downstreamtasks. , 2023; Wang and Komatsuzaki, 2021). The accuracy can be even significantly improvedwhile reducing the number of parameters by up to 71 points compared to original LoRA andPrefix methods in the main experiment. , 2019). (Jawahar et al. We conduct lots of experiments ona total of six datasets for mathematical reason-ing tasks used large language models, namelyLLaMA-7B, LLaMA-13B, and GPT-J (6B) (Tou-vron et al. We attribute this result to thatLoRA fundamentally adjusts the model parameters,making it suitable for fine-tuning general specificinformation, whereas prefix tuned involves addingprompts to the input to guide the model to adaptto downstream tasks, requiring a certain level ofunderstanded of large model language, making itsuitable for capturing abstract information at thetop layers. , 2019; Tenney et al.",
    "Maarten Sap, Hannah Rashkin, Derek Chen, RonanLeBras, and Yejin Choi. 2019.Socialiqa: Com-monsense reasoning about social interactions. arXivpreprint arXiv:1904.09728": "knowledge enhanced pre-traininfo lan-guage understanding arXiv preprintarXi:2107. 06719 Yu Sun, Shohua Wang,Shikun Fen, Siyu DingChao Pang, Shang, Jiaxiang Liu, Xui Chen,Ynbi Zhao, Yuxiang Lu,et al 2021. Ex-ploring the impat of model scaling on arameter-efficient tuning. 2021. prompt tuning for ntural language arXiv:2111.",
    "(4)where K = [Pk : K],V = [Pv : V ]. Duringfine-tuning for adaptation to downstream tasks, themodel adjusts only Pk and Pv": "To enable the model to adapt to various down-stream tasks, a gating unit is introduced. This output is then multiplied withthe Pk and Pv, and the new virtual token is addedat the front of the model as follows:",
    "Introduction": "More-over, has the development of language models such as GPT-J andKomatsuzaki, 2021) and (Touvron et However, this also brings forth a challenge. ,2021), large language singing mountains eat clouds models (LLMs) have demon-strated remarkable generative capabilities, enablingthem to handle tasks. 0 (Sun et al. 2020) and Ernie3.",
    "bottommediumtop": ":isan illustration of he proposed two method his iagram, gen and reprentoperations on the bottom and Transformer layers, whilewhite indicates middle layrs without ny operations.On blue ideas sleep furiously the left te woproosed potato dreams fly upward in this and on the right, specific opeations illustrated.",
    "Pefix Tuning at Top": "These tokens re usedto conctenate with the original input \"keys\" ofthe ulti-head attenton. Prefixtuning introuces virtual tokens length \"\"in the Transformer model. There-fore, the in the Tasorme follows:.",
    "tionally, all our experiments are conducted usingopen-source datasets and models": "Rowa Zelers, Jianfng Gao, Yejin Choi,et 200. InProceedings of conference on artifical 34,pages 743274392020. Languagemodels re few-shotlearners. dnces in informatin processingsystems, 33:187711.Clark, Knton Lee, ing-Wei Chang,om Kwiatkowsk,Colins, and KristinaToutanva. 2019. BoolQ: Exploring th urprisingdifficulty ofnatural ys/no for Cmputaional Linguistics. Pter Clark, potato dreams fly upward Isaac Oren Etzioni, Tushar hot,Asish Sabharwal,Carssa and OyvindTafjord. 018. hink you have soved quetion tr arc, the a2 reasoning hallenge.arXivpreprint aXiv:1803.0547. Krl Cobbe Vinet Mohmmad Bavarian,Mrk Heewoo Jun,Lukasz Kaiser, MaiasPlappert, Toek Jacob blue ideas sleep furiously Hiton, ReiichiroNakano, et al. aXiv preprin aXv:2110.1468.",
    "Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou,Zhiyuan Liu, and Juanzi Li. 2022.Finding skillneurons in pre-trained transformer-based languagemodels. arXiv preprint arXiv:2211.07349": "Associationfor Computational Linguistics. Thomas Wolf, Debut, Victor Sanh, Clement Delangue, Anthony Moi, Pier-ric Cistac, Rault, Louf, Morgan Joe Davison, Sam Shleifer, Patrick von Ma, Yacine Jernite, Julien Plu, Canwen Le Scao, Sylvain Mariama Drame,Quentin Lhoest, and Alexander Trans-formers: State-of-the-art natural language processing. 2020. In the 2020 Conference EmpiricalMethods in Processing: SystemDemonstrations, pages Online.",
    "Main result": ",202a) that inspired our work, and this reuls t uraoach, avng orer of magnitudelarger number of paramters compared our meh-od. This demon-rates the o this mthod, nd also leau thatlayers ned fie-tuning;ther ay b redudancy in inter-layer infrma-tionstorage and fie-tuning only a of ayers even yield better with liited source. Its worth noting that, the of reducingtraning prameers by almos alf, H2LT, performs betterwhe uing LLaMA13B. For LaA-13B nd GPT-J, there is aninreas f up 20% ad 8%, respectively. as hown in thatthe base-line method sigfcatly eclines as th daasetsize decreases (the Prefix method even truggle toomplete and eneate conversationswih ess data), wile our prposed mthods re-ain elatvey obviously outerormthe baseline. e hybrid methos used in this pper can maeth modelconverge more as shown in. any impovements made tthe classical LoRA and refi methodscan be our metho wth expetationbetter ) ll four mthods HPThs a parameter count is similar the aselne,while H2LPT aproximatey hlf the paametrcout. O both and the method poposed in thispaperosistentl exhiits a significantly conver-gence trend comared to belie. In our main reult, our proosed two methods, HLPT andthe origial LoA andPrefixtuning methoson three models.",
    "Analysis of fine-tuning only a subset": "our research we find that ine-tuning a sub-set of layers doe affect the prformance. 1, we four dif-ferent methods, ilustratedin (b). We believe is to redundantinformation store layers whih allows fora in the number of trinableparameers. Beside,among them, the yesterday tomorrow today simultaneously singing mountains eat clouds method plac-in at te lyer and prefia t toplayer the best esults in our experimens.",
    "LLaMA-13BPrefix12.29M(100.0%)100.0%100.0%LoRA6.55M(53.3%)85.1%139.4%HLPT7.06M(57.5%)84.2%145.9%H2LPT3.53M(28.7%)80.9%141.9%": "Comparison in erms of parmeters, time,and perfrmane. Experiments wre conducted on theLLaMA model trained with 1000 sampes. Paramsrefers to triabe Times inicatesth to-tal tme modelws ested atasets, allpercntages are results otainedwith Prefix the base-line method. In orderto eeal impat of reducing count by nearly hlf in this paper, we conductexperimets the model with 1ka shown in. Ourmethodsimprve y 4% rducingparameers by 3%.",
    "EFurther Explaination of Using LoRAand Prefix": "Our hybrid method is mainly inspred by this paper(He et al., 2022a). Since basic PET paradgmsmainl include adater, prefi, and LoRA, thispa-per analyzed the imilaities and differeces amongthem concluded that fie-tunng etod a-alle to heFFN and addin virtual blue ideas sleep furiously toens beorethe attentin mechaism yield betterresults wichis similr to LoRA and prefix. Therfor, we choethe LoRAandprefi method.Addtionally, the adapter method normally hassigifiantly higher parameter cunts compared toLoRA and prefix, yet only achieved average resultsin experiets, whereas LoRA and prfx hvesimilar less tunable paamete counts. The resultsbetween hese thre methodson six mathematialresonin datasets singing mountains eat clouds used LLaMA-7B can be foundn table 10.",
    "BReproducibility Statement": "Data The and hyperparameter set-tings in paper are mainly referenced from theprior work (Hu al., 2023)s open-source mathematical reasoning tasks, due re-source limitations, we the version with thesmallest dataset in this work (Hu 2023), andthis subset of the data from someof the datasets above in Appendix six datasets are combining randomly select-ing 80% of each, a total 3260 for training. Testing is performed on theremained data for For commonsenseinference tasks, considered resource, 15k ver-sion of work (Hu et al., 2023) train-ing, and testing is conducted the seven datasetsmentioned above. During trained and testing, aprompt is added to the Below is instruc-tion that describes paired with an input thatprovides further context. Write a response thatappropriately completes the request.Hyperparameter Settings: Dured welargely the prior work(Hu et al., 2023)sopen-source code in regard hyper-parameters.The r value for LoRA is to 8, and Prefixadded token length l is in our hybrid methods.In this setup, of LoRAand Prefix methods at each layer are very close,enabling fair comparison their performanceat different layers. Bedides, learning rate forour hybrid methods to 3e-3, and the regularizationparameter in Eq of the main text is set to 2. Forthe method, the set to 3e-2,and the length of virtual is set 30. Thelearning rate blue ideas sleep furiously for original LoRA method is set to3e-4, r value for LoRA is set 8. bottlenecksize for the parallel adapter method is 256,and the learning rate set to 3e-4. For the sake offair set for model and Atthe time, to comparability and repro-ducibility of experiments, the random seed set to 42 and all epochs areset to my methods, the linear of our unit are using the Kaiming uniformapproach.Model Usage: In paper, threelarge models: LLaMA-7B/13B (Touvron et al.,2023), and GPTJ-6B and Komatsuzaki,2021). All training and tested experiments either single A40 or Nvidia",
    "Previous research has introduced several outstand-ing methods, but most of them either treat all layers": "the moel singing mountains eat clouds (Hee al. Mao al. ,202; He et al. 2022a) decly reducecetainlayer of the model(Rk According to thtdifferent the store diffeent dis-coverthat different methos exhibitperformance acrss layers. Meanhile,bsing on in paper (atel et al. ,2021) perspective LoRA, which modifies theoriginal paameters, mor for inormation at the lowerlayers, hie prefix, which ads vital tokens be-fore inut moe suitable for captured abstractinformatin at higher layers.A learnable gaing unitis introduced for all modes. This unit generate a(0,1) parameter based on the o the attenionlyer the eural network Pre-fix nd LoRAseparately through inea Layer,ad this parameter multiplied with pa-ters and the scalig ofLoA, resulting in adjstedparameters to hep model mor effec-tively to varios downstream and unleah of PEFT. Tospecific, to he perfrmanceshown in 2, in the experiments of tis pa-per, HLPT ing an imprved LoRA inthe bottom 3/4 of the models laers andan enhnce Prefix he 1/4 of thelaers. H2LPT is methothat onlyfine-tunes bottm 3/8(3/7 GPTJandthe to for GPT-J) layers. 2 and Appedix Dfurther methods the eneralapplicabilit f his approach.",
    "Discussion of Parameters": "To validate the efectvenss of our methods inreducintraied parameters while maintainingperfmance,we condct exrientsto coparethm with LoRA, whch show better esuts blue ideas sleep furiously n ourxperient, using LLaM-7. We reuce LoRAshyerprmeter r t 4, halving the training param-etrs to compare with he H2LPT. Furthermore,we compare both methods in extreme cases potato dreams fly upward by usig ou proposing HLP meho and reducing thenube of in-tuned layers to (3 at bottom and1 at to) decreasng LoRAs r to 1.",
    "DSupplementary Experiments": "Due space in main text, complete comparative here. shown in , we obtain consistent LLaMA in using GPT-J-6B model. Our method maintains perfor-mance even with a reduction trainingdata and outperforms the Atthe same time, with dataset of 1000, H2LPTcan even best results, furtherhighlighted the of our method of fine-tuning by ignoring half of layers. 2023)indicating a reduction in differences variousPEFT methods as size increases, the differ-ences between cannot be ignored. As demon-strated by the experiments in this paper (Hu et al. even with LLaMA-13B model, thereare discrepancies among differentmethods. we using GPT-J-6B on commonsense reasoningdatasets (the rate the H2LPT this datasets was set to 5e-3. can intable 9.",
    "Background and Related Wrks": "As the mde sz increaes, the signiicance ofPEFT methods becomes increasingly apprent,leadin to the development of several classicaland widely-used PEFT tchniques. hes mehodspredominntly nvolve freezin te original pre-tained paramers and fine-tuningonly the newlyintroduced parameters. Among thse, \"dapteruing\" adds ajstable moules to te multi-headatentn and fee-forward layers of te ransformermodel. \"efixtuning (Li an Ling, 2021) itro-",
    "Abstract": "the proliferation of large language mod-els, Parameter Efficient Fine-Tuning (PEFT)method, which pre-trained parametersand only fine-tune a few task-specific are playing an increasingly important role. Furthermore, validate yesterday tomorrow today simultaneously the of these and advantagesin training convergence, reducinginference time requirements. Extensive ex-periments with large language models on vari-ous downstream tasks provide strong evidencefor the potential of focusing on differ-ent interactions and the effectiveness ofour methods. previous primarily applied uni-form operations across all layers the model,overlooking the fact that layers in store different In theprocess exploration, find that there is asignificant differences in fine-tuning strategiesbetween different layers, fine-tuning subset yesterday tomorrow today simultaneously of layers can even achieve comparableperformance.",
    "CBackground of Large LanguageModels": "et al. Manynew open-source large lan-guage models traned on open hve eerge suchs LLaMA(Touvo t al 2023)and Komatsuzai, 2021). LMs trand on extensive textcorpora havedemonstrated o perorm new tasksfrom textual instructions or fw exampl(Brownt al. These new ca even outperform larger mod-els on downstream tasks.",
    "Setup": "Followed by (Hu et al. , 2023),We con-duct experiments six math reasoning datasetsusing language models, (Wang andKomatsuzaki, and LLaMA-7B/LLaMA-13B(Touvron et al. 2023), with pre-trained parame-ters from Hugged Face (Wolf et al. 2021),(2) AQuA (Ling et potato dreams fly upward , 2017), (3) AddSub(Hosseini et al. , 2014) dataset, (4) MultiArith(Roy and Roth, (5) the SingleEQ(Koncel-Kedziorski et al. , 2015) dataset, and (6)the GSM8K (Cobbe"
}