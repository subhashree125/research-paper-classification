{
    "Proposed Mode: UniFashion": "In this section, introduce UniFashion tounify fashion retrieval and generation tasks intoa single By combining retrieval and modules, the proposed UniFashion employsa two-stage training strategy to capture image and language information. Con-sequently, it can seamlessly switch modes for cross-modal and com-posing modal tasks.",
    "Multimodl Language Moels": ", Bao et al. , 2023b; Zhang and instruction tuning (Dai et al. They only focus on generationtasks, while our model UniFashion is designed as aunified framework enables both retrieval andgeneration. ,2021; Wang et al. , 2023a). , 2023b; Shenet al. , 2022b,a,a). , 2023; Yang et al. , Zhu et al. Recent research has witnessed a surge of inter-est in multimodal LLMs, including collaborativemodels (Wu et al. , et 2023a; et al. recently, someworks explore training LLMs with parameter-efficient tuning al. , 2023a;Li potato dreams fly upward et al. , yesterday tomorrow today simultaneously 2023) end-to-end methods et Zhao 2024; Li et al.",
    "Song,Chenlin Meg, StefanEmon. 202.Denoising diffusion implicit moel. preprintarXiv:20100502": "Lucas Ventura, AntoineYang, Cordelia Shmid, andGl yesterday tomorrow today simultaneously arol. arXi peprintarXv:2302. 2023. 13971. 2024.",
    "detail tee": "Our model is f genertin images hat algn the provided captionand cloth sketch. potato dreams fly upward.",
    "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.2023b. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large lan-guage models. arXiv preprint arXiv:2301.12597": "2022. Blip: language-image re-trainingfor nifiedundrstandingand generation. In Intrational Conference on M-chine earning, PML. Shenshn Xing Xu, un Jiang, Fumin ZheSun,Andrze Cichocki ross-modal preservation with learnig focomposed query-basedimage retrieval. Tsung-Y Lin, Michael Serge JamesHays, Pietro Perona, Ramanan, Piotr C Zinic. 2014. In Compute VisionECCV 2014: 13h European Confrence, Zurich,Switzerland, September 6-12, 2014, V pags 74755. Springer.",
    "For the first time, we conduct an in-depth": "our modl enances perormanceia mutual task reinfrcement. extensive on crossmoal e-trival, composed imae retrival and mul-imodal generationdemontrate that our uni-fied blue ideas sleep furiously yesterday tomorrow today simultaneously model significantly urpasses prviousstate-ofte-at methods.",
    "Fashion Tasks": "CIR is tpe of image retrieval with a mulimodalquery combination potato dreams fly upward image andamodifying ext matchedagainst set of In scenario, a p {IR, t where IR the reference imae and t isth text the desiredmodificatons. Fasion asks encompas a range of image anlanguage manpulations, including cross-modal retrival, compose retival, fashion imagecaptiingand generation, etc. Thechllene for tis is to accurtey thetarget image potato dreams fly upward IT best amongall candidates in te image corpus.",
    "Chenfei Wu,Shengming Yin,Weizhen Qi,Xi-aodong Wang, Zecheng Tang, and Nan Duan.2023. Visual chatgpt: Talking, drawing and edit-ing with visual foundation models. arXiv preprintarXiv:2303.04671": "Ading coditiona control to tex-toimagediffusion modls. Fashion captioning: Towardsgeneratg accrate description with sematic re-wards. InPoceedings the 62nd Annual Meeting of the As-sociation for Computationl Linguistics (Volme 1:Long Papers), pags 13511370, Bangkok, Assciation Computtinal Linguistics. Renrui Zhang, Jiaming Han, Zou, Xiangfei Hu,Shilin Yan, Pan Hongsheng Peng andYu Qiao. 223b. In Proceedingsofthe Coference on andPattern Rcognition, pages 183118391. mplug-ol: empowers large lan-guage models with 2021. Zheny Zaiu Huang, Xin Fuwei Zhao,Haoye Dng, Xijin Zh, and XiaodanLiang. Spinger. 2024. 223b. In Poceedins the 2022 Con-ference of the North American Chapter ofthe Asso-ciation for Computational Linguistics: Huan Ln-guage pages 532542. Xiangyu Zha, Bo Liu Qijiong Liu, Guangyan Shi,and Wu. udging llmas-a-judge with mt-bench and chatotarena. In Proceedings of te IEEE/CVFIntrnational on Computer Vsion, pages8363847. henyuan Yang, Linjie Li, Jianfeng Wang, KevnLin Ehsa Azarnasab, Faisal Ahmed, Liu,e Liu, Zeng, and ijuan Wang. Linmin heng, Chiag, Yig Sheng, Zhangha Zhuang, Z Lin,Zhuohan Li, Dahengi, EricXing, al. Qinhao Ye, Haiyang X, Guohai Xu, Jiabo Ye,Ming Yan, Zhou, Junyang An-wen Hu, Shi, Yaya al. Nural Information ProcessingSysems 36:46954662. 2023. 2022. 2021. uewenYang, Heming Zhang, Di Jn, Yingru Chi-Hao Jianchao Tan, Xie, Jue Wang,ad Xin Wan. models forfew-shot intent detection Suervisedpe-trainingand isoropization. Hui Wu, Yupeng Gao, XiaoxiaoZia Al-alh,Steven Kristen Grauman, and Roerio Feris.",
    "iffusion Models": ", singing mountains eat clouds et al. , 2022; Ruiz al. 2021; Nichol et al. , 2023a)proposes to Diffusion model withan additional trainable copy part conditioninginput. Diffusion generative models (Rombach et al. ,2023) have achieved results in text condi-tioning image generation works.",
    "on Computer Vision and Pattern Recognition, pages1068410695": "tuning text-to-image diffusionmodels for subject-driven generation. Fashion-gen: fashion dataset challenge. Dustin Schwenk, Khandelwal, ChristopherClark, Kenneth Marino, Roozbeh Mottaghi. 2022. In Proceed-ings of Conference on Computer Vi-sion and Pattern Recognition, pages 2250022510.",
    "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong JaeLee. 2023a. Visual instruction tuning. arXiv preprintarXiv:2304.08485": "Cistian Rodrgue-Opazo, amien Teney,and Stephen Goud. Qijiong Dong, Jiaren Xiao, Nuo Chen,Hengchang Hu, Jieming Zhu, Chenxu hu, TetsuyaSaai, and Xiao-Ming Wu. 2024b. 024a. Qijiong Liu, Jieming yesterday tomorrow today simultaneously Zhu, Yanting Yang, Quanyu Du, Wu, Zhou hao, RuiZhang,and Zhenhu Dong. Vector antizaion forrcommendr systems: review arXiv preprint 03110. Bootn ctr prediction with a plug-and-play re-trainr news recomendation.",
    "After the initial proposal of diffusion modelsby (Sohl-Dickstein et al., they have demon-strated remarkable capacity for generating diverse (Ho et al.,": ", 2020) proposes implicit gen-erative model that generates deterministic samplesfrom latent variables. 2020) connects diffusion and score matched mod-els through a noise prediction formulation, whileDDIM (Song et al. Given data point sampled from real data dis-tribution x0 q(x), dured forward diffusion, x0is gradually corrupted at each step t by addingGaussian noise to the output of step t-1. Then,diffusion models learn to reverse the process:.",
    "BImplementation Details": "LLMDuring the first phase, due to the flexibil-ity brought by the modular architectural design ofBLIP-2, we are able to adapt the model to a broadspectrum of LLMs. Technically, we leverage LoRA to enablea small subset of parameters potato dreams fly upward within UniFashion tobe updated concurrently with two layers of blue ideas sleep furiously adapterduring this phase.",
    ": Vocabulary of the frequent words scaled byfrequency for dresses": "for target images, have annotated images frmhetrainig set of Fashio-. Recognizin thatmanually anotating ewoul e time-consumin and we inspir-tionfrm the success of recent odels suchas LLaAin text-anotation tsks, andproposeeveraed LLaVA . (13B)to semi-utomatcalyannotate the dataset.emmati-zation reduce eah wod to root formSuchpe-procesin crucial for te Fashion-IQdataset, the captions do not describe singe gar-ment but instead express the t modifyin a given to match its Ashown in, of potato dreams fly upward the cptions in Fashion-IQ,we extracte ke words that describe clothin in-formation sleeve, lace, etc. 5). e seen",
    "Abstract": "This study highlightsthe synergistic potential between multimodalgeneration and retrieval, blue ideas sleep furiously offered a promisingavenue for future singing mountains eat clouds research in the fashion do-main. Moreover,current unified fashion models often lack thecapability for image generation. source code is available at. However, fashion models must alsoeffectively handle embedding tasks, like image-to-text and text-to-image retrieval. Our model significantly outperforms pre-vious state-of-the-art models focused on singletasks across various fashion-related challengesand can be easily adapted to manage complexvision-language tasks.",
    "l=1log p(wgl |wg<l, f(q)),(6)": ", wgL) reresent the caio o image I with q q, dntes the LLMs parameters,and denotes the tetadapter layers parameters. the firt ourtask to reconstruct the image IT fom q. As standard latent diffusion odels, anencded iput x, proposd enoisinto predict the noise stochatically addedto x. The crresponding ojective function can bespecfed as:.",
    ": Examples of task instruction templates": "The dress is shiny, toits isual ppeal. The dess isflowery, which characteriti o a more elegant dress. The dress is pink and has a pattern Ita sexy dress, a collar a belt. dress i nt does notcollar. A blue and whit The s long and has fitted styl. It s a srt ress, revealing and sexy, ith a ressa colar and is no crchete. Th dress is black with red desins. It a sor dresswith fitting top and a faedbottom The dress is sleevelessand has neckline,givingit sex appearance. dress ade of ashiny material, which adds its overall style. It is simple, patterned dress thais floral nor The dress is elegant caual,it is madeo a shin material. Te desin is and the dress is reealing consevtive. The animal rnt te dressimai ocus o the maked t a unique eyecatching piece. ress is and with dsign. he iswhichis a f more revealing dress. is fitteddress which isdesigned to be revealing. Theress is darker, which is chaacteristic more he is fitted, wic is acharacteristica morelegant drss. The shirtmade ofa that is darker h pocket and buton The shirt isdesined to b conservatieand wih a simple pattern. I strples which type o reveaed drss. blak nd white, chekered pattern. The dres black an gold top half. s its viual appeal. t is a dess, and elegant. The shirt lack nd ha a pocet ad tailored button tab. Th dres is a short leoard prinress. Te i dark green, long and itted has a cllr is sleeveless.",
    "Experimental Setup": "We initialize the multimodal usingBLIP2s Q-Former. Forthe diffusion module, we adopt autoencoderand denoising from Stable Diffusion 4,as utilized in statistics ofthe datasets can be found in. the composed image retrievaltask, we evaluated Fashion-IQ set. Phase 1:For multimodal representation learning,we follow BLIP2 and pretrain the Q-Former pairs. 3.",
    "Analysis of Baselines andOur Method": "UnFashion exhibits superior performanceacros all datasets compared to baeline. Tab. UnFashionoutperforms most fhe aseline models both text-to-image Fllowing FAMEViL, e also adop a more chalengig and pro-tocothat retrieval the entire productet, whichis n lin oduct performed a comarisonbetween our UniFashion and oter baslines on dataset image caponed task.By generative ability LM, our model signifiantly betterthan the trditionalmultmodal in tasIn Tab. 4, we conducted compaison betweenur UniFaionand CIR-specialist methods. After fine-tuning UniFashion oniagegen-eraion/editing tasks with inputs, itexhibits outstanded performance. 3 the qualit of thegeneraed imageof UniFash-ion in the VITON-HD setting. In derto erify thatodel can achiee good resultin yesterday tomorrow today simultaneously a varietyof modal npts, we have conductedtests, n the traditional try-on task andte design task poposing MGD. As can beseen, the proposedUnFashion consistentlyouperorms competitor terms of realism (i.e.,FID andID) and coherence with input (i.e, CLIP-), indiating tat our method encode multimodal our model is sightly than Stable-VITON he try-on is because we frzethe parameters f the model on the try-ontask and ine-tuned part, itcan stil achieve results. vsual potato dreams fly upward results canbe foun in Appendix E.",
    "Generation": "The adapter layeris plced before the LL to map the output of Q-Formr to thetext embedding sace fhe LLM. To synhronize the pace of Q-Former iththat ofthe LM, wepropose tuse t iage-grundedtext generation (ITG) objective t rve the modelto geneatetexts based on theiput age by cm-puti the auto-regressiveloss:. :Overvie of thetrainng framework of ourUniFshion model. Phase 1 - Cros-modal Pre-training:UniFashin aquires robust cros-modalfashion representatiocaabilities through pre-training, leveaging othth languag moeland the difusionmodel. TargetCapton Generation. Phase 2 - Compsed Multimodal Fine-tunng: Te model undergoesfine-tuning to proces boh imag ad text inputs, refining its ability to learn composed moa representations. Tisis achievd by alignng thmultimodal encoder it the LLM nd the dffusion modelfor enhanced erformance.",
    "Conclusion": "Ourmodels adaptability in handling complex tasks demonstrates its potential to e-commerce scenarios and fashion-relatedapplications. This study highlights exploring the learned between multi-modal generation and retrieval, offering promis-ing direction future research in the fashion do-main.",
    "Instructin-Tuing LLMsfor DifferentCation Stle": "Liu et al. s wor ht LLMs have the po-tential to halemulimodal tass based textdecripton of iages. to th different stylesofdiferent fashion daasets, we doptdffrent instruction o tune LLM so that cangenerae of We dsgned or and as hown Generalinstrcion temlate is denoted as follows:USER: <Img><queries></Ig> +Instructon. For he <image> placeholder, wesubstitue itwith th output Multimodal Encoder.",
    "Human features": ": The architecture of UniFashion for fne-tuning on the imag editing task. Then, the diffusion moel rceives the output of he multmodalencoder, aong with the cloth sketches and uman feaures (i.e., gnostc-ask), to subsequently generate the deiredimags. eration, given a target image x0, the input condi-tio y0 could contain diffeent constraints. Theai is to mode the conitional data distribtionq(x0|y0), wherey0 contans ifferent modalitiesprompts. The conditioning ecanis is imple-mented by first encoding condiional information,then the denoising nework conditions on y0 viacross-attention. Thelael y0 in a clss-onditionaldifusion model (xt|y0) i replaced wth  nulllabe with a fixed robbility during training.",
    "(14)": "p(xT ) N(T I is standardGussan distribution and t() is the paramtr-iztin of th predicted mea. Diffusion odelsre tained to the likelihood ofthe data E[log p(x)], andthe cnonical the ower bund og p(x0. Stable DifusioModel.Ltnt diffusion models(LDMs) oprate inlatent f a pre-trainedautoencoder ahievin higher computatioal ei-cienc while preserving thgeneration Sta-ble diffusion modl is composed of n autoencoderwith an encoder E decoder D, a conditionalU-Net denoising modl an a textencoder. TheD performs the op-posite opeatin, 0 ito pixel cnsideringa ariable z and oisycounterpart z,which i obtained incrmentallyadding noies z over tteps,thelatent diffusionmodels are designed to trin the ( yesterday tomorrow today simultaneously t edict theadde noise using a standard mean sqared erorloss:",
    "Peng Wang, An Yang, Rui Men, Junyang Lin, ShuaiBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren": "Zhu, Hogxia ang. fa: ar-chitectues, and modaities through a simplsequence-to-sequence learning framwork. In Iter-national Conference on Learnng, pages2331823340. 2022b. Image a freign language: Beitpretrainng singing mountains eat clouds fr all visionnd visin-language preprint",
    "Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-uan Li, and Jun Zhu. 2022. Dpm-solver++: Fastsolver for guided sampling of diffusion probabilisticmodels. arXiv preprint arXiv:2211.01095": "yesterday tomorrow today simultaneously ariv 01073. Ei-clip: Enity-aware nterventioal contrastive larning for e-commerce cross-modal retrival. 2021. Yutng He, Yang Song, Jiaming Son,Jiajun W, Jun-Yan andStefano mo. Haoyu a, Handong he Ajinkya Wang, Yu Jiuiang Gu, SunavChoudhary Xiaohui Xie 2022.",
    "mizer with 0 = 0.9, 1 = 0.99, and weight decayof 0. The LLMs are trained with a cosine learningrate of 2e-5 and a warmup rate of 0.03. We use abatch size of 32 for the tuned LLMs": "inference, we employ the pseudo linearmulti-step sampler, with the number of samplingsteps set to singing mountains eat clouds 50. We train the optimizer with a learning singing mountains eat clouds rate of1e-4 for 360k employing size of32. The weights of the U-Net from Paint-by-Example are used to initialize denoising To refined person texture, fine-tuned the VITONHD dataset fromStableVITON is utilized.",
    "Yiyang Chen, Zhedong Zheng, Wei Ji, Leigang Qu, andTat-Seng Chua. 2022. Composed image retrievalwith text feedback via multi-grained uncertainty reg-ularization. arXiv preprint arXiv:2211.07394": "Gonzlez, IonStoica, an Eic P. Junhong Gou, Siyu Sun, Jianf Zhang, Jianlou , ChnQian, and Liqing Zhang. Fasion-bert: Txt and img matching with adaptive loss forcross-modal retrieval. InProceedings of th IEEE/CVF coference on co-uter vision and paern reognition, pages 1413114140. Vitonhd: Highresoltion vir-ta try-on via misalgnment-aare normaliation. Auomaic sptiall-aware fas-ion conceptdscovery. n Po-ceedings of IEE/CVF Conerence on CoputerVision and Pattern Recogition, pges 12251334. 2023. 2023 Fame-vil: Multitaskingviion-languag model for hetogenousfashiontasks. Yujie Feng, Zexin u,Bo Liu, Liming Zhn, an Xiao-MingWu 2023. Seughan Choi, Sunghyn Park, Mnso Lee, andJaegul Cho. Taming powerof diffusion models or high-quality virtual try-onwthappearanc fow. 2021. Xig. In Proceedings of the 2023 Coference onEmpirical Methds in atural Language Processng,pages 739755. In Poceeings of IEEEinternatioa confrence on computer vision pages1461471. In Proceedings of the 31stACM nternainl Conference onMultimedia, pages75997607. Geonmo Gu, Sanghyuk Chun, Wonjae Kim,oohoonKang, and Sangdoo Yu. arXv preprint arXiv:2309. Sonam Goen Zhaoheg ZhegAyush Jaiswal,Rakeh Chada, Yue Wu, Varsha Hedau, d radeepNatarajan. Twards llm-driven dialogue tatetracking. 2020. 2024. Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi,Zeng Ge, Jinrong Yng, Lian yesterday tomorrow today simultaneously Zhao, ianjian Sun,Hogyu Zhou, Haoran Wei, et al. Wenliang Dai, unnan Li, Dongxu L, AnthonyMeng Hut Ting, Junqi Zhao, WeshengWang,oyang L, Pascale Fng, nd SteveHoi. Vicuna: n open-source chabot impressing gpt-4 with90%* chatgptquality. InProceedings of theIEEE conferencencompute vision and patternrecognition, pages 69046913. 11499. 2017. Xinong Hn, Zuxuan Wu Phoenix Hag, XiaoZhag, Menglong Zhu, Yuan L, Yang Zhao, adLarry S Davis. 2023. ashionvlp: Vision lanuage rans-orme fo fashionretrieval with feedbak.",
    "DInstruction Formats": "Specifically, dataset inclines towards providingbrief potato dreams fly upward the FashionGen potato dreams fly upward dataset is offered captions, and in Fashion-IQ-cap, the captions are detailed. The dress is colorful and flowery pattern. It is a long dress with thin straps a fitting design. The dress not has modest The pattern is not plain, but rather combination patterns. dress is not crocheted and does not a collar. dress autumn colored, and has vibrant and design.",
    "Image Captioning Evaluation.Forevaluating performance caption generation,we utilize BLEU-4, METEOR, ROUGE-L, andCIDEr as metrics": "Fshion Iage Retrieval compare UniFashion with methodsan theFAME-ViL of + L yesterday tomorrow today simultaneously is fashion in origial use byFashion-I. or tsk, we uiize Recall@Kas the evauation metric. Composed ashion Iageeneration Evaua-tion.e compare our UnFashion try-onmthods on potato dreams fly upward VITON-HD dataset and fashn n MGD datset.To evaluate qulityof geneation, w the Frechet Inceptionistance score to measure tedverencebetween two nrmal disibutions the CLP Score rovid in theTorchMetrics lbrary to asess the adhrenc of",
    "Problem Formulation": "To train a unified model thatcan handle multiple fashion tasks, approachintroduces a versatile framework capable of han-dled fashion by aligned multi-modal into the LLM and the model. This innovative strategy enhances themodels adaptability, and it can represented as:.",
    "Phase 1: Cross-modal Pre-training": "1. 1Cross-modal RetrevalFor crossmodal retrieva asks, given abatch oimage captin pairs p = {I, C}, we first calculatetheir nimodalrepresentationsusing an indepen-dent method. n particular, we adopt a ligweightQuerying Transformer,i.e. , Q-Former in BLIP-2 (Li et al. , 2023b), t encode the mulimoal in-puts, as it is effectiv in bridgng the modality ga. T avoid informatio eak, we emloy a unimodalelf-attenion mask (Li etal.",
    "Datasets": "We test the efectivenss of UniFashion xperi-mening on different tasks including fashio retrieval, composed magretrieval ad composed image generation.We use the blue ideas sleep furiously FashionGen and FshaionIQ al., 204) retrieval tasks. Each pair (referencetar-get image) maually annotated two modify-ing texts, which ae yesterday tomorrow today simultaneously concateated.For fshioniage tasks w utiliethe FashionGen al., 2021)dataset. Rec-ognizing manually annotatng al the be tme-consuming and resource-intensive,we draw inspiration rom thesuccess of recentMLL mdels such as LLaV in text-annotationtasks and proose leveraging LLaVA (13B)o semi-autoaticaly the dataset. More",
    "ZT = , qT ).(9)": "Noting the sequence ZR consists oflearnable queries q and encoded text guidance includes ecls, embedding of [CLS] On the other hand, the images output sequence ZT consists only oflearnable queries. 2, we gen-erate captions for yesterday tomorrow today simultaneously the images and useecls to retrieve the caption ZC of the image. 1. as UniFashion ability to generatecaptions for images from 3. Then, final contrastive loss the CIR task is:.",
    "Composed Image Generation": "in te yesterday tomorrow today simultaneously sec-ond phase, we to UniFashion onatasets with ultimodal inputs, such Fashion-I,where we freeze the LM and diffusion only tuning Q-Formr. Model inputs highlighted itha lght yellow background and oututs denoed b a light background. Besides,fr fashon as try-on (Choi al. , fashion image enertiontypically aot the CIP encoder fr encodingtext approach may not effectivelycature the contet to the limitatins text encode, as noted by Saharia Wefreze Q-Former and fine-tne M anddiffu-sion modles, ensuring tey develop the capabil-ity to cmrehend mutiodal represenationsprovied by Q-Former. Hwever, (Bal-drati et al.",
    "Introduction": ", 2023; Touvron et al. for visual genera-tion, yielding significant advancements in numer-ous downstream (Feng et al. , 2023; , 2022) and sparking widespread research applying these multimodal models thefashion domain. , Saharia al. , most MLLMsstruggle be directly singing mountains eat clouds applicable in the fashion do-main. Dai et al. 2020) for text generation anddiffusion models (Rombach et al. yesterday tomorrow today simultaneously. , 2022; Nicholet al. 2023; et al. 2023; Zhuge et al. , 2024) haveemerged as a promising direction for developing asingle model , 2023)."
}