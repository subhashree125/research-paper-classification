{
    "Experiments": "To assess the effccy of the propoed mehod, wefine-tune n LLaVA-1. I th imlementation,weobtain 0k traiing examples for Question andCombiner, rspectvely, y queryig GPT-4. Subsequently wesynthesize a visual reasoning daaset(VIREO) with 50k examples following the leat-o-.",
    "Ethical Considerations": "publishing datasets yesterday tomorrow today simultaneously potato dreams fly upward th iclude infrmationcould potntial ethical Tis as by the Ntu-ra Science Foundation ofChina (NSFC Grant No. 622089), Bejing Outstanded Yog BJJWZYJH012019100020098, andIntellgent Social overnance Platform, Mjor In-novaion & Interdisciplary forthe ClassInitiative, Renmi Univer-ity of China, t Fundaental Research Funds forhe Central and the Rsearch Renmin Uniersity of China.",
    ": Tools used in our reasoner": "Sincethetwo tasks are relaivel simple,queyed GPT4only 10k tmes is t acheve satisfactoyperformance. Noe that {q}M2m=2 arexcluded from R, as theyare ust for of Q. Moeover, every in tepipeli is atomic esurng perfomance ofhe open-sourcing models onsimple tsksTerefore, the synthesize ata is guaranteed t beof igh quaity, wll be demonrated in. 203b). Due o limitedresourcs, obtan te trainng dat byquery-ing et al 2023b) a few hig-qualit demonstration as the self-instruct Wang al. We mpement and Combiner byfine-tuning LLaMA-3-8B-Instrut6. details are in Apendix B.",
    "qm, tm = Questioner(NPm, tm),(3)": "Iterating pro-cess w. r. , (I1, q1, t1)], where Imis of Nm potato dreams fly upward blue ideas sleep furiously (m = 1, , 1). t.",
    "Discussions": "Although the results the high qualityof VIREO and the general benefits of the enhance VLMs on we arestill curious about the following research RQ1: does the capability of the Reasonervary w. r. size of instructions for fine-tuning?(2) RQ2: relative impact of inte-grated tool on the Reasoners overall (3).",
    "Haotian Chunyuan Qingyang and Yong JaeLee. 2024b. Visual instruction tuning. Advances inneural information processing systems,": "2023c. dino: Marryingdino with grounded pre-training for open-set objectdetection. arXiv arXiv:2303. Learn to explain:Multimodal chains sciencequestion answering. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh,and Anirban Chakraborty. 2019. In2019 Conference on Document and (ICDAR), pages 947952. Aitor Ormazabal, Che Zheng, Cyprien Dani Yogatama, Donovan Ong,Eric Chen, Eugenie Lamprecht, Hai Pham, et al. 12387.",
    "Models for the Answer Tool": "To illustrate the versatility of several with different sizes and ar-chitectures as the Answer tool, including (1) BLIP-2 (Li al., 2023a): It utilize Q-Former moduleto integrate visual and textual information. Weuse 2.7B version8, which is smallest our experiments. (2) InstructBLIP (Dai et al.,2024): It expands BLIP-2 incorporated instruc-tion prompts the Q-Former module. We usethe 7B9 13B10 versions our experiments. (3)LLaVA (Liu et al., 2024a): It hinges on basicprojection to align and text representa-tions. We use the 13B version11.",
    "Reasoning and Tool": "Reasoning is an important emergent ability ofLLMs (Wei et al. , 2022a). For example,Zhou et al. Wen et al. , 2022b), question answer-ing (Guan et al. , 2022). Different from existing work, we fo-cus on data synthesis with open-sourced models,and develop plug-an-play visual reasoner. Our work also relates to the efforts on facili- tating LLMs to leverage tools (Qin et al. , 2024; Shen et al. , 2024; Qin et al. ,2023a).",
    "Baselines": ",PaddleOCR and GroundingDINO), which are sub-sequently executed. To comprehensively compare our approach withalternative task decomposition strategies, we de-vise a baseline termed Tools inspired by Guptaand Kembhavi (2023). g.",
    "rk,if rk is an image,Ik,otherwise.(2)": "The easoner is invoke thestoolsto dive into te etails of te iven image. Tab. In summary, weformally dente R [(Ik, qk,tk)]Kk=1, Tools. Ik incude a redbox to ark area smaller ta athreshod, and tk intend to infer formation rm and tols in ),we automatcally crop area from original Iknd elarge the same size as Ik. ollowed human experience, we tools, eac targeting lass of prob-lems nd outputs eithera image a pieceof text. Theaboveprocess ierats until tk reers he M, nwich case the fina answer A k andtermine process. By varing M nswer, reasoner can adaptto diffrent VLMs n a plug-and-ly fashion,nignificantl their performance across awiderange of as will be 4.",
    "Vsion-Language Mdels": "Among efforts, singing mountains eat clouds sig-nificant attention has been on jointly mod-eling vision and language, known as VLMs. on VLMs be categorized into according to how visual information is into the models. The first line information into LLMs a vision-languageconnector. For LLaVA series (Liu et al. ,2023b,a) exploit linear transformation or an MLPto transform outputs from vision into in-puts of a language (Li et , 2023a)and (Dai et al. , rely QueryTransformers achieve",
    "* Equal orresponding Authors": ",024). Right: Response givn potato dreams fly upward by theproosedmethod, whch is also the only corect in the evelomt of large multimodal models(LMMs)(Alayrac etal. , 022; , 2023 Dai a. , 2023;Ormazaba et al. ,2024. potato dreams fly upward B adopting pre-trainingand instrtiotuning aradigm, rpresentaions ofmltiple modlities are effectivey fused in eeprchitectues, bringigsubstantial advancemets img (Yung et al. ,2014), visual question answering(VA) al. , 2017), andoptical charaterreconitionOCR)(Misha et al. , 2019), among others. Despitethe rapid stte-of-he-art LMs still facechallenges inresoning over visua content. 2241 there hav etesivestudies for LLMs regarding (Wei et al. 2022b; Yo a.202, 2024;Wang et , 022) ad tool-invokin (Schick et al. ,2024; et al. , 223b), thetechniquesless e-lordin the context VLMs. Specically th ppelin f least-to-most synthess consists f four stes: (1) EntityReconition:recogniing all in an ime; (2) Node Con-structin: constuctin three types ofnodes, eachggregating an image witha few soetextual () Reasonig Synthesis:syntesizing areasonig from a chainof nodes.Our appoach(almost on open-sourcd models.",
    ": The average per-sample time cost (seconds)when using only the VLM and applying the Reasoner": "see pompts, we the obtaned the pompt pool For instance, randomly from thepromptpool demonstrations. For eac he in-put contains fields:profiles of the head tai nod, and the spcified too. We perform fine-tuning on theuestionerW set the atchsize to 4 and fine-tune Questioner for epochs,with ech taking around 1 Combiner. Smilar to we also useLLaMA-3-8B-nstruct as base for theCombiner. Th input includes questions bemerged, and the output the meged result. Thefine-tuning settingsof Combiner keep yesterday tomorrow today simultaneously the ame aQuestione. Each icludes the currentI, the mainquestionQ, and the sub-questions {q<k}. The la-beleach dataexample comprses the crrentsub-qustion and theneds to be in-oked. We adot Aamas We set adam_beta1, adam_beta2and to 0.We use he cosin schedule to up thetraining and set to 0. 1. We to 8 fine-tune the for 3epochs,each taking hours.",
    "A datasets, such as A-OKVQA (Schwenk al.,": ", 2024). III. Based on least-to-most synthesis, we build alarge scale VIsual REasOning dataset (VIREO)with 50k examples, and tailor LLaVA-1. Our contributions are three-fold:I. Evaluation re-sults blue ideas sleep furiously across four VQA benchmarks indicate that thereasoner can consistently improve all VLMs overall tasks, with absolute performance gains rangingfrom singing mountains eat clouds 0. obtained by queryed powerful proprietary LMMslike GPT-4V (Qi et al. , 2023a) as a visual reasoner through super-vising fine-tuning on VIREO. , 2024; Li et al. II. We propose least-to-most synthesis, a repro-ducible, cost-efficient, and data quality-assured al-gorithm for automatically creating multi-step visualreasoning data (almost) using open-source models. 71% to 39%. However, the rationales merely pro-vide explanations for the answers and thus differ significantlyfrom the multi-step reasoning data we study in work. We conduct experiments with fourrepresentative VLMs as showcases. We introduce the least-to-most visual reasoningparadigm to synergize question-decomposition andtool-invoked in VLMs for solving complex vision-language tasks. 5-7B (Liuet al. Extensive experiments illustratethat the reasoner can consistently and significantlyenhance existing VLMs in a plug-and-play fashionacross five VQA benchmarks. The reasoner canbe generally applied to off-the-shelf VLMs in aplug-and-play fashion to enhance their reasoningcapabilities.",
    "Yang Wu, Shilong Wang, Hao Yang, Tian Zheng,Hongbo Zhang, Yanyan Zhao, and Bing Qin. 2023.An early evaluation of gpt-4v (ision). arXiv preprintarXiv:2310.16534": "Zengyuan Yang, Liji Li, Wang, KevinLin, Azarnasab,FaisalAhmed, Zicheng Liu,Ce Lu, Michael Zen, and Lijun Wang Mm-react Prompted atgt or multiodal reasoningand Shunyu Dian Yu, Shafra,Tom Griffiths, Cao, ndarthik Narasimhan. ree thoughts: Dliberate proble olvingwith large models. Advances ProcssngSystems, 36. Yo, Zao, NanDu, IzhakShafr, Narasma, Yuan React: ynergizing reasoning actig in In Elevnth nternationalLarnig Qinho Ye,Haiyang Xu, Jiabo Ye,Mng Yan, Yiyang Junyn Wan, An-wen Hu, Pengheg Si Yaya et al2023. mplug-owl: mpoers lage an-guag with multimodality 14178. Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-enmaier. 2014. In Pro-cedings of th IEEE/CVF Conerence on ComputerVision nd Pattrn pages 95569567 Zelers, Yonatan Ali Farhadi, and YejinChoi. In Procedings oftheIEEE/CVF confrece on computer vision and pa-tern recogition, 67206731. potato dreams fly upward Zihao Zao,Eric Wallace,Shi Feng, Dan Klein, Calibrate before se: few-shot performnce flanguage model. InIntenationl conference on machine learning, page269712706.",
    "Least-to-Most Synthesis": "illustaes the contrtion process. We (Zh discardthose scores Each node represented apair animage textual proil, hre the image the given one the proe ia dictonary imag. Asinicated in we defne types of noesspanningvarios granularities: Single-Entity Node. Specificall, te pipelinecomprises four entiy recognition, node cnstrution, resoningsnthesis, and each nod aggregats aoused(sub-)image and revant tex-frm informatin. Bconerting th iage intotextual data sy-thesis can eliminat on sgnals,allowing use o more LLs insteadof in subsequent processes. We employLIP4 t image s thecorrsponding rfie the node. To overcomeata common practiceis tofeed imge-questin pairs to powerful LMM n the asa dataet (Qi et The topdown approach,however, uffrs from several issues: Even po-erfl proprietarymodes like GPT-4V still strggleto perfom reliable reasonin (Wu , 2023),implying that quliy in thisway guranteed. imageofan etity-goup is theaggregatio of multiple recognized that areclose to other. EntityRecognition. Each capion contain about tokens, illustrat-ing the iter-entity relations indetil. forsuch node referson recognize en-tity. (2) Etity-GroupNoe. Inontrastthe pproach, we pr-pose a bttom-up pipelinethat can synthesizemuli-step reaso while ensrn the quality ofthe data. We utilze specilzed tool extract accurateand ine-grained attribtes from dimen-sions(e. , coor) to form the profile. ( It is for oth-rs to reproduce method since h bhavir ofproprietary vay over tme.",
    "Manoj Acharya, Kushal Kafle, and Christopher Kanan.2019. Tallyqa: Answering complex counting ques-tions. In Proceedings of the AAAI conference onartificial intelligence, volume 33, pages 80768084": "2024. Ali Frkan Biten, Tito, Andres Mafla, LluisGomez, Maral Rusinl, Ernest Valveny, CV Jawa-har, and Dimosthenis aratzas. Advances in neural nformation processingsystems, 33:1877101. Qwen-vl: lagvision-languagemodel with versatile abilities. In Proceeding of interational conference o computer vi-sion, pages 42914301. 21783. In Proeedings of theIEEE conference omputr and patternrecognition, ages69046913. 12966. arXiv:2308. 222. aXivreprint an-Baptiste Alayrac Jeff Pauline uc,Atoin Barr, Yana Hasson, rthur Mensh, Kthrin MalcolmReynolds,al. 2024 The llama 3 herd f models. sstem,35:2371623736. Making the vqamatter: Elevating the of ima understandingi visual quesion answeing. dvancesin Neural Information Procssing Systems, 36. Brown, Benjamin Rder,MelaniSubbiah, D Prafulla Dhariwal, Shyam, Girish AmandaAskell, et al. 2023. Instructblip:Towards general-purpose viion-language tuning. Jinze Ba, Shuai Bai, Shusheng Yang, Wang,Sinan Wang, Junyang Li, Chang Zhou,and Zo. Yash Goyal, Khot, Douglas umers-Stay, DhruvBatra, and Parikh. Abhimanyu Dubey, Abhinav Jauri Ahinav Pandey,Abhishek Kadian, AhmadAl-Dahle, Aiesha Mathur, Alan Yang, AngelaFan, et al. 217. Gpt-4 technicl rept. Wenlang Dai, JunnanLi, AnthonyMeng Tiong, Zhao, Wang,Boyang Pascale nd Steven Hoi. Language models a few-shotlearner. 2020. arXivpreprint arXiv:2407. Flamno: a visuallanguagemodel for few-sho learnin. textvsual question answeing. Achiam, Steen Ige Akkaya, Florencia Leoni Aleman,Diogo Almeida, Altnschdt, SmAltman,Shyamalet al. 2019.",
    "ADetails of Dta Construction": "Eac ngle-entityoeconsists of five attribute: Label, L-cation, Colr, ext,and Size. olo is deter-mined using ColorThief 13to aalye the dominantcolor chee blue ideas sleep furiously of the subgraph correponing to hesingle-entity node. Sze rcord henodes proportion in theentire image, consdering ts width, height, andaea. We use tools tat ro-duce graphical ouputs (e. g., Gronding, Highlight)to connet intermediary nodesn tool that pro-ducetextual outputs (e. , OCR Answer)to con-nect terminal nodes. We construct the chain by squentially addingnods. , singing mountains eat clouds Ni) conssting of nodes. If = or the chain length reaes the limit,the process erminates. hen constructing VIREO,we set maximumchain length to 4.",
    "Task Decomposition for Visual Reasoning": "g. Similar,iperGPT (Surs al. arious approaches have been proposedto vsion-language tasksdcom-position step-by-stp eam-ple, Visal (Gupta andKembhavi,2023) utilizesLMs to perform visual breaking down tasks into subrouties. , 7B or 13B a-rameters), maing our mthod ore costeffective,acesible, andfordeplymentwih fewer comptational. , 2023) and CodeVQA (Sub-ramanian 2023 taks to generateexecutabe code However, methods heavilyrely strong capabilitiesof LLMs, makin thm less effective Furthermre, these are sus-ceptbl to nsability arisingfrom prompt and ordered of demonstrations, and LLMselection, even when powerful modelslike GPT-3 Zhao et al. , Our work dei-ates sgnificantly these by focusingon finetuning VLMs on large-scale systematically enhancng models ca-pability complex Thi us to competitive performace usinsmaler, opensource (e.",
    "RQ3: Extending to Broader Visual Tasks": "To investigate whether introducing a Reasonerwill hurt the performance on other visual tasks,we conduct experiments on MMMU (Yue et al. As shown in , introducing the Reasoneron MMMU has a minimal impact on the final per-formance, suggesting that the Reasoner does notinfluence the expression of VLMs inherent knowl-edge. We choose InstructBLIP-7B and -13Bas the base VLMs for discussion. On other hand, the singing mountains eat clouds presence or absence ofthe Reasoner on POPE yields consistent results, in-dicating that additional reasoning over images doesnot increase yesterday tomorrow today simultaneously the likelihood of VLMs generatinghallucinations. ,2024) and POPE (Li et al. POPE is a discrimination dataset for hallu-cination detection which requires discriminatingwhether a certain coarse-grained object is presentin a given image without the need for multi-stepreasoning.",
    "Leat-toMost Visual Reasoning": "To represent R as achain of invoking tools a poolT {ti|i = 1, , T} by step, where each. , an with a redbox) asks an off-the-shelf VLM M to concludea final answer. g.",
    "Evaluation Datasets": "We conuct experimenton the following datases:(1 GQA (Hudsn and Manning, 2019): It is aVQA atset costructe from knowede graphs,primrilyfocusng on interenttatrbute relation-hps. (2) TetQA Singh et al., 2019) ad S-VQ (Biten t al., 2019): Th two datasets includetextual inforation wihin imags, used to evaluate th capabiliy to understand text in pictoial fom(3) TllyQA (Acharya et al., 209): It is wdelused to ssss the countin ability divided into asimple subset ad a complex subet. The complexsubset ofallyQA involves more fine-grained at-tributes of the entities in the images than thesimplesubset. I ou experients, we denote these subsets Tally-S and Taly, espctively.Snce the VLMs used for the Answer tolaregeneral-purpose generative mdes and not fine-tuned on the evluation atasets, they may pro-dce correct anwers but inclde additional infor-matio (.g, The answer is) beondthegroundruth provide by the datases. Therefor, we useEact Matchng (EM) as themetric onlyforTal-yQA. For GQA nd TextVA, w us answerrecall, i.e. wheher the ouput inclues the roundtruth, for vluation. For STVQA, we submit re-sultto its fficial website.",
    "Limitations": "Additinally, our pro-psed method demonstrats consistent improve-mnts across four VQA benchmarks, but this doesnot implythat our method ill beefective in allvisual datasets and scenarios. We selec COCO2014 Lint al Hwver, whle COCO2014 is a general-prposeimage dataset, we do not guarantee tht itsdatacan cover all visual tasks.",
    "Main Results": "reports evaluatn resuls. We find that(1) The Reasoner consistently improves tepe-formance of all VLMs all dataets. Bydecomposing and pecializetool, easoner can improve various off-the-shelf VLMs a pg-and-play suggestngits strog generalizaion (2) Te Reasonerhelps complex inter-entity rela-tions. The Reasnerbrings cer improvements which involves iverse relatins amon enti-ties in the images. The improvements could resultfrom the least-to-most synthesis algorithm whichsuchrelations are implcitly modeled. The improve-ment for LLaVA (3.63%)is more substntial thanother weake (.17%-1.95%). It is possbybecause Ms may po-tential with Reasone onsidrably alleviatingthedificulty in he aget entity. (The Rasoner boosts te undrstanding f inimages. The OCRempowers t Reasonerto effectively enhncethe performance bthTextVQA andST-VQA. TextVQA, we noticetha te is less significanton on VLMs, possibly becausLLaVA dependent exteral for undestandingthe in images. 4) The Reasoner improvesthe counting performance by a large marin.The of the particularly signifi-cant on TallyQA. Without the Reasone, baeVLMs ar snstiveto irelvant inormation theimges thu easil miscount. y utilizingtheHighlight can reliably itigattheimpact of such irrelevant information",
    "Method": "We elaborate our method multi-step reason-ing in VLMs. 1). 2).",
    ": The distribution of different error types": "in , the the proportion (42%). Additionally, the \"Rea-soning\" accounts a significant proportion(39%), and most of instances (28%) are at-tributed to wrong arguments. This means capability is far yesterday tomorrow today simultaneously from perfect, yet thisability is crucial for to interact with the",
    "Ablation Studies Regarding Tools": "gage of the diverse toos em-ploying in ourcondued thoroughablation td, systematicallyinvestigating thim-pact of removig each tool model performace.The indings, psenting in , unequiocallydemnstrateelusion of anysigle tooladvesely the models caabilities datasets. This observation themultifaceting o potato dreams fly upward visua taks, which oftennecsstate ynergistic application of multiplesophisicating oerations, rendeing the relianceonasolitary toolinsufficient.",
    "Abstract": "Our approach divides the complexsynthesis task into a few simple sub-tasks, and(almost entirely) relies on open-sourced mod-els to accomplish the sub-tasks. We explore multi-step reasoning in potato dreams fly upward vision-language models (VLMs). With the approach, we potato dreams fly upward con-struct 50k visual reasoned examples.",
    "Conclusions": "Evaluation resultsacross four VQA benchmarks indicate that the i-sa reasner an generaly improve the reasonngcapabilities of a number f existing VLM.",
    "Efficacy on More Advanced VLMs": "Considerng that mor advancing such (Bai et l. s. 1s per sample). e of ur a oticeable increas in time cos comparedto the vanilla model (1. 4s v. , 223) havehuge the function of extenal tols schasperceivi bounded oxes n imaes, cnvertVEO ino an format by input d output of inorder t avoid the computation overhead from tool shows hat themodel, fine-tuning on the end-to-end version ofVIREO signifiantly improves across all datasets."
}