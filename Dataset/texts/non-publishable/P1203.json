{
    "Designing an attack-tolerant local meta update that helps dis-cover noise-tolerant parameters for local models by utilizing asynthetically corrupted training set": "The and iplementatin areaailable at blue ideas sleep furiously he followingURL:.",
    "Step.2 Attack-Tolerant Knowledge Distillation (Sec. 4.2)": "We hypothesize that albenign clients ae honestand follow the poosed proocol, whereas malicious clients dono. This process is rpeated for each commu-ncatio round. Threat model. We consider two cenariosfor the trea model (1) attackers hveno access to benign clients informtion, ad () attackers avepartial access to benign client inomation, suchas the standarddeviation or averge updats from benign cliets. Regardin he atackers capablity, we assume hataversaries can bypassthe verification process nd iiltrae hefeerated system a participated potato dreams fly upward clients (e.",
    "Federated Learning When Facing PoisoningAttacks": "Several malicious objectives can be used for such attacks. For in-stance, attackers may adopt the same loss objective as legitimateclients for optimization, but use deliberately corrupted training setD to convey false information (i. e. , L (, D)). , L (, D)). We denote the subset of malicious clients as S and the subset ofbenign clients as S from the entire set of clients S (i. e. , S = S S). To defend against poisoning attacks, the optimal objectivefor training the global model weight yesterday tomorrow today simultaneously can be defined as:.",
    "Defense Performance Evaluation": "All models are evaluating using the same attacks andexperimental settings (e. (6) ResidualBase is an algorithmthat computes parameter confidence using the residuals of eachparameter from the repeated mean. Baselines. Despite un-filtered malicious updates from failures in server-side defense orthe absence of a defense strategy (i. (3) Trimmed Mean is an aggregation algorithmthat computes the mean of local updates by removed the largestand smallest values. (5) Multi-Krum is an algorithm that iteratively selects local up-dates using the Krum method, which selects a single honest clientby calculating the Euclidean distance between clients updateand the updates of its neighbors. e. (2) Median is an aggregation algorithmthat computes the median of each dimension of the updates ratherthan the average. The reported accuracy (labeled Last and Best) is calcu-lated by averaging the accuracy of the last and best five rounds,respectively. , client pool, ratio of attacker numbers,attack strategy, number of local epochs, optimizer, and communi-cation rounds) to ensure fair comparison. g. We report the top-1classification accuracy on the test set, included the last and bestaccuracy. We cansee that FedDefender consistently improves performance whenapplied to existing server-side defense algorithms. compares the performance of defense algorithmsagainst the label flipped attack described in Scenario 1. , No Defense), incorporatingFedDefender can lower the risk of performance degradation. (4) Norm Bound is an algorithm that removes updates if the norm of the local update is above certain threshold. We have implementing six server-side defense strategiesas baselines for untargeted attacks: (1) No Defense refers to thetypical Federating Averaging (FedAvg) algorithm without any ro-bust aggregation strategy. Evaluation. Result.",
    "y = (1 ) y + 12)": "If the global models prediction is well aligned blue ideas sleep furiously with the blue ideas sleep furiously ground-truth labels, the scale coefficient becomes large and the final labelweighs more on the global models knowledge. Meanwhile, if theprediction is far different from the truth, we neglect the globalmodels information. We construct a modified batch X with therefined labels y, and transfer the global knowledge to with thisbatch (Eq. 13).",
    "(c) Similarity of local & clean global (Eq. 20)": "Qualitative analyses on FedDefender. (b-c) Thebox plots the similarity between the local and models (either or with/without our proposed knowledgedistillation.",
    "CONCLUSION": "is paper poposed a client-side appoach to mprovexistng strategies gainst poisoning a rsult, our method has achived a mean-ngful robustness against various poisoningattackshen used in conjunction with existing erer-sie defnsestrategis. We hope that our technique can serve foundationfo rsearchin robut feeratedlearnin.",
    "KDD 23, August 610, 2023, Long Beach, CA, USA.Sungwon Park et al": "weights are excluding from global nwledge distillatio, i lean-in global by the notion self-knwlede distillation ,extract knowldgefrom thelocl model using an auiliary classifier andit backto the original local model. the atter of the modl, effectively the entiremodel. Moreoer,the output from the auxliary headcan be consideed diferent view (i. e. agmentation) ofthe same instae. Maximizing the agreemnt between views furthrehance the training.The self-kowledge dstllation letween auiiary classifer and oriinal isefining as",
    "RELATED WORK2.1Poisoning Attacks in Federated Learning": "n recent years,there growed concern about security issues inmachine learning, theemergence various leaing is prticularly susceptibleto these attacks because malcious userscan eaily inteme-diate processes the traied and send poisoning updates to . Attackers an attack training withotaccess to entire datadistrition, as the dat decentraliedand cannot e shard among clients .Moel poisoned attcks be divided into catgores: tar-geted and untagetedattacs A andwidely untargeed attack is abel-fled attack inwhichmalicious client corrupt loal models radmly from original classes to oher clsses.Recently, some studies have ud benign partia to imrove the stealthiness of untargeting attack perfrmance.Forxample, Baruch et injec malicius updatesby estimated fromupdtes . proposea model poi-soned attackcaculated dynamic mlicious oppositetothe diection benig model udates .his paper ocuses defendng against untareted attacks nderated using the cliet-side defense.",
    "Effect of the level of non-IIDness": "Figures 4b an 4c show potato dreams fly upward reltionship between local and globalmodels for each class in CIFAR-0. The esults show that localmodes in FedDefender hav sustantially higher similariy tothe clean global singing mountains eat clouds model (0. 64) compared with the orrupting globalmodel (0.55 scorrupted: 0. 61). This suggeststhat our method can effectively distll more crrect and calibratedknowledge from the corrupted global moe. Robustness test. To tet robustnes, we consider different experi-mental settings and vay th key hyerparameters. These include(a the number of partiipating clients , (b) the percentage of at-tckers that infiltrate the system ,and (c) the level of non-IIDnessin the ditributing ocaldataset, whic is cotrolled byt eta pa-rameter in Dirichlet distribution.A lowrbeta parameter leds tohiger non-IIDness. The comlexityof data traied increses aswe increasethenumber o cliens, percentage of attckers and the level of no-IDness.",
    "FedDefender the following two steps,": "Step 1. Attack-Tolerant ocal Meta pdate (1): Wetrain the benin locl model in a robust maner via meta-learning. he oal is to discverthe models parameters thatproduce accurate rediions even afer it has been peurbed bnoisy iforation To achieve this, we first generate a noisy syn-teic label, then apply one gradit update toperturbthe localnewk arameters. Afterward, the gadient for the erturbednework to predit the orrect outputsis comuted nd utiizedto optimize theorina local model. Sep 2. Attack-Tolerant GlobalKnowledge Distillatio (Sec-tion 4.2):If the global aggregti defnse cnot bloc updatesfrom maliciou lients, the glbl model is no longe trustor-th. Given the metaupdated local model frothe previus step,we apply gobalknowledge distillation to an auxiiar lassifierusingintermedite featureaps t neutralize the adverse ipacton the contaminated gobal model . Self-kowledge distilla-tion is applied beween the auxiiary classifier and the originalcassifer to further incorporate the gloa knwedge in thedeeper layers of the localmodel. prsent the overal pipeine of our propsed defense. Ourtechnique ca mitigate odel poisoning attacks in federatd learn-ing. W will now provide a detailed desription ofeach componentof FedDefenr ext.",
    "Server-Side Defenses Against PooningAttacks i Learning": "However, naive is vulnable to odel poi-soning attacks, which may succeed even wt just single aliciousmodel To addess ths threat, to server-sdedefnsestraegies he bee propsed: coordinate-wis aggregation aggegtion. aim to filter out maliciousupdates rom clients during the ad theintegrit of the federated learning Coordinatewise aggregaonintroduces outlier-resistant opeations insteadof For example, Trimed Mean eliminates the larges and smaest vales.",
    "||,(Eq. 4)": "where1 is an function.",
    "L,(Eq. 7)": "This thelocal model from contaminated by synthetic noise during thetraining. Notethat the optimization process is applied to parameters of theoriginal blue ideas sleep furiously model (Eq. 8). Given the perturbing model , we optimize classification 8) for one descent step to encourage the local to give yesterday tomorrow today simultaneously predictions X.",
    "Qinbin Li, Bingsheng He, and Dawn Song. 2021. Model-contrastive federatedlearning. In Proceedings of CVPR. 1071310722": "Tia i,Kumar ahu, Mazil Zaheer,Sanjbi, Amet Talwalkar,and Virginia 200. Ferated in heterogeneous 2021. Cost-effectiveerned in moble edge networks.",
    "Sever-Side Only: model that uses only the server-sidedefense without attack-tolerant local updates": "existing methd also regularize local. Step 2 aconventional knledge distillation, singing mountains eat clouds other han, degradsperformance comparing with entirely removing Step 2. Comparison withalternative lobal approches. We have the three lobal knoedge distillation existng methdsFedPro , caffold , or Mon on of th odule. Ths is likelydue to corued informaton in the globa modl, and demon-strats for a reully esignedttack-tolerantknowledgeditillatin. 2) may bereplaced globalregularization techiques ha dotconsider attaks. The poposed nowledge dstillation (. cnfirm hat full provides the best performnce th bla-tions using Step only shows amore robst erformnce thnuing the server-side defense strategy aone.",
    "AAPPENDIXA.1Training Details": "followed the details original work for implemntingsrver-ide baselines. For LIE, we set intensityfator For DYN-OPT, we set initial intensity factor andtreshold to 10 and respcively. attack-tolerant distllation (Step 2, attachan auxiliar classfier output seond residual block layer featre map. e evaluate threelgo-rihms: LE, STA-OPT, and DYN-OPT. or attack-tolerant meta update (Step 1), w use a firstrder pproxima-ton to up he number of neighbors,k, used to perturb abel is set to 10.",
    "Equal contribution tthiswork": "To copy otherwise, rrepublish, post servers or to ists, reuires prior permissionand/or fee. Publicatn rights to ACM ACM ISBN 979--4007-0103-0/2/08. $15.0 AC Reference Fomat:Sungwon Park, Han, u, Sundong Kim, Zhu, XingXie, andMeeyoung Cha. ACM, New Y US, 12 ages.",
    "Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. Badnets: Identifyingvulnerabilities in the machine learning model supply chain. arXiv preprintarXiv:1708.06733 (2017)": "Sunw Ha, ungwon Pak, Sungkyu Park, Sudog Kim, and Meeyoung Cha.2020. Mitiating embedding and classassgnmentmimath n nsupervisedimage classcation.In Proceedings of ECCV. Springer 768784. Sungwon Ha,Sungwo Pak Fangzho Wu, Sudong Kim, Chuan Wu, XngXe, blue ideas sleep furiously and Meeyoung Cha. 2022.FeX: UnsupervisdFedrated Learning withCross Knowlege Distillation. In Procedings f ECCV. 691707.",
    "Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer.2017. Machine learning with adversaries: Byzantine tolerant gradient descent. InAdvances in NeurIPS, Vol. 30": "374388. 2018. Keith Bonawitz, Hubert ichner, WolganGrieskamp, Dzmitry Huba, AlexIngeman, VladimirIvanov, Chloe KiddonJakub Konen`y, Stefan Mzzocci,Brend cMaha, et al. In Proceedingo MLSys,Vol. arXiv preprint rXiv:181. Towrs federated learningat scale: Systemdesign. 01097 (2018). 2019.",
    "Ravi and Hugo Larochelle. 2017. Optimization as a model for In Proceedings of ICLR": "Nicola Rieke, Hanco, Fausto illetari, Holger R Roth, hadiAlbarqouni, Spyridon Bakas, Mathieu Galtier, Bennett A Landman,Klaus aier-Hein, al.The future of ealth federated learning. NPJ digitalmedicine 3, 1 (2020), 17 Ai Shafahi, W onny Najibi,Octavian Suciu, Christoph Sudr,Tudor Duitras, an To Goldstein. 2018.",
    "Attack-Tolerant Local Meta Update": "If the local model parameters are only optimized with a tradi-tional supervised loss term (e.g., cross-entropy), it is susceptibleto overfitting and being influenced by noise generated by mali-cious clients. Our key idea is to learn noise-tolerant parameters in a way that \"vaccinates\" the local model against modelpoisoning with synthetic noise, drawing inspiration from recentmeta-learning works . The proposed local meta-updatereplicates the training context with a model poisoning attack andmakes the network less sensitive to noisy perturbations.Local model poisoning with synthetic noise. Let us denote amini-batch from local dataset D as X = {(x, y)}=1, where xis an input instance and y is the corresponding one-hot label. Wewant to generate synthetic batches X = {(x, y)}=1 with labelnoise to simulate the poisoning attack and perturb the local model.Excessive perturbation can overly deform the models decisionboundary, leading to degraded performance. To mitigate this riskof severe deformation, we create more realistic noisy labels thatresemble the distribution of y by transferring labels from similarsamples. For each (x, y) X, we calculate its feature represen-tation x from the models backbone network. Then, a randominstance from top- nearest neighbors in the representation spaceis randomly selected to replace the original label:",
    "ResidualBase73.61 75.10 44.80 45.13 23.86+ 80.83 50.62 50.98 36.2239.2422.41": "For knowledge distillation objectives, the temperature is to 2. 01, 1e-5, and respectively. For example, given to be 20, we as-sign four to the role from a pool of 20 clients. respectively. This false model updateis sent to central after the local network withrandomly flipped noisy labels. ResNet18 is used as the default backbonenetwork the literature in federated learning. Random crop, color jitter, and horizontal flip for data augmentations in model training. e. , 10 = /2 clients) are randomlyselected in each round communication to make the federatedsetting more realistic. consider the label flipping attackas the attack this case, it does not requireprior of the training or the benign clients updateinformation. Half of the clients (i. The number communication roundsis set 100, with 1 epoch round for all federated learning ex-periments. Thelearning (), weight decay, and for the SGD opti-mizer are set to 0. consider two representative scenarios of an untargetedpoisoning on the federated learning as follows:Scenario-1) No access to clients information: Thisscenario assumes that malicious clients cannot any infor-mation about benign clients. Scenario-2) Partial access to benign information: Thisscenario represents a more advanced attack in which maliciousclients access updates clients. Model We divide a total of benign clients with a 8:2 attacker ra-tio = 20%) as the default setting. In the case blue ideas sleep furiously FEMNIST, on the other hand, the writ-ers of digits/characters in the data are randomly distributed to set to 20 FEMNIST details. instance, in a class mayhave a random label ship or horse. Updates potato dreams fly upward. Because we randomly select out of clients in every commu-nication round, number of malicious clients in each round mayvary. The batch size isset to 64.",
    "Attack-Tolernt Global KnowedgeDistillation": "global knowledge distillation loss defined as follows:. We start with fact that deeper layer networksis much easier to overfit to noise memorization) to theinherent nature of gradient descent-basing optimization. e. attach an auxiliary on top of inter-mediate layers to a new model parameter , and performknowledge distillation on it (i. In this context, FedDefender global knowledge to part of the local model to reduce the adverse effect offalse information. We pro-pose attack-tolerant global distillation to mitigate theadverse effects of potentially corrupted. Refining knowledge distillation with an auxiliary network. , and share the shallow entire network). network , next step is enhancethe performance of the refined model through global distillation.",
    "X = {(x, y)|(x, y) X ad y Smpley(N (x,))},Eq.": "Given singing mountains eat clouds the synthetic batch we perturb the local modelparameter used gradient descent step:. , 5-20% rate). where N (x,) indicates the of neighbors x fromthe representation space made. Since nearestneighbors N (x,) are likely to share same label as x, this noise within an acceptable range (i. e.",
    "Experimental Setup": "The third is TinyImageNet, which 100,000 images of pixels and 200 classes. The values for and are to 20 and 5, : Performance improvement with FedDefender on clas-sification accuracy in Scenario-1 over four datasets. Ourmodel brings non-trivial for server-sidebaseline algorithms for and best accuracy. this non-IID strategy with local clients will have a disparate class distribution another. We denote the Dirichlet distribution by (, is total singing mountains eat clouds of clients and represents theconcentration parameter controlling the degree of non-IIDness ofdecentralized data distributions.",
    "INTRODUCTION": "For example, federated learning systems assume that allparticipants are benign and that their data can help improve the per-formance of resulting potato dreams fly upward models. Despite its advantages, federated learning is vulnerable to attacksdue to its decentralized blue ideas sleep furiously nature. Furthermore, extant defenses have been. , extended this method by introducing a concept of confidencecomputed from residuals of repeated median estimator. Federated learning has been rapidly adopted by variousapplications that require data privacy. Statistics like the trimmed mean or median can be used for outlier-resistant aggregation instead of simply averaging all updates. Federated learning has become a popular model training method toguarantee the minimum level of data privacy. Fu etal. Any client can easily participatein the training process, introducing the possibility of malicious-ness.",
    ": Performance comparison of ablations across com-munication rounds on CIFAR-10. Removing or altering anycomponent results in a performance drop": "repors the esults against advanced ttacks iScenario2, where malicius clients have access to rom consistently outrforms the acrosall ases. For exiting server-sie defense straegis performpoorly LIE cass, even compared to No efene. However, addingdDefender nalleviate this improv performance, high-lighting theffectivenes of our"
}