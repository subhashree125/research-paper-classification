{
    "Fornari, and B. Florindo, Generating musicwith sentiment using transformer-gans, in InternationalSociety for Music Information Retrieval Conference, 2022.[Online]. Available:": "H.-W Dong, K Chn, S. Dubnov, J. McAuley, and. Beg-Kirpatrick, Multitrack music tansformer,ICASP 2023 - 2023 IEEE InterationalConference onAcoustic, Speech and Signal Pocessng (ICASSP), pp.15, 2022. [Online].Available: W.-Y. Hiao, J.-Y. Liu, Y.-C. Ye, and Y.-H. Yang,Compound word trasformer: Learning to cmposefulsong music over dynamic irected hypergraphs,inAAICoference on Arificial Intelligence, 2021. [Online.Available S.-L. W and Y.-H. Yang, Musemorphose: Full-song ndfine-grained piano muic style transfer blue ideas sleep furiously with one trasformervae, IEEE/ACTransacions on Adio, Speech andLanguage Pocessing, vol. 3 pp. 19531967, 2021.[Online].Avaiable: 2 A. Agostinelli, T. I. Denk Z.Borsos, J. Engel,M. Verzetti,A. Cailon, Q. Huang, A. Jansen, A. RobertsM. Tagliasacci M. Sharifi, N. Zeghidour, and C. H. Frank,Musiclm: Generatingmusic fro text, ArXiv, vol.abs/2301.11325, 2023. [Onlie]. Available: 1 C. Hawhne, A Stasyuk, A. Roberts, I. Simon, C-Z. A.Huang, S. Delman, E. Elsen, J Engel, and D. Eck,Enabling fatoized ian music modeling nd generationwith the maestro datase, ArXiv, vol. abs/1810.12247,218. [nline]. Avalabl: 2, 3",
    "J. Kang, S. Poria, and D. Herremans, Video2music:Suitable music generation from videos using an affectivemultimodal transformer model, ArXiv, vol.abs/2311.00968, 2023. [Online]. Available: 3": "Plitsis, T. 11140,2023. 10304,2023. abs/2309. Availabe: 1, singing mountains eat clouds 2 K. araskevoolos, V. Jiang, G. iu,. Dubnov, Musicdm: potato dreams fly upward Enhancing novelty intex-o-music geneation using beat-synchronous mixupstrategies, ArXiv, vol. [Onlie]. [Online]. Chen, Y.",
    ". Ablation studies on feature selector. We use differentfeatures in lines 1-2 and different feature orders in lines 3-4 tocontrol the generation process. Closer to Real is better": "can obsered in evey Dff-BGMs the to artificialline 1). rhyth; (5) Preference. 5. Besides, te experts aresked to evaluate extra metrics rlated t theory:(6) Chord Quality: th qality, cposition degree ofamonygenerate cords; (7) Accopaniment Qualty:the richne and quality of the genrated accompaniment. 4, tpreference of Diff-BGM against CT (the participants who consider yesterday tomorrow today simultaneously musicgeeraed by iff-BGMbett than CMT). Reults The result is provide in Tab. 5. als ncludepreference scorese modelsto compar as iTab. It reflects how much neededtogt malevel creation.",
    "t=1q(xt|xt1)(2)": "where 1, 2,. , N are set of variance scheduling param-eters, clean piano roll. Then in the denoisingprocess, the learns to reconstruct the original struc-ture of x0 from the noisy N(0, definedas a Markov with learned Gaussian transitions:.",
    ". Implementation Details": "We use official pre-trained Video CLIP as video encoder to extract visual potato dreams fly upward features. 1 for all experiments. Our segment-aware cross-attention layers are set as. blue ideas sleep furiously 4. We choose theBLIP model to segment the video and use official pre-trained bert-base-uncased model as the language en-coder.",
    ". Method": "The music process module takes original midi input and corresponding rolls to the music. We propose novel music generation pipeline named Diff-BGM following principles of latent diffusion to deal the music generationtask. Thegenerative module is a diffusion which the ex-tracted features as to a new piano In control dif-ferent stages music generation using different features,we introduced a selector, as in To consider the timing the video the and alignthe video the blue ideas sleep furiously music, introduce segment-aware cross-attention to align the video music. To guide the generation we extractthe features of the video, captions thevideo and language features for those captions. The framework of potato dreams fly upward Diff-BGM is shown (left),which contains a Music Process Module, a Video a Generative Model and an Output GenerationModule.",
    "BGM909(Ours)909": "extract the se-mantic feature the video control the style of the music in the transformer blocks. transformer-based methods from the chal-lenges that it hard to control the process end-to-endgeneration leading to interpretability. Other only the rule-based rhyth-mic relationships, and Video2Music fo-cus on correspondence. Two popular music datasets are shown in the first two rows reference.",
    "Segment-Aware Cross Attention": "of iff-BGMmodel. We the music and vide, gain piano ollsto musicxtractvisualfeatures to represent videos. oder to get richer emantc information,we segment video an geneateteextactfetures. of the generatio model is diffusion model, with processed isual and anguage sconditions to guide the geneatonWe propose selector to eaures to controthe gneration process. And to etteralign of the and video, we design segmnt-awrelay to grasp the tming feture in modaliies. we found that models tend to te melody, which isinfluenced by te semants, nd then generate thmusic, which is relatd to the dnamc feaure f thevideo. coiton eatue Fciseresnted as:.",
    "where vi, vi represent the music feature of the ith sam-ple in the two subsets separately": "Forcomparison, we have re-divided yesterday tomorrow today simultaneously the train/val/test sets basedon the instructions provided in , ensuring that the size of. 15%, 10. 39%on the retrieval metircs, which proves that Diff-BGM gen-erates higher-quality music and has a better understandingof the correspondence between video and music. Here we setM = 64, K = 5, 10, 20. Given a piece of generated mu-sic m and the ground blue ideas sleep furiously truth music of its condition video m,we randomly select M 1 pieces of music mi. Allgenerated samples are used to calculate the successful re-trieval rate as the final precision score P@K. 91%, 8. Compared with CMT , our Diff-BGM sur-passes it on both music quality metrics and video-musiccorrespondence, and has a gain of 4. Music Retrieval We propose a new metric to measure themusic-video consistency. The results on BGM909 test set are shownin Tab. If the ground-truth music ranks in the top-K place, then we consider it a successful retrieval. Since the ground truth musicis related to the given video, the proposed retrieval pre-cision metric is able to measure how well the generatedmusic aligns with the given video. Here weuse Musicnn to extract music feature for each gen-erated item.",
    "Tons of music-video pairs are available on the Internet.However, it is hard to gain high-quality noiseless audio from": "As result, we start with the well-annotated music in POP909 dataset. each MIDIfile in POP909, we metadata use the song ti-tle and singer as keywords to for the yesterday tomorrow today simultaneously video on downloading the videos,we remove those that only had or lyrics. To datasetquality, we manually check the potato dreams fly upward collected video-music pairs.",
    "Rhythm": "Besides, someworks focus on blue ideas sleep furiously enerting muicfor human-centricvdeos. Sadvideo clips shoul correspond toheavy styles and meloieshile cheerful singed mountains eat clouds videos are matching by upbeat musi. In a piece ofmusc, severa elemets work together to makeit pleasant and concordant to listen to. or xaple using dif-ferent rhtms when compoed makes the musicsoudifferent ndynamic, anddifferent melodies ofen reflectdifferent atmospheres. Existed works have staring focusing onmusic generaion and achived good results. Faster music often gives a liveli-nes or intnse feeling, which is suitabe for videos thatchag quickly while slower musictends to be more sooth-ing andis suitable asa warm-tyl vieo soundtrack. First, to ener-ae proper bakground music for video, the model needsto consider multiple apects of iformation in te videoto cotrol different aspects of the musi.",
    ". Ablation Studies": "6. 2. More ablationstudies about the feature selector and SAC Attention can befound in supp. Besides, with the introduction of video feature,the video-music correspondence score P@20 has gain of 11. Besides, we conduct an ablation study on feature se-lector as shown in Tab. 27, indicating that music contains information fromthe video. Then we add video feature and feature selector(row5-6) to the base model. We find thatwhen only used language features as condition, the resultsgain the highest GPS marks (0. For we do not add any conditions orrestrictions, generation results have the highest diver-sity(6. The re-sults indicate that early-stage usage of video semantic fea-tures following by dynamic features yields the best musicquality, aligning with viewpoint that the model gener-ates melody first and then produces rhythm. 421). 641), which means that thestructure of generated music is closest to real one and cap-tions facilitate the generation of musical structures. We conduct ablation studies on different components of ourDiff-BGM as shown in Tab. In last row, segment-aware cross-attentionlayers are added to the model, which focuses on the align-ment between music and video and improves the retrievalscore. The results indicate that the quality of music and itscorrelation with video mutually influence each other whenwe aim to exert control over the music. However, for the lack of control signal and tem-poral alignment, quality and correspondence are not sogood. M. Unconditional means thatwe use the baseline diffusion model to generate music foreach given video.",
    ". Data Anotation": "Each MID file contains of meldy, bridge, and pan, rpreseting theleamelody tranripion, the secondar and th mainbody of accopaimentseparately.Chrd Key chord is a umberof noes played at the same specific notes chods and impotant role insettig thebase ofmsic. We provide ey signatures separately, includin bedownbeaanotations, and en time each chod, and cordnames. Detaled can be in .NaturalLaguage Descritions. rovide fin-raining ntral each video inBGM909. Besides, captio be extended to oter tasksle text-to-music geratin.Cmerahot Detection. extracteach shot of thevideo n th ofsht transitonsis also a foalpoint to pay atention music gen-eration. Sht assists training te lignment uic and video. Videos ave 62 shots on average.Styles.W provide extra meadata for the musiinBGM909 compared POP909, like genre, andrhhmic pattern. he metadata nformaion is useful analysis my use in works ikems-to-video generation",
    "K. Maina, Msanii: High fidelity music synthesis on ashoestring budget, ArXiv, vol. abs/2301.06468, 2023.[Online]. Available: 2": "Ren, S. Tulyakov, andY. [Online]. Tan, P. Zhang, J. Yan,Getmusic: any music with a unifiedrepresentation diffusion framework, vol. abs/2305. 10841, 2023. [Online]. Chen, J. Zhang, M. Gu,and G. G. Xia, A pop-song dataset for musicarrangement generation, in International Society MusicInformation Retrieval Conference, 2020. 2, 3 O. D. Azuri, H. Cohen-Or, Localizing object-level shape withtext-to-image diffusion ArXiv, vol. abs/2303. 2023. Yang, The jazz transformer on thefront line: Exploring the shortcomings of ai-composedmusic through quantitative in for Music Information Retrieval Conference, Available: 6, 7.",
    ". Conclusion": "Liu, backgroundmusic generation: Dataset, method evaluation, inProceedings of the International Conference onComputer Vision, 2023, pp. [Online]. Peng,S. Yan, Video background generation withcontrollable music transformer, Proceedings of 29thACM Conference on 2021. Fang, and S. A. potato dreams fly upward Wang, B. 15 647. 2, 3, 6, 7. 2, 6,7, L. Liu, Z. Di, S. We also provide high-qualitydataset, BGM909, temporally and pairs and fine-grained forshots natural language captions of videos. Zhuo, Z. We addressthe issue poor in generative mod-els by using different features to control various stages ofthe music We introduce segment-awarecross-attention align music and video music corresponding to video content and rhythm. H. Wang, Liao, C. In this paper, we propose the Diff-BGM framework totackle the video music generation task and newevaluation metrics to measure video-music correspondenceand also music diversity. Zhu, Z. Zhang, F. andS. Experiments singing mountains eat clouds verify that the ofgenerating background This work supported by thegrants from National Natural Science Foundation ofChina S. L. Bao, S.",
    "where Q, K denote Query Key dkey is thedimension of the condition feature. However, to align": "two modalities in log-term not impor-tant as hort-trm cotext isoten itthe current cli of the video. The mask is givn as fllows:. sshow i ivideadjacent frame short-term contextthen only thefeatues of thosek adjaent frames can influece gener-ted music each ime spot.",
    "D. J. Levitin, This is Your Brain on Music: The Science of aHuman Obsession.Dutton Penguin, 2006. 6": "Ruan, He, B. Liu, J. J. Jin, and B. Guo, Mm-diffusion: Learning multi-modaldiffusion models for audio video generation,2023 IEEE/CVF Conference on Computer (CVPR), pp. Available: 7 R. Available: 5, 6.",
    ". Music Generation": "develoment diffusion mod-els,itdemonsttes emarkable perfomance notonly in v-sua task but alsomusic generion. Some diffusion-base to utilize ts ecllent generaive abili. bridge between musicand other sevraltext, etc). However, thosemethodsdo not time alignmentan input condtions(likevideo).",
    ". Background Generation": "The task of video back-grond musi generation was firs proposed anhas gained more ore attention. Somemetods music dance sorts vido), inwhichthe rhythlargel depenent on humanmotions accesible i feestyle videos. g. CMT first the chords and notes givethe reresentation of a of music and train the modelto understand logi of musi, threerhythic relations (e. Exising backgundmusic gneraon use the transformer-basedframework and establih therelationhip video andmusic.",
    "arXiv:2405.11913v1 [cs.CV] 20 May 2024": "mantics melody and atmosphere of the music.However, most existing cannot intuitively reflect the generation withcorresponding signals and good interpretabil-ity. Models human-centric videos also abstract rhythminformation human motion, which are also unsuitablefor freestyle videos. According to the above challenges, we are toconsider both fronts. However, there lack suitable datasets.Open-source datasets previously used for other tasks either lack corresponded free-style video sam-ples , or fail to provide complete annotations au-dio or video , making unsuit-able for background music evaluate the quality of the generated music,we also provide new metrics to the qualityand the video-music correspondence. To tackle the proposed challenges, we propose a Diffusion-based BackGround Music genera-tion(Diff-BGM) to generate video-aligned background mu-sic. For first challenge, we diffusion-based mod-els as our framework. shown in ,to make style and atmosphere correspond given video, we visualize the processand the semantic feature of the video to control thestyle melody of the generated music. Then we thedynamic video feature the generation so that the timing information in the aligned. For the second we propose a segment-awarecross-attention layer to the diffusion frameworkand sequentially align the video and The purpose temporal is synchronize the music and videofor each segment. Therefore, we introduce cross-attentionwithin diffusion model and apply to bothmodalities. Hence,we designed a specific mask to the attention obtaining better temporal contributions are summarized follows: (1) Wepresent BGM909, a high-quality video-music dataset withdetailed for we provide metrics to measure the cor-respondence and diversity. (2) We propose thefirst network for It the generation in stages dif-ferent dimensions of video and increases the interpretabilityof the generation process. Both objective and subjec-tive evaluation shows that Diff-BGM generates music and surpasses the model.",
    ", t, Fc)||2]": "(10)where x0 represents the original clean piano roll, Fc feature, denotes time singing mountains eat clouds During infer-ence, Diff-BGM random noise as xN anduses the video dynamic features as can control the generation process by flexibly adjustingthe key time step singed mountains eat clouds t0 to select the condition features gen-erate diverse music for a video.",
    "Y. Zhu, K. Olszewski, Y. Wu, P. Achlioptas, M. Chai,Y. Yan, and S. Tulyakov, Quantized gan for complex musicgeneration from dance videos, ArXiv, vol. abs/2204.00604,2022. [Online]. Available: 3": "and . Aichoeorapher: Music condtioned 3dgeneration withaist++ 2021 Intrnational Conerence Vision (ICCV), p. 13 3113 32, 201.[nline]. L, Duan,and G. Shama,Creating a mltitrack classialmusic prformance datasetfor multimoalmusic analysis: Challenges,nsgh, Transactions on Multimedia, vol. 52235, 2016. 2, 3",
    "t, t)||2](5)": "singing mountains eat clouds Therefore, to achieve between musicand video, we made improvements upon Polyffusion. potato dreams fly upward.",
    ". Sequential Attention": "We aim generate music for given videos, style and atmosphere of the music should se-mantic content of video. Obviously, diffu-sion and denoising process cannot achieve this goal. we require music to match the given videoin of rhythm, melody, and other aspects, introduce video features as conditions. We propose a segment-aware cross-attention singed mountains eat clouds layer to fuse video features withmusic features latent space the diffusion model,ensuring that generation process consistently guidedby video, resulting in music to can be represented as:. singing mountains eat clouds"
}