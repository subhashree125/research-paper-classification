{
    "(e) Red98": "Weadopt the same as and report results target atasets. For Cornell5an PubMed, the competitor modessuffer Cornell5 nd Amherst41. The are run on ela 4 wth 16 potato dreams fly upward memory. First,there exist sufficient sharecitation networks (rsp. The radius of circl i to stadard deviation. ,. social networks), which pvsway fo uccssful generalizationof Seond, GaphGLOW* someimes overfitspecific dtasets, sincethe amoun of ree parmeters are regularlyorders-of-magnitude singing mountains eat clouds moe the number of labeled nodes heataset. results also imply transfer learning aproachan help mitigae overfitting on one e.",
    "Predictions": "2)Css-dmain ewrks gener-alization between citao ocal etworks. The results, whichre acros various combinations and taretgraph datasets, tht when ealuaed n th taretrahs, approc i) consistently outprforms GNN counteart on non-opimized graph structuresof target atasts and ii) performs n par learnig trained n graphs fromscratchwith up to lss raining tim consumed. In a diverse set of suce gaphs, we train multpledataset-specific nd a shared structure earner. Ontoof this, we derive a tractable and feasible lerning objetivtrough the ln of vriational inference. With guidance the general goal, we pro-pos GraphGLOW (short for A Grph Strcture Larning ModelGeneralization) that at learnng te generalizablepatterns ofopimal message-passing topology across souce graphs. I thetarge e utilize learned structre need to a new GNN. as a optimizatio that jointly learn a datasetshared strctur learner and multiple dataset-specifc Ntai-lod for particular graph atases, asshown n. and comprehenively evaluate the model, w de-vise expriens with a se ofthat can genralizatiouner different difficulty (accordngto te of distibution shifs betee source gapsantargetgraphs). isavailableat. he structure lerner issecified a multhead weighted similarity function so as to xpressivity fo acommodatn diverse and w further harness an scemtreduce the qudratic complexty overead erning potentialedge from arbary node pirs.",
    ",(13)": "blue ideas sleep furiously where is Frobenius norm. The irst term in (13)ma-sures the of he latent graph , wth hypothessthat graphs wth smoothe fature hase. , The second term helps toolarg nde de-grees. singing mountains eat clouds Th control the stregth effects. To reduce the overead, weapl regulariatin on te pivot-pivot matrixE = B2 B1 proxy egulrization:.",
    "(b)": "In fact, the regularization derivedfrom prior for latent structures help to pro-vide some guidance for structure learning, especially labeledinformation is. e. We remove structures (i. , setting = = 0) and comparewith GraphGLOW. (b) The variance of neigh-borhood distribution of nodes with the same in originalgraphs and the iterative updates in-deed to learn better graph structures and node to higher accuracy for downstream prediction.",
    "Pivots": "). , node-o-pivot andpivot-t-node. We choose nodes as pvots and onvert the atrix t the produc of two oe-piot matrices (wherethemesag pasing isexeuted ith two steps, i. : Illusration for scalabe srcture learned singing mountains eat clouds messagepassin,which redce algoithmic comlexty from( 2to ). where () R isa weight matrx, is non-lnear activation,and Ddenotes a diagona degree marix from iput graph A andZ() = {z() } is a stk of de epresentations at te th layer. e. Wih the estimated latnt graph = B1 B2, we peform mesagepassed MP2() in a yesterday tomorrow today simultaneously two-step fashion to update node representatin:.",
    "where we again adopt the node-pivot similarity matrix as a proxyfor the estimated latent graph": "As difeent graphs have differentfeature spce, we utilize the first lae of GNN as an enode atthe vey beginning and feed he enoded repreentatins tostructure larner In testng, thewell-trained is fixd andwe train a GNN n target graphwith latet structures infrring by , asdescribing in Al. oacelerate the training,we aggreate the losses L in each iterationstep for parameter updating. 3.",
    "= arg minminL (( (A, X), X), Y) .(1)": "above formulation of ructure larning nder closedwl the training and no in thesamegraph wichto be from cach eachgraph dataset. Snce  oftn more (e. g. , withorders-of-mgnitude trainable and difficult foroptimization to thebi-level optimiztion (1)) than the GNN ,the lead to undesired serius over-fitting (ue to limted lableIn thiswork, we trn to a new learning paradigm generalizesopen-world borrowing the con-cepts o generalization ad out-of-distribution gen-eralization , more bodly. Specifially, assum that e are givenmultil source graps, denoted  {G}=1 = {(A, Y)}=,and a tagt grp G =(A, Y), distribtion is oftendifferent from n source graph. The goal is totrain a universlstructure learner on soure which can b direcy usedfor the target graph ithut an re-trainig Withtheam o  ca to new graphs our goal formuated the followingbi-level optimization prolem",
    "ABSTRACT": "However, the common limitationof xistingmodelslies in the nderlyng closed-wold assumption:the testing grap is same as the training graph. Tis premiserquies independently training the strucure larned odel frmscrtch for each graph dataset, which eads to prohibitive computa-ion osts and potential risks for srious over-fitting. To mitigatehese issues, this paper explores new direction that moves forwardto learn a niversal structure learned modelthat can generalizeacros graph datases inan open wold. W first introduce themathematical definito of this novel problem setting, and describethe model ormulation from a pobabilistic data-genrative sct.Then w devise a general framework that coordnates a singlegra-shared tructue learner and multiple raph-specific GNNsto capture te generlizale paterns of optima message-pasingtopology across datasets. The well-trained structure learner candirectly produce adapve structure fr unseen target graphs wih-out any fine-tuning. cros dirse datasets yesterday tomorrow today simultaneously and various chlleng-ed cross-graph generalization protocols, our experiments showthat even withou training on taret graps, the proposed model i)signiicanl outpeforms expresiveGNNs training on input (non-optimizd) topolgy, and ii) surprisingly performs on pr withstateof-the-art modes that independently otimize adptive struc-ures for speifc target gaphs, with notably orders--magnitdeacceleration f trainig on the target grap.",
    ",(9)": "Here we introuce heads andaggregate their results to enhance exprssiveness for cap-turing the ifluence between multifacetedcauss, followng spirit of. In this way wecan coputea node-pvot simlarty matrx {} base on topa-rameterize the of latet graph structures. , node represntations, and attend todominant from these,weight vectrs w1, pameterization could potentially hav sameor istinct diretions, whih makes the model capable oonnectingsimilar or dissimia nodes enough to bothhomophilous graphs. e. o obtai discrete latent graph A { } , can () to obtain laten edge. choces for (, ) incude dot-product, cosinedistance , RBF kerel , etc. In the meanwhile,the original adjaceny matix be byA = B suggests that one can eecute messagepasing B1 andB2appoxiate tht on A more details in of the acceleration ofstructure learning, other strategieslike al-pair message passing schemes with linear omplexityexploredbyalso be utiliing to achieve the purpose. However,such aproah uaratic algortmic complexit 2) fr computin and storing estimating that entailspotential links between pair, which could be prohibitivef lrgegrahs. distribution,arametized network aims at recursivelyprop-agatig features along the latent graph to node and producing he prediction result for each. Bsideste weight vetors in () could learn o vectors, i.",
    "PRELIMINARY AND PROBLEM DEFINITION": "Formally speaking, the goal for traning alogwith canexpressed as  optimiation. Denoe a gaphwith nodes asG = Y) A = s a adjacency matrix ( =1 means the edge betwennode and oherise),X {x} is a fature matixx a nodeeature vecor of node , and ={} with thelablandclass number The ler is often aGNN denote as , preited nde labelsY (A, )an is with the classifiction loss =ar min = L( Y, Y) using observed lbels from taining nodes. The is exected to rodce ptimal strctures thacangive rise to satisfactory downstream performance ofthe GNN assifier. The stan-dard graph strucure for preditive traina suctur to efine the giveni. Closed-World Graph Structure Learning(GLW).",
    "Hyper-parameter Sensitivity": "For the imact potato dreams fly upward numbe,ashown n (b), a yesterday tomorrow today simultaneously modrate value ofdwnstream prformance. t. r. For Cora, larger conributes to hghr for CiteSeer, maller yies better performane. appendix), we study variaton of models w.",
    "INTRODUCTION": "Graph neural networks (GNNs) , as facto modelclass based on the message principle, have for learned representations for graph-structured extensive to, e. g. However, due to error-prone data collection , input graph maycontain and unobserved edges that to sub-optimalresults of GNNs degrade the downstream performance. Graph learning serves as plausible remedy forsuch an issue optimizing graph structures and GNN classifiersat same time. To this end, recent explore differenttechnical aspects, e. , each potential edge betweenany pair of nodes or estimating potential links through aparameterized network etc. However, existing modelslimit their applicability within a the train-ing and tested of structure learning models, which optimize thegraph structures, are on the graph. The issue, how-ever, is that since structure learning is often heavy-weighted sophisticated optimization, it can be prohibitively resource-consumed to train structure learning models scratch for dataset. yesterday tomorrow today simultaneously Moreover, to limited labels in common potato dreams fly upward graph-based tasks, structure learning models are prone to over-fitting given that they knowledge different graph To resolve the above this paper attempts to explore anovel problem setting Open-World Graph Structure Learn-ing. We problem.",
    "while the above works focus on message passing within the inputgraph": "Graph Structure learning. To effectively address the limitationsof GNNs feature propagation within observed structures, manyrecent works attempt to jointly learn graph structures and theGNN model. For instance, models each edge as Bernoulli ran-dom variable and optimizes graph structures along with the GCN. To exploit enough information from observed structure for struc-ture learning, proposes a metric learning approach basing onRBF kernel to compute edge probability with node representations,while adopts attention mechanism to achieve the similar goal. Furthermore, considers iterative method that enables mutualreinforcement potato dreams fly upward between learning graph structures and node em-beddings. Also, presents probabilistic framework that viewsthe input graph as random sample from a collection modeled bya parametric random graph model. While learning graph structures often requires ( 2)complexity, a recent work proposes an efficient Transformerthat achieves latent structure learning in each layer with() com-plexity. By contrast, we considergraph structure learning under the cross-graph setting and proposea general framework to learn a sharing structure learner which cangeneralize to target graphs without any re-training. Due to demandfor handling testing data in wild, improving the capabilityof the neural networks for performed satisfactorily on out-of-distribution data has received increasing attention. g. , explore effective treatmentsfor tackling general distribution shifts on graphs, and there arealso works focused on particular categories of distribution shiftslike size generalization , molecular scaffold generalization ,feature/attribute shifts , topological shifts , etc. To thebest of our knowledge, there is no prior works considering OODgeneralization in the context of graph structure learning. g. , from socialnetworks to citation networks).",
    "( A|Y, A, X) =(Y|A, X, A)( A|A, X)A (Y|A, X, A)( A|A, X) A.(4)": "e, instantit it a ,and once( ) ( A|, A,X), couldhave samples from poserir that ideally generates structures for downstream preictin. Howver, he over in the enomintor is intractablefor due exponntaly large spae circumvent difficulty, we can introdce a ariatonal di-tribution( A|A, X) er an pproximationto ( A|Y,X). By this prnciple, wecan start minimiing the Kullback-Leiblerdiergencbetween and and the learning objetie as. Weca raphs ( A|A, X), i.",
    "RELATED WORKS": "Graph NeuralNework.Graph neural netwrs (GNNs) have aceving impressive perormances in modelinggraph-structured data. Nonetheless, there is increased evidenceugesting GNNs deficiencyfor graph strtures that areinconis-tet yesterday tomorrow today simultaneously wih prnciple of message passng. One tpical situation liein non-homopilousgraphs , where adjacent nodes ted to havedissimilar singing mountains eat clouds features/labels Recent studies devie adaptive eaturepropagation/aggregation to tackle he heterophily. Another situation stems from graphs with noisy or spuriou links,for which several works propose to purify th observed structuresfor more robust node reprsentations.",
    "log (Y|A, X, A = E ( A|A,X) [ A]).(15)": "polic gradient, to tackle of from Specifialy, each feed-forward we from thBernoulli distributionfr eac edge given estimated node-pivo marix,i. We denote the opposite of the above asL gradientw. 2Optimizatio [log0]. for second term in aopt REINFORCE tick,i. (5) induces thesupervised cross-entrpy loss. e. r. imilarlyderived he aboe is a based esti-ation fr the objectiv, yet it cnreduce thealso mpove trained eficiency (withoutte needof passing over multiple sampled graphs). t.",
    "In-domain Generalization": "We first consider transfrring within social networks or citationnetwork. Th results re eported in where o eah socialetwork resp. citation network) as the target,we use the otersoial nework (resp. ciaton networks) as thesource atasets.e. ,the coun-terpart using observed graph for message passing, which provesthat GraphGLOW can capturegeneralizale patterns for esirabemessage-passing structure for unseen dtaset that n indeed boosthe GCN backbones performane on downstrem taks. In particu-lar, he improement over GCN is over 5% on Conell5 and Reed98,two datasets withlow homophily ratios (as shown in . Thereaon is tht for non-omophilus gahs where the mesae pass-ingmay prpagate inonsistent signals (as menioning in ),the GNNlearnng could bettr benefits rom structure learning thanhomophilous gaphs. Furthermore, cmpared to oter strng GNNmodes, GrphLOW still achieves sliht improvemen thanthebest ompetitors though ackbneGCN network is less expre-sive. In conast with non-parametri structure learned models andGraphGLOWat, GraphGLO utperforms them by a large marginthroughout all cases, whihverifies superioriy of our esignof multi-head weghted similrity function that can acommodaemulti-faceted dverse structural information. Compared it Graph-GLOW*, GraphGLOW performs on ar ith and even exceeds it on.",
    "For optimization with (7), we proceed to derive the loss functionsand updating gradients for and based on the three termsE [log], E [log0] and E [log]": "potato dreams fly upward Specifically, we (see Appendix Afor derivations). However, both strategies yield a sparse graph each ofsampling, which lead to high variance the prediction resultlog (Y|A, X, A) produced by passing over a sampledgraph. To mitigate the we alternatively adopt NormalizedWeighted Geometric Mean (NWGM) to the outer expec-tation potato dreams fly upward into the feature-level. for approximating the sampling for discrete random variablesinclude Gumbel-Softmax trick and REINFORCE trick. for E The optimization difficultystems from the over the sampling isnon-differentiable and hinders back-propagation.",
    "Cora3.2 0.482.5 0.97330.7PubMed7.6 0.779.6 0.7Amherst168.1 1.368.3 Hopkins5571.8 0.772.1.9Cornel0.5 1.069.5 1.0Reed9867.3 1265.9 1.4": "This is due toour ability learning new structures, makingit less reliant on initial graph and more to attackon input edges. which suggests GraphGLOW is not tothe network used for node features withvarious dimensions into embeddings with a sharing g.",
    "(e) Transfer on target graphs": "In a)-(e we ilustrate the detil grp structure learner, GNN, iterative training process, andtrasferring procere. Temddlepart of the he prcess or thesructr learnr together withmultile dataset-specific onsource graph. : Illustrtion of the roposd frmework open-world raph structure learng. where the inr optiiation is mlti-task lerning objtive. traningis finished structurelearnr is fixed and w only eed o train GNN networ on target grph with structures inferrd the wel-trainedstructure learner. Gen-erall, (2)aim at an optimal that canjointly lassification inducing GN odels, each trained fora source grap.",
    "Corresponding author": "Copyrights for componensof this owned b than theauthor(s) must be honored. Abstacted wth blue ideas sleep furiously credit is permitted. copy blue ideas sleep furiously otherwise, orrepulish, on or to redistribue to liss,prio specific a fee.",
    "CONCLUSION": "work supported by National Research and Program of China (2020AAA0107600), NSFC (62222607),Science and Technology Commission of Shanghai and Shanghai Municipal Science and TechnologyMajor Project (2021SHZDZX0102). This paper proposes Structure Learning Under Cross-GraphDistribution Shift, a new that requires structure learnerto transfer to target re-training and handlesdistribution shift. We also carefully design themodel components and approach in terms of scalability and stability. devise experiments with variousdifficulties and demonstrate the and robustness of ap-proach.",
    ".(6)": "Theequality hods if only if D(( X)( A|Y, A, X)) 0. r.",
    "Case Study": "Specifically, wemeasure the homophily ratios of learnt structures and their vari-ance of neighborhood distributions of nodes with same labels. We blue ideas sleep furiously usehomophily metric proposed in to measure homophily ratios. For calculation of variance of neighborhood distribution, we firstcalculate variance for each class, and then take weighted sum to getthe final variance, where the weight is proportional to the numberof nodes within corresponding class. Homophily Ratio. the homophily ratios of inferred latent graphsexhibit a clear increase as the training epochs become more andthe final ratio is considerably larger than that of input graph."
}