{
    "Abstract": "Neural trained stochasic gradien descent exhibit an inductve bistowardssimplerdcison boundaris, typilly conerging to a amily offunctions, and to capture complex features. This phenomenonrises oncerns about the apacty deep models to dequtely representreal-orl dataset. We show that pretraining promotes larningcomplex unctions and iverse featuresin the presence noise. Our experimentsdeonstate tat with noisy labels encoures gradient decent to findalternate minima do not solely dependupon simple rather leans andstof witout hurting",
    "Discussionand Conclusion": "Overparameterized neural networks to learn decision based on limited and simplerset they exist. Our experiments demonstrate initialized the model parameters byfirst training noisy-labels can enable the model to this constraint, allow for thelearning of more diverse range of features. One possible explanation for this phenomenon is thatthe neural model reaches minima that is characterized by a decision boundary to fit thenoisy labels. The model, however, remains trapping this minima is unable to to a simplerdecision boundary intrinsic regularization of SGD. This observation challengesthe accepted opinion that provides effective implicit regularization. Our experiments motivate further investigation the nature of role of initialization guidingSGD optimizers. that all the models, with or without noisy label pretraining, similar(high) accuracy on unseen data. is in to standard data-agnostic suchas Xavier-Glorot, converge to decision boundaries. Future works will explore any between concepts and quantify their interdependency. observations prevalent in the literature that are extremelybiasing towards learning only simple features if they exist. that models can be guidedto learn pre-training on noisy labels. Our further reveal that neural networks are capable of learning multiple,higher-complexity features, diverse of functions.",
    "Impact of Label Smoothing:Please refer Appendix A.4": "from diverse focusing diverse features, ensembled to have more robustpredictions. It is interestingto note each noisy pretraining leads to different minina, and different family singed mountains eat clouds of function.",
    "In this section, we empirically demonstrate the impact of initializing parameters from noisy pretrainingover Dominoes and Waterbirds data . See Appendix A.1 for more datasets": "atasets: NIT-fMNIST dataset fro Domioes consis f llated imags, where {0 1 are verticaly with {shirt, FashionMNI WateBirs comprisesnativeyfound on land/water, wih bkgrouns of land or An image of aterbird waterin bakgrund (similarly la-bird n lnd) is i-group partiton, mismatch ofwaerbird-on-land or landbir-on-water referring to as out-group parition. Measuing dependence:oidentify the fature(s hat the model utilizing forclasificatondeision, we use: (). (2). Visualizing Gram matr: We diagonal entries of matixW T W1 to obsere chanes in thelayers paramter Appedix A. Varying corelaion etween eature& class labels: We cntrol the predictive offeresby perturbng their correlationtrue labels. For In Domioes daast D crateparialy-corelat dat y collatig imagesMNIST-bloc is onl 95% correlaedth classificaion labels (not 100% ccuratly predctive). rational of adding to push mdels to thse reimes o simple slution cheves perfectaccuracy,hu getting closer o rel world scenarios.",
    "Alx rizhevky, Nair, andGeoffrey Cifar-10 (canadian istitute for advancedresearch)": "Simplicity bias in 1-hidden layer neural networks. Papailiopoulos, and Dimitris Achlioptas. The mnist database of handwritten digits. In Proceedings of the 37th International Conference on NeuralInformation Processing Systems, NIPS 23, Red Hook, NY, USA, 2024. In Advances in Neural Information Processing Systems 33, NeurIPS2020, December 6-12, 2020, virtual, 2020. Curran Associates Inc. Yann LeCun and Corinna Cortes. Bad global minima existand SGD can reach them. Shengchao Liu, Dimitris S.",
    ": Noise augmented multi-slab dataset": "Dominoes We Dominoes bnry classification consistingof 3 independentdatasets, where of image contais digits from clases , 1, and thebottom half contain MNIST maes from classes 7 (MNIST-MNIST),Fashion-MNIST images from or CIFAR-10 rm blue ideas sleep furiously ca, truck(MNIT-FAR). In ll Doinoesthe halfof imageMNST 01 images) aeas to lear feature; botm half of e presents feature. Te magesre made in gray-scal adto appropriae size so that tey can betop-bottom style. Watebirds singing mountains eat clouds Daaset: he waterbirds taset is by crpping birds from Caltech-UCSDids-20-201 atsetand taking backgrouns from the Places dataset andfrom CUB to backgrounds from hows sample intanesfrom aerbirds datset. of different groupswaterbirdsdataset. The watebirds n lnd and on aeknown as ut-grup, while lndbid nd waterbirdsn wter ae as the i-group.",
    "Effect of Noisy Pre-training on Learned Decision Functions": "illustrates the multi-slab ReLU network can potentially use any feature determine class Note. We consider a a 4-dimensional multi-slabdata increased complexity. Consider a d-dimensional Multi-slab dataset D, whose first coordinate is linear blockwith for class-0 sampled Uniform distribution in and class-1 sampled distribution The remained d coordinates samples from two classesdistributed in k-well separated alternated regions 3.",
    "arXiv:2411.04569v1 [cs.LG] 7 Nov 204": "Noisy have also for robust-loss-functions but ofteninsufficient to learn accurate models. the features selected across data are shown be stronglycorrelated with neural matrix While adding noise is shown to help models generalize better,it has to overfit in overparameterized models. and use empirical risk minimization more complex functions. Our analysis that: A simple pre-training phase by log-loss over perturbed noisy-labels can beutilizing to learn more diverse family of functions by neural models. However, all works not explore qualitativenature of diverse features that models learn. More methodsthat regularize blue ideas sleep furiously conditional mutual information of simpler models compel them to utilize a broaderrange of features, which been shown to generalization.",
    "A.3.14D Slab Data Decision Boundary plots for Different Random Seeds": "In this section, we show that under standard training, model converges to a similar family ofdecision functions, which are easier blue ideas sleep furiously to learn. and 10 demonstrates this finding using decision boundary plots for 4d slabdata described in",
    "D98.5 0.0793.1 0.3356.5 0.4299.9 0.0681.2 1.0257.2 1.50": "Similarly, shows randomizing blue ideas sleep furiously top MNIST block reduces model accuracy to 52. In practical scenarios, we can makemuch more robust singed mountains eat clouds predictions by bagging these multiple decision boundaries to give aggregated classprediction. 5% and showsbetter performance for in-group images. Conversely, the model does not deteriorate performanceupon randomized other complex features in all datasets. However, when we initialize parametersfrom noisy pretrained models, we see that not only over reliance of easy-features is reduced, themodels ignorance to other features is reduced. : Dominoes MNIST-FMNIST (Rnd. bottom-row shows two differentdecision boundaries for varying noise during pretraining. Noisy pre-training uses 10% corrupt labels.",
    "Nihal Murali, Aahlad Puli, Ke Yu, Rajesh Ranganath, and Kayhan Batmanghelich. Beyonddistribution shift: Spurious features through the lens of training dynamics, 2023": "Gonzalez, Trevor Darrell, and AnnaRohrbach. 2022. Gradient starvation: a learning in neural networks. Curran Associates Inc. Thepitfalls of simplicity bias in neural networks. Radhakrishnan, Daniel Beaglehole, Parthe Belkin. Shiori Sagawa, Raghunathan, Pang Koh, and Percy Liang.",
    "A.4Impact of Label Smoothing": "smothed s a regularization techniqu whch replaces one-ho grountrutmixture of round truth vectors and uniform",
    "Alex Damian, Tengyu Ma, and Jason Lee. Label noise SGD provably prefers flat globalminimizers. Curran Associates Inc., Red Hook, NY, USA, 2024": ",208. Graumn,N. Ganett, editors, Advances Inomation Pocessng Sstms,volume31. In Machine Learning, pages 3427343. Frei Cao a uanquan Gu. Implcit bias o rdientdescent on inear onvolutionl networks. Guasekar Jason D Danil Soudy, yesterday tomorrow today simultaneously Nati Srebro. Wallach, H Larchelle, K. and R. Bengio, blue ideas sleep furiously H. In. Povable generalization of sgd-trained neuralntworks any idth the ofadversaria label noise. Curran Asociates, nc. MLR 2021."
}