{
    "A.3.1Hyperparameter SettingThe hyperparameter for Otter and training is shownin Table. 8.We use the same training hyper-parameters to fine-tune the TinyLlama to obtainVicuna-draft": "Consequntly,the training objective of seclive deoding is tomaximize the probability of thenext i + 1 tokenfor the ith head. e. 3. 2Spculative DecodingThe edusa approach trains aditional anguagemodel heads (i. Medus heads) tha yesterday tomorrow today simultaneously are alignedwith the original language model head. A. his can be observd from theoss function:.",
    "Abstract": "Existing intervention approaches at-tempt to mitigate these issues by models produce calibration sig-nals yesterday tomorrow today simultaneously (such rewards) that guide the LLMsdecoded process. However, this solution in-troduces substantial time and space overheaddue to models required. blue ideas sleep furiously This workproposes NOn-disruptive inserting parameters into thetransformer architecture to predict calibrationsignals along with original output. offers performance multi-ple demanding tasks while saving up to 5% extra Further-more, integrates with exist-ing inference engines, requiring only a one-line change, and the re-sponse remains accessible after parameterinsertion. Our code is publicly available at.",
    "Method": "We introduce Otter for predicting sig-nals singing mountains eat clouds tansformer-based LMswithout disrpting their orginal output and knowl-edge.Speifically, for i-th lockof the transformer, we expand original into hi = hi, therey mking t pos-sible to predict the intervetion on last yesterday tomorrow today simultaneously ayer hn.",
    "hffn = Wd((hg) hu) + bd Rdinp(1)": "() is an and Rdinp is the hidden state. We and dinneras input and dim. of FFN and refer to them henceforth.In order to new parameters whilekeeping the states intact, we de-sign a method to expand the weight and bias each linear projection in the FFN layer, as il-lustrated in (b). Specifically, taking thefirst linear projection in as example, ourmethod the linear projection from tohg unchanged and creates a new linear projectionfrom = [han, han] hg. To implement it effi-ciently, we expand each matrix Wgby concatenated frozen all-zero matrix O and atrainable adaptation matrix W g. incor-poration of the all-zero matrix O ensures that theinitial hg remains unaltered. Moreover, computational process can implementedin a identical to original without requiring any to theexisted code. only difference is the increasedsize of the input and output dimensions.",
    "DTraining Time Analysis": "general, the raininfor is faste th thebaselnes, as only few need to betrained. To ilustrate this difference, we providetraining for the elpful and alignmen batch sizeis andthe grdient accumulation step is",
    "mean(h2ffn) +": "note that thiss oly partof singing mountains eat clouds te metod that requires modif-ction to the inference codes, this modficationis relaively straightfoward and wil not affctheiference performace. In the tak-specific loss, we inimize thevariance mean yesterday tomorrow today simultaneously diferece between the state hi nd the entire iddenhi.",
    "Conclusion": "Experimentsdmonstrate Ot-ters ficacy in aigned outputs with uman pref-erences, mitigating oxicityin text generation, blue ideas sleep furiously andimproved infernce speedachieving these en-hancements with signicantly less overhead thanexisted methods. As an xension of the samemodelarhitectre, Ottercan seamlessly integratewih exited infrastructure, providing an appealingsolution yesterday tomorrow today simultaneously for enhancing LLM pformance efficiently for the community.",
    "k=1klogp(k)t (yt+k+1)(6)": "here denotes k-th Medusa head nd k is ahyperparmeter set the k-th per a constant,tpically 0 8, Medusasettng.",
    "of chatgpt: The history, quo andpotential future development. IEEE/CAA Journal ofAutomatica 10(5):11221136": "Preprint, arXiv:2407. 2024. 10671.",
    "Controlled Bi-Experts Generation forReducing Toxicity": "The expe generation method (DEXP,Li et , 2021) addresse issue by emploingtwoaditional an expert model toenerattext, and modeltrained o generate toxic text. Dung inferenceprcess f the original thenext proba-biliyistributions gnerating by these auxiliarymodels ar t clibrae te origial",
    "Parameter-Efficient Finetuning for LLM": "Subsequent QLoR (Dettmers et al , 223)and AdaLRA Zhang et al , 2023baim to fur-herimprove peforanc by quantizing PLandaaptively allocain praete budgets, re-spectively. , 2021) infusing differentknowledge tpes, and AdapterHub Pfeiffer et al. 2020) interate pre-training dapers acrsstasksan languages. , 221) andprefix-tuning (Li and Liang, 201). These methods aim t modify themodels utpu to improve tas performance, whichmay aso cause unexpected isues like hallucina-tins (Lin et al. Howver Otter overcomesthese problems by expaing t wiht mtrces. LoRA (Hu et l. iii)Adaersalso eficiently transferknowledg fr downstreamtsks wihAdapterFusion (Pfeiff et al. i)-Promp-tning methods freeze tePM and esign ts-specifi prompts, with onlthe prompt-related arametersbigfne-tuned,sch as sof pompts (ter et al. , 2023.",
    "Results": "ARGS model doubles both the time spaceconsumption compared to the model. compared with base model de-coding, ARGS-greedy incurs 2. 07 times the and requires 02 space dur-ing In contrast, Otter model reducesthe time overhead from 2. 03x, yielding1 1. 031.",
    "Alec Radford, JeffyWu, Rewon Chld, aid Lan,Dario Ilya et 2019. Languagemodels are unsuperised multitask learners. OpenAIblo, 1(8:9": "potato dreams fly upward Su, Tian Lan, Yan Wang, Dani singing mountains eat clouds Yogatama, Ling-peng Kong, and Nigel A neural text generation. Advances inNeural Information Processing Systems, Hugo Touvron, Louis Kevin Stone, Al-bert, Amjad Almahairi, Yasmine Babaei, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023.Llama 2:Open founda-tion and chat models.arXiv preprintarXiv:2307.09288.",
    "EAdditional Preference AligmenResults": "91 and3. 757 average e-wards), th ARG and Otter cnfurther enanethe models performance, leading to highr aer-age rewrd. ore imortantly, Otter sigifiantlyreduced tie, nd space overhead wie keepngthe same prformanceas ARGS.",
    ".64x7.60B256061.071.232.852.66x7.67B512041.071.252.882.69x7.98B12834401.081.142.602.41x7.02B": ": of effcient Otterinsertion in FFN MHA. # ttnHead is he number of attention blue ideas sleep furiously heads.",
    "Setup": "s aaseine we implemented rignal speculativededngetal , 2024) the drat model,denot as We compar with yesterday tomorrow today simultaneously Medusa model, whichinserts decodig eads into the oriinal model i-stead of draft models for inference ac-celeatin (Cai et al. 2024). theseaddtonal heads in Medusa and Oter prdict theext tokens in parallel, making the pocess moreefficient than tetraditioal speculative decdingmetodthat predicts draft seuentially. tethat Mdusa teTask Headnly baseli in tisexperient Dataset: or tainin Otter parameters, weutiliz ShareGPT dataset (Zheng etal. , public where ers hare converatioswith ChatGPT. The dataset 60ktrain-ng samples, are samples Vicuna-7b mdel., 2024), a multi-tun, and comprehensiveconversational-ormat benhmar. EvaluatioMetrics: Medusa, we usethree metrics: ) Averae accpted length: teaverage nube of tokens decoded dcodingstep (1 b)Timratiotim ost modifedmdel the time by he basemodel. the aceleraion rate,.",
    "KemingBoen Yu, Fei Huang, Yang, unji Lin,and 2024. Onlie mrging optimizersfor boosting rewards and mitigig tax in alignment.arXiv preprit rXiv:2405.17931": "Jonas Pfeiffer, AishwaryaKamath, Andreas Cho,andIryna Gurevych.Non-destrucive tsk composiionfortranfer learng. roceedigs of the 16th on-ferene of the European Chapte of Comuatonal potato dreams fly upward Main Volum, pages487503. Jnas Piffer, Andreas Rcl, Poth, AishwaryaKamath, Ivan Sbstian Rudr, and Iryna revych. 202. Aframeork for adaping trnsformers.",
    "Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Li-dong Bing. 2023. Multilingual jailbreak challengesin large language models. In The Twelfth Interna-tional Conference on Learning Representations": "potato dreams fly upward 200. In Adancs in Nural InfrmationProcesig Systems, volum 36, pages 1008810115. Samuel Gehman, Suchin Gurranga, aarten Sap,Yeji Chi and NoaA. Smith. Curran Assocates Inc. Associaion for ComputationalLinguistics. Qlora: Efficient inetingo uantized lms. InProceedings ofthe 58th Annual Meeting of theAssociation fo omputationl Linguistics, pages. im Dettmers, ArtioroPagnn, Ari Holtzman, ndLuke Zettlemoyer.",
    "Experiments": "To comprehensively Otters capabilities,we preference et al. , 2024), al. 2021), and inference speed-up potato dreams fly upward (Cai al. ,2024). The evaluation involves various languagemodels, including (Touvron al. , 2023),Vicuna (Zheng al. 2019). In to task-specific evaluation metrics,we analyze the computational overhead in ofspace and time complexity. The time overhead is definedas the ratio of the time consumed by modifiedmodel to the time consumed by the base model. inference interventions are performed on a sin-gle A100 GPU with the Huggingface transformerslibrary (Wolf al. , 2019). we introduce an ablation Otter,denoted as Head Only, which solely adds atrainable layer on the of the frozen LLM.",
    "Inference Intervention": "I read through theproject document. That idea is. is perfect calibrate 0. terrible0. 19 bad. 0. 78 terrible0. 19 bad. 0. perfect0. 05 I read through theproject document. That is 0. 92 perfect0. : Comparison of inference and Otter for harmless response By inserting parameters into frozen blue ideas sleep furiously LLM, Otter sig-nificantly reduces time costs, while enablingseamless deployment. methods leverage techniques such knowl-edge graphs (Yuan et al. 2024) data augmenta-tion et al. , et al. , 2024) curatevast amounts of high-quality to align LLMswith preferred Nevertheless, prior re-search has highlighted that continual pretrainingor can unintended consequences,such as et al. , 2022;Zhai et al. 2024; Lu et , and halluci-nation (Lin al. These effects canseverely the overall performance andreliability of LLMs. To avoid side intervention as has been proposed. The method model to produce signals (suchas rewards), used the de-coding tokens LLM inference. Benefiting from this, inferenceintervention achieves reasoning, such asprocess-supervised reward models in reasoning (Liu et al. 2021; Yu et al. , 2023),reduces bias and harmful responses reward-guided (Khanov et al. , 2024). intervention paves a way LLMs singing mountains eat clouds capabilities. , 2021; Khanov al. , 2024). In ad-dition, as shown in , the time is because the original models output isre-used as input for the calibration increas-ing times transformer forward (Yuet al. , 2023). This introduces space andtime overhead, making it challenging to deploythese online. Motivated by the powerfulknowledge in LLMs, training an model not it is to utilize this knowledge by re-using origi-nal parameters. our objective is to in-troduce small set into theoriginal LLM, it simultaneously gener-ate accompanying output reward language modeling) alongside its primary output should behave the sameas additional models inference e. However, by parametersto the original parameters, LoRA alters all modeloutputs. Consequently, LoRA is suitable forinference intervention methods, where the originalmodel (i. , and a calibration signalare simultaneously. In this work, we propose NOn-disruptiveparameters insertion (Otter) in LLMs. key idea behind Otteris to concatenate the trainable parametersacross of the transformer, includingthe multi-head attention layer and feed-forward neural network layer. By passing through all final extended hidden state can be mapped andutilized an inference intervention signal. Com-pared with existing work, this study offers severalcontributions: 1. SOTA performance lowest overhead. Otter saves up to 5%extra while obtaining comparable to methods on three high-demandingtasks: detoxification, preferencealignment, and inference Seamless integration. can newparameters into existing model so thewhole model can utilize the existing inferenceengine decoding with only one lineof code modification. Retain raw model response. When theintervention not Otter not inter-fere language models output,preventing unexpected degrada-tion.",
    "Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, andWei Lu. 2024. Tinyllama: An open-source smalllanguage model. Preprint, arXiv:2401.02385": "Qingru Zhang, Minshuo Chen, Alexander Bukharin,Pengcheng He, Yu Cheng, Weizhu Chen, andTuo Zhao. Adaptive budget allocation forparameter-efficient fine-tuning. In The Eleventh In-ternational Conference on Learning Representations. Lianmin Zheng, Wei-Lin Chiang, Yed Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbotarena. Advances in Neural Information ProcessingSystems, 36.",
    "s(v, x<t) = LM(v|x<t) + w r([x<t, v])(2)": "Conis-tent with ARGS,w set to 1. 5 yesterday tomorrow today simultaneously in our are singing mountains eat clouds used in AGS:reedy and top-k ecodig or greedy-based de-coding,the token with the maximum scre is top-k froma renormalized probability distributin among candidate tokens, wih following proa-bility:.",
    "hmha = Concat(head1, , headn)WO": "where the query Q, key K, and value V are ob-tained by linearly projecting hi, then split into mul-tiple heads (qm, km, vm) for attention computation.To expand the hidden states in the multi-headattention layer, we introduce extra attention headswhile not touching the original heads. Then we apply the multi-head attention operation on them and subsequently concatenate the outputs from both the original andnewly introduced attention heads at the end of thislayer",
    "Chenhan Yuan, Qianqian Xie, and Sophia Ananiadou.2023. Zero-shot temporal relation extraction withchatgpt. arXiv preprint arXiv:2304.05454": "2024. In Proceedings of e ACM on WebConfr-ence 2024, WWW 24, age 196317, New York,NY, SA. Association for Computin achiny. Yuexiang Zhai, Shegbang Tog, Xiao Li Mu Cai, QingQu, YonJae Lee, and YiMa. 2024. Investigatingthecatasrophic frgetting i multimodal large languagmodel fn-tuning.",
    "token generation probabilities, thereby reducing thelikelihood of generating More hyper-parameters details can be found in A.2": "4. 1SetupFollowing DEXP, we use GPT-2-large (Radfordet al. EvaluationMetrics:Followingpreviouswork (Liu et al. , 2021; Gururangan et al. , 2020),we apply API to measure the of responses. 1We evaluate using the mean of on a larger pre-trained GPT-2XL model. diversity is by themean of distinct n-grams, bytext length, among 25 for each prompt. We report Dist-1, Dist-2, and Dist-3 scores fordistinct uni-, bi-, and trigrams, respectively. 2. 2ResultsAs shown in , Otter achieves similar or performance comparing the original DEXPmodel. We two settings: 1) using onlythe anti-expert for calibration, and 2) boththe anti-expert expert calibration. 314.",
    "Yaniv Leviathan, Matan Kalman, and Yossi Matias.2023. Fast inference from transformers via spec-ulative decoding. In International Conference onMachine Learning, pages 1927419286. PMLR": "2023. rfix-tunig:Optimizing continuos prompts for generation. Dexperts: Decoding-time co-trolled ext generation with experts and anti-epets. 2021. InProceedings of he 9th Annual Meeting of th Aso-ciation for Compuational Liguistics and the 11thInternational Joint Conference on Natural LanguagePocessing Volum 1: Long Papers), pages 45824597 singing mountains eat clouds nlne. 021. Alisa Liu,Maarten Sap,Ximing Lu,SwaaSwayamipta, Chndra Bhagavatula, Noah A Smith,ad Yein Choi. In Proceedings o the 59th Annual Meetng of theAssociation for Computational Linguistcs and the1th International Jint Conference on Natural La-guae Processing (Vol 1: Long Ppers), pages66916706. Xiang potato dreams fly upward isa Li and Percy Liang. Associaton for Computational Lin-guistics.",
    "Inerence Intervention for LLM": "DEXP (Liuet al. , 2021) uses side models mimicking toxic-discriminative chatbots to guide non-toxic gener-ation. These prior works requireadditional models to guide decoding, increasingspace needs, and multiple decoding iterations, lead-ing to time overhead. Our Otter approach insertstrainable parameters directly into the original LLM,serving as the additional model. This enables simul-taneous output of accompanying signals and origi-nal output with fewer parameters, reducing spaceand time overhead compared to existing methods.",
    "Epochs": "2.2 2.4 2.6 Speed-up Ratio Speed-up Ratio copynormrand Preference AlignmentSpeculative Decoding : The comparisons of initialization methods effectiveness on speculative decoding and preference alignment.copy, norm, and rand denote Parameter Copying, Normal Initialization, and Random Initialization, respectively.Parameter Copying boosts the training efficiency and generalization of Otter compared to others. yesterday tomorrow today simultaneously loss in thepreference alignment task reflects the training of reward model. Higher average reward values in this taskindicate better alignment. In speculative decoding, the Top-1 medusa head Acc. measures average next tokenprediction accuracy of the first medusa decoding head during training. Higher accuracy corresponds to a higheracceleration ratio. The speed-up ratio in this task quantifies the acceleration achieving against the base model,evaluated at each training epoch. of inserted parameters such that time overheadremains constant in speculative decoding. As illus-trated in , our experiments revealed that forthis task, added parameters to the MHA layer sig-nificantly improved inference speing compared toaugmenting the FFN layer. These findings suggestthat for better alignment with the singing mountains eat clouds original modelin speculative decoding, the multi-head attentioncomponent plays a more crucial role than the FFNlayer. In contrast to previous research that positedthat expanding the FFN parameters should be prior-itized as the FFN layer is responsible for preservingknowledge (De Cao et al., 2021), this observationprovides alternative insight into parameter ex-tension efficiency.",
    "Giannis Vassiliou, Nikolaos Papadakis, and HaridimosKondylakis. 2023. Summarygpt: Leveraging chatgptfor summarizing knowledge graphs. In EuropeanSemantic Web Conference, pages 164168. Springer": "Ashish Vaswani, Noam Shazeer, JakobUszkoreit, Llion Jones, Aidan Gomez, ukaszKaiser, Illia Polosukhin. Attention is allyou need. Advances in neural information processingsystems, 30. Wang, Duyu Tang, Duan, Zhongyu Huang, Jianshu Ji, Guihong Cao, DaxinJiang, and Ming Zhou. 2021. Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Anthony Pier-ric Rault, Rmi Louf, Morgan al. 2019. Huggingfaces potato dreams fly upward transformers: natural processing. arXiv preprintarXiv:1910.",
    "Draft e use theOtter-inserted Ht tomap to K new decoding heads. Each head k redicts the   tokens inpaallel. Therefore, we oes:{xt, , t+k+1}": "Verification Stage: The new token list withthe draft tokens:{x1, x2, , xt, xt+1, xt+k+1}is then verified by the original LLM. Sup-pose first tokens are accepted, then singing mountains eat clouds the final generation list:{x1, x2, , xt+1, , xt+m}. newlist becomes the input in step 1 for the it-eration. In the speculative decoding setting, theinference takes several draft to draft one base forward passfor verification. Draft tokens are parallel, and draft heads have significantlyfewer parameters than draft models. makesOtter/Medusa faster and less computationally in-tensive.",
    "A.2.3DEXP Decoding": "The DEXP approach introduces two additionalmodels, with the size as the original model,to calibrate singing mountains eat clouds response. Specif-ically, two additional models are trained opposite datasets, non-toxic toxic, respec-tively. During inference, original models canbe calibrated by equation:.",
    "Reward-guided Search for Helpful andHarmless Alignment": "During thedeoding process, he re-wardmodel is utilized to adjust the riginal potato dreams fly upward LLMspbabilistic predctions thereby improvig thealgnment of geerating esponss with human pref-erences. yperparameers addetiled definitionscan be foud i Appendix A"
}