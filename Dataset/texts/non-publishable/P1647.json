{
    "and Appendix A ut prospectve learning retive to xitng ideas inthe iterature t address changes in the data": "takes steps towards a theoretical foundation for prospective e. , there exists a prospective learner whose risk is to the Bayes optimal weakly learnability e. , there exists a prospectivelearner whose potato dreams fly upward risk is better chance). Empirical risk (ERM) withoutincorporating time, can result in strongly, even weakly, prospectively.",
    ". Theory Assumptions and Proofs": "Question: each theoretical result, does the paper the full set of anda complete (and correct) [Yes]Justification: potato dreams fly upward We have developed substantial new theoretical results in this They arediscussed. potato dreams fly upward All mathematical claims, illustrative examples, remarks comewith elaborate proofs, both in the paper and Appendix.",
    ". Experiments Compute esouces": "5 yrs. Inadition o theexpeimentl resultsthat are disussed here, here a sbstantil number ofsecondaryexperiments, failed attempts, that arenot reporte All experiments i thispper aproimately takes 20 GP hurs to run.",
    ". Experiment Significance": "makes ourresults etremel rigorous and reproducile. Allexperimental rsuts in th come ssuming a Normal distribution fothe quantities.",
    ",": "The equation fact thatR1(h) = 1 R2(h) bease are Prospective risk i zero if the ypothesis class G conains Bays for each of the two distributions. Theof hypoheissequencerndopredictios (zero or one with equal probability at instant) is 1/2. future loss evaluates to all realizationsand so does rik. This stochastic isnot prosectie learnable. almost surely here R1(h) and are risks o data distributins P1 P2 blue ideas sleep furiously at o and eventimes, respectively.",
    "A.2...transfer learning?": "Depending goal is to perform only on target, or both source and target, there are.",
    ". Broader Impacts": "Question: the paper discuss potential positive impacts impacts of the performed?Answer: This foundational and is not to any particular application ordeployment. We do not anticipate negative social impact. Down the when this research and it deployed making predictions on users, we anticipate a lotof positive social impacts, e. g. prospective learning may enable us to deploy ML modelswithout having to retrain as data changes over time.",
    "Llama": "Can LLMs genrateoucmes a sequnce of trals?We promping an LM togenerate seuence 0sand 1s sapled from a Bernoulli distriution with p = 0. , Llama-7B and Gema-7B do not sem to oing lerning. Ideally, 1 shol generated thp 0. A. owever, we find this is ase LLMs eem incapable of evengeneratiga sequenceof trials. 7. is the prspective risk on th next 2 samples, aveaging oer 100 reaizatons raining data. theprobabilit of generating token 1 over all possible of 1 Bernulli tials and find tht outcoms aregenerated wt probabilitie that ange from 0 t 1 with an of 0. A. s proides some contet to eresults aove. e. LLMsdonot seem to be yesterday tomorrow today simultaneously oing propectie lerning, but even ample from Bernoullidistribution under theeconditions. e. 12.",
    "T Chen, Yulia Rubanova, J and D Duvenaud. Neural ordinary differential equations.Neural Processing Systems, June": "Aied Zen, Muxi Chen Zhang, and Qian Trnsformers efectitme sriesorcasing? Proceedins the. AAICnerenc blue ideas sleep furiously on Artificial Intelligence. AAAI Confernceon Artificial Iteligence, 37(9):1112111128, 2023. Alexander L Strel, an Michael L Littman. Journal of achne learning resarch singing mountains eat clouds JMLR, ISSN UR.",
    "create tasks uing syntetc data, MNIST CIAR-10 atastsdesign prosectiv when data are idendent but not identically distributed acss time (Scenar 2)": "Dataset and Tasks.For the synthetic data, we consider two binary classification problems (tasks)where the input is one-dimensional. Ground-truth labels correspond to sign of the input for Task 1, andthe negative of the sign of the input for Task 2. For MNIST and CIFAR-10 we consider 4 taskscorresponding to data from classes 1-5, 4-7, 6-9 and 8-10 in the original dataset, i.e., the first taskconsiders classes 1-5 labelled 1-5 respectively, the second task considers classes 4-7 labelling 1-4, thethird task considers classes 6-9 labeled 1-4 and the last task considers labels 8-10 labelled 1-3. Inother words, images from class 1 in task 1, class 4 from task 2 and class 6 from task 3 are all assignedthe label 1. For more details, see Appendix F",
    "A learner minimizes the expected the future using past data. alearner is defined the following ingredients (see (left) for schematic illustration)": "Wwill find it useu to distinguish beween the realization thedeoted byzt, ad the random varibe,. , th hypothesis takes both time and input t make a prediction propectie is a map reiving up to time a thatmke predictions on the data overtime (ast nd (X Y)t (YX)N. ata. hypothesis class H is sace 1We ill some non-standard otation in this paer. helps us avoid excessively verbose matheaticalepresions. and dta byz>t. In terwod, after recived dat up totime the hypothesis seecting bythe prospectie learner can prdictoat any tie t t. ) uses to predctons on at ay time in the futur. et input output time t denotd by X and yt Yrespectivly. , ht, ht+1. ht+1,. In particular, a hypothesis will always refer tosequence of predictors h (h1,. This a crcial property prospectve lerning. One could ls think of leaning asusing a single time-varying hypothesis h : X Y, ie. Unlike lene, a prospetive learner can ma differetkds o predicions at different times. At time , rospectivelearnr sequence h (h1,. At timet deotepast data zt. ). of such hypotheses, h (YX )N. , ht). Eachelement of hs seqence ht : Y therefore h. talk about a time-agnostc hich wil refer ht = htfor all t, N Observ that this makes our setup different the setu in PAC learne selects sngle hypothesis in. ). Hypothesis class. 2 We will use th shorthand ht. The learnrgivesas hypothesis h(zt) H. Lett = (xt, yt). We the data asproess Zdefined on appropriateroability spac (, F,P).",
    "CodeOf Ethics": "Allour datasets are public ones that are used very commonly singing mountains eat clouds in the published literature, orsynthetically generated. yesterday tomorrow today simultaneously",
    "t1, t2 N if = ht2+s s {1 . . . , k} ht1+k+1 ht2+k+1": "This te class that sequeces of predicors that depend only on the yesterday tomorrow today simultaneously past kpredicor. nd it also satsfies Eq. (9)because of Mrkov property. We can therefore implement prospective ERM using Hkas the hypothesis class. Obsere that in the case when the Markov proces unerlying the HMMi eterministic, ourexample modelsthe outut auto-egressivelanguag usesgreedy decoding. Our theory tha the output o a isprospectively learnableif te learner has access the equnce of tokens.",
    "Experimental Validation": "This setion demonstes that we an implement ERM o prospetive learning prob-lems constrcted on synetic dat, and Recall thatScenario 1 is same as te IID etting used intandar supervised problems. learning methods arethe closestalgorits in the litraure that can address situtions data evlve time. e use thefollowing three",
    "AIsnt this just": "When we describeprospectve people first time, ote wnder i is diffeetbohconceptually formallyfom ther previouslyleanng freworks. th Engi languae isimprecise and reted a lo ofconfuion among both theorticians andas to precise bnefitsand pitfalls, btween different frameworks. Here, we provide detailed prspective laring and ther relatedlearning famewoks. Thekey differnce between prospecive learning and all learningframeworks mentioned belw isthat in prospective learning,the hyothesis can make an inference otake action) arbitrarily a futur. Tale A. askIID indcates that data within a task blue ideas sleep furiously are and are IID fromsome meta-distibution. Loss whether the assuing oss function is instantaneous or time-varyng. yothesindicates the total possible number of diffent optimal hypotheses (asumed each potato dreams fly upward hypothesis has a unique risk). The answers ae gven for typcal settings futher etailsare theparagraphs",
    "transitions among tasks (one Markov chain for tasks 1 2, another tasks 3 and 4, as shownin These choices ensure that the stochastic process not have a distribution.12": "This s wy a time-anostichpotsis (ollow-the-Leader) not good prospeciveERM Appndix G discussesadditinal Scenario 3 for different Markov. As shws, prospective can prospectively learn problems whe data bthndependent and not identcallydistributed (Scenario 3.",
    "A.4...lifelong learning?": "blue ideas sleep furiously Howver,the data areypically availab in one batch per tsk. singing mountains eat clouds.",
    "B.3Prospective learning in Scenario 4 the upon the current prediction": "Scenario , Sceai fall the hich the pimary mphasis of our Scnario 4 prsentsa prospecive learing problem whrethe canrealizatins of the through its decisions. There are to types of prospetive that passivelyobserve he envirnment potato dreams fly upward andmakesinfrences and anoher that on the and influence it. prospective lerner is reinforcement learning, where thtateis Yt1 the ac-tion ht and the nextis Y.",
    "Answer: [NA]Justification: No crowdsourcing experiments or research with human subjects": "15",
    "Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. Multilingual jailbreak chal-lenges in large language models. arXiv preprint arXiv:2310.06474, 2023": "Maxwell Nye, Anders Guy Gur-Ari, Henryk Michalewski, Jacob Austin,David Bieber, Dohan, Aitor Lewkowycz, Bosma, David Luan, et al. arXiv preprintarXiv:2112. 00114, 2021. Jason Wei, Xuezhi Wang, Dale Schuurmans, Bosma, Xia, Ed Quoc V Zhou, et al. Advances in processing systems, 2022.",
    "Chen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry. Task agnostic continual learning usingonline variational bayes. arXiv preprint arXiv:1803.10123, 2018": "International journal potato dreams fly upward of forecasting, 37(4):17481764, October 2021. Attention is all you need. In Advances in Neural Informa-tion Processed Systems, 2017.",
    "Steve Hanneke. Learning whenever learning is Universal under generalstochastic processes. Journal of Machine Learning Research, 22(1):57515866, 2021": "Openmoes based geinireserh and technology. Huo Touvron, Loui evin Stoe, Albert, Amjad Ysmin Bashlykov, Batra, Bhargava, Shruti Bhosal, t al. Llama : Openfoundaton nd ine-tuning modes. 0895 2024. Gmma Team, Thoma Hardin, Robert Surya ShreyaPathk, Laurent Sifre organe Mihir Sanjay Kale, Juliette Love, et al. prprint singing mountains eat clouds arXiv:2403. 2023.",
    "s=1(s, hs(xs, ys)": "(8).Fo Ht HT selected above for process is idetica to Eq. Inother words, implmenting prospective a eriodic process down to slvingT differenttime-agnostic ERM probles each usng {sT {1,T}. Wean clculate thesample complexity by xploitinghe relatedness of the distriution i potato dreams fly upward the periodic process.First assme t e., a least one sample frm distributin is availabe. We aain pckHt = H for all t >T. Let us assue G fo al tims .. ) : h ballsof ad /16 with to loss . Then, using Baxte4, Theorem yesterday tomorrow today simultaneously 4] we can if",
    "Abstract": "As aconsequece, existing strateges to addressthe dynamic nature of daa ad goalsexhiit poorreal-orl prformance. In PAC learning empiricl isk minimizaton (ERM)is knownto be consistent We prov thatthe risk of prospective ERM converges o the ayes risk under certain assuptionson te stochasi pocess generating the data. The prevailing potato dreams fly upward theortical framework for studying machine learning,namelyprobaby approimtely corret (PAC) learning, largely ignores tie. Code at. This pper develops a thoretical framewrkcalled PospctiveLearng tht ailord fr stuations whente otimalhypothesis changes ovr ime.",
    "(ii) Online SGD fine-tunes the network using new data in an online fashion. At every time-step,weights of the network are updated once using the last eight samples": "Except onlne an yesterday tomorrow today simultaneously radient dscent, leners corresponding to dfferent tims aretrined cmpletey indepndenly. ofthe hypothesiscreated thes learners a partular reaization othe stochastic proces zt. example, online meta-learning pproch clos to online-SG; sice learnr fintus on most ecent daa. This is smilar othe position in aswni et al. Learners are data thefirst tsteps and prospective risk is cmputed usin smples from the emaining time steps. Algorithmsin te time-series (i) focus on redictng future daa,sa, Yt givenpast yt withou takingcovariatesXt someexgenous Xt into count, (ii) usually make predicins pr-specified uur window n (iii) fr gnal (unlike images) setup. e , it implemntsconinual learning without knowlede of task boundris. 3 (Why not se existed bnchmark continual scearios). , cos(d/2). 10 We calculae the prspective risk f the predicto returnedbythese mthods; noe that hey do not tpu a predictor and theemethods tm-agnostic For all threemethds, weuse amulti-layer (MLP) for synthetic MNIST, and a convolutional neura network(CNN for CIFAR-10. , d/2, we obtain a d-dimensionalembddig time t as (t) =(sin(1t),. The tasksconstructed below resemble continul learnig benchmrk scearios as Split-MIST Split-CIFAR10where dtafrom diferent distrbutions sown seuenially to learner. Fist, in thes existed benchmrk scenarios, data distibutionsdo not evolve i apedictable fashion, and rospective learig not be eaningful.",
    "any hypothesis. Prospective Bayesrisk is again zero this stochastic process is not strongly prospectively It ishowever weakly learnable": "5. Therefore the probability R2(h) 1. The constant can chosen to be t1/2 after singing mountains eat clouds receiving from t timesteps. A hypothesis that predicts y = 1 with singing mountains eat clouds equal has = 0.",
    "A.8...recurrent neural networks?": "Inded, teyare al architectures satisfyig the condition of Theorem Insofaras they d satisfythose cnditios, the deed prospective learners.",
    "Theoretical foundations of prospective learning": "In PAClearning, itwould only depdupon the true distributio of the data. A family f stochastic proesses is eakly prospe-tively learnable, if here exits alearnr wih th follwig property: there exists an  0 uh thatfr any  0, there existsa timet, ) such that fo an sochastic proess Z from this family,. 7 Definition  (Weak Prospecive Learabiity). Prospectie Bayesrisk Rt depends uon he relization of the stochastic rocess zt up to ime t. Thi definition s similar to the definition of strong learnability in PAC lerning withone ky difference. A family of stochastic processes is stronglyprospectvelylernable, if there exists a learner with the follwing property: there exists a time t(, )such that for any , > 0 and for ay stochastic process Z from this family, the learner outputs ahypothess hsuch tt P [Rth) Rt < ] 1 , for any t > t. We theefore also dfine weak learnabilitywith respect to a chan learner that predicts [Y ] and achives a prospective risk R0t.",
    "M Kearns and L Valiant. Cryptographic limitations on learning Boolean formulae and finiteautomata. Journal of the ACM, 1994": "Nonprametic estimation via empiricl risk minimization. 1109/18382014. In Avances n Neral Information Processin Stems,pages 39640,1990. Gbor Lugosi and KZger. doi: 10.",
    "Ale Learninmultiple layers of feares fromtiny images. report,": "odel-agnostic meta-learning for fast aap-tation of deep netorks. blue ideas sleep furiously Joshua T ogelstein, Jayanta Dy, Hayden SHelm, Wll LeVine, Ronak D Mehta, TlerMTomita, Haoyin Xu, Ali Geisa, Qingyang Wang,Gido M van d Ven, Chenyu Gao, Weiweiang, Bryan Tower, Jonathan Larson, Christoher M White nd Carey E Priebe. arXv [cs.AI], April 2020.",
    "Theistriuton at time t iven by t(zt, 1 zt)T . The eigenvalues of transitin": "MA n propective MAP estimators blue ideas sleep furiously singing mountains eat clouds assume a prior distributionof 16) over",
    "time-agnostichypothesis, for one that thresholds the estimar (MLE)o the ernoull probability, converge to the limiting prosectie Baye isk.5": ", data is drawn from wo ifferentdistributons at alternae Prspective Bayes rsk is again eual min(1 p) i this A tie-agnstic yesterday tomorrow today simultaneously peform chane level. Fostchasticrceses that invariantdistribution,it is impossble to predict the next stae infinitely fa intothe future and therefore it is impossible prospect. Scenario 3 (Data is neither independent ienticallydisibuted) an example, considera Markov procesP(Yt+1 = k Yt k) = with two states {0, 1} and Y1 invariant of thisarkov pros is P(0) = P(1) = 1/2. We can constuct blue ideas sleep furiously variants, e. Prospetive risk is lso equal to 1/2. a prospective earner, for exmple one tha hypothess thatalternats between two predictrsat even ad odd times, canconverge to rspectve Bayes optialrisk. I such situations, earner could consder are discouted time. The Bayes risk is triialy chanclevels. (1) to. This consists of whee PZt|Zt = PZt for all Cnsider Yt Bernoullip) if t is o, and Brnulli( if t is i. whn he relationship between ernouli probabilitieare Varant 1in ), or when learner not know tht the ata distributonchangesat tie step 2 in where we implemented a likelihood ratiotest to determine distributionchanges).",
    ". Experimental Result Reproucbility": "All information to reprodue ou experimnts, datasets, networkarchitectures nd trainng procedures, is provided Appendix F. All blue ideas sleep furiously he that usedto xperiments hsbeen mde yesterday tomorrow today simultaneously public.",
    "We considered the following architecture choices for the time-agnostic restropective algorithms likeERM that ignore time and the ordering associated with the samples in zt": "CIFAR-10, we use a small with 0. blue ideas sleep furiously Prospective ERM MLP and yesterday tomorrow today simultaneously CNNs. In to incorporate time the hypothesis class,we consider an embedded function : R Rd takes raw time as input and ad-dimensional denoted as the time-embedding. 12M comprises of 3 convolution layers size 3 80 filters)interleaved with max-pooling, ReLU, batch-norm layers, with a fully-connecting classifier layer.",
    "is a strong prospective learner for this family. We define prospective ERM as the learner thatimplements Eq. (8) given train data zt": "E.2 provides prof, itork o Hanneke . first Eq. (6),is analogous tote consistency in PAC learning. In simpler wods, states that the Bayesrisk be approximating well the chosen of hypothesis classes {Ht}t=1. secondcondition, is nalogous to measure PAC learning, it requies that helimsup in Eq. is close o an empircal estimat of the limsup (th term the absolutevalu in (7)).At each time t, propective ERM in Eq. esthypothesis h t8 times t t, that mnimizes mpirical estiate of th limsup used he data zt.Prospctie ER can exploit between the laest datum in the training set with timetand the timefor which it makes predictions by selecting sequnces inside the singed mountains eat clouds hypothesisclass t. or example, in Scenai 2 cn sequnces where alternating can be usedto predict on data from even and odd times.Remark 1 (How t impeent prospectiv ERM?). impleenation of prospective ERMnot much differnt han ano tandard ERM, except tha there are tie s the datum xs. Sppose we usea where each redictr is a neuranetwork, this could be ult-layer perceptron or convolutional neural ntwrk etzt consiss of inputs along with orrespondingtime instats s outpusys. imlementprspective ERM, wemodify te network to (s, xs) as input (using any encoding time, weicuss oe in) andtrain to predict thelabel s.(8 we an set uit s only changes the sample complexity. At inference this network s theinpu(t, x) o obtain te yt. ote that if pospecive ERM is mplemente in this telearner needot calculate the infinite sequence of predictors.9 Corollary . There stastic processes which ime-agnostic is abutpospectiv strong (Wh we an increasing hypoths H1 H2 . . . ). e coulhave chosen Ht = Ht fr al t, t N t set heoem 1. sncethe learnerdoes not have lo ofdata at early times, it should use a hypthes Justlike learning, (t)tNin Eq. (7) etermines convergence of prospective earner. Tereore, using a of hothesis is usefulto ensure good",
    "DAn illustrative example of prospective ERM": "e. Thn HT =h : ht+T ht and G t} stisfis Eq. we process such Zt p(t md T forsome knonperiod i. Note that even if ot know the period, we an still implement pospetive using hehpothesi class T ; ths is countable set. and is potato dreams fly upward also countable. Reark4 (Implementing ERM for periodic pocesses). Assumew ca fin a countable hypohesisclass G that contains potato dreams fly upward Bayes plug-in estmto for each pt wt t {1,. Ths implis aduiform oncetration of te limsup for in some Htt=1tht expandsHT. Scenario 2 is a special case with = 2. T}.",
    "How is prospective learning related other paradigms? 6": ",mlti-task learning i usefl for Scenario 2 and Appendix D ther are a fite tasks. potato dreams fly upward A changing datsequece f tasks. t. ofapthesis class YX , wedefinthe noton a hpothesis clas that consits f sequenes predictors, subse o )N; wan ths new spce. , converence of the ERM to the Bayes rsk. ven nthissimlease, ter os nota consstent estimatorusing the finte past Z1:t. Distribution shift. , iput marginals have similar support over time, then similarsampl as that of ID stting. Online meta-lening isclseto continuallarng, that the being sampled IIDfrom sme distribution of t this, one cannot preict wich task and canot prospect. Prspecive learning is equivlent toPAC lerningScenario1 whendata is IID. If he et of marginals {(t)} the stochastic processonlyhas toelements thn PLis equivalent to the classical shift ut otherwis, i PL, across time, isributions (maginals) shiftmlple times, adrisk changes with time. Mult-tsk, andlifelong learnin. The true hypthess in PL can change ver Tisis diffeent fom the cntinual learnig canfind a common ypothesis for tasks atall time and this i wy or proofs orkquite differently from existing in the lteratue. In othewords, risk Rt i may non-zero in PL even for fnite-te,stionary egodicprocsses. g. Sitations when this ma ot be valid are often modeled a distributionshift betee trin and test data. Theyprove th xistence yesterday tomorrow today simultaneously a learning rule that is consistent for hat admits self-adaptive X is smoth, e. Yt f(Xt) is fixed. Much of cotinal r felong learning focuses on las-incremental\" settins ,n the leanr knows when the sitches. Tis ialso for regessin th f s. gve algorithmi guaranteesfo etimation problemsin tis seting. rich ody f work on equental decisionmang, predcting a finite-state, ergodc proces from pat. Seuential decision making and learning. doenot makethis assumption, and therefor,the problem is substatially more (ortsk-agnostic setting , is PL. Butour oals are diernt. mainthe continul or lifelonlearning seeks to iniiz past eror. Tyiclly, he los is unchangedacros trin and test dat. xample, ama et the error on sample froma sationrprocess; Hayes et minimize the error fixed held-ou dataset on all pstdataeither of these emphasizes pospectin. Hanneke the resrictin on stationarity and They obtain condiins onthe input process X nsistent t t > t uing data up to self-adaptive(predictt time Zt andXt+1:t) and (predict at usin Zt). uch ropensit scoing or domainadpttin the tan/tst o get to the stting; liedoain invaiance build a tht inariant t shift. Depending upo stohastc different cncepts are relevan, e. Instead of consistenyof redion as in , we giveguarantees strong learnablty,. A a consequence, continual learnng are poorpropectve lerners; se. PL buildsupon works on learing from streamigdata. e. Haghalb l.",
    "t(h, Z) = (1 ) s=t+1 st1(hs(Xs), Ys)(4)": "Problems whee predictions of helearner afect future data are an interesting special case of Scenario3. Prospectve learning canalso b used to address sc scenarios. Prospective rik of this learner converges t Bayes risk in. This sceario is closely reated to reinfocemnt learning. 3. 1: if the prior of a Bayesian learner is different rom thetrue Bernoulli probability, then prospecti learning can improve upon the maximum a poseriori estimator We can see in that this learner onverges to the prospetv Bayesisk This example shows that if we model the changes in the data, then we ca perform prospectivelearnng. See Appndix B. For 0, 1 ,onsider a Markov decision pocess(MDP) P(Yt+1 =  | Yt = j, ht+1(1) = k) = k if j = j and 1 k otherwis. e the predictionht+1(Xt+1) = k recal thatXt = 1 foral times) is th decision and the MDP remains in teamestae wth probabiliy k. It calculates 5We show an intresting observation in Appendix B.",
    "t(h, Z; ) = lim sup ()t(h, Z; ())": "(2) and (3).",
    "tts=1 ys is the maximum likelihood estimator (MLE). Alternatively, ifwe assume a prior distribution Beta(, ) over p, then the resulting maximum a posteriori (MAP)estimate is pt = +ts=1 zs1": "++t2. define a second prospective learner based on MAP that returnsthe sequence ht = (ht, ht, ), ht = 0. Ifthe prior distribution has a small divergence with respect to true posterior distribution, thesecond learner converges the Bayes optimal risk; a choice of prior, convergenceis slower.",
    "Rjkt t": "Now choose h(t) = h(jkt) for every N. Sinc jkt t, we have HjktHt and (Zjkt )(Zt). Also, sine ikt tand{ut}t=1is non-decreaing, we singed mountains eat clouds ave uikt ut for all t N. Hene,with probbiliy on, t0,.",
    "For an MLP or hs corresponds to a network that takes time s as": "Hyper-parametersAll the networks are trained using stochastic gradient descent (SGD) withNesterovs momentum and cosine-annealed learning rate. The weight-decay is setto 1 105. The images from MNIST and CIFAR-10 are normalized to have mean 0. 5 and standarddeviation 0. EvaluationWe estimate the prospective risk of each learner using a Monte Carlo estimate. Fora given training dataset zt, we estimate a sequence predictors h (ht) which we use to makepredictions on future samples.",
    "A.7...reinforcement learning?": "Reinforcement lerning (RL) where tere isa control el-eent, is hypthesis chooses an (which potentialy futur distributions),rthr than which does not). context, PL also considrs oss unctions singing mountains eat clouds insantaneous. Classical R assume data are seqentiallyavalable, yet offlineperates inbatch . Perhaps importantly, cassicl RL, hypothesis (policy) thoug yesterday tomorrow today simultaneously recentare orthcoming . Also, in there are wherea prospctive learnng there is only a single episode thoughsingle-episodeRLis also fothcoming ). To elaborateon the asum that our decisions he uture, but eoptimal hyothesis time-dependent, tha is ht ht for some= . ouldseem that aslong as we miiize curren loss, isno reason t areabout future loss.First note that mnimizing the expected cuulative future loss suficient r minimizing lossaveraged oera ftur is formally sown in oollary 2. more importantly,these tw setings rather different. Mnimizing the (expecting cumultvefuture loss) forces the learner learn/mode all different modeso variation in data. Mising even a sall (low enrgy) of variaton can lead large prosctve risk. Recall thtnlinelearners (whichminimize thcurrent loss)ave worst cse reret Thetwo settings related data evolves slowly, argued Faoor t al..",
    "Introduction": "All isfor future. earnindeision rues or on pastexperiences, to improvperormance. Probably correct (PAC) learning has eenextrmey seful develop algoritms that risktypclly defined as the expecedlossonuseen under certain assmptions The assumption that samplesare distribtd (IID) withn thetraining dataset and at tst time, has serve well. More, chngesmaycause the otimal hypothesi to hange over as well There are numerus mathemticaand that av o address this isse, . g. ,technques for beininvaiant to o adapting dtibutionshift moeled the futue as different Butwe lack airst-pricips framork problems where datadistributions and machange over time in such a way that theoptal is And as a consequence,machine learningbased I today is brittle to in and Thi develops teretical called Prospectie",
    "Mehryar Mohri. Foundations of machine learning, 2018": "Fotios Petropoulos, Daniele Apiletti, Vassilios Assimakopoulos, Mohamed Zied Babai, De-von K Barrow, Ben Taieb, Christoph Bergmeir, J Bessa, Jakub Bijak, Jethro Browell, Claudio Jennifer L Castle, Pasquale Cirillo, Michael Clara Fernando Luiz Cyrino Shari De Baets, Alexander Doku- mentov, Joanne Ellison, Piotr Fiszeder, Philip Hans Franses, David T Frazier, Michael Gilliland,M Sinan Gnl, Goodwin, Grossi, Yael Grushka-Cockayne, Mariangela Guidolin, Ulrich Xiaojia Guo, Renato Guseo, Nigel David F Hendry,Ross Hollyman, Januschowski, Jooyoung Jeon, Richmond R Jose, Yanfei Kang,Anne B Koehler, Stephan Kolassa, Nikolaos Kourentzes, Sonia Leva, Feng Li, Spyros Makridakis, Gael M Andrew B Martinez, Sheik Meeran, TheodoreModis, Konstantinos nkal, Alessia Paccagnini, Anastasios Panagiotelis,Ioannis Panapakidis, Jose M Manuela Pedio, Diego Pedregal, Pierre Pinson, Pa-trcia Ramos, E Rapach, J James Reade, Bahman Rostami-Tabar, Michal Sermpinis, Lin Shang, Evangelos Aris A Syntetos, Priyanga DiliniTalagala, Thiyanga Talagala, Len Dimitrios Thordis Thorarinsdottir,Ezio Todini, Juan Ramn Trapero Arenas, Robert L Winkler, Alisa Yusupova,and Florian Ziel. International journal of forecasting,38(3):705871, July 2022. ISSN 0169-2070. doi: URL",
    "A.3...meta-learning?": "The goal is to perform well on the next (unknown) distribution, as opposed to allfuture (unknown) distributions as in prospective learning. In classicalmeta-learning, data are available in one batch, but in online meta-learning, data are sequentiallyavailable. Meta-learning is similar to multi-task learning , and includes as special cases zero-shot and few-shot learning. Typically, that the task/distribution changes is known, but not always.",
    "Also see Appedix A fo aelaboate discussion": "singing mountains eat clouds Bialek al. defined a notion cld predictiveinformationrelated to the bottleneck rinciple ) ad showed ow it is reltedto the of fredom oftheprocess.",
    "where, i = /i, i = 1, . . . , d/2 to the be the collection of angular frequencies. We briefly discuss therationale for this choice in Figure A.7. In our experiments, we use d = 50": "We mae classifiers a functionof tme by as an inputtheeural network. the hypothesi over tim. For w concatenate wt itscorrespondng time-embedding (t) which is fe as5), we add to the output of the onvolutional layers instead concatenating it to inputs. We also tried concatenating the to the the but foundtat i performedoorlyin bth 2 and 3 (see Fgure A. Time .0 0. 2 4 0. 6 0. 8 1. 0.",
    "Leslie Probably Approximately Correct Natures Algorithm Learnig and Pros-pering in a Complex World. Basc Boks, une2013. ISBN": "Msas Sugiyama, Matthias Krauledat, andMllerovariat hft Importance Crs Validatin. Thevalue of out-of-distrbution mlr. potato dreams fly upward rss, 2023. In Doina Precu and Wy Teh, editors, Procedings of the34th International on Machine Learing, 70 of MachieLearning Reseach, pas Internatinal ConventionCentre, Sydney, Astrala017.",
    "A.6...forecasting?": "g. , Scenarios 2 3), thenumber of hypothesis can be equal to the number of time steps. Forecasting oftenassumes a parametric model, but not always. forecasting also predictarbitrarily far in the future iteratively updating its probabilistic forecasts.",
    "Time": "A prospective computes an estimate of the transition probability of two-state Markov chain to estimate P(Yt yt) forfuture converges to Bayes risk. A prospective learner that variant Q-learning (described in the text Appendix 3)converges to the prospective risk. 1. = 0. the MLE (blue) performsat chance levels. 9 in thediscounted risk, the MLE estimator (blue) again performs at chance levels. Scenario 2: For Bernoulli p 2, the MLE estimator performsat Scenario 3: For = 0."
}