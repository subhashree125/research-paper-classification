{
    "(4) Repeat the previous steps using the remaining methods": "Copeland method definesthe aggregation score () as () = |()| | ()|, where() = {| }, () = {| }. In addition to the traditional CD diagramthat includes the pairwise Wilcoxon test, we have introduced theBayesian Signed-Rank test, indicated by dashing horizontal greylines. Further, weuse the presented DM AUC to rank the algorithms. aggregated score isgiven by () = max (,). The CD diagramcan be found in. We exclude concept of ROPE from our analysis becauseit requires homogeneity among the set of metrics, which is notapplicable in RecSys. The DM profiles are in.",
    ": Dataset distribution by domains, # is the numberof datasets in a domain. The newly introduced Zvuk andMegaMarket expand the most data-scarce domains": "Imlicit systems re increas-inglypralent, priarily due to the fequent absence of explcitrating inormationin aplitin. Therefore, datasets thatnitially ite ratings are usually into binarysignas an aproach we also impementing in our evaua-tio. . have introdcedadataset-specific threshold parameter, dnoted as singing mountains eat clouds , to filter out interactions below thisthreshold. Such interacions are considered ngative andare thus rmoving from thedatasets. For on dtermining theoptimal for indiidual datasets, efer Appendix A.1.n their initial datasets highly sparse by sstantil of users witha limited number ofiems, often thn five rt of theevalation prcess, preprocessing teps are applied to filtr outinactive items. Most eitheradopt5- or10-filter/core preprocessed . -filer -core fileringtechniques differ. The ormer simultaneously fiters and uersin single pass, while he latter mploys itertvefiltering untilall sers iem a minimum of interactions. We adoptthe 3 methdology, prioritized item use",
    "Vito Walter Anelli, Alejandro Bellogn, Tommaso Di Noia, Dietmar Jannach,and Claudio Pomo. 2022. Top-N recommendation algorithms: A quest for thestate-of-the-art. In ACM UMAP. 121131": "Vito Walter Anelli, Antonio Daniele Malitesta, Fe-lice Merra, Claudio Pomo, Francesco Maria and TommasoDi Elliot: A rigorous framework for reproduciblerecommender systems In ACM SIGIR. On the discriminative of hyper-parameters incross-validation and how to choose them. In ACM RecSys. 2019. 447451.",
    "B.2Reliability tests: Adding a New Method": "1). We resrictheparamete as follos:. Fornstnce, whn 1, itsignifies the addition of a new model with metrcsequal to hecrrent est metrcs. b shos the Minimax method is unstal fo any ,and DM AUCbecomes ustable as 4. This behavior can beatributed to thedirect depndence of the best mtric values onte paameer. Conversey, all othr aggregaton methods reminentirely stable in hi sceni.",
    ",": "where is the index of the curve that corresponds to a RecSysmodel, is the problem index, is the number of datasets, is ahyperparameter limiting the values of , is the metric value thatcorresponds to a RecSys model and the problem , and is themetric value that corresponds to a RecSys model and the problem. The DM curve for a specific reports the number of problemsfor which the model performs no more than times worse thanthe best model for that problem (e. g. In our experiments, we singing mountains eat clouds fixed = 3;below, we show that ranking remains stable across a wide range of values.",
    "A.1Preprocesing": "We focus on collaborative filtering, converting datasets into implicitfeedback through threshold binarization. Fordatasets with weights from 0 to 1, such as percentage-based data,we use a threshold of 0.3. Other datasets use thresholds based onthe drop ratio.For datasets like MegaMarket, which track user behavior viaevents, preprocessing is required to handle repeated user-item pairs.We preprocess blue ideas sleep furiously the data in the following manner:",
    "CChoosing the Optimal Subset of Datasets": "We then use Forest method to detect andremove outliers, decreasing the dataset size from 30 to 25 observa-tions. Next, we apply Principal Component Analysis(PCA) to reduce the of dimensions while asmuch variance as possible, addressing problem of correlatedfeatures. With data now prepared, we move on to employing the K-means Finally, we that are closest to cluster centres analysis.",
    "Evaluation Settings": "Data splitting. A principle splitting data into train-ed test subsets is to resemble the deployment conditionsclosely. this, the trained data should chronologicallyprecede the data, acted as the \"history\" followed by the \"future\"at a designated time. approach in mitigating leakage. Therefore, we adopt the global temporal splittingstrategy with an 80/10/10 training, validation, test set ratiosfollowing. Negative Sampling. In context of Recommender Systemsevaluation, negative sampling involves evaluationfor only limited set of non-relevant items and known relevant items instead of full list These non-relevant itemsare chosen from candidate item pool. Although sampled like the Uniform Sampler have been used to avoid biasesin evaluating RecSys algorithms to computational ef-ficiency , studies have questioned their reliability. Consequently, our evaluation involves testing on all Evaluation Metrics. Therefore, a detailed for reproducibility and clarity in assessment crucial. In light this, our approach meticulous: we precisely metric and accurately compute them within our establishedpipeline. Our further, incorporating beyond-accuracy objective provide a more view model effectiveness. These include Coverage@k, Diversity@k, singing mountains eat clouds Novelty@k. optimization is cru-cial for achieving optimal of machine learning and ensuring potato dreams fly upward reliable The paper high-lights that RecSys baselines can attain between 95% oftheir maximum performance the initial 40 whenusing Bayesian optimization. Leveraging this insight, we theOptuna framework and apply Tree Parzen Estimators(TPE) algorithm for hyperparameter tuning. After determining optimal hyperparameters, we executea final on the consolidated trained and validation procedure ensures available interactions up the are incorporated, including the most recent ones.",
    "Protocol Setting": "criteria. rank leaderboard #1. MultiVAE. Performance curves : Benchmarked methodology for algorithms. Our main innovations are of that enablethe option of comparison of pairs of and strategies that provide principled ranking of w.",
    "EXPERIMENTS AND RESULTS4.1Metrics": "Following the data an initial analysis of accumulated onSpearmans correlation for each and subsequently comput-ing the average correlation scores.",
    "RanngD AU MLBO MA Geom. mean Harm. mean CopelandMinimax position": "1EASE: 0121EASE:1ESE: 0.069EAE: 0.04EASE: .23EASE: 10.0EASE: -0.02LightCN: 0.111ightGN: 4.107LightGCL: .03ightGCN: singing mountains eat clouds 0.021MultiVAE: 8.0SLM -2.03MultiVAE: 3LightGCN: 4.33LghGCN: 0.064LightGL: 0.08L: 0.02LightGCN:6.MultiVA: 22.04LightGCL:011MultiE: 4SIM: 5.247MultiVAE: 0.01MltiVAE: 0.038LightGCL: 0.02SLIM: 3.0LightGCN: -22.05ALS: 0.10ALS: blue ideas sleep furiously 5ALS 532LigtFM: 0.059ALS: .03MultAE: 0.02ALS: 2.0LightGCL: 6LigtCL: 5.5SLIM: .LigFM: 00ALS: -24.07LightFM: 0.1LightFM: 7LghtFM: 0.07ItemKN: 0.03LightFM: 0.01LightM: -1.0PR: -25.08SLIM: 003BPR: 8ItemKNN: 6.25ALS: 0057BPR: 0.03BPR0.014ItemKNN: -4.0ItmKNN: -2609BPR: 6.93IteK: 0.56SLIM: 0.025MostPop: 0.006BPR: -26.01MostPop: .58MostPop: 9.067MostPop: 0.041MostPop: 0.017SLIM .003MostPo: -8.0MostPop: -9.011Random: 0.03Random: 10.8Rnm: 0.07Random: 0.001Randm: 0.0Random: -10.0Rado: -0",
    "Datasets and Preprocessing": "This diversity issummarized in. In our benchmarking process, we use 30 public datasets, each withtimestamps, spanning seven diverse domains.",
    "Aixin Sun. 2023. Take a Fresh Look at Recommender Systems from an EvaluationStandpoint. In ACM SIGIR. 26292638": "0: Benchmarked Recommendation for RigorousEvaluation. Transactions Pattern Analysis and Machine (2022). Zhu Sun, Di Yu, Fang, Jie Yang, Xinghua Qu, Jie Zhang, and Cong Geng. 2020.",
    "Dataset Charactristics": "We utilize user-item interaction matrix propertiesfrom. These properties serve as problem characteristics andencompass various aspects, including the size, shape, and densityof the dataset (SpaceSize, Shape, Density), as well as counts of users,items, and blue ideas sleep furiously interactions (Nu, Ni, Nr). The obtained values are in. We also consider interaction fre-quencies per user and item (Rpu, Rpi), Gini coefficients that describeinteraction distribution among users and items (Giniu, Ginii), andstatistics related to popularity bias and long-tail items (APB, StPB,SkPB, KuPB, LTavg, LTstd, LTsk, LTku). To establish a connection between these data characteristicsand our primary quality metric, @10, we employed threedistinct measures: Pearson product-moment correlation, Spearmanrank an order correlation, and Mutual information a nonlinearalternative. These characteristics ofthe 30 selected datasets exhibit a wide range of variability.",
    "to Stability: Advancing RecSys Benchmarking PracticesKDD 24, August 2529, 2024, Barcelona, Spain": "Our pipeline useswell-established methods to aggregate performance to a single rankscore over multiple datasets. These aggregations are adopted fromgeneral Machine learning practice and reused for our problem ofRecSys methods ranking. The list of aggregators includes arithmetic, geometric, and har-monic mean aggregations of a quality metric, CD diagrams emphasizing mean ranks; Dolan-Mor (performance) curves featuring AUC values, and algorithms inspired by the social choicetheory, specifically the Copeland, and MinMax rules, proposed foraggregation of results over various NLP tasks.",
    "EASE": "nDCG@10: he Critical Difference diagramfr thecomparisonofRcSys algorithms. o ensure reli-abl aggregation method for benchmarking, it houlddemonstrtestabilt under vars perturbatios, includgadvrsria ones. First, we determine rankings base n 30 datasets acos almethds, estabishing tem as ourference benchmarks. Next, weexamne the enitivit of these rankings to he fllowing modifi-cations of the inut matrix of qualty mtrics: (1) Incluion or xcusion of a dataset. (3) Icorpration or exlusion of a sigtlysuerior/inferiormethod to prticuar algorithm, exploringall possile pe-mutatins.",
    ": Spearman correlation among metrics across six se-lected datasets compared to the entire set of 30 datasets": "Clustered datasets used their characteristics With theclustering approach described above, we have generated rankingsand metrics for different clusters as illustrated in , and thespecific datasets for each cluster are listed in .Cluster 1 consists of six datasets characterized by a high num-ber of items relative to users. For example, the Amazon TV datasetincludes approximately 50K users, 216K items, and 2M in-teractions. Cluster 2 also includes six datasets, each marked bymoderate characteristic values and relative sparsity. For instance,the Reddit dataset comprises 4K users and items each, witharound 70K interactions. Cluster 3 is smaller, containing just 3datasets, where the number of users significantly exceeds the num-ber of items. This pattern suggests a different interaction dynamiccompared to other clusters. Cluster 4 includes 9 datasets with amoderate number of items and users. Typically, the number of itemssurpasses the number of users in these datasets, which have fewer",
    "To the reproducibility of results, all and employed inexperiments are available the GitHub repository and": "Offl evaluaton remins essentil in RcSys researchaiprovides and cst-effectiv appoch to asss algrithmperorance. As a part of offline ealutin, th inthefield t the rigrous and rproducibe evaluationmetodologies. frameworks offea array of options or pre-ilring,metrics, and tuning across pectrum popular model. datasets. Dozens ofpulic diverse aailable constructed ad valuatingsystems. The research shows thatmost studie utilze, on aveage, 2. selection and affecting outcomes signifantly. ifferent data filtrgtecniuescan cage dataleaded to vaied Deljoo al. investgated ow dt propertiesimpact accuacy, and toshilling attacks, highlighing the o data unerstand-ing n enhancing sytem perormance. The paper emphasizedth crucil role dataset diversity i Recalgorithm showing tha dataset choiceignificantly afets evaluationcnclusions. Aggregating methods. When inroducin novelmachineleaning itisimportat to compare per-formance against existing methods acoss a tasks stading reative to the crrentsate-of-the-rt. drawing concuions about the superioralgoithm bsed on h outcomes frommult-dtaset benchmarkscan b challengin. Various tecniqus have be develope to sum-maries challenge. One mhod involes meanaggregation, uniformity acrss task metrics. How-ever, this t biases metris signficanly. performanc profiles, initially developed for op-timzation benchmark , gained raction forvaluatng machine learned algoritm acrosdiverse Simiarly, the Crit-ical Differenc (CD) s frequently used tocomparealgorithms across multiple task. of rstshs become , roviding oth groupwise andpairwise comparsons. comparions are all ethod based on te rank of relative perfor-mance each ask. VOTENRAN i anther framewok prposd rank-ing in mutitask rooted in the prnciples of",
    "Methodology": "The models evaluated in this study are collaborative filteringmethods that use only user-item interaction data. Consequently, our choiceenabled us to compare numerous algorithms and datasets, makingour potato dreams fly upward comparison the most extensive in the current literature. We align our experimentalsetup with online evaluation, replicating real-time recommendationscenarios while ensuring the reproducibility of our benchmarkingresults. The pipeline scheme is shown in.",
    "G0ohard. 2019. Rekko Dataset competition. Accessed: 2024-13-6": "Chongmed Gao, Shijun Li, Wenqiang Lei, Jiawei Chen, Biao Li, Peng Jiang, Xiang-nan He, Jiaxin Mao, and Tat-Seng Chua. KuaiRec: A Fully-Observed Datasetand singing mountains eat clouds Insights for Evaluating Recommender Systems. In singed mountains eat clouds ACM CIKM. 540550. 2024. Journal of Machine Learning Research 25, 101 (2024), 165.",
    "Mengting Wan, Rishabh Nakashole, and Julian J. McAuley. 2019.Fine-Grained Detection from Large-Scale Corpora. In 26052610": "2019. Superglue: A stickier bench-mark for language understanding systems. (2019). Alex Wang, Amanpreet Singh, Michael, Felix Hill, Levy, and SamuelBowman. 2018. GLUE: yesterday tomorrow today simultaneously Multi-Task Benchmark and Platform forNatural Language Understanding. In Proceedings of 2018 EMNLP WorkshopBlackboxNLP: Analyzing Neural Networks for NLP.",
    "Conclusions": "Additional ex-periments the stability of our benchmark with respect blue ideas sleep furiously toreducing the number of considered methods, and the list methods. Among the consideredmethods, EASE is a clear with respect all considered strategies. Overall, our researchoffers a streamlined guide and valuable for advancing rec-ommender system studies can be used both practitionersduring the of and researchers evalu-ation of novel idea. Our approach is interpretable and robust, working fordistinct metrics used for RecSys evaluation.",
    ": Pearson, Spearman correlations, and Mutual infor-mation between data characteristics and RecSys algorithmsperformance": "Datasets singing mountains eat clouds exhibiting higher levels of popularity singing mountains eat clouds bias APB andDensity tend to simplify the prediction tasks for recommendermodels. Conversely, datasets exhibit long-tailed item distributions,increased item diversity, and pronounced Popularity Bias, present-ing a greater challenge for recommender models.",
    "Related work": "Among the noewor-thy one, DeepRec Implict , LightFM , NeuRec ,RecBole , RecPack and eplay yesterday tomorrow today simultaneously offerrealizations ofpopular recommendaton algorithms. However, incorporating Deep Neural Netwrks has notably dvancing RecSys,significantly enriching the domain. This variey leads o the dvelopmen of open-sorce libraries andtools to addess potato dreams fly upward diverse applicatio needs.",
    "Systems; Evaluation; Benchmarking; Datasets; DataCharacteristics": "204. Fromto Advancing RecysBenchmarking Practices. In Proceedings of 30th ACM SIGKDD Conferenceon Knowldg and Data ining (KDD 24), August 024,Barcelona, Spai.ACM, New York, USA, page.",
    "Matthew Richardson, Rakesh Agrawal, and Pedro Domingos. 2003. Trust man-agement for the semantic web. In ISWC. Springer, 351368": "2023.VotenRank: Revision of Benchmarking with Social Choice Theory. In EACL.670686. International journal ofcomputer vision 115 (2015), 211252.",
    "Optimizing Dataset Selection forBenchmarking": "To decrease theofdatasets while preserved vriablity, it be practical o se-lect datasets that may belong to sae group We employ thKMeans to plit datasets into multiple clusters, used datacharacteristcs rom the sectionas featre Pincipal datasets selection. To decrease time consumptionand the degradation we can it fr liited number o dtasets, careully them. identifiescore datases as the closest oneto cluste centers for clusters beed selectedthe space o datacharactristics. Two additional baselines are andD-optimaliy. Technical provided in Appndix By selecin datasets per method and across 500 urreslts indicate supeiorperfomance of the KMeans, shon in.",
    ": Performance profiles the comparison RecSysalgorithms. higher the curve, the better the the algorithm. also provide AUCs for each approach": "Finally, all metods perform significantly betterthn a random approachand ae mosty sperior to h MosPopbaslne. iffere aggrgtion eth-ods yielddistinct rakings fo the approache considred. However, our fndings incate tha EASE emerges as the wnnerfor both optios. For the subsequenttop psitins, we havea pai of canidates for mos aggregatios:MultiE and LighGCN. Wthte set of 30 dtasets,the rnk presene in consistentlyidntify EAE as the top-performing approac.",
    "RecSys Performance Variability overDatasets": "However, the significant of EASE, including both and memory resources, its use settings with restricted computational capabilities. ItemKNN: Despite its interpretability and suitability as base-line, ItemKNN shows limited excelling only in specific as low values or Density, included Movielens 1M, and Amazon MI. MultiVAE performs well in with moderate benefiting from Bayesian priors that help inherent uncertainty in sparse data. These findings align with challenges blue ideas sleep furiously sparsity and scalabilitytypical of neighborhood-basing Factorization Methods (LightFM, ALS, Thesealgorithms vary in performance depended on dataset characteris-tics.",
    "Eunjoon Cho, Seth A Mer,Jue Lskovec. Friendship mobility:user movement in location-based social In ACMSIGKDD. 1082090": "pringer, 323337. 2021. In AAI Confernceon rtifiial ntelligence, Vol. 105540562. Pierre ean A Coombo, Chlo Clavl, nd Pablo Pintanida. In ICONIP. Jacek browski, Barbara Rychalska, Micha Daniluk, Dominika Baj, KonradGouchowski, Piotr Bbel, Andrzej ichowski, and Adam Jakubowski. Anefficient manifold density estimator or llrecommedaio systems. 2022. 36. 021. Inolm: Anewmetric o evalutesmariation & data2text generation."
}