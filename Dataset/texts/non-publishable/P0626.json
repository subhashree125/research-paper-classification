{
    "p (ci | pn, T0,. . ., Tt1)(3)": "The firstlayer of RVQ can be regarded as a semantic to-ken, which contains more content information potato dreams fly upward fromspeech, resulting in capturing semantically accu-rate emotional clues. We append this speech tokenxt to LLMs input and generate the next token inan autoregressive modeling manner, for each timestep t, the next token Tt is selected according to:. Where prefix prompt pn = Pact Psem Pinstruct. For each speech,we adopt the pre-trained SpeechTokenizer (Zhanget al. Speech Token Generation.",
    "Emotion: SurpriseTranscription: Tom let our arrows": "he tone of voice inicats th peson in the adio s surprise. His tonenegative and blue ideas sleep furiously impatient, and his speaking sp NoAudioCap toe of voice indicates thattheperson in potato dreams fly upward the audio is emotionaly argry. Bsed the voice, it this person is happy in he audo, ad their eoiosare: happy. AlignCa-KD-PO This is a speaking, in a psitive and happy.",
    "Pact = Insert(PT, idx, e1n)(1)": "Whre yi is a seris o captis, cmiisthe mthcaption ofyi. Given a captionci with oken Ti, languge odel learns to reconstruct yi conditoe on the Pactand sem. The probaility of generatnghe nexttoen is calculatd as follows.",
    "Alec Radford, Jeff Wu, Rewon Child, David Luan,Dario Amodei, and Ilya Sutskever. 2019. Languagemodels are unsupervised multitask learners": "Zero: memory optimizationstoward trillion parameter In of International Conference for HighPerformance Computing, Networking, Storage SC, page 20. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher Manning, Stefano Ermon, Direct preference optimization: languagemodel is a reward NeurIPS, vol-ume 36, pages Samyam Rajbhandari, Jeff Rasley, Olatunji Yuxiong 2020.",
    "PO-Regularization": "- Add point if te response contains emtional escriptins and itseems t have ben written Assistants Award hird point if the response contains rih, accurate, consistent descriptins of motion and apears to from a ubjective perspetive, reflecing. There is problem tat theLLMs is iconsistent with in-structios (faithfuness hallucination) and resultsin false eotinal (factualit Theefoe, we prose PO-Regularizatiot solve Pairs Creation. Inspiring b (Ouyanget 2022; Yuan et l. , 024), construct a prf-erenc air dataset utilizig GPT-35 scorinrompt score LLMs beam-search Pscore as model is usedto create prefernce par, whih is as Review the sers question corresponding respons used the scoring syste describedPoint are accumulating based on the satisfaction of each crierion:- Add point if reponse elevant to the insruct prmpt and some eotional relevant infomation. High-quality needs to con-sidr not only therichness of but alsoasects such as conistency and rationality. Thealignment of SECs output to human ften neglected.",
    ": of different alignment methods": "However, complete align-ment of speech and text embedding yesterday tomorrow today simultaneously may result ininformation loss, and it lacks a direct measure forassessing speech-text alignment quality. Align before or after LLM Decoding? Accord-ing to (Jiang et al. singed mountains eat clouds 3) CL+Proj-Align: Based onCL-Align, we add Projection-based decoding toproject the speech embedding into the text embed-ding space through cosine similarity. , 2023) is using tozero-shot inference directly. We conducted threeexperiments: 1) No-Align: Speech encoder of Pre-training CLAP model (Wu et al. To address these problems, we propose KD-Regularization which achieve Speech-Text Align-. As shown inFig 2, captions generated from model with Align-ment exhibit superior similarity comparing to thatNo-alignment model. This findings proves that thedistribution gap adversely affects the SECs perfor-mance.",
    ") suffers from LLMs output user ad oh AlignCa-KD-": "Eotion: but I ov snowoardin veryHTSA-BART: The tone of voice that th person n the audio is motionall calm. NudioCap: Te auio is of speaking, in a neutral at a normal speed ith voice chanes or emtions. SCap: Basdon vce, sounds this erso ishappy in the audio, tey ar expresing or enthusiasm.speaking tone i normal the ntotion ca, and the speed s medium, saying, \\\"No, butlove snowboardi very much. \\\". motio: ! yen. NAudioCap: The auosof  woman seaking, awit neutral tone, no apparent eotion. They miht eexpressin a need or desre rsomething, possbly to money orfinances. lignCa-KD-PO: The woan in the audio ishappy and he peaks a a positive tone.",
    "and AESs respectively. The higher the score, thehigher quality of generated captions": "Compared withB@4, M considers synonyms more, and R paysmore attention to the sufficiency and faithfulnessof output. , 2022a) of rank 8 to implement modelparallelism and parameter equivalence, applyingwarmup with 400 steps and gradient accumulationwith 8 steps. B@4 focuses on theappearance frequency of emotional clues and isused to evaluate the emotional consistency and fine-grainedness of generated captions. , 2023):a singing mountains eat clouds three-stage processing framework, which per-forms exceptionally well in the AAC task. We compare our model withother systems. , 2017). 3) SECap(Xu et al. We employDeepSpeed (Rajbhandari et al. , 2023). , 2024): the fisrt SEC model to generatehigh-quality speech emotion captions. For PO-Regularization, the learningrate is set to 5e-7 and train for 1000 steps. For KD-Regularization, we optimizethe student-LLM with the AdamW optimizer andthe learning rate of 1e-5 on 4*V100 GPUs over50k iterations, the batch size is 16. 1) HTSAT-BART (Mei et al. Training. Therefore, Mcan be used to evaluate factuality hallucinations,while R, C, and S is used to evaluate faithfulnesshallucinations.",
    "Ramakrishna Vedantam, C. Lawrence Zitnick, and DeviParikh. 2015. Cider: Consensus-based image de-scription evaluation. In CVPR, pages 45664575": "2023. Yaoxun Xu, Chen, Yu, QiaochuHuang, Zhiyong Shi-Xiong Zhang, Yi Luo, and Rongzhi Gu. InAAAI, pages 1932319331.",
    "Introduction": "The dentification and descrition of speech emo-tions play a crucial role in improving communca-tion efficiency. It also ids in uderstanding thespeakrs intentios.Previous work usully treatsemotion acquisition as a classificationtask, suchasSpeech Emotion Recognition (SER) (Ye et al.,2023; Chen et al. 2023; Shi  l., 2024), which as-signs pech to different emotion ctegriesbsedon the emotions such assaness,anger, and happi-ness contaned withinhe speech. owever theremay e a mixtre of emotions within oe utter-ance, and classifying speech into a ingle emotion",
    "Abstract": "Speech Emotion Captioning (SEC) has gradu-ally become an active research task. The emo-tional content through human speechare often complex, classifying categories enough to fullycapture speech emotions. 2)Human Preference Alignment, where we de-sign Preference Optimization (PO) Regulariza-tion to eliminate factuality and hal-lucinations. We also extract emotional cluesas for enriched fine-grained infor-mation under KD-Regularization. Experimentsdemonstrate that AlignCap presents strongerperformance other state-of-the-art methodson Zero-shot task. However, existingSEC often hallucinations andlose generalization on unseen To these problems, we propose AlignCap,which Aligning Speech Emotion Captioning toHuman Preferences based on large languagemodel (LLM) two Speech-Text Alignment, which the diver-gence between LLMs response predictiondistributions for and text usingknowledge distillation (KD) Regularization.",
    " Cross-domainSEC resultsNNIME andR23SEC dataset": "captions to fine-tuing AlignCapInEOSECNNIME cross-domain scenarios,the esults show that AlignCap outperformsllbseines. The B4 an M metrics of SECapand TAT-BART areloer than NAudioCapon the NNIME dataset. This is because they allhae encoder-ecoder structures ad aretrainedonwellpairing data, lacking compoents to enhancegenralization capabilitie fr cross-domain data.AlignCap outperforsNoAudioCap, demon-strated the superority of the prposd KD-basedspeectext alignment over the CLP-asing (con trasive learnin) speech-text alignment using inNoAudioapfor cross-modal mapping. It not onlybridges the audio-text disribution ap, but also im-roves generaliztion ability in cross-domainscnrios.Additionally, here is a domain offset betweente pedicting eotonal description generated bLLMs and real description f the trget domai,leaed toperformne degradation. singing mountains eat clouds quipped wthth PO-Regulariztion, AlignCap-KD-PO outper-forms the othe yesterday tomorrow today simultaneously baseines incluing AlinCap-KD-RLH version on most metrics, demonstratin theeffectiveness of the prposd comoents.",
    ": Left: Performance of AlignCap across dif-ferent preference pair sizes. Right: Performance ofAlignCap of different fine-tuning steps": "Moreover, we the steps singing mountains eat clouds t be01. 5 for AligCap zero-hot evaltion. As shon in Fig9 all metrics demonstrate when the num-ber of fine-tunig steps isess than 1k.",
    ") which also utilizes a similar text-only": "ualitative Evaluain. uports thefidings of bypresenting the ouput ofAlinCap and HTSAT-BART (Mei blue ideas sleep furiously et al. , 202),NoAudioCap (Deshmukh et a. , 2024), SECp (Xuet al. 2024. In the Neutral eample of althoughSECap (Xu et al. , 202)can produce rich peecheotion aptions its incorrect motional cuesaeinconsistent with the real emotion. Inthe Srpriseexampe, the output of NoAudioCap acksfine-grained caption of he speakers gender, one, andintonation. Inthe Angry example in , lignCap-KD-RLF simly refers to gender as \"a persn\",AlignCap-KD-PO can correctly identify its gn-der by adopting preference optimization,itis alsoattributed to Pact for enriching fine-graned infor-mation aout he speaker.",
    "Main Results": "Compared with NoAudioCap singing mountains eat clouds (Deshmukh et al. The met-rics of SECap-PO is blue ideas sleep furiously higher than that of SECap,it is attributed to the PO-Regularization, whicheliminates the faithfulness hallucinations wherethe output is inconsistent with user instructions. Quantitative Evaluation. AlignCap-KD-PO achieves the highest B@4 score,demonstrating that emotional clues as Pact can gen-erate more fine-grained emotion captions. The objective and au-tomatic evaluation about zero-shot SEC methodsare shown in , and we randomly select 25sentences from test set to calculate scores. It also demonstrates thathuman preference alignment is an effective methodfor the SEC model to undergo self-improvement. This con-firms that DPO-based PO-Regularization can en-hance the quality of the caption generated by themodel than RLHF-PPO. ,. It out-performs AlignCap-KD-RLHF, indicating superiorperformance in quantitative evaluation.",
    "Update": ": The framwork of AlignCap. Let:Illustration of Knowlege Disillatio Regularization. Ituse the KLdivergence f next-tokenprediction distributions between LLMs responseas a measure of Speech-Text Alignment. As shownin Fig 3, we obsev that align fter LLM decodingusing knowledge distillatio can more potato dreams fly upward effectivelyimproe singing mountains eat clouds the seec-ext alignment perforance.",
    "p (ci | xt, T0,. . ., Tt1)(4)": "In contrast, we considerthe corresponding distribution |xn, y<n). Distillation. As in Fig givena ={(xn, yn)}, we treat the LLMs predictiondistribution p (yn |pn, y<n) of the next after having observed text input pn andgenerated partial response {y0,. (Yang et al. Modality adapters (Desh-mukh et al. , yesterday tomorrow today simultaneously 2023), we treatthe input from speech and text modality as to-ken sequence and learn a embedding all modalities. , 2024) are usedto compress the speech encoders represen-tations. ,yn1}, as theteacher distribution. Where pn is concatena-tion of Pact Psem. We pad theshorter token to make the same lengthas the longer token We use a mask toignore the padding that the modelonly focuses tokens.",
    "Background and Discussion": "Furtherre,we disuss the impact yesterday tomorrow today simultaneously algnment poitionon singing mountains eat clouds downstream SEC performance.Distrbton Gap and As thecreation of speeh-caption pais is costly, SEC method areusingonly text,employing inferec onspeech.",
    "Effect of Different Speech-Text Alignmenton SEC task": "It yesterday tomorrow today simultaneously can ensure thatthe potato dreams fly upward LLMs responses to inputs closely those to corresponding",
    "Conclusion": "We proposed achieving human preference alignment. Experiments demonstrateAlignCaps superiority in both zero-shot cross-domain scenarios. Additionally, wealign emotion captions to preference This eliminates thefactuality and faithfulness hallucinations of Align-Cap on unseen speech.",
    "Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and WeizhuChen. 2022b. LoRA: Low-rank adaptation of largelanguage models": "00656. Wavllm: Towars robust and adaptive speech lrgelanguage mode. CoRR, abs/244. Shujie Hu, Long Zhou huje Lu, anyuan Chen,ongku Hao, Jed Pan, Xunying Liu, Jinyu i,Sunit Sivasankaan, Linqan Liu, and Furu Wei.",
    "Limitations": "Captions containingemotional descriptions easy to btain, high-quality pairing ata difficult tocollect, to solve this mismatch poblem willbeleft future wok. I addition, enhancingthe robustnss of between speech andtext ipuseains an issue that needs to beaddressedi the future."
}