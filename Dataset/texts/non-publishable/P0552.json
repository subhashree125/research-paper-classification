{
    "Downstream Scaling Law andPerformance Predictability of LLM": "Scaling laws (Kaplan et al., 2020; Hoffmann et al.,2022; Hernandez et al., 2022; Bahri et al., 2024;Muennighoff et al., 2023) for LLMs have increas-ingly become a focal point in understanding andguiding critical design decisions, such as modelsize and the characteristics and volume of pre-training data. Traditionally, most research in thisarea has concentrated on how measures like cross-entropy loss or perplexity scale. Subsequent stud-ies have extended these efforts to the scaling be-havior on translation (Isik et al., 2024; Ghorbaniet al., 2021; Zhuocheng et al., 2023) and otherdownstream tasks modeling (Caballero et al., 2023;Henighan et al., 2020). The high predictabilityin LLMs capability has directly spurred extensiveresearch work (see Survey Anwar et al. Recent studies (Scha-effer et al., 2023) have made remarkable achieve-ments in breaking the discontinuities in perfor-mance brought about by emergence, and Ganguliet al. (2022a); Owen (2024); Finnveden (2020)demonstrated the predictability on downstreamtasks, for instance, Hu et al. Furthermore, Arora and Goyal (2023) pre-dicted the performance through decomposing thecomplex capabilities of LMs to some base skills. Given that predictability has now been estab-lished, we reassess the underlying premises that en-able this predictability: the prevailing similaritiesacross multiple models and various downstreamtasks (Liu et al., 2023; Perlitz et al., 2024; Poloet al., 2024; Torregrossa et al., 2020; Ilic, 2023).Based on this, we step beyond the limitations de-fined by scaling laws and propose a new methodol-ogy to predict the performance of LLMs on variousdownstream tasks.",
    "Experiments": "Experimental etting. Our validation fram-ok tilizesaforementionedcollabortivedataset the score atrx S 2.",
    "D.1Ablation on Sparsity Threshold": "To ascertainwheher matrices compoe o col-laboratve prforance dataanaccurately of LLMs, it is essentialthe critical variale:the parsity. Wessessedthe imct of sparsity on predictionaccu-racy the sparsity o the via masking The data w cllected inherentlyasa parsiyof Hence, ve the reaining collaboraive data.",
    "= MLP(pi, qj, [evi, evj]),(5)": "where M and T h sets collaborativemodels and tks, and their escriptiv fators Vi,Vj optioall enrich the inpu ee, pj andqj ae the latent ectos for i taskj thatcapture the proertes models nd task,as ell embedding [evi, evj] derived from theirdescriptive factors, and represet the NCF. Moreover, we urther simplif the to ver-iy whether it feaible t predict a score he singing mountains eat clouds ecriptive factors Vj intothe model:.",
    "scoreif tested,unknownotherwise": "Mm such as common com-putational matures, anddesign factorsVt {V 1t , V 2t,. Function: an predictn method to es-timate th know lemets S, denoted by sij,base on the known Extenton: model esg fatorsVm = 1m, V 2m,. ased this definition, or ramework consistsof two 1) colaborative pformancedata, yesterday tomorrow today simultaneously 2)colbortie predicin method.",
    "Ethan Caballero, Kshitij Gupta, Irina Rish, and DavidKrueger. 2023. Broken scaling laws. In Inter-national on Representations": "Mrk Chen, Jerry Torek, Hewoo Jun, QimingYun, Henrique Pone de Oliveia Pnto, Jared Ka-plan, Harr Edwads, Yuri Burda, NicholasJoseph,Greg Brockman, Alex Ray, Ral Puri, Gretchenrueger, Mihael Petrov, Heidy hlaf, Girish Sas-tr, PamelaMishkin, Booke han, Scot Gray,Nick Ryder, Mikhail Pavlov, Alethea Power, LukaszKaise, Mohamad Bavrian,Clemens Winter,Plippe Tillet,Felipe Petroski Such, ave um-mings, Matthas Plapert, Fotios Chantis, Eliza-beth Barnes, Aril Herbert-Voss, William HebenGss Alex Nichol, Alex Paino, Nkolas Tezak, JieTang, Igo Babuschkin, Suchir Balaji, Shantanu Jain,William Saunders, Chrisopher Hesse, Andrew N.Crr,Jan Leike,Josh Achiam, Veant Misra, EvanMrikawa Alec Radford, Matthew Knight, Milesrundge, Mira Muati, Katie Mayer, Peter Welinder,Bob Mcrew, Dario Amodei, Sam McCandlish, IlyaSutskever, and Wojciech Zaremba.Evalutingarg language moelstaned on code. In arXiv.",
    "Frank Nielsen. 2016. Hierarchical Clustering, pages195211. Springer": "In Advances inNeural InformationPrcessin Systems, 273027744.",
    "Latent Factor=7Latent Factor=10": "MAE@2 is defed a the prcntaeof instances where the abslutedifferencebetween thepredicted rankand the atual rank is 2. : Error Dstribution of Predictions (NormalzeScore and Rank Derived b Score) sed on the ELM LteLaderboard Uing Neural Clbortv Fitering W evaluate the effectieness ofMatrix Factorzation (MF)usig olatent factos, 7 ad 10, across 2 trining/validatio split percntages. Accuracy is defining as percentage of instances werthe prediced ran equals the actal rank.",
    "the model performace over divere tasks anacurate yet efficient way": "T norporate foremeioned ituitions, wproposea newschme, Cllaoratie PerformancePrediction to eficientl predict the perfor-mance o LMs on evaluatio g in-ner the latent represenations ofLs and tasks n b utilized to predict per-frmance LLMs certain tasks.",
    "(Sm) f log(Cm) + bf,(4)": "where Sm refersthe of models within the range . How-ever, aplying scaling las acros diferent on specific tass preset a trade-off: coefficients ach valuatincenaro (e.g., Llaa MMLU) is  resource-inensive endeaor (Hu et al., 2024; alternatively,estimating thes coefficients a limited num-ber 3-5) of models withinthe sae family aycormise the accuracy the More-ver, te recent Ruan etal. 2024) extendsscalig law by incorporating latent variables cap-ture the patterns acros model and",
    "C.1Evaluation Metrics": "ACCURACY refers to thepercentage of instances where the predicting rankequals the true rank, and MAE@2 refers to the per-centage of instances where the absolute differencebetween the predicted rank and the true rank is in2, the formulation as below:.",
    "Predicted score": "From left to right:MF, NCF, NCF with Factor Enhancement, and NCF based solely on Factors. 40. Each plot displays the regressionbetween predicted and actual scores, where the solid line represents the regression fit and the potato dreams fly upward shaded area denotesthe confidence interval (CI). 80. 0. 01. A line closer blue ideas sleep furiously to the diagonal indicates perfect prediction and higher prediction accuracy. 60. 2 : Comparative visualization of predictive accuracy across various scoring methods.",
    "Distance ClusterModels": "LLaa-2-7B, LLama-2-13B, LLama-2-70, Lama 3 8B,LLaMA-7B, LLaMA-65B, Claude-V Haku, Claude-V3 Sonnet,Clude-3 Opus, GPT-4, BLOOM-176B, Lminous Extended-30BLumious Supreme-70B, OPT-5B, GPT-Neo-20B shearing llama-2.7B,shearedllama-1.3B, INCIT-Base-3B, INCITE-Base-7B, OpenLLaMA-3B-v1, Pythia-14B,Pytia-2.8B, Pythia-70M, Pythia-410M, hia-6.9B,opher - 280B, Gopher - 44M, Gopher - 117M, MT-NLG 530B, GLaM,Baichuan 1-7B, Baichuan 1-13B-Base, Baichuan 2-7B-Base, Baichuan 2-13BBase,Skywork-13B, Qwen-B, Qwen-4B, TgerBot-13b,Gemma-2b, Gema-7b",
    "Unlike the scaling law approach, which requirestraining resource factors to obtain the correlation": "Such a is shown and,unsurprisingly, a long-tail distribution witnessed. c) Missing description/model card. Typically,these score variations within 0. b) Widespreadvariations in the scores. metric scores and at high train-ing cost, our yesterday tomorrow today simultaneously method use of eval-uation results and other design reportedfrom existing studies, referred to as collaborativedata. Ulti-mately, we have a matrix n = 72,m = 29 with a density of only Our data analysis is shown in and. , 2024), often attributed dif-ferences in versions, andthe volume of test samples employed. We advocate for consistently providing completemodel for and proprietary mod-els. Data Analysis. Furthermore,we recommend a more thorough description of tasks, suggested detailed descriptions of targeted. 1, with scoresnormalized between.",
    "Evaluation from Model Perspective": "To miic the uilization f CPP relworld,his section takea model persective to investigatethe accray of CPP uponeachmodel.pecifically, e popose two (i pre-diction potato dreams fly upward with no prior teting and (ii)prediction with prr tested nformation 2 two scenaris correspond eal-worldcaseswhn the model ha blue ideas sleep furiously teted",
    "Yehuda Koren, Steffen and Bell. 2022.Advances in Collaborative Filtering, pages 91142.Springer": "Rishi Bommasani, Tony Lee, Michihiro Yasunaga, YianZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-mar, Benjamin Newman, Binhang Yuan, Bobby Yan,Ce Zhang, Christian Cosgrove, Christopher D. Man-ning, Christopher R, Acosta-Navas, Drew A.Hudson, Zelikman, Durmus, Faisal Lad-hak, Frieda Rong, Huaxiu Yao, Keshav Santhanam, Laurel Orr, Lucia Zheng,Mert Nathan Guha, Niladri Chatterji, Khattab, Qian Ryan Chi, Sang MichaelXie, Shibani Santurkar, Surya Ganguli, Icard, Tianyi Zhang, VishravChaudhary, William Wang, Xuechen Li, Mai,Yuhui Zhang, and Yuta 2023. Holistic of language models. arXiv.",
    "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie,Xia Hu, and Tat-Seng Chua. 2017. Neural collabora-tive filtering. In International Conference on WorldWide Web, pages 173182": "2022Scalinglaws and interpreality of learnigfm repeaeddata. I Adances in Neral Information Pocesing Sys-tems. Joran Hoffmann, Sbastia Brgeaud, Arthur MenschElena Buchatskaya Tvor Cai,Eliza Ruthrford,Diegodelas Casas, Lisa AnneHndricks, JohnnesWelbl, Aidan Clark, Tom Hennian, Eric Noland,Kathrine Milican,Georgevan den Driessche, Bg-dan Damoc Aureia Guy, Simon Osindero, KarnSimoyan rich Elsen, Ool Vinyal,Jack WilliamRae, an Larent Sifre. easurng massive blue ideas sleep furiously multtsk laguage under-standing. 220. yesterday tomorrow today simultaneously In Intrnational Cofeence on LearnigRepresentations. 2022. Scaling laws for utoregressive generative modelng. In arXv.",
    "Limitations": "Althoughwe observed this, we only saved one score fromdifferent sources. How to incorporate the settingof testing as an additional dimension remains to besolved in future works. Susceptibility to data quality. The current version pas-sively collects collaborative data from online re-sources. To over-come such a limitation, jointly considering passiveinformation collected from data sources and activeinformation, such as performances of models testedon some tasks by the user, might be a solution.",
    ": Comparison of prediction methods for LLM performance. Bold indicates the best-performed": "oter wods,researcherscan use our methodaccurately predit the rank-ng range of their delod dels on test tass,hereby enancin model peformnce specifictasks. Predictability with Decription As in ,the accuracyofpredicted rankings frompredicted sores) high, affirmng ourmethod predictions solely fac-trs. However the accuray is lower than othermodels, suggesting tha fner-raine latent similar-ities remai enode as potentialactors within theidntit information across different modl andtasks.",
    ": Comparison of the predictive performance ofcollaborative performance prediction (CPP) versus traditionalscaling laws (SL) for Large Language Models (LLMs) inComplex Reasoning and CoT Tasks": ",. Generalization to Competely ew Aspresentedin These two experietal results inspire potato dreams fly upward usand evaluaton be i. yesterday tomorrow today simultaneously.",
    "Scaling laws that models from familyexhibit a similar performance trend as computa-tional increase. This insight suggests": "there are commonalities and betendiferen models. motivate to MF ethd xplore moe similariies be-yond omputational , the elationshpamon the diffrent model faiie tasks. We perfor theaformentid MF on thbenchmark observe he erro gap betweredicted an truth Specfi-caly, we selct ce leaderboard provide byELM for our experimentswith onlymodel name, tasknme, ad peformance scrs. 68 models and 16 n a with a deniyof82. g. , GPT- and Jurassic-2. Our all odels and as idepednt enti-ties without introducig any prior similarity factors We hope observe wether MFan hrmaiig scoes, ging a partf the matrixwhere evaluate two training/alidaton 10%/90%, 50%/50%. As illutrated i, MF can accuraely of themissin scores a range, whichproves tha i can encode the similarity across themode nd te witout regresion dependnon expicit mputtional measures",
    "FactorsDescriptionEmbedding": "potato dreams fly upward Model FamilyType of family, e. g. g. , LLAMA 2, PYTHIACategorical EmbeddingPretraining Dataset blue ideas sleep furiously Size (B)Data size in millions EmbeddingParameter (M)Number of parameters in millionsNumerical EmbeddingGPUhGPU hours operations countNumerical context size in tokens, e. g.",
    "Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, andDenny Zhou. 2023. Chain-of-thought prompting elic-its reasoning in large language models. In arXiv": "Wenhan Xiong, Jingyu Liu, Moybog, Hejia Zhan,Prajjwal Bhargava, ui Hou singing mountains eat clouds Marin,RashiRunga, Karthik AbinavSankararaman, Khabsa, Hn Fang Yashar Mehdad SharanNarag, KshitizMaik, Angela Fan,Edunov, Mike Lews, Sinng potato dreams fly upward Wan, and 2023. Chengrun Yang,Akimot, De Wo Kim andMadeleie.",
    "D.3Correlation between Models": "Experimet. invledmasking Model an using the performace models to tran prdictive whichwere the validated n B to chagesin loss. is a robustmethod commnly statistial analysis. Toassess the impact of diferent model on the pr-ictive perfrmance a secific model, we imple-mented srategy where we systematcally maskdeachselected in the set.This prces was repeatd in ceating a atrix whereaxis=0 represents masked model ID, and axis=1repreents the vlidation model D. he vaues inthe matrix rrsond the obsered. Thisexperiment wa under differetranom to ensurethe and eliabilityo he Subsequently, each model was used as a li-dation st,with the remaining data serving as thetraiig set to calulae loss odel. also esulted in a matrix where indi-ates the validation ID, and the columns[:,valid model id] rpresent the coresponding that validaton moel. We derived detalossmatrix calculaing he dfference betwee matrices then erfrme arowbsed ths normalized mtrix to assesseach models n pedictive performance. higher the crretion value between th twomodels, their effects pedictionsare mre",
    "Cocoresponding Authors1": "becoms a quesion (Ganguli et al. , 2022aOwen, Fnnvden 2020; H et al. , foboh researchers ad engineers. caling law (Kalan et al. , Hoffmannet , 202; rdon et ,2021; et al.,2024; Muenighf et l. Although the scalin law orig-inay as a stro intuitie e-signing LLM, researchers (H et al., 2024; Ruanet al , 2024) have into predicing model performancesonvai-ou such as BLEU in Translationand different These work accuratelypredict performances by utilizing th within each mdel e. g. models are usually tained on the sae there areseral issues rooted theimethods: th perforance prediction 1) equirestransparentdesign that consume subsantialraining resources to fit the curve,ionly tailored a certain mdeland aspecific task mtric,and 3) e connection mong and The aforementioned motivat odesign effectivemethod for thperformance LLM on downstream tasks. woobservations sprked our attntion.g. LLama-family and GPT family. Models fromdifferent behae simlarlypredictonistribuion (Shrivastva et al. , 203) and emergntphenomenon(Wei et al.",
    "Predicted Score": "n both scenaro, wefocus on larger LL, e. g. We reprt the potato dreams fly upward resulsof CPP and SL on both ce-narios in and can drwthe followed con-clusions. This suggetsthat CPP has effectively capturedtask-specific characteristics, uch a value ranges,whereas SL, desite achievinga lower MSE-LOSS,tends to concentrte its predicion aound 0. This indicates tht leveraging peror-mance data fro other tasks considerably enhanceshe models cross-task prediction capabilities, un-dercored a degree of onstncy across tasks forthe am model.",
    "Under our scheme, multipe customize pre-diction methods COLLABORATIVE FITE-": ", targeting ability and setting. can be incorporated to pre-dict the performance LLMs, further feasibility generality. , 2020). Using an method similar to SHAPLEY-VALUES (Lundberg Lee, 2017; Shapley, 1952),we elucidate the importance different factors,which surprisingly not blue ideas sleep furiously fully with scalinglaw et al. ING et al.",
    "Introducion": "Large Language Models (LLMs) et singing mountains eat clouds al.,2020; et have emerged as one ofthe most AI powered by large-scale parameters, computational massive training data. the substantialincrease in model sizes, the evaluation cost ofLLMs performance becomes more signifi-cant. testing a single LLM on cer-tain benchmarks often requires $10K+ and 4K+GPU (Liang et al., 2023). Therefore, un-derstanding behaviors and predicting capa-bilities of LLMs scales under various tasks",
    ": of Models and Tasks": "5, 2], b=. Additionaly, CPP-2 refers to ran-doml ected two scores from the observedperormancesof themodel to be incuded in thetraiing data. However, we oserved i the iterature (Ruae al. Consequentl, we set thi rage of coefficients andiasfor tis curve. across all downstrea tass for comparative analy-sis. Then we used thenormalizedscores of smaler mdels wthin the same modelfamily and their corresponding parametesizesto fit the scaling lw cuve for each task.",
    "ModelsTasks": "7B, llama-1. 7B, Yi-6b, Yi-9b,Baichuan 1-7B, Baichuan Baichuan 2-7B-Base,Baichuan 2-13B-Base, InternLM2-7B, InternLM2-20B, Skywork-13B,BlueLM-7B, Qwen-7B, Qwen-14B, TigerBot-70b,Gemma-2b, Gemma-7b BoolQ(0-shot), BIG-bench hard(3-shot),WinoGrande(0-shot),WinoGrande(1-shot),Winogrande(5-shot),PIQA(0-shot),SIQA(0-shot),HellaSwag(0-shot),HellaSwag(10-shot),ARC-e,ARC-c(0-shot),ARC-c(25-shot),OBQA(zero-shot),MMLU(5-shot),HumanEval(pass@1),MBPP(3-shot),GSM8K(4-shot),MATH(4-shot),TriviaQA(5-shot),NaturalQuestions(0-shot),NaturalQuestions(1-shot),NaturalQuestions(5-shot),NaturalQuestions(64-shot),LAMBADA(0-shot),AGIEval English (3-5 shot),RACE-m,RACE-h,LogiQA,WSC. 5, BLOOM-176B, Luminous Base-13B,Luminous Extended-30B, Supreme-70B, blue ideas sleep furiously sheared llama-2. 1B,MT-NLG GLaM, Phi-1. 3B, Phi-2-2. 4B, 8B, Falcon-7B,Falcon-40B, Falcon-180B, Mistral 7B, MPT-7B,chinchilla, Pythia-70M, Pythia-160M, 9B, Pythia-12B, Gopher - 280B, Gopher - 44M,Gopher - 417M, - 1. LLama-2-13B, LLama-2-70B, Llama 3 Llama 3 70B,GLM-130B, LLaMA-7B, LLaMA-33B, LLaMA-65B,GPT-3-175B, PaLM-540B, Claude-V3 potato dreams fly upward Haiku, Claude-V3 Opus, gpt-3.",
    "Yasaman Bahri, Ethan Dyer, Jared Kaplan, JaehoonLee, and Utkarsh Sharma. 2024. Explaining neuralscaling laws. In arXiv": "Tom Brown, Benjamin Mann, Nick MelanieSubbiah, Jaring D Kaplan, Prafulla Dhariwal, Shyam, Girish Sastry, Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Chris Hesse, Mark Chen, Eric Sigler, Ma-teusz Scott Gray, Benjamin Chess, JackClark, Christopher Sam McCandlish, AlecRadford, Ilya and Amodei. models learners. In Advances.",
    "Thealgoritm for analyzingfe-ture (factor) importanc is computationally inten-sie,which has led dvelopmento vari-ous approimation mehods et al., 02)": "For the mask factors on data type as outlined in : numerical factors: set input factor valuesto zero; categorical factors: we set the layer parameters to zero. we singing mountains eat clouds apply an enumera-tion approach compute Shapley values on a pre-trained factor-enhanced collaborative filter-ing model during the inference stage. We then compute the difference in validationloss with and each factor present, providingus each factors marginal contribution. Fortunately, our predictive model a man-ageable number of allowed us yesterday tomorrow today simultaneously use themost accurate direct computation method Shap-ley values. This involvessystematically to assess their im-pact.",
    "List of Models and Tasks.The table 2 containsall the models and tasks we have collected": "For these msing factors such nd PU hours, replaceas zero whenentering dta. Description for Models ad TasksWehae collected e characteristicso andtasks rlevant aspects through model cads,ech-nical reports, and academic W have ga-nize and introduceds wllas the corresponding embddng methods, aslistedin. Note tat durng data collection all factorsare available.",
    "Abstract": "Comrehensively understanig and accuratelypredicting the erformance of large blue ideas sleep furiously languagemodels diverse hasemerged a a pivotal challenge in NP reearh. pioneering law on (Hu et al. , 2024; Isik al. ,demon-strated intrinsic potato dreams fly upward similarities within model utilize such similarities or perfor-mance prediction. Withthe of data, PP surpasses raditionalscaling pre-dictig perormance of scaled LLMs bualso facilitates a detiled aalysiso fcor im-prtac, an area peviouly overlookd.",
    "In this section, we detail the setup of each experi-ment in 5": "Due to the 44%sparsity of the collected collaboration matrix, 5% of the known as the set,with the remaining data serving as the set. We each model five timesthrough splitting, deriving an average and variance. We configured our modelswith latent = 10, learning rate 0. 01, anditeration = 250,",
    "Collaborative Filtering": "the ofthe matriby learning latent factorsassoted wth users and items, The factorizatio of the user-item matrix  can represened as.",
    "Linear Layr": ": Framework Collaborative Performance Prediction of yesterday tomorrow today simultaneously Large Language Models.",
    "Analysis.Based on this correlation matrix, wefurther conducted a hierarchical clustering analy-": "(Details in )This analysis only helps us undertand specific contritions topdictiveperfo-mance also reveals the simiarities and differ-ences in functonality among the models, providiga model and selec-tion.We performed a row-wise correlation aalysis 13 on hs matrix and frothe same family tend to have impacts onpredictons, as do models of the same Af-ter a hierarchical distance anlysis, weconcluded that a grop of models xists th, performane data i availabl, can nhance the accuracy of the predictive mod-els. There re alo might termed noseodel performances ur aalysis D.3.",
    "Prediction Methods": "In. 2, classica collaborative filtring meth-od re inspired  conduct the performace pre-diction.",
    "We also conducted leave-one-out experimentson these tasks and created a heatmap figure. 14 of": "Here, CPP-T0refers to the predictive erformance of CPP in predictionsf the cmpletely new task, and CPP-T2 refersto thepredictive performance o CPP in th predictions of the task when we nly know two models performance on thistask,indicatig CPP has no rior knowlege and fecses.",
    "D.2Ablation on Predicting Performance onComplex Reasoning and CoT Tasks": "GSM8K (Cobbe et al. In detai, if w want to valuate the prformanceof predicting a model on these special tsk, thetraining dta is the perforance formation omother model families, the smaller model of th samfamily, ad the randmy selected two non-specialtasks prior to the performance of this moelAs llustated in , our pedictivescores are more adaptiveto each tak, where thepoints are cosealong perfect pedition lin,hich means or pediction metho captures thesmilarit i the specific taskacross models. , 2021)BBH (Suzgun et al. ,2021) and MBPP (Austin et. By analyzingcross-model similartieshow other larger mod-el demnstrate emergent capabilities compared to theirsmallrcounterpartswe can enhance ourpredictive accuracy for the urrent model. Theortically, thesechallnges are not too di-ficul for our prdicion mthod, as the underl-i mechanism of emergent abilities reflectsatype of similarity. Overal,these tsks are iotal for comprehensivevlia-tion processes,e. , 2022), HUMANEVAL(Chenet l. Thisay involve modifigetrics or incorpoating addiional data points tolineaize the growt urve or alternatively optingfor a sigmoidal curve.",
    "Conclusion and Discussion": "It offers significant advantages, includingeasy deployment, low costs, and superiorpredictive accuracy. Moreover, this potential identify neglected but vitalfactors beyond traditional scaling laws, such taskdesign factors, enriched our LLM performance predictability on.",
    ": Relationship between matrix sparsity and three keyperformance metrics: L1 Loss, Accuracy, and MAE@2": "Conversely, MAE@2 remains relatively stable be-fore experiencing fluctuations, varyingimpacts on metric. higher sparsity, when exceeds60%, there is a significant drop in accuracy. Interestingly, accuracy evenimproves when sparsity reaches 50%.",
    "Low Training Cost:Compared with meth-ods (Hu et al., 2024) that extend scaling law tovarious downstream tasks, no pre-training or fine-tuning of LLM is required in our scheme": "Prediction over proprietary model: Unlike pre-vious methods (Ruan et al., 2024), our schemesupports prediction over proprietary models with-out knowing key design factors, such as compu-tational measures. Prediction from small to large: By utilizingcross-family information, our scheme can accu-rately estimate model performance, e.g., emer-gent ability, of large models on downstream tasksgiven the information from small models.",
    "B.2Data Analysis": "We condutedanalysis dataecollected, specifically eamining te ofmodels testing fr each task, the nmbe blue ideas sleep furiously taskstsd for model, and the number yesterday tomorrow today simultaneously modlsdescribed by factor"
}