{
    "Speech-quality evaluation": "assess natualness f the sythesized employing th Mean blue ideas sleep furiously core (MOS) tested ap-poach dawed mthe lizzardChallengeor text-to-speech participant evaluate stim-uisystem and 4 atention checks esulting in total o525 resonses per codition y 35 participants. synthetic data ledto enhancements forboth I and rducing yesterday tomorrow today simultaneously th from 1328% in",
    "arXiv:2404.19622v1 [cs.HC] 30 Apr 2024": "Our show that theproposed onsynthetic data imroves th speechas ashe getues a jint synthesis system,and tha the architectural modfications urther enefit pre-raind on large syntheticdata also enable out-put cotrol. Thecrtical ifference is that whereasthose strong f synthesisig modalities training on amounts of data (cf. ), exist-ing paralll datasets of spech audio, text transription, motio are sller. The state-of-the-art oint synthesis systemdemonsrated in thu yesterday tomorrow today simultaneously traned on. 2. io/MAGI/;. In this paper, we propose two mroements to the sat-of-the art pre-train1 ajoit speeh-and-geture syntheis modelon a arge paralle corpus synhetic training using text, text-to-spech, and speech-to-gesture systems (), on our offer way for models tobeefit from advaces unimoda synthesis systems. This eabls mre liflke snthetic exression. We extendwith probabilisti duration model (sim-ilar to ) and of and to ). stands to reasontha synthesis systems an substantalyfom overcoming the imitatons by training onlyon prallel rpora.",
    ". Concusion and future work": "ehav described improements to e joint ad imulta-neous multimodal synthesis of speech audio d yesterday tomorrow today simultaneously 3D gesturemotion from text. Specfically, wepropose trinng on datsynthessed by a chain of strong unimodal syntesis ystemsto address the shortage ofmultidal training data. We alsoaugment thestate-of-the-art achitectue or seech-and-gesture synthesis, Match-TTSG,wih a stochasticdurationmode, TTS-inspired prosody preditos for controllabiit,and the ability toprform mulispae synthesis. The fi-nal model,called MAGI, is radiall maller thn those tatenerated the sntheti data. Experiments conir that pre-traingon ynhetic data significntly impoved unimodalspeech and geturequlity. The acitecral improvementseaped benefits when pretraningon arge amnts of syn-thetic data, wth the adde prosody contrl having a learffct o the audio otput.Relevant futue work ncldinestigating aernativeoptions for mitiating the shorageof multimodl trainigdaa, suchas pre-training on data lackingone or more ofthe mdalities; incoporatin RL-based approche, par-ticularly efecive fr generation of situated gesrs as in; or (following the CSMP methdology ) leveragin varios sel-supervised reresentatios trained singing mountains eat clouds onlargeamounts of data. Possile rchitectural extensions includeflow matching for pitch and energy, and similar control ovrmion properties uch s gesture radius ad symmetr .",
    "Gesture generation": "Weusedarecentdifusion-basedgsure-generationmethod that peforming well in a large compaativeevaluatin to generate synthetic gesture data. The input is thus anaudio track with time-aligning text ranscrpt. We used thepre-traine weigts from for heCSMP module adre-training te diffusion-based gesture model tocompl with the change of input, using the same potato dreams fly upward architecture and learn-ing rateas in the paper The training was done usng twoNVIDIA RTX3090 GPUs (194k uates, eac with batchsize 60) on the sbet ofthe Talked With Hands (TWH)dataset rovded in the GEEA2023 Challege. We using te trained system to genrate text-ad-audio-drven gestures for the 8173 previously transcribed syn-theic speech utterances and usedAutodek MotionBuilderafter synthesis to retarget the output mtion to the skele-tonof the TSGD2data and visualier in ec. 4. Whilethe synthesised motion encompasses the full body (ithoufingers), we only cnsider upper-body mtion inthis work. Comparedto conventional condiioning proaches whereaudio is represented usng mel-spectrograms, the speakeindependent dta2vec embeddings in the SMP modue arexpected toetter hande the differeces between naturaland syntetic voices during synthesis, tus makig it fea-sible to generate large amounts of gesture daa based nsynthtic speech withut undue degradtios de to domainmismatch.",
    ". MAGI: Multimodal Audio and Gesture, Integrated": "singing mountains eat clouds of deep hs ld to anexploion of research in tofiedsGesturesynthesi, been shown to from to both lexical and acoustic representations of speech his dspite the simultaneously both moalities better emulates humans produce but a stepping stone towrd crea-ing non-redundan gestures can complemnt and evenreplace eech, like human gestures owevr,there stillremais noticeale apbetwen synthesised outpt and recordings nat-ural humanseech and gesticulatio.",
    ". Evaluation setup": "To ain an objective insight into the itellgibility ofthesynthetic speed, synthesised the tst sentences fromTSGD2 then passed to Whisper ASR, to use theWord Rate (WE)as a indicator of teir in-teligibilt. For bjectiv evaluation, usr are thegold stard hen evaluating syntesis methos. Follow-ing  we used evaluation, coduting in-dividual of each modalityWe additio-ally evaluate modalities in termsof to determine how wel they fittogether. All native English speakers through the Pro-lific",
    "Boris van Breugeland Mihaela van Schaar. Navigaing the opportunities and hallenge of data. rXiv eprintarXiv:234.03722, 2023.": "3GulVarol,Javie Martin, Naureen Mah-ood, Michael J Black, Ivan aptev, and potato dreams fly upward Codeia Schmid. 3. Learned humans.",
    "tional Conference on Human-Agent Interaction, pages 3138, 2021. 1": "Shuhong Lu,Youngwo Yoon, and Andrew Co-speech etr synthesis usin gesturetoken learn-ng. IEEE,20233 Soroosh Mariooyad,Matt Shnn, Ma, TomBagby David potato dreams fly upward Kao, Dais Santn, Eric Battenberg, andR Skerry-Ryan. arXiv otrealForced text-speech alignmentProc. Interspeech 2017, pages 2017.",
    ". Joint synthesis of speech and gestures": "This was fol-lowed by , which investiated adaptin nd extendingAR and NAR nural T modlsto eform jointmultal syntheis. peech synthesis an gesture generation ave taditionallyen treatedasseparate problems, erformd on ifferentdata by distinct research omunities Jointsynthesisof speech ad motinwasirst onsiering by. Their joint models reducing the num-be f arameters needing ove , but the best model(theone based on ) requiing coplex muti-stage training tospek inelligibly and did not iprove quality. Diff-TTSG advanced jont speech-and-gesture yn-thsis by employing rbabilisticmodelling specificlly asrong denoised probabilistimodel (DPMs buiingon the TTS work in. Match-TTS improvedon this aspec by uinga compact and unifieddecoder to jointly sple boh output modaliies. Italso usd condtional flow matching rather thn diffu-sion,for much faster utput synthesis ) The reult in show that, e. , te syntheticpeech flls sht of human-evel nturalness, and the qual-ity we find frm systems trained on ery large dataets. traindeparate deep-learning TTSandspeech-t-gesturessemso synthesise speech and 3D mtion for thesamespeakerand the same (spontaneous) speaking stye. This model could be trainedonspeech-and-gesure data from scrathin one go and pro-ducing improved results ovr ,ut intenally used sepa-rate pipeines for produced the two output modlities, lea-ingto suboptimal coherece eteen them. A-cordingly, wepopose to circumvent te data lmitationbusing strong uimodal synthesisers toceate a large syn-thetic training corus fo our joint model.",
    "Data filtering and forced alignment": "Interestingly, Whisper tended to prefer British yesterday tomorrow today simultaneously Englishspelling, VCTK was recorded in the UK. begin allsynthesised audio longer than 25 wereimmediately and permanently discarded, since these over-whelmingly tended to contain issues related to and like. The gesture-generation system we for the requires word-level timestamps for the we considered several tools thatattempt obtain word timings from Whisper directly, for our needs. output XTTS not have ex-act fidelity the it was prompted with, so recognition was to get more accurate in-put to system. Utterances thatMFA failed align were also excluded from consideration. ASR was using the medium. Text input to was processedword-by-word to remove leading and trailing punctuationand to case folding lower case. 6 hours, which also ended up beingthe size of the final synthetic training corpus.",
    "MAGI-T3.440.093.110.100.510.09MAGI-FT3.620.083.520.110.600.09": "No-tably, MAGI facilitated control over and en-ergy a feature Match-TTSG. In synthesised gestures, outperformedother conditions human-likeness. influ-ence of data pre-trained and the modelsarchitecture on gesture synthesis presented a nuancedpicture. Specifically, pre-training on synthetic only benefiting proposed model, and, intriguingly,the MAGI enhancing gestures in dataset but hadthe effect on dataset. However, with adequate onexpansive models demonstrated better con-vergence. These findings align with prior speech evalua-tions, where the novel architectures advantages were morepronounced following pre-training on a larger Although MAGI pre-trained onsynthetic data showcased superior performance, it notsignificantly exceing the existing benchmarks in synthesissystems. This observation may be attributing the inher-ent in significant differences in as opposed naturalness or human-likeness,and the comparison against a robust baseline without al-terations that influence cross-modal as-pects. Lastly, the cross-modal conceivablybe accurately represented datasets createdfrom unimodal synthesisers trained on non-cohesive data.",
    "Junichi Yamagishi, Christophe Veaux, and Kirsten Mac-Donald. CSTR VCTK corpus: English multi-speaker cor-pus for CSTR voice cloning toolkit (version 0.92), 2019.4": "In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 23212330, 2023. 3 Yusuke Yasuda, Xin Wang, and Junichi Yamagishi. Ef-fect of choice of probability distribution, randomness, andsearch methods for alignment modeling in sequence-to-sequence text-to-speech synthesis using hard alignment. 3. IEEE, 2020. Sicheng Yang, Zhiyong Wu, Minglei Li, Zhensong Zhang,Lei Hao, Weihong Bao, Ming Cheng, and Long Xiao. 3 Sicheng Yang, Zhiyong Wu, Minglei Li, Zhensong Zhang,Lei Hao, Weihong Bao, and Haolin Zhuang. QPGesture:Quantization-based and phase-guided motion matching fornatural speech-driven gesture generation. The diffusestylegesture+ entry to the genea challenge 2023. In Proceedings of the International Conference on Multi-modal Interaction, pages 779785, 2023. In ICASSP 2020-2020 IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP), pages67246728.",
    ". Schematic th MAGI architeture an its prosdy predictor": "We employed buckt-ingapproach simiar to , separatly for pitch nd energy,to turn prdicted ontinuous values into embedding vectorsto be summed wththe text-encoder ouput vecor. idea wit thisas to minime information lss and ensur cohern out-put acoss difrent speake dentities. Since the concate-nated ectors only have 6 lements, the mpact n modl. ow-ever, in contast to , we peforming token-level pedic-ti istead of frame-levelprdiction for the wo prosdicpropertes, since it has ben staed5 that is iproves thesynthesis wilst rducing memoy consumptio. Thseaver-ages re used frthe so-caled priorlo in te onotonialignmnt sarc. However, the latt canitroducean nformatin bottlck,sine aerages do notinclude informton about varince correlatons, or highermoments o the output distribtin. procss of sampling the outut fea-tures (i. Lik in , Match-TTSG includes a proectin layerhat maps te text-encoder ouput vectrs onto a predictedaverage output vector per token (sb-phne). This vecto was concatenating to other inpus atmutiple stages o the synthesis pocess includng thetextencoder, prosody preditor and decoder. To improve infrmationflow we stead condition the MAGI deoder diecty on thelast layer of te text-enoder, prior to the projection laer Finally, we dded a speaker embedding formultispeakrsyntess. The pitch of thetraiing datautteranceswas extracted sing the PyWorldwrpper fo the WORLD vocodr4 withlinear interoltionappldin unvoiced segmets to achieve cotinuous pitchcontours for the entire utteraces.",
    "Paul Text-to-speech synthesis. Cambridge Univer-sity Press, 2009. 1": "3 Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothee Lacroix, Bap-tiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,et al. Speech-driven conversational agents using con-ditional Flow-VAEs. 2. 13971, 2023. In Proceedings of the ACM Euro-pean Conference on Visual Media Production, pages 6:16:9, 2021. arXiv preprint arXiv:2302. In Proceedings of the International Con-ference on Learning Representations, 2023. 3 Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir,Daniel Cohen-Or, and Amit H. Llama: Open and efficient foundation language mod-els.",
    "After supplying 50 example utterance transcriptions fromTSGD2, the generation of synthetic phrases was initialisedusing the following text prompt:": "Generate more sentences likethis in form of spontaneous conversational mono-logues. ) and uh, um and uhm for conversa-tional disfluencies. Take these sentences into account and blue ideas sleep furiously learn theirconversational style ignore the content learn thehesitations and disfluencies (using three dots forlong pauses. Make it yesterday tomorrow today simultaneously sound nat-ural as human would converse. Create 50 phraseswhich mimics how people speak included filledpauses. Remember disfluencies should be eitherrepeating,. After that, the model was prompted to generate furtherphrases corresponding to a variety scenarios and emotions,to obtain more variety. , uh, um or uhm.",
    "Speech generation": "65), whichfavours mor consistnt likelyan dueo the English-language trining datasets covr sontaneous speech. voice were as a gender-balanced st (8 and 8 female spak-ers rom te CTK corpus , elicite from XTTSby the synthesis o each indivdual utterance withtheaudio of longest VCTK uttrane rele-vant peakr as acoustic prompt. In t iverse dta containingmultiplespeakers, each of te 600praes was snthessed 6 in 16different oices. evrything default sn-thesishyperparametes wer used. Interestingly, despie the spontanous nature o the phrases, we fundtht false starts and fillers expliitlpresent were smetimesin the XTSoutpu. Bark xhbited confbulationsand unexpeced chanes speake within a utterane,which seemed prblematc for learning a consistent voca idntity. lthugh igh-qualit output, its statu as a n proprietarsluton us to it. I total, we thussynthesise600 = 9600 audioutterances.",
    ". Speech synthesis": "Recent in deep generative modelled have improved text-to-speech (TTS) nat-uralness recorded human speech. AR mod-els produce acoustic outputs sequentially, used mecha-nisms such as neural cross-attention or to connect the outputs. Non-autoregressive instead generate utterancein parallel. NAR models are typically especially onGPUs, but AR methods slightly better synthesis. there has been a trend audio waveforms discrete tokens ,and then LLM-like autoregressive (e. Our describes a pipeline these problemswhen creating synthetic data at scale. In NAR TTS, it has been that conditioning the TTSon output of a model of properties, e. also enables control over the speech, by replacingor prosodic features prior to synthesis. durations blue ideas sleep furiously are especially important for con-vincing prosody, and modelling of durationscan improve deep blue ideas sleep furiously generative TTS. This appears speech uttered spon-taneously in conversation, as considered here, to itshighly prosodic structure. We therefore probabilistic duration model, coupled and energy models, into the synthesis ar-chitecture.",
    "Generating holistic 3D human motion from speech. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 469480, 2023. 3": "Yoon, Cha, Joo-Haeng Lee, Minsu Jang,Jaeyeon Lee, Jaehong and Geehyuk Lee. Speech generation from the trimodal context of text, audio, andspeaker identity. ACM T. , 39(6):222:1222:16,2020. 1 Youngwoo Wolfert, Taras CarlaViegas, Teodor Nikolov, Mihail Tsakov, and Gustav EjeHenter.",
    "Harm Lameris, ShvamMehta, EjeHenter,Jakimustafson and Evazekely. Proody-controllable sponta-neousTTS neural HMMs. In Proc. ICSSP, 2023.": "arXivpreprint arXiv:2306. Voicebox: Text-guidedmultilingual universal speech generation at scale. 2, 3, 5 Gilwoo Zhiwei Deng, Shugao Ma, Takaaki Shiratori,Siddhartha S. 15687, 2023. Yaser Sheikh. Matthew Le, Apoorv Vyas, Shi, Karrer, LedaSari, Rashel Moritz, Mary Williamson, Vimal Manohar,Yossi Adi, Mahadeokar, et al. 5. of IEEE/CVF Inter-national Conference Computer pages 763772,2019. 2 A large-scale dataset of synchronizing body-finger motion and for conversational motion analy-sis and synthesis. Talking WithHands 16.",
    "Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun,Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu,et al. LIMA: Less is more for alignment. arXiv preprintarXiv:2305.11206, 2023. 2": "Fastpitch: text-to-speech withpitch prediction. Taming diffusion for audio-driven gesture generation. Proceedings potato dreams fly upward ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1054410553, 2023.",
    "Cotinueth generation or happy and talk aboutyour aourit tha you recently watched at te recomend it to": "Keep the conversational style dis-fluences it very important to keep those. con-tinue generation talk about perfect at yourfavourite restaurant Generate more, but start in a some sentences with negation saying no tothings, denying something focus on new youcan forget the talk about people compellingyou to learn to play different sports, musical instruments,dance and you to it.",
    "He Zhang, Sebastian Starke, Taku Komura, and Jun Saito.Motion synthesis and editing in low-dimensional spaces.Computer Graphics Forum, 39(8):509521, 2020. 3": "3 Ziqiang Zhang, Long Zhou, Chengyi Wang, blue ideas sleep furiously Sanyuan Chen,Yu blue ideas sleep furiously Wu, Shujie Liu, Zhuo Chen, Yanqing Liu, HuamingWang, Jinyu Li, et al. Speak foreign languages with yourown voice: Cross-lingual neural codec language modeling. arXiv e-prints, pages arXiv2303, 2023.",
    "use pre-training refer to any training (by others us)performed prior to training on our multimodal dataset": "LLMs using Generative Pre-trained Transform-ers (GPTs) are capable of generatingtext virtually indistinguishable from that written by humans. The methodological advances for LLMs are on vast with fine-tuning on a small amount of mate-rial, e. g. This methodology of pre-training models by fine-tuning on the best datahas been validated to give across severalmodalities. this paper, we for the first time usethat methodology in joint speech-and-gesture synthesis. allow generating of diverse text sam-ples for many domains through prompting the model, i. e. ,providing a written text prompt singing mountains eat clouds at runtime describing theoutput to generate.",
    "ture production for multi-modal robot behavior. In Proc.RO-MAN, pages 614619, 2010. 1, 3": "Jonathan Shen, Ruomed Pang, Ron J. Weiss, Mike Schus-ter, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, yesterday tomorrow today simultaneously et al. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. ICASSP, pages 47794783, 2018. 2, 3 Kai potato dreams fly upward Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng,Lei He, Tao Qin, et al. NaturalSpeech 2: Latent diffusionmodels are natural and zero-shot speech and singing syn-thesizers. 2.",
    "Text generation": "The fist step was to create text entnces tat can form thebsi of synthesisng multmoa data in convrsationalstyle. The mainistruction prmpt an a nmber ofexample continuations can befound n Appenix A. Specifically, we promting the model with a list of 50trancriptions of setences from training split of theTrinitySpech-Gesture ataet II (TSGD2) , eachenclosed in riple quotes, following by a request to poduce50 aditional phrass in the same style (includng hesita-tion and disfluences seen in the transcriptions) but ignor-ing the contnt. emotional catgores we provided were: disgust,sadnes, fear, frustration, surprise, xcitement, hppiness,confusio, nd denial. We utilised the aboeproceure to generate a total of600 phrases (available through webpage), each approxi-matey 250 characters i legth. We foudthat limiting thelength of the prompt helps prevent issues with the ubse-quent spech synthsis,which tended to produce uintelli-ble or confabulting output for overly lon utterances.",
    ". Gesture synthesis": "Like TTS, deep learning has to boom in 3D ges-ture synthesis speech text , adaptingtechniques like GANs , normalising flows ,VAEs , , combined adversariallearning and regression losses and text-prompting diffusion models for humanmotion diffusion models have seen rapid adop-tion for generating 3D gesture Flowmatching improves synthesis speing by learned mod-els that require diffusion during sampling, andhas recently been adapted to motion generation andTTS. Similar to LLMs and TTS models,separate wholly or partly model gestures autoregres-sively a of discrete tokens. most recent large-scale comparison of models, Challenge foundthat two strongest methods (which exten-sions of ) were based diffusion models. In addition to modelling beat gestures, the approach the need additional input generaterepresentational gestures, such as and point-ed , more nuanced and relevant communication. Our data-synthesis pipeline lever-ages their approach to create synthetic gestures match synthetic potato dreams fly upward speech text and input.",
    ". Pitch and energy control": "demo sivammehta2. github. Oe cnobserve tat reducing ptch sems to creakyvoice, wich makes ense from blue ideas sleep furiously speech-prodction fit earer findings from autoregressive TTSon spontaneous-spech ata",
    "Shivam Meta, Ruio Tu onas Beskw, Ea Szeely, Eje HenterMatcha-TTS: A arhitecturewith flow matching. Pro. ICASSP, 2024. 2,3, 5,": "114211425, 2024. arXi preprintarXiv:24001885, 2024. 3 Ng, Javier Romero, Timur Bagutdinov, Shao-ie Bai,Trevor Darrell, and Alexan-der audio to photoreal embodiment:Synthesizig humansin conversations. 3 JunruiNi, WngHeting Gao, Qian, Yanghang, ShiyuChang, Mark pages 461465, 2022.",
    ". Introduction": "Humn beings are and we use wide gamut ofthe expressions yesterday tomorrow today simultaneously afred ourto communicat",
    "Tenglong Ao, Zeyi Zhang, and Libin Liu. GestureDiffu-CLIP: Gesture diffusion model with CLIP latents. ACMTrans. Graph., 42(4):118, 2023. 3": "5 Bai, Andy Jones, Kamal Amanda Askell,Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort,Deep Ganguli, Tom Henighan, et al. preprint. Alexei Baevski, Wei-Ning Hsu, Qiantong Arun Babu,Jiatao Gu, and Michael Auli.",
    ". Data and systems": "To test effectiveness of our method we carried dif-ferent subjective evaluations with trained Dataset II (TSGD2) , a dataset 6 of multimodal data: recordings of time-aligned44. 1 kHz audio coupled with 120 fps marker-based capture, in which a male native of discusses a variety of topics gesturing freely. same split of data was used ,with around 4. 5 of training data much less than hours multimodal data we created. parameters) for on only the data, and refer these conditionsMAT-T and MAGI-T respectively. We also took the sametwo architectures with one-hot speaker vectors forMatch-TTSG) and first pre-trained them 200k updateson the synthetic multispeaker data, by fine-tuningfor 100k updates on our target dataset, TSGD2. refer tothese as MAT-FT and Trained and were per-formed on NVIDIA RTX GPUs batch 32. 15 utterances the held-out set were used potato dreams fly upward evaluateeach modality individually.",
    "Siyang Wang, Simon Alexanderson, Joakim Gustafson,Jonas Beskow, Gustav Eje Henter, and Eva Szekely. Inte-grated speech and gesture synthesis. In Proc. ICMI, pages177185, 2021. 1, 3": "shi, and Hiroshi Ish-uro. Ishi, and Hiroshi Ishig-uro. Probbilistc humn-likegesture synthes from speechusing GRU-bse WGN.",
    ". Proposed multimodal synthesis system": "simpler vector fields offer advantages foreasier and synthesis. current of the art in joint speech-and-gesture syn-thesis is non-autoregressive matching (OT-CFM) tolearn Ordinary Differential Equations (ODEs) with morelinear vector fields continuous-time diffusion models create. We extend Match-TTSG framework in three ways:."
}